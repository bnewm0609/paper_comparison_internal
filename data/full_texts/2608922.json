{"id": 2608922, "updated": "2023-10-08 00:36:35.874", "metadata": {"title": "Places: A 10 Million Image Database for Scene Recognition", "authors": "[{\"first\":\"Bolei\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Agata\",\"last\":\"Lapedriza\",\"middle\":[]},{\"first\":\"Aditya\",\"last\":\"Khosla\",\"middle\":[]},{\"first\":\"Aude\",\"last\":\"Oliva\",\"middle\":[]},{\"first\":\"Antonio\",\"last\":\"Torralba\",\"middle\":[]}]", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2732026016", "acl": null, "pubmed": "28692961", "pubmedcentral": null, "dblp": "journals/pami/ZhouLKO018", "doi": "10.1109/tpami.2017.2723009"}}, "content": {"source": {"pdf_hash": "90e0fe05018ba16a88fe3503ebd3032fb4027d5d", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "publisher-specific, author manuscript", "open_access_url": "https://doi.org/10.1109/tpami.2017.2723009", "status": "HYBRID"}}, "grobid": {"id": "afdef2f28ae05d06c5fdf0042303d32fbfa953d2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/90e0fe05018ba16a88fe3503ebd3032fb4027d5d.txt", "contents": "\nIEEE Transactions on Pattern Analysis and Machine Intelligence Places: A 10 million Image Database for Scene Recognition\n10.1109/TPAMI.2017.2723009,\n\nBolei Zhou \nAgata Lapedriza \nAditya Khosla \nAude Oliva \nAntonio Torralba \nIEEE Transactions on Pattern Analysis and Machine Intelligence Places: A 10 million Image Database for Scene Recognition\n10.1109/TPAMI.2017.2723009,10.1109/TPAMI.2017.2723009This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 1Index Terms-Scene classificationvisual recognitiondeep learningdeep featureimage dataset\nThe rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach nearhuman semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.\n\nINTRODUCTION\n\nIf a current state-of-the-art visual recognition system would send you a text to describe what it sees, the text might read something like: \"There is a sofa facing a TV set. A person is sitting on the sofa holding a remote control. The TV is on and a talk show is playing\". Reading this, you would likely imagine a living-room. However, that scenery can very well happen in a resort by the beach.\n\nFor an agent acting into the world, there is no doubt that object and event recognition should be a primary goal of its visual system. But knowing the place or context in which the objects appear is as equally important for an intelligent system to understand what might have happened in the past and what may happen in the future. For instance, a table inside a kitchen can be used to eat or prepare a meal, while a table inside a classroom is intended to support a notebook or a laptop to take notes.\n\nA key aspect of scene recognition is to identify the place in which the objects seat (e.g., beach, forest, corridor, office, street, ...). Although one can avoid using the place category by providing a more exhaustive list of the objects in the picture and a description of their spatial relationships, a place category provides the appropriate level of abstraction to avoid such a long and complex description. Note that one could avoid using object categories in a description by only listing parts (i.e. two eyes on top of a mouth for a face). Like objects, places have functions and attributes. They are composed of parts and some of those parts can be named and correspond to objects, just like objects are composed of parts, some of which are nameable as well (e.g., legs, eyes). Whereas most datasets have focused on object categories (providing labels, bounding boxes or segmentations), here we describe the Places database, a quasi-exhaustive repository of 10 million scene photographs, labeled with 434 scene semantic categories, comprising about 98 percent of the type of places a human can encounter in the world. Image samples are shown in Fig. 1 while Fig. 2 shows the number of images per category, sorted in decreasing order.\n\nDeparting from Zhou et al. [1], we describe in depth the construction of the Places Database, and evaluate the performance of several state-of-the-art Convolutional Neural Networks (CNNs) for place recognition. We compare how the features learned in a CNN for scene classification behave when used as generic features in other visual recognition tasks. Finally, we visualize the internal representations of the CNNs and discuss one major consequence of training a deep learning model to perform scene recognition: object detectors emerge as an intermediate representation of the network [2]. Therefore, while the Places database does not contain any object labels or segmentations, it can be used to train new object classifiers.\n\n\nThe Rise of Multi-million Datasets\n\nWhat does it take to reach human-level performance with a machine-learning algorithm? In the case of supervised learning, the problem is two-fold. First, the algorithm must be suitable for the task, such as Convolutional Neural Networks in the large scale visual recognition [1], [3] and Recursive Neural Networks for natural language processing [4], [5]. Second, it must have access to a training dataset of appropriate coverage (quasi-exhaustive representation of classes and variety of exemplars) and density (enough samples to cover the diversity of each class). The optimal space for these datasets is often task-dependent, but the rise of multi-million-item sets has enabled unprecedented performance in many domains of artificial intelligence.  The successes of Deep Blue in chess, Watson in \"Jeopardy!\", and AlphaGo in Go against their expert human opponents may thus be seen as not just advances in algorithms, but the increasing availability of very large datasets: 700,000, 8.6 million, and 30 million items, respectively [6]- [8]. Convolutional Neural Networks [3], [9] have likewise achieved near human-level visual recognition, trained on 1.2 million object [10]- [12] and 2.5 million scene images [1]. Expansive coverage of the space of classes and samples allows getting closer to the right ecosystem of data that a natural system, like a human, would experience. The history of image datasets for scene recognition also sees the rapid growing in the image samples as follows.\n\n\nScene-centric Datasets\n\nThe first benchmark for scene recognition was the Scene15 database [13], extended from the initial 8 scene dataset in [14]. This dataset contains only 15 scene categories with a few hundred images per class, and current classifiers are saturated, reaching near human performance with 95%. The MIT Indoor67 database [15] with 67 indoor categories and the SUN (Scene Understanding, with 397 categories and 130,519 images) database [16] provided a larger coverage of place categories, but failed short in term of quantity of data needed to feed deep learning algorithms. To complement large object-centric datasets such as ImageNet [11], we build the Places dataset described here.\n\nMeanwhile, the Pascal VOC dataset [17] is one of the earliest image dataset with diverse object annotations in scene context. The Pascal VOC challenge has greatly advanced the development of models for object detection and segmentation tasks. Nowadays, COCO dataset [18] focuses on collecting object instances both in polygon and bounding box annotations for images depicting everyday scenes of common objects. The recent Visual Genome dataset [19] aims at collecting dense annotations of objects, attributes, and their relationships. ADE20K [20] collects precise dense annotation of scenes, objects, parts of objects with a large and open vocabulary. Altogether, annotated datasets further enable artificial systems to learn visual knowledge linking 2 PLACES DATABASE\n\n\nCoverage of the categorical space\n\nThe first asset of a high-quality dataset is an expansive coverage of the categorical space to be learned. The strategy of Places is to provide an exhaustive list of the categories of environments encountered in the world, bounded by spaces where a human body would fit (e.g. closet, shower). The SUN (Scene UNderstanding) dataset [16] provided that initial list of semantic categories. The SUN dataset was built around a quasi-exhaustive list of scene categories with different functionalities, namely categories with unique identities in discourse. Through the use of WordNet [21], the SUN database team selected 70,000 words and concrete terms that described scenes, places and environments that can be used to complete the phrase \"I am in a place\", or \"let's go to the/a place\". Most of the words referred to basic and entry-level names ( [22]), resulting in a corpus of 900 different scene categories after bundling together synonyms, and separating classes described by the same word but referring to different environments (e.g. inside and outside views of churches). Details about the building of that initial corpus can be found in [16]. Places Database has inherited the same list of scene categories from the SUN dataset, with a few changes that are described in section 2.2.4.\n\n\nConstruction of the database\n\nThe construction of the Places Database is composed of four steps, from querying and downloading images, labeling images with ground truth category, to scaling up the dataset using a classifier, and further improving the separation of similar classes. The detail of each step is introduced in the following sections.\n\nThe data collection process of the Place Database is similar to the image collection in other common datasets, like ImageNet and COCO. The definition of categories for the ImageNet dataset [11] is based on the synset of WordNet [21]. Candidate images are queried from several Image search engines using the set of WordNet synonyms. Images are cleaned up through AMT in the format of the binary task similar to the ours. Quality control is done by multiple users annotating the same image. There are about 500-1200 ground-truth images per synset. On the other hand, COCO dataset [18] focuses on annotating the object instances inside the images with more scene context. The candidate images are mainly collected from Flickr, in order to include less iconic images commonly returned by image search engines. The image annotation process of COCO is split into category labeling, instance spotting, and instance segmentation, with all the tasks done by AMT workers. COCO has 80 object categories with more than 2 million object instances.\n\n\n2.2.1\n\nStep 1: Downloading images using scene category and attributes From online image search engines (Google Images, Bing Images, and Flickr), candidate images were downloaded using a query word from the list of scene classes provided by the SUN database [16]. In order to increase the diversity of visual appearances in the Places dataset, each scene class query was combined with 696 common English adjectives 1 (e.g., messy, spare, sunny, desolate, etc.). In Fig. 3) we show some examples of images in Places grouped by queries. About 60 million images (color images of at least 200\u00d7200 pixels size) with unique URLs were identified. Importantly, the Places and SUN datasets are complementary: PCAbased duplicate removal was conducted within each scene category in both databases, so that they do not contain the same images.\n\n\n2.2.2\n\nStep 2: Labeling images with ground truth category Image ground truth label verification was done by crowdsourcing the task to Amazon Mechanical Turk (AMT). Fig.4 illustrates the experimental paradigm used. First, AMT workers were given instructions relating to a particular category at a time (e.g. cliff), with a definition, sample images belonging to the category (true images), and sample images not belonging to the category (false images). As an example, Fig.4.a shows the instructions for the category cliff. Workers then performed a verification task for the corresponding category. Fig.4.b shows the AMT interface for the verification task. The experimental interface displayed a central image, flanked by smaller version of images the worker had just responded (on the left), and the images the worker will respond to next (on the right). Information gleaned from the construction of the SUN dataset suggests that in the first iteration of labeling more than 50% of the the downloaded images are not true exemplars of the category. For this reason the default answer in the interface the default answer was set to NO (notice that all the smaller versions of the images in the left are marked with a bold red contour, which denotes that the image do not belong to the category). Thus, if the worker just presses the space bar to move, images will keep the default NO label. Whenever a true exemplar appears in the center, the worker can press a specific key to mark it as a positive exemplar (responding YES). As the response is set to YES the bold contour of the image turns to green. The interface also allows moving backwards to revise previous annotations. Each AMT HIT (Human Intelligence Task, one assignment for one worker), consisted of 750 images for manual annotation. A control set of 30 positive samples and 30 negative samples with ground-truth category labels from the SUN database were intermixed in the HIT as well. As a quality control measure, only worker HITs with an accuracy of 90% or higher on these control images were kept.   The positive images resulting from the first cleaning iteration were sent for a second iteration of cleaning. We used the same task interface but with the default answer was set to YES. In this second iteration, 25.4% of the images were relabeled as NO. We tested a third cleaning iteration on a few exemplars but did not pursue it further as the percentage of images relabeled as NO was not significant.\n\nAfter the two iterations of annotation, we collected one scene label for 7,076,580 images pertaining to 476 scene categories. As expected, the number of images per scene category vary greatly (i.e. there are many more images of bedroom than cave on the web). There were 413 scene categories that ended up with at least 1000 exemplars, and 98 scene categories with more than 20,000 exemplars.\n\n\n2.2.3\n\nStep 3: Scaling up the dataset using a classifier As a result of the previous round of image annotation, there were 53 million remaining downloaded images not assigned to any of the 476 scene categories (e.g. a bedroom picture could have been downloaded when querying images for living-room category, but marked as negative by the AMT worker). Therefore, a third annotation task was designed to re-classify then re-annotate those images, using a semiautomatic bootstrapping approach.\n\nA deep learning-based scene classifier, AlexNet [3], was trained to classify the remaining 53 million images: We first randomly selected 1,000 images per scene category as training set and 50 images as validation set (for the 413 categories which had more than 1000 samples). AlexNet achieved 32% scene classification accuracy on the validation set after training. The trained AlexNet was then used to classify the 53 million images. We used the predicted class score by the AlexNet to rank the images within one scene category as follow: for a given category with too few exemplars, the top ranked images with predicted class confidence higher than 0.8 were sent to AMT for a third round of manual annotation using the same interface shown in Fig.4. The default answer was set to NO.\n\nAfter completing this third round of AMT annotation, the distribution of the number of images per category flattened out: 401 scene categories had more than 5,000 images per category and 240 scene categories had more than 20,000 images. In total, about 3 million images were added into the dataset.\n\n\nStep 4: Improving the separation of similar classes\n\nDespite the initial effort to bundle synonyms from Word-Net, the scene list from the SUN database still contained a few categories with very close synonyms (e.g. 'ski lodge' and 'ski resort', or 'garbage dump' and 'landfill'). We manually identified 46 synonym pairs like these and merged their images into a single category.\n\nAdditionally, we observed that some scene categories could be easily confused with blurry categorical boundaries, as illustrated in Fig. 5. This means that, for images in these blurry boundaries, answering the question \"Does image I belong to class A?\" might be difficult. However, it can be easier to answer the question \"Does image I belong to class A or B?\". With this question, the decision boundary becomes clearer for a human observer and it also gets closer to the final task that a computer system will be trained to solve, which is actually separating classes even when the boundaries are blurry. After checking the annotations, we confirmed that in the previous three steps of the AMT annotation, workers were confused with some pairs (or groups) of scene categories. For instance, there was an overlap between 'canyon' and 'mountain' or 'butte' and 'mountain'. There were also images mixed in the following category pairs: 'jacuzzi' and 'swimming pool indoor'; 'pond' and 'lake'; 'volcano' and 'mountain'; 'runway' and 'highway and road'; 'operating room' and 'hospital room'; among others. In the whole set of categories, we identified 53 different ambiguous pairs.\n\n\nField Forest\n\nTo further differentiate the images from the categories with shared content, we designed a new interface for a fourth annotation step. The instructions for the task are shown Fig. 6.a, while Fig. 6.b shows the annotation interface. The interface combines exemplar images from the two categories with shared content (such as 'art school' and 'art studio'), and AMT workers were asked to classify images into one of the categories or neither of them.\n\nAfter this fourth annotation step, the Places database was finalized with over 10 millions labeled exemplars (10,624,928 images) from 434 place categories.\n\n\nPLACES BENCHMARKS\n\nHere we describe four subsets of Places database as benchmarks. Places205 and Places88 are from [1]. Two new benchmarks have been added: from the 434 categories, we selected 365 categories with more than 4000 images each to create Places365-Standard and Places365-Challenge. The details of each benchmark are the following:  [1]. We call the other two corresponding subsets ImageNet88 and SUN88 respectively. These subsets are used to compare performances across different scene-centric databases, as the three datasets contain different exemplars per category (i.e. none of these three datasets contain common images). Note that finding correspondences between the classes defined in ImageNet and Places brings some challenges. ImageNet follows the WordNet definitions, but some WordNet definitions are not always appropriate for describing places. For instance, the class 'elevator' in ImageNet refers to an object. In Places, 'elevator' takes different meanings depending on the location of the observer: elevator door, elevator interior, or elevator lobby. Many categories in ImageNet do not differentiate between indoor and outdoor (e.g., ice-skating rink) while in Places, indoor and outdoor versions are separated as they do not necessarily afford the same function.\n\u2022 Places365-Standard\nthe largest scene-centric image dataset so far. The next subsection presents a comparison of these three datasets in terms of image diversity.\n\n\nDataset Diversity\n\nGiven the types of images found on the internet, some categories will be more biased than others in terms of viewpoints, types of objects, or even image style [23]. However, bias can be compensated with a high diversity of images, with many appearances represented in the dataset. In this section we describe a measure of dataset diversity to compare how diverse images from three scene-centric datasets (Places88, SUN88 and ImageNet88) are.\n\nComparing datasets is an open problem. Even datasets covering the same visual classes have notable differences providing different generalization performances when used to train a classifier [23]. Beyond the number of images and categories, there are aspects that are important but difficult to quantify, like the variability in camera poses, in decoration styles or in the type of objects that appear in the scene.\n\nAlthough the quality of a database is often task dependent, it is reasonable to assume that a good database should be dense (with a high degree of data concentration), and diverse (it should include a high variability of appearances and viewpoints). Imagine, for instance, a dataset composed of 100,000 images all taken within the same bedroom. This dataset would have a very high density but a very low diversity as all the images will look very similar. An ideal dataset, expected to generalize well, should have high diversity as well. While one can achieve high density by collecting a large number of images, diversity is not an obvious quantity to estimate in image sets, as it assumes some notion of similarity between images. One way to estimate similarity is to ask the question are these two images similar? However, similarity in the wild is a subjective and loose concept, as two images can be viewed as similar if they contain similar objects, and/or have similar spatial configurations, and/or have similar decoration styles and so on. A way to circumvent this problem is to define relative measures of similarity for comparing datasets.\n\nSeveral measures of diversity have been proposed, particularly in biology for characterizing the richness of an ecosystem (see [24] for a review). Here, we propose to use a measure inspired by the Simpson index of diversity [25]. The Simpson index measures the probability that two random individuals from an ecosystem belong to the same species. It is a measure of how well distributed the individuals across different species are in an ecosystem, and it is related to the entropy of the distribution. Extending this measure for evaluating the diversity of images within a category is non-trivial if there are no annotations of subcategories. For this reason, we propose to measure the relative diversity of image datasets A and B based on the following idea: if set A is more diverse than set B, then two random images from set B are more likely to be visually similar than two random samples from A. Then, the diversity of A with respect to B can be defined as\nDiv B (A) = 1 \u2212 p(d(a 1 , a 2 ) < d(b 1 , b 2 ))(1)\nwhere a 1 , a 2 \u2208 A and b 1 , b 2 \u2208 B are randomly selected. With this definition of relative diversity we have that A is more diverse than B if, and only if, Div B (A) > Div A (B).\n\nFor an arbitrary number of datasets, A 1 , ..., A N , the diversity of A 1 with respect to A 2 , ..., A N can be defined as\nDiv A2,...,A N (A 1 ) = 1\u2212p(d(a 11 , a 12 ) < min i=2:N d(a i1 , a i2 ))(2)\nwhere a i1 , a i2 \u2208 A i are randomly selected, i = 2 : N . We measured the relative diversities between SUN88, ImageNet88 and Places88 using AMT. Workers were presented with different pairs of images and they had to select the pair that contained the most similar images. The pairs were randomly sampled from each database. Each trial was composed of 4 pairs from each database, giving a total of 12 pairs to choose from. We used 4 pairs per database to increase the chances of finding a similar pair and avoiding users having to skip trials. AMT workers had to select the most similar pair on each trial. We ran 40 trials per category and two observers per trial, for the 88 categories in common between ImageNet88, SUN88 and Places88 databases. Fig. 8.a-b shows some examples of pairs from the diversity experiments for the scene categories playground (a) and bedroom (b). In the figure only one pair from each database is shown. We observed that different annotators were consistent in deciding whether a pair of images was more similar than another pair of images. Fig. 8.c shows the histograms of relative diversity for all the 88 scene categories common to the three databases. If the three datasets were identical in terms of diversity, the average diversity should be 2/3 for the three datasets. Note that this measure of diversity is a relative measure between the three datasets. In the experiment, users selected pairs from the SUN database to be the closest to each other 50% of the time, while the pairs from the Places database were judged to be the most similar only on 17% of the trials. ImageNet88 pairs were selected 33% of the time.\n\nThe results show that there is a large variation in terms of diversity among the three datasets, showing Places to be the most diverse of the three datasets. The average relative diversity on each dataset is 0.83 for Places, 0.67 for ImageNet88 and 0.50 for SUN. The categories with the largest variation in diversity across the three datasets were playground, veranda and waiting room.\n\n\nCross Dataset Generalization\n\nAs discussed in [23], training and testing across different datasets generally results in a drop of performance due to the dataset bias problem. In this case, the bias between datasets is due, among other factors, to the differences in      the diversity between the three datasets. Fig. 9 shows the classification results obtained from the training and testing on different permutations of the 3 datasets. For these results we use the features extracted from a pre-trained ImageNet-CNN and a linear SVM. In all three cases training and testing on the same dataset provides the best performance for a fixed number of training examples. As the Places database is very large, it achieves the best performance on two of the test sets when all the training data is used.\n\n\nCONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION\n\nGiven the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the Im-ageNet benchmark [3], [12], we choose three popular CNN architectures, AlexNet [3], GoogLeNet [26], and VGG 16 convolutional-layer CNN [27], then train them on Places205 and Places365-Standard respectively to create baseline CNN models. The trained CNNs are named as PlacesSubset-CNN, i.e., Places205-AlexNet or Places365-VGG.\n\nAll the Places-CNNs presented here were trained using the Caffe package [28] on Nvidia GPUs Tesla K40 and Titan X 3 . Additionally, given the recent breakthrough performances of the Residual Network (ResNet) on ImageNet classification [29], we further fine-tuned ResNet152 on the Places365-Standard (termed as Places365-ResNet) and compared it with the other trained-from-scratch Places-CNNs for scene classification.\n\n\nResults on Places205 and Places365\n\nAfter training the various Places-CNNs, we used the final output layer of each network to classify the test set images of Places205 and SUN205 (see [1]). The classification results for Top-1 accuracy and Top-5 accuracy are listed in Table 1. The Top-1 accuracy is the percentage of the testing images where the top predicted label exactly match the ground-truth label. The Top-5 accuracy is that the percentage of testing images where the ground-truth label is among the top ranked 5 predicted labels given by an algorithm. Since there are some ambiguity between some scene categories, the Top-5 accuracy is a more suitable criteria of measuring scene classification performance.\n\nAs a baseline comparison, we show the results of a linear SVM trained on ImageNet-CNN features of 5000 images per category in Places205 and 50 images per category in SUN205 respectively. We observe that Places-CNNs perform much better than the ImageNet feature+SVM baseline while, as expected, Places205-GoogLeNet and Places205-VGG outperformed Places205-AlexNet with a large margin, due to their deeper structures. To date (Oct 2, 2016) the top ranked results on the test set of Places205 leaderboard 4 is 64.10% on Top-1 accuracy and 90.65% on Top-5 accuracy. Note that for the test set of SUN205, we did not fine-tune the Places-CNNs on the training set of SUN205, as we directly evaluated them on the test set of SUN.\n\nWe further evaluated the baseline Places365-CNNs on the validation set and test set of Places365. The results are shown in Table 2. We can see that Places365-VGG and Places365-ResNet have similar top performances compared with the other two CNNs 5 . Even though Places365 has 160 more categories than Places205, the Top-5 accuracy of the Places205-CNNs (trained on the previous version of Places [1]) on the test set only drops by 2.5%.\n\nTo evaluate how extra categories bring improvements, we compute the accuracy for the 182 common categories between Places205 and Places365 (we merge some categories in Places205 when building Places365 thus there are less common categories) for Places205-CNN and Places365-CNN. On the validation set of Places365, we select the images of the 182 common categories, then use the aligned 182 outputs of the Places205-AlexNet and Places365-AlexNet to predict the labels respectively. The Top1 accuracy for Places205-AlexNet is 0.572, the one for 3 Places365-AlexNet is 0.577. Thus Places365-AlexNet not only predicts more categories, but also has better accuracy on the previous existing categories. Fig.10 shows the responses to examples correctly predicted by the Places365-VGG. Notice that most of the Top-5 responses are very relevant to the scene description. Some failure or ambiguous cases are shown in Fig.11. Broadly, we can identify two kinds of mis-classification given the current label attribution of Places: 1) less-typical activities happening in a scene, such as taking group photo in a construction site and camping in a junkyard; 2) images composed of multiple scene parts, which make one ground-truth scene label not sufficient to describe the whole environment. These results suggest the need to have multi-ground truth labels for describing environments.\n\nIt is important to emphasize that for many scene categories the Top-1 accuracy might be an ill-defined measure: environments are inherently multi-labels in terms of their semantic description. Different observers will use different terms to refer to the same place, or different parts of the same environment, and all the labels might fit well the description of the scene, as we observe in the examples of Fig.11.\n\n\nWeb-demo for Scene Recognition\n\nBased on our trained Places-CNN, we created a webdemo for scene recognition 6 , accessible through a computer browser or mobile phone. People can upload photos to the web-demo to predict the type of environment, with the 5 most likely semantic categories, and relevant scene attributes. Two screenshots of the prediction result on the mobile phone are shown in Fig.12. Note that people can submit feedback about the result. The top-5 recognition accuracy of our recognition web-demo in the wild is about 72% (from the 9,925 anonymous feedbacks dated from Oct.19, 2014 to May 5, 2016), which is impressive given that people uploaded all kinds of photos from real-life and not necessarily places-like photos. Places205-AlexNet is the back-end prediction model in the demo.\n\n\nPlaces365 Challenge Result\n\nThe subset Places365-Challenge, which contains more than 8 million images from 365 scene categories, was used in the Places Challenge 2016 held as part of the ILSVRC Challenge in the European Conference on Computer Vision (ECCV) 2016.\n\nThe rule of the challenge is that each team can only use the provided data in the Places365-Challenge to train their networks. Standard CNN models trained on Imagenet-1.2million and previous Places are allowed to use. Each team can submit at most 5 prediction results. Ranks are based on the top-5 classification error of each submission. Winners teams are then invited to give talks at the ILSVRC-COCO Joint Workshop at ECCV'16. 6. http://places.csail.mit.edu/demo. html  TABLE 1 Classification accuracy on the test set of Places205 and the test set of SUN205. We use the class score averaged over 10-crops of each test image to classify the image. * shows the top 2 ranked results from the Places205 leaderboard.   There were totally 92 valid submissions from 27 teams. Finally team Hikvision [30] won the 1st place with 9.01% top-5 error, team MW [31] won the 2nd place with 10.19% top-5 error, and team Trimps-Soushen [32] won the 3rd place with 10.30% top-5 error. The leaderboard and the team information are available at the challenge result page 7 . The ranked results of all the submissions are plotted in Fig.13. The entry from the winner team outperforms our best baseline with a large margin (\u223c 6% in top-5 accuracy). Note that our baselines are trained with the Places365-Standard while those challenge entries are trained on the Places365-Challenge which has 5.5 million more training 7. http://places2.csail.mit.edu/results2016.html images.\n\n\nGeneric Visual Features from Places-CNNs and ImageNet-CNNs\n\nWe further used the activation from the trained Places-CNNs as generic features for visual recognition tasks using different image classification benchmarks. Activations from the higher-level layers of a CNN, also termed deep features, have proven to be effective generic features with state-ofthe-art performance on various image datasets [33], [34]. But most of the deep features are from the CNNs trained on ImageNet, which is mostly an object-centric dataset.\n\nHere we evaluated the classification performances of the deep features from scene-centric CNNs and object-  Fig. 11. Examples of predictions rated as incorrect in the validation set by the Places365-VGG. GT states for ground truth label. Note that some of the top-5 responses are often not wrong per se, predicting semantic categories near by the GT category. See the text for details. Fig. 12. Two screenshots of the scene recognition demo based on the Places-CNN. The web-demo predicts the type of environment, the semantic categories, and associated scene attributes for uploaded photos.   centric CNNs in a systematic way. The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [16], MIT Indoor67 [15], Scene15 [13], SUN Attribute [35], Caltech101 [36], Caltech256 [37], Stanford Action40 [38], and UIUC Event8 [39]. All of the presented experiments follow the standards in the mentioned papers. In the SUN397 experiment [16], the training set size is 50 images per category. Experiments were run on 5 splits of the training set and test set given in the dataset. In the MIT Indoor67 experiment [15], the training set size is 100 images per category. The experiment is run on the split of the training set and test set given in the dataset. In the Scene15 experiment [13], the training set size is 50 images per category. Experiments are run on 10 random splits of the training set and test set. In the SUN Attribute experiment [35], the training set size is 150 images per attribute. The reported result is the average precision. The splits of the training set and test set are given in the paper. In Caltech101 and Caltech256 experiment [36], [37], the training set size is 30 images per category. The experiments are run on 10 random splits of the training set and test set. In the Stanford Action40 experiment [38], the training set size is 100 images per category. Experiments are run on 10 random splits of the training set and test set. The reported result is the classification accuracy. In the UIUC Event8 experiment [39], the training set size is 70 images per category and the test set size is 60 images per category. The experiments are run on 10 random splits of the training set and test set.\n\nPlaces-CNNs and ImageNet-CNNs have the same network architectures for AlexNet, GoogLeNet, and VGG, but they are trained on scene-centric data (Places) and object-centric data (ImageNet) respectively. For AlexNet and VGG, we used the 4096-dimensional feature vector from the activation of the Fully Connected Layer (fc7) of the CNN. For GoogLeNet, we used the 1024-dimensional feature vector from the response of the global average pool- Combined kernel [37.5] HoG2x2 [26.3] DenseSIFT [23.5] Ssim [22.7] Geo texton [ [16].\n\ning layer before softmax producing the class predictions. The classifier in all of the experiments is a linear SVM with the default parameter for all of the features. Table 3 summarizes the classification accuracy on various datasets for the deep features of Places-CNNs and the deep features of the ImageNet-CNNs. Fig.14 plots the classification accuracy for different visual features on the SUN397 database over different numbers of training samples per category. The classifier is a linear SVM with the same default parameters for the two deep feature layers (C=1) [40]. The Places-CNN features show impressive performance on scene-related datasets, outperforming the ImageNet-CNN features. On the other hand, the ImageNet-CNN features show better performance on object-related image datasets. Importantly, our comparison shows that Places-CNN and ImageNet-CNN have complementary strengths on scenecentric tasks and object-centric tasks, as expected from the type of the datasets used to train these networks. On the other hand, the deep features from the Places365-VGG achieve the best performance (63.24%) on the most challenging scene classification dataset SUN397, while the deep features of Places205-VGG performs the best on the MIT Indoor67 dataset. As far as we know, they are the state-of-the-art scores achieved by a single feature + linear SVM on those two datasets. Furthermore, we merge the 1000 classes from the ImageNet and the 365 classes from the Places365-Standard to train a VGG (Hybrid1365-VGG). The deep feature from the Hybrid1365-VGG achieves the best score averaged over all the eight image datasets.\n\n\nVisualization of the Internal Units\n\nThrough the visualization of the unit responses for various levels of network layers, we can have a better understanding of what has been learned inside CNNs and what are the differences between the object-centric CNN trained on Im-ageNet and the scene-centric CNN trained on Places given that they share the same architecture AlexNet. Following the methodology in [2] we feed 100,000 held-out testing images into the two networks and record the max activation of each unit pooled over the whole spatial feature map for each of the images respectively. For each unit, we get the top three ranked images by ranking their max activations, then we segment the images by bilinear upsampling the binarized spatial feature map mask.\n\nThe image segmentation results of the units from different layers are shown in Fig.15. We can see that from conv1 to conv5, the units detect visual concepts from low-level edge/texture to high-level object/scene parts. Furthermore, in the object-centric ImageNet-CNN there are more units detecting object parts such as dog and people's heads in the conv5 layer, while in the scene centric Places-CNN there are more units detecting scene parts such as bed, chair, or buildings in the conv5 layer.\n\nThus the specialty of the units in the object-centric CNN and scene-centric CNN yield very different performances of generic visual features on a variety of recognition benchmarks (object-centric datasets vs scene-centric datasets) in Table 3.\n\nWe further synthesized preferred input images for the Places-CNN by using the image synthesis technique proposed in [41]. This method uses a learned prior deep generator network to generate images which maximize the final class activation or the intermediate unit activation of the Places-CNN. The synthetic images for 50 scene categories are shown in Fig.16. These abstract image contents reveal the knowledge of the specific scene learned and memorized by the Places-CNN: examples include the buses within a road environment in the bus station, and the tents surrounded by forest-types of features for the campsite. Here we used Places365-AlexNet (other Places365-CNNs generated similar results). We further used the synthesis technique to generate the images preferred by the units in the conv5 layer of Places365-AlexNet. As shown in Fig.17, the synthesized images are very similar to the segmented image regions by the estimated receptive field of the units.\n\n\nCONCLUSION\n\nFrom the Tiny Image dataset [42], to ImageNet [11] and Places [1], the rise of multi-million-item dataset initiatives and other densely labeled datasets [18], [20], [43], [44] have enabled data-hungry machine learning algorithms to reach near-human semantic classification of visual patterns, like objects and scenes. With its category coverage and highdiversity of exemplars, Places offers an ecosystem of visual context to guide progress on scene understanding problems. Such problems could include determining the actions happening in a given environment, spotting inconsistent objects or human behaviors for a particular place, and predicting future events or the cause of events given a scene.   \n\nFig. 1 .\n1Image samples from various categories of the Places Database (two samples per category). The dataset contains three macro-classes: Indoor, Nature, and Urban.\n\nFig. 2 .\n2Sorted distribution of image number per category in the Places Database. Places contains 10,624,928 images from 434 categories. Category names are shown for every 6 intervals.\n\nFig. 3 .Fig. 4 .\n34Image samples from four scene categories grouped by queries to illustrate the diversity of the dataset. For each query we show 9 annotated images. Annotation interface in the Amazon Mechanical Turk for selecting the correct exemplars of the scene from the downloaded images. a) instruction given to the workers in which we define positive and negative examples. b) binary selection interface.\n\nFig. 5 .Fig. 6 .\n56Boundaries between place categories can be blurry, as some images can be made of a mixture of different components. The images shown in this figure show a soft transition between a field and a forest. Although the extreme images can be easily classified as field and forest scenes, the middle images can be ambiguous. Annotation interface in Amazon Mechanical Turk for differentiating images from two similar categories. a) instruction in which we give several typical examples in each category. b) the binary selection interface, in which the worker has to classify the shown image into one of the classes or none of them.\n\nFig. 7 .Fig. 8 .\n78Comparison of the number of images per scene category for the common 88 scene categories in Places, ImageNet, and SUN datasets. Examples of pairs for the diversity experiment for a) playground and b) bedroom. Which pair shows the most similar images? The bottom pairs were chosen in these examples. c) Histogram of relative diversity per each category (88 categories) and dataset. Places88 (in blue line) contains the most diverse set of images, then ImageNet88 (in red line) and the lowest diversity is in the SUN88 database (in yellow line) as most images are prototypical of their class.\n\nFig. 9 .\n9Cross dataset generalization of training on the 88 common scenes between Places, SUN and ImageNet then testing on the 88 common scenes from: a) SUN, b) ImageNet and c) Places database.\n\nFig. 10 .\n10The predictions given by the Places365-VGG for the images from the validation set. The groundtruth label (GT) and the top 5 predictions are shown. The number beside each label indicates the prediction confidence.\n\nFig. 13 .\n13The ranked results of all the 92 valid submissions. The best baseline trained on Places365standard is the Resnet152 which has the top5-error as 14.9%, while the winner network from HikVision gets 9.01% top5-error which outperform the baseline with large margin.\n\nFig. 15 .\n15a) Visualization of the units' receptive fields at different layers for the ImageNet-CNN and Places-CNN. Subsets of units at each layer are shown. In each row we show the top 3 most activated images. Images are segmented based on the binarized spatial feature map of the units at different layers of ImageNet-CNN and Places-CNN. Here we take ImageNet-AlexNet and Places205-AlexNet as the comparison examples. See the detailed visualization methodology in[2].\n\nFig. 16 .\n16The synthesized images preferred by the final output of Places365-AlexNet for 20 scene categories.Fig. 17. The synthesized images preferred by the conv5 units of the Places365-AlexNet corresponds to the segmented images by the receptive fields of those units. The synthetic images are very similar to the segmented image regions of the units. Each row of the segmented images correspond to one unit.Bolei Zhou is a Ph.D. Candidate in Computer Science and Artificial Intelligence Lab (CSAIL) at the Massachusetts Institute of Technology. He received M.Phil. degree in Information Engineering from the Chinese University of Hong Kong and B.Eng. degree in Biomedical Engineering from Shanghai Jiao Tong University in 2010. His research interests are computer vision and machine learning. He is an award recipient of the Facebook Fellowship, the Microsoft Research Asia Fellowship, and the MIT Greater China Fellowship. Agata Lapedriza is an Associate Professor at the Universitat Oberta de Catalunya. She received her MS deegree in Mathematics at the Universitat de Barcelona in 2003, and her Ph.D. degree in Computer Science at the Computer Vision Center in 2009, at the Universitat Autonoma Barcelona. She was working as a visiting researcher in the Computer Science and Artificial Intelligence Lab, at the Massachusetts Institute of Technology, from 2012 until 2015. Her research interests are related to high-level image understanding, scene and object recognition, and affective computing. Aditya Khosla received the BS degree in computer science, electrical engineering and economics from the California Institute of Technology, and the MS degree in computer science from Stanford University, in 2009 and 2011 respectively. He completed his PhD in computer science from the Massachusetts Institute of Technology in 2016 with a focus on computer vision and machine learning. In his thesis, he developed machine learning techniques that predict human behavior and the impact of visual media on people.Aude Oliva is a Principal Research Scientist at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). After a French baccalaureate in Physics and Mathematics, she received two M.Sc. degrees and a Ph.D in Cognitive Science from the Institut National Polytechnique of Grenoble, France. She joined the MIT faculty in the Department of Brain and Cognitive Sciences in 2004 and CSAIL in 2012. Her research on vision and memory is cross-disciplinary, spanning human perception and cognition, computer vision, and human neuroscience. She received the 2006 National Science Foundation (NSF) Career award, the 2014 Guggenheim and the 2016 Vannevar Bush fellowships. Antonio Torralba received the degree in telecommunications engineering from Telecom BCN, Spain, in 1994 and the Ph.D. degree in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, France, in 2000. From 2000 to 2005, he spent postdoctoral training at the Brain and Cognitive Science Department and the Computer Science and Artificial Intelligence Laboratory, MIT. He is now a Professor of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology (MIT). Prof. Torralba is an Associate Editor of the International Journal in Computer Vision, and has served as program chair for the Computer Vision and Pattern Recognition conference in 2015. He received the 2008 National Science Foundation (NSF) Career award, the best student paper award at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) in 2009, and the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition (IAPR).\n\n\u2022 B .\nBZhou, A. Khosla, A. Oliva, A. Torralba are with the Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, USA. \u2022 A. Lapedriza is with Universitat Oberta de Catalunya, Spain.\n\n\nhas 1,803,460 training images with the image number per class varying from 3,068 to 5,000. The validation set has 50 images per class and the test set has 900 images per class. Note that the experiments in this paper are reported on Places365-Standard. \u2022 Places365-Challenge contains the same categories as Places365-Standard, but the training set is significantly larger with a total of 8 million training images. The validation set and testing set are the same as the Places365-Standard. This subset was released for the Places Challenge 2016 2 held in conjunction with the European Conference on Computer Vision (ECCV) 2016, as part of the ILSVRC Challenge. \u2022 Places205. Places205, described in [1], has 2.5 million images from 205 scene categories. The image number per class varies from 5,000 to 15,000. The training set has 2,448,873 total images, with 100 images per category for the validation set and 200 images per category for the test set. \u2022 Places88. Places88 contains the 88 common scene categories among the ImageNet [12], SUN [16] and Places205 databases. Places88 contains only the images obtained in round 2 of annotations, from the first version of Places used in\n\n\nTrain on SUN 88 [63.3] Train on ImageNet 88 [62.8] Train on Places 88 [69.5]Number of training samples per category \nClassi cation accuracy \n\nTest on ImageNet Scene 88 \n\nTrain on ImageNet 88 [65.6] \nTrain on Places 88 [60.3] \nTrain on SUN 88 [49.2] \n\n10 0 \n10 1 \n10 2 \n10 3 \n10 4 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\nNumber of training samples per category \nClassi cation accuracy \n\nTest on SUN 88 \n\n\n\n\n. All the Places-CNNs are available at https://github.com/ CSAILvision/places365 4. http://places.csail.mit.edu/user/leaderboard.php 5. The performance of the ResNet might result from fine-tuning or under-training, as the ResNet is not trained from scratch.\n\nTABLE 2\n2Classification accuracy on the validation set and test set of Places365. We use the class score averaged over 10-crops of each testing image to classify the image.Validation Set of Places365 \nTest Set of Places365 \nTop-1 acc. \nTop-5 acc. \nTop-1 acc. \nTop-5 acc. \nPlaces365-AlexNet \n53.17% \n82.89% \n53.31% \n82.75% \nPlaces365-GoogLeNet \n53.63% \n83.88% \n53.59% \n84.01% \nPlaces365-VGG \n55.24% \n84.91% \n55.19% \n85.01% \nPlaces365-ResNet \n54.74% \n85.08% \n54.65% \n85.07% \n\n\n\nThe deep features of Places365-VGG outperforms other deep features and hand-designed features by a large margin. Results of other hand-designed features/kernels are fetched from22.1] \n\nTexton [21.6] \n\nGist [16.3] \n\nLBP [14.7] \n\nImageNet\u2212AlexNet [42.6] \n\nPlaces205\u2212AlexNet [54.3] \n\nPlaces365\u2212VGG [63.24] \n\nFig. 14. Classification accuracy on the SUN397 \nDataset. We compare the deep features of Places365-\nVGG, Places205-AlexNet (result reported in [1]), and \nImageNet-AlexNet, to hand-designed features (HOG, \ngist, etc). \n\nTABLE 3\n3Classification accuracy/precision on scene-centric databases (the first four datasets) and object-centric databases (the last four datasets) for the deep features of various Places-CNNs and ImageNet-CNNs. All the accuracy/precision is the top-1 accuracy/precision.Deep Feature \nSUN397 MIT Indoor67 Scene15 SUN Attribute \nCaltech101 Caltech256 Action40 Event8 \nAverage \nPlaces365-AlexNet \n56.12 \n70.72 \n89.25 \n92.98 \n66.40 \n46.45 \n46.82 \n90.63 \n69.92 \nPlaces205-AlexNet \n54.32 \n68.24 \n89.87 \n92.71 \n65.34 \n45.30 \n43.26 \n94.17 \n69.15 \nImageNet-AlexNet \n42.61 \n56.79 \n84.05 \n91.27 \n87.73 \n66.95 \n55.00 \n93.71 \n72.26 \nPlaces365-GoogLeNet \n58.37 \n73.30 \n91.25 \n92.64 \n61.85 \n44.52 \n47.52 \n91.00 \n70.06 \nPlaces205-GoogLeNet \n57.00 \n75.14 \n90.92 \n92.09 \n54.41 \n39.27 \n45.17 \n92.75 \n68.34 \nImageNet-GoogLeNet \n43.88 \n59.48 \n84.95 \n90.70 \n89.96 \n75.20 \n65.39 \n96.13 \n75.71 \nPlaces365-VGG \n63.24 \n76.53 \n91.97 \n92.99 \n67.63 \n49.20 \n52.90 \n90.96 \n73.18 \nPlaces205-VGG \n61.99 \n79.76 \n91.61 \n92.07 \n67.58 \n49.28 \n53.33 \n93.33 \n73.62 \nImageNet-VGG \n48.29 \n64.87 \n86.28 \n91.78 \n88.42 \n74.96 \n66.63 \n95.17 \n77.05 \nHybrid1365-VGG \n61.77 \n79.49 \n92.15 \n92.93 \n88.22 \n76.04 \n68.11 \n93.13 \n81.48 \n\nImageNet-CNN \n\nPlaces-CNN \n\nconv1 \nconv2 \nconv3 \nconv4 \nconv5 \n\n\nCOMPARING SCENE-CENTRIC DATASETSScene-centric datasets correspond to images labeled with a scene, or place name, as opposed to object-centric datasets, where images are labeled with object names. In this section we use the Places88 benchmark to compare Places dataset with the tow other biggest scene datasets: ImageNet88 and SUN88.Fig. 7illustrates the differences among the number of images found in the different categories for Places88, ImageNet88 and SUN88. Notice that Places Database is 2. http://places2.csail.mit.edu/challenge.html\nACKNOWLEDGMENTSThe authors would like to thank Santani Teng, Zoya Bylinskii, Mathew Monfort and Caitlin Mullin for comments on the paper. Over the years, the Places project was supported by the NationalScience Foundation\nGT: classroom top-1: locker room (0.585) top-2: lecture room (0.135) top-3: conference center (0.061) top-4: classroom (0.033). top-3: drugstore (0.120) top-4: department store (0.087) top-5: pharmacy (0.052GT: greenhouse indoor top-1: greenhouse. GT: natural canal top-1: swamp (0.529). indoor (0.479) top-2: greenhouse outdoor (0.055) top-3: botanical garden (0.044) top-4: assembly line (0.025) top-5: vegetable garden (0.022) GT: market outdoor top-1: promenade (0.569) top-2: bazaar outdoor (0.137) top-3: boardwalk (0.118) top-4: market outdoor (0.074) top-5: flea market indoor (0.029GT: natural canal top-1: swamp (0.529) top-2: marsh (0.232) top-3: natural canal (0.063) top-4: lagoon (0.047) top-5: rainforest (0.029) GT: chalet top-1: ski resort (0.141) top-2: ice floe (0.129) top-3: igloo (0.114) top-4: balcony exterior (0.103) top-5: courtyard (0.083) GT: classroom top-1: locker room (0.585) top-2: lecture room (0.135) top-3: conference center (0.061) top-4: classroom (0.033) top-5: elevator door (0.025) GT: creek top-1: forest broadleaf (0.307) top-2: forest path (0.208) top-3: creek (0.086) top-4: rainforest (0.076) top-5: cemetery (0.049) GT: crosswalk top-1: crosswalk (0.720) top-2: plaza (0.060) top-3: street (0.055) top-4: shopping mall indoor (0.039) top-5: bazaar outdoor (0.021) GT: drugstore top-1: supermarket (0.286) top-2: hardware store (0.248) top-3: drugstore (0.120) top-4: department store (0.087) top-5: pharmacy (0.052) GT: greenhouse indoor top-1: greenhouse indoor (0.479) top-2: greenhouse outdoor (0.055) top-3: botanical garden (0.044) top-4: assembly line (0.025) top-5: vegetable garden (0.022) GT: market outdoor top-1: promenade (0.569) top-2: bazaar outdoor (0.137) top-3: boardwalk (0.118) top-4: market outdoor (0.074) top-5: flea market indoor (0.029)\n\nLearning deep features for scene recognition using places database. B Zhou, A Lapedriza, J Xiao, A Torralba, A Oliva, Advances in Neural Information Processing Systems. B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, \"Learning deep features for scene recognition using places database,\" in In Advances in Neural Information Processing Systems, 2014.\n\nObject detectors emerge in deep scene cnns. B Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba, International Conference on Learning Representations. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, \"Object detectors emerge in deep scene cnns,\" International Conference on Learning Representations, 2015.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classifi- cation with deep convolutional neural networks.\" in In Advances in Neural Information Processing Systems, 2012.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, 1997.\n\nFoundations of statistical natural language processing. C D Manning, H Sch\u00fctze, MIT PressC. D. Manning and H. Sch\u00fctze, Foundations of statistical natural language processing. MIT Press, 1999.\n\nDeep blue. M Campbell, A J Hoane, F.-H Hsu, Artificial intelligence. M. Campbell, A. J. Hoane, and F.-h. Hsu, \"Deep blue,\" Artificial intelligence, 2002.\n\nWatson: beyond jeopardy!. D Ferrucci, A Levas, S Bagchi, D Gondek, E T Mueller, Artificial Intelligence. D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. T. Mueller, \"Watson: beyond jeopardy!\" Artificial Intelligence, 2013.\n\nMastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., \"Mastering the game of go with deep neural networks and tree search,\" Nature, 2016.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. the IEEEY. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proceedings of the IEEE, 1998.\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, Proc. CVPR. CVPRK. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,\" in Proc. CVPR, 2015.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Proc. CVPR. CVPRJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in Proc. CVPR, 2009.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, Int'l Journal of Computer Vision. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \"Imagenet large scale visual recognition challenge,\" Int'l Journal of Computer Vision, 2015.\n\nBeyond bags of features: Spatial pyramid matching for recognizing natural scene categories. S Lazebnik, C Schmid, J Ponce, Proc. CVPR. CVPRS. Lazebnik, C. Schmid, and J. Ponce, \"Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories,\" in Proc. CVPR, 2006.\n\nModeling the shape of the scene: A holistic representation of the spatial envelope. A Oliva, A Torralba, Int'l Journal of Computer Vision. A. Oliva and A. Torralba, \"Modeling the shape of the scene: A holistic representation of the spatial envelope,\" Int'l Journal of Computer Vision, 2001.\n\nRecognizing indoor scenes. A Quattoni, A Torralba, Proc. CVPR. CVPRA. Quattoni and A. Torralba, \"Recognizing indoor scenes,\" in Proc. CVPR, 2009.\n\nSun database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, Proc. CVPR. CVPRJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, \"Sun database: Large-scale scene recognition from abbey to zoo,\" in Proc. CVPR, 2010.\n\nThe pascal visual object classes challenge 2007 (voc2007) results. M Everingham, A Zisserman, C K Williams, L Van Gool, M Allan, C M Bishop, O Chapelle, N Dalal, T Deselaers, G Dork\u00f3, M. Everingham, A. Zisserman, C. K. Williams, L. Van Gool, M. Al- lan, C. M. Bishop, O. Chapelle, N. Dalal, T. Deselaers, G. Dork\u00f3 et al., \"The pascal visual object classes challenge 2007 (voc2007) results,\" 2007.\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European Conference on Computer Vision. SpringerT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \"Microsoft coco: Common objects in context,\" in European Conference on Computer Vision. Springer, 2014, pp. 740-755.\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, ijcvR. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al., \"Visual genome: Connecting language and vision using crowdsourced dense image annotations,\" ijcv, 2016.\n\nScene paring through ade20k dataset. B Zhou, H Zhao, X Puig, S Fidler, A Barriuso, A Torralba, Proc. CVPR. CVPRB. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, \"Scene paring through ade20k dataset,\" Proc. CVPR, 2017.\n\nWordnet: a lexical database for english. G A Miller, Communications of the ACM. 3811G. A. Miller, \"Wordnet: a lexical database for english,\" Communi- cations of the ACM, vol. 38, no. 11, pp. 39-41, 1995.\n\nPictures and names: Making the connection. P Jolicoeur, M A Gluck, S M Kosslyn, Cognitive psychology. P. Jolicoeur, M. A. Gluck, and S. M. Kosslyn, \"Pictures and names: Making the connection,\" Cognitive psychology, 1984.\n\nUnbiased look at dataset bias. A Torralba, A A Efros, Proc. CVPR. CVPRA. Torralba and A. A. Efros, \"Unbiased look at dataset bias,\" in Proc. CVPR, 2011.\n\nIndices of diversity and evenness. C Heip, P Herman, K Soetaert, Oceanis. C. Heip, P. Herman, and K. Soetaert, \"Indices of diversity and evenness,\" Oceanis, 1998.\n\nMeasurement of diversity. E H Simpson, Nature. E. H. Simpson, \"Measurement of diversity.\" Nature, 1949.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proc. CVPR. CVPRC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" Proc. CVPR, 2015.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" International Conference on Learning Representations, 2014.\n\nCaffe: An open source convolutional architecture for fast feature embedding. Y Jia, Y. Jia, \"Caffe: An open source convolutional architecture for fast feature embedding,\" http://caffe.berkeleyvision.org/, 2013.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. CVPR. CVPRK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" Proc. CVPR, 2016.\n\nTowards good practices for recognition and detection. Q Zhong, C Li, Y Zhang, H Sun, S Yang, D Xie, S Pu, Q. Zhong, C. Li, Y. Zhang, H. Sun, S. Yang, D. Xie, and S. Pu, \"To- wards good practices for recognition and detection,\" http://image-net. org/challenges/talks/2016/Hikvision at ImageNet 2016.pdf, 2016.\n\nPlaces401 and places365 models. L Shen, Z Lin, G Sun, J Hu, L. Shen, Z. Lin, G. Sun, and J. Hu, \"Places401 and places365 models,\" https://github.com/lishen-shirley/Places2-CNNs, 2016.\n\nGood pratices for deep feature fusion. J Shao, X Zhang, Z Ding, Y Zhao, Y Chen, J Zhou, W Wang, L Mei, C Hu, J. Shao, X. Zhang, Z. Ding, Y. Zhao, Y. Chen, J. Zhou, W. Wang, L. Mei, and C. Hu, \"Good pratices for deep feature fu- sion,\" http://image-net.org/challenges/talks/2016/Trimps-Soushen@ ILSVRC2016.pdf, 2016.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, Proc. ICML. ICMLJ. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, \"Decaf: A deep convolutional activation feature for generic visual recognition,\" in Proc. ICML, 2014.\n\nCnn features off-the-shelf: an astounding baseline for recognition. A S Razavian, H Azizpour, J Sullivan, S Carlsson, CVPR workshop. A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, \"Cnn features off-the-shelf: an astounding baseline for recognition,\" CVPR workshop, 2014.\n\nSun attribute database: Discovering, annotating, and recognizing scene attributes. G Patterson, J Hays, Proc. CVPR. CVPRG. Patterson and J. Hays, \"Sun attribute database: Discovering, annotating, and recognizing scene attributes,\" in Proc. CVPR, 2012.\n\nLearning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. L Fei-Fei, R Fergus, P Perona, Computer Vision and Image Understanding. L. Fei-Fei, R. Fergus, and P. Perona, \"Learning generative visual models from few training examples: An incremental bayesian ap- proach tested on 101 object categories,\" Computer Vision and Image Understanding, 2007.\n\nCaltech-256 object category dataset. G Griffin, A Holub, P Perona, G. Griffin, A. Holub, and P. Perona, \"Caltech-256 object category dataset,\" 2007.\n\nHuman action recognition by learning bases of action attributes and parts. B Yao, X Jiang, A Khosla, A L Lin, L Guibas, L Fei-Fei, Proc. ICCV. ICCVB. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and L. Fei-Fei, \"Human action recognition by learning bases of action attributes and parts,\" in Proc. ICCV, 2011.\n\nWhat, where and who? classifying events by scene and object recognition. L.-J Li, L Fei-Fei, Proc. ICCV. ICCVL.-J. Li and L. Fei-Fei, \"What, where and who? classifying events by scene and object recognition,\" in Proc. ICCV, 2007.\n\nLIBLINEAR: A library for large linear classification. R.-E Fan, K.-W Chang, C.-J Hsieh, X.-R Wang, C.-J Lin, R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, \"LIBLINEAR: A library for large linear classification,\" 2008.\n\nSynthesizing the preferred inputs for neurons in neural networks via deep generator networks. A Nguyen, A Dosovitskiy, T Yosinski, Jason Brox, J Clune, Advances in Neural Information Processing Systems. A. Nguyen, A. Dosovitskiy, T. Yosinski, Jason band Brox, and J. Clune, \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks,\" In Advances in Neural Information Processing Systems, 2016.\n\n80 million tiny images: A large data set for nonparametric object and scene recognition. A Torralba, R Fergus, W T Freeman, IEEE Trans. on Pattern Analysis and Machine Intelligence. A. Torralba, R. Fergus, and W. T. Freeman, \"80 million tiny images: A large data set for nonparametric object and scene recognition,\" IEEE Trans. on Pattern Analysis and Machine Intelligence, 2008.\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, Int'l Journal of Computer Vision. M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser- man, \"The pascal visual object classes (voc) challenge,\" Int'l Journal of Computer Vision, 2010.\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proc. CVPR. CVPRM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be- nenson, U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban scene understanding,\" Proc. CVPR, 2016.\n", "annotations": {"author": "[{\"end\":162,\"start\":151},{\"end\":179,\"start\":163},{\"end\":194,\"start\":180},{\"end\":206,\"start\":195},{\"end\":224,\"start\":207},{\"end\":162,\"start\":151},{\"end\":179,\"start\":163},{\"end\":194,\"start\":180},{\"end\":206,\"start\":195},{\"end\":224,\"start\":207}]", "publisher": null, "author_last_name": "[{\"end\":161,\"start\":157},{\"end\":178,\"start\":169},{\"end\":193,\"start\":187},{\"end\":205,\"start\":200},{\"end\":223,\"start\":215},{\"end\":161,\"start\":157},{\"end\":178,\"start\":169},{\"end\":193,\"start\":187},{\"end\":205,\"start\":200},{\"end\":223,\"start\":215}]", "author_first_name": "[{\"end\":156,\"start\":151},{\"end\":168,\"start\":163},{\"end\":186,\"start\":180},{\"end\":199,\"start\":195},{\"end\":214,\"start\":207},{\"end\":156,\"start\":151},{\"end\":168,\"start\":163},{\"end\":186,\"start\":180},{\"end\":199,\"start\":195},{\"end\":214,\"start\":207}]", "author_affiliation": null, "title": "[{\"end\":121,\"start\":1},{\"end\":345,\"start\":225},{\"end\":121,\"start\":1},{\"end\":345,\"start\":225}]", "venue": null, "abstract": "[{\"end\":1606,\"start\":675},{\"end\":1606,\"start\":675}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3797,\"start\":3794},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4357,\"start\":4354},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4813,\"start\":4810},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4818,\"start\":4815},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4884,\"start\":4881},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4889,\"start\":4886},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5576,\"start\":5573},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5611,\"start\":5608},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5616,\"start\":5613},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5717,\"start\":5713},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5750,\"start\":5747},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6125,\"start\":6121},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6176,\"start\":6172},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6373,\"start\":6369},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6487,\"start\":6483},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6687,\"start\":6683},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6772,\"start\":6768},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7004,\"start\":7000},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7182,\"start\":7178},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7280,\"start\":7276},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7875,\"start\":7871},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8122,\"start\":8118},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8387,\"start\":8383},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8685,\"start\":8681},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9372,\"start\":9368},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9411,\"start\":9407},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9761,\"start\":9757},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10477,\"start\":10473},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14458,\"start\":14455},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17794,\"start\":17791},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18023,\"start\":18020},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19317,\"start\":19313},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19792,\"start\":19788},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21298,\"start\":21294},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21395,\"start\":21391},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24658,\"start\":24654},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25589,\"start\":25586},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25595,\"start\":25591},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25651,\"start\":25648},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25667,\"start\":25663},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25708,\"start\":25704},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25973,\"start\":25969},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26136,\"start\":26132},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26504,\"start\":26501},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28156,\"start\":28153},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28739,\"start\":28738},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31486,\"start\":31485},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31854,\"start\":31850},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31909,\"start\":31905},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31981,\"start\":31977},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32917,\"start\":32913},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32923,\"start\":32919},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33794,\"start\":33790},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33813,\"start\":33809},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33827,\"start\":33823},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33847,\"start\":33843},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33864,\"start\":33860},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33881,\"start\":33877},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33905,\"start\":33901},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33927,\"start\":33923},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34037,\"start\":34033},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34211,\"start\":34207},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34383,\"start\":34379},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34544,\"start\":34540},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34755,\"start\":34751},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34761,\"start\":34757},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34930,\"start\":34926},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":35142,\"start\":35138},{\"end\":35779,\"start\":35773},{\"end\":35793,\"start\":35787},{\"end\":35810,\"start\":35804},{\"end\":35822,\"start\":35816},{\"end\":35835,\"start\":35834},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35840,\"start\":35836},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36415,\"start\":36411},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":37878,\"start\":37875},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":39100,\"start\":39096},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":39990,\"start\":39986},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":40008,\"start\":40004},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":40023,\"start\":40020},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40115,\"start\":40111},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40121,\"start\":40117},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40127,\"start\":40123},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":40133,\"start\":40129},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":43851,\"start\":43848},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3797,\"start\":3794},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4357,\"start\":4354},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4813,\"start\":4810},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4818,\"start\":4815},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4884,\"start\":4881},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4889,\"start\":4886},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5576,\"start\":5573},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5611,\"start\":5608},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5616,\"start\":5613},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5717,\"start\":5713},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5750,\"start\":5747},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6125,\"start\":6121},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6176,\"start\":6172},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6373,\"start\":6369},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6487,\"start\":6483},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6687,\"start\":6683},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6772,\"start\":6768},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7004,\"start\":7000},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7182,\"start\":7178},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7280,\"start\":7276},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7875,\"start\":7871},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8122,\"start\":8118},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8387,\"start\":8383},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8685,\"start\":8681},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9372,\"start\":9368},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9411,\"start\":9407},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9761,\"start\":9757},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10477,\"start\":10473},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14458,\"start\":14455},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17794,\"start\":17791},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18023,\"start\":18020},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19317,\"start\":19313},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19792,\"start\":19788},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21298,\"start\":21294},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21395,\"start\":21391},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24658,\"start\":24654},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25589,\"start\":25586},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25595,\"start\":25591},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25651,\"start\":25648},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25667,\"start\":25663},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25708,\"start\":25704},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25973,\"start\":25969},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26136,\"start\":26132},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26504,\"start\":26501},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28156,\"start\":28153},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28739,\"start\":28738},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31486,\"start\":31485},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31854,\"start\":31850},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31909,\"start\":31905},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31981,\"start\":31977},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32917,\"start\":32913},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32923,\"start\":32919},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33794,\"start\":33790},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33813,\"start\":33809},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33827,\"start\":33823},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33847,\"start\":33843},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33864,\"start\":33860},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33881,\"start\":33877},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33905,\"start\":33901},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33927,\"start\":33923},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34037,\"start\":34033},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34211,\"start\":34207},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34383,\"start\":34379},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34544,\"start\":34540},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34755,\"start\":34751},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34761,\"start\":34757},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34930,\"start\":34926},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":35142,\"start\":35138},{\"end\":35779,\"start\":35773},{\"end\":35793,\"start\":35787},{\"end\":35810,\"start\":35804},{\"end\":35822,\"start\":35816},{\"end\":35835,\"start\":35834},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35840,\"start\":35836},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36415,\"start\":36411},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":37878,\"start\":37875},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":39100,\"start\":39096},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":39990,\"start\":39986},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":40008,\"start\":40004},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":40023,\"start\":40020},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40115,\"start\":40111},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40121,\"start\":40117},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40127,\"start\":40123},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":40133,\"start\":40129},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":43851,\"start\":43848}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40828,\"start\":40660},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41015,\"start\":40829},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41428,\"start\":41016},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42072,\"start\":41429},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42683,\"start\":42073},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42879,\"start\":42684},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43105,\"start\":42880},{\"attributes\":{\"id\":\"fig_12\"},\"end\":43380,\"start\":43106},{\"attributes\":{\"id\":\"fig_14\"},\"end\":43852,\"start\":43381},{\"attributes\":{\"id\":\"fig_15\"},\"end\":47534,\"start\":43853},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47758,\"start\":47535},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48943,\"start\":47759},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49346,\"start\":48944},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49606,\"start\":49347},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50081,\"start\":49607},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50606,\"start\":50082},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":51859,\"start\":50607},{\"attributes\":{\"id\":\"fig_0\"},\"end\":40828,\"start\":40660},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41015,\"start\":40829},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41428,\"start\":41016},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42072,\"start\":41429},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42683,\"start\":42073},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42879,\"start\":42684},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43105,\"start\":42880},{\"attributes\":{\"id\":\"fig_12\"},\"end\":43380,\"start\":43106},{\"attributes\":{\"id\":\"fig_14\"},\"end\":43852,\"start\":43381},{\"attributes\":{\"id\":\"fig_15\"},\"end\":47534,\"start\":43853},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47758,\"start\":47535},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48943,\"start\":47759},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49346,\"start\":48944},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49606,\"start\":49347},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50081,\"start\":49607},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50606,\"start\":50082},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":51859,\"start\":50607}]", "paragraph": "[{\"end\":2018,\"start\":1622},{\"end\":2522,\"start\":2020},{\"end\":3765,\"start\":2524},{\"end\":4496,\"start\":3767},{\"end\":6027,\"start\":4535},{\"end\":6732,\"start\":6054},{\"end\":7502,\"start\":6734},{\"end\":8828,\"start\":7540},{\"end\":9177,\"start\":8861},{\"end\":10213,\"start\":9179},{\"end\":11046,\"start\":10223},{\"end\":13519,\"start\":11056},{\"end\":13912,\"start\":13521},{\"end\":14405,\"start\":13922},{\"end\":15191,\"start\":14407},{\"end\":15491,\"start\":15193},{\"end\":15872,\"start\":15547},{\"end\":17051,\"start\":15874},{\"end\":17516,\"start\":17068},{\"end\":17673,\"start\":17518},{\"end\":18968,\"start\":17695},{\"end\":19132,\"start\":18990},{\"end\":19595,\"start\":19154},{\"end\":20012,\"start\":19597},{\"end\":21165,\"start\":20014},{\"end\":22130,\"start\":21167},{\"end\":22364,\"start\":22183},{\"end\":22489,\"start\":22366},{\"end\":24217,\"start\":22566},{\"end\":24605,\"start\":24219},{\"end\":25404,\"start\":24638},{\"end\":25895,\"start\":25463},{\"end\":26314,\"start\":25897},{\"end\":27032,\"start\":26353},{\"end\":27755,\"start\":27034},{\"end\":28193,\"start\":27757},{\"end\":29567,\"start\":28195},{\"end\":29983,\"start\":29569},{\"end\":30788,\"start\":30018},{\"end\":31053,\"start\":30819},{\"end\":32510,\"start\":31055},{\"end\":33036,\"start\":32573},{\"end\":35318,\"start\":33038},{\"end\":35841,\"start\":35320},{\"end\":37470,\"start\":35843},{\"end\":38236,\"start\":37510},{\"end\":38733,\"start\":38238},{\"end\":38978,\"start\":38735},{\"end\":39943,\"start\":38980},{\"end\":40659,\"start\":39958},{\"end\":2018,\"start\":1622},{\"end\":2522,\"start\":2020},{\"end\":3765,\"start\":2524},{\"end\":4496,\"start\":3767},{\"end\":6027,\"start\":4535},{\"end\":6732,\"start\":6054},{\"end\":7502,\"start\":6734},{\"end\":8828,\"start\":7540},{\"end\":9177,\"start\":8861},{\"end\":10213,\"start\":9179},{\"end\":11046,\"start\":10223},{\"end\":13519,\"start\":11056},{\"end\":13912,\"start\":13521},{\"end\":14405,\"start\":13922},{\"end\":15191,\"start\":14407},{\"end\":15491,\"start\":15193},{\"end\":15872,\"start\":15547},{\"end\":17051,\"start\":15874},{\"end\":17516,\"start\":17068},{\"end\":17673,\"start\":17518},{\"end\":18968,\"start\":17695},{\"end\":19132,\"start\":18990},{\"end\":19595,\"start\":19154},{\"end\":20012,\"start\":19597},{\"end\":21165,\"start\":20014},{\"end\":22130,\"start\":21167},{\"end\":22364,\"start\":22183},{\"end\":22489,\"start\":22366},{\"end\":24217,\"start\":22566},{\"end\":24605,\"start\":24219},{\"end\":25404,\"start\":24638},{\"end\":25895,\"start\":25463},{\"end\":26314,\"start\":25897},{\"end\":27032,\"start\":26353},{\"end\":27755,\"start\":27034},{\"end\":28193,\"start\":27757},{\"end\":29567,\"start\":28195},{\"end\":29983,\"start\":29569},{\"end\":30788,\"start\":30018},{\"end\":31053,\"start\":30819},{\"end\":32510,\"start\":31055},{\"end\":33036,\"start\":32573},{\"end\":35318,\"start\":33038},{\"end\":35841,\"start\":35320},{\"end\":37470,\"start\":35843},{\"end\":38236,\"start\":37510},{\"end\":38733,\"start\":38238},{\"end\":38978,\"start\":38735},{\"end\":39943,\"start\":38980},{\"end\":40659,\"start\":39958}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18989,\"start\":18969},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22182,\"start\":22131},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22565,\"start\":22490},{\"attributes\":{\"id\":\"formula_0\"},\"end\":18989,\"start\":18969},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22182,\"start\":22131},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22565,\"start\":22490}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26593,\"start\":26586},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27887,\"start\":27880},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31535,\"start\":31522},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36017,\"start\":36010},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38977,\"start\":38970},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26593,\"start\":26586},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27887,\"start\":27880},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31535,\"start\":31522},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36017,\"start\":36010},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38977,\"start\":38970}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1620,\"start\":1608},{\"attributes\":{\"n\":\"1.1\"},\"end\":4533,\"start\":4499},{\"attributes\":{\"n\":\"1.2\"},\"end\":6052,\"start\":6030},{\"attributes\":{\"n\":\"2.1\"},\"end\":7538,\"start\":7505},{\"attributes\":{\"n\":\"2.2\"},\"end\":8859,\"start\":8831},{\"end\":10221,\"start\":10216},{\"end\":11054,\"start\":11049},{\"end\":13920,\"start\":13915},{\"attributes\":{\"n\":\"2.2.4\"},\"end\":15545,\"start\":15494},{\"end\":17066,\"start\":17054},{\"attributes\":{\"n\":\"3\"},\"end\":17693,\"start\":17676},{\"attributes\":{\"n\":\"4.1\"},\"end\":19152,\"start\":19135},{\"attributes\":{\"n\":\"4.2\"},\"end\":24636,\"start\":24608},{\"attributes\":{\"n\":\"5\"},\"end\":25461,\"start\":25407},{\"attributes\":{\"n\":\"5.1\"},\"end\":26351,\"start\":26317},{\"attributes\":{\"n\":\"5.2\"},\"end\":30016,\"start\":29986},{\"attributes\":{\"n\":\"5.3\"},\"end\":30817,\"start\":30791},{\"attributes\":{\"n\":\"5.4\"},\"end\":32571,\"start\":32513},{\"attributes\":{\"n\":\"5.5\"},\"end\":37508,\"start\":37473},{\"attributes\":{\"n\":\"6\"},\"end\":39956,\"start\":39946},{\"end\":40669,\"start\":40661},{\"end\":40838,\"start\":40830},{\"end\":41033,\"start\":41017},{\"end\":41446,\"start\":41430},{\"end\":42090,\"start\":42074},{\"end\":42693,\"start\":42685},{\"end\":42890,\"start\":42881},{\"end\":43116,\"start\":43107},{\"end\":43391,\"start\":43382},{\"end\":43863,\"start\":43854},{\"end\":47541,\"start\":47536},{\"end\":49615,\"start\":49608},{\"end\":50615,\"start\":50608},{\"attributes\":{\"n\":\"1\"},\"end\":1620,\"start\":1608},{\"attributes\":{\"n\":\"1.1\"},\"end\":4533,\"start\":4499},{\"attributes\":{\"n\":\"1.2\"},\"end\":6052,\"start\":6030},{\"attributes\":{\"n\":\"2.1\"},\"end\":7538,\"start\":7505},{\"attributes\":{\"n\":\"2.2\"},\"end\":8859,\"start\":8831},{\"end\":10221,\"start\":10216},{\"end\":11054,\"start\":11049},{\"end\":13920,\"start\":13915},{\"attributes\":{\"n\":\"2.2.4\"},\"end\":15545,\"start\":15494},{\"end\":17066,\"start\":17054},{\"attributes\":{\"n\":\"3\"},\"end\":17693,\"start\":17676},{\"attributes\":{\"n\":\"4.1\"},\"end\":19152,\"start\":19135},{\"attributes\":{\"n\":\"4.2\"},\"end\":24636,\"start\":24608},{\"attributes\":{\"n\":\"5\"},\"end\":25461,\"start\":25407},{\"attributes\":{\"n\":\"5.1\"},\"end\":26351,\"start\":26317},{\"attributes\":{\"n\":\"5.2\"},\"end\":30016,\"start\":29986},{\"attributes\":{\"n\":\"5.3\"},\"end\":30817,\"start\":30791},{\"attributes\":{\"n\":\"5.4\"},\"end\":32571,\"start\":32513},{\"attributes\":{\"n\":\"5.5\"},\"end\":37508,\"start\":37473},{\"attributes\":{\"n\":\"6\"},\"end\":39956,\"start\":39946},{\"end\":40669,\"start\":40661},{\"end\":40838,\"start\":40830},{\"end\":41033,\"start\":41017},{\"end\":41446,\"start\":41430},{\"end\":42090,\"start\":42074},{\"end\":42693,\"start\":42685},{\"end\":42890,\"start\":42881},{\"end\":43116,\"start\":43107},{\"end\":43391,\"start\":43382},{\"end\":43863,\"start\":43854},{\"end\":47541,\"start\":47536},{\"end\":49615,\"start\":49608},{\"end\":50615,\"start\":50608}]", "table": "[{\"end\":49346,\"start\":49022},{\"end\":50081,\"start\":49780},{\"end\":50606,\"start\":50261},{\"end\":51859,\"start\":50881},{\"end\":49346,\"start\":49022},{\"end\":50081,\"start\":49780},{\"end\":50606,\"start\":50261},{\"end\":51859,\"start\":50881}]", "figure_caption": "[{\"end\":40828,\"start\":40671},{\"end\":41015,\"start\":40840},{\"end\":41428,\"start\":41036},{\"end\":42072,\"start\":41449},{\"end\":42683,\"start\":42093},{\"end\":42879,\"start\":42695},{\"end\":43105,\"start\":42893},{\"end\":43380,\"start\":43119},{\"end\":43852,\"start\":43394},{\"end\":47534,\"start\":43866},{\"end\":47758,\"start\":47543},{\"end\":48943,\"start\":47761},{\"end\":49022,\"start\":48946},{\"end\":49606,\"start\":49349},{\"end\":49780,\"start\":49617},{\"end\":50261,\"start\":50084},{\"end\":50881,\"start\":50617},{\"end\":40828,\"start\":40671},{\"end\":41015,\"start\":40840},{\"end\":41428,\"start\":41036},{\"end\":42072,\"start\":41449},{\"end\":42683,\"start\":42093},{\"end\":42879,\"start\":42695},{\"end\":43105,\"start\":42893},{\"end\":43380,\"start\":43119},{\"end\":43852,\"start\":43394},{\"end\":47534,\"start\":43866},{\"end\":47758,\"start\":47543},{\"end\":48943,\"start\":47761},{\"end\":49022,\"start\":48946},{\"end\":49606,\"start\":49349},{\"end\":49780,\"start\":49617},{\"end\":50261,\"start\":50084},{\"end\":50881,\"start\":50617}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3683,\"start\":3677},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3696,\"start\":3690},{\"end\":10686,\"start\":10680},{\"end\":11218,\"start\":11213},{\"end\":11522,\"start\":11517},{\"end\":11652,\"start\":11647},{\"end\":15156,\"start\":15151},{\"end\":16012,\"start\":16006},{\"end\":17249,\"start\":17243},{\"end\":17265,\"start\":17259},{\"end\":23323,\"start\":23313},{\"end\":23641,\"start\":23635},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24927,\"start\":24921},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28898,\"start\":28892},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29108,\"start\":29102},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29982,\"start\":29976},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30385,\"start\":30379},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32176,\"start\":32170},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33153,\"start\":33146},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33431,\"start\":33424},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36164,\"start\":36158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38323,\"start\":38317},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39338,\"start\":39332},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39824,\"start\":39818},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3683,\"start\":3677},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3696,\"start\":3690},{\"end\":10686,\"start\":10680},{\"end\":11218,\"start\":11213},{\"end\":11522,\"start\":11517},{\"end\":11652,\"start\":11647},{\"end\":15156,\"start\":15151},{\"end\":16012,\"start\":16006},{\"end\":17249,\"start\":17243},{\"end\":17265,\"start\":17259},{\"end\":23323,\"start\":23313},{\"end\":23641,\"start\":23635},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24927,\"start\":24921},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28898,\"start\":28892},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29108,\"start\":29102},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29982,\"start\":29976},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30385,\"start\":30379},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32176,\"start\":32170},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33153,\"start\":33146},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33431,\"start\":33424},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36164,\"start\":36158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38323,\"start\":38317},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39338,\"start\":39332},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39824,\"start\":39818}]", "bib_author_first_name": "[{\"end\":54500,\"start\":54499},{\"end\":54508,\"start\":54507},{\"end\":54521,\"start\":54520},{\"end\":54529,\"start\":54528},{\"end\":54541,\"start\":54540},{\"end\":54838,\"start\":54837},{\"end\":54846,\"start\":54845},{\"end\":54856,\"start\":54855},{\"end\":54869,\"start\":54868},{\"end\":54878,\"start\":54877},{\"end\":55177,\"start\":55176},{\"end\":55191,\"start\":55190},{\"end\":55204,\"start\":55203},{\"end\":55206,\"start\":55205},{\"end\":55471,\"start\":55470},{\"end\":55485,\"start\":55484},{\"end\":55663,\"start\":55662},{\"end\":55665,\"start\":55664},{\"end\":55676,\"start\":55675},{\"end\":55811,\"start\":55810},{\"end\":55823,\"start\":55822},{\"end\":55825,\"start\":55824},{\"end\":55837,\"start\":55833},{\"end\":55981,\"start\":55980},{\"end\":55993,\"start\":55992},{\"end\":56002,\"start\":56001},{\"end\":56012,\"start\":56011},{\"end\":56022,\"start\":56021},{\"end\":56024,\"start\":56023},{\"end\":56251,\"start\":56250},{\"end\":56261,\"start\":56260},{\"end\":56270,\"start\":56269},{\"end\":56272,\"start\":56271},{\"end\":56284,\"start\":56283},{\"end\":56292,\"start\":56291},{\"end\":56301,\"start\":56300},{\"end\":56322,\"start\":56321},{\"end\":56339,\"start\":56338},{\"end\":56353,\"start\":56352},{\"end\":56371,\"start\":56370},{\"end\":56681,\"start\":56680},{\"end\":56690,\"start\":56689},{\"end\":56700,\"start\":56699},{\"end\":56710,\"start\":56709},{\"end\":56986,\"start\":56985},{\"end\":56992,\"start\":56991},{\"end\":57001,\"start\":57000},{\"end\":57008,\"start\":57007},{\"end\":57238,\"start\":57237},{\"end\":57246,\"start\":57245},{\"end\":57254,\"start\":57253},{\"end\":57267,\"start\":57263},{\"end\":57273,\"start\":57272},{\"end\":57279,\"start\":57278},{\"end\":57496,\"start\":57495},{\"end\":57511,\"start\":57510},{\"end\":57519,\"start\":57518},{\"end\":57525,\"start\":57524},{\"end\":57535,\"start\":57534},{\"end\":57547,\"start\":57546},{\"end\":57553,\"start\":57552},{\"end\":57562,\"start\":57561},{\"end\":57574,\"start\":57573},{\"end\":57584,\"start\":57583},{\"end\":57935,\"start\":57934},{\"end\":57947,\"start\":57946},{\"end\":57957,\"start\":57956},{\"end\":58220,\"start\":58219},{\"end\":58229,\"start\":58228},{\"end\":58455,\"start\":58454},{\"end\":58467,\"start\":58466},{\"end\":58638,\"start\":58637},{\"end\":58646,\"start\":58645},{\"end\":58654,\"start\":58653},{\"end\":58656,\"start\":58655},{\"end\":58667,\"start\":58666},{\"end\":58676,\"start\":58675},{\"end\":58918,\"start\":58917},{\"end\":58932,\"start\":58931},{\"end\":58945,\"start\":58944},{\"end\":58947,\"start\":58946},{\"end\":58959,\"start\":58958},{\"end\":58971,\"start\":58970},{\"end\":58980,\"start\":58979},{\"end\":58982,\"start\":58981},{\"end\":58992,\"start\":58991},{\"end\":59004,\"start\":59003},{\"end\":59013,\"start\":59012},{\"end\":59026,\"start\":59025},{\"end\":59295,\"start\":59291},{\"end\":59302,\"start\":59301},{\"end\":59311,\"start\":59310},{\"end\":59323,\"start\":59322},{\"end\":59331,\"start\":59330},{\"end\":59341,\"start\":59340},{\"end\":59352,\"start\":59351},{\"end\":59362,\"start\":59361},{\"end\":59364,\"start\":59363},{\"end\":59727,\"start\":59726},{\"end\":59738,\"start\":59737},{\"end\":59745,\"start\":59744},{\"end\":59754,\"start\":59753},{\"end\":59765,\"start\":59764},{\"end\":59773,\"start\":59772},{\"end\":59784,\"start\":59783},{\"end\":59792,\"start\":59791},{\"end\":59809,\"start\":59805},{\"end\":59815,\"start\":59814},{\"end\":59817,\"start\":59816},{\"end\":60091,\"start\":60090},{\"end\":60099,\"start\":60098},{\"end\":60107,\"start\":60106},{\"end\":60115,\"start\":60114},{\"end\":60125,\"start\":60124},{\"end\":60137,\"start\":60136},{\"end\":60332,\"start\":60331},{\"end\":60334,\"start\":60333},{\"end\":60539,\"start\":60538},{\"end\":60552,\"start\":60551},{\"end\":60554,\"start\":60553},{\"end\":60563,\"start\":60562},{\"end\":60565,\"start\":60564},{\"end\":60749,\"start\":60748},{\"end\":60761,\"start\":60760},{\"end\":60763,\"start\":60762},{\"end\":60907,\"start\":60906},{\"end\":60915,\"start\":60914},{\"end\":60925,\"start\":60924},{\"end\":61062,\"start\":61061},{\"end\":61064,\"start\":61063},{\"end\":61173,\"start\":61172},{\"end\":61184,\"start\":61183},{\"end\":61191,\"start\":61190},{\"end\":61198,\"start\":61197},{\"end\":61210,\"start\":61209},{\"end\":61218,\"start\":61217},{\"end\":61230,\"start\":61229},{\"end\":61239,\"start\":61238},{\"end\":61252,\"start\":61251},{\"end\":61509,\"start\":61508},{\"end\":61521,\"start\":61520},{\"end\":61826,\"start\":61825},{\"end\":62007,\"start\":62006},{\"end\":62013,\"start\":62012},{\"end\":62022,\"start\":62021},{\"end\":62029,\"start\":62028},{\"end\":62210,\"start\":62209},{\"end\":62219,\"start\":62218},{\"end\":62225,\"start\":62224},{\"end\":62234,\"start\":62233},{\"end\":62241,\"start\":62240},{\"end\":62249,\"start\":62248},{\"end\":62256,\"start\":62255},{\"end\":62498,\"start\":62497},{\"end\":62506,\"start\":62505},{\"end\":62513,\"start\":62512},{\"end\":62520,\"start\":62519},{\"end\":62690,\"start\":62689},{\"end\":62698,\"start\":62697},{\"end\":62707,\"start\":62706},{\"end\":62715,\"start\":62714},{\"end\":62723,\"start\":62722},{\"end\":62731,\"start\":62730},{\"end\":62739,\"start\":62738},{\"end\":62747,\"start\":62746},{\"end\":62754,\"start\":62753},{\"end\":63047,\"start\":63046},{\"end\":63058,\"start\":63057},{\"end\":63065,\"start\":63064},{\"end\":63076,\"start\":63075},{\"end\":63087,\"start\":63086},{\"end\":63096,\"start\":63095},{\"end\":63105,\"start\":63104},{\"end\":63383,\"start\":63382},{\"end\":63385,\"start\":63384},{\"end\":63397,\"start\":63396},{\"end\":63409,\"start\":63408},{\"end\":63421,\"start\":63420},{\"end\":63682,\"start\":63681},{\"end\":63695,\"start\":63694},{\"end\":63980,\"start\":63979},{\"end\":63991,\"start\":63990},{\"end\":64001,\"start\":64000},{\"end\":64307,\"start\":64306},{\"end\":64318,\"start\":64317},{\"end\":64327,\"start\":64326},{\"end\":64495,\"start\":64494},{\"end\":64502,\"start\":64501},{\"end\":64511,\"start\":64510},{\"end\":64521,\"start\":64520},{\"end\":64523,\"start\":64522},{\"end\":64530,\"start\":64529},{\"end\":64540,\"start\":64539},{\"end\":64809,\"start\":64805},{\"end\":64815,\"start\":64814},{\"end\":65021,\"start\":65017},{\"end\":65031,\"start\":65027},{\"end\":65043,\"start\":65039},{\"end\":65055,\"start\":65051},{\"end\":65066,\"start\":65062},{\"end\":65294,\"start\":65293},{\"end\":65304,\"start\":65303},{\"end\":65319,\"start\":65318},{\"end\":65335,\"start\":65330},{\"end\":65343,\"start\":65342},{\"end\":65720,\"start\":65719},{\"end\":65732,\"start\":65731},{\"end\":65742,\"start\":65741},{\"end\":65744,\"start\":65743},{\"end\":66062,\"start\":66061},{\"end\":66076,\"start\":66075},{\"end\":66088,\"start\":66087},{\"end\":66090,\"start\":66089},{\"end\":66102,\"start\":66101},{\"end\":66110,\"start\":66109},{\"end\":66386,\"start\":66385},{\"end\":66396,\"start\":66395},{\"end\":66405,\"start\":66404},{\"end\":66414,\"start\":66413},{\"end\":66425,\"start\":66424},{\"end\":66438,\"start\":66437},{\"end\":66450,\"start\":66449},{\"end\":66460,\"start\":66459},{\"end\":66468,\"start\":66467},{\"end\":54500,\"start\":54499},{\"end\":54508,\"start\":54507},{\"end\":54521,\"start\":54520},{\"end\":54529,\"start\":54528},{\"end\":54541,\"start\":54540},{\"end\":54838,\"start\":54837},{\"end\":54846,\"start\":54845},{\"end\":54856,\"start\":54855},{\"end\":54869,\"start\":54868},{\"end\":54878,\"start\":54877},{\"end\":55177,\"start\":55176},{\"end\":55191,\"start\":55190},{\"end\":55204,\"start\":55203},{\"end\":55206,\"start\":55205},{\"end\":55471,\"start\":55470},{\"end\":55485,\"start\":55484},{\"end\":55663,\"start\":55662},{\"end\":55665,\"start\":55664},{\"end\":55676,\"start\":55675},{\"end\":55811,\"start\":55810},{\"end\":55823,\"start\":55822},{\"end\":55825,\"start\":55824},{\"end\":55837,\"start\":55833},{\"end\":55981,\"start\":55980},{\"end\":55993,\"start\":55992},{\"end\":56002,\"start\":56001},{\"end\":56012,\"start\":56011},{\"end\":56022,\"start\":56021},{\"end\":56024,\"start\":56023},{\"end\":56251,\"start\":56250},{\"end\":56261,\"start\":56260},{\"end\":56270,\"start\":56269},{\"end\":56272,\"start\":56271},{\"end\":56284,\"start\":56283},{\"end\":56292,\"start\":56291},{\"end\":56301,\"start\":56300},{\"end\":56322,\"start\":56321},{\"end\":56339,\"start\":56338},{\"end\":56353,\"start\":56352},{\"end\":56371,\"start\":56370},{\"end\":56681,\"start\":56680},{\"end\":56690,\"start\":56689},{\"end\":56700,\"start\":56699},{\"end\":56710,\"start\":56709},{\"end\":56986,\"start\":56985},{\"end\":56992,\"start\":56991},{\"end\":57001,\"start\":57000},{\"end\":57008,\"start\":57007},{\"end\":57238,\"start\":57237},{\"end\":57246,\"start\":57245},{\"end\":57254,\"start\":57253},{\"end\":57267,\"start\":57263},{\"end\":57273,\"start\":57272},{\"end\":57279,\"start\":57278},{\"end\":57496,\"start\":57495},{\"end\":57511,\"start\":57510},{\"end\":57519,\"start\":57518},{\"end\":57525,\"start\":57524},{\"end\":57535,\"start\":57534},{\"end\":57547,\"start\":57546},{\"end\":57553,\"start\":57552},{\"end\":57562,\"start\":57561},{\"end\":57574,\"start\":57573},{\"end\":57584,\"start\":57583},{\"end\":57935,\"start\":57934},{\"end\":57947,\"start\":57946},{\"end\":57957,\"start\":57956},{\"end\":58220,\"start\":58219},{\"end\":58229,\"start\":58228},{\"end\":58455,\"start\":58454},{\"end\":58467,\"start\":58466},{\"end\":58638,\"start\":58637},{\"end\":58646,\"start\":58645},{\"end\":58654,\"start\":58653},{\"end\":58656,\"start\":58655},{\"end\":58667,\"start\":58666},{\"end\":58676,\"start\":58675},{\"end\":58918,\"start\":58917},{\"end\":58932,\"start\":58931},{\"end\":58945,\"start\":58944},{\"end\":58947,\"start\":58946},{\"end\":58959,\"start\":58958},{\"end\":58971,\"start\":58970},{\"end\":58980,\"start\":58979},{\"end\":58982,\"start\":58981},{\"end\":58992,\"start\":58991},{\"end\":59004,\"start\":59003},{\"end\":59013,\"start\":59012},{\"end\":59026,\"start\":59025},{\"end\":59295,\"start\":59291},{\"end\":59302,\"start\":59301},{\"end\":59311,\"start\":59310},{\"end\":59323,\"start\":59322},{\"end\":59331,\"start\":59330},{\"end\":59341,\"start\":59340},{\"end\":59352,\"start\":59351},{\"end\":59362,\"start\":59361},{\"end\":59364,\"start\":59363},{\"end\":59727,\"start\":59726},{\"end\":59738,\"start\":59737},{\"end\":59745,\"start\":59744},{\"end\":59754,\"start\":59753},{\"end\":59765,\"start\":59764},{\"end\":59773,\"start\":59772},{\"end\":59784,\"start\":59783},{\"end\":59792,\"start\":59791},{\"end\":59809,\"start\":59805},{\"end\":59815,\"start\":59814},{\"end\":59817,\"start\":59816},{\"end\":60091,\"start\":60090},{\"end\":60099,\"start\":60098},{\"end\":60107,\"start\":60106},{\"end\":60115,\"start\":60114},{\"end\":60125,\"start\":60124},{\"end\":60137,\"start\":60136},{\"end\":60332,\"start\":60331},{\"end\":60334,\"start\":60333},{\"end\":60539,\"start\":60538},{\"end\":60552,\"start\":60551},{\"end\":60554,\"start\":60553},{\"end\":60563,\"start\":60562},{\"end\":60565,\"start\":60564},{\"end\":60749,\"start\":60748},{\"end\":60761,\"start\":60760},{\"end\":60763,\"start\":60762},{\"end\":60907,\"start\":60906},{\"end\":60915,\"start\":60914},{\"end\":60925,\"start\":60924},{\"end\":61062,\"start\":61061},{\"end\":61064,\"start\":61063},{\"end\":61173,\"start\":61172},{\"end\":61184,\"start\":61183},{\"end\":61191,\"start\":61190},{\"end\":61198,\"start\":61197},{\"end\":61210,\"start\":61209},{\"end\":61218,\"start\":61217},{\"end\":61230,\"start\":61229},{\"end\":61239,\"start\":61238},{\"end\":61252,\"start\":61251},{\"end\":61509,\"start\":61508},{\"end\":61521,\"start\":61520},{\"end\":61826,\"start\":61825},{\"end\":62007,\"start\":62006},{\"end\":62013,\"start\":62012},{\"end\":62022,\"start\":62021},{\"end\":62029,\"start\":62028},{\"end\":62210,\"start\":62209},{\"end\":62219,\"start\":62218},{\"end\":62225,\"start\":62224},{\"end\":62234,\"start\":62233},{\"end\":62241,\"start\":62240},{\"end\":62249,\"start\":62248},{\"end\":62256,\"start\":62255},{\"end\":62498,\"start\":62497},{\"end\":62506,\"start\":62505},{\"end\":62513,\"start\":62512},{\"end\":62520,\"start\":62519},{\"end\":62690,\"start\":62689},{\"end\":62698,\"start\":62697},{\"end\":62707,\"start\":62706},{\"end\":62715,\"start\":62714},{\"end\":62723,\"start\":62722},{\"end\":62731,\"start\":62730},{\"end\":62739,\"start\":62738},{\"end\":62747,\"start\":62746},{\"end\":62754,\"start\":62753},{\"end\":63047,\"start\":63046},{\"end\":63058,\"start\":63057},{\"end\":63065,\"start\":63064},{\"end\":63076,\"start\":63075},{\"end\":63087,\"start\":63086},{\"end\":63096,\"start\":63095},{\"end\":63105,\"start\":63104},{\"end\":63383,\"start\":63382},{\"end\":63385,\"start\":63384},{\"end\":63397,\"start\":63396},{\"end\":63409,\"start\":63408},{\"end\":63421,\"start\":63420},{\"end\":63682,\"start\":63681},{\"end\":63695,\"start\":63694},{\"end\":63980,\"start\":63979},{\"end\":63991,\"start\":63990},{\"end\":64001,\"start\":64000},{\"end\":64307,\"start\":64306},{\"end\":64318,\"start\":64317},{\"end\":64327,\"start\":64326},{\"end\":64495,\"start\":64494},{\"end\":64502,\"start\":64501},{\"end\":64511,\"start\":64510},{\"end\":64521,\"start\":64520},{\"end\":64523,\"start\":64522},{\"end\":64530,\"start\":64529},{\"end\":64540,\"start\":64539},{\"end\":64809,\"start\":64805},{\"end\":64815,\"start\":64814},{\"end\":65021,\"start\":65017},{\"end\":65031,\"start\":65027},{\"end\":65043,\"start\":65039},{\"end\":65055,\"start\":65051},{\"end\":65066,\"start\":65062},{\"end\":65294,\"start\":65293},{\"end\":65304,\"start\":65303},{\"end\":65319,\"start\":65318},{\"end\":65335,\"start\":65330},{\"end\":65343,\"start\":65342},{\"end\":65720,\"start\":65719},{\"end\":65732,\"start\":65731},{\"end\":65742,\"start\":65741},{\"end\":65744,\"start\":65743},{\"end\":66062,\"start\":66061},{\"end\":66076,\"start\":66075},{\"end\":66088,\"start\":66087},{\"end\":66090,\"start\":66089},{\"end\":66102,\"start\":66101},{\"end\":66110,\"start\":66109},{\"end\":66386,\"start\":66385},{\"end\":66396,\"start\":66395},{\"end\":66405,\"start\":66404},{\"end\":66414,\"start\":66413},{\"end\":66425,\"start\":66424},{\"end\":66438,\"start\":66437},{\"end\":66450,\"start\":66449},{\"end\":66460,\"start\":66459},{\"end\":66468,\"start\":66467}]", "bib_author_last_name": "[{\"end\":54505,\"start\":54501},{\"end\":54518,\"start\":54509},{\"end\":54526,\"start\":54522},{\"end\":54538,\"start\":54530},{\"end\":54547,\"start\":54542},{\"end\":54843,\"start\":54839},{\"end\":54853,\"start\":54847},{\"end\":54866,\"start\":54857},{\"end\":54875,\"start\":54870},{\"end\":54887,\"start\":54879},{\"end\":55188,\"start\":55178},{\"end\":55201,\"start\":55192},{\"end\":55213,\"start\":55207},{\"end\":55482,\"start\":55472},{\"end\":55497,\"start\":55486},{\"end\":55673,\"start\":55666},{\"end\":55684,\"start\":55677},{\"end\":55820,\"start\":55812},{\"end\":55831,\"start\":55826},{\"end\":55841,\"start\":55838},{\"end\":55990,\"start\":55982},{\"end\":55999,\"start\":55994},{\"end\":56009,\"start\":56003},{\"end\":56019,\"start\":56013},{\"end\":56032,\"start\":56025},{\"end\":56258,\"start\":56252},{\"end\":56267,\"start\":56262},{\"end\":56281,\"start\":56273},{\"end\":56289,\"start\":56285},{\"end\":56298,\"start\":56293},{\"end\":56319,\"start\":56302},{\"end\":56336,\"start\":56323},{\"end\":56350,\"start\":56340},{\"end\":56368,\"start\":56354},{\"end\":56379,\"start\":56372},{\"end\":56687,\"start\":56682},{\"end\":56697,\"start\":56691},{\"end\":56707,\"start\":56701},{\"end\":56718,\"start\":56711},{\"end\":56989,\"start\":56987},{\"end\":56998,\"start\":56993},{\"end\":57005,\"start\":57002},{\"end\":57012,\"start\":57009},{\"end\":57243,\"start\":57239},{\"end\":57251,\"start\":57247},{\"end\":57261,\"start\":57255},{\"end\":57270,\"start\":57268},{\"end\":57276,\"start\":57274},{\"end\":57287,\"start\":57280},{\"end\":57508,\"start\":57497},{\"end\":57516,\"start\":57512},{\"end\":57522,\"start\":57520},{\"end\":57532,\"start\":57526},{\"end\":57544,\"start\":57536},{\"end\":57550,\"start\":57548},{\"end\":57559,\"start\":57554},{\"end\":57571,\"start\":57563},{\"end\":57581,\"start\":57575},{\"end\":57594,\"start\":57585},{\"end\":57944,\"start\":57936},{\"end\":57954,\"start\":57948},{\"end\":57963,\"start\":57958},{\"end\":58226,\"start\":58221},{\"end\":58238,\"start\":58230},{\"end\":58464,\"start\":58456},{\"end\":58476,\"start\":58468},{\"end\":58643,\"start\":58639},{\"end\":58651,\"start\":58647},{\"end\":58664,\"start\":58657},{\"end\":58673,\"start\":58668},{\"end\":58685,\"start\":58677},{\"end\":58929,\"start\":58919},{\"end\":58942,\"start\":58933},{\"end\":58956,\"start\":58948},{\"end\":58968,\"start\":58960},{\"end\":58977,\"start\":58972},{\"end\":58989,\"start\":58983},{\"end\":59001,\"start\":58993},{\"end\":59010,\"start\":59005},{\"end\":59023,\"start\":59014},{\"end\":59032,\"start\":59027},{\"end\":59299,\"start\":59296},{\"end\":59308,\"start\":59303},{\"end\":59320,\"start\":59312},{\"end\":59328,\"start\":59324},{\"end\":59338,\"start\":59332},{\"end\":59349,\"start\":59342},{\"end\":59359,\"start\":59353},{\"end\":59372,\"start\":59365},{\"end\":59735,\"start\":59728},{\"end\":59742,\"start\":59739},{\"end\":59751,\"start\":59746},{\"end\":59762,\"start\":59755},{\"end\":59770,\"start\":59766},{\"end\":59781,\"start\":59774},{\"end\":59789,\"start\":59785},{\"end\":59803,\"start\":59793},{\"end\":59812,\"start\":59810},{\"end\":59824,\"start\":59818},{\"end\":60096,\"start\":60092},{\"end\":60104,\"start\":60100},{\"end\":60112,\"start\":60108},{\"end\":60122,\"start\":60116},{\"end\":60134,\"start\":60126},{\"end\":60146,\"start\":60138},{\"end\":60341,\"start\":60335},{\"end\":60549,\"start\":60540},{\"end\":60560,\"start\":60555},{\"end\":60573,\"start\":60566},{\"end\":60758,\"start\":60750},{\"end\":60769,\"start\":60764},{\"end\":60912,\"start\":60908},{\"end\":60922,\"start\":60916},{\"end\":60934,\"start\":60926},{\"end\":61072,\"start\":61065},{\"end\":61181,\"start\":61174},{\"end\":61188,\"start\":61185},{\"end\":61195,\"start\":61192},{\"end\":61207,\"start\":61199},{\"end\":61215,\"start\":61211},{\"end\":61227,\"start\":61219},{\"end\":61236,\"start\":61231},{\"end\":61249,\"start\":61240},{\"end\":61263,\"start\":61253},{\"end\":61518,\"start\":61510},{\"end\":61531,\"start\":61522},{\"end\":61830,\"start\":61827},{\"end\":62010,\"start\":62008},{\"end\":62019,\"start\":62014},{\"end\":62026,\"start\":62023},{\"end\":62033,\"start\":62030},{\"end\":62216,\"start\":62211},{\"end\":62222,\"start\":62220},{\"end\":62231,\"start\":62226},{\"end\":62238,\"start\":62235},{\"end\":62246,\"start\":62242},{\"end\":62253,\"start\":62250},{\"end\":62259,\"start\":62257},{\"end\":62503,\"start\":62499},{\"end\":62510,\"start\":62507},{\"end\":62517,\"start\":62514},{\"end\":62523,\"start\":62521},{\"end\":62695,\"start\":62691},{\"end\":62704,\"start\":62699},{\"end\":62712,\"start\":62708},{\"end\":62720,\"start\":62716},{\"end\":62728,\"start\":62724},{\"end\":62736,\"start\":62732},{\"end\":62744,\"start\":62740},{\"end\":62751,\"start\":62748},{\"end\":62757,\"start\":62755},{\"end\":63055,\"start\":63048},{\"end\":63062,\"start\":63059},{\"end\":63073,\"start\":63066},{\"end\":63084,\"start\":63077},{\"end\":63093,\"start\":63088},{\"end\":63102,\"start\":63097},{\"end\":63113,\"start\":63106},{\"end\":63394,\"start\":63386},{\"end\":63406,\"start\":63398},{\"end\":63418,\"start\":63410},{\"end\":63430,\"start\":63422},{\"end\":63692,\"start\":63683},{\"end\":63700,\"start\":63696},{\"end\":63988,\"start\":63981},{\"end\":63998,\"start\":63992},{\"end\":64008,\"start\":64002},{\"end\":64315,\"start\":64308},{\"end\":64324,\"start\":64319},{\"end\":64334,\"start\":64328},{\"end\":64499,\"start\":64496},{\"end\":64508,\"start\":64503},{\"end\":64518,\"start\":64512},{\"end\":64527,\"start\":64524},{\"end\":64537,\"start\":64531},{\"end\":64548,\"start\":64541},{\"end\":64812,\"start\":64810},{\"end\":64823,\"start\":64816},{\"end\":65025,\"start\":65022},{\"end\":65037,\"start\":65032},{\"end\":65049,\"start\":65044},{\"end\":65060,\"start\":65056},{\"end\":65070,\"start\":65067},{\"end\":65301,\"start\":65295},{\"end\":65316,\"start\":65305},{\"end\":65328,\"start\":65320},{\"end\":65340,\"start\":65336},{\"end\":65349,\"start\":65344},{\"end\":65729,\"start\":65721},{\"end\":65739,\"start\":65733},{\"end\":65752,\"start\":65745},{\"end\":66073,\"start\":66063},{\"end\":66085,\"start\":66077},{\"end\":66099,\"start\":66091},{\"end\":66107,\"start\":66103},{\"end\":66120,\"start\":66111},{\"end\":66393,\"start\":66387},{\"end\":66402,\"start\":66397},{\"end\":66411,\"start\":66406},{\"end\":66422,\"start\":66415},{\"end\":66435,\"start\":66426},{\"end\":66447,\"start\":66439},{\"end\":66457,\"start\":66451},{\"end\":66465,\"start\":66461},{\"end\":66476,\"start\":66469},{\"end\":54505,\"start\":54501},{\"end\":54518,\"start\":54509},{\"end\":54526,\"start\":54522},{\"end\":54538,\"start\":54530},{\"end\":54547,\"start\":54542},{\"end\":54843,\"start\":54839},{\"end\":54853,\"start\":54847},{\"end\":54866,\"start\":54857},{\"end\":54875,\"start\":54870},{\"end\":54887,\"start\":54879},{\"end\":55188,\"start\":55178},{\"end\":55201,\"start\":55192},{\"end\":55213,\"start\":55207},{\"end\":55482,\"start\":55472},{\"end\":55497,\"start\":55486},{\"end\":55673,\"start\":55666},{\"end\":55684,\"start\":55677},{\"end\":55820,\"start\":55812},{\"end\":55831,\"start\":55826},{\"end\":55841,\"start\":55838},{\"end\":55990,\"start\":55982},{\"end\":55999,\"start\":55994},{\"end\":56009,\"start\":56003},{\"end\":56019,\"start\":56013},{\"end\":56032,\"start\":56025},{\"end\":56258,\"start\":56252},{\"end\":56267,\"start\":56262},{\"end\":56281,\"start\":56273},{\"end\":56289,\"start\":56285},{\"end\":56298,\"start\":56293},{\"end\":56319,\"start\":56302},{\"end\":56336,\"start\":56323},{\"end\":56350,\"start\":56340},{\"end\":56368,\"start\":56354},{\"end\":56379,\"start\":56372},{\"end\":56687,\"start\":56682},{\"end\":56697,\"start\":56691},{\"end\":56707,\"start\":56701},{\"end\":56718,\"start\":56711},{\"end\":56989,\"start\":56987},{\"end\":56998,\"start\":56993},{\"end\":57005,\"start\":57002},{\"end\":57012,\"start\":57009},{\"end\":57243,\"start\":57239},{\"end\":57251,\"start\":57247},{\"end\":57261,\"start\":57255},{\"end\":57270,\"start\":57268},{\"end\":57276,\"start\":57274},{\"end\":57287,\"start\":57280},{\"end\":57508,\"start\":57497},{\"end\":57516,\"start\":57512},{\"end\":57522,\"start\":57520},{\"end\":57532,\"start\":57526},{\"end\":57544,\"start\":57536},{\"end\":57550,\"start\":57548},{\"end\":57559,\"start\":57554},{\"end\":57571,\"start\":57563},{\"end\":57581,\"start\":57575},{\"end\":57594,\"start\":57585},{\"end\":57944,\"start\":57936},{\"end\":57954,\"start\":57948},{\"end\":57963,\"start\":57958},{\"end\":58226,\"start\":58221},{\"end\":58238,\"start\":58230},{\"end\":58464,\"start\":58456},{\"end\":58476,\"start\":58468},{\"end\":58643,\"start\":58639},{\"end\":58651,\"start\":58647},{\"end\":58664,\"start\":58657},{\"end\":58673,\"start\":58668},{\"end\":58685,\"start\":58677},{\"end\":58929,\"start\":58919},{\"end\":58942,\"start\":58933},{\"end\":58956,\"start\":58948},{\"end\":58968,\"start\":58960},{\"end\":58977,\"start\":58972},{\"end\":58989,\"start\":58983},{\"end\":59001,\"start\":58993},{\"end\":59010,\"start\":59005},{\"end\":59023,\"start\":59014},{\"end\":59032,\"start\":59027},{\"end\":59299,\"start\":59296},{\"end\":59308,\"start\":59303},{\"end\":59320,\"start\":59312},{\"end\":59328,\"start\":59324},{\"end\":59338,\"start\":59332},{\"end\":59349,\"start\":59342},{\"end\":59359,\"start\":59353},{\"end\":59372,\"start\":59365},{\"end\":59735,\"start\":59728},{\"end\":59742,\"start\":59739},{\"end\":59751,\"start\":59746},{\"end\":59762,\"start\":59755},{\"end\":59770,\"start\":59766},{\"end\":59781,\"start\":59774},{\"end\":59789,\"start\":59785},{\"end\":59803,\"start\":59793},{\"end\":59812,\"start\":59810},{\"end\":59824,\"start\":59818},{\"end\":60096,\"start\":60092},{\"end\":60104,\"start\":60100},{\"end\":60112,\"start\":60108},{\"end\":60122,\"start\":60116},{\"end\":60134,\"start\":60126},{\"end\":60146,\"start\":60138},{\"end\":60341,\"start\":60335},{\"end\":60549,\"start\":60540},{\"end\":60560,\"start\":60555},{\"end\":60573,\"start\":60566},{\"end\":60758,\"start\":60750},{\"end\":60769,\"start\":60764},{\"end\":60912,\"start\":60908},{\"end\":60922,\"start\":60916},{\"end\":60934,\"start\":60926},{\"end\":61072,\"start\":61065},{\"end\":61181,\"start\":61174},{\"end\":61188,\"start\":61185},{\"end\":61195,\"start\":61192},{\"end\":61207,\"start\":61199},{\"end\":61215,\"start\":61211},{\"end\":61227,\"start\":61219},{\"end\":61236,\"start\":61231},{\"end\":61249,\"start\":61240},{\"end\":61263,\"start\":61253},{\"end\":61518,\"start\":61510},{\"end\":61531,\"start\":61522},{\"end\":61830,\"start\":61827},{\"end\":62010,\"start\":62008},{\"end\":62019,\"start\":62014},{\"end\":62026,\"start\":62023},{\"end\":62033,\"start\":62030},{\"end\":62216,\"start\":62211},{\"end\":62222,\"start\":62220},{\"end\":62231,\"start\":62226},{\"end\":62238,\"start\":62235},{\"end\":62246,\"start\":62242},{\"end\":62253,\"start\":62250},{\"end\":62259,\"start\":62257},{\"end\":62503,\"start\":62499},{\"end\":62510,\"start\":62507},{\"end\":62517,\"start\":62514},{\"end\":62523,\"start\":62521},{\"end\":62695,\"start\":62691},{\"end\":62704,\"start\":62699},{\"end\":62712,\"start\":62708},{\"end\":62720,\"start\":62716},{\"end\":62728,\"start\":62724},{\"end\":62736,\"start\":62732},{\"end\":62744,\"start\":62740},{\"end\":62751,\"start\":62748},{\"end\":62757,\"start\":62755},{\"end\":63055,\"start\":63048},{\"end\":63062,\"start\":63059},{\"end\":63073,\"start\":63066},{\"end\":63084,\"start\":63077},{\"end\":63093,\"start\":63088},{\"end\":63102,\"start\":63097},{\"end\":63113,\"start\":63106},{\"end\":63394,\"start\":63386},{\"end\":63406,\"start\":63398},{\"end\":63418,\"start\":63410},{\"end\":63430,\"start\":63422},{\"end\":63692,\"start\":63683},{\"end\":63700,\"start\":63696},{\"end\":63988,\"start\":63981},{\"end\":63998,\"start\":63992},{\"end\":64008,\"start\":64002},{\"end\":64315,\"start\":64308},{\"end\":64324,\"start\":64319},{\"end\":64334,\"start\":64328},{\"end\":64499,\"start\":64496},{\"end\":64508,\"start\":64503},{\"end\":64518,\"start\":64512},{\"end\":64527,\"start\":64524},{\"end\":64537,\"start\":64531},{\"end\":64548,\"start\":64541},{\"end\":64812,\"start\":64810},{\"end\":64823,\"start\":64816},{\"end\":65025,\"start\":65022},{\"end\":65037,\"start\":65032},{\"end\":65049,\"start\":65044},{\"end\":65060,\"start\":65056},{\"end\":65070,\"start\":65067},{\"end\":65301,\"start\":65295},{\"end\":65316,\"start\":65305},{\"end\":65328,\"start\":65320},{\"end\":65340,\"start\":65336},{\"end\":65349,\"start\":65344},{\"end\":65729,\"start\":65721},{\"end\":65739,\"start\":65733},{\"end\":65752,\"start\":65745},{\"end\":66073,\"start\":66063},{\"end\":66085,\"start\":66077},{\"end\":66099,\"start\":66091},{\"end\":66107,\"start\":66103},{\"end\":66120,\"start\":66111},{\"end\":66393,\"start\":66387},{\"end\":66402,\"start\":66397},{\"end\":66411,\"start\":66406},{\"end\":66422,\"start\":66415},{\"end\":66435,\"start\":66426},{\"end\":66447,\"start\":66439},{\"end\":66457,\"start\":66451},{\"end\":66465,\"start\":66461},{\"end\":66476,\"start\":66469}]", "bib_entry": "[{\"attributes\":{\"doi\":\"top-3: drugstore (0.120) top-4: department store (0.087) top-5: pharmacy (0.052\",\"id\":\"b0\"},\"end\":54429,\"start\":52622},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1849990},\"end\":54791,\"start\":54431},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8217340},\"end\":55109,\"start\":54793},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":195908774},\"end\":55444,\"start\":55111},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1915014},\"end\":55604,\"start\":55446},{\"attributes\":{\"id\":\"b5\"},\"end\":55797,\"start\":55606},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":662187},\"end\":55952,\"start\":55799},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13111582},\"end\":56180,\"start\":55954},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":515925},\"end\":56621,\"start\":56182},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14542261},\"end\":56890,\"start\":56623},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13740328},\"end\":57182,\"start\":56892},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57246310},\"end\":57442,\"start\":57184},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2930547},\"end\":57840,\"start\":57444},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2421251},\"end\":58133,\"start\":57842},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":11664336},\"end\":58425,\"start\":58135},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7910040},\"end\":58572,\"start\":58427},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1309931},\"end\":58848,\"start\":58574},{\"attributes\":{\"id\":\"b17\"},\"end\":59246,\"start\":58850},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14113767},\"end\":59634,\"start\":59248},{\"attributes\":{\"id\":\"b19\"},\"end\":60051,\"start\":59636},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5636055},\"end\":60288,\"start\":60053},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1671874},\"end\":60493,\"start\":60290},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7849920},\"end\":60715,\"start\":60495},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2777306},\"end\":60869,\"start\":60717},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17281169},\"end\":61033,\"start\":60871},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15311271},\"end\":61138,\"start\":61035},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206592484},\"end\":61438,\"start\":61140},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14124313},\"end\":61746,\"start\":61440},{\"attributes\":{\"id\":\"b28\"},\"end\":61958,\"start\":61748},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206594692},\"end\":62153,\"start\":61960},{\"attributes\":{\"id\":\"b30\"},\"end\":62463,\"start\":62155},{\"attributes\":{\"id\":\"b31\"},\"end\":62648,\"start\":62465},{\"attributes\":{\"id\":\"b32\"},\"end\":62965,\"start\":62650},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6161478},\"end\":63312,\"start\":62967},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6383532},\"end\":63596,\"start\":63314},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8724974},\"end\":63849,\"start\":63598},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2156851},\"end\":64267,\"start\":63851},{\"attributes\":{\"id\":\"b37\"},\"end\":64417,\"start\":64269},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7455708},\"end\":64730,\"start\":64419},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6909858},\"end\":64961,\"start\":64732},{\"attributes\":{\"id\":\"b40\"},\"end\":65197,\"start\":64963},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5284428},\"end\":65628,\"start\":65199},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":7487588},\"end\":66009,\"start\":65630},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4246903},\"end\":66320,\"start\":66011},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":502946},\"end\":66684,\"start\":66322},{\"attributes\":{\"doi\":\"top-3: drugstore (0.120) top-4: department store (0.087) top-5: pharmacy (0.052\",\"id\":\"b0\"},\"end\":54429,\"start\":52622},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1849990},\"end\":54791,\"start\":54431},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8217340},\"end\":55109,\"start\":54793},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":195908774},\"end\":55444,\"start\":55111},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1915014},\"end\":55604,\"start\":55446},{\"attributes\":{\"id\":\"b5\"},\"end\":55797,\"start\":55606},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":662187},\"end\":55952,\"start\":55799},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13111582},\"end\":56180,\"start\":55954},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":515925},\"end\":56621,\"start\":56182},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14542261},\"end\":56890,\"start\":56623},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13740328},\"end\":57182,\"start\":56892},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57246310},\"end\":57442,\"start\":57184},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2930547},\"end\":57840,\"start\":57444},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2421251},\"end\":58133,\"start\":57842},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":11664336},\"end\":58425,\"start\":58135},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7910040},\"end\":58572,\"start\":58427},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1309931},\"end\":58848,\"start\":58574},{\"attributes\":{\"id\":\"b17\"},\"end\":59246,\"start\":58850},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14113767},\"end\":59634,\"start\":59248},{\"attributes\":{\"id\":\"b19\"},\"end\":60051,\"start\":59636},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5636055},\"end\":60288,\"start\":60053},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1671874},\"end\":60493,\"start\":60290},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7849920},\"end\":60715,\"start\":60495},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2777306},\"end\":60869,\"start\":60717},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17281169},\"end\":61033,\"start\":60871},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15311271},\"end\":61138,\"start\":61035},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206592484},\"end\":61438,\"start\":61140},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14124313},\"end\":61746,\"start\":61440},{\"attributes\":{\"id\":\"b28\"},\"end\":61958,\"start\":61748},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206594692},\"end\":62153,\"start\":61960},{\"attributes\":{\"id\":\"b30\"},\"end\":62463,\"start\":62155},{\"attributes\":{\"id\":\"b31\"},\"end\":62648,\"start\":62465},{\"attributes\":{\"id\":\"b32\"},\"end\":62965,\"start\":62650},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6161478},\"end\":63312,\"start\":62967},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6383532},\"end\":63596,\"start\":63314},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8724974},\"end\":63849,\"start\":63598},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2156851},\"end\":64267,\"start\":63851},{\"attributes\":{\"id\":\"b37\"},\"end\":64417,\"start\":64269},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7455708},\"end\":64730,\"start\":64419},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6909858},\"end\":64961,\"start\":64732},{\"attributes\":{\"id\":\"b40\"},\"end\":65197,\"start\":64963},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5284428},\"end\":65628,\"start\":65199},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":7487588},\"end\":66009,\"start\":65630},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4246903},\"end\":66320,\"start\":66011},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":502946},\"end\":66684,\"start\":66322}]", "bib_title": "[{\"end\":52748,\"start\":52622},{\"end\":54497,\"start\":54431},{\"end\":54835,\"start\":54793},{\"end\":55174,\"start\":55111},{\"end\":55468,\"start\":55446},{\"end\":55808,\"start\":55799},{\"end\":55978,\"start\":55954},{\"end\":56248,\"start\":56182},{\"end\":56678,\"start\":56623},{\"end\":56983,\"start\":56892},{\"end\":57235,\"start\":57184},{\"end\":57493,\"start\":57444},{\"end\":57932,\"start\":57842},{\"end\":58217,\"start\":58135},{\"end\":58452,\"start\":58427},{\"end\":58635,\"start\":58574},{\"end\":59289,\"start\":59248},{\"end\":60088,\"start\":60053},{\"end\":60329,\"start\":60290},{\"end\":60536,\"start\":60495},{\"end\":60746,\"start\":60717},{\"end\":60904,\"start\":60871},{\"end\":61059,\"start\":61035},{\"end\":61170,\"start\":61140},{\"end\":61506,\"start\":61440},{\"end\":62004,\"start\":61960},{\"end\":63044,\"start\":62967},{\"end\":63380,\"start\":63314},{\"end\":63679,\"start\":63598},{\"end\":63977,\"start\":63851},{\"end\":64492,\"start\":64419},{\"end\":64803,\"start\":64732},{\"end\":65291,\"start\":65199},{\"end\":65717,\"start\":65630},{\"end\":66059,\"start\":66011},{\"end\":66383,\"start\":66322},{\"end\":52748,\"start\":52622},{\"end\":54497,\"start\":54431},{\"end\":54835,\"start\":54793},{\"end\":55174,\"start\":55111},{\"end\":55468,\"start\":55446},{\"end\":55808,\"start\":55799},{\"end\":55978,\"start\":55954},{\"end\":56248,\"start\":56182},{\"end\":56678,\"start\":56623},{\"end\":56983,\"start\":56892},{\"end\":57235,\"start\":57184},{\"end\":57493,\"start\":57444},{\"end\":57932,\"start\":57842},{\"end\":58217,\"start\":58135},{\"end\":58452,\"start\":58427},{\"end\":58635,\"start\":58574},{\"end\":59289,\"start\":59248},{\"end\":60088,\"start\":60053},{\"end\":60329,\"start\":60290},{\"end\":60536,\"start\":60495},{\"end\":60746,\"start\":60717},{\"end\":60904,\"start\":60871},{\"end\":61059,\"start\":61035},{\"end\":61170,\"start\":61140},{\"end\":61506,\"start\":61440},{\"end\":62004,\"start\":61960},{\"end\":63044,\"start\":62967},{\"end\":63380,\"start\":63314},{\"end\":63679,\"start\":63598},{\"end\":63977,\"start\":63851},{\"end\":64492,\"start\":64419},{\"end\":64803,\"start\":64732},{\"end\":65291,\"start\":65199},{\"end\":65717,\"start\":65630},{\"end\":66059,\"start\":66011},{\"end\":66383,\"start\":66322}]", "bib_author": "[{\"end\":54507,\"start\":54499},{\"end\":54520,\"start\":54507},{\"end\":54528,\"start\":54520},{\"end\":54540,\"start\":54528},{\"end\":54549,\"start\":54540},{\"end\":54845,\"start\":54837},{\"end\":54855,\"start\":54845},{\"end\":54868,\"start\":54855},{\"end\":54877,\"start\":54868},{\"end\":54889,\"start\":54877},{\"end\":55190,\"start\":55176},{\"end\":55203,\"start\":55190},{\"end\":55215,\"start\":55203},{\"end\":55484,\"start\":55470},{\"end\":55499,\"start\":55484},{\"end\":55675,\"start\":55662},{\"end\":55686,\"start\":55675},{\"end\":55822,\"start\":55810},{\"end\":55833,\"start\":55822},{\"end\":55843,\"start\":55833},{\"end\":55992,\"start\":55980},{\"end\":56001,\"start\":55992},{\"end\":56011,\"start\":56001},{\"end\":56021,\"start\":56011},{\"end\":56034,\"start\":56021},{\"end\":56260,\"start\":56250},{\"end\":56269,\"start\":56260},{\"end\":56283,\"start\":56269},{\"end\":56291,\"start\":56283},{\"end\":56300,\"start\":56291},{\"end\":56321,\"start\":56300},{\"end\":56338,\"start\":56321},{\"end\":56352,\"start\":56338},{\"end\":56370,\"start\":56352},{\"end\":56381,\"start\":56370},{\"end\":56689,\"start\":56680},{\"end\":56699,\"start\":56689},{\"end\":56709,\"start\":56699},{\"end\":56720,\"start\":56709},{\"end\":56991,\"start\":56985},{\"end\":57000,\"start\":56991},{\"end\":57007,\"start\":57000},{\"end\":57014,\"start\":57007},{\"end\":57245,\"start\":57237},{\"end\":57253,\"start\":57245},{\"end\":57263,\"start\":57253},{\"end\":57272,\"start\":57263},{\"end\":57278,\"start\":57272},{\"end\":57289,\"start\":57278},{\"end\":57510,\"start\":57495},{\"end\":57518,\"start\":57510},{\"end\":57524,\"start\":57518},{\"end\":57534,\"start\":57524},{\"end\":57546,\"start\":57534},{\"end\":57552,\"start\":57546},{\"end\":57561,\"start\":57552},{\"end\":57573,\"start\":57561},{\"end\":57583,\"start\":57573},{\"end\":57596,\"start\":57583},{\"end\":57946,\"start\":57934},{\"end\":57956,\"start\":57946},{\"end\":57965,\"start\":57956},{\"end\":58228,\"start\":58219},{\"end\":58240,\"start\":58228},{\"end\":58466,\"start\":58454},{\"end\":58478,\"start\":58466},{\"end\":58645,\"start\":58637},{\"end\":58653,\"start\":58645},{\"end\":58666,\"start\":58653},{\"end\":58675,\"start\":58666},{\"end\":58687,\"start\":58675},{\"end\":58931,\"start\":58917},{\"end\":58944,\"start\":58931},{\"end\":58958,\"start\":58944},{\"end\":58970,\"start\":58958},{\"end\":58979,\"start\":58970},{\"end\":58991,\"start\":58979},{\"end\":59003,\"start\":58991},{\"end\":59012,\"start\":59003},{\"end\":59025,\"start\":59012},{\"end\":59034,\"start\":59025},{\"end\":59301,\"start\":59291},{\"end\":59310,\"start\":59301},{\"end\":59322,\"start\":59310},{\"end\":59330,\"start\":59322},{\"end\":59340,\"start\":59330},{\"end\":59351,\"start\":59340},{\"end\":59361,\"start\":59351},{\"end\":59374,\"start\":59361},{\"end\":59737,\"start\":59726},{\"end\":59744,\"start\":59737},{\"end\":59753,\"start\":59744},{\"end\":59764,\"start\":59753},{\"end\":59772,\"start\":59764},{\"end\":59783,\"start\":59772},{\"end\":59791,\"start\":59783},{\"end\":59805,\"start\":59791},{\"end\":59814,\"start\":59805},{\"end\":59826,\"start\":59814},{\"end\":60098,\"start\":60090},{\"end\":60106,\"start\":60098},{\"end\":60114,\"start\":60106},{\"end\":60124,\"start\":60114},{\"end\":60136,\"start\":60124},{\"end\":60148,\"start\":60136},{\"end\":60343,\"start\":60331},{\"end\":60551,\"start\":60538},{\"end\":60562,\"start\":60551},{\"end\":60575,\"start\":60562},{\"end\":60760,\"start\":60748},{\"end\":60771,\"start\":60760},{\"end\":60914,\"start\":60906},{\"end\":60924,\"start\":60914},{\"end\":60936,\"start\":60924},{\"end\":61074,\"start\":61061},{\"end\":61183,\"start\":61172},{\"end\":61190,\"start\":61183},{\"end\":61197,\"start\":61190},{\"end\":61209,\"start\":61197},{\"end\":61217,\"start\":61209},{\"end\":61229,\"start\":61217},{\"end\":61238,\"start\":61229},{\"end\":61251,\"start\":61238},{\"end\":61265,\"start\":61251},{\"end\":61520,\"start\":61508},{\"end\":61533,\"start\":61520},{\"end\":61832,\"start\":61825},{\"end\":62012,\"start\":62006},{\"end\":62021,\"start\":62012},{\"end\":62028,\"start\":62021},{\"end\":62035,\"start\":62028},{\"end\":62218,\"start\":62209},{\"end\":62224,\"start\":62218},{\"end\":62233,\"start\":62224},{\"end\":62240,\"start\":62233},{\"end\":62248,\"start\":62240},{\"end\":62255,\"start\":62248},{\"end\":62261,\"start\":62255},{\"end\":62505,\"start\":62497},{\"end\":62512,\"start\":62505},{\"end\":62519,\"start\":62512},{\"end\":62525,\"start\":62519},{\"end\":62697,\"start\":62689},{\"end\":62706,\"start\":62697},{\"end\":62714,\"start\":62706},{\"end\":62722,\"start\":62714},{\"end\":62730,\"start\":62722},{\"end\":62738,\"start\":62730},{\"end\":62746,\"start\":62738},{\"end\":62753,\"start\":62746},{\"end\":62759,\"start\":62753},{\"end\":63057,\"start\":63046},{\"end\":63064,\"start\":63057},{\"end\":63075,\"start\":63064},{\"end\":63086,\"start\":63075},{\"end\":63095,\"start\":63086},{\"end\":63104,\"start\":63095},{\"end\":63115,\"start\":63104},{\"end\":63396,\"start\":63382},{\"end\":63408,\"start\":63396},{\"end\":63420,\"start\":63408},{\"end\":63432,\"start\":63420},{\"end\":63694,\"start\":63681},{\"end\":63702,\"start\":63694},{\"end\":63990,\"start\":63979},{\"end\":64000,\"start\":63990},{\"end\":64010,\"start\":64000},{\"end\":64317,\"start\":64306},{\"end\":64326,\"start\":64317},{\"end\":64336,\"start\":64326},{\"end\":64501,\"start\":64494},{\"end\":64510,\"start\":64501},{\"end\":64520,\"start\":64510},{\"end\":64529,\"start\":64520},{\"end\":64539,\"start\":64529},{\"end\":64550,\"start\":64539},{\"end\":64814,\"start\":64805},{\"end\":64825,\"start\":64814},{\"end\":65027,\"start\":65017},{\"end\":65039,\"start\":65027},{\"end\":65051,\"start\":65039},{\"end\":65062,\"start\":65051},{\"end\":65072,\"start\":65062},{\"end\":65303,\"start\":65293},{\"end\":65318,\"start\":65303},{\"end\":65330,\"start\":65318},{\"end\":65342,\"start\":65330},{\"end\":65351,\"start\":65342},{\"end\":65731,\"start\":65719},{\"end\":65741,\"start\":65731},{\"end\":65754,\"start\":65741},{\"end\":66075,\"start\":66061},{\"end\":66087,\"start\":66075},{\"end\":66101,\"start\":66087},{\"end\":66109,\"start\":66101},{\"end\":66122,\"start\":66109},{\"end\":66395,\"start\":66385},{\"end\":66404,\"start\":66395},{\"end\":66413,\"start\":66404},{\"end\":66424,\"start\":66413},{\"end\":66437,\"start\":66424},{\"end\":66449,\"start\":66437},{\"end\":66459,\"start\":66449},{\"end\":66467,\"start\":66459},{\"end\":66478,\"start\":66467},{\"end\":54507,\"start\":54499},{\"end\":54520,\"start\":54507},{\"end\":54528,\"start\":54520},{\"end\":54540,\"start\":54528},{\"end\":54549,\"start\":54540},{\"end\":54845,\"start\":54837},{\"end\":54855,\"start\":54845},{\"end\":54868,\"start\":54855},{\"end\":54877,\"start\":54868},{\"end\":54889,\"start\":54877},{\"end\":55190,\"start\":55176},{\"end\":55203,\"start\":55190},{\"end\":55215,\"start\":55203},{\"end\":55484,\"start\":55470},{\"end\":55499,\"start\":55484},{\"end\":55675,\"start\":55662},{\"end\":55686,\"start\":55675},{\"end\":55822,\"start\":55810},{\"end\":55833,\"start\":55822},{\"end\":55843,\"start\":55833},{\"end\":55992,\"start\":55980},{\"end\":56001,\"start\":55992},{\"end\":56011,\"start\":56001},{\"end\":56021,\"start\":56011},{\"end\":56034,\"start\":56021},{\"end\":56260,\"start\":56250},{\"end\":56269,\"start\":56260},{\"end\":56283,\"start\":56269},{\"end\":56291,\"start\":56283},{\"end\":56300,\"start\":56291},{\"end\":56321,\"start\":56300},{\"end\":56338,\"start\":56321},{\"end\":56352,\"start\":56338},{\"end\":56370,\"start\":56352},{\"end\":56381,\"start\":56370},{\"end\":56689,\"start\":56680},{\"end\":56699,\"start\":56689},{\"end\":56709,\"start\":56699},{\"end\":56720,\"start\":56709},{\"end\":56991,\"start\":56985},{\"end\":57000,\"start\":56991},{\"end\":57007,\"start\":57000},{\"end\":57014,\"start\":57007},{\"end\":57245,\"start\":57237},{\"end\":57253,\"start\":57245},{\"end\":57263,\"start\":57253},{\"end\":57272,\"start\":57263},{\"end\":57278,\"start\":57272},{\"end\":57289,\"start\":57278},{\"end\":57510,\"start\":57495},{\"end\":57518,\"start\":57510},{\"end\":57524,\"start\":57518},{\"end\":57534,\"start\":57524},{\"end\":57546,\"start\":57534},{\"end\":57552,\"start\":57546},{\"end\":57561,\"start\":57552},{\"end\":57573,\"start\":57561},{\"end\":57583,\"start\":57573},{\"end\":57596,\"start\":57583},{\"end\":57946,\"start\":57934},{\"end\":57956,\"start\":57946},{\"end\":57965,\"start\":57956},{\"end\":58228,\"start\":58219},{\"end\":58240,\"start\":58228},{\"end\":58466,\"start\":58454},{\"end\":58478,\"start\":58466},{\"end\":58645,\"start\":58637},{\"end\":58653,\"start\":58645},{\"end\":58666,\"start\":58653},{\"end\":58675,\"start\":58666},{\"end\":58687,\"start\":58675},{\"end\":58931,\"start\":58917},{\"end\":58944,\"start\":58931},{\"end\":58958,\"start\":58944},{\"end\":58970,\"start\":58958},{\"end\":58979,\"start\":58970},{\"end\":58991,\"start\":58979},{\"end\":59003,\"start\":58991},{\"end\":59012,\"start\":59003},{\"end\":59025,\"start\":59012},{\"end\":59034,\"start\":59025},{\"end\":59301,\"start\":59291},{\"end\":59310,\"start\":59301},{\"end\":59322,\"start\":59310},{\"end\":59330,\"start\":59322},{\"end\":59340,\"start\":59330},{\"end\":59351,\"start\":59340},{\"end\":59361,\"start\":59351},{\"end\":59374,\"start\":59361},{\"end\":59737,\"start\":59726},{\"end\":59744,\"start\":59737},{\"end\":59753,\"start\":59744},{\"end\":59764,\"start\":59753},{\"end\":59772,\"start\":59764},{\"end\":59783,\"start\":59772},{\"end\":59791,\"start\":59783},{\"end\":59805,\"start\":59791},{\"end\":59814,\"start\":59805},{\"end\":59826,\"start\":59814},{\"end\":60098,\"start\":60090},{\"end\":60106,\"start\":60098},{\"end\":60114,\"start\":60106},{\"end\":60124,\"start\":60114},{\"end\":60136,\"start\":60124},{\"end\":60148,\"start\":60136},{\"end\":60343,\"start\":60331},{\"end\":60551,\"start\":60538},{\"end\":60562,\"start\":60551},{\"end\":60575,\"start\":60562},{\"end\":60760,\"start\":60748},{\"end\":60771,\"start\":60760},{\"end\":60914,\"start\":60906},{\"end\":60924,\"start\":60914},{\"end\":60936,\"start\":60924},{\"end\":61074,\"start\":61061},{\"end\":61183,\"start\":61172},{\"end\":61190,\"start\":61183},{\"end\":61197,\"start\":61190},{\"end\":61209,\"start\":61197},{\"end\":61217,\"start\":61209},{\"end\":61229,\"start\":61217},{\"end\":61238,\"start\":61229},{\"end\":61251,\"start\":61238},{\"end\":61265,\"start\":61251},{\"end\":61520,\"start\":61508},{\"end\":61533,\"start\":61520},{\"end\":61832,\"start\":61825},{\"end\":62012,\"start\":62006},{\"end\":62021,\"start\":62012},{\"end\":62028,\"start\":62021},{\"end\":62035,\"start\":62028},{\"end\":62218,\"start\":62209},{\"end\":62224,\"start\":62218},{\"end\":62233,\"start\":62224},{\"end\":62240,\"start\":62233},{\"end\":62248,\"start\":62240},{\"end\":62255,\"start\":62248},{\"end\":62261,\"start\":62255},{\"end\":62505,\"start\":62497},{\"end\":62512,\"start\":62505},{\"end\":62519,\"start\":62512},{\"end\":62525,\"start\":62519},{\"end\":62697,\"start\":62689},{\"end\":62706,\"start\":62697},{\"end\":62714,\"start\":62706},{\"end\":62722,\"start\":62714},{\"end\":62730,\"start\":62722},{\"end\":62738,\"start\":62730},{\"end\":62746,\"start\":62738},{\"end\":62753,\"start\":62746},{\"end\":62759,\"start\":62753},{\"end\":63057,\"start\":63046},{\"end\":63064,\"start\":63057},{\"end\":63075,\"start\":63064},{\"end\":63086,\"start\":63075},{\"end\":63095,\"start\":63086},{\"end\":63104,\"start\":63095},{\"end\":63115,\"start\":63104},{\"end\":63396,\"start\":63382},{\"end\":63408,\"start\":63396},{\"end\":63420,\"start\":63408},{\"end\":63432,\"start\":63420},{\"end\":63694,\"start\":63681},{\"end\":63702,\"start\":63694},{\"end\":63990,\"start\":63979},{\"end\":64000,\"start\":63990},{\"end\":64010,\"start\":64000},{\"end\":64317,\"start\":64306},{\"end\":64326,\"start\":64317},{\"end\":64336,\"start\":64326},{\"end\":64501,\"start\":64494},{\"end\":64510,\"start\":64501},{\"end\":64520,\"start\":64510},{\"end\":64529,\"start\":64520},{\"end\":64539,\"start\":64529},{\"end\":64550,\"start\":64539},{\"end\":64814,\"start\":64805},{\"end\":64825,\"start\":64814},{\"end\":65027,\"start\":65017},{\"end\":65039,\"start\":65027},{\"end\":65051,\"start\":65039},{\"end\":65062,\"start\":65051},{\"end\":65072,\"start\":65062},{\"end\":65303,\"start\":65293},{\"end\":65318,\"start\":65303},{\"end\":65330,\"start\":65318},{\"end\":65342,\"start\":65330},{\"end\":65351,\"start\":65342},{\"end\":65731,\"start\":65719},{\"end\":65741,\"start\":65731},{\"end\":65754,\"start\":65741},{\"end\":66075,\"start\":66061},{\"end\":66087,\"start\":66075},{\"end\":66101,\"start\":66087},{\"end\":66109,\"start\":66101},{\"end\":66122,\"start\":66109},{\"end\":66395,\"start\":66385},{\"end\":66404,\"start\":66395},{\"end\":66413,\"start\":66404},{\"end\":66424,\"start\":66413},{\"end\":66437,\"start\":66424},{\"end\":66449,\"start\":66437},{\"end\":66459,\"start\":66449},{\"end\":66467,\"start\":66459},{\"end\":66478,\"start\":66467}]", "bib_venue": "[{\"end\":52868,\"start\":52829},{\"end\":54598,\"start\":54549},{\"end\":54941,\"start\":54889},{\"end\":55264,\"start\":55215},{\"end\":55517,\"start\":55499},{\"end\":55660,\"start\":55606},{\"end\":55866,\"start\":55843},{\"end\":56057,\"start\":56034},{\"end\":56387,\"start\":56381},{\"end\":56743,\"start\":56720},{\"end\":57024,\"start\":57014},{\"end\":57299,\"start\":57289},{\"end\":57628,\"start\":57596},{\"end\":57975,\"start\":57965},{\"end\":58272,\"start\":58240},{\"end\":58488,\"start\":58478},{\"end\":58697,\"start\":58687},{\"end\":58915,\"start\":58850},{\"end\":59412,\"start\":59374},{\"end\":59724,\"start\":59636},{\"end\":60158,\"start\":60148},{\"end\":60368,\"start\":60343},{\"end\":60595,\"start\":60575},{\"end\":60781,\"start\":60771},{\"end\":60943,\"start\":60936},{\"end\":61080,\"start\":61074},{\"end\":61275,\"start\":61265},{\"end\":61585,\"start\":61533},{\"end\":61823,\"start\":61748},{\"end\":62045,\"start\":62035},{\"end\":62207,\"start\":62155},{\"end\":62495,\"start\":62465},{\"end\":62687,\"start\":62650},{\"end\":63125,\"start\":63115},{\"end\":63445,\"start\":63432},{\"end\":63712,\"start\":63702},{\"end\":64049,\"start\":64010},{\"end\":64304,\"start\":64269},{\"end\":64560,\"start\":64550},{\"end\":64835,\"start\":64825},{\"end\":65015,\"start\":64963},{\"end\":65400,\"start\":65351},{\"end\":65810,\"start\":65754},{\"end\":66154,\"start\":66122},{\"end\":66488,\"start\":66478},{\"end\":52868,\"start\":52829},{\"end\":54598,\"start\":54549},{\"end\":54941,\"start\":54889},{\"end\":55264,\"start\":55215},{\"end\":55517,\"start\":55499},{\"end\":55660,\"start\":55606},{\"end\":55866,\"start\":55843},{\"end\":56057,\"start\":56034},{\"end\":56387,\"start\":56381},{\"end\":56743,\"start\":56720},{\"end\":57024,\"start\":57014},{\"end\":57299,\"start\":57289},{\"end\":57628,\"start\":57596},{\"end\":57975,\"start\":57965},{\"end\":58272,\"start\":58240},{\"end\":58488,\"start\":58478},{\"end\":58697,\"start\":58687},{\"end\":58915,\"start\":58850},{\"end\":59412,\"start\":59374},{\"end\":59724,\"start\":59636},{\"end\":60158,\"start\":60148},{\"end\":60368,\"start\":60343},{\"end\":60595,\"start\":60575},{\"end\":60781,\"start\":60771},{\"end\":60943,\"start\":60936},{\"end\":61080,\"start\":61074},{\"end\":61275,\"start\":61265},{\"end\":61585,\"start\":61533},{\"end\":61823,\"start\":61748},{\"end\":62045,\"start\":62035},{\"end\":62207,\"start\":62155},{\"end\":62495,\"start\":62465},{\"end\":62687,\"start\":62650},{\"end\":63125,\"start\":63115},{\"end\":63445,\"start\":63432},{\"end\":63712,\"start\":63702},{\"end\":64049,\"start\":64010},{\"end\":64304,\"start\":64269},{\"end\":64560,\"start\":64550},{\"end\":64835,\"start\":64825},{\"end\":65015,\"start\":64963},{\"end\":65400,\"start\":65351},{\"end\":65810,\"start\":65754},{\"end\":66154,\"start\":66122},{\"end\":66488,\"start\":66478},{\"end\":56753,\"start\":56745},{\"end\":57030,\"start\":57026},{\"end\":57305,\"start\":57301},{\"end\":57981,\"start\":57977},{\"end\":58494,\"start\":58490},{\"end\":58703,\"start\":58699},{\"end\":60164,\"start\":60160},{\"end\":60787,\"start\":60783},{\"end\":61281,\"start\":61277},{\"end\":62051,\"start\":62047},{\"end\":63131,\"start\":63127},{\"end\":63718,\"start\":63714},{\"end\":64566,\"start\":64562},{\"end\":64841,\"start\":64837},{\"end\":66494,\"start\":66490},{\"end\":56753,\"start\":56745},{\"end\":57030,\"start\":57026},{\"end\":57305,\"start\":57301},{\"end\":57981,\"start\":57977},{\"end\":58494,\"start\":58490},{\"end\":58703,\"start\":58699},{\"end\":60164,\"start\":60160},{\"end\":60787,\"start\":60783},{\"end\":61281,\"start\":61277},{\"end\":62051,\"start\":62047},{\"end\":63131,\"start\":63127},{\"end\":63718,\"start\":63714},{\"end\":64566,\"start\":64562},{\"end\":64841,\"start\":64837},{\"end\":66494,\"start\":66490}]"}}}, "year": 2023, "month": 12, "day": 17}