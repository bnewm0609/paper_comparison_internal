{"id": 85517269, "updated": "2023-11-08 16:08:32.857", "metadata": {"title": "Attention Based Glaucoma Detection: A Large-scale Database and CNN Model", "authors": "[{\"first\":\"Liu\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Mai\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Xiaofei\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lai\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Hanruo\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 3, "day": 26}, "abstract": "Recently, the attention mechanism has been successfully applied in convolutional neural networks (CNNs), significantly boosting the performance of many computer vision tasks. Unfortunately, few medical image recognition approaches incorporate the attention mechanism in the CNNs. In particular, there exists high redundancy in fundus images for glaucoma detection, such that the attention mechanism has potential in improving the performance of CNN-based glaucoma detection. This paper proposes an attention-based CNN for glaucoma detection (AG-CNN). Specifically, we first establish a large-scale attention based glaucoma (LAG) database, which includes 5,824 fundus images labeled with either positive glaucoma (2,392) or negative glaucoma (3,432). The attention maps of the ophthalmologists are also collected in LAG database through a simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed, including an attention prediction subnet, a pathological area localization subnet and a glaucoma classification subnet. Different from other attention-based CNN methods, the features are also visualized as the localized pathological area, which can advance the performance of glaucoma detection. Finally, the experiment results show that the proposed AG-CNN approach significantly advances state-of-the-art glaucoma detection.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.10831", "mag": "2963142663", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiXWJL19", "doi": "10.1109/cvpr.2019.01082"}}, "content": {"source": {"pdf_hash": "8318e993cba35791d402d7d3d612002dc562a446", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.10831v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1903.10831", "status": "GREEN"}}, "grobid": {"id": "d7670d8b366e7c71d592e0cd4a480746bf19b417", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8318e993cba35791d402d7d3d612002dc562a446.txt", "contents": "\nAttention Based Glaucoma Detection: A Large-scale Database and CNN Model\n\n\nLiu Li \nSchool of Electronic and Information Engineering\nBeihang University\nBeijingChina\n\nMai Xu maixu@buaa.edu.cn \nSchool of Electronic and Information Engineering\nBeihang University\nBeijingChina\n\n\u00a7Beijing Institute of Ophthalmology\nHangzhou Innovation InstituteBeihang University\nHangzhouZhejiangChina\n\nBeijing Tongren Hospital\nBeijingChina\n\nXiaofei Wang xfwang@buaa.edu.cn \nSchool of Electronic and Information Engineering\nBeihang University\nBeijingChina\n\nLai Jiang \nSchool of Electronic and Information Engineering\nBeihang University\nBeijingChina\n\nHanruo Liu \nAttention Based Glaucoma Detection: A Large-scale Database and CNN Model\n\nRecently, the attention mechanism has been successfully applied in convolutional neural networks (CNNs), significantly boosting the performance of many computer vision tasks. Unfortunately, few medical image recognition approaches incorporate the attention mechanism in the CNNs. In particular, there exists high redundancy in fundus images for glaucoma detection, such that the attention mechanism has potential in improving the performance of CNN-based glaucoma detection. This paper proposes an attentionbased CNN for glaucoma detection (AG-CNN). Specifically, we first establish a large-scale attention based glaucoma (LAG) database, which includes 5,824 fundus images labeled with either positive glaucoma (2,392) or negative glaucoma (3,432). The attention maps of the ophthalmologists are also collected in LAG database through a simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed, including an attention prediction subnet, a pathological area localization subnet and a glaucoma classification subnet. Different from other attention-based CNN methods, the features are also visualized as the localized pathological area, which can advance the performance of glaucoma detection. Finally, the experiment results show that the proposed AG-CNN approach significantly advances state-of-the-art glaucoma detection.\n\nIntroduction\n\nIn recently years, the attention mechanism has been successfully applied in deep learning based computer vision tasks, i.e., object detection [3,31,28], image caption [35,39,2] and action recognition [30]. The basic idea of the attention mechanism is to locate the most salient parts of the features in deep neural networks (DNNs), such that redundancy is removed for the vision tasks. In general, the * Mai Xu is the corresponding author of this paper.  [15] by an occlusion experiment [40]. The Pearson Correlation Coefficient (CC) results between the visualized heat maps and ground-truth ophthalmologist attention maps are 0.33 and 0.14 for correct and incorrect glaucoma classification, respectively. attention mechanism is embedded in DNNs by leveraging the attention maps. Specifically, on the one hand, the attention maps in [31,28,35,30] are yielded in a self-learned pattern, with other information weakly supervising the attention maps, i.e., the classification labels. On the other hand, [39,37] utilize the human attention information to guide the DNNs focusing on the region of interest (ROI). Redundancy also exists in medical image recognition, interfering the recognition results. In particular, there exists heavy redundancy in fundus images for disease recognition. For example, the pathological areas of fundus images are in the region of optic cup and disc, or its surrounding blood vessel and optic nerve area [25]; other regions such as the boundary of the eye ball are redundant for the medical diagnosis. As shown in Figure 1, glaucoma, an irreversible optic disease, can be correctly detected by a convolutional neural network (CNN) [15], when the visualized heat maps are consistent with the attention maps of ophthalmologists. Otherwise, glaucoma is mislabeled by the CNN model when the visualized heat maps focus on redundant regions. Therefore, it is reasonable to combine the attention mechanism in the CNN model for using fundus images to detect ophthalmic disease.\n\nHowever, to our best knowledge, there has been no works incorporating the human attention in medical image recognition. This is mainly because there lacks the doctor attention database, which needs the qualified doctors and a special technique of capturing the doctor attention in the diagnosis. As such, in this paper, we first collect a large-scale attention based fundus image database for glaucoma detection (LAG), including 5,824 images with diagnose labels and human attention maps. Based on the real human attention, we propose an attention based CNN method (called AG-CNN) for glaucoma detection based on fundus images.\n\nAlthough human attention is able to reduce heavy redundancy in fundus images for disease recognition, it may also miss some of the pathological area which is helpful for disease detection. As a result, the existing CNN models have outperformed the doctors in medical image recognition [18,27,26]. Thus, we propose to refine the predicted attention maps by incorporating a feature visualization structure for glaucoma detection. As such, the gap between human attention and pathological area can be bridged. In fact, there have been several methods for automatically locating the pathological area [41,12,8,11,24], based on the class activation mapping model (CAM) [42]. However, these methods cannot locate the pathological area at a small region due to the limitation of its feature size. In this paper, we employ the guided back propagation (BP) method [33] to locate the tiny pathological area, based on the predicted attention maps. Consequently, the attention maps can be refined and then used to highlight the most critical regions for glaucoma detection.\n\nThe main contributions of this paper are: (1) We establish a LAG database with 5,824 fundus images, along with their labels and attention maps. (2) We propose incorporating the attention maps in AG-CNN, such that the redundancy can be removed from fundus images for glaucoma detection. (3) We develop a new architecture of AG-CNN, which visualizes the CNN feature maps for locating pathological area and then classifies binary glaucoma.\n\n\nMedical Background\n\nThe recent success of deep learning methods has benefitted medical diagnosis [7,4,38], especially for automatically detecting oculopathy in fundus images [13,10,34]. Specifically, [13,10] worked on classification of diabetic retinopathy using the CNN models. [34] further proposed deep learning systems for multi-ophthalmological diseases detection. However, the above works all transfered some classic CNN model for nature image classification to medical image classification, regardless of the characteristic of fundus images.\n\nGlaucoma detection methods can be basically divided into 2 categories, i.e., heuristic methods and deep learning methods. The heuristic glaucoma detection methods extract features based on some image processing techniques [1,6,17,32]. Specifically, [1] extracted the texture features and higher order spectra features for glaucoma detection. [6] used the wavelet-based energy features for glaucoma detection. Both [1,6] applied support vector machine (SVM)and naive Bayesian classifier to classify the handcrafted features. However, the above heuristic methods only consider a handful of features on fundus images, leading to lower classification accuracy.\n\nAnother category of glaucoma detection methods is based on deep learning [29,43,5,22,23]. Specifically, [29,43] reported their deep learning work on glaucoma detection based on automatic segmentation of optic cup and disc. However, their work assume that only the optical cup and disc are related to glaucoma, lacking end-to-end training. On the other hand, [5] firstly proposed a CNN method for glaucoma detection in an end-to-end mannar. [22] followed Chen's work and proposed an advanced CNN structure combining the holistic and local features for glaucoma classification. To regularize the input images, both [5,22] preprocessed the original fundus images to remove the redundant regions. However, due to the limited training data and simple structure of networks, the previous works did not achieve high sensitivity and specificity. Most recently, a deeper CNN structure has been proposed in [23]. However, the fundus images exist large redundancy irrelevant to glaucoma detection, leading to the low efficiency for [23].\n\n\nDatabase\n\n\nEstablishment\n\nIn this work, we establish a large-scale attention based glaucoma detection database. Our LAG database contains 5,824 fundus images with 2,392 positive and 3,432 negative glaucoma samples obtained from Beijing Tongren Hospital 1 . Our work is conducted according to the tenets of Helsinki Declaration. As the retrospective nature and fully anonymized usage of color retinal fundus images, we are exempted by the medical ethics committee to inform the patients. Each fundus image is diagnosed by qualified glaucoma specialists, taking the consideration of both morphologic and functional analysis, i.e, intra-ocular pressure, visual field loss and manual optic disc assessment. As a result, the binary labels of positive or negative glaucoma of all fundus images are confirmed, seen as the gold standard.\n\nBased on the above labelled fundus images, we further conduct an experiment to capture the attention regions of the ophthalmologists in glaucoma diagnosis. The experiment is based on an alternative method for eye tracking [19],  in which mouse clicks are used by the ophthalmologists to explore ROI for glaucoma diagnosis. Specifically, all the fundus images are initially displayed blurred, and then the ophthalmologists use the mouse as an eraser to successively clear the circle regions for diagnosing glaucoma. Note that the radius of all circle regions is set to 40 pixels, while all fundus images are with 500 \u00d7 500 pixels. This ensures that the circle regions are approximately equivalent to the fovea (2 \u2022 \u22123 \u2022 ) of the human vision system at a comfortable viewing distance (3-4 times of screen height). The order of clearing the blurred regions represents the degree of attention by ophthalmologists, as the GT of the attention map. Once the ophthalmologist is able to diagnose glaucoma with the partly cleared fundus image, the above region clearing process is terminated and the next fundus image is displayed for diagnosis. In the above experiment, the fixations of ophthalmologists are represented by the center coordinate (x j i , y j i ) of the cleared circle region for the i-th fixation of the j-th ophthalmologist. Then, the attention map A of one fundus image can be generated by convoluting all fixations\n{(x j i , y j i )} Ij ,J i=1,j=1\nwith the 2D Gaussian filter at square decay according to the order of i, where J is the total number of ophthalmologists (=4 in our experiment) and I j is the number of fixations from the j-th ophthalmologist on the fundus image. Here, the standard deviation of the Gaussian filter is set to 25, according to [36]. Figure 2 shows an example of the fixations of one ophthalmologist and attention map of all ophthalmologists for a fundus image.\n\n\nData analysis\n\nNow, we mine our LAG database to investigate the attention maps of all fundus images in glaucoma diagnosis. Specifically, we have the following findings. Finding 1: The ROI in fundus images is consistent across ophthalmologists for glaucoma diagnosis.\n\nAnalysis: In this analysis, we calculate the Pearson correlation coefficients (CC) of attention maps between one ophthalmologist and the remaining three ophthalmologists. Table 1 reports the CC results averaged over all fundus images in our LAG database. In this table, we also show the CC results of attention maps between one ophthalmologist and the random baseline. Note that the random baseline generates the attention maps by making their values follow the Gaussian distribution. We can see from Table 1 that the CC values of attention maps between one and the remaining ophthalmologists are all above 0.55, significantly larger than those of the random baseline. This implies that attention exists consistency among ophthalmologists in glaucoma diagnosis. This completes the analysis of Finding 2.\n\nFinding 2: The ROI in fundus images concentrates on small regions for glaucoma diagnosis.\n\nAnalysis: In this analysis, we calculate the percentage of regions that ophthalmologists cleared for glaucoma diagnosis. Figure 3 (Left) shows the percentage of the cleared circle regions for each ophthalmologist, which is averaged over all 5,824 fundus images of our LAG database. We can see that the average ROI accounts for 14.3% of the total area in the fundus images, with a maximum of 17.8% (the 3 rd ophthalmologist) and a minimum of 11.8% (the 4 th ophthalmologist). Moreover, we calculate the proportion of regions in attention maps, the values of which are above a varying threshold. The result is shown in Figure 3 (Right). The fast decreasing curve shows that most attention only focuses on small regions of fundus images for glaucoma diagnosis. This completes the analysis of Finding 2.\n\nFinding 3: The ROI for glaucoma diagnosis is of different scales.\n\nAnalysis: Finding 2 shows that the ROI is small for glaucoma diagnosis, comparing with the whole fundus images.\n\nHere, although ROI is small, its scale is various across all the fundus images. Figure 4 visualizes the fixation maps of some fundus images, in which the ROI are with different scales. As shown in Figure 4, the sizes of the optic discs for pathological myopia are considerably larger than others. As such, we use myopia and non-myopia to select samples with various scales of ROI (large or small optic cups). We further find that the images of both positive and negative glaucoma have various-scaled ROI, as demonstrated in Figure 4. For each image in our LAG database, Figure 5 further plots the proportion of the ROI in the fixation maps, the values of which are larger than a threshold. We can see that the ROI is at different scale for glaucoma diagnosis. Finally, the analysis of Finding 3 can be accomplished.\n\n\nMethod\n\n\nFramework\n\nIn this section, we discuss the proposed AG-CNN method. Since Findings 1 and 2 show that glaucoma diagnosis is highly related to small ROI regions, the attention prediction subnet is developed in AG-CNN for reducing the redundancy of fundus images. In addition, we design a pathological area localization subnet, which is achieved by visualizing the CNN feature map, based on ROI regions of the attention prediction subnet. Based on the pathological area, the glaucoma classification subnet is developed for producing the binary labels of glaucoma, in which the multi-scale features are learned and extracted. The introduction of multi-scale features is according to Finding 3.\n\nThe framework of AG-CNN is shown in Figure 6, and its components, including multi-scale building block, deconvolutional module and feature normalization, are further demonstrated in Figure 7. As shown in Figure 6, the input to AG-CNN is the RGB channels of a fundus image, while the output is (1) the located pathological area and (2) the binary glaucoma label. In addition, the located pathological area is obtained in our AG-CNN in two 2 stages. In the first stage, the ROI of glaucoma detection is learned from the attention prediction subnet, aiming to predict human attention on diagnosing glaucoma. In the second stage, the predicted attention map is embedded in the pathological area localization subnet, and then the feature map of this subnet is visualized to locate the pathological area. Finally, the lo- cated pathological area is further used to to mask the input and features of the glaucoma classification subnet, for outputting the binary labels of glaucoma.\n\nThe main structure of AG-CNN is based on residual networks [15], in which the basic module is building block. Note that all convolutional layers in AG-CNN are followed by a batch normalization layer and a ReLU layer for increasing the nonlinearity of AG-CNN, such that the convergence rate can be sped up. The process of training AG-CNN is in an end-to-end manner with three parts of supervision, i.e., attention prediction loss, pathological area localization loss and glaucoma classification loss.\n\n\nAttention prediction subnet\n\nIn AG-CNN, an attention prediction subnet is designed to generate the attention maps of the fundus images, which are then used for pathological area localization and glaucoma detection. Specifically, the input of the attention prediction subnet is the RGB channels of a fundus image, which is represented by the tensor (size: 224 \u00d7 224 \u00d7 3 ). Then, the input tensor is fed to one convolutional layer with kernel size of 7 \u00d7 7, followed by one max-pooling layer. Subsequently, the features flow into 8 building blocks for extracting the hierarchical features. For more details about the building blocks, refer to [15]. Afterwards, the features of 4 hieratical building blocks are processed by feature normalization (FN), the structure of which is shown in Figure  7 (Right). As a result, four 28 \u00d7 28 \u00d7 128 features are obtained. They are concatenated to form 28 \u00d7 28 \u00d7 512 deep multi-scale features. Given the deep multi-scale features, a deconvolutional module is applied to generate the gray attention map with the size of 112 \u00d7 112 \u00d7 1. The structure of the deconvolutional module is also shown in Figure 7 (middle). As shown in this figure, the deconvolutional module is comprised by 4 convolutional layers and 2 deconvolutional layers. Finally, a 112 \u00d7 112 \u00d7 1 attention map can be yielded, the values of which range from 0 to 1. In AG-CNN, the yielded attention maps are used to weight the input fundus images and the extracted features of the pathological area localization subnet. This is to be discussed in the next  \n\n\nPathological area localization subnet\n\nAfter predicting the attention maps, we further design a pathological area localization subnet to visualize the CNN feature map in glaucoma classification. The predicted attention maps can effectively make the network focus on the salient region with reduced redundancy; however, the network may inevitably miss some potential features useful for glaucoma classification. Moreover, it has been verified that the deep learning methods outperform human in the task of image classification both on nature images [14,21] and medical images [18,27,26]. Therefore, we further design a subnet to visualize the CNN features for finding the pathological area.\n\nSpecifically, the pathological area localization subnet is mainly composed of convolutional layers and fully connected layers. In addition, the predicted attention maps are used to mask the input fundus images and the extracted feature maps at different layers of the pathological area localization subnet. The structure of this subnet is the same as the glaucoma classification subnet, which is to be discussed in section 4.4. Then, the visualization map of pathological area is yielded through guided BP [33] from the output of the fully connection layer to the input RGB channels fundus images. Finally, the visualization map is down-sampled to 112 \u00d7 112 with its values being normalized to 0 \u2212 1, as the output of the pathological area localization subnet.\n\n\nGlaucoma classification subnet\n\nIn addition to the attention prediction subnet and pathological area localization subnet, we design a glaucoma classification subnet for the binary classification of positive or negative glaucoma. Similar to the attention prediction subnet, the glaucoma classification subnet is composed of one 7 \u00d7 7 convolutional layer, one max-pooling layer, 4 multiscale building blocks.\n\nThe multi-scale building blocks differ from the tradi- tional building block of [15] from the following aspect. As shown in Figure 7 (Left), 4 channels of convolutional layers C 1 , C 2 , C 3 and C 4 with different kernel sizes are concatenated to extract multi scale features, comparing with the traditional building block which only has a single convolutional channel. Finally, 2 fully connected layers are applied to output the classification result.\n\nThe main difference between the glaucoma classification subnet and the conventional residual network [15] is that the visualization maps of pathological area weight both the input image and extracted features to focus on the ROI. Assume that the visualization map generated by the pathological area localization subnet isV. Mathematically, the features F in the glaucoma classification subnet can be masked byV as follows,\nF = F (1 \u2212 \u03b8)V \u2295 \u03b8 ,(1)\nwhere \u03b8 (=0.5 in this paper) is a threshold to control the impact of the visualization map. In the above equation, and \u2295 represent the element-wise multiplication and addition. In the glaucoma classification subnet, the input fundus image is masked with the visualization map in the same way. Finally, in our AG-CNN method, the redundant features irrelevant to glaucoma detection can be inhibited and the pathological area can be highlighted. \n\n\nLoss function\n\nIn order to achieve end-to-end training, we supervise the training process of AG-CNN through attention prediction loss (denoted by Loss a ) , feature visualization loss (denoted by Loss f ) and glaucoma classification loss (denoted by Loss c ), as shown in Figure 6. In our LAG database, both the glaucoma label l (\u2208 {0, 1}) and the attention map A (with its elements A i,j \u2208 [0, 1]) are available for each fundus image, seen as the GT in the loss function. We assume thatl (\u2208 {0, 1}) and\u00c2 (with its elements\u00c2 i,j \u2208 [0, 1]) are the predicted glaucoma label and attention map, respectively. Following [16], we utilize the Kullback-Leibler (KL) divergence function as the human-attention loss Loss a . Specifically, the human-attention loss is represented by\nLoss a = 1 I \u00b7 J I i=1 J j=1 A ij log( A i\u0135 A ij ),(2)\nwhere I and J are the length and width of attention maps. Furthermore, the pathological area localization subnet and glaucoma classification subnet are all supervised by the glaucoma label l based on the cross-entropy function, which measures the distance between the predicted labell and its corresponding GT label l. Mathematically, Loss f is calculated as follows,\nLoss c = l log( 1 1 + e \u2212lc ) + (1 \u2212 l) log(1 \u2212 1 1 + e \u2212lc ),(3)\nwherel c represents the predicted label from the glaucoma classification subnet. Similar way is used to calculate Lossf, which replacesl c byl f in 3. Finally, the overall loss is the linear combination of Loss a , Loss f and Loss c :\nLoss = \u03b1 \u00b7 Loss a + \u03b2 \u00b7 Loss f + \u03b3 \u00b7 Loss c ,(4)\nwhere \u03b1, \u03b2 and \u03b3 are hyper-parameters for balancing the trade-off among attention loss, visualization loss and classification loss. At the begining of training AG-CNN, we choose to set \u03b1 \u03b2 = \u03b3 to speed the convergence of attention prediction subnet. Then, we set \u03b1 \u03b2 = \u03b3 to minimize the feature visualization loss and the classification loss, thus realizing the convergence of prediction. Given the loss function of (4), our AG-CNN model can be end-to-end trained for glaucoma detection and pathological location. \n\n\nExperiments and Results\n\n\nSettings\n\nIn this section, the experiment results are presented to validate the performance of our method in glaucoma detection and pathological area localization. In our experiment, the 5,824 fundus images in our LAG database are randomly divided into training (4,792 images), validation (200 images) and test (832 images) sets. To test the generalization ability of our AG-CNN, we further validate the performance of our method on another public database RIM-ONE [9]. Before inputting to AG-CNN, the RGB channels of fundus images are all resized to 224 \u00d7 224. In training AG-CNN, the gray attention maps are downsampled to 112 \u00d7 112 with their values normalized to be 0 \u223c 1. The loss function of (4) for training the AG-CNN model is minimized through the gradient descent algorithm with Adam optimizer [20]. The initial learning rate is 1 \u00d7 10 \u22125 . We first set \u03b1 = 20 and \u03b2 = \u03b3 = 1 in (4) until the loss of the attention prediction subnet converges, and then set \u03b1 = 1 and \u03b2 = \u03b3 = 10 for focusing on the feature visualization loss and glaucoma classification loss. Additionally, batch size is set to be 8.\n\nGiven the trained AG-CNN model, our method is evaluated and compared with two other state-of-the-art glaucoma detection methods [5,23], in terms of different metrics. Specifically, the metrics of sensitivity and specificity are defined as follows,\nSensitivity = TP TP + FN ,(5)Specificity = TN TN + FP ,(6)\nwhere TP, TN, FP and FN are the numbers of the true positive glaucoma, true negative glaucoma, false positive glaucoma and false negative glaucoma, respectively. Based on TP, FP and FN, the F \u03b2 \u2212score is calculated by\nF \u03b2 \u2212score = (1 + \u03b2 2 ) \u00b7 TP (1 + \u03b2 2 ) \u00b7 TP + \u03b2 2 \u00b7 FN + FP .(7)\nIn the above equation, \u03b2 is the hyper-parameter balancing the trade-off between sensitivity and specificity, and it is set to 2 as the sensitivity is more important in medical diagnosis. In addition, receiver operating characteristic curve (ROC) and area under ROC (AUC) are also evaluated for comparing the performance of glaucoma detection. All experiments are conducted on a computer with an Intel(R)  Core(TM) i7-4770 CPU@3.40GHz, 32GB RAM and a single Nvidia GeForce GTX 1080 GPU. Benefiting from the GPU, our method is able to detect glaucoma of 30 fundus images per second, and it is comparable to 83 and 21 fundus images per second for [5] and [23].\n\n\nEvaluation on glaucoma detection\n\nIn this section, we compare the glaucoma detection performance of our AG-CNN method with two other methods [5,23]. Note that the models of other methods are retrained over our LAG database for fair comparison. Table 2 lists the results of accuracy, sensitivity, specificity, F 2 \u2212score and AUC. As seen in Table 2, our AG-CNN method achieves 95.3%, 95.4% and 95.2% in terms of accuracy, sensitivity and specificity, respectively, which are considerably better than other two methods. Then, the F 2 \u2212score of our method is 0.951, while [5] and [23] only have F 2 \u2212scores of 0.894 and 0.901. The above results indicate that our AG-CNN method significantly outperforms other two methods in all metrics.\n\nIn addition, Figure 8 (Left) plots the ROC curves of our and other methods, for visualizing the trade-off between sensitivity and specificity. We can see from this figure that the ROC curve of our method is closer to the upper-left corner, when comparing with other two methods. This means that the sensitivity of our method is always higher than those of [5,23] at the same specificity. We further quantify ROC performance of three methods through AUC. The AUC results are also reported in Table 2. As shown in this table, our method has larger AUC than other two compared methods. In summary, we can conclude that our method performs better in all metrics than [5,23] in glaucoma detection.\n\nTo evaluate the generalization ability, we further compare the performance of glaucoma detection by our method with other 2 methods [5,23] on the RIM-ONE database [9]. To our best knowledge, there is no other public database of fundus images for glaucoma. The results are shown in Table 3 and Figure 8 (Right). As shown in Table 3, all metrics of our AG-CNN method over the RIM-ONE database are above 0.83, despite slightly smaller than the results over our LAG database. The performance of our method is con- siderably better than other two methods (except specificity of [23]). It is worth mentioning that the metric of sensitivity is more important than that of specificity in glaucoma detection, as other indicators, e.g., intra-ocular pressure and the field of vision, can be further used for confirming the diagnosis of glaucoma. This implies that our method has high generalization ability.\n\nMore importantly, Table 3 and Figure 8 (Right) show that our AG-CNN method performs significantly better than other methods especially in terms of sensitivity. In particular, the performance of [23] severely degrades, as incurring the over-fitting issue. In a word, our AG-CNN method performs well in the generalization ability, considerably better than other state-of-the-art methods [5,23].\n\n\nEvaluation on attention prediction and pathological area localization\n\nWe first evaluate the accuracy of the attention model embedded in our AG-CNN model. Figure 9 visualizes the attention maps predicted by our AG-CNN method over the LAG database and RIM-ONE database. We can see from this figure that the predicted attention maps of AG-CNN are close to those of GT, when testing on our LAG database. The CC between the predicted attention maps and the GT is 0.934 on average, with a variance of 0.0032. This implies the attention prediction subnet of AG-CNN is able to predict attention maps with high accuracy. We can further see from Figure 9 that the attention maps locate the salient optic cup and disc for the RIM-ONE database, in which the scales of fundus images are totally different from those of LAG database. Thus, our method is robust to the scales of fundus images in predicting attention maps.\n\nThen, we focus on the performance of pathological area localization. Figure 10 visualizes the located pathological area over the LAG database. Comparing the GT pathological area with our localization results, we can see from areas of optic cup and disc and the region of retinal nerve fiber layer defect, especially for the pathological areas of the upper and lower optic disc edge. Besides, we calculate the CC between the located pathological area and the GT attention maps of ophthalmologists, with an average of 0.581 and a variance of 0.028. This also implies that (1) on one hand, the pathological area localization results are consistent with the attention maps of ophthalmologists; (2) on the other hand, the pathological area cannot be completely covered by the attention maps. Moreover, we also compare our attention based pathological area localization results with a state-of-art method [12], which is based on the CAM model [42]. The results of [12] are shown in the 3 rd row of Figure 10. We can see that it can roughly highlight the ROI but cannot pinpoint the tiny pathological area, e.g., the upper and lower edge of the optic disc boundary. In some cases, [12] highlight the boundary of the eyeball, indicating that the CAM based methods extracted some unuseful features (i.e., redundancy) for classification. Therefore, the pathological area localization in our approach is effective and reliable, especially compared to the CAM based method that does not incorporate human attention.\n\n\nResults of ablation experiments\n\nIn our ablation experiments, we first illustrate the impact of predicted attention maps for located pathological area. To this end, we simply remove the attention prediction subnet, and then compare the pathological localization results with and without predicted attention maps. The results are shown in Figure 10. We can see that the pathological area can be effectively localized by using the attention maps. In contrast, the located pathological area distributes over the whole fundus image, once the attention maps are not incorporated. Therefore, the above results verify the effec- tiveness and necessity of predicting the attention maps for pathological area localization in our AG-CNN approach. Next, we assess the impact of the predicted attention map and the located pathological area on the performance of glaucoma detection. To this end, we simply remove the attention prediction subnet and pathological area localization subnet of AG-CNN, respectively, for classifying the binary labels of glaucoma. The results are shown in Table 4. As seen in this table, the introduction of both the predicted attention map and located pathological area can improve accuracy, sensitivity, specificity and F 2 \u2212score by 4.5%, 4.3%, 4.7% and 4.7%. However, the performance of only embedding the pathological area localization subnet and without the attention prediction subnet is even worse than removing them both. It verifies the necessity of our attention prediction subnet for pathological area localization and glaucoma detection.\n\nHence, the attention prediction subnet and pathological area localization subnet are able to improve the performance of glaucoma detection in AG-CNN. Additionally, we show the effectiveness of the proposed multi-scale block in AG-CNN, via replacing it by the default conventional shortcut connection in residual network [15]. The results are also shown in Table 4. We can see that the multi-scale block can also enhance the performance of glaucoma detection.\n\n\nConclusion\n\nIn this paper, we have proposed a new deep learning method, named AG-CNN, for automatic glaucoma detection and pathological area localization upon fundus images. Our AG-CNN model is composed of the subnets of attention prediction, pathological area localization and glaucoma classification. As such, glaucoma could be detected using the deep features highlighted by the visualized maps of pathological areas, based on the predicted attention maps. For training the AG-CNN model, we established the LAG database with 5,824 fundus images labeled with either positive or negative glaucoma, along with their attention maps on glaucoma detection. The experiment results showed that the predicted attention maps significantly improve the performance of glaucoma detection and pathological area localization in our AG-CNN method, far better than other state-of-the-art methods.\n\n\nAcknowledgement\n\nThis work was supported by BMSTC under Grants Z181100001918035.\n\nFigure 1 .\n1Examples of glaucoma fundus images, attention maps by ophthalmologists in glaucoma diagnosis and visualization results of a CNN model (Bottom)\n\nFigure 2 .\n2An example of capturing fixations of an ophthalmologist in glaucoma diagnosis. (Left): Original blurred fundus images. (Middleleft): Fixations of the ophthalmologist with cleared regions. (Middleright): The order of clearing the blurred regions. Note that the size of the white circles represents the order of fixations. (Right): The generated attention map based on the captured fixations.\n\nFigure 3 .\n3(Left): Proportion of regions in the fundus images cleared by different ophthalmologists for glaucoma diagnosis. (Right): Proportion of regions in attention maps with values being above a varying threshold. Note that the values of the attention maps range from 0 to 1.\n\nFigure 4 .\n4Fundus images with or without glaucoma for both positive and negative pathological myopia.\n\nFigure 5 .\n5Proportion of ROI above the threshold of 0.10, 0.15 and 0.20, for all of the fundus images in LAG database.\n\nFigure 6 .\n6Architecture of our AG-CNN network for glaucoma detection. The sizes of the feature maps and convolutional kernels are shown in this figure.section.\n\nFigure 7 .\n7Components of the AG-CNN architecture.\n\nFigure 8 .\n8Comparison of ROC curves among different methods. (Left): Testing on our LAG testing set. (Right): Testing on RIM-ONE database.\n\nFigure 9 .\n9Attention maps predicted by AG-CNN ramdomly selected from the test dataset. The fundus images are from our LAG (upper) and RIM-ONE (lower) database. Note that the RIM-ONE database has not the GT of the attention map.\n\nFigure 10 .\n10Figure 10 that our AG-CNN model can accurately located the Comparison of pathological area localization results for glaucoma detection. (1 st row): The pathological areas located by ophthalmologists. Optic cup and disc are labeled in blue and the regions of retinal nerve fiber layer defect are labeled in green. (2 nd row): The result of our method. (3 rd row): The result of CAM based method. (4 th row): The result of ablation experiment.\n\nTable 1 .\n1CC values of attention maps between one ophthalmologist and the mean of the rest ophthalmologists.Ophthalmologist \none v.s. others \none v.s. random \n\n1 st \n0.594 \n6.59 \u00d7 10 \u22124 \n2 nd \n0.636 \n2.49 \u00d7 10 \u22124 \n3 rd \n0.687 \n2.49 \u00d7 10 \u22124 \n4 th \n0.585 \n8.44 \u00d7 10 \u22124 \n\n\n\nTable 2 .\n2Performance of three methods for glaucoma detection over the test set of our LAG database.Method Accuracy Sensitivity Specificity AUC F2\u2212score \n\nOurs \n95.3% \n95.4% \n95.2% \n0.975 \n0.951 \nChen et al. 89.2% \n90.6% \n88.2% \n0.956 \n0.894 \nLi et al. \n89.7% \n91.4% \n88.4% \n0.960 \n0.901 \n\n\n\nTable 3 .\n3Performance of three methods for glaucoma detection over the RIM-ONE database.Method Accuracy Sensitivity Specificity AUC F2\u2212score \n\nOurs \n85.2% \n84.8% \n85.5% \n0.916 \n0.837 \nChen et al. 80.0% \n69.6% \n87.0% \n0.831 \n0.711 \nLi et al. \n66.1% \n71.7% \n62.3% \n0.681 \n0.679 \n\n\n\nTable 4 .\n4Ablation results over the test set of our LAG database. APS represents the attention prediction subnet. PAL represents the pathological area localization subnet. Accuracy Sensitivity Specificity AUC F2\u2212scoreMethod \nFull AG-CNN \n95.3% \n95.4% \n95.2% \n0.975 \n0.951 \nW APS W/O PAL \n94.0% \n94.0% \n94.0% \n0.973 \n0.936 \nW/O APS W PAL \n87.1% \n87.7% \n86.7% \n0.941 \n0.867 \nW/O APS W/O PAL \n90.8% \n91.1% \n90.5% \n0.966 \n0.904 \nW/O multi-scale block 92.2% \n92.0% \n92.3% \n0.974 \n0.915 \n\n\nThe database is available for online access upon request.\n\nAutomated diagnosis of glaucoma using texture and higher order spectra features. U Acharya, S Dua, Xian Du, Chua Kuang Vinitha Sree, Chua, IEEE Transactions on Information Technology in Biomedicine. 153U. R Acharya, S Dua, Xian Du, S Vinitha Sree, and Chua Kuang Chua. Automated diagnosis of glaucoma using texture and higher order spectra features. IEEE Transactions on Information Technology in Biomedicine, 15(3):449-455, 2011.\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, 2018.\n\nMultiple object recognition with visual attention. Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu, arXiv:1412.7755arXiv preprintJimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Mul- tiple object recognition with visual attention. arXiv preprint arXiv:1412.7755, 2015.\n\nJing Qin, and Pheng Ann Heng. Mitosis detection in breast cancer histology images via deep cascaded networks. Qi Hao Chen, Xi Dou, Wang, The AAAI Conference on Artificial Intelligence. Hao Chen, Qi Dou, Xi Wang, Jing Qin, and Pheng Ann Heng. Mitosis detection in breast cancer histology images via deep cascaded networks. In The AAAI Conference on Artificial Intelligence, pages 1160-1166, 2016.\n\nGlaucoma detection based on deep convolutional neural network. Xiangyu Chen, Yanwu Xu, Damon Wing Kee Wong, Tien Yin Wong, Jiang Liu, 37th Annual International Conference of the IEEE. 715Xiangyu Chen, Yanwu Xu, Damon Wing Kee Wong, Tien Yin Wong, and Jiang Liu. Glaucoma detection based on deep convolutional neural network. In Engineering in Medicine and Biology Society (EMBC), 37th Annual Inter- national Conference of the IEEE, page 715, 2015.\n\nWavelet-based energy features for glaucomatous image classification. U R S Dua, Acharya, S V Chowriappa, Sree, IEEE Transactions on Information Technology in Biomedicine. 161S Dua, U. R. Acharya, P Chowriappa, and S. V. Sree. Wavelet-based energy features for glaucomatous image clas- sification. IEEE Transactions on Information Technology in Biomedicine, 16(1):80-7, 2012.\n\nDermatologist-level classification of skin cancer with deep neural networks. A Esteva, R A Kuprel, Novoa, S M Ko, H M Swetter, S Blau, Thrun, Nature. 5427639A Esteva, B Kuprel, R. A. Novoa, J Ko, S. M. Swetter, H. M. Blau, and S Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115- 118, 2017.\n\nDiscriminative localization in cnns for weaklysupervised segmentation of pulmonary nodules. Xinyang Feng, Jie Yang, F Andrew, Elsa D Laine, Angelini, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerXinyang Feng, Jie Yang, Andrew F Laine, and Elsa D Angelini. Discriminative localization in cnns for weakly- supervised segmentation of pulmonary nodules. In In- ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages 568-576. Springer, 2017.\n\nRim-one: An open retinal image database for optic nerve evaluation. F Fumero, S Alayon, J L Sanchez, J Sigut, M Gonzalez-Hernandez, International Symposium on Computer-Based Medical Systems. F. Fumero, S. Alayon, J. L. Sanchez, J. Sigut, and M. Gonzalez-Hernandez. Rim-one: An open retinal image database for optic nerve evaluation. In International Sympo- sium on Computer-Based Medical Systems, pages 1-6, 2011.\n\nAutomated identification of diabetic retinopathy using deep learning. R Gargeya, T Leng, Ophthalmology. 1247R Gargeya and T. Leng. Automated identification of di- abetic retinopathy using deep learning. Ophthalmology, 124(7):962-969, 2017.\n\nSkin disease recognition using deep saliency features and multimodal learning of dermoscopy and clinical images. Zongyuan Ge, Sergey Demyanov, Rajib Chakravorty, Adrian Bowling, Rahil Garnavi, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerZongyuan Ge, Sergey Demyanov, Rajib Chakravorty, Adrian Bowling, and Rahil Garnavi. Skin disease recognition us- ing deep saliency features and multimodal learning of der- moscopy and clinical images. In International Conference on Medical Image Computing and Computer-Assisted Inter- vention, pages 250-258. Springer, 2017.\n\nWeakly-supervised localization of diabetic retinopathy lesions in retinal fundus images. M Waleed, Jan M Gondal, Ren\u00e9 K\u00f6hler, Grzeszick, A Gernot, Michael Fink, Hirsch, 2017 IEEE International Conference on. IEEEImage Processing (ICIPWaleed M Gondal, Jan M K\u00f6hler, Ren\u00e9 Grzeszick, Gernot A Fink, and Michael Hirsch. Weakly-supervised localization of diabetic retinopathy lesions in retinal fundus images. In Im- age Processing (ICIP), 2017 IEEE International Conference on, pages 2069-2073. IEEE, 2017.\n\nDevelopment and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. L V Gulshan, Peng, M C Coram, D Stumpe, Wu, Narayanaswamy, Venugopalan, Widner, J Madams, Cuadros, Jama. 316222402V Gulshan, L. Peng, M Coram, M. C. Stumpe, D. Wu, A Narayanaswamy, S Venugopalan, K Widner, T Madams, and J Cuadros. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fun- dus photographs. Jama, 316(22):2402, 2016.\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level perfor- mance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n\nSalicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks. Xun Huang, Chengyao Shen, Xavier Boix, Qi Zhao, IEEE International Conference on Computer Vision. Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao. Sal- icon: Reducing the semantic gap in saliency prediction by adapting deep neural networks. In IEEE International Con- ference on Computer Vision, pages 262-270, 2015.\n\nAn adaptive threshold based image processing technique for improved glaucoma detection and classification. Ashish Issac, M Partha Sarathi, Malay Kishore Dutta, Elsevier North-Holland, IncAshish Issac, M. Partha Sarathi, and Malay Kishore Dutta. An adaptive threshold based image processing technique for improved glaucoma detection and classification. Elsevier North-Holland, Inc., 2015.\n\nIdentifying medical diagnoses and treatable diseases by imagebased deep learning. Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina C S Valentim, Huiying Liang, Sally L Baxter, Alex Mckeown, Ge Yang, Xiaokang Wu, Fangbing Yan, 1122C1131.e9Cell. 1725Daniel S. Kermany, Michael Goldbaum, Wenjia Cai, Car- olina C. S. Valentim, Huiying Liang, Sally L. Baxter, Alex Mckeown, Ge Yang, Xiaokang Wu, and Fangbing Yan. Iden- tifying medical diagnoses and treatable diseases by image- based deep learning. Cell, 172(5):1122C1131.e9, 2018.\n\nBubbleview: an interface for crowdsourcing image importance maps and tracking visual attention. Nam Wook Kim, Zoya Bylinskii, Michelle A Borkin, Krzysztof Z Gajos, Aude Oliva, Fredo Durand, Hanspeter Pfister, ACM Transactions on Computer-Human Interaction. 245Nam Wook Kim, Zoya Bylinskii, Michelle A. Borkin, Krzysztof Z. Gajos, Aude Oliva, Fredo Durand, and Hanspeter Pfister. Bubbleview: an interface for crowd- sourcing image importance maps and tracking visual atten- tion. ACM Transactions on Computer-Human Interaction, 24(5):1-40, 2017.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Computer Science. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Computer Science, 2014.\n\nDeep learning. nature. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, 521436Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.\n\n. A Li, J Cheng, D W Wong, J Liu, Annan Li, Cheng, Damon Wing Kee Wong, Jiang Liu, J. Cheng, and D. W.A. Li, J. Cheng, D. W. Wong, J. Liu, Annan Li, Jun Cheng, Damon Wing Kee Wong, Jiang Liu, J. Cheng, and D. W.\n\nIntegrating holistic and local deep features for glaucoma classification. Wong, 38th Annual International Conference of the IEEE. EMBC1328Wong. Integrating holistic and local deep features for glau- coma classification. In Engineering in Medicine and Biol- ogy Society (EMBC), 38th Annual International Conference of the IEEE, page 1328, 2016.\n\nEfficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs. Z Li, He, Keel, Meng, M Chang, He, Ophthalmology. Z Li, Y He, S Keel, W Meng, RT Chang, and M He. Effi- cacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs. Oph- thalmology, 2018.\n\nZhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li-Jia Li, Fei-Fei Li, arXiv:1711.06373Thoracic disease identification and localization with limited supervision. arXiv preprintZhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li- Jia Li, and Fei-Fei Li. Thoracic disease identification and localization with limited supervision. arXiv preprint arXiv:1711.06373, 2017.\n\nSupernormal vision and high-resolution retinal imaging through adaptive optics. J Liang, D R Williams, D T Miller, Journal of the Optical Society of America A Optics Image Science & Vision. 1411J. Liang, D. R. Williams, and D. T. Miller. Supernormal vision and high-resolution retinal imaging through adaptive optics. Journal of the Optical Society of America A Optics Image Science & Vision, 14(11):2884-92, 1997.\n\nRyan Poplin, V Avinash, Katy Varadarajan, Yun Blumer, Michael V Liu, Greg S Mcconnell, Lily Corrado, Dale R Peng, Webster, arXiv:1708.09843Predicting cardiovascular risk factors from retinal fundus photographs using deep learning. arXiv preprintRyan Poplin, Avinash V. Varadarajan, Katy Blumer, Yun Liu, Michael V. Mcconnell, Greg S. Corrado, Lily Peng, and Dale R. Webster. Predicting cardiovascular risk factors from retinal fundus photographs using deep learning. arXiv preprint arXiv:1708.09843, 2017.\n\nPranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, arXiv:1711.05225Radiologistlevel pneumonia detection on chest x-rays with deep learning. arXiv preprintPranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist- level pneumonia detection on chest x-rays with deep learn- ing. arXiv preprint arXiv:1711.05225, 2017.\n\nFaster r-cnn: towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, International Conference on Neural Information Processing Systems. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In International Conference on Neural Information Processing Systems, pages 91-99, 2015.\n\nJoint optic disc and cup segmentation using fully convolutional and adversarial networks. M Sharath, Keerthi Shankaranarayana, Kaushik Ram, Mohanasankar Mitra, Sivaprakasam, Fetal, Infant and Ophthalmic Medical Image Analysis. SpringerSharath M Shankaranarayana, Keerthi Ram, Kaushik Mitra, and Mohanasankar Sivaprakasam. Joint optic disc and cup segmentation using fully convolutional and adversarial net- works. In Fetal, Infant and Ophthalmic Medical Image Anal- ysis, pages 168-176. Springer, 2017.\n\nAction recognition using visual attention. Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov, arXiv:1511.04119arXiv preprintShikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using visual attention. arXiv preprint arXiv:1511.04119, 2016.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nImage processing based automatic diagnosis of glaucoma using wavelet features of segmented optic disc from fundus image. Anushikha Singh, M Malay Kishore Dutta, Vaclav Parthasarathi, Radim Uher, Burget, Computer Methods & Programs in Biomedicine. 124108Anushikha Singh, Malay Kishore Dutta, M. Parthasarathi, Vaclav Uher, and Radim Burget. Image processing based automatic diagnosis of glaucoma using wavelet features of segmented optic disc from fundus image. Computer Methods & Programs in Biomedicine, 124(C):108, 2016.\n\nStriving for simplicity: The all convolutional net. Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller, arXiv:1412.6806arXiv preprintJost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.\n\nDevelopment and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. Dsw Ting, C Y Cheung, G Lim, Gsw Tan, N D Quang, A Gan, H Hamzah, R Garciafranco, Iy Yeo San, S Y Lee, Jama. 318222211Dsw Ting, C. Y. Cheung, G. Lim, Gsw Tan, N. D. Quang, A. Gan, H. Hamzah, R. Garciafranco, Iy Yeo San, and S. Y. Lee. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. Jama, 318(22):2211, 2017.\n\nRich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, International conference on machine learning. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption gen- eration with visual attention. In International conference on machine learning, pages 2048-2057, 2015.\n\nLearning to detect video saliency with hevc features. Mai Xu, Lai Jiang, Xiaoyan Sun, Zhaoting Ye, Zulin Wang, IEEE Transactions on Image Processing. 261Mai Xu, Lai Jiang, Xiaoyan Sun, Zhaoting Ye, and Zulin Wang. Learning to detect video saliency with hevc fea- tures. IEEE Transactions on Image Processing, 26(1):369- 385, 2017.\n\nA subjective visual quality assessment method of panoramic videos. Mai Xu, Chen Li, Yufan Liu, Xin Deng, Jiaxin Lu, 2017 IEEE International Conference on Multimedia and Expo. IEEEMai Xu, Chen Li, Yufan Liu, Xin Deng, and Jiaxin Lu. A subjective visual quality assessment method of panoramic videos. In 2017 IEEE International Conference on Multi- media and Expo, pages 517-522. IEEE, 2017.\n\nVolumetric convnets with mixed residual connections for automated prostate segmentation from 3d mr images. Lequan Yu, Xin Yang, Chen Hao, Jing Qin, Pheng Ann Heng, The AAAI Conference on Artificial Intelligence. Lequan Yu, Xin Yang, Chen Hao, Jing Qin, and Pheng Ann Heng. Volumetric convnets with mixed residual connections for automated prostate segmentation from 3d mr images. In The AAAI Conference on Artificial Intelligence, 2017.\n\nSupervising neural attention models for video captioning by human gaze data. Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, Gunhee Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYoungjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, and Gunhee Kim. Supervising neural atten- tion models for video captioning by human gaze data. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2680-29, 2017.\n\nVisualizing and understanding convolutional networks. D Matthew, Rob Zeiler, Fergus, Matthew D. Zeiler and Rob Fergus. Visualizing and under- standing convolutional networks. pages 818-833, 2014.\n\nWeakly-supervised evidence pinpointing and description. Qiang Zhang, Abhir Bhalerao, Charles Hutchinson, International Conference on Information Processing in Medical Imaging. SpringerQiang Zhang, Abhir Bhalerao, and Charles Hutchinson. Weakly-supervised evidence pinpointing and description. In International Conference on Information Processing in Med- ical Imaging, pages 210-222. Springer, 2017.\n\nLearning deep features for discriminative localization. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discrimi- native localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2921- 2929, 2016.\n\nGlaucoma detection using entropy sampling and ensemble learning for automatic optic cup and disc segmentation. Julian Zilly, Joachim M Buhmann, Dwarikanath Mahapatra, Computerized Medical Imaging and Graphics. 55Julian Zilly, Joachim M. Buhmann, and Dwarikanath Maha- patra. Glaucoma detection using entropy sampling and en- semble learning for automatic optic cup and disc segmenta- tion. Computerized Medical Imaging and Graphics, 55:28- 41, 2017.\n", "annotations": {"author": "[{\"end\":165,\"start\":76},{\"end\":419,\"start\":166},{\"end\":534,\"start\":420},{\"end\":627,\"start\":535},{\"end\":639,\"start\":628}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":80},{\"end\":172,\"start\":170},{\"end\":432,\"start\":428},{\"end\":544,\"start\":539},{\"end\":638,\"start\":635}]", "author_first_name": "[{\"end\":79,\"start\":76},{\"end\":169,\"start\":166},{\"end\":427,\"start\":420},{\"end\":538,\"start\":535},{\"end\":634,\"start\":628}]", "author_affiliation": "[{\"end\":164,\"start\":84},{\"end\":272,\"start\":192},{\"end\":379,\"start\":274},{\"end\":418,\"start\":381},{\"end\":533,\"start\":453},{\"end\":626,\"start\":546}]", "title": "[{\"end\":73,\"start\":1},{\"end\":712,\"start\":640}]", "venue": null, "abstract": "[{\"end\":2057,\"start\":714}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2218,\"start\":2215},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2221,\"start\":2218},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2224,\"start\":2221},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2244,\"start\":2240},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2247,\"start\":2244},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2249,\"start\":2247},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2277,\"start\":2273},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2532,\"start\":2528},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2564,\"start\":2560},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2910,\"start\":2906},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2913,\"start\":2910},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2916,\"start\":2913},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2919,\"start\":2916},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3077,\"start\":3073},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3080,\"start\":3077},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3509,\"start\":3505},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3736,\"start\":3732},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4990,\"start\":4986},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4993,\"start\":4990},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4996,\"start\":4993},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5302,\"start\":5298},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5305,\"start\":5302},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5307,\"start\":5305},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5310,\"start\":5307},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5313,\"start\":5310},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5369,\"start\":5365},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5560,\"start\":5556},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6303,\"start\":6300},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6305,\"start\":6303},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6308,\"start\":6305},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6381,\"start\":6377},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6384,\"start\":6381},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6387,\"start\":6384},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6407,\"start\":6403},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6410,\"start\":6407},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6486,\"start\":6482},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6978,\"start\":6975},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6980,\"start\":6978},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6983,\"start\":6980},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6986,\"start\":6983},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7005,\"start\":7002},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7098,\"start\":7095},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7170,\"start\":7167},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7172,\"start\":7170},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7488,\"start\":7484},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7491,\"start\":7488},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7493,\"start\":7491},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7496,\"start\":7493},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7499,\"start\":7496},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7519,\"start\":7515},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7522,\"start\":7519},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7772,\"start\":7769},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7855,\"start\":7851},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8027,\"start\":8024},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8030,\"start\":8027},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8312,\"start\":8308},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8436,\"start\":8432},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9497,\"start\":9493},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11042,\"start\":11038},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15875,\"start\":15871},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16959,\"start\":16955},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18424,\"start\":18420},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18427,\"start\":18424},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18451,\"start\":18447},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18454,\"start\":18451},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18457,\"start\":18454},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19073,\"start\":19069},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19818,\"start\":19814},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20294,\"start\":20290},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21701,\"start\":21697},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23638,\"start\":23635},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23978,\"start\":23974},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24411,\"start\":24408},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24414,\"start\":24411},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25518,\"start\":25515},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25527,\"start\":25523},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25675,\"start\":25672},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25678,\"start\":25675},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26103,\"start\":26100},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26112,\"start\":26108},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26625,\"start\":26622},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26628,\"start\":26625},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26932,\"start\":26929},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26935,\"start\":26932},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27095,\"start\":27092},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27098,\"start\":27095},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27126,\"start\":27123},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27537,\"start\":27533},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28057,\"start\":28053},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28247,\"start\":28244},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28250,\"start\":28247},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30067,\"start\":30063},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30105,\"start\":30101},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30126,\"start\":30122},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30342,\"start\":30338},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32562,\"start\":32558}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33820,\"start\":33665},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34224,\"start\":33821},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34506,\"start\":34225},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34610,\"start\":34507},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34731,\"start\":34611},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34893,\"start\":34732},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34945,\"start\":34894},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35086,\"start\":34946},{\"attributes\":{\"id\":\"fig_9\"},\"end\":35316,\"start\":35087},{\"attributes\":{\"id\":\"fig_10\"},\"end\":35773,\"start\":35317},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36045,\"start\":35774},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36338,\"start\":36046},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36619,\"start\":36339},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37105,\"start\":36620}]", "paragraph": "[{\"end\":4070,\"start\":2073},{\"end\":4699,\"start\":4072},{\"end\":5762,\"start\":4701},{\"end\":6200,\"start\":5764},{\"end\":6751,\"start\":6223},{\"end\":7409,\"start\":6753},{\"end\":8437,\"start\":7411},{\"end\":9269,\"start\":8466},{\"end\":10695,\"start\":9271},{\"end\":11171,\"start\":10729},{\"end\":11440,\"start\":11189},{\"end\":12245,\"start\":11442},{\"end\":12336,\"start\":12247},{\"end\":13137,\"start\":12338},{\"end\":13204,\"start\":13139},{\"end\":13317,\"start\":13206},{\"end\":14134,\"start\":13319},{\"end\":14834,\"start\":14157},{\"end\":15810,\"start\":14836},{\"end\":16311,\"start\":15812},{\"end\":17869,\"start\":16343},{\"end\":18561,\"start\":17911},{\"end\":19323,\"start\":18563},{\"end\":19732,\"start\":19358},{\"end\":20187,\"start\":19734},{\"end\":20611,\"start\":20189},{\"end\":21079,\"start\":20636},{\"end\":21853,\"start\":21097},{\"end\":22276,\"start\":21909},{\"end\":22577,\"start\":22343},{\"end\":23141,\"start\":22627},{\"end\":24278,\"start\":23180},{\"end\":24527,\"start\":24280},{\"end\":24804,\"start\":24587},{\"end\":25528,\"start\":24871},{\"end\":26264,\"start\":25565},{\"end\":26958,\"start\":26266},{\"end\":27857,\"start\":26960},{\"end\":28251,\"start\":27859},{\"end\":29162,\"start\":28325},{\"end\":30667,\"start\":29164},{\"end\":32236,\"start\":30703},{\"end\":32696,\"start\":32238},{\"end\":33581,\"start\":32711},{\"end\":33664,\"start\":33601}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10728,\"start\":10696},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20635,\"start\":20612},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21908,\"start\":21854},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22342,\"start\":22277},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22626,\"start\":22578},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24557,\"start\":24528},{\"attributes\":{\"id\":\"formula_6\"},\"end\":24586,\"start\":24557},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24870,\"start\":24805}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11620,\"start\":11613},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11950,\"start\":11943},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25782,\"start\":25775},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25878,\"start\":25871},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26764,\"start\":26757},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27248,\"start\":27241},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27290,\"start\":27283},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27884,\"start\":27877},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32601,\"start\":32594}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2071,\"start\":2059},{\"attributes\":{\"n\":\"2.\"},\"end\":6221,\"start\":6203},{\"attributes\":{\"n\":\"3.\"},\"end\":8448,\"start\":8440},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8464,\"start\":8451},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11187,\"start\":11174},{\"attributes\":{\"n\":\"4.\"},\"end\":14143,\"start\":14137},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14155,\"start\":14146},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16341,\"start\":16314},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17909,\"start\":17872},{\"attributes\":{\"n\":\"4.4.\"},\"end\":19356,\"start\":19326},{\"attributes\":{\"n\":\"4.5.\"},\"end\":21095,\"start\":21082},{\"attributes\":{\"n\":\"5.\"},\"end\":23167,\"start\":23144},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23178,\"start\":23170},{\"attributes\":{\"n\":\"5.2.\"},\"end\":25563,\"start\":25531},{\"attributes\":{\"n\":\"5.3.\"},\"end\":28323,\"start\":28254},{\"attributes\":{\"n\":\"5.4.\"},\"end\":30701,\"start\":30670},{\"attributes\":{\"n\":\"6.\"},\"end\":32709,\"start\":32699},{\"attributes\":{\"n\":\"7.\"},\"end\":33599,\"start\":33584},{\"end\":33676,\"start\":33666},{\"end\":33832,\"start\":33822},{\"end\":34236,\"start\":34226},{\"end\":34518,\"start\":34508},{\"end\":34622,\"start\":34612},{\"end\":34743,\"start\":34733},{\"end\":34905,\"start\":34895},{\"end\":34957,\"start\":34947},{\"end\":35098,\"start\":35088},{\"end\":35329,\"start\":35318},{\"end\":35784,\"start\":35775},{\"end\":36056,\"start\":36047},{\"end\":36349,\"start\":36340},{\"end\":36630,\"start\":36621}]", "table": "[{\"end\":36045,\"start\":35884},{\"end\":36338,\"start\":36148},{\"end\":36619,\"start\":36429},{\"end\":37105,\"start\":36839}]", "figure_caption": "[{\"end\":33820,\"start\":33678},{\"end\":34224,\"start\":33834},{\"end\":34506,\"start\":34238},{\"end\":34610,\"start\":34520},{\"end\":34731,\"start\":34624},{\"end\":34893,\"start\":34745},{\"end\":34945,\"start\":34907},{\"end\":35086,\"start\":34959},{\"end\":35316,\"start\":35100},{\"end\":35773,\"start\":35332},{\"end\":35884,\"start\":35786},{\"end\":36148,\"start\":36058},{\"end\":36429,\"start\":36351},{\"end\":36839,\"start\":36632}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3623,\"start\":3615},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11052,\"start\":11044},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12467,\"start\":12459},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12963,\"start\":12955},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13407,\"start\":13399},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13524,\"start\":13516},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13851,\"start\":13843},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13897,\"start\":13889},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14880,\"start\":14872},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":15026,\"start\":15018},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15048,\"start\":15040},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":17107,\"start\":17098},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":17452,\"start\":17444},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19866,\"start\":19858},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21362,\"start\":21354},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26287,\"start\":26279},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27261,\"start\":27253},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27897,\"start\":27889},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":28417,\"start\":28409},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":28899,\"start\":28891},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29242,\"start\":29233},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30165,\"start\":30156},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31017,\"start\":31008}]", "bib_author_first_name": "[{\"end\":37247,\"start\":37246},{\"end\":37258,\"start\":37257},{\"end\":37268,\"start\":37264},{\"end\":37283,\"start\":37273},{\"end\":37687,\"start\":37682},{\"end\":37706,\"start\":37698},{\"end\":37716,\"start\":37711},{\"end\":37732,\"start\":37726},{\"end\":37744,\"start\":37740},{\"end\":37761,\"start\":37754},{\"end\":37772,\"start\":37769},{\"end\":38255,\"start\":38250},{\"end\":38269,\"start\":38260},{\"end\":38281,\"start\":38276},{\"end\":38577,\"start\":38575},{\"end\":38590,\"start\":38588},{\"end\":38932,\"start\":38925},{\"end\":38944,\"start\":38939},{\"end\":38963,\"start\":38949},{\"end\":38974,\"start\":38970},{\"end\":38978,\"start\":38975},{\"end\":38990,\"start\":38985},{\"end\":39381,\"start\":39380},{\"end\":39383,\"start\":39382},{\"end\":39401,\"start\":39400},{\"end\":39403,\"start\":39402},{\"end\":39765,\"start\":39764},{\"end\":39775,\"start\":39774},{\"end\":39777,\"start\":39776},{\"end\":39794,\"start\":39793},{\"end\":39796,\"start\":39795},{\"end\":39802,\"start\":39801},{\"end\":39804,\"start\":39803},{\"end\":39815,\"start\":39814},{\"end\":40134,\"start\":40127},{\"end\":40144,\"start\":40141},{\"end\":40152,\"start\":40151},{\"end\":40165,\"start\":40161},{\"end\":40167,\"start\":40166},{\"end\":40630,\"start\":40629},{\"end\":40640,\"start\":40639},{\"end\":40650,\"start\":40649},{\"end\":40652,\"start\":40651},{\"end\":40663,\"start\":40662},{\"end\":40672,\"start\":40671},{\"end\":41047,\"start\":41046},{\"end\":41058,\"start\":41057},{\"end\":41338,\"start\":41330},{\"end\":41349,\"start\":41343},{\"end\":41365,\"start\":41360},{\"end\":41385,\"start\":41379},{\"end\":41400,\"start\":41395},{\"end\":41922,\"start\":41921},{\"end\":41934,\"start\":41931},{\"end\":41936,\"start\":41935},{\"end\":41949,\"start\":41945},{\"end\":41970,\"start\":41969},{\"end\":41986,\"start\":41979},{\"end\":42462,\"start\":42461},{\"end\":42481,\"start\":42480},{\"end\":42483,\"start\":42482},{\"end\":42492,\"start\":42491},{\"end\":42542,\"start\":42541},{\"end\":42946,\"start\":42939},{\"end\":42958,\"start\":42951},{\"end\":42974,\"start\":42966},{\"end\":42984,\"start\":42980},{\"end\":43410,\"start\":43403},{\"end\":43422,\"start\":43415},{\"end\":43438,\"start\":43430},{\"end\":43448,\"start\":43444},{\"end\":43897,\"start\":43894},{\"end\":43913,\"start\":43905},{\"end\":43926,\"start\":43920},{\"end\":43935,\"start\":43933},{\"end\":44328,\"start\":44322},{\"end\":44337,\"start\":44336},{\"end\":44344,\"start\":44338},{\"end\":44367,\"start\":44354},{\"end\":44692,\"start\":44686},{\"end\":44694,\"start\":44693},{\"end\":44711,\"start\":44704},{\"end\":44728,\"start\":44722},{\"end\":44742,\"start\":44734},{\"end\":44746,\"start\":44743},{\"end\":44764,\"start\":44757},{\"end\":44777,\"start\":44772},{\"end\":44779,\"start\":44778},{\"end\":44792,\"start\":44788},{\"end\":44804,\"start\":44802},{\"end\":44819,\"start\":44811},{\"end\":44832,\"start\":44824},{\"end\":45246,\"start\":45238},{\"end\":45256,\"start\":45252},{\"end\":45276,\"start\":45268},{\"end\":45278,\"start\":45277},{\"end\":45296,\"start\":45287},{\"end\":45298,\"start\":45297},{\"end\":45310,\"start\":45306},{\"end\":45323,\"start\":45318},{\"end\":45341,\"start\":45332},{\"end\":45733,\"start\":45732},{\"end\":45749,\"start\":45744},{\"end\":45909,\"start\":45905},{\"end\":45923,\"start\":45917},{\"end\":45940,\"start\":45932},{\"end\":46051,\"start\":46050},{\"end\":46057,\"start\":46056},{\"end\":46066,\"start\":46065},{\"end\":46068,\"start\":46067},{\"end\":46076,\"start\":46075},{\"end\":46087,\"start\":46082},{\"end\":46721,\"start\":46720},{\"end\":46743,\"start\":46742},{\"end\":46961,\"start\":46958},{\"end\":46971,\"start\":46966},{\"end\":46981,\"start\":46978},{\"end\":46991,\"start\":46987},{\"end\":47000,\"start\":46997},{\"end\":47012,\"start\":47006},{\"end\":47024,\"start\":47017},{\"end\":47406,\"start\":47405},{\"end\":47415,\"start\":47414},{\"end\":47417,\"start\":47416},{\"end\":47429,\"start\":47428},{\"end\":47431,\"start\":47430},{\"end\":47745,\"start\":47741},{\"end\":47755,\"start\":47754},{\"end\":47769,\"start\":47765},{\"end\":47786,\"start\":47783},{\"end\":47802,\"start\":47795},{\"end\":47804,\"start\":47803},{\"end\":47814,\"start\":47810},{\"end\":47816,\"start\":47815},{\"end\":47832,\"start\":47828},{\"end\":47846,\"start\":47842},{\"end\":47848,\"start\":47847},{\"end\":48254,\"start\":48248},{\"end\":48272,\"start\":48266},{\"end\":48286,\"start\":48280},{\"end\":48299,\"start\":48292},{\"end\":48313,\"start\":48306},{\"end\":48325,\"start\":48321},{\"end\":48337,\"start\":48332},{\"end\":48349,\"start\":48344},{\"end\":48363,\"start\":48357},{\"end\":48379,\"start\":48374},{\"end\":48859,\"start\":48852},{\"end\":48878,\"start\":48874},{\"end\":48887,\"start\":48883},{\"end\":49286,\"start\":49285},{\"end\":49303,\"start\":49296},{\"end\":49329,\"start\":49322},{\"end\":49347,\"start\":49335},{\"end\":49749,\"start\":49742},{\"end\":49762,\"start\":49758},{\"end\":49776,\"start\":49770},{\"end\":50032,\"start\":50027},{\"end\":50049,\"start\":50043},{\"end\":50366,\"start\":50357},{\"end\":50375,\"start\":50374},{\"end\":50403,\"start\":50397},{\"end\":50424,\"start\":50419},{\"end\":50816,\"start\":50812},{\"end\":50844,\"start\":50838},{\"end\":50864,\"start\":50858},{\"end\":50877,\"start\":50871},{\"end\":51263,\"start\":51260},{\"end\":51271,\"start\":51270},{\"end\":51273,\"start\":51272},{\"end\":51283,\"start\":51282},{\"end\":51292,\"start\":51289},{\"end\":51299,\"start\":51298},{\"end\":51301,\"start\":51300},{\"end\":51310,\"start\":51309},{\"end\":51317,\"start\":51316},{\"end\":51327,\"start\":51326},{\"end\":51344,\"start\":51342},{\"end\":51355,\"start\":51354},{\"end\":51357,\"start\":51356},{\"end\":51805,\"start\":51799},{\"end\":51815,\"start\":51810},{\"end\":51824,\"start\":51820},{\"end\":51841,\"start\":51832},{\"end\":51852,\"start\":51847},{\"end\":51870,\"start\":51864},{\"end\":52258,\"start\":52255},{\"end\":52266,\"start\":52263},{\"end\":52281,\"start\":52274},{\"end\":52295,\"start\":52287},{\"end\":52305,\"start\":52300},{\"end\":52603,\"start\":52600},{\"end\":52612,\"start\":52608},{\"end\":52622,\"start\":52617},{\"end\":52631,\"start\":52628},{\"end\":52644,\"start\":52638},{\"end\":53037,\"start\":53031},{\"end\":53045,\"start\":53042},{\"end\":53056,\"start\":53052},{\"end\":53066,\"start\":53062},{\"end\":53081,\"start\":53072},{\"end\":53447,\"start\":53439},{\"end\":53460,\"start\":53452},{\"end\":53474,\"start\":53467},{\"end\":53485,\"start\":53480},{\"end\":53499,\"start\":53491},{\"end\":53511,\"start\":53505},{\"end\":53980,\"start\":53979},{\"end\":53993,\"start\":53990},{\"end\":54183,\"start\":54178},{\"end\":54196,\"start\":54191},{\"end\":54214,\"start\":54207},{\"end\":54584,\"start\":54579},{\"end\":54597,\"start\":54591},{\"end\":54611,\"start\":54606},{\"end\":54627,\"start\":54623},{\"end\":54642,\"start\":54635},{\"end\":55154,\"start\":55148},{\"end\":55169,\"start\":55162},{\"end\":55171,\"start\":55170},{\"end\":55192,\"start\":55181}]", "bib_author_last_name": "[{\"end\":37255,\"start\":37248},{\"end\":37262,\"start\":37259},{\"end\":37271,\"start\":37269},{\"end\":37296,\"start\":37284},{\"end\":37302,\"start\":37298},{\"end\":37696,\"start\":37688},{\"end\":37709,\"start\":37707},{\"end\":37724,\"start\":37717},{\"end\":37738,\"start\":37733},{\"end\":37752,\"start\":37745},{\"end\":37767,\"start\":37762},{\"end\":37778,\"start\":37773},{\"end\":38258,\"start\":38256},{\"end\":38274,\"start\":38270},{\"end\":38293,\"start\":38282},{\"end\":38586,\"start\":38578},{\"end\":38594,\"start\":38591},{\"end\":38600,\"start\":38596},{\"end\":38937,\"start\":38933},{\"end\":38947,\"start\":38945},{\"end\":38968,\"start\":38964},{\"end\":38983,\"start\":38979},{\"end\":38994,\"start\":38991},{\"end\":39389,\"start\":39384},{\"end\":39398,\"start\":39391},{\"end\":39414,\"start\":39404},{\"end\":39420,\"start\":39416},{\"end\":39772,\"start\":39766},{\"end\":39784,\"start\":39778},{\"end\":39791,\"start\":39786},{\"end\":39799,\"start\":39797},{\"end\":39812,\"start\":39805},{\"end\":39820,\"start\":39816},{\"end\":39827,\"start\":39822},{\"end\":40139,\"start\":40135},{\"end\":40149,\"start\":40145},{\"end\":40159,\"start\":40153},{\"end\":40173,\"start\":40168},{\"end\":40183,\"start\":40175},{\"end\":40637,\"start\":40631},{\"end\":40647,\"start\":40641},{\"end\":40660,\"start\":40653},{\"end\":40669,\"start\":40664},{\"end\":40691,\"start\":40673},{\"end\":41055,\"start\":41048},{\"end\":41063,\"start\":41059},{\"end\":41341,\"start\":41339},{\"end\":41358,\"start\":41350},{\"end\":41377,\"start\":41366},{\"end\":41393,\"start\":41386},{\"end\":41408,\"start\":41401},{\"end\":41929,\"start\":41923},{\"end\":41943,\"start\":41937},{\"end\":41956,\"start\":41950},{\"end\":41967,\"start\":41958},{\"end\":41977,\"start\":41971},{\"end\":41991,\"start\":41987},{\"end\":41999,\"start\":41993},{\"end\":42472,\"start\":42463},{\"end\":42478,\"start\":42474},{\"end\":42489,\"start\":42484},{\"end\":42499,\"start\":42493},{\"end\":42503,\"start\":42501},{\"end\":42518,\"start\":42505},{\"end\":42531,\"start\":42520},{\"end\":42539,\"start\":42533},{\"end\":42549,\"start\":42543},{\"end\":42558,\"start\":42551},{\"end\":42949,\"start\":42947},{\"end\":42964,\"start\":42959},{\"end\":42978,\"start\":42975},{\"end\":42988,\"start\":42985},{\"end\":43413,\"start\":43411},{\"end\":43428,\"start\":43423},{\"end\":43442,\"start\":43439},{\"end\":43452,\"start\":43449},{\"end\":43903,\"start\":43898},{\"end\":43918,\"start\":43914},{\"end\":43931,\"start\":43927},{\"end\":43940,\"start\":43936},{\"end\":44334,\"start\":44329},{\"end\":44352,\"start\":44345},{\"end\":44373,\"start\":44368},{\"end\":44702,\"start\":44695},{\"end\":44720,\"start\":44712},{\"end\":44732,\"start\":44729},{\"end\":44755,\"start\":44747},{\"end\":44770,\"start\":44765},{\"end\":44786,\"start\":44780},{\"end\":44800,\"start\":44793},{\"end\":44809,\"start\":44805},{\"end\":44822,\"start\":44820},{\"end\":44836,\"start\":44833},{\"end\":45250,\"start\":45247},{\"end\":45266,\"start\":45257},{\"end\":45285,\"start\":45279},{\"end\":45304,\"start\":45299},{\"end\":45316,\"start\":45311},{\"end\":45330,\"start\":45324},{\"end\":45349,\"start\":45342},{\"end\":45742,\"start\":45734},{\"end\":45756,\"start\":45750},{\"end\":45760,\"start\":45758},{\"end\":45915,\"start\":45910},{\"end\":45930,\"start\":45924},{\"end\":45947,\"start\":45941},{\"end\":46054,\"start\":46052},{\"end\":46063,\"start\":46058},{\"end\":46073,\"start\":46069},{\"end\":46080,\"start\":46077},{\"end\":46090,\"start\":46088},{\"end\":46339,\"start\":46335},{\"end\":46724,\"start\":46722},{\"end\":46728,\"start\":46726},{\"end\":46734,\"start\":46730},{\"end\":46740,\"start\":46736},{\"end\":46749,\"start\":46744},{\"end\":46753,\"start\":46751},{\"end\":46964,\"start\":46962},{\"end\":46976,\"start\":46972},{\"end\":46985,\"start\":46982},{\"end\":46995,\"start\":46992},{\"end\":47004,\"start\":47001},{\"end\":47015,\"start\":47013},{\"end\":47027,\"start\":47025},{\"end\":47412,\"start\":47407},{\"end\":47426,\"start\":47418},{\"end\":47438,\"start\":47432},{\"end\":47752,\"start\":47746},{\"end\":47763,\"start\":47756},{\"end\":47781,\"start\":47770},{\"end\":47793,\"start\":47787},{\"end\":47808,\"start\":47805},{\"end\":47826,\"start\":47817},{\"end\":47840,\"start\":47833},{\"end\":47853,\"start\":47849},{\"end\":47862,\"start\":47855},{\"end\":48264,\"start\":48255},{\"end\":48278,\"start\":48273},{\"end\":48290,\"start\":48287},{\"end\":48304,\"start\":48300},{\"end\":48319,\"start\":48314},{\"end\":48330,\"start\":48326},{\"end\":48342,\"start\":48338},{\"end\":48355,\"start\":48350},{\"end\":48372,\"start\":48364},{\"end\":48390,\"start\":48380},{\"end\":48872,\"start\":48860},{\"end\":48881,\"start\":48879},{\"end\":48896,\"start\":48888},{\"end\":48901,\"start\":48898},{\"end\":49294,\"start\":49287},{\"end\":49320,\"start\":49304},{\"end\":49333,\"start\":49330},{\"end\":49353,\"start\":49348},{\"end\":49367,\"start\":49355},{\"end\":49756,\"start\":49750},{\"end\":49768,\"start\":49763},{\"end\":49790,\"start\":49777},{\"end\":50041,\"start\":50033},{\"end\":50059,\"start\":50050},{\"end\":50372,\"start\":50367},{\"end\":50395,\"start\":50376},{\"end\":50417,\"start\":50404},{\"end\":50429,\"start\":50425},{\"end\":50437,\"start\":50431},{\"end\":50836,\"start\":50817},{\"end\":50856,\"start\":50845},{\"end\":50869,\"start\":50865},{\"end\":50888,\"start\":50878},{\"end\":51268,\"start\":51264},{\"end\":51280,\"start\":51274},{\"end\":51287,\"start\":51284},{\"end\":51296,\"start\":51293},{\"end\":51307,\"start\":51302},{\"end\":51314,\"start\":51311},{\"end\":51324,\"start\":51318},{\"end\":51340,\"start\":51328},{\"end\":51352,\"start\":51345},{\"end\":51361,\"start\":51358},{\"end\":51808,\"start\":51806},{\"end\":51818,\"start\":51816},{\"end\":51830,\"start\":51825},{\"end\":51845,\"start\":51842},{\"end\":51862,\"start\":51853},{\"end\":51883,\"start\":51871},{\"end\":52261,\"start\":52259},{\"end\":52272,\"start\":52267},{\"end\":52285,\"start\":52282},{\"end\":52298,\"start\":52296},{\"end\":52310,\"start\":52306},{\"end\":52606,\"start\":52604},{\"end\":52615,\"start\":52613},{\"end\":52626,\"start\":52623},{\"end\":52636,\"start\":52632},{\"end\":52647,\"start\":52645},{\"end\":53040,\"start\":53038},{\"end\":53050,\"start\":53046},{\"end\":53060,\"start\":53057},{\"end\":53070,\"start\":53067},{\"end\":53086,\"start\":53082},{\"end\":53450,\"start\":53448},{\"end\":53465,\"start\":53461},{\"end\":53478,\"start\":53475},{\"end\":53489,\"start\":53486},{\"end\":53503,\"start\":53500},{\"end\":53515,\"start\":53512},{\"end\":53988,\"start\":53981},{\"end\":54000,\"start\":53994},{\"end\":54008,\"start\":54002},{\"end\":54189,\"start\":54184},{\"end\":54205,\"start\":54197},{\"end\":54225,\"start\":54215},{\"end\":54589,\"start\":54585},{\"end\":54604,\"start\":54598},{\"end\":54621,\"start\":54612},{\"end\":54633,\"start\":54628},{\"end\":54651,\"start\":54643},{\"end\":55160,\"start\":55155},{\"end\":55179,\"start\":55172},{\"end\":55202,\"start\":55193}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8430297},\"end\":37595,\"start\":37165},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3753452},\"end\":38197,\"start\":37597},{\"attributes\":{\"doi\":\"arXiv:1412.7755\",\"id\":\"b2\"},\"end\":38463,\"start\":38199},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":44316720},\"end\":38860,\"start\":38465},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4359165},\"end\":39309,\"start\":38862},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6908220},\"end\":39685,\"start\":39311},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3767412},\"end\":40033,\"start\":39687},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3525590},\"end\":40559,\"start\":40035},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14779650},\"end\":40974,\"start\":40561},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5156963},\"end\":41215,\"start\":40976},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":11350469},\"end\":41830,\"start\":41217},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3491382},\"end\":42334,\"start\":41832},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":26657811},\"end\":42844,\"start\":42336},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13740328},\"end\":43355,\"start\":42846},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":43800,\"start\":43357},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7676247},\"end\":44213,\"start\":43802},{\"attributes\":{\"id\":\"b16\"},\"end\":44602,\"start\":44215},{\"attributes\":{\"doi\":\"1122C1131.e9\",\"id\":\"b17\",\"matched_paper_id\":3516426},\"end\":45140,\"start\":44604},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":229322349},\"end\":45686,\"start\":45142},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6628106},\"end\":45880,\"start\":45688},{\"attributes\":{\"id\":\"b20\"},\"end\":46046,\"start\":45882},{\"attributes\":{\"id\":\"b21\"},\"end\":46259,\"start\":46048},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":34622355},\"end\":46604,\"start\":46261},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3733964},\"end\":46956,\"start\":46606},{\"attributes\":{\"doi\":\"arXiv:1711.06373\",\"id\":\"b24\"},\"end\":47323,\"start\":46958},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15729199},\"end\":47739,\"start\":47325},{\"attributes\":{\"doi\":\"arXiv:1708.09843\",\"id\":\"b26\"},\"end\":48246,\"start\":47741},{\"attributes\":{\"doi\":\"arXiv:1711.05225\",\"id\":\"b27\"},\"end\":48770,\"start\":48248},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10328909},\"end\":49193,\"start\":48772},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":32462883},\"end\":49697,\"start\":49195},{\"attributes\":{\"doi\":\"arXiv:1511.04119\",\"id\":\"b30\"},\"end\":49957,\"start\":49699},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b31\"},\"end\":50234,\"start\":49959},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":29660433},\"end\":50758,\"start\":50236},{\"attributes\":{\"doi\":\"arXiv:1412.6806\",\"id\":\"b33\"},\"end\":51090,\"start\":50760},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3606254},\"end\":51688,\"start\":51092},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1055111},\"end\":52199,\"start\":51690},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":8012688},\"end\":52531,\"start\":52201},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":26722110},\"end\":52922,\"start\":52533},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":29157946},\"end\":53360,\"start\":52924},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":25392305},\"end\":53923,\"start\":53362},{\"attributes\":{\"id\":\"b40\"},\"end\":54120,\"start\":53925},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":20545262},\"end\":54521,\"start\":54122},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6789015},\"end\":55035,\"start\":54523},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6219158},\"end\":55486,\"start\":55037}]", "bib_title": "[{\"end\":37244,\"start\":37165},{\"end\":37680,\"start\":37597},{\"end\":38573,\"start\":38465},{\"end\":38923,\"start\":38862},{\"end\":39378,\"start\":39311},{\"end\":39762,\"start\":39687},{\"end\":40125,\"start\":40035},{\"end\":40627,\"start\":40561},{\"end\":41044,\"start\":40976},{\"end\":41328,\"start\":41217},{\"end\":41919,\"start\":41832},{\"end\":42459,\"start\":42336},{\"end\":42937,\"start\":42846},{\"end\":43401,\"start\":43357},{\"end\":43892,\"start\":43802},{\"end\":44684,\"start\":44604},{\"end\":45236,\"start\":45142},{\"end\":45730,\"start\":45688},{\"end\":46333,\"start\":46261},{\"end\":46718,\"start\":46606},{\"end\":47403,\"start\":47325},{\"end\":48850,\"start\":48772},{\"end\":49283,\"start\":49195},{\"end\":50355,\"start\":50236},{\"end\":51258,\"start\":51092},{\"end\":51797,\"start\":51690},{\"end\":52253,\"start\":52201},{\"end\":52598,\"start\":52533},{\"end\":53029,\"start\":52924},{\"end\":53437,\"start\":53362},{\"end\":54176,\"start\":54122},{\"end\":54577,\"start\":54523},{\"end\":55146,\"start\":55037}]", "bib_author": "[{\"end\":37257,\"start\":37246},{\"end\":37264,\"start\":37257},{\"end\":37273,\"start\":37264},{\"end\":37298,\"start\":37273},{\"end\":37304,\"start\":37298},{\"end\":37698,\"start\":37682},{\"end\":37711,\"start\":37698},{\"end\":37726,\"start\":37711},{\"end\":37740,\"start\":37726},{\"end\":37754,\"start\":37740},{\"end\":37769,\"start\":37754},{\"end\":37780,\"start\":37769},{\"end\":38260,\"start\":38250},{\"end\":38276,\"start\":38260},{\"end\":38295,\"start\":38276},{\"end\":38588,\"start\":38575},{\"end\":38596,\"start\":38588},{\"end\":38602,\"start\":38596},{\"end\":38939,\"start\":38925},{\"end\":38949,\"start\":38939},{\"end\":38970,\"start\":38949},{\"end\":38985,\"start\":38970},{\"end\":38996,\"start\":38985},{\"end\":39391,\"start\":39380},{\"end\":39400,\"start\":39391},{\"end\":39416,\"start\":39400},{\"end\":39422,\"start\":39416},{\"end\":39774,\"start\":39764},{\"end\":39786,\"start\":39774},{\"end\":39793,\"start\":39786},{\"end\":39801,\"start\":39793},{\"end\":39814,\"start\":39801},{\"end\":39822,\"start\":39814},{\"end\":39829,\"start\":39822},{\"end\":40141,\"start\":40127},{\"end\":40151,\"start\":40141},{\"end\":40161,\"start\":40151},{\"end\":40175,\"start\":40161},{\"end\":40185,\"start\":40175},{\"end\":40639,\"start\":40629},{\"end\":40649,\"start\":40639},{\"end\":40662,\"start\":40649},{\"end\":40671,\"start\":40662},{\"end\":40693,\"start\":40671},{\"end\":41057,\"start\":41046},{\"end\":41065,\"start\":41057},{\"end\":41343,\"start\":41330},{\"end\":41360,\"start\":41343},{\"end\":41379,\"start\":41360},{\"end\":41395,\"start\":41379},{\"end\":41410,\"start\":41395},{\"end\":41931,\"start\":41921},{\"end\":41945,\"start\":41931},{\"end\":41958,\"start\":41945},{\"end\":41969,\"start\":41958},{\"end\":41979,\"start\":41969},{\"end\":41993,\"start\":41979},{\"end\":42001,\"start\":41993},{\"end\":42474,\"start\":42461},{\"end\":42480,\"start\":42474},{\"end\":42491,\"start\":42480},{\"end\":42501,\"start\":42491},{\"end\":42505,\"start\":42501},{\"end\":42520,\"start\":42505},{\"end\":42533,\"start\":42520},{\"end\":42541,\"start\":42533},{\"end\":42551,\"start\":42541},{\"end\":42560,\"start\":42551},{\"end\":42951,\"start\":42939},{\"end\":42966,\"start\":42951},{\"end\":42980,\"start\":42966},{\"end\":42990,\"start\":42980},{\"end\":43415,\"start\":43403},{\"end\":43430,\"start\":43415},{\"end\":43444,\"start\":43430},{\"end\":43454,\"start\":43444},{\"end\":43905,\"start\":43894},{\"end\":43920,\"start\":43905},{\"end\":43933,\"start\":43920},{\"end\":43942,\"start\":43933},{\"end\":44336,\"start\":44322},{\"end\":44354,\"start\":44336},{\"end\":44375,\"start\":44354},{\"end\":44704,\"start\":44686},{\"end\":44722,\"start\":44704},{\"end\":44734,\"start\":44722},{\"end\":44757,\"start\":44734},{\"end\":44772,\"start\":44757},{\"end\":44788,\"start\":44772},{\"end\":44802,\"start\":44788},{\"end\":44811,\"start\":44802},{\"end\":44824,\"start\":44811},{\"end\":44838,\"start\":44824},{\"end\":45252,\"start\":45238},{\"end\":45268,\"start\":45252},{\"end\":45287,\"start\":45268},{\"end\":45306,\"start\":45287},{\"end\":45318,\"start\":45306},{\"end\":45332,\"start\":45318},{\"end\":45351,\"start\":45332},{\"end\":45744,\"start\":45732},{\"end\":45758,\"start\":45744},{\"end\":45762,\"start\":45758},{\"end\":45917,\"start\":45905},{\"end\":45932,\"start\":45917},{\"end\":45949,\"start\":45932},{\"end\":46056,\"start\":46050},{\"end\":46065,\"start\":46056},{\"end\":46075,\"start\":46065},{\"end\":46082,\"start\":46075},{\"end\":46092,\"start\":46082},{\"end\":46341,\"start\":46335},{\"end\":46726,\"start\":46720},{\"end\":46730,\"start\":46726},{\"end\":46736,\"start\":46730},{\"end\":46742,\"start\":46736},{\"end\":46751,\"start\":46742},{\"end\":46755,\"start\":46751},{\"end\":46966,\"start\":46958},{\"end\":46978,\"start\":46966},{\"end\":46987,\"start\":46978},{\"end\":46997,\"start\":46987},{\"end\":47006,\"start\":46997},{\"end\":47017,\"start\":47006},{\"end\":47029,\"start\":47017},{\"end\":47414,\"start\":47405},{\"end\":47428,\"start\":47414},{\"end\":47440,\"start\":47428},{\"end\":47754,\"start\":47741},{\"end\":47765,\"start\":47754},{\"end\":47783,\"start\":47765},{\"end\":47795,\"start\":47783},{\"end\":47810,\"start\":47795},{\"end\":47828,\"start\":47810},{\"end\":47842,\"start\":47828},{\"end\":47855,\"start\":47842},{\"end\":47864,\"start\":47855},{\"end\":48266,\"start\":48248},{\"end\":48280,\"start\":48266},{\"end\":48292,\"start\":48280},{\"end\":48306,\"start\":48292},{\"end\":48321,\"start\":48306},{\"end\":48332,\"start\":48321},{\"end\":48344,\"start\":48332},{\"end\":48357,\"start\":48344},{\"end\":48374,\"start\":48357},{\"end\":48392,\"start\":48374},{\"end\":48874,\"start\":48852},{\"end\":48883,\"start\":48874},{\"end\":48898,\"start\":48883},{\"end\":48903,\"start\":48898},{\"end\":49296,\"start\":49285},{\"end\":49322,\"start\":49296},{\"end\":49335,\"start\":49322},{\"end\":49355,\"start\":49335},{\"end\":49369,\"start\":49355},{\"end\":49758,\"start\":49742},{\"end\":49770,\"start\":49758},{\"end\":49792,\"start\":49770},{\"end\":50043,\"start\":50027},{\"end\":50061,\"start\":50043},{\"end\":50374,\"start\":50357},{\"end\":50397,\"start\":50374},{\"end\":50419,\"start\":50397},{\"end\":50431,\"start\":50419},{\"end\":50439,\"start\":50431},{\"end\":50838,\"start\":50812},{\"end\":50858,\"start\":50838},{\"end\":50871,\"start\":50858},{\"end\":50890,\"start\":50871},{\"end\":51270,\"start\":51260},{\"end\":51282,\"start\":51270},{\"end\":51289,\"start\":51282},{\"end\":51298,\"start\":51289},{\"end\":51309,\"start\":51298},{\"end\":51316,\"start\":51309},{\"end\":51326,\"start\":51316},{\"end\":51342,\"start\":51326},{\"end\":51354,\"start\":51342},{\"end\":51363,\"start\":51354},{\"end\":51810,\"start\":51799},{\"end\":51820,\"start\":51810},{\"end\":51832,\"start\":51820},{\"end\":51847,\"start\":51832},{\"end\":51864,\"start\":51847},{\"end\":51885,\"start\":51864},{\"end\":52263,\"start\":52255},{\"end\":52274,\"start\":52263},{\"end\":52287,\"start\":52274},{\"end\":52300,\"start\":52287},{\"end\":52312,\"start\":52300},{\"end\":52608,\"start\":52600},{\"end\":52617,\"start\":52608},{\"end\":52628,\"start\":52617},{\"end\":52638,\"start\":52628},{\"end\":52649,\"start\":52638},{\"end\":53042,\"start\":53031},{\"end\":53052,\"start\":53042},{\"end\":53062,\"start\":53052},{\"end\":53072,\"start\":53062},{\"end\":53088,\"start\":53072},{\"end\":53452,\"start\":53439},{\"end\":53467,\"start\":53452},{\"end\":53480,\"start\":53467},{\"end\":53491,\"start\":53480},{\"end\":53505,\"start\":53491},{\"end\":53517,\"start\":53505},{\"end\":53990,\"start\":53979},{\"end\":54002,\"start\":53990},{\"end\":54010,\"start\":54002},{\"end\":54191,\"start\":54178},{\"end\":54207,\"start\":54191},{\"end\":54227,\"start\":54207},{\"end\":54591,\"start\":54579},{\"end\":54606,\"start\":54591},{\"end\":54623,\"start\":54606},{\"end\":54635,\"start\":54623},{\"end\":54653,\"start\":54635},{\"end\":55162,\"start\":55148},{\"end\":55181,\"start\":55162},{\"end\":55204,\"start\":55181}]", "bib_venue": "[{\"end\":37921,\"start\":37859},{\"end\":43111,\"start\":43059},{\"end\":43595,\"start\":43533},{\"end\":53658,\"start\":53596},{\"end\":54794,\"start\":54732},{\"end\":37362,\"start\":37304},{\"end\":37857,\"start\":37780},{\"end\":38248,\"start\":38199},{\"end\":38648,\"start\":38602},{\"end\":39044,\"start\":38996},{\"end\":39480,\"start\":39422},{\"end\":39835,\"start\":39829},{\"end\":40271,\"start\":40185},{\"end\":40750,\"start\":40693},{\"end\":41078,\"start\":41065},{\"end\":41496,\"start\":41410},{\"end\":42038,\"start\":42001},{\"end\":42564,\"start\":42560},{\"end\":43057,\"start\":42990},{\"end\":43531,\"start\":43454},{\"end\":43990,\"start\":43942},{\"end\":44320,\"start\":44215},{\"end\":44854,\"start\":44850},{\"end\":45397,\"start\":45351},{\"end\":45778,\"start\":45762},{\"end\":45903,\"start\":45882},{\"end\":46389,\"start\":46341},{\"end\":46768,\"start\":46755},{\"end\":47118,\"start\":47045},{\"end\":47513,\"start\":47440},{\"end\":47970,\"start\":47880},{\"end\":48479,\"start\":48408},{\"end\":48968,\"start\":48903},{\"end\":49420,\"start\":49369},{\"end\":49740,\"start\":49699},{\"end\":50025,\"start\":49959},{\"end\":50481,\"start\":50439},{\"end\":50810,\"start\":50760},{\"end\":51367,\"start\":51363},{\"end\":51929,\"start\":51885},{\"end\":52349,\"start\":52312},{\"end\":52706,\"start\":52649},{\"end\":53134,\"start\":53088},{\"end\":53594,\"start\":53517},{\"end\":53977,\"start\":53925},{\"end\":54296,\"start\":54227},{\"end\":54730,\"start\":54653},{\"end\":55245,\"start\":55204}]"}}}, "year": 2023, "month": 12, "day": 17}