{"id": 243985620, "updated": "2023-10-05 20:08:35.273", "metadata": {"title": "Spatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision Prediction", "authors": "[{\"first\":\"Arnav\",\"last\":\"Malawade\",\"middle\":[\"V.\"]},{\"first\":\"Shih-Yuan\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Brandon\",\"last\":\"Hsu\",\"middle\":[]},{\"first\":\"Deepan\",\"last\":\"Muthirayan\",\"middle\":[]},{\"first\":\"Pramod\",\"last\":\"Khargonekar\",\"middle\":[\"P.\"]},{\"first\":\"Mohammad\",\"last\":\"Faruque\",\"middle\":[\"A.\",\"Al\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 11, "day": 11}, "abstract": "In autonomous vehicles (AVs), early warning systems rely on collision prediction to ensure occupant safety. However, state-of-the-art methods using deep convolutional networks either fail at modeling collisions or are too expensive/slow, making them less suitable for deployment on AV edge hardware. To address these limitations, we propose sg2vec, a spatio-temporal scene-graph embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) layers to predict future collisions via visual scene perception. We demonstrate that sg2vec predicts collisions 8.11% more accurately and 39.07% earlier than the state-of-the-art method on synthesized datasets, and 29.47% more accurately on a challenging real-world collision dataset. We also show that sg2vec is better than the state-of-the-art at transferring knowledge from synthetic datasets to real-world driving datasets. Finally, we demonstrate that sg2vec performs inference 9.3x faster with an 88.0% smaller model, 32.4% less power, and 92.8% less energy than the state-of-the-art method on the industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for implementation on the edge.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.06123", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/iotj/MalawadeYHMKF22", "doi": "10.1109/jiot.2022.3141044"}}, "content": {"source": {"pdf_hash": "7dfd9b5cc96488aba8ae3308a94b70ebc3a6d318", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.06123v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "62c8ea275d962af684791162578a6d17ac5d7324", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7dfd9b5cc96488aba8ae3308a94b70ebc3a6d318.txt", "contents": "\nSpatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision Prediction\n\n\nArnav V Malawade malawada@uci.edu \nDepartment of Electrical Engineering & Computer Science\nUniversity of California -Irvine\n92697IrvineCAUSA\n\nShih-Yuan Yu shihyuay@uci.edu \nDepartment of Electrical Engineering & Computer Science\nUniversity of California -Irvine\n92697IrvineCAUSA\n\nBrandon Hsu bdhsu@uci.edu \nDepartment of Electrical Engineering & Computer Science\nUniversity of California -Irvine\n92697IrvineCAUSA\n\nDeepan Muthirayan \nDepartment of Electrical Engineering & Computer Science\nUniversity of California -Irvine\n92697IrvineCAUSA\n\nPramod P Khargonekar pramod.khargonekar@uci.edu \nDepartment of Electrical Engineering & Computer Science\nUniversity of California -Irvine\n92697IrvineCAUSA\n\nMohammad A Al Faruque alfaruqu@uci.edu \nDepartment of Electrical Engineering & Computer Science\nUniversity of California -Irvine\n92697IrvineCAUSA\n\nSpatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision Prediction\n1Index Terms-scene graphcollision predictionautonomous vehiclesgraph learningADAS\nIn autonomous vehicles (AV), early warning systems rely on collision prediction to ensure occupant safety. However, state-of-the-art methods using deep convolutional networks either fail at modeling collisions or are too expensive/slow, making them less suitable for deployment on AV edge hardware. To address these limitations, we propose SG2VEC, a spatio-temporal scenegraph embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) layers to predict future collisions via visual scene perception. We demonstrate that SG2VEC predicts collisions 8.11% more accurately and 39.07% earlier than the state-of-the-art method on synthesized datasets, and 29.47% more accurately on a challenging real-world collision dataset. We also show that SG2VEC is better than the state-ofthe-art at transferring knowledge from synthetic datasets to realworld driving datasets. Finally, we demonstrate that SG2VEC performs inference 9.3x faster with an 88.0% smaller model, 32.4% less power, and 92.8% less energy than the state-of-the-art method on the industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for implementation on the edge.\n\nI. INTRODUCTION\n\nThe synergy of Artificial Intelligence (AI) and the Internet of Things (IoT) has accelerated the advancement of Autonomous Vehicle (AV) technologies, which is expected to revolutionize transportation by reducing traffic and improving road safety [1], [2]. However, recent reports of AV crashes suggest that there are still significant limitations. For example, multiple fatal Tesla Autopilot crashes can primarily be attributed to perception system failures [3], [4]. Additionally, the infamous fatal collision between an Uber self-driving vehicle and a pedestrian can be attributed to perception and prediction failures by the AV [5]. These accidents (among others) have eroded public trust in AVs, and nearly 50% or more of the public have expressed their mistrust in AVs [6]. Current statistics indicate that perception and prediction errors were factors in over 40% of driver-related crashes between conventional vehicles [7]. However, a significant number of reported AV collisions are also the result of these errors [8], [9]. Thus, in this paper, we aim to improve the safety and acceptance of AVs by incorporating scene-graphs into the perception pipeline of collision prediction systems to improve scene understanding. * Arnav V. Malawade and Shih-Yuan Yu contributed equally to this article.\n\nFor the past several years, automotive manufacturers have begun equipping consumer vehicles with statistics-based collision avoidance systems based on calculated Single Behavior Threat Metrics (SBTMs) such as Time to Collision (TTC), Time to React (TTR), etc. [10], [11]. However, these methods lack robustness since they make significant assumptions about the behavior of vehicles on the road. A very limiting assumption they make is that vehicles do not diverge from their current trajectories [11]. SBTMs can also fail in specific scenarios. For example, TTC can fail when following a vehicle at the same velocity within a very short distance [11]. As a result, these methods are less capable of generalizing and can perform poorly in complex road scenarios. Moreover, to reduce false positives, these systems are designed to respond at the last possible moment [12]. Under such circumstances, the AV control system can fail to take timely corrective actions [13] if the system fails to predict a collision or estimates the TTC inaccurately.\n\nMore effective collision prediction methods using Deep Learning (DL) have also been proposed in the literature. However, these approaches can be limited because they do not explicitly capture the relationships between the objects in a traffic scene. Understanding these relationships could be critical as it is suggested that a human's ability to understand complex scenarios and identify potential risks relies on cognitive mechanisms for representing structure and reasoning about inter-object relationships [14]. These models also require large datasets that are often costly or unsafe to generate. Synthetic datasets are typically used to augment the limited real-world data to train the models in such cases [15]. However, these trained models must then be able to transfer the knowledge gained from synthetic datasets to real-world driving scenarios. Furthermore, DL models contain millions of parameters and require IoT edge devices with significant computational power and memory to run efficiently. Likewise, hosting these models on the cloud is infeasible because it requires persistent lowlatency internet connections.\n\nIn summary, the key research challenges associated with autonomous vehicle collision prediction are: 4) Developing algorithms that can run efficiently on automotive IoT edge devices. In this work, we propose the use of scene-graphs to represent road scenes and model inter-object relationships as we hypothesize that this will improve perception and scene understanding. Recently, several works have shown that graphbased methods that capture and model complex relationships between entities can improve performance at high-level tasks such as behavior classification [16], [17] and semantic segmentation [18]. Scene-graphs are used in many domains to abstract the locations and attributes of objects in a scene and model inter-object relationships [16], [17], [19]. In our prior work, [20], we demonstrated that a scene-graph sequence classification approach can better assess the subjective risk of driving clips compared to a conventional CNN+LSTMbased approach; our approach could also better generalize and transfer knowledge across datasets. This paper extends the approach presented in [20] to collision prediction by enabling the prediction of future road states via changes to the temporal modeling components of the architecture and changes in the problem formulation. The scene-graph representation we propose represents traffic objects as nodes and the relationships between them as edges. The novelty of our scene-graph representation lies in our graph construction technique that is specifically designed for higher-level AV decisions such as collision prediction. One of our key contributions is showing that a graph-based intermediate representation can be very effective and efficient for higher-level AV tasks directly related to driving decisions.\n\nOur proposed methodology for collision prediction, SG2VEC, is shown in Figure 1. It combines the scene-graph representation with a graph-embedding architecture to generate a sequence of scene-graph embeddings for the sequence of visual inputs perceived by an AV. The graph embedding tech-nique we use is based on the core MR-GCN framework [16] adapted for the collision prediction problem. The sequence of graph embeddings is then input to a Long Short-Term Memory (LSTM) network to make the final prediction on the possibility of a future collision. To the best of our knowledge, our work is the first to propose using scene-graphs for early collision prediction.\n\nOur paper makes the following key research contributions: 1) We demonstrate that our SG2VEC collision prediction methodology significantly outperforms the current state of the art on simulated lane-change datasets and a very challenging real-world collision dataset containing a wide range of driving actions, collision types, and weather/road conditions. 2) We demonstrate that SG2VEC can transfer knowledge gained from simulated data to real-world driving data more effectively than the state-of-the-art method. 3) We show that SG2VEC performs faster inference and requires less power than the state-of-the-art method on the industry-standard Nvidia DRIVE PX 2 autonomous driving hardware platform, used in all 2016-2018 Tesla models for their Autopilot system [21].\n\n\nII. RELATED WORK\n\n\nA. Early Collision Prediction\n\nSince collision prediction is key to the safety of AVs, a wide range of solutions have been proposed by academia and industry. As mentioned earlier, current consumer vehicles use statistics-based SBTMs for collision prediction but can perform poorly in complex situations [10], [11] or react too late to avoid collisions [12], [13]. Expanding on these approaches, companies like Mobileye and Nvidia have proposed more comprehensive mathematical models for ensuring AV safety, namely Responsibility-Sensitive Safety (RSS) [22] and Nvidia Safety Force Field [23], respectively. However, these models are heavily rule-based and can thus be fragile in complex situations with high uncertainty. Additionally, computing future trajectory constraints with RSS is non-trivial and can require vehicle-specific calibration [24].\n\nModel-based probabilistic and deep learning approaches for collision prediction have also been proposed. For example, [25] proposes a model-based probabilistic technique that uses the roadway geometry, ego trajectory, and position/velocity of road objects to predict future object positions. However, this model is highly conservative and is likely to have a high false-positive rate. Similarly, [26] and [27] use modelbased approaches but require significant domain knowledge about the driving scene, such as road geometry information as well as accurate vehicle position and velocity information. [28] proposes a deep learning collision prediction approach. Still, due to its use of pre-processed trajectory data captured from cameras overlooking a highway, it is not ego-centric and cannot be practically used for on-vehicle collision prediction. In a different approach, [29] proposes a Deep Predictive Model (DPM) that used a Bayesian Convolutional LSTM for collision risk assessment where image data, vehicle telemetry data, and driving inputs were all factors in the risk assessment decision.\n\nHowever, this approach was only evaluated on simulated street scenes containing two vehicles and no other dynamic objects. Thus, DPM's performance may suffer when evaluated on more complex road scenarios.\n\nIn contrast to these existing works, we propose SG2VEC which captures structural and relational information of a road scene in a scene-graph representation and computes a spatiotemporal embedding to predict collisions. Additionally, we perform experiments that were not done in many prior works, such as evaluating each model's capability to transfer knowledge, efficiency on AV hardware, performance on a complex real-world crash dataset, and ability to predict collisions early. We primarily compare our methodology with the DPM as it is the state-of-the-art data-driven collision prediction framework for AVs that considers both spatial and temporal factors. Although the DPM uses multiple modalities for sensing, the results in [29] show that it achieves an accuracy (of 81.95%) that is just 0.24% less using just the image sensing modality. In this work, we compare our proposed SG2VEC methodology and the DPM on image-only datasets, which is fair because the DPM's performance does not vary much with the inclusion of other modalities.\n\n\nB. AV Scene-Graphs and Optimization Techniques\n\nSeveral works have proposed graph-based methods for scene understanding. For example, [16] proposed a multi-relational graph convolutional network (MR-GCN) that uses both spatial and temporal information to classify vehicle driving behavior. Similarly, in [17], an Ego-Thing and Ego-Stuff graph are used to model and classify the ego vehicle's interactions with moving and stationary objects, respectively. In our prior work, we demonstrated that a scene-graph sequence embedding approach assesses driving risk better than the state-of-the-art CNN-LSTM approach [20]. In [20], we utilized an architecture consisting of MR-GCN layers for spatial modeling and an LSTM with attention for temporal modeling; however, this architecture was only capable of performing binary sequencelevel classification over a complete video clip. Thus, although our prior architecture could accurately assess the subjective risk of complete driving sequences, it was not capable of predicting the future state of a scene.\n\nCurrent autonomous driving systems consume a substantial amount of power (up to 500 Watts for the Nvidia DRIVE AGX Pegasus), demanding more robust cooling and power delivery mechanisms. Thus, many have tried to optimize AV tasks for efficiency without sacrificing performance. Existing approaches have proposed methods for jointly optimizing power consumption and latency for localization [30], perception [31], and control [32]. However, to the best of our knowledge, no work has explored this optimization for AV safety systems, such as collision prediction systems.\n\n\nIII. SCENE-GRAPH EMBEDDING METHODOLOGY\n\nIn SG2VEC, we formulate the problem of collision prediction as a time-series classification problem where the goal is to predict if a collision will occur in the near future. Our goal is to accurately model the spatio-temporal function f , where\nY n = f ({I 1 , ..., I n\u22121 , I n }), Y n \u2208 {0, 1}, for n > 2,(1)\nwhere Y n = 1 implies a collision in the near future and Y n = 0 otherwise. Here the variable I n denotes the image captured by the on-board camera at time n. The interval between each frame varies with the camera sampling rate. SG2VEC consists of two parts ( Figure 3) : (i) the scenegraph extraction, and (ii) collision prediction through spatiotemporal embedding, described in Section III-A and Section III-B respectively.\n\n\nA. Scene-Graph Extraction\n\nThe first step of our methodology is the extraction of scenegraphs for the images of a driving scene. The extraction pipeline forms the scene-graph for an image as in [33], [34] by first detecting the objects in the image and then identifying their relations based on their attributes. The difference from prior works lies in the construction of a scene-graph that is designed for higher-level AV decisions. We propose extracting a minimal set of relations such as directional relations and proximity relations. From our design space exploration we found that adding many relation edges to the scene-graph adds noise and impacts convergence while using too few relation types reduces our model's expressivity. The best approach we found across applications involves constructing mostly ego-centric relations for a moderate range of relation types. Figure 2 shows an example of the graph extraction process.\n\nWe denote the extracted scene-graph for the frame I n by G n = {O n , A n }. Each scene-graph G n is a directed, heterogeneous multi-graph, where O n denotes the nodes and A n is the adjacency matrix of the graph G n . As shown in Fig. 2, nodes represent the identified objects such as lanes, roads, traffic signs, vehicles, pedestrians, etc., in a traffic scene. The adjacency matrix A n indicates the pair-wise relations between each object in O n . The extraction pipeline first identifies the objects O n by using Mask R-CNN [35]. Then, it generates an inverse perspective mapping (also known as a \"birds-eye view\" projection) of the image to estimate the locations of objects relative to the ego car, which are used to is assigned to a pair of objects, in this case between the ego-car and another car in the view, based on their relative orientation and only if they are within the Near threshold distance from one another. Additionally, the isIn relation identifies which vehicles are on which lanes (see Fig. 2). We use each vehicle's horizontal displacement relative to the ego vehicle to assign vehicles to either the Left Lane, Middle Lane, or Right Lane using the known lane width. Our abstraction only considers three-lane areas, and, as such, we map vehicles in all left lanes and all right lanes to the same Left Lane node Right Lane node respectively. If a vehicle overlaps two lanes (i.e., during a lane change), it is mapped to both lanes.\n\n\nGraph Readout\n\n\nScene-Graph\n\n\nB. Collision Prediction\n\nAs shown in Figure 3, in our collision prediction methodology, each image I n is first converted into a scene-graph G n = {O n , A n } with the pipeline mentioned in Section III-A. Each node v \u2208 O n is initialized by a one-hot vector (embedding), denoted by h (0) v . Then, the MR-GCN [36] layers are used to update these embeddings via the edges in A n . Specifically, the l-th MR-GCN layer computes the node embedding for each node v, denoted as h (l) v , as follows:\nh (l) v = \u03a6 0 \u00b7 h (l\u22121) v + r\u2208An u\u2208Nr(v) 1 |N r (v)| \u03a6 r \u00b7 h (l\u22121) u ,(2)\nwhere N r (v) denotes the set of neighbors of node v with respect to the relation r \u2208 A n , \u03a6 r is a trainable relation-specific transformation for relation r, and \u03a6 0 is the self-connection for each node v that accounts for the influence of h The final embeddings for scene-graph G n , denoted by X prop n , are then passed through a self-attention graph pooling (SAGPooling) layer that filters out irrelevant nodes from the graph, creating the pooled set of node embeddings X pool n and their edges A pool n . In this layer, we use a graph convolution layer to predict the score \u03b1 = SCORE(X prop n , A prop n ) and then use \u03b1 to perform top-k filtering to filter out the irrelevant nodes in the scene-graph [37].\n\nThen, for each scene-graph G n , the corresponding X pool n is passed through the graph readout layer that condenses the node embeddings (using operations such as sum, mean, max, etc.) to a single graph embedding h Gn . Then, this spatial embedding h Gn is passed to the temporal model (LSTM) to generate a spatio-temporal embedding z n as follows:\nz n , s n = LSTM(h Gn , s n\u22121 )(3)\nThe hidden state s n of the LSTM is updated for each timestamp n. Lastly, each spatio-temporal embedding z n is then passed through a Multi-Layer Perceptron (MLP) that outputs each class's confidence value. The two outputs of the MLP are compared, and\u0176 n is set to the index of the class with the greater confidence value (0 for no-collision or 1 for collision). During training, we calculate the cross-entropy loss between each set of non-binarized outputs\u0176 n and the corresponding labels for backpropagation.\n\n\nIV. EXPERIMENTAL RESULTS\n\nThis section provides extensive experimental results to demonstrate SG2VEC's performance, efficiency, and transferability compared to the state-of-the-art collision prediction model, DPM [29]. For SG2VEC, we used 2 MR-GCN layers, each of size 64, one SAGPooling layer with a pooling ratio of 0.25, one add-readout layer, one LSTM layer with hidden size 20, one MLP layer with an output of size 2, and a LogSoftmax to generate the final confidence value for each class. For the DPM, we followed the architecture used in [29], which uses one 64x64x5 Convolutional LSTM (ConvLSTM) layer, one 32x32x5 ConvLSTM layer, one 16x16x5 ConvLSTM layer, one MLP layer with output size 64, one MLP layer with output size 2, and a Softmax to generate the final confidence value. For both models, we used a dropout of 0.1 and ReLU activation. The learning rates were 0.00005 for SG2VEC and 0.0001 for DPM. We ran the experiments shown in Sections IV-B and IV-C on a Windows PC with an AMD Ryzen Threadripper 1950X processor, 16 GB RAM, and an Nvidia GeForce RTX 2080 Super GPU.\n\n\nA. Dataset Preparation\n\nWe prepared three types of datasets for our experiments: (i) synthesized datasets, (ii) a typical real-world driving dataset, and (iii) a complex real-world driving dataset. Examples from each dataset are shown in Figure 4. Our synthetic datasets focus on the highway lane change scenario as it is a common AV task. To evaluate the transferability of each model from synthetic datasets to real-world driving, we prepared a typical real-world dataset containing lane-change driving clips. Finally, we prepared the complex real-world driving dataset to evaluate each model's performance on a challenging dataset containing a broad spectrum of collision types, road conditions, and vehicle maneuvers. All datasets were collected at a 1280x720 resolution, and each clip spans 1-5 seconds.\n\n1) Synthetic Datasets: To synthesize the datasets, we developed a tool using CARLA [15], an open-source driving simulator, and CARLA Scenario Runner 1 to generate lane change video clips with/without collisions. We generated a wide range of simulated lane changes with different numbers of cars, pedestrians, weather and lighting conditions, etc. We also customized each vehicle's driving behavior, such as their intended speed, probability of ignoring traffic lights, or the chance of avoiding collisions with other vehicles. We generated two synthetic datasets: a 271-syn dataset and a 1043syn dataset, containing 271 and 1,043 video clips, respectively. These datasets have no-collision:collision label distributions of 6.12:1 and 7.91:1, respectively. In addition, we sub-sampled the 1043-syn dataset to create 306-syn: a balanced dataset that has a 1:1 distribution. Our synthetic scene-graph datasets 2 and our source code 3 are open-source and available online.\n\n2) Typical Real-World Driving Dataset: This dataset, denoted as 571-honda, is a subset of the Honda Driving Dataset (HDD) [38] containing 571 lane-change video clips from realworld driving with a distribution of 7.21:1. The HDD was recorded on the same vehicle during mostly safe driving in the California Bay Area.\n\n3) Complex Real-World Driving Dataset: Our complex real-world driving dataset, denoted as 620-dash, contains very challenging real-world collision scenarios drawn from the Detection of Traffic Anomaly dataset [39]. This dataset contains a wide range of drivers, car models, driving maneuvers, weather/road conditions, and collision types, as recorded by on-board dashboard cameras. Since the original dataset contains only collision clips, we prepared 620-dash by splitting  each clip in the original dataset into two parts: (i) the beginning of the clip until 1 second before the collision, and (ii) from 1 second before the collision until the end of the collision. We then labeled part (i) as 'no-collision' and part (ii) as 'collision.'\n\nThe 620-dash dataset contains 315 collision video clips and 342 non-collision driving clips.\n\n\n4) Labeling and Pre-Processing:\n\nWe labeled the synthetic datasets and the 571-honda dataset using human annotators. The final label assigned to a clip is the average of the labels assigned by the human annotators rounded to 0 (no collision) and 1 (collision/near collision). Each frame in a video clip is given a label identical to the entire clip's label to train the model to identify the preconditions of a future collision.\n\nFor SG2VEC, all the datasets were pre-processed using the scene-graph extraction pipeline mentioned in Section III-A to construct the scene-graphs for each video clip. For a given sequence, SG2VEC can leverage the full history of prior frames for each new prediction. For the DPM, the datasets were pre-processed to match the input format used in its original implementation [29]. Thus, the DPM uses 64x64 grayscale versions of the clips in the datasets turned into sets of subsequences J n for a clip of length l defined as follows.\n\nJ n = {I n , I n+1 , I n+2 , I n+3 , I n+4 }, for n \u2208 [1, l \u2212 4] (4)\n\nSince DPM only uses five prior frames to make each prediction, we also present results for SG2VEC using the same length of history, denoted as SG2VEC (5-frames) in the results.\n\n\nB. Collision Prediction Performance\n\nWe evaluated SG2VEC and the DPM using classification accuracy, area under the ROC curve (AUC) [40], and Matthews Correlation Coefficient (MCC) [41]. MCC is considered a balanced measure of performance for binary classification even on datasets with significant class imbalances. The MCC score outputs a value between -1.0 and 1.0, where 1.0 corresponds to a perfect classifier, 0.0 to a random classifier, and -1.0 to an always incorrect classifier. Although class re-weighting helps compensate for the dataset imbalance during training, classification accuracy is typically less reliable for imbalanced datasets, so the primary metric we use to compare the models is MCC. We used stratified 5-fold cross-validation to produce the final results shown in Table I and Figure 5.  Fig. 4: Examples of driving scenes from our a) synthetic datasets, b) typical real-world dataset, and c) complex real-world dataset. In a), all driving scenes occur on highways with the same camera position and clearly defined road markings; lighting and weather are dynamically simulated in CARLA. In b) driving scenes occur on multiple types of clearly marked roads but lighting, camera angle, and weather are consistent across scenes. c) contains a much broader range of camera angles as well as more diverse weather and lighting conditions, including rain, snow, and night-time driving; it also contains a large number of clips on unpaved or unmarked roadways, as shown.\n\n\n1) Synthetic Datasets:\n\nThe performance of SG2VEC and the DPM on our synthetic datasets is shown in Table I. We find that our SG2VEC achieves higher accuracy, AUC, and MCC on every dataset, even when only using five prior frames as input. In addition to predicting collisions more accurately, SG2VEC also infers 5.5x faster than the DPM on average. We attribute this to the differences in model complexity between our SG2VEC architecture and the much larger DPM model. Interestingly, SG2VEC (5-frames) achieves slightly better accuracy and AUC than SG2VEC on the imbalanced datasets and slightly lower overall performance on the balanced datasets. This is likely because the large number of safe lane changes in the imbalanced datasets adds noise during training and makes the full-history version of the model perform slightly worse. However, the full model can learn long-tail patterns for collision scenarios and performs better on the balanced datasets.\n\nThe DPM achieves relatively high accuracy and AUC on the imbalanced 271-syn and 1043-syn datasets, but suffers significantly on the balanced 306-syn dataset. This drop indicates that the DPM could not identify the minority class (collision) well and tended to over-predict the majority class (no-collision). In terms of MCC, the DPM scores higher on the 306-syn dataset than what it scores on the other datasets. This result is because the 306-syn dataset has a balanced class distribution compared to the other datasets, which could enable the DPM to improve its prediction accuracy on the collision class.\n\nIn contrast, the SG2VEC methodology performs well on both balanced and imbalanced synthetic datasets with an average MCC of 0.5860, an average accuracy of 87.97%, and an average AUC of 0.9369. Since MCC is scaled from -1.0 to 1.0, SG2VEC achieves a 14.72% higher average MCC score than the DPM model.\n\nThe results from our SG2VEC ablation study are shown in Table III and support our hypothesis that both spatial modeling with MRGCN and temporal modeling with LSTM are core to SG2VEC's collision prediction performance. However, the MRGCN appears to be slightly more critical to performance than the LSTM. Interestingly the choice of pooling layer (no pooling, Top-K pooling, or SAG Pooling) does not seem to significantly affect performance at this task as long as LSTM is used; when no LSTM is used SAG Pooling presents a clear performance improvement.\n\n2) Complex Real-World Dataset: The performance of both the models significantly drops on the highly complex realworld 620-dash dataset due to the variations in the driving scenes and collision scenarios. This drop is to be expected as this dataset contains a wide range of driving actions, road environments, and collision scenarios, increasing the difficulty of the problem significantly. We took several steps to try and address this performance drop. First, we improved the birdseye view (BEV) calibration on this dataset in comparison to the other datasets. Since the varying camera angles and road conditions in this dataset impact our ability to properly calibrate SG2VEC's BEV projection in a single step, we created custom BEV calibrations for each clip in the dataset, which improved performance somewhat. However, as shown in Figure 4c, a significant part of the dataset consists of driving clips on roads without any discernible lane markings, such as snowy, unpaved, or unmarked roadways. These factors make it challenging to correlate known fixed distances (i.e., the width and length of lane markings) with the projections of these clips. To further improve performance on this particular dataset, we performed extensive architecture and hyperparameter tuning. We found that, with one MRGCN layer of size 64, one LSTM layer with hidden size 100, no SAGPooling layer, and a  high learning rate and batch size, we achieved significantly better performance than the model architecture discussed in the beginning of Section IV (2 MRGCN layers of size 64, one LSTM layer with hidden size 20, and a SAGPooling layer with a keep ratio of 0.5). We believe this indicates that the temporal features of each clip in this dataset are more closely related to collision likelihood than the spatial features in each clip. As a result, the additional spatial modeling components were likely causing overfitting and skewing the spatial embedding output.\n\nThe spatial embeddings remained more general with a simpler spatial model (1 MRGCN and no SAGPooling). This change, combined with using a larger LSTM layer, enabled the model to capture more temporal features when modeling each clip and better generalize to the testing set. Model performance on this dataset and similar datasets could likely be improved by acquiring more consistent data via higher-resolution cameras with fixed camera angles and more accurate BEV projection approaches. However, as collisions are rare events, there are little to no datasets containing real-world collisions that meet these requirements. Despite these limitations, SG2VEC outperforms the DPM model by a significant margin, achieving 21.17% higher accuracy, 31.40% higher AUC, and a 21.92% higher MCC score. Since DPM achieves a negative MCC score, its performance on this dataset is worse than that of a random classifier (MCC of 0.0). Consistent with the synthetic dataset results, sg2vec using all frames performs better on the balanced 620-dash dataset than SG2VEC (5-frames). Overall, these results show that, on very challenging and complex realworld driving scenarios, SG2VEC can perform much better than the current state-of-the-art.\n\n\n3) Time of Prediction:\n\nSince collision prediction is a timesensitive problem, we evaluated our methodology and the DPM on their average time-of-prediction (ATP) for video clips containing collisions. To calculate the ATP, we recorded the first frame index in each collision clip when the model correctly predicts that a collision would occur. We then averaged these indices and compared them with the average collision video clip length. Essentially, ATP gives an estimate of how early each model can predict a future collision. These results are shown in Table II. On the 1043-syn dataset, SG2VEC achieves 0.1725 for the ratio of the ATP and the average sequence length while the DPM achieves a ratio of 0.2382, indicating that SG2VEC predicts future collisions 39.07% earlier than the DPM on average. In the context of real-world collision prediction, the average sequence in the 1043-syn dataset represents 1.867 seconds of data. Thus, our methodology predicted collisions 122.7 milliseconds earlier than DPM on average. This extra time can be critical for ensuring that the AV avoids an impending collision.   \n\n\nC. Transferability From Synthetic to Real-World Datasets\n\nThe collision prediction models trained on simulated datasets must be transferable to real-world driving as it can differ significantly from simulations. To evaluate each model's ability to transfer knowledge, we trained each model on a synthetic dataset before testing it on the 571-honda dataset. No additional domain adaptation was performed. We did not evaluate transferability to the 620-dash dataset because it contains a wide range of highly dynamic driving maneuvers that were not present in our synthesized datasets. As such, evaluating transferability between our synthesized datasets and the 620-dash dataset would yield poor performance and would not provide insight. Figure 5 compares the accuracy and MCC for both the models on each training dataset and the 571-honda dataset after transferring the trained model.\n\nWe observe that the SG2VEC model achieves a significantly higher MCC score than the DPM model after the transfer, suggesting that our methodology can better transfer knowledge from a synthetic to a real-world dataset compared to the stateof-the-art DPM model. The drop in MCC values observed for both the models when transferred to the 571-honda dataset can be attributed to the characteristic differences between the simulated and real-world datasets; the 571-honda dataset contains a more heterogeneous set of road environments, lighting conditions, driving styles, etc., so a drop in performance after the transfer is expected. We also note that the MCC score for the SG2VEC model trained on 271-syn dataset drops more than the model trained on the 1043-syn dataset after the transfer, likely due to the smaller training dataset size. Regarding accuracy, the SG2VEC model trained on 1043-syn achieves 4.37% higher accuracy and the model trained 271-syn dataset achieves 1.47% lower accuracy than the DPM model trained on the same datasets. The DPM's similar accuracy after transfer likely results from the class imbalance in the 571honda dataset. Overall, we hypothesize that SG2VEC's use of an intermediate representation (i.e., scene-graphs) inherently improves its ability to generalize and thus results in an improved ability to transfer knowledge compared to CNNbased deep learning approaches.\n\n\nD. Evaluation on Industry-Standard AV Hardware\n\nTo demonstrate that the SG2VEC is implementable on industry-standard AV hardware, we measured its inference time (milliseconds), model size (kilobytes), power consumption (watts), and energy consumption per frame (milli-joules) on the industry-standard Nvidia DRIVE PX 2 platform, which was used by Tesla for their Autopilot system from 2016 to 2018 [21]. Our hardware setup is shown in Figure 6. For the inference time, we evaluated the average inference time (AIT) in milliseconds taken by each algorithm to process each frame. We recorded power usage metrics using a power meter connected to the power supply of the PX 2. To ensure that the reported numbers only reflected each model's power consumption and not that of background processes, we subtracted the hardware's idle power consumption from the averages recorded during each test. For a fair comparison, we captured the metrics for the core algorithm (i.e., the SG2VEC and DPM model), excluding the contribution from data loading and pre-processing. Both models were run with a batch size of 1 to emulate the real-world data stream where images are processed as they are received. For comparison, we also show the AIT on a PC for the two models.\n\nOur results are shown in Table IV. SG2VEC performs inference 9.3x faster than the DPM on the PX 2 with an 88.0% smaller model and 32.4% less power, making it undoubtedly more practical for real-world deployment. Our model also uses 92.8% less energy to process each frame, which can be beneficial for electric vehicles with limited battery capacity. With an AIT of 0.4828 ms, SG2VEC can theoretically process up to 2,071 frames/second (fps). In contrast, with an AIT of 4.535 ms, the DPM can only process up to 220 fps. In the context of real-world collision prediction, this means that SG2VEC could easily support multiple 60 fps camera inputs from the AV while DPM would struggle to support more than three.  In the context of realworld collision prediction, these results indicate that SG2VEC is a more practical choice for AV safety and could significantly improve consumer trust in AVs. Few works have explored graph-based solutions for other complex AV challenges such as localization, path planning, and control. These are open research problems that we reserve for future work. \n\nFig. 2 :\n2An illustration of our scene-graph extraction process.\n\n\n36]. After multiple MR-GCN layers, the output of each layer is concatenated to produce the final embedding for each node, denoted by H L v = CONCAT({h (l) v }|l = 0, 1, ..., L), where L is the index of the last layer.\n\nFig. 5 :\n5Performance after transferring the models trained on synthetic 271-syn and 1043-syn datasets to the real-world 571honda dataset.\n\nFig. 6 :\n6Our experimental setup for evaluating SG2VEC and DPM on the industry-standard Nvidia DRIVE PX 2 hardware.\n\n\nThe University of Michigan. From 2001 to 2009, he was Dean of the College of Engineering and Eckis Professor of Electrical and Computer Engineering at the University of Florida till 2016. After serving briefly as Deputy Director of Technology at ARPA-E in 2012-13, he was appointed by the National Science Foundation (NSF) to serve as Assistant Director for the Directorate of Engineering (ENG) in March 2013, a position he held till June 2016. Currently, he is Vice Chancellor for Research and Distinguished Professor of Electrical Engineering and Computer Science at the University of California, Irvine. His research and teaching interests are centered on theory and applications of systems and control. He has received numerous honors and awards including IEEE Control Systems Award, IEEE Baker Prize, IEEE CSS Axelby Award, NSF Presidential Young Investigator Award, AACC Eckman Award, and is a Fellow of IEEE, IFAC, and AAAS.Mohammad Abdullah Al Faruque (M'06, SM'15) received his B.Sc. degree in Computer Science and Engineering (CSE) from Bangladesh University of Engineering and Technology (BUET) in 2002, and M.Sc. and Ph.D. degrees in Computer Science from Aachen Technical University and Karlsruhe Institute of Technology, Germany in 2004 and 2009, respectively. He is currently with the University of California Irvine (UCI) as an Associate Professor and Directing the Embedded and Cyber-Physical Systems Lab. He served as an Emulex Career Development Chair from October 2012 till July 2015. Before, he was with Siemens Corporate Research and Technology in Princeton, NJ as a Research Scientist. His current research is focused on the system-level design of embedded and Cyber-Physical-Systems (CPS) with special interest in low-power design, CPS security, data-driven CPS design, etc. He is an ACM senior member. He is the author of 2 published books. Besides many other awards, he is the recipient of the School of Engineering Mid-Career Faculty Award for Research 2019, the IEEE Technical Committee on Cyber-Physical Systems Early-Career Award 2018, and the IEEE CEDA Ernest S. Kuh Early Career Award 2016. He is also the recipient of the UCI Academic Senate Distinguished Early-Career Faculty Award for Research 2017 and the School of Engineering Early-Career Faculty Award for Research 2017. Besides 120+ IEEE/ACM publications in the premier journals and conferences, he holds 9 US patents.\n\n1 )\n1Capturing complex relationships between road participants. 2) Detecting future collisions early enough such that the AV can take corrective actions. 3) Generalizing to a wide range of traffic scenarios. arXiv:2111.06123v1 [cs.CV] 11 Nov 2021t=0.5s \n\nt=2.0s \n\nCollision Likelihood: \n0.15 \n\nCollision Likelihood: \n1.0 \n\nisIn \n\nisIn \n\nisIn \nisIn \n\nisIn \n\nisIn \nisIn isIn \n\nisIn \n\nisIn \nisIn \n\nisIn \n\nisIn \nisIn isIn \n\nisIn \nisIn \n\natDRearOf \n\natDRearOf \n\nnear \n\nnear \ntoRightOf toLeftOf \ninDFrontOf \n\ninDFrontOf \nnear_coll \n\nnear_coll \n\nFig. 1: How SG2VEC predicts collisions using scene-graphs. \nEach node's color indicates its attention score (importance to \nthe collision likelihood) from orange (high) to green (low). \n\n\n\n\nFig. 3: An illustration of SG2VEC's architecture.construct the pair-wise relations between objects in A n . For each camera angle, we calibrate the birds-eye view projection settings using known fixed distances, such as the lane length and width, as defined by the highway code. This enables us to estimate longitudinal and lateral distances accurately in the projection. For datasets captured by a single vehicle, this step only needs to be performed once. However, for datasets with a wide range of camera angles such as the 620-dash dataset introduced later in the paper, this process needs to be performed once per vehicle. With a human operator, we found that this calibration step takes approximately 1 minute per camera angle on average.The extraction pipeline identifies three kinds of pair-wise relations: proximity relations (e.g.visible, near, very near, etc.), directional (e.g. Front Left, Rear Right, etc.) relations, and belonging (e.g. car 1 isIn left lane) relations. Two objects are assigned the proximity relation, r \u2208 {Near Collision (4 ft.), Super Near (7 ft.), Very Near (10 ft.), Near (16 ft.), Visible (25 ft.)} provided the objects are physically separated by a distance that is within that relation's threshold. The directional relation, r \u2208 {Front Left, Left Front, Left Rear, Rear Left, Rear Right, Right Rear, Right Front, Front Right},Extraction \n\nObject \nDetection \n\nObjects = \n{c1, c2, c3, \u2026} \nAttributes = \n{c1:[x, y, type] \u2026} \n\nGraph \nConvolution \n\nmessage \npassing \n\nSelf-Attention \nGraph Pooling \n\nremove \nirrelevant \nnodes \n\nmean/max/sum \n\nh G n \n\n= [0.2, 1.1, 0.9, \u2026] \n\nG n = {O n, A n } \n\nLSTM \nMLP/Log \n\nSoftmax \n\n\u0176 n \n\nI n-1 \n\nI n \n\nI n+1 \n\n\u0176 n-1 \n\n\u0176 n+1 \n\n\u2026 \n\n\u2026 \n\nSpatial Modeling \nTemporal Modeling \n\n\n\nTABLE I :\nIClassification accuracy, AUC, and MCC for \nSG2VEC (Ours) and DPM. \n\n\n\nTABLE II :\nIIAverage time of prediction (ATP) for collisions.\n\nTABLE III :\nIIISG2VEC ablation study on the 1043-syn dataset.\n\nTABLE IV :\nIVPerformance evaluation of inference on 271-syn on the Nvidia DRIVE PX 2. V. CONCLUSION With our experiments, we demonstrated that our scenegraph embedding methodology for collision prediction, SG2VEC, outperforms the state-of-the-art method, DPM, in terms of average MCC (0.5055 vs. 0.2096), average inference time (0.255 ms vs. 1.39 ms), and average time of prediction (39.07% sooner than DPM). Additionally, we demonstrated that SG2VEC could transfer knowledge from synthetic datasets to real-world driving datasets more effectively than the DPM, achieving an average transfer MCC of 0.327 vs. 0.060. Finally, we showed that our methodology performs faster inference than the DPM (0.4828 ms vs. 4.535 ms) with a smaller model size (331 KB vs. 2,764 KB) and reduced power consumption (2.99 W vs. 4.42 W) on the industry-standard Nvidia DRIVE PX 2 autonomous driving platform.\nhttps://github.com/carla-simulator/scenario runner 2 https://dx.doi.org/10.21227/c0z9-1p30 3 https://github.com/AICPS/sg-collision-prediction\n\nA fail-safe architecture for automated driving. S Dorff, B B\u00f6ddeker, 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE). S. vom Dorff, B. B\u00f6ddeker et al., \"A fail-safe architecture for automated driving,\" 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 828-833, 2020.\n\nA distributed safety mechanism using middleware and hypervisors for autonomous vehicles. T Bijlsma, A Buriachevskyi, 2020T. Bijlsma, A. Buriachevskyi et al., \"A distributed safety mechanism using middleware and hypervisors for autonomous vehicles,\" 2020\n\nDesign, Europe Conference & Exhibition (DATE). Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 1175-1180, 2020.\n\nCollision Between a Sport Utility Vehicle Operating With Partial Driving Automation and a Crash Attenuator. NTSB/HAR-20/01National Transportation Safety Board. 2020National Transportation Safety BoardTech. Rep.National Transportation Safety Board, \"Collision Between a Sport Utility Vehicle Operating With Partial Driving Automation and a Crash Attenuator,\" National Transportation Safety Board, Tech. Rep. NTSB/HAR-20/01, 2020.\n\nCollision Between Car Operating with Partial Driving Automation and Truck-Tractor Semitrailer. NTSB/HAB-20/01National Transportation Safety Board. 2020Tech. Rep.--, \"Collision Between Car Operating with Partial Driving Au- tomation and Truck-Tractor Semitrailer,\" National Transportation Safety Board, Tech. Rep. NTSB/HAB-20/01, 2020.\n\nCollision between vehicle controlled by developmental automated driving system and pedestrian. NTSB/HAR-19/03National Transportation Safety Board. Tech. Rep.--, \"Collision between vehicle controlled by developmental auto- mated driving system and pedestrian,\" National Transportation Safety Board, Tech. Rep. NTSB/HAR-19/03, 2019.\n\nGlobal Automotive Consumer Study -North America. Deloitte Development, Llc , Deloitte Development LLC, Tech. Rep. Deloitte Development LLC, \"Global Automotive Consumer Study - North America,\" Deloitte Development LLC, Tech. Rep., 2020.\n\nWhat humanlike errors do autonomous vehicles need to avoid to maximize safety?. A S Mueller, J B Cicchino, D S Zuby, Journal of Safety Research. A. S. Mueller, J. B. Cicchino, and D. S. Zuby, \"What humanlike errors do autonomous vehicles need to avoid to maximize safety?\" Journal of Safety Research, 2020.\n\nA preliminary analysis of real-world crashes involving self-driving vehicles. B Schoettle, M Sivak, UMTRI-2015-34University of Michigan Transportation Research InstituteTech. Rep.B. Schoettle and M. Sivak, \"A preliminary analysis of real-world crashes involving self-driving vehicles,\" University of Michigan Transportation Research Institute, Tech. Rep. UMTRI-2015-34, 2015.\n\nStatistical analysis of the patterns and characteristics of connected and autonomous vehicle involved crashes. C Xu, Z Ding, Journal of safety research. 71C. Xu, Z. Ding et al., \"Statistical analysis of the patterns and character- istics of connected and autonomous vehicle involved crashes,\" Journal of safety research, vol. 71, pp. 41-47, 2019.\n\nVolvo collision avoidance features: initial results. H L D Institute, Highway Loss Data Institute Bulletin. 29H. L. D. Institute, \"Volvo collision avoidance features: initial results,\" Highway Loss Data Institute Bulletin, vol. 29, no. 5, 2012.\n\nCollision avoidance: A literature review on threat-assessment techniques. J Dahl, G R De Campos, IEEE Transactions on Intelligent Vehicles. 41J. Dahl, G. R. de Campos et al., \"Collision avoidance: A literature review on threat-assessment techniques,\" IEEE Transactions on Intelligent Vehicles, vol. 4, no. 1, pp. 101-113, 2018.\n\nWorst-case analysis of the time-to-react using reachable sets. S Sontges, M Koschi, M Althoff, 2018 IEEE Intelligent Vehicles Symposium (IV). IEEES. Sontges, M. Koschi, and M. Althoff, \"Worst-case analysis of the time-to-react using reachable sets,\" in 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2018, pp. 1891-1897.\n\nWorst-case analysis of automotive collision avoidance systems. J Nilsson, A C \u00d6dblom, J Fredriksson, IEEE Transactions on Vehicular Technology. 654J. Nilsson, A. C.\u00d6dblom, and J. Fredriksson, \"Worst-case analysis of automotive collision avoidance systems,\" IEEE Transactions on Vehicular Technology, vol. 65, no. 4, pp. 1899-1911, 2015.\n\nRelational inductive biases, deep learning, and graph networks. P W Battaglia, J B Hamrick, arXiv:1806.01261arXiv preprintP. W. Battaglia, J. B. Hamrick et al., \"Relational inductive biases, deep learning, and graph networks,\" arXiv preprint arXiv:1806.01261, 2018.\n\nCarla: An open urban driving simulator. A Dosovitskiy, G Ros, arXiv:1711.03938arXiv preprintA. Dosovitskiy, G. Ros et al., \"Carla: An open urban driving simulator,\" arXiv preprint arXiv:1711.03938, 2017.\n\nTowards accurate vehicle behaviour classification with multi-relational graph convolutional networks. S Mylavarapu, M Sandhu, arXiv:2002.00786arXiv preprintS. Mylavarapu, M. Sandhu et al., \"Towards accurate vehicle behaviour classification with multi-relational graph convolutional networks,\" arXiv preprint arXiv:2002.00786, 2020.\n\nLearning 3d-aware egocentric spatial-temporal interaction via graph convolutional networks. C Li, Y Meng, 2020 IEEE International Conference on Robotics and Automation (ICRA). C. Li, Y. Meng et al., \"Learning 3d-aware egocentric spatial-temporal interaction via graph convolutional networks,\" 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 8418-8424, 2020.\n\nReading between the lanes: Road layout reconstruction from partially segmented scenes. L Kunze, T Bruls, 21st International Conference on Intelligent Transportation Systems (ITSC). L. Kunze, T. Bruls et al., \"Reading between the lanes: Road layout reconstruction from partially segmented scenes,\" 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pp. 401-408, 2018.\n\nUnderstanding dynamic scenes using graph convolution networks. S Mylavarapu, M Sandhu, arXiv:2005.04437arXiv preprintS. Mylavarapu, M. Sandhu et al., \"Understanding dynamic scenes using graph convolution networks,\" arXiv preprint arXiv:2005.04437, 2020.\n\nScene-graph augmented data-driven risk assessment of autonomous vehicle decisions. S.-Y Yu, A V Malawade, IEEE Transactions on Intelligent Transportation Systems. S.-Y. Yu, A. V. Malawade et al., \"Scene-graph augmented data-driven risk assessment of autonomous vehicle decisions,\" IEEE Transactions on Intelligent Transportation Systems, 2021.\n\nAll new Teslas are equipped with NVIDIA's new Drive PX 2 AI platform for self-driving -Electrek. Online; accessed 9\"All new Teslas are equipped with NVIDIA's new Drive PX 2 AI platform for self-driving -Electrek,\" https://electrek.co/2016/10/21/all- new-teslas-are-equipped-with-nvidias-new-drive-px-2-ai-platform-for- self-driving, Oct 2016, [Online; accessed 9. Nov. 2020].\n\nOn a formal model of safe and scalable self-driving cars. S Shalev-Shwartz, S Shammah, A Shashua, arXiv:1708.06374arXiv preprintS. Shalev-Shwartz, S. Shammah, and A. Shashua, \"On a formal model of safe and scalable self-driving cars,\" arXiv preprint arXiv:1708.06374, 2017.\n\nThe safety force field. D Nist\u00e9r, H.-L Lee, NVIDIA White PaperD. Nist\u00e9r, H.-L. Lee et al., \"The safety force field,\" NVIDIA White Paper, 2019.\n\nIntegration of formal safety models on system level using the example of responsibility sensitive safety and carla driving simulator. B Gassmann, F Pasch, International Conference on Computer Safety, Reliability, and Security. SpringerB. Gassmann, F. Pasch et al., \"Integration of formal safety models on system level using the example of responsibility sensitive safety and carla driving simulator,\" in International Conference on Computer Safety, Reliability, and Security. Springer, 2020, pp. 358-369.\n\nModel-based probabilistic collision detection in autonomous driving. M Althoff, O Stursberg, M Buss, IEEE Transactions on Intelligent Transportation Systems. 102M. Althoff, O. Stursberg, and M. Buss, \"Model-based probabilistic colli- sion detection in autonomous driving,\" IEEE Transactions on Intelligent Transportation Systems, vol. 10, no. 2, pp. 299-310, 2009.\n\nTrajectory planning and safety assessment of autonomous vehicles based on motion prediction and model predictive control. Y Wang, Z Liu, IEEE Transactions on Vehicular Technology. 689Y. Wang, Z. Liu et al., \"Trajectory planning and safety assessment of autonomous vehicles based on motion prediction and model predictive control,\" IEEE Transactions on Vehicular Technology, vol. 68, no. 9, pp. 8546-8556, 2019.\n\nSurrounding vehicles motion prediction for risk assessment and motion planning of autonomous vehicle in highway scenarios. L Zhang, W Xiao, IEEE Access. 8L. Zhang, W. Xiao et al., \"Surrounding vehicles motion prediction for risk assessment and motion planning of autonomous vehicle in highway scenarios,\" IEEE Access, vol. 8, pp. 209 356-209 376, 2020.\n\nA real-time collision prediction mechanism with deep learning for intelligent transportation system. X Wang, J Liu, IEEE transactions on vehicular technology. 699X. Wang, J. Liu et al., \"A real-time collision prediction mechanism with deep learning for intelligent transportation system,\" IEEE transactions on vehicular technology, vol. 69, no. 9, pp. 9497-9508, 2020.\n\nDeep predictive models for collision risk assessment in autonomous driving. M Strickland, G Fainekos, H B Amor, IEEE International Conference on Robotics and Automation (ICRA). M. Strickland, G. Fainekos, and H. B. Amor, \"Deep predictive models for collision risk assessment in autonomous driving,\" 2018 IEEE In- ternational Conference on Robotics and Automation (ICRA), pp. 1-8, 2018.\n\nPisces: Power-aware implementation of slam by customizing efficient sparse algebra. B Asgari, R Hadidi, 2020 57th ACM/IEEE Design Automation Conference (DAC). B. Asgari, R. Hadidi et al., \"Pisces: Power-aware implementation of slam by customizing efficient sparse algebra,\" 2020 57th ACM/IEEE Design Automation Conference (DAC), pp. 1-6, 2020.\n\nVehicular and edge computing for emerging connected and autonomous vehicle applications. S Baidya, Y.-J Ku, 2020 57th ACM/IEEE Design Automation Conference (DAC). S. Baidya, Y.-J. Ku et al., \"Vehicular and edge computing for emerging connected and autonomous vehicle applications,\" 2020 57th ACM/IEEE Design Automation Conference (DAC), pp. 1-6, 2020.\n\nOpportunistic intermittent control with safety guarantees for autonomous systems. C Huang, S Xu, 2020 57th ACM/IEEE Design Automation Conference (DAC). C. Huang, S. Xu et al., \"Opportunistic intermittent control with safety guarantees for autonomous systems,\" 2020 57th ACM/IEEE Design Automation Conference (DAC), pp. 1-6, 2020.\n\nGraph r-cnn for scene graph generation. J Yang, J Lu, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)J. Yang, J. Lu et al., \"Graph r-cnn for scene graph generation,\" Proceedings of the European conference on computer vision (ECCV), pp. 670-685, 2018.\n\nScene graph generation by iterative message passing. D Xu, Y Zhu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionD. Xu, Y. Zhu et al., \"Scene graph generation by iterative message passing,\" Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5410-5419, 2017.\n\nMask r-cnn. K He, G Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionK. He, G. Gkioxari et al., \"Mask r-cnn,\" Proceedings of the IEEE international conference on computer vision, pp. 2961-2969, 2017.\n\nModeling relational data with graph convolutional networks. M Schlichtkrull, T N Kipf, European Semantic Web ConferenceM. Schlichtkrull, T. N. Kipf et al., \"Modeling relational data with graph convolutional networks,\" European Semantic Web Conference, pp. 593- 607, 2018.\n\nSelf-attention graph pooling. J Lee, I Lee, J Kang, arXiv:1904.08082arXiv preprintJ. Lee, I. Lee, and J. Kang, \"Self-attention graph pooling,\" arXiv preprint arXiv:1904.08082, 2019.\n\nToward driving scene understanding: A dataset for learning driver behavior and causal reasoning. V Ramanishka, Y.-T Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionV. Ramanishka, Y.-T. Chen et al., \"Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning,\" Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7699-7707, 2018.\n\nWhen, where, and what? a new dataset for anomaly detection in driving videos. Y Yao, X Wang, arXiv:2004.03044arXiv preprintY. Yao, X. Wang et al., \"When, where, and what? a new dataset for anomaly detection in driving videos,\" arXiv preprint arXiv:2004.03044, 2020.\n\nThe use of the area under the roc curve in the evaluation of machine learning algorithms. A P Bradley, Pattern recognition. 307A. P. Bradley, \"The use of the area under the roc curve in the evaluation of machine learning algorithms,\" Pattern recognition, vol. 30, no. 7, pp. 1145-1159, 1997.\n\nThe advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation. D Chicco, G Jurman, BMC genomics. 2116D. Chicco and G. Jurman, \"The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation,\" BMC genomics, vol. 21, no. 1, p. 6, 2020.\n\nHe is currently a Ph.D. student at UCI under the supervision of Professor Mohammad Al Faruque. His research interests include the design and security of cyber-physical systems. UCI). Arnav Vaibhav Malawade received a B.S. in Computer Science and Engineering and an M.S. in Computer Engineering from the University of California Irvinein connected/autonomous vehicles, manufacturing, IoT, and healthcareArnav Vaibhav Malawade received a B.S. in Computer Science and Engineering and an M.S. in Computer Engineering from the University of Cali- fornia Irvine (UCI) in 2018 and 2021, respectively. He is currently a Ph.D. student at UCI under the supervision of Professor Mohammad Al Faruque. His research interests include the design and security of cyber-physical systems in connected/autonomous vehicles, manufacturing, IoT, and healthcare.\n\nHe worked at MediaTek for 4 years. Currently he is a Ph.D. student in the University of California, Irvine. Now his research interests are about design automation of embedded systems using data-driven system modeling approaches. It covers incorporating machine learning methods to identify potential security issues in systems. B S Shih-Yuan Yu Received The, M S , degrees in Computer Science and Information Engineering from the National Taiwan University ; NTUShih-Yuan Yu received the B.S. and M.S. degrees in Computer Science and Information Engineering from the National Taiwan University (NTU) in 2014. He worked at MediaTek for 4 years. Currently he is a Ph.D. student in the University of California, Irvine. Now his research interests are about design automation of embedded systems using data-driven system modeling approaches. It covers incorporat- ing machine learning methods to identify potential security issues in systems.\n\nUCI) in 2021. His research interests lie broadly in machine and statistical learning with current focus on perception, representation learning. Brandon Hsu received a B.S. degree in Computer Engineering from the University of California Irvineand autonomous vehiclesBrandon Hsu received a B.S. degree in Computer Engineering from the University of California Irvine (UCI) in 2021. His research interests lie broadly in machine and statistical learning with current focus on perception, representation learning, and autonomous vehicles.\n\nHis doctoral thesis work focused on market mechanisms for integrating demand flexibility in energy systems. Before his term at UC Irvine he was a post-doctoral associate at Cornell University where his work focused on online scheduling algorithms for managing demand flexibility. His current research interests include control theory, machine learning, topics at the intersection of learning and control, online learning. Deepan Muthirayan is currently a Post-doctoral Researcher in the department of Electrical Engineering and Computer Science at University of California at IrvineHe obtained his Phd from the University of California at Berkeley (2016) and B.Tech/M.tech degree from the Indian Institute of Technology Madras. online algorithms, game theory, and their application to smart systemsDeepan Muthirayan is currently a Post-doctoral Researcher in the department of Electrical Engi- neering and Computer Science at University of California at Irvine. He obtained his Phd from the University of California at Berkeley (2016) and B.Tech/M.tech degree from the Indian Institute of Technology Madras (2010). His doctoral thesis work focused on market mechanisms for integrating de- mand flexibility in energy systems. Before his term at UC Irvine he was a post-doctoral associate at Cornell University where his work focused on online scheduling algorithms for managing demand flexibility. His current research interests include control theory, machine learning, topics at the intersection of learning and control, online learning, online algorithms, game theory, and their application to smart systems.\n", "annotations": {"author": "[{\"end\":226,\"start\":85},{\"end\":364,\"start\":227},{\"end\":498,\"start\":365},{\"end\":624,\"start\":499},{\"end\":780,\"start\":625},{\"end\":927,\"start\":781}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":93},{\"end\":239,\"start\":237},{\"end\":376,\"start\":373},{\"end\":516,\"start\":506},{\"end\":645,\"start\":634},{\"end\":802,\"start\":792}]", "author_first_name": "[{\"end\":90,\"start\":85},{\"end\":92,\"start\":91},{\"end\":236,\"start\":227},{\"end\":372,\"start\":365},{\"end\":505,\"start\":499},{\"end\":631,\"start\":625},{\"end\":633,\"start\":632},{\"end\":789,\"start\":781},{\"end\":791,\"start\":790}]", "author_affiliation": "[{\"end\":225,\"start\":120},{\"end\":363,\"start\":258},{\"end\":497,\"start\":392},{\"end\":623,\"start\":518},{\"end\":779,\"start\":674},{\"end\":926,\"start\":821}]", "title": "[{\"end\":82,\"start\":1},{\"end\":1009,\"start\":928}]", "venue": null, "abstract": "[{\"end\":2265,\"start\":1092}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2533,\"start\":2530},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2538,\"start\":2535},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2745,\"start\":2742},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2750,\"start\":2747},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2918,\"start\":2915},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3061,\"start\":3058},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3213,\"start\":3210},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3310,\"start\":3307},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3315,\"start\":3312},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3851,\"start\":3847},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3857,\"start\":3853},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4087,\"start\":4083},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4237,\"start\":4233},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4456,\"start\":4452},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4553,\"start\":4549},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5147,\"start\":5143},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5350,\"start\":5346},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6336,\"start\":6332},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6342,\"start\":6338},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6373,\"start\":6369},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6517,\"start\":6513},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6523,\"start\":6519},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6529,\"start\":6525},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6554,\"start\":6550},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6861,\"start\":6857},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7875,\"start\":7871},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8965,\"start\":8961},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9295,\"start\":9291},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9301,\"start\":9297},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9344,\"start\":9340},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9350,\"start\":9346},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9544,\"start\":9540},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9579,\"start\":9575},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9836,\"start\":9832},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9961,\"start\":9957},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10239,\"start\":10235},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10248,\"start\":10244},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10442,\"start\":10438},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10718,\"start\":10714},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11882,\"start\":11878},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12328,\"start\":12324},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12498,\"start\":12494},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12804,\"start\":12800},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12813,\"start\":12809},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13633,\"start\":13629},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13650,\"start\":13646},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13668,\"start\":13664},{\"end\":14397,\"start\":14391},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14788,\"start\":14784},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14794,\"start\":14790},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16058,\"start\":16054},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17329,\"start\":17325},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18297,\"start\":18293},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19414,\"start\":19410},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19746,\"start\":19742},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21184,\"start\":21180},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22193,\"start\":22189},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22597,\"start\":22593},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24030,\"start\":24026},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24570,\"start\":24566},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24619,\"start\":24615},{\"end\":30404,\"start\":30377},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35343,\"start\":35339}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37349,\"start\":37284},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37569,\"start\":37350},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37709,\"start\":37570},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37826,\"start\":37710},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40237,\"start\":37827},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40965,\"start\":40238},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42712,\"start\":40966},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42793,\"start\":42713},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42856,\"start\":42794},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":42919,\"start\":42857},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":43810,\"start\":42920}]", "paragraph": "[{\"end\":3585,\"start\":2284},{\"end\":4631,\"start\":3587},{\"end\":5762,\"start\":4633},{\"end\":7530,\"start\":5764},{\"end\":8196,\"start\":7532},{\"end\":8966,\"start\":8198},{\"end\":9837,\"start\":9019},{\"end\":10938,\"start\":9839},{\"end\":11144,\"start\":10940},{\"end\":12187,\"start\":11146},{\"end\":13238,\"start\":12238},{\"end\":13808,\"start\":13240},{\"end\":14096,\"start\":13851},{\"end\":14587,\"start\":14162},{\"end\":15523,\"start\":14617},{\"end\":16982,\"start\":15525},{\"end\":17509,\"start\":17040},{\"end\":18298,\"start\":17584},{\"end\":18648,\"start\":18300},{\"end\":19194,\"start\":18684},{\"end\":20284,\"start\":19223},{\"end\":21095,\"start\":20311},{\"end\":22065,\"start\":21097},{\"end\":22382,\"start\":22067},{\"end\":23124,\"start\":22384},{\"end\":23218,\"start\":23126},{\"end\":23649,\"start\":23254},{\"end\":24184,\"start\":23651},{\"end\":24254,\"start\":24186},{\"end\":24432,\"start\":24256},{\"end\":25923,\"start\":24472},{\"end\":26883,\"start\":25950},{\"end\":27492,\"start\":26885},{\"end\":27794,\"start\":27494},{\"end\":28348,\"start\":27796},{\"end\":30301,\"start\":28350},{\"end\":31529,\"start\":30303},{\"end\":32647,\"start\":31556},{\"end\":33535,\"start\":32708},{\"end\":34938,\"start\":33537},{\"end\":36195,\"start\":34989},{\"end\":37283,\"start\":36197}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14161,\"start\":14097},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17583,\"start\":17510},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18683,\"start\":18649}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25233,\"start\":25226},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26033,\"start\":26026},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27861,\"start\":27852},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32097,\"start\":32089},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36230,\"start\":36222}]", "section_header": "[{\"end\":2282,\"start\":2267},{\"end\":8985,\"start\":8969},{\"end\":9017,\"start\":8988},{\"end\":12236,\"start\":12190},{\"end\":13849,\"start\":13811},{\"end\":14615,\"start\":14590},{\"end\":16998,\"start\":16985},{\"end\":17012,\"start\":17001},{\"end\":17038,\"start\":17015},{\"end\":19221,\"start\":19197},{\"end\":20309,\"start\":20287},{\"end\":23252,\"start\":23221},{\"end\":24470,\"start\":24435},{\"end\":25948,\"start\":25926},{\"end\":31554,\"start\":31532},{\"end\":32706,\"start\":32650},{\"end\":34987,\"start\":34941},{\"end\":37293,\"start\":37285},{\"end\":37579,\"start\":37571},{\"end\":37719,\"start\":37711},{\"end\":40242,\"start\":40239},{\"end\":42723,\"start\":42714},{\"end\":42805,\"start\":42795},{\"end\":42869,\"start\":42858},{\"end\":42931,\"start\":42921}]", "table": "[{\"end\":40965,\"start\":40485},{\"end\":42712,\"start\":42333},{\"end\":42793,\"start\":42725}]", "figure_caption": "[{\"end\":37349,\"start\":37295},{\"end\":37569,\"start\":37352},{\"end\":37709,\"start\":37581},{\"end\":37826,\"start\":37721},{\"end\":40237,\"start\":37829},{\"end\":40485,\"start\":40244},{\"end\":42333,\"start\":40968},{\"end\":42856,\"start\":42808},{\"end\":42919,\"start\":42873},{\"end\":43810,\"start\":42934}]", "figure_ref": "[{\"end\":7611,\"start\":7603},{\"end\":14430,\"start\":14422},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15473,\"start\":15465},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15762,\"start\":15756},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16543,\"start\":16537},{\"end\":17060,\"start\":17052},{\"end\":20533,\"start\":20525},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25246,\"start\":25238},{\"end\":25255,\"start\":25249},{\"end\":29195,\"start\":29186},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33396,\"start\":33388},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35384,\"start\":35376}]", "bib_author_first_name": "[{\"end\":44003,\"start\":44002},{\"end\":44012,\"start\":44011},{\"end\":44363,\"start\":44362},{\"end\":44374,\"start\":44373},{\"end\":45820,\"start\":45812},{\"end\":45837,\"start\":45834},{\"end\":46081,\"start\":46080},{\"end\":46083,\"start\":46082},{\"end\":46094,\"start\":46093},{\"end\":46096,\"start\":46095},{\"end\":46108,\"start\":46107},{\"end\":46110,\"start\":46109},{\"end\":46387,\"start\":46386},{\"end\":46400,\"start\":46399},{\"end\":46797,\"start\":46796},{\"end\":46803,\"start\":46802},{\"end\":47087,\"start\":47086},{\"end\":47091,\"start\":47088},{\"end\":47354,\"start\":47353},{\"end\":47362,\"start\":47361},{\"end\":47364,\"start\":47363},{\"end\":47672,\"start\":47671},{\"end\":47683,\"start\":47682},{\"end\":47693,\"start\":47692},{\"end\":48000,\"start\":47999},{\"end\":48011,\"start\":48010},{\"end\":48013,\"start\":48012},{\"end\":48023,\"start\":48022},{\"end\":48339,\"start\":48338},{\"end\":48341,\"start\":48340},{\"end\":48354,\"start\":48353},{\"end\":48356,\"start\":48355},{\"end\":48582,\"start\":48581},{\"end\":48597,\"start\":48596},{\"end\":48849,\"start\":48848},{\"end\":48863,\"start\":48862},{\"end\":49172,\"start\":49171},{\"end\":49178,\"start\":49177},{\"end\":49552,\"start\":49551},{\"end\":49561,\"start\":49560},{\"end\":49926,\"start\":49925},{\"end\":49940,\"start\":49939},{\"end\":50204,\"start\":50200},{\"end\":50210,\"start\":50209},{\"end\":50212,\"start\":50211},{\"end\":50898,\"start\":50897},{\"end\":50916,\"start\":50915},{\"end\":50927,\"start\":50926},{\"end\":51139,\"start\":51138},{\"end\":51152,\"start\":51148},{\"end\":51393,\"start\":51392},{\"end\":51405,\"start\":51404},{\"end\":51834,\"start\":51833},{\"end\":51845,\"start\":51844},{\"end\":51858,\"start\":51857},{\"end\":52253,\"start\":52252},{\"end\":52261,\"start\":52260},{\"end\":52666,\"start\":52665},{\"end\":52675,\"start\":52674},{\"end\":52998,\"start\":52997},{\"end\":53006,\"start\":53005},{\"end\":53343,\"start\":53342},{\"end\":53357,\"start\":53356},{\"end\":53369,\"start\":53368},{\"end\":53371,\"start\":53370},{\"end\":53738,\"start\":53737},{\"end\":53748,\"start\":53747},{\"end\":54088,\"start\":54087},{\"end\":54101,\"start\":54097},{\"end\":54434,\"start\":54433},{\"end\":54443,\"start\":54442},{\"end\":54723,\"start\":54722},{\"end\":54731,\"start\":54730},{\"end\":55056,\"start\":55055},{\"end\":55062,\"start\":55061},{\"end\":55400,\"start\":55399},{\"end\":55406,\"start\":55405},{\"end\":55731,\"start\":55730},{\"end\":55748,\"start\":55747},{\"end\":55750,\"start\":55749},{\"end\":55974,\"start\":55973},{\"end\":55981,\"start\":55980},{\"end\":55988,\"start\":55987},{\"end\":56224,\"start\":56223},{\"end\":56241,\"start\":56237},{\"end\":56704,\"start\":56703},{\"end\":56711,\"start\":56710},{\"end\":56983,\"start\":56982},{\"end\":56985,\"start\":56984},{\"end\":57311,\"start\":57310},{\"end\":57321,\"start\":57320},{\"end\":58713,\"start\":58712},{\"end\":58715,\"start\":58714},{\"end\":58744,\"start\":58743},{\"end\":58746,\"start\":58745}]", "bib_author_last_name": "[{\"end\":44009,\"start\":44004},{\"end\":44021,\"start\":44013},{\"end\":44371,\"start\":44364},{\"end\":44388,\"start\":44375},{\"end\":44534,\"start\":44528},{\"end\":45832,\"start\":45821},{\"end\":46091,\"start\":46084},{\"end\":46105,\"start\":46097},{\"end\":46115,\"start\":46111},{\"end\":46397,\"start\":46388},{\"end\":46406,\"start\":46401},{\"end\":46800,\"start\":46798},{\"end\":46808,\"start\":46804},{\"end\":47101,\"start\":47092},{\"end\":47359,\"start\":47355},{\"end\":47374,\"start\":47365},{\"end\":47680,\"start\":47673},{\"end\":47690,\"start\":47684},{\"end\":47701,\"start\":47694},{\"end\":48008,\"start\":48001},{\"end\":48020,\"start\":48014},{\"end\":48035,\"start\":48024},{\"end\":48351,\"start\":48342},{\"end\":48364,\"start\":48357},{\"end\":48594,\"start\":48583},{\"end\":48601,\"start\":48598},{\"end\":48860,\"start\":48850},{\"end\":48870,\"start\":48864},{\"end\":49175,\"start\":49173},{\"end\":49183,\"start\":49179},{\"end\":49558,\"start\":49553},{\"end\":49567,\"start\":49562},{\"end\":49937,\"start\":49927},{\"end\":49947,\"start\":49941},{\"end\":50207,\"start\":50205},{\"end\":50221,\"start\":50213},{\"end\":50913,\"start\":50899},{\"end\":50924,\"start\":50917},{\"end\":50935,\"start\":50928},{\"end\":51146,\"start\":51140},{\"end\":51156,\"start\":51153},{\"end\":51402,\"start\":51394},{\"end\":51411,\"start\":51406},{\"end\":51842,\"start\":51835},{\"end\":51855,\"start\":51846},{\"end\":51863,\"start\":51859},{\"end\":52258,\"start\":52254},{\"end\":52265,\"start\":52262},{\"end\":52672,\"start\":52667},{\"end\":52680,\"start\":52676},{\"end\":53003,\"start\":52999},{\"end\":53010,\"start\":53007},{\"end\":53354,\"start\":53344},{\"end\":53366,\"start\":53358},{\"end\":53376,\"start\":53372},{\"end\":53745,\"start\":53739},{\"end\":53755,\"start\":53749},{\"end\":54095,\"start\":54089},{\"end\":54104,\"start\":54102},{\"end\":54440,\"start\":54435},{\"end\":54446,\"start\":54444},{\"end\":54728,\"start\":54724},{\"end\":54734,\"start\":54732},{\"end\":55059,\"start\":55057},{\"end\":55066,\"start\":55063},{\"end\":55403,\"start\":55401},{\"end\":55415,\"start\":55407},{\"end\":55745,\"start\":55732},{\"end\":55755,\"start\":55751},{\"end\":55978,\"start\":55975},{\"end\":55985,\"start\":55982},{\"end\":55993,\"start\":55989},{\"end\":56235,\"start\":56225},{\"end\":56246,\"start\":56242},{\"end\":56708,\"start\":56705},{\"end\":56716,\"start\":56712},{\"end\":56993,\"start\":56986},{\"end\":57318,\"start\":57312},{\"end\":57328,\"start\":57322},{\"end\":58741,\"start\":58716}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":219854000},\"end\":44271,\"start\":43954},{\"attributes\":{\"id\":\"b1\"},\"end\":44526,\"start\":44273},{\"attributes\":{\"id\":\"b2\"},\"end\":44663,\"start\":44528},{\"attributes\":{\"doi\":\"NTSB/HAR-20/01\",\"id\":\"b3\"},\"end\":45093,\"start\":44665},{\"attributes\":{\"doi\":\"NTSB/HAB-20/01\",\"id\":\"b4\"},\"end\":45429,\"start\":45095},{\"attributes\":{\"doi\":\"NTSB/HAR-19/03\",\"id\":\"b5\"},\"end\":45761,\"start\":45431},{\"attributes\":{\"id\":\"b6\"},\"end\":45998,\"start\":45763},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":228913478},\"end\":46306,\"start\":46000},{\"attributes\":{\"doi\":\"UMTRI-2015-34\",\"id\":\"b8\"},\"end\":46683,\"start\":46308},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":209433331},\"end\":47031,\"start\":46685},{\"attributes\":{\"id\":\"b10\"},\"end\":47277,\"start\":47033},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":67869918},\"end\":47606,\"start\":47279},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":21706949},\"end\":47934,\"start\":47608},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":36488190},\"end\":48272,\"start\":47936},{\"attributes\":{\"doi\":\"arXiv:1806.01261\",\"id\":\"b14\"},\"end\":48539,\"start\":48274},{\"attributes\":{\"doi\":\"arXiv:1711.03938\",\"id\":\"b15\"},\"end\":48744,\"start\":48541},{\"attributes\":{\"doi\":\"arXiv:2002.00786\",\"id\":\"b16\"},\"end\":49077,\"start\":48746},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":202712726},\"end\":49462,\"start\":49079},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":52023693},\"end\":49860,\"start\":49464},{\"attributes\":{\"doi\":\"arXiv:2005.04437\",\"id\":\"b19\"},\"end\":50115,\"start\":49862},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":221655625},\"end\":50460,\"start\":50117},{\"attributes\":{\"id\":\"b21\"},\"end\":50837,\"start\":50462},{\"attributes\":{\"doi\":\"arXiv:1708.06374\",\"id\":\"b22\"},\"end\":51112,\"start\":50839},{\"attributes\":{\"id\":\"b23\"},\"end\":51256,\"start\":51114},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":221305603},\"end\":51762,\"start\":51258},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12487174},\"end\":52128,\"start\":51764},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":199585116},\"end\":52540,\"start\":52130},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":227277711},\"end\":52894,\"start\":52542},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":222417205},\"end\":53264,\"start\":52896},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3761047},\"end\":53651,\"start\":53266},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":218595454},\"end\":53996,\"start\":53653},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":221134971},\"end\":54349,\"start\":53998},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":218570994},\"end\":54680,\"start\":54351},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":51894526},\"end\":55000,\"start\":54682},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1780254},\"end\":55385,\"start\":55002},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":54465873},\"end\":55668,\"start\":55387},{\"attributes\":{\"id\":\"b36\"},\"end\":55941,\"start\":55670},{\"attributes\":{\"doi\":\"arXiv:1904.08082\",\"id\":\"b37\"},\"end\":56124,\"start\":55943},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52846393},\"end\":56623,\"start\":56126},{\"attributes\":{\"doi\":\"arXiv:2004.03044\",\"id\":\"b39\"},\"end\":56890,\"start\":56625},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13806304},\"end\":57183,\"start\":56892},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":209528322},\"end\":57541,\"start\":57185},{\"attributes\":{\"id\":\"b42\"},\"end\":58382,\"start\":57543},{\"attributes\":{\"id\":\"b43\"},\"end\":59322,\"start\":58384},{\"attributes\":{\"id\":\"b44\"},\"end\":59859,\"start\":59324},{\"attributes\":{\"id\":\"b45\"},\"end\":61471,\"start\":59861}]", "bib_title": "[{\"end\":44000,\"start\":43954},{\"end\":44771,\"start\":44665},{\"end\":45188,\"start\":45095},{\"end\":45524,\"start\":45431},{\"end\":45810,\"start\":45763},{\"end\":46078,\"start\":46000},{\"end\":46794,\"start\":46685},{\"end\":47084,\"start\":47033},{\"end\":47351,\"start\":47279},{\"end\":47669,\"start\":47608},{\"end\":47997,\"start\":47936},{\"end\":49169,\"start\":49079},{\"end\":49549,\"start\":49464},{\"end\":50198,\"start\":50117},{\"end\":51390,\"start\":51258},{\"end\":51831,\"start\":51764},{\"end\":52250,\"start\":52130},{\"end\":52663,\"start\":52542},{\"end\":52995,\"start\":52896},{\"end\":53340,\"start\":53266},{\"end\":53735,\"start\":53653},{\"end\":54085,\"start\":53998},{\"end\":54431,\"start\":54351},{\"end\":54720,\"start\":54682},{\"end\":55053,\"start\":55002},{\"end\":55397,\"start\":55387},{\"end\":56221,\"start\":56126},{\"end\":56980,\"start\":56892},{\"end\":57308,\"start\":57185},{\"end\":57718,\"start\":57543}]", "bib_author": "[{\"end\":44011,\"start\":44002},{\"end\":44023,\"start\":44011},{\"end\":44373,\"start\":44362},{\"end\":44390,\"start\":44373},{\"end\":44536,\"start\":44528},{\"end\":45834,\"start\":45812},{\"end\":45840,\"start\":45834},{\"end\":46093,\"start\":46080},{\"end\":46107,\"start\":46093},{\"end\":46117,\"start\":46107},{\"end\":46399,\"start\":46386},{\"end\":46408,\"start\":46399},{\"end\":46802,\"start\":46796},{\"end\":46810,\"start\":46802},{\"end\":47103,\"start\":47086},{\"end\":47361,\"start\":47353},{\"end\":47376,\"start\":47361},{\"end\":47682,\"start\":47671},{\"end\":47692,\"start\":47682},{\"end\":47703,\"start\":47692},{\"end\":48010,\"start\":47999},{\"end\":48022,\"start\":48010},{\"end\":48037,\"start\":48022},{\"end\":48353,\"start\":48338},{\"end\":48366,\"start\":48353},{\"end\":48596,\"start\":48581},{\"end\":48603,\"start\":48596},{\"end\":48862,\"start\":48848},{\"end\":48872,\"start\":48862},{\"end\":49177,\"start\":49171},{\"end\":49185,\"start\":49177},{\"end\":49560,\"start\":49551},{\"end\":49569,\"start\":49560},{\"end\":49939,\"start\":49925},{\"end\":49949,\"start\":49939},{\"end\":50209,\"start\":50200},{\"end\":50223,\"start\":50209},{\"end\":50915,\"start\":50897},{\"end\":50926,\"start\":50915},{\"end\":50937,\"start\":50926},{\"end\":51148,\"start\":51138},{\"end\":51158,\"start\":51148},{\"end\":51404,\"start\":51392},{\"end\":51413,\"start\":51404},{\"end\":51844,\"start\":51833},{\"end\":51857,\"start\":51844},{\"end\":51865,\"start\":51857},{\"end\":52260,\"start\":52252},{\"end\":52267,\"start\":52260},{\"end\":52674,\"start\":52665},{\"end\":52682,\"start\":52674},{\"end\":53005,\"start\":52997},{\"end\":53012,\"start\":53005},{\"end\":53356,\"start\":53342},{\"end\":53368,\"start\":53356},{\"end\":53378,\"start\":53368},{\"end\":53747,\"start\":53737},{\"end\":53757,\"start\":53747},{\"end\":54097,\"start\":54087},{\"end\":54106,\"start\":54097},{\"end\":54442,\"start\":54433},{\"end\":54448,\"start\":54442},{\"end\":54730,\"start\":54722},{\"end\":54736,\"start\":54730},{\"end\":55061,\"start\":55055},{\"end\":55068,\"start\":55061},{\"end\":55405,\"start\":55399},{\"end\":55417,\"start\":55405},{\"end\":55747,\"start\":55730},{\"end\":55757,\"start\":55747},{\"end\":55980,\"start\":55973},{\"end\":55987,\"start\":55980},{\"end\":55995,\"start\":55987},{\"end\":56237,\"start\":56223},{\"end\":56248,\"start\":56237},{\"end\":56710,\"start\":56703},{\"end\":56718,\"start\":56710},{\"end\":56995,\"start\":56982},{\"end\":57320,\"start\":57310},{\"end\":57330,\"start\":57320},{\"end\":58743,\"start\":58712},{\"end\":58749,\"start\":58743}]", "bib_venue": "[{\"end\":44094,\"start\":44023},{\"end\":44360,\"start\":44273},{\"end\":44573,\"start\":44536},{\"end\":44823,\"start\":44787},{\"end\":45240,\"start\":45204},{\"end\":45576,\"start\":45540},{\"end\":45875,\"start\":45840},{\"end\":46143,\"start\":46117},{\"end\":46384,\"start\":46308},{\"end\":46836,\"start\":46810},{\"end\":47139,\"start\":47103},{\"end\":47417,\"start\":47376},{\"end\":47748,\"start\":47703},{\"end\":48078,\"start\":48037},{\"end\":48336,\"start\":48274},{\"end\":48579,\"start\":48541},{\"end\":48846,\"start\":48746},{\"end\":49253,\"start\":49185},{\"end\":49643,\"start\":49569},{\"end\":49923,\"start\":49862},{\"end\":50278,\"start\":50223},{\"end\":50557,\"start\":50462},{\"end\":50895,\"start\":50839},{\"end\":51136,\"start\":51114},{\"end\":51483,\"start\":51413},{\"end\":51920,\"start\":51865},{\"end\":52308,\"start\":52267},{\"end\":52693,\"start\":52682},{\"end\":53053,\"start\":53012},{\"end\":53441,\"start\":53378},{\"end\":53810,\"start\":53757},{\"end\":54159,\"start\":54106},{\"end\":54501,\"start\":54448},{\"end\":54800,\"start\":54736},{\"end\":55145,\"start\":55068},{\"end\":55484,\"start\":55417},{\"end\":55728,\"start\":55670},{\"end\":55971,\"start\":55943},{\"end\":56325,\"start\":56248},{\"end\":56701,\"start\":56625},{\"end\":57014,\"start\":56995},{\"end\":57342,\"start\":57330},{\"end\":57724,\"start\":57720},{\"end\":58710,\"start\":58384},{\"end\":59466,\"start\":59324},{\"end\":60281,\"start\":59861},{\"end\":54851,\"start\":54802},{\"end\":55209,\"start\":55147},{\"end\":55538,\"start\":55486},{\"end\":56389,\"start\":56327}]"}}}, "year": 2023, "month": 12, "day": 17}