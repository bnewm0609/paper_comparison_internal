{"id": 254275009, "updated": "2023-10-05 07:18:58.017", "metadata": {"title": "Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning", "authors": "[{\"first\":\"Ukyo\",\"last\":\"Honda\",\"middle\":[]},{\"first\":\"Taro\",\"last\":\"Watanabe\",\"middle\":[]},{\"first\":\"Yuji\",\"last\":\"Matsumoto\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Discriminativeness is a desirable feature of image captions: captions should describe the characteristic details of input images. However, recent high-performing captioning models, which are trained with reinforcement learning (RL), tend to generate overly generic captions despite their high performance in various other criteria. First, we investigate the cause of the unexpectedly low discriminativeness and show that RL has a deeply rooted side effect of limiting the output words to high-frequency words. The limited vocabulary is a severe bottleneck for discriminativeness as it is difficult for a model to describe the details beyond its vocabulary. Then, based on this identification of the bottleneck, we drastically recast discriminative image captioning as a much simpler task of encouraging low-frequency word generation. Hinted by long-tail classification and debiasing methods, we propose methods that easily switch off-the-shelf RL models to discriminativeness-aware models with only a single-epoch fine-tuning on the part of the parameters. Extensive experiments demonstrate that our methods significantly enhance the discriminativeness of off-the-shelf RL models and even outperform previous discriminativeness-aware methods with much smaller computational costs. Detailed analysis and human evaluation also verify that our methods boost the discriminativeness without sacrificing the overall quality of captions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.03230", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/HondaWM23", "doi": "10.1109/wacv56688.2023.00118"}}, "content": {"source": {"pdf_hash": "5c43607c7f10284003e0072b8632ef7427d3df06", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.03230v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "641cd12a0754e783ba0366706560428ca7ccc685", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5c43607c7f10284003e0072b8632ef7427d3df06.txt", "contents": "\nSwitching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning\n\n\nUkyo Honda hondaukyo@cyberagent.co.jptaro@is.naist.jpyuji.matsumoto@riken.jp \nCyberAgent, Inc\n\n\nRIKEN\n\n\nTaro Watanabe \nNara Institute of Science and Technology\n\n\nYuji Matsumoto \nRIKEN\n\n\nSwitching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning\n\nDiscriminativeness is a desirable feature of image captions: captions should describe the characteristic details of input images. However, recent high-performing captioning models, which are trained with reinforcement learning (RL), tend to generate overly generic captions despite their high performance in various other criteria. First, we investigate the cause of the unexpectedly low discriminativeness and show that RL has a deeply rooted side effect of limiting the output words to high-frequency words. The limited vocabulary is a severe bottleneck for discriminativeness as it is difficult for a model to describe the details beyond its vocabulary. Then, based on this identification of the bottleneck, we drastically recast discriminative image captioning as a much simpler task of encouraging low-frequency word generation. Hinted by long-tail classification and debiasing methods, we propose methods that easily switch off-the-shelf RL models to discriminativenessaware models with only a single-epoch fine-tuning on the part of the parameters. Extensive experiments demonstrate that our methods significantly enhance the discriminativeness of off-the-shelf RL models and even outperform previous discriminativeness-aware methods with much smaller computational costs. Detailed analysis and human evaluation also verify that our methods boost the discriminativeness without sacrificing the overall quality of captions. 1\n\nIntroduction\n\nImage captioning plays a fundamental role at the intersection of computer vision and natural language processing by converting the information in images into natural language descriptions. Generated captions can be used in various downstream tasks: aiding visually impaired users [19], visual question answering on images and videos [16,31], visual dialogue [68], and news generation [79].  For those downstream tasks, captions should be discriminative: captions should describe the characteristic and important details of the input images [51]. However, current captioning models tend to generate overly generic captions [12,11,64,66]. In particular, models trained with the standard reinforcement learning (RL) [50], which is the de facto standard training method in current image captioning [55], unexpectedly perform poorly in discriminativeness despite the significant advantages in various other criteria [39,62]. For example, a high-performing Transformer [57] captioning model trained with RL generates exactly the same caption for the four different images shown in Figure 1, ignoring the other salient details of each image.\n\nTo address the problem of overly generic captions, studies have been intensely conducted on discriminative image captioning, which is also called distinctive image captioning or descriptive image captioning. Previous research has created new RL rewards regarding discriminativeness or new model architectures to enhance discriminativeness. These approaches improved the discriminativeness; however, their models come with additional computations, re-quire retraining from scratch, and do not shed light on the cause of existing models' low discriminativeness.\n\nInstead of creating or paying those computational costs, we first analyze the cause of the unexpectedly low discriminativeness of off-the-shelf RL models, i.e., pre-trained, existing RL models, to explore ways to improve their discriminativeness. Our first contribution is the identification of a deeply rooted side effect in RL that limits output words to high-frequency words. The limited vocabulary is a severe bottleneck for discriminativeness as it is difficult for a model to describe the details beyond its vocabulary.\n\nGiven this identification of the bottleneck, now we can directly address the bottleneck by simply encouraging the generation of low-frequency words. This task relaxation allows us to introduce long-tail classification and debiasing methods to discriminative image captioning for the first time. Our second contribution is our effective and efficient methods that switch any off-the-shelf RL models to discriminativeness-aware models with only a singleepoch fine-tuning on the part of the parameters. Unlike previous approaches, our methods do not require any discriminativeness rewards, new model architectures, or retraining from scratch.\n\nExtensive experiments demonstrate that increasing lowfrequency words in outputs significantly boosts discriminativeness from off-the-shelf RL models and even outperforms previous discriminativeness-aware models with much smaller computational costs. These results verify that the limited vocabulary of RL models has been the major cause of their low discriminativeness. Detailed analysis and human evaluation also show that our methods enhance the discriminativeness without sacrificing the overall quality. We believe that our novel findings on the cause of low discriminativeness and the practical solutions to it will significantly impact future research on discriminative image captioning.\n\n\nDiscriminativeness and a Bottleneck of RL\n\nCurrently, RL is the de facto standard training method for models used in image captioning because it significantly improves the performance in various evaluation metrics [55]. However, it does not improve discriminativeness and may even decrease it [39,62]. In this section, we examine the cause of the unexpectedly low discriminativeness.\n\n\nRL in Image Captioning\n\nWe provide a brief overview of the standard RL algorithm used in image captioning. It was proposed by [48] and refined by [50]. Their goal was to directly optimize non-differentiable test-time metrics by minimizing the negative expected reward: where w s = (w s 1 , ..., w s T ) is a sequence sampled from a policy p \u03b8 , I is an input image, and r(\u00b7) is a reward function. To compute the gradient of L(\u03b8), [48] applied the REIN-FORCE algorithm [69] to text generation. The algorithm approximates the gradient as follows:\nL RL (\u03b8) = \u2212E w s \u223cp \u03b8 (w s |I) [r(w s )],(1)\u2207 \u03b8 L RL (\u03b8) \u2248 \u2212(r(w s ) \u2212 b)\u2207 \u03b8 log p \u03b8 (w s | I). (2)\nHere, b is a baseline reward that reduces the variance in the gradient. Typically, the reward function r(\u00b7) is CIDEr [59], and the baseline reward b is a reward for a sequence sampled with greedy decoding [50].\n\n\nRL Limits Vocabulary\n\nDespite its effectiveness, RL has been found not to improve discriminativeness and somehow decrease the number of unique n-grams in output captions [39,62]. As the relation between RL and these two negative effects is not obvious, it has been just considered a curious case.\n\nWe elucidate for the first time the relation between RL and limited vocabulary by combining two recent findings.\n\n(1) RL has been shown to make the output distribution peaky [8,30]. RL samples sequences from policy p \u03b8 (See Eq. (1)). Typically, p \u03b8 is initialized with a text-generation model pre-trained with the Cross-Entropy (CE) loss on ground-truth text. In text generation, however, the initialized p \u03b8 outputs peaky distributions, and thus, RL samples and rewards the words at the peak only, shaping more peaky distributions [8]. Then, where does p \u03b8 tend to be peaky?\n\n(2) Text-generation models have been theoretically and empirically shown to output distributions peaky at highfrequency words in the training corpus [46,49,13,24]. These two findings conclude that RL shifts the probability mass from low-frequency words to high-frequency words by only sampling and rewarding the latter. Figure 2 confirms the above by plotting the relative frequency of the words sampled for the training images. The words are sorted by their frequency in ground-truth captions and divided into 200 bins. Compared to the ground-truth captions and the sequences sampled with a CE model, the sequences sampled with an RL model are clearly limited to the high-frequency words, forming a peaky distribution 2 .\n\n\nVocabulary Limits Discriminativeness\n\nNeural captioning models typically generate captions using sequential vocabulary-size classification [61]. However, the actual vocabulary a model can generate is much smaller than the entire vocabulary as the output distribution is highly skewed towards high-frequency words. If the actual vocabulary cannot cover the details of an image, the model is forced to avoid those details and output only the information that high-frequency words can describe. For example, the blue words in Figure 1 are not in the actual vocabulary of the RL model; these words have never been generated during evaluation. As a result, the RL model had to ignore the characteristic relations tied and docked and ended up describing exactly the same for all four images.\n\nBased on the observations, now we can hypothesize that the unexpectedly low discriminativeness of RL models has been rooted in the limited vocabulary. This identification of the bottleneck is a key contribution as it allows us to address the low discriminativeness directly at the root.\n\n\nMethods to Relieve the Bottleneck\n\nWe have shown that RL results in the limited vocabulary as it steals the probability mass from low-frequency words. Thus, increasing those low-frequency words is the easy yet critical solution to the bottleneck. One way to achieve this is to jointly optimize both the RL loss and the CE loss on ground-truth captions so that the low-frequency words in ground-truth captions would be more likely to be sampled during RL training [64]. However, this approach still relies on the sampling from a skewed policy and requires retraining from scratch.\n\nTo increase the actual vocabulary more effectively and efficiently, we refine the mapping from encoded features to low-frequency words. This refinement can be applied to any RL models and can be achieved by modifying only the mapping function parameters with a single-epoch fine-tuning.\n\n\nSimple Fine-Tuning (sFT)\n\nThe first method is a simple fine-tuning (sFT). It is based on a decoupled two-stage training [27], which is a current strong baseline model for long-tail classification [56,44,65]. [27] decoupled the learning procedure into representation learning and classification, and then found that classification, i.e., the mapping from representations to label distributions, is critical for long-tail classification. They decoupled the classification model f \u03b8 (\u00b7) into an encoder g \u03b8e (\u00b7) and a classifier consisting of weight and bias parameters: f \u03b8 (x) = W \u22a4 g \u03b8e (x)+b. Representation learning is the first stage of training, where they trained the entire classification model f \u03b8 (\u00b7) on a full training dataset. The second stage is classification, where they fixed the encoder parameters \u03b8 e and adjusted only the classifier parameters. For the second-stage adjustment, they applied class-balanced sampling to encourage learning on low-frequency labels.\n\nFollowing [27], we decouple a captioning model into an encoder and a classifier. In image captioning, the firststage training of [27] corresponds to RL training on the full training dataset. The second-stage training corresponds to adjusting the classifier parameters on the vocabularybalanced sequences. However, sampling from the skewed policy of text-generation models cannot provide sequences containing low-frequency words (Section 2.2). Thus, we use ground-truth captions as relatively vocabulary-balanced samples. sFT simply fine-tunes the classifier parameters of a pre-trained RL captioning model by minimizing the CE loss on ground-truth captions:\nL CE (\u03b8) = \u2212 1 T T t=1 log p\u03b8(w g t | w g <t , I),(3)\nwhere w g = (w g 1 , ..., w g T ) is a ground-truth caption of image I and\u03b8 denotes the model parameters \u03b8 that are initialized with RL training. Let the softmax function \u03d5(\u00b7) be\n\u03d5 wi,\u03b2 (z) = exp(\u03b2z wi ) wj \u2208W exp(\u03b2z wj ) ,(4)\nwhere z wi indicates the element of a vector z \u2208 R |W| at the index of a word w i \u2208 W. W is the entire vocabulary. \u03b2 is an inverse-temperature hyperparameter that controls the steepness of the softmax distribution. Then, the conditional probability p \u03b8 (w g t | w g <t , I) is computed as follows:\np \u03b8 (w g t | w g <t , I) = \u03d5 w g t ,\u03b2 (s t \u03b8 (w g , I)),(5)s t \u03b8 (w g , I) = W \u22a4 g \u03b8e (w g <t , I) + b,(6)\nwhere W \u2208 R d\u00d7|W| and b \u2208 R |W| . d is the dimension of the hidden states of an encoder g \u03b8e (\u00b7). We use LSTM [23] or Transformer [57] for g \u03b8e (\u00b7). During fine-tuning, only the classifier parameters {W , b} \u2208\u03b8 are updated with the gradients \u2207 W L CE (\u03b8) and \u2207 b L CE (\u03b8), respectively.\n\n\nWeighted Fine-Tuning (wFT)\n\nGround-truth captions contain more low-frequency words than sampled sequences, but some low-frequency words are still difficult to learn because of their low frequency. Our second method is weighted fine-tuning (wFT), which further pursues vocabulary balance by rebalancing the loss of high-frequency words and low-frequency words in ground-truth captions.\n\nTo rebalance the loss, we exploit the frequency bias of RL models: RL models overly assign the probability to high-frequency words but not to low-frequency words. Given the properties of the frequency bias, fine-tuning for discriminativeness should focus more on the words that an RL model is not confident of but should be avoided on the words that an RL model is confident of. wFT incorporates these heuristics by modifying the probability p \u03b8 of L CE to the bias product (BP) [9,20,22] probability, p \u03b8,\u03b8 \u2032 :\np \u03b8,\u03b8 \u2032 (w g t | w g <t , I) = \u03d5 w g t ,1 log p \u03b8 (\u00b7 | w g <t , I) \u03d5 \u00b7,\u03b2 (s t \u03b8 (w g , I)) + log p \u03b8 \u2032 (\u00b7 | w g <t , I) \u03d5 \u00b7,\u03b2 \u2032 (s t \u03b8 \u2032 (w g , I)) ,(7)\nwhere \u03d5 \u00b7,\u03b2 (z) \u2208 R |W| . By inserting p \u03b8,\u03b8 \u2032 into L CE , we define the objective function of wFT as follows:\nL BP (\u03b8) = \u2212 1 T T t=1 log p\u03b8 ,\u03b8 \u2032 (w g t | w g <t , I).(8)\nSimilar to sFT, the parameters \u03b8 and \u03b8 \u2032 are initialized with the same RL model to be\u03b8 and\u03b8 \u2032 . The difference is that, although the classifier parameters of\u03b8 are updated, all the parameters of\u03b8 \u2032 are fixed during fine-tuning 3 . Figure 3 shows the change in the BP loss compared to the CE loss. The BP severely suppresses the loss when the frequency-biased policy p \u03b8 \u2032 is confident, and largely increases the loss when p \u03b8 \u2032 is not confident. In this way, the BP allows models to unlearn the frequency bias learned with RL. As with sFT, only the classifier parameters {W , b} \u2208\u03b8 are updated with the gradients \u2207 W L BP (\u03b8) and \u2207 b L BP (\u03b8), respectively. The previous BP methods used the probability p \u03b8 during evaluation to avoid incorporating the bias of p \u03b8 \u2032 into the predictions [9,20]. Although it worked well in their classification tasks, we found this train-test gap makes the decoding unstable in text generation. To mitigate the train-test gap, we use two variants of decoding: (1) decode with p \u03b8 but use Figure 3. Visualization of the CE loss \u2212 log p \u03b8 (wi) and BP loss \u2212 log p \u03b8,\u03b8 \u2032 (wi). To compute the BP loss, we need the entire distribution of {p \u03b8 (wi)}w i \u2208W and {p \u03b8 \u2032 (wi)}w i \u2208W . Here, we set the index i to 1 and assigned 1 5 (1 \u2212 p \u03b8 (w1)) to the words of the next five indices, w2, ..., w6. This is because we observed that the five most probable words occupied 99% of the probability in the output distribution of the RL models. We assumed that the five most probable words were the same between p \u03b8 and p \u03b8 \u2032 as the parameters were initialized with the same RL model. Thus, we assigned 1 5 (1 \u2212 p \u03b8 \u2032 (w1)) to the words of the next five indices, w2, ..., w6, likewise p \u03b8 . Here, \u03b2 and \u03b2 \u2032 were set to 1. a small \u03b2 \u2032 for p \u03b8 \u2032 during training to ease the gap between p \u03b8 and p \u03b8,\u03b8 \u2032 , or (2) use p \u03b8,\u03b8 \u2032 during both training and decoding (BP decoding) as p \u03b8,\u03b8 \u2032 itself is already less biased than p \u03b8 \u2032 .\n\n\nExperiments\n\n\nSetup\n\nDataset and Metrics. We used the MS COCO captioning dataset 4 [38,6] with Karpathy splitting [28]. After preprocessing, the entire vocabulary size |W| was 9,487 5 . In the evaluation, the captions were decoded using a beam search of size 5 and evaluated using various evaluation metrics. Specifically, we used CIDEr [59] SPICE [1], Improved BERTScore (BERTS+) [74], TIGEr [25], CLIP-Score (CLIPS), and RefCLIPScore (RefCLIPS) [21]. Note that the correlation with human judgments increases in the above order, with RefCLIPS indicating the state-ofthe-art correlation [21,29]. Following the previous studies [39,62,54], we evaluated discriminativeness with R@K scores: the percentage of captions with which a pre-trained image-text retrieval model [15] Table 1. Comparison of baseline models, our models , and state-of-the-art discriminativeness-aware models. Automatic evaluation results on the MS COCO test set. Unique-1 and Unique-S indicate the number of unique unigrams and sentences, respectively. Length is the average length of the output captions. Scores with \u2020 were reported in [39]. Other scores were reproduced by us.\n\nthat the model generates more discriminative captions with characteristic information of images. Evaluation was conducted in a single run for each model. See Appendix 4 for the libraries and settings we used for these evaluations.\n\nComparison Models. Following [62], we used Att2in [50], UpDown [2], and Transformer [57] as the baseline models. The models were pre-trained with the standard RL [50] and are publicly available 6 . In addition to the baseline models, we compared our models with state-ofthe-art discriminativeness-aware models: CIDErBtw [62], NLI [54], DiscCap [41], and Visual Paraphrase [39]. The first three created new discriminativeness rewards to be optimized with RL. Visual Paraphrase introduced a new model architecture to paraphrase simpler captions to more complex captions. See Section 5 for more details of these models. As we mentioned in the beginning of Section 3, the CE loss on ground-truth captions can be utilized in a different way from our methods. We report the results of jointly optimizing the RL loss and CE loss (Joint CE [64,14]). It optimizes L Joint (\u03b8) = \u03bbL RL (\u03b8) + (1 \u2212 \u03bb)L CE (\u03b8) during RL training. We also tested Only CE, which sets \u03bb = 0 to solely optimize the CE loss, as the baseline without RL. See Appendices 12 and 13 for more comparisons [76,36,7]. Hyperparameters. Our models used the same hyperparameters as the baseline models, except for the epoch size, 6 https://github.com/ruotianluo/self-critical.py torch: {Att2in, UpDown, Transformer}+self critical learning rate, and \u03b2 \u2032 in Eq. (7). We set the epoch size for fine-tuning to 1 and searched for the best learning rate from {1e-3, 1e-4, 1e-5, 1e-6}. For BP in Eq. (7), we set \u03b2 = 1 and searched for the best \u03b2 \u2032 from {0.1, 1}. As with our models, we set all hyperparameters of the CE-based models to the same as the baseline models except for the \u03bb \u2208 {0, 0.2, 0.5, 0.8}. We disabled scheduled sampling [4] for our fine-tuning and the CE loss to separate them from the RL loss strictly. We took the best hyperparameters according to the R@1 scores in the validation set. Note that we used different hyperparameters for the wFT with different decoding methods (See Section 3.2). Appendix 5 shows the best hyperparameters. We followed the previous work for the hyperparameters of the other models.\n\nAll the models except Visual Paraphrase had the same size of trainable parameters as their baselines. See Appendix 6 for the exact number of parameters. Our finetuning was completed in around 10 minutes using a single GPU of 16 GB memory. See Appendix 7 for the exact time for training and comparison with other methods.\n\n\nComparison with Baseline Models and\n\nDiscriminativeness-Aware Models Table 1 shows the results compared to those obtained with the baseline models and state-of-the-art discriminativeness-aware models. Vocabulary. First, we observe that our methods (sFT and wFT) successfully increase the actual vocabulary size: both of them considerably increased Unique-1 compared to all the baseline models. wFT increased the vocabulary more than sFT, indicating that rebalancing the loss further encouraged low-frequency word generation. The increased vocabulary resulted in more specific captions to each image: Unique-S also increased significantly. Consistent with previous studies [64,39,62], the models trained with the CE loss (Joint CE and Only CE) achieved the larger vocabulary than the baseline RL models. However, the improvement of our methods was even larger than these CE-based models. Despite the significant increase in the vocabulary size, our method kept the captions concise: the average sentence length was close to those of the baseline models. Discriminativeness. Our goal is to enhance the discriminativeness of RL models by addressing their limited vocabulary. As expected, our methods successfully improved the discriminativeness: the R@K scores of our models were considerably higher than those of the baselines. Corresponding to the better improvement in vocabulary size, wFT increased discriminativeness more than sFT. These results confirm our hypothesis that the limited vocabulary of RL models has been a major bottleneck for discriminativeness.\n\nAmong the Att2in-based models, Visual Paraphrase achieved the highest discriminativeness. However, this model is not directly comparable to the others because it increases the trainable parameters for its specialized model architecture. Moreover, its improvement in discriminativeness was achieved at the expense of conciseness, which is another desirable property for discriminative image captions [51]: its sentence length was substantially longer than the other models. DiscCap performed comparably with our models, but its reward requires high computational costs. CIDErBtw and NLI proposed more lightweight rewards to be applicable to larger models, but they still need retraining from scratch. Among the larger models (UpDown and Transformer), our models achieved the highest discriminativeness despite the small computational cost. Standard Evaluation. As our methods increase lowfrequency words in outputs, the outputs are likely to include the words that are out-of-references (OOR). That is, lowfrequency words may not be covered by reference captions regardless of their correctness due to the low frequency. These low-frequency OOR words unfairly decrease scores in conventional evaluation metrics because those metrics count exact matches in the surface form of text 7 .\n\nTo fairly evaluate the OOR words, recent metric research has focused on soft matching metrics [26,21]. Softmatching metrics can evaluate the semantic similarity between target captions and reference captions beyond the surface form of text by utilizing pre-trained language mod-els (PLMs) [74,77] or pre-trained cross-modal models (PCMs) [25,33,21]. Their correlation with human judgments is significantly higher than that of exact-matching metrics in both precision and recall [29]. In particular, PCM-based metrics, which can utilize image features in addition to reference captions, have substantially enhanced the evaluation performance and have achieved the state-of-theart correlation with human judgments [21,29].\n\nGiven the above advantages, we employed soft-matching metrics in addition to conventional exact-matching metrics. Not surprisingly, our models decreased the scores in the exact-matching metrics (CIDEr and SPICE). However, our models scored comparably with the baselines in the PLMbased metric (BERTS+) and rather outperformed them in the state-of-the-art PCM-based metrics (TIGEr, CLIPS, and RefCLIPS). The higher performance in the superior softmatching metrics indicates that our methods do not degrade the overall quality of captions. To further validate the overall quality of our output captions, the following Section 4.3 analyzes the cause of this performance gap in more detail.\n\n\nAnalysis of the Performance Gap\n\nProperties of OOR Words. The critical difference between the conventional exact-matching metrics and the recent soft-matching metrics is the (in)ability to evaluate OOR words 8 . Based on the difference, we hypothesize that the performance gap is caused by a difference in the properties of OOR words. We analyzed the OOR words of our models, comparing with those of RL baselines and Only CE, which scores similarly to our models in exact-matching metrics but decreases soft-matching scores in contrast to our models. Table 2 shows the number of OOR words and their average frequency rank. The frequency rank refers to the order of words when sorted by their frequency in training captions; the most frequent word ranks 1st, and the value of rank increases as the frequency decreases. Although our models and Only CE output the similar number of OOR words, the significant difference in the frequency rank indicates that the properties of our OOR words are different from those of Only CE; that is, the OOR words of our models consist of much more low-frequency words than those of Only CE. Low-frequency words are likely to be OOR by the nature of their frequency, regardless of their correctness.\n\nThe soft-matching metrics could tell this difference and scored our models higher than Only CE models and even higher than baseline RL models. Especially, this tendency was more clear in the state-of-the-art PCM-based metrics  Table 3. Human evaluation results on the subset of the MS COCO test set. The discriminativeness score of Transformer RL was fixed at 3.00 because we set it as the baseline. * / * * indicates that a score is statistically significantly different from that of the baseline model (t-test with p < 0.05/0.01); one-sample t-test for discriminativeness and independent two-sample t-test for the other criteria.\n\n(TIGEr, CLIPS, and RefCLIPS). On the contrary, the exactmatching metrics (CIDEr and SPICE) could not tell the difference by definition and decreased the scores roughly in proportion to the number of OOR words. Appendix 8 shows the qualitative analysis of the underrated captions.\n\nComparison with Human-Annotated Captions. Humanannotated captions are known to show low exact-matching scores despite their high quality [29,39,11]. In Table 2, we observe that human-annotated captions (Human) 9 have similar properties to ours: a large number of low-frequency OOR words, low exact-matching scores, but high scores in the state-of-the-art metrics (CLIPS and RefCLIPS). Repetition. We also confirmed that the decrease in exactmatching scores was not caused by repetition, which is a typical side effect of heavily maximizing discriminativeness rewards [64,60]. Table 2 shows that our models' repetition rates 10 were rather lower than those of baselines. 9 Following [39,11], we randomly sampled one reference caption for each image and evaluated the similarity against the rest of the references. 10 Let C be a set of captions; f n (\u00b7) and u n (\u00b7) be the functions to return n-grams and unique n-grams, respectively. We computed the repetition rate (Rep) by 1\n|C|N |C| i=1 N n=1 1 \u2212 |u n (C i )| |f n (C i )| , where we set N = 4.\nConclusion. From the above results, we conclude that the lower exact-matching scores of our models are caused by the nature of low-frequency words and the deficiency of exact-matching metrics, not by the degeneration of our models. The results of the human evaluation in the following Section 4.4 further support this conclusion.\n\n\nHuman Evaluation\n\nAs discussed in Sections 4.2 and 4.3, automatic evaluation of our models has difficulty due to the OOR words caused by the low frequency. To further validate the performance of our models, we conducted human evaluations using Amazon Mechanical Turk (AMT) on three criteria: discriminativeness, correctness, and fluency. Correctness and fluency are absolute scores: we instructed workers to give a maximum score 5 to the captions that did not contain incorrect information (ungrammatical or unnatural expressions) in terms of correctness (fluency). In contrast, discriminativeness is designed as a relative score because it is difficult to set an absolute standard for discriminativeness; unlike correctness or fluency, we cannot define the perfectly discriminative captions. Following [62], we instructed the workers to determine the discriminativeness of a caption by comparing the caption with that of a baseline model 11 .\n\nWe evaluated the Transformer-based models, which performed the best in the automatic evaluation. Although wFT with BP decoding performed better, here we picked up wFT with p \u03b8 decoding to set the total number of parameters for decoding strictly the same across the models. Following [62], we randomly selected 50 images from the MS COCO test set and assigned five workers to each image. See Appendix 9 for more details on the AMT instruction. Table 3 shows the results. wFT, which had the highest R@K scores, also achieved the highest discriminativeness here. wFT achieved the same or higher correctness and fluency than the baseline model, in contrast to the exact-matching scores in Table 2. These results are consistent with the results of the state-of-the-art soft-matching metrics, confirming again that our methods do not degrade the quality of captions.\n\n\nRelated Work\n\nImage Captioning is the task of describing images in natural languages. The quality of captions has been remarkably improved by recent advances such as the encoderdecoder captioning model [61], attention mechanism [73], RL training [48,50], attention over bounding box features [2], large-scale pre-training [36], and large-scale captioning datasets [75,38,6,32,52]. Despite these advancements, current captioning models generate overly generic captions [12,11,64,66]. Discriminative Image Captioning has been explored to generate more informative captions. [51] was the first to study it. They defined the more informative captions as the captions that concisely describe the information discriminative from distractor images, i.e., images similar to an input image. [3] proposed neural listener and speaker models that cooperate to generate discriminative captions for abstract scenes. [45] adapted the models to single-colored images. [58,10] extended the domain to real images and improved inference efficiency. [63] proposed a memory attention network to describe unique objects among distractor images. [43] introduced a dataset with harder distractor images.\n\nThese approaches require selecting distractor images for inference. [41] and [40] proposed the methods that do not require this step. Their models learn to generate discriminative captions by maximizing the R@K scores for sampled captions using RL [50]. The R@K scores are computed with a pre-trained image-text retrieval model [15] over images in a mini-batch. [60] proposed a method to jointly train the image-text retrieval model and captioning model. Despite their effectiveness, R@K scores are associated with high computational costs and require a large batch size. Recently, CIDErBtw [62] and NLI [54] achieved state-of-the-art discriminativeness with more lightweight rewards. They weighted the contribution of ground-truth captions for the CIDEr reward according to their differences from similar but different captions [62] or their entailment scores against other ground-truth captions [54]. Another approach exploited unrelated captions as negative examples and trained caption generators with contrastive learning [12] or GAN [12,17].\n\nVisual Paraphrase [39] and [70] are related to our work in that they exploited low-frequency n-grams to enhance discriminativeness. [39] divided ground-truth captions into two subsets according to n-gram TF-IDF scores and proposed a new model to paraphrase low TF-IDF captions into high TF-IDF ones. [70] proposed the use of n-gram TF-IDF scores as an additional reward to a variant of R@K reward. Different from above approaches, our objective is set to remedy the low discriminativeness of existing RL models. Our models can be achieved with single-epoch fine-tuning of pre-trained RL models, without requiring either drastic changes in the model architecture [39], additional computational costs of rewards [70], or retraining from scratch. Diverse Image Captioning is the task of generating a set of diverse captions for a given image [67]. Diverse image captioning is aimed at enumerating various pieces of information with a set of captions, whereas discriminative image captioning aims to concisely describe the most characteristic information with a single caption. Similar to this study, some studies utilized captions that contained more lowfrequency words, such as ground-truth captions [64,42] or captions sampled from CE models [53]. Their models learn to generate these captions in addition to the captions sampled from RL models. However, these approaches still rely on sampling from skewed policies and require retraining of a model from scratch. Long-Tail Classification has been studied extensively in various tasks as label imbalance is prevalent across datasets [78,35]. In text-generation tasks, label imbalance exists in the frequency of words. Previous approaches have addressed the imbalance by normalizing classifier weights [46,49] or using variants of Focal loss [49,18,26,72,37]. In contrast to these approaches, we adapted longtail classification to mitigate the side effects of RL in the context of discriminative image captioning. Appendix 10 shows that our methods outperformed these approaches.\n\n\nConclusion\n\nWe have investigated the cause of overly generic captions of RL models and found out that RL decreases the discriminativeness by limiting the output words to highfrequency words. We propose the lightweight fine-tuning methods to address the bottleneck directly and achieve significantly higher discriminativeness with only the slight modification on off-the-shelf RL models. Our identification of the bottleneck and practical solutions will significantly impact future research on discriminative image captioning.\n\nAs an additional practical advantage, our models can control the granularity of descriptions from coarse to fine by just switching the off-the-shelf/fine-tuned classifier parameters. In terms of broader impact, our methods can be easily applied to the RL models in other text generation tasks, such as machine translation [71], summarization [47], and dialogue generation [34] to enrich the output vocabulary.  \n\n\nLimitations and Ethical Considerations\n\nOur experiments were limited to the MS COCO dataset, which is the standard dataset for image captioning. The images belong to the general domain (real images of common objects), and the captions are in English only. To compensate for the limitation, we have demonstrated the effectiveness of our methods with the multiple baseline models.\n\nOur current methods have a limitation in that they cannot select discriminative ones among low-frequency words. Although discriminative in general, low-frequency words do not always describe more specific information than others. Figure 1 shows the examples. Our model output relatively low-frequency hypernyms such as person, animal, and fur-niture instead of the more frequent but more specific hyponyms: man, sheep, and couch. Utilizing thesauruses like WordNet [4] will be a promising approach to reduce those relatively low-frequency hypernyms from outputs.\n\nThe dataset contains social biases, and captioning models have the risk of amplifying those biases [24,25,6]. Our methods are also not free from the risk, as they are not designed to reduce those social biases from existing models. These examples further support our hypothesis that the limited vocabulary of RL models hinders discriminativeness. Figure 3 shows the results of the relative frequency of the words sampled for the training images by the LSTM-based models: Att2in [18] and UpDown [1]. Similar to the Transformer model, the sequences sampled with the LSTM-based RL models are clearly limited to high-frequency words, forming the peaky distributions.\n\n\nFurther Output Examples\n\n\nPeaky Distributions in Other Models\n\n\nLibraries for Evaluation\n\nWe used the following libraries for evaluation with all the hyperparameters set to the default values. CIDEr, SPICE, CLIPS, and RefCLIPS https://gi thub.com/jmhessel/pycocoevalcap BERTS+ https://github.com/ck0123/improv ed-bertscore-for-image-captioning-eval Transformer RL: a group of birds standing in the water +wFT: a large group of flamingos stand in shallow water NLI: a group of pink umbrellas are standing in the water Human: a flock of pink flamingos standing in shallow water  \n\n\nBest Hyperparameters\n\nWe searched for the best hyperparameters for the learning rate from {1e-3, 1e-4, 1e-5, 1e-6}, and the inversetemperature hyperparameter \u03b2 \u2032 of Eq. (7) from {0.1, 1}. The best learning rate was 1e-5 for Transformer models and 1e-4 for the other models (Att2in and UpDown). The best \u03b2 \u2032 was 0.1 for wFT with p \u03b8 decoding and 1 for wFT with BP decoding. Note that sFT does not use \u03b2 \u2032 . The best learning rate was the same in CE-based models (Joint CE and Only CE): 1e-5 for Transformer and 1e-4 for the others. The best \u03bb \u2208 {0.2, 0.5, 0.8} for Joint CE was 0.8 for Transformer and 0.2 for the others.\n\n\nThe Number of Parameters\n\nThe exact number of parameters was 14,451,985 for Att2in, 52,125,025 for UpDown, and 57,474,832 for Transformer. Note that the parameters \u03b8 \u2032 are not included because they are not trainable and fixed through the entire training and evaluation; rather, the actual trainable parameters are decreased to the classifier parameters in our models. Visual Paraphrase has double decoders of Att2in; thus, it increases  Table 1. Time to train discriminativeness-aware captioning models. Note that we excluded the time for initialization before RL because there is not much difference among the methods. Results for the baseline RL models are shown in gray text because we did not train these models but used publicly-available pre-trained models.\n\nthe number of trainable parameters and requires training of the specialized model from scratch. Table 1 shows the time to train discriminativeness-aware captioning models. We used a single GPU of 16 GB memory for all training. Clearly, our methods require far less time for training. This is because our methods do not require retraining from scratch but only require a single-epoch fine-tuning to publicly-available pre-trained RL models. Figure 4 shows caption examples, automatic evaluation scores, and reference captions. Clearly, our wFT model correctly described all five images with diverse vocabulary. However, the CIDEr scores for our captions were considerably lower than those for the baseline model captions. The cause of this underrating is the small coverage of the reference captions: the reference captions rarely include the lowfrequency words colored in blue due to their low frequency. Conventional exact-matching metrics such as CIDEr cannot evaluate those correct-but-OOR words by the definition of exact-matching. In contrast, RefCLIPS, the state-of-the-art soft-matching metric, can consider the information not covered by reference captions by incorporating image features. Figure 4 shows that RefCLIPS evaluated the correct-but-OOR words more correctly and gave more plausible scores to our captions. These examples further support our conclusion that the lower exact-matching scores of our models are  caused by the nature of low-frequency words and the deficiency of exact-matching metrics, not by the degeneration of our models.\n\n\nComparison of Computational Cost\n\n\nQualitative Analysis of Underrated Captions\n\n\nDetails of Human Evaluation\n\nWe show our AMT interface in Figure 5. Each image was evaluated with the five questions in the discrete 5-point scale. We required workers to satisfy the following qualifications: being an AMT Master and living in the U.S. Workers were notified that this experiment was intended to evaluate caption quality. We paid $0.1 for each image, and the median of the actual working time was 41 seconds per image. The hourly reward was estimated as $8.78, which is higher than the minimum wage in the U.S., $7.25 per hour.\n\n\nComparison with Other Long-Tail Classification Methods\n\nWe adapted the long-tail classification method of [9] to relieve the bottleneck of RL and proposed sFT and wFT. Both methods were carefully designed for RL models, but these were not the only way to employ long-tail classification methods. In this section, we discuss the other possible adaptations based on [17].\n\n[17] explored ways to employ long-tail classification methods for machine translation. Their first method was \u03c4normalization (\u03c4 -norm), which directly adopted the method of [9]. Based on an observation that the norm of classifier parameters correlates with the frequency of the classes, they normalized the classifier weight W as follows:\nW wi = W wi \u2225W wi \u2225 \u03c4 ,(1)\nwhere W wi \u2208 R d indicates a vector at the index of a word w i and \u03c4 is a temperature hyperparameter that controls the degree of the normalization. The other methods of [17] were Focal loss (FL) and Anti-Focal loss (AFL). AFL is a variant of FL [11], which was aimed at reweighting the loss according to the confidence of the model predictions. Let p t \u03b8 = p \u03b8 (w g t | w g <t , I). FL and AFL in image captioning are then written as follows:\nL FL (\u03b8) = \u2212 1 T T t=1 (1 \u2212 p t \u03b8 ) \u03b3 log p t \u03b8 ,(2)L AFL (\u03b8) = \u2212 1 T T t=1 (1 + \u03b1p t \u03b8 ) \u03b3 log p t \u03b8 ,(3)\nwhere \u03b3 and \u03b1 are hyperparamters that control the degree of the reweighting. Other work also explored ways to employ long-tail classification methods for text generation, but those approaches are categorized as either \u03c4 -norm [14] or variants of FL [5,8,22], which we already explored above. We compared our methods (sFT and wFT) with \u03c4 -norm, FL, and AFL. In our experiments, we normalized the bias term b 1 in addition to the weight term W as we found it performed better than normalizing the weight term only. We applied FL and AFL as the alternative weighting to BP for a fair comparison with our methods. That is, we fine-tuned the classifier parameters by optimizing L FL (\u03b8) or L AFL (\u03b8), where\u03b8 were initialized with the pre-trained RL models. We used the best hyperparameters reported in [17]: \u03c4 = 0.2, \u03b3 = 1, and \u03b1 = 1. Similar to our models, other hyperparameters were set to the same values as the baseline models, except for the epoch size and learning rate. We explored the same values for these hyperparameters as our models: we set the epoch size for fine-tuning to 1 and searched for the best learning rates from {1e-3, 1e-4, 1e-5, 1e-6}. We selected the best learning rate according to the R@1 scores in the validation set. The best learning rate was 1e-4 for Att2in RL + FL/AFL, 1e-4 for UpDown RL + FL/AFL, and 1e-5 for Transformer RL + FL/AFL. Note that we did not explore the learning rate for \u03c4 -norm because it does not require training.\n\nIn open-ended text generation tasks, e.g., story generation and text generation after prompts, stochastic sampling methods are used instead of beam search to increase the diversity in output text [7,2,13]. Although image captioning does not fall in the category of open-ended text generation 1b = b \u2225b\u2225 \u03c4 , where the value of the hyperparameter \u03c4 was set to the same as that of W . as input images tightly scope the correctness of captions, we additionally test whether the randomness in stochastic sampling can increase the output vocabulary. We used Nucleus sampling [7] with a hyperparameter p = 0.95, which is the best hyperparameter reported [7,13]. Table 2 shows the results. \u03c4 -norm and Nucleus sampling showed the similar results. Both methods slightly increased the output vocabulary but the performance generally remained the same as the baseline models. These results indicate that the output vocabulary cannot be significantly increased while maintaining the relative probability of words: Nucleus sampling samples according to the original output distributions and \u03c4 -norm changes the distribution only by the difference in the norm, basically flattening the distribution. In contrast, FL and AFL drastically change the relative probability of words by refining the mapping from encoded features to low-frequency words, as with sFT and wFT. They successfully increased the vocabulary size and discriminativeness. However, the gains were smaller than those of wFT.\n\nTo analyze the cause of the difference between FL, AFL, and the BP loss (wFT), we visualized the losses in Figure 6. FL suppresses the loss when a model is confident, whereas AFL increases the loss when a model is moderately confident. Compared with these losses, BP changes the loss more drastically. When the frequency-biased policy p \u03b8 \u2032 is highly confident, BP strictly suppresses the loss to prevent further learning on that word; when p \u03b8 \u2032 is not confident, BP highly increases the loss to encourage the learning on that word. This drastic rebalancing of the loss resulted in wFT's Figure 6. Visualization of the losses: CE \u2212 log p \u03b8 (wi), BP \u2212 log p \u03b8,\u03b8 \u2032 (wi), FL (1 \u2212 p \u03b8 (wi)) \u03b3 log p \u03b8 (wi), and AFL (1 + \u03b1p \u03b8 (wi)) \u03b3 log p \u03b8 (wi). We set \u03b2 = \u03b2 \u2032 = 1, \u03b3 = 1, and \u03b1 = 1. larger vocabulary size and higher discriminativeness. Table 3 shows the performance of our models on the MS COCO validation set. We report these results for the future reproduction of our experiments. The code is available at https://github.com/ukyh/switch dis c caption.git.  \n\n\nValidation Performance for Reproduction\n\n\nVocabulary\n\n\nStandard Evaluation Discriminativeness\n\n\nEffectiveness on More Recent Models\n\nTo further demonstrate the effectiveness of our methods, we tested our fine-tuning methods on a more recent captioning model, VinVL [23,10]. VinVL boosts its performance through large-scale cross-modal pre-training. The significant performance improvements have made VinVL a popular captioning model and one of the most advanced captioning models available today [20,21,15].\n\nWe used the best-performing pre-trained model as our baseline: coco captioning large scst model that is publicly available at https://github.com/micro soft/Oscar/blob/master/VinVL MODEL ZOO .md#Image-Captioning-on-COCO. Note that this model was trained with the standard RL [18].\n\nAs in the previous experiments, we applied our finetuning methods for one epoch only. We searched for the best learning rates for fine-tuning from {1e-5, 1e-6}, and the inverse-temperature hyperparameter \u03b2 \u2032 of Eq. (7) from {0.01, 0.1, 1}. Other hyperparameters were set to the same as the baseline model. The best learning rate was 1e-5. The best \u03b2 \u2032 was 0.01 for wFT with p \u03b8 decoding and 1 for wFT with BP decoding. Note that sFT does not use \u03b2 \u2032 . Table 4 shows similar results as Table 1 in the main paper. Our methods significantly increased the vocabulary size from the baseline and accordingly enhanced the discriminativeness. The standard evaluation metrics also showed the same tendency. Although our models scored lower than the baseline in the conventional exact-matching metrics (CIDEr and SPICE), the gap be-came smaller in the more advanced soft-matching metrics (BERTS+ and TIGEr). In the state-of-the-art soft-matching metrics (CLIPS and RefCLIPS), our models achieved the same or even higher scores than the baseline. These results show that our methods are also effective on the more recent model. Moreover, these results further validate that our methods can switch any off-the-shelf RL models to discriminativeness-aware models while maintaining the overall quality of captions.\n\n\nComparison and Combination with More Recent Discriminativeness-Aware Models\n\nContemporaneous to our work, [3] showed that maximizing reference-free CLIPS-based reward enhanced discriminativeness significantly. In this section, we clarify the advantages of our methods over the CLIPS-based RL by comparing and combining our methods with it.\n\nThe pre-trained models of [3] are publicly available at https://github.com/j-min/CLIP-Captio n-Reward. We used the transformer model trained with the standard CIDEr reward (Transformer* RL (CIDEr); clipRN50 cider) and the one trained with the reward proposed by [3] (Transformer* RL (CLIPS + Grammar); clipRN50 clips grammar) 2 . The proposed reward is computed by the weighted sum of CLIPS and grammaticality scores. We also included Transformer* Only CE 2 Note that clipRN50 does not mean that the model used the CLIPSbased reward. It denotes that the model used CLIP [16] as the image encoder, unlike the other models tested in this paper.  Table 5. Test on the more recent discriminativeness-aware model. Transformer* used a different image encoder than the other transformer models tested in this paper. Automatic evaluation results on the MS COCO test set. Unique-1 and Unique-S indicate the number of unique unigrams and sentences, respectively. Length is the average length of output captions.\n\n(clipRN50 mle) in the comparison as the baseline without RL. As in the previous experiments, we applied our finetuning methods for one epoch only. We searched for the best learning rates for fine-tuning from {1e-5, 1e-6, 1e-7}, and the inverse-temperature hyperparameter \u03b2 \u2032 of Eq. (7) from {0.01, 0.1, 1}. Other hyperparameters were set to the same as the baseline model. The best learning rate for Transformer* RL (CIDEr) was 1e-5; the best \u03b2 \u2032 was 0.1 for wFT with p \u03b8 decoding and 1 for wFT with BP decoding. The best learning rates for Transformer* RL (CLIPS + Grammar) were 1e-6 for wFT with BP decoding and 1e-7 for the others; the best \u03b2 \u2032 was 1 for wFT with both decoding methods. Note that sFT does not use \u03b2 \u2032 . Table 5 shows the results. Similar to the previous results, our methods significantly enhanced the vocabulary size and discriminativeness from the RL models while maintaining or even increasing the scores in the state-of-the-art softmatching metrics. The CLIPS + Grammar reward also achieved the high discriminativeness compared with the standard CIDEr reward.\n\nHowever, the improvement of the CLIPS-based RL came at the expense of the conciseness and overall quality of captions in contrast to our methods: compared to Transformer* RL (CIDEr), Transformer* RL (CLIPS + Grammar) significantly increased the sentence length and decreased scores in the standard evaluation metrics, including the current bestperforming metric, RefCLIPS. Although increasing the sentence length is one way to describe images in detail, concise description is more desirable to convey the most characteristic information clearly and efficiently [19]. Despite the longer sentence length, the side effect was still observed: CLIPS-based RL decreased the output vocabulary from the Only CE baseline.\n\nThese results indicate that our methods and the CLIPSbased RL increased discriminativeness by different factors: more specific vocabulary and longer descriptions, respectively. In other words, the contribution of our methods is orthogonal to that of the CLIPS-based RL. To utilize the strength of each, we applied our methods to the CLIPS-based RL model. Although the CLIPS-based RL achieved the high discriminativeness and relatively large vocabulary size due to the longer sentences, our methods further enhanced the discriminativeness and vocabulary size. Surprisingly, our methods also improved the standard evaluation scores, including exact-matching scores. This result suggests that our fine-tuning with ground-truth captions restored the overall quality of captions, which was degraded by over-optimization for reference-free CLIPS.\n\nAnother critical advantage of our methods is computational efficiency. Training of CLIPS-based RL took one day using eight GPUs [3], while ours only took 40 minutes using a single GPU.\n\nThe above results conclude that our methods are orthogonal to the more recent discriminative image captioning method and have important advantages in conciseness and efficiency.\n\n\nTransformer RL: a group of boats sitting in the water +wFT: a body of water with boats on it Transformer RL: a group of boats sitting in the water +wFT: a black and white photo of boats docked at a pier Transformer RL: a group of boats sitting in the water +wFT: many small boats tied together at night Transformer RL: a group of boats sitting in the water +wFT: a row of small boats tied to a dock\n\nFigure 1 .\n1Caption examples in the MS COCO validation set.Transformer RL is a Transformer captioning model trained with RL and wFT is our fine-tuning method. Transformer RL generates exactly the same caption for the four images. The underlined words indicate the characteristic information that are not mentioned by Transformer RL, and the blue words are those that have never appeared in the outputs of the model. See Appendix 2 for more examples.\n\nFigure 2 .\n2Relative frequency of the words in the sequences sampled for the MS COCO training images. Five sequences were sampled for each image. The words (9,486 unique words excluding an out-of-vocabulary token \u27e8unk\u27e9) are sorted by their frequency in ground-truth captions and divided into 200 bins. We show the first 10 bins and the sum of the rest. GT is the ground-truth caption of the training images, CE is the output of a captioning model trained with the CE loss, and RL is the output of a captioning model trained with RL. Here, we used the Transformer model.\n\nFigure 1 .\n1Examples of the limitation of our methods. All the examples are from the MS COCO validation set. The underlined words are relatively low-frequency hypernyms.\n\nFigure 2\n2shows caption examples in the MS COCO validation set. The blue words are those that have never appeared in the output captions of the baseline model. We observe that these blue words express various types of characteristic information of the images. Here, weather vane and flamingos are characteristic objects of the images (a) and (b); shallow, funny, and staring straight ahead are characteristic attributes of the images (b) and (c); and racing and sniffing are characteristic relations in the images (d) and (e).\n\nFigure 2 .\n2Caption examples in the MS COCO validation set. The blue words are those that have never appeared in the output captions of the baseline model (Transformer RL). Human shows a ground-truth caption of each image.uation TIGEr https://github.com/SeleenaJM/CapEv al R@K https://github.com/fartashf/vsep p; following[12], we used a publicly available model, coco vse++ resnet restval finetune.\n\nFigure 3 .\n3Relative frequency of the words in the sequences sampled for the training images. Five sequences were sampled for each image. The words (9,486 unique words excluding an out-ofvocabulary token \u27e8unk\u27e9) are sorted by their frequency in groundtruth captions and divided into 200 bins. We show the first 10 bins and the sum of the rest. GT is the ground-truth caption of the training images, CE is the output of a captioning model trained with the CE loss, and RL is the output of a captioning model trained with RL.\n\nFigure 4 .\n4Underrated captions in the MS COCO validation set. The blue words are those that have never appeared in the output captions of the baseline model (Transformer RL). Reference Coverage shows the number of reference captions (out of five) that cover at least one of the blue words.\n\nFigure 5 .\n5A screenshot of our AMT interface.\n\n\ncould correctly retrieve the original images from the entire validation/test images within the rank of K \u2208 {1, 5, 10}. A higher R@K indicatesVocabulary \n\nStandard Evaluation \nDiscriminativeness \n\nUnique-1 Unique-S Length CIDEr SPICE BERTS+ TIGEr CLIPS RefCLIPS R@1 R@5 R@10 \n\nAtt2in \n\nAtt2in RL \n445 \n2,524 \n9.3 \n117.4 \n20.5 \n43.6 \n73.9 \n73.0 \n79.7 \n16.3 \n41.9 \n57.2 \n+ sFT \n880 \n3,156 \n9.0 \n115.4 \n20.4 \n43.9 \n74.3 \n73.7 \n80.3 \n20.1 \n48.0 \n62.8 \n+ wFT \n1,197 \n3,732 \n8.9 \n104.3 \n19.5 \n43.1 \n74.2 \n73.9 \n80.2 \n20.6 \n49.7 \n64.5 \n+ wFT (BP decoding) \n1,102 \n3,615 \n9.4 \n109.3 \n20.1 \n43.7 \n74.4 \n74.0 \n80.2 \n21.1 \n50.5 \n64.8 \nCIDErBtw \n470 \n2,630 \n9.3 \n119.0 \n20.7 \n43.8 \n74.1 \n73.1 \n79.8 \n17.2 \n44.1 \n58.7 \nNLI \n465 \n2,626 \n9.2 \n118.9 \n20.6 \n43.8 \n74.1 \n73.2 \n79.9 \n17.6 \n44.4 \n59.8 \nDiscCap  \u2020 \n3,093 \n9.3 \n114.2 \n21.0 \n21.6 \n50.3 \n65.4 \nJoint CE \n700 \n2,907 \n9.1 \n111.7 \n19.9 \n43.5 \n74.0 \n73.3 \n80.0 \n19.1 \n46.7 \n61.5 \nOnly CE \n689 \n2,845 \n9.2 \n110.7 \n20.1 \n43.5 \n74.0 \n73.3 \n79.9 \n19.0 \n46.6 \n61.1 \nVisual Paraphrase  \u2020 \n4,576 \n12.9 \n86.9 \n21.1 \n26.3 \n57.2 \n70.8 \n\nUpDown \n\nUpDown RL \n577 \n3,103 \n9.5 \n122.7 \n21.5 \n44.2 \n74.6 \n74.0 \n80.5 \n21.1 \n49.9 \n64.6 \n+ sFT \n1,190 \n3,788 \n9.2 \n115.9 \n21.0 \n44.2 \n74.9 \n74.8 \n80.9 \n25.0 \n56.8 \n71.2 \n+ wFT \n1,479 \n4,268 \n9.1 \n101.8 \n19.5 \n43.1 \n74.6 \n74.9 \n80.7 \n26.0 \n57.6 \n72.2 \n+ wFT (BP decoding) \n1,275 \n4,177 \n9.6 \n110.0 \n20.6 \n44.1 \n74.9 \n75.0 \n80.8 \n26.7 \n58.7 \n72.4 \nCIDErBtw \n582 \n3,108 \n9.4 \n123.0 \n21.5 \n44.4 \n74.6 \n74.2 \n80.7 \n21.9 \n50.9 \n65.9 \nNLI \n575 \n3,144 \n9.4 \n122.4 \n21.4 \n44.4 \n74.6 \n74.1 \n80.6 \n21.5 \n50.7 \n65.6 \nJoint CE \n857 \n3,120 \n9.4 \n111.8 \n20.5 \n43.7 \n74.3 \n73.8 \n80.2 \n21.8 \n51.2 \n65.2 \nOnly CE \n878 \n3,126 \n9.4 \n109.2 \n20.1 \n43.4 \n74.2 \n73.6 \n80.0 \n21.8 \n49.9 \n64.5 \n\nTransformer \n\nTransformer RL \n753 \n3,433 \n9.2 \n127.7 \n22.5 \n45.1 \n75.0 \n75.0 \n81.3 \n26.6 \n56.2 \n70.5 \n+ sFT \n1,458 \n3,959 \n9.1 \n118.7 \n21.7 \n44.8 \n75.2 \n75.6 \n81.5 \n30.6 \n62.3 \n75.7 \n+ wFT \n1,776 \n4,274 \n9.1 \n103.1 \n20.0 \n43.3 \n74.8 \n75.8 \n81.2 \n32.5 \n64.5 \n77.1 \n+ wFT (BP decoding) \n1,964 \n4,373 \n9.4 \n107.3 \n21.1 \n44.2 \n75.2 \n76.1 \n81.5 \n33.5 \n65.9 \n78.2 \nCIDErBtw \n837 \n3,609 \n9.5 \n128.2 \n22.6 \n45.1 \n75.2 \n75.0 \n81.2 \n27.7 \n57.6 \n71.6 \nNLI \n876 \n3,744 \n9.5 \n129.1 \n23.0 \n45.4 \n75.3 \n75.5 \n81.5 \n29.8 \n59.9 \n73.4 \nJoint CE \n1,083 \n3,491 \n9.3 \n123.8 \n21.9 \n45.0 \n74.8 \n75.0 \n81.2 \n27.3 \n57.2 \n70.8 \nOnly CE \n935 \n3,599 \n9.4 \n112.2 \n20.8 \n44.0 \n74.5 \n74.8 \n80.9 \n26.5 \n55.8 \n69.7 \n\n\n\n\nText-BasedText-and-Image-Based Rep (%) \u2193 Number \u2193 Rank \u2191 CIDEr SPICE BERTS+ TIGEr CLIPS RefCLIPSTable 2. Comparison of OOR words and the resulting difference in exact-matching and soft-matching metrics. We report the results on the MS COCO test set. A higher value in Rank indicates a lower frequency rank of the OOR words. We also report the rate of repetition.Repetition \nOOR \nExact-Matching \nSoft-Matching \n\nAtt2in RL \n4.1 \n8,665 \n79.4 \n117.4 \n20.5 \n43.6 \n73.9 \n73.0 \n79.7 \n+ sFT \n3.8 \n8,813 \n164.0 \n115.4 \n20.4 \n43.9 \n74.3 \n73.7 \n80.3 \n+ wFT \n3.2 \n10.454 \n237.9 \n104.3 \n19.5 \n43.1 \n74.2 \n73.9 \n80.2 \n+ wFT (BP decoding) \n3.6 \n10,386 \n204.7 \n109.3 \n20.1 \n43.7 \n74.4 \n74.0 \n80.2 \nOnly CE \n3.9 \n9,913 \n133.1 \n110.7 \n20.1 \n43.5 \n74.0 \n73.3 \n79.9 \n\nUpDown RL \n3.9 \n8,463 \n100.1 \n122.7 \n21.5 \n44.2 \n74.6 \n74.0 \n80.5 \n+ sFT \n3.6 \n9,252 \n225.8 \n115.9 \n21.0 \n44.2 \n74.9 \n74.8 \n80.9 \n+ wFT \n3.0 \n11,478 \n301.0 \n101.8 \n19.5 \n43.1 \n74.6 \n74.9 \n80.7 \n+ wFT (BP decoding) \n3.4 \n11,065 \n236.9 \n110.0 \n20.6 \n44.1 \n74.9 \n75.0 \n80.8 \nOnly CE \n3.7 \n10,874 \n152.9 \n109.2 \n20.1 \n43.4 \n74.2 \n73.6 \n80.0 \n\nTransformer RL \n3.6 \n7,824 \n129.8 \n127.7 \n22.5 \n45.1 \n75.0 \n75.0 \n81.3 \n+ sFT \n3.2 \n9,397 \n296.0 \n118.7 \n21.7 \n44.8 \n75.2 \n75.6 \n81.5 \n+ wFT \n2.6 \n11,930 \n379.7 \n103.1 \n20.0 \n43.3 \n74.8 \n75.8 \n81.2 \n+ wFT (BP decoding) \n2.9 \n11,673 \n461.0 \n107.3 \n21.1 \n44.2 \n75.2 \n76.1 \n81.5 \nOnly CE \n3.3 \n10,661 \n165.6 \n112.2 \n20.8 \n44.0 \n74.5 \n74.8 \n80.9 \n\nHuman \n2.4 \n17,963 \n815.6 \n88.4 \n21.2 \n42.9 \n73.3 \n77.7 \n82.0 \n\nDiscriminativeness Correctness Fluency \n\nTransformer RL 3.00 \n4.42 \n4.83 \n+ wFT \n3.34  *  *  \n4.45 \n4.84 \nNLI \n3.18  *  *  \n4.54 \n4.76 \n\n\n\n\nSwitching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning -Supplementary MaterialUkyo Honda 1,2 \nTaro Watanabe 3 \nYuji Matsumoto 2 \n1 CyberAgent, Inc. \n2 RIKEN \n3 Nara Institute of Science and Technology \n\nhonda ukyo@cyberagent.co.jp \ntaro@is.naist.jp \nyuji.matsumoto@riken.jp \n\nTransformer RL: \na man sitting at a desk with a computer \n\n+wFT: \na person sitting at a desk with multiple \ncomputers \n\nTransformer RL: \na sheep laying on the grass in a field \n\n+wFT: \nan animal that is laying down on some \ngrass \n\nTransformer RL: \na living room with a couch and a table \n\n+wFT: \na living room filled with white furniture \nand red walls \n\n\n\n\nTransformer RL: a tower with a clock on top of it +wFT: a clock tower with a weather vane on top NLI: a tower with a clock on the top of it Human: a weather vane atop a cathedral clock tower Transformer RL: a black cat wearing a hat on top of a table +wFT: a cat wears a funny hat while staring straight ahead NLI: a black cat wearing a hat sitting on a table Human: the cute black cat is wearing a bee's hat Transformer RL: a group of people riding motorcycles on a road +wFT: a group of people racing motorcycles on a race track NLI: a group of people riding motorcycles on a race track Human: people are racing motorcycles on a race track Transformer RL: a dog next to a cup of coffee +wFT: a dog is sniffing a cup of coffee NLI: a dog standing next to a coffee cup on a table Human: a squinting dog on a brick patio sniffs a cup of coffee\n\n\nEpoch Batch Hour/Epoch Total HourAtt2in RL \n20 \n10 \n0.68 \n13.54 \n+ sFT \n1 \n10 \n0.08 \n0.08 \n+ wFT \n1 \n10 \n0.12 \n0.12 \nCIDErBtw \n50 \n10 \n0.70 \n35.11 \nNLI \n50 \n16 \n0.87 \n43.55 \nJoint CE \n20 \n10 \n1.15 \n22.97 \n\nUpDown RL \n20 \n10 \n0.71 \n14.16 \n+ sFT \n1 \n10 \n0.09 \n0.09 \n+ wFT \n1 \n10 \n0.14 \n0.14 \nCIDErBtw \n50 \n10 \n0.76 \n38.09 \nNLI \n50 \n16 \n0.87 \n43.74 \nJoint CE \n20 \n10 \n1.08 \n21.67 \n\nTransformer RL \n25 \n10 \n3.23 \n80.66 \n+ sFT \n1 \n10 \n0.11 \n0.11 \n+ wFT \n1 \n10 \n0.18 \n0.18 \nCIDErBtw \n25 \n10 \n3.27 \n81.76 \nNLI \n25 \n16 \n2.74 \n68.54 \nJoint CE \n25 \n10 \n4.06 \n101.43 \n\n\n\n\nUnique-1 Unique-S Length CIDEr SPICE BERTS+ TIGEr CLIPS RefCLIPS R@1 R@5 R@10Table 3. Automatic evaluation results on the MS COCO validation set. Unique-1 and Unique-S indicate the number of unique unigrams and sentences, respectively. Length is the average length of output captions. TIGEr scores are N/A as the TIGEr evaluation tool currently does not support evaluation on the MS COCO validation set.Unique-1 Unique-S Length CIDEr SPICE BERTS+ TIGEr CLIPS RefCLIPS R@1 R@5 R@10Att2in RL \n435 \n2,583 \n9.3 \n116.5 \n20.3 \n43.6 \nN/A \n73.1 \n79.8 \n16.2 \n42.5 \n57.0 \n+ sFT \n874 \n3,189 \n9.0 \n113.7 \n20.1 \n43.7 \nN/A \n73.9 \n80.3 \n19.2 \n47.9 \n62.9 \n+ wFT \n1,196 \n3,792 \n9.0 \n104.8 \n19.3 \n43.2 \nN/A \n74.2 \n80.3 \n19.6 \n50.4 \n64.6 \n+ wFT (BP decoding) \n1,105 \n3,633 \n9.4 \n108.6 \n20.0 \n43.7 \nN/A \n74.1 \n80.3 \n20.6 \n50.6 \n64.9 \n\nUpDown RL \n563 \n3,161 \n9.5 \n122.3 \n21.3 \n44.2 \nN/A \n74.2 \n80.6 \n20.6 \n50.2 \n65.7 \n+ sFT \n1,222 \n3,805 \n9.2 \n115.3 \n20.7 \n44.1 \nN/A \n74.9 \n80.9 \n24.6 \n56.2 \n70.9 \n+ wFT \n1,502 \n4,301 \n9.1 \n100.5 \n19.2 \n43.0 \nN/A \n75.0 \n80.7 \n26.1 \n57.4 \n71.4 \n+ wFT (BP decoding) \n1,278 \n4,226 \n9.6 \n108.9 \n20.5 \n43.9 \nN/A \n75.1 \n80.9 \n26.4 \n58.6 \n73.6 \n\nTransformer RL \n713 \n3,432 \n9.2 \n126.4 \n22.1 \n45.0 \nN/A \n75.0 \n81.2 \n25.4 \n56.3 \n69.8 \n+ sFT \n1,496 \n3,953 \n9.1 \n118.4 \n21.4 \n44.6 \nN/A \n75.7 \n81.5 \n30.2 \n62.7 \n75.8 \n+ wFT \n1,836 \n4,268 \n9.1 \n102.2 \n19.8 \n43.2 \nN/A \n75.9 \n81.3 \n32.2 \n64.3 \n76.8 \n+ wFT (BP decoding) \n2,004 \n4,392 \n9.4 \n105.6 \n20.6 \n43.9 \nN/A \n76.1 \n81.4 \n32.8 \n66.1 \n79.0 \n\nVocabulary \nStandard Evaluation \nDiscriminativeness \n\nVinVL RL \n1,126 \n4,298 \n10.0 \n140.9 \n25.2 \n46.1 \n75.7 \n77.6 \n83.3 \n36.1 \n68.5 \n80.2 \n+ sFT \n1,834 \n4,649 \n10.0 \n126.0 \n23.8 \n45.5 \n75.6 \n78.2 \n83.3 \n39.2 \n72.1 \n83.8 \n+ wFT \n1,852 \n4,652 \n10.0 \n124.9 \n23.7 \n45.5 \n75.6 \n78.2 \n83.3 \n39.2 \n72.0 \n83.9 \n+ wFT (BP decoding) \n1,734 \n4,717 \n9.8 \n122.4 \n23.5 \n45.2 \n75.7 \n78.2 \n83.3 \n39.6 \n72.1 \n84.6 \n\n\n\nTable 4 .\n4Test on the more recent captioning model. Automatic evaluation results on the MS COCO test set. Unique-1 and Unique-S indicate the number of unique unigrams and sentences, respectively. Length is the average length of output captions.\n\n\nUnique-1 Unique-S Length CIDEr SPICE BERTS+ TIGEr CLIPS RefCLIPS R@1 R@5 R@10Vocabulary \n\nStandard Evaluation \nDiscriminativeness \n\nTransformer* RL (CIDEr) \n691 \n3,650 \n9.5 \n126.0 \n22.8 \n45.2 \n74.6 \n75.8 \n81.6 \n27.1 \n57.2 \n70.6 \n+ sFT \n1,265 \n4,071 \n9.1 \n122.9 \n22.2 \n45.2 \n74.8 \n76.4 \n82.0 \n31.4 \n62.0 \n75.0 \n+ wFT \n1,546 \n4,337 \n9.0 \n111.3 \n21.0 \n44.2 \n74.5 \n76.5 \n81.8 \n31.6 \n63.3 \n75.7 \n+ wFT (BP decoding) \n1,543 \n4,471 \n9.5 \n112.3 \n21.7 \n44.9 \n74.8 \n76.9 \n81.9 \n34.0 \n65.4 \n78.4 \n\nTransformer* RL (CLIPS + Grammar) \n952 \n4,847 \n13.0 \n74.1 \n19.8 \n43.6 \n75.0 \n79.2 \n81.2 \n44.2 \n77.0 \n86.9 \n+ sFT \n969 \n4,848 \n12.8 \n76.4 \n20.1 \n43.8 \n75.0 \n79.2 \n81.2 \n44.6 \n77.3 \n87.0 \n+ wFT \n969 \n4,847 \n12.9 \n76.4 \n20.1 \n43.8 \n75.0 \n79.2 \n81.2 \n44.8 \n77.2 \n87.1 \n+ wFT (BP decoding) \n1,001 \n4,853 \n12.2 \n82.5 \n20.6 \n44.1 \n75.0 \n79.2 \n81.3 \n45.5 \n77.2 \n87.1 \n\nTransformer* Only CE \n1,174 \n3,637 \n9.4 \n113.8 \n20.9 \n44.1 \n74.0 \n75.1 \n81.1 \n26.2 \n55.2 \n68.6 \n\n\nThe code is available at https://github.com/ukyh/switch disc caption.git\nAlthoughFigure 2shows only the results obtained with the Transformer captioning model, we also confirmed that other models output peaky distributions[50, 2]. See Appendix 3 for the details.\n [5]  also utilized fixed pre-trained models to reweight their loss for stylized image captioning. However, their method is designed to train new models from scratch and is not applicable to refining pre-trained models; their loss function (Eq. (6) in[5]) is stuck at zero when we initialize the parameters with the same pre-trained model. This requirement for retraining from scratch is a fundamental deviation from our goal of improving the discriminativeness of off-the-shelf RL models.\nEach split of training/validation/test contained 113,287/5,000/5,000 images, and each image had around five ground-truth captions.5  The words that occur less than five times in the training captions were converted to \u27e8unk\u27e9 token.\nSome metrics use stemming, lemmatization, and/or WordNet synsets to evaluate synonyms but their coverage is limited.\nNote that this difference does not mean that exact-matching metrics represent precision, and soft-matching metrics represent recall. Exactmatching metrics cannot represent precision because the reference captions do not cover all correct descriptions. That is, exact-matching metrics can only represent the flawed precision with false negatives. Actually, exact-matching metrics correlate with human judgments worse than softmatching metrics not only in recall but also in precision[29].\nIf a target caption describes the same information as a baseline caption, the workers give the target caption a score of 3; if the target caption describes more (less) characteristic information than the baseline caption, the workers give the target caption a score of 4 or 5 (1 or 2).\n\nSpice: Semantic propositional image caption evaluation. Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, ECCV. Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image cap- tion evaluation. In ECCV, 2016.\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, CVPR. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.\n\nReasoning about pragmatics with neural listeners and speakers. Jacob Andreas, Dan Klein, EMNLP. Jacob Andreas and Dan Klein. Reasoning about pragmatics with neural listeners and speakers. In EMNLP, 2016.\n\nScheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, In NeurIPS. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NeurIPS, 2015.\n\nfactual\"or\"emotional\": Stylized image captioning with adaptive learning and attention. Tianlang Chen, Zhongping Zhang, Quanzeng You, Chen Fang, Zhaowen Wang, Hailin Jin, Jiebo Luo, ECCV. Tianlang Chen, Zhongping Zhang, Quanzeng You, Chen Fang, Zhaowen Wang, Hailin Jin, and Jiebo Luo. \"fac- tual\"or\"emotional\": Stylized image captioning with adaptive learning and attention. In ECCV, 2018.\n\nMicrosoft coco captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, arXiv:1504.00325arXiv preprintXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan- tam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n\nFine-grained image captioning with CLIP reward. Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Dernoncourt, Trung Bui, Mohit Bansal, Findings of the Association for Computational Linguistics: NAACL 2022. Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Der- noncourt, Trung Bui, and Mohit Bansal. Fine-grained image captioning with CLIP reward. In Findings of the Association for Computational Linguistics: NAACL 2022, 2022.\n\nOn the weaknesses of reinforcement learning for neural machine translation. Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend, ICLR. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. On the weaknesses of reinforcement learning for neural machine translation. In ICLR, 2020.\n\nDon't take the easy way out: Ensemble based methods for avoiding known dataset biases. Christopher Clark, Mark Yatskar, Luke Zettlemoyer, EMNLP-IJCNLP. Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don't take the easy way out: Ensemble based methods for avoiding known dataset biases. In EMNLP-IJCNLP, 2019.\n\nPragmatically informative image captioning with character-level inference. Reuben Cohn-Gordon, Noah Goodman, Christopher Potts, NAACL-HLT. Reuben Cohn-Gordon, Noah Goodman, and Christopher Potts. Pragmatically informative image captioning with character-level inference. In NAACL-HLT, 2018.\n\nTowards diverse and natural image descriptions via a conditional gan. Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin, Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. To- wards diverse and natural image descriptions via a condi- tional gan. In ICCV, 2017.\n\nContrastive learning for image captioning. Bo Dai, Dahua Lin, NeurIPS. Bo Dai and Dahua Lin. Contrastive learning for image cap- tioning. In NeurIPS, 2017.\n\nStolen probability: A structural weakness of neural language models. David Demeter, Gregory Kimmel, Doug Downey, ACL. David Demeter, Gregory Kimmel, and Doug Downey. Stolen probability: A structural weakness of neural language mod- els. In ACL, 2020.\n\nClassical structured prediction losses for sequence to sequence learning. Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc&apos;aurelio Ranzato, NAACL-HLT. Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc'Aurelio Ranzato. Classical structured prediction losses for sequence to sequence learning. In NAACL-HLT, 2018.\n\nVse++: Improving visual-semantic embeddings with hard negatives. Fartash Faghri, J David, Jamie Ryan Fleet, Sanja Kiros, Fidler, BMVC. Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. In BMVC, 2018.\n\nCapwap: Captioning with a purpose. Adam Fisch, Kenton Lee, Ming-Wei Chang, Jonathan H Clark, Regina Barzilay, EMNLP. Adam Fisch, Kenton Lee, Ming-Wei Chang, Jonathan H Clark, and Regina Barzilay. Capwap: Captioning with a pur- pose. In EMNLP, 2020.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NeurIPS. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\n\nToken-level adaptive training for neural machine translation. Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie Zhou, Dong Yu, EMNLP. Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie Zhou, and Dong Yu. Token-level adaptive training for neural machine translation. In EMNLP, 2020.\n\nMeng Zhang, and Nilavra Bhattacharya. Captioning images taken by people who are blind. Danna Gurari, Yinan Zhao, ECCV. Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhat- tacharya. Captioning images taken by people who are blind. In ECCV, 2020.\n\nUnlearn dataset bias in natural language inference by fitting the residual. He He, Sheng Zha, Haohan Wang, EMNLP-IJCNLP. He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. In EMNLP-IJCNLP, 2019.\n\nClipscore: A reference-free evaluation metric for image captioning. Jack Hessel, Ari Holtzman, Maxwell Forbes, Yejin Ronan Le Bras, Choi, EMNLP. 2021Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation met- ric for image captioning. In EMNLP, 2021.\n\nTraining products of experts by minimizing contrastive divergence. E Geoffrey, Hinton, Neural computation. 148Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771-1800, 2002.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\nThe curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, ICLR. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020.\n\nTiger: Text-to-image grounding for image caption evaluation. Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe Gan, Jana Diesner, Jianfeng Gao, EMNLP-IJCNLP. Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe Gan, Jana Diesner, and Jianfeng Gao. Tiger: Text-to-image grounding for image caption evalua- tion. In EMNLP-IJCNLP, 2019.\n\nImproving neural response diversity with frequencyaware cross-entropy loss. Shaojie Jiang, Pengjie Ren, Christof Monz, Maarten De Rijke, WWWShaojie Jiang, Pengjie Ren, Christof Monz, and Maarten de Rijke. Improving neural response diversity with frequency- aware cross-entropy loss. In WWW, 2019.\n\nDecoupling representation and classifier for long-tailed recognition. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis, ICLR. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decou- pling representation and classifier for long-tailed recogni- tion. In ICLR, 2020.\n\nDeep visual-semantic alignments for generating image descriptions. Andrej Karpathy, Li Fei-Fei, CVPR. Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align- ments for generating image descriptions. In CVPR, 2015.\n\nTransparent human evaluation for image captioning. Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Yejin Ronan Le Bras, Noah A Choi, Smith, NAACL-HLT. Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Ronan Le Bras, Yejin Choi, and Noah A. Smith. Transparent human evaluation for image captioning. In NAACL-HLT, 2022.\n\nRevisiting the weaknesses of reinforcement learning for neural machine translation. Samuel Kiegeland, Julia Kreutzer, NAACL-HLT. Samuel Kiegeland and Julia Kreutzer. Revisiting the weak- nesses of reinforcement learning for neural machine transla- tion. In NAACL-HLT, 2021.\n\nDensecaption matching and frame-selection gating for temporal localization in videoqa. Hyounghun Kim, Zineng Tang, Mohit Bansal, ACL. Hyounghun Kim, Zineng Tang, and Mohit Bansal. Dense- caption matching and frame-selection gating for temporal lo- calization in videoqa. In ACL, 2020.\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, Li Fei-Fei, IJCV. 1231Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vi- sion using crowdsourced dense image annotations. IJCV, 123(1):32-73, 2017.\n\nViL-BERTScore: Evaluating image caption using vision-andlanguage BERT. Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Soon Doo, Trung Kim, Kyomin Bui, Jung, The First Workshop on Evaluation and Comparison of NLP Systems. Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, and Kyomin Jung. ViL- BERTScore: Evaluating image caption using vision-and- language BERT. In The First Workshop on Evaluation and Comparison of NLP Systems, 2020.\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, EMNLP. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In EMNLP, 2016.\n\nDice loss for data-imbalanced nlp tasks. Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li, ACL. Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. Dice loss for data-imbalanced nlp tasks. In ACL, 2020.\n\nObject-semantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, ECCV. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, 2020.\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, ICCV. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n\nGenerating diverse and descriptive image captions using visual paraphrases. Lixin Liu, Jiajun Tang, Xiaojun Wan, Zongming Guo, ICCV. Lixin Liu, Jiajun Tang, Xiaojun Wan, and Zongming Guo. Generating diverse and descriptive image captions using vi- sual paraphrases. In ICCV, 2019.\n\nShow, tell and discriminate: Image captioning by self-retrieval with partially labeled data. Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, Xiaogang Wang, ECCV. Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, and Xi- aogang Wang. Show, tell and discriminate: Image captioning by self-retrieval with partially labeled data. In ECCV, 2018.\n\nDiscriminability objective for training descriptive captions. Ruotian Luo, Brian Price, Scott Cohen, Gregory Shakhnarovich, CVPR. Ruotian Luo, Brian Price, Scott Cohen, and Gregory Shakhnarovich. Discriminability objective for training de- scriptive captions. In CVPR, 2018.\n\nAnalysis of diversity-accuracy tradeoff in image captioning. Ruotian Luo, Gregory Shakhnarovich, arXiv:2002.11848arXiv preprintRuotian Luo and Gregory Shakhnarovich. Analysis of diversity-accuracy tradeoff in image captioning. arXiv preprint arXiv:2002.11848, 2020.\n\nRethinking the reference-based distinctive image captioning. Yangjun Mao, Long Chen, Zhihong Jiang, Dong Zhang, Zhimeng Zhang, Jian Shao, Jun Xiao, ACM MM. Yangjun Mao, Long Chen, Zhihong Jiang, Dong Zhang, Zhimeng Zhang, Jian Shao, and Jun Xiao. Rethinking the reference-based distinctive image captioning. In ACM MM, 2022.\n\nLong-tail learning via logit adjustment. Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, Sanjiv Kumar, ICLR. Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. In ICLR, 2020.\n\nColors in context: A pragmatic neural model for grounded language understanding. Will Monroe, X D Robert, Hawkins, D Noah, Christopher Goodman, Potts, TACL. 5Will Monroe, Robert XD Hawkins, Noah D Goodman, and Christopher Potts. Colors in context: A pragmatic neural model for grounded language understanding. TACL, 5:325- 338, 2017.\n\nImproving lexical choice in neural machine translation. Q Toan, David Nguyen, Chiang, NAACL-HLT. Toan Q Nguyen and David Chiang. Improving lexical choice in neural machine translation. In NAACL-HLT, 2018.\n\nMulti-reward reinforced summarization with saliency and entailment. Ramakanth Pasunuru, Mohit Bansal, NAACL-HLT. ACL. Ramakanth Pasunuru and Mohit Bansal. Multi-reward re- inforced summarization with saliency and entailment. In NAACL-HLT. ACL, 2018.\n\nSequence level training with recurrent neural networks. Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, In ICLR. Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2015.\n\nOn long-tailed phenomena in neural machine translation. Vikas Raunak, Siddharth Dalmia, Vivek Gupta, Florian Metze, Findings of the Association for Computational Linguistics: EMNLP 2020. Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena in neural machine transla- tion. In Findings of the Association for Computational Lin- guistics: EMNLP 2020, 2020.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, CVPR. Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, 2017.\n\nImage description with a goal: Building efficient discriminating expressions for images. Amir Sadovnik, Yi-I Chiu, Noah Snavely, Shimon Edelman, Tsuhan Chen, CVPR. Amir Sadovnik, Yi-I Chiu, Noah Snavely, Shimon Edelman, and Tsuhan Chen. Image description with a goal: Building efficient discriminating expressions for images. In CVPR, 2012.\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, ACL. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, im- age alt-text dataset for automatic image captioning. In ACL, 2018.\n\nPartial off-policy learning: Balance accuracy and diversity for human-oriented image captioning. Jiahe Shi, Yali Li, Shengjin Wang, ICCV. 2021Jiahe Shi, Yali Li, and Shengjin Wang. Partial off-policy learning: Balance accuracy and diversity for human-oriented image captioning. In ICCV, 2021.\n\nEnhancing descriptive image captioning with natural language inference. Zhan Shi, Hui Liu, Xiaodan Zhu, ACL. 2021Zhan Shi, Hui Liu, and Xiaodan Zhu. Enhancing descriptive image captioning with natural language inference. In ACL, 2021.\n\nFrom show to tell: A survey on image captioning. Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, Rita Cucchiara, arXiv:2107.06912arXiv preprintMatteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, and Rita Cucchiara. From show to tell: A survey on image captioning. arXiv preprint arXiv:2107.06912, 2021.\n\nLongtailed classification by keeping the good and removing the bad momentum causal effect. Kaihua Tang, Jianqiang Huang, Hanwang Zhang, NeurIPS. Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long- tailed classification by keeping the good and removing the bad momentum causal effect. In NeurIPS, 2020.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, NeurIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nContext-aware captions from context-agnostic supervision. Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal Chechik, CVPR. Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. Context-aware captions from context-agnostic supervision. In CVPR, 2017.\n\nCider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, CVPR. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evalua- tion. In CVPR, 2015.\n\nJoint optimization for cooperative image captioning. Gilad Vered, Gal Oren, Yuval Atzmon, Gal Chechik, ICCV. Gilad Vered, Gal Oren, Yuval Atzmon, and Gal Chechik. Joint optimization for cooperative image captioning. In ICCV, 2019.\n\nShow and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, CVPR. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du- mitru Erhan. Show and tell: A neural image caption gen- erator. In CVPR, 2015.\n\nCompare and reweight: Distinctive image captioning using similar images sets. Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B Chan, ECCV. Jiuniu Wang, Wenjia Xu, Qingzhong Wang, and Antoni B Chan. Compare and reweight: Distinctive image captioning using similar images sets. In ECCV, 2020.\n\nGroup-based distinctive image captioning with memory attention. Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B Chan, ACM MM. Jiuniu Wang, Wenjia Xu, Qingzhong Wang, and Antoni B Chan. Group-based distinctive image captioning with mem- ory attention. In ACM MM, 2021.\n\nDescribing like humans: on diversity in image captioning. Qingzhong Wang, Antoni B Chan, CVPR. Qingzhong Wang and Antoni B Chan. Describing like hu- mans: on diversity in image captioning. In CVPR, 2019.\n\nLong-tailed recognition by routing diverse distribution-aware experts. Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella Yu, ICLR. Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella Yu. Long-tailed recognition by routing diverse distribution-aware experts. In ICLR, 2020.\n\nTowards unique and informative captioning of images. Zeyu Wang, Berthy Feng, Karthik Narasimhan, Olga Russakovsky, ECCV. Zeyu Wang, Berthy Feng, Karthik Narasimhan, and Olga Russakovsky. Towards unique and informative captioning of images. In ECCV, 2020.\n\nZitong Zhang, and Yueting Zhuang. Diverse image captioning via grouptalk. Zhuhao Wang, Fei Wu, Weiming Lu, Jun Xiao, Xi Li, IJCAI. Zhuhao Wang, Fei Wu, Weiming Lu, Jun Xiao, Xi Li, Zitong Zhang, and Yueting Zhuang. Diverse image captioning via grouptalk. In IJCAI, 2016.\n\nOpen-domain clarification question generation without question examples. Julia White, Gabriel Poesia, Robert Hawkins, Dorsa Sadigh, Noah Goodman, EMNLP. 2021Julia White, Gabriel Poesia, Robert Hawkins, Dorsa Sadigh, and Noah Goodman. Open-domain clarification question generation without question examples. In EMNLP, 2021.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83Ronald J Williams. Simple statistical gradient-following al- gorithms for connectionist reinforcement learning. Machine learning, 8(3):229-256, 1992.\n\nFine-grained image captioning with global-local discriminative objective. Jie Wu, Tianshui Chen, Hefeng Wu, Zhi Yang, Guangchun Luo, Liang Lin, IEEE Transactions on Multimedia. 23Jie Wu, Tianshui Chen, Hefeng Wu, Zhi Yang, Guangchun Luo, and Liang Lin. Fine-grained image captioning with global-local discriminative objective. IEEE Transactions on Multimedia, 23:2413-2427, 2021.\n\nA study of reinforcement learning for neural machine translation. Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu, In EMNLP. Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning for neural machine translation. In EMNLP, 2018.\n\nImportance-aware learning for neural headline editing. Qingyang Wu, Lei Li, Hao Zhou, Ying Zeng, Zhou Yu, AAAI. Qingyang Wu, Lei Li, Hao Zhou, Ying Zeng, and Zhou Yu. Importance-aware learning for neural headline editing. In AAAI, 2020.\n\nRich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, ICML. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption gen- eration with visual attention. In ICML, 2015.\n\nImproving image captioning evaluation by considering inter references variance. Yanzhi Yi, Hangyu Deng, Jinglu Hu, ACL. Yanzhi Yi, Hangyu Deng, and Jinglu Hu. Improving image captioning evaluation by considering inter references vari- ance. In ACL, 2020.\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, TACL. 2Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken- maier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descrip- tions. TACL, 2:67-78, 2014.\n\nVinvl: Revisiting visual representations in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, CVPR. 2021Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.\n\nBertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger, Artzi, ICLR. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein- berger, and Yoav Artzi. Bertscore: Evaluating text genera- tion with bert. In ICLR, 2020.\n\nDeep long-tailed learning: A survey. Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, Jiashi Feng, arXiv:2110.04596arXiv preprintYifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: A survey. arXiv preprint arXiv:2110.04596, 2021.\n\nShow and write: Entity-aware news generation with image information. Zhongping Zhang, Yiwen Gu, Bryan A Plummer, arXiv:2112.05917arXiv preprintReferencesZhongping Zhang, Yiwen Gu, and Bryan A Plummer. Show and write: Entity-aware news generation with image infor- mation. arXiv preprint arXiv:2112.05917, 2021. References\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, CVPR. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.\n\nMirostat: A neural text decoding algorithm that directly controls perplexity. Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, Lav R Varshney, ICLR. 2021Sourya Basu, Govardana Sachitanandam Ramachandran, Ni- tish Shirish Keskar, and Lav R Varshney. Mirostat: A neural text decoding algorithm that directly controls perplexity. In ICLR, 2021.\n\nFine-grained image captioning with CLIP reward. Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Dernoncourt, Trung Bui, Mohit Bansal, Findings of the Association for Computational Linguistics: NAACL 2022. Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Der- noncourt, Trung Bui, and Mohit Bansal. Fine-grained image captioning with CLIP reward. In Findings of the Association for Computational Linguistics: NAACL 2022, 2022.\n\nWordNet: An Electronic Lexical Database. Christiane Fellbaum, The MIT PressChristiane Fellbaum. WordNet: An Electronic Lexical Database. The MIT Press, 1998.\n\nToken-level adaptive training for neural machine translation. Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie Zhou, Dong Yu, EMNLP. Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie Zhou, and Dong Yu. Token-level adaptive training for neural machine translation. In EMNLP, 2020.\n\nWomen also snowboard: Overcoming bias in captioning models. Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, Anna Rohrbach, In ECCV. Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women also snowboard: Over- coming bias in captioning models. In ECCV, 2018.\n\nThe curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, ICLR. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020.\n\nImproving neural response diversity with frequencyaware cross-entropy loss. Shaojie Jiang, Pengjie Ren, Christof Monz, Maarten De Rijke, WWWShaojie Jiang, Pengjie Ren, Christof Monz, and Maarten de Rijke. Improving neural response diversity with frequency- aware cross-entropy loss. In WWW, 2019.\n\nDecoupling representation and classifier for long-tailed recognition. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis, ICLR. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decou- pling representation and classifier for long-tailed recogni- tion. In ICLR, 2020.\n\nObject-semantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, ECCV. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, 2020.\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, ICCV. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.\n\nGenerating diverse and descriptive image captions using visual paraphrases. Lixin Liu, Jiajun Tang, Xiaojun Wan, Zongming Guo, ICCV. Lixin Liu, Jiajun Tang, Xiaojun Wan, and Zongming Guo. Generating diverse and descriptive image captions using vi- sual paraphrases. In ICCV, 2019.\n\nClara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell, arXiv:2202.00666Typical decoding for natural language generation. arXiv preprintClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cot- terell. Typical decoding for natural language generation. arXiv preprint arXiv:2202.00666, 2022.\n\nImproving lexical choice in neural machine translation. Q Toan, David Nguyen, Chiang, NAACL-HLT. Toan Q Nguyen and David Chiang. Improving lexical choice in neural machine translation. In NAACL-HLT, 2018.\n\nGrit: Faster and better image captioning transformer using dual visual features. Masanori Van-Quang Nguyen, Takayuki Suganuma, Okatani, ECCV. 2022Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani. Grit: Faster and better image captioning trans- former using dual visual features. In ECCV, 2022.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, ICML. 2021Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\nOn long-tailed phenomena in neural machine translation. Vikas Raunak, Siddharth Dalmia, Vivek Gupta, Florian Metze, Findings of the Association for Computational Linguistics: EMNLP 2020. Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena in neural machine transla- tion. In Findings of the Association for Computational Lin- guistics: EMNLP 2020, 2020.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, CVPR. Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, 2017.\n\nImage description with a goal: Building efficient discriminating expressions for images. Amir Sadovnik, Yi-I Chiu, Noah Snavely, Shimon Edelman, Tsuhan Chen, CVPR. Amir Sadovnik, Yi-I Chiu, Noah Snavely, Shimon Edelman, and Tsuhan Chen. Image description with a goal: Building efficient discriminating expressions for images. In CVPR, 2012.\n\nFrom show to tell: A survey on image captioning. Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, Rita Cucchiara, arXiv:2107.06912arXiv preprintMatteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, and Rita Cucchiara. From show to tell: A survey on image captioning. arXiv preprint arXiv:2107.06912, 2021.\n\nOfa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, ICML. 2022Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In ICML, 2022.\n\nImportance-aware learning for neural headline editing. Qingyang Wu, Lei Li, Hao Zhou, Ying Zeng, Zhou Yu, AAAI. Qingyang Wu, Lei Li, Hao Zhou, Ying Zeng, and Zhou Yu. Importance-aware learning for neural headline editing. In AAAI, 2020.\n\nVinvl: Revisiting visual representations in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, CVPR. 2021Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.\n\nUnderstanding and evaluating racial biases in image captioning. Dora Zhao, Angelina Wang, Olga Russakovsky, ICCV. 2021Dora Zhao, Angelina Wang, and Olga Russakovsky. Under- standing and evaluating racial biases in image captioning. In ICCV, 2021.\n\nMen also like shopping: Reducing gender bias amplification using corpus-level constraints. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang, Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In EMNLP, 2017.\n", "annotations": {"author": "[{\"end\":203,\"start\":100},{\"end\":261,\"start\":204},{\"end\":285,\"start\":262}]", "publisher": null, "author_last_name": "[{\"end\":110,\"start\":105},{\"end\":217,\"start\":209},{\"end\":276,\"start\":267}]", "author_first_name": "[{\"end\":104,\"start\":100},{\"end\":208,\"start\":204},{\"end\":266,\"start\":262}]", "author_affiliation": "[{\"end\":194,\"start\":178},{\"end\":202,\"start\":196},{\"end\":260,\"start\":219},{\"end\":284,\"start\":278}]", "title": "[{\"end\":97,\"start\":1},{\"end\":382,\"start\":286}]", "venue": null, "abstract": "[{\"end\":1815,\"start\":384}]", "bib_ref": "[{\"end\":2115,\"start\":2111},{\"end\":2168,\"start\":2164},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2171,\"start\":2168},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2193,\"start\":2189},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":2219,\"start\":2215},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2375,\"start\":2371},{\"end\":2457,\"start\":2453},{\"end\":2460,\"start\":2457},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2463,\"start\":2460},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":2466,\"start\":2463},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2548,\"start\":2544},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2629,\"start\":2625},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2746,\"start\":2742},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2749,\"start\":2746},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2798,\"start\":2794},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5610,\"start\":5606},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5689,\"start\":5685},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":5692,\"start\":5689},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5908,\"start\":5904},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5928,\"start\":5924},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6212,\"start\":6208},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":6250,\"start\":6246},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6545,\"start\":6541},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6633,\"start\":6629},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6811,\"start\":6807},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6814,\"start\":6811},{\"end\":7112,\"start\":7109},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7115,\"start\":7112},{\"end\":7470,\"start\":7467},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7665,\"start\":7661},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7668,\"start\":7665},{\"end\":7671,\"start\":7668},{\"end\":7674,\"start\":7671},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":8380,\"start\":8376},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9780,\"start\":9776},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10307,\"start\":10303},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10383,\"start\":10379},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10386,\"start\":10383},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10389,\"start\":10386},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10395,\"start\":10391},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11177,\"start\":11173},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11296,\"start\":11292},{\"end\":12621,\"start\":12617},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12641,\"start\":12637},{\"end\":13664,\"start\":13661},{\"end\":13667,\"start\":13664},{\"end\":13670,\"start\":13667},{\"end\":14807,\"start\":14804},{\"end\":14810,\"start\":14807},{\"end\":15270,\"start\":15267},{\"end\":16039,\"start\":16038},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16044,\"start\":16040},{\"end\":16046,\"start\":16044},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16075,\"start\":16071},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16298,\"start\":16294},{\"end\":16308,\"start\":16305},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":16342,\"start\":16338},{\"end\":16354,\"start\":16350},{\"end\":16408,\"start\":16404},{\"end\":16548,\"start\":16544},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16551,\"start\":16548},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16588,\"start\":16584},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":16591,\"start\":16588},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16594,\"start\":16591},{\"end\":16728,\"start\":16724},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17068,\"start\":17064},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":17372,\"start\":17368},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17393,\"start\":17389},{\"end\":17405,\"start\":17402},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":17427,\"start\":17423},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17505,\"start\":17501},{\"end\":17534,\"start\":17533},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":17663,\"start\":17659},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":17673,\"start\":17669},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17687,\"start\":17683},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17715,\"start\":17711},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":18175,\"start\":18171},{\"end\":18178,\"start\":18175},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":18407,\"start\":18403},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18410,\"start\":18407},{\"end\":18412,\"start\":18410},{\"end\":18524,\"start\":18523},{\"end\":18656,\"start\":18653},{\"end\":19027,\"start\":19024},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":20417,\"start\":20413},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20420,\"start\":20417},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":20423,\"start\":20420},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21709,\"start\":21705},{\"end\":22587,\"start\":22586},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22689,\"start\":22685},{\"end\":22692,\"start\":22689},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":22884,\"start\":22880},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":22887,\"start\":22884},{\"end\":22933,\"start\":22929},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22936,\"start\":22933},{\"end\":22939,\"start\":22936},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23073,\"start\":23069},{\"end\":23307,\"start\":23303},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23310,\"start\":23307},{\"end\":24211,\"start\":24210},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26290,\"start\":26286},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26293,\"start\":26290},{\"end\":26296,\"start\":26293},{\"end\":26360,\"start\":26359},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":26720,\"start\":26716},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":26723,\"start\":26720},{\"end\":26820,\"start\":26819},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26835,\"start\":26831},{\"end\":26838,\"start\":26835},{\"end\":26964,\"start\":26962},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":28335,\"start\":28331},{\"end\":28469,\"start\":28467},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":28760,\"start\":28756},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":29542,\"start\":29538},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":29568,\"start\":29564},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29586,\"start\":29582},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":29589,\"start\":29586},{\"end\":29631,\"start\":29628},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29662,\"start\":29658},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":29704,\"start\":29700},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29707,\"start\":29704},{\"end\":29709,\"start\":29707},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29712,\"start\":29709},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29715,\"start\":29712},{\"end\":29808,\"start\":29804},{\"end\":29811,\"start\":29808},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":29814,\"start\":29811},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":29817,\"start\":29814},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29912,\"start\":29908},{\"end\":30121,\"start\":30118},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":30242,\"start\":30238},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":30292,\"start\":30288},{\"end\":30295,\"start\":30292},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":30370,\"start\":30366},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30463,\"start\":30459},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30589,\"start\":30585},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30598,\"start\":30594},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30769,\"start\":30765},{\"end\":30849,\"start\":30845},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":30883,\"start\":30879},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":31112,\"start\":31108},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31125,\"start\":31121},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":31350,\"start\":31346},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31418,\"start\":31414},{\"end\":31548,\"start\":31544},{\"end\":31560,\"start\":31556},{\"end\":31563,\"start\":31560},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31588,\"start\":31584},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":31597,\"start\":31593},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31702,\"start\":31698},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":31870,\"start\":31866},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32232,\"start\":32228},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":32280,\"start\":32276},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":32409,\"start\":32405},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":32768,\"start\":32764},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32771,\"start\":32768},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":32811,\"start\":32807},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":33152,\"start\":33148},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33155,\"start\":33152},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":33320,\"start\":33316},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33323,\"start\":33320},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33360,\"start\":33356},{\"end\":33363,\"start\":33360},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33366,\"start\":33363},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":33369,\"start\":33366},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33372,\"start\":33369},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":34449,\"start\":34445},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":34469,\"start\":34465},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34499,\"start\":34495},{\"end\":35385,\"start\":35382},{\"end\":35584,\"start\":35580},{\"end\":35587,\"start\":35584},{\"end\":35589,\"start\":35587},{\"end\":35963,\"start\":35959},{\"end\":35978,\"start\":35975},{\"end\":40408,\"start\":40405},{\"end\":40667,\"start\":40663},{\"end\":40846,\"start\":40843},{\"end\":41209,\"start\":41205},{\"end\":41285,\"start\":41281},{\"end\":41816,\"start\":41812},{\"end\":41838,\"start\":41835},{\"end\":41840,\"start\":41838},{\"end\":41843,\"start\":41840},{\"end\":42387,\"start\":42383},{\"end\":43248,\"start\":43245},{\"end\":43250,\"start\":43248},{\"end\":43253,\"start\":43250},{\"end\":43621,\"start\":43618},{\"end\":43699,\"start\":43696},{\"end\":43702,\"start\":43699},{\"end\":45858,\"start\":45854},{\"end\":45861,\"start\":45858},{\"end\":46089,\"start\":46085},{\"end\":46092,\"start\":46089},{\"end\":46095,\"start\":46092},{\"end\":46376,\"start\":46372},{\"end\":47790,\"start\":47787},{\"end\":48051,\"start\":48048},{\"end\":48287,\"start\":48284},{\"end\":48596,\"start\":48592},{\"end\":50676,\"start\":50672},{\"end\":51798,\"start\":51795},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":64878,\"start\":64874},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":66239,\"start\":66235}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52431,\"start\":52031},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52882,\"start\":52432},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53453,\"start\":52883},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53624,\"start\":53454},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54152,\"start\":53625},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54553,\"start\":54153},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55077,\"start\":54554},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55369,\"start\":55078},{\"attributes\":{\"id\":\"fig_8\"},\"end\":55417,\"start\":55370},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":57842,\"start\":55418},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59477,\"start\":57843},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":60154,\"start\":59478},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":60999,\"start\":60155},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61560,\"start\":61000},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":63456,\"start\":61561},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":63703,\"start\":63457},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":64651,\"start\":63704}]", "paragraph": "[{\"end\":2965,\"start\":1831},{\"end\":3526,\"start\":2967},{\"end\":4053,\"start\":3528},{\"end\":4694,\"start\":4055},{\"end\":5389,\"start\":4696},{\"end\":5775,\"start\":5435},{\"end\":6322,\"start\":5802},{\"end\":6634,\"start\":6424},{\"end\":6933,\"start\":6659},{\"end\":7047,\"start\":6935},{\"end\":7510,\"start\":7049},{\"end\":8234,\"start\":7512},{\"end\":9022,\"start\":8275},{\"end\":9310,\"start\":9024},{\"end\":9892,\"start\":9348},{\"end\":10180,\"start\":9894},{\"end\":11161,\"start\":10209},{\"end\":11820,\"start\":11163},{\"end\":12053,\"start\":11875},{\"end\":12399,\"start\":12102},{\"end\":12793,\"start\":12507},{\"end\":13180,\"start\":12824},{\"end\":13693,\"start\":13182},{\"end\":13957,\"start\":13847},{\"end\":15954,\"start\":14018},{\"end\":17105,\"start\":15978},{\"end\":17337,\"start\":17107},{\"end\":19416,\"start\":17339},{\"end\":19738,\"start\":19418},{\"end\":21304,\"start\":19778},{\"end\":22589,\"start\":21306},{\"end\":23311,\"start\":22591},{\"end\":23999,\"start\":23313},{\"end\":25233,\"start\":24035},{\"end\":25866,\"start\":25235},{\"end\":26147,\"start\":25868},{\"end\":27124,\"start\":26149},{\"end\":27525,\"start\":27196},{\"end\":28471,\"start\":27546},{\"end\":29333,\"start\":28473},{\"end\":30515,\"start\":29350},{\"end\":31564,\"start\":30517},{\"end\":33593,\"start\":31566},{\"end\":34121,\"start\":33608},{\"end\":34534,\"start\":34123},{\"end\":34915,\"start\":34577},{\"end\":35479,\"start\":34917},{\"end\":36143,\"start\":35481},{\"end\":36723,\"start\":36236},{\"end\":37346,\"start\":36748},{\"end\":38112,\"start\":37375},{\"end\":39670,\"start\":38114},{\"end\":40296,\"start\":39783},{\"end\":40668,\"start\":40355},{\"end\":41008,\"start\":40670},{\"end\":41478,\"start\":41036},{\"end\":43047,\"start\":41586},{\"end\":44525,\"start\":43049},{\"end\":45586,\"start\":44527},{\"end\":46096,\"start\":45722},{\"end\":46377,\"start\":46098},{\"end\":47678,\"start\":46379},{\"end\":48020,\"start\":47758},{\"end\":49023,\"start\":48022},{\"end\":50108,\"start\":49025},{\"end\":50823,\"start\":50110},{\"end\":51665,\"start\":50825},{\"end\":51851,\"start\":51667},{\"end\":52030,\"start\":51853}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6368,\"start\":6323},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6423,\"start\":6368},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11874,\"start\":11821},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12101,\"start\":12054},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12459,\"start\":12400},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12506,\"start\":12459},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13846,\"start\":13694},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14017,\"start\":13958},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27195,\"start\":27125},{\"attributes\":{\"id\":\"formula_9\"},\"end\":41035,\"start\":41009},{\"attributes\":{\"id\":\"formula_10\"},\"end\":41531,\"start\":41479},{\"attributes\":{\"id\":\"formula_11\"},\"end\":41585,\"start\":41531}]", "table_ref": "[{\"end\":16736,\"start\":16729},{\"end\":19817,\"start\":19810},{\"end\":24560,\"start\":24553},{\"end\":25469,\"start\":25462},{\"end\":26308,\"start\":26301},{\"end\":26732,\"start\":26725},{\"end\":28923,\"start\":28916},{\"end\":29165,\"start\":29158},{\"end\":37793,\"start\":37786},{\"end\":38217,\"start\":38210},{\"end\":43711,\"start\":43704},{\"end\":45370,\"start\":45363},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":46838,\"start\":46831},{\"end\":46871,\"start\":46864},{\"end\":48673,\"start\":48666},{\"end\":49755,\"start\":49748}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1829,\"start\":1817},{\"attributes\":{\"n\":\"2.\"},\"end\":5433,\"start\":5392},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5800,\"start\":5778},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6657,\"start\":6637},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8273,\"start\":8237},{\"attributes\":{\"n\":\"3.\"},\"end\":9346,\"start\":9313},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10207,\"start\":10183},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12822,\"start\":12796},{\"attributes\":{\"n\":\"4.\"},\"end\":15968,\"start\":15957},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15976,\"start\":15971},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19776,\"start\":19741},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24033,\"start\":24002},{\"attributes\":{\"n\":\"4.4.\"},\"end\":27544,\"start\":27528},{\"attributes\":{\"n\":\"5.\"},\"end\":29348,\"start\":29336},{\"attributes\":{\"n\":\"6.\"},\"end\":33606,\"start\":33596},{\"attributes\":{\"n\":\"1.\"},\"end\":34575,\"start\":34537},{\"attributes\":{\"n\":\"2.\"},\"end\":36169,\"start\":36146},{\"attributes\":{\"n\":\"3.\"},\"end\":36207,\"start\":36172},{\"attributes\":{\"n\":\"4.\"},\"end\":36234,\"start\":36210},{\"attributes\":{\"n\":\"5.\"},\"end\":36746,\"start\":36726},{\"attributes\":{\"n\":\"6.\"},\"end\":37373,\"start\":37349},{\"attributes\":{\"n\":\"7.\"},\"end\":39705,\"start\":39673},{\"attributes\":{\"n\":\"8.\"},\"end\":39751,\"start\":39708},{\"attributes\":{\"n\":\"9.\"},\"end\":39781,\"start\":39754},{\"attributes\":{\"n\":\"10.\"},\"end\":40353,\"start\":40299},{\"attributes\":{\"n\":\"11.\"},\"end\":45628,\"start\":45589},{\"end\":45641,\"start\":45631},{\"end\":45682,\"start\":45644},{\"attributes\":{\"n\":\"12.\"},\"end\":45720,\"start\":45685},{\"attributes\":{\"n\":\"13.\"},\"end\":47756,\"start\":47681},{\"end\":52443,\"start\":52433},{\"end\":52894,\"start\":52884},{\"end\":53465,\"start\":53455},{\"end\":53634,\"start\":53626},{\"end\":54164,\"start\":54154},{\"end\":54565,\"start\":54555},{\"end\":55089,\"start\":55079},{\"end\":55381,\"start\":55371},{\"end\":63467,\"start\":63458}]", "table": "[{\"end\":57842,\"start\":55561},{\"end\":59477,\"start\":58207},{\"end\":60154,\"start\":59600},{\"end\":61560,\"start\":61035},{\"end\":63456,\"start\":62043},{\"end\":64651,\"start\":63783}]", "figure_caption": "[{\"end\":52431,\"start\":52033},{\"end\":52882,\"start\":52445},{\"end\":53453,\"start\":52896},{\"end\":53624,\"start\":53467},{\"end\":54152,\"start\":53636},{\"end\":54553,\"start\":54166},{\"end\":55077,\"start\":54567},{\"end\":55369,\"start\":55091},{\"end\":55417,\"start\":55383},{\"end\":55561,\"start\":55420},{\"end\":58207,\"start\":57845},{\"end\":59600,\"start\":59480},{\"end\":60999,\"start\":60157},{\"end\":61035,\"start\":61002},{\"end\":62043,\"start\":61563},{\"end\":63703,\"start\":63469},{\"end\":63783,\"start\":63706}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2914,\"start\":2906},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7840,\"start\":7832},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8768,\"start\":8760},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14256,\"start\":14248},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":15045,\"start\":15037},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35155,\"start\":35147},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35836,\"start\":35828},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38562,\"start\":38554},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39320,\"start\":39312},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":39820,\"start\":39812},{\"end\":44642,\"start\":44634},{\"end\":45124,\"start\":45116}]", "bib_author_first_name": "[{\"end\":66589,\"start\":66584},{\"end\":66606,\"start\":66600},{\"end\":66621,\"start\":66617},{\"end\":66638,\"start\":66631},{\"end\":66882,\"start\":66877},{\"end\":66901,\"start\":66893},{\"end\":66911,\"start\":66906},{\"end\":66927,\"start\":66921},{\"end\":66939,\"start\":66935},{\"end\":66956,\"start\":66949},{\"end\":66967,\"start\":66964},{\"end\":67252,\"start\":67247},{\"end\":67265,\"start\":67262},{\"end\":67468,\"start\":67464},{\"end\":67482,\"start\":67477},{\"end\":67499,\"start\":67492},{\"end\":67512,\"start\":67508},{\"end\":67785,\"start\":67777},{\"end\":67801,\"start\":67792},{\"end\":67817,\"start\":67809},{\"end\":67827,\"start\":67823},{\"end\":67841,\"start\":67834},{\"end\":67854,\"start\":67848},{\"end\":67865,\"start\":67860},{\"end\":68151,\"start\":68145},{\"end\":68161,\"start\":68158},{\"end\":68176,\"start\":68168},{\"end\":68193,\"start\":68182},{\"end\":68211,\"start\":68204},{\"end\":68224,\"start\":68219},{\"end\":68243,\"start\":68233},{\"end\":68555,\"start\":68549},{\"end\":68570,\"start\":68561},{\"end\":68584,\"start\":68577},{\"end\":68597,\"start\":68591},{\"end\":68616,\"start\":68611},{\"end\":68627,\"start\":68622},{\"end\":69011,\"start\":69005},{\"end\":69025,\"start\":69021},{\"end\":69036,\"start\":69031},{\"end\":69051,\"start\":69047},{\"end\":69313,\"start\":69302},{\"end\":69325,\"start\":69321},{\"end\":69339,\"start\":69335},{\"end\":69614,\"start\":69608},{\"end\":69632,\"start\":69628},{\"end\":69653,\"start\":69642},{\"end\":69897,\"start\":69895},{\"end\":69908,\"start\":69903},{\"end\":69923,\"start\":69917},{\"end\":69938,\"start\":69933},{\"end\":70132,\"start\":70130},{\"end\":70143,\"start\":70138},{\"end\":70318,\"start\":70313},{\"end\":70335,\"start\":70328},{\"end\":70348,\"start\":70344},{\"end\":70576,\"start\":70570},{\"end\":70589,\"start\":70585},{\"end\":70602,\"start\":70595},{\"end\":70614,\"start\":70609},{\"end\":70642,\"start\":70625},{\"end\":70911,\"start\":70904},{\"end\":70921,\"start\":70920},{\"end\":70934,\"start\":70929},{\"end\":70939,\"start\":70935},{\"end\":70952,\"start\":70947},{\"end\":71161,\"start\":71157},{\"end\":71175,\"start\":71169},{\"end\":71189,\"start\":71181},{\"end\":71205,\"start\":71197},{\"end\":71207,\"start\":71206},{\"end\":71221,\"start\":71215},{\"end\":71404,\"start\":71401},{\"end\":71421,\"start\":71417},{\"end\":71442,\"start\":71437},{\"end\":71454,\"start\":71450},{\"end\":71464,\"start\":71459},{\"end\":71486,\"start\":71479},{\"end\":71499,\"start\":71494},{\"end\":71517,\"start\":71511},{\"end\":71780,\"start\":71774},{\"end\":71792,\"start\":71785},{\"end\":71807,\"start\":71800},{\"end\":71818,\"start\":71814},{\"end\":71832,\"start\":71825},{\"end\":71841,\"start\":71838},{\"end\":71852,\"start\":71848},{\"end\":72122,\"start\":72117},{\"end\":72136,\"start\":72131},{\"end\":72358,\"start\":72356},{\"end\":72368,\"start\":72363},{\"end\":72380,\"start\":72374},{\"end\":72608,\"start\":72604},{\"end\":72620,\"start\":72617},{\"end\":72638,\"start\":72631},{\"end\":72652,\"start\":72647},{\"end\":72914,\"start\":72913},{\"end\":73114,\"start\":73110},{\"end\":73133,\"start\":73127},{\"end\":73325,\"start\":73322},{\"end\":73339,\"start\":73336},{\"end\":73348,\"start\":73346},{\"end\":73360,\"start\":73353},{\"end\":73374,\"start\":73369},{\"end\":73577,\"start\":73573},{\"end\":73592,\"start\":73585},{\"end\":73603,\"start\":73600},{\"end\":73614,\"start\":73611},{\"end\":73630,\"start\":73621},{\"end\":73641,\"start\":73638},{\"end\":73651,\"start\":73647},{\"end\":73669,\"start\":73661},{\"end\":73965,\"start\":73958},{\"end\":73980,\"start\":73973},{\"end\":73994,\"start\":73986},{\"end\":74008,\"start\":74001},{\"end\":74256,\"start\":74250},{\"end\":74270,\"start\":74263},{\"end\":74282,\"start\":74276},{\"end\":74301,\"start\":74293},{\"end\":74313,\"start\":74307},{\"end\":74327,\"start\":74321},{\"end\":74340,\"start\":74334},{\"end\":74629,\"start\":74623},{\"end\":74642,\"start\":74640},{\"end\":74831,\"start\":74826},{\"end\":74846,\"start\":74839},{\"end\":74865,\"start\":74858},{\"end\":74880,\"start\":74875},{\"end\":74896,\"start\":74891},{\"end\":74916,\"start\":74912},{\"end\":74918,\"start\":74917},{\"end\":75216,\"start\":75210},{\"end\":75233,\"start\":75228},{\"end\":75497,\"start\":75488},{\"end\":75509,\"start\":75503},{\"end\":75521,\"start\":75516},{\"end\":75783,\"start\":75777},{\"end\":75797,\"start\":75793},{\"end\":75809,\"start\":75803},{\"end\":75823,\"start\":75817},{\"end\":75838,\"start\":75833},{\"end\":75851,\"start\":75845},{\"end\":75870,\"start\":75861},{\"end\":75883,\"start\":75877},{\"end\":75902,\"start\":75896},{\"end\":75912,\"start\":75907},{\"end\":75914,\"start\":75913},{\"end\":75930,\"start\":75923},{\"end\":75944,\"start\":75942},{\"end\":76344,\"start\":76337},{\"end\":76359,\"start\":76350},{\"end\":76372,\"start\":76366},{\"end\":76390,\"start\":76386},{\"end\":76401,\"start\":76396},{\"end\":76413,\"start\":76407},{\"end\":76786,\"start\":76781},{\"end\":76795,\"start\":76791},{\"end\":76808,\"start\":76804},{\"end\":76820,\"start\":76817},{\"end\":76837,\"start\":76831},{\"end\":76854,\"start\":76846},{\"end\":77067,\"start\":77061},{\"end\":77079,\"start\":77072},{\"end\":77091,\"start\":77085},{\"end\":77104,\"start\":77098},{\"end\":77115,\"start\":77112},{\"end\":77125,\"start\":77120},{\"end\":77335,\"start\":77329},{\"end\":77342,\"start\":77340},{\"end\":77356,\"start\":77348},{\"end\":77370,\"start\":77361},{\"end\":77385,\"start\":77378},{\"end\":77393,\"start\":77390},{\"end\":77407,\"start\":77401},{\"end\":77421,\"start\":77414},{\"end\":77428,\"start\":77426},{\"end\":77439,\"start\":77435},{\"end\":77739,\"start\":77731},{\"end\":77750,\"start\":77745},{\"end\":77762,\"start\":77758},{\"end\":77957,\"start\":77949},{\"end\":77970,\"start\":77963},{\"end\":77983,\"start\":77978},{\"end\":77999,\"start\":77994},{\"end\":78012,\"start\":78006},{\"end\":78025,\"start\":78021},{\"end\":78040,\"start\":78035},{\"end\":78059,\"start\":78049},{\"end\":78339,\"start\":78334},{\"end\":78351,\"start\":78345},{\"end\":78365,\"start\":78358},{\"end\":78379,\"start\":78371},{\"end\":78638,\"start\":78633},{\"end\":78653,\"start\":78644},{\"end\":78662,\"start\":78658},{\"end\":78675,\"start\":78669},{\"end\":78690,\"start\":78682},{\"end\":78951,\"start\":78944},{\"end\":78962,\"start\":78957},{\"end\":78975,\"start\":78970},{\"end\":78990,\"start\":78983},{\"end\":79226,\"start\":79219},{\"end\":79239,\"start\":79232},{\"end\":79493,\"start\":79486},{\"end\":79503,\"start\":79499},{\"end\":79517,\"start\":79510},{\"end\":79529,\"start\":79525},{\"end\":79544,\"start\":79537},{\"end\":79556,\"start\":79552},{\"end\":79566,\"start\":79563},{\"end\":79798,\"start\":79792},{\"end\":79806,\"start\":79799},{\"end\":79820,\"start\":79814},{\"end\":79838,\"start\":79833},{\"end\":79860,\"start\":79852},{\"end\":79874,\"start\":79867},{\"end\":79887,\"start\":79881},{\"end\":80150,\"start\":80146},{\"end\":80160,\"start\":80159},{\"end\":80162,\"start\":80161},{\"end\":80181,\"start\":80180},{\"end\":80199,\"start\":80188},{\"end\":80457,\"start\":80456},{\"end\":80469,\"start\":80464},{\"end\":80683,\"start\":80674},{\"end\":80699,\"start\":80694},{\"end\":80920,\"start\":80913},{\"end\":80932,\"start\":80927},{\"end\":80949,\"start\":80942},{\"end\":80966,\"start\":80958},{\"end\":81196,\"start\":81191},{\"end\":81214,\"start\":81205},{\"end\":81228,\"start\":81223},{\"end\":81243,\"start\":81236},{\"end\":81582,\"start\":81581},{\"end\":81598,\"start\":81591},{\"end\":81614,\"start\":81607},{\"end\":81632,\"start\":81626},{\"end\":81649,\"start\":81641},{\"end\":81915,\"start\":81911},{\"end\":81930,\"start\":81926},{\"end\":81941,\"start\":81937},{\"end\":81957,\"start\":81951},{\"end\":81973,\"start\":81967},{\"end\":82269,\"start\":82263},{\"end\":82281,\"start\":82278},{\"end\":82297,\"start\":82288},{\"end\":82311,\"start\":82307},{\"end\":82606,\"start\":82601},{\"end\":82616,\"start\":82612},{\"end\":82629,\"start\":82621},{\"end\":82874,\"start\":82870},{\"end\":82883,\"start\":82880},{\"end\":82896,\"start\":82889},{\"end\":83089,\"start\":83083},{\"end\":83109,\"start\":83101},{\"end\":83125,\"start\":83118},{\"end\":83141,\"start\":83135},{\"end\":83163,\"start\":83155},{\"end\":83177,\"start\":83173},{\"end\":83515,\"start\":83509},{\"end\":83531,\"start\":83522},{\"end\":83546,\"start\":83539},{\"end\":83757,\"start\":83751},{\"end\":83771,\"start\":83767},{\"end\":83785,\"start\":83781},{\"end\":83799,\"start\":83794},{\"end\":83816,\"start\":83811},{\"end\":83829,\"start\":83824},{\"end\":83831,\"start\":83830},{\"end\":83845,\"start\":83839},{\"end\":83859,\"start\":83854},{\"end\":84123,\"start\":84112},{\"end\":84138,\"start\":84134},{\"end\":84152,\"start\":84147},{\"end\":84165,\"start\":84161},{\"end\":84177,\"start\":84174},{\"end\":84410,\"start\":84399},{\"end\":84429,\"start\":84421},{\"end\":84443,\"start\":84439},{\"end\":84646,\"start\":84641},{\"end\":84657,\"start\":84654},{\"end\":84669,\"start\":84664},{\"end\":84681,\"start\":84678},{\"end\":84874,\"start\":84869},{\"end\":84893,\"start\":84884},{\"end\":84906,\"start\":84902},{\"end\":84922,\"start\":84915},{\"end\":85154,\"start\":85148},{\"end\":85167,\"start\":85161},{\"end\":85181,\"start\":85172},{\"end\":85194,\"start\":85188},{\"end\":85196,\"start\":85195},{\"end\":85432,\"start\":85426},{\"end\":85445,\"start\":85439},{\"end\":85459,\"start\":85450},{\"end\":85472,\"start\":85466},{\"end\":85474,\"start\":85473},{\"end\":85699,\"start\":85690},{\"end\":85712,\"start\":85706},{\"end\":85714,\"start\":85713},{\"end\":85914,\"start\":85908},{\"end\":85925,\"start\":85921},{\"end\":85939,\"start\":85932},{\"end\":85951,\"start\":85946},{\"end\":85963,\"start\":85957},{\"end\":86182,\"start\":86178},{\"end\":86195,\"start\":86189},{\"end\":86209,\"start\":86202},{\"end\":86226,\"start\":86222},{\"end\":86461,\"start\":86455},{\"end\":86471,\"start\":86468},{\"end\":86483,\"start\":86476},{\"end\":86491,\"start\":86488},{\"end\":86500,\"start\":86498},{\"end\":86731,\"start\":86726},{\"end\":86746,\"start\":86739},{\"end\":86761,\"start\":86755},{\"end\":86776,\"start\":86771},{\"end\":86789,\"start\":86785},{\"end\":87069,\"start\":87068},{\"end\":87336,\"start\":87333},{\"end\":87349,\"start\":87341},{\"end\":87362,\"start\":87356},{\"end\":87370,\"start\":87367},{\"end\":87386,\"start\":87377},{\"end\":87397,\"start\":87392},{\"end\":87711,\"start\":87706},{\"end\":87719,\"start\":87716},{\"end\":87729,\"start\":87726},{\"end\":87744,\"start\":87735},{\"end\":87757,\"start\":87750},{\"end\":87980,\"start\":87972},{\"end\":87988,\"start\":87985},{\"end\":87996,\"start\":87993},{\"end\":88007,\"start\":88003},{\"end\":88018,\"start\":88014},{\"end\":88270,\"start\":88264},{\"end\":88280,\"start\":88275},{\"end\":88289,\"start\":88285},{\"end\":88306,\"start\":88297},{\"end\":88317,\"start\":88312},{\"end\":88335,\"start\":88329},{\"end\":88655,\"start\":88649},{\"end\":88666,\"start\":88660},{\"end\":88679,\"start\":88673},{\"end\":88948,\"start\":88943},{\"end\":88961,\"start\":88956},{\"end\":88972,\"start\":88967},{\"end\":88986,\"start\":88981},{\"end\":89289,\"start\":89280},{\"end\":89303,\"start\":89297},{\"end\":89315,\"start\":89308},{\"end\":89327,\"start\":89320},{\"end\":89337,\"start\":89334},{\"end\":89351,\"start\":89345},{\"end\":89363,\"start\":89358},{\"end\":89378,\"start\":89370},{\"end\":89641,\"start\":89635},{\"end\":89655,\"start\":89649},{\"end\":89670,\"start\":89665},{\"end\":89676,\"start\":89675},{\"end\":89689,\"start\":89685},{\"end\":89903,\"start\":89898},{\"end\":89917,\"start\":89911},{\"end\":89929,\"start\":89924},{\"end\":89945,\"start\":89936},{\"end\":89957,\"start\":89951},{\"end\":90219,\"start\":90210},{\"end\":90232,\"start\":90227},{\"end\":90244,\"start\":90237},{\"end\":90554,\"start\":90549},{\"end\":90573,\"start\":90565},{\"end\":90583,\"start\":90578},{\"end\":90599,\"start\":90593},{\"end\":90611,\"start\":90607},{\"end\":90628,\"start\":90621},{\"end\":90639,\"start\":90636},{\"end\":90940,\"start\":90934},{\"end\":90956,\"start\":90947},{\"end\":90991,\"start\":90985},{\"end\":91013,\"start\":91008},{\"end\":91278,\"start\":91272},{\"end\":91293,\"start\":91284},{\"end\":91307,\"start\":91300},{\"end\":91320,\"start\":91314},{\"end\":91339,\"start\":91334},{\"end\":91350,\"start\":91345},{\"end\":91703,\"start\":91693},{\"end\":91879,\"start\":91873},{\"end\":91891,\"start\":91884},{\"end\":91906,\"start\":91899},{\"end\":91917,\"start\":91913},{\"end\":91931,\"start\":91924},{\"end\":91940,\"start\":91937},{\"end\":91951,\"start\":91947},{\"end\":92193,\"start\":92189},{\"end\":92198,\"start\":92194},{\"end\":92216,\"start\":92210},{\"end\":92228,\"start\":92224},{\"end\":92243,\"start\":92237},{\"end\":92257,\"start\":92253},{\"end\":92487,\"start\":92484},{\"end\":92501,\"start\":92498},{\"end\":92510,\"start\":92508},{\"end\":92522,\"start\":92515},{\"end\":92536,\"start\":92531},{\"end\":92757,\"start\":92750},{\"end\":92772,\"start\":92765},{\"end\":92786,\"start\":92778},{\"end\":92800,\"start\":92793},{\"end\":93048,\"start\":93042},{\"end\":93062,\"start\":93055},{\"end\":93074,\"start\":93068},{\"end\":93093,\"start\":93085},{\"end\":93105,\"start\":93099},{\"end\":93119,\"start\":93113},{\"end\":93132,\"start\":93126},{\"end\":93419,\"start\":93413},{\"end\":93426,\"start\":93424},{\"end\":93440,\"start\":93432},{\"end\":93454,\"start\":93445},{\"end\":93469,\"start\":93462},{\"end\":93477,\"start\":93474},{\"end\":93491,\"start\":93485},{\"end\":93505,\"start\":93498},{\"end\":93512,\"start\":93510},{\"end\":93523,\"start\":93519},{\"end\":93823,\"start\":93815},{\"end\":93834,\"start\":93829},{\"end\":93846,\"start\":93842},{\"end\":94071,\"start\":94066},{\"end\":94083,\"start\":94077},{\"end\":94097,\"start\":94090},{\"end\":94111,\"start\":94103},{\"end\":94277,\"start\":94272},{\"end\":94292,\"start\":94287},{\"end\":94307,\"start\":94303},{\"end\":94319,\"start\":94315},{\"end\":94623,\"start\":94622},{\"end\":94635,\"start\":94630},{\"end\":94861,\"start\":94853},{\"end\":94888,\"start\":94880},{\"end\":95151,\"start\":95147},{\"end\":95165,\"start\":95161},{\"end\":95170,\"start\":95166},{\"end\":95181,\"start\":95176},{\"end\":95197,\"start\":95191},{\"end\":95213,\"start\":95206},{\"end\":95227,\"start\":95219},{\"end\":95243,\"start\":95237},{\"end\":95258,\"start\":95252},{\"end\":95273,\"start\":95267},{\"end\":95287,\"start\":95283},{\"end\":95303,\"start\":95295},{\"end\":95317,\"start\":95313},{\"end\":95673,\"start\":95668},{\"end\":95691,\"start\":95682},{\"end\":95705,\"start\":95700},{\"end\":95720,\"start\":95713},{\"end\":96059,\"start\":96058},{\"end\":96075,\"start\":96068},{\"end\":96091,\"start\":96084},{\"end\":96109,\"start\":96103},{\"end\":96126,\"start\":96118},{\"end\":96392,\"start\":96388},{\"end\":96407,\"start\":96403},{\"end\":96418,\"start\":96414},{\"end\":96434,\"start\":96428},{\"end\":96450,\"start\":96444},{\"end\":96696,\"start\":96690},{\"end\":96716,\"start\":96708},{\"end\":96732,\"start\":96725},{\"end\":96748,\"start\":96742},{\"end\":96770,\"start\":96762},{\"end\":96784,\"start\":96780},{\"end\":97138,\"start\":97134},{\"end\":97147,\"start\":97145},{\"end\":97157,\"start\":97154},{\"end\":97170,\"start\":97163},{\"end\":97181,\"start\":97176},{\"end\":97194,\"start\":97187},{\"end\":97206,\"start\":97199},{\"end\":97216,\"start\":97211},{\"end\":97230,\"start\":97223},{\"end\":97244,\"start\":97237},{\"end\":97570,\"start\":97562},{\"end\":97578,\"start\":97575},{\"end\":97586,\"start\":97583},{\"end\":97597,\"start\":97593},{\"end\":97608,\"start\":97604},{\"end\":97822,\"start\":97813},{\"end\":97836,\"start\":97830},{\"end\":97848,\"start\":97841},{\"end\":97860,\"start\":97853},{\"end\":97870,\"start\":97867},{\"end\":97884,\"start\":97878},{\"end\":97896,\"start\":97891},{\"end\":97911,\"start\":97903},{\"end\":98187,\"start\":98183},{\"end\":98202,\"start\":98194},{\"end\":98213,\"start\":98209},{\"end\":98463,\"start\":98458},{\"end\":98476,\"start\":98470},{\"end\":98487,\"start\":98483},{\"end\":98504,\"start\":98497},{\"end\":98521,\"start\":98514}]", "bib_author_last_name": "[{\"end\":66598,\"start\":66590},{\"end\":66615,\"start\":66607},{\"end\":66629,\"start\":66622},{\"end\":66644,\"start\":66639},{\"end\":66891,\"start\":66883},{\"end\":66904,\"start\":66902},{\"end\":66919,\"start\":66912},{\"end\":66933,\"start\":66928},{\"end\":66947,\"start\":66940},{\"end\":66962,\"start\":66957},{\"end\":66973,\"start\":66968},{\"end\":67260,\"start\":67253},{\"end\":67271,\"start\":67266},{\"end\":67475,\"start\":67469},{\"end\":67490,\"start\":67483},{\"end\":67506,\"start\":67500},{\"end\":67520,\"start\":67513},{\"end\":67790,\"start\":67786},{\"end\":67807,\"start\":67802},{\"end\":67821,\"start\":67818},{\"end\":67832,\"start\":67828},{\"end\":67846,\"start\":67842},{\"end\":67858,\"start\":67855},{\"end\":67869,\"start\":67866},{\"end\":68156,\"start\":68152},{\"end\":68166,\"start\":68162},{\"end\":68180,\"start\":68177},{\"end\":68202,\"start\":68194},{\"end\":68217,\"start\":68212},{\"end\":68231,\"start\":68225},{\"end\":68251,\"start\":68244},{\"end\":68559,\"start\":68556},{\"end\":68575,\"start\":68571},{\"end\":68589,\"start\":68585},{\"end\":68609,\"start\":68598},{\"end\":68620,\"start\":68617},{\"end\":68634,\"start\":68628},{\"end\":69019,\"start\":69012},{\"end\":69029,\"start\":69026},{\"end\":69045,\"start\":69037},{\"end\":69057,\"start\":69052},{\"end\":69319,\"start\":69314},{\"end\":69333,\"start\":69326},{\"end\":69351,\"start\":69340},{\"end\":69626,\"start\":69615},{\"end\":69640,\"start\":69633},{\"end\":69659,\"start\":69654},{\"end\":69901,\"start\":69898},{\"end\":69915,\"start\":69909},{\"end\":69931,\"start\":69924},{\"end\":69942,\"start\":69939},{\"end\":70136,\"start\":70133},{\"end\":70147,\"start\":70144},{\"end\":70326,\"start\":70319},{\"end\":70342,\"start\":70336},{\"end\":70355,\"start\":70349},{\"end\":70583,\"start\":70577},{\"end\":70593,\"start\":70590},{\"end\":70607,\"start\":70603},{\"end\":70623,\"start\":70615},{\"end\":70650,\"start\":70643},{\"end\":70918,\"start\":70912},{\"end\":70927,\"start\":70922},{\"end\":70945,\"start\":70940},{\"end\":70958,\"start\":70953},{\"end\":70966,\"start\":70960},{\"end\":71167,\"start\":71162},{\"end\":71179,\"start\":71176},{\"end\":71195,\"start\":71190},{\"end\":71213,\"start\":71208},{\"end\":71230,\"start\":71222},{\"end\":71415,\"start\":71405},{\"end\":71435,\"start\":71422},{\"end\":71448,\"start\":71443},{\"end\":71457,\"start\":71455},{\"end\":71477,\"start\":71465},{\"end\":71492,\"start\":71487},{\"end\":71509,\"start\":71500},{\"end\":71524,\"start\":71518},{\"end\":71783,\"start\":71781},{\"end\":71798,\"start\":71793},{\"end\":71812,\"start\":71808},{\"end\":71823,\"start\":71819},{\"end\":71836,\"start\":71833},{\"end\":71846,\"start\":71842},{\"end\":71855,\"start\":71853},{\"end\":72129,\"start\":72123},{\"end\":72141,\"start\":72137},{\"end\":72361,\"start\":72359},{\"end\":72372,\"start\":72369},{\"end\":72385,\"start\":72381},{\"end\":72615,\"start\":72609},{\"end\":72629,\"start\":72621},{\"end\":72645,\"start\":72639},{\"end\":72666,\"start\":72653},{\"end\":72672,\"start\":72668},{\"end\":72923,\"start\":72915},{\"end\":72931,\"start\":72925},{\"end\":73125,\"start\":73115},{\"end\":73145,\"start\":73134},{\"end\":73334,\"start\":73326},{\"end\":73344,\"start\":73340},{\"end\":73351,\"start\":73349},{\"end\":73367,\"start\":73361},{\"end\":73379,\"start\":73375},{\"end\":73583,\"start\":73578},{\"end\":73598,\"start\":73593},{\"end\":73609,\"start\":73604},{\"end\":73619,\"start\":73615},{\"end\":73636,\"start\":73631},{\"end\":73645,\"start\":73642},{\"end\":73659,\"start\":73652},{\"end\":73673,\"start\":73670},{\"end\":73971,\"start\":73966},{\"end\":73984,\"start\":73981},{\"end\":73999,\"start\":73995},{\"end\":74017,\"start\":74009},{\"end\":74261,\"start\":74257},{\"end\":74274,\"start\":74271},{\"end\":74291,\"start\":74283},{\"end\":74305,\"start\":74302},{\"end\":74319,\"start\":74314},{\"end\":74332,\"start\":74328},{\"end\":74351,\"start\":74341},{\"end\":74638,\"start\":74630},{\"end\":74650,\"start\":74643},{\"end\":74837,\"start\":74832},{\"end\":74856,\"start\":74847},{\"end\":74873,\"start\":74866},{\"end\":74889,\"start\":74881},{\"end\":74910,\"start\":74897},{\"end\":74923,\"start\":74919},{\"end\":74930,\"start\":74925},{\"end\":75226,\"start\":75217},{\"end\":75242,\"start\":75234},{\"end\":75501,\"start\":75498},{\"end\":75514,\"start\":75510},{\"end\":75528,\"start\":75522},{\"end\":75791,\"start\":75784},{\"end\":75801,\"start\":75798},{\"end\":75815,\"start\":75810},{\"end\":75831,\"start\":75824},{\"end\":75843,\"start\":75839},{\"end\":75859,\"start\":75852},{\"end\":75875,\"start\":75871},{\"end\":75894,\"start\":75884},{\"end\":75905,\"start\":75903},{\"end\":75921,\"start\":75915},{\"end\":75940,\"start\":75931},{\"end\":75952,\"start\":75945},{\"end\":76348,\"start\":76345},{\"end\":76364,\"start\":76360},{\"end\":76384,\"start\":76373},{\"end\":76394,\"start\":76391},{\"end\":76405,\"start\":76402},{\"end\":76417,\"start\":76414},{\"end\":76423,\"start\":76419},{\"end\":76789,\"start\":76787},{\"end\":76802,\"start\":76796},{\"end\":76815,\"start\":76809},{\"end\":76829,\"start\":76821},{\"end\":76844,\"start\":76838},{\"end\":76858,\"start\":76855},{\"end\":77070,\"start\":77068},{\"end\":77083,\"start\":77080},{\"end\":77096,\"start\":77092},{\"end\":77110,\"start\":77105},{\"end\":77118,\"start\":77116},{\"end\":77128,\"start\":77126},{\"end\":77338,\"start\":77336},{\"end\":77346,\"start\":77343},{\"end\":77359,\"start\":77357},{\"end\":77376,\"start\":77371},{\"end\":77388,\"start\":77386},{\"end\":77399,\"start\":77394},{\"end\":77412,\"start\":77408},{\"end\":77424,\"start\":77422},{\"end\":77433,\"start\":77429},{\"end\":77443,\"start\":77440},{\"end\":77743,\"start\":77740},{\"end\":77756,\"start\":77751},{\"end\":77771,\"start\":77763},{\"end\":77961,\"start\":77958},{\"end\":77976,\"start\":77971},{\"end\":77992,\"start\":77984},{\"end\":78004,\"start\":78000},{\"end\":78019,\"start\":78013},{\"end\":78033,\"start\":78026},{\"end\":78047,\"start\":78041},{\"end\":78067,\"start\":78060},{\"end\":78343,\"start\":78340},{\"end\":78356,\"start\":78352},{\"end\":78369,\"start\":78366},{\"end\":78383,\"start\":78380},{\"end\":78642,\"start\":78639},{\"end\":78656,\"start\":78654},{\"end\":78667,\"start\":78663},{\"end\":78680,\"start\":78676},{\"end\":78695,\"start\":78691},{\"end\":78955,\"start\":78952},{\"end\":78968,\"start\":78963},{\"end\":78981,\"start\":78976},{\"end\":79004,\"start\":78991},{\"end\":79230,\"start\":79227},{\"end\":79253,\"start\":79240},{\"end\":79497,\"start\":79494},{\"end\":79508,\"start\":79504},{\"end\":79523,\"start\":79518},{\"end\":79535,\"start\":79530},{\"end\":79550,\"start\":79545},{\"end\":79561,\"start\":79557},{\"end\":79571,\"start\":79567},{\"end\":79812,\"start\":79807},{\"end\":79831,\"start\":79821},{\"end\":79850,\"start\":79839},{\"end\":79865,\"start\":79861},{\"end\":79879,\"start\":79875},{\"end\":79893,\"start\":79888},{\"end\":80157,\"start\":80151},{\"end\":80169,\"start\":80163},{\"end\":80178,\"start\":80171},{\"end\":80186,\"start\":80182},{\"end\":80207,\"start\":80200},{\"end\":80214,\"start\":80209},{\"end\":80462,\"start\":80458},{\"end\":80476,\"start\":80470},{\"end\":80484,\"start\":80478},{\"end\":80692,\"start\":80684},{\"end\":80706,\"start\":80700},{\"end\":80925,\"start\":80921},{\"end\":80940,\"start\":80933},{\"end\":80956,\"start\":80950},{\"end\":80971,\"start\":80967},{\"end\":80980,\"start\":80973},{\"end\":81203,\"start\":81197},{\"end\":81221,\"start\":81215},{\"end\":81234,\"start\":81229},{\"end\":81249,\"start\":81244},{\"end\":81589,\"start\":81583},{\"end\":81605,\"start\":81599},{\"end\":81624,\"start\":81615},{\"end\":81639,\"start\":81633},{\"end\":81654,\"start\":81650},{\"end\":81660,\"start\":81656},{\"end\":81924,\"start\":81916},{\"end\":81935,\"start\":81931},{\"end\":81949,\"start\":81942},{\"end\":81965,\"start\":81958},{\"end\":81978,\"start\":81974},{\"end\":82276,\"start\":82270},{\"end\":82286,\"start\":82282},{\"end\":82305,\"start\":82298},{\"end\":82319,\"start\":82312},{\"end\":82610,\"start\":82607},{\"end\":82619,\"start\":82617},{\"end\":82634,\"start\":82630},{\"end\":82878,\"start\":82875},{\"end\":82887,\"start\":82884},{\"end\":82900,\"start\":82897},{\"end\":83099,\"start\":83090},{\"end\":83116,\"start\":83110},{\"end\":83133,\"start\":83126},{\"end\":83153,\"start\":83142},{\"end\":83171,\"start\":83164},{\"end\":83187,\"start\":83178},{\"end\":83520,\"start\":83516},{\"end\":83537,\"start\":83532},{\"end\":83552,\"start\":83547},{\"end\":83765,\"start\":83758},{\"end\":83779,\"start\":83772},{\"end\":83792,\"start\":83786},{\"end\":83809,\"start\":83800},{\"end\":83822,\"start\":83817},{\"end\":83837,\"start\":83832},{\"end\":83852,\"start\":83846},{\"end\":83870,\"start\":83860},{\"end\":84132,\"start\":84124},{\"end\":84145,\"start\":84139},{\"end\":84159,\"start\":84153},{\"end\":84172,\"start\":84166},{\"end\":84185,\"start\":84178},{\"end\":84419,\"start\":84411},{\"end\":84437,\"start\":84430},{\"end\":84450,\"start\":84444},{\"end\":84652,\"start\":84647},{\"end\":84662,\"start\":84658},{\"end\":84676,\"start\":84670},{\"end\":84689,\"start\":84682},{\"end\":84882,\"start\":84875},{\"end\":84900,\"start\":84894},{\"end\":84913,\"start\":84907},{\"end\":84928,\"start\":84923},{\"end\":85159,\"start\":85155},{\"end\":85170,\"start\":85168},{\"end\":85186,\"start\":85182},{\"end\":85201,\"start\":85197},{\"end\":85437,\"start\":85433},{\"end\":85448,\"start\":85446},{\"end\":85464,\"start\":85460},{\"end\":85479,\"start\":85475},{\"end\":85704,\"start\":85700},{\"end\":85719,\"start\":85715},{\"end\":85919,\"start\":85915},{\"end\":85930,\"start\":85926},{\"end\":85944,\"start\":85940},{\"end\":85955,\"start\":85952},{\"end\":85966,\"start\":85964},{\"end\":86187,\"start\":86183},{\"end\":86200,\"start\":86196},{\"end\":86220,\"start\":86210},{\"end\":86238,\"start\":86227},{\"end\":86466,\"start\":86462},{\"end\":86474,\"start\":86472},{\"end\":86486,\"start\":86484},{\"end\":86496,\"start\":86492},{\"end\":86503,\"start\":86501},{\"end\":86737,\"start\":86732},{\"end\":86753,\"start\":86747},{\"end\":86769,\"start\":86762},{\"end\":86783,\"start\":86777},{\"end\":86797,\"start\":86790},{\"end\":87076,\"start\":87070},{\"end\":87086,\"start\":87078},{\"end\":87339,\"start\":87337},{\"end\":87354,\"start\":87350},{\"end\":87365,\"start\":87363},{\"end\":87375,\"start\":87371},{\"end\":87390,\"start\":87387},{\"end\":87401,\"start\":87398},{\"end\":87714,\"start\":87712},{\"end\":87724,\"start\":87720},{\"end\":87733,\"start\":87730},{\"end\":87748,\"start\":87745},{\"end\":87761,\"start\":87758},{\"end\":87983,\"start\":87981},{\"end\":87991,\"start\":87989},{\"end\":88001,\"start\":87997},{\"end\":88012,\"start\":88008},{\"end\":88021,\"start\":88019},{\"end\":88273,\"start\":88271},{\"end\":88283,\"start\":88281},{\"end\":88295,\"start\":88290},{\"end\":88310,\"start\":88307},{\"end\":88327,\"start\":88318},{\"end\":88348,\"start\":88336},{\"end\":88658,\"start\":88656},{\"end\":88671,\"start\":88667},{\"end\":88682,\"start\":88680},{\"end\":88954,\"start\":88949},{\"end\":88965,\"start\":88962},{\"end\":88979,\"start\":88973},{\"end\":88998,\"start\":88987},{\"end\":89295,\"start\":89290},{\"end\":89306,\"start\":89304},{\"end\":89318,\"start\":89316},{\"end\":89332,\"start\":89328},{\"end\":89343,\"start\":89338},{\"end\":89356,\"start\":89352},{\"end\":89368,\"start\":89364},{\"end\":89382,\"start\":89379},{\"end\":89647,\"start\":89642},{\"end\":89663,\"start\":89656},{\"end\":89673,\"start\":89671},{\"end\":89683,\"start\":89677},{\"end\":89700,\"start\":89690},{\"end\":89707,\"start\":89702},{\"end\":89909,\"start\":89904},{\"end\":89922,\"start\":89918},{\"end\":89934,\"start\":89930},{\"end\":89949,\"start\":89946},{\"end\":89962,\"start\":89958},{\"end\":90225,\"start\":90220},{\"end\":90235,\"start\":90233},{\"end\":90252,\"start\":90245},{\"end\":90563,\"start\":90555},{\"end\":90576,\"start\":90574},{\"end\":90591,\"start\":90584},{\"end\":90605,\"start\":90600},{\"end\":90619,\"start\":90612},{\"end\":90634,\"start\":90629},{\"end\":90645,\"start\":90640},{\"end\":90945,\"start\":90941},{\"end\":90983,\"start\":90957},{\"end\":91006,\"start\":90992},{\"end\":91022,\"start\":91014},{\"end\":91282,\"start\":91279},{\"end\":91298,\"start\":91294},{\"end\":91312,\"start\":91308},{\"end\":91332,\"start\":91321},{\"end\":91343,\"start\":91340},{\"end\":91357,\"start\":91351},{\"end\":91712,\"start\":91704},{\"end\":91882,\"start\":91880},{\"end\":91897,\"start\":91892},{\"end\":91911,\"start\":91907},{\"end\":91922,\"start\":91918},{\"end\":91935,\"start\":91932},{\"end\":91945,\"start\":91941},{\"end\":91954,\"start\":91952},{\"end\":92208,\"start\":92199},{\"end\":92222,\"start\":92217},{\"end\":92235,\"start\":92229},{\"end\":92251,\"start\":92244},{\"end\":92266,\"start\":92258},{\"end\":92496,\"start\":92488},{\"end\":92506,\"start\":92502},{\"end\":92513,\"start\":92511},{\"end\":92529,\"start\":92523},{\"end\":92541,\"start\":92537},{\"end\":92763,\"start\":92758},{\"end\":92776,\"start\":92773},{\"end\":92791,\"start\":92787},{\"end\":92809,\"start\":92801},{\"end\":93053,\"start\":93049},{\"end\":93066,\"start\":93063},{\"end\":93083,\"start\":93075},{\"end\":93097,\"start\":93094},{\"end\":93111,\"start\":93106},{\"end\":93124,\"start\":93120},{\"end\":93143,\"start\":93133},{\"end\":93422,\"start\":93420},{\"end\":93430,\"start\":93427},{\"end\":93443,\"start\":93441},{\"end\":93460,\"start\":93455},{\"end\":93472,\"start\":93470},{\"end\":93483,\"start\":93478},{\"end\":93496,\"start\":93492},{\"end\":93508,\"start\":93506},{\"end\":93517,\"start\":93513},{\"end\":93527,\"start\":93524},{\"end\":93827,\"start\":93824},{\"end\":93840,\"start\":93835},{\"end\":93855,\"start\":93847},{\"end\":94075,\"start\":94072},{\"end\":94088,\"start\":94084},{\"end\":94101,\"start\":94098},{\"end\":94115,\"start\":94112},{\"end\":94285,\"start\":94278},{\"end\":94301,\"start\":94293},{\"end\":94313,\"start\":94308},{\"end\":94329,\"start\":94320},{\"end\":94628,\"start\":94624},{\"end\":94642,\"start\":94636},{\"end\":94650,\"start\":94644},{\"end\":94878,\"start\":94862},{\"end\":94897,\"start\":94889},{\"end\":94906,\"start\":94899},{\"end\":95159,\"start\":95152},{\"end\":95174,\"start\":95171},{\"end\":95189,\"start\":95182},{\"end\":95204,\"start\":95198},{\"end\":95217,\"start\":95214},{\"end\":95235,\"start\":95228},{\"end\":95250,\"start\":95244},{\"end\":95265,\"start\":95259},{\"end\":95281,\"start\":95274},{\"end\":95293,\"start\":95288},{\"end\":95311,\"start\":95304},{\"end\":95327,\"start\":95318},{\"end\":95680,\"start\":95674},{\"end\":95698,\"start\":95692},{\"end\":95711,\"start\":95706},{\"end\":95726,\"start\":95721},{\"end\":96066,\"start\":96060},{\"end\":96082,\"start\":96076},{\"end\":96101,\"start\":96092},{\"end\":96116,\"start\":96110},{\"end\":96131,\"start\":96127},{\"end\":96137,\"start\":96133},{\"end\":96401,\"start\":96393},{\"end\":96412,\"start\":96408},{\"end\":96426,\"start\":96419},{\"end\":96442,\"start\":96435},{\"end\":96455,\"start\":96451},{\"end\":96706,\"start\":96697},{\"end\":96723,\"start\":96717},{\"end\":96740,\"start\":96733},{\"end\":96760,\"start\":96749},{\"end\":96778,\"start\":96771},{\"end\":96794,\"start\":96785},{\"end\":97143,\"start\":97139},{\"end\":97152,\"start\":97148},{\"end\":97161,\"start\":97158},{\"end\":97174,\"start\":97171},{\"end\":97185,\"start\":97182},{\"end\":97197,\"start\":97195},{\"end\":97209,\"start\":97207},{\"end\":97221,\"start\":97217},{\"end\":97235,\"start\":97231},{\"end\":97249,\"start\":97245},{\"end\":97573,\"start\":97571},{\"end\":97581,\"start\":97579},{\"end\":97591,\"start\":97587},{\"end\":97602,\"start\":97598},{\"end\":97611,\"start\":97609},{\"end\":97828,\"start\":97823},{\"end\":97839,\"start\":97837},{\"end\":97851,\"start\":97849},{\"end\":97865,\"start\":97861},{\"end\":97876,\"start\":97871},{\"end\":97889,\"start\":97885},{\"end\":97901,\"start\":97897},{\"end\":97915,\"start\":97912},{\"end\":98192,\"start\":98188},{\"end\":98207,\"start\":98203},{\"end\":98225,\"start\":98214},{\"end\":98468,\"start\":98464},{\"end\":98481,\"start\":98477},{\"end\":98495,\"start\":98488},{\"end\":98512,\"start\":98505},{\"end\":98527,\"start\":98522}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11933981},\"end\":66790,\"start\":66528},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3753452},\"end\":67182,\"start\":66792},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":337390},\"end\":67387,\"start\":67184},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1820089},\"end\":67688,\"start\":67389},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49667950},\"end\":68079,\"start\":67690},{\"attributes\":{\"doi\":\"arXiv:1504.00325\",\"id\":\"b5\"},\"end\":68499,\"start\":68081},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":249097750},\"end\":68927,\"start\":68501},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195791459},\"end\":69213,\"start\":68929},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202539031},\"end\":69531,\"start\":69215},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4899062},\"end\":69823,\"start\":69533},{\"attributes\":{\"id\":\"b10\"},\"end\":70085,\"start\":69825},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7900381},\"end\":70242,\"start\":70087},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":218517141},\"end\":70494,\"start\":70244},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3718988},\"end\":70837,\"start\":70496},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6095318},\"end\":71120,\"start\":70839},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":226262359},\"end\":71370,\"start\":71122},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1033682},\"end\":71710,\"start\":71372},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":222271951},\"end\":72028,\"start\":71712},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":211204968},\"end\":72278,\"start\":72030},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":201657818},\"end\":72534,\"start\":72280},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233296711},\"end\":72844,\"start\":72536},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":207596505},\"end\":73084,\"start\":72846},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1915014},\"end\":73274,\"start\":73086},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":127986954},\"end\":73510,\"start\":73276},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202537622},\"end\":73880,\"start\":73512},{\"attributes\":{\"id\":\"b25\"},\"end\":74178,\"start\":73882},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204800400},\"end\":74554,\"start\":74180},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8517067},\"end\":74773,\"start\":74556},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":244270388},\"end\":75124,\"start\":74775},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":235097487},\"end\":75399,\"start\":75126},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":218614041},\"end\":75685,\"start\":75401},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4492210},\"end\":76264,\"start\":75687},{\"attributes\":{\"id\":\"b32\"},\"end\":76726,\"start\":76266},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3147007},\"end\":77018,\"start\":76728},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207847250},\"end\":77262,\"start\":77020},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":215754208},\"end\":77660,\"start\":77264},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206771220},\"end\":77904,\"start\":77662},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14113767},\"end\":78256,\"start\":77906},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":204964783},\"end\":78538,\"start\":78258},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4100657},\"end\":78880,\"start\":78540},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3875506},\"end\":79156,\"start\":78882},{\"attributes\":{\"doi\":\"arXiv:2002.11848\",\"id\":\"b41\"},\"end\":79423,\"start\":79158},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":251018407},\"end\":79749,\"start\":79425},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":220525799},\"end\":80063,\"start\":79751},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1854889},\"end\":80398,\"start\":80065},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3541996},\"end\":80604,\"start\":80400},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4940548},\"end\":80855,\"start\":80606},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":7147309},\"end\":81133,\"start\":80857},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":222290834},\"end\":81525,\"start\":81135},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206594923},\"end\":81820,\"start\":81527},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":1456503},\"end\":82162,\"start\":81822},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":51876975},\"end\":82502,\"start\":82164},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":244430079},\"end\":82796,\"start\":82504},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":236460007},\"end\":83032,\"start\":82798},{\"attributes\":{\"doi\":\"arXiv:2107.06912\",\"id\":\"b54\"},\"end\":83416,\"start\":83034},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":221970271},\"end\":83722,\"start\":83418},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":13756489},\"end\":84052,\"start\":83724},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":2808607},\"end\":84344,\"start\":84054},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":9026666},\"end\":84586,\"start\":84346},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":208000044},\"end\":84818,\"start\":84588},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":1169492},\"end\":85068,\"start\":84820},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":220514258},\"end\":85360,\"start\":85070},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":237260163},\"end\":85630,\"start\":85362},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":85543308},\"end\":85835,\"start\":85632},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":222134104},\"end\":86123,\"start\":85837},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":221554607},\"end\":86379,\"start\":86125},{\"attributes\":{\"id\":\"b66\"},\"end\":86651,\"start\":86381},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":239024611},\"end\":86975,\"start\":86653},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":2332513},\"end\":87257,\"start\":86977},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":220665840},\"end\":87638,\"start\":87259},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":52100616},\"end\":87915,\"start\":87640},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":208548565},\"end\":88153,\"start\":87917},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":1055111},\"end\":88567,\"start\":88155},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":220045419},\"end\":88823,\"start\":88569},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":3104920},\"end\":89210,\"start\":88825},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":235692795},\"end\":89584,\"start\":89212},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":127986044},\"end\":89859,\"start\":89586},{\"attributes\":{\"doi\":\"arXiv:2110.04596\",\"id\":\"b77\"},\"end\":90139,\"start\":89861},{\"attributes\":{\"doi\":\"arXiv:2112.05917\",\"id\":\"b78\"},\"end\":90462,\"start\":90141},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":3753452},\"end\":90854,\"start\":90464},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":231636243},\"end\":91222,\"start\":90856},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":249097750},\"end\":91650,\"start\":91224},{\"attributes\":{\"id\":\"b82\"},\"end\":91809,\"start\":91652},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":222271951},\"end\":92127,\"start\":91811},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":4384334},\"end\":92436,\"start\":92129},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":127986954},\"end\":92672,\"start\":92438},{\"attributes\":{\"id\":\"b86\"},\"end\":92970,\"start\":92674},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":204800400},\"end\":93346,\"start\":92972},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":215754208},\"end\":93744,\"start\":93348},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":206771220},\"end\":93988,\"start\":93746},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":204964783},\"end\":94270,\"start\":93990},{\"attributes\":{\"doi\":\"arXiv:2202.00666\",\"id\":\"b91\"},\"end\":94564,\"start\":94272},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":3541996},\"end\":94770,\"start\":94566},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":250698884},\"end\":95074,\"start\":94772},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":231591445},\"end\":95610,\"start\":95076},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":222290834},\"end\":96002,\"start\":95612},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":206594923},\"end\":96297,\"start\":96004},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":1456503},\"end\":96639,\"start\":96299},{\"attributes\":{\"doi\":\"arXiv:2107.06912\",\"id\":\"b98\"},\"end\":97023,\"start\":96641},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":246634906},\"end\":97505,\"start\":97025},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":208548565},\"end\":97743,\"start\":97507},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":235692795},\"end\":98117,\"start\":97745},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":235446842},\"end\":98365,\"start\":98119},{\"attributes\":{\"id\":\"b103\"},\"end\":98710,\"start\":98367}]", "bib_title": "[{\"end\":66582,\"start\":66528},{\"end\":66875,\"start\":66792},{\"end\":67245,\"start\":67184},{\"end\":67462,\"start\":67389},{\"end\":67775,\"start\":67690},{\"end\":68547,\"start\":68501},{\"end\":69003,\"start\":68929},{\"end\":69300,\"start\":69215},{\"end\":69606,\"start\":69533},{\"end\":70128,\"start\":70087},{\"end\":70311,\"start\":70244},{\"end\":70568,\"start\":70496},{\"end\":70902,\"start\":70839},{\"end\":71155,\"start\":71122},{\"end\":71399,\"start\":71372},{\"end\":71772,\"start\":71712},{\"end\":72115,\"start\":72030},{\"end\":72354,\"start\":72280},{\"end\":72602,\"start\":72536},{\"end\":72911,\"start\":72846},{\"end\":73108,\"start\":73086},{\"end\":73320,\"start\":73276},{\"end\":73571,\"start\":73512},{\"end\":74248,\"start\":74180},{\"end\":74621,\"start\":74556},{\"end\":74824,\"start\":74775},{\"end\":75208,\"start\":75126},{\"end\":75486,\"start\":75401},{\"end\":75775,\"start\":75687},{\"end\":76335,\"start\":76266},{\"end\":76779,\"start\":76728},{\"end\":77059,\"start\":77020},{\"end\":77327,\"start\":77264},{\"end\":77729,\"start\":77662},{\"end\":77947,\"start\":77906},{\"end\":78332,\"start\":78258},{\"end\":78631,\"start\":78540},{\"end\":78942,\"start\":78882},{\"end\":79484,\"start\":79425},{\"end\":79790,\"start\":79751},{\"end\":80144,\"start\":80065},{\"end\":80454,\"start\":80400},{\"end\":80672,\"start\":80606},{\"end\":80911,\"start\":80857},{\"end\":81189,\"start\":81135},{\"end\":81579,\"start\":81527},{\"end\":81909,\"start\":81822},{\"end\":82261,\"start\":82164},{\"end\":82599,\"start\":82504},{\"end\":82868,\"start\":82798},{\"end\":83507,\"start\":83418},{\"end\":83749,\"start\":83724},{\"end\":84110,\"start\":84054},{\"end\":84397,\"start\":84346},{\"end\":84639,\"start\":84588},{\"end\":84867,\"start\":84820},{\"end\":85146,\"start\":85070},{\"end\":85424,\"start\":85362},{\"end\":85688,\"start\":85632},{\"end\":85906,\"start\":85837},{\"end\":86176,\"start\":86125},{\"end\":86453,\"start\":86381},{\"end\":86724,\"start\":86653},{\"end\":87066,\"start\":86977},{\"end\":87331,\"start\":87259},{\"end\":87704,\"start\":87640},{\"end\":87970,\"start\":87917},{\"end\":88262,\"start\":88155},{\"end\":88647,\"start\":88569},{\"end\":88941,\"start\":88825},{\"end\":89278,\"start\":89212},{\"end\":89633,\"start\":89586},{\"end\":90547,\"start\":90464},{\"end\":90932,\"start\":90856},{\"end\":91270,\"start\":91224},{\"end\":91871,\"start\":91811},{\"end\":92187,\"start\":92129},{\"end\":92482,\"start\":92438},{\"end\":93040,\"start\":92972},{\"end\":93411,\"start\":93348},{\"end\":93813,\"start\":93746},{\"end\":94064,\"start\":93990},{\"end\":94620,\"start\":94566},{\"end\":94851,\"start\":94772},{\"end\":95145,\"start\":95076},{\"end\":95666,\"start\":95612},{\"end\":96056,\"start\":96004},{\"end\":96386,\"start\":96299},{\"end\":97132,\"start\":97025},{\"end\":97560,\"start\":97507},{\"end\":97811,\"start\":97745},{\"end\":98181,\"start\":98119}]", "bib_author": "[{\"end\":66600,\"start\":66584},{\"end\":66617,\"start\":66600},{\"end\":66631,\"start\":66617},{\"end\":66646,\"start\":66631},{\"end\":66893,\"start\":66877},{\"end\":66906,\"start\":66893},{\"end\":66921,\"start\":66906},{\"end\":66935,\"start\":66921},{\"end\":66949,\"start\":66935},{\"end\":66964,\"start\":66949},{\"end\":66975,\"start\":66964},{\"end\":67262,\"start\":67247},{\"end\":67273,\"start\":67262},{\"end\":67477,\"start\":67464},{\"end\":67492,\"start\":67477},{\"end\":67508,\"start\":67492},{\"end\":67522,\"start\":67508},{\"end\":67792,\"start\":67777},{\"end\":67809,\"start\":67792},{\"end\":67823,\"start\":67809},{\"end\":67834,\"start\":67823},{\"end\":67848,\"start\":67834},{\"end\":67860,\"start\":67848},{\"end\":67871,\"start\":67860},{\"end\":68158,\"start\":68145},{\"end\":68168,\"start\":68158},{\"end\":68182,\"start\":68168},{\"end\":68204,\"start\":68182},{\"end\":68219,\"start\":68204},{\"end\":68233,\"start\":68219},{\"end\":68253,\"start\":68233},{\"end\":68561,\"start\":68549},{\"end\":68577,\"start\":68561},{\"end\":68591,\"start\":68577},{\"end\":68611,\"start\":68591},{\"end\":68622,\"start\":68611},{\"end\":68636,\"start\":68622},{\"end\":69021,\"start\":69005},{\"end\":69031,\"start\":69021},{\"end\":69047,\"start\":69031},{\"end\":69059,\"start\":69047},{\"end\":69321,\"start\":69302},{\"end\":69335,\"start\":69321},{\"end\":69353,\"start\":69335},{\"end\":69628,\"start\":69608},{\"end\":69642,\"start\":69628},{\"end\":69661,\"start\":69642},{\"end\":69903,\"start\":69895},{\"end\":69917,\"start\":69903},{\"end\":69933,\"start\":69917},{\"end\":69944,\"start\":69933},{\"end\":70138,\"start\":70130},{\"end\":70149,\"start\":70138},{\"end\":70328,\"start\":70313},{\"end\":70344,\"start\":70328},{\"end\":70357,\"start\":70344},{\"end\":70585,\"start\":70570},{\"end\":70595,\"start\":70585},{\"end\":70609,\"start\":70595},{\"end\":70625,\"start\":70609},{\"end\":70652,\"start\":70625},{\"end\":70920,\"start\":70904},{\"end\":70929,\"start\":70920},{\"end\":70947,\"start\":70929},{\"end\":70960,\"start\":70947},{\"end\":70968,\"start\":70960},{\"end\":71169,\"start\":71157},{\"end\":71181,\"start\":71169},{\"end\":71197,\"start\":71181},{\"end\":71215,\"start\":71197},{\"end\":71232,\"start\":71215},{\"end\":71417,\"start\":71401},{\"end\":71437,\"start\":71417},{\"end\":71450,\"start\":71437},{\"end\":71459,\"start\":71450},{\"end\":71479,\"start\":71459},{\"end\":71494,\"start\":71479},{\"end\":71511,\"start\":71494},{\"end\":71526,\"start\":71511},{\"end\":71785,\"start\":71774},{\"end\":71800,\"start\":71785},{\"end\":71814,\"start\":71800},{\"end\":71825,\"start\":71814},{\"end\":71838,\"start\":71825},{\"end\":71848,\"start\":71838},{\"end\":71857,\"start\":71848},{\"end\":72131,\"start\":72117},{\"end\":72143,\"start\":72131},{\"end\":72363,\"start\":72356},{\"end\":72374,\"start\":72363},{\"end\":72387,\"start\":72374},{\"end\":72617,\"start\":72604},{\"end\":72631,\"start\":72617},{\"end\":72647,\"start\":72631},{\"end\":72668,\"start\":72647},{\"end\":72674,\"start\":72668},{\"end\":72925,\"start\":72913},{\"end\":72933,\"start\":72925},{\"end\":73127,\"start\":73110},{\"end\":73147,\"start\":73127},{\"end\":73336,\"start\":73322},{\"end\":73346,\"start\":73336},{\"end\":73353,\"start\":73346},{\"end\":73369,\"start\":73353},{\"end\":73381,\"start\":73369},{\"end\":73585,\"start\":73573},{\"end\":73600,\"start\":73585},{\"end\":73611,\"start\":73600},{\"end\":73621,\"start\":73611},{\"end\":73638,\"start\":73621},{\"end\":73647,\"start\":73638},{\"end\":73661,\"start\":73647},{\"end\":73675,\"start\":73661},{\"end\":73973,\"start\":73958},{\"end\":73986,\"start\":73973},{\"end\":74001,\"start\":73986},{\"end\":74019,\"start\":74001},{\"end\":74263,\"start\":74250},{\"end\":74276,\"start\":74263},{\"end\":74293,\"start\":74276},{\"end\":74307,\"start\":74293},{\"end\":74321,\"start\":74307},{\"end\":74334,\"start\":74321},{\"end\":74353,\"start\":74334},{\"end\":74640,\"start\":74623},{\"end\":74652,\"start\":74640},{\"end\":74839,\"start\":74826},{\"end\":74858,\"start\":74839},{\"end\":74875,\"start\":74858},{\"end\":74891,\"start\":74875},{\"end\":74912,\"start\":74891},{\"end\":74925,\"start\":74912},{\"end\":74932,\"start\":74925},{\"end\":75228,\"start\":75210},{\"end\":75244,\"start\":75228},{\"end\":75503,\"start\":75488},{\"end\":75516,\"start\":75503},{\"end\":75530,\"start\":75516},{\"end\":75793,\"start\":75777},{\"end\":75803,\"start\":75793},{\"end\":75817,\"start\":75803},{\"end\":75833,\"start\":75817},{\"end\":75845,\"start\":75833},{\"end\":75861,\"start\":75845},{\"end\":75877,\"start\":75861},{\"end\":75896,\"start\":75877},{\"end\":75907,\"start\":75896},{\"end\":75923,\"start\":75907},{\"end\":75942,\"start\":75923},{\"end\":75954,\"start\":75942},{\"end\":76350,\"start\":76337},{\"end\":76366,\"start\":76350},{\"end\":76386,\"start\":76366},{\"end\":76396,\"start\":76386},{\"end\":76407,\"start\":76396},{\"end\":76419,\"start\":76407},{\"end\":76425,\"start\":76419},{\"end\":76791,\"start\":76781},{\"end\":76804,\"start\":76791},{\"end\":76817,\"start\":76804},{\"end\":76831,\"start\":76817},{\"end\":76846,\"start\":76831},{\"end\":76860,\"start\":76846},{\"end\":77072,\"start\":77061},{\"end\":77085,\"start\":77072},{\"end\":77098,\"start\":77085},{\"end\":77112,\"start\":77098},{\"end\":77120,\"start\":77112},{\"end\":77130,\"start\":77120},{\"end\":77340,\"start\":77329},{\"end\":77348,\"start\":77340},{\"end\":77361,\"start\":77348},{\"end\":77378,\"start\":77361},{\"end\":77390,\"start\":77378},{\"end\":77401,\"start\":77390},{\"end\":77414,\"start\":77401},{\"end\":77426,\"start\":77414},{\"end\":77435,\"start\":77426},{\"end\":77445,\"start\":77435},{\"end\":77745,\"start\":77731},{\"end\":77758,\"start\":77745},{\"end\":77773,\"start\":77758},{\"end\":77963,\"start\":77949},{\"end\":77978,\"start\":77963},{\"end\":77994,\"start\":77978},{\"end\":78006,\"start\":77994},{\"end\":78021,\"start\":78006},{\"end\":78035,\"start\":78021},{\"end\":78049,\"start\":78035},{\"end\":78069,\"start\":78049},{\"end\":78345,\"start\":78334},{\"end\":78358,\"start\":78345},{\"end\":78371,\"start\":78358},{\"end\":78385,\"start\":78371},{\"end\":78644,\"start\":78633},{\"end\":78658,\"start\":78644},{\"end\":78669,\"start\":78658},{\"end\":78682,\"start\":78669},{\"end\":78697,\"start\":78682},{\"end\":78957,\"start\":78944},{\"end\":78970,\"start\":78957},{\"end\":78983,\"start\":78970},{\"end\":79006,\"start\":78983},{\"end\":79232,\"start\":79219},{\"end\":79255,\"start\":79232},{\"end\":79499,\"start\":79486},{\"end\":79510,\"start\":79499},{\"end\":79525,\"start\":79510},{\"end\":79537,\"start\":79525},{\"end\":79552,\"start\":79537},{\"end\":79563,\"start\":79552},{\"end\":79573,\"start\":79563},{\"end\":79814,\"start\":79792},{\"end\":79833,\"start\":79814},{\"end\":79852,\"start\":79833},{\"end\":79867,\"start\":79852},{\"end\":79881,\"start\":79867},{\"end\":79895,\"start\":79881},{\"end\":80159,\"start\":80146},{\"end\":80171,\"start\":80159},{\"end\":80180,\"start\":80171},{\"end\":80188,\"start\":80180},{\"end\":80209,\"start\":80188},{\"end\":80216,\"start\":80209},{\"end\":80464,\"start\":80456},{\"end\":80478,\"start\":80464},{\"end\":80486,\"start\":80478},{\"end\":80694,\"start\":80674},{\"end\":80708,\"start\":80694},{\"end\":80927,\"start\":80913},{\"end\":80942,\"start\":80927},{\"end\":80958,\"start\":80942},{\"end\":80973,\"start\":80958},{\"end\":80982,\"start\":80973},{\"end\":81205,\"start\":81191},{\"end\":81223,\"start\":81205},{\"end\":81236,\"start\":81223},{\"end\":81251,\"start\":81236},{\"end\":81591,\"start\":81581},{\"end\":81607,\"start\":81591},{\"end\":81626,\"start\":81607},{\"end\":81641,\"start\":81626},{\"end\":81656,\"start\":81641},{\"end\":81662,\"start\":81656},{\"end\":81926,\"start\":81911},{\"end\":81937,\"start\":81926},{\"end\":81951,\"start\":81937},{\"end\":81967,\"start\":81951},{\"end\":81980,\"start\":81967},{\"end\":82278,\"start\":82263},{\"end\":82288,\"start\":82278},{\"end\":82307,\"start\":82288},{\"end\":82321,\"start\":82307},{\"end\":82612,\"start\":82601},{\"end\":82621,\"start\":82612},{\"end\":82636,\"start\":82621},{\"end\":82880,\"start\":82870},{\"end\":82889,\"start\":82880},{\"end\":82902,\"start\":82889},{\"end\":83101,\"start\":83083},{\"end\":83118,\"start\":83101},{\"end\":83135,\"start\":83118},{\"end\":83155,\"start\":83135},{\"end\":83173,\"start\":83155},{\"end\":83189,\"start\":83173},{\"end\":83522,\"start\":83509},{\"end\":83539,\"start\":83522},{\"end\":83554,\"start\":83539},{\"end\":83767,\"start\":83751},{\"end\":83781,\"start\":83767},{\"end\":83794,\"start\":83781},{\"end\":83811,\"start\":83794},{\"end\":83824,\"start\":83811},{\"end\":83839,\"start\":83824},{\"end\":83854,\"start\":83839},{\"end\":83872,\"start\":83854},{\"end\":84134,\"start\":84112},{\"end\":84147,\"start\":84134},{\"end\":84161,\"start\":84147},{\"end\":84174,\"start\":84161},{\"end\":84187,\"start\":84174},{\"end\":84421,\"start\":84399},{\"end\":84439,\"start\":84421},{\"end\":84452,\"start\":84439},{\"end\":84654,\"start\":84641},{\"end\":84664,\"start\":84654},{\"end\":84678,\"start\":84664},{\"end\":84691,\"start\":84678},{\"end\":84884,\"start\":84869},{\"end\":84902,\"start\":84884},{\"end\":84915,\"start\":84902},{\"end\":84930,\"start\":84915},{\"end\":85161,\"start\":85148},{\"end\":85172,\"start\":85161},{\"end\":85188,\"start\":85172},{\"end\":85203,\"start\":85188},{\"end\":85439,\"start\":85426},{\"end\":85450,\"start\":85439},{\"end\":85466,\"start\":85450},{\"end\":85481,\"start\":85466},{\"end\":85706,\"start\":85690},{\"end\":85721,\"start\":85706},{\"end\":85921,\"start\":85908},{\"end\":85932,\"start\":85921},{\"end\":85946,\"start\":85932},{\"end\":85957,\"start\":85946},{\"end\":85968,\"start\":85957},{\"end\":86189,\"start\":86178},{\"end\":86202,\"start\":86189},{\"end\":86222,\"start\":86202},{\"end\":86240,\"start\":86222},{\"end\":86468,\"start\":86455},{\"end\":86476,\"start\":86468},{\"end\":86488,\"start\":86476},{\"end\":86498,\"start\":86488},{\"end\":86505,\"start\":86498},{\"end\":86739,\"start\":86726},{\"end\":86755,\"start\":86739},{\"end\":86771,\"start\":86755},{\"end\":86785,\"start\":86771},{\"end\":86799,\"start\":86785},{\"end\":87078,\"start\":87068},{\"end\":87088,\"start\":87078},{\"end\":87341,\"start\":87333},{\"end\":87356,\"start\":87341},{\"end\":87367,\"start\":87356},{\"end\":87377,\"start\":87367},{\"end\":87392,\"start\":87377},{\"end\":87403,\"start\":87392},{\"end\":87716,\"start\":87706},{\"end\":87726,\"start\":87716},{\"end\":87735,\"start\":87726},{\"end\":87750,\"start\":87735},{\"end\":87763,\"start\":87750},{\"end\":87985,\"start\":87972},{\"end\":87993,\"start\":87985},{\"end\":88003,\"start\":87993},{\"end\":88014,\"start\":88003},{\"end\":88023,\"start\":88014},{\"end\":88275,\"start\":88264},{\"end\":88285,\"start\":88275},{\"end\":88297,\"start\":88285},{\"end\":88312,\"start\":88297},{\"end\":88329,\"start\":88312},{\"end\":88350,\"start\":88329},{\"end\":88660,\"start\":88649},{\"end\":88673,\"start\":88660},{\"end\":88684,\"start\":88673},{\"end\":88956,\"start\":88943},{\"end\":88967,\"start\":88956},{\"end\":88981,\"start\":88967},{\"end\":89000,\"start\":88981},{\"end\":89297,\"start\":89280},{\"end\":89308,\"start\":89297},{\"end\":89320,\"start\":89308},{\"end\":89334,\"start\":89320},{\"end\":89345,\"start\":89334},{\"end\":89358,\"start\":89345},{\"end\":89370,\"start\":89358},{\"end\":89384,\"start\":89370},{\"end\":89649,\"start\":89635},{\"end\":89665,\"start\":89649},{\"end\":89675,\"start\":89665},{\"end\":89685,\"start\":89675},{\"end\":89702,\"start\":89685},{\"end\":89709,\"start\":89702},{\"end\":89911,\"start\":89898},{\"end\":89924,\"start\":89911},{\"end\":89936,\"start\":89924},{\"end\":89951,\"start\":89936},{\"end\":89964,\"start\":89951},{\"end\":90227,\"start\":90210},{\"end\":90237,\"start\":90227},{\"end\":90254,\"start\":90237},{\"end\":90565,\"start\":90549},{\"end\":90578,\"start\":90565},{\"end\":90593,\"start\":90578},{\"end\":90607,\"start\":90593},{\"end\":90621,\"start\":90607},{\"end\":90636,\"start\":90621},{\"end\":90647,\"start\":90636},{\"end\":90947,\"start\":90934},{\"end\":90985,\"start\":90947},{\"end\":91008,\"start\":90985},{\"end\":91024,\"start\":91008},{\"end\":91284,\"start\":91272},{\"end\":91300,\"start\":91284},{\"end\":91314,\"start\":91300},{\"end\":91334,\"start\":91314},{\"end\":91345,\"start\":91334},{\"end\":91359,\"start\":91345},{\"end\":91714,\"start\":91693},{\"end\":91884,\"start\":91873},{\"end\":91899,\"start\":91884},{\"end\":91913,\"start\":91899},{\"end\":91924,\"start\":91913},{\"end\":91937,\"start\":91924},{\"end\":91947,\"start\":91937},{\"end\":91956,\"start\":91947},{\"end\":92210,\"start\":92189},{\"end\":92224,\"start\":92210},{\"end\":92237,\"start\":92224},{\"end\":92253,\"start\":92237},{\"end\":92268,\"start\":92253},{\"end\":92498,\"start\":92484},{\"end\":92508,\"start\":92498},{\"end\":92515,\"start\":92508},{\"end\":92531,\"start\":92515},{\"end\":92543,\"start\":92531},{\"end\":92765,\"start\":92750},{\"end\":92778,\"start\":92765},{\"end\":92793,\"start\":92778},{\"end\":92811,\"start\":92793},{\"end\":93055,\"start\":93042},{\"end\":93068,\"start\":93055},{\"end\":93085,\"start\":93068},{\"end\":93099,\"start\":93085},{\"end\":93113,\"start\":93099},{\"end\":93126,\"start\":93113},{\"end\":93145,\"start\":93126},{\"end\":93424,\"start\":93413},{\"end\":93432,\"start\":93424},{\"end\":93445,\"start\":93432},{\"end\":93462,\"start\":93445},{\"end\":93474,\"start\":93462},{\"end\":93485,\"start\":93474},{\"end\":93498,\"start\":93485},{\"end\":93510,\"start\":93498},{\"end\":93519,\"start\":93510},{\"end\":93529,\"start\":93519},{\"end\":93829,\"start\":93815},{\"end\":93842,\"start\":93829},{\"end\":93857,\"start\":93842},{\"end\":94077,\"start\":94066},{\"end\":94090,\"start\":94077},{\"end\":94103,\"start\":94090},{\"end\":94117,\"start\":94103},{\"end\":94287,\"start\":94272},{\"end\":94303,\"start\":94287},{\"end\":94315,\"start\":94303},{\"end\":94331,\"start\":94315},{\"end\":94630,\"start\":94622},{\"end\":94644,\"start\":94630},{\"end\":94652,\"start\":94644},{\"end\":94880,\"start\":94853},{\"end\":94899,\"start\":94880},{\"end\":94908,\"start\":94899},{\"end\":95161,\"start\":95147},{\"end\":95176,\"start\":95161},{\"end\":95191,\"start\":95176},{\"end\":95206,\"start\":95191},{\"end\":95219,\"start\":95206},{\"end\":95237,\"start\":95219},{\"end\":95252,\"start\":95237},{\"end\":95267,\"start\":95252},{\"end\":95283,\"start\":95267},{\"end\":95295,\"start\":95283},{\"end\":95313,\"start\":95295},{\"end\":95329,\"start\":95313},{\"end\":95682,\"start\":95668},{\"end\":95700,\"start\":95682},{\"end\":95713,\"start\":95700},{\"end\":95728,\"start\":95713},{\"end\":96068,\"start\":96058},{\"end\":96084,\"start\":96068},{\"end\":96103,\"start\":96084},{\"end\":96118,\"start\":96103},{\"end\":96133,\"start\":96118},{\"end\":96139,\"start\":96133},{\"end\":96403,\"start\":96388},{\"end\":96414,\"start\":96403},{\"end\":96428,\"start\":96414},{\"end\":96444,\"start\":96428},{\"end\":96457,\"start\":96444},{\"end\":96708,\"start\":96690},{\"end\":96725,\"start\":96708},{\"end\":96742,\"start\":96725},{\"end\":96762,\"start\":96742},{\"end\":96780,\"start\":96762},{\"end\":96796,\"start\":96780},{\"end\":97145,\"start\":97134},{\"end\":97154,\"start\":97145},{\"end\":97163,\"start\":97154},{\"end\":97176,\"start\":97163},{\"end\":97187,\"start\":97176},{\"end\":97199,\"start\":97187},{\"end\":97211,\"start\":97199},{\"end\":97223,\"start\":97211},{\"end\":97237,\"start\":97223},{\"end\":97251,\"start\":97237},{\"end\":97575,\"start\":97562},{\"end\":97583,\"start\":97575},{\"end\":97593,\"start\":97583},{\"end\":97604,\"start\":97593},{\"end\":97613,\"start\":97604},{\"end\":97830,\"start\":97813},{\"end\":97841,\"start\":97830},{\"end\":97853,\"start\":97841},{\"end\":97867,\"start\":97853},{\"end\":97878,\"start\":97867},{\"end\":97891,\"start\":97878},{\"end\":97903,\"start\":97891},{\"end\":97917,\"start\":97903},{\"end\":98194,\"start\":98183},{\"end\":98209,\"start\":98194},{\"end\":98227,\"start\":98209},{\"end\":98470,\"start\":98458},{\"end\":98483,\"start\":98470},{\"end\":98497,\"start\":98483},{\"end\":98514,\"start\":98497},{\"end\":98529,\"start\":98514}]", "bib_venue": "[{\"end\":66650,\"start\":66646},{\"end\":66979,\"start\":66975},{\"end\":67278,\"start\":67273},{\"end\":67532,\"start\":67522},{\"end\":67875,\"start\":67871},{\"end\":68143,\"start\":68081},{\"end\":68705,\"start\":68636},{\"end\":69063,\"start\":69059},{\"end\":69365,\"start\":69353},{\"end\":69670,\"start\":69661},{\"end\":69893,\"start\":69825},{\"end\":70156,\"start\":70149},{\"end\":70360,\"start\":70357},{\"end\":70661,\"start\":70652},{\"end\":70972,\"start\":70968},{\"end\":71237,\"start\":71232},{\"end\":71533,\"start\":71526},{\"end\":71862,\"start\":71857},{\"end\":72147,\"start\":72143},{\"end\":72399,\"start\":72387},{\"end\":72679,\"start\":72674},{\"end\":72951,\"start\":72933},{\"end\":73165,\"start\":73147},{\"end\":73385,\"start\":73381},{\"end\":73687,\"start\":73675},{\"end\":73956,\"start\":73882},{\"end\":74357,\"start\":74353},{\"end\":74656,\"start\":74652},{\"end\":74941,\"start\":74932},{\"end\":75253,\"start\":75244},{\"end\":75533,\"start\":75530},{\"end\":75958,\"start\":75954},{\"end\":76487,\"start\":76425},{\"end\":76865,\"start\":76860},{\"end\":77133,\"start\":77130},{\"end\":77449,\"start\":77445},{\"end\":77777,\"start\":77773},{\"end\":78073,\"start\":78069},{\"end\":78389,\"start\":78385},{\"end\":78701,\"start\":78697},{\"end\":79010,\"start\":79006},{\"end\":79217,\"start\":79158},{\"end\":79579,\"start\":79573},{\"end\":79899,\"start\":79895},{\"end\":80220,\"start\":80216},{\"end\":80495,\"start\":80486},{\"end\":80722,\"start\":80708},{\"end\":80989,\"start\":80982},{\"end\":81320,\"start\":81251},{\"end\":81666,\"start\":81662},{\"end\":81984,\"start\":81980},{\"end\":82324,\"start\":82321},{\"end\":82640,\"start\":82636},{\"end\":82905,\"start\":82902},{\"end\":83081,\"start\":83034},{\"end\":83561,\"start\":83554},{\"end\":83879,\"start\":83872},{\"end\":84191,\"start\":84187},{\"end\":84456,\"start\":84452},{\"end\":84695,\"start\":84691},{\"end\":84934,\"start\":84930},{\"end\":85207,\"start\":85203},{\"end\":85487,\"start\":85481},{\"end\":85725,\"start\":85721},{\"end\":85972,\"start\":85968},{\"end\":86244,\"start\":86240},{\"end\":86510,\"start\":86505},{\"end\":86804,\"start\":86799},{\"end\":87104,\"start\":87088},{\"end\":87434,\"start\":87403},{\"end\":87771,\"start\":87763},{\"end\":88027,\"start\":88023},{\"end\":88354,\"start\":88350},{\"end\":88687,\"start\":88684},{\"end\":89004,\"start\":89000},{\"end\":89388,\"start\":89384},{\"end\":89713,\"start\":89709},{\"end\":89896,\"start\":89861},{\"end\":90208,\"start\":90141},{\"end\":90651,\"start\":90647},{\"end\":91028,\"start\":91024},{\"end\":91428,\"start\":91359},{\"end\":91691,\"start\":91652},{\"end\":91961,\"start\":91956},{\"end\":92275,\"start\":92268},{\"end\":92547,\"start\":92543},{\"end\":92748,\"start\":92674},{\"end\":93149,\"start\":93145},{\"end\":93533,\"start\":93529},{\"end\":93861,\"start\":93857},{\"end\":94121,\"start\":94117},{\"end\":94395,\"start\":94347},{\"end\":94661,\"start\":94652},{\"end\":94912,\"start\":94908},{\"end\":95333,\"start\":95329},{\"end\":95797,\"start\":95728},{\"end\":96143,\"start\":96139},{\"end\":96461,\"start\":96457},{\"end\":96688,\"start\":96641},{\"end\":97255,\"start\":97251},{\"end\":97617,\"start\":97613},{\"end\":97921,\"start\":97917},{\"end\":98231,\"start\":98227},{\"end\":98456,\"start\":98367}]"}}}, "year": 2023, "month": 12, "day": 17}