{"id": 2454882, "updated": "2023-09-28 23:59:25.941", "metadata": {"title": "Deal or No Deal? End-to-End Learning for Negotiation Dialogues", "authors": "[{\"first\":\"Mike\",\"last\":\"Lewis\",\"middle\":[]},{\"first\":\"Denis\",\"last\":\"Yarats\",\"middle\":[]},{\"first\":\"Yann\",\"last\":\"Dauphin\",\"middle\":[\"N.\"]},{\"first\":\"Devi\",\"last\":\"Parikh\",\"middle\":[]},{\"first\":\"Dhruv\",\"last\":\"Batra\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2017, "month": 6, "day": 16}, "abstract": "Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1706.05125", "mag": "2962852262", "acl": "D17-1259", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/LewisYDPB17", "doi": "10.18653/v1/d17-1259"}}, "content": {"source": {"pdf_hash": "618c1ca0794847f365f9ec1b2ac49f20cb274863", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1706.05125v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D17-1259.pdf", "status": "HYBRID"}}, "grobid": {"id": "c5cc08661d4d17dec681808d43ec0fb0b60d985d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/618c1ca0794847f365f9ec1b2ac49f20cb274863.txt", "contents": "\nDeal or No Deal? End-to-End Learning for Negotiation Dialogues\n16 Jun 2017\n\nMike Lewis mikelewis@fb.com \nFacebook AI Research\n\n\nDenis Yarats denisy@fb.com \nFacebook AI Research\n\n\nYann N Dauphin \nFacebook AI Research\n\n\nDevi Parikh parikh@gatech.edu \nFacebook AI Research\n\n\nGeorgia Institute of Technology\n\n\nDhruv Batra dbatra@gatech.edu \nFacebook AI Research\n\n\nGeorgia Institute of Technology\n\n\nDeal or No Deal? End-to-End Learning for Negotiation Dialogues\n16 Jun 2017\nMuch of human dialogue occurs in semicooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available. 1\n\nIntroduction\n\nIntelligent agents often need to cooperate with others who have different goals, and typically use natural language to agree on decisions. Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realised. Such dialogues contain both cooperative and adversarial elements, and require agents to understand, plan, and generate utterances to achieve their goals (Traum et al., 2008;Asher et al., 2012).\n\nWe collect the first large dataset of natural language negotiations between two people, and show 1 https://github.com/facebookresearch/end-to-end-negotiator that end-to-end neural models can be trained to negotiate by maximizing the likelihood of human actions. This approach is scalable and domainindependent, but does not model the strategic skills required for negotiating well. We further show that models can be improved by training and decoding to maximize reward instead of likelihood-by training with self-play reinforcement learning, and using rollouts to estimate the expected reward of utterances during decoding.\n\nTo study semi-cooperative dialogue, we gather a dataset of 5808 dialogues between humans on a negotiation task. Users were shown a set of items with a value for each, and asked to agree how to divide the items with another user who has a different, unseen, value function ( Figure 1).\n\nWe first train recurrent neural networks to imitate human actions. We find that models trained to maximise the likelihood of human utterances can generate fluent language, but make comparatively poor negotiators, which are overly willing to compromise. We therefore explore two methods for improving the model's strategic reasoning skillsboth of which attempt to optimise for the agent's goals, rather than simply imitating humans:\n\nFirstly, instead of training to optimise likelihood, we show that our agents can be considerably improved using self play, in which pre-trained models practice negotiating with each other in order to optimise performance. To avoid the models diverging from human language, we interleave reinforcement learning updates with supervised updates. For the first time, we show that end-toend dialogue agents trained using reinforcement learning outperform their supervised counterparts in negotiations with humans.\n\nSecondly, we introduce a new form of planning for dialogue called dialogue rollouts, in which an agent simulates complete dialogues during decoding to estimate the reward of utterances. We show Figure 1: A dialogue in our Mechanical Turk interface, which we used to collect a negotiation dataset. that decoding to maximise the reward function (rather than likelihood) significantly improves performance against both humans and machines.\n\nAnalysing the performance of our agents, we find evidence of sophisticated negotiation strategies. For example, we find instances of the model feigning interest in a valueless issue, so that it can later 'compromise' by conceding it. Deceit is a complex skill that requires hypothesising the other agent's beliefs, and is learnt relatively late in child development (Talwar and Lee, 2002). Our agents have learnt to deceive without any explicit human design, simply by trying to achieve their goals.\n\nThe rest of the paper proceeds as follows: \u00a72 describes the collection of a large dataset of humanhuman negotiation dialogues. \u00a73 describes a baseline supervised model, which we then show can be improved by goal-based training ( \u00a74) and decoding ( \u00a75). \u00a76 measures the performance of our models and humans on this task, and \u00a77 gives a detailed analysis and suggests future directions.\n\n\nData Collection\n\n\nOverview\n\nTo enable end-to-end training of negotiation agents, we first develop a novel negotiation task and curate a dataset of human-human dialogues for this task. This task and dataset follow our proposed general framework for studying semicooperative dialogue. Initially, each agent is shown an input specifying a space of possible actions and a reward function which will score the outcome of the negotiation. Agents then sequentially take turns of either sending natural language messages, or selecting that a final decision has been reached. When one agent selects that an agreement has been made, both agents independently output what they think the agreed decision was. If conflicting decisions are made, both agents are given zero reward.\n\n\nTask\n\nOur task is an instance of multi issue bargaining (Fershtman, 1990), and is based on DeVault et al. (2015). Two agents are both shown the same collection of items, and instructed to divide them so that each item assigned to one agent.\n\nEach agent is given a different randomly generated value function, which gives a non-negative value for each item. The value functions are constrained so that: (1) the total value for a user of all items is 10; (2) each item has non-zero value to at least one user; and (3) some items have nonzero value to both users. These constraints enforce that it is not possible for both agents to receive a maximum score, and that no item is worthless to both agents, so the negotiation will be competitive. After 10 turns, we allow agents the option to complete the negotiation with no agreement, which is worth 0 points to both users. We use 3 item types (books, hats, balls), and between 5 and 7 total items in the pool. Figure 1 shows our interface.\n\n\nData Collection\n\nWe collected a set of human-human dialogues using Amazon Mechanical Turk. Workers were paid $0.15 per dialogue, with a $0.05 bonus for maximal scores. We only used workers based in the United States with a 95% approval rating and at least 5000 previous HITs. Our data collection interface was adapted from that of Das et al. (2016).\n\nWe collected a total of 5808 dialogues, based on 2236 unique scenarios (where a scenario is the  The perspectives differ on their input goals, output choice, and in special tokens marking whether a statement was read or written. We train conditional language models to predict the dialogue given the input, and additional models to predict the output given the dialogue.\n\navailable items and values for the two users). We held out a test set of 252 scenarios (526 dialogues). Holding out test scenarios means that models must generalise to new situations.\n\n\nLikelihood Model\n\nWe propose a simple but effective baseline model for the conversational agent, in which a sequenceto-sequence model is trained to produce the complete dialogue, conditioned on an agent's input.\n\n\nData Representation\n\nEach dialogue is converted into two training examples, showing the complete conversation from the perspective of each agent. The examples differ on their input goals, output choice, and whether utterances were read or written.\n\nTraining examples contain an input goal g, specifying the available items and their values, a dialogue x, and an output decision o specifying which items each agent will receive. Specifically, we represent g as a list of six integers corresponding to the count and value of each of the three item types. Dialogue x is a list of tokens x 0..T containing the turns of each agent interleaved with symbols marking whether a turn was written by the agent or their partner, terminating in a special token indicating one agent has marked that an agree-ment has been made. Output o is six integers describing how many of each of the three item types are assigned to each agent. See Figure 2.\n\n\nSupervised Learning\n\nWe train a sequence-to-sequence network to generate an agent's perspective of the dialogue conditioned on the agent's input goals ( Figure 3a).\n\nThe model uses 4 recurrent neural networks, implemented as GRUs \n: GRU w , GRU g , GRU\u2212 \u2192 o , and GRU\u2190 \u2212 o .\nThe agent's input goals g are encoded using GRU g . We refer to the final hidden state as h g . The model then predicts each token x t from left to right, conditioned on the previous tokens and h g . At each time step t, GRU w takes as input the previous hidden state h t\u22121 , previous token x t\u22121 (embedded with a matrix E), and input encoding h g . Conditioning on the input at each time step helps the model learn dependencies between language and goals.\nh t = GRU w (h t\u22121 , [Ex t\u22121 , h g ])(1)\nThe token at each time step is predicted with a softmax, which uses weight tying with the embedding matrix E (Mao et al., 2015): Figure 3: Our model: tokens are predicted conditioned on previous words and the input, then the output is predicted using attention over the complete dialogue. In supervised training (3a), we train the model to predict the tokens of both agents. During decoding and reinforcement learning (3b) some tokens are sampled from the model, but some are generated by the other agent and are only encoded by the model. Note that the model predicts both agent's words, enabling its use as a forward model in Section 5. At the end of the dialogue, the agent outputs a set of tokens o representing the decision. We generate each output conditionally independently, using a separate classifier for each. The classifiers share bidirectional GRU o and attention mechanism  over the dialogue, and additionally conditions on the input goals.\np \u03b8 (x t |x 0..t\u22121 , g) \u221d exp(E T h t )(2)h \u2212 \u2192 o t = GRU\u2212 \u2192 o (h \u2212 \u2192 o t\u22121 , [Ex t , h t ]) (3) h \u2190 \u2212 o t = GRU\u2190 \u2212 o (h \u2190 \u2212 o t+1 , [Ex t , h t ]) (4) h o t = [h \u2190 \u2212 o t , h \u2212 \u2192 o t ] (5) h a t = W [tanh(W \u2032 h o t )] (6) \u03b1 t = exp(w \u00b7 h a t ) t \u2032 exp(w \u00b7 h a t \u2032 ) (7) h s = tanh(W s [h g , t \u03b1 t h t ])(8)\nThe output tokens are predicted using softmax:\np \u03b8 (o i |x 0..t , g) \u221d exp(W o i h s )(9)\nThe model is trained to minimize the negative log likelihood of the token sequence x 0..T conditioned on the input goals g, and of the outputs o conditioned on x and g. The two terms are weighted with a hyperparameter \u03b1.\nL(\u03b8) = \u2212 x,g t log p \u03b8 (x t |x 0..t\u22121 , g) Token prediction loss \u2212 \u03b1 x,g,o j log p \u03b8 (o j |x 0..T , g)\nOutput choice prediction loss (10)\n\nUnlike the Neural Conversational Model (Vinyals and Le, 2015), our approach shares all parameters for reading and generating tokens.\n\n\nDecoding\n\nDuring decoding, the model must generate an output token x t conditioned on dialogue history x 0..t\u22121 and input goals g, by sampling from p \u03b8 :\nx t \u223c p \u03b8 (x t |x 0..t\u22121 , g)(11)\nIf the model generates a special end-of-turn token, it then encodes a series of tokens output by the other agent, until its next turn (Figure 3b).\n\nThe dialogue ends when either agent outputs a special end-of-dialogue token. The model then outputs a set of choices o. We choose each item independently, but enforce consistency by checking the solution is in a feasible set O:\no * = argmax o\u2208O i p \u03b8 (o i |x 0..T , g)(12)\nIn our task, a solution is feasible if each item is assigned to exactly one agent. The space of solutions is small enough to be tractably enumerated.\n\n\nGoal-based Training\n\nSupervised learning aims to imitate the actions of human users, but does not explicitly attempt to maximise an agent's goals. Instead, we explore pre-training with supervised learning, and then fine-tuning against the evaluation metric using reinforcement learning. Similar two-stage learning strategies have been used previously (e.g. Li et al. (2016); Das et al. (2017)). During reinforcement learning, an agent A attempts to improve its parameters from conversations with another agent B. While the other agent B could be a human, in our experiments we used our fixed supervised model that was trained to imitate humans. The second model is fixed as we found that updating the parameters of both agents led to divergence from human language. In effect, agent A learns to improve by simulating conversations with the help of a surrogate forward model. Agent A reads its goals g and then generates tokens x 0..n by sampling from p \u03b8 . When x generates an end-of-turn marker, it then reads in tokens x n+1..m generated by agent B. These turns alternate until one agent emits a token ending the dialogue. Both agents then output a decision o and collect a reward from the environment (which will be 0 if they output different decisions). We denote the subset of tokens generated by A as X A (e.g. tokens with incoming arrows in Figure 3b).\n\nAfter a complete dialogue has been generated, we update agent A's parameters based on the outcome of the negotiation. Let r A be the score agent A achieved in the completed dialogue, T be the length of the dialogue, \u03b3 be a discount factor that rewards actions at the end of the dialogue more strongly, and \u00b5 be a running average of completed dialogue rewards so far 2 . We define the future reward R for an action x t \u2208 X A as follows:\nR(x t ) = xt\u2208X A \u03b3 T \u2212t (r A (o) \u2212 \u00b5)(13)\nWe then optimise the expected reward of each action x t \u2208 X A :\nL RL \u03b8 = E xt\u223cp \u03b8 (xt|x 0..t\u22121 ,g) [R(x t )](14)\nThe gradient of L RL \u03b8 is calculated as in REIN-FORCE (Williams, 1992):\n\u2207 \u03b8 L RL \u03b8 = xt\u2208X A E xt [R(x t )\u2207 \u03b8 log(p \u03b8 (x t |x 0..t\u22121 , g))](15)\n2 As all rewards are non-negative, we instead re-scale them by subtracting the mean reward found during self play. Shifting in this way can reduce the variance of our estimator. j \u2190 j + 1 7: k \u2190 k + 1\nx j \u223c p \u03b8 (x j |x 0..j\u22121 , g) 8: while x k / \u2208 {read:, choose:} 9: u \u2190 x i+1 ..x j \u22b2 u is\n14:\n\nx k \u223c p \u03b8 (x k |x 0..k\u22121 , g) \u22b2 Calculate rollout output and reward 15:\no \u2190 argmax o \u2032 \u2208O p(o \u2032 |x 0..k , g) 16: R(u) \u2190 R(u) + r(o)p(o \u2032 |x 0..k , g) 17:\nif R(u) > R(u * ) then 18:\nu * \u2190 u 19:\nreturn u * \u22b2 Return best move\n\n\nGoal-based Decoding\n\nLikelihood-based decoding ( \u00a73.3) may not be optimal. For instance, an agent may be choosing between accepting an offer, or making a counter offer. The former will often have a higher likelihood under our model, as there are fewer ways to agree than to make another offer, but the latter may lead to a better outcome. Goal-based decoding also allows more complex dialogue strategies. For example, a deceptive utterance is likely to have a low model score (as users were generally honest in the supervised data), but may achieve high reward. We instead explore decoding by maximising expected reward. We achieve this by using p \u03b8 as a forward model for the complete dialogue, and then deterministically computing the reward. Rewards for an utterance are averaged over samples to calculate expected future reward (Figure 4).\n\nWe use a two stage process: First, we generate c candidate utterances U = u 0..c , representing possible complete turns that the agent could make, which are generated by sampling from p \u03b8 until the end-of-turn token is reached. Let x 0..n\u22121 be current dialogue history. We then calculate the expected reward R(u) of candidate utterance u = x n,n+k by repeatedly sampling x n+k+1,T from p \u03b8 , then choosing the best output o using Equation 12, and finally deterministically computing the reward r(o). The reward is scaled by the probability of the output given the dialogue, because if the agents select different outputs then they both receive 0 reward.\nR(x n..n+k ) = E x (n+k+1..T ;o) \u223cp \u03b8 [r(o)p \u03b8 (o|x 0..T )](16)\nWe then return the utterance maximizing R.\nu * = argmax u\u2208U R(u)(17)\nWe use 5 rollouts for each of 10 candidate turns.\n\n\nExperiments\n\n\nTraining Details\n\nWe implement our models using PyTorch. All hyper-parameters were chosen on a development dataset. The input tokens are embedded into a 64-dimensional space, while the dialogue tokens are embedded with 256-dimensional embeddings (with no pre-training). The input GRU g has a hidden layer of size 64 and the dialogue GRU w is of size 128. The output GRU\u2212 \u2192 o and GRU\u2190 \u2212 o both have a hidden state of size 256, the size of h s is 256 as well. During supervised training, we optimise using stochastic gradient descent with a minibatch size of 16, an initial learning rate of 1.0, Nesterov momentum with \u00b5=0.1 (Nesterov, 1983), and clipping gradients whose L 2 norm exceeds 0.5. We train the model for 30 epochs and pick the snapshot of the model with the best validation perplexity. We then annealed the learning rate by a factor of 5 each epoch. We weight the terms in the loss function (Equation 10) using \u03b1=0.5. We do not train against output decisions where humans selected different agreements. Tokens occurring fewer than 20 times are replaced with an 'unknown' token.\n\nDuring reinforcement learning, we use a learning rate of 0.1, clip gradients above 1.0, and use a discount factor of \u03b3=0.95. After every 4 reinforcement learning updates, we make a supervised update with mini-batch size 16 and learning rate 0.5, and we clip gradients at 1.0. We used 4086 simulated conversations.\n\nWhen sampling words from p \u03b8 , we reduce the variance by doubling the values of logits (i.e. using temperature of 0.5).\n\n\nComparison Systems\n\nWe compare the performance of the following: LIKELIHOOD uses supervised training and decoding ( \u00a73), RL is fine-tuned with goal-based selfplay ( \u00a74), ROLLOUTS uses supervised training combined with goal-based decoding using rollouts ( \u00a75), and RL+ROLLOUTS uses rollouts with a base model trained with reinforcement learning.\n\n\nIntrinsic Evaluation\n\nFor development, we use measured the perplexity of user generated utterances, conditioned on the input and previous dialogue.\n\nResults are shown in Table 3, and show that the simple LIKELIHOOD model produces the most human-like responses, and the alternative training and decoding strategies cause a divergence from human language. Note however, that this divergence may not necessarily correspond to lower quality language-it may also indicate different strategic decisions about what to say. Results in \u00a76.4 show all models could converse with humans.\n\n\nEnd-to-End Evaluation\n\nWe measure end-to-end performance in dialogues both with the likelihood-based agent and with humans on Mechanical Turk, on held out scenarios. Humans were told that they were interacting with other humans, as they had been during the collection of our dataset (and few appeared to realize they were in conversation with machines).\n\nWe measure the following statistics: Score: The average score for each agent (which could be a human or model), out of 10. Agreement: The percentage of dialogues where both agents agreed on the same decision. Pareto Optimality: The percentage of Pareto optimal solutions for agreed deals (a solution is Pareto optimal if neither agent's score can be improved without lowering the other's score). Lower scores indicate inefficient negotiations.   Results are shown in Table 1. Firstly, we see that the RL and ROLLOUTS models achieve significantly better results when negotiating with the LIKELIHOOD model, particularly the RL+ROLLOUTS model. The percentage of Pareto optimal solutions also increases, showing a better exploration of the solution space. Compared to human-human negotiations (Table 2), the best models achieve a higher agreement rate, better scores, and similar Pareto efficiency. This result confirms that attempting to maximise reward can outperform simply imitating humans. Similar trends hold in dialogues with humans, with goal-based reasoning outperforming imitation learning. The ROLLOUTS model achieves comparable scores to its human partners, and the RL+ROLLOUTS model actually achieves higher scores. However, we also find significantly more cases of the goal-based models failing to agree a deal with humans-largely a consequence of their more aggressive negotiation tactics (see \u00a77). Table 1 shows large gains from goal-based methods. In this section, we explore the strengths and weaknesses of our models.\n\n\nAnalysis\n\nGoal-based models negotiate harder. The RL+ROLLOUTS model has much longer dialogues with humans than LIKELIHOOD (7.2 turns vs. 5.3 on average), indicating that the model is accepting deals less quickly, and negotiating harder.\n\nA negative consequence of this more aggressive negotiation strategy is that humans were more likely to walk away with no deal, which is reflected in the lower agreement rates. Even though failing to agree was worth 0 points, people often preferred this course over capitulating to an uncompromising opponent-a factor not well captured by the simulated partner in reinforcement learning training or rollouts (as reflected by the larger gains from goal-based models in dialogues with the LIKELIHOOD model). In particular, the goal-based models are prone to simply rephrasing the same demand each turn, which is a more effective strategy against the LIKELIHOOD model than humans. Future work should address this issue. Figure 5 shows an example of our goal-based model stubbornly negotiating until it achieves a good outcome.\n\nModels learn to be deceptive. Deception can be an effective negotiation tactic. We found numerous cases of our models initially feigning interest in a valueless item, only to later 'compromise' by conceding it. Figure 7 shows an example.\n\n\nModels produce meaningful novel sentences.\n\nOne interesting question is whether our models are capable of generating novel sentences in the new circumstances they find themselves in, or if they simply repeat messages from the training data verbatim. We find that 76% of messages produced by the LIKELIHOOD model in self-play were found in the training data. We manually examined the novel  utterances produced by our model, and found that the overwhelming majority were fluent English sentences in isolation-showing that the model has learnt a good language model for the domain (in addition to results that show it uses language effectively to achieve its goals). These results suggest that although neural models are prone to the safer option of repeating sentences from training data, they are capable of generalising when necessary. Future work should choose domains that force a higher degree of diversity in utterances.\n\nMaintaining multi-sentence coherence is challenging. One common linguistic error we see RL+ROLLOUTS make is to start a message by indicating agreement (e.g. I agree or Deal), but then going on to propose a counter offer-a behaviour that human partners found frustrating. One explanation is that the model has learnt that in the supervised data, messages beginning with I agree are often at the end of the dialogue, and partners rarely reply with further negotiation-so the models using rollouts and reinforcement learning believe this tactic will help their offer to be accepted.\n\n\nRelated Work\n\nMost work on goal orientated dialogue systems has assumed that state representations are anno-  (2017) use task-specific rules to combine the task input and dialogue history into a more structured state representation than ours.\n\nReinforcement learning (RL) has been applied in many dialogue settings.\n\nRL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002;Pietquin et al., 2011;Rieser and Lemon, 2011;Ga\u0161ic et al., 2013;Fatemi et al., 2016). In contrast, our end-toend approach has no explicit dialogue manager. Li et al. (2016) improve metrics such as diversity for non-goal-orientated dialogue using RL, which would make an interesting extension to our work. Das et al. (2017) use reinforcement learning to improve cooperative bot-bot dialogues. RL has also been used to allow agents to invent new languages (Das et al., 2017;Mordatch and Abbeel, 2017). To our knowledge, our model is the first to use RL to improve the performance of an end-toend goal orientated dialogue system in dialogues with humans.\n\nWork on learning end-to-end dialogues has concentrated on 'chat' settings, without explicit goals (Ritter et al., 2011;Vinyals and Le, 2015;Li et al., 2015). These dialogues contain a much greater diversity of vocabulary than our domain, but do not have the challenging adversarial elements. Such models are notoriously hard to evaluate (Liu et al., 2016), because the huge diversity of reasonable responses, whereas our task has a clear objective. Our end-to-end approach would also be much more straightforward to integrate into a generalpurpose dialogue agent than one that relied on annotated dialogue states (Dodge et al., 2016).\n\nThere is a substantial literature on multi-agent bargaining in game-theory, e.g. Nash Jr (1950). There has also been computational work on modelling negotiations (Baarslag et al., 2013)-our work differs in that agents communicate in unrestricted natural language, rather than pre-specified symbolic actions, and our focus on improving performance relative to humans rather than other automated systems. Our task is based on that of DeVault et al. (2015), who study natural language negotiations for pedagogical purposes-their version includes speech rather than textual dialogue, and embodied agents, which would make interesting extensions to our work. The only automated natural language negotiations systems we are aware of have first mapped language to domainspecific logical forms, and then focused on choosing the next dialogue act (Rosenfeld et al., 2014;Cuay\u00e1huitl et al., 2015;Keizer et al., 2017). Our end-to-end approach is the first to to learn comprehension, reasoning and generation skills in a domain-independent data driven way.\n\nOur use of a combination of supervised and reinforcement learning for training, and stochastic rollouts for decoding, builds on strategies used in game playing agents such as AlphaGo (Silver et al., 2016). Our work is a step towards real-world applications for these techniques. Our use of rollouts could be extended by choosing the other agent's responses based on sampling, using Monte Carlo Tree Search (MCTS) (Kocsis and Szepesv\u00e1ri, 2006). However, our setting has a higher branching factor than in domains where MCTS has been successfully applied, such as Go (Silver et al., 2016)-future work should explore scaling tree search to dialogue modelling.\n\n\nConclusion\n\nWe have introduced end-to-end learning of natural language negotiations as a task for AI, arguing that it challenges both linguistic and reasoning skills while having robust evaluation metrics. We gathered a large dataset of human-human ne-gotiations, which contain a variety of interesting tactics. We have shown that it is possible to train dialogue agents end-to-end, but that their ability can be much improved by training and decoding to maximise their goals, rather than likelihood. There remains much potential for future work, particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language. We will also explore other negotiation tasks, to investigate whether models can learn to share negotiation strategies across domains.\n\nFigure 2 :\n2Converting a crowd-sourced dialogue (left) into two training examples (right), from the perspective of each user.\n\nFigure 6 :\n6Example of model compromising.\n\n\nTake one hat read: I need two write: deal . . . Take one hat read: I need two write: deal . . .Input Encoder \n\nOutput Decoder \n\nwrite: (a) Supervised Training \n\nInput Encoder \nOutput Decoder \n\nwrite: (b) Decoding, and Reinforcement Learning \n\n\n\nread :\nreadYou get one book and I'll take everything else. Decoding through rollouts: The model first generates a small set of candidate responses. For each candidate it simulates the future conversation by sampling, and estimates the expected future reward by averaging the scores. The system outputs the candidate with the highest expected reward.write: Great deal, \nthanks! \n\nwrite: No way, I \nneed all 3 hats \nread: Ok, fine \n\nread: I'll give you 2 \n\nread: No problem \n\nread: Any time \n\nchoose: 3x hat \n\nchoose: 2x hat \n\nchoose: 1x book \n\nchoose: 1x book \n\n9 \n\n6 \n\n1 \n\n1 \n\nDialogue history \nCandidate responses \nSimulation of rest of dialogue \nScore \n\nFigure 4: \n\n\nfor s \u2208 {1..S} do \u22b2 S samples per move11:    k \u2190 j \u22b2 Start rollout from end of ucandidate move \n\n10: \n\n12: \n\nwhile x k = choose: do \n\u22b2 Rollout to end of dialogue \n\n13: \n\n\n\n\nTable 1: End task evaluation on heldout scenarios, against the LIKELIHOOD model and humans from Mechanical Turk. The maximum score is 10. Score (all) gives 0 points when agents failed to agree.vs. LIKELIHOOD \n\nvs. Human \n\nModel \nScore \n(all) \n\nScore \n(agreed) \n\n% \nAgreed \n\n% Pareto \nOptimal \n\nScore \n(all) \n\nScore \n(agreed) \n\n% \nAgreed \n\n% Pareto \nOptimal \n\nLIKELIHOOD \n\n5.4 vs. 5.5 6.2 vs. 6.2 \n87.9 \n49.6 \n4.7 vs. 5.8 6.2 vs. 7.6 \n76.5 \n66.2 \n\nRL \n\n7.1 vs. 4.2 7.9 vs. 4.7 \n89.9 \n58.6 \n4.3 vs. 5.0 6.4 vs. 7.5 \n67.3 \n69.1 \n\nROLLOUTS \n\n7.3 vs. 5.1 7.9 vs. 5.5 \n92.9 \n63.7 \n5.2 vs. 5.4 7.1 vs. 7.4 \n72.1 \n78.3 \nRL+ROLLOUTS 8.3 vs. 4.2 8.8 vs. 4.5 \n94.4 \n74.8 \n4.6 vs. 4.2 8.0 vs. 7.1 \n57.2 \n82.4 \n\nMetric \nDataset \nNumber of Dialogues \n5808 \nAverage Turns per Dialogue \n6.6 \nAverage Words per Turn \n7.6 \n% Agreed \n80.1 \nAverage Score (/10) \n6.0 \n% Pareto Optimal \n76.9 \n\n\n\nTable 2 :\n2Statistics on our dataset of crowd-\nsourced dialogues between humans. \n\nModel \nValid PPL Test PPL Test Avg. Rank \n\nLIKELIHOOD \n\n5.62 \n5.47 \n521.8 \n\nRL \n\n6.03 \n5.86 \n517.6 \n\nROLLOUTS \n\n-\n-\n844.1 \n\nRL+ROLLOUTS \n\n-\n-\n859.8 \n\n\n\nTable 3 :\n3Intrinsic evaluation showing the average \nperplexity of tokens and rank of complete turns \n(out of 2083 unique human messages from the test \nset). Lower is more human-like for both. \n\n\n\n\nInput RL+ROLLOUTS 4xbook value=0 2xhat value=5 1xball value=0 Human 4xbook value=1 2xhat value=2 1xball value=2 RL+ROLLOUTS You can have the ball if I can have everything else. Human No can't do that the ball is not going to give me anything RL+ROLLOUTS I need the hats and you can have the rest Human Deal RL+ROLLOUTS Great! Thank you!Figure 7: Dialogue in which the model's initial interest in the valueless books allows it to compromise while achieving a maximum score. tated in the training data (Williams and Young, 2007; Henderson et al., 2014; Wen et al., 2016).The use of state annotations allows a cleaner separation of the reasoning and natural language aspects of dialogues, but our end-to-end approach makes data collection cheaper and allows tasks where it is unclear how to annotate state. explore end-toend goal orientated dialogue with a supervised model-we show improvements over supervised learning with goal-based training and decoding. Recently, He et al.Output \nReward \nRL+ROLLOUTS 2xhat \n10/10 \nHuman \n4xbook 1xball \n6/10 \n\n\nAcknowledgmentsWe would like to thank Luke Zettlemoyer and the anonymous EMNLP reviewers for their insightful comments, and the Mechanical Turk workers who helped us collect data.\nModelling Strategic Conversation: The STAC project. Nicholas Asher, Alex Lascarides, Oliver Lemon, Markus Guhe, Verena Rieser, Philippe Muller, Stergos Afantenos, Farah Benamara, Laure Vieu, Pascal Denis, Proceedings of SemDial. SemDial27Nicholas Asher, Alex Lascarides, Oliver Lemon, Markus Guhe, Verena Rieser, Philippe Muller, Ster- gos Afantenos, Farah Benamara, Laure Vieu, Pascal Denis, et al. 2012. Modelling Strategic Conversa- tion: The STAC project. Proceedings of SemDial page 27.\n\nEvaluating Practical Negotiating Agents: Results and Analysis of the 2011 International Competition. Tim Baarslag, Katsuhide Fujita, H Enrico, Koen Gerding, Takayuki Hindriks, Ito, R Nicholas, Catholijn Jennings, Sarit Jonker, Raz Kraus, Valentin Lin, Robu, Artificial Intelligence. 198Tim Baarslag, Katsuhide Fujita, Enrico H Gerding, Koen Hindriks, Takayuki Ito, Nicholas R Jennings, Catholijn Jonker, Sarit Kraus, Raz Lin, Valentin Robu, et al. 2013. Evaluating Practical Negotiating Agents: Results and Analysis of the 2011 Interna- tional Competition. Artificial Intelligence 198:73- 103.\n\nDzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473Neural Machine Translation by Jointly Learning to Align and Translate. arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv preprint arXiv:1409.0473 .\n\nLearning End-to-End Goal-oriented Dialog. Antoine Bordes, Jason Weston, arXiv:1605.07683arXiv preprintAntoine Bordes and Jason Weston. 2016. Learning End-to-End Goal-oriented Dialog. arXiv preprint arXiv:1605.07683 .\n\nOn the properties of neural machine translation. Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, Yoshua Bengio, arXiv:1409.1259Encoder-decoder approaches. arXiv preprintKyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. arXiv preprint arXiv:1409.1259 .\n\nHeriberto Cuay\u00e1huitl, Simon Keizer, Oliver Lemon, arXiv:1511.08099Strategic Dialogue Management via Deep Reinforcement Learning. arXiv preprintHeriberto Cuay\u00e1huitl, Simon Keizer, and Oliver Lemon. 2015. Strategic Dialogue Management via Deep Reinforcement Learning. arXiv preprint arXiv:1511.08099 .\n\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, M F Jos\u00e9, Devi Moura, Dhruv Parikh, Batra, arXiv:1611.08669Visual Dialog. arXiv preprintAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra. 2016. Visual Dialog. arXiv preprint arXiv:1611.08669 .\n\n. Abhishek Das, Satwik Kottur, M F Jos\u00e9, Stefan Moura, Dhruv Lee, Batra, arXiv:1703.06585Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning. arXiv preprintAbhishek Das, Satwik Kottur, Jos\u00e9 MF Moura, Stefan Lee, and Dhruv Batra. 2017. Learning Coopera- tive Visual Dialog Agents with Deep Reinforcement Learning. arXiv preprint arXiv:1703.06585 .\n\nToward Natural Turn-taking in a Virtual Human Negotiation Agent. David Devault, Johnathan Mell, Jonathan Gratch, AAAI Spring Symposium on Turn-taking and Coordination in Human-Machine Interaction. Stanford, CAAAAI PressDavid DeVault, Johnathan Mell, and Jonathan Gratch. 2015. Toward Natural Turn-taking in a Virtual Hu- man Negotiation Agent. In AAAI Spring Sympo- sium on Turn-taking and Coordination in Human- Machine Interaction. AAAI Press, Stanford, CA.\n\nEvaluating Prerequisite Qualities for Learning End-to-End Dialog Systems. Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine Bordes, Sumit Chopra, Alexander H Miller, Arthur Szlam, Jason Weston, ICLR abs/1511.06931Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine Bordes, Sumit Chopra, Alexander H. Miller, Arthur Szlam, and Jason Weston. 2016. Evaluating Pre- requisite Qualities for Learning End-to-End Dialog Systems. ICLR abs/1511.06931.\n\nMehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, Kaheer Suleman, arXiv:1606.03152Policy Networks with Two-stage Training for Dialogue Systems. arXiv preprintMehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, and Kaheer Suleman. 2016. Policy Networks with Two-stage Training for Dialogue Systems. arXiv preprint arXiv:1606.03152 .\n\nThe Importance of the Agenda in Bargaining. Chaim Fershtman, Games and Economic Behavior. 23Chaim Fershtman. 1990. The Importance of the Agenda in Bargaining. Games and Economic Be- havior 2(3):224-238.\n\nPOMDPbased Dialogue Manager Adaptation to Extended Domains. Milica Ga\u0161ic, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Proceedings of SIGDIAL. SIGDIALBlaise Thomson, Pirros Tsiakoulis, and Steve YoungMilica Ga\u0161ic, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2013. POMDP- based Dialogue Manager Adaptation to Extended Domains. In Proceedings of SIGDIAL.\n\nLearning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. H He, A Balakrishnan, M Eric, P Liang, Association for Computational Linguistics (ACL). H. He, A. Balakrishnan, M. Eric, and P. Liang. 2017. Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. In As- sociation for Computational Linguistics (ACL).\n\nThe Second Dialog State Tracking Challenge. Matthew Henderson, Blaise Thomson, Jason Williams, 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. 263Matthew Henderson, Blaise Thomson, and Jason Williams. 2014. The Second Dialog State Tracking Challenge. In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. volume 263.\n\nEvaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents. Simon Keizer, Markus Guhe, Heriberto Cuay\u00e1huitl, Ioannis Efstathiou, Klaus-Peter Engelbrecht, Mihai Dobre, Alexandra Lascarides, Oliver Lemon, Proceedings of the European Chapter of the Association for Computational Linguistics. the European Chapter of the Association for Computational LinguisticsSimon Keizer, Markus Guhe, Heriberto Cuay\u00e1huitl, Ioannis Efstathiou, Klaus-Peter Engelbrecht, Mihai Dobre, Alexandra Lascarides, and Oliver Lemon. 2017. Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents. In Proceedings of the European Chapter of the Association for Computational Lin- guistics (EACL 2017).\n\nBandit based Monte-Carlo Planning. In European conference on machine learning. Levente Kocsis, Csaba Szepesv\u00e1ri, SpringerLevente Kocsis and Csaba Szepesv\u00e1ri. 2006. Bandit based Monte-Carlo Planning. In European confer- ence on machine learning. Springer, pages 282-293.\n\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, arXiv:1510.03055A Diversity-promoting Objective Function for Neural Conversation Models. arXiv preprintJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015. A Diversity-promoting Ob- jective Function for Neural Conversation Models. arXiv preprint arXiv:1510.03055 .\n\nJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, arXiv:1606.01541Deep Reinforcement Learning for Dialogue Generation. arXiv preprintJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep Rein- forcement Learning for Dialogue Generation. arXiv preprint arXiv:1606.01541 .\n\nHow NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. Chia-Wei Liu, Ryan Lowe, V Iulian, Michael Serban, Laurent Noseworthy, Joelle Charlin, Pineau, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingChia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT To Evaluate Your Dialogue Sys- tem: An Empirical Study of Unsupervised Evalua- tion Metrics for Dialogue Response Generation. In Proceedings of the Conference on Empirical Meth- ods in Natural Language Processing.\n\nLearning Like a Child: Fast Novel Visual Concept Learning From Sentence Descriptions of Images. Junhua Mao, Xu Wei, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L Yuille, The IEEE International Conference on Computer Vision (ICCV). Junhua Mao, Xu Wei, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan L. Yuille. 2015. Learning Like a Child: Fast Novel Visual Concept Learning From Sentence Descriptions of Images. In The IEEE In- ternational Conference on Computer Vision (ICCV).\n\nIgor Mordatch, Pieter Abbeel, arXiv:1703.04908Emergence of Grounded Compositional Language in Multi-Agent Populations. arXiv preprintIgor Mordatch and Pieter Abbeel. 2017. Emergence of Grounded Compositional Language in Multi-Agent Populations. arXiv preprint arXiv:1703.04908 .\n\nThe Bargaining Problem. F John, NashJr, Econometrica: Journal of the Econometric Society. John F Nash Jr. 1950. The Bargaining Problem. Econometrica: Journal of the Econometric Society pages 155-162.\n\nA Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2). Yurii Nesterov, Soviet Mathematics Doklady. 27Yurii Nesterov. 1983. A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2). In Soviet Mathematics Doklady. volume 27, pages 372-376.\n\nSampleefficient Batch Reinforcement Learning for Dialogue Management Optimization. Olivier Pietquin, Matthieu Geist, Senthilkumar Chandramohan, Herv\u00e9 Frezza-Buet, ACM Trans. Speech Lang. Process. 73Olivier Pietquin, Matthieu Geist, Senthilkumar Chan- dramohan, and Herv\u00e9 Frezza-Buet. 2011. Sample- efficient Batch Reinforcement Learning for Dia- logue Management Optimization. ACM Trans. Speech Lang. Process. 7(3):7:1-7:21.\n\nReinforcement Learning for Adaptive Dialogue Systems: A Datadriven Methodology for Dialogue Management and Natural Language Generation. Verena Rieser, Oliver Lemon, Springer Science & Business MediaVerena Rieser and Oliver Lemon. 2011. Reinforcement Learning for Adaptive Dialogue Systems: A Data- driven Methodology for Dialogue Management and Natural Language Generation. Springer Science & Business Media.\n\nData-driven Response Generation in Social Media. Alan Ritter, Colin Cherry, William B Dolan, Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. the Conference on Empirical Methods in Natural Language Processing. Association for Computational LinguisticsAlan Ritter, Colin Cherry, and William B Dolan. 2011. Data-driven Response Generation in Social Me- dia. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Associa- tion for Computational Linguistics, pages 583-593.\n\nNegoChat: A Chat-based Negotiation Agent. Avi Rosenfeld, Inon Zuckerman, Erel Segal-Halevi, Osnat Drein, Sarit Kraus, Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems. International Foundation for Autonomous Agents and Multiagent Systems. the 2014 International Conference on Autonomous Agents and Multi-agent Systems. International Foundation for Autonomous Agents and Multiagent SystemsRichland, SC, AAMAS14Avi Rosenfeld, Inon Zuckerman, Erel Segal-Halevi, Osnat Drein, and Sarit Kraus. 2014. NegoChat: A Chat-based Negotiation Agent. In Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems. International Foun- dation for Autonomous Agents and Multiagent Sys- tems, Richland, SC, AAMAS '14, pages 525-532.\n\nMastering the Game of Go with Deep Neural Networks and Tree Search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 5297587David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Ju- lian Schrittwieser, Ioannis Antonoglou, Veda Pan- neershelvam, Marc Lanctot, et al. 2016. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature 529(7587):484-489.\n\nOptimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Satinder Singh, Diane Litman, Michael Kearns, Marilyn Walker, Journal of Artificial Intelligence Research. 16Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing Dialogue Man- agement with Reinforcement Learning: Experi- ments with the NJFun System. Journal of Artificial Intelligence Research 16:105-133.\n\nDevelopment of lying to conceal a transgression: Children's control of expressive behaviour during verbal deception. Victoria Talwar, Kang Lee, International Journal of Behavioral Development. 265Victoria Talwar and Kang Lee. 2002. Development of lying to conceal a transgression: Children's con- trol of expressive behaviour during verbal decep- tion. International Journal of Behavioral Develop- ment 26(5):436-444.\n\nMulti-party, Multiissue, Multi-strategy Negotiation for Multi-modal Virtual Agents. David Traum, Stacy C Marsella, Jonathan Gratch, Jina Lee, Arno Hartholt, Proceedings of the 8th International Conference on Intelligent Virtual Agents. the 8th International Conference on Intelligent Virtual AgentsDavid Traum, Stacy C. Marsella, Jonathan Gratch, Jina Lee, and Arno Hartholt. 2008. Multi-party, Multi- issue, Multi-strategy Negotiation for Multi-modal Virtual Agents. In Proceedings of the 8th Inter- national Conference on Intelligent Virtual Agents.\n\n. Springer-Verlag, Berlin, Heidelberg, IVA '08Springer-Verlag, Berlin, Heidelberg, IVA '08, pages 117-130.\n\nOriol Vinyals, Quoc Le, arXiv:1506.05869A Neural Conversational Model. arXiv preprintOriol Vinyals and Quoc Le. 2015. A Neural Conversa- tional Model. arXiv preprint arXiv:1506.05869 .\n\nDavid Tsung-Hsien Wen, Nikola Vandyke, Milica Mrksic, Lina M Gasic, Pei-Hao Rojas-Barahona, Su, arXiv:1604.04562Stefan Ultes, and Steve Young. 2016. A Networkbased End-to-End Trainable Task-oriented Dialogue System. arXiv preprintTsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2016. A Network- based End-to-End Trainable Task-oriented Dialogue System. arXiv preprint arXiv:1604.04562 .\n\nPartially Observable Markov Decision Processes for Spoken Dialog Systems. D Jason, Steve Williams, Young, Computer Speech & Language. 212Jason D Williams and Steve Young. 2007. Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech & Language 21(2):393-422.\n\nSimple Statistical Gradientfollowing Algorithms for Connectionist Reinforcement Learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. 1992. Simple Statistical Gradient- following Algorithms for Connectionist Reinforce- ment Learning. Machine learning 8(3-4):229-256.\n", "annotations": {"author": "[{\"end\":128,\"start\":77},{\"end\":179,\"start\":129},{\"end\":218,\"start\":180},{\"end\":306,\"start\":219},{\"end\":394,\"start\":307}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":82},{\"end\":141,\"start\":135},{\"end\":194,\"start\":187},{\"end\":230,\"start\":224},{\"end\":318,\"start\":313}]", "author_first_name": "[{\"end\":81,\"start\":77},{\"end\":134,\"start\":129},{\"end\":184,\"start\":180},{\"end\":186,\"start\":185},{\"end\":223,\"start\":219},{\"end\":312,\"start\":307}]", "author_affiliation": "[{\"end\":127,\"start\":106},{\"end\":178,\"start\":157},{\"end\":217,\"start\":196},{\"end\":271,\"start\":250},{\"end\":305,\"start\":273},{\"end\":359,\"start\":338},{\"end\":393,\"start\":361}]", "title": "[{\"end\":63,\"start\":1},{\"end\":457,\"start\":395}]", "venue": null, "abstract": "[{\"end\":1376,\"start\":470}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1838,\"start\":1818},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1857,\"start\":1838},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4541,\"start\":4519},{\"end\":4905,\"start\":4891},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5883,\"start\":5866},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5922,\"start\":5901},{\"end\":6720,\"start\":6700},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7147,\"start\":7130},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9757,\"start\":9739},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11404,\"start\":11382},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12612,\"start\":12596},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12631,\"start\":12614},{\"end\":14261,\"start\":14234},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17192,\"start\":17176},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24146,\"start\":24126},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24168,\"start\":24146},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24191,\"start\":24168},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24210,\"start\":24191},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24230,\"start\":24210},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24318,\"start\":24302},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24468,\"start\":24451},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24618,\"start\":24600},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24644,\"start\":24618},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24918,\"start\":24897},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24939,\"start\":24918},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24955,\"start\":24939},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25154,\"start\":25136},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25432,\"start\":25412},{\"end\":25530,\"start\":25524},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25619,\"start\":25597},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25888,\"start\":25867},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26297,\"start\":26273},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26321,\"start\":26297},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26341,\"start\":26321},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26685,\"start\":26664},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26923,\"start\":26894},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27065,\"start\":27045}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28084,\"start\":27958},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28128,\"start\":28085},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28374,\"start\":28129},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29042,\"start\":28375},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":29215,\"start\":29043},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30090,\"start\":29216},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30325,\"start\":30091},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30522,\"start\":30326},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":31571,\"start\":30523}]", "paragraph": "[{\"end\":1858,\"start\":1392},{\"end\":2484,\"start\":1860},{\"end\":2770,\"start\":2486},{\"end\":3203,\"start\":2772},{\"end\":3713,\"start\":3205},{\"end\":4151,\"start\":3715},{\"end\":4652,\"start\":4153},{\"end\":5038,\"start\":4654},{\"end\":5807,\"start\":5069},{\"end\":6050,\"start\":5816},{\"end\":6796,\"start\":6052},{\"end\":7148,\"start\":6816},{\"end\":7520,\"start\":7150},{\"end\":7705,\"start\":7522},{\"end\":7919,\"start\":7726},{\"end\":8169,\"start\":7943},{\"end\":8854,\"start\":8171},{\"end\":9021,\"start\":8878},{\"end\":9087,\"start\":9023},{\"end\":9588,\"start\":9132},{\"end\":10584,\"start\":9630},{\"end\":10939,\"start\":10893},{\"end\":11203,\"start\":10983},{\"end\":11341,\"start\":11307},{\"end\":11475,\"start\":11343},{\"end\":11631,\"start\":11488},{\"end\":11812,\"start\":11666},{\"end\":12041,\"start\":11814},{\"end\":12236,\"start\":12087},{\"end\":13598,\"start\":12260},{\"end\":14035,\"start\":13600},{\"end\":14141,\"start\":14078},{\"end\":14262,\"start\":14191},{\"end\":14534,\"start\":14334},{\"end\":14628,\"start\":14625},{\"end\":14701,\"start\":14630},{\"end\":14810,\"start\":14784},{\"end\":14852,\"start\":14823},{\"end\":15698,\"start\":14876},{\"end\":16353,\"start\":15700},{\"end\":16460,\"start\":16418},{\"end\":16536,\"start\":16487},{\"end\":17641,\"start\":16571},{\"end\":17956,\"start\":17643},{\"end\":18077,\"start\":17958},{\"end\":18424,\"start\":18100},{\"end\":18574,\"start\":18449},{\"end\":19002,\"start\":18576},{\"end\":19358,\"start\":19028},{\"end\":20892,\"start\":19360},{\"end\":21131,\"start\":20905},{\"end\":21955,\"start\":21133},{\"end\":22194,\"start\":21957},{\"end\":23122,\"start\":22241},{\"end\":23703,\"start\":23124},{\"end\":23948,\"start\":23720},{\"end\":24021,\"start\":23950},{\"end\":24797,\"start\":24023},{\"end\":25433,\"start\":24799},{\"end\":26479,\"start\":25435},{\"end\":27136,\"start\":26481},{\"end\":27957,\"start\":27151}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9131,\"start\":9088},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9629,\"start\":9589},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10627,\"start\":10585},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10892,\"start\":10627},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10982,\"start\":10940},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11306,\"start\":11204},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11665,\"start\":11632},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12086,\"start\":12042},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14077,\"start\":14036},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14190,\"start\":14142},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14333,\"start\":14263},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14624,\"start\":14535},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14783,\"start\":14702},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14822,\"start\":14811},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16417,\"start\":16354},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16486,\"start\":16461}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":18604,\"start\":18597},{\"end\":19834,\"start\":19827},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":20158,\"start\":20149},{\"end\":20777,\"start\":20770}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1390,\"start\":1378},{\"attributes\":{\"n\":\"2\"},\"end\":5056,\"start\":5041},{\"attributes\":{\"n\":\"2.1\"},\"end\":5067,\"start\":5059},{\"attributes\":{\"n\":\"2.2\"},\"end\":5814,\"start\":5810},{\"attributes\":{\"n\":\"2.3\"},\"end\":6814,\"start\":6799},{\"attributes\":{\"n\":\"3\"},\"end\":7724,\"start\":7708},{\"attributes\":{\"n\":\"3.1\"},\"end\":7941,\"start\":7922},{\"attributes\":{\"n\":\"3.2\"},\"end\":8876,\"start\":8857},{\"attributes\":{\"n\":\"3.3\"},\"end\":11486,\"start\":11478},{\"attributes\":{\"n\":\"4\"},\"end\":12258,\"start\":12239},{\"attributes\":{\"n\":\"5\"},\"end\":14874,\"start\":14855},{\"attributes\":{\"n\":\"6\"},\"end\":16550,\"start\":16539},{\"attributes\":{\"n\":\"6.1\"},\"end\":16569,\"start\":16553},{\"attributes\":{\"n\":\"6.2\"},\"end\":18098,\"start\":18080},{\"attributes\":{\"n\":\"6.3\"},\"end\":18447,\"start\":18427},{\"attributes\":{\"n\":\"6.4\"},\"end\":19026,\"start\":19005},{\"attributes\":{\"n\":\"7\"},\"end\":20903,\"start\":20895},{\"end\":22239,\"start\":22197},{\"attributes\":{\"n\":\"8\"},\"end\":23718,\"start\":23706},{\"attributes\":{\"n\":\"9\"},\"end\":27149,\"start\":27139},{\"end\":27969,\"start\":27959},{\"end\":28096,\"start\":28086},{\"end\":28382,\"start\":28376},{\"end\":30101,\"start\":30092},{\"end\":30336,\"start\":30327}]", "table": "[{\"end\":28374,\"start\":28226},{\"end\":29042,\"start\":28725},{\"end\":29215,\"start\":29125},{\"end\":30090,\"start\":29411},{\"end\":30325,\"start\":30103},{\"end\":30522,\"start\":30338},{\"end\":31571,\"start\":31500}]", "figure_caption": "[{\"end\":28084,\"start\":27971},{\"end\":28128,\"start\":28098},{\"end\":28226,\"start\":28131},{\"end\":28725,\"start\":28387},{\"end\":29125,\"start\":29045},{\"end\":29411,\"start\":29218},{\"end\":31500,\"start\":30525}]", "figure_ref": "[{\"end\":2768,\"start\":2760},{\"end\":3917,\"start\":3909},{\"end\":6775,\"start\":6767},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8853,\"start\":8845},{\"end\":9020,\"start\":9010},{\"end\":11810,\"start\":11800},{\"end\":13597,\"start\":13587},{\"end\":15697,\"start\":15687},{\"end\":21857,\"start\":21849},{\"end\":22176,\"start\":22168}]", "bib_author_first_name": "[{\"end\":31812,\"start\":31804},{\"end\":31824,\"start\":31820},{\"end\":31843,\"start\":31837},{\"end\":31857,\"start\":31851},{\"end\":31870,\"start\":31864},{\"end\":31887,\"start\":31879},{\"end\":31903,\"start\":31896},{\"end\":31920,\"start\":31915},{\"end\":31936,\"start\":31931},{\"end\":31949,\"start\":31943},{\"end\":32349,\"start\":32346},{\"end\":32369,\"start\":32360},{\"end\":32379,\"start\":32378},{\"end\":32392,\"start\":32388},{\"end\":32410,\"start\":32402},{\"end\":32427,\"start\":32426},{\"end\":32447,\"start\":32438},{\"end\":32463,\"start\":32458},{\"end\":32475,\"start\":32472},{\"end\":32491,\"start\":32483},{\"end\":32847,\"start\":32840},{\"end\":32867,\"start\":32858},{\"end\":32879,\"start\":32873},{\"end\":33202,\"start\":33195},{\"end\":33216,\"start\":33211},{\"end\":33429,\"start\":33420},{\"end\":33439,\"start\":33435},{\"end\":33464,\"start\":33457},{\"end\":33481,\"start\":33475},{\"end\":33751,\"start\":33742},{\"end\":33769,\"start\":33764},{\"end\":33784,\"start\":33778},{\"end\":34051,\"start\":34043},{\"end\":34063,\"start\":34057},{\"end\":34078,\"start\":34072},{\"end\":34089,\"start\":34086},{\"end\":34104,\"start\":34097},{\"end\":34113,\"start\":34112},{\"end\":34115,\"start\":34114},{\"end\":34126,\"start\":34122},{\"end\":34139,\"start\":34134},{\"end\":34380,\"start\":34372},{\"end\":34392,\"start\":34386},{\"end\":34402,\"start\":34401},{\"end\":34404,\"start\":34403},{\"end\":34417,\"start\":34411},{\"end\":34430,\"start\":34425},{\"end\":34811,\"start\":34806},{\"end\":34830,\"start\":34821},{\"end\":34845,\"start\":34837},{\"end\":35281,\"start\":35276},{\"end\":35296,\"start\":35289},{\"end\":35308,\"start\":35303},{\"end\":35323,\"start\":35316},{\"end\":35337,\"start\":35332},{\"end\":35355,\"start\":35346},{\"end\":35357,\"start\":35356},{\"end\":35372,\"start\":35366},{\"end\":35385,\"start\":35380},{\"end\":35645,\"start\":35640},{\"end\":35659,\"start\":35654},{\"end\":35662,\"start\":35660},{\"end\":35675,\"start\":35669},{\"end\":35688,\"start\":35684},{\"end\":35699,\"start\":35693},{\"end\":36026,\"start\":36021},{\"end\":36247,\"start\":36241},{\"end\":36264,\"start\":36255},{\"end\":36281,\"start\":36274},{\"end\":36299,\"start\":36293},{\"end\":36311,\"start\":36305},{\"end\":36721,\"start\":36720},{\"end\":36727,\"start\":36726},{\"end\":36743,\"start\":36742},{\"end\":36751,\"start\":36750},{\"end\":37057,\"start\":37050},{\"end\":37075,\"start\":37069},{\"end\":37090,\"start\":37085},{\"end\":37490,\"start\":37485},{\"end\":37505,\"start\":37499},{\"end\":37521,\"start\":37512},{\"end\":37541,\"start\":37534},{\"end\":37565,\"start\":37554},{\"end\":37584,\"start\":37579},{\"end\":37601,\"start\":37592},{\"end\":37620,\"start\":37614},{\"end\":38232,\"start\":38225},{\"end\":38246,\"start\":38241},{\"end\":38422,\"start\":38417},{\"end\":38433,\"start\":38427},{\"end\":38447,\"start\":38442},{\"end\":38466,\"start\":38458},{\"end\":38476,\"start\":38472},{\"end\":38779,\"start\":38774},{\"end\":38788,\"start\":38784},{\"end\":38801,\"start\":38797},{\"end\":38816,\"start\":38810},{\"end\":38833,\"start\":38825},{\"end\":38842,\"start\":38839},{\"end\":39253,\"start\":39245},{\"end\":39263,\"start\":39259},{\"end\":39271,\"start\":39270},{\"end\":39287,\"start\":39280},{\"end\":39303,\"start\":39296},{\"end\":39322,\"start\":39316},{\"end\":39919,\"start\":39913},{\"end\":39927,\"start\":39925},{\"end\":39935,\"start\":39933},{\"end\":39947,\"start\":39942},{\"end\":39961,\"start\":39954},{\"end\":39973,\"start\":39969},{\"end\":39975,\"start\":39974},{\"end\":40294,\"start\":40290},{\"end\":40311,\"start\":40305},{\"end\":40595,\"start\":40594},{\"end\":40857,\"start\":40852},{\"end\":41149,\"start\":41142},{\"end\":41168,\"start\":41160},{\"end\":41188,\"start\":41176},{\"end\":41208,\"start\":41203},{\"end\":41627,\"start\":41621},{\"end\":41642,\"start\":41636},{\"end\":41948,\"start\":41944},{\"end\":41962,\"start\":41957},{\"end\":41980,\"start\":41971},{\"end\":42520,\"start\":42517},{\"end\":42536,\"start\":42532},{\"end\":42552,\"start\":42548},{\"end\":42572,\"start\":42567},{\"end\":42585,\"start\":42580},{\"end\":43344,\"start\":43339},{\"end\":43356,\"start\":43353},{\"end\":43369,\"start\":43364},{\"end\":43371,\"start\":43370},{\"end\":43388,\"start\":43382},{\"end\":43402,\"start\":43395},{\"end\":43416,\"start\":43410},{\"end\":43432,\"start\":43426},{\"end\":43451,\"start\":43444},{\"end\":43471,\"start\":43467},{\"end\":43488,\"start\":43484},{\"end\":43918,\"start\":43910},{\"end\":43931,\"start\":43926},{\"end\":43947,\"start\":43940},{\"end\":43963,\"start\":43956},{\"end\":44372,\"start\":44364},{\"end\":44385,\"start\":44381},{\"end\":44755,\"start\":44750},{\"end\":44768,\"start\":44763},{\"end\":44770,\"start\":44769},{\"end\":44789,\"start\":44781},{\"end\":44802,\"start\":44798},{\"end\":44812,\"start\":44808},{\"end\":45332,\"start\":45327},{\"end\":45346,\"start\":45342},{\"end\":45518,\"start\":45513},{\"end\":45542,\"start\":45536},{\"end\":45558,\"start\":45552},{\"end\":45571,\"start\":45567},{\"end\":45573,\"start\":45572},{\"end\":45588,\"start\":45581},{\"end\":46055,\"start\":46054},{\"end\":46068,\"start\":46063},{\"end\":46365,\"start\":46364}]", "bib_author_last_name": "[{\"end\":31818,\"start\":31813},{\"end\":31835,\"start\":31825},{\"end\":31849,\"start\":31844},{\"end\":31862,\"start\":31858},{\"end\":31877,\"start\":31871},{\"end\":31894,\"start\":31888},{\"end\":31913,\"start\":31904},{\"end\":31929,\"start\":31921},{\"end\":31941,\"start\":31937},{\"end\":31955,\"start\":31950},{\"end\":32358,\"start\":32350},{\"end\":32376,\"start\":32370},{\"end\":32386,\"start\":32380},{\"end\":32400,\"start\":32393},{\"end\":32419,\"start\":32411},{\"end\":32424,\"start\":32421},{\"end\":32436,\"start\":32428},{\"end\":32456,\"start\":32448},{\"end\":32470,\"start\":32464},{\"end\":32481,\"start\":32476},{\"end\":32495,\"start\":32492},{\"end\":32501,\"start\":32497},{\"end\":32856,\"start\":32848},{\"end\":32871,\"start\":32868},{\"end\":32886,\"start\":32880},{\"end\":33209,\"start\":33203},{\"end\":33223,\"start\":33217},{\"end\":33433,\"start\":33430},{\"end\":33455,\"start\":33440},{\"end\":33473,\"start\":33465},{\"end\":33488,\"start\":33482},{\"end\":33762,\"start\":33752},{\"end\":33776,\"start\":33770},{\"end\":33790,\"start\":33785},{\"end\":34055,\"start\":34052},{\"end\":34070,\"start\":34064},{\"end\":34084,\"start\":34079},{\"end\":34095,\"start\":34090},{\"end\":34110,\"start\":34105},{\"end\":34120,\"start\":34116},{\"end\":34132,\"start\":34127},{\"end\":34146,\"start\":34140},{\"end\":34153,\"start\":34148},{\"end\":34384,\"start\":34381},{\"end\":34399,\"start\":34393},{\"end\":34409,\"start\":34405},{\"end\":34423,\"start\":34418},{\"end\":34434,\"start\":34431},{\"end\":34441,\"start\":34436},{\"end\":34819,\"start\":34812},{\"end\":34835,\"start\":34831},{\"end\":34852,\"start\":34846},{\"end\":35287,\"start\":35282},{\"end\":35301,\"start\":35297},{\"end\":35314,\"start\":35309},{\"end\":35330,\"start\":35324},{\"end\":35344,\"start\":35338},{\"end\":35364,\"start\":35358},{\"end\":35378,\"start\":35373},{\"end\":35392,\"start\":35386},{\"end\":35652,\"start\":35646},{\"end\":35667,\"start\":35663},{\"end\":35682,\"start\":35676},{\"end\":35691,\"start\":35689},{\"end\":35707,\"start\":35700},{\"end\":36036,\"start\":36027},{\"end\":36253,\"start\":36248},{\"end\":36272,\"start\":36265},{\"end\":36291,\"start\":36282},{\"end\":36303,\"start\":36300},{\"end\":36319,\"start\":36312},{\"end\":36724,\"start\":36722},{\"end\":36740,\"start\":36728},{\"end\":36748,\"start\":36744},{\"end\":36757,\"start\":36752},{\"end\":37067,\"start\":37058},{\"end\":37083,\"start\":37076},{\"end\":37099,\"start\":37091},{\"end\":37497,\"start\":37491},{\"end\":37510,\"start\":37506},{\"end\":37532,\"start\":37522},{\"end\":37552,\"start\":37542},{\"end\":37577,\"start\":37566},{\"end\":37590,\"start\":37585},{\"end\":37612,\"start\":37602},{\"end\":37626,\"start\":37621},{\"end\":38239,\"start\":38233},{\"end\":38257,\"start\":38247},{\"end\":38425,\"start\":38423},{\"end\":38440,\"start\":38434},{\"end\":38456,\"start\":38448},{\"end\":38470,\"start\":38467},{\"end\":38482,\"start\":38477},{\"end\":38782,\"start\":38780},{\"end\":38795,\"start\":38789},{\"end\":38808,\"start\":38802},{\"end\":38823,\"start\":38817},{\"end\":38837,\"start\":38834},{\"end\":38851,\"start\":38843},{\"end\":39257,\"start\":39254},{\"end\":39268,\"start\":39264},{\"end\":39278,\"start\":39272},{\"end\":39294,\"start\":39288},{\"end\":39314,\"start\":39304},{\"end\":39330,\"start\":39323},{\"end\":39338,\"start\":39332},{\"end\":39923,\"start\":39920},{\"end\":39931,\"start\":39928},{\"end\":39940,\"start\":39936},{\"end\":39952,\"start\":39948},{\"end\":39967,\"start\":39962},{\"end\":39982,\"start\":39976},{\"end\":40303,\"start\":40295},{\"end\":40318,\"start\":40312},{\"end\":40600,\"start\":40596},{\"end\":40606,\"start\":40602},{\"end\":40866,\"start\":40858},{\"end\":41158,\"start\":41150},{\"end\":41174,\"start\":41169},{\"end\":41201,\"start\":41189},{\"end\":41220,\"start\":41209},{\"end\":41634,\"start\":41628},{\"end\":41648,\"start\":41643},{\"end\":41955,\"start\":41949},{\"end\":41969,\"start\":41963},{\"end\":41986,\"start\":41981},{\"end\":42530,\"start\":42521},{\"end\":42546,\"start\":42537},{\"end\":42565,\"start\":42553},{\"end\":42578,\"start\":42573},{\"end\":42591,\"start\":42586},{\"end\":43351,\"start\":43345},{\"end\":43362,\"start\":43357},{\"end\":43380,\"start\":43372},{\"end\":43393,\"start\":43389},{\"end\":43408,\"start\":43403},{\"end\":43424,\"start\":43417},{\"end\":43442,\"start\":43433},{\"end\":43465,\"start\":43452},{\"end\":43482,\"start\":43472},{\"end\":43503,\"start\":43489},{\"end\":43512,\"start\":43505},{\"end\":43924,\"start\":43919},{\"end\":43938,\"start\":43932},{\"end\":43954,\"start\":43948},{\"end\":43970,\"start\":43964},{\"end\":44379,\"start\":44373},{\"end\":44389,\"start\":44386},{\"end\":44761,\"start\":44756},{\"end\":44779,\"start\":44771},{\"end\":44796,\"start\":44790},{\"end\":44806,\"start\":44803},{\"end\":44821,\"start\":44813},{\"end\":45236,\"start\":45221},{\"end\":45340,\"start\":45333},{\"end\":45349,\"start\":45347},{\"end\":45534,\"start\":45519},{\"end\":45550,\"start\":45543},{\"end\":45565,\"start\":45559},{\"end\":45579,\"start\":45574},{\"end\":45603,\"start\":45589},{\"end\":45607,\"start\":45605},{\"end\":46061,\"start\":46056},{\"end\":46077,\"start\":46069},{\"end\":46084,\"start\":46079},{\"end\":46372,\"start\":46366},{\"end\":46382,\"start\":46374}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17550560},\"end\":32243,\"start\":31752},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16203745},\"end\":32838,\"start\":32245},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b2\"},\"end\":33151,\"start\":32840},{\"attributes\":{\"doi\":\"arXiv:1605.07683\",\"id\":\"b3\"},\"end\":33369,\"start\":33153},{\"attributes\":{\"doi\":\"arXiv:1409.1259\",\"id\":\"b4\"},\"end\":33740,\"start\":33371},{\"attributes\":{\"doi\":\"arXiv:1511.08099\",\"id\":\"b5\"},\"end\":34041,\"start\":33742},{\"attributes\":{\"doi\":\"arXiv:1611.08669\",\"id\":\"b6\"},\"end\":34368,\"start\":34043},{\"attributes\":{\"doi\":\"arXiv:1703.06585\",\"id\":\"b7\"},\"end\":34739,\"start\":34370},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1095695},\"end\":35200,\"start\":34741},{\"attributes\":{\"id\":\"b9\"},\"end\":35638,\"start\":35202},{\"attributes\":{\"doi\":\"arXiv:1606.03152\",\"id\":\"b10\"},\"end\":35975,\"start\":35640},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":55310122},\"end\":36179,\"start\":35977},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11193872},\"end\":36628,\"start\":36181},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3051772},\"end\":37004,\"start\":36630},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1294169},\"end\":37377,\"start\":37006},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":216804296},\"end\":38144,\"start\":37379},{\"attributes\":{\"id\":\"b16\"},\"end\":38415,\"start\":38146},{\"attributes\":{\"doi\":\"arXiv:1510.03055\",\"id\":\"b17\"},\"end\":38772,\"start\":38417},{\"attributes\":{\"doi\":\"arXiv:1606.01541\",\"id\":\"b18\"},\"end\":39113,\"start\":38774},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9197196},\"end\":39815,\"start\":39115},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3511040},\"end\":40288,\"start\":39817},{\"attributes\":{\"doi\":\"arXiv:1703.04908\",\"id\":\"b21\"},\"end\":40568,\"start\":40290},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":153422092},\"end\":40769,\"start\":40570},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":145918791},\"end\":41057,\"start\":40771},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11928084},\"end\":41483,\"start\":41059},{\"attributes\":{\"id\":\"b25\"},\"end\":41893,\"start\":41485},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":780171},\"end\":42473,\"start\":41895},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12951604},\"end\":43269,\"start\":42475},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":515925},\"end\":43813,\"start\":43271},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2198541},\"end\":44245,\"start\":43815},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":18877716},\"end\":44664,\"start\":44247},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":42005938},\"end\":45217,\"start\":44666},{\"attributes\":{\"id\":\"b32\"},\"end\":45325,\"start\":45219},{\"attributes\":{\"doi\":\"arXiv:1506.05869\",\"id\":\"b33\"},\"end\":45511,\"start\":45327},{\"attributes\":{\"doi\":\"arXiv:1604.04562\",\"id\":\"b34\"},\"end\":45978,\"start\":45513},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":13903063},\"end\":46272,\"start\":45980},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2332513},\"end\":46557,\"start\":46274}]", "bib_title": "[{\"end\":31802,\"start\":31752},{\"end\":32344,\"start\":32245},{\"end\":34804,\"start\":34741},{\"end\":36019,\"start\":35977},{\"end\":36239,\"start\":36181},{\"end\":36718,\"start\":36630},{\"end\":37048,\"start\":37006},{\"end\":37483,\"start\":37379},{\"end\":39243,\"start\":39115},{\"end\":39911,\"start\":39817},{\"end\":40592,\"start\":40570},{\"end\":40850,\"start\":40771},{\"end\":41140,\"start\":41059},{\"end\":41942,\"start\":41895},{\"end\":42515,\"start\":42475},{\"end\":43337,\"start\":43271},{\"end\":43908,\"start\":43815},{\"end\":44362,\"start\":44247},{\"end\":44748,\"start\":44666},{\"end\":46052,\"start\":45980},{\"end\":46362,\"start\":46274}]", "bib_author": "[{\"end\":31820,\"start\":31804},{\"end\":31837,\"start\":31820},{\"end\":31851,\"start\":31837},{\"end\":31864,\"start\":31851},{\"end\":31879,\"start\":31864},{\"end\":31896,\"start\":31879},{\"end\":31915,\"start\":31896},{\"end\":31931,\"start\":31915},{\"end\":31943,\"start\":31931},{\"end\":31957,\"start\":31943},{\"end\":32360,\"start\":32346},{\"end\":32378,\"start\":32360},{\"end\":32388,\"start\":32378},{\"end\":32402,\"start\":32388},{\"end\":32421,\"start\":32402},{\"end\":32426,\"start\":32421},{\"end\":32438,\"start\":32426},{\"end\":32458,\"start\":32438},{\"end\":32472,\"start\":32458},{\"end\":32483,\"start\":32472},{\"end\":32497,\"start\":32483},{\"end\":32503,\"start\":32497},{\"end\":32858,\"start\":32840},{\"end\":32873,\"start\":32858},{\"end\":32888,\"start\":32873},{\"end\":33211,\"start\":33195},{\"end\":33225,\"start\":33211},{\"end\":33435,\"start\":33420},{\"end\":33457,\"start\":33435},{\"end\":33475,\"start\":33457},{\"end\":33490,\"start\":33475},{\"end\":33764,\"start\":33742},{\"end\":33778,\"start\":33764},{\"end\":33792,\"start\":33778},{\"end\":34057,\"start\":34043},{\"end\":34072,\"start\":34057},{\"end\":34086,\"start\":34072},{\"end\":34097,\"start\":34086},{\"end\":34112,\"start\":34097},{\"end\":34122,\"start\":34112},{\"end\":34134,\"start\":34122},{\"end\":34148,\"start\":34134},{\"end\":34155,\"start\":34148},{\"end\":34386,\"start\":34372},{\"end\":34401,\"start\":34386},{\"end\":34411,\"start\":34401},{\"end\":34425,\"start\":34411},{\"end\":34436,\"start\":34425},{\"end\":34443,\"start\":34436},{\"end\":34821,\"start\":34806},{\"end\":34837,\"start\":34821},{\"end\":34854,\"start\":34837},{\"end\":35289,\"start\":35276},{\"end\":35303,\"start\":35289},{\"end\":35316,\"start\":35303},{\"end\":35332,\"start\":35316},{\"end\":35346,\"start\":35332},{\"end\":35366,\"start\":35346},{\"end\":35380,\"start\":35366},{\"end\":35394,\"start\":35380},{\"end\":35654,\"start\":35640},{\"end\":35669,\"start\":35654},{\"end\":35684,\"start\":35669},{\"end\":35693,\"start\":35684},{\"end\":35709,\"start\":35693},{\"end\":36038,\"start\":36021},{\"end\":36255,\"start\":36241},{\"end\":36274,\"start\":36255},{\"end\":36293,\"start\":36274},{\"end\":36305,\"start\":36293},{\"end\":36321,\"start\":36305},{\"end\":36726,\"start\":36720},{\"end\":36742,\"start\":36726},{\"end\":36750,\"start\":36742},{\"end\":36759,\"start\":36750},{\"end\":37069,\"start\":37050},{\"end\":37085,\"start\":37069},{\"end\":37101,\"start\":37085},{\"end\":37499,\"start\":37485},{\"end\":37512,\"start\":37499},{\"end\":37534,\"start\":37512},{\"end\":37554,\"start\":37534},{\"end\":37579,\"start\":37554},{\"end\":37592,\"start\":37579},{\"end\":37614,\"start\":37592},{\"end\":37628,\"start\":37614},{\"end\":38241,\"start\":38225},{\"end\":38259,\"start\":38241},{\"end\":38427,\"start\":38417},{\"end\":38442,\"start\":38427},{\"end\":38458,\"start\":38442},{\"end\":38472,\"start\":38458},{\"end\":38484,\"start\":38472},{\"end\":38784,\"start\":38774},{\"end\":38797,\"start\":38784},{\"end\":38810,\"start\":38797},{\"end\":38825,\"start\":38810},{\"end\":38839,\"start\":38825},{\"end\":38853,\"start\":38839},{\"end\":39259,\"start\":39245},{\"end\":39270,\"start\":39259},{\"end\":39280,\"start\":39270},{\"end\":39296,\"start\":39280},{\"end\":39316,\"start\":39296},{\"end\":39332,\"start\":39316},{\"end\":39340,\"start\":39332},{\"end\":39925,\"start\":39913},{\"end\":39933,\"start\":39925},{\"end\":39942,\"start\":39933},{\"end\":39954,\"start\":39942},{\"end\":39969,\"start\":39954},{\"end\":39984,\"start\":39969},{\"end\":40305,\"start\":40290},{\"end\":40320,\"start\":40305},{\"end\":40602,\"start\":40594},{\"end\":40610,\"start\":40602},{\"end\":40868,\"start\":40852},{\"end\":41160,\"start\":41142},{\"end\":41176,\"start\":41160},{\"end\":41203,\"start\":41176},{\"end\":41222,\"start\":41203},{\"end\":41636,\"start\":41621},{\"end\":41650,\"start\":41636},{\"end\":41957,\"start\":41944},{\"end\":41971,\"start\":41957},{\"end\":41988,\"start\":41971},{\"end\":42532,\"start\":42517},{\"end\":42548,\"start\":42532},{\"end\":42567,\"start\":42548},{\"end\":42580,\"start\":42567},{\"end\":42593,\"start\":42580},{\"end\":43353,\"start\":43339},{\"end\":43364,\"start\":43353},{\"end\":43382,\"start\":43364},{\"end\":43395,\"start\":43382},{\"end\":43410,\"start\":43395},{\"end\":43426,\"start\":43410},{\"end\":43444,\"start\":43426},{\"end\":43467,\"start\":43444},{\"end\":43484,\"start\":43467},{\"end\":43505,\"start\":43484},{\"end\":43514,\"start\":43505},{\"end\":43926,\"start\":43910},{\"end\":43940,\"start\":43926},{\"end\":43956,\"start\":43940},{\"end\":43972,\"start\":43956},{\"end\":44381,\"start\":44364},{\"end\":44391,\"start\":44381},{\"end\":44763,\"start\":44750},{\"end\":44781,\"start\":44763},{\"end\":44798,\"start\":44781},{\"end\":44808,\"start\":44798},{\"end\":44823,\"start\":44808},{\"end\":45238,\"start\":45221},{\"end\":45342,\"start\":45327},{\"end\":45351,\"start\":45342},{\"end\":45536,\"start\":45513},{\"end\":45552,\"start\":45536},{\"end\":45567,\"start\":45552},{\"end\":45581,\"start\":45567},{\"end\":45605,\"start\":45581},{\"end\":45609,\"start\":45605},{\"end\":46063,\"start\":46054},{\"end\":46079,\"start\":46063},{\"end\":46086,\"start\":46079},{\"end\":46374,\"start\":46364},{\"end\":46384,\"start\":46374}]", "bib_venue": "[{\"end\":31988,\"start\":31981},{\"end\":34950,\"start\":34938},{\"end\":36402,\"start\":36345},{\"end\":37783,\"start\":37714},{\"end\":39489,\"start\":39423},{\"end\":42223,\"start\":42114},{\"end\":42927,\"start\":42759},{\"end\":44964,\"start\":44902},{\"end\":31979,\"start\":31957},{\"end\":32526,\"start\":32503},{\"end\":32972,\"start\":32903},{\"end\":33193,\"start\":33153},{\"end\":33418,\"start\":33371},{\"end\":33869,\"start\":33808},{\"end\":34184,\"start\":34171},{\"end\":34936,\"start\":34854},{\"end\":35274,\"start\":35202},{\"end\":35785,\"start\":35725},{\"end\":36065,\"start\":36038},{\"end\":36343,\"start\":36321},{\"end\":36806,\"start\":36759},{\"end\":37176,\"start\":37101},{\"end\":37712,\"start\":37628},{\"end\":38223,\"start\":38146},{\"end\":38571,\"start\":38500},{\"end\":38920,\"start\":38869},{\"end\":39421,\"start\":39340},{\"end\":40043,\"start\":39984},{\"end\":40407,\"start\":40336},{\"end\":40658,\"start\":40610},{\"end\":40894,\"start\":40868},{\"end\":41253,\"start\":41222},{\"end\":41619,\"start\":41485},{\"end\":42112,\"start\":41988},{\"end\":42757,\"start\":42593},{\"end\":43520,\"start\":43514},{\"end\":44015,\"start\":43972},{\"end\":44438,\"start\":44391},{\"end\":44900,\"start\":44823},{\"end\":45396,\"start\":45367},{\"end\":45727,\"start\":45625},{\"end\":46112,\"start\":46086},{\"end\":46400,\"start\":46384}]"}}}, "year": 2023, "month": 12, "day": 17}