{"id": 259187509, "updated": "2023-10-04 23:39:35.773", "metadata": {"title": "GRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval", "authors": "[{\"first\":\"Iain\",\"last\":\"Mackie\",\"middle\":[]},{\"first\":\"Ivan\",\"last\":\"Sekulic\",\"middle\":[]},{\"first\":\"Shubham\",\"last\":\"Chatterjee\",\"middle\":[]},{\"first\":\"Jeffrey\",\"last\":\"Dalton\",\"middle\":[]},{\"first\":\"Fabio\",\"last\":\"Crestani\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Recent studies show that Generative Relevance Feedback (GRF), using text generated by Large Language Models (LLMs), can enhance the effectiveness of query expansion. However, LLMs can generate irrelevant information that harms retrieval effectiveness. To address this, we propose Generative Relevance Modeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more accurate weighting of expansion terms. Specifically, we identify similar real documents for each generated document and use a neural re-ranker to estimate their relevance. Experiments on three standard document ranking benchmarks show that GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.09938", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-09938", "doi": "10.48550/arxiv.2306.09938"}}, "content": {"source": {"pdf_hash": "0b8eaf52001bafa01dda642a0358ce3355318bc9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.09938v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "aaecbe8599ff6117e95b652dc90280218beecca3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0b8eaf52001bafa01dda642a0358ce3355318bc9.txt", "contents": "\nGRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval\n\n\nIain Mackie i.mackie.1@research.gla.ac.uk \nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\nUniversity of Glasgow\nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\n\n\nIvan Sekuli\u0107 ivan.sekulic@usi.ch \nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\nUniversity of Glasgow\nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\n\n\nShubham Chatterjee shubham.chatterjee@glasgow.ac.uk \nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\nUniversity of Glasgow\nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\n\n\nJeffrey Dalton jeff.dalton@glasgow.ac.uk \nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\nUniversity of Glasgow\nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\n\n\nFabio Crestani fabio.crestani@usi.ch \nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\nUniversity of Glasgow\nUniversity of Glasgow\nUniversit\u00e0 della Svizzera italiana\n\n\nGRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval\nCCS CONCEPTS \u2022 Information systems \u2192 Information retrieval KEYWORDS Text GenerationDocument RetrievalRelevance Modeling\nRecent studies show that Generative Relevance Feedback (GRF), using text generated by Large Language Models (LLMs), can enhance the effectiveness of query expansion. However, LLMs can generate irrelevant information that harms retrieval effectiveness. To address this, we propose Generative Relevance Modeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more accurate weighting of expansion terms. Specifically, we identify similar real documents for each generated document and use a neural re-ranker to estimate their relevance. Experiments on three standard document ranking benchmarks show that GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.\n\nINTRODUCTION\n\nThe classical approach to addressing vocabulary mismatch [2] has been through Pseudo-Relevance Feedback (PRF), where the query is expanded with terms derived from the top-documents in a feedback set [1,2,26,27,41]. Recent research on Generative-Relevance Feedback (GRF) [22] reveals that Large Language Models (LLMs) are capable of producing textual content that provides effective terms for query expansion. Nevertheless, LLMs are subject to generating \"hallucinations\" (text that isn't contained within the relevant documents), which is a considerable drawback. To address this issue, we propose the Generative Relevance Model (GRM) (see Figure 1) which leverages generated documents within a relevance modeling framework. However, we innovatively estimate the relevance of these generated documents based on their semantic similarity to relevant documents within the target collection, effectively addressing the shortcomings of the LLM-based generative expansion.\n\nWe focus on \"complex\" topics [3,23] that demand context, reasoning [34], and understanding of multiple concepts [24], for example, \"challenges Bitcoin faces to become a widely accepted currency\". Our proposed GRM generates documents covering various subtopics related to the information need. This strategy essentially \"dissects\" the query into its constituent parts, which aids in better understanding the query itself. This, in turn, can improve the system's ability to retrieve relevant documents. However, there is a significant variance in the effectiveness of retrieval based on the selected documents for query expansion (see Section 5). Initial experiments show that directly ranking the generated documents is ineffective due to the ranking model's unawareness of the target collection's documents. Therefore, we introduce Relevance-Aware Sample Estimation (RASE) that links our generated documents to similar \"real documents\" in the target collection. This method assists in weighing our GRM query expansion.\n\nWe experiment on TREC Robust04 [34] and CODEC [24] document ranking datasets. We generate 50 documents per topic to give a reasonable sample and observe a significant variance in expansion effectiveness. Specifically, if we could select the best document for query expansion, Recall@1k would be 0.83, while the worse generated document would result in a Recall@1k of 0.59. Building upon this analysis, we show that GRM combined with RASE using a neural re-ranker [29] shows significant improvements over prior generative methods [22] and achieves new state-of-the-art results.\n\nOur contributions are as follows:\n\n\u2022 We demonstrate LLMs can generate documents addressing subtopics of complex information needs. However, the effectiveness of expansion varies significantly depending on the document selected. \u2022 We present GRM, a novel approach for modeling the relevance of generated documents based on the estimated relevance of semantically similar documents from the collection. \u2022 We show that our approach improves MAP by 6-9% and Re-call@1k by 2-4% over prior state-of-the-art expansion methods.\n\n\nRELATED WORK\n\nUsers typically express their information needs in the form of query, which is often under-specified or suffer from a lexical mismatch [2]. Although several methods have been developed to address this issue, query expansion remains a notable solution. Query expansion involves expanding the user's original query with terms more accurately representing the underlying information need [32]. These methods extract potentially useful terms from either explicit user feedback or, more commonly, through a pseudo-relevance feedback approach. Some well-known examples include vector space model feedback (like Rocchio [32]), query likelihood model (like RM3 expansion [1]), KL expansion [41], Relevance Modeling [26], and Latent Concept Expansion [27]. Alternatively, query expansion could be achieved by extracting terms from a knowledge base [6,25,38]. Recent advancements in Large Language Models (LLMs) have spurred progress across multiple IR research directions [39], including query rewriting [33,37], context and facet generation [9,17,18], query-specific reasoning [7,30], and dataset generation [4]. Additionally, researchers have explored dense vector representations for PRF [14], leading to models like ANCE-PRF [40], ColBERT PRF [35], and SPLADE-based PRF [12]. However, these efforts rely on documents retrieved in response to the original query, while our study highlights the effectiveness of multiple LLM-generated documents as context for query expansions.\n\nRecent studies on Generative-Relevance Feedback (GRF) [22] reveal that LLMs, specifically GPT-3 [5], can generate query-specific text independent of first-pass retrieval to enhance query expansion. This approach has been expanded [21] to dense and learned sparse retrieval paradigms, where generated content is encoded into vectors for query contextualization. Our proposed approach varies in several ways. First, we propose a multi-turn prompting strategy to generate diverse documents based on subtopics. Second, we address the issue of \"hallucinations\" in generated content, where the generated text does not align with text within relevant documents from the collection. Specifically, we introduce our unique relevance-aware sample estimation technique to ensure that the most relevant generated documents are used for query expansion.\n\n\nAPPROACH: GRM WITH RASE\n\nIn this section, we formally define our approach of generative relevance modeling (GRM) using relevance-aware sample estimation (RASE). We focus on the document retrieval task: Given an information need , retrieve a ranked list of relevant documents, [D 1 , D 2 , . . . , D ] from a target collection. Specifically, we first use an LLM to generate a feedback set of synthetic documents, [D 1 , D 2 , . . . , D ] for query expansion. To mitigate the variance in query expansion effectiveness based on the generated documents used within our relevance model, we propose Relevance-Aware Sample Estimation (RASE). This novel method estimates the relevance of generated documents based on the estimated relevance of semantically similar \"real\" documents from the target collection. The underlying assumption is that generated documents that resemble relevant documents from the collection should provide superior feedback signals. This method aids in identifying useful generated documents, thus reducing the impact of hallucinations and non-relevant terms within our query expansion. Together, these techniques aim to improve document retrieval by generating more diverse and relevant query expansions, mitigating the impact of less useful or off-topic synthetic content. Document Generation. A complex query typically has multiple facets or subtopics. For example, different subtopics for the query \"What technological challenges does Bitcoin face to becoming a widely used currency?\" could be \"environmental cost\", \"payment costs\", or \"lack of privacy\". To provide a more comprehensive view of the information related to the query, we generate documents that cover different subtopics. Specifically, given the initial query , we utilize ChatGPT [5] with chain-of-thought reasoning [36] to first generate subtopics for the query . Then, we prompt the LLM to generate documents based on these subtopics,\nD = {D 1 , D 2 , . . . , D }, where D\n, the -th LLM-generated document, covers the -th subtopic for the query. This strategy helps avoid missing relevant documents that might be overlooked if we only focus on one aspect of the query. To provide a balance between depth (exploring a subtopic in more detail through multiple documents) and breadth (covering multiple subtopics), we perform this generation times, giving us ( \u00d7 ) diverse query-specific generated documents.\n\nGenerative Relevance Model. Our GRM builds upon prior work on query expansion, most notably RM3 [1]. In this framework, we presume our generated documents, denoted as D , are relevant and thereby define them as our relevant set R. Our key task is to estimate the probability of observing a word given the relevance set, i.e., ( |R). Formally, we compute this as:\n( |R) = \u2211\ufe01 \u2208 R ( | ) ( | ) \u2032 \u2208 R ( | \u2032 )(1)\nIn this equation, ( | ) is the query likelihood score, which quantifies the relevance of a document to the original query . This score is computed via the Relevance-Aware Sample Estimation (RASE) approach, discussed next.\n\nRelevance-Aware Sample Estimation. RASE focuses on estimating the query likelihood score, ( | ), by modeling the relevance of documents in a collection that are similar to the generated document . Specifically, for a given document collection , we identify a subset of documents D = [D 1 , D 2 , . . . , D ] that are closest to according to a similarity function ( , ). We adopt the BM25 metric [31] to measure this similarity, as inspired by recent work [20]. Next, we compute ( | ) using the relevance signals from ( | ), operationalized via a Discounted Cumulative Gain (DCG) approach [11]:\n( | ) = ( | ) + \u2211\ufe01 =2 ( | ) 2 ( )(2)\nHere, the DCG technique enables us to integrate a variety of relevance estimation models, from simpler methods like BM25 to more complex neural re-rankers like MonoT5 [29]. Additionally, we can estimate an upper bound on our relevance estimation by leveraging query relevance judgments (\"gold estimation\"). In our experiments, we find that MonoT5 significantly improves the performance of our generative expansion methods [22].\n\n\nEXPERIMENTAL SETUP\n\nDatasets. We conduct our experiments on two document ranking datasets. CODEC [24] is designed based on the complex information needs of social science researchers. It consists of 42 challenging essay-style topics produced by domain experts such as economists, historians, and politicians. The dataset contains a focused web corpus of 770k long documents, including sources such as BBC, Reuters, Brookings, Forbes, and eHistory.\n\nTREC Robust04 [34] is created to improve retrieval effectiveness by targeting poorly performing topics. It comprises 249 topics with short keyword \"titles\" and longer natural language \"descriptions\" queries. The newswire collection contains 528k long documents (TREC Disks 4 and 5), which include sources such as the FT, Congressional Record, Federal Register, and the LA Times.\n\nIndexing. For indexing the corpora, we use Pyserini version 0.16.0 [15], removing stopwords and using Porter stemming. Details of the hyperparameter tuning are provided with each method. We use cross-validation and optimize Recall@1000 on standard folds for Robust04 [10] and CODEC [24].\n\nEvaluation. We evaluate our system runs up to a depth of 1,000, with the primary measure being Recall@1k. This choice is made due to the importance of recall-oriented evaluation in initial retrieval. We also include measures such as nDCG and MAP for additional comparison and to assess precision. All evaluations are conducted using ir-measures [19], and we use a paired t-test with a 95% confidence interval to determine statistical significance. Document Generation. We use the GPT-3 Chat API [5] for our document generation. Specifically, we use the gpt-3.5-turbo model on Chat mode with parameters:\n\n= 0.7, _ = 1.0, _ = 0.0, _ = 0.0, and maximum length of 512. We 1-shot prompt ChatGPT to create five subtopics ( = 5) before we generate documents based on these subtopics. We repeat this process = 10 times to create a reasonable set of 50 diverse generated documents per topic. We will release all prompts and generated documents for reproducibility.\n\nRetrieval and Expansion. For the preliminary run, we employ a fine-tuned BM25 model [31]. Specifically, we tune the 1 parameter within the range of 0.1 to 5.0, with a step size of 0.2, and the parameter within the range of 0.1 to 1.0, with a step size of 0.1.\n\nFor RASE , we use prior work on document-to-document similarities [20] as a reference. We apply the tuned BM25 model for the document similarity function , treating the generated document as a query. Additionally, we tune the number of documents retrieved from the target collection, ranging from 10 to 100 with a step of 10.\n\nRelevance Estimate Functions. For our relevance estimate function, ( | ), we use the following four formulations and normalize scores:\n\n\u2022 GRM-Uniform: We set the relevance estimation to 1.0 to show the impact of other methods. \u2022 GRM-BM25: Use the tuned BM25 [31] model. \u2022 GRM-T5: We use the T5-3B [29] re-ranker and max-passage aggregate to calculate the document scores. \u2022 GRM-Gold: We use scaled relevance judgments to show oracle.\n\nOther GRM Hyperparameters. . We tune the remaining GRM hyperparameters: the number of feedback docs ( _ : 5 to 95 with a step of 10), the number of feedback terms ( _ : 5 to 50 with a step of 10), the interpolation between the original terms and generative expansion terms ( _ _ \u210e : 0.1 to 0.9 with a step of 0.1). The tuning methodology is the same as BM25, BM25 with RM3 expansion and GRF to make them directly comparable.\n\nBaselines. We compare our approach to the following systems: We use Pyserini's [15] \"impact\" searcher, max-passage aggregatation, and naver/splade-cocondenser-ensembledistil. We tune _ (5,10,15,20,25,30), _ (20,40,60,80,100), and _ _ \u210e (0.1 to 0.9 with a step of 0.1). (4) TCT+PRF [13]: Roccio PRF using ColBERT-TCT [16]. We employ max-passage approach with TCT-ColBERT-v2-HNP checkpoint. We tune Roccio PRF parameters: \u210e (2,3,5,7,10,17), (0.1 to 0.9 with a step of 0.1), and (0.1 to 0.9 with a step of 0.1). (5) GRF [22] : Expands the query based on the language model from text aggregated across multiple LLM-generation subtasks. We use the full GRF method and the GRF-News variant for comparison, using the runs provided by the author.\n\n\nRESULTS AND ANALYSIS\n\nRQ1: Does selection of generated documents impact query expansion effectiveness? Figure 2 displays how the choice of generated documents used for expansion impacts retrieval effectiveness on Robust04 title queries. The boxplot represents the query effectiveness distribution if we select documents for expansion from the worst to the best (from left to right).\n\nEffectiveness based on selection quality. The selection of generated documents significantly affects the overall effectiveness. For instance, MAP ranges from 0.21 for the worst-generated documents per query to 0.34 for the oracle-generated document. Similarly, the worst possible document results in an R@1000 of 0.59, whereas the best-generated document increases recall by 0.24 to 0.83.\n\nQuery variance. The boxplots show that the effectiveness differs greatly based on the query, suggesting high variance even with  Figure 2: MAP and R@1000 boxplot of varying generated documents ordered by effectiveness on Robust04 titles, i.e. the worst generated document for expansion for each topic to the left (1) and best generated document to the right (50).\n\nconstant selection quality. For example, some queries achieve MAP of 0.0 (even with the Oracle document), indicating no relevant documents were retrieved. Conversely, some queries achieve MAP of 0.81 and R@1000 of 1.0 even with the worst possible document. This demonstrates that generating multiple documents doesn't necessarily alleviate the difficulty of hard queries. These findings strongly suggest the potential of LLMs for generating documents for query expansion, especially for complex topics. However, they also highlight an issue: the effectiveness of different generated documents used for query expansion can vary dramatically. While some documents align with the content in relevant documents from the target collection, others do not. In the following section, we propose a solution to weigh generated documents more effectively based on their semantic similarity to the collection's relevant documents.\n\nRQ2: Does generative relevance modeling improve retrieval effectiveness? Table 1 presents document retrieval effectiveness on Robust04 and CODEC datasets using different relevance estimate functions (outlined in Section 4).\n\nInterestingly, GRM-Uniform performs similarly to GRF on Ro-bust04, suggesting that blindly generating diverse subtopics may not always improve query expansion for difficult, specific topics. However, using BM25 for relevance estimation leads to small but consistent gains over Uniform, although still not significant gains.\n\nUtilizing a neural re-ranker [29], such as in GRM-T5, reveals that better RASE can more effectively weigh beneficial generated documents for query expansion. Specifically, this results in significant gains across all measures on Robust04 descriptions and on MAP and nDCG on Robust04 titles over GRF. In fact, GRM-T5 outperforms all PRF models. Our results also indicate that an Oracle relevance estimation could further boost performance by for MAP 13-19% and by 2-3% for R@1k.\n\nOn CODEC, all GRM methods significantly outperform GRF. Since CODEC's complex topics often encompass multiple subtopics, our diverse subtopic-driven prompting approach greatly enhances effectiveness for GRM-Uniform. Relevance estimation via BM25 offers little improvement in R@1k, but T5 significantly improves all measures. Interestingly, while GRM-Gold boosts precision in MAP and nDCG, it shows less improvement in recall.\n\nOverall, our results demonstrate that using a neural re-ranking for RASE in GRM significantly improves effectiveness over GRF methods, leading to an increase of 6-9% in MAP, 2-4% in nDCG, and 2-4% in R@1k. This highlights the promising potential of GRM for state-of-the-art document retrieval across multiple datasets.\n\nIn conclusion, this work presents Generative Relevance Modeling (GRM), a novel approach to document retrieval that leverages large language models to generate diverse synthetic documents covering a wide array of subtopics related to an initial query. Our study on challenging datasets demonstrates that our approach significantly enhances document retrieval effectiveness. We found a considerable variation in retrieval effectiveness, with some generated documents providing near-perfect context while others veered the query off-topic. To address this, we introduced Relevance-Aware Sample Estimation (RASE) within GRM to estimate the relevance of generated documents based on their similarity to relevant documents from the target collection. We show that GRM with RASE significantly improves upon traditional generative expansion methods, increasing MAP by 6-9% and R@1k by 2-4%. In summary, our work underscores the potential of using LLMs in information retrieval systems and encourages further research on improving the generation and weighting process of synthetic documents.\n\nFigure 1 :\n1Generate Relevance Modeling (GRM) through Relevance-Aware Sample Estimation (RASE). This approach allows a diverse range of generated documents to be weighted within an expansion model based on the estimated relevance of semantically similar documents from the collection.\n\n)\nCEQE[28]: Utilizes query-focused vectors for query expansion. We use the CEQE-MaxPool runs provided by the author.(3) SPLADE+RM3: We use SPLADE[8] with RM3[1] expansion.\n\nTable 1 :\n1Document retrieval effectiveness of GRM with different RASE methods (Uniform, BM25, T5, Gold). \"+\" indicates significant improvements against the full GRF and bold depicts the best system.Robust04 -Titles \nRobust04 -Descriptions \nCODEC \nMAP \nnDCG R@1k MAP \nnDCG R@1k \nMAP \nnDCG R@1k \n\nPRF \n\nTCT+PRF \n0.274 \n0.541 \n0.684 \n0.245 \n0.493 \n0.628 \n0.239 \n0.532 \n0.757 \nSPLADE+RM3 \n0.248 \n0.518 \n0.703 \n0.268 \n0.535 \n0.715 \n0.216 \n0.506 \n0.770 \nCEQE [28] \n0.310 \n0.579 \n0.764 \n-\n-\n-\n-\n-\n-\nBM25+RM3 \n0.292 \n0.571 \n0.777 \n0.278 \n0.551 \n0.750 \n0.239 \n0.530 \n0.816 \n\nGRF \nGRF-News [22] \n0.287 \n0.571 \n0.745 \n0.274 \n0.557 \n0.717 \n0.270 \n0.573 \n0.828 \nGRF [22] \n0.307 \n0.603 \n0.788 \n0.318 \n0.605 \n0.776 \n0.285 \n0.585 \n0.830 \n\nGRM \n\nGRM-Uniform (Ours) 0.306 \n0.594 \n0.778 \n0.313 \n0.605 \n0.779 \n0.306 + 0.611 + 0.850 + \nGRM-BM25 (Ours) \n0.312 \n0.599 \n0.781 \n0.315 \n0.607 \n0.779 \n0.303 + 0.608 + 0.843 \nGRM-T5 (Ours) \n0.327 + 0.615 + 0.796 0.342 + 0.631 + 0.805 + 0.309 + 0.611 + 0.848 + \nGRM-Gold (Ours) \n0.388 + 0.672 + 0.819 + 0.387 + 0.675 + 0.823 + 0.336 + 0.642 + 0.855 + \n\n\nACKNOWLEDGEMENTS\nUMass at TREC 2004: Novelty and HARD. Nasreen Abdul-Jaleel, James Allan, Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, D Mark, Courtney Smucker, Wade, Computer Science Department Faculty Publication Series. 189Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. Computer Science Department Faculty Publication Series (2004), 189.\n\nASK for information retrieval: Part I. Background and theory. J Nicholas, Robert N Belkin, Helen M Oddy, Brooks, Journal of documentation. Nicholas J Belkin, Robert N Oddy, and Helen M Brooks. 1982. ASK for information retrieval: Part I. Background and theory. Journal of documentation (1982).\n\nA Non-Factoid Question-Answering Taxonomy. Valeriia Bolotova, Vladislav Blinov, Falk Scholer, Bruce Croft, Mark Sanderson, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalValeriia Bolotova, Vladislav Blinov, Falk Scholer, W Bruce Croft, and Mark Sander- son. 2022. A Non-Factoid Question-Answering Taxonomy. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval. 1196-1207.\n\nInpars: Unsupervised dataset generation for information retrieval. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Unsupervised dataset generation for information retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2387-2392.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\n\nEntity query feature expansion using knowledge base links. Jeffrey Dalton, Laura Dietz, James Allan, Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. the 37th international ACM SIGIR conference on Research & development in information retrievalJeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. 365-374.\n\nFernando Ferraretto, Thiago Laitz, Roberto Lotufo, Rodrigo Nogueira, arXiv:2301.10521ExaRanker: Explanation-Augmented Neural Ranker. arXiv preprintFernando Ferraretto, Thiago Laitz, Roberto Lotufo, and Rodrigo Nogueira. 2023. ExaRanker: Explanation-Augmented Neural Ranker. arXiv preprint arXiv:2301.10521 (2023).\n\nSPLADE: Sparse lexical and expansion model for first stage ranking. Thibault Formal, Benjamin Piwowarski, St\u00e9phane Clinchant, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalThibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. SPLADE: Sparse lexical and expansion model for first stage ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2288-2292.\n\n. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, arXiv:2212.104962022. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprintLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).\n\nParameters learned in the comparison of retrieval models using term dependencies. Ir. Samuel Huston, W Bruce Croft, University of MassachusettsSamuel Huston and W Bruce Croft. 2014. Parameters learned in the comparison of retrieval models using term dependencies. Ir, University of Massachusetts (2014).\n\nCumulated gain-based evaluation of IR techniques. Kalervo J\u00e4rvelin, Jaana Kek\u00e4l\u00e4inen, ACM Transactions on Information Systems (TOIS). 20Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422-446.\n\nCarlos Lassance, St\u00e9phane Clinchant, arXiv:2302.12574Naver Labs Europe (SPLADE)@ TREC Deep Learning 2022. arXiv preprintCarlos Lassance and St\u00e9phane Clinchant. 2023. Naver Labs Europe (SPLADE)@ TREC Deep Learning 2022. arXiv preprint arXiv:2302.12574 (2023).\n\nPseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls. Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, G Zuccon, ArXiv abs/2108.11044Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, and G. Zuccon. 2021. Pseudo Relevance Feedback with Deep Language Models and Dense Re- trievers: Successes and Pitfalls. ArXiv abs/2108.11044 (2021).\n\nImproving query representations for dense retrieval with pseudo relevance feedback: A reproducibility study. Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, Guido Zuccon, ECIR 2022Advances in Information Retrieval: 44th European Conference on IR Research. Stavanger, NorwaySpringerProceedings, Part IHang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, and Guido Zuccon. 2022. Improving query representations for dense retrieval with pseudo relevance feedback: A reproducibility study. In Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part I. Springer, 599-612.\n\nPyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python toolkit for reproducible infor- mation retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2356-2362.\n\nIn-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. Jheng-Hong Sheng-Chieh Lin, Jimmy Yang, Lin, Proceedings of the 6th Workshop on Representation Learning for NLP. the 6th Workshop on Representation Learning for NLPSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP- 2021). 163-173.\n\nLinqing Liu, Minghan Li, Jimmy Lin, Sebastian Riedel, arXiv:2210.07093and Pontus Stenetorp. 2022. Query Expansion Using Contextual Clue Sampling with Language Models. arXiv preprintLinqing Liu, Minghan Li, Jimmy Lin, Sebastian Riedel, and Pontus Stenetorp. 2022. Query Expansion Using Contextual Clue Sampling with Language Models. arXiv preprint arXiv:2210.07093 (2022).\n\nSean Macavaney, Craig Macdonald, Roderick Murray-Smith, arXiv:2108.04026and Iadh Ounis. 2021. IntenT5: Search Result Diversification using Causal Language Models. arXiv preprintSean MacAvaney, Craig Macdonald, Roderick Murray-Smith, and Iadh Ounis. 2021. IntenT5: Search Result Diversification using Causal Language Models. arXiv preprint arXiv:2108.04026 (2021).\n\nStreamlining Evaluation with ir-measures. Sean Macavaney, Craig Macdonald, Iadh Ounis, European Conference on Information Retrieval. SpringerSean MacAvaney, Craig Macdonald, and Iadh Ounis. 2022. Streamlining Evalua- tion with ir-measures. In European Conference on Information Retrieval. Springer, 305-310.\n\nAdaptive Re-Ranking with a Corpus Graph. Sean Macavaney, Nicola Tonellotto, Craig Macdonald, 10.1145/3511808.355723131st ACM International Conference on Information and Knowledge Management. Sean MacAvaney, Nicola Tonellotto, and Craig Macdonald. 2022. Adaptive Re- Ranking with a Corpus Graph. In 31st ACM International Conference on Informa- tion and Knowledge Management. https://doi.org/10.1145/3511808.3557231\n\nGenerative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval. Iain Mackie, Shubham Chatterjee, Jeffrey Dalton, arXiv:2305.07477arXiv preprintIain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023. Generative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval. arXiv preprint arXiv:2305.07477 (2023).\n\nGenerative Relevance Feedback with Large Language Models. Iain Mackie, Shubham Chatterjee, Jeffrey Dalton, 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. Iain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023. Generative Relevance Feedback with Large Language Models. 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (2023).\n\nHow deep is your learning: The DL-HARD annotated deep learning dataset. Iain Mackie, Jeffrey Dalton, Andrew Yates, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalIain Mackie, Jeffrey Dalton, and Andrew Yates. 2021. How deep is your learn- ing: The DL-HARD annotated deep learning dataset. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2335-2341.\n\nCODEC: Complex Document and Entity Collection. Iain Mackie, Paul Owoicho, Carlos Gemmell, Sophie Fischer, Sean Macavaney, Jeffery Dalton, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalIain Mackie, Paul Owoicho, Carlos Gemmell, Sophie Fischer, Sean MacAvaney, and Jeffery Dalton. 2022. CODEC: Complex Document and Entity Collection. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\nConceptual language models for domain-specific retrieval. Edgar Meij, Dolf Trieschnigg, Maarten De Rijke, Wessel Kraaij, Information Processing & Management. 46Edgar Meij, Dolf Trieschnigg, Maarten De Rijke, and Wessel Kraaij. 2010. Con- ceptual language models for domain-specific retrieval. Information Processing & Management 46, 4 (2010), 448-469.\n\nA markov random field model for term dependencies. Donald Metzler, W Bruce Croft, Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. the 28th annual international ACM SIGIR conference on Research and development in information retrievalDonald Metzler and W Bruce Croft. 2005. A markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. 472-479.\n\nLatent concept expansion using markov random fields. Donald Metzler, W Bruce Croft, Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. the 30th annual international ACM SIGIR conference on Research and development in information retrievalDonald Metzler and W Bruce Croft. 2007. Latent concept expansion using markov random fields. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. 311-318.\n\nCeqe: Contextualized embeddings for query expansion. Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, James Allan, Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event. SpringerProceedings, Part I 43Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, and James Allan. 2021. Ceqe: Contextualized embeddings for query expansion. In Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28-April 1, 2021, Proceedings, Part I 43. Springer, 467-482.\n\nDocument Ranking with a Pretrained Sequence-to-Sequence Model. Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, Jimmy Lin, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with a Pretrained Sequence-to-Sequence Model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 708-718.\n\nVisconde: Multi-document QA with GPT-3 and Neural Reranking. Jayr Pereira, Robson Fidalgo, Roberto Lotufo, Rodrigo Nogueira, Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023. Dublin, IrelandSpringerProceedings, Part IIJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Rodrigo Nogueira. 2023. Vis- conde: Multi-document QA with GPT-3 and Neural Reranking. In Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part II. Springer, 534-543.\n\nSome simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. E Stephen, Steve Robertson, Walker, SIGIR'94. SpringerStephen E Robertson and Steve Walker. 1994. Some simple effective approxi- mations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR'94. Springer, 232-241.\n\nRelevance feedback in information retrieval. The Smart retrieval system-experiments in automatic document processing. Joseph Rocchio, Joseph Rocchio. 1971. Relevance feedback in information retrieval. The Smart retrieval system-experiments in automatic document processing (1971), 313-323.\n\nQuestion rewriting for conversational question answering. Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu, Raviteja Anantha, Proceedings of the 14th ACM international conference on web search and data mining. the 14th ACM international conference on web search and data miningSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu, and Raviteja Anantha. 2021. Question rewriting for conversational question answering. In Proceedings of the 14th ACM international conference on web search and data mining. 355-363.\n\nOverview of the TREC 2004 Robust Track. Ellen M Voorhees, Proceedings of the Thirteenth Text REtrieval Conference. the Thirteenth Text REtrieval ConferenceGaithersburg, MarylandEllen M. Voorhees. 2004. Overview of the TREC 2004 Robust Track. In Proceedings of the Thirteenth Text REtrieval Conference (TREC 2004). Gaithersburg, Maryland, 52-69.\n\nColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval. Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis, ACM Transactions on the Web. Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2022. ColBERT- PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval. ACM Transactions on the Web (2022).\n\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, H Ed, Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems. n. d.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. [n. d.]. Chain-of-Thought Prompting Elicits Rea- soning in Large Language Models. In Advances in Neural Information Processing Systems.\n\nCONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, Gaurav Singh Tomar, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsZeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, and Gaurav Singh Tomar. 2022. CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 10000-10014. https://aclanthology.org/2022.emnlp-main.679\n\nQuery Expansion with Freebase. Chenyan Xiong, Jamie Callan, 10.1145/2808194.2809446Proceedings of the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15). the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15)New York, NY, USAAssociation for Computing MachineryChenyan Xiong and Jamie Callan. 2015. Query Expansion with Freebase. In Proceedings of the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15). Association for Computing Machinery, New York, NY, USA, 111-120. https://doi.org/10.1145/2808194.2809446\n\nPretrained Transformers for Text Ranking: BERT and Beyond. Andrew Yates, Rodrigo Nogueira, Jimmy Lin, Proceedings of the 14th ACM International Conference on Web Search and Data Mining. the 14th ACM International Conference on Web Search and Data MiningAndrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers for Text Ranking: BERT and Beyond. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining. 1154-1156.\n\nImproving Query Representations for Dense Retrieval with Pseudo Relevance Feedback. Hongchien Yu, Chenyan Xiong, Jamie Callan, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementHongChien Yu, Chenyan Xiong, and Jamie Callan. 2021. Improving Query Repre- sentations for Dense Retrieval with Pseudo Relevance Feedback. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 3592-3596.\n\nModel-based feedback in the language modeling approach to information retrieval. Chengxiang Zhai, John Lafferty, Proceedings of the tenth international conference on Information and knowledge management. the tenth international conference on Information and knowledge managementChengxiang Zhai and John Lafferty. 2001. Model-based feedback in the lan- guage modeling approach to information retrieval. In Proceedings of the tenth international conference on Information and knowledge management. 403-410.\n", "annotations": {"author": "[{\"end\":281,\"start\":101},{\"end\":453,\"start\":282},{\"end\":644,\"start\":454},{\"end\":824,\"start\":645},{\"end\":1000,\"start\":825}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":106},{\"end\":294,\"start\":287},{\"end\":472,\"start\":462},{\"end\":659,\"start\":653},{\"end\":839,\"start\":831}]", "author_first_name": "[{\"end\":105,\"start\":101},{\"end\":286,\"start\":282},{\"end\":461,\"start\":454},{\"end\":652,\"start\":645},{\"end\":830,\"start\":825}]", "author_affiliation": "[{\"end\":280,\"start\":144},{\"end\":452,\"start\":316},{\"end\":643,\"start\":507},{\"end\":823,\"start\":687},{\"end\":999,\"start\":863}]", "title": "[{\"end\":98,\"start\":1},{\"end\":1098,\"start\":1001}]", "venue": null, "abstract": "[{\"end\":1906,\"start\":1219}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1982,\"start\":1979},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2124,\"start\":2121},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2126,\"start\":2124},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2129,\"start\":2126},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2132,\"start\":2129},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2135,\"start\":2132},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2196,\"start\":2192},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2923,\"start\":2920},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2926,\"start\":2923},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2962,\"start\":2958},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3007,\"start\":3003},{\"end\":3534,\"start\":3524},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3946,\"start\":3942},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3961,\"start\":3957},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4378,\"start\":4374},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4444,\"start\":4440},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5163,\"start\":5160},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5414,\"start\":5410},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5642,\"start\":5638},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5691,\"start\":5688},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5711,\"start\":5707},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5736,\"start\":5732},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5771,\"start\":5767},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5867,\"start\":5864},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5870,\"start\":5867},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5873,\"start\":5870},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5992,\"start\":5988},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6024,\"start\":6020},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6027,\"start\":6024},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6061,\"start\":6058},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6064,\"start\":6061},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6067,\"start\":6064},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6097,\"start\":6094},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6100,\"start\":6097},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6128,\"start\":6125},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6211,\"start\":6207},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6249,\"start\":6245},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6267,\"start\":6263},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6294,\"start\":6290},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6555,\"start\":6551},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6596,\"start\":6593},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6731,\"start\":6727},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9110,\"start\":9107},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9147,\"start\":9143},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9835,\"start\":9832},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10765,\"start\":10761},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10825,\"start\":10821},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10958,\"start\":10954},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11168,\"start\":11164},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11423,\"start\":11419},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11528,\"start\":11524},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11894,\"start\":11890},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12327,\"start\":12323},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12527,\"start\":12523},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12542,\"start\":12538},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12894,\"start\":12890},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13043,\"start\":13040},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13590,\"start\":13586},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13833,\"start\":13829},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14352,\"start\":14348},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14391,\"start\":14387},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15034,\"start\":15030},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15139,\"start\":15136},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15142,\"start\":15139},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15145,\"start\":15142},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15148,\"start\":15145},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15151,\"start\":15148},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15154,\"start\":15151},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15236,\"start\":15232},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15271,\"start\":15267},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15472,\"start\":15468},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18334,\"start\":18330},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20907,\"start\":20903},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21045,\"start\":21042},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21057,\"start\":21054}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20895,\"start\":20610},{\"attributes\":{\"id\":\"fig_1\"},\"end\":21068,\"start\":20896},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22145,\"start\":21069}]", "paragraph": "[{\"end\":2889,\"start\":1922},{\"end\":3909,\"start\":2891},{\"end\":4487,\"start\":3911},{\"end\":4522,\"start\":4489},{\"end\":5008,\"start\":4524},{\"end\":6495,\"start\":5025},{\"end\":7336,\"start\":6497},{\"end\":9263,\"start\":7364},{\"end\":9734,\"start\":9302},{\"end\":10098,\"start\":9736},{\"end\":10364,\"start\":10143},{\"end\":10959,\"start\":10366},{\"end\":11424,\"start\":10997},{\"end\":11874,\"start\":11447},{\"end\":12254,\"start\":11876},{\"end\":12543,\"start\":12256},{\"end\":13147,\"start\":12545},{\"end\":13500,\"start\":13149},{\"end\":13761,\"start\":13502},{\"end\":14088,\"start\":13763},{\"end\":14224,\"start\":14090},{\"end\":14523,\"start\":14226},{\"end\":14949,\"start\":14525},{\"end\":15689,\"start\":14951},{\"end\":16074,\"start\":15714},{\"end\":16464,\"start\":16076},{\"end\":16829,\"start\":16466},{\"end\":17749,\"start\":16831},{\"end\":17974,\"start\":17751},{\"end\":18299,\"start\":17976},{\"end\":18778,\"start\":18301},{\"end\":19205,\"start\":18780},{\"end\":19525,\"start\":19207},{\"end\":20609,\"start\":19527}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9301,\"start\":9264},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10142,\"start\":10099},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10996,\"start\":10960}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17831,\"start\":17824}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1920,\"start\":1908},{\"attributes\":{\"n\":\"2\"},\"end\":5023,\"start\":5011},{\"attributes\":{\"n\":\"3\"},\"end\":7362,\"start\":7339},{\"attributes\":{\"n\":\"4\"},\"end\":11445,\"start\":11427},{\"attributes\":{\"n\":\"5\"},\"end\":15712,\"start\":15692},{\"end\":20621,\"start\":20611},{\"end\":20898,\"start\":20897},{\"end\":21079,\"start\":21070}]", "table": "[{\"end\":22145,\"start\":21269}]", "figure_caption": "[{\"end\":20895,\"start\":20623},{\"end\":21068,\"start\":20899},{\"end\":21269,\"start\":21081}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2570,\"start\":2562},{\"end\":15803,\"start\":15795},{\"end\":16603,\"start\":16595}]", "bib_author_first_name": "[{\"end\":22208,\"start\":22201},{\"end\":22228,\"start\":22223},{\"end\":22241,\"start\":22236},{\"end\":22257,\"start\":22249},{\"end\":22268,\"start\":22264},{\"end\":22284,\"start\":22277},{\"end\":22290,\"start\":22289},{\"end\":22305,\"start\":22297},{\"end\":22681,\"start\":22680},{\"end\":22698,\"start\":22692},{\"end\":22700,\"start\":22699},{\"end\":22714,\"start\":22709},{\"end\":22716,\"start\":22715},{\"end\":22964,\"start\":22956},{\"end\":22984,\"start\":22975},{\"end\":22997,\"start\":22993},{\"end\":23012,\"start\":23007},{\"end\":23024,\"start\":23020},{\"end\":23583,\"start\":23579},{\"end\":23599,\"start\":23595},{\"end\":23617,\"start\":23610},{\"end\":23633,\"start\":23626},{\"end\":24170,\"start\":24162},{\"end\":24188,\"start\":24184},{\"end\":24202,\"start\":24195},{\"end\":24215,\"start\":24210},{\"end\":24233,\"start\":24225},{\"end\":24248,\"start\":24242},{\"end\":24265,\"start\":24259},{\"end\":24285,\"start\":24279},{\"end\":24299,\"start\":24293},{\"end\":24659,\"start\":24652},{\"end\":24673,\"start\":24668},{\"end\":24686,\"start\":24681},{\"end\":25142,\"start\":25134},{\"end\":25161,\"start\":25155},{\"end\":25176,\"start\":25169},{\"end\":25192,\"start\":25185},{\"end\":25525,\"start\":25517},{\"end\":25542,\"start\":25534},{\"end\":25563,\"start\":25555},{\"end\":26054,\"start\":26050},{\"end\":26068,\"start\":26060},{\"end\":26078,\"start\":26073},{\"end\":26089,\"start\":26084},{\"end\":26445,\"start\":26439},{\"end\":26461,\"start\":26454},{\"end\":26715,\"start\":26708},{\"end\":26731,\"start\":26726},{\"end\":26966,\"start\":26960},{\"end\":26985,\"start\":26977},{\"end\":27322,\"start\":27318},{\"end\":27332,\"start\":27327},{\"end\":27349,\"start\":27341},{\"end\":27363,\"start\":27358},{\"end\":27374,\"start\":27373},{\"end\":27722,\"start\":27718},{\"end\":27735,\"start\":27727},{\"end\":27749,\"start\":27744},{\"end\":27766,\"start\":27758},{\"end\":27776,\"start\":27771},{\"end\":27787,\"start\":27782},{\"end\":28410,\"start\":28405},{\"end\":28424,\"start\":28416},{\"end\":28440,\"start\":28429},{\"end\":28456,\"start\":28446},{\"end\":28468,\"start\":28463},{\"end\":28485,\"start\":28478},{\"end\":29157,\"start\":29147},{\"end\":29180,\"start\":29175},{\"end\":29568,\"start\":29561},{\"end\":29581,\"start\":29574},{\"end\":29591,\"start\":29586},{\"end\":29606,\"start\":29597},{\"end\":29938,\"start\":29934},{\"end\":29955,\"start\":29950},{\"end\":29975,\"start\":29967},{\"end\":30345,\"start\":30341},{\"end\":30362,\"start\":30357},{\"end\":30378,\"start\":30374},{\"end\":30653,\"start\":30649},{\"end\":30671,\"start\":30665},{\"end\":30689,\"start\":30684},{\"end\":31116,\"start\":31112},{\"end\":31132,\"start\":31125},{\"end\":31152,\"start\":31145},{\"end\":31441,\"start\":31437},{\"end\":31457,\"start\":31450},{\"end\":31477,\"start\":31470},{\"end\":31875,\"start\":31871},{\"end\":31891,\"start\":31884},{\"end\":31906,\"start\":31900},{\"end\":32429,\"start\":32425},{\"end\":32442,\"start\":32438},{\"end\":32458,\"start\":32452},{\"end\":32474,\"start\":32468},{\"end\":32488,\"start\":32484},{\"end\":32507,\"start\":32500},{\"end\":33053,\"start\":33048},{\"end\":33064,\"start\":33060},{\"end\":33085,\"start\":33078},{\"end\":33102,\"start\":33096},{\"end\":33400,\"start\":33394},{\"end\":33417,\"start\":33410},{\"end\":33931,\"start\":33925},{\"end\":33948,\"start\":33941},{\"end\":34466,\"start\":34458},{\"end\":34482,\"start\":34475},{\"end\":34497,\"start\":34491},{\"end\":34510,\"start\":34505},{\"end\":35016,\"start\":35009},{\"end\":35034,\"start\":35027},{\"end\":35047,\"start\":35042},{\"end\":35062,\"start\":35057},{\"end\":35555,\"start\":35551},{\"end\":35571,\"start\":35565},{\"end\":35588,\"start\":35581},{\"end\":35604,\"start\":35597},{\"end\":36167,\"start\":36166},{\"end\":36182,\"start\":36177},{\"end\":36521,\"start\":36515},{\"end\":36754,\"start\":36746},{\"end\":36772,\"start\":36766},{\"end\":36790,\"start\":36782},{\"end\":36803,\"start\":36795},{\"end\":37241,\"start\":37236},{\"end\":37243,\"start\":37242},{\"end\":37636,\"start\":37632},{\"end\":37648,\"start\":37643},{\"end\":37666,\"start\":37660},{\"end\":37683,\"start\":37679},{\"end\":37993,\"start\":37988},{\"end\":38005,\"start\":37999},{\"end\":38016,\"start\":38012},{\"end\":38036,\"start\":38029},{\"end\":38047,\"start\":38044},{\"end\":38054,\"start\":38053},{\"end\":38065,\"start\":38064},{\"end\":38077,\"start\":38072},{\"end\":38473,\"start\":38468},{\"end\":38480,\"start\":38478},{\"end\":38493,\"start\":38487},{\"end\":38508,\"start\":38503},{\"end\":38526,\"start\":38518},{\"end\":38543,\"start\":38539},{\"end\":38567,\"start\":38555},{\"end\":39269,\"start\":39262},{\"end\":39282,\"start\":39277},{\"end\":39895,\"start\":39889},{\"end\":39910,\"start\":39903},{\"end\":39926,\"start\":39921},{\"end\":40387,\"start\":40378},{\"end\":40399,\"start\":40392},{\"end\":40412,\"start\":40407},{\"end\":40925,\"start\":40915},{\"end\":40936,\"start\":40932}]", "bib_author_last_name": "[{\"end\":22221,\"start\":22209},{\"end\":22234,\"start\":22229},{\"end\":22247,\"start\":22242},{\"end\":22262,\"start\":22258},{\"end\":22275,\"start\":22269},{\"end\":22287,\"start\":22285},{\"end\":22295,\"start\":22291},{\"end\":22313,\"start\":22306},{\"end\":22319,\"start\":22315},{\"end\":22690,\"start\":22682},{\"end\":22707,\"start\":22701},{\"end\":22721,\"start\":22717},{\"end\":22729,\"start\":22723},{\"end\":22973,\"start\":22965},{\"end\":22991,\"start\":22985},{\"end\":23005,\"start\":22998},{\"end\":23018,\"start\":23013},{\"end\":23034,\"start\":23025},{\"end\":23593,\"start\":23584},{\"end\":23608,\"start\":23600},{\"end\":23624,\"start\":23618},{\"end\":23642,\"start\":23634},{\"end\":24182,\"start\":24171},{\"end\":24193,\"start\":24189},{\"end\":24208,\"start\":24203},{\"end\":24223,\"start\":24216},{\"end\":24240,\"start\":24234},{\"end\":24257,\"start\":24249},{\"end\":24277,\"start\":24266},{\"end\":24291,\"start\":24286},{\"end\":24306,\"start\":24300},{\"end\":24314,\"start\":24308},{\"end\":24666,\"start\":24660},{\"end\":24679,\"start\":24674},{\"end\":24692,\"start\":24687},{\"end\":25153,\"start\":25143},{\"end\":25167,\"start\":25162},{\"end\":25183,\"start\":25177},{\"end\":25201,\"start\":25193},{\"end\":25532,\"start\":25526},{\"end\":25553,\"start\":25543},{\"end\":25573,\"start\":25564},{\"end\":26058,\"start\":26055},{\"end\":26071,\"start\":26069},{\"end\":26082,\"start\":26079},{\"end\":26096,\"start\":26090},{\"end\":26452,\"start\":26446},{\"end\":26467,\"start\":26462},{\"end\":26724,\"start\":26716},{\"end\":26742,\"start\":26732},{\"end\":26975,\"start\":26967},{\"end\":26995,\"start\":26986},{\"end\":27325,\"start\":27323},{\"end\":27339,\"start\":27333},{\"end\":27356,\"start\":27350},{\"end\":27371,\"start\":27364},{\"end\":27381,\"start\":27375},{\"end\":27725,\"start\":27723},{\"end\":27742,\"start\":27736},{\"end\":27756,\"start\":27750},{\"end\":27769,\"start\":27767},{\"end\":27780,\"start\":27777},{\"end\":27794,\"start\":27788},{\"end\":28414,\"start\":28411},{\"end\":28427,\"start\":28425},{\"end\":28444,\"start\":28441},{\"end\":28461,\"start\":28457},{\"end\":28476,\"start\":28469},{\"end\":28494,\"start\":28486},{\"end\":29173,\"start\":29158},{\"end\":29185,\"start\":29181},{\"end\":29190,\"start\":29187},{\"end\":29572,\"start\":29569},{\"end\":29584,\"start\":29582},{\"end\":29595,\"start\":29592},{\"end\":29613,\"start\":29607},{\"end\":29948,\"start\":29939},{\"end\":29965,\"start\":29956},{\"end\":29988,\"start\":29976},{\"end\":30355,\"start\":30346},{\"end\":30372,\"start\":30363},{\"end\":30384,\"start\":30379},{\"end\":30663,\"start\":30654},{\"end\":30682,\"start\":30672},{\"end\":30699,\"start\":30690},{\"end\":31123,\"start\":31117},{\"end\":31143,\"start\":31133},{\"end\":31159,\"start\":31153},{\"end\":31448,\"start\":31442},{\"end\":31468,\"start\":31458},{\"end\":31484,\"start\":31478},{\"end\":31882,\"start\":31876},{\"end\":31898,\"start\":31892},{\"end\":31912,\"start\":31907},{\"end\":32436,\"start\":32430},{\"end\":32450,\"start\":32443},{\"end\":32466,\"start\":32459},{\"end\":32482,\"start\":32475},{\"end\":32498,\"start\":32489},{\"end\":32514,\"start\":32508},{\"end\":33058,\"start\":33054},{\"end\":33076,\"start\":33065},{\"end\":33094,\"start\":33086},{\"end\":33109,\"start\":33103},{\"end\":33408,\"start\":33401},{\"end\":33423,\"start\":33418},{\"end\":33939,\"start\":33932},{\"end\":33954,\"start\":33949},{\"end\":34473,\"start\":34467},{\"end\":34489,\"start\":34483},{\"end\":34503,\"start\":34498},{\"end\":34516,\"start\":34511},{\"end\":35025,\"start\":35017},{\"end\":35040,\"start\":35035},{\"end\":35055,\"start\":35048},{\"end\":35066,\"start\":35063},{\"end\":35563,\"start\":35556},{\"end\":35579,\"start\":35572},{\"end\":35595,\"start\":35589},{\"end\":35613,\"start\":35605},{\"end\":36175,\"start\":36168},{\"end\":36192,\"start\":36183},{\"end\":36200,\"start\":36194},{\"end\":36529,\"start\":36522},{\"end\":36764,\"start\":36755},{\"end\":36780,\"start\":36773},{\"end\":36793,\"start\":36791},{\"end\":36811,\"start\":36804},{\"end\":37252,\"start\":37244},{\"end\":37641,\"start\":37637},{\"end\":37658,\"start\":37649},{\"end\":37677,\"start\":37667},{\"end\":37689,\"start\":37684},{\"end\":37997,\"start\":37994},{\"end\":38010,\"start\":38006},{\"end\":38027,\"start\":38017},{\"end\":38042,\"start\":38037},{\"end\":38051,\"start\":38048},{\"end\":38057,\"start\":38055},{\"end\":38062,\"start\":38059},{\"end\":38070,\"start\":38066},{\"end\":38080,\"start\":38078},{\"end\":38086,\"start\":38082},{\"end\":38476,\"start\":38474},{\"end\":38485,\"start\":38481},{\"end\":38501,\"start\":38494},{\"end\":38516,\"start\":38509},{\"end\":38537,\"start\":38527},{\"end\":38553,\"start\":38544},{\"end\":38573,\"start\":38568},{\"end\":39275,\"start\":39270},{\"end\":39289,\"start\":39283},{\"end\":39901,\"start\":39896},{\"end\":39919,\"start\":39911},{\"end\":39930,\"start\":39927},{\"end\":40390,\"start\":40388},{\"end\":40405,\"start\":40400},{\"end\":40419,\"start\":40413},{\"end\":40930,\"start\":40926},{\"end\":40945,\"start\":40937}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16221853},\"end\":22616,\"start\":22163},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52861858},\"end\":22911,\"start\":22618},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":250340380},\"end\":23510,\"start\":22913},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":250340449},\"end\":24121,\"start\":23512},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b4\"},\"end\":24591,\"start\":24123},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12620927},\"end\":25132,\"start\":24593},{\"attributes\":{\"doi\":\"arXiv:2301.10521\",\"id\":\"b6\"},\"end\":25447,\"start\":25134},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":235792467},\"end\":26046,\"start\":25449},{\"attributes\":{\"doi\":\"arXiv:2212.10496\",\"id\":\"b8\"},\"end\":26351,\"start\":26048},{\"attributes\":{\"id\":\"b9\"},\"end\":26656,\"start\":26353},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1981391},\"end\":26958,\"start\":26658},{\"attributes\":{\"doi\":\"arXiv:2302.12574\",\"id\":\"b11\"},\"end\":27218,\"start\":26960},{\"attributes\":{\"doi\":\"ArXiv abs/2108.11044\",\"id\":\"b12\"},\"end\":27607,\"start\":27220},{\"attributes\":{\"doi\":\"ECIR 2022\",\"id\":\"b13\",\"matched_paper_id\":245124479},\"end\":28289,\"start\":27609},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235366815},\"end\":29048,\"start\":28291},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235720578},\"end\":29559,\"start\":29050},{\"attributes\":{\"doi\":\"arXiv:2210.07093\",\"id\":\"b16\"},\"end\":29932,\"start\":29561},{\"attributes\":{\"doi\":\"arXiv:2108.04026\",\"id\":\"b17\"},\"end\":30297,\"start\":29934},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":244709531},\"end\":30606,\"start\":30299},{\"attributes\":{\"doi\":\"10.1145/3511808.3557231\",\"id\":\"b19\",\"matched_paper_id\":251643649},\"end\":31022,\"start\":30608},{\"attributes\":{\"doi\":\"arXiv:2305.07477\",\"id\":\"b20\"},\"end\":31377,\"start\":31024},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":258332122},\"end\":31797,\"start\":31379},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":234741735},\"end\":32376,\"start\":31799},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":248665509},\"end\":32988,\"start\":32378},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10929365},\"end\":33341,\"start\":32990},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1118305},\"end\":33870,\"start\":33343},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14811099},\"end\":34403,\"start\":33872},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219162111},\"end\":34944,\"start\":34405},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":212725651},\"end\":35488,\"start\":34946},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":254854129},\"end\":36066,\"start\":35490},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2218552},\"end\":36395,\"start\":36068},{\"attributes\":{\"id\":\"b31\"},\"end\":36686,\"start\":36397},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":216868342},\"end\":37194,\"start\":36688},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":59910946},\"end\":37540,\"start\":37196},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":253763641},\"end\":37915,\"start\":37542},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":246411621},\"end\":38384,\"start\":37917},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":245218563},\"end\":39229,\"start\":38386},{\"attributes\":{\"doi\":\"10.1145/2808194.2809446\",\"id\":\"b37\",\"matched_paper_id\":7311615},\"end\":39828,\"start\":39231},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":222310837},\"end\":40292,\"start\":39830},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":237363901},\"end\":40832,\"start\":40294},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1043470},\"end\":41338,\"start\":40834}]", "bib_title": "[{\"end\":22199,\"start\":22163},{\"end\":22678,\"start\":22618},{\"end\":22954,\"start\":22913},{\"end\":23577,\"start\":23512},{\"end\":24650,\"start\":24593},{\"end\":25515,\"start\":25449},{\"end\":26706,\"start\":26658},{\"end\":27716,\"start\":27609},{\"end\":28403,\"start\":28291},{\"end\":29145,\"start\":29050},{\"end\":30339,\"start\":30299},{\"end\":30647,\"start\":30608},{\"end\":31435,\"start\":31379},{\"end\":31869,\"start\":31799},{\"end\":32423,\"start\":32378},{\"end\":33046,\"start\":32990},{\"end\":33392,\"start\":33343},{\"end\":33923,\"start\":33872},{\"end\":34456,\"start\":34405},{\"end\":35007,\"start\":34946},{\"end\":35549,\"start\":35490},{\"end\":36164,\"start\":36068},{\"end\":36744,\"start\":36688},{\"end\":37234,\"start\":37196},{\"end\":37630,\"start\":37542},{\"end\":37986,\"start\":37917},{\"end\":38466,\"start\":38386},{\"end\":39260,\"start\":39231},{\"end\":39887,\"start\":39830},{\"end\":40376,\"start\":40294},{\"end\":40913,\"start\":40834}]", "bib_author": "[{\"end\":22223,\"start\":22201},{\"end\":22236,\"start\":22223},{\"end\":22249,\"start\":22236},{\"end\":22264,\"start\":22249},{\"end\":22277,\"start\":22264},{\"end\":22289,\"start\":22277},{\"end\":22297,\"start\":22289},{\"end\":22315,\"start\":22297},{\"end\":22321,\"start\":22315},{\"end\":22692,\"start\":22680},{\"end\":22709,\"start\":22692},{\"end\":22723,\"start\":22709},{\"end\":22731,\"start\":22723},{\"end\":22975,\"start\":22956},{\"end\":22993,\"start\":22975},{\"end\":23007,\"start\":22993},{\"end\":23020,\"start\":23007},{\"end\":23036,\"start\":23020},{\"end\":23595,\"start\":23579},{\"end\":23610,\"start\":23595},{\"end\":23626,\"start\":23610},{\"end\":23644,\"start\":23626},{\"end\":24184,\"start\":24162},{\"end\":24195,\"start\":24184},{\"end\":24210,\"start\":24195},{\"end\":24225,\"start\":24210},{\"end\":24242,\"start\":24225},{\"end\":24259,\"start\":24242},{\"end\":24279,\"start\":24259},{\"end\":24293,\"start\":24279},{\"end\":24308,\"start\":24293},{\"end\":24316,\"start\":24308},{\"end\":24668,\"start\":24652},{\"end\":24681,\"start\":24668},{\"end\":24694,\"start\":24681},{\"end\":25155,\"start\":25134},{\"end\":25169,\"start\":25155},{\"end\":25185,\"start\":25169},{\"end\":25203,\"start\":25185},{\"end\":25534,\"start\":25517},{\"end\":25555,\"start\":25534},{\"end\":25575,\"start\":25555},{\"end\":26060,\"start\":26050},{\"end\":26073,\"start\":26060},{\"end\":26084,\"start\":26073},{\"end\":26098,\"start\":26084},{\"end\":26454,\"start\":26439},{\"end\":26469,\"start\":26454},{\"end\":26726,\"start\":26708},{\"end\":26744,\"start\":26726},{\"end\":26977,\"start\":26960},{\"end\":26997,\"start\":26977},{\"end\":27327,\"start\":27318},{\"end\":27341,\"start\":27327},{\"end\":27358,\"start\":27341},{\"end\":27373,\"start\":27358},{\"end\":27383,\"start\":27373},{\"end\":27727,\"start\":27718},{\"end\":27744,\"start\":27727},{\"end\":27758,\"start\":27744},{\"end\":27771,\"start\":27758},{\"end\":27782,\"start\":27771},{\"end\":27796,\"start\":27782},{\"end\":28416,\"start\":28405},{\"end\":28429,\"start\":28416},{\"end\":28446,\"start\":28429},{\"end\":28463,\"start\":28446},{\"end\":28478,\"start\":28463},{\"end\":28496,\"start\":28478},{\"end\":29175,\"start\":29147},{\"end\":29187,\"start\":29175},{\"end\":29192,\"start\":29187},{\"end\":29574,\"start\":29561},{\"end\":29586,\"start\":29574},{\"end\":29597,\"start\":29586},{\"end\":29615,\"start\":29597},{\"end\":29950,\"start\":29934},{\"end\":29967,\"start\":29950},{\"end\":29990,\"start\":29967},{\"end\":30357,\"start\":30341},{\"end\":30374,\"start\":30357},{\"end\":30386,\"start\":30374},{\"end\":30665,\"start\":30649},{\"end\":30684,\"start\":30665},{\"end\":30701,\"start\":30684},{\"end\":31125,\"start\":31112},{\"end\":31145,\"start\":31125},{\"end\":31161,\"start\":31145},{\"end\":31450,\"start\":31437},{\"end\":31470,\"start\":31450},{\"end\":31486,\"start\":31470},{\"end\":31884,\"start\":31871},{\"end\":31900,\"start\":31884},{\"end\":31914,\"start\":31900},{\"end\":32438,\"start\":32425},{\"end\":32452,\"start\":32438},{\"end\":32468,\"start\":32452},{\"end\":32484,\"start\":32468},{\"end\":32500,\"start\":32484},{\"end\":32516,\"start\":32500},{\"end\":33060,\"start\":33048},{\"end\":33078,\"start\":33060},{\"end\":33096,\"start\":33078},{\"end\":33111,\"start\":33096},{\"end\":33410,\"start\":33394},{\"end\":33425,\"start\":33410},{\"end\":33941,\"start\":33925},{\"end\":33956,\"start\":33941},{\"end\":34475,\"start\":34458},{\"end\":34491,\"start\":34475},{\"end\":34505,\"start\":34491},{\"end\":34518,\"start\":34505},{\"end\":35027,\"start\":35009},{\"end\":35042,\"start\":35027},{\"end\":35057,\"start\":35042},{\"end\":35068,\"start\":35057},{\"end\":35565,\"start\":35551},{\"end\":35581,\"start\":35565},{\"end\":35597,\"start\":35581},{\"end\":35615,\"start\":35597},{\"end\":36177,\"start\":36166},{\"end\":36194,\"start\":36177},{\"end\":36202,\"start\":36194},{\"end\":36531,\"start\":36515},{\"end\":36766,\"start\":36746},{\"end\":36782,\"start\":36766},{\"end\":36795,\"start\":36782},{\"end\":36813,\"start\":36795},{\"end\":37254,\"start\":37236},{\"end\":37643,\"start\":37632},{\"end\":37660,\"start\":37643},{\"end\":37679,\"start\":37660},{\"end\":37691,\"start\":37679},{\"end\":37999,\"start\":37988},{\"end\":38012,\"start\":37999},{\"end\":38029,\"start\":38012},{\"end\":38044,\"start\":38029},{\"end\":38053,\"start\":38044},{\"end\":38059,\"start\":38053},{\"end\":38064,\"start\":38059},{\"end\":38072,\"start\":38064},{\"end\":38082,\"start\":38072},{\"end\":38088,\"start\":38082},{\"end\":38478,\"start\":38468},{\"end\":38487,\"start\":38478},{\"end\":38503,\"start\":38487},{\"end\":38518,\"start\":38503},{\"end\":38539,\"start\":38518},{\"end\":38555,\"start\":38539},{\"end\":38575,\"start\":38555},{\"end\":39277,\"start\":39262},{\"end\":39291,\"start\":39277},{\"end\":39903,\"start\":39889},{\"end\":39921,\"start\":39903},{\"end\":39932,\"start\":39921},{\"end\":40392,\"start\":40378},{\"end\":40407,\"start\":40392},{\"end\":40421,\"start\":40407},{\"end\":40932,\"start\":40915},{\"end\":40947,\"start\":40932}]", "bib_venue": "[{\"end\":22375,\"start\":22321},{\"end\":22755,\"start\":22731},{\"end\":23147,\"start\":23036},{\"end\":23755,\"start\":23644},{\"end\":24160,\"start\":24123},{\"end\":24803,\"start\":24694},{\"end\":25265,\"start\":25219},{\"end\":25686,\"start\":25575},{\"end\":26437,\"start\":26353},{\"end\":26790,\"start\":26744},{\"end\":27064,\"start\":27013},{\"end\":27316,\"start\":27220},{\"end\":27879,\"start\":27805},{\"end\":28607,\"start\":28496},{\"end\":29258,\"start\":29192},{\"end\":29726,\"start\":29631},{\"end\":30095,\"start\":30006},{\"end\":30430,\"start\":30386},{\"end\":30797,\"start\":30724},{\"end\":31110,\"start\":31024},{\"end\":31578,\"start\":31486},{\"end\":32025,\"start\":31914},{\"end\":32627,\"start\":32516},{\"end\":33146,\"start\":33111},{\"end\":33543,\"start\":33425},{\"end\":34074,\"start\":33956},{\"end\":34618,\"start\":34518},{\"end\":35164,\"start\":35068},{\"end\":35710,\"start\":35615},{\"end\":36210,\"start\":36202},{\"end\":36513,\"start\":36397},{\"end\":36895,\"start\":36813},{\"end\":37309,\"start\":37254},{\"end\":37718,\"start\":37691},{\"end\":38137,\"start\":38088},{\"end\":38661,\"start\":38575},{\"end\":39413,\"start\":39314},{\"end\":40014,\"start\":39932},{\"end\":40511,\"start\":40421},{\"end\":41036,\"start\":40947},{\"end\":23245,\"start\":23149},{\"end\":23853,\"start\":23757},{\"end\":24899,\"start\":24805},{\"end\":25784,\"start\":25688},{\"end\":27898,\"start\":27881},{\"end\":28705,\"start\":28609},{\"end\":29311,\"start\":29260},{\"end\":32123,\"start\":32027},{\"end\":32725,\"start\":32629},{\"end\":33648,\"start\":33545},{\"end\":34179,\"start\":34076},{\"end\":35247,\"start\":35166},{\"end\":35727,\"start\":35712},{\"end\":36964,\"start\":36897},{\"end\":37373,\"start\":37311},{\"end\":38765,\"start\":38663},{\"end\":39516,\"start\":39415},{\"end\":40083,\"start\":40016},{\"end\":40588,\"start\":40513},{\"end\":41112,\"start\":41038}]"}}}, "year": 2023, "month": 12, "day": 17}