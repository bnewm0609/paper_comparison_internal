{"id": 235593383, "updated": "2023-10-06 02:10:44.838", "metadata": {"title": "A Vertical Federated Learning Framework for Graph Convolutional Network", "authors": "[{\"first\":\"Xiang\",\"last\":\"Ni\",\"middle\":[]},{\"first\":\"Xiaolong\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Lingjuan\",\"last\":\"Lyu\",\"middle\":[]},{\"first\":\"Changhua\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Weiqiang\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 6, "day": 22}, "abstract": "Recently, Graph Neural Network (GNN) has achieved remarkable success in various real-world problems on graph data. However in most industries, data exists in the form of isolated islands and the data privacy and security is also an important issue. In this paper, we propose FedVGCN, a federated GCN learning paradigm for privacy-preserving node classification task under data vertically partitioned setting, which can be generalized to existing GCN models. Specifically, we split the computation graph data into two parts. For each iteration of the training process, the two parties transfer intermediate results to each other under homomorphic encryption. We conduct experiments on benchmark data and the results demonstrate the effectiveness of FedVGCN in the case of GraphSage.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.11593", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2106-11593", "doi": null}}, "content": {"source": {"pdf_hash": "7257f55bfd06780b46b07420ef390fced4b7e5ec", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.11593v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3c4befa4e226b87a57fed65f1385ec30fa36474e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7257f55bfd06780b46b07420ef390fced4b7e5ec.txt", "contents": "\nA Vertical Federated Learning Framework for Graph Convolutional Network\n\n\nXiang Ni nixiang85@gmail.com \nAnt Group\n\n\nXiaolong Xu \nAnt Group\n\n\nLinjuan Lyu \nAnt Group\n\n\nChanghua Meng \nAnt Group\n\n\nWeiqiang Wang weiqiang.wwq@antgroup.com \nAnt Group\n\n\nA Vertical Federated Learning Framework for Graph Convolutional Network\n\nRecently, Graph Neural Network (GNN) has achieved remarkable success in various real-world problems on graph data. However in most industries, data exists in the form of isolated islands and the data privacy and security is also an important issue. In this paper, we propose FedVGCN, a federated GCN learning paradigm for privacy-preserving node classification task under data vertically partitioned setting, which can be generalized to existing GCN models. Specifically, we split the computation graph data into two parts. For each iteration of the training process, the two parties transfer intermediate results to each other under homomorphic encryption. We conduct experiments on benchmark data and the results demonstrate the effectiveness of FedVGCN in the case of GraphSage.\n\nIntroduction\n\nThe protection of user privacy is an important concern in machine learning, as evidenced by the rolling out of the General Data Protection Regulation (GDPR) in the European Union (EU) in May 2018 ( [Chi et al., 2018]). The GDPR is designed to give users to protect their personal data, which motivates us to explore machine learning frameworks with data sharing while not violating user privacy ([Balle and Wang, 2018;Bonawitz et al., 2019]).\n\nPrivacy is an important challenge when data aggregation and collaborative learning happens across different entities [Lyu et al., 2020b;Lyu et al., 2020a]. To address this privacy issue many endeavors have been taken in different directions, among which, two important techniques are differential privacy ( [Acar et al., 2018;Aono et al., 2016]) and fully homomorphic encryption ([Zhang et al., 2015]). Recent advance in fully homomorphic encryption (FHE) ([Yuan and Yu, 2013]) allows users to encrypt data with the public key and offload computation to the cloud. The cloud computes on the encrypted data and generates encrypted results. Without the secret key, cloud simply serves as a computation platform but cannot access any user information. This powerful technique has been integrated with deep learning in the pioneering work of convolutional neural network ([Lin et al., 2013;Veidinger, 1960]), known as CryptoNets.\n\nMoreover, federated Learning [McMahan et al., 2017] provides a privacy-aware solution for scenarios where data is sensitive (e.g., biomedical records, private images, personal text and speech, and personally identifiable information like location, purchase etc.). Federated learning allows multiple clients to train a shared model without collecting their data. The model training is conducted by aggregating locally-computed updates and the data in clients will not be transferred to anywhere for data privacy. In vertical setting, two data sets share the same sample ID space but differ in feature space [Hardy et al., 2017;Yang et al., 2019]. For example, consider two different companies in the same city, one is a bank, and the other is an e-commerce company. Their user sets are likely to contain most of the residents of the area, so the intersection of their user space is large. Vertical federated learning on logistic regression, xgboost, multi-tasking learning, neural network, transfer learning, etc, have been previously studied ([Acar et al., 2018;Mohassel and Zhang, 2017;Kone\u010dn\u1ef3 et al., 2016;Nock et al., 2018]). However, few research has studied how to train federated GNNs in a privacy-preserving manner when data are vertically partitioned, which popularly exists in practice. To fill in this gap, in this paper, we propose a vertical federated learning framework on graph convolutional network (GCN). In particular, we test our ideas on GraphSage ( [Chen et al., 2020b;Krizhevsky et al., 2009]).\n\nOur main contributions are the following:\n\n\u2022 We introduce a vertical federated learning algorithm for graph convolutional network in a privacy-preserving setting to provide solutions for federation problems beyond the scope of existing federated learning approaches;\n\n\u2022 We provide a novel approach which adopts additively homomorphic encryption (HE) to ensure privacy, while maintaining accuracy. Experimental results on three benchmark datasets demonstrate that our algorithm significantly outperforms the GNN models trained on the isolated data and achieves comparable performance with the traditional GNN trained on the combined plaintext data. \n< \u03a8 i , \u03a8 j >= b a \u03a8 i (x)\u03a8 j (x)w(x)dx = \u03b4 ij(1)\n\nLeast-Squares Approximation\n\nMovivation: Suppose f \u2208 C[a, b], find a orthogonal polynomial P n (x) of degree at most n to approximate f such that b a (f (x) \u2212 P n (x)) 2 dx is a minimum ([Ali, 2020;Carothers, 1998]).\n\nLet polynomial P n (x) be P n (x) = n k=0 a k x k which minimizes the error\nE = E(a 0 , a 1 , ..., a n ) = b a (f (x) \u2212 n k=0 (a k x k ) 2 dx (2)\nThe problem is to find a 0 , ..., a n that will minimize E. The necessary condition for a 0 , ..., a n to minimize E is \u2202E \u2202aj = 0, which gives the normal equations:\nn k=0 a k b a x j+k dx = b a x j f (x)dx for j = 0, 1, ..., n.\n(3) We can now find a least-square polynomial approximation of the form\np n (x) = n i=0 b i \u03a8 i (x)(4)\nwhere {\u03a8 i } i\u2208{0,1...,n} is a set of orthogonal polynomials. The Legendre polynomials {P 0 (x) = 1, P 1 (x) = x, P 2 (x) = 3 2 x 2 \u2212 1 2 , ... is a set of orthogonal functions with respect to the weight function w(x) = 1 over the interval [\u22121, 1]. In this case, the coefficients of the least-squares polynomial approximation p n are expressed as follows\nb i = 1 C i f (x)P i (x)dx, i \u2208 {0, 1, ..., n}(5)\nBased on this approach, we can approximate the ReLU function using a polynomial of degree two as\np(x) = 4 3\u03c0a x 2 + 1 2 x + a 2\u03c0 (6)\nwhere a is determined by the data you are working on.\n\n\nPaillier Homomorphic Encryption\n\nHomomorphic encryption allows secure computation over encrypted data. To cope with operations in deep neural nets (mainly multiplication and addition), we adopt a well-known partially homomorphic encryption system called Paillier ( [Chen et al., 2020a;Chi et al., 2018] \ns 1 ) \u2295 ([[M 2 ]] \u2297 s 2 ) \u2295 \u00b7 \u00b7 \u00b7 \u2295 ([[M g ]] \u2297 s g ). without knowing the plaintext message. Here, [[M g ]]\nrepresents the ciphertext. \u2295 is the homomorphic addition with ciphertext and \u2297 is the homomorphic multiplication between a ciphertext and a scalar constant ( [Barni et al., 2006;Balle and Wang, 2018;Chi et al., 2018]).\n\n\nGraph Convolutional Neural Networks\n\nGraph convolutional network (GCN) [Kipf and Welling, 2017] generalizes the operation of convolution from grid data to graph data. The main idea is to generate a node v's representation by aggregating its own features x v and neighbors' features x u , where u \u2208 N (v). Different from Recurrent Graph Neural Network, GCN stacks multiple graph convolutional layers to extract high-level node representations. GCN plays a central role in building up many other complex GNN models ( [Chen et al., 2020b]). Moreover, many other popular algorithms, such as Graph attention networks (GAT, shown in Figure 1) [Veli\u010dkovi\u0107 et al., 2018], and Graph-SAGE (shown in Figure 2) ), typically perform well in graph tasks. However, GCN is the transductive method which is not suitable for large graphs.\n\nThe main difference between different GNN methods is the aggregation process. The general aggregation function can be written as follows:\nh l i = w l i,i\u0125 l\u22121 i + w l i,j\u0125 l\u22121 j (i = j)(7)\nHere,\u0125 l i is the hidden status in l-th layer for the i-th node. Different aggregation functions define different w i,i and w i,j . In GraphSAGE method, the aggregation function is mean. And w i,j is calculated by adopting the attention mechanism.\n\nAdditively homomorphic encryption and polynomial approximations have been widely used for privacy-preserving machine learning, and the trade-offs between efficiency and privacy by adopting such approximations have been discussed intensively [Ali, 2020]. Here we use a second order Taylor approximation for loss and gradients computations:\n\nDefine\nL(w 1 , w 2 ) = p(w 1 h 1 + w 2 h 2 ) (8) Then \u2202L \u2202w 1 = 8h 3\u03c0a (w 1 h 1 + w 2 h 2 ) * h 1 + 1 2 h 1 (9) \u2202L \u2202w 2 = 8h 3\u03c0a (w 1 h 1 + w 2 h 2 ) * h 2 + 1 2 h 2(10)\nApplying the Homomorphic encryption to L and \u2202L \u2202wi gives\n[[L]] = 4 3\u03c0a [[(w 1 h 1 + w 2 h 2 ) 2 ]] + 1 2 [[(w 1 h 1 + w 2 h 2 )]] + a 2\u03c0 (11) [[L A ]] = 4 3\u03c0a [[(w 1 h 1 ) 2 ]] + 1 2 [[(w 1 h 1 ]] + a 4\u03c0 (12) [[L B ]] = 4 3\u03c0a [[(w 2 h 2 ) 2 ]] + 1 2 [[(w 2 h 2 ]] + a 4\u03c0 (13) [[L AB ]] = 4 3\u03c0a [[2w 1 h 1 w 2 h 2 ]](14)\n[\n[L]] = [[L A ]] + [[L B ]] + [[L AB ]](15)\nand\n[[ \u2202L \u2202w 1 ]] = 8h 3\u03c0a [[(w 1 h 1 + w 2 h 2 ) * h 1 ]] + 1 2 [[h 1 ]] (16) [[ \u2202L \u2202w 2 ]] = 8h 3\u03c0a [[(w 1 h 1 + w 2 h 2 ) * h 2 ]] + 1 2 [[h 2 ]](17)\nHere [[x]] is the ciphertext of x. The reason to use quadratic orthogonal polynomials to approximate/replace the relu activation is to preserve the multiplication/addition under homomorphic encryption. Suppose that companies A and B would like to jointly train a machine learning model, and their business systems each have their own data. In addition, Company B also has label data that the model needs to predict. We call company A the passive party and the company B the active party. For data privacy and security reasons, the passive party and the active party cannot directly exchange data. In order to ensure the confidentiality of the data during the training process, a thirdparty collaborator C is involved, which is called the server party. Here we assume the server party is honest-but-curious and does not collude with the passive or the active party, but the passive and active party are honest-but-curious to each other. The passive party trusted the server party is a reasonable assumption since the server party can be played by authorities such as governments or replaced by secure computing node such as Intel Software Guard Extensions (SGX) ([Costan and Devadas, 2016]).\n\n\nAlgorithm 1 FedVGCN: Forward Propagation\n\nActive Party compute w 1 * h 1 and N 1 , and send to the server party Passive Party compute w 2 * h 2 and N 2 , and send to the server party Server Party compute [[w 1 * h 1 + w 2 * h 2 ]] and decrypt them to the active party and the passive party\n\n\nUnsupervised loss function\n\nExisting FL algorithms are mainly for supervised learning. In order to learn useful, predictive representations in a fully unsupervised setting, we adopt the loss function in [Hamiltonm, 2017]. That is, the graph-based loss function encourages nearby nodes to have similar representations, while enforcing that the representations of disparate nodes are highly distinct:\nJ G (z u ) = \u2212log(\u03c3(z T u z v )) \u2212 Q \u00b7 E V N Pn(v) log(\u03c3(\u2212z T u z vn ))(18)\nwhere v is a node that co-occurs near u on fixed-length random walk, \u03c3 is the sigmoid function, P n is a negative sampling distribution, and Q defines the number of negative samples.\n\n\nAlgorithms\n\nIn isolated GNNs, both node features and edges are hold by different parties. The first step under vertically data split setting is secure ID alignment, also known as Private Set Intersection (PSI) [Pinkas et al., 2014]. That is, data holders align their nodes without exposing those that do not overlap with each other. In this work, we assume data holders have aligned their nodes beforehand and are ready for performing privacy preserving GNN training.\n\nIn the case of GCN, the forward propagation and backward propagation of our proposed vertical federated algorithm are presented in Algorithm 1 and Algorithm 2 respectively. Here N i is the number of edges at party i. The backward propagation algorithm can be illustrated by Figure 3.\n\n\nSecurity Analysis\n\nTheorem 4.1. If the number of samples is much greater than the number of features, then the algorithm FedVGCN is secure.\n\nProof. In the above protocol, the passive party learns its gradient at each step, but this is not enough for the passive party to learn any information from the active party, because the security of scalar product protocol is well-established based on the inability of solving n equations in more than n unknowns. Here we assume the number of samples N A is much greater than n A , where n A is the number of features. Similarly, the active party can not learn any information from the passive party. Therefore the security of the protocol is proved.\n\n\nComplexity Analysis\n\n\nCommunication Cost\n\nThe communication cost is analyzed as the total number of messages transmitted between the client and server. We assume a unit message size for encrypted data. For an n-layer  \n\n\nServer Party\n\nThe Server party decrypts L, send \u2202L \u2202w1 + \u03c3 A to the passive party and \u2202L \u2202w2 + \u03c3 B to the active party. return w 1 , w 2 network, in the forward propagation, the communication cost is 2m(n \u2212 1), where m is the number of activations in a layer. This is because total m ciphertexts need to be transmitted by the client and the server, for the sum of inputs z i and activations a i , respectively, at each layer. In the backpropagation, the client needs at most 7 * (m 2 + m + 1) messages for model updates between two consecutive layers. Except the final layer, the client interacts with the server to calculate the gradients, which requires transmitting encrypted error [[\u03b4 i + 1]]c in m messages between client and server for each layer. Summing up the costs from the forward and backpropagation, the entire network requires O(nm 2 ) communication messages for an iteration.\n\n\nComputation Cost\n\nArithmetic multiplications and additions are mapped to modular exponentiations and modular multiplications over ciphertext, respectively. Here, we denote such cost of conducting homomorphic arithemtics in Paillier by p. For n layers, both forward and backpropagations take O(nm 2 p) so the total computation cost is O(nm 2 p).\n\n\nExperiments\n\nFor experiments, we mainly focus on GraphSage algorithm for illustration purpose (in this case, we use FedVGraphSage to denote FedVGCN). This part aims to answer the following questions:\n\n\u2022 Q1: whether FedVGraphSage outperforms the Graph-Sage models that are trained on the isolated data.\n\n\u2022 Q2: how does FedVGraphSage behave comparing with the centralized but insecure model that is trained on the plaintext mixed data.\n\n\nExperimental Setup\n\nWe first describe the datasets, comparison methods, and parameter settings used in our experiments. Datasets. We use three benchmark datasets which are popularly used to evaluate the performance of GNN, i.e., Cora, Pubmed, and Citeseer ([Balle and Wang, 2018]), as shown in Table 1. We split node features and edges randomly and assume they are hold by two parties.\n\nComparison methods. We compare FedVGraphSage with GraphSAGE models ( [Chen et al., 2020b]) that are trained using isolated data and mixed plaintext data respectively to answer Q1 and Q2. For all the experiments, we split the datasets randomly, use five-fold cross validation and adopt accuracy as the evaluation metric.\n\nParameter settings. For the models which are trained on isolated data, we use relu as the activation function. For the deep neural network on server, we set the dropout rate to 0.5 and network structure as (64, 64, |C|), where |C| is the number of classes. We set the learning rate as 10 \u22125 .  Analysis: GraphSage A and GraphSage B can only use partial feature and edge information hold by A and B, respectively. In contrast, FedVGraphSage provides a solution for A and B to train GraghSages collaboratively without compromising their own data. By doing this, FedVGraphSage can use the information from the data of both A and B simultaneously, and therefore achieve better performance.\n\nAnswer to Q2: We then compare FedVGraphSage with GraphSage A+B that is trained on the mixed plaintext data. It can be seen from Table 2 that FedVGraphSage has comparable performance with GraphSage A+B , e.g., 0.6770 vs. 0.7080 on Cora dataset, 0.7830 vs. 0.7890 on Pubmed dataset, 0.6820 vs. 0.6983 on Citeseer dataset.\n\nAnalysis: First, we use Paillier Homomorphic Encryption for A and B to securely communicate intermidiate results, as described in Algorithm 1 and Algorithm 2. Second, we use the quadratic orthogonal polynomials activation functions to preserve the sum and multiplication operations which acts on the encrypted results. Therefore, FedVGraphSage has comparable performance with GraphSage A+B .\n\n\nConclusion\n\nIn this work, we introduce a novel vertical federated learning algorithm for graph neural network. We adopt additively homomorphic encryption to ensure privacy, while maintaining accuracy. Experimental results on three benchmark datasets demonstrate that our algorithm significantly outperforms the GNN models trained on the isolated data and has comparable performance with the traditional GNN trained on the combined plaintext data. In this paper, we mainly investigate the vertical federated GNN by adopting GraphSAGE, and in the future, we will extend our method to more complex GNN models, such as GAT, GIN, etc. We also plan to apply our method to real industrial applications.\n\nFigure 1 :\n1A Sample of GAT.\n\nFigure 2 :\n2A Sample of GraphSAGE.\n\nFigure 3 :\n3FedVGCN\n\n\nDataset #Node #Edge #Feature #ClassesCora \n2708 \n5409 \n1433 \n7 \nPubmed 19717 44338 \n500 \n3 \nCiteseer \n3327 \n4732 \n3703 \n6 \n\nTable 1: Dataset statistics \n\nDataset \n\nCora \nPubmed Citeseer \n\nGraphSage A \n0.5222 0.6936 \n0.4630 \nGraphSage B \n0.4867 0.6801 \n0.5510 \nFedVGraphSage 0.6770 0.7830 \n0.6820 \nGraphSage A+B 0.7080 0.7890 \n0.6983 \n\n\n\nTable 2 :\n2Accuracy comparison on three datasets 6.2 Comparison Results and Analysis Answer to Q1: We compare FedVGraphSage with the Graph-Sages that are trained on the isolated feature and edge data, i.e., GraphSage A and GraphSage B . From Table 2, we can find that, FedVGraphSage significantly outperforms GraphSage A and GraphSage B on all the three datasets. Take Cora for example, our FedVGraphSage improves GraphSage A and GraphSage B by as high as 29.6% and 39.1%, in terms of accuracy.\n\nA survey on homomorphic encryption schemes: Theory and implementation. Acar, Ramy E. On polynomial approximations for privacypreserving and verifiable relu networks. Jinhyun Avestimehr A. Salman Ali51Ali, 2020References [Acar et al., 2018] Abbas Acar, Hidayet Aksu, A Selcuk Ulu- agac, and Mauro Conti. A survey on homomorphic encryp- tion schemes: Theory and implementation. ACM Comput- ing Surveys (CSUR), 51(4):1-35, 2018. [Ali, 2020] So Jinhyun Avestimehr A. Salman Ali, Ramy E. On polynomial approximations for privacy- preserving and verifiable relu networks. arXiv preprint https://arxiv.org/abs/2011.05530, 2020.\n\nImproving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. [ Aono, Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy. the Sixth ACM Conference on Data and Application Security and PrivacyPMLRInternational Conference on Machine Learning[Aono et al., 2016] Yoshinori Aono, Takuya Hayashi, Le Trieu Phong, and Lihua Wang. Scalable and secure logistic regression via homomorphic encryption. In Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy, pages 142-144, 2016. [Balle and Wang, 2018] Borja Balle and Yu-Xiang Wang. Im- proving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In Interna- tional Conference on Machine Learning, pages 394-403. PMLR, 2018.\n\nA privacy-preserving protocol for neuralnetwork-based computation. [ Barni, arXiv:1902.01046Proceedings of the 8th workshop on Multimedia and security. the 8th workshop on Multimedia and securityarXiv preprintTowards federated learning at scale: System design[Barni et al., 2006] Mauro Barni, Claudio Orlandi, and Alessandro Piva. A privacy-preserving protocol for neural- network-based computation. In Proceedings of the 8th workshop on Multimedia and security, pages 146-151, 2006. [Bonawitz et al., 2019] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone\u010dn\u1ef3, Stefano Mazzocchi, H Brendan McMahan, et al. Towards fed- erated learning at scale: System design. arXiv preprint arXiv:1902.01046, 2019.\n\n; Carothers, Neal L Carothers, A short course on approximation theory. Department of Mathematics and Statistics. Bowling green State UniversityCarothers, 1998] Neal L Carothers. A short course on ap- proximation theory. Department of Mathematics and Statis- tics, Bowling green State University, 1998.\n\nSecure social recommendation based on secret sharing. [ Chen, arXiv:2002.02088arXiv preprint[Chen et al., 2020a] Chaochao Chen, Liang Li, Bingzhe Wu, Cheng Hong, Li Wang, and Jun Zhou. Secure social rec- ommendation based on secret sharing. arXiv preprint arXiv:2002.02088, 2020.\n\nFede: Embedding knowledge graphs in federated setting. [ Chen, arXiv:2010.12882arXiv preprint[Chen et al., 2020b] Mingyang Chen, Wen Zhang, Zonggang Yuan, Yantao Jia, and Huajun Chen. Fede: Embedding knowledge graphs in federated setting. arXiv preprint arXiv:2010.12882, 2020.\n\nPrivacy partitioning: Protecting user data during the deep learning inference phase. [ Chi, arXiv:1812.02863arXiv preprint[Chi et al., 2018] Jianfeng Chi, Emmanuel Owusu, Xuwang Yin, Tong Yu, William Chan, Patrick Tague, and Yuan Tian. Privacy partitioning: Protecting user data during the deep learning inference phase. arXiv preprint arXiv:1812.02863, 2018.\n\nVictor Costan and Srinivas Devadas. Intel sgx explained. IACR Cryptol. ePrint Arch. 86Costan and Devadas[Costan and Devadas, 2016] Victor Costan and Srinivas De- vadas. Intel sgx explained. IACR Cryptol. ePrint Arch., 2016(86):1-118, 2016.\n\nInductive representation learning on large graphs. Ying Rexleskovec Jure Hamiltonm, L William, [Hamiltonm, 2017] Ying RexLeskovec Jure Hamiltonm, William L. Inductive representation learning on large graphs. arXiv preprint https://arxiv.org/abs/1706.02216, 2017.\n\nPrivate federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption. [ Hardy, arXiv:1711.10677arXiv preprint[Hardy et al., 2017] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity resolution and additively ho- momorphic encryption. arXiv preprint arXiv:1711.10677, 2017.\n\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. Kipf, N Welling ; Thomas, Max Kipf, ; Welling, Kone\u010dn\u1ef3, arXiv:1610.02527arXiv:1803.04035Federated optimization: Distributed machine learning for on-device intelligence. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettYuan and Yu2arXiv preprintIEEE Transactions on Computers[Kipf and Welling, 2017] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2017. [Kone\u010dn\u1ef3 et al., 2016] Jakub Kone\u010dn\u1ef3, H Brendan McMa- han, Daniel Ramage, and Peter Richt\u00e1rik. Federated opti- mization: Distributed machine learning for on-device intel- ligence. arXiv preprint arXiv:1610.02527, 2016. [Krizhevsky et al., 2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [Lin et al., 2013] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013. [Lyu et al., 2020a] Lingjuan Lyu, Han Yu, Xingjun Ma, Lichao Sun, Jun Zhao, Qiang Yang, and Philip S Yu. Pri- vacy and robustness in federated learning: Attacks and defenses. arXiv preprint arXiv:2012.06337, 2020. [Lyu et al., 2020b] Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning: A survey. arXiv preprint arXiv:2003.02133, 2020. [McMahan et al., 2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273-1282, 2017. [Mohassel and Zhang, 2017] Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine learning. In 2017 IEEE Symposium on Security and Privacy (SP), pages 19-38. IEEE, 2017. [Nock et al., 2018] Richard Nock, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Giorgio Patrini, Guillaume Smith, and Brian Thorne. Entity resolution and feder- ated learning get a federated resolution. arXiv preprint arXiv:1803.04035, 2018. [Pinkas et al., 2014] Benny Pinkas, Thomas Schneider, and Michael Zohner. Faster private set intersection based on {OT} extension. In 23rd {USENIX} Security Symposium ({USENIX} Security 14), pages 797-812, 2014. [Veidinger, 1960] L Veidinger. On the numerical determina- tion of the best approximations in the chebyshev sense. Numerische Mathematik, 2(1):99-105, 1960. [Veli\u010dkovi\u0107 et al., 2018] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks, 2018. [Will et al., 2017] Hamilton Will, Ying Zhitao, and Leskovec Jure. Inductive representation learning on large graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neu- ral Information Processing Systems 30, pages 1024-1034. Curran Associates and Inc., 2017. [Yang et al., 2019] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19, 2019. [Yuan and Yu, 2013] Jiawei Yuan and Shucheng Yu. Privacy preserving back-propagation neural network learning made practical with cloud computing. IEEE Transactions on Parallel and Distributed Systems, 25(1):212-221, 2013. [Zhang et al., 2015] Qingchen Zhang, Laurence T Yang, and Zhikui Chen. Privacy preserving deep computation model on cloud for big data feature learning. IEEE Transactions on Computers, 65(5):1351-1362, 2015.\n", "annotations": {"author": "[{\"end\":116,\"start\":75},{\"end\":141,\"start\":117},{\"end\":166,\"start\":142},{\"end\":193,\"start\":167},{\"end\":246,\"start\":194}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":81},{\"end\":128,\"start\":126},{\"end\":153,\"start\":150},{\"end\":180,\"start\":176},{\"end\":207,\"start\":203}]", "author_first_name": "[{\"end\":80,\"start\":75},{\"end\":125,\"start\":117},{\"end\":149,\"start\":142},{\"end\":175,\"start\":167},{\"end\":202,\"start\":194}]", "author_affiliation": "[{\"end\":115,\"start\":105},{\"end\":140,\"start\":130},{\"end\":165,\"start\":155},{\"end\":192,\"start\":182},{\"end\":245,\"start\":235}]", "title": "[{\"end\":72,\"start\":1},{\"end\":318,\"start\":247}]", "venue": null, "abstract": "[{\"end\":1101,\"start\":320}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1333,\"start\":1315},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1535,\"start\":1512},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1557,\"start\":1535},{\"end\":1697,\"start\":1678},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1715,\"start\":1697},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1887,\"start\":1868},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1905,\"start\":1887},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1961,\"start\":1940},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2037,\"start\":2017},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2447,\"start\":2428},{\"end\":2463,\"start\":2447},{\"end\":2539,\"start\":2508},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3114,\"start\":3094},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3132,\"start\":3114},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3550,\"start\":3530},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3575,\"start\":3550},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3596,\"start\":3575},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3614,\"start\":3596},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3977,\"start\":3957},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4001,\"start\":3977},{\"end\":4903,\"start\":4891},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4919,\"start\":4903},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6280,\"start\":6260},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6297,\"start\":6280},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6586,\"start\":6566},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6607,\"start\":6586},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6624,\"start\":6607},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6724,\"start\":6700},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7164,\"start\":7144},{\"end\":7291,\"start\":7266},{\"end\":8141,\"start\":8130},{\"end\":8927,\"start\":8923},{\"end\":10106,\"start\":10079},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10623,\"start\":10606},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11294,\"start\":11273},{\"end\":14688,\"start\":14627},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14885,\"start\":14865}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17244,\"start\":17215},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17280,\"start\":17245},{\"attributes\":{\"id\":\"fig_2\"},\"end\":17301,\"start\":17281},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":17639,\"start\":17302},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":18135,\"start\":17640}]", "paragraph": "[{\"end\":1559,\"start\":1117},{\"end\":2486,\"start\":1561},{\"end\":4003,\"start\":2488},{\"end\":4046,\"start\":4005},{\"end\":4271,\"start\":4048},{\"end\":4653,\"start\":4273},{\"end\":4921,\"start\":4734},{\"end\":4998,\"start\":4923},{\"end\":5234,\"start\":5069},{\"end\":5369,\"start\":5298},{\"end\":5755,\"start\":5401},{\"end\":5902,\"start\":5806},{\"end\":5992,\"start\":5939},{\"end\":6298,\"start\":6028},{\"end\":6626,\"start\":6408},{\"end\":7449,\"start\":6666},{\"end\":7588,\"start\":7451},{\"end\":7887,\"start\":7640},{\"end\":8227,\"start\":7889},{\"end\":8235,\"start\":8229},{\"end\":8456,\"start\":8399},{\"end\":8721,\"start\":8720},{\"end\":8768,\"start\":8765},{\"end\":10108,\"start\":8918},{\"end\":10400,\"start\":10153},{\"end\":10801,\"start\":10431},{\"end\":11060,\"start\":10878},{\"end\":11530,\"start\":11075},{\"end\":11815,\"start\":11532},{\"end\":11957,\"start\":11837},{\"end\":12509,\"start\":11959},{\"end\":12730,\"start\":12554},{\"end\":13623,\"start\":12747},{\"end\":13970,\"start\":13644},{\"end\":14172,\"start\":13986},{\"end\":14274,\"start\":14174},{\"end\":14406,\"start\":14276},{\"end\":14794,\"start\":14429},{\"end\":15115,\"start\":14796},{\"end\":15802,\"start\":15117},{\"end\":16123,\"start\":15804},{\"end\":16516,\"start\":16125},{\"end\":17214,\"start\":16531}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4703,\"start\":4654},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5068,\"start\":4999},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5297,\"start\":5235},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5400,\"start\":5370},{\"attributes\":{\"id\":\"formula_4\"},\"end\":5805,\"start\":5756},{\"attributes\":{\"id\":\"formula_5\"},\"end\":5938,\"start\":5903},{\"attributes\":{\"id\":\"formula_6\"},\"end\":6407,\"start\":6299},{\"attributes\":{\"id\":\"formula_7\"},\"end\":7639,\"start\":7589},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8398,\"start\":8236},{\"attributes\":{\"id\":\"formula_9\"},\"end\":8719,\"start\":8457},{\"attributes\":{\"id\":\"formula_10\"},\"end\":8764,\"start\":8722},{\"attributes\":{\"id\":\"formula_11\"},\"end\":8917,\"start\":8769},{\"attributes\":{\"id\":\"formula_12\"},\"end\":10877,\"start\":10802}]", "table_ref": "[{\"end\":14710,\"start\":14703},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15939,\"start\":15932}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1115,\"start\":1103},{\"attributes\":{\"n\":\"2.2\"},\"end\":4732,\"start\":4705},{\"attributes\":{\"n\":\"2.3\"},\"end\":6026,\"start\":5995},{\"attributes\":{\"n\":\"2.4\"},\"end\":6664,\"start\":6629},{\"end\":10151,\"start\":10111},{\"attributes\":{\"n\":\"3.1\"},\"end\":10429,\"start\":10403},{\"attributes\":{\"n\":\"3.2\"},\"end\":11073,\"start\":11063},{\"attributes\":{\"n\":\"4\"},\"end\":11835,\"start\":11818},{\"attributes\":{\"n\":\"5\"},\"end\":12531,\"start\":12512},{\"attributes\":{\"n\":\"5.1\"},\"end\":12552,\"start\":12534},{\"end\":12745,\"start\":12733},{\"attributes\":{\"n\":\"5.2\"},\"end\":13642,\"start\":13626},{\"attributes\":{\"n\":\"6\"},\"end\":13984,\"start\":13973},{\"attributes\":{\"n\":\"6.1\"},\"end\":14427,\"start\":14409},{\"attributes\":{\"n\":\"7\"},\"end\":16529,\"start\":16519},{\"end\":17226,\"start\":17216},{\"end\":17256,\"start\":17246},{\"end\":17292,\"start\":17282},{\"end\":17650,\"start\":17641}]", "table": "[{\"end\":17639,\"start\":17341}]", "figure_caption": "[{\"end\":17244,\"start\":17228},{\"end\":17280,\"start\":17258},{\"end\":17301,\"start\":17294},{\"end\":17341,\"start\":17304},{\"end\":18135,\"start\":17652}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7264,\"start\":7256},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7326,\"start\":7318},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11814,\"start\":11806}]", "bib_author_first_name": "[{\"end\":18865,\"start\":18864},{\"end\":19645,\"start\":19644},{\"end\":20352,\"start\":20351},{\"end\":20709,\"start\":20708},{\"end\":20991,\"start\":20990},{\"end\":21300,\"start\":21299},{\"end\":21888,\"start\":21867},{\"end\":21901,\"start\":21900},{\"end\":22200,\"start\":22199},{\"end\":22667,\"start\":22666},{\"end\":22689,\"start\":22686},{\"end\":22697,\"start\":22696}]", "bib_author_last_name": "[{\"end\":18212,\"start\":18208},{\"end\":18870,\"start\":18866},{\"end\":19651,\"start\":19646},{\"end\":20362,\"start\":20353},{\"end\":20380,\"start\":20364},{\"end\":20714,\"start\":20710},{\"end\":20996,\"start\":20992},{\"end\":21304,\"start\":21301},{\"end\":21898,\"start\":21889},{\"end\":21909,\"start\":21902},{\"end\":22206,\"start\":22201},{\"end\":22664,\"start\":22660},{\"end\":22684,\"start\":22668},{\"end\":22694,\"start\":22690},{\"end\":22705,\"start\":22698},{\"end\":22714,\"start\":22707}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":220423847},\"end\":18757,\"start\":18137},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":21713075},\"end\":19575,\"start\":18759},{\"attributes\":{\"doi\":\"arXiv:1902.01046\",\"id\":\"b2\",\"matched_paper_id\":16709789},\"end\":20349,\"start\":19577},{\"attributes\":{\"id\":\"b3\"},\"end\":20652,\"start\":20351},{\"attributes\":{\"doi\":\"arXiv:2002.02088\",\"id\":\"b4\"},\"end\":20933,\"start\":20654},{\"attributes\":{\"doi\":\"arXiv:2010.12882\",\"id\":\"b5\"},\"end\":21212,\"start\":20935},{\"attributes\":{\"doi\":\"arXiv:1812.02863\",\"id\":\"b6\"},\"end\":21573,\"start\":21214},{\"attributes\":{\"id\":\"b7\"},\"end\":21814,\"start\":21575},{\"attributes\":{\"id\":\"b8\"},\"end\":22078,\"start\":21816},{\"attributes\":{\"doi\":\"arXiv:1711.10677\",\"id\":\"b9\"},\"end\":22531,\"start\":22080},{\"attributes\":{\"doi\":\"arXiv:1610.02527\",\"id\":\"b10\"},\"end\":26117,\"start\":22533}]", "bib_title": "[{\"end\":18206,\"start\":18137},{\"end\":18862,\"start\":18759},{\"end\":19642,\"start\":19577},{\"end\":21609,\"start\":21575},{\"end\":22658,\"start\":22533}]", "bib_author": "[{\"end\":18214,\"start\":18208},{\"end\":18872,\"start\":18864},{\"end\":19653,\"start\":19644},{\"end\":20364,\"start\":20351},{\"end\":20382,\"start\":20364},{\"end\":20716,\"start\":20708},{\"end\":20998,\"start\":20990},{\"end\":21306,\"start\":21299},{\"end\":21900,\"start\":21867},{\"end\":21911,\"start\":21900},{\"end\":22208,\"start\":22199},{\"end\":22666,\"start\":22660},{\"end\":22686,\"start\":22666},{\"end\":22696,\"start\":22686},{\"end\":22707,\"start\":22696},{\"end\":22716,\"start\":22707}]", "bib_venue": "[{\"end\":18301,\"start\":18214},{\"end\":18956,\"start\":18872},{\"end\":19727,\"start\":19669},{\"end\":20462,\"start\":20382},{\"end\":20706,\"start\":20654},{\"end\":20988,\"start\":20935},{\"end\":21297,\"start\":21214},{\"end\":21657,\"start\":21611},{\"end\":21865,\"start\":21816},{\"end\":22197,\"start\":22080},{\"end\":22827,\"start\":22748},{\"end\":19027,\"start\":18958},{\"end\":19772,\"start\":19729}]"}}}, "year": 2023, "month": 12, "day": 17}