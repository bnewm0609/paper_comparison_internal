{"id": 125617073, "updated": "2023-09-29 06:04:18.853", "metadata": {"title": "Gaussian Error Linear Units (GELUs)", "authors": "[{\"first\":\"Dan\",\"last\":\"Hendrycks\",\"middle\":[]},{\"first\":\"Kevin\",\"last\":\"Gimpel\",\"middle\":[]}]", "venue": "ArXiv", "journal": "arXiv: Learning", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1606.08415", "mag": "2899663614", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/1606.08415v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0d0075640ebec4258d08dda3d7c884cb08bd1a02", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/de5e7320729f5d3cbb6709eb6329ec41ace8c95d.txt", "contents": "\nGAUSSIAN ERROR LINEAR UNITS (GELUS)\n\n\nDan Hendrycks hendrycks@berkeley.edu \nKevin Gimpel kgimpel@ttic.edu \n\nUniversity of California\nBerkeley\n\n\nToyota Technological Institute at Chicago\n\n\nGAUSSIAN ERROR LINEAR UNITS (GELUS)\n\nWe propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is x\u03a6(x), where \u03a6(x) the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (x1 x>0 ). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.\n\nINTRODUCTION\n\nEarly artificial neurons utilized binary threshold units (Hopfield, 1982;McCulloch & Pitts, 1943). These hard binary decisions are smoothed with sigmoid activations, enabling a neuron to have a \"firing rate\" interpretation and to train with backpropagation. But as networks became deeper, training with sigmoid activations proved less effective than the non-smooth, less-probabilistic ReLU (Nair & Hinton, 2010) which makes hard gating decisions based upon an input's sign. Despite having less of a statistical motivation, the ReLU remains a competitive engineering solution which often enables faster and better convergence than sigmoids. Building on the successes of ReLUs, a recent modification called ELUs (Clevert et al., 2016) allows a ReLU-like nonlinearity to output negative values which sometimes increases training speed. In all, the activation choice has remained a necessary architecture decision for neural networks lest the network be a deep linear classifier.\n\nDeep nonlinear classifiers can fit their data so well that network designers are often faced with the choice of including stochastic regularizer like adding noise to hidden layers or applying dropout (Srivastava et al., 2014), and this choice remains separate from the activation function. Some stochastic regularizers can make the network behave like an ensemble of networks, a pseudoensemble (Bachman et al., 2014), and can lead to marked accuracy increases. For example, the stochastic regularizer dropout creates a pseudoensemble by randomly altering some activation decisions through zero multiplication. Nonlinearities and dropout thus determine a neuron's output together, yet the two innovations have remained distinct. More, neither subsumed the other because popular stochastic regularizers act irrespectively of the input and nonlinearities are aided by such regularizers.\n\nIn this work, we introduce a new nonlinearity, the Gaussian Error Linear Unit (GELU). It relates to stochastic regularizers in that it is the expectation of a modification to Adaptive Dropout (Ba & Frey, 2013). This suggests a more probabilistic view of a neuron's output. We find that this novel nonlinearity matches or exceeds models with ReLUs or ELUs across tasks from computer vision, natural language processing, and automatic speech recognition.\n\n\nGELU FORMULATION\n\nWe motivate our activation function by combining properties from dropout, zoneout, and ReLUs. First note that a ReLU and dropout both yield a neuron's output with the ReLU deterministically multiplying the input by zero or one and dropout stochastically multiplying by zero. Also, a new RNN regularizer called zoneout stochastically multiplies inputs by one (Krueger et al., 2016). We merge this functionality by multiplying the input by zero or one, but the values of this zero-one mask are stochastically determined while also dependent upon the input. Specifically, we can multiply the neuron input x by m \u223c Bernoulli(\u03a6(x)), where \u03a6(x) = P (X \u2264 x), X \u223c N (0, 1) is the cumulative distribution function of the standard normal distribution. We choose this distribution since neuron inputs tend to follow a normal distribution, especially with Batch Normalization. In this setting, inputs have a higher probability of being \"dropped\" as x decreases, so the transformation applied to x is stochastic yet depends upon the input. Masking inputs in this fashion retains non-determinism but maintains dependency upon the input value. A stochastically chosen mask amounts to a stochastic zero or identity transformation of the input. This is much like Adaptive Dropout (Ba & Frey, 2013), but adaptive dropout is used in tandem with nonlinearities and uses a logistic not standard normal distribution. We found that it is possible to train competitive MNIST and TIMIT networks solely with this stochastic regularizer, all without using any nonlinearity.\n\nWe often want a deterministic decision from a neural network, and this gives rise to our new nonlinearity. The nonlinearity is the expected transformation of the stochastic regularizer on an input x, which is \u03a6(\nx) \u00d7 Ix + (1 \u2212 \u03a6(x)) \u00d7 0x = x\u03a6(x).\nLoosely, this expression states that we scale x by how much greater it is than other inputs. Since the cumulative distribution function of a Gaussian is often computed with the error function, we define the Gaussian Error Linear Unit (GELU) as\nGELU(x) = xP (X \u2264 x) = x\u03a6(x) = x \u00b7 1 2 1 + erf(x/ \u221a 2) .\nWe can approximate the GELU with\n0.5x(1 + tanh[ 2/\u03c0(x + 0.044715x 3 )]) or x\u03c3(1.702x),\nif greater feedforward speed is worth the cost of exactness.\n\nWe could use different CDFs. For example we could use Logistic Distribution CDF \u03c3(x) to get what we call the Sigmoid Linear Unit (SiLU) x\u03c3(x). We could use the CDF of N (\u00b5, \u03c3 2 ) and have \u00b5 and \u03c3 be learnable hyperparameters, but throughout this work we simply let \u00b5 = 0 and \u03c3 = 1. Consequently, we do not introduce any new hyperparameters in the following experiments. In the next section, we show that the GELU exceeds ReLUs and ELUs across numerous tasks.  Maas et al. (2013) for a description of LReLUs).\n\n\nGELU EXPERIMENTS\n\n\nMNIST CLASSIFICATION\n\nLet us verify that this nonlinearity competes with previous activation functions by replicating an experiment from Clevert et al. (2016). To this end, we train a fully connected neural network with GELUs (\u00b5 = 0, \u03c3 = 1), ReLUs, and ELUs (\u03b1 = 1). Each 8-layer, 128 neuron wide neural network is trained for 50 epochs with a batch size of 128. This experiment differs from those of Using different nonlinearities, we record the test set accuracy decline and log loss increase as inputs are noised. The MNIST classifier trained without dropout received inputs with uniform noise Unif[\u2212a, a] added to each example at different levels a, where a = 3 is the greatest noise strength. Here GELUs display robustness matching or exceeding ELUs and ReLUs.\n\nClevert et al. in that we use the Adam optimizer (Kingma & Ba, 2015) rather than stochastic gradient descent without momentum, and we also show how well nonlinearities cope with dropout. Weights are initialized with unit norm rows, as this has positive impact on each nonlinearity's performance (Hendrycks & Gimpel, 2016;Mishkin & Matas, 2016;Saxe et al., 2014). Note that we tune over the learning rates {10 \u22123 , 10 \u22124 , 10 \u22125 } with 5k validation examples from the training set and take the median results for five runs. Using these classifiers, we demonstrate in Figure 3 that classifiers using a GELU can be more robust to noised inputs. Figure 2 shows that the GELU tends to have the lowest median training log loss with and without dropout. Consequently, although the GELU is inspired by a different stochastic process, it comports well with dropout.\n\n\nMNIST AUTOENCODER\n\nWe now consider a self-supervised setting and train a deep autoencoder on MNIST (Desjardins et al., 2015). To accomplish this, we use a network with layers of width 1000, 500, 250, 30, 250, 500, 1000, in that order. We again use the Adam optimizer and a batch size of 64. Our loss is the mean squared loss. We vary the learning rate from 10 \u22123 to 10 \u22124 . We also tried a learning rate of 0.01 but ELUs diverged, and GELUs and RELUs converged poorly. The results in Figure 4 indicate the GELU accommodates different learning rates and significantly outperforms the other nonlinearities. \n\n\nTWITTER POS TAGGING\n\nMany datasets in natural language processing are relatively small, so it is important that an activation generalize well from few examples. To meet this challenge we compare the nonlinearities on POSannotated tweets (Gimpel et al., 2011;Owoputi et al., 2013) which contain 25 tags. The tweet tagger is simply a two-layer network with pretrained word vectors trained on a corpus of 56 million tweets (Owoputi et al., 2013). The input is the concatenation of the vector of the word to be tagged and those of its left and right neighboring words. Each layer has 256 neurons, a dropout keep probability of 0.8, and the network is optimized with Adam while tuning over the learning rates {10 \u22123 , 10 \u22124 , 10 \u22125 }. We train each network five times per learning rate, and the median test set error is 12.57% for the GELU, 12.67% for the ReLU, and 12.91% for the ELU.\n\n\nTIMIT FRAME CLASSIFICATION\n\nOur next challenge is phone recognition with the TIMIT dataset which has recordings of 680 speakers in a noiseless environment. The system is a five-layer, 2048-neuron wide classifier as in (Mohamed et al., 2012) with 39 output phone labels and a dropout rate of 0.5 as in (Srivastava, 2013). This network takes as input 11 frames and must predict the phone of the center frame using 26 MFCC, energy, and derivative features per frame. We tune over the learning rates {10 \u22123 , 10 \u22124 , 10 \u22125 } and optimize with Adam. After five runs per setting, we obtain the median curves in Figure 5, and median test error chosen at the lowest validation error is 29.3% for the GELU, 29.5% for the ReLU, and 29.6% for the ELU.\n\n3.5 CIFAR-10/100 CLASSIFICATION Next, we demonstrate that for more intricate architectures the GELU nonlinearity again outperforms other nonlinearities. We evaluate this activation function using CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009) on shallow and deep convolutional neural networks, respectively.\n\nOur shallower convolutional neural network is a 9-layer network with the architecture and training procedure from Salimans & Kingma (2016) while using batch normalization to speed up training. The architecture is described in appendix A and recently obtained state of the art on CIFAR-10 without data augmentation. No data augmentation was used to train this network. We tune over the learning initial rates {10 \u22123 , 10 \u22124 , 10 \u22125 } with 5k validation examples then train on the whole training set again based upon the learning rate from cross validation. The network is optimized with Adam for 200 epochs, and at the 100th epoch the learning rate linearly decays to zero. Results are shown in Figure 6, and each curve is a median of three runs. Ultimately, the GELU obtains a median error rate of 7.89%, the ReLU obtains 8.16%, and the ELU obtains 8.41%.\n\nNext we consider a wide residual network on CIFAR-100 with 40 layers and a widening factor of 4 (Zagoruyko & Komodakis, 2016). We train for 50 epochs with the learning rate schedule described in (Loshchilov & Hutter, 2016) (T 0 = 50, \u03b7 = 0.1) with Nesterov momentum, and with a dropout keep probability of 0.7. Some have noted that ELUs have an exploding gradient with residual networks (Shah et al., 2016), and this is alleviated with batch normalization at the end of a residual block. Consequently, we use a Conv-Activation-Conv-Activation-BatchNorm block architecture to be charitable to ELUs. Over three runs we obtain the median convergence curves in Figure 7. Meanwhile, the GELU achieves a median error of 20.74%, the ReLU obtains 21.77% (without our changes described above, the original 40-4 WideResNet with a ReLU obtains 22.89% (Zagoruyko & Komodakis, 2016)), and the ELU obtains 22.98%.\n\n\nDISCUSSION\n\nAcross several experiments, the GELU outperformed previous nonlinearities, but it bears semblance to the ReLU and ELU in other respects. For example, as \u03c3 \u2192 0 and if \u00b5 = 0, the GELU becomes a ReLU. More, the ReLU and GELU are equal asymptotically. In fact, the GELU can be viewed as a way to smooth a ReLU. To see this, recall that ReLU = max( 1 is the indicator function), while the GELU is x\u03a6(x) if \u00b5 = 0, \u03c3 = 1. Then the CDF is a smooth approximation to the binary function the ReLU uses, like how the sigmoid smoothed binary threshold activations. Unlike the ReLU, the GELU and ELU can be both negative and positive. In fact, if we used the cumulative distribution function of the standard Cauchy distribution, then the ELU (when \u03b1 = 1/\u03c0) is asymptotically equal to xP (C \u2264 x), C \u223c Cauchy(0, 1) for negative values and for positive values is xP (C \u2264 x) if we shift the line down by 1/\u03c0. These are some fundamental relations to previous nonlinearities.\n\nHowever, the GELU has several notable differences. This non-convex, non-monotonic function is not linear in the positive domain and exhibits curvature at all points. Meanwhile ReLUs and ELUs, which are convex and monotonic activations, are linear in the positive domain and thereby can lack curvature. As such, increased curvature and non-monotonicity may allow GELUs to more easily approximate complicated functions than can ReLUs or ELUs. Also, since ReLU(x) = x1(x > 0) and GELU(x) = x\u03a6(x) if \u00b5 = 0, \u03c3 = 1, we can see that the ReLU gates the input depending upon its sign, while the GELU weights its input depending upon how much greater it is than other inputs. In addition and significantly, the GELU has a probabilistic interpretation given that it is the expectation of a stochastic regularizer.\n\nWe also have two practical tips for using the GELU. First we advise using an optimizer with momentum when training with a GELU, as is standard for deep neural networks. Second, using a close approximation to the cumulative distribution function of a Gaussian distribution is important. A sigmoid function \u03c3(x) = 1/(1 + e \u2212x ) is an approximation of a cumulative distribution function of a normal distribution. However, we found that a Sigmoid Linear Unit (SiLU) x\u03c3(x) performs worse than GELUs but usually better than ReLUs and ELUs, so our SiLU is also a reasonable nonlinearity choice. Instead of using a x\u03c3(x) to approximate \u03a6(x), we used 0.5x(1 + tanh[ 2/\u03c0(x + 0.044715x 3 )]) (Choudhury, 2014) 1 or x\u03c3(1.702x). Both are sufficiently fast, easy-to-implement approximations, and we used the former in every experiment in this paper.\n\n\nCONCLUSION\n\nFor the numerous datasets evaluated in this paper, the GELU exceeded the accuracy of the ELU and ReLU consistently, making it a viable alternative to previous nonlinearities.\n\n\nACKNOWLEDGMENT\n\nWe would like to thank NVIDIA Corporation for donating several TITAN X GPUs used in this research. A NEURAL NETWORK ARCHITECTURE FOR CIFAR-10 EXPERIMENTS \n\n\nB HISTORY OF THE GELU AND SILU\n\nThis paper arose from DH's first research internship as an undergraduate in June 2016. The start of the week after, this paper was put on arXiv, in which we discuss smoother ReLU activation functions (x \u00d7 P (X \u2264 x)) and their relation to stochastic regularizers. In 2016, we submitted the paper to ICLR and made the paper and code publicly available. In the paper, we introduced and coined the Sigmoid Linear Unit (SiLU) as x \u00b7 \u03c3(x).\n\nIn the first half of 2017, Elfwing et al. published a paper that proposed the same activation function as SiLU, x \u00b7 \u03c3(x), which they called \"SIL.\" At the end of 2017, over a year after this paper was first released, Quoc Le and others from Google Brain put out a paper proposing x \u00b7 \u03c3(x) without citing either the Elfwing et al. paper or this work. Upon learning this, we contacted both parties. Elfwing quickly updated their work to call the activation the \"SiLU\" instead of \"SIL\" to recognize that we originally introduced the activation.\n\nUnlike Elfwing et al., the Google Brain researchers continued calling the activation \"swish.\" However, there was no novelty. The first author of the \"swish\" paper stated their oversight in public, saying, \"As has been pointed out, we missed prior works that proposed the same activation function. The fault lies entirely with me for not conducting a thorough enough literature search.\" To subdue criticism, an update to the paper was released a week later. Rather than give credit to this work for the SiLU, the update only cited this work for the GELU so that the \"swish\" appeared more novel. In the updated paper, a learnable hyperparameter \u03b2 was introduced, and the swish was changed from x \u00b7 \u03c3(x) to x \u00b7 \u03c3(\u03b2 \u00b7 x). This staked all of the idea's novelty on an added learnable hyperparameter \u03b2.\n\nDespite the addition of the hyperparameter beta, nearly all of the community still used the original \"swish\" function without \u03b2 (i.e., with \u03b2 = 1). Since this paper was from Google Brain, the Tensorflow implementation ended up being called \"swish,\" and the default setting removed \u03b2, rendering it identical to the SiLU. The practice of adding an unused hyperparameter allowed claiming novelty while effectively receiving credit for an idea that originated elsewhere. Future papers with the same senior authors persistently referred to the \"swish\" function even when not using \u03b2, making it identical to the SiLU, originally proposed in this work. This resulted in the \"swish\" paper inappropriately gaining credit for the idea.\n\nThings changed as the GELU began to be used in BERT and GPT, becoming the default activation for state-of-the-art Transformers. Now it is substantially more commonly used than the SiLU.\n\nSeparately, a reddit post \"Google has a credit assignment problem in research\" became popular and focused on how they refer to the SiLU as the swish. As an example, they mentioned \"Smooth Adversarial Training\" as an example of poor credit assignment. In the \"Smooth Adversarial Training\" paper, which came from the senior author of the swish, the term \"swish\" was used instead of \"SiLU.\" To reduce blowback from the post, the authors updated the paper and replaced \"swish\" with the \"SiLU,\" recognizing this paper as the original source of the idea. After this post, popular libraries such as Tensorflow and PyTorch also began to rename the function to \"SiLU\" instead of \"swish.\" For close observers, this issue has been largely settled, and we are grateful for the proper recognition that has largely come to pass.\n\nFigure 1 :\n1The GELU (\u00b5 = 0, \u03c3 = 1), ReLU, and ELU (\u03b1 = 1).\n\nFigure 2 :Figure 3 :\n23MNIST Classification Results. Left are the loss curves without dropout, and right are curves with a dropout rate of 0.5. Each curve is the the median of five runs. Training set log losses are the darker, lower curves, and the fainter, upper curves are the validation set log loss curves. MNIST Robustness Results.\n\nFigure 4 :Figure 5 :\n45MNIST Autoencoding Results. Each curve is the median of three runs. Left are loss curves for a learning rate of 10 \u22123 , and the right figure is for a 10 \u22124 learning rate. Light, thin curves correspond to test set log losses. TIMIT Frame Classification. Learning curves show training set convergence, and the lighter curves show the validation set convergence.\n\nFigure 6 :\n6CIFAR-10 Results. Each curve is the median of three runs. Learning curves show training set error rates, and the lighter curves show the test set error rates.\n\nFigure 7 :\n7x, 0) = x1(x > 0) CIFAR-100 Wide Residual Network Results. Learning curves show training set convergence with dropout on, and the lighter curves show the test set convergence with dropout off.\n\n\nTim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Neural Information Processing Systems, 2016.Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-\nnamics of learning in deep linear neural networks. In International Conference on Learning \nRepresentations, 2014. \n\nAnish Shah, Sameer Shinde, Eashan Kadam, Hena Shah, and Sandip Shingade. Deep residual \nnetworks with exponential linear unit. In Vision Net, 2016. \n\nNitish Srivastava. Improving neural networks with dropout. In University of Toronto, 2013. \n\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \nDropout: A simple way to prevent neural networks from overfitting. In Journal of Machine \nLearning Research, 2014. \n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Confer-\nence, 2016. \n\n\nTable 1 :\n1Neural network architecture for CIFAR-10.Layer Type \n# channels x, y dimension \n\nraw RGB input \n3 \n32 \nZCA whitening \n3 \n32 \nGaussian noise \u03c3 = 0.15 \n3 \n32 \n3 \u00d7 3 conv with activation 96 \n32 \n3 \u00d7 3 conv with activation 96 \n32 \n3 \u00d7 3 conv with activation 96 \n32 \n2 \u00d7 2 max pool, stride 2 \n96 \n16 \ndropout with p = 0.5 \n96 \n16 \n3 \u00d7 3 conv with activation 192 \n16 \n3 \u00d7 3 conv with activation 192 \n16 \n3 \u00d7 3 conv with activation 192 \n16 \n2 \u00d7 2 max pool, stride 2 \n192 \n8 \ndropout with p = 0.5 \n192 \n8 \n3 \u00d7 3 conv with activation 192 \n6 \n1 \u00d7 1 conv with activation 192 \n6 \n1 \u00d7 1 conv with activation 192 \n6 \nglobal average pool \n192 \n1 \nsoftmax output \n10 \n1 \n\n\nThank you to Dmytro Mishkin for bringing an approximation like this to our attention.\n\nAdaptive dropout for training deep neural networks. Jimmy Ba, Brendan Frey, Neural Information Processing Systems. Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Neural Infor- mation Processing Systems, 2013.\n\nLearning with pseudo-ensembles. Philip Bachman, Ouais Alsharif, Doina Precup, Neural Information Processing Systems. Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Neural Information Processing Systems, 2014.\n\nA simple approximation to the area under standard normal curve. Amit Choudhury, Mathematics and Statistics. Amit Choudhury. A simple approximation to the area under standard normal curve. In Mathematics and Statistics, 2014.\n\nFast and accurate deep network learning by exponential linear units (ELUs). Djork-Arn\u00e9 Clevert, Thomas Unterthiner, Sepp Hochreiter, International Conference on Learning Representations. Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs). In International Conference on Learning Represen- tations, 2016.\n\nGuillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu, Natural neural networks. In arXiv. Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and Koray Kavukcuoglu. Natural neural networks. In arXiv, 2015.\n\nPart-of-Speech Tagging for Twitter: Annotation, Features, and Experiments. Kevin Gimpel, Nathan Schneider, O \u2032 Brendan, Dipanjan Connor, Daniel Das, Jacob Mills, Michael Eisenstein, Dani Heilman, Jeffrey Yogatama, Noah A Flanigan, Smith, Association for Computational LinguisticsKevin Gimpel, Nathan Schneider, Brendan O \u2032 Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments. Association for Computational Linguistics (ACL), 2011.\n\nAdjusting for dropout variance in batch normalization and weight initialization. Dan Hendrycks, Kevin Gimpel, arXivDan Hendrycks and Kevin Gimpel. Adjusting for dropout variance in batch normalization and weight initialization. In arXiv, 2016.\n\nNeural networks and physical systems with emergent collective computational abilities. John Hopfield, Proceedings of the National Academy of Sciences of the USA. the National Academy of Sciences of the USAJohn Hopfield. Neural networks and physical systems with emergent collective computational abil- ities. In Proceedings of the National Academy of Sciences of the USA, 1982.\n\nAdam: A Method for Stochastic Optimization. Diederik Kingma, Jimmy Ba, International Conference for Learning Representations. Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. International Conference for Learning Representations, 2015.\n\nLearning Multiple Layers of Features from Tiny Images. Alex Krizhevsky, Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images, 2009.\n\nZoneout: Regularizing RNNs by randomly preserving hidden activations. David Krueger, Tegan Maharaj, J\u00e1nos Kram\u00e1r, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke1, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, Chris Pal, Neural Information Processing Systems. David Krueger, Tegan Maharaj, J\u00e1nos Kram\u00e1r, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke1, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, and Chris Pal. Zoneout: Regularizing RNNs by randomly preserving hidden activations. In Neural Information Processing Systems, 2016.\n\nSGDR: Stochastic gradient descent with restarts. Ilya Loshchilov, Frank Hutter, arXivIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with restarts. arXiv, 2016.\n\nRectifier nonlinearities improve neural network acoustic models. Andrew L Maas, Awni Y Hannun, Andrew Y Ng, International Conference on Machine Learning. Andrew L. Maas, Awni Y. Hannun, , and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In International Conference on Machine Learning, 2013.\n\nA logical calculus of the ideas immanent in nervous activity. S Warren, Walter Mcculloch, Pitts, In Bulletin of Mathematical Biophysics. Warren S. McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. In Bulletin of Mathematical Biophysics, 1943.\n\nAll you need is a good init. Dmytro Mishkin, Jiri Matas, International Conference on Learning Representations. Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016.\n\nAcoustic modeling using deep belief networks. Abdelrahman Mohamed, George E Dahl, Geoffrey E Hinton, IEEE Transactions on Audio, Speech, and Language Processing. Abdelrahman Mohamed, George E. Dahl, and Geoffrey E. Hinton. Acoustic modeling using deep belief networks. In IEEE Transactions on Audio, Speech, and Language Processing, 2012.\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, International Conference on Machine Learning. Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In International Conference on Machine Learning, 2010.\n\nImproved part-of-speech tagging for online conversational text with word clusters. Olutobi Owoputi, O&apos; Brendan, Chris Connor, Kevin Dyer, Nathan Gimpel, Noah A Schneider, Smith, North American Chapter of the Association for Computational Linguistics (NAACL). Olutobi Owoputi, Brendan O'Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. Improved part-of-speech tagging for online conversational text with word clusters. In North American Chapter of the Association for Computational Linguistics (NAACL), 2013.\n", "annotations": {"author": "[{\"end\":76,\"start\":39},{\"end\":107,\"start\":77},{\"end\":143,\"start\":108},{\"end\":188,\"start\":144}]", "publisher": null, "author_last_name": "[{\"end\":52,\"start\":43},{\"end\":89,\"start\":83}]", "author_first_name": "[{\"end\":42,\"start\":39},{\"end\":82,\"start\":77}]", "author_affiliation": "[{\"end\":142,\"start\":109},{\"end\":187,\"start\":145}]", "title": "[{\"end\":36,\"start\":1},{\"end\":224,\"start\":189}]", "venue": null, "abstract": "[{\"end\":767,\"start\":226}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":856,\"start\":840},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":880,\"start\":856},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1194,\"start\":1173},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1514,\"start\":1493},{\"end\":1985,\"start\":1960},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2176,\"start\":2154},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2854,\"start\":2837},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3498,\"start\":3476},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4398,\"start\":4381},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5841,\"start\":5823},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6051,\"start\":6030},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6728,\"start\":6709},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6981,\"start\":6955},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7003,\"start\":6981},{\"end\":7021,\"start\":7003},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7643,\"start\":7618},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8385,\"start\":8364},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8406,\"start\":8385},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8569,\"start\":8547},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9250,\"start\":9228},{\"end\":9329,\"start\":9311},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9997,\"start\":9980},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11143,\"start\":11117},{\"end\":11328,\"start\":11309}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":18392,\"start\":18332},{\"attributes\":{\"id\":\"fig_1\"},\"end\":18730,\"start\":18393},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19114,\"start\":18731},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19286,\"start\":19115},{\"attributes\":{\"id\":\"fig_4\"},\"end\":19492,\"start\":19287},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20452,\"start\":19493},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21121,\"start\":20453}]", "paragraph": "[{\"end\":1758,\"start\":783},{\"end\":2643,\"start\":1760},{\"end\":3097,\"start\":2645},{\"end\":4664,\"start\":3118},{\"end\":4877,\"start\":4666},{\"end\":5156,\"start\":4913},{\"end\":5246,\"start\":5214},{\"end\":5361,\"start\":5301},{\"end\":5871,\"start\":5363},{\"end\":6658,\"start\":5915},{\"end\":7516,\"start\":6660},{\"end\":8124,\"start\":7538},{\"end\":9007,\"start\":8148},{\"end\":9750,\"start\":9038},{\"end\":10063,\"start\":9752},{\"end\":10920,\"start\":10065},{\"end\":11821,\"start\":10922},{\"end\":12791,\"start\":11836},{\"end\":13595,\"start\":12793},{\"end\":14432,\"start\":13597},{\"end\":14621,\"start\":14447},{\"end\":14794,\"start\":14640},{\"end\":15262,\"start\":14829},{\"end\":15804,\"start\":15264},{\"end\":16601,\"start\":15806},{\"end\":17328,\"start\":16603},{\"end\":17515,\"start\":17330},{\"end\":18331,\"start\":17517}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4912,\"start\":4878},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5213,\"start\":5157},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5300,\"start\":5247}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":781,\"start\":769},{\"attributes\":{\"n\":\"2\"},\"end\":3116,\"start\":3100},{\"attributes\":{\"n\":\"3\"},\"end\":5890,\"start\":5874},{\"attributes\":{\"n\":\"3.1\"},\"end\":5913,\"start\":5893},{\"attributes\":{\"n\":\"3.2\"},\"end\":7536,\"start\":7519},{\"attributes\":{\"n\":\"3.3\"},\"end\":8146,\"start\":8127},{\"attributes\":{\"n\":\"3.4\"},\"end\":9036,\"start\":9010},{\"attributes\":{\"n\":\"4\"},\"end\":11834,\"start\":11824},{\"attributes\":{\"n\":\"5\"},\"end\":14445,\"start\":14435},{\"end\":14638,\"start\":14624},{\"end\":14827,\"start\":14797},{\"end\":18343,\"start\":18333},{\"end\":18414,\"start\":18394},{\"end\":18752,\"start\":18732},{\"end\":19126,\"start\":19116},{\"end\":19298,\"start\":19288},{\"end\":20463,\"start\":20454}]", "table": "[{\"end\":20452,\"start\":19677},{\"end\":21121,\"start\":20506}]", "figure_caption": "[{\"end\":18392,\"start\":18345},{\"end\":18730,\"start\":18417},{\"end\":19114,\"start\":18755},{\"end\":19286,\"start\":19128},{\"end\":19492,\"start\":19300},{\"end\":19677,\"start\":19495},{\"end\":20506,\"start\":20465}]", "figure_ref": "[{\"end\":7234,\"start\":7226},{\"end\":7310,\"start\":7302},{\"end\":8011,\"start\":8003},{\"end\":9623,\"start\":9615},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10767,\"start\":10759},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11587,\"start\":11579}]", "bib_author_first_name": "[{\"end\":21266,\"start\":21261},{\"end\":21278,\"start\":21271},{\"end\":21492,\"start\":21486},{\"end\":21507,\"start\":21502},{\"end\":21523,\"start\":21518},{\"end\":21770,\"start\":21766},{\"end\":22014,\"start\":22004},{\"end\":22030,\"start\":22024},{\"end\":22048,\"start\":22044},{\"end\":22327,\"start\":22318},{\"end\":22345,\"start\":22340},{\"end\":22362,\"start\":22356},{\"end\":22377,\"start\":22372},{\"end\":22625,\"start\":22620},{\"end\":22640,\"start\":22634},{\"end\":22655,\"start\":22652},{\"end\":22673,\"start\":22665},{\"end\":22688,\"start\":22682},{\"end\":22699,\"start\":22694},{\"end\":22714,\"start\":22707},{\"end\":22731,\"start\":22727},{\"end\":22748,\"start\":22741},{\"end\":22763,\"start\":22759},{\"end\":22765,\"start\":22764},{\"end\":23206,\"start\":23203},{\"end\":23223,\"start\":23218},{\"end\":23458,\"start\":23454},{\"end\":23798,\"start\":23790},{\"end\":23812,\"start\":23807},{\"end\":24067,\"start\":24063},{\"end\":24234,\"start\":24229},{\"end\":24249,\"start\":24244},{\"end\":24264,\"start\":24259},{\"end\":24281,\"start\":24273},{\"end\":24299,\"start\":24292},{\"end\":24311,\"start\":24308},{\"end\":24320,\"start\":24312},{\"end\":24333,\"start\":24326},{\"end\":24347,\"start\":24341},{\"end\":24360,\"start\":24356},{\"end\":24378,\"start\":24373},{\"end\":24395,\"start\":24390},{\"end\":24788,\"start\":24784},{\"end\":24806,\"start\":24801},{\"end\":24988,\"start\":24982},{\"end\":24990,\"start\":24989},{\"end\":25001,\"start\":24997},{\"end\":25003,\"start\":25002},{\"end\":25018,\"start\":25012},{\"end\":25020,\"start\":25019},{\"end\":25307,\"start\":25306},{\"end\":25322,\"start\":25316},{\"end\":25563,\"start\":25557},{\"end\":25577,\"start\":25573},{\"end\":25820,\"start\":25809},{\"end\":25836,\"start\":25830},{\"end\":25838,\"start\":25837},{\"end\":25853,\"start\":25845},{\"end\":25855,\"start\":25854},{\"end\":26170,\"start\":26165},{\"end\":26185,\"start\":26177},{\"end\":26187,\"start\":26186},{\"end\":26485,\"start\":26478},{\"end\":26502,\"start\":26495},{\"end\":26517,\"start\":26512},{\"end\":26531,\"start\":26526},{\"end\":26544,\"start\":26538},{\"end\":26557,\"start\":26553},{\"end\":26559,\"start\":26558}]", "bib_author_last_name": "[{\"end\":21269,\"start\":21267},{\"end\":21283,\"start\":21279},{\"end\":21500,\"start\":21493},{\"end\":21516,\"start\":21508},{\"end\":21530,\"start\":21524},{\"end\":21780,\"start\":21771},{\"end\":22022,\"start\":22015},{\"end\":22042,\"start\":22031},{\"end\":22059,\"start\":22049},{\"end\":22338,\"start\":22328},{\"end\":22354,\"start\":22346},{\"end\":22370,\"start\":22363},{\"end\":22389,\"start\":22378},{\"end\":22632,\"start\":22626},{\"end\":22650,\"start\":22641},{\"end\":22663,\"start\":22656},{\"end\":22680,\"start\":22674},{\"end\":22692,\"start\":22689},{\"end\":22705,\"start\":22700},{\"end\":22725,\"start\":22715},{\"end\":22739,\"start\":22732},{\"end\":22757,\"start\":22749},{\"end\":22774,\"start\":22766},{\"end\":22781,\"start\":22776},{\"end\":23216,\"start\":23207},{\"end\":23230,\"start\":23224},{\"end\":23467,\"start\":23459},{\"end\":23805,\"start\":23799},{\"end\":23815,\"start\":23813},{\"end\":24078,\"start\":24068},{\"end\":24242,\"start\":24235},{\"end\":24257,\"start\":24250},{\"end\":24271,\"start\":24265},{\"end\":24290,\"start\":24282},{\"end\":24306,\"start\":24300},{\"end\":24324,\"start\":24321},{\"end\":24339,\"start\":24334},{\"end\":24354,\"start\":24348},{\"end\":24371,\"start\":24361},{\"end\":24388,\"start\":24379},{\"end\":24399,\"start\":24396},{\"end\":24799,\"start\":24789},{\"end\":24813,\"start\":24807},{\"end\":24995,\"start\":24991},{\"end\":25010,\"start\":25004},{\"end\":25023,\"start\":25021},{\"end\":25314,\"start\":25308},{\"end\":25332,\"start\":25323},{\"end\":25339,\"start\":25334},{\"end\":25571,\"start\":25564},{\"end\":25583,\"start\":25578},{\"end\":25828,\"start\":25821},{\"end\":25843,\"start\":25839},{\"end\":25862,\"start\":25856},{\"end\":26175,\"start\":26171},{\"end\":26194,\"start\":26188},{\"end\":26493,\"start\":26486},{\"end\":26510,\"start\":26503},{\"end\":26524,\"start\":26518},{\"end\":26536,\"start\":26532},{\"end\":26551,\"start\":26545},{\"end\":26569,\"start\":26560},{\"end\":26576,\"start\":26571}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8151505},\"end\":21452,\"start\":21209},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8307266},\"end\":21700,\"start\":21454},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":122655110},\"end\":21926,\"start\":21702},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5273326},\"end\":22316,\"start\":21928},{\"attributes\":{\"id\":\"b4\"},\"end\":22543,\"start\":22318},{\"attributes\":{\"id\":\"b5\"},\"end\":23120,\"start\":22545},{\"attributes\":{\"doi\":\"arXiv\",\"id\":\"b6\"},\"end\":23365,\"start\":23122},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":784288},\"end\":23744,\"start\":23367},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6628106},\"end\":24006,\"start\":23746},{\"attributes\":{\"id\":\"b9\"},\"end\":24157,\"start\":24008},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12200521},\"end\":24733,\"start\":24159},{\"attributes\":{\"id\":\"b11\"},\"end\":24915,\"start\":24735},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16489696},\"end\":25242,\"start\":24917},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15619658},\"end\":25526,\"start\":25244},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2780493},\"end\":25761,\"start\":25528},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9530137},\"end\":26101,\"start\":25763},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15539264},\"end\":26393,\"start\":26103},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1528374},\"end\":26929,\"start\":26395}]", "bib_title": "[{\"end\":21259,\"start\":21209},{\"end\":21484,\"start\":21454},{\"end\":21764,\"start\":21702},{\"end\":22002,\"start\":21928},{\"end\":23452,\"start\":23367},{\"end\":23788,\"start\":23746},{\"end\":24227,\"start\":24159},{\"end\":24980,\"start\":24917},{\"end\":25304,\"start\":25244},{\"end\":25555,\"start\":25528},{\"end\":25807,\"start\":25763},{\"end\":26163,\"start\":26103},{\"end\":26476,\"start\":26395}]", "bib_author": "[{\"end\":21271,\"start\":21261},{\"end\":21285,\"start\":21271},{\"end\":21502,\"start\":21486},{\"end\":21518,\"start\":21502},{\"end\":21532,\"start\":21518},{\"end\":21782,\"start\":21766},{\"end\":22024,\"start\":22004},{\"end\":22044,\"start\":22024},{\"end\":22061,\"start\":22044},{\"end\":22340,\"start\":22318},{\"end\":22356,\"start\":22340},{\"end\":22372,\"start\":22356},{\"end\":22391,\"start\":22372},{\"end\":22634,\"start\":22620},{\"end\":22652,\"start\":22634},{\"end\":22665,\"start\":22652},{\"end\":22682,\"start\":22665},{\"end\":22694,\"start\":22682},{\"end\":22707,\"start\":22694},{\"end\":22727,\"start\":22707},{\"end\":22741,\"start\":22727},{\"end\":22759,\"start\":22741},{\"end\":22776,\"start\":22759},{\"end\":22783,\"start\":22776},{\"end\":23218,\"start\":23203},{\"end\":23232,\"start\":23218},{\"end\":23469,\"start\":23454},{\"end\":23807,\"start\":23790},{\"end\":23817,\"start\":23807},{\"end\":24080,\"start\":24063},{\"end\":24244,\"start\":24229},{\"end\":24259,\"start\":24244},{\"end\":24273,\"start\":24259},{\"end\":24292,\"start\":24273},{\"end\":24308,\"start\":24292},{\"end\":24326,\"start\":24308},{\"end\":24341,\"start\":24326},{\"end\":24356,\"start\":24341},{\"end\":24373,\"start\":24356},{\"end\":24390,\"start\":24373},{\"end\":24401,\"start\":24390},{\"end\":24801,\"start\":24784},{\"end\":24815,\"start\":24801},{\"end\":24997,\"start\":24982},{\"end\":25012,\"start\":24997},{\"end\":25025,\"start\":25012},{\"end\":25316,\"start\":25306},{\"end\":25334,\"start\":25316},{\"end\":25341,\"start\":25334},{\"end\":25573,\"start\":25557},{\"end\":25585,\"start\":25573},{\"end\":25830,\"start\":25809},{\"end\":25845,\"start\":25830},{\"end\":25864,\"start\":25845},{\"end\":26177,\"start\":26165},{\"end\":26196,\"start\":26177},{\"end\":26495,\"start\":26478},{\"end\":26512,\"start\":26495},{\"end\":26526,\"start\":26512},{\"end\":26538,\"start\":26526},{\"end\":26553,\"start\":26538},{\"end\":26571,\"start\":26553},{\"end\":26578,\"start\":26571}]", "bib_venue": "[{\"end\":23572,\"start\":23529},{\"end\":21322,\"start\":21285},{\"end\":21569,\"start\":21532},{\"end\":21808,\"start\":21782},{\"end\":22113,\"start\":22061},{\"end\":22424,\"start\":22391},{\"end\":22618,\"start\":22545},{\"end\":23201,\"start\":23122},{\"end\":23527,\"start\":23469},{\"end\":23870,\"start\":23817},{\"end\":24061,\"start\":24008},{\"end\":24438,\"start\":24401},{\"end\":24782,\"start\":24735},{\"end\":25069,\"start\":25025},{\"end\":25379,\"start\":25341},{\"end\":25637,\"start\":25585},{\"end\":25923,\"start\":25864},{\"end\":26240,\"start\":26196},{\"end\":26657,\"start\":26578}]"}}}, "year": 2023, "month": 12, "day": 17}