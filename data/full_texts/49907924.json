{"id": 49907924, "updated": "2023-09-30 19:34:10.936", "metadata": {"title": "Hardware Optimizations of Dense Binary Hyperdimensional Computing: Rematerialization of Hypervectors, Binarized Bundling, and Combinational Associative Memory", "authors": "[{\"first\":\"Manuel\",\"last\":\"Schmuck\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Benini\",\"middle\":[]},{\"first\":\"Abbas\",\"last\":\"Rahimi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 7, "day": 20}, "abstract": "Brain-inspired hyperdimensional (HD) computing models neural activity patterns of the very size of the brain's circuits with points of a hyperdimensional space, that is, with hypervectors. Hypervectors are $D$-dimensional (pseudo)random vectors with independent and identically distributed (i.i.d.) components constituting ultra-wide holographic words: $D = 10,000$ bits, for instance. At its very core, HD computing manipulates a set of seed hypervectors to build composite hypervectors representing objects of interest. It demands memory optimizations with simple operations for an e cient hardware realization. In this paper, we propose hardware techniques for optimizations of HD computing, in a synthesizable VHDL library, to enable co-located implementation of both learning and classification tasks on only a small portion of Xilinx(R) UltraScale(TM) FPGAs: (1) We propose simple logical operations to rematerialize the hypervectors on the fly rather than loading them from memory. These operations massively reduce the memory footprint by directly computing the composite hypervectors whose individual seed hypervectors do not need to be stored in memory. (2) Bundling a series of hypervectors over time requires a multibit counter per every hypervector component. We instead propose a binarized back-to-back bundling without requiring any counters. This truly enables on-chip learning with minimal resources as every hypervector component remains binary over the course of training to avoid otherwise multibit component. (3) For every classification event, an associative memory is in charge of finding the closest match between a set of learned hypervectors and a query hypervector by using a distance metric. This operator is proportional to [...]", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1807.08583", "mag": "2963492949", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jetc/SchmuckBR19", "doi": "10.1145/3314326"}}, "content": {"source": {"pdf_hash": "b3b20a21d78f24a19a0422224fa97de922f1ea64", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1807.08583v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/338354/1/JETC19.pdf", "status": "GREEN"}}, "grobid": {"id": "1ffbd5c523ae8937d62ad04183de5352b3ab85f9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b3b20a21d78f24a19a0422224fa97de922f1ea64.txt", "contents": "\nHardware Optimizations of Dense Binary Hyperdimensional Computing: Rematerialization of Hypervectors, Binarized Bundling, and Combinational Associative Memory\n\n\nManuel Schmuck \nLUCA BENINI\nETH Z\u00fcrich\nUniversit\u00e0 di Bologna ABBAS RAHIMI\nETH Z\u00fcrich\n\n\nEth Z\u00fcrich \nLUCA BENINI\nETH Z\u00fcrich\nUniversit\u00e0 di Bologna ABBAS RAHIMI\nETH Z\u00fcrich\n\n\nHardware Optimizations of Dense Binary Hyperdimensional Computing: Rematerialization of Hypervectors, Binarized Bundling, and Combinational Associative Memory\nCCS Concepts: \u2022 eory of computation \u2192 Random projections and metric embeddingsOnline learn- ing theoryActive learning\u2022Computer systems organization \u2192 System on a chipEmbedded hard- wareNeural networks\u2022Hardware \u2192 Hardware acceleratorsAdditional Key Words and Phrases: Hyperdimensional computing, on-chip learning, FPGA, rematerialization, binarized temporal bundling, single-cycle associative memory, electromyography, biosignals\nBrain-inspired hyperdimensional (HD) computing models neural activity pa erns of the very size of the brain's circuits with points of a hyperdimensional space, that is, with hypervectors. Hypervectors are D-dimensional (pseudo)random vectors with independent and identically distributed (i.i.d.) components constituting ultra-wide holographic words: D = 10, 000 bits, for instance. At its very core, HD computing manipulates a set of seed hypervectors to build composite hypervectors representing objects of interest. It demands memory optimizations with simple operations for an e cient hardware realization. In this paper, we propose hardware techniques for optimizations of HD computing, in a synthesizable open-source VHDL library, to enable co-located implementation of both learning and classi cation tasks on only a small portion of Xilinx \u00ae UltraScale \u2122 FPGAs: (1) We propose simple logical operations to rematerialize the hypervectors on the y rather than loading them from memory. ese operations massively reduce the memory footprint by directly computing the composite hypervectors whose individual seed hypervectors do not need to be stored in memory.(2) Bundling a series of hypervectors over time requires a multibit counter per every hypervector component. We instead propose a binarized back-to-back bundling without requiring any counters. is truly enables on-chip learning with minimal resources as every hypervector component remains binary over the course of training to avoid otherwise multibit components. (3) For every classi cation event, an associative memory is in charge of nding the closest match between a set of learned hypervectors and a query hypervector by using a distance metric. is operator is proportional to hypervector dimension (D), and hence may take O(D) cycles per classi cation event. Accordingly, we signi cantly improve the throughput of classi cation by proposing associative memories that steadily reduce the latency of classi cation to the extreme of a single cycle. (4) We perform a design space exploration incorporating the proposed techniques on FPGAs for a wearable biosignal processing application as a case study. Our techniques achieve up to 2.39\u00d7 area saving, or 2337\u00d7 throughput improvement. e Pareto optimal HD architecture is mapped on only 18340 con gurable logic blocks (CLBs) to learn and classify ve hand gestures using four electromyography sensors.\n\nINTRODUCTION\n\nHyperdimensional (HD) computing [13,15] is a brain-inspired computational approach based on the understanding that brains compute with pa erns of neural activity that are not readily associated with scalar numbers. In fact, the brain's ability to calculate with numbers is feeble. However, due to the very size of the brain's circuits, we can model neural activity pa erns with points of a hyperdimensional space, that is, with hypervectors. When the dimensionality is in the thousands, operations with hypervectors create a computational behavior with remarkable properties [11]. HD computing builds upon a well-de ned set of highly parallel operations with random hypervectors, is extremely robust in the presence of failures, and o ers a complete computational paradigm that is easily applied to many learning applications [29]. Examples include analogy-based reasoning [14], latent semantic analysis [16], language recognition [10,32], text classi cation [23], speech recognition [8,34], physical activity prediction [35,36], robot learning by demonstration [19,24], and several biosignal processing tasks such as electromyography (EMG) [18,21,22,28], electroencephalography (EEG) [31,33], electrocorticography (ECoG) [1], and in general ExG [30].\n\nIn contrast with traditional approaches, learning in HD computing is fast and computationally balanced with respect to classi cation by reusing the same algorithmic and architectural constructs for both modes of operation. Its learning is not iterative and thus requires far fewer operations than other approaches (see [30] for an overview). Another advantage of HD computing is the simplicity of its basic operations, which is an important factor for energy e ciency. For instance, HD computing achieves 2\u00d7 lower energy consumption at iso-accuracy when compared to a highlyoptimized support vector machine (SVM) with xed-point operations on a commercial embedded ARM Cortex M4 processor for an EMG classi cation task [22]. In what follows, we brief these basic operations.\n\nAt its very core, HD computing is all about generating, manipulating, and comparing hypervectors as ultra-wide words. As the rst step, D-dimensional hypervectors are initially generated with independent and identically distributed (i.i.d.) components. Second, these seed hypervectors are manipulated to construct composite hypervectors, as richer representations, with componentwise arithmetic operations by needing to communicate with only a local component or an immediate neighbor. By using dense binary codes for hypervectors [12], the arithmetic operations simply involve bitwise XOR, shi (or rotate), and majority gates [32]. Finally, the constructed hypervectors are compared for similarity using a distance metric whose computation involves a reduction operator proportional to the hypervector dimension (D) [9,32]. See Section 2 for details. ese operators-at the basis of both learning and inference (Section 3)-demand a memory-centric architecture for e cient and local ultra-wide word processing. Emerging nanotechnologies with dense 3D integration can provide a natural t [20,39,40].\n\nIn this paper, we propose hardware techniques to optimize the aforementioned operators to build an e cient acceleration engine on an FPGA. As a result of our hardware optimizations (Section 4), we provide a synthesizable VHDL library 1 of fully con gurable modules exploring trade-o s between area and throughput of the operators. Our contributions are as follows:\n\n(1) We propose a generic hypervector manipulator (MAN) module as a fully combinational logic consisting of OR-XOR gates and preprogrammed connections. e MAN module substitutes the expensive memory storage for maintaining seed hypervectors with cheaper logical operations to rematerialize them. Hence, representations of composite hypervectors are constructed directly by rematerializing the seed hypervectors as a consequence of reusing the generic MAN modules that form a combinational network architecture without requiring any memory storage.\n\n(2) e arithmetic operations of HD computing with dense binary code exhibit their simplest form by performing local and bitwise operations on binary components. is however does not hold for the majority gate when it is applied to bundle a series of hypervectors over time, i.e., among di erent training examples. Implementation of the majority gate requires to maintain intermediate (i.e., partially bundled) hypervector representation using a set of D multibit countersevery counter counts the number of 1s in a speci c dimension. We rather reuse the generic MAN module that replaces the multibit hypervector components with binarized hypervector components by incrementally applying an approximate majority gate for every training example. Such a binarized back-to-back bundling enables the representational system to continuously stay in the binary space that is essential for e cient on-chip learning during the course of online learning.\n\n(3) e common denominator of all architectures of HD computing is the extensive use of distance computation in the associative memory that typically takes O(D) cycles per every event of classi cation. We propose associative memories to signi cantly reduce the classi cation latency to single cycle.\n\n(4) We perform a design space exploration of our library modules for an application which recognizes hand gestures from four EMG senors (Section 5). It shows that functionally equivalent HD architectures can be composed achieving up to 2.39\u00d7 area saving, or 2337\u00d7 throughput improvement. e Pareto optimal HD architecture is fully synthesized on only 18340 CLBs of the Xilinx \u00ae UltraScale \u2122 FPGAs, and shows simultaneous 2.39\u00d7 area and 986\u00d7 throughput improvements compared to a baseline HD architecture.\n\n\nBACKGROUND\n\nHD computing is rooted in the observation that key aspects of human memory, perception and cognition can be explained by the mathematical properties of hyperdimensional spaces, and that a powerful system of computing can be built on the rich algebra of hypervectors [13]. A further motivation is the fact that brains compute with pa erns of neural activity that are not readily associated with numbers. In fact, recognizing the very size of the brain's circuits, we can model neural activity pa erns with points in a hyperdimensional space. Computing in hyperdimensional space is understood partly in terms of the linear algebra and probability of arti cial neural nets, and partly in terms of the abstract algebra and geometry of hyperdimensional spaces. Groups, rings, and elds over hypervectors become the underlying computing structure, with permutations, mappings, and inverses as primitive computing operations, and with randomness as a way to label new objects and entities.\n\nHypervectors are D-dimensional, holographic, and (pseudo)random with i.i.d. components. It means that the contained information in a hypervector is distributed equally over all D components: neither a component nor a subset of them have a speci c meaning, hence the information degrades in relation to the number of failing components irrespective of their position. e high dimensionality yields a huge number of di erent, nearly orthogonal hypervectors in such space [11]. ey can be mathematically manipulated for solving cognitive tasks, e.g., Raven's progressive matrices [4], analogical reasoning [14], and practical learning and classi cation tasks [7, 8, 10, 18-24, 28, 29, 31-36, 40]. Examples of such computing include Holographic Reduced Representation [25,26], Binary Spa er Code [12], Multiply-Add-Permute architecture [5], Random Indexing [16], and Semantic Pointer Architecture Uni ed Network [3], collectively referred to as Vector Symbolic Architecture [6]. ey di er in the type of components, and the types of operations, however, the key properties are shared by hypervectors of many kinds, all of which can serve as the computational infrastructure. To ease the hardware realization, we focus on Binary Spa er Code (BSC), where the components of hypervectors are binary and dense, meaning the probability of having a 1 or a 0 is equal (p = 1/2) [12].\n\n\nMeasure of Similarity\n\nUsing BSC, {0, 1} D , the similarity between two hypervectors is given by the number of components at which they di er, the so-called Hamming distance. We use the normalized version of this metric by dividing by D denoted as: d(X , Y ) : {0, 1} D \u00d7 {0, 1} D \u2192 [0, 1] to express the distance on a real scale of 0 to 1. Figure 1 shows the normalized Hamming distance distribution of hypervectors in D-dimensional spaces where D \u2208 {100, 1000, 10000}. As we go to higher dimensions from D = 100 to D = 10, 000, we observe an outstanding property: most points are D/2 bits apart from each other, which yields a normalized Hamming distance of d \u2248 0.5, and stands for two nearly orthogonal hypervectors. is stems from the binomial distribution for p = 1/2 and n = D, where D/2 is the mean. Correlated hypervectors yield d \u2248 0 whereas d \u2248 1 implies anti-correlation [13].\n\nOrthogonality Condition. When approximating the discrete binomial distribution with the continuous normal distribution, its standard deviation is \u221a D/2. According to the normal distribution, \u2248 68.2% of the space lies within one standard deviation from the mean or within \u221a D \u00b1 1 standard deviations from a point in the hyperdimensional space [11]. If we increase the range to 6 standard deviations, already \u2248 99.9999998% of the space lies within that range. is marks our orthogonality threshold as d or t ho onal it =\n\u221a D \u00b7( \u221a D\u00b16) 2\u00b7D\nwhich states that with a chance of \u2248 99.9999998% two random hypervectors exhibit a normalized Hamming distance in the aforementioned range. For D = 10, 000 this yields a range between 0.47 and 0.53 [11]. In other words, almost all the space lies at approximately the mean distance of [0.47,0.53] from a chosen random point; this implies that for any signi cant deviation from distance 0.5, the distribution quickly becomes very sparse.\n\n\nHD Arithmetic Operations\n\ne HD algorithm starts by choosing a set of seed hypervectors as initial items. ey are stored in a so-called item memory (IM) as a symbol table or dictionary of all the hypervectors de ned in the system. ey stay xed throughout the computation, and they serve as seeds from which further representations are made. HD computing builds upon a well-de ned set of operations with the seed hypervectors [13]. ese arithmetic operations are used for encoding and decoding pa erns. e power and versatility of arithmetic derives from the fact that addition and multiplication form an algebraic eld, and permutation of hypervector components takes it beyond both arithmetic and linear algebra.\n\nAddition (Bundling). e sum of binary hypervectors is de ned as the componentwise majority function (also called the median operator) with ties broken at random. is means, when adding an even number of hypervectors, in case of disagreement for a component (equal number of 1s and 0s), the majority is randomly chosen. It is denoted as A \u2295 B. e sum of two hypervectors stores information from both hypervectors, due to the mathematical properties of vector addition, therefore the operation is also called bundling. Bundling two hypervectors yields a hypervector which is similar to both of them, hence it is well-suited for representing sets or multisets. However, when breaking ties at random, the bundling operation becomes non-causal. Furthermore, the bundling is commutative but not associative and is only approximately invertible.\n\nMultiplication (Binding). e product of two binary hypervectors is de ned as the componentwise XOR or \"addition modulo 2\", and is denoted as A \u2297 B.\n\ne resulting hypervector is dissimilar (orthogonal) to both its constituent hypervectors, which is why multiplication is well-suited for binding two hypervectors. Binding is commutative, associative and distributes over bundling. e operation can be inverted and also preserves distances between hypervectors, meaning two similar hypervectors (a er binding) are mapped to equally similar ones.\n\nPermutation. e third operation, denoted \u03c1(A), is the permutation operation, which shu es a hypervector's components by rotating it in space. It is implemented as a cyclic shi by one position. Permuting a hypervector produces a dissimilar, pseudo-orthogonal hypervector, which can be exploited to bypass the commutativity of the other operations. is is crucial when storing sequences, where e.g., a-b-c should be distinguishable from b-c-a. Permutation is invertible and preserves distances. It distributes over both bundling and binding. ese three operations can be combined to encode structures such as variable/value records, sequences, and lists-essentially any data structure. For example, let us consider three variables x, , z and their values a, b, c. Each of them is mapped to a (random) hypervector X , Y , A, B etc., which are stored in the IM. en, the entire of a record is encoded to a single hypervector by binding each value to its variable and bundle them to form the holistic record:\nR = (X \u2297 A) \u2295 (Y \u2297 B) \u2295 (Z \u2297 C).\nTo nd the value of x, we unbind the record with the inverse of X (which is X itself),\u00c3 = X \u2297 R which gives us a hypervector\u00c3 as noisy version of A. A er comparing it with the hypervectors that are stored in the AM, we nd A to be the most similar one (i.e., the lowest Hamming distance), and thus the sought value.\n\n\nLEARNING AND CLASSIFYING MULTICHANNEL BIOSIGNALS WITH HD\n\nCOMPUTING In this section, we describe how to use HD computing for learning and classi cation tasks. We focus on wearable biosignal processing applications with multichannel noisy sensors for which HD computing achieves faster training and lower energy consumption and memory than SVMs [1,22]. One application example includes recognizing hand gestures from a stream of EMG sensors to control a prosthetic device [22,28]. e performance of HD computing however depends on good design of a network architecture that demands a recon gurable (FPGA) fabric to e ciently arrange the HD primitive operations based on the given task. We present a generic architecture to project multichannel sensory inputs from original representation to hyperdimensional space, where the arithmetic operations are combined to learn and classify examples. While this paper focuses on EMG signals, other streaming multichannel sensor data such as ECoG [1], EEG [31,33], ExG [30], speech [8,34], smell [7] can be equally applicable.\n\ne dataset [28] used in this paper is based on a four-channel EMG data acquisition, among ve subjects, for the most common hand gestures in daily life. e selected gestures are: closed hand, open hand, 2-nger pinch, point index, and the rest position. e recording is composed of 10 trials of every gestures three seconds each. We use 25% of this dataset for training that can be performed online. e gestures are sampled at 500 Hz, followed by a low pass lter, and an envelope signal extraction; [28] provides further details about the setup.\n\n\nHD Architecture\n\nAs shown in Figure 2, an HD architecture consists of three main modules: mapping and spatial encoder, temporal encoder, and associative memory. e mapping and encoding modules intend to capture information that can be extracted from the inputs (i.e., the enveloped EMG signals), into a hypervector representing a gesture. Gesture hypervectors, extracted from various trials, are bundled to form a prototype hypervector representing a class of gestures. e associative memory (AM) stores a prototype hypervector for every class, which contains the encoded information of all labelled inputs during the training phase. During inference, classifying input data is carried out by comparing the unlabelled encoded hypervectors with all stored prototype hypervectors, and returning the label of the most similar one.\n\n\nMapping and Spatial Encoder\n\nFirst, the analog EMG signals have to be quantized to q discrete levels, where q indicates the resolution of the signal. In analogy to the record example in the previous section, the di erent EMG channels represent the variables or elds, and the discretized signals represent the values of the variables. All channels are treated as separate and independent, therefore we allocate each one a random and thus orthogonal hypervector, which are xed throughout the computation in the IM: Figure 3a shows the IM with four channels. Each of the channel variables has a corresponding value, i.e., the discretized signals. When mapping quantities from the discrete number space to the hypervector space, we want to retain their similarity: e.g., with a resolution of q = 21 levels, a value of 5 is only slightly larger than a value of 4, hence their allocated hypervectors shall not be orthogonal [28]. For mapping such quantized or even continuous values into hypervectors various techniques can be used including thermometer codes, locality-sensitive hashing, or generally, random projection [27]. We use the following simple method to map the values to a continuous vector space. A random seed hypervector is taken for the smallest value and the hypervectors for the other levels are generated such that they are gradually further away from the seed up to the largest value, whose hypervector is orthogonal to the seed. We can accomplish this by randomly choosing D/2 components of the seed and split them into q \u2212 1 groups which equally contain (D/2)/(q \u2212 1) components. e hypervectors are then generated from the seed by taking one group a er the other and ipping their components. For the last hypervector, exactly D/2 components are ipped, making it orthogonal to the seed. ese generated signal hypervectors are denoted by S where \u2208 [0, q \u2212 1], that are stored in the so-called continuous item memory (CIM). Figure 3b illustrates a CIM with q = 21:\nC 1 \u22a5 C 2 \u22a5 C 3 ... \u22a5 C n .d(S n , S n+i ) = 0.5 \u00b7 i q\u22121 hence d(S 0 , S q\u22121 ) = 0.5 .\nAs mentioned in Section 2.2, we aim to bind the values to their variables and bundle them to form a holistic record (R) to capture spatial information between all channels. e signal hypervector of a channel at time t, is denoted by\nS [t] where \u2208 [0, q \u2212 1].\nHence, a record is computed for a given time-aligned sample of all channels:\nR[t] = (C 1 \u2297S [t]) \u2295 (C 2 \u2297S [t]) \u2295 (C 3 \u2297S [t]) \u2295 (C 4 \u2297S [t])\n. As shown in Figure 2, this record contains the signal information of all channels, while distinguishing the source of the signals (i.e., the channels).\n\n\nTemporal Encoder\n\nWe can encode sequences by using the permutation operation \u03c1. Hence, we can capture not only the spatial correlation across the channels, but also the temporal correlation between subsequent samples. We call a sequence of N record hypervectors as an N -gram hypervector.\n\nAs already mentioned, a sequence of hypervectors can be encoded uniquely by permuting the hypervectors before binding them. e sequence is encoded by rotating the rst spatial record N \u2212 1 times, the second N \u2212 2 times, and the (N \u2212 1)th only once. e N th hypervector is untouched (not permuted). ese new hypervectors are nally bound to an N -gram (see Figure 2). For large Ngrams, this becomes:\nN -gram[t] = N \u22121 i=0 \u03c1 i (R[t \u2212 i]\n). An N -gram contains the spatial information of N subsequent samples with di erent timestamps, making it a spatiotemporal hypervector.\n\n\nLearning and Classification in Associative Memory\n\nIn a typical training se ing, a set of labelled examples is provided per every class. By encoding the sensory data, a current gesture example is represented by an N -gram[t] hypervector. e HD architecture learns from these N -gram hypervectors that are produced over time. A number of N -gram hypervector examples (e.g., k) with the same label are bundled to produce a prototype hypervector representing the class of interest:\nP Label i = N -gram Label i [t] \u2295 ... \u2295 N -gram Label i [t + k].\nOnce training is done, the binarized prototype hypervectors are stored in the AM as learned pa erns.\n\nis temporal bundling of N -grams over the course of training requires D counters and thresholders to implement the majority function.   As soon as the AM is trained for each class, it can identify the corresponding class of an unlabelled N -gram, which is called a query hypervector. More speci cally, the AM computes the Hamming distance between the query hypervector and each of its prototype hypervector. It then selects the highest similarity and returns its associated label. As shown in Figure 2, the same construct is reused during inference, the only di erence is that during training the prototypes are wri en into the AM while during inference they are read and compared with the query.\n\n\nHARDWARE OPTIMIZATIONS OF DENSE BINARY HD COMPUTING\n\nIn this section, we present the main contributions of the paper. We present our techniques to optimize hardware realization of HD computing suitable for CMOS fabrics. HD computing demands a large amount of bits to be stored for each data item that further poses a memory bandwidth issue, for instance the IP RAMs of FPGAs are optimized for usually no more than 72 bits in parallel [41]. Storing or loading one hypervector in this fashion would require hundreds of cycles. Accordingly, optimizing the architecture of HD computing should focus on minimizing the number of stored hypervectors. Furthermore, the bitwise operations need to be kept as simple as possible, since they are replicated over the whole dimension of a hypervector. Most architectural constructs are shared among various HD classi ers and thus the optimizations virtually concern all HD computing applications.\n\nAs a result of various hardware optimizations, we introduce a synthesizable VHDL library of fully con gurable modules which comprises di erent implementations. e VHDL library consists of interchangeable modules including three types of spatial encoder, two types of temporal encoder, and three types of AM, that are listed in Table 1. A functioning HD architecture can be con gured by connecting one type of each of the modules in series. e modules operate independently and pass hypervectors a er synchronizing via handshake signals. ey all di er greatly in area and throughput, where the number of cycles needed to process a data item (CPDI) has the biggest in uence on throughput. Table 1 shows the CPDI for the di erent modules.\n\n\nMapping Multichannel Sensory Inputs\n\nMapping the input data of more than one channel to the hyperdimensional space can be done in a parallel fashion as shown in Figure 2. e required memory for the IM and the CIM is n c \u00d7 q \u00d7 D where n c is the number of channels, q is the quantization of input signal, and D the hypervector dimension. For the EMG task (see Figure 2) this would be equal to 840 kbits to only store the seed hypervectors. is poses limitations when a large number of channels [21] or input quantization is used. A rst step is to trade the high throughput against a smaller memory footprint by sharing the resources. Table 1. Cycles per data item (CPDI) orders of the di erent library modules.\n\n(a) Spatial encoder modules.\n\n\nModule CPDI\nLUT O(1) CA O(n channels ) MAN O(n channels ) (b) Temporal encoder modules. Module CPDI BC O(1) a B2B\nO (1) a is holds only for inference. During training, the order is of the number of training samples.\n\n(c) Associative memory modules. To reduce this memory footprint we can exploit the holographic nature of HD representation: the individual bits in a hypervector do not represent anything. What is important is the relation or similarity between two hypervectors. A hypervector can be altered or \"manipulated\" to a di erent hypervector by switching certain bits as a function of the similarity that we want to establish. For example, to obtain an orthogonal hypervector, we have to switch half of its bits (which ones does not ma er), whereas to obtain a similar hypervector, we only switch a (small) portion of the bits (see Section 2.1).\n\n\nModule CPDI\nBS O(D) CMB O(1) VS O(n classes )\nManipulating hypervectors in a controlled manner can replace complex constructs throughout the whole architecture. For this purpose, a generic hypervector manipulator (MAN) module is designed (Figure 4), which can be con gured in depth and width, and is xed by a connectivity matrix, which determines the connections between wires. An example connectivity matrix used for mapping is shown in Figure 7.\n\nEvery cell of the connectivity matrix a ects, whether a certain bit of the input hypervector can be switched by a bit (or even several bits) of the input manipulator. e MAN module is a simple combination of OR and XOR gates. If a cell (m, n) of the connectivity matrix is set to 1, the m-th bit of the input manipulator can a ect the n-th bit of the input hypervector: when the m-th bit of the input manipulator is logically high it toggles the n-th bit of the input hypervector. e number of 1s in a row of connectivity matrix also represents how dissimilar the output hypervector will be to the input hypervector when the input manipulator bit of that row is logical high: the fewer the number, the more similar.\n\nAs described in Section 3.2, \"close\" input values are mapped to similar hypervectors using a CIM. is CIM can be replaced by a MAN module that produces similar hypervectors according to the input value. First, the quantized input value in binary representation is mapped to an s-hot representation (by e.g., a small lookup table), where s is the input/signal value (see Figure 6).\n\nis s-hot code serves as the input manipulator, and gradually switches more and more bits of a seed input hypervector as the input value goes higher, and eventually produces an orthogonal hypervector when all q bits are hot (q is the quantization). is allows to rematerialize desired hypervectors from a seed by keeping track of the input value.\n\nWhich bits are switched is chosen randomly (without the possibility to choose a bit twice), only the number of bits per \"input quantum\"-represented by a row in the connectivity matrix-is determined. It is equal to D/2/(q \u2212 1). Moreover, every input hypervector bit can only be switched by one input manipulator bit. is results in a MAN module containing only XOR gates. e input hypervector that is manipulated is a constant seed hypervector (S 0 ) which represents the lowest input value, or 0-hot. is seed hypervector is simply hardwired connections to source and ground. Summing up, the whole continuous item memory, or CIM, is replaced with a rather small s-hot lookup table memory of size q \u00d7 q, some wires, and D/2 XOR gates.\n\n\n4.1.2\n\nReproducing IM with Cellular Automata. As mentioned in Section 3.2, we account for the spatial multichannel information to determine which channel the data originated from. is is done by binding a channel hypervector, that is unique for every channel, with the signal hypervector. e channel hypervectors are typically stored in the IM with a memory of size n c \u00d7 D. When mapping the input data in the parallel fashion, the IM can be replaced by hard wires tied to source and ground since the channel hypervectors are constant. However, with the serial mapping, they need to be stored in the IM.\n\nOne way to replace the IM is by using a one-dimensional cellular automaton (CA) with a neighborhood of 3, applying rule 30 [38]. is rule exhibits chaotic behaviour that is well-matched to produce a sequence of (quasi-)random hypervectors. When using a CA with D cells and a random hypervector as initial state, it generates (quasi-)random and orthogonal hypervectors every cycle (see Figure 8). By rese ing the CA registers, the same sequence can be reproduced (i.e.,  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64   1  2  3  4  5  is allows us to replace the IM (see Figure 9) by only de ning the initial state of the CA as a seed hypervector and le ing it generate the other orthogonal hypervectors 2 for the rest of the channels. anks to the chaotic behaviour of the CA, this approach works for virtually any number of channels: clocking the CA for 500 cycles produces the channel hypervectors for 500 channels only from the initial state hypervector (see Figure 8).\n\nAlthough the gate logic required for each cell in CA is quite simple-only consisting of 3 inverters, 4 two-input AND gates and 2 two-input OR gates-it is still replicated D times. When looking for a solution to generate orthogonal hypervectors at relatively low costs, CA are an excellent choice, whereas when looking for an optimal solution for spatial encoding, further improvement can be done as described in the following section. e connectivity matrix in this case is identical to an IM and has an average of D/2 1s per row as shown in Figure 10. Feeding signal hypervector to the second MAN module and se ing one bit of its input manipulator logical high at a time yields the same outcome as binding the signal hypervector with a channel hypervector (see Figure 13c). e second MAN module (replacing the IM) requires more gates due to its dense connections than the rst one (replacing the CIM). e chance that a channel hypervector switches a certain bit is 0.5 (the probability of having a 1 in a component), hence this yields an average of n c /2 connections per column in the connectivity matrix (see Figure 10) which have to be OR-ed before 2 In case the \"randomness\" of rule 30 is not enough, the neighbourhood can be extended to form a more complex CA as in [37]. going into the XOR gate. is operator per hypervector bit is replicated D times to replace the whole IM.\n\n\nSpatial\n\nEncoding e hypervectors that contain information of the input signal values and the channels should be bundled in the spatial encoder. In Section 2.2, the bundling operation is characterized as a method to store the information of multiple hypervectors in a single hypervector, called a record, which is similar to all of the input hypervectors. e information of a hypervector is contained in another as long as they do not violate the similarity condition (Section 2.1). Here, we investigate how well this task is accomplished by the majority function, and how it can be implemented in hardware and whether there are other approaches to achieve the same goal.\n\n\nThe Three Problems of the Majority Function. e Majority Function of an Even Number of Inputs. e majority function (or vote) for binary inputs is self-explanatory and only yields a clear result with an odd number of inputs.\n\nis is why the concept of braking ties at random is introduced [13], which makes the operation noncausal for an even number of inputs and is identical to bundling an additional random (and thus orthogonal) hypervector into the record. erefore, two records, that are supposed to be equal, become (slightly) dissimilar. Instead of \"wasting\" said similarity, an additional hypervector can be introduced, that contains useful information, to break the ties. In the case of bundling hypervectors from multichannel, useful information could come from an additional channel. If this is not an option, we can synthetically create that information. It should be \"useful\" in the sense, that it is unique for the given input and also causal. Binding a constant hypervector would lead to all output hypervectors being slightly similar to each other even if they are supposed to be orthogonal. Instead, by simply binding any two of the input hypervectors (see Figure 11), we can create an additional feature, which represents the input data and is useful as stated before.\n\nUnfairness of the Majority Function. Bundling hypervectors with the majority vote does not yield their mean hypervector but strongly tends to the majority of the hypervectors. is means, if we want to store the information of e.g., three hypervectors, where two of them are equal and the   1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63   other is orthogonal, the information of the la er is lost entirely (see Figure 12b). e same situation occurs when bundling two sets of hypervectors to one record, where the sets are dissimilar to each other, but similar within. e smaller set will not be recalled at all. In Section 4.5.1, another bundling approach will be presented, which is completely fair in this case.\n\nWhen bundling only orthogonal hypervectors, this problem does not occur and the majority function is fair (Figure 12a). is rises the question of the \"capacity\" of the bundling operations (see Section 4.5.2).\n\nLack of Associativity. When a empting to implement the bundling operation, one quickly comes across a mathematical property that is necessary to conduct an operation in an iterative manner: associativity. e majority function lacks this property, meaning a set of hypervectors can only be bundled altogether, but not step by step: a \u2295 b \u2295 c (a \u2295 b) \u2295 c. Fortunately, one is not tied to mathematical properties, when it comes to the algorithmic and architectural implementation of an operation. e workaround lies in storing the current vote over an iteration.\n\n\nBidirectional Saturating Counters as a Hardware Implementation of the Majority Function.\n\nA naive approach to store the current majority vote would be to count the vote for 1s and 0s with two separate counters and compare their values to get the majority. is would require a memory of 2 \u00b7 D \u00d7 log 2 (n c + 1) which, for only n c = 4 input channels in our EMG task would already yield 60, 000 bits. e two counters can be combined to a single one that counts up or down depending on the value of the current bit to reduce the memory to D \u00d7 ( log 2 (n c + 1) + 1). e next big improvement is made by exploiting the random nature of orthogonal hypervectors. Observing a single component of the input vectors, the probability of a long sequence of either 1s or 0s is small, implying the counter usually does not have to count all the way up to the maximum possible vote, but stays within a certain range. Taking a counter with a xed width and forcing it to saturate whenever it would traverse that range, assures that the vote is not passed to the other extreme, which occurs when le ing it wrap around. With this approach, the maximum accuracy of the majority function can be reached with a certain width of the counter. For a hypervector dimension D = 10, 000 the maximum width is 5 bits resulting in a memory of 50, 000 bits which is independent from the number of hypervectors to be bundled, and is maximally memory-saving for a large number of input channels. e downside is the complexity of a saturating counter. Due to the orthogonality of the hypervectors for bundling inside the spatial encoder, the saturating counter method is the preferred approach because of its large capacity and moderate complexity.\n\n\nLibrary: Spatial Encoder Modules\n\ne following library modules emerged from the optimizations in Section 4.1 and 4.2:\n\n\u2022 LUT. A purely combinational, LUT-based spatial encoder architecture. is is the starting point for optimizations and was described in [28]. See Figure 13a. A summary of the CPDI of all library modules can be found in Table 1.\n\n\nTemporal Encoding\n\nAs mentioned in Section 3.3, the temporal encoder considers consecutive samples over time. is is done by rotating and binding the record hypervectors to an N-gram hypervector:\nN -gram[t] = R[t] \u2297 \u03c1(R[t \u2212 1]) \u2297 \u03c1 2 (R[t \u2212 2]) \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u03c1 N \u22121 (R[t \u2212 (N \u2212 1)]) .\nIn order to deliver a new N -gram every cycle, the records of the last N \u2212 1 cycles have to be kept in memory. For this, the rst record is rotated and stored. In the next cycle it is again rotated and stored, while the new record is rotated and stored where the last record was stored, and so on. In parallel, the current record is bound with all stored records and a valid N -gram is produced every cycle (see Figure 14).\n\n\nBundling N -gram Hypervectors\n\nAll the modules that are described so far in this section form an HD projection along with a spatiotemporal encoder. is also constitutes a shared construct between learning and inference because the hypervectors that are produced at the output of spatiotemporal encoder (i.e., the N -gram hypervectors) contain all the information about the event of interest (e.g., a gesture) for training or classi cation. e AM is another part of the shared construct; however, the output of encoder queries the AM during classi cation while updates it during training. For training a certain class, its N -gram hypervectors need to be bundled before writing into the AM. Di erent examples of a gesture are usually encoded to similar N -gram hypervectors, since they belong to the same class. is calls for a bundling method that does not require the capacity of an accurate majority function implemented with the complex saturating counters.     1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64   1  2  3  4  5  6  7  8  9   We propose a binarized implementation of an approximate bundling operation by reusing the MAN module. It continuously stays in the binary space during the execution of the bundling operation, hence it enables e cient online and incremental updates to the prototypes of the AM. e rst step is to avoid trying to store the current majority vote and instead bundling the hypervectors iteratively, giving every vote a certain \"weight. \" is is achieved by assigning them a certain chance to be capable of turning the majority around. However, the vote is only turned around if the current one is di erent.\n\ne rst vote has a probability of P = 1, the second P = 1/2 and so on. Generally the i-th vote has a probability of P i = 1/i to be able to turn the majority around. Considering all dimensions of the hypervector, this probability turns into a weight. In an abstract sense, these probabilities can be hardwired into the architecture with a connectivity matrix. For large dimensions, the m-th row shows \u2248 D/m connections, which determine whether the vote at that position can turn around the majority. e maximum number of hypervectors in the bundling record (i.e., the rows in the connectivity matrix) should be predetermined. Figure 15 shows an example of connectivity matrix to bundle 10 hypervectors with dimensionality D = 64.\n\nWe refer to the example of bundling three hypervectors, where two are equal and one is orthogonal. When bundling with the proposed approach, the orthogonal hypervector is not lost, but is similar to the record as shown in Figure 16 (c.f. Figure 12). Furthermore, when interchanging this approximate method with the ordinary majority vote, the classi cation accuracy does not change.\n\nAs suggested, these characteristics can be implemented using the MAN module to generate a hypervector which is similar to the current bundled hypervector, where the Hamming distance (i.e., the degree of similarity) is determined by the connectivity matrix. en, the majority vote of three hypervectors is calculated from the input N -gram hypervector, the current bundled hypervector, and it's derived similar (manipulated) hypervector as depicted in Figure 17. e similar hypervector gives the input N -gram hypervector a weight of 1/i and the current bundled hypervector a weight of 1 \u2212 1/i. Compared to the bundling with saturating counters, this approach is far more e cient since it only requires a memory of D bits (fully binarized) without adders and saturation logic.\n\n\nHypervector Capacity of di erent Bundling\n\nApproaches. e proposed approximate bundling method slightly decreases the capacity of hypervectors. Although for similar hypervectors, as it is the case for N -gram hypervectors among a class (opposed to the bound hypervectors in the spatial encoder), a large capacity is not a requirement. Nevertheless, it is necessary to evaluate how much information a hypervector can store, or how many hypervectors can be bundled into a hypervector (i.e., the capacity of a bundling method). e capacity can be measured by bundling an increasing number of orthogonal hypervectors and trying to recall the information by measuring the similarity between the bundled hypervector and all compound hypervectors. As long as none of the compound hypervectors crosses the orthogonality threshold (see Section 2.1), their information is still contained in the bundled hypervector. As soon as one of the compound hypervectors becomes orthogonal to the bundled, the bundling method has failed to capture all the information.\n\nFor comparison, the ordinary majority vote (see Section 2.2) is used as the reference bundling method.\n\nis approach is referred to as the golden method. e two other approaches are the  binarized back-to-back (B2B) method from Section 4.5.1 and the bundle counter (BC) method (Section 4.2.2), which can be viewed as a very close approximation of the golden method. e capacity of the binarized back-to-back method in comparison with the golden method is depicted in Figure 18b. e golden method is capable of storing the information of about 60-70 orthogonal hypervectors for a dimensionality of D = 10000, whereas the back-to-back binary method saturates between 10-15 hypervectors.\n\nHowever, the capacity of the counter method is dependent on the number of bits (i.e., width) used to represent the current vote. e smaller the width, the fewer the resources required but the smaller its capacity. is can be seen in Figure 18a. We observe that a width of 5 bits is su cient to achieve the same capacity as the golden method. When bundling fewer hypervectors, the width should be adjusted to ones needs to minimize the required resources.  \n\n\nAssociative Memory (AM)\n\ne associative memory (AM) is the part of the architecture that is the most challenging to optimize. One reason is the memory required to store the \"trained\", prototype, or rather the bundled hypervectors that represent the classes. Another reason is the nature of the Hamming distance, that has to be computed between the query hypervector-of which we want to nd the class it belongs to-and each trained hypervectors.\n\nAs described in Section 2. those two hypervectors. So far, digital methods for AMs count through all components resulting in a classi cation latency in the order O(D) [8,9,29,32]. We focus on reducing this latency by adding up all hypervector components.\n\n\nDeep Adder Trees.\n\nWhen trying to add up all bits of a hypervector, working with tree structures is the most e cient way. In this manner, the AM takes only one clock cycle to compute the Hamming distance, at a cost to long logic delay and gate counts. For a perfect binary tree, which is the case for hypervectors of dimension D = 2 n , the depth is log 2 (D) = n which is also the number of adder stages. e amount of adders in stage i is D/2 i and the width of the adders in stage i equals to i. In the simple case of using ripple-carry-adders, the logic delay of the adder tree is equal to n i=1 i = n(n+1) 2 delays of a 1-bit-adder. For a dimension D = 2 13 = 8192, this amounts to the delay of 91 1-bit-adders, which will most likely result in the longest path in the architecture. is could be reduced with pipeline registers close to the root, i.e., the nal result. e total equivalent of 1-bit-adders for the whole tree can be calculated as follows: n i=1 D \u00b7i 2 i which, for a dimension D = 2 13 yields 16369 1-bit-adders.\n\nAlthough this number of adders seems very high, an FPGA can handle it easily with lookup tables. Furthermore, using the counters as an alternative might seem resource friendlier at rst, but turns out an incompetent choice. e reason is that each bit of the hypervector somehow has to be directed to the counter. is requires either huge multiplexers or shi registers with input multiplexers, which both leads to immense area overhead. While the overhead is considerable, the cycles needed to compute the Hamming distance is of the order O(D). is is a poor trade-o compared to the high throughput and moderate overhead of adder tree architectures.\n\nUsing the adder trees to compute the Hamming distance between two hypervectors, two AM variations emerge. A fully parallel architecture with replicated adders, leading to O(1) computation cycles, and a vector-sequential architecture, which shares one adder tree to compute the Hamming distance of all hypervectors one a er the other, hence leading to O(n classes ) computation cycles. \u2022 BS. A bit-sequential AM architecture. is is the starting point for optimizations and was described in [8,9,29,32]. See Figure 20a. \u2022 CMB. An AM architecture based on adder trees as described in Section 4.7.1. See Figure 20b.\n\n\u2022 VS. A vector-sequential AM architecture based on adder trees as described in Section 4.7.1.\n\nSee Figure 20c.\n\n\nDESIGN SPACE EXPLORATION AND EXPERIMENTAL RESULTS\n\nIn order to evaluate the library modules, they are con gured for the EMG-based hand gesture recognition task, and all possible combinations of HD architectures (i.e., our design space) are synthesized for a Xilinx \u00ae Virtex UltraScale \u2122 FPGA [41]. All the HD architectures are functionally equivalent and exhibit iso-accuracy. e parameters for the con gured architectures are listed in Table 2. e library can be con gured to conduct virtually any learning and classi cation task. Each HD architecture is composed of three modules in series: a type of mapping and spatial encoder followed by a type of temporal encoder, and nally a type of AM. To conduct the design space exploration, each architecture's throughput is plo ed against its area e ciency (de ned as 1/CLBs) in Figure 21. e quality of an architecture increases when going from le to right and/or bo om to top. e color coding represents HD architectures with the same type of AM.\n\nOur starting point is the LUT+BC+BS architecture as an improved version of [28] using bidirectional saturating counters. What can be observed is that by replacing the LUT module with the proposed MAN and CA modules, a signi cant area saving is achieved. is area saving is consistent with any combination of temporal encoder and AM. A similar area improvement can be observed when replacing the BC module with the novel B2B module. Combining both optimization leads to an area improvement of up to \u00d72.39. On the other hand, a massive throughput improvement of up to \u00d72337 can be achieved by moving from an AM with the BS module to VS and nally CMB.\n\nDi erent combinations of the modules produce architectures with varying area/throughput improvements. Eventually, four architectures stand out as pareto optimal architectures (see Table 3).\n\nese o er di erent trade-o s and can be selected depending on the user's requirements. e throughput of these architectures is signi cantly higher than the classi cation constraint for realtime EMG tasks [2,17]. Note that di erent con gurations may lead to di erent pareto optimal architectures.  \n\n\nScalability: Larger number of Channels and Classes\n\nHere, we assess the scalability of our proposed methods when doubling the number of channels and classes. e spatial encoder with the CA module shows the best area e ciency for applications with a larger number of channels, followed by the spatial encoder with the MAN module. e memory  footprint of CA module is independent of the number of channels since only a seed hypervector to initialize the CA state needs to be stored, hence the area will not increase (see Table 4a). However, it requires almost twice clock cycles to produce the channel hypervectors for the doubled number of channels. e spatial encoder with the LUT shows opposite scalability: it maintains almost the same throughput but increases the area by 2.41\u00d7. Focusing on the AM module, an application with twice the number of classes will impose a larger area to the CMB and BS modules, whereas the VS' area is mostly una ected, apart from the storage for additional trained hypervectors (see Table 4b).\n\n6 CONCLUSIONS is paper proposes hardware optimizations-in an open-source VHDL library-for dense binary HD computing that enable e cient synthesis of acceleration engines handling both inference and training tasks on an FPGA. e Pareto optimal design is mapped on only 18340 CLBs of a Xilinx \u00ae UltraScale \u2122 FPGA achieving simultaneous 2.39\u00d7 lower area and 986\u00d7 higher throughput compared to the baseline. is is accomplished by: (1) rematerializing hypervectors on the y by substituting the cheap logical operations for the expensive memory accesses to seed hypervectors; (2) online and incremental learning from di erent gesture examples while staying in the binary space; (3) combinational associative memories to steadily reduce the latency of classi cation. Our future work will target an ASIC implementation of the library modules.\n\nFig. 1 .\n1Normalized Hamming distance distribution of hypervectors in D-dimensional spaces. As the dimensionality increases, the standard deviation (1/(2 \u221a D)) of the normalized distance distribution between two random hypervectors decreases. This implies that the probabilty of two random hypervectors lying about d \u2248 0.5 apart from each other increases with the dimension D.\n\nFig. 2 .\n2Example of an HD architecture for hand gestures learning and classification from EMG biosignals.\n\n\n) The IM with four channels\n\n\n) The CIM with q = 21 levelsFig. 3. Similarity, depicted as a heatmap, between hypervectors of the IM (a) and the CIM (b).\n\nFig. 4 .Fig. 5 .\n45The hypervector manipulator (MAN) module and its symbol representation. The connectivity matrix serves as an example. The other symbols used throughout this paper can be found inFigure 5. The symbols used for schematic drawings throughout this paper.4.1.1 Rematerialization: Replacing CIM with MAN. A single CIM implemented as a lookup table requires q \u00d7 D bits of storage.\n\nFig. 6 .\n6Data dependency graph of the MAN module to replace a CIM.\n\n6 7 Fig. 7 .\n77Example of a connectivity matrix to map an input with q = 8 to a hypervector of dimension D = 64. rematerialized) over and over.\n\n\nBoth IM and CIM with MAN. e MAN module in Section 4.1.1 can also be applied to replace the IM. Instead of storing the channel hypervectors, their pa erns can be incorporated in the connections of the MAN module.\n\nFig. 8 .Fig. 9 .\n89A heat map showing the orthogonality (normalized Hamming distance) between hypervectors produced by the CA (rule 30) over 500 cycles. Each dot (x, ) on the graph shows the Hamming distance between the hypervector produced in cycle x and the one produced in cycle . As shown in the minimum and maximum values of the color scale on the right, the orthogonality condition from Section 2.1 is met. Data dependency graph of the spatial encoding architecture with the cellular automaton (CA).\n\nFig. 10 .Fig. 11 .\n1011Example of a connectivity matrix to replace the IM for a hypervector dimension D = 64 bits and 10 input channels. Data dependency graph of the bundling with even inputs using an additional feature.\n\n\n) a = b \u22a5 cFig. 12. Similarity between three hypervectors and the bundled hypervector a \u2295 b \u2295 c using majority vote.\n\n\n\u2022 CA. A sequential spatial encoder architecture, where the IM is reproduced by a cellular automaton (CA) as described in Section 4.1.2. e bound hypervectors are bundled by a block of bidirectional saturating counters as described in Section 4.2.2. SeeFigure 13b.\u2022 MAN. A sequential spatial encoder architecture, where the IM is \"hardwired\" in a manipulator's connectivity matrix as described in 4.1.3. e same bundling method as in the CA module is used. SeeFigure 13c.\n\n\ntable (LUT): Parallel encoder with LUT-based CIMs and no IMs. automaton (CA): Sequential encoder with CA (replacing IM), and MAN module (replacing CIM).\n\n\n) MAN: Sequential encoder replacing both CIM and IM with two cascaded MAN modules.\n\nFig. 13 .\n13The spatial encoder architectures available in the library.\n\nFig. 14 .\n14Top: Data dependency graph of the window-shi ing N -gram encoder. Bo om: Depiction of the timeline of generated N -grams.\n\n\n10 \n\nFig. 15 .\n15Example of a connectivity matrix to bundle 10 hypervectors of dimension D = 64 iteratively. 4.5.1 Binarized Back-to-back Bundling as a Hardware-Friendly Approach for Approximate Bundling.\n\nFig. 16 .Fig. 17 .\n1617Similarity between three hypervectors and the bundled hypervector a \u2295 b \u2295 c using binarized back-to-back bundling. Data dependency graphs of the binarized back-to-back bundling to approximate temporal majority gate.\n\n\nCapacity of back-to-back (B2B) bundling.\n\nFig. 18 .\n18Capacity of di erent bundling approaches compared with the golden method for a dimensionality of D = 10000. The graphs show the maximum normalized Hamming distance between the bundled hypervector and its compound hypervectors.\n\n4. 6\n6Library: Temporal Encoder Modules e following library modules emerged from the optimizations in Section 4.4 and 4.5: \u2022 BC. A temporal encoder architecture using counter-based bundling as described in Section 4.4 and 4.2.2. See Figure 19a. \u2022 B2B. A temporal encoder architecture using manipulator-based back-to-back binary bundling as described in Section 4.4 and 4.5.1. See Figure 19b\n\n\n.\n\n\nCombinational (CMB): This single-cycle AM has a latency of O(1). Vector-sequential (VS): This AM has a latency of O(n classes ).\n\nFig. 20 .\n20The associative memory architectures available in the library.\n\nFig. 21 .\n21Design space exploration of HD architectures using all possible combinations of the modules available in the library. Colors indicate the architectures with the same type of AM. Pareto optimal architectures are marked with a diamond and connected by a green line representing the Pareto frontier.\n\n\n1, the Hamming distance measures the number of positions at which two hypervectors di er. is is equal to computing the population count of a hypervector binding (a) Bundle counter (BC): Using saturating bidirectional counters to bundle N -gram hypervectors. Binarized Back-to-back (B2B): Using the MAN module to approximate bundling of N -gram hypervectors.Fig. 19. The temporal encoder architectures available in the library.rol \n\nInput (Record) \nHypervector \n\nrol \n\nOutput (Trained / Query) \nHypervector \n\nBlock of \nSaturating \nBidirectional \nCounters \n\nrol \nrol \n\n1-Hot \nShift Register \n\nInput (Record) \nHypervector \n\nOutput (Trained / Query) \nHypervector \n\n(b) \n\nTable 2 .\n2Parameter configuration for the case study. Library: Associative Memory Modules e following library modules emerged from the optimizations in Section 4.7:Parameter \nValue \nHypervector Dimension (D) \n8192 \nChannels \n4 \nClasses \n5 \nantization \n21 \nN -gram Size \n3 \nBundle Counter Width (MAN & CA) \n3 \nBundle Counter Width (BC) \n5 \nMax. Bundle Cycles (B2B) \n256 \n\n4.8 \n\nTable 3 .\n3Area and throughput results of the \"starting point\" and the Pareto optimal architectures.Architecture \nroughput \n[classi cations/s] \n\nroughput \nImprovement \nArea [CLBs] \nArea \nImprovement \nLUT+BC+BS \n4.69 \u00b7 10 3 \n\u00d71 \n43825 \n\u00d71 \nMAN+B2B+VS \n4.62 \u00b7 10 6 \n\u00d7986 \n18340 \n\u00d72.39 \nCA+B2B+CMB \n5.33 \u00b7 10 6 \n\u00d71136 \n28788 \n\u00d71.52 \nLUT+B2B+CMB \n8.94 \u00b7 10 6 \n\u00d71906 \n45961 \n\u00d70.95 \nLUT+BC+CMB \n10.96 \u00b7 10 6 \n\u00d72337 \n47068 \n\u00d70.93 \n\n\n\nTable 4 .\n4Scalability of the library modules.(a) Throughput and area scaling of the spatial encoder modules when doubling the number of channels from 4 to 8.(b) Throughput and area scaling of the AM modules when doubling the number of classes from 6 to 12.Module \nroughput \nScaling \n\nArea \nScaling \nLUT \n\u00d70.94 \n\u00d72.41 \nCA \n\u00d70.45 \n\u00d70.99 \nMAN \n\u00d70.61 \n\u00d71.01 \n\nModule \nroughput \nScaling \n\nArea \nScaling \nBS \n\u00d70.49 \n\u00d71.89 \nCMB \n\u00d70.63 \n\u00d72.14 \nVS \n\u00d70.59 \n\u00d71.10 \n\n\nACKNOWLEDGMENTS Support was received from the ETH Zurich Postdoctoral Fellowship program, the Marie Curie Actions for People COFUND Program, and the European Union's Horizon 2020 Research and Innovation Program through the project MNEMOSENE under Grant 780215.\nOne-shot learning for iEEG seizure detection using end-to-end binary operations: Local binary pa erns with hyperdimensional computing. Alessio Burrello, Kaspar Schindler, Luca Benini, Abbas Rahimi, Biomedical Circuits and Systems Conference. IEEEAlessio Burrello, Kaspar Schindler, Luca Benini, and Abbas Rahimi. 2018. One-shot learning for iEEG seizure detection using end-to-end binary operations: Local binary pa erns with hyperdimensional computing. In Biomedical Circuits and Systems Conference (BioCAS), 2018 IEEE.\n\nA Real-Time EMG Pa ern Recognition System Based on Linear-Nonlinear Feature Projection for a Multifunction Myoelectric Hand. J U Chu, I Moon, M S Mun, 10.1109/TBME.2006.883695IEEE Transactions on Biomedical Engineering. 5311J. U. Chu, I. Moon, and M. S. Mun. 2006. A Real-Time EMG Pa ern Recognition System Based on Linear-Nonlinear Feature Projection for a Multifunction Myoelectric Hand. IEEE Transactions on Biomedical Engineering 53, 11 (Nov 2006), 2232-2239. h ps://doi.org/10.1109/TBME.2006.883695\n\nHow to Build a Brain: A Neural Architecture for Biological Cognition. Chris Eliasmith , Oxford Series on Cognitive Models and Architectures. Chris Eliasmith. 2013. How to Build a Brain: A Neural Architecture for Biological Cognition. Oxford Series on Cognitive Models and Architectures.\n\nAnalogical mapping and inference with binary spa er codes and sparse distributed memory. B Emruli, R W Gayler, F Sandin, 10.1109/IJCNN.2013.67068292013 International Joint Conference on Neural Networks (IJCNN). 1-8. B. Emruli, R. W. Gayler, and F. Sandin. 2013. Analogical mapping and inference with binary spa er codes and sparse distributed memory. In e 2013 International Joint Conference on Neural Networks (IJCNN). 1-8. h ps: //doi.org/10.1109/IJCNN.2013.6706829\n\nMultiplicative Binding, Representation Operators & Analogy. W Ross, Gayler, Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences. Gentner, D., Holyoak, K. J., Kokinov, B. N.New Bulgarian UniversityRoss W. Gayler. 1998. Multiplicative Binding, Representation Operators & Analogy. In Gentner, D., Holyoak, K. J., Kokinov, B. N. (Eds.), Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences. New Bulgarian University, So a, Bulgaria, 1-4. h p://cogprints.org/502/\n\nVector Symbolic Architectures Answer Jackendo 's Challenges for Cognitive Neuroscience. Ross W Gayler, Proceedings of the Joint International Conference on Cognitive Science. ICCS/ASCS. the Joint International Conference on Cognitive Science. ICCS/ASCSRoss W. Gayler. 2003. Vector Symbolic Architectures Answer Jackendo 's Challenges for Cognitive Neuroscience. In Proceedings of the Joint International Conference on Cognitive Science. ICCS/ASCS. 133-138.\n\nA Bio-Inspired Analog Gas Sensing Front End. P C Huang, J M Rabaey, 10.1109/TCSI.2017.2697945IEEE Transactions on Circuits and Systems I: Regular Papers. 649P. C. Huang and J. M. Rabaey. 2017. A Bio-Inspired Analog Gas Sensing Front End. IEEE Transactions on Circuits and Systems I: Regular Papers 64, 9 (Sept 2017), 2611-2623. h ps://doi.org/10.1109/TCSI.2017.2697945\n\nVoiceHD: Hyperdimensional Computing for E cient Speech Recognition. M Imani, D Kong, A Rahimi, T Rosing, 10.1109/ICRC.2017.81236502017 IEEE International Conference on Rebooting Computing (ICRC). 1-8. M. Imani, D. Kong, A. Rahimi, and T. Rosing. 2017. VoiceHD: Hyperdimensional Computing for E cient Speech Recognition. In 2017 IEEE International Conference on Rebooting Computing (ICRC). 1-8. h ps://doi.org/10.1109/ICRC. 2017.8123650\n\nExploring Hyperdimensional Associative Memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, 10.1109/HPCA.2017.28doi.org/10. 1109/HPCA.2017.282017 IEEE International Symposium on High Performance Computer Architecture (HPCA). M. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey. 2017. Exploring Hyperdimensional Associative Memory. In 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA). 445-456. h ps://doi.org/10. 1109/HPCA.2017.28\n\nLanguage Geometry Using Random Indexing. Aditya Joshi, Johan T Halseth, Pen Kanerva, 10.1007/978-3-319-52289-0_21antum Interaction: 10th International Conference, QI 2016. Revised Selected Papers, Jose Acacio de Barros, Bob Coecke, and Emmanuel PothosSan Francisco, CA, USA; ChamSpringer International PublishingAditya Joshi, Johan T. Halseth, and Pen i Kanerva. 2017. Language Geometry Using Random Indexing. In antum Interaction: 10th International Conference, QI 2016, San Francisco, CA, USA, July 20-22, 2016, Revised Selected Papers, Jose Acacio de Barros, Bob Coecke, and Emmanuel Pothos (Eds.). Springer International Publishing, Cham, 265-274. h ps://doi.org/10.1007/978-3-319-52289-0 21\n\nPen I Kanerva, Sparse Distributed Memory. MIT Press CambridgePen i Kanerva. 1988. Sparse Distributed Memory. MIT Press Cambridge.\n\nBinary Spa er-Coding of ordered k -tuples. Pen I Kanerva, ICANN'96, Proceedings of the International Conference on Arti cial Neural Networks (Lecture Notes in Computer Science). Springer1112Pen i Kanerva. 1996. Binary Spa er-Coding of ordered k -tuples. In ICANN'96, Proceedings of the International Conference on Arti cial Neural Networks (Lecture Notes in Computer Science), (Ed.), Vol. 1112. Springer, 869-873.\n\nHyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors. Pen I Kanerva, 10.1007/s12559-009-9009-8doi.org/10.1007/ s12559-009-9009-8Cognitive Computation. 1Pen i Kanerva. 2009. Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors. Cognitive Computation 1, 2 (2009), 139-159. h ps://doi.org/10.1007/ s12559-009-9009-8\n\nWhat's the Dollar of Mexico?\": Prototypes and Mapping in Concept Space. Pen I Kanerva, AAAI Fall Symposium: antum Informatics for Cognitive, Social, and Semantic Processes. What We Mean When We SayPen i Kanerva. 2010. What We Mean When We Say \"What's the Dollar of Mexico?\": Prototypes and Mapping in Concept Space. In AAAI Fall Symposium: antum Informatics for Cognitive, Social, and Semantic Processes. 2-6.\n\nComputing with 10,000-Bit Words. Pen I Kanerva, Proc. 52nd Annual Allerton Conference on Communication, Control, and Computing. 52nd Annual Allerton Conference on Communication, Control, and ComputingPen i Kanerva. 2014. Computing with 10,000-Bit Words. In Proc. 52nd Annual Allerton Conference on Communication, Control, and Computing.\n\nRandom Indexing of Text Samples for Latent Semantic Analysis. Jan Pen I Kanerva, Anders Kristoferson, Holst, Proceedings of the 22nd Annual Conference of the Cognitive Science Society. Erlbaum, 1036. the 22nd Annual Conference of the Cognitive Science Society. Erlbaum, 1036Pen i Kanerva, Jan Kristoferson, and Anders Holst. 2000. Random Indexing of Text Samples for Latent Semantic Analysis. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society. Erlbaum, 1036. h p://www.rni. org/kanerva/cogsci2k-poster.txt\n\nReal-time intelligent pa ern recognition algorithm for surface EMG signals. Mahdi Khezri, Mehran Jahed, 10.1186/1475-925X-6-45BioMedical Engineering OnLine. 6Mahdi Khezri and Mehran Jahed. 2007. Real-time intelligent pa ern recognition algorithm for surface EMG signals. BioMedical Engineering OnLine 6, 1 (03 Dec 2007), 45. h ps://doi.org/10.1186/1475-925X-6-45\n\nClassi cation and Recall With Binary Hyperdimensional Computing: Tradeo s in Choice of Density and Mapping Characteristics. D Kleyko, A Rahimi, D A Rachkovskij, E Osipov, J M Rabaey, 10.1109/TNNLS.2018.2814400IEEE Transactions on Neural Networks and Learning Systems. D. Kleyko, A. Rahimi, D. A. Rachkovskij, E. Osipov, and J. M. Rabaey. 2018. Classi cation and Recall With Binary Hyperdimensional Computing: Tradeo s in Choice of Density and Mapping Characteristics. IEEE Transactions on Neural Networks and Learning Systems (2018), 1-19. h ps://doi.org/10.1109/TNNLS.2018.2814400\n\nLearning Behavior Hierarchies via High-dimensional Sensor Projection. Simon D Levy, Suraj Bajracharya, Ross W Gayler, Proceedings of the 12th AAAI Conference on Learning Rich Representations from Low-Level Sensors (AAAIWS'13-12). the 12th AAAI Conference on Learning Rich Representations from Low-Level Sensors (AAAIWS'13-12)AAAI PressSimon D. Levy, Suraj Bajracharya, and Ross W. Gayler. 2013. Learning Behavior Hierarchies via High-dimensional Sensor Projection. In Proceedings of the 12th AAAI Conference on Learning Rich Representations from Low-Level Sensors (AAAIWS'13-12). AAAI Press, 25-27. h p://dl.acm.org/citation.cfm?id=2908225.2908230\n\nHyperdimensional computing with 3D VRRAM in-memory kernels: Device-architecture co-design for energy-e cient, error-resilient language recognition. H Li, T F Wu, A Rahimi, K S Li, M Rusch, C H Lin, J L Hsu, M M Sabry, S B Eryilmaz, J Sohn, W C Chiu, M C Chen, T T Wu, J M Shieh, W K Yeh, J M Rabaey, S Mitra, H S P Wong, 10.1109/IEDM.2016.78384282016 IEEE International Electron Devices Meeting. IEDM). 16.1.1-16.1.4. hH. Li, T. F. Wu, A. Rahimi, K. S. Li, M. Rusch, C. H. Lin, J. L. Hsu, M. M. Sabry, S. B. Eryilmaz, J. Sohn, W. C. Chiu, M. C. Chen, T. T. Wu, J. M. Shieh, W. K. Yeh, J. M. Rabaey, S. Mitra, and H. S. P. Wong. 2016. Hyperdimensional computing with 3D VRRAM in-memory kernels: Device-architecture co-design for energy-e cient, error-resilient language recognition. In 2016 IEEE International Electron Devices Meeting (IEDM). 16.1.1-16.1.4. h ps://doi.org/10.1109/IEDM.2016.7838428\n\nAn EMG Gesture Recognition System with Flexible High-Density Sensors and Brain-Inspired High-Dimensional Classi er. A Moin, A Zhou, A Rahimi, S Bena I, A Menon, S Tamakloe, J Ting, N Yamamoto, Y Khan, F Burghardt, L Benini, A C Arias, J M Rabaey, 10.1109/ISCAS.2018.83516132018 IEEE International Symposium on Circuits and Systems (ISCAS). 1-5. A. Moin, A. Zhou, A. Rahimi, S. Bena i, A. Menon, S. Tamakloe, J. Ting, N. Yamamoto, Y. Khan, F. Burghardt, L. Benini, A. C. Arias, and J. M. Rabaey. 2018. An EMG Gesture Recognition System with Flexible High-Density Sensors and Brain-Inspired High-Dimensional Classi er. In 2018 IEEE International Symposium on Circuits and Systems (ISCAS). 1-5. h ps://doi.org/10.1109/ISCAS.2018.8351613\n\nPULP-HD: Accelerating Brain-inspired High-dimensional Computing on a Parallel Ultra-low Power Platform. Fabio Montagna, Abbas Rahimi, Simone Bena I, Davide Rossi, Luca Benini, 10.1145/3195970.3196096doi.org/10.1145/ 3195970.3196096Proceedings of the 55th Annual Design Automation Conference (DAC '18). the 55th Annual Design Automation Conference (DAC '18)New York, NY, USA, ArticleACM111Fabio Montagna, Abbas Rahimi, Simone Bena i, Davide Rossi, and Luca Benini. 2018. PULP-HD: Accelerating Brain-inspired High-dimensional Computing on a Parallel Ultra-low Power Platform. In Proceedings of the 55th Annual Design Automation Conference (DAC '18). ACM, New York, NY, USA, Article 111, 6 pages. h ps://doi.org/10.1145/ 3195970.3196096\n\nHyperdimensional Computing for Text Classi cation. Design, Automation Test in Europe Conference Exhibition (DATE). Abbas Fateme Rasti Najafabadi, Rahimi, Jan M Pen I Kanerva, Rabaey, University BoothFateme Rasti Najafabadi, Abbas Rahimi, Pen i Kanerva, and Jan M. Rabaey. 2016. Hyperdimensional Computing for Text Classi cation. Design, Automation Test in Europe Conference Exhibition (DATE), University Booth (2016). h ps://www.date-conference.com/system/ les/ le/date16/ubooth/37923.pdf\n\nLearning Vector Symbolic Architectures for Reactive Robot Behaviours. P Neubert, S Schubert, P Protzel, Proc. of Intl. Conf. on Intelligent Robots and Systems (IROS) Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in Robotics. of Intl. Conf. on Intelligent Robots and Systems (IROS) Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in RoboticsP. Neubert, S. Schubert, and P. Protzel. 2016. Learning Vector Symbolic Architectures for Reactive Robot Behaviours. In Proc. of Intl. Conf. on Intelligent Robots and Systems (IROS) Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in Robotics.\n\nHolographic reduced representations. T A Plate, IEEE Transactions on Neural Networks. 6T.A. Plate. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks 6, 3 (1995), 623-641.\n\nHolographic Reduced Representations. T A Plate, CLSI Publications300T.A. Plate. 2003. Holographic Reduced Representations. CLSI Publications. 300 pages.\n\nBinary Vectors for Fast Distance and Similarity Estimation. D A Rachkovskij, 10.1007/s10559-017-9914-xCybernetics and Systems Analysis. 53D. A. Rachkovskij. 2017. Binary Vectors for Fast Distance and Similarity Estimation. Cybernetics and Systems Analysis 53, 1 (01 Jan 2017), 138-156. h ps://doi.org/10.1007/s10559-017-9914-x\n\nHyperdimensional Biosignal Processing: A Case Study for EMG-based Hand Gesture Recognition. Abbas Rahimi, Simone Bena I, Pen I Kanerva, Luca Benini, Jan M Rabaey, IEEE International Conference on Rebooting Computing. Abbas Rahimi, Simone Bena i, Pen i Kanerva, Luca Benini, and Jan M. Rabaey. 2016. Hyperdimensional Biosignal Processing: A Case Study for EMG-based Hand Gesture Recognition. In IEEE International Conference on Rebooting Computing.\n\nHigh-Dimensional Computing as a Nanoscalable Paradigm. A Rahimi, S Da A, D Kleyko, E P Frady, B Olshausen, P Kanerva, J M Rabaey, 10.1109/TCSI.2017.2705051IEEE Transactions on Circuits and Systems I: Regular Papers. 649A. Rahimi, S. Da a, D. Kleyko, E. P. Frady, B. Olshausen, P. Kanerva, and J. M. Rabaey. 2017. High-Dimensional Computing as a Nanoscalable Paradigm. IEEE Transactions on Circuits and Systems I: Regular Papers 64, 9 (Sept 2017), 2508-2521. h ps://doi.org/10.1109/TCSI.2017.2705051\n\nE cient Biosignal Processing Using Hyperdimensional Computing: Network Templates for Combined Learning and Classi cation of ExG Signals. A Rahimi, P Kanerva, L Benini, J M Rabaey, 10.1109/JPROC.2018.2871163Proc. IEEE. A. Rahimi, P. Kanerva, L. Benini, and J. M. Rabaey. 2018. E cient Biosignal Processing Using Hyperdimensional Computing: Network Templates for Combined Learning and Classi cation of ExG Signals. Proc. IEEE (2018), 1-21. h ps://doi.org/10.1109/JPROC.2018.2871163\n\nHyperdimensional Computing for Noninvasive Brain-Computer Interfaces: Blind and One-Shot Classi cation of EEG Error-Related Potentials. Abbas Rahimi, Jos\u00e9 Pen I Kanerva, Jan M Del R Mill\u00e1n, Rabaey, 10Abbas Rahimi, Pen i Kanerva, Jos\u00e9 del R Mill\u00e1n, and Jan M. Rabaey. 2017. Hyperdimensional Computing for Noninvasive Brain-Computer Interfaces: Blind and One-Shot Classi cation of EEG Error-Related Potentials. 10th\n\nACM/EAI International Conference on Bio-inspired Information and Communications Technologies (BICT. ACM/EAI International Conference on Bio-inspired Information and Communications Technologies (BICT) (2017).\n\nA Robust and Energy E cient Classi er Using Brain-Inspired Hyperdimensional Computing. Abbas Rahimi, Jan M Pen I Kanerva, Rabaey, Low Power Electronics and Design. Abbas Rahimi, Pen i Kanerva, and Jan M. Rabaey. 2016. A Robust and Energy E cient Classi er Using Brain-Inspired Hyperdimensional Computing. In Low Power Electronics and Design (ISLPED), 2016 IEEE/ACM International Symposium on.\n\nHyperdimensional Computing for Blind and One-Shot Classi cation of EEG Error-Related Potentials. Mobile Networks and Applications. Abbas Rahimi, Artiom Tchouprina, Jos\u00e9 Pen I Kanerva, Jan M Del R. Mill\u00e1n, Rabaey, 10.1007/s11036-017-0942-6Abbas Rahimi, Artiom Tchouprina, Pen i Kanerva, Jos\u00e9 del R. Mill\u00e1n, and Jan M. Rabaey. 2017. Hyperdimensional Computing for Blind and One-Shot Classi cation of EEG Error-Related Potentials. Mobile Networks and Applications (03 Oct 2017). h ps://doi.org/10.1007/s11036-017-0942-6\n\nGenerating Hyperdimensional Distributed Representations from Continuous Valued Multivariate Sensory Input. O R\u00e4s\u00e4nen, Proceedings of the 37th Annual Meeting of the Cognitive Science Society. the 37th Annual Meeting of the Cognitive Science SocietyO. R\u00e4s\u00e4nen. 2015. Generating Hyperdimensional Distributed Representations from Continuous Valued Multivariate Sensory Input. In Proceedings of the 37th Annual Meeting of the Cognitive Science Society. 1943-1948.\n\nModeling Dependencies in Multiple Parallel Data Streams with Hyperdimensional Computing. O R\u00e4s\u00e4nen, S Kakouros, 10.1109/LSP.2014.2320573IEEE Signal Processing Le ers. 217O. R\u00e4s\u00e4nen and S. Kakouros. 2014. Modeling Dependencies in Multiple Parallel Data Streams with Hyperdimensional Computing. IEEE Signal Processing Le ers 21, 7 (July 2014), 899-903. h ps://doi.org/10.1109/LSP.2014.2320573\n\nSequence Prediction With Sparse Distributed Hyperdimensional Coding Applied to the Analysis of Mobile Phone Use Pa erns. O R\u00e4s\u00e4nen, J Saarinen, 10.1109/TNNLS.2015.2462721IEEE Transactions on Neural Networks and Learning Systems PP. O. R\u00e4s\u00e4nen and J. Saarinen. 2015. Sequence Prediction With Sparse Distributed Hyperdimensional Coding Applied to the Analysis of Mobile Phone Use Pa erns. IEEE Transactions on Neural Networks and Learning Systems PP, 99 (2015), 1-12. h ps://doi.org/10.1109/TNNLS.2015.2462721\n\nSearch for Optimal Five-Neighbor FPGA-Based Cellular Automata Random Number Generators. R Santoro, S Roy, O Sentieys, 10.1109/ISSSE.2007.4294483doi.org/10. 1109/ISSSE.2007.42944832007 International Symposium on Signals, Systems and Electronics. R. Santoro, S. Roy, and O. Sentieys. 2007. Search for Optimal Five-Neighbor FPGA-Based Cellular Automata Random Number Generators. In 2007 International Symposium on Signals, Systems and Electronics. 343-346. h ps://doi.org/10. 1109/ISSSE.2007.4294483\n\nRandom sequence generation by cellular automata. Stephen Wolfram, 10.1016/0196-8858(86)90028-XAdvances in Applied Mathematics. 78690028Stephen Wolfram. 1986. Random sequence generation by cellular automata. Advances in Applied Mathematics 7, 2 (1986), 123 -169. h ps://doi.org/10.1016/0196-8858(86)90028-X\n\nHyperdimensional Computing Exploiting Carbon Nanotube FETs, Resistive RAM, and eir Monolithic 3D Integration. T F Wu, H Li, P Huang, A Rahimi, G Hills, B Hodson, W Hwang, J M Rabaey, H P Wong, M M Shulaker, S Mitra, 10.1109/JSSC.2018.2870560IEEE Journal of Solid-State Circuits. 53T. F. Wu, H. Li, P. Huang, A. Rahimi, G. Hills, B. Hodson, W. Hwang, J. M. Rabaey, H. . P. Wong, M. M. Shulaker, and S. Mitra. 2018. Hyperdimensional Computing Exploiting Carbon Nanotube FETs, Resistive RAM, and eir Monolithic 3D Integration. IEEE Journal of Solid-State Circuits 53, 11 (Nov 2018), 3183-3196. h ps://doi.org/10.1109/JSSC.2018.2870560\n\nBrain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study. T F Wu, H Li, P C Huang, A Rahimi, J M Rabaey, H S P Wong, M M Shulaker, S Mitra, 10.1109/ISSCC.2018.83103992018 IEEE International Solid -State Circuits Conference -(ISSCC). T. F. Wu, H. Li, P. C. Huang, A. Rahimi, J. M. Rabaey, H. S. P. Wong, M. M. Shulaker, and S. Mitra. 2018. Brain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study. In 2018 IEEE International Solid -State Circuits Conference -(ISSCC). 492-494. h ps://doi.org/10.1109/ISSCC.2018.8310399\n\nUltraScale Architecture and Product Data Sheet: Overview. h ps://www.xilinx.com/support/ documentation/data sheets/ds890-ultrascale-overview. Xilinx, pdf DS890 (v3.1Xilinx. 2017. UltraScale Architecture and Product Data Sheet: Overview. h ps://www.xilinx.com/support/ documentation/data sheets/ds890-ultrascale-overview.pdf DS890 (v3.1) November 15, 2017.\n", "annotations": {"author": "[{\"end\":248,\"start\":162},{\"end\":331,\"start\":249}]", "publisher": null, "author_last_name": "[{\"end\":176,\"start\":169},{\"end\":259,\"start\":253}]", "author_first_name": "[{\"end\":168,\"start\":162},{\"end\":252,\"start\":249}]", "author_affiliation": "[{\"end\":247,\"start\":178},{\"end\":330,\"start\":261}]", "title": "[{\"end\":159,\"start\":1},{\"end\":490,\"start\":332}]", "venue": null, "abstract": "[{\"end\":3335,\"start\":920}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3387,\"start\":3383},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3390,\"start\":3387},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3930,\"start\":3926},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4181,\"start\":4177},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4228,\"start\":4224},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4259,\"start\":4255},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4286,\"start\":4282},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4289,\"start\":4286},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4314,\"start\":4310},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4338,\"start\":4335},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4341,\"start\":4338},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4376,\"start\":4372},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4379,\"start\":4376},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4417,\"start\":4413},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4420,\"start\":4417},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4496,\"start\":4492},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4499,\"start\":4496},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4502,\"start\":4499},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4505,\"start\":4502},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4540,\"start\":4536},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4543,\"start\":4540},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4576,\"start\":4573},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4601,\"start\":4597},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4927,\"start\":4923},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5326,\"start\":5322},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5913,\"start\":5909},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6009,\"start\":6005},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6198,\"start\":6195},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6201,\"start\":6198},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6467,\"start\":6463},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6470,\"start\":6467},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6473,\"start\":6470},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9419,\"start\":9415},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10604,\"start\":10600},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10710,\"start\":10707},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10737,\"start\":10733},{\"end\":10822,\"start\":10786},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10898,\"start\":10894},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10901,\"start\":10898},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10926,\"start\":10922},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10965,\"start\":10962},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10987,\"start\":10983},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11041,\"start\":11038},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11103,\"start\":11100},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11499,\"start\":11495},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12388,\"start\":12384},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12737,\"start\":12733},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13129,\"start\":13125},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13791,\"start\":13787},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17148,\"start\":17145},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17151,\"start\":17148},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17276,\"start\":17272},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17279,\"start\":17276},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17789,\"start\":17786},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17799,\"start\":17795},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17802,\"start\":17799},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17812,\"start\":17808},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17824,\"start\":17821},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17827,\"start\":17824},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17838,\"start\":17835},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17881,\"start\":17877},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18364,\"start\":18360},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20159,\"start\":20155},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20356,\"start\":20352},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24498,\"start\":24494},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26224,\"start\":26220},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30680,\"start\":30676},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32874,\"start\":32873},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32996,\"start\":32992},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34066,\"start\":34062},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38705,\"start\":38701},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":46027,\"start\":46024},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":46029,\"start\":46027},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":46032,\"start\":46029},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":46035,\"start\":46032},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":48282,\"start\":48279},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":48284,\"start\":48282},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":48287,\"start\":48284},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":48290,\"start\":48287},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":48812,\"start\":48808},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":49587,\"start\":49583},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":50553,\"start\":50550},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":50556,\"start\":50553},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":55739,\"start\":55737}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52882,\"start\":52505},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52990,\"start\":52883},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53020,\"start\":52991},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53145,\"start\":53021},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53539,\"start\":53146},{\"attributes\":{\"id\":\"fig_5\"},\"end\":53608,\"start\":53540},{\"attributes\":{\"id\":\"fig_6\"},\"end\":53753,\"start\":53609},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53967,\"start\":53754},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54474,\"start\":53968},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54696,\"start\":54475},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54815,\"start\":54697},{\"attributes\":{\"id\":\"fig_12\"},\"end\":55286,\"start\":54816},{\"attributes\":{\"id\":\"fig_13\"},\"end\":55441,\"start\":55287},{\"attributes\":{\"id\":\"fig_14\"},\"end\":55526,\"start\":55442},{\"attributes\":{\"id\":\"fig_15\"},\"end\":55599,\"start\":55527},{\"attributes\":{\"id\":\"fig_16\"},\"end\":55734,\"start\":55600},{\"attributes\":{\"id\":\"fig_17\"},\"end\":55740,\"start\":55735},{\"attributes\":{\"id\":\"fig_18\"},\"end\":55941,\"start\":55741},{\"attributes\":{\"id\":\"fig_19\"},\"end\":56181,\"start\":55942},{\"attributes\":{\"id\":\"fig_20\"},\"end\":56224,\"start\":56182},{\"attributes\":{\"id\":\"fig_21\"},\"end\":56464,\"start\":56225},{\"attributes\":{\"id\":\"fig_22\"},\"end\":56856,\"start\":56465},{\"attributes\":{\"id\":\"fig_23\"},\"end\":56860,\"start\":56857},{\"attributes\":{\"id\":\"fig_24\"},\"end\":56991,\"start\":56861},{\"attributes\":{\"id\":\"fig_25\"},\"end\":57067,\"start\":56992},{\"attributes\":{\"id\":\"fig_26\"},\"end\":57377,\"start\":57068},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":58045,\"start\":57378},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":58423,\"start\":58046},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":58850,\"start\":58424},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":59308,\"start\":58851}]", "paragraph": "[{\"end\":4602,\"start\":3351},{\"end\":5377,\"start\":4604},{\"end\":6474,\"start\":5379},{\"end\":6840,\"start\":6476},{\"end\":7387,\"start\":6842},{\"end\":8330,\"start\":7389},{\"end\":8629,\"start\":8332},{\"end\":9134,\"start\":8631},{\"end\":10130,\"start\":9149},{\"end\":11500,\"start\":10132},{\"end\":12389,\"start\":11526},{\"end\":12908,\"start\":12391},{\"end\":13362,\"start\":12927},{\"end\":14072,\"start\":13391},{\"end\":14909,\"start\":14074},{\"end\":15057,\"start\":14911},{\"end\":15450,\"start\":15059},{\"end\":16451,\"start\":15452},{\"end\":16798,\"start\":16485},{\"end\":17865,\"start\":16859},{\"end\":18406,\"start\":17867},{\"end\":19234,\"start\":18426},{\"end\":21213,\"start\":19266},{\"end\":21532,\"start\":21301},{\"end\":21635,\"start\":21559},{\"end\":21854,\"start\":21701},{\"end\":22145,\"start\":21875},{\"end\":22540,\"start\":22147},{\"end\":22713,\"start\":22577},{\"end\":23193,\"start\":22767},{\"end\":23359,\"start\":23259},{\"end\":24057,\"start\":23361},{\"end\":24992,\"start\":24113},{\"end\":25726,\"start\":24994},{\"end\":26436,\"start\":25766},{\"end\":26466,\"start\":26438},{\"end\":26684,\"start\":26583},{\"end\":27323,\"start\":26686},{\"end\":27773,\"start\":27372},{\"end\":28488,\"start\":27775},{\"end\":28869,\"start\":28490},{\"end\":29215,\"start\":28871},{\"end\":29947,\"start\":29217},{\"end\":30551,\"start\":29957},{\"end\":31722,\"start\":30553},{\"end\":33101,\"start\":31724},{\"end\":33773,\"start\":33113},{\"end\":35058,\"start\":34000},{\"end\":35965,\"start\":35060},{\"end\":36174,\"start\":35967},{\"end\":36733,\"start\":36176},{\"end\":38445,\"start\":36826},{\"end\":38564,\"start\":38482},{\"end\":38792,\"start\":38566},{\"end\":38989,\"start\":38814},{\"end\":39497,\"start\":39075},{\"end\":41337,\"start\":39531},{\"end\":42065,\"start\":41339},{\"end\":42449,\"start\":42067},{\"end\":43224,\"start\":42451},{\"end\":44272,\"start\":43270},{\"end\":44376,\"start\":44274},{\"end\":44954,\"start\":44378},{\"end\":45410,\"start\":44956},{\"end\":45855,\"start\":45438},{\"end\":46111,\"start\":45857},{\"end\":47142,\"start\":46133},{\"end\":47788,\"start\":47144},{\"end\":48401,\"start\":47790},{\"end\":48496,\"start\":48403},{\"end\":48513,\"start\":48498},{\"end\":49506,\"start\":48567},{\"end\":50155,\"start\":49508},{\"end\":50346,\"start\":50157},{\"end\":50643,\"start\":50348},{\"end\":51669,\"start\":50698},{\"end\":52504,\"start\":51671}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12926,\"start\":12909},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16484,\"start\":16452},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21241,\"start\":21214},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21300,\"start\":21241},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21558,\"start\":21533},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21700,\"start\":21636},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22576,\"start\":22541},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23258,\"start\":23194},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26582,\"start\":26481},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27371,\"start\":27338},{\"attributes\":{\"id\":\"formula_10\"},\"end\":39074,\"start\":38990}]", "table_ref": "[{\"end\":25327,\"start\":25320},{\"end\":25685,\"start\":25678},{\"end\":26367,\"start\":26360},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31283,\"start\":31022},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35590,\"start\":35343},{\"end\":38791,\"start\":38784},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":40735,\"start\":40462},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48959,\"start\":48952},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":50344,\"start\":50337},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":51171,\"start\":51163},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":51667,\"start\":51659}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3349,\"start\":3337},{\"attributes\":{\"n\":\"2\"},\"end\":9147,\"start\":9137},{\"attributes\":{\"n\":\"2.1\"},\"end\":11524,\"start\":11503},{\"attributes\":{\"n\":\"2.2\"},\"end\":13389,\"start\":13365},{\"attributes\":{\"n\":\"3\"},\"end\":16857,\"start\":16801},{\"attributes\":{\"n\":\"3.1\"},\"end\":18424,\"start\":18409},{\"attributes\":{\"n\":\"3.2\"},\"end\":19264,\"start\":19237},{\"attributes\":{\"n\":\"3.3\"},\"end\":21873,\"start\":21857},{\"attributes\":{\"n\":\"3.4\"},\"end\":22765,\"start\":22716},{\"attributes\":{\"n\":\"4\"},\"end\":24111,\"start\":24060},{\"attributes\":{\"n\":\"4.1\"},\"end\":25764,\"start\":25729},{\"end\":26480,\"start\":26469},{\"end\":27337,\"start\":27326},{\"end\":29955,\"start\":29950},{\"attributes\":{\"n\":\"4.2\"},\"end\":33111,\"start\":33104},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":33998,\"start\":33776},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":36824,\"start\":36736},{\"attributes\":{\"n\":\"4.3\"},\"end\":38480,\"start\":38448},{\"attributes\":{\"n\":\"4.4\"},\"end\":38812,\"start\":38795},{\"attributes\":{\"n\":\"4.5\"},\"end\":39529,\"start\":39500},{\"attributes\":{\"n\":\"4.5.2\"},\"end\":43268,\"start\":43227},{\"attributes\":{\"n\":\"4.7\"},\"end\":45436,\"start\":45413},{\"attributes\":{\"n\":\"4.7.1\"},\"end\":46131,\"start\":46114},{\"attributes\":{\"n\":\"5\"},\"end\":48565,\"start\":48516},{\"attributes\":{\"n\":\"5.1\"},\"end\":50696,\"start\":50646},{\"end\":52514,\"start\":52506},{\"end\":52892,\"start\":52884},{\"end\":53163,\"start\":53147},{\"end\":53549,\"start\":53541},{\"end\":53622,\"start\":53610},{\"end\":53985,\"start\":53969},{\"end\":54494,\"start\":54476},{\"end\":55537,\"start\":55528},{\"end\":55610,\"start\":55601},{\"end\":55751,\"start\":55742},{\"end\":55961,\"start\":55943},{\"end\":56235,\"start\":56226},{\"end\":56470,\"start\":56466},{\"end\":57002,\"start\":56993},{\"end\":57078,\"start\":57069},{\"end\":58056,\"start\":58047},{\"end\":58434,\"start\":58425},{\"end\":58861,\"start\":58852}]", "table": "[{\"end\":58045,\"start\":57806},{\"end\":58423,\"start\":58212},{\"end\":58850,\"start\":58525},{\"end\":59308,\"start\":59109}]", "figure_caption": "[{\"end\":52882,\"start\":52516},{\"end\":52990,\"start\":52894},{\"end\":53020,\"start\":52993},{\"end\":53145,\"start\":53023},{\"end\":53539,\"start\":53166},{\"end\":53608,\"start\":53551},{\"end\":53753,\"start\":53625},{\"end\":53967,\"start\":53756},{\"end\":54474,\"start\":53988},{\"end\":54696,\"start\":54499},{\"end\":54815,\"start\":54699},{\"end\":55286,\"start\":54818},{\"end\":55441,\"start\":55289},{\"end\":55526,\"start\":55444},{\"end\":55599,\"start\":55540},{\"end\":55734,\"start\":55613},{\"end\":55740,\"start\":55737},{\"end\":55941,\"start\":55754},{\"end\":56181,\"start\":55966},{\"end\":56224,\"start\":56184},{\"end\":56464,\"start\":56238},{\"end\":56856,\"start\":56472},{\"end\":56860,\"start\":56859},{\"end\":56991,\"start\":56863},{\"end\":57067,\"start\":57005},{\"end\":57377,\"start\":57081},{\"end\":57806,\"start\":57380},{\"end\":58212,\"start\":58058},{\"end\":58525,\"start\":58436},{\"end\":59109,\"start\":58863}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11852,\"start\":11844},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18446,\"start\":18438},{\"end\":19759,\"start\":19750},{\"end\":21182,\"start\":21173},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21723,\"start\":21715},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22506,\"start\":22498},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23862,\"start\":23854},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25898,\"start\":25890},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26095,\"start\":26087},{\"end\":27574,\"start\":27564},{\"end\":27772,\"start\":27764},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28867,\"start\":28859},{\"end\":30945,\"start\":30937},{\"end\":31329,\"start\":31321},{\"end\":31720,\"start\":31712},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32274,\"start\":32265},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32495,\"start\":32485},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32841,\"start\":32832},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34955,\"start\":34946},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35675,\"start\":35665},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36085,\"start\":36073},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38721,\"start\":38711},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39495,\"start\":39486},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41971,\"start\":41962},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42298,\"start\":42289},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42315,\"start\":42305},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42910,\"start\":42901},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44748,\"start\":44738},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45197,\"start\":45187},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48306,\"start\":48296},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48400,\"start\":48390},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48512,\"start\":48502},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49348,\"start\":49339}]", "bib_author_first_name": "[{\"end\":59712,\"start\":59705},{\"end\":59729,\"start\":59723},{\"end\":59745,\"start\":59741},{\"end\":59759,\"start\":59754},{\"end\":60218,\"start\":60217},{\"end\":60220,\"start\":60219},{\"end\":60227,\"start\":60226},{\"end\":60235,\"start\":60234},{\"end\":60237,\"start\":60236},{\"end\":60672,\"start\":60667},{\"end\":60682,\"start\":60673},{\"end\":60975,\"start\":60974},{\"end\":60985,\"start\":60984},{\"end\":60987,\"start\":60986},{\"end\":60997,\"start\":60996},{\"end\":61415,\"start\":61414},{\"end\":62033,\"start\":62029},{\"end\":62035,\"start\":62034},{\"end\":62445,\"start\":62444},{\"end\":62447,\"start\":62446},{\"end\":62456,\"start\":62455},{\"end\":62458,\"start\":62457},{\"end\":62838,\"start\":62837},{\"end\":62847,\"start\":62846},{\"end\":62855,\"start\":62854},{\"end\":62865,\"start\":62864},{\"end\":63254,\"start\":63253},{\"end\":63263,\"start\":63262},{\"end\":63273,\"start\":63272},{\"end\":63281,\"start\":63280},{\"end\":63291,\"start\":63290},{\"end\":63293,\"start\":63292},{\"end\":63728,\"start\":63722},{\"end\":63741,\"start\":63736},{\"end\":63743,\"start\":63742},{\"end\":63756,\"start\":63753},{\"end\":66194,\"start\":66191},{\"end\":66216,\"start\":66210},{\"end\":66745,\"start\":66740},{\"end\":66760,\"start\":66754},{\"end\":67153,\"start\":67152},{\"end\":67163,\"start\":67162},{\"end\":67173,\"start\":67172},{\"end\":67175,\"start\":67174},{\"end\":67190,\"start\":67189},{\"end\":67200,\"start\":67199},{\"end\":67202,\"start\":67201},{\"end\":67686,\"start\":67681},{\"end\":67688,\"start\":67687},{\"end\":67700,\"start\":67695},{\"end\":67718,\"start\":67714},{\"end\":67720,\"start\":67719},{\"end\":68409,\"start\":68408},{\"end\":68415,\"start\":68414},{\"end\":68417,\"start\":68416},{\"end\":68423,\"start\":68422},{\"end\":68433,\"start\":68432},{\"end\":68435,\"start\":68434},{\"end\":68441,\"start\":68440},{\"end\":68450,\"start\":68449},{\"end\":68452,\"start\":68451},{\"end\":68459,\"start\":68458},{\"end\":68461,\"start\":68460},{\"end\":68468,\"start\":68467},{\"end\":68470,\"start\":68469},{\"end\":68479,\"start\":68478},{\"end\":68481,\"start\":68480},{\"end\":68493,\"start\":68492},{\"end\":68501,\"start\":68500},{\"end\":68503,\"start\":68502},{\"end\":68511,\"start\":68510},{\"end\":68513,\"start\":68512},{\"end\":68521,\"start\":68520},{\"end\":68523,\"start\":68522},{\"end\":68529,\"start\":68528},{\"end\":68531,\"start\":68530},{\"end\":68540,\"start\":68539},{\"end\":68542,\"start\":68541},{\"end\":68549,\"start\":68548},{\"end\":68551,\"start\":68550},{\"end\":68561,\"start\":68560},{\"end\":68570,\"start\":68569},{\"end\":68574,\"start\":68571},{\"end\":69276,\"start\":69275},{\"end\":69284,\"start\":69283},{\"end\":69292,\"start\":69291},{\"end\":69302,\"start\":69301},{\"end\":69312,\"start\":69311},{\"end\":69321,\"start\":69320},{\"end\":69333,\"start\":69332},{\"end\":69341,\"start\":69340},{\"end\":69353,\"start\":69352},{\"end\":69361,\"start\":69360},{\"end\":69374,\"start\":69373},{\"end\":69384,\"start\":69383},{\"end\":69386,\"start\":69385},{\"end\":69395,\"start\":69394},{\"end\":69397,\"start\":69396},{\"end\":70003,\"start\":69998},{\"end\":70019,\"start\":70014},{\"end\":70034,\"start\":70028},{\"end\":70049,\"start\":70043},{\"end\":70061,\"start\":70057},{\"end\":70749,\"start\":70744},{\"end\":70786,\"start\":70783},{\"end\":70788,\"start\":70787},{\"end\":71190,\"start\":71189},{\"end\":71201,\"start\":71200},{\"end\":71213,\"start\":71212},{\"end\":71824,\"start\":71823},{\"end\":71826,\"start\":71825},{\"end\":72026,\"start\":72025},{\"end\":72028,\"start\":72027},{\"end\":72203,\"start\":72202},{\"end\":72205,\"start\":72204},{\"end\":72567,\"start\":72562},{\"end\":72582,\"start\":72576},{\"end\":72610,\"start\":72606},{\"end\":72622,\"start\":72619},{\"end\":72624,\"start\":72623},{\"end\":72975,\"start\":72974},{\"end\":72985,\"start\":72984},{\"end\":72993,\"start\":72992},{\"end\":73003,\"start\":73002},{\"end\":73005,\"start\":73004},{\"end\":73014,\"start\":73013},{\"end\":73027,\"start\":73026},{\"end\":73038,\"start\":73037},{\"end\":73040,\"start\":73039},{\"end\":73557,\"start\":73556},{\"end\":73567,\"start\":73566},{\"end\":73578,\"start\":73577},{\"end\":73588,\"start\":73587},{\"end\":73590,\"start\":73589},{\"end\":74041,\"start\":74036},{\"end\":74054,\"start\":74050},{\"end\":74073,\"start\":74070},{\"end\":74075,\"start\":74074},{\"end\":74616,\"start\":74611},{\"end\":74628,\"start\":74625},{\"end\":74630,\"start\":74629},{\"end\":75054,\"start\":75049},{\"end\":75069,\"start\":75063},{\"end\":75086,\"start\":75082},{\"end\":75105,\"start\":75102},{\"end\":75107,\"start\":75106},{\"end\":75544,\"start\":75543},{\"end\":75986,\"start\":75985},{\"end\":75997,\"start\":75996},{\"end\":76410,\"start\":76409},{\"end\":76421,\"start\":76420},{\"end\":76886,\"start\":76885},{\"end\":76897,\"start\":76896},{\"end\":76904,\"start\":76903},{\"end\":77351,\"start\":77344},{\"end\":77713,\"start\":77712},{\"end\":77715,\"start\":77714},{\"end\":77721,\"start\":77720},{\"end\":77727,\"start\":77726},{\"end\":77736,\"start\":77735},{\"end\":77746,\"start\":77745},{\"end\":77755,\"start\":77754},{\"end\":77765,\"start\":77764},{\"end\":77774,\"start\":77773},{\"end\":77776,\"start\":77775},{\"end\":77786,\"start\":77785},{\"end\":77788,\"start\":77787},{\"end\":77796,\"start\":77795},{\"end\":77798,\"start\":77797},{\"end\":77810,\"start\":77809},{\"end\":78351,\"start\":78350},{\"end\":78353,\"start\":78352},{\"end\":78359,\"start\":78358},{\"end\":78365,\"start\":78364},{\"end\":78367,\"start\":78366},{\"end\":78376,\"start\":78375},{\"end\":78386,\"start\":78385},{\"end\":78388,\"start\":78387},{\"end\":78398,\"start\":78397},{\"end\":78402,\"start\":78399},{\"end\":78410,\"start\":78409},{\"end\":78412,\"start\":78411},{\"end\":78424,\"start\":78423}]", "bib_author_last_name": "[{\"end\":59721,\"start\":59713},{\"end\":59739,\"start\":59730},{\"end\":59752,\"start\":59746},{\"end\":59766,\"start\":59760},{\"end\":60224,\"start\":60221},{\"end\":60232,\"start\":60228},{\"end\":60241,\"start\":60238},{\"end\":60982,\"start\":60976},{\"end\":60994,\"start\":60988},{\"end\":61004,\"start\":60998},{\"end\":61420,\"start\":61416},{\"end\":61428,\"start\":61422},{\"end\":62042,\"start\":62036},{\"end\":62453,\"start\":62448},{\"end\":62465,\"start\":62459},{\"end\":62844,\"start\":62839},{\"end\":62852,\"start\":62848},{\"end\":62862,\"start\":62856},{\"end\":62872,\"start\":62866},{\"end\":63260,\"start\":63255},{\"end\":63270,\"start\":63264},{\"end\":63278,\"start\":63274},{\"end\":63288,\"start\":63282},{\"end\":63300,\"start\":63294},{\"end\":63734,\"start\":63729},{\"end\":63751,\"start\":63744},{\"end\":63764,\"start\":63757},{\"end\":64391,\"start\":64378},{\"end\":64565,\"start\":64552},{\"end\":65062,\"start\":65049},{\"end\":65465,\"start\":65452},{\"end\":65837,\"start\":65824},{\"end\":66208,\"start\":66195},{\"end\":66229,\"start\":66217},{\"end\":66236,\"start\":66231},{\"end\":66752,\"start\":66746},{\"end\":66766,\"start\":66761},{\"end\":67160,\"start\":67154},{\"end\":67170,\"start\":67164},{\"end\":67187,\"start\":67176},{\"end\":67197,\"start\":67191},{\"end\":67209,\"start\":67203},{\"end\":67693,\"start\":67689},{\"end\":67712,\"start\":67701},{\"end\":67727,\"start\":67721},{\"end\":68412,\"start\":68410},{\"end\":68420,\"start\":68418},{\"end\":68430,\"start\":68424},{\"end\":68438,\"start\":68436},{\"end\":68447,\"start\":68442},{\"end\":68456,\"start\":68453},{\"end\":68465,\"start\":68462},{\"end\":68476,\"start\":68471},{\"end\":68490,\"start\":68482},{\"end\":68498,\"start\":68494},{\"end\":68508,\"start\":68504},{\"end\":68518,\"start\":68514},{\"end\":68526,\"start\":68524},{\"end\":68537,\"start\":68532},{\"end\":68546,\"start\":68543},{\"end\":68558,\"start\":68552},{\"end\":68567,\"start\":68562},{\"end\":68579,\"start\":68575},{\"end\":69281,\"start\":69277},{\"end\":69289,\"start\":69285},{\"end\":69299,\"start\":69293},{\"end\":69309,\"start\":69303},{\"end\":69318,\"start\":69313},{\"end\":69330,\"start\":69322},{\"end\":69338,\"start\":69334},{\"end\":69350,\"start\":69342},{\"end\":69358,\"start\":69354},{\"end\":69371,\"start\":69362},{\"end\":69381,\"start\":69375},{\"end\":69392,\"start\":69387},{\"end\":69404,\"start\":69398},{\"end\":70012,\"start\":70004},{\"end\":70026,\"start\":70020},{\"end\":70041,\"start\":70035},{\"end\":70055,\"start\":70050},{\"end\":70068,\"start\":70062},{\"end\":70773,\"start\":70750},{\"end\":70781,\"start\":70775},{\"end\":70802,\"start\":70789},{\"end\":70810,\"start\":70804},{\"end\":71198,\"start\":71191},{\"end\":71210,\"start\":71202},{\"end\":71221,\"start\":71214},{\"end\":71832,\"start\":71827},{\"end\":72034,\"start\":72029},{\"end\":72217,\"start\":72206},{\"end\":72574,\"start\":72568},{\"end\":72604,\"start\":72583},{\"end\":72617,\"start\":72611},{\"end\":72631,\"start\":72625},{\"end\":72982,\"start\":72976},{\"end\":72990,\"start\":72986},{\"end\":73000,\"start\":72994},{\"end\":73011,\"start\":73006},{\"end\":73024,\"start\":73015},{\"end\":73035,\"start\":73028},{\"end\":73047,\"start\":73041},{\"end\":73564,\"start\":73558},{\"end\":73575,\"start\":73568},{\"end\":73585,\"start\":73579},{\"end\":73597,\"start\":73591},{\"end\":74048,\"start\":74042},{\"end\":74068,\"start\":74055},{\"end\":74088,\"start\":74076},{\"end\":74096,\"start\":74090},{\"end\":74623,\"start\":74617},{\"end\":74644,\"start\":74631},{\"end\":74652,\"start\":74646},{\"end\":75061,\"start\":75055},{\"end\":75080,\"start\":75070},{\"end\":75100,\"start\":75087},{\"end\":75121,\"start\":75108},{\"end\":75129,\"start\":75123},{\"end\":75552,\"start\":75545},{\"end\":75994,\"start\":75987},{\"end\":76006,\"start\":75998},{\"end\":76418,\"start\":76411},{\"end\":76430,\"start\":76422},{\"end\":76894,\"start\":76887},{\"end\":76901,\"start\":76898},{\"end\":76913,\"start\":76905},{\"end\":77359,\"start\":77352},{\"end\":77718,\"start\":77716},{\"end\":77724,\"start\":77722},{\"end\":77733,\"start\":77728},{\"end\":77743,\"start\":77737},{\"end\":77752,\"start\":77747},{\"end\":77762,\"start\":77756},{\"end\":77771,\"start\":77766},{\"end\":77783,\"start\":77777},{\"end\":77793,\"start\":77789},{\"end\":77807,\"start\":77799},{\"end\":77816,\"start\":77811},{\"end\":78356,\"start\":78354},{\"end\":78362,\"start\":78360},{\"end\":78373,\"start\":78368},{\"end\":78383,\"start\":78377},{\"end\":78395,\"start\":78389},{\"end\":78407,\"start\":78403},{\"end\":78421,\"start\":78413},{\"end\":78430,\"start\":78425},{\"end\":79016,\"start\":79010}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":60090,\"start\":59570},{\"attributes\":{\"doi\":\"10.1109/TBME.2006.883695\",\"id\":\"b1\",\"matched_paper_id\":9963931},\"end\":60595,\"start\":60092},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":60125307},\"end\":60883,\"start\":60597},{\"attributes\":{\"doi\":\"10.1109/IJCNN.2013.6706829\",\"id\":\"b3\",\"matched_paper_id\":15654769},\"end\":61352,\"start\":60885},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14487970},\"end\":61939,\"start\":61354},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5943414},\"end\":62397,\"start\":61941},{\"attributes\":{\"doi\":\"10.1109/TCSI.2017.2697945\",\"id\":\"b6\",\"matched_paper_id\":25989002},\"end\":62767,\"start\":62399},{\"attributes\":{\"doi\":\"10.1109/ICRC.2017.8123650\",\"id\":\"b7\",\"matched_paper_id\":21351739},\"end\":63204,\"start\":62769},{\"attributes\":{\"doi\":\"10.1109/HPCA.2017.28\",\"id\":\"b8\",\"matched_paper_id\":1677864},\"end\":63679,\"start\":63206},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":39020350},\"end\":64376,\"start\":63681},{\"attributes\":{\"id\":\"b10\"},\"end\":64507,\"start\":64378},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12261165},\"end\":64922,\"start\":64509},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":733980},\"end\":65378,\"start\":64924},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7149851},\"end\":65789,\"start\":65380},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13894815},\"end\":66127,\"start\":65791},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":60571601},\"end\":66662,\"start\":66129},{\"attributes\":{\"id\":\"b16\"},\"end\":67026,\"start\":66664},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":51614496},\"end\":67609,\"start\":67028},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15905828},\"end\":68258,\"start\":67611},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":25209638},\"end\":69157,\"start\":68260},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3553418},\"end\":69892,\"start\":69159},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49291228},\"end\":70627,\"start\":69894},{\"attributes\":{\"id\":\"b22\"},\"end\":71117,\"start\":70629},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53060237},\"end\":71784,\"start\":71119},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2352281},\"end\":71986,\"start\":71786},{\"attributes\":{\"id\":\"b25\"},\"end\":72140,\"start\":71988},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":125616749},\"end\":72468,\"start\":72142},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12008695},\"end\":72917,\"start\":72470},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10569020},\"end\":73417,\"start\":72919},{\"attributes\":{\"id\":\"b29\"},\"end\":73898,\"start\":73419},{\"attributes\":{\"id\":\"b30\"},\"end\":74313,\"start\":73900},{\"attributes\":{\"id\":\"b31\"},\"end\":74522,\"start\":74315},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9812826},\"end\":74916,\"start\":74524},{\"attributes\":{\"id\":\"b33\"},\"end\":75434,\"start\":74918},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15600360},\"end\":75894,\"start\":75436},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1690456},\"end\":76286,\"start\":75896},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15258913},\"end\":76795,\"start\":76288},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":24719176},\"end\":77293,\"start\":76797},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":53995453},\"end\":77600,\"start\":77295},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":53227383},\"end\":78233,\"start\":77602},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3869844},\"end\":78866,\"start\":78235},{\"attributes\":{\"id\":\"b41\"},\"end\":79223,\"start\":78868}]", "bib_title": "[{\"end\":59703,\"start\":59570},{\"end\":60215,\"start\":60092},{\"end\":60665,\"start\":60597},{\"end\":60972,\"start\":60885},{\"end\":61412,\"start\":61354},{\"end\":62027,\"start\":61941},{\"end\":62442,\"start\":62399},{\"end\":62835,\"start\":62769},{\"end\":63251,\"start\":63206},{\"end\":63720,\"start\":63681},{\"end\":64550,\"start\":64509},{\"end\":65047,\"start\":64924},{\"end\":65450,\"start\":65380},{\"end\":65822,\"start\":65791},{\"end\":66189,\"start\":66129},{\"end\":66738,\"start\":66664},{\"end\":67150,\"start\":67028},{\"end\":67679,\"start\":67611},{\"end\":68406,\"start\":68260},{\"end\":69273,\"start\":69159},{\"end\":69996,\"start\":69894},{\"end\":71187,\"start\":71119},{\"end\":71821,\"start\":71786},{\"end\":72200,\"start\":72142},{\"end\":72560,\"start\":72470},{\"end\":72972,\"start\":72919},{\"end\":73554,\"start\":73419},{\"end\":74609,\"start\":74524},{\"end\":75541,\"start\":75436},{\"end\":75983,\"start\":75896},{\"end\":76407,\"start\":76288},{\"end\":76883,\"start\":76797},{\"end\":77342,\"start\":77295},{\"end\":77710,\"start\":77602},{\"end\":78348,\"start\":78235}]", "bib_author": "[{\"end\":59723,\"start\":59705},{\"end\":59741,\"start\":59723},{\"end\":59754,\"start\":59741},{\"end\":59768,\"start\":59754},{\"end\":60226,\"start\":60217},{\"end\":60234,\"start\":60226},{\"end\":60243,\"start\":60234},{\"end\":60685,\"start\":60667},{\"end\":60984,\"start\":60974},{\"end\":60996,\"start\":60984},{\"end\":61006,\"start\":60996},{\"end\":61422,\"start\":61414},{\"end\":61430,\"start\":61422},{\"end\":62044,\"start\":62029},{\"end\":62455,\"start\":62444},{\"end\":62467,\"start\":62455},{\"end\":62846,\"start\":62837},{\"end\":62854,\"start\":62846},{\"end\":62864,\"start\":62854},{\"end\":62874,\"start\":62864},{\"end\":63262,\"start\":63253},{\"end\":63272,\"start\":63262},{\"end\":63280,\"start\":63272},{\"end\":63290,\"start\":63280},{\"end\":63302,\"start\":63290},{\"end\":63736,\"start\":63722},{\"end\":63753,\"start\":63736},{\"end\":63766,\"start\":63753},{\"end\":64393,\"start\":64378},{\"end\":64567,\"start\":64552},{\"end\":65064,\"start\":65049},{\"end\":65467,\"start\":65452},{\"end\":65839,\"start\":65824},{\"end\":66210,\"start\":66191},{\"end\":66231,\"start\":66210},{\"end\":66238,\"start\":66231},{\"end\":66754,\"start\":66740},{\"end\":66768,\"start\":66754},{\"end\":67162,\"start\":67152},{\"end\":67172,\"start\":67162},{\"end\":67189,\"start\":67172},{\"end\":67199,\"start\":67189},{\"end\":67211,\"start\":67199},{\"end\":67695,\"start\":67681},{\"end\":67714,\"start\":67695},{\"end\":67729,\"start\":67714},{\"end\":68414,\"start\":68408},{\"end\":68422,\"start\":68414},{\"end\":68432,\"start\":68422},{\"end\":68440,\"start\":68432},{\"end\":68449,\"start\":68440},{\"end\":68458,\"start\":68449},{\"end\":68467,\"start\":68458},{\"end\":68478,\"start\":68467},{\"end\":68492,\"start\":68478},{\"end\":68500,\"start\":68492},{\"end\":68510,\"start\":68500},{\"end\":68520,\"start\":68510},{\"end\":68528,\"start\":68520},{\"end\":68539,\"start\":68528},{\"end\":68548,\"start\":68539},{\"end\":68560,\"start\":68548},{\"end\":68569,\"start\":68560},{\"end\":68581,\"start\":68569},{\"end\":69283,\"start\":69275},{\"end\":69291,\"start\":69283},{\"end\":69301,\"start\":69291},{\"end\":69311,\"start\":69301},{\"end\":69320,\"start\":69311},{\"end\":69332,\"start\":69320},{\"end\":69340,\"start\":69332},{\"end\":69352,\"start\":69340},{\"end\":69360,\"start\":69352},{\"end\":69373,\"start\":69360},{\"end\":69383,\"start\":69373},{\"end\":69394,\"start\":69383},{\"end\":69406,\"start\":69394},{\"end\":70014,\"start\":69998},{\"end\":70028,\"start\":70014},{\"end\":70043,\"start\":70028},{\"end\":70057,\"start\":70043},{\"end\":70070,\"start\":70057},{\"end\":70775,\"start\":70744},{\"end\":70783,\"start\":70775},{\"end\":70804,\"start\":70783},{\"end\":70812,\"start\":70804},{\"end\":71200,\"start\":71189},{\"end\":71212,\"start\":71200},{\"end\":71223,\"start\":71212},{\"end\":71834,\"start\":71823},{\"end\":72036,\"start\":72025},{\"end\":72219,\"start\":72202},{\"end\":72576,\"start\":72562},{\"end\":72606,\"start\":72576},{\"end\":72619,\"start\":72606},{\"end\":72633,\"start\":72619},{\"end\":72984,\"start\":72974},{\"end\":72992,\"start\":72984},{\"end\":73002,\"start\":72992},{\"end\":73013,\"start\":73002},{\"end\":73026,\"start\":73013},{\"end\":73037,\"start\":73026},{\"end\":73049,\"start\":73037},{\"end\":73566,\"start\":73556},{\"end\":73577,\"start\":73566},{\"end\":73587,\"start\":73577},{\"end\":73599,\"start\":73587},{\"end\":74050,\"start\":74036},{\"end\":74070,\"start\":74050},{\"end\":74090,\"start\":74070},{\"end\":74098,\"start\":74090},{\"end\":74625,\"start\":74611},{\"end\":74646,\"start\":74625},{\"end\":74654,\"start\":74646},{\"end\":75063,\"start\":75049},{\"end\":75082,\"start\":75063},{\"end\":75102,\"start\":75082},{\"end\":75123,\"start\":75102},{\"end\":75131,\"start\":75123},{\"end\":75554,\"start\":75543},{\"end\":75996,\"start\":75985},{\"end\":76008,\"start\":75996},{\"end\":76420,\"start\":76409},{\"end\":76432,\"start\":76420},{\"end\":76896,\"start\":76885},{\"end\":76903,\"start\":76896},{\"end\":76915,\"start\":76903},{\"end\":77361,\"start\":77344},{\"end\":77720,\"start\":77712},{\"end\":77726,\"start\":77720},{\"end\":77735,\"start\":77726},{\"end\":77745,\"start\":77735},{\"end\":77754,\"start\":77745},{\"end\":77764,\"start\":77754},{\"end\":77773,\"start\":77764},{\"end\":77785,\"start\":77773},{\"end\":77795,\"start\":77785},{\"end\":77809,\"start\":77795},{\"end\":77818,\"start\":77809},{\"end\":78358,\"start\":78350},{\"end\":78364,\"start\":78358},{\"end\":78375,\"start\":78364},{\"end\":78385,\"start\":78375},{\"end\":78397,\"start\":78385},{\"end\":78409,\"start\":78397},{\"end\":78423,\"start\":78409},{\"end\":78432,\"start\":78423},{\"end\":79018,\"start\":79010}]", "bib_venue": "[{\"end\":62193,\"start\":62127},{\"end\":63960,\"start\":63932},{\"end\":65991,\"start\":65919},{\"end\":66403,\"start\":66329},{\"end\":67936,\"start\":67841},{\"end\":70276,\"start\":70196},{\"end\":71515,\"start\":71373},{\"end\":75683,\"start\":75627},{\"end\":59810,\"start\":59768},{\"end\":60310,\"start\":60267},{\"end\":60736,\"start\":60685},{\"end\":61099,\"start\":61032},{\"end\":61545,\"start\":61430},{\"end\":62125,\"start\":62044},{\"end\":62551,\"start\":62492},{\"end\":62968,\"start\":62899},{\"end\":63433,\"start\":63351},{\"end\":63851,\"start\":63794},{\"end\":64418,\"start\":64393},{\"end\":64685,\"start\":64567},{\"end\":65144,\"start\":65123},{\"end\":65551,\"start\":65467},{\"end\":65917,\"start\":65839},{\"end\":66327,\"start\":66238},{\"end\":66819,\"start\":66790},{\"end\":67294,\"start\":67237},{\"end\":67839,\"start\":67729},{\"end\":68654,\"start\":68606},{\"end\":69502,\"start\":69432},{\"end\":70194,\"start\":70125},{\"end\":70742,\"start\":70629},{\"end\":71371,\"start\":71223},{\"end\":71870,\"start\":71834},{\"end\":72023,\"start\":71988},{\"end\":72276,\"start\":72244},{\"end\":72685,\"start\":72633},{\"end\":73133,\"start\":73074},{\"end\":73635,\"start\":73625},{\"end\":74034,\"start\":73900},{\"end\":74413,\"start\":74315},{\"end\":74686,\"start\":74654},{\"end\":75047,\"start\":74918},{\"end\":75625,\"start\":75554},{\"end\":76061,\"start\":76032},{\"end\":76518,\"start\":76458},{\"end\":77040,\"start\":76976},{\"end\":77420,\"start\":77389},{\"end\":77879,\"start\":77843},{\"end\":78523,\"start\":78458},{\"end\":79008,\"start\":78868}]"}}}, "year": 2023, "month": 12, "day": 17}