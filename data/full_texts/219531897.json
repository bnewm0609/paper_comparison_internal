{"id": 219531897, "updated": "2023-10-06 14:24:52.419", "metadata": {"title": "Peer Collaborative Learning for Online Knowledge Distillation", "authors": "[{\"first\":\"Guile\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Shaogang\",\"last\":\"Gong\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 7}, "abstract": "Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a smaller student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training strategy. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation. Specifically, we employ a multi-branch network (each branch is a peer) and assemble the features from peers with an additional classifier as the peer ensemble teacher to transfer knowledge from the high-capacity teacher to peers and to further optimise the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which facilitates to optimise a more stable model and alleviate the accumulation of training error among peers. Integrating them into a unified framework takes full advantage of online ensembling and network collaboration for improving the quality of online distillation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method not only significantly improves the generalisation capability of various backbone networks, but also outperforms the state-of-the-art alternative methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2006.04147", "mag": "3034169498", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/WuG21b", "doi": "10.1609/aaai.v35i12.17234"}}, "content": {"source": {"pdf_hash": "a7058717ce5d37094fed32147d3b98d0edc0df16", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.04147v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "55880f9feb15a01674d03bb2d7d79e809eb56a09", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a7058717ce5d37094fed32147d3b98d0edc0df16.txt", "contents": "\nPeer Collaborative Learning for Online Knowledge Distillation\n\n\nGuile Wu guile.wu@qmul.ac.uk \nQueen Mary University of London\n\n\nShaogang Gong s.gong@qmul.ac.uk \nQueen Mary University of London\n\n\nPeer Collaborative Learning for Online Knowledge Distillation\n\nTraditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a highcapacity teacher model to a smaller student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training strategy. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation. Specifically, we employ a multi-branch network (each branch is a peer) and assemble the features from peers with an additional classifier as the peer ensemble teacher to transfer knowledge from the high-capacity teacher to peers and to further optimise the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which facilitates to optimise a more stable model and alleviate the accumulation of training error among peers. Integrating them into a unified framework takes full advantage of online ensembling and network collaboration for improving the quality of online distillation. Extensive experiments on CIFAR-10, CIFAR-100 and Ima-geNet show that the proposed method not only significantly improves the generalisation capability of various backbone networks, but also outperforms the state-of-the-art alternative methods.\n\nIntroduction\n\nDeep learning has achieved incredible success in many computer vision tasks in recent years. Whilst many studies focus on developing deeper and/or wider networks for improving the performance [5,26,24], these cumbersome networks require more computational resources hindering their deployments in resource-limited scenarios. To alle-viate this problem, knowledge distillation is developed to transfer knowledge from a stronger teacher [6] or an online ensemble [13] to a student model, which is more suitable for deployment.\n\nTraditionally, knowledge distillation requires to pre-train a high-capacity teacher model in the first stage, and then transfer the knowledge of the teacher to a smaller student model in the second stage [6,18,17]. Via aligning the soft prediction [6] or the feature representation [18] between the teacher and the student, the student model usually obtains approximate accuracy as the teacher, but significantly reduces the model complexity for deployment. However, this traditional strategy usually requires more training time and computational cost, since the teacher and the student are trained in two separate stages.\n\nOn the other hand, recent online knowledge distillation [13,28,1] proposes to directly optimise the target network, following a one-stage end-to-end training strategy. Instead of pre-training a high-capacity teacher, online distillation typically integrates the teacher into the student model using a hierarchical network with shared intermediate-level representations [21] (Fig. 1(a)), multiple parallel networks for mutual distillation [28] (Fig. 1(b)), or a multi-branch network with online ensembling [13] (Fig. 1(c)). Although these methods have shown their superiority over their traditional counterparts, collaborative learning and mutual learning fail to construct a stronger ensemble teacher to transfer knowledge from a high-capacity teacher to a student, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher.\n\nIn this work, we propose a novel Peer Collaborative Learning (PCL) method for online knowledge distillation. As shown in Fig. 1(d), we integrate online ensembling and network collaboration into a unified framework to take full advantage of them for improving the quality of online distillation without pre-training a high-capacity teacher model. Specifically, we construct a multi-branch network (each branch is a peer), in which the low-level layers are shared and the high-level layers are separated. To facilitate the online distillation, wo employ two type of peer collaborations:  (1) We assemble the feature representations of peers with an additional classifier as the peer ensemble teacher; (2) We use the temporal mean model of each peer as the peer mean teacher. The first teacher is a high-capacity model, which helps to distil ensembled knowledge from a stronger teacher to each peer, and in turn further improves the ensemble teacher model. Instead of using peer logit summation as the ensemble teacher [13], we assemble the features of peers with an additional classifier as the ensemble teacher to learn discriminative information among the peer feature representations and to further optimise the teacher. Since peers are separated in the multi-branch network, the training error among peers will be accumulated during online ensembling, which impairs the collaboration among peers.\n\u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 (a) (b) (c) (d)\nTo alleviate this limitation, we use the second teacher to collaboratively distil knowledge among peers. Instead of using mutual learning among peers [28], we utilise the temporal mean of shared low-level layers and separated high-level layers to construct the peer mean teacher to distil knowledge among peers, which helps to alleviate the accumulation of training error among peers and to facilitate the optimisation of a more stable model. Besides, we perform random augmentation multiple times on the input to each peer to further enhance the diversity of knowledge learned in the peers. In test, we use the temporal mean network of a peer for deployment, which has the identical number of parameters as the backbone network, so there is no extra inference cost for deployment. Furthermore, the output feature representations plus the additional classifier from the peer mean teachers forms a high-capacity ensemble for deployment to get better performance in the scenarios where computational cost is less constrained.\n\nExtensive experiments on CIFAR-10 [11], CIFAR-100 [11] and ImageNet [19] using various backbone net-works (ResNet [5], VGG [20], DenseNet [9], WRN [26] and ResNeXt [24]) show that the proposed method significantly improves the generalisation capability of the backbone networks and outperforms the state-of-the-art alternatives.\n\n\nRelated Work\n\nTraditional Knowledge Distillation [6] is one of the most effective solutions to compress a cumbersome model or an ensemble of models into a smaller model. In [6], Hinton et al. firstly introduce the process of transferring the knowledge from a high-capacity teacher model to a compact student model as \"distillation\", which is accomplished by aligning the soft output prediction between the teacher and the student. After that, many promising knowledge distillation methods have been designed to facilitate the optimisation process of distillation via exploiting various \"knowledge\", such as intermediate representations [18], flow between layers [25], attention maps [27], structural relations [16] and activation similarity [23]. Although these methods have shown promising performance in compressing the model for deployment, they typically follow a two-stage training solution by pre-training a high-capacity teacher model for transferring knowledge to a compact student model, which requires more training time and computational cost. Online Knowledge Distillation [13, 10, 1, 28] follows a one-stage end-to-end training strategy to optimise the target network for deployment with knowledge distillation among multiple networks or branches without pre-training a highcapacity teacher. In [21], Song et al. propose to distil knowledge among multiple classifier heads of a hierarchical network for improving the generalisation capability of the network without extra inference cost. In [28], Zhang et al. introduce a mutual learning solution to distil knowledge among multiple parallel networks with the same input. Although these methods help to improve the generalisation of the target network, they only distil limited knowledge among parallel networks or heads and fail to construct stronger online teachers to further improve the students. More similar to our work, Lan et al. [13] use a multi-branch network and assemble logits from multiple branches (students) as the teacher to improve the generalisation of each student network. However, the logit summation impedes the further optimisation of the ensemble teacher, and online ensembling ignores the collaboration among branches, resulting in suboptimal performance. In [10], Kim et al. integrate feature representations of multiple branches into the online ensembling, but their method requires more convolutional operations for the feature fusion and also fails to exploit the collaboration among branches. To address these limitations, in our work: (2) we assemble the features from peers with an additional classifier as the peer ensemble teacher to distil knowledge from a stronger teacher to each peer and to further optimise the teacher; (1) we exploit the temporal mean models of each peer as the peer mean teacher to distil knowledge among peers, which helps to optimise a more stable model and alleviate the accumulation of training error. The integration of these two teachers into a unified framework enables to significantly improve the generalisation capability of each peer and the ensemble, resulting in better performance.\n\nNeural Network Ensembling is a simple and effective solution for improving the generalisation performance of a model [4,29,15]. Although this can usually bring better performance, training multiple neural networks to create an ensemble requires significantly more training time and computational cost. Recent trend in neural network ensembling focuses on training a single model and exploiting different phases of the model as an ensemble for performance improvement. In [8], Huang et al. force the model to visit multiple local minima and use the corresponding models as the snapshots for neural network ensembling. In [12], Laine et al. propose to use temporal ensembling of network predictions over multiple training epochs as the teacher to facilitate the optimisation of the current model for semisupervised learning. Our work differs from these works in that we use the feature representations of peers from a multi-branch network as the ensemble teacher for online knowledge distillation, instead of using the network predictions from different phases or generating multiple networks for ensembling. The peer collaborative distillation by peer mean teachers in our method shares the merit of mean teacher [22]. In [22], network weights over previous training epochs are averaged as a teacher to minimise the distance of predictions between the student and the teacher as the consistency regularisation for semi-supervised learn-ing. In contrast, our method uses the shared layers and multiple separated layers to form multiple peer mean teachers as the more accurate peer mean teachers to alleviate the accumulation of training error and to generate a more stable teacher for online distillation.\n\n\nPeer Collaborative Learning\n\n\nApproach Overview\n\nThe overview of the proposed Peer Collaborative Learning (PCL) is depicted in Fig. 2. We employ a m-branch network for model training and define each branch as a peer. Since the low-level layers across different branches usually contain similar low-level features regarding minor details of images, sharing them enables to reduce the training cost and improve the collaboration among peers. We therefore share the low-level layers and separate the high-level layers in the m-branch network.\n\nAs shown in Fig. 2, to facilitate online knowledge distillation, we use the feature concatenation of peers as the peer ensemble teacher and use the temporal mean model of each peer as the peer mean teacher. The training optimisation objective of PCL contains three components: The first component is the standard cross-entropy loss for multiclass classification of the peers (L p ce ) and the peer ensemble teacher (L t ce ); The second component is the peer ensemble distillation loss L pe for transferring knowledge from a stronger teacher to the student, which in turn improves the ensemble teacher; The third component is the peer mean distillation loss L pm for collaboratively distilling knowledge among peers. The overall training objective L is formulated as:\nL = L p ce + L t ce + L pe + L pm(1)\nIn test, we use a temporal mean network of a peer for deployment, which has the identical number of parameters as the backbone network, so there is no extra inference cost for deployment. In the scenarios where computational cost is less constrained, the output feature representations from the peer mean teachers can form a high-capacity ensemble teacher model to get better accuracy for deployment.\n\n\nPeer Ensemble Teacher\n\nInput Augmentation for Peers. Suppose there are n samples in a training dataset\n{(x i , y i )} n i=1 , where x i is the i-th input sample, y i \u2208 {1, 2, .\n.., C} is the corresponding label, and C is the number of classes in the dataset (C\u2264n). Existing multi-branch online distillation methods [13,1] directly use x i (applying random augmentation once) as the input to all the branches, which causes the homogenisation among peers and decreases the generalisation of the network. To alleviate this problem, we apply random augmentation m times to x i to produce m counterparts of x i X Random augmentation Figure 2. The overview of Peer Collaborative Learning (PCL) for online knowledge distillation. In the multi-branch network, the low-level layers are shared and the high-level layers are separated. The input is randomly transformed multiple times to generate the individual input to each peer. The output features of each peer form the stronger peer ensemble teacher, while the temporal mean models of peers form the peer mean teachers, which together form a unified framework for online knowledge distillation.\nX 1 X m \u2026 Low-level layers \u2026 High-level layers \u2026 \u2026 Feature Logit Temporal mean network \u2112 ce \u2112 ce \u2112 ce \u2112 pe \u2112 pe \u2112 pm \u2112 pm Branch-1 Branch-m \u2026 FC layer p t p(i.e. {x 1 i , x 2 i , ..\n., x m i }, and use each counterpart as the input to each peer. This provides additional richer knowledge of the inputs, which improves the generalisation among peers in addition to the random model initialisation. Online Ensembling. To construct a stronger teacher online for improving online distillation, logits from multiple networks/branches are usually summed (w/ or w/o attention gates) [13]. However, this hinders the ensemble teacher from further optimisation and ignores the discriminative information among peer representations, which might lead to a suboptimal solution since the summation is not further learned. In our work, we concatenate the features from peers and use an additional fully connected layer for classification to produce a stronger peer ensemble teacher. Therefore, the multi-class classification is performed for both the peer and the teacher as:\nL p ce = \u2212 m j=1 C c=1 y c log exp(z p j,c ) C k=1 exp(z p j,k )(2)L t ce = \u2212 C c=1 y c log exp(z t c ) C k=1 exp(z t k )(3)\nwhere z p j,c is the output logit from the last fully connected layer of the j-th peer over a class c, y c is the ground-truth la-bel indicator, z t c is the output logit from the fully connected layer of the peer ensemble teacher over a class c.\n\nTo transfer knowledge from the ensemble teacher to each peer, we compute the soft prediction of the j-th peer and the ensemble teacher as:\np p j,c = exp(z p j,c /T ) C k=1 exp(z p j,k /T ) , p t c = exp(z t c /T ) C k=1 exp(z t k /T )(4)\nwhere T is a temperature parameter [6], p p j,c is the soft prediction of the j-th peer over a class c, and p t c is the soft prediction of the ensemble teacher over a class c. Using Kullback Leibler (KL) divergence, the peer ensemble distillation loss L pe is formulated as:\nL pe = \u03c9(e)\u00b7T 2 m j=1 C c=1 p t c \u00b7log p t c p p j,c(5)\nwhere e is the current training epoch and \u03c9(\u00b7) is a weight ramp-up function [12] defined as:\n\u03c9(e) = \uf8f1 \uf8f2 \uf8f3 \u03bb\u00b7exp(\u22125 * (1 \u2212 e \u03b1 ) 2 ) , e \u2264 \u03b1 \u03bb , e > \u03b1(6)\nwhere \u03b1 is the epoch threshold for the ramp-up function and \u03bb is a parameter weighting the gradient magnitude.\n\nRemarks. The proposed peer ensemble teacher differs from existing feature fusion [7,10,2] in that we concatenate features from a multi-branch network and use a fully connected layer for classification without using additional convolutional operation or multiple networks. More importantly, as the input to each peer is performed with random augmentation, richer peer knowledge is exploited in the online ensembling resulting in the further improvement of online knowledge distillation.\n\n\nPeer Mean Teacher\n\nOnline ensembling is capable of constructing a stronger teacher for online distillation, but it ignores the collaboration among peers. On the other hand, mutual learning [28] and collaborative learning [21] benefit from mutual distillation among networks/heads, but they fail to construct a stronger teacher to further facilitate the optimisation among peers. In our work, we further use peer mutual distillation for improving the collaboration among peers. Instead of directly transferring knowledge among peers, which might accumulate the training error, we use temporal mean models [22] of each peer as the peer mean teacher for peer collaborative distillation.\n\nWe denote the weights of the shared low-level layers as \u03b8 l and the weights of the separated high-level layers of the j-th peer as \u03b8 h,j . At the g-th global training step 1 , the j-th peer mean teacher {\u03b8 t l,g , \u03b8 t h,j,g } is formulated as: \u03b8 t l,g = \u03c6(g)\u00b7\u03b8 t l,g\u22121 + (1 \u2212 \u03c6(g))\u00b7\u03b8 l,g \u03b8 t h,j,g = \u03c6(g)\u00b7\u03b8 t h,j,g\u22121 + (1 \u2212 \u03c6(g))\u00b7\u03b8 h,j,g\n\nwhere \u03b8 t l,g are the weights of the shared low-level layers of the peer mean teachers, \u03b8 t h,j,g are the weights of the separated high-level layers of the j-th peer mean teacher, \u03c6(g) is a smoothing coefficient function defined as:\n\u03c6(g) = min(1 \u2212 1 g , \u03b2)(8)\nwhere \u03b2 is the smoothing coefficient hyper-parameter. Note that, the additional classifier of the peer ensemble teacher is also aggregated for the ensemble deployment. We compute the soft predictions p mt j,c of the j-th mean teacher over a class c using Eq. (4) with the output logit z mt l,c of this mean teacher. Thus, the peer mean teacher distillation loss L pm is formulated as: \n\nRemarks. Traditional mean teacher is used for semisupervised/unsupervised learning [22,14,3], which mainly 1 In mini-batch training, g = e\u00b7Batchnum + Batch inx , where Batchnum is the total number of training batches and Batch inx is the index of the current batch. Concatenate features as the peer ensemble teacher 7: Compute the logit z t of the ensemble teacher 8: Compute peer classification loss L p ce (Eq.(2)) 9:\n\nCompute ensemble classification loss L t ce (Eq.(3)) 10:\n\nCompute peer ensemble loss L pe (Eq.(5)) 11: Compute mean teacher loss L pm (Eq.(9)) 12:\n\nUpdate peer models with Eq. (1) 13:\n\nUpdate peer mean teachers with Eq. (7) 14: end for 15: /* Testing */ 16: Deploy with a single target model {\u03b8 t l , \u03b8 t h,1 } 17: Deploy with an ensemble model {\u03b8 t l , \u03b8 t h,j } m j=1 enforces the distance between the model predictions to be close. In our work, we integrate it into a multi-branch network for the peer collaborative distillation during online knowledge distillation by aligning the soft distributions between the peer and its counterpart's mean teacher. Compared with mutual distillation among peers [28] which might accumulate the training error, averaging model weights temporally over training epochs enables the peer mean teacher to stabilise soft predictions for improving peer collaboration (see Experiment for further analysis). Besides, as the input to each peer/mean teacher is randomly transformed, L pm helps each peer to learn richer knowledge, resulting in the improvement of online distillation.\n\nSummary. As summarised in Algorithm 1, the proposed PCL follows a one-stage end-to-end training strategy without pre-training a high-capacity teacher model. With the peer ensemble teacher and peer mean teachers in a multibranch network, peers collaborate to improve the quality of online knowledge distillation. In test, we use a single peer mean teacher model as the target model (PCL) without adding extra inference cost. Besides, the ensemble peer mean teachers can also form a high-capacity teacher model (PCL-E) for deployment. Implementation Details. We implemented the proposed PCL with a variety of backbone network architectures, including ResNet [5], VGG [20], DenseNet [9], WRN [26], and ResNeXt [24]. Following [13], the last block and the classifier layer of each backbone network were separated (on ImageNet, the last two blocks were separated), while the other low-level layers were shared. We set m = 3, so there are three peers in the multi-branch network. For fair comparison with the alternative methods, we applied standard random crop and horizontal flip for the random augmentation to generate counterparts of inputs, but other augmentation approaches [12] are applicable. We used SGD as the optimiser with Nesterov momentum 0.9 and weight decay 5e-4. We trained the network for Epoch max = 300 epochs on CIFAR-10/100 and 90 epochs on ImageNet. The initial learning rate was set to 0.1 and dropped to {0.01, 0.001} at {150, 225} epochs on CIFAR-10/100 and at {30, 60} epochs on ImageNet. We empirically set the mini-batch size as 128, T = 3 to generate soft predictions, \u03b1 = 80 epochs for ramp-up weighting, \u03b2 = 0.999 to learn temporal mean models, \u03bb = 1.0 for CIFAR-10/100 and \u03bb = 0.1 for ImageNet. We reported the average results with standard deviation over 3 runs.\n\n\nComparison with the State-of-the-Arts\n\nCompetitors. We compared the proposed PCL with the backbone network (Baseline) and five state-of-the-art online knowledge distillation methods (DML [28], CL [21], ONE [13], FFL-S [10], OKDDip [1]). For fair comparison, we employed three-branch networks (the low-level layers are shared) in ONE, CL, FFL-S, OKDDip and PCL, and used three parallel sub-networks in DML. Results. As shown in Table 1 and Table 2, the proposed PCL improves the performance of various backbone networks (baseline) by approximately 1% and 2% on CIFAR-10 and CIFAR-100, respectively. This shows the effectiveness of PCL for improving the generalisation performance of various backbone networks in online distillation. On CIFAR-10 and CIFAR-100, PCL achieves the best top-1 error rates compared with the state-of-the-art online distillation methods. For example, on CIFAR-10, PCL improves the state-of-the-arts by approximately 0.13% and 0.34% with ResNet-32 and ResNet-110, respectively; Whilst on CIFAR-100, PCL improves the state-of-the-arts by about 0.65% and 1.15% with ResNet-32 and ResNet-110, respectively. These improvements attribute to the integration of the peer mean teacher and the online peer ensemble teacher into a unified framework. When extended to the largescale ImageNet benchmark, as shown in Table 3, PCL improves the baseline by approximately 0.9% with ResNet-18. Compared with the state-of-the-art alternative methods, PCL still achieves the best top-1 error rate (29.58%\u00b10.13% with ResNet-18), which verifies the effectiveness of PCL on  Table 4 shows the evaluation on the component effectiveness of PCL. We can observe that: (1) With all components, PCL (full model) achieves the best performance, which shows the effectiveness of integrating of peer ensemble teacher and peer mean teacher into a unified framework for online distillation. (2) Backbone+L p ce +L t ce +L pe significantly improves the performance of the Backbone by approximately 2.6%, which verifies the effectiveness of the peer ensemble teacher. (3) PCL (full model) improves Backbone+L p ce +L t ce +L pe by about 1.1%, which indicates the effectiveness of the peer mean teacher. (4) Replacing P.E. or P.M. with some contemporary variants leads to performance degradation, which demonstrates the superiority of the proposed PCL. Furthermore, from Fig. 3, we can observe that PCL with all components (red line) gets better generalisation capability. Interestingly, the test top-1 error rate (red line) of PCL (full model) drops rapidly during 0 to 50 epochs, and then gradually reaches to the optimal value; In contrast, other test lines fluctuate dramatically, especially during 0 to 225 epochs. This shows the importance of the peer mean teacher for alleviating the accumulation of error among peers to stabilise online knowledge distillation.    \n\n\nComponent Effectiveness Evaluation\n\n\nEnsemble Effectiveness Evaluation\n\nWe compare the proposed ensemble PCL-E with three alternative online ensembles: ONE-E [13], FFL (FFL-S with fused ensembles) [10], and OKDDip-E [1]. As shown in Table 5, PCL-E improves the state-of-the-arts by about 0.35% and 0.61% on CIFAR-10 and CIFAR-100, respectively. This validates the effectiveness of the proposed peer online ensembling and collaboration. Besides, from Table 5, we can see that compared with ONE-E, the alternative method with the fewest model parameters, although PCL-E achieves significantly better performance, it only increases the number of model parameters by 0.01M and 0.08M with ResNet-110 on CIFAR-10 and CIFAR-100, respectively.\n\n\nPeer Variance for Online Ensembling Analysis\n\nIn Fig. 4, we further analyse the peer (branch) variance for online ensembling over the training epochs. Here, we compute the average Euclidean distance between the pre-  Here, we use top-1 accuracy for better visualisation. dictions of two branches as the branch diversity and use the average diversity of m branches as the branch variance. From Fig. 4, we can observe that: (1) From 0 to 150 epochs, the top-1 accuracy of PCL-E soars to a high level outperforming the alternative methods, and meanwhile, the branch variance of PCL (PCL-BranVar) is larger than the alternatives. This indicates that at the early stage, although generalisation capability of the model is poor, each branch in PCL collaborates to facilitate online ensembling with richer knowledge. (2) From 150 to 300 epochs, the top-1 accuracy of PCL-E is still better than the alternatives, whilst the branch variance of PCL becomes smaller than the alternatives. The main reason is that at this stage, the generalisation capabilities of temporal mean models of peers (branches) have been improved to a high level with accurate and similar prediction. In other words, since the accumulation of training error among peers has been significantly alleviated, each branch becomes stable and gets close generalisation capability, resulting in a stronger ensemble model (as shown in Table 5) and a more generalised target model (as shown in Table 2).\n\n\nFurther Analysis and Discussion\n\nComparison with Two-Stage Distillation. We compare the PCL with the traditional two-stage knowledge distillation (KD) [6] in Table 6. We can see that although PCL doesn't pretrain a high-capacity teacher model (e.g. ResNet-110), it still achieves better performance than the two-stage KD. This attributes to the integration of online ensembling and network collaboration into a unified framework for on- line distillation. Branch Number. As shown in Fig. 5, the performance of PCL improves when more peers/branches are exploited. Interestingly, the performance of PCL with two branches is already better than the state-of-the-art alternatives with three branches, which further shows the superiority of PCL. Input Augmentation. As shown in Fig. 6, without using multiple input augmentation (PCL w/o InAu), the performance of PCL slightly decreases by about 0.5%, but it still achieves the state-of-the-art performance. This further verifies the effectiveness of the model formulation in PCL.\n\n\nConclusion\n\nIn this work, we propose a Peer Collaborative Learning (PCL) method for online knowledge distillation. It facilitates the collaboration among the peers in a multi-branch network by exploiting the peer feature concatenation as the high-capacity ensembling teacher and the peer temporal average models as the peer mean teachers. Doing so allows to improve the quality of online knowledge distillation in a one-stage end-to-end trainable fashion. Extensive experiments with a variety of backbone network architectures show the superiority of the proposed method over the stateof-the-art alternative methods on CIFAR-10, CIFAR-100 and ImageNet. In-depth ablation analyses further verify the effectiveness of the components in the proposed PCL.\n\nFigure 1 .\n1The diagrams of four online knowledge distillation mechanisms. (a) Collaborative learning. (b) Mutual learning. (c) Online ensembling. (d) Peer collaborative learning (Proposed). Our method integrates two types of peer collaborations (i.e. peer ensemble teacher and peer mean teacher) into a unified framework to improve the quality of online distillation.\n\n\nInitialisation: Randomly initialise model parameters3:  for e = 0\u2192Epoch max do /* Mini-Batch SGD *\n\nFigure 3 .\n3Component comparison with ResNet-110 during training and testing on CIFAR-100.\n\nFigure 4 .\n4Peer variance for online ensembling analysis with ResNet-110 on CIFAR-100. '-BranVar': the branch variance.\n\nFigure 5 .Figure 6 .\n56Evaluating PCL with different number of branches using ResNet-110 on CIFAR-100. Evaluating the impact of multiple input augmentation for PCL with ResNet-110 on CIFAR-100.\n\nTable 1 .\n1Comparisons with the state-of-the-arts on CIFAR-10. Top-1 error rates (%). ResNeXt-29-2\u00d764d 18.94\u00b10.01 18.41\u00b10.07 18.60\u00b10.25 20.18\u00b10.33 18.50\u00b10.11 20.57\u00b10.43 17.38\u00b10.23 4. Experiment 4.1. Datasets and Settings Datasets. We used three image classification benchmarks for evaluation: (1) CIFAR-10 [11] contains 60000 images in 10 classes, with 5000 training images and 1000 test images per class. (2) CIFAR-100 [11] consists of 60000 images in 100 classes, with 500 training images and 100 test images per class. (3) ImageNet ILSVRC 2012 [19] contains 1.2 million training images and 50000 validation images in 1000 classes.Network \nDML [28] \nCL [21] \nONE [13] FFL-S [10] OKDDip [1] \nBaseline \nPCL(ours) \nResNet-32 \n6.06\u00b10.07 5.98\u00b10.28 5.80\u00b10.12 5.99\u00b10.11 \n5.83\u00b10.15 \n6.74\u00b10.15 5.67\u00b10.12 \nResNet-110 \n5.47\u00b10.25 4.81\u00b10.11 4.84\u00b10.30 5.28\u00b10.06 \n4.86\u00b10.10 \n5.01\u00b10.10 4.47\u00b10.16 \nVGG-16 \n5.87\u00b10.07 5.86\u00b10.15 5.86\u00b10.23 6.78\u00b10.08 \n6.02\u00b10.06 \n6.04\u00b10.13 5.26\u00b10.02 \nDenseNet-40-12 \n6.41\u00b10.26 6.95\u00b10.25 6.92\u00b10.21 6.72\u00b10.16 \n7.36\u00b10.22 \n6.81\u00b10.02 5.87\u00b10.13 \nWRN-20-8 \n4.80\u00b10.13 5.41\u00b10.08 5.30\u00b10.14 5.28\u00b10.13 \n5.17\u00b10.15 \n5.32\u00b10.01 4.58\u00b10.04 \nResNeXt-29-2\u00d764d 4.46\u00b10.16 4.45\u00b10.18 4.27\u00b10.10 4.67\u00b10.04 \n4.34\u00b10.02 \n4.72\u00b10.03 3.93\u00b10.09 \n\nTable 2. Comparisons with the state-of-the-arts on CIFAR-100. Top-1 error rates (%). \n\nNetwork \nDML [28] \nCL [21] \nONE [13] \nFFL-S [10] OKDDip [1] \nBaseline \nPCL(ours) \nResNet-32 \n26.32\u00b10.14 27.67\u00b10.46 26.21\u00b10.41 27.82\u00b10.11 26.75\u00b10.38 28.72\u00b10.19 25.86\u00b10.16 \nResNet-110 \n22.14\u00b10.50 21.17\u00b10.58 21.60\u00b10.36 22.78\u00b10.41 21.46\u00b10.26 23.79\u00b10.57 20.02\u00b10.55 \nVGG-16 \n24.48\u00b10.10 25.67\u00b10.08 25.63\u00b10.39 29.13\u00b10.99 25.32\u00b10.05 25.68\u00b10.19 23.11\u00b10.25 \nDenseNet-40-12 \n26.94\u00b10.31 28.55\u00b10.34 28.40\u00b10.38 28.75\u00b10.35 28.77\u00b10.14 28.97\u00b10.15 26.91\u00b10.16 \nWRN-20-8 \n20.23\u00b10.07 20.60\u00b10.12 20.90\u00b10.39 21.78\u00b10.14 21.17\u00b10.06 21.97\u00b10.40 19.49\u00b10.49 \n\n\nTable 3 .Table 4 .\n34Comparisons with the state-of-the-arts on ImageNet. Top-1 error rates (%). 18\u00b10.08 29.96\u00b10.05 29.82\u00b10.13 31.15\u00b10.07 30.07\u00b10.06 30.49\u00b10.14 29.58\u00b10.13 Component effectiveness evaluation with ResNet-110 on CIFAR-100. Top-1 error rates (%). P.E.: Peer Ensemble teacher. P.M.: Peer Mean teacher.Network \nDML [28] \nCL [21] \nONE [13] \nFFL-S [10] OKDDip [1] \nBaseline \nPCL(ours) \nResNet-18 30.Component \nCIFAR-100 \nBackbone \n23.79\u00b10.57 \nProposed \nBackbone+L p \n\nce \n\n23.56\u00b10.50 \nBackbone+L p \nce +L t \n\nce \n\n23.48\u00b10.99 \nBackbone+L p \nce +L t \nce +L pe \n21.19\u00b10.62 \nBackbone+L p \nce +L t \nce +L pe +L pm (full model) 20.02\u00b10.55 \nVariant \nP.E. + Mutual Distillation [28] \n21.09\u00b10.18 \nP.E. + Traditional Mean Model [22] \n26.80\u00b10.35 \nONE [13] + P.M. \n20.43\u00b10.71 \nP.E. + P.M. (full model) \n20.02\u00b10.55 \n\nthe large-scale benchmark. \nDiscussion. These results validate the performance advan-\ntages of PCL for online knowledge distillation over the \nstate-of-the-art alternatives. Besides, since we only use a \ntemporal mean model of a peer for deployment, which has \nthe identical number of parameters to the backbone net-\nwork, our method doesn't require extra inference cost for \ndeployment. \n\n\n\n\n\u2112pm-Test PCL w/o \u2112pm \u2112pe-Train PCL w/o \u2112pm \u2112pe-Test PCL w/o \u2112pm \u2112pe \u2112tce-Train PCL w/o \u2112pm \u2112pe \u2112tce-TestBase-Train \nBase-Test \nPCL-Train \nPCL-Test \nPCL w/o \u2112pm-Train \nPCL w/o (Top-1 error rate %) \n\n(epochs) \n\n\n\nTable 5 .\n5Ensemble effectiveness evaluation with ResNet-110 on CIFAR-10/100. Top-1 error rates (%) and number of model parameters are reported. 75\u00b10.27 2.89M 20.10\u00b10.24 2.96M FFL (fused) 4.99\u00b10.07 3.10M 21.78\u00b10.28 3.19M OKDDip-E 4.79\u00b10.12 2.91M 20.93\u00b10.57 2.98M PCL-E(ours) 4.42\u00b10.12 2.90M 19.49\u00b10.49 3.04MMethod \nCIFAR-10 \nCIFAR-100 \nTop-1 \nParam. \nTop-1 \nParam. \nONE-E \n4.\n\nTable 6 .\n6Comparison with two-stage distillation with ResNet-32 on CIFAR-10/100. Top-1 error rates (%). \u2020 : Use ResNet-110 as the teacher model. CIFAR-100 28.72\u00b10.19 26.23\u00b10.21 25.86\u00b10.16Dataset \nBaseline \nKD  \u2020 \nPCL \nCIFAR-10 \n6.74\u00b10.15 \n5.82\u00b10.12 \n5.67\u00b10.12 \n\n\nOnline knowledge distillation with diverse peers. Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, Chun Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceDefang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online knowledge distillation with diverse peers. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.\n\nPerson reidentification by deep learning multi-scale representations. Yanbei Chen, Xiatian Zhu, Shaogang Gong, Proceedings of the IEEE International Conference on Computer Vision Workshop. the IEEE International Conference on Computer Vision WorkshopYanbei Chen, Xiatian Zhu, and Shaogang Gong. Person re- identification by deep learning multi-scale representations. In Proceedings of the IEEE International Conference on Computer Vision Workshop, pages 2590-2600, 2017.\n\nMutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. Yixiao Ge, Dapeng Chen, Hongsheng Li, International Conference on Learning Representations. Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean- teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. In International Con- ference on Learning Representations, 2020.\n\nNeural network ensembles. Lars Kai Hansen, Peter Salamon, IEEE Transactions on Pattern Analysis and Machine Intelligence. 1210Lars Kai Hansen and Peter Salamon. Neural network ensem- bles. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(10):993-1001, 1990.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nDualnet: Learn complementary features for image recognition. Saihui Hou, Xu Liu, Zilei Wang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionSaihui Hou, Xu Liu, and Zilei Wang. Dualnet: Learn com- plementary features for image recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 502-510, 2017.\n\nSnapshot ensembles: Train 1, get m for free. Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, Kilian Q Weinberger, International Conference on Learning Representations. Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. In International Conference on Learning Representations, 2017.\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil- ian Q Weinberger. Densely connected convolutional net- works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4700-4708, 2017.\n\nFeature fusion for online mutual knowledge distillation. Jangho Kim, Minsung Hyun, Inseop Chung, Nojun Kwak, arXiv:1904.09058arXiv preprintJangho Kim, Minsung Hyun, Inseop Chung, and Nojun Kwak. Feature fusion for online mutual knowledge distil- lation. arXiv preprint arXiv:1904.09058, 2019.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Technical reportAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nTemporal ensembling for semisupervised learning. Samuli Laine, Timo Aila, International Conference on Learning Representations. Samuli Laine and Timo Aila. Temporal ensembling for semi- supervised learning. In International Conference on Learn- ing Representations, 2017.\n\nKnowledge distillation by on-the-fly native ensemble. Xu Lan, Xiatian Zhu, Shaogang Gong, Advances in Neural Information Processing Systems. Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge distillation by on-the-fly native ensemble. In Advances in Neural Information Processing Systems, pages 7517-7527, 2018.\n\nSemi-supervised semantic segmentation with high-and lowlevel consistency. Sudhanshu Mittal, Maxim Tatarchenko, Thomas Brox, IEEE Transactions on Pattern Analysis and Machine Intelligence. Sudhanshu Mittal, Maxim Tatarchenko, and Thomas Brox. Semi-supervised semantic segmentation with high-and low- level consistency. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n\nBoosted convolutional neural networks. Mohammad Moghimi, J Serge, Belongie, J Mohammad, Jian Saberian, Nuno Yang, Li-Jia Vasconcelos, Li, British Machine Vision Conference. Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno Vasconcelos, and Li-Jia Li. Boosted convolutional neural networks. In British Machine Vision Conference, 2016.\n\nRelational knowledge distillation. Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Rela- tional knowledge distillation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3967-3976, 2019.\n\nTowards understanding knowledge distillation. Mary Phuong, Christoph Lampert, International Conference on Machine Learning. Mary Phuong and Christoph Lampert. Towards understand- ing knowledge distillation. In International Conference on Machine Learning, pages 5142-5151, 2019.\n\nFitnets: Hints for thin deep nets. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, International Conference on Learning Representations. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fit- nets: Hints for thin deep nets. In International Conference on Learning Representations, 2015.\n\nImageNet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International Journal of Computer Vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, International Conference on Learning Representations. Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. In Inter- national Conference on Learning Representations, 2015.\n\nCollaborative learning for deep neural networks. Guocong Song, Wei Chai, Advances in Neural Information Processing Systems. Guocong Song and Wei Chai. Collaborative learning for deep neural networks. In Advances in Neural Information Processing Systems, pages 1832-1841, 2018.\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, Advances in Neural Information Processing Systems. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, pages 1195-1204, 2017.\n\nSimilarity-preserving knowledge distillation. Frederick Tung, Greg Mori, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionFrederick Tung and Greg Mori. Similarity-preserving knowl- edge distillation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1365-1374, 2019.\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017.\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4133-4141, 2017.\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, British Machine Vision Conference. Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In British Machine Vision Conference, 2016.\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Sergey Zagoruyko, Nikos Komodakis, International Conference on Learning Representations. Sergey Zagoruyko and Nikos Komodakis. Paying more at- tention to attention: Improving the performance of convolu- tional neural networks via attention transfer. In International Conference on Learning Representations, 2017.\n\nDeep mutual learning. Ying Zhang, Tao Xiang, Timothy M Hospedales, Huchuan Lu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYing Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4320-4328, 2018.\n\nEnsembling neural networks: many could be better than all. Zhi-Hua Zhou, Jianxin Wu, Wei Tang, Artificial Intelligence. 1371-2Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neu- ral networks: many could be better than all. Artificial Intel- ligence, 137(1-2):239-263, 2002.\n", "annotations": {"author": "[{\"end\":128,\"start\":65},{\"end\":195,\"start\":129}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":71},{\"end\":142,\"start\":138}]", "author_first_name": "[{\"end\":70,\"start\":65},{\"end\":137,\"start\":129}]", "author_affiliation": "[{\"end\":127,\"start\":95},{\"end\":194,\"start\":162}]", "title": "[{\"end\":62,\"start\":1},{\"end\":257,\"start\":196}]", "venue": null, "abstract": "[{\"end\":1922,\"start\":259}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2133,\"start\":2130},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2136,\"start\":2133},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2139,\"start\":2136},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2376,\"start\":2373},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2403,\"start\":2399},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2671,\"start\":2668},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2674,\"start\":2671},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2677,\"start\":2674},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2715,\"start\":2712},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2750,\"start\":2746},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3148,\"start\":3144},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3151,\"start\":3148},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3153,\"start\":3151},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3461,\"start\":3457},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3530,\"start\":3526},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3597,\"start\":3593},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4705,\"start\":4702},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5023,\"start\":5019},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5598,\"start\":5594},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6507,\"start\":6503},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6523,\"start\":6519},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6541,\"start\":6537},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6586,\"start\":6583},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6596,\"start\":6592},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6610,\"start\":6607},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6620,\"start\":6616},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6637,\"start\":6633},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6852,\"start\":6849},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6976,\"start\":6973},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7440,\"start\":7436},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7466,\"start\":7462},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7487,\"start\":7483},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7514,\"start\":7510},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7545,\"start\":7541},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8112,\"start\":8108},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8308,\"start\":8304},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8704,\"start\":8700},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9051,\"start\":9047},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10038,\"start\":10035},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10041,\"start\":10038},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10044,\"start\":10041},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10392,\"start\":10389},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10542,\"start\":10538},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11134,\"start\":11130},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11143,\"start\":11139},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13692,\"start\":13688},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13694,\"start\":13692},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15092,\"start\":15088},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16222,\"start\":16219},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16596,\"start\":16592},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16865,\"start\":16862},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16868,\"start\":16865},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16870,\"start\":16868},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17462,\"start\":17458},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17494,\"start\":17490},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17877,\"start\":17873},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19027,\"start\":19023},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19030,\"start\":19027},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19032,\"start\":19030},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19048,\"start\":19047},{\"end\":19258,\"start\":19256},{\"end\":19307,\"start\":19305},{\"end\":19463,\"start\":19460},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20068,\"start\":20064},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21134,\"start\":21131},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21144,\"start\":21140},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21158,\"start\":21155},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21168,\"start\":21164},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21186,\"start\":21182},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21202,\"start\":21198},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21653,\"start\":21649},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22459,\"start\":22455},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22468,\"start\":22464},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22478,\"start\":22474},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22490,\"start\":22486},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22502,\"start\":22499},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25292,\"start\":25288},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25331,\"start\":25327},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25349,\"start\":25346},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27483,\"start\":27480}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29477,\"start\":29108},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29578,\"start\":29478},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29670,\"start\":29579},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29791,\"start\":29671},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29986,\"start\":29792},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31831,\"start\":29987},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33034,\"start\":31832},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33246,\"start\":33035},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33623,\"start\":33247},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33887,\"start\":33624}]", "paragraph": "[{\"end\":2462,\"start\":1938},{\"end\":3086,\"start\":2464},{\"end\":4001,\"start\":3088},{\"end\":5401,\"start\":4003},{\"end\":6467,\"start\":5444},{\"end\":6797,\"start\":6469},{\"end\":9916,\"start\":6814},{\"end\":11621,\"start\":9918},{\"end\":12163,\"start\":11673},{\"end\":12932,\"start\":12165},{\"end\":13370,\"start\":12970},{\"end\":13475,\"start\":13396},{\"end\":14511,\"start\":13550},{\"end\":15572,\"start\":14694},{\"end\":15944,\"start\":15698},{\"end\":16084,\"start\":15946},{\"end\":16459,\"start\":16184},{\"end\":16608,\"start\":16516},{\"end\":16779,\"start\":16669},{\"end\":17266,\"start\":16781},{\"end\":17952,\"start\":17288},{\"end\":18291,\"start\":17954},{\"end\":18525,\"start\":18293},{\"end\":18938,\"start\":18553},{\"end\":19359,\"start\":18940},{\"end\":19417,\"start\":19361},{\"end\":19507,\"start\":19419},{\"end\":19544,\"start\":19509},{\"end\":20473,\"start\":19546},{\"end\":22265,\"start\":20475},{\"end\":25127,\"start\":22307},{\"end\":25865,\"start\":25202},{\"end\":27326,\"start\":25914},{\"end\":28353,\"start\":27362},{\"end\":29107,\"start\":28368}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5443,\"start\":5402},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12969,\"start\":12933},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13549,\"start\":13476},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14668,\"start\":14512},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14693,\"start\":14668},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15640,\"start\":15573},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15697,\"start\":15640},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16183,\"start\":16085},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16515,\"start\":16460},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16668,\"start\":16609},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18552,\"start\":18526}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22714,\"start\":22695},{\"end\":23603,\"start\":23596},{\"end\":23852,\"start\":23845},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25370,\"start\":25363},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27266,\"start\":27259},{\"end\":27324,\"start\":27317},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27494,\"start\":27487}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1936,\"start\":1924},{\"attributes\":{\"n\":\"2.\"},\"end\":6812,\"start\":6800},{\"attributes\":{\"n\":\"3.\"},\"end\":11651,\"start\":11624},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11671,\"start\":11654},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13394,\"start\":13373},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17286,\"start\":17269},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22305,\"start\":22268},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25164,\"start\":25130},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25200,\"start\":25167},{\"attributes\":{\"n\":\"4.5.\"},\"end\":25912,\"start\":25868},{\"attributes\":{\"n\":\"4.6.\"},\"end\":27360,\"start\":27329},{\"attributes\":{\"n\":\"5.\"},\"end\":28366,\"start\":28356},{\"end\":29119,\"start\":29109},{\"end\":29590,\"start\":29580},{\"end\":29682,\"start\":29672},{\"end\":29813,\"start\":29793},{\"end\":29997,\"start\":29988},{\"end\":31851,\"start\":31833},{\"end\":33257,\"start\":33248},{\"end\":33634,\"start\":33625}]", "table": "[{\"end\":31831,\"start\":30621},{\"end\":33034,\"start\":32144},{\"end\":33246,\"start\":33141},{\"end\":33623,\"start\":33555},{\"end\":33887,\"start\":33813}]", "figure_caption": "[{\"end\":29477,\"start\":29121},{\"end\":29578,\"start\":29480},{\"end\":29670,\"start\":29592},{\"end\":29791,\"start\":29684},{\"end\":29986,\"start\":29816},{\"end\":30621,\"start\":29999},{\"end\":32144,\"start\":31854},{\"end\":33141,\"start\":33037},{\"end\":33555,\"start\":33259},{\"end\":33813,\"start\":33636}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3472,\"start\":3462},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3541,\"start\":3531},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3608,\"start\":3598},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4133,\"start\":4124},{\"end\":11757,\"start\":11751},{\"end\":12183,\"start\":12177},{\"end\":14009,\"start\":14001},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24632,\"start\":24626},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25923,\"start\":25917},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26267,\"start\":26256},{\"end\":27818,\"start\":27812},{\"end\":28108,\"start\":28102}]", "bib_author_first_name": "[{\"end\":33945,\"start\":33939},{\"end\":33961,\"start\":33952},{\"end\":33970,\"start\":33967},{\"end\":33980,\"start\":33977},{\"end\":33991,\"start\":33987},{\"end\":34369,\"start\":34363},{\"end\":34383,\"start\":34376},{\"end\":34397,\"start\":34389},{\"end\":34878,\"start\":34872},{\"end\":34889,\"start\":34883},{\"end\":34905,\"start\":34896},{\"end\":35211,\"start\":35207},{\"end\":35215,\"start\":35212},{\"end\":35229,\"start\":35224},{\"end\":35511,\"start\":35504},{\"end\":35523,\"start\":35516},{\"end\":35539,\"start\":35531},{\"end\":35549,\"start\":35545},{\"end\":35911,\"start\":35903},{\"end\":35925,\"start\":35920},{\"end\":35939,\"start\":35935},{\"end\":36224,\"start\":36218},{\"end\":36232,\"start\":36230},{\"end\":36243,\"start\":36238},{\"end\":36612,\"start\":36609},{\"end\":36626,\"start\":36620},{\"end\":36636,\"start\":36631},{\"end\":36651,\"start\":36645},{\"end\":36661,\"start\":36657},{\"end\":36663,\"start\":36662},{\"end\":36682,\"start\":36674},{\"end\":36993,\"start\":36990},{\"end\":37007,\"start\":37001},{\"end\":37020,\"start\":37013},{\"end\":37045,\"start\":37037},{\"end\":37486,\"start\":37480},{\"end\":37499,\"start\":37492},{\"end\":37512,\"start\":37506},{\"end\":37525,\"start\":37520},{\"end\":37776,\"start\":37772},{\"end\":37797,\"start\":37789},{\"end\":37994,\"start\":37988},{\"end\":38006,\"start\":38002},{\"end\":38268,\"start\":38266},{\"end\":38281,\"start\":38274},{\"end\":38295,\"start\":38287},{\"end\":38608,\"start\":38599},{\"end\":38622,\"start\":38617},{\"end\":38642,\"start\":38636},{\"end\":38961,\"start\":38953},{\"end\":38972,\"start\":38971},{\"end\":38991,\"start\":38990},{\"end\":39006,\"start\":39002},{\"end\":39021,\"start\":39017},{\"end\":39034,\"start\":39028},{\"end\":39313,\"start\":39307},{\"end\":39326,\"start\":39320},{\"end\":39335,\"start\":39332},{\"end\":39345,\"start\":39340},{\"end\":39733,\"start\":39729},{\"end\":39751,\"start\":39742},{\"end\":40005,\"start\":39998},{\"end\":40021,\"start\":40014},{\"end\":40036,\"start\":40030},{\"end\":40045,\"start\":40037},{\"end\":40060,\"start\":40053},{\"end\":40076,\"start\":40071},{\"end\":40090,\"start\":40084},{\"end\":40414,\"start\":40410},{\"end\":40431,\"start\":40428},{\"end\":40441,\"start\":40438},{\"end\":40454,\"start\":40446},{\"end\":40470,\"start\":40463},{\"end\":40485,\"start\":40481},{\"end\":40497,\"start\":40490},{\"end\":40511,\"start\":40505},{\"end\":40528,\"start\":40522},{\"end\":40544,\"start\":40537},{\"end\":40946,\"start\":40941},{\"end\":40963,\"start\":40957},{\"end\":41258,\"start\":41251},{\"end\":41268,\"start\":41265},{\"end\":41606,\"start\":41601},{\"end\":41623,\"start\":41618},{\"end\":41973,\"start\":41964},{\"end\":41984,\"start\":41980},{\"end\":42355,\"start\":42348},{\"end\":42365,\"start\":42361},{\"end\":42381,\"start\":42376},{\"end\":42397,\"start\":42390},{\"end\":42409,\"start\":42402},{\"end\":42897,\"start\":42892},{\"end\":42910,\"start\":42903},{\"end\":42922,\"start\":42916},{\"end\":42933,\"start\":42928},{\"end\":43368,\"start\":43362},{\"end\":43385,\"start\":43380},{\"end\":43666,\"start\":43660},{\"end\":43683,\"start\":43678},{\"end\":44000,\"start\":43996},{\"end\":44011,\"start\":44008},{\"end\":44026,\"start\":44019},{\"end\":44028,\"start\":44027},{\"end\":44048,\"start\":44041},{\"end\":44451,\"start\":44444},{\"end\":44465,\"start\":44458},{\"end\":44473,\"start\":44470}]", "bib_author_last_name": "[{\"end\":33950,\"start\":33946},{\"end\":33965,\"start\":33962},{\"end\":33975,\"start\":33971},{\"end\":33985,\"start\":33981},{\"end\":33996,\"start\":33992},{\"end\":34374,\"start\":34370},{\"end\":34387,\"start\":34384},{\"end\":34402,\"start\":34398},{\"end\":34881,\"start\":34879},{\"end\":34894,\"start\":34890},{\"end\":34908,\"start\":34906},{\"end\":35222,\"start\":35216},{\"end\":35237,\"start\":35230},{\"end\":35514,\"start\":35512},{\"end\":35529,\"start\":35524},{\"end\":35543,\"start\":35540},{\"end\":35553,\"start\":35550},{\"end\":35918,\"start\":35912},{\"end\":35933,\"start\":35926},{\"end\":35944,\"start\":35940},{\"end\":36228,\"start\":36225},{\"end\":36236,\"start\":36233},{\"end\":36248,\"start\":36244},{\"end\":36618,\"start\":36613},{\"end\":36629,\"start\":36627},{\"end\":36643,\"start\":36637},{\"end\":36655,\"start\":36652},{\"end\":36672,\"start\":36664},{\"end\":36693,\"start\":36683},{\"end\":36999,\"start\":36994},{\"end\":37011,\"start\":37008},{\"end\":37035,\"start\":37021},{\"end\":37056,\"start\":37046},{\"end\":37490,\"start\":37487},{\"end\":37504,\"start\":37500},{\"end\":37518,\"start\":37513},{\"end\":37530,\"start\":37526},{\"end\":37787,\"start\":37777},{\"end\":37804,\"start\":37798},{\"end\":38000,\"start\":37995},{\"end\":38011,\"start\":38007},{\"end\":38272,\"start\":38269},{\"end\":38285,\"start\":38282},{\"end\":38300,\"start\":38296},{\"end\":38615,\"start\":38609},{\"end\":38634,\"start\":38623},{\"end\":38647,\"start\":38643},{\"end\":38969,\"start\":38962},{\"end\":38978,\"start\":38973},{\"end\":38988,\"start\":38980},{\"end\":39000,\"start\":38992},{\"end\":39015,\"start\":39007},{\"end\":39026,\"start\":39022},{\"end\":39046,\"start\":39035},{\"end\":39050,\"start\":39048},{\"end\":39318,\"start\":39314},{\"end\":39330,\"start\":39327},{\"end\":39338,\"start\":39336},{\"end\":39349,\"start\":39346},{\"end\":39740,\"start\":39734},{\"end\":39759,\"start\":39752},{\"end\":40012,\"start\":40006},{\"end\":40028,\"start\":40022},{\"end\":40051,\"start\":40046},{\"end\":40069,\"start\":40061},{\"end\":40082,\"start\":40077},{\"end\":40097,\"start\":40091},{\"end\":40426,\"start\":40415},{\"end\":40436,\"start\":40432},{\"end\":40444,\"start\":40442},{\"end\":40461,\"start\":40455},{\"end\":40479,\"start\":40471},{\"end\":40488,\"start\":40486},{\"end\":40503,\"start\":40498},{\"end\":40520,\"start\":40512},{\"end\":40535,\"start\":40529},{\"end\":40554,\"start\":40545},{\"end\":40955,\"start\":40947},{\"end\":40973,\"start\":40964},{\"end\":41263,\"start\":41259},{\"end\":41273,\"start\":41269},{\"end\":41616,\"start\":41607},{\"end\":41631,\"start\":41624},{\"end\":41978,\"start\":41974},{\"end\":41989,\"start\":41985},{\"end\":42359,\"start\":42356},{\"end\":42374,\"start\":42366},{\"end\":42388,\"start\":42382},{\"end\":42400,\"start\":42398},{\"end\":42412,\"start\":42410},{\"end\":42901,\"start\":42898},{\"end\":42914,\"start\":42911},{\"end\":42926,\"start\":42923},{\"end\":42937,\"start\":42934},{\"end\":43378,\"start\":43369},{\"end\":43395,\"start\":43386},{\"end\":43676,\"start\":43667},{\"end\":43693,\"start\":43684},{\"end\":44006,\"start\":44001},{\"end\":44017,\"start\":44012},{\"end\":44039,\"start\":44029},{\"end\":44051,\"start\":44049},{\"end\":44456,\"start\":44452},{\"end\":44468,\"start\":44466},{\"end\":44478,\"start\":44474}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":208526905},\"end\":34291,\"start\":33889},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4729614},\"end\":34763,\"start\":34293},{\"attributes\":{\"id\":\"b2\"},\"end\":35179,\"start\":34765},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":993561},\"end\":35456,\"start\":35181},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206594692},\"end\":35901,\"start\":35458},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b5\"},\"end\":36155,\"start\":35903},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":27384186},\"end\":36562,\"start\":36157},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6820006},\"end\":36946,\"start\":36564},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9433631},\"end\":37421,\"start\":36948},{\"attributes\":{\"doi\":\"arXiv:1904.09058\",\"id\":\"b9\"},\"end\":37715,\"start\":37423},{\"attributes\":{\"id\":\"b10\"},\"end\":37937,\"start\":37717},{\"attributes\":{\"id\":\"b11\"},\"end\":38210,\"start\":37939},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":48352434},\"end\":38523,\"start\":38212},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":201058657},\"end\":38912,\"start\":38525},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6952305},\"end\":39270,\"start\":38914},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":131765296},\"end\":39681,\"start\":39272},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":174800711},\"end\":39961,\"start\":39683},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2723173},\"end\":40357,\"start\":39963},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2930547},\"end\":40871,\"start\":40359},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14124313},\"end\":41200,\"start\":40873},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":44119099},\"end\":41478,\"start\":41202},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2759724},\"end\":41916,\"start\":41480},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":198179476},\"end\":42284,\"start\":41918},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8485068},\"end\":42791,\"start\":42286},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206596723},\"end\":43336,\"start\":42793},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15276198},\"end\":43539,\"start\":43338},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":829159},\"end\":43972,\"start\":43541},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":26071966},\"end\":44383,\"start\":43974},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":15455464},\"end\":44662,\"start\":44385}]", "bib_title": "[{\"end\":33937,\"start\":33889},{\"end\":34361,\"start\":34293},{\"end\":34870,\"start\":34765},{\"end\":35205,\"start\":35181},{\"end\":35502,\"start\":35458},{\"end\":36216,\"start\":36157},{\"end\":36607,\"start\":36564},{\"end\":36988,\"start\":36948},{\"end\":37986,\"start\":37939},{\"end\":38264,\"start\":38212},{\"end\":38597,\"start\":38525},{\"end\":38951,\"start\":38914},{\"end\":39305,\"start\":39272},{\"end\":39727,\"start\":39683},{\"end\":39996,\"start\":39963},{\"end\":40408,\"start\":40359},{\"end\":40939,\"start\":40873},{\"end\":41249,\"start\":41202},{\"end\":41599,\"start\":41480},{\"end\":41962,\"start\":41918},{\"end\":42346,\"start\":42286},{\"end\":42890,\"start\":42793},{\"end\":43360,\"start\":43338},{\"end\":43658,\"start\":43541},{\"end\":43994,\"start\":43974},{\"end\":44442,\"start\":44385}]", "bib_author": "[{\"end\":33952,\"start\":33939},{\"end\":33967,\"start\":33952},{\"end\":33977,\"start\":33967},{\"end\":33987,\"start\":33977},{\"end\":33998,\"start\":33987},{\"end\":34376,\"start\":34363},{\"end\":34389,\"start\":34376},{\"end\":34404,\"start\":34389},{\"end\":34883,\"start\":34872},{\"end\":34896,\"start\":34883},{\"end\":34910,\"start\":34896},{\"end\":35224,\"start\":35207},{\"end\":35239,\"start\":35224},{\"end\":35516,\"start\":35504},{\"end\":35531,\"start\":35516},{\"end\":35545,\"start\":35531},{\"end\":35555,\"start\":35545},{\"end\":35920,\"start\":35903},{\"end\":35935,\"start\":35920},{\"end\":35946,\"start\":35935},{\"end\":36230,\"start\":36218},{\"end\":36238,\"start\":36230},{\"end\":36250,\"start\":36238},{\"end\":36620,\"start\":36609},{\"end\":36631,\"start\":36620},{\"end\":36645,\"start\":36631},{\"end\":36657,\"start\":36645},{\"end\":36674,\"start\":36657},{\"end\":36695,\"start\":36674},{\"end\":37001,\"start\":36990},{\"end\":37013,\"start\":37001},{\"end\":37037,\"start\":37013},{\"end\":37058,\"start\":37037},{\"end\":37492,\"start\":37480},{\"end\":37506,\"start\":37492},{\"end\":37520,\"start\":37506},{\"end\":37532,\"start\":37520},{\"end\":37789,\"start\":37772},{\"end\":37806,\"start\":37789},{\"end\":38002,\"start\":37988},{\"end\":38013,\"start\":38002},{\"end\":38274,\"start\":38266},{\"end\":38287,\"start\":38274},{\"end\":38302,\"start\":38287},{\"end\":38617,\"start\":38599},{\"end\":38636,\"start\":38617},{\"end\":38649,\"start\":38636},{\"end\":38971,\"start\":38953},{\"end\":38980,\"start\":38971},{\"end\":38990,\"start\":38980},{\"end\":39002,\"start\":38990},{\"end\":39017,\"start\":39002},{\"end\":39028,\"start\":39017},{\"end\":39048,\"start\":39028},{\"end\":39052,\"start\":39048},{\"end\":39320,\"start\":39307},{\"end\":39332,\"start\":39320},{\"end\":39340,\"start\":39332},{\"end\":39351,\"start\":39340},{\"end\":39742,\"start\":39729},{\"end\":39761,\"start\":39742},{\"end\":40014,\"start\":39998},{\"end\":40030,\"start\":40014},{\"end\":40053,\"start\":40030},{\"end\":40071,\"start\":40053},{\"end\":40084,\"start\":40071},{\"end\":40099,\"start\":40084},{\"end\":40428,\"start\":40410},{\"end\":40438,\"start\":40428},{\"end\":40446,\"start\":40438},{\"end\":40463,\"start\":40446},{\"end\":40481,\"start\":40463},{\"end\":40490,\"start\":40481},{\"end\":40505,\"start\":40490},{\"end\":40522,\"start\":40505},{\"end\":40537,\"start\":40522},{\"end\":40556,\"start\":40537},{\"end\":40957,\"start\":40941},{\"end\":40975,\"start\":40957},{\"end\":41265,\"start\":41251},{\"end\":41275,\"start\":41265},{\"end\":41618,\"start\":41601},{\"end\":41633,\"start\":41618},{\"end\":41980,\"start\":41964},{\"end\":41991,\"start\":41980},{\"end\":42361,\"start\":42348},{\"end\":42376,\"start\":42361},{\"end\":42390,\"start\":42376},{\"end\":42402,\"start\":42390},{\"end\":42414,\"start\":42402},{\"end\":42903,\"start\":42892},{\"end\":42916,\"start\":42903},{\"end\":42928,\"start\":42916},{\"end\":42939,\"start\":42928},{\"end\":43380,\"start\":43362},{\"end\":43397,\"start\":43380},{\"end\":43678,\"start\":43660},{\"end\":43695,\"start\":43678},{\"end\":44008,\"start\":43996},{\"end\":44019,\"start\":44008},{\"end\":44041,\"start\":44019},{\"end\":44053,\"start\":44041},{\"end\":44458,\"start\":44444},{\"end\":44470,\"start\":44458},{\"end\":44480,\"start\":44470}]", "bib_venue": "[{\"end\":34059,\"start\":33998},{\"end\":34480,\"start\":34404},{\"end\":34962,\"start\":34910},{\"end\":35301,\"start\":35239},{\"end\":35632,\"start\":35555},{\"end\":36006,\"start\":35962},{\"end\":36317,\"start\":36250},{\"end\":36747,\"start\":36695},{\"end\":37135,\"start\":37058},{\"end\":37478,\"start\":37423},{\"end\":37770,\"start\":37717},{\"end\":38065,\"start\":38013},{\"end\":38351,\"start\":38302},{\"end\":38711,\"start\":38649},{\"end\":39085,\"start\":39052},{\"end\":39428,\"start\":39351},{\"end\":39805,\"start\":39761},{\"end\":40151,\"start\":40099},{\"end\":40596,\"start\":40556},{\"end\":41027,\"start\":40975},{\"end\":41324,\"start\":41275},{\"end\":41682,\"start\":41633},{\"end\":42058,\"start\":41991},{\"end\":42491,\"start\":42414},{\"end\":43016,\"start\":42939},{\"end\":43430,\"start\":43397},{\"end\":43747,\"start\":43695},{\"end\":44130,\"start\":44053},{\"end\":44503,\"start\":44480},{\"end\":34107,\"start\":34061},{\"end\":34543,\"start\":34482},{\"end\":35696,\"start\":35634},{\"end\":36371,\"start\":36319},{\"end\":37199,\"start\":37137},{\"end\":39492,\"start\":39430},{\"end\":42112,\"start\":42060},{\"end\":42555,\"start\":42493},{\"end\":43080,\"start\":43018},{\"end\":44194,\"start\":44132}]"}}}, "year": 2023, "month": 12, "day": 17}