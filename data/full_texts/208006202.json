{"id": 208006202, "updated": "2023-10-06 22:46:38.186", "metadata": {"title": "RandAugment: Practical automated data augmentation with a reduced search space", "authors": "[{\"first\":\"Ekin\",\"last\":\"Cubuk\",\"middle\":[\"D.\"]},{\"first\":\"Barret\",\"last\":\"Zoph\",\"middle\":[]},{\"first\":\"Jonathon\",\"last\":\"Shlens\",\"middle\":[]},{\"first\":\"Quoc\",\"last\":\"Le\",\"middle\":[\"V.\"]}]", "venue": "ArXiv", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over baseline augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1909.13719", "mag": "3102631365", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/CubukZS020", "doi": "10.1109/cvprw50498.2020.00359"}}, "content": {"source": {"pdf_hash": "87f6a7c014ce206ac5b57299c07e10667d194b39", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.13719v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1909.13719", "status": "GREEN"}}, "grobid": {"id": "df900464283fa2ac4f5488cf3a977e8c4612291c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/87f6a7c014ce206ac5b57299c07e10667d194b39.txt", "contents": "\nRandAugment: Practical automated data augmentation with a reduced search space\n\n\nEkin D Cubuk cubuk@google.com \nGoogle Research\nBrain Team\n\nBarret Zoph barretzoph@google.com \nGoogle Research\nBrain Team\n\nJonathon Shlens shlens@google.com \nGoogle Research\nBrain Team\n\nQuoc V Le \nGoogle Research\nBrain Team\n\nRandAugment: Practical automated data augmentation with a reduced search space\n\nRecent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over baseline augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online. 1 * Authors contributed equally. 1\n Table 1\n. RandAugment matches or exceeds predictive performance of other augmentation methods with a significantly reduced search space. We report the search space size and the test accuracy achieved for AutoAugment (AA) [5], Fast AutoAugment [25], Population Based Augmentation (PBA) [20] and the proposed RandAugment (RA) on CIFAR-10 [22], SVHN [34], and ImageNet [6] classification tasks. Architectures presented include PyramidNet [15], Wide-ResNet-28-10 [53], ResNet-50 [17], and EfficientNet-B7 [47]. Search space size is reported as the order of magnitude of the number of possible augmentation policies. All accuracies are the percentage on a cross-validated validation or test split. Dash indicates that results are not available.\n\n\nIntroduction\n\nData augmentation is a widely used method for generating additional data to improve machine learning systems, for image classification [43,23,7,54], object detection [13], instance segmentation [10], and speech recognition [21,16,36]. Unfortunately, data augmentation methods require expertise, and manual work to design policies that capture prior knowledge in each domain. This requirement makes it difficult to extend existing data augmentation methods to other applications and domains.\n\nLearning policies for data augmentation has recently emerged as a method to automate the design of augmentation strategies and therefore has the potential to address some weaknesses of traditional data augmentation methods [5,57,20,25]. Training a machine learning model with a learned data augmentation policy may significantly improve accuracy [5], model robustness [32,52,41], and performance on semi-supervised learning [50] for image classification; likewise, for object detection tasks on COCO and PASCAL-VOC [57]. Notably, unlike engineering bet-ter network architectures [59], all of these improvements in predictive performance incur no additional computational cost at inference time.\n\nIn spite of the benefits of learned data augmentation policies, the computational requirements as well as the added complexity of two separate optimization procedures can be prohibitive. The original presentation of neural architecture search (NAS) realized an analogous scenario in which the dual optimization procedure resulted in superior predictive performance, but the original implementation proved prohibitive in terms of complexity and computational demand. Subsequent work accelerated training efficiency and the efficacy of the procedure [30,38,28,29], eventually making the method amenable to a unified optimization based on a differentiable process [30]. In the case of learned augmentations, subsequent work identified more efficient search methods [20,25], however such methods still require a separate optimization procedure, which significantly increases the computational cost and complexity of training a machine learning model.\n\nThe original formulation for automated data augmentation postulated a separate search on a small, proxy task whose results may be transferred to a larger target task [59,58]. This formulation makes a strong assumption that the proxy task provides a predictive indication of the larger task [28,2]. In the case of learned data augmentation, we provide experimental evidence to challenge this core assumption. In particular, we demonstrate that this strategy is sub-optimal as the strength of the augmentation depends strongly on model and dataset size. These results suggest that an improved data augmentation may be possible if one could remove the separate search phase on a proxy task.\n\nIn this work, we propose a practical method for automated data augmentation -termed RandAugment -that does not require a separate search. In order to remove a separate search, we find it necessary to dramatically reduce the search space for data augmentation. The reduction in parameter space is in fact so dramatic that simple grid search is sufficient to find a data augmentation policy that outperforms all learned augmentation methods that employ a separate search phase. Our contributions can be summarized as follows:\n\n\u2022 We demonstrate that the optimal strength of a data augmentation depends on the model size and training set size. This observation indicates that a separate optimization of an augmentation policy on a smaller proxy task may be sub-optimal for learning and transferring augmentation policies.\n\n\u2022 We introduce a vastly simplified search space for data augmentation containing 2 interpretable hyperparameters. One may employ simple grid search to tailor the augmentation policy to a model and dataset, removing the need for a separate search process.\n\n\u2022 Leveraging this formulation, we demonstrate state-ofthe-art results on CIFAR [22], SVHN [34], and Im-ageNet [6]. On object detection [27], our method is within 0.3% mAP of state-of-the-art. On ImageNet we achieve a state-of-the-art accuracy of 85.0%, a 0.6% increment over previous methods and 1.0% over baseline augmentation.\n\n\nRelated Work\n\nData augmentation has played a central role in the training of deep vision models. On natural images, horizontal flips and random cropping or translations of the images are commonly used in classification and detection models [53,23,13]. On MNIST, elastic distortions across scale, position, and orientation have been applied to achieve impressive results [43,4,49,42]. While previous examples augment the data while keeping it in the training set distribution, operations that do the opposite can also be effective in increasing generalization. Some methods randomly erase or add noise to patches of images for increased validation accuracy [8,55], robustness [46,52,11], or both [32]. Mixup [54] is a particularly effective augmentation method on CIFAR-10 and ImageNet, where the neural network is trained on convex combinations of images and their corresponding labels. Object-centric cropping is commonly used for object detection tasks [31], whereas [9] adds new objects on training images by cut-and-paste.\n\nMoving away from individual operations to augment data, other work has focused on finding optimal strategies for combining different operations. For example, Smart Augmentation learns a network that merges two or more samples from the same class to generate new data [24]. Tran et al. generate augmented data via a Bayesian approach, based on the distribution learned from the training set [48]. DeVries et al. use transformations (e.g. noise, interpolations and extrapolations) in the learned feature space to augment data [7]. Furthermore, generative adversarial networks (GAN) have been used to choose optimal sequences of data augmentation operations [39]. GANs have also been used to generate training data directly [37,33,56,1,44], however this approach does not seem to be as beneficial as learning sequences of data augmentation operations that are pre-defined [40].\n\nAnother approach to learning data augmentation strategies from data is AutoAugment [5], which originally used reinforcement learning to choose a sequence of operations as well as their probability of application and magnitude. Application of AutoAugment policies involves stochasticity at multiple levels: 1) for every image in every minibatch, a sub-policy is chosen with uniform probability. 2) operations in each sub-policy has an associated probability of In these examples N =2 and three magnitudes are shown corresponding to the optimal distortion magnitudes for ResNet-50, EfficientNet-B5 and EfficientNet-B7, respectively. As the distortion magnitude increases, the strength of the augmentation increases. application. 3) Some operations have stochasticity over direction. For example, an image can be rotated clockwise or counter-clockwise. The layers of stochasticity increase the amount of diversity that the network is trained on, which in turn was found to significantly improve generalization on many datasets. More recently, several papers used the Au-toAugment search space and formalism with improved optimization algorithms to find AutoAugment policies more efficiently [20,25]. Although the time it takes to search for policies has been reduced significantly, having to implement these methods in a separate search phase reduces the applicability of AutoAugment. For this reason, this work aims to eliminate the search phase on a separate proxy task completely.\n\nSome of the developments in RandAugment were inspired by the recent improvements to searching over data augmentation policies. For example, Population Based Augmentation (PBA) [20] found that the optimal magnitude of augmentations increased during the course of training, which inspired us to not search over optimal magnitudes for each transformation but have a fixed magnitude schedule, which we discuss in detail in Section 3. Furthermore, authors of Fast AutoAugment [25] found that a data augmentation policy that is trained for density matching leads to improved generalization accuracy, which inspired our first order differentiable term for improving augmentation (see Section 4.7).  \n\n\nMethods\n\nThe primary goal of RandAugment is to remove the need for a separate search phase on a proxy task. The reason we wish to remove the search phase is because a separate search phase significantly complicates training and is computationally expensive. More importantly, the proxy task may provide sub-optimal results (see Section 4.1). In order to remove a separate search phase, we aspire to fold the parameters for the data augmentation strategy into the hyper-parameters for training a model. Given that previous learned augmentation methods contained 30+ parameters [5,25,20], we focus on vastly reducing the parameter space for data augmentation.\n\nPrevious work indicates that the main benefit of learned augmentation policies arise from increasing the diversity of examples [5,20,25]. Indeed, previous work enumerated a policy in terms of choosing which transformations to apply out of K=14 available transformations, and probabilities for applying each transformation:\n\u2022 identity \u2022 autoContrast \u2022 equalize \u2022 rotate \u2022 solarize \u2022 color \u2022 posterize \u2022 contrast \u2022 brightness \u2022 sharpness \u2022 shear-x \u2022 shear-y \u2022 translate-x \u2022 translate-y\nIn order to reduce the parameter space but still maintain image diversity, we replace the learned policies and probabilities for applying each transformation with a parameter-free procedure of always selecting a transformation with uniform probability 1 K . Given N transformations for a training image, RandAugment may thus express K N potential policies.\n\nThe final set of parameters to consider is the magnitude of the each augmentation distortion. Following [5], we employ the same linear scale for indicating the strength of each transformation. Briefly, each transformation resides on an integer scale from 0 to 10 where a value of 10 indicates the maximum scale for a given transformation. A data augmentation policy consists of identifying an integer for each augmentation [5,25,20]. In order to reduce the parameter space further, we observe that the learned magnitude for each transformation follows a similar schedule during training (e.g. Figure 4 in [20]) and postulate that a single global distortion M may suffice for parameterizing all transformations. We experimented with four methods for the schedule of M during training: constant magnitude, random magnitude, a linearly increasing magnitude, and a random magnitude with increasing upper bound. The details of this experiment can be found in Appendix A.1.1.\n\nThe resulting algorithm contains two parameters N and M and may be expressed simply in two lines of Python code ( Figure 2). Both parameters are human-interpretable such that larger values of N and M increase regularization strength. Standard methods may be employed to efficiently perform hyperparameter optimization [45,14], however given the extremely small search space we find that naive grid search is quite effective (Section 4.1). We justify all of the choices of this proposed algorithm in this subsequent sections by comparing the efficacy of the learned augmentations to all previous learned data augmentation methods.\n\n\nResults\n\nTo explore the space of data augmentations, we experiment with core image classification and object detection tasks. In particular, we focus on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets as well as COCO object detection so that we may compare with previous work. For all of these datasets, we replicate the corresponding architectures and set of data transformations. Our goal is to demonstrate the relative benefits of employing this method over previous learned augmentation methods.\n\n\nSystematic failures of a separate proxy task\n\nA central premise of learned data augmentation is to construct a small, proxy task that may be reflective of a larger task [58,59,5]. Although this assumption is sufficient for identifying learned augmentation policies to improve performance [5,57,36,25,20], it is unclear if this assumption is overly stringent and may lead to sub-optimal data augmentation policies.\n\nIn this first section, we challenge the hypothesis that formulating the problem in terms of a small proxy task is appropriate for learned data augmentation. In particular, we explore this question along two separate dimensions that are commonly restricted to achieve a small proxy task: model size and dataset size. To explore this hypothesis, we systematically measure the effects of data augmentation policies on CIFAR-10. First, we train a family of Wide-ResNet  Figure 3a demonstrates the relative gain in accuracy of a model trained across increasing distortion magnitudes for three Wide-ResNet models. The squares indicate the distortion magnitude with which achieves the highest accuracy. Note that in spite of the measurement noise, Figure  3a demonstrates systematic trends across distortion magnitudes. In particular, plotting all Wide-ResNet architectures versus the optimal distortion magnitude highlights a clear monotonic trend across increasing network sizes ( Figure  3b). Namely, larger networks demand larger data distortions for regularization. Figure 1 highlights the visual difference in the optimal distortion magnitude for differently sized models. Conversely, a learned policy based on [5] provides a fixed distortion magnitude (Figure 3b, dashed line) for all architectures that is clearly sub-optimal.\n\nA second dimension for constructing a small proxy task is to train the proxy on a small subset of the training data. Figure 3c demonstrates the relative gain in accuracy of Wide-ResNet-28-10 trained across increasing distortion magnitudes for varying amounts of CIFAR-10 training data. The squares indicate the distortion magnitude with that achieves the highest accuracy. Note that in spite of the measurement noise, Figure 3c demonstrates systematic trends across distortion magnitudes. We first observe that models trained on smaller training sets may gain more improvement from data augmentation (e.g. 3.0% versus 1.5% in Figure 3c). Furthermore, we see that the optimal distortion magnitude is larger for models that are trained on larger datasets. At first glance, this may disagree with the expectation that smaller datasets require stronger regularization. Figure 3d demonstrates that the optimal distortion magnitude increases monotonically with training set size. One hypothesis for this counter-intuitive behavior is that aggressive data augmentation leads to a low signal-to-noise ratio in small datasets. Regardless, this trend highlights the need for increasing the strength of data augmentation on larger datasets and the shortcomings of optimizing learned augmentation policies on a proxy task comprised of a subset of the training data. Namely, the learned augmentation may learn an augmentation strength more tailored to the proxy task instead of the larger task of interest.\n\nThe dependence of augmentation strength on the dataset and model size indicate that a small proxy task may provide a sub-optimal indicator of performance on a larger task. This empirical result suggests that a distinct strategy may be necessary for finding an optimal data augmentation policy. In particular, we propose in this work to focus on a unified optimization of the model weights and data augmentation policy. Figure 3 suggest that merely searching for a shared distortion magnitude M across all transformations may provide sufficient gains that exceed learned optimization methods [5]. Additionally, we see that optimizing individual magnitudes further leads to minor improvement in performance (see Section A.1.2 in Appendix).\n\nFurthermore, Figure 3a and 3c indicate that merely sampling a few distortion magnitudes is sufficient to achieve good results. Coupled with a second free parameter N , we consider these results to prescribe an algorithm for learning an augmentation policy. In the subsequent sections, we identify two free parameters N and M specifying RandAugment through a minimal grid search and compare these results against computationally-heavy learned data augmentations based on proxy tasks.\n\n\nCIFAR\n\nCIFAR-10 has been extensively studied with previous data augmentation methods and we first test this proposed method on this data. The default augmentations for all methods include flips, pad-and-crop and Cutout [8]. N and M were selected based on the validation performance on 5K held out examples from the training set for 1 and 5 settings for N and M , respectively. Results indicate that RandAugment achieves either competitive (i.e. within 0.1%) or stateof-the-art on CIFAR-10 across four network architectures ( Table 2). As a more challenging task, we additionally compare the efficacy of RandAugment on CIFAR-100 for Wide-ResNet-28-2 and Wide-ResNet-28-10. On the held out 5K dataset, we sampled 2 and 4 settings for N and M , respectively (i.e. N ={1, 2} and M ={2, 6, 10, 14}). For Wide-ResNet-28-2 and Wide-ResNet-28-10, we find that N =1, M =2 and N =2, M =14 achieves best results, respectively. Again, RandAugment achieves competitive or superior results across both architectures ( Table 2).\n\n\nSVHN\n\nBecause SVHN is composed of numbers instead of natural images, the data augmentation strategy for SVHN may differ substantially from CIFAR-10. Indeed, [5] identified a qualitatively different policy for CIFAR-10 than SVHN. Likewise, in a semi-supervised setting for CIFAR-10, a policy learned from CIFAR-10 performs better than a policy learned from SVHN [50].\n\nSVHN has a core training set of 73K images [34]. In addition, SVHN contains 531K less difficult \"extra\" images to augment training. We compare the performance of the augmentation methods on SVHN with and without the extra data on Wide-ResNet-28-2 and Wide-ResNet-28-10 ( Table 2). In spite of the large differences between SVHN and CIFAR, RandAugment consistently matches or outperforms previous methods with no alteration to the list of transformations employed. Notably, for Wide-ResNet-28-2, applying RandAugment to the core training dataset improves performance more than augmenting with 531K additional training images (98.3% vs. 98.2%). For, Wide-ResNet-28-10, RandAugment is competitive with augmenting the core training set with 531K training images (i.e. within 0.2%). Nonetheless, Wide-ResNet-28-10 with Ran-dAugment matches the previous state-of-the-art accuracy on SVHN which used a more advanced model [5].\n\n\nImageNet\n\nData augmentation methods that improve CIFAR-10 and SVHN models do not always improve large-scale tasks such as ImageNet. For instance, Cutout substantially improves CIFAR and SVHN performance [8], but fails to improve ImageNet [32]. Likewise, AutoAugment does not increase the performance on ImageNet as much as other tasks [5], especially for large networks (e.g. +0.4% for AmoebaNet-C [5] and +0.1% for EfficientNet-B5 [47]). One plausible reason for the lack of strong gains is that the small proxy task was particularly impoverished by restricting the task to \u223c10% of the 1000 ImageNet classes. Table 3 compares the performance of RandAugment to other learned augmentation approaches on ImageNet. Ran-dAugment matches the performance of AutoAugment and Fast AutoAugment on the smallest model (ResNet-50), but on larger models RandAugment significantly outperforms other methods achieving increases of up to +1.3% above the baseline. For instance, on EfficientNet-B7, the resulting model achieves 85.0% -a new state-of-the-art accuracyexhibiting a 1.0% improvement over the baseline augmentation. These systematic gains are similar to the improvements achieved with engineering new architectures [59,28], however these gains arise without incurring additional computational cost at inference time.\n\n\nCOCO\n\nTo further test the generality of this approach, we next explore a related task of large-scale object detection on the COCO dataset [27]. Learned augmentation policies have improved object detection and lead to state-of-the-art results [57]. We followed previous work by training on the same architectures and following the same training schedules (see Appendix A.3). Briefly, we employed RetinaNet [26] with ResNet-101 and ResNet-200 as a backbone [17]. Models were trained for 300 epochs from random initialization. Table 4 compares results between a baseline model, Au-toAugment and RandAugment. AutoAugment leveraged additional, specialized transformations not afforded to Ran-dAugment in order to augment the localized bounding box of an image [57]. In addition, note that AutoAugment expended \u223c15K GPU hours for search, where as Ran-dAugment was tuned by on merely 6 values of the hyperparameters (see Appendix A.3). In spite of the smaller library of specialized transformations and the lack of a separate search phase, RandAugment surpasses the baseline model and provides competitive accuracy with AutoAugment. We reserve for future work to expand the transformation library to include bounding box specific transformation to potentially improve RandAugment results even further.\n\n\nInvestigating the dependence on the included transformations\n\nRandAugment achieves state-of-the-art results across different tasks and datasets using the same list of transformations. This result suggests that RandAugment is largely insensitive to the selection of transformations for different datasets. To further study the sensitivity, we experi-  Table 3. ImageNet results. Top-1 and Top-5 accuracies (%) on ImageNet. Baseline and AutoAugment (AA) results on ResNet-50 are from [5]. Fast AutoAugment (Fast AA) results are from [25]. EfficientNet results with and without AutoAugment are from [47].\n\nHighest accuracy for each model is presented in bold. Note that Population Based Augmentation (PBA) [20] has not been implemented on ImageNet.  Table 4. Results on object detection. Mean average precision (mAP) on COCO detection task. Higher is better. Search space size is reported as the order of magnitude of the number of possible augmentation policies. Models are trained for 300 epochs from random initialization following [57]. mented with RandAugment on a Wide-ResNet-28-2 trained on CIFAR-10 for randomly sampled subsets of the full list of 14 transformations. We did not use flips, pad-and-crop, or cutout to only focus on the improvements due to Ran-dAugment with random subsets. Figure 4a suggests that the median validation accuracy due to RandAugment improves as the number of transformations is increased. However, even with only two transformations, RandAugment leads to more than 1% improvement in validation accuracy on average.\n\nTo get a sense for the effect of individual transformations, we calculate the average improvement in validation accuracy for each transformation when they are added to a random subset of transformations. We list the transformations in order of most helpful to least helpful in Table 5. We see that while geometric transformations individually make the most difference, some of the color transformations lead to a degradation of validation accuracy on average. Note that while Table 5 shows the average effect of adding individual transformations to randomly sampled subsets of transformations, Figure 4a shows that including all transformations together leads to a good result. The transformation rotate is most helpful on average, which was also observed previously [5,57]. To see the effect of representative transformations in more detail, we repeat the analysis in Figure 4a for subsets with and without (rotate, translate-x, and posterize). Surprisingly, rotate can significantly improve performance and lower variation even when included in small subsets of RandAugment transformations, while posterize seems to hurt all subsets of all sizes.\n\n\nLearning the probabilities for selecting image transformations\n\nRandAugment selects all image transformations with equal probability. This opens up the question of whether learning K probabilities may improve performance further. Most of the image transformations (except posterize, equalize, and autoContrast) are differentiable, which permits backpropagation to learn the K probabilities [30]. Let us denote \u03b1 ij as the learned probability of selecting image transformation i for operation j. For K=14 image transformations and N =2 operations, \u03b1 ij constitutes 28 parameters. We initialize all weights such that each transformation is equal probability (i.e. RandAugment), and update these parameters based on how well a model classifies a held out set of  . Models trained on reduced CIFAR-10 were trained for 500 epochs. CIFAR-10 models trained using the same hyperparameters as previous. Each result is averaged over 10 independent runs. validation images distorted by \u03b1 ij . This approach was inspired by density matching [25], but instead uses a differentiable approach in lieu of Bayesian optimization. We label this method as a 1 st -order density matching approximation.\n\nTo test the efficacy of density matching to learn the probabilities of each transformation, we trained Wide-ResNet-28-2 and Wide-ResNet-28-10 on CIFAR-10 and the reduced form of CIFAR-10 containing 4K training samples. Table 6 indicates that learning the probabilities \u03b1 ij slightly improves performance on reduced and full CIFAR-10 (RA vs 1 st ). The 1 st -order method improves accuracy by more than 3.0% for both models on reduced CIFAR-10 compared to the baseline of flips and pad-and-crop. On CIFAR-10, the 1 st -order method improves accuracy by 0.9% on the smaller model and 1.2% on the larger model compared to the baseline. We further see that the 1 st -order method always performs better than RandAugment, with the largest improvement on Wide-ResNet-28-10 trained on reduced CIFAR-10 (87.4% vs. 86.8%). On CIFAR-10, the 1 st -order method outperforms AutoAugment on Wide-ResNet-28-2 (96.1% vs. 95.9%) and matches AutoAugment on Wide-ResNet-28-10 3 . Although the density matching approach is promis- 3 As a baseline comparison, in preliminary experiments we additionally ing, this method can be expensive as one must apply all K transformations N times to each image independently. Hence, because the computational demand of KN transformations is prohibitive for large images, we reserve this for future exploration. In summary, we take these results to indicate that learning the probabilities through density matching may improve the performance on small-scale tasks and reserve explorations to larger-scale tasks for the future.\n\n\nDiscussion\n\nData augmentation is a necessary method for achieving state-of-the-art performance [43,23,7,54,13,36]. Learned data augmentation strategies have helped automate the design of such strategies and likewise achieved state-of-theart results [5,25,20,57]. In this work, we demonstrated that previous methods of learned augmentation suffers from systematic drawbacks. Namely, not tailoring the number of distortions and the distortion magnitude to the dataset size nor the model size leads to sub-optimal performance. To remedy this situation, we propose a simple parameterization for targeting augmentation to particular model and dataset sizes. We demonstrate that RandAugment is competitive with or outperforms previous approaches [5,25,20,57] on CIFAR-10/100, SVHN, ImageNet and COCO without a separate search for data augmentation policies.\n\nIn previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, AutoAugment and Fast AutoAugment could only be optimized for small models on reduced subsets of data [5,25]; population based augmentation was not reported for large-scale problems [20]. The proposed method scales quite well to datasets such as ImageNet and COCO while incurring minimal computational cost (e.g. 2 hyperparameters), but notable predictive performance gains. An open question remains how this method may improve model robustness [32,52,41] or semi-supervised learning [50]. Future work will study how this method applies to other machine learning domains, where data augmentation is known to improve predictive performance, such as image segmentation [3], 3-D perception [35], speech recognition [19] or audio recognition [18]. In particular, we wish to better understand if or when datasets or tasks may require a separate search phase to achieve optimal performance. Finally, an open question remains how one may tailor the set of transformations to a given tasks in order to further improve the predictive performance of a given model. learn \u03b1 ij based on differentiating through a virtual training step [30]. In this approach, the 2 nd -order approximation yielded consistently negative results (see Appendix A.1).\n\n\nA. Appendix\n\n\nA.1. Second order term from bilevel optimization\n\nFor the second order term for the optimization of augmentation parameters, we follow the formulation in [30], which we summarize below. We treat the optimization of augmentation parameters and weights of the neural network as a bilevel optimization problem, where \u03b1 are the augmentation parameters and w are the weights of the neural network. Then the goal is to find the optimal augmentation parameters \u03b1 such that when weights are optimized on the training set using data augmentation given by \u03b1 parameters, the validation loss is minimized. In other words:\nmin \u03b1 L val (w * (\u03b1), \u03b1) s.t. w * (\u03b1) = argmin w L train (w, \u03b1).(1)\nThen, again following [30], we approximate this bilevel optimization by a single virtual training step,\n\u2207 \u03b1 L val (w * (\u03b1), \u03b1) \u2248 \u2207 \u03b1 L val (w \u2212 \u03be\u2207 w L train (w, \u03b1), \u03b1),(2)\nwhere \u03be is the virtual learning rate. Eq. 2 can be expanded as\n\u2207 \u03b1 L val (w * (\u03b1), \u03b1) \u2248 \u2207 \u03b1 L val (w \u2212 \u03be\u2207 w L train (w, \u03b1), \u03b1) \u2212 \u03be\u2207 2 \u03b1,w L train (w, \u03b1)\u2207 w L val (w , \u03b1),(3)\nwhere w = w \u2212 \u03be\u2207 w L train (w, \u03b1). In the case where the virtual learning rate, \u03be, is zero, the second term disappears and the first term becomes \u2207L val (w, \u03b1), which was called the first-order approximation [30]. This first-order approximation was found to be highly significant for architecture search, where most of the improvement (0.3% out of 0.5%) could be achieved using this approximation in a more efficient manner (1.5 days as opposed to 4 days). Unfortunately, when \u03b1 represents augmentation parameters, firstorder approximation is irrelevant since the predictions of a model on the clean validation images do not depend on the augmentation parameters \u03b1. Then we are left with just the second order approximation, where \u03be > 0, which we approximate via finite difference approximation as\n\u2207 2 \u03b1,w L train (w, \u03b1)\u2207 w L val (w , \u03b1) \u2248 \u2207 \u03b1 L train (w + , \u03b1) \u2212 \u2207 \u03b1 L train (w \u2212 , \u03b1) 2 ,(4)\nwhere w \u00b1 = w \u00b1 \u2207 w L val (w , \u03b1) and is a small number.\n\nA.  Table 7. Results for different ways of setting the global magnitude parameter M . All magnitude methods were run on CIFAR-10 with Wide-ResNet-28-10 for 200 epochs. The reported accuracy is the average of 10 runs on the validation set for the best hyperparamter setting for that magnitude method. All magnitude methods searched over had 48 different hyperparameter settings tried. Figure 5. Performance when magnitude is changed for one image transformation. This plot uses a shared magnitude for all image transformations and then changes the magnitude of only one operation while keeping the others fixed. Two different architectures were tried (WRN-28-2 and WRN-28-10) and two different image transformations were changed (Rotate and TranslateX), which results in the 4 lines shown. Twenty different magnitudes were tried for the selected transformation ([0 \u2212 19]). The squares indicate the optimal magnitude found and the diamonds indicate the magnitude used for all other transformations (4 for WRN-28-2 and 5 for WRN-28-10).\n\nduring the course of training. A linearly increasing magnitude interpolates the distortion magnitude during training between two values. A random magnitude with increasing upper bound is similar to a random magnitude, but the upper bound is increased linearly during training. In preliminary experiments, we found that all strategies worked equally well. Thus, we selected a constant magnitude because this strategy includes only a single hyper-parameter, and we employ this for the rest of the work. The results from our experiment on trying the different magnitude strategies can be see in Table 7. Figure 5 demonstrates that changing the magnitude for one transformation, when keeping the rest fixed results in a very minor accuracy change. This suggests that tying all magnitudes together into a single value M is not greatly hurting the model performance. Across all for settings in Figure 5 the difference in accuracy of the tied magnitude vs the optimal one found was 0.19% 0.18% for the rotation operation experiments and 0.07% 0.05% for the TranslateX experiments. Changing one transformation does not have a huge impact on performance, which leads us to think that tying all magnitude parameters together is a sensible approach that drastically reduces the size of the search-space.\n\n\nA.1.2 Optimizing individual transformation magnitudes\n\n\nA.2. Experimental Details\n\n\nA.2.1 CIFAR\n\nThe Wide-ResNet models were trained for 200 epochs with a learning rate of 0.1, batch size of 128, weight decay of 5e-4, and cosine learning rate decay. Shake-Shake [12] model was trained for 1800 epochs with a learning rate of 0.01, batch size of 128, weight decay of 1e-3, and cosine learning rate decay. ShakeDrop [51] models were trained for 1800 epochs with a learning rate of 0.05, batch size of 64 (as 128 did not fit on a single GPU), weight decay of 5e-5, and cosine learning rate decay. On CIFAR-10, we used 3 for the number of operations applied (N ) and tried 4, 5, 7, 9, and 11 for magnitude. For Wide-ResNet-2 and Wide-ResNet-10, we find that the optimal magnitude is 4 and 5, respectively. For Shake-Shake (26 2x96d) and PyramidNet + ShakeDrop models, the optimal magnitude was 9 and 7, respectively.\n\n\nA.2.2 SVHN\n\nFor both SVHN datasets, we applied cutout after RandAugment as was done for AutoAugment and related methods. On core SVHN, for both Wide-ResNet-28-2 and Wide-ResNet-28-10, we used a learning rate of 5e-3, weight decay of 5e-3, and cosine learning rate decay for 200 epochs. We set N = 3 and tried 5, 7, 9, and 11 for magnitude. For both Wide-ResNet-28-2 and Wide-ResNet-28-10, we find the optimal magnitude to be 9.\n\nOn full SVHN, for both Wide-ResNet-28-2 and Wide-ResNet-28-10, we used a learning rate of 5e-3, weight decay of 1e-3, and cosine learning rate decay for 160 epochs. We set N = 3 and tried 5, 7, 9, and 11 for magnitude. For Wide-ResNet-28-2, we find the optimal magnitude to be 5; whereas for Wide-ResNet-28-10, we find the optimal magnitude to be 7.\n\n\nA.2.3 ImageNet\n\nThe ResNet models were trained for 180 epochs using the standard ResNet-50 training hyperparameters. The image size was 224 by 244, the weight decay was 0.0001 and the momentum optimizer with a momentum parameter of 0.9 was used. The learning rate was 0.1, which gets scaled by the batch size divided by 256. A global batch size of 4096 was used, split across 32 workers. For ResNet-50 the optimal distortion magnitude was 9 and (N = 2). The distortion magnitudes we tried were 5,7,9,11,13,15 and the values of N that were tried were 1, 2 and 3.\n\nThe EfficientNet experiments used the default hyper parameters and training schedule, which can be found in [47]. We trained for 350 epochs, used a batch size of 4096 split across 256 replicas. The learning rate was 0.016, which gets scaled by the batch size divided by 256. We used the RM-SProp optimizer with a momentum rate of 0.9, epsilon of 0.001 and a decay of 0.9. The weight decay used was 1e-5. For EfficientNet B5 the image size was 456 by 456 and for EfficientNet B7 it was 600 by 600. For EfficientNet B5 we tried N = 2 and N = 3 and found them to perform about the same. We found the optimal distortion magnitude for B5 to be 17. The different magnitudes we tried were 8,11,14,17,21. For EfficientNet B7 we used N = 2 and found the optimal distortion magnitude to be 28. The magnitudes tried were 17, 25, 28, 31.\n\nThe default augmentation of horizontal flipping and random crops were used on ImageNet, applied before Ran-dAugment. The standard training and validation splits were employed for training and evaluation.\n\n\nA.3. COCO\n\nWe applied horizontal flipping and scale jitters in addition to RandAugment. We used the same list of data augmentation transformations as we did in all other classification tasks. Geometric operations transformed the bounding boxes the way it was defined in Ref. [57]. We used a learning rate of 0.08 and a weight decay of 1e 4. The focal loss parameters are set to be \u03b1 = 0.25 and \u03b3 = 1.5. We set N = 1 and tried distortion magnitudes between 4 and 9. We found the optimal distortion magnitude for ResNet-101 and ResNet-200 to be 5 and 6, respectively.\n\nFigure 1 .\n1Example images augmented by RandAugment.\n\n\nnp.random.choice(transforms, N) return [(op, M) for op in sampled_ops]\n\nFigure 2 .\n2Python code for RandAugment based on numpy.\n\nFigure 3 .\n3Optimal magnitude of augmentation depends on the size of the model and the training set. All results report CIFAR-10 validation accuracy for Wide-ResNet model architectures [53] averaged over 20 random initializations, where N = 1. (a) Accuracy of Wide-ResNet-28-2, Wide-ResNet-28-7, and Wide-ResNet-28-10 across varying distortion magnitudes. Models are trained for 200 epochs on 45K training set examples. Squares indicate the distortion magnitude that achieves the maximal accuracy. (b) Optimal distortion magnitude across 7 Wide-ResNet-28 architectures with varying widening parameters (k). (c) Accuracy of Wide-ResNet-28-10 for three training set sizes (1K, 4K, and 10K) across varying distortion magnitudes. Squares indicate the distortion magnitude that achieves the maximal accuracy. (d) Optimal distortion magnitude across 8 training set sizes. Dashed curves show the scaled expectation value of the distortion magnitude in the AutoAugment policy [5].\n\nFigure 4 .\n4Average performance improves when more transformations are included in RandAugment. All panels report median CIFAR-10 validation accuracy for Wide-ResNet-28-2 model architectures[53] trained with RandAugment (N = 3, M = 4) using randomly sampled subsets of transformations. No other data augmentation is included in training. Error bars indicate 30 th and 70 th percentile. (a) Median accuracy for randomly sampled subsets of transformations. (b) Median accuracy for subsets with and without the Rotate transformation. (c) Median accuracy for subsets with and without the translate-x transformation. (d) Median accuracy for subsets with and without the posterize transformation. Dashed curves show the accuracy of the model trained without any augmentations.\n\n.\nAverage improvement due to each transformation. Average difference in validation accuracy (%) when a particular transformation is added to a randomly sampled set of transformations. For this ablation study, Wide-ResNet-28-2 models were trained on CIFAR-10 using RandAugment (N = 3, M = 4) with the randomly sampled set of transformations, with no other\n\n\nRA). Note that baseline and AA are replicated in this work. SVHN core set consists of 73K examples.The Shake-Shake model[12] employed a 26 2\u00d796d configuration, and the PyramidNet model used the ShakeDrop regularization[51]. Results reported by us are averaged over 10 independent runs. Bold indicates best results.architectures[53], where the model size may be systematically altered through the widening parameter governing the number of convolutional filters. For each of these networks, we train the model on CIFAR-10 and measure the final accuracy compared to a baseline model trained with default data augmentations (i.e. flip left-right and random translations). The Wide-ResNet models are trained with the additional K=14 data augmentations (see Methods) over a range of global distortion magnitudes M parameterized on a uniform linear scale ranging from [0, 30] 2 .baseline PBA Fast AA AA \n\nRA \nCIFAR-10 \nWide-ResNet-28-2 \n94.9 \n-\n-\n95.9 95.8 \nWide-ResNet-28-10 \n96.1 \n97.4 \n97.3 \n97.4 97.3 \nShake-Shake \n97.1 \n98.0 \n98.0 \n98.0 98.0 \nPyramidNet \n97.3 \n98.5 \n98.3 \n98.5 98.5 \nCIFAR-100 \nWide-ResNet-28-2 \n75.4 \n-\n-\n78.5 78.3 \nWide-ResNet-28-10 \n81.2 \n83.3 \n82.7 \n82.9 83.3 \nSVHN (core set) \nWide-ResNet-28-2 \n96.7 \n-\n-\n98.0 \n98.3 \nWide-ResNet-28-10 \n96.9 \n-\n-\n98.1 \n98.3 \nSVHN \nWide-ResNet-28-2 \n98.2 \n-\n-\n98.7 98.7 \nWide-ResNet-28-10 \n98.5 \n98.9 \n98.8 \n98.9 \n99.0 \n\nTable 2. Test accuracy (%) on CIFAR-10, CIFAR-100, SVHN \nand SVHN core set. Comparisons across default data augmenta-\ntion (baseline), Population Based Augmentation (PBA) [20] and \nFast AutoAugment (Fast AA) [25], AutoAugment (AA) [5] and \nproposed RandAugment (\n\nTable 6 .\n6Differentiableoptimization for augmentation can im-\nprove RandAugment. Test accuracy (%) from differentiable Ran-\ndAugment for reduced (4K examples) and full CIFAR-10. The \n1 st -order approximation (1 st ) is based on density matching (Sec-\ntion 4.7)\n\n\n1.1 Magnitude methods A random magnitude uniformly randomly samples the distortion magnitude between two values. A constant magnitude sets the distortion magnitude to a constant number Random Magnitude with Increasing Upper Bound 97.3Magnitude Method \nAccuracy \nRandom Magnitude \n97.3 \nConstant Magnitude \n97.2 \nLinearly Increasing Magnitude \n97.2 \n\nNote that the range of magnitudes exceeds the specified range of magnitudes in the Methods because we wish to explore a larger range of magnitudes for this preliminary experiment. We retain the same scale as[5] for a value of 10 to maintain comparable results.\nAcknowledgementsWe thank Samy Bengio, Daniel Ho, Ildoo Kim, Jaehoon Lee, Zhaoqi Leng, Hanxiao Liu, Raphael Gontijo Lopes, Ruoming Pang, Ben Poole, Mingxing Tan, and the rest of the Brain team for their help.\nAntreas Antoniou, Amos Storkey, Harrison Edwards, arXiv:1711.04340Data augmentation generative adversarial networks. arXiv preprintAntreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017. 2\n\nSearching for efficient multi-scale architectures for dense image prediction. Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jon Shlens, Advances in Neural Information Processing Systems. Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, and Jon Shlens. Searching for efficient multi-scale archi- tectures for dense image prediction. In Advances in Neural Information Processing Systems, pages 8699-8710, 2018. 2\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848, 2017. 8\n\nMulticolumn deep neural networks for image classification. Dan Ciregan, Ueli Meier, J\u00fcrgen Schmidhuber, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionDan Ciregan, Ueli Meier, and J\u00fcrgen Schmidhuber. Multi- column deep neural networks for image classification. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3642-3649. IEEE, 2012. 2\n\nBarret Ekin D Cubuk, Dandelion Zoph, Vijay Mane, Quoc V Vasudevan, Le, Autoaugment, arXiv:1805.09501Learning augmentation policies from data. 7arXiv preprintEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. 1, 2, 3, 4, 5, 6, 7, 8\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern Recognition1Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 1, 2\n\nDataset augmentation in feature space. Terrance Devries, W Graham, Taylor, arXiv:1702.05538arXiv preprintTerrance DeVries and Graham W Taylor. Dataset augmen- tation in feature space. arXiv preprint arXiv:1702.05538, 2017. 1, 2, 8\n\nTerrance Devries, W Graham, Taylor, arXiv:1708.04552Improved regularization of convolutional neural networks with cutout. 26arXiv preprintTerrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 2, 6\n\nCut, paste and learn: Surprisingly easy synthesis for instance detection. Debidatta Dwibedi, Ishan Misra, Martial Hebert, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDebidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance de- tection. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 1301-1310, 2017. 2\n\nInstaboost: Boosting instance segmentation via probability map guided copypasting. Jianhua Hao-Shu Fang, Runzhong Sun, Minghao Wang, Yong-Lu Gou, Cewu Li, Lu, arXiv:1908.07801arXiv preprintHao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy- pasting. arXiv preprint arXiv:1908.07801, 2019. 1\n\nAdversarial examples are a natural consequence of test error in noise. Nic Ford, Justin Gilmer, Nicolas Carlini, Dogus Cubuk, arXiv:1901.10513arXiv preprintNic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. Adversarial examples are a natural consequence of test error in noise. arXiv preprint arXiv:1901.10513, 2019. 2\n\nXavier Gastaldi, arXiv:1705.07485Shake-shake regularization. 413arXiv preprintXavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017. 4, 13\n\n. Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. Detectron. Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. Detectron, 2018. 1, 2, 8\n\nGoogle vizier: A service for black-box optimization. Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, D Sculley, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMDaniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1487-1495. ACM, 2017. 4\n\nDeep pyramidal residual networks. Dongyoon Han, Jiwhan Kim, Junmo Kim, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)IEEEDongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyrami- dal residual networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6307-6315. IEEE, 2017. 1\n\nDeep speech: Scaling up end-to-end speech recognition. Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, arXiv:1412.5567arXiv preprintAwni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014. 1\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)16Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016. 1, 6\n\nCnn architectures for large-scale audio classification. Shawn Hershey, Sourish Chaudhuri, P W Daniel, Ellis, F Jort, Aren Gemmeke, Channing Jansen, Manoj Moore, Devin Plakal, Platt, A Rif, Bryan Saurous, Seybold, 2017 ieee international conference on acoustics, speech and signal processing (icassp). IEEEShawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi- tectures for large-scale audio classification. In 2017 ieee in- ternational conference on acoustics, speech and signal pro- cessing (icassp), pages 131-135. IEEE, 2017. 8\n\nDeep neural networks for acoustic modeling in speech recognition. Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, IEEE Signal processing magazine. 298Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012. 8\n\nPopulation based augmentation: Efficient learning of augmentation policy schedules. Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, Xi Chen, arXiv:1905.05393arXiv preprintDaniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation: Efficient learn- ing of augmentation policy schedules. arXiv preprint arXiv:1905.05393, 2019. 1, 2, 3, 4, 7, 8\n\nElastic spectral distortion for low resource speech recognition with deep neural networks. Naoyuki Kanda, Ryu Takeda, Yasunari Obuchi, 2013 IEEE Workshop on Automatic Speech Recognition and Understanding. IEEENaoyuki Kanda, Ryu Takeda, and Yasunari Obuchi. Elastic spectral distortion for low resource speech recognition with deep neural networks. In 2013 IEEE Workshop on Auto- matic Speech Recognition and Understanding, pages 309- 314. IEEE, 2013. 1\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 1Technical reportUniversity of TorontoAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Uni- versity of Toronto, 2009. 1, 2\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in Neural Information Processing Sys- tems, 2012. 1, 2, 8\n\nSmart augmentation learning an optimal data augmentation strategy. Joseph Lemley, Shabab Bazrafkan, Peter Corcoran, IEEE Access. 52Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858-5869, 2017. 2\n\n. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim, arXiv:1905.003977Fast autoaugment. arXiv preprintSungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. arXiv preprint arXiv:1905.00397, 2019. 1, 2, 3, 4, 7, 8\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017. 6\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Springer26Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 2, 6\n\nChenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy, arXiv:1712.00559Progressive neural architecture search. 26arXiv preprintChenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Mur- phy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017. 2, 6\n\nHierarchical representations for efficient architecture search. Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu, International Conference on Learning Representations. Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representa- tions for efficient architecture search. In International Con- ference on Learning Representations, 2018. 2\n\nHanxiao Liu, Karen Simonyan, Yiming Yang, arXiv:1806.09055Darts: Differentiable architecture search. 812arXiv preprintHanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. 2, 7, 8, 12\n\nSsd: Single shot multibox detector. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg, European conference on computer vision. SpringerWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European con- ference on computer vision, pages 21-37. Springer, 2016. 2\n\nImproving robustness without sacrificing accuracy with patch gaussian augmentation. Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, Ekin D Cubuk, arXiv:1906.02611arXiv preprintRaphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk. Improving robustness without sacrificing accuracy with patch gaussian augmentation. arXiv preprint arXiv:1906.02611, 2019. 1, 2, 6, 8\n\nGenerative adversarial network based acoustic scene training set augmentation and selection using svm hyperplane. Sangwook Seongkyu Mun, Park, K David, Hanseok Han, Ko, Detection and Classification of Acoustic Scenes and Events Workshop. Seongkyu Mun, Sangwook Park, David K Han, and Hanseok Ko. Generative adversarial network based acoustic scene training set augmentation and selection using svm hyper- plane. In Detection and Classification of Acoustic Scenes and Events Workshop, 2017. 2\n\nReading digits in natural images with unsupervised feature learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y Ng, NIPS Workshop on Deep Learning and Unsupervised Feature Learning. 6Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Work- shop on Deep Learning and Unsupervised Feature Learning, 2011. 1, 2, 6\n\nJiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Alsharif, Patrick Nguyen, arXiv:1908.11069Targeted computation for object detection in point clouds. arXiv preprintJiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Al- sharif, Patrick Nguyen, et al. Starnet: Targeted compu- tation for object detection in point clouds. arXiv preprint arXiv:1908.11069, 2019. 8\n\nSpecaugment: A simple data augmentation method for automatic speech recognition. S Daniel, William Park, Yu Chan, Chung-Cheng Zhang, Barret Chiu, Zoph, D Ekin, Quoc V Cubuk, Le, arXiv:1904.08779arXiv preprintDaniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaug- ment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. 1, 4, 8\n\nThe effectiveness of data augmentation in image classification using deep learning. Luis Perez, Jason Wang, arXiv:1712.04621arXiv preprintLuis Perez and Jason Wang. The effectiveness of data aug- mentation in image classification using deep learning. arXiv preprint arXiv:1712.04621, 2017. 2\n\nEfficient neural architecture search via parameter sharing. Hieu Pham, Y Melody, Barret Guan, Zoph, V Quoc, Jeff Le, Dean, International Conference on Machine Learning. Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In International Conference on Machine Learning, 2018. 2\n\nLearning to compose domain-specific transformations for data augmentation. J Alexander, Henry Ratner, Zeshan Ehrenberg, Jared Hussain, Christopher Dunnmon, R\u00e9, Advances in Neural Information Processing Systems. Alexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher R\u00e9. Learning to compose domain-specific transformations for data augmentation. In Advances in Neural Information Processing Systems, pages 3239-3249, 2017. 2\n\nClassification accuracy score for conditional generative models. Suman Ravuri, Oriol Vinyals, arXiv:1905.10887arXiv preprintSuman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models. arXiv preprint arXiv:1905.10887, 2019. 2\n\nDo imagenet classifiers generalize to imagenet?. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, arXiv:1902.108111arXiv preprintBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to im- agenet? arXiv preprint arXiv:1902.10811, 2019. 1, 8\n\nIkuro Sato, Hiroki Nishimura, Kensuke Yokoi, arXiv:1505.03229Augmented pattern classification with neural networks. ApacarXiv preprintIkuro Sato, Hiroki Nishimura, and Kensuke Yokoi. Apac: Augmented pattern classification with neural networks. arXiv preprint arXiv:1505.03229, 2015. 2\n\nBest practices for convolutional neural networks applied to visual document analysis. Y Patrice, David Simard, John C Steinkraus, Platt, Proceedings of International Conference on Document Analysis and Recognition. International Conference on Document Analysis and Recognition1Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of International Confer- ence on Document Analysis and Recognition, 2003. 1, 2, 8\n\nRendergan: Generating realistic labeled data. Leon Sixt, Benjamin Wild, Tim Landgraf, arXiv:1611.01331arXiv preprintLeon Sixt, Benjamin Wild, and Tim Landgraf. Render- gan: Generating realistic labeled data. arXiv preprint arXiv:1611.01331, 2016. 2\n\nPractical bayesian optimization of machine learning algorithms. Jasper Snoek, Hugo Larochelle, Ryan P Adams, Advances in neural information processing systems. Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Prac- tical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951-2959, 2012. 4\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, arXiv:1312.6199Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 2\n\nMingxing Tan, V Quoc, Le, Efficientnet, arXiv:1905.11946Rethinking model scaling for convolutional neural networks. 713arXiv preprintMingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 1, 6, 7, 13\n\nA bayesian data augmentation approach for learning deep models. Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, Ian Reid, Advances in Neural Information Processing Systems. Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid. A bayesian data augmentation approach for learn- ing deep models. In Advances in Neural Information Pro- cessing Systems, pages 2794-2803, 2017. 2\n\nRegularization of neural networks using dropconnect. Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, Rob Fergus, International Conference on Machine Learning. Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using drop- connect. In International Conference on Machine Learning, pages 1058-1066, 2013. 2\n\n. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V Le, arXiv:1904.128486Unsupervised data augmentation. arXiv preprintQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019. 1, 6, 8\n\nYoshihiro Yamada, Masakazu Iwamura, Koichi Kise, arXiv:1802.02375Shakedrop regularization. 413arXiv preprintYoshihiro Yamada, Masakazu Iwamura, and Koichi Kise. Shakedrop regularization. arXiv preprint arXiv:1802.02375, 2018. 4, 13\n\nA fourier perspective on model robustness in computer vision. Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, D Ekin, Justin Cubuk, Gilmer, arXiv:1906.08988arXiv preprintDong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. arXiv preprint arXiv:1906.08988, 2019. 1, 2, 8\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, British Machine Vision Conference. 57Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In British Machine Vision Conference, 2016. 1, 2, 4, 5, 7\n\nHongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 1, 2, 8\n\n. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang, arXiv:1708.04896Random erasing data augmentation. arXiv preprintZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. arXiv preprint arXiv:1708.04896, 2017. 2\n\nData augmentation in emotion classification using generative adversarial networks. Xinyue Zhu, Yifan Liu, Zengchang Qin, Jiahong Li, arXiv:1711.00648arXiv preprintXinyue Zhu, Yifan Liu, Zengchang Qin, and Jiahong Li. Data augmentation in emotion classification using genera- tive adversarial networks. arXiv preprint arXiv:1711.00648, 2017. 2\n\nLearning data augmentation strategies for object detection. Barret Zoph, D Ekin, Golnaz Cubuk, Tsung-Yi Ghiasi, Jonathon Lin, Quoc V Shlens, Le, arXiv:1906.11172813arXiv preprintBarret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning data aug- mentation strategies for object detection. arXiv preprint arXiv:1906.11172, 2019. 1, 4, 6, 7, 8, 13\n\nNeural architecture search with reinforcement learning. Barret Zoph, V Quoc, Le, International Conference on Learning Representations. 24Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017. 2, 4\n\nLearning transferable architectures for scalable image recognition. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V Le, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern Recognition6Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of IEEE Conference on Com- puter Vision and Pattern Recognition, 2017. 2, 4, 6\n", "annotations": {"author": "[{\"end\":140,\"start\":82},{\"end\":203,\"start\":141},{\"end\":266,\"start\":204},{\"end\":305,\"start\":267}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":89},{\"end\":152,\"start\":148},{\"end\":219,\"start\":213},{\"end\":276,\"start\":274}]", "author_first_name": "[{\"end\":86,\"start\":82},{\"end\":88,\"start\":87},{\"end\":147,\"start\":141},{\"end\":212,\"start\":204},{\"end\":271,\"start\":267},{\"end\":273,\"start\":272}]", "author_affiliation": "[{\"end\":139,\"start\":113},{\"end\":202,\"start\":176},{\"end\":265,\"start\":239},{\"end\":304,\"start\":278}]", "title": "[{\"end\":79,\"start\":1},{\"end\":384,\"start\":306}]", "venue": null, "abstract": "[{\"end\":2328,\"start\":386}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2554,\"start\":2551},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2577,\"start\":2573},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2619,\"start\":2615},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2670,\"start\":2666},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2681,\"start\":2677},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2699,\"start\":2696},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2769,\"start\":2765},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2793,\"start\":2789},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2809,\"start\":2805},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2835,\"start\":2831},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3225,\"start\":3221},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3228,\"start\":3225},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3230,\"start\":3228},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3233,\"start\":3230},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3256,\"start\":3252},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3284,\"start\":3280},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3313,\"start\":3309},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3316,\"start\":3313},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3319,\"start\":3316},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3804,\"start\":3801},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3807,\"start\":3804},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3810,\"start\":3807},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3813,\"start\":3810},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3927,\"start\":3924},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3950,\"start\":3946},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3953,\"start\":3950},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3956,\"start\":3953},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4006,\"start\":4002},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4097,\"start\":4093},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":4161,\"start\":4157},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4826,\"start\":4822},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4829,\"start\":4826},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4832,\"start\":4829},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4835,\"start\":4832},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4939,\"start\":4935},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5040,\"start\":5036},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5043,\"start\":5040},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":5392,\"start\":5388},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5395,\"start\":5392},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5516,\"start\":5512},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5518,\"start\":5516},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7069,\"start\":7065},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7080,\"start\":7076},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7099,\"start\":7096},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7125,\"start\":7121},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7561,\"start\":7557},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7564,\"start\":7561},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7567,\"start\":7564},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7691,\"start\":7687},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7693,\"start\":7691},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7696,\"start\":7693},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7699,\"start\":7696},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7976,\"start\":7973},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7979,\"start\":7976},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7996,\"start\":7992},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7999,\"start\":7996},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8002,\"start\":7999},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8016,\"start\":8012},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8028,\"start\":8024},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8276,\"start\":8272},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8289,\"start\":8286},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8616,\"start\":8612},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8739,\"start\":8735},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8872,\"start\":8869},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9004,\"start\":9000},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9070,\"start\":9066},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9073,\"start\":9070},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9076,\"start\":9073},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9078,\"start\":9076},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9081,\"start\":9078},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9218,\"start\":9214},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9307,\"start\":9304},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10413,\"start\":10409},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10416,\"start\":10413},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10883,\"start\":10879},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11178,\"start\":11174},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11977,\"start\":11974},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11980,\"start\":11977},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11983,\"start\":11980},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12187,\"start\":12184},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12190,\"start\":12187},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12193,\"start\":12190},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13006,\"start\":13003},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13325,\"start\":13322},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13328,\"start\":13325},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13331,\"start\":13328},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13508,\"start\":13504},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14192,\"start\":14188},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14195,\"start\":14192},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":15178,\"start\":15174},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":15181,\"start\":15178},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15183,\"start\":15181},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15296,\"start\":15293},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15299,\"start\":15296},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15302,\"start\":15299},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15305,\"start\":15302},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15308,\"start\":15305},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16633,\"start\":16630},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18838,\"start\":18835},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19690,\"start\":19687},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20644,\"start\":20641},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20849,\"start\":20845},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20899,\"start\":20895},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21770,\"start\":21767},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21980,\"start\":21977},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22016,\"start\":22012},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22112,\"start\":22109},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22175,\"start\":22172},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22210,\"start\":22206},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":22988,\"start\":22984},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22991,\"start\":22988},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23230,\"start\":23226},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23334,\"start\":23330},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23497,\"start\":23493},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23547,\"start\":23543},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23847,\"start\":23843},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24870,\"start\":24867},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24920,\"start\":24916},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24985,\"start\":24981},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25092,\"start\":25088},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25421,\"start\":25417},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26706,\"start\":26703},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":26709,\"start\":26706},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27481,\"start\":27477},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28120,\"start\":28116},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29282,\"start\":29281},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29914,\"start\":29910},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29917,\"start\":29914},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29919,\"start\":29917},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":29922,\"start\":29919},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29925,\"start\":29922},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29928,\"start\":29925},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30067,\"start\":30064},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30070,\"start\":30067},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30073,\"start\":30070},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30076,\"start\":30073},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30558,\"start\":30555},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30561,\"start\":30558},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30564,\"start\":30561},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30567,\"start\":30564},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30896,\"start\":30893},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30899,\"start\":30896},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30977,\"start\":30973},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31240,\"start\":31236},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31243,\"start\":31240},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31246,\"start\":31243},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":31279,\"start\":31275},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31461,\"start\":31458},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31482,\"start\":31478},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31507,\"start\":31503},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31533,\"start\":31529},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31918,\"start\":31914},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32200,\"start\":32196},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32746,\"start\":32742},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33278,\"start\":33274},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36613,\"start\":36609},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":36765,\"start\":36761},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38539,\"start\":38537},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38541,\"start\":38539},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38543,\"start\":38541},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38546,\"start\":38543},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38549,\"start\":38546},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38551,\"start\":38549},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":38718,\"start\":38714},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39290,\"start\":39288},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39293,\"start\":39290},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":39296,\"start\":39293},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39299,\"start\":39296},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39301,\"start\":39299},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":39918,\"start\":39914},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":41558,\"start\":41554},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42617,\"start\":42613},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":42715,\"start\":42711},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":42824,\"start\":42820},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":44956,\"start\":44953}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40258,\"start\":40205},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40331,\"start\":40259},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40388,\"start\":40332},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41362,\"start\":40389},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42134,\"start\":41363},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42490,\"start\":42135},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44129,\"start\":42491},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44393,\"start\":44130},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44745,\"start\":44394}]", "paragraph": "[{\"end\":3069,\"start\":2338},{\"end\":3576,\"start\":3086},{\"end\":4272,\"start\":3578},{\"end\":5220,\"start\":4274},{\"end\":5909,\"start\":5222},{\"end\":6434,\"start\":5911},{\"end\":6728,\"start\":6436},{\"end\":6984,\"start\":6730},{\"end\":7314,\"start\":6986},{\"end\":8343,\"start\":7331},{\"end\":9219,\"start\":8345},{\"end\":10701,\"start\":9221},{\"end\":11395,\"start\":10703},{\"end\":12055,\"start\":11407},{\"end\":12379,\"start\":12057},{\"end\":12897,\"start\":12541},{\"end\":13868,\"start\":12899},{\"end\":14499,\"start\":13870},{\"end\":15002,\"start\":14511},{\"end\":15418,\"start\":15051},{\"end\":16747,\"start\":15420},{\"end\":18242,\"start\":16749},{\"end\":18981,\"start\":18244},{\"end\":19465,\"start\":18983},{\"end\":20481,\"start\":19475},{\"end\":20850,\"start\":20490},{\"end\":21771,\"start\":20852},{\"end\":23085,\"start\":21784},{\"end\":24382,\"start\":23094},{\"end\":24986,\"start\":24447},{\"end\":25934,\"start\":24988},{\"end\":27084,\"start\":25936},{\"end\":28268,\"start\":27151},{\"end\":29812,\"start\":28270},{\"end\":30666,\"start\":29827},{\"end\":32025,\"start\":30668},{\"end\":32651,\"start\":32092},{\"end\":32823,\"start\":32720},{\"end\":32954,\"start\":32892},{\"end\":33863,\"start\":33066},{\"end\":34015,\"start\":33959},{\"end\":35050,\"start\":34017},{\"end\":36344,\"start\":35052},{\"end\":37259,\"start\":36444},{\"end\":37689,\"start\":37274},{\"end\":38040,\"start\":37691},{\"end\":38604,\"start\":38059},{\"end\":39431,\"start\":38606},{\"end\":39636,\"start\":39433},{\"end\":40204,\"start\":39650}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12540,\"start\":12380},{\"attributes\":{\"id\":\"formula_1\"},\"end\":32719,\"start\":32652},{\"attributes\":{\"id\":\"formula_2\"},\"end\":32891,\"start\":32824},{\"attributes\":{\"id\":\"formula_3\"},\"end\":33065,\"start\":32955},{\"attributes\":{\"id\":\"formula_4\"},\"end\":33958,\"start\":33864}]", "table_ref": "[{\"end\":2337,\"start\":2330},{\"end\":20000,\"start\":19993},{\"end\":20479,\"start\":20472},{\"end\":21130,\"start\":21123},{\"end\":22391,\"start\":22384},{\"end\":23619,\"start\":23612},{\"end\":24743,\"start\":24736},{\"end\":25139,\"start\":25132},{\"end\":26220,\"start\":26213},{\"end\":26419,\"start\":26412},{\"end\":34028,\"start\":34021},{\"end\":35651,\"start\":35644}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3084,\"start\":3072},{\"attributes\":{\"n\":\"2.\"},\"end\":7329,\"start\":7317},{\"attributes\":{\"n\":\"3.\"},\"end\":11405,\"start\":11398},{\"attributes\":{\"n\":\"4.\"},\"end\":14509,\"start\":14502},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15049,\"start\":15005},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19473,\"start\":19468},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20488,\"start\":20484},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21782,\"start\":21774},{\"attributes\":{\"n\":\"4.5.\"},\"end\":23092,\"start\":23088},{\"attributes\":{\"n\":\"4.6.\"},\"end\":24445,\"start\":24385},{\"attributes\":{\"n\":\"4.7.\"},\"end\":27149,\"start\":27087},{\"attributes\":{\"n\":\"5.\"},\"end\":29825,\"start\":29815},{\"end\":32039,\"start\":32028},{\"end\":32090,\"start\":32042},{\"end\":36400,\"start\":36347},{\"end\":36428,\"start\":36403},{\"end\":36442,\"start\":36431},{\"end\":37272,\"start\":37262},{\"end\":38057,\"start\":38043},{\"end\":39648,\"start\":39639},{\"end\":40216,\"start\":40206},{\"end\":40343,\"start\":40333},{\"end\":40400,\"start\":40390},{\"end\":41374,\"start\":41364},{\"end\":42137,\"start\":42136},{\"end\":44140,\"start\":44131}]", "table": "[{\"end\":44129,\"start\":43366},{\"end\":44393,\"start\":44156},{\"end\":44745,\"start\":44630}]", "figure_caption": "[{\"end\":40258,\"start\":40218},{\"end\":40331,\"start\":40261},{\"end\":40388,\"start\":40345},{\"end\":41362,\"start\":40402},{\"end\":42134,\"start\":41376},{\"end\":42490,\"start\":42138},{\"end\":43366,\"start\":42493},{\"end\":44156,\"start\":44142},{\"end\":44630,\"start\":44396}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13500,\"start\":13492},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13992,\"start\":13984},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15895,\"start\":15886},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16171,\"start\":16161},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16406,\"start\":16396},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16492,\"start\":16484},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16682,\"start\":16672},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16875,\"start\":16866},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17176,\"start\":17167},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17384,\"start\":17375},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17623,\"start\":17614},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18671,\"start\":18663},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19005,\"start\":18996},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25688,\"start\":25679},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26539,\"start\":26530},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26814,\"start\":26805},{\"end\":34409,\"start\":34401},{\"end\":35661,\"start\":35653},{\"end\":35948,\"start\":35940}]", "bib_author_first_name": "[{\"end\":45222,\"start\":45215},{\"end\":45237,\"start\":45233},{\"end\":45255,\"start\":45247},{\"end\":45582,\"start\":45571},{\"end\":45596,\"start\":45589},{\"end\":45611,\"start\":45606},{\"end\":45623,\"start\":45617},{\"end\":45642,\"start\":45636},{\"end\":45656,\"start\":45649},{\"end\":45673,\"start\":45666},{\"end\":45683,\"start\":45680},{\"end\":46152,\"start\":46141},{\"end\":46165,\"start\":46159},{\"end\":46185,\"start\":46178},{\"end\":46201,\"start\":46196},{\"end\":46214,\"start\":46210},{\"end\":46216,\"start\":46215},{\"end\":46644,\"start\":46641},{\"end\":46658,\"start\":46654},{\"end\":46672,\"start\":46666},{\"end\":47045,\"start\":47039},{\"end\":47069,\"start\":47060},{\"end\":47081,\"start\":47076},{\"end\":47094,\"start\":47088},{\"end\":47447,\"start\":47444},{\"end\":47457,\"start\":47454},{\"end\":47471,\"start\":47464},{\"end\":47486,\"start\":47480},{\"end\":47494,\"start\":47491},{\"end\":47501,\"start\":47499},{\"end\":47913,\"start\":47905},{\"end\":47924,\"start\":47923},{\"end\":48106,\"start\":48098},{\"end\":48117,\"start\":48116},{\"end\":48474,\"start\":48465},{\"end\":48489,\"start\":48484},{\"end\":48504,\"start\":48497},{\"end\":48952,\"start\":48945},{\"end\":48975,\"start\":48967},{\"end\":48988,\"start\":48981},{\"end\":49002,\"start\":48995},{\"end\":49012,\"start\":49008},{\"end\":49332,\"start\":49329},{\"end\":49345,\"start\":49339},{\"end\":49361,\"start\":49354},{\"end\":49376,\"start\":49371},{\"end\":49592,\"start\":49586},{\"end\":49761,\"start\":49757},{\"end\":49777,\"start\":49772},{\"end\":49798,\"start\":49791},{\"end\":50017,\"start\":50011},{\"end\":50035,\"start\":50027},{\"end\":50053,\"start\":50044},{\"end\":50066,\"start\":50062},{\"end\":50082,\"start\":50078},{\"end\":50091,\"start\":50090},{\"end\":50610,\"start\":50602},{\"end\":50622,\"start\":50616},{\"end\":50633,\"start\":50628},{\"end\":51043,\"start\":51039},{\"end\":51056,\"start\":51052},{\"end\":51068,\"start\":51063},{\"end\":51082,\"start\":51077},{\"end\":51098,\"start\":51094},{\"end\":51112,\"start\":51107},{\"end\":51124,\"start\":51120},{\"end\":51141,\"start\":51134},{\"end\":51158,\"start\":51152},{\"end\":51173,\"start\":51169},{\"end\":51510,\"start\":51503},{\"end\":51522,\"start\":51515},{\"end\":51538,\"start\":51530},{\"end\":51548,\"start\":51544},{\"end\":51991,\"start\":51986},{\"end\":52008,\"start\":52001},{\"end\":52021,\"start\":52020},{\"end\":52023,\"start\":52022},{\"end\":52040,\"start\":52039},{\"end\":52051,\"start\":52047},{\"end\":52069,\"start\":52061},{\"end\":52083,\"start\":52078},{\"end\":52096,\"start\":52091},{\"end\":52113,\"start\":52112},{\"end\":52124,\"start\":52119},{\"end\":52654,\"start\":52646},{\"end\":52665,\"start\":52663},{\"end\":52676,\"start\":52672},{\"end\":52687,\"start\":52681},{\"end\":52705,\"start\":52694},{\"end\":52722,\"start\":52715},{\"end\":52737,\"start\":52731},{\"end\":52753,\"start\":52746},{\"end\":52772,\"start\":52765},{\"end\":52786,\"start\":52781},{\"end\":53197,\"start\":53191},{\"end\":53206,\"start\":53202},{\"end\":53217,\"start\":53214},{\"end\":53232,\"start\":53226},{\"end\":53243,\"start\":53241},{\"end\":53584,\"start\":53577},{\"end\":53595,\"start\":53592},{\"end\":53612,\"start\":53604},{\"end\":53999,\"start\":53995},{\"end\":54020,\"start\":54012},{\"end\":54283,\"start\":54279},{\"end\":54300,\"start\":54296},{\"end\":54320,\"start\":54312},{\"end\":54322,\"start\":54321},{\"end\":54650,\"start\":54644},{\"end\":54665,\"start\":54659},{\"end\":54682,\"start\":54677},{\"end\":54872,\"start\":54865},{\"end\":54883,\"start\":54878},{\"end\":54895,\"start\":54889},{\"end\":54908,\"start\":54901},{\"end\":54923,\"start\":54914},{\"end\":55198,\"start\":55190},{\"end\":55209,\"start\":55204},{\"end\":55221,\"start\":55217},{\"end\":55615,\"start\":55607},{\"end\":55628,\"start\":55621},{\"end\":55641,\"start\":55636},{\"end\":55657,\"start\":55652},{\"end\":55670,\"start\":55664},{\"end\":55683,\"start\":55679},{\"end\":55698,\"start\":55693},{\"end\":55717,\"start\":55707},{\"end\":56030,\"start\":56024},{\"end\":56042,\"start\":56036},{\"end\":56057,\"start\":56049},{\"end\":56069,\"start\":56066},{\"end\":56081,\"start\":56075},{\"end\":56088,\"start\":56086},{\"end\":56102,\"start\":56098},{\"end\":56119,\"start\":56111},{\"end\":56132,\"start\":56127},{\"end\":56492,\"start\":56485},{\"end\":56503,\"start\":56498},{\"end\":56519,\"start\":56514},{\"end\":56539,\"start\":56529},{\"end\":56555,\"start\":56550},{\"end\":56852,\"start\":56845},{\"end\":56863,\"start\":56858},{\"end\":56880,\"start\":56874},{\"end\":57143,\"start\":57140},{\"end\":57157,\"start\":57149},{\"end\":57175,\"start\":57168},{\"end\":57192,\"start\":57183},{\"end\":57207,\"start\":57202},{\"end\":57224,\"start\":57214},{\"end\":57240,\"start\":57229},{\"end\":57618,\"start\":57603},{\"end\":57630,\"start\":57626},{\"end\":57639,\"start\":57636},{\"end\":57653,\"start\":57647},{\"end\":57668,\"start\":57662},{\"end\":58040,\"start\":58032},{\"end\":58062,\"start\":58061},{\"end\":58077,\"start\":58070},{\"end\":58485,\"start\":58480},{\"end\":58497,\"start\":58494},{\"end\":58508,\"start\":58504},{\"end\":58527,\"start\":58517},{\"end\":58540,\"start\":58538},{\"end\":58553,\"start\":58545},{\"end\":58870,\"start\":58864},{\"end\":58886,\"start\":58878},{\"end\":58897,\"start\":58894},{\"end\":58910,\"start\":58903},{\"end\":58923,\"start\":58917},{\"end\":58933,\"start\":58930},{\"end\":58942,\"start\":58939},{\"end\":58951,\"start\":58949},{\"end\":58961,\"start\":58956},{\"end\":58979,\"start\":58972},{\"end\":59404,\"start\":59403},{\"end\":59420,\"start\":59413},{\"end\":59429,\"start\":59427},{\"end\":59447,\"start\":59436},{\"end\":59461,\"start\":59455},{\"end\":59475,\"start\":59474},{\"end\":59488,\"start\":59482},{\"end\":59848,\"start\":59844},{\"end\":59861,\"start\":59856},{\"end\":60117,\"start\":60113},{\"end\":60125,\"start\":60124},{\"end\":60140,\"start\":60134},{\"end\":60154,\"start\":60153},{\"end\":60165,\"start\":60161},{\"end\":60481,\"start\":60480},{\"end\":60498,\"start\":60493},{\"end\":60513,\"start\":60507},{\"end\":60530,\"start\":60525},{\"end\":60551,\"start\":60540},{\"end\":60929,\"start\":60924},{\"end\":60943,\"start\":60938},{\"end\":61179,\"start\":61171},{\"end\":61194,\"start\":61187},{\"end\":61210,\"start\":61204},{\"end\":61228,\"start\":61220},{\"end\":61440,\"start\":61435},{\"end\":61453,\"start\":61447},{\"end\":61472,\"start\":61465},{\"end\":61808,\"start\":61807},{\"end\":61823,\"start\":61818},{\"end\":61836,\"start\":61832},{\"end\":61838,\"start\":61837},{\"end\":62289,\"start\":62285},{\"end\":62304,\"start\":62296},{\"end\":62314,\"start\":62311},{\"end\":62559,\"start\":62553},{\"end\":62571,\"start\":62567},{\"end\":62590,\"start\":62584},{\"end\":62853,\"start\":62844},{\"end\":62871,\"start\":62863},{\"end\":62885,\"start\":62881},{\"end\":62901,\"start\":62897},{\"end\":63230,\"start\":63222},{\"end\":63237,\"start\":63236},{\"end\":63577,\"start\":63573},{\"end\":63589,\"start\":63584},{\"end\":63603,\"start\":63596},{\"end\":63618,\"start\":63614},{\"end\":63630,\"start\":63627},{\"end\":63959,\"start\":63957},{\"end\":63972,\"start\":63965},{\"end\":63986,\"start\":63981},{\"end\":63998,\"start\":63994},{\"end\":64010,\"start\":64007},{\"end\":64268,\"start\":64263},{\"end\":64280,\"start\":64274},{\"end\":64292,\"start\":64286},{\"end\":64309,\"start\":64299},{\"end\":64323,\"start\":64317},{\"end\":64549,\"start\":64540},{\"end\":64566,\"start\":64558},{\"end\":64582,\"start\":64576},{\"end\":64839,\"start\":64835},{\"end\":64852,\"start\":64845},{\"end\":64860,\"start\":64853},{\"end\":64876,\"start\":64868},{\"end\":64886,\"start\":64885},{\"end\":64899,\"start\":64893},{\"end\":65168,\"start\":65162},{\"end\":65185,\"start\":65180},{\"end\":65363,\"start\":65357},{\"end\":65380,\"start\":65371},{\"end\":65393,\"start\":65388},{\"end\":65661,\"start\":65657},{\"end\":65674,\"start\":65669},{\"end\":65690,\"start\":65682},{\"end\":65703,\"start\":65697},{\"end\":65710,\"start\":65708},{\"end\":66010,\"start\":66004},{\"end\":66021,\"start\":66016},{\"end\":66036,\"start\":66027},{\"end\":66049,\"start\":66042},{\"end\":66331,\"start\":66325},{\"end\":66339,\"start\":66338},{\"end\":66352,\"start\":66346},{\"end\":66368,\"start\":66360},{\"end\":66385,\"start\":66377},{\"end\":66397,\"start\":66391},{\"end\":66713,\"start\":66707},{\"end\":66721,\"start\":66720},{\"end\":67014,\"start\":67008},{\"end\":67026,\"start\":67021},{\"end\":67046,\"start\":67038},{\"end\":67061,\"start\":67055}]", "bib_author_last_name": "[{\"end\":45231,\"start\":45223},{\"end\":45245,\"start\":45238},{\"end\":45263,\"start\":45256},{\"end\":45587,\"start\":45583},{\"end\":45604,\"start\":45597},{\"end\":45615,\"start\":45612},{\"end\":45634,\"start\":45624},{\"end\":45647,\"start\":45643},{\"end\":45664,\"start\":45657},{\"end\":45678,\"start\":45674},{\"end\":45690,\"start\":45684},{\"end\":46157,\"start\":46153},{\"end\":46176,\"start\":46166},{\"end\":46194,\"start\":46186},{\"end\":46208,\"start\":46202},{\"end\":46223,\"start\":46217},{\"end\":46652,\"start\":46645},{\"end\":46664,\"start\":46659},{\"end\":46684,\"start\":46673},{\"end\":47058,\"start\":47046},{\"end\":47074,\"start\":47070},{\"end\":47086,\"start\":47082},{\"end\":47104,\"start\":47095},{\"end\":47108,\"start\":47106},{\"end\":47121,\"start\":47110},{\"end\":47452,\"start\":47448},{\"end\":47462,\"start\":47458},{\"end\":47478,\"start\":47472},{\"end\":47489,\"start\":47487},{\"end\":47497,\"start\":47495},{\"end\":47509,\"start\":47502},{\"end\":47921,\"start\":47914},{\"end\":47931,\"start\":47925},{\"end\":47939,\"start\":47933},{\"end\":48114,\"start\":48107},{\"end\":48124,\"start\":48118},{\"end\":48132,\"start\":48126},{\"end\":48482,\"start\":48475},{\"end\":48495,\"start\":48490},{\"end\":48511,\"start\":48505},{\"end\":48965,\"start\":48953},{\"end\":48979,\"start\":48976},{\"end\":48993,\"start\":48989},{\"end\":49006,\"start\":49003},{\"end\":49015,\"start\":49013},{\"end\":49019,\"start\":49017},{\"end\":49337,\"start\":49333},{\"end\":49352,\"start\":49346},{\"end\":49369,\"start\":49362},{\"end\":49382,\"start\":49377},{\"end\":49601,\"start\":49593},{\"end\":49770,\"start\":49762},{\"end\":49789,\"start\":49778},{\"end\":49807,\"start\":49799},{\"end\":50025,\"start\":50018},{\"end\":50042,\"start\":50036},{\"end\":50060,\"start\":50054},{\"end\":50076,\"start\":50067},{\"end\":50088,\"start\":50083},{\"end\":50099,\"start\":50092},{\"end\":50614,\"start\":50611},{\"end\":50626,\"start\":50623},{\"end\":50637,\"start\":50634},{\"end\":51050,\"start\":51044},{\"end\":51061,\"start\":51057},{\"end\":51075,\"start\":51069},{\"end\":51092,\"start\":51083},{\"end\":51105,\"start\":51099},{\"end\":51118,\"start\":51113},{\"end\":51132,\"start\":51125},{\"end\":51150,\"start\":51142},{\"end\":51167,\"start\":51159},{\"end\":51180,\"start\":51174},{\"end\":51513,\"start\":51511},{\"end\":51528,\"start\":51523},{\"end\":51542,\"start\":51539},{\"end\":51552,\"start\":51549},{\"end\":51999,\"start\":51992},{\"end\":52018,\"start\":52009},{\"end\":52030,\"start\":52024},{\"end\":52037,\"start\":52032},{\"end\":52045,\"start\":52041},{\"end\":52059,\"start\":52052},{\"end\":52076,\"start\":52070},{\"end\":52089,\"start\":52084},{\"end\":52103,\"start\":52097},{\"end\":52110,\"start\":52105},{\"end\":52117,\"start\":52114},{\"end\":52132,\"start\":52125},{\"end\":52141,\"start\":52134},{\"end\":52661,\"start\":52655},{\"end\":52670,\"start\":52666},{\"end\":52679,\"start\":52677},{\"end\":52692,\"start\":52688},{\"end\":52713,\"start\":52706},{\"end\":52729,\"start\":52723},{\"end\":52744,\"start\":52738},{\"end\":52763,\"start\":52754},{\"end\":52779,\"start\":52773},{\"end\":52796,\"start\":52787},{\"end\":53200,\"start\":53198},{\"end\":53212,\"start\":53207},{\"end\":53224,\"start\":53218},{\"end\":53239,\"start\":53233},{\"end\":53248,\"start\":53244},{\"end\":53590,\"start\":53585},{\"end\":53602,\"start\":53596},{\"end\":53619,\"start\":53613},{\"end\":54010,\"start\":54000},{\"end\":54027,\"start\":54021},{\"end\":54294,\"start\":54284},{\"end\":54310,\"start\":54301},{\"end\":54329,\"start\":54323},{\"end\":54657,\"start\":54651},{\"end\":54675,\"start\":54666},{\"end\":54691,\"start\":54683},{\"end\":54876,\"start\":54873},{\"end\":54887,\"start\":54884},{\"end\":54899,\"start\":54896},{\"end\":54912,\"start\":54909},{\"end\":54927,\"start\":54924},{\"end\":55202,\"start\":55199},{\"end\":55215,\"start\":55210},{\"end\":55230,\"start\":55222},{\"end\":55619,\"start\":55616},{\"end\":55634,\"start\":55629},{\"end\":55650,\"start\":55642},{\"end\":55662,\"start\":55658},{\"end\":55677,\"start\":55671},{\"end\":55691,\"start\":55684},{\"end\":55705,\"start\":55699},{\"end\":55725,\"start\":55718},{\"end\":56034,\"start\":56031},{\"end\":56047,\"start\":56043},{\"end\":56064,\"start\":56058},{\"end\":56073,\"start\":56070},{\"end\":56084,\"start\":56082},{\"end\":56096,\"start\":56089},{\"end\":56109,\"start\":56103},{\"end\":56125,\"start\":56120},{\"end\":56139,\"start\":56133},{\"end\":56496,\"start\":56493},{\"end\":56512,\"start\":56504},{\"end\":56527,\"start\":56520},{\"end\":56548,\"start\":56540},{\"end\":56567,\"start\":56556},{\"end\":56856,\"start\":56853},{\"end\":56872,\"start\":56864},{\"end\":56885,\"start\":56881},{\"end\":57147,\"start\":57144},{\"end\":57166,\"start\":57158},{\"end\":57181,\"start\":57176},{\"end\":57200,\"start\":57193},{\"end\":57212,\"start\":57208},{\"end\":57227,\"start\":57225},{\"end\":57245,\"start\":57241},{\"end\":57624,\"start\":57619},{\"end\":57634,\"start\":57631},{\"end\":57645,\"start\":57640},{\"end\":57660,\"start\":57654},{\"end\":57674,\"start\":57669},{\"end\":58053,\"start\":58041},{\"end\":58059,\"start\":58055},{\"end\":58068,\"start\":58063},{\"end\":58081,\"start\":58078},{\"end\":58085,\"start\":58083},{\"end\":58492,\"start\":58486},{\"end\":58502,\"start\":58498},{\"end\":58515,\"start\":58509},{\"end\":58536,\"start\":58528},{\"end\":58543,\"start\":58541},{\"end\":58556,\"start\":58554},{\"end\":58876,\"start\":58871},{\"end\":58892,\"start\":58887},{\"end\":58901,\"start\":58898},{\"end\":58915,\"start\":58911},{\"end\":58928,\"start\":58924},{\"end\":58937,\"start\":58934},{\"end\":58947,\"start\":58943},{\"end\":58954,\"start\":58952},{\"end\":58970,\"start\":58962},{\"end\":58986,\"start\":58980},{\"end\":59411,\"start\":59405},{\"end\":59425,\"start\":59421},{\"end\":59434,\"start\":59430},{\"end\":59453,\"start\":59448},{\"end\":59466,\"start\":59462},{\"end\":59472,\"start\":59468},{\"end\":59480,\"start\":59476},{\"end\":59494,\"start\":59489},{\"end\":59498,\"start\":59496},{\"end\":59854,\"start\":59849},{\"end\":59866,\"start\":59862},{\"end\":60122,\"start\":60118},{\"end\":60132,\"start\":60126},{\"end\":60145,\"start\":60141},{\"end\":60151,\"start\":60147},{\"end\":60159,\"start\":60155},{\"end\":60168,\"start\":60166},{\"end\":60174,\"start\":60170},{\"end\":60491,\"start\":60482},{\"end\":60505,\"start\":60499},{\"end\":60523,\"start\":60514},{\"end\":60538,\"start\":60531},{\"end\":60559,\"start\":60552},{\"end\":60563,\"start\":60561},{\"end\":60936,\"start\":60930},{\"end\":60951,\"start\":60944},{\"end\":61185,\"start\":61180},{\"end\":61202,\"start\":61195},{\"end\":61218,\"start\":61211},{\"end\":61236,\"start\":61229},{\"end\":61445,\"start\":61441},{\"end\":61463,\"start\":61454},{\"end\":61478,\"start\":61473},{\"end\":61816,\"start\":61809},{\"end\":61830,\"start\":61824},{\"end\":61849,\"start\":61839},{\"end\":61856,\"start\":61851},{\"end\":62294,\"start\":62290},{\"end\":62309,\"start\":62305},{\"end\":62323,\"start\":62315},{\"end\":62565,\"start\":62560},{\"end\":62582,\"start\":62572},{\"end\":62596,\"start\":62591},{\"end\":62861,\"start\":62854},{\"end\":62879,\"start\":62872},{\"end\":62895,\"start\":62886},{\"end\":62907,\"start\":62902},{\"end\":63234,\"start\":63231},{\"end\":63242,\"start\":63238},{\"end\":63246,\"start\":63244},{\"end\":63260,\"start\":63248},{\"end\":63582,\"start\":63578},{\"end\":63594,\"start\":63590},{\"end\":63612,\"start\":63604},{\"end\":63625,\"start\":63619},{\"end\":63635,\"start\":63631},{\"end\":63963,\"start\":63960},{\"end\":63979,\"start\":63973},{\"end\":63992,\"start\":63987},{\"end\":64005,\"start\":63999},{\"end\":64017,\"start\":64011},{\"end\":64272,\"start\":64269},{\"end\":64284,\"start\":64281},{\"end\":64297,\"start\":64293},{\"end\":64315,\"start\":64310},{\"end\":64326,\"start\":64324},{\"end\":64556,\"start\":64550},{\"end\":64574,\"start\":64567},{\"end\":64587,\"start\":64583},{\"end\":64843,\"start\":64840},{\"end\":64866,\"start\":64861},{\"end\":64883,\"start\":64877},{\"end\":64891,\"start\":64887},{\"end\":64905,\"start\":64900},{\"end\":64913,\"start\":64907},{\"end\":65178,\"start\":65169},{\"end\":65195,\"start\":65186},{\"end\":65369,\"start\":65364},{\"end\":65386,\"start\":65381},{\"end\":65408,\"start\":65394},{\"end\":65419,\"start\":65410},{\"end\":65667,\"start\":65662},{\"end\":65680,\"start\":65675},{\"end\":65695,\"start\":65691},{\"end\":65706,\"start\":65704},{\"end\":65715,\"start\":65711},{\"end\":66014,\"start\":66011},{\"end\":66025,\"start\":66022},{\"end\":66040,\"start\":66037},{\"end\":66052,\"start\":66050},{\"end\":66336,\"start\":66332},{\"end\":66344,\"start\":66340},{\"end\":66358,\"start\":66353},{\"end\":66375,\"start\":66369},{\"end\":66389,\"start\":66386},{\"end\":66404,\"start\":66398},{\"end\":66408,\"start\":66406},{\"end\":66718,\"start\":66714},{\"end\":66726,\"start\":66722},{\"end\":66730,\"start\":66728},{\"end\":67019,\"start\":67015},{\"end\":67036,\"start\":67027},{\"end\":67053,\"start\":67047},{\"end\":67064,\"start\":67062}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1711.04340\",\"id\":\"b0\"},\"end\":45491,\"start\":45215},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52195590},\"end\":46026,\"start\":45493},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3429309},\"end\":46580,\"start\":46028},{\"attributes\":{\"id\":\"b3\"},\"end\":47037,\"start\":46582},{\"attributes\":{\"doi\":\"arXiv:1805.09501\",\"id\":\"b4\"},\"end\":47389,\"start\":47039},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":57246310},\"end\":47864,\"start\":47391},{\"attributes\":{\"doi\":\"arXiv:1702.05538\",\"id\":\"b6\"},\"end\":48096,\"start\":47866},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b7\"},\"end\":48389,\"start\":48098},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2229234},\"end\":48860,\"start\":48391},{\"attributes\":{\"doi\":\"arXiv:1908.07801\",\"id\":\"b9\"},\"end\":49256,\"start\":48862},{\"attributes\":{\"doi\":\"arXiv:1901.10513\",\"id\":\"b10\"},\"end\":49584,\"start\":49258},{\"attributes\":{\"doi\":\"arXiv:1705.07485\",\"id\":\"b11\"},\"end\":49753,\"start\":49586},{\"attributes\":{\"id\":\"b12\"},\"end\":49956,\"start\":49755},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":19971112},\"end\":50566,\"start\":49958},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5398883},\"end\":50982,\"start\":50568},{\"attributes\":{\"doi\":\"arXiv:1412.5567\",\"id\":\"b15\"},\"end\":51455,\"start\":50984},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206594692},\"end\":51928,\"start\":51457},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8810481},\"end\":52578,\"start\":51930},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7230302},\"end\":53105,\"start\":52580},{\"attributes\":{\"doi\":\"arXiv:1905.05393\",\"id\":\"b19\"},\"end\":53484,\"start\":53107},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":24413267},\"end\":53938,\"start\":53486},{\"attributes\":{\"id\":\"b21\"},\"end\":54212,\"start\":53940},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195908774},\"end\":54575,\"start\":54214},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16846534},\"end\":54861,\"start\":54577},{\"attributes\":{\"doi\":\"arXiv:1905.00397\",\"id\":\"b24\"},\"end\":55119,\"start\":54863},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":47252984},\"end\":55562,\"start\":55121},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14113767},\"end\":56022,\"start\":55564},{\"attributes\":{\"doi\":\"arXiv:1712.00559\",\"id\":\"b27\"},\"end\":56419,\"start\":56024},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":23873820},\"end\":56843,\"start\":56421},{\"attributes\":{\"doi\":\"arXiv:1806.09055\",\"id\":\"b29\"},\"end\":57102,\"start\":56845},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2141740},\"end\":57517,\"start\":57104},{\"attributes\":{\"doi\":\"arXiv:1906.02611\",\"id\":\"b31\"},\"end\":57916,\"start\":57519},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":30950104},\"end\":58409,\"start\":57918},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":16852518},\"end\":58862,\"start\":58411},{\"attributes\":{\"doi\":\"arXiv:1908.11069\",\"id\":\"b34\"},\"end\":59320,\"start\":58864},{\"attributes\":{\"doi\":\"arXiv:1904.08779\",\"id\":\"b35\"},\"end\":59758,\"start\":59322},{\"attributes\":{\"doi\":\"arXiv:1712.04621\",\"id\":\"b36\"},\"end\":60051,\"start\":59760},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3638969},\"end\":60403,\"start\":60053},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":384205},\"end\":60857,\"start\":60405},{\"attributes\":{\"doi\":\"arXiv:1905.10887\",\"id\":\"b39\"},\"end\":61120,\"start\":60859},{\"attributes\":{\"doi\":\"arXiv:1902.10811\",\"id\":\"b40\"},\"end\":61433,\"start\":61122},{\"attributes\":{\"doi\":\"arXiv:1505.03229\",\"id\":\"b41\"},\"end\":61719,\"start\":61435},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4659176},\"end\":62237,\"start\":61721},{\"attributes\":{\"doi\":\"arXiv:1611.01331\",\"id\":\"b43\"},\"end\":62487,\"start\":62239},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":632197},\"end\":62842,\"start\":62489},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b45\"},\"end\":63220,\"start\":62844},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b46\"},\"end\":63507,\"start\":63222},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6314431},\"end\":63902,\"start\":63509},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2936324},\"end\":64259,\"start\":63904},{\"attributes\":{\"doi\":\"arXiv:1904.12848\",\"id\":\"b49\"},\"end\":64538,\"start\":64261},{\"attributes\":{\"doi\":\"arXiv:1802.02375\",\"id\":\"b50\"},\"end\":64771,\"start\":64540},{\"attributes\":{\"doi\":\"arXiv:1906.08988\",\"id\":\"b51\"},\"end\":65136,\"start\":64773},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":15276198},\"end\":65355,\"start\":65138},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b53\"},\"end\":65653,\"start\":65357},{\"attributes\":{\"doi\":\"arXiv:1708.04896\",\"id\":\"b54\"},\"end\":65919,\"start\":65655},{\"attributes\":{\"doi\":\"arXiv:1711.00648\",\"id\":\"b55\"},\"end\":66263,\"start\":65921},{\"attributes\":{\"doi\":\"arXiv:1906.11172\",\"id\":\"b56\"},\"end\":66649,\"start\":66265},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":12713052},\"end\":66938,\"start\":66651},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":12227989},\"end\":67423,\"start\":66940}]", "bib_title": "[{\"end\":45569,\"start\":45493},{\"end\":46139,\"start\":46028},{\"end\":46639,\"start\":46582},{\"end\":47442,\"start\":47391},{\"end\":48463,\"start\":48391},{\"end\":50009,\"start\":49958},{\"end\":50600,\"start\":50568},{\"end\":51501,\"start\":51457},{\"end\":51984,\"start\":51930},{\"end\":52644,\"start\":52580},{\"end\":53575,\"start\":53486},{\"end\":54277,\"start\":54214},{\"end\":54642,\"start\":54577},{\"end\":55188,\"start\":55121},{\"end\":55605,\"start\":55564},{\"end\":56483,\"start\":56421},{\"end\":57138,\"start\":57104},{\"end\":58030,\"start\":57918},{\"end\":58478,\"start\":58411},{\"end\":60111,\"start\":60053},{\"end\":60478,\"start\":60405},{\"end\":61805,\"start\":61721},{\"end\":62551,\"start\":62489},{\"end\":63571,\"start\":63509},{\"end\":63955,\"start\":63904},{\"end\":65160,\"start\":65138},{\"end\":66705,\"start\":66651},{\"end\":67006,\"start\":66940}]", "bib_author": "[{\"end\":45233,\"start\":45215},{\"end\":45247,\"start\":45233},{\"end\":45265,\"start\":45247},{\"end\":45589,\"start\":45571},{\"end\":45606,\"start\":45589},{\"end\":45617,\"start\":45606},{\"end\":45636,\"start\":45617},{\"end\":45649,\"start\":45636},{\"end\":45666,\"start\":45649},{\"end\":45680,\"start\":45666},{\"end\":45692,\"start\":45680},{\"end\":46159,\"start\":46141},{\"end\":46178,\"start\":46159},{\"end\":46196,\"start\":46178},{\"end\":46210,\"start\":46196},{\"end\":46225,\"start\":46210},{\"end\":46654,\"start\":46641},{\"end\":46666,\"start\":46654},{\"end\":46686,\"start\":46666},{\"end\":47060,\"start\":47039},{\"end\":47076,\"start\":47060},{\"end\":47088,\"start\":47076},{\"end\":47106,\"start\":47088},{\"end\":47110,\"start\":47106},{\"end\":47123,\"start\":47110},{\"end\":47454,\"start\":47444},{\"end\":47464,\"start\":47454},{\"end\":47480,\"start\":47464},{\"end\":47491,\"start\":47480},{\"end\":47499,\"start\":47491},{\"end\":47511,\"start\":47499},{\"end\":47923,\"start\":47905},{\"end\":47933,\"start\":47923},{\"end\":47941,\"start\":47933},{\"end\":48116,\"start\":48098},{\"end\":48126,\"start\":48116},{\"end\":48134,\"start\":48126},{\"end\":48484,\"start\":48465},{\"end\":48497,\"start\":48484},{\"end\":48513,\"start\":48497},{\"end\":48967,\"start\":48945},{\"end\":48981,\"start\":48967},{\"end\":48995,\"start\":48981},{\"end\":49008,\"start\":48995},{\"end\":49017,\"start\":49008},{\"end\":49021,\"start\":49017},{\"end\":49339,\"start\":49329},{\"end\":49354,\"start\":49339},{\"end\":49371,\"start\":49354},{\"end\":49384,\"start\":49371},{\"end\":49603,\"start\":49586},{\"end\":49772,\"start\":49757},{\"end\":49791,\"start\":49772},{\"end\":49809,\"start\":49791},{\"end\":50027,\"start\":50011},{\"end\":50044,\"start\":50027},{\"end\":50062,\"start\":50044},{\"end\":50078,\"start\":50062},{\"end\":50090,\"start\":50078},{\"end\":50101,\"start\":50090},{\"end\":50616,\"start\":50602},{\"end\":50628,\"start\":50616},{\"end\":50639,\"start\":50628},{\"end\":51052,\"start\":51039},{\"end\":51063,\"start\":51052},{\"end\":51077,\"start\":51063},{\"end\":51094,\"start\":51077},{\"end\":51107,\"start\":51094},{\"end\":51120,\"start\":51107},{\"end\":51134,\"start\":51120},{\"end\":51152,\"start\":51134},{\"end\":51169,\"start\":51152},{\"end\":51182,\"start\":51169},{\"end\":51515,\"start\":51503},{\"end\":51530,\"start\":51515},{\"end\":51544,\"start\":51530},{\"end\":51554,\"start\":51544},{\"end\":52001,\"start\":51986},{\"end\":52020,\"start\":52001},{\"end\":52032,\"start\":52020},{\"end\":52039,\"start\":52032},{\"end\":52047,\"start\":52039},{\"end\":52061,\"start\":52047},{\"end\":52078,\"start\":52061},{\"end\":52091,\"start\":52078},{\"end\":52105,\"start\":52091},{\"end\":52112,\"start\":52105},{\"end\":52119,\"start\":52112},{\"end\":52134,\"start\":52119},{\"end\":52143,\"start\":52134},{\"end\":52663,\"start\":52646},{\"end\":52672,\"start\":52663},{\"end\":52681,\"start\":52672},{\"end\":52694,\"start\":52681},{\"end\":52715,\"start\":52694},{\"end\":52731,\"start\":52715},{\"end\":52746,\"start\":52731},{\"end\":52765,\"start\":52746},{\"end\":52781,\"start\":52765},{\"end\":52798,\"start\":52781},{\"end\":53202,\"start\":53191},{\"end\":53214,\"start\":53202},{\"end\":53226,\"start\":53214},{\"end\":53241,\"start\":53226},{\"end\":53250,\"start\":53241},{\"end\":53592,\"start\":53577},{\"end\":53604,\"start\":53592},{\"end\":53621,\"start\":53604},{\"end\":54012,\"start\":53995},{\"end\":54029,\"start\":54012},{\"end\":54296,\"start\":54279},{\"end\":54312,\"start\":54296},{\"end\":54331,\"start\":54312},{\"end\":54659,\"start\":54644},{\"end\":54677,\"start\":54659},{\"end\":54693,\"start\":54677},{\"end\":54878,\"start\":54865},{\"end\":54889,\"start\":54878},{\"end\":54901,\"start\":54889},{\"end\":54914,\"start\":54901},{\"end\":54929,\"start\":54914},{\"end\":55204,\"start\":55190},{\"end\":55217,\"start\":55204},{\"end\":55232,\"start\":55217},{\"end\":55621,\"start\":55607},{\"end\":55636,\"start\":55621},{\"end\":55652,\"start\":55636},{\"end\":55664,\"start\":55652},{\"end\":55679,\"start\":55664},{\"end\":55693,\"start\":55679},{\"end\":55707,\"start\":55693},{\"end\":55727,\"start\":55707},{\"end\":56036,\"start\":56024},{\"end\":56049,\"start\":56036},{\"end\":56066,\"start\":56049},{\"end\":56075,\"start\":56066},{\"end\":56086,\"start\":56075},{\"end\":56098,\"start\":56086},{\"end\":56111,\"start\":56098},{\"end\":56127,\"start\":56111},{\"end\":56141,\"start\":56127},{\"end\":56498,\"start\":56485},{\"end\":56514,\"start\":56498},{\"end\":56529,\"start\":56514},{\"end\":56550,\"start\":56529},{\"end\":56569,\"start\":56550},{\"end\":56858,\"start\":56845},{\"end\":56874,\"start\":56858},{\"end\":56887,\"start\":56874},{\"end\":57149,\"start\":57140},{\"end\":57168,\"start\":57149},{\"end\":57183,\"start\":57168},{\"end\":57202,\"start\":57183},{\"end\":57214,\"start\":57202},{\"end\":57229,\"start\":57214},{\"end\":57247,\"start\":57229},{\"end\":57626,\"start\":57603},{\"end\":57636,\"start\":57626},{\"end\":57647,\"start\":57636},{\"end\":57662,\"start\":57647},{\"end\":57676,\"start\":57662},{\"end\":58055,\"start\":58032},{\"end\":58061,\"start\":58055},{\"end\":58070,\"start\":58061},{\"end\":58083,\"start\":58070},{\"end\":58087,\"start\":58083},{\"end\":58494,\"start\":58480},{\"end\":58504,\"start\":58494},{\"end\":58517,\"start\":58504},{\"end\":58538,\"start\":58517},{\"end\":58545,\"start\":58538},{\"end\":58558,\"start\":58545},{\"end\":58878,\"start\":58864},{\"end\":58894,\"start\":58878},{\"end\":58903,\"start\":58894},{\"end\":58917,\"start\":58903},{\"end\":58930,\"start\":58917},{\"end\":58939,\"start\":58930},{\"end\":58949,\"start\":58939},{\"end\":58956,\"start\":58949},{\"end\":58972,\"start\":58956},{\"end\":58988,\"start\":58972},{\"end\":59413,\"start\":59403},{\"end\":59427,\"start\":59413},{\"end\":59436,\"start\":59427},{\"end\":59455,\"start\":59436},{\"end\":59468,\"start\":59455},{\"end\":59474,\"start\":59468},{\"end\":59482,\"start\":59474},{\"end\":59496,\"start\":59482},{\"end\":59500,\"start\":59496},{\"end\":59856,\"start\":59844},{\"end\":59868,\"start\":59856},{\"end\":60124,\"start\":60113},{\"end\":60134,\"start\":60124},{\"end\":60147,\"start\":60134},{\"end\":60153,\"start\":60147},{\"end\":60161,\"start\":60153},{\"end\":60170,\"start\":60161},{\"end\":60176,\"start\":60170},{\"end\":60493,\"start\":60480},{\"end\":60507,\"start\":60493},{\"end\":60525,\"start\":60507},{\"end\":60540,\"start\":60525},{\"end\":60561,\"start\":60540},{\"end\":60565,\"start\":60561},{\"end\":60938,\"start\":60924},{\"end\":60953,\"start\":60938},{\"end\":61187,\"start\":61171},{\"end\":61204,\"start\":61187},{\"end\":61220,\"start\":61204},{\"end\":61238,\"start\":61220},{\"end\":61447,\"start\":61435},{\"end\":61465,\"start\":61447},{\"end\":61480,\"start\":61465},{\"end\":61818,\"start\":61807},{\"end\":61832,\"start\":61818},{\"end\":61851,\"start\":61832},{\"end\":61858,\"start\":61851},{\"end\":62296,\"start\":62285},{\"end\":62311,\"start\":62296},{\"end\":62325,\"start\":62311},{\"end\":62567,\"start\":62553},{\"end\":62584,\"start\":62567},{\"end\":62598,\"start\":62584},{\"end\":62863,\"start\":62844},{\"end\":62881,\"start\":62863},{\"end\":62897,\"start\":62881},{\"end\":62909,\"start\":62897},{\"end\":63236,\"start\":63222},{\"end\":63244,\"start\":63236},{\"end\":63248,\"start\":63244},{\"end\":63262,\"start\":63248},{\"end\":63584,\"start\":63573},{\"end\":63596,\"start\":63584},{\"end\":63614,\"start\":63596},{\"end\":63627,\"start\":63614},{\"end\":63637,\"start\":63627},{\"end\":63965,\"start\":63957},{\"end\":63981,\"start\":63965},{\"end\":63994,\"start\":63981},{\"end\":64007,\"start\":63994},{\"end\":64019,\"start\":64007},{\"end\":64274,\"start\":64263},{\"end\":64286,\"start\":64274},{\"end\":64299,\"start\":64286},{\"end\":64317,\"start\":64299},{\"end\":64328,\"start\":64317},{\"end\":64558,\"start\":64540},{\"end\":64576,\"start\":64558},{\"end\":64589,\"start\":64576},{\"end\":64845,\"start\":64835},{\"end\":64868,\"start\":64845},{\"end\":64885,\"start\":64868},{\"end\":64893,\"start\":64885},{\"end\":64907,\"start\":64893},{\"end\":64915,\"start\":64907},{\"end\":65180,\"start\":65162},{\"end\":65197,\"start\":65180},{\"end\":65371,\"start\":65357},{\"end\":65388,\"start\":65371},{\"end\":65410,\"start\":65388},{\"end\":65421,\"start\":65410},{\"end\":65669,\"start\":65657},{\"end\":65682,\"start\":65669},{\"end\":65697,\"start\":65682},{\"end\":65708,\"start\":65697},{\"end\":65717,\"start\":65708},{\"end\":66016,\"start\":66004},{\"end\":66027,\"start\":66016},{\"end\":66042,\"start\":66027},{\"end\":66054,\"start\":66042},{\"end\":66338,\"start\":66325},{\"end\":66346,\"start\":66338},{\"end\":66360,\"start\":66346},{\"end\":66377,\"start\":66360},{\"end\":66391,\"start\":66377},{\"end\":66406,\"start\":66391},{\"end\":66410,\"start\":66406},{\"end\":66720,\"start\":66707},{\"end\":66728,\"start\":66720},{\"end\":66732,\"start\":66728},{\"end\":67021,\"start\":67008},{\"end\":67038,\"start\":67021},{\"end\":67055,\"start\":67038},{\"end\":67066,\"start\":67055}]", "bib_venue": "[{\"end\":46819,\"start\":46761},{\"end\":47644,\"start\":47586},{\"end\":48634,\"start\":48582},{\"end\":50284,\"start\":50201},{\"end\":50786,\"start\":50721},{\"end\":51709,\"start\":51640},{\"end\":55353,\"start\":55301},{\"end\":61555,\"start\":61551},{\"end\":61997,\"start\":61936},{\"end\":67199,\"start\":67141},{\"end\":45330,\"start\":45281},{\"end\":45741,\"start\":45692},{\"end\":46287,\"start\":46225},{\"end\":46759,\"start\":46686},{\"end\":47179,\"start\":47139},{\"end\":47584,\"start\":47511},{\"end\":47903,\"start\":47866},{\"end\":48218,\"start\":48150},{\"end\":48580,\"start\":48513},{\"end\":48943,\"start\":48862},{\"end\":49327,\"start\":49258},{\"end\":49645,\"start\":49619},{\"end\":49848,\"start\":49809},{\"end\":50199,\"start\":50101},{\"end\":50719,\"start\":50639},{\"end\":51037,\"start\":50984},{\"end\":51638,\"start\":51554},{\"end\":52229,\"start\":52143},{\"end\":52829,\"start\":52798},{\"end\":53189,\"start\":53107},{\"end\":53689,\"start\":53621},{\"end\":53993,\"start\":53940},{\"end\":54380,\"start\":54331},{\"end\":54704,\"start\":54693},{\"end\":55299,\"start\":55232},{\"end\":55765,\"start\":55727},{\"end\":56195,\"start\":56157},{\"end\":56621,\"start\":56569},{\"end\":56944,\"start\":56903},{\"end\":57285,\"start\":57247},{\"end\":57601,\"start\":57519},{\"end\":58154,\"start\":58087},{\"end\":58622,\"start\":58558},{\"end\":59061,\"start\":59004},{\"end\":59401,\"start\":59322},{\"end\":59842,\"start\":59760},{\"end\":60220,\"start\":60176},{\"end\":60614,\"start\":60565},{\"end\":60922,\"start\":60859},{\"end\":61169,\"start\":61122},{\"end\":61549,\"start\":61496},{\"end\":61934,\"start\":61858},{\"end\":62283,\"start\":62239},{\"end\":62647,\"start\":62598},{\"end\":63011,\"start\":62924},{\"end\":63336,\"start\":63278},{\"end\":63686,\"start\":63637},{\"end\":64063,\"start\":64019},{\"end\":64629,\"start\":64605},{\"end\":64833,\"start\":64773},{\"end\":65230,\"start\":65197},{\"end\":65478,\"start\":65437},{\"end\":66002,\"start\":65921},{\"end\":66323,\"start\":66265},{\"end\":66784,\"start\":66732},{\"end\":67139,\"start\":67066}]"}}}, "year": 2023, "month": 12, "day": 17}