{"id": 253456740, "updated": "2023-10-29 19:55:01.209", "metadata": {"title": "So Many Fuzzers, So Little Time\u2731: Experience from Evaluating Fuzzers on the Contiki-NG Network (Hay)Stack", "authors": "[{\"first\":\"Clement\",\"last\":\"Poncelet\",\"middle\":[]},{\"first\":\"Konstantinos\",\"last\":\"Sagonas\",\"middle\":[]},{\"first\":\"Nicolas\",\"last\":\"Tsiftes\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Fuzz testing (\u201cfuzzing\u201d) is a widely-used and effective dynamic technique to discover crashes and security vulnerabilities in software, supported by numerous tools, which keep improving in terms of their detection capabilities and speed of execution. In this paper, we report our findings from using state-of-the-art mutation-based and hybrid fuzzers (AFL, Angora, Honggfuzz, Intriguer, MOpt-AFL, QSym, and SymCC) on a non-trivial code base, that of Contiki-NG, to expose and fix serious vulnerabilities in various layers of its network stack, during a period of more than three years. As a by-product, we provide a Git-based platform which allowed us to create and apply a new, quite challenging, open-source bug suite for evaluating fuzzers on real-world software vulnerabilities. Using this bug suite, we present an impartial and extensive evaluation of the effectiveness of these fuzzers, and measure the impact that sanitizers have on it. Finally, we offer our experiences and opinions on how fuzzing tools should be used and evaluated in the future.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/kbse/PonceletST22", "doi": "10.1145/3551349.3556946"}}, "content": {"source": {"pdf_hash": "481691d57683431258c66ae2310d09537f8b22a3", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "12d27ff46f17fc9415f6d8a2f3eb922368dbda60", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/481691d57683431258c66ae2310d09537f8b22a3.txt", "contents": "\nSo Many Fuzzers, So Little Time * Experience from Evaluating Fuzzers on the Contiki-NG Network (Hay)Stack\n\n\nCl\u00e9ment Poncelet clement.poncelet@it.uu.se \nNicolas Tsiftes nicolas.tsiftes@ri.se \n\nUppsala University Uppsala\nSweden Konstantinos Sagonas\n\n\nUppsala University Uppsala\nSweden\n\n\nRISE Research Institutes of Sweden Stockholm\nNational Technical University of Athens Athens\nGreece, Sweden\n\n\nKTH Digital Futures Stockholm\nSweden\n\nSo Many Fuzzers, So Little Time * Experience from Evaluating Fuzzers on the Contiki-NG Network (Hay)Stack\nACM Reference Format: Cl\u00e9ment Poncelet, Konstantinos Sagonas, and Nicolas Tsiftes. 2022. So Many Fuzzers, So Little Time: Experience from Evaluating Fuzzers on the Contiki-NG Network (Hay)Stack. In 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10-14, 2022, Rochester, MI, USA. ACM, New York, NY, USA, 12 pages. https://Software securitysecurity testingfuzz testingcoverage-guided fuzzinghybrid fuzzingIoTContiki-NG\nFuzz testing (\"fuzzing\") is a widely-used and effective dynamic technique to discover crashes and security vulnerabilities in software, supported by numerous tools, which keep improving in terms of their detection capabilities and speed of execution. In this paper, we report our findings from using state-of-the-art mutation-based and hybrid fuzzers (AFL, Angora, Honggfuzz, Intriguer, MOpt-AFL, QSym, and SymCC) on a non-trivial code base, that of Contiki-NG, to expose and fix serious vulnerabilities in various layers of its network stack, during a period of more than three years. As a by-product, we provide a Git-based platform which allowed us to create and apply a new, quite challenging, open-source bug suite for evaluating fuzzers on real-world software vulnerabilities. Using this bug suite, we present an impartial and extensive evaluation of the effectiveness of these fuzzers, and measure the impact that sanitizers have on it. Finally, we offer our experiences and opinions on how fuzzing tools should be used and evaluated in the future.CCS CONCEPTS\u2022 Security and privacy \u2192 Software security engineering; \u2022 Software and its engineering \u2192 Software defect analysis.\n\nINTRODUCTION\n\nA famous quote attributed to Dijkstra states that \"If debugging is the process of removing software bugs, then programming must be the process of putting them in. \" Perhaps due to most programming still done by humans, all non-trivial software contains bugs. Some of these bugs are serious vulnerabilities and often exploitable. Moreover, there exist application domains where pretty much all serious software vulnerabilities are exploitable. One such domain, of interest to us, is that of IoT systems; more specifically, that of operating systems for resource-constrained IoT devices. In this domain, a software bug that crashes the OS has severe consequences: it results in DoS at best or, worse, brings down the whole IoT system (e.g., a home alarm, an environmental monitoring system, etc.) permanently. So, naturally, developers and researchers interested in such OSes want techniques and tools that help them find such software bugs and fix them before they make it into OS releases.\n\nIn recent years, a successful technique to discover crashes in software is fuzz testing [16] or simply fuzzing. In this paper, we specifically focus on coverage-guided grey-box fuzzing [27,30] techniques. Since mid 2018, we have jumped on the so-called fuzzing hype-train [19], and have been using various mutation-based greybox and hybrid fuzzers to discover bugs in Contiki-NG [17], the Next Generation OS for IoT devices forked from its predecessor, Contiki. During the same period, we have been following closely the research published in the area, esp. that related to hybrid fuzzing techniques, and trying out fuzzers which were already available or ever since released as open source. Some research questions that naturally come to mind from engaging in such a project are:\n\nRQ.1 (Effectiveness) Are hybrid fuzzers superior in exposing bugs and vulnerabilities than mutation-based fuzzers? RQ.2 (Efficiency) Do some of the fuzzers employ techniques which allow them to expose bugs fast? If so, which? RQ.3 (Consistency) Are any fuzzer implementations able to expose (some of) the bugs in all/most of their runs?\n\nThis paper provides its answers to these questions by following a systematic evaluation. More precisely, we employ a total of eight different fuzzers, four mutation-based (two AFL variants [30], Honggfuzz [27], and MOpt-AFL [14]) and four recent hybrid fuzzers (Angora [4], QSym [29], Intriguer [5], and SymCC [21]) to produce inputs that reliably trigger crashes in the code base of Contiki-NG's low-power IPv6 stack. Over a period of a bit more than three years, we have been precisely documenting these bugs, reporting them to Contiki-NG's developers so that they get fixed, and obtaining CVEs for them when the pull requests which fixed them got merged into its development branch. Our efforts have made Contiki-NG more robust and secure, but have also made us more knowledgeable. Based on the experience we have gained, we have formed some opinions both on how fuzzing tools should be used and on how they should be evaluated and compared to each other. Of course, we are not the first to notice and report views and findings on these topics. For example, several papers published in top conferences and journals [11][12][13] report that various questionable practices have been used to evaluate and compare fuzzing techniques and tools, and that there is a clear need for better, more \"real-world\", and more challenging benchmarks. In this paper, besides corroborating experiences and recommendations on these topics published in the literature, we contribute some of our own. We also mention when we do not fully agree with some of the above papers, and where our findings differ from them and why.\n\nWe hold that reporting such user experiences is valuable for the fuzzing research community anyway, but we go beyond that. We have created benchmarking infrastructure that can be used to evaluate existing and, more importantly, future fuzzing techniques and tools on the Contiki-NG code base. Our paper's artifact, which is available on GitHub, currently comes with a suite of a total of eighteen ground truth bugs, all discovered during our fuzzing train journey, that correspond to vulnerabilities at different layers of Contiki-NG's code base, and are increasingly difficult to expose. The scripts that the artifact contains use the Git commit history in order to rewind and replay the evolution of Contiki-NG's code base starting from a particular commit where a number of these eighteen vulnerabilities have been fixed (and the remaining have not). We also evaluate the eight fuzzers mentioned above on this suite, and report our findings from running several 24-hour experiments for individual ground truth bugs. As we will see, some of the bugs in our suite are currently quite challenging for most contemporary mutation-based and hybrid fuzzers. This is due to inherent characteristics of Contiki-NG's code base: it implements a network stack, and in order to reach some particular layer of its code base, the input has to pass the checks of other layers.\n\nQuite often, fuzzing is aided by sanitizers, i.e., runtime analysis tools that employ instrumentation to discover crashes or undefined behaviour in software. Sanitizer-aided fuzzing has pros and cons: Sanitizers help in exposing and triaging bugs and vulnerabilities more accurately [23], but they also impose a non-negligible runtime overhead, which means that fuzzers explore (significantly) fewer executions within a given time budget when using them. As far as we know, there is no published work that investigates this tradeoff, i.e., reports whether sanitizers have a clear positive effect on bug discovery or whether it is better to not slow down the fuzzer and allow it to explore more inputs during a time-limited fuzzing campaign. This is the fourth and last question we investigate.\n\nRQ.4 (Sanitizer Impact) Do sanitizers pay off for their runtime overhead in terms of exposing more vulnerabilities within a time-limited fuzzing run?\n\nIn summary, this work:\n\n\u2022 Reports findings from using eight state-of-the-art mutationbased and hybrid fuzzers on a non-trivial code base, that of the Contiki-NG OS, to discover and fix serious vulnerabilities, many of which have CVEs (cf. Table 2). \u2022 Presents an impartial evaluation of the effectiveness of these eight fuzzers, not with an aim to declare a \"winner\" among them, but so as to highlight those that are expected to perform well on code bases with Contiki-NG's characteristics.\n\n\u2022 Evaluates the impact that two sanitizers (AddressSanitizer and Effective Type Sanitizer) have on fuzzing tools. \u2022 Offers a new, quite challenging, bug suite for evaluating fuzzers on real-world software bugs. \u2022 Comes with a Git-based platform as artifact, and argues for the pros of using such a benchmarking approach for evaluating fuzzers in terms of their ability to expose vulnerabilities.\n\nThe remainder of this paper starts by presenting relevant information about the fuzzing tools we use and a brief review of Contiki-NG's layered architecture ( \u00a72). The next and main sections of this paper present the details of our ground truth suite with the results of our evaluation ( \u00a73) and our investigation on sanitizer impacts on fuzzing tool effectiveness ( \u00a74). The paper ends with related work ( \u00a75) and some final remarks.\n\n\nBACKGROUND\n\nFirst, in \u00a72.1, we overview the fuzzing techniques and tools we use in our evaluation. Note that we do not aim to present an exhaustive account of fuzzing technology; instead, we concentrate on recently proposed and state-of-the-art mutation-based and hybrid fuzzers, focusing on characteristics that can influence answers to our research questions. We then briefly present Contiki-NG in \u00a72.2.\n\n\nFuzzing\n\nFuzz testing [15,16] discovers software bugs by randomly generating inputs and feeding them to a program under test (the \"target\"). Since most targets expect that their inputs have a specific structure, mutation-based fuzzers demand an initial set of inputs (\"seeds\") from their users, and start applying small mutations to them. Their aim is to generate mutants satisfying the target's consistency checks and manage to penetrate its code deeply. Coverage-guided grey-box fuzzers, such as the popular American Fuzzy Lop (AFL) [30], instrument the target lightly to return to the fuzzer some feedback after each target execution (e.g., which code blocks were visited, information which is maintained in a coverage bitmap in the case of AFL). AFL's instrumentation is implemented by two companion utilities that act as a drop in replacement for gcc and clang: afl-gcc and afl-clang use an ad hoc script to instrument the target at the assembly level. A third utility, afl-clang-fast [1], uses true compiler-level instrumentation, claiming to produce a target configuration that can be up to ten times faster than the corresponding target produced by afl-clang. 1 Mutation-based fuzzers implement a set of mutators (i.e., operators that modify the input in some way), and must choose how frequently to apply each of them. In essence, this is a priority scheduling (i.e., an optimization) problem. However, it is a dynamic optimization problem, because the mutator that will generate an interesting mutant depends on the current input, and may greatly vary during a fuzzing session. MOpt [14] proposes to use a particle swarm optimization algorithm to compute the optimal selection probability distribution over those mutators, instead of using a fixed mutator selection strategy. For our evaluation, we used MOpt-AFL, the authors' algorithm implementation within AFL (henceforth referred to as MOpt, for simplicity). We also include Honggfuzz [27] to our evaluation, a tool recently reported to outperform many other fuzzers in an experimental study similar to ours [2]. This mutation-based fuzzer relies on the ptrace API together with UndefinedSanitizer [28] to detect target's crashes. With this implementation, Honggfuzz provides a different capability of detecting vulnerabilities than AFL, and provides better information to the users about why a crash occurred.\n\nVarious researchers have proposed hybrid fuzzers [18], i.e., fuzzing tools that also run some heavier code analysis tool(s) alongside a mutation-based fuzzer. A hybrid fuzzer feeds its dynamic analysis component with fuzzer's mutants in order to generate inputs that a grey-box fuzzer is unlikely to generate due to its lack of knowledge about the program. The inputs produced by the dynamic analysis component are then fed back into the mutation-based fuzzer, which in turn can then use them to penetrate new 'easily-reachable' code. Driller [26] was the first hybrid fuzzer to popularize this idea, by running a concolic execution engine whenever AFL did not show any signs of progress (i.e., in periods when AFL failed to come up with mutants that increase code coverage). Driller's approach was subsequently adopted and improved upon by the authors of QSym [29]. QSym's concolic execution engine executes an input generated by mutation and, for every encountered branch, it tries to generate inputs that follow the alternative paths. QSym implements a complete concolic engine executing binary instructions, and adds heuristics to further speedup the execution, trading the strict soundness requirements of conventional concolic execution for better performance [29]. Interestingly, Angora [4], which is also a hybrid fuzzer, has opted for a different approach. It uses taint analysis with a machine learning algorithm to generate inputs leading to alternative paths. Taint analysis provides target's instructions and the input bytes they accessed during an execution. With that knowledge, Angora focuses on 'flipping' a target branch by modifying only the tainted bytes of the input. Additionally, Angora extends AFL's coverage feedback component to maintain information at finer granularity, which however requires more time and memory. Intriguer [5] is a fuzzer that combines taint analysis with symbolic execution. From a tainted trace, it removes any mov-like instructions, and reconstructs the symbolic expressions by mapping input to machine instructions' values. As a result, Intriguer's symbolic execution component becomes cheaper at the cost of the previous steps. Finally, SymCC [21], a more recent hybrid fuzzer, directly embeds the symbolic process of executing an input into the target. This way, for a given input, the instrumented target executes symbolically in native speed. SymCC's technique allows to use any symbolic reasoning tool as a backend, and proposes two implementations: one simply creating symbolic expressions and offloading them to an SMT solver; the other using the concolic execution engine of QSym. We have selected SymCC-QSym (SymCC with QSym backend) for our evaluation, because it is the faster implementation.\n\n\nContiki-NG Network Stack\n\nThe fuzzers' target is Contiki-NG [17], which is an open-source operating system designed for resource-constrained IoT devices. Its main feature is its low-power IPv6 stack, which includes a variety of  standard communication protocols implemented in a compact manner. Figure 1 shows the key components of Contiki-NG's network stack arranged at different layers.\n\nAt its lowest layers, where incoming packets enter the network stack in a deployed network, Contiki-NG supports communication standards such as IEEE 802.15.4 and Bluetooth Low Energy (BLE). For medium access control, Contiki-NG provides the option between a basic Carrier Sense Multiple Access (CSMA) protocol and the IEEE 802.15.4 Time-Slotted Channel Hopping (TSCH) protocol. Between the MAC layer and the network layer resides the 6LoWPAN adaptation layer, which performs header compression and fragmentation of IPv6 packets. At the network layer, the central component is the IPv6 implementation, which is based on the original IP in the Contiki OS [8]. It relies on ICMPv6 for control messages, and IPv6 neighbor discovery (IPv6 ND) for translating IPv6 addresses to link-layer addresses and for keeping track of neighbors and routers. For routing within multi-hop wireless networks, Contiki-NG implements the Routing Protocol for Low-power and lossy networks (RPL), which uses ICMPv6 to exchange messages. At the transport layer, Contiki-NG supports both UDP and TCP communication. Both of these implementations are integrated deeply into the IP component in order to achieve small code size. Furthermore, Contiki-NG supports the DTLS protocol to provide transport layer security on top of UDP. At the application layer, Contiki-NG supports the Constrained Application Protocol (CoAP). In addition, the implementation of the network management protocol SNMP relies on UDP. On top of TCP, one can use either MQTT or HTTP as application layer protocols.\n\nEach of these protocol implementations can be a possible attack target for anyone who has the capability to inject packets into it.\n\nOne important point to keep in mind for our work is that, when fuzz testing the Contiki-NG network stack, one has to consider its layered architecture when deciding where to inject packets. One option is to inject all packets at the 6LoWPAN layer or lower, but this entails that a packet will have to pass a large number of checks of different headers before it reaches an upper layer. Hence, the fuzzers will have difficulty in achieving an adequate coverage in the protocol implementations at the upper layers of the network stack; e.g., in the CoAP implementation. Alternatively, one can inject packets directly into the protocol implementation of interest for more focused fuzz testing, at the expense of not covering key parts of the network stack. In our experiments, we focus on injecting packets at the IPv6 and 6LoWPAN layers, which are the lowest points of the stack where significant packet processing occurs, and where input data has the potential to reach code in a variety of protocol implementations on top of these layers.\n\n\nGROUND TRUTH EXPERIMENTS\n\nIn this section, we evaluate the eight fuzzers on the set of eighteen vulnerabilities that we have detected and fixed using fuzz testing. We run fuzzing campaigns on Contiki-NG and compare fuzzing tools' performance to answer the first three research questions.\n\nFuzzer Selection and Some Experiences. We have selected fuzzers to include in our evaluation as follows. In fall 2018, we started fuzzing Contiki-NG's IP layer using AFL. We continued using AFL on IP, with moderate success (we discovered the first three vulnerabilities of Table 2), until we hit a wall and no more bugs could be found with the test harnesses we were using at the time. We therefore turned our attention to hybrid fuzzers that, at least in principle, are more powerful, and tried out Angora and Driller. However, we experienced usability issues with Driller, and quickly abandoned it for QSym. Using Angora and QSym, we were able to trigger more bugs. Up to that point, all fuzzers were run on the laptops and servers of our group, and maintenance was painful whenever software on these machines was updated. In spring 2020, we created Docker containers for running the fuzzers, which allowed us to quickly include Intriguer in our tool set. In the fall of 2020, we learned about MOpt and SymCC, but it was easy to include them in our toolset, because it had evolved further at that point. This turned out to be a good choice, because we were able to discover one more, hard to trigger and reproduce, Contiki-NG vulnerability using them.\n\nWe remark at this point that the set of fuzzers that we use has a small common intersection with those used in three recent papers, which also propose benchmark platforms for evaluating fuzzers (Magma [11], UniFuzz [13], and one based on FuzzBench [2]). Moreover, these three papers and ours investigate different sets of research questions, but, even in the common ones, their conclusions slightly differ from ours. More on this in \u00a75.\n\nPlatform and Configuration. Our artifact runs the fuzzers within Docker containers. The configurations these use are shown in Table 1. To run trials in parallel, 20 to 30 at a time, the machine we used for the measurements we report is a server with two Intel(R) Xeon(R) Platinum 8168 CPUs (2.70GHz with 24 physical cores each, i.e., a total of 48 physical cores / 96 with hyperthreading) and 192GB of RAM running Debian 10.7. For all the trials, two instances of AFL (MOpt-AFL for MOpt) are running using AFL parallelization mode. More precisely, every AFL or MOpt-AFL trial runs two processes, Honggfuzz runs with two threads in its thread pool, and every hybrid fuzzer runs three processes (two instances of AFL plus one more for the symbolic/concolic execution component).\n\nFor MOpt, we set MOpt-AFL's pacemaker mode option to -L 0 for all trials. (We picked this setting, which controls the time after which AFL's deterministic mutations are disabled, because it gave the best performance after multiple 24-hour trials.) Due to bad detection of free CPUs from AFL within our configuration, we disabled AFL's CPU confinement. AFL's execution speed is around 1, 200 execs/sec. We also used RAMDisk to avoid slow disk accesses. Seeds. We use two entry points to inject fuzzed data packets at different layers in the Contiki-NG network stack: one to the IP module and one to the 6LoWPAN module. As a result, we use two different sets of seeds. However, for a given entry point, all fuzzers and all trials use the same set of seeds. For IP, the seeds are built from four well-formed IPv6 packets. They all contain a valid IPv6 header followed by different optional extension headers (Hop-by-hop, Routing, or ICMPv6) with possible options (such as RPL configuration). For 6LoWPAN, we follow the corresponding standard [22] and encapsulate IPv6 packets after 6LoWPAN's lowerlayer headers. This provides a valid input for the 6LoWPAN entry point and still covers what the IP seeds cover. Another possibility is to generate inputs with 6LoWPAN compression or fragmentation header. However, for this paper, we want to generate simple and valid seeds to evaluate how well fuzzing tools explore different headers of each protocol. Then, we use the AFL's corpus and test case minimization tools (afl-cmin and afl-tmin) to optimize the sets of seeds for AFL-removing redundant inputs and trimming the files-and use the resulting sets to initialize our fuzzers.\n\nBenchmark Trials and Numbers Reported. All the times we present in the tables of this section are averages of a total of ten 24-hour trials for each fuzzer. Each fuzzer trial specifically targets a singleton set consisting of some particular ground-truth bug (e.g., uIP-len). After each 24-hour trial finishes, a post-processing script checks all \"unique crashes\" and hangs detected by the fuzzer, validates whether some of them correspond to a bug in the target set and, if so, extracts the earliest time that each such bug was exposed. All such times from ten trials are then averaged and presented in tables of this section as mean time-to-exposure. We use the symbol to denote that a bug was not detected in any of the ten trials. In the cases where non-trivial bugs are exposed consistently (i.e., in all ten out of ten trials) by some fuzzers, and this happens fast or in time that is considerably shorter than that of most other fuzzers that also consistently expose these bugs, we highlight those times.\n\nDesign of \"Ground Truth\" Benchmark Suite. Our benchmark suite consists of selected vulnerabilities we discovered in the code base of Contiki-NG using fuzzing. The vulnerabilities are listed in Table 2; we invite the reader to read its detailed caption at this point.\n\nLet us provide some additional information about these vulnerabilities and our experiences. We started with fuzzing Contiki-NG's code base, at its IPv6 network layer, with IP as entry point using AFL 2.36. Even on the first day, AFL exposed the first crash (uIP-overflow) within the first half hour of fuzzing. However, after running for about two hours, AFL also reported around 100 more \"unique crashes\" which turned out to have the same culprit as the Table 2: Vulnerabilities selected for our experiments. Each of them is given an identifier (Id), has a pull request that fixes it (PR\u266f), which in turn is associated with two Git commit SHAs before and after the fix was merged. The last four columns show the protocol implementation where the bug was located, the CVE number assigned to it (if any), a short description of the error, and the fuzzer that first discovered it, although this mostly reflects on when we first started using each fuzzer. Vulnerabilities are shown in chronological order with which their fixes were merged. This order is almost identical to the chronological order in which vulnerabilities were discovered, with two exceptions (the pairs 6LoWPAN-ext-hdr, ND6-overflow and SNMP-oob-varbinds, SNMP-validate-input) for which the vulnerabilities were discovered in time close to each other, but in the opposite order. first. Once we discovered its root cause, and the Contiki-NG developers applied a fix similar to that of PR#813, all \"unique crashes\" simply vanished! This taught us something which by now is wellknown about fuzzers and their evaluations [11][12][13]. Namely, that\n\nThe number of (so called) unique crashes is not a good measure of a fuzzer's efficacy. It also taught us something concerning a fuzzer's use. Namely, that one should stop a fuzzer-at least all those that rely on AFL-soon after it has come up with the first few \"unique crashes. \" At that point, one should (try to) understand the crashes fuzz testing has exposed, ideally find a fix for them (if they indeed correspond to real bugs in the code), and re-run the fuzzer to check whether similar \"unique crashes\" show up again or not. More importantly for this paper, our experiences result in a recommendation regarding evaluations and comparisons of fuzzers.\n\nBenchmark suites for evaluating fuzzers should try to also capture the process of how bugs are detected and fixed in real software, where often eliminating some bug(s) makes exposition of other bugs a more difficult and time consuming process. One natural advantage of such a fuzzer benchmark suite is that it comes with a good definition of what constitutes a bug, and a more accurate indication of how many bugs it contains: the number of pull requests (PRs) that fix a set of related crashes or hangs.\n\nWe therefore created a ground truth benchmark suite for fuzzers that comprises the evolution of some non-trivial software, in our case Contiki-NG. To that effect, our ground truth suite, currently containing = 18 known vulnerabilities, comes with scripts that allow to check out a Git history point of Contiki-NG's development where the first vulnerabilities are fixed (i.e., not there) and the remaining \u2212 are still present in its code base. Other scripts check whether a particular vulnerability of interest is exposed by a fuzzer during a trial that resulted in crashes or hangs. In short, our ground truth suite allows for fuzzing experiments focused on some particular vulnerability. This is exactly what we evaluate in this section: whether fuzzers expose the vulnerability that we are after, and if so how consistently and fast they do this.\n\nBefore presenting the results of our evaluation, we mention one additional feature of our ground truth suite, which is related to Contiki-NG's layered architecture. Using only IP entry point with the RPL-Lite routing protocol did not expose many vulnerabilities. In fact, it resulted in only three PRs; these are shown in the first three rows of Table 2, which fixed vulnerabilities in the code of IP and RPL protocols. However, by starting the fuzzing at a different layer, namely 6LoWPAN, and changing Contiki-NG configuration, we managed to expose and fix fifteen more vulnerabilities. Finally, note that 6LoWPAN-hdr-iphc and the last four vulnerabilities in Table 2 were discovered by augmenting fuzzing campaigns with sanitizers.\n\nVulnerabilities Using IP as Entry Point. Table 3 shows results of how effective these fuzzers are in exposing bugs in the code of IP.\n\nFor the first two vulnerabilities, we notice the following: (i) The first of them (uIP-overflow) is exposed by seven of the eight fuzzers and the second (uIP-ext-hdr) is exposed by all fuzzers consistently. (ii) Two of the fuzzers (MOpt and SymCC) are clear winners here in terms of the average time it takes to expose these bugs. (iii) SymCC outperforms QSym, which it uses as a symbolic backend, by a factor similar to the one reported in the SymCC paper [21]. (iv) MOpt's strategy for selecting mutator is particularly effective here; it clearly outperforms the two AFL variants, which are also mutation-based fuzzers, and comes very close to the speed that SymCC achieves.\n\nThe third vulnerability (uIP-len) is clearly more challenging. First, no fuzzer exposes it in all ten 24-hour trials, and none exposes it fast-or considerably faster than others-either. Second, AFL-cf    never manages to expose it. Quite likely, this is because it uses Clang to instrument the target. A take-away lesson is that the instrumentation component of a fuzzing tool can cause it to miss vulnerabilities in addition to influencing the speed of its execution. In retrospect, this should not come as a surprise, because vulnerabilities often exist in code which confuses a compiler or is undefined behavior in the language. In such cases, the compiler is allowed to do whatever it pleases with the code; e.g., even remove it. Another way of stating this lesson is that it is often very difficult to do fair comparisons between different fuzzing tools because they are complex pieces of engineering and assembly of components. For example, SymCC is not just SymCC-QSym (i.e., a fuzzer that uses QSym as a symbolic backend), but is more something like SymCC-QSym-AFL-Clang, and possibly many other parts as well, all of them of particular versions.\n\nVulnerabilities Using 6LoWPAN as Entry Point. The second set of our evaluation results is shown in Table 4. The first two vulnerabilities (6LoWPAN-frag and SRH-param) are quite easy to expose for six of the fuzzers (AFL-gcc, MOpt, Angora, QSym, Intriguer, and SymCC), and quite difficult or impossible for AFL-cl and Honggfuzz. However, beyond this point, vulnerabilities become (much) more difficult for most of the fuzzers to expose in 24-hour trials. For example, QSym is the only tool that exposes ND6-overflow. (It is also the fuzzer that discovered it.) The next vulnerability, 6LoWPAN-ext-hdr, is exposed by all but one of the fuzzers. However, note that only MOpt manages to expose it consistently. Similarly, SRH-addr-ptr is exposed by five of the eight fuzzers -and by three of them quite fast-but only QSym manages to detect it consistently.\n\nThe last vulnerability that we discovered without sanitizers, 6LoWPAN-decompr, is not exposed by any of the fuzzers in any of the ten 24-hour experiments that we ran to record timing measurements. For this vulnerability, we executed some more trials for all fuzzers -some of them longer than two days-but they did not expose it either. However, as also shown in Table 2, this vulnerability was originally discovered by both MOpt and SymCC, without the use of sanitizers, so at least these two fuzzers could have exposed it. (Note that for each vulnerability of interest, we use the Git commit SHA before the PR that fixed it was merged in the code base.)\n\nWe mention that our suites come with Proof of Vulnerability (PoV) support, in the form of inputs that trigger crashes or hangs and scripts to trigger them. For example, the 6LoWPAN-decompr vulnerability is easily triggered when Contiki-NG's code is instrumented with AddressSanitizer [24].\n\n\nAnswers to RQ.1-RQ.3\n\nRegarding RQ.1, our results do show some advantage of using hybrid fuzzers in complex code bases such as Contiki-NG's network stack compared to mutation-based fuzzers, but do not show any clear superiority of hybrid fuzzing techniques. We also discover that no fuzzer is uniformly better than all the rest in terms of exposing more vulnerabilities. Still, there are three fuzzers that stand out among the eight we consider: MOpt, SymCC, and QSym. Besides exposing bugs faster than the rest (RQ.2), they are the fuzzers that expose bugs more consistently (RQ.3) among them; cf the entries of Tables 3 and 4.\n\nWe mention that these results agree with the conclusions of the UniFuzz paper [13], which also considers MOpt and QSym (but not SymCC) in its evaluation and places them on top. However, it differs from a recently published paper on fuzzing effectiveness using FuzzBench [2], which also considers MOpt, but places AFL++ and Honggfuzz on top. Our work does not consider AFL++, so we cannot say anything about this fuzzer, but on Contiki-NG's code base Honggfuzz does not perform well. We were curious about this result and also tried Honggfuzz in configurations with many threads in its thread pool, but the result did not change.\n\nLet us highlight two reasons why, in our opinion, hybrid fuzzers are not more effective on Contiki-NG's codebase.\n\nFirst, hybrid fuzzers tend to restrain themselves in doing two things: (1) getting seeds from coverage-guided grey-box fuzzers, and (2) applying heavier analyses to produce better mutants. Doing so, hybrid fuzzers heavily rely on mutation-based fuzzers, and get stuck whenever these cannot generate any new mutants. In short:\n\nThe consistency and effectiveness of a hybrid fuzzer is dependent on the consistency and effectiveness of its mutation-based component.   Table 3: a positive impact is denoted with an upward arrow (\u25b2) and negative impact with a downward arrow (\u25bc). An integer denotes the change in the number of trials exposing the vulnerability; for similar number of trials the time difference is shown. A number of trials and a time denote vulnerabilities that a fuzzing tool exposed only on the sanitized code. \n\u25bc 4 - \u25bc 5 \u25bc 5 \u25bc 3 \u25b2 1 uIP-RPL-classic-prefix \u25bc 4 \u25bc 2 \u25bc 5 - \u25bc 5 \u25bc 9 \u25bc 2 \u25b2 1 uIP-RPL-classic-div \u25bc 7 \u25bc 6 \u25bc 8 \u25bc 2 \u25bc 3 \u25bc 5 \u25bc 3 \u25bc 6\nAn idea that follows naturally from this observation is that hybrid fuzzers can consider integration with multiple mutation-based fuzzers to mitigate this dependency. Second, the symbolic/concolic execution component of a hybrid fuzzer focuses on generating mutants that penetrate new code blocks or explore new paths (i.e., on increasing coverage) rather than mutants that have increased changes to detect more bugs.\n\nRegarding this second point, sanitizers can in principle aid a fuzzer's dynamic analysis to expose difficult to find bugs. To investigate this point, we augment our fuzzing experiments with sanitizer instrumentation in the next section.\n\n\nIMPACT OF SANITIZERS\n\nSanitizers are dynamic bug-finding tools that analyze a single program execution using different types of instrumentation. In theory, sanitizers can significantly increase the defect detection capability of fuzzers [23,25]. On the other hand, sanitizers impose a nonnegligible execution overhead, due to code instrumentation. Thus, sanitizers represent a trade-off as far as fuzz testing is concerned, since fuzzing tools rely on their high execution speed (esp. compared to other testing techniques such as symbolic execution) to be able to execute the SUT with a large number of inputs.\n\nIn this section, we investigate RQ.4. To answer that question, we evaluate the tools on the same set of vulnerabilities using two sanitizers: AddressSanitizer [24] (ASan) and Effective Type Sanitizer [7] (EffectiveSan). More precisely, we add sanitizer instrumentation during the compilation of the coverage-guided, grey-box targets (i.e., the binaries used by AFL and MOpt). Then, as in \u00a73, we run ten fuzzing campaigns for every tool feeding them those augmented targets. Following the usual methodology when fuzzing, we configure the sanitizers to crash whenever they detect a suspicious input, which consequently alerts the fuzzer of the error.\n\nAs in the experiments of \u00a73, we report the number of trials exposing a witness with the mean time-to-exposure (TTE). To ease the comparison between fuzzing without and with the aid of a sanitizer, we include a second table focusing on the tools' difference from the results in Tables 3 and 4. This table depicts an integer if a different number of trials are exposing the vulnerability or the added mean TTE in case of similar effectiveness. Due to space limitations, we do not present results for cases where vulnerabilities are not discovered with or without sanitizers (as e.g., is the case for uIP-buf-next-hdr and uIP-RPL-classic-sllao), but these results can be found in the paper's artifact. For the vulnerabilities exposed by the sanitizers only, we report the number of trials together with the mean TTE. Finally, we denote a positive sanitizer impact, i.e., more trials exposing the vulnerability or a shorter mean TTE, using upper arrows (\u25b2). Down arrows (\u25bc) denote a negative impact.\n\n\nFuzzing with AddressSanitizer\n\nAddressSanitizer [24] (ASan) is a popular sanitizer adding extra memory areas, called red zones, around memory allocations. These red zones dynamically detect any invalid dereferences that are pointing within such addresses. ASan's technique is not complete and might miss some dereferences that do not point to a red zone, but it improves the exposure of out-of-bound memory access errors.\n\nWe investigate the impact of ASan on the fuzzing tools in exposing vulnerabilities in Contiki-NG's code base. To combine the sanitizer with AFL's instrumentation, we use the environment variable provided by the companion scripts, passing sanitizer options to the compiler. At runtime, ASan requires disabling the memory limit and increasing the timeout as its initialization pre-allocates a consequent chunk of memory, causing the AFL to stop. When fuzzing the Contiki-NG's code base, AFL reports only 100 exec/s compared to 1 200 exec/s for the fuzzing campaigns in \u00a73, i.e., a runtime overhead of more than 10\u00d7. Tables 5 and 7 show the tools performance with ASan instrumentation. The differences with Tables 3 and 4 are shown in Tables 6 and 8, respectively.\n\nVulnerabilities Using IP as Entry Point. Tables 5 and 6 show a fair impact for the two first vulnerabilities. With ASan instrumentation, AFL-gcc exposes uIP-overflow in two trials fewer than when running without a sanitizer showing a noticeable drop. SymCC, which is another fuzzer negatively affected by the sanitizer overhead, now requires on average thirty minutes instead of less than two to expose uIP-overflow. On the other hand, Intriger, Angora, and AFL-cf expose the uIP-ext-hdr vulnerability in more trials or   faster on average, whereas the other tools have only slight slowdowns. The impact is very negative for uIP-len. All the tools but SymCC failed to expose the vulnerability. Notice that SymCC uses the most recent compiler within the tools' configurations, and we experienced a better detection of the vulnerability with ASan and recent Clang. None of the tools exposed uIP-buf-next-hdr, a vulnerability which has been discovered using Effective Type Sanitizer. Moreover, only a few fuzzers expose uIP-RPL-classic-prefix, and this happens only once or twice.\n\nVulnerabilities Using 6LoWPAN as Entry Point. Next, we look at Tables 7 and 8. AFL-cf and SymCC expose 6LoWPAN-frag clearly better. Interestingly, the impact of ASan is positive only for the tools based on the Clang compiler. For SRH-param, none of the tools exposes the vulnerability with ASan instrumentation, and later, we see similar results for SRH-addr-ptr. Note that ASan's red zones algorithm is known to be incomplete, and such behaviors may be due to this incompleteness. On the other hand, ASan's impact is positive on all the tools in exposing the three following vulnerabilities. All tools consistently expose 6LoWPAN-ext-hdr and 6LoWPAN-decompr, and pretty fast for the latter. Notice that the former vulnerability is quite challenging, and none of the tools found a witness without ASan for the latter. The 6LoWPAN-hdr-iphc vulnerability has been discovered by adding ASan instrumentation to our fuzzing campaigns, but only AFL-cf, and MOpt are exposing it consistently.\n\n\nFuzzing with Effective Type Sanitizer\n\nEffective Type Sanitizer [7] (EffectiveSan) implements a dynamic type checking technique that tracks type values during the execution, preserving static high-level C/C++ type information and employing extended pointers. As a result, this sanitizer checks: 1) whether any dynamic type matches its corresponding static type, and 2) in case of pointers, whether the offset is within bounds. This technique is particularly efficient to detect out-of-bound memory accesses based on object boundaries, which can happen when accessing buffers without proper offset validation.\n\nEffectiveSan and AFL Configuration. EffectiveSan is available as an extension of Clang-4.0.1. Consequently, we cannot use AFL-gcc companion scripts anymore and need to change several configurations of Table 1: (1) we use AFL-clang instead of AFL-gcc instrumentation, and (2) we set EffectiveSan's Clang to apply both AFL and EffectiveSan instrumentation during the compilation of the augmented targets. (For SymCC in Ubuntu-20, we compile Clang with gcc-4.8.2.) Unfortunately, we had an issue combining AFL-clang-fast and EffectiveSan, due to an AFL's global variable. To fix the issue, we had to disable the EffectiveSan's track of global variables, leaving EffectiveSan with a lighter analysis. Notice that EffectiveSan does not require heavy memory allocations, and AFL can fuzz the target without increasing its usual limits. As a consequence, AFL reports between 100 and 500 exec/s, giving a better throughput than when using ASan. Tables 9 and 11 (on top of the next page) depict the tools' performance when fuzzing with Effec-tiveSan instrumentation. Differences with Tables 3 and 4 are shown  in Tables 10 and 12.\n\nVulnerabilities Using IP as Entry Point. Compared to Table 3,  Tables 9 and 10 show impressive results. Six of the eight fuzzers now consistently expose the three first vulnerabilities with Effec-tiveSan, often with better average time-to-exposures than the ones in Table 3. Furthermore, for all fuzzers except AFL-clang-fast, there is a remarkable improvement in exposing the uIP-len vulnerability. Like in the ASan campaigns, there is a drop in the number of trials that expose the bugs for both vulnerabilities on the RPL-Classic routing protocol. However, we can also notice that SymCC shows a better exposure of uIP-RPL-classic-prefix. Notice the AFL-cf exception; disabling the sanitizer track of global variables is apparently affecting the results in this case. Finally, the tools are exposing uIP-buf-next-hdr with EffectiveSan. However, the vulnerability is still challenging to expose consistently, though we notice increased effectiveness and better performance for SymCC.\n\nVulnerabilities Using 6LoWPAN as Entry Point. There is also good news with the vulnerabilities in Tables 11 and 12. Almost all the tools expose all six of the nine vulnerabilities. (Compared to Table 4 there are three vulnerabilities that still remain unexposed.) Angora managed to expose SRH-param in only seven of its ten trials. Only MOpt and QSym managed to expose the most challenging vulnerability (6LoWPAN-frag), and did that only once, when running with sanitizer instrumentation.\n\n\nFeeding Corpora of Fuzzers to Sanitizers\n\nIn the previous experiments, we injected sanitizer instrumentation during the compilation of the mutation-based fuzzers' targets and        launched fuzzing campaigns with those targets. In this final experiment, we employ corpora (the good and bad inputs) generated from the ten trials of \u00a73 and feed these inputs to the mutation-based fuzzers' target augmented with sanitizers. In other words, we apply sanitizer analyses afterward without launching a new campaign but we use the mutants generated by a previous one (and without sanitizer instrumentation). Consequently, for a time budget of 24 hours, we compare the method of using sanitizers within fuzzing campaigns (henceforth denoted as Method A), as is done in \u00a74.1 and \u00a74.2, against the method of using sanitizers on the output queue generated from such fuzzing campaigns (denoted as Method B).\n\u25b2 6 \u25b2 10 \u25b2 5 \u25b2 5 \u25b2 7 \u25b2 9 uIP-buf-next-hdr \u25b2 2 - \u25b2 3 - \u25b2 1 \u25b2 2 \u25b2 2 \u25b2 7 uIP-RPL-classic-prefix \u25bc 6 \u25bc 2 \u25bc 4 -\nFor the evaluation, we use the same binaries: the SUT compiled with both fuzzer and sanitizer instrumentation. Tables 13 and 14 report selected results, showing the number of trials detecting a witness for the hard to detect vulnerabilities of Tables 3 and 4. (The complete set of results can be found in the paper's artifact.)\n\nFeeding Corpora to ASan. Table 13 shows the number of trials exposing vulnerabilities using both methods for ASan. As a general comment, effectiveness does change for five out of the ten hard-to-expose vulnerabilities with ASan. Most of the tools do not expose any of the two first vulnerabilities. That is not a surprise for uIP-buf-next-hdr, which has been discovered by EffectiveSan. However, regarding uIP-len, almost all the witnesses from \u00a73 are  not detected anymore by adding ASan to the targets. That is surprising and tells us that fuzzing with ASan can make the exposure of vulnerabilities (when staying within a time budget) actually more difficult due to the runtime overhead that its instrumentation adds. Exposing the two first vulnerabilities using IP entry point and the RPL-Classic routing protocol is not a problem anymore following Method B. Notice, though, that the number of trials did not   Table 4, SRH-param and SRH-addr-ptr are not exposed following Method B either. Except for SymCC, which has a more recent configuration, those behaviors are similar to the ones we have seen for uIP-len: the ASan instrumentation prevents the fuzzer from exposing those vulnerabilities.\n\nFor the vulnerability SRH-addr-ptr, which is exposed better by SymCC following Method A, we must launch the fuzzing campaigns with sanitizer instrumentation to expose it. To understand the reason, let us assume that a fuzzer has just generated an input witnessing SRH-addr-ptr. If this input neither crashes-because the sanitizer instrumentation is not enabled-nor provides new coverage, the fuzzer will discard it. Consequently, the witness is lost and even after feeding the corpus to ASan, the tools will not expose the vulnerability. We call this kind of vulnerability path-sensitive. Path-sensitive vulnerabilities are interesting because they require a good analysis during the fuzzing campaigns to be exposed.\n\nFeeding Corpora to EffectiveSan. Table 14 depicts the number of trials exposing the vulnerabilities of Tables 3 and 4 that were not reported in \u00a74.2. Surprisingly, following Method B, none of the tools expose uIP-buf-next-hdr, even though it was originally discovered by EffectiveSan. We have good reasons to believe that the vulnerability is path-sensitive (like SRH-addr-ptr with ASan, its witnesses are discarded by the coverage-guided grey-box component). With the exception of uIP-RPL-classic-sllao, the tools expose better the vulnerabilities on the code of RPL-Classic routing protocol. Also, first running standard fuzzing campaigns and then feeding the corpus to EffectiveSan makes the tools expose uIP-RPL-classic-prefix and uIP-RPL-classic-div in almost all the cases. The uIP-RPL-classic-sllao vulnerability is still undetected due to a conflict with AFL instrumentation. Indeed, only after running the same files with EffectiveSan instrumentation, could we expose uIP-RPL-classic-sllao witnesses.\n\nLet us now look at 6LoWPAN vulnerabilities, in particular the 6LoWPAN-frag one, which is hardly exposed even following Method B. In fact, due to EffectiveSan, most of the previous witnesses are still crashing after the fixes. Hence, it is much more difficult for the fuzzers to find an input fixed by the 6LoWPAN-frag pull request. This case highlights the potential conflicts other bugs have on exposing a specific vulnerability even with a heavier analysis. For the last two vulnerabilities, there are no surprises: QSym still exposes ND6-overflow in four of the ten trials, and none of tools exposes 6LoWPAN-UDP-hdr.\n\n\nAnswer to RQ.4\n\nFrom our experiments, we find that, overall, ASan and EffectiveSan pay off for their overhead. Sanitizers are essential for exposing five vulnerabilities that cannot be detected without sanitizer use, and the eight tools expose Contiki-NG vulnerabilities more consistently. Furthermore, we see that using sanitizers afterward can expose deep vulnerabilities, but not path-sensitive ones. The tools expose the latter only during fuzzing campaigns with sanitizer instrumentation enabled. This point should motivate the fuzzing community to improve fuzzers' analyses during target execution.\n\nAs a last comment, note that what we do not know whether Contiki-NG's code base at the point where the last vulnerability we have detected (uIP-RPL-classic-sllao) is fixed does not contain any other vulnerabilities that the fuzzers and sanitizers we have used so far cannot expose. On the other hand, we see this as a positive feature and an opportunity to expand our ground truth suite with more vulnerabilities and more fuzzing tools in the future, especially more powerful ones than the ones we have selected for this paper.\n\n\nRELATED WORK\n\nIn recent years, several efforts have been made to evaluate and compare the performance of different fuzz testing tools on realworld software. In this section, we cover some related work on fuzzer benchmark suites and comparative evaluations of fuzzers.\n\nCommonly Used Benchmark Suites. LAVA-M [6] and the DARPA Cyber Grand Challenge (CGC) [3] benchmark suites are the first to propose a common set of buggy software for evaluating fuzzers. LAVA-M automatically injects numerous out-of-bounds memory accesses into programs in coreutils-8.14. However, all bugs are triggered by \"magic value\" comparisons, which does not accurately represent the complexity and diversity of software bugs encountered in the real-world. The DARPA CGC bug suite proposes a wider range of bug types, but its synthetic programs are relatively simple and small. In contrast, our work offers a ground truth benchmark suite for fuzzers using the Contiki-NG network stack code base. We built that suite from real vulnerabilities, which correspond to several different types of software errors. All the bugs come with crashing inputs (aka proof of vulnerability), fixes that correct them, and capture different time points in the evolution of Contiki-NG's code base. Also, our suite comes with bugs that have a natural progression in the level of difficulty to detect them, something which is missing in all existing benchmark suites for evaluating fuzzers.\n\nGoogle FuzzBench [10], previously Google Fuzzer Test Suite [9], is an online service to evaluate fuzzers. FuzzBench proposes an easy integration of new fuzzers together with a periodic evaluation of them on a set of benchmarks. The service makes it easier for fuzzer developers to evaluate some performance aspects of their tools, but reports only code coverage statistics which, although indicative, overapproximate bug coverage and are insufficient to compare the effectiveness of different fuzzing techniques and tools (i.e., a fuzzer should not only reach the statement where a bug exists, but it also needs to expose that bug).\n\nTwo recent efforts, running concurrently with our work, have proposed benchmarking platforms with different characteristics, both between them and from the suite we have put forward.\n\nUniFuzz. The first effort, UniFuzz, offers a \"holistic and pragmatic metrics-driven platform for evaluating fuzzers, \" currently incorporating numerous existing fuzzers and 20 real-world programs. Clearly it is the result of significant effort, and provides a platform which is much broader than ours. Similarly to what we do, UniFuzz comes with support for Docker containers to easily reproduce experiments and to add new fuzzers if desired. However, unlike our suites, UniFuzz triages a fuzzer's observed crashes by mapping crash stack traces to known CVEs, which is sometimes problematic in our opinion, and does not explicitly specify the number of bugs a fuzzer is expected to find (i.e., the UniFuzz suite lacks ground truth knowledge). It is also unclear whether UniFuzz allows for trials that focus only on specific (subset of) bugs. Finally, UniFuzz does not allow to test the evolution of the target programs, an aspect we consider important in fuzzer comparisons (e.g., for determining fuzzers that stop being effective beyond some particular point).\n\nThe UniFuzz paper [13] evaluates eight fuzzers (AFL, AFLFast, Angora, Honggfuzz, MOpt, QSym, T-Fuzz, and VUzzer64), i.e., has five fuzzers in common with our evaluation. Similar to our results, its evaluation also finds that none of these fuzzers outperforms all the others across all target programs, but that MOpt and QSym are in the top category, a statement that our results also corroborate.\n\nMagma. The second effort, Magma [11], provides a \"ground truth fuzzing benchmark that enables fuzzing evolution and comparison. \" Magma incorporates a large diversity of programs and bug types. One of its key design decisions is to forward-port pull requests that fix bugs into the latest version of a program's code together with the condition to trigger them. This is a very interesting idea, which is complementary and 'in the other direction' to what we do in our ground truth suite. The bug conditions allow Magma to monitor whether a fuzzer has triggered a bug, but was unable to detect it (i.e., if the bug condition is satisfied but the target did not crash). This metric nicely refines the runtime information provided by the benchmark during fuzzing. However, not all the forward-ported bugs have an updated proof of vulnerability, and some may not be possible to trigger in the latest version of the program's code. Furthermore, the monitoring instrumentation adds restrictions on how fuzzers should execute a target.\n\nThe Magma paper [11] evaluates seven fuzzers (AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, Honggfuzz, and SymCC) on the Magma suite using an extensive set of trial runs. The fuzzing tools in common to those we use in this paper is three mutation-based (AFL, MOpt-AFL, and Honggfuzz) and only one hybrid fuzzer (SymCC).\n\nIn contrast to Magma, in our suite we do not port the bugs into a different context. Instead, we directly checkout to the corresponding Git history points. Every vulnerability has a pair of associated commits: those before and after the bug fix. Using this pair of commits, we can ensure the bug reproducibility and provide the exact same environment and conditions for bug detection.\n\nEvaluation of Fuzz Testing. Klees et al. warn that missing the fuzzer's randomness factor leads to mis-interpretation during experiments [12]. Furthermore, their paper shows the bias of using crash-based metrics, and the authors argue that the community should use bug metrics with statistical relevance instead. We have followed this methodology by running our experiments ten times with a timeout of 24 hours. Moreover, when fuzzers find the bug in all trials, we compute the Mann-Whitney significance of the corresponding best time-to-exposure. However, due to lack of space, this data is only available in our artifact.\n\n\nCONCLUDING REMARKS\n\nThere exist at least two different ways to read this paper. The first, perhaps less exciting one, is to view it as an experience report of using different state-of-the-art fuzzing tools to detect vulnerabilities in the complex code base of a widely-used OS for IoT devices, and improve its robustness and security. In this respect, our advice to other developers is: \"use as many fuzzers as you can get your hands to and fix, or at least try to understand, the issues that they report.\" We offer some strong evidence that, currently, no single fuzzing tool outperforms all the others or is able to consistently expose the bugs that exist or, worse, that itself has previously discovered. In our opinion, this calls for research that makes fuzzers more consistent, besides making them faster and/or more effective.\n\nAnother way to read this paper is as offering an independent and extensive evaluation of the effectiveness of state-of-the-art mutation-based and hybrid fuzzers on a real-world code base. It also proposes a new benchmark suite for evaluating fuzzers, which has special properties due to the layered-based characteristics of Contiki-NG's code base. Finally, it offers some new ideas on how fuzzing tools should be evaluated. Evaluating fuzz testing tools accurately and consistently is not an easy task, and will remain challenging as techniques mature and get incorporated into tools that come with more and more knobs, bells and whistles. We hope that our benchmarking platform proves useful to researchers and developers of the area, and that it will get extended as more vulnerabilities are exposed in Contiki-NG's code base.\n\nAs a final comment, we note that none of the fuzzers we have used in this evaluation takes into account the stateful nature of the protocols implemented in Contiki-NG's low-power IPv6 stack. When we started this work, stateful grey-box fuzzers were nonexistent (or still in their infancy), but since then some such fuzzers have been developed. Extending our experiments with a set of stateful fuzzers and discovering whether the known bugs are exposed faster, more consistently, or measure how many more new bugs such fuzzers are able to expose are intesting directions for continuing this work. Similarly, one can include ensemble fuzzers in a future comparison. However, as noted in the paper's title, there are so many fuzzers and so little time...\n\nFigure 1 :\n1Protocols at different layers in the Contiki-NG lowpower IPv6 stack. The components in bold show where our fuzzing experiments have revealed code vulnerabilities.\n\n\nall: adding ASan did not expose other bad inputs missed during \u00a73 experiments.Let us now focus on the vulnerabilities starting with 6LoWPAN entry point. Though there are witnesses in\n\nTable 1 :\n1Docker image configurations for our experiments.Tool \nVersion Ubuntu OS AFL Instrumentation Compiler \nAFL-gcc \n2.57b \n16.04 LTS \nafl-gcc \ngcc 5.4.0 \nAFL-cf \n2.57b \n16.04 LTS \nafl-clang-fast \nclang 3.8.0 \nMOpt \ne3e6936 16.04 LTS \nafl-gcc \ngcc 5.4.0 \nHonggfuzz 0b4cd5b1 20.04 LTS \nclang 8.0.1 \n\nAngora \n1.2.2 \n16.04 LTS \nafl-gcc \ngcc 5.4.0 \nQSym \n4fa4363 16.04 LTS \nafl-gcc \ngcc 5.4.0 \nIntriguer \n4d41176* 16.04 LTS \nafl-gcc \ngcc 5.4.0 \nSymCC \ne29fc5a 20.04 LTS \nafl-clang \nclang 10.0.0 \n* We have fixed a disk space issue for Intriguer. \n\n\n\n\nOut-of-bounds read in uipbuf. MOpt + EffectiveSan uIP-RPL-classic-sllao 1654 8512556-e58b583 IPv6 ND CVE-2022-35926 Out-of-bounds read in ND6 option headers.EffectiveSan into SymCCId \nPR\u266f \nCommit SHAs \nProtocol \nCVE \nError description \nDiscovered by \n\nuIP-overflow \n\n813 a1cba56-ea6c688 \nuIP \nInteger overflows in IPv6 extension header options. \nAFL \n\nuIP-ext-hdr \n\n867 150a3fe-b5d997f uIP/RPL* \nUnsafe IPv6 extension header processing. \nAFL \n\nuIP-len \n\n871 b5d997f-8340735 \nuIP \nCVE-2020-13985 Unverified IPv6 header length before packet processing. AFL \n\n6LoWPAN-frag \n\n972 6553688-5884a12 6LoWPAN \nBuffer overflow in 6LoWPAN fragment reassembly. \nAFL + external \n\nSRH-param \n\n1183 beff30b-ebd4cae \nRPL* \nCVE-2021-21282 Unverified Source Routing Header (SRH) parameter. \nAngora + QSym \n\nND6-overflow \n\n1410 f417a5f-5bfb30d IPv6 ND CVE-2021-21279 Infinite loop in ND6 due to integer wrap around. \nQSym \n\n6LoWPAN-ext-hdr \n\n1409 5bfb30d-48a3799 6LoWPAN CVE-2021-21280 Out-of-bounds write when processing external header. \nAngora + QSym \n\nSRH-addr-ptr \n\n1431 3a3dbfe-3f9a601 \nRPL* \nCVE-2021-21257 Unverified address pointer in the Source Routing Header. AFL \n\n6LoWPAN-decompr \n\n1482 425587d-aa6e26f 6LoWPAN CVE-2021-21410 Out-of-bounds read when decompressing packets. \nMOpt + SymCC \n\n6LoWPAN-hdr-iphc \n\n1506 0dada69-6c8373d 6LoWPAN \nOut-of-bounds read from hc06 _ ptr in a loop condition. \nmany tools but with ASan \n\nSNMP-oob-varbinds \n\n1541 285cee0-457fa6c \nSNMP \nOut-of-bounds read from varbinds in a loop condition. \nAFL \n\nSNMP-validate-input \n\n1517 457fa6c-9daacb6 \nSNMP \nBad length check for SNMP input packets. \nAFL \n\nuIP-RPL-classic-prefix 1589 cd208ed-7c2d686 \nRPL \nCVE-2022-35927 Unverified DIO prefix info lengths. \nexternal \n\nuIP-RPL-classic-div \n\n1598 f608483-e427f48 \nRPL \nDivision by zero from DIO with O lifetimes. \nAFL \n\n6LoWPAN-UDP-hdr \n\n1646 b65cfa3-92783e8 6LoWPAN CVE-2022-36052 Out-of-bounds read when decompressing UDP header. \nMOpt + EffectiveSan \n\n6LoWPAN-payload \n\n1647 92783e8-2dfbaee 6LoWPAN CVE-2022-36054 Out-of-bounds write when decompressing payload. \nMOpt + EffectiveSan \n\nuIP-buf-next-hdr \n\n1648 2dfbaee-80a5479 \nuIP \nCVE-2022-36053 \n\nTable 3 :\n3Number of times and mean time-to-exposure (HH:MM:SS) for the seven vulnerabilities in the code base of IP.Id \nAFL-gcc \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\nuIP-overflow \n\n\n\nTable 4 :\n4Number of times and mean time-to-exposure for the nine vulnerabilities starting the fuzzing from 6LoWPAN.Id \nAFL-gcc \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\n6LoWPAN-frag \n\n\n\nTable 5 :\n5Number of times and mean time-to-exposure for the IP vulnerabilities using AddressSanitizer instrumentation.Id \nAFL-gcc \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\nuIP-overflow \n\n8 00:17:24 \n10 00:34:34 \n10 00:19:53 \n0 \n10 00:48:04 \n10 00:15:08 \n10 00:37:30 \n10 00:31:03 \n\nuIP-ext-hdr \n\n10 05:15:10 \n10 02:30:14 \n10 01:20:44 \n10 01:11:22 \n10 02:17:21 \n10 01:53:00 \n10 03:33:16 \n10 02:38:00 \n\nuIP-len \n\n0 \n0 \n0 \n0 \n0 \n0 \n0 \n2 11:57:49 \n\nuIP-RPL-classic-prefix \n\n2 13:25:17 \n0 \n2 21:58:18 \n0 \n1 03:59:56 \n1 08:19:18 \n0 \n1 17:06:14 \n\nuIP-RPL-classic-div \n\n0 \n0 \n0 \n2 09:50:03 \n1 02:41:05 \n0 \n0 \n0 \n\n\n\nTable 6 :\n6Impact of AddressSanitizer for the vulnerabilities in the code base of IP. The table shows performance differences from\n\nTable 7 :\n7Number of times and mean time-to-exposure for the 6LoWPAN vulnerabilities and AddressSanitizer instrumentation.Id \nAFL-gcc \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\n6LoWPAN-frag \n\n9 01:39:07 \n8 01:06:08 \n8 02:19:24 \n0 \n10 00:28:16 \n10 00:31:59 \n10 02:03:18 \n10 00:40:38 \n\nSRH-param \n\n0 \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n\n6LoWPAN-ext-hdr \n\n10 01:01:17 \n10 01:11:11 \n10 00:18:16 \n10 06:59:36 \n10 00:36:27 \n10 00:16:53 \n10 01:16:03 \n10 00:39:00 \n\nSRH-addr-ptr \n\n0 \n0 \n0 \n0 \n0 \n0 \n0 \n4 03:37:12 \n\n6LoWPAN-decompr \n\n10 00:03:25 \n10 00:03:15 \n10 00:01:45 \n10 00:00:19 \n10 00:02:07 \n10 00:02:26 \n10 00:03:00 \n10 00:01:19 \n\n6LoWPAN-hdr-iphc \n\n9 08:38:23 \n10 06:21:53 \n10 03:19:03 \n10 02:00:48 \n8 07:57:00 \n8 03:32:29 \n9 09:28:19 \n9 10:04:55 \n\n\n\nTable 8 :\n8Impact of AddressSanitizer for the vulnerabilities starting the fuzzing from 6LoWPAN (differences fromTable 4).Id \nAFL-gcc \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\n6LoWPAN-frag \n\n\u25bc \n\n1 \n\n\u25b2 \n\n5 \n\n\u25bc \n\n2 \n-\n\n\u25bc 00:09:26 \n\u25bc 00:23:27 \n\u25bc 01:39:59 \n\u25b2 01:00:01 \n\nSRH-param \n\n\n\nTable 9 :\n9Number of times and mean time-to-exposure for the IP vulnerabilities and EffectiveSan instrumentation.Id \nAFL-clang \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\nuIP-overflow \n\n\n\nTable 10 :\n10Impact of EffectiveSan for the vulnerabilities in the code base of IP (differences fromTable 3).Id \nAFL-gcc/-clang \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\nuIP-overflow \n\n\u25b2 \n\n00:06:31 \n\n\u25b2 00:26:21 \n\u25bc 00:13:19 \n\n-\n\n\u25b2 00:47:22 \n\u25b2 00:09:03 \n\u25b2 00:29:43 \n\u25bc 00:04:13 \n\nuIP-ext-hdr \n\n\u25b2 \n\n02:29:10 \n\n\u25bc 00:11:45 \n\u25bc 00:11:53 \n\u25bc \n\n10 \n\n\u25b2 02:02:15 \n\u25bc 00:17:45 \n\u25b2 \n\n1 \n\n\u25bc 00:12:30 \n\nuIP-len \n\n\u25b2 \n\n5 \n-\n\n\n\nTable 11 :\n11Number of times and mean time-to-exposure for the 6LoWPAN vulnerabilities and EffectiveSan instrumentation.Id \nAFL-clang \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\n6LoWPAN-frag \n\n\n\nTable 12 :\n12Impact of EffectiveSan for the vulnerabilities starting the fuzzing from 6LoWPAN (differences fromTable 4).Id \nAFL-gcc/-clang \nAFL-cf \nMOpt \nHonggfuzz \nAngora \nQSym \nIntriguer \nSymCC \n\n6LoWPAN-frag \n\n\n\nTable 13 :\n13Number of times the targets with AFL and ASan instrumentation expose the challenging vulnerabilities from \u00a74.1. On the left, we show the trials of \u00a74.1, i.e., during ASan campaigns. On the right, we depict the trials exposing the vulnerability by feeding corpora of \u00a73 to the targets.Id \nAF L-g cc \nAF L-c f \nMO pt \nAn go ra \nQS ym \nIn tri gu er \nSy mC C \n\nuIP-len \n\n\n\nTable 14 :\n14Similar to Table 13 but with EffectiveSan.Id \nAF L-c l \nAF L-c f \nMO pt \nAn go ra \nQS ym \nIn tri gu er \nSy mC C \n\nuIP-buf-next-hdr \n\n\nIn our evaluation, we include two variants of AFL: afl-gcc and afl-clang-fast (which we abbreviate AFL-cf). AFL-cf highlights the impact that instrumentation can have on AFL's effectiveness, and also on the effectiveness of hybrid fuzzers that also instrument using afl-clang-fast. As we will see, our experiences and results agree with observations made by Poeplau and Aur\u00e9lien[20] about how Intermediate Representations or instrumentation for symbolic execution may affect not only fuzzers' speed but also their ability to detect bugs. More on that in \u00a73.\nACKNOWLEDGMENTSThis research has been supported in part by the Swedish Foundation for Strategic Research through the aSSIsT project and by the Swedish Research Council through grant #621-2017-04812. We thank the anonymous reviewers for their time and their comments.\nAFL-clang-fast 2019. Fast LLVM-based instrumentation for afl-fuzz. AFL-clang-fast 2019. Fast LLVM-based instrumentation for afl-fuzz. https: //github.com/google/AFL/tree/master/llvm_mode.\n\nComparing Fuzzers on a Level Playing Field with FuzzBench. Dario Asprone, Jonathan Metzman, Arya Abhishek, Guizzo Giovani, Federica Sarro, 10.1109/ICST53961.2022.0003915th IEEE International Conference on Software Testing, Verification and Validation. Valencia, SpainIEEEICST 2022Dario Asprone, Jonathan Metzman, Arya Abhishek, Guizzo Giovani, and Federica Sarro. 2022. Comparing Fuzzers on a Level Playing Field with FuzzBench. In 15th IEEE International Conference on Software Testing, Verification and Validation (Valencia, Spain) (ICST 2022). IEEE, 302-311. https://doi.org/10.1109/ICST53961. 2022.00039\n\nCyber Grand Challenge Corpus. Brian Caswell, Brian Caswell. 2016. Cyber Grand Challenge Corpus. http://www.lungetech. com/cgc-corpus/.\n\nAngora: Efficient Fuzzing by Principled Search. Peng Chen, Hao Chen, 10.1109/SP.2018.00046IEEE Symposium on Security and Privacy. San Francisco, CAIEEEPeng Chen and Hao Chen. 2018. Angora: Efficient Fuzzing by Principled Search. In IEEE Symposium on Security and Privacy (San Francisco, CA) (SP 2018). IEEE, 711-725. https://doi.org/10.1109/SP.2018.00046\n\nIntriguer: Field-Level Constraint Solving for Hybrid Fuzzing. Mingi Cho, Seoyoung Kim, Taekyoung Kwon, Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. the 2019 ACM SIGSAC Conference on Computer and Communications SecurityLondon, UKCCS '19)Mingi Cho, Seoyoung Kim, and Taekyoung Kwon. 2019. Intriguer: Field-Level Constraint Solving for Hybrid Fuzzing. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (London, UK) (CCS '19).\n\n. 10.1145/3319535.3354249ACMNew York, NY, USAACM, New York, NY, USA, 515-530. https://doi.org/10.1145/3319535.3354249\n\nLAVA: Large-Scale Automated Vulnerability Addition. Brendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti, William K Robertson, Frederick Ulrich, Ryan Whelan, 10.1109/SP.2016.15IEEE Symposium on Security and Privacy. San Jose, CA, USAIEEE Computer SocietyBrendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti, William K. Robertson, Frederick Ulrich, and Ryan Whelan. 2016. LAVA: Large- Scale Automated Vulnerability Addition. In IEEE Symposium on Security and Privacy (San Jose, CA, USA) (SP 2016). IEEE Computer Society, 110-121. https: //doi.org/10.1109/SP.2016.15\n\nEffectiveSan: Type and Memory Error Detection Using Dynamically Typed C/C++. J Gregory, Duck, H C Roland, Yap, 10.1145/3192366.3192388Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 39th ACM SIGPLAN Conference on Programming Language Design and ImplementationPhiladelphia, PA, USA; New York, NY, USAACMGregory J. Duck and Roland H. C. Yap. 2018. EffectiveSan: Type and Memory Error Detection Using Dynamically Typed C/C++. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation (Philadelphia, PA, USA) (PLDI 2018). ACM, New York, NY, USA, 181-195. https: //doi.org/10.1145/3192366.3192388\n\nFull TCP/IP for 8-Bit Architectures. Adam Dunkels, 10.1145/1066116.10661181st International Conference on Mobile Systems, Applications and Services. San Francisco, CA, USAAdam Dunkels. 2003. Full TCP/IP for 8-Bit Architectures. In 1st International Conference on Mobile Systems, Applications and Services (San Francisco, CA, USA) (MobiSys 2003). USENIX, 85-98. https://doi.org/10.1145/1066116.1066118\n\nFTS 2018. Fuzzer Test Suite. FTS 2018. Fuzzer Test Suite. https://github.com/google/fuzzer-test-suite.\n\nFuzzBench: Fuzzer Benchmarking As a Service. FuzzBench 2020. FuzzBench 2020. FuzzBench: Fuzzer Benchmarking As a Service. https://github. com/google/fuzzbench.\n\nMagma: A Ground-Truth Fuzzing Benchmark. Ahmad Hazimeh, Adrian Herrera, Mathias Payer, 10.1145/3428334Proc. ACM Meas. Anal. Comput. Syst. 4Ahmad Hazimeh, Adrian Herrera, and Mathias Payer. 2020. Magma: A Ground- Truth Fuzzing Benchmark. Proc. ACM Meas. Anal. Comput. Syst. 4, 3, Article 49 (Nov. 2020), 29 pages. https://doi.org/10.1145/3428334\n\nEvaluating Fuzz Testing. George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, Michael Hicks, 10.1145/3243734.3243804Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. the 2018 ACM SIGSAC Conference on Computer and Communications SecurityToronto, Canada; New York, NY, USAACMCCS '18)George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018. Evaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (Toronto, Canada) (CCS '18). ACM, New York, NY, USA, 2123-2138. https://doi.org/10.1145/3243734.3243804\n\nUNIFUZZ: A Holistic and Pragmatic Metrics-Driven Platform for Evaluating Fuzzers. Yuwei Li, Shouling Ji, Yuan Chen, Sizhuang Liang, Wei-Han Lee, Yueyao Chen, Chenyang Lyu, Chunming Wu, Raheem Beyah, Peng Cheng, Kangjie Lu, Ting Wang, 30th USENIX Security Symposium (USENIX Security 21). USENIX Association. Yuwei Li, Shouling Ji, Yuan Chen, Sizhuang Liang, Wei-Han Lee, Yueyao Chen, Chenyang Lyu, Chunming Wu, Raheem Beyah, Peng Cheng, Kangjie Lu, and Ting Wang. 2021. UNIFUZZ: A Holistic and Pragmatic Metrics-Driven Plat- form for Evaluating Fuzzers. In 30th USENIX Security Symposium (USENIX Secu- rity 21). USENIX Association, 2777-2794. https://www.usenix.org/conference/ usenixsecurity21/presentation/li-yuwei\n\nMOpt: Optimized Mutation Scheduling for Fuzzers. Chenyang Lyu, Shouling Ji, Chao Zhang, Yuwei Li, Wei-Han Lee, Raheem Beyah, 28th USENIX Security Symposium (USENIX Security 19). Santa Clara, CA, USAUSENIX AssociationChenyang Lyu, Shouling Ji, Chao Zhang, Yuwei Li, Wei-Han Lee, and Raheem Beyah. 2019. MOpt: Optimized Mutation Scheduling for Fuzzers. In 28th USENIX Security Symposium (USENIX Security 19) (Santa Clara, CA, USA). USENIX As- sociation, 1949-1966. https://www.usenix.org/conference/usenixsecurity19/ presentation/lyu\n\nThe Art, Science, and Engineering of Fuzzing: A Survey. J M Valentin, Hyungseok Man\u00e8s, Choongwoo Han, Sang Kil Han, Manuel Cha, Edward J Egele, Maverick Schwartz, Woo, 10.1109/TSE.2019.2946563IEEE Transactions on Software Engineering. 47Valentin J. M. Man\u00e8s, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J. Schwartz, and Maverick Woo. 2021. The Art, Science, and Engineering of Fuzzing: A Survey. IEEE Transactions on Software Engineering 47, 11 (2021), 2312-2331. https://doi.org/10.1109/TSE.2019.2946563\n\nAn Empirical Study of the Reliability of UNIX Utilities. P Barton, Louis Miller, Bryan Fredriksen, So, 10.1145/96267.96279Commun. ACM. 33Barton P. Miller, Louis Fredriksen, and Bryan So. 1990. An Empirical Study of the Reliability of UNIX Utilities. Commun. ACM 33, 12 (Dec. 1990), 32-44. https://doi.org/10.1145/96267.96279\n\nYasuyuki Tanaka, and Nicolas Tsiftes. 2022. The Contiki-NG open source operating system for next generation IoT devices. George Oikonomou, Simon Duquennoy, Atis Elsts, Joakim Eriksson, 10.1016/j.softx.2022.101089SoftwareX. 18101089George Oikonomou, Simon Duquennoy, Atis Elsts, Joakim Eriksson, Yasuyuki Tanaka, and Nicolas Tsiftes. 2022. The Contiki-NG open source operating system for next generation IoT devices. SoftwareX 18 (2022), 101089. https://doi.org/10. 1016/j.softx.2022.101089\n\nMaster's thesis. Brian S Pak, Hybrid Fuzz Testing: Discovering Software Bugs via Fuzzing and Symbolic Execution. School of Computer Science, Carnegie Mellon UniversityBrian S. Pak. 2012. Hybrid Fuzz Testing: Discovering Software Bugs via Fuzzing and Symbolic Execution. Master's thesis. School of Computer Science, Carnegie Mellon University. http://reports-archive.adm.cs.cmu.edu/anon/2012/CMU-CS- 12-116.pdf CMU-CS-12-116.\n\nThe Fuzzing Hype-Train: How Random Testing Triggers Thousands of Crashes. Mathias Payer, 10.1109/MSEC.2018.2889892IEEE Security & Privacy. 17Mathias Payer. 2019. The Fuzzing Hype-Train: How Random Testing Triggers Thousands of Crashes. IEEE Security & Privacy 17, 1 (Jan.-Feb. 2019), 78-82. https://doi.org/10.1109/MSEC.2018.2889892\n\nSystematic Comparison of Symbolic Execution Systems: Intermediate Representation and Its Generation. Sebastian Poeplau, Aur\u00e9lien Francillon, 10.1145/3359789.3359796Proceedings of the 35th Annual Computer Security Applications Conference. the 35th Annual Computer Security Applications ConferenceSan Juan, PR, USA; New York, NY, USAACMACSAC'19)Sebastian Poeplau and Aur\u00e9lien Francillon. 2019. Systematic Comparison of Symbolic Execution Systems: Intermediate Representation and Its Generation. In Proceedings of the 35th Annual Computer Security Applications Conference (San Juan, PR, USA) (ACSAC'19). ACM, New York, NY, USA, 163-176. https: //doi.org/10.1145/3359789.3359796\n\nSymbolic execution with SymCC: Don't interpret, compile!. Sebastian Poeplau, Aur\u00e9lien Francillon, 29th USENIX Security Symposium (USENIX Security 20. Boston, MA, USAUSENIX AssociationSebastian Poeplau and Aur\u00e9lien Francillon. 2020. Symbolic execution with SymCC: Don't interpret, compile!. In 29th USENIX Security Symposium (USENIX Security 20) (Boston, MA, USA). USENIX Association, 181-198. https://www. usenix.org/conference/usenixsecurity20/presentation/poeplau\n\nTransmission of IPv6 Packets over IEEE 802. RFC 2007. RFC 2007. Transmission of IPv6 Packets over IEEE 802.15.4 Networks. https: //tools.ietf.org/html/rfc4944.\n\nSanitize, Fuzz, and Harden Your C++ Code. Kostya Serebryany, Talk at USENIX Enigma. Kostya Serebryany. 2016. Sanitize, Fuzz, and Harden Your C++ Code. Talk at USENIX Enigma. https://www.usenix.org/conference/enigma2016/conference- program/presentation/serebryany\n\nAddressSanitizer: A Fast Address Sanity Checker. Konstantin Serebryany, Derek Bruening, Alexander Potapenko, Dmitriy Vyukov, 2012 USENIX Annual Technical Conference (USENIX ATC 12. Boston, MA, USAUSENIX AssociationKonstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitriy Vyukov. 2012. AddressSanitizer: A Fast Address Sanity Checker. In 2012 USENIX Annual Technical Conference (USENIX ATC 12) (Boston, MA, USA). USENIX Asso- ciation, 309-318. https://www.usenix.org/conference/atc12/technical-sessions/ presentation/serebryany\n\nSoK: Sanitizing for Security. Dokyung Song, Julian Lettner, Prabhu Rajasekaran, Yeoul Na, Stijn Volckaert, Per Larsen, Michael Franz, 10.1109/SP.2019.00010IEEE Symposium on Security and Privacy. USAIEEEDokyung Song, Julian Lettner, Prabhu Rajasekaran, Yeoul Na, Stijn Volckaert, Per Larsen, and Michael Franz. 2019. SoK: Sanitizing for Security. In IEEE Symposium on Security and Privacy (SP 2019). IEEE, USA, 1275-1295. https://doi.org/10.1109/ SP.2019.00010\n\nDriller: Augmenting Fuzzing through Selective Symbolic Execution. Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang, Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, Giovanni Vigna, 23rd Annual Network and Distributed System Security Symposium. San Diego, CA, USA16The Internet SocietyNick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang, Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Gio- vanni Vigna. 2016. Driller: Augmenting Fuzzing through Selective Sym- bolic Execution. In 23rd Annual Network and Distributed System Secu- rity Symposium (San Diego, CA, USA) (NDSS 2016). The Internet Society, 16 pages. http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2017/ 09/driller-augmenting-fuzzing-through-selective-symbolic-execution.pdf\n\n. Robert Swiecki, Robert Swiecki. 2010. Honggfuzz. https://honggfuzz.dev/.\n\nThe Clang Team. The Clang Team. 2014. UndefinedBehaviorSanitizer. https://clang.llvm.org/docs/ UndefinedBehaviorSanitizer.html.\n\nQSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing. Insu Yun, Sangho Lee, Meng Xu, Yeongjin Jang, Taesoo Kim, 27th USENIX Security Symposium (USENIX Security 18. Baltimore, MD, USAUSENIX AssociationInsu Yun, Sangho Lee, Meng Xu, Yeongjin Jang, and Taesoo Kim. 2018. QSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing. In 27th USENIX Security Symposium (USENIX Security 18) (Baltimore, MD, USA). USENIX Association, 745-761. https://www.usenix.org/conference/usenixsecurity18/ presentation/yun\n\nAmerican Fuzzy Lop. Micha\u0142 Zalewski, Micha\u0142 Zalewski. 2013. American Fuzzy Lop. http://lcamtuf.coredump.cx/afl/.\n", "annotations": {"author": "[{\"end\":152,\"start\":109},{\"end\":191,\"start\":153},{\"end\":248,\"start\":192},{\"end\":284,\"start\":249},{\"end\":393,\"start\":285},{\"end\":432,\"start\":394}]", "publisher": null, "author_last_name": "[{\"end\":125,\"start\":117},{\"end\":168,\"start\":161}]", "author_first_name": "[{\"end\":116,\"start\":109},{\"end\":160,\"start\":153}]", "author_affiliation": "[{\"end\":247,\"start\":193},{\"end\":283,\"start\":250},{\"end\":392,\"start\":286},{\"end\":431,\"start\":395}]", "title": "[{\"end\":106,\"start\":1},{\"end\":538,\"start\":433}]", "venue": null, "abstract": "[{\"end\":2181,\"start\":1000}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3280,\"start\":3276},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3377,\"start\":3373},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3380,\"start\":3377},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3464,\"start\":3460},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3571,\"start\":3567},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4501,\"start\":4497},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4517,\"start\":4513},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4536,\"start\":4532},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4580,\"start\":4577},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4591,\"start\":4587},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4606,\"start\":4603},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4622,\"start\":4618},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5430,\"start\":5426},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5434,\"start\":5430},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5438,\"start\":5434},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7567,\"start\":7563},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9986,\"start\":9982},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9989,\"start\":9986},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10499,\"start\":10495},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11557,\"start\":11553},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11913,\"start\":11909},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12035,\"start\":12032},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12126,\"start\":12122},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12389,\"start\":12385},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12883,\"start\":12879},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13201,\"start\":13197},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13606,\"start\":13602},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13633,\"start\":13630},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14192,\"start\":14189},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14535,\"start\":14531},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15157,\"start\":15153},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16139,\"start\":16136},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19965,\"start\":19961},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19979,\"start\":19975},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20011,\"start\":20008},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22019,\"start\":22015},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25519,\"start\":25515},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25523,\"start\":25519},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25527,\"start\":25523},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28890,\"start\":28886},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32061,\"start\":32057},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32777,\"start\":32773},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32968,\"start\":32965},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35292,\"start\":35288},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":35295,\"start\":35292},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35826,\"start\":35822},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35866,\"start\":35863},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37363,\"start\":37359},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40631,\"start\":40628},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50103,\"start\":50100},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50149,\"start\":50146},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":51258,\"start\":51254},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":51299,\"start\":51296},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53140,\"start\":53136},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":53552,\"start\":53548},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":54566,\"start\":54562},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":55389,\"start\":55385},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":65549,\"start\":65545}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":58466,\"start\":58291},{\"attributes\":{\"id\":\"fig_9\"},\"end\":58651,\"start\":58467},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59202,\"start\":58652},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":61370,\"start\":59203},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61575,\"start\":61371},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61779,\"start\":61576},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":62403,\"start\":61780},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62535,\"start\":62404},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":63288,\"start\":62536},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":63585,\"start\":63289},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":63788,\"start\":63586},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":64211,\"start\":63789},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":64421,\"start\":64212},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":64636,\"start\":64422},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":65018,\"start\":64637},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":65166,\"start\":65019}]", "paragraph": "[{\"end\":3186,\"start\":2197},{\"end\":3968,\"start\":3188},{\"end\":4306,\"start\":3970},{\"end\":5913,\"start\":4308},{\"end\":7278,\"start\":5915},{\"end\":8073,\"start\":7280},{\"end\":8224,\"start\":8075},{\"end\":8248,\"start\":8226},{\"end\":8716,\"start\":8250},{\"end\":9113,\"start\":8718},{\"end\":9549,\"start\":9115},{\"end\":9957,\"start\":9564},{\"end\":12334,\"start\":9969},{\"end\":15090,\"start\":12336},{\"end\":15481,\"start\":15119},{\"end\":17040,\"start\":15483},{\"end\":17173,\"start\":17042},{\"end\":18213,\"start\":17175},{\"end\":18503,\"start\":18242},{\"end\":19758,\"start\":18505},{\"end\":20196,\"start\":19760},{\"end\":20974,\"start\":20198},{\"end\":22649,\"start\":20976},{\"end\":23662,\"start\":22651},{\"end\":23930,\"start\":23664},{\"end\":25541,\"start\":23932},{\"end\":26200,\"start\":25543},{\"end\":26706,\"start\":26202},{\"end\":27556,\"start\":26708},{\"end\":28292,\"start\":27558},{\"end\":28427,\"start\":28294},{\"end\":29105,\"start\":28429},{\"end\":30261,\"start\":29107},{\"end\":31115,\"start\":30263},{\"end\":31771,\"start\":31117},{\"end\":32062,\"start\":31773},{\"end\":32693,\"start\":32087},{\"end\":33323,\"start\":32695},{\"end\":33438,\"start\":33325},{\"end\":33765,\"start\":33440},{\"end\":34265,\"start\":33767},{\"end\":34810,\"start\":34393},{\"end\":35048,\"start\":34812},{\"end\":35661,\"start\":35073},{\"end\":36311,\"start\":35663},{\"end\":37308,\"start\":36313},{\"end\":37732,\"start\":37342},{\"end\":38495,\"start\":37734},{\"end\":39574,\"start\":38497},{\"end\":40561,\"start\":39576},{\"end\":41172,\"start\":40603},{\"end\":42295,\"start\":41174},{\"end\":43281,\"start\":42297},{\"end\":43771,\"start\":43283},{\"end\":44669,\"start\":43816},{\"end\":45104,\"start\":44777},{\"end\":46303,\"start\":45106},{\"end\":47021,\"start\":46305},{\"end\":48032,\"start\":47023},{\"end\":48653,\"start\":48034},{\"end\":49260,\"start\":48672},{\"end\":49789,\"start\":49262},{\"end\":50059,\"start\":49806},{\"end\":51235,\"start\":50061},{\"end\":51869,\"start\":51237},{\"end\":52053,\"start\":51871},{\"end\":53116,\"start\":52055},{\"end\":53514,\"start\":53118},{\"end\":54544,\"start\":53516},{\"end\":54860,\"start\":54546},{\"end\":55246,\"start\":54862},{\"end\":55871,\"start\":55248},{\"end\":56707,\"start\":55894},{\"end\":57537,\"start\":56709},{\"end\":58290,\"start\":57539}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":34392,\"start\":34266},{\"attributes\":{\"id\":\"formula_1\"},\"end\":44776,\"start\":44670}]", "table_ref": "[{\"end\":8472,\"start\":8465},{\"end\":18785,\"start\":18778},{\"end\":23864,\"start\":23857},{\"end\":24394,\"start\":24387},{\"end\":27911,\"start\":27904},{\"end\":28227,\"start\":28220},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28342,\"start\":28335},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30369,\"start\":30362},{\"end\":31486,\"start\":31479},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32692,\"start\":32678},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33912,\"start\":33905},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36635,\"start\":36590},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38362,\"start\":38348},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":38452,\"start\":38438},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38552,\"start\":38538},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39653,\"start\":39639},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41382,\"start\":41375},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42294,\"start\":42249},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42375,\"start\":42350},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":42570,\"start\":42563},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43397,\"start\":43381},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43484,\"start\":43464},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44904,\"start\":44888},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":45035,\"start\":45021},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45139,\"start\":45131},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":46027,\"start\":46020},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":47064,\"start\":47056}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2195,\"start\":2183},{\"attributes\":{\"n\":\"2\"},\"end\":9562,\"start\":9552},{\"attributes\":{\"n\":\"2.1\"},\"end\":9967,\"start\":9960},{\"attributes\":{\"n\":\"2.2\"},\"end\":15117,\"start\":15093},{\"attributes\":{\"n\":\"3\"},\"end\":18240,\"start\":18216},{\"end\":32085,\"start\":32065},{\"attributes\":{\"n\":\"4\"},\"end\":35071,\"start\":35051},{\"attributes\":{\"n\":\"4.1\"},\"end\":37340,\"start\":37311},{\"attributes\":{\"n\":\"4.2\"},\"end\":40601,\"start\":40564},{\"attributes\":{\"n\":\"4.3\"},\"end\":43814,\"start\":43774},{\"end\":48670,\"start\":48656},{\"attributes\":{\"n\":\"5\"},\"end\":49804,\"start\":49792},{\"attributes\":{\"n\":\"6\"},\"end\":55892,\"start\":55874},{\"end\":58302,\"start\":58292},{\"end\":58662,\"start\":58653},{\"end\":61381,\"start\":61372},{\"end\":61586,\"start\":61577},{\"end\":61790,\"start\":61781},{\"end\":62414,\"start\":62405},{\"end\":62546,\"start\":62537},{\"end\":63299,\"start\":63290},{\"end\":63596,\"start\":63587},{\"end\":63800,\"start\":63790},{\"end\":64223,\"start\":64213},{\"end\":64433,\"start\":64423},{\"end\":64648,\"start\":64638},{\"end\":65030,\"start\":65020}]", "table": "[{\"end\":59202,\"start\":58712},{\"end\":61370,\"start\":59385},{\"end\":61575,\"start\":61489},{\"end\":61779,\"start\":61693},{\"end\":62403,\"start\":61900},{\"end\":63288,\"start\":62659},{\"end\":63585,\"start\":63412},{\"end\":63788,\"start\":63700},{\"end\":64211,\"start\":63899},{\"end\":64421,\"start\":64333},{\"end\":64636,\"start\":64543},{\"end\":65018,\"start\":64935},{\"end\":65166,\"start\":65075}]", "figure_caption": "[{\"end\":58466,\"start\":58304},{\"end\":58651,\"start\":58469},{\"end\":58712,\"start\":58664},{\"end\":59385,\"start\":59205},{\"end\":61489,\"start\":61383},{\"end\":61693,\"start\":61588},{\"end\":61900,\"start\":61792},{\"end\":62535,\"start\":62416},{\"end\":62659,\"start\":62548},{\"end\":63412,\"start\":63301},{\"end\":63700,\"start\":63598},{\"end\":63899,\"start\":63803},{\"end\":64333,\"start\":64226},{\"end\":64543,\"start\":64436},{\"end\":64935,\"start\":64651},{\"end\":65075,\"start\":65033}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15396,\"start\":15388}]", "bib_author_first_name": "[{\"end\":66245,\"start\":66240},{\"end\":66263,\"start\":66255},{\"end\":66277,\"start\":66273},{\"end\":66294,\"start\":66288},{\"end\":66312,\"start\":66304},{\"end\":66825,\"start\":66820},{\"end\":66978,\"start\":66974},{\"end\":66988,\"start\":66985},{\"end\":67349,\"start\":67344},{\"end\":67363,\"start\":67355},{\"end\":67378,\"start\":67369},{\"end\":67965,\"start\":67958},{\"end\":67987,\"start\":67980},{\"end\":68000,\"start\":67995},{\"end\":68011,\"start\":68008},{\"end\":68024,\"start\":68018},{\"end\":68043,\"start\":68036},{\"end\":68045,\"start\":68044},{\"end\":68066,\"start\":68057},{\"end\":68079,\"start\":68075},{\"end\":68598,\"start\":68597},{\"end\":68615,\"start\":68614},{\"end\":68617,\"start\":68616},{\"end\":69250,\"start\":69246},{\"end\":69922,\"start\":69917},{\"end\":69938,\"start\":69932},{\"end\":69955,\"start\":69948},{\"end\":70253,\"start\":70247},{\"end\":70267,\"start\":70261},{\"end\":70279,\"start\":70274},{\"end\":70293,\"start\":70288},{\"end\":70306,\"start\":70299},{\"end\":70922,\"start\":70917},{\"end\":70935,\"start\":70927},{\"end\":70944,\"start\":70940},{\"end\":70959,\"start\":70951},{\"end\":70974,\"start\":70967},{\"end\":70986,\"start\":70980},{\"end\":71001,\"start\":70993},{\"end\":71015,\"start\":71007},{\"end\":71026,\"start\":71020},{\"end\":71038,\"start\":71034},{\"end\":71053,\"start\":71046},{\"end\":71062,\"start\":71058},{\"end\":71609,\"start\":71601},{\"end\":71623,\"start\":71615},{\"end\":71632,\"start\":71628},{\"end\":71645,\"start\":71640},{\"end\":71657,\"start\":71650},{\"end\":71669,\"start\":71663},{\"end\":72142,\"start\":72141},{\"end\":72144,\"start\":72143},{\"end\":72164,\"start\":72155},{\"end\":72181,\"start\":72172},{\"end\":72191,\"start\":72187},{\"end\":72195,\"start\":72192},{\"end\":72207,\"start\":72201},{\"end\":72219,\"start\":72213},{\"end\":72221,\"start\":72220},{\"end\":72237,\"start\":72229},{\"end\":72670,\"start\":72669},{\"end\":72684,\"start\":72679},{\"end\":72698,\"start\":72693},{\"end\":73065,\"start\":73059},{\"end\":73082,\"start\":73077},{\"end\":73098,\"start\":73094},{\"end\":73112,\"start\":73106},{\"end\":73451,\"start\":73446},{\"end\":73453,\"start\":73452},{\"end\":73936,\"start\":73929},{\"end\":74299,\"start\":74290},{\"end\":74317,\"start\":74309},{\"end\":74932,\"start\":74923},{\"end\":74950,\"start\":74942},{\"end\":75541,\"start\":75535},{\"end\":75816,\"start\":75806},{\"end\":75834,\"start\":75829},{\"end\":75854,\"start\":75845},{\"end\":75873,\"start\":75866},{\"end\":76337,\"start\":76330},{\"end\":76350,\"start\":76344},{\"end\":76366,\"start\":76360},{\"end\":76385,\"start\":76380},{\"end\":76395,\"start\":76390},{\"end\":76410,\"start\":76407},{\"end\":76426,\"start\":76419},{\"end\":76831,\"start\":76827},{\"end\":76846,\"start\":76842},{\"end\":76866,\"start\":76855},{\"end\":76880,\"start\":76874},{\"end\":76895,\"start\":76890},{\"end\":76908,\"start\":76902},{\"end\":76922,\"start\":76919},{\"end\":76951,\"start\":76940},{\"end\":76969,\"start\":76961},{\"end\":77592,\"start\":77586},{\"end\":77866,\"start\":77862},{\"end\":77878,\"start\":77872},{\"end\":77888,\"start\":77884},{\"end\":77901,\"start\":77893},{\"end\":77914,\"start\":77908},{\"end\":78351,\"start\":78345}]", "bib_author_last_name": "[{\"end\":66253,\"start\":66246},{\"end\":66271,\"start\":66264},{\"end\":66286,\"start\":66278},{\"end\":66302,\"start\":66295},{\"end\":66318,\"start\":66313},{\"end\":66833,\"start\":66826},{\"end\":66983,\"start\":66979},{\"end\":66993,\"start\":66989},{\"end\":67353,\"start\":67350},{\"end\":67367,\"start\":67364},{\"end\":67383,\"start\":67379},{\"end\":67978,\"start\":67966},{\"end\":67993,\"start\":67988},{\"end\":68006,\"start\":68001},{\"end\":68016,\"start\":68012},{\"end\":68034,\"start\":68025},{\"end\":68055,\"start\":68046},{\"end\":68073,\"start\":68067},{\"end\":68086,\"start\":68080},{\"end\":68606,\"start\":68599},{\"end\":68612,\"start\":68608},{\"end\":68624,\"start\":68618},{\"end\":68629,\"start\":68626},{\"end\":69258,\"start\":69251},{\"end\":69930,\"start\":69923},{\"end\":69946,\"start\":69939},{\"end\":69961,\"start\":69956},{\"end\":70259,\"start\":70254},{\"end\":70272,\"start\":70268},{\"end\":70286,\"start\":70280},{\"end\":70297,\"start\":70294},{\"end\":70312,\"start\":70307},{\"end\":70925,\"start\":70923},{\"end\":70938,\"start\":70936},{\"end\":70949,\"start\":70945},{\"end\":70965,\"start\":70960},{\"end\":70978,\"start\":70975},{\"end\":70991,\"start\":70987},{\"end\":71005,\"start\":71002},{\"end\":71018,\"start\":71016},{\"end\":71032,\"start\":71027},{\"end\":71044,\"start\":71039},{\"end\":71056,\"start\":71054},{\"end\":71067,\"start\":71063},{\"end\":71613,\"start\":71610},{\"end\":71626,\"start\":71624},{\"end\":71638,\"start\":71633},{\"end\":71648,\"start\":71646},{\"end\":71661,\"start\":71658},{\"end\":71675,\"start\":71670},{\"end\":72153,\"start\":72145},{\"end\":72170,\"start\":72165},{\"end\":72185,\"start\":72182},{\"end\":72199,\"start\":72196},{\"end\":72211,\"start\":72208},{\"end\":72227,\"start\":72222},{\"end\":72246,\"start\":72238},{\"end\":72251,\"start\":72248},{\"end\":72677,\"start\":72671},{\"end\":72691,\"start\":72685},{\"end\":72709,\"start\":72699},{\"end\":72713,\"start\":72711},{\"end\":73075,\"start\":73066},{\"end\":73092,\"start\":73083},{\"end\":73104,\"start\":73099},{\"end\":73121,\"start\":73113},{\"end\":73457,\"start\":73454},{\"end\":73942,\"start\":73937},{\"end\":74307,\"start\":74300},{\"end\":74328,\"start\":74318},{\"end\":74940,\"start\":74933},{\"end\":74961,\"start\":74951},{\"end\":75552,\"start\":75542},{\"end\":75827,\"start\":75817},{\"end\":75843,\"start\":75835},{\"end\":75864,\"start\":75855},{\"end\":75880,\"start\":75874},{\"end\":76342,\"start\":76338},{\"end\":76358,\"start\":76351},{\"end\":76378,\"start\":76367},{\"end\":76388,\"start\":76386},{\"end\":76405,\"start\":76396},{\"end\":76417,\"start\":76411},{\"end\":76432,\"start\":76427},{\"end\":76840,\"start\":76832},{\"end\":76853,\"start\":76847},{\"end\":76872,\"start\":76867},{\"end\":76888,\"start\":76881},{\"end\":76900,\"start\":76896},{\"end\":76917,\"start\":76909},{\"end\":76938,\"start\":76923},{\"end\":76959,\"start\":76952},{\"end\":76975,\"start\":76970},{\"end\":77600,\"start\":77593},{\"end\":77870,\"start\":77867},{\"end\":77882,\"start\":77879},{\"end\":77891,\"start\":77889},{\"end\":77906,\"start\":77902},{\"end\":77918,\"start\":77915},{\"end\":78360,\"start\":78352}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":66179,\"start\":65992},{\"attributes\":{\"doi\":\"10.1109/ICST53961.2022.00039\",\"id\":\"b1\",\"matched_paper_id\":249475654},\"end\":66788,\"start\":66181},{\"attributes\":{\"id\":\"b2\"},\"end\":66924,\"start\":66790},{\"attributes\":{\"doi\":\"10.1109/SP.2018.00046\",\"id\":\"b3\",\"matched_paper_id\":3729194},\"end\":67280,\"start\":66926},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207959980},\"end\":67785,\"start\":67282},{\"attributes\":{\"doi\":\"10.1145/3319535.3354249\",\"id\":\"b5\"},\"end\":67904,\"start\":67787},{\"attributes\":{\"doi\":\"10.1109/SP.2016.15\",\"id\":\"b6\",\"matched_paper_id\":206579056},\"end\":68518,\"start\":67906},{\"attributes\":{\"doi\":\"10.1145/3192366.3192388\",\"id\":\"b7\",\"matched_paper_id\":4918751},\"end\":69207,\"start\":68520},{\"attributes\":{\"doi\":\"10.1145/1066116.1066118\",\"id\":\"b8\",\"matched_paper_id\":9147627},\"end\":69609,\"start\":69209},{\"attributes\":{\"id\":\"b9\"},\"end\":69713,\"start\":69611},{\"attributes\":{\"id\":\"b10\"},\"end\":69874,\"start\":69715},{\"attributes\":{\"doi\":\"10.1145/3428334\",\"id\":\"b11\",\"matched_paper_id\":221137226},\"end\":70220,\"start\":69876},{\"attributes\":{\"doi\":\"10.1145/3243734.3243804\",\"id\":\"b12\",\"matched_paper_id\":52127357},\"end\":70833,\"start\":70222},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":222132978},\"end\":71550,\"start\":70835},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":201130320},\"end\":72083,\"start\":71552},{\"attributes\":{\"doi\":\"10.1109/TSE.2019.2946563\",\"id\":\"b15\",\"matched_paper_id\":102351047},\"end\":72610,\"start\":72085},{\"attributes\":{\"doi\":\"10.1145/96267.96279\",\"id\":\"b16\",\"matched_paper_id\":14313707},\"end\":72936,\"start\":72612},{\"attributes\":{\"doi\":\"10.1016/j.softx.2022.101089\",\"id\":\"b17\",\"matched_paper_id\":248463137},\"end\":73427,\"start\":72938},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8591486},\"end\":73853,\"start\":73429},{\"attributes\":{\"doi\":\"10.1109/MSEC.2018.2889892\",\"id\":\"b19\",\"matched_paper_id\":90263473},\"end\":74187,\"start\":73855},{\"attributes\":{\"doi\":\"10.1145/3359789.3359796\",\"id\":\"b20\",\"matched_paper_id\":204792914},\"end\":74863,\"start\":74189},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":221178890},\"end\":75330,\"start\":74865},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":57168037},\"end\":75491,\"start\":75332},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":133205452},\"end\":75755,\"start\":75493},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11024896},\"end\":76298,\"start\":75757},{\"attributes\":{\"doi\":\"10.1109/SP.2019.00010\",\"id\":\"b25\",\"matched_paper_id\":48364047},\"end\":76759,\"start\":76300},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2388545},\"end\":77582,\"start\":76761},{\"attributes\":{\"id\":\"b27\"},\"end\":77658,\"start\":77584},{\"attributes\":{\"id\":\"b28\"},\"end\":77787,\"start\":77660},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":52047247},\"end\":78323,\"start\":77789},{\"attributes\":{\"id\":\"b30\"},\"end\":78437,\"start\":78325}]", "bib_title": "[{\"end\":66238,\"start\":66181},{\"end\":66972,\"start\":66926},{\"end\":67342,\"start\":67282},{\"end\":67956,\"start\":67906},{\"end\":68595,\"start\":68520},{\"end\":69244,\"start\":69209},{\"end\":69758,\"start\":69715},{\"end\":69915,\"start\":69876},{\"end\":70245,\"start\":70222},{\"end\":70915,\"start\":70835},{\"end\":71599,\"start\":71552},{\"end\":72139,\"start\":72085},{\"end\":72667,\"start\":72612},{\"end\":73057,\"start\":72938},{\"end\":73444,\"start\":73429},{\"end\":73927,\"start\":73855},{\"end\":74288,\"start\":74189},{\"end\":74921,\"start\":74865},{\"end\":75374,\"start\":75332},{\"end\":75533,\"start\":75493},{\"end\":75804,\"start\":75757},{\"end\":76328,\"start\":76300},{\"end\":76825,\"start\":76761},{\"end\":77860,\"start\":77789}]", "bib_author": "[{\"end\":66255,\"start\":66240},{\"end\":66273,\"start\":66255},{\"end\":66288,\"start\":66273},{\"end\":66304,\"start\":66288},{\"end\":66320,\"start\":66304},{\"end\":66835,\"start\":66820},{\"end\":66985,\"start\":66974},{\"end\":66995,\"start\":66985},{\"end\":67355,\"start\":67344},{\"end\":67369,\"start\":67355},{\"end\":67385,\"start\":67369},{\"end\":67980,\"start\":67958},{\"end\":67995,\"start\":67980},{\"end\":68008,\"start\":67995},{\"end\":68018,\"start\":68008},{\"end\":68036,\"start\":68018},{\"end\":68057,\"start\":68036},{\"end\":68075,\"start\":68057},{\"end\":68088,\"start\":68075},{\"end\":68608,\"start\":68597},{\"end\":68614,\"start\":68608},{\"end\":68626,\"start\":68614},{\"end\":68631,\"start\":68626},{\"end\":69260,\"start\":69246},{\"end\":69932,\"start\":69917},{\"end\":69948,\"start\":69932},{\"end\":69963,\"start\":69948},{\"end\":70261,\"start\":70247},{\"end\":70274,\"start\":70261},{\"end\":70288,\"start\":70274},{\"end\":70299,\"start\":70288},{\"end\":70314,\"start\":70299},{\"end\":70927,\"start\":70917},{\"end\":70940,\"start\":70927},{\"end\":70951,\"start\":70940},{\"end\":70967,\"start\":70951},{\"end\":70980,\"start\":70967},{\"end\":70993,\"start\":70980},{\"end\":71007,\"start\":70993},{\"end\":71020,\"start\":71007},{\"end\":71034,\"start\":71020},{\"end\":71046,\"start\":71034},{\"end\":71058,\"start\":71046},{\"end\":71069,\"start\":71058},{\"end\":71615,\"start\":71601},{\"end\":71628,\"start\":71615},{\"end\":71640,\"start\":71628},{\"end\":71650,\"start\":71640},{\"end\":71663,\"start\":71650},{\"end\":71677,\"start\":71663},{\"end\":72155,\"start\":72141},{\"end\":72172,\"start\":72155},{\"end\":72187,\"start\":72172},{\"end\":72201,\"start\":72187},{\"end\":72213,\"start\":72201},{\"end\":72229,\"start\":72213},{\"end\":72248,\"start\":72229},{\"end\":72253,\"start\":72248},{\"end\":72679,\"start\":72669},{\"end\":72693,\"start\":72679},{\"end\":72711,\"start\":72693},{\"end\":72715,\"start\":72711},{\"end\":73077,\"start\":73059},{\"end\":73094,\"start\":73077},{\"end\":73106,\"start\":73094},{\"end\":73123,\"start\":73106},{\"end\":73459,\"start\":73446},{\"end\":73944,\"start\":73929},{\"end\":74309,\"start\":74290},{\"end\":74330,\"start\":74309},{\"end\":74942,\"start\":74923},{\"end\":74963,\"start\":74942},{\"end\":75554,\"start\":75535},{\"end\":75829,\"start\":75806},{\"end\":75845,\"start\":75829},{\"end\":75866,\"start\":75845},{\"end\":75882,\"start\":75866},{\"end\":76344,\"start\":76330},{\"end\":76360,\"start\":76344},{\"end\":76380,\"start\":76360},{\"end\":76390,\"start\":76380},{\"end\":76407,\"start\":76390},{\"end\":76419,\"start\":76407},{\"end\":76434,\"start\":76419},{\"end\":76842,\"start\":76827},{\"end\":76855,\"start\":76842},{\"end\":76874,\"start\":76855},{\"end\":76890,\"start\":76874},{\"end\":76902,\"start\":76890},{\"end\":76919,\"start\":76902},{\"end\":76940,\"start\":76919},{\"end\":76961,\"start\":76940},{\"end\":76977,\"start\":76961},{\"end\":77602,\"start\":77586},{\"end\":77872,\"start\":77862},{\"end\":77884,\"start\":77872},{\"end\":77893,\"start\":77884},{\"end\":77908,\"start\":77893},{\"end\":77920,\"start\":77908},{\"end\":78362,\"start\":78345}]", "bib_venue": "[{\"end\":66448,\"start\":66433},{\"end\":67073,\"start\":67056},{\"end\":67552,\"start\":67472},{\"end\":68163,\"start\":68146},{\"end\":68873,\"start\":68752},{\"end\":69380,\"start\":69358},{\"end\":70528,\"start\":70424},{\"end\":71750,\"start\":71730},{\"end\":74520,\"start\":74427},{\"end\":75030,\"start\":75015},{\"end\":75953,\"start\":75938},{\"end\":76498,\"start\":76495},{\"end\":77058,\"start\":77040},{\"end\":77990,\"start\":77972},{\"end\":66057,\"start\":65992},{\"end\":66431,\"start\":66348},{\"end\":66818,\"start\":66790},{\"end\":67054,\"start\":67016},{\"end\":67470,\"start\":67385},{\"end\":68144,\"start\":68106},{\"end\":68750,\"start\":68654},{\"end\":69356,\"start\":69283},{\"end\":69638,\"start\":69611},{\"end\":69774,\"start\":69760},{\"end\":70012,\"start\":69978},{\"end\":70422,\"start\":70337},{\"end\":71140,\"start\":71069},{\"end\":71728,\"start\":71677},{\"end\":72318,\"start\":72277},{\"end\":72745,\"start\":72734},{\"end\":73159,\"start\":73150},{\"end\":73540,\"start\":73459},{\"end\":73992,\"start\":73969},{\"end\":74425,\"start\":74353},{\"end\":75013,\"start\":74963},{\"end\":75384,\"start\":75376},{\"end\":75575,\"start\":75554},{\"end\":75936,\"start\":75882},{\"end\":76493,\"start\":76455},{\"end\":77038,\"start\":76977},{\"end\":77674,\"start\":77660},{\"end\":77970,\"start\":77920},{\"end\":78343,\"start\":78325}]"}}}, "year": 2023, "month": 12, "day": 17}