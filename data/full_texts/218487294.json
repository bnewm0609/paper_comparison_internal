{"id": 218487294, "updated": "2023-10-06 15:46:36.142", "metadata": {"title": "Heterogeneous Knowledge Distillation using Information Flow Modeling", "authors": "[{\"first\":\"Nikolaos\",\"last\":\"Passalis\",\"middle\":[]},{\"first\":\"Maria\",\"last\":\"Tzelepi\",\"middle\":[]},{\"first\":\"Anastasios\",\"last\":\"Tefas\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": 5, "day": 2}, "abstract": "Knowledge Distillation (KD) methods are capable of transferring the knowledge encoded in a large and complex teacher into a smaller and faster student. Early methods were usually limited to transferring the knowledge only between the last layers of the networks, while latter approaches were capable of performing multi-layer KD, further increasing the accuracy of the student. However, despite their improved performance, these methods still suffer from several limitations that restrict both their efficiency and flexibility. First, existing KD methods typically ignore that neural networks undergo through different learning phases during the training process, which often requires different types of supervision for each one. Furthermore, existing multi-layer KD methods are usually unable to effectively handle networks with significantly different architectures (heterogeneous KD). In this paper we propose a novel KD method that works by modeling the information flow through the various layers of the teacher model and then train a student model to mimic this information flow. The proposed method is capable of overcoming the aforementioned limitations by using an appropriate supervision scheme during the different phases of the training process, as well as by designing and training an appropriate auxiliary teacher model that acts as a proxy model capable of\"explaining\"the way the teacher works to the student. The effectiveness of the proposed method is demonstrated using four image datasets and several different evaluation setups.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.00727", "mag": "3035163969", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/PassalisTT20", "doi": "10.1109/cvpr42600.2020.00241"}}, "content": {"source": {"pdf_hash": "df9cf466b04975070d071d765b5e6e4b0a07a127", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.00727v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.00727", "status": "GREEN"}}, "grobid": {"id": "2963a96424b03fc527237ab061cb3777e9dd8072", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/df9cf466b04975070d071d765b5e6e4b0a07a127.txt", "contents": "\nHeterogeneous Knowledge Distillation using Information Flow Modeling\n\n\nN Passalis passalis@csd.auth.gr \nDepartment of Informatics\nAristotle University of Thessaloniki\nGreece\n\nM Tzelepi mtzelepi@csd.auth.gr \nDepartment of Informatics\nAristotle University of Thessaloniki\nGreece\n\nA Tefas tefas@csd.auth.gr \nDepartment of Informatics\nAristotle University of Thessaloniki\nGreece\n\nHeterogeneous Knowledge Distillation using Information Flow Modeling\n\nKnowledge Distillation (KD) methods are capable of transferring the knowledge encoded in a large and complex teacher into a smaller and faster student. Early methods were usually limited to transferring the knowledge only between the last layers of the networks, while latter approaches were capable of performing multi-layer KD, further increasing the accuracy of the student. However, despite their improved performance, these methods still suffer from several limitations that restrict both their efficiency and flexibility. First, existing KD methods typically ignore that neural networks undergo through different learning phases during the training process, which often requires different types of supervision for each one. Furthermore, existing multi-layer KD methods are usually unable to effectively handle networks with significantly different architectures (heterogeneous KD). In this paper we propose a novel KD method that works by modeling the information flow through the various layers of the teacher model and then train a student model to mimic this information flow. The proposed method is capable of overcoming the aforementioned limitations by using an appropriate supervision scheme during the different phases of the training process, as well as by designing and training an appropriate auxiliary teacher model that acts as a proxy model capable of \"explaining\" the way the teacher works to the student. The effectiveness of the proposed method is demonstrated using four image datasets and several different evaluation setups.\n\nIntroduction\n\nDespite the tremendous success of Deep Learning (DL) in a wide range of domain [12], most DL methods suffer from a significant drawback: powerful hardware is needed for training and deploying DL models. This significantly hinders DL applications on resource-scarce environments, such as embedded and mobile devices, leading to the development of various methods for overcoming these limitations. Among the most prominent methods for this task  Figure 1. Existing knowledge distillation approaches ignore the existence of critical learning periods when transferring the knowledge, even when multi-layer transfer approaches are used. However, as argued in [1], the information plasticity rapidly declines after the first few training epochs, reducing the effectiveness of knowledge distillation. On the other hand, the proposed method models the information flow in the teacher network and provides the appropriate supervision during the first few critical learning epochs in order to ensure that the necessary connections between successive layers of the networks will be formed. Note that even though this process initially slows down the convergence of the network slightly (epochs 1-8), it allows for rapidly increasing the rate of convergence after the critical learning period ends (epochs [10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25]. The parameter \u03b1 controls the relative importance of transferring the knowledge from the intermediate layers during the various learning phases, as described in detail in Section 3. is knowledge distillation (KD) [9], which is also known as knowledge transfer (KT) [27]. These approaches aim to transfer the knowledge encoded in a large and complex neural network into a smaller and faster one. In this way, it is possible to increase the accuracy of the smaller model, compared to the same model trained without employing KD.\n\nTypically, the smaller model is called student model, while the larger model is called teacher model. Early KD approaches focused on transferring the knowledge between the last layer of the teacher and student mod-els [4,9,16,23,25,28]. This allowed for providing richer training targets to the student model, which capture more information regarding the similarities between different samples, reducing overfitting and increasing the student's accuracy. Later methods further increased the efficiency of KD by modeling and transferring the knowledge encoded in the intermediate layers of the teacher [19,27,29]. These approaches usually attempt to implicitly model the way information gets transformed through the various layers of a network, providing additional hints to the student model regarding the way that the teacher model process the information.\n\nEven though these methods were indeed able to further increase the accuracy of models trained with KD, they also suffer from several limitations that restrict both their efficiency and flexibility. First, note that neural networks exhibit an evolving behavior, undergoing several different and distinct phases during the training process. For example, during the first few epochs critical connections are formed [1], defining almost permanently the future information flow paths on a network. After fixing these paths, the training process can only fine-tune them, while forming new paths is significantly less probable after the critical learning period ends [1]. After forming these critical connections, the fitting and compression (when applicable) phases follow [21,20]. Despite this dynamic time-dependent behavior of neural networks, virtually all existing KD approaches ignores the phases that neural networks undergo during the training. This observation leads us to the first research question of this paper: Is a different type of supervision needed during the different learning phases of the student and is it possible to use a stronger teacher to provide this supervision?\n\nTo this end, we propose a simple, yet effective way to exploit KD to train a student that mimics the information flow paths of the teacher, while also providing further evidence confirming the existence of critical learning periods during the training phase of a neural network, as originally described in [1]. Indeed, as also demonstrated in the ablation study shown in Fig. 1, providing the correct supervision during the critical learning period of a neural network can have a significant effect on the overall training process, increasing the accuracy of the student model. More information regarding this ablation study are provided in Section 4. It is worth noting that the additional supervision, which is employed to ensure that the student will form similar information paths to the teacher, actually slows down the learning process until the critical learning period is completed. However, after the information flow paths are formed, the rate of convergence is significantly accelerated compared to the student networks that do not take into account the existence of critical learning periods.\n\nAnother limitation of existing KD approaches that em- ploy multiple intermediate layers is their ability to handle heterogeneous multi-layer knowledge distillation, i.e., transfer the knowledge between teachers and students with vastly different architectures. Existing methods almost exclusively use network architectures that provide a trivial one-to-one matching between the layers of the student and teacher, e.g., ResNets with the same number of blocks are often used, altering only the number of layers inside of each residual block [27,29]. Many of these approaches, such as [27], are even more restrictive, also requiring the layers of the teacher and student to have the same dimensionality. As a result, it is especially difficult to perform multilayer KD between networks with vastly different architectures, since even if just one layer of the teacher model is incorrectly matched to a layer of the student model, then the accuracy of the student can be significantly reduced, either due to over-regularizing the network or by forcing to early compress the representations of the student. This be-havior is demonstrated in Fig. 2, where the knowledge is transferred from the 3rd layer of two different teachers to various layers of the student. These findings lead us to the second research question of this paper: Is it possible to handle heterogeneous KD in a structured way to avoid such phenomena?\n\nTo this end, in this work, we propose a simple, yet effective approach for training an auxiliary teacher model, which is closer to the architecture of the student model. This auxiliary teacher is responsible for explaining the way the larger teacher works to the student model. Indeed, this approach can significantly increase the accuracy of the teacher, as demonstrated both in Fig 2, as well as on the rest of the experiments conducted in this paper. It is worth noting that during our initial experiments it was almost impossible to find a layer matching that would actually help us to improve the accuracy of the student model without first designing an appropriate auxiliary teacher model, highlighting the importance of using auxiliary teachers in heterogeneous KD scenarios, as also highlighted in [15].\n\nThe main contribution of this paper is proposing a KD method that works by modeling the information flow through the teacher model and then training a student model to mimic this information flow. However, as it was explained previously and experimentally demonstrated in this paper, this process is often very difficult, especially when there is no obvious layer matching between the teacher and student models, which can often process the information in vastly different ways. In fact, even a single layer mismatch, i.e., overly regularizing the network or forcing for an early compression of the representation, can significantly reduce the accuracy of the student model. To overcome these limitations the proposed method works by a) designing and training an appropriate auxiliary teacher model that allows for a direct and effective one-to-one matching between the layers of the student and teacher models, as well as b) employing a critical-learning aware KD scheme that ensures that critical connections will be formed allowing for effectively mimicking the teacher's information flow instead of just learning a student that mimics the output of the student.\n\nThe effectiveness of the proposed method is demonstrated using several different tasks, ranging from metric learning and classification to mimicking handcrafted feature extractors for providing fast neural network-based implementations for low power embedded hardware. The experimental evaluation also includes an extensive representation learning evaluation, given its increasing importance in many embedded DL and robotic applications and following the evaluation protocol of recently proposed KD methods [16,28]. An open-source implementation of the proposed method is provided in https://github.com/ passalis/pkth.\n\nThe rest of the paper is structured as follows. First, the related work is briefly discussed and compared to the proposed method in Section 2. Then, the proposed method is presented in Section 3, while the experimental evaluation is provided in Section 4. Finally, conclusions are drawn in Section 5.\n\n\nRelated Work\n\nA large number of knowledge transfer methods which build upon the neural network distillation approach have been proposed [2,4,9,23,25]. These methods typically use a teacher model to generate soft-labels and then use these soft-labels for training a smaller student network. It is worth noting that several extensions to this approach have been proposed. For example, soft-labels can be used for pretraining a large network [22] and performing domain adaption [25], while an embedding-based approach for transferring the knowledge was proposed in [17]. Also, online distillation methods, such as [3,30], employ a co-training strategy, training both the student and teacher models simultaneously. However, none of these approaches take into account that deep neural networks transition through several learning phases, each one with different characteristics, which requires handling them in different ways. On the other hand, the proposed method models the information flow in the teacher model and then employs a weighting scheme that provides the appropriate supervision during the initial critical learning period of the student, ensuring that the critical connections and information paths formed in the teacher model will be transferred to the student.\n\nFurthermore, several methods that support multi-layer KD have been proposed, such as using hints [19], the flow of solution procedure matrix (FSP) [27], attention transfer [29], or singular value decomposition to extract major features from each layer [13]. However, these approaches usually only target networks with compatible architecture, e.g., residual networks with the same number of residual blocks, both for the teacher and student models. Also, it is not straightforward to use them to successfully transfer the knowledge between heterogeneous models, since even a slight layer mismatch can have a devastating effect on the student's accuracy, as demonstrated in Fig. 2. It is also worth noting that we were actually unable to effectively apply most of these methods for heterogenous KD, since either they do not support transferring the knowledge between layers of different dimensionality, e.g., [27], or they are prone to over-regularization or representation collapse (as demonstrated in Fig. 2) reducing the overall performance of the student.\n\nIn contrast with the aforementioned approaches, the proposed method provides a way to perform heterogeneous multi-layer KD by appropriately designing and training an auxiliary network and exploiting the knowledge encoded by the earlier layers of this network. In this way, the proposed method provides an efficient way for handling any possible network architecture by employing an auxiliary network that is close to the architecture of the student model regardless the architecture of the teacher model. Using the proposed auxiliary network strategy ensures that the teacher model will transform the representations extracted from the data in a way compatible with the student model, allowing for providing a one-to-one matching between the intermediate layers of the networks. It is also worth nothing that the use of a similar auxiliary network, which is used as an intermediate step for KD, was also proposed in [15]. However in contrast with the proposed method, the auxiliary network used in [15] was employed for merely improving the performance of KD between the final classification layers, instead of designing an auxiliary network that can facilitate efficient multi-layer KD, as proposed in this paper. Finally, to the best of our knowledge, in this work we propose the first architecture-agnostic probabilistic KD approach that works by modeling the information flow through the various layers of the teacher model using a hybrid kernel formulation, can support heterogeneous network architectures and can effectively supervise the student model during its critical learning period.\n\n\nProposed Method\n\nLet T = {t 1 , t 2 , . . . , t N } denote the transfer set that contains N transfer samples and it is used to transfer the knowledge from the teacher model to the student model. Note that the proposed method can also work in a purely unsupervised fashion and, as a result, unlabeled data samples can be also used for transferring the knowledge. Also, let x (l) i = f (t i , l) denote the representation extracted from the l-th layer of the teacher model f (\u00b7) and y (l) i = g(t i , l, W) denote the representation extracted from the l-th layer of the student model g(\u00b7). Note that the trainable parameters of the student model are denoted by W. The proposed method aims to train the student model g(\u00b7), i.e., learn the appropriate parameters W, in order to \"mimic\" the behavior of f (\u00b7) as close as possible.\n\nFurthermore, let X (l) denote the random variable that describes the representation extracted from the l-th layer of the teacher model and Y (l) the corresponding random variable for the student model. Also, let Z denote the random variable that describes the training targets for the teacher model. In this work, the information flow of the teacher network is defined as progression of mutual information between every layer representation of the network and the training targets, i.e., I(X (l) , Z) \u2200 l. Note that even though the training targets are required for modeling the information flow, they are not actually needed during the KD process, as we will demonstrate later. Then, we can define the information flow vector that characterizes the way the network process infor-mation as: (1) where N Lt is the number of layers of the teacher model. Similarly, the information flow vector for the student model is defined as:\n\u03c9 t := I(X (1) , Z), . . . , I(X (N L t ) , Z) T \u2208 R N L t ,\u03c9 s := I(Y (1) , Z), . . . , I(Y (N Ls ) , Z) T \u2208 R N Ls ,(2)\nwhere again N Ls is the number of layers of the student model. The proposed method works by minimizing the divergence between the information flow in the teacher and student models, i.e.,\nD F (\u03c9 s , \u03c9 t ), where D F (\u00b7)\nis a metric used to measure the divergence between two, possibly heterogeneous, networks. To this end, the information flow divergence is defined as the sum of squared differences between each paired element of the information flow vectors:\n(3) D F (\u03c9 s , \u03c9 t ) = N Ls i=1 [\u03c9 s ] i \u2212 [\u03c9 t ] \u03ba(i) 2 ,\nwhere the layer of the teacher \u03ba(i) is chosen in order to minimize the divergence with the corresponding layer of the teacher:\n\u03ba(i) = N Lt if i = N Ls arg min j ([\u03c9 s ] i \u2212 [\u03c9 t ] j ) 2 , otherwise(4)\nand the notation [x] i is used to refer to the i-th element of vector x. This definition employs the optimal matching between the layers (considering the discriminative power of each layer), except from the final one, which corresponds to the task hand. In this way, it allows for measuring the flow divergence between networks with different architectures. At the same time it is also expected to minimize the impact of over-regularization and/or representation collapse phenomena, such as those demonstrated in Fig. 2, which often occur when there is large divergence between the layers used for transferring the knowledge. However, this also implies that for networks with vastly different architectures or for networks not yet trained for the task at hand, the same layer of the teacher may be used for transferring the knowledge to multiple layers of the student model, leading to a significant loss of granularity during the KD and leading to stability issues. In Subsection 3.2 we provide a simple, yet effective way to overcome this issue by using auxiliary teacher models. Note that more advanced methods, such as employing fuzzy assignments between different sets of layers can be also used.\n\n\nTractable Information Flow Divergence Measures using Quadratic Mutual Information\n\nIn order to effectively transfer the knowledge between two different networks, we have to provide an efficient way to calculate the mutual information, as well as to train the student model to match the mutual information between two layers of different networks. Recently, it has been demonstrated that when the Quadratic Mutual Information (QMI) [24] is used, it is possible to efficiently minimize the difference between the mutual information of a specific layer of the teacher and student by appropriately relaxing the optimization problem [16]. More specifically, the problem of matching the mutual information between two layers can be reduced into a simpler probability matching problem that involves only the pairwise interactions between the transfer samples. Therefore, to transfer the knowledge between a specific layer of the student and an another layer of the teacher, it is adequate to minimize the divergence between the teacher's and student's conditional probability distributions, which can be estimated as [16]:\np (t,l t ) i|j = K(x (l t ) i , x (l t ) j ) N i=1,i =j K(x (l t ) i , x (l t ) j ) \u2208 [0, 1],(5)\nand p (s,ls) i|j\n= K(y (ls) i , y (ls) j ) N i=1,i =j K(y (ls) i , y (ls) j ) \u2208 [0, 1],(6)\nwhere K(\u00b7) is a kernel function and l t and l s refer to the student and teacher layers used for the transfer. These probabilities also express how probable is for each sample to select each of its neighbors [14], modeling in this way the geometry of the feature space, while matching these two distributions also ensures that the mutual information between the models and a set of (possibly unknown) classes is maintained [16]. Note that the actual training labels are not required during this process, and, as a result, the proposed method can work in a purely unsupervised fashion. The kernel choice can have a significant effect on the quality of the KD, since it alters how the mutual information is estimated [16]. Apart from the well known Gaussian kernel, which is however often hard to tune, other kernel choices include cosine-based kernels [16], e.g., K c (a, b) = 1 2 ( a T b ||a||2||b||2 + 1), where a and b are two vectors, and the T-student kernel, i.e.,\nK T (a, b) = 1 1+||a\u2212b|| d 2 ,\nwhere d is typically set to 1. Selecting the most appropriate kernel for the task at hand can lead to significant performance improvements, e.g., cosine-based kernels perform better for retrieval tasks, while using kernel ensembles, i.e., estimating the probability distribution using multiple kernels, can also improve the robustness of mutual information estimation. Therefore, in this paper a hybrid objective that aims at minimizing the divergence calculated using both the cosine kernel, which ensures the good performance of the learned representation for retrieval tasks, and the T-student kernel, which experimentally demonstrated good performance for classification tasks, is used: where D(\u00b7) is a probability divergence metric and the notation P (t,lt) c and P (t,lt) T is used to denote the conditional probabilities of the teacher calculated using the cosine and T-student kernels respectively. Again, the representations used for KD were extracted from the l t -th/l s -th layer. The student probability distribution is denoted similarly by P (s,ls) c and P (s,ls) T . The divergence between these distributions can be calculated using a symmetric version of the Kullback-Leibler (KL) divergence, the Jeffreys divergence [10]:\nL ((8) D(P (t,l t ) ||P (s,ls) ) = N i=1 N j=1,i =j p (t,l t ) j|i \u2212 p (s,ls) j|i \u00b7 log p (t,l t ) j|i \u2212 log p (s,ls) j|i ,\nwhich can be sampled at a finite number of points during the optimization, e.g., using batches of 64-128 samples. This batch-based strategy has been successfully employed in a number of different works [16,28], without any significant effect on the optimization process.\n\n\nAuxiliary Networks and Information Flow\n\nEven though the flow divergence metric defined in (3) takes into account the way different networks process the information, it suffers from a significant drawback: if the teacher process the information in a significantly different way compared to the student, then the same layer of the teacher model might be used for transferring the knowledge to multiple layers of the student model, leading to a significant loss in the granularity of information flow used for KD. Furthermore, this problem can also arise even when the student model is capable of processing the information in a way compatible with the teacher, but it has not been yet appropriately trained for the task at hand. To better understand this, note that the information flow divergence in (3) is calculated based on the estimated mutual information and not the actual learning capacity of each model. Therefore, directly using the flow divergence definition presented in (3) is not optimal for KD. It is worth noting that this issue is especially critical for every KD method that employs multiple layer, since as we demonstrate in Section 4, if the layer pairs are not carefully selected, the accuracy of the student model is often lower compared to a model trained without using multi-layer transfer at all.\n\nUnfortunately, due to the poor understanding of the way that neural networks transform the probability distribution of the input data, there is currently no way to select the most appropriate layers for transferring the knowledge a priori. This process can be especially difficult and tedious, especially when the architectures of the student and teacher differ a lot. To overcome this critical limitation in this work we propose constructing an appropriate auxiliary proxy for the teacher model, that will allow for directly matching between all the layers of the auxiliary model and the student model, \n\n\nTeacher Information Flow Vector Student Information Flow Vector\n\n\nInformation Flow Divergence\n\n\nCritical Period-aware Optimization\n\nStep 2: Information Flow Divergence Minimization\n\nStep 1: KD to Auxiliary Figure 3. First, the knowledge is transferred to an appropriate auxiliary teacher, which will better facilitate the process of KD. Then, the proposed method minimizes the information flow divergence between the two models, taking into account the existence of critical learning periods. as shown in Fig. 3. In this way, the proposed method employs an auxiliary network, that has a compatible architecture with the student model, to better facilitate the process of KD. A simple, yet effective approach for designing the auxiliary network is employed in this work: the auxiliary network follows the same architecture as the student model, but using twice the neurons/convolutional filters per layer. Thus, the greater learning capacity of the auxiliary network ensures that enough knowledge will be always available to the auxiliary network (when compared to the student model), leading to better results compared to directly transferring the knowledge from the teacher model. Designing the most appropriate auxiliary network is an open research area and significantly better ways than the proposed one might exist. However, even this simple approach was adequate to significantly enhance the performance of KD and demonstrate the potential of information flow modeling, as further demonstrated in the ablation studies provided in Section 4. Also, note that a hierarchy of auxiliary teachers can be trained in this fashion, as also proposed in [15].\n\nThe final loss used be optimize the student model, when an auxiliary network is employed, is calculated as:\nL = N Ls i=1 \u03b1 i L (i,i) ,(9)\nwhere \u03b1 i is a hyper-parameter that controls the relative weight of transferring the knowledge from the i-th layer of the teacher to the i-th layer of the student and the loss L (i,i) defined in (7) is calculated using the auxiliary teacher, instead of the initial teacher. The value of \u03b1 i can be dynamically selected during the training process, to ensure that the applied KD scheme takes into account the current learning state of the network, as further discussed in Subsection 3.3. Finally, stochastic gradient descent is employed to train the student model: \u2206W = \u2212 \u03b7 \u2202L \u2202W , where W is the matrix with the parameters of the student model and \u03b7 is the employed learning rate.\n\n\nCritical Period-aware Divergence Minimization\n\nNeural networks transition through different learning phases during the training process, with the first few epochs being especially critical for the later behavior of the network [1]. Using a stronger teacher model provides the opportunity of guiding the student model during the initial critical learning period in order to form the appropriate connectivity between the layers, before the information plasticity declines. However, just minimizing the information flow divergence does not ensure that the appropriate connections will be formed. To better understand this, we have to consider that the gradients back-propagated through the network depend both on the training target, as well as on the initialization of the network. Therefore, for a randomly initialized student, the task of forming the appropriate connections between the intermediate layers might not facilitate the final task at hand (until reaching a certain critical point). This was clearly demonstrated in Fig 1, where the convergence of the network was initially slower, when the proposed method was used, until reaching the point at which the critical learning period ends and the convergence of the network is accelerated.\n\nTherefore, in this work we propose using an appropriate weighting scheme for calculating the value of the hyperparameter \u03b1 i during the training process. More specifically, during the critical learning period a significantly higher weight is given to match the information flow for the earlier layers, ignoring the task at hand dictated by the final layer of the teacher, while this weight gradually decays to 0, as the training process progresses. Therefore, the parameter \u03b1 i is calculated as:\n(10) \u03b1 i = 1, if i = N L S \u03b1 init \u00b7 \u03b3 k otherwise ,\nwhere k is the current training epoch, \u03b3 is a decay factor and \u03b1 init is the initial weight used for matching the information flow in the intermediate layers. The parameter \u03b3 was set to 0.7, while \u03b1 init was set to 100 for all the experiments conducted in this paper (unless otherwise stated). Therefore, during the first few epochs (1-10) the final task at hand has a minimal impact on the optimization objective. However, as the training process progresses the importance of matching the information flow for the intermediate layers gradually diminishes and the optimization switches to fine-tuning the network for the task at hand.\n\n\nExperimental Evaluation\n\nThe experimental evaluation of the proposed method is provided in this Section. The proposed method was eval-  and compared to four competitive KD methods: neural network distillation [9], hint-based transfer [19], probabilistic knowledge transfer (PKT) [16] and metric knowledge transfer (abbreviated as MKT) [28]. A variety of different evaluation setups were used to evaluate various aspects of the proposed method. Please refer to the appendix for a detailed description of the employed networks and evaluation setups. First, the proposed method was evaluated in a metric learning setup using the CIFAR-10 dataset ( Table 1). The methods were evaluated under two different settings: a) using contrastive supervision (by adding a contrastive loss term in the loss function [8]), as well as b) using a purely unsupervised setting (cloning the responses of the powerful teacher model). The simple variants (Hint, MKT, PKT) refer to transferring the knowledge only from the penultimate layer of the teacher, while the \"-H\" variants refer to transferring the knowledge simultaneously from all the layers of the auxiliary model. The abbreviation \"e\" is used to refer to retrieval using the Euclidean similarity metric, while \"c\" is used to refer to retrieval using the cosine similarity.\n\nFirst, note that using all the layers for distilling the knowledge provides small to no improvements in the retrieval precision, with the exception of the MKT method (when applied without any form of supervision). Actually, in some cases (e.g., when hint-based transfer is employed) the performance when multiple layers are used is worse. This behavior further confirms and highlights the difficulty in applying multi-layer KD methods between heterogeneous architectures. Also, using contrastive supervision seems to provide more consistent results for the competitive methods, especially for the MKT method. Using the proposed method leads to a significant increase in the mAP, as well as in the top-K precision. For example, mAP (c) increases by over 2.5% (relative increase) over the next best performing method (PKT-H). At the same time, note that the proposed method seems to lead to overall better results when there is no additional supervision. This is again linked to the existence of critical learning periods. As explained before, forming the appropriate information flow paths requires little to no supervision from the final layers, when the network is randomly initialized (since forming these paths usually change the way the network process information, increasing temporarily the loss related to the final task at hand). Similar conclusions can be also drawn from the classification evaluation using the CIFAR-10 dataset. The results are reported in Table 2. Again, the proposed method leads to a relative increase of about 0.7% over the next bestperforming method. Next, the proposed method was evaluated under a distribution shift setup using the STL-10 dataset (Table 3). For these experiments, the teacher model was trained using the CIFAR-10 dataset, but the KD was conducted using the unlabeled split of the STL dataset. Again, similar results as with the CIFAR-10 dataset are observed, with the proposed method outperforming the rest of the evaluated methods over all the evaluated metrics. Again, it is worth noting that directly transferring the knowledge between all the layers of the network often harms the retrieval precision for the competitive approaches. This behavior is also confirmed using the more challenging CUB-200 data set (Table 4), where the proposed method again outperforms the rest of the evaluated approaches both for the retrieval evaluation, as well as for the classification evaluation. For the latter, a quite large improvement is observed, since the accuracy increases by over 1.5% over the next best performing method. Furthermore, we also conducted a HoG [7] cloning experiment, in which the knowledge was transferred from a handcrafted feature extractor to demonstrate the flexibility of the proposed approach. The same strategy as in the previous experiments were used, i.e., the knowledge was first transferred to an auxiliary model and then further distilled to the student model. It is worth noting that this setup has several emerging applications, as discussed in a variety of recent works [16,5], since it allows for pre-training deep neural networks for domains for which it is difficult to acquire large annotated datasets, as well as providing a straightforward way to exploit the highly optimized deep learning libraries for embedded devices to provide neural networkbased implementations of hand-crafted features. The evaluation results for this setup are reported in Table 5, confirming again that the proposed method outperforms the rest of the evaluated methods.\n\nFinally, several ablation studies have been conducted. First, in Fig. 1 we evaluated the effect of using the proposed weighting scheme that takes into account the existence of critical learning periods. The proposed scheme indeed leads to faster convergence over both single layer KD using the PKT method, as well as over the multi-layer PKT-H method. To validate that the improved results arise from the higher weight given to the intermediate layers over the critical learning period, we used the same decaying scheme for the PKT-H method, but with the initial \u03b1 init set to 1 instead of 100. Next, we also demonstrated the impact of matching the correct layers in Fig. 2. Several interesting conclusions can be drawn from the results reported in Fig. 2. For example, note that over-regularization occurs when transferring the knowledge from a teacher layer that has lower MI with the targets (lower NCC accuracy). On the other hand, using a layer with slightly lower discriminative power (Layer 1 of ResNet-18) can have a slightly positive regularization effect. At the same time, using too discriminative layers (Layer 3 and 4 of ResNet-18) can lead to an early collapse of the representation, harming the precision of the student. The accuracy of the student increases only when the correct layers of the auxiliary teacher are matched to the student (Layers 2 and 3 of CNN-1-A). Furthermore, we also evaluated the effect of using auxiliary models of different sizes on the precision of the student model trained with the proposed method. The evaluation results are provided in Table 6. Two different student models are used: CNN-1 (15k parameters) and CNN-1-L (6k parameters). As expected, the auxiliary models that are closer to the complexity of the student lead to improved performance compared both to the more complex and the less complex teachers. That is, when the CNN-1 model is used as student, the CNN-1-A teacher achieves the best results, while when the CNN-1-L is used as student, the weaker CNN-1 teacher achieves the highest precision. Note that as the complexity of the student increases, the efficiency of the KD process declines.\n\n\nConclusions\n\nIn this paper we presented a novel KD method that that works by modeling the information flow through the various layers of the teacher model. The proposed method was able to overcome several limitations of existing KD approaches, especially when used for training very lightweight deep learning models with architectures that differ significantly from the teacher, by a) designing and training an appropriate auxiliary teacher model, and b) employing a criticallearning aware KD scheme that ensures that critical connections will be formed to effectively mimic the information flow paths of the auxiliary teacher.\n\n\nA. Appendix\n\n\nA.1. Datasets and Evaluation Setups\n\nThe proposed method was evaluated using four different datasets: the CIFAR-10 [11] dataset, the STL-10 [6] dataset, the CUB-200 [26] dataset and the SUN Attribute [18] dataset. For the CIFAR-10, the training split was used for training and transferring the knowledge to the student models, while for the retrieval evaluation the training split was also used to compile the database. Then, the test set was used to query the database and measure the retrieval performance of various representations. For the STL-10 dataset we followed the same setup as for the CIFAR-10, but we also used the provided unlabeded training split for transferring the knowledge to the student models. For the CUB-200 we also followed the same setup, however the experiments were conducted using the first 30 classes of the data, due to the significantly restricted learning capacity of the employed student models (recall that among the objectives of the paper is to evaluate the performance of KD approaches for ultra-lightweight network architectures and heterogeneous KD setups). Finally, images from the eight most common categories (for which at least 40 images exist) were used for training and evaluating the methods when the SUN Attribute dataset was employed, since a very small number of images exist for the rest of the categories. The 80% of the extracted images was used for training the networks and building the database, while the rest 20% was used to query the database. The evaluation process was repeated 5 times and the mean and standard deviation of the evaluated metrics are reported. For the SUN attribute dataset, the knowledge was distilled from a 2 \u00d7 2 HoG features.\n\nFor the CIFAR-10 and STL dataset we used the supplied images without performing any resizing (the original 32 \u00d7 32 images were used). However, the training dataset was augmented by randomly performing horizontal flipping and randomly cropping the images using padding of 4 pixels. A similar augmentation protocol was used for the CUB-200 dataset. However, the images of the CUB-200 dataset were first resized into 256 \u00d7 256 pixels and then a random crop of 224\u00d7224 pixels was used (a center crop of the same size was used during the evaluation process). Also, random rotation up to 20 \u2022 was used when training the models. Finally, the images of the SUN attribute dataset were resized into 128\u00d7128 pixels, before feeding them into the network, following the protocol used in [16].\n\n\nA.2. Network Architectures\n\nThe network architectures used for the conducted experiments are shown in Fig. 4. The CNN-1 family was used for the experiments conducted using the CIFAR-10 and STL dataset, the CNN-2 family was used for the experiments conducted using the CUB-200 dataset, while the CNN-3\n\n\nConvolutional 2D (3 x 3, 8 filters)\n\n\nCNN-1\n\n\nConvolutional 2D (3 x 3, 16 filters)\n\n\nConvolutional 2D (3 x 3, 32 filters)\n\nFully Connected (64 Neurons)\n\n\nFully Connected (N C Neurons)\n\n\nConvolutional 2D (3 x 3, 4 filters)\n\n\nCNN-1-L\n\n\nConvolutional 2D (3 x 3, 8 filters)\n\n\nConvolutional 2D (3 x 3, 16 filters)\n\nFully Connected (64 Neurons)\n\n\nFully Connected (N C Neurons)\n\n\nConvolutional 2D (3 x 3, 16 filters)\n\n\nCNN-1-A\n\n\nConvolutional 2D (3 x 3, 32 filters)\n\n\nConvolutional 2D (3 x 3, 64 filters)\n\nFully Connected (128 Neurons)\n\n\nFully Connected (N C Neurons)\n\n\nConvolutional 2D (3 x 3, 32 filters)\n\n\nCNN-1-H\n\n\nConvolutional 2D (3 x 3, 64 filters)\n\nConvolutional 2D (3 x 3, 128   family was used for the SUN Attribute dataset. The suffix \"-A\" is used to denote the model that was used as the auxiliary teacher. The auxiliary teacher was trained using the PKT method [16], by transferring the knowledge from the penultimate layer of a ResNet-18 teacher (for the CIFAR-10, STL and CUB-200 datasets) or from handcrafted features (for the SUN Attribute dataset). The ReLU activation function was used for all the layers, while the batch normalization was used after each convolutional layer.\n\n\nCNN-2-A\n\n\nConvolutional 2D (5 x 5, 64 filters)\n\n\nFully Connected (128 Neurons)\n\n\nConvolutional 2D (3 x 3, 4 filters)\n\n\nCNN-3\n\n\nConvolutional 2D (3 x 3, 4 filters)\n\n\nFully Connected (32 Neurons)\n\n\nConvolutional 2D (3 x 3, 8 filters)\n\n\nCNN-3-A\n\n\nConvolutional 2D (3 x 3, 16 filters)\n\n\nFully Connected (128 Neurons)\n\n\nConvolutional 2D (5 x 5, 32 filters)\n\n\nConvolutional 2D (5 x 5, 128 filters)\n\n\nA.3. Training Hyper-parameters\n\nFor all the conducted experiments we used the Adam optimizer, with the default training hyper-parameters. For the experiments conducted using the CIFAR-10 dataset the optimization ran for 50 training epochs with a learning rate of 0.001 (batches of 128 samples were used) for all the evaluated methods. For the ablation results reported in Fig. 2 of the main manuscript the optimization ran for 20 epochs. For the STL dataset the optimization ran for 30 training epochs with a learning rate of 0.001 and batch size equal to 128. For the CUB-200 dataset the optimization ran for 100 training epochs, using a learning rate of 0.001 for the first 50 training epochs and 0.0001 for the subsequent 50 training epochs. Also, for the SUN Attribute dataset the optimization ran for 20 training epochs. Furthermore, the decay factor \u03b3 was set to 0.6 for this dataset, due to the smaller number of training epochs. Finally, note that for the experiments conducted with the contrastive supervision (CIFAR-10) we employed the contrastive loss with the margin set to 1 and the loss was combined with the KD loss after weighting it with 0.1. Also, for the classification experiments reported in Table 2, all the methods were also trained using a supervised classification term (cross-entropy loss). Finally, for all the experiments conducted using the distillation loss, a temperature of T = 2 was used.\n\n\nlt,ls) = D(P (t,lt) c ||P (s,ls) c ) + D(P (t,lt) T ||P (s,ls) T ), (7)\n\nFigure 4 .\n4Network architectures used for the conducted experiments. The green model was used as the student for the conducted experiments (unless otherwise stated), while the red model was used as the auxiliary teacher. For experiments involving classification, an additional fully connected layer with NC (number of classes) neurons was added.\n\n\nNCC) is reported for the representations extracted from each layer (in order to provide an intuitive measure of how each layer transforms the representations extracted from the input data). The final precision is reported for a student model trained by either not using intermediate layer supervision (upper black values) or by using different layers of the teacher (4 subsequent precision values). Several different phenomena are observed when the knowledge is transferred from different layers, while the proposed auxiliary teacher allows for achieving the highest precision and provides a straightforward way to match the layers between the models (the auxiliary teacher transforms the data representations in a way that is closer to the student model, as measure through the NCC accuracy).Layer 1 \n\nNCC: 41.04% \n\nLayer 2 \n\nNCC: 48.82% \n\nLayer 3 \n\nNCC: 59.70% \n\nLayer 4 \n\nNCC: 65.95% \n\nLayer 1 \nNCC: 46.40% \n\nLayer 2 \nNCC: 69.19% \n\nLayer 3 \nNCC: 86.18% \n\nLayer 4 \nNCC: 92.14% \n\nLayer 1 \nNCC: 43.09% \n\nLayer 2 \nNCC: 57.39% \n\nLayer 3 \nNCC: 65.75% \n\nLayer 4 \nNCC: 74.71% \n\nNo Intermediate \nLayer Supervision \n76.18% \n\n76.32% \n\n74.30% \n\n69.78% \n\n74.98% \n\nNo Intermediate \nLayer Supervision \n\n77.93% \n\n77.39% \n\nStudent precision (top-1) \n@ layer 4 \n\n77.27% \n\n77.76% \n\n76.40% \n\nTeacher \nStudent \n\nAuxiliary Teacher \n\nRepresentation \nCollapse \n\nOver-\nregularization \n\nCorrect \nlayer \nmatching \n\nPositive \nregularization \neffect \n\nResNet-18 \n\nCNN-1 \nCNN-1-A \n\nFigure 2. Examining the effect of transferring the knowledge from \ndifferent layers of a teacher model into the third layer of the stu-\ndent model. Two different teachers are used, a strong teacher \n(ResNet-18, where each layer refers to each layer block) and an \nauxiliary teacher (CNN-1-A). The nearest centroid classifier accu-\nracy (\n\nTable 1 .\n1Metric Learning Evaluation: CIFAR-10Method \nmAP (e) \nmAP (c) \ntop-100 (e) \ntop-100 (c) \nBaseline Models \nTeacher (ResNet-18) \n87.18 \n90.47 \n92.15 \n92.26 \nAux. (CNN1-A) \n62.12 \n66.78 \n73.72 \n75.91 \nWith Constrastive Supervision \nStudent (CNN1) \n47.69 \n48.72 \n57.46 \n58.50 \nHint. \n43.56 \n48.73 \n60.44 \n62.43 \nMKT \n45.34 \n46.84 \n55.89 \n57.10 \nPKT \n48.87 \n49.95 \n58.44 \n59.48 \nHint-H \n43.24 \n47.46 \n58.97 \n61.07 \nMKT-H \n44.83 \n47.12 \n56.28 \n57.90 \nPKT-H \n48.69 \n50.09 \n58.71 \n60.20 \nProposed \n49.55 \n50.82 \n59.50 \n60.79 \nWithout Constrastive Supervision \nStudent (CNN1) \n35.30 \n39.00 \n55.87 \n58.77 \nDistill. \n37.39 \n40.53 \n56.17 \n58.56 \nHint. \n43.99 \n48.99 \n60.69 \n62.42 \nMKT \n36.26 \n38.20 \n50.55 \n52.72 \nPKT \n48.07 \n51.56 \n60.02 \n62.50 \nHint-H \n42.65 \n46.46 \n58.51 \n60.59 \nMKT-H \n41.16 \n43.99 \n55.10 \n57.63 \nPKT-H \n48.05 \n51.73 \n60.39 \n63.01 \nProposed \n49.20 \n53.06 \n61.54 \n64.24 \n\n\n\nTable 2 .\n2Classification Evaluation: CIFAR-10 \nMethod \nTrain Accuracy Test Accuracy \nDistill \n72.50 \n70.68 \nHint. \n71.29 \n70.59 \nMKT \n69.73 \n69.13 \nPKT \n72.70 \n70.44 \nHint-H \n70.93 \n69.52 \nMKT-H \n69.67 \n68.82 \nPKT-H \n73.43 \n71.44 \nProposed \n73.24 \n71.97 \n\nuated using four different datasets (CIFAR-10 [11], STL-\n10 [6], CUB-200 [26] and SUN Attribute [18] datasets) \n\n\nTable 3 .\n3Metric Learning Evaluation: STL Distribution ShiftMethod \nmAP (e) \nmAP (c) \ntop-100 (e) \ntop-100 (c) \nTeacher (ResNet-18) \n57.40 \n61.20 \n66.75 \n69.70 \nAux. (CNN1-A) \n44.89 \n48.48 \n53.54 \n56.26 \nStudent (CNN1) \n30.60 \n33.04 \n39.08 \n41.69 \nDistill \n33.56 \n36.23 \n43.32 \n46.01 \nHint. \n37.11 \n40.33 \n46.60 \n49.46 \nMKT \n33.46 \n35.91 \n40.65 \n43.23 \nPKT \n37.22 \n40.26 \n44.73 \n47.98 \nHint-H \n35.56 \n37.85 \n43.83 \n46.13 \nMKT-H \n33.57 \n35.23 \n40.20 \n42.11 \nPKT-H \n37.56 \n39.77 \n44.76 \n47.17 \nProposed \n38.11 \n40.35 \n48.44 \n50.57 \n\n\n\nTable 4 .\n4Metric Learning and Classification Evaluation: CUB-200Method \nmAP (e) \nmAP (c) \ntop-10 (e) \ntop-10 (c) \nAcc. \nTeacher \n63.17 \n78.17 \n76.02 \n81.64 \n72.16 \nAux. \n17.01 \n18.98 \n25.77 \n27.07 \n32.33 \nStudent \n15.60 \n17.24 \n23.40 \n24.89 \n34.08 \nDistill \n16.40 \n18.55 \n24.82 \n26.57 \n35.21 \nHint. \n14.34 \n15.98 \n22.31 \n23.41 \n28.71 \nMDS \n12.99 \n13.39 \n20.60 \n20.59 \n30.46 \nPKT \n16.36 \n18.57 \n24.68 \n26.70 \n34.96 \nHint-H \n13.94 \n15.37 \n21.75 \n22.61 \n28.34 \nMDS-H \n13.83 \n15.39 \n21.27 \n22.76 \n32.08 \nPKT-H \n15.58 \n17.77 \n23.50 \n25.39 \n33.83 \nProposed \n16.70 \n19.01 \n25.41 \n27.67 \n36.95 \n\nTable 5. HoG Cloning Network: SUN Dataset \n\nMethod \nmAP (c) \ntop-1 (e) \ntop-10 (c) \nHoG \n32.06 \u00b1 1.20 \n62.55 \u00b1 1.10 \n47.93 \u00b1 1.73 \nAux. \n29.69 \u00b1 2.09 \n55.26 \u00b1 3.03 \n42.34 \u00b1 3.71 \nHint \n20.87 \u00b1 2.13 \n44.14 \u00b1 4.11 \n31.15 \u00b1 4.48 \nMDS \n21.65 \u00b1 2.79 \n43.43 \u00b1 4.87 \n31.29 \u00b1 4.24 \nPKT \n27.22 \u00b1 2.60 \n49.90 \u00b1 3.67 \n36.92 \u00b1 2.64 \nProposed \n27.63 \u00b1 0.62 51.18 \u00b1 1.74 38.59 \u00b1 1.00 \n\n\n\nTable 6 .\n6Effect of using auxiliary networks of different sizes (CNN order in term of parameters: CNN-1-H > CNN-1-A > CNN-1 > CNN-1-L)Method \nmAP (e) \nmAP (c) \ntop-100 (e) \ntop-100 (c) \nCNN1-L \u2192 CNN1 \n35.03 \n37.89 \n46.31 \n49.27 \nCNN1-A \u2192 CNN1 \n49.20 \n53.06 \n61.54 \n64.24 \nCNN1-H \u2192 CNN1 \n48.82 \n52.77 \n61.25 \n63.99 \nCNN1 \u2192 CNN1-L \n36.49 \n39.25 \n48.21 \n50.88 \nCNN-1-A \u2192 CNN-1-L \n35.72 \n38.61 \n47.25 \n50.13 \nCNN-1-H \u2192 CNN-1-L \n34.90 \n37.51 \n45.83 \n48.50 \n\n\n\n\nfilters)Fully Connected \n(128 Neurons) \n\nFully Connected \n(N C Neurons) \n\nConvolutional 2D \n(9 x 9, 8 filters, \nstride 2) \n\nCNN-2 \n\nConvolutional 2D \n(5 x 5, 16 filters) \n\nFully Connected \n(64 Neurons) \n\nConvolutional 2D \n(9 x 9, 32 filters, \nstride 2) \n\n\nAcknowledgmentThis work was supported by the European Union's Horizon 2020 Research and Innovation Program (OpenDR) under Grant 871449. This publication reflects the authors\" views only. The European Commission is not responsible for any use that may be made of the information it contains.\nAlessandro Achille, Matteo Rovere, Stefano Soatto, arXiv:1711.08856Critical learning periods in deep neural networks. 6arXiv preprintAlessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep neural networks. arXiv preprint arXiv:1711.08856, 2017. 1, 2, 6\n\nVariational information distillation for knowledge transfer. Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, D Neil, Zhenwen Lawrence, Dai, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information distil- lation for knowledge transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9163-9171, 2019. 3\n\nLarge scale distributed neural network training through online distillation. Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, Geoffrey E Hinton, arXiv:1804.03235arXiv preprintRohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Or- mandi, George E Dahl, and Geoffrey E Hinton. Large scale distributed neural network training through online distilla- tion. arXiv preprint arXiv:1804.03235, 2018. 3\n\nModel compression. Cristian Bucilu, Rich Caruana, Alexandru Niculescu-Mizil, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data mining23Cristian Bucilu, Rich Caruana, and Alexandru Niculescu- Mizil. Model compression. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data mining, pages 535-541, 2006. 2, 3\n\nDistilling the knowledge from handcrafted features for human activity recognition. Zhenghua Chen, Le Zhang, Zhiguang Cao, Jing Guo, IEEE Transactions on Industrial Informatics. 1410Zhenghua Chen, Le Zhang, Zhiguang Cao, and Jing Guo. Distilling the knowledge from handcrafted features for hu- man activity recognition. IEEE Transactions on Industrial Informatics, 14(10):4334-4342, 2018. 8\n\nAn analysis of single-layer networks in unsupervised feature learning. Adam Coates, Andrew Ng, Honglak Lee, Proceedings of the Conference on Artificial Intelligence and Statistics. the Conference on Artificial Intelligence and Statistics79Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Conference on Artificial Intelligence and Statistics, pages 215-223, 2011. 7, 9\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, Proceedings of the Computer Society Conference on Computer Vision and Pattern Recognition. the Computer Society Conference on Computer Vision and Pattern RecognitionN. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Proceedings of the Computer Soci- ety Conference on Computer Vision and Pattern Recognition, pages 886-893, 2005. 8\n\nDimensionality reduction by learning an invariant mapping. Raia Hadsell, Sumit Chopra, Yann Lecun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensional- ity reduction by learning an invariant mapping. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1735-1742, 2006. 7\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Proceedings of the Neural Information Processing Systems Deep Learning Workshop. the Neural Information Processing Systems Deep Learning WorkshopGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In Proceedings of the Neural Information Processing Systems Deep Learning Workshop, 2014. 1, 2, 3, 7\n\nAn invariant form for the prior probability in estimation problems. Harold Jeffreys, Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences. 1865Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453-461, 1946. 5\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 79Technical ReportAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009. 7, 9\n\nDeep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, Nature. 5217553Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, 2015. 1\n\nSelfsupervised knowledge distillation using singular value decomposition. Dae Ha Seung Hyun Lee, Byung Cheol Kim, Song, European Conference on Computer Vision. SpringerSeung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self- supervised knowledge distillation using singular value de- composition. In European Conference on Computer Vision, pages 339-354. Springer, 2018. 3\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of Machine Learning Research. 95Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9:2579-2605, 2008. 5\n\nImproved knowledge distillation via teacher assistant: Bridging the gap between student and teacher. Mehrdad Seyed-Iman Mirzadeh, Ang Farajtabar, Hassan Li, Ghasemzadeh, 6arXiv preprint: 1902.03393, 2019. 3, 4Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher. arXiv preprint: 1902.03393, 2019. 3, 4, 6\n\nLearning deep representations with probabilistic knowledge transfer. Nikolaos Passalis, Anastasios Tefas, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision89Nikolaos Passalis and Anastasios Tefas. Learning deep rep- resentations with probabilistic knowledge transfer. In Pro- ceedings of the European Conference on Computer Vision, pages 268-284, 2018. 2, 3, 5, 7, 8, 9\n\nUnsupervised knowledge transfer using similarity embeddings. Nikolaos Passalis, Anastasios Tefas, IEEE Transactions on Neural Networks and Learning Systems. 303Nikolaos Passalis and Anastasios Tefas. Unsupervised knowledge transfer using similarity embeddings. IEEE Transactions on Neural Networks and Learning Systems, 30(3):946-950, 2018. 3\n\nSun attribute database: Discovering, annotating, and recognizing scene attributes. Genevieve Patterson, James Hays, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition79Genevieve Patterson and James Hays. Sun attribute database: Discovering, annotating, and recognizing scene attributes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2751-2758, 2012. 7, 9\n\nFitnets: Hints for thin deep nets. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations27Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In Proceedings of the International Conference on Learning Representations, 2015. 2, 3, 7\n\nOn the information bottleneck theory of deep learning. Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, David Daniel Cox, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsAndrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. In Proceedings of the International Confer- ence on Learning Representations. 2\n\nOpening the black box of deep neural networks via information. Ravid Shwartz, -Ziv , Naftali Tishby, arXiv:1703.00810arXiv preprintRavid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017. 2\n\n. Zhiyuan Tang, Dong Wang, Yiqiao Pan, Zhiyong Zhang, Knowledge transfer pre-training. arXiv preprint: 1506.02256Zhiyuan Tang, Dong Wang, Yiqiao Pan, and Zhiyong Zhang. Knowledge transfer pre-training. arXiv preprint: 1506.02256, 2015. 3\n\nRecurrent neural network training with dark knowledge transfer. Zhiyuan Tang, Dong Wang, Zhiyong Zhang, Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. the IEEE International Conference on Acoustics, Speech and Signal Processing23Zhiyuan Tang, Dong Wang, and Zhiyong Zhang. Recurrent neural network training with dark knowledge transfer. In Proceedings of the IEEE International Conference on Acous- tics, Speech and Signal Processing, pages 5900-5904, 2016. 2, 3\n\nFeature extraction by non-parametric mutual information maximization. Kari Torkkola, Journal of Machine Learning Research. 35Kari Torkkola. Feature extraction by non-parametric mutual information maximization. Journal of Machine Learning Re- search, 3(Mar):1415-1438, 2003. 5\n\nSimultaneous deep transfer across domains and tasks. Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision23Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Com- puter Vision, pages 4068-4076, 2015. 2, 3\n\nCaltech-ucsd birds 200. Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, Pietro Perona, 79Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010. 7, 9\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition13Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 7130-7138, 2017. 1, 2, 3\n\nLearning metrics from teachers: Compact networks for image embedding. Lu Yu, Vacit Oguz Yazici, Xialei Liu, Joost Van De Weijer, Yongmei Cheng, Arnau Ramisa, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition57Lu Yu, Vacit Oguz Yazici, Xialei Liu, Joost van de Weijer, Yongmei Cheng, and Arnau Ramisa. Learning metrics from teachers: Compact networks for image embedding. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2907-2916, 2019. 2, 3, 5, 7\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Sergey Zagoruyko, Nikos Komodakis, arXiv:1612.0392823arXiv preprintSergey Zagoruyko and Nikos Komodakis. Paying more at- tention to attention: Improving the performance of convolu- tional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016. 2, 3\n\nDeep mutual learning. Ying Zhang, Tao Xiang, Timothy M Hospedales, Huchuan Lu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYing Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4320-4328, 2018. 3\n", "annotations": {"author": "[{\"end\":175,\"start\":72},{\"end\":278,\"start\":176},{\"end\":376,\"start\":279}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":74},{\"end\":185,\"start\":178},{\"end\":286,\"start\":281}]", "author_first_name": "[{\"end\":73,\"start\":72},{\"end\":177,\"start\":176},{\"end\":280,\"start\":279}]", "author_affiliation": "[{\"end\":174,\"start\":105},{\"end\":277,\"start\":208},{\"end\":375,\"start\":306}]", "title": "[{\"end\":69,\"start\":1},{\"end\":445,\"start\":377}]", "venue": null, "abstract": "[{\"end\":1997,\"start\":447}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2096,\"start\":2092},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2670,\"start\":2667},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3311,\"start\":3307},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3315,\"start\":3311},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3319,\"start\":3315},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3323,\"start\":3319},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3327,\"start\":3323},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3331,\"start\":3327},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3335,\"start\":3331},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3339,\"start\":3335},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3343,\"start\":3339},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3347,\"start\":3343},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3351,\"start\":3347},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3355,\"start\":3351},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3359,\"start\":3355},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3363,\"start\":3359},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3367,\"start\":3363},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3371,\"start\":3367},{\"end\":3553,\"start\":3543},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3588,\"start\":3585},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3641,\"start\":3637},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4121,\"start\":4118},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4123,\"start\":4121},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4126,\"start\":4123},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4129,\"start\":4126},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4132,\"start\":4129},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4135,\"start\":4132},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4505,\"start\":4501},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4508,\"start\":4505},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4511,\"start\":4508},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5174,\"start\":5171},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5422,\"start\":5419},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5530,\"start\":5526},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5533,\"start\":5530},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6256,\"start\":6253},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7596,\"start\":7592},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7599,\"start\":7596},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7639,\"start\":7635},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9278,\"start\":9274},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10959,\"start\":10955},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10962,\"start\":10959},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11510,\"start\":11507},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11512,\"start\":11510},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11514,\"start\":11512},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11517,\"start\":11514},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11520,\"start\":11517},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11814,\"start\":11810},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11850,\"start\":11846},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11937,\"start\":11933},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11985,\"start\":11982},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11988,\"start\":11985},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12746,\"start\":12742},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12796,\"start\":12792},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12821,\"start\":12817},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12901,\"start\":12897},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13557,\"start\":13553},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14625,\"start\":14621},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14707,\"start\":14703},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16924,\"start\":16921},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19540,\"start\":19536},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19737,\"start\":19733},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20219,\"start\":20215},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20621,\"start\":20617},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20836,\"start\":20832},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21128,\"start\":21124},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21264,\"start\":21260},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22648,\"start\":22644},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22980,\"start\":22976},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22983,\"start\":22980},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26629,\"start\":26625},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27683,\"start\":27680},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30098,\"start\":30095},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30124,\"start\":30120},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30169,\"start\":30165},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30225,\"start\":30221},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30690,\"start\":30687},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33810,\"start\":33807},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34253,\"start\":34249},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34255,\"start\":34253},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37650,\"start\":37646},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37674,\"start\":37671},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":37700,\"start\":37696},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37735,\"start\":37731},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40018,\"start\":40014},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41196,\"start\":41192}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":43405,\"start\":43332},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43753,\"start\":43406},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45548,\"start\":43754},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46440,\"start\":45549},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46811,\"start\":46441},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47345,\"start\":46812},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48308,\"start\":47346},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48764,\"start\":48309},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49022,\"start\":48765}]", "paragraph": "[{\"end\":3898,\"start\":2013},{\"end\":4757,\"start\":3900},{\"end\":5945,\"start\":4759},{\"end\":7051,\"start\":5947},{\"end\":8466,\"start\":7053},{\"end\":9279,\"start\":8468},{\"end\":10446,\"start\":9281},{\"end\":11066,\"start\":10448},{\"end\":11368,\"start\":11068},{\"end\":12643,\"start\":11385},{\"end\":13703,\"start\":12645},{\"end\":15300,\"start\":13705},{\"end\":16128,\"start\":15320},{\"end\":17057,\"start\":16130},{\"end\":17367,\"start\":17180},{\"end\":17640,\"start\":17400},{\"end\":17826,\"start\":17700},{\"end\":19102,\"start\":17901},{\"end\":20220,\"start\":19188},{\"end\":20334,\"start\":20318},{\"end\":21378,\"start\":20409},{\"end\":22649,\"start\":21410},{\"end\":23044,\"start\":22774},{\"end\":24367,\"start\":23088},{\"end\":24973,\"start\":24369},{\"end\":25156,\"start\":25108},{\"end\":26630,\"start\":25158},{\"end\":26739,\"start\":26632},{\"end\":27450,\"start\":26770},{\"end\":28699,\"start\":27500},{\"end\":29196,\"start\":28701},{\"end\":29883,\"start\":29249},{\"end\":31196,\"start\":29911},{\"end\":34730,\"start\":31198},{\"end\":36884,\"start\":34732},{\"end\":37514,\"start\":36900},{\"end\":39238,\"start\":37568},{\"end\":40019,\"start\":39240},{\"end\":40322,\"start\":40050},{\"end\":40476,\"start\":40448},{\"end\":40663,\"start\":40635},{\"end\":40853,\"start\":40824},{\"end\":41513,\"start\":40975},{\"end\":43331,\"start\":41942}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17118,\"start\":17058},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17179,\"start\":17118},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17399,\"start\":17368},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17699,\"start\":17641},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17900,\"start\":17827},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20317,\"start\":20221},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20408,\"start\":20335},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21409,\"start\":21379},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22653,\"start\":22650},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22773,\"start\":22653},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26769,\"start\":26740},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29248,\"start\":29197}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30538,\"start\":30531},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":32672,\"start\":32665},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32888,\"start\":32879},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33471,\"start\":33462},{\"end\":34640,\"start\":34633},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36321,\"start\":36314},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":43130,\"start\":43123}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2011,\"start\":1999},{\"attributes\":{\"n\":\"2.\"},\"end\":11383,\"start\":11371},{\"attributes\":{\"n\":\"3.\"},\"end\":15318,\"start\":15303},{\"attributes\":{\"n\":\"3.1.\"},\"end\":19186,\"start\":19105},{\"attributes\":{\"n\":\"3.2.\"},\"end\":23086,\"start\":23047},{\"end\":25039,\"start\":24976},{\"end\":25069,\"start\":25042},{\"end\":25106,\"start\":25072},{\"attributes\":{\"n\":\"3.3.\"},\"end\":27498,\"start\":27453},{\"attributes\":{\"n\":\"4.\"},\"end\":29909,\"start\":29886},{\"attributes\":{\"n\":\"5.\"},\"end\":36898,\"start\":36887},{\"end\":37528,\"start\":37517},{\"end\":37566,\"start\":37531},{\"end\":40048,\"start\":40022},{\"end\":40360,\"start\":40325},{\"end\":40368,\"start\":40363},{\"end\":40407,\"start\":40371},{\"end\":40446,\"start\":40410},{\"end\":40508,\"start\":40479},{\"end\":40546,\"start\":40511},{\"end\":40556,\"start\":40549},{\"end\":40594,\"start\":40559},{\"end\":40633,\"start\":40597},{\"end\":40695,\"start\":40666},{\"end\":40734,\"start\":40698},{\"end\":40744,\"start\":40737},{\"end\":40783,\"start\":40747},{\"end\":40822,\"start\":40786},{\"end\":40885,\"start\":40856},{\"end\":40924,\"start\":40888},{\"end\":40934,\"start\":40927},{\"end\":40973,\"start\":40937},{\"end\":41523,\"start\":41516},{\"end\":41562,\"start\":41526},{\"end\":41594,\"start\":41565},{\"end\":41632,\"start\":41597},{\"end\":41640,\"start\":41635},{\"end\":41678,\"start\":41643},{\"end\":41709,\"start\":41681},{\"end\":41747,\"start\":41712},{\"end\":41757,\"start\":41750},{\"end\":41796,\"start\":41760},{\"end\":41828,\"start\":41799},{\"end\":41867,\"start\":41831},{\"end\":41907,\"start\":41870},{\"end\":41940,\"start\":41910},{\"end\":43417,\"start\":43407},{\"end\":45559,\"start\":45550},{\"end\":46451,\"start\":46442},{\"end\":46822,\"start\":46813},{\"end\":47356,\"start\":47347},{\"end\":48319,\"start\":48310}]", "table": "[{\"end\":45548,\"start\":44549},{\"end\":46440,\"start\":45597},{\"end\":46811,\"start\":46453},{\"end\":47345,\"start\":46874},{\"end\":48308,\"start\":47412},{\"end\":48764,\"start\":48445},{\"end\":49022,\"start\":48775}]", "figure_caption": "[{\"end\":43405,\"start\":43334},{\"end\":43753,\"start\":43419},{\"end\":44549,\"start\":43756},{\"end\":45597,\"start\":45561},{\"end\":46874,\"start\":46824},{\"end\":47412,\"start\":47358},{\"end\":48445,\"start\":48321},{\"end\":48775,\"start\":48767}]", "figure_ref": "[{\"end\":2465,\"start\":2457},{\"end\":6324,\"start\":6318},{\"end\":8194,\"start\":8188},{\"end\":8854,\"start\":8848},{\"end\":13324,\"start\":13318},{\"end\":13653,\"start\":13647},{\"end\":18420,\"start\":18414},{\"end\":25190,\"start\":25182},{\"end\":25487,\"start\":25481},{\"end\":28486,\"start\":28480},{\"end\":34803,\"start\":34797},{\"end\":35405,\"start\":35399},{\"end\":35487,\"start\":35481},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":40130,\"start\":40124},{\"end\":41003,\"start\":40992},{\"end\":42288,\"start\":42282}]", "bib_author_first_name": "[{\"end\":49324,\"start\":49314},{\"end\":49340,\"start\":49334},{\"end\":49356,\"start\":49349},{\"end\":49669,\"start\":49662},{\"end\":49680,\"start\":49675},{\"end\":49683,\"start\":49681},{\"end\":49695,\"start\":49688},{\"end\":49707,\"start\":49706},{\"end\":49721,\"start\":49714},{\"end\":50209,\"start\":50204},{\"end\":50223,\"start\":50216},{\"end\":50242,\"start\":50233},{\"end\":50257,\"start\":50251},{\"end\":50273,\"start\":50267},{\"end\":50275,\"start\":50274},{\"end\":50290,\"start\":50282},{\"end\":50292,\"start\":50291},{\"end\":50582,\"start\":50574},{\"end\":50595,\"start\":50591},{\"end\":50614,\"start\":50605},{\"end\":51105,\"start\":51097},{\"end\":51114,\"start\":51112},{\"end\":51130,\"start\":51122},{\"end\":51140,\"start\":51136},{\"end\":51480,\"start\":51476},{\"end\":51495,\"start\":51489},{\"end\":51507,\"start\":51500},{\"end\":51914,\"start\":51913},{\"end\":51923,\"start\":51922},{\"end\":52358,\"start\":52354},{\"end\":52373,\"start\":52368},{\"end\":52386,\"start\":52382},{\"end\":52815,\"start\":52807},{\"end\":52829,\"start\":52824},{\"end\":52843,\"start\":52839},{\"end\":53264,\"start\":53258},{\"end\":53631,\"start\":53627},{\"end\":53652,\"start\":53644},{\"end\":53820,\"start\":53816},{\"end\":53834,\"start\":53828},{\"end\":53851,\"start\":53843},{\"end\":54051,\"start\":54048},{\"end\":54054,\"start\":54052},{\"end\":54082,\"start\":54071},{\"end\":54384,\"start\":54377},{\"end\":54409,\"start\":54401},{\"end\":54700,\"start\":54693},{\"end\":54725,\"start\":54722},{\"end\":54744,\"start\":54738},{\"end\":55095,\"start\":55087},{\"end\":55116,\"start\":55106},{\"end\":55510,\"start\":55502},{\"end\":55531,\"start\":55521},{\"end\":55877,\"start\":55868},{\"end\":55894,\"start\":55889},{\"end\":56316,\"start\":56309},{\"end\":56332,\"start\":56325},{\"end\":56347,\"start\":56341},{\"end\":56356,\"start\":56348},{\"end\":56371,\"start\":56364},{\"end\":56387,\"start\":56382},{\"end\":56401,\"start\":56395},{\"end\":56833,\"start\":56827},{\"end\":56854,\"start\":56848},{\"end\":56867,\"start\":56863},{\"end\":56882,\"start\":56877},{\"end\":56897,\"start\":56891},{\"end\":56917,\"start\":56910},{\"end\":56924,\"start\":56918},{\"end\":56938,\"start\":56933},{\"end\":56945,\"start\":56939},{\"end\":57412,\"start\":57407},{\"end\":57426,\"start\":57422},{\"end\":57436,\"start\":57429},{\"end\":57627,\"start\":57620},{\"end\":57638,\"start\":57634},{\"end\":57651,\"start\":57645},{\"end\":57664,\"start\":57657},{\"end\":57928,\"start\":57921},{\"end\":57939,\"start\":57935},{\"end\":57953,\"start\":57946},{\"end\":58441,\"start\":58437},{\"end\":58701,\"start\":58697},{\"end\":58713,\"start\":58709},{\"end\":58729,\"start\":58723},{\"end\":58743,\"start\":58739},{\"end\":59119,\"start\":59114},{\"end\":59135,\"start\":59130},{\"end\":59152,\"start\":59145},{\"end\":59168,\"start\":59159},{\"end\":59181,\"start\":59174},{\"end\":59196,\"start\":59191},{\"end\":59213,\"start\":59207},{\"end\":59476,\"start\":59471},{\"end\":59489,\"start\":59482},{\"end\":59501,\"start\":59495},{\"end\":59512,\"start\":59507},{\"end\":59999,\"start\":59997},{\"end\":60009,\"start\":60004},{\"end\":60029,\"start\":60023},{\"end\":60040,\"start\":60035},{\"end\":60063,\"start\":60056},{\"end\":60076,\"start\":60071},{\"end\":60634,\"start\":60628},{\"end\":60651,\"start\":60646},{\"end\":60927,\"start\":60923},{\"end\":60938,\"start\":60935},{\"end\":60953,\"start\":60946},{\"end\":60955,\"start\":60954},{\"end\":60975,\"start\":60968}]", "bib_author_last_name": "[{\"end\":49332,\"start\":49325},{\"end\":49347,\"start\":49341},{\"end\":49363,\"start\":49357},{\"end\":49673,\"start\":49670},{\"end\":49686,\"start\":49684},{\"end\":49704,\"start\":49696},{\"end\":49712,\"start\":49708},{\"end\":49730,\"start\":49722},{\"end\":49735,\"start\":49732},{\"end\":50214,\"start\":50210},{\"end\":50231,\"start\":50224},{\"end\":50249,\"start\":50243},{\"end\":50265,\"start\":50258},{\"end\":50280,\"start\":50276},{\"end\":50299,\"start\":50293},{\"end\":50589,\"start\":50583},{\"end\":50603,\"start\":50596},{\"end\":50630,\"start\":50615},{\"end\":51110,\"start\":51106},{\"end\":51120,\"start\":51115},{\"end\":51134,\"start\":51131},{\"end\":51144,\"start\":51141},{\"end\":51487,\"start\":51481},{\"end\":51498,\"start\":51496},{\"end\":51511,\"start\":51508},{\"end\":51920,\"start\":51915},{\"end\":51930,\"start\":51924},{\"end\":52366,\"start\":52359},{\"end\":52380,\"start\":52374},{\"end\":52392,\"start\":52387},{\"end\":52822,\"start\":52816},{\"end\":52837,\"start\":52830},{\"end\":52848,\"start\":52844},{\"end\":53273,\"start\":53265},{\"end\":53642,\"start\":53632},{\"end\":53659,\"start\":53653},{\"end\":53826,\"start\":53821},{\"end\":53841,\"start\":53835},{\"end\":53858,\"start\":53852},{\"end\":54069,\"start\":54055},{\"end\":54086,\"start\":54083},{\"end\":54092,\"start\":54088},{\"end\":54399,\"start\":54385},{\"end\":54416,\"start\":54410},{\"end\":54720,\"start\":54701},{\"end\":54736,\"start\":54726},{\"end\":54747,\"start\":54745},{\"end\":54760,\"start\":54749},{\"end\":55104,\"start\":55096},{\"end\":55122,\"start\":55117},{\"end\":55519,\"start\":55511},{\"end\":55537,\"start\":55532},{\"end\":55887,\"start\":55878},{\"end\":55899,\"start\":55895},{\"end\":56323,\"start\":56317},{\"end\":56339,\"start\":56333},{\"end\":56362,\"start\":56357},{\"end\":56380,\"start\":56372},{\"end\":56393,\"start\":56388},{\"end\":56408,\"start\":56402},{\"end\":56846,\"start\":56834},{\"end\":56861,\"start\":56855},{\"end\":56875,\"start\":56868},{\"end\":56889,\"start\":56883},{\"end\":56908,\"start\":56898},{\"end\":56931,\"start\":56925},{\"end\":56949,\"start\":56946},{\"end\":57420,\"start\":57413},{\"end\":57443,\"start\":57437},{\"end\":57632,\"start\":57628},{\"end\":57643,\"start\":57639},{\"end\":57655,\"start\":57652},{\"end\":57670,\"start\":57665},{\"end\":57933,\"start\":57929},{\"end\":57944,\"start\":57940},{\"end\":57959,\"start\":57954},{\"end\":58450,\"start\":58442},{\"end\":58707,\"start\":58702},{\"end\":58721,\"start\":58714},{\"end\":58737,\"start\":58730},{\"end\":58750,\"start\":58744},{\"end\":59128,\"start\":59120},{\"end\":59143,\"start\":59136},{\"end\":59157,\"start\":59153},{\"end\":59172,\"start\":59169},{\"end\":59189,\"start\":59182},{\"end\":59205,\"start\":59197},{\"end\":59220,\"start\":59214},{\"end\":59480,\"start\":59477},{\"end\":59493,\"start\":59490},{\"end\":59505,\"start\":59502},{\"end\":59516,\"start\":59513},{\"end\":60002,\"start\":60000},{\"end\":60021,\"start\":60010},{\"end\":60033,\"start\":60030},{\"end\":60054,\"start\":60041},{\"end\":60069,\"start\":60064},{\"end\":60083,\"start\":60077},{\"end\":60644,\"start\":60635},{\"end\":60661,\"start\":60652},{\"end\":60933,\"start\":60928},{\"end\":60944,\"start\":60939},{\"end\":60966,\"start\":60956},{\"end\":60978,\"start\":60976}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1711.08856\",\"id\":\"b0\"},\"end\":49599,\"start\":49314},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":118649278},\"end\":50125,\"start\":49601},{\"attributes\":{\"doi\":\"arXiv:1804.03235\",\"id\":\"b2\"},\"end\":50553,\"start\":50127},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11253972},\"end\":51012,\"start\":50555},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52927917},\"end\":51403,\"start\":51014},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":308212},\"end\":51857,\"start\":51405},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206590483},\"end\":52293,\"start\":51859},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8281592},\"end\":52759,\"start\":52295},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7200347},\"end\":53188,\"start\":52761},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":19490929},\"end\":53570,\"start\":53190},{\"attributes\":{\"id\":\"b10\"},\"end\":53799,\"start\":53572},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1779661},\"end\":53972,\"start\":53801},{\"attributes\":{\"id\":\"b12\"},\"end\":54345,\"start\":53974},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5855042},\"end\":54590,\"start\":54347},{\"attributes\":{\"id\":\"b14\"},\"end\":55016,\"start\":54592},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52012952},\"end\":55439,\"start\":55018},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":51721116},\"end\":55783,\"start\":55441},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8724974},\"end\":56272,\"start\":55785},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2723173},\"end\":56770,\"start\":56274},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49584497},\"end\":57342,\"start\":56772},{\"attributes\":{\"doi\":\"arXiv:1703.00810\",\"id\":\"b20\"},\"end\":57616,\"start\":57344},{\"attributes\":{\"id\":\"b21\"},\"end\":57855,\"start\":57618},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14006976},\"end\":58365,\"start\":57857},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3181596},\"end\":58642,\"start\":58367},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2655115},\"end\":59088,\"start\":58644},{\"attributes\":{\"id\":\"b25\"},\"end\":59370,\"start\":59090},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206596723},\"end\":59925,\"start\":59372},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":102351826},\"end\":60507,\"start\":59927},{\"attributes\":{\"doi\":\"arXiv:1612.03928\",\"id\":\"b28\"},\"end\":60899,\"start\":60509},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":26071966},\"end\":61312,\"start\":60901}]", "bib_title": "[{\"end\":49660,\"start\":49601},{\"end\":50572,\"start\":50555},{\"end\":51095,\"start\":51014},{\"end\":51474,\"start\":51405},{\"end\":51911,\"start\":51859},{\"end\":52352,\"start\":52295},{\"end\":52805,\"start\":52761},{\"end\":53256,\"start\":53190},{\"end\":53814,\"start\":53801},{\"end\":54046,\"start\":53974},{\"end\":54375,\"start\":54347},{\"end\":55085,\"start\":55018},{\"end\":55500,\"start\":55441},{\"end\":55866,\"start\":55785},{\"end\":56307,\"start\":56274},{\"end\":56825,\"start\":56772},{\"end\":57919,\"start\":57857},{\"end\":58435,\"start\":58367},{\"end\":58695,\"start\":58644},{\"end\":59469,\"start\":59372},{\"end\":59995,\"start\":59927},{\"end\":60921,\"start\":60901}]", "bib_author": "[{\"end\":49334,\"start\":49314},{\"end\":49349,\"start\":49334},{\"end\":49365,\"start\":49349},{\"end\":49675,\"start\":49662},{\"end\":49688,\"start\":49675},{\"end\":49706,\"start\":49688},{\"end\":49714,\"start\":49706},{\"end\":49732,\"start\":49714},{\"end\":49737,\"start\":49732},{\"end\":50216,\"start\":50204},{\"end\":50233,\"start\":50216},{\"end\":50251,\"start\":50233},{\"end\":50267,\"start\":50251},{\"end\":50282,\"start\":50267},{\"end\":50301,\"start\":50282},{\"end\":50591,\"start\":50574},{\"end\":50605,\"start\":50591},{\"end\":50632,\"start\":50605},{\"end\":51112,\"start\":51097},{\"end\":51122,\"start\":51112},{\"end\":51136,\"start\":51122},{\"end\":51146,\"start\":51136},{\"end\":51489,\"start\":51476},{\"end\":51500,\"start\":51489},{\"end\":51513,\"start\":51500},{\"end\":51922,\"start\":51913},{\"end\":51932,\"start\":51922},{\"end\":52368,\"start\":52354},{\"end\":52382,\"start\":52368},{\"end\":52394,\"start\":52382},{\"end\":52824,\"start\":52807},{\"end\":52839,\"start\":52824},{\"end\":52850,\"start\":52839},{\"end\":53275,\"start\":53258},{\"end\":53644,\"start\":53627},{\"end\":53661,\"start\":53644},{\"end\":53828,\"start\":53816},{\"end\":53843,\"start\":53828},{\"end\":53860,\"start\":53843},{\"end\":54071,\"start\":54048},{\"end\":54088,\"start\":54071},{\"end\":54094,\"start\":54088},{\"end\":54401,\"start\":54377},{\"end\":54418,\"start\":54401},{\"end\":54722,\"start\":54693},{\"end\":54738,\"start\":54722},{\"end\":54749,\"start\":54738},{\"end\":54762,\"start\":54749},{\"end\":55106,\"start\":55087},{\"end\":55124,\"start\":55106},{\"end\":55521,\"start\":55502},{\"end\":55539,\"start\":55521},{\"end\":55889,\"start\":55868},{\"end\":55901,\"start\":55889},{\"end\":56325,\"start\":56309},{\"end\":56341,\"start\":56325},{\"end\":56364,\"start\":56341},{\"end\":56382,\"start\":56364},{\"end\":56395,\"start\":56382},{\"end\":56410,\"start\":56395},{\"end\":56848,\"start\":56827},{\"end\":56863,\"start\":56848},{\"end\":56877,\"start\":56863},{\"end\":56891,\"start\":56877},{\"end\":56910,\"start\":56891},{\"end\":56933,\"start\":56910},{\"end\":56951,\"start\":56933},{\"end\":57422,\"start\":57407},{\"end\":57429,\"start\":57422},{\"end\":57445,\"start\":57429},{\"end\":57634,\"start\":57620},{\"end\":57645,\"start\":57634},{\"end\":57657,\"start\":57645},{\"end\":57672,\"start\":57657},{\"end\":57935,\"start\":57921},{\"end\":57946,\"start\":57935},{\"end\":57961,\"start\":57946},{\"end\":58452,\"start\":58437},{\"end\":58709,\"start\":58697},{\"end\":58723,\"start\":58709},{\"end\":58739,\"start\":58723},{\"end\":58752,\"start\":58739},{\"end\":59130,\"start\":59114},{\"end\":59145,\"start\":59130},{\"end\":59159,\"start\":59145},{\"end\":59174,\"start\":59159},{\"end\":59191,\"start\":59174},{\"end\":59207,\"start\":59191},{\"end\":59222,\"start\":59207},{\"end\":59482,\"start\":59471},{\"end\":59495,\"start\":59482},{\"end\":59507,\"start\":59495},{\"end\":59518,\"start\":59507},{\"end\":60004,\"start\":59997},{\"end\":60023,\"start\":60004},{\"end\":60035,\"start\":60023},{\"end\":60056,\"start\":60035},{\"end\":60071,\"start\":60056},{\"end\":60085,\"start\":60071},{\"end\":60646,\"start\":60628},{\"end\":60663,\"start\":60646},{\"end\":60935,\"start\":60923},{\"end\":60946,\"start\":60935},{\"end\":60968,\"start\":60946},{\"end\":60980,\"start\":60968}]", "bib_venue": "[{\"end\":49430,\"start\":49381},{\"end\":49814,\"start\":49737},{\"end\":50202,\"start\":50127},{\"end\":50725,\"start\":50632},{\"end\":51189,\"start\":51146},{\"end\":51584,\"start\":51513},{\"end\":52021,\"start\":51932},{\"end\":52471,\"start\":52394},{\"end\":52929,\"start\":52850},{\"end\":53363,\"start\":53275},{\"end\":53625,\"start\":53572},{\"end\":53866,\"start\":53860},{\"end\":54132,\"start\":54094},{\"end\":54454,\"start\":54418},{\"end\":54691,\"start\":54592},{\"end\":55181,\"start\":55124},{\"end\":55596,\"start\":55539},{\"end\":55978,\"start\":55901},{\"end\":56481,\"start\":56410},{\"end\":57022,\"start\":56951},{\"end\":57405,\"start\":57344},{\"end\":58052,\"start\":57961},{\"end\":58488,\"start\":58452},{\"end\":58819,\"start\":58752},{\"end\":59112,\"start\":59090},{\"end\":59595,\"start\":59518},{\"end\":60162,\"start\":60085},{\"end\":60626,\"start\":60509},{\"end\":61057,\"start\":60980},{\"end\":49878,\"start\":49816},{\"end\":50805,\"start\":50727},{\"end\":51642,\"start\":51586},{\"end\":52097,\"start\":52023},{\"end\":52535,\"start\":52473},{\"end\":52995,\"start\":52931},{\"end\":55225,\"start\":55183},{\"end\":56042,\"start\":55980},{\"end\":56539,\"start\":56483},{\"end\":57080,\"start\":57024},{\"end\":58130,\"start\":58054},{\"end\":58873,\"start\":58821},{\"end\":59659,\"start\":59597},{\"end\":60226,\"start\":60164},{\"end\":61121,\"start\":61059}]"}}}, "year": 2023, "month": 12, "day": 17}