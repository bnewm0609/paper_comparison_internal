{"id": 118649278, "updated": "2023-10-01 20:27:22.785", "metadata": {"title": "Variational Information Distillation for Knowledge Transfer", "authors": "[{\"first\":\"Sungsoo\",\"last\":\"Ahn\",\"middle\":[]},{\"first\":\"Shell\",\"last\":\"Hu\",\"middle\":[\"Xu\"]},{\"first\":\"Andreas\",\"last\":\"Damianou\",\"middle\":[]},{\"first\":\"Neil\",\"last\":\"Lawrence\",\"middle\":[\"D.\"]},{\"first\":\"Zhenwen\",\"last\":\"Dai\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 4, "day": 11}, "abstract": "Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding hand-crafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.05835", "mag": "2951513317", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/AhnHDLD19", "doi": "10.1109/cvpr.2019.00938"}}, "content": {"source": {"pdf_hash": "df441a4a1e30b4c5c60a890556bc34e9981bf2ef", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.05835v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.05835", "status": "GREEN"}}, "grobid": {"id": "4c9d1c0fbc97da93e5d80593910932d134c96034", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/df441a4a1e30b4c5c60a890556bc34e9981bf2ef.txt", "contents": "\nVariational Information Distillation for Knowledge Transfer\n\n\nSungsoo Ahn sungsoo.ahn@kaist.ac.kr \nKorea Advanced Institute of Science and Technology Daejeon\ncole des Ponts ParisTechChamps-sur-MarneKorea, France, United Kingdom, United Kingdom, United Kingdom\n\nShell Xu Hu \nKorea Advanced Institute of Science and Technology Daejeon\ncole des Ponts ParisTechChamps-sur-MarneKorea, France, United Kingdom, United Kingdom, United Kingdom\n\nAndreas Damianou damianou@amazon.com \nKorea Advanced Institute of Science and Technology Daejeon\ncole des Ponts ParisTechChamps-sur-MarneKorea, France, United Kingdom, United Kingdom, United Kingdom\n\nNeil D Lawrence lawrennd@amazon.com \nKorea Advanced Institute of Science and Technology Daejeon\ncole des Ponts ParisTechChamps-sur-MarneKorea, France, United Kingdom, United Kingdom, United Kingdom\n\nZhenwen Dai zhenwend@amazon.com \nKorea Advanced Institute of Science and Technology Daejeon\ncole des Ponts ParisTechChamps-sur-MarneKorea, France, United Kingdom, United Kingdom, United Kingdom\n\nVariational Information Distillation for Knowledge Transfer\n\nTransferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding handcrafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.\n\nIntroduction\n\nDeep neural networks (DNNs) play important roles in various computer vision tasks, e.g., depth estimation [8], pose estimation [26], optical flow [7], object classification [11], detection [10], and segmentation [25]. A typical DNN approach for a computer vision task is to train a sophisticated end-to-end neural network with a large amount of labeled data. Such an approach often delivers state-of-theart performance if a sufficient amount of data is available. * Contributed during an internship at Amazon. Figure 1: Conceptual diagram of the proposed knowledge transfer method. The student network efficiently learns the target task by minimizing the cross-entropy (CE) loss while retaining high mutual information (MI) with the teacher network. The mutual information is maximized by learning to estimate the distribution of the activations in the teacher network, provoking the transfer of knowledge.\n\nHowever, in many cases, it is impossible to gather sufficiently large data to train a DNN. For example, in many medical image applications [24], the amount of available data is constrained by the number of patients of a particular disease.\n\nA popular approach for handling such lack of data is transfer learning [19], where the goal is to transfer knowledge from the source task to facilitate learning on the target task. Typically, one considers the source task to be generic with a larger amount of available data that contains useful knowledge for learning the target task, e.g., knowledge from natural image classification [23] is likely to be useful for fine-grained bird classification [29]. Hinton et al. [12] proposed the teacher-student framework for transferring such knowledge between DNNs being trained on the source and target tasks respectively. The high-level idea is to introduce an additional regularization for the DNN being trained on the target task, i.e., the student network, which allows learning the knowledge existing in the DNN that was pre-trained on the source task, i.e., the teacher network. While the framework was originally designed for knowledge transfer between DNNs on the same dataset, recent works [30,31] started exploiting its potential for more general transfer learning tasks, i.e., when the source data and the target data are different.\n\nMany knowledge transfer methods have been proposed with various intuitions. Hinton et al. [12] and Ba and Caruana [2] propose to match the final layers of the teacher and the student network, as the outputs from the final layer of the teacher network provide more information than raw labels. Romero et al. [22] proposes to match intermediate layers of the student network to the corresponding layers of the teacher network. Recent works [3,6,13,30,31] relax the regularization of matching the entire layer by matching carefully designed features/statistics extracted from intermediate layers of the teacher and the student networks, e.g., attention maps [31] and maximum mean discrepancy [13].\n\nEvidently, there is no commonly agreed theory behind knowledge transfer. This causes difficulty in understanding empirical results and in developing new methods in a more principled way. In this paper, we propose variational information distillation (VID) as an attempt towards this direction in which we formulate the knowledge transfer as maximization of the mutual information between the teacher and the student networks. This framework proposes an actionable objective for knowledge transfer and allows us to quantify the amount of information that is transferred from a teacher network to a student network. Since the mutual information is computationally intractable, we employ a variational information maximization [1] scheme to maximize the variational lower bound instead. See Figure 1 for the conceptual diagram of the proposed knowledge transfer method. We further show that several existing knowledge transfer methods [16,22] can be derived as specific implementations of our framework by choosing different forms of the variational lower bound. We empirically validate the VID framework, which significantly outperforms existing methods. We observe the gap is especially large in the cases of small data and heterogeneous architectures.\n\nIn summary, the overall contributions of our paper are as follows:\n\n\u2022 We propose variational information distillation, a prin-cipled knowledge transfer framework through maximizing mutual information between two networks based on the variational information maximization technique.\n\n\u2022 We demonstrate that VID generalizes several existing knowledge transfer methods. In addition, our implementation of the framework empirically outperforms state-of-the-art knowledge transfer methods on various knowledge transfer experiments, including knowledge transfer between (heterogeneous) DNNs on the same dataset or on different datasets.\n\n\u2022 Finally, we demonstrate that heterogeneous knowledge transfer between a convolutional neural networks (CNN) and a multilayer perceptrons (MLP) is possible on CIFAR-10. Our method yields a student MLP that significantly outperforms the best-reported MLPs [17,27] in the literature.\n\n\nVariational information distillation (VID)\n\nIn this section, we describe VID as a general framework for knowledge transfer in the teacher-student framework. Specifically, consider training a student neural network on a target task, given another teacher neural network pre-trained on a similar (or related) source task. Note that the source task and the target task could be the same, e.g., for model compression or knowledge distillation. The underlying assumption is that the layers in the teacher network have been trained to represent certain attributes of given inputs that exist in both the source task and the target task. For a successful knowledge transfer, the student network must learn how to incorporate the knowledge of such attributes to its own learning.\n\nFrom a perspective of information theory, knowledge transfer can be expressed as retaining high mutual information between the layers of the teacher and the student networks. More specifically, consider an input random variable x drawn from the target data distribution p(x) and K pairs of layers R = {(T (k) , S (k) )} K k=1 , where each pair (T (k) , S (k) ) is selected from the teacher network and the student network respectively. Feedforwarding the input x through the networks induces K pairs of random variables {(t (k) , s (k) )} K k=1 which indicate activations of the selected layers, e.g., t (k) = T (k) (x). The mutual information between the pair of random variables (t, s) is defined by:\nI(t; s) = H(t) \u2212 H(t|s) = \u2212E t [log p(t)] + E t,s [log p(t|s)],(1)\nwhere the entropy H(t) and the conditional entropy H(t|s) are derived from the joint distribution p(t, s). Empirically, the joint distribution p(t, s) is a result of aggregation over the layers with input x sampled from the input distribution p(x). Intuitively, the definition of I(t; s) can be understood as a reduction in uncertainty in the knowledge of the teacher encoded in its layer t when the the student layer s is known. We now define the following loss function which aims to learn a student network for the target task while encouraging high mutual information with the teacher network:\nL = L S \u2212 K k=1 \u03bb k I(t (k) , s (k) ),(2)\nwhere L S is the task-specific loss function for the target task and \u03bb k > 0 is a hyper-parameter introduced for regularization of the mutual information in each layer. Equation (2) needs to be minimized with respect to the parameters of the student network. However, the minimization is hard since exact computation of the mutual information is intractable. We instead propose a variational lower bound for each mutual information term I(t; s), in which we define a variational distribution q(t|s) that approximates p(t|s):\nI(t; s) = H(t) \u2212 H(t|s) = H(t) + E t,s [log p(t|s)] = H(t) + E t,s [log q(t|s)] + E s [D KL (p(t|s)||q(t|s))] \u2265 H(t) + E t,s [log q(t|s)],(3)\nwhere the expectations are over the distribution p(t, s) and the last inequality is due to the non-negativity of the Kullback-Leiber divergence D KL (\u00b7). This technique is known as the variational information maximization [1]. Finally, we obtain VID by applying the variational information maximization to each mutual information term I(t (k) , s (k) ) in (2), leading to a minimization of the following loss function:\nL = L S \u2212 K k=1 \u03bb k E t (k) ,s (k) [log q(t (k) |s (k) )].(4)\nThe objective L is jointly minimized over the parameters of the student network and the variational distribution q(t|s). Note that the entropy term H(t) has been removed from the equation (3) since it is constant with respect to the parameters to be optimized. Alternatively, one could interpret the objective (4) as jointly training the student network for the target task and maximization of the conditional likelihood to fit the activations of the selected layers from the teacher network. By doing so, the student network obtains the \"compressed\" knowledge required for recovering activations of the selected layers in the teacher network.\n\n\nAlgorithm formulation\n\nWe further specify our framework by choosing a form made for the variational distribution q(t|s). In general, we employ a Gaussian distribution with heteroscedastic mean \u00b5(\u00b7) and homoscedastic variance \u03c3 as the variational distribution q(t|s), i.e., the mean \u00b5(\u00b7) is a function of s and the standard deviation \u03c3 is not. Next, the parameterization of \u00b5(\u00b7) and \u03c3 is further specified by the type of layer corresponding to t. When t corresponds to intermediate layer of the teacher network with spatial dimensions indicating channel, height and width respectively, i.e., t \u2208 R C\u00d7H\u00d7W , our choice of variational distribution is expressed as follows:\n\u2212 log q(t|s) = \u2212 C c=1 H h=1 W w=1 log q(t c,h,w |s) (5) = C c=1 H h=1 W w=1 log \u03c3 c + (t c,h,w \u2212 \u00b5 c,h,w (s)) 2 2\u03c3 2 c + constant,\nwhere t c,h,w denote scalar components of t indexed by (c, h, w). Further, \u00b5 c,h,w represents the output of a single unit from the neural network \u00b5(\u00b7) consisting of convolutional layers and the variance is ensured to be positive using the softplus function, i.e., \u03c3 2 c = log(1 + exp(\u03b1 c )) + where \u03b1 c \u2208 R being the parameter to be optimized and > 0 is minimum variance introduced for numerical stability. Typically, one can choose s from the student network with similar hierarchy and spatial dimension as t. When spatial dimension of two layers are equal, 1 \u00d7 1 convolutional layers are typically used for efficient parameterization of \u00b5(\u00b7). Otherwise, convolution or transposed convolution with larger kernel size could be used to match the spatial dimensions.\n\nWe additionally consider the case when the layer t = T (logit) (x) \u2208 R N corresponds to the logit layer of the teacher network. Here, our choice of the variational distribution is expressed as follows:\n\u2212 log q(t|s) = \u2212 N n=1 log q(t n |s) (6) = N n=1 log \u03c3 n + (t n \u2212 \u00b5 n (s)) 2 2\u03c3 2 n + constant,\nwhere t n indicates the n-th entry of the vector t, \u00b5 n represents the output of a single unit of neural network \u00b5(\u00b7) and \u03c3 n is, again, parameterized by softplus function to enforce positivity. For this case, the corresponding layer s in the student network is the penultimate layer S (pen) instead of the logit layer to match the hierarchy of two layers without being too restrictive on the output of the student network. Furthermore, we found that using a simple linear transformation for the parameterization of the mean function was sufficient in practice, i.e., \u00b5(s) = Ws for some weight matrix W.\n\nThe aforementioned implementations turned out to perform satisfactorily during the experiments. We also consid-  Figure 2: Plots for the heat maps corresponding to the variational distribution evaluated for spatial dimensions of the intermediate layer in the teacher network, i.e., log q(t h,w |s) = c log q(t c,h,w |s). Each figure corresponds to (a) original input image, (b, c, d) log-likelihood log q(t h,w |s) that was normalized and interpolated to fit the spatial dimension of the input image (red pixels correspond to high probability), (d) log-likelihood of variational distribution optimized for the student network trained without any knowledge transfer applied and (f) magnitude of the layer t averaged for each spatial dimensions. ered using heteroscedastic variance \u03c3(\u00b7), but it gave unstable training with ignorable improvements. Other types of parameterizations such as a heavy-tailed distribution or the mixture density network [5] could be used to gain additional performance. We leave these ideas for future exploration. See Figure 2 for an illustration of the training VID using the implementation based on equation (5). Here, we display the change in the evaluated log-likelihood of the variational distribution aggregated over channels, i.e., log q(t h,w |s) = c log q(t c,h,w |s), given input x (Figure 2a) throughout the VID training process. One observes that the student network is trained gradually for the variational distribution to estimate the density of the intermediate layer from the teacher network (Figure 2b, 2c and 2d). As a comparison, we also optimize the variational distribution for the student network trained without knowledge transfer, (Figure 2e). For this case, we observe that this particular instance of the variational distribution fails to obtain high log-likelihoods, indicating low mutual information between the teacher and the student networks. Interestingly, the parts that correspond to the background achieve higher magnitudes compared to that of the foreground in general. Our explanation is that the output of layers corresponding to the background that mostly corresponds to zero activations ( Figure 2f) and contains less information, being a relatively easier target for maximizing the log-likelihood of the variational distribution.\n\n\nConnections to existing works\n\nThe infomax principle. We first describe the relationship between our framework and the celebrated infomax principle [18] applied to representation learning [28], stating that \"good representation\" is likely to contain much information in the corresponding input. Especially, such a principle has been successfully applied to semi-supervised learning for neural networks by maximizing the mutual information between the input and output of the intermediate layer as a regularization to learning the target task, e.g., learning to reconstruct input based on autoencoders [21]. Our framework can be viewed similarly as an instance of semi-supervised learning with modification of the infomax principle: layers of the teacher network contain important information for the target task, and a good representation of the student network is likely to retain much of their information. One recovers the traditional semi-supervised learning infomax principle when we set t (k) = x in the equation (2).\n\nGeneralizing mean squared error matching. Next, we explain how existing knowledge transfer methods based on mean squared error matching can be seen as a specific instance of the proposed framework. In general, the methods will be induced from the equation (4) by making a specific choice of the layers R = {(T (k) , S (k) )} K k=1 for knowledge transfer and parameterization of heteroscedastic mean \u00b5(\u00b7) in the variational distribution:\n\u2212 log q(t|s) = N n=1 (t n \u2212 \u00b5 n (s)) 2 2 + constant.(7)\nNote that Equation (7) corresponds to a Gaussian distribution with unit variance over every dimension of the layer in the teacher network. Ba and Caruana [2] showed that knowledge can be transferred between the teacher and the student networks that were designed for the same task, by matching the output of logit layers T (logit) , S (logit) from the teacher and the student networks with respect to mean squared error. Such a formulation is induced from the equation (7) by letting R = {(T (logit) , S (logit) )}, and \u00b5(s) = s in the equation (7). This was later extended for knowledge transfer between the teacher and the student networks designed for different tasks by Li and Hoiem [16], through adding an additional linear layer on top of the penultimate layer S (pen) in the student network to matching with logit layer T (logit) in the teacher network. This is induced similarly from the equation (7) by letting R = {(T (logit) , S (pen) )}, and \u00b5(\u00b7) being a linear transformation, i.e., \u00b5(s) = Ws. Next, Romero et al. [22] proposed a knowledge transfer loss for minimizing the mean squared error between intermediate layers from the teacher and the student networks, with additional convolutional layer introduced for adapting different dimension size between each pair of matched layers. This is recovered from the regularization term in the equation (7) by choosing layers for the knowledge transfer to be intermediate layers of the teacher and the student networks, and \u00b5(\u00b7) being a linear transformation corresponding to a single 1 \u00d7 1 convolutional layer. These methods are all similar to our implementation of the framework in that they all use Gaussian distribution as the variational distribution. However, our method differs in two key ways: (a) allowing the use of a more flexible nonlinear functions for heteroscedastic mean and (b) modeling different variances for each dimension in the variational distribution. This allows transferring mutual information in a more flexible manner without wasting model capacity. Especially, modeling unit variance for all dimensions of the layer t in the teacher network could be highly restrictive for the student network. To illustrate, the layer of the teacher network might include an activation t n that contains information irrelevant to the task of the student network, yet requires much capacity for regression of \u00b5 n (s) to t n . This would raise over-regularization issues, i.e., wasting the majority of the student network's capacity on trying to fit such a unit. Instead, modeling high homoscedastic variance \u03c3 n for such dimension make its contribution ignorable to the overall loss, allowing one to \"filter\" out such unit in an efficient way.\n\nComparison with feature matching. Besides the knowledge transfer methods based on mean squared error matching, several works [6,13,30,31] have proposed indirectly matching the handcrafted features extracted from intermediate layers. More specifically, Zagoruyko and Komodakis [31] proposed matching the \"attention maps\" generated from activations from the layers. Huang and Wang [13] later generalized the attention map to matching the maximum mean discrepancy of the activations. Yim et al. [30] proposed matching the feature called the Flow of Solution Procedure (FSP) defined by the Gram matrix of layers adjacent in the same network. Chen et al. [6] considered matching the reconstructed input image from the intermediate layers of the teacher and the student networks. These methods could be seen as smartly avoiding the aforementioned over-regularization issue by filtering out information in the teacher network using expert knowledge. However, such methods potentially lead to suboptimal results when the feature extraction method is not apt for the particular knowledge transfer task and may discard important information from the layer of the teacher network in an irreversible way.\n\n\nExperiments\n\nWe demonstrate the performance of the proposed knowledge transfer framework by comparing VID to state-ofthe-art knowledge transfer methods on image classification. We apply VID to two different locations: (a) VID between intermediate layers of the teacher and the student network (VID-I) and (b) VID between the logit layer of the teacher network and the penultimate layer of the student network (VID-LP). For comparison, we consider the following knowledge transfer methods: the original knowledge distillation (KD) [12], learning without forgetting (LwF) [16], hint based transfer (FitNet) [31], activationbased attention transfer (AT) [31] and polynomial kernelbased neural selectivity transfer (NST) [13]. Note that we consider FitNet as a regularization for training the student network [31] instead of a stage-wise training procedure as first proposed in [22]. We compare knowledge transfer methods for knowledge transfer between same and different datasets, which is commonly referred to as the knowledge distillation and transfer learning tasks respectively.\n\nIn all the experiments, we select the same pairs of intermediate layers for knowledge transfer based on VID-I, FitNet, AT and NST. Similarly, the same pairs of layers for knowledge transfer are used for LwF and VID-LP. All the hyper-parameters of all the methods are chosen according to the performance on a validation set, which is 20% of \n\n\nKnowledge distillation\n\nWe first compare knowledge transfer methods on the traditional knowledge distillation task, where a student network is trained on the same task as the teacher network. By distilling the knowledge from a large teacher network into a small student network, we can speed up the computation for prediction. We further investigate two problems for this task: whether we can benefit from knowledge transfer in the small data regime and how much performance we lose by reducing the size of the student network? Note that we do not evaluate the performance of VID-LP and LwF as they are designed for transfer learning. When applied, KD, VID-LP and LwF delivered similar performance.\n\nReducing training data. Knowledge transfer can be a computationally expensive task. Given a pre-trained teacher network on the whole training data set, we explore the possibility of using a small portion of the training set for knowledge transfer. We demonstrate the effect of a reduced training set by applying knowledge distillation on CIFAR-10 [15] with four different sizes of training data. We employ wide residual networks (WRN) [15] for the teacher network (WRN-40-2) and the student network (WRN-16-1), where the teacher network is pre-trained on the whole training set of CIFAR-10. Knowledge distillation is applied to four different sizes of training set: 5000 (the full size), 1000, 500, 100 data points per class.  We compare VID-I with KD, FitNet, AT and NST. We also provide performances of the teacher network (Teacher) and the student network trained without any knowledge transfer (Student) as baselines. We choose four pairs of intermediate layers similarly to [31], each of which is located at the end of a group of residual blocks. We implemented VID-I using three 1 \u00d7 1 convolutional layers with hidden channel size as twice of the output channel size.\n\nThe results are shown in Table 1. Our method, VID-I, outperforms other knowledge transfer methods consistently across all regimes. The performance gap increases as the size of dataset get smaller, e.g., VID-I only drops 10.26% of accuracy even when 100 data points per each class are provided to the student network. There is a 31.88% drop without knowledge transfer and a 15.52% drop for the best baseline, i.e., KD + AT.\n\nVarying the size of the student network. The size of the student network gives a trade-off between the speed and the performance in knowledge transfer. We evaluate the performance of knowledge transfer methods on different sizes of the student network. The teacher network (WRN-40-2) is pre-trained on the whole training set of CIFAR-100. The results are shown in in Table 1. As also noticed by Furlanello et al. [9], the student network with the same size as the teacher network outperforms the teacher network with all the knowledge transfer methods. One observes that VID-I consistently outperforms FitNet, AT and NST, which correspond to the same choice of layers for knowledge transfer. It also outperforms KD except for the case when the structure of the student network is identical to that of the teacher network, i.e., WRN-40-2, where two methods can be combined to yield the best performance.\n\n\nTransfer learning\n\nWe evaluate knowledge transfer methods on transfer learning. The teacher network is a residual network (ResNet-34) [11] pre-trained on the ImageNet dataset [23]. We apply transfer learning to improve the performance of two separate image classification tasks. The first task is a fine-grained bird species classification based on the CUB-200-2011 dataset [29], which contains 11,788 images in total for 200 bird species. The second task is an indoor scene classification based on the MIT-67 dataset [20], which contains 15,620 images for 67 classes of indoor scenes. For both tasks, there are a relatively few images per class, which can significantly benefit from knowledge transfer from the ImageNet classification task. To evaluate the performance at various levels of data scarcity, we subsample both datasets into three different sizes (50, 25, 10 per class for MIT-67 and 20, 10, 5 per class for CUB-200-2011) and compare the knowledge transfer methods.\n\nWe evaluate the knowledge transfer methods in two scenarios: a smaller student network of the same architecture (ResNet-18) and different architecture (VGG-9) [25]. We compare our VID-I and VID-LP with LwF, FitNet, AT and NST. We evaluate the performance of the student network without transfer learning (Student) as a baseline. For the teacher and the student network with ResNet architecture, we choose the outputs of the third and fourth groups of residual blocks (from the input) as the intermediate layers for knowledge transfer. In the case of the VGG-9 student network, we choose the fourth and fifth max-pooling layers as the intermediate layers for knowledge transfer, which corresponds to the same spatial dimension as the intermediate layers selected from the teacher network. For applying VID-I to the ResNet-18 student network, we use two 1 \u00d7 1 convolutional layers with the size of intermediate channels as half of the output channel size. When the student network is VGG-9, a single 1 \u00d7 1 convolutional layer without non-linearity is used.\n\nThe results are shown in Table 3. The knowledge transfer from ResNet-34 to VGG-9 gives very similar performance to the transfer from ResNet-34 to ResNet-18 for all the knowledge transfer methods. This shows that knowledge transfer methods are robust against small architecture changes. Our methods outperform other knowledge transfer methods in all regions of comparison. Both VID-I and VID-LP outperforms baselines that correspond to the same choice of layers for knowledge transfer. For the MIT-67 dataset, we observe that our algorithm outperforms even the finetuning method, which requires pre-training of the student network on the source task.\n\n\nKnowledge transfer from CNN to MLP\n\nThe transfer learning experiments show the robustness of the knowledge transfer method against small architecture changes. This leads to an interesting question: whether a knowledge transfer method can work between two completely different network architectures. A solution to this question can open a new direction of knowledge transfer and potentially offer solutions to many problems, e.g., speeding up prediction of recurrent neural networks (RNNs) by transferring knowledge from a RNN to a CNN, speeding up prediction of CNN on CPU or low-energy device by transferring knowledge from a CNN to a multi-layer perceptron (MLP).\n\nIn this paper, we evaluate the performance of knowledge transfer from CNN to MLP on CIFAR-10. There is a well-known performance gap between CNN and MLP on CIFAR-10 [17,27]. The state-of-the-art performance on CIFAR-10 with MLP is 78.62% with initialization from auto-encoders [17] and 74.32% using knowledge distillation [27]. Urban et al. [27] also trained a single convolutional layer achieving the performance of 84.6% using knowledge distillation.\n\nWe apply the knowledge transfer methods in the knowledge distillation setting as mentioned in Section 3.1. We use a teacher network with convolutional layers (WRN-40-2) pre-trained on CIFAR-10. We use a MLP with five fully connected hidden layers as the student network, constructed by stacking one linear layer, three bottleneck linear layers and one linear layer in sequence. Each is followed by a nonlinearity activation in between. Here, the bottleneck layer indicates a composition of two linear layers without nonlinearity that is introduced to speed up learning by reducing the number of parameters. All the hidden layers have the same h number of units and the bottleneck linear layer is composed of two linear layers with a size of h \u00d7 h 4 and h 4 \u00d7 h.\n\nThe knowledge transfer between intermediate layers is defined between the outputs of four residual groups of the teacher network and the outputs of the first four fully connected layers of the student network. We compare VID-I with KD and FitNet since these knowledge transfer methods do not rely on spatial structures. For the same reason, AT and NST are not applicable to multilayer perceptrons. VID-I is implemented with multiple transposed convolutional layers without non-linearities. Specifically, the inputs for the variational distributions, i.e., the hidden layers of the MLP are treated as a tensor with 1 \u00d7 1 spatial dimensions. Single transposed convolutional layer with a 4 \u00d7 4 kernel, unit stride and zero padding is followed by multiple transposed convolutional layers with a 4 \u00d7 4 kernel, two strides, and single padding to match the spatial dimension of the corresponding layer of the teacher network for knowledge transfer. More details on implementations of the student  Urban et al. [27] 74.32 Lin et al. [17] 78.62 Table 4: Experimental result (test accuracy) of distillation on CIFAR-10 from the convolutional teacher network (WRN-40-2) to the fully connected student network (MLPh) with varying size of hidden dimensions h.\n\nnetwork and the auxiliary distribution are in the supplementary material.\n\nThe results are shown in Table 4. Both FitNet and VID-I improve the performance comparing the baseline of directly training the intermediate layers of the student net-work. VID-I significantly outperforms FitNet on MLPs with different sizes. Furthermore, MLP-4096 outperforms the the state-of-the-art performance with MLP reported by Lin et al. [17] (78.62%) and Ba et al. [27] (74.32%) significantly. More importantly, our method bridges the performance gap between CNN (84.6% using one convolutional layer [27]) and MLP shown in previous works.\n\n\nConclusion\n\nIn this work, we proposed the VID framework for effective knowledge transfer by maximizing the variational lower bound of the mutual information between two neural networks. The implementation of our algorithm is based on Gaussian observation models and is empirically shown to outperform other benchmarks in the distillation and transfer learning tasks. Using more flexible recognition models, e.g., [14], for accurate maximization of mutual information and alternative estimation of mutual information, e.g., [4], are both ideas of future interest. For the WRNs and ResNets used throughout the experiments, we use the same architectures as originally described by Zagoruyko et al., [32] and He et al., [11] respectively. For the VGG-9 network used in transfer learning, i.e., Section 3.2, we make a slight modification from the VGG-11 network [25] without deviating from the VGG design philosophy. It is conducted by first stacking eight 3 \u00d7 3 convolutional layers with 64, 128, 256, 256, 512, 512, 512, 512 channels in order with batch normalization and rectified linear unit (ReLU) after every convolutional layers. Furthermore, additional max-pooling layers are inserted after the {1, 2, 4, 6, 8}-th ReLUs. Then the final max-pooling layer is followed by global average pooling layer and a linear layer leading up to the prediction of the labels. For the MLP used in knowledge transfer from CNN to MLP, i.e., Section 3.3, we sequentially stack one linear layer, three bottleneck linear layers and one linear layer leading to the prediction of labels, where dropout with drop rate of 0.2, batch normalization and ReLU was inserted between each layers. Here, the bottleneck layer indicates a composition of two linear layers without non-linearity that is introduced to speed up learning by reducing the number of parameters. All of the hidden layers have the same h number of units and the bottleneck linear layer is composed of two linear layers with a size of h \u00d7 h 4 and h 4 \u00d7 h.\n\n\nA.2. Parameterization of VID\n\nIn the knowledge distillation experiments, i.e., Section 3.1, we parameterize the mean function \u00b5(\u00b7) in equation (5) for VID-I by three 1 \u00d7 1 convolutional layers with batch normalization and ReLU between each layers. The hidden channel sizes were chosen to be twice of the output channel size. For the transfer learning experiments, i.e., Section 3.2, we first parameterize the mean function \u00b5(\u00b7) in equation (5) for VID-I by two 1 \u00d7 1 convolutional layers with batch normalization and ReLU between the layers. For this case, the hidden channel sizes were chosen to be half of the output channel size. Furthermore, VID-LP was parameterized as in equation (6) with mean function \u00b5(\u00b7) being a single linear layer, i.e., a linear transformation. Finally, we consider the knowledge transfer from CNN to MLP, i.e., Section 3.3, based on VID-I with equation (5). For this case, the mean function maps the one-dimensional input s from in-termediate layer of the student network (MLP) into a threedimensional output t corresponding to intermediate layer of the teacher network (CNN), i.e., \u00b5 : R N \u2192 R C\u00d7H\u00d7W . To this end, the input is first treated as a three-dimensional tensor with with unit spatial dimensions, i.e., s \u2208 R N \u00d71\u00d71 . Then the input goes through a single transposed convolutional layer with a 4 \u00d7 4 kernel, unit stride and zero padding followed by multiple transposed convolutional layers with a 4 \u00d7 4 kernel, two strides and unit padding. The number of transposed convolutional layers were varied for corresponding layer of the teacher network, in order to match the spatial dimension.\n\n\nA.3. Loss function and training scheme\n\nIn the experiments, the loss function for VID takes the following form:\nL = \u03bb 1 L S \u2212 K k=1 \u03bb 2 N k E t (k) ,s (k) [log q(t (k) |s (k) )],(8)\nwhere L S is the task-specific loss function for the target task, \u03bb 1 , \u03bb 2 > 0 are hyper-parameters introduced for balancing between the cross-entropy and the regularization terms, and N k denotes the total number of dimensions for each layer selected from the teacher network for knowledge transfer, i.e., Finally, we describe the training scheme used for the experiments. Due to unstable gradients in some cases, we clipped the norm of gradients by 100 throughout the experiments. Additionally, the homoscedastic variance for the variational distribution in equation (5) and (6) was initialized with value of 5.0. In the knowledge distillation experiments, i.e., Section 3.1, when training on the full dataset, we used stochastic gradient descent (SGD) for 200 epochs with batch size of 128 and weight decay of 0.0005. Initial learning rate of 0.1 is decayed 0.2 times at {60, 120, 160}-th \nt (k) \u2208 R N k or N k = C k H k W k when t (k) \u2208 R C k \u00d7H k \u00d7W k .\n\nB. Additional Experiments\n\n\nB.1. Visualization of learned parameters\n\nIn order to examine the behavior of the learned variance parameters \u03c3 n in VID, we plot its channel-wise value for different layers in Figure 3. Here, one can observe that the   \n\n\nB.2. Transfer learning from SVHN to MNIST\n\nWe also provide additional experimental results for transfer learning from SVHN to MNIST in Table 5. To this end, the teacher network is trained on the full SVHN dataset that was converted to grayscale, then the student network is trained on MNIST with 200 data points per class. We employ LeNet-like architectures for both networks. Again, one observes that VID outperforms over other methods.\n\n\nB.3. Comparison with adversarial network compression\n\nWe additionally compare with the recently proposed adversarial network compression [3]. by repeating the knowledge distillation experiment on CIFAR-10 between ResNets presented by [3]. The corresponding results are reported in Table 6. One observes that our methods outperforms the ANC with a small margin.\n\n\nB.4. Experimental results with standard deviation\n\nIn Table 7, 8, 9, 10, 11 and 12, we provide full experimental results corresponding to the Table 1, 2, 3a, 3b, 3d and 4 respectively with additional standard deviations for the three repeated runs.\n\n\nB.5. Additional heat maps for VID training\n\nIn Figure 4, we provide additional visualization results of the knowledge transfer based on VID that was plotted in the same way as in Figure 2.          : Plots for the heat maps corresponding to the variational distribution evaluated for spatial dimensions of the intermediate layer in the teacher network, i.e., log q(t h,w |s) = c log q(t c,h,w |s). Each figure corresponds to (a) original input image, (b, c, d) log-likelihood log q(t h,w |s) that was normalized and interpolated to fit the spatial dimension of the input image (red pixels correspond to high probability), (d) log-likelihood of variational distribution optimized for the student network trained without any knowledge transfer applied and (f) magnitude of the layer t averaged for each spatial dimensions.\n\n\nepoch (c) 40-th epoch (d) 160-th epoch (e) no transfer (f) magnitude of t h,w\n\n\nA student network with four choices of size, i.e., WRN-40-2, WRN-16-2, WRN-40-1, WRN-16-1, is trained on the whole training set of CIFAR-100. We compare our VID-I with KD, FitNet, AT and NST along with the Teacher and Student baselines. The choices of intermediate layers are the same as the previous experiment.\n\n\nFor all of the experiments and both VID-I and VID-LP, we select \u03bb 1 and \u03bb 2 from {0.1, 1} and {10, 100} respectively, based on the performance evaluated on the validation set. For other knowledge transfer methods, we also choose the scaling of the cross-entropy term, i.e., \u03bb 1 > 0, from {0.1, 1}. Furthermore, the corresponding regularization terms are scaledby {1, 10}, {10, 100}, {100, 1000}, {5, 50}for KD, FitNet, AT and NST respectively, based on the performance on the validation set. Additionally, KD was implemented with temperature scaling parameter set to T = 4.\n\nFigure 3 :\n3Channel-wise variance \u03c3 2 n = \u03c3 2 c (sorted) learned by VID-I in transfer learning from ResNet34 trained on Im-ageNet to ResNet18 trained on CUB-200, corresponding to the ends of third (top) and fourth (bottom) residual blocks respectively. epoch. When training on subset of the dataset, the numbers are appropriately scaled to have similar number of updates for parameters. In the transfer learning experiments, i.e., Section 3.2, when training on the full dataset for ResNet-34 and ResNet-18, we use SGD for 250 epochs with batch size 128 and weight decay of 0.0005. Initial learning rate of 0.05 is decayed by 0.2 times at {150, 200}-th epoch. For the case of VGG-9, we use SGD for 250 epochs with batch size 12 without weight decay. Initial learning rate of 0.01 is decayed by 0.2 times at 150 and 200-th epoch. Again, the numbers are appropriately scaled to match the number of updates for parameters when training on subset of the full dataset.In the knowledge transfer from CNN to MLP experiments, i.e., Section 3.3, we used SGD for 700 epochs with batch size of 128 and weight decay of 0.0005. Initial learning rate of 0.001 was decayed by 0.2 times at 500 and 600-th epoch.\n\nM\n\n\nFigure 4\n4Figure 4: Plots for the heat maps corresponding to the variational distribution evaluated for spatial dimensions of the intermediate layer in the teacher network, i.e., log q(t h,w |s) = c log q(t c,h,w |s). Each figure corresponds to (a) original input image, (b, c, d) log-likelihood log q(t h,w |s) that was normalized and interpolated to fit the spatial dimension of the input image (red pixels correspond to high probability), (d) log-likelihood of variational distribution optimized for the student network trained without any knowledge transfer applied and (f) magnitude of the layer t averaged for each spatial dimensions.\n\nTable 2 :\n2Experimental results (test accuracy) of knowl-\nedge distillation on the CIFAR-100 dataset from the teacher \nnetwork (WRN-40-2) to the student networks (WRN-d-w) \nwith varying factor of depth d and width w. \n\n\n\nTable 3 :\n3Experimental results (test accuracy) of transfer learning from the teacher network (ResNet-34) to the student network \n(ResNet-18/VGG-9) for the MIT-67/CUB-200-2011 dataset with varying number of data points per class (denoted by M ). \nWe use M \u2248 M avg to denote the setting where the number of data points per class is non-uniform and M avg in average. \nFine-tuning gives good results on transfer learning, but is not directly comparable as it is not a knowledge transfer method. \n\nNetwork \nMLP-4096 MLP-2048 MLP-1024 \n\nStudent \n70.60 \n70.78 \n70.90 \nKD \n70.42 \n70.53 \n70.79 \nFitNet \n76.02 \n74.08 \n72.91 \nVID-I \n85.18 \n83.47 \n78.57 \n\n\n\nSupplementary Material :\nMaterialVariational Information Distillation for Knowledge TransferA. Implementation details \n\nA.1. Network architectures \n\n\n\nTable 5 :\n5Experimental results (test accuracy) of transfer learning from grayscale-SVHN to MNIST with 200 samples per class for LeNet-based architectures.Teacher \nStudent \nKD \nFitNet ANC VID-I \n\n92.36 / 93.43 91.69 / 91.42 91.12 91.61 91.92 92.15 \n\n\n\nTable 6 :\n6Experimental results (validation accuracy) in comparison to Adversarial Network Compression (ANC), for knowledge distillation from ResNet-164 to ResNet-20 on CIFAR-10 dataset. Underlined numbers are results reported by Belagiannis et al.[3].learned variance parameters \u03c3 n are diverse, especially accross different layers. Hence, modeling of homoscedastic variance is necessary for obtaining a tighter lower bound of the mutual information in the equation(3).\n\n\nKD + VID-I 92.31 (\u00b1 0.31) 89.33 (\u00b1 0.21) 87.34 (\u00b1 0.19) 81.80 (\u00b1 0.01)5000 \n1000 \n500 \n100 \n\nTeacher \n94.36 (\u00b1 0.27) \n-\n-\n-\nStudent \n90.82 (\u00b1 0.17)) 84.64 (\u00b1 0.05) 79.64 (\u00b1 0.05) 55.03 (\u00b1 6.59) \n\nKD \n91.66 (\u00b1 0.13) 85.52 (\u00b1 0.02) 81.48 (\u00b1 0.24) 55.03 (\u00b1 0.05) \nFitNet \n90.79 (\u00b1 0.31) 84.84 (\u00b1 0.35) 80.82 (\u00b1 0.19) 68.57 (\u00b1 0.84) \nAT \n91.54 (\u00b1 0.10) 87.43 (\u00b1 0.35) 84.78 (\u00b1 0.27) 73.96 (\u00b1 0.96) \nNST \n91.11 (\u00b1 0.12) 86.76 (\u00b1 0.37) 82.68 (\u00b1 0.13) 64.76 (\u00b1 0.45) \nVID-I \n91.94 (\u00b1 0.31) 89.76 (\u00b1 0.07) 88.33 (\u00b1 0.43) 82.03 (\u00b1 1.13) \n\nKD + AT \n91.39 (\u00b1 0.26) 87.11 (\u00b1 0.03) 84.54 (\u00b1 0.01) 75.11 (\u00b1 0.83) \n\n\nTable 7 :\n7Experimental results (test accuracy) of knowledge distillation on the CIFAR-10 dataset from teacher network (WRN-40-2) to student network (WRN-16-1) with varying number of data points per class (denoted by M ). KD + VID-I 75.90 (\u00b1 0.26) 73.50 (\u00b1 0.06) 72.47 (\u00b1 0.21) 66.91 (\u00b1 0.06)(d, w) \n(40,2) \n(16, 2) \n(40, 1) \n(16, 1) \n\nTeacher \n74.16 (\u00b1 0.33) \n-\n-\n-\nStudent \n74.34 (\u00b1 0.46) 70.42 (\u00b1 0.63) 68.79 (\u00b1 0.19) 65.46 (\u00b1 0.13) \n\nKD \n75.54 (\u00b1 0.25) 72.94 (\u00b1 0.38) 71.34 (\u00b1 0.19) 66.97 (\u00b1 0.46) \nFitNet \n74.29 (\u00b1 0.17) 70.89 (\u00b1 0.61) 68.66 (\u00b1 0.27) 65.38 (\u00b1 0.05) \nAT \n74.76 (\u00b1 0.36) 71.06 (\u00b1 0.07) 69.85 (\u00b1 0.51) 65.31 (\u00b1 0.51) \nNST \n74.81 (\u00b1 0.19) 71.19 (\u00b1 0.54) 68.00 (\u00b1 0.20) 64.95 (\u00b1 0.33) \nVID-I \n75.25 (\u00b1 0.37) 73.31 (\u00b1 0.30) 71.51 (\u00b1 0.15) 66.32 (\u00b1 0.52) \n\nKD + AT \n75.90 (\u00b1 0.40) 73.16 (\u00b1 0.15) 71.48 (\u00b1 0.15) 66.48 (\u00b1 0.67) \n\n\nTable 8 :\n8Experimental results (test accuracy) of knowledge distillation on the CIFAR-100 dataset from the teacher network (WRN-40-2) to the student networks (WRN-d-w) with varying factor of depth d and width w. VID-LP + VID-I 71.69 (\u00b1 0.37) 66.87 (\u00b1 0.59) 61.29 (\u00b1 0.04) 49.65 (\u00b1 0.97)M \n\u224880 \n50 \n25 \n10 \n\nStudent \n48.78 (\u00b1 0.72) 37.46 (\u00b1 0.88) 25.52 (\u00b1 1.37) 14.68 (\u00b1 0.41) \nFinetuned \n71.22 (\u00b1 0.85) 65.30 (\u00b1 0.83) 58.56 (\u00b1 0.55) 48.86 (\u00b1 0.87) \n\nLwF \n61.34 (\u00b1 0.54) 50.07 (\u00b1 0.22) 38.76 (\u00b1 0.34) 22.09 (\u00b1 0.58) \nFitNet \n70.37 (\u00b1 0.97) 61.34 (\u00b1 0.94) 54.60 (\u00b1 1.31) 36.54 (\u00b1 0.34) \nAT \n57.99 (\u00b1 0.39) 48.66 (\u00b1 0.67) 42.51 (\u00b1 1.09) 25.90 (\u00b1 1.29) \nNST \n56.79 (\u00b1 1.20) 46.92 (\u00b1 0.80) 34.38 (\u00b1 1.19) 20.70 (\u00b1 0.22) \nVID-LP \n67.54 (\u00b1 0.42) 59.18 (\u00b1 0.76) 47.89 (\u00b1 0.75) 31.22 (\u00b1 1.12) \nVID-I \n72.04 (\u00b1 0.62) 66.42 (\u00b1 0.45) 60.77 (\u00b1 0.91) 50.60 (\u00b1 1.06) \n\nLwF + FitNet \n70.32 (\u00b1 0.69) 61.19 (\u00b1 0.45) 53.83 (\u00b1 0.91) 36.67 (\u00b1 0.88) \n\n\nTable 9 :\n9Experimental results (test accuracy) of transfer learning from the teacher network (ResNet-34) to the student network (ResNet-18) for the MIT-67 dataset with varying number of data points per class (denoted by M ). VID-LP + VID-I 71.44 (\u00b1 1.21) 66.67 (\u00b1 0.50) 57.59 (\u00b1 0.23) 46.42 (\u00b1 1.01)M \n\n\u224880 \n50 \n25 \n10 \n\nStudent \n54.13 (\u00b1 0.50) 44.13 (\u00b1 0.30) 29.05 (\u00b1 0.72) 15.92 (\u00b1 0.67) \nFinetuned \n66.39 (\u00b1 0.41) 58.51 (\u00b1 0.45) 51.97 (\u00b1 0.31) 39.93 (\u00b1 0.58) \n\nLwF \n58.18 (\u00b1 0.53) 49.68 (\u00b1 2.09) 38.08 (\u00b1 3.33) 26.09 (\u00b1 1.08) \nFitNet \n71.00 (\u00b1 0.60) 64.05 (\u00b1 0.63) 55.30 (\u00b1 1.42) 40.67 (\u00b1 0.13) \nAT \n60.57 (\u00b1 0.30) 53.11 (\u00b1 0.83) 42.64 (\u00b1 0.57) 26.12 (\u00b1 0.52) \nNST \n55.40 (\u00b1 0.34) 47.29 (\u00b1 1.23) 34.03 (\u00b1 1.19) 21.27 (\u00b1 0.71) \nVID-LP \n68.21 (\u00b1 0.59) 61.77 (\u00b1 0.57) 50.75 (\u00b1 0.49) 39.23 (\u00b1 0.11) \nVID-I \n71.99 (\u00b1 0.19) 66.62 (\u00b1 0.75) 59.00 (\u00b1 0.38) 46.24 (\u00b1 0.31) \n\nLwF + FitNet \n70.75 (\u00b1 0.47) 64.38 (\u00b1 1.13) 55.60 (\u00b1 0.13) 41.34 (\u00b1 0.33) \n\n\nTable 10 :\n10Experimental results (test accuracy) of transfer learning from the teacher network (ResNet-34) to the student network (VGG-9) for the MIT-67 dataset with varying number of data points per class (denoted by M ). VID-LP + VID-I 70.03 (\u00b1 0.05) 63.46 (\u00b1 0.40) 48.79 (\u00b1 0.04) 32.35 (\u00b1 0.24)M \n\u224829.95 \n20 \n10 \n5 \n\nStudent \n44.59 (\u00b1 1.93) 32.10 (\u00b1 0.65) 15.69 (\u00b1 0.27) \n9.66 (\u00b1 0.22) \nFinetuned \n60.96 (\u00b1 1.88) 51.86 (\u00b1 0.99) 46.88 (\u00b1 0.92) 39.98 (\u00b1 0.33) \n\nLwF \n52.54 (\u00b1 0.12) 36.38 (\u00b1 0.14) 22.79 (\u00b1 0.35) 11.52 (\u00b1 0.15) \nFitNet \n68.96 (\u00b1 0.45) 61.52 (\u00b1 0.80) 48.04 (\u00b1 0.64) 32.89 (\u00b1 1.95) \nAT \n56.28 (\u00b1 1.75) 43.96 (\u00b1 0.80) 28.33 (\u00b1 0.17) 13.98 (\u00b1 1.01) \nNST \n56.55 (\u00b1 2.05) 44.95 (\u00b1 0.36) 28.43 (\u00b1 0.35) 14.66 (\u00b1 2.48) \nVID-LP \n66.82 (\u00b1 0.41) 55.94 (\u00b1 0.27) 38.10 (\u00b1 0.83) 30.47 (\u00b1 0.31) \nVID-I \n71.51 (\u00b1 1.48) 65.69 (\u00b1 0.68) 53.29 (\u00b1 1.20) 38.09 (\u00b1 1.05) \n\nLwF + FitNet \n68.40 (\u00b1 0.50) 61.40 (\u00b1 0.40) 45.57 (\u00b1 0.04) 28.41 (\u00b1 0.24) \n\n\nTable 11 :\n11Experimental results (test accuracy) of transfer learning from the teacher network (ResNet-34) to the student network (VGG-9) for the CUB-200 dataset with varying number of data points per class (denoted by M ).Network \nMLP-4096 \nMLP-2048 \nMLP-1024 \n\nStudent \n70.60 (\u00b1 0.26) 70.78 (\u00b1 0.45) 70.90 (\u00b1 0.13) \nKD \n70.42 (\u00b1 0.26) 70.53 (\u00b1 0.18) 70.79 (\u00b1 0.35) \nFitNet \n76.02 (\u00b1 0.26) 74.08 (\u00b1 0.18) 72.91 (\u00b1 0.35) \nVID-I \n85.18 (\u00b1 0.20) 83.47 (\u00b1 0.29) 78.57 (\u00b1 0.11) \n\n\n\nTable 12 :\n12Experimental result (test accuracy) of distillation on CIFAR-10 from the convolutional teacher network (WRN-40-2) to the fully connected student network (MLP-h) with varying size of hidden dimensions h.\n\nThe IM algorithm: a variational approach to information maximization. D B F Agakov, D. B. F. Agakov. The IM algorithm: a variational approach to information maximization. 2004.\n\nDo deep nets really need to be deep?. J Ba, R Caruana, Advances in neural information processing systems. J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654-2662, 2014.\n\nAdversarial network compression. V Belagiannis, A Farshad, F Galasso, European Conference on Computer Vision. SpringerV. Belagiannis, A. Farshad, and F. Galasso. Adversarial net- work compression. In European Conference on Computer Vision, pages 431-449. Springer, 2018.\n\nMine: mutual information neural estimation. I Belghazi, S Rajeswar, A Baratin, R D Hjelm, A Courville, arXiv:1801.04062arXiv preprintI. Belghazi, S. Rajeswar, A. Baratin, R. D. Hjelm, and A. Courville. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.\n\nMixture density networks. C M Bishop, CiteseerTechnical reportC. M. Bishop. Mixture density networks. Technical report, Citeseer, 1994.\n\nCoupled end-to-end transfer learning with generalized Fisher information. S Chen, C Zhang, M Dong, Computer Vision and Pattern Recognition. S. Chen, C. Zhang, and M. Dong. Coupled end-to-end trans- fer learning with generalized Fisher information. In Com- puter Vision and Pattern Recognition, 2018.\n\nFlownet: Learning optical flow with convolutional networks. A Dosovitskiy, P Fischer, E Ilg, P Hausser, C Hazirbas, V Golkov, P Van Der, D Smagt, T Cremers, Brox, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionA. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 2758-2766, 2015.\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in neural information processing systems. D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366-2374, 2014.\n\nBorn again neural networks. T Furlanello, Z C Lipton, M Tschannen, L Itti, A Anandkumar, In ICML. T. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and A. Anandkumar. Born again neural networks. In ICML, 2018.\n\nFast r-CNN. R Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionR. Girshick. Fast r-CNN. In Proceedings of the IEEE inter- national conference on computer vision, pages 1440-1448, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770-778, 2016.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.02531arXiv preprintG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nLike what you like: Knowledge distill via neuron selectivity transfer. Z Huang, N Wang, arXiv:1707.01219arXiv preprintZ. Huang and N. Wang. Like what you like: Knowl- edge distill via neuron selectivity transfer. arXiv preprint arXiv:1707.01219, 2017.\n\nImproved variational inference with inverse autoregressive flow. D P Kingma, T Salimans, R Jozefowicz, X Chen, I Sutskever, M Welling, Advances in Neural Information Processing Systems. D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural In- formation Processing Systems, pages 4743-4751, 2016.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, CiteseerTechnical reportA. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\n\nLearning without forgetting. Z Li, D Hoiem, IEEE Transactions on Pattern Analysis and Machine Intelligence. Z. Li and D. Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\nHow far can we go without convolution: Improving fully-connected networks. Z Lin, R Memisevic, K Konda, arXiv:1511.02580arXiv preprintZ. Lin, R. Memisevic, and K. Konda. How far can we go without convolution: Improving fully-connected networks. arXiv preprint arXiv:1511.02580, 2015.\n\nAn application of the principle of maximum information preservation to linear systems. R Linsker, Advances in neural information processing systems. R. Linsker. An application of the principle of maximum in- formation preservation to linear systems. In Advances in neu- ral information processing systems, pages 186-194, 1989.\n\nA survey on transfer learning. S J Pan, Q Yang, S. J. Pan, Q. Yang, et al. A survey on transfer learning. 2010.\n\nRecognizing indoor scenes. A Quattoni, A Torralba, Computer Vision and Pattern Recognition. IEEECVPR 2009. IEEE Conference onA. Quattoni and A. Torralba. Recognizing indoor scenes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 413-420. IEEE, 2009.\n\nSemi-supervised learning with ladder networks. A Rasmus, M Berglund, M Honkala, H Valpola, T Raiko, Advances in Neural Information Processing Systems. A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems, pages 3546-3554, 2015.\n\nA Romero, N Ballas, S E Kahou, A Chassang, C Gatta, Y Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. arXiv preprintA. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International Journal of Computer Vision. 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.\n\nUnsupervised pre-training across image domains improves lung tissue classification. T Schlegl, J Ofner, G Langs, International MICCAI Workshop on Medical Computer Vision. SpringerT. Schlegl, J. Ofner, and G. Langs. Unsupervised pre-training across image domains improves lung tissue classification. In International MICCAI Workshop on Medical Computer Vi- sion, pages 82-93. Springer, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nDeeppose: Human pose estimation via deep neural networks. A Toshev, C Szegedy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionA. Toshev and C. Szegedy. Deeppose: Human pose esti- mation via deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 1653-1660, 2014.\n\nG Urban, K J Geras, S E Kahou, O Aslan, S Wang, A Mohamed, M Philipose, M Richardson, R Caruana, Do deep convolutional nets really need to be deep and convolutional? In ICLR. G. Urban, K. J. Geras, S. E. Kahou, O. Aslan, S. Wang, A. Mohamed, M. Philipose, M. Richardson, and R. Caru- ana. Do deep convolutional nets really need to be deep and convolutional? In ICLR, 2017.\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. P Vincent, H Larochelle, I Lajoie, Y Bengio, P.-A Manzagol, Journal of machine learning research. 11P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising cri- terion. Journal of machine learning research, 11(Dec):3371- 3408, 2010.\n\nCaltech-UCSD Birds 200. P Welinder, S Branson, T Mita, C Wah, F Schroff, S Belongie, P Perona, CNS-TR-2010-001California Institute of TechnologyTechnical ReportP. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be- longie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technol- ogy, 2010.\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. J Yim, D Joo, J Bae, J Kim, J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowl- edge distillation: Fast optimization, network minimization and transfer learning.\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. S Zagoruyko, N Komodakis, ICLR. S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neu- ral networks via attention transfer. In ICLR, 2016.\n\nS Zagoruyko, N Komodakis, arXiv:1605.07146Wide residual networks. arXiv preprintS. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n", "annotations": {"author": "[{\"end\":261,\"start\":63},{\"end\":436,\"start\":262},{\"end\":636,\"start\":437},{\"end\":835,\"start\":637},{\"end\":1030,\"start\":836}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":71},{\"end\":273,\"start\":271},{\"end\":453,\"start\":445},{\"end\":652,\"start\":644},{\"end\":847,\"start\":844}]", "author_first_name": "[{\"end\":70,\"start\":63},{\"end\":267,\"start\":262},{\"end\":270,\"start\":268},{\"end\":444,\"start\":437},{\"end\":641,\"start\":637},{\"end\":643,\"start\":642},{\"end\":843,\"start\":836}]", "author_affiliation": "[{\"end\":260,\"start\":100},{\"end\":435,\"start\":275},{\"end\":635,\"start\":475},{\"end\":834,\"start\":674},{\"end\":1029,\"start\":869}]", "title": "[{\"end\":60,\"start\":1},{\"end\":1090,\"start\":1031}]", "venue": null, "abstract": "[{\"end\":2193,\"start\":1092}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2318,\"start\":2315},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2340,\"start\":2336},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2358,\"start\":2355},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2386,\"start\":2382},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2402,\"start\":2398},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2425,\"start\":2421},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3260,\"start\":3256},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3433,\"start\":3429},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3748,\"start\":3744},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3813,\"start\":3809},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3833,\"start\":3829},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4357,\"start\":4353},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4360,\"start\":4357},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4593,\"start\":4589},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4616,\"start\":4613},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4810,\"start\":4806},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4940,\"start\":4937},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4942,\"start\":4940},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4945,\"start\":4942},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4948,\"start\":4945},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4951,\"start\":4948},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5158,\"start\":5154},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5192,\"start\":5188},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5922,\"start\":5919},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6131,\"start\":6127},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6134,\"start\":6131},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7339,\"start\":7335},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7342,\"start\":7339},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9727,\"start\":9724},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10438,\"start\":10435},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14758,\"start\":14755},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14949,\"start\":14946},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16261,\"start\":16257},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16301,\"start\":16297},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16714,\"start\":16710},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17131,\"start\":17128},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17784,\"start\":17781},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18318,\"start\":18314},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18658,\"start\":18654},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20470,\"start\":20467},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20473,\"start\":20470},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20476,\"start\":20473},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20479,\"start\":20476},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20622,\"start\":20618},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20725,\"start\":20721},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20838,\"start\":20834},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20995,\"start\":20992},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22071,\"start\":22067},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22111,\"start\":22107},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22146,\"start\":22142},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22192,\"start\":22188},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22258,\"start\":22254},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22346,\"start\":22342},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22415,\"start\":22411},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24012,\"start\":24008},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24100,\"start\":24096},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24644,\"start\":24640},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25676,\"start\":25673},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26303,\"start\":26299},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26344,\"start\":26340},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26543,\"start\":26539},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26687,\"start\":26683},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27308,\"start\":27304},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29688,\"start\":29684},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29691,\"start\":29688},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29800,\"start\":29796},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29845,\"start\":29841},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29864,\"start\":29860},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31743,\"start\":31739},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31765,\"start\":31761},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32408,\"start\":32404},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32436,\"start\":32432},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32571,\"start\":32567},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33025,\"start\":33021},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33134,\"start\":33131},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33308,\"start\":33304},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33328,\"start\":33324},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33469,\"start\":33465},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35051,\"start\":35048},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35494,\"start\":35491},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37001,\"start\":36998},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38212,\"start\":38209},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38309,\"start\":38306},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":43843,\"start\":43840}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39586,\"start\":39507},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39901,\"start\":39587},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40477,\"start\":39902},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41673,\"start\":40478},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41677,\"start\":41674},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42319,\"start\":41678},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42540,\"start\":42320},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43187,\"start\":42541},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43338,\"start\":43188},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":43590,\"start\":43339},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44062,\"start\":43591},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":44665,\"start\":44063},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":45509,\"start\":44666},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":46441,\"start\":45510},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":47387,\"start\":46442},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":48332,\"start\":47388},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":48811,\"start\":48333},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":49028,\"start\":48812}]", "paragraph": "[{\"end\":3115,\"start\":2209},{\"end\":3356,\"start\":3117},{\"end\":4497,\"start\":3358},{\"end\":5193,\"start\":4499},{\"end\":6446,\"start\":5195},{\"end\":6514,\"start\":6448},{\"end\":6729,\"start\":6516},{\"end\":7077,\"start\":6731},{\"end\":7361,\"start\":7079},{\"end\":8134,\"start\":7408},{\"end\":8838,\"start\":8136},{\"end\":9503,\"start\":8906},{\"end\":10070,\"start\":9546},{\"end\":10631,\"start\":10213},{\"end\":11337,\"start\":10694},{\"end\":12008,\"start\":11363},{\"end\":12905,\"start\":12141},{\"end\":13108,\"start\":12907},{\"end\":13808,\"start\":13205},{\"end\":16106,\"start\":13810},{\"end\":17132,\"start\":16140},{\"end\":17570,\"start\":17134},{\"end\":20340,\"start\":17627},{\"end\":21534,\"start\":20342},{\"end\":22616,\"start\":21550},{\"end\":22958,\"start\":22618},{\"end\":23659,\"start\":22985},{\"end\":24834,\"start\":23661},{\"end\":25258,\"start\":24836},{\"end\":26162,\"start\":25260},{\"end\":27143,\"start\":26184},{\"end\":28199,\"start\":27145},{\"end\":28850,\"start\":28201},{\"end\":29518,\"start\":28889},{\"end\":29971,\"start\":29520},{\"end\":30734,\"start\":29973},{\"end\":31982,\"start\":30736},{\"end\":32057,\"start\":31984},{\"end\":32605,\"start\":32059},{\"end\":34605,\"start\":32620},{\"end\":36235,\"start\":34638},{\"end\":36349,\"start\":36278},{\"end\":37313,\"start\":36420},{\"end\":37629,\"start\":37451},{\"end\":38069,\"start\":37675},{\"end\":38432,\"start\":38126},{\"end\":38683,\"start\":38486},{\"end\":39506,\"start\":38730}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8905,\"start\":8839},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9545,\"start\":9504},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10212,\"start\":10071},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10693,\"start\":10632},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12140,\"start\":12009},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13204,\"start\":13109},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17626,\"start\":17571},{\"attributes\":{\"id\":\"formula_7\"},\"end\":36419,\"start\":36350},{\"attributes\":{\"id\":\"formula_8\"},\"end\":37379,\"start\":37314}]", "table_ref": "[{\"end\":24868,\"start\":24861},{\"end\":25634,\"start\":25627},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28233,\"start\":28226},{\"end\":31779,\"start\":31772},{\"end\":32091,\"start\":32084},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":37774,\"start\":37767},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":38360,\"start\":38353},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":38496,\"start\":38489},{\"end\":38584,\"start\":38577}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2207,\"start\":2195},{\"attributes\":{\"n\":\"2.\"},\"end\":7406,\"start\":7364},{\"attributes\":{\"n\":\"2.1.\"},\"end\":11361,\"start\":11340},{\"attributes\":{\"n\":\"2.2.\"},\"end\":16138,\"start\":16109},{\"attributes\":{\"n\":\"3.\"},\"end\":21548,\"start\":21537},{\"attributes\":{\"n\":\"3.1.\"},\"end\":22983,\"start\":22961},{\"attributes\":{\"n\":\"3.2.\"},\"end\":26182,\"start\":26165},{\"attributes\":{\"n\":\"3.3.\"},\"end\":28887,\"start\":28853},{\"attributes\":{\"n\":\"4.\"},\"end\":32618,\"start\":32608},{\"end\":34636,\"start\":34608},{\"end\":36276,\"start\":36238},{\"end\":37406,\"start\":37381},{\"end\":37449,\"start\":37409},{\"end\":37673,\"start\":37632},{\"end\":38124,\"start\":38072},{\"end\":38484,\"start\":38435},{\"end\":38728,\"start\":38686},{\"end\":40489,\"start\":40479},{\"end\":41676,\"start\":41675},{\"end\":41687,\"start\":41679},{\"end\":42330,\"start\":42321},{\"end\":42551,\"start\":42542},{\"end\":43213,\"start\":43189},{\"end\":43349,\"start\":43340},{\"end\":43601,\"start\":43592},{\"end\":44676,\"start\":44667},{\"end\":45520,\"start\":45511},{\"end\":46452,\"start\":46443},{\"end\":47399,\"start\":47389},{\"end\":48344,\"start\":48334},{\"end\":48823,\"start\":48813}]", "table": "[{\"end\":42540,\"start\":42332},{\"end\":43187,\"start\":42553},{\"end\":43338,\"start\":43281},{\"end\":43590,\"start\":43495},{\"end\":44665,\"start\":44135},{\"end\":45509,\"start\":44959},{\"end\":46441,\"start\":45798},{\"end\":47387,\"start\":46743},{\"end\":48332,\"start\":47687},{\"end\":48811,\"start\":48558}]", "figure_caption": "[{\"end\":39586,\"start\":39509},{\"end\":39901,\"start\":39589},{\"end\":40477,\"start\":39904},{\"end\":41673,\"start\":40491},{\"end\":42319,\"start\":41689},{\"end\":43281,\"start\":43222},{\"end\":43495,\"start\":43351},{\"end\":44062,\"start\":43603},{\"end\":44135,\"start\":44065},{\"end\":44959,\"start\":44678},{\"end\":45798,\"start\":45522},{\"end\":46743,\"start\":46454},{\"end\":47687,\"start\":47402},{\"end\":48558,\"start\":48347},{\"end\":49028,\"start\":48826}]", "figure_ref": "[{\"end\":2727,\"start\":2719},{\"end\":5991,\"start\":5983},{\"end\":13931,\"start\":13923},{\"end\":14862,\"start\":14854},{\"end\":15138,\"start\":15128},{\"end\":15501,\"start\":15491},{\"end\":15974,\"start\":15965},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37594,\"start\":37586},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38741,\"start\":38733},{\"end\":38873,\"start\":38865}]", "bib_author_first_name": "[{\"end\":49101,\"start\":49100},{\"end\":49105,\"start\":49102},{\"end\":49247,\"start\":49246},{\"end\":49253,\"start\":49252},{\"end\":49485,\"start\":49484},{\"end\":49500,\"start\":49499},{\"end\":49511,\"start\":49510},{\"end\":49768,\"start\":49767},{\"end\":49780,\"start\":49779},{\"end\":49792,\"start\":49791},{\"end\":49803,\"start\":49802},{\"end\":49805,\"start\":49804},{\"end\":49814,\"start\":49813},{\"end\":50036,\"start\":50035},{\"end\":50038,\"start\":50037},{\"end\":50221,\"start\":50220},{\"end\":50229,\"start\":50228},{\"end\":50238,\"start\":50237},{\"end\":50508,\"start\":50507},{\"end\":50523,\"start\":50522},{\"end\":50534,\"start\":50533},{\"end\":50541,\"start\":50540},{\"end\":50552,\"start\":50551},{\"end\":50564,\"start\":50563},{\"end\":50574,\"start\":50573},{\"end\":50585,\"start\":50584},{\"end\":50594,\"start\":50593},{\"end\":51078,\"start\":51077},{\"end\":51087,\"start\":51086},{\"end\":51098,\"start\":51097},{\"end\":51377,\"start\":51376},{\"end\":51391,\"start\":51390},{\"end\":51393,\"start\":51392},{\"end\":51403,\"start\":51402},{\"end\":51416,\"start\":51415},{\"end\":51424,\"start\":51423},{\"end\":51574,\"start\":51573},{\"end\":51876,\"start\":51875},{\"end\":51882,\"start\":51881},{\"end\":51891,\"start\":51890},{\"end\":51898,\"start\":51897},{\"end\":52283,\"start\":52282},{\"end\":52293,\"start\":52292},{\"end\":52304,\"start\":52303},{\"end\":52535,\"start\":52534},{\"end\":52544,\"start\":52543},{\"end\":52782,\"start\":52781},{\"end\":52784,\"start\":52783},{\"end\":52794,\"start\":52793},{\"end\":52806,\"start\":52805},{\"end\":52820,\"start\":52819},{\"end\":52828,\"start\":52827},{\"end\":52841,\"start\":52840},{\"end\":53184,\"start\":53183},{\"end\":53356,\"start\":53355},{\"end\":53362,\"start\":53361},{\"end\":53630,\"start\":53629},{\"end\":53637,\"start\":53636},{\"end\":53650,\"start\":53649},{\"end\":53927,\"start\":53926},{\"end\":54199,\"start\":54198},{\"end\":54201,\"start\":54200},{\"end\":54208,\"start\":54207},{\"end\":54308,\"start\":54307},{\"end\":54320,\"start\":54319},{\"end\":54618,\"start\":54617},{\"end\":54628,\"start\":54627},{\"end\":54640,\"start\":54639},{\"end\":54651,\"start\":54650},{\"end\":54662,\"start\":54661},{\"end\":54909,\"start\":54908},{\"end\":54919,\"start\":54918},{\"end\":54929,\"start\":54928},{\"end\":54931,\"start\":54930},{\"end\":54940,\"start\":54939},{\"end\":54952,\"start\":54951},{\"end\":54961,\"start\":54960},{\"end\":55233,\"start\":55232},{\"end\":55248,\"start\":55247},{\"end\":55256,\"start\":55255},{\"end\":55262,\"start\":55261},{\"end\":55272,\"start\":55271},{\"end\":55284,\"start\":55283},{\"end\":55290,\"start\":55289},{\"end\":55299,\"start\":55298},{\"end\":55311,\"start\":55310},{\"end\":55321,\"start\":55320},{\"end\":55698,\"start\":55697},{\"end\":55709,\"start\":55708},{\"end\":55718,\"start\":55717},{\"end\":56074,\"start\":56073},{\"end\":56086,\"start\":56085},{\"end\":56323,\"start\":56322},{\"end\":56333,\"start\":56332},{\"end\":56679,\"start\":56678},{\"end\":56688,\"start\":56687},{\"end\":56690,\"start\":56689},{\"end\":56699,\"start\":56698},{\"end\":56701,\"start\":56700},{\"end\":56710,\"start\":56709},{\"end\":56719,\"start\":56718},{\"end\":56727,\"start\":56726},{\"end\":56738,\"start\":56737},{\"end\":56751,\"start\":56750},{\"end\":56765,\"start\":56764},{\"end\":57169,\"start\":57168},{\"end\":57180,\"start\":57179},{\"end\":57194,\"start\":57193},{\"end\":57204,\"start\":57203},{\"end\":57217,\"start\":57213},{\"end\":57545,\"start\":57544},{\"end\":57557,\"start\":57556},{\"end\":57568,\"start\":57567},{\"end\":57576,\"start\":57575},{\"end\":57583,\"start\":57582},{\"end\":57594,\"start\":57593},{\"end\":57606,\"start\":57605},{\"end\":57967,\"start\":57966},{\"end\":57974,\"start\":57973},{\"end\":57981,\"start\":57980},{\"end\":57988,\"start\":57987},{\"end\":58252,\"start\":58251},{\"end\":58265,\"start\":58264},{\"end\":58452,\"start\":58451},{\"end\":58465,\"start\":58464}]", "bib_author_last_name": "[{\"end\":49112,\"start\":49106},{\"end\":49250,\"start\":49248},{\"end\":49261,\"start\":49254},{\"end\":49497,\"start\":49486},{\"end\":49508,\"start\":49501},{\"end\":49519,\"start\":49512},{\"end\":49777,\"start\":49769},{\"end\":49789,\"start\":49781},{\"end\":49800,\"start\":49793},{\"end\":49811,\"start\":49806},{\"end\":49824,\"start\":49815},{\"end\":50045,\"start\":50039},{\"end\":50226,\"start\":50222},{\"end\":50235,\"start\":50230},{\"end\":50243,\"start\":50239},{\"end\":50520,\"start\":50509},{\"end\":50531,\"start\":50524},{\"end\":50538,\"start\":50535},{\"end\":50549,\"start\":50542},{\"end\":50561,\"start\":50553},{\"end\":50571,\"start\":50565},{\"end\":50582,\"start\":50575},{\"end\":50591,\"start\":50586},{\"end\":50602,\"start\":50595},{\"end\":50608,\"start\":50604},{\"end\":51084,\"start\":51079},{\"end\":51095,\"start\":51088},{\"end\":51105,\"start\":51099},{\"end\":51388,\"start\":51378},{\"end\":51400,\"start\":51394},{\"end\":51413,\"start\":51404},{\"end\":51421,\"start\":51417},{\"end\":51435,\"start\":51425},{\"end\":51583,\"start\":51575},{\"end\":51879,\"start\":51877},{\"end\":51888,\"start\":51883},{\"end\":51895,\"start\":51892},{\"end\":51902,\"start\":51899},{\"end\":52290,\"start\":52284},{\"end\":52301,\"start\":52294},{\"end\":52309,\"start\":52305},{\"end\":52541,\"start\":52536},{\"end\":52549,\"start\":52545},{\"end\":52791,\"start\":52785},{\"end\":52803,\"start\":52795},{\"end\":52817,\"start\":52807},{\"end\":52825,\"start\":52821},{\"end\":52838,\"start\":52829},{\"end\":52849,\"start\":52842},{\"end\":53195,\"start\":53185},{\"end\":53359,\"start\":53357},{\"end\":53368,\"start\":53363},{\"end\":53634,\"start\":53631},{\"end\":53647,\"start\":53638},{\"end\":53656,\"start\":53651},{\"end\":53935,\"start\":53928},{\"end\":54205,\"start\":54202},{\"end\":54213,\"start\":54209},{\"end\":54317,\"start\":54309},{\"end\":54329,\"start\":54321},{\"end\":54625,\"start\":54619},{\"end\":54637,\"start\":54629},{\"end\":54648,\"start\":54641},{\"end\":54659,\"start\":54652},{\"end\":54668,\"start\":54663},{\"end\":54916,\"start\":54910},{\"end\":54926,\"start\":54920},{\"end\":54937,\"start\":54932},{\"end\":54949,\"start\":54941},{\"end\":54958,\"start\":54953},{\"end\":54968,\"start\":54962},{\"end\":55245,\"start\":55234},{\"end\":55253,\"start\":55249},{\"end\":55259,\"start\":55257},{\"end\":55269,\"start\":55263},{\"end\":55281,\"start\":55273},{\"end\":55287,\"start\":55285},{\"end\":55296,\"start\":55291},{\"end\":55308,\"start\":55300},{\"end\":55318,\"start\":55312},{\"end\":55331,\"start\":55322},{\"end\":55706,\"start\":55699},{\"end\":55715,\"start\":55710},{\"end\":55724,\"start\":55719},{\"end\":56083,\"start\":56075},{\"end\":56096,\"start\":56087},{\"end\":56330,\"start\":56324},{\"end\":56341,\"start\":56334},{\"end\":56685,\"start\":56680},{\"end\":56696,\"start\":56691},{\"end\":56707,\"start\":56702},{\"end\":56716,\"start\":56711},{\"end\":56724,\"start\":56720},{\"end\":56735,\"start\":56728},{\"end\":56748,\"start\":56739},{\"end\":56762,\"start\":56752},{\"end\":56773,\"start\":56766},{\"end\":57177,\"start\":57170},{\"end\":57191,\"start\":57181},{\"end\":57201,\"start\":57195},{\"end\":57211,\"start\":57205},{\"end\":57226,\"start\":57218},{\"end\":57554,\"start\":57546},{\"end\":57565,\"start\":57558},{\"end\":57573,\"start\":57569},{\"end\":57580,\"start\":57577},{\"end\":57591,\"start\":57584},{\"end\":57603,\"start\":57595},{\"end\":57613,\"start\":57607},{\"end\":57971,\"start\":57968},{\"end\":57978,\"start\":57975},{\"end\":57985,\"start\":57982},{\"end\":57992,\"start\":57989},{\"end\":58262,\"start\":58253},{\"end\":58275,\"start\":58266},{\"end\":58462,\"start\":58453},{\"end\":58475,\"start\":58466}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":49206,\"start\":49030},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11536917},\"end\":49449,\"start\":49208},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4375646},\"end\":49721,\"start\":49451},{\"attributes\":{\"doi\":\"arXiv:1801.04062\",\"id\":\"b3\"},\"end\":50007,\"start\":49723},{\"attributes\":{\"id\":\"b4\"},\"end\":50144,\"start\":50009},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":49542008},\"end\":50445,\"start\":50146},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12552176},\"end\":51000,\"start\":50447},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2255738},\"end\":51346,\"start\":51002},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4110009},\"end\":51559,\"start\":51348},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206770307},\"end\":51827,\"start\":51561},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":52234,\"start\":51829},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b11\"},\"end\":52461,\"start\":52236},{\"attributes\":{\"doi\":\"arXiv:1707.01219\",\"id\":\"b12\"},\"end\":52714,\"start\":52463},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11514441},\"end\":53126,\"start\":52716},{\"attributes\":{\"id\":\"b14\"},\"end\":53324,\"start\":53128},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4853851},\"end\":53552,\"start\":53326},{\"attributes\":{\"doi\":\"arXiv:1511.02580\",\"id\":\"b16\"},\"end\":53837,\"start\":53554},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18127428},\"end\":54165,\"start\":53839},{\"attributes\":{\"id\":\"b18\"},\"end\":54278,\"start\":54167},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7910040},\"end\":54568,\"start\":54280},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5855183},\"end\":54906,\"start\":54570},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b21\"},\"end\":55179,\"start\":54908},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2930547},\"end\":55611,\"start\":55181},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14748306},\"end\":56003,\"start\":55613},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b24\"},\"end\":56262,\"start\":56005},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206592152},\"end\":56676,\"start\":56264},{\"attributes\":{\"id\":\"b26\"},\"end\":57050,\"start\":56678},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":17804904},\"end\":57518,\"start\":57052},{\"attributes\":{\"doi\":\"CNS-TR-2010-001\",\"id\":\"b28\"},\"end\":57865,\"start\":57520},{\"attributes\":{\"id\":\"b29\"},\"end\":58130,\"start\":57867},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":829159},\"end\":58449,\"start\":58132},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b31\"},\"end\":58624,\"start\":58451}]", "bib_title": "[{\"end\":49244,\"start\":49208},{\"end\":49482,\"start\":49451},{\"end\":50218,\"start\":50146},{\"end\":50505,\"start\":50447},{\"end\":51075,\"start\":51002},{\"end\":51374,\"start\":51348},{\"end\":51571,\"start\":51561},{\"end\":51873,\"start\":51829},{\"end\":52779,\"start\":52716},{\"end\":53353,\"start\":53326},{\"end\":53924,\"start\":53839},{\"end\":54305,\"start\":54280},{\"end\":54615,\"start\":54570},{\"end\":55230,\"start\":55181},{\"end\":55695,\"start\":55613},{\"end\":56320,\"start\":56264},{\"end\":57166,\"start\":57052},{\"end\":58249,\"start\":58132}]", "bib_author": "[{\"end\":49114,\"start\":49100},{\"end\":49252,\"start\":49246},{\"end\":49263,\"start\":49252},{\"end\":49499,\"start\":49484},{\"end\":49510,\"start\":49499},{\"end\":49521,\"start\":49510},{\"end\":49779,\"start\":49767},{\"end\":49791,\"start\":49779},{\"end\":49802,\"start\":49791},{\"end\":49813,\"start\":49802},{\"end\":49826,\"start\":49813},{\"end\":50047,\"start\":50035},{\"end\":50228,\"start\":50220},{\"end\":50237,\"start\":50228},{\"end\":50245,\"start\":50237},{\"end\":50522,\"start\":50507},{\"end\":50533,\"start\":50522},{\"end\":50540,\"start\":50533},{\"end\":50551,\"start\":50540},{\"end\":50563,\"start\":50551},{\"end\":50573,\"start\":50563},{\"end\":50584,\"start\":50573},{\"end\":50593,\"start\":50584},{\"end\":50604,\"start\":50593},{\"end\":50610,\"start\":50604},{\"end\":51086,\"start\":51077},{\"end\":51097,\"start\":51086},{\"end\":51107,\"start\":51097},{\"end\":51390,\"start\":51376},{\"end\":51402,\"start\":51390},{\"end\":51415,\"start\":51402},{\"end\":51423,\"start\":51415},{\"end\":51437,\"start\":51423},{\"end\":51585,\"start\":51573},{\"end\":51881,\"start\":51875},{\"end\":51890,\"start\":51881},{\"end\":51897,\"start\":51890},{\"end\":51904,\"start\":51897},{\"end\":52292,\"start\":52282},{\"end\":52303,\"start\":52292},{\"end\":52311,\"start\":52303},{\"end\":52543,\"start\":52534},{\"end\":52551,\"start\":52543},{\"end\":52793,\"start\":52781},{\"end\":52805,\"start\":52793},{\"end\":52819,\"start\":52805},{\"end\":52827,\"start\":52819},{\"end\":52840,\"start\":52827},{\"end\":52851,\"start\":52840},{\"end\":53197,\"start\":53183},{\"end\":53361,\"start\":53355},{\"end\":53370,\"start\":53361},{\"end\":53636,\"start\":53629},{\"end\":53649,\"start\":53636},{\"end\":53658,\"start\":53649},{\"end\":53937,\"start\":53926},{\"end\":54207,\"start\":54198},{\"end\":54215,\"start\":54207},{\"end\":54319,\"start\":54307},{\"end\":54331,\"start\":54319},{\"end\":54627,\"start\":54617},{\"end\":54639,\"start\":54627},{\"end\":54650,\"start\":54639},{\"end\":54661,\"start\":54650},{\"end\":54670,\"start\":54661},{\"end\":54918,\"start\":54908},{\"end\":54928,\"start\":54918},{\"end\":54939,\"start\":54928},{\"end\":54951,\"start\":54939},{\"end\":54960,\"start\":54951},{\"end\":54970,\"start\":54960},{\"end\":55247,\"start\":55232},{\"end\":55255,\"start\":55247},{\"end\":55261,\"start\":55255},{\"end\":55271,\"start\":55261},{\"end\":55283,\"start\":55271},{\"end\":55289,\"start\":55283},{\"end\":55298,\"start\":55289},{\"end\":55310,\"start\":55298},{\"end\":55320,\"start\":55310},{\"end\":55333,\"start\":55320},{\"end\":55708,\"start\":55697},{\"end\":55717,\"start\":55708},{\"end\":55726,\"start\":55717},{\"end\":56085,\"start\":56073},{\"end\":56098,\"start\":56085},{\"end\":56332,\"start\":56322},{\"end\":56343,\"start\":56332},{\"end\":56687,\"start\":56678},{\"end\":56698,\"start\":56687},{\"end\":56709,\"start\":56698},{\"end\":56718,\"start\":56709},{\"end\":56726,\"start\":56718},{\"end\":56737,\"start\":56726},{\"end\":56750,\"start\":56737},{\"end\":56764,\"start\":56750},{\"end\":56775,\"start\":56764},{\"end\":57179,\"start\":57168},{\"end\":57193,\"start\":57179},{\"end\":57203,\"start\":57193},{\"end\":57213,\"start\":57203},{\"end\":57228,\"start\":57213},{\"end\":57556,\"start\":57544},{\"end\":57567,\"start\":57556},{\"end\":57575,\"start\":57567},{\"end\":57582,\"start\":57575},{\"end\":57593,\"start\":57582},{\"end\":57605,\"start\":57593},{\"end\":57615,\"start\":57605},{\"end\":57973,\"start\":57966},{\"end\":57980,\"start\":57973},{\"end\":57987,\"start\":57980},{\"end\":57994,\"start\":57987},{\"end\":58264,\"start\":58251},{\"end\":58277,\"start\":58264},{\"end\":58464,\"start\":58451},{\"end\":58477,\"start\":58464}]", "bib_venue": "[{\"end\":49098,\"start\":49030},{\"end\":49312,\"start\":49263},{\"end\":49559,\"start\":49521},{\"end\":49765,\"start\":49723},{\"end\":50033,\"start\":50009},{\"end\":50284,\"start\":50245},{\"end\":50677,\"start\":50610},{\"end\":51156,\"start\":51107},{\"end\":51444,\"start\":51437},{\"end\":51652,\"start\":51585},{\"end\":51981,\"start\":51904},{\"end\":52280,\"start\":52236},{\"end\":52532,\"start\":52463},{\"end\":52900,\"start\":52851},{\"end\":53181,\"start\":53128},{\"end\":53432,\"start\":53370},{\"end\":53627,\"start\":53554},{\"end\":53986,\"start\":53937},{\"end\":54196,\"start\":54167},{\"end\":54370,\"start\":54331},{\"end\":54719,\"start\":54670},{\"end\":55018,\"start\":54985},{\"end\":55373,\"start\":55333},{\"end\":55782,\"start\":55726},{\"end\":56071,\"start\":56005},{\"end\":56420,\"start\":56343},{\"end\":56851,\"start\":56775},{\"end\":57264,\"start\":57228},{\"end\":57542,\"start\":57520},{\"end\":57964,\"start\":57867},{\"end\":58281,\"start\":58277},{\"end\":58515,\"start\":58493},{\"end\":50731,\"start\":50679},{\"end\":51706,\"start\":51654},{\"end\":52045,\"start\":51983},{\"end\":56484,\"start\":56422}]"}}}, "year": 2023, "month": 12, "day": 17}