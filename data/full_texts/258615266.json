{"id": 258615266, "updated": "2023-10-05 01:00:37.916", "metadata": {"title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "authors": "[{\"first\":\"Wenliang\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Junnan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Dongxu\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Anthony\",\"last\":\"Tiong\",\"middle\":[\"Meng\",\"Huat\"]},{\"first\":\"Junqi\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Weisheng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Boyang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Pascale\",\"last\":\"Fung\",\"middle\":[]},{\"first\":\"Steven\",\"last\":\"Hoi\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.06500", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-06500", "doi": "10.48550/arxiv.2305.06500"}}, "content": {"source": {"pdf_hash": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.06500v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a2d50cc2279641e0433829f5161571087234e71b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8bd6a2a89503be083176f2cc26fabedb79238cbd.txt", "contents": "\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\n\n\nWenliang Dai \nSalesforce Research\n\n\nHong Kong University of Science and Technology\n\n\nJunnan Li junnan.li@salesforce.com \nDongxu Li \nSalesforce Research\n\n\nAnthony Meng \nSalesforce Research\n\n\nHuat Tiong \nSalesforce Research\n\n\nNanyang Technological University\nSingapore\n\nJunqi Zhao \nNanyang Technological University\nSingapore\n\nWeisheng Wang \nNanyang Technological University\nSingapore\n\nBoyang Li \nNanyang Technological University\nSingapore\n\nPascale Fung \nHong Kong University of Science and Technology\n\n\nSteven Hoi shoi@salesforce.com \nSalesforce Research\n\n\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\n\u2020 Equal contribution Corresponding authors:\nLarge-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, Instruct-BLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced. * Work done during internship at Salesforce.Preprint. Under review.\n\nIntroduction\n\nA longstanding aspiration of Artificial Intelligence (AI) research is to build a single model that can solve arbitrary tasks specified by the user. In natural language processing (NLP), instruction tuning [46,7] proves to be a promising approach toward that goal. By finetuning a large language model (LLM) on a wide range of tasks described by natural language instructions, instruction tuning enables the model to follow arbitrary instructions. Recently, instruction-tuned LLMs have also been leveraged for vision-language tasks. For example, BLIP-2 [20] effectively adapts frozen instructiontuned LLMs to understand visual inputs and exhibits preliminary capabilities to follow instructions in image-to-text generation.\n\nCompared to NLP tasks, vision-language tasks are more diverse in nature due to the additional visual inputs from various domains. This poses a greater challenge to a unified model that is supposed to generalize to diverse vision-language tasks, many unseen during training. Most previous work can be grouped into two approaches. The first approach, multitask learning [6,27], formulates various vision-language tasks into the same input-output format. However, we empirically find multitask learning without instructions (Table 4) does not generalize well to unseen datasets and tasks. The\n\nWhat could have happened based on the current scene?\n\nBased on the current scene in the image, it is possible that a hurricane or severe weather event caused significant damage to the buildings and infrastructure in the area. The person walking through the debris-covered area suggests that they are inspecting the damage caused by the disaster. The presence of palm trees indicates that the location may be in a tropical or sub-tropical region, which can be more susceptible to hurricanes and other natural disasters.\n\nIntroduce me this painting in detail.\n\nThe painting depicts a young girl with a pearl earring, also known as \"The Girl with the Pearl Earring\" or \"Girl with a Pearl Earring\". She is wearing a blue dress and has a pearl earring on her left ear. Her hair is pulled back into a bun, and she is looking directly at the viewer. This painting is a famous work of art by Dutch painter Johannes Vermeer, created in 1665. It is considered one of his masterpieces and is widely recognized for its intricate details and realistic portrayal of the young girl's facial expression.\n\nCan yon tell me about this image in detail?\n\nThe image depicts a collection of various vegetables including carrots, cucumbers, tomatoes, and nuts arranged on a stone surface. There are several jars filled with different types of ingredients, such as peanuts, cashews, sunflower seeds, and pumpkin seeds. These ingredients are likely to be part of a healthy meal or snack.\n\nCan you give me an example of such meal?\n\nAn example of a healthy meal or snack that incorporates some of the ingredients in the image could be a salad made with mixed greens, carrots, cucumbers, tomatoes, and nuts. The salad can be dressed with a homemade vinaigrette made with olive oil, lemon juice, salt, and pepper. Additionally, the salad can be topped with pumpkin seeds or sunflower seeds for added crunch and nutritional value.\n\nSounds good! Show me steps of making this salad.\n\nSure! Here are the steps to make this healthy salad: 1. Wash and chop the mixed greens, carrots, cucumbers, and tomatoes into bite-sized pieces. 2. In a small bowl, whisk together olive oil, lemon juice, salt, and pepper to create a homemade vinaigrette dressing. 3. Place the chopped vegetables in a large bowl and toss them with the vinaigrette dressing. 4. Sprinkle pumpkin seeds or sunflower seeds on top of the salad for added crunch and nutritional value. 5. Serve the salad immediately or store it in the refrigerator until ready to enjoy.\n\nCan you describe this image in detail? The image depicts a person standing in front of an open doorway, looking out into space. They are wearing a jacket and appears to be contemplating the vastness of the universe beyond the doorway. In the background, there are multiple stars visible, creating a stunning celestial scene. The person's gaze is directed towards the center of the image, where a bright star can be seen shining brightly. There are also smaller stars scattered throughout the scene, adding to the overall cosmic atmosphere.\n\nWhat if they jump over the doorway?\n\nIf the person were to jump over the doorway, they would be leaving the confines of their current location and venturing into the vastness of space. This could be a metaphorical representation of stepping out of one's comfort zone or embarking on a new adventure. However, it is important to note that such an action would be extremely dangerous and potentially lifethreatening due to the harsh conditions of space.\n\nWhat makes this dog special?\n\nThe dog in the image is wearing futuristic armor and has a glowing green eye, giving it a unique and futuristic appearance. This combination of elements makes the dog stand out from other dogs and adds an element of fantasy or sci-fi to the image. Additionally, the dog's posture and facial expression convey a sense of confidence and determination, further emphasizing its unique features. second approach [20,4] extends a pre-trained LLM with additional visual components, and trains the visual components with image caption data. Nevertheless, such data are too limited to allow broad generalization to vision-language tasks that require more than visual descriptions.\n\nTo address the aforementioned challenges, this paper presents InstructBLIP, a vision-language instruction tuning framework that enables general-purpose models to solve a wide range of visuallanguage tasks through a unified natural language interface. InstructBLIP uses a diverse set of instruction data to train a multimodal LLM. Specifically, we initialize training with a pre-trained BLIP-2 model consisting of an image encoder, an LLM, and a Query Transformer (Q-Former) to bridge the two. During instruction tuning, we finetune the Q-Former while keeping the image encoder and LLM frozen. Our paper makes the following key contributions:\n\n\u2022 We perform a comprehensive and systematic study on vision-language instruction tuning. We transform 26 datasets into the instruction tuning format and group them into 11 task categories. We use 13 held-in datasets for instruction tuning and 13 held-out datasets for zero-shot evaluation. Moreover, we withhold four entire task categories for zero-shot evaluation at the task level. Exhaustive quantitative and qualitative results demonstrate the effectiveness of InstructBLIP on vision-language zero-shot generalization.\n\n\u2022 We propose instruction-aware visual feature extraction, a novel mechanism that enables flexible and informative feature extraction according to the given instructions. Specifically, the textual instruction is given not only to the frozen LLM, but also to the Q-Former, so that it can extract instruction-aware visual features from the frozen image encoder. Also, we propose a balanced sampling strategy to synchronize learning progress across datasets.\n\n\u2022 We evaluate and open-source a suite of InstructBLIP models using two families of LLMs: 1) FlanT5 [7], an encoder-decoder LLM finetuned from T5 [34]; 2) Vicuna [2], a decoder-only LLM finetuned from LLaMA [41]. The InstructBLIP models achieve state-of-the-art zero-shot performance on a wide range of vision-language tasks. Furthermore, InstructBLIP models lead to state-of-the-art finetuning performance when used as the model initialization on individual downstream tasks.\n\n\nVision-Language Instruction Tuning\n\nInstructBLIP aims to address the unique challenges in vision-language instruction tuning and provide a systematic study on the models' improved generalization ability to unseen data and tasks. In this section, we first introduce the construction of instruction-tuning data, followed by the training and evaluation protocols. Next, we delineate two techniques to improve instruction-tuning performance from the model and data perspectives, respectively. Lastly, we present the implementation details.\n\n\nTasks and Datasets\n\nTo ensure the diversity of instruction tuning data while considering their accessibility, we gather comprehensive set of publicly available vision-language datasets, and transform them into the instruction tuning format. As shown in Figure 2, the final collection covers 11 task categories and 26 datasets, including image captioning [23,3,51], image captioning with reading comprehension [38], visual reasoning [16,24,29], image question answering [11,12], knowledge-grounded image question answering [30,36,28], image question answering with reading comprehension [31,39], image question generation (adapted from the QA datasets), video question answering [47,49], visual conversational question answering [8], image classification [18], and LLaVA-Instruct-150K [25]. We include detailed descriptions and statistics of each dataset in Appendix C.\n\nFor every task, we meticulously craft 10 to 15 distinct instruction templates in natural language. These templates serve as the foundation for constructing instruction tuning data, which articulates the task and the objective. For public datasets inherently favoring short responses, we use terms such as short and briefly into some of their corresponding instruction templates to reduce the risk of the model overfitting to always generating short outputs. For the LLaVA-Instruct-150K dataset, we do not incorporate additional instruction templates since it is naturally structured in the instruction format. The full list of instruction templates can be found in Appendix D.  Figure 2: Tasks and their corresponding datasets used for vision-language instruction tuning. The held-in datasets are indicated by yellow and the held-out datasets by white.\n\n\nTraining and Evaluation Protocols\n\nTo ensure sufficient data and tasks for training and zero-shot evaluation, we divide the 26 datasets into 13 held-in datasets and 13 held-out datasets, indicated by yellow and white respectively in Figure 2. We employ the training sets of the held-in datasets for instruction tuning and their validation or test sets for held-in evaluation.\n\nFor held-out evaluation, our aim is to understand how instruction tuning improves the model's zero-shot performance on unseen data. We define two types of held-out data: 1) datasets not exposed to the model during training, but whose tasks are present in the held-in cluster; 2) datasets and their associated tasks that remain entirely unseen during training. Addressing the first type of held-out evaluation is nontrivial due to the data distribution shift between held-in and held-out datasets. For the second type, we hold out several tasks completely, including visual reasoning, video question answering, visual conversational QA, and image classification.\n\nTo avoid data contamination, datasets are selected carefully so that no evaluation data appear in the held-in training cluster across different datasets. During instruction tuning, we mix all the held-in training sets and sample instruction templates uniformly for each dataset. The models are trained with the standard language modeling loss to directly generate the response given the instruction. Furthermore, for datasets that involve scene texts, we add OCR tokens in the instruction as supplementary information.\n\n\nInstruction-aware Visual Feature Extraction\n\nExisting zero-shot image-to-text generation methods, including BLIP-2, take an instruction-agnostic approach when extracting visual features. That results in a set of static visual representations being fed into the LLM, regardless of the task. In contrast, an instruction-aware vision model can adapt to the task instruction and produce visual representations most conducive to the task at hand. This is clearly advantageous if we expect the task instructions to vary considerably for the same input image.\n\nWe show the architecture of InstructBLIP in Figure 3. Similarly to BLIP-2 [20], InstructBLIP utilizes a Query Transformer, or Q-Former, to extract visual features from a frozen image encoder. The input to the Q-Former contains a set of K learnable query embeddings, which interact with the image encoder's output through cross attention. The output of the Q-Former consists of K encoded visual vectors, one per query embedding, which then go through a linear projection and are fed to the frozen LLM. As in BLIP-2, the Q-Former is pretrained in two stages using image-caption data before instruction tuning. The first stage pretrains the Q-Former with the frozen image encoder for vision-language representation learning. The second stage adapts the output of Q-Former as soft visual prompts for text generation with a frozen LLM . After pretraining, we finetune the Q-Former with instruction tuning, where the LLM receives as input the visual encodings from the Q-Former and the task instruction.\n\nExtending BLIP-2, InstructBLIP proposes an instruction-aware Q-former module, which takes in the instruction text tokens as additional input. The instruction interacts with the query embeddings through self-attention layers of the Q-Former, and encourages the extraction of task-relevant image features. As a result, the LLM receives visual information conducive to instruction following. We demonstrate empirically ( Table 2) that instruction-aware visual feature extraction provides substantial performance improvements for both held-in and held-out evaluations.\n\n\nBalancing Training Datasets\n\nDue to the large number of training datasets and the significant differences in the size of each dataset, mixing them uniformly could cause the model to overfit smaller datasets and underfit larger datasets.\n\nTo mitigate the problem, we propose to sample datasets with probabilities proportional to the square root of their sizes, or the numbers of training samples. Generally, given D datasets with sizes {S 1 , S 2 , . . . , S D }, the probability of a data sample being selected from a dataset d during training\nis p d = \u221a S d D i=1\n\u221a Si . On top of this formula, we make manual adjustments to the weights of certain datasets to improve optimization. This is warranted by inherent differences in the datasets and tasks that require varying levels of training intensity despite similar sizes. To be specific, we lower the weight of A-OKVQA, which features multiple-choice questions, and increase the weight of OKVQA, which requires open-ended text generation. In Table 2, we show that the balanced dataset sampling strategy improves overall performance for both held-in evaluation and held-out generalization.\n\n\nInference Methods\n\nDuring inference time, we adopt two slightly different generation approaches for evaluation on different datasets. For the majority of datasets, such as image captioning and open-ended VQA, the instruction-tuned model is directly prompted to generate responses, which are subsequently compared to the ground truth to calculate metrics. On the other hand, for classification and multi-choice VQA tasks, we employ a vocabulary ranking method following previous works [46,22,21]. Specifically, we still prompt the model to generate answers, but restrict its vocabulary to a list of candidates. Then, we calculate log-likelihood for each candidate and select the one with the highest value as the final prediction. This ranking method is applied to ScienceQA, IconQA, A-OKVQA (multiple-choice), HatefulMemes, Visual Dialog, MSVD, and MSRVTT datasets. Furthermore, for binary classification, we expand the positive and negative labels into a slightly broader set of verbalizers to exploit word frequencies in natural text (e.g., yes and true for the positive class; no and false for the negative class).\n\nFor the video question-answering task, we utilize four uniformly-sampled frames per video. Each frame is processed by the image encoder and Q-Former individually, and the extracted visual features are concatenated before being fed into the LLM.\n\n\nImplementation Details\n\nArchitecture. Thanks to the flexibility enabled by the modular architectural design of BLIP-2, we can quickly adapt the model to a wide range of LLMs. In our experiments, we adopt four variations of BLIP-2 with the same image encoder (ViT-g/14 [10]) but different frozen LLMs, including FlanT5-XL (3B), FlanT5-XXL (11B), Vicuna-7B and Vicuna-13B. FlanT5 [7] is an instruction-tuned model based on the encoder-decoder Transformer T5 [34]. Vicuna [2], on the other hand, is a recently released decoder-only Transformer instruction-tuned from LLaMA [41]. During vision-language instruction tuning, we initialize the model from pre-trained BLIP-2 checkpoints, and only finetune the parameters of Q-Former while keeping both the image encoder and the LLM frozen. Since the original BLIP-2 models do not include checkpoints for Vicuna, we perform pre-training with Vicuna using the same procedure as BLIP-2.\n\nTraining and Hyper-parameters. We use the LAVIS library [19] for implementation, training, and evaluation. All models are instruction-tuned with a maximum of 60K steps and we validate model's performance every 3K steps. For each model, a single optimal checkpoint is selected and used for evaluations on all datasets. We employ a batch size of 192, 128, and 64 for the 3B, 7B, and 11/13B models, respectively. The AdamW [26] optimizer is used, with \u03b2 1 = 0.9, \u03b2 2 = 0.999, and a weight decay of 0.05. Additionally, we apply a linear warmup of the learning rate during the initial 1,000 steps, increasing from 10 \u22128 to 10 \u22125 , followed by a cosine decay with a minimum learning rate of 0. All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.\n\n\nExperimental Results and Analysis\n\n\nZero-shot Evaluation\n\nWe first evaluate InstructBLIP models on the set of 13 held-out datasets with instructions provided in Appendix E. We compare InstructBLIP with the previous SOTA models BLIP-2 and Flamingo. As demonstrated in Table 1  For the Visual Dialog dataset, we choose to report the Mean Reciprocal Rank (MRR) over the Normalized Discounted Cumulative Gain (NDCG) metric. This is because NDCG favors generic and uncertain answers while MRR prefers certain responses [32], making MRR better aligned with the zero-shot evaluation scenario.\n\n\nAblation Study on Instruction Tuning Techniques\n\nTo investigate the impact of the instruction-aware visual feature extraction (Section 2.3) and the balanced dataset sampling strategy (Section 2.4), we conduct ablation studies during the instruction tuning process. As illustrated in Table 2, the removal of instruction awareness in visual features downgrades performance significantly across all datasets. The performance drop is more severe in datasets that involve spatial visual reasoning (e.g., ScienceQA) or temporal visual reasoning (e.g., iVQA), where the instruction input to the Q-Former can guide visual features to attend to informative image regions. The removal of the data balancing strategy causes unstable and uneven training, as different datasets achieve peak performance at drastically different training steps. The lack of synchronized progress over multiple datasets harms the overall performance.\n\n\nQualitative Evaluation\n\nBesides the systematic evaluation on public benchmarks, we further qualitatively examine Instruct-BLIP with more diverse images and instructions. As illustrated in Figure 1, InstructBLIP demonstrates its capacity for complex visual reasoning. For example, it can reasonably infer from the visual scene what could have happened and deduce the type of disaster from the location of the scene, which it extrapolates based on visual evidence like the palm trees. Moreover, InstructBLIP is capable of connecting visual input with embedded textual knowledge and generate informative responses, such as intruducing a famous painting. Furthermore, in descriptions of the overall atmosphere, InstructBLIP exhibits the ability to comprehend metaphorical implications of the visual imagery. Finally, we show that InstructBLIP can engage in multi-turn conversations, effectively considering the dialog history when making new responses.\n\nIn Appendix B, we qualitatively compare InstructBLIP with concurrent multimodal models (GPT-4 [33], LLaVA [25], MiniGPT-4 [52]). Although all models are capable of generating long-form responses, InstructBLIP's outputs generally contains more proper visual details and exhibits logically coherent reasoning steps. Importantly, we argue that long-form responses are not always preferable. For example, in Figure 2 of the Appendix, InstructBLIP directly addresses the user's intent by adaptively adjusting the response length, while LLaVA and MiniGPT-4 generate long and less relevant sentences. These advantages of InstructBLIP are a result of the diverse instruction tuning data and an effective architectural design.  A direct analogue to instruction tuning is multitask learning, a widely used method that involves the simultaneous training of multiple datasets with the goal of improving the performance of each individual dataset. To investigate whether the improvement in zero-shot generalization observed in instruction tuning is mainly from the formatting of instructions or merely from multitasking, we conduct a comparative analysis between these two approaches under identical training settings.\n\n\nInstruction Tuning vs. Multitask\n\nFollowing [46], we consider two multitask training approaches. In the first approach, the model is trained using the vanilla input-output format of the training datasets without instructions. During evaluation, instructions are still provided to the model, indicating the specific task to be performed. However, an exception is made for image captioning, as the model achieves better scores when only receiving the image as input. For the second approach, we take a step towards instruction tuning by prepending a [Task:Dataset] identifier to the text input during training. For example, we prepend [Visual question answering:VQAv2] for the VQAv2 dataset. During evaluation, we explore both instructions and this identifier. Particularly, for the identifier of held-out datasets, we only use the task name since the model never sees the dataset name.\n\nThe results are shown in Figure 4, including BLIP-2 zero-shot, multitask training, and instruction tuning. All of these models are based on the BLIP-2 FlanT5 XL backbone and adhere to the identical training configurations delineated in Section 2. Overall, we can conclude two insights from the results. Firstly, instruction tuning and multitask learning exhibit similar performance on the held-in datasets. This suggests that the model can fit these two different input patterns comparably well, as long as it has been trained with such data. On the other hand, instruction tuning yields a significant improvement over multitask learning on unseen held-out datasets, whereas multitask learning still performs on par with the original BLIP-2. This indicates that instruction tuning is the key to enhance the model's zero-shot generalization ability.\n\n\nFinetuning InstructBLIP on Downstream Tasks\n\nWe further finetune the InstructBLIP models to investigate its performance on learning a specific dataset. Compared to most previous methods (e.g., Flamingo, BLIP-2) which increase the input image resolution and finetune the visual encoder on downstream tasks, InstructBLIP maintains the same image resolution (224\u00d7224) during instruction tuning and keeps the visual encoder frozen during finetuning. This significantly reduces the number of trainable parameters from 1.2B to 188M, thus greatly improves finetuning efficiency.  \n\n\nRelated Work\n\nInstruction tuning aims to teach language models to follow natural language instructions, which has been shown to improve their generalization performance to unseen tasks. Some methods collect instruction tuning data by converting existing NLP datasets into instruction format using templates [46,7,35,45]. Others use LLMs (e.g., GPT-3 [5]) to generate instruction data [2,13,44,40] with improved diversity.\n\nInstruction-tuned LLMs have been adapted for vision-to-language generation tasks by injecting visual information to the LLMs. BLIP-2 [20] uses frozen FlanT5 models, and trains a Q-Former to extract visual features as input to the LLMs. MiniGPT-4 [52] uses the same pretrained visual encoder and Q-Former from BLIP-2, but uses Vicuna [2] as the LLM and performs training using ChatGPT [1]-generated image captions longer than the BLIP-2 training data. LLaVA [25] directly projects the output of a visual encoder as input to a LLaMA/Vinuca LLM, and finetunes the LLM on vision-language conversational data generated by GPT-4 [33]. mPLUG-owl [50] performs low-rank adaption [14] to a LLaMA [41] model using both text instruction data and vision-language instruction data from LLaVA. A separate work is MultiInstruct [48], which performs vision-language instruction tuning without a pretrained LLM, leading to less competitive performance.\n\nCompared to existing methods, InstructBLIP uses a much wider range of vision-language instruction data, covering both template-based converted data and LLM-generated data. Architecture wise, InstructBLIP proposes an instruction-aware visual feature extraction mechanism. Furthermore, our paper provides a comprehensive analysis on various aspects of vision-language instruction tuning, validating its advantages on generalizing to unseen tasks.\n\n\nConclusion\n\nIn this paper, we present InstructBLIP, a simple yet novel instruction tuning framework towards generalized vision-language models. We perform a comprehensive study on vision-language instruction tuning and demonstrate the capability of InstructBLIP models to generalize to a wide range of unseen tasks with state-of-the-art performance. Qualitative examples also exhibit InstructBLIP's various capabilities on instruction following, such as complex visual reasoning, knowledge-grounded image description, and multi-turn conversations. Furthermore, we show that InstructBLIP can serve as an enhanced model initialization for downstream task finetuning, achieving state-of-the-art results. We hope that InstructBLIP can spur new research in general-purpose multimodal AI and its applications.\n\n\nA Broader Impact\n\nInstructBLIP uses off-the-shelf frozen LLMs. Therefore it inherits some of the shortcomings from the original LLMs, such as hallucinating ungrounded text or generating outputs with bias. We mitigate such shortcomings by improving the model's grounding on the vision and instruction input, and performing vision-language instruction tuning on a diverse set of high-quality datasets. Nevertheless, we do not recommend applying InstructBLIP models to any downstream applications without a prior assessment on safety and fairness specific to that application.\n\n\nB More Case Studies\n[Input Image]\nWhat is unusual about this image?\n[Model Response] InstructBLIP\nThe unusual aspect of this image is that a man is ironing clothes on the back of a yellow taxi cab while it is driving down a bus city street. He is standing on top of the car's trunk, holding an iron and using it to press his clothing items. This situation is not only unconventional but also potentially hazardous, as the man could be at risk of falling off the vehicle or causing an accident due to his actions.\n\n\nLLaVA\n\nThe unusual aspect of this image is a man ironing clothes on the back of a minivan or van. This is not a typical place to perform this activity, as one would usually iron clothes in a more stationary and safe location, such as a home, using a regular ironing board. The scene depicted in the image is peculiar as it involves a makeshift ironing setup on a vehicle, which can be both unsafe and unconventional. Additionally, it is not clear how the man is able to maintain balance and stability while ironing clothes in such an unstable environment.\n\nMiniGPT-4\n\nThe image shows a man standing on the back of a yellow taxi, with a yellow shirt and black pants, and a blue backpack on his back. The taxi is driving on a city street with othr cars and taxis in the background.\n\n\nGPT-4\n\nThe unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi.\n\n[Input Instruction] The painting depicts a woman, commonly believed to be Mona Lisa, the famous artwork by Leonardo da Vinci. It is a portrait painting that showcases the woman's enigmatic smile and has become one of the most famous and iconic art pieces in the world. The original work is displayed in the Louvre Museum in Paris, and it is known for its intricate details, use of oil paint, and the artist's innovative techniques that contributed to its enduring appeal and mystery.\n\nThe painting is the Mona Lisa, also known as the Mona Lisa, by Leonardo da Vinci. It is one of the most famous paintings in the world and is known for its enigmatic smile and the mysterious identity of the sitter. The painting is housed in the Louvre Museum in Paris, France.\n\nLeonardo da Vinci.\n\n[Input Instruction] Figure 6: In this example, we illustrate that long-form responses are not always preferable. Instruct-BLIP can directly address the user's intent by adaptively adjusting the response length, while other models tend to generate lengthy paragraphs with less-relevant sentences. The response from LLaVA is taken from the paper, and for MiniGPT-4, we utilize its official demo.\n\n[Input Image]\n\n[Input Instruction] Introduce me this painting in detail.\n\n\n[Model Response]\n\nInstructBLIP LLaVA MiniGPT-4\n\nThe painting features a woman with a headscarf, possibly of Dutch origin, dressed in blue and white clothing. She is wearing a large blue hat and has a red mouth, which adds a vibrant touch to the portrait. The woman is looking directly at the viewer, creating a sense of connection. The background of the painting is predominantly blue, further emphasizing the color theme of the piece. The overall composition and attire of the woman suggest that the painting might be inspired by the style of Rembrandt.\n\nThe painting depicts a young woman with a pearl earring, wearing a blue dress and a white apron. She is shown with a serious expression, looking directly at the viewer. The background is a dark, neutral tone, with no visible objects or elements. The overall mood of the painting is somber and contemplative. The painting is well-executed, with a high level of detail and realism in the depiction of the woman's features and clothing. The use of shading and lighting creates a sense of depth and dimension, making the woman appear three-dimensional. The artist has used a limited color palette, with the woman's dress and apron providing the only sources of color in the painting. The painting is well-composed, with the woman's head and shoulders positioned in the center of the canvas, creating a sense of balance and symmetry. The use of negative space around the woman's figure adds to the overall somber mood of the painting. Overall, this is a well-executed and thought-provoking painting that effectively conveys a sense of contemplation and introspection.\n\nThe painting depicts a young girl with a pearl earring, also known as \"The Girl with the Pearl Earring\" or \"Girl with a Pearl Earring\". She is wearing a blue dress and has a pearl earring on her left ear. Her hair is pulled back into a bun, and she is looking directly at the viewer. This painting is a famous work of art by Dutch painter Johannes Vermeer, created in 1665. It is considered one of his masterpieces and is widely recognized for its intricate details and realistic portrayal of the young girl's facial expression.  TextVQA [39] \u2713 (val) TextVQA requires models to comprehend visual text to answer questions.\n\n\nC Instruction Tuning Datasets\n\nHatefulMemes [18] \u2713 (val) A binary classification dataset to justify whether a meme contains hateful content.\n\nLLaVA-Instruct-150K [25] \u2717 An instruction tuning dataset which has three parts: detailed caption (23K), reasoning (77K), conversation (58K).\n\n\nMSVD-QA [47]\n\n\u2713 (test) We use the test set (13K video QA pairs) of MSVD-QA for held-out testing.\n\nMSRVTT-QA [47] \u2713 (test) MSRVTT-QA has more complex scenes than MSVD, with 72K video QA pairs as the test set.\n\niVQA [49] \u2713 (test) iVQA is a video QA dataset with mitigated language biases. It has 6K/2K/2K samples for train/val/test. <Image>What is the answer to the following question? \"{Question}\" <Image>The question \"{Question}\" can be answered using the image. A short answer is VQG <Image>Given the image, generate a question whose answer is: {Answer}. Question: <Image>Based on the image, provide a question with the answer: {Answer}. Question: <Image>Given the visual representation, create a question for which the answer is \"{Answer}\". <Image>From the image provided, craft a question that leads to the reply: {Answer}. Question: <Image>Considering the picture, come up with a question where the answer is: {Answer}. <Image>Taking the image into account, generate an question that has the answer: {Answer}. Question: Table 5: Instruction templates used for transforming held-in datasets into instruction tuning data. For datasets with OCR tokens, we simply add \"OCR tokens:\" after the image query embeddings.\n\n\nE Instructions for Zero-shot Inference\n\nWe provide instructions used for zero-shot inference. Note that for instructions with options, we separate options with the alphabetical order, e.g. HatefulMemes <Image> This is an image with: \"{}\" written on it. Is it hateful? Answer:\n\nVSR <Image> Based on the image, is this statement true or false? \"{}\" Answer:\n\nVisual Dialog <Image> Dialog history: {}\\n Question: {} Short answer:\n\nFigure 1 :\n1A few qualitative examples generated by our InstructBLIP Vicuna model. Here, a range of its diverse capabilities are demonstrated, including complex visual scene understanding and reasoning, knowledge-grounded image description, multi-turn visual conversation, etc.\n\nFigure 3 :\n3correct option to the following question: which picture shows the pizza inside the over? Options: (a) left one (b) right one. Model architecture of InstructBLIP. The Q-Former extracts instruction-aware visual features from the output embeddings of the frozen image encoder, and feeds the visual features as soft prompt input to the frozen LLM. We instruction-tune the model with the language modeling loss to generate the response.\n\nFigure 4 :\n4Comparison of instruction tuning and multitask training based on BLIP-2 FlanT5 XL backbone. For held-in evaluation, we compute the average score across all held-in datasets. For held-out evaluation, we compute the average score across GQA, TextVQA, VSR, HatefulMemes, IconQA, ScienceQA, iVQA, VizWiz.\n\nFigure 5 :\n5The response from InstructBLIP is more comprehensive than GPT-4, more visuallygrounded than LLaVA, and more logical than MiniGPT-4. The responses of GPT-4 and LLaVA are obtained from their respective papers, while the official demo is used for MiniGPT-4. Source:\n\nFigure 7 :\n7InstructBLIP shows the ability to connect textual knowledge with visual input, while the other models depict the picture plainly. MiniGPT-4 exhibits poorer results, which may be due to its training with only long captions. Responses of LLaVA and MiniGPT-4 are generated by their official demos.\n\n\n(a) blue (b) yellow (c) pink (d) black. GQA, VizWiz, iVQA, MSVD, MSRVTT <Image> Question: {} Short answer: NoCaps, Flickr30k <Image> A short image description: TextVQA <Image> OCR tokens: {}. Question: {} Short answer: IconQA <Image> Question: {} Options: {}. Short answer: ScienceQA <Image> Context: {} Question: {} Options: {}. Answer:\n\n\nTable 1: Zero-shot results on the held-out datasets. Here, Visdial, HM and SciQA denote the Visual Dialog, HatefulMemes and ScienceQA datasets, respectively. For ScienceQA, we only evaluate on the set with image context. Following previous works[4,49,32], we report the CIDEr score[42] for NoCaps and Flickr30K, iVQA accuracy for iVQA, AUC score for HatefulMemes, and Mean Reciprocal Rank (MRR) for Visual Dialog. For all other datasets, we report the top-1 accuracy (%).NoCaps \n\nFlickr \n30K \nGQA VSR IconQA TextVQA Visdial HM VizWiz \n\nSciQA \nimage \n\nMSVD \nQA \n\nMSRVTT \nQA \n\niVQA \n\nFlamingo-3B [4] \n-\n60.6 \n-\n-\n-\n30.1 \n-\n53.7 28.9 \n-\n27.5 \n11.0 \n32.7 \n\nFlamingo-9B [4] \n-\n61.5 \n-\n-\n-\n31.8 \n-\n57.0 28.8 \n-\n30.2 \n13.7 \n35.2 \n\nFlamingo-80B [4] \n-\n67.2 \n-\n-\n-\n35.0 \n-\n46.4 31.6 \n-\n35.6 \n17.4 \n40.7 \n\nBLIP-2 (FlanT5 XL ) [20] \n104.5 76.1 44.0 60.5 45.5 \n43.1 \n45.7 53.0 29.8 \n54.9 33.7 \n16.2 \n40.4 \n\nBLIP-2 (FlanT5 XXL ) [20] \n98.4 \n73.7 44.6 68.2 45.4 \n44.1 \n46.9 52.0 29.4 \n64.5 34.4 \n17.4 \n45.8 \n\nBLIP-2 (Vicuna-7B) \n107.5 74.9 38.6 50.0 39.7 \n40.1 \n44.9 50.6 25.3 \n53.8 18.3 \n9.2 \n27.5 \n\nBLIP-2 (Vicuna-13B) \n103.9 71.6 41.0 50.9 40.6 \n42.5 \n45.1 53.7 19.6 \n61.0 20.3 \n10.3 \n23.5 \n\nInstructBLIP (FlanT5 XL ) \n119.9 84.5 48.4 64.8 50.0 \n46.6 \n46.6 56.6 32.7 \n70.4 43.4 \n25.0 \n53.1 \n\nInstructBLIP (FlanT5 XXL ) \n120.0 83.5 47.9 65.6 51.2 \n46.6 \n48.5 54.1 30.9 \n70.6 44.3 \n25.6 \n53.8 \n\nInstructBLIP (Vicuna-7B) \n123.1 82.4 49.2 54.3 43.1 \n50.1 \n45.2 59.6 34.5 \n60.5 41.8 \n22.1 \n52.2 \n\nInstructBLIP (Vicuna-13B) 121.9 82.8 49.5 52.1 44.8 \n50.7 \n45.4 57.5 33.4 \n63.1 41.2 \n24.8 \n51.0 \n\n\n\nTable 2 :\n2Results of ablation studies that remove the instruction-aware Visual Features (Section 2.3) \nand the balanced data sampling strategy (Section 2.4). For held-in evaluation, we compute the average \nscore of four datasets, including COCO Caption, OKVQA, A-OKVQA, and TextCaps. For held-out \nevaluation, we show five datasets from different tasks. \n\ndemonstrating the effectiveness of vision-language instruction tuning. For instance, InstructBLIP \nFlanT5 XL yields an average relative improvement of 15.0% when compared to BLIP-2 FlanT5 XL . \nFurthermore, instruction tuning boosts zero-shot generalization on unseen task categories such as \nvideo QA. InstructBLIP achieves up to 47.1% relative improvement on MSRVTT-QA over the \nprevious SOTA despite having never been trained with temporal video data. Finally, our smallest \nInstructBLIP FlanT5 XL with 4B parameters outperforms Flamingo-80B on all six shared evaluation \ndatasets with an average relative improvement of 24.8%. \n\n\n\nTable 3 :\n3Results of finetuning BLIP-2 and InstructBLIP on downstream datasets. Compared to \nBLIP-2, InstructBLIP provides a better weight initialization model and achieves SOTA performance \non three out of four datasets. \n\nThe results are shown in Table 3. Compared to BLIP-2, InstructBLIP leads to better finetuning \nperformance on all datasets, which validates InstructBLIP as a better weight initialization model \nfor task-specific finetuning. InstructBLIP sets new state-of-the-art finetuning performance on Sci-\nenceQA (IMG), OCR-VQA, A-OKVQA, and is outperformed on OKVQA by PaLM-E [9] with \n562B parameters. \n\nAdditionally, we observe that the FlanT5-based InstructBLIP is superior at multi-choice tasks, \nwhereas Vicuna-based InstructBLIP is generally better at open-ended generation tasks. This disparity \ncan be primarily attributed to the capabilities of their frozen LLMs, as they both employ the same \nimage encoder. Although FlanT5 and Vicuna are both instruction-tuned LLMs, their instruction data \nsignificantly differ. FlanT5 is mainly finetuned on NLP benchmarks containing many multi-choice \nQA and classification datasets, while Vicuna is finetuned on open-ended instruction-following data. \n\n\n\n\nWe use the large-scale COCO dataset for the image captioning task. Specifically, Karpathy split[17] is used, which divides the data into 82K/5K/5K images for the train/val/test sets.Web CapFilt\u2717 14M image-text pairs collected from the web with additional BLIP-generated synthetic captions, used in BLIP[21] and BLIP-2[20]. The Flickr30k dataset consists of 31K images collected from Flickr, each image has five ground truth captions. We use the test split as the held-out which contains 1K images. TextCaps is an image captioning dataset that requires the model to comprehend and reason the text in images. Its train/val/test sets contain 21K/3K/3K images, respectively. VQAv2 is dataset for open-ended image question answering. It is split into 82K/40K/81K for train/val/test. GQA contains image questions for scene understanding and reasoning. We use the balanced test-dev set as held-out. VSR is a collection of image-text pairs, in which the text describes the spatial relation of two objects in the image. Models are required to classify true/false for the description. We use the zero-shot data split given in its official github repository. IconQA measures the abstract diagram understanding and comprehensive cognitive reasoning abilities of models. We use the test set of its multi-text-choice task for held-out evaluation.OKVQA[30] \u2717 OKVQA contains visual questions that require outside knowledge to answer. It has been split into 9K/5K for train and test.A-OKVQA [36] \u2717A-OKVQA is a successor of OKVQA with more challenging and diverse questions. It has 17K/1K/6K questions for train/val/test.ScienceQA[28] \u2713 (test) ScienceQA covers diverse science topics with corresponding lectures and explanations. In out settings, we only use the part with image context (IMG). Visual dialog is a conversational question answering dataset. We use the val split as the held-out, which contains 2,064 images and each has 10 rounds.OCR-VQA [31]\u2717 OCR-VQA contains visual questions that require models to read text in the image. It has 800K/100K/100K for train/val/test, respectively.Dataset Name \nHeld-out \nDataset Description \n\nCOCO Caption [23] \n\u2717 \nNoCaps [3] \n\u2713 (val) \nNoCaps contains 15,100 images with 166,100 human-written captions for novel object \nimage captioning. \n\nFlickr30K [51] \n\u2713 (test) \nTextCaps [38] \n\u2717 \nVQAv2 [11] \n\u2717 \nVizWiz [12] \n\u2713 (test-dev) \nA dataset contains visual questions asked by people who are blind. 8K images are used for \nthe held-out evaluation. \n\nGQA [16] \n\u2713 (test-dev) \nVisual Spatial Reasoning \n\u2713 (test) \nIconQA [29] \n\u2713 (test) \nVisual Dialog [8] \n\u2713 (val) \n\n\nTable 4 :\n4Description of datasets in our held-in instruction tuning and held-out zero-shot evaluations. <Image>A short image caption: <Image>A short image description: <Image>A photo of <Image>An image that shows <Image>Write a short description for the image. <Image>Write a description for the photo. <Image>Provide a description of what is presented in the photo. <Image>Briefly describe the content of the image. <Image>Can you briefly explain what you see in the image? <Image>Could you use a few words to describe what you perceive in the photo? <Image>Please provide a short depiction of the picture. <Image>Using language, provide a short account of the image. <Image>Use a few words to illustrate what is happening in the picture. {Question} Short answer: <Image>Given the image, answer the following question with no more than three words. {Question} <Image>Based on the image, respond to this question with a short answer: {Question}. Answer: <Image>Use the provided image to answer the question: {Question} Provide your answer as short as possible:D Instruction Templates \n\nTask \nInstruction Template \n\nImage \nCaptioning \n\nVQA \n\n<Image>{Question} \n<Image>Question: {Question} \n<Image>{Question} A short answer to the question is \n<Image>Q: {Question} A: \n<Image>Question: \n\n. Chatgpt, Chatgpt. https://openai.com/blog/chatgpt, 2023. 9\n\n. Vicuna, 69Vicuna. https://github.com/lm-sys/FastChat, 2023. 3, 6, 9\n\nnocaps: novel object captioning at scale. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson, ICCV. 316Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, pages 8948-8957, 2019. 3, 16\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katherine Mensch, Malcolm Millican, Roman Reynolds, Eliza Ring, Serkan Rutherford, Tengda Cabi, Zhitao Han, Sina Gong, Marianne Samangooei, Monteiro, L Jacob, Sebastian Menick, Andy Borgeaud, Aida Brock, Sahand Nematzadeh, Sharifzadeh, Ricardo Miko\u0142 Aj Bi\u0144kowski, Oriol Barreira, Andrew Vinyals, Kar\u00e9n Zisserman, Simonyan, NeurIPS, 2022. S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh36Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko\u0142 aj Bi\u0144kowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\u00e9n Simonyan. Flamingo: a visual language model for few-shot learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, NeurIPS, 2022. 3, 6\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordarXiv preprintTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 9\n\nUnifying vision-and-language tasks via text generation. Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal, arXiv:2102.02779arXiv preprintJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. arXiv preprint arXiv:2102.02779, 2021. 1\n\n. Hyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Siddhartha Dehghani, Albert Brahma, Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Mishra, Adams Yu, Vincent YHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y.\n\nScaling instruction-finetuned language models. Yanping Zhao, Andrew M Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, arXiv:2210.1141669arXiv preprintZhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 1, 3, 6, 9\n\nVisual dialog. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, M F Jose, Devi Moura, Dhruv Parikh, Batra, CVPR. 316Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In CVPR, 2017. 3, 16\n\nPalm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete FlorenceDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model, 2023. 9\n\nExploring the limits of masked visual representation learning at scale. ArXiv. Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen, Yu Sun, Xinggang Wu, Tiejun Wang, Xinlong Huang, Yue Wang, Cao, Eva, abs/2211.07636Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. ArXiv, abs/2211.07636, 2022. 6\n\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, CVPR. 316Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, July 2017. 3, 16\n\nVizwiz grand challenge: Answering visual questions from blind people. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P Bigham, In CVPR. 316Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 3, 16\n\nUnnatural instructions: Tuning language models with (almost) no human labor. Or Honovich, Thomas Scialom, Omer Levy, Timo Schick, abs/2212.09689ArXiv. 20229Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. ArXiv, abs/2212.09689, 2022. 9\n\nLora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, ICLR, 2022. 9Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 9\n\nPromptcap: Promptguided task-aware image captioning. Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, Jiebo Luo, 2023Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, and Jiebo Luo. Promptcap: Prompt- guided task-aware image captioning, 2023. 9\n\nGqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, CVPR. 316Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 3, 16\n\nDeep visual-semantic alignments for generating image descriptions. Andrej Karpathy, Li Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)16Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 16\n\nThe hateful memes challenge: Detecting hate speech in multimodal memes. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, Davide Testuggine, NeurIPS. 316Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In NeurIPS, 2020. 3, 16\n\nLavis: A library for language-vision intelligence. Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, Steven C H Hoi, 2022Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C. H. Hoi. Lavis: A library for language-vision intelligence, 2022. 6\n\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML. 916Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 1, 3, 4, 6, 9, 16\n\nBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, ICML. 516Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. 5, 16\n\nAlign before fuse: Vision and language representation learning with momentum distillation. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu Hong Hoi, NeurIPS. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS, 2021. 5\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. 316Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 3, 16\n\nVisual spatial reasoning. Fangyu Liu, Guy Edward Toh Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 20233Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 2023. 3\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023. 3, 7. 916Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. 2023. 3, 7, 9, 16\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6\n\n12-in-1: Multi-task vision and language representation learning. Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee, CVPR. 2020Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In CVPR, 2020. 1\n\nLearn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, NeurIPS, 2022. 316Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 3, 16\n\nIconqa: A new benchmark for abstract diagram understanding and visual language reasoning. Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu, NeurIPS Track on Datasets and Benchmarks. 316Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In NeurIPS Track on Datasets and Benchmarks, 2021. 3, 16\n\nOk-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, CVPR. 316Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In CVPR, 2019. 3, 16\n\nOcr-vqa: Visual question answering by reading text in images. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, Anirban Chakraborty, ICDAR. 316Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 3, 16\n\nLarge-scale pretraining for visual dialog: A simple state-of-the-art baseline. Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm67Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek Das. Large-scale pretraining for visual dialog: A simple state-of-the-art baseline. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, ECCV, 2020. 6, 7\n\n. abs/2303.08774OpenAI. Gpt-4 technical report. ArXiv. 79OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 7, 9\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 36Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 2020. 3, 6\n\nMultitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Chhablani, V Nihal, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Shen, ICLR. Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le ScaoStella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush2022Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An- toine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In ICLR, 2022. 9\n\nAokvqa: A benchmark for visual question answering using world knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner316Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A- okvqa: A benchmark for visual question answering using world knowledge. In Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner, editors, ECCV, 2022. 3, 16\n\nPrompting large language models with answer heuristics for knowledge-based visual question answering. Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu, Computer Vision and Pattern Recognition (CVPR). 2023Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question answering. Computer Vision and Pattern Recognition (CVPR), 2023. 9\n\nTextcaps: a dataset for image captioningwith reading comprehension. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh, 316Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioningwith reading comprehension. 2020. 3, 16\n\nTowards vqa models that can read. Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, Marcus Rohrbach, CVPR. 316Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, pages 8317-8326, 2019. 3, 16\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023. 9\n\nLlama: Open and efficient foundation language models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, arXiv:2302.1397169arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 6, 9\n\nCider: Consensus-based image description evaluation. C Lawrence Ramakrishna Vedantam, Devi Zitnick, Parikh, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4566-4575, 2015. 6\n\nGit: A generative image-to-text transformer for vision and language. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, 2022Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language, 2022. 9\n\nSelf-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, abs/2212.10560, 2022. 9ArXiv. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. ArXiv, abs/2212.10560, 2022. 9\n\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, EMNLP. Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen2022Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super- NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In EMNLP, 2022. 9\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, ICLR, 2022. 1, 5. 89Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR, 2022. 1, 5, 8, 9\n\nHanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Proceedings of the 25th ACM International Conference on Multimedia. the 25th ACM International Conference on Multimedia316Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM International Conference on Multimedia, page 1645-1653, 2017. 3, 16\n\nMultiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Zhiyang Xu, Ying Shen, Lifu Huang, abs/2212.10773ArXiv. 20229Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. ArXiv, abs/2212.10773, 2022. 9\n\nJust ask: Learning to answer questions from millions of narrated videos. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid, ICCV. 616Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In ICCV, pages 1686-1697, 2021. 3, 6, 16\n\nJi Chao Zhang, and Feiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, 2023Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and Feiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. 2023. 9\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, Transactions of the Association for Computational Linguistics. 216Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2, 2014. 3, 16\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, 79Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. 7, 9\n", "annotations": {"author": "[{\"end\":172,\"start\":88},{\"end\":208,\"start\":173},{\"end\":241,\"start\":209},{\"end\":277,\"start\":242},{\"end\":355,\"start\":278},{\"end\":411,\"start\":356},{\"end\":470,\"start\":412},{\"end\":525,\"start\":471},{\"end\":588,\"start\":526},{\"end\":642,\"start\":589}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":97},{\"end\":182,\"start\":180},{\"end\":218,\"start\":216},{\"end\":254,\"start\":250},{\"end\":288,\"start\":283},{\"end\":366,\"start\":362},{\"end\":425,\"start\":421},{\"end\":480,\"start\":478},{\"end\":538,\"start\":534},{\"end\":599,\"start\":596}]", "author_first_name": "[{\"end\":96,\"start\":88},{\"end\":179,\"start\":173},{\"end\":215,\"start\":209},{\"end\":249,\"start\":242},{\"end\":282,\"start\":278},{\"end\":361,\"start\":356},{\"end\":420,\"start\":412},{\"end\":477,\"start\":471},{\"end\":533,\"start\":526},{\"end\":595,\"start\":589}]", "author_affiliation": "[{\"end\":122,\"start\":102},{\"end\":171,\"start\":124},{\"end\":240,\"start\":220},{\"end\":276,\"start\":256},{\"end\":310,\"start\":290},{\"end\":354,\"start\":312},{\"end\":410,\"start\":368},{\"end\":469,\"start\":427},{\"end\":524,\"start\":482},{\"end\":587,\"start\":540},{\"end\":641,\"start\":621}]", "title": "[{\"end\":85,\"start\":1},{\"end\":727,\"start\":643}]", "venue": null, "abstract": "[{\"end\":2194,\"start\":772}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2419,\"start\":2415},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2421,\"start\":2419},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2766,\"start\":2762},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3305,\"start\":3302},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3308,\"start\":3305},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5622,\"start\":5621},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5741,\"start\":5740},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7459,\"start\":7455},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7461,\"start\":7459},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9446,\"start\":9443},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9493,\"start\":9489},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9508,\"start\":9505},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9554,\"start\":9550},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10718,\"start\":10714},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10720,\"start\":10718},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10723,\"start\":10720},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10773,\"start\":10769},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10796,\"start\":10792},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10799,\"start\":10796},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10802,\"start\":10799},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10833,\"start\":10829},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10836,\"start\":10833},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10886,\"start\":10882},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10889,\"start\":10886},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10892,\"start\":10889},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10950,\"start\":10946},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10953,\"start\":10950},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11042,\"start\":11038},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11045,\"start\":11042},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11091,\"start\":11088},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11118,\"start\":11114},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11148,\"start\":11144},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14278,\"start\":14274},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17397,\"start\":17393},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17400,\"start\":17397},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17403,\"start\":17400},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18547,\"start\":18543},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18656,\"start\":18653},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18735,\"start\":18731},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18747,\"start\":18744},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18849,\"start\":18845},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19262,\"start\":19258},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19626,\"start\":19622},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20504,\"start\":20500},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22543,\"start\":22539},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22555,\"start\":22551},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22571,\"start\":22567},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23701,\"start\":23697},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":26277,\"start\":26273},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26279,\"start\":26277},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26282,\"start\":26279},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26285,\"start\":26282},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26319,\"start\":26316},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26353,\"start\":26350},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26356,\"start\":26353},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26359,\"start\":26356},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26362,\"start\":26359},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26526,\"start\":26522},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26639,\"start\":26635},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26725,\"start\":26722},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26850,\"start\":26846},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27016,\"start\":27012},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27032,\"start\":27028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27064,\"start\":27060},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27080,\"start\":27076},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27206,\"start\":27202},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34001,\"start\":33997},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34131,\"start\":34127},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34249,\"start\":34245},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":34480,\"start\":34476},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34586,\"start\":34582},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38224,\"start\":38221},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":38227,\"start\":38224},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":38230,\"start\":38227},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":38261,\"start\":38257},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41867,\"start\":41863},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":42074,\"start\":42070},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":42089,\"start\":42085},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":43109,\"start\":43105},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":43384,\"start\":43380}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36290,\"start\":36012},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36735,\"start\":36291},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37049,\"start\":36736},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37325,\"start\":37050},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37633,\"start\":37326},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37973,\"start\":37634},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39556,\"start\":37974},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40548,\"start\":39557},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41765,\"start\":40549},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44353,\"start\":41766},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45640,\"start\":44354}]", "paragraph": "[{\"end\":2932,\"start\":2210},{\"end\":3523,\"start\":2934},{\"end\":3577,\"start\":3525},{\"end\":4043,\"start\":3579},{\"end\":4082,\"start\":4045},{\"end\":4612,\"start\":4084},{\"end\":4657,\"start\":4614},{\"end\":4986,\"start\":4659},{\"end\":5028,\"start\":4988},{\"end\":5424,\"start\":5030},{\"end\":5474,\"start\":5426},{\"end\":6022,\"start\":5476},{\"end\":6563,\"start\":6024},{\"end\":6600,\"start\":6565},{\"end\":7016,\"start\":6602},{\"end\":7046,\"start\":7018},{\"end\":7719,\"start\":7048},{\"end\":8362,\"start\":7721},{\"end\":8886,\"start\":8364},{\"end\":9342,\"start\":8888},{\"end\":9819,\"start\":9344},{\"end\":10357,\"start\":9858},{\"end\":11228,\"start\":10380},{\"end\":12082,\"start\":11230},{\"end\":12460,\"start\":12120},{\"end\":13123,\"start\":12462},{\"end\":13643,\"start\":13125},{\"end\":14198,\"start\":13691},{\"end\":15197,\"start\":14200},{\"end\":15763,\"start\":15199},{\"end\":16002,\"start\":15795},{\"end\":16309,\"start\":16004},{\"end\":16906,\"start\":16331},{\"end\":18026,\"start\":16928},{\"end\":18272,\"start\":18028},{\"end\":19200,\"start\":18299},{\"end\":19983,\"start\":19202},{\"end\":20571,\"start\":20044},{\"end\":21492,\"start\":20623},{\"end\":22443,\"start\":21519},{\"end\":23650,\"start\":22445},{\"end\":24537,\"start\":23687},{\"end\":25387,\"start\":24539},{\"end\":25963,\"start\":25435},{\"end\":26387,\"start\":25980},{\"end\":27324,\"start\":26389},{\"end\":27770,\"start\":27326},{\"end\":28576,\"start\":27785},{\"end\":29152,\"start\":28597},{\"end\":29222,\"start\":29189},{\"end\":29667,\"start\":29253},{\"end\":30225,\"start\":29677},{\"end\":30236,\"start\":30227},{\"end\":30449,\"start\":30238},{\"end\":30585,\"start\":30459},{\"end\":31070,\"start\":30587},{\"end\":31347,\"start\":31072},{\"end\":31367,\"start\":31349},{\"end\":31762,\"start\":31369},{\"end\":31777,\"start\":31764},{\"end\":31836,\"start\":31779},{\"end\":31885,\"start\":31857},{\"end\":32393,\"start\":31887},{\"end\":33457,\"start\":32395},{\"end\":34080,\"start\":33459},{\"end\":34223,\"start\":34114},{\"end\":34365,\"start\":34225},{\"end\":34464,\"start\":34382},{\"end\":34575,\"start\":34466},{\"end\":35583,\"start\":34577},{\"end\":35861,\"start\":35626},{\"end\":35940,\"start\":35863},{\"end\":36011,\"start\":35942}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16330,\"start\":16310},{\"attributes\":{\"id\":\"formula_1\"},\"end\":29188,\"start\":29175},{\"attributes\":{\"id\":\"formula_2\"},\"end\":29252,\"start\":29223}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":3464,\"start\":3455},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15624,\"start\":15617},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16767,\"start\":16760},{\"end\":20260,\"start\":20253},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20864,\"start\":20857},{\"end\":35399,\"start\":35392}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2208,\"start\":2196},{\"attributes\":{\"n\":\"2\"},\"end\":9856,\"start\":9822},{\"attributes\":{\"n\":\"2.1\"},\"end\":10378,\"start\":10360},{\"attributes\":{\"n\":\"2.2\"},\"end\":12118,\"start\":12085},{\"attributes\":{\"n\":\"2.3\"},\"end\":13689,\"start\":13646},{\"attributes\":{\"n\":\"2.4\"},\"end\":15793,\"start\":15766},{\"attributes\":{\"n\":\"2.5\"},\"end\":16926,\"start\":16909},{\"attributes\":{\"n\":\"2.6\"},\"end\":18297,\"start\":18275},{\"attributes\":{\"n\":\"3\"},\"end\":20019,\"start\":19986},{\"attributes\":{\"n\":\"3.1\"},\"end\":20042,\"start\":20022},{\"attributes\":{\"n\":\"3.2\"},\"end\":20621,\"start\":20574},{\"attributes\":{\"n\":\"3.3\"},\"end\":21517,\"start\":21495},{\"attributes\":{\"n\":\"3.4\"},\"end\":23685,\"start\":23653},{\"attributes\":{\"n\":\"3.5\"},\"end\":25433,\"start\":25390},{\"attributes\":{\"n\":\"4\"},\"end\":25978,\"start\":25966},{\"attributes\":{\"n\":\"5\"},\"end\":27783,\"start\":27773},{\"end\":28595,\"start\":28579},{\"end\":29174,\"start\":29155},{\"end\":29675,\"start\":29670},{\"end\":30457,\"start\":30452},{\"end\":31855,\"start\":31839},{\"end\":34112,\"start\":34083},{\"end\":34380,\"start\":34368},{\"end\":35624,\"start\":35586},{\"end\":36023,\"start\":36013},{\"end\":36302,\"start\":36292},{\"end\":36747,\"start\":36737},{\"end\":37061,\"start\":37051},{\"end\":37337,\"start\":37327},{\"end\":39567,\"start\":39558},{\"end\":40559,\"start\":40550},{\"end\":44364,\"start\":44355}]", "table": "[{\"end\":39556,\"start\":38447},{\"end\":40548,\"start\":39569},{\"end\":41765,\"start\":40561},{\"end\":44353,\"start\":43845},{\"end\":45640,\"start\":45416}]", "figure_caption": "[{\"end\":36290,\"start\":36025},{\"end\":36735,\"start\":36304},{\"end\":37049,\"start\":36749},{\"end\":37325,\"start\":37063},{\"end\":37633,\"start\":37339},{\"end\":37973,\"start\":37636},{\"end\":38447,\"start\":37976},{\"end\":43845,\"start\":41768},{\"end\":45416,\"start\":44366}]", "figure_ref": "[{\"end\":10621,\"start\":10613},{\"end\":11916,\"start\":11908},{\"end\":12326,\"start\":12318},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14252,\"start\":14244},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21691,\"start\":21683},{\"end\":22857,\"start\":22849},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24572,\"start\":24564},{\"end\":31397,\"start\":31389}]", "bib_author_first_name": "[{\"end\":45822,\"start\":45817},{\"end\":45837,\"start\":45832},{\"end\":45850,\"start\":45845},{\"end\":45863,\"start\":45857},{\"end\":45877,\"start\":45870},{\"end\":45888,\"start\":45884},{\"end\":45903,\"start\":45898},{\"end\":45915,\"start\":45911},{\"end\":45930,\"start\":45924},{\"end\":45941,\"start\":45936},{\"end\":46242,\"start\":46238},{\"end\":46273,\"start\":46266},{\"end\":46290,\"start\":46283},{\"end\":46300,\"start\":46296},{\"end\":46312,\"start\":46308},{\"end\":46324,\"start\":46319},{\"end\":46339,\"start\":46333},{\"end\":46355,\"start\":46346},{\"end\":46371,\"start\":46364},{\"end\":46387,\"start\":46382},{\"end\":46403,\"start\":46398},{\"end\":46416,\"start\":46410},{\"end\":46435,\"start\":46429},{\"end\":46448,\"start\":46442},{\"end\":46458,\"start\":46454},{\"end\":46473,\"start\":46465},{\"end\":46497,\"start\":46496},{\"end\":46514,\"start\":46505},{\"end\":46527,\"start\":46523},{\"end\":46542,\"start\":46538},{\"end\":46556,\"start\":46550},{\"end\":46589,\"start\":46582},{\"end\":46615,\"start\":46610},{\"end\":46632,\"start\":46626},{\"end\":46647,\"start\":46642},{\"end\":47345,\"start\":47342},{\"end\":47347,\"start\":47346},{\"end\":47363,\"start\":47355},{\"end\":47374,\"start\":47370},{\"end\":47389,\"start\":47382},{\"end\":47404,\"start\":47399},{\"end\":47421,\"start\":47413},{\"end\":47438,\"start\":47432},{\"end\":47458,\"start\":47452},{\"end\":47472,\"start\":47466},{\"end\":47487,\"start\":47481},{\"end\":47504,\"start\":47496},{\"end\":47519,\"start\":47514},{\"end\":47542,\"start\":47534},{\"end\":47555,\"start\":47552},{\"end\":47571,\"start\":47566},{\"end\":47585,\"start\":47579},{\"end\":47600,\"start\":47594},{\"end\":47602,\"start\":47601},{\"end\":47619,\"start\":47612},{\"end\":47631,\"start\":47624},{\"end\":47651,\"start\":47640},{\"end\":47663,\"start\":47659},{\"end\":47674,\"start\":47670},{\"end\":47690,\"start\":47683},{\"end\":48516,\"start\":48510},{\"end\":48525,\"start\":48522},{\"end\":48534,\"start\":48531},{\"end\":48545,\"start\":48540},{\"end\":48745,\"start\":48743},{\"end\":48759,\"start\":48753},{\"end\":48771,\"start\":48765},{\"end\":48783,\"start\":48781},{\"end\":48797,\"start\":48790},{\"end\":48807,\"start\":48803},{\"end\":48821,\"start\":48815},{\"end\":48833,\"start\":48826},{\"end\":48850,\"start\":48840},{\"end\":48867,\"start\":48861},{\"end\":48889,\"start\":48884},{\"end\":48906,\"start\":48900},{\"end\":48916,\"start\":48911},{\"end\":48928,\"start\":48922},{\"end\":48946,\"start\":48937},{\"end\":48959,\"start\":48953},{\"end\":48977,\"start\":48971},{\"end\":49349,\"start\":49342},{\"end\":49362,\"start\":49356},{\"end\":49364,\"start\":49363},{\"end\":49379,\"start\":49372},{\"end\":49389,\"start\":49385},{\"end\":49396,\"start\":49394},{\"end\":49398,\"start\":49397},{\"end\":49411,\"start\":49407},{\"end\":49422,\"start\":49417},{\"end\":49433,\"start\":49429},{\"end\":49447,\"start\":49442},{\"end\":49461,\"start\":49457},{\"end\":49463,\"start\":49462},{\"end\":49475,\"start\":49470},{\"end\":49788,\"start\":49780},{\"end\":49800,\"start\":49794},{\"end\":49815,\"start\":49809},{\"end\":49826,\"start\":49823},{\"end\":49841,\"start\":49834},{\"end\":49850,\"start\":49849},{\"end\":49852,\"start\":49851},{\"end\":49863,\"start\":49859},{\"end\":49876,\"start\":49871},{\"end\":50107,\"start\":50102},{\"end\":50119,\"start\":50116},{\"end\":50126,\"start\":50125},{\"end\":50128,\"start\":50127},{\"end\":50141,\"start\":50136},{\"end\":50160,\"start\":50151},{\"end\":50173,\"start\":50168},{\"end\":50191,\"start\":50185},{\"end\":50208,\"start\":50200},{\"end\":50220,\"start\":50216},{\"end\":50236,\"start\":50230},{\"end\":50251,\"start\":50244},{\"end\":50262,\"start\":50256},{\"end\":50276,\"start\":50270},{\"end\":50293,\"start\":50287},{\"end\":50310,\"start\":50304},{\"end\":50329,\"start\":50322},{\"end\":50343,\"start\":50338},{\"end\":50359,\"start\":50355},{\"end\":50913,\"start\":50908},{\"end\":50923,\"start\":50920},{\"end\":50936,\"start\":50930},{\"end\":50954,\"start\":50952},{\"end\":50968,\"start\":50960},{\"end\":50979,\"start\":50973},{\"end\":50993,\"start\":50986},{\"end\":51004,\"start\":51001},{\"end\":51366,\"start\":51362},{\"end\":51379,\"start\":51374},{\"end\":51393,\"start\":51386},{\"end\":51413,\"start\":51408},{\"end\":51425,\"start\":51421},{\"end\":51721,\"start\":51716},{\"end\":51734,\"start\":51730},{\"end\":51746,\"start\":51739},{\"end\":51748,\"start\":51747},{\"end\":51763,\"start\":51757},{\"end\":51772,\"start\":51769},{\"end\":51785,\"start\":51778},{\"end\":51800,\"start\":51795},{\"end\":51813,\"start\":51806},{\"end\":51815,\"start\":51814},{\"end\":52121,\"start\":52119},{\"end\":52138,\"start\":52132},{\"end\":52152,\"start\":52148},{\"end\":52163,\"start\":52159},{\"end\":52422,\"start\":52416},{\"end\":52424,\"start\":52423},{\"end\":52435,\"start\":52429},{\"end\":52449,\"start\":52442},{\"end\":52464,\"start\":52458},{\"end\":52483,\"start\":52476},{\"end\":52493,\"start\":52488},{\"end\":52502,\"start\":52500},{\"end\":52515,\"start\":52509},{\"end\":52774,\"start\":52769},{\"end\":52783,\"start\":52779},{\"end\":52798,\"start\":52789},{\"end\":52811,\"start\":52805},{\"end\":52821,\"start\":52817},{\"end\":52823,\"start\":52822},{\"end\":52836,\"start\":52831},{\"end\":53078,\"start\":53077},{\"end\":53096,\"start\":53085},{\"end\":53098,\"start\":53097},{\"end\":53352,\"start\":53346},{\"end\":53365,\"start\":53363},{\"end\":53812,\"start\":53807},{\"end\":53825,\"start\":53820},{\"end\":53841,\"start\":53834},{\"end\":53856,\"start\":53849},{\"end\":53875,\"start\":53866},{\"end\":53889,\"start\":53883},{\"end\":53906,\"start\":53900},{\"end\":54201,\"start\":54195},{\"end\":54212,\"start\":54206},{\"end\":54221,\"start\":54217},{\"end\":54234,\"start\":54226},{\"end\":54247,\"start\":54241},{\"end\":54264,\"start\":54258},{\"end\":54268,\"start\":54265},{\"end\":54533,\"start\":54527},{\"end\":54544,\"start\":54538},{\"end\":54555,\"start\":54549},{\"end\":54572,\"start\":54566},{\"end\":54892,\"start\":54886},{\"end\":54903,\"start\":54897},{\"end\":54915,\"start\":54908},{\"end\":54929,\"start\":54923},{\"end\":55222,\"start\":55216},{\"end\":55238,\"start\":55227},{\"end\":55258,\"start\":55250},{\"end\":55274,\"start\":55268},{\"end\":55288,\"start\":55281},{\"end\":55311,\"start\":55296},{\"end\":55594,\"start\":55586},{\"end\":55607,\"start\":55600},{\"end\":55620,\"start\":55615},{\"end\":55636,\"start\":55631},{\"end\":55649,\"start\":55643},{\"end\":55662,\"start\":55658},{\"end\":55677,\"start\":55672},{\"end\":55687,\"start\":55686},{\"end\":55696,\"start\":55688},{\"end\":55937,\"start\":55931},{\"end\":55946,\"start\":55943},{\"end\":55972,\"start\":55967},{\"end\":56210,\"start\":56203},{\"end\":56224,\"start\":56216},{\"end\":56237,\"start\":56229},{\"end\":56246,\"start\":56242},{\"end\":56250,\"start\":56247},{\"end\":56444,\"start\":56440},{\"end\":56462,\"start\":56457},{\"end\":56639,\"start\":56633},{\"end\":56651,\"start\":56644},{\"end\":56667,\"start\":56661},{\"end\":56682,\"start\":56678},{\"end\":56697,\"start\":56691},{\"end\":56963,\"start\":56960},{\"end\":56975,\"start\":56968},{\"end\":56988,\"start\":56984},{\"end\":56999,\"start\":56994},{\"end\":57012,\"start\":57005},{\"end\":57029,\"start\":57020},{\"end\":57041,\"start\":57035},{\"end\":57056,\"start\":57051},{\"end\":57070,\"start\":57064},{\"end\":57428,\"start\":57425},{\"end\":57438,\"start\":57433},{\"end\":57449,\"start\":57444},{\"end\":57460,\"start\":57456},{\"end\":57472,\"start\":57466},{\"end\":57482,\"start\":57479},{\"end\":57494,\"start\":57490},{\"end\":57506,\"start\":57499},{\"end\":57523,\"start\":57514},{\"end\":57913,\"start\":57906},{\"end\":57930,\"start\":57922},{\"end\":57945,\"start\":57942},{\"end\":57962,\"start\":57955},{\"end\":58218,\"start\":58213},{\"end\":58235,\"start\":58227},{\"end\":58256,\"start\":58245},{\"end\":58271,\"start\":58264},{\"end\":58542,\"start\":58535},{\"end\":58558,\"start\":58553},{\"end\":58570,\"start\":58566},{\"end\":58587,\"start\":58579},{\"end\":59110,\"start\":59105},{\"end\":59123,\"start\":59119},{\"end\":59137,\"start\":59133},{\"end\":59156,\"start\":59147},{\"end\":59168,\"start\":59162},{\"end\":59184,\"start\":59177},{\"end\":59198,\"start\":59193},{\"end\":59208,\"start\":59205},{\"end\":59220,\"start\":59213},{\"end\":59605,\"start\":59599},{\"end\":59618,\"start\":59612},{\"end\":59632,\"start\":59627},{\"end\":59648,\"start\":59641},{\"end\":59650,\"start\":59649},{\"end\":59664,\"start\":59657},{\"end\":59679,\"start\":59675},{\"end\":59697,\"start\":59690},{\"end\":59713,\"start\":59707},{\"end\":59728,\"start\":59724},{\"end\":59740,\"start\":59735},{\"end\":59752,\"start\":59746},{\"end\":59765,\"start\":59759},{\"end\":59776,\"start\":59770},{\"end\":59791,\"start\":59786},{\"end\":59814,\"start\":59807},{\"end\":59832,\"start\":59826},{\"end\":59850,\"start\":59849},{\"end\":59867,\"start\":59858},{\"end\":59883,\"start\":59875},{\"end\":59895,\"start\":59891},{\"end\":59917,\"start\":59914},{\"end\":59931,\"start\":59925},{\"end\":59943,\"start\":59938},{\"end\":60990,\"start\":60984},{\"end\":61006,\"start\":61000},{\"end\":61030,\"start\":61019},{\"end\":61045,\"start\":61038},{\"end\":61061,\"start\":61054},{\"end\":61560,\"start\":61553},{\"end\":61571,\"start\":61567},{\"end\":61580,\"start\":61576},{\"end\":61590,\"start\":61587},{\"end\":61927,\"start\":61920},{\"end\":61945,\"start\":61937},{\"end\":61956,\"start\":61950},{\"end\":61976,\"start\":61967},{\"end\":62179,\"start\":62170},{\"end\":62192,\"start\":62187},{\"end\":62207,\"start\":62203},{\"end\":62216,\"start\":62214},{\"end\":62230,\"start\":62224},{\"end\":62241,\"start\":62237},{\"end\":62256,\"start\":62250},{\"end\":62510,\"start\":62505},{\"end\":62524,\"start\":62518},{\"end\":62542,\"start\":62536},{\"end\":62554,\"start\":62550},{\"end\":62570,\"start\":62563},{\"end\":62581,\"start\":62575},{\"end\":62597,\"start\":62592},{\"end\":62614,\"start\":62605},{\"end\":62616,\"start\":62615},{\"end\":62925,\"start\":62921},{\"end\":62942,\"start\":62935},{\"end\":62958,\"start\":62951},{\"end\":62974,\"start\":62968},{\"end\":62995,\"start\":62985},{\"end\":63013,\"start\":63005},{\"end\":63028,\"start\":63023},{\"end\":63051,\"start\":63047},{\"end\":63065,\"start\":63059},{\"end\":63082,\"start\":63074},{\"end\":63096,\"start\":63090},{\"end\":63115,\"start\":63108},{\"end\":63133,\"start\":63124},{\"end\":63569,\"start\":63568},{\"end\":63578,\"start\":63570},{\"end\":63605,\"start\":63601},{\"end\":63986,\"start\":63978},{\"end\":64002,\"start\":63993},{\"end\":64016,\"start\":64009},{\"end\":64027,\"start\":64021},{\"end\":64037,\"start\":64032},{\"end\":64046,\"start\":64043},{\"end\":64059,\"start\":64052},{\"end\":64067,\"start\":64065},{\"end\":64079,\"start\":64073},{\"end\":64360,\"start\":64353},{\"end\":64374,\"start\":64367},{\"end\":64389,\"start\":64382},{\"end\":64403,\"start\":64398},{\"end\":64413,\"start\":64409},{\"end\":64415,\"start\":64414},{\"end\":64429,\"start\":64423},{\"end\":64448,\"start\":64440},{\"end\":64807,\"start\":64800},{\"end\":64821,\"start\":64814},{\"end\":64835,\"start\":64830},{\"end\":64861,\"start\":64854},{\"end\":64877,\"start\":64869},{\"end\":64894,\"start\":64887},{\"end\":64906,\"start\":64901},{\"end\":64918,\"start\":64914},{\"end\":64946,\"start\":64940},{\"end\":64963,\"start\":64958},{\"end\":64976,\"start\":64970},{\"end\":64992,\"start\":64985},{\"end\":65014,\"start\":65008},{\"end\":65025,\"start\":65020},{\"end\":65041,\"start\":65035},{\"end\":65055,\"start\":65050},{\"end\":65071,\"start\":65066},{\"end\":65085,\"start\":65080},{\"end\":66131,\"start\":66126},{\"end\":66144,\"start\":66137},{\"end\":66153,\"start\":66152},{\"end\":66169,\"start\":66163},{\"end\":66181,\"start\":66176},{\"end\":66185,\"start\":66182},{\"end\":66196,\"start\":66191},{\"end\":66204,\"start\":66201},{\"end\":66219,\"start\":66213},{\"end\":66221,\"start\":66220},{\"end\":66230,\"start\":66226},{\"end\":66232,\"start\":66231},{\"end\":66600,\"start\":66594},{\"end\":66609,\"start\":66605},{\"end\":66619,\"start\":66616},{\"end\":66629,\"start\":66626},{\"end\":67116,\"start\":67109},{\"end\":67125,\"start\":67121},{\"end\":67136,\"start\":67132},{\"end\":67401,\"start\":67394},{\"end\":67415,\"start\":67408},{\"end\":67428,\"start\":67423},{\"end\":67440,\"start\":67436},{\"end\":67457,\"start\":67449},{\"end\":67783,\"start\":67776},{\"end\":67795,\"start\":67788},{\"end\":67806,\"start\":67800},{\"end\":67816,\"start\":67811},{\"end\":67825,\"start\":67821},{\"end\":67833,\"start\":67831},{\"end\":67846,\"start\":67840},{\"end\":67858,\"start\":67853},{\"end\":67872,\"start\":67863},{\"end\":67882,\"start\":67878},{\"end\":67897,\"start\":67888},{\"end\":67910,\"start\":67902},{\"end\":67921,\"start\":67915},{\"end\":67935,\"start\":67928},{\"end\":67947,\"start\":67942},{\"end\":68374,\"start\":68369},{\"end\":68387,\"start\":68382},{\"end\":68398,\"start\":68393},{\"end\":68412,\"start\":68407},{\"end\":68843,\"start\":68838},{\"end\":68852,\"start\":68849},{\"end\":68867,\"start\":68859},{\"end\":68879,\"start\":68874},{\"end\":68891,\"start\":68884}]", "bib_author_last_name": "[{\"end\":45651,\"start\":45644},{\"end\":45712,\"start\":45706},{\"end\":45830,\"start\":45823},{\"end\":45843,\"start\":45838},{\"end\":45855,\"start\":45851},{\"end\":45868,\"start\":45864},{\"end\":45882,\"start\":45878},{\"end\":45896,\"start\":45889},{\"end\":45909,\"start\":45904},{\"end\":45922,\"start\":45916},{\"end\":45934,\"start\":45931},{\"end\":45950,\"start\":45942},{\"end\":46264,\"start\":46243},{\"end\":46281,\"start\":46274},{\"end\":46294,\"start\":46291},{\"end\":46306,\"start\":46301},{\"end\":46317,\"start\":46313},{\"end\":46331,\"start\":46325},{\"end\":46344,\"start\":46340},{\"end\":46362,\"start\":46356},{\"end\":46380,\"start\":46372},{\"end\":46396,\"start\":46388},{\"end\":46408,\"start\":46404},{\"end\":46427,\"start\":46417},{\"end\":46440,\"start\":46436},{\"end\":46452,\"start\":46449},{\"end\":46463,\"start\":46459},{\"end\":46484,\"start\":46474},{\"end\":46494,\"start\":46486},{\"end\":46503,\"start\":46498},{\"end\":46521,\"start\":46515},{\"end\":46536,\"start\":46528},{\"end\":46548,\"start\":46543},{\"end\":46567,\"start\":46557},{\"end\":46580,\"start\":46569},{\"end\":46608,\"start\":46590},{\"end\":46624,\"start\":46616},{\"end\":46640,\"start\":46633},{\"end\":46657,\"start\":46648},{\"end\":46667,\"start\":46659},{\"end\":47353,\"start\":47348},{\"end\":47368,\"start\":47364},{\"end\":47380,\"start\":47375},{\"end\":47397,\"start\":47390},{\"end\":47411,\"start\":47405},{\"end\":47430,\"start\":47422},{\"end\":47450,\"start\":47439},{\"end\":47464,\"start\":47459},{\"end\":47479,\"start\":47473},{\"end\":47494,\"start\":47488},{\"end\":47512,\"start\":47505},{\"end\":47532,\"start\":47520},{\"end\":47550,\"start\":47543},{\"end\":47564,\"start\":47556},{\"end\":47577,\"start\":47572},{\"end\":47592,\"start\":47586},{\"end\":47610,\"start\":47603},{\"end\":47622,\"start\":47620},{\"end\":47638,\"start\":47632},{\"end\":47657,\"start\":47652},{\"end\":47668,\"start\":47664},{\"end\":47681,\"start\":47675},{\"end\":47697,\"start\":47691},{\"end\":48520,\"start\":48517},{\"end\":48529,\"start\":48526},{\"end\":48538,\"start\":48535},{\"end\":48552,\"start\":48546},{\"end\":48741,\"start\":48732},{\"end\":48751,\"start\":48746},{\"end\":48763,\"start\":48760},{\"end\":48779,\"start\":48772},{\"end\":48788,\"start\":48784},{\"end\":48801,\"start\":48798},{\"end\":48813,\"start\":48808},{\"end\":48824,\"start\":48822},{\"end\":48838,\"start\":48834},{\"end\":48859,\"start\":48851},{\"end\":48874,\"start\":48868},{\"end\":48882,\"start\":48876},{\"end\":48898,\"start\":48890},{\"end\":48909,\"start\":48907},{\"end\":48920,\"start\":48917},{\"end\":48935,\"start\":48929},{\"end\":48951,\"start\":48947},{\"end\":48969,\"start\":48960},{\"end\":48984,\"start\":48978},{\"end\":48992,\"start\":48986},{\"end\":49354,\"start\":49350},{\"end\":49370,\"start\":49365},{\"end\":49383,\"start\":49380},{\"end\":49392,\"start\":49390},{\"end\":49405,\"start\":49399},{\"end\":49415,\"start\":49412},{\"end\":49427,\"start\":49423},{\"end\":49440,\"start\":49434},{\"end\":49455,\"start\":49448},{\"end\":49468,\"start\":49464},{\"end\":49478,\"start\":49476},{\"end\":49483,\"start\":49480},{\"end\":49792,\"start\":49789},{\"end\":49807,\"start\":49801},{\"end\":49821,\"start\":49816},{\"end\":49832,\"start\":49827},{\"end\":49847,\"start\":49842},{\"end\":49857,\"start\":49853},{\"end\":49869,\"start\":49864},{\"end\":49883,\"start\":49877},{\"end\":49890,\"start\":49885},{\"end\":50114,\"start\":50108},{\"end\":50123,\"start\":50120},{\"end\":50134,\"start\":50129},{\"end\":50149,\"start\":50142},{\"end\":50166,\"start\":50161},{\"end\":50183,\"start\":50174},{\"end\":50198,\"start\":50192},{\"end\":50214,\"start\":50209},{\"end\":50228,\"start\":50221},{\"end\":50242,\"start\":50237},{\"end\":50254,\"start\":50252},{\"end\":50268,\"start\":50263},{\"end\":50285,\"start\":50277},{\"end\":50302,\"start\":50294},{\"end\":50320,\"start\":50311},{\"end\":50336,\"start\":50330},{\"end\":50353,\"start\":50344},{\"end\":50367,\"start\":50360},{\"end\":50378,\"start\":50369},{\"end\":50918,\"start\":50914},{\"end\":50928,\"start\":50924},{\"end\":50940,\"start\":50937},{\"end\":50950,\"start\":50942},{\"end\":50958,\"start\":50955},{\"end\":50971,\"start\":50969},{\"end\":50984,\"start\":50980},{\"end\":50999,\"start\":50994},{\"end\":51009,\"start\":51005},{\"end\":51014,\"start\":51011},{\"end\":51019,\"start\":51016},{\"end\":51372,\"start\":51367},{\"end\":51384,\"start\":51380},{\"end\":51406,\"start\":51394},{\"end\":51419,\"start\":51414},{\"end\":51432,\"start\":51426},{\"end\":51728,\"start\":51722},{\"end\":51737,\"start\":51735},{\"end\":51755,\"start\":51749},{\"end\":51767,\"start\":51764},{\"end\":51776,\"start\":51773},{\"end\":51793,\"start\":51786},{\"end\":51804,\"start\":51801},{\"end\":51822,\"start\":51816},{\"end\":52130,\"start\":52122},{\"end\":52146,\"start\":52139},{\"end\":52157,\"start\":52153},{\"end\":52170,\"start\":52164},{\"end\":52427,\"start\":52425},{\"end\":52440,\"start\":52436},{\"end\":52456,\"start\":52450},{\"end\":52474,\"start\":52465},{\"end\":52486,\"start\":52484},{\"end\":52498,\"start\":52494},{\"end\":52507,\"start\":52503},{\"end\":52520,\"start\":52516},{\"end\":52777,\"start\":52775},{\"end\":52787,\"start\":52784},{\"end\":52803,\"start\":52799},{\"end\":52815,\"start\":52812},{\"end\":52829,\"start\":52824},{\"end\":52840,\"start\":52837},{\"end\":53083,\"start\":53079},{\"end\":53105,\"start\":53099},{\"end\":53114,\"start\":53107},{\"end\":53361,\"start\":53353},{\"end\":53373,\"start\":53366},{\"end\":53818,\"start\":53813},{\"end\":53832,\"start\":53826},{\"end\":53847,\"start\":53842},{\"end\":53864,\"start\":53857},{\"end\":53881,\"start\":53876},{\"end\":53898,\"start\":53890},{\"end\":53917,\"start\":53907},{\"end\":54204,\"start\":54202},{\"end\":54215,\"start\":54213},{\"end\":54224,\"start\":54222},{\"end\":54239,\"start\":54235},{\"end\":54256,\"start\":54248},{\"end\":54272,\"start\":54269},{\"end\":54536,\"start\":54534},{\"end\":54547,\"start\":54545},{\"end\":54564,\"start\":54556},{\"end\":54576,\"start\":54573},{\"end\":54895,\"start\":54893},{\"end\":54906,\"start\":54904},{\"end\":54921,\"start\":54916},{\"end\":54933,\"start\":54930},{\"end\":55225,\"start\":55223},{\"end\":55248,\"start\":55239},{\"end\":55266,\"start\":55259},{\"end\":55279,\"start\":55275},{\"end\":55294,\"start\":55289},{\"end\":55315,\"start\":55312},{\"end\":55598,\"start\":55595},{\"end\":55613,\"start\":55608},{\"end\":55629,\"start\":55621},{\"end\":55641,\"start\":55637},{\"end\":55656,\"start\":55650},{\"end\":55670,\"start\":55663},{\"end\":55684,\"start\":55678},{\"end\":55704,\"start\":55697},{\"end\":55941,\"start\":55938},{\"end\":55965,\"start\":55947},{\"end\":55980,\"start\":55973},{\"end\":56214,\"start\":56211},{\"end\":56227,\"start\":56225},{\"end\":56240,\"start\":56238},{\"end\":56254,\"start\":56251},{\"end\":56455,\"start\":56445},{\"end\":56469,\"start\":56463},{\"end\":56642,\"start\":56640},{\"end\":56659,\"start\":56652},{\"end\":56676,\"start\":56668},{\"end\":56689,\"start\":56683},{\"end\":56701,\"start\":56698},{\"end\":56966,\"start\":56964},{\"end\":56982,\"start\":56976},{\"end\":56992,\"start\":56989},{\"end\":57003,\"start\":57000},{\"end\":57018,\"start\":57013},{\"end\":57033,\"start\":57030},{\"end\":57049,\"start\":57042},{\"end\":57062,\"start\":57057},{\"end\":57077,\"start\":57071},{\"end\":57431,\"start\":57429},{\"end\":57442,\"start\":57439},{\"end\":57454,\"start\":57450},{\"end\":57464,\"start\":57461},{\"end\":57477,\"start\":57473},{\"end\":57488,\"start\":57483},{\"end\":57497,\"start\":57495},{\"end\":57512,\"start\":57507},{\"end\":57527,\"start\":57524},{\"end\":57920,\"start\":57914},{\"end\":57940,\"start\":57931},{\"end\":57953,\"start\":57946},{\"end\":57971,\"start\":57963},{\"end\":58225,\"start\":58219},{\"end\":58243,\"start\":58236},{\"end\":58262,\"start\":58257},{\"end\":58283,\"start\":58272},{\"end\":58551,\"start\":58543},{\"end\":58564,\"start\":58559},{\"end\":58577,\"start\":58571},{\"end\":58591,\"start\":58588},{\"end\":59117,\"start\":59111},{\"end\":59131,\"start\":59124},{\"end\":59145,\"start\":59138},{\"end\":59160,\"start\":59157},{\"end\":59175,\"start\":59169},{\"end\":59191,\"start\":59185},{\"end\":59203,\"start\":59199},{\"end\":59211,\"start\":59209},{\"end\":59224,\"start\":59221},{\"end\":59610,\"start\":59606},{\"end\":59625,\"start\":59619},{\"end\":59639,\"start\":59633},{\"end\":59655,\"start\":59651},{\"end\":59673,\"start\":59665},{\"end\":59688,\"start\":59680},{\"end\":59705,\"start\":59698},{\"end\":59722,\"start\":59714},{\"end\":59733,\"start\":59729},{\"end\":59744,\"start\":59741},{\"end\":59757,\"start\":59753},{\"end\":59768,\"start\":59766},{\"end\":59784,\"start\":59777},{\"end\":59805,\"start\":59792},{\"end\":59824,\"start\":59815},{\"end\":59836,\"start\":59833},{\"end\":59847,\"start\":59838},{\"end\":59856,\"start\":59851},{\"end\":59873,\"start\":59868},{\"end\":59889,\"start\":59884},{\"end\":59901,\"start\":59896},{\"end\":59912,\"start\":59903},{\"end\":59923,\"start\":59918},{\"end\":59936,\"start\":59932},{\"end\":59950,\"start\":59944},{\"end\":59956,\"start\":59952},{\"end\":60998,\"start\":60991},{\"end\":61017,\"start\":61007},{\"end\":61036,\"start\":61031},{\"end\":61052,\"start\":61046},{\"end\":61070,\"start\":61062},{\"end\":61565,\"start\":61561},{\"end\":61574,\"start\":61572},{\"end\":61585,\"start\":61581},{\"end\":61593,\"start\":61591},{\"end\":61935,\"start\":61928},{\"end\":61948,\"start\":61946},{\"end\":61965,\"start\":61957},{\"end\":61982,\"start\":61977},{\"end\":62185,\"start\":62180},{\"end\":62201,\"start\":62193},{\"end\":62212,\"start\":62208},{\"end\":62222,\"start\":62217},{\"end\":62235,\"start\":62231},{\"end\":62248,\"start\":62242},{\"end\":62265,\"start\":62257},{\"end\":62516,\"start\":62511},{\"end\":62534,\"start\":62525},{\"end\":62548,\"start\":62543},{\"end\":62561,\"start\":62555},{\"end\":62573,\"start\":62571},{\"end\":62590,\"start\":62582},{\"end\":62603,\"start\":62598},{\"end\":62626,\"start\":62617},{\"end\":62933,\"start\":62926},{\"end\":62949,\"start\":62943},{\"end\":62966,\"start\":62959},{\"end\":62983,\"start\":62975},{\"end\":63003,\"start\":62996},{\"end\":63021,\"start\":63014},{\"end\":63045,\"start\":63029},{\"end\":63057,\"start\":63052},{\"end\":63072,\"start\":63066},{\"end\":63088,\"start\":63083},{\"end\":63106,\"start\":63097},{\"end\":63122,\"start\":63116},{\"end\":63139,\"start\":63134},{\"end\":63147,\"start\":63141},{\"end\":63599,\"start\":63579},{\"end\":63613,\"start\":63606},{\"end\":63621,\"start\":63615},{\"end\":63991,\"start\":63987},{\"end\":64007,\"start\":64003},{\"end\":64019,\"start\":64017},{\"end\":64030,\"start\":64028},{\"end\":64041,\"start\":64038},{\"end\":64050,\"start\":64047},{\"end\":64063,\"start\":64060},{\"end\":64071,\"start\":64068},{\"end\":64084,\"start\":64080},{\"end\":64365,\"start\":64361},{\"end\":64380,\"start\":64375},{\"end\":64396,\"start\":64390},{\"end\":64407,\"start\":64404},{\"end\":64421,\"start\":64416},{\"end\":64438,\"start\":64430},{\"end\":64459,\"start\":64449},{\"end\":64812,\"start\":64808},{\"end\":64828,\"start\":64822},{\"end\":64852,\"start\":64836},{\"end\":64867,\"start\":64862},{\"end\":64885,\"start\":64878},{\"end\":64899,\"start\":64895},{\"end\":64912,\"start\":64907},{\"end\":64938,\"start\":64919},{\"end\":64956,\"start\":64947},{\"end\":64968,\"start\":64964},{\"end\":64983,\"start\":64977},{\"end\":65006,\"start\":64993},{\"end\":65018,\"start\":65015},{\"end\":65033,\"start\":65026},{\"end\":65048,\"start\":65042},{\"end\":65064,\"start\":65056},{\"end\":65078,\"start\":65072},{\"end\":65091,\"start\":65086},{\"end\":66135,\"start\":66132},{\"end\":66150,\"start\":66145},{\"end\":66161,\"start\":66154},{\"end\":66174,\"start\":66170},{\"end\":66189,\"start\":66186},{\"end\":66199,\"start\":66197},{\"end\":66211,\"start\":66205},{\"end\":66224,\"start\":66222},{\"end\":66236,\"start\":66233},{\"end\":66240,\"start\":66238},{\"end\":66603,\"start\":66601},{\"end\":66614,\"start\":66610},{\"end\":66624,\"start\":66620},{\"end\":66632,\"start\":66630},{\"end\":67119,\"start\":67117},{\"end\":67130,\"start\":67126},{\"end\":67142,\"start\":67137},{\"end\":67406,\"start\":67402},{\"end\":67421,\"start\":67416},{\"end\":67434,\"start\":67429},{\"end\":67447,\"start\":67441},{\"end\":67464,\"start\":67458},{\"end\":67786,\"start\":67784},{\"end\":67798,\"start\":67796},{\"end\":67809,\"start\":67807},{\"end\":67819,\"start\":67817},{\"end\":67829,\"start\":67826},{\"end\":67838,\"start\":67834},{\"end\":67851,\"start\":67847},{\"end\":67861,\"start\":67859},{\"end\":67876,\"start\":67873},{\"end\":67886,\"start\":67883},{\"end\":67900,\"start\":67898},{\"end\":67913,\"start\":67911},{\"end\":67926,\"start\":67922},{\"end\":67940,\"start\":67936},{\"end\":67950,\"start\":67948},{\"end\":68380,\"start\":68375},{\"end\":68391,\"start\":68388},{\"end\":68405,\"start\":68399},{\"end\":68424,\"start\":68413},{\"end\":68847,\"start\":68844},{\"end\":68857,\"start\":68853},{\"end\":68872,\"start\":68868},{\"end\":68882,\"start\":68880},{\"end\":68901,\"start\":68892}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":45702,\"start\":45642},{\"attributes\":{\"id\":\"b1\"},\"end\":45773,\"start\":45704},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":56517630},\"end\":46179,\"start\":45775},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":248476411},\"end\":47340,\"start\":46181},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b4\"},\"end\":48452,\"start\":47342},{\"attributes\":{\"doi\":\"arXiv:2102.02779\",\"id\":\"b5\"},\"end\":48728,\"start\":48454},{\"attributes\":{\"id\":\"b6\"},\"end\":49293,\"start\":48730},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b7\"},\"end\":49763,\"start\":49295},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1820614},\"end\":50053,\"start\":49765},{\"attributes\":{\"id\":\"b9\"},\"end\":50827,\"start\":50055},{\"attributes\":{\"doi\":\"abs/2211.07636\",\"id\":\"b10\"},\"end\":51260,\"start\":50829},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8081284},\"end\":51644,\"start\":51262},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3831582},\"end\":52040,\"start\":51646},{\"attributes\":{\"doi\":\"abs/2212.09689\",\"id\":\"b13\",\"matched_paper_id\":254853659},\"end\":52362,\"start\":52042},{\"attributes\":{\"doi\":\"ICLR, 2022. 9\",\"id\":\"b14\"},\"end\":52714,\"start\":52364},{\"attributes\":{\"id\":\"b15\"},\"end\":52986,\"start\":52716},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":152282269},\"end\":53277,\"start\":52988},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8517067},\"end\":53733,\"start\":53279},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":218581273},\"end\":54142,\"start\":53735},{\"attributes\":{\"id\":\"b19\"},\"end\":54421,\"start\":54144},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":256390509},\"end\":54778,\"start\":54423},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":246411402},\"end\":55123,\"start\":54780},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":236034189},\"end\":55541,\"start\":55125},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14113767},\"end\":55903,\"start\":55543},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":248496506},\"end\":56201,\"start\":55905},{\"attributes\":{\"id\":\"b25\"},\"end\":56399,\"start\":56203},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53592270},\"end\":56566,\"start\":56401},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":208637516},\"end\":56868,\"start\":56568},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":252383606},\"end\":57333,\"start\":56870},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":239885946},\"end\":57828,\"start\":57335},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":173991173},\"end\":58149,\"start\":57830},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":209413409},\"end\":58454,\"start\":58151},{\"attributes\":{\"id\":\"b32\"},\"end\":58896,\"start\":58456},{\"attributes\":{\"doi\":\"abs/2303.08774\",\"id\":\"b33\"},\"end\":59020,\"start\":58898},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":204838007},\"end\":59530,\"start\":59022},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":239009562},\"end\":60909,\"start\":59532},{\"attributes\":{\"id\":\"b36\"},\"end\":61449,\"start\":60911},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":257353536},\"end\":61850,\"start\":61451},{\"attributes\":{\"id\":\"b38\"},\"end\":62134,\"start\":61852},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":85553602},\"end\":62448,\"start\":62136},{\"attributes\":{\"id\":\"b40\"},\"end\":62865,\"start\":62450},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b41\"},\"end\":63513,\"start\":62867},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":9026666},\"end\":63907,\"start\":63515},{\"attributes\":{\"id\":\"b43\"},\"end\":64278,\"start\":63909},{\"attributes\":{\"doi\":\"abs/2212.10560, 2022. 9\",\"id\":\"b44\",\"matched_paper_id\":254877310},\"end\":64707,\"start\":64280},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":253098274},\"end\":66074,\"start\":64709},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":237416585},\"end\":66459,\"start\":66076},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3864050},\"end\":67027,\"start\":66461},{\"attributes\":{\"doi\":\"abs/2212.10773\",\"id\":\"b48\",\"matched_paper_id\":254926784},\"end\":67319,\"start\":67029},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":227238996},\"end\":67664,\"start\":67321},{\"attributes\":{\"id\":\"b50\"},\"end\":68249,\"start\":67666},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":3104920},\"end\":68748,\"start\":68251},{\"attributes\":{\"id\":\"b52\"},\"end\":69072,\"start\":68750}]", "bib_title": "[{\"end\":45815,\"start\":45775},{\"end\":46236,\"start\":46181},{\"end\":49778,\"start\":49765},{\"end\":51360,\"start\":51262},{\"end\":51714,\"start\":51646},{\"end\":52117,\"start\":52042},{\"end\":53075,\"start\":52988},{\"end\":53344,\"start\":53279},{\"end\":53805,\"start\":53735},{\"end\":54525,\"start\":54423},{\"end\":54884,\"start\":54780},{\"end\":55214,\"start\":55125},{\"end\":55584,\"start\":55543},{\"end\":55929,\"start\":55905},{\"end\":56438,\"start\":56401},{\"end\":56631,\"start\":56568},{\"end\":56958,\"start\":56870},{\"end\":57423,\"start\":57335},{\"end\":57904,\"start\":57830},{\"end\":58211,\"start\":58151},{\"end\":59103,\"start\":59022},{\"end\":59597,\"start\":59532},{\"end\":61551,\"start\":61451},{\"end\":62168,\"start\":62136},{\"end\":63566,\"start\":63515},{\"end\":64351,\"start\":64280},{\"end\":64798,\"start\":64709},{\"end\":66124,\"start\":66076},{\"end\":66592,\"start\":66461},{\"end\":67107,\"start\":67029},{\"end\":67392,\"start\":67321},{\"end\":68367,\"start\":68251}]", "bib_author": "[{\"end\":45653,\"start\":45644},{\"end\":45714,\"start\":45706},{\"end\":45832,\"start\":45817},{\"end\":45845,\"start\":45832},{\"end\":45857,\"start\":45845},{\"end\":45870,\"start\":45857},{\"end\":45884,\"start\":45870},{\"end\":45898,\"start\":45884},{\"end\":45911,\"start\":45898},{\"end\":45924,\"start\":45911},{\"end\":45936,\"start\":45924},{\"end\":45952,\"start\":45936},{\"end\":46266,\"start\":46238},{\"end\":46283,\"start\":46266},{\"end\":46296,\"start\":46283},{\"end\":46308,\"start\":46296},{\"end\":46319,\"start\":46308},{\"end\":46333,\"start\":46319},{\"end\":46346,\"start\":46333},{\"end\":46364,\"start\":46346},{\"end\":46382,\"start\":46364},{\"end\":46398,\"start\":46382},{\"end\":46410,\"start\":46398},{\"end\":46429,\"start\":46410},{\"end\":46442,\"start\":46429},{\"end\":46454,\"start\":46442},{\"end\":46465,\"start\":46454},{\"end\":46486,\"start\":46465},{\"end\":46496,\"start\":46486},{\"end\":46505,\"start\":46496},{\"end\":46523,\"start\":46505},{\"end\":46538,\"start\":46523},{\"end\":46550,\"start\":46538},{\"end\":46569,\"start\":46550},{\"end\":46582,\"start\":46569},{\"end\":46610,\"start\":46582},{\"end\":46626,\"start\":46610},{\"end\":46642,\"start\":46626},{\"end\":46659,\"start\":46642},{\"end\":46669,\"start\":46659},{\"end\":47355,\"start\":47342},{\"end\":47370,\"start\":47355},{\"end\":47382,\"start\":47370},{\"end\":47399,\"start\":47382},{\"end\":47413,\"start\":47399},{\"end\":47432,\"start\":47413},{\"end\":47452,\"start\":47432},{\"end\":47466,\"start\":47452},{\"end\":47481,\"start\":47466},{\"end\":47496,\"start\":47481},{\"end\":47514,\"start\":47496},{\"end\":47534,\"start\":47514},{\"end\":47552,\"start\":47534},{\"end\":47566,\"start\":47552},{\"end\":47579,\"start\":47566},{\"end\":47594,\"start\":47579},{\"end\":47612,\"start\":47594},{\"end\":47624,\"start\":47612},{\"end\":47640,\"start\":47624},{\"end\":47659,\"start\":47640},{\"end\":47670,\"start\":47659},{\"end\":47683,\"start\":47670},{\"end\":47699,\"start\":47683},{\"end\":48522,\"start\":48510},{\"end\":48531,\"start\":48522},{\"end\":48540,\"start\":48531},{\"end\":48554,\"start\":48540},{\"end\":48743,\"start\":48732},{\"end\":48753,\"start\":48743},{\"end\":48765,\"start\":48753},{\"end\":48781,\"start\":48765},{\"end\":48790,\"start\":48781},{\"end\":48803,\"start\":48790},{\"end\":48815,\"start\":48803},{\"end\":48826,\"start\":48815},{\"end\":48840,\"start\":48826},{\"end\":48861,\"start\":48840},{\"end\":48876,\"start\":48861},{\"end\":48884,\"start\":48876},{\"end\":48900,\"start\":48884},{\"end\":48911,\"start\":48900},{\"end\":48922,\"start\":48911},{\"end\":48937,\"start\":48922},{\"end\":48953,\"start\":48937},{\"end\":48971,\"start\":48953},{\"end\":48986,\"start\":48971},{\"end\":48994,\"start\":48986},{\"end\":49356,\"start\":49342},{\"end\":49372,\"start\":49356},{\"end\":49385,\"start\":49372},{\"end\":49394,\"start\":49385},{\"end\":49407,\"start\":49394},{\"end\":49417,\"start\":49407},{\"end\":49429,\"start\":49417},{\"end\":49442,\"start\":49429},{\"end\":49457,\"start\":49442},{\"end\":49470,\"start\":49457},{\"end\":49480,\"start\":49470},{\"end\":49485,\"start\":49480},{\"end\":49794,\"start\":49780},{\"end\":49809,\"start\":49794},{\"end\":49823,\"start\":49809},{\"end\":49834,\"start\":49823},{\"end\":49849,\"start\":49834},{\"end\":49859,\"start\":49849},{\"end\":49871,\"start\":49859},{\"end\":49885,\"start\":49871},{\"end\":49892,\"start\":49885},{\"end\":50116,\"start\":50102},{\"end\":50125,\"start\":50116},{\"end\":50136,\"start\":50125},{\"end\":50151,\"start\":50136},{\"end\":50168,\"start\":50151},{\"end\":50185,\"start\":50168},{\"end\":50200,\"start\":50185},{\"end\":50216,\"start\":50200},{\"end\":50230,\"start\":50216},{\"end\":50244,\"start\":50230},{\"end\":50256,\"start\":50244},{\"end\":50270,\"start\":50256},{\"end\":50287,\"start\":50270},{\"end\":50304,\"start\":50287},{\"end\":50322,\"start\":50304},{\"end\":50338,\"start\":50322},{\"end\":50355,\"start\":50338},{\"end\":50369,\"start\":50355},{\"end\":50380,\"start\":50369},{\"end\":50920,\"start\":50908},{\"end\":50930,\"start\":50920},{\"end\":50942,\"start\":50930},{\"end\":50952,\"start\":50942},{\"end\":50960,\"start\":50952},{\"end\":50973,\"start\":50960},{\"end\":50986,\"start\":50973},{\"end\":51001,\"start\":50986},{\"end\":51011,\"start\":51001},{\"end\":51016,\"start\":51011},{\"end\":51021,\"start\":51016},{\"end\":51374,\"start\":51362},{\"end\":51386,\"start\":51374},{\"end\":51408,\"start\":51386},{\"end\":51421,\"start\":51408},{\"end\":51434,\"start\":51421},{\"end\":51730,\"start\":51716},{\"end\":51739,\"start\":51730},{\"end\":51757,\"start\":51739},{\"end\":51769,\"start\":51757},{\"end\":51778,\"start\":51769},{\"end\":51795,\"start\":51778},{\"end\":51806,\"start\":51795},{\"end\":51824,\"start\":51806},{\"end\":52132,\"start\":52119},{\"end\":52148,\"start\":52132},{\"end\":52159,\"start\":52148},{\"end\":52172,\"start\":52159},{\"end\":52429,\"start\":52416},{\"end\":52442,\"start\":52429},{\"end\":52458,\"start\":52442},{\"end\":52476,\"start\":52458},{\"end\":52488,\"start\":52476},{\"end\":52500,\"start\":52488},{\"end\":52509,\"start\":52500},{\"end\":52522,\"start\":52509},{\"end\":52779,\"start\":52769},{\"end\":52789,\"start\":52779},{\"end\":52805,\"start\":52789},{\"end\":52817,\"start\":52805},{\"end\":52831,\"start\":52817},{\"end\":52842,\"start\":52831},{\"end\":53085,\"start\":53077},{\"end\":53107,\"start\":53085},{\"end\":53116,\"start\":53107},{\"end\":53363,\"start\":53346},{\"end\":53375,\"start\":53363},{\"end\":53820,\"start\":53807},{\"end\":53834,\"start\":53820},{\"end\":53849,\"start\":53834},{\"end\":53866,\"start\":53849},{\"end\":53883,\"start\":53866},{\"end\":53900,\"start\":53883},{\"end\":53919,\"start\":53900},{\"end\":54206,\"start\":54195},{\"end\":54217,\"start\":54206},{\"end\":54226,\"start\":54217},{\"end\":54241,\"start\":54226},{\"end\":54258,\"start\":54241},{\"end\":54274,\"start\":54258},{\"end\":54538,\"start\":54527},{\"end\":54549,\"start\":54538},{\"end\":54566,\"start\":54549},{\"end\":54578,\"start\":54566},{\"end\":54897,\"start\":54886},{\"end\":54908,\"start\":54897},{\"end\":54923,\"start\":54908},{\"end\":54935,\"start\":54923},{\"end\":55227,\"start\":55216},{\"end\":55250,\"start\":55227},{\"end\":55268,\"start\":55250},{\"end\":55281,\"start\":55268},{\"end\":55296,\"start\":55281},{\"end\":55317,\"start\":55296},{\"end\":55600,\"start\":55586},{\"end\":55615,\"start\":55600},{\"end\":55631,\"start\":55615},{\"end\":55643,\"start\":55631},{\"end\":55658,\"start\":55643},{\"end\":55672,\"start\":55658},{\"end\":55686,\"start\":55672},{\"end\":55706,\"start\":55686},{\"end\":55943,\"start\":55931},{\"end\":55967,\"start\":55943},{\"end\":55982,\"start\":55967},{\"end\":56216,\"start\":56203},{\"end\":56229,\"start\":56216},{\"end\":56242,\"start\":56229},{\"end\":56256,\"start\":56242},{\"end\":56457,\"start\":56440},{\"end\":56471,\"start\":56457},{\"end\":56644,\"start\":56633},{\"end\":56661,\"start\":56644},{\"end\":56678,\"start\":56661},{\"end\":56691,\"start\":56678},{\"end\":56703,\"start\":56691},{\"end\":56968,\"start\":56960},{\"end\":56984,\"start\":56968},{\"end\":56994,\"start\":56984},{\"end\":57005,\"start\":56994},{\"end\":57020,\"start\":57005},{\"end\":57035,\"start\":57020},{\"end\":57051,\"start\":57035},{\"end\":57064,\"start\":57051},{\"end\":57079,\"start\":57064},{\"end\":57433,\"start\":57425},{\"end\":57444,\"start\":57433},{\"end\":57456,\"start\":57444},{\"end\":57466,\"start\":57456},{\"end\":57479,\"start\":57466},{\"end\":57490,\"start\":57479},{\"end\":57499,\"start\":57490},{\"end\":57514,\"start\":57499},{\"end\":57529,\"start\":57514},{\"end\":57922,\"start\":57906},{\"end\":57942,\"start\":57922},{\"end\":57955,\"start\":57942},{\"end\":57973,\"start\":57955},{\"end\":58227,\"start\":58213},{\"end\":58245,\"start\":58227},{\"end\":58264,\"start\":58245},{\"end\":58285,\"start\":58264},{\"end\":58553,\"start\":58535},{\"end\":58566,\"start\":58553},{\"end\":58579,\"start\":58566},{\"end\":58593,\"start\":58579},{\"end\":59119,\"start\":59105},{\"end\":59133,\"start\":59119},{\"end\":59147,\"start\":59133},{\"end\":59162,\"start\":59147},{\"end\":59177,\"start\":59162},{\"end\":59193,\"start\":59177},{\"end\":59205,\"start\":59193},{\"end\":59213,\"start\":59205},{\"end\":59226,\"start\":59213},{\"end\":59612,\"start\":59599},{\"end\":59627,\"start\":59612},{\"end\":59641,\"start\":59627},{\"end\":59657,\"start\":59641},{\"end\":59675,\"start\":59657},{\"end\":59690,\"start\":59675},{\"end\":59707,\"start\":59690},{\"end\":59724,\"start\":59707},{\"end\":59735,\"start\":59724},{\"end\":59746,\"start\":59735},{\"end\":59759,\"start\":59746},{\"end\":59770,\"start\":59759},{\"end\":59786,\"start\":59770},{\"end\":59807,\"start\":59786},{\"end\":59826,\"start\":59807},{\"end\":59838,\"start\":59826},{\"end\":59849,\"start\":59838},{\"end\":59858,\"start\":59849},{\"end\":59875,\"start\":59858},{\"end\":59891,\"start\":59875},{\"end\":59903,\"start\":59891},{\"end\":59914,\"start\":59903},{\"end\":59925,\"start\":59914},{\"end\":59938,\"start\":59925},{\"end\":59952,\"start\":59938},{\"end\":59958,\"start\":59952},{\"end\":61000,\"start\":60984},{\"end\":61019,\"start\":61000},{\"end\":61038,\"start\":61019},{\"end\":61054,\"start\":61038},{\"end\":61072,\"start\":61054},{\"end\":61567,\"start\":61553},{\"end\":61576,\"start\":61567},{\"end\":61587,\"start\":61576},{\"end\":61595,\"start\":61587},{\"end\":61937,\"start\":61920},{\"end\":61950,\"start\":61937},{\"end\":61967,\"start\":61950},{\"end\":61984,\"start\":61967},{\"end\":62187,\"start\":62170},{\"end\":62203,\"start\":62187},{\"end\":62214,\"start\":62203},{\"end\":62224,\"start\":62214},{\"end\":62237,\"start\":62224},{\"end\":62250,\"start\":62237},{\"end\":62267,\"start\":62250},{\"end\":62518,\"start\":62505},{\"end\":62536,\"start\":62518},{\"end\":62550,\"start\":62536},{\"end\":62563,\"start\":62550},{\"end\":62575,\"start\":62563},{\"end\":62592,\"start\":62575},{\"end\":62605,\"start\":62592},{\"end\":62628,\"start\":62605},{\"end\":62935,\"start\":62921},{\"end\":62951,\"start\":62935},{\"end\":62968,\"start\":62951},{\"end\":62985,\"start\":62968},{\"end\":63005,\"start\":62985},{\"end\":63023,\"start\":63005},{\"end\":63047,\"start\":63023},{\"end\":63059,\"start\":63047},{\"end\":63074,\"start\":63059},{\"end\":63090,\"start\":63074},{\"end\":63108,\"start\":63090},{\"end\":63124,\"start\":63108},{\"end\":63141,\"start\":63124},{\"end\":63149,\"start\":63141},{\"end\":63601,\"start\":63568},{\"end\":63615,\"start\":63601},{\"end\":63623,\"start\":63615},{\"end\":63993,\"start\":63978},{\"end\":64009,\"start\":63993},{\"end\":64021,\"start\":64009},{\"end\":64032,\"start\":64021},{\"end\":64043,\"start\":64032},{\"end\":64052,\"start\":64043},{\"end\":64065,\"start\":64052},{\"end\":64073,\"start\":64065},{\"end\":64086,\"start\":64073},{\"end\":64367,\"start\":64353},{\"end\":64382,\"start\":64367},{\"end\":64398,\"start\":64382},{\"end\":64409,\"start\":64398},{\"end\":64423,\"start\":64409},{\"end\":64440,\"start\":64423},{\"end\":64461,\"start\":64440},{\"end\":64814,\"start\":64800},{\"end\":64830,\"start\":64814},{\"end\":64854,\"start\":64830},{\"end\":64869,\"start\":64854},{\"end\":64887,\"start\":64869},{\"end\":64901,\"start\":64887},{\"end\":64914,\"start\":64901},{\"end\":64940,\"start\":64914},{\"end\":64958,\"start\":64940},{\"end\":64970,\"start\":64958},{\"end\":64985,\"start\":64970},{\"end\":65008,\"start\":64985},{\"end\":65020,\"start\":65008},{\"end\":65035,\"start\":65020},{\"end\":65050,\"start\":65035},{\"end\":65066,\"start\":65050},{\"end\":65080,\"start\":65066},{\"end\":65093,\"start\":65080},{\"end\":66137,\"start\":66126},{\"end\":66152,\"start\":66137},{\"end\":66163,\"start\":66152},{\"end\":66176,\"start\":66163},{\"end\":66191,\"start\":66176},{\"end\":66201,\"start\":66191},{\"end\":66213,\"start\":66201},{\"end\":66226,\"start\":66213},{\"end\":66238,\"start\":66226},{\"end\":66242,\"start\":66238},{\"end\":66605,\"start\":66594},{\"end\":66616,\"start\":66605},{\"end\":66626,\"start\":66616},{\"end\":66634,\"start\":66626},{\"end\":67121,\"start\":67109},{\"end\":67132,\"start\":67121},{\"end\":67144,\"start\":67132},{\"end\":67408,\"start\":67394},{\"end\":67423,\"start\":67408},{\"end\":67436,\"start\":67423},{\"end\":67449,\"start\":67436},{\"end\":67466,\"start\":67449},{\"end\":67788,\"start\":67776},{\"end\":67800,\"start\":67788},{\"end\":67811,\"start\":67800},{\"end\":67821,\"start\":67811},{\"end\":67831,\"start\":67821},{\"end\":67840,\"start\":67831},{\"end\":67853,\"start\":67840},{\"end\":67863,\"start\":67853},{\"end\":67878,\"start\":67863},{\"end\":67888,\"start\":67878},{\"end\":67902,\"start\":67888},{\"end\":67915,\"start\":67902},{\"end\":67928,\"start\":67915},{\"end\":67942,\"start\":67928},{\"end\":67952,\"start\":67942},{\"end\":68382,\"start\":68369},{\"end\":68393,\"start\":68382},{\"end\":68407,\"start\":68393},{\"end\":68426,\"start\":68407},{\"end\":68849,\"start\":68838},{\"end\":68859,\"start\":68849},{\"end\":68874,\"start\":68859},{\"end\":68884,\"start\":68874},{\"end\":68903,\"start\":68884}]", "bib_venue": "[{\"end\":47876,\"start\":47788},{\"end\":53530,\"start\":53461},{\"end\":60206,\"start\":60146},{\"end\":66753,\"start\":66702},{\"end\":45956,\"start\":45952},{\"end\":46682,\"start\":46669},{\"end\":47786,\"start\":47715},{\"end\":48508,\"start\":48454},{\"end\":49340,\"start\":49295},{\"end\":49896,\"start\":49892},{\"end\":50100,\"start\":50055},{\"end\":50906,\"start\":50829},{\"end\":51438,\"start\":51434},{\"end\":51831,\"start\":51824},{\"end\":52191,\"start\":52186},{\"end\":52414,\"start\":52364},{\"end\":52767,\"start\":52716},{\"end\":53120,\"start\":53116},{\"end\":53459,\"start\":53375},{\"end\":53926,\"start\":53919},{\"end\":54193,\"start\":54144},{\"end\":54582,\"start\":54578},{\"end\":54939,\"start\":54935},{\"end\":55324,\"start\":55317},{\"end\":55710,\"start\":55706},{\"end\":56043,\"start\":55982},{\"end\":56293,\"start\":56256},{\"end\":56475,\"start\":56471},{\"end\":56707,\"start\":56703},{\"end\":57092,\"start\":57079},{\"end\":57569,\"start\":57529},{\"end\":57977,\"start\":57973},{\"end\":58290,\"start\":58285},{\"end\":58533,\"start\":58456},{\"end\":58951,\"start\":58914},{\"end\":59266,\"start\":59226},{\"end\":59962,\"start\":59958},{\"end\":60982,\"start\":60911},{\"end\":61641,\"start\":61595},{\"end\":61918,\"start\":61852},{\"end\":62271,\"start\":62267},{\"end\":62503,\"start\":62450},{\"end\":62919,\"start\":62867},{\"end\":63693,\"start\":63623},{\"end\":63976,\"start\":63909},{\"end\":64489,\"start\":64484},{\"end\":65098,\"start\":65093},{\"end\":66258,\"start\":66242},{\"end\":66700,\"start\":66634},{\"end\":67163,\"start\":67158},{\"end\":67470,\"start\":67466},{\"end\":67774,\"start\":67666},{\"end\":68487,\"start\":68426},{\"end\":68836,\"start\":68750}]"}}}, "year": 2023, "month": 12, "day": 17}