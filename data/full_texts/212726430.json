{"id": 212726430, "updated": "2023-10-06 22:49:47.859", "metadata": {"title": "Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics", "authors": "[{\"first\":\"Yuezun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Pu\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Honggang\",\"last\":\"Qi\",\"middle\":[]},{\"first\":\"Siwei\",\"last\":\"Lyu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for large-scale datasets. However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "1909.12962", "mag": "3034713808", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiYSQL20", "doi": "10.1109/cvpr42600.2020.00327"}}, "content": {"source": {"pdf_hash": "300d08e8f5c310c2b194b7eb94398e480994d5cc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.12962v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1909.12962", "status": "GREEN"}}, "grobid": {"id": "fd31eca2220b5073992831c80ad1568ccb1e0076", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/300d08e8f5c310c2b194b7eb94398e480994d5cc.txt", "contents": "\nCeleb-DF: A Large-scale Challenging Dataset for DeepFake Forensics\n\n\nYuezun Li \nUniversity at Albany\nState University of New York\nUSA\n\nXin Yang \nUniversity at Albany\nState University of New York\nUSA\n\nPu Sun \nUniversity of Chinese Academy of Sciences\nChina\n\nHonggang Qi \nUniversity of Chinese Academy of Sciences\nChina\n\nSiwei Lyu \nUniversity at Albany\nState University of New York\nUSA\n\nCeleb-DF: A Large-scale Challenging Dataset for DeepFake Forensics\n\nAI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for large-scale datasets. However, current DeepFake datasets suffer from low visual quality and do not resemble Deep-Fake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5, 639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF.\n\nIntroduction\n\nA recent twist to the disconcerting problem of online disinformation is falsified videos created by AI technologies, in particular, deep neural networks (DNNs). Although fabrication and manipulation of digital images and videos are not new [16], the use of DNNs has made the process to create convincing fake videos increasingly easier and faster.\n\nOne particular type of DNN-based fake videos, commonly known as DeepFakes, has recently drawn much attention. In a DeepFake video, the faces of a target individual are replaced by the faces of a donor individual synthesized by DNN models, retaining the target's facial expressions and head poses. Since faces are intrinsically associated with identity, well-crafted DeepFakes can create illusions of a person's presence and activities that do not occur in reality, which can lead to serious political, social, financial, and legal consequences [11].\n\nWith the escalated concerns over the DeepFakes, there is a surge of interest in developing DeepFakes detection methods recently [6,17,27,53,33,28,41,40,35,34,36], with an upcoming dedicated global DeepFake Detection Challenge 1 . The availability of large-scale datasets of DeepFake videos is an enabling factor in the development of DeepFake detection method. To date, we have the UADFV dataset [53], the DeepFake-TIMIT dataset (DF-TIMIT) [25], the FaceForenscics++ dataset (FF-DF) [40] 2 , the Google Deep-Fake detection dataset (DFD) [15], and the FaceBook Deep-Fake detection challenge (DFDC) dataset [14].\n\nHowever, a closer look at the DeepFake videos in existing datasets reveals stark contrasts in visual quality to the actual DeepFake videos circulated on the Internet. Several common visual artifacts that can be found in these datasets are highlighted in Fig.1, including low-quality synthesized faces, visible splicing boundaries, color mismatch, visible parts of the original face, and inconsistent synthesized face orientations. These artifacts are likely the result of imperfect steps of the synthesis method and the lack of curating of the synthesized videos before included in the datasets. Moreover, DeepFake videos with such low visual qualities can hardly be convincing, and are unlikely to have real impact. Correspondingly, high detection performance on these dataset may not bear strong relevance when the detection methods are deployed in the wild.\n\nIn this work, we present a new large-scale and challenging DeepFake video dataset, Celeb-DF 3 , for the development and evaluation of DeepFake detection algorithms. There are in total 5, 639 DeepFake videos, corresponding more than 2 million frames, in the Celeb-DF dataset. The real source videos are based on publicly available YouTube video clips of 59 celebrities of diverse genders, ages, and ethic groups. The DeepFake videos are generated using an improved DeepFake synthesis method. As a result, the overall visual quality of the synthesized DeepFake videos in Celeb-DF is greatly improved when compared to existing datasets, with significantly fewer notable visual artifacts, see Fig.2. Based on the Celeb-DF dataset and other existing datasets, we conduct an evaluation of current Deep-Fake detection methods. This is the most comprehensive performance evaluation of DeepFake detection methods to UADFV DF-TIMIT-HQ FF-DF DFD DFDC Figure 1. Visual artifacts of DeepFake videos in existing datasets. Note some common types of visual artifacts in these video frames, including low-quality synthesized faces (row 1 col 1, row 3 col 2, row 5 col 3), visible splicing boundaries (row 3 col 1, row 4 col 2, row 5 col 2), color mismatch (row 5 col 1), visible parts of the original face (row 1 col 1, row 2 col 1, row 4 col 3), and inconsistent synthesized face orientations (row 3 col 3). This figure is best viewed in color.\n\ndate. The results show that Celeb-DF is challenging to most of the existing detection methods, even though many Deep-Fake detection methods are shown to achieve high, sometimes near perfect, accuracy on previous datasets.\n\n\nBackgrounds\n\n\nDeepFake Video Generation\n\nAlthough in recent years there have been many sophisticated algorithms for generating realistic synthetic face videos [9,13,46,51,26,47,37,20,23,10,21,50], most of these have not been in mainstream as open-source software tools that anyone can use. It is a much simpler method based on the work of neural image style transfer [29] that becomes the tool of choice to create DeepFake videos in scale, with several independent open-source implementations, e.g., FakeApp [5], DFaker [2], faceswap-GAN [3], faceswap [4], and DeepFaceLab [1]. We refer to this method as the basic DeepFake maker, and it is underneath many DeepFake videos circulated on the Internet or in the existing datasets.\n\nThe overall pipeline of the basic DeepFake maker is shown in Fig.3 (left). From an input video, faces of the target are detected, from which facial landmarks are further extracted. The landmarks are used to align the faces to a standard configuration [22]. The aligned faces are then cropped and fed to an auto-encoder [24] to synthesize faces of the donor with the same facial expressions as the original target's faces.\n\nThe auto-encoder is usually formed by two convoluntional neural networks (CNNs), i.e., the encoder and the decoder. The encoder E converts the input target's face to a vector known as the code. To ensure the encoder capture identity-independent attributes such as facial expressions, there is one single encoder regardless the identities of the subjects. On the other hand, each identity has a dedicated decoder D i , which generates a face of the corresponding subject from the code. The encoder and decoder are trained in tandem using uncorresponded face sets of multiple subjects in an unsupervised manner, Fig.3 (right). Specifically, an encoder-decoder pair is formed alternatively using E and D i for input face of each subject, and optimize their parameters to minimize the reconstruction errors ( 1 difference between the input and reconstructed faces). The parameter update is performed with the back-propagation until convergence.\n\nThe synthesized faces are then warped back to the configuration of the original target's faces and trimmed with a mask from the facial landmarks. The last step involves smoothing the boundaries between the synthesized regions and the original video frames. The whole process is automatic and runs with little manual intervention.    Since synthesized faces are spliced into the original video frames, state-of-the-art DNN splicing detection methods, e.g., [54,55,30,8], can be applied. There have also been algorithms dedicated to the detection of Deep-Fake videos that fall into three categories. Methods in the first category are based on inconsistencies exhibited in the physical/physiological aspects in the DeepFake videos. The method in work of [27] exploits the observation that many DeepFake videos lack reasonable eye blinking due to the use of online portraits as training data, which usually do not have closed eyes for aesthetic reasons. Incoherent head poses in DeepFake videos are utilized in [53]  expose DeepFake videos. In [7], the idiosyncratic behavioral patterns of a particular individual are captured by the time series of facial landmarks extracted from real videos are used to spot DeepFake videos. The second category of DeepFake detection algorithms (e.g., [33,28]) use signallevel artifacts introduced during the synthesis process such as those described in the Introduction. The third category of DeepFake detection methods (e.g., [6,17,35,36]) are data-driven, which directly employ various types of DNNs trained on real and DeepFake videos, not relying on any specific artifact.\n\n\nExisting DeepFake Datasets\n\nDeepFake detection methods require training data and need to be evaluated. As such, there is an increasing need for large-scale DeepFake video datasets. Table 1 lists the current DeepFake datasets. UADFV: The UADFV dataset [53] contains 49 real YouTube and 49 DeepFake videos. The DeepFake videos are generated using the DNN model with FakeAPP [5]. DF-TIMIT: The DeepFake-TIMIT dataset [25] includes 640 DeepFake videos generated with faceswap-GAN [3] and based on the Vid-TIMIT dataset [43]. The videos are divided into two equal-sized subsets: DF-TIMIT-LQ and DF-TIMIT-HQ, with synthesized faces of size 64 \u00d7 64 and 128 \u00d7 128 pixels, respectively. FF-DF: The FaceForensics++ dataset [40] includes a subset of DeepFakes videos, which has 1, 000 real YouTube videos and the same number of synthetic videos generated using faceswap [4]. DFD: The Google/Jigsaw DeepFake detection dataset [15] has 3, 068 DeepFake videos generated based on 363 original videos of 28 consented individuals of various genders, ages and ethnic groups. The details of the synthesis algorithm are not disclosed, but it is likely to be an improved implementation of the basic DeepFake maker algorithm. DFDC: The Facebook DeepFake detection challenge dataset [14] is part of the DeepFake detection challenge, which has 4, 113 DeepFake videos created based on 1, 131 original videos of 66 consented individuals of various genders, ages and ethnic groups 4 . This dataset is created us-\n\n\nBasic Information\n\nThe Celeb-DF dataset is comprised of 590 real videos and 5, 639 DeepFake videos (corresponding to over two million video frames). The average length of all videos is approximate 13 seconds with the standard frame rate of 30 frame-per-second. The real videos are chosen from publicly available YouTube videos, corresponding to interviews of 59 celebrities with a diverse distribution in their genders, ages, and ethnic groups 5 . 56.8% subjects in the real videos are male, and 43.2% are female. 8.5% are of age 60 and above, 30.5% are between 50 -60, 26.6% are 40s, 28.0% are 30s, and 6.4% are younger than 30. 5.1% are Asians, 6.8% are African Americans and 88.1% are Caucasians. In addition, the real videos exhibit large range of changes in aspects such as the subjects' face sizes (in pixels), orientations, lighting conditions, and backgrounds. The DeepFake videos are generated by swapping faces for each pair of the 59 subjects. The final videos are in MPEG4.0 format.\n\n\nSynthesis Method\n\nThe DeepFake videos in Celeb-DF are generated using an improved DeepFake synthesis algorithm, which is key to the improved visual quality as shown in Fig.2. Specifically, the basic DeepFake maker algorithm is refined in several aspects targeting the following specific visual artifacts observed in existing datasets.\n\nLow resolution of synthesized faces: The basic DeepFake maker algorithm generate low-resolution faces (typically 64 \u00d7 64 or 128 \u00d7 128 pixels). We improve the resolution of the synthesized face to 256 \u00d7 256 pixels. This is achieved by using encoder and decoder models with more layers and increased dimensions. We determine the structure empirically for a balance between increased training time and better synthesis result. The higher resolution of the synthesized faces are of better visual quality and less affected by resizing and rotation operations in accommodating the input target faces, Fig.4. Color mismatch: Color mismatch between the synthesized donor's face with the original target's face in Celeb-DF is significantly reduced by training data augmentation and post processing. Specifically, in each training epoch, we randomly perturb the colors of the training faces, which forces the DNNs to synthesize an image containing the same color pattern with input image. We also apply a color transfer algorithm [38] between the synthesized donor face and the input target face. Fig.5 shows an example of synthesized face without (left) and with (right) color correction. leaves the boundaries of the mask visible. We improve the mask generation step for Celeb-DF. We first synthesize a face with more surrounding context, so as to completely cover the original facial parts after warping. We then create a smoothness mask based on the landmarks on eyebrow and interpolated points on cheeks and between lower lip and chin. The difference in mask generation used in existing datasets and Celeb-DF is highlighted in Fig.6 with an example.\n\nTemporal flickering: We reduce temporal flickering of synthetic faces in the DeepFake videos by incorporating temporal correlations among the detected face landmarks. Specifically, the temporal sequence of the face landmarks are filtered using a Kalman smoothing algorithm to reduce imprecise variations of landmarks in each frame.\n\n\nVisual Quality\n\nThe refinements to the synthesis algorithm improve the visual qualities of the DeepFake videos in the Celeb-DF dataset, as demonstrated in Fig.2. We would like have a more quantitative evaluation of the improvement in visual quality of the DeepFake videos in Celeb-DF and compare with the previous DeepFake datasets. Ideally, a referencefree face image quality metric is the best choice for this purpose. However, unfortunately, to date there is no such metric that is agreed upon and widely adopted.\n\nInstead, we follow the face in-painting work [45] and use the Mask-SSIM score [32] as a referenced quantitative metric of visual quality of synthesized DeepFake video frames. Mask-SSIM corresponds to the SSIM score [52] between the head regions (including face and hair) of the DeepFake video frame and the corresponding original video frame, i.e., the head region of the original target is the reference for visual quality evaluation. As such, low Mask-SSIM score may be due to inferior visual quality as well as changes of the identity from the target to the donor. On the other hand,  \n\n\nEvaluating DeepFake Detection Methods\n\nUsing Celeb-DF and other existing DeepFake datasets, we perform the most comprehensive performance evaluation of DeepFake detection to date, with the largest number of DeepFake detection methods and datasets considered. There are two purposes of this evaluation. First, using the average detection performance as an indicator of the challenge levels of various DeepFake datasets, we further compare Celeb-DF with existing DeepFake datasets. Furthermore, we survey the performance of the current DeepFake detection methods on a large diversity of DeepFake videos, in particular, the high-quality ones in Celeb-DF.\n\n\nCompared DeepFake Detection Methods\n\nWe consider nine DeepFake detection methods in our experiments. Because of the need to run each method on the Celeb-DF dataset, we choose only those that have code and the corresponding DNN-model publicly available or obtained from the authors directly.\n\n\u2022 Two-stream [54] uses a two-stream CNN to achieve state-of-the-art performance in general-purpose image forgery detection. The underlying CNN is the GoogLeNet InceptionV3 model [48] trained on the SwapMe dataset [54]. We use it as a baseline to compare other dedicated DeepFake detection methods. \u2022 MesoNet [6] is a CNN-based DeepFake detection method targeting on the mesoscopic properties of images. The model is trained on unpublished DeepFake datasets collected by the authors. We evaluate two variants of MesoNet, namely, Meso4 and MesoIncep-tion4. Meso4 uses conventional convolutional layers, while MesoInception4 is based on the more sophisticated Inception modules [49]. \u2022 HeadPose [53] detects DeepFake videos using the inconsistencies in the head poses of the synthesized videos, based on a SVM model on estimated 3D head orientations from each video. The SVM model in this method is trained on the UADFV dataset. \u2022 FWA [28] detects DeepFake videos using a ResNet-50 [19] to expose the face warping artifacts introduced by the resizing and interpolation operations in the basic DeepFake maker algorithm. This model is trained on self-collected face images. \u2022 VA [33] is a recent DeepFake detection method based on capturing visual artifacts in the eyes, teeth and facial contours of the synthesized faces. There are two variants of this method: VA-MLP is based on a multilayer feedforward neural network classifier, and VA-LogReg uses a simpler logistic regression model. These models are trained on unpublished dataset, of which real images are cropped from CelebA dataset [31] and the DeepFake videos are from YouTube. \u2022 Xception [40] corresponds to a DeepFake detection method based on the XceptionNet model [12] trained on the FaceForensics++ dataset. There are three variants of Xception, namely, Xception-raw, Xception-c23 and Xception-c40: Xception-raw are trained on raw videos, while Xception-c23 and Xception-c40 are trained on H.264 videos with medium (23) and high degrees (40) of compression, respectively. \u2022 Multi-task [34] is another recent DeepFake detection method that uses a CNN model to simultaneously detect manipulated images and segment manipulated areas as a multi-task learning problem. This model is trained on the FaceForensics dataset [39]. \u2022 Capsule [36] uses capsule structures [42] based on a VGG19 [44] network as the backbone architecture for DeepFake classification. This model is trained on the FaceForensics++ dataset. \u2022 DSP-FWA is a recently further improved method based on FWA, which includes a spatial pyramid pooling (SPP) module [18] to better handle the variations in the resolutions of the original target faces. This method is trained on self-collected face images. A concise summary of the underlying model, source code, and training datasets of the DeepFake detection methods considered in our experiments is given in Table 3.\n\n\nExperimental Settings\n\nWe evaluate the overall detection performance using the area under ROC curve (AUC) score at the frame level for all key frames. There are several reasons for this choice. First, all compared methods analyze individual frames (usu-\n\n\nMethods\n\n\nModel Type\n\nTraining Dataset Repositories Release Date Two-stream [54] GoogLeNet InceptionV3 [48] SwapMe [54] Unpublished code provided by the authors 2018.03 MesoNet [6] Designed CNN Unpublished https://github.com/DariusAf/MesoNet 2018.09 HeadPose [53] SVM UADFV [53] https://bitbucket.org/ericyang3721/headpose_forensic/ 2018.11 FWA [28] ResNet-50 [19] Unpublished https://github.com/danmohaha/CVPRW2019_Face_Artifacts 2018.11 VA-MLP [33] Designed CNN Unpublished https://github.com/FalkoMatern/Exploiting-Visual-Artifacts 2019.01 VA-LogReg [33] Logistic Regression Model Xception [40] XceptionNet [12] FaceForensics++ [40] https://github.com/ondyari/FaceForensics 2019.01 Multi-task [34] Designed CNN FaceForensics [39] https://github.com/nii-yamagishilab/ClassNSeg 2019.06 Capsule [36] Designed CapsuleNet [42]   ally key frames of a video) and output a classification score for each frame. Using frame-level AUC thus avoids differences caused by different approaches to aggregating framelevel scores for each video. Second, using frame level AUC score obviates the necessity of calibrating the classification outputs of these methods across different datasets. To increase robustness to numerical imprecision, the classification scores are rounded to five digits after the decimal point, i.e., with a precision of 10 \u22125 . As the videos are compressed, we perform evaluations only on the key frames.\n\nwe compare performance of each detection method using the inference code and the published pre-trained models. This is because most of these methods do not have published code for training the machine learning models. As such, we could not practically re-train these models on all datasets we considered. We use the default parameters provided with each compared detection method.\n\n\nResults and Analysis\n\nIn Table 4 we list individual frame-level AUC scores of all compared DeepFake detection methods over all datasets including Celeb-DF, and Fig.9 shows the frame-level ROC curves of several top detection methods on several datasets.\n\nComparing different datasets, in Fig.7, we show the average frame-level AUC scores of all compared detection methods on each dataset. Celeb-DF is in general the most challenging to the current detection methods, and their overall performance on Celeb-DF is lowest across all datasets. These results are consistent with the differences in visual quality. Note many current detection methods predicate on visual artifacts such as low resolution and color mismatch, which are improved in synthesis algorithm for the Celeb-DF dataset. Furthermore, the difficulty level for detection 55   is clearly higher for the second generation datasets (DFD, DFDC, and Celeb-DF, with average AUC scores lower than 70%), while some detection methods achieve near perfect detection on the first generation datasets (UADFV, DF-TIMIT, and FF-DF, with average AUC scores around 80%).\n\nIn term of individual detection methods, Fig.8 shows the comparison of average AUC score of each detection method on all DeepFake datasets. These results show that detection has also made progress with the most recent DSP-FWA method achieves the overall top performance (87.4%).\n\nAs online videos are usually recompressed to different formats (MPEG4.0 and H264) and in different qualities during the process of uploading and redistribution, it is also important to evaluate the robustness of detection performance with regards to video compression. Table 5 shows the average frame-level AUC scores of four state-of-the-art DeepFake detection methods on original MPEG4.0 videos, and medium (23), and high (40) degrees of H.264 compressed videos of Celeb-DF, respectively. The results show that the performance of each method is reduced along with the compression degree increased. In particular, the performance of FWA and DSP-FWA degrades significantly on recompressed video, while the performance of Xception-c23 and Xception-c40 is not significantly affected. This is expected because the latter methods were trained on compressed H.264 videos such that they are more robust in this setting.\n\nMethods\u2193 Datasets\u2192 UADFV [53] DF-TIMIT [25] FF-DF [40] DFD [15] DFDC [14] Celeb-DF LQ HQ   \n\n\nConclusion\n\nWe present a new challenging large-scale dataset for the development and evaluation of DeepFake detection methods. The Celeb-DF dataset reduces the gap in visual quality of DeepFake datasets and the actual DeepFake videos circulated online. Based on the Celeb-DF dataset, we perform a comprehensive performance evaluation of current Deep-Fake detection methods, and show that there is still much room for improvement.\n\nFor future works, the foremost task is to enlarge the Celeb-DF dataset and improve the visual quality of the synthesized videos. This entails improving the running efficiency and model structure of the current synthesis algorithm. Furthermore, while the forgers can improve the visual quality in general, they may also adopt anti-forensic techniques, which aim to hide traces of DeepFake synthesis on which the detection methods predicate. Anticipating such counter-measures at the forgers' disposal, we aim to incorporate anti-forensic techniques in Celeb-DF.\n\nFigure 2 .\n2Example frames from the Celeb-DF dataset. Left column is the frame of real videos and right five columns are corresponding DeepFake frames generated using different donor subject.\n\nFace\n\n\nFigure 3 .\n3Synthesis (left) and training (right) of the basic DeepFake maker algorithm. See texts for more details.\n\nFigure 4 .\n4Comparison of DeepFake frames with different sizes of the synthesized faces. Note the improved smoothness of the 256 \u00d7 256 synthesized face, which is used in Celeb-DF. This figure is best viewed in color.\n\nFigure 5 .Figure 6 .\n56DeepFake frames using synthesized face without (left) and with (right) color correction. Note the reduced color mismatch between the synthesized face region and the other part of the face. Synthesis method with color correction is used to generate Celeb-DF. This figure is best viewed in color.Inaccurate face masks: In previous datasets, the face masks are either rectangular, which may not completely cover the facial parts in the original video frame, or the convex hull of landmarks on eyebrow and lower lip, which Mask generation in existing datasets (Top two rows) andCeleb-DF (3rd row).(a) warped synthesized face overlaying the target's face. (b) mask generation. (c) final synthesis result.\n\nFigure 7 .\n7Average AUC performance of all detection methods on each dataset.\n\nFigure 8 .\n8Average AUC performance of each detection method on all evaluated datasets.\n\nFigure 9 .\n9ROC curves of six state-of-the-art detection methods (FWA, Meso4, MesoInception4, Xception-c23, Xception-40 and DSP-FWA) on four largest datasets (FF-DF, DFD, DFDC and Celeb-DF).\n\nTable 2 .\n2Average Mask-SSIM scores of different DeepFake datasets. Computing Mask-SSIM requires exact corresponding pairs of DeepFake synthesized frames and original video frames, which is not the case for DFD and DFDC. For these two datasets, we calculate the Mask-SSIM on videos that we have exact correspondences, i.e., 311 videos in DFD and 2, 025 videos in DFDC. with higher value corresponding to better image quality. Table 2 shows the average Mask-SSIM scores for all compared datasets, with Celeb-DF having the highest scores. This confirms the visual observation that Celeb-DF has improved visual quality, as shown inFig.2.since we only compare frames from DeepFake videos, the \nerrors caused by identity changes are biased in a similar \nfashion to all compared datasets. Therefore, the numerical \nvalues of Mask-SSIM may not be meaningful to evaluate \nthe absolute visual quality of the synthesized faces, but the \ndifference between Mask-SSIM reflects the difference in vi-\nsual quality. \nThe Mask-SSIM score takes value in the range of [0, 1] \n\n\n\nFrame-level AUC scores (%) of various methods on compared datasets. Bold faces correspond to the top performance.Two-stream [54] \n85.1 \n83.5 \n73.5 \n70.1 \n52.8 \n61.4 \n53.8 \nMeso4 [6] \n84.3 \n87.8 \n68.4 \n84.7 \n76.0 \n75.3 \n54.8 \nMesoInception4 \n82.1 \n80.4 \n62.7 \n83.0 \n75.9 \n73.2 \n53.6 \nHeadPose [53] \n89.0 \n55.1 \n53.2 \n47.3 \n56.1 \n55.9 \n54.6 \nFWA [28] \n97.4 \n99.9 \n93.2 \n80.1 \n74.3 \n72.7 \n56.9 \nVA-MLP [33] \n70.2 \n61.4 \n62.1 \n66.4 \n69.1 \n61.9 \n55.0 \nVA-LogReg \n54.0 \n77.0 \n77.3 \n78.0 \n77.2 \n66.2 \n55.1 \nXception-raw [40] \n80.4 \n56.7 \n54.0 \n99.7 \n53.9 \n49.9 \n48.2 \nXception-c23 \n91.2 \n95.9 \n94.4 \n99.7 \n85.9 \n72.2 \n65.3 \nXception-c40 \n83.6 \n75.8 \n70.5 \n95.5 \n65.8 \n69.7 \n65.5 \nMulti-task [34] \n65.8 \n62.2 \n55.3 \n76.3 \n54.1 \n53.6 \n54.3 \nCapsule [36] \n61.3 \n78.4 \n74.4 \n96.6 \n64.0 \n53.3 \n57.5 \nDSP-FWA \n97.7 \n99.9 \n99.7 \n93.0 \n81.1 \n75.5 \n64.6 \nTable 4. \n\n\nTable 5. AUC performance of four top detection methods on original, medium (23) and high(40) degrees of H.264 compressed Celeb-DF respectively.Original \nc23 \nc40 \n\nFWA \n56.9 \n54.6 52.2 \nXception-c23 \n65.3 \n65.5 52.5 \nXception-c40 \n65.5 \n65.4 59.4 \nDSP-FWA \n64.6 \n57.7 47.2 \n\nhttps://deepfakedetectionchallenge.ai.\nFaceForensics++ contains other types of fake videos. We consider only the DeepFake videos.3 http://www.cs.albany.edu/\u02dclsw/ celeb-deepfakeforensics.html.\n.2. DeepFake Detection MethodsSince DeepFakes become a global phenomenon, there has been an increasing interest in DeepFake detection methods. Most of the current DeepFake detection methods use data-driven deep neural networks (DNNs) as backbone.\nThe full set of DFDC has not been released at the time of CVPR submission, and information is based on the first round release in[14].ing two different synthesis algorithms, but the details of the synthesis algorithm are not disclosed.Based on release time and synthesis algorithms, we categorize UADFV, DF-TIMIT, and FF-DF as the first generation of DeepFake datasets, while DFD, DFDC, and the proposed Celeb-DF datasets are the second generation. In general, the second generation datasets improve in both quantity and quality over the first generation.3. The Celeb-DF DatasetAlthough the current DeepFake datasets have sufficient number of videos, as discussed in the Introduction and demonstrate inFig.1, DeepFake videos in these datasets have various visual artifacts that easily distinguish them from the real videos. To provide more relevant data to evaluate and support the future development DeepFake detection methods, we construct the Celeb-DF dataset. A comparison of the Celeb-DF dataset with other existing Deep-Fake datasets is summarized inTable 1.\nWe choose celebrities' faces as they are more familiar to the viewers so that any visual artifacts can be more readily identified. Furthermore, celebrities are anecdotally the main targets of DeepFake videos.\n\n. Deepfacelab Github, DeepFaceLab github. https://github.com/ iperov/DeepFaceLab, Accessed Nov 4, 2019.\n\n. Dfaker, DFaker github. https://github.com/dfaker/df, Accessed Nov 4, 2019.\n\n. Faceswap-Gan Github, faceswap-GAN github. https://github.com/ shaoanlu/faceswap-GAN, Accessed Nov 4, 2019.\n\nFakeApp. FakeApp. https://www.malavida.com/en/soft/ fakeapp/, Acessed Nov 4, 2019.\n\nMesonet: a compact facial video forgery detection network. Darius Afchar, Vincent Nozick, Junichi Yamagishi, Isao Echizen, IEEE International Workshop on Information Forensics and Security (WIFS). Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. Mesonet: a compact facial video forgery detection network. In IEEE International Workshop on Information Forensics and Security (WIFS), 2018.\n\nProtecting world leaders against deep fakes. Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, Hao Li, IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao Li. Protecting world leaders against deep fakes. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.\n\nHybrid lstm and encoder-decoder architecture for detection of image forgeries. H Jawadul, Cody Bappy, Lakshmanan Simons, Nataraj, Amit K Roy-Chowdhury Bs Manjunath, IEEE Transactions on Image Processing. Jawadul H Bappy, Cody Simons, Lakshmanan Nataraj, BS Manjunath, and Amit K Roy-Chowdhury. Hybrid lstm and encoder-decoder architecture for detection of image forg- eries. IEEE Transactions on Image Processing (TIP), 2019.\n\nFace swapping: automatically replacing faces in photographs. Dmitri Bitouk, Neeraj Kumar, Samreen Dhillon, Peter Belhumeur, Shree K Nayar, ACM Transactions on Graphics. Dmitri Bitouk, Neeraj Kumar, Samreen Dhillon, Peter Bel- humeur, and Shree K Nayar. Face swapping: automati- cally replacing faces in photographs. ACM Transactions on Graphics (TOG), 2008.\n\nEverybody dance now. Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A Efros, ICCV. Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In ICCV, 2019.\n\nDeep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. Robert Chesney, Danielle Keats Citron, California Law Review. 107ForthcomingRobert Chesney and Danielle Keats Citron. Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. 107 California Law Review (2019, Forthcoming);\n\nPublic Law Research Paper No. 692; U of Maryland Legal Studies Research Paper No. U of Texas LawU of Texas Law, Public Law Research Paper No. 692; U of Maryland Legal Studies Research Paper No. 2018-21, 2018.\n\nXception: Deep learning with depthwise separable convolutions. Fran\u00e7ois Chollet, CVPR. Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017.\n\nVideo face replacement. Kevin Dale, Kalyan Sunkavalli, Micah K Johnson, Daniel Vlasic, Wojciech Matusik, Hanspeter Pfister, ACM Transactions on Graphics. Kevin Dale, Kalyan Sunkavalli, Micah K Johnson, Daniel Vlasic, Wojciech Matusik, and Hanspeter Pfister. Video face replacement. ACM Transactions on Graphics (TOG), 2011.\n\nNicole Baram, and Cristian Canton Ferrer. The deepfake detection challenge (DFDC) preview dataset. Brian Dolhansky, Russ Howes, Ben Pflaum, arXiv:1910.08854arXiv preprintBrian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, and Cristian Canton Ferrer. The deepfake detec- tion challenge (DFDC) preview dataset. arXiv preprint arXiv:1910.08854, 2019.\n\nNicholas Dufour, Andrew Gully, Per Karlsson, Alexey Victor Vorbyov, Thomas Leung, Jeremiah Childs, and Christoph Bregler. Deepfakes detection dataset by google & jigsaw. Nicholas Dufour, Andrew Gully, Per Karlsson, Alexey Vic- tor Vorbyov, Thomas Leung, Jeremiah Childs, and Christoph Bregler. Deepfakes detection dataset by google & jigsaw.\n\nHany Farid, Digital Image Forensics. MIT PressHany Farid. Digital Image Forensics. MIT Press, 2012.\n\nDeepfake video detection using recurrent neural networks. David G\u00fcera, J Edward, Delp, In AVSS. David G\u00fcera and Edward J Delp. Deepfake video detection using recurrent neural networks. In AVSS, 2018.\n\nSpatial pyramid pooling in deep convolutional networks for visual recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE transactions. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE transactions on pattern analysis and machine intelligence (TPAMI), 2015.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nProgressive growing of GANs for improved quality, stability, and variation. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, ICLR. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018.\n\nA style-based generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, CVPR. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019.\n\nOne millisecond face alignment with an ensemble of regression trees. Vahid Kazemi, Josephine Sullivan, CVPR. Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees. In CVPR, 2014.\n\nDeep Video Portraits. H Kim, P Garrido, A Tewari, W Xu, J Thies, N Nie\u00dfner, P P\u00e9rez, C Richardt, M Zollh\u00f6fer, C Theobalt, ACM Transactions on Graphics. TOG)H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, N. Nie\u00dfner, P. P\u00e9rez, C. Richardt, M. Zollh\u00f6fer, and C. Theobalt. Deep Video Portraits. ACM Transactions on Graphics 2018 (TOG), 2018.\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, ICLR. Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. In ICLR, 2014.\n\nDeepfakes: a new threat to face recognition? assessment and detection. Pavel Korshunov, S\u00e9bastien Marcel, arXiv:1812.08685arXiv preprintPavel Korshunov and S\u00e9bastien Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018.\n\nFast face-swap using convolutional neural networks. Iryna Korshunova, Wenzhe Shi, Joni Dambre, Lucas Theis, ICCV. Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas Theis. Fast face-swap using convolutional neural networks. In ICCV, 2017.\n\nIn ictu oculi: Exposing AI generated fake face videos by detecting eye blinking. Yuezun Li, Ming-Ching Chang, Siwei Lyu, IEEE International Workshop on Information Forensics and Security (WIFS). Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi: Exposing AI generated fake face videos by detecting eye blinking. In IEEE International Workshop on Information Forensics and Security (WIFS), 2018.\n\nExposing deepfake videos by detecting face warping artifacts. Yuezun Li, Siwei Lyu, IEEE Conference on Computer Vision and Pattern Recognition Workshops. CVPRWYuezun Li and Siwei Lyu. Exposing deepfake videos by de- tecting face warping artifacts. In IEEE Conference on Com- puter Vision and Pattern Recognition Workshops (CVPRW), 2019.\n\nUnsupervised image-to-image translation networks. Ming-Yu Liu, Thomas Breuel, Jan Kautz, NeurIPS. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In NeurIPS, 2017.\n\nImage forgery localization based on multi-scale convolutional neural networks. Yaqi Liu, Qingxiao Guan, Xianfeng Zhao, Yun Cao, ACM Workshop on Information Hiding and Multimedia Security (IHMMSec). Yaqi Liu, Qingxiao Guan, Xianfeng Zhao, and Yun Cao. Im- age forgery localization based on multi-scale convolutional neural networks. In ACM Workshop on Information Hiding and Multimedia Security (IHMMSec), 2018.\n\nDeep learning face attributes in the wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, ICCV. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015.\n\nTinne Tuytelaars, and Luc Van Gool. Pose guided person image generation. Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, NeurIPS. Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuyte- laars, and Luc Van Gool. Pose guided person image genera- tion. In NeurIPS, 2017.\n\nExploiting visual artifacts to expose deepfakes and face manipulations. Falko Matern, Christian Riess, Marc Stamminger, IEEE Winter Applications of Computer Vision Workshops (WACVW). Falko Matern, Christian Riess, and Marc Stamminger. Ex- ploiting visual artifacts to expose deepfakes and face manip- ulations. In IEEE Winter Applications of Computer Vision Workshops (WACVW), 2019.\n\nMulti-task learning for detecting and segmenting manipulated facial images and videos. H Huy, Fuming Nguyen, Junichi Fang, Isao Yamagishi, Echizen, IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS). Huy H Nguyen, Fuming Fang, Junichi Yamagishi, and Isao Echizen. Multi-task learning for detecting and segmenting manipulated facial images and videos. In IEEE International Conference on Biometrics: Theory, Applications and Sys- tems (BTAS), 2019.\n\nCapsule-forensics: Using capsule networks to detect forged images and videos. H Huy, Junichi Nguyen, Isao Yamagishi, Echizen, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. Capsule-forensics: Using capsule networks to detect forged images and videos. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019.\n\nUse of a capsule network to detect fake images and videos. H Huy, Junichi Nguyen, Isao Yamagishi, Echizen, arXiv:1910.12467arXiv preprintHuy H Nguyen, Junichi Yamagishi, and Isao Echizen. Use of a capsule network to detect fake images and videos. arXiv preprint arXiv:1910.12467, 2019.\n\nGenerative adversarial talking head: Bringing portraits to life with a weakly supervised neural network. X Hai, Yuting Pham, Vladimir Wang, Pavlovic, arXiv:1803.07716arXiv preprintHai X Pham, Yuting Wang, and Vladimir Pavlovic. Gen- erative adversarial talking head: Bringing portraits to life with a weakly supervised neural network. arXiv preprint arXiv:1803.07716, 2018.\n\nColor transfer between images. Erik Reinhard, Michael Adhikhmin, Bruce Gooch, Peter Shirley, IEEE Computer graphics and applications. Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Computer graphics and applications, 2001.\n\nAndreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, arXiv:1803.09179Christian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv preprintAndreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Chris- tian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforen- sics: A large-scale video dataset for forgery detection in hu- man faces. arXiv preprint arXiv:1803.09179, 2018.\n\nFaceForen-sics++: Learning to detect manipulated facial images. Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. ICCVAndreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Chris- tian Riess, Justus Thies, and Matthias Nie\u00dfner. FaceForen- sics++: Learning to detect manipulated facial images. In ICCV, 2019.\n\nWael AbdAlmageed, Iacopo Masi, and Prem Natarajan. Recurrentconvolution approach to deepfake detection-state-of-art results on faceforensics++. Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, arXiv:1905.00582arXiv preprintEkraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAl- mageed, Iacopo Masi, and Prem Natarajan. Recurrent- convolution approach to deepfake detection-state-of-art re- sults on faceforensics++. arXiv preprint arXiv:1905.00582, 2019.\n\nDynamic routing between capsules. Sara Sabour, Nicholas Frosst, Geoffrey E Hinton, NeurIPS. Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dy- namic routing between capsules. In NeurIPS, 2017.\n\nMulti-region probabilistic histograms for robust and scalable identity inference. Conrad Sanderson, C Brian, Lovell, International Conference on Biometrics. Conrad Sanderson and Brian C Lovell. Multi-region proba- bilistic histograms for robust and scalable identity inference. In International Conference on Biometrics, 2009.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nNatural and effective obfuscation by head inpainting. Qianru Sun, Liqian Ma, Luc Seong Joon Oh, Bernt Van Gool, Mario Schiele, Fritz, CVPR. Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, and Mario Fritz. Natural and effective ob- fuscation by head inpainting. In CVPR, 2018.\n\nWhat makes tom hanks look like tom hanks. Supasorn Suwajanakorn, M Steven, Ira Seitz, Kemelmacher-Shlizerman, ICCV. Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. What makes tom hanks look like tom hanks. In ICCV, 2015.\n\nSynthesizing obama: learning lip sync from audio. Supasorn Suwajanakorn, M Steven, Ira Seitz, Kemelmacher-Shlizerman, ACM Transactions on Graphics. Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Synthesizing obama: learn- ing lip sync from audio. ACM Transactions on Graphics (TOG), 2017.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n\nDeferred neural rendering: Image synthesis using neural textures. Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, SIGGRAPH. Justus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. De- ferred neural rendering: Image synthesis using neural tex- tures. In SIGGRAPH, 2019.\n\nChristian Theobalt, and Matthias Niessner. Face2face: Real-time face capture and reenactment of rgb videos. Justus Thies, Michael Zollhofer, Marc Stamminger, CVPR. Justus Thies, Michael Zollhofer, Marc Stamminger, Chris- tian Theobalt, and Matthias Niessner. Face2face: Real-time face capture and reenactment of rgb videos. In CVPR, June 2016.\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Sheikh, P Eero, Simoncelli, IEEE Transactions on Image Processing (TIP). Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simon- celli, et al. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Process- ing (TIP), 2004.\n\nExposing deep fakes using inconsistent head poses. Xin Yang, Yuezun Li, Siwei Lyu, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In IEEE International Confer- ence on Acoustics, Speech and Signal Processing (ICASSP), 2019.\n\nTwo-stream neural networks for tampered face detection. Peng Zhou, Xintong Han, I Vlad, Larry S Morariu, Davis, IEEE Conference on Computer Vision and Pattern Recognition Workshops. CVPRWPeng Zhou, Xintong Han, Vlad I Morariu, and Larry S Davis. Two-stream neural networks for tampered face detection. In IEEE Conference on Computer Vision and Pattern Recogni- tion Workshops (CVPRW), 2017.\n\nLearning rich features for image manipulation detection. Peng Zhou, Xintong Han, I Vlad, Larry S Morariu, Davis, CVPR. Peng Zhou, Xintong Han, Vlad I Morariu, and Larry S Davis. Learning rich features for image manipulation detection. In CVPR, 2018.\n", "annotations": {"author": "[{\"end\":135,\"start\":70},{\"end\":200,\"start\":136},{\"end\":257,\"start\":201},{\"end\":319,\"start\":258},{\"end\":385,\"start\":320}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":77},{\"end\":144,\"start\":140},{\"end\":207,\"start\":204},{\"end\":269,\"start\":267},{\"end\":329,\"start\":326}]", "author_first_name": "[{\"end\":76,\"start\":70},{\"end\":139,\"start\":136},{\"end\":203,\"start\":201},{\"end\":266,\"start\":258},{\"end\":325,\"start\":320}]", "author_affiliation": "[{\"end\":134,\"start\":81},{\"end\":199,\"start\":146},{\"end\":256,\"start\":209},{\"end\":318,\"start\":271},{\"end\":384,\"start\":331}]", "title": "[{\"end\":67,\"start\":1},{\"end\":452,\"start\":386}]", "venue": null, "abstract": "[{\"end\":1157,\"start\":454}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1417,\"start\":1413},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2070,\"start\":2066},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2204,\"start\":2201},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2207,\"start\":2204},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2210,\"start\":2207},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2213,\"start\":2210},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2216,\"start\":2213},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2219,\"start\":2216},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2222,\"start\":2219},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2225,\"start\":2222},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2228,\"start\":2225},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2231,\"start\":2228},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2234,\"start\":2231},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2300,\"start\":2299},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2473,\"start\":2469},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2517,\"start\":2513},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2560,\"start\":2556},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2614,\"start\":2610},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2682,\"start\":2678},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5363,\"start\":5360},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5366,\"start\":5363},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5369,\"start\":5366},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5372,\"start\":5369},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5375,\"start\":5372},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5378,\"start\":5375},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5381,\"start\":5378},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5384,\"start\":5381},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5387,\"start\":5384},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5390,\"start\":5387},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5393,\"start\":5390},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5396,\"start\":5393},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5572,\"start\":5568},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5712,\"start\":5709},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5724,\"start\":5721},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5742,\"start\":5739},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5777,\"start\":5774},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6186,\"start\":6182},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6254,\"start\":6250},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7756,\"start\":7752},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7759,\"start\":7756},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7762,\"start\":7759},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7764,\"start\":7762},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8051,\"start\":8047},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8307,\"start\":8303},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8339,\"start\":8336},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8583,\"start\":8579},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8586,\"start\":8583},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8758,\"start\":8755},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8761,\"start\":8758},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8764,\"start\":8761},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8767,\"start\":8764},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9162,\"start\":9158},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9282,\"start\":9279},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9325,\"start\":9321},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9386,\"start\":9383},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9426,\"start\":9422},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9624,\"start\":9620},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9825,\"start\":9821},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10171,\"start\":10167},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10840,\"start\":10839},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12752,\"start\":12748},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14275,\"start\":14271},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14308,\"start\":14304},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":14445,\"start\":14441},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15780,\"start\":15776},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15945,\"start\":15941},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15980,\"start\":15976},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16074,\"start\":16071},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16442,\"start\":16438},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16459,\"start\":16455},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16699,\"start\":16695},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16746,\"start\":16742},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16941,\"start\":16937},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17353,\"start\":17349},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17411,\"start\":17407},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17490,\"start\":17486},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17812,\"start\":17808},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18042,\"start\":18038},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18058,\"start\":18054},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18087,\"start\":18083},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18109,\"start\":18105},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18350,\"start\":18346},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18987,\"start\":18983},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19014,\"start\":19010},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19026,\"start\":19022},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19087,\"start\":19084},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19170,\"start\":19166},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19185,\"start\":19181},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19256,\"start\":19252},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19271,\"start\":19267},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19357,\"start\":19353},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19464,\"start\":19460},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19504,\"start\":19500},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19521,\"start\":19517},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19542,\"start\":19538},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19607,\"start\":19603},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19639,\"start\":19635},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19706,\"start\":19702},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19731,\"start\":19727},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":21540,\"start\":21538},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22516,\"start\":22512},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23047,\"start\":23043},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23061,\"start\":23057},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23072,\"start\":23068},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23081,\"start\":23077},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23091,\"start\":23087},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27729,\"start\":27725},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28484,\"start\":28480}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24296,\"start\":24104},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24303,\"start\":24297},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24421,\"start\":24304},{\"attributes\":{\"id\":\"fig_3\"},\"end\":24639,\"start\":24422},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25363,\"start\":24640},{\"attributes\":{\"id\":\"fig_5\"},\"end\":25442,\"start\":25364},{\"attributes\":{\"id\":\"fig_6\"},\"end\":25531,\"start\":25443},{\"attributes\":{\"id\":\"fig_7\"},\"end\":25723,\"start\":25532},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":26783,\"start\":25724},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":27634,\"start\":26784},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":27911,\"start\":27635}]", "paragraph": "[{\"end\":1520,\"start\":1173},{\"end\":2071,\"start\":1522},{\"end\":2683,\"start\":2073},{\"end\":3545,\"start\":2685},{\"end\":4975,\"start\":3547},{\"end\":5198,\"start\":4977},{\"end\":5929,\"start\":5242},{\"end\":6352,\"start\":5931},{\"end\":7294,\"start\":6354},{\"end\":8904,\"start\":7296},{\"end\":10392,\"start\":8935},{\"end\":11389,\"start\":10414},{\"end\":11726,\"start\":11410},{\"end\":13372,\"start\":11728},{\"end\":13705,\"start\":13374},{\"end\":14224,\"start\":13724},{\"end\":14814,\"start\":14226},{\"end\":15468,\"start\":14856},{\"end\":15761,\"start\":15508},{\"end\":18648,\"start\":15763},{\"end\":18904,\"start\":18674},{\"end\":20320,\"start\":18929},{\"end\":20702,\"start\":20322},{\"end\":20957,\"start\":20727},{\"end\":21821,\"start\":20959},{\"end\":22101,\"start\":21823},{\"end\":23016,\"start\":22103},{\"end\":23109,\"start\":23018},{\"end\":23541,\"start\":23124},{\"end\":24103,\"start\":23543}]", "formula": null, "table_ref": "[{\"end\":9095,\"start\":9088},{\"end\":18647,\"start\":18640},{\"end\":20737,\"start\":20730},{\"end\":22379,\"start\":22372}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1171,\"start\":1159},{\"attributes\":{\"n\":\"2.\"},\"end\":5212,\"start\":5201},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5240,\"start\":5215},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8933,\"start\":8907},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10412,\"start\":10395},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11408,\"start\":11392},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13722,\"start\":13708},{\"attributes\":{\"n\":\"4.\"},\"end\":14854,\"start\":14817},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15506,\"start\":15471},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18672,\"start\":18651},{\"end\":18914,\"start\":18907},{\"end\":18927,\"start\":18917},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20725,\"start\":20705},{\"attributes\":{\"n\":\"5.\"},\"end\":23122,\"start\":23112},{\"end\":24115,\"start\":24105},{\"end\":24302,\"start\":24298},{\"end\":24315,\"start\":24305},{\"end\":24433,\"start\":24423},{\"end\":24661,\"start\":24641},{\"end\":25375,\"start\":25365},{\"end\":25454,\"start\":25444},{\"end\":25543,\"start\":25533},{\"end\":25734,\"start\":25725}]", "table": "[{\"end\":26783,\"start\":26359},{\"end\":27634,\"start\":26899},{\"end\":27911,\"start\":27780}]", "figure_caption": "[{\"end\":24296,\"start\":24117},{\"end\":24421,\"start\":24317},{\"end\":24639,\"start\":24435},{\"end\":25363,\"start\":24664},{\"end\":25442,\"start\":25377},{\"end\":25531,\"start\":25456},{\"end\":25723,\"start\":25545},{\"end\":26359,\"start\":25736},{\"end\":26899,\"start\":26786},{\"end\":27780,\"start\":27637}]", "figure_ref": "[{\"end\":2944,\"start\":2939},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4241,\"start\":4236},{\"end\":4495,\"start\":4487},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5997,\"start\":5992},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6969,\"start\":6964},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11565,\"start\":11560},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12328,\"start\":12323},{\"end\":12820,\"start\":12815},{\"end\":13355,\"start\":13350},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13868,\"start\":13863},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20870,\"start\":20865},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20997,\"start\":20992},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21869,\"start\":21864}]", "bib_author_first_name": "[{\"end\":29639,\"start\":29628},{\"end\":30068,\"start\":30062},{\"end\":30084,\"start\":30077},{\"end\":30100,\"start\":30093},{\"end\":30116,\"start\":30112},{\"end\":30462,\"start\":30456},{\"end\":30476,\"start\":30472},{\"end\":30490,\"start\":30484},{\"end\":30503,\"start\":30495},{\"end\":30512,\"start\":30508},{\"end\":30524,\"start\":30521},{\"end\":30897,\"start\":30896},{\"end\":30911,\"start\":30907},{\"end\":30929,\"start\":30919},{\"end\":30967,\"start\":30947},{\"end\":31311,\"start\":31305},{\"end\":31326,\"start\":31320},{\"end\":31341,\"start\":31334},{\"end\":31356,\"start\":31351},{\"end\":31375,\"start\":31368},{\"end\":31632,\"start\":31624},{\"end\":31644,\"start\":31639},{\"end\":31661,\"start\":31654},{\"end\":31674,\"start\":31668},{\"end\":31676,\"start\":31675},{\"end\":31876,\"start\":31870},{\"end\":31894,\"start\":31886},{\"end\":32396,\"start\":32388},{\"end\":32538,\"start\":32533},{\"end\":32551,\"start\":32545},{\"end\":32569,\"start\":32564},{\"end\":32571,\"start\":32570},{\"end\":32587,\"start\":32581},{\"end\":32604,\"start\":32596},{\"end\":32623,\"start\":32614},{\"end\":32938,\"start\":32933},{\"end\":32954,\"start\":32950},{\"end\":32965,\"start\":32962},{\"end\":33194,\"start\":33186},{\"end\":33209,\"start\":33203},{\"end\":33220,\"start\":33217},{\"end\":33237,\"start\":33231},{\"end\":33244,\"start\":33238},{\"end\":33260,\"start\":33254},{\"end\":33533,\"start\":33529},{\"end\":33693,\"start\":33688},{\"end\":33702,\"start\":33701},{\"end\":33917,\"start\":33910},{\"end\":33929,\"start\":33922},{\"end\":33945,\"start\":33937},{\"end\":33955,\"start\":33951},{\"end\":34246,\"start\":34239},{\"end\":34258,\"start\":34251},{\"end\":34274,\"start\":34266},{\"end\":34284,\"start\":34280},{\"end\":34493,\"start\":34489},{\"end\":34506,\"start\":34502},{\"end\":34519,\"start\":34513},{\"end\":34533,\"start\":34527},{\"end\":34779,\"start\":34775},{\"end\":34794,\"start\":34788},{\"end\":34806,\"start\":34802},{\"end\":35025,\"start\":35020},{\"end\":35043,\"start\":35034},{\"end\":35205,\"start\":35204},{\"end\":35212,\"start\":35211},{\"end\":35223,\"start\":35222},{\"end\":35233,\"start\":35232},{\"end\":35239,\"start\":35238},{\"end\":35248,\"start\":35247},{\"end\":35259,\"start\":35258},{\"end\":35268,\"start\":35267},{\"end\":35280,\"start\":35279},{\"end\":35293,\"start\":35292},{\"end\":35556,\"start\":35555},{\"end\":35570,\"start\":35567},{\"end\":35756,\"start\":35751},{\"end\":35777,\"start\":35768},{\"end\":36022,\"start\":36017},{\"end\":36041,\"start\":36035},{\"end\":36051,\"start\":36047},{\"end\":36065,\"start\":36060},{\"end\":36294,\"start\":36288},{\"end\":36309,\"start\":36299},{\"end\":36322,\"start\":36317},{\"end\":36679,\"start\":36673},{\"end\":36689,\"start\":36684},{\"end\":37006,\"start\":36999},{\"end\":37018,\"start\":37012},{\"end\":37030,\"start\":37027},{\"end\":37242,\"start\":37238},{\"end\":37256,\"start\":37248},{\"end\":37271,\"start\":37263},{\"end\":37281,\"start\":37278},{\"end\":37619,\"start\":37614},{\"end\":37629,\"start\":37625},{\"end\":37643,\"start\":37635},{\"end\":37656,\"start\":37650},{\"end\":37860,\"start\":37854},{\"end\":37867,\"start\":37865},{\"end\":37879,\"start\":37873},{\"end\":37890,\"start\":37885},{\"end\":38128,\"start\":38123},{\"end\":38146,\"start\":38137},{\"end\":38158,\"start\":38154},{\"end\":38523,\"start\":38522},{\"end\":38535,\"start\":38529},{\"end\":38551,\"start\":38544},{\"end\":38562,\"start\":38558},{\"end\":38997,\"start\":38996},{\"end\":39010,\"start\":39003},{\"end\":39023,\"start\":39019},{\"end\":39409,\"start\":39408},{\"end\":39422,\"start\":39415},{\"end\":39435,\"start\":39431},{\"end\":39742,\"start\":39741},{\"end\":39754,\"start\":39748},{\"end\":39769,\"start\":39761},{\"end\":40046,\"start\":40042},{\"end\":40064,\"start\":40057},{\"end\":40081,\"start\":40076},{\"end\":40094,\"start\":40089},{\"end\":40297,\"start\":40290},{\"end\":40313,\"start\":40307},{\"end\":40330,\"start\":40325},{\"end\":40809,\"start\":40802},{\"end\":40825,\"start\":40819},{\"end\":40842,\"start\":40837},{\"end\":41249,\"start\":41243},{\"end\":41263,\"start\":41257},{\"end\":41276,\"start\":41271},{\"end\":41587,\"start\":41583},{\"end\":41604,\"start\":41596},{\"end\":41621,\"start\":41613},{\"end\":41623,\"start\":41622},{\"end\":41837,\"start\":41831},{\"end\":41850,\"start\":41849},{\"end\":42150,\"start\":42145},{\"end\":42167,\"start\":42161},{\"end\":42414,\"start\":42408},{\"end\":42426,\"start\":42420},{\"end\":42434,\"start\":42431},{\"end\":42455,\"start\":42450},{\"end\":42471,\"start\":42466},{\"end\":42700,\"start\":42692},{\"end\":42716,\"start\":42715},{\"end\":42728,\"start\":42725},{\"end\":42953,\"start\":42945},{\"end\":42969,\"start\":42968},{\"end\":42981,\"start\":42978},{\"end\":43250,\"start\":43241},{\"end\":43263,\"start\":43260},{\"end\":43277,\"start\":43269},{\"end\":43289,\"start\":43283},{\"end\":43305,\"start\":43300},{\"end\":43320,\"start\":43312},{\"end\":43338,\"start\":43331},{\"end\":43353,\"start\":43346},{\"end\":43371,\"start\":43365},{\"end\":43626,\"start\":43617},{\"end\":43639,\"start\":43636},{\"end\":43653,\"start\":43645},{\"end\":43665,\"start\":43659},{\"end\":43681,\"start\":43676},{\"end\":43696,\"start\":43688},{\"end\":43714,\"start\":43707},{\"end\":43729,\"start\":43722},{\"end\":43747,\"start\":43741},{\"end\":44033,\"start\":44027},{\"end\":44048,\"start\":44041},{\"end\":44068,\"start\":44060},{\"end\":44347,\"start\":44341},{\"end\":44362,\"start\":44355},{\"end\":44378,\"start\":44374},{\"end\":44656,\"start\":44652},{\"end\":44667,\"start\":44663},{\"end\":44669,\"start\":44668},{\"end\":44678,\"start\":44677},{\"end\":44695,\"start\":44694},{\"end\":45010,\"start\":45007},{\"end\":45023,\"start\":45017},{\"end\":45033,\"start\":45028},{\"end\":45364,\"start\":45360},{\"end\":45378,\"start\":45371},{\"end\":45385,\"start\":45384},{\"end\":45399,\"start\":45392},{\"end\":45757,\"start\":45753},{\"end\":45771,\"start\":45764},{\"end\":45778,\"start\":45777},{\"end\":45792,\"start\":45785}]", "bib_author_last_name": "[{\"end\":29646,\"start\":29640},{\"end\":29739,\"start\":29733},{\"end\":29830,\"start\":29811},{\"end\":30075,\"start\":30069},{\"end\":30091,\"start\":30085},{\"end\":30110,\"start\":30101},{\"end\":30124,\"start\":30117},{\"end\":30470,\"start\":30463},{\"end\":30482,\"start\":30477},{\"end\":30493,\"start\":30491},{\"end\":30506,\"start\":30504},{\"end\":30519,\"start\":30513},{\"end\":30527,\"start\":30525},{\"end\":30905,\"start\":30898},{\"end\":30917,\"start\":30912},{\"end\":30936,\"start\":30930},{\"end\":30945,\"start\":30938},{\"end\":30980,\"start\":30968},{\"end\":31318,\"start\":31312},{\"end\":31332,\"start\":31327},{\"end\":31349,\"start\":31342},{\"end\":31366,\"start\":31357},{\"end\":31381,\"start\":31376},{\"end\":31637,\"start\":31633},{\"end\":31652,\"start\":31645},{\"end\":31666,\"start\":31662},{\"end\":31682,\"start\":31677},{\"end\":31884,\"start\":31877},{\"end\":31907,\"start\":31895},{\"end\":32404,\"start\":32397},{\"end\":32543,\"start\":32539},{\"end\":32562,\"start\":32552},{\"end\":32579,\"start\":32572},{\"end\":32594,\"start\":32588},{\"end\":32612,\"start\":32605},{\"end\":32631,\"start\":32624},{\"end\":32948,\"start\":32939},{\"end\":32960,\"start\":32955},{\"end\":32972,\"start\":32966},{\"end\":33201,\"start\":33195},{\"end\":33215,\"start\":33210},{\"end\":33229,\"start\":33221},{\"end\":33252,\"start\":33245},{\"end\":33266,\"start\":33261},{\"end\":33539,\"start\":33534},{\"end\":33699,\"start\":33694},{\"end\":33709,\"start\":33703},{\"end\":33715,\"start\":33711},{\"end\":33920,\"start\":33918},{\"end\":33935,\"start\":33930},{\"end\":33949,\"start\":33946},{\"end\":33959,\"start\":33956},{\"end\":34249,\"start\":34247},{\"end\":34264,\"start\":34259},{\"end\":34278,\"start\":34275},{\"end\":34288,\"start\":34285},{\"end\":34500,\"start\":34494},{\"end\":34511,\"start\":34507},{\"end\":34525,\"start\":34520},{\"end\":34542,\"start\":34534},{\"end\":34786,\"start\":34780},{\"end\":34800,\"start\":34795},{\"end\":34811,\"start\":34807},{\"end\":35032,\"start\":35026},{\"end\":35052,\"start\":35044},{\"end\":35209,\"start\":35206},{\"end\":35220,\"start\":35213},{\"end\":35230,\"start\":35224},{\"end\":35236,\"start\":35234},{\"end\":35245,\"start\":35240},{\"end\":35256,\"start\":35249},{\"end\":35265,\"start\":35260},{\"end\":35277,\"start\":35269},{\"end\":35290,\"start\":35281},{\"end\":35302,\"start\":35294},{\"end\":35565,\"start\":35557},{\"end\":35577,\"start\":35571},{\"end\":35586,\"start\":35579},{\"end\":35766,\"start\":35757},{\"end\":35784,\"start\":35778},{\"end\":36033,\"start\":36023},{\"end\":36045,\"start\":36042},{\"end\":36058,\"start\":36052},{\"end\":36071,\"start\":36066},{\"end\":36297,\"start\":36295},{\"end\":36315,\"start\":36310},{\"end\":36326,\"start\":36323},{\"end\":36682,\"start\":36680},{\"end\":36693,\"start\":36690},{\"end\":37010,\"start\":37007},{\"end\":37025,\"start\":37019},{\"end\":37036,\"start\":37031},{\"end\":37246,\"start\":37243},{\"end\":37261,\"start\":37257},{\"end\":37276,\"start\":37272},{\"end\":37285,\"start\":37282},{\"end\":37623,\"start\":37620},{\"end\":37633,\"start\":37630},{\"end\":37648,\"start\":37644},{\"end\":37661,\"start\":37657},{\"end\":37863,\"start\":37861},{\"end\":37871,\"start\":37868},{\"end\":37883,\"start\":37880},{\"end\":37898,\"start\":37891},{\"end\":38135,\"start\":38129},{\"end\":38152,\"start\":38147},{\"end\":38169,\"start\":38159},{\"end\":38527,\"start\":38524},{\"end\":38542,\"start\":38536},{\"end\":38556,\"start\":38552},{\"end\":38572,\"start\":38563},{\"end\":38581,\"start\":38574},{\"end\":39001,\"start\":38998},{\"end\":39017,\"start\":39011},{\"end\":39033,\"start\":39024},{\"end\":39042,\"start\":39035},{\"end\":39413,\"start\":39410},{\"end\":39429,\"start\":39423},{\"end\":39445,\"start\":39436},{\"end\":39454,\"start\":39447},{\"end\":39746,\"start\":39743},{\"end\":39759,\"start\":39755},{\"end\":39774,\"start\":39770},{\"end\":39784,\"start\":39776},{\"end\":40055,\"start\":40047},{\"end\":40074,\"start\":40065},{\"end\":40087,\"start\":40082},{\"end\":40102,\"start\":40095},{\"end\":40305,\"start\":40298},{\"end\":40323,\"start\":40314},{\"end\":40340,\"start\":40331},{\"end\":40817,\"start\":40810},{\"end\":40835,\"start\":40826},{\"end\":40852,\"start\":40843},{\"end\":41255,\"start\":41250},{\"end\":41269,\"start\":41264},{\"end\":41284,\"start\":41277},{\"end\":41594,\"start\":41588},{\"end\":41611,\"start\":41605},{\"end\":41630,\"start\":41624},{\"end\":41847,\"start\":41838},{\"end\":41856,\"start\":41851},{\"end\":41864,\"start\":41858},{\"end\":42159,\"start\":42151},{\"end\":42177,\"start\":42168},{\"end\":42418,\"start\":42415},{\"end\":42429,\"start\":42427},{\"end\":42448,\"start\":42435},{\"end\":42464,\"start\":42456},{\"end\":42479,\"start\":42472},{\"end\":42486,\"start\":42481},{\"end\":42713,\"start\":42701},{\"end\":42723,\"start\":42717},{\"end\":42734,\"start\":42729},{\"end\":42758,\"start\":42736},{\"end\":42966,\"start\":42954},{\"end\":42976,\"start\":42970},{\"end\":42987,\"start\":42982},{\"end\":43011,\"start\":42989},{\"end\":43258,\"start\":43251},{\"end\":43267,\"start\":43264},{\"end\":43281,\"start\":43278},{\"end\":43298,\"start\":43290},{\"end\":43310,\"start\":43306},{\"end\":43329,\"start\":43321},{\"end\":43344,\"start\":43339},{\"end\":43363,\"start\":43354},{\"end\":43382,\"start\":43372},{\"end\":43634,\"start\":43627},{\"end\":43643,\"start\":43640},{\"end\":43657,\"start\":43654},{\"end\":43674,\"start\":43666},{\"end\":43686,\"start\":43682},{\"end\":43705,\"start\":43697},{\"end\":43720,\"start\":43715},{\"end\":43739,\"start\":43730},{\"end\":43758,\"start\":43748},{\"end\":44039,\"start\":44034},{\"end\":44058,\"start\":44049},{\"end\":44076,\"start\":44069},{\"end\":44353,\"start\":44348},{\"end\":44372,\"start\":44363},{\"end\":44389,\"start\":44379},{\"end\":44661,\"start\":44657},{\"end\":44675,\"start\":44670},{\"end\":44684,\"start\":44679},{\"end\":44692,\"start\":44686},{\"end\":44700,\"start\":44696},{\"end\":44712,\"start\":44702},{\"end\":45015,\"start\":45011},{\"end\":45026,\"start\":45024},{\"end\":45037,\"start\":45034},{\"end\":45369,\"start\":45365},{\"end\":45382,\"start\":45379},{\"end\":45390,\"start\":45386},{\"end\":45407,\"start\":45400},{\"end\":45414,\"start\":45409},{\"end\":45762,\"start\":45758},{\"end\":45775,\"start\":45772},{\"end\":45783,\"start\":45779},{\"end\":45800,\"start\":45793},{\"end\":45807,\"start\":45802}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":29729,\"start\":29626},{\"attributes\":{\"id\":\"b1\"},\"end\":29807,\"start\":29731},{\"attributes\":{\"id\":\"b2\"},\"end\":29917,\"start\":29809},{\"attributes\":{\"id\":\"b3\"},\"end\":30001,\"start\":29919},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52157475},\"end\":30409,\"start\":30003},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":195732375},\"end\":30815,\"start\":30411},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":70350052},\"end\":31242,\"start\":30817},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1154654},\"end\":31601,\"start\":31244},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52070144},\"end\":31789,\"start\":31603},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":158865631},\"end\":32113,\"start\":31791},{\"attributes\":{\"id\":\"b10\"},\"end\":32323,\"start\":32115},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2375110},\"end\":32507,\"start\":32325},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8692593},\"end\":32832,\"start\":32509},{\"attributes\":{\"doi\":\"arXiv:1910.08854\",\"id\":\"b13\"},\"end\":33184,\"start\":32834},{\"attributes\":{\"id\":\"b14\"},\"end\":33527,\"start\":33186},{\"attributes\":{\"id\":\"b15\"},\"end\":33628,\"start\":33529},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":61808533},\"end\":33829,\"start\":33630},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":436933},\"end\":34191,\"start\":33831},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":34411,\"start\":34193},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3568073},\"end\":34699,\"start\":34413},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":54482423},\"end\":34949,\"start\":34701},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2031947},\"end\":35180,\"start\":34951},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":44073530},\"end\":35520,\"start\":35182},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":216078090},\"end\":35678,\"start\":35522},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b24\"},\"end\":35963,\"start\":35680},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14191515},\"end\":36205,\"start\":35965},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":47013913},\"end\":36609,\"start\":36207},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53298495},\"end\":36947,\"start\":36611},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3783306},\"end\":37157,\"start\":36949},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8723979},\"end\":37569,\"start\":37159},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":459456},\"end\":37779,\"start\":37571},{\"attributes\":{\"id\":\"b31\"},\"end\":38049,\"start\":37781},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":61810032},\"end\":38433,\"start\":38051},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":189928086},\"end\":38916,\"start\":38435},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":53081848},\"end\":39347,\"start\":38918},{\"attributes\":{\"doi\":\"arXiv:1910.12467\",\"id\":\"b35\"},\"end\":39634,\"start\":39349},{\"attributes\":{\"doi\":\"arXiv:1803.07716\",\"id\":\"b36\"},\"end\":40009,\"start\":39636},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14088925},\"end\":40288,\"start\":40011},{\"attributes\":{\"doi\":\"arXiv:1803.09179\",\"id\":\"b38\"},\"end\":40736,\"start\":40290},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":59292011},\"end\":41097,\"start\":40738},{\"attributes\":{\"doi\":\"arXiv:1905.00582\",\"id\":\"b40\"},\"end\":41547,\"start\":41099},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3603485},\"end\":41747,\"start\":41549},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":7071215},\"end\":42075,\"start\":41749},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b43\"},\"end\":42352,\"start\":42077},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3970249},\"end\":42648,\"start\":42354},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":16508231},\"end\":42893,\"start\":42650},{\"attributes\":{\"id\":\"b46\"},\"end\":43207,\"start\":42895},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206592484},\"end\":43583,\"start\":43209},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206592484},\"end\":43959,\"start\":43585},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":219950625},\"end\":44231,\"start\":43961},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":56894332},\"end\":44576,\"start\":44233},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":207761262},\"end\":44954,\"start\":44578},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":53295714},\"end\":45302,\"start\":44956},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":4533859},\"end\":45694,\"start\":45304},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":44149078},\"end\":45945,\"start\":45696}]", "bib_title": "[{\"end\":30060,\"start\":30003},{\"end\":30454,\"start\":30411},{\"end\":30894,\"start\":30817},{\"end\":31303,\"start\":31244},{\"end\":31622,\"start\":31603},{\"end\":31868,\"start\":31791},{\"end\":32386,\"start\":32325},{\"end\":32531,\"start\":32509},{\"end\":33686,\"start\":33630},{\"end\":33908,\"start\":33831},{\"end\":34237,\"start\":34193},{\"end\":34487,\"start\":34413},{\"end\":34773,\"start\":34701},{\"end\":35018,\"start\":34951},{\"end\":35202,\"start\":35182},{\"end\":35553,\"start\":35522},{\"end\":36015,\"start\":35965},{\"end\":36286,\"start\":36207},{\"end\":36671,\"start\":36611},{\"end\":36997,\"start\":36949},{\"end\":37236,\"start\":37159},{\"end\":37612,\"start\":37571},{\"end\":37852,\"start\":37781},{\"end\":38121,\"start\":38051},{\"end\":38520,\"start\":38435},{\"end\":38994,\"start\":38918},{\"end\":40040,\"start\":40011},{\"end\":40800,\"start\":40738},{\"end\":41581,\"start\":41549},{\"end\":41829,\"start\":41749},{\"end\":42406,\"start\":42354},{\"end\":42690,\"start\":42650},{\"end\":42943,\"start\":42895},{\"end\":43239,\"start\":43209},{\"end\":43615,\"start\":43585},{\"end\":44025,\"start\":43961},{\"end\":44339,\"start\":44233},{\"end\":44650,\"start\":44578},{\"end\":45005,\"start\":44956},{\"end\":45358,\"start\":45304},{\"end\":45751,\"start\":45696}]", "bib_author": "[{\"end\":29648,\"start\":29628},{\"end\":29741,\"start\":29733},{\"end\":29832,\"start\":29811},{\"end\":30077,\"start\":30062},{\"end\":30093,\"start\":30077},{\"end\":30112,\"start\":30093},{\"end\":30126,\"start\":30112},{\"end\":30472,\"start\":30456},{\"end\":30484,\"start\":30472},{\"end\":30495,\"start\":30484},{\"end\":30508,\"start\":30495},{\"end\":30521,\"start\":30508},{\"end\":30529,\"start\":30521},{\"end\":30907,\"start\":30896},{\"end\":30919,\"start\":30907},{\"end\":30938,\"start\":30919},{\"end\":30947,\"start\":30938},{\"end\":30982,\"start\":30947},{\"end\":31320,\"start\":31305},{\"end\":31334,\"start\":31320},{\"end\":31351,\"start\":31334},{\"end\":31368,\"start\":31351},{\"end\":31383,\"start\":31368},{\"end\":31639,\"start\":31624},{\"end\":31654,\"start\":31639},{\"end\":31668,\"start\":31654},{\"end\":31684,\"start\":31668},{\"end\":31886,\"start\":31870},{\"end\":31909,\"start\":31886},{\"end\":32406,\"start\":32388},{\"end\":32545,\"start\":32533},{\"end\":32564,\"start\":32545},{\"end\":32581,\"start\":32564},{\"end\":32596,\"start\":32581},{\"end\":32614,\"start\":32596},{\"end\":32633,\"start\":32614},{\"end\":32950,\"start\":32933},{\"end\":32962,\"start\":32950},{\"end\":32974,\"start\":32962},{\"end\":33203,\"start\":33186},{\"end\":33217,\"start\":33203},{\"end\":33231,\"start\":33217},{\"end\":33254,\"start\":33231},{\"end\":33268,\"start\":33254},{\"end\":33541,\"start\":33529},{\"end\":33701,\"start\":33688},{\"end\":33711,\"start\":33701},{\"end\":33717,\"start\":33711},{\"end\":33922,\"start\":33910},{\"end\":33937,\"start\":33922},{\"end\":33951,\"start\":33937},{\"end\":33961,\"start\":33951},{\"end\":34251,\"start\":34239},{\"end\":34266,\"start\":34251},{\"end\":34280,\"start\":34266},{\"end\":34290,\"start\":34280},{\"end\":34502,\"start\":34489},{\"end\":34513,\"start\":34502},{\"end\":34527,\"start\":34513},{\"end\":34544,\"start\":34527},{\"end\":34788,\"start\":34775},{\"end\":34802,\"start\":34788},{\"end\":34813,\"start\":34802},{\"end\":35034,\"start\":35020},{\"end\":35054,\"start\":35034},{\"end\":35211,\"start\":35204},{\"end\":35222,\"start\":35211},{\"end\":35232,\"start\":35222},{\"end\":35238,\"start\":35232},{\"end\":35247,\"start\":35238},{\"end\":35258,\"start\":35247},{\"end\":35267,\"start\":35258},{\"end\":35279,\"start\":35267},{\"end\":35292,\"start\":35279},{\"end\":35304,\"start\":35292},{\"end\":35567,\"start\":35555},{\"end\":35579,\"start\":35567},{\"end\":35588,\"start\":35579},{\"end\":35768,\"start\":35751},{\"end\":35786,\"start\":35768},{\"end\":36035,\"start\":36017},{\"end\":36047,\"start\":36035},{\"end\":36060,\"start\":36047},{\"end\":36073,\"start\":36060},{\"end\":36299,\"start\":36288},{\"end\":36317,\"start\":36299},{\"end\":36328,\"start\":36317},{\"end\":36684,\"start\":36673},{\"end\":36695,\"start\":36684},{\"end\":37012,\"start\":36999},{\"end\":37027,\"start\":37012},{\"end\":37038,\"start\":37027},{\"end\":37248,\"start\":37238},{\"end\":37263,\"start\":37248},{\"end\":37278,\"start\":37263},{\"end\":37287,\"start\":37278},{\"end\":37625,\"start\":37614},{\"end\":37635,\"start\":37625},{\"end\":37650,\"start\":37635},{\"end\":37663,\"start\":37650},{\"end\":37865,\"start\":37854},{\"end\":37873,\"start\":37865},{\"end\":37885,\"start\":37873},{\"end\":37900,\"start\":37885},{\"end\":38137,\"start\":38123},{\"end\":38154,\"start\":38137},{\"end\":38171,\"start\":38154},{\"end\":38529,\"start\":38522},{\"end\":38544,\"start\":38529},{\"end\":38558,\"start\":38544},{\"end\":38574,\"start\":38558},{\"end\":38583,\"start\":38574},{\"end\":39003,\"start\":38996},{\"end\":39019,\"start\":39003},{\"end\":39035,\"start\":39019},{\"end\":39044,\"start\":39035},{\"end\":39415,\"start\":39408},{\"end\":39431,\"start\":39415},{\"end\":39447,\"start\":39431},{\"end\":39456,\"start\":39447},{\"end\":39748,\"start\":39741},{\"end\":39761,\"start\":39748},{\"end\":39776,\"start\":39761},{\"end\":39786,\"start\":39776},{\"end\":40057,\"start\":40042},{\"end\":40076,\"start\":40057},{\"end\":40089,\"start\":40076},{\"end\":40104,\"start\":40089},{\"end\":40307,\"start\":40290},{\"end\":40325,\"start\":40307},{\"end\":40342,\"start\":40325},{\"end\":40819,\"start\":40802},{\"end\":40837,\"start\":40819},{\"end\":40854,\"start\":40837},{\"end\":41257,\"start\":41243},{\"end\":41271,\"start\":41257},{\"end\":41286,\"start\":41271},{\"end\":41596,\"start\":41583},{\"end\":41613,\"start\":41596},{\"end\":41632,\"start\":41613},{\"end\":41849,\"start\":41831},{\"end\":41858,\"start\":41849},{\"end\":41866,\"start\":41858},{\"end\":42161,\"start\":42145},{\"end\":42179,\"start\":42161},{\"end\":42420,\"start\":42408},{\"end\":42431,\"start\":42420},{\"end\":42450,\"start\":42431},{\"end\":42466,\"start\":42450},{\"end\":42481,\"start\":42466},{\"end\":42488,\"start\":42481},{\"end\":42715,\"start\":42692},{\"end\":42725,\"start\":42715},{\"end\":42736,\"start\":42725},{\"end\":42760,\"start\":42736},{\"end\":42968,\"start\":42945},{\"end\":42978,\"start\":42968},{\"end\":42989,\"start\":42978},{\"end\":43013,\"start\":42989},{\"end\":43260,\"start\":43241},{\"end\":43269,\"start\":43260},{\"end\":43283,\"start\":43269},{\"end\":43300,\"start\":43283},{\"end\":43312,\"start\":43300},{\"end\":43331,\"start\":43312},{\"end\":43346,\"start\":43331},{\"end\":43365,\"start\":43346},{\"end\":43384,\"start\":43365},{\"end\":43636,\"start\":43617},{\"end\":43645,\"start\":43636},{\"end\":43659,\"start\":43645},{\"end\":43676,\"start\":43659},{\"end\":43688,\"start\":43676},{\"end\":43707,\"start\":43688},{\"end\":43722,\"start\":43707},{\"end\":43741,\"start\":43722},{\"end\":43760,\"start\":43741},{\"end\":44041,\"start\":44027},{\"end\":44060,\"start\":44041},{\"end\":44078,\"start\":44060},{\"end\":44355,\"start\":44341},{\"end\":44374,\"start\":44355},{\"end\":44391,\"start\":44374},{\"end\":44663,\"start\":44652},{\"end\":44677,\"start\":44663},{\"end\":44686,\"start\":44677},{\"end\":44694,\"start\":44686},{\"end\":44702,\"start\":44694},{\"end\":44714,\"start\":44702},{\"end\":45017,\"start\":45007},{\"end\":45028,\"start\":45017},{\"end\":45039,\"start\":45028},{\"end\":45371,\"start\":45360},{\"end\":45384,\"start\":45371},{\"end\":45392,\"start\":45384},{\"end\":45409,\"start\":45392},{\"end\":45416,\"start\":45409},{\"end\":45764,\"start\":45753},{\"end\":45777,\"start\":45764},{\"end\":45785,\"start\":45777},{\"end\":45802,\"start\":45785},{\"end\":45809,\"start\":45802}]", "bib_venue": "[{\"end\":29926,\"start\":29919},{\"end\":30198,\"start\":30126},{\"end\":30605,\"start\":30529},{\"end\":31019,\"start\":30982},{\"end\":31411,\"start\":31383},{\"end\":31688,\"start\":31684},{\"end\":31930,\"start\":31909},{\"end\":32195,\"start\":32115},{\"end\":32410,\"start\":32406},{\"end\":32661,\"start\":32633},{\"end\":32931,\"start\":32834},{\"end\":33354,\"start\":33268},{\"end\":33564,\"start\":33541},{\"end\":33724,\"start\":33717},{\"end\":33978,\"start\":33961},{\"end\":34294,\"start\":34290},{\"end\":34548,\"start\":34544},{\"end\":34817,\"start\":34813},{\"end\":35058,\"start\":35054},{\"end\":35332,\"start\":35304},{\"end\":35592,\"start\":35588},{\"end\":35749,\"start\":35680},{\"end\":36077,\"start\":36073},{\"end\":36400,\"start\":36328},{\"end\":36763,\"start\":36695},{\"end\":37045,\"start\":37038},{\"end\":37355,\"start\":37287},{\"end\":37667,\"start\":37663},{\"end\":37907,\"start\":37900},{\"end\":38232,\"start\":38171},{\"end\":38667,\"start\":38583},{\"end\":39125,\"start\":39044},{\"end\":39406,\"start\":39349},{\"end\":39739,\"start\":39636},{\"end\":40143,\"start\":40104},{\"end\":40490,\"start\":40358},{\"end\":40905,\"start\":40854},{\"end\":41241,\"start\":41099},{\"end\":41639,\"start\":41632},{\"end\":41904,\"start\":41866},{\"end\":42143,\"start\":42077},{\"end\":42492,\"start\":42488},{\"end\":42764,\"start\":42760},{\"end\":43041,\"start\":43013},{\"end\":43388,\"start\":43384},{\"end\":43764,\"start\":43760},{\"end\":44086,\"start\":44078},{\"end\":44395,\"start\":44391},{\"end\":44757,\"start\":44714},{\"end\":45120,\"start\":45039},{\"end\":45484,\"start\":45416},{\"end\":45813,\"start\":45809}]"}}}, "year": 2023, "month": 12, "day": 17}