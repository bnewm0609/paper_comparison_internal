{"id": 248228056, "updated": "2023-10-05 15:12:11.8", "metadata": {"title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking", "authors": "[{\"first\":\"Yupan\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Tengchao\",\"last\":\"Lv\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Cui\",\"middle\":[]},{\"first\":\"Yutong\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Furu\",\"last\":\"Wei\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose \\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at \\url{https://aka.ms/layoutlmv3}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.08387", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/HuangL0LW22", "doi": "10.1145/3503161.3548112"}}, "content": {"source": {"pdf_hash": "c689d1f3ae2447fd5b2f108b5b4436276e4d3761", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.08387v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "47dd0cb43ad487afe169d2d2c1850866996fa136", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c689d1f3ae2447fd5b2f108b5b4436276e4d3761.txt", "contents": "\nLay-outLMv3: Pre-training for Document AI with Unified Text and Image Mask-ing\nACMCopyright ACM2022. October 10-14, 2022. October 10-14, 2022\n\nYupan Huang \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nTengchao Lv tengchaolv@microsoft.com \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nLei Cui \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nYutong Lu luyutong@mail.sysu.edu.cn \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nFuru Wei fuwei@microsoft.com \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nYupan Huang \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nTengchao Lv \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nLei Cui \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nYutong Lu \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nFuru Wei \nSun Yat-sen University\nSun Yat-sen University\nMicrosoft Research Asia\nMicrosoft Research Asia\nMicrosoft Research Asia\n\n\nLay-outLMv3: Pre-training for Document AI with Unified Text and Image Mask-ing\n\nProceedings of the 30th ACM International Conference on Multimedia (MM '22)\nthe 30th ACM International Conference on Multimedia (MM '22)Lisboa, Portugal; New York, NY, USA, MM '22; Lisboa, PortugalACM2022. October 10-14, 2022. October 10-14, 202210.1145/3503161.3548112ACM Reference Format: 10 pages. https://doi.org/10.1145/3503161.3548112 * Contribution during internship at Microsoft Research. Corresponding authors: Lei Cui and Furu Wei.CCS CONCEPTSApplied computing \u2192 Document analysis;Computing methodologies \u2192 Natural language processing KEYWORDS document ai, layoutlm, multimodal pre-training, vision-and-language\nSelf-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn crossmodal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pretrained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves stateof-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3.\n\nINTRODUCTION\n\nIn recent years, pre-training techniques have been making waves in the Document AI community by achieving remarkable progress on document understanding tasks [2, 13-15, 17, 25, 28, 31, 32, 40, 41, 50, 52, 54-56]. As shown in Figure 1, a pre-trained Document AI model can parse layout and extract key information for various documents such as scanned forms and academic papers, which is important for industrial applications and academic research [8].\n\nSelf-supervised pre-training techniques have made rapid progress in representation learning due to their successful applications of reconstructive pre-training objectives. In NLP research, BERT firstly proposed \"masked language modeling\" (MLM) to learn bidirectional representations by predicting the original vocabulary id of a randomly masked word token based on its context [9]. Whereas most performant multimodal pre-trained Document AI models use the MLM proposed by BERT for text modality, they differ in pretraining objectives for image modality as depicted in Figure 2. For example, DocFormer learns to reconstruct image pixels through a CNN decoder [2], which tends to learn noisy details rather than high-level structures such as document layouts [43,45]. SelfDoc proposes to regress masked region features [31], which is noisier and harder to learn than classifying discrete features in a smaller vocabulary [6,18]. The different granularities of image (e.g., dense image pixels or contiguous region features) and text (i.e., discrete Figure 2: Comparisons with existing works (e.g., DocFormer [2] and SelfDoc [31]) on (1) image embedding: our Lay-outLMv3 uses linear patches to reduce the computational bottleneck of CNNs and eliminate the need for region supervision in training object detectors; (2) pre-training objectives on image modality: our LayoutLMv3 learns to reconstruct discrete image tokens of masked patches instead of raw pixels or region features to capture high-level layout structures rather than noisy details.\n\ntokens) objectives further add difficulty to cross-modal alignment learning, which is essential to multimodal representation learning.\n\nTo overcome the discrepancy in pre-training objectives of text and image modalities and facilitate multimodal representation learning, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking objectives MLM and MIM. As shown in Figure 3, LayoutLMv3 learns to reconstruct masked word tokens of the text modality and symmetrically reconstruct masked patch tokens of the image modality. Inspired by DALL-E [43] and BEiT [3], we obtain the target image tokens from latent codes of a discrete VAE. For documents, each text word corresponds to an image patch. To learn this cross-modal alignment, we propose a Word-Patch Alignment (WPA) objective to predict whether the corresponding image patch of a text word is masked. Inspired by ViT [11] and ViLT [22], LayoutLMv3 directly leverages raw image patches from document images without complex pre-processing steps such as page object detection. LayoutLMv3 jointly learns image, text and multimodal representations in a Transformer model with unified MLM, MIM and WPA objectives. This makes LayoutLMv3 the first multimodal pre-trained Document AI model without CNNs for image embeddings, which significantly saves parameters and gets rid of region annotations. The simple unified architecture and objectives make LayoutLMv3 a generalpurpose pre-trained model for both text-centric tasks and imagecentric Document AI tasks.\n\nWe evaluated pre-trained LayoutLMv3 models across five public benchmarks, including text-centric benchmarks: FUNSD [20] for form understanding, CORD [39] for receipt understanding, DocVQA [38] for document visual question answering, and imagecentric benchmarks: RVL-CDIP [16] for document image classification, PubLayNet [59] for document layout analysis. Experiment results demonstrate that LayoutLMv3 achieves state-of-the-art performance on these benchmarks with parameter efficiency. Furthermore, LayoutLMv3 is easy to reproduce for its simple and neat architecture and pre-training objectives.\n\nOur contributions are summarized as follows:\n\n\u2022 LayoutLMv3 is the first multimodal model in Document AI that does not rely on a pre-trained CNN or Faster R-CNN backbone to extract visual features, which significantly saves parameters and eliminates region annotations. \u2022 LayoutLMv3 mitigates the discrepancy between text and image multimodal representation learning with unified discrete token reconstructive objectives MLM and MIM. We further propose a Word-Patch Alignment (WPA) objective to facilitate cross-modal alignment learning. \u2022 LayoutLMv3 is a general-purpose model for both text-centric and image-centric Document AI tasks. For the first time, we demonstrate the generality of multimodal Transformers to vision tasks in Document AI. \u2022 Experimental results show that LayoutLMv3 achieves stateof-the-art performance in text-centric tasks and image-centric tasks in Document AI. The code and models are publicly available at https://aka.ms/layoutlmv3. Figure 3 gives an overview of the LayoutLMv3.\n\n\nLAYOUTLMV3\n\n\nModel Architecture\n\nLayoutLMv3 applies a unified text-image multimodal Transformer to learn cross-modal representations. The Transformer has a multilayer architecture and each layer mainly consists of multi-head self-attention and position-wise fully connected feed-forward networks [49]. The input of Transformer is a concatenation of text embedding Y = y 1: and image embedding X = x 1: sequences, where and are sequence lengths for text and image respectively. Through the Transformer, the last layer outputs text-and-image contextual representations. Text Embedding. Text embedding is a combination of word embeddings and position embeddings. We pre-processed document images with an off-the-shelf OCR toolkit to obtain textual content and corresponding 2D position information. We initialize the word embeddings with a word embedding matrix from a pre-trained model RoBERTa [36]. The position embeddings include 1D position and 2D layout position embeddings, where the 1D position refers to the index of tokens within the text sequence, and the 2D layout position refers to the bounding box coordinates of the text \n(T 1 ) (T 2 ) (V 2 ) (V 3 )\nLinear Embedding Word Embedding sequence. Following the LayoutLM, we normalize all coordinates by the size of images, and use embedding layers to embed x-axis, y-axis, width and height features separately [54]. The LayoutLM and LayoutLMv2 adopt word-level layout positions, where each word has its positions. Instead, we adopt segment-level layout positions that words in a segment share the same 2D position since the words usually express the same semantic meaning [28]. Image Embedding. Existing multimodal models in Document AI either extract CNN grid features [2,56] or rely on an object detector like Faster R-CNN [44] to extract region features [14,31,40,54] for image embeddings, which accounts for heavy computation bottleneck or require region supervision. Inspired by ViT [11] and ViLT [22], we represent document images with linear projection features of image patches before feeding them into the multimodal Transformer. Specifically, we resize a document image into \u00d7 and denote the image with I \u2208 R \u00d7 \u00d7 , where , and are the channel size, width and height of the image respectively. We then split the image into a sequence of uniform \u00d7 patches, linearly project the image patches to dimensions and flatten them into a sequence of vectors, which length is = / 2 . Then we add learnable 1D position embeddings to each patch since we have not observed improvements from using 2D position embeddings in our preliminary experiments. LayoutLMv3 is the first multimodal model in Document AI that does not rely on CNNs to extract image features, which is vital to Document AI models to reduce parameters or remove complex pre-processing steps. We insert semantic 1D relative position and spatial 2D relative position as bias terms in self-attention networks for text and image modalities following LayoutLMv2 [56].\n\n\nPre-training Objectives\n\nLayoutLMv3 is pre-trained with the MLM, MIM, and WPA objectives to learn multimodal representation in a self-supervised learning manner. Full pre-training objectives of LayoutLMv3 is defined as = + + . Objective I: Masked Language Modeling (MLM). For the language side, our MLM is inspired by the masked language modeling in BERT [9] and masked visual-language modeling in Lay-outLM [54] and LayoutLMv2 [56]. We mask 30% of text tokens with a span masking strategy with span lengths drawn from a Poisson distribution ( = 3) [21,27]. The pre-training objective is to maximize the log-likelihood of the correct masked text tokens y based on the contextual representations of corrupted sequences of image tokens X \u2032 and text tokens Y \u2032 , where \u2032 and \u2032 represent the masked positions. We denote parameters of the Transformer model with and minimize the subsequent cross-entropy loss:\n( ) = \u2212 \u2032 \u2211\ufe01 =1 log y \u2113 | X \u2032 , Y \u2032(1)\nAs we keep the layout information unchanged, this objective facilitates the model to learn the correspondence between layout information and text and image context. Objective II: Masked Image Modeling (MIM). To encourage the model to interpret visual content from contextual text and image representations, we adapt the MIM pre-training objective in BEiT [3] to our multimodal Transformer model. The MIM objective is a symmetry to the MLM objective, that we randomly mask a percentage of about 40% image tokens with the blockwise masking strategy [3]. The MIM objective is driven by a cross-entropy loss to reconstruct the masked image tokens x under the context of their surrounding text and image tokens.\n( ) = \u2212 \u2032 \u2211\ufe01 =1 log x | X \u2032 , Y \u2032(2)\nThe labels of image tokens come from an image tokenizer, which can transform dense image pixels into discrete tokens according to a visual vocabulary [43]. Thus MIM facilitates learning high-level layout structures rather than noisy low-level details.\n\nObjective III: Word-Patch Alignment (WPA). For documents, each text word corresponds to an image patch. As we randomly mask text and image tokens with MLM and MIM respectively, there is no explicit alignment learning between text and image modalities. We thus propose a WPA objective to learn a fine-grained alignment between text words and image patches. The WPA objective is to predict whether the corresponding image patches of a text word are masked. Specifically, we assign an aligned label to an unmasked text token when its corresponding image tokens are also unmasked. Otherwise, we assign an unaligned label. We exclude the masked text tokens when calculating WPA loss to prevent the model from learning a correspondence between masked text words and image patches. We use a two-layer MLP head that inputs contextual text and image and outputs the binary aligned/unaligned labels with a binary cross-entropy loss:\n( ) = \u2212 \u2212 \u2032 \u2211\ufe01 \u2113=1 log z \u2113 | X \u2032 , Y \u2032 ,(3)\nwhere \u2212 \u2032 is the number of unmasked text tokens, z \u2113 is the binary label of language token in the \u2113 position.\n\n\nEXPERIMENTS 3.1 Model Configurations\n\nThe network architecture of LayoutLMv3 follows that of LayoutLM [54] and LayoutLMv2 [56] for a fair comparison. We use base and large model sizes for LayoutLMv3. LayoutLMv3 BASE adopts a 12-layer Transformer encoder with 12-head self-attention, hidden size of = 768, and 3,072 intermediate size of feed-forward networks. LayoutLMv3 LARGE adopts a 24-layer Transformer encoder with 16-head self-attention, hidden size of = 1, 024, and 4,096 intermediate size of feed-forward networks. To pre-process the text input, we tokenize the text sequence with Byte-Pair Encoding (BPE) [46] with a maximum sequence length = 512. We add a [CLS] and a [SEP] token at the beginning and end of each text sequence. When the length of the text sequence is shorter than , we append [PAD] tokens to it. The bounding box coordinates of these special tokens are all zeros. The parameters for image embedding are\n\u00d7 \u00d7 = 3 \u00d7 224 \u00d7 224, = 16, = 196.\nWe adopt distributed and mixed-precision training to reduce memory costs and speed up training procedures. We also use a gradient accumulation mechanism to split the batch of samples into several mini-batches to overcome memory constraints for large batch sizes. We further use a gradient checkpointing technique for document layout analysis to reduce memory costs. To stabilize training, we follow CogView [10] to change the computation of attention to softmax\nQ K \u221a = softmax Q \u221a K \u2212 max Q \u221a K \u00d7 , where is 32.\n\nPre-training LayoutLMv3\n\nTo learn a universal representation for various document tasks, we pre-train LayoutLMv3 on a large IIT-CDIP dataset. The IIT-CDIP Test Collection 1.0 is a large-scale scanned document image dataset, which contains about 11 million document images and can split into 42 million pages [26]. We only use 11 million of them to train LayoutLMv3. We do not do image augmentation following LayoutLM models [54,56]. For the multimodal Transformer encoder along with the text embedding layer, LayoutLMv3 is initialized from the pre-trained weights of RoBERTa [36]. Our image tokenizer is initialized from a pre-trained image tokenizer in DiT, a self-supervised pre-trained document image Transformer model [30]. The vocabulary size of image tokens is 8,192. We randomly initialized the rest model parameters. We pre-train Lay-outLMv3 using Adam optimizer [23] with a batch size of 2,048 for 500,000 steps. We use a weight decay of 1 \u2212 2, and ( 1, 2) = (0.9, 0.98). For the LayoutLMv3 BASE model, we use a learning rate of 1 \u2212 4, and we linearly warm up the learning rate over the first 4.8% steps. For LayoutLMv3 LARGE , the learning rate and warm-up ratio are 5 \u2212 5 and 10%, respectively.\n\n\nFine-tuning on Multimodal Tasks\n\nWe compare LayoutLMv3 with typical self-supervised pre-training approaches and categorize them by their pre-training modalities.\n\n\u2022 [T] text modality: BERT [9] and RoBERTa [36] are typical pre-trained language models which only use text information with Transformer architecture. We use FUNSD and RVL-CDIP results of the RoBERTa from LayoutLM [54] and results of BERT from LayoutLMv2 [56]. We reproduce and report the CORD and DocVQA results of the RoBERTa. \u2022 [T+L] text and layout modalities: LayoutLM incorporates layout information by adding word-level spatial embeddings to embeddings of BERT [54]. StructuralLM leverages segmentlevel layout information [28]. BROS encodes relative layout Table 1: Comparison with existing published models on the CORD [39], FUNSD [20], RVL-CDIP [16], and DocVQA [38] datasets. \"T/L/I\" denotes \"text/layout/image\" modality. \"R/G/P\" denotes \"region/grid/patch\" image embedding. We multiply all values by a hundred for better readability. \u2020 In the UDoc paper [14], the CORD splits are 626/247 receipts for training/test instead of the official 800/100 training/test receipts adopted by other works. Thus the score \u2020 is not directly comparable to other scores. Models denoted with \u2021 use more data to train DocVQA and are expected to score higher. For example, TILT introduces one more supervised training stage on more QA datasets [40]. StructuralLM additionally uses the validation set in training [28]. positions [17]. LILT fine-tunes on different languages with pre-trained textual models [50]. FormNet leverages the spatial relationship between tokens in a form [25]. We fine-tune LayoutLMv3 on multimodal tasks on publicly available benchmarks. Results are shown in Table 1. Task I: Form and Receipt Understanding. Form and receipt understanding tasks require extracting and structuring forms and receipts' textual content. The tasks are a sequence labeling problem aiming to tag each word with a label. We predict the label of the last hidden state of each text token with a linear layer and an MLP classifier for form and receipt understanding tasks, respectively.\n\n\nModel\n\nWe conduct experiments on the FUNSD dataset and the CORD dataset. The FUNSD [20] is a noisy scanned form understanding dataset sampled from the RVL-CDIP dataset [16]. The FUNSD dataset contains 199 documents with comprehensive annotations for 9,707 semantic entities. We focus on the semantic entity labeling task on the FUNSD dataset to assign each semantic entity a label among \"question\", \"answer\", \"header\" or \"other\". The training and test splits contain 149 and 50 samples, respectively. CORD [39] is a receipt key information extraction dataset with 30 semantic labels defined under 4 categories. It contains 1,000 receipts of 800 training, 100 validation, and 100 test examples. We use officially-provided images and OCR annotations. We fine-tune LayoutLMv3 for 1,000 steps with a learning rate of 1 \u2212 5 and a batch size of 16 for FUNSD, and 5 \u2212 5 and 64 for CORD. We report F1 scores for this task. For the large model size, the LayoutLMv3 achieves an F1 score of 92.08 on the FUNSD dataset, which significantly outperforms the SOTA result of 85.14 provided by StructuralLM [28]. Note that LayoutLMv3 and StructuralLM use segment-level layout positions, while the other works use wordlevel layout positions. Using segment-level positions may benefit the semantic entity labeling task on FUNSD [28], so the two types of work are not directly comparable. The LayoutLMv3 also achieves SOTA F1 scores on the CORD dataset for both base and large model sizes. The results show that LayoutLMv3 can significantly benefit the text-centric form and receipt understanding tasks. Task II: Document Image Classification. The document image classification task aims to predict the category of document images. We feed the output hidden state of the special classification token ([CLS]) into an MLP classifier to predict the class labels.\n\nWe conduct experiments on the RVL-CDIP dataset. It is a subset of the IIT-CDIP collection labeled with 16 categories [16]. RVL-CDIP dataset contains 400,000 document images, among them 320,000 are training images, 40,000 are validation images, and 40,000 are test images. We extract text and layout information using Microsoft Read API. We fine-tune LayoutLMv3 for 20,000 steps with a batch size of 64 and a learning rate of 2 \u2212 5.\n\nThe evaluation metric is the overall classification accuracy. Lay-outLMv3 achieves better or comparable results with a much smaller model size than previous works. For example, compared to Lay-outLMv2, LayoutLMv3 achieves an absolute improvement of 0.19% and 0.29% in the base model and large model size, respectively, with a much simpler image embedding (i.e., Linear vs. ResNeXt101-FPN). The results show that our simple image embeddings can achieve desirable results on image-centric tasks. Task III: Document Visual Question Answering. Document visual question answering requires a model to take a document image and a question as input and output an answer [38]. We formalize this task as an extractive QA problem, where the model predicts start and end positions by classifying the last hidden state of each text token with a binary classifier.\n\nWe conduct experiments on the DocVQA dataset, a standard dataset for visual question answering on document images [38]. The official partition of the DocVQA dataset consists of 10,194/1,286/1,287 images and 39,463/5,349/5,188 questions for training/validation/test set, respectively. We train our model on the training set, evaluate the model on the test set, and report results by submitting them to the official evaluation website. We use Microsoft Read API to extract text and bounding boxes from images and use heuristics to find given answers in the extracted text as in LayoutLMv2. We fine-tune LayoutLMv3 BASE for 100,000 steps with a batch size of 128, a learning rate of 3 \u2212 5, and a warmup ratio of 0.048. For LayoutLMv3 LARGE , the step size, batch size, learning rate and warmup ratio are 200,000, 32, 1 \u2212 5, and 0.1, respectively.\n\nWe report the commonly-used edit distance-based metric ANLS (also known as Average Normalized Levenshtein Similarity). The LayoutLMv3 BASE improves the ANLS score of LayoutLMv2 BASE from 78.08 to 78.76, with much simpler image embedding (i.e., from ResNeXt101-FPN to Linear embedding). The LayoutLMv3 LARGE further gains an absolute ANLS score of 4.61 over LayoutLMv3 BASE . The results show that LayoutLMv3 is effective for the document visual question answering task.\n\n\nFine-tuning on a Vision Task\n\nTo demonstrate the generality of LayoutLMv3 from the multimodal domain to the visual domain, we transfer LayoutLMv3 to a document layout analysis task. This task is about detecting the layouts of unstructured digital documents by providing bounding boxes and categories such as tables, figures, texts, etc. This task helps parse the documents into a machine-readable format for downstream applications. We model this task as an object detection problem without text embedding, which is effective in existing works [14,30,59]. We integrate the LayoutLMv3 as feature backbone in the Cascade R-CNN detector [4] with FPN [34] implemented using the Detec-tron2 [53]. We adopt the standard practice to extract single-scale features from different Transformer layers, such as layers 4, 6, 8, and 12 of the LayoutLMv3 base model. We use resolution-modifying modules to convert the single-scale features into the multiscale FPN features [1,30,33].\n\nWe conduct experiments on PubLayNet dataset [59]. The dataset contains research paper images annotated with bounding boxes and polygonal segmentation across five document layout categories: text, title, list, figure, and table. The official splits contain 335,703 training images, 11,245 validation images, and 11,405 test images. We train our model on the training split and evaluate our model on the validation split following standard practice [14,30,59]. We train our model for 60,000 steps using the AdamW optimizer with 1,000 warm-up steps and a weight decay of 0.05 following DiT [30]. Since LayoutLMv3 is pre-trained with inputs from both vision and language modalities, we use a larger batch size of 32 and a lower learning rate of 2 \u2212 4 empirically. We do not use flipping or cropping augmentation strategy in the fine-tuning stage to be consistent Table 3: Ablation study on image embeddings and pre-training objectives on typical text-centric tasks (form and receipt understanding on FUNSD and CORD) and image-centric tasks (document image classification on RVL-CDIP and document layout analysis on PubLayNet). All models were trained at BASE size on 1 million data for 150,000 steps with learning rate 3 \u2212 4.  with our pre-training stage. We do not use relative positions in selfattention networks as DiT. We measure the performance using the mean average precision (MAP) @ intersection over union (IOU) [0.50:0.95] of bounding boxes and report results in Table 2. We compare with the ResNets [14,59] and the concurrent vision Transformer [30] backbones. LayoutLMv3 outperforms the other models in all metrics, achieving an overall mAP score of 95.1. LayoutLMv3 achieves a high gain in the \"Title\" category. Since titles are typically much smaller than other categories and can be identified by their textual content, we attribute this improvement to our incorporation of language modality in pre-training LayoutLMv3. These results demonstrate the generality and superiority of LayoutLMv3.\n\n\nAblation Study\n\nIn Table 3 we study the effect of our image embeddings and pretraining objectives. We first build a baseline model #1 that uses text and layout information, pre-trained with MLM objective. Then we use linearly projected image patches as the image embedding of the baseline model, denoted as model #2. We further pre-train model #2 with MIM and WPA objectives step by step and denote the new models as #3 and #4, respectively.\n\nIn Figure 4, we visualize losses of models #2, #3, and #4 when fine-tuned on the PubLayNet dataset with a batch size of 16 and a learning rate of 2 \u2212 4. We have tried to train the model #2 with learning rates of {1 \u2212 4, 2 \u2212 4, 4 \u2212 4} combined with batch sizes of {16, 32}, but the loss of model #2 did not converge and the mAP score on PubLayNet is near zero. Effect of Linear Image Embedding. We observe that model #1 without image embedding has achieved good results on some tasks. This suggests that language modality, including text and layout information, plays a vital role in document understanding. However, the results are still unsatisfactory. Moreover, model #1 cannot conduct some image-centric document analysis tasks without vision modality. For example, the vision modality is critical for the document layout analysis task on PubLayNet because bounding boxes are tightly integrated with images. Our simple design of linear image embedding combined with appropriate pre-training objectives can consistently improve not only image-centric tasks, but also some text-centric tasks further. Effect of MIM pre-training objective. Simply concatenating linear image embedding with text embedding as input to model #2 deteriorates performance on CORD and RVL-CDIP, while the loss on PubLayNet diverges. We speculate that the model failed to learn meaningful visual representation on the linear patch embeddings without any pre-training objective associated with image modality. The MIM objective mitigates this problem by preserving the image information until the last layer of the model by randomly masking out a portion of input image patches and reconstructing them in the output [22]. Comparing the results of model #3 and model #2, the MIM objective benefits CORD and RVL-CDIP. As simply using linear image embedding has improved FUNSD, MIM does not further contribute to FUNSD. By incorporating the MIM objective in training, the loss converges when fine-tuning PubLayNet as shown in Figure 4, and we obtain a desirable mAP score. The results indicate that MIM can help regularize the training. Thus MIM is critical for vision tasks like document layout analysis on PubLayNet. Effect of WPA pre-training objective. By comparing models #3 and #4 in Table 3, we observe that the WPA objective consistently improves all tasks. Moreover, the WPA objective decreases the loss of the vision task on PubLayNet in Figure 4. These results confirm the effectiveness of WPA not only in cross-modal representation learning, but also in image representation learning.\n\nParameter Comparisons. The table shows that incorporating image embedding for a 16\u00d716 patch projection (#1 \u2192 #2) introduces only 0.6M parameters. The parameters are negligible compared to the parameters of CNN backbones (e.g., 44M for ResNet-101). A MIM head and a WPA head introduce 6.9M and 0.6M parameters in the pre-training stage. The parameter overhead introduced by image embedding is marginal compared to the MLM head, which has 39.2M parameters for a text vocabulary size of 50,265. We did not take count of the image tokenizer when calculating parameters as the tokenizer is a standalone module for generating the labels of MIM but is not integrated into the Transformer backbone.\n\n\nRELATED WORK\n\nMultimodal self-supervised pre-training technique has made a rapid progress in document intelligence due to its successful applications of document layout and image representation learning [2, 13-15, 17, 25, 28, 31, 32, 40, 41, 50, 52, 54-56]. LayoutLM and following works joint layout representation learning by encoding spatial coordinates of text [17,25,28,54]. Various works then joint image representation learning by combining CNNs with Transformer [49] self-attention networks. These works either extract CNN grid features [2,56] or rely on an object detector to extract region features [14,31,40,54], which accounts for heavy computation bottleneck or requires region supervision. In the field of natural images vision-and-language pre-training (VLP), research works have seen a shift from region features [5,47,48] to grid features [19] to lift limitations of pre-defined object classes and region supervision. Inspired by vision Transformer (ViT) [11], there have also been recent efforts in VLP without CNNs to overcome the weakness of CNN. Still, most rely on separate self-attention networks to learn visual features; thus, their computational cost is not reduced [12,29,57]. An exception is ViLT, which learns visual features with a lightweight linear layer and significantly cuts down the model size and running time [22]. Inspired by ViLT, our LayoutLMv3 is the first multimodal model in Document AI that utilizes image embeddings without CNNs.\n\nReconstructive pre-training objectives revolutionized representation learning. In NLP research, BERT firstly proposed \"masked language modeling\" (MLM) to learn bidirectional representations and advanced the state of the arts on broad language understanding tasks [9]. In the field of CV, Masked Image Modeling (MIM) aims to learn rich visual representations via predicting masked content conditioning in visible context. For example, ViT reconstructs the mean color of masked patches, which leads to performance gains in ImageNet classification [11]. BEiT reconstructs visual tokens learned by a discrete VAE, achieving competitive results in image classification and semantic segmentation [3]. DiT extends BEiT to document images to document layout analysis [30].\n\nInspired by MLM and MIM, researchers in the field of visionand-language have explored reconstructive objectives for multimodal representation learning. Whereas most well-performing vision-and-language pre-training (VLP) models use the MLM proposed by BERT on text modality, they differ in their pre-training objectives for the image modality. There are three variants of MIM corresponding to different image embeddings: masked region modeling (MRM), masked grid modeling (MGM), and masked patch modeling (MPM). MRM has been proven to be effective in regressing original region features [5,31,48] or classifying object labels [5,37,48] for masked regions. MGM has also been explored in the SOHO, whose objective is to predict the mapping index in a visual dictionary for masked grid features [19]. For patch-level image embedding, Visual Parsing [57] proposed to mask visual tokens according to the attention weights in their self-attention image encoder, which does not apply to simple linear image encoders. ViLT [22] and METER [12] attempt to leverage MPM similar to ViT [11] and BEiT [3], which respectively reconstruct the mean color and discrete tokens in visual vocabularies for image patches, but resulted in degraded performance on downstream tasks. Our LayoutLMv3 firstly demonstrates the effectiveness of MIM for linear patch image embedding.\n\nVarious cross-modal objectives are further developed for vision and language (VL) alignment learning in multimodal models. Image-text matching is widely used to learn a coarse-grained VL alignment [2,5,19,22,56]. To learn a fine-grained VL alignment, UNITER proposes a word-region alignment objective based on optimal transports, which calculates the minimum cost of transporting the contextualized image embeddings to word embeddings [5]. ViLT extends this objective to patch-level image embeddings [22]. Unlike natural images, document images imply an explicit finegrained alignment relationship between text words and image areas. Using this relationship, UDoc uses contrastive learning and similarity distillation to align the image and text belonging to the same area [14]. LayoutLMv2 covers some text lines in raw images and predicts whether each text token is covered [56]. In contrast, we naturally utilize the masking operations in MIM to construct aligned/unaligned pairs in an effective and unified way.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we present LayoutLMv3 to pre-train the multimodal Transformer for Document AI, which redesigns the model architecture and pre-training objectives for LayoutLM. Distinguishing from the existing multimodal model in Document AI, LayoutLMv3 does not rely on a pre-trained CNN or Faster R-CNN backbone to extract visual features, significantly saving parameters and eliminating region annotations. We use unified text and image masking pre-training objectives: masked language modeling, masked image modeling, and word-patch alignment, to learn multimodal representations. Extensive experimental results have demonstrated the generality and superiority of LayoutLMv3 for both text-centric and image-centric Document AI tasks with the simple architecture and unified objectives. In future research, we will investigate scaling up pre-trained models so that the models can leverage more training data to drive SOTA results further. In addition, we will explore fewshot and zero-shot learning capabilities to facilitate more real-world business scenarios in the Document AI industry.\n\n\nACKNOWLEDGEMENT\n\nWe are grateful to Yiheng Xu for fruitful discussions and inspiration. This work was supported by the NSFC (U1811461) and the Program for Guangdong Introducing Innovative and Entrepreneurial Teams under Grant NO.2016ZT06D211. Pre-training LayoutLMv3 in Chinese. To demonstrate the effectiveness of LayoutLMv3 in not only English but also in the Chinese language, we pre-train a LayoutLMv3-Chinese model in base size. It is trained on 50 million document pages in Chinese. We collect large-scale Chinese documents by downloading publicly available digital-born documents and following the principles of Common Crawl (https://commoncrawl.org/) to process these documents. For the multimodal Transformer encoder along with the text embedding layer, LayoutLMv3-Chinese is initialized from the pre-trained weights of XLM-R [7]. We randomly initialized the rest model parameters. Other training setting is the same as LayoutLMv3.\n\nFine-tuning on Visual Information Extraction. The visual information extraction (VIE) requires extracting key information from document images. The task is a sequence labeling problem aiming to tag each word with a pre-defined label. We predict the label of the last hidden state of each text token with a linear layer.\n\nWe conduct experiments on the EPHOIE dataset. The EPHOIE [51] is a visual information extraction dataset consisting of examination paper heads with diverse layouts and backgrounds. It contains 1,494 images with comprehensive annotations for 15,771 Chinese text instances. We focus on a token-level entity labeling task on the EPHOIE dataset to assign each character a label among ten pre-defined categories. The training and test sets contain 1,183 and 311 images, respectively. We fine-tune LayoutLMv3-Chinese for 100 epochs. The batch size is 16, and the learning rate is 5 \u2212 5 with linear warmup over the first epoch.\n\nWe report F1 scores for this task and report results in Table 4. The LayoutLMv3-Chinese shows superior performance on most metrics and achieves a SOTA mean F1 score of 99.21%. The results show that LayoutLMv3 significantly benefits the VIE task in Chinese.\n\nFigure 3 :\n3The architecture and pre-training objectives of LayoutLMv3. LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking objectives. Given an input document image and its corresponding text and layout position information, the model takes the linear projection of patches and word tokens as inputs and encodes them into contextualized vector representations. LayoutLMv3 is pre-trained with discrete token reconstructive objectives of Masked Language Modeling (MLM) and Masked Image Modeling (MIM). Additionally, LayoutLMv3 is pre-trained with a Word-Patch Alignment (WPA) objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. \"Seg\" denotes segment-level positions. \"[CLS]\", \"[MASK]\", \"[SEP]\" and \"[SPE]\" are special tokens.\n\nFigure 4 :\n4Loss convergence curves of fine-tuning the ablated models of LayoutLMv3 on PubLayNet dataset. The loss of model #2 did not converge. By incorporating the MIM objective, the loss converges normally. The WPA objective further decreases the loss. Best viewed in color.\n\n\n\u2022 [T+L+I (R)] text, layout and image modalities with Faster R-CNN region features: This line of works extract im-age region features from RoI heads in the Faster R-CNN \nmodel [44]. Among them, LayoutLM [54] and TILT [40] use \nOCR words' bounding box to serve as region proposals and \nadd the region features to corresponding text embeddings. \nSelfDoc [31] and UDoc [14] use document object proposals \nand concatenate region features with text embeddings. \n\u2022 [T+L+I (G)] text, layout and image modalities with CNN \ngrid features: LayoutLMv2 [56] and DocFormer [2] extract \nimage grid features with a CNN backbone without object \ndetection. XYLayoutLM [15] adopts the architecture of Lay-\noutLMv2 and improves layout representation. \n\u2022 [T+L+I (P)] text, layout, and image modalities with lin-\near patch features: LayoutLMv3 replaces CNN backbones \nwith simple linear embedding to encode image patches. \n\n\n\nTable 2 :\n2Document layout analysis mAP @ IOU [0.50:0.95] on PubLayNet validation set. All models use only information from the vision modality. LayoutLMv3 outperforms the compared ResNets[14,59] and vision Transformer[30] backbones.Model \nFramework \nBackbone Text Title List Table Figure Overall \n\nPubLayNet[59] \nMask R-CNN \nResNet-101 \n91.6 \n84.0 88.6 \n96.0 \n94.9 \n91.0 \nDiT BASE [30] \nMask R-CNN \nTransformer 93.4 \n87.1 92.9 \n97.3 \n96.7 \n93.5 \nUDoc [14] \nFaster R-CNN \nResNet-50 \n93.9 \n88.5 93.7 \n97.3 \n96.4 \n93.9 \nDiT BASE [30] \nCascade R-CNN Transformer 94.4 \n88.9 94.8 \n97.6 \n96.9 \n94.5 \n\nLayoutLMv3 BASE (Ours) Cascade R-CNN Transformer 94.5 90.6 95.5 97.9 \n97.0 \n95.1 \n\n\n\nTable 4 :\n4Visual information extraction in Chinese F1 score on the EPHOIE test set. Subject Test Time Name School #Examination #Seat Class #Student Grade Score MeanModel \nBiLSTM+CRF [24] \n98.51 \n100.0 \n98.87 \n98.80 \n75.86 \n72.73 94.04 84.44 \n98.18 \n69.57 89.10 \nGCN-based [35] \n98.18 \n100.0 \n99.52 \n100.0 \n88.17 \n86.00 97.39 80.00 \n94.44 \n81.82 92.55 \nGraphIE [42] \n94.00 \n100.0 \n95.84 \n97.06 \n82.19 \n84.44 93.07 85.33 \n94.44 \n76.19 90.26 \nTRIE [58] \n98.79 \n100.0 \n99.46 \n99.64 \n88.64 \n85.92 97.94 84.32 \n97.02 \n80.39 93.21 \nVIES [51] \n99.39 \n100.0 \n99.67 \n99.28 \n91.81 \n88.73 99.29 89.47 \n98.35 \n86.27 95.23 \nStrucTexT [32] \n99.25 \n100.0 \n99.47 \n99.83 \n97.98 \n95.43 98.29 97.33 \n99.25 \n93.73 97.95 \n\nLayoutLMv3-Chinese BASE (Ours) 98.99 \n100.0 \n99.77 99.20 \n100.0 \n100.0 98.82 99.78 \n98.31 \n97.27 99.21 \n\nA APPENDIX \nA.1 LayoutLMv3 in Chinese \n\n\n\nXcit: Cross-covariance image transformers. Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, NeurIPS. Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. 2021. Xcit: Cross-covariance image transformers. In NeurIPS.\n\nDocFormer: End-to-End Transformer for Document Understanding. Srikar Appalaraju, Bhavan Jasani, Yusheng Bhargava Urala Kota, R Xie, Manmatha, ICCV. Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Man- matha. 2021. DocFormer: End-to-End Transformer for Document Understanding. In ICCV.\n\nBEiT: BERT Pre-Training of Image Transformers. Hangbo Bao, Li Dong, Songhao Piao, Furu Wei, ICLR. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. 2022. BEiT: BERT Pre- Training of Image Transformers. In ICLR.\n\nCascade r-cnn: Delving into high quality object detection. Zhaowei Cai, Nuno Vasconcelos, CVPR. Zhaowei Cai and Nuno Vasconcelos. 2018. Cascade r-cnn: Delving into high quality object detection. In CVPR.\n\nUniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, ECCV. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In ECCV.\n\nHannaneh Hajishirzi, and Aniruddha Kembhavi. 2020. X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers. Jaemin Cho, Jiasen Lu, Dustin Schwenk, EMNLP. Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, and Aniruddha Kembhavi. 2020. X-LXMERT: Paint, Caption and Answer Questions with Multi- Modal Transformers. In EMNLP.\n\nUnsupervised Cross-lingual Representation Learning at Scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, ACL. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil- laume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In ACL.\n\nLei Cui, Yiheng Xu, Tengchao Lv, Furu Wei, arXiv:2111.08609Document AI: Benchmarks, Models and Applications. arXiv preprintLei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. 2021. Document AI: Benchmarks, Models and Applications. arXiv preprint arXiv:2111.08609 (2021).\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL.\n\nCogview: Mastering text-to-image generation via transformers. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, NeurIPS. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. 2021. Cogview: Mastering text-to-image generation via transformers. In NeurIPS.\n\nJakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR.\n\nAn Empirical Study of Training End-to-End Vision-and-Language Transformers. Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Zicheng Liu, Michael Zeng, arXiv:2111.02387arXiv preprintZi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Zicheng Liu, Michael Zeng, et al. 2021. An Empirical Study of Training End-to-End Vision-and-Language Transformers. arXiv preprint arXiv:2111.02387 (2021).\n\nLAMBERT: Layout-Aware Language Modeling for Information Extraction. \u0141ukasz Garncarek, Rafa\u0142 Powalski, Tomasz Stanis\u0142awek, Bartosz Topolski, Piotr Halama, Micha\u0142 Turski, Filip Grali\u0144ski, ICDAR. \u0141ukasz Garncarek, Rafa\u0142 Powalski, Tomasz Stanis\u0142awek, Bartosz Topolski, Piotr Halama, Micha\u0142 Turski, and Filip Grali\u0144ski. 2021. LAMBERT: Layout-Aware Language Modeling for Information Extraction. In ICDAR.\n\nUniDoc: Unified Pretraining Framework for Document Understanding. Jiuxiang Gu, Jason Kuen, Vlad Morariu, Handong Zhao, Rajiv Jain, Nikolaos Barmpalios, Ani Nenkova, and Tong Sun. 2021. NeurIPSJiuxiang Gu, Jason Kuen, Vlad Morariu, Handong Zhao, Rajiv Jain, Nikolaos Barmpalios, Ani Nenkova, and Tong Sun. 2021. UniDoc: Unified Pretraining Framework for Document Understanding. In NeurIPS.\n\nXYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding. Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang, Ming Gu, Liqing Zhang, CVPR. Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang, Ming Gu, and Liqing Zhang. 2022. XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding. In CVPR.\n\nEvaluation of Deep Convolutional Nets for Document Image Classification and Retrieval. W Adam, Alex Harley, Konstantinos G Ufkes, Derpanis, ICDAR. Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis. 2015. Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval. In ICDAR.\n\nDaehyun Nam, and Sungrae Park. 2022. BROS: A Pre-Trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, AAAI. Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park. 2022. BROS: A Pre-Trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. In AAAI.\n\nUnifying multimodal transformer for bi-directional image and text generation. Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu, ACM Multimedia. Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. 2021. Unifying multimodal transformer for bi-directional image and text generation. In ACM Multimedia.\n\nSeeing out of the box: End-to-end pre-training for visionlanguage representation learning. Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu, CVPR. Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. 2021. Seeing out of the box: End-to-end pre-training for vision- language representation learning. In CVPR.\n\nFunsd: A dataset for form understanding in noisy scanned documents. Guillaume Jaume, Jean-Philippe Hazim Kemal Ekenel, Thiran, ICDARW. Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. 2019. Funsd: A dataset for form understanding in noisy scanned documents. In ICDARW.\n\nSpanbert: Improving pre-training by representing and predicting spans. Mandar Joshi, Danqi Chen, Yinhan Liu, S Daniel, Luke Weld, Omer Zettlemoyer, Levy, Transactions of the Association for Computational Linguistics. 8Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predict- ing spans. Transactions of the Association for Computational Linguistics 8 (2020), 64-77.\n\nVilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, ICML. Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In ICML.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\n\nNeural Architectures for Named Entity Recognition. Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, NAACL HLT. Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In NAACL HLT.\n\nChen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, and Tomas Pfister. 2022. FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. ACLChen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, and Tomas Pfister. 2022. FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. In ACL.\n\nBuilding a Test Collection for Complex Document Information Processing. D Lewis, G Agam, S Argamon, O Frieder, D Grossman, J Heard, SIGIR. D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. 2006. Building a Test Collection for Complex Document Information Processing. In SIGIR.\n\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, ACL. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In ACL.\n\nStructuralLM: Structural Pre-training for Form Understanding. Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, Luo Si, ACL. Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2021. StructuralLM: Structural Pre-training for Form Understanding. In ACL.\n\nAlign before fuse: Vision and language representation learning with momentum distillation. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu Hong Hoi, NeurIPS. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language repre- sentation learning with momentum distillation. In NeurIPS.\n\nDiT: Self-supervised Pre-training for Document Image Transformer. Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei, arXiv:2203.02378arXiv preprintJunlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. 2022. DiT: Self-supervised Pre-training for Document Image Transformer. arXiv preprint arXiv:2203.02378 (2022).\n\nSelfDoc: Self-Supervised Document Representation Learning. Peizhao Li, Jiuxiang Gu, Jason Kuen, I Vlad, Handong Morariu, Rajiv Zhao, Varun Jain, Hongfu Manjunatha, Liu, CVPR. Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu. 2021. SelfDoc: Self-Supervised Document Representation Learning. In CVPR.\n\nStrucTexT: Structured Text Understanding with Multi-Modal Transformers. Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, Errui Ding, ACM Multimedia. Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, and Errui Ding. 2021. StrucTexT: Structured Text Understanding with Multi-Modal Transformers. In ACM Multimedia.\n\nKaiming He, and Ross Girshick. 2021. Benchmarking detection transfer learning with vision transformers. Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, arXiv:2111.11429arXiv preprintYanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, and Ross Girshick. 2021. Benchmarking detection transfer learning with vision transformers. arXiv preprint arXiv:2111.11429 (2021).\n\nKaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. In CVPR.\n\nGraph Convolution for Multimodal Information Extraction from Visually Rich Documents. Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao, NAACL HLT. Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. 2019. Graph Convolu- tion for Multimodal Information Extraction from Visually Rich Documents. In NAACL HLT.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, In NeurIPSJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS.\n\nDocvqa: A dataset for vqa on document images. Minesh Mathew, Dimosthenis Karatzas, C V Jawahar, WACV. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: A dataset for vqa on document images. In WACV.\n\nCORD: A Consolidated Receipt Dataset for Post-OCR Parsing. Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, Hwalsuk Lee, Document Intelligence Workshop at Neural Information Processing Systems. Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. 2019. CORD: A Consolidated Receipt Dataset for Post- OCR Parsing. In Document Intelligence Workshop at Neural Information Processing Systems.\n\nTomasz Dwojak, Michal Pietruszka, and Gabriela Pa\u0142ka. 2021. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. Rafal Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, ICDAR. Rafal Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Michal Pietruszka, and Gabriela Pa\u0142ka. 2021. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. In ICDAR.\n\nTowards a multi-modal, multi-task learning based pre-training framework for document representation learning. Subhojeet Pramanik, Shashank Mujumdar, Hima Patel, arXiv:2009.14457arXiv preprintSubhojeet Pramanik, Shashank Mujumdar, and Hima Patel. 2020. Towards a multi-modal, multi-task learning based pre-training framework for document representation learning. arXiv preprint arXiv:2009.14457 (2020).\n\nA graph-based framework for information extraction. Yujie Qian, Massachusetts Institute of TechnologyPh. D. DissertationYujie Qian. 2019. A graph-based framework for information extraction. Ph. D. Dissertation. Massachusetts Institute of Technology.\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, ICML. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In ICML.\n\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Kaiming Shaoqing Ren, Ross B He, Jian Girshick, Sun, TPAMI. 39Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. TPAMI 39, 1137-1149.\n\nPixel-CNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications. Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P Kingma, ICLR. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. Pixel- CNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications. In ICLR.\n\nNeural Machine Translation of Rare Words with Subword Units. Rico Sennrich, Barry Haddow, Alexandra Birch, ACL. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In ACL.\n\nVL-BERT: Pre-training of Generic Visual-Linguistic Representations. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, ICLR. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2019. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In ICLR.\n\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers. Hao Tan, Mohit Bansal, EMNLP. Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In EMNLP.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, NeurIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.\n\nLiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding. Jiapeng Wang, Lianwen Jin, Kai Ding, ACL. Jiapeng Wang, Lianwen Jin, and Kai Ding. 2022. LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Under- standing. In ACL.\n\nTowards robust visual information extraction in real world: new dataset and novel solution. Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang, Shuaitao Zhang, Qianying Wang, Yaqiang Wu, Mingxiang Cai, AAAI. Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang, Shuaitao Zhang, Qianying Wang, Yaqiang Wu, and Mingxiang Cai. 2021. Towards robust visual information extraction in real world: new dataset and novel solution. In AAAI.\n\nTe-Lin Wu, Cheng Li, Mingyang Zhang, Tao Chen, arXiv:2104.08405Spurthi Amba Hombaiah, and Michael Bendersky. 2021. LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding. arXiv preprintTe-Lin Wu, Cheng Li, Mingyang Zhang, Tao Chen, Spurthi Amba Hombaiah, and Michael Bendersky. 2021. LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding. arXiv preprint arXiv:2104.08405 (2021).\n\n. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick, Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. 2019. Detectron2. https://github.com/facebookresearch/detectron2.\n\nLayoutlm: Pre-training of text and layout for document image understanding. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, KDD. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. Layoutlm: Pre-training of text and layout for document image understanding. In KDD.\n\nYiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei, arXiv:2104.08836LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. arXiv preprintYiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, and Furu Wei. 2021. LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. arXiv preprint arXiv:2104.08836 (2021).\n\nLayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou, ACL. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. 2021. LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understand- ing. In ACL.\n\nProbing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training. Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, Jiebo Luo, NeurIPS. Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo. 2021. Probing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training. In NeurIPS.\n\nTRIE: end-to-end text reading and information extraction for document understanding. Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, Fei Wu, ACM Multimedia. Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, and Fei Wu. 2020. TRIE: end-to-end text reading and information extraction for document understanding. In ACM Multimedia.\n\nPubLayNet: largest dataset ever for document layout analysis. Xu Zhong, Jianbin Tang, Antonio Jimeno Yepes, ICDAR. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019. PubLayNet: largest dataset ever for document layout analysis. In ICDAR.\n", "annotations": {"author": "[{\"end\":276,\"start\":144},{\"end\":434,\"start\":277},{\"end\":563,\"start\":435},{\"end\":720,\"start\":564},{\"end\":870,\"start\":721},{\"end\":1003,\"start\":871},{\"end\":1136,\"start\":1004},{\"end\":1265,\"start\":1137},{\"end\":1396,\"start\":1266},{\"end\":1526,\"start\":1397}]", "publisher": "[{\"end\":83,\"start\":80},{\"end\":1807,\"start\":1804}]", "author_last_name": "[{\"end\":155,\"start\":150},{\"end\":288,\"start\":286},{\"end\":442,\"start\":439},{\"end\":573,\"start\":571},{\"end\":729,\"start\":726},{\"end\":882,\"start\":877},{\"end\":1015,\"start\":1013},{\"end\":1144,\"start\":1141},{\"end\":1275,\"start\":1273},{\"end\":1405,\"start\":1402}]", "author_first_name": "[{\"end\":149,\"start\":144},{\"end\":285,\"start\":277},{\"end\":438,\"start\":435},{\"end\":570,\"start\":564},{\"end\":725,\"start\":721},{\"end\":876,\"start\":871},{\"end\":1012,\"start\":1004},{\"end\":1140,\"start\":1137},{\"end\":1272,\"start\":1266},{\"end\":1401,\"start\":1397}]", "author_affiliation": "[{\"end\":275,\"start\":157},{\"end\":433,\"start\":315},{\"end\":562,\"start\":444},{\"end\":719,\"start\":601},{\"end\":869,\"start\":751},{\"end\":1002,\"start\":884},{\"end\":1135,\"start\":1017},{\"end\":1264,\"start\":1146},{\"end\":1395,\"start\":1277},{\"end\":1525,\"start\":1407}]", "title": "[{\"end\":79,\"start\":1},{\"end\":1605,\"start\":1527}]", "venue": "[{\"end\":1682,\"start\":1607}]", "abstract": "[{\"end\":3450,\"start\":2229}]", "bib_ref": "[{\"end\":3677,\"start\":3624},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3915,\"start\":3912},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4298,\"start\":4295},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4579,\"start\":4576},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4679,\"start\":4675},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4682,\"start\":4679},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4739,\"start\":4735},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4840,\"start\":4837},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4843,\"start\":4840},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5026,\"start\":5023},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5043,\"start\":5039},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5231,\"start\":5228},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6058,\"start\":6054},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6071,\"start\":6068},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6387,\"start\":6383},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6401,\"start\":6397},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7137,\"start\":7133},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7171,\"start\":7167},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7210,\"start\":7206},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7293,\"start\":7289},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7343,\"start\":7339},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8927,\"start\":8923},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9523,\"start\":9519},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9998,\"start\":9994},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10260,\"start\":10256},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10357,\"start\":10354},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10360,\"start\":10357},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10413,\"start\":10409},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10445,\"start\":10441},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10448,\"start\":10445},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10451,\"start\":10448},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10454,\"start\":10451},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10576,\"start\":10572},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10590,\"start\":10586},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11609,\"start\":11605},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11971,\"start\":11968},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12025,\"start\":12021},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":12045,\"start\":12041},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12166,\"start\":12162},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12169,\"start\":12166},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12915,\"start\":12912},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13107,\"start\":13104},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13455,\"start\":13451},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14739,\"start\":14735},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":14759,\"start\":14755},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15250,\"start\":15246},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16007,\"start\":16003},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16422,\"start\":16418},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16538,\"start\":16534},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16541,\"start\":16538},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16689,\"start\":16685},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16836,\"start\":16832},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16879,\"start\":16877},{\"end\":16882,\"start\":16879},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16985,\"start\":16981},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17510,\"start\":17507},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17527,\"start\":17523},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":17698,\"start\":17694},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":17739,\"start\":17735},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":17952,\"start\":17948},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18013,\"start\":18009},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18111,\"start\":18107},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18123,\"start\":18119},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18138,\"start\":18134},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18155,\"start\":18151},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18349,\"start\":18345},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18720,\"start\":18716},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18788,\"start\":18784},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18804,\"start\":18800},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18881,\"start\":18877},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18955,\"start\":18951},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19546,\"start\":19542},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19631,\"start\":19627},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19969,\"start\":19965},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20553,\"start\":20549},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20772,\"start\":20768},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21421,\"start\":21417},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22399,\"start\":22395},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22703,\"start\":22699},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24450,\"start\":24446},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24453,\"start\":24450},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24456,\"start\":24453},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24539,\"start\":24536},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24553,\"start\":24549},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24592,\"start\":24588},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24863,\"start\":24860},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24866,\"start\":24863},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24869,\"start\":24866},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24920,\"start\":24916},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25323,\"start\":25319},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25326,\"start\":25323},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":25329,\"start\":25326},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25463,\"start\":25459},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26382,\"start\":26378},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":26385,\"start\":26382},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26428,\"start\":26424},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29015,\"start\":29011},{\"end\":30839,\"start\":30786},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30951,\"start\":30947},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30954,\"start\":30951},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30957,\"start\":30954},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30960,\"start\":30957},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31056,\"start\":31052},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31130,\"start\":31127},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":31133,\"start\":31130},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31195,\"start\":31191},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31198,\"start\":31195},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31201,\"start\":31198},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31204,\"start\":31201},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31414,\"start\":31411},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31417,\"start\":31414},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":31420,\"start\":31417},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31442,\"start\":31438},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31558,\"start\":31554},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31778,\"start\":31774},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31781,\"start\":31778},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31783,\"start\":31781},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31933,\"start\":31929},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32325,\"start\":32322},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32608,\"start\":32604},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32752,\"start\":32749},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32822,\"start\":32818},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33414,\"start\":33411},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":33417,\"start\":33414},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33420,\"start\":33417},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33453,\"start\":33450},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33456,\"start\":33453},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33459,\"start\":33456},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33620,\"start\":33616},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":33674,\"start\":33670},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33843,\"start\":33839},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33858,\"start\":33854},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33902,\"start\":33898},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33915,\"start\":33912},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34379,\"start\":34376},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34381,\"start\":34379},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34384,\"start\":34381},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34387,\"start\":34384},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":34390,\"start\":34387},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34617,\"start\":34614},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34683,\"start\":34679},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34956,\"start\":34952},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":35058,\"start\":35054},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37155,\"start\":37152},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":40680,\"start\":40676},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":40683,\"start\":40680},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":40710,\"start\":40706}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":39302,\"start\":38459},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39581,\"start\":39303},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40486,\"start\":39582},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41166,\"start\":40487},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42015,\"start\":41167}]", "paragraph": "[{\"end\":3916,\"start\":3466},{\"end\":5459,\"start\":3918},{\"end\":5595,\"start\":5461},{\"end\":7016,\"start\":5597},{\"end\":7616,\"start\":7018},{\"end\":7662,\"start\":7618},{\"end\":8624,\"start\":7664},{\"end\":9760,\"start\":8660},{\"end\":11610,\"start\":9789},{\"end\":12517,\"start\":11638},{\"end\":13263,\"start\":12557},{\"end\":13552,\"start\":13301},{\"end\":14476,\"start\":13554},{\"end\":14630,\"start\":14521},{\"end\":15561,\"start\":14671},{\"end\":16057,\"start\":15596},{\"end\":17315,\"start\":16135},{\"end\":17479,\"start\":17351},{\"end\":19456,\"start\":17481},{\"end\":21298,\"start\":19466},{\"end\":21731,\"start\":21300},{\"end\":22583,\"start\":21733},{\"end\":23428,\"start\":22585},{\"end\":23899,\"start\":23430},{\"end\":24870,\"start\":23932},{\"end\":26874,\"start\":24872},{\"end\":27318,\"start\":26893},{\"end\":29888,\"start\":27320},{\"end\":30580,\"start\":29890},{\"end\":32057,\"start\":30597},{\"end\":32823,\"start\":32059},{\"end\":34177,\"start\":32825},{\"end\":35193,\"start\":34179},{\"end\":36314,\"start\":35224},{\"end\":37257,\"start\":36334},{\"end\":37578,\"start\":37259},{\"end\":38200,\"start\":37580},{\"end\":38458,\"start\":38202}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9788,\"start\":9761},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12556,\"start\":12518},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13300,\"start\":13264},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14520,\"start\":14477},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15595,\"start\":15562},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16108,\"start\":16058}]", "table_ref": "[{\"end\":18051,\"start\":18044},{\"end\":19063,\"start\":19056},{\"end\":25738,\"start\":25731},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26348,\"start\":26341},{\"end\":26903,\"start\":26896},{\"end\":29589,\"start\":29582},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38265,\"start\":38258}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3464,\"start\":3452},{\"attributes\":{\"n\":\"2\"},\"end\":8637,\"start\":8627},{\"attributes\":{\"n\":\"2.1\"},\"end\":8658,\"start\":8640},{\"attributes\":{\"n\":\"2.2\"},\"end\":11636,\"start\":11613},{\"attributes\":{\"n\":\"3\"},\"end\":14669,\"start\":14633},{\"attributes\":{\"n\":\"3.2\"},\"end\":16133,\"start\":16110},{\"attributes\":{\"n\":\"3.3\"},\"end\":17349,\"start\":17318},{\"end\":19464,\"start\":19459},{\"attributes\":{\"n\":\"3.4\"},\"end\":23930,\"start\":23902},{\"attributes\":{\"n\":\"3.5\"},\"end\":26891,\"start\":26877},{\"attributes\":{\"n\":\"4\"},\"end\":30595,\"start\":30583},{\"attributes\":{\"n\":\"5\"},\"end\":35222,\"start\":35196},{\"attributes\":{\"n\":\"6\"},\"end\":36332,\"start\":36317},{\"end\":38470,\"start\":38460},{\"end\":39314,\"start\":39304},{\"end\":40497,\"start\":40488},{\"end\":41177,\"start\":41168}]", "table": "[{\"end\":40486,\"start\":39697},{\"end\":41166,\"start\":40721},{\"end\":42015,\"start\":41333}]", "figure_caption": "[{\"end\":39302,\"start\":38472},{\"end\":39581,\"start\":39316},{\"end\":39697,\"start\":39584},{\"end\":40721,\"start\":40499},{\"end\":41333,\"start\":41179}]", "figure_ref": "[{\"end\":3699,\"start\":3691},{\"end\":4494,\"start\":4486},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5887,\"start\":5879},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8587,\"start\":8579},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27331,\"start\":27323},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29326,\"start\":29318},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29748,\"start\":29740}]", "bib_author_first_name": "[{\"end\":42069,\"start\":42060},{\"end\":42079,\"start\":42075},{\"end\":42097,\"start\":42089},{\"end\":42110,\"start\":42105},{\"end\":42131,\"start\":42123},{\"end\":42145,\"start\":42139},{\"end\":42158,\"start\":42154},{\"end\":42174,\"start\":42167},{\"end\":42192,\"start\":42185},{\"end\":42208,\"start\":42203},{\"end\":42522,\"start\":42516},{\"end\":42541,\"start\":42535},{\"end\":42557,\"start\":42550},{\"end\":42580,\"start\":42579},{\"end\":42820,\"start\":42814},{\"end\":42828,\"start\":42826},{\"end\":42842,\"start\":42835},{\"end\":42853,\"start\":42849},{\"end\":43044,\"start\":43037},{\"end\":43054,\"start\":43050},{\"end\":43245,\"start\":43237},{\"end\":43258,\"start\":43252},{\"end\":43270,\"start\":43263},{\"end\":43280,\"start\":43275},{\"end\":43283,\"start\":43281},{\"end\":43297,\"start\":43291},{\"end\":43308,\"start\":43305},{\"end\":43316,\"start\":43314},{\"end\":43332,\"start\":43324},{\"end\":43653,\"start\":43647},{\"end\":43665,\"start\":43659},{\"end\":43676,\"start\":43670},{\"end\":43939,\"start\":43933},{\"end\":43957,\"start\":43949},{\"end\":43975,\"start\":43970},{\"end\":43990,\"start\":43983},{\"end\":44011,\"start\":44002},{\"end\":44029,\"start\":44020},{\"end\":44045,\"start\":44038},{\"end\":44057,\"start\":44053},{\"end\":44067,\"start\":44063},{\"end\":44088,\"start\":44081},{\"end\":44355,\"start\":44352},{\"end\":44367,\"start\":44361},{\"end\":44380,\"start\":44372},{\"end\":44389,\"start\":44385},{\"end\":44706,\"start\":44701},{\"end\":44723,\"start\":44715},{\"end\":44737,\"start\":44731},{\"end\":44751,\"start\":44743},{\"end\":45001,\"start\":44997},{\"end\":45014,\"start\":45008},{\"end\":45026,\"start\":45021},{\"end\":45038,\"start\":45033},{\"end\":45051,\"start\":45046},{\"end\":45060,\"start\":45058},{\"end\":45073,\"start\":45066},{\"end\":45081,\"start\":45079},{\"end\":45091,\"start\":45087},{\"end\":45105,\"start\":45098},{\"end\":45447,\"start\":45441},{\"end\":45466,\"start\":45461},{\"end\":45483,\"start\":45474},{\"end\":45500,\"start\":45496},{\"end\":45521,\"start\":45514},{\"end\":45534,\"start\":45528},{\"end\":45555,\"start\":45548},{\"end\":45574,\"start\":45566},{\"end\":45590,\"start\":45585},{\"end\":45607,\"start\":45600},{\"end\":46005,\"start\":46000},{\"end\":46018,\"start\":46011},{\"end\":46026,\"start\":46023},{\"end\":46040,\"start\":46032},{\"end\":46055,\"start\":46047},{\"end\":46068,\"start\":46062},{\"end\":46084,\"start\":46075},{\"end\":46097,\"start\":46090},{\"end\":46110,\"start\":46103},{\"end\":46468,\"start\":46462},{\"end\":46485,\"start\":46480},{\"end\":46502,\"start\":46496},{\"end\":46523,\"start\":46516},{\"end\":46539,\"start\":46534},{\"end\":46554,\"start\":46548},{\"end\":46568,\"start\":46563},{\"end\":46868,\"start\":46860},{\"end\":46878,\"start\":46873},{\"end\":46889,\"start\":46885},{\"end\":46906,\"start\":46899},{\"end\":46918,\"start\":46913},{\"end\":47288,\"start\":47279},{\"end\":47301,\"start\":47293},{\"end\":47310,\"start\":47308},{\"end\":47320,\"start\":47317},{\"end\":47334,\"start\":47326},{\"end\":47345,\"start\":47341},{\"end\":47356,\"start\":47350},{\"end\":47658,\"start\":47657},{\"end\":47669,\"start\":47665},{\"end\":47692,\"start\":47678},{\"end\":48038,\"start\":48031},{\"end\":48053,\"start\":48045},{\"end\":48064,\"start\":48059},{\"end\":48076,\"start\":48069},{\"end\":48390,\"start\":48385},{\"end\":48405,\"start\":48398},{\"end\":48414,\"start\":48411},{\"end\":48426,\"start\":48420},{\"end\":48700,\"start\":48692},{\"end\":48716,\"start\":48708},{\"end\":48728,\"start\":48723},{\"end\":48739,\"start\":48736},{\"end\":48752,\"start\":48745},{\"end\":48765,\"start\":48757},{\"end\":49044,\"start\":49035},{\"end\":49065,\"start\":49052},{\"end\":49328,\"start\":49322},{\"end\":49341,\"start\":49336},{\"end\":49354,\"start\":49348},{\"end\":49361,\"start\":49360},{\"end\":49374,\"start\":49370},{\"end\":49385,\"start\":49381},{\"end\":49801,\"start\":49795},{\"end\":49814,\"start\":49807},{\"end\":49825,\"start\":49820},{\"end\":50019,\"start\":50018},{\"end\":50035,\"start\":50030},{\"end\":50261,\"start\":50252},{\"end\":50276,\"start\":50270},{\"end\":50297,\"start\":50290},{\"end\":50317,\"start\":50311},{\"end\":50333,\"start\":50328},{\"end\":50522,\"start\":50515},{\"end\":50538,\"start\":50528},{\"end\":50550,\"start\":50543},{\"end\":50565,\"start\":50558},{\"end\":50580,\"start\":50573},{\"end\":50588,\"start\":50585},{\"end\":50600,\"start\":50594},{\"end\":50617,\"start\":50610},{\"end\":51094,\"start\":51093},{\"end\":51103,\"start\":51102},{\"end\":51111,\"start\":51110},{\"end\":51122,\"start\":51121},{\"end\":51133,\"start\":51132},{\"end\":51145,\"start\":51144},{\"end\":51438,\"start\":51434},{\"end\":51452,\"start\":51446},{\"end\":51463,\"start\":51458},{\"end\":51477,\"start\":51471},{\"end\":51504,\"start\":51493},{\"end\":51518,\"start\":51514},{\"end\":51532,\"start\":51525},{\"end\":51547,\"start\":51543},{\"end\":51898,\"start\":51889},{\"end\":51906,\"start\":51903},{\"end\":51915,\"start\":51911},{\"end\":51924,\"start\":51921},{\"end\":51939,\"start\":51931},{\"end\":51950,\"start\":51947},{\"end\":51961,\"start\":51958},{\"end\":52226,\"start\":52220},{\"end\":52242,\"start\":52231},{\"end\":52262,\"start\":52254},{\"end\":52278,\"start\":52272},{\"end\":52292,\"start\":52285},{\"end\":52315,\"start\":52300},{\"end\":52620,\"start\":52613},{\"end\":52631,\"start\":52625},{\"end\":52644,\"start\":52636},{\"end\":52652,\"start\":52649},{\"end\":52661,\"start\":52658},{\"end\":52673,\"start\":52669},{\"end\":52958,\"start\":52951},{\"end\":52971,\"start\":52963},{\"end\":52981,\"start\":52976},{\"end\":52989,\"start\":52988},{\"end\":53003,\"start\":52996},{\"end\":53018,\"start\":53013},{\"end\":53030,\"start\":53025},{\"end\":53043,\"start\":53037},{\"end\":53332,\"start\":53327},{\"end\":53341,\"start\":53337},{\"end\":53355,\"start\":53348},{\"end\":53367,\"start\":53360},{\"end\":53382,\"start\":53373},{\"end\":53393,\"start\":53390},{\"end\":53402,\"start\":53399},{\"end\":53413,\"start\":53408},{\"end\":53426,\"start\":53419},{\"end\":53437,\"start\":53432},{\"end\":53790,\"start\":53783},{\"end\":53802,\"start\":53795},{\"end\":53814,\"start\":53808},{\"end\":53826,\"start\":53821},{\"end\":54174,\"start\":54166},{\"end\":54185,\"start\":54180},{\"end\":54198,\"start\":54194},{\"end\":54466,\"start\":54458},{\"end\":54477,\"start\":54472},{\"end\":54488,\"start\":54483},{\"end\":54502,\"start\":54496},{\"end\":54690,\"start\":54684},{\"end\":54700,\"start\":54696},{\"end\":54711,\"start\":54706},{\"end\":54726,\"start\":54719},{\"end\":54737,\"start\":54731},{\"end\":54750,\"start\":54745},{\"end\":54761,\"start\":54757},{\"end\":54772,\"start\":54768},{\"end\":54784,\"start\":54780},{\"end\":54805,\"start\":54798},{\"end\":55247,\"start\":55241},{\"end\":55257,\"start\":55252},{\"end\":55269,\"start\":55265},{\"end\":55284,\"start\":55278},{\"end\":55522,\"start\":55516},{\"end\":55542,\"start\":55531},{\"end\":55554,\"start\":55553},{\"end\":55556,\"start\":55555},{\"end\":55755,\"start\":55746},{\"end\":55767,\"start\":55762},{\"end\":55778,\"start\":55774},{\"end\":55791,\"start\":55784},{\"end\":55805,\"start\":55797},{\"end\":55819,\"start\":55812},{\"end\":55832,\"start\":55825},{\"end\":56300,\"start\":56295},{\"end\":56317,\"start\":56311},{\"end\":56334,\"start\":56329},{\"end\":56681,\"start\":56672},{\"end\":56700,\"start\":56692},{\"end\":56715,\"start\":56711},{\"end\":57022,\"start\":57017},{\"end\":57258,\"start\":57252},{\"end\":57274,\"start\":57267},{\"end\":57290,\"start\":57283},{\"end\":57301,\"start\":57296},{\"end\":57315,\"start\":57308},{\"end\":57326,\"start\":57322},{\"end\":57340,\"start\":57336},{\"end\":57351,\"start\":57347},{\"end\":57623,\"start\":57616},{\"end\":57642,\"start\":57638},{\"end\":57644,\"start\":57643},{\"end\":57653,\"start\":57649},{\"end\":57953,\"start\":57950},{\"end\":57970,\"start\":57964},{\"end\":57983,\"start\":57981},{\"end\":57998,\"start\":57990},{\"end\":58000,\"start\":57999},{\"end\":58267,\"start\":58263},{\"end\":58283,\"start\":58278},{\"end\":58301,\"start\":58292},{\"end\":58514,\"start\":58508},{\"end\":58525,\"start\":58519},{\"end\":58534,\"start\":58531},{\"end\":58543,\"start\":58540},{\"end\":58553,\"start\":58548},{\"end\":58562,\"start\":58558},{\"end\":58574,\"start\":58568},{\"end\":58824,\"start\":58821},{\"end\":58835,\"start\":58830},{\"end\":59002,\"start\":58996},{\"end\":59016,\"start\":59012},{\"end\":59030,\"start\":59026},{\"end\":59044,\"start\":59039},{\"end\":59061,\"start\":59056},{\"end\":59074,\"start\":59069},{\"end\":59076,\"start\":59075},{\"end\":59090,\"start\":59084},{\"end\":59104,\"start\":59099},{\"end\":59412,\"start\":59405},{\"end\":59426,\"start\":59419},{\"end\":59435,\"start\":59432},{\"end\":59712,\"start\":59705},{\"end\":59726,\"start\":59719},{\"end\":59739,\"start\":59732},{\"end\":59751,\"start\":59745},{\"end\":59764,\"start\":59758},{\"end\":59780,\"start\":59772},{\"end\":59796,\"start\":59788},{\"end\":59810,\"start\":59803},{\"end\":59824,\"start\":59815},{\"end\":60079,\"start\":60073},{\"end\":60089,\"start\":60084},{\"end\":60102,\"start\":60094},{\"end\":60113,\"start\":60110},{\"end\":60495,\"start\":60490},{\"end\":60509,\"start\":60500},{\"end\":60529,\"start\":60520},{\"end\":60544,\"start\":60537},{\"end\":60553,\"start\":60549},{\"end\":60791,\"start\":60785},{\"end\":60803,\"start\":60796},{\"end\":60811,\"start\":60808},{\"end\":60824,\"start\":60817},{\"end\":60836,\"start\":60832},{\"end\":60846,\"start\":60842},{\"end\":61027,\"start\":61021},{\"end\":61040,\"start\":61032},{\"end\":61048,\"start\":61045},{\"end\":61060,\"start\":61054},{\"end\":61073,\"start\":61067},{\"end\":61083,\"start\":61078},{\"end\":61098,\"start\":61095},{\"end\":61110,\"start\":61106},{\"end\":61555,\"start\":61551},{\"end\":61566,\"start\":61560},{\"end\":61579,\"start\":61571},{\"end\":61587,\"start\":61584},{\"end\":61597,\"start\":61593},{\"end\":61609,\"start\":61603},{\"end\":61622,\"start\":61616},{\"end\":61632,\"start\":61627},{\"end\":61647,\"start\":61644},{\"end\":61663,\"start\":61655},{\"end\":61672,\"start\":61669},{\"end\":61686,\"start\":61680},{\"end\":62044,\"start\":62037},{\"end\":62055,\"start\":62050},{\"end\":62066,\"start\":62063},{\"end\":62078,\"start\":62072},{\"end\":62093,\"start\":62085},{\"end\":62106,\"start\":62098},{\"end\":62116,\"start\":62111},{\"end\":62425,\"start\":62421},{\"end\":62438,\"start\":62433},{\"end\":62451,\"start\":62443},{\"end\":62467,\"start\":62459},{\"end\":62476,\"start\":62472},{\"end\":62486,\"start\":62481},{\"end\":62495,\"start\":62493},{\"end\":62504,\"start\":62501},{\"end\":62792,\"start\":62790},{\"end\":62807,\"start\":62800},{\"end\":62828,\"start\":62814}]", "bib_author_last_name": "[{\"end\":42073,\"start\":42070},{\"end\":42087,\"start\":42080},{\"end\":42103,\"start\":42098},{\"end\":42121,\"start\":42111},{\"end\":42137,\"start\":42132},{\"end\":42152,\"start\":42146},{\"end\":42165,\"start\":42159},{\"end\":42183,\"start\":42175},{\"end\":42201,\"start\":42193},{\"end\":42216,\"start\":42209},{\"end\":42533,\"start\":42523},{\"end\":42548,\"start\":42542},{\"end\":42577,\"start\":42558},{\"end\":42584,\"start\":42581},{\"end\":42594,\"start\":42586},{\"end\":42824,\"start\":42821},{\"end\":42833,\"start\":42829},{\"end\":42847,\"start\":42843},{\"end\":42857,\"start\":42854},{\"end\":43048,\"start\":43045},{\"end\":43066,\"start\":43055},{\"end\":43250,\"start\":43246},{\"end\":43261,\"start\":43259},{\"end\":43273,\"start\":43271},{\"end\":43289,\"start\":43284},{\"end\":43303,\"start\":43298},{\"end\":43312,\"start\":43309},{\"end\":43322,\"start\":43317},{\"end\":43336,\"start\":43333},{\"end\":43657,\"start\":43654},{\"end\":43668,\"start\":43666},{\"end\":43684,\"start\":43677},{\"end\":43947,\"start\":43940},{\"end\":43968,\"start\":43958},{\"end\":43981,\"start\":43976},{\"end\":44000,\"start\":43991},{\"end\":44018,\"start\":44012},{\"end\":44036,\"start\":44030},{\"end\":44051,\"start\":44046},{\"end\":44061,\"start\":44058},{\"end\":44079,\"start\":44068},{\"end\":44097,\"start\":44089},{\"end\":44359,\"start\":44356},{\"end\":44370,\"start\":44368},{\"end\":44383,\"start\":44381},{\"end\":44393,\"start\":44390},{\"end\":44713,\"start\":44707},{\"end\":44729,\"start\":44724},{\"end\":44741,\"start\":44738},{\"end\":44761,\"start\":44752},{\"end\":45006,\"start\":45002},{\"end\":45019,\"start\":45015},{\"end\":45031,\"start\":45027},{\"end\":45044,\"start\":45039},{\"end\":45056,\"start\":45052},{\"end\":45064,\"start\":45061},{\"end\":45077,\"start\":45074},{\"end\":45085,\"start\":45082},{\"end\":45096,\"start\":45092},{\"end\":45110,\"start\":45106},{\"end\":45459,\"start\":45448},{\"end\":45472,\"start\":45467},{\"end\":45494,\"start\":45484},{\"end\":45512,\"start\":45501},{\"end\":45526,\"start\":45522},{\"end\":45546,\"start\":45535},{\"end\":45564,\"start\":45556},{\"end\":45583,\"start\":45575},{\"end\":45598,\"start\":45591},{\"end\":45613,\"start\":45608},{\"end\":46009,\"start\":46006},{\"end\":46021,\"start\":46019},{\"end\":46030,\"start\":46027},{\"end\":46045,\"start\":46041},{\"end\":46060,\"start\":46056},{\"end\":46073,\"start\":46069},{\"end\":46088,\"start\":46085},{\"end\":46101,\"start\":46098},{\"end\":46115,\"start\":46111},{\"end\":46478,\"start\":46469},{\"end\":46494,\"start\":46486},{\"end\":46514,\"start\":46503},{\"end\":46532,\"start\":46524},{\"end\":46546,\"start\":46540},{\"end\":46561,\"start\":46555},{\"end\":46578,\"start\":46569},{\"end\":46871,\"start\":46869},{\"end\":46883,\"start\":46879},{\"end\":46897,\"start\":46890},{\"end\":46911,\"start\":46907},{\"end\":46923,\"start\":46919},{\"end\":47291,\"start\":47289},{\"end\":47306,\"start\":47302},{\"end\":47315,\"start\":47311},{\"end\":47324,\"start\":47321},{\"end\":47339,\"start\":47335},{\"end\":47348,\"start\":47346},{\"end\":47362,\"start\":47357},{\"end\":47663,\"start\":47659},{\"end\":47676,\"start\":47670},{\"end\":47698,\"start\":47693},{\"end\":47708,\"start\":47700},{\"end\":48043,\"start\":48039},{\"end\":48057,\"start\":48054},{\"end\":48067,\"start\":48065},{\"end\":48082,\"start\":48077},{\"end\":48396,\"start\":48391},{\"end\":48409,\"start\":48406},{\"end\":48418,\"start\":48415},{\"end\":48429,\"start\":48427},{\"end\":48706,\"start\":48701},{\"end\":48721,\"start\":48717},{\"end\":48734,\"start\":48729},{\"end\":48743,\"start\":48740},{\"end\":48755,\"start\":48753},{\"end\":48768,\"start\":48766},{\"end\":49050,\"start\":49045},{\"end\":49084,\"start\":49066},{\"end\":49092,\"start\":49086},{\"end\":49334,\"start\":49329},{\"end\":49346,\"start\":49342},{\"end\":49358,\"start\":49355},{\"end\":49368,\"start\":49362},{\"end\":49379,\"start\":49375},{\"end\":49397,\"start\":49386},{\"end\":49403,\"start\":49399},{\"end\":49805,\"start\":49802},{\"end\":49818,\"start\":49815},{\"end\":49829,\"start\":49826},{\"end\":50028,\"start\":50020},{\"end\":50042,\"start\":50036},{\"end\":50046,\"start\":50044},{\"end\":50268,\"start\":50262},{\"end\":50288,\"start\":50277},{\"end\":50309,\"start\":50298},{\"end\":50326,\"start\":50318},{\"end\":50338,\"start\":50334},{\"end\":50526,\"start\":50523},{\"end\":50541,\"start\":50539},{\"end\":50556,\"start\":50551},{\"end\":50571,\"start\":50566},{\"end\":50583,\"start\":50581},{\"end\":50592,\"start\":50589},{\"end\":50608,\"start\":50601},{\"end\":50622,\"start\":50618},{\"end\":51100,\"start\":51095},{\"end\":51108,\"start\":51104},{\"end\":51119,\"start\":51112},{\"end\":51130,\"start\":51123},{\"end\":51142,\"start\":51134},{\"end\":51151,\"start\":51146},{\"end\":51444,\"start\":51439},{\"end\":51456,\"start\":51453},{\"end\":51469,\"start\":51464},{\"end\":51491,\"start\":51478},{\"end\":51512,\"start\":51505},{\"end\":51523,\"start\":51519},{\"end\":51541,\"start\":51533},{\"end\":51559,\"start\":51548},{\"end\":51901,\"start\":51899},{\"end\":51909,\"start\":51907},{\"end\":51919,\"start\":51916},{\"end\":51929,\"start\":51925},{\"end\":51945,\"start\":51940},{\"end\":51956,\"start\":51951},{\"end\":51964,\"start\":51962},{\"end\":52229,\"start\":52227},{\"end\":52252,\"start\":52243},{\"end\":52270,\"start\":52263},{\"end\":52283,\"start\":52279},{\"end\":52298,\"start\":52293},{\"end\":52319,\"start\":52316},{\"end\":52623,\"start\":52621},{\"end\":52634,\"start\":52632},{\"end\":52647,\"start\":52645},{\"end\":52656,\"start\":52653},{\"end\":52667,\"start\":52662},{\"end\":52677,\"start\":52674},{\"end\":52961,\"start\":52959},{\"end\":52974,\"start\":52972},{\"end\":52986,\"start\":52982},{\"end\":52994,\"start\":52990},{\"end\":53011,\"start\":53004},{\"end\":53023,\"start\":53019},{\"end\":53035,\"start\":53031},{\"end\":53054,\"start\":53044},{\"end\":53059,\"start\":53056},{\"end\":53335,\"start\":53333},{\"end\":53346,\"start\":53342},{\"end\":53358,\"start\":53356},{\"end\":53371,\"start\":53368},{\"end\":53388,\"start\":53383},{\"end\":53397,\"start\":53394},{\"end\":53406,\"start\":53403},{\"end\":53417,\"start\":53414},{\"end\":53430,\"start\":53427},{\"end\":53442,\"start\":53438},{\"end\":53793,\"start\":53791},{\"end\":53806,\"start\":53803},{\"end\":53819,\"start\":53815},{\"end\":53833,\"start\":53827},{\"end\":54178,\"start\":54175},{\"end\":54192,\"start\":54186},{\"end\":54207,\"start\":54199},{\"end\":54470,\"start\":54467},{\"end\":54481,\"start\":54478},{\"end\":54494,\"start\":54489},{\"end\":54507,\"start\":54503},{\"end\":54694,\"start\":54691},{\"end\":54704,\"start\":54701},{\"end\":54717,\"start\":54712},{\"end\":54729,\"start\":54727},{\"end\":54743,\"start\":54738},{\"end\":54755,\"start\":54751},{\"end\":54766,\"start\":54762},{\"end\":54778,\"start\":54773},{\"end\":54796,\"start\":54785},{\"end\":54814,\"start\":54806},{\"end\":55250,\"start\":55248},{\"end\":55263,\"start\":55258},{\"end\":55276,\"start\":55270},{\"end\":55288,\"start\":55285},{\"end\":55529,\"start\":55523},{\"end\":55551,\"start\":55543},{\"end\":55564,\"start\":55557},{\"end\":55760,\"start\":55756},{\"end\":55772,\"start\":55768},{\"end\":55782,\"start\":55779},{\"end\":55795,\"start\":55792},{\"end\":55810,\"start\":55806},{\"end\":55823,\"start\":55820},{\"end\":55836,\"start\":55833},{\"end\":56309,\"start\":56301},{\"end\":56327,\"start\":56318},{\"end\":56345,\"start\":56335},{\"end\":56690,\"start\":56682},{\"end\":56709,\"start\":56701},{\"end\":56721,\"start\":56716},{\"end\":57027,\"start\":57023},{\"end\":57265,\"start\":57259},{\"end\":57281,\"start\":57275},{\"end\":57294,\"start\":57291},{\"end\":57306,\"start\":57302},{\"end\":57320,\"start\":57316},{\"end\":57334,\"start\":57327},{\"end\":57345,\"start\":57341},{\"end\":57361,\"start\":57352},{\"end\":57636,\"start\":57624},{\"end\":57647,\"start\":57645},{\"end\":57662,\"start\":57654},{\"end\":57667,\"start\":57664},{\"end\":57962,\"start\":57954},{\"end\":57979,\"start\":57971},{\"end\":57988,\"start\":57984},{\"end\":58007,\"start\":58001},{\"end\":58276,\"start\":58268},{\"end\":58290,\"start\":58284},{\"end\":58307,\"start\":58302},{\"end\":58517,\"start\":58515},{\"end\":58529,\"start\":58526},{\"end\":58538,\"start\":58535},{\"end\":58546,\"start\":58544},{\"end\":58556,\"start\":58554},{\"end\":58566,\"start\":58563},{\"end\":58578,\"start\":58575},{\"end\":58828,\"start\":58825},{\"end\":58842,\"start\":58836},{\"end\":59010,\"start\":59003},{\"end\":59024,\"start\":59017},{\"end\":59037,\"start\":59031},{\"end\":59054,\"start\":59045},{\"end\":59067,\"start\":59062},{\"end\":59082,\"start\":59077},{\"end\":59097,\"start\":59091},{\"end\":59115,\"start\":59105},{\"end\":59417,\"start\":59413},{\"end\":59430,\"start\":59427},{\"end\":59440,\"start\":59436},{\"end\":59717,\"start\":59713},{\"end\":59730,\"start\":59727},{\"end\":59743,\"start\":59740},{\"end\":59756,\"start\":59752},{\"end\":59770,\"start\":59765},{\"end\":59786,\"start\":59781},{\"end\":59801,\"start\":59797},{\"end\":59813,\"start\":59811},{\"end\":59828,\"start\":59825},{\"end\":60082,\"start\":60080},{\"end\":60092,\"start\":60090},{\"end\":60108,\"start\":60103},{\"end\":60118,\"start\":60114},{\"end\":60498,\"start\":60496},{\"end\":60518,\"start\":60510},{\"end\":60535,\"start\":60530},{\"end\":60547,\"start\":60545},{\"end\":60562,\"start\":60554},{\"end\":60794,\"start\":60792},{\"end\":60806,\"start\":60804},{\"end\":60815,\"start\":60812},{\"end\":60830,\"start\":60825},{\"end\":60840,\"start\":60837},{\"end\":60851,\"start\":60847},{\"end\":61030,\"start\":61028},{\"end\":61043,\"start\":61041},{\"end\":61052,\"start\":61049},{\"end\":61065,\"start\":61061},{\"end\":61076,\"start\":61074},{\"end\":61093,\"start\":61084},{\"end\":61104,\"start\":61099},{\"end\":61114,\"start\":61111},{\"end\":61558,\"start\":61556},{\"end\":61569,\"start\":61567},{\"end\":61582,\"start\":61580},{\"end\":61591,\"start\":61588},{\"end\":61601,\"start\":61598},{\"end\":61614,\"start\":61610},{\"end\":61625,\"start\":61623},{\"end\":61642,\"start\":61633},{\"end\":61653,\"start\":61648},{\"end\":61667,\"start\":61664},{\"end\":61678,\"start\":61673},{\"end\":61691,\"start\":61687},{\"end\":62048,\"start\":62045},{\"end\":62061,\"start\":62056},{\"end\":62070,\"start\":62067},{\"end\":62083,\"start\":62079},{\"end\":62096,\"start\":62094},{\"end\":62109,\"start\":62107},{\"end\":62120,\"start\":62117},{\"end\":62431,\"start\":62426},{\"end\":62441,\"start\":62439},{\"end\":62457,\"start\":62452},{\"end\":62470,\"start\":62468},{\"end\":62479,\"start\":62477},{\"end\":62491,\"start\":62487},{\"end\":62499,\"start\":62496},{\"end\":62507,\"start\":62505},{\"end\":62798,\"start\":62793},{\"end\":62812,\"start\":62808},{\"end\":62834,\"start\":62829}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":235458262},\"end\":42452,\"start\":42017},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":235592814},\"end\":42765,\"start\":42454},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":235436185},\"end\":42976,\"start\":42767},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206596979},\"end\":43181,\"start\":42978},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":216080982},\"end\":43517,\"start\":43183},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":219964325},\"end\":43870,\"start\":43519},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207880568},\"end\":44350,\"start\":43872},{\"attributes\":{\"doi\":\"arXiv:2111.08609\",\"id\":\"b7\"},\"end\":44617,\"start\":44352},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52967399},\"end\":44933,\"start\":44619},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235212350},\"end\":45322,\"start\":44935},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225039882},\"end\":45922,\"start\":45324},{\"attributes\":{\"doi\":\"arXiv:2111.02387\",\"id\":\"b11\"},\"end\":46392,\"start\":45924},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":235262539},\"end\":46792,\"start\":46394},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":244950822},\"end\":47182,\"start\":46794},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":247446585},\"end\":47568,\"start\":47184},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2760893},\"end\":47875,\"start\":47570},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":237485613},\"end\":48305,\"start\":47877},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":239011704},\"end\":48599,\"start\":48307},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":233169113},\"end\":48965,\"start\":48601},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":173188931},\"end\":49249,\"start\":48967},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":198229624},\"end\":49712,\"start\":49251},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":231839613},\"end\":49972,\"start\":49714},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b22\"},\"end\":50199,\"start\":49974},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6042994},\"end\":50513,\"start\":50201},{\"attributes\":{\"id\":\"b24\"},\"end\":51019,\"start\":50515},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":19516087},\"end\":51317,\"start\":51021},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204960716},\"end\":51825,\"start\":51319},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":235166279},\"end\":52127,\"start\":51827},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":236034189},\"end\":52545,\"start\":52129},{\"attributes\":{\"doi\":\"arXiv:2203.02378\",\"id\":\"b29\"},\"end\":52890,\"start\":52547},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":235358528},\"end\":53253,\"start\":52892},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":236950714},\"end\":53677,\"start\":53255},{\"attributes\":{\"doi\":\"arXiv:2111.11429\",\"id\":\"b32\"},\"end\":54060,\"start\":53679},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10716717},\"end\":54370,\"start\":54062},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":85528598},\"end\":54682,\"start\":54372},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b35\"},\"end\":55141,\"start\":54684},{\"attributes\":{\"id\":\"b36\"},\"end\":55468,\"start\":55143},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220280200},\"end\":55685,\"start\":55470},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":207900784},\"end\":56148,\"start\":55687},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":231951453},\"end\":56560,\"start\":56150},{\"attributes\":{\"doi\":\"arXiv:2009.14457\",\"id\":\"b40\"},\"end\":56963,\"start\":56562},{\"attributes\":{\"id\":\"b41\"},\"end\":57214,\"start\":56965},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":232035663},\"end\":57534,\"start\":57216},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":10328909},\"end\":57842,\"start\":57536},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12663716},\"end\":58200,\"start\":57844},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1114678},\"end\":58438,\"start\":58202},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":201317624},\"end\":58744,\"start\":58440},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":201103729},\"end\":58967,\"start\":58746},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":13756489},\"end\":59295,\"start\":58969},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":247158521},\"end\":59611,\"start\":59297},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":231924690},\"end\":60071,\"start\":59613},{\"attributes\":{\"doi\":\"arXiv:2104.08405\",\"id\":\"b51\"},\"end\":60486,\"start\":60073},{\"attributes\":{\"id\":\"b52\"},\"end\":60707,\"start\":60488},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":209515395},\"end\":61019,\"start\":60709},{\"attributes\":{\"doi\":\"arXiv:2104.08836\",\"id\":\"b54\"},\"end\":61470,\"start\":61021},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":229923949},\"end\":61938,\"start\":61472},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":235652051},\"end\":62334,\"start\":61940},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":218900797},\"end\":62726,\"start\":62336},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":201124789},\"end\":62970,\"start\":62728}]", "bib_title": "[{\"end\":42058,\"start\":42017},{\"end\":42514,\"start\":42454},{\"end\":42812,\"start\":42767},{\"end\":43035,\"start\":42978},{\"end\":43235,\"start\":43183},{\"end\":43645,\"start\":43519},{\"end\":43931,\"start\":43872},{\"end\":44699,\"start\":44619},{\"end\":44995,\"start\":44935},{\"end\":45439,\"start\":45324},{\"end\":46460,\"start\":46394},{\"end\":46858,\"start\":46794},{\"end\":47277,\"start\":47184},{\"end\":47655,\"start\":47570},{\"end\":48029,\"start\":47877},{\"end\":48383,\"start\":48307},{\"end\":48690,\"start\":48601},{\"end\":49033,\"start\":48967},{\"end\":49320,\"start\":49251},{\"end\":49793,\"start\":49714},{\"end\":50250,\"start\":50201},{\"end\":51091,\"start\":51021},{\"end\":51432,\"start\":51319},{\"end\":51887,\"start\":51827},{\"end\":52218,\"start\":52129},{\"end\":52949,\"start\":52892},{\"end\":53325,\"start\":53255},{\"end\":54164,\"start\":54062},{\"end\":54456,\"start\":54372},{\"end\":55514,\"start\":55470},{\"end\":55744,\"start\":55687},{\"end\":56293,\"start\":56150},{\"end\":57250,\"start\":57216},{\"end\":57614,\"start\":57536},{\"end\":57948,\"start\":57844},{\"end\":58261,\"start\":58202},{\"end\":58506,\"start\":58440},{\"end\":58819,\"start\":58746},{\"end\":58994,\"start\":58969},{\"end\":59403,\"start\":59297},{\"end\":59703,\"start\":59613},{\"end\":60783,\"start\":60709},{\"end\":61549,\"start\":61472},{\"end\":62035,\"start\":61940},{\"end\":62419,\"start\":62336},{\"end\":62788,\"start\":62728}]", "bib_author": "[{\"end\":42075,\"start\":42060},{\"end\":42089,\"start\":42075},{\"end\":42105,\"start\":42089},{\"end\":42123,\"start\":42105},{\"end\":42139,\"start\":42123},{\"end\":42154,\"start\":42139},{\"end\":42167,\"start\":42154},{\"end\":42185,\"start\":42167},{\"end\":42203,\"start\":42185},{\"end\":42218,\"start\":42203},{\"end\":42535,\"start\":42516},{\"end\":42550,\"start\":42535},{\"end\":42579,\"start\":42550},{\"end\":42586,\"start\":42579},{\"end\":42596,\"start\":42586},{\"end\":42826,\"start\":42814},{\"end\":42835,\"start\":42826},{\"end\":42849,\"start\":42835},{\"end\":42859,\"start\":42849},{\"end\":43050,\"start\":43037},{\"end\":43068,\"start\":43050},{\"end\":43252,\"start\":43237},{\"end\":43263,\"start\":43252},{\"end\":43275,\"start\":43263},{\"end\":43291,\"start\":43275},{\"end\":43305,\"start\":43291},{\"end\":43314,\"start\":43305},{\"end\":43324,\"start\":43314},{\"end\":43338,\"start\":43324},{\"end\":43659,\"start\":43647},{\"end\":43670,\"start\":43659},{\"end\":43686,\"start\":43670},{\"end\":43949,\"start\":43933},{\"end\":43970,\"start\":43949},{\"end\":43983,\"start\":43970},{\"end\":44002,\"start\":43983},{\"end\":44020,\"start\":44002},{\"end\":44038,\"start\":44020},{\"end\":44053,\"start\":44038},{\"end\":44063,\"start\":44053},{\"end\":44081,\"start\":44063},{\"end\":44099,\"start\":44081},{\"end\":44361,\"start\":44352},{\"end\":44372,\"start\":44361},{\"end\":44385,\"start\":44372},{\"end\":44395,\"start\":44385},{\"end\":44715,\"start\":44701},{\"end\":44731,\"start\":44715},{\"end\":44743,\"start\":44731},{\"end\":44763,\"start\":44743},{\"end\":45008,\"start\":44997},{\"end\":45021,\"start\":45008},{\"end\":45033,\"start\":45021},{\"end\":45046,\"start\":45033},{\"end\":45058,\"start\":45046},{\"end\":45066,\"start\":45058},{\"end\":45079,\"start\":45066},{\"end\":45087,\"start\":45079},{\"end\":45098,\"start\":45087},{\"end\":45112,\"start\":45098},{\"end\":45461,\"start\":45441},{\"end\":45474,\"start\":45461},{\"end\":45496,\"start\":45474},{\"end\":45514,\"start\":45496},{\"end\":45528,\"start\":45514},{\"end\":45548,\"start\":45528},{\"end\":45566,\"start\":45548},{\"end\":45585,\"start\":45566},{\"end\":45600,\"start\":45585},{\"end\":45615,\"start\":45600},{\"end\":46011,\"start\":46000},{\"end\":46023,\"start\":46011},{\"end\":46032,\"start\":46023},{\"end\":46047,\"start\":46032},{\"end\":46062,\"start\":46047},{\"end\":46075,\"start\":46062},{\"end\":46090,\"start\":46075},{\"end\":46103,\"start\":46090},{\"end\":46117,\"start\":46103},{\"end\":46480,\"start\":46462},{\"end\":46496,\"start\":46480},{\"end\":46516,\"start\":46496},{\"end\":46534,\"start\":46516},{\"end\":46548,\"start\":46534},{\"end\":46563,\"start\":46548},{\"end\":46580,\"start\":46563},{\"end\":46873,\"start\":46860},{\"end\":46885,\"start\":46873},{\"end\":46899,\"start\":46885},{\"end\":46913,\"start\":46899},{\"end\":46925,\"start\":46913},{\"end\":47293,\"start\":47279},{\"end\":47308,\"start\":47293},{\"end\":47317,\"start\":47308},{\"end\":47326,\"start\":47317},{\"end\":47341,\"start\":47326},{\"end\":47350,\"start\":47341},{\"end\":47364,\"start\":47350},{\"end\":47665,\"start\":47657},{\"end\":47678,\"start\":47665},{\"end\":47700,\"start\":47678},{\"end\":47710,\"start\":47700},{\"end\":48045,\"start\":48031},{\"end\":48059,\"start\":48045},{\"end\":48069,\"start\":48059},{\"end\":48084,\"start\":48069},{\"end\":48398,\"start\":48385},{\"end\":48411,\"start\":48398},{\"end\":48420,\"start\":48411},{\"end\":48431,\"start\":48420},{\"end\":48708,\"start\":48692},{\"end\":48723,\"start\":48708},{\"end\":48736,\"start\":48723},{\"end\":48745,\"start\":48736},{\"end\":48757,\"start\":48745},{\"end\":48770,\"start\":48757},{\"end\":49052,\"start\":49035},{\"end\":49086,\"start\":49052},{\"end\":49094,\"start\":49086},{\"end\":49336,\"start\":49322},{\"end\":49348,\"start\":49336},{\"end\":49360,\"start\":49348},{\"end\":49370,\"start\":49360},{\"end\":49381,\"start\":49370},{\"end\":49399,\"start\":49381},{\"end\":49405,\"start\":49399},{\"end\":49807,\"start\":49795},{\"end\":49820,\"start\":49807},{\"end\":49831,\"start\":49820},{\"end\":50030,\"start\":50018},{\"end\":50044,\"start\":50030},{\"end\":50048,\"start\":50044},{\"end\":50270,\"start\":50252},{\"end\":50290,\"start\":50270},{\"end\":50311,\"start\":50290},{\"end\":50328,\"start\":50311},{\"end\":50340,\"start\":50328},{\"end\":50528,\"start\":50515},{\"end\":50543,\"start\":50528},{\"end\":50558,\"start\":50543},{\"end\":50573,\"start\":50558},{\"end\":50585,\"start\":50573},{\"end\":50594,\"start\":50585},{\"end\":50610,\"start\":50594},{\"end\":50624,\"start\":50610},{\"end\":51102,\"start\":51093},{\"end\":51110,\"start\":51102},{\"end\":51121,\"start\":51110},{\"end\":51132,\"start\":51121},{\"end\":51144,\"start\":51132},{\"end\":51153,\"start\":51144},{\"end\":51446,\"start\":51434},{\"end\":51458,\"start\":51446},{\"end\":51471,\"start\":51458},{\"end\":51493,\"start\":51471},{\"end\":51514,\"start\":51493},{\"end\":51525,\"start\":51514},{\"end\":51543,\"start\":51525},{\"end\":51561,\"start\":51543},{\"end\":51903,\"start\":51889},{\"end\":51911,\"start\":51903},{\"end\":51921,\"start\":51911},{\"end\":51931,\"start\":51921},{\"end\":51947,\"start\":51931},{\"end\":51958,\"start\":51947},{\"end\":51966,\"start\":51958},{\"end\":52231,\"start\":52220},{\"end\":52254,\"start\":52231},{\"end\":52272,\"start\":52254},{\"end\":52285,\"start\":52272},{\"end\":52300,\"start\":52285},{\"end\":52321,\"start\":52300},{\"end\":52625,\"start\":52613},{\"end\":52636,\"start\":52625},{\"end\":52649,\"start\":52636},{\"end\":52658,\"start\":52649},{\"end\":52669,\"start\":52658},{\"end\":52679,\"start\":52669},{\"end\":52963,\"start\":52951},{\"end\":52976,\"start\":52963},{\"end\":52988,\"start\":52976},{\"end\":52996,\"start\":52988},{\"end\":53013,\"start\":52996},{\"end\":53025,\"start\":53013},{\"end\":53037,\"start\":53025},{\"end\":53056,\"start\":53037},{\"end\":53061,\"start\":53056},{\"end\":53337,\"start\":53327},{\"end\":53348,\"start\":53337},{\"end\":53360,\"start\":53348},{\"end\":53373,\"start\":53360},{\"end\":53390,\"start\":53373},{\"end\":53399,\"start\":53390},{\"end\":53408,\"start\":53399},{\"end\":53419,\"start\":53408},{\"end\":53432,\"start\":53419},{\"end\":53444,\"start\":53432},{\"end\":53795,\"start\":53783},{\"end\":53808,\"start\":53795},{\"end\":53821,\"start\":53808},{\"end\":53835,\"start\":53821},{\"end\":54180,\"start\":54166},{\"end\":54194,\"start\":54180},{\"end\":54209,\"start\":54194},{\"end\":54472,\"start\":54458},{\"end\":54483,\"start\":54472},{\"end\":54496,\"start\":54483},{\"end\":54509,\"start\":54496},{\"end\":54696,\"start\":54684},{\"end\":54706,\"start\":54696},{\"end\":54719,\"start\":54706},{\"end\":54731,\"start\":54719},{\"end\":54745,\"start\":54731},{\"end\":54757,\"start\":54745},{\"end\":54768,\"start\":54757},{\"end\":54780,\"start\":54768},{\"end\":54798,\"start\":54780},{\"end\":54816,\"start\":54798},{\"end\":55252,\"start\":55241},{\"end\":55265,\"start\":55252},{\"end\":55278,\"start\":55265},{\"end\":55290,\"start\":55278},{\"end\":55531,\"start\":55516},{\"end\":55553,\"start\":55531},{\"end\":55566,\"start\":55553},{\"end\":55762,\"start\":55746},{\"end\":55774,\"start\":55762},{\"end\":55784,\"start\":55774},{\"end\":55797,\"start\":55784},{\"end\":55812,\"start\":55797},{\"end\":55825,\"start\":55812},{\"end\":55838,\"start\":55825},{\"end\":56311,\"start\":56295},{\"end\":56329,\"start\":56311},{\"end\":56347,\"start\":56329},{\"end\":56692,\"start\":56672},{\"end\":56711,\"start\":56692},{\"end\":56723,\"start\":56711},{\"end\":57029,\"start\":57017},{\"end\":57267,\"start\":57252},{\"end\":57283,\"start\":57267},{\"end\":57296,\"start\":57283},{\"end\":57308,\"start\":57296},{\"end\":57322,\"start\":57308},{\"end\":57336,\"start\":57322},{\"end\":57347,\"start\":57336},{\"end\":57363,\"start\":57347},{\"end\":57638,\"start\":57616},{\"end\":57649,\"start\":57638},{\"end\":57664,\"start\":57649},{\"end\":57669,\"start\":57664},{\"end\":57964,\"start\":57950},{\"end\":57981,\"start\":57964},{\"end\":57990,\"start\":57981},{\"end\":58009,\"start\":57990},{\"end\":58278,\"start\":58263},{\"end\":58292,\"start\":58278},{\"end\":58309,\"start\":58292},{\"end\":58519,\"start\":58508},{\"end\":58531,\"start\":58519},{\"end\":58540,\"start\":58531},{\"end\":58548,\"start\":58540},{\"end\":58558,\"start\":58548},{\"end\":58568,\"start\":58558},{\"end\":58580,\"start\":58568},{\"end\":58830,\"start\":58821},{\"end\":58844,\"start\":58830},{\"end\":59012,\"start\":58996},{\"end\":59026,\"start\":59012},{\"end\":59039,\"start\":59026},{\"end\":59056,\"start\":59039},{\"end\":59069,\"start\":59056},{\"end\":59084,\"start\":59069},{\"end\":59099,\"start\":59084},{\"end\":59117,\"start\":59099},{\"end\":59419,\"start\":59405},{\"end\":59432,\"start\":59419},{\"end\":59442,\"start\":59432},{\"end\":59719,\"start\":59705},{\"end\":59732,\"start\":59719},{\"end\":59745,\"start\":59732},{\"end\":59758,\"start\":59745},{\"end\":59772,\"start\":59758},{\"end\":59788,\"start\":59772},{\"end\":59803,\"start\":59788},{\"end\":59815,\"start\":59803},{\"end\":59830,\"start\":59815},{\"end\":60084,\"start\":60073},{\"end\":60094,\"start\":60084},{\"end\":60110,\"start\":60094},{\"end\":60120,\"start\":60110},{\"end\":60500,\"start\":60490},{\"end\":60520,\"start\":60500},{\"end\":60537,\"start\":60520},{\"end\":60549,\"start\":60537},{\"end\":60564,\"start\":60549},{\"end\":60796,\"start\":60785},{\"end\":60808,\"start\":60796},{\"end\":60817,\"start\":60808},{\"end\":60832,\"start\":60817},{\"end\":60842,\"start\":60832},{\"end\":60853,\"start\":60842},{\"end\":61032,\"start\":61021},{\"end\":61045,\"start\":61032},{\"end\":61054,\"start\":61045},{\"end\":61067,\"start\":61054},{\"end\":61078,\"start\":61067},{\"end\":61095,\"start\":61078},{\"end\":61106,\"start\":61095},{\"end\":61116,\"start\":61106},{\"end\":61560,\"start\":61551},{\"end\":61571,\"start\":61560},{\"end\":61584,\"start\":61571},{\"end\":61593,\"start\":61584},{\"end\":61603,\"start\":61593},{\"end\":61616,\"start\":61603},{\"end\":61627,\"start\":61616},{\"end\":61644,\"start\":61627},{\"end\":61655,\"start\":61644},{\"end\":61669,\"start\":61655},{\"end\":61680,\"start\":61669},{\"end\":61693,\"start\":61680},{\"end\":62050,\"start\":62037},{\"end\":62063,\"start\":62050},{\"end\":62072,\"start\":62063},{\"end\":62085,\"start\":62072},{\"end\":62098,\"start\":62085},{\"end\":62111,\"start\":62098},{\"end\":62122,\"start\":62111},{\"end\":62433,\"start\":62421},{\"end\":62443,\"start\":62433},{\"end\":62459,\"start\":62443},{\"end\":62472,\"start\":62459},{\"end\":62481,\"start\":62472},{\"end\":62493,\"start\":62481},{\"end\":62501,\"start\":62493},{\"end\":62509,\"start\":62501},{\"end\":62800,\"start\":62790},{\"end\":62814,\"start\":62800},{\"end\":62836,\"start\":62814}]", "bib_venue": "[{\"end\":42225,\"start\":42218},{\"end\":42600,\"start\":42596},{\"end\":42863,\"start\":42859},{\"end\":43072,\"start\":43068},{\"end\":43342,\"start\":43338},{\"end\":43691,\"start\":43686},{\"end\":44102,\"start\":44099},{\"end\":44459,\"start\":44411},{\"end\":44768,\"start\":44763},{\"end\":45119,\"start\":45112},{\"end\":45619,\"start\":45615},{\"end\":45998,\"start\":45924},{\"end\":46585,\"start\":46580},{\"end\":46977,\"start\":46925},{\"end\":47368,\"start\":47364},{\"end\":47715,\"start\":47710},{\"end\":48088,\"start\":48084},{\"end\":48445,\"start\":48431},{\"end\":48774,\"start\":48770},{\"end\":49100,\"start\":49094},{\"end\":49466,\"start\":49405},{\"end\":49835,\"start\":49831},{\"end\":50016,\"start\":49974},{\"end\":50349,\"start\":50340},{\"end\":50760,\"start\":50624},{\"end\":51158,\"start\":51153},{\"end\":51564,\"start\":51561},{\"end\":51969,\"start\":51966},{\"end\":52328,\"start\":52321},{\"end\":52611,\"start\":52547},{\"end\":53065,\"start\":53061},{\"end\":53458,\"start\":53444},{\"end\":53781,\"start\":53679},{\"end\":54213,\"start\":54209},{\"end\":54518,\"start\":54509},{\"end\":54887,\"start\":54832},{\"end\":55239,\"start\":55143},{\"end\":55570,\"start\":55566},{\"end\":55909,\"start\":55838},{\"end\":56352,\"start\":56347},{\"end\":56670,\"start\":56562},{\"end\":57015,\"start\":56965},{\"end\":57367,\"start\":57363},{\"end\":57674,\"start\":57669},{\"end\":58013,\"start\":58009},{\"end\":58312,\"start\":58309},{\"end\":58584,\"start\":58580},{\"end\":58849,\"start\":58844},{\"end\":59124,\"start\":59117},{\"end\":59445,\"start\":59442},{\"end\":59834,\"start\":59830},{\"end\":60259,\"start\":60136},{\"end\":60856,\"start\":60853},{\"end\":61220,\"start\":61132},{\"end\":61696,\"start\":61693},{\"end\":62129,\"start\":62122},{\"end\":62523,\"start\":62509},{\"end\":62841,\"start\":62836}]"}}}, "year": 2023, "month": 12, "day": 17}