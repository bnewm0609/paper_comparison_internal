{"id": 258959117, "updated": "2023-10-05 00:13:04.129", "metadata": {"title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models", "authors": "[{\"first\":\"Zechun\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Barlas\",\"last\":\"Oguz\",\"middle\":[]},{\"first\":\"Changsheng\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Ernie\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Pierre\",\"last\":\"Stock\",\"middle\":[]},{\"first\":\"Yashar\",\"last\":\"Mehdad\",\"middle\":[]},{\"first\":\"Yangyang\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Raghuraman\",\"last\":\"Krishnamoorthi\",\"middle\":[]},{\"first\":\"Vikas\",\"last\":\"Chandra\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.17888", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-17888", "doi": "10.48550/arxiv.2305.17888"}}, "content": {"source": {"pdf_hash": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.17888v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1e05b7e43994c278ee9d9fd9a02822f9f41170c3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2.txt", "contents": "\nLLM-QAT: Data-Free Quantization Aware Training for Large Language Models\n\n\nZechun Liu zechunliu@meta.com \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nBarlas Oguz barlaso@meta.com \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nMeta Ai \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nChangsheng Zhao cszhao@meta.com \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nErnie Chang \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nPierre Stock \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nMeta Ai \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nYashar Mehdad \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nMeta Ai \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nYangyang Shi \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nRaghuraman Krishnamoorthi \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nVikas Chandra \nReality Labs Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc. Reality Labs, Meta Inc\nMeta Inc\n\n\nLLM-QAT: Data-Free Quantization Aware Training for Large Language Models\n\nSeveral post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to posttraining quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4bits. We observe large improvements over training-free methods, especially in the low-bit settings. * Equal contribution Preprint. Under review. arXiv:2305.17888v1 [cs.CL] 29 May 2023 2 MethodQuantizing large language models (LLMs) using quantization-aware training (QAT) is a nontrivial task with challenges in two key aspects. First, LLMs are pre-trained to excel in zero-shot generalization, and it is crucial to preserve this capability after quantization. Therefore, selecting an appropriate fine-tuning dataset is important. If the QAT data is too narrow in domain or significantly different than the original pre-training distribution, this is likely to hurt model performance. On the other hand, it is difficult to replicate the original training setup exactly, due to the scale and complexity of LLM training. In Section 2.1, we introduce data-free quantization-aware training (QAT) which produces QAT data using next token data generation. This method demonstrates superior performance compared to using subsets of the original pre-training data. Second, LLMs exhibit unique weight and activation distributions characterized by a significant presence of outliers, which distinguishes them from smaller models. Consequently, the state-of-the-art quantization clipping methods for small\n\nIntroduction\n\nFollowing GPT-3 (Brown et al., 2020), several families of large language models (LLMs) such as OPT , PALM (Chowdhery et al., 2022), BLOOM (Scao et al., 2022), Chinchilla (Hoffmann et al., 2022) and LLaMA (Touvron et al., 2023) have established that increasing model size leads to improved model capabilities. As a result, language models with tens of billions or even hundreds of billions of parameters have become the norm in today's AI landscape. Despite the growing excitement around LLMs, serving such models to the benefit of billions of users faces significant hurdles due to their large computational cost and environmental footprint.\n\nFortunately, there has been an increasing effort to accurately quantize LLMs, with multiple recent works (Xiao et al., 2022;Yao et al., 2022) focusing on 8-bit post-training quantization of weights and activations and achieving little to no loss of accuracy. However, a 65 billion parameter LLaMA model still takes up 65GB of GPU memory with only its weights. Moreover, the key-value (KV) cache holding activations for the attention layers can easily go into the tens of GBs, and is the throughput bottleneck in the long sequence length regime common in today's applications. The aforementioned works do not consider KV cache quantization along with weight and activation  Figure 1: Overview of LLM-QAT. We generate data from the pretrained model with next token generation, which is sampled from top-k candidates. Then we use the generated data as input and the teacher model prediction as label to guide quantized model finetuning.\n\nquantization. Unfortunately, SoTA post-training quantization methods dramatically degrade in quality when pushed beyond 8-bits. For higher quantization levels, we find it necessary to resort to quantization-aware training (QAT).\n\nTo our knowledge, QAT for LLMs has not been investigated before. This is understandable for two reasons. First, LLM training is technically difficult and resource intensive. Second, QAT needs training data, which for LLMs is difficult to obtain. The sheer scale and diversity of pre-training data is itself an obstacle. Pre-processing might be prohibitive, or worse, some data might simply not be available due to legal restrictions. It is also incresingly common to train LLMs in multiple stages, involving instruction tuning and reinforcement learning (Ouyang et al., 2022), which would be very difficult to replicate during QAT. In this work, we side-step this issue by using generated data from the LLM itself for knowledge distillation. This simple workaround, which we refer to as data-free knowledge-distillation is applicable to any generative model independent of whether or not the original training data is available. We show that this method is better able to preserve the original model's output distribution, even compared to training on large subsets of the original training set. Moreover, we can successfully distill quantized models using only a small set (100k) of sampled data, thus keeping computational costs reasonable. All of our experiments are conducted using a single 8-gpu training node.\n\nAs a result, we are able to distill the 7B, 13B and 30B LLaMA models with weights and KV cache quantized down to 4-bits. In this regard, our approach exhibits significant enhancements in quality compared to post-training quantization. Notably, larger models employing QAT outperform smaller models utilizing floating-point 16-bit representations, despite having similar model sizes. Furthermore, we have successfully quantized activations to 6-bit precision, surpassing what was possible with existing methods. For a comprehensive analysis of our experimental results and detailed ablations, please refer to Section 3.\n\nIn summary, we present the first application of QAT to LLMs, resulting in the first accurate 4-bit quantized LLMs. We also demonstrate quantizing the KV cache simultaneously with weights and activations, which is critical to alleviate throughput bottlenecks for long sequence generation. All of this is achieved by a novel data-free distillation method, which makes QAT practical for large pre-trained generative models.  Figure 2: Overview of the quantized transformer in LLM-QAT. We quantize all the weights and input activations in fully-connected linear layers. The KV cache is also quantized if specified. models do not work out of the box for LLMs. In Section 2.2, we identify suitable quantizers for LLMs.\n\n\nData-free Distillation\n\nIn order to closely synthesize the distribution of the pre-training data with a limited amount of fine-tuning data, we proposed next token data generation from the original pre-trained model. As shown in Figure 1 (a), we randomize the first token <start> from vocabulary and let the pre-trained model to generate the next token <out1>, then the generated token is appended to the start token for generating new output <out2>. We repeat this iterative procedure until we reach either the end of sentence token or the maximum generation length.\n\nWe test three different sampling strategies in the next token generation. The most straightforward way is to pick the top-1 candidate as the next token. However, the generated sentence lacks of diversity and will cyclically repeat several tokens. To address this issue, we instead stochastically sample the next token from the distribution using the SoftMax output of the pre-trained model as the probability. This sampling strategy yields more diverse sentences and greatly enhances the accuracy of the fine-tuned student model. Furthermore, we discover that the initial few tokens play a crucial role in determining the prediction trend. Therefore, it is important for them to have higher confidence. In our generative process, we employ a hybrid sampling strategy that deterministically selects the top-1 predictions for the first 3~5 tokens and stochastically samples the remaining tokens. A detailed ablation study comparing different generated data and real data is presented in Section3.3.1.\n\n\nQuantization-Aware Training\n\n\nPreliminaries\n\nIn this work, we study linear quantization i.e., uniform quantization. Linear quantization can be categorized into two categories based on whether the real values are clipped or not: MinMax quantization, which preserves all value ranges, and clipping-based quantization.\n\nIn MinMax quantization, the quantization process can be formulated as:\nX i Q = \u03b1X i Q = \u03b1\u230a X i R \u2212 \u03b2 \u03b1 \u2309 + \u03b2. (1) Activation ! \u2026 Weights \" * (a) \u2026 Per-token (b) \u2026 value $ \u2026 \u2026 \u2026 key #\n\nPer-token Per-token\n\nPer-channel Figure 3: Illustration of (a) per-channel weight quantization and per-token activation quantization (b) per-token quantization for KV cache. The KV cache is updated by appending the current key and value to it. Thus we adopt per-token quantization for both key and value.\n\nHere X Q and X R denote the quantized and full-precision variables, respectively. i refers to the i-th element in the tensor. \u03b1 is the scaling factor and \u03b2 is the zero-point value. For symmetric quantization,\n\u03b1 = max(|X R |) 2 N \u22121 \u22121 , \u03b2 = 0. And for asymmetric quantization, \u03b1 = max(X R )\u2212min(X R ) 2 N \u22121 , \u03b2 = min(X R ).\nCompared to the MinMax Quantization, clipping the outliers can help improve the precision and allocate more bits to the intermediate values. Thus, many recent work (Shen et al., 2020a;Zhang et al., 2020) adopts clipping-based quantization for transformer-based language models. The quantization can be formulated as:\nX i Q = \u03b1X i Q = \u03b1\u230aClip( X i R \u2212 \u03b2 \u03b1 , 0, 1)\u2309 + \u03b2.(2)\nwhere the scale \u03b1 and zero-point value \u03b2 can be calculated statistically or learned through gradients.\n\n\nQuantization for Large Language Models\n\nQuantization function We illustrate our quantized transformer model in Figure 2. In line with the findings in (Dettmers et al., 2022;Xiao et al., 2022), we have also observed a significant presence of outliers in both the weights and activations of large language models (LLMs). These outliers have a notable impact on the quantization process, as they contribute to an increase in the quantization step size while diminishing the precision of intermediate values. Nevertheless, clipping these outliers during quantization proves detrimental to LLM performance. During the initial stages of training, any clipping-based method will lead to exceptionally high perplexity scores (i.e., > 10000), causing a substantial loss of information that proves to be difficult to recover through fine-tuning. Therefore, we choose to retain these outliers instead. Moreover, we find that in the model with the gated linear unit (GLU), the activations are weights are mostly symmetrically distributed. Based on our analysis and empirical observations, we choose symmetric MinMax quantization for both weights and activations:\nX i Q = \u03b1\u230a X i R \u03b1 \u2309, \u03b1 = max(|X R |) 2 N \u22121 \u2212 1(3)\nHere X Q denotes the quantized weights or activations and X R denotes the real-valued weights or activations. To ensure efficient quantization, we adopt the per-token activation quantization and per-channel weight quantization as illustrated in Figure 3 (a). For a comprehensive evaluation of the different quantizer choices, we provide the ablation study in Section 3.3.2.\n\nQuantization-aware training for key-value cache In addition to weight and activation quantization, the key-value cache (KV cache) in large language models (LLMs) also consumes a non-negligible amount of memory. However, only a few previous works have addressed the KV cache quantization in LLMs, with the methods primarily limited to post-training quantization (Sheng et al., 2023). In our study, we demonstrate that a similar quantization-aware training approach used for activation quantization can be employed to quantize the KV cache. As illustrated in Figure 3, we adopt per-token quantization in Eq. 3, given that the key and value are generated token by token. During the generation process, the current key and value are quantized, and their corresponding scaling factor is stored. During the training process for QAT, we apply quantization to the entire activation tensors of both the keys and values, as shown in Figure 2. By integrating the quantization function into the gradient calculation, we ensure effective training using quantized key-value pairs.\n\nKnowledge distillation We use cross-entropy based logits distillation for training the quantized student network from the full-precision pre-trained teacher network:\nL CE = \u2212 1 n c n i=1 p T c (X i ) log(p S c (X i )),(4)\nHere i denotes the i th sample in the current batch with n total sentences. c denotes the number of classes and in our case, it equals the size of the vocabulary. T and S are the teacher network and student network, respectively.\n\nAs discussed in Section 2.1, in the data generation process, it is important to sample the next token from distribution rather than always selecting the top-1 candidate. By doing so, the next token does not necessarily represent the optimal label for training the student model, as the sampling introduces inherent noise. Consequently, we propose to utilize the predictions from the pre-trained model as soft labels, which provides more informative targets for guiding the training of the student model. We present a comprehensive ablation study in Section 3.3.3 to delve into the specifics of this approach.\n\n\nExperiments\n\nWe assess the effectiveness of our approach by conducting experiments on LLaMA-7B/13B/30B models and presenting results on various tasks. Specifically, we report the zero-shot performance on Common Sense Reasoning tasks such as BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC , and OBQA (Mihaylov et al., 2018). We also assess the few-shot performance on TriviaQA (Joshi et al., 2017) and MMLU (Hendrycks et al., 2020) datasets, along with perplexity scores on WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020) datasets.\n\n\nExperimental Settings\n\nIn our quantized network training process, we initialize the model with a pre-trained model and employ it as the teacher for knowledge distillation. To optimize the model, we utilize the AdamW (Loshchilov & Hutter, 2017) optimizer with zero weight decay. Each GPU is assigned a batch size of 1, and the learning rate is set to 2e-5, following a cosine learning-rate decay strategy. For data generation, we utilize the LLaMA-7B model, and the maximum length of generated sequences is set to 1024.\n\n\nMain Results\n\nWe consider three post-training quantization (PTQ) methods, round-to-nearest (RTN), GPT-Q (Frantar et al., 2022) and SmoothQuant (Xiao et al., 2022) as baselines. We compare to them in several different settings, where the weights, activations and KV cache values are quantized to different levels (denoted as W-A-KV). Different PTQ methods perform well in different settings, and we compare our method to the best PTQ result in each setting.   this setting might not be currently practical due to lack of hardware support, it's a promising data point for sub-8-bit computation for LLMs. Unfortunately 4-bit activation quantization did not work well for the settings that we tried (see Section 3.4).\n(\u2191) (\u2191) (\u2191) (\u2191) (\u2191) (\u2191) (\u2191) (\u2191)(\u2191)\nOne important question for practitioners is whether to use a small model at full precision, or a larger quantized model of similar inference cost. While the exact trade-offs can vary based on several factors, we can make several recommendations based on our results. First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B. As a result, we recommend 4-bit LLM-QAT models for the best efficiency-accuracy tradeoff.\n\n\nAblation\n\nWe conduct the ablation study regarding the data choice, quantization methods, and knowledge distillation methods in Section 3.3.1, Section 3.3.2 and Section 3.3.3, respectively. We report both the perplexity scores on WikiText2 (Merity et al., 2016)/C4 (Raffel et al., 2020) datasets and the performance on zero-shot common sense reasoning tasks.\n\n\nData Choice\n\nIn Table 3, we observe that WikiText (Merity et al., 2016), which is constructed using text extracted from Wikipedia, does not encompass all the information utilized during pre-training. Consequently, a model fine-tuned solely on WikiText tends to overfit on this specific dataset and struggles to generalize well to other datasets. On the other hand, the Crawled Corpus (C4) dataset (Raffel et al., 2020) comprises hundreds of gigabytes of clean English text collected from the web. Fine-tuning the model on C4 yields reasonable transfer accuracy when evaluated on the WikiText dataset. However, it exhibits poor accuracy when tasked with zero-shot inference tasks.\n\nCompared to the existing data, the model fine-tuned on generated data demonstrates superior generalizability, particularly in zero-shot tasks. Moreover, the data generated through sampling from the distribution exhibits greater diversity compared to the data generated without sampling. This enhanced diversity leads to significantly improved performance across all tasks. Table 3: Effects of the finetuning data to the performance in downstream tasks. We use 4-bit weight 6bit activation LLaMA-7B for the experiments. We test three strategies for data generation. Generated data 1 refers to always picking the top-1 candidate without sampling. Generated data 2 refers to sampling the next token from the distribution. Generated data 3 refers to first 3~5 tokens are generated with deterministic selection while the rest are stochastically sampled from the distribution.  \n\n\nQuantization Function\n\nWe compare the no-clipping quantization method with clipping-based methods in Table 4. Following the practice in previous works (Liu et al., 2022b(Liu et al., , 2023, we use StatsQ (Liu et al., 2022a), a statisticallycalculated scaling factor for clipping-based weight quantization and LSQ (Esser et al., 2019), the learnable scaling factor for clipping-based activation quantization. However, our findings indicate that these two state-of-the-art clipping-based quantization methods do not surpass the performance achieved by the MinMax non-clipping method. This observation reinforces the argument that preserving the outliers is critical to the performance of large language models.\n\nFurthermore, we observe that for LLaMA models, the activations and weights exhibit predominantly symmetric distributions, which makes using symmetric quantizers the best choice. It is important to note, however, that this conclusion may not hold true for other large language models, especially those incorporating GeLU layers. Table 5 shows that different knowledge distillation methods have a significant impact on the final accuracy of fine-tuned models. Notably, utilizing the next token alone as the label is sub-optimal due to the inherent randomness and noise introduced by sampling from a distribution of candidates during the generation process. In contrast, logit distillation, which utilizes the complete logit distribution prediction from the teacher model, leads to superior performance of fine-tuned models compared to label-based training approaches. Interestingly, we have observed that incorporating attention distillation or hidden layer distillation actually hampers the performance. Consequently, we exclusively employ logit distillation in all our experiments.\n\n\nKnowledge Distillation\n\n\nCompatibility with SmoothQuant\n\nOur method is also compatible with the weight activation rescale technique proposed in SmoothQuant (Xiao et al., 2022). Table 6 shows that incorporating SmoothQuant into 4-bit weight 4-bit activation (W4A4) quantization can further improve accuracy. However, in the case where the activation bit is greater than the weight bit (i.e, W4A8), adding SmoothQuant does not yield any improvement and may even harm the performance.  \n\n\nRelated Works\n\nQuantization Neural network quantization is proved to be a valuable tool in compressing model size and reducing storage consumption. Classic quantization methods, such as MinMax quantization (Jacob et al., 2018;Krishnamoorthi, 2018), Learned step-size quantization (Esser et al., 2019), PACT (Choi et al., 2018), N2UQ (Liu et al., 2022a) and etc, have primarily been developed for convolutional neural networks. While several recent works have explored language model compression, they are mostly focused on smaller models (Zafrir et al., 2019;Fan et al., 2020;Shen et al., 2020b;Zadeh et al., 2020;Bai et al., 2021;Qin et al., 2021;Liu et al., 2022b) like BERT (Devlin et al., 2019) or BART (Lewis et al., 2019). For large language models (LLMs), the available quantization methods are mostly limited to post-training quantization (Xiao et al., 2022;Yao et al., 2022;Frantar et al., 2022), due to the lack of accessible training data or the prohibitive resource requirements for fine-tuning on the entire pre-training dataset. To the best of our knowledge, no previous work has addressed the specific challenge of quantization-aware training for LLMs.\n\nData generation Data generation for QAT remains a relatively unexplored field of research. While there are several works in the vision domain fine-tuning student networks Liu et al., 2022c;Cai et al., 2020) using data generated by pre-trained teacher models, these methods mainly focus on image data. They involve updating the noise input based on gradients computed from the label and reconstructing images by accumulating these gradients. In contrast, our proposed approach introduces next token data generation in the language domain. This method is more natural and proves to be effective for fine-tuning quantized language models.\n\n\nA Appendix\n\nA.1 Few-shot Evaluation Results Table 7 presents the few-shot performance of the quantized model on the MMLU Hendrycks et al. (2020) and TriviaQA Joshi et al. (2017) benchmarks.  \n\n\nA.2 Memory consumption of KV cache\n\nWe compute the memory required to store the key-value cache (KV cache) in large language models for various sequence lengths, as shown in Table 8. It is evident that the size of the KV cache can quickly exceed the model size when dealing with longer sequences. Given the increasing use of long inputs and contexts in various applications, it becomes crucial to compress the KV cache.\n\n\nA.3 Evaluation Benchmarks\n\nA.3.1 Zero-shot Common Sense Reasoning tasks\n\nBoolQ (Clark et al., 2019) is a reading comprehension dataset of naturally occurring yes/no questions. Each example consists of a question (Q), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity.\n\nPIQA (Bisk et al., 2020), short for Physical Interaction: Question Answering, is a benchmark for evaluating and studying physical commonsense understanding in natural language models.\n\nSIQA (Sap et al., 2019) aims to measure the social and emotional intelligence of computational models through multiple choice question answering (QA).\n\nHellaSwag (Zellers et al., 2019) is a benchmark for physically situated commonsense natural language inference. It consists the four-way multiple-choice problems that are trivial for humans (> 95% accuracy), but challenging for the language models.\n\nWinoGrande (Sakaguchi et al., 2021) is a benchmark for commonsense reasoning. It comprises a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations.\n\nARC , the AI2 Reasoning Challenge, contains a collection of 7787 natural science questions. It is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\n\nOBQA (Mihaylov et al., 2018) is a dataset of about 6000 questions for open book question answering. The task focuses on the challenge of combining a corpus of provided science facts (open book) with external broad common knowledge.\n\n\nA.3.2 Few-shot Tasks\n\nTriviaQA (Joshi et al., 2017) is a closed-book question answering benchmark. It contains over 650K question-answer evidence triples, that are derived by combining 95K Trivia enthusiast authored question-answer pairs with on average six supporting evidence documents per question.\n\nMMLU (Hendrycks et al., 2020), the Massive Multitask Language Understanding(MMLU) benchmark Hendrycks et al. (2020), consists of multiple choice questions covering various domains of knowledge, including humanities, STEM and social sciences.\n\n\nA.4 Generation Tasks\n\nWikiText2 (Merity et al., 2016) is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n\nTable 1 ,\n1table 2 and table 7 (in Appendix)give the comparisons of the proposed QAT methods with SOTA PTQ methods for LLMs on Zero-shot tasks on Common Sense Reasoning tasks, perplexity evaluation on Wiki2 and C4 and few shot exact match on the MMLU and TriviaQA benchmarks respectively. The perplexity evaluations verify whether the quantize models are able to preserve the output distribution of the model on a diverse sample of its training domains. The zero-shot and few-shot evaluations measure if the model's capabilities on downstream tasks are retained.The trends in each table are similar. All methods tend to do well in the 8-bit setting across all model sizes. This holds even when the KV cache is also quantized to 8-bits, together with weights and activations. However, when either of these three values are quantized to less than 8-bits, PTQ methods result in accuracy loss, whereas LLM-QAT holds up much better. For example in the 8-8-4 setting, 30B LLM-QAT achieves an average zero-shot accuracy of 69.7, compared to 50.7 with SmoothQuant(Table 1,rows 68-69). The difference is smaller in the 4-8-8 setting, however LLM-QAT still outperforms the best PTQ method (RTN in this case) by 1.4 points (rows 55, 57). In the 4-8-4 setting, where both weights and the KV cache are quantized to 4 bits, all PTQ methods produce poor results, whereas LLM-QAT achieves 69.9, only trailing the full precision model by 1.5 points on average. LLM-QAT also works reasonably well for 6-bit activation quantization. WhileTable 1: Zero-shot performance on Common Sense Reasoning tasks.BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. \nMethod \n#Bits Size (GB) \n\n\nTable 2 :\n2Perplexity evaluation results on WikiTextMerity et al. (2016) andC4 Raffel et al. (2020)    Perplexity \nPerplexity \nPerplexity \n#Bits \nMethod \nC4 (\u2193) Wiki2 (\u2193) Method \nC4 (\u2193) Wiki2 (\u2193) Method \nC4 (\u2193) Wiki2 (\u2193) \n\n1 \n\n16-16-16 LLaMA-7B \n7.2 \n10.4 \nLLaMA-13B \n6.7 \n9.7 \nLLaMA-30B \n6.0 \n7.0 \n\n2 \n\n4-8-4 \nRTN \n55.1 \n151.4 \nRTN \n25.0 \n103.6 \nRTN \n8.2 \n8.9 \n\n3 \n\n4-8-4 \nSmoothQuant 81.1 \n163.6 \nSmoothQuant 26.0 \n60.1 \nSmoothQuant 10.6 \n12.0 \n\n4 \n\n4-8-4 \nLLM-QAT \n8.6 \n11.6 \nLLM-QAT \n7.6 \n10.2 \nLLM-QAT \n7.3 \n7.7 \n\n5 \n\n4-8-8 \nRTN \n8.4 \n13.9 \nRTN \n7.3 \n12.5 \nRTN \n7.4 \n8.2 \n\n6 \n\n4-8-8 \nSmoothQuant \n9.1 \n13.7 \nSmoothQuant \n8.8 \n12.5 \nSmoothQuant \n8.7 \n9.8 \n\n7 \n\n4-8-8 \nLLM-QAT \n7.5 \n11.2 \nLLM-QAT \n6.8 \n10.0 \nLLM-QAT \n6.9 \n7.5 \n\n8 \n\n4-6-16 RTN \n10.5 \n20.0 \nRTN \n11.3 \n32.7 \nRTN \n11.4 \n15.4 \n\n9 \n\n4-6-16 SmoothQuant \n9.9 \n14.7 \nSmoothQuant \n9.1 \n13.6 \nSmoothQuant \n8.7 \n12.5 \n\n10 \n\n4-6-16 LLM-QAT \n7.7 \n10.8 \nLLM-QAT \n7.1 \n10.5 \nLLM-QAT \n7.3 \n7.9 \n\n11 \n\n4-8-16 RTN \n8.6 \n14.0 \nRTN \n7.5 \n12.5 \nRTN \n7.4 \n8.2 \n\n12 \n\n4-8-16 SmoothQuant \n9.1 \n13.7 \nSmoothQuant \n8.7 \n12.6 \nSmoothQuant \n8.7 \n9.8 \n\n13 \n\n4-8-16 LLM-QAT \n7.4 \n10.9 \nLLM-QAT \n6.8 \n10.0 \nLLM-QAT \n6.9 \n7.5 \n\n14 \n\n4-16-16 RTN \n8.5 \n14.4 \nRTN \n7.3 \n11.9 \nRTN \n7.0 \n7.7 \n\n15 \n\n4-16-16 GPTQ \n8.4 \n17.4 \nGPTQ \n6.8 \n10.7 \nGPTQ \n6.2 \n7.9 \n\n16 \n\n4-16-16 LLM-QAT \n7.4 \n10.9 \nLLM-QAT \n6.5 \n9.6 \nLLM-QAT \n6.5 \n7.3 \n\n17 \n\n8-8-4 \nRTN \n42.1 \n105.1 \nRTN \n15.4 \n43.4 \nRTN \n7.0 \n7.8 \n\n18 \n\n8-8-4 \nSmoothQuant 30.8 \n77.9 \nSmoothQuant 13.9 \n40.9 \nSmoothQuant \n6.7 \n7.5 \n\n19 \n\n8-8-4 \nLLM-QAT \n7.6 \n10.2 \nLLM-QAT \n7.5 \n11.3 \nLLM-QAT \n6.8 \n7.4 \n\n20 \n\n8-8-8 \nRTN \n7.1 \n10.7 \nRTN \n6.6 \n10.0 \nRTN \n6.3 \n7.3 \n\n21 \n\n8-8-8 \nSmoothQuant \n7.0 \n10.5 \nSmoothQuant \n6.5 \n9.8 \nSmoothQuant \n6.1 \n7.1 \n\n22 \n\n8-8-8 \nLLM-QAT \n7.0 \n10.3 \nLLM-QAT \n7.0 \n9.4 \nLLM-QAT \n6.3 \n7.1 \n\n23 \n\n8-8-16 RTN \n7.3 \n10.7 \nRTN \n6.8 \n10.1 \nRTN \n6.3 \n7.3 \n\n24 \n\n8-8-16 SmoothQuant \n7.0 \n10.5 \nSmoothQuant \n6.5 \n9.7 \nSmoothQuant \n6.1 \n7.1 \n\n25 \n\n8-8-16 LLM-QAT \n7.0 \n10.3 \nLLM-QAT \n6.5 \n9.5 \nLLM-QAT \n6.3 \n7.1 \n\n\n\nTable 4 :\n4Ablation study on the effects of the quantization methods on LLaMA-7B model. The quantization level is set to 4-bit weight and 8-bit activation.C4 Wiki2 BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. \nWeight Activation (\u2193) (\u2193) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n1 (Pretrained Model) \n7.2 10.7 \n76.8 \n79.3 48.6 \n76.1 \n70.0 \n73.0 \n48.0 \n57.6 66.2 \n2 StatsQ \nLSQ \n9.0 11.9 \n64.9 \n66.8 43.6 \n63.5 \n56.1 \n51.0 \n31.4 \n33.8 51.4 \n3 MinMax LSQ \n9.4 12.8 \n63.5 \n62.4 42.4 \n61.2 \n52.9 \n45.6 \n29.6 \n33.8 48.9 \n4 StatsQ \nMinMax \n8.2 11.0 \n71.7 \n75.1 43.7 \n69.5 \n58.9 \n62.6 \n35.2 \n37.8 56.8 \n5 MinMax MinMax \n7.4 10.9 \n74.8 \n77.8 48.6 \n73.6 \n69.0 \n69.7 \n45.8 \n55.8 64.4 \n6 Asym \nAsym \n7.3 10.4 \n75.0 \n78.4 48.0 \n73.9 \n69.3 \n71.9 \n45.7 \n52.6 64.3 \n7 Sym \nAsym \n7.4 11.0 \n72.7 \n77.9 48.8 \n73.3 \n67.9 \n69.2 \n45.2 \n56.0 63.9 \n8 Asym \nSym \n7.4 10.9 \n73.3 \n78.4 48.0 \n73.9 \n68.9 \n71.4 \n46.4 \n54.0 64.3 \n9 Sym \nSym \n7.4 10.9 \n74.8 \n77.8 48.6 \n73.6 \n69.0 \n69.7 \n45.8 \n55.8 64.4 \n\n\n\nTable 5 :\n5Ablation study on the knowledge distillation choices on LLaMA-7B model with generated data. The quantization level is set to 4-bit weight and 6-bit activation.C4 Wiki2 BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. \nMethod \n(\u2193) \n(\u2193) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n1 (Pretrained Model) \n7.2 10.4 \n76.8 \n79.3 48.6 \n76.1 \n70.0 \n73.0 \n48.0 \n57.6 66.2 \n2 Label \n8.1 11.9 \n69.4 \n77.3 48.7 \n72.1 \n67.1 \n67.6 \n45.4 \n51.4 62.4 \n3 Lable + Attention \n8.8 18.6 \n70.2 \n75.3 47.6 \n68.9 \n67.2 \n65.6 \n42.6 \n51.2 61.1 \n4 Lable + Hidden \n10.9 16.2 \n61.0 \n53.5 41.1 \n32.6 \n50.2 \n25.8 \n23.1 \n25.0 37.7 \n5 Lable + Logits \n7.8 11.0 \n70.8 \n77.3 48.3 \n72.5 \n66.7 \n68.2 \n46.5 \n55.4 63.2 \n6 Logits \n7.7 10.8 \n72.9 \n76.8 47.9 \n72.4 \n68.3 \n68.8 \n44.2 \n53.2 63.1 \n7 Logits + Attention \n7.9 12.2 \n73.2 \n74.6 47.2 \n69.1 \n65.1 \n64.8 \n42.1 \n52.8 61.1 \n8 Logits + Hidden \n22.3 52.6 \n38.0 \n50.4 38.6 \n25.6 \n50.5 \n26.3 \n24.3 \n25.8 34.9 \n9 Logits + Hidden + Attention 21.9 46.0 \n55.0 \n47.8 39.0 \n33.4 \n48.5 \n29.7 \n26.4 \n25.8 38.2 \n\n\n\nTable 6 :\n6Combine LLM-QAT with SmoothQuant for lower quantization bit settingsBoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. \nMethod \n#Bits \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n\n1 \n\nLLaMA-7B \n16-16-16 76.8 \n79.3 48.6 \n76.1 \n70.0 \n73.0 \n48.0 \n57.6 66.2 \n\n2 \n\nRTN \n4-8-16 \n67.6 \n77.4 47.1 \n71.6 \n66.9 \n67.1 \n45.8 \n52.0 61.9 \n\n3 \n\nSmoothQuant \n4-8-16 \n70.2 \n76.4 44.8 \n68.1 \n66.0 \n67.3 \n42.9 \n49.0 60.6 \n\n4 \n\nLLM-QAT \n4-8-16 \n74.8 \n77.8 48.6 \n73.6 \n69.0 \n69.7 \n45.8 \n55.8 64.4 \n\n5 \n\nLLM-QAT + SmoothQuant 4-8-16 \n74.1 \n77.2 47.8 \n71.9 \n67.7 \n69.6 \n44.8 \n55.4 63.6 \n\n6 \n\nRTN \n4-4-16 \n51.3 \n49.8 36.9 \n26.2 \n47.9 \n25.7 \n24.5 \n31.2 36.7 \n\n7 \n\nSmoothQuant \n4-4-16 \n54.1 \n62.8 41.8 \n41.5 \n52.6 \n50.6 \n32.9 \n36.4 46.6 \n\n8 \n\nLLM-QAT \n4-4-16 \n57.9 \n47.5 39.9 \n25.8 \n47.6 \n27.2 \n25.8 \n29.4 37.6 \n\n9 \n\nLLM-QAT + SmoothQuant 4-4-16 \n63.5 \n64.3 41.8 \n55.6 \n52.9 \n50.3 \n30.2 \n35.0 49.2 \n10 RTN \n4-4-4 \n50.2 \n50.5 37.1 \n26.0 \n49.6 \n26.1 \n24.4 \n28.6 36.6 \n11 SmoothQuant \n4-4-4 \n49.1 \n49.8 39.1 \n27.4 \n48.0 \n30.4 \n25.8 \n29.2 37.4 \n12 LLM-QAT \n4-4-4 \n61.3 \n51.5 39.2 \n31.1 \n51.9 \n27.9 \n23.9 \n29.4 39.5 \n13 LLM-QAT + SmoothQuant 4-4-4 \n62.4 \n55.9 40.9 \n47.8 \n50.6 \n35.5 \n26.4 \n34.6 44.3 \n\n\n\nTable 7 :\n75-shot few-shot exact match performance on the TriviaQA dataset and 5-shot accuracy on \nMassive Multitask Language Understanding (MMLU) dataset . \n\nMMLU \nHumanities STEM Social Sciences Other Average TriviaQA \nMethod \n#Bits Size (GB) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n(\u2191) \n1 LLaMA-7B 16-16-16 12.6 \n33.5 \n30.6 \n38.4 \n39.1 \n35.2 \n57.0 \n2 RTN \n4-8-4 \n3.5 \n23.9 \n26.8 \n26.5 \n24.4 \n25.2 \n0.3 \n3 SmoothQuant 4-8-4 \n3.5 \n24.3 \n27.5 \n26.2 \n24.6 \n25.5 \n3.9 \n4 LLM-QAT \n4-8-4 \n3.5 \n25.6 \n24.3 \n24.0 \n27.8 \n25.5 \n42.6 \n5 RTN \n4-8-8 \n3.5 \n30.1 \n25.6 \n27.5 \n32.5 \n29.1 \n44.5 \n6 SmoothQuant 4-8-8 \n3.5 \n27.1 \n28.9 \n28.0 \n31.9 \n28.7 \n39.6 \n7 LLM-QAT \n4-8-8 \n3.5 \n30.0 \n27.4 \n28.4 \n34.2 \n30.0 \n50.8 \n8 RTN \n4-6-16 \n3.5 \n27.0 \n26.0 \n25.8 \n27.0 \n26.5 \n36.0 \n9 SmoothQuant 4-6-16 \n3.5 \n26.2 \n27.0 \n27.5 \n29.9 \n27.5 \n36.2 \n10 LLM-QAT \n4-6-16 \n3.5 \n28.9 \n27.3 \n31.6 \n33.0 \n30.0 \n49.0 \n11 RTN \n4-8-16 \n3.5 \n30.2 \n25.9 \n26.8 \n32.0 \n28.9 \n44.9 \n12 SmoothQuant 4-8-16 \n3.5 \n26.9 \n28.6 \n29.6 \n32.0 \n29.0 \n40.0 \n13 LLM-QAT \n4-8-16 \n3.5 \n30.3 \n28.1 \n30.3 \n34.5 \n30.8 \n50.8 \n14 RTN \n8-8-4 \n6.5 \n24.2 \n27.3 \n25.8 \n24.5 \n25.3 \n14.8 \n15 SmoothQuant 8-8-4 \n6.5 \n24.4 \n26.4 \n25.6 \n24.2 \n25.1 \n32.8 \n16 LLM-QAT \n8-8-4 \n6.5 \n28.3 \n25.5 \n28.7 \n30.4 \n28.2 \n46.2 \n17 RTN \n8-8-8 \n6.5 \n34.3 \n31.9 \n38.5 \n40.5 \n36.1 \n56.6 \n18 SmoothQuant 8-8-8 \n6.5 \n33.2 \n31.5 \n38.5 \n38.9 \n35.3 \n56.7 \n19 LLM-QAT \n8-8-8 \n6.5 \n32.9 \n29.7 \n37.9 \n37.9 \n34.4 \n56.1 \n20 RTN \n8-8-16 \n6.5 \n34.4 \n31.8 \n39.3 \n39.9 \n36.1 \n56.6 \n21 SmoothQuant 8-8-16 \n6.5 \n33.0 \n30.5 \n38.7 \n38.8 \n35.0 \n56.8 \n22 LLM-QAT \n8-8-16 \n6.5 \n32.2 \n29.4 \n37.0 \n37.6 \n33.8 \n56.1 \n23 LLaMA-13B 16-16-16 24.2 \n44.4 \n36.2 \n54.3 \n53.3 \n46.7 \n63.7 \n24 RTN \n4-8-4 \n6.5 \n25.5 \n24.9 \n24.3 \n26.5 \n25.3 \n22.2 \n25 SmoothQuant 4-8-4 \n6.5 \n25.6 \n22.8 \n23.4 \n26.4 \n24.7 \n32.7 \n26 LLM-QAT \n4-8-4 \n6.5 \n29.4 \n28.5 \n31.9 \n35.8 \n31.1 \n54.3 \n27 RTN \n4-8-8 \n6.5 \n38.3 \n32.7 \n45.3 \n46.4 \n40.4 \n57.9 \n28 SmoothQuant 4-8-8 \n6.5 \n30.9 \n28.6 \n33.4 \n37.1 \n32.3 \n46.6 \n29 LLM-QAT \n4-8-8 \n6.5 \n38.7 \n32.8 \n47.1 \n47.7 \n41.2 \n59.3 \n30 RTN \n4-6-16 \n6.5 \n28.5 \n27.8 \n29.5 \n32.0 \n29.3 \n39.6 \n31 SmoothQuant 4-6-16 \n6.5 \n30.3 \n29.6 \n33.5 \n37.1 \n32.4 \n44.8 \n32 LLM-QAT \n4-6-16 \n6.5 \n37.4 \n33.4 \n45.1 \n46.0 \n40.1 \n57.7 \n33 RTN \n4-8-16 \n6.5 \n38.7 \n32.6 \n45.2 \n45.8 \n40.3 \n57.9 \n34 SmoothQuant 4-8-16 \n6.5 \n30.3 \n27.8 \n34.3 \n37.5 \n32.2 \n46.6 \n35 LLM-QAT \n4-8-16 \n6.5 \n40.1 \n32.4 \n47.6 \n48.0 \n41.8 \n59.8 \n36 RTN \n8-8-4 \n12.4 \n27.8 \n26.2 \n27.0 \n29.6 \n27.6 \n44.3 \n37 SmoothQuant 8-8-4 \n12.4 \n27.8 \n28.1 \n28.6 \n32.3 \n29.1 \n49.6 \n38 LLM-QAT \n8-8-4 \n12.4 \n34.1 \n29.3 \n38.7 \n40.7 \n35.5 \n58.8 \n39 RTN \n8-8-8 \n12.4 \n44.2 \n35.6 \n52.2 \n52.5 \n45.9 \n62.9 \n40 SmoothQuant 8-8-8 \n12.4 \n44.5 \n36.1 \n53.5 \n53.3 \n46.6 \n63.4 \n41 LLM-QAT \n8-8-8 \n12.4 \n43.5 \n36.1 \n52.6 \n52.5 \n45.8 \n63.3 \n42 RTN \n8-8-16 \n12.4 \n44.3 \n34.9 \n51.7 \n53.0 \n45.7 \n63.1 \n43 SmoothQuant 8-8-16 \n12.4 \n44.5 \n36.4 \n53.7 \n53.4 \n46.7 \n63.4 \n44 LLM-QAT \n8-8-16 \n12.4 \n43.6 \n36.1 \n53.8 \n53.2 \n46.3 \n63.4 \n23 LLaMA-30B 16-16-16 60.6 \n55.8 \n46.0 \n66.7 \n63.4 \n57.8 \n69.9 \n46 RTN \n4-8-4 \n15.7 \n24.4 \n26.2 \n27.2 \n26.4 \n25.9 \n19.2 \n47 SmoothQuant 4-8-4 \n15.7 \n23.9 \n27.5 \n23.2 \n24.1 \n24.6 \n7.5 \n48 LLM-QAT \n4-8-4 \n15.7 \n47.6 \n40.4 \n55.9 \n54.5 \n49.3 \n63.5 \n49 RTN \n4-8-8 \n15.7 \n51.0 \n43.6 \n62.2 \n60.6 \n53.9 \n66.8 \n50 SmoothQuant 4-8-8 \n15.7 \n35.2 \n35.1 \n46.9 \n45.2 \n40.0 \n57.9 \n51 LLM-QAT \n4-8-8 \n15.7 \n52.2 \n44.3 \n61.4 \n61.0 \n54.4 \n65.9 \n52 RTN \n4-6-16 \n15.7 \n29.5 \n31.3 \n32.1 \n36.2 \n32.0 \n39.3 \n53 SmoothQuant 4-6-16 \n15.7 \n31.6 \n34.3 \n43.4 \n42.3 \n37.2 \n56.7 \n54 LLM-QAT \n4-6-16 \n15.7 \n47.7 \n41.7 \n58.9 \n57.5 \n51.0 \n64.2 \n55 RTN \n4-8-16 \n15.7 \n50.9 \n44.0 \n62.8 \n61.3 \n54.2 \n67.1 \n56 SmoothQuant 4-8-16 \n15.7 \n35.6 \n36.2 \n48.6 \n45.7 \n40.8 \n58.5 \n57 LLM-QAT \n4-8-16 \n15.7 \n52.8 \n44.4 \n63.6 \n61.2 \n55.1 \n67.1 \n58 RTN \n8-8-4 \n30.7 \n26.1 \n27.6 \n28.6 \n29.0 \n27.6 \n30.2 \n59 SmoothQuant 8-8-4 \n30.7 \n27.9 \n29.1 \n31.7 \n33.1 \n30.1 \n38.9 \n60 LLM-QAT \n8-8-4 \n30.7 \n49.7 \n42.2 \n60.8 \n59.7 \n52.7 \n67.9 \n61 RTN \n8-8-8 \n30.7 \n55.6 \n45.8 \n66.3 \n63.4 \n57.5 \n70.4 \n62 SmoothQuant 8-8-8 \n30.7 \n56.0 \n46.0 \n67.3 \n64.1 \n58.0 \n70.2 \n63 LLM-QAT \n8-8-8 \n30.7 \n56.5 \n47.7 \n66.9 \n64.2 \n58.5 \n69.4 \n64 RTN \n8-8-16 \n30.7 \n56.3 \n45.6 \n66.8 \n63.7 \n57.8 \n70.3 \n65 SmoothQuant 8-8-16 \n30.7 \n56.0 \n46.7 \n67.5 \n63.8 \n58.2 \n70.3 \n66 LLM-QAT \n8-8-16 \n30.7 \n54.9 \n45.9 \n66.7 \n63.6 \n57.4 \n70.0 \n\n\nTable 8 :\n8The memory consumption of key-value cache (KV cache) for different sequence length. GB 0.13 GB 0.06 GB 0.39 GB 0.20 GB 0.10 GB 0.76 GB 0.38 GB 0.19 GB 2k 0.50 GB 0.25 GB 0.13 GB 0.78 GB 0.39 GB 0.20 GB 1.52 GB 0.76 GB 0.38 GB 4k 1.00 GB 0.50 GB 0.25 GB 1.56 GB 0.78 GB 0.39 GB 3.05 GB 1.52 GB 0.76 GB 8k 2.00 GB 1.00 GB 0.50 GB 3.13 GB 1.56 GB 0.78 GB 6.09 GB 3.05 GB 1.52 GB 16k 4.00 GB 2.00 GB 1.00 GB 6.25 GB 3.13 GB 1.56 GB 12.19 GB 6.09 GB 3.05 GB 32k 8.00 GB 4.00 GB 2.00 GB 12.50 GB 6.25 GB 3.13 GB 24.38 GB 12.19 GB 6.09 GBLLaMA-7B \nLLaMA-13B \nLLaMA-30B \nText Length 16-bit \n8-bit \n4-bit \n16-bit \n8-bit \n4-bit \n16-bit \n8-bit \n4-bit \n1k \n0.25 \nhttp://commoncrawl.org/\nConclusion and LimitationsWe proposed data-free quantization-aware training for LLMs and showed accurate, 4-bit quantization is possible using this technique. Given the generality of the training-data-agnostic distillation method, and the growing cost of LLM deployments, we expect our method to have wide applicability. For instance, the method could also be used for models trained in several stages, e.g. with instruction tuning or reinforcement learning(Ouyang et al., 2022). We leave this investigation to future work. Since 4-bit quantization does not have hardware support out-of-the-box, we haven't included hardware implementation as part of this work. However, we're working with partners to enable this in the near future. While our method works well for 4-bit weights, 4-bit KV cache and 8-bit activations, it isn't sufficient for 4-bit activation quantization. This case needs further research.\nBinarybert: Pushing the limit of bert quantization. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Irwin Michael R Lyu, King, ACL/IJCNLP. 2021Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. In ACL/IJCNLP (1), 2021.\n\nReasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432-7439, 2020.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n\nZeroq: A novel zero shot quantization framework. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13169-13178, 2020.\n\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, Parameterized clipping activation for quantized neural networks. arXiv e-prints. 1805Jungwook Choi, Zhuo Wang, Swagath Venkataramani, et al. Pact: Parameterized clipping activation for quantized neural networks. arXiv e-prints, pp. arXiv-1805, 2018.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nBoolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.10044arXiv preprintChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.05457arXiv preprintPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8 (Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT (1). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), 2019.\n\nLearned step size quantization. K Steven, Jeffrey L Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, International Conference on Learning Representations. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S Modha. Learned step size quantization. In International Conference on Learning Representations, 2019.\n\nTraining with quantization noise for extreme model compression. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herve Jegou, Armand Joulin, arXiv:2004.07320arXiv preprintAngela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, arXiv:2009.03300Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprintDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n\nTraining compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556arXiv preprintJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\nQuantization and training of neural networks for efficient integer-arithmetic-only inference. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2704-2713, 2018.\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, S Daniel, Luke Weld, Zettlemoyer, arXiv:1705.03551arXiv preprintMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\n\nQuantizing deep convolutional networks for efficient inference: A whitepaper. Raghuraman Krishnamoorthi, arXiv:1806.08342arXiv preprintRaghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018.\n\nBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv preprintMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n\nOscillation-free quantization for low-bit vision transformers. Shih-Yang Liu, Zechun Liu, Kwang-Ting Cheng, ICML. Shih-Yang Liu, Zechun Liu, and Kwang-Ting Cheng. Oscillation-free quantization for low-bit vision transformers. In ICML, 2023.\n\nNonuniform-touniform quantization: Towards accurate quantization via generalized straight-through estimation. Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, Zhiqiang Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform-to- uniform quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4942-4952, 2022a.\n\nZechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, Yashar Mehdad, arXiv:2205.13016Bit: Robustly binarized multi-distilled transformer. arXiv preprintZechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoor- thi, and Yashar Mehdad. Bit: Robustly binarized multi-distilled transformer. arXiv preprint arXiv:2205.13016, 2022b.\n\nData-free neural architecture search via recursive label calibration. Zechun Liu, Zhiqiang Shen, Yun Long, Eric Xing, Kwang-Ting Cheng, Chas Leichner, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part XXIVZechun Liu, Zhiqiang Shen, Yun Long, Eric Xing, Kwang-Ting Cheng, and Chas Leichner. Data-free neural architecture search via recursive label calibration. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIV, pp. 391-406. Springer, 2022c.\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843arXiv preprintStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, arXiv:1809.02789arXiv preprintTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022.\n\nBibert: Accurate fully binarized bert. Haotong Qin, Yifu Ding, Mingyuan Zhang, Yan Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, Xianglong Liu, International Conference on Learning Representations. Haotong Qin, Yifu Ding, Mingyuan Zhang, YAN Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xianglong Liu. Bibert: Accurate fully binarized bert. In International Conference on Learning Representations, 2021.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.\n\nWinogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 649Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.\n\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Lebras, Yejin Choi, Socialiqa, arXiv:1904.09728Commonsense reasoning about social interactions. arXiv preprintMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Matthias Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nQ-BERT: hessian based ultra low precision quantization of BERT. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, Kurt Keutzer, AAAI. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. Q-BERT: hessian based ultra low precision quantization of BERT. In AAAI, 2020a.\n\nQ-bert: Hessian based ultra low precision quantization of bert. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8815-8821, 2020b.\n\nHigh-throughput generative inference of large language models with a single gpu. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Y Daniel, Zhiqiang Fu, Beidi Xie, Clark Chen, Joseph E Barrett, Gonzalez, arXiv:2303.06865arXiv preprintYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.10438arXiv preprintGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, Advances in Neural Information Processing Systems. 35Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168-27183, 2022.\n\nDreaming to distill: Data-free knowledge transfer via deepinversion. Pavlo Hongxu Yin, Molchanov, M Jose, Zhizhong Alvarez, Arun Li, Derek Mallya, Hoiem, K Niraj, Jan Jha, Kautz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8715-8724, 2020.\n\nGobo: Quantizing attention-based nlp models for low latency and energy efficient inference. Ali Hadi Zadeh, Isak Edo, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEOmar Mohamed Awad, and Andreas MoshovosAli Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 811-824. IEEE, 2020.\n\nQ8bert: Quantized 8bit bert. Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat, 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEEOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 36-39. IEEE, 2019.\n\nHellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.07830arXiv preprintRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nTernarybert: Distillation-aware ultra-low bit BERT. Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu, EMNLP. Bonnie Webber, Trevor Cohn, Yulan He, and Yang LiuWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit BERT. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), EMNLP, 2020.\n\n2020), abbreviate for Colossal Clean Crawled Corpus. Common Crawl 2 is a publicly-available web archive that provides \"web extracted text\" by removing markup and other non-text content from the scraped HTML files. C4 filters Common Crawl's web-extracted text and produces a collection of. C4 (raffel, text that comprises clean and natural English textC4 (Raffel et al., 2020), abbreviate for Colossal Clean Crawled Corpus. Common Crawl 2 is a publicly-available web archive that provides \"web extracted text\" by removing markup and other non-text content from the scraped HTML files. C4 filters Common Crawl's web-extracted text and produces a collection of text that comprises clean and natural English text.\n", "annotations": {"author": "[{\"end\":249,\"start\":76},{\"end\":422,\"start\":250},{\"end\":574,\"start\":423},{\"end\":750,\"start\":575},{\"end\":906,\"start\":751},{\"end\":1063,\"start\":907},{\"end\":1215,\"start\":1064},{\"end\":1373,\"start\":1216},{\"end\":1525,\"start\":1374},{\"end\":1682,\"start\":1526},{\"end\":1852,\"start\":1683},{\"end\":2010,\"start\":1853}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":83},{\"end\":261,\"start\":257},{\"end\":430,\"start\":428},{\"end\":590,\"start\":586},{\"end\":762,\"start\":757},{\"end\":919,\"start\":914},{\"end\":1071,\"start\":1069},{\"end\":1229,\"start\":1223},{\"end\":1381,\"start\":1379},{\"end\":1538,\"start\":1535},{\"end\":1708,\"start\":1694},{\"end\":1866,\"start\":1859}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":256,\"start\":250},{\"end\":427,\"start\":423},{\"end\":585,\"start\":575},{\"end\":756,\"start\":751},{\"end\":913,\"start\":907},{\"end\":1068,\"start\":1064},{\"end\":1222,\"start\":1216},{\"end\":1378,\"start\":1374},{\"end\":1534,\"start\":1526},{\"end\":1693,\"start\":1683},{\"end\":1858,\"start\":1853}]", "author_affiliation": "[{\"end\":248,\"start\":107},{\"end\":421,\"start\":280},{\"end\":573,\"start\":432},{\"end\":749,\"start\":608},{\"end\":905,\"start\":764},{\"end\":1062,\"start\":921},{\"end\":1214,\"start\":1073},{\"end\":1372,\"start\":1231},{\"end\":1524,\"start\":1383},{\"end\":1681,\"start\":1540},{\"end\":1851,\"start\":1710},{\"end\":2009,\"start\":1868}]", "title": "[{\"end\":73,\"start\":1},{\"end\":2083,\"start\":2011}]", "venue": null, "abstract": "[{\"end\":4256,\"start\":2085}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4308,\"start\":4288},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4402,\"start\":4378},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4429,\"start\":4404},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4465,\"start\":4442},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4498,\"start\":4476},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5039,\"start\":5020},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5056,\"start\":5039},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6655,\"start\":6634},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11617,\"start\":11597},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11636,\"start\":11617},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12082,\"start\":12059},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12100,\"start\":12082},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13868,\"start\":13848},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15886,\"start\":15866},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15912,\"start\":15893},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15937,\"start\":15919},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15971,\"start\":15949},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16008,\"start\":15984},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16048,\"start\":16025},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16122,\"start\":16102},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16156,\"start\":16132},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16230,\"start\":16209},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16259,\"start\":16238},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16515,\"start\":16488},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16919,\"start\":16897},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16955,\"start\":16936},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18723,\"start\":18703},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18749,\"start\":18728},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18895,\"start\":18874},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19242,\"start\":19221},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20549,\"start\":20531},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20568,\"start\":20549},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20603,\"start\":20584},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22349,\"start\":22330},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22886,\"start\":22866},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22907,\"start\":22886},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22986,\"start\":22967},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23012,\"start\":22993},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23219,\"start\":23198},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23236,\"start\":23219},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23255,\"start\":23236},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23274,\"start\":23255},{\"end\":23291,\"start\":23274},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23308,\"start\":23291},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23326,\"start\":23308},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23358,\"start\":23337},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23387,\"start\":23367},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23526,\"start\":23507},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23543,\"start\":23526},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23564,\"start\":23543},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24018,\"start\":24000},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24035,\"start\":24018},{\"end\":24611,\"start\":24583},{\"end\":24644,\"start\":24616},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25181,\"start\":25162},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25413,\"start\":25394},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25597,\"start\":25579},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25757,\"start\":25736},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26010,\"start\":25987},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26561,\"start\":26538},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26817,\"start\":26798},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27099,\"start\":27075},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27185,\"start\":27162},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27366,\"start\":27346},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29229,\"start\":29209}]", "figure": "[{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29155,\"start\":27487},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31168,\"start\":29156},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32163,\"start\":31169},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":33198,\"start\":32164},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34391,\"start\":33199},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":38664,\"start\":34392},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":39327,\"start\":38665}]", "paragraph": "[{\"end\":4913,\"start\":4272},{\"end\":5848,\"start\":4915},{\"end\":6078,\"start\":5850},{\"end\":7395,\"start\":6080},{\"end\":8015,\"start\":7397},{\"end\":8729,\"start\":8017},{\"end\":9298,\"start\":8756},{\"end\":10298,\"start\":9300},{\"end\":10616,\"start\":10346},{\"end\":10688,\"start\":10618},{\"end\":11106,\"start\":10823},{\"end\":11316,\"start\":11108},{\"end\":11749,\"start\":11433},{\"end\":11906,\"start\":11804},{\"end\":13059,\"start\":11949},{\"end\":13485,\"start\":13112},{\"end\":14553,\"start\":13487},{\"end\":14720,\"start\":14555},{\"end\":15006,\"start\":14777},{\"end\":15616,\"start\":15008},{\"end\":16269,\"start\":15632},{\"end\":16790,\"start\":16295},{\"end\":17506,\"start\":16807},{\"end\":18461,\"start\":17542},{\"end\":18821,\"start\":18474},{\"end\":19503,\"start\":18837},{\"end\":20377,\"start\":19505},{\"end\":21088,\"start\":20403},{\"end\":22171,\"start\":21090},{\"end\":22657,\"start\":22231},{\"end\":23827,\"start\":22675},{\"end\":24464,\"start\":23829},{\"end\":24658,\"start\":24479},{\"end\":25080,\"start\":24697},{\"end\":25154,\"start\":25110},{\"end\":25387,\"start\":25156},{\"end\":25572,\"start\":25389},{\"end\":25724,\"start\":25574},{\"end\":25974,\"start\":25726},{\"end\":26241,\"start\":25976},{\"end\":26531,\"start\":26243},{\"end\":26764,\"start\":26533},{\"end\":27068,\"start\":26789},{\"end\":27311,\"start\":27070},{\"end\":27486,\"start\":27336}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10800,\"start\":10689},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11432,\"start\":11317},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11803,\"start\":11750},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13111,\"start\":13060},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14776,\"start\":14721},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17541,\"start\":17507}]", "table_ref": "[{\"end\":18847,\"start\":18840},{\"end\":19885,\"start\":19878},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":20488,\"start\":20481},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":21425,\"start\":21418},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":22358,\"start\":22351},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":24518,\"start\":24511},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":24842,\"start\":24835}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4270,\"start\":4258},{\"attributes\":{\"n\":\"2.1\"},\"end\":8754,\"start\":8732},{\"attributes\":{\"n\":\"2.2\"},\"end\":10328,\"start\":10301},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":10344,\"start\":10331},{\"end\":10821,\"start\":10802},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":11947,\"start\":11909},{\"attributes\":{\"n\":\"3\"},\"end\":15630,\"start\":15619},{\"attributes\":{\"n\":\"3.1\"},\"end\":16293,\"start\":16272},{\"attributes\":{\"n\":\"3.2\"},\"end\":16805,\"start\":16793},{\"attributes\":{\"n\":\"3.3\"},\"end\":18472,\"start\":18464},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":18835,\"start\":18824},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":20401,\"start\":20380},{\"attributes\":{\"n\":\"3.3.3\"},\"end\":22196,\"start\":22174},{\"attributes\":{\"n\":\"3.4\"},\"end\":22229,\"start\":22199},{\"attributes\":{\"n\":\"4\"},\"end\":22673,\"start\":22660},{\"end\":24477,\"start\":24467},{\"end\":24695,\"start\":24661},{\"end\":25108,\"start\":25083},{\"end\":26787,\"start\":26767},{\"end\":27334,\"start\":27314},{\"end\":27497,\"start\":27488},{\"end\":29166,\"start\":29157},{\"end\":31179,\"start\":31170},{\"end\":32174,\"start\":32165},{\"end\":33209,\"start\":33200},{\"end\":34402,\"start\":34393},{\"end\":38675,\"start\":38666}]", "table": "[{\"end\":29155,\"start\":29070},{\"end\":31168,\"start\":29260},{\"end\":32163,\"start\":31325},{\"end\":33198,\"start\":32335},{\"end\":34391,\"start\":33279},{\"end\":38664,\"start\":34404},{\"end\":39327,\"start\":39208}]", "figure_caption": "[{\"end\":29070,\"start\":27499},{\"end\":29260,\"start\":29168},{\"end\":31325,\"start\":31181},{\"end\":32335,\"start\":32176},{\"end\":33279,\"start\":33211},{\"end\":39208,\"start\":38677}]", "figure_ref": "[{\"end\":5596,\"start\":5588},{\"end\":8447,\"start\":8439},{\"end\":8968,\"start\":8960},{\"end\":10843,\"start\":10835},{\"end\":12028,\"start\":12020},{\"end\":13365,\"start\":13357},{\"end\":14052,\"start\":14044},{\"end\":14418,\"start\":14410}]", "bib_author_first_name": "[{\"end\":40317,\"start\":40312},{\"end\":40326,\"start\":40323},{\"end\":40336,\"start\":40334},{\"end\":40348,\"start\":40342},{\"end\":40359,\"start\":40356},{\"end\":40368,\"start\":40365},{\"end\":40379,\"start\":40376},{\"end\":40390,\"start\":40385},{\"end\":40675,\"start\":40668},{\"end\":40687,\"start\":40682},{\"end\":40705,\"start\":40697},{\"end\":40716,\"start\":40711},{\"end\":41101,\"start\":41098},{\"end\":41117,\"start\":41109},{\"end\":41128,\"start\":41124},{\"end\":41143,\"start\":41136},{\"end\":41158,\"start\":41153},{\"end\":41160,\"start\":41159},{\"end\":41177,\"start\":41169},{\"end\":41194,\"start\":41188},{\"end\":41214,\"start\":41208},{\"end\":41228,\"start\":41222},{\"end\":41243,\"start\":41237},{\"end\":41632,\"start\":41626},{\"end\":41644,\"start\":41638},{\"end\":41654,\"start\":41650},{\"end\":41665,\"start\":41661},{\"end\":41676,\"start\":41675},{\"end\":41690,\"start\":41686},{\"end\":42111,\"start\":42103},{\"end\":42122,\"start\":42118},{\"end\":42136,\"start\":42129},{\"end\":42412,\"start\":42403},{\"end\":42430,\"start\":42424},{\"end\":42444,\"start\":42439},{\"end\":42460,\"start\":42453},{\"end\":42474,\"start\":42468},{\"end\":42487,\"start\":42483},{\"end\":42501,\"start\":42497},{\"end\":42528,\"start\":42521},{\"end\":42545,\"start\":42536},{\"end\":42972,\"start\":42961},{\"end\":42986,\"start\":42980},{\"end\":43000,\"start\":42992},{\"end\":43011,\"start\":43008},{\"end\":43032,\"start\":43025},{\"end\":43050,\"start\":43042},{\"end\":43394,\"start\":43389},{\"end\":43407,\"start\":43402},{\"end\":43420,\"start\":43416},{\"end\":43436,\"start\":43430},{\"end\":43449,\"start\":43443},{\"end\":43468,\"start\":43461},{\"end\":43486,\"start\":43480},{\"end\":43760,\"start\":43757},{\"end\":43775,\"start\":43771},{\"end\":43789,\"start\":43783},{\"end\":43803,\"start\":43799},{\"end\":44168,\"start\":44163},{\"end\":44185,\"start\":44177},{\"end\":44199,\"start\":44193},{\"end\":44213,\"start\":44205},{\"end\":44446,\"start\":44445},{\"end\":44462,\"start\":44455},{\"end\":44464,\"start\":44463},{\"end\":44479,\"start\":44472},{\"end\":44503,\"start\":44491},{\"end\":44525,\"start\":44513},{\"end\":44868,\"start\":44862},{\"end\":44880,\"start\":44874},{\"end\":44896,\"start\":44888},{\"end\":44912,\"start\":44905},{\"end\":44924,\"start\":44920},{\"end\":44941,\"start\":44936},{\"end\":44955,\"start\":44949},{\"end\":45292,\"start\":45287},{\"end\":45309,\"start\":45302},{\"end\":45329,\"start\":45326},{\"end\":45571,\"start\":45568},{\"end\":45589,\"start\":45583},{\"end\":45603,\"start\":45597},{\"end\":45616,\"start\":45612},{\"end\":46002,\"start\":45996},{\"end\":46022,\"start\":46013},{\"end\":46039,\"start\":46033},{\"end\":46053,\"start\":46048},{\"end\":46073,\"start\":46067},{\"end\":46084,\"start\":46079},{\"end\":46102,\"start\":46097},{\"end\":46115,\"start\":46111},{\"end\":46120,\"start\":46116},{\"end\":46136,\"start\":46128},{\"end\":46153,\"start\":46148},{\"end\":46564,\"start\":46558},{\"end\":46582,\"start\":46572},{\"end\":46593,\"start\":46591},{\"end\":46608,\"start\":46600},{\"end\":46621,\"start\":46614},{\"end\":46634,\"start\":46628},{\"end\":46650,\"start\":46643},{\"end\":46663,\"start\":46657},{\"end\":47237,\"start\":47231},{\"end\":47251,\"start\":47245},{\"end\":47259,\"start\":47258},{\"end\":47272,\"start\":47268},{\"end\":47604,\"start\":47594},{\"end\":47915,\"start\":47911},{\"end\":47929,\"start\":47923},{\"end\":47940,\"start\":47935},{\"end\":47954,\"start\":47948},{\"end\":47981,\"start\":47970},{\"end\":47995,\"start\":47991},{\"end\":48005,\"start\":48002},{\"end\":48020,\"start\":48016},{\"end\":48418,\"start\":48409},{\"end\":48430,\"start\":48424},{\"end\":48446,\"start\":48436},{\"end\":48704,\"start\":48698},{\"end\":48720,\"start\":48710},{\"end\":48732,\"start\":48728},{\"end\":48744,\"start\":48740},{\"end\":48746,\"start\":48745},{\"end\":48761,\"start\":48753},{\"end\":49218,\"start\":49212},{\"end\":49230,\"start\":49224},{\"end\":49243,\"start\":49237},{\"end\":49254,\"start\":49251},{\"end\":49266,\"start\":49261},{\"end\":49276,\"start\":49272},{\"end\":49291,\"start\":49281},{\"end\":49314,\"start\":49308},{\"end\":49693,\"start\":49687},{\"end\":49707,\"start\":49699},{\"end\":49717,\"start\":49714},{\"end\":49728,\"start\":49724},{\"end\":49745,\"start\":49735},{\"end\":49757,\"start\":49753},{\"end\":50178,\"start\":50174},{\"end\":50196,\"start\":50191},{\"end\":50427,\"start\":50420},{\"end\":50443,\"start\":50436},{\"end\":50456,\"start\":50451},{\"end\":50474,\"start\":50467},{\"end\":50747,\"start\":50742},{\"end\":50763,\"start\":50758},{\"end\":50777,\"start\":50771},{\"end\":50790,\"start\":50784},{\"end\":51098,\"start\":51094},{\"end\":51114,\"start\":51107},{\"end\":51121,\"start\":51119},{\"end\":51134,\"start\":51129},{\"end\":51151,\"start\":51144},{\"end\":51170,\"start\":51164},{\"end\":51185,\"start\":51180},{\"end\":51201,\"start\":51193},{\"end\":51219,\"start\":51211},{\"end\":51231,\"start\":51227},{\"end\":51630,\"start\":51623},{\"end\":51640,\"start\":51636},{\"end\":51655,\"start\":51647},{\"end\":51666,\"start\":51663},{\"end\":51682,\"start\":51676},{\"end\":51696,\"start\":51688},{\"end\":51708,\"start\":51703},{\"end\":51723,\"start\":51714},{\"end\":52084,\"start\":52079},{\"end\":52097,\"start\":52093},{\"end\":52111,\"start\":52107},{\"end\":52130,\"start\":52121},{\"end\":52142,\"start\":52136},{\"end\":52158,\"start\":52151},{\"end\":52172,\"start\":52167},{\"end\":52182,\"start\":52179},{\"end\":52194,\"start\":52187},{\"end\":52589,\"start\":52582},{\"end\":52603,\"start\":52601},{\"end\":52618,\"start\":52611},{\"end\":52630,\"start\":52625},{\"end\":52869,\"start\":52862},{\"end\":52881,\"start\":52875},{\"end\":52896,\"start\":52891},{\"end\":52908,\"start\":52903},{\"end\":52922,\"start\":52917},{\"end\":53196,\"start\":53190},{\"end\":53223,\"start\":53212},{\"end\":53234,\"start\":53229},{\"end\":53248,\"start\":53242},{\"end\":53264,\"start\":53258},{\"end\":53276,\"start\":53271},{\"end\":53295,\"start\":53286},{\"end\":53301,\"start\":53296},{\"end\":53320,\"start\":53312},{\"end\":53339,\"start\":53331},{\"end\":53786,\"start\":53781},{\"end\":53797,\"start\":53793},{\"end\":53809,\"start\":53804},{\"end\":53821,\"start\":53814},{\"end\":53832,\"start\":53826},{\"end\":53842,\"start\":53838},{\"end\":53859,\"start\":53852},{\"end\":53861,\"start\":53860},{\"end\":53875,\"start\":53871},{\"end\":54150,\"start\":54145},{\"end\":54161,\"start\":54157},{\"end\":54173,\"start\":54168},{\"end\":54185,\"start\":54178},{\"end\":54196,\"start\":54190},{\"end\":54206,\"start\":54202},{\"end\":54217,\"start\":54216},{\"end\":54231,\"start\":54227},{\"end\":54718,\"start\":54714},{\"end\":54733,\"start\":54726},{\"end\":54748,\"start\":54741},{\"end\":54762,\"start\":54755},{\"end\":54770,\"start\":54767},{\"end\":54782,\"start\":54781},{\"end\":54799,\"start\":54791},{\"end\":54809,\"start\":54804},{\"end\":54820,\"start\":54815},{\"end\":54833,\"start\":54827},{\"end\":54835,\"start\":54834},{\"end\":55157,\"start\":55153},{\"end\":55174,\"start\":55167},{\"end\":55190,\"start\":55183},{\"end\":55206,\"start\":55200},{\"end\":55227,\"start\":55217},{\"end\":55245,\"start\":55237},{\"end\":55260,\"start\":55255},{\"end\":55283,\"start\":55279},{\"end\":55297,\"start\":55291},{\"end\":55750,\"start\":55741},{\"end\":55759,\"start\":55757},{\"end\":55772,\"start\":55765},{\"end\":55787,\"start\":55781},{\"end\":55801,\"start\":55797},{\"end\":56136,\"start\":56130},{\"end\":56146,\"start\":56142},{\"end\":56154,\"start\":56147},{\"end\":56172,\"start\":56166},{\"end\":56187,\"start\":56180},{\"end\":56200,\"start\":56192},{\"end\":56212,\"start\":56205},{\"end\":56602,\"start\":56597},{\"end\":56627,\"start\":56626},{\"end\":56642,\"start\":56634},{\"end\":56656,\"start\":56652},{\"end\":56666,\"start\":56661},{\"end\":56683,\"start\":56682},{\"end\":56694,\"start\":56691},{\"end\":57240,\"start\":57237},{\"end\":57257,\"start\":57253},{\"end\":57687,\"start\":57683},{\"end\":57699,\"start\":57696},{\"end\":57715,\"start\":57710},{\"end\":57728,\"start\":57723},{\"end\":58143,\"start\":58138},{\"end\":58156,\"start\":58153},{\"end\":58174,\"start\":58167},{\"end\":58184,\"start\":58181},{\"end\":58199,\"start\":58194},{\"end\":58407,\"start\":58402},{\"end\":58422,\"start\":58415},{\"end\":58436,\"start\":58431},{\"end\":58449,\"start\":58444},{\"end\":58463,\"start\":58459},{\"end\":58477,\"start\":58470},{\"end\":58495,\"start\":58484},{\"end\":58507,\"start\":58503},{\"end\":58518,\"start\":58514},{\"end\":58525,\"start\":58523},{\"end\":58907,\"start\":58904},{\"end\":58917,\"start\":58915},{\"end\":58929,\"start\":58923},{\"end\":58941,\"start\":58935},{\"end\":58953,\"start\":58949},{\"end\":58963,\"start\":58960},{\"end\":58974,\"start\":58971}]", "bib_author_last_name": "[{\"end\":40321,\"start\":40318},{\"end\":40332,\"start\":40327},{\"end\":40340,\"start\":40337},{\"end\":40354,\"start\":40349},{\"end\":40363,\"start\":40360},{\"end\":40374,\"start\":40369},{\"end\":40383,\"start\":40380},{\"end\":40404,\"start\":40391},{\"end\":40410,\"start\":40406},{\"end\":40680,\"start\":40676},{\"end\":40695,\"start\":40688},{\"end\":40709,\"start\":40706},{\"end\":40721,\"start\":40717},{\"end\":41107,\"start\":41102},{\"end\":41122,\"start\":41118},{\"end\":41134,\"start\":41129},{\"end\":41151,\"start\":41144},{\"end\":41167,\"start\":41161},{\"end\":41186,\"start\":41178},{\"end\":41206,\"start\":41195},{\"end\":41220,\"start\":41215},{\"end\":41235,\"start\":41229},{\"end\":41250,\"start\":41244},{\"end\":41636,\"start\":41633},{\"end\":41648,\"start\":41645},{\"end\":41659,\"start\":41655},{\"end\":41673,\"start\":41666},{\"end\":41684,\"start\":41677},{\"end\":41698,\"start\":41691},{\"end\":41707,\"start\":41700},{\"end\":42116,\"start\":42112},{\"end\":42127,\"start\":42123},{\"end\":42150,\"start\":42137},{\"end\":42422,\"start\":42413},{\"end\":42437,\"start\":42431},{\"end\":42451,\"start\":42445},{\"end\":42466,\"start\":42461},{\"end\":42481,\"start\":42475},{\"end\":42495,\"start\":42488},{\"end\":42508,\"start\":42502},{\"end\":42519,\"start\":42510},{\"end\":42534,\"start\":42529},{\"end\":42552,\"start\":42546},{\"end\":42562,\"start\":42554},{\"end\":42978,\"start\":42973},{\"end\":42990,\"start\":42987},{\"end\":43006,\"start\":43001},{\"end\":43023,\"start\":43012},{\"end\":43040,\"start\":43033},{\"end\":43060,\"start\":43051},{\"end\":43400,\"start\":43395},{\"end\":43414,\"start\":43408},{\"end\":43428,\"start\":43421},{\"end\":43441,\"start\":43437},{\"end\":43459,\"start\":43450},{\"end\":43478,\"start\":43469},{\"end\":43494,\"start\":43487},{\"end\":43769,\"start\":43761},{\"end\":43781,\"start\":43776},{\"end\":43797,\"start\":43790},{\"end\":43815,\"start\":43804},{\"end\":44175,\"start\":44169},{\"end\":44191,\"start\":44186},{\"end\":44203,\"start\":44200},{\"end\":44223,\"start\":44214},{\"end\":44453,\"start\":44447},{\"end\":44470,\"start\":44465},{\"end\":44489,\"start\":44480},{\"end\":44511,\"start\":44504},{\"end\":44535,\"start\":44526},{\"end\":44542,\"start\":44537},{\"end\":44872,\"start\":44869},{\"end\":44886,\"start\":44881},{\"end\":44903,\"start\":44897},{\"end\":44918,\"start\":44913},{\"end\":44934,\"start\":44925},{\"end\":44947,\"start\":44942},{\"end\":44962,\"start\":44956},{\"end\":45300,\"start\":45293},{\"end\":45324,\"start\":45310},{\"end\":45337,\"start\":45330},{\"end\":45347,\"start\":45339},{\"end\":45581,\"start\":45572},{\"end\":45595,\"start\":45590},{\"end\":45610,\"start\":45604},{\"end\":45620,\"start\":45617},{\"end\":46011,\"start\":46003},{\"end\":46031,\"start\":46023},{\"end\":46046,\"start\":46040},{\"end\":46065,\"start\":46054},{\"end\":46077,\"start\":46074},{\"end\":46095,\"start\":46085},{\"end\":46109,\"start\":46103},{\"end\":46126,\"start\":46121},{\"end\":46146,\"start\":46137},{\"end\":46159,\"start\":46154},{\"end\":46166,\"start\":46161},{\"end\":46570,\"start\":46565},{\"end\":46589,\"start\":46583},{\"end\":46598,\"start\":46594},{\"end\":46612,\"start\":46609},{\"end\":46626,\"start\":46622},{\"end\":46641,\"start\":46635},{\"end\":46655,\"start\":46651},{\"end\":46676,\"start\":46664},{\"end\":47243,\"start\":47238},{\"end\":47256,\"start\":47252},{\"end\":47266,\"start\":47260},{\"end\":47277,\"start\":47273},{\"end\":47290,\"start\":47279},{\"end\":47619,\"start\":47605},{\"end\":47921,\"start\":47916},{\"end\":47933,\"start\":47930},{\"end\":47946,\"start\":47941},{\"end\":47968,\"start\":47955},{\"end\":47989,\"start\":47982},{\"end\":48000,\"start\":47996},{\"end\":48014,\"start\":48006},{\"end\":48032,\"start\":48021},{\"end\":48422,\"start\":48419},{\"end\":48434,\"start\":48431},{\"end\":48452,\"start\":48447},{\"end\":48708,\"start\":48705},{\"end\":48726,\"start\":48721},{\"end\":48738,\"start\":48733},{\"end\":48751,\"start\":48747},{\"end\":48766,\"start\":48762},{\"end\":49222,\"start\":49219},{\"end\":49235,\"start\":49231},{\"end\":49249,\"start\":49244},{\"end\":49259,\"start\":49255},{\"end\":49270,\"start\":49267},{\"end\":49279,\"start\":49277},{\"end\":49306,\"start\":49292},{\"end\":49321,\"start\":49315},{\"end\":49697,\"start\":49694},{\"end\":49712,\"start\":49708},{\"end\":49722,\"start\":49718},{\"end\":49733,\"start\":49729},{\"end\":49751,\"start\":49746},{\"end\":49766,\"start\":49758},{\"end\":50189,\"start\":50179},{\"end\":50203,\"start\":50197},{\"end\":50434,\"start\":50428},{\"end\":50449,\"start\":50444},{\"end\":50465,\"start\":50457},{\"end\":50481,\"start\":50475},{\"end\":50756,\"start\":50748},{\"end\":50769,\"start\":50764},{\"end\":50782,\"start\":50778},{\"end\":50800,\"start\":50791},{\"end\":51105,\"start\":51099},{\"end\":51117,\"start\":51115},{\"end\":51127,\"start\":51122},{\"end\":51142,\"start\":51135},{\"end\":51162,\"start\":51152},{\"end\":51178,\"start\":51171},{\"end\":51191,\"start\":51186},{\"end\":51209,\"start\":51202},{\"end\":51225,\"start\":51220},{\"end\":51235,\"start\":51232},{\"end\":51634,\"start\":51631},{\"end\":51645,\"start\":51641},{\"end\":51661,\"start\":51656},{\"end\":51674,\"start\":51667},{\"end\":51686,\"start\":51683},{\"end\":51701,\"start\":51697},{\"end\":51712,\"start\":51709},{\"end\":51727,\"start\":51724},{\"end\":52091,\"start\":52085},{\"end\":52105,\"start\":52098},{\"end\":52119,\"start\":52112},{\"end\":52134,\"start\":52131},{\"end\":52149,\"start\":52143},{\"end\":52165,\"start\":52159},{\"end\":52177,\"start\":52173},{\"end\":52185,\"start\":52183},{\"end\":52198,\"start\":52195},{\"end\":52599,\"start\":52590},{\"end\":52609,\"start\":52604},{\"end\":52623,\"start\":52619},{\"end\":52642,\"start\":52631},{\"end\":52648,\"start\":52644},{\"end\":52873,\"start\":52870},{\"end\":52889,\"start\":52882},{\"end\":52901,\"start\":52897},{\"end\":52915,\"start\":52909},{\"end\":52927,\"start\":52923},{\"end\":52938,\"start\":52929},{\"end\":53210,\"start\":53197},{\"end\":53227,\"start\":53224},{\"end\":53240,\"start\":53235},{\"end\":53256,\"start\":53249},{\"end\":53269,\"start\":53265},{\"end\":53284,\"start\":53277},{\"end\":53310,\"start\":53302},{\"end\":53329,\"start\":53321},{\"end\":53344,\"start\":53340},{\"end\":53351,\"start\":53346},{\"end\":53791,\"start\":53787},{\"end\":53802,\"start\":53798},{\"end\":53812,\"start\":53810},{\"end\":53824,\"start\":53822},{\"end\":53836,\"start\":53833},{\"end\":53850,\"start\":53843},{\"end\":53869,\"start\":53862},{\"end\":53883,\"start\":53876},{\"end\":54155,\"start\":54151},{\"end\":54166,\"start\":54162},{\"end\":54176,\"start\":54174},{\"end\":54188,\"start\":54186},{\"end\":54200,\"start\":54197},{\"end\":54214,\"start\":54207},{\"end\":54225,\"start\":54218},{\"end\":54239,\"start\":54232},{\"end\":54248,\"start\":54241},{\"end\":54724,\"start\":54719},{\"end\":54739,\"start\":54734},{\"end\":54753,\"start\":54749},{\"end\":54765,\"start\":54763},{\"end\":54779,\"start\":54771},{\"end\":54789,\"start\":54783},{\"end\":54802,\"start\":54800},{\"end\":54813,\"start\":54810},{\"end\":54825,\"start\":54821},{\"end\":54843,\"start\":54836},{\"end\":54853,\"start\":54845},{\"end\":55165,\"start\":55158},{\"end\":55181,\"start\":55175},{\"end\":55198,\"start\":55191},{\"end\":55215,\"start\":55207},{\"end\":55235,\"start\":55228},{\"end\":55253,\"start\":55246},{\"end\":55277,\"start\":55261},{\"end\":55289,\"start\":55284},{\"end\":55304,\"start\":55298},{\"end\":55311,\"start\":55306},{\"end\":55755,\"start\":55751},{\"end\":55763,\"start\":55760},{\"end\":55779,\"start\":55773},{\"end\":55795,\"start\":55788},{\"end\":55805,\"start\":55802},{\"end\":56140,\"start\":56137},{\"end\":56164,\"start\":56155},{\"end\":56178,\"start\":56173},{\"end\":56190,\"start\":56188},{\"end\":56203,\"start\":56201},{\"end\":56215,\"start\":56213},{\"end\":56613,\"start\":56603},{\"end\":56624,\"start\":56615},{\"end\":56632,\"start\":56628},{\"end\":56650,\"start\":56643},{\"end\":56659,\"start\":56657},{\"end\":56673,\"start\":56667},{\"end\":56680,\"start\":56675},{\"end\":56689,\"start\":56684},{\"end\":56698,\"start\":56695},{\"end\":56705,\"start\":56700},{\"end\":57251,\"start\":57241},{\"end\":57261,\"start\":57258},{\"end\":57694,\"start\":57688},{\"end\":57708,\"start\":57700},{\"end\":57721,\"start\":57716},{\"end\":57739,\"start\":57729},{\"end\":58151,\"start\":58144},{\"end\":58165,\"start\":58157},{\"end\":58179,\"start\":58175},{\"end\":58192,\"start\":58185},{\"end\":58204,\"start\":58200},{\"end\":58413,\"start\":58408},{\"end\":58429,\"start\":58423},{\"end\":58442,\"start\":58437},{\"end\":58457,\"start\":58450},{\"end\":58468,\"start\":58464},{\"end\":58482,\"start\":58478},{\"end\":58501,\"start\":58496},{\"end\":58512,\"start\":58508},{\"end\":58521,\"start\":58519},{\"end\":58538,\"start\":58526},{\"end\":58913,\"start\":58908},{\"end\":58921,\"start\":58918},{\"end\":58933,\"start\":58930},{\"end\":58947,\"start\":58942},{\"end\":58958,\"start\":58954},{\"end\":58969,\"start\":58964},{\"end\":58978,\"start\":58975},{\"end\":59544,\"start\":59534}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":229923538},\"end\":40608,\"start\":40260},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208290939},\"end\":41057,\"start\":40610},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":41575,\"start\":41059},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":209531713},\"end\":42101,\"start\":41577},{\"attributes\":{\"id\":\"b4\"},\"end\":42401,\"start\":42103},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b5\"},\"end\":42887,\"start\":42403},{\"attributes\":{\"doi\":\"arXiv:1905.10044\",\"id\":\"b6\"},\"end\":43307,\"start\":42889},{\"attributes\":{\"doi\":\"arXiv:1803.05457\",\"id\":\"b7\"},\"end\":43755,\"start\":43309},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b8\"},\"end\":44079,\"start\":43757},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52967399},\"end\":44411,\"start\":44081},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":67788003},\"end\":44796,\"start\":44413},{\"attributes\":{\"doi\":\"arXiv:2004.07320\",\"id\":\"b11\"},\"end\":45202,\"start\":44798},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b12\"},\"end\":45566,\"start\":45204},{\"attributes\":{\"doi\":\"arXiv:2009.03300\",\"id\":\"b13\"},\"end\":45946,\"start\":45568},{\"attributes\":{\"doi\":\"arXiv:2203.15556\",\"id\":\"b14\"},\"end\":46462,\"start\":45948},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":39867659},\"end\":47139,\"start\":46464},{\"attributes\":{\"doi\":\"arXiv:1705.03551\",\"id\":\"b16\"},\"end\":47514,\"start\":47141},{\"attributes\":{\"doi\":\"arXiv:1806.08342\",\"id\":\"b17\"},\"end\":47794,\"start\":47516},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b18\"},\"end\":48344,\"start\":47796},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":256615191},\"end\":48586,\"start\":48346},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":244715141},\"end\":49210,\"start\":48588},{\"attributes\":{\"doi\":\"arXiv:2205.13016\",\"id\":\"b21\"},\"end\":49615,\"start\":49212},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":244896494},\"end\":50170,\"start\":49617},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b23\"},\"end\":50385,\"start\":50172},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b24\"},\"end\":50651,\"start\":50387},{\"attributes\":{\"doi\":\"arXiv:1809.02789\",\"id\":\"b25\"},\"end\":51023,\"start\":50653},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":246426909},\"end\":51582,\"start\":51025},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":247446635},\"end\":51994,\"start\":51584},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":204838007},\"end\":52517,\"start\":51996},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":198893658},\"end\":52860,\"start\":52519},{\"attributes\":{\"doi\":\"arXiv:1904.09728\",\"id\":\"b30\"},\"end\":53188,\"start\":52862},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b31\"},\"end\":53715,\"start\":53190},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202565587},\"end\":54079,\"start\":53717},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":202565587},\"end\":54631,\"start\":54081},{\"attributes\":{\"doi\":\"arXiv:2303.06865\",\"id\":\"b34\"},\"end\":55151,\"start\":54633},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b35\"},\"end\":55649,\"start\":55153},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b36\"},\"end\":56035,\"start\":55651},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":249395624},\"end\":56526,\"start\":56037},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":209405263},\"end\":57143,\"start\":56528},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":218571099},\"end\":57652,\"start\":57145},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":204509218},\"end\":58081,\"start\":57654},{\"attributes\":{\"doi\":\"arXiv:1905.07830\",\"id\":\"b41\"},\"end\":58400,\"start\":58083},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b42\"},\"end\":58850,\"start\":58402},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":221970445},\"end\":59243,\"start\":58852},{\"attributes\":{\"id\":\"b44\"},\"end\":59954,\"start\":59245}]", "bib_title": "[{\"end\":40310,\"start\":40260},{\"end\":40666,\"start\":40610},{\"end\":41096,\"start\":41059},{\"end\":41624,\"start\":41577},{\"end\":44161,\"start\":44081},{\"end\":44443,\"start\":44413},{\"end\":46556,\"start\":46464},{\"end\":48407,\"start\":48346},{\"end\":48696,\"start\":48588},{\"end\":49685,\"start\":49617},{\"end\":51092,\"start\":51025},{\"end\":51621,\"start\":51584},{\"end\":52077,\"start\":51996},{\"end\":52580,\"start\":52519},{\"end\":53779,\"start\":53717},{\"end\":54143,\"start\":54081},{\"end\":56128,\"start\":56037},{\"end\":56595,\"start\":56528},{\"end\":57235,\"start\":57145},{\"end\":57681,\"start\":57654},{\"end\":58902,\"start\":58852}]", "bib_author": "[{\"end\":40323,\"start\":40312},{\"end\":40334,\"start\":40323},{\"end\":40342,\"start\":40334},{\"end\":40356,\"start\":40342},{\"end\":40365,\"start\":40356},{\"end\":40376,\"start\":40365},{\"end\":40385,\"start\":40376},{\"end\":40406,\"start\":40385},{\"end\":40412,\"start\":40406},{\"end\":40682,\"start\":40668},{\"end\":40697,\"start\":40682},{\"end\":40711,\"start\":40697},{\"end\":40723,\"start\":40711},{\"end\":41109,\"start\":41098},{\"end\":41124,\"start\":41109},{\"end\":41136,\"start\":41124},{\"end\":41153,\"start\":41136},{\"end\":41169,\"start\":41153},{\"end\":41188,\"start\":41169},{\"end\":41208,\"start\":41188},{\"end\":41222,\"start\":41208},{\"end\":41237,\"start\":41222},{\"end\":41252,\"start\":41237},{\"end\":41638,\"start\":41626},{\"end\":41650,\"start\":41638},{\"end\":41661,\"start\":41650},{\"end\":41675,\"start\":41661},{\"end\":41686,\"start\":41675},{\"end\":41700,\"start\":41686},{\"end\":41709,\"start\":41700},{\"end\":42118,\"start\":42103},{\"end\":42129,\"start\":42118},{\"end\":42152,\"start\":42129},{\"end\":42424,\"start\":42403},{\"end\":42439,\"start\":42424},{\"end\":42453,\"start\":42439},{\"end\":42468,\"start\":42453},{\"end\":42483,\"start\":42468},{\"end\":42497,\"start\":42483},{\"end\":42510,\"start\":42497},{\"end\":42521,\"start\":42510},{\"end\":42536,\"start\":42521},{\"end\":42554,\"start\":42536},{\"end\":42564,\"start\":42554},{\"end\":42980,\"start\":42961},{\"end\":42992,\"start\":42980},{\"end\":43008,\"start\":42992},{\"end\":43025,\"start\":43008},{\"end\":43042,\"start\":43025},{\"end\":43062,\"start\":43042},{\"end\":43402,\"start\":43389},{\"end\":43416,\"start\":43402},{\"end\":43430,\"start\":43416},{\"end\":43443,\"start\":43430},{\"end\":43461,\"start\":43443},{\"end\":43480,\"start\":43461},{\"end\":43496,\"start\":43480},{\"end\":43771,\"start\":43757},{\"end\":43783,\"start\":43771},{\"end\":43799,\"start\":43783},{\"end\":43817,\"start\":43799},{\"end\":44177,\"start\":44163},{\"end\":44193,\"start\":44177},{\"end\":44205,\"start\":44193},{\"end\":44225,\"start\":44205},{\"end\":44455,\"start\":44445},{\"end\":44472,\"start\":44455},{\"end\":44491,\"start\":44472},{\"end\":44513,\"start\":44491},{\"end\":44537,\"start\":44513},{\"end\":44544,\"start\":44537},{\"end\":44874,\"start\":44862},{\"end\":44888,\"start\":44874},{\"end\":44905,\"start\":44888},{\"end\":44920,\"start\":44905},{\"end\":44936,\"start\":44920},{\"end\":44949,\"start\":44936},{\"end\":44964,\"start\":44949},{\"end\":45302,\"start\":45287},{\"end\":45326,\"start\":45302},{\"end\":45339,\"start\":45326},{\"end\":45349,\"start\":45339},{\"end\":45583,\"start\":45568},{\"end\":45597,\"start\":45583},{\"end\":45612,\"start\":45597},{\"end\":45622,\"start\":45612},{\"end\":46013,\"start\":45996},{\"end\":46033,\"start\":46013},{\"end\":46048,\"start\":46033},{\"end\":46067,\"start\":46048},{\"end\":46079,\"start\":46067},{\"end\":46097,\"start\":46079},{\"end\":46111,\"start\":46097},{\"end\":46128,\"start\":46111},{\"end\":46148,\"start\":46128},{\"end\":46161,\"start\":46148},{\"end\":46168,\"start\":46161},{\"end\":46572,\"start\":46558},{\"end\":46591,\"start\":46572},{\"end\":46600,\"start\":46591},{\"end\":46614,\"start\":46600},{\"end\":46628,\"start\":46614},{\"end\":46643,\"start\":46628},{\"end\":46657,\"start\":46643},{\"end\":46678,\"start\":46657},{\"end\":47245,\"start\":47231},{\"end\":47258,\"start\":47245},{\"end\":47268,\"start\":47258},{\"end\":47279,\"start\":47268},{\"end\":47292,\"start\":47279},{\"end\":47621,\"start\":47594},{\"end\":47923,\"start\":47911},{\"end\":47935,\"start\":47923},{\"end\":47948,\"start\":47935},{\"end\":47970,\"start\":47948},{\"end\":47991,\"start\":47970},{\"end\":48002,\"start\":47991},{\"end\":48016,\"start\":48002},{\"end\":48034,\"start\":48016},{\"end\":48424,\"start\":48409},{\"end\":48436,\"start\":48424},{\"end\":48454,\"start\":48436},{\"end\":48710,\"start\":48698},{\"end\":48728,\"start\":48710},{\"end\":48740,\"start\":48728},{\"end\":48753,\"start\":48740},{\"end\":48768,\"start\":48753},{\"end\":49224,\"start\":49212},{\"end\":49237,\"start\":49224},{\"end\":49251,\"start\":49237},{\"end\":49261,\"start\":49251},{\"end\":49272,\"start\":49261},{\"end\":49281,\"start\":49272},{\"end\":49308,\"start\":49281},{\"end\":49323,\"start\":49308},{\"end\":49699,\"start\":49687},{\"end\":49714,\"start\":49699},{\"end\":49724,\"start\":49714},{\"end\":49735,\"start\":49724},{\"end\":49753,\"start\":49735},{\"end\":49768,\"start\":49753},{\"end\":50191,\"start\":50174},{\"end\":50205,\"start\":50191},{\"end\":50436,\"start\":50420},{\"end\":50451,\"start\":50436},{\"end\":50467,\"start\":50451},{\"end\":50483,\"start\":50467},{\"end\":50758,\"start\":50742},{\"end\":50771,\"start\":50758},{\"end\":50784,\"start\":50771},{\"end\":50802,\"start\":50784},{\"end\":51107,\"start\":51094},{\"end\":51119,\"start\":51107},{\"end\":51129,\"start\":51119},{\"end\":51144,\"start\":51129},{\"end\":51164,\"start\":51144},{\"end\":51180,\"start\":51164},{\"end\":51193,\"start\":51180},{\"end\":51211,\"start\":51193},{\"end\":51227,\"start\":51211},{\"end\":51237,\"start\":51227},{\"end\":51636,\"start\":51623},{\"end\":51647,\"start\":51636},{\"end\":51663,\"start\":51647},{\"end\":51676,\"start\":51663},{\"end\":51688,\"start\":51676},{\"end\":51703,\"start\":51688},{\"end\":51714,\"start\":51703},{\"end\":51729,\"start\":51714},{\"end\":52093,\"start\":52079},{\"end\":52107,\"start\":52093},{\"end\":52121,\"start\":52107},{\"end\":52136,\"start\":52121},{\"end\":52151,\"start\":52136},{\"end\":52167,\"start\":52151},{\"end\":52179,\"start\":52167},{\"end\":52187,\"start\":52179},{\"end\":52200,\"start\":52187},{\"end\":52601,\"start\":52582},{\"end\":52611,\"start\":52601},{\"end\":52625,\"start\":52611},{\"end\":52644,\"start\":52625},{\"end\":52650,\"start\":52644},{\"end\":52875,\"start\":52862},{\"end\":52891,\"start\":52875},{\"end\":52903,\"start\":52891},{\"end\":52917,\"start\":52903},{\"end\":52929,\"start\":52917},{\"end\":52940,\"start\":52929},{\"end\":53212,\"start\":53190},{\"end\":53229,\"start\":53212},{\"end\":53242,\"start\":53229},{\"end\":53258,\"start\":53242},{\"end\":53271,\"start\":53258},{\"end\":53286,\"start\":53271},{\"end\":53312,\"start\":53286},{\"end\":53331,\"start\":53312},{\"end\":53346,\"start\":53331},{\"end\":53353,\"start\":53346},{\"end\":53793,\"start\":53781},{\"end\":53804,\"start\":53793},{\"end\":53814,\"start\":53804},{\"end\":53826,\"start\":53814},{\"end\":53838,\"start\":53826},{\"end\":53852,\"start\":53838},{\"end\":53871,\"start\":53852},{\"end\":53885,\"start\":53871},{\"end\":54157,\"start\":54145},{\"end\":54168,\"start\":54157},{\"end\":54178,\"start\":54168},{\"end\":54190,\"start\":54178},{\"end\":54202,\"start\":54190},{\"end\":54216,\"start\":54202},{\"end\":54227,\"start\":54216},{\"end\":54241,\"start\":54227},{\"end\":54250,\"start\":54241},{\"end\":54726,\"start\":54714},{\"end\":54741,\"start\":54726},{\"end\":54755,\"start\":54741},{\"end\":54767,\"start\":54755},{\"end\":54781,\"start\":54767},{\"end\":54791,\"start\":54781},{\"end\":54804,\"start\":54791},{\"end\":54815,\"start\":54804},{\"end\":54827,\"start\":54815},{\"end\":54845,\"start\":54827},{\"end\":54855,\"start\":54845},{\"end\":55167,\"start\":55153},{\"end\":55183,\"start\":55167},{\"end\":55200,\"start\":55183},{\"end\":55217,\"start\":55200},{\"end\":55237,\"start\":55217},{\"end\":55255,\"start\":55237},{\"end\":55279,\"start\":55255},{\"end\":55291,\"start\":55279},{\"end\":55306,\"start\":55291},{\"end\":55313,\"start\":55306},{\"end\":55757,\"start\":55741},{\"end\":55765,\"start\":55757},{\"end\":55781,\"start\":55765},{\"end\":55797,\"start\":55781},{\"end\":55807,\"start\":55797},{\"end\":56142,\"start\":56130},{\"end\":56166,\"start\":56142},{\"end\":56180,\"start\":56166},{\"end\":56192,\"start\":56180},{\"end\":56205,\"start\":56192},{\"end\":56217,\"start\":56205},{\"end\":56615,\"start\":56597},{\"end\":56626,\"start\":56615},{\"end\":56634,\"start\":56626},{\"end\":56652,\"start\":56634},{\"end\":56661,\"start\":56652},{\"end\":56675,\"start\":56661},{\"end\":56682,\"start\":56675},{\"end\":56691,\"start\":56682},{\"end\":56700,\"start\":56691},{\"end\":56707,\"start\":56700},{\"end\":57253,\"start\":57237},{\"end\":57263,\"start\":57253},{\"end\":57696,\"start\":57683},{\"end\":57710,\"start\":57696},{\"end\":57723,\"start\":57710},{\"end\":57741,\"start\":57723},{\"end\":58153,\"start\":58138},{\"end\":58167,\"start\":58153},{\"end\":58181,\"start\":58167},{\"end\":58194,\"start\":58181},{\"end\":58206,\"start\":58194},{\"end\":58415,\"start\":58402},{\"end\":58431,\"start\":58415},{\"end\":58444,\"start\":58431},{\"end\":58459,\"start\":58444},{\"end\":58470,\"start\":58459},{\"end\":58484,\"start\":58470},{\"end\":58503,\"start\":58484},{\"end\":58514,\"start\":58503},{\"end\":58523,\"start\":58514},{\"end\":58540,\"start\":58523},{\"end\":58915,\"start\":58904},{\"end\":58923,\"start\":58915},{\"end\":58935,\"start\":58923},{\"end\":58949,\"start\":58935},{\"end\":58960,\"start\":58949},{\"end\":58971,\"start\":58960},{\"end\":58980,\"start\":58971},{\"end\":59546,\"start\":59534}]", "bib_venue": "[{\"end\":40422,\"start\":40412},{\"end\":40784,\"start\":40723},{\"end\":41301,\"start\":41252},{\"end\":41790,\"start\":41709},{\"end\":42231,\"start\":42152},{\"end\":42619,\"start\":42580},{\"end\":42959,\"start\":42889},{\"end\":43387,\"start\":43309},{\"end\":43886,\"start\":43833},{\"end\":44238,\"start\":44225},{\"end\":44596,\"start\":44544},{\"end\":44860,\"start\":44798},{\"end\":45285,\"start\":45204},{\"end\":45737,\"start\":45638},{\"end\":45994,\"start\":45948},{\"end\":46755,\"start\":46678},{\"end\":47229,\"start\":47141},{\"end\":47592,\"start\":47516},{\"end\":47909,\"start\":47796},{\"end\":48458,\"start\":48454},{\"end\":48849,\"start\":48768},{\"end\":49390,\"start\":49339},{\"end\":49819,\"start\":49768},{\"end\":50418,\"start\":50387},{\"end\":50740,\"start\":50653},{\"end\":51286,\"start\":51237},{\"end\":51781,\"start\":51729},{\"end\":52240,\"start\":52200},{\"end\":52675,\"start\":52650},{\"end\":53003,\"start\":52956},{\"end\":53425,\"start\":53369},{\"end\":53889,\"start\":53885},{\"end\":54311,\"start\":54250},{\"end\":54712,\"start\":54633},{\"end\":55374,\"start\":55329},{\"end\":55739,\"start\":55651},{\"end\":56266,\"start\":56217},{\"end\":56788,\"start\":56707},{\"end\":57341,\"start\":57263},{\"end\":57849,\"start\":57741},{\"end\":58136,\"start\":58083},{\"end\":58600,\"start\":58556},{\"end\":58985,\"start\":58980},{\"end\":59532,\"start\":59245},{\"end\":40832,\"start\":40786},{\"end\":41858,\"start\":41792},{\"end\":46819,\"start\":46757},{\"end\":48917,\"start\":48851},{\"end\":49837,\"start\":49821},{\"end\":54359,\"start\":54313},{\"end\":56856,\"start\":56790}]"}}}, "year": 2023, "month": 12, "day": 17}