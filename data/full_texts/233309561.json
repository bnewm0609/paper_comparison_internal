{"id": 233309561, "updated": "2023-02-10 22:38:42.893", "metadata": {"title": "Prospective assessment of breast cancer risk from multimodal multiview ultrasound images via clinically applicable deep learning", "authors": "[{\"first\":\"Xuejun\",\"last\":\"Qian\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Pei\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Xinxin\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Chunguang\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Hanqi\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Weiwei\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"K.\",\"last\":\"Shung\",\"middle\":[\"Kirk\"]}]", "venue": "Nature Biomedical Engineering", "journal": "Nature Biomedical Engineering", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The clinical application of breast ultrasound for the assessment of cancer risk and of deep learning for the classification of breast-ultrasound images has been hindered by inter-grader variability and high false positive rates and by deep-learning models that do not follow Breast Imaging Reporting and Data System (BI-RADS) standards, lack explainability features and have not been tested prospectively. Here, we show that an explainable deep-learning system trained on 10,815 multimodal breast-ultrasound images of 721 biopsy-confirmed lesions from 634 patients across two hospitals and prospectively tested on 912 additional images of 152 lesions from 141 patients predicts BI-RADS scores for breast cancer as accurately as experienced radiologists, with areas under the receiver operating curve of 0.922 (95% confidence interval (CI)\u2009=\u20090.868\u20130.959) for bimodal images and 0.955 (95% CI\u2009=\u20090.909\u20130.982) for multimodal images. Multimodal multiview breast-ultrasound images augmented with heatmaps for malignancy risk predicted via deep learning may facilitate the adoption of ultrasound imaging in screening mammography workflows. An explainable deep-learning system prospectively predicts clinical scores for breast cancer risk from multimodal breast-ultrasound images as accurately as experienced radiologists.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "33875840", "pubmedcentral": null, "dblp": null, "doi": "10.1038/s41551-021-00711-2"}}, "content": {"source": {"pdf_hash": "0e225395d75aa737a16e37eca29428fa919018aa", "pdf_src": "SpringerNature", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "64668a2fca1529a83cd54ccd93326615e4f3b815", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0e225395d75aa737a16e37eca29428fa919018aa.txt", "contents": "\nProspective assessment of breast cancer risk from multimodal multiview ultrasound images via clinically applicable deep learning\n\n\nXuejun Qian xuejunqi@usc.edu \nJing Pei \nHui Zheng \u2709 \nXuejun Qian \nDepartment of Biomedical Engineering\nUniversity of Southern California\nLos AngelesCAUSA\n\nKeck School of Medicine\nUniversity of Southern California\nLos AngelesCAUSA\n\n\u2709 \nJing Pei \nDepartment of Breast Surgery\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nDepartment of General Surgery\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nHui Zheng \nDepartment of Ultrasound\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nXinxin Xie \nDepartment of Ultrasound\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nLin Yan \nSchool of Computer Science and Technology\nXidian University\nXi'anChina\n\nHao Zhang \nDepartment of Neurosurgery\nUniversity Hospital Heidelberg\nHeidelbergGermany\n\nChunguang Han \nXiang Gao \nDepartment of Breast Surgery\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nDepartment of General Surgery\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nDepartment of Electrical and Computer Engineering\nMing Hsieh\nUniversity of Southern California\nLos AngelesCAUSA\n\nHanqi Zhang \nDepartment of Ultrasound\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nWeiwei Zheng \nDepartment of Ultrasound\nXuancheng People's Hospital\nXuanchengChina\n\nQiang Sun \nDepartment of Breast Surgery\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nDepartment of General Surgery\nThe First Affiliated Hospital of Anhui Medical University\nHefeiChina\n\nLu Lu \nDepartment of Electrical and Computer Engineering\nMing Hsieh\nUniversity of Southern California\nLos AngelesCAUSA\n\nK Kirk Shung \nDepartment of Biomedical Engineering\nUniversity of Southern California\nLos AngelesCAUSA\n\nProspective assessment of breast cancer risk from multimodal multiview ultrasound images via clinically applicable deep learning\n10.1038/s41551-021-00711-2Articles 10 These authors contributed equally:\nB reast cancer is the most frequently diagnosed cancer and the second leading cause of cancer death among women worldwide 1 . The incidence of breast cancer is increasing, with over 1.6 million cases in 2010 and projections of 2.1 million by 2030 2,3 . Mammography is the recommended screening test to reduce breast cancer-related mortality. However, mammography has low sensitivity in dense breast parenchyma 4 and is not widely available in all countries, which may cause delayed diagnosis and worse outcomes 5 . Because of its low cost, wide availability and lack of ionizing radiation, the application of ultrasound (US) has been proposed to be expanded from the differentiation of cysts from solid masses to screening for breast cancer, especially for women with dense breasts 6 . The American College of Radiology published Breast Imaging Reporting and Data System (BI-RADS) lexicon guidelines for breast cancer screening, to standardize image interpretation by radiologists and dictate management recommendations. Despite improved consistency, subjective characterization of imaging findings and persistent intra-and inter-observer variability in medical image interpretation still remain as limitations 7,8 . In addition, there still exists a shortage of human experts to provide timely diagnosis and refer patients to the appropriate clinical care.Machine learning has been leveraged for many years in computer-aided diagnosis in various types of cancer 9 , including breast cancer 10 . Much of the previous work has focused on hand-engineered features, which involve computing explicit features selected by domain experts, resulting in algorithms designed for certain texture features or specific imaging modes. With the development of artificial intelligence (AI), deep convolutional neural networks-a special type of deep-learning technique 11 allowing an algorithm to learn the appropriate predictive features on the basis of imaging examples-have repeatedly been shown to be superior to hand-engineered features in the field of computer vision 12 . At the same time, deep learning has gained traction in advanced biomedical image analysis as a powerful tool to increase efficiency and reproducibility 13 . Recent advances in deep-learning studies have led to comparable sensitivity and specificity to readings by board-certified medical experts, as demonstrated through the use of hundreds of thousands of images[14][15][16][17]. Notably, deep-learning architectures have demonstrated ophthalmologist-level performance on optical coherence tomography images 14 and have achieved dermatologist-level classification of skin cancer 15 .Despite the fact that feasibility of applying deep learning on the classification of breast-US images has been demonstrated 18,19 and recently improved using ensemble 20,21 or modified 22,23 deep-learning architectures, different US modalities 24-26 or integration of BI-RADS features into the framework of AI 27,28 , these studies were designed to neither demonstrate real-world clinical relevance nor illuminate potential guidance to clinicians in making clinical decisions. In other words, the clinical applicability of this technology has remained unresolved due to four key challenges. First, the design of previous AI studies deviated from existing BI-RADS reporting standards 29 , resulting in potential issues relevant to clinical acceptance and diagnostic accuracy. In routine clinical workflows where either fourth-edition BI-RADS (via the assessment of US B-mode The clinical application of breast ultrasound for the assessment of cancer risk and of deep learning for the classification of breast-ultrasound images has been hindered by inter-grader variability and high false positive rates and by deep-learning models that do not follow Breast Imaging Reporting and Data System (BI-RADS) standards, lack explainability features and have not been tested prospectively. Here, we show that an explainable deep-learning system trained on 10,815 multimodal breast-ultrasound images of 721 biopsy-confirmed lesions from 634 patients across two hospitals and prospectively tested on 912 additional images of 152 lesions from 141 patients predicts BI-RADS scores for breast cancer as accurately as experienced radiologists, with areas under the receiver operating curve of 0.922 (95% confidence interval (CI) = 0.868-0.959) for bimodal images and 0.955 (95% CI = 0.909-0.982) for multimodal images. Multimodal multiview breast-ultrasound images augmented with heatmaps for malignancy risk predicted via deep learning may facilitate the adoption of ultrasound imaging in screening mammography workflows. Nature BiomediCaL eNGiNeeriNG | VOL 5 | JUNE 2021 | 522-532 | www.nature.com/natbiomedeng 522\n\nB reast cancer is the most frequently diagnosed cancer and the second leading cause of cancer death among women worldwide 1 . The incidence of breast cancer is increasing, with over 1.6 million cases in 2010 and projections of 2.1 million by 2030 2,3 . Mammography is the recommended screening test to reduce breast cancer-related mortality. However, mammography has low sensitivity in dense breast parenchyma 4 and is not widely available in all countries, which may cause delayed diagnosis and worse outcomes 5 . Because of its low cost, wide availability and lack of ionizing radiation, the application of ultrasound (US) has been proposed to be expanded from the differentiation of cysts from solid masses to screening for breast cancer, especially for women with dense breasts 6 . The American College of Radiology published Breast Imaging Reporting and Data System (BI-RADS) lexicon guidelines for breast cancer screening, to standardize image interpretation by radiologists and dictate management recommendations. Despite improved consistency, subjective characterization of imaging findings and persistent intra-and inter-observer variability in medical image interpretation still remain as limitations 7,8 . In addition, there still exists a shortage of human experts to provide timely diagnosis and refer patients to the appropriate clinical care.\n\nMachine learning has been leveraged for many years in computer-aided diagnosis in various types of cancer 9 , including breast cancer 10 . Much of the previous work has focused on hand-engineered features, which involve computing explicit features selected by domain experts, resulting in algorithms designed for certain texture features or specific imaging modes. With the development of artificial intelligence (AI), deep convolutional neural networks-a special type of deep-learning technique 11 allowing an algorithm to learn the appropriate predictive features on the basis of imaging examples-have repeatedly been shown to be superior to hand-engineered features in the field of computer vision 12 . At the same time, deep learning has gained traction in advanced biomedical image analysis as a powerful tool to increase efficiency and reproducibility 13 . Recent advances in deep-learning studies have led to comparable sensitivity and specificity to readings by board-certified medical experts, as demonstrated through the use of hundreds of thousands of images [14][15][16][17] . Notably, deep-learning architectures have demonstrated ophthalmologist-level performance on optical coherence tomography images 14 and have achieved dermatologist-level classification of skin cancer 15 .\n\nDespite the fact that feasibility of applying deep learning on the classification of breast-US images has been demonstrated 18,19 and recently improved using ensemble 20,21 or modified 22,23 deep-learning architectures, different US modalities [24][25][26] or integration of BI-RADS features into the framework of AI 27,28 , these studies were designed to neither demonstrate real-world clinical relevance nor illuminate potential guidance to clinicians in making clinical decisions. In other words, the clinical applicability of this technology has remained unresolved due to four key challenges. First, the design of previous AI studies deviated from existing BI-RADS reporting standards 29  and US colour Doppler images) or fifth-edition BI-RADS (via the assessment of US B-mode, US colour Doppler and US elastography images) lexicon is adopted, multimodal multiview US images should be considered as part of the design of AI tools as the workflow standard. Second, previous AI studies were not evaluated in a prospective setting and had small or even no human comparison groups; therefore, they were at high risk of bias 30 . To address this issue, a prospective setting and an appropriately large human sample for fair comparison with AI are essential for ensuring reliability. Third, to gain trust from clinicians, the inner workings of medical AI systems, which were opaque in former AI studies, should be understandable. Lastly, previous AI studies failed to demonstrate whether the AI system can improve and/or guide clinical outcomes such as clinicians' decision-making on BI-RADS categorization.\n\nTo move beyond the limitations of previous AI approaches and accelerate a broader adoption of deep-learning technology by clinicians, we aim to develop a clinically applicable AI system for automatic breast cancer risk prediction using a deep-learning architecture, which creates an opportunity to mimic the routine clinical workflow by utilizing multimodal (that is, US B-mode, US colour Doppler and US elastography) and multiview (that is, transverse and longitudinal) breast-US images. The most important contribution of this work is the enhancement of clinical applicability. Specifically, the performance of the AI system is compared with that of seven human experts in a setting of prospectively consecutive patients. In addition, we create numerical heatmaps that emphasize the importance of the feature map relevant to the predictions of the AI system. Such explainability features allow assessment of the region of interest (ROI), with potential clinical value for each imaging mode (that is, they guide the clinicians to investigate the corresponding regions in the original US images and then re-evaluate their clinical value). As a consequence, combining such auxiliary information with the original clinicians' assessments could potentially increase confidence levels of clinicians when making final informed decisions such as BI-RADS categorization and routine referral biopsy/follow-up suggestions. We demonstrate that AI systems have the potential to offer reliable diagnoses, good generalizability and efficient deployment, all of which will greatly improve routine breast-US examinations and facilitate their development in clinical settings.\n\n\nresults\n\nWith this work, we proposed a multipathway deep-learning architecture, as shown in Fig. 1. Deep-learning models to predict the risk of breast cancer were developed from a retrospectively collected development dataset consisting of 10,815 US images from 721 lesions of 634 patients who underwent breast-US examinations between October 2016 and December 2018. The retrospective workflow followed the BI-RADS guidelines (see Supplementary Table 1), including the inclusion and exclusion criteria, and an overview is depicted in Fig. 2. Out of the 721 lesions used, 556 lesions were benign and 165 lesions were malignant, based on biopsy-confirmed pathology results. Detailed patient demographics and breast lesion characteristics for the development dataset are summarized in Table 1. Lesions in the development dataset were randomly assigned to one of two sets: a training set (70%) and a validation set (30%).\n\nPerformance of deep-learning models. The multipathway deep convolutional neural network was trained using view-level multimodal US images and biopsy-confirmed labels ( Supplementary  Fig. 1). A convolution operator and SENet module 31 enabled the network to extract informative features through investigating both the spatial relationship and the channel relationship in each pathway (see Methods). Fully connected layers distilled meaningful representations to perform decision-making. To enhance the interpretability of the proposed deep-learning model on breast cancer risk prediction, features from the final convolutional layer in each pathway were extracted to generate a heatmap of each imaging modality via the Grad-CAM technique 32 , which can aid human experts to understand highlighting decisions made by the AI system.\n\nThe effectiveness of our proposed multipathway deep-learning model was validated using three different aspects. First, ResNet-18 combined with the SENet backbone showed better performance than basic ResNet-18, as shown in Supplementary Fig. 2a. Second, to find the most suitable base model for predicting the risk of breast cancer, we compared our backbone network with other prevalent models, including VGG19 (ref. 33 ), ResNet-50 (ref. 34 ) and Inception-v3 (ref. 35 ), all of which were integrated with the SENet block to ensure fair comparison. As displayed in Supplementary  Fig. 2b, our model maintained the best performance with less complexity (for example, a complicated training model may cause overfitting in the condition of insufficient training data size) compared with VGG19 with SENet, ResNet-50 with SENet and Inception-v3 with SENet. Lastly, in Supplementary Fig. 2c, we show the efficacy of our proposed model via a cross-validation strategy on the development dataset.\n\nTwo deep-learning models were involved in our AI systemnamely, a bimodal model (inputs with US B-mode and US colour Doppler images) and a multimodal model (inputs with US B-mode, US colour Doppler and US elastography images). As indicated in Supplementary Fig. 3, the bimodal and multimodal models were designed to mimic the routine clinical workflow of clinicians who use fourth-and fifth-edition BI-RADS lexicon, respectively. In the validation set, we evaluated the performance of the proposed AI system using view-level US images in terms of the area under the curve (AUC) of the receiver operating characteristic curve (ROC). The bimodal model achieved an AUC score of 0.877 (95% confidence interval (CI) = 0.830-0.914). With the additional US elastography information, the multimodal model attained a significantly better AUC score of 0.923 (95% CI = 0.883-0.953) with P < 0.05 ( Supplementary Fig. 4).\n\nTo strengthen the generalizability of the AI system, it must not only be used to evaluate retrospective data, but also it should be assessed in a prospective study setting 36,37 . Therefore, we used one prospective clinical test set amassed from two different hospitals.  Table 2 summarizes the statistical comparisons among various views and models. In terms of view-level US images, the performance of the AI system in the prospective test set was consistent with that in the retrospectively collected validation set, which demonstrates the generalizability of the AI system.\n\nTo mimic the workflow of radiologists who take multiview US images into consideration in clinical settings, we combined the malignancy risk probabilities from both views to produce an overall probability for the clinical test set, as a lesion-level US imaging evaluation. Varying the threshold of the overall probability in the interval 0-1 generated new ROCs of sensitivities and specificities. To summarize, the bimodal model attained an overall AUC of 0.922 (95% CI = 0.868-0.959). In comparison, the multimodal model fulfilled the best overall AUC of 0.955 (95% CI = 0.909-0.982), which was significantly higher than that of the bimodal model (P < 0.05), as shown in Fig. 3. On both the bimodal model and the multimodal model, the AUC evaluated on lesion-level US images showed better performance than that on view-level US images.\n\nFor comparison with radiologists on sensitivity and specificity, we varied the operating threshold to produce three different breast cancer risk scores (BCRSs)-namely, BCRS 4a+, 4b+ and 4c+. These BCRS modes corresponded to the decision-making modes of human experts in BI-RADS 3 versus 4a+, BI-RADS 3,4a versus 4b+ and BI-RADS 3,4a,4b versus 4c+, respectively (see Methods). At the BCRS 4a+ mode, the bimodal model achieved a sensitivity of 88.6%, which was higher than the 75.0 and 47.7% observed in the other two BCRS modes. The best specificity of 97.2% was obtained for the BCRS 4c+ mode while balanced sensitivity and specificity were achieved at the BCRS 4b+ mode (Supplementary Table 3). With respect to the multimodal model, improved sensitivity of breast-US lesion assessment without loss of specificity was observed at the BCRS 4b+ and 4c+ modes (Supplementary Table 4). However, at the BCRS 4a+ mode, the main diagnostic improvement was specificity rather than sensitivity.\n\nAI system performance compared with radiologists. We conducted a two-part reader study with seven experienced radiologists with a range of experience of 5-30 years (average = 14 years) on the same test set as a way to compare with the AI system. In part 1 of the reader study, readers determined BI-RADS scores for the bimodal multiview US images. We compared breast cancer risk prediction of the bimodal model with that of the readers in two ways. One way was to compare the bimodal model with each individual reader's performance by measuring the sensitivity and specificity at three BCRS modes. The AI system achieved significantly better specificity (P < 0.05 for six of the seven readers) at the BCRS 4a+ mode ( Fig. 3a and Supplementary Table 3). In other BCRS modes, the bimodal model's sensitivities and specificities showed comparable results with those of the readers. The other way was to compare the performance of the bimodal model with that of the average reader. Three averaged readers' assessments were located slightly below the bimodal model's ROC curve (Fig. 3a). Supplementary Table 5 lists the sensitivity and specificity results of the bimodal model and the averaged readers for three BCRS modes.\n\nIn part 2 of the reader study, in addition to bimodal US images, the multiview US elastography images were provided for review by the same readers. We performed the same comparison as in the previous reader study. On the assessment of multimodal multiview US images, the sensitivities and specificities of the multimodal model were on par with each reader's predictions, except for improved specificity (P < 0.05 for five of the seven readers) for the BCRS 4a+ mode ( Fig. 3b and Supplementary Table 4). Figure 3b shows that the AUC of the multimodal model outperformed or matched most individual readers, as well as the averaged readers, since the sensitivity-specificity points of readers lie below the ROC curve. Compared with evaluation on bimodal US images, reduction of false negatives and false positives in the assessment of multimodal US images was observed with the AI system and for most readers.\n\nA major challenge for clinicians in decision-making is to upgrade or downgrade BI-RADS 4 lesions since the error rate between BI-RADS 2, BI-RADS 3 and BI-RADS 5 lesions and biopsy results is minimal. In Fig. 3c,d, we have shown a comparison between the models and readers for the assessment of BI-RADS 4 lesions only in the clinical test set. The multimodal model achieved an AUC of 0.920 (95% CI = 0.840-0.969), indicating an improvement over the bimodal model, which had an AUC of 0.880 (95% CI = 0.790-0.940). Overall, the results from both the whole clinical test set and the subset of suspicious malignancy BI-RADS 4 lesions show that the AI system has reached the level of experienced radiologists with high reliability and accuracy, especially for the multimodal model.\n\n\nHeatmaps for understanding AI decision-making.\n\nTo further enhance the AI system's likelihood of clinical applicability, the decisions made by the AI system should be understandable to clinicians.\n\n\nCancer risk prediction model Clinical decision-making Lesion-level prospective US images\n\nStep 1\n\nStep 2\n\n\nBI-RADS lexicon\n\nAssist AI  Fig. 1). For each prospective clinical test lesion, the AI system utilizes one-view multimodal US images as inputs each time, evaluates the suspicious lesion from multiple views (that is, transverse and longitudinal views) and outputs an overall malignancy probability. Three different BCRSs were proposed in the AI system by varying the operating threshold to compare with and assist clinicians.\n\nTherefore, we incorporated a heatmap that highlights the important regions relevant to AI predictions to aid radiologists in their assessment of US images (see examples of AI prediction basis in Fig. 4).\n\nHeatmaps clearly facilitated assessment of the ROI of each US imaging mode with potential clinical value (see more heatmap interpretations with various lesion types in Supplementary Figs  Owing to the nature of retrospective investigation, multiview US images are not completely preserved and/or view descriptions are not clearly labelled in some lesions. To utilize the existing large multimodal US imaging dataset, the AI system was developed based on view-level multimodal US images (transverse or longitudinal view not distinguished). Lesion-level multimodal US images with explicitly labelled orthogonal views were collected in a prospective setting. It should be pointed out that all BI-RADS categories in this study were determined on US imaging exclusively. and 7). To be specific, there were three locations valuable for predicting the risk of breast cancer on US B-mode images (namely, the lesion margin, the echo pattern inside the lesion and the presence/ absence of calcifications). With regard to US colour Doppler images, the emphasis of the heatmaps focused on the locations of the vasculature, especially for malignant lesions with abundant angiogenesis. In regard to heatmaps derived from US elastography, we found two meaningful locations: a high stiffness region and a marginal region.\n\nTo some extent, the explainability features (that is, the heatmap used in this study) provided potential insight that could assist clinicians in their exploration and visualization of the suspicious lesions when there is inconsistency between the original assessment of clinicians and AI BCRS prediction. For the purpose of investigating the advantageous use of explainability features by radiologists, we designed another AI-assisted reader study using the original readers' BI-RADS scores in reader study part 2 as the baseline for each reader (Fig. 5). In addition to the original multimodal multiview US images, corresponding heatmaps and AI predictions of malignancy risk probability were both presented to the same readers to help them understand the justification of the AI system. According to the feedback of the readers, the AI system could potentially guide them to make a better clinical BI-RADS decision. Specifically, if the AI BCRS prediction is in line with the clinician's original assessment, it would increase the confidence levels of clinicians for making BI-RADS decisions. Otherwise, if discrepancy exists, the heatmaps could aid clinicians in their understanding of the basis of the AI prediction decision, then guide them in analysing features of the respective highlighted regions. Table 2 shows each reader's BI-RADS adjustments (no change, upgrade or downgrade) with the assistance of our AI system. After re-evaluating the updated BI-RADS categorization, we found that the readers preferred to downgrade more BI-RADS categories in benign lesions and upgrade more BI-RADS categories in malignant lesions, in accordance with our expectations. Moreover, with respect to the optimization of binary decision on follow-up/biopsy (that is, BI-RADS 3 versus 4a+), we observed that three more biopsies on average were avoided from 108 benign lesions and one more biopsy on average was recommended from 44 malignant lesions.\n\n\ndiscussion\n\nA lack of consensus (that is, variability in inter-and intra-reader reproducibility) and high false positive rates 6 in breast-US examinations have been widely recognized; thus, both clinicians and engineers have put a substantial amount of effort towards improving the diagnostic output of breast-US examinations. An AI system that combines high sensitivity and specificity with consistency of interpretation could serve as an assistance for clinicians. Here, we developed a deep-learning-based breast cancer risk prediction AI system using over 10,000 US images, including US B-mode, US colour Doppler and US elastography images. Such an AI system using multimodal multiview US images closely matches routine clinical breast-US scanning and decision-making. It is worth noting that US elastography is a very recently developed technique that is not yet widely available in existing US scanning machines in the clinic. To make the AI system more applicable to clinical use, our proposed AI system incorporates both bimodal and multimodal models, and therefore has the ability to meet diagnostic requirements in accordance with either the fourth-or fifth-edition BI-RADS lexicon in clinical settings. We report that our AI system can be used to assess breast lesions at a level comparable to that of experienced human experts in a prospective setting. We have demonstrated that the incorporation of explainability features enhanced the AI system's clinical applicability, and further investigated the potential advantages of the understandable AI system in guiding clinicians' decision-making.\n\nThe majority of existing work in which deep learning was used for automated diagnoses of breast-US imaging was exclusively developed on retrospectively collected single-view US B-mode images, which deviated from existing BI-RADS lexicon reporting standards 29 . More specifically, in a study of 520 sonograms 18 , deep-learning architecture was proposed to assess US breast lesions with an AUC of 0.896 \u00b1 0.039. Later, in a larger dataset of 7,408 breast-US B-mode images 19 , a deep-learning framework reported an AUC of >0.9. In addition, the performance of previous AI studies was not convincing due to a lack of large human comparator groups in a prospective setting. Particularly, generic deep-learning analysis software 38 has been implemented to classify breast cancer with an accuracy comparable to two radiologists and one medical student. Subsequently, a deep convolution neural network was proposed to mimic human decision-making in the classification of US breast lesions; however, its performance was compared with that of only two human radiologists 39 . Most recently, although various strategies [20][21][22][23][24][25][26][27][28] have been further investigated to strengthen the diagnosis performance of AI algorithms, a lack of evidence of interpretability for deep-learning systems, as well as a lack of guidance for relevant clinical decision-making, was identified as an impediment to the likelihood of clinical applicability. In contrast, we have shown that the proposed AI system has high overall performance with the independent clinical test set Taking both transverse and longitudinal views into clinical consideration is needed to ensure the quality of breast-US examinations in clinical practice 40 . In other words, the standard breast-US examination protocol requires radiologists to scan and analyse both views. Our study demonstrated that performance on lesion-level US images was in general better than on view-level US images. However, performance improvement was not remarkable owing to the fact that the corresponding diagnostic results evaluated from transverse and longitudinal views might be overlapped (Fig. 4a).\n\nAs stated by clinicians, the information extracted from the second view predominantly serves as an auxiliary diagnosis to that from the first view in clinical decisions. The clinical value of evaluating breast cancer risk through a second view by both clinicians and the AI system is to diminish the risk of bias (Fig. 4b) and ensure clinical outcomes. As a consequence, to conform to the routine workflow of clinicians, our AI system was designed and evaluated on lesion-level US images.\n\nTo ensure a fair comparison between AI and clinicians, the AI system should be implemented in a prospective setting 30 . Therefore, our AI system was tested and compared with seven clinicians in an independent clinical dataset collected after development of the algorithm and from patients who underwent US examination with orthogonal views in two hospitals. Our AI system showed the ability to predict risk of breast cancer at a level comparable to that of the experienced radiologists in the study in terms of accuracy, sensitivity and specificity, as determined from ROC curves. With the additional information of US elastography in part 2 of the reader study, the AI system showed improved specificity of breast lesion assessment without loss of sensitivity for the BRCS 4a+ mode. These results were consistent with both our reader studies and previous clinical studies [41][42][43] via considering BI-RADS category 4a or higher as test positive for malignancy (BI-RADS 3 versus 4a+). Notably, despite the fact that both the AI system and reader performance in part 2 of the reader study improved relative to part 1, the AI system showed a more favourable improvement than that of human experts, demonstrating the AI system's potential advantages for comprehensive interpretation of multimodal multiview US images. It should be noted that the decision to upgrade or downgrade BI-RADS 4 lesions is a major challenge for clinicians. According to the American College of Radiology BI-RADS Atlas 29 , diagnoses of BI-RADS 4 are confirmed by the presence or absence of breast cancer via biopsy. With the aim of reducing unnecessary biopsy for patients classified as BI-RADS 4, an AI diagnostic tool that provides deeper clinical insight, particularly for patients in the BI-RADS 4 category, is highly desirable 44 . On the test subset with only BI-RADS 4 lesions, both reader and AI system performance dropped relative to that in the whole test set as a result of only suspicious malignancy lesions being involved. However, the AI system still achieved  Fig. 4 | examples of ai prediction basis. a,b, Colour-coded heatmaps overlaid with the corresponding US images were generated from the final convolution layer using the Grad-CAM approach. a, Examples of transverse (first) and longitudinal (second) views of a malignant lesion. Combining two malignancy probability scores, the AI system correctly classified the true positive lesion. b, Example of a benign lesion. Despite the fact that there is a disagreement between the false positive from the transverse view and the true negative from the longitudinal view, combing orthogonal views may mitigate overall prediction bias. The prediction basis can assist human experts to understand the justification of the decision made by the AI system.  The workflow of the AI-assisted reader study is shown. For each randomly assigned and displayed test lesion, the reader examined information from three aspects, including original multimodal multiview US images, the corresponding highlighted heatmaps and the AI prediction on malignancy risk probability, then finally determined the BI-RADS categorization.\n\n\nUS\n\nan AUC of 0.880 on bimodal US images and an AUC of 0.920 on multimodal US images, reaching the same high level attained by human experts.\n\nThe black box nature of deep learning has been identified as an obstacle to establishing trust with human experts in clinical practice 45 . The United States Food and Drug Administration requires that any AI system that supports clinical decision-making must explain the rationale for its decisions to enable the clinicians to review the recommendation basis 46 . As indicated by a previous study 47 , an understandable AI tool not only increases confidence levels of clinicians in making or excluding a diagnosis but also provides educational feedback that will benefit non-experts such as non-radiologist clinicians or general radiologists. Therefore, to enhance the AI system's likelihood of clinical applicability, we incorporated explainability features (that is, heatmaps that make the AI decisions understandable to human experts).\n\nAs opposed to a malignancy risk score, such heatmaps allowed clinicians in our studies to visualize the basis of the AI predictioninformation that could then be used to help guide their clinical decisions. In particular, the heatmap displays depicted highly relevant and informative features such as the value of the lesion margin, the interior echo pattern in US B-mode images, the presence of vasculature in US colour Doppler images and the stiffness distribution in US elastography images. Since malignant breast lesions were typically associated with distinct features, such as irregular shapes, non-circumscribed margins (for example, indistinct margins or lobulated margins), heterogeneous echo areas inside the lesions, abundant vessel signals and high stiffness regions, the highlighted regions in the heatmaps were helpful to identify these representative characteristics of malignant lesions. In contrast, most benign breast lesions rarely have detectable vasculature and have relatively uniform stiffness mapping. Therefore, the entire region in the neighbourhood of benign lesions is of importance in AI interpretation.\n\nIt must be pointed out that it is the clinicians who make the final clinical BI-RADS decisions. The heatmaps were proposed to aid clinicians in their understanding of the prediction basis of the AI decisions, and then to guide them in analysing/re-evaluating the highlighted regions in each imaging mode if necessary, resulting in the potential to make a better clinical BI-RADS decision. In the AI-assisted reader study, we demonstrated the potential advantages of an understandable AI system in improving the radiologists' clinical outcomes (that is, an average decrease of 7% biopsy in benign lesions and an average increase of 2% biopsy in malignant lesions). Such an observation implied that our AI system was more effective in improving the specificity rather than sensitivity of the clinicians' decisions, which could further diminish the false positive rate of breast-US examinations. Future work can now directly augment the size of prospective datasets for comprehensively evaluating the advantages of understandable AI systems. Despite the fact that the interpretability of deep learning is still at an early stage, our AI-assisted workflow has demonstrated potential insights for clinicians' decision-making.\n\nThere are several limitations to our study. As we collected data from two hospitals in China, the proposed deep-learning system may only apply to the Asian population in current form. All patient data were acquired using Aixplorer US scanners while excluding the variability generated from different US machines. Therefore, there is a limit to the conclusions that can be drawn about generalizability. Future validation of the AI system should include data from several scanner manufacturers. Another limitation is that the patient population in the utilized test set is not representative of the natural distribution of patients with cancer in the screening population. Since only patients with biopsy-confirmed results were included in this study, the dataset lacked information for patients who underwent follow-up procedures. Moreover, further development of our system should include patients' medical histories, which would be relevant and could improve the performance of the AI decision. Lastly, although data augmentation was implemented here to alleviate the influence of unavailable data and overfitting, such a methodology did not mimic true physical phenomena (for example, bad gel coupling, acoustic phase reverberation or US attenuation) and instrumentation settings (for example, time-gain compensation, dynamic range compression or nonlinear filtering) impacting the appearance of US images. In other words, the data-augmentation procedures implemented in this study have less capability to reflect the variance attributed to US physics and inter-subject variability of clinical US scans. Future studies will require the broader variability of clinical multimodal US images encountered in practice to develop the AI system.\n\nIn conclusion, these results represent a step towards automated breast cancer risk prediction through the use of a deep-learning-based AI system. The combination of multimodal and multiview US images in the workflow matches clinical US examination/screening reporting standards. The heatmap display and breast cancer risk probability provided by this understandable AI system has the potential to guide clinical decisions in a prospective setting. Such a clinically applicable AI system may be incorporated into future breast cancer US screening, as well as support assisted or second-read workflows. methods Ethical approval. Our retrospective study was approved by the institutional review board of the hospitals, with a waiver granted for the requirement of informed consent. The prospective study was not an interventional prospective trial and was performed under guidelines approved by the institutional review board. All of the participants were informed about all aspects of the prospective study and gave informed, written consent. All images processed for this investigation were de-identified in accordance with the Health Insurance Portability and Accountability Act before transfer to study investigators.\n\n\nDevelopment dataset and clinical test dataset.\n\nWe retrospectively collected breast diagnostic reports from patients who underwent US examination between October 2016 and December 2018 at The First Affiliated Hospital of Anhui Medical University (hospital 1) and Xuancheng People's Hospital of China (hospital 2). The breast-US examinations/scans were performed using an Aixplorer US system (SuperSonic Imagine) equipped with either an SL15-4 or an SL10-2 linear array transducer under the preset settings. Examinations/scans were performed by one of eight radiologists, each of whom had over 10 years of experience in breast US. According to the availability of multimodal US images (that is, US B-mode images for the assessment of morphology, US colour Doppler images for the assessment of vascularity and US elastography images for the assessment of tissue elasticity), as well as corresponding pathology results, a total of 10,815 US images from 721 breast lesions (165 positive for cancer) of 634 patients were selected and then assigned to the development dataset. Details of the inclusion and exclusion criteria of the development dataset are shown in Fig. 2. As part of the clinical routine in the hospitals, between 10 and 20 images were taken and saved for each breast lesion, including three to six images each for US B-mode, US colour Doppler or US elastography imaging (US elastography in this study refers to shear wave elastography, a quantitative measurement of tissue stiffness expressed in kPa). Owing to the nature of retrospective study, multiview US image data were not completely preserved in all patients, and in some cases the corresponding view descriptions were not clearly labelled. As a result, the retrospectively collected multimodal US images were from either the transverse view, the longitudinal view or both. To utilize the existing large multimodal US imaging dataset for model development, the development dataset included 3,605 view-level multimodal US images (transverse or longitudinal view not distinguished), and biopsy-confirmed pathology results served as the ground truth. Lesions in the development dataset were randomly assigned to a training set (70%) or a validation set (30%).\n\nTo further evaluate the model's performance and compare it with that of clinicians, we prospectively collected 90 lesions from 82 consecutive patients in hospital 1 and 62 lesions from 59 consecutive patients in hospital 2. These patients underwent multimodal multiview breast-US scans from April 2019 to August 2019. The breast-US examinations were performed by one of three radiologists who had 12, 9 and 14 years of experience in breast US using the Aixplorer US system (SuperSonic Imagine) with the preset instrument settings. To distinguish from the view-level US images in the retrospective dataset, we defined the collected US images from each lesion in the prospective setting as lesion-level data, which represented multimodal US images with two orthogonal views. These data were collected after development of the AI system and were thus not used for training of the model. The ground truth for the clinical test set was labelled based on core-needle biopsy or surgical confirmation of cancer via manual review of the pathology note. Table 1 lists the patient demographics and breast lesion characteristics in this study. It should be emphasized that all BI-RADS categories included in this study were determined on breast-US imaging only. Pathology results were available for the patients or lesions classified as BI-RADS 2 or 3 following breast US due to either classification as BI-RADS 4a or higher following mammography or magnetic resonance imaging or requests from patients themselves.\n\nImaging preprocessing. Before the introduction of US images into the deep-learning network, a custom annotation tool (written in JavaScript using Electron 5.2 as its User Interfaces framework) was utilized by the radiologists to eliminate irrelevant information, such as text and instrument settings, in raw US images via a square segmentation mask. More specifically, during the US examinations, the radiologists were required to manually place a sampling box region where the corresponding vascularity (via US colour Doppler image) and elasticity (via US elastography image) measurements could be performed. Since the US B-mode and US elastography images were displayed on the screen simultaneously, there was a corresponding box region for US B-mode image as well. As a consequence, such box regions in each of the three US images provided guidance to the radiologists to eliminate irrelevant information.\n\nThe size of the square segmentation masks was first adjusted to the maximum, to maintain a sufficient margin (the margin is defined as the distance between the lesion boundary and the boundary of the segmentation mask itself) while not exceeding the sampling box boundary. Then, each of the segmentation masks was further altered by experienced radiologists to ensure similar lesion-to-mask proportions in each imaging mode. Finally, all of the cropped US images were resized to a 300 \u00d7 300 aspect ratio for quality control and fed as the network inputs. In summary, only multimodal US images of the lesions were retained for model development.\n\nModel development. Supplementary Fig. 1 illustrates the detailed architecture of our proposed multipathway deep-learning network for multimodal US image classification. The front part of our network processed each imaging modality exclusively. Feature maps were first extracted by a convolutional neural network ResNet-18 model 34 with pre-trained weights from the ImageNet dataset 12 via transfer learning 48 . A SENet module 31 was then implemented to adaptively recalibrate channel-wise features by explicitly modelling interdependencies between channels (that is, feature channel relationships) in each pathway. Next, several convolutional layers followed by BatchNorm and ReLU as the activation functions were used. At the end of each pathway, the 1 \u00d7 1 two-dimensional average pooling layers aggregated features from the spatial domain. To obtain the global multimodal feature vector, the outputs of these pathways were concatenated by merging features from each modality. To evaluate breast malignancy risk, probability was predicted by two fully connected layers and a softmax function. The learning object was to minimize the sum of the softmax cross entropy loss. To avoid overfitting, dropout with a probability of 0.5 was implemented throughout fully connected layers. With regard to the bimodal model, the deep-learning architecture only retained two pathways to match the inputs of US B-mode and US colour Doppler images.\n\nWe implemented our models on the PyTorch (version 3.7.4; pytorch.org) deep-learning framework with a Manjaro Linux 18.04 computer equipped with two Intel Xeon central processing units and two NVIDIA RTX 2080 Ti graphics processing units for training, validation and clinical testing. Optimization of the model was performed using an adaptive moment estimation (ADAM) optimizer in a batch size of 20 with an initial learning rate of 0.0001, which then decayed every 50 epochs with a decay factor of 0.5. The maximum iteration was set to 13,000 steps and an early stopping criterion was used to terminate training due to the absence of further improvement in both loss and accuracy. During model development, we augmented our training data by applying colour jitter and geometric transformations. More specifically, brightness levels of 0-2, contrasts of 0-2, saturation levels of 0.3-1.7, hues of \u22120.5-0.5 (since US colour Doppler and US elastography images have adopted the default colour bar in clinical settings, hue was therefore not applied to these two imaging modes) and entire image rotation angles of \u22128\u00b0-8\u00b0 were used for augmentation. All of these parameters were randomly and uniformly selected in the predefined ranges.\n\nThe bimodal model was trained to perform breast cancer risk prediction using both US B-mode and US colour Doppler images. The multimodal model was developed by taking US B-mode, US colour Doppler and US elastography imaging into consideration. Overall, the AI system was trained to take view-level US imaging candidates and automatically produced one malignancy risk probability score.\n\nTo predict the breast cancer risk of patients corresponding to lesion-level data with orthogonal-view candidates in a prospective setting, we averaged the malignancy probability from the transverse view (P 1 ) and longitudinal view (P 2 ) to form a final breast malignancy probability score (P overall ), as shown in Fig. 1. The ROC of the model was plotted by varying the threshold of overall probability (P overall ) in the interval 0-1. The AUC is the model's measure of performance, with a maximum value of 1.\n\nInterpretability of the AI system. To assure trust by human experts, an understandable decision-making process is desired in clinical practice. A class activation mapping (CAM)-based approach 49 , which uses a global average pooling layer at the end of neural networks instead of a fully connected layer, was recently proposed to highlight the class-specific discriminative regions. Despite the fact that excellent heatmaps of attention can be generated, the CAM approach has restrictions using global average pooling, and can only be applied to visualize the final-layer heatmaps.\n\nThe gradient-weighted CAM (Grad-CAM) technique 32 , as a generalization to CAM, is applicable to a broader range of convolutional neural network model families without architectural changes or retraining, resulting in promising textual explanations for model decisions. Given the model prediction to a target class c, we defined the neuron importance weights \u03b1 c k via global average pooling using equation (1):\n\u03b1 c k = 1 S \u2211 i \u2211 j \u2202y c \u2202A k ij (1)\nwhere y c is the gradient of the score for class c before being fed through the softmax function, A k ij corresponds to feature maps of a convolutional layer and S is the size of each feature map from the last block. The heatmap M c is generated by a weighted combination of forward activation maps followed by a ReLU using equation (2):\nM c = ReLU ( \u2211 k \u03b1 c k A k )(2)\nwhere ReLU represents a rectified linear unit. Supplementary Fig. 8 shows heatmap examples of biopsy-confirmed breast-US lesions generated by both CAM and Grad-CAM techniques. It was observed that CAM and Grad-CAM can identify similar ROIs with subtle discrepancies. Therefore, as a more generalized approach, Grad-CAM was adopted here to create the heatmap from the final convolutional layer in a test image, which could aid human experts in their understanding of the justification of the AI system for breast cancer risk prediction. Since the coarse heatmap was the same size as the convolutional feature map, we interpolated the grid size to the original size of the US image and then overlaid the corresponding image patches.\n\nOperating point selection. We defined three operating points that represented, respectively, the decision-making modes of BCRS 4a+, BCRS 4b+ and BCRS 4c+, as a way to compare the model with reader performance. The three operating points were chosen by thresholding the overall malignancy risk probability to match a likelihood of 10, 50 or 90%. It is important to note that the proposed three operating points were primarily used for comparison between the AI system and the readers who used BI-RADS categorization in clinical practice. Selection of optimal parameters of operating points with respect to the proper trade-off between sensitivity and specificity is still under investigation.\n\n\nReader study.\n\nA two-part reader study was conducted to compare the performance of the model with that of seven radiologists with an average of 14 years of clinical experience (range = 5-30 years). A total of 152 breast lesions (44 positive for cancer) from 141 patients in the clinical test set were presented to readers and the model in random order. The readers were blinded to each other, to the original radiologist's interpretation and to the deep-learning model assessment. Each reader reviewed the same set of lesions independently and applied the BI-RADS criteria to determine a BI-RADS score 29,50 . The main features considered by readers on assessment of US images were summarized as follows. Lesion margin, lesion morphology, aspect ratio, interior acoustic echo and micro-calcification were mainly considered for US B-mode images. For US colour Doppler images, the readers focused on the position of vascularity and the number of detected blood signals. With respect to US elastography images, both stiffness distribution and maximum elasticity were considered by the readers.\n\nReader study part 1 included both transverse and longitudinal views of US B-mode and US colour Doppler images for each breast lesion. After the readers completed part 1, the same lesions were re-presented to the same readers, now with additional information of US elastography, as reader study part 2. For each lesion, readers were given access to associated patient demographics (mainly age information), whereas the deep-learning model did not have access to this information.\n\nPerformance comparison between the model and readers was evaluated from two aspects. First, we computed the sensitivity and specificity of the model at each of the three BCRS modes (4a+, 4b+ and 4c+). Next, we compared the model's BCRS results with each reader using corresponding BI-RADS 3 versus 4a+, BI-RADS 3,4a versus 4b+ and BI-RADS 3,4a,4b versus 4c+ modes, respectively. Second, comparisons for cancer risk prediction were made between the model and the average reader. We computed the average reader sensitivity and specificity by averaging the seven individual reader sensitivities and specificities at three modes, respectively. We adjusted the threshold of the model's sensitivity to match the average reader sensitivity and then compared the specificity. To compare sensitivity, the model's specificity was set to match the average reader specificity.\n\nAI-assisted reader study. To evaluate the advantageous use of explainability features (that is, heatmaps) by the radiologists in guiding clinical decisions, an AI-assisted reader study was carried out. The same 152 breast lesions in the clinical test set were presented to the same seven readers again. For each lesion, together with the original multimodal multiview US images, the corresponding lesion heatmaps and AI prediction on malignancy risk probability were provided to the readers at the same time. Each reader was given the opportunity to upgrade or downgrade their original BI-RADS decisions, where the baseline BI-RADS scores were the results in reader study part 2.\n\nThe heatmap allowed the readers to assess the ROI with potential clinical value in each imaging mode. The malignancy risk prediction results from each individual view (P 1 and P 2 ) and the overall malignancy risk probability (P overall ) provided quantitative estimations to the readers, which might assist the readers to select the view with the highest probability of malignancy to review.\n\nTo evaluate the relevance of the heatmap to the clinician, we requested that all radiologists involved in our AI-assisted reader study summarize the information they obtained from the heatmaps over the whole clinical test set. Specifically, the readers were required to select the existing terms defined in BI-RADS lexicon to match the information they observed from the heatmaps. After collecting the information individually from the readers, we determined the relevant features of the heatmap based on the majority of the readers' observations (that is, at least four readers identified the informative features).\n\nStatistical analysis. The diagnostic performances of the bimodal and multimodal models on the basis of view-and lesion-level US images were expressed as AUC values and compared using Delong's test 51 with binomial exact confidence intervals. The confidence intervals in sensitivity and specificity were computed based on 1,000 bootstraps of the data. McNemar's test was used to calculate two-sided P values for the sensitivity and specificity between the models and human readers. P < 0.05 was considered to be the threshold for a statistically significant difference. All statistical analyses were performed using standard statistical software (SPSS (version 22.0; IBM) or MedCalc (version 19.0.7; MedCalc software)).\n\nReporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.\n\n\ndata availability\n\nThe authors declare that the main data supporting the results of this study are available within the paper and its Supplementary Information. The raw US datasets from The First Affiliated Hospital of Anhui Medical University and Xuancheng People's Hospital of China cannot be made available for public release because of patient privacy. However, some data can be made available for academic purposes from the corresponding author on reasonable request, subject to permission from the institutional review boards of the hospitals.\n\n\nCode availability\n\nThe deep-learning models were developed using standard libraries and scripts available in PyTorch. The pre-trained weights for ResNet were obtained from the torchvision library. Custom codes and the annotation tool for the deployment of the system are available for research purposes from the corresponding author upon reasonable request.\n\n\nnature research | reporting summary\n\nApril 2020\n\nCorresponding author(s): Xuejun Qian Last updated by author(s): Feb 14, 2021 Reporting Summary Nature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency in reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist.\n\n\nStatistics\n\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\n\n\nn/a Confirmed\n\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\n\nThe statistical test(s) used AND whether they are one-or two-sided Only common tests should be described solely by name; describe more complex techniques in the Methods section.\n\nA description of all covariates tested A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\n\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \n\n\nSoftware and code\n\nPolicy information about availability of computer code Data collection All patient data were acquired with Aixplorer ultrasound machine (SuperSonic Imagine, Aix-en-Provence, France).\n\nA custom annotation tool (written in JavaScript using Electron 5.2 as its UI framework) was used to eliminate irrelevant information such as text and instrument settings, and for labeling the multimodal ultrasound images. For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\n\n\nData\n\nPolicy information about availability of data All manuscripts must include a data availability statement. This statement should provide the following information, where applicable:\n\n-Accession codes, unique identifiers, or web links for publicly available datasets -A list of figures that have associated raw data -A description of any restrictions on data availability\n\n\n, resulting in potential issues relevant to clinical acceptance and diagnostic accuracy. In routine clinical workflows where either fourth-edition BI-RADS (via the assessment of US B-mode Prospective assessment of breast cancer risk from multimodal multiview ultrasound images via clinically applicable deep learning Xuejun Qian 1,2,10 \u2709 , Jing Pei 3,4,10 , Hui Zheng 5,10 , Xinxin Xie 5 , Lin Yan 6 , Hao Zhang 7 , Chunguang Han 3,4 , Xiang Gao 8 , Hanqi Zhang 5 , Weiwei Zheng 9 , Qiang Sun 3,4 , Lu Lu 8 and K. Kirk Shung 1 The clinical application of breast ultrasound for the assessment of cancer risk and of deep learning for the classification of breast-ultrasound images has been hindered by inter-grader variability and high false positive rates and by deep-learning models that do not follow Breast Imaging Reporting and Data System (BI-RADS) standards, lack explainability features and have not been tested prospectively. Here, we show that an explainable deep-learning system trained on 10,815 multimodal breast-ultrasound images of 721 biopsy-confirmed lesions from 634 patients across two hospitals and prospectively tested on 912 additional images of 152 lesions from 141 patients predicts BI-RADS scores for breast cancer as accurately as experienced radiologists, with areas under the receiver operating curve of 0.922 (95% confidence interval (CI) = 0.868-0.959) for bimodal images and 0.955 (95% CI = 0.909-0.982) for multimodal images. Multimodal multiview breast-ultrasound images augmented with heatmaps for malignancy risk predicted via deep learning may facilitate the adoption of ultrasound imaging in screening mammography workflows.\n\n\nOn the test set of 152 lesions from 141 patients (Supplementary Fig. 5), the bimodal model obtained an AUC of 0.890 (95% CI = 0.829-0.935) on transverse-view US images and an AUC of 0.898 (95% CI = 0.839-0.942) on longitudinal-view US images. In contrast, the multimodal model accomplished superior performance to the bimodal model, reaching a significantly higher AUC of 0.936 (95% CI = 0.885-0.970) on transverse-view US images and an AUC of 0.935 (95% CI = 0.883-0.968) on longitudinal-view US images (P < 0.05). Supplementary\n\nFig. 2 |\n2overview of the retrospective and prospective workflow.\n\n\nused. Under the presence of US B-mode and US colour Doppler images, the bimodal model accomplished an AUC of 0.890 (95% CI = 0.829\u22120.935) for transverse views, an AUC of 0.898 (95% CI = 0.839\u22120.942) for longitudinal views and an overall AUC of 0.922 (95% CI = 0.868\u22120.959) for the combination of both views. With the comprehensive information of US B-mode, US colour Doppler and US elastography images, the AUC of the multimodal model was notably higher, reaching 0.936 (95% CI = 0.885\u22120.970) for transverse views, 0.935 (95% CI = 0.883\u22120.968) for longitudinal views and 0.955 (95% CI = 0.909\u22120.982) for the integration of two candidates. We observed that the multimodal model achieved substantially superior performance to the bimodal model, which implied the clinical value of US elastography for predicting breast cancer risk in the fifth edition of BI-RADS lexicon.\n\nFig. 3 |\n3Performance of the ai system and readers in predicting the risk of breast cancer on the prospective clinical test set using lesion-level uS images. a-d, The results correspond to bimodal (a,c) and multimodal (b,d) images of the whole set (a,b) and BI-RADS 4 subset (c,d). The performance of our AI system was compared with each of the seven readers and the averaged performance of the seven readers at three decision modes. Error bars represent the 95% CIs, which were calculated based on 1,000 bootstraps of the data.\n\nFig. 5 |\n5understandable ai system potentially guides human experts to make better clinical decisions.\n\n\nGive P values as exact values whenever suitable.For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated Our web collection on statistics for biologists contains articles on many of the points above.\n\n\nPyTorch (version 1.3.1): used to train, validate and test the deep-learning models. MATLAB (version 2018a): used to plot ROC curves and for the data analysis. MedCalc (version 19.0.7) and SPSS (version 22.0): used for statistical analysis.\n\n\nFig. 1 | overall ai system for breast cancer risk prediction. The model was developed on view-level multimodal US images (that is, US B-mode, US colour Doppler and US elastography images) using the deep-learning framework (see details in SupplementaryPre-trained weights from ImageNet \nLearned features \nView-level retrospective US images \nfrom either transverse or longitudinal view \n\nUS B mode \n\nUS colour \nDoppler \n\nUS \nelastography \n\nTransverse view \n\nLongitudinal view \n\nUS B mode \nUS colour Doppler \nUS elastography \n\nUS B mode \nUS colour Doppler \nUS elastography \n\nHeatmaps 1 \n2 \n3 \n+ \n\n\u03a3 \nP overall = average (P 1 , P 2 ) \n\nMalignancy risk probability \nfrom longitudinal view \n\nMalignancy risk probability \nfrom transverse view \n\nHeatmaps 1 \n2 \n3 \n+ \nP 1 \n\nP 2 \n\nVarying operating points \n\nMalignant \nBenign \n\n2 \n3 \n\n1 BCRS 4a+ \nBCRS 4b+ \nBCRS 4c+ \n\nInvestigate the AI's prediction \u2190\u2192 clinicians \n\nMake a BI-RADS decision confidently \n\nAdjust decision (if needed) on the basis of \npotential clinical value on each heatmap \n\n\n\n. 6\n.Breast-US examinations retrospectively collected from October 2016 to December 2018Image inclusion criteria \nAt least one view of a multimodal US image (B \nmode/colour Doppler/elastography) available \nImage exclusion criteria \nPoor image quality \n\nBreast lesion selection \n(1) For each patient's breast with multiple \nlesions, lesion with highest BI-RADS \ncategory selected. If multiple lesions \npresented in same category, largest \nlesion chosen (biopsy usually performed \non most suspicious lesion). Consequently, \nonly one lesion per breast, and at most two \nlesions per patient, selected. \n\n(2) Lesions classified as BI-RADS 2-5 with positive \nfindings selected. \n(3) Solid or mostly solid lesions selected (cystic \ncomponent < 25%). \n\nDevelopment dataset \nPatients = 634 \nAge (mean) = 46.6 (range = 23-73) \nLesions = 721 (556 benign; 165 malignant) \nEligible multimodal US images = 10,815 \n(3,605 \u00d7 3 at view level) \n\nTraining set \nValidation set \n\nClinical test set \nPatients = 141 \nAge (mean) = 48.7 (range = 26-81) \nLesions = 152 (108 benign; 44 malignant) \nEligible multimodal US images = 912 \n(152 \u00d7 6 at lesion level) \n\nReader study (parts 1 and 2) \nPatient demographics (for example, age) \nprovided to readers \n\nBlind to original radiologist's \ninterpretation \n\nBlind to AI assistance \nReader determines BI-RADS score \n\nEligible patients \n\nProspective patients from April 2019 to August 2019 \n\nHospital 1 \nPatients = 82 \nLesions = 90 \n\nHospital 2 \nPatients = 59 \nLesions = 62 \n\nBreast-US image collection \nAll multimodal US images (that is, 3 \nmodalities \u00d7 2 orthogonal views) acquired \n1 or 2 days before performing biopsy test \n\n70 and 30% of lesions randomly split \ninto training and validation sets, \nrespectively \n\nAI-assisted reader study \nAI prediction results (for example, malignancy \nrisk probability) provided to readers \nLesion heatmaps provided to \nreaders \n\nReader selects no change, downgrade or \nupgrade to original BI-RADS score \n\n(1) Total of 7 radiologists involved \n(average = 14 years experience; \nrange = 5-30 years) \n(2) Breast lesions presented to readers in \nrandom order \n(3) For each lesion, all US images \ndisplayed simultaneously (that is, \nbimodal multiview for reader study \npart 1 and multimodal multiview for \nreader study part 2) \n\nPatient inclusion criteria \n(1) Biopsy-confirmed pathology results \nPatient exclusion criteria \n(1) Mental illness or major underlying diseases \n(for example, tumour) \n(2) Women with implants, pregnancy or lactation \n(3) Those who have undergone surgery or \nchemotherapy \n\nEligible patients who meet breast lesion \nselection criteria \n\nWhole clinical test set \n(152 lesions) \n\nSubset of BI-RADS 4 lesions \n(82 lesions) \n\nNote that BI-RADS 4 is based on the original radiologist's \ninterpretation, not the readers involved \n\n\n\nTable 1 |\n1Patient demographics and breast lesion characteristicsThe BI-RADS category is based on the interpretation of the radiologist who originally performed the US examinations before the biopsy test, not the radiologists involved in the reader study. It should be noted that all BI-RADS categories involved in this study were determined on breast-US images only. b Includes non-specific malignant results. c Includes adenosis, hyperplasia, benign phyllodes tumours and papillomas.Characteristics \nretrospective \ndataset \n\nProspective \ndataset \n\nNumber of patients \n634 \n141 \n\nAge (years) (mean) \n46.6 (23-73) \n48.7 (26-81) \n\nNumber of lesions \n721 \n152 \n\nLesion size (mm) \n\n<10 \n199 (27.6%) \n36 (23.7%) \n\n10-19.9 \n334 (46.3%) \n69 (45.4%) \n\n20-29.9 \n135 (18.7%) \n37 (24.3%) \n\n>30 \n53 (7.4%) \n10 (6.6%) \n\nLesion depth (mm) \n\n<5 \n343 (47.6%) \n64 (42.1%) \n\n5-9.9 \n282 (39.1%) \n68 (44.7%) \n\n10-14.9 \n86 (11.9%) \n16 (10.5%) \n\n>15 \n10 (1.4%) \n4 (2.7%) \n\nBI-RADS category a \n\n2 \n3 (0.4%) \n1 (0.7%) \n\n3 \n326 (45.2%) \n58 (38.2%) \n\n4a \n232 (32.2%) \n49 (32.2%) \n\n4b \n84 (11.7%) \n19 (12.5%) \n\n4c \n44 (6.1%) \n14 (9.2%) \n\n5 \n32 (4.4%) \n11 (7.2%) \n\nLesion type \n\nInvasive ductal carcinoma 128 (17.8%) \n37 (24.3%) \n\nInvasive lobular carcinoma 4 (0.6%) \n1 (0.7%) \n\nDuctal carcinoma in situ \n9 (1.2%) \n2 (1.3%) \n\nOther malignant b \n24 (3.3%) \n4 (2.6%) \n\nFibroadenoma \n227 (31.5%) \n46 (30.3%) \n\nOther benign c \n329 (45.6%) \n62 (40.8%) \n\na \n\n\n\n\nInitial diagnosis made by human expertsInvestigate consistency with AI prediction Make a BI-RADS decision confidently (1) re-evaluate ROIs on the basis of heatmaps (2) adjust if needed \u2192 final BI-RADS decisionB mode \n\nUS colour \nDoppler \n\nUS \nelastography \n\nFirst \nview \n\nSecond \nview \n\nFirst \nheatmap \n\nSecond \nheatmap \n\nThird \nheatmap \nAssign \nand \ndisplay \n\nHighlight \nimportant \nregions \n\n152-lesion test \n\n#152 \n\n#2 \n\n#1 \n\nHuman experts \n(radiologists) \n\nWorkflow of \nhuman experts \n\nWorkflow of \nAI assistant \n\nBreast cancer \nrisk probabilities \n\nP 1 / P 2 / P overall \n\n\n\nTable 2 |\n2Summary of the changes of Bi-radS and biopsy decisions made by radiologists r1-r7 in completing the ai-assisted reader studyadjustment \nBenign lesions (n = 108) \nmalignant lesions (n = 44) \n\n3 versus 4a+ a \nBiopsy \n3 versus 4a+ a \nBiopsy \n\n+ \n\u2212 \nSolo \n+ai \n+ \n\u2212 \nSolo \n+ai \n\nR1 \nNo change \n102 \n1 \n0 \n51 \n52 \n42 \n0 \n0 \n44 \n44 \n\nDowngrade \n5 \n1 \n\nUpgrade \n1 \n1 \n\nR2 \nNo change \n96 \n7 \n4 \n20 \n23 \n30 \n2 \n0 \n40 \n42 \n\nDowngrade \n5 \n2 \n\nUpgrade \n7 \n12 \n\nR3 \nNo change \n70 \n6 \n11 \n61 \n56 \n23 \n0 \n0 \n44 \n44 \n\nDowngrade \n28 \n5 \n\nUpgrade \n10 \n16 \n\nR4 \nNo change \n98 \n2 \n4 \n15 \n13 \n25 \n1 \n0 \n41 \n42 \n\nDowngrade \n4 \n0 \n\nUpgrade \n6 \n19 \n\nR5 \nNo change \n78 \n4 \n16 \n48 \n36 \n42 \n0 \n0 \n44 \n44 \n\nDowngrade \n26 \n1 \n\nUpgrade \n4 \n1 \n\nR6 \nNo change \n90 \n1 \n6 \n40 \n35 \n33 \n2 \n0 \n42 \n44 \n\nDowngrade \n12 \n5 \n\nUpgrade \n6 \n6 \n\nR7 \nNo change \n103 \n1 \n2 \n62 \n61 \n41 \n1 \n0 \n43 \n44 \n\nDowngrade \n4 \n0 \n\nUpgrade \n1 \n3 \n\na \n\nNumber of lesions that the radiologists altered from BI-RADS 3 to 4a+ (+) or from BI-RADS 4a+ to 3 (\u2212) using computer assistance. Considering BI-RADS 4a+ as test positive for malignancy, a \nrecommendation for biopsy was made by the readers without (solo) or with (+AI) computer assistance. \n\nNature BiomediCaL eNGiNeeriNG | VOL 5 | JUNE 2021 | 522-532 | www.nature.com/natbiomedeng\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2021 Nature BiomediCaL eNGiNeeriNG | VOL 5 | JUNE 2021 | 522-532 | www.nature.com/natbiomedeng\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nThe authors declare that the main data supporting the results in this study are available within the paper and its supplementary information. The raw ultrasound datasets from the First Affiliated Hospital of Anhui Medical University and Xuancheng People's Hospital of China cannot be made available for public release because of patient privacy. However, some data can be made available for academic purposes from the corresponding author on reasonable request, subject to permission from the Institutional Review Boards of the hospitals.nature research | reporting summaryApril 2020Field-specific reporting Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.Life sciencesBehavioural & social sciences Ecological, evolutionary & environmental sciencesFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdfLife sciences study designAll studies must disclose on these points even when the disclosure is negative.Sample sizeOn the basis of published literature, it is generally agreed that a deep-learning system requires on the order of tens of thousands of examples. Thus, we collected as much available data as possible, according to the inclusion criteria.Data exclusions The exclusion criteria were:ReplicationWe tested the models using a prospectively collected clinical dataset from two hospitals. This dataset was not used for the training and validation of the models.Randomization Samples meeting the inclusion criteria were randomly allocated to the training and validation datasets.BlindingThe radiologists who participated in the clinical evaluation were blinded to the ground truth and were not involved in dataset collection.Reporting for specific materials, systems and methodsWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response.RecruitmentThe retrospective study was approved by the institutional review board (IRB) of the hospitals with a waiver granted for the requirement of informed consent. For the prospective study, all participants signed an informed consent approved by the IRB. We collected breast-ultrasound images according to predetermined criteria.Ethics oversightThe First Affiliated Hospital of Anhui Medical University Ethics Committee and Xuancheng People's Hospital Ethics Committee.Competing interestsThe authors declare no competing interests.additional informationSupplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41551-021-00711-2.Correspondence and requests for materials should be addressed to X.Q.Peer review informationPolicy information about clinical studiesAll manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions.Clinical trial registration This study is not an interventional prospective trial.Study protocolThis study is not an interventional prospective trial.Data collectionAll data were collected using the Aixplorer ultrasound system (SuperSonic Imagine, Aix-en-Provence, France) under the preset settings in the Department of ultrasound in two hospitals.OutcomesThis study is not an interventional prospective trial.\nGlobal cancer statistics. A Jemal, CA Cancer J. Clin. 61Jemal, A. et al. Global cancer statistics. CA Cancer J. Clin. 61, 69-90 (2011).\n\nGlobal cancer transitions according to the Human Development Index (2008-2030): a population-based study. F Bray, A Jemal, N Grey, J Ferlay, D Forman, Lancet Oncol. 13Bray, F., Jemal, A., Grey, N., Ferlay, J. & Forman, D. Global cancer transitions according to the Human Development Index (2008-2030): a population-based study. Lancet Oncol. 13, 790-801 (2012).\n\nBreast and cervical cancer in 187 countries between 1980 and 2010: a systematic analysis. M H Forouzanfar, Lancet. 378Forouzanfar, M. H. et al. Breast and cervical cancer in 187 countries between 1980 and 2010: a systematic analysis. Lancet 378, 1461-1484 (2011).\n\nDetection of breast cancer with addition of annual screening ultrasound or a single screening MRI to mammography in women with elevated breast cancer risk. W A Berg, J. Am. Med. Assoc. 307Berg, W. A. et al. Detection of breast cancer with addition of annual screening ultrasound or a single screening MRI to mammography in women with elevated breast cancer risk. J. Am. Med. Assoc. 307, 1394-1404 (2012).\n\nSensitivity and specificity of mammography and adjunctive ultrasonography to screen for breast cancer in the Japan Strategic Anti-cancer Randomized Trial (J-START): a randomised controlled trial. N Ohuchi, Lancet. 387Ohuchi, N. et al. Sensitivity and specificity of mammography and adjunctive ultrasonography to screen for breast cancer in the Japan Strategic Anti-cancer Randomized Trial (J-START): a randomised controlled trial. Lancet 387, 341-348 (2016).\n\nUltrasound as the primary screening test for breast cancer: analysis from ACRIN 6666. W A Berg, J. Natl Cancer Inst. 108367Berg, W. A. et al. Ultrasound as the primary screening test for breast cancer: analysis from ACRIN 6666. J. Natl Cancer Inst. 108, djv367 (2015).\n\nObserver variability of Breast Imaging Reporting and Data System (BI-RADS) for breast ultrasound. H.-J Lee, Eur. J. Radiol. 65Lee, H.-J. et al. Observer variability of Breast Imaging Reporting and Data System (BI-RADS) for breast ultrasound. Eur. J. Radiol. 65, 293-298 (2008).\n\nBreast Imaging Reporting and Data System lexicon for US: interobserver agreement for assessment of breast masses. N Abdullah, B Mesurolle, M El-Khoury, E Kao, Radiology. 252Abdullah, N., Mesurolle, B., El-Khoury, M. & Kao, E. Breast Imaging Reporting and Data System lexicon for US: interobserver agreement for assessment of breast masses. Radiology 252, 665-672 (2009).\n\nComputer-aided diagnosis in medical imaging: historical review, current status and future potential. K Doi, Comput. Med. Imaging Graph. 31Doi, K. Computer-aided diagnosis in medical imaging: historical review, current status and future potential. Comput. Med. Imaging Graph. 31, 198-211 (2007).\n\nMachine learning techniques for breast cancer computer aided diagnosis using different image modalities: a systematic review. N I Yassin, S Omran, E M El Houby, H Allam, Comput. Methods Programs Biomed. 156Yassin, N. I., Omran, S., El Houby, E. M. & Allam, H. Machine learning techniques for breast cancer computer aided diagnosis using different image modalities: a systematic review. Comput. Methods Programs Biomed. 156, 25-45 (2018).\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 521LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015).\n\nImageNet large scale visual recognition challenge. O Russakovsky, Int. J. Comput. Vis. 115Russakovsky, O. et al. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. 115, 211-252 (2015).\n\nDeep learning in medical image analysis. D Shen, G Wu, H.-I Suk, Annu. Rev. Biomed. Eng. 19Shen, D., Wu, G. & Suk, H.-I. Deep learning in medical image analysis. Annu. Rev. Biomed. Eng. 19, 221-248 (2017).\n\nDevelopment and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. V Gulshan, J. Am. Med. Assoc. 316Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. J. Am. Med. Assoc. 316, 2402-2410 (2016).\n\nDermatologist-level classification of skin cancer with deep neural networks. A Esteva, Nature. 542Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115-118 (2017).\n\nClinically applicable deep learning for diagnosis and referral in retinal disease. J De Fauw, Nat. Med. 24De Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat. Med. 24, 1342-1350 (2018).\n\nEnd-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. D Ardila, Nat. Med. 25Ardila, D. et al. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nat. Med. 25, 954-961 (2019).\n\nComputer-aided diagnosis with deep learning architecture: applications to breast lesions in US images and pulmonary nodules in CT scans. J.-Z Cheng, Sci. Rep. 624454Cheng, J.-Z. et al. Computer-aided diagnosis with deep learning architecture: applications to breast lesions in US images and pulmonary nodules in CT scans. Sci. Rep. 6, 24454 (2016).\n\nA deep learning framework for supporting the classification of breast lesions in ultrasound images. S Han, Phys. Med. Biol. 62Han, S. et al. A deep learning framework for supporting the classification of breast lesions in ultrasound images. Phys. Med. Biol. 62, 7714-7728 (2017).\n\nComputer-aided diagnosis system for breast ultrasound images using deep learning. H Tanaka, S.-W Chiu, T Watanabe, S Kaoku, T Yamaguchi, Phys. Med. Biol. 64235013Tanaka, H., Chiu, S.-W., Watanabe, T., Kaoku, S. & Yamaguchi, T. Computer-aided diagnosis system for breast ultrasound images using deep learning. Phys. Med. Biol. 64, 235013 (2019).\n\nComputer-aided diagnosis of breast ultrasound images using ensemble learning from convolutional neural networks. W K Moon, Comput. Methods Prog. Biomed. 190105361Moon, W. K. et al. Computer-aided diagnosis of breast ultrasound images using ensemble learning from convolutional neural networks. Comput. Methods Prog. Biomed. 190, 105361 (2020).\n\nJoint weakly and semi-supervised deep learning for localization and classification of masses in breast ultrasound images. S Y Shin, S Lee, I D Yun, S M Kim, K M Lee, IEEE Trans. Med. Imaging. 38Shin, S. Y., Lee, S., Yun, I. D., Kim, S. M. & Lee, K. M. Joint weakly and semi-supervised deep learning for localization and classification of masses in breast ultrasound images. IEEE Trans. Med. Imaging 38, 762-774 (2018).\n\nAutomated diagnosis of breast ultrasonography images using deep neural networks. X Qi, Med. Image Anal. 52Qi, X. et al. Automated diagnosis of breast ultrasonography images using deep neural networks. Med. Image Anal. 52, 185-198 (2019).\n\nDeep learning based classification of breast tumors with shear-wave elastography. Q Zhang, Ultrasonics. 72Zhang, Q. et al. Deep learning based classification of breast tumors with shear-wave elastography. Ultrasonics 72, 150-157 (2016).\n\nA combined ultrasonic B-mode and color Doppler system for the classification of breast masses using neural network. X Qian, Eur. Radiol. 30Qian, X. et al. A combined ultrasonic B-mode and color Doppler system for the classification of breast masses using neural network. Eur. Radiol. 30, 3023-3033 (2020).\n\nA radiomics approach with CNN for shear-wave elastography breast tumor classification. Y Zhou, IEEE Trans. Biomed. Eng. 65Zhou, Y. et al. A radiomics approach with CNN for shear-wave elastography breast tumor classification. IEEE Trans. Biomed. Eng. 65, 1935-1942 (2018).\n\nComputer-aided diagnosis for breast ultrasound using computerized BI-RADS features and machine learning methods. J Shan, S K Alam, B Garra, Y Zhang, T Ahmed, Ultrasound Med. Biol. 42Shan, J., Alam, S. K., Garra, B., Zhang, Y. & Ahmed, T. Computer-aided diagnosis for breast ultrasound using computerized BI-RADS features and machine learning methods. Ultrasound Med. Biol. 42, 980-988 (2016).\n\nBIRADS features-oriented semi-supervised deep learning for breast ultrasound computer-aided diagnosis. E Zhang, S Seiler, M Chen, W Lu, X Gu, Phys. Med. Biol. 65125005Zhang, E., Seiler, S., Chen, M., Lu, W. & Gu, X. BIRADS features-oriented semi-supervised deep learning for breast ultrasound computer-aided diagnosis. Phys. Med. Biol. 65, 125005 (2020).\n\nE B Mendelson, ACR BI-RADS\u00ae Atlas, Breast Imaging Reporting and Data System. American College of RadiologyMendelson, E. B. et al. ACR BI-RADS\u00ae Ultrasound In ACR BI-RADS\u00ae Atlas, Breast Imaging Reporting and Data System (American College of Radiology, 2013).\n\nArtificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies. M Nagendran, Br. Med. J. 368689Nagendran, M. et al. Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies. Br. Med. J. 368, m689 (2020).\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, 10.1109/CVPR.2018.00745IEEE Conference on Computer Vision and Pattern Recognition. IEEEHu, J., Shen, L. & Sun, G. Squeeze-and-excitation networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) https://doi. org/10.1109/CVPR.2018.00745 (IEEE, 2018).\n\nGrad-CAM: visual explanations from deep networks via gradient-based localization. R R Selvaraju, 10.1109/ICCV.2017.74IEEE International Conference on Computer Vision (ICCV). IEEESelvaraju, R. R. et al. Grad-CAM: visual explanations from deep networks via gradient-based localization. In IEEE International Conference on Computer Vision (ICCV) https://doi.org/10.1109/ICCV.2017.74 (IEEE, 2017).\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. preprint atSimonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (2015); preprint at https://arxiv.org/abs/1409.1556 (2014).\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.90IEEE Conference on Computer Vision and Pattern Recognition. IEEEHe, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) https://doi.org/10.1109/CVPR.2016.90 (IEEE, 2016).\n\nRethinking theInception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, 10.1109/CVPR.2016.308IEEE Conference on Computer Vision and Pattern Recognition. IEEESzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking theInception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) https://doi.org/10.1109/ CVPR.2016.308 (IEEE, 2016).\n\nMachine learning and prediction in medicinebeyond the peak of inflated expectations. J H Chen, S M Asch, N. Engl. J. Med. 376Chen, J. H. & Asch, S. M.Machine learning and prediction in medicine- beyond the peak of inflated expectations. N. Engl. J. Med. 376, 2507-2509 (2017).\n\nMethodologic guide for evaluating clinical performance and effect of artificial intelligence technology for medical diagnosis and prediction. S H Park, K Han, Radiology. 286Park, S. H. & Han, K. Methodologic guide for evaluating clinical performance and effect of artificial intelligence technology for medical diagnosis and prediction. Radiology 286, 800-809 (2018).\n\nClassification of breast cancer in ultrasound imaging using a generic deep learning analysis software: a pilot study. A S Becker, Br. J. Radiol. 9120170576Becker, A. S. et al. Classification of breast cancer in ultrasound imaging using a generic deep learning analysis software: a pilot study. Br. J. Radiol. 91, 20170576 (2018).\n\nAutomatic classification of ultrasound breast lesions using a deep convolutional neural network mimicking human decision-making. A Ciritsis, Eur. Radiol. 29Ciritsis, A. et al. Automatic classification of ultrasound breast lesions using a deep convolutional neural network mimicking human decision-making. Eur. Radiol. 29, 5458-5468 (2019).\n\nBreast cancer classification in automated breast ultrasound using multiview convolutional neural network with transfer learning. Y Wang, Ultrasound Med. Biol. 46Wang, Y. et al. Breast cancer classification in automated breast ultrasound using multiview convolutional neural network with transfer learning. Ultrasound Med. Biol. 46, 1119-1132 (2020).\n\nShear-wave elastography improves the specificity of breast US: the BE1 multinational study of 939 masses. W A Berg, Radiology. 262Berg, W. A. et al. Shear-wave elastography improves the specificity of breast US: the BE1 multinational study of 939 masses. Radiology 262, 435-449 (2012).\n\nEvaluation of screening US-detected breast masses by combined use of elastography and color Doppler US with B-mode US in women with dense breasts: a multicenter prospective study. S H Lee, Radiology. 285Lee, S. H. et al. Evaluation of screening US-detected breast masses by combined use of elastography and color Doppler US with B-mode US in women with dense breasts: a multicenter prospective study. Radiology 285, 660-669 (2017).\n\nDistinguishing benign from malignant masses at breast US: combined US elastography and color Doppler US-influence on radiologist accuracy. N Cho, Radiology. 262Cho, N. et al. Distinguishing benign from malignant masses at breast US: combined US elastography and color Doppler US-influence on radiologist accuracy. Radiology 262, 80-90 (2012).\n\nAdded value of quantitative ultrasound and machine learning in BI-RADS 4-5 assessment of solid breast lesions. F Destrempes, Ultrasound Med. Biol. 46Destrempes, F. et al. Added value of quantitative ultrasound and machine learning in BI-RADS 4-5 assessment of solid breast lesions. Ultrasound Med. Biol. 46, 436-444 (2020).\n\nCan we open the black box of AI?. D Castelvecchi, Nature. 538Castelvecchi, D.Can we open the black box of AI? Nature 538, 20-23 (2016).\n\nClinical and Patient Decision Support Software (Draft Guidance) (Food and Drug Administration. Clinical and Patient Decision Support Software (Draft Guidance) (Food and Drug Administration, 2018).\n\nAn explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets. H Lee, Nat. Biomed. Eng. 3Lee, H. et al. An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets. Nat. Biomed. Eng. 3, 173-182 (2019).\n\nDeep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. H.-C Shin, IEEE Trans. Med. Imaging. 35Shin, H.-C. et al. Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans. Med. Imaging 35, 1285-1298 (2016).\n\nLearning deep features for discriminative localization. B Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba, 10.1109/CVPR.2016.319IEEE Conference on Computer Vision and Pattern Recognition. IEEEZhou, B., Khosla, A., Lapedriza, A., Oliva, A. & Torralba, A. Learning deep features for discriminative localization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) https://doi.org/10.1109/ CVPR.2016.319 (IEEE, 2016).\n\nWFUMB guidelines and recommendations for clinical use of ultrasound elastography: part 2: breast. R G Barr, Ultrasound Med. Biol. 41Barr, R. G. et al. WFUMB guidelines and recommendations for clinical use of ultrasound elastography: part 2: breast. Ultrasound Med. Biol. 41, 1148-1160 (2015).\n\nComparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. E R Delong, D M Delong, D L Clarke-Pearson, Biometrics. 44DeLong, E. R., DeLong, D. M. & Clarke-Pearson, D. L.Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics 44, 837-845 (1988).\n\n. H Zheng, Y Liu, X Shuai, G Zhang, J.-S Zhang, W Yao, J.-X , We are grateful to H. Zheng, Y. Liu, X. Shuai, G. Zhang, J.-S. Zhang, W. Yao and J.-X.\n\nWe acknowledge help from Y. Lu and R. Wodnicki in manuscript revision and editing. author contributions. Zhang for participation in the reader studiesZhang for participation in the reader studies. We acknowledge help from Y. Lu and R. Wodnicki in manuscript revision and editing. author contributions\n\ndeveloped the deep-learning framework and software tools necessary for the experiments. X Q ; J P , H. ; X Q , L Y , X.G ; X Q , J P , H Zheng, X X Hao Zhang, C H , X.Q., L.Y., X.G., Hao Zhang and L.L.collected the raw US images and patients' pathology results in the clinic. executed the research and performed the statistical analysis. K.K.S. advised on the US imaging techniques. X.Q., J.P. and H. Zheng wrote the manuscript. All authors contributed to reviewing and editing the manuscriptX.Q. conceived of, designed and supervised the project. J.P. and H. Zheng provided clinical expertise and guidance on the study design. X.Q., L.Y. and X.G. developed the deep-learning framework and software tools necessary for the experiments. X.Q., J.P., H. Zheng, X.X., Hao Zhang and C.H. created the datasets, interpreted the data and defined the clinical labels. X.X., C.H., Hanqi Zhang, W.Z. and Q.S. collected the raw US images and patients' pathology results in the clinic. X.Q., L.Y., X.G., Hao Zhang and L.L. executed the research and performed the statistical analysis. K.K.S. advised on the US imaging techniques. X.Q., J.P. and H. Zheng wrote the manuscript. All authors contributed to reviewing and editing the manuscript.\n\nPatients have mental illness, major underlying diseases (that is, have had a tumour). Patients have mental illness, major underlying diseases (that is, have had a tumour).\n\nDone surgery or chemotherapy. Done surgery or chemotherapy.\n\nWomen with implants, pregnancy or lactation. Women with implants, pregnancy or lactation.\n\nPoor image quality. Poor image quality.\n\nImages with breast lesions that were not confirmed by histology. For more details, please refer to Methods and to Fig. Images with breast lesions that were not confirmed by histology. For more details, please refer to Methods and to Fig. 2.\n", "annotations": {"author": "[{\"end\":161,\"start\":132},{\"end\":171,\"start\":162},{\"end\":184,\"start\":172},{\"end\":362,\"start\":185},{\"end\":365,\"start\":363},{\"end\":574,\"start\":366},{\"end\":680,\"start\":575},{\"end\":787,\"start\":681},{\"end\":868,\"start\":788},{\"end\":956,\"start\":869},{\"end\":971,\"start\":957},{\"end\":1294,\"start\":972},{\"end\":1402,\"start\":1295},{\"end\":1485,\"start\":1403},{\"end\":1695,\"start\":1486},{\"end\":1815,\"start\":1696},{\"end\":1918,\"start\":1816}]", "publisher": null, "author_last_name": "[{\"end\":143,\"start\":139},{\"end\":170,\"start\":167},{\"end\":196,\"start\":192},{\"end\":374,\"start\":371},{\"end\":584,\"start\":579},{\"end\":691,\"start\":688},{\"end\":795,\"start\":792},{\"end\":878,\"start\":873},{\"end\":970,\"start\":967},{\"end\":981,\"start\":978},{\"end\":1306,\"start\":1301},{\"end\":1415,\"start\":1410},{\"end\":1495,\"start\":1492},{\"end\":1701,\"start\":1699},{\"end\":1828,\"start\":1823}]", "author_first_name": "[{\"end\":138,\"start\":132},{\"end\":166,\"start\":162},{\"end\":175,\"start\":172},{\"end\":183,\"start\":176},{\"end\":191,\"start\":185},{\"end\":364,\"start\":363},{\"end\":370,\"start\":366},{\"end\":578,\"start\":575},{\"end\":687,\"start\":681},{\"end\":791,\"start\":788},{\"end\":872,\"start\":869},{\"end\":966,\"start\":957},{\"end\":977,\"start\":972},{\"end\":1300,\"start\":1295},{\"end\":1409,\"start\":1403},{\"end\":1491,\"start\":1486},{\"end\":1698,\"start\":1696},{\"end\":1817,\"start\":1816},{\"end\":1822,\"start\":1818}]", "author_affiliation": "[{\"end\":285,\"start\":198},{\"end\":361,\"start\":287},{\"end\":473,\"start\":376},{\"end\":573,\"start\":475},{\"end\":679,\"start\":586},{\"end\":786,\"start\":693},{\"end\":867,\"start\":797},{\"end\":955,\"start\":880},{\"end\":1080,\"start\":983},{\"end\":1180,\"start\":1082},{\"end\":1293,\"start\":1182},{\"end\":1401,\"start\":1308},{\"end\":1484,\"start\":1417},{\"end\":1594,\"start\":1497},{\"end\":1694,\"start\":1596},{\"end\":1814,\"start\":1703},{\"end\":1917,\"start\":1830}]", "title": "[{\"end\":129,\"start\":1},{\"end\":2047,\"start\":1919}]", "venue": null, "abstract": "[{\"end\":6868,\"start\":2121}]", "bib_ref": "[{\"end\":6993,\"start\":6992},{\"end\":7281,\"start\":7280},{\"end\":7382,\"start\":7381},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7653,\"start\":7652},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8083,\"start\":8081},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8084,\"start\":8083},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8365,\"start\":8363},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8727,\"start\":8725},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8932,\"start\":8930},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9089,\"start\":9087},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9303,\"start\":9299},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9307,\"start\":9303},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9311,\"start\":9307},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9315,\"start\":9311},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9448,\"start\":9446},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9519,\"start\":9517},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9650,\"start\":9647},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9652,\"start\":9650},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9693,\"start\":9690},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9695,\"start\":9693},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9771,\"start\":9767},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9775,\"start\":9771},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9779,\"start\":9775},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9843,\"start\":9840},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9845,\"start\":9843},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10215,\"start\":10213},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10650,\"start\":10648},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14453,\"start\":14451},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14963,\"start\":14961},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14985,\"start\":14983},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15013,\"start\":15011},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16620,\"start\":16617},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16622,\"start\":16620},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27811,\"start\":27809},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28026,\"start\":28024},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28618,\"start\":28616},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28668,\"start\":28664},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28672,\"start\":28668},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28676,\"start\":28672},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28680,\"start\":28676},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28684,\"start\":28680},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28688,\"start\":28684},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28692,\"start\":28688},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28696,\"start\":28692},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28700,\"start\":28696},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29280,\"start\":29278},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30316,\"start\":30314},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31076,\"start\":31072},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31080,\"start\":31076},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31084,\"start\":31080},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32010,\"start\":32008},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33633,\"start\":33631},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":33857,\"start\":33855},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33895,\"start\":33893},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":45350,\"start\":45348},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":48706,\"start\":48704},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":51945,\"start\":51942},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":51947,\"start\":51945},{\"end\":57326,\"start\":57315},{\"end\":57366,\"start\":57354}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":61628,\"start\":59967},{\"attributes\":{\"id\":\"fig_1\"},\"end\":62160,\"start\":61629},{\"attributes\":{\"id\":\"fig_2\"},\"end\":62227,\"start\":62161},{\"attributes\":{\"id\":\"fig_3\"},\"end\":63099,\"start\":62228},{\"attributes\":{\"id\":\"fig_4\"},\"end\":63629,\"start\":63100},{\"attributes\":{\"id\":\"fig_5\"},\"end\":63733,\"start\":63630},{\"attributes\":{\"id\":\"fig_6\"},\"end\":64187,\"start\":63734},{\"attributes\":{\"id\":\"fig_7\"},\"end\":64429,\"start\":64188},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":65464,\"start\":64430},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":68273,\"start\":65465},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":69701,\"start\":68274},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":70281,\"start\":69702},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":71477,\"start\":70282}]", "paragraph": "[{\"end\":8227,\"start\":6870},{\"end\":9521,\"start\":8229},{\"end\":11129,\"start\":9523},{\"end\":12791,\"start\":11131},{\"end\":13711,\"start\":12803},{\"end\":14543,\"start\":13713},{\"end\":15533,\"start\":14545},{\"end\":16443,\"start\":15535},{\"end\":17022,\"start\":16445},{\"end\":17859,\"start\":17024},{\"end\":18846,\"start\":17861},{\"end\":20066,\"start\":18848},{\"end\":20975,\"start\":20068},{\"end\":21753,\"start\":20977},{\"end\":21952,\"start\":21804},{\"end\":22051,\"start\":22045},{\"end\":22059,\"start\":22053},{\"end\":22486,\"start\":22079},{\"end\":22691,\"start\":22488},{\"end\":23998,\"start\":22693},{\"end\":25942,\"start\":24000},{\"end\":27550,\"start\":25957},{\"end\":29706,\"start\":27552},{\"end\":30196,\"start\":29708},{\"end\":33350,\"start\":30198},{\"end\":33494,\"start\":33357},{\"end\":34334,\"start\":33496},{\"end\":35467,\"start\":34336},{\"end\":36689,\"start\":35469},{\"end\":38431,\"start\":36691},{\"end\":39651,\"start\":38433},{\"end\":41879,\"start\":39702},{\"end\":43383,\"start\":41881},{\"end\":44293,\"start\":43385},{\"end\":44939,\"start\":44295},{\"end\":46376,\"start\":44941},{\"end\":47608,\"start\":46378},{\"end\":47995,\"start\":47610},{\"end\":48510,\"start\":47997},{\"end\":49093,\"start\":48512},{\"end\":49506,\"start\":49095},{\"end\":49881,\"start\":49544},{\"end\":50644,\"start\":49914},{\"end\":51337,\"start\":50646},{\"end\":52430,\"start\":51355},{\"end\":52910,\"start\":52432},{\"end\":53776,\"start\":52912},{\"end\":54457,\"start\":53778},{\"end\":54851,\"start\":54459},{\"end\":55469,\"start\":54853},{\"end\":56189,\"start\":55471},{\"end\":56326,\"start\":56191},{\"end\":56878,\"start\":56348},{\"end\":57238,\"start\":56900},{\"end\":57288,\"start\":57278},{\"end\":57659,\"start\":57290},{\"end\":57815,\"start\":57674},{\"end\":58068,\"start\":57833},{\"end\":58247,\"start\":58070},{\"end\":58654,\"start\":58249},{\"end\":58797,\"start\":58656},{\"end\":59001,\"start\":58819},{\"end\":59588,\"start\":59003},{\"end\":59777,\"start\":59597},{\"end\":59966,\"start\":59779}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":49543,\"start\":49507},{\"attributes\":{\"id\":\"formula_1\"},\"end\":49913,\"start\":49882}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13247,\"start\":13225},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13583,\"start\":13576},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16724,\"start\":16717},{\"end\":18554,\"start\":18547},{\"end\":18740,\"start\":18733},{\"end\":19598,\"start\":19591},{\"end\":19952,\"start\":19945},{\"end\":20569,\"start\":20562},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25314,\"start\":25307},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":42932,\"start\":42925}]", "section_header": "[{\"end\":12801,\"start\":12794},{\"end\":21802,\"start\":21756},{\"end\":22043,\"start\":21955},{\"end\":22077,\"start\":22062},{\"end\":25955,\"start\":25945},{\"end\":33355,\"start\":33353},{\"end\":39700,\"start\":39654},{\"end\":51353,\"start\":51340},{\"end\":56346,\"start\":56329},{\"end\":56898,\"start\":56881},{\"attributes\":{\"n\":\"1\"},\"end\":57276,\"start\":57241},{\"end\":57672,\"start\":57662},{\"end\":57831,\"start\":57818},{\"end\":58817,\"start\":58800},{\"end\":59595,\"start\":59591},{\"end\":62170,\"start\":62162},{\"end\":63109,\"start\":63101},{\"end\":63639,\"start\":63631},{\"end\":65469,\"start\":65466},{\"end\":68284,\"start\":68275},{\"end\":70292,\"start\":70283}]", "table": "[{\"end\":65464,\"start\":64683},{\"end\":68273,\"start\":65554},{\"end\":69701,\"start\":68760},{\"end\":70281,\"start\":69913},{\"end\":71477,\"start\":70418}]", "figure_caption": "[{\"end\":61628,\"start\":59969},{\"end\":62160,\"start\":61631},{\"end\":62227,\"start\":62172},{\"end\":63099,\"start\":62230},{\"end\":63629,\"start\":63111},{\"end\":63733,\"start\":63641},{\"end\":64187,\"start\":63736},{\"end\":64429,\"start\":64190},{\"end\":64683,\"start\":64432},{\"end\":65554,\"start\":65471},{\"end\":68760,\"start\":68286},{\"end\":69913,\"start\":69704},{\"end\":70418,\"start\":70294}]", "figure_ref": "[{\"end\":12892,\"start\":12886},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13334,\"start\":13328},{\"end\":13903,\"start\":13881},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14788,\"start\":14767},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15132,\"start\":15110},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15429,\"start\":15408},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15797,\"start\":15777},{\"end\":16442,\"start\":16421},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17701,\"start\":17695},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19572,\"start\":19565},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19929,\"start\":19920},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20543,\"start\":20536},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20581,\"start\":20572},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21187,\"start\":21180},{\"end\":22096,\"start\":22090},{\"end\":22689,\"start\":22683},{\"end\":22879,\"start\":22861},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24553,\"start\":24546},{\"end\":29705,\"start\":29696},{\"end\":30030,\"start\":30021},{\"end\":32296,\"start\":32251},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40819,\"start\":40813},{\"end\":44980,\"start\":44960},{\"end\":48320,\"start\":48314},{\"end\":49981,\"start\":49961}]", "bib_author_first_name": "[{\"end\":75436,\"start\":75435},{\"end\":75653,\"start\":75652},{\"end\":75661,\"start\":75660},{\"end\":75670,\"start\":75669},{\"end\":75678,\"start\":75677},{\"end\":75688,\"start\":75687},{\"end\":76000,\"start\":75999},{\"end\":76002,\"start\":76001},{\"end\":76331,\"start\":76330},{\"end\":76333,\"start\":76332},{\"end\":76777,\"start\":76776},{\"end\":77127,\"start\":77126},{\"end\":77129,\"start\":77128},{\"end\":77412,\"start\":77408},{\"end\":77704,\"start\":77703},{\"end\":77716,\"start\":77715},{\"end\":77729,\"start\":77728},{\"end\":77742,\"start\":77741},{\"end\":78063,\"start\":78062},{\"end\":78384,\"start\":78383},{\"end\":78386,\"start\":78385},{\"end\":78396,\"start\":78395},{\"end\":78405,\"start\":78404},{\"end\":78407,\"start\":78406},{\"end\":78419,\"start\":78418},{\"end\":78712,\"start\":78711},{\"end\":78721,\"start\":78720},{\"end\":78731,\"start\":78730},{\"end\":78882,\"start\":78881},{\"end\":79079,\"start\":79078},{\"end\":79087,\"start\":79086},{\"end\":79096,\"start\":79092},{\"end\":79370,\"start\":79369},{\"end\":79667,\"start\":79666},{\"end\":79895,\"start\":79894},{\"end\":80162,\"start\":80161},{\"end\":80482,\"start\":80478},{\"end\":80792,\"start\":80791},{\"end\":81055,\"start\":81054},{\"end\":81068,\"start\":81064},{\"end\":81076,\"start\":81075},{\"end\":81088,\"start\":81087},{\"end\":81097,\"start\":81096},{\"end\":81432,\"start\":81431},{\"end\":81434,\"start\":81433},{\"end\":81786,\"start\":81785},{\"end\":81788,\"start\":81787},{\"end\":81796,\"start\":81795},{\"end\":81803,\"start\":81802},{\"end\":81805,\"start\":81804},{\"end\":81812,\"start\":81811},{\"end\":81814,\"start\":81813},{\"end\":81821,\"start\":81820},{\"end\":81823,\"start\":81822},{\"end\":82165,\"start\":82164},{\"end\":82405,\"start\":82404},{\"end\":82677,\"start\":82676},{\"end\":82955,\"start\":82954},{\"end\":83254,\"start\":83253},{\"end\":83262,\"start\":83261},{\"end\":83264,\"start\":83263},{\"end\":83272,\"start\":83271},{\"end\":83281,\"start\":83280},{\"end\":83290,\"start\":83289},{\"end\":83638,\"start\":83637},{\"end\":83647,\"start\":83646},{\"end\":83657,\"start\":83656},{\"end\":83665,\"start\":83664},{\"end\":83671,\"start\":83670},{\"end\":83891,\"start\":83890},{\"end\":83893,\"start\":83892},{\"end\":84279,\"start\":84278},{\"end\":84525,\"start\":84524},{\"end\":84531,\"start\":84530},{\"end\":84539,\"start\":84538},{\"end\":84900,\"start\":84899},{\"end\":84902,\"start\":84901},{\"end\":85281,\"start\":85280},{\"end\":85293,\"start\":85292},{\"end\":85631,\"start\":85630},{\"end\":85637,\"start\":85636},{\"end\":85646,\"start\":85645},{\"end\":85653,\"start\":85652},{\"end\":86006,\"start\":86005},{\"end\":86017,\"start\":86016},{\"end\":86030,\"start\":86029},{\"end\":86039,\"start\":86038},{\"end\":86049,\"start\":86048},{\"end\":86471,\"start\":86470},{\"end\":86473,\"start\":86472},{\"end\":86481,\"start\":86480},{\"end\":86483,\"start\":86482},{\"end\":86806,\"start\":86805},{\"end\":86808,\"start\":86807},{\"end\":86816,\"start\":86815},{\"end\":87151,\"start\":87150},{\"end\":87153,\"start\":87152},{\"end\":87493,\"start\":87492},{\"end\":87834,\"start\":87833},{\"end\":88162,\"start\":88161},{\"end\":88164,\"start\":88163},{\"end\":88523,\"start\":88522},{\"end\":88525,\"start\":88524},{\"end\":88915,\"start\":88914},{\"end\":89231,\"start\":89230},{\"end\":89479,\"start\":89478},{\"end\":89892,\"start\":89891},{\"end\":90217,\"start\":90213},{\"end\":90507,\"start\":90506},{\"end\":90515,\"start\":90514},{\"end\":90525,\"start\":90524},{\"end\":90538,\"start\":90537},{\"end\":90547,\"start\":90546},{\"end\":90983,\"start\":90982},{\"end\":90985,\"start\":90984},{\"end\":91296,\"start\":91295},{\"end\":91298,\"start\":91297},{\"end\":91308,\"start\":91307},{\"end\":91310,\"start\":91309},{\"end\":91320,\"start\":91319},{\"end\":91322,\"start\":91321},{\"end\":91557,\"start\":91556},{\"end\":91566,\"start\":91565},{\"end\":91573,\"start\":91572},{\"end\":91582,\"start\":91581},{\"end\":91594,\"start\":91590},{\"end\":91603,\"start\":91602},{\"end\":91613,\"start\":91609},{\"end\":92095,\"start\":92094},{\"end\":92103,\"start\":92096},{\"end\":92112,\"start\":92106},{\"end\":92114,\"start\":92113},{\"end\":92118,\"start\":92117},{\"end\":92120,\"start\":92119},{\"end\":92130,\"start\":92123},{\"end\":92132,\"start\":92131},{\"end\":92136,\"start\":92135},{\"end\":92138,\"start\":92137},{\"end\":92142,\"start\":92141},{\"end\":92151,\"start\":92150},{\"end\":92153,\"start\":92152},{\"end\":92166,\"start\":92165},{\"end\":92168,\"start\":92167}]", "bib_author_last_name": "[{\"end\":75442,\"start\":75437},{\"end\":75658,\"start\":75654},{\"end\":75667,\"start\":75662},{\"end\":75675,\"start\":75671},{\"end\":75685,\"start\":75679},{\"end\":75695,\"start\":75689},{\"end\":76014,\"start\":76003},{\"end\":76338,\"start\":76334},{\"end\":76784,\"start\":76778},{\"end\":77134,\"start\":77130},{\"end\":77416,\"start\":77413},{\"end\":77713,\"start\":77705},{\"end\":77726,\"start\":77717},{\"end\":77739,\"start\":77730},{\"end\":77746,\"start\":77743},{\"end\":78067,\"start\":78064},{\"end\":78393,\"start\":78387},{\"end\":78402,\"start\":78397},{\"end\":78416,\"start\":78408},{\"end\":78425,\"start\":78420},{\"end\":78718,\"start\":78713},{\"end\":78728,\"start\":78722},{\"end\":78738,\"start\":78732},{\"end\":78894,\"start\":78883},{\"end\":79084,\"start\":79080},{\"end\":79090,\"start\":79088},{\"end\":79100,\"start\":79097},{\"end\":79378,\"start\":79371},{\"end\":79674,\"start\":79668},{\"end\":79903,\"start\":79896},{\"end\":80169,\"start\":80163},{\"end\":80488,\"start\":80483},{\"end\":80796,\"start\":80793},{\"end\":81062,\"start\":81056},{\"end\":81073,\"start\":81069},{\"end\":81085,\"start\":81077},{\"end\":81094,\"start\":81089},{\"end\":81107,\"start\":81098},{\"end\":81439,\"start\":81435},{\"end\":81793,\"start\":81789},{\"end\":81800,\"start\":81797},{\"end\":81809,\"start\":81806},{\"end\":81818,\"start\":81815},{\"end\":81827,\"start\":81824},{\"end\":82168,\"start\":82166},{\"end\":82411,\"start\":82406},{\"end\":82682,\"start\":82678},{\"end\":82960,\"start\":82956},{\"end\":83259,\"start\":83255},{\"end\":83269,\"start\":83265},{\"end\":83278,\"start\":83273},{\"end\":83287,\"start\":83282},{\"end\":83296,\"start\":83291},{\"end\":83644,\"start\":83639},{\"end\":83654,\"start\":83648},{\"end\":83662,\"start\":83658},{\"end\":83668,\"start\":83666},{\"end\":83674,\"start\":83672},{\"end\":83903,\"start\":83894},{\"end\":84289,\"start\":84280},{\"end\":84528,\"start\":84526},{\"end\":84536,\"start\":84532},{\"end\":84543,\"start\":84540},{\"end\":84912,\"start\":84903},{\"end\":85290,\"start\":85282},{\"end\":85303,\"start\":85294},{\"end\":85634,\"start\":85632},{\"end\":85643,\"start\":85638},{\"end\":85650,\"start\":85647},{\"end\":85657,\"start\":85654},{\"end\":86014,\"start\":86007},{\"end\":86027,\"start\":86018},{\"end\":86036,\"start\":86031},{\"end\":86046,\"start\":86040},{\"end\":86055,\"start\":86050},{\"end\":86478,\"start\":86474},{\"end\":86488,\"start\":86484},{\"end\":86813,\"start\":86809},{\"end\":86820,\"start\":86817},{\"end\":87160,\"start\":87154},{\"end\":87502,\"start\":87494},{\"end\":87839,\"start\":87835},{\"end\":88169,\"start\":88165},{\"end\":88529,\"start\":88526},{\"end\":88919,\"start\":88916},{\"end\":89242,\"start\":89232},{\"end\":89492,\"start\":89480},{\"end\":89896,\"start\":89893},{\"end\":90222,\"start\":90218},{\"end\":90512,\"start\":90508},{\"end\":90522,\"start\":90516},{\"end\":90535,\"start\":90526},{\"end\":90544,\"start\":90539},{\"end\":90556,\"start\":90548},{\"end\":90990,\"start\":90986},{\"end\":91305,\"start\":91299},{\"end\":91317,\"start\":91311},{\"end\":91337,\"start\":91323},{\"end\":91563,\"start\":91558},{\"end\":91570,\"start\":91567},{\"end\":91579,\"start\":91574},{\"end\":91588,\"start\":91583},{\"end\":91600,\"start\":91595},{\"end\":91607,\"start\":91604},{\"end\":92148,\"start\":92143},{\"end\":92163,\"start\":92154}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15611977},\"end\":75544,\"start\":75409},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4827907},\"end\":75907,\"start\":75546},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5413708},\"end\":76172,\"start\":75909},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":28063616},\"end\":76578,\"start\":76174},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":45883204},\"end\":77038,\"start\":76580},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9396290},\"end\":77308,\"start\":77040},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":29808180},\"end\":77587,\"start\":77310},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":21345996},\"end\":77959,\"start\":77589},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":17233601},\"end\":78255,\"start\":77961},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3470275},\"end\":78694,\"start\":78257},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1779661},\"end\":78828,\"start\":78696},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2930547},\"end\":79035,\"start\":78830},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7961631},\"end\":79242,\"start\":79037},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":26657811},\"end\":79587,\"start\":79244},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3767412},\"end\":79809,\"start\":79589},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":51974607},\"end\":80050,\"start\":79811},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":159041422},\"end\":80339,\"start\":80052},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15183161},\"end\":80689,\"start\":80341},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206093532},\"end\":80970,\"start\":80691},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":202837755},\"end\":81316,\"start\":80972},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211013852},\"end\":81661,\"start\":81318},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":25682830},\"end\":82081,\"start\":81663},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":58571472},\"end\":82320,\"start\":82083},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12976880},\"end\":82558,\"start\":82322},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":210985797},\"end\":82865,\"start\":82560},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":51614421},\"end\":83138,\"start\":82867},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":28214797},\"end\":83532,\"start\":83140},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":91184616},\"end\":83888,\"start\":83534},{\"attributes\":{\"id\":\"b28\"},\"end\":84146,\"start\":83890},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":214679094},\"end\":84489,\"start\":84148},{\"attributes\":{\"doi\":\"10.1109/CVPR.2018.00745\",\"id\":\"b30\",\"matched_paper_id\":140309863},\"end\":84815,\"start\":84491},{\"attributes\":{\"doi\":\"10.1109/ICCV.2017.74\",\"id\":\"b31\",\"matched_paper_id\":15019293},\"end\":85210,\"start\":84817},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14124313},\"end\":85582,\"start\":85212},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b33\",\"matched_paper_id\":206594692},\"end\":85945,\"start\":85584},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.308\",\"id\":\"b34\",\"matched_paper_id\":206593880},\"end\":86383,\"start\":85947},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10646588},\"end\":86661,\"start\":86385},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3400408},\"end\":87030,\"start\":86663},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":207211531},\"end\":87361,\"start\":87032},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":85566835},\"end\":87702,\"start\":87363},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":211121874},\"end\":88053,\"start\":87704},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1244545},\"end\":88340,\"start\":88055},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":207676203},\"end\":88773,\"start\":88342},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":33044216},\"end\":89117,\"start\":88775},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":208536808},\"end\":89442,\"start\":89119},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4465871},\"end\":89579,\"start\":89444},{\"attributes\":{\"id\":\"b45\"},\"end\":89777,\"start\":89581},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":56482317},\"end\":90080,\"start\":89779},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3333267},\"end\":90448,\"start\":90082},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.319\",\"id\":\"b48\",\"matched_paper_id\":6789015},\"end\":90882,\"start\":90450},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":25484713},\"end\":91176,\"start\":90884},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":21877334},\"end\":91552,\"start\":91178},{\"attributes\":{\"id\":\"b51\"},\"end\":91702,\"start\":91554},{\"attributes\":{\"id\":\"b52\"},\"end\":92004,\"start\":91704},{\"attributes\":{\"id\":\"b53\"},\"end\":93233,\"start\":92006},{\"attributes\":{\"id\":\"b54\"},\"end\":93406,\"start\":93235},{\"attributes\":{\"id\":\"b55\"},\"end\":93467,\"start\":93408},{\"attributes\":{\"id\":\"b56\"},\"end\":93558,\"start\":93469},{\"attributes\":{\"id\":\"b57\"},\"end\":93599,\"start\":93560},{\"attributes\":{\"id\":\"b58\"},\"end\":93841,\"start\":93601}]", "bib_title": "[{\"end\":75433,\"start\":75409},{\"end\":75650,\"start\":75546},{\"end\":75997,\"start\":75909},{\"end\":76328,\"start\":76174},{\"end\":76774,\"start\":76580},{\"end\":77124,\"start\":77040},{\"end\":77406,\"start\":77310},{\"end\":77701,\"start\":77589},{\"end\":78060,\"start\":77961},{\"end\":78381,\"start\":78257},{\"end\":78709,\"start\":78696},{\"end\":78879,\"start\":78830},{\"end\":79076,\"start\":79037},{\"end\":79367,\"start\":79244},{\"end\":79664,\"start\":79589},{\"end\":79892,\"start\":79811},{\"end\":80159,\"start\":80052},{\"end\":80476,\"start\":80341},{\"end\":80789,\"start\":80691},{\"end\":81052,\"start\":80972},{\"end\":81429,\"start\":81318},{\"end\":81783,\"start\":81663},{\"end\":82162,\"start\":82083},{\"end\":82402,\"start\":82322},{\"end\":82674,\"start\":82560},{\"end\":82952,\"start\":82867},{\"end\":83251,\"start\":83140},{\"end\":83635,\"start\":83534},{\"end\":84276,\"start\":84148},{\"end\":84522,\"start\":84491},{\"end\":84897,\"start\":84817},{\"end\":85278,\"start\":85212},{\"end\":85628,\"start\":85584},{\"end\":86003,\"start\":85947},{\"end\":86468,\"start\":86385},{\"end\":86803,\"start\":86663},{\"end\":87148,\"start\":87032},{\"end\":87490,\"start\":87363},{\"end\":87831,\"start\":87704},{\"end\":88159,\"start\":88055},{\"end\":88520,\"start\":88342},{\"end\":88912,\"start\":88775},{\"end\":89228,\"start\":89119},{\"end\":89476,\"start\":89444},{\"end\":89889,\"start\":89779},{\"end\":90211,\"start\":90082},{\"end\":90504,\"start\":90450},{\"end\":90980,\"start\":90884},{\"end\":91293,\"start\":91178}]", "bib_author": "[{\"end\":75444,\"start\":75435},{\"end\":75660,\"start\":75652},{\"end\":75669,\"start\":75660},{\"end\":75677,\"start\":75669},{\"end\":75687,\"start\":75677},{\"end\":75697,\"start\":75687},{\"end\":76016,\"start\":75999},{\"end\":76340,\"start\":76330},{\"end\":76786,\"start\":76776},{\"end\":77136,\"start\":77126},{\"end\":77418,\"start\":77408},{\"end\":77715,\"start\":77703},{\"end\":77728,\"start\":77715},{\"end\":77741,\"start\":77728},{\"end\":77748,\"start\":77741},{\"end\":78069,\"start\":78062},{\"end\":78395,\"start\":78383},{\"end\":78404,\"start\":78395},{\"end\":78418,\"start\":78404},{\"end\":78427,\"start\":78418},{\"end\":78720,\"start\":78711},{\"end\":78730,\"start\":78720},{\"end\":78740,\"start\":78730},{\"end\":78896,\"start\":78881},{\"end\":79086,\"start\":79078},{\"end\":79092,\"start\":79086},{\"end\":79102,\"start\":79092},{\"end\":79380,\"start\":79369},{\"end\":79676,\"start\":79666},{\"end\":79905,\"start\":79894},{\"end\":80171,\"start\":80161},{\"end\":80490,\"start\":80478},{\"end\":80798,\"start\":80791},{\"end\":81064,\"start\":81054},{\"end\":81075,\"start\":81064},{\"end\":81087,\"start\":81075},{\"end\":81096,\"start\":81087},{\"end\":81109,\"start\":81096},{\"end\":81441,\"start\":81431},{\"end\":81795,\"start\":81785},{\"end\":81802,\"start\":81795},{\"end\":81811,\"start\":81802},{\"end\":81820,\"start\":81811},{\"end\":81829,\"start\":81820},{\"end\":82170,\"start\":82164},{\"end\":82413,\"start\":82404},{\"end\":82684,\"start\":82676},{\"end\":82962,\"start\":82954},{\"end\":83261,\"start\":83253},{\"end\":83271,\"start\":83261},{\"end\":83280,\"start\":83271},{\"end\":83289,\"start\":83280},{\"end\":83298,\"start\":83289},{\"end\":83646,\"start\":83637},{\"end\":83656,\"start\":83646},{\"end\":83664,\"start\":83656},{\"end\":83670,\"start\":83664},{\"end\":83676,\"start\":83670},{\"end\":83905,\"start\":83890},{\"end\":84291,\"start\":84278},{\"end\":84530,\"start\":84524},{\"end\":84538,\"start\":84530},{\"end\":84545,\"start\":84538},{\"end\":84914,\"start\":84899},{\"end\":85292,\"start\":85280},{\"end\":85305,\"start\":85292},{\"end\":85636,\"start\":85630},{\"end\":85645,\"start\":85636},{\"end\":85652,\"start\":85645},{\"end\":85659,\"start\":85652},{\"end\":86016,\"start\":86005},{\"end\":86029,\"start\":86016},{\"end\":86038,\"start\":86029},{\"end\":86048,\"start\":86038},{\"end\":86057,\"start\":86048},{\"end\":86480,\"start\":86470},{\"end\":86490,\"start\":86480},{\"end\":86815,\"start\":86805},{\"end\":86822,\"start\":86815},{\"end\":87162,\"start\":87150},{\"end\":87504,\"start\":87492},{\"end\":87841,\"start\":87833},{\"end\":88171,\"start\":88161},{\"end\":88531,\"start\":88522},{\"end\":88921,\"start\":88914},{\"end\":89244,\"start\":89230},{\"end\":89494,\"start\":89478},{\"end\":89898,\"start\":89891},{\"end\":90224,\"start\":90213},{\"end\":90514,\"start\":90506},{\"end\":90524,\"start\":90514},{\"end\":90537,\"start\":90524},{\"end\":90546,\"start\":90537},{\"end\":90558,\"start\":90546},{\"end\":90992,\"start\":90982},{\"end\":91307,\"start\":91295},{\"end\":91319,\"start\":91307},{\"end\":91339,\"start\":91319},{\"end\":91565,\"start\":91556},{\"end\":91572,\"start\":91565},{\"end\":91581,\"start\":91572},{\"end\":91590,\"start\":91581},{\"end\":91602,\"start\":91590},{\"end\":91609,\"start\":91602},{\"end\":91616,\"start\":91609},{\"end\":92106,\"start\":92094},{\"end\":92117,\"start\":92106},{\"end\":92123,\"start\":92117},{\"end\":92135,\"start\":92123},{\"end\":92141,\"start\":92135},{\"end\":92150,\"start\":92141},{\"end\":92165,\"start\":92150},{\"end\":92171,\"start\":92165}]", "bib_venue": "[{\"end\":75461,\"start\":75444},{\"end\":75709,\"start\":75697},{\"end\":76022,\"start\":76016},{\"end\":76357,\"start\":76340},{\"end\":76792,\"start\":76786},{\"end\":77155,\"start\":77136},{\"end\":77432,\"start\":77418},{\"end\":77757,\"start\":77748},{\"end\":78095,\"start\":78069},{\"end\":78458,\"start\":78427},{\"end\":78746,\"start\":78740},{\"end\":78915,\"start\":78896},{\"end\":79124,\"start\":79102},{\"end\":79397,\"start\":79380},{\"end\":79682,\"start\":79676},{\"end\":79913,\"start\":79905},{\"end\":80179,\"start\":80171},{\"end\":80498,\"start\":80490},{\"end\":80813,\"start\":80798},{\"end\":81124,\"start\":81109},{\"end\":81469,\"start\":81441},{\"end\":81853,\"start\":81829},{\"end\":82185,\"start\":82170},{\"end\":82424,\"start\":82413},{\"end\":82695,\"start\":82684},{\"end\":82985,\"start\":82962},{\"end\":83318,\"start\":83298},{\"end\":83691,\"start\":83676},{\"end\":83965,\"start\":83905},{\"end\":84301,\"start\":84291},{\"end\":84626,\"start\":84568},{\"end\":84989,\"start\":84934},{\"end\":85357,\"start\":85305},{\"end\":85737,\"start\":85679},{\"end\":86136,\"start\":86078},{\"end\":86505,\"start\":86490},{\"end\":86831,\"start\":86822},{\"end\":87175,\"start\":87162},{\"end\":87515,\"start\":87504},{\"end\":87861,\"start\":87841},{\"end\":88180,\"start\":88171},{\"end\":88540,\"start\":88531},{\"end\":88930,\"start\":88921},{\"end\":89264,\"start\":89244},{\"end\":89500,\"start\":89494},{\"end\":89674,\"start\":89581},{\"end\":89914,\"start\":89898},{\"end\":90248,\"start\":90224},{\"end\":90637,\"start\":90579},{\"end\":91012,\"start\":90992},{\"end\":91349,\"start\":91339},{\"end\":91807,\"start\":91704},{\"end\":92092,\"start\":92006},{\"end\":93319,\"start\":93235},{\"end\":93436,\"start\":93408},{\"end\":93512,\"start\":93469},{\"end\":93578,\"start\":93560},{\"end\":93718,\"start\":93601}]"}}}, "year": 2023, "month": 12, "day": 17}