{"id": 248798584, "updated": "2023-11-08 05:10:33.09", "metadata": {"title": "VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder", "authors": "[{\"first\":\"Yuchao\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Xintao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Liangbin\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Chao\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Gen\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Shan\",\"middle\":[]},{\"first\":\"Ming-Ming\",\"last\":\"Cheng\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Although generative facial prior and geometric prior have recently demonstrated high-quality results for blind face restoration, producing fine-grained facial details faithful to inputs remains a challenging problem. Motivated by the classical dictionary-based methods and the recent vector quantization (VQ) technique, we propose a VQ-based face restoration method - VQFR. VQFR takes advantage of high-quality low-level feature banks extracted from high-quality faces and can thus help recover realistic facial details. However, the simple application of the VQ codebook cannot achieve good results with faithful details and identity preservation. Therefore, we further introduce two special network designs. 1). We first investigate the compression patch size in the VQ codebook and find that the VQ codebook designed with a proper compression patch size is crucial to balance the quality and fidelity. 2). To further fuse low-level features from inputs while not\"contaminating\"the realistic details generated from the VQ codebook, we proposed a parallel decoder consisting of a texture decoder and a main decoder. Those two decoders then interact with a texture warping module with deformable convolution. Equipped with the VQ codebook as a facial detail dictionary and the parallel decoder design, the proposed VQFR can largely enhance the restored quality of facial details while keeping the fidelity to previous methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.06803", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/GuWXDLSC22", "doi": "10.48550/arxiv.2205.06803"}}, "content": {"source": {"pdf_hash": "d70beae5de1a1e26e286966edab80408f6cb3f31", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2205.06803v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0058bef9eed39e694701435ef96d60a297191bd9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d70beae5de1a1e26e286966edab80408f6cb3f31.txt", "contents": "\nVQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder\n\n\nYuchao Gu \nARC Lab\nTencent PCG\n\nXintao Wang \nARC Lab\nTencent PCG\n\nLiangbin Xie \nARC Lab\nTencent PCG\n\nShenzhen Institute of Advanced Technology\nChinese Academy of Sciences\n\n\nChao Dong \nShenzhen Institute of Advanced Technology\nChinese Academy of Sciences\n\n\nGen Li \nPlatform Technologies\nLaboratory\nTencent Online Video 4 Shanghai AI\n\nYing Shan \nARC Lab\nTencent PCG\n\nMing-Ming Cheng \n\nNankai University\n5550\u22128758] 1 TMCC0000\u22120001CS\n\nVQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder\nBlind Face RestorationVector QuantizationParallel Decoder\nAlthough generative facial prior and geometric prior have recently demonstrated high-quality results for blind face restoration, producing fine-grained facial details faithful to inputs remains a challenging problem. Motivated by the classical dictionary-based methods and the recent vector quantization (VQ) technique, we propose a VQ-based face restoration method -VQFR. VQFR takes advantage of high-quality low-level feature banks extracted from high-quality faces and can thus help recover realistic facial details. However, the simple application of the VQ codebook cannot achieve good results with faithful details and identity preservation. Therefore, we further introduce two special network designs. 1). We first investigate the compression patch size in the VQ codebook and find that the VQ codebook designed with a proper compression patch size is crucial to balance the quality and fidelity. 2). To further fuse low-level features from inputs while not \"contaminating\" the realistic details generated from the VQ codebook, we proposed a parallel decoder consisting of a texture decoder and a main decoder. Those two decoders then interact with a texture warping module with deformable convolution. Equipped with the VQ codebook as a facial detail dictionary and the parallel decoder design, the proposed VQFR can largely enhance the restored quality of facial details while keeping the fidelity to previous methods.The limitations of the facial component dictionary motivate us to explore Vector Quantized (VQ) codebook, a dictionary constructed for all facial areas. The proposed face restoration method -VQFR, takes advantage of both dictionary-based methods and GAN training, yet does not require any geometric or GAN prior. Compared to the facial component dictionary [25], the VQ codebook could provide a more comprehensive lowlevel feature bank that is not restricted to limited facial components. It is also learned in an end-to-end manner by the face reconstruction task. Besides, the mechanism of vector quantization makes it more robust for diverse degradations. Nevertheless, it is not easy to achieve good results simply by applying the VQ codebook. We further introduce two special network designs, which allow VQFR to surpass previous methods in both detail generation and identity preserving.First, to generate realistic details, we find that it is crucial to select a proper compression patch size f , which indicates \"how large a patch is represented\" by an atom of the codebook. As shown inFig. 2, a larger f could lead to better visual quality but worse fidelity. After a comprehensive investigation, we suggest using f = 32 for the input image size 512 \u00d7 512. However, such a selection is only a trade-off between quality and fidelity. The expression and identity could also change a lot even with a proper compression patch size (seeFig. 2). A straightforward solution is to fuse low-level features from input into different decoder layers, just like in GFP- GAN [36]. Although the input features could bring more fidelity information, they will also \"contaminate\" the realistic details generated from the VQ codebook. This problem leads to our second network design -a parallel decoder. Specifically, the parallel decoder structure consists of a texture decoder and a main decoder. The texture decoder only receives information from the latent representations from the VQ codebook, while the main decoder warps the features from the texture decoder to match the information from degraded input. In order to eliminate the loss of high-quality details and better match the degraded faces, we further adopt a texture warping module with deformable convolution [48]  in the main decoder. Equipped with the VQ codebook as a facial dictionary and the parallel decoder design, we can achieve more high-quality facial details while reserving the fidelity for face restoration.Our contributions are summarized as follows:\n\nIntroduction\n\nBlind face restoration aims at recovering low-quality (LQ) faces with unknown degradations, such as noise [45], blur [23,32], down-sampling [11,27], etc. This task becomes more challenging in real-world scenarios, where there are more complicated degradations, diverse face poses and expressions. Previous works typically exploit face-specific priors, including geometric priors [7,44,6], generative priors [36,42] and reference priors [25,26,10]. Specifically, geometric priors usually consist of facial landmarks [7], face parsing maps [7,6] and facial component heatmaps [44]. They could provide global guidance for restoring accurate face shapes, but do not help generate realistic details. Besides, geometric priors are estimated from degraded images and thus become inaccurate for inputs with severe degradations. These properties motivate researchers to find better priors.\n\nRecent works [36,42] begin to investigate generative priors in face restoration and achieve superior performance. They usually leverage the powerful generation ability of a pre-trained face generative adversarial network (e.g., StyleGAN [20,21]) to generate realistic textures. These methods typically project the degraded images back into the GAN latent space, and then decode high-quality (HQ) faces with the pre-trained generator. Although GAN-prior-based methods achieve decent overall restoration quality at first glance, they still fail to produce fine-grained facial details, especially the fine hair and delicate facial components (see examples in Fig. 1). This can be partially attributed to the imperfect latent space of the well-trained GAN model. Reference-based methods explore the high-quality guided faces [10,26] or facial component dictionary [25] to solve face restoration problems. DFDNet [25] is a representative method, which does not need to access the faces of the same identity. It explicitly establishes a high-quality \"texture bank\" for several facial components and then replaces degraded facial components with the nearest HQ facial components in the dictionary. Such a discrete replacement operation directly bridges the gap between the low-quality facial components and high-quality ones, thus having the potential to provide decent facial details. However, the facial component dictionary in DFDNet still has two weaknesses. 1) It offline generates the facial component dictionary with a pre-trained VGGFace [4] network, which is optimized for the recognition task but is sub-optimal for restoration. 2) It only focuses on several facial components (i.e., eyes, nose, and mouth), but does not include other important areas, such as hair and skin.\n\n1. We propose the VQ dictionary of HQ facial details for face restoration. Our analysis of the VQ codebook shows the potential and limitations of the VQ codebook, together with the importance of the compression patch sizes in face restoration. 2. A parallel decoder is proposed to gradually fuse input features and texture features from VQ codebooks, which keeps the fidelity without sacrificing HQ facial details. 3. Extensive experiments with quantitative and qualitative comparisons show VQFR largely surpasses previous works in restoration quality while keeping high fidelity.\n\n\nRelated Work\n\nBlind Face Restoration Early works explore different facial priors in face restoration. Those priors can be categorized into three types: geometric priors [7,44,6], generative priors [36,42] and reference priors [25,26,10]. The geometric priors include facial landmark [7], face parsing maps [7,6] and facial component heatmaps [44]. Those priors are estimated from degraded images and thus become inaccurate for inputs with severe degradations. Besides, the geometric structures cannot provide sufficient information to recover facial details.\n\nIn this work, we do not explicitly integrate geometric priors, but we use landmark distance to estimate restoration fidelity. Recent works investigate the generative priors to provide facial details and achieve decent performance. In the early arts, GAN inversion methods [28,16] [4] for face recognition. It then conducts discrete replacement, i.e., replacing the degraded facial components with high-quality ones in the dictionary by the nearest search. With the facial component dictionary, DFDNet [25] restores better facial components. However, it still has two limitations: 1) The dictionary is offline generated with a recognition model, which is sub-optimal for face restoration. 2) It only builds on several facial components (i.e., eyes, nose and mouth), leaving other facial regions like skin and hair untouched. In this work, we explore the VQ codebook as a facial dictionary, which can be end-to-end trained by a reconstruction objective and provide realistic facial details. Recent RestoreFormer [38] also exploits the VQ codebook, but their work mainly discusses the diverse cross-attention mechanism for LQ latent and HQ code interaction. It does not explore deeply in VQ codebook while we show the dilemma between fidelity and realness when using the VQ codebook of different scales. Moreover, restoreformer does not exploit the input feature of larger resolution and thus results in limited restoration fidelity. Instead, we explore the parallel decoder to achieve realness and fidelity simultaneously.\n\n\nVector-Quantized Codebook\n\nThe vector-quantized codebook is first introduced in VQ-VAE [34]. With this codebook, the encoder network outputs are discrete rather than continuous, and the prior encapsulated in the codebook is learned rather than static. The following works propose different improvements to codebook learning. VQVAE2 [31] introduces a multiscale codebook for better image generation. VQGAN [13] trains the codebook with the adversarial objective and thus the codebook can achieve high perceptual quality. To improve the codebook usage, some works [24,43] explore training techniques like L2normalization or periodically re-initialization. Such a VQ codebook is a patch tokenizer and can be adopted in multiple tasks, like image generation [5,43,13], multi-modal generation [39] and large-scale pretraining [12,1]. Different from previous works that use the VQ codebook to get token features, we explore the potential of the VQ codebook as an HQ facial details dictionary. In the reconstruction experiment, we analyze the HQ reconstruction (row 1) and LQ reconstruction (row 2) based on pretrained HQ codebook. In the restoration experiment, we visualized the restoration result of faces of large degradation (row 3) and small degradation (row 4), respectively.\n\n\nMethodology\n\nWe describe the proposed VQFR framework in this section. Our goal is to restore highquality faces with realistic facial details while reserving the fidelity of degraded faces. To achieve this goal, VQFR explores two key ingredients: Vector-Quantized (VQ) dictionary and parallel decoder. The overview of VQFR framework is illustrated in Fig. 3. VQFR contains an encoder, a parallel decoder and a pretrained HQ codebook. We first learn a VQ codebook from only HQ faces with an encoder-decoder structure by the vector-quantization technique [34,13]. Then, a degraded face is encoded to a compact latent representation with a downsampling factor f . At each spatial location of this latent representation, we replace the latent vector with its nearest code in the HQ codebook. After that, the substituted latent representation is then decoded back to the image space, i.e., restored image with high-quality facial details. The remaining parts are arranged as follows: In Sec. 3.1, we first analyze the potential and limitation of vector-quantization technique as a texture dictionary in face restoration task. Then we introduce the parallel decoder to promote the fidelity while maintaining the rich high-quality facial details in Sec. 3.2. In Sec. 3.3, we describe the overall training objective of VQFR framework.\n\n\nVector-Quantized Codebook\n\nPreliminary. The Vector-Quantized (VQ) codebook is first introduced in VQVAE [34], which aims to learn discrete priors to encode images. The following work VQGAN [13] proposes a perceptual codebook by further using perceptual loss [19] and adversarial training objectives [18]. We briefly describe the VQGAN model with its codebook in this section, and more details can be found in [13]. VQGAN is comprised of an encoder E, a decoder G and a codebook Z = {z k } K k=1 with K discrete codes. For an input image x \u2208 R H\u00d7W \u00d73 , the encoder E maps the image x to its spatial latent representation\u1e91 = E(x) \u2208 R h\u00d7w\u00d7nz , where n z is the dimension of latent vectors. The vector-quantized representation z q is then obtained by applying element-wise quantization q(\u00b7) of each spatial code\u1e91 ij \u2208 R nz onto its closest codebook entry z k :\nz q = q(\u1e91) := arg min z k \u2208Z \u2225\u1e91 ij \u2212 z k \u2225 \u2208 R h\u00d7w\u00d7nz .(1)\nThe decoder G maps the quantized representation z q back to the image space, and the overall reconstructionx \u2248 x can be formulated as:\nx = G(z q ) = G (q(E(x))) .(2)\nThe encoder E maps images of size H \u00d7W into discrete codes of size H/f \u00d7W/f , where f denotes the downsampling factor. It can be regarded as compressing each f \u00d7f patch in the image x into one code. In other words, for each code in z q , f also denotes the corresponding spatial size in the original image x. We name this downsampling factor f as the compression patch size in our paper.\n\nSince the quantization operation in Equ. 2 is discrete and non-differentiable, VQ-GAN adopts straight-through gradient estimator [2], which simply copies the gradients from the decoder to the encoder. Thus, the model and codebook can be trained end-toend via the loss function L vq . VQGAN also employs perceptual loss and adversarial loss to encourage reconstructions with better perceptual quality. The full training objective of VQGAN and its codebook is: \nL(E, G, Z) = \u2225x \u2212x\u2225 1 + \u2225sg[E(x)] \u2212 z q \u2225 2 2 + \u03b2\u2225sg[z q ] \u2212 E(x)\u2225 2 2 Lvq +L per + L adv ,(3)[E(x)] \u2212 z q \u2225 2 2 . \u03b2\u2225sg[z q ] \u2212 E(x)\u2225 2 2\nis the commitment loss [34] to reduce the discrepancy between the encoded latent vectors and codes, where \u03b2 is the commitment weight and set to 0.25 in all experiments.\n\nAnalysis In order to better understand the potential and limitations of VQ codebooks for face restoration, we conduct several preliminary experiments and draw the following observations.\n\nObservation 1: Degradations in LQ faces can be removed by VQ codebooks trained only with HQ faces, when we adopt a proper compression patch size f . Following [13] and Equ. 3, we first train VQGAN with perceptual codebooks on HQ faces using different compression patch sizes f = {8, 16, 32, 64}. All our experiments are conducted on 512 \u00d7 512 input faces. The illustration of training architectures is shown in Fig. 4. We then examine the reconstruction quality for different compression patch sizes f . Note that the reconstruction output is expected to be the same as the input. As shown in row 1 of Fig. 2, the reconstruction quality of HQ faces is as expected. The reconstruction quality of HQ faces exhibits a reasonable trend: a smaller compression patch size f will lead to more faithful outputs. After that, we are curious whether the codebook trained only on HQ faces can also reconstruct LQ faces. We use LQ faces as inputs and examine the outputs. Interestingly, we can observe that under the compression patch sizes f = 32, the degradation in LQ faces can be removed (see row 2 in Fig. 2). This is because the codebook trained only on HQ faces has almost no degradation-related codes. During reconstruction, the vector quantization operation can replace the \"degraded\" vectors of inputs with the \"clean\" codes in codebooks.\n\nSuch a phenomenon shows the potential of the VQ codebook. However, it only happens with a large compression patch size f , as the codebook with a small compression patch size cannot well distinguish the degradations and detailed textures. In the extreme, for f = 1 with each pixel having a quantized code, both the degradation and detailed textures can be well recovered by a combination of codebook entries. On the other hand, a too large compression patch size (e.g., f = 64) will result in a significant change in identity, even it could also reconstruct \"clean\" face images.\n\nObservation 2: When training for the restoration task, there is also a trade-off between improved detailed textures and fidelity changes. We then investigate the VQ codebooks in face restoration, i.e., training with LQ-HQ pairs as most face restoration works do [36]. Based on the trained model for reconstruction, we then fix the VQ codebooks, and finetune the encoder and decoder with LQ-HQ pairs (the training details are the same as that in Sec. 4.1). We denote this simple VQ model for face restoration as SimVQFR. We can observe from Fig. 2 that there is still a trade-off between improved detailed textures and fidelity changes. The key influential factor is the compression patch size f . With a small f (i.e., {8, 16}), the SimVQFR model fails to remove degradations and cannot recover sufficient detailed textures. While with a large f (i.e., {32, 64}), the textures are largely improved but the fidelity (i.e., expression and identity) also changes a lot by the codebook. Our choice. Based on the above analysis, we can conclude that the VQ codebook, as a texture dictionary, has its value in generating high-quality facial textures and removing input degradations. However, there is a trade-off between the improved detailed textures and fidelity changes. The key influential factor is the compression patch size f . In order to better leverage the strength of generating high-quality facial textures, we choose the compression patch size f = 32 for 512 \u00d7 512 input faces. The left problem is how to preserve the fidelity with the VQ codebook of f = 32. We will present our parallel decoder solution to address this problem in Sec. 3.2.\n\u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 Fixed Params VQ Codebook Encoder Texture Decoder Main Decoder (a) VQGAN (b) SimVQFR (c) Single Branch (d) VQFR xx x d x r x d x r x d x m x t f = 1 f = 2 f = 4 \u00b7 \u00b7 \u00b7 f = 32\n\nParallel Decoder\n\nThe VQ codebook with the compression patch size of f = 32 can be used as a texture bank to provide realistic facial details, but it also brings the problem of fidelity changes. From the results in Fig. 2, we can find that the position of facial components and the facial lines are changed, making the expression and identity largely deviate from the inputs. A straightforward solution is to integrate the feature information from degraded inputs to help improve the fidelity. However, simply fusing input features into the decoder with a single branch (as shown in Fig. 4) will lead to inferior details (see Fig. 8). In other words, such a single branch fusion strategy tends to corrupt the generated highquality details. Though input features can bring more fidelity information, these features also contain input degradations. During feature propagation, i.e., the upsampling process from small spatial size to large spatial size, the intermediate features will largely be influenced by input features, and gradually contain fewer high-quality facial details from the VQ codebook.\n\nTo overcome the dilemma of keeping the realistic facial details and promoting fidelity, we propose a parallel decoder structure with a texture warping module. The core idea of the parallel decoder (Fig. 4) is to decouple the two goals of face restoration, i.e., generating high-quality facial details and keeping the fidelity. As shown in Fig. 3, given a degraded face x d \u2208 R H\u00d7W \u00d73 , we first encode it to the latent vector z d by\nz d = E(x d ).\nThe encoder consists of several residual blocks and downsampling operations. Then we replace the code z d with the HQ codebooks by Equ. 1 to get the quantized code z d q . Since the quantized code z d q is from the HQ codebook, it contains HQ facial details without degradations. In order to keep its realistic textures, we use a texture decoder G t to decode it back to image space\nx t = G t (z d q )\n. We denote the multi-level feature of the texture branch as F t = {F t i }. Since the texture branch only decodes from the HQ code, F t can keep realistic facial details.\n\nThe main branch decoder G m aims to generate faces x m with high fidelity while having the realistic facial details from the texture decoder G t . As illustrated in Fig. 3, the main branch decoder G m warps the texture feature F t based on the input feature extracted from degraded inputs at multiple spatial levels. We use the input feature with the largest spatial resolution as it retains the richest fidelity information of degraded faces. We then directly downsample the input feature to different resolution levels to get the multi-level features of degraded faces\nF d = {F d i }.\nFor the i-th resolution level, we first warp F t i with high-quality facial details towards F d i by a texture warping module, which will be described later. After that, we fuse the warped feature F w i and the upsampled feature from F i\u22121 , to get the F i feature in the main decoder. The process can be formulated as:\nF w i = T W M (F d i , F t i ), F i = Conv(Concat(U psample(F i\u22121 ), F w i )),(4)\nwhere T W M is the texture warping module. Our parallel decoder shares the same spirits as reference-based restoration [26]. In our case, the features from the texture decoder serve as the reference features containing high-quality details. Unlike reference-based restoration, our method does not require extra high-quality images with rich textures. Instead, the parallel decoder learns the main feature and \"reference\" feature jointly. Texture Warping Module (TWM) The texture warping module aims to warp realistic facial details to match the fidelity of degraded inputs, especially the position of facial components and expressions. There are two inputs of TWM, one is the input feature F d and the other is the texture feature F t . As analyzed above, the texture features are decoded from the HQ codebook and have high-quality facial details. But their fidelity probably deviates from inputs. Therefore, we adopt a deformable convolution [48] to better warp the realistic facial details F t towards the input feature F d . Specifically, we first concatenate those two features to generate offsets. Then the offset is used in the deformable convolution to warp the texture features to match the fidelity of input, which can be formulated as:\nof f set = Conv(Concat(F d i , F t i )), F w i = Dconv(F t i , of f set),(5)\nwhere Dconv denotes the deformable convolution. We also adopt a separable convolution with a large kernel size to model large position offsets between texture features and input features.\n\n\nModel Objective\n\nThe training objective of VQFR consists of 1) pixel reconstruction loss that constraints the restored outputs close to the corresponding HQ faces; 2) code alignment loss that forces the codes of LQ inputs to match the codes of the corresponding HQ inputs; 3) perceptual loss to improve the perceptual quality in feature space; and 4) adversarial loss for restoring realistic textures. We denote the degraded face as x d , decoder restored results as x r = {x t , x m } and the ground truth HQ image as x h . The loss definitions are as follows.\n\nPixel Reconstruction Loss. We use the widely-used L1 loss in the pixel space as the reconstruction loss, which is denoted as: L pix = \u2225x r \u2212 x h \u2225 1 . Empirically, we find that with the input feature of degraded faces, the pixel reconstruction loss has a negative impact on facial details. Therefore, we discard pixel reconstruction loss in the main decoder.\n\nCode Alignment Loss. The code alignment loss aims to improve the performance of matching the codes of LQ images with codes of HQ images. We adopt the L2 loss to measure the distance, which can be formulated as:\nL code = \u2225z d \u2212 z h q \u2225 2 2 ,\nwhere z h q is the ground truth code obtained from encoding HQ face to the quantized code by pretrained VQGAN. Perceptual Loss. We use the widely used perceptual loss [19,46] for both two decoder outputs: L per = \u2225\u03d5(x r ) \u2212 \u03d5(x h )\u2225 1 + \u03bb style \u2225Gram(\u03d5(x r )) \u2212 Gram(\u03d5(x h ))\u2225 1 , where \u03d5 is the pretrained VGG-16 [33] network and Gram means the Gram matrix [15]. The former term measures the content distance and the latter term measures the style difference between the restoration results and the corresponding HQ images. Adversarial Loss. We employ the global discriminator D g in SWAGAN [14] to encourage VQFR to favor the solutions in the natural image manifold and generate realistic textures. In order to increase the local quality of facial details, we further adopt the local discriminator D l of PatchGAN [18] in our training. The objectives of the global and local discriminators are defined as\nL global adv = \u2212E x r [softplus(D g (x r ))], L local adv = E x r [log D l (x h ) + log(1 \u2212 D l (x r ))].(6)\nTotal objective. The total training objective is the combination of above losses:   and periodically re-initialize the codebook with k-means clustering. More implementation details are provided in the Sec. 6.\nL total = \u03bb pix L pix + \u03bb code L code + \u03bb per L per + \u03bb global L global adv + \u03bb local L local adv ,(7)\nIn the first stage of codebook training, we adopt ADAM [22] optimizer with a learning rate of 1e \u2212 4. The training iteration is set to 800K with a batch size of 16. For the second stage training for restoration, the loss weights are set to \u03bb pix =\u03bb code =\u03bb per =1, \u03bb style =2000 and the adversarial loss weights are set to \u03bb local =\u03bb global = 0.5. We train VQFR for 200K iterations by ADAM optimizer with the learning rate of 1e \u2212 5. , we conduct experiments on the synthetic dataset CelebA-Test and three real-world datasets -LFW-Test, CelebChild-Test and WebPhoto-Test. These datasets have diverse and complicated degradations. All these datasets have no overlap with the training dataset. Evaluation Metrics. Our evaluation metrics contain two widely-used non-reference perceptual metrics: FID [17] and NIQE [29]. We also measure the pixel-wise metrics (PSNR and SSIM) and perceptual metric (LPIPS [47]) for benchmarking CelebA-Test with Ground-Truth (GT). We follow previous work [36] to use the embedding angle of ArcFace [8] as the identity metric, which is denoted by 'Deg.'. In order to better measure the fidelity with accurate facial positions and expressions, we further adopt landmark distance (LMD) as the fidelity metric. More details are provided in the Sec. 6.\n\n\nComparisons with State-of-the-art Methods\n\nWe compare our VQFR with several state-of-the-art face restoration methods: Wan et al.  Table. 1, VQFR achieves the lowest LPIPS, implying that VQFR generates the restored faces with the closest perceptual quality to ground-truth. VQFR also achieves the best FID and NIQE, with a large improvement over GFP-GAN, demonstrating that results of VQFR are closer to real faces and have more realistic details. For fidelity, VQFR can achieve comparable landmark distance and identity degree to GFP-GAN, showing that it can recover accurate facial expressions and detail positions. Qualitative results are presented in Fig. 5. Thanks to the VQ codebook design, VQFR generates high-quality facial components like eyes and mouth as well as other facial regions.VQFR also maintains the fidelity with the help of the parallel decoder. Real-World LFW, CelebChild, and WedPhoto-Test. We evaluate VQFR on three real-world test datasets to test the generalization ability. Table. 2 shows the quantitative results. It is observed that VQFR largely improves the realness and perceptual quality of all three real-world datasets. PULSE [28] achieves a higher perceptual quality on CelebChild, but its fidelity is severely affected. From the qualitative results in Fig. 6, the face restored by VQFR is of the most high quality on different facial regions.\n\n\nAblation Study\n\nImportance of input features from degraded faces. The SimVQFR model directly uses the VQ codebook without exploiting input features. As shown in Fig. 7(a)  it can achieve high perceptual quality (low FID and NIQE), its landmark distance is large, indicating the low fidelity. After incorporating the input features of degraded faces, the fidelity improves significantly. From the visualization of Fig. 8(a), we can observe that the facial lines of SimVQFR are deviated from LQ faces, resulting in an expression change. Such a phenomenon can also be observed in the examples in Fig. 3, where the texture decoder output changes the woman's expression apparently.\n\nImportance of parallel decoder. When comparing Variant 1 and Variant 2, the NIQE (which favors high-quality details) of Variants 1 is higher and clearly inferior to Variant 2 ( Fig. 7 (a)). From the visualization in Fig. 8(b), we observe that, in Variant 1 without the parallel decoder, the eyes and hair lose high-frequency details and are biased to degraded ones. While with the parallel decoder, the details are clearer and more realistic.\n\nTexture Warping Module. As shown in Fig. 7 (a)   of Fig. 8(c), the TWM module helps accurately warp the high-quality facial details to better match the degraded input.\n\n\nConclusion\n\nIn this paper, we propose the vector-quantized (VQ) dictionary of high-quality facial details for face restoration. Our analysis of the VQ codebook shows the potential and limitations of the VQ codebook. In order to keep the fidelity without the loss of highquality facial details, a parallel decoder is further proposed to gradually fuse input features and texture features from VQ codebooks. Equipped with the VQ codebook as a dictionary and the parallel decoder, our proposed vector-quantized face restoration (VQFR) can produce high-quality facial details while preserving fidelity. Extensive experiments show that our methods surpass previous works on both synthetic and realworld datasets. \n\n\nAppendix\n\nIn this section, we first present the architecture details of VQFR in Sec. 6.1. Then we give more details about evaluation metrics in Sec. 6.2. The limitations of VQFR are discussed in Sec. 6.3. We then provide more visual comparisons of ablation studies to help better understand the VQFR designs in Sec. 6.4. More visual comparisons with previous methods on the real-world datasets are shown in Sec. 6.5.\n\n\nNetwork Architectures\n\nVQFR: The detailed architecture of VQFR is illustrated in Table. 3. There are six resolution levels, i.e., f = {1, 2, 4, 8, 16, 32}, and the quantization operation is conducted on the feature level of f 32. Each level of the encoder contains two residual blocks, and each level of the texture branch in the decoder contains three residual blocks. Each level of the main branch in the decoder contains one texture warping module and one residual block. We use a bilinear upsample/downsample followed by a 1\u00d71 convolution to change the resolutions. VQFR has 76.3M params (1.07 TFlops) and takes 0.36s to process a 512 2 image on Nvidia A100.  \n\n\nEvaluation Metrics\n\nOur evaluation metrics contain two widely-used non-reference perceptual metrics: FID [17] and NIQE [29]. We also measure the pixel-wise metrics (PSNR and SSIM) and perceptual metric (LPIPS [47]) for benchmarking CelebA-Test with Ground-Truth (GT). However, as pointed out in [3], the distortion measure (e.g., PSNR, SSIM) and perceptual quality are at odds with each other. Similar to GFP-GAN [36], we pursue perceptual quality in VQFR and provide PSNR/SSIM for reference only. In the Table. 1 of the main manuscript, the best PSNR and SSIM are achieved by degraded inputs, as all other methods are optimized for the perceptual quality instead of the distortion measures.\n\nFor fidelity measurement, we follow previous work [36] to use the embedding angle of ArcFace [8] as the identity metric, which is denoted by 'Deg.'. However, this Deg. metric actually cannot well reflect the fidelity due to the following reasons. 1) The Arc-Face model downsamples the face images into 128\u00d7128 during inference, which loses the spatial dimension. Thus, it cannot evaluate detailed facial positions. 2) The ArcFace is designed for the recognition task and is trained with the invariance to expressions. While the expression is important for measuring fidelity in face restoration. In order to better measure the fidelity with accurate detailed facial positions and expressions, we further adopt landmark distance (LMD) as the fidelity metric. Specially, we use AWing [37] to obtain 98 landmarks for both the restored face and the ground-truth face. Then we calculate the L2 distance for each landmark and average the distance as the final score of the LMD metric.\n\n\nLimitation\n\nThe limitations of VQFR are two-folds. 1) As shown in Fig. 9(a), faces in extreme poses lead to poor restoration results, since the codebook is built from the training dataset, in which most samples are frontal faces. One potential solution is to increase the dataset diversity and codebook size, which will help build a more comprehensive dictionary. 2) As shown in Fig. 9(b), the restoration from extremely less informative faces is far  from satisfactory, since the VQFR does not build upon a generative model. Moreover, VQ may further lead to divergent codebook quantization due to the less informative inputs. One promising direction to improve is to equip VQFR with generation ability. For example, when the input faces contain extreme low-information and thus the code mapping is ambiguous, the auto-regressive [13] or bi-directional [9] transformer can help model the code selection.\n\n\nMore Visualizations of Ablation Study\n\nImportance of input features of degraded faces. Input features of degraded faces play an important role in preserving fidelity. We compare the SimVQFR without input features and our VQFR with input features. As shown in Fig. 10, with the input features of degraded faces, VQFR could generate more faithful expressions (the first and fourth row), more faithful facial lines (the second and forth row), and facial components (the third row) than SimVQFR. The facial lines and components can be roughly recovered from the input LQ faces but can be easily changed by the discrete quantization, thus influencing the final recovered expressions and identity. Our VQFR incorporates the input features from degraded faces at different spatial levels and preserve better fidelity. Importance of the parallel decoder. The parallel decoder is the key design of VQFR to preserve high-quality facial details when fusing texture features of the VQ codebook and input features from degraded faces. We compare the variant-1 (single branch) and variant-2 (parallel decoder) in Fig. 11. With the parallel decoder, variant-2 could generate high-quality facial components (the first row), realistic hairs (the first row) and skins (the second row). In Fig. 12, we provide more visual examples to show the importance of the parallel decoder design in generating realistic skins (the first and second rows) high-quality hairs and eyes (the third and fourth rows). Influence of dual discriminators. We adopt dual discriminators to remove the regular pattern when utilizing facial textures of the VQ codebook in VQFR. We adopt stylebased wavelet-driven discriminator [14] as the global discriminator and adopt Patch-GAN discriminator [18] as the local discriminator. We show the influence of dual discriminators in Fig. 13. When we only use the global discriminator (the second column), we can find that there are regular patterns on skin and hair. When adding the patch discriminator as the local discriminator, the regular patterns are removed (the third column).  Fig. 11: Comparisons between the Variant-1 (single branch) and Variant-2 (parallel decoder). With the proposed parallel decoder, high-quality facial details from the VQ codebook could be preserved. Therefore, Variant-2 could generate better facial components (the first row), more realistic hairs (the first row) and skin (the second row) than Variant-1.\n\n\nInput\n\nVariant-1 (Single Branch) Variant-2 (Parallel Decoder) Fig. 12: Comparisons between the Variant-1 (single branch) and Variant-2 (parallel decoder). With the parallel decoder, Variant-2 could generate more realistic skins (the first and second rows), eyes (the third and fourth rows) and hairs (the third and fourth rows) than Variant-1.\n\n\nInput\n\nGlobal D Global D + Local D Fig. 13: Influence of dual discriminators. With only the global discriminator (the second column), there are regular patterns on hairs and skins. When adding the patch discriminator as a local discriminator, the regular patterns are removed (the third column).\n\n\nMore Qualitative Results on Real-World Data\n\nWe show more qualitative results on the real-world dataset, i.e., LFW-Test, CelebChild and WebPhoto. We compare our VQFR with several state-of-the-art face restoration methods: DFDNet [25], PSFRGAN [6], PULSE [28] and GFPGAN [36]. The qualitative comparisons on the WebPhoto are shown in Fig. 14, Fig. 15 and Fig. 16. The qualitative comparisons on the CelebChild are present in Fig. 17 and Fig. 18. Moreover, qualitative comparisons on LFW-Test are shown in Fig. 19, Fig. 20 and Fig. 21. Our VQFR produces high-quality facial components and more realistic hairs and skins than previous methods.  Fig. 21: Qualitative comparison on the real-world LFW-Test dataset. Our VQFR could restore high-quality facial components (eyes) and more realistic skins and hairs than previous methods. (Zoom in for best view).\n\n\nVQFR(ours)\n\nFig. 1 :\n1Comparisons of restoration quality between GFP-GAN [36] and VQFR. Our VQFR can restore high-quality facial details on various facial regions and keep the fidelity as well, while GFP-GAN lacks realistic fine details. (Zoom in for best view)\n\nFig. 2 :\n2Reconstruction and restoration results based on codebook with compression patch size f = {8, 16, 32, 64}.\n\nFig. 3 :\n3Overview of VQFR framework. It consists of an encoder to map degraded face into latent and a parallel decoder to exploit the HQ code and input feature. The encoder and decoder are bridged by vector quantization model and a pretrained HQ codebook to replace the encoded latent to HQ code.\n\nFig. 4 :\n4Illustration of the architecture variants. (a) The VQGAN structure is used in codebook training. (b) The SimVQFR structure. (c) Single branch decoder. (d) The proposed parallel decoder.\n\n\nwhere the \u03bb pix , \u03bb code , \u03bb per , \u03bb global and \u03bb local are scale factors of corresponding loss.\n\n\nWe implement the VQGAN and VQFR with six resolution levels, i.e., {1, 2, 4, 8, 16, 32}. For the VQ codebook, we use 1024 codebook entries with 256\n\n\nTraining Datasets. The VQFR is trained on the FFHQ dataset [20] including 70,000 high-quality faces. The images are resized to 512 2 during training. Following the common practice in [25,26,36], we use the following degradation model to synthesize training pairs: x = [(y \u229b k \u03c3 ) \u2193 r +n \u03b4 ] JPEGq , where \u03c3, r, \u03b4 and q are randomly sampled from {0.2 : 10}, {1 : 8}, {0 : 15} and {60 : 100}, respectively. Testing Datasets. Following the practice in GFP-GAN [36]\n\nFig. 6 :\n6Qualitative comparisons on three real-world datasets. Zoom in for best view.\n\n\nVisualization of the balance between realness of fidelity of different configurations.\n\nFig. 7 :Fig. 8 :\n78Quantitative ablation studies of key designs in VQFR.Input (c) Ablation of TWM for fidelity and realness.Qualitative ablation studies of key designs on VQFR.\n\n19 .\n19Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and superresolution. In: European conference on computer vision. pp. 694-711. Springer (2016) 6, 10 20. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4401-4410 (2019) 2, 4, 11 21. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8110-8119 (2020) 2 22. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 11 23. Kupyn, O., Budzan, V., Mykhailych, M., Mishkin, D., Matas, J.: Deblurgan: Blind motion deblurring using conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 8183-8192 (2018) 1 24. \u0141a\u0144cucki, A., Chorowski, J., Sanchez, G., Marxer, R., Chen, N., Dolfing, H.J., Khurana, S., Alum\u00e4e, T., Laurent, A.: Robust training of vector quantized bottleneck models. In: 2020 International Joint Conference on Neural Networks (IJCNN). pp. 1-7. IEEE (2020) 4, 11 25. Li, X., Chen, C., Zhou, S., Lin, X., Zuo, W., Zhang, L.: Blind face restoration via deep multiscale component dictionaries. In: European Conference on Computer Vision. pp. 399-415. Springer (2020) 1, 2, 3, 4, 11, 12, 26 26. Li, X., Liu, M., Ye, Y., Zuo, W., Lin, L., Yang, R.: Learning warped guidance for blind face restoration. In: Proceedings of the European conference on computer vision (ECCV). pp. 272-289 (2018) 1, 2, 3, 4, 9, 11 27. Lim, B., Son, S., Kim, H., Nah, S., Mu Lee, K.: Enhanced deep residual networks for single image super-resolution. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 136-144 (2017) 1 28. Menon, S., Damian, A., Hu, S., Ravi, N., Rudin, C.: Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In: Proceedings of the ieee/cvf conference on computer vision and pattern recognition. pp. 2437-2445 (2020) 4, 11, 12, 26 29. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a \"completely blind\" image quality analyzer. IEEE Signal processing letters 20(3), 209-212 (2012) 11, 19 30. Ramachandran, P., Zoph, B., Le, Q.V.: Searching for activation functions. arXiv preprint arXiv:1710.05941 (2017) 18 31. Razavi, A., Van den Oord, A., Vinyals, O.: Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems 32 (2019) 4 32. Shen, Z., Lai, W.S., Xu, T., Kautz, J., Yang, M.H.: Deep semantic face deblurring. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 8260-8269 (2018) 1 33. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 10 34. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. Advances in neural information processing systems 30 (2017) 4, 5, 6, 7 35. Wan, Z., Zhang, B., Chen, D., Zhang, P., Chen, D., Liao, J., Wen, F.: Bringing old photos back to life. In: proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2747-2757 (2020) 11, 12 36. Wang, X., Li, Y., Zhang, H., Shan, Y.: Towards real-world blind face restoration with generative facial prior. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9168-9178 (2021) 1, 2, 3, 4, 8, 11, 12, 19, 26 37. Wang, X., Bo, L., Fuxin, L.: Adaptive wing loss for robust face alignment via heatmap regression. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6971-6981 (2019) 19 38. Wang, Z., Zhang, J., Chen, R., Wang, W., Luo, P.: Restoreformer: High-quality blind face restoration from undegraded key-value pairs. arXiv preprint arXiv:2201.06374 (2022) 4 39. Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., Duan, N.: N\\\" uwa: Visual synthesis pre-training for neural visual world creation. arXiv preprint arXiv:2111.12417 (2021) 4 40. Wu, Y., He, K.: Group normalization. In: Proceedings of the European conference on computer vision (ECCV). pp. 3-19 (2018) 18 41. Yang, L., Wang, S., Ma, S., Gao, W., Liu, C., Wang, P., Ren, P.: Hifacegan: Face renovation via collaborative suppression and replenishment. In: Proceedings of the 28th ACM International Conference on Multimedia. pp. 1551-1560 (2020) 11, 12 42. Yang, T., Ren, P., Xie, X., Zhang, L.: Gan prior embedded network for blind face restoration in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 672-681 (2021) 1, 2, 3, 4 43. Yu, J., Li, X., Koh, J.Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., Wu, Y.: Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627 (2021) 4 44. Yu, X., Fernando, B., Ghanem, B., Porikli, F., Hartley, R.: Face super-resolution guided by facial component heatmaps. In: Proceedings of the European conference on computer vision (ECCV). pp. 217-233 (2018) 1, 3, 4 45. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing 26(7), 3142-3155 (2017) 1 46. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018) 10 47. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586-595 (2018) 11, 19 48. Zhu, X., Hu, H., Lin, S., Dai, J.: Deformable convnets v2: More deformable, better results. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9308-9316 (2019) 3, 9, 18\n\n\non faces of extremely poses. (b) Results on extremely less informative faces.\n\nFig. 9 :\n9Limitations of VQFR.\n\nFig. 14 :Fig. 15 :Fig. 16 :Fig. 17 :Fig. 18 :Fig. 19 :Fig. 20 :\n14151617181920Qualitative comparison on the real-world WebPhoto dataset. Our VQFR could restore more realistic facial components (eyes and ears) than previous methods. (Zoom in for best view). Qualitative comparison on the real-world WebPhoto dataset. Our VQFR could restore more realistic facial components (eyes and ears) and more realistic skins than previous methods. (Zoom in for best view). Qualitative comparison on the real-world WebPhoto dataset. Our VQFR could restore more realistic facial components (eyes and ears) and more realistic skins and hairs than previous methods. (Zoom in for best view). Qualitative comparison on the real-world Celeb-Child dataset. Our VQFR could restore more realistic eyes and hairs than previous methods. (Zoom in for best view). Qualitative comparison on the real-world Celeb-Child dataset. Our VQFR could restore more realistic eyes and hairs than previous methods. (Zoom in for best view). Qualitative comparison on the real-world LFW-Test dataset. Our VQFR could restore high-quality facial components (eyes and hairs) and more realistic skins than previous methods. (Zoom in for best view). Qualitative comparison on the real-world LFW-Test dataset. Our VQFR could restore high-quality facial components (eyes) and more realistic skins than previous methods. (Zoom in for best view).\n\n\naim to find the closest latent vector in the GAN space given an input image. Within this category, PULSE [28] iteratively optimizes the latent code of a pre-trained StyleGAN [20]. mGANprior [16] simultaneously optimize several codes to promote its reconstruction. Recent works GFP-GAN [36] and GPEN [42] extract fidelity information from inputs and then leverage the pre-trained GAN as a decoder, which achieves a good balance between visual quality and fidelity. Those methods still fail to produce fine-grained facial details. We conjecture that StyleGAN constructs a continuous latent space, and thus GAN-prior methods easily project degraded faces into a suboptimal latent code. Reference priors [25,26,10] typically rely on reference images of the same identity. To address this issue, DFDNet [25] constructs an offline facial component dictionary [25] with VGGFace\n\n\nwhere \u2225x \u2212x\u2225 1 is the reconstruction loss and sg[\u00b7] denotes stop gradient operation. The codebook is updated by \u2225sg\n\nTable 1 :\n1Quantitative comparison on the CelebA-Test dataset for blind face restoration. Red and blue indicates the best and second best performance.Fig. 5: Qualitative comparisons on the CelebA-Test. VQFR is able to restore highquality facial details in various facial components, e.g., eyes and mouth. channels in all experiments. In order to increase the codebook usage, we follow [24]Methods \nLPIPS\u2193 FID\u2193 NIQE \u2193 Deg.\u2193 LMD.\u2193 PSNR\u2191 SSIM\u2191 \nInput \n0.4866 143.98 13.440 47.94 3.76 \n25.35 0.6848 \nWan et al. [35] 0.4826 67.58 5.356 43.00 2.92 \n24.71 0.6320 \nHiFaceGAN [41] 0.4770 66.09 4.916 42.18 3.16 \n24.92 0.6195 \nDFDNet [25] \n0.4341 59.08 4.341 40.31 3.31 \n23.68 0.6622 \nPSFRGAN [6] 0.4240 47.59 5.123 39.69 3.41 \n24.71 0.6557 \nmGANprior [16] 0.4584 82.27 6.422 55.45 411.72 24.30 0.6758 \nPULSE [28] \n0.4851 67.56 5.305 69.55 7.35 \n21.61 0.6200 \nGFP-GAN [36] 0.3646 42.62 4.077 34.60 2.41 \n25.08 0.6777 \nVQFR (ours) \n0.3515 41.28 3.693 35.75 2.43 \n24.14 0.6360 \n\nInput \nDFDNet \nPSFRGAN \nPULSE \nGFP-GAN \nVQFR \nGT \n\n\n\nTable 2 :\n2Quantitative comparison on the real-world LFW, CelebChild, WebPhoto. Red and blue indicates the best and second best performance.Dataset \nLFW-Test \nCelebChild \nWebPhoto \nMethods \nFID\u2193 NIQE \u2193 FID\u2193 NIQE \u2193 FID\u2193 NIQE \u2193 \nInput \n137.56 11.214 144.42 9.170 170.11 12.755 \nWan et al. [35] 73.19 5.034 115.70 4.849 100.40 5.705 \nHiFaceGAN [41] 64.50 4.510 113.00 4.855 116.12 4.885 \nDFDNet [25] \n62.57 4.026 111.55 4.414 100.68 5.293 \nPSFRGAN [6] 51.89 5.096 107.40 4.804 88.45 5.582 \nmGANprior [16] 73.00 6.051 126.54 6.841 120.75 7.226 \nPULSE [28] \n64.86 5.097 102.74 5.225 86.45 5.146 \nGFP-GAN [36] 49.96 3.882 111.78 4.349 87.35 4.144 \nVQFR (ours) \n50.64 3.589 105.18 3.936 75.38 3.607 \n\n\n\n\n, thoughLFW-Test \n\nCelebChild-Test \n\nWebPhoto-Test \n\nInput HiFaceGAN DFDNet PSFRGAN \nPULSE \nGFP-GAN \nVQFR \n\n\n\n\n, Variant 2 does not have a TWM module and directly utilizes concatenation fusion of texture features and input features of degraded features. It lacks the ability to dynamically adjust the features with warping. Therefore, Variant 2 without TWM keeps the high-quality textures but changes the fidelity, as it cannot well adjust the fine details and expression. From the visualizationModels \nConfigurations \nCelebA-Test \nInp Feat. Para Dec. TWM FID\u2193 NIQE\u2193 LMD \u2193 \nSimVQFR \n40.51 3.844 2.96 \nVariant 1 \n\u2713 \n39.64 4.019 2.36 \nVariant 2 \n\u2713 \n\u2713 \n42.61 3.714 2.48 \nVQFR \n\u2713 \n\u2713 \n\u2713 41.28 3.693 2.43 \n\n(a) Ablation results on CelebA-Test. Inp Feat.: \ninput feature of degraded faces; Para Dec.: par-\nallel decoder; TWM: texture warping module. \n\n\n\nTable 3 :\n3The detailed architecture of VQFR. The residual block consists of 3\u00d73 Conv-GN [40]-Swish [30]-3\u00d73 Conv-GN-Swish. g: groups in GroupNorm (GN); c: channels; \ndg: deformable groups in deformable convolution [48]; f: compression patch size. \n\n\nTable 4 :\n4the offsets and the texture features are fed into the deformable convolution, outputting the warped feature.The detailed architecture of the texture warping module (TWM). OConv: con-\nvolution for generating offsets; DConv: deformable convolution; c: channels; dg: de-\nformable groups. \n\nInput size \nf 1 : 512\u00d7512 \nf 2 : 256\u00d7256 \nf 4 : 128\u00d7128 \n\nTWM \nOConv: \n\n\uf8f1 \n\uf8f2 \n\n\uf8f3 \n\nConv 1\u00d71, (128+32)-c \u2192 128-c \nDepthwise Conv 7\u00d77, 128-c \nConv 1\u00d71, 128-c \n\n\uf8fc \n\uf8fd \n\n\uf8fe \n\nOConv: \n\n\uf8f1 \n\uf8f2 \n\n\uf8f3 \n\nConv 1\u00d71, (128+32)-c \u2192 128-c \nDepthwise Conv 7\u00d77, 128-c \nConv 1\u00d71, 256-c \n\n\uf8fc \n\uf8fd \n\n\uf8fe \n\nOConv: \n\n\uf8f1 \n\uf8f2 \n\n\uf8f3 \n\nConv 1\u00d71, (256+32)-c \u2192 256-c \nDepthwise Conv 7\u00d77, 256-c \nConv 1\u00d71, 256-c \n\n\uf8fc \n\uf8fd \n\n\uf8fe \nDConv: Deformable Conv 3\u00d73, 128-c, 4-dg DConv: Deformable Conv 3\u00d73, 128-c, 4-dg DConv: Deformable Conv 3\u00d73, 256-c, 4-dg \n\nInput size \nf 8 : 64\u00d764 \nf 16 : 32\u00d732 \nf 32 : 16\u00d716 \n\nTWM \nOConv: \n\n\uf8f1 \n\uf8f2 \n\n\uf8f3 \n\nConv 1\u00d71, (256+32)-c \u2192 256-c \nDepthwise Conv 7\u00d77, 256-c \nConv 1\u00d71, 256-c \n\n\uf8fc \n\uf8fd \n\n\uf8fe \n\nOConv: \n\n\uf8f1 \n\uf8f2 \n\n\uf8f3 \n\nConv 1\u00d71, (256+32)-c \u2192 256-c \nDepthwise Conv 7\u00d77, 256-c \nConv 1\u00d71, 256-c \n\n\uf8fc \n\uf8fd \n\n\uf8fe \n\nOConv: \n\n\uf8f1 \n\uf8f2 \n\n\uf8f3 \n\nConv 1\u00d71, (512+32)-c \u2192 512-c \nDepthwise Conv 7\u00d77, 512-c \nConv 1\u00d71, 512-c \n\n\uf8fc \n\uf8fd \n\n\uf8fe \nDConv: Deformable Conv 3\u00d73, 256-c, 4-dg DConv: Deformable Conv 3\u00d73, 256-c, 4-dg DConv: Deformable Conv 3\u00d73, 512-c, 4-dg \n\n\n\n\nFig. 10: Comparisons between the SimVQFR (without input features) and our VQFR (with input features). With input features of degraded faces, our VQFR could generate more faithful expressions (the first and third row), facial lines (the second and fourth row) and facial components (the fourth row) than SimVQFR.Input \nSimVQFR (w/o inp feat.) \nVQFR (w/ inp feat.) \n\nInput \nVariant-1 (Single Branch) \nVariant-2 (Parallel Decoder) \n\n\n\nH Bao, L Dong, F Wei, arXiv:2106.08254Beit: Bert pre-training of image transformers. 4arXiv preprintBao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254 (2021) 4\n\nEstimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, N L\u00e9onard, A Courville, arXiv:1308.34327arXiv preprintBengio, Y., L\u00e9onard, N., Courville, A.: Estimating or propagating gradients through stochas- tic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013) 7\n\nThe perception-distortion tradeoff. Y Blau, T Michaeli, Proceedings of the IEEE. the IEEE19Blau, Y., Michaeli, T.: The perception-distortion tradeoff. In: Proceedings of the IEEE con- ference on computer vision and pattern recognition. pp. 6228-6237 (2018) 19\n\nVggface2: A dataset for recognising faces across pose and age. Q Cao, L Shen, W Xie, O M Parkhi, A Zisserman, 13th IEEE international conference on automatic face & gesture recognition. 24Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: Vggface2: A dataset for recognising faces across pose and age. In: 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018). pp. 67-74. IEEE (2018) 2, 4\n\nH Chang, H Zhang, L Jiang, C Liu, W T Freeman, arXiv:2202.04200Maskgit: Masked generative image transformer. 4arXiv preprintChang, H., Zhang, H., Jiang, L., Liu, C., Freeman, W.T.: Maskgit: Masked generative image transformer. arXiv preprint arXiv:2202.04200 (2022) 4\n\nProgressive semantic-aware style transformation for blind face restoration. C Chen, X Li, L Yang, X Lin, L Zhang, K Y K Wong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1226Chen, C., Li, X., Yang, L., Lin, X., Zhang, L., Wong, K.Y.K.: Progressive semantic-aware style transformation for blind face restoration. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11896-11905 (2021) 1, 3, 4, 11, 12, 26\n\nFsrnet: End-to-end learning face superresolution with facial priors. Y Chen, Y Tai, X Liu, C Shen, J Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition14Chen, Y., Tai, Y., Liu, X., Shen, C., Yang, J.: Fsrnet: End-to-end learning face super- resolution with facial priors. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2492-2501 (2018) 1, 3, 4\n\nArcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition1219Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face recognition. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4690-4699 (2019) 12, 19\n\nJ Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. 20arXiv preprintDevlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018) 20\n\nExemplar guided face image super-resolution without facial landmarks. B Dogan, S Gu, R Timofte, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops14Dogan, B., Gu, S., Timofte, R.: Exemplar guided face image super-resolution without facial landmarks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 0-0 (2019) 1, 2, 3, 4\n\nLearning a deep convolutional network for image super-resolution. C Dong, C C Loy, K He, X Tang, European conference on computer vision. Springer1Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for image super-resolution. In: European conference on computer vision. pp. 184-199. Springer (2014) 1\n\nPeco: Perceptual codebook for bert pre-training of vision transformers. X Dong, J Bao, T Zhang, D Chen, W Zhang, L Yuan, D Chen, F Wen, N Yu, arXiv:2111.127104arXiv preprintDong, X., Bao, J., Zhang, T., Chen, D., Zhang, W., Yuan, L., Chen, D., Wen, F., Yu, N.: Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710 (2021) 4\n\nTaming transformers for high-resolution image synthesis. P Esser, R Rombach, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition420Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image synthe- sis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion. pp. 12873-12883 (2021) 4, 5, 6, 7, 20\n\nSwagan: A style-based wavelet-driven generative model. R Gal, D C Hochberg, A Bermano, D Cohen-Or, ACM Transactions on Graphics (TOG). 40421Gal, R., Hochberg, D.C., Bermano, A., Cohen-Or, D.: Swagan: A style-based wavelet-driven generative model. ACM Transactions on Graphics (TOG) 40(4), 1-11 (2021) 10, 21\n\nImage style transfer using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition10Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural net- works. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2414-2423 (2016) 10\n\nImage processing using multi-code gan prior. J Gu, Y Shen, B Zhou, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition412Gu, J., Shen, Y., Zhou, B.: Image processing using multi-code gan prior. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 3012-3021 (2020) 4, 11, 12\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, 3019Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017) 11, 19\n\nImage-to-image translation with conditional adversarial networks. P Isola, J Y Zhu, T Zhou, A A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition621Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional ad- versarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1125-1134 (2017) 6, 10, 21\n", "annotations": {"author": "[{\"end\":117,\"start\":86},{\"end\":151,\"start\":118},{\"end\":258,\"start\":152},{\"end\":341,\"start\":259},{\"end\":418,\"start\":342},{\"end\":450,\"start\":419},{\"end\":467,\"start\":451},{\"end\":516,\"start\":468},{\"end\":117,\"start\":86},{\"end\":151,\"start\":118},{\"end\":258,\"start\":152},{\"end\":341,\"start\":259},{\"end\":418,\"start\":342},{\"end\":450,\"start\":419},{\"end\":467,\"start\":451},{\"end\":516,\"start\":468}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":93},{\"end\":129,\"start\":125},{\"end\":164,\"start\":161},{\"end\":268,\"start\":264},{\"end\":348,\"start\":346},{\"end\":428,\"start\":424},{\"end\":466,\"start\":461},{\"end\":95,\"start\":93},{\"end\":129,\"start\":125},{\"end\":164,\"start\":161},{\"end\":268,\"start\":264},{\"end\":348,\"start\":346},{\"end\":428,\"start\":424},{\"end\":466,\"start\":461}]", "author_first_name": "[{\"end\":92,\"start\":86},{\"end\":124,\"start\":118},{\"end\":160,\"start\":152},{\"end\":263,\"start\":259},{\"end\":345,\"start\":342},{\"end\":423,\"start\":419},{\"end\":460,\"start\":451},{\"end\":92,\"start\":86},{\"end\":124,\"start\":118},{\"end\":160,\"start\":152},{\"end\":263,\"start\":259},{\"end\":345,\"start\":342},{\"end\":423,\"start\":419},{\"end\":460,\"start\":451}]", "author_affiliation": "[{\"end\":116,\"start\":97},{\"end\":150,\"start\":131},{\"end\":185,\"start\":166},{\"end\":257,\"start\":187},{\"end\":340,\"start\":270},{\"end\":417,\"start\":350},{\"end\":449,\"start\":430},{\"end\":515,\"start\":469},{\"end\":116,\"start\":97},{\"end\":150,\"start\":131},{\"end\":185,\"start\":166},{\"end\":257,\"start\":187},{\"end\":340,\"start\":270},{\"end\":417,\"start\":350},{\"end\":449,\"start\":430},{\"end\":515,\"start\":469}]", "title": "[{\"end\":83,\"start\":1},{\"end\":599,\"start\":517},{\"end\":83,\"start\":1},{\"end\":599,\"start\":517}]", "venue": null, "abstract": "[{\"end\":4604,\"start\":658},{\"end\":4604,\"start\":658}]", "bib_ref": "[{\"end\":4730,\"start\":4726},{\"end\":4741,\"start\":4737},{\"end\":4744,\"start\":4741},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4764,\"start\":4760},{\"end\":4767,\"start\":4764},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5002,\"start\":4999},{\"end\":5005,\"start\":5002},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5007,\"start\":5005},{\"end\":5031,\"start\":5027},{\"end\":5034,\"start\":5031},{\"end\":5060,\"start\":5056},{\"end\":5063,\"start\":5060},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5066,\"start\":5063},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5138,\"start\":5135},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5161,\"start\":5158},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5163,\"start\":5161},{\"end\":5198,\"start\":5194},{\"end\":5519,\"start\":5515},{\"end\":5522,\"start\":5519},{\"end\":5743,\"start\":5730},{\"end\":5746,\"start\":5743},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6327,\"start\":6323},{\"end\":6330,\"start\":6327},{\"end\":6366,\"start\":6362},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7044,\"start\":7041},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7697,\"start\":7696},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8036,\"start\":8033},{\"end\":8039,\"start\":8036},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8041,\"start\":8039},{\"end\":8065,\"start\":8061},{\"end\":8068,\"start\":8065},{\"end\":8094,\"start\":8090},{\"end\":8097,\"start\":8094},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8100,\"start\":8097},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8150,\"start\":8147},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8173,\"start\":8170},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8175,\"start\":8173},{\"end\":8210,\"start\":8206},{\"end\":8700,\"start\":8696},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8703,\"start\":8700},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8707,\"start\":8704},{\"end\":10038,\"start\":10034},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10356,\"start\":10352},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10704,\"start\":10701},{\"end\":10707,\"start\":10704},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10710,\"start\":10707},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10772,\"start\":10768},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10774,\"start\":10772},{\"end\":11781,\"start\":11777},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11784,\"start\":11781},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12746,\"start\":12742},{\"end\":12815,\"start\":12811},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12856,\"start\":12852},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12966,\"start\":12962},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14156,\"start\":14153},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15143,\"start\":15139},{\"end\":17163,\"start\":17159},{\"end\":21979,\"start\":21975},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24895,\"start\":24891},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25129,\"start\":25125},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25353,\"start\":25349},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26662,\"start\":26658},{\"end\":26676,\"start\":26672},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26891,\"start\":26888},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31719,\"start\":31715},{\"end\":31733,\"start\":31729},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31908,\"start\":31905},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32399,\"start\":32396},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34118,\"start\":34114},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34140,\"start\":34137},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35876,\"start\":35872},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35943,\"start\":35939},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37519,\"start\":37516},{\"end\":37547,\"start\":37543},{\"end\":4730,\"start\":4726},{\"end\":4741,\"start\":4737},{\"end\":4744,\"start\":4741},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4764,\"start\":4760},{\"end\":4767,\"start\":4764},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5002,\"start\":4999},{\"end\":5005,\"start\":5002},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5007,\"start\":5005},{\"end\":5031,\"start\":5027},{\"end\":5034,\"start\":5031},{\"end\":5060,\"start\":5056},{\"end\":5063,\"start\":5060},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5066,\"start\":5063},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5138,\"start\":5135},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5161,\"start\":5158},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5163,\"start\":5161},{\"end\":5198,\"start\":5194},{\"end\":5519,\"start\":5515},{\"end\":5522,\"start\":5519},{\"end\":5743,\"start\":5730},{\"end\":5746,\"start\":5743},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6327,\"start\":6323},{\"end\":6330,\"start\":6327},{\"end\":6366,\"start\":6362},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7044,\"start\":7041},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7697,\"start\":7696},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8036,\"start\":8033},{\"end\":8039,\"start\":8036},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8041,\"start\":8039},{\"end\":8065,\"start\":8061},{\"end\":8068,\"start\":8065},{\"end\":8094,\"start\":8090},{\"end\":8097,\"start\":8094},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8100,\"start\":8097},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8150,\"start\":8147},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8173,\"start\":8170},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8175,\"start\":8173},{\"end\":8210,\"start\":8206},{\"end\":8700,\"start\":8696},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8703,\"start\":8700},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8707,\"start\":8704},{\"end\":10038,\"start\":10034},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10356,\"start\":10352},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10704,\"start\":10701},{\"end\":10707,\"start\":10704},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10710,\"start\":10707},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10772,\"start\":10768},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10774,\"start\":10772},{\"end\":11781,\"start\":11777},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11784,\"start\":11781},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12746,\"start\":12742},{\"end\":12815,\"start\":12811},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12856,\"start\":12852},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12966,\"start\":12962},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14156,\"start\":14153},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15143,\"start\":15139},{\"end\":17163,\"start\":17159},{\"end\":21979,\"start\":21975},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24895,\"start\":24891},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25129,\"start\":25125},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25353,\"start\":25349},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26662,\"start\":26658},{\"end\":26676,\"start\":26672},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26891,\"start\":26888},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31719,\"start\":31715},{\"end\":31733,\"start\":31729},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31908,\"start\":31905},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32399,\"start\":32396},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34118,\"start\":34114},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34140,\"start\":34137},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35876,\"start\":35872},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35943,\"start\":35939},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37519,\"start\":37516},{\"end\":37547,\"start\":37543}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38390,\"start\":38140},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38507,\"start\":38391},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38806,\"start\":38508},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39003,\"start\":38807},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39102,\"start\":39004},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39251,\"start\":39103},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39715,\"start\":39252},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39803,\"start\":39716},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39892,\"start\":39804},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40070,\"start\":39893},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46111,\"start\":40071},{\"attributes\":{\"id\":\"fig_11\"},\"end\":46191,\"start\":46112},{\"attributes\":{\"id\":\"fig_12\"},\"end\":46223,\"start\":46192},{\"attributes\":{\"id\":\"fig_13\"},\"end\":47621,\"start\":46224},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48494,\"start\":47622},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48612,\"start\":48495},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49632,\"start\":48613},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50328,\"start\":49633},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50439,\"start\":50329},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51176,\"start\":50440},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51427,\"start\":51177},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":52727,\"start\":51428},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":53160,\"start\":52728},{\"attributes\":{\"id\":\"fig_0\"},\"end\":38390,\"start\":38140},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38507,\"start\":38391},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38806,\"start\":38508},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39003,\"start\":38807},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39102,\"start\":39004},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39251,\"start\":39103},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39715,\"start\":39252},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39803,\"start\":39716},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39892,\"start\":39804},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40070,\"start\":39893},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46111,\"start\":40071},{\"attributes\":{\"id\":\"fig_11\"},\"end\":46191,\"start\":46112},{\"attributes\":{\"id\":\"fig_12\"},\"end\":46223,\"start\":46192},{\"attributes\":{\"id\":\"fig_13\"},\"end\":47621,\"start\":46224},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48494,\"start\":47622},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48612,\"start\":48495},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49632,\"start\":48613},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50328,\"start\":49633},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50439,\"start\":50329},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51176,\"start\":50440},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51427,\"start\":51177},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":52727,\"start\":51428},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":53160,\"start\":52728}]", "paragraph": "[{\"end\":5500,\"start\":4620},{\"end\":7279,\"start\":5502},{\"end\":7861,\"start\":7281},{\"end\":8422,\"start\":7878},{\"end\":9944,\"start\":8424},{\"end\":11222,\"start\":9974},{\"end\":12550,\"start\":11238},{\"end\":13409,\"start\":12580},{\"end\":13603,\"start\":13469},{\"end\":14022,\"start\":13635},{\"end\":14483,\"start\":14024},{\"end\":14790,\"start\":14622},{\"end\":14978,\"start\":14792},{\"end\":16315,\"start\":14980},{\"end\":16895,\"start\":16317},{\"end\":18545,\"start\":16897},{\"end\":19842,\"start\":18760},{\"end\":20276,\"start\":19844},{\"end\":20674,\"start\":20292},{\"end\":20865,\"start\":20694},{\"end\":21437,\"start\":20867},{\"end\":21773,\"start\":21454},{\"end\":23101,\"start\":21856},{\"end\":23366,\"start\":23179},{\"end\":23930,\"start\":23386},{\"end\":24290,\"start\":23932},{\"end\":24502,\"start\":24292},{\"end\":25439,\"start\":24533},{\"end\":25757,\"start\":25549},{\"end\":27137,\"start\":25861},{\"end\":28518,\"start\":27183},{\"end\":29197,\"start\":28537},{\"end\":29641,\"start\":29199},{\"end\":29810,\"start\":29643},{\"end\":30521,\"start\":29825},{\"end\":30940,\"start\":30534},{\"end\":31607,\"start\":30966},{\"end\":32301,\"start\":31630},{\"end\":33281,\"start\":32303},{\"end\":34187,\"start\":33296},{\"end\":36626,\"start\":34229},{\"end\":36972,\"start\":36636},{\"end\":37270,\"start\":36982},{\"end\":38126,\"start\":37318},{\"end\":5500,\"start\":4620},{\"end\":7279,\"start\":5502},{\"end\":7861,\"start\":7281},{\"end\":8422,\"start\":7878},{\"end\":9944,\"start\":8424},{\"end\":11222,\"start\":9974},{\"end\":12550,\"start\":11238},{\"end\":13409,\"start\":12580},{\"end\":13603,\"start\":13469},{\"end\":14022,\"start\":13635},{\"end\":14483,\"start\":14024},{\"end\":14790,\"start\":14622},{\"end\":14978,\"start\":14792},{\"end\":16315,\"start\":14980},{\"end\":16895,\"start\":16317},{\"end\":18545,\"start\":16897},{\"end\":19842,\"start\":18760},{\"end\":20276,\"start\":19844},{\"end\":20674,\"start\":20292},{\"end\":20865,\"start\":20694},{\"end\":21437,\"start\":20867},{\"end\":21773,\"start\":21454},{\"end\":23101,\"start\":21856},{\"end\":23366,\"start\":23179},{\"end\":23930,\"start\":23386},{\"end\":24290,\"start\":23932},{\"end\":24502,\"start\":24292},{\"end\":25439,\"start\":24533},{\"end\":25757,\"start\":25549},{\"end\":27137,\"start\":25861},{\"end\":28518,\"start\":27183},{\"end\":29197,\"start\":28537},{\"end\":29641,\"start\":29199},{\"end\":29810,\"start\":29643},{\"end\":30521,\"start\":29825},{\"end\":30940,\"start\":30534},{\"end\":31607,\"start\":30966},{\"end\":32301,\"start\":31630},{\"end\":33281,\"start\":32303},{\"end\":34187,\"start\":33296},{\"end\":36626,\"start\":34229},{\"end\":36972,\"start\":36636},{\"end\":37270,\"start\":36982},{\"end\":38126,\"start\":37318}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13468,\"start\":13410},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13634,\"start\":13604},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14578,\"start\":14484},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14621,\"start\":14578},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18740,\"start\":18546},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20291,\"start\":20277},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20693,\"start\":20675},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21453,\"start\":21438},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21855,\"start\":21774},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23178,\"start\":23102},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24532,\"start\":24503},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25548,\"start\":25440},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25860,\"start\":25758},{\"attributes\":{\"id\":\"formula_0\"},\"end\":13468,\"start\":13410},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13634,\"start\":13604},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14578,\"start\":14484},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14621,\"start\":14578},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18740,\"start\":18546},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20291,\"start\":20277},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20693,\"start\":20675},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21453,\"start\":21438},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21855,\"start\":21774},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23178,\"start\":23102},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24532,\"start\":24503},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25548,\"start\":25440},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25860,\"start\":25758}]", "table_ref": "[{\"end\":27277,\"start\":27271},{\"end\":28147,\"start\":28141},{\"end\":31030,\"start\":31024},{\"end\":32121,\"start\":32115},{\"end\":27277,\"start\":27271},{\"end\":28147,\"start\":28141},{\"end\":31030,\"start\":31024},{\"end\":32121,\"start\":32115}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4618,\"start\":4606},{\"attributes\":{\"n\":\"2\"},\"end\":7876,\"start\":7864},{\"end\":9972,\"start\":9947},{\"attributes\":{\"n\":\"3\"},\"end\":11236,\"start\":11225},{\"attributes\":{\"n\":\"3.1\"},\"end\":12578,\"start\":12553},{\"attributes\":{\"n\":\"3.2\"},\"end\":18758,\"start\":18742},{\"attributes\":{\"n\":\"3.3\"},\"end\":23384,\"start\":23369},{\"attributes\":{\"n\":\"4.2\"},\"end\":27181,\"start\":27140},{\"attributes\":{\"n\":\"4.3\"},\"end\":28535,\"start\":28521},{\"attributes\":{\"n\":\"5\"},\"end\":29823,\"start\":29813},{\"attributes\":{\"n\":\"6\"},\"end\":30532,\"start\":30524},{\"attributes\":{\"n\":\"6.1\"},\"end\":30964,\"start\":30943},{\"attributes\":{\"n\":\"6.2\"},\"end\":31628,\"start\":31610},{\"attributes\":{\"n\":\"6.3\"},\"end\":33294,\"start\":33284},{\"attributes\":{\"n\":\"6.4\"},\"end\":34227,\"start\":34190},{\"end\":36634,\"start\":36629},{\"end\":36980,\"start\":36975},{\"attributes\":{\"n\":\"6.5\"},\"end\":37316,\"start\":37273},{\"end\":38139,\"start\":38129},{\"end\":38149,\"start\":38141},{\"end\":38400,\"start\":38392},{\"end\":38517,\"start\":38509},{\"end\":38816,\"start\":38808},{\"end\":39725,\"start\":39717},{\"end\":39910,\"start\":39894},{\"end\":40076,\"start\":40072},{\"end\":46201,\"start\":46193},{\"end\":46288,\"start\":46225},{\"end\":48623,\"start\":48614},{\"end\":49643,\"start\":49634},{\"end\":51187,\"start\":51178},{\"end\":51438,\"start\":51429},{\"attributes\":{\"n\":\"1\"},\"end\":4618,\"start\":4606},{\"attributes\":{\"n\":\"2\"},\"end\":7876,\"start\":7864},{\"end\":9972,\"start\":9947},{\"attributes\":{\"n\":\"3\"},\"end\":11236,\"start\":11225},{\"attributes\":{\"n\":\"3.1\"},\"end\":12578,\"start\":12553},{\"attributes\":{\"n\":\"3.2\"},\"end\":18758,\"start\":18742},{\"attributes\":{\"n\":\"3.3\"},\"end\":23384,\"start\":23369},{\"attributes\":{\"n\":\"4.2\"},\"end\":27181,\"start\":27140},{\"attributes\":{\"n\":\"4.3\"},\"end\":28535,\"start\":28521},{\"attributes\":{\"n\":\"5\"},\"end\":29823,\"start\":29813},{\"attributes\":{\"n\":\"6\"},\"end\":30532,\"start\":30524},{\"attributes\":{\"n\":\"6.1\"},\"end\":30964,\"start\":30943},{\"attributes\":{\"n\":\"6.2\"},\"end\":31628,\"start\":31610},{\"attributes\":{\"n\":\"6.3\"},\"end\":33294,\"start\":33284},{\"attributes\":{\"n\":\"6.4\"},\"end\":34227,\"start\":34190},{\"end\":36634,\"start\":36629},{\"end\":36980,\"start\":36975},{\"attributes\":{\"n\":\"6.5\"},\"end\":37316,\"start\":37273},{\"end\":38139,\"start\":38129},{\"end\":38149,\"start\":38141},{\"end\":38400,\"start\":38392},{\"end\":38517,\"start\":38509},{\"end\":38816,\"start\":38808},{\"end\":39725,\"start\":39717},{\"end\":39910,\"start\":39894},{\"end\":40076,\"start\":40072},{\"end\":46201,\"start\":46193},{\"end\":46288,\"start\":46225},{\"end\":48623,\"start\":48614},{\"end\":49643,\"start\":49634},{\"end\":51187,\"start\":51178},{\"end\":51438,\"start\":51429}]", "table": "[{\"end\":49632,\"start\":49003},{\"end\":50328,\"start\":49774},{\"end\":50439,\"start\":50339},{\"end\":51176,\"start\":50826},{\"end\":51427,\"start\":51264},{\"end\":52727,\"start\":51548},{\"end\":53160,\"start\":53041},{\"end\":49632,\"start\":49003},{\"end\":50328,\"start\":49774},{\"end\":50439,\"start\":50339},{\"end\":51176,\"start\":50826},{\"end\":51427,\"start\":51264},{\"end\":52727,\"start\":51548},{\"end\":53160,\"start\":53041}]", "figure_caption": "[{\"end\":38390,\"start\":38151},{\"end\":38507,\"start\":38402},{\"end\":38806,\"start\":38519},{\"end\":39003,\"start\":38818},{\"end\":39102,\"start\":39006},{\"end\":39251,\"start\":39105},{\"end\":39715,\"start\":39254},{\"end\":39803,\"start\":39727},{\"end\":39892,\"start\":39806},{\"end\":40070,\"start\":39913},{\"end\":46111,\"start\":40079},{\"end\":46191,\"start\":46114},{\"end\":46223,\"start\":46203},{\"end\":47621,\"start\":46303},{\"end\":48494,\"start\":47624},{\"end\":48612,\"start\":48497},{\"end\":49003,\"start\":48625},{\"end\":49774,\"start\":49645},{\"end\":50339,\"start\":50331},{\"end\":50826,\"start\":50442},{\"end\":51264,\"start\":51189},{\"end\":51548,\"start\":51440},{\"end\":53041,\"start\":52730},{\"end\":38390,\"start\":38151},{\"end\":38507,\"start\":38402},{\"end\":38806,\"start\":38519},{\"end\":39003,\"start\":38818},{\"end\":39102,\"start\":39006},{\"end\":39251,\"start\":39105},{\"end\":39715,\"start\":39254},{\"end\":39803,\"start\":39727},{\"end\":39892,\"start\":39806},{\"end\":40070,\"start\":39913},{\"end\":46111,\"start\":40079},{\"end\":46191,\"start\":46114},{\"end\":46223,\"start\":46203},{\"end\":47621,\"start\":46303},{\"end\":48494,\"start\":47624},{\"end\":48612,\"start\":48497},{\"end\":49003,\"start\":48625},{\"end\":49774,\"start\":49645},{\"end\":50339,\"start\":50331},{\"end\":50826,\"start\":50442},{\"end\":51264,\"start\":51189},{\"end\":51548,\"start\":51440},{\"end\":53041,\"start\":52730}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6164,\"start\":6158},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11581,\"start\":11575},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15397,\"start\":15391},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15588,\"start\":15582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16079,\"start\":16073},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17443,\"start\":17437},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18963,\"start\":18957},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19331,\"start\":19325},{\"end\":19374,\"start\":19368},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20049,\"start\":20041},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20189,\"start\":20183},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21038,\"start\":21032},{\"end\":27801,\"start\":27795},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28434,\"start\":28428},{\"end\":28691,\"start\":28682},{\"end\":28943,\"start\":28934},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29120,\"start\":29114},{\"end\":29386,\"start\":29376},{\"end\":29424,\"start\":29415},{\"end\":29689,\"start\":29679},{\"end\":29704,\"start\":29695},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":33359,\"start\":33350},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":33672,\"start\":33663},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34456,\"start\":34449},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35296,\"start\":35289},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35468,\"start\":35461},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36027,\"start\":36020},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36279,\"start\":36272},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36698,\"start\":36691},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37017,\"start\":37010},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37622,\"start\":37606},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37634,\"start\":37627},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37704,\"start\":37697},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37716,\"start\":37709},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37793,\"start\":37777},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37805,\"start\":37798},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37922,\"start\":37915},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6164,\"start\":6158},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11581,\"start\":11575},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15397,\"start\":15391},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15588,\"start\":15582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16079,\"start\":16073},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17443,\"start\":17437},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18963,\"start\":18957},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19331,\"start\":19325},{\"end\":19374,\"start\":19368},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20049,\"start\":20041},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20189,\"start\":20183},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21038,\"start\":21032},{\"end\":27801,\"start\":27795},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28434,\"start\":28428},{\"end\":28691,\"start\":28682},{\"end\":28943,\"start\":28934},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29120,\"start\":29114},{\"end\":29386,\"start\":29376},{\"end\":29424,\"start\":29415},{\"end\":29689,\"start\":29679},{\"end\":29704,\"start\":29695},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":33359,\"start\":33350},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":33672,\"start\":33663},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34456,\"start\":34449},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35296,\"start\":35289},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35468,\"start\":35461},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36027,\"start\":36020},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36279,\"start\":36272},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36698,\"start\":36691},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37017,\"start\":37010},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37622,\"start\":37606},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37634,\"start\":37627},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37704,\"start\":37697},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37716,\"start\":37709},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37793,\"start\":37777},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37805,\"start\":37798},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37922,\"start\":37915}]", "bib_author_first_name": "[{\"end\":53163,\"start\":53162},{\"end\":53170,\"start\":53169},{\"end\":53178,\"start\":53177},{\"end\":53472,\"start\":53471},{\"end\":53482,\"start\":53481},{\"end\":53493,\"start\":53492},{\"end\":53747,\"start\":53746},{\"end\":53755,\"start\":53754},{\"end\":54035,\"start\":54034},{\"end\":54042,\"start\":54041},{\"end\":54050,\"start\":54049},{\"end\":54057,\"start\":54056},{\"end\":54059,\"start\":54058},{\"end\":54069,\"start\":54068},{\"end\":54404,\"start\":54403},{\"end\":54413,\"start\":54412},{\"end\":54422,\"start\":54421},{\"end\":54431,\"start\":54430},{\"end\":54438,\"start\":54437},{\"end\":54440,\"start\":54439},{\"end\":54749,\"start\":54748},{\"end\":54757,\"start\":54756},{\"end\":54763,\"start\":54762},{\"end\":54771,\"start\":54770},{\"end\":54778,\"start\":54777},{\"end\":54787,\"start\":54786},{\"end\":54791,\"start\":54788},{\"end\":55290,\"start\":55289},{\"end\":55298,\"start\":55297},{\"end\":55305,\"start\":55304},{\"end\":55312,\"start\":55311},{\"end\":55320,\"start\":55319},{\"end\":55768,\"start\":55767},{\"end\":55776,\"start\":55775},{\"end\":55783,\"start\":55782},{\"end\":55790,\"start\":55789},{\"end\":56180,\"start\":56179},{\"end\":56190,\"start\":56189},{\"end\":56192,\"start\":56191},{\"end\":56201,\"start\":56200},{\"end\":56208,\"start\":56207},{\"end\":56579,\"start\":56578},{\"end\":56588,\"start\":56587},{\"end\":56594,\"start\":56593},{\"end\":57068,\"start\":57067},{\"end\":57076,\"start\":57075},{\"end\":57078,\"start\":57077},{\"end\":57085,\"start\":57084},{\"end\":57091,\"start\":57090},{\"end\":57401,\"start\":57400},{\"end\":57409,\"start\":57408},{\"end\":57416,\"start\":57415},{\"end\":57425,\"start\":57424},{\"end\":57433,\"start\":57432},{\"end\":57442,\"start\":57441},{\"end\":57450,\"start\":57449},{\"end\":57458,\"start\":57457},{\"end\":57465,\"start\":57464},{\"end\":57761,\"start\":57760},{\"end\":57770,\"start\":57769},{\"end\":57781,\"start\":57780},{\"end\":58219,\"start\":58218},{\"end\":58226,\"start\":58225},{\"end\":58228,\"start\":58227},{\"end\":58240,\"start\":58239},{\"end\":58251,\"start\":58250},{\"end\":58531,\"start\":58530},{\"end\":58533,\"start\":58532},{\"end\":58542,\"start\":58541},{\"end\":58544,\"start\":58543},{\"end\":58553,\"start\":58552},{\"end\":58957,\"start\":58956},{\"end\":58963,\"start\":58962},{\"end\":58971,\"start\":58970},{\"end\":59457,\"start\":59456},{\"end\":59467,\"start\":59466},{\"end\":59479,\"start\":59478},{\"end\":59494,\"start\":59493},{\"end\":59505,\"start\":59504},{\"end\":59812,\"start\":59811},{\"end\":59821,\"start\":59820},{\"end\":59823,\"start\":59822},{\"end\":59830,\"start\":59829},{\"end\":59838,\"start\":59837},{\"end\":59840,\"start\":59839},{\"end\":53163,\"start\":53162},{\"end\":53170,\"start\":53169},{\"end\":53178,\"start\":53177},{\"end\":53472,\"start\":53471},{\"end\":53482,\"start\":53481},{\"end\":53493,\"start\":53492},{\"end\":53747,\"start\":53746},{\"end\":53755,\"start\":53754},{\"end\":54035,\"start\":54034},{\"end\":54042,\"start\":54041},{\"end\":54050,\"start\":54049},{\"end\":54057,\"start\":54056},{\"end\":54059,\"start\":54058},{\"end\":54069,\"start\":54068},{\"end\":54404,\"start\":54403},{\"end\":54413,\"start\":54412},{\"end\":54422,\"start\":54421},{\"end\":54431,\"start\":54430},{\"end\":54438,\"start\":54437},{\"end\":54440,\"start\":54439},{\"end\":54749,\"start\":54748},{\"end\":54757,\"start\":54756},{\"end\":54763,\"start\":54762},{\"end\":54771,\"start\":54770},{\"end\":54778,\"start\":54777},{\"end\":54787,\"start\":54786},{\"end\":54791,\"start\":54788},{\"end\":55290,\"start\":55289},{\"end\":55298,\"start\":55297},{\"end\":55305,\"start\":55304},{\"end\":55312,\"start\":55311},{\"end\":55320,\"start\":55319},{\"end\":55768,\"start\":55767},{\"end\":55776,\"start\":55775},{\"end\":55783,\"start\":55782},{\"end\":55790,\"start\":55789},{\"end\":56180,\"start\":56179},{\"end\":56190,\"start\":56189},{\"end\":56192,\"start\":56191},{\"end\":56201,\"start\":56200},{\"end\":56208,\"start\":56207},{\"end\":56579,\"start\":56578},{\"end\":56588,\"start\":56587},{\"end\":56594,\"start\":56593},{\"end\":57068,\"start\":57067},{\"end\":57076,\"start\":57075},{\"end\":57078,\"start\":57077},{\"end\":57085,\"start\":57084},{\"end\":57091,\"start\":57090},{\"end\":57401,\"start\":57400},{\"end\":57409,\"start\":57408},{\"end\":57416,\"start\":57415},{\"end\":57425,\"start\":57424},{\"end\":57433,\"start\":57432},{\"end\":57442,\"start\":57441},{\"end\":57450,\"start\":57449},{\"end\":57458,\"start\":57457},{\"end\":57465,\"start\":57464},{\"end\":57761,\"start\":57760},{\"end\":57770,\"start\":57769},{\"end\":57781,\"start\":57780},{\"end\":58219,\"start\":58218},{\"end\":58226,\"start\":58225},{\"end\":58228,\"start\":58227},{\"end\":58240,\"start\":58239},{\"end\":58251,\"start\":58250},{\"end\":58531,\"start\":58530},{\"end\":58533,\"start\":58532},{\"end\":58542,\"start\":58541},{\"end\":58544,\"start\":58543},{\"end\":58553,\"start\":58552},{\"end\":58957,\"start\":58956},{\"end\":58963,\"start\":58962},{\"end\":58971,\"start\":58970},{\"end\":59457,\"start\":59456},{\"end\":59467,\"start\":59466},{\"end\":59479,\"start\":59478},{\"end\":59494,\"start\":59493},{\"end\":59505,\"start\":59504},{\"end\":59812,\"start\":59811},{\"end\":59821,\"start\":59820},{\"end\":59823,\"start\":59822},{\"end\":59830,\"start\":59829},{\"end\":59838,\"start\":59837},{\"end\":59840,\"start\":59839}]", "bib_author_last_name": "[{\"end\":53167,\"start\":53164},{\"end\":53175,\"start\":53171},{\"end\":53182,\"start\":53179},{\"end\":53479,\"start\":53473},{\"end\":53490,\"start\":53483},{\"end\":53503,\"start\":53494},{\"end\":53752,\"start\":53748},{\"end\":53764,\"start\":53756},{\"end\":54039,\"start\":54036},{\"end\":54047,\"start\":54043},{\"end\":54054,\"start\":54051},{\"end\":54066,\"start\":54060},{\"end\":54079,\"start\":54070},{\"end\":54410,\"start\":54405},{\"end\":54419,\"start\":54414},{\"end\":54428,\"start\":54423},{\"end\":54435,\"start\":54432},{\"end\":54448,\"start\":54441},{\"end\":54754,\"start\":54750},{\"end\":54760,\"start\":54758},{\"end\":54768,\"start\":54764},{\"end\":54775,\"start\":54772},{\"end\":54784,\"start\":54779},{\"end\":54796,\"start\":54792},{\"end\":55295,\"start\":55291},{\"end\":55302,\"start\":55299},{\"end\":55309,\"start\":55306},{\"end\":55317,\"start\":55313},{\"end\":55325,\"start\":55321},{\"end\":55773,\"start\":55769},{\"end\":55780,\"start\":55777},{\"end\":55787,\"start\":55784},{\"end\":55800,\"start\":55791},{\"end\":56187,\"start\":56181},{\"end\":56198,\"start\":56193},{\"end\":56205,\"start\":56202},{\"end\":56218,\"start\":56209},{\"end\":56585,\"start\":56580},{\"end\":56591,\"start\":56589},{\"end\":56602,\"start\":56595},{\"end\":57073,\"start\":57069},{\"end\":57082,\"start\":57079},{\"end\":57088,\"start\":57086},{\"end\":57096,\"start\":57092},{\"end\":57406,\"start\":57402},{\"end\":57413,\"start\":57410},{\"end\":57422,\"start\":57417},{\"end\":57430,\"start\":57426},{\"end\":57439,\"start\":57434},{\"end\":57447,\"start\":57443},{\"end\":57455,\"start\":57451},{\"end\":57462,\"start\":57459},{\"end\":57468,\"start\":57466},{\"end\":57767,\"start\":57762},{\"end\":57778,\"start\":57771},{\"end\":57787,\"start\":57782},{\"end\":58223,\"start\":58220},{\"end\":58237,\"start\":58229},{\"end\":58248,\"start\":58241},{\"end\":58260,\"start\":58252},{\"end\":58539,\"start\":58534},{\"end\":58550,\"start\":58545},{\"end\":58560,\"start\":58554},{\"end\":58960,\"start\":58958},{\"end\":58968,\"start\":58964},{\"end\":58976,\"start\":58972},{\"end\":59464,\"start\":59458},{\"end\":59476,\"start\":59468},{\"end\":59491,\"start\":59480},{\"end\":59502,\"start\":59495},{\"end\":59516,\"start\":59506},{\"end\":59818,\"start\":59813},{\"end\":59827,\"start\":59824},{\"end\":59835,\"start\":59831},{\"end\":59846,\"start\":59841},{\"end\":53167,\"start\":53164},{\"end\":53175,\"start\":53171},{\"end\":53182,\"start\":53179},{\"end\":53479,\"start\":53473},{\"end\":53490,\"start\":53483},{\"end\":53503,\"start\":53494},{\"end\":53752,\"start\":53748},{\"end\":53764,\"start\":53756},{\"end\":54039,\"start\":54036},{\"end\":54047,\"start\":54043},{\"end\":54054,\"start\":54051},{\"end\":54066,\"start\":54060},{\"end\":54079,\"start\":54070},{\"end\":54410,\"start\":54405},{\"end\":54419,\"start\":54414},{\"end\":54428,\"start\":54423},{\"end\":54435,\"start\":54432},{\"end\":54448,\"start\":54441},{\"end\":54754,\"start\":54750},{\"end\":54760,\"start\":54758},{\"end\":54768,\"start\":54764},{\"end\":54775,\"start\":54772},{\"end\":54784,\"start\":54779},{\"end\":54796,\"start\":54792},{\"end\":55295,\"start\":55291},{\"end\":55302,\"start\":55299},{\"end\":55309,\"start\":55306},{\"end\":55317,\"start\":55313},{\"end\":55325,\"start\":55321},{\"end\":55773,\"start\":55769},{\"end\":55780,\"start\":55777},{\"end\":55787,\"start\":55784},{\"end\":55800,\"start\":55791},{\"end\":56187,\"start\":56181},{\"end\":56198,\"start\":56193},{\"end\":56205,\"start\":56202},{\"end\":56218,\"start\":56209},{\"end\":56585,\"start\":56580},{\"end\":56591,\"start\":56589},{\"end\":56602,\"start\":56595},{\"end\":57073,\"start\":57069},{\"end\":57082,\"start\":57079},{\"end\":57088,\"start\":57086},{\"end\":57096,\"start\":57092},{\"end\":57406,\"start\":57402},{\"end\":57413,\"start\":57410},{\"end\":57422,\"start\":57417},{\"end\":57430,\"start\":57426},{\"end\":57439,\"start\":57434},{\"end\":57447,\"start\":57443},{\"end\":57455,\"start\":57451},{\"end\":57462,\"start\":57459},{\"end\":57468,\"start\":57466},{\"end\":57767,\"start\":57762},{\"end\":57778,\"start\":57771},{\"end\":57787,\"start\":57782},{\"end\":58223,\"start\":58220},{\"end\":58237,\"start\":58229},{\"end\":58248,\"start\":58241},{\"end\":58260,\"start\":58252},{\"end\":58539,\"start\":58534},{\"end\":58550,\"start\":58545},{\"end\":58560,\"start\":58554},{\"end\":58960,\"start\":58958},{\"end\":58968,\"start\":58964},{\"end\":58976,\"start\":58972},{\"end\":59464,\"start\":59458},{\"end\":59476,\"start\":59468},{\"end\":59491,\"start\":59480},{\"end\":59502,\"start\":59495},{\"end\":59516,\"start\":59506},{\"end\":59818,\"start\":59813},{\"end\":59827,\"start\":59824},{\"end\":59835,\"start\":59831},{\"end\":59846,\"start\":59841}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2106.08254\",\"id\":\"b0\"},\"end\":53377,\"start\":53162},{\"attributes\":{\"doi\":\"arXiv:1308.3432\",\"id\":\"b1\"},\"end\":53708,\"start\":53379},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215763824},\"end\":53969,\"start\":53710},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":216009},\"end\":54401,\"start\":53971},{\"attributes\":{\"doi\":\"arXiv:2202.04200\",\"id\":\"b4\"},\"end\":54670,\"start\":54403},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221802293},\"end\":55218,\"start\":54672},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4564405},\"end\":55700,\"start\":55220},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8923541},\"end\":56177,\"start\":55702},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b8\"},\"end\":56506,\"start\":56179},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":189928566},\"end\":56999,\"start\":56508},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18874645},\"end\":57326,\"start\":57001},{\"attributes\":{\"doi\":\"arXiv:2111.12710\",\"id\":\"b11\"},\"end\":57701,\"start\":57328},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":229297973},\"end\":58161,\"start\":57703},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":231879960},\"end\":58470,\"start\":58163},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206593710},\"end\":58909,\"start\":58472},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":209377041},\"end\":59320,\"start\":58911},{\"attributes\":{\"id\":\"b16\"},\"end\":59743,\"start\":59322},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6200260},\"end\":60218,\"start\":59745},{\"attributes\":{\"doi\":\"arXiv:2106.08254\",\"id\":\"b0\"},\"end\":53377,\"start\":53162},{\"attributes\":{\"doi\":\"arXiv:1308.3432\",\"id\":\"b1\"},\"end\":53708,\"start\":53379},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215763824},\"end\":53969,\"start\":53710},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":216009},\"end\":54401,\"start\":53971},{\"attributes\":{\"doi\":\"arXiv:2202.04200\",\"id\":\"b4\"},\"end\":54670,\"start\":54403},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221802293},\"end\":55218,\"start\":54672},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4564405},\"end\":55700,\"start\":55220},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8923541},\"end\":56177,\"start\":55702},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b8\"},\"end\":56506,\"start\":56179},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":189928566},\"end\":56999,\"start\":56508},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18874645},\"end\":57326,\"start\":57001},{\"attributes\":{\"doi\":\"arXiv:2111.12710\",\"id\":\"b11\"},\"end\":57701,\"start\":57328},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":229297973},\"end\":58161,\"start\":57703},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":231879960},\"end\":58470,\"start\":58163},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206593710},\"end\":58909,\"start\":58472},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":209377041},\"end\":59320,\"start\":58911},{\"attributes\":{\"id\":\"b16\"},\"end\":59743,\"start\":59322},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6200260},\"end\":60218,\"start\":59745}]", "bib_title": "[{\"end\":53744,\"start\":53710},{\"end\":54032,\"start\":53971},{\"end\":54746,\"start\":54672},{\"end\":55287,\"start\":55220},{\"end\":55765,\"start\":55702},{\"end\":56576,\"start\":56508},{\"end\":57065,\"start\":57001},{\"end\":57758,\"start\":57703},{\"end\":58216,\"start\":58163},{\"end\":58528,\"start\":58472},{\"end\":58954,\"start\":58911},{\"end\":59809,\"start\":59745},{\"end\":53744,\"start\":53710},{\"end\":54032,\"start\":53971},{\"end\":54746,\"start\":54672},{\"end\":55287,\"start\":55220},{\"end\":55765,\"start\":55702},{\"end\":56576,\"start\":56508},{\"end\":57065,\"start\":57001},{\"end\":57758,\"start\":57703},{\"end\":58216,\"start\":58163},{\"end\":58528,\"start\":58472},{\"end\":58954,\"start\":58911},{\"end\":59809,\"start\":59745}]", "bib_author": "[{\"end\":53169,\"start\":53162},{\"end\":53177,\"start\":53169},{\"end\":53184,\"start\":53177},{\"end\":53481,\"start\":53471},{\"end\":53492,\"start\":53481},{\"end\":53505,\"start\":53492},{\"end\":53754,\"start\":53746},{\"end\":53766,\"start\":53754},{\"end\":54041,\"start\":54034},{\"end\":54049,\"start\":54041},{\"end\":54056,\"start\":54049},{\"end\":54068,\"start\":54056},{\"end\":54081,\"start\":54068},{\"end\":54412,\"start\":54403},{\"end\":54421,\"start\":54412},{\"end\":54430,\"start\":54421},{\"end\":54437,\"start\":54430},{\"end\":54450,\"start\":54437},{\"end\":54756,\"start\":54748},{\"end\":54762,\"start\":54756},{\"end\":54770,\"start\":54762},{\"end\":54777,\"start\":54770},{\"end\":54786,\"start\":54777},{\"end\":54798,\"start\":54786},{\"end\":55297,\"start\":55289},{\"end\":55304,\"start\":55297},{\"end\":55311,\"start\":55304},{\"end\":55319,\"start\":55311},{\"end\":55327,\"start\":55319},{\"end\":55775,\"start\":55767},{\"end\":55782,\"start\":55775},{\"end\":55789,\"start\":55782},{\"end\":55802,\"start\":55789},{\"end\":56189,\"start\":56179},{\"end\":56200,\"start\":56189},{\"end\":56207,\"start\":56200},{\"end\":56220,\"start\":56207},{\"end\":56587,\"start\":56578},{\"end\":56593,\"start\":56587},{\"end\":56604,\"start\":56593},{\"end\":57075,\"start\":57067},{\"end\":57084,\"start\":57075},{\"end\":57090,\"start\":57084},{\"end\":57098,\"start\":57090},{\"end\":57408,\"start\":57400},{\"end\":57415,\"start\":57408},{\"end\":57424,\"start\":57415},{\"end\":57432,\"start\":57424},{\"end\":57441,\"start\":57432},{\"end\":57449,\"start\":57441},{\"end\":57457,\"start\":57449},{\"end\":57464,\"start\":57457},{\"end\":57470,\"start\":57464},{\"end\":57769,\"start\":57760},{\"end\":57780,\"start\":57769},{\"end\":57789,\"start\":57780},{\"end\":58225,\"start\":58218},{\"end\":58239,\"start\":58225},{\"end\":58250,\"start\":58239},{\"end\":58262,\"start\":58250},{\"end\":58541,\"start\":58530},{\"end\":58552,\"start\":58541},{\"end\":58562,\"start\":58552},{\"end\":58962,\"start\":58956},{\"end\":58970,\"start\":58962},{\"end\":58978,\"start\":58970},{\"end\":59466,\"start\":59456},{\"end\":59478,\"start\":59466},{\"end\":59493,\"start\":59478},{\"end\":59504,\"start\":59493},{\"end\":59518,\"start\":59504},{\"end\":59820,\"start\":59811},{\"end\":59829,\"start\":59820},{\"end\":59837,\"start\":59829},{\"end\":59848,\"start\":59837},{\"end\":53169,\"start\":53162},{\"end\":53177,\"start\":53169},{\"end\":53184,\"start\":53177},{\"end\":53481,\"start\":53471},{\"end\":53492,\"start\":53481},{\"end\":53505,\"start\":53492},{\"end\":53754,\"start\":53746},{\"end\":53766,\"start\":53754},{\"end\":54041,\"start\":54034},{\"end\":54049,\"start\":54041},{\"end\":54056,\"start\":54049},{\"end\":54068,\"start\":54056},{\"end\":54081,\"start\":54068},{\"end\":54412,\"start\":54403},{\"end\":54421,\"start\":54412},{\"end\":54430,\"start\":54421},{\"end\":54437,\"start\":54430},{\"end\":54450,\"start\":54437},{\"end\":54756,\"start\":54748},{\"end\":54762,\"start\":54756},{\"end\":54770,\"start\":54762},{\"end\":54777,\"start\":54770},{\"end\":54786,\"start\":54777},{\"end\":54798,\"start\":54786},{\"end\":55297,\"start\":55289},{\"end\":55304,\"start\":55297},{\"end\":55311,\"start\":55304},{\"end\":55319,\"start\":55311},{\"end\":55327,\"start\":55319},{\"end\":55775,\"start\":55767},{\"end\":55782,\"start\":55775},{\"end\":55789,\"start\":55782},{\"end\":55802,\"start\":55789},{\"end\":56189,\"start\":56179},{\"end\":56200,\"start\":56189},{\"end\":56207,\"start\":56200},{\"end\":56220,\"start\":56207},{\"end\":56587,\"start\":56578},{\"end\":56593,\"start\":56587},{\"end\":56604,\"start\":56593},{\"end\":57075,\"start\":57067},{\"end\":57084,\"start\":57075},{\"end\":57090,\"start\":57084},{\"end\":57098,\"start\":57090},{\"end\":57408,\"start\":57400},{\"end\":57415,\"start\":57408},{\"end\":57424,\"start\":57415},{\"end\":57432,\"start\":57424},{\"end\":57441,\"start\":57432},{\"end\":57449,\"start\":57441},{\"end\":57457,\"start\":57449},{\"end\":57464,\"start\":57457},{\"end\":57470,\"start\":57464},{\"end\":57769,\"start\":57760},{\"end\":57780,\"start\":57769},{\"end\":57789,\"start\":57780},{\"end\":58225,\"start\":58218},{\"end\":58239,\"start\":58225},{\"end\":58250,\"start\":58239},{\"end\":58262,\"start\":58250},{\"end\":58541,\"start\":58530},{\"end\":58552,\"start\":58541},{\"end\":58562,\"start\":58552},{\"end\":58962,\"start\":58956},{\"end\":58970,\"start\":58962},{\"end\":58978,\"start\":58970},{\"end\":59466,\"start\":59456},{\"end\":59478,\"start\":59466},{\"end\":59493,\"start\":59478},{\"end\":59504,\"start\":59493},{\"end\":59518,\"start\":59504},{\"end\":59820,\"start\":59811},{\"end\":59829,\"start\":59820},{\"end\":59837,\"start\":59829},{\"end\":59848,\"start\":59837}]", "bib_venue": "[{\"end\":53245,\"start\":53200},{\"end\":53469,\"start\":53379},{\"end\":53789,\"start\":53766},{\"end\":54155,\"start\":54081},{\"end\":54510,\"start\":54466},{\"end\":54879,\"start\":54798},{\"end\":55404,\"start\":55327},{\"end\":55883,\"start\":55802},{\"end\":56316,\"start\":56236},{\"end\":56695,\"start\":56604},{\"end\":57136,\"start\":57098},{\"end\":57398,\"start\":57328},{\"end\":57870,\"start\":57789},{\"end\":58296,\"start\":58262},{\"end\":58639,\"start\":58562},{\"end\":59059,\"start\":58978},{\"end\":59454,\"start\":59322},{\"end\":59925,\"start\":59848},{\"end\":53245,\"start\":53200},{\"end\":53469,\"start\":53379},{\"end\":53789,\"start\":53766},{\"end\":54155,\"start\":54081},{\"end\":54510,\"start\":54466},{\"end\":54879,\"start\":54798},{\"end\":55404,\"start\":55327},{\"end\":55883,\"start\":55802},{\"end\":56316,\"start\":56236},{\"end\":56695,\"start\":56604},{\"end\":57136,\"start\":57098},{\"end\":57398,\"start\":57328},{\"end\":57870,\"start\":57789},{\"end\":58296,\"start\":58262},{\"end\":58639,\"start\":58562},{\"end\":59059,\"start\":58978},{\"end\":59454,\"start\":59322},{\"end\":59925,\"start\":59848},{\"end\":53799,\"start\":53791},{\"end\":54947,\"start\":54881},{\"end\":55468,\"start\":55406},{\"end\":55951,\"start\":55885},{\"end\":56773,\"start\":56697},{\"end\":57938,\"start\":57872},{\"end\":58703,\"start\":58641},{\"end\":59127,\"start\":59061},{\"end\":59989,\"start\":59927},{\"end\":53799,\"start\":53791},{\"end\":54947,\"start\":54881},{\"end\":55468,\"start\":55406},{\"end\":55951,\"start\":55885},{\"end\":56773,\"start\":56697},{\"end\":57938,\"start\":57872},{\"end\":58703,\"start\":58641},{\"end\":59127,\"start\":59061},{\"end\":59989,\"start\":59927}]"}}}, "year": 2023, "month": 12, "day": 17}