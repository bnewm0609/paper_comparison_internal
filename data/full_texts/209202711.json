{"id": 209202711, "updated": "2023-10-06 20:49:46.64", "metadata": {"title": "GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences", "authors": "[{\"first\":\"Prune\",\"last\":\"Truong\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Danelljan\",\"middle\":[]},{\"first\":\"Radu\",\"last\":\"Timofte\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 12, "day": 11}, "abstract": "Establishing dense correspondences between a pair of images is an important and general problem, covering geometric matching, optical flow and semantic correspondences. While these applications share fundamental challenges, such as large displacements, pixel-accuracy, and appearance changes, they are currently addressed with specialized network architectures, designed for only one particular task. This severely limits the generalization capabilities of such networks to new scenarios, where e.g. robustness to larger displacements or higher accuracy is required. In this work, we propose a universal network architecture that is directly applicable to all the aforementioned dense correspondence problems. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution. The proposed GLU-Net achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, when using the same network and weights.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1912.05524", "mag": "3035477606", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/TruongDT20", "doi": "10.1109/cvpr42600.2020.00629"}}, "content": {"source": {"pdf_hash": "73f2e46efc7114d32d4b3df526d14a75a9cfbe7d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1912.05524v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/455563/4/GLUNet_CVPR.pdf", "status": "GREEN"}}, "grobid": {"id": "bdcf123e73d9387673b5105446e611e804c51ec0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/73f2e46efc7114d32d4b3df526d14a75a9cfbe7d.txt", "contents": "\nGLU-Net: Global-Local Universal Network for Dense Flow and Correspondences\n\n\nPrune Truong truongp@ethz.ch \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nMartin Danelljan martin.danelljan@ethz.ch \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nRadu Timofte radu.timofte@ethz.ch \nComputer Vision Lab\nETH Zurich\nD-ITETSwitzerland\n\nGLU-Net: Global-Local Universal Network for Dense Flow and Correspondences\n\nEstablishing dense correspondences between a pair of images is an important and general problem, covering geometric matching, optical flow and semantic correspondences. While these applications share fundamental challenges, such as large displacements, pixel-accuracy, and appearance changes, they are currently addressed with specialized network architectures, designed for only one particular task. This severely limits the generalization capabilities of such networks to new scenarios, where e.g. robustness to larger displacements or higher accuracy is required.In this work, we propose a universal network architecture that is directly applicable to all the aforementioned dense correspondence problems. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution. The proposed GLU-Net achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, when using the same network and weights.\n\nIntroduction\n\nFinding pixel-to-pixel correspondences between images continues to be a fundamental problem in Computer Vision [20,16]. This is due to its many important applications, including visual localization [53,60], 3Dreconstruction [2], structure-from-motion [52], image manipulation [18,40], action recognition [56] and autonomous driving [31]. Due to the astonishing developments in deep learning in recent years and its impressive performance, end-to-end trainable Convolutional Neural Networks (CNNs) are now applied for this task in all the aforementioned domains.\n\nThe general problem of estimating correspondences between pairs of images can be divided into several different tasks, depending on the origin of the images. In the geometric matching task [20], the images constitute different views of the same scene, taken by a single or multiple cameras. The images may be taken from radically different viewpoints, leading to large displacements and appearance transformations between the frames. On the other hand, optical flow [22,6] aims to estimate accurate pixel-wise displacements between two consecutive frames of a sequence or video. In the semantic matching problem [40,19] (also referred as semantic flow), the task is instead to find semantically meaningful correspondences between different instances of the same scene category or object, such as 'car' or 'horse'. Current methods generally address one of these tasks, using specialized architectures that generalize poorly to related correspondence problems. In this work, we therefore set out to design a universal architecture that jointly addresses all aforementioned tasks.\n\nOne key architectural aspect shared by a variety of cor-respondence networks is the reliance on correlation layers, computing local similarities between deep features extracted from the two images. This provide strong cues when establishing correspondences. Optical flow methods typically employ local correlation layers [15,28,59,58,25,26], evaluating similarities in a local neighborhood around an image coordinate. While suitable for small displacements, they are unable to capture large viewpoints changes. On the contrary, geometric and semantic matching architectures utilize global correlations [43,48,49,50,35], where similarities are evaluated between all pairs of locations in the dense feature maps. While capable of handling longrange matches, global correlation layers are computationally unfeasible at high resolutions. Moreover, they constrain the input image size to a pre-determined resolution, which severely hampers accuracy for high-resolution images. Contributions: In this paper, we propose GLU-Net, a Global-Local Universal Network for estimating dense correspondences. Our architecture is robust to large viewpoint changes and appearance transformations, while capable of estimating small displacements with high accuracy. The main contributions of this work are: (i) We introduce a single unified architecture, applicable to geometric matching, semantic matching and optical flow. (ii) Our network carefully integrates global and local correlation layers to handle both large and small displacements. (iii) To circumvent the fixed input resolution imposed by the global cost volume, we propose an adaptive resolution strategy that enables our network to take any image resolution as input, crucial for high-accuracy displacements. (iv) We train our network in a self-supervised manner, relying on synthetic warps of real images, thus requiring no annotated ground-truth flow. We perform comprehensive experiments on the three aforementioned tasks, providing detailed analysis of our approach and thorough comparisons with recent state-ofthe-art. Our approach outperforms previous methods for dense geometric correspondences on the HPatches [7] and ETH3D [54] datasets, while setting a new state-of-theart for semantic correspondences on the TSS [61] dataset. Moreover, our network, without any retraining or finetuning, generalizes to optical flow by providing highly competitive results on the KITTI [17] dataset. Both training code and models will be available at [1].\n\n\nRelated work\n\nFinding correspondences between a pair of images is a classical computer vision problem, uniting optical flow, geometric correspondences and semantic matching. This problem dates back several decades [22], with most classical techniques relying on hand crafted [4,21,39,3,8,41,51] or trained [45,14,62] feature detectors/descriptors, or variational formulations [22,6,40]. In recent years, CNNs have revolutionised most areas within vision, includ-ing different aspects of the image correspondence problem. Here, we focus on Convolutional Neural Network (CNN)based methods for generating dense correspondences or flow fields, as these are most related to our work. Optical Flow: Dosovitskiy et al. [15] constructed the first trainable CNN for optical flow estimation, FlowNet, based on a U-Net denoising autoencoder architecture [63] and trained it on a large synthetic FlyingChair dataset. Ilg et al. [28] stacked several basic FlowNet models into a large one, called FlowNet2, which performed on par with classical state-of-the-art on the Sintel benchmark [9]. Subsequently, Ranjan and Black [47] introduced SpyNet, a compact spatial image pyramid network.\n\nRecent notable contributions to end-to-end trainable optical flow include PWC-Net [59,58] and LiteFlowNet [25], followed by LiteFlowNet2 [26]. They employ multiple constrained correlation layers operating on a feature pyramid, where the features at each level are warped by the current flow estimate, yielding more compact and effective networks. Nevertheless, while these networks excel at small to medium displacements with small appearance changes, they perform poorly on strong geometric transformations or when the visual appearance is significantly different. Geometric Correspondence: Unlike optical flow, geometric correspondence estimation focuses on large geometric displacements, which can cause significant appearance distortions between the frames. Motivated by recent advancements in optical flow architectures, Melekhov et al. [43] introduced DGC-Net, a coarse-to-fine CNN-based framework that generates dense 2D correspondences between image pairs. It relies on a global cost volume constructed at the coarsest resolution. However, the input size is constrained to a fixed resolution (240\u00d7240), severely limiting its performance on higher resolution images. Rocco et al. [50] aim at increasing the performance of the global correlation layer by proposing an end-to-end trainable neighborhood consensus network, NC-Net, to filter out ambiguous matches and keep only the locally and cyclically consistent ones. Furthermore, Laskar et al. [38] utilize a modified version of DGC-Net, focusing on image retrieval. Semantic Correspondence: Unlike optical flow or geometric matching, semantic correspondence poses additional challenges due to intra-class appearance and shape variations among different instances from the same object or scene category. Rocco et al. [48,49] proposed the CNNGeo matching architecture, predicting globally parametrized affine and TPS transformations between image pairs. Other approaches aim to predict richer geometric deformations [12,34,33,50] using e.g. Spatial Transformer Networks [30]. Recently, Jeon et al. [32] introduced PARN, a pyramidal model where dense affine transformation fields are progressively estimated in a coarse-to-fine manner. SAM-Net [35] obtains better results by jointly learning semantic correspondence and attribute transfer. Huang et al. [24] proposed DCCNet, which fuses correlation maps derived from local features and from a newly designed context-aware semantic feature representation.\n\n\nMethod\n\nWe address the problem of finding pixel-wise correspondences between a pair of images I s \u2208 R H\u00d7W \u00d73 and I t \u2208 R H\u00d7W \u00d73 . In this work, we put no particular assumptions on the origin of the image pair itself. It may correspond to two different views of the same scene, two consecutive frames in a video, or two images with similar semantic content. Our goal is to estimate a dense displacement field, often referred to as flow, w \u2208 R H\u00d7W \u00d72 that warps image I s towards I t such that,\nI t (x) \u2248 I s (x + w(x)) .\n(1)\n\nThe flow w represents the pixel-wise 2D motion vectors in the target image coordinate system. It is directly related to the pixel correspondence map m(x) = x + w(x), which directly maps an image coordinate x in the target image to its corresponding position in the source image.\n\nIn this work, we design an architecture capable of robustly finding both long-range correspondences and accurate estimation of pixel-wise displacements. We thereby achieve a universal network for predicting dense flow fields, applicable to geometric matching, semantic correspondences and optical flow. The overall architecture follows a CNN feature-based coarse-to-fine strategy, which has proved widely successful for specific tasks [25,59,43,32,35]. However, contrary to previous works, our architecture combines global and local correlation layers, as discussed in Section 3.1 and 3.2, to benefit from their complementary properties. We further circumvent the input resolution restriction imposed by the global correlation layer by introducing an adaptive resolution strategy in Section 3.3. It is based on a two-stream feature pyramid, which allows dense correspondence prediction for any input resolution image. Our final architecture is detailed in Section 3.4 and the training procedure explained in 3.5.\n\n\nLocal and Global Correlations\n\nCurrent state-of-the-art architectures [59,25,24,32,43] for estimating image correspondences or optical flow rely on measuring local similarities between the source and target images. This is performed in a deep feature space, which provides a discriminative embedding with desirable invariances. The result, generally referred to as a correlation or cost volume, provides an extremely powerful cue when deriving the final correspondence or flow estimate. The correlation can be performed in a local or global manner. Local correlation: In a local correlation layer, the feature similarity is only evaluated in the neighborhood of the target image coordinate, specified by a search radius R. Formally, the correlation c l between the target F l t \u2208 R H l \u00d7W l \u00d7d l and source F l s \u2208 R H l \u00d7W l \u00d7d l feature maps is defined as,\nc l (x, d) = F l t (x) T F l s (x + d) , d \u221e \u2264 R ,(2)\nwhere x \u2208 Z 2 is a coordinate in the target feature map and d \u2208 Z 2 is the displacement from this location. The displacement is constrained to d \u221e \u2264 R, i.e. the maximum motion in any direction is R. We let l denote the level in the feature pyramid. While most naturally thought of as a 4-dimensional tensor, the two displacement dimensions are usually vectorized into one to simplify further processing in the CNN. The resulting 3D correlation volume c l thus has a dimensionality of H l \u00d7 W l \u00d7 (2R + 1) 2 .\n\nGlobal correlation: A global correlation layer evaluates the pairwise similarities between all locations in the target and source feature maps. The correlation volume C l \u2208 R H l \u00d7W l \u00d7H l \u00d7W l contains at each target image location x \u2208 Z 2 the scalar products between corresponding feature vector F l t (x) and the vectors F l s (x ) \u2208 R d extracted from all source feature map coordinates x ,\nC l (x, x ) = F l t (x) T F l s (x ) .(3)\nAs for the local cost volume, we vectorize the source dimensions, leading to a 3D tensor of size H l \u00d7 W l \u00d7 (H l W l ).\n\nComparison: Local and global correlation layers have a few key contrary properties and behaviors. Local correlations are popularly employed in architectures designed for optical flow [15,59,25], where the displacements are generally small. Thanks to their restricted search region, local correlation layers can be applied for high-resolution feature maps, which allows accurate estimation of small displacements. On the other hand, a local correlation based architecture is limited to a certain maximum range of displacements. Conversely, a global correlation based architecture does not suffer from this limitation, encapsulating arbitrary long-range displacements. The major disadvantage of the global cost volume is that its dimensionality scales with the size of the feature map H l \u00d7 W l . Therefore, due to the quadratic O((H l W l ) 2 ) scaling in computation and memory, global cost volumes are only suitable at coarse resolutions. Moreover, postprocessing layers implemented with 2D convolutions expect a fix channel dimensionality. Since the channel dimension H l W l of the cost volume depends on its spatial dimensions H l \u00d7 W l , this effectively constrains the network input resolution to a fixed pre-determined value, referred to as H L \u00d7 W L . The network can thus not leverage the more detailed structure in high-resolution images and lacks precision, since the images require down-scaling to H L \u00d7 W L before being processed by the network. Architectures with only local correlations (Local-Net) or with a unique global correlation (Global-Net) are represented in Figure 2a Figure 2. Schematic representation of different architectures for dense flow field estimation w. Local-Net (a) and Global-Net (b) employ only local and global correlation layers, respectively. Our GLOCAL-Net (c) combines both to effectively handle short and long-range displacements. GLU-Net (d) additionally employs our adaptive resolution strategy, thus capable of processing high-resolution images.\n\n\nGlobal-Local Architecture\n\nWe introduce a unified network that leverages the advantages of both global and local correlation layers and which also circumvents the limitations of both. Our goal is to handle any kind of geometric transformations -including large displacements -while achieving high precision for detailed and small displacements. This is performed by carefully integrating global and local correlation layers in a feature pyramid based network architecture.\n\nInspired by DGC-Net [43], we employ a global correlation layer at the coarsest level. The purpose of this layer is to handle the long-range correspondences. Since these are best captured in the coarsest scale, only a single global correlation is needed. In subsequent layers, the dense flow field is refined by computing image feature similarity using local correlations. This allows precise estimation of the displacements. Combining global and local correlation layers allows us to achieve robust and accurate prediction of both long and small-range motions. Such an architecture is visualized with GLOCAL-Net in Figure 2c. However, this network is still restricted to a certain input resolution. Next, we introduce a design strategy that circumvents this issue.\n\n\nAdaptive resolution\n\nAs previously discussed, the global correlation layer imposes a pre-determined input resolution for the network to ensure a constant channel dimensionality of the global cost volume. This severely limits the applicability and accuracy of the correspondence network, since higher resolution images requires down-scaling before being processed by the network, followed by up-scaling of the resulting flow. In this section, we address this key issue by introducing an architecture capable of taking images of any resolution, while still benefiting from a global correlation.\n\nOur adaptive-resolution architecture consists of two subnetworks, which operate on two different image resolutions. The first, termed L-Net, takes source and target images downscaled to a fixed resolution H L \u00d7 W L , which allows a global correlation layer to be integrated. The H-Net on the other hand, operates directly on the original image resolution H \u00d7 W , which is not constrained to any specific value. It refines the flow estimate generated by the L-Net with local correlations applied to a shallow feature pyramid constructed directly from the original images. It is schematically represented in Figure 2d.\n\nBoth sub-networks are based on a coarse-to-fine architecture, employing the same feature extractor backbone. In details, the L-Net relies on a global correlation at the coarsest level in order to effectively handle any kind of geometric transformations, including very large displacements. Subsequent levels of L-Net employ local correlations to refine the flow field. It is then up-sampled to the coarsest resolution of H-Net, where it serves as the initial flow estimate used for warping the source features F s . Subsequently, the flow prediction is refined numerous times within H-Net, that operates on the full scale images, thus providing a very detailed, sub-pixel accurate final estimation of the dense flow field relating I s and I t .\n\nFor high-resolution images, the upscaling factor between the finest pyramid level, l L , of L-Net and the coarsest, l H , of H-Net (see Figure 2d) can be significant. Our adaptive resolution strategy allows additional refinement steps of the flow estimate between those two levels during inference, thus improving the accuracy of the estimated flow, without training any additional weights. This is performed by recursively applying the l H layer weights at intermediate resolutions obtained by down-sampling the source and target features from l H . In summary, our adaptive resolution network is capable of seamlessly predicting an accurate flow field in the original input resolution, while also benefiting from robustness to long-range correspondences provided by the global layer. The entire network is trained end-to-end.\n\n\nArchitecture details\n\nIn this section, we provide a detailed description of our architecture. While any feature extractor backbone can be employed, we use the VGG-16 [10] network trained on Im-ageNet [37] to provide a fair comparison to previous works in geometric [43] and semantic correspondences [32]. For our L-Net, we set the input resolution to (H L \u00d7 W L ) = (256 \u00d7 256). It is composed of two pyramid levels, using Conv5-3 (16\u00d716 resolution) and Conv4-3 (32\u00d732 resolution) respectively. The former employs global correlation,  and H 4 \u00d7 W 4 respectively. The H-Net is purely based on local correlation layers. Our final architecture, composed of four pyramid levels in total, is detailed in Figure 3. Next, we describe the various architectural components. Coarsest resolution and mapping estimation: We compute a global correlation from the L 2 -normalized source and target features. The cost volume is further postprocessed by applying channel-wise L 2 -normalisation followed by ReLU [44] to strongly down-weight ambiguous matches [48]. Similar to DGC-Net [43], the resulting global correlation C is then fed into a correspondence map decoder M top to estimate a 2D dense correspondence map m at the coarsest level L1 of the feature pyramid:\nm 1 = M top C F 1 t , F 1 s .(4)\nThe correspondence map is then converted to a displacement field, as\nw 1 (x) = m 1 (x) \u2212 x.\nSubsequent flow estimations: The flow is refined by local correlation modules. At level l, the flow decoder M infers the residual flow \u2206w l as,\n\u2206w l = M c F l t , F l s ; R , up(w l\u22121 ) .(5)\nc is a local correlation (2) with search radius R and F l s (x) = F l s x + up w l\u22121 (x) is the warped source feature map F s according to the upsampled flow up w l\u22121 . The complete flow field is computed asw l = \u2206w l + up w l\u22121 . Flow refinement:\n\nContextual information have been shown advantageous for pixel-wise prediction tasks [11,24]. We thus use a sub-network R, called the refinement network, to post-process the estimated flow at the highest levels of L-Net and H-Net (L2 and L4 in Figure 3) by effectively enlarging the receptive field size. It takes the features f l of the second last layer from the flow decoder M l as input and outputs the refined flow w l = R f l +w l . For the other pyramid level (L3), the final flow field is w l =w l . Cyclic consistency: Since the quality of the correlation is of primary importance for the flow estimation process, we introduce an additional filtering step on the global cost volume to enforce the reciprocity constraint on matches. We employ the soft mutual nearest neighbor filtering introduced by [50] and apply it to post-process the global correlation.\n\n\nTraining\n\nLoss: We train our network in a single phase. We fix the pre-trained feature backbone during training and following FlowNet [15], we apply supervision at every pyramid level using the endpoint error (EPE) loss with respect to the ground truth displacements. Dataset: Our network is solely trained on pairs generated by applying random warps to the original images. Since our network is designed to also estimate correspondences between high-resolution images, training data of sufficient resolution is preferred in order to utilize the full potential of our architecture. We use a combination of the DPED [27], CityScapes [13] and ADE-20K [64] datasets, which have images larger than 750 \u00d7 750 with sufficiently diverse content. On the total dataset of 40, 000 images, we apply the same synthetic transformations as in [43]. The resulting image pairs are cropped to 520 \u00d7 520 for training. We call this dataset DPED-CityScape-ADE. We provide additional training and architectural details in the appendix. \n\n\nExperimental Validation\n\nIn this section, we comprehensively evaluate our approach for three diverse problems: geometric matching, semantic correspondences and optical flow. Importantly, we use the same network and model weights, trained on DPED-CityScape-ADE, for all three applications. More detailed results are available in the supplementary material.\n\n\nGeometric matching\n\nWe first apply our universal correspondence network for the task of geometric matching. In this problem, the images consist of different views of the same scene and include large geometric transformations. HP: We use the HPatches dataset [7], consisting of 59 sequences of real images with varying photometric and geometric changes. Each image sequence contains a source image and 5 target images taken under different viewpoints, with sizes ranging from 450 \u00d7 600 to 1613 \u00d7 1210. In addition to evaluating on the original image resolution (referred to as HP), we also evaluate on downscaled (240 \u00d7 240) images and ground-truths (HP-240) following [43]. ETH3D: To validate our approach for real 3D scenes, where image transformations are not constrained to simple homographies, we also employ the Multi-view dataset ETH3D [54]. It contains 10 image sequences at 480\u00d7752 or 514 \u00d7 955 resolution, depicting indoor and outdoor scenes and resulting from the movement of a camera completely unconstrained, used for benchmarking 3D reconstruction. The authors additionally provide a set of sparse geometrically consistent image correspondences (outputted by [52]) that have been optimized over the entire image sequence using the reprojection error. We sample image pairs from each sequence at different intervals to analyze varying magnitude of geometric transformations, and use the provided points as sparse ground truth correspondences. This results in about 500 image pairs in total for each selected interval. Metrics: In line with [43], we employ the Average End-Point Error (AEPE) and Percentage of Correct Keypoints (PCK) as the evaluation metrics. AEPE is defined as the Euclidean distance between estimated and ground truth flow fields, averaged over all valid pixels of the target image. PCK is computed as the percentage of correspondencesx j with an Euclidean distance error x j \u2212 x j \u2264 \u03b4, w.r.t. to the ground truth x j , that is smaller than a threshold \u03b4. Compared methods: We compare with DGC-Net [43], which is the current state-of-the-art for dense geometric matching, as well as with two state-of-the-art optical flow methods, PWC-Net [59] and LiteFlowNet [25], both trained on Flying-Chairs [15] followed by 3D-things [28]. We use the pre-trained weights provided by the authors. Results: We first present results on the HP and HP-240 in Table 1. Our model strongly outperforms all others by a large margin both in terms of accuracy (PCK) and robustness (AEPE). It is interesting to note that while our model is already better than DGC-Net on the small resolution HP-240, the gap in performance further broadens when  Figure 5. Quantitative results on geometric matching dataset ETH3D [54]. AEPE and PCK-5 are computed on pairs of images sampled from consecutive images of ETH3D at different intervals.\n\nincreasing the image resolution. Particularly, GLU-Net obtains a PCK-1px value almost four times higher than that of DGC-Net on HP. This demonstrates the benefit of our adaptive resolution strategy, which enables to process highresolution images with great accuracy. Figure 4 shows qualitative examples of different networks applied to HP images and ETH3D image pairs taken by two different cameras.\n\nOur GLU-Net is robust to large view-points variations as well as drastic changes in illumination.\n\nIn Figure 5, we plot AEPE and PCK-5px obtained on the ETH3D scenes for different intervals between image pairs. For small intervals, finding correspondences strongly resembles optical flow task while increasing it leads to larger displacements. Therefore, specialised optical flow methods PWC-Net [59] and LiteFlowNet [25] obtain slightly better AEPE and PCK for low intervals, but rapidly degrade for larger ones. In all cases, our approach consistently outperforms DGC-Net [43] in both metrics by a large margin.\n\n\nSemantic matching\n\nHere, we perform experiments for the task of semantic matching, where images depict different instances of the same object category, such as cars or horses. We use the same model and weights as in the previous section. Dataset and metric: We use the TSS dataset [61], which provides dense flow fields annotations for the foreground object in each pair. It contains 400 image pairs, divided into three groups: FG3DCAR, JODS, and PASCAL, according to the origins of the images. Following Taniai et al. [61], we report the PCK with a distance threshold equal to \u03b1 \u00b7 max(H s , W s ), where H s and W s are the dimensions of the source image and \u03b1 = 0.05.\n\n\nCompared methods:\n\nWe compare to several recent state-of-the-art methods specialised in semantic matching [49,50,32,24,33,35]. In addition to our universal network, we evaluate a version that adopts two architectural details that are used in the semantic correspondence literature. Specifically, we add a consensus network [50] for the global correlation layer and concatenate features from different levels in the L-Net, similarly to [32] (see Section 4.4 for an analysis). We call this version Semantic-GLU-Net. To accommodate reflections, which do not occur in geo- metric correspondence scenarios, we infer the flow field on original and flipped versions of the target image and output the flow field with least horizontal average magnitude.\n\nResults: We report results on TSS in Table 2. Our universal network obtains state-of-the-art performance on average over the three TSS groups. Moreover, individual results on FG3Dcar and PASCAL are very close to best metrics. This shows the generalization properties of our network, which is not trained on the same magnitude of semantic data. In contrast, most specialized approaches fine-tuned on PASCAL data [19]. Finally, including architectural details specifically for semantic matching, termed Semantic-GLU-Net, further improves our performance, setting a new state-ofthe-art on TSS, by improving a substantial 1.0% PCK over the previous best. Interestingly, we outperform methods that use a deeper, more powerful feature backbone. Qualitative examples of our approach are shown in Figure 6.\n\n\nOptical flow\n\nFinally, we apply our network, with the same weights as previously, for the task of optical flow estimation. Here, the image pairs stem from consecutive frames of a video. Dataset and metric: For optical flow evaluation, we use the KITTI dataset [17], which is composed of real road sequences captured by a car-mounted stereo camera rig. The 2012 set only consists of static scenes while the 2015 set is extended to dynamic scenes. For this task, we follow the standard evaluation metric, namely the Average End-Point Error (AEPE). We also use the KITTI-specific F1 metric, which represents the percentage of outliers.   Table 3. Quantitative results on optical flow KITTI training datasets [17]. Fl-all: Percentage of outliers averaged over all pixels. Inliers are defined as AEP E < 3 pixels or < 5%. Lower F1 and AEPE are best. (Note: * the values are computed using the trained models provided by the authors).\n\nCompared methods: We employ state-of-the-art PWC-Net [59,58] and LiteFlowNet [25] trained on Flying-Chairs [15] and 3D-things [28]. We also compare to DGC-Net [43] for completeness. Results: Since we do not finetune our model, we only evaluate on the KITTI training sets. For fair comparison, we compare to models not finetuned on the KITTI training data. The results are shown in Table 3 and a qualitative example is illustrated in Figure 7.\n\nOur network obtains highest AEPE on both KITTI-2012 and KITTI-2015. Nevertheless, we observe that our approach achieves a larger F1 measure on KITTI-2015 compared to approaches specifically trained and designed for optical flow. This is largely due to our self-supervised training data, which currently does not model independently moving objects or occlusions, but could be included to pursue a more purposed optical flow solution. Yet, our approach demonstrates competitive results for this challenging task, without training on any optical flow data. This clearly shows that our network can not only robustly estimate long-range correspondences, but also accurate small displacements.\n\n\nAblation study\n\nHere, we perform a detailed analysis of our approach. Local-global architecture: We first analyze the impact of global and local correlation layers in our dense correspondence framework. We compare using only local layers (Local-Net), a global layer (Global-Net) and our combination (GLOCAL-Net), presented in Figure 2. As shown in Table 4, Local-Net fails on the HP dataset, due to its inability to capture large displacements. While the Global-Net can handle large viewpoint changes, it achieves inferior accuracy compared to GLOCAL-Net, which additionaly integrates local correlations layers. Adaptive resolution: By further adding the adaptive reso-    lution strategy (Section 3.3), our approach (GLU-Net in Table 4) achieves a large performance gain in all metrics compared to GLOCAL-Net. This improvement is most prominent for high resolution images, i.e. the original HP data. Iterative refinement: From Table 4, applying iterative refinement (it-R) clearly benefits accuracy for high-resolution images (HP). This further allows us to seamlessly add extra flow refinements, without incurring any additional network weights, in order to process images of high resolution. Global correlation: Lastly, we explore design choices for the global correlation block in our architecture. As shown in Table 5, adding cyclic consistency (CC) [50] as a postprocessing brings improvements for all datasets. Subsequently adding NC-Net and concatenating features of L-Net (Concat-F) lead to major overall gain on the HP [7] and TSS [61] datasets. However, we observe a slight degradation in accuracy, as seen on KITTI [17]. We therefore only include these components for the Semantic-GLU-Net version (Section 4.2) and not in our universal GLU-Net.\n\n\nConclusion\n\nWe propose a universal coarse-to-fine architecture for estimating dense flow fields from a pair of images. By carefully combining global and local correlation layers, our network effectively estimates long-range displacements while also achieving high accuracy. Crucially, we introduce an adaptive resolution strategy to counter the fixed input resolution otherwise imposed by the global correlation. Our universal GLU-Net is thoroughly evaluated for the three diverse tasks of geometric correspondences, semantic matching and optical flow. When using the same model weights, our network achieves state-of-the-art performance on all above tasks, demonstrating its universal applicability.\n\nSince the quality of the correlation layer output is of primary importance for the flow estimation process, we introduce an additional filtering step on the global cost volume to enforce the reciprocity constraint on matches. To encourage matched features to be mutual nearest neighbours, we employ the soft mutual nearest neighbor filtering introduced by [50] and apply it to post-process the global correlation.\n\nThe soft mutual nearest neighbor module filters a global correlation C \u2208 R H\u00d7W \u00d7H\u00d7W into\u0108 \u2208 R H\u00d7W \u00d7H\u00d7W such that: C(i, j, k, l) = r t (i, j, k, l) \u00b7 r s (i, j, k, l) \u00b7 C(i, j, k, l) (6) with r s (i, j, k, l) and r t (i, j, k, l) the ratios of the score of the particular match C(i, j, k, l) with the best scores along each pair of dimensions corresponding to images I s and I t respectively. We present the formula for r s (i, j, k, l) below, the same applies for r t (i, j, k, l).\nr t (i, j, k, l) = C(i, j, k, l) max ab C(a, b, k, l)(7)\nThis cyclic consistency post-processing step does not add any training weights.\n\n\nA.2. Mapping decoder M top\n\nIn this sub-section, we give additional details of the mapping decoder M top for the global correlation layer (Eq. 4 and Figure 3 in the paper). We compute a global correlation from the L 2 -normalized source and target features. The cost volume is further post-processed by applying channel-wise L 2 -normalisation followed by ReLU [44] to strongly downweight ambiguous matches [48]. Similar to DGC-Net [43], the resulting global correlation layer C is then fed into a correspondence map decoder M top to estimate a 2D dense correspondence map m at the coarsest level L 1 of the feature pyramid,\nm 1 = M top C F 1 t F 1 t , F 1 s F 1 s .(8)\nThe outputted mapping estimate is parameterized such that each predicted pixel location in the map belongs to the interval [\u22121; 1] representing width and height normalized image coordinates. The correspondence map is then re-scaled to image coordinates and converted to a displacement field.\nw 1 (x) = m 1 (x) \u2212 x .(9)\nThe decoder M top consists of 5 feed-forward convolutional blocks with a 3 \u00d7 3 spatial kernel. The number of feature channels of each convolutional layers are respectively 128, 128, 96, 64, and 32. The final output of the mapping decoder is the result of a linear 2D convolution, without any activation.\n\n\nA.3. Flow decoder M\n\nHere, we give additional details of the flow decoder M for the local correlation layers (Eq. 5 and Figure 3 in the paper). At level l, the flow decoder M infers the residual flow \u2206w l as,\n\u2206w l = M c F l t , F l s ; R , up(w l\u22121 ) .(10)\nHere, c is a local correlation volume with search radius R and F l s (x) = F l s x + up w l\u22121 (x) is the warped source feature map F s according to the upsampled flow up w l\u22121 . The complete flow field is then computed asw l = \u2206w l + up w l\u22121 .\n\nThe flow decoder at level 4 (see Figure 3) additionally takes an input de 2 (f l\u22121 ), obtained by applying a transposed convolution layer de 2 to the features f l\u22121 of the second last layer from the flow decoder M l\u22121 . This additional inputs was first introduced and utilized in PWC-Net [59] at every pyramid level. It enables the decoder of the current level to obtain some information about the correlation at the previous level. In GLU-Net, this additional input to the flow decoder only appears in H-Net since in L-Net, a global correlation and mapping decoder precede the flow decoder.\n\nAs for the flow decoder M , we employ a similar architecture to the one in PWCNet [59]. It consists of 5 convolutional layers with DenseNet connections [23]. The numbers of feature channels at each convolutional layers are respectively 128, 128, 96, 64, and 32, and the spatial kernel of each convolution is 3 \u00d7 3. DenseNet connections are used since they have been shown to lead to significant improvement in image classification [23] and optical flow estimation [59]. The final output of the flow decoder is the result of a linear 2D convolution, without any activation.\n\n\nA.4. Refinement network R\n\nHere, we explain in more details the refinement network R (Figure 3 in the paper). The refinement network aims to refine the pixel-level flow fieldw l , thus preventing erroneous flows from being amplified by up-sampling and passing to the next pyramid level. Its architecture is the same than the context network employed in PWC-Net [59]. It is a feed-forward CNN with 7 dilated convolutional layers [57], with varying dilation rates. Dilated convolutions enlarge the receptive field without increasing the number of weights. From bottom to top, the dilation constants are 1, 2, 4, 8, 16, 1, and 1. The spatial kernel is set to 3 \u00d7 3 for all convolutional layers.\n\n\nA.5. Details about Local-net, Global-Net and GLOCAL-Net\n\nIn Figure 2 of the main paper, we introduced Local-Net, Global-Net and GLOCAL-Net to investigate the differences between architectures based on local correlation layers, a global correlation layer or a combination of the two, respectively. All three networks are composed of three pyramid levels and use the same feature extractor backbone VGG-16 [10]. The mapping and flow decoders have the same architecture as those used for GLU-Net and described above. For Global-Net, the pyramid levels following the global correlation level employ concatenation of the target and warped source feature maps, as suggested in DGC-Net [43]. They are fed to the flow estimation decoder along with the up-sampled flow from the previous resolution. Finally, Global-Net and GLOCAL-Net are both restricted to a pre-determined input resolution H L \u00d7 W L due to their global correlation at the coarsest pyramid level. On the other hand, Local-Net, which only relies on global correlations, can take input images of any resolutions.\n\n\nA.6. Iterative refinement\n\nHere we provide more details about the iterative refinement procedure described in Section 3.3 in the paper. For high-resolution images, the upscaling factor between the finest pyramid level, l L , of L-Net and the coarsest, l H , of H-Net (see Figure 8) can be significant. Our adaptive resolution strategy allows additional refinement steps of the flow estimate between those two levels during inference, thus improving the accuracy of the estimated flow, without training any additional weights. This is performed by recursively applying the l H layer weights at intermediate resolutions obtained by down-sampling the source and target features from l H .\n\nParticularly, we apply iterative refinement if the ratio between the resolutions of the l H and l L levels is larger than three. We then iteratively perform refinements at intermediate resolutions, obtained by a reduction of factor 2 from l H in each step, until the ratio between the resolution of the coarsest intermediate level and the resolution of l L is smaller than 2.\n\nIn mode details, we construct a local correlation layer from the source and target feature maps of level l H downsampled to the desired intermediate resolution. We then apply the weights of the level l H decoder to the local correlation, therefore obtaining an intermediate refinement of the flow field. This process is illustrated in Figure 8, where the gap between l L and l H here allows for two additional flow field refinements.\n\n\nB. Training details\n\nHere, we provide additional details about the training procedure and the training dataset.\n\n\nB.1. Loss\n\nWe freeze the weights of the feature extractor during training. Let \u03b8 denote the learnable parameters of the network. Let w l \u03b8 = (w x l , w y l ) \u2208 R H l \u00d7W l \u00d72 denote the flow field estimated by the network at the l th pyramid level. w l GT refers to the corresponding dense flow ground-truth, computed from the random warp. We employ the multiscale training loss, first introduced in FlowNet [15],\nL(\u03b8) = L l=L1 \u03b1 l x w l \u03b8 (x) \u2212 w l GT (x) + \u03b3 \u03b8 ,(11)\nwhere \u03b1 l are the weights applied to each pyramid level and the second term of the loss regularizes the weights of the network. We do not apply any mask during training, which means that occluded regions (that do not have visible matches) are included in the training loss. Since the image pairs are related by synthetic transformations, these regions do have a correct ground-truth flow value. For our adaptive resolution strategy, we down-sample and scale the ground truth from original resolution H \u00d7 W to H L \u00d7 W L in order to obtain the ground truth flow fields for L-Net. Similarly to FlowNet [15] and PWC-Net [59], we down-sample the ground truth from the base resolution to the different pyramid resolutions without further scaling, so as to obtain the supervision signals at the different levels.\n\n\nB.2. Dataset\n\nTo use the full potential of our GLU-Net, training should be performed on high-resolution images. We create the training dataset following the procedure in DGC-Net [43], but enforcing the condition of high resolution. We use the same 40, 000 synthetic transformations (affine, thin-plate and homographies), but apply them to our higher resolution images collected from the DPED [27], CityScapes [13] and ADE-20K [64] datasets. Indeed, DPED images are very large, however the DPED training dataset is composed of  only approximately 5000 sets of images taken by four different cameras. We use the images from two cameras, resulting in around 10, 000 images. CityScapes additionally adds about 23, 000 images. We complement with a random sample of ADE-20K images with a minimum resolution of 750 \u00d7 750.\n\n\nB.3. Implementation details\n\nAs a preprocessing step, the training images are meancentered and normalized using mean and standard deviation of ImageNet dataset [37]. For the training of Global-Net, Local-Net and GLOCAL-Net, we use a batch size of 32 and an initial learning rate of 10 \u22122 which is gradually decreased during training. The weights in the training loss are set to be \u03b1 1 = 0.32, \u03b1 2 = 0.08, \u03b1 3 = 0.02.\n\nOur final network GLU-Net is trained with a batch size of 16 and the learning rate initially equal to 10 \u22124 . The weights in the training loss are set to be \u03b1 1 = 0.32, \u03b1 2 = 0.08, \u03b1 3 = 0.02, \u03b1 4 = 0.01. Our system is implemented using Pytorch [46] and our networks are trained using Adam optimizer [36] with learning rate decay of 0.0004.\n\n\nC. Detailed results\n\n\nC.1. Run time\n\nWe compare the run time of our method with state-ofthe-art approaches over the HP-240 images in Table 6  scaling operation is included in the estimated time.\n\nOur network GLU-Net obtains similar run time than PWC-Net and is three times faster than DGC-Net. This is due to the fact that PWC-Net, LiteFlowNet and GLU-Net outputs a flow at a quarter image resolution whereas DGC-Net refines the estimated flow field with two additional pyramid levels until the fixed resolution of 240\u00d7240.\n\n\nC.2. Geometric matching\n\nWe provide the detailed results on HP and ETH3D datasets, as well as extensive additional qualitative examples.\n\n\nC.2.1 Results on HPatches dataset\n\nComparison to different training datasets:\n\nDGC-Net [43] is trained on pairs of 240 \u00d7 240 images created from applying synthetic transformations to the 480 \u00d7 640 Tokyo Time Machine dataset [5] and cropping them (denoted as tokyo). Since we cannot train GLU-Net on the same tokyo dataset due to its small resolution, for completeness, we additionally trained our GLOCAL-Net, that also has a fixed input resolution, on tokyo and compare the results to GLOCAL-Net trained on our CityScape-DPED-ADE dataset. It is important to note that GLOCAL-Net has 3 pyramid levels, the finest one at 64 \u00d7 64 resolution for a pre-determined input resolution of 256 \u00d7 256. On the other hand, DGC-Net [43] has 5 pyramid levels, the last one applied on input resolution 240 \u00d7 240. We evaluate those networks on HP-240 and HP and present the results in Table 8.\n\nGLOCAL-Net obtains similar results on both HP and HP-240 datasets, independently of its training data tokyo or CityScape-DPED-ADE. Since both datasets were created by applying the same synthetic transformations, this support the fact that transformation and displacement statistics are more important for generalization properties than image content [42,55,58].\n\nDetailed results on HP and HP-240: Detailed results obtained by different models on the various view-points of the HP and HP-240 datasets are presented in Table 7. It corresponds to Table 1 of the main paper. We outperform all other methods for each viewpoint ID on both low resolution (HP-240) and high-resolution images (HP). Particularly, our network permits to gain a lot of accuracy (in the order of 3 to 4 times higher for PCK-1 on HP) as compared to DGC-Net.\n\nAdditional qualitative examples are shown in Figure 12.\n\nWe additionally present the PCK curves computed over the different viewpoints of HP, as a function of the relative distance threshold. We do not set a pixel-level thresholds for the PCK curves since HP image pairs have different resolutions in general. GLU-Net achieves better accuracy (better PCK) for all thresholds compared to PWC-Net [59], Lite-FLowNet [25] and DGC-Net [43]. Importantly, GLU-Net obtains significantly better PCK for low thresholds.\n\n\nC.2.2 Results on ETH3D\n\nIn the main paper, Figure 5, we quantitatively evaluated our approach over pairs of ETH3D images sampled from consecutive frames at different intervals. In Table 9, we give the corresponding detailed evaluation metrics (AEPE and PCK) obtained by PWC-Net, LiteFlowNet, DGC-Net and GLU-Net.\n\nHere, we additionally provide qualitative examples of the different networks and GLU-Net applied to pairs of im-   ages at different intervals in Figure 11. It is visible that while optical flow methods achieve good results for low intervals, the warped source images according to their outputted flows get worst when increasing the intervals between image pairs. On the other hand, our model produces flow fields of constant quality.\n\nQualitative results: We additionally use ETH3D images to demonstrate the superiority of our approach to deal with extreme viewpoint changes on the one hand, and radical illumination and appearance variations on the other hand.\n\nIn addition to the medium resolution images evaluated previously, ETH3D [54] also provides several additional scenes taken with high-resolution cameras, acquiring images at 24 Megapixel (6048 \u00d7 4032). Since the images of a sequence are taken by a unique camera, consecutive pairs of images show only little lightning variations, however they are related by very wide view-point changes. As there are no ground-truth correspondences provided along with the images, we only evaluate qualitatively on consecutive pairs of images. The original images of 6048 \u00d7 4032 are down-samled by a factor of 2 for practical purposes. We show quantitative results over a few of those images in On the other hand, our network can also handle large appearances changes due to variation in illumination or due to the use of different optics. For this purpose, we utilize additional examples of pairs of images from ETH3D taken by two different cameras simultaneously. The camera of the first images has a field-of-view of 54 degrees while the other camera has a field of view of 83 degrees. They capture images at a resolution of 480 \u00d7 752 or 514 \u00d7 955 depending on the scenes and on the camera. The exposure settings of the cameras are set to automatic for all datasets, allowing the device to adapt to illumination changes. Qualitative examples of state-of-the-art methods and GLU-Net applied to such pairs of images are presented in Figure 13. GLU-Net is robust to changes in lightning conditions as well as to artifacts. While DGC-Net [43] obtains satisfactory results, the warped image according to its outputted flow is often blurry whereas we always obtain sharp, almost perfect warped source images. \n\n\nC.3. Semantic correspondences\n\nIn Figure 14, we present additional qualitative results on the TSS [61] dataset of our universal network (GLU-Net) and its modified version (Semantic-GLU-Net), which includes NC-Net [50] and feature concatenation [32].  Table 9. Metrics evaluated over scenes of ETH3D with different intervals between consecutive pairs of images (taken by the same camera). Small AEPE and high PCK are better.\n\n\nC.4. Detailed ablative analysis\n\nIn this section, we provide additional ablation experiments. All networks are trained on CityScape-DPED-ADE dataset.\n\nCoarse-to-fine-approach: We first defend the use of a coarse-to-fine approach with a feature pyramid. We report AEPE and PCK metrics for the flow estimates obtained at different levels of the feature pyramid of GLU-Net model in Table 10. On the flow field estimated at each level, we apply bilinear interpolation to the original image resolution and multiply the estimated flow with the corresponding scale factor for the levels of L-Net. The end-point error decreases from the coarsest level to the highest level of the pyramid while the accuracy (PCK) increases. This supports the use of a pyramidal model. Scale pyramid level of the adaptive resolution: In Table 11, we present the influence of the pyramid level at which the adaptive resolution module is integrated in the four-level pyramid network. Having a single level in L-Net (corresponding to the global correlation layer) and three pyramid levels in H-Net (referred to as 3L) lead to poor results on all datasets, even compared to GLOCAL-Net. On the other hand, both other alternatives (1 or 2 levels in H-Net) bring about major improvements of robustness (AEPE) and accuracy (PCK) on HPatches dataset, particularly on the high-resolution images HP. However, having only one level in H-Net (1L) degrades the performances obtained on the semantic dataset TSS. H-Net and L-Net both comprised of 2 pyramid levels (2L) appears as the best option to achieve competitive results on geometric matching, optical flow as well as semantic matching.   Table 10. Effect of coarse-to-fine approach for our GLU-Net: Metrics calculated over HP images. The flow estimated at each pyramid level is up-sampled to original image resolution and metrics are calculated at this resolution. Table 11. Effect of adaptive resolution and its position. All networks are without iterative refinement and without cyclic consistency. 2 H-Net levels (2L) is the only alternative for a universal network applicable to geometric matching, semantic correspondence and optical flow.   Qualitative examples of our universal network GLU-Net as well as GLU-Net with specific architectural details from the semantic correspondence literature applied to TSS images. The additional architectural modules are the Neighborhood Consensus Network NC-Net [50] and concatenating features within the L-Net [32]. Adopting those two modules leads to Semantic-GLU-Net. The source images are warped according to the flow fields outputted by the different networks. The warped source images should resemble the target images and the ground-truths.\n\nFigure 1 .\n1Our GLU-Net estimates dense correspondences between a source (left) and a target (right) image. The estimated correspondences are here used to warp (center) the source image. The warped result (center) accurately matches the target image (right). The same network and weights are applied for Geometric matching (top row), Semantic matching (second row) and Optical flow (bottom row) tasks.\n\n\n, b.\n\nFigure 3 .\n3Architectural details of our GLU-Net. It is composed of two modules, operating on two different image resolutions. The L-Net (right) relies on a global correlation for long-range matches, while the H-Net (left) refines the flow estimate with local correlations. while the latter is based on a local correlation. The H-Net is composed of two feature pyramid levels extracted from the original image resolution H \u00d7 W . For this purpose, we employ Conv4-3 and Conv3-3 having resolutions H 8 \u00d7 W 8\n\nFigure 4 .\n4Qualitative comparison with state-of-the-art on geometric correspondence datasets. Top: Pairs of HP images. Bottom: Pairs of images from ETH3D taken by two different cameras. Our approach effectively handles large variations in view-point and appearance.\n\nFigure 6 .\n6Qualitative examples of GLU-Net (Ours) and Semantic-GLU-Net applied to TSS images[61].\n\nFigure 7 .\n7Visualization of the flow outputted by different methods for a KITTI-2012 image.\n\n\n) GLU-Net with iterative refinement between L-Net and H-Net.\n\nFigure 8 .\n8Schematic representation of iterative refinement. The features and weights of lH level of H-Net are iteratively applied at intermediate resolutions between L-Net and H-Net.\n\n\n. The timings have been obtained on the same desktop with an NVIDIA GTX 1080 Ti GPU. The HP-240 images are of size 240 \u00d7 240, which corresponds to the pre-determined input resolution of DGC-Net. For PWC-Net, LiteFlowNet and GLU-Net, the images are resized to 256 \u00d7 256 before being passed through the networks. We do not consider this resizing in the estimated time. They all output a flow at a quarter resolution the input image. We up-scale to the image resolution 240 \u00d7 240 with bilinear interpolation. This up-PWC-Net LiteFlowNet DGC-Net GLU-Net (Ours) Run-time [ms] 38.51 45.10 138.30 38.10\n\nFigure 9 .\n9PCK curves obtained by state-of-the-art methods and GLU-Net over the different view-points of HP.\n\n\nFig-ure 10. GLU-Net is capable of handling very large motions, where the other methods partly (DGC-Net) or completely fail (PWC-Net and LiteFlowNet).\n\nFigure 10 .\n10Qualitative examples of state-of-the-art methods applied to very high-resolution images of different scenes of ETH3D. The presented image pairs show substantial view-point changes, and thus very large motions.\n\nFigure 11 .\n11Qualitative examples of multiple networks and our GLU-Net applied to pairs of ETH3D dataset taken at different intervals. The source images are warped according to the flow fields outputted by the different networks. The warped source images should resemble the target images. Optical flow methods obtain good qualitative results for low intervals (3 and 5) but largely degrade on bigger intervals. On the contrary, GLU-Net has a steady performance over all intervals. AEPE PCK-1px [%] PCK-5px [%] Level\n\nFigure 12 .\n12Qualitative examples of different state-of-the-art algorithms and our GLU-Net applied to HP images. The source images are warped according to the flow fields outputted by the different networks. The warped source images should resemble the target images. Our method GLU-Net is robust to drastic view-point changes.\n\nFigure 13 .\n13Qualitative examples of ETH3D pairs of images taken simultaneously by two different cameras. The two cameras have different field-of-views and sometimes different resolutions. Pairs of images experience drastic differences in lightning conditions. The source images are warped according to the flow fields outputted by different state-of-the-art networks and by our GLU-Net. The warped source images should resemble the target images.\n\nFigure 14 .\n14Figure 14. Qualitative examples of our universal network GLU-Net as well as GLU-Net with specific architectural details from the semantic correspondence literature applied to TSS images. The additional architectural modules are the Neighborhood Consensus Network NC-Net [50] and concatenating features within the L-Net [32]. Adopting those two modules leads to Semantic-GLU-Net. The source images are warped according to the flow fields outputted by the different networks. The warped source images should resemble the target images and the ground-truths.\n\n\nAEPE-all F1-all [%] AEPE-all F1-all [%]KITTI-2012 \n\nKITTI-2015 \nLiteFlowNet [25] \n4.00 \n17.50  *  \n10.39 \n28.50 \nPWC-Net [59, 58] \n4.14 \n20.01  *  \n10.35 \n33.67 \nDGC-Net [43] \n8.50  *  \n32.28  *  \n14.97  *  \n50.98  *  \nGLU-Net \n3.34 \n18.93 \n9.79 \n37.52 \n\n\n\nTable 4 .\n4Effect of global and local correlations as well of adaptive \nresolution strategy. it-R: iterative refinement, introduced with our \nadaptive resolution (Section 3.3), CC: cyclic-consistency [50]. \n\nNo CC + CC (Ours) + NC-Net + Concat-F \n\nHP \n\nAEPE \n25.09 \n25.05 \n22.00 \n21.40 \nPCK-1px [%] 36.81 \n39.55 \n37.62 \n38.49 \nPCK-5px [%] 77.55 \n78.54 \n79.41 \n79.50 \n\nKITTI-AEPE \n3.56 \n3.34 \n3.80 \n3.85 \n2012 \nF1-all [%] \n21.67 \n18.93 \n23.49 \n23.84 \n\nTSS \nPCK [%] \n78.97 \n79.21 \n82.10 \n82.76 \n\n\n\nTable 5 .\n5Effect of additional architectural details. All models are \nwith iterative refinement. We add CC: cyclic-consistency [50], \nNC-Net: Neighborhood Consensus network [50], Concat-F: Con-\ncatenation of features of L-Net [32]. \n\n\n\nTable 6 .\n6Run time of our methods compared to optical-flow com-\npetitors PWC-Net and LiteFlowNet as well as geometric matching \ncompetitor DGC-Net, averaged over 295 image pairs of HP-240. \n\n\n\nHP-240x240 HP AEPE PCK-1px PCK-5px AEPE PCK-1px PCK-5pxTable 8. Metrics evaluated on geometric matching dataset HPatches for DGC-Net and our networks trained on different datasets.DGC-Net (tokyo) \n9.07 \n50.01 % \n77.40 % \n33.26 12.00 % \n58.06 % \n\nGLOCAL-Net (CityScape-DPED-ADE) 8.77 \n48.53 % \n78.12 % \n31.64 10.23 % \n56.72 % \nGLOCAL-Net (tokyo) \n8.48 \n41.00 % \n77.86 % \n31.16 7.31 % \n49.08 % \nGLU-Net (CityScape-DPED-ADE) \n7.40 \n59.92 % \n83.47% \n25.05 39.55% \n78.54 % \n\n\n\n\nLiteFlowNet PWC-Net DGC-Net OursAEPE \n1.774 \n1.84 \n2.53 \n2.06 \ninterval = 3 \nPCK-1px [%] \n58.87 \n54.14 \n31.50 \n47.47 \nPCK-5px [%] \n92.64 \n92.44 \n88.34 \n91.03 \n\nAEPE \n2.676 \n2.175 \n3.321 \n2.61 \ninterval = 5 \nPCK-1px [%] \n53.64 \n47.02 \n25.23 \n40.22 \nPCK-5px [%] \n90.55 \n90.53 \n83.07 \n87.74 \n\nAEPE \n6.13 \n3.269 \n4.212 \n3.54 \ninterval = 7 \nPCK-1px [%] \n46.97 \n39.69 \n20.90 \n34.41 \nPCK-5px [%] \n86.274 \n86.88 \n78.17 \n84.06 \n\nAEPE \n13.012 \n5.63 \n5.38 \n4.28 \ninterval = 9 \nPCK-1px [%] \n39.52 \n32.61 \n17.61 \n30.25 \nPCK-5px [%] \n78.38 \n81.00 \n73.58 \n80.58 \n\nAEPE \n29.768 \n14.394 \n6.81 \n5.65 \ninterval = 11 PCK-1px [%] \n31.10 \n26.15 \n14.88 \n26.54 \nPCK-5px [%] \n65.95 \n71.74 \n69.09 \n76.61 \n\nAEPE \n52.40 \n27.52 \n9.04 \n7.59 \ninterval = 13 PCK-1px [%] \n24.89 \n21.30 \n12.83 \n23.45 \nPCK-5px [%] \n54.96 \n63.07 \n64.10 \n72.16 \n\nAEPE \n74.94 \n43.43 \n12.25 \n10.82 \ninterval = 15 PCK-1px [%] \n19.92 \n17.03 \n10.69 \n20.48 \nPCK-5px [%] \n46.10 \n54.25 \n58.52 \n67.64 \n\n\n\n\n1 [16 \u00d7 16] [H/8 \u00d7 W/8] 26.43 30.47 74.44 Level 4 [H/4 \u00d7 W/4] 25.05 39.55 78.5445.49 0.70 \n13.53 \nLevel 2 [32 \u00d7 32] \n30.00 6.27 \n50.29 \nLevel 3 \nAppendixIn this supplementary material, we first provide details about the architecture of the different modules of our network GLU-Net in Section A. We then explain the training procedure in more depth in Section B. Finally, we present additional qualitative results and more detailed quantitative experiments in Section C.A. Architecture detailsIn this section, we provide additional details about cyclic consistency as a post processing step of the global correlation. We also give a detailed architectural description of the mapping and flow decoders, along with the refinement network. Lastly, we explain in depth the iterative refinement allowed by our adaptive resolution strategy. In the following, a convolution layer or block refers to the composition of a 2D-convolution followed by batch norm[29]and ReLU[44](Conv-BN-ReLU).A.1. Cyclic consistency post-processing step for improved global correlation\n. / Prunetruong, Glu-Net, PruneTruong/GLU-Net, 2019. 2\n\nBuilding rome in a day. Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, Richard Szeliski, Commun. ACM. 5410Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si- mon, Brian Curless, Steven M. Seitz, and Richard Szeliski. Building rome in a day. Commun. ACM, 54(10):105-112, 2011. 1\n\nFreak: Fast retina keypoint. Alexandre Alahi, Raphael Ortiz, Pierre Vandergheynst, CVPR. IEEE Computer SocietyAlexandre Alahi, Raphael Ortiz, and Pierre Vandergheynst. Freak: Fast retina keypoint. In CVPR, pages 510-517. IEEE Computer Society, 2012. 2\n\nKaze features. Pablo Fernndez Alcantarilla, Adrien Bartoli, Andrew J Davison, Lecture Notes in Computer Science. Andrew W. Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid75776SpringerPablo Fernndez Alcantarilla, Adrien Bartoli, and Andrew J. Davison. Kaze features. In Andrew W. Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid, editors, ECCV (6), volume 7577 of Lecture Notes in Com- puter Science, pages 214-227. Springer, 2012. 2\n\nNetvlad: CNN architecture for weakly supervised place recognition. Relja Arandjelovic, Petr Gron\u00e1t, Akihiko Torii, Tom\u00e1s Pajdla, Josef Sivic, IEEE Trans. Pattern Anal. Mach. Intell. 40615Relja Arandjelovic, Petr Gron\u00e1t, Akihiko Torii, Tom\u00e1s Pa- jdla, and Josef Sivic. Netvlad: CNN architecture for weakly supervised place recognition. IEEE Trans. Pattern Anal. Mach. Intell., 40(6):1437-1451, 2018. 15\n\nA database and evaluation methodology for optical flow. Simon Baker, J P Scharstein, Stefan Lewis, Michael J Roth, Richard Black, Szeliski, International Journal of Computer Vision. 921Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth, Michael J. Black, and Richard Szeliski. A database and eval- uation methodology for optical flow. International Journal of Computer Vision, 92(1):1-31, 2011. 1, 2\n\nHpatches: A benchmark and evaluation of handcrafted and learned local descriptors. Vassileios Balntas, Karel Lenc, Andrea Vedaldi, Krystian Mikolajczyk, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USA6Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys- tian Mikolajczyk. Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 3852-3861, 2017. 2, 6, 8\n\nSURF: speeded up robust features. Herbert Bay, Tinne Tuytelaars, Luc Van Gool, Computer Vision -ECCV 2006, 9th European Conference on Computer Vision. Graz, AustriaProceedings, Part IHerbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF: speeded up robust features. In Computer Vision -ECCV 2006, 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part I, pages 404- 417, 2006. 2\n\nA naturalistic open source movie for optical flow evaluation. Daniel J Butler, Jonas Wulff, Garrett B Stanley, Michael J Black, Computer Vision -ECCV 2012 -12th. Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and Michael J. Black. A naturalistic open source movie for opti- cal flow evaluation. In Computer Vision -ECCV 2012 -12th\n\nEuropean Conference on Computer Vision. Florence, ItalyProceedings, Part VIEuropean Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI, pages 611-625, 2012. 2\n\nReturn of the devil in the details: Delving deep into convolutional nets. K Chatfield, K Simonyan, A Vedaldi, A Zisserman, BMVC. 413K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convo- lutional nets. In BMVC, 2014. 4, 13\n\nRethinking atrous convolution for semantic image segmentation. Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam, abs/1706.05587CoRRLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017. 5\n\nUniversal correspondence network. Junyoung Christopher Bongsoo Choy, Silvio Gwak, Manmohan Krishna Savarese, Chandraker, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Barcelona, SpainChristopher Bongsoo Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Krishna Chandraker. Universal correspondence network. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural In- formation Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2406-2414, 2016. 2\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, abs/1604.01685CoRR513Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. CoRR, abs/1604.01685, 2016. 5, 13\n\nSuperpoint: Self-supervised interest point detection and description. Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, abs/1712.07629CoRRDaniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Superpoint: Self-supervised interest point detection and description. CoRR, abs/1712.07629, 2017. 2\n\nFlownet: Learning optical flow with convolutional networks. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip H\u00e4usser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der, Daniel Smagt, Thomas Cremers, Brox, 2015 IEEE International Conference on Computer Vision. Santiago, Chile813Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip H\u00e4usser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learn- ing optical flow with convolutional networks. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 2758-2766, 2015. 2, 3, 5, 6, 8, 13\n\nComputer Vision -A Modern Approach, Second Edition. A David, Jean Forsyth, Ponce, PitmanDavid A. Forsyth and Jean Ponce. Computer Vision -A Mod- ern Approach, Second Edition. Pitman, 2012. 1\n\nVision meets robotics: The kitti dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, I. J. Robotic Res. 3211Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. I. J. Robotic Res., 32(11):1231-1237, 2013. 2, 7, 8\n\nNon-rigid dense correspondence with applications for image enhancement. Yoav Hacohen, Eli Shechtman, Dan B Goldman, Dani Lischinski, ACM Trans. Graph. 30470Yoav HaCohen, Eli Shechtman, Dan B. Goldman, and Dani Lischinski. Non-rigid dense correspondence with applica- tions for image enhancement. ACM Trans. Graph., 30(4):70, 2011. 1\n\nProposal flow: Semantic correspondences from object proposals. Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce, IEEE Trans. Pattern Anal. Mach. Intell. 4077Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean Ponce. Proposal flow: Semantic correspondences from ob- ject proposals. IEEE Trans. Pattern Anal. Mach. Intell., 40(7):1711-1725, 2018. 1, 7\n\nMultiple view geometry in computer vision. Andrew Harltey, Andrew Zisserman, Cambridge University PressAndrew Harltey and Andrew Zisserman. Multiple view ge- ometry in computer vision (2. ed.). Cambridge University Press, 2006. 1\n\nA combined corner and edge detector. G Christopher, Mike Harris, Stephens, Proceedings of the Alvey Vision Conference. the Alvey Vision ConferenceManchester, UKChristopher G. Harris and Mike Stephens. A combined cor- ner and edge detector. In Proceedings of the Alvey Vision Conference, AVC 1988, Manchester, UK, September, 1988, pages 1-6, 1988. 2\n\ndetermining optical flow\": A retrospective. K P Berthold, Brian G Horn, Schunck, Artif. Intell. 591-2Berthold K. P. Horn and Brian G. Schunck. \"determining optical flow\": A retrospective. Artif. Intell., 59(1-2):81-87, 1993. 1, 2\n\n. Gao Huang, Zhuang Liu, Kilian Q Weinberger, abs/1608.0699312Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. CoRR, abs/1608.06993, 2016. 12\n\nDynamic context correspondence network for semantic alignment. Shuaiyi Huang, Qiuyue Wang, Songyang Zhang, Shipeng Yan, Xuming He, abs/1909.03444CoRR37Shuaiyi Huang, Qiuyue Wang, Songyang Zhang, Shipeng Yan, and Xuming He. Dynamic context correspondence net- work for semantic alignment. CoRR, abs/1909.03444, 2019. 3, 5, 7\n\nLiteflownet: A lightweight convolutional neural network for optical flow estimation. Tak-Wai Hui, Xiaoou Tang, Chen Change Loy, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USA815Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite- flownet: A lightweight convolutional neural network for op- tical flow estimation. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8981-8989, 2018. 2, 3, 6, 7, 8, 15\n\nA lightweight optical flow CNN -revisiting data fidelity and regularization. Tak-Wai Hui, Xiaoou Tang, Chen Change Loy, abs/1903.07414CoRRTak-Wai Hui, Xiaoou Tang, and Chen Change Loy. A lightweight optical flow CNN -revisiting data fidelity and regularization. CoRR, abs/1903.07414, 2019. 2\n\nDslr-quality photos on mobile devices with deep convolutional networks. Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, Luc Van Gool, IEEE International Conference on Computer Vision. Venice, Italy513Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. Dslr-quality photos on mobile devices with deep convolutional networks. In IEEE Interna- tional Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 3297-3305, 2017. 5, 13\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox, abs/1612.01925CoRREddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evo- lution of optical flow estimation with deep networks. CoRR, abs/1612.01925, 2016. 2, 6, 8\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine LearningLille, France12Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In Proceedings of the 32nd International Con- ference on Machine Learning, ICML 2015, Lille, France, 6- 11 July 2015, pages 448-456, 2015. 12\n\nSpatial transformer networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaMax Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. In Ad- vances in Neural Information Processing Systems 28: An- nual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2017-2025, 2015. 2\n\nComputer vision for autonomous vehicles: Problems, datasets and state-of-the-art. Joel Janai, Fatma G\u00fcney, Aseem Behl, Andreas Geiger, abs/1704.05519CoRRJoel Janai, Fatma G\u00fcney, Aseem Behl, and Andreas Geiger. Computer vision for autonomous vehicles: Prob- lems, datasets and state-of-the-art. CoRR, abs/1704.05519, 2017. 1\n\nPARN: pyramidal affine regression networks for dense semantic correspondence. Seungryong Sangryul Jeon, Dongbo Kim, Kwanghoon Min, Sohn, Computer Vision -ECCV 2018 -15th European Conference. Germany1721Proceedings, Part VISangryul Jeon, Seungryong Kim, Dongbo Min, and Kwanghoon Sohn. PARN: pyramidal affine regression net- works for dense semantic correspondence. In Computer Vision -ECCV 2018 -15th European Conference, Mu- nich, Germany, September 8-14, 2018, Proceedings, Part VI, pages 355-371, 2018. 2, 3, 4, 7, 8, 17, 21\n\nRecurrent transformer networks for semantic correspondence. Seungryong Kim, Stephen Lin, Sangryul Jeon, Dongbo Min, Kwanghoon Sohn, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. NeurIPS; Montr\u00e9al, Canada27Seungryong Kim, Stephen Lin, Sangryul Jeon, Dongbo Min, and Kwanghoon Sohn. Recurrent transformer networks for semantic correspondence. In Advances in Neural Informa- tion Processing Systems 31: Annual Conference on Neu- ral Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00e9al, Canada., pages 6129-6139, 2018. 2, 7\n\nFCSS: fully convolutional selfsimilarity for dense semantic correspondence. Seungryong Kim, Dongbo Min, Bumsub Ham, Stephen Lin, Kwanghoon Sohn, IEEE Trans. Pattern Anal. Mach. Intell. 413Seungryong Kim, Dongbo Min, Bumsub Ham, Stephen Lin, and Kwanghoon Sohn. FCSS: fully convolutional self- similarity for dense semantic correspondence. IEEE Trans. Pattern Anal. Mach. Intell., 41(3):581-595, 2019. 2\n\nSemantic attribute matching networks. Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, Sangryul Jeon, Kwanghoon Sohn, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA27Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, Sangryul Jeon, and Kwanghoon Sohn. Semantic attribute matching networks. In IEEE Conference on Computer Vi- sion and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 12339-12348, 2019. 2, 3, 7\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USA14Conference Track ProceedingsDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 14\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems. Lake Tahoe, Nevada, United States414Proceedings of a meeting held December 3-6Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in Neural Information Processing Sys- tems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held De- cember 3-6, 2012, Lake Tahoe, Nevada, United States., pages 1106-1114, 2012. 4, 14\n\nGeometric image correspondence verification by dense pixel matching. Zakaria Laskar, Iaroslav Melekhov, R Hamed, Juha Tavakoli, Juho Ylioinas, Kannala, abs/1904.06882CoRRZakaria Laskar, Iaroslav Melekhov, Hamed R. Tavakoli, Juha Ylioinas, and Juho Kannala. Geometric image cor- respondence verification by dense pixel matching. CoRR, abs/1904.06882, 2019. 2\n\nBRISK: binary robust invariant scalable keypoints. Stefan Leutenegger, Margarita Chli, Roland Siegwart, IEEE International Conference on Computer Vision, ICCV 2011. Barcelona, SpainStefan Leutenegger, Margarita Chli, and Roland Siegwart. BRISK: binary robust invariant scalable keypoints. In IEEE International Conference on Computer Vision, ICCV 2011, Barcelona, Spain, November 6-13, 2011, pages 2548-2555, 2011. 2\n\nSIFT flow: Dense correspondence across scenes and its applications. Ce Liu, Jenny Yuen, Antonio Torralba, IEEE Trans. Pattern Anal. Mach. Intell. 335Ce Liu, Jenny Yuen, and Antonio Torralba. SIFT flow: Dense correspondence across scenes and its applications. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):978-994, 2011. 1, 2\n\nDistinctive image features from scaleinvariant keypoints. David G Lowe, International Journal of Computer Vision. 602David G. Lowe. Distinctive image features from scale- invariant keypoints. International Journal of Computer Vi- sion, 60(2):91-110, 2004. 2\n\nWhat makes good synthetic training data for learning disparity and optical flow estimation?. Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox, International Journal of Computer Vision. 126915Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazir- bas, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. What makes good synthetic training data for learning dispar- ity and optical flow estimation? International Journal of Computer Vision, 126(9):942-960, 2018. 15\n\nDgc-net: Dense geometric correspondence network. Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys, Esa Rahtu, Juho Kannala, abs/1810.083931516Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys, Esa Rahtu, and Juho Kannala. Dgc-net: Dense geometric correspondence network. CoRR, abs/1810.08393, 2018. 2, 3, 4, 5, 6, 7, 8, 12, 13, 15, 16\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th International Conference on Machine Learning (ICML-10). the 27th International Conference on Machine Learning (ICML-10)Haifa, Israel512Vinod Nair and Geoffrey E. Hinton. Rectified linear units im- prove restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML- 10), June 21-24, 2010, Haifa, Israel, pages 807-814, 2010. 5, 12\n\nLf-net: Learning local features from images. Yuki Ono, Eduard Trulls, Pascal Fua, Kwang Moo Yi, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. NeurIPS; Montr\u00e9al, CanadaYuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. Lf-net: Learning local features from images. In Advances in Neural Information Processing Systems 31: Annual Con- ference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00e9al, Canada, pages 6237-6247, 2018. 2\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Autodiff Workshop. 14Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic dif- ferentiation in PyTorch. In NIPS Autodiff Workshop, 2017. 14\n\nOptical flow estimation using a spatial pyramid network. Anurag Ranjan, Michael J Black, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAAnurag Ranjan and Michael J. Black. Optical flow estima- tion using a spatial pyramid network. In 2017 IEEE Con- ference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2720- 2729, 2017. 2\n\nConvolutional neural network architecture for geometric matching. Ignacio Rocco, Relja Arandjelovic, Josef Sivic, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USA512Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convo- lutional neural network architecture for geometric matching. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 39-48, 2017. 2, 5, 12\n\nEnd-toend weakly-supervised semantic alignment. Ignacio Rocco, Relja Arandjelovic, Josef Sivic, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USA27Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. End-to- end weakly-supervised semantic alignment. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6917-6925, 2018. 2, 7\n\nNeighbourhood consensus networks. Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovic, Akihiko Torii, Tom\u00e1s Pajdla, Josef Sivic, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. NeurIPS; Montr\u00e9al, Canada1721Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovic, Akihiko Torii, Tom\u00e1s Pajdla, and Josef Sivic. Neighbourhood consen- sus networks. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Pro- cessing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00e9al, Canada., pages 1658-1669, 2018. 2, 5, 7, 8, 12, 17, 21\n\nORB: an efficient alternative to SIFT or SURF. Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary R Bradski, IEEE International Conference on Computer Vision. Barcelona, SpainEthan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski. ORB: an efficient alternative to SIFT or SURF. In IEEE International Conference on Computer Vision, ICCV 2011, Barcelona, Spain, November 6-13, 2011, pages 2564- 2571, 2011. 2\n\nStructurefrom-motion revisited. L Johannes, Jan-Michael Sch\u00f6nberger, Frahm, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USA16Johannes L. Sch\u00f6nberger and Jan-Michael Frahm. Structure- from-motion revisited. In 2016 IEEE Conference on Com- puter Vision and Pattern Recognition, CVPR 2016, Las Ve- gas, NV, USA, June 27-30, 2016, pages 4104-4113, 2016. 1, 6\n\nSemantic visual localization. CoRR. Johannes L Sch\u00f6nberger, Marc Pollefeys, Andreas Geiger, Torsten Sattler, abs/1712.05773Johannes L. Sch\u00f6nberger, Marc Pollefeys, Andreas Geiger, and Torsten Sattler. Semantic visual localization. CoRR, abs/1712.05773, 2017. 1\n\nA multi-view stereo benchmark with highresolution images and multi-camera videos. Thomas Sch\u00f6ps, Johannes L Sch\u00f6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, Andreas Geiger, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USA716Thomas Sch\u00f6ps, Johannes L. Sch\u00f6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An- dreas Geiger. A multi-view stereo benchmark with high- resolution images and multi-camera videos. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2538-2547, 2017. 2, 6, 7, 16\n\nAn empirical evaluation study on the training of SDC features for dense pixel matching. Ren\u00e9 Schuster, Oliver Wasenm\u00fcller, Christian Unger, Didier Stricker, IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops. Long Beach, CA, USA15Ren\u00e9 Schuster, Oliver Wasenm\u00fcller, Christian Unger, and Didier Stricker. An empirical evaluation study on the train- ing of SDC features for dense pixel matching. In IEEE Con- ference on Computer Vision and Pattern Recognition Work- shops, CVPR Workshops 2019, Long Beach, CA, USA, June 16-20, 2019, page 0, 2019. 15\n\nTwo-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199. Karen Simonyan, Andrew Zisserman, Karen Simonyan and Andrew Zisserman. Two-stream con- volutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. 1\n\nThinplate spline approximation for image registration. Rainer Sprengel, Karl Rohr, H Siegfried Stiehl, Annual International Conference of the IEEE Engineering in Medicine and Biology -Proceedings. Rainer Sprengel, Karl Rohr, and H. Siegfried Stiehl. Thin- plate spline approximation for image registration. In An- nual International Conference of the IEEE Engineering in Medicine and Biology -Proceedings, 1996. 13\n\nModels matter, so does training: An empirical study of cnns for optical flow estimation. Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz, abs/1809.05571CoRR815Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Models matter, so does training: An empirical study of cnns for optical flow estimation. CoRR, abs/1809.05571, 2018. 2, 6, 8, 15\n\nPwc-net: Cnns for optical flow using pyramid, warping, and cost volume. Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USA1315Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8934-8943, 2018. 2, 3, 6, 7, 8, 12, 13, 15\n\nInloc: Indoor visual localization with dense matching and view synthesis. Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tom\u00e1s Pajdla, Akihiko Torii, abs/1803.10368CoRR. 1Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tom\u00e1s Pajdla, and Ak- ihiko Torii. Inloc: Indoor visual localization with dense matching and view synthesis. CoRR, abs/1803.10368, 2018. 1\n\nJoint recovery of dense correspondence and cosegmentation in two images. Tatsunori Taniai, N Sudipta, Yoichi Sinha, Sato, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USA817Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato. Joint re- covery of dense correspondence and cosegmentation in two images. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 4246-4255, 2016. 2, 7, 8, 17\n\nGlampoints: Greedily learned accurate match points. Prune Truong, Stefanos Apostolopoulos, Agata Mosinska, Samuel Stucky, Carlos Ciller, Sandro De Zanet, abs/1908.06812CoRRPrune Truong, Stefanos Apostolopoulos, Agata Mosinska, Samuel Stucky, Carlos Ciller, and Sandro De Zanet. Glam- points: Greedily learned accurate match points. CoRR, abs/1908.06812, 2019. 2\n\nExtracting and composing robust features with denoising autoencoders. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol, Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008). Helsinki, FinlandPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Machine Learn- ing, Proceedings of the Twenty-Fifth International Confer- ence (ICML 2008), Helsinki, Finland, June 5-9, 2008, pages 1096-1103, 2008. 2\n\nSemantic understanding of scenes through the ADE20K dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, abs/1608.05442CoRR513Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understand- ing of scenes through the ADE20K dataset. CoRR, abs/1608.05442, 2016. 5, 13\n\nGLOCAL-Net 1L = 1 H-Net level 2L = 2 H-Net levels 3L = 3 H-Net levels. GLOCAL-Net 1L = 1 H-Net level 2L = 2 H-Net levels 3L = 3 H-Net levels\n", "annotations": {"author": "[{\"end\":157,\"start\":78},{\"end\":250,\"start\":158},{\"end\":335,\"start\":251}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":84},{\"end\":174,\"start\":165},{\"end\":263,\"start\":256}]", "author_first_name": "[{\"end\":83,\"start\":78},{\"end\":164,\"start\":158},{\"end\":255,\"start\":251}]", "author_affiliation": "[{\"end\":156,\"start\":108},{\"end\":249,\"start\":201},{\"end\":334,\"start\":286}]", "title": "[{\"end\":75,\"start\":1},{\"end\":410,\"start\":336}]", "venue": null, "abstract": "[{\"end\":1548,\"start\":412}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1679,\"start\":1675},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1682,\"start\":1679},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1766,\"start\":1762},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":1769,\"start\":1766},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1791,\"start\":1788},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":1819,\"start\":1815},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1844,\"start\":1840},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1847,\"start\":1844},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1872,\"start\":1868},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1900,\"start\":1896},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2320,\"start\":2316},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2597,\"start\":2593},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2599,\"start\":2597},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2743,\"start\":2739},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2746,\"start\":2743},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3531,\"start\":3527},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3534,\"start\":3531},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3537,\"start\":3534},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3540,\"start\":3537},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3543,\"start\":3540},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3546,\"start\":3543},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3812,\"start\":3808},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3815,\"start\":3812},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3818,\"start\":3815},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3821,\"start\":3818},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3824,\"start\":3821},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5374,\"start\":5371},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5389,\"start\":5385},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":5480,\"start\":5476},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5636,\"start\":5632},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5922,\"start\":5918},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5982,\"start\":5979},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5985,\"start\":5982},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5988,\"start\":5985},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5990,\"start\":5988},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5992,\"start\":5990},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5995,\"start\":5992},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5998,\"start\":5995},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6014,\"start\":6010},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6017,\"start\":6014},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6020,\"start\":6017},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6084,\"start\":6080},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6086,\"start\":6084},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6089,\"start\":6086},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6420,\"start\":6416},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6551,\"start\":6547},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6624,\"start\":6620},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6779,\"start\":6776},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6816,\"start\":6812},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6964,\"start\":6960},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6967,\"start\":6964},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6988,\"start\":6984},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7019,\"start\":7015},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7724,\"start\":7720},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8069,\"start\":8065},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8334,\"start\":8330},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8657,\"start\":8653},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8660,\"start\":8657},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8855,\"start\":8851},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8858,\"start\":8855},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8861,\"start\":8858},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8864,\"start\":8861},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8909,\"start\":8905},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8937,\"start\":8933},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9082,\"start\":9078},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9191,\"start\":9187},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10585,\"start\":10581},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10588,\"start\":10585},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10591,\"start\":10588},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10594,\"start\":10591},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10597,\"start\":10594},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11235,\"start\":11231},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11238,\"start\":11235},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11241,\"start\":11238},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11244,\"start\":11241},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11247,\"start\":11244},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13330,\"start\":13326},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13333,\"start\":13330},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13336,\"start\":13333},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15637,\"start\":15633},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19338,\"start\":19334},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19372,\"start\":19368},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19437,\"start\":19433},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19471,\"start\":19467},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20168,\"start\":20164},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20215,\"start\":20211},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20240,\"start\":20236},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21075,\"start\":21071},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21078,\"start\":21075},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21798,\"start\":21794},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21992,\"start\":21988},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22473,\"start\":22469},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22490,\"start\":22486},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":22507,\"start\":22503},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22687,\"start\":22683},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23491,\"start\":23488},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23902,\"start\":23898},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":24076,\"start\":24072},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24406,\"start\":24402},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24786,\"start\":24782},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25263,\"start\":25259},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":25404,\"start\":25400},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25425,\"start\":25421},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25461,\"start\":25457},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25488,\"start\":25484},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25955,\"start\":25951},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":26871,\"start\":26867},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26892,\"start\":26888},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27049,\"start\":27045},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":27372,\"start\":27368},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":27610,\"start\":27606},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":27869,\"start\":27865},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27872,\"start\":27869},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27875,\"start\":27872},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27878,\"start\":27875},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27881,\"start\":27878},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27884,\"start\":27881},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":28086,\"start\":28082},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28198,\"start\":28194},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28921,\"start\":28917},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29571,\"start\":29567},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30016,\"start\":30012},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":30294,\"start\":30290},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":30297,\"start\":30294},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30318,\"start\":30314},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30348,\"start\":30344},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30367,\"start\":30363},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30400,\"start\":30396},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32730,\"start\":32726},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32903,\"start\":32900},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":32916,\"start\":32912},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33002,\"start\":32998},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34192,\"start\":34188},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35233,\"start\":35229},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35279,\"start\":35275},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35304,\"start\":35300},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":36958,\"start\":36954},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":37345,\"start\":37341},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37415,\"start\":37411},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37694,\"start\":37690},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":37727,\"start\":37723},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":38199,\"start\":38195},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":38266,\"start\":38262},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38936,\"start\":38932},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":39211,\"start\":39207},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41624,\"start\":41620},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42284,\"start\":42280},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":42301,\"start\":42297},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":42671,\"start\":42667},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42885,\"start\":42881},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42902,\"start\":42898},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":42919,\"start\":42915},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":43470,\"start\":43466},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":43973,\"start\":43969},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":44028,\"start\":44024},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44823,\"start\":44819},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":44959,\"start\":44956},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":45453,\"start\":45449},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":45963,\"start\":45959},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":45966,\"start\":45963},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":45969,\"start\":45966},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":46838,\"start\":46834},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":46857,\"start\":46853},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":46874,\"start\":46870},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":48006,\"start\":48002},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":49454,\"start\":49450},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":49724,\"start\":49720},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":49839,\"start\":49835},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":49870,\"start\":49866},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":52474,\"start\":52470},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":52523,\"start\":52519},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":54039,\"start\":54035}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53158,\"start\":52756},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53165,\"start\":53159},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53672,\"start\":53166},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53940,\"start\":53673},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54040,\"start\":53941},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54134,\"start\":54041},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54197,\"start\":54135},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54383,\"start\":54198},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54981,\"start\":54384},{\"attributes\":{\"id\":\"fig_10\"},\"end\":55092,\"start\":54982},{\"attributes\":{\"id\":\"fig_11\"},\"end\":55244,\"start\":55093},{\"attributes\":{\"id\":\"fig_12\"},\"end\":55469,\"start\":55245},{\"attributes\":{\"id\":\"fig_13\"},\"end\":55988,\"start\":55470},{\"attributes\":{\"id\":\"fig_14\"},\"end\":56318,\"start\":55989},{\"attributes\":{\"id\":\"fig_15\"},\"end\":56768,\"start\":56319},{\"attributes\":{\"id\":\"fig_16\"},\"end\":57339,\"start\":56769},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":57597,\"start\":57340},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":58093,\"start\":57598},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":58330,\"start\":58094},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58523,\"start\":58331},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":58996,\"start\":58524},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":59939,\"start\":58997},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":60086,\"start\":59940}]", "paragraph": "[{\"end\":2125,\"start\":1564},{\"end\":3204,\"start\":2127},{\"end\":5701,\"start\":3206},{\"end\":6876,\"start\":5718},{\"end\":9338,\"start\":6878},{\"end\":9833,\"start\":9349},{\"end\":9864,\"start\":9861},{\"end\":10144,\"start\":9866},{\"end\":11158,\"start\":10146},{\"end\":12019,\"start\":11192},{\"end\":12582,\"start\":12074},{\"end\":12978,\"start\":12584},{\"end\":13141,\"start\":13021},{\"end\":15136,\"start\":13143},{\"end\":15611,\"start\":15166},{\"end\":16377,\"start\":15613},{\"end\":16972,\"start\":16401},{\"end\":17590,\"start\":16974},{\"end\":18336,\"start\":17592},{\"end\":19165,\"start\":18338},{\"end\":20421,\"start\":19190},{\"end\":20523,\"start\":20455},{\"end\":20690,\"start\":20547},{\"end\":20985,\"start\":20738},{\"end\":21851,\"start\":20987},{\"end\":22869,\"start\":21864},{\"end\":23227,\"start\":22897},{\"end\":26068,\"start\":23250},{\"end\":26469,\"start\":26070},{\"end\":26568,\"start\":26471},{\"end\":27084,\"start\":26570},{\"end\":27756,\"start\":27106},{\"end\":28504,\"start\":27778},{\"end\":29304,\"start\":28506},{\"end\":30235,\"start\":29321},{\"end\":30679,\"start\":30237},{\"end\":31368,\"start\":30681},{\"end\":33127,\"start\":31387},{\"end\":33830,\"start\":33142},{\"end\":34245,\"start\":33832},{\"end\":34728,\"start\":34247},{\"end\":34865,\"start\":34786},{\"end\":35492,\"start\":34896},{\"end\":35829,\"start\":35538},{\"end\":36160,\"start\":35857},{\"end\":36371,\"start\":36184},{\"end\":36664,\"start\":36420},{\"end\":37257,\"start\":36666},{\"end\":37831,\"start\":37259},{\"end\":38525,\"start\":37861},{\"end\":39596,\"start\":38585},{\"end\":40284,\"start\":39626},{\"end\":40661,\"start\":40286},{\"end\":41096,\"start\":40663},{\"end\":41210,\"start\":41120},{\"end\":41625,\"start\":41224},{\"end\":42486,\"start\":41681},{\"end\":43303,\"start\":42503},{\"end\":43722,\"start\":43335},{\"end\":44064,\"start\":43724},{\"end\":44261,\"start\":44104},{\"end\":44590,\"start\":44263},{\"end\":44729,\"start\":44618},{\"end\":44809,\"start\":44767},{\"end\":45607,\"start\":44811},{\"end\":45970,\"start\":45609},{\"end\":46437,\"start\":45972},{\"end\":46494,\"start\":46439},{\"end\":46949,\"start\":46496},{\"end\":47264,\"start\":46976},{\"end\":47700,\"start\":47266},{\"end\":47928,\"start\":47702},{\"end\":49619,\"start\":47930},{\"end\":50045,\"start\":49653},{\"end\":50197,\"start\":50081},{\"end\":52755,\"start\":50199}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9860,\"start\":9834},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12073,\"start\":12020},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13020,\"start\":12979},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20454,\"start\":20422},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20546,\"start\":20524},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20737,\"start\":20691},{\"attributes\":{\"id\":\"formula_6\"},\"end\":34785,\"start\":34729},{\"attributes\":{\"id\":\"formula_7\"},\"end\":35537,\"start\":35493},{\"attributes\":{\"id\":\"formula_8\"},\"end\":35856,\"start\":35830},{\"attributes\":{\"id\":\"formula_9\"},\"end\":36419,\"start\":36372},{\"attributes\":{\"id\":\"formula_10\"},\"end\":41680,\"start\":41626}]", "table_ref": "[{\"end\":25611,\"start\":25604},{\"end\":28550,\"start\":28543},{\"end\":29949,\"start\":29942},{\"end\":30625,\"start\":30618},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":31726,\"start\":31719},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32107,\"start\":32100},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32306,\"start\":32299},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32693,\"start\":32686},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":44207,\"start\":44200},{\"end\":45606,\"start\":45599},{\"end\":46134,\"start\":46127},{\"end\":46161,\"start\":46154},{\"end\":47139,\"start\":47132},{\"end\":49880,\"start\":49873},{\"end\":50435,\"start\":50427},{\"end\":50867,\"start\":50859},{\"end\":51710,\"start\":51702},{\"end\":51937,\"start\":51929}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1562,\"start\":1550},{\"attributes\":{\"n\":\"2.\"},\"end\":5716,\"start\":5704},{\"attributes\":{\"n\":\"3.\"},\"end\":9347,\"start\":9341},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11190,\"start\":11161},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15164,\"start\":15139},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16399,\"start\":16380},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19188,\"start\":19168},{\"attributes\":{\"n\":\"3.5.\"},\"end\":21862,\"start\":21854},{\"attributes\":{\"n\":\"4.\"},\"end\":22895,\"start\":22872},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23248,\"start\":23230},{\"attributes\":{\"n\":\"4.2.\"},\"end\":27104,\"start\":27087},{\"end\":27776,\"start\":27759},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29319,\"start\":29307},{\"attributes\":{\"n\":\"4.4.\"},\"end\":31385,\"start\":31371},{\"attributes\":{\"n\":\"5.\"},\"end\":33140,\"start\":33130},{\"end\":34894,\"start\":34868},{\"end\":36182,\"start\":36163},{\"end\":37859,\"start\":37834},{\"end\":38583,\"start\":38528},{\"end\":39624,\"start\":39599},{\"end\":41118,\"start\":41099},{\"end\":41222,\"start\":41213},{\"end\":42501,\"start\":42489},{\"end\":43333,\"start\":43306},{\"end\":44086,\"start\":44067},{\"end\":44102,\"start\":44089},{\"end\":44616,\"start\":44593},{\"end\":44765,\"start\":44732},{\"end\":46974,\"start\":46952},{\"end\":49651,\"start\":49622},{\"end\":50079,\"start\":50048},{\"end\":52767,\"start\":52757},{\"end\":53177,\"start\":53167},{\"end\":53684,\"start\":53674},{\"end\":53952,\"start\":53942},{\"end\":54052,\"start\":54042},{\"end\":54209,\"start\":54199},{\"end\":54993,\"start\":54983},{\"end\":55257,\"start\":55246},{\"end\":55482,\"start\":55471},{\"end\":56001,\"start\":55990},{\"end\":56331,\"start\":56320},{\"end\":56781,\"start\":56770},{\"end\":57608,\"start\":57599},{\"end\":58104,\"start\":58095},{\"end\":58341,\"start\":58332}]", "table": "[{\"end\":57597,\"start\":57381},{\"end\":58093,\"start\":57610},{\"end\":58330,\"start\":58106},{\"end\":58523,\"start\":58343},{\"end\":58996,\"start\":58706},{\"end\":59939,\"start\":59031},{\"end\":60086,\"start\":60021}]", "figure_caption": "[{\"end\":53158,\"start\":52769},{\"end\":53165,\"start\":53161},{\"end\":53672,\"start\":53179},{\"end\":53940,\"start\":53686},{\"end\":54040,\"start\":53954},{\"end\":54134,\"start\":54054},{\"end\":54197,\"start\":54137},{\"end\":54383,\"start\":54211},{\"end\":54981,\"start\":54386},{\"end\":55092,\"start\":54995},{\"end\":55244,\"start\":55095},{\"end\":55469,\"start\":55260},{\"end\":55988,\"start\":55485},{\"end\":56318,\"start\":56004},{\"end\":56768,\"start\":56334},{\"end\":57339,\"start\":56784},{\"end\":57381,\"start\":57342},{\"end\":58706,\"start\":58526},{\"end\":59031,\"start\":58999},{\"end\":60021,\"start\":59942}]", "figure_ref": "[{\"end\":14734,\"start\":14725},{\"end\":14743,\"start\":14735},{\"end\":16237,\"start\":16228},{\"end\":17589,\"start\":17580},{\"end\":18483,\"start\":18474},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19875,\"start\":19867},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21238,\"start\":21230},{\"end\":25892,\"start\":25884},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26345,\"start\":26337},{\"end\":26581,\"start\":26573},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29303,\"start\":29295},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30678,\"start\":30670},{\"end\":31705,\"start\":31697},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35025,\"start\":35017},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36291,\"start\":36283},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36707,\"start\":36699},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37928,\"start\":37919},{\"end\":38596,\"start\":38588},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39879,\"start\":39871},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":41006,\"start\":40998},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46493,\"start\":46484},{\"end\":47003,\"start\":46995},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47421,\"start\":47412},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49356,\"start\":49347},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49665,\"start\":49656}]", "bib_author_first_name": "[{\"end\":61002,\"start\":61001},{\"end\":61085,\"start\":61079},{\"end\":61103,\"start\":61095},{\"end\":61118,\"start\":61114},{\"end\":61131,\"start\":61128},{\"end\":61144,\"start\":61139},{\"end\":61160,\"start\":61154},{\"end\":61162,\"start\":61161},{\"end\":61177,\"start\":61170},{\"end\":61421,\"start\":61412},{\"end\":61436,\"start\":61429},{\"end\":61450,\"start\":61444},{\"end\":61656,\"start\":61651},{\"end\":61686,\"start\":61680},{\"end\":61702,\"start\":61696},{\"end\":61704,\"start\":61703},{\"end\":62203,\"start\":62198},{\"end\":62222,\"start\":62218},{\"end\":62238,\"start\":62231},{\"end\":62251,\"start\":62246},{\"end\":62265,\"start\":62260},{\"end\":62595,\"start\":62590},{\"end\":62604,\"start\":62603},{\"end\":62606,\"start\":62605},{\"end\":62625,\"start\":62619},{\"end\":62640,\"start\":62633},{\"end\":62642,\"start\":62641},{\"end\":62656,\"start\":62649},{\"end\":63034,\"start\":63024},{\"end\":63049,\"start\":63044},{\"end\":63062,\"start\":63056},{\"end\":63080,\"start\":63072},{\"end\":63525,\"start\":63518},{\"end\":63536,\"start\":63531},{\"end\":63552,\"start\":63549},{\"end\":63970,\"start\":63964},{\"end\":63972,\"start\":63971},{\"end\":63986,\"start\":63981},{\"end\":64001,\"start\":63994},{\"end\":64003,\"start\":64002},{\"end\":64020,\"start\":64013},{\"end\":64022,\"start\":64021},{\"end\":64511,\"start\":64510},{\"end\":64524,\"start\":64523},{\"end\":64536,\"start\":64535},{\"end\":64547,\"start\":64546},{\"end\":64797,\"start\":64786},{\"end\":64810,\"start\":64804},{\"end\":64830,\"start\":64823},{\"end\":64847,\"start\":64840},{\"end\":65080,\"start\":65072},{\"end\":65113,\"start\":65107},{\"end\":65136,\"start\":65120},{\"end\":65672,\"start\":65666},{\"end\":65688,\"start\":65681},{\"end\":65705,\"start\":65696},{\"end\":65717,\"start\":65713},{\"end\":65733,\"start\":65727},{\"end\":65752,\"start\":65745},{\"end\":65766,\"start\":65763},{\"end\":65781,\"start\":65775},{\"end\":65793,\"start\":65788},{\"end\":66139,\"start\":66133},{\"end\":66154,\"start\":66148},{\"end\":66174,\"start\":66168},{\"end\":66432,\"start\":66426},{\"end\":66453,\"start\":66446},{\"end\":66467,\"start\":66463},{\"end\":66479,\"start\":66473},{\"end\":66494,\"start\":66489},{\"end\":66513,\"start\":66505},{\"end\":66529,\"start\":66522},{\"end\":66545,\"start\":66539},{\"end\":66559,\"start\":66553},{\"end\":67064,\"start\":67063},{\"end\":67076,\"start\":67072},{\"end\":67252,\"start\":67245},{\"end\":67267,\"start\":67261},{\"end\":67283,\"start\":67274},{\"end\":67299,\"start\":67293},{\"end\":67571,\"start\":67567},{\"end\":67584,\"start\":67581},{\"end\":67599,\"start\":67596},{\"end\":67601,\"start\":67600},{\"end\":67615,\"start\":67611},{\"end\":67898,\"start\":67892},{\"end\":67909,\"start\":67904},{\"end\":67923,\"start\":67915},{\"end\":67936,\"start\":67932},{\"end\":68228,\"start\":68222},{\"end\":68244,\"start\":68238},{\"end\":68448,\"start\":68447},{\"end\":68466,\"start\":68462},{\"end\":68805,\"start\":68804},{\"end\":68807,\"start\":68806},{\"end\":68823,\"start\":68818},{\"end\":68825,\"start\":68824},{\"end\":68996,\"start\":68993},{\"end\":69010,\"start\":69004},{\"end\":69022,\"start\":69016},{\"end\":69024,\"start\":69023},{\"end\":69246,\"start\":69239},{\"end\":69260,\"start\":69254},{\"end\":69275,\"start\":69267},{\"end\":69290,\"start\":69283},{\"end\":69302,\"start\":69296},{\"end\":69593,\"start\":69586},{\"end\":69605,\"start\":69599},{\"end\":69623,\"start\":69612},{\"end\":70104,\"start\":70097},{\"end\":70116,\"start\":70110},{\"end\":70134,\"start\":70123},{\"end\":70391,\"start\":70385},{\"end\":70408,\"start\":70401},{\"end\":70423,\"start\":70419},{\"end\":70440,\"start\":70433},{\"end\":70453,\"start\":70450},{\"end\":70891,\"start\":70887},{\"end\":70905,\"start\":70897},{\"end\":70919,\"start\":70913},{\"end\":70935,\"start\":70928},{\"end\":70950,\"start\":70944},{\"end\":70970,\"start\":70964},{\"end\":71298,\"start\":71292},{\"end\":71315,\"start\":71306},{\"end\":71771,\"start\":71768},{\"end\":71788,\"start\":71783},{\"end\":71805,\"start\":71799},{\"end\":71822,\"start\":71817},{\"end\":72361,\"start\":72357},{\"end\":72374,\"start\":72369},{\"end\":72387,\"start\":72382},{\"end\":72401,\"start\":72394},{\"end\":72688,\"start\":72678},{\"end\":72710,\"start\":72704},{\"end\":72725,\"start\":72716},{\"end\":73199,\"start\":73189},{\"end\":73212,\"start\":73205},{\"end\":73226,\"start\":73218},{\"end\":73239,\"start\":73233},{\"end\":73254,\"start\":73245},{\"end\":73831,\"start\":73821},{\"end\":73843,\"start\":73837},{\"end\":73855,\"start\":73849},{\"end\":73868,\"start\":73861},{\"end\":73883,\"start\":73874},{\"end\":74197,\"start\":74187},{\"end\":74209,\"start\":74203},{\"end\":74219,\"start\":74215},{\"end\":74232,\"start\":74227},{\"end\":74246,\"start\":74238},{\"end\":74262,\"start\":74253},{\"end\":74679,\"start\":74678},{\"end\":74695,\"start\":74690},{\"end\":75107,\"start\":75103},{\"end\":75124,\"start\":75120},{\"end\":75144,\"start\":75136},{\"end\":75146,\"start\":75145},{\"end\":75799,\"start\":75792},{\"end\":75816,\"start\":75808},{\"end\":75828,\"start\":75827},{\"end\":75840,\"start\":75836},{\"end\":75855,\"start\":75851},{\"end\":76139,\"start\":76133},{\"end\":76162,\"start\":76153},{\"end\":76175,\"start\":76169},{\"end\":76570,\"start\":76568},{\"end\":76581,\"start\":76576},{\"end\":76595,\"start\":76588},{\"end\":76890,\"start\":76885},{\"end\":76892,\"start\":76891},{\"end\":77187,\"start\":77179},{\"end\":77199,\"start\":77195},{\"end\":77212,\"start\":77205},{\"end\":77227,\"start\":77222},{\"end\":77244,\"start\":77238},{\"end\":77260,\"start\":77254},{\"end\":77280,\"start\":77274},{\"end\":77668,\"start\":77660},{\"end\":77686,\"start\":77679},{\"end\":77703,\"start\":77696},{\"end\":77717,\"start\":77713},{\"end\":77732,\"start\":77729},{\"end\":77744,\"start\":77740},{\"end\":78051,\"start\":78046},{\"end\":78066,\"start\":78058},{\"end\":78068,\"start\":78067},{\"end\":78529,\"start\":78525},{\"end\":78541,\"start\":78535},{\"end\":78556,\"start\":78550},{\"end\":78571,\"start\":78562},{\"end\":79058,\"start\":79054},{\"end\":79070,\"start\":79067},{\"end\":79085,\"start\":79078},{\"end\":79103,\"start\":79096},{\"end\":79118,\"start\":79112},{\"end\":79132,\"start\":79125},{\"end\":79147,\"start\":79141},{\"end\":79158,\"start\":79153},{\"end\":79174,\"start\":79170},{\"end\":79187,\"start\":79183},{\"end\":79506,\"start\":79500},{\"end\":79522,\"start\":79515},{\"end\":79524,\"start\":79523},{\"end\":79927,\"start\":79920},{\"end\":79940,\"start\":79935},{\"end\":79960,\"start\":79955},{\"end\":80373,\"start\":80366},{\"end\":80386,\"start\":80381},{\"end\":80406,\"start\":80401},{\"end\":80798,\"start\":80791},{\"end\":80812,\"start\":80806},{\"end\":80826,\"start\":80821},{\"end\":80848,\"start\":80841},{\"end\":80861,\"start\":80856},{\"end\":80875,\"start\":80870},{\"end\":81433,\"start\":81428},{\"end\":81449,\"start\":81442},{\"end\":81462,\"start\":81458},{\"end\":81477,\"start\":81473},{\"end\":81479,\"start\":81478},{\"end\":81831,\"start\":81830},{\"end\":81853,\"start\":81842},{\"end\":82245,\"start\":82237},{\"end\":82247,\"start\":82246},{\"end\":82265,\"start\":82261},{\"end\":82284,\"start\":82277},{\"end\":82300,\"start\":82293},{\"end\":82551,\"start\":82545},{\"end\":82568,\"start\":82560},{\"end\":82570,\"start\":82569},{\"end\":82591,\"start\":82584},{\"end\":82609,\"start\":82602},{\"end\":82625,\"start\":82619},{\"end\":82641,\"start\":82637},{\"end\":82660,\"start\":82653},{\"end\":83213,\"start\":83209},{\"end\":83230,\"start\":83224},{\"end\":83253,\"start\":83244},{\"end\":83267,\"start\":83261},{\"end\":83797,\"start\":83792},{\"end\":83814,\"start\":83808},{\"end\":84024,\"start\":84018},{\"end\":84039,\"start\":84035},{\"end\":84047,\"start\":84046},{\"end\":84057,\"start\":84048},{\"end\":84474,\"start\":84468},{\"end\":84488,\"start\":84480},{\"end\":84502,\"start\":84495},{\"end\":84511,\"start\":84508},{\"end\":84803,\"start\":84797},{\"end\":84817,\"start\":84809},{\"end\":84831,\"start\":84824},{\"end\":84840,\"start\":84837},{\"end\":85319,\"start\":85313},{\"end\":85336,\"start\":85327},{\"end\":85353,\"start\":85346},{\"end\":85369,\"start\":85363},{\"end\":85382,\"start\":85378},{\"end\":85399,\"start\":85394},{\"end\":85412,\"start\":85407},{\"end\":85428,\"start\":85421},{\"end\":85773,\"start\":85764},{\"end\":85783,\"start\":85782},{\"end\":85799,\"start\":85793},{\"end\":86248,\"start\":86243},{\"end\":86265,\"start\":86257},{\"end\":86287,\"start\":86282},{\"end\":86304,\"start\":86298},{\"end\":86319,\"start\":86313},{\"end\":86334,\"start\":86328},{\"end\":86337,\"start\":86335},{\"end\":86630,\"start\":86624},{\"end\":86644,\"start\":86640},{\"end\":86663,\"start\":86657},{\"end\":86686,\"start\":86672},{\"end\":87171,\"start\":87166},{\"end\":87182,\"start\":87178},{\"end\":87195,\"start\":87189},{\"end\":87207,\"start\":87202},{\"end\":87221,\"start\":87216},{\"end\":87239,\"start\":87232}]", "bib_author_last_name": "[{\"end\":61014,\"start\":61003},{\"end\":61023,\"start\":61016},{\"end\":61093,\"start\":61086},{\"end\":61112,\"start\":61104},{\"end\":61126,\"start\":61119},{\"end\":61137,\"start\":61132},{\"end\":61152,\"start\":61145},{\"end\":61168,\"start\":61163},{\"end\":61186,\"start\":61178},{\"end\":61427,\"start\":61422},{\"end\":61442,\"start\":61437},{\"end\":61464,\"start\":61451},{\"end\":61678,\"start\":61657},{\"end\":61694,\"start\":61687},{\"end\":61712,\"start\":61705},{\"end\":62216,\"start\":62204},{\"end\":62229,\"start\":62223},{\"end\":62244,\"start\":62239},{\"end\":62258,\"start\":62252},{\"end\":62271,\"start\":62266},{\"end\":62601,\"start\":62596},{\"end\":62617,\"start\":62607},{\"end\":62631,\"start\":62626},{\"end\":62647,\"start\":62643},{\"end\":62662,\"start\":62657},{\"end\":62672,\"start\":62664},{\"end\":63042,\"start\":63035},{\"end\":63054,\"start\":63050},{\"end\":63070,\"start\":63063},{\"end\":63092,\"start\":63081},{\"end\":63529,\"start\":63526},{\"end\":63547,\"start\":63537},{\"end\":63561,\"start\":63553},{\"end\":63979,\"start\":63973},{\"end\":63992,\"start\":63987},{\"end\":64011,\"start\":64004},{\"end\":64028,\"start\":64023},{\"end\":64521,\"start\":64512},{\"end\":64533,\"start\":64525},{\"end\":64544,\"start\":64537},{\"end\":64557,\"start\":64548},{\"end\":64802,\"start\":64798},{\"end\":64821,\"start\":64811},{\"end\":64838,\"start\":64831},{\"end\":64852,\"start\":64848},{\"end\":65105,\"start\":65081},{\"end\":65118,\"start\":65114},{\"end\":65145,\"start\":65137},{\"end\":65157,\"start\":65147},{\"end\":65679,\"start\":65673},{\"end\":65694,\"start\":65689},{\"end\":65711,\"start\":65706},{\"end\":65725,\"start\":65718},{\"end\":65743,\"start\":65734},{\"end\":65761,\"start\":65753},{\"end\":65773,\"start\":65767},{\"end\":65786,\"start\":65782},{\"end\":65801,\"start\":65794},{\"end\":66146,\"start\":66140},{\"end\":66166,\"start\":66155},{\"end\":66185,\"start\":66175},{\"end\":66444,\"start\":66433},{\"end\":66461,\"start\":66454},{\"end\":66471,\"start\":66468},{\"end\":66487,\"start\":66480},{\"end\":66503,\"start\":66495},{\"end\":66520,\"start\":66514},{\"end\":66537,\"start\":66530},{\"end\":66551,\"start\":66546},{\"end\":66567,\"start\":66560},{\"end\":66573,\"start\":66569},{\"end\":67070,\"start\":67065},{\"end\":67084,\"start\":67077},{\"end\":67091,\"start\":67086},{\"end\":67259,\"start\":67253},{\"end\":67272,\"start\":67268},{\"end\":67291,\"start\":67284},{\"end\":67307,\"start\":67300},{\"end\":67579,\"start\":67572},{\"end\":67594,\"start\":67585},{\"end\":67609,\"start\":67602},{\"end\":67626,\"start\":67616},{\"end\":67902,\"start\":67899},{\"end\":67913,\"start\":67910},{\"end\":67930,\"start\":67924},{\"end\":67942,\"start\":67937},{\"end\":68236,\"start\":68229},{\"end\":68254,\"start\":68245},{\"end\":68460,\"start\":68449},{\"end\":68473,\"start\":68467},{\"end\":68483,\"start\":68475},{\"end\":68816,\"start\":68808},{\"end\":68830,\"start\":68826},{\"end\":68839,\"start\":68832},{\"end\":69002,\"start\":68997},{\"end\":69014,\"start\":69011},{\"end\":69035,\"start\":69025},{\"end\":69252,\"start\":69247},{\"end\":69265,\"start\":69261},{\"end\":69281,\"start\":69276},{\"end\":69294,\"start\":69291},{\"end\":69305,\"start\":69303},{\"end\":69597,\"start\":69594},{\"end\":69610,\"start\":69606},{\"end\":69627,\"start\":69624},{\"end\":70108,\"start\":70105},{\"end\":70121,\"start\":70117},{\"end\":70138,\"start\":70135},{\"end\":70399,\"start\":70392},{\"end\":70417,\"start\":70409},{\"end\":70431,\"start\":70424},{\"end\":70448,\"start\":70441},{\"end\":70462,\"start\":70454},{\"end\":70895,\"start\":70892},{\"end\":70911,\"start\":70906},{\"end\":70926,\"start\":70920},{\"end\":70942,\"start\":70936},{\"end\":70962,\"start\":70951},{\"end\":70975,\"start\":70971},{\"end\":71304,\"start\":71299},{\"end\":71323,\"start\":71316},{\"end\":71781,\"start\":71772},{\"end\":71797,\"start\":71789},{\"end\":71815,\"start\":71806},{\"end\":71834,\"start\":71823},{\"end\":72367,\"start\":72362},{\"end\":72380,\"start\":72375},{\"end\":72392,\"start\":72388},{\"end\":72408,\"start\":72402},{\"end\":72702,\"start\":72689},{\"end\":72714,\"start\":72711},{\"end\":72729,\"start\":72726},{\"end\":72735,\"start\":72731},{\"end\":73203,\"start\":73200},{\"end\":73216,\"start\":73213},{\"end\":73231,\"start\":73227},{\"end\":73243,\"start\":73240},{\"end\":73259,\"start\":73255},{\"end\":73835,\"start\":73832},{\"end\":73847,\"start\":73844},{\"end\":73859,\"start\":73856},{\"end\":73872,\"start\":73869},{\"end\":73888,\"start\":73884},{\"end\":74201,\"start\":74198},{\"end\":74213,\"start\":74210},{\"end\":74225,\"start\":74220},{\"end\":74236,\"start\":74233},{\"end\":74251,\"start\":74247},{\"end\":74267,\"start\":74263},{\"end\":74688,\"start\":74680},{\"end\":74702,\"start\":74696},{\"end\":74706,\"start\":74704},{\"end\":75118,\"start\":75108},{\"end\":75134,\"start\":75125},{\"end\":75153,\"start\":75147},{\"end\":75806,\"start\":75800},{\"end\":75825,\"start\":75817},{\"end\":75834,\"start\":75829},{\"end\":75849,\"start\":75841},{\"end\":75864,\"start\":75856},{\"end\":75873,\"start\":75866},{\"end\":76151,\"start\":76140},{\"end\":76167,\"start\":76163},{\"end\":76184,\"start\":76176},{\"end\":76574,\"start\":76571},{\"end\":76586,\"start\":76582},{\"end\":76604,\"start\":76596},{\"end\":76897,\"start\":76893},{\"end\":77193,\"start\":77188},{\"end\":77203,\"start\":77200},{\"end\":77220,\"start\":77213},{\"end\":77236,\"start\":77228},{\"end\":77252,\"start\":77245},{\"end\":77272,\"start\":77261},{\"end\":77285,\"start\":77281},{\"end\":77677,\"start\":77669},{\"end\":77694,\"start\":77687},{\"end\":77711,\"start\":77704},{\"end\":77727,\"start\":77718},{\"end\":77738,\"start\":77733},{\"end\":77752,\"start\":77745},{\"end\":78056,\"start\":78052},{\"end\":78075,\"start\":78069},{\"end\":78533,\"start\":78530},{\"end\":78548,\"start\":78542},{\"end\":78560,\"start\":78557},{\"end\":78574,\"start\":78572},{\"end\":79065,\"start\":79059},{\"end\":79076,\"start\":79071},{\"end\":79094,\"start\":79086},{\"end\":79110,\"start\":79104},{\"end\":79123,\"start\":79119},{\"end\":79139,\"start\":79133},{\"end\":79151,\"start\":79148},{\"end\":79168,\"start\":79159},{\"end\":79181,\"start\":79175},{\"end\":79193,\"start\":79188},{\"end\":79513,\"start\":79507},{\"end\":79530,\"start\":79525},{\"end\":79933,\"start\":79928},{\"end\":79953,\"start\":79941},{\"end\":79966,\"start\":79961},{\"end\":80379,\"start\":80374},{\"end\":80399,\"start\":80387},{\"end\":80412,\"start\":80407},{\"end\":80804,\"start\":80799},{\"end\":80819,\"start\":80813},{\"end\":80839,\"start\":80827},{\"end\":80854,\"start\":80849},{\"end\":80868,\"start\":80862},{\"end\":80881,\"start\":80876},{\"end\":81440,\"start\":81434},{\"end\":81456,\"start\":81450},{\"end\":81471,\"start\":81463},{\"end\":81487,\"start\":81480},{\"end\":81840,\"start\":81832},{\"end\":81865,\"start\":81854},{\"end\":81872,\"start\":81867},{\"end\":82259,\"start\":82248},{\"end\":82275,\"start\":82266},{\"end\":82291,\"start\":82285},{\"end\":82308,\"start\":82301},{\"end\":82558,\"start\":82552},{\"end\":82582,\"start\":82571},{\"end\":82600,\"start\":82592},{\"end\":82617,\"start\":82610},{\"end\":82635,\"start\":82626},{\"end\":82651,\"start\":82642},{\"end\":82667,\"start\":82661},{\"end\":83222,\"start\":83214},{\"end\":83242,\"start\":83231},{\"end\":83259,\"start\":83254},{\"end\":83276,\"start\":83268},{\"end\":83806,\"start\":83798},{\"end\":83824,\"start\":83815},{\"end\":84033,\"start\":84025},{\"end\":84044,\"start\":84040},{\"end\":84064,\"start\":84058},{\"end\":84478,\"start\":84475},{\"end\":84493,\"start\":84489},{\"end\":84506,\"start\":84503},{\"end\":84517,\"start\":84512},{\"end\":84807,\"start\":84804},{\"end\":84822,\"start\":84818},{\"end\":84835,\"start\":84832},{\"end\":84846,\"start\":84841},{\"end\":85325,\"start\":85320},{\"end\":85344,\"start\":85337},{\"end\":85361,\"start\":85354},{\"end\":85376,\"start\":85370},{\"end\":85392,\"start\":85383},{\"end\":85405,\"start\":85400},{\"end\":85419,\"start\":85413},{\"end\":85434,\"start\":85429},{\"end\":85780,\"start\":85774},{\"end\":85791,\"start\":85784},{\"end\":85805,\"start\":85800},{\"end\":85811,\"start\":85807},{\"end\":86255,\"start\":86249},{\"end\":86280,\"start\":86266},{\"end\":86296,\"start\":86288},{\"end\":86311,\"start\":86305},{\"end\":86326,\"start\":86320},{\"end\":86343,\"start\":86338},{\"end\":86638,\"start\":86631},{\"end\":86655,\"start\":86645},{\"end\":86670,\"start\":86664},{\"end\":86695,\"start\":86687},{\"end\":87176,\"start\":87172},{\"end\":87187,\"start\":87183},{\"end\":87200,\"start\":87196},{\"end\":87214,\"start\":87208},{\"end\":87230,\"start\":87222},{\"end\":87248,\"start\":87240}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":61053,\"start\":60999},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7448214},\"end\":61381,\"start\":61055},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3346458},\"end\":61634,\"start\":61383},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5720491},\"end\":62129,\"start\":61636},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":44604205},\"end\":62532,\"start\":62131},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":316800},\"end\":62939,\"start\":62534},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2474496},\"end\":63482,\"start\":62941},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":161878},\"end\":63900,\"start\":63484},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4637111},\"end\":64236,\"start\":63902},{\"attributes\":{\"id\":\"b9\"},\"end\":64434,\"start\":64238},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7204540},\"end\":64721,\"start\":64436},{\"attributes\":{\"doi\":\"abs/1706.05587\",\"id\":\"b11\"},\"end\":65036,\"start\":64723},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12578058},\"end\":65601,\"start\":65038},{\"attributes\":{\"doi\":\"abs/1604.01685\",\"id\":\"b13\"},\"end\":66061,\"start\":65603},{\"attributes\":{\"doi\":\"abs/1712.07629\",\"id\":\"b14\"},\"end\":66364,\"start\":66063},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12552176},\"end\":67009,\"start\":66366},{\"attributes\":{\"id\":\"b16\"},\"end\":67201,\"start\":67011},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9455111},\"end\":67493,\"start\":67203},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13911644},\"end\":67827,\"start\":67495},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6050505},\"end\":68177,\"start\":67829},{\"attributes\":{\"id\":\"b20\"},\"end\":68408,\"start\":68179},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1694378},\"end\":68758,\"start\":68410},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18199642},\"end\":68989,\"start\":68760},{\"attributes\":{\"doi\":\"abs/1608.06993\",\"id\":\"b23\"},\"end\":69174,\"start\":68991},{\"attributes\":{\"doi\":\"abs/1909.03444\",\"id\":\"b24\"},\"end\":69499,\"start\":69176},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":29162783},\"end\":70018,\"start\":69501},{\"attributes\":{\"doi\":\"abs/1903.07414\",\"id\":\"b26\"},\"end\":70311,\"start\":70020},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4676177},\"end\":70815,\"start\":70313},{\"attributes\":{\"doi\":\"abs/1612.01925\",\"id\":\"b28\"},\"end\":71196,\"start\":70817},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5808102},\"end\":71736,\"start\":71198},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6099034},\"end\":72273,\"start\":71738},{\"attributes\":{\"doi\":\"abs/1704.05519\",\"id\":\"b31\"},\"end\":72598,\"start\":72275},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":49654964},\"end\":73127,\"start\":72600},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":53096051},\"end\":73743,\"start\":73129},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2038500},\"end\":74147,\"start\":73745},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":102352655},\"end\":74632,\"start\":74149},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6628106},\"end\":75036,\"start\":74634},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195908774},\"end\":75721,\"start\":75038},{\"attributes\":{\"doi\":\"abs/1904.06882\",\"id\":\"b38\"},\"end\":76080,\"start\":75723},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1211102},\"end\":76498,\"start\":76082},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10458500},\"end\":76825,\"start\":76500},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":221242327},\"end\":77084,\"start\":76827},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4078515},\"end\":77609,\"start\":77086},{\"attributes\":{\"doi\":\"abs/1810.08393\",\"id\":\"b43\"},\"end\":77982,\"start\":77611},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":15539264},\"end\":78478,\"start\":77984},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":43977194},\"end\":79014,\"start\":78480},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":40027675},\"end\":79441,\"start\":79016},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1379674},\"end\":79852,\"start\":79443},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1824134},\"end\":80316,\"start\":79854},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1840436},\"end\":80755,\"start\":80318},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":53084192},\"end\":81379,\"start\":80757},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":206769866},\"end\":81796,\"start\":81381},{\"attributes\":{\"id\":\"b52\"},\"end\":82199,\"start\":81798},{\"attributes\":{\"doi\":\"abs/1712.05773\",\"id\":\"b53\"},\"end\":82461,\"start\":82201},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":20603040},\"end\":83119,\"start\":82463},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":119308663},\"end\":83701,\"start\":83121},{\"attributes\":{\"id\":\"b56\"},\"end\":83961,\"start\":83703},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":15889275},\"end\":84377,\"start\":83963},{\"attributes\":{\"doi\":\"abs/1809.05571\",\"id\":\"b58\"},\"end\":84723,\"start\":84379},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":30824366},\"end\":85237,\"start\":84725},{\"attributes\":{\"doi\":\"abs/1803.10368\",\"id\":\"b60\",\"matched_paper_id\":4704473},\"end\":85689,\"start\":85239},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":9677455},\"end\":86189,\"start\":85691},{\"attributes\":{\"doi\":\"abs/1908.06812\",\"id\":\"b62\"},\"end\":86552,\"start\":86191},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":207168299},\"end\":87103,\"start\":86554},{\"attributes\":{\"doi\":\"abs/1608.05442\",\"id\":\"b64\"},\"end\":87455,\"start\":87105},{\"attributes\":{\"id\":\"b65\"},\"end\":87597,\"start\":87457}]", "bib_title": "[{\"end\":61077,\"start\":61055},{\"end\":61410,\"start\":61383},{\"end\":61649,\"start\":61636},{\"end\":62196,\"start\":62131},{\"end\":62588,\"start\":62534},{\"end\":63022,\"start\":62941},{\"end\":63516,\"start\":63484},{\"end\":63962,\"start\":63902},{\"end\":64508,\"start\":64436},{\"end\":65070,\"start\":65038},{\"end\":66424,\"start\":66366},{\"end\":67243,\"start\":67203},{\"end\":67565,\"start\":67495},{\"end\":67890,\"start\":67829},{\"end\":68445,\"start\":68410},{\"end\":68802,\"start\":68760},{\"end\":69584,\"start\":69501},{\"end\":70383,\"start\":70313},{\"end\":71290,\"start\":71198},{\"end\":71766,\"start\":71738},{\"end\":72676,\"start\":72600},{\"end\":73187,\"start\":73129},{\"end\":73819,\"start\":73745},{\"end\":74185,\"start\":74149},{\"end\":74676,\"start\":74634},{\"end\":75101,\"start\":75038},{\"end\":76131,\"start\":76082},{\"end\":76566,\"start\":76500},{\"end\":76883,\"start\":76827},{\"end\":77177,\"start\":77086},{\"end\":78044,\"start\":77984},{\"end\":78523,\"start\":78480},{\"end\":79052,\"start\":79016},{\"end\":79498,\"start\":79443},{\"end\":79918,\"start\":79854},{\"end\":80364,\"start\":80318},{\"end\":80789,\"start\":80757},{\"end\":81426,\"start\":81381},{\"end\":81828,\"start\":81798},{\"end\":82543,\"start\":82463},{\"end\":83207,\"start\":83121},{\"end\":84016,\"start\":83963},{\"end\":84795,\"start\":84725},{\"end\":85311,\"start\":85239},{\"end\":85762,\"start\":85691},{\"end\":86622,\"start\":86554}]", "bib_author": "[{\"end\":61016,\"start\":61001},{\"end\":61025,\"start\":61016},{\"end\":61095,\"start\":61079},{\"end\":61114,\"start\":61095},{\"end\":61128,\"start\":61114},{\"end\":61139,\"start\":61128},{\"end\":61154,\"start\":61139},{\"end\":61170,\"start\":61154},{\"end\":61188,\"start\":61170},{\"end\":61429,\"start\":61412},{\"end\":61444,\"start\":61429},{\"end\":61466,\"start\":61444},{\"end\":61680,\"start\":61651},{\"end\":61696,\"start\":61680},{\"end\":61714,\"start\":61696},{\"end\":62218,\"start\":62198},{\"end\":62231,\"start\":62218},{\"end\":62246,\"start\":62231},{\"end\":62260,\"start\":62246},{\"end\":62273,\"start\":62260},{\"end\":62603,\"start\":62590},{\"end\":62619,\"start\":62603},{\"end\":62633,\"start\":62619},{\"end\":62649,\"start\":62633},{\"end\":62664,\"start\":62649},{\"end\":62674,\"start\":62664},{\"end\":63044,\"start\":63024},{\"end\":63056,\"start\":63044},{\"end\":63072,\"start\":63056},{\"end\":63094,\"start\":63072},{\"end\":63531,\"start\":63518},{\"end\":63549,\"start\":63531},{\"end\":63563,\"start\":63549},{\"end\":63981,\"start\":63964},{\"end\":63994,\"start\":63981},{\"end\":64013,\"start\":63994},{\"end\":64030,\"start\":64013},{\"end\":64523,\"start\":64510},{\"end\":64535,\"start\":64523},{\"end\":64546,\"start\":64535},{\"end\":64559,\"start\":64546},{\"end\":64804,\"start\":64786},{\"end\":64823,\"start\":64804},{\"end\":64840,\"start\":64823},{\"end\":64854,\"start\":64840},{\"end\":65107,\"start\":65072},{\"end\":65120,\"start\":65107},{\"end\":65147,\"start\":65120},{\"end\":65159,\"start\":65147},{\"end\":65681,\"start\":65666},{\"end\":65696,\"start\":65681},{\"end\":65713,\"start\":65696},{\"end\":65727,\"start\":65713},{\"end\":65745,\"start\":65727},{\"end\":65763,\"start\":65745},{\"end\":65775,\"start\":65763},{\"end\":65788,\"start\":65775},{\"end\":65803,\"start\":65788},{\"end\":66148,\"start\":66133},{\"end\":66168,\"start\":66148},{\"end\":66187,\"start\":66168},{\"end\":66446,\"start\":66426},{\"end\":66463,\"start\":66446},{\"end\":66473,\"start\":66463},{\"end\":66489,\"start\":66473},{\"end\":66505,\"start\":66489},{\"end\":66522,\"start\":66505},{\"end\":66539,\"start\":66522},{\"end\":66553,\"start\":66539},{\"end\":66569,\"start\":66553},{\"end\":66575,\"start\":66569},{\"end\":67072,\"start\":67063},{\"end\":67086,\"start\":67072},{\"end\":67093,\"start\":67086},{\"end\":67261,\"start\":67245},{\"end\":67274,\"start\":67261},{\"end\":67293,\"start\":67274},{\"end\":67309,\"start\":67293},{\"end\":67581,\"start\":67567},{\"end\":67596,\"start\":67581},{\"end\":67611,\"start\":67596},{\"end\":67628,\"start\":67611},{\"end\":67904,\"start\":67892},{\"end\":67915,\"start\":67904},{\"end\":67932,\"start\":67915},{\"end\":67944,\"start\":67932},{\"end\":68238,\"start\":68222},{\"end\":68256,\"start\":68238},{\"end\":68462,\"start\":68447},{\"end\":68475,\"start\":68462},{\"end\":68485,\"start\":68475},{\"end\":68818,\"start\":68804},{\"end\":68832,\"start\":68818},{\"end\":68841,\"start\":68832},{\"end\":69004,\"start\":68993},{\"end\":69016,\"start\":69004},{\"end\":69037,\"start\":69016},{\"end\":69254,\"start\":69239},{\"end\":69267,\"start\":69254},{\"end\":69283,\"start\":69267},{\"end\":69296,\"start\":69283},{\"end\":69307,\"start\":69296},{\"end\":69599,\"start\":69586},{\"end\":69612,\"start\":69599},{\"end\":69629,\"start\":69612},{\"end\":70110,\"start\":70097},{\"end\":70123,\"start\":70110},{\"end\":70140,\"start\":70123},{\"end\":70401,\"start\":70385},{\"end\":70419,\"start\":70401},{\"end\":70433,\"start\":70419},{\"end\":70450,\"start\":70433},{\"end\":70464,\"start\":70450},{\"end\":70897,\"start\":70887},{\"end\":70913,\"start\":70897},{\"end\":70928,\"start\":70913},{\"end\":70944,\"start\":70928},{\"end\":70964,\"start\":70944},{\"end\":70977,\"start\":70964},{\"end\":71306,\"start\":71292},{\"end\":71325,\"start\":71306},{\"end\":71783,\"start\":71768},{\"end\":71799,\"start\":71783},{\"end\":71817,\"start\":71799},{\"end\":71836,\"start\":71817},{\"end\":72369,\"start\":72357},{\"end\":72382,\"start\":72369},{\"end\":72394,\"start\":72382},{\"end\":72410,\"start\":72394},{\"end\":72704,\"start\":72678},{\"end\":72716,\"start\":72704},{\"end\":72731,\"start\":72716},{\"end\":72737,\"start\":72731},{\"end\":73205,\"start\":73189},{\"end\":73218,\"start\":73205},{\"end\":73233,\"start\":73218},{\"end\":73245,\"start\":73233},{\"end\":73261,\"start\":73245},{\"end\":73837,\"start\":73821},{\"end\":73849,\"start\":73837},{\"end\":73861,\"start\":73849},{\"end\":73874,\"start\":73861},{\"end\":73890,\"start\":73874},{\"end\":74203,\"start\":74187},{\"end\":74215,\"start\":74203},{\"end\":74227,\"start\":74215},{\"end\":74238,\"start\":74227},{\"end\":74253,\"start\":74238},{\"end\":74269,\"start\":74253},{\"end\":74690,\"start\":74678},{\"end\":74704,\"start\":74690},{\"end\":74708,\"start\":74704},{\"end\":75120,\"start\":75103},{\"end\":75136,\"start\":75120},{\"end\":75155,\"start\":75136},{\"end\":75808,\"start\":75792},{\"end\":75827,\"start\":75808},{\"end\":75836,\"start\":75827},{\"end\":75851,\"start\":75836},{\"end\":75866,\"start\":75851},{\"end\":75875,\"start\":75866},{\"end\":76153,\"start\":76133},{\"end\":76169,\"start\":76153},{\"end\":76186,\"start\":76169},{\"end\":76576,\"start\":76568},{\"end\":76588,\"start\":76576},{\"end\":76606,\"start\":76588},{\"end\":76899,\"start\":76885},{\"end\":77195,\"start\":77179},{\"end\":77205,\"start\":77195},{\"end\":77222,\"start\":77205},{\"end\":77238,\"start\":77222},{\"end\":77254,\"start\":77238},{\"end\":77274,\"start\":77254},{\"end\":77287,\"start\":77274},{\"end\":77679,\"start\":77660},{\"end\":77696,\"start\":77679},{\"end\":77713,\"start\":77696},{\"end\":77729,\"start\":77713},{\"end\":77740,\"start\":77729},{\"end\":77754,\"start\":77740},{\"end\":78058,\"start\":78046},{\"end\":78077,\"start\":78058},{\"end\":78535,\"start\":78525},{\"end\":78550,\"start\":78535},{\"end\":78562,\"start\":78550},{\"end\":78576,\"start\":78562},{\"end\":79067,\"start\":79054},{\"end\":79078,\"start\":79067},{\"end\":79096,\"start\":79078},{\"end\":79112,\"start\":79096},{\"end\":79125,\"start\":79112},{\"end\":79141,\"start\":79125},{\"end\":79153,\"start\":79141},{\"end\":79170,\"start\":79153},{\"end\":79183,\"start\":79170},{\"end\":79195,\"start\":79183},{\"end\":79515,\"start\":79500},{\"end\":79532,\"start\":79515},{\"end\":79935,\"start\":79920},{\"end\":79955,\"start\":79935},{\"end\":79968,\"start\":79955},{\"end\":80381,\"start\":80366},{\"end\":80401,\"start\":80381},{\"end\":80414,\"start\":80401},{\"end\":80806,\"start\":80791},{\"end\":80821,\"start\":80806},{\"end\":80841,\"start\":80821},{\"end\":80856,\"start\":80841},{\"end\":80870,\"start\":80856},{\"end\":80883,\"start\":80870},{\"end\":81442,\"start\":81428},{\"end\":81458,\"start\":81442},{\"end\":81473,\"start\":81458},{\"end\":81489,\"start\":81473},{\"end\":81842,\"start\":81830},{\"end\":81867,\"start\":81842},{\"end\":81874,\"start\":81867},{\"end\":82261,\"start\":82237},{\"end\":82277,\"start\":82261},{\"end\":82293,\"start\":82277},{\"end\":82310,\"start\":82293},{\"end\":82560,\"start\":82545},{\"end\":82584,\"start\":82560},{\"end\":82602,\"start\":82584},{\"end\":82619,\"start\":82602},{\"end\":82637,\"start\":82619},{\"end\":82653,\"start\":82637},{\"end\":82669,\"start\":82653},{\"end\":83224,\"start\":83209},{\"end\":83244,\"start\":83224},{\"end\":83261,\"start\":83244},{\"end\":83278,\"start\":83261},{\"end\":83808,\"start\":83792},{\"end\":83826,\"start\":83808},{\"end\":84035,\"start\":84018},{\"end\":84046,\"start\":84035},{\"end\":84066,\"start\":84046},{\"end\":84480,\"start\":84468},{\"end\":84495,\"start\":84480},{\"end\":84508,\"start\":84495},{\"end\":84519,\"start\":84508},{\"end\":84809,\"start\":84797},{\"end\":84824,\"start\":84809},{\"end\":84837,\"start\":84824},{\"end\":84848,\"start\":84837},{\"end\":85327,\"start\":85313},{\"end\":85346,\"start\":85327},{\"end\":85363,\"start\":85346},{\"end\":85378,\"start\":85363},{\"end\":85394,\"start\":85378},{\"end\":85407,\"start\":85394},{\"end\":85421,\"start\":85407},{\"end\":85436,\"start\":85421},{\"end\":85782,\"start\":85764},{\"end\":85793,\"start\":85782},{\"end\":85807,\"start\":85793},{\"end\":85813,\"start\":85807},{\"end\":86257,\"start\":86243},{\"end\":86282,\"start\":86257},{\"end\":86298,\"start\":86282},{\"end\":86313,\"start\":86298},{\"end\":86328,\"start\":86313},{\"end\":86345,\"start\":86328},{\"end\":86640,\"start\":86624},{\"end\":86657,\"start\":86640},{\"end\":86672,\"start\":86657},{\"end\":86697,\"start\":86672},{\"end\":87178,\"start\":87166},{\"end\":87189,\"start\":87178},{\"end\":87202,\"start\":87189},{\"end\":87216,\"start\":87202},{\"end\":87232,\"start\":87216},{\"end\":87250,\"start\":87232}]", "bib_venue": "[{\"end\":61199,\"start\":61188},{\"end\":61470,\"start\":61466},{\"end\":61747,\"start\":61714},{\"end\":62311,\"start\":62273},{\"end\":62714,\"start\":62674},{\"end\":63157,\"start\":63094},{\"end\":63633,\"start\":63563},{\"end\":64062,\"start\":64030},{\"end\":64276,\"start\":64238},{\"end\":64563,\"start\":64559},{\"end\":64784,\"start\":64723},{\"end\":65271,\"start\":65159},{\"end\":65664,\"start\":65603},{\"end\":66131,\"start\":66063},{\"end\":66628,\"start\":66575},{\"end\":67061,\"start\":67011},{\"end\":67326,\"start\":67309},{\"end\":67644,\"start\":67628},{\"end\":67982,\"start\":67944},{\"end\":68220,\"start\":68179},{\"end\":68527,\"start\":68485},{\"end\":68854,\"start\":68841},{\"end\":69237,\"start\":69176},{\"end\":69692,\"start\":69629},{\"end\":70095,\"start\":70020},{\"end\":70512,\"start\":70464},{\"end\":70885,\"start\":70817},{\"end\":71393,\"start\":71325},{\"end\":71948,\"start\":71836},{\"end\":72355,\"start\":72275},{\"end\":72789,\"start\":72737},{\"end\":73373,\"start\":73261},{\"end\":73928,\"start\":73890},{\"end\":74338,\"start\":74269},{\"end\":74764,\"start\":74708},{\"end\":75272,\"start\":75155},{\"end\":75790,\"start\":75723},{\"end\":76245,\"start\":76186},{\"end\":76644,\"start\":76606},{\"end\":76939,\"start\":76899},{\"end\":77327,\"start\":77287},{\"end\":77658,\"start\":77611},{\"end\":78155,\"start\":78077},{\"end\":78688,\"start\":78576},{\"end\":79217,\"start\":79195},{\"end\":79595,\"start\":79532},{\"end\":80031,\"start\":79968},{\"end\":80477,\"start\":80414},{\"end\":80995,\"start\":80883},{\"end\":81537,\"start\":81489},{\"end\":81948,\"start\":81874},{\"end\":82235,\"start\":82201},{\"end\":82732,\"start\":82669},{\"end\":83362,\"start\":83278},{\"end\":83790,\"start\":83703},{\"end\":84158,\"start\":84066},{\"end\":84466,\"start\":84379},{\"end\":84911,\"start\":84848},{\"end\":85454,\"start\":85450},{\"end\":85887,\"start\":85813},{\"end\":86241,\"start\":86191},{\"end\":86783,\"start\":86697},{\"end\":87164,\"start\":87105},{\"end\":87526,\"start\":87457},{\"end\":63176,\"start\":63159},{\"end\":63648,\"start\":63635},{\"end\":64293,\"start\":64278},{\"end\":65289,\"start\":65273},{\"end\":66645,\"start\":66630},{\"end\":68570,\"start\":68529},{\"end\":69717,\"start\":69694},{\"end\":70527,\"start\":70514},{\"end\":71461,\"start\":71395},{\"end\":71974,\"start\":71950},{\"end\":72798,\"start\":72791},{\"end\":73400,\"start\":73375},{\"end\":74359,\"start\":74340},{\"end\":74784,\"start\":74766},{\"end\":75307,\"start\":75274},{\"end\":76263,\"start\":76247},{\"end\":78233,\"start\":78157},{\"end\":78715,\"start\":78690},{\"end\":79614,\"start\":79597},{\"end\":80050,\"start\":80033},{\"end\":80502,\"start\":80479},{\"end\":81022,\"start\":80997},{\"end\":81555,\"start\":81539},{\"end\":81968,\"start\":81950},{\"end\":82751,\"start\":82734},{\"end\":83383,\"start\":83364},{\"end\":84936,\"start\":84913},{\"end\":85907,\"start\":85889},{\"end\":86802,\"start\":86785}]"}}}, "year": 2023, "month": 12, "day": 17}