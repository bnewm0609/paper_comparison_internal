{"id": 253237200, "updated": "2023-10-20 13:28:24.575", "metadata": {"title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers", "authors": "[{\"first\":\"Elias\",\"last\":\"Frantar\",\"middle\":[]},{\"first\":\"Saleh\",\"last\":\"Ashkboos\",\"middle\":[]},{\"first\":\"Torsten\",\"last\":\"Hoefler\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Alistarh\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.17323", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2210-17323", "doi": "10.48550/arxiv.2210.17323"}}, "content": {"source": {"pdf_hash": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.17323v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "40a758646927702cf4ee78e55994fd9bc4388a30", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6.txt", "contents": "\nPublished as a conference paper at ICLR 2023 GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS\n\n\nElias Frantar \nSaleh Ashkboos \nEth Zurich \nTorsten Hoefler \nEth Zurich \nDan Alistarh \n\nIST\nAustria\n\n\nIST\nAustria & NeuralMagic\n\nPublished as a conference paper at ICLR 2023 GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS\n\nGenerative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highlyaccurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.\n\nINTRODUCTION\n\nPre-trained generative models from the Transformer (Vaswani et al., 2017) family, commonly known as GPT or OPT (Radford et al., 2019;Brown et al., 2020;, have shown breakthrough performance for complex language modelling tasks, leading to massive academic and practical interest. One major obstacle to their usability is computational and storage cost, which ranks among the highest for known models. For instance, the best-performing model variants, e.g. GPT3-175B, have in the order of 175 billion parameters and require tens-to-hundreds of GPU years to train . Even the simpler task of inferencing over a pre-trained model, which is our focus in this paper, is highly challenging: for instance, the parameters of GPT3-175B occupy 326GB (counting in multiples of 1024) of memory when stored in a compact float16 format. This exceeds the capacity of even the highest-end single GPUs, and thus inference must be performed using more complex and expensive setups, such as multi-GPU deployments.\n\nAlthough a standard approach to eliminating these overheads is model compression, e.g. (Hoefler et al., 2021;Gholami et al., 2021), surprisingly little is known about compressing such models for inference. One reason is that more complex methods for low-bitwidth quantization or model pruning usually require model retraining, which is extremely expensive for billion-parameter models. Alternatively, post-training methods (Nagel et al., 2020;Wang et al., 2020;Hubara et al., 2020;Nahshan et al., 2021), which compress the model in one shot, without retraining, would be very appealing. Unfortunately, the more accurate variants of such methods (Li et al., 2021;Hubara et al., 2021;Frantar et al., 2022) are complex and challenging to scale to billions of parameters . To date, only basic variants of round-to-nearest quantization Dettmers et al., 2022) have been applied at the scale of GPT-175B; while this works well for low compression targets, e.g., 8-bit weights, they fail to preserve accuracy at higher rates. It therefore remains open whether one-shot post-training quantization to higher compression rates is generally-feasible. BLOOM Model Family 3bit RTN 3bit GPTQ FP16 Figure 1: Quantizing OPT models to 4 and BLOOM models to 3 bit precision, comparing GPTQ with the FP16 baseline and round-to-nearest (RTN) Dettmers et al., 2022).\n\nContribution. In this paper, we present a new post-training quantization method, called GPTQ, 1 which is efficient enough to execute on models with hundreds of billions of parameters in at most a few hours, and precise enough to compress such models to 3 or 4 bits per parameter without significant loss of accuracy. For illustration, GPTQ can quantize the largest publicly-available models, OPT-175B and BLOOM-176B, in approximately four GPU hours, with minimal increase in perplexity, known to be a very stringent accuracy metric.\n\nFurther, we show that our model can also provide robust results in the extreme quantization regime, in which models are quantized to 2 bits per component, or even ternary values. On the practical side, we develop an execution harness which allows us to execute the resulting compressed models efficiently for generative tasks. Specifically, we are able to run the compressed OPT-175B model for the first time on a single NVIDIA A100 GPU, or using only two more cost-effective NVIDIA A6000 GPUs. We also implement bespoke GPU kernels which are able to leverage compression for faster memory loading, resulting in speedups of \u2248 3.25\u00d7 when using A100 GPUs, and 4.5\u00d7 when using A6000 GPUs.\n\nTo our knowledge, we are the first to show that extremely accurate language models with hundreds of billions of parameters can be quantized to 3-4 bits/component: prior post-training methods only remain accurate at 8 bits Dettmers et al., 2022), while prior training-based techniques have only tackled models that are smaller by one to two orders of magnitude . This high degree of compression may appear natural, as these networks are overparametrized; yet, as we discuss in our detailed analysis of results, compression induces non-trivial tradeoffs between the accuracy of the language modeling (perplexity), bit-width, and the size of the original model.\n\nWe hope that our work will stimulate further research in this area, and can be a further step towards making these models available to a wider audience. In terms of limitations, our method currently does not provide speedups for the actual multiplications, due to the lack of hardware support for mixed-precision operands (e.g. FP16 x INT4) on mainstream architectures. Moreover, our current results do not include activation quantization, as they are not a significant bottleneck in our target scenarios; however, this can be supported using orthogonal techniques .\n\n\nRELATED WORK\n\nQuantization methods fall broadly into two categories: quantization during training, and posttraining methods. The former quantize models during typically extensive retraining and/or finetuning, using some approximate differentiation mechanism for the rounding operation (Gholami et al., 2021;Nagel et al., 2021). By contrast, post-training (\"one-shot\") methods quantize a pre-trained model using modest resources, typically a few thousand data samples and a few hours of computation. Post-training approaches are particularly interesting for massive models, for which full model training or even finetuning can be expensive. We focus on this scenario here.\n\nPost-training Quantization. Most post-training methods have focused on vision models. Usually, accurate methods operate by quantizing either individual layers, or small blocks of consecutive layers. (See Section 3 for more details.) The AdaRound method (Nagel et al., 2020) computes a data-dependent rounding by annealing a penalty term, which encourages weights to move towards grid points corresponding to quantization levels. BitSplit (Wang et al., 2020) constructs quantized values bit-by-bit using a squared error objective on the residual error, while AdaQuant (Hubara et al., 2021) performs direct optimization based on straight-through estimates. BRECQ (Li et al., 2021) introduces Fisher information into the objective, and optimizes layers within a single residual block jointly. Finally, Optimal Brain Quantization (OBQ) (Frantar et al., 2022) generalizes the classic Optimal Brain Surgeon (OBS) second-order weight pruning framework (Hassibi et al., 1993;Singh & Alistarh, 2020;Frantar et al., 2021) to apply to quantization. OBQ quantizes weights one-by-one, in order of quantization error, always adjusting the remaining weights. While these approaches can produce good results for models up to \u2248 100 million parameters in a few GPU hours, scaling them to networks orders of magnitude larger is challenging.\n\n\nLarge-model Quantization.\n\nWith the recent open-source releases of language models like BLOOM (Lauren\u00e7on et al., 2022) or OPT-175B , researchers have started to develop affordable methods for compressing such giant networks for inference. While all existing works-ZeroQuant , LLM.int8() (Dettmers et al., 2022), and nuQmm (Park et al., 2022)-carefully select quantization granularity, e.g., vector-wise, they ultimately just round weights to the nearest (RTN) quantization level, in order to maintain acceptable runtimes for very large models. ZeroQuant further proposes layer-wise knowledge distillation, similar to AdaQuant, but the largest model it can apply this approach to has only 1.3 billion parameters. At this scale, ZeroQuant already takes \u2248 3 hours of compute; GPTQ quantizes models 100\u00d7 larger in \u2248 4 hours. LLM.int8() observes that activation outliers in a few feature dimensions break the quantization of larger models, and proposes to fix this problem by keeping those dimensions in higher precision. Lastly, nuQmm develops efficient GPU kernels for a specific binary-coding based quantization scheme.\n\nRelative to this line of work, we show that a significantly more complex and accurate quantizer can be implemented efficiently at large model scale. Specifically, GPTQ more than doubles the amount of compression relative to these prior techniques, at similar accuracy.\n\n\nBACKGROUND\n\nLayer-Wise Quantization. At a high level, our method follows the structure of state-of-the-art post-training quantization methods (Nagel et al., 2020;Wang et al., 2020;Hubara et al., 2021;Frantar et al., 2022), by performing quantization layer-by-layer, solving a corresponding reconstruction problem for each layer. Concretely, let W be the weights corresponding to a linear layer and let X denote the layer input corresponding to a small set of m data points running through the network. Then, the objective is to find a matrix of quantized weights W which minimizes the squared error, relative to the full precision layer output. Formally, this can be restated as\nargmin W ||WX \u2212 WX|| 2 2 .(1)\nFurther, similar to (Nagel et al., 2020;Li et al., 2021;Frantar et al., 2022), we assume that the quantization grid for W is fixed before the process, and that individual weights can move freely as in (Hubara et al., 2021;Frantar et al., 2022).\n\nOptimal Brain Quantization. Our approach builds on the recently-proposed Optimal Brain Quanization (OBQ) method (Frantar et al., 2022) for solving the layer-wise quantization problem defined above, to which we perform a series of major modifications, which allow it to scale to large language models, providing more than three orders of magnitude computational speedup. To aid understanding, we first briefly summarize the original OBQ method.\n\nThe OBQ method starts from the observation that Equation (1) can be written as the sum of the squared errors, over each row of W. Then, OBQ handles each row w independently, quantizing one weight at a time while always updating all not-yet-quantized weights, in order to compensate for the error incurred by quantizing a single weight. Since the corresponding objective is a quadratic, whose Hessian is H F = 2X F X F , where F denotes the set of remaining full-precision weights, the greedy-optimal weight to quantize next, which we denote by w q , and the corresponding optimal update of all weights in F , denoted by \u03b4 F , are given by the following formulas, where quant(w) rounds w to the nearest value on the quantization grid:\nw q = argmin wq (quant(w q ) \u2212 w q ) 2 [H \u22121 F ] qq , \u03b4 F = \u2212 w q \u2212 quant(w q ) [H \u22121 F ] qq \u00b7 (H \u22121 F ) :,q .(2)\nOBQ quantizes weights iteratively using these two equations, until all the weights of w are quantized. This is done efficiently, avoiding expensive full recomputations of H \u22121 , by removing the qth row and column of H, which is necessary after quantizing w q , directly in the inverse via one step of Gaussian elimination. Namely, the updated inverse is given by the formula\nH \u22121 \u2212q = H \u22121 \u2212 1 [H \u22121 ] qq H \u22121 :,q H \u22121 q,: \u2212p .(3)\nThis method comes with a vectorized implementation, handling multiple rows of W in parallel. Eventually, the algorithm can achieve reasonable runtimes on medium-sized models: for instance, it can fully quantize the ResNet-50 model (25M parameters) in \u2248 1 hour on a single GPU, which is roughly in line with other post-training methods achieving state-of-the-art accuracy (Frantar et al., 2022). However, the fact that OBQ's runtime for a d row \u00d7 d col matrix W has cubic input dependency O(d row \u00b7 d 3 col ) means that applying it to models with billions of parameters is extremely expensive.\n\n\nTHE GPTQ ALGORITHM\n\nStep 1: Arbitrary Order Insight. As explained in the previous section, OBQ quantizes weights in greedy order, i.e. it always picks the weight which currently incurs the least additional quantization error. Interestingly, we find that, while this quite natural strategy does indeed seem to perform very well, its improvement over quantizing the weights in arbitrary order is generally small, in particular on large, heavily-parametrized layers. Most likely, this is because the slightly lower number of quantized weights with large individual error is balanced out by those weights being quantized towards the end of the process, when only few other unquantized weights that can be adjusted for compensation remain. As we will now discuss, this insight that any fixed order may perform well, especially on large models, has interesting ramifications.\n\n\nInverse Layer Hessian (Cholesky Form)\n\ncomputed initially block i quantized recursively column-by-column\n\n\nWeight Matrix / Block\n\nunquantized weights that are updated quantized weights Figure 2: GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.\n\nThe original OBQ method quantizes rows of W independently, in a specific order defined by the corresponding errors. By contrast, we will aim to quantize the weights of all rows in the same order, and will show that this typically yields results with a final squared error that is similar to the original solutions. As a consequence, the set of unquantized weights F and similarly H \u22121 F is always the same for all rows (see Figure 2 for an illustration). In more detail, the latter is due to the fact that H F depends only on the layer inputs X F , which are the same for all rows, and not on any weights. Therefore, we have to perform the update of H \u22121 F given by Equation (3) only d col times, once per column, rather than d row \u00b7d col times, once per weight. This reduces the overall runtime from O\n(d row \u00b7 d 3 col ) to O(max {d row \u00b7 d 2 col , d 3 col }), i.e.\n, by a factor of min {d row , d col }. For larger models, this difference consists of several orders of magnitude. However, before this algorithm can actually be applied to very large models in practice, two additional major problems need to be addressed.\n\nStep 2: Lazy Batch-Updates. First, a direct implementation of the scheme described previously will not be fast in practice, because the algorithm has a relatively low compute-to-memory-access ratio. For example, Equation (3) needs to update all elements of a potentially huge matrix using just a few FLOPs for each entry. Such operations cannot properly utilize the massive compute capabilities of modern GPUs, and will be bottlenecked by the significantly lower memory bandwidth.\n\nFortunately, this problem can be resolved by the following observation: The final rounding decisions for column i are only affected by updates performed on this very column, and so updates to later columns are irrelevant at this point in the process. This makes it possible to \"lazily batch\" updates together, thus achieving much better GPU utilization. Concretely, we apply the algorithm to B = 128 columns at a time, keeping updates contained to those columns and the corresponding B \u00d7 B block of H \u22121 (see also Figure 2). Only once a block has been fully processed, we perform global updates of the entire H \u22121 and W matrices using the multi-weight versions of Equations (2) and (3) given below, with Q denoting a set of indices, and H \u22121 \u2212Q denoting the inverse matrix with the corresponding rows and columns removed:\n\u03b4 F = \u2212(w Q \u2212 quant(w Q ))([H \u22121 F ] QQ ) \u22121 (H \u22121 F ) :,Q ,(4)H \u22121 \u2212Q = H \u22121 \u2212 H \u22121 :,Q ([H \u22121 ] QQ ) \u22121 H \u22121 Q,: \u2212Q .(5)\nAlthough this strategy does not reduce the theoretical amount of compute, it effectively addresses the memory-throughput bottleneck. This provides an order of magnitude speedup for very large models in practice, making it a critical component of our algorithm.\n\nStep 3: Cholesky Reformulation. The final technical issue we have to address is given by numerical inaccuracies, which can become a major problem at the scale of existing models, especially when combined with the block updates discussed in the previous step. Specifically, it can occur that the matrix H \u22121 F becomes indefinite, which we notice can cause the algorithm to aggressively update the remaining weights in incorrect directions, resulting in an arbitrarily-bad quantization of the corresponding layer. In practice, we observed that the probability of this happening increases with model size: concretely, it almost certainly occurs for at least a few layers on models that are larger than a few billion parameters. The main issue appears to be the repeated applications of Equation (5), which accumulate various numerical errors, especially through the additional matrix inversion.\n\nFor smaller models, applying dampening, that is adding a small constant \u03bb (we always choose 1% of the average diagonal value) to the diagonal elements of H appears to be sufficient to avoid numerical issues. However, larger models require a more robust and general approach.\n\nTo address this, we begin by noting that the only information required from H \u22121 Fq , where F q denotes the set of unquantized weights when quantizing weight q, is row q, or more precisely, the elements in this row starting with the diagonal. The consequence is that we could precompute all of these rows using a more numerically-stable method without any significant increase in memory consumption. Indeed, the row removal via (3) for our symmetric H \u22121 essentially corresponds to taking a Cholesky decomposition, except for the minor difference that the latter divides row q by ([H \u22121 Fq ] qq ) 1/2 . Hence, we can leverage state-of-the-art Cholesky kernels to compute all information we will need from H \u22121 upfront. In combination with mild dampening, the resulting method is robust enough to execute on huge models without issues. As a bonus, using a well-optimized Cholesky kernel also yields further speedup. We detail all small changes necessary for the Cholesky version of the algorithm next.\n\nThe Full Algorithm. Finally, we present the full pseudocode for GPTQ in Algorithm 1, including the optimizations discussed above.\n\nAlgorithm 1 Quantize W given inverse Hessian H \u22121 = (2XX + \u03bbI) \u22121 and blocksize B. \nQ \u2190 0 drow\u00d7d col // quantized output E \u2190 0 drow\u00d7B // block quantization errors H \u22121 \u2190 Cholesky(H \u22121 ) // Hessian inverse information for i = 0, B, 2B, . . . do for j = i, . . . , i + B \u2212 1 do Q:,\n\nEXPERIMENTAL VALIDATION\n\nOverview. We begin our experiments by validating the accuracy of GPTQ relative to other accuratebut-expensive quantizers, on smaller models, for which these methods provide reasonable runtimes. Next, we examine GPTQ's runtime scaling for very large models. Then, we present 3-and 4-bit quantization results for the entire BLOOM and OPT model families, evaluated via perplexity on challenging language generation tasks. In addition, we show that our method is also stable for 2-bit quantization when the granularity is reduced to small blocks of consecutive weights. To complement this perplexity analysis, we also evaluate the resulting quantized models on a series of standard zeroshot tasks. Finally, we focus on the two largest (and interesting) openly-available models, Bloom-176B and OPT-175B, where we perform a detailed evaluation on several tasks. For these models, we also present practical improvements, namely reducing the number of GPUs required for inference as well as end-to-end speedups for generative tasks.\n\nSetup. We implemented GPTQ in PyTorch (Paszke et al., 2019) and worked with the HuggingFace integrations of the BLOOM (Lauren\u00e7on et al., 2022) and OPT  model families. We quantized all models (including the 175 billion parameter variants) using a single NVIDIA A100 GPU with 80GB of memory. Our entire GPTQ calibration data consists of 128 random 2048 token segments from the C4 dataset (Raffel et al., 2020), i.e., excerpts from randomly crawled websites, which represents generic text data. We emphasize that this means that GPTQ does not see any task-specific data, and our results thus remain actually \"zero-shot\". We perform standard uniform per-row asymmetric quantization on the min-max grid, similar to Dettmers et al. (2022). Additional evaluation details can be found in Appendix A.2.1.\n\nTo ensure that the entire compression procedure can be performed with significantly less GPU memory than what would be required to run the full precision model, some care must be taken. Specifically, we always load one Transformer block, consisting of 6 layers, at a time into GPU memory and then accumulate the layer-Hessians and perform quantization. Finally, the current block inputs are sent through the fully quantized block again to produce the new inputs for the quantization of the next block. Hence, the quantization process operates not on the layer inputs in the full precision model but on the actual layer inputs in the already partially quantized one. We find that this brings noticeable improvements at negligible extra cost.\n\nBaselines. Our primary baseline, denoted by RTN, consists of rounding all weights to the nearest quantized value on exactly the same asymmetric per-row grid that is also used for GPTQ, meaning that it corresponds precisely to the state-of-the-art weight quantization of LLM.int8(). This is currently the method of choice in all works on quantization of very large language models (Dettmers et al., 2022;Park et al., 2022): its runtime scales well to networks with many billions of parameters, as it simply performs direct rounding. As we will also discuss further, more accurate methods, such as AdaRound (Nagel et al., 2020) or BRECQ (Li et al., 2021), are currently too slow for models with many billions of parameters, the main focus of this work. Nevertheless, we also show that GPTQ is competitive with such methods for small models, while scaling to huge ones like OPT-175B as well.\n\nQuantizing Small Models. As a first ablation study, we compare GPTQ's performance relative to state-of-the-art post-training quantization (PTQ) methods, on ResNet18 and ResNet50, which are standard PTQ benchmarks, in the same setup as (Frantar et al., 2022). As can be seen in Table 1, GPTQ performs on par at 4-bit, and slightly worse than the most accurate methods at 3-bit. At the same time, it significantly outperforms AdaQuant, the fastest amongst prior PTQ methods. Further, we compare against the full greedy OBQ method on two smaller language models: BERT-base (Devlin et al., 2019) and OPT-125M. The results are shown in Appendix Table 8. At 4 bits, both methods perform similarly, and for 3 bits, GPTQ surprisingly performs slightly better. We suspect that this is because some of the additional heuristics used by OBQ, such as early outlier rounding, might require careful adjustments for optimal performance on non-vision models. Overall, GPTQ appears to be competitive with state-of-the-art post-training methods for smaller models, while taking only < 1 minute rather than \u2248 1 hour. This enables scaling to much larger models.\n\nRuntime. Next we measure the full model quantization time (on a single NVIDIA A100 GPU) via GPTQ; the results are shown in Table 2. As can be seen, GPTQ quantizes 1-3 billion parameter models in a matter of minutes and 175B ones in a few hours. For reference, the straight-through based method ZeroQuant-LKD  reports a 3 hour runtime (on the same hardware) for a 1.3B model, which would linearly extrapolate to several hundred hours (a few weeks) for 175B  models. Adaptive rounding-based methods typically employ a lot more SGD steps and would thus be even more expensive (Nagel et al., 2020;Li et al., 2021).\n\nLanguage Generation. We begin our large-scale study by compressing the entire OPT and BLOOM model families to 3-and 4-bit. We then evaluate those models on several language tasks including WikiText2 ( 3). We focus on these perplexitybased tasks, as they are known to be particularly sensitive to model quantization . On OPT models, GPTQ clearly outperforms RTN, by significant margins. For example, GPTQ loses only 0.03 perplexity at 4-bit on the 175B model, while RTN drops 2.2 points, performing worse than the 10\u00d7 smaller full-precision 13B model. At 3-bit, RTN collapses completely, while GPTQ can still maintain reasonable perplexity, in particular for larger models. BLOOM shows a similar pattern: the gaps between methods are however usually a bit smaller, indicating that this model family might be easier to quantize. One interesting trend (see also Figure 1) is that larger models generally (with the exception of OPT-66B 2 ) appear easier to quantize. This is good news for practical applications, as these are the cases where compression is also the most necessary.   175 Billion Parameter Models. We now examine BLOOM-176B and OPT-175B, the largest dense openly-available models. Table 5 summarizes results across Wikitext-2, PTB, C4. We observe that, at 4 bits, GPTQ models reach only \u2264 0.25 lower perplexity than the full-precision versions, with a large gap to RTN results on OPT-175B. At 3-bit, RTN collapses, while GPTQ is still able to maintain good performance on most tasks, losing only 0.3 \u2212 0.6 points for more than 5\u00d7 compression. We note that GPTQ's accuracy can be further improved via finer-granularity grouping (Park et al., 2022 We note that grouping interacts very well with GPTQ, as the group parameters can be determined during the quantization process of each layer, always using the most current updated weights.  Table 5: Results summary for OPT-175B and BLOOM-176B. \"g1024\" and \"g128\" denote results with groupings of size 1024 and 128, respectively.\n\n\nOPT\n\nPractical Speedups. Finally, we study practical applications. As an interesting use-case, we focus on the OPT-175B model: quantized to 3 bits, this model takes approximately 63GB of memory, including the embeddings and the output layer, which are kept in full FP16 precision. Additionally, storing the complete history of keys and values for all layers, a common optimization for generation tasks, consumes another \u2248 9GB for the maximum of 2048 tokens. Hence, we can actually fit the entire quantized model into a single 80GB A100 GPU, which can be executed by dynamically dequantizing layers as they are required during inference (the model would not fully fit using 4 bits). For reference, standard FP16 execution requires 5x80GB GPUs, and the state-of-the-art 8bit LLM.int8() quantizer (Dettmers et al., 2022) requires 3 such GPUs.\n\nNext, we consider language generation, one of the most appealing applications of these models, with the goal of latency reduction. Unlike LLM.int8(), which reduces memory costs but has the same runtime as the FP16 baseline, we show that our quantized models can achieve significant speedups for this application. For language generation, the model processes and outputs one token at-a-time, which for OPT-175B can easily take a few 100s of milliseconds per token. Increasing the speed at which the user receives generated results is challenging, as compute is dominated by matrix-vector products. Unlike matrix-matrix products, these are primarily limited by memory bandwidth. We address this problem by developing a quantized-matrix full-precision-vector product kernel which performs a matrix vector product by dynamically dequantizing weights when needed. Most notably, this does not require any activation quantization. While dequantization consumes extra compute, the kernel has to access a lot less memory, leading to significant speedups, as shown in Table 6. We note that almost all of the speedup is due to our kernels, as communication costs are negligible in our standard HuggingFace-accelerate-like setting (see Appendix A.2.2 for details).  For example, using our kernels, the 3-bit OPT-175B model obtained via GPTQ running on a single A100 is about 3.25\u00d7 faster than the FP16 version (running on 5 GPUs) in terms of average time per token. More accessible GPUs, such as the NVIDIA A6000, have much lower memory bandwidth, so this strategy is even more effective: executing the 3-bit OPT-175B model on 2x A6000 GPUs reduces latency from 589 milliseconds for FP16 inference (on 8 GPUs) to 130 milliseconds, a 4.5\u00d7 latency reduction.\n\nZero-Shot Tasks. While our focus is on language generation, we also evaluate the performance of quantized models on some popular zero-shot tasks, namely LAMBADA (Paperno et al., 2016), ARC (Easy and Challenge) (Boratko et al., 2018) and PIQA (Tata & Patel, 2003). Figure 3 visualizes model performance on LAMBADA (and see also \"Lamb.\" results in Table 5). We observe similar behavior as before: the outliers are that 1) quantization appears \"easier\" across the whole spectrum of models at 4-bit, where even RTN performs relatively well, and 2) at 3-bit, RTN breaks down, while GPTQ still provides good accuracy. We provide additional results in Appendix A.4. Additional Tricks. While our experiments so far have focused exclusively on vanilla row-wise quantization, we want to emphasize that GPTQ is compatible with essentially any choice of quantization grid. For example, it is easily combined with standard grouping (Alistarh et al., 2017;Park et al., 2022), i.e. applying independent quantization to groups of g consecutive weights. As shown in the last rows of Table 5, this can bring noticeable extra accuracy for the largest models at 3-bit. Further, as visualized in Figure 4, it significantly reduces the accuracy losses for medium sized models at 4-bit precision.\n\nModel FP16 g128 g64 g32 3-bit OPT-175B 8.34 9.58 9.18 8.94 8.68 BLOOM 8.11 9.55 9.17 8.83 8.64 Table 7: 2-bit GPTQ quantization results with varying group-sizes; perplexity on WikiText2. Extreme Quantization. Lastly, grouping also makes it possible to achieve reasonable performance for extreme quantization, to around 2-bits per component on average. Table 7 shows results on WikiText2 when quantizing the biggest models to 2-bit with varying group-sizes. At \u2248 2.2 bit (group-size 128; using FP16 scale and 2-bit zero point per group) the perplexity increase is already less than 1.5 points, while dropping to 0.6 -0.7 at \u2248 2.6 bit (group-size 32), which is only slightly worse than vanilla 3-bit and might be interesting for practical kernel implementations. Further, if we reduce group size to 8, we can apply ternary (-1, 0, +1) quantization, which achieves 9.20 WikiText2 PPL on OPT-175B, a less than 1 point drop. While this leads to worse compression on average relative to the 2-bit numbers above, this pattern could be efficiently implemented on custom hardware such as FPGAs. In summary, these results are an encouraging first step towards pushing highly-accurate one-shot compression of very large language models, even lower than 3 bits per value on average.\n\n\nSUMMARY AND LIMITATIONS\n\nWe have presented GPTQ, an approximate second-order method for quantizing truly large language models. GPTQ can accurately compress some of the largest publicly-available models down to 3 and 4 bits, which leads to significant usability improvements, and to end-to-end speedups, at low accuracy loss. We hope that our method will make these models accessible to more researchers and practitioners. At the same time, we emphasize some significant limitations: On the technical side, our method obtains speedups from reduced memory movement, and does not lead to computational reductions. In addition, our study focuses on generative tasks, and does not consider activation quantization. These are natural directions for future work, and we believe this can be achieved with carefully-designed GPU kernels and existing techniques  \n\n\nETHICS STATEMENT\n\nOur work introduces a general method for compressing large language models (LLMs) via quantization, with little-to-no accuracy loss in terms of standard accuracy metrics such as perplexity. Our method is task-agnostic, as it only uses a tiny amount of randomly-chosen data for calibration. We therefore do not foresee any significant ethical implications arising directly from the technical details of our method. However, one possible consideration is that our study focused on \"leading accuracy\" metrics that are standard in the literature, such as perplexity, which is essentially standard in the literature (Dettmers et al., 2022;. We believe a thorough study of the impact of compression upon secondary measures, and in particular bias effects (Bender et al., 2021) is warranted, and may be rendered easier through our work. At the same time, our work makes inference on extremely large language models more accessible, for better or for worse. We believe that, in time, such tools will become much easier to use and deploy, making the need to understand their power and limitations even more stringent.\n\n\nREPRODUCIBILITY STATEMENT\n\nIn the Supplementary Materials, we provide code to reproduce all experiments in this paper. More specifically, this includes:\n\n\u2022 Compressing all models from the OPT and BLOOM model families to 2/3/4 bits.\n\n\u2022 Evaluating perplexity of the quantized models.\n\n\u2022 Our 3-bit CUDA kernel together with compressed inference benchmarking features.\n\n\u2022 Code for the ZeroShot experiments.\n\n\u2022 A README file providing sample commands and information on how to run all scripts.\n\n\nA APPENDIX\n\n\nA.1 ADDITIONAL COMPARISON WITH OBQ\n\nWe now provide an additional comparison between GPTQ and OBQ on BERT-base/SQuAD Rajpurkar et al. (2016) and OPT-125M/WikiText2, which is one of the largest models to which OBQ can be reasonably applied.\n\n\nMethod\n\nBERT   Radford et al. (2019), as follows: First, the entire validation set is concatenated using two linebreaks as separators and encoded using the default HuggingFace tokenizer of each model. Next, the sequence is split into non-overlapping segments of width 2048, the full context size of our models. These are sent through the model to collect the log-probabilities corresponding to the next token each. Their exponentiated average is the final perplexity we report.\n\nFor zero-shot tasks we follow the EleutherAI evaluation harness 3 in terms of data preprocessing and final score calculation. We note that we evaluate all individual samples separately and thus do not apply any padding.\n\n\nA.2.2 TIMING EXPERIMENT SETUP\n\nOur timing experiments are performed following the standard HuggingFace/accelerate 4 setup also used by the recent work LLM.int8() (Dettmers et al., 2022). In this setting, the model is split by distributing chunks of consecutive layers across GPUs. Importantly, in this setup the communication costs are minimal, < 5% of the total runtime even when working with 8 GPUs. This means almost all of the reported speedups are due to our quantized-matrix full-precision vector product kernels. We emphasize that the only difference between the FP16 baseline and our quantized models are the kernels used to perform the underlying matrix-vector products.\n\nThis means all overheads due to HuggingFace, attention or non-quantized operations like residuals or LayerNorms are exactly the same. Consequently, our quantized models should benefit from more advanced distribution strategies (Zheng et al., 2022) or more efficient attention kernels (Dao et al., 2022) just as much as our baseline.\n\nIn general, our kernels target generative inference in the low batch-size setting (for simplicity, we consider only batchsize 1) where the underlying (close to) matrix-vector products are memorybound. For non-generative and large-batch applications, operations may be compute-rather than memory-bound and our kernels thus not directly applicable. Instead, one could simply decompress the matrix before performing the corresponding matrix-matrix calculations: this takes < 1.5ms on an A100 and < 3ms on an A6000 compared to 76ms/365ms for the subsequent OPT-175B FC2 layer computation with batchsize 16\u00d71024 tokens. Hence, for such applications our methods significantly reduce the required number of GPUs at very little computational overhead. This is similar to recent work (Dettmers et al., 2022), but we achieve a 2.5\u00d7 higher compression rate.\n\n\nA.3 ADDITIONAL LANGUAGE GENERATION RESULTS\n\nTables 9, 10, 11 and 12 show additional results for language generation tasks.               \n\n\nj \u2190 quant(W:,j) // quantize column E:,j\u2212i \u2190 (W:,j \u2212 Q:,j) / [H \u22121 ]jj // quantization error W :,j:(i+B) \u2190 W :,j:(i+B) \u2212 E:,j\u2212i \u00b7 H \u22121 j,j:(i+B) // update weights in block end for W :,(i+B): \u2190 W :,(i+B): \u2212 E \u00b7 H \u22121 i:(i+B),(i+B):// update all remaining weights end for\n\n\nMerity et al., 2016) (seeFigure 1as well as Tables 3 and 4), Penn Treebank (PTB)(Marcus et al., 1994) and C4(Raffel et al., 2020) (both in Appendix A.\n\n\n): group-size 1024 (\u2248 0.02 extra bits) improves perplexities by about 0.2 on average and group-size 128 (\u2248 0.15 extra bits) by another 0.1, which is only 0.1 \u2212 0.3 off from the uncompressed accuracy.\n\nFigure 3 :\n3The accuracy of OPT and BLOOM models post-GPTQ, measured on LAMBADA.\n\nFigure 4 :\n4GPTQ at 4-bit with different group-sizes on medium sized OPT models.\n\nTable 2 :\n2GPTQ runtime for full quantization of the 4 largest OPT and BLOOM models.\n\nTable 3 :\n3OPT perplexity results on WikiText2.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n22.42 17.69 15.39 13.48 11.37 8.11 \n\nRTN \n4 \n25.90 22.00 16.97 14.76 12.10 8.37 \nGPTQ \n4 \n24.03 19.05 16.48 14.20 11.73 8.21 \n\nRTN \n3 \n57.08 50.19 63.59 39.36 17.38 \n571 \nGPTQ \n3 \n32.31 25.08 21.11 17.40 13.47 8.64 \n\n\n\nTable 4 :\n4BLOOM perplexity results for WikiText2.\n\nTable 6 :\n6Average per-token latency (batch size 1) when generating sequences of length 128.\n\n\n). Elias Frantar and Dan Alistarh gratefully acknowledge funding from the European Research Council (ERC) under the European Union's Horizon 2020 programme (grant agreement No. 805223 ScaleML), as well as experimental support from Eldar Kurtic, and from the IST Austria IT department, in particular Stefano Elefante, Andrei Hornoiu, and Alois Schloegl. The work of Saleh Ashkboos and Torsten Hoefler was supported by the PASC DaCeMI project, received EuroHPC-JU funding under grant MAELSTROM, No. 955513. We thank the Swiss National Supercomputing Center (CSCS) for supporting us with compute infrastructure.ACKNOWLEDGMENTS \n\n\n\nTable 8 :\n8Comparison of GPTQ relative to OBQ on BERT-base/SQuAD and OPT-125M/WikiText2.A.2 EXPERIMENT DETAILS \n\nThis section provides additional details about our experiment setup, in particular regarding the model \nevaluation and the setup of our timing experiments. \n\nA.2.1 EVALUATION \n\nFor language generation experiments, we calculate the perplexity, in standard fashion like \n\nTable 9 :\n9OPT perplexity results on PTB.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n43.69 57.96 30.00 25.34 20.83 14.59 \n\nRTN \n4 \n51.10 66.85 33.58 27.68 22.42 15.00 \nGPTQ \n4 \n46.97 62.47 31.84 26.49 21.67 14.75 \n\nRTN \n3 \n126. \n185. \n106. 66.78 35.04 107. \nGPTQ \n3 \n70.35 87.04 46.11 34.02 26.14 15.57 \n\n\n\nTable 10 :\n10BLOOM perplexity results for PTB. 22.59 16.07 14.34 12.71 12.06 11.44 10.99 10.13OPT \nBits 125M 350M 1.3B \n2.7B \n6.7B \n13B \n30B \n66B \n175B \n\nfull \n16 \n26.56 RTN \n4 \n33.91 26.21 24.51 18.43 14.36 13.36 13.46 309. 11.61 \nGPTQ \n4 \n29.22 24.63 16.97 15.00 13.18 12.26 11.57 11.23 10.28 \n\nRTN \n3 \n834 \n55.49 5.2e3 1.1e4 5.3e3 3.1e3 1.4e3 3.5e3 4.6e3 \nGPTQ \n3 \n42.41 31.33 21.63 18.17 17.14 13.34 12.23 14.59 10.67 \n\n\n\nTable 11 :\n11OPT perplexity results on C4. We note that the calibration data used by GPTQ is sampled from the C4 training set, this task is thus not fully zero-shot.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n26.60 22.05 19.49 17.49 15.20 11.71 \n\nRTN \n4 \n29.89 24.44 21.26 18.76 16.06 12.04 \nGPTQ \n4 \n28.00 23.25 20.55 18.10 15.60 11.81 \n\nRTN \n3 \n67.49 60.71 113. 80.49 22.59 598. \nGPTQ \n3 \n35.78 28.83 25.34 21.25 17.67 12.27 \n\n\n\nTable 12 :\n12BLOOM perplexity results for C4. We note that the calibration data used by GPTQ is sampled from the C4 training set, this task is thus not fully zero-shot.A.4 ADDITIONAL ZEROSHOT RESULTSThis section contains additional results for zero-shot tasks.OPT \nBits 125M 350M 1.3B \n2.7B \n6.7B \n13B \n30B \n66B \n175B \n\nfull \n16 \n39.16 46.67 58.80 64.82 68.72 70.23 72.39 74.93 75.59 \n\nRTN \n4 \n18.34 40.62 36.31 59.27 64.66 67.38 70.48 13.08 71.34 \nGPTQ \n4 \n34.74 48.38 56.45 62.97 66.37 69.12 72.40 74.50 76.80 \n\nRTN \n3 \n0.10 \n27.36 \n0.00 \n0.00 \n0.00 \n0.06 \n1.46 \n2.00 \n0.00 \nGPTQ \n3 \n13.93 32.31 37.26 52.26 54.98 64.18 69.69 57.02 76.19 \n\n\n\nTable 13 :\n13OPT accuracy on LAMBADA.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n34.06 42.85 46.71 52.12 57.79 67.40 \n\nRTN \n4 \n26.00 39.06 41.92 45.84 50.48 66.70 \nGPTQ \n4 \n31.75 39.80 46.28 51.41 54.65 67.71 \n\nRTN \n3 \n9.10 \n15.95 15.02 24.55 29.90 0.17 \nGPTQ \n3 \n21.31 28.70 33.65 43.12 47.41 65.10 \n\n\n\nTable 14 :\n14BLOOM accuracy on LAMBADA.OPT \nBits 125M 350M 1.3B \n2.7B \n6.7B \n13B \n30B \n66B \n175B \n\nfull \n16 \n62.02 64.74 72.36 74.81 76.39 76.88 78.18 79.76 81.07 \n\nRTN \n4 \n61.43 63.44 67.63 73.72 76.44 76.01 77.26 60.07 78.23 \nGPTQ \n4 \n61.26 63.71 70.73 73.99 76.28 76.61 79.00 79.33 81.00 \n\nRTN \n3 \n56.09 60.61 52.77 51.90 50.49 52.99 56.37 50.87 51.25 \nGPTQ \n3 \n59.25 61.32 68.34 71.38 73.29 75.24 77.58 71.27 80.03 \n\n\n\nTable 15 :\n15OPT accuracy on PIQA.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n65.07 67.14 69.97 70.51 73.72 79.16 \n\nRTN \n4 \n63.11 65.29 67.74 69.86 72.69 79.00 \nGPTQ \n4 \n64.31 66.05 68.77 69.42 72.96 79.00 \n\nRTN \n3 \n58.60 60.80 60.88 66.28 69.70 53.32 \nGPTQ \n3 \n61.62 62.62 65.18 68.34 70.95 77.70 \n\n\n\nTable 16 :\n16BLOOM accuracy on PIQA.OPT \nBits 125M 350M 1.3B \n2.7B \n6.7B \n13B \n30B \n66B \n175B \n\nfull \n16 \n39.69 40.36 50.93 54.34 60.14 61.83 65.40 67.26 71.04 \n\nRTN \n4 \n36.32 38.55 49.20 52.90 57.68 61.31 61.11 40.66 63.93 \nGPTQ \n4 \n39.02 37.92 59.97 53.11 59.72 61.32 65.11 65.35 68.69 \n\nRTN \n3 \n30.43 36.07 27.97 26.05 25.04 30.60 34.22 25.84 26.77 \nGPTQ \n3 \n36.15 36.91 46.17 48.19 53.41 56.82 59.72 52.44 65.36 \n\n\n\nTable 17 :\n17OPT accuracy on ARC-easy.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n41.71 45.41 48.11 53.24 57.37 67.47 \n\nRTN \n4 \n39.40 42.51 44.70 51.35 56.14 66.33 \nGPTQ \n4 \n40.24 44.49 44.49 52.82 56.14 67.42 \n\nRTN \n3 \n45.44 46.87 37.58 45.08 48.61 28.87 \nGPTQ \n3 \n39.14 41.79 42.85 46.63 51.56 62.84 \n\n\n\nTable 18 :\n18BLOOM accuracy on ARC-easy.OPT \nBits 125M 350M 1.3B \n2.7B \n6.7B \n13B \n30B \n66B \n175B \n\nfull \n16 \n22.87 24.06 29.44 31.31 34.56 35.75 38.14 40.02 43.94 \n\nRTN \n4 \n22.44 23.81 24.91 29.18 32.59 35.24 35.41 22.87 37.71 \nGPTQ \n4 \n22.95 24.83 28.24 30.12 33.70 34.90 37.80 39.16 42.75 \n\nRTN \n3 \n21.76 22.18 23.55 25.43 25.85 23.81 19.97 25.77 23.81 \nGPTQ \n3 \n22.53 25.09 27.65 27.82 31.91 33.02 35.84 31.66 41.04 \n\n\n\nTable 19 :\n19OPT accuracy on ARC-challenge.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n24.15 25.68 26.79 30.55 33.45 44.97 \n\nRTN \n4 \n23.89 23.34 26.45 29.52 32.17 43.17 \nGPTQ \n4 \n23.46 25.51 25.94 28.92 32.25 44.20 \n\nRTN \n3 \n21.67 22.86 23.29 27.13 31.31 24.74 \nGPTQ \n3 \n23.21 24.06 24.91 28.58 30.97 40.70 \n\n\n\nTable 20 :\n20BLOOM accuracy on ARC-challenge.OPT \nBits 125M 350M 1.3B \n2.7B \n6.7B \n13B \n30B \n66B \n175B \n\nfull \n16 \n59.96 63.21 70.78 71.74 74.60 76.64 77.28 77.34 79.82 \n\nRTN \n4 \n60.02 63.08 59.13 70.78 73.65 74.47 75.37 51.24 78.04 \nGPTQ \n4 \n59.58 63.46 69.64 70.46 73.90 76.19 77.08 77.15 80.08 \n\nRTN \n3 \n49.65 56.78 47.61 46.98 48.12 49.20 49.84 48.19 46.47 \nGPTQ \n3 \n57.03 60.15 65.25 68.43 70.97 73.07 75.68 71.23 78.04 \n\n\n\nTable 21 :\n21OPT accuracy on StoryCloze.BLOOM Bits 560M 1.1B \n1.7B \n3B \n7.1B 176B \n\nfull \n16 \n61.94 63.27 65.44 67.79 71.99 76.89 \n\nRTN \n4 \n60.15 60.66 62.95 67.09 70.72 76.00 \nGPTQ \n4 \n61.17 62.32 64.48 67.22 71.36 76.32 \n\nRTN \n3 \n54.87 56.08 55.79 59.83 66.20 48.50 \nGPTQ \n3 \n57.80 59.77 61.81 63.97 69.26 75.37 \n\n\n\nTable 22 :\n22BLOOM accuracy on StoryCloze.\nThis merges the name of the OPT model family with the abbreviation for post-training quantization (PTQ).\nUpon closer inspection of the OPT-66B model, it appears that this is correlated with the fact that this trained model has a significant fraction of dead units in the early layers, which may make it harder to compress.\nhttps://github.com/EleutherAI/lm-evaluation-harness 4 https://huggingface.co/docs/accelerate/index\n\nQSGD: Randomized quantization for communication-efficient stochastic gradient descent. Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic, Conference on Neural Information Processing Systems (NeurIPS). Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Randomized quantization for communication-efficient stochastic gradient descent. In Conference on Neural Information Processing Systems (NeurIPS), 2017.\n\nOn the dangers of stochastic parrots: Can language models be too big. M Emily, Timnit Bender, Angelina Gebru, Shmargaret Mcmillan-Major, Shmitchell, 2021 ACM Conference on Fairness, Accountability, and Transparency. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021.\n\nA systematic classification of knowledge, reasoning, and context within the ARC dataset. Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew Mccallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, arXiv:1806.00358arXiv preprintMichael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, et al. A systematic classification of knowledge, reasoning, and context within the ARC dataset. arXiv preprint arXiv:1806.00358, 2018.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Conference on Neural Information Processing Systems (NeurIPS). 2020Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\n\nFlashAttention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Y Daniel, Stefano Fu, Atri Ermon, Christopher Rudra, R\u00e9, arXiv:2205.14135arXiv preprintTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8(Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, North American Chapter of the Association for Computational Linguistics (NAACL). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Associ- ation for Computational Linguistics (NAACL), 2019.\n\nM-FAC: Efficient matrix-free approximations of second-order information. Elias Frantar, Eldar Kurtic, Dan Alistarh, Conference on Neural Information Processing Systems (NeurIPS). 2021Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-FAC: Efficient matrix-free approximations of second-order information. In Conference on Neural Information Processing Systems (NeurIPS), 2021.\n\nOptimal Brain Compression: A framework for accurate post-training quantization and pruning. Elias Frantar, Sidak Pal, Dan Singh, Alistarh, arXiv:2208.11580arXiv preprintAccepted to NeurIPS 2022, to appearElias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal Brain Compression: A framework for ac- curate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022. Accepted to NeurIPS 2022, to appear.\n\nA survey of quantization methods for efficient neural network inference. Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, W Michael, Kurt Mahoney, Keutzer, arXiv:2103.13630arXiv preprintAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n\nOptimal brain surgeon and general network pruning. Babak Hassibi, G David, Gregory J Stork, Wolff, IEEE International Conference on Neural Networks. Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network pruning. In IEEE International Conference on Neural Networks, 1993.\n\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, Alexandra Peste, arXiv:2102.00554Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. arXiv preprintTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. arXiv preprint arXiv:2102.00554, 2021.\n\nImproving post training neural quantization: Layer-wise calibration and integer programming. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, Daniel Soudry, arXiv:2006.10518arXiv preprintItay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020.\n\nAccurate post training quantization with small calibration sets. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, Daniel Soudry, International Conference on Machine Learning (ICML). 2021Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post train- ing quantization with small calibration sets. In International Conference on Machine Learning (ICML), 2021.\n\nThe BigScience corpus: A 1.6 TB composite multilingual dataset. Hugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki ; Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, Albert Villanova del MoralHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, et al. The BigScience corpus: A 1.6 TB composite multilingual dataset. 2022.\n\nBRECQ: Pushing the limit of post-training quantization by block reconstruction. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu, International Conference on Learning Representations (ICLR. 2021Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations (ICLR), 2021.\n\nThe penn treebank: Annotating predicate argument structure. Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger, Human Language Technology: Proceedings of a Workshop. Plainsboro, New JerseyMitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843arXiv preprintStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\nUp or down? Adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning (ICML). 2020Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.\n\nMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, Tijmen Blankevoort, arXiv:2106.08295A white paper on neural network quantization. arXiv preprintMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021.\n\nLoss aware post-training quantization. Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, Avi Mendelson, Machine Learning. 110Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bron- stein, and Avi Mendelson. Loss aware post-training quantization. Machine Learning, 110(11): 3245-3262, 2021.\n\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fern\u00e1ndez, arXiv:1606.06031The LAMBADA dataset: Word prediction requiring a broad discourse context. arXiv preprintDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nnuQmm: Quantized matmul for efficient inference of large-scale generative language models. Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, arXiv:2206.09557arXiv preprintGunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuQmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n\nPytorch: An imperative style, highperformance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Conference on Neural Information Processing Systems (NeurIPS). Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. In Conference on Neural Information Processing Systems (NeurIPS), 2019.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Conference on Empirical Methods in Natural Language Processing (EMNLP). Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n\nWoodFisher: Efficient second-order approximation for neural network compression. Pal Sidak, Dan Singh, Alistarh, Conference on Neural Information Processing Systems (NeurIPS). 2020Sidak Pal Singh and Dan Alistarh. WoodFisher: Efficient second-order approximation for neural network compression. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\n\nPiQA: An algebra for querying protein data sets. Sandeep Tata, M Jignesh, Patel, International Conference on Scientific and Statistical Database Management. Sandeep Tata and Jignesh M Patel. PiQA: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Conference on Neural Information Processing Systems (NeurIPS). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Conference on Neural In- formation Processing Systems (NeurIPS), 2017.\n\nTowards accurate post-training network quantization via bit-split and stitching. Peisong Wang, Qiang Chen, Xiangyu He, Jian Cheng, International Conference on Machine Learning (ICML). 2020Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In International Conference on Machine Learning (ICML), 2020.\n\nExtreme compression for pre-trained transformers made simple and efficient. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He, arXiv:2206.01859arXiv preprintXiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for pre-trained transformers made simple and efficient. arXiv preprint arXiv:2206.01859, 2022.\n\nZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, arXiv:2206.01861arXiv preprintZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo- pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nLianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Joseph E Gonzalez, arXiv:2201.12023Automating inter-and intra-operator parallelism for distributed deep learning. arXiv preprintLianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Joseph E Gonzalez, et al. Alpa: Automating inter-and intra-operator parallelism for distributed deep learning. arXiv preprint arXiv:2201.12023, 2022.\n", "annotations": {"author": "[{\"end\":144,\"start\":130},{\"end\":160,\"start\":145},{\"end\":172,\"start\":161},{\"end\":189,\"start\":173},{\"end\":201,\"start\":190},{\"end\":215,\"start\":202},{\"end\":229,\"start\":216},{\"end\":257,\"start\":230}]", "publisher": null, "author_last_name": "[{\"end\":143,\"start\":136},{\"end\":159,\"start\":151},{\"end\":171,\"start\":165},{\"end\":188,\"start\":181},{\"end\":200,\"start\":194},{\"end\":214,\"start\":206}]", "author_first_name": "[{\"end\":135,\"start\":130},{\"end\":150,\"start\":145},{\"end\":164,\"start\":161},{\"end\":180,\"start\":173},{\"end\":193,\"start\":190},{\"end\":205,\"start\":202}]", "author_affiliation": "[{\"end\":228,\"start\":217},{\"end\":256,\"start\":231}]", "title": "[{\"end\":127,\"start\":1},{\"end\":384,\"start\":258}]", "venue": null, "abstract": "[{\"end\":2160,\"start\":386}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2249,\"start\":2227},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2309,\"start\":2287},{\"end\":2328,\"start\":2309},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3280,\"start\":3258},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3301,\"start\":3280},{\"end\":3614,\"start\":3594},{\"end\":3632,\"start\":3614},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3652,\"start\":3632},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3673,\"start\":3652},{\"end\":3833,\"start\":3816},{\"end\":3853,\"start\":3833},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3874,\"start\":3853},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4024,\"start\":4002},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4514,\"start\":4492},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5982,\"start\":5960},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7274,\"start\":7252},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7293,\"start\":7274},{\"end\":7913,\"start\":7893},{\"end\":8097,\"start\":8078},{\"end\":8228,\"start\":8207},{\"end\":8318,\"start\":8301},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8494,\"start\":8472},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8607,\"start\":8585},{\"end\":8630,\"start\":8607},{\"end\":8651,\"start\":8630},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9082,\"start\":9058},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9274,\"start\":9251},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9305,\"start\":9286},{\"end\":10516,\"start\":10496},{\"end\":10534,\"start\":10516},{\"end\":10554,\"start\":10534},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10575,\"start\":10554},{\"end\":11103,\"start\":11083},{\"end\":11119,\"start\":11103},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11140,\"start\":11119},{\"end\":11285,\"start\":11264},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11306,\"start\":11285},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11443,\"start\":11421},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13426,\"start\":13404},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21569,\"start\":21548},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21652,\"start\":21628},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21918,\"start\":21897},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22243,\"start\":22221},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23453,\"start\":23430},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23471,\"start\":23453},{\"end\":23675,\"start\":23655},{\"end\":23702,\"start\":23685},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24197,\"start\":24175},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24531,\"start\":24510},{\"end\":25676,\"start\":25656},{\"end\":25692,\"start\":25676},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27352,\"start\":27334},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28500,\"start\":28478},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30454,\"start\":30432},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30503,\"start\":30481},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30533,\"start\":30513},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31213,\"start\":31190},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31231,\"start\":31213},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34328,\"start\":34305},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34464,\"start\":34443},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35448,\"start\":35425},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35586,\"start\":35565},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36436,\"start\":36413},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37179,\"start\":37159},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":37234,\"start\":37216},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38064,\"start\":38041},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38544,\"start\":38525},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38626,\"start\":38605},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38654,\"start\":38633}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":38522,\"start\":38253},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38675,\"start\":38523},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38877,\"start\":38676},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38959,\"start\":38878},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39041,\"start\":38960},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39127,\"start\":39042},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39447,\"start\":39128},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39499,\"start\":39448},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":39593,\"start\":39500},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":40222,\"start\":39594},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40605,\"start\":40223},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":40922,\"start\":40606},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":41348,\"start\":40923},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":41789,\"start\":41349},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":42433,\"start\":41790},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":42747,\"start\":42434},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":43170,\"start\":42748},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":43482,\"start\":43171},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":43902,\"start\":43483},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":44218,\"start\":43903},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":44642,\"start\":44219},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":44963,\"start\":44643},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":45392,\"start\":44964},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":45710,\"start\":45393},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":45754,\"start\":45711}]", "paragraph": "[{\"end\":3169,\"start\":2176},{\"end\":4515,\"start\":3171},{\"end\":5049,\"start\":4517},{\"end\":5736,\"start\":5051},{\"end\":6396,\"start\":5738},{\"end\":6964,\"start\":6398},{\"end\":7638,\"start\":6981},{\"end\":8961,\"start\":7640},{\"end\":10081,\"start\":8991},{\"end\":10351,\"start\":10083},{\"end\":11032,\"start\":10366},{\"end\":11307,\"start\":11063},{\"end\":11752,\"start\":11309},{\"end\":12487,\"start\":11754},{\"end\":12976,\"start\":12602},{\"end\":13625,\"start\":13033},{\"end\":14497,\"start\":13648},{\"end\":14604,\"start\":14539},{\"end\":15061,\"start\":14630},{\"end\":15865,\"start\":15063},{\"end\":16185,\"start\":15930},{\"end\":16667,\"start\":16187},{\"end\":17490,\"start\":16669},{\"end\":17874,\"start\":17614},{\"end\":18767,\"start\":17876},{\"end\":19043,\"start\":18769},{\"end\":20045,\"start\":19045},{\"end\":20176,\"start\":20047},{\"end\":20261,\"start\":20178},{\"end\":21508,\"start\":20484},{\"end\":22306,\"start\":21510},{\"end\":23048,\"start\":22308},{\"end\":23938,\"start\":23050},{\"end\":25081,\"start\":23940},{\"end\":25693,\"start\":25083},{\"end\":27681,\"start\":25695},{\"end\":28523,\"start\":27689},{\"end\":30269,\"start\":28525},{\"end\":31544,\"start\":30271},{\"end\":32816,\"start\":31546},{\"end\":33673,\"start\":32844},{\"end\":34802,\"start\":33694},{\"end\":34957,\"start\":34832},{\"end\":35036,\"start\":34959},{\"end\":35086,\"start\":35038},{\"end\":35169,\"start\":35088},{\"end\":35207,\"start\":35171},{\"end\":35293,\"start\":35209},{\"end\":35547,\"start\":35345},{\"end\":36027,\"start\":35558},{\"end\":36248,\"start\":36029},{\"end\":36930,\"start\":36282},{\"end\":37264,\"start\":36932},{\"end\":38112,\"start\":37266},{\"end\":38252,\"start\":38159}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11062,\"start\":11033},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12601,\"start\":12488},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13032,\"start\":12977},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15929,\"start\":15866},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17554,\"start\":17491},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17613,\"start\":17554},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20457,\"start\":20262}]", "table_ref": "[{\"end\":24224,\"start\":24217},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":24587,\"start\":24580},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25213,\"start\":25206},{\"end\":26895,\"start\":26888},{\"end\":27550,\"start\":27543},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29590,\"start\":29583},{\"end\":30624,\"start\":30617},{\"end\":31344,\"start\":31337},{\"end\":31648,\"start\":31641},{\"end\":31905,\"start\":31898}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2174,\"start\":2162},{\"attributes\":{\"n\":\"2\"},\"end\":6979,\"start\":6967},{\"end\":8989,\"start\":8964},{\"attributes\":{\"n\":\"3\"},\"end\":10364,\"start\":10354},{\"attributes\":{\"n\":\"4\"},\"end\":13646,\"start\":13628},{\"end\":14537,\"start\":14500},{\"end\":14628,\"start\":14607},{\"attributes\":{\"n\":\"5\"},\"end\":20482,\"start\":20459},{\"end\":27687,\"start\":27684},{\"attributes\":{\"n\":\"6\"},\"end\":32842,\"start\":32819},{\"attributes\":{\"n\":\"7\"},\"end\":33692,\"start\":33676},{\"attributes\":{\"n\":\"8\"},\"end\":34830,\"start\":34805},{\"end\":35306,\"start\":35296},{\"end\":35343,\"start\":35309},{\"end\":35556,\"start\":35550},{\"end\":36280,\"start\":36251},{\"end\":38157,\"start\":38115},{\"end\":38889,\"start\":38879},{\"end\":38971,\"start\":38961},{\"end\":39052,\"start\":39043},{\"end\":39138,\"start\":39129},{\"end\":39458,\"start\":39449},{\"end\":39510,\"start\":39501},{\"end\":40233,\"start\":40224},{\"end\":40616,\"start\":40607},{\"end\":40934,\"start\":40924},{\"end\":41360,\"start\":41350},{\"end\":41801,\"start\":41791},{\"end\":42445,\"start\":42435},{\"end\":42759,\"start\":42749},{\"end\":43182,\"start\":43172},{\"end\":43494,\"start\":43484},{\"end\":43914,\"start\":43904},{\"end\":44230,\"start\":44220},{\"end\":44654,\"start\":44644},{\"end\":44975,\"start\":44965},{\"end\":45404,\"start\":45394},{\"end\":45722,\"start\":45712}]", "table": "[{\"end\":39447,\"start\":39176},{\"end\":40222,\"start\":40204},{\"end\":40605,\"start\":40312},{\"end\":40922,\"start\":40648},{\"end\":41348,\"start\":41018},{\"end\":41789,\"start\":41515},{\"end\":42433,\"start\":42051},{\"end\":42747,\"start\":42472},{\"end\":43170,\"start\":42788},{\"end\":43482,\"start\":43206},{\"end\":43902,\"start\":43520},{\"end\":44218,\"start\":43942},{\"end\":44642,\"start\":44260},{\"end\":44963,\"start\":44687},{\"end\":45392,\"start\":45010},{\"end\":45710,\"start\":45434}]", "figure_caption": "[{\"end\":38522,\"start\":38255},{\"end\":38675,\"start\":38525},{\"end\":38877,\"start\":38678},{\"end\":38959,\"start\":38891},{\"end\":39041,\"start\":38973},{\"end\":39127,\"start\":39054},{\"end\":39176,\"start\":39140},{\"end\":39499,\"start\":39460},{\"end\":39593,\"start\":39512},{\"end\":40204,\"start\":39596},{\"end\":40312,\"start\":40235},{\"end\":40648,\"start\":40618},{\"end\":41018,\"start\":40937},{\"end\":41515,\"start\":41363},{\"end\":42051,\"start\":41804},{\"end\":42472,\"start\":42448},{\"end\":42788,\"start\":42762},{\"end\":43206,\"start\":43185},{\"end\":43520,\"start\":43497},{\"end\":43942,\"start\":43917},{\"end\":44260,\"start\":44233},{\"end\":44687,\"start\":44657},{\"end\":45010,\"start\":44978},{\"end\":45434,\"start\":45407},{\"end\":45754,\"start\":45725}]", "figure_ref": "[{\"end\":4361,\"start\":4353},{\"end\":14693,\"start\":14685},{\"end\":17191,\"start\":17183},{\"end\":26562,\"start\":26554},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30543,\"start\":30535},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31454,\"start\":31446}]", "bib_author_first_name": "[{\"end\":46268,\"start\":46265},{\"end\":46285,\"start\":46279},{\"end\":46299,\"start\":46294},{\"end\":46309,\"start\":46304},{\"end\":46324,\"start\":46319},{\"end\":46703,\"start\":46702},{\"end\":46717,\"start\":46711},{\"end\":46734,\"start\":46726},{\"end\":46752,\"start\":46742},{\"end\":47173,\"start\":47166},{\"end\":47190,\"start\":47183},{\"end\":47210,\"start\":47201},{\"end\":47231,\"start\":47224},{\"end\":47248,\"start\":47240},{\"end\":47260,\"start\":47254},{\"end\":47276,\"start\":47271},{\"end\":47291,\"start\":47284},{\"end\":47314,\"start\":47309},{\"end\":47336,\"start\":47328},{\"end\":47732,\"start\":47729},{\"end\":47748,\"start\":47740},{\"end\":47759,\"start\":47755},{\"end\":47774,\"start\":47767},{\"end\":47789,\"start\":47784},{\"end\":47791,\"start\":47790},{\"end\":47808,\"start\":47800},{\"end\":47825,\"start\":47819},{\"end\":47845,\"start\":47839},{\"end\":47859,\"start\":47853},{\"end\":47874,\"start\":47868},{\"end\":48303,\"start\":48300},{\"end\":48310,\"start\":48309},{\"end\":48326,\"start\":48319},{\"end\":48335,\"start\":48331},{\"end\":48354,\"start\":48343},{\"end\":48585,\"start\":48582},{\"end\":48600,\"start\":48596},{\"end\":48614,\"start\":48608},{\"end\":48628,\"start\":48624},{\"end\":48990,\"start\":48985},{\"end\":49007,\"start\":48999},{\"end\":49021,\"start\":49015},{\"end\":49035,\"start\":49027},{\"end\":49447,\"start\":49442},{\"end\":49462,\"start\":49457},{\"end\":49474,\"start\":49471},{\"end\":49842,\"start\":49837},{\"end\":49866,\"start\":49863},{\"end\":50247,\"start\":50243},{\"end\":50263,\"start\":50257},{\"end\":50273,\"start\":50269},{\"end\":50286,\"start\":50280},{\"end\":50293,\"start\":50292},{\"end\":50307,\"start\":50303},{\"end\":50611,\"start\":50606},{\"end\":50622,\"start\":50621},{\"end\":50639,\"start\":50630},{\"end\":50873,\"start\":50866},{\"end\":50886,\"start\":50883},{\"end\":50900,\"start\":50897},{\"end\":50916,\"start\":50910},{\"end\":50934,\"start\":50925},{\"end\":51395,\"start\":51391},{\"end\":51408,\"start\":51404},{\"end\":51422,\"start\":51418},{\"end\":51434,\"start\":51431},{\"end\":51449,\"start\":51443},{\"end\":51761,\"start\":51757},{\"end\":51774,\"start\":51770},{\"end\":51788,\"start\":51784},{\"end\":51800,\"start\":51797},{\"end\":51815,\"start\":51809},{\"end\":52150,\"start\":52146},{\"end\":52168,\"start\":52162},{\"end\":52185,\"start\":52179},{\"end\":52203,\"start\":52192},{\"end\":52234,\"start\":52227},{\"end\":52238,\"start\":52235},{\"end\":52254,\"start\":52246},{\"end\":52267,\"start\":52260},{\"end\":52276,\"start\":52268},{\"end\":52292,\"start\":52289},{\"end\":52673,\"start\":52667},{\"end\":52684,\"start\":52678},{\"end\":52693,\"start\":52691},{\"end\":52703,\"start\":52699},{\"end\":52714,\"start\":52710},{\"end\":52721,\"start\":52719},{\"end\":52736,\"start\":52729},{\"end\":52744,\"start\":52741},{\"end\":52754,\"start\":52751},{\"end\":53135,\"start\":53130},{\"end\":53149,\"start\":53144},{\"end\":53159,\"start\":53155},{\"end\":53163,\"start\":53160},{\"end\":53185,\"start\":53179},{\"end\":53200,\"start\":53197},{\"end\":53211,\"start\":53207},{\"end\":53227,\"start\":53222},{\"end\":53240,\"start\":53234},{\"end\":53671,\"start\":53664},{\"end\":53687,\"start\":53680},{\"end\":53700,\"start\":53695},{\"end\":53718,\"start\":53711},{\"end\":53965,\"start\":53959},{\"end\":53977,\"start\":53973},{\"end\":53981,\"start\":53978},{\"end\":53993,\"start\":53989},{\"end\":54014,\"start\":54006},{\"end\":54030,\"start\":54024},{\"end\":54321,\"start\":54315},{\"end\":54335,\"start\":54329},{\"end\":54353,\"start\":54349},{\"end\":54357,\"start\":54354},{\"end\":54372,\"start\":54365},{\"end\":54389,\"start\":54385},{\"end\":54408,\"start\":54402},{\"end\":54738,\"start\":54734},{\"end\":54753,\"start\":54748},{\"end\":54767,\"start\":54762},{\"end\":54783,\"start\":54776},{\"end\":54803,\"start\":54800},{\"end\":54816,\"start\":54812},{\"end\":54818,\"start\":54817},{\"end\":54833,\"start\":54830},{\"end\":55072,\"start\":55067},{\"end\":55088,\"start\":55082},{\"end\":55109,\"start\":55101},{\"end\":55125,\"start\":55121},{\"end\":55141,\"start\":55132},{\"end\":55154,\"start\":55148},{\"end\":55170,\"start\":55165},{\"end\":55186,\"start\":55181},{\"end\":55201,\"start\":55195},{\"end\":55692,\"start\":55687},{\"end\":55707,\"start\":55699},{\"end\":55716,\"start\":55714},{\"end\":55738,\"start\":55728},{\"end\":55752,\"start\":55744},{\"end\":55765,\"start\":55758},{\"end\":56093,\"start\":56089},{\"end\":56105,\"start\":56102},{\"end\":56122,\"start\":56113},{\"end\":56134,\"start\":56130},{\"end\":56147,\"start\":56142},{\"end\":56165,\"start\":56158},{\"end\":56180,\"start\":56174},{\"end\":56196,\"start\":56190},{\"end\":56209,\"start\":56202},{\"end\":56226,\"start\":56222},{\"end\":56652,\"start\":56648},{\"end\":56669,\"start\":56662},{\"end\":56679,\"start\":56674},{\"end\":56692,\"start\":56687},{\"end\":56704,\"start\":56699},{\"end\":56717,\"start\":56713},{\"end\":56999,\"start\":56994},{\"end\":57012,\"start\":57008},{\"end\":57026,\"start\":57022},{\"end\":57045,\"start\":57036},{\"end\":57057,\"start\":57051},{\"end\":57073,\"start\":57066},{\"end\":57087,\"start\":57082},{\"end\":57097,\"start\":57094},{\"end\":57107,\"start\":57102},{\"end\":57488,\"start\":57482},{\"end\":57504,\"start\":57500},{\"end\":57522,\"start\":57512},{\"end\":57537,\"start\":57532},{\"end\":57911,\"start\":57908},{\"end\":57922,\"start\":57919},{\"end\":58251,\"start\":58244},{\"end\":58259,\"start\":58258},{\"end\":58554,\"start\":58548},{\"end\":58568,\"start\":58564},{\"end\":58582,\"start\":58578},{\"end\":58596,\"start\":58591},{\"end\":58613,\"start\":58608},{\"end\":58626,\"start\":58621},{\"end\":58628,\"start\":58627},{\"end\":58642,\"start\":58636},{\"end\":58656,\"start\":58651},{\"end\":59047,\"start\":59040},{\"end\":59059,\"start\":59054},{\"end\":59073,\"start\":59066},{\"end\":59082,\"start\":59078},{\"end\":59428,\"start\":59421},{\"end\":59439,\"start\":59433},{\"end\":59451,\"start\":59445},{\"end\":59467,\"start\":59459},{\"end\":59479,\"start\":59472},{\"end\":59796,\"start\":59790},{\"end\":59806,\"start\":59802},{\"end\":59814,\"start\":59807},{\"end\":59832,\"start\":59826},{\"end\":59847,\"start\":59840},{\"end\":59860,\"start\":59852},{\"end\":59872,\"start\":59865},{\"end\":60136,\"start\":60131},{\"end\":60151,\"start\":60144},{\"end\":60165,\"start\":60160},{\"end\":60178,\"start\":60173},{\"end\":60192,\"start\":60188},{\"end\":60206,\"start\":60199},{\"end\":60224,\"start\":60213},{\"end\":60236,\"start\":60232},{\"end\":60247,\"start\":60243},{\"end\":60254,\"start\":60252},{\"end\":60590,\"start\":60583},{\"end\":60605,\"start\":60598},{\"end\":60613,\"start\":60610},{\"end\":60628,\"start\":60621},{\"end\":60644,\"start\":60637},{\"end\":60658,\"start\":60651},{\"end\":60670,\"start\":60666},{\"end\":60686,\"start\":60677},{\"end\":60698,\"start\":60691},{\"end\":60711,\"start\":60705},{\"end\":60713,\"start\":60712}]", "bib_author_last_name": "[{\"end\":46277,\"start\":46269},{\"end\":46292,\"start\":46286},{\"end\":46302,\"start\":46300},{\"end\":46317,\"start\":46310},{\"end\":46333,\"start\":46325},{\"end\":46709,\"start\":46704},{\"end\":46724,\"start\":46718},{\"end\":46740,\"start\":46735},{\"end\":46767,\"start\":46753},{\"end\":46779,\"start\":46769},{\"end\":47181,\"start\":47174},{\"end\":47199,\"start\":47191},{\"end\":47222,\"start\":47211},{\"end\":47238,\"start\":47232},{\"end\":47252,\"start\":47249},{\"end\":47269,\"start\":47261},{\"end\":47282,\"start\":47277},{\"end\":47307,\"start\":47292},{\"end\":47326,\"start\":47315},{\"end\":47343,\"start\":47337},{\"end\":47738,\"start\":47733},{\"end\":47753,\"start\":47749},{\"end\":47765,\"start\":47760},{\"end\":47782,\"start\":47775},{\"end\":47798,\"start\":47792},{\"end\":47817,\"start\":47809},{\"end\":47837,\"start\":47826},{\"end\":47851,\"start\":47846},{\"end\":47866,\"start\":47860},{\"end\":47881,\"start\":47875},{\"end\":48307,\"start\":48304},{\"end\":48317,\"start\":48311},{\"end\":48329,\"start\":48327},{\"end\":48341,\"start\":48336},{\"end\":48360,\"start\":48355},{\"end\":48364,\"start\":48362},{\"end\":48594,\"start\":48586},{\"end\":48606,\"start\":48601},{\"end\":48622,\"start\":48615},{\"end\":48640,\"start\":48629},{\"end\":48997,\"start\":48991},{\"end\":49013,\"start\":49008},{\"end\":49025,\"start\":49022},{\"end\":49045,\"start\":49036},{\"end\":49455,\"start\":49448},{\"end\":49469,\"start\":49463},{\"end\":49483,\"start\":49475},{\"end\":49850,\"start\":49843},{\"end\":49861,\"start\":49852},{\"end\":49872,\"start\":49867},{\"end\":49882,\"start\":49874},{\"end\":50255,\"start\":50248},{\"end\":50267,\"start\":50264},{\"end\":50278,\"start\":50274},{\"end\":50290,\"start\":50287},{\"end\":50301,\"start\":50294},{\"end\":50315,\"start\":50308},{\"end\":50324,\"start\":50317},{\"end\":50619,\"start\":50612},{\"end\":50628,\"start\":50623},{\"end\":50645,\"start\":50640},{\"end\":50652,\"start\":50647},{\"end\":50881,\"start\":50874},{\"end\":50895,\"start\":50887},{\"end\":50908,\"start\":50901},{\"end\":50923,\"start\":50917},{\"end\":50940,\"start\":50935},{\"end\":51402,\"start\":51396},{\"end\":51416,\"start\":51409},{\"end\":51429,\"start\":51423},{\"end\":51441,\"start\":51435},{\"end\":51456,\"start\":51450},{\"end\":51768,\"start\":51762},{\"end\":51782,\"start\":51775},{\"end\":51795,\"start\":51789},{\"end\":51807,\"start\":51801},{\"end\":51822,\"start\":51816},{\"end\":52160,\"start\":52151},{\"end\":52177,\"start\":52169},{\"end\":52190,\"start\":52186},{\"end\":52225,\"start\":52204},{\"end\":52244,\"start\":52239},{\"end\":52258,\"start\":52255},{\"end\":52287,\"start\":52277},{\"end\":52299,\"start\":52293},{\"end\":52676,\"start\":52674},{\"end\":52689,\"start\":52685},{\"end\":52697,\"start\":52694},{\"end\":52708,\"start\":52704},{\"end\":52717,\"start\":52715},{\"end\":52727,\"start\":52722},{\"end\":52739,\"start\":52737},{\"end\":52749,\"start\":52745},{\"end\":52757,\"start\":52755},{\"end\":53142,\"start\":53136},{\"end\":53153,\"start\":53150},{\"end\":53177,\"start\":53164},{\"end\":53195,\"start\":53186},{\"end\":53205,\"start\":53201},{\"end\":53220,\"start\":53212},{\"end\":53232,\"start\":53228},{\"end\":53252,\"start\":53241},{\"end\":53678,\"start\":53672},{\"end\":53693,\"start\":53688},{\"end\":53709,\"start\":53701},{\"end\":53725,\"start\":53719},{\"end\":53971,\"start\":53966},{\"end\":53987,\"start\":53982},{\"end\":54004,\"start\":53994},{\"end\":54022,\"start\":54015},{\"end\":54042,\"start\":54031},{\"end\":54327,\"start\":54322},{\"end\":54347,\"start\":54336},{\"end\":54363,\"start\":54358},{\"end\":54383,\"start\":54373},{\"end\":54400,\"start\":54390},{\"end\":54420,\"start\":54409},{\"end\":54746,\"start\":54739},{\"end\":54760,\"start\":54754},{\"end\":54774,\"start\":54768},{\"end\":54798,\"start\":54784},{\"end\":54810,\"start\":54804},{\"end\":54828,\"start\":54819},{\"end\":54843,\"start\":54834},{\"end\":55080,\"start\":55073},{\"end\":55099,\"start\":55089},{\"end\":55119,\"start\":55110},{\"end\":55130,\"start\":55126},{\"end\":55146,\"start\":55142},{\"end\":55163,\"start\":55155},{\"end\":55179,\"start\":55171},{\"end\":55193,\"start\":55187},{\"end\":55208,\"start\":55202},{\"end\":55219,\"start\":55210},{\"end\":55697,\"start\":55693},{\"end\":55712,\"start\":55708},{\"end\":55726,\"start\":55717},{\"end\":55742,\"start\":55739},{\"end\":55756,\"start\":55753},{\"end\":55769,\"start\":55766},{\"end\":56100,\"start\":56094},{\"end\":56111,\"start\":56106},{\"end\":56128,\"start\":56123},{\"end\":56140,\"start\":56135},{\"end\":56156,\"start\":56148},{\"end\":56172,\"start\":56166},{\"end\":56188,\"start\":56181},{\"end\":56200,\"start\":56197},{\"end\":56220,\"start\":56210},{\"end\":56233,\"start\":56227},{\"end\":56660,\"start\":56653},{\"end\":56672,\"start\":56670},{\"end\":56685,\"start\":56680},{\"end\":56697,\"start\":56693},{\"end\":56711,\"start\":56705},{\"end\":56727,\"start\":56718},{\"end\":57006,\"start\":57000},{\"end\":57020,\"start\":57013},{\"end\":57034,\"start\":57027},{\"end\":57049,\"start\":57046},{\"end\":57064,\"start\":57058},{\"end\":57080,\"start\":57074},{\"end\":57092,\"start\":57088},{\"end\":57100,\"start\":57098},{\"end\":57111,\"start\":57108},{\"end\":57498,\"start\":57489},{\"end\":57510,\"start\":57505},{\"end\":57530,\"start\":57523},{\"end\":57543,\"start\":57538},{\"end\":57917,\"start\":57912},{\"end\":57928,\"start\":57923},{\"end\":57938,\"start\":57930},{\"end\":58256,\"start\":58252},{\"end\":58267,\"start\":58260},{\"end\":58274,\"start\":58269},{\"end\":58562,\"start\":58555},{\"end\":58576,\"start\":58569},{\"end\":58589,\"start\":58583},{\"end\":58606,\"start\":58597},{\"end\":58619,\"start\":58614},{\"end\":58634,\"start\":58629},{\"end\":58649,\"start\":58643},{\"end\":58667,\"start\":58657},{\"end\":59052,\"start\":59048},{\"end\":59064,\"start\":59060},{\"end\":59076,\"start\":59074},{\"end\":59088,\"start\":59083},{\"end\":59431,\"start\":59429},{\"end\":59443,\"start\":59440},{\"end\":59457,\"start\":59452},{\"end\":59470,\"start\":59468},{\"end\":59482,\"start\":59480},{\"end\":59800,\"start\":59797},{\"end\":59824,\"start\":59815},{\"end\":59838,\"start\":59833},{\"end\":59850,\"start\":59848},{\"end\":59863,\"start\":59861},{\"end\":59875,\"start\":59873},{\"end\":60142,\"start\":60137},{\"end\":60158,\"start\":60152},{\"end\":60171,\"start\":60166},{\"end\":60186,\"start\":60179},{\"end\":60197,\"start\":60193},{\"end\":60211,\"start\":60207},{\"end\":60230,\"start\":60225},{\"end\":60241,\"start\":60237},{\"end\":60250,\"start\":60248},{\"end\":60267,\"start\":60255},{\"end\":60596,\"start\":60591},{\"end\":60608,\"start\":60606},{\"end\":60619,\"start\":60614},{\"end\":60635,\"start\":60629},{\"end\":60649,\"start\":60645},{\"end\":60664,\"start\":60659},{\"end\":60675,\"start\":60671},{\"end\":60689,\"start\":60687},{\"end\":60703,\"start\":60699},{\"end\":60722,\"start\":60714}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":46630,\"start\":46178},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":232040593},\"end\":47075,\"start\":46632},{\"attributes\":{\"doi\":\"arXiv:1806.00358\",\"id\":\"b2\"},\"end\":47688,\"start\":47077},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":48221,\"start\":47690},{\"attributes\":{\"doi\":\"arXiv:2205.14135\",\"id\":\"b4\"},\"end\":48580,\"start\":48223},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b5\"},\"end\":48901,\"start\":48582},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":49367,\"start\":48903},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":243861553},\"end\":49743,\"start\":49369},{\"attributes\":{\"doi\":\"arXiv:2208.11580\",\"id\":\"b8\"},\"end\":50168,\"start\":49745},{\"attributes\":{\"doi\":\"arXiv:2103.13630\",\"id\":\"b9\"},\"end\":50553,\"start\":50170},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":61815367},\"end\":50864,\"start\":50555},{\"attributes\":{\"doi\":\"arXiv:2102.00554\",\"id\":\"b11\"},\"end\":51296,\"start\":50866},{\"attributes\":{\"doi\":\"arXiv:2006.10518\",\"id\":\"b12\"},\"end\":51690,\"start\":51298},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":235825979},\"end\":52080,\"start\":51692},{\"attributes\":{\"id\":\"b14\"},\"end\":52585,\"start\":52082},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":231861390},\"end\":53068,\"start\":52587},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5151364},\"end\":53629,\"start\":53070},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b17\"},\"end\":53895,\"start\":53631},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":216056295},\"end\":54313,\"start\":53897},{\"attributes\":{\"doi\":\"arXiv:2106.08295\",\"id\":\"b19\"},\"end\":54693,\"start\":54315},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":208139197},\"end\":55065,\"start\":54695},{\"attributes\":{\"doi\":\"arXiv:1606.06031\",\"id\":\"b21\"},\"end\":55594,\"start\":55067},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b22\"},\"end\":56018,\"start\":55596},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":202786778},\"end\":56593,\"start\":56020},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":160025533},\"end\":56909,\"start\":56595},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":204838007},\"end\":57419,\"start\":56911},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":11816014},\"end\":57825,\"start\":57421},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":220364055},\"end\":58193,\"start\":57827},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1545102},\"end\":58519,\"start\":58195},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13756489},\"end\":58957,\"start\":58521},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221506623},\"end\":59343,\"start\":58959},{\"attributes\":{\"doi\":\"arXiv:2206.01859\",\"id\":\"b31\"},\"end\":59695,\"start\":59345},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b32\"},\"end\":60129,\"start\":59697},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b33\"},\"end\":60581,\"start\":60131},{\"attributes\":{\"doi\":\"arXiv:2201.12023\",\"id\":\"b34\"},\"end\":61104,\"start\":60583}]", "bib_title": "[{\"end\":46263,\"start\":46178},{\"end\":46700,\"start\":46632},{\"end\":47727,\"start\":47690},{\"end\":48983,\"start\":48903},{\"end\":49440,\"start\":49369},{\"end\":50604,\"start\":50555},{\"end\":51755,\"start\":51692},{\"end\":52665,\"start\":52587},{\"end\":53128,\"start\":53070},{\"end\":53957,\"start\":53897},{\"end\":54732,\"start\":54695},{\"end\":56087,\"start\":56020},{\"end\":56646,\"start\":56595},{\"end\":56992,\"start\":56911},{\"end\":57480,\"start\":57421},{\"end\":57906,\"start\":57827},{\"end\":58242,\"start\":58195},{\"end\":58546,\"start\":58521},{\"end\":59038,\"start\":58959}]", "bib_author": "[{\"end\":46279,\"start\":46265},{\"end\":46294,\"start\":46279},{\"end\":46304,\"start\":46294},{\"end\":46319,\"start\":46304},{\"end\":46335,\"start\":46319},{\"end\":46711,\"start\":46702},{\"end\":46726,\"start\":46711},{\"end\":46742,\"start\":46726},{\"end\":46769,\"start\":46742},{\"end\":46781,\"start\":46769},{\"end\":47183,\"start\":47166},{\"end\":47201,\"start\":47183},{\"end\":47224,\"start\":47201},{\"end\":47240,\"start\":47224},{\"end\":47254,\"start\":47240},{\"end\":47271,\"start\":47254},{\"end\":47284,\"start\":47271},{\"end\":47309,\"start\":47284},{\"end\":47328,\"start\":47309},{\"end\":47345,\"start\":47328},{\"end\":47740,\"start\":47729},{\"end\":47755,\"start\":47740},{\"end\":47767,\"start\":47755},{\"end\":47784,\"start\":47767},{\"end\":47800,\"start\":47784},{\"end\":47819,\"start\":47800},{\"end\":47839,\"start\":47819},{\"end\":47853,\"start\":47839},{\"end\":47868,\"start\":47853},{\"end\":47883,\"start\":47868},{\"end\":48309,\"start\":48300},{\"end\":48319,\"start\":48309},{\"end\":48331,\"start\":48319},{\"end\":48343,\"start\":48331},{\"end\":48362,\"start\":48343},{\"end\":48366,\"start\":48362},{\"end\":48596,\"start\":48582},{\"end\":48608,\"start\":48596},{\"end\":48624,\"start\":48608},{\"end\":48642,\"start\":48624},{\"end\":48999,\"start\":48985},{\"end\":49015,\"start\":48999},{\"end\":49027,\"start\":49015},{\"end\":49047,\"start\":49027},{\"end\":49457,\"start\":49442},{\"end\":49471,\"start\":49457},{\"end\":49485,\"start\":49471},{\"end\":49852,\"start\":49837},{\"end\":49863,\"start\":49852},{\"end\":49874,\"start\":49863},{\"end\":49884,\"start\":49874},{\"end\":50257,\"start\":50243},{\"end\":50269,\"start\":50257},{\"end\":50280,\"start\":50269},{\"end\":50292,\"start\":50280},{\"end\":50303,\"start\":50292},{\"end\":50317,\"start\":50303},{\"end\":50326,\"start\":50317},{\"end\":50621,\"start\":50606},{\"end\":50630,\"start\":50621},{\"end\":50647,\"start\":50630},{\"end\":50654,\"start\":50647},{\"end\":50883,\"start\":50866},{\"end\":50897,\"start\":50883},{\"end\":50910,\"start\":50897},{\"end\":50925,\"start\":50910},{\"end\":50942,\"start\":50925},{\"end\":51404,\"start\":51391},{\"end\":51418,\"start\":51404},{\"end\":51431,\"start\":51418},{\"end\":51443,\"start\":51431},{\"end\":51458,\"start\":51443},{\"end\":51770,\"start\":51757},{\"end\":51784,\"start\":51770},{\"end\":51797,\"start\":51784},{\"end\":51809,\"start\":51797},{\"end\":51824,\"start\":51809},{\"end\":52162,\"start\":52146},{\"end\":52179,\"start\":52162},{\"end\":52192,\"start\":52179},{\"end\":52227,\"start\":52192},{\"end\":52246,\"start\":52227},{\"end\":52260,\"start\":52246},{\"end\":52289,\"start\":52260},{\"end\":52301,\"start\":52289},{\"end\":52678,\"start\":52667},{\"end\":52691,\"start\":52678},{\"end\":52699,\"start\":52691},{\"end\":52710,\"start\":52699},{\"end\":52719,\"start\":52710},{\"end\":52729,\"start\":52719},{\"end\":52741,\"start\":52729},{\"end\":52751,\"start\":52741},{\"end\":52759,\"start\":52751},{\"end\":53144,\"start\":53130},{\"end\":53155,\"start\":53144},{\"end\":53179,\"start\":53155},{\"end\":53197,\"start\":53179},{\"end\":53207,\"start\":53197},{\"end\":53222,\"start\":53207},{\"end\":53234,\"start\":53222},{\"end\":53254,\"start\":53234},{\"end\":53680,\"start\":53664},{\"end\":53695,\"start\":53680},{\"end\":53711,\"start\":53695},{\"end\":53727,\"start\":53711},{\"end\":53973,\"start\":53959},{\"end\":53989,\"start\":53973},{\"end\":54006,\"start\":53989},{\"end\":54024,\"start\":54006},{\"end\":54044,\"start\":54024},{\"end\":54329,\"start\":54315},{\"end\":54349,\"start\":54329},{\"end\":54365,\"start\":54349},{\"end\":54385,\"start\":54365},{\"end\":54402,\"start\":54385},{\"end\":54422,\"start\":54402},{\"end\":54748,\"start\":54734},{\"end\":54762,\"start\":54748},{\"end\":54776,\"start\":54762},{\"end\":54800,\"start\":54776},{\"end\":54812,\"start\":54800},{\"end\":54830,\"start\":54812},{\"end\":54845,\"start\":54830},{\"end\":55082,\"start\":55067},{\"end\":55101,\"start\":55082},{\"end\":55121,\"start\":55101},{\"end\":55132,\"start\":55121},{\"end\":55148,\"start\":55132},{\"end\":55165,\"start\":55148},{\"end\":55181,\"start\":55165},{\"end\":55195,\"start\":55181},{\"end\":55210,\"start\":55195},{\"end\":55221,\"start\":55210},{\"end\":55699,\"start\":55687},{\"end\":55714,\"start\":55699},{\"end\":55728,\"start\":55714},{\"end\":55744,\"start\":55728},{\"end\":55758,\"start\":55744},{\"end\":55771,\"start\":55758},{\"end\":56102,\"start\":56089},{\"end\":56113,\"start\":56102},{\"end\":56130,\"start\":56113},{\"end\":56142,\"start\":56130},{\"end\":56158,\"start\":56142},{\"end\":56174,\"start\":56158},{\"end\":56190,\"start\":56174},{\"end\":56202,\"start\":56190},{\"end\":56222,\"start\":56202},{\"end\":56235,\"start\":56222},{\"end\":56662,\"start\":56648},{\"end\":56674,\"start\":56662},{\"end\":56687,\"start\":56674},{\"end\":56699,\"start\":56687},{\"end\":56713,\"start\":56699},{\"end\":56729,\"start\":56713},{\"end\":57008,\"start\":56994},{\"end\":57022,\"start\":57008},{\"end\":57036,\"start\":57022},{\"end\":57051,\"start\":57036},{\"end\":57066,\"start\":57051},{\"end\":57082,\"start\":57066},{\"end\":57094,\"start\":57082},{\"end\":57102,\"start\":57094},{\"end\":57113,\"start\":57102},{\"end\":57500,\"start\":57482},{\"end\":57512,\"start\":57500},{\"end\":57532,\"start\":57512},{\"end\":57545,\"start\":57532},{\"end\":57919,\"start\":57908},{\"end\":57930,\"start\":57919},{\"end\":57940,\"start\":57930},{\"end\":58258,\"start\":58244},{\"end\":58269,\"start\":58258},{\"end\":58276,\"start\":58269},{\"end\":58564,\"start\":58548},{\"end\":58578,\"start\":58564},{\"end\":58591,\"start\":58578},{\"end\":58608,\"start\":58591},{\"end\":58621,\"start\":58608},{\"end\":58636,\"start\":58621},{\"end\":58651,\"start\":58636},{\"end\":58669,\"start\":58651},{\"end\":59054,\"start\":59040},{\"end\":59066,\"start\":59054},{\"end\":59078,\"start\":59066},{\"end\":59090,\"start\":59078},{\"end\":59433,\"start\":59421},{\"end\":59445,\"start\":59433},{\"end\":59459,\"start\":59445},{\"end\":59472,\"start\":59459},{\"end\":59484,\"start\":59472},{\"end\":59802,\"start\":59790},{\"end\":59826,\"start\":59802},{\"end\":59840,\"start\":59826},{\"end\":59852,\"start\":59840},{\"end\":59865,\"start\":59852},{\"end\":59877,\"start\":59865},{\"end\":60144,\"start\":60131},{\"end\":60160,\"start\":60144},{\"end\":60173,\"start\":60160},{\"end\":60188,\"start\":60173},{\"end\":60199,\"start\":60188},{\"end\":60213,\"start\":60199},{\"end\":60232,\"start\":60213},{\"end\":60243,\"start\":60232},{\"end\":60252,\"start\":60243},{\"end\":60269,\"start\":60252},{\"end\":60598,\"start\":60583},{\"end\":60610,\"start\":60598},{\"end\":60621,\"start\":60610},{\"end\":60637,\"start\":60621},{\"end\":60651,\"start\":60637},{\"end\":60666,\"start\":60651},{\"end\":60677,\"start\":60666},{\"end\":60691,\"start\":60677},{\"end\":60705,\"start\":60691},{\"end\":60724,\"start\":60705}]", "bib_venue": "[{\"end\":46396,\"start\":46335},{\"end\":46846,\"start\":46781},{\"end\":47164,\"start\":47077},{\"end\":47944,\"start\":47883},{\"end\":48298,\"start\":48223},{\"end\":48711,\"start\":48658},{\"end\":49126,\"start\":49047},{\"end\":49546,\"start\":49485},{\"end\":49835,\"start\":49745},{\"end\":50241,\"start\":50170},{\"end\":50702,\"start\":50654},{\"end\":51059,\"start\":50958},{\"end\":51389,\"start\":51298},{\"end\":51875,\"start\":51824},{\"end\":52144,\"start\":52082},{\"end\":52817,\"start\":52759},{\"end\":53306,\"start\":53254},{\"end\":53662,\"start\":53631},{\"end\":54095,\"start\":54044},{\"end\":54482,\"start\":54438},{\"end\":54861,\"start\":54845},{\"end\":55309,\"start\":55237},{\"end\":55685,\"start\":55596},{\"end\":56296,\"start\":56235},{\"end\":56740,\"start\":56729},{\"end\":57149,\"start\":57113},{\"end\":57615,\"start\":57545},{\"end\":58001,\"start\":57940},{\"end\":58350,\"start\":58276},{\"end\":58730,\"start\":58669},{\"end\":59141,\"start\":59090},{\"end\":59419,\"start\":59345},{\"end\":59788,\"start\":59697},{\"end\":60329,\"start\":60285},{\"end\":60817,\"start\":60740},{\"end\":53330,\"start\":53308}]"}}}, "year": 2023, "month": 12, "day": 17}