{"id": 239890182, "updated": "2023-04-05 20:48:05.616", "metadata": {"title": "Scrooge: A Cost-Effective Deep Learning Inference System", "authors": "[{\"first\":\"Yitao\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Rajrup\",\"last\":\"Ghosh\",\"middle\":[]},{\"first\":\"Ramesh\",\"last\":\"Govindan\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the ACM Symposium on Cloud Computing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Advances in deep learning (DL) have prompted the development of cloud-hosted DL-based media applications that process video and audio streams in real-time. Such applications must satisfy throughput and latency objectives and adapt to novel types of dynamics, while incurring minimal cost. Scrooge, a system that provides media applications as a service, achieves these objectives by packing computations efficiently into GPU-equipped cloud VMs, using an optimization formulation to find the lowest cost VM allocations that meet the performance objectives, and rapidly reacting to variations in input complexity (e.g., changes in participants in a video). Experiments show that Scrooge can save serving cost by 16-32% (which translate to tens of thousands of dollars per year) relative to the state-of-the-art while achieving latency objectives for over 98% under dynamic workloads.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cloud/HuGG21", "doi": "10.1145/3472883.3486993"}}, "content": {"source": {"pdf_hash": "90b2c3b6a7189065fcba078eff5684176fdccce0", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3472883.3486993", "status": "BRONZE"}}, "grobid": {"id": "10fe15cb69eeb6d4568106ade42b7d47434aecaf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/90b2c3b6a7189065fcba078eff5684176fdccce0.txt", "contents": "\nScrooge: A Cost-Effective Deep Learning Inference System\n\n\nYitao Hu yitaoh@usc.edu \nRajrup Ghosh rajrupgh@usc.edu \nRamesh Govindan ramesh@usc.edu \n\nUniversity of Southern California\nUniversity of Southern\nCalifornia\n\n\nUniversity of Southern\nCalifornia\n\nScrooge: A Cost-Effective Deep Learning Inference System\n10.1145/3472883.3486993CCS Concepts \u2022 General and reference \u2192 Performance\u2022 Computing methodologies \u2192 Neural networksComputer visionNatural language processing\u2022 Software and its engi- neering \u2192 Scheduling Keywords Cloud computing, deep learning inference, auto-scaling\nAdvances in deep learning (DL) have prompted the development of cloud-hosted DL-based media applications that process video and audio streams in real-time. Such applications must satisfy throughput and latency objectives and adapt to novel types of dynamics, while incurring minimal cost. Scrooge, a system that provides media applications as a service, achieves these objectives by packing computations efficiently into GPU-equipped cloud VMs, using an optimization formulation to find the lowest cost VM allocations that meet the performance objectives, and rapidly reacting to variations in input complexity (e.g., changes in participants in a video). Experiments show that Scrooge can save serving cost by 16-32% (which translate to tens of thousands of dollars per year) relative to the state-of-the-art while achieving latency objectives for over 98% under dynamic workloads.\n Figure 1\n: Examples of media applications. The pink rectangles represent GPU modules, while the white ones represent CPU modules. The circled S denotes a stateful module.\n\nneural networks, RNNs [49]), or processing natural language (transformers [8,48]).\n\nInspired by these advances, application developers are starting to use DL models to build cloud-based applications that process video or audio streams in real-time [5,6,18,40,42,45,47,54]. Figure 1 lists several examples of such media processing applications (henceforth just applications). Prior work has often modeled these as acyclic data-flow graphs (following [18], we call these media DAGs or mDAGs) in which a vertices represent either a GPU-based DL model invocation or a CPU computation, and an edge represents data transfer between the corresponding vertices. The input to an application is a stream of audio or video, and a single application can invoke one or more DL models. For example, the traffic monitoring application [55] uses an object detection model to identify pedestrians and vehicles in video streams, then uses other models to count vehicles or recognize faces. The language translation application in [18] translates an audio stream from one language to another using three DL models in sequence: speech recognition, neural translation and speech synthesis.\n\nThis paper considers the design and implementation of a system, called Scrooge, that provides mDAGs as a service hosted on the cloud. Scrooge is a generalization of a cloudbased inference serving system [5,6,40,42,45,47,54], which applies DL models to client inputs (usually images and video). In Scrooge, a client sends a video or audio stream to an application mDAG hosted on the cloud; the mDAG processes the stream and returns the results to the client. Scrooge can host multiple mDAGs. It also multiplexes multiple client streams onto a single mDAG, such as when multiple clients concurrently request traffic monitoring.\n\nScrooge must satisfy two performance objectives of media processing. An end-to-end latency objective (or latency SLO) specifies a bound on the tail latency that the client can tolerate [5,6,42]. A rate objective ensures that the mDAG throughput matches the client's frame rate for video or audio. Goal and Approach. The primary goal of Scrooge is to minimize cost while satisfying these performance objectives. Cloud providers today provide a range of GPU-enabled virtual machine (VM) configurations, at different price points ( \u00a72). To minimize cost, the mDAG service provider must choose, at each instant, the set of VM configurations that achieves this goal for its current workload. To do this, Scrooge uses three related ways to minimize cost: it finds the lowest cost VM configuration that can meet the performance objectives; it packs as much computation as possible into a VM by leveraging additional parallelism, especially for GPUs, whenever possible; it continuously adapts resource usage to workload changes and input complexity (the semantic richness in the audio or video input). This ensures just-enough resource usage, thereby also minimizing cost. Contributions. In achieving minimum-cost mDAG serving, Scrooge makes the following contributions.\n\nScrooge develops more efficient methods to pack mDAGs onto VMs (specifically, into GPUs assigned to the VMs) than prior work. For DL-based workloads, prior work [5,42] relies on batching to improve throughput and GPU utilization. Batching can adversely impact latency so the efficacy of this technique depends upon the latency objective. Recent work [33,52] has explored spatial multiplexing, which seeks to exploit parallelism to improve throughput and GPU utilization. However, unless the degree of parallelism is chosen carefully, spatial multiplexing can actually degrade throughput and lower efficiency [18]. Scrooge uses a novel combination of batching and spatial multiplexing, by developing profiled models for each mDAG on each unique VM configuration. Each model represents the relationship between throughput and latency of that mDAG for that VM configuration, for different choices of batch sizes and the degree of parallelism.\n\nScrooge's second contribution is an optimization formulation that jointly, for a given workload, selects the lowest cost VM configuration while at the same time ensuring maximum packing efficiency. Unlike existing inference systems which either assume homogeneous hardware [42] or only consider one type of GPU [5,41], Scrooge can reason about heterogeneous VM configurations. It uses the profiled models as input to a mixed-integer linear programming (MILP) formulation to derive the minimum cost configuration. To scale the optimization, Scrooge's optimization formulation leverages the elasticity of the cloud and simply selects the best fit VM configuration, decoupling this choice from placement of an mDAG module on a VM instance.\n\nScrooge identifies, and adapts rapidly to, new sources of dynamics that prior work has not considered. A serving system must deal with changing resource requirements resulting from client arrivals and departures. However, the processing requirements for an mDAG can also depend on input complexity, in two ways. First, depending on the complexity of the input video or audio stream, the data transmitted on an mDAG edge can vary over time, resulting in varying resource requirements at the receiving. For example, in the traffic monitoring application (Figure 1), the object detector extracts a bounding box of each pedestrian, and the face detector is applied to each bounding box independently. So, the processing requirements for face detection can increase linearly with the number of pedestrians. Second, input complexity can also impact the latency of processing a frame using DL models. Most prior work has considered CNN-based model architectures, whose execution time is independent of input complexity. In more recent model architectures, like RNNs and transformers, execution latency can depend upon input complexity. Scrooge develops lightweight methods to track input complexity, and minimally adapts inputs to its optimization formulation to adapt resource allocation as necessary to meet mDAG requirements while minimizing cost. Evaluation Results. Using an implementation of Scrooge on a cluster of 16 GPUs ( \u00a74.1), we show that Scrooge is able to satisfy the end-to-end latency objective and rate objective under workload dynamics resulting from input complexity. To achieve comparable performance, Nexus [42], the state-ofthe art, incurs a serving cost 1.160 to 1.319 times of Scrooge's ( \u00a74.2), even as it satisfies the latency SLO of 8% fewer frames than Scrooge. To put these numbers in perspective, Scrooge's cost advantages can result in savings of tens of thousands of dollars over a year. Scrooge's MILP formulation incurs less than 100 ms latency and produces lower-cost allocations (by up to 5% in some cases) than a greedy heuristic. Its allocations are off the optimal by about 3%, but computing the optimal using a brute-force technique is intractable for the problem sizes we consider. Spatial multiplexing reduces Scrooge's serving cost by 2% to 58%.  \n\n\nMotivation\n\nThree design dimensions distinguish Scrooge from prior work ( \u00a75): cost-awareness, packing efficiency, and adaptation to input complexity. Scrooge must be cost-aware. Cloud providers today permit customers to choose from range of VM configurations. Table 1 shows the number of GPU-enabled VM configurations in three large cloud providers as of this writing: Google's GCP [12], Amazon's AWS [1] and Microsoft Azure [30]. Each provides 10-22 different GPU-enabled VM configurations, and the price ratio between the cheapest to the most expensive VM is 7-8. This ratio is not correlated with GPU resources; the ratio of highest to lowest GPU TFlops is 2.41. The cost of a VM is also not correlated with performance. Table 2 shows the price per hour for VMs with two different GPU types. It also depicts the throughput of two models for object detection: SSD-Inception [19], and YOLO [39]. The ratio of throughputs are different for different models, and also different from the cost ratios.\n\nTakeaway. Scrooge must explicitly provision resources based both on cost and performance (throughput and latency). Some existing inference systems either assume homogeneous hardware [42] or only consider one type of GPU [5,41]. Scrooge must pack DNN models efficiently to reduce cost. Prior work has considered two packing strategies: temporal multiplexing with batching, and spatial multiplexing.\n\nTemporal multiplexing with batching. Temporal multiplexing uses time-slicing to share the GPU. It runs only one DL model at each instant, and switches between DL models in a round-robin fashion. To increase resource utilization, temporal multiplexing is used with batching [20], which increases throughput by combining multiple inputs into a single request. Batching can reduce I/O requests, as well as potentially invoke larger GPU kernels for DL computation to better leverage GPU's parallel compute capability [6,11,42]. Usually, a larger batch size leads to a higher throughput, but can also lead to higher latency per batch. For example, when increasing the batch size from 1 to 8, the throughput for SSD-Inception [19]    img/sec, but the worst case latency also increases from 24 ms to 236 ms 1 . Therefore, existing inference systems [6,42] pick a proper batch size to maximize inference throughput, while satisfying the user's latency objectives. More generally, batching (a) exhibits diminishing returns with larger batch sizes and (b) utilizes the GPU in a bursty fashion. We are not the first to make these observations [33,52]. To illustrate these, consider Figure 2, which shows the impact of batching for SSD-Inception on the P100 GPU (measurements obtained using NVIDIA's profiling tool [32]).\n\nWith increasing batch sizes (Figure 2a), GPU utilization levels off (blue line), but beyond a relatively small batch size of 16, latency increases almost linearly (red line). The batch size at which these diminishing returns occur depends on the DL model structure and the implementation of GPU kernels. Figure 2b depicts GPU utilization across four consecutive DL model computations; GPU utilization is highly bursty . It reaches 100% for a short period of time, but remains below 20% for most of the time.\n\nSpatial multiplexing. To further increase throughput, recent work [18,33,52] has used spatial multiplexing, employing GPU CUDA streams [13] to run multiple DL jobs on the same GPU concurrently. Concurrent execution can potentially introduce interference, in which the throughput of one DL model is impacted by another. To avoid this, prior work requires the developer to specify which jobs should be spatially multiplexed [33], uses trial-and-error to minimize interference [52], or explicitly profiles model resource usage [18].\n\nTakeaway. One option that has not been explored in previous inference systems is a combination of batching and spatial multiplexing, which can pack models more efficiently, resulting in reduced cost. However, such an approach must simultaneously avoid increased latency due to batching and interference due to spatial multiplexing. Scrooge must adapt to input complexity. The semantic complexity in an audio or video stream can affect the compute latency of or the number of inputs to a node in an mDAG.\n\nIn an mDAG, a node that processes a video frame might emit a processed representation of the frame: e.g., bounding boxes of objects. Depending on input complexity, the average number of elements in this processed representation per frame (the scaling factor [42]) can vary significantly. Figure 3a shows the scaling factor for a traffic monitoring application [17,42] which includes three DL models: an object detector followed by a vehicle classifier or a human face detector. The scaling factor can vary significantly both within a video, and between videos. In Figure 3a, 1 has an average scaling factor of 8.94 -the object detector can detect close to 9 vehicles per frame.\n\n3's average scaling factor is 0.37. Moreover, 2's scaling factor ranges from 0 to 8 in a 200-frame window.\n\nPrevious inference systems [5,42] mainly focused on scheduling CNN models, whose execution time is constant for different inputs. More recent architectures like RNNs and transformers can have a varying execution time depending on input complexity. For example, Figure 3b shows the execution time for an RNN model for video captioning, S2VT [49], for three different input video streams. The execution time for frames from different videos, as well as different frames from the same video, vary significantly, ranging from 0.686 to 1.102 seconds. We call this phenomenon ballooning, and represent the inflation/deflation of execution time due to changes in input complexity by a ballooning factor ( \u00a73.2).\n\nTakeaway. Scrooge should explicitly track changing resource needs resulting from changes in input complexity; otherwise latency SLO may be violated significantly. For example, for 2, when the scaling factor changes from 0 to 8, if the vehicle classifier or the face detector is provisioned for a low scaling factor, its execution latency will increase, potentially violating end-to-end latency. Llama [41] reports a similar observation on input complexity by scaling factor, but it does not consider ballooning ( \u00a75). \n\n\nControl Flow Data Flow\n\nWorker-2 Figure 4: Scrooge overview.\n\n\nScrooge Design\n\nIn this section, we begin with an overview of Scrooge, followed by a detailed description of its components.\n\n\nOverview\n\nTerminology. Following prior work [18,40,42], Scrooge represents media processing applications with a data-flow graph called an mDAG. Each distinct media processing application has its own mDAG ( Figure 1). In an mDAG, a vertex (called a module) represents either a CPU computation (e.g., data preprocessing), or an invocation of a DL model on a GPU. An edge in the mDAG represents a data dependency. For instance, the video captioning mDAG includes 1 CPU module and 2 GPU modules: the GPU modules perform feature extraction and caption generation, and the CPU module guarantees the order of intermediate results, and maintains stateful variables 2 across frames. Scrooge runs on a cloud cluster, and each client sends a stream of video or audio data to be processed by an mDAG. Each stream has a fixed input rate, expressed in terms of video or audio frames [46] per second. As with other cloud services, Scrooge attempts to ensure a latency SLO for each frame processed by a given mDAG: this represents the latency incurred by the frame across the entire mDAG, but does not include propagation and transmission latency between the client and the cloud cluster.\n\nScrooge aggregates all streams destined to a given mDAG in a session. Thus, a session includes streams from different clients, each with potentially different input rates, but all with the same latency SLO. 3 Scrooge assumes that the session input rate from a given client is fixed, and each client can join or leave the session independently at any time. Therefore, the total ingress rate for a session (the sum of all the input rates) can vary over time. How Scrooge minimizes cost. For a given session with specific total ingress rate and latency SLO, the Scrooge coordinator schedules the session and allocates the lowest total cost computation resources (i.e., worker VMs) that can process the requests and satisfy the throughput and latency requirements ( Figure 4). The coordinator directs clients to send streams directly to workers assigned to the corresponding session. Workers collectively execute various mDAG modules, while respecting data dependencies, then return the results to the clients. Each Scrooge worker is equipped with multiple CPU cores and exactly one GPU.\n\nScrooge achieves cost-effectiveness as follows: \u25b6 It packs as much of DL processing into a GPU as possible by combining batching and spatial multiplexing ( \u00a72). To do this, for each mDAG, it builds performance profiles that characterize the throughput-vs-latency tradeoffs for different degrees of batching and spatial multiplexing ( \u00a73.2). \u25b6 Its coordinator solves an optimization formulation that attempts to determine, for each mDAG session, at each instant, the type (Table 1) of VM worker to allocate, such that overall cost is minimized without violating SLOs for any mDAG. Scrooge's optimization scales because it decouples resource allocation ( \u00a73.3) from instance placement ( \u00a73.4): there are relatively few VM types to search over ( \u00a72), and once it has determined the type of VM, Scrooge determines which VM instance to allocate. \u25b6 Its runtime closely tracks mDAGs to determine the impact of input complexity ( \u00a73.5). Specifically, it tracks node processing latency to determine ballooning. It also tracks mDAG edges to determine scaling factor changes ( \u00a72). It then rapidly adapts resource usage by re-applying the optimization formulation. This ensures that, at each instant, Scrooge uses just-enough resources (Scrooge includes other optimizations to reduce cost, such as reusing pre-warmed up workers when possible, \u00a73.6).\n\n\nEstimating Resource Usage\n\nProblem, Challenges and Approach. To reduce cost, Scrooge must efficiently pack mDAG modules into CPUs and GPUs. To do this, it needs to estimate each module's resource usage. Two challenges arise in doing this. First, the packing efficiency of GPU modules can be a function of both the degree of batching, and of spatial multiplexing ( \u00a72). Second, Scrooge must adapt to varying resource usage due to changes in input complexity (in addition to adapting to varying workload).\n\nIn general, there are two approaches to this problem: (a) proactively building performance models for each module through profiling, and (b) reactively adapting to variations in resource usage. A purely proactive approach is difficult, since that requires being able to accurately predict changes to input complexity. A purely reactive approach can result in unacceptable rates of latency SLO violations if the system is unable to increase a session's resource allocation quickly enough to meet demand.\n\nScrooge chooses a combination of these approaches. It profiles performance models for each GPU module that capture the relationship between throughput and latency for different degrees of batching and spatial multiplexing. At each instant it tracks: (a) the ingress rate for the session and (b) the scaling and ballooning factors for each module, and reactively adapts the session's resource allocations to match these. The Throughput-Latency Profile. To enable efficient packing, Scrooge derives empirical models (by profiling offline 4 ) of the relationship between throughput (work) and latency for a given mDAG module executing on a given worker (e.g., a given type of VM with a fixed set of resources). For a CPU module, we simply profile the average latency of executing the module on a single input.\n\nFor a GPU module, such as SSD-Inception, the relationship between throughput and latency depends on two factors ( \u00a72): the batch size, and the degree of spatial multiplexing (we use the term concurrency level to denote this). To derive models of this relationship, Scrooge profiles the throughput and latency achieved by each GPU module on each worker, for different choices of batch sizes and concurrency levels. Spatial multiplexing models on a GPU can result in interference.\n\nPrior work [18] shows that, with higher spatial multiplexing, throughput increases linearly then drops at the point when interference manifests. Scrooge conservatively assumes the maximum point on this as the capacity of the GPU. Figure 5 shows one sample profile: each curve represents a concurrency level, and each point represents a different batch size (doubling at each step from 1 to 128). In general, we can make two observations. First, at a given concurrency level, increasing the batch size increases throughput and latency up to a maximum, after which throughput drops but latency continues to increase. Thus, increasing the batch size from 1 to 8, its throughput increases from 40.98 img/sec to 67.52 img/sec. When the batch size is larger than 8, SSD-Inception invokes another type of GPU kernel, leading to a lower throughput. Second, higher concurrency levels can sustain higher maximum throughput, but at higher latency. For a batch size of 8, a concurrency level of 1 has a throughput of 67.52 img/sec, while a concurrency level of 4 (four batches of SSD-Inception with batch size of 8 run on the GPU concurrently) has a throughput of 77.51 img/sec. Scrooge's profiler obtains each point in the profile by averaging over several hundred batches. As such, these profiles cannot capture resource need variability resulting from variations in input complexity. Scrooge reacts to changes in input complexity as described later in \u00a73.6. We show in \u00a74 that the cost of profiling is very small, since these profiles can be reused across all clients of each mDAG. Scrooge uses this profile in its allocation algorithm \u00a73.3. Intuitively, that algorithm tries to search for a \"split\" of the latency SLO budget across mDAG modules such that the total cost of resources is minimized. The algorithm uses the profile as follows: if it allocates 0.2 seconds to the GPU module (the vertical line in Figure 5), then it uses the batch size and concurrency level point which has the highest throughput, but a latency lower than 0.2 sec. In Figure 5, this corresponds to a batch size of 4, and a concurrency level of 2. We now describe the allocation algorithm in more detail.\n\n\nAllocation\n\nOverview. Given an mDAG and its associated session (the aggregate client traffic for that mDAG), the primary challenge in Scrooge is to determine the minimum cost set of workers that can collectively satisfy the session's ingress rate, without violating the session's latency SLO. To tackle this challenge, Scrooge decouples allocation -determining the set of worker types needed for the mDAG from placementdetermining the actual worker instances. By narrowing the search space, this decoupling allows Scrooge to find the minimum cost allocation using an MILP formulation, rather than resorting to heuristics as in prior work [5,42]. \u00a74 shows that, with this decoupling, for practical problem sizes, allocation can usually complete in about 100 ms, even though the problem is known to be NP-hard [2]. In this section, we discuss the allocation formulation; \u00a73.4 discusses placement. Allocation: An Example. Before we present the allocation algorithm, we use a simple example to explain the intuition behind it, and to demonstrate how it uses throughput-latency profiles. Consider an mDAG with two modules \u2192 . For example might be an object detector, and might be a vehicle counting module. Table 3 and Table 4, respectively show the throughput-latency profiles for these two modules in tabular form, for two different worker types and . Thus, for example, module can sustain 60 fps on worker with a batch size of 4 and concurrency level of 2. For the same configuration -we use the term configuration to denote one Batch size Concurrency Latency Throughput Type  2  1  40  50  4  2  133  60  2  1  25  81  4  2  95  84   Table 3: Profile for module on worker type ($2 per hour) and worker type ($3 per hour). Latency is in milliseconds.\n\nchoice of batch size and concurrency level -module can achieve 84 fps at a lower latency on . Now, suppose Scrooge wishes to allocate resources to the mDAG to satisfy an end-to-end latency SLO of 300 ms. Suppose also that, at the current instant, the ingress rate to the mDAG is 80, and the scaling factor is 4.0. Thus, the input rate (the number of data items input per second to a module) for module is 80, while the input rate for module is 320. These, together with the two profiles, constitute the input to the allocation formulation.\n\nThe key idea behind the MILP solver is to try to split the 300 ms end-to-end latency between and , while minimizing cost and satisfying the throughput requirements. Consider, for example, an allocation in which module uses the < 4, 2 > configuration (batch size of 4, concurrency level of 2) on . This satisfies the input rate of 80 and incurs a latency of 145 ms (95 ms of processing and 50 ms delay to accumulate a batch of 4 frames). Then, can use 2 < 2, 1 > configured instances of to satisfy its input rate, resulting in a total latency of 0.171 s and a total cost of $9 (per hour). However, this allocation does not have the lowest cost! Instead, the cost-optimal allocation incurs a cost of $7.55 an hour at a total latency of 0.273 s (still within the SLO), as follows. For , it assigns two workers. First, it assigns an worker with a < 4, 2 > configuration to satisfy an input rate of 60. This is the maximum sustainable rate on the worker, and, since no other module can be scheduled on this worker, we call it a full allocation. Then, it assigns a worker with a < 2, 1 > configuration to handle the residual input rate of 20. Since this only partially utilizes the capacity of the worker (it can sustain a rate of 81), we call this a partial allocation. Finally, for it allocates two workers using a < 4, 2 > configuration; one of these is a full allocation, and the other is a partial allocation.\n\nThis example highlights a few insights. The solver trades off latency to lower cost, while still ensuring the latency  SLO. The optimal solution doesn't always select the cheaper worker type for every module; is cheaper than , but is not selected for . Finally, partial allocations can share a worker instance. For instance, both 's and 's partial allocations use worker type and can be placed on the same worker instance, avoiding resource fragmentation and thereby reducing cost. For this, Scrooge needs to assess worker capacity. Prior work [18] has demonstrated input rate proportionality: if a module has an input rate of , but the worker can sustain a maximum rate of , then the module uses / of the worker's capacity. Using this, we can see that 's partial instance of uses 20/81 of the capacity, while 's uses 120/200; so, those two can be co-located in one instance ( \u00a73.4). MILP Formulation. We now formally describe the allocation algorithm.\n\nInputs. The inputs to the allocation algorithm, summarized in Table 5, include (a) an mDAG , such that ( , ) denotes an edge between modules and , (b) the input rate for the -th module, (c) a set of throughput-latency profile entries , for the -th module on the -th worker type, where the -th entry maps a configuration < , , > to a profiled throughput and latency value ( is the batch size, the concurrency level, and is the batch execution time) 5 , (d) the cost of the -th worker type (e.g., as in Table 2), and (e) the latency SLO budget for the entire mDAG. Outputs. The algorithm has two outputs (Table 6). First, which worker types a module should use -the indicator variable , if set, denotes that module should use configuration on type . Second, how many workers it should use -the indicator variable , if set, denotes how many instances of configuration on type should use. For example, if is 4.6 for module on worker type , then has 4 full allocations and one partial allocation (at 0.6 capacity). Thus, is a measure of the cluster capacity allocated to (Table 6). These outputs are fed into the placement algorithm ( \u00a73.4).\n\nThe cost model. The allocation algorithm minimizes total cost for the entire mDAG, including both the CPU modules and the GPU modules as shown in Figure 1. In doing so, it assumes that the cost charged to a module is proportional to the capacity of the worker that it uses. In our example above, an instance of module is only charged 0.6 of the cost of worker type ( ) for partial allocation, while total cost of fully allocated instances is 4 .\n\nDecision Variables. In addition to the output variables, the formulation uses three other decision variables (Table 6). Throughput rate is non-zero only for decision = 1 and captures the part of the total input rate of module assigned 5 Each module can be either a CPU module or a GPU module as shown in Figure 1. For CPU modules, batch size is always 1.\n\nSymbol Description mDAG, ( , ) = 1 denotes an edge between and . \u210e profile < , , > for module on worker type . Cost of worker type . Input rate for module . Latency SLO budget for the mDAG. to workers of type using configuration . It is related to , as follows:\n= \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u00d7 , if = 1 0, otherwise(1)\nThe fraction in the denominator \u00d7 represents the rate at which a single worker can process inputs ( is the profiled execution time for a batch size ), so the complete term determines the number of workers allocated to of type at configuration .\n\nTo meet the latency SLO, the allocation algorithm attempts to track the critical path latency on the mDAG. It does this using two decision variables. for module is the latency along the longest path in the mDAG from source module to module .\n\nis the critical latency for the entire mDAG. Optimization formulation. An mDAG can be considered as partial order relations among the vertices represented by their edges [50]. If an edge goes from vertex to , an input can be processed by after . The allocation decision needs to find a worker type along with its configuration for each module, such that the longest latency for an input in an mDAG satisfy the SLO. We can formulate this scheduling problem as a mixed-integer linear programming (MILP) optimization satisfying linear constraints:\nmin \u00d7 | | =0 (2a) s.t. | | =0 \u00d7 = , \u2200 ,(2b)+ \u00d7 ( ) \u2264 , \u2200( , , ( , ) = 1), (2c) \u2264 (2d)\nIn this formulation ( ) is the worst-case latency, determined by the sum of the time to form a batch, and the profiled batch execution time . The batch creation time for a module is dependent on selected batch size and the maximum rate that a worker type can support. For example, the maximum rate supported by with configuration <4, 2, 200> (batch size = 4, concurrency level = 2 and batch execution Variables Description \u2208 {0, 1} Module use worker type . \u2208 R \u22650 Throughput rate for module on worker type . \u2208 R \u22650 Capacity allocated to module on worker type . \u2208 Z \u22650\n\nCritical latency for module in milliseconds. \u2208 Z \u22650 Critical latency for mDAG in milliseconds.  The optimization is subject to three constraints: (2b) the sum of the input rates for all module instances should equal total input rate for the module; (2c) if module immediately precedes module in an mDAG, the critical latency for module from the source should be greater than sum of critical latency of module from source and 's worst case latency; (2d) the critical latency of the entire mDAG, the sum of critical latency of the last node and it's worst case latency, must be within the latency SLO.\n\nDealing with partial allocations. When profiling a module with a configuration , the profile determines the highest throughput achievable for that configuration, and the corresponding latency. This latency includes the processing latency, and the queueing delay required to form a batch.\n\nIn a partial allocation, because the assigned input rate is lower, the queueing delay to wait for a batch to form is higher. Thus, the MILP output, if it has any partial allocations, when actually run on the cluster, may violate SLO. To avoid this, the allocation algorithm refines the configuration of the partial allocation in a second step. To do this, it replaces ( ) with\n( ) * = +(3)\nin 2c and re-runs the MILP for only this worker on this machine type, searching for a smaller batch size configuration that will preserve the latency SLO. However, this makes 2c a non-convex constraint because we use an inverse of a decision variable. The Gurobi [29] solver internally translates the constraint to linear approximations. For some inputs, the solver doesn't converge (but returns quickly when it determines it is infeasible); in these cases, Scrooge reverts to a greedy search for the best configuration for the partial allocation. Partial allocations result is slightly sub-optimal allocations ( \u00a74).\n\n\nPlacement\n\nFor the -th module in an mDAG, the allocation algorithm determines that it should use worker instances. For each instance, the algorithm determines (a) the type of worker and (b) the configuration to use. In general, \u2212 1 of these instances will be full -the module requires the entire capacity of the worker, and no other computation can run on that instance -and one other worker may be partially allocated.\n\nFinding a complete instance. Scrooge is able to decouple placement from allocation because of elasticity of cloud services; it assumes that it can (almost always) find an unused instance of a given type of worker to allocate to the module. When it cannot find a worker of type , it simply re-runs the allocation algorithm without that worker type.\n\nFinding a partial instance. An mDAG service will serve a number of different mDAGs, and each mDAG's modules will likely occupy a worker partially. The residual capacity of a partial worker is determined by input rate proportionality ( \u00a73.1). To accommodate a partial instance, Scrooge searches for the best-fit residual capacity. If it cannot find such a worker, it requests a free instance from the cloud service, and allocates that worker to the module; the remaining capacity can be allocated to another module from the same mDAG, or another mDAG.\n\nWhen are allocation and placement invoked? The Scrooge coordinator invokes allocation and placement when (a) a client leaves or joins and (b) when input complexity changes significantly. We discuss the latter below.\n\nRe-routing client streams. If, as a result of these changes, the allocation changes and instances have to be allocated/deallocated, the coordinator must re-route client streams to different worker instances (and possibly migrate state). We use mechanisms for these inspired by the literature on network function virtualization (NFV [25,36,51]) and omit the details of these mechanisms for brevity.\n\nDealing with worker failure. The coordinator uses the same mechanisms for client re-routing and state migration when a worker instance fails (this assumes that modules use resilient underlying cloud storage).\n\n\nReacting to Input Complexity Variation\n\nInput complexity can either (a) increase the input rate to a module, or (b) inflate the execution time of a module.\n\nMonitoring. Each Scrooge worker continuously monitors (a) the average execution time of the module (to determine ballooning \u00a72), and (b) the average input rate to the module (to determine the scaling factor). When either of these changes significantly, the worker notifies the coordinator, which invokes the allocation and (possibly) placement algorithms.\n\nInvoking the allocation algorithm. When the input rate to the -th module changes, the coordinator simply invokes the allocation algorithm for the mDAG, with the new observed input rate (Table 5). When the execution time of the module increases, the coordinator re-runs the allocation algorithm, but with the observed execution latency, instead of the profiled execution latency. 67\n\n\nOther Cost Optimizations\n\nWorker reuse and turn-off delay. Popular serving frameworks [45,47] use just-in-time (JIT) compilation [14] to optimize DNN execution, so the first few frames can incur a significantly longer latency. To minimize JIT's impact on the latency SLO, Scrooge's placement always re-uses a worker that has already loaded the module, when possible. To maximize worker reuse, Scrooge places an unused worker in a warmup state for a configurable interval (called the turn-off delay) before returning the worker to the cloud service. In \u00a74, we show that this optimization can reduce the rate of latency SLO violations. Module pipelining. A Scrooge's GPU module requires some CPU operations for serializing input and output, resizing the inputs etc. Scrooge automatically pipelines CPU operations with GPU computation to maximize throughput, and to minimize end-to-end latency. Early dropping. When the workload increases, inputs wait in a module's input queue until the coordinator increases resource allocation for the module. Before executing each queued input, the Scrooge worker uses the profiled (or recently observed) execution latency to determine if this input would violate the latency SLO given that it may have incurred queueing delay. If so, the worker drops the input without invoking the module on the input.\n\n\nEvaluation\n\nWe compare Scrooge against the state-of-the-art inference system, Nexus [42], then quantify the importance of various design decisions.\n\n\nMethodology\n\nImplementation. We have implemented all the features of Scrooge described in \u00a73. Each Scrooge worker runs in a separate container, and uses TF-Serving [45] to serve DL models 6 In theory, Scrooge can overreact if input changes significantly every frame. In practice, we expect input complexity to vary at human activity timescales of a few seconds (Figure 3a), not on every frame. Scrooge tracks execution time and input rate changes over sub-second windows, so it can avoid overreacting. 7 More precisely, let's say that, for the selected batch size and concurrency level, the profiled execution time was and the observed execution time is . Then, Scrooge inflates all profiled execution latencies by the ballooning factor / and then re-runs the allocation algorithm. on the worker's dedicated GPU. DL models are either written in Tensorflow [44] or converted 8 from other languages like PyTorch [37] or MXNet [4] using tools like ONNX [35]. Scrooge has 14, 930 lines of Python code, which includes the client, coordinator and worker implementations. The mDAG library is an additional 8, 617 lines of Python code. Testbed. We deployed Scrooge on a cluster containing 16 VMs with a total of 16 GPUs (8 P100 GPUs and 8 V100 GPUs, each with 6 vCPUs, 112 GB RAM and a 128 GB SSD.) from Microsoft Azure [30]. We deliberately designed the testbed to demonstrate Scrooge's capability to minimize the total serving cost across heterogeneous workers. Comparison Alternative. 9 Nexus [42] is a DL serving system that schedules DL execution in a GPU cluster, while satisfying client's latency SLO. Nexus profiles DL latency under different batch sizes, and derives the maximum batch size that can satisfy the latency SLO to maximize throughput. It monitors incoming workload, and adjusts scheduling and placement decisions accordingly. Workloads. We use all seven mDAGs in Figure 1 in our experiments. Traffic monitoring (traffic) [55] uses SSD-Inception [19] to identify pedestrians and vehicles in video streams, then uses other models to count vehicles or recognize faces. Avatar extraction (face) uses a face detector and PRNet [10] to extract facial keypoints. Person tracking (tracker) detects pedestrians in video streams and then uses Triplet [15] for person re-identification across frames. Human pose extraction (pose) uses OpenPose [3] to detect human keypoints and then uses an action recognition model [38] to recognize human poses. Video captioning (caption) [49] uses S2VT to generate text description of video streams by analyzing the output of the fully-connected layer from AlexNet [26]. Language translation (audio) [18] translates audio script from English to German. Complex activity detection (actdet) [28] detects human activity across cameras.\n\nThe video and audio streams we use in our evaluations comes from public datasets or the application developer [9,28,38,42,49]. Our comparisons ( \u00a74.2) only use three of these mDAGs (traffic, face and tracker), because the other four (pose, caption, audio and actdet) either require stateful modules or take audio streams as input, neither of which Nexus supports. We use all seven mDAGs in other evaluations ( \u00a74.4). Metrics. We focus on two metrics: (1) The cost to serve the media application charged by the cloud provider. For example, for the pricing shown in Table 2 (b) Figure 6: (a) The total cost for traffic, face and tracker; (b) The finish rate for traffic, face and tracker. For face and tracker, Nexus's total cost is lower than Scrooge's; however, its finish rate is less than 1/3 of Scrooge's. When finish rates are comparable, Nexus's total cost is 16-32% higher than that of Scrooge.\n\nand one V100 machine are used, the total cost is $7.20/hr.\n\n(2) The finish rate for each session in the cluster, defined as the ratio between the number of frames that satisfy the latency SLO and the number of frames sent during a one second window. For example, for a time window of [ , + 1), if Scrooge receives 20 frames, but processes only 19 of them within the latency SLO, the finish rate for [ , + 1) is 95%. The former measures resource efficiency, while the latter measures the ability of the system to adapt to dynamics. A good DL serving system should have a high finish rate with a low total cost.\n\n\nComparison Results\n\nExperiment details. To demonstrate Scrooge's capability to minimize the cost and to achieve high finish rate under dynamic workload, and to compare it with Nexus, we generate three workloads for traffic, face and tracker. Each workload lasts for 1000 seconds and has 12 clients, with clients joining and leaving at random times. Each client stream for traffic has a fixed frame rate between 2 and 5, for face between 12 and 30, and for tracker between 15 and 40. The highest ingress rate for each mDAG saturates our 16 node cluster, hence the choice of frame rates. All three workloads have a latency SLO of 400 ms to match Nexus evaluations.\n\nResults. Figure 6 summarizes the results of the comparison; we discuss results for each mDAG separately. Traffic. Scrooge outperforms Nexus for this mDAG. Nexus's end-to-end cost is 1.218 times of Scrooge's and its finish rate is 86.3% while Scrooge's finish rate is 97.7%. Figure 7 shows the time evolution of cost and finish rate during the 1000-sec window. The top panel shows the input rate for traffic's modules in the second layer, which varies significantly with time, illustrating variability due to changes in input complexity and client dynamics. (Figure 3(a) more directly quantifies variability in input complexity; we use those videos in our experiments). The middle panel shows the real-time cost for Nexus and Scrooge; Nexus's cost is higher than Scrooge for most of the experiment. The bottom panel shows the real-time finish rate; Nexus exhibits significant SLO violations. Scrooge outperforms Nexus in both total cost and finish rate for three major reasons. First, on GPU modules, Nexus uses batching only, while Scrooge uses spatial multiplexing and batching to further increase the maximum throughput each machine can support, leading to a lower total cost. In theory, Scrooge should, at every instant of time, have a higher cost than that of Nexus. In practice, due to experimental variability, the ingress rates at each instant for each system are not identical; Scrooge's ingress rates are slightly higher sometimes, resulting in occasionally higher cost Figure 7. \u00a74.4 explores the impact of Scrooge's spatial multiplexing in greater detail.\n\nSecond, Scrooge decouples placement from allocation, so can derive a near-optimal solution using the Gurobi solver, while Nexus achieves scheduling and placement using a greedy heuristic, leading to potentially sub-optimal solutions. In \u00a74.4, we quantify the optimality and execution time of Scrooge's decoupling strategy.\n\nThird, our experimental testbed is deliberately heterogeneous ( \u00a74.1). Scrooge's allocation algorithm takes workers heterogeneity into account, while Nexus's scheduling algorithm implicitly assumes homogeneous hardware.\n\nFace. As shown in Figure 6, for face, Nexus's end-to-end cost is only 0.456 times of Scrooge's. However, Nexus's finish rate was 30.6% -nearly 70% of the requests to Nexus either violated the latency SLO or were never received by the client for the final output. Meanwhile, Scrooge's 98.2% finish rate is much higher than Nexus's.\n\nIn addition to the reasons we mentioned above, Scrooge outperforms Nexus for face because Scrooge profiles and provisions resources for both CPU and GPU modules, while Nexus focuses on GPU modules and executes CPU-based computation 10 in its front-end [42]. However, for face, the CPU module can be compute-intensive. In our evaluation, Nexus allocated only one front-end for face's CPU module, which resulted in long queues at front-end leading to increased latency SLO violations.\n\nTo verify this hypothesis, we manually increased the number of front-ends for Nexus from 1 to 3 then 6, denoted as _ _3 and _ _6 . Figure 6 and Figure 8 shows the overall total cost and finish rate, as well as the real-time cost and finish rate for _ _3 and _ _6 . By increasing the number of front-ends from 1 to 3, Nexus's finish rate increased from 30.6% to 75.7%, suggesting that 3 front-ends aren't enough for this workload. Nexus requires a minimum of 6 frontends to avoid queuing for the CPU module in face. Thus, _ _6 always uses 6 workers 11 during the 1000-sec window. With 6 frontends, Nexus's finish rate is 90.5%, and its total cost is 1.319 times of Scrooge's. Thus, for face, Nexus requires significant trial-and-error to determine the optimal number of front-ends, and even then has lower finish rate and higher cost than Scrooge.\n\nTracker. Like face, tracker also has a CPU module to extract image patches from the object detection results. With a single front-end, Nexus's cost is only 0.740 times of Scrooge's, but its finish rate is 31.9%. Therefore, we manually increased Nexus's number of frontends for tracker's CPU module from 1 to 3 then 4, denoted as _ _3 and _ _4 , as shown in Figure 6 and Figure 9. Nexus requires at least 4 frontends to avoid queuing at the front-ends. With these, Nexus's finish rate is 97.6%, and its total cost is 1.160 times Scrooge's.\n\nSummary. Nexus' cost is 16-32% higher than that of Scrooge. While this may seem modest, this translates to significant cost savings for a long-lived inference system. Consider the traffic mDAG in Figure 6. During our approximately 15 minute run with just 12 clients, the cost 10 In the Nexus architecture, front-ends receive requests from clients and perform any necessary CPU processing, while backends serve DL models. 11 For CPU modules, Scrooge and Nexus is charged at the rate of a non-GPU enabled worker, $0.83/h. savings from Scrooge was $0.6. Over a year, even with this modest workload, the total cost savings for a cluster of 16 VMs running the traffic mDAG is over $21K! At larger scales (\u223c1000 VMs), Scrooge can theoretically save over $1M over a year. Thus, we argue that careful, cost-effective resource allocation is especially important for DL-based media processing workloads.\n\n\nThe Importance of Cost-Aware Allocation\n\nMILP-based allocation is essential. We now explore whether, instead of using MILP: (a) we could have searched brute-force for the optimal allocation and (b) we could have used a fast greedy heuristic. Figure 10 plots the distribution of the cost of the allocations generated by Scrooge, relative to these two other approaches. This distribution is over all the allocation decisions we make for the experiments in \u00a74.2. This figure, whose y-axis is truncated for clarity, shows that Scrooge generates the same cost configurations as the brute-force optimal about 85% of the time. However, it deviates from the optimal by less than 3% in the rest of the cases. This is due to partial allocations. In a partial allocation, our algorithm re-runs the MILP formulation with a non-convex constraint, relying on the underlying solver to use approximations to converge, and falling back to a greedy heuristic when the solver cannot converge ( \u00a73.3). This is the sole cause for the sub-optimality (we have verified this by inspection). However, brute force is clearly infeasible: brute-force can take over 10 s to compute the optimal solution, while MILP always converges in less than 100 ms, and often faster ( Figure 11).\n\nOur experiments only use 2 VM types mainly due to budget limits; however, even with 30 VM types -more than the number of VM types in Table 1 -our MILP completes within 130 ms across all our chains (line labeled Scrooge-30 in Figure 11). At this scale, brute force is unable to determine an allocation on our server. Figure 10 also depicts the performance of a greedy heuristic, which selects the module (and associated worker type and configuration) whose ratio of throughput gain to the product of cost and latency increase is highest. This runs faster than MILP of course (Figure 11), and produces solutions with identical cost 80% of the time. However, as Figure 10 shows, it incurs sub-optimal costs (up to 5% higher in some cases) for the rest of the decisions.\n\nFor this reason, Scrooge uses MILP-based allocation; it is tractable in our setting, and provides lower cost allocations than our greedy heuristic. Less stringent SLOs lead to lower cost allocations. To evaluate the impact of latency SLO on total cost, we run Scrooge under 4 different latency SLOs. Figure 12 shows the normalized cost of Scrooge under different latency SLOs. In Figure 12a, when the latency SLO increases from 400 ms to 1000 ms, the total cost reduced up to 18.14% for traffic mDAG, 4.63% for face and 25.59% for tracker. In Figure 12b, for the other four mDAGs, a larger latency SLO can reduce the total cost up to 39.20%, 6.29%, 4.87% and 12.54% respectively.\n\nA larger latency SLO increases the latency budget of each module in the mDAG, and Scrooge can more aggressively batch and/or spatially multiplex to increase throughput when latency budget is larger ( Figure 5). Specifically, for traffic mDAG, when increasing latency SLO from 400 ms to 1000 ms, Inception's average batch size increased from 1.91 to 7.78, its average concurrency level decreased from 2.77 to 2.06 12 , and its average throughput increased from 24.9 to 33.6.\n\nNot all mDAGs benefit equally from relaxing the latency SLO; the gains depend on the design of DL models. For example, by increasing latency SLO from 400 ms to 1000 ms, the tracker mDAG can reduce 25.59% of cost, while the face mDAG only benefits by 4.63% of cost. Figure 13b explains why -the average throughput of Mobilenet from the tracker mDAG increased from 27.9 to 38.2, while the average throughput of PRNet from the face mDAG only increased from 239.9 to 240.8. 12 A larger latency budget might decrease module's batch size or concurrency level, but its throughput will always increase.   \n\n\nJustifying Design Decisions\n\nSpatial multiplexing is crucial. We now compare Scrooge against Scrooge-no-spatial (Scrooge-ns), which uses a fixed concurrency level of 1. Figure 13a shows the cost ratio between Scrooge-ns and Scrooge under various latency SLO for three mDAGs used in \u00a74.2. For traffic, the cost ratio ranges from 1.17 to 1.32 (14.48%-24.36% savings). For face and tracker, cost savings ranges are 57.90%-58.28% and 29.16%-39.10% respectively. Figure 13b explains the impact of relaxing the SLO budget. For traffic's Inception and tracker's Mobilenet, throughput gains are evident in Scrooge, but less so in Scrooge-ns. This suggests that spatial multiplexing is essential for exploiting relaxed SLOs.\n\nThe other four chains (pose, caption, audio, and actdet) confirm these observations (omitted for brevity). Of these, caption shows the least cost reductions from relaxed SLOs; its RNN-based S2VT model only accepts a batch size of 1. Profiling costs are negligible. Scrooge explicitly chose to profile mDAGs offline. Profiling enables it to get good initial estimates of mDAG resource needs. For the mDAGs we used in this paper, Scrooge's profiling cost was $7.67. These profiles can be used in perpetuity, unless the mDAGs change.\n\nTo put this number in content, the cost of running the traffic mDAG for just one day is $267. Turnoff delay can increase finish rate. In this experiment, we quantify turnoff delay's influence on finish rate for each mDAG ( \u00a74.3). Figure 14a shows the finish rate for traffic, face and tracker under three turnoff delay values: 0, 30 and 120 sec. When there is no turnoff delay (blue bars in Figure 14a), Scrooge's finish rate ranges from 91.9% to 93.8%. This is due to its input complexity; when the scaling factor increases sharply, the finish rate drops until Scrooge is able to provision additional resources. It can take up to 5 seconds [14,18] to load the model weight into GPU memory and finish just-in-time compilation. Turn-off delay avoids this overhead thereby reducing impact on finish rate: a turnoff delay of 30 seconds can increase the finish rate to over 97%, while a turnoff delay of 120 seconds can further increase to over 98%.  [42] VM no video heuristic scaling batching no Clipper [6] VM no video and audio no no any no Rim [18] VM/Edge yes video and audio no no spatial yes Triton [34] VM no video and audio no no any yes InferLine [5] VM no video heuristic scaling batching yes INFaaS [40] VM no video heuristic scaling batching no OoO [23] VM no video no no spatial no MArk [54] VM and FaaS no video and audio heuristic scaling batching no Llama [41] FaaS no video heuristic scaling batching yes   Figure 14b shows the finish rate for the other four mDAGs from Figure 1 under three turnoff delay values. For pose, Scrooge achieves high finish rate even with a turnoff delay of 0 sec. This is because pose's resource requirements are not significantly affected by input complexity. For caption, audio and actdet, a turnoff delay of 30 seconds can increase the finish rate to over 96%, while a turnoff delay of 120 seconds can further increase to over 97%.\n\n\nRelated Work\n\nDNN Inference Systems. DL model serving has been explored a fair bit in recent years, but Scrooge occupies a unique point in the design space. It differs from Nexus [42] in supporting CPU modules, heterogeneous hardware, using spatial multiplexing and dealing with ballooning exhibited by newer model architectures. Inferline [5] and INFaaS [40] only supports a single GPU type and uses batching. MArk [54] focuses on provisioning GPU modules only and uses heuristic methods to make resource allocation decisions. Moreover, InFaaS and MArk do not support module DAGs. Clipper [6] does not adapt resource allocation to input complexity changes, and does not use spatial multiplexing. Rim [18] also does not adapt to input complexity changes, and performs heuristic allocation (but uses spatial multiplexing and supports heterogeneous GPUs). Triton [33], OoO [23] and Space-Time [22] combine batching and spatial multiplexing, but do not adapt to input complexity changes and do not explicitly seek to optimize cost. Llama [41] supports inference on FaaS platforms, which can have high latency. It supports scaling factor adaptation, but not ballooning factor adaptation. DL Training Systems. Scrooge is also inspired by DL training systems. Gandiva [52] identified the importance of spatial multiplexing for training, Gandiva the importance of supporting heterogeneous GPU hardware for training and DS2 [24] for reactively scaling stream processing in response to changes in processing time. Dataflow Systems. Prior work has developed dataflow languages and run-time support [7,21,31,53] for cloud-based applications. As in Scrooge, these approaches represent applications as DAGs, and automatically manage resources and schedule computations. However, these systems focus on throughput for general dataflow workloads, while Scrooge is specialized for DL inference and minimizes the total serving cost while ensuring latency and finish rate constraints.\n\n\nConclusions\n\nIn the paper, we presented a cost-efficient deep learning inference system named Scrooge, which packs DL workload efficiently to maximize inference throughput while satisfying DL application's latency and rate objectives. Scrooge decouples placement from allocation to derive the most costefficient scheduling for most cases, and continuously adapts resource allocation to workload changes, which ensures system performance under dynamic workload. Experiments show that Scrooge can save 16 \u2212 32% of serving cost over the state-of-art while achieving latency objectives for over 98%.\n\nFigure 2 :\n2(a) Execution time (blue line) and average GPU utilization (red line) for batch size from 1 to 128; (b) GPU utilization for four consecutive requests.\n\nFigure 3 :\n3(a) Number of vehicle classifier invocation per frame; (b) Execution time per frame.\n\nFigure 5 :\n5The throughput-latency profile for a GPU module. Each curve represents a different concurrency level. Each point corresponds to a different batch size, ranging from 1 to 128 (each batch is twice the previous).\n\n\ntime = 200 ms) for is 4 \u00d7 2/0.2 = 40 img/sec. It takes 4/40 = 100 ms to create a batch of size 4. Thus, the worstcase latency for module on this worker is 100 + 200 = 400 ms.\n\nFigure 7 :Figure 8 :\n78The cost and finish rate on workload of traffic for Scrooge and Nexus. The cost and finish rate on face for Scrooge and Nexus.\n\nFigure 9 :\n9The cost and finish rate on workload of tracker for Scrooge and Nexus.\n\nFigure 10 :Figure 11 :\n1011The cost ratio of Scrooge over optimal cost derived from brute force. Latency distribution for allocation decisions\n\nFigure 12 :\n12(a) The normalized cost of Scrooge under different latency SLOs for traffic, face and tracker; (b) The normalized cost of Scrooge under different latency SLOs for pose, caption, audio and actdet.\n\nFigure 13 :\n13(a) The cost ratio of Scrooge-ns over Scrooge under various latency SLO for traffic, face and tracker; (b) The average throughput for Inception, PRNet and Mobilenet under various latency SLO.\n\nFigure 14 :\n14(a) The finish rate of Scrooge under different turnoff delay for traffic, face and tracker; (b) The finish rate of Scrooge under different turnoff delay for pose, caption, audio and actdet.\n\nTable 1 :\n1Comparison between cloud providers.\n\n\nincreases from 40.98 img/sec to 67.52P100 V100 Ratio \nVM Pricing ($/h) \n2.07 \n3.06 \n1.48 \nSSD-Inception [19] throughput 14.69 16.31 \n1.11 \nYOLO [39] throughput \n22.17 26.72 \n1.21 \n\n\n\nTable 2 :\n2Pricing, hardware information and throughput for Microsoft Azure virtual machine with GPUs.\n\nTable 4 :\n4Profile for module on worker type ($2 per \nhour) and worker type ($3 per hour). Latency is in mil-\nliseconds. \n\n\nTable 5 :\n5Input parameters to MILP.\n\nTable 6 :\n6Decision Variables in MILP formulation.\n\n\n, if two P100 machinesTraffic \nFace \nTracker \n\n0 \n\n1 \n\n2 \n\n3 \n\n4 \n\n5 \n\nTotal Cost ($) \n\n3.1 \n\n1.9 \n\n3.6 \n3.7 \n\n0.9 \n\n2.7 \n\n1.6 \n\n2.5 \n\n3.9 \n4.2 \n\nScrooge \nNexus \nNexus_face_3f \n\nNexus_face_6f \nNexus_tracker_3f \nNexus_tracker_4f \n\n(a) \n\nTraffic \nFace \nTracker \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nFinish Rate (%) \n\n97.7 \n98.2 \n98.4 \n86.3 \n\n30.6 \n31.9 \n\n75.7 \n\n90.5 \n88.4 \n97.6 \n\nScrooge \nNexus \nNexus_face_3f \n\nNexus_face_6f \nNexus_tracker_3f \nNexus_tracker_4f \n\n\n\nTable 7 :\n7Comparison of Inference Systemstraffic \nface \ntracker \n\n90 \n\n92 \n\n94 \n\n96 \n\n98 \n\n100 \n\nFinish rate (%) \n\n91.9 \n93.8 \n92.7 \n\n97.7 \n98.2 \n98.4 \n99.2 \n98.4 \n98.5 \n\ndelay = 0 \ndelay = 30 \n\ndelay = 120 \n\n(a) \n\npose caption audio actdet \n\n90 \n\n92 \n\n94 \n\n96 \n\n98 \n\n100 \n\nFinish rate (%) \n\n97.3 \n\n94.7 \n95.9 \n\n92.0 \n\n98.1 \n97.7 \n98.8 \n\n96.5 \n98.4 \n97.7 \n99.1 \n97.2 \n\ndelay = 0 \ndelay = 30 \n\ndelay = 120 \n\n\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\nThe 24 ms worst case latency only includes a 24 ms execution time, while the 236 ms worst case latency includes a 118 ms execution time and another 118 ms maximum queuing delay.\nNote that modules in Scrooge can be stateful unlike some prior work[6,42].3 Today, cloud services do not expose latency SLO choices to individual end user clients, and we adopt the same model. We have left it to future work to explore a generalization where different clients can potentially express different SLOs.\nMost proposed inference systems use profiling[5,6,18,40,42,54] for resource provisioning.\nFuture work can extend Scrooge to support pytorch models without conversion using torchserve[47].9 Table 7tabulates related work. Of these, Nexus has been shown to be better than Clipper. Others do not support DAGs (e.g., Triton), are focused on edge rather than cloud (Rim), build upon FaaS unlike Scrooge (e.g., Llama), or have not made their code available (Inferline).\nAcknowledgmentsThis work was supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.\n. Amazon Web Services. Amazon Web Services 2020. https://aws.amazon.com/.\n\nOn the complexity of scheduling problems for parallel/pipelined machines. D Bernstein, M Rodeh, I Gertner, 10.1109/12.29469IEEE Trans. Comput. 38D. Bernstein, M. Rodeh, and I. Gertner. 1989. On the complexity of scheduling problems for parallel/pipelined machines. IEEE Trans. Comput. 38, 9 (1989), 1308-1313. https://doi.org/10.1109/12.29469\n\nOpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields. Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh, 43Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2019. OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields. IEEE transactions on pattern analysis and machine intelligence 43, 1 (2019), 172-186.\n\nMxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, Zheng Zhang, arXiv:1512.01274arXiv preprintTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274 (2015).\n\nInferLine: latencyaware provisioning and scaling for prediction serving pipelines. Daniel Crankshaw, Gur-Eyal, Xiangxi Sela, Corey Mo, Ion Zumar, Joseph Stoica, Alexey Gonzalez, Tumanov, Proceedings of the 11th ACM Symposium on Cloud Computing. the 11th ACM Symposium on Cloud ComputingDaniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion Stoica, Joseph Gonzalez, and Alexey Tumanov. 2020. InferLine: latency- aware provisioning and scaling for prediction serving pipelines. In Proceedings of the 11th ACM Symposium on Cloud Computing. 477-491.\n\nClipper: A Low-Latency Online Prediction Serving System. Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, Ion Stoica, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). USENIX Association. Boston, MADaniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. 2017. Clipper: A Low-Latency Online Prediction Serving System. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). USENIX As- sociation, Boston, MA, 613-627. https://www.usenix.org/conference/ nsdi17/technical-sessions/presentation/crankshaw\n\nMapReduce: simplified data processing on large clusters. Jeffrey Dean, Sanjay Ghemawat, Commun. ACM. 51Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing on large clusters. Commun. ACM 51, 1 (2008), 107-113.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n\nEarthCam 2021. EarthCam 2021. https://www.earthcam.com/.\n\nJoint 3d face reconstruction and dense alignment with position map regression network. Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, Xi Zhou, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi Zhou. 2018. Joint 3d face reconstruction and dense alignment with position map regression network. In Proceedings of the European Conference on Com- puter Vision (ECCV). 534-551.\n\nLow latency rnn inference with cellular batching. Pin Gao, Lingfan Yu, Yongwei Wu, Jinyang Li, Proceedings of the Thirteenth EuroSys Conference. the Thirteenth EuroSys ConferencePin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency rnn inference with cellular batching. In Proceedings of the Thirteenth EuroSys Conference. 1-15.\n\n. Google Cloud Platform, Google Cloud Platform 2020. https://cloud.google.com/.\n\nGPU Pro Tip: CUDA 7 Streams Simplify Concurrency 2021. GPU Pro Tip: CUDA 7 Streams Simplify Concurrency 2021. https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams- simplify-concurrency/.\n\nCUDA Pro Tip: Understand Fat Binaries and JIT Caching. Mark Harris, Mark Harris. 2013. CUDA Pro Tip: Understand Fat Binaries and JIT Caching. https://devblogs.nvidia.com/cuda-pro-tip-understand-fat- binaries-jit-caching/.\n\nAlexander Hermans, Lucas Beyer, Bastian Leibe, arXiv:1703.07737defense of the triplet loss for person re-identification. arXiv preprintAlexander Hermans, Lucas Beyer, and Bastian Leibe. 2017. In de- fense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737 (2017).\n\nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam, arXiv:1704.04861Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. CoRR abs/1704.04861 (2017). arXiv:1704.04861 http://arxiv.org/abs/1704.04861\n\nFocus: Querying Large Video Datasets with Low Latency and Low Cost. Kevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B Gibbons, Onur Mutlu, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). USENIX Association. Carlsbad, CAKevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B. Gibbons, and Onur Mutlu. 2018. Focus: Querying Large Video Datasets with Low Latency and Low Cost. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). USENIX Association, Carlsbad, CA, 269-286. https://www.usenix.org/conference/osdi18/ presentation/hsieh\n\nRim: Offloading Inference to the Edge. Yitao Hu, Weiwu Pang, Xiaochen Liu, Rajrup Ghosh, Bongjun Ko, Wei-Han Lee, Ramesh Govindan, Proceedings of the 6th ACM/IEEE Conference on Internet of Things Design and Implementation. the 6th ACM/IEEE Conference on Internet of Things Design and ImplementationYitao Hu, Weiwu Pang, Xiaochen Liu, Rajrup Ghosh, Bongjun Ko, Wei-Han Lee, and Ramesh Govindan. 2021. Rim: Offloading Inference to the Edge.. In Proceedings of the 6th ACM/IEEE Conference on Internet of Things Design and Implementation.\n\nSpeed/accuracy trade-offs for modern convolutional object detectors. Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. 2017. Speed/accuracy trade-offs for modern convolutional object detectors. In Proceedings of the IEEE conference on computer vision and pattern recognition. 7310-7311.\n\nThe Next Step in GPU-Accelerated Deep Learning. [20] Inference: The Next Step in GPU-Accelerated Deep Learning 2015. https://developer.nvidia.com/blog/inference-next-step-gpu- accelerated-deep-learning/.\n\nDryad: distributed data-parallel programs from sequential building blocks. Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, Dennis Fetterly, Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems. the 2nd ACM SIGOPS/EuroSys European Conference on Computer SystemsMichael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. 2007. Dryad: distributed data-parallel programs from sequen- tial building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007. 59-72.\n\nParas Jain, Xiangxi Mo, Ajay Jain, Harikaran Subbaraj, Rehan Sohail Durrani, Alexey Tumanov, Joseph Gonzalez, Ion Stoica, arXiv:1901.00041Dynamic Space-Time Scheduling for GPU Inference. arXiv preprintParas Jain, Xiangxi Mo, Ajay Jain, Harikaran Subbaraj, Rehan So- hail Durrani, Alexey Tumanov, Joseph Gonzalez, and Ion Stoica. 2018. Dynamic Space-Time Scheduling for GPU Inference. arXiv preprint arXiv:1901.00041 (2018).\n\nParas Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E Gonzalez, Ion Stoica, arXiv:1901.10008The OoO VLIW JIT Compiler for GPU Inference. arXiv preprintParas Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E Gonzalez, and Ion Stoica. 2019. The OoO VLIW JIT Compiler for GPU Inference. arXiv preprint arXiv:1901.10008 (2019).\n\nThree steps is all you need: fast, accurate, automatic scaling decisions for distributed streaming dataflows. Vasiliki Kalavri, John Liagouris, Moritz Hoffmann, Desislava Dimitrova, Matthew Forshaw, Timothy Roscoe, 13th {USENIX} Symposium on Operating Systems Design and Implementation. Vasiliki Kalavri, John Liagouris, Moritz Hoffmann, Desislava Dim- itrova, Matthew Forshaw, and Timothy Roscoe. 2018. Three steps is all you need: fast, accurate, automatic scaling decisions for distributed streaming dataflows. In 13th {USENIX} Symposium on Operating Sys- tems Design and Implementation ({OSDI} 18). 783-798.\n\nMetron: NFV Service Chains at the True Speed of the Underlying Hardware. P Georgios, Tom Katsikas, Dejan Barbette, Rebecca Kosti\u0107, Gerald Q Steinert, MaguireJr, 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18). USENIX Association. Renton, WAGeorgios P. Katsikas, Tom Barbette, Dejan Kosti\u0107, Rebecca Steinert, and Gerald Q. Maguire Jr. 2018. Metron: NFV Service Chains at the True Speed of the Underlying Hardware. In 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18). USENIX As- sociation, Renton, WA, 171-186. https://www.usenix.org/conference/ nsdi18/presentation/katsikas\n\nImagenet Classification with Deep Convolutional Neural Networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet Classification with Deep Convolutional Neural Networks. In Advances in neural information processing systems. 1097-1105.\n\nDeep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, nature. 521Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436-444.\n\nCaesar: cross-camera complex activity recognition. Xiaochen Liu, Pradipta Ghosh, Oytun Ulutan, Kevin Bs Manjunath, Ramesh Chan, Govindan, Proceedings of the 17th Conference on Embedded Networked Sensor Systems. the 17th Conference on Embedded Networked Sensor SystemsXiaochen Liu, Pradipta Ghosh, Oytun Ulutan, BS Manjunath, Kevin Chan, and Ramesh Govindan. 2019. Caesar: cross-camera complex activity recognition. In Proceedings of the 17th Conference on Embedded Networked Sensor Systems. 232-244.\n\nGurobi Optimization LLC. 2020. Gurobi Optimizer Reference Manual. Gurobi Optimization LLC. 2020. Gurobi Optimizer Reference Manual. http://www.gurobi.com.\n\n. Microsoft Azure, Microsoft Azure 2020. https://azure.microsoft.com/en-us/.\n\nNaiad: a timely dataflow system. G Derek, Frank Murray, Rebecca Mcsherry, Michael Isaacs, Paul Isard, Mart\u00edn Barham, Abadi, Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. the Twenty-Fourth ACM Symposium on Operating Systems PrinciplesDerek G Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham, and Mart\u00edn Abadi. 2013. Naiad: a timely dataflow system. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. 439-455.\n\nNVIDIA System Management Interface. NVIDIA System Management Interface 2012. https://developer.nvidia. com/nvidia-system-management-interface.\n\n. NVIDIA Triton Inference Server. NVIDIA Triton Inference Server 2021. https://developer.nvidia.com/ nvidia-triton-inference-server.\n\n. Nvidia&apos;s Tensorrt, NVIDIA's TensorRT 2019. https://developer.nvidia.com/tensorrt.\n\nONNX 2021. ONNX 2021. https://onnx.ai/.\n\nNetBricks: Taking the V out of NFV. Aurojit Panda, Sangjin Han, Keon Jang, Melvin Walls, Sylvia Ratnasamy, Scott Shenker, 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). USENIX Association. Savannah, GAAurojit Panda, Sangjin Han, Keon Jang, Melvin Walls, Sylvia Rat- nasamy, and Scott Shenker. 2016. NetBricks: Taking the V out of NFV. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). USENIX Association, Savannah, GA, 203- 216. https://www.usenix.org/conference/osdi16/technical-sessions/ presentation/panda\n\nRealtime action recognition. Realtime action recognition 2019. https://github.com/felixchenfy/ Realtime-Action-Recognition.\n\nJoseph Redmon, Ali Farhadi, YOLOv3: an Incremental Improvement. arXiv. Joseph Redmon and Ali Farhadi. 2018. YOLOv3: an Incremental Im- provement. arXiv (2018).\n\nINFaaS: A model-less inference serving system. Francisco Romero, Qian Li, J Neeraja, Christos Yadwadkar, Kozyrakis, arXiv:1905.13348arXiv preprintFrancisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. 2019. INFaaS: A model-less inference serving system. arXiv preprint arXiv:1905.13348 (2019).\n\nFrancisco Romero, Mark Zhao, J Neeraja, Christos Yadwadkar, Kozyrakis, arXiv:2102.01887Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines. arXiv preprintFrancisco Romero, Mark Zhao, Neeraja J Yadwadkar, and Christos Kozyrakis. 2021. Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines. arXiv preprint arXiv:2102.01887 (2021).\n\nNexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis. Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, Ravi Sundaram, 10.1145/3341301.3359658Proceedings of the 27th ACM Symposium on Operating Systems Principles. the 27th ACM Symposium on Operating Systems PrinciplesHuntsville, Ontario, Canada; New York, NY, USAAssociation for Computing MachinerySOSP '19)Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019. Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis. In Proceedings of the 27th ACM Symposium on Operating Systems Principles (Huntsville, Ontario, Canada) (SOSP '19). Association for Computing Machinery, New York, NY, USA, 322-337. https://doi. org/10.1145/3341301.3359658\n\nInception-v4, Inception-Resnet and the Impact of Residual Connections on Learning. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander A Alemi, Thirty-First AAAI Conference on Artificial Intelligence. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. 2017. Inception-v4, Inception-Resnet and the Impact of Residual Connections on Learning. In Thirty-First AAAI Conference on Artificial Intelligence.\n\n. TensorFlow Serving. TensorFlow Serving 2021. https://github.com/tensorflow/serving.\n\nThe Private Life of MP3 Frames 2021. The Private Life of MP3 Frames 2021. http://id3lib.sourceforge.net/ id3/mp3frame.html.\n\nTorchServe 2021. TorchServe 2021. https://pytorch.org/serve/.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. At- tention is all you need. In Advances in neural information processing systems. 5998-6008.\n\nSequence to sequence-video to text. Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Proceedings of the IEEE. the IEEESubhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Ray- mond Mooney, Trevor Darrell, and Kate Saenko. 2015. Sequence to sequence-video to text. In Proceedings of the IEEE international confer- ence on computer vision. 4534-4542.\n\nOptimal instruction scheduling using integer programming. Kent Wilken, Jack Liu, Mark Heffernan, 10.1145/349299.349318Proceedings of the 2000 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI). the 2000 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)Kent Wilken, Jack Liu, and Mark Heffernan. 2000. Optimal instruction scheduling using integer programming. In Proceedings of the 2000 ACM SIGPLAN Conference on Programming Language Design and Implemen- tation (PLDI), 2000. 121-133. https://doi.org/10.1145/349299.349318\n\nElastic Scaling of Stateful Network Functions. Shinae Woo, Justine Sherry, Sangjin Han, Sue Moon, Sylvia Ratnasamy, Scott Shenker, 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18). USENIX Association. Renton, WAShinae Woo, Justine Sherry, Sangjin Han, Sue Moon, Sylvia Ratnasamy, and Scott Shenker. 2018. Elastic Scaling of Stateful Network Func- tions. In 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18). USENIX Association, Renton, WA, 299-312. https://www.usenix.org/conference/nsdi18/presentation/woo\n\nGandiva: Introspective Cluster Scheduling for Deep Learning. Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, Lidong Zhou, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). USENIX Association. Carlsbad, CAWencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Si- vathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, and Lidong Zhou. 2018. Gan- diva: Introspective Cluster Scheduling for Deep Learning. In 13th USENIX Symposium on Operating Systems Design and Implementa- tion (OSDI 18). USENIX Association, Carlsbad, CA, 595-610. https: //www.usenix.org/conference/osdi18/presentation/xiao\n\nSpark: Cluster computing with working sets. Matei Zaharia, Mosharaf Chowdhury, J Michael, Scott Franklin, Ion Shenker, Stoica, 1095Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, Ion Stoica, et al. 2010. Spark: Cluster computing with working sets. HotCloud 10, 10-10 (2010), 95.\n\nMark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan, 2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19. Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. 2019. Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. In 2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19). 1049-1062.\n\nLive video analytics at scale with approximation and delay-tolerance. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose, Paramvir Bahl, Michael J Freedman, 14th {USENIX} Symposium on Networked Systems Design and Implementation. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Phili- pose, Paramvir Bahl, and Michael J Freedman. 2017. Live video analyt- ics at scale with approximation and delay-tolerance. In 14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17). 377-392.\n", "annotations": {"author": "[{\"end\":84,\"start\":60},{\"end\":115,\"start\":85},{\"end\":147,\"start\":116},{\"end\":217,\"start\":148},{\"end\":253,\"start\":218}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":66},{\"end\":97,\"start\":92},{\"end\":131,\"start\":123}]", "author_first_name": "[{\"end\":65,\"start\":60},{\"end\":91,\"start\":85},{\"end\":122,\"start\":116}]", "author_affiliation": "[{\"end\":216,\"start\":149},{\"end\":252,\"start\":219}]", "title": "[{\"end\":57,\"start\":1},{\"end\":310,\"start\":254}]", "venue": null, "abstract": "[{\"end\":1460,\"start\":579}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1660,\"start\":1656},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1711,\"start\":1708},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1714,\"start\":1711},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1885,\"start\":1882},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1887,\"start\":1885},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1890,\"start\":1887},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1893,\"start\":1890},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1896,\"start\":1893},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1899,\"start\":1896},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":1902,\"start\":1899},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":1905,\"start\":1902},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2087,\"start\":2083},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2458,\"start\":2454},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2650,\"start\":2646},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3010,\"start\":3007},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3012,\"start\":3010},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3015,\"start\":3012},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3018,\"start\":3015},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3021,\"start\":3018},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3024,\"start\":3021},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3027,\"start\":3024},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3619,\"start\":3616},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3621,\"start\":3619},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3624,\"start\":3621},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4859,\"start\":4856},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4862,\"start\":4859},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5049,\"start\":5045},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5052,\"start\":5049},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5307,\"start\":5303},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5913,\"start\":5909},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5950,\"start\":5947},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5953,\"start\":5950},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8000,\"start\":7996},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9048,\"start\":9044},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9066,\"start\":9063},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9091,\"start\":9087},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9542,\"start\":9538},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9557,\"start\":9553},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9848,\"start\":9844},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9885,\"start\":9882},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9888,\"start\":9885},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10577,\"start\":10574},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10580,\"start\":10577},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10583,\"start\":10580},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10785,\"start\":10781},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10906,\"start\":10903},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10909,\"start\":10906},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11197,\"start\":11193},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11200,\"start\":11197},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11368,\"start\":11364},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11951,\"start\":11947},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11954,\"start\":11951},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11957,\"start\":11954},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12020,\"start\":12016},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12307,\"start\":12303},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12359,\"start\":12355},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12409,\"start\":12405},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13179,\"start\":13175},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13281,\"start\":13277},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13284,\"start\":13281},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13734,\"start\":13731},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13737,\"start\":13734},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14048,\"start\":14044},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14815,\"start\":14811},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15169,\"start\":15165},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15172,\"start\":15169},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15175,\"start\":15172},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15994,\"start\":15990},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16503,\"start\":16502},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21033,\"start\":21029},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23835,\"start\":23832},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23838,\"start\":23835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24005,\"start\":24002},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27443,\"start\":27439},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29670,\"start\":29669},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30762,\"start\":30758},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":33335,\"start\":33331},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35563,\"start\":35559},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35566,\"start\":35563},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35569,\"start\":35566},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36825,\"start\":36821},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36828,\"start\":36825},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36868,\"start\":36864},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38163,\"start\":38159},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":38393,\"start\":38389},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38414,\"start\":38413},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38728,\"start\":38727},{\"end\":39085,\"start\":39081},{\"end\":39139,\"start\":39135},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39152,\"start\":39149},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":39179,\"start\":39175},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39541,\"start\":39537},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39717,\"start\":39713},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":40163,\"start\":40159},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40187,\"start\":40183},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":40364,\"start\":40360},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":40483,\"start\":40479},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40574,\"start\":40571},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40647,\"start\":40643},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":40705,\"start\":40701},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40832,\"start\":40828},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40867,\"start\":40863},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40956,\"start\":40952},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41110,\"start\":41107},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41113,\"start\":41110},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41116,\"start\":41113},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41119,\"start\":41116},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":41122,\"start\":41119},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":45876,\"start\":45872},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47770,\"start\":47768},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":47915,\"start\":47913},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":52040,\"start\":52038},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":54062,\"start\":54058},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":54065,\"start\":54062},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":54368,\"start\":54364},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":54422,\"start\":54419},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":54466,\"start\":54462},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":54524,\"start\":54520},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":54574,\"start\":54571},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":54629,\"start\":54625},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":54680,\"start\":54676},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":54719,\"start\":54715},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":54791,\"start\":54787},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":55481,\"start\":55477},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":55641,\"start\":55638},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":55657,\"start\":55653},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":55718,\"start\":55714},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":55891,\"start\":55888},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":56003,\"start\":55999},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":56163,\"start\":56159},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":56173,\"start\":56169},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":56193,\"start\":56189},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":56337,\"start\":56333},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":56564,\"start\":56560},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":56718,\"start\":56714},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":56889,\"start\":56886},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":56892,\"start\":56889},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":56895,\"start\":56892},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":56898,\"start\":56895},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":61281,\"start\":61278},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":61284,\"start\":61281},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":61286,\"start\":61285},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":61575,\"start\":61572},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":61577,\"start\":61575},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":61580,\"start\":61577},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":61583,\"start\":61580},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":61586,\"start\":61583},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":61589,\"start\":61586},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":61713,\"start\":61709},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":61715,\"start\":61714}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":58026,\"start\":57863},{\"attributes\":{\"id\":\"fig_1\"},\"end\":58124,\"start\":58027},{\"attributes\":{\"id\":\"fig_2\"},\"end\":58347,\"start\":58125},{\"attributes\":{\"id\":\"fig_3\"},\"end\":58524,\"start\":58348},{\"attributes\":{\"id\":\"fig_4\"},\"end\":58675,\"start\":58525},{\"attributes\":{\"id\":\"fig_5\"},\"end\":58759,\"start\":58676},{\"attributes\":{\"id\":\"fig_6\"},\"end\":58903,\"start\":58760},{\"attributes\":{\"id\":\"fig_7\"},\"end\":59114,\"start\":58904},{\"attributes\":{\"id\":\"fig_8\"},\"end\":59321,\"start\":59115},{\"attributes\":{\"id\":\"fig_9\"},\"end\":59526,\"start\":59322},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59574,\"start\":59527},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59758,\"start\":59575},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":59862,\"start\":59759},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":59986,\"start\":59863},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":60024,\"start\":59987},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":60076,\"start\":60025},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":60536,\"start\":60077},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":60946,\"start\":60537}]", "paragraph": "[{\"end\":1632,\"start\":1471},{\"end\":1716,\"start\":1634},{\"end\":2802,\"start\":1718},{\"end\":3429,\"start\":2804},{\"end\":4693,\"start\":3431},{\"end\":5634,\"start\":4695},{\"end\":6372,\"start\":5636},{\"end\":8658,\"start\":6374},{\"end\":9660,\"start\":8673},{\"end\":10059,\"start\":9662},{\"end\":11370,\"start\":10061},{\"end\":11879,\"start\":11372},{\"end\":12410,\"start\":11881},{\"end\":12915,\"start\":12412},{\"end\":13594,\"start\":12917},{\"end\":13702,\"start\":13596},{\"end\":14408,\"start\":13704},{\"end\":14928,\"start\":14410},{\"end\":14991,\"start\":14955},{\"end\":15118,\"start\":15010},{\"end\":16293,\"start\":15131},{\"end\":17378,\"start\":16295},{\"end\":18718,\"start\":17380},{\"end\":19224,\"start\":18748},{\"end\":19728,\"start\":19226},{\"end\":20536,\"start\":19730},{\"end\":21016,\"start\":20538},{\"end\":23191,\"start\":21018},{\"end\":24942,\"start\":23206},{\"end\":25483,\"start\":24944},{\"end\":26893,\"start\":25485},{\"end\":27847,\"start\":26895},{\"end\":28985,\"start\":27849},{\"end\":29432,\"start\":28987},{\"end\":29788,\"start\":29434},{\"end\":30051,\"start\":29790},{\"end\":30343,\"start\":30099},{\"end\":30586,\"start\":30345},{\"end\":31132,\"start\":30588},{\"end\":31786,\"start\":31219},{\"end\":32387,\"start\":31788},{\"end\":32676,\"start\":32389},{\"end\":33054,\"start\":32678},{\"end\":33685,\"start\":33068},{\"end\":34107,\"start\":33699},{\"end\":34456,\"start\":34109},{\"end\":35008,\"start\":34458},{\"end\":35225,\"start\":35010},{\"end\":35624,\"start\":35227},{\"end\":35834,\"start\":35626},{\"end\":35992,\"start\":35877},{\"end\":36349,\"start\":35994},{\"end\":36732,\"start\":36351},{\"end\":38072,\"start\":36761},{\"end\":38222,\"start\":38087},{\"end\":40995,\"start\":38238},{\"end\":41897,\"start\":40997},{\"end\":41957,\"start\":41899},{\"end\":42508,\"start\":41959},{\"end\":43173,\"start\":42531},{\"end\":44741,\"start\":43175},{\"end\":45065,\"start\":44743},{\"end\":45286,\"start\":45067},{\"end\":45618,\"start\":45288},{\"end\":46102,\"start\":45620},{\"end\":46950,\"start\":46104},{\"end\":47490,\"start\":46952},{\"end\":48385,\"start\":47492},{\"end\":49642,\"start\":48429},{\"end\":50410,\"start\":49644},{\"end\":51091,\"start\":50412},{\"end\":51566,\"start\":51093},{\"end\":52165,\"start\":51568},{\"end\":52883,\"start\":52197},{\"end\":53415,\"start\":52885},{\"end\":55295,\"start\":53417},{\"end\":57264,\"start\":55312},{\"end\":57862,\"start\":57280}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":30098,\"start\":30052},{\"attributes\":{\"id\":\"formula_1\"},\"end\":31176,\"start\":31133},{\"attributes\":{\"id\":\"formula_2\"},\"end\":31218,\"start\":31176},{\"attributes\":{\"id\":\"formula_3\"},\"end\":33067,\"start\":33055}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8929,\"start\":8922},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":9393,\"start\":9386},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17860,\"start\":17851},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24415,\"start\":24396},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24834,\"start\":24744},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27918,\"start\":27911},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28357,\"start\":28350},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28460,\"start\":28451},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28923,\"start\":28915},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29552,\"start\":29543},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":36545,\"start\":36536},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":41568,\"start\":41561},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49784,\"start\":49777}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":8671,\"start\":8661},{\"end\":14953,\"start\":14931},{\"attributes\":{\"n\":\"3\"},\"end\":15008,\"start\":14994},{\"attributes\":{\"n\":\"3.1\"},\"end\":15129,\"start\":15121},{\"attributes\":{\"n\":\"3.2\"},\"end\":18746,\"start\":18721},{\"attributes\":{\"n\":\"3.3\"},\"end\":23204,\"start\":23194},{\"attributes\":{\"n\":\"3.4\"},\"end\":33697,\"start\":33688},{\"attributes\":{\"n\":\"3.5\"},\"end\":35875,\"start\":35837},{\"attributes\":{\"n\":\"3.6\"},\"end\":36759,\"start\":36735},{\"attributes\":{\"n\":\"4\"},\"end\":38085,\"start\":38075},{\"attributes\":{\"n\":\"4.1\"},\"end\":38236,\"start\":38225},{\"attributes\":{\"n\":\"4.2\"},\"end\":42529,\"start\":42511},{\"attributes\":{\"n\":\"4.3\"},\"end\":48427,\"start\":48388},{\"attributes\":{\"n\":\"4.4\"},\"end\":52195,\"start\":52168},{\"attributes\":{\"n\":\"5\"},\"end\":55310,\"start\":55298},{\"attributes\":{\"n\":\"6\"},\"end\":57278,\"start\":57267},{\"end\":57874,\"start\":57864},{\"end\":58038,\"start\":58028},{\"end\":58136,\"start\":58126},{\"end\":58546,\"start\":58526},{\"end\":58687,\"start\":58677},{\"end\":58783,\"start\":58761},{\"end\":58916,\"start\":58905},{\"end\":59127,\"start\":59116},{\"end\":59334,\"start\":59323},{\"end\":59537,\"start\":59528},{\"end\":59769,\"start\":59760},{\"end\":59873,\"start\":59864},{\"end\":59997,\"start\":59988},{\"end\":60035,\"start\":60026},{\"end\":60547,\"start\":60538}]", "table": "[{\"end\":59758,\"start\":59614},{\"end\":59986,\"start\":59875},{\"end\":60536,\"start\":60101},{\"end\":60946,\"start\":60580}]", "figure_caption": "[{\"end\":58026,\"start\":57876},{\"end\":58124,\"start\":58040},{\"end\":58347,\"start\":58138},{\"end\":58524,\"start\":58350},{\"end\":58675,\"start\":58549},{\"end\":58759,\"start\":58689},{\"end\":58903,\"start\":58788},{\"end\":59114,\"start\":58919},{\"end\":59321,\"start\":59130},{\"end\":59526,\"start\":59337},{\"end\":59574,\"start\":59539},{\"end\":59614,\"start\":59577},{\"end\":59862,\"start\":59771},{\"end\":60024,\"start\":59999},{\"end\":60076,\"start\":60037},{\"end\":60101,\"start\":60079},{\"end\":60580,\"start\":60549}]", "figure_ref": "[{\"end\":1470,\"start\":1462},{\"end\":1915,\"start\":1907},{\"end\":6935,\"start\":6926},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11240,\"start\":11232},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11410,\"start\":11400},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11685,\"start\":11676},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13214,\"start\":13205},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13490,\"start\":13481},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13974,\"start\":13965},{\"end\":14972,\"start\":14964},{\"end\":15335,\"start\":15327},{\"end\":17065,\"start\":17057},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21256,\"start\":21248},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22926,\"start\":22918},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23064,\"start\":23056},{\"end\":29141,\"start\":29133},{\"end\":29746,\"start\":29738},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38597,\"start\":38586},{\"end\":40109,\"start\":40101},{\"end\":41581,\"start\":41573},{\"end\":43192,\"start\":43184},{\"end\":43457,\"start\":43449},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43741,\"start\":43732},{\"end\":44662,\"start\":44654},{\"end\":45314,\"start\":45306},{\"end\":46243,\"start\":46235},{\"end\":46256,\"start\":46248},{\"end\":47317,\"start\":47309},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":47330,\"start\":47322},{\"end\":47696,\"start\":47688},{\"end\":48639,\"start\":48630},{\"end\":49640,\"start\":49631},{\"end\":49878,\"start\":49869},{\"end\":49969,\"start\":49960},{\"end\":50229,\"start\":50218},{\"end\":50312,\"start\":50303},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50721,\"start\":50712},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50802,\"start\":50792},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50965,\"start\":50955},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":51301,\"start\":51293},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51843,\"start\":51833},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52347,\"start\":52337},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52636,\"start\":52626},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":53657,\"start\":53647},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":53818,\"start\":53808},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":54849,\"start\":54839},{\"end\":54910,\"start\":54902}]", "bib_author_first_name": "[{\"end\":62316,\"start\":62315},{\"end\":62329,\"start\":62328},{\"end\":62338,\"start\":62337},{\"end\":62667,\"start\":62664},{\"end\":62678,\"start\":62673},{\"end\":62693,\"start\":62688},{\"end\":62708,\"start\":62701},{\"end\":62719,\"start\":62714},{\"end\":63072,\"start\":63066},{\"end\":63081,\"start\":63079},{\"end\":63092,\"start\":63086},{\"end\":63100,\"start\":63097},{\"end\":63112,\"start\":63106},{\"end\":63125,\"start\":63119},{\"end\":63139,\"start\":63132},{\"end\":63150,\"start\":63146},{\"end\":63162,\"start\":63155},{\"end\":63175,\"start\":63170},{\"end\":63566,\"start\":63560},{\"end\":63595,\"start\":63588},{\"end\":63607,\"start\":63602},{\"end\":63615,\"start\":63612},{\"end\":63629,\"start\":63623},{\"end\":63644,\"start\":63638},{\"end\":64095,\"start\":64089},{\"end\":64110,\"start\":64107},{\"end\":64123,\"start\":64117},{\"end\":64137,\"start\":64130},{\"end\":64139,\"start\":64138},{\"end\":64156,\"start\":64150},{\"end\":64158,\"start\":64157},{\"end\":64172,\"start\":64169},{\"end\":64727,\"start\":64720},{\"end\":64740,\"start\":64734},{\"end\":64904,\"start\":64899},{\"end\":64921,\"start\":64913},{\"end\":64935,\"start\":64929},{\"end\":64949,\"start\":64941},{\"end\":65416,\"start\":65413},{\"end\":65426,\"start\":65423},{\"end\":65437,\"start\":65431},{\"end\":65451,\"start\":65444},{\"end\":65460,\"start\":65458},{\"end\":65867,\"start\":65864},{\"end\":65880,\"start\":65873},{\"end\":65892,\"start\":65885},{\"end\":65904,\"start\":65897},{\"end\":66165,\"start\":66159},{\"end\":66493,\"start\":66489},{\"end\":66666,\"start\":66657},{\"end\":66681,\"start\":66676},{\"end\":66696,\"start\":66689},{\"end\":67043,\"start\":67037},{\"end\":67045,\"start\":67044},{\"end\":67062,\"start\":67054},{\"end\":67070,\"start\":67068},{\"end\":67083,\"start\":67077},{\"end\":67104,\"start\":67098},{\"end\":67117,\"start\":67111},{\"end\":67131,\"start\":67126},{\"end\":67150,\"start\":67143},{\"end\":67539,\"start\":67534},{\"end\":67553,\"start\":67547},{\"end\":67577,\"start\":67572},{\"end\":67593,\"start\":67585},{\"end\":67616,\"start\":67608},{\"end\":67630,\"start\":67623},{\"end\":67649,\"start\":67642},{\"end\":67651,\"start\":67650},{\"end\":67665,\"start\":67661},{\"end\":68235,\"start\":68230},{\"end\":68245,\"start\":68240},{\"end\":68260,\"start\":68252},{\"end\":68272,\"start\":68266},{\"end\":68287,\"start\":68280},{\"end\":68299,\"start\":68292},{\"end\":68311,\"start\":68305},{\"end\":68804,\"start\":68796},{\"end\":68817,\"start\":68812},{\"end\":68830,\"start\":68826},{\"end\":68844,\"start\":68836},{\"end\":68855,\"start\":68850},{\"end\":68876,\"start\":68869},{\"end\":68887,\"start\":68884},{\"end\":68905,\"start\":68897},{\"end\":68917,\"start\":68913},{\"end\":68930,\"start\":68924},{\"end\":69694,\"start\":69687},{\"end\":69707,\"start\":69702},{\"end\":69719,\"start\":69715},{\"end\":69730,\"start\":69724},{\"end\":69746,\"start\":69740},{\"end\":70167,\"start\":70162},{\"end\":70181,\"start\":70174},{\"end\":70190,\"start\":70186},{\"end\":70206,\"start\":70197},{\"end\":70222,\"start\":70217},{\"end\":70245,\"start\":70239},{\"end\":70261,\"start\":70255},{\"end\":70275,\"start\":70272},{\"end\":70592,\"start\":70587},{\"end\":70606,\"start\":70599},{\"end\":70615,\"start\":70611},{\"end\":70628,\"start\":70622},{\"end\":70644,\"start\":70638},{\"end\":70646,\"start\":70645},{\"end\":70660,\"start\":70657},{\"end\":71040,\"start\":71032},{\"end\":71054,\"start\":71050},{\"end\":71072,\"start\":71066},{\"end\":71092,\"start\":71083},{\"end\":71111,\"start\":71104},{\"end\":71128,\"start\":71121},{\"end\":71609,\"start\":71608},{\"end\":71623,\"start\":71620},{\"end\":71639,\"start\":71634},{\"end\":71657,\"start\":71650},{\"end\":71672,\"start\":71666},{\"end\":71674,\"start\":71673},{\"end\":72239,\"start\":72235},{\"end\":72256,\"start\":72252},{\"end\":72276,\"start\":72268},{\"end\":72278,\"start\":72277},{\"end\":72550,\"start\":72546},{\"end\":72564,\"start\":72558},{\"end\":72581,\"start\":72573},{\"end\":72764,\"start\":72756},{\"end\":72778,\"start\":72770},{\"end\":72791,\"start\":72786},{\"end\":72805,\"start\":72800},{\"end\":72826,\"start\":72820},{\"end\":73373,\"start\":73364},{\"end\":73474,\"start\":73473},{\"end\":73487,\"start\":73482},{\"end\":73503,\"start\":73496},{\"end\":73521,\"start\":73514},{\"end\":73534,\"start\":73530},{\"end\":73548,\"start\":73542},{\"end\":74385,\"start\":74378},{\"end\":74400,\"start\":74393},{\"end\":74410,\"start\":74406},{\"end\":74423,\"start\":74417},{\"end\":74437,\"start\":74431},{\"end\":74454,\"start\":74449},{\"end\":75049,\"start\":75043},{\"end\":75061,\"start\":75058},{\"end\":75260,\"start\":75251},{\"end\":75273,\"start\":75269},{\"end\":75279,\"start\":75278},{\"end\":75297,\"start\":75289},{\"end\":75525,\"start\":75516},{\"end\":75538,\"start\":75534},{\"end\":75546,\"start\":75545},{\"end\":75564,\"start\":75556},{\"end\":75994,\"start\":75987},{\"end\":76006,\"start\":76001},{\"end\":76019,\"start\":76013},{\"end\":76032,\"start\":76025},{\"end\":76045,\"start\":76039},{\"end\":76059,\"start\":76052},{\"end\":76077,\"start\":76071},{\"end\":76097,\"start\":76093},{\"end\":76862,\"start\":76853},{\"end\":76878,\"start\":76872},{\"end\":76893,\"start\":76886},{\"end\":76914,\"start\":76905},{\"end\":76916,\"start\":76915},{\"end\":77514,\"start\":77508},{\"end\":77528,\"start\":77524},{\"end\":77542,\"start\":77538},{\"end\":77556,\"start\":77551},{\"end\":77573,\"start\":77568},{\"end\":77586,\"start\":77581},{\"end\":77588,\"start\":77587},{\"end\":77602,\"start\":77596},{\"end\":77616,\"start\":77611},{\"end\":77952,\"start\":77942},{\"end\":77972,\"start\":77966},{\"end\":77990,\"start\":77983},{\"end\":78007,\"start\":78000},{\"end\":78022,\"start\":78016},{\"end\":78036,\"start\":78032},{\"end\":78377,\"start\":78373},{\"end\":78390,\"start\":78386},{\"end\":78400,\"start\":78396},{\"end\":78950,\"start\":78944},{\"end\":78963,\"start\":78956},{\"end\":78979,\"start\":78972},{\"end\":78988,\"start\":78985},{\"end\":79001,\"start\":78995},{\"end\":79018,\"start\":79013},{\"end\":79532,\"start\":79525},{\"end\":79544,\"start\":79539},{\"end\":79567,\"start\":79555},{\"end\":79583,\"start\":79576},{\"end\":79600,\"start\":79595},{\"end\":79616,\"start\":79609},{\"end\":79630,\"start\":79622},{\"end\":79642,\"start\":79638},{\"end\":79654,\"start\":79649},{\"end\":79667,\"start\":79661},{\"end\":79678,\"start\":79675},{\"end\":79691,\"start\":79685},{\"end\":80296,\"start\":80291},{\"end\":80314,\"start\":80306},{\"end\":80327,\"start\":80326},{\"end\":80342,\"start\":80337},{\"end\":80356,\"start\":80353},{\"end\":80657,\"start\":80647},{\"end\":80672,\"start\":80665},{\"end\":80680,\"start\":80677},{\"end\":80691,\"start\":80687},{\"end\":81070,\"start\":81065},{\"end\":81084,\"start\":81078},{\"end\":81108,\"start\":81103},{\"end\":81123,\"start\":81116},{\"end\":81143,\"start\":81135},{\"end\":81159,\"start\":81150}]", "bib_author_last_name": "[{\"end\":62326,\"start\":62317},{\"end\":62335,\"start\":62330},{\"end\":62346,\"start\":62339},{\"end\":62671,\"start\":62668},{\"end\":62686,\"start\":62679},{\"end\":62699,\"start\":62694},{\"end\":62712,\"start\":62709},{\"end\":62726,\"start\":62720},{\"end\":63077,\"start\":63073},{\"end\":63084,\"start\":63082},{\"end\":63095,\"start\":63093},{\"end\":63104,\"start\":63101},{\"end\":63117,\"start\":63113},{\"end\":63130,\"start\":63126},{\"end\":63144,\"start\":63140},{\"end\":63153,\"start\":63151},{\"end\":63168,\"start\":63163},{\"end\":63181,\"start\":63176},{\"end\":63576,\"start\":63567},{\"end\":63586,\"start\":63578},{\"end\":63600,\"start\":63596},{\"end\":63610,\"start\":63608},{\"end\":63621,\"start\":63616},{\"end\":63636,\"start\":63630},{\"end\":63653,\"start\":63645},{\"end\":63662,\"start\":63655},{\"end\":64105,\"start\":64096},{\"end\":64115,\"start\":64111},{\"end\":64128,\"start\":64124},{\"end\":64148,\"start\":64140},{\"end\":64167,\"start\":64159},{\"end\":64179,\"start\":64173},{\"end\":64732,\"start\":64728},{\"end\":64749,\"start\":64741},{\"end\":64911,\"start\":64905},{\"end\":64927,\"start\":64922},{\"end\":64939,\"start\":64936},{\"end\":64959,\"start\":64950},{\"end\":65421,\"start\":65417},{\"end\":65429,\"start\":65427},{\"end\":65442,\"start\":65438},{\"end\":65456,\"start\":65452},{\"end\":65465,\"start\":65461},{\"end\":65871,\"start\":65868},{\"end\":65883,\"start\":65881},{\"end\":65895,\"start\":65893},{\"end\":65907,\"start\":65905},{\"end\":66180,\"start\":66166},{\"end\":66500,\"start\":66494},{\"end\":66674,\"start\":66667},{\"end\":66687,\"start\":66682},{\"end\":66702,\"start\":66697},{\"end\":67052,\"start\":67046},{\"end\":67066,\"start\":67063},{\"end\":67075,\"start\":67071},{\"end\":67096,\"start\":67084},{\"end\":67109,\"start\":67105},{\"end\":67124,\"start\":67118},{\"end\":67141,\"start\":67132},{\"end\":67155,\"start\":67151},{\"end\":67545,\"start\":67540},{\"end\":67570,\"start\":67554},{\"end\":67583,\"start\":67578},{\"end\":67606,\"start\":67594},{\"end\":67621,\"start\":67617},{\"end\":67640,\"start\":67631},{\"end\":67659,\"start\":67652},{\"end\":67671,\"start\":67666},{\"end\":68238,\"start\":68236},{\"end\":68250,\"start\":68246},{\"end\":68264,\"start\":68261},{\"end\":68278,\"start\":68273},{\"end\":68290,\"start\":68288},{\"end\":68303,\"start\":68300},{\"end\":68320,\"start\":68312},{\"end\":68810,\"start\":68805},{\"end\":68824,\"start\":68818},{\"end\":68834,\"start\":68831},{\"end\":68848,\"start\":68845},{\"end\":68867,\"start\":68856},{\"end\":68882,\"start\":68877},{\"end\":68895,\"start\":68888},{\"end\":68911,\"start\":68906},{\"end\":68922,\"start\":68918},{\"end\":68941,\"start\":68931},{\"end\":69700,\"start\":69695},{\"end\":69713,\"start\":69708},{\"end\":69722,\"start\":69720},{\"end\":69738,\"start\":69731},{\"end\":69755,\"start\":69747},{\"end\":70172,\"start\":70168},{\"end\":70184,\"start\":70182},{\"end\":70195,\"start\":70191},{\"end\":70215,\"start\":70207},{\"end\":70237,\"start\":70223},{\"end\":70253,\"start\":70246},{\"end\":70270,\"start\":70262},{\"end\":70282,\"start\":70276},{\"end\":70597,\"start\":70593},{\"end\":70609,\"start\":70607},{\"end\":70620,\"start\":70616},{\"end\":70636,\"start\":70629},{\"end\":70655,\"start\":70647},{\"end\":70667,\"start\":70661},{\"end\":71048,\"start\":71041},{\"end\":71064,\"start\":71055},{\"end\":71081,\"start\":71073},{\"end\":71102,\"start\":71093},{\"end\":71119,\"start\":71112},{\"end\":71135,\"start\":71129},{\"end\":71618,\"start\":71610},{\"end\":71632,\"start\":71624},{\"end\":71648,\"start\":71640},{\"end\":71664,\"start\":71658},{\"end\":71683,\"start\":71675},{\"end\":71692,\"start\":71685},{\"end\":72250,\"start\":72240},{\"end\":72266,\"start\":72257},{\"end\":72285,\"start\":72279},{\"end\":72556,\"start\":72551},{\"end\":72571,\"start\":72565},{\"end\":72588,\"start\":72582},{\"end\":72768,\"start\":72765},{\"end\":72784,\"start\":72779},{\"end\":72798,\"start\":72792},{\"end\":72818,\"start\":72806},{\"end\":72831,\"start\":72827},{\"end\":72841,\"start\":72833},{\"end\":73379,\"start\":73374},{\"end\":73480,\"start\":73475},{\"end\":73494,\"start\":73488},{\"end\":73512,\"start\":73504},{\"end\":73528,\"start\":73522},{\"end\":73540,\"start\":73535},{\"end\":73555,\"start\":73549},{\"end\":73562,\"start\":73557},{\"end\":74235,\"start\":74213},{\"end\":74391,\"start\":74386},{\"end\":74404,\"start\":74401},{\"end\":74415,\"start\":74411},{\"end\":74429,\"start\":74424},{\"end\":74447,\"start\":74438},{\"end\":74462,\"start\":74455},{\"end\":75056,\"start\":75050},{\"end\":75069,\"start\":75062},{\"end\":75267,\"start\":75261},{\"end\":75276,\"start\":75274},{\"end\":75287,\"start\":75280},{\"end\":75307,\"start\":75298},{\"end\":75318,\"start\":75309},{\"end\":75532,\"start\":75526},{\"end\":75543,\"start\":75539},{\"end\":75554,\"start\":75547},{\"end\":75574,\"start\":75565},{\"end\":75585,\"start\":75576},{\"end\":75999,\"start\":75995},{\"end\":76011,\"start\":76007},{\"end\":76023,\"start\":76020},{\"end\":76037,\"start\":76033},{\"end\":76050,\"start\":76046},{\"end\":76069,\"start\":76060},{\"end\":76091,\"start\":76078},{\"end\":76106,\"start\":76098},{\"end\":76870,\"start\":76863},{\"end\":76884,\"start\":76879},{\"end\":76903,\"start\":76894},{\"end\":76922,\"start\":76917},{\"end\":77522,\"start\":77515},{\"end\":77536,\"start\":77529},{\"end\":77549,\"start\":77543},{\"end\":77566,\"start\":77557},{\"end\":77579,\"start\":77574},{\"end\":77594,\"start\":77589},{\"end\":77609,\"start\":77603},{\"end\":77627,\"start\":77617},{\"end\":77964,\"start\":77953},{\"end\":77981,\"start\":77973},{\"end\":77998,\"start\":77991},{\"end\":78014,\"start\":78008},{\"end\":78030,\"start\":78023},{\"end\":78043,\"start\":78037},{\"end\":78384,\"start\":78378},{\"end\":78394,\"start\":78391},{\"end\":78410,\"start\":78401},{\"end\":78954,\"start\":78951},{\"end\":78970,\"start\":78964},{\"end\":78983,\"start\":78980},{\"end\":78993,\"start\":78989},{\"end\":79011,\"start\":79002},{\"end\":79026,\"start\":79019},{\"end\":79537,\"start\":79533},{\"end\":79553,\"start\":79545},{\"end\":79574,\"start\":79568},{\"end\":79593,\"start\":79584},{\"end\":79607,\"start\":79601},{\"end\":79620,\"start\":79617},{\"end\":79636,\"start\":79631},{\"end\":79647,\"start\":79643},{\"end\":79659,\"start\":79655},{\"end\":79673,\"start\":79668},{\"end\":79683,\"start\":79679},{\"end\":79696,\"start\":79692},{\"end\":80304,\"start\":80297},{\"end\":80324,\"start\":80315},{\"end\":80335,\"start\":80328},{\"end\":80351,\"start\":80343},{\"end\":80364,\"start\":80357},{\"end\":80372,\"start\":80366},{\"end\":80663,\"start\":80658},{\"end\":80675,\"start\":80673},{\"end\":80685,\"start\":80681},{\"end\":80695,\"start\":80692},{\"end\":81076,\"start\":81071},{\"end\":81101,\"start\":81085},{\"end\":81114,\"start\":81109},{\"end\":81133,\"start\":81124},{\"end\":81148,\"start\":81144},{\"end\":81168,\"start\":81160}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":62239,\"start\":62166},{\"attributes\":{\"doi\":\"10.1109/12.29469\",\"id\":\"b1\",\"matched_paper_id\":5614052},\"end\":62583,\"start\":62241},{\"attributes\":{\"id\":\"b2\"},\"end\":62968,\"start\":62585},{\"attributes\":{\"doi\":\"arXiv:1512.01274\",\"id\":\"b3\"},\"end\":63475,\"start\":62970},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":222296313},\"end\":64030,\"start\":63477},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1701442},\"end\":64661,\"start\":64032},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":214797870},\"end\":64897,\"start\":64663},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b7\"},\"end\":65266,\"start\":64899},{\"attributes\":{\"id\":\"b8\"},\"end\":65324,\"start\":65268},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3996281},\"end\":65812,\"start\":65326},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4430053},\"end\":66155,\"start\":65814},{\"attributes\":{\"id\":\"b11\"},\"end\":66236,\"start\":66157},{\"attributes\":{\"id\":\"b12\"},\"end\":66432,\"start\":66238},{\"attributes\":{\"id\":\"b13\"},\"end\":66655,\"start\":66434},{\"attributes\":{\"doi\":\"arXiv:1703.07737\",\"id\":\"b14\"},\"end\":66951,\"start\":66657},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b15\"},\"end\":67464,\"start\":66953},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":31635004},\"end\":68189,\"start\":67466},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":234773176},\"end\":68725,\"start\":68191},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206595627},\"end\":69405,\"start\":68727},{\"attributes\":{\"id\":\"b19\"},\"end\":69610,\"start\":69407},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1428008},\"end\":70160,\"start\":69612},{\"attributes\":{\"doi\":\"arXiv:1901.00041\",\"id\":\"b21\"},\"end\":70585,\"start\":70162},{\"attributes\":{\"doi\":\"arXiv:1901.10008\",\"id\":\"b22\"},\"end\":70920,\"start\":70587},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52988395},\"end\":71533,\"start\":70922},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3639169},\"end\":72168,\"start\":71535},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":195908774},\"end\":72529,\"start\":72170},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1779661},\"end\":72703,\"start\":72531},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":204784145},\"end\":73204,\"start\":72705},{\"attributes\":{\"id\":\"b28\"},\"end\":73360,\"start\":73206},{\"attributes\":{\"id\":\"b29\"},\"end\":73438,\"start\":73362},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14796003},\"end\":73931,\"start\":73440},{\"attributes\":{\"id\":\"b31\"},\"end\":74075,\"start\":73933},{\"attributes\":{\"id\":\"b32\"},\"end\":74209,\"start\":74077},{\"attributes\":{\"id\":\"b33\"},\"end\":74299,\"start\":74211},{\"attributes\":{\"id\":\"b34\"},\"end\":74340,\"start\":74301},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14139651},\"end\":74916,\"start\":74342},{\"attributes\":{\"id\":\"b36\"},\"end\":75041,\"start\":74918},{\"attributes\":{\"id\":\"b37\"},\"end\":75202,\"start\":75043},{\"attributes\":{\"doi\":\"arXiv:1905.13348\",\"id\":\"b38\"},\"end\":75514,\"start\":75204},{\"attributes\":{\"doi\":\"arXiv:2102.01887\",\"id\":\"b39\"},\"end\":75914,\"start\":75516},{\"attributes\":{\"doi\":\"10.1145/3341301.3359658\",\"id\":\"b40\",\"matched_paper_id\":204812163},\"end\":76768,\"start\":75916},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1023605},\"end\":77204,\"start\":76770},{\"attributes\":{\"id\":\"b42\"},\"end\":77291,\"start\":77206},{\"attributes\":{\"id\":\"b43\"},\"end\":77416,\"start\":77293},{\"attributes\":{\"id\":\"b44\"},\"end\":77479,\"start\":77418},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":13756489},\"end\":77904,\"start\":77481},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4228546},\"end\":78313,\"start\":77906},{\"attributes\":{\"doi\":\"10.1145/349299.349318\",\"id\":\"b47\",\"matched_paper_id\":174802184},\"end\":78895,\"start\":78315},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4788641},\"end\":79462,\"start\":78897},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52987896},\"end\":80245,\"start\":79464},{\"attributes\":{\"id\":\"b50\"},\"end\":80547,\"start\":80247},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":196810567},\"end\":80993,\"start\":80549},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":11894138},\"end\":81526,\"start\":80995}]", "bib_title": "[{\"end\":62313,\"start\":62241},{\"end\":63558,\"start\":63477},{\"end\":64087,\"start\":64032},{\"end\":64718,\"start\":64663},{\"end\":65411,\"start\":65326},{\"end\":65862,\"start\":65814},{\"end\":67532,\"start\":67466},{\"end\":68228,\"start\":68191},{\"end\":68794,\"start\":68727},{\"end\":69685,\"start\":69612},{\"end\":71030,\"start\":70922},{\"end\":71606,\"start\":71535},{\"end\":72233,\"start\":72170},{\"end\":72544,\"start\":72531},{\"end\":72754,\"start\":72705},{\"end\":73471,\"start\":73440},{\"end\":74376,\"start\":74342},{\"end\":75985,\"start\":75916},{\"end\":76851,\"start\":76770},{\"end\":77506,\"start\":77481},{\"end\":77940,\"start\":77906},{\"end\":78371,\"start\":78315},{\"end\":78942,\"start\":78897},{\"end\":79523,\"start\":79464},{\"end\":80645,\"start\":80549},{\"end\":81063,\"start\":80995}]", "bib_author": "[{\"end\":62328,\"start\":62315},{\"end\":62337,\"start\":62328},{\"end\":62348,\"start\":62337},{\"end\":62673,\"start\":62664},{\"end\":62688,\"start\":62673},{\"end\":62701,\"start\":62688},{\"end\":62714,\"start\":62701},{\"end\":62728,\"start\":62714},{\"end\":63079,\"start\":63066},{\"end\":63086,\"start\":63079},{\"end\":63097,\"start\":63086},{\"end\":63106,\"start\":63097},{\"end\":63119,\"start\":63106},{\"end\":63132,\"start\":63119},{\"end\":63146,\"start\":63132},{\"end\":63155,\"start\":63146},{\"end\":63170,\"start\":63155},{\"end\":63183,\"start\":63170},{\"end\":63578,\"start\":63560},{\"end\":63588,\"start\":63578},{\"end\":63602,\"start\":63588},{\"end\":63612,\"start\":63602},{\"end\":63623,\"start\":63612},{\"end\":63638,\"start\":63623},{\"end\":63655,\"start\":63638},{\"end\":63664,\"start\":63655},{\"end\":64107,\"start\":64089},{\"end\":64117,\"start\":64107},{\"end\":64130,\"start\":64117},{\"end\":64150,\"start\":64130},{\"end\":64169,\"start\":64150},{\"end\":64181,\"start\":64169},{\"end\":64734,\"start\":64720},{\"end\":64751,\"start\":64734},{\"end\":64913,\"start\":64899},{\"end\":64929,\"start\":64913},{\"end\":64941,\"start\":64929},{\"end\":64961,\"start\":64941},{\"end\":65423,\"start\":65413},{\"end\":65431,\"start\":65423},{\"end\":65444,\"start\":65431},{\"end\":65458,\"start\":65444},{\"end\":65467,\"start\":65458},{\"end\":65873,\"start\":65864},{\"end\":65885,\"start\":65873},{\"end\":65897,\"start\":65885},{\"end\":65909,\"start\":65897},{\"end\":66182,\"start\":66159},{\"end\":66502,\"start\":66489},{\"end\":66676,\"start\":66657},{\"end\":66689,\"start\":66676},{\"end\":66704,\"start\":66689},{\"end\":67054,\"start\":67037},{\"end\":67068,\"start\":67054},{\"end\":67077,\"start\":67068},{\"end\":67098,\"start\":67077},{\"end\":67111,\"start\":67098},{\"end\":67126,\"start\":67111},{\"end\":67143,\"start\":67126},{\"end\":67157,\"start\":67143},{\"end\":67547,\"start\":67534},{\"end\":67572,\"start\":67547},{\"end\":67585,\"start\":67572},{\"end\":67608,\"start\":67585},{\"end\":67623,\"start\":67608},{\"end\":67642,\"start\":67623},{\"end\":67661,\"start\":67642},{\"end\":67673,\"start\":67661},{\"end\":68240,\"start\":68230},{\"end\":68252,\"start\":68240},{\"end\":68266,\"start\":68252},{\"end\":68280,\"start\":68266},{\"end\":68292,\"start\":68280},{\"end\":68305,\"start\":68292},{\"end\":68322,\"start\":68305},{\"end\":68812,\"start\":68796},{\"end\":68826,\"start\":68812},{\"end\":68836,\"start\":68826},{\"end\":68850,\"start\":68836},{\"end\":68869,\"start\":68850},{\"end\":68884,\"start\":68869},{\"end\":68897,\"start\":68884},{\"end\":68913,\"start\":68897},{\"end\":68924,\"start\":68913},{\"end\":68943,\"start\":68924},{\"end\":69702,\"start\":69687},{\"end\":69715,\"start\":69702},{\"end\":69724,\"start\":69715},{\"end\":69740,\"start\":69724},{\"end\":69757,\"start\":69740},{\"end\":70174,\"start\":70162},{\"end\":70186,\"start\":70174},{\"end\":70197,\"start\":70186},{\"end\":70217,\"start\":70197},{\"end\":70239,\"start\":70217},{\"end\":70255,\"start\":70239},{\"end\":70272,\"start\":70255},{\"end\":70284,\"start\":70272},{\"end\":70599,\"start\":70587},{\"end\":70611,\"start\":70599},{\"end\":70622,\"start\":70611},{\"end\":70638,\"start\":70622},{\"end\":70657,\"start\":70638},{\"end\":70669,\"start\":70657},{\"end\":71050,\"start\":71032},{\"end\":71066,\"start\":71050},{\"end\":71083,\"start\":71066},{\"end\":71104,\"start\":71083},{\"end\":71121,\"start\":71104},{\"end\":71137,\"start\":71121},{\"end\":71620,\"start\":71608},{\"end\":71634,\"start\":71620},{\"end\":71650,\"start\":71634},{\"end\":71666,\"start\":71650},{\"end\":71685,\"start\":71666},{\"end\":71696,\"start\":71685},{\"end\":72252,\"start\":72235},{\"end\":72268,\"start\":72252},{\"end\":72287,\"start\":72268},{\"end\":72558,\"start\":72546},{\"end\":72573,\"start\":72558},{\"end\":72590,\"start\":72573},{\"end\":72770,\"start\":72756},{\"end\":72786,\"start\":72770},{\"end\":72800,\"start\":72786},{\"end\":72820,\"start\":72800},{\"end\":72833,\"start\":72820},{\"end\":72843,\"start\":72833},{\"end\":73381,\"start\":73364},{\"end\":73482,\"start\":73473},{\"end\":73496,\"start\":73482},{\"end\":73514,\"start\":73496},{\"end\":73530,\"start\":73514},{\"end\":73542,\"start\":73530},{\"end\":73557,\"start\":73542},{\"end\":73564,\"start\":73557},{\"end\":74237,\"start\":74213},{\"end\":74393,\"start\":74378},{\"end\":74406,\"start\":74393},{\"end\":74417,\"start\":74406},{\"end\":74431,\"start\":74417},{\"end\":74449,\"start\":74431},{\"end\":74464,\"start\":74449},{\"end\":75058,\"start\":75043},{\"end\":75071,\"start\":75058},{\"end\":75269,\"start\":75251},{\"end\":75278,\"start\":75269},{\"end\":75289,\"start\":75278},{\"end\":75309,\"start\":75289},{\"end\":75320,\"start\":75309},{\"end\":75534,\"start\":75516},{\"end\":75545,\"start\":75534},{\"end\":75556,\"start\":75545},{\"end\":75576,\"start\":75556},{\"end\":75587,\"start\":75576},{\"end\":76001,\"start\":75987},{\"end\":76013,\"start\":76001},{\"end\":76025,\"start\":76013},{\"end\":76039,\"start\":76025},{\"end\":76052,\"start\":76039},{\"end\":76071,\"start\":76052},{\"end\":76093,\"start\":76071},{\"end\":76108,\"start\":76093},{\"end\":76872,\"start\":76853},{\"end\":76886,\"start\":76872},{\"end\":76905,\"start\":76886},{\"end\":76924,\"start\":76905},{\"end\":77524,\"start\":77508},{\"end\":77538,\"start\":77524},{\"end\":77551,\"start\":77538},{\"end\":77568,\"start\":77551},{\"end\":77581,\"start\":77568},{\"end\":77596,\"start\":77581},{\"end\":77611,\"start\":77596},{\"end\":77629,\"start\":77611},{\"end\":77966,\"start\":77942},{\"end\":77983,\"start\":77966},{\"end\":78000,\"start\":77983},{\"end\":78016,\"start\":78000},{\"end\":78032,\"start\":78016},{\"end\":78045,\"start\":78032},{\"end\":78386,\"start\":78373},{\"end\":78396,\"start\":78386},{\"end\":78412,\"start\":78396},{\"end\":78956,\"start\":78944},{\"end\":78972,\"start\":78956},{\"end\":78985,\"start\":78972},{\"end\":78995,\"start\":78985},{\"end\":79013,\"start\":78995},{\"end\":79028,\"start\":79013},{\"end\":79539,\"start\":79525},{\"end\":79555,\"start\":79539},{\"end\":79576,\"start\":79555},{\"end\":79595,\"start\":79576},{\"end\":79609,\"start\":79595},{\"end\":79622,\"start\":79609},{\"end\":79638,\"start\":79622},{\"end\":79649,\"start\":79638},{\"end\":79661,\"start\":79649},{\"end\":79675,\"start\":79661},{\"end\":79685,\"start\":79675},{\"end\":79698,\"start\":79685},{\"end\":80306,\"start\":80291},{\"end\":80326,\"start\":80306},{\"end\":80337,\"start\":80326},{\"end\":80353,\"start\":80337},{\"end\":80366,\"start\":80353},{\"end\":80374,\"start\":80366},{\"end\":80665,\"start\":80647},{\"end\":80677,\"start\":80665},{\"end\":80687,\"start\":80677},{\"end\":80697,\"start\":80687},{\"end\":81078,\"start\":81065},{\"end\":81103,\"start\":81078},{\"end\":81116,\"start\":81103},{\"end\":81135,\"start\":81116},{\"end\":81150,\"start\":81135},{\"end\":81170,\"start\":81150}]", "bib_venue": "[{\"end\":63763,\"start\":63722},{\"end\":64291,\"start\":64281},{\"end\":65582,\"start\":65533},{\"end\":65992,\"start\":65959},{\"end\":67785,\"start\":67773},{\"end\":68489,\"start\":68414},{\"end\":69084,\"start\":69022},{\"end\":69906,\"start\":69840},{\"end\":71806,\"start\":71796},{\"end\":72972,\"start\":72916},{\"end\":73707,\"start\":73644},{\"end\":74576,\"start\":74564},{\"end\":76302,\"start\":76202},{\"end\":78078,\"start\":78070},{\"end\":78626,\"start\":78538},{\"end\":79138,\"start\":79128},{\"end\":79810,\"start\":79798},{\"end\":62187,\"start\":62168},{\"end\":62382,\"start\":62364},{\"end\":62662,\"start\":62585},{\"end\":63064,\"start\":62970},{\"end\":63720,\"start\":63664},{\"end\":64279,\"start\":64181},{\"end\":64762,\"start\":64751},{\"end\":65057,\"start\":64977},{\"end\":65281,\"start\":65268},{\"end\":65531,\"start\":65467},{\"end\":65957,\"start\":65909},{\"end\":66291,\"start\":66238},{\"end\":66487,\"start\":66434},{\"end\":66776,\"start\":66720},{\"end\":67035,\"start\":66953},{\"end\":67771,\"start\":67673},{\"end\":68412,\"start\":68322},{\"end\":69020,\"start\":68943},{\"end\":69453,\"start\":69407},{\"end\":69838,\"start\":69757},{\"end\":70347,\"start\":70300},{\"end\":70728,\"start\":70685},{\"end\":71207,\"start\":71137},{\"end\":71794,\"start\":71696},{\"end\":72336,\"start\":72287},{\"end\":72596,\"start\":72590},{\"end\":72914,\"start\":72843},{\"end\":73270,\"start\":73206},{\"end\":73642,\"start\":73564},{\"end\":73967,\"start\":73933},{\"end\":74109,\"start\":74079},{\"end\":74310,\"start\":74301},{\"end\":74562,\"start\":74464},{\"end\":74945,\"start\":74918},{\"end\":75112,\"start\":75071},{\"end\":75249,\"start\":75204},{\"end\":75690,\"start\":75603},{\"end\":76200,\"start\":76131},{\"end\":76979,\"start\":76924},{\"end\":77226,\"start\":77208},{\"end\":77328,\"start\":77293},{\"end\":77433,\"start\":77418},{\"end\":77678,\"start\":77629},{\"end\":78068,\"start\":78045},{\"end\":78536,\"start\":78433},{\"end\":79126,\"start\":79028},{\"end\":79796,\"start\":79698},{\"end\":80289,\"start\":80247},{\"end\":80757,\"start\":80697},{\"end\":81240,\"start\":81170}]"}}}, "year": 2023, "month": 12, "day": 17}