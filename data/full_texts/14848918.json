{"id": 14848918, "updated": "2023-09-30 06:03:14.286", "metadata": {"title": "Normalized cuts and image segmentation", "authors": "[{\"first\":\"Jianbo\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"J.\",\"last\":\"Malik\",\"middle\":[]}]", "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "journal": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 1997, "month": null, "day": null}, "abstract": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2998263090", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ShiM97", "doi": "10.1109/cvpr.1997.609407"}}, "content": {"source": {"pdf_hash": "57e0f3280200c2d00305c25c0f06d9a340ff370b", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf", "status": "GREEN"}}, "grobid": {"id": "bad8ce3099507049bff63f461a00b8b2ebcbe064", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/57e0f3280200c2d00305c25c0f06d9a340ff370b.txt", "contents": "\nNormalized Cuts and Image Segmentation\nAUGUST 2000\n\nJianbo Shi jshi@cs.cmu.edu \nUniversity of California at Berkeley\n15213., 94720Pittsburgh, BerkeleyPA, CA\n\nMember, IEEEJitendra Malik malik@cs.berkeley.edu. \nUniversity of California at Berkeley\n15213., 94720Pittsburgh, BerkeleyPA, CA\n\nForbes Ave \nUniversity of California at Berkeley\n15213., 94720Pittsburgh, BerkeleyPA, CA\n\nNormalized Cuts and Image Segmentation\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n228AUGUST 2000Manuscript received 4 Feb. 1998; accepted 16 Nov. 1999.. J. Shi is with the Robotics Institute, Carnegie Mellon University, 5000 . J. Malik is with the Electrical Engineering and Computer Science Division, Recommended for acceptance by M. Shah. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number 107618. 0162-8828/00/$10.00 \u00df 2000 IEEE\nAbstract\u00d0We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.\n\nae 1I NTRODUCTION N EARLY 75 years ago, Wertheimer [24] pointed out the importance of perceptual grouping and organization in vision and listed several key factors, such as similarity, proximity, and good continuation, which lead to visual grouping. However, even to this day, many of the computational issues of perceptual grouping have remained unresolved. In this paper, we present a general framework for this problem, focusing specifically on the case of image segmentation.\n\nSince there are many possible partitions of the domain I of an image into subsets, how do we pick the \u00aaright\u00ba one? There are two aspects to be considered here. The first is that there may not be a single correct answer. A Bayesian view is appropriate\u00d0there are several possible interpretations in the context of prior world knowledge. The difficulty, of course, is in specifying the prior world knowledge. Some of it is low level, such as coherence of brightness, color, texture, or motion, but equally important is mid-or highlevel knowledge about symmetries of objects or object models. The second aspect is that the partitioning is inherently hierarchical. Therefore, it is more appropriate to think of returning a tree structure corresponding to a hierarchical partition instead of a single \u00aaflat\u00ba partition.\n\nThis suggests that image segmentation based on lowlevel cues cannot and should not aim to produce a complete final \u00aacorrect\u00ba segmentation. The objective should instead be to use the low-level coherence of brightness, color, texture, or motion attributes to sequentially come up with hierarchical partitions. Mid-and high-level knowledge can be used to either confirm these groups or select some for further attention. This attention could result in further repartitioning or grouping. The key point is that image partitioning is to be done from the big picture downward, rather like a painter first marking out the major areas and then filling in the details.\n\nPrior literature on the related problems of clustering, grouping and image segmentation is huge. The clustering community [12] has offered us agglomerative and divisive algorithms; in image segmentation, we have region-based merge and split algorithms. The hierarchical divisive approach that we advocate produces a tree, the dendrogram. While most of these ideas go back to the 1970s (and earlier), the 1980s brought in the use of Markov Random Fields [10] and variational formulations [17], [2], [14]. The MRF and variational formulations also exposed two basic questions:\n\n1. What is the criterion that one wants to optimize? 2. Is there an efficient algorithm for carrying out the optimization? Many an attractive criterion has been doomed by the inability to find an effective algorithm to find its mini-mum\u00d0greedy or gradient descent type approaches fail to find global optima for these high-dimensional, nonlinear problems.\n\nOur approach is most related to the graph theoretic formulation of grouping. The set of points in an arbitrary feature space are represented as a weighted undirected graph G G V V;E E, where the nodes of the graph are the points in the feature space, and an edge is formed between every pair of nodes. The weight on each edge, wi i; j j,i sa function of the similarity between nodes i i and j j.\n\nIn grouping, we seek to partition the set of vertices into disjoint sets V 1 ; V 2 ; ...; V m , where by some measure the similarity among the vertices in a set V i is high and, across different sets V i , V j is low.\n\nTo partition a graph, we need to also ask the following questions:\n\n1. What is the precise criterion for a good partition? 2. How can such a partition be computed efficiently? In the image segmentation and data clustering community, there has been much previous work using variations of the minimal spanning tree or limited neighborhood set approaches. Although those use efficient computational methods, the segmentation criteria used in most of them are based on local properties of the graph. Because perceptual grouping is about extracting the global impressions of a scene, as we saw earlier, this partitioning criterion often falls short of this main goal.\n\nIn this paper, we propose a new graph-theoretic criterion for measuring the goodness of an image partition\u00d0the normalized cut. We introduce and justify this criterion in Section 2. The minimization of this criterion can be formulated as a generalized eigenvalue problem. The eigenvectors can be used to construct good partitions of the image and the process can be continued recursively as desired (Section 2.1). Section 3 gives a detailed explanation of the steps of our grouping algorithm. In Section 4, we show experimental results. The formulation and minimization of the normalized cut criterion draws on a body of results from the field of spectral graph theory (Section 5). Relationship to work in computer vision is discussed in Section 6 and comparison with related eigenvector based segmentation methods is represented in Section 6.1. We conclude in Section 7.\n\nThe main results in this paper were first presented in [20].\n\n\n2G ROUPING AS GRAPH PARTITIONING\n\nA graph G V; E can be partitioned into two disjoint sets, A; B, A B V , A B Y, by simply removing edges connecting the two parts. The degree of dissimilarity between these two pieces can be computed as total weight of the edges that have been removed. In graph theoretic language, it is called the cut:\ncutA; B X uPA;vPB wu; v: 1\nThe optimal bipartitioning of a graph is the one that minimizes this cut value. Although there are an exponential number of such partitions, finding the minimum cut of a graph is a well-studied problem and there exist efficient algorithms for solving it. Wu and Leahy [25] proposed a clustering method based on this minimum cut criterion. In particular, they seek to partition a graph into k-subgraphs such that the maximum cut across the subgroups is minimized. This problem can be efficiently solved by recursively finding the minimum cuts that bisect the existing segments. As shown in Wu and Leahy's work, this globally optimal criterion can be used to produce good segmentation on some of the images.\n\nHowever, as Wu and Leahy also noticed in their work, the minimum cut criteria favors cutting small sets of isolated nodes in the graph. This is not surprising since the cut defined in (1) increases with the number of edges going across the two partitioned parts. Fig. 1 illustrates one such case. Assuming the edge weights are inversely proportional to the distance between the two nodes, we see the cut that partitions out node n 1 or n 2 will have a very small value. In fact, any cut that partitions out individual nodes on the right half will have smaller cut value than the cut that partitions the nodes into the left and right halves.\n\nTo avoid this unnatural bias for partitioning out small sets of points, we propose a new measure of disassociation between two groups. Instead of looking at the value of total edge weight connecting the two partitions, our measure computes the cut cost as a fraction of the total edge connections to all the nodes in the graph. We call this disassociation measure the normalized cut (Ncut):\nNcutA; B cutA; B assocA; V cutA; B assocB; V ; 2\nwhere assocA; V P uPA;tPV wu; t is the total connection from nodes in A to all nodes in the graph and assocB; V is similarly defined. With this definition of the disassociation between the groups, the cut that partitions out small isolated points will no longer have small Ncut value, since the cut value will almost certainly be a large percentage of the total connection from that small set to all other nodes. In the case illustrated in Fig. 1, we see that the cut 1 value across node n 1 will be 100 percent of the total connection from that node.\n\nIn the same spirit, we can define a measure for total normalized association within groups for a given partition: Hence, the two partition criteria that we seek in our grouping algorithm, minimizing the disassociation between the groups and maximizing the association within the groups, are in fact identical and can be satisfied simultaneously. In our algorithm, we will use this normalized cut as the partition criterion.\nNassocA\nUnfortunately, minimizing normalized cut exactly is NPcomplete, even for the special case of graphs on grids. The proof, due to Papadimitriou, can be found in Appendix A.\n\nHowever, we will show that, when we embed the normalized cut problem in the real value domain, an approximate discrete solution can be found efficiently.\n\n\nComputing the Optimal Partition\n\nGiven a partition of nodes of a graph, V, into two sets A and B, let x x be an N jV V j dimensional indicator vector, x i 1 if node i is in A and \u00c01, otherwise. Let d di P j wi; j be the total connection from node i i to all other nodes. With the definitions x x and d d, we can rewrite NcutA; B as:\nNcutA; B cutA; B assocA; V cutB; A assocB; V P x xi>0;x xj<0 \u00c0w ij x x i x x j P x xi>0 d d i P x xi<0;x xj>0 \u00c0w ij x x i x x j P x xi<0 d d i : Let D be an N \u00c2 N diagonal matrix with d d on its diagonal, W be an N \u00c2 N symmetrical matrix with Wi; jw ij , k P x i >0 d d i P i d d i ;\nand 1 be an N \u00c2 1 vector of all ones. Using the fact 1 1x x 2 and 1 1\u00c0x x 2 are indicator vectors for x i > 0 and x i < 0, respectively, we can rewrite 4Ncutx x as:\n1 x x T D \u00c0 W1 x x k1 T D1 1 \u00c0 x x T D \u00c0 W1 \u00c0 x x 1 \u00c0 k1 T D1 x x T D \u00c0 Wx x 1 T D \u00c0 W1 k1 \u00c0 k1 T D1 21 \u00c0 2k1 T D \u00c0 Wx x k1 \u00c0 k1 T D1 :\n\nLet\n\nx xx x T D \u00c0 Wx x;\n\nx x1 T D \u00c0 Wx x;\n1 T D \u00c0 W1;\nand M 1 T D1;\n\nwe can then further expand the above equation as:\nx x21 \u00c0 2kx x k1 \u00c0 kM x x21 \u00c0 2kx x k1 \u00c0 kM \u00c0 2x x M 2x x M 2 M :\nDropping the last constant term, which in this case equals 0,\nwe get 1 \u00c0 2k 2k 2 x x21 \u00c0 2kx x k1 \u00c0 kM 2x x M 1\u00c02k2k 2 1\u00c0k 2 x x 21\u00c02k 1\u00c0k 2 x x k 1\u00c0k M 2x x M :\nLetting b k 1\u00c0k , and since 0, it becomes\n1 b 2 x x21 \u00c0 b 2 x x bM 2bx x bM 1 b 2 x x bM 21 \u00c0 b 2 x x bM 2bx x bM \u00c0 2b bM 1 b 2 x x T D \u00c0 Wx x 1 T D \u00c0 W1 b1 T D1 21 \u00c0 b 2 1 T D \u00c0 Wx x b1 T D1 2bx x T D \u00c0 Wx x b1 T D1 \u00c0 2b1 T D \u00c0 W1 b1 T D1 1 x x T D \u00c0 W1 x x b1 T D1 b 2 1 \u00c0 x x T D \u00c0 W1 \u00c0 x x b1 T D1 \u00c0 2b1 \u00c0 x x T D \u00c0 W1 x x b1 T D1 1 x x\u00c0b1 \u00c0 x x T D \u00c0 W1 x x\u00c0b1 \u00c0 x x b1 T D1 :\nSetting y y 1 x x\u00c0b1 \u00c0 x x, it is easy to see that\ny y T D1 X x i >0 d d i \u00c0 b X x i <0 d d i 0 4 since b k 1\u00c0k P x i >0 d di P x i <0 d d i and y y T Dy y X xi>0 d d i b 2 X xi<0 d d i b X xi<0 d d i b 2 X xi<0 d d i b X xi<0 d d i b X xi<0 d d i b1 T D1:\nPutting everything together we have, min x x Ncutx xmin y y y y T D \u00c0 Wy y y y T Dy y ; 5\n\nwith the condition y yiPf1; \u00c0bg and y y T D1 0. Note that the above expression is the Rayleigh quotient [11]. If y y is relaxed to take on real values, we can minimize (5) by solving the generalized eigenvalue system, D \u00c0 Wy y Dy y: 6\n\nHowever, we have two constraints on y y which come from the condition on the corresponding indicator vector x x. First, consider the constraint y y T D1 0. We can show this constraint on y y is automatically satisfied by the solution of the generalized eigensystem. We will do so by first transforming (6) into a standard eigensystem and showing the corresponding condition is satisfied there. Rewrite (6) as\nD \u00c0 1 2 D \u00c0 WD \u00c0 1 2 z z z z; 7\nwhere z z D 1 2 y y. One can easily verify that z z 0 D 1 2 1 is an eigenvector of (7) with eigenvalue of 0. Furthermore, D \u00c0 1 2 D \u00c0 WD \u00c0 1 2 is symmetric positive semidefinite since D \u00c0 W, also called the Laplacian matrix, is known to be positive semidefinite [18]. Hence, z z 0 is, in fact, the smallest eigenvector of (7) and all eigenvectors of (7) are perpendicular to each other. In particular, z z 1 , the second smallest eigenvector, is perpendicular to z z 0 . Translating this statement back into the general eigensystem (6), we have: 1) y y 0 1 is the smallest eigenvector with eigenvalue of 0 and 2) 0 z z T 1 z z 0 y y T 1 D1, where y y 1 is the second smallest eigenvector of (6). Now, recall a simple fact about the Rayleigh quotient [11]:\n\nLet A be a real symmetric matrix. Under the constraint that x x is orthogonal to the j-1 smallest eigenvectors x x 1 ; ...;x x j\u00c01 , the quotient x x T Ax x x x T x x is minimized by the next smallest eigenvector x x j and its minimum value is the corresponding eigenvalue j .\n\nAs a result, we obtain: z z 1 arg:min z z T z z00 z z T D \u00c0 1 2 D \u00c0 WD \u00c0 1 2 z z z z T z z 8\n\nand, consequently, y y 1 arg:min y y T D10 y y T D \u00c0 Wy y y y T Dy y : 9\n\nThus, the second smallest eigenvector of the generalized eigensystem (6) is the real valued solution to our normalized cut problem. The only reason that it is not necessarily the solution to our original problem is that the second constraint on y y that y yi takes on two discrete values is not automatically satisfied. In fact, relaxing this constraint is what makes this optimization problem tractable in the first place. We will show in Section 3 how this real valued solution can be transformed into a discrete form.\n\nA similar argument can also be made to show that the eigenvector with the third smallest eigenvalue is the real valued solution that optimally subpartitions the first two parts. In fact, this line of argument can be extended to show that one can subdivide the existing graphs, each time using the eigenvector with the next smallest eigenvalue. However, in practice, because the approximation error from the real valued solution to the discrete valued solution accumulates with every eigenvector taken and all eigenvectors have to satisfy a global mutual orthogonality constraint, solutions based on higher eigenvectors become unreliable. It is best to restart solving the partitioning problem on each subgraph individually.\n\nIt is interesting to note that, while the second smallest eigenvector y y of (6) only approximates the optimal normalized cut solution, it exactly minimizes the following problem:\ninf y y T D10 P i P j y yi\u00c0y yj 2 w ij P i y yi 2 di ; 10\nin real-valued domain, where diDi; i.R o u g h l y speaking, this forces the indicator vector y y to take similar values for nodes i and j that are tightly coupled (large w ij ). In summary, we propose using the normalized cut criterion for graph partitioning and we have shown how this criterion can be computed efficiently by solving a generalized eigenvalue problem.\n\n\n3T HE GROUPING ALGORITHM\n\nOur grouping algorithm consists of the following steps:\n\n1. Given an image or image sequence, set up a weighted graph G V; E and set the weight on the edge connecting two nodes to be a measure of the similarity between the two nodes. 2. Solve D \u00c0 Wx x Dx x for eigenvectors with the smallest eigenvalues. 3. Use the eigenvector with the second smallest eigenvalue to bipartition the graph. 4. Decide if the current partition should be subdivided and recursively repartition the segmented parts if necessary. The grouping algorithm, as well as its computational complexity, can be best illustrated by using the following example. 1. Construct a weighted graph G V; E by taking each pixel as a node and connecting each pair of pixels by an edge. The weight on that edge should reflect the likelihood that the two pixels belong to one object. Using just the brightness value of the pixels and their spatial location, we can define the graph edge weight connecting the two nodes i and j as:\n\n\nExample: Brightness Images\nw ij e \u00c0 kF F i \u00c0F F j k 2 2 2 I \u00c3 e \u00c0 kX X i \u00c0X X j k 2 2 2 X if kX Xi\u00c0X Xjk 2 <r 0 otherwise: 8 < : 11 2.\nSolve for the eigenvectors with the smallest eigenvalues of the system D \u00c0 Wy y Dy y: 12\n\nAs we saw above, the generalized eigensystem in (12) can be transformed into a standard eigenvalue problem of\nD \u00c0 1 2 D \u00c0 WD \u00c0 1 2 x x x x: 13\nSolving a standard eigenvalue problem for all eigenvectors takes On 3 operations, where n is the number of nodes in the graph. This becomes impractical for image segmentation applications where n is the number of pixels in an image.\n\nFortunately, our graph partitioning has the following properties: 1) The graphs are often only locally connected and the resulting eigensystems are very sparse, 2) only the top few eigenvectors are needed for graph partitioning, and 3) the precision requirement for the eigenvectors is low, often only the right sign bit is required. These special properties of our problem can be fully exploited by an eigensolver called the Lanczos method. The running time of a Lanczos algorithm is OmnOmMn [11], where m is the maximum number of matrix-vector computations required and Mn is the cost of a matrix-vector computation of Ax x, where A D \u00c0 1 2 D \u00c0 WD \u00c0 1 2 . Note that the sparsity structure of A is identical to that of the weight matrix W. Since W is sparse, so is A and the matrix-vector computation is only On.\n\nTo see why this is the case, we will look at the cost of the inner product of one row of A A with a vector x.\nLet y i A A i \u00c1 x P j A A ij x j . For a fixed i, A A ij is only nonzero if node j is in a spatial neighborhood of i.\nHence, there are only a fixed number of operations required for each A A i \u00c1 x and the total cost of computing A Ax is On.\n\nThe constant factor is determined by the size of the spatial neighborhood of a node. It turns out that we can substantially cut down additional connections from each node to its neighbors by randomly selecting the connections within the neighborhood for the weighted graph. Empirically, we have found that one can remove up to 90 percent of the total connections with each of the neighborhoods when the neighborhoods are large without affecting the eigenvector solution to the system.\n\nPutting everything together, each of the matrixvector computations cost On operations with a small constant factor. The number m depends on many factors [11]. In our experiments on image segmentation, we observed that m is typically less than On 1 2 . Fig. 3 shows the smallest eigenvectors computed for the generalized eigensystem with the weight matrix defined above.\n\n\n3.\n\nOnce the eigenvectors are computed, we can partition the graph into two pieces using the second smallest eigenvector. In the ideal case, the eigenvector should only take on two discrete values and the signs of the values can tell us exactly how to partition the graph. However, our eigenvectors can take on continuous values and we need to choose a splitting point to partition it into two parts. There are many different ways of choosing such a splitting point. One can take 0 or the median value as the splitting point or one can search for the splitting point such that the resulting partition has the best NcutA; B value. We take the latter approach in our work. Currently, the search is done by checking l evenly spaced possible splitting points, and computing the best Ncut among them. In our experiments, the values in the eigenvectors are usually well separated and this method of choosing a splitting point is very reliable even with a small l. 4. After the graph is broken into two pieces, we can recursively run our algorithm on the two partitioned parts. Or, equivalently, we could take advantage of the special properties of the other top eigenvectors as explained in the previous section to subdivide the graph based on those eigenvectors. The recursion stops once the Ncut value exceeds certain limit. We also impose a stability criterion on the partition. As we saw earlier, and as we see in the eigenvectors with the seventh to ninth smallest eigenvalues ( Fig. 3g-h), sometimes an eigenvector can take on the shape of a continuous function, rather that the discrete indicator function that we seek. From the view of segmentation, such an eigenvector is attempting to subdivide an image region where there is no sure way of breaking it. In fact, if we are forced to partition the image based on this eigenvector, we will see there are many different splitting points which have similar Ncut values. Hence, the partition will be highly uncertain and unstable. In our current segmentation scheme, we simply choose to ignore all those eigenvectors which have smoothly varying eigenvector values. We achieve this by imposing a stability criterion which measures the degree of smoothness in the eigenvector values. The simplest measure is based on first computing the histogram of the eigenvector values and then computing the ratio between the minimum and maximum values in the bins. When the eigenvector values are continuously varying, the values in the histogram bins will stay relatively the same and the ratio will be relatively high. In our experiments, we find that simple thresholding on the ratio described above can be used to exclude unstable eigenvectors. We have set that value to be 0.06 in all our experiments. Fig. 4 shows the final segmentation for the image shown in Fig. 2.\n\n\nRecursive Two-Way Ncut\n\nIn summary, our grouping algorithm consists of the following steps: 1. Given a set of features, set up a weighted graph G V; E, compute the weight on each edge, and summarize the information into W and D. 2. Solve D \u00c0 Wx x Dx x for eigenvectors with the smallest eigenvalues. 3. Use the eigenvector with the second smallest eigenvalue to bipartition the graph by finding the splitting point such that Ncut is minimized. 4. Decide if the current partition should be subdivided by checking the stability of the cut, and make sure Ncut is below the prespecified value. 5. Recursively repartition the segmented parts if necessary. The number of groups segmented by this method is controlled directly by the maximum allowed Ncut.\n\n\nSimultanous K-Way Cut with Multiple Eigenvectors\n\nOne drawback of the recursive 2-way cut is its treatment of the oscillatory eigenvectors. The stability criteria keeps us from cutting oscillatory eigenvectors, but it also prevents us cutting the subsequent eigenvectors which might be perfect partitioning vectors. Also, the approach is computationally wasteful; only the second eigenvector is used, whereas the next few small eigenvectors also contain useful partitioning information.\n\nInstead of finding the partition using recursive 2-way cut as described above, one can use all of the top eigenvectors to simultanously obtain a K-way partition. In this method, the n top eigenvectors are used as n dimensional indicator vectors for each pixel. In the first step, a simple clustering algorithm, such as the k-means algorithm, is used to obtain an oversegmentation of the image into k H groups. No attempt is made to identify and exclude oscillatory eigenvectors\u00d0they exacerbate the oversegmentation, but that will be dealt with subsequently.\n\nIn the second step, one can proceed in the following two ways:\n\n1. Greedy pruning: Iteratively merge two segments at a time until only k segments are left. At each merge step, those two segments are merged that minimize the k-way Ncut criterion defined as: where A i is the ith subset of whole set V. This computation can be efficiently carried out by iteratively updating the compacted weight matrix W c , with W c i; jassocA i ; A j . 2. Global recursive cut. From the initial k H segments, we can build a condensed graph G c V c ; E c , where each segment A i corresponds to a node V c i of the graph. The weight on each graph edge W c i; j is defined to be assocA i ; A j , the total edge weights from elements in A i to elements in A j . From this condensed graph, we then recursively bipartition the graph according the Ncut criterion. This can be carried out either with the generalized eigenvalue system, as in Section 3.2, or with exhaustive search in the discrete domain. Exhaustive search is possible in this case since k H is small, typically k H 100. We have experimented with this simultanous k-way cut method on our recent test images. However, the results presented in this paper are all based on the recursive 2-way partitioning algorithm outlined in Section 3.2.\nNcut k cutA 1 ; V \u00c0 A 1 assocA 1 ; V cutA 2 ; V \u00c0 A 2 assocA 2 ; V ...\n\n4E XPERIMENTS\n\nWe have applied our grouping algorithm to image segmentation based on brightness, color, texture, or motion information. In the monocular case, we construct the graph G V; E by taking each pixel as a node and define the edge weight w ij between node i and j as the product of a feature similarity term and spatial proximity term:\nw ij e \u00c0kF F i\u00c0F F jk 2 2 I \u00c3 e \u00c0kX Xi\u00c0X Xjk 2 2 X if kX Xi\u00c0X Xjk 2 <r 0 otherwise;\n( where X Xi is the spatial location of node i, and F F i is a feature vector based on intensity, color, or texture information at that node defined as:\n\n. F F i1, in the case of segmenting point sets,\n\n. F F iI Ii, the intensity value, for segmenting brightness images,\n. F F i v; v \u00c1 s \u00c1 sinh;v\u00c1 s \u00c1 cosh\ni,w h e r eh; s; v are the HSV values, for color segmentation, . F F i jI I \u00c3 f 1 j; ...; jI I \u00c3 f n j i,w h e r et h ef i are DOOG filters at various scales and orientations as used in [16], in the case of texture segmentation. Note that the weight w ij 0 for any pair of nodes i and j that are more than r pixels apart.\n\nWe first tested our grouping algorithm on spatial point sets. Fig. 5 shows a point set and the segmentation result. The normalized cut criterion is indeed able to partition the point set in a desirable way.\n\nFigs. 4, 6, 7, and 8 show the result of our segmentation algorithm on various brightness images. Figs. 6 and 7 are synthetic images with added noise. Figs. 4 and 8 are natural images. Note that the \u00aaobjects\u00ba in Fig. 8 have rather illdefined boundaries, which would make edge detection perform poorly. Fig. 9 shows the segmentation on a color image, reproduced in gray scale in these transactions. The original image and many other examples can be found at web site http://www.cs.berkeley.edu/~jshi/Grouping.\n\nNote that, in all these examples, the algorithm is able to extract the major components of scene while ignoring small intracomponent variations. As desired, recursive partitioning can be used to further decompose each piece. Fig. 10 shows preliminary results on texture segmentation for a natural image of a zebra against a background. Note that the measure we have used is orientation-variant and, therefore, parts of the zebra skin with different stripe orientation should be marked as separate regions.\n\nIn the motion case, we will treat the image sequence as a spatiotemporal data set. Given an image sequence, a weighted graph is constructed by taking each pixel in the image sequence as a node and connecting pixels that are in the spatiotemporal neighborhood of each other. The weight on each graph edge is defined as: ( where di; j is the \u00aamotion distance\u00ba between two pixels i and j. Note that X X i in this case represents the spatialtemporal position of pixel i. To compute this \u00aamotion distance,\u00ba we will use a motion feature called motion profile.B ymotion profile we seek to estimate the probability distribution of image velocity at each pixel. Let I I t X denote a image window centered at the pixel at location X P R 2 at time t. We denote by P i dx the motion profile of an image patch at node i, I I t X i , at time t corresponding to another image patch I I t1 X i dx at time t 1. P i dx can be estimated by first computing the similarity S i dx between I I t X i and I I t1 X i dx and normalizing it to get a probability distribution:\nP i dx S i dx P dx S i dx : 15\nThere are many ways one can compute similarity between two image patches; we will use a measure that is based on the sum of squared differences (SSD):\nS i dxexp \u00c0 X w I I t X i w \u00c0 I I t1 X i dx w 2 = 2 ssd ; 16\nwhere w P R 2 is within a local neighborhood of image patch I I t X i . The \u00aamotion distance\u00ba between two image pixels is then defined as one minus the cross-correlation of the motion profiles:\n\ndi; j1 \u00c0 X dx P i dxP j dx: 17\n\nIn Fig. 11, we show results of the normalized cut algorithm on a synthetic random dot motion sequence and a indoor motion sequence, respectively. For more elaborate discussion on motion segmentation using normalized cut, as well as how to segment and track over long image sequences, readers might want to refer to our paper [21].\n\n\nComputation Time\n\nAs we saw from Section 3.1, the running time of the normalized cut algorithm is Omn, where n is the number   of pixels and m is the number of steps Lanczos takes to converge. On the 100 \u00c2 120 test images shown here, the normalized cut algorithm takes about 2 minutes on Intel Pentium 200MHz machines.\n\nA multiresolution implementation can be used to reduce this running time further on larger images. In our current experiments, with this implementation, the running time on a 300 \u00c2 400 image can be reduced to about 20 seconds on Intel Pentium 300MHz machines. Furthermore, the bottleneck of the computation, a sparse matrix-vector multiplication step, can be easily parallelized taking advantage of future computer chip designs.\n\nIn our current implementation, the sparse eigenvalue decomposition is computed using the LASO2 numerical package developed by Scott.\n\n\nChoice of Graph Edge Weight\n\nIn the examples shown here, we used an exponential function of the form of wxe \u00c0dx= 2 on the weighted graph edge with feature similarity of dx. The value of is    10. (a) shows an image of a zebra. The remaining images show the major components of the partition. The texture features used correspond to convolutions with DOOG filters [16] at six orientations and five scales.\n\ntypically set to 10 to 20 percent of the total range of the feature distance function dx. The exponential weighting function is chosen here for its relative simplicity, as well as neutrality, since the focus of this paper is on developing a general segmentation procedure, given a feature similarity measure. We found this choice of weight function is quite adequate for typical image and feature spaces. Section 6.1 shows the effect of using different weighting functions and parameters on the output of the normalized cut algorithm.\n\nHowever, the general problem of defining feature similarity incorporating a variety of cues is not a trivial one. The grouping cues could be of different abstraction levels and types and they could be in conflict with each other. Furthermore, the weighting function could vary from image region to image region, particularly in a textured image. Some of these issues are addressed in [15].\n\n\n5R ELATIONSHIP TO SPECTRAL GRAPH THEORY\n\nThe computational approach that we have developed for image segmentation is based on concepts from spectral graph theory. The core idea is to use matrix theory and linear algebra to study properties of the incidence matrix, W, and the Laplacian matrix, D \u00c0 W, of the graph and relate them back to various properties of the original graph. This is a rich area of mathematics and the idea of using eigenvectors of the Laplacian for finding partitions of graphs can be traced back to Cheeger [4], Donath and Hoffman [7], and Fiedler [9]. This area has also seen contributions by theoretical computer scientists [1], [3], [22], [23]. It can be shown that our notion of normalized cut is related by a constant factor to the concept of conductance in [22].\n\nFor a tutorial introduction to spectral graph theory, we recommend the recent monograph by Chung [5]. In this monograph, Chung proposes a \u00aanormalized\u00ba definition of the Laplacian, as D \u00c0 1 2 D \u00c0 WD \u00c0 1 2 . The eigenvectors for this \u00aanormalized\u00ba Laplacian, when multiplied by D \u00c0 1 2 ,a r e exactly the generalized eigenvectors we used to compute normalized cut. Chung points out that the eigenvalues of this \u00aanormalized\u00ba Laplacian relate well to graph invariants for general graph in ways that eigenvalues of the standard Laplacian have failed to do.\n\nSpectral graph theory provides us some guidance on the goodness of the approximation to the normalized cut provided by the second eigenvalue of the normalized Laplacian. One way is through bounds on the normalized Cheeger constant [5] which, in our terminology, can be defined as h G inf cutA; B minassocA; V ; assocB; V : 18\n\nThe eigenvalues of (6) are related to the Cheeger constant by the inequality [5]:\n2h G ! 1 > h 2 G 2 : 19\nEarlier work on spectral partitioning used the second eigenvectors of the Laplacian of the graph defined as D \u00c0 W to partition a graph. The second smallest eigenvalue of D \u00c0 W is sometimes known as the Fiedler value. Several results have been derived relating the ratio cut and the Fiedler value. A ratio cut of a partition of V , P A; V \u00c0 A, which, in fact, is the standard definition of the Cheeger constant, is defined as cutA;V \u00c0A minjAj;jV \u00c0Aj . It was shown that if the Fiedler value is small, partitioning the graph based on the Fiedler vector will lead to good ratio cut [1], [23]. Our derivation in Section 2. simple relationship to the average association, which can be analogously defined as assocA;A jAj assocV \u00c0A;V \u00c0A jV \u00c0Aj\n\n. Consequently, one cannot simultaneously minimize the disassociation across the partitions while maximizing the association within the groups. When we applied both techniques to the image segmentation problem, we found that the normalized cut produces better results in practice. There are also other explanations why the normalized cut has better behavior from graph theoretical point of view, as pointed out by Chung [5].\n\nOur work, originally presented in [20], represents the first application of spectral partitioning to computer vision or image analysis. There is, however, one application area that has seen substantial application of spectral partitio-ning\u00d0the area of parallel scientific computing. The problem there is to balance the workload over multiple processors taking into account communication needs. One of the early papers is [18]. The generalized eigenvalue approach was first applied to graph partitioning by [8] for dynamically balancing computational load in a parallel computer. Their algorithm is motivated by [13]'s paper on representing a hypergraph in a Euclidean space.\n\nThe normalized cut criteria is also closely related to key properties of a Markov Random Walk. The similarity matrix W can be normalized to define a probability transition matrix P of a random walk on the pixels. It can be shown that the conductance [22] of this random walk is the normalized cut value and the normalized cut vectors of (12) are exactly the right eigenvectors of P.\n\n\nA Physical Interpretation\n\nAs one might expect, a physical analogy can be set up for the generalized eigenvalue system (6) that we used to approximate the solution of normalized cut. We can construct a spring-mass system from the weighted graph by taking graph nodes as physical nodes and graph edges as springs connecting each pair of nodes. Furthermore, we will define the graph edge weight as the spring stiffness and the total edge weights connecting to a node as its mass.\n\nImagine what would happen if we were to give a hard shake to this spring-mass system, forcing the nodes to oscillate in the direction perpendicular to the image plane. Nodes that have stronger spring connections among them will likely oscillate together. As the shaking becomes more violent, weaker springs connecting to this group of node will be overstretched. Eventually, the group will \u00aapop\u00ba off from the image plane. The overall steady state behavior of the nodes can be described by its fundamental mode of oscillation. In fact, it can be shown that the fundamental modes of oscillation of this spring mass system are exactly the generalized eigenvectors of (6).\n\nLet k ij be the spring stiffness connecting nodes i and j. Define K to be the n \u00c2 n stiffness matrix, with Ki; i P i k ij and Ki; j\u00c0k ij . Define the diagonal n \u00c2 n mass matrix M as Mi; i P i k ij . Let xt be the n \u00c2 1 vector describing the motion of each node. This spring-mass dynamic system can be described by:\n\nKxt\u00c0M xt: 20\n\nAssuming the solution takes the form of xtv k cos! k t , the steady state solutions of this spring-mass system satisfy:\nKv k ! 2 k Mv k ; 21\nanalogous to (6) for normalized cut.\n\nEach solution pair ! k ; v k of (21) describes a fundamental mode of the spring-mass system. The eigenvectors v k give the steady state displacement of the oscillation in each mode and the eigenvalues ! 2 k give the energy required to sustain each mode of oscillation. Therefore, finding graph partitions that have small normalized cut values is, in effect, the same as finding a way to \u00aapop\u00ba off image regions with minimal effort.\n\n\n6R ELATIONSHIP TO OTHER GRAPH THEORETIC APPROACHES TO IMAGE SEGMENTATION\n\nIn the computer vision community, there has been some been previous work on image segmentation formulated as a graph partition problem. Wu and Leahy [25] use the minimum cut criterion for their segmentation. As mentioned earlier, our criticism of this criterion is that it tends to favor cutting off small regions, which is undesirable in the context of image segmentation. In an attempt to get more balanced partitions, Cox et al. [6] seek to minimize the ratio\ncutA;V \u00c0A weightA ;A & V ,\nwhere weightA is some function of the set A. When weightA is taken to be the sum of the elements in A, we see that this criterion becomes one of the terms in the definition of average cut above. Cox et al. use an efficient discrete algorithm to solve their optimization problem assuming the graph is planar. Sarkar and Boyer [19] use the eigenvector with the largest eigenvalue of the system Wx x x x for finding the most coherent region in an edge map. Using a similar derivation as in Section 2.1, we can see that the first largest eigenvector of their system approximates min A&V assocA;A jAj and the second largest eigenvector approximates min A&V;B&V assocA;A jAj assocB;B jBj . However, the approximation is not tight and there is no guarantee that A B V . As we will see later in the section, this situation can happen quite often in practice. Since this algorithm is essentially looking for clusters that have tight within-grouping similarity, we will call this criteria average association.\n\n\nComparison with Related Eigenvector-Based Methods\n\nThe normalized cut formulation has a certain resemblance to the average cut, the standard spectral graph partitioning, as well as average association formulation. All three of these algorithms can be reduced to solving certain eigenvalue systems. How are they related to each other? Fig. 12 summarizes the relationship between these three algorithms. On one hand, both the normalized cut and the average cut algorithm are trying to find a \u00aabalanced partition\u00ba of a weighted graph, while, on the other hand, the normalized association and the average association are trying to find \u00aatight\u00ba clusters in the graph. Since the normalized association is exactly 2 \u00c0 ncut, the normalized cut value, the normalized cut formulation seeks a balance between the goal of clustering and segmentation. It is, therefore, not too surprising to see that the normalized cut vector can be approximated with the generalized eigenvector of D \u00c0 Wx Dx, as well as that of Wx Dx.\n\nJudging from the discrete formulations of these three grouping criteria, it can be seen that the average association, To illustrate these points, let us first consider a set of randomly distributed data in 1D shown in Fig. 13. The 1D data is made up by two subsets of points, one randomly distributed from 0 to 0.5 and the other from 0.65 to 1.0. Each data point is taken as a node in the graph and the weighted graph edge connecting two points is defined to be inversely proportional to the distance between two nodes. We will use three monotonically decreasing weighting functions, wxfdx, defined on the distance function, dx, with different rate of fall-off. The three weighting functions are plotted in Figs. 14a, 15a, and 16a.\n\nThe first function, wxe \u00c0 dx 0:1 2 , plotted in Fig. 14a, has the fastest decreasing rate among the three. With this weighting function, only close-by points are connected, as shown in the graph weight matrix W plotted in Fig. 14b. In this case, average association fails to find the right partition.\n\nInstead, it focuses on finding small clusters in each of the two main subgroups.\n\nThe second function, wx1 \u00c0 dx, plotted in Fig. 15a, has the slowest decreasing rate among the three. With this weighting function, most points have some nontrivial connections to the rest. To find a cut of the graph, a number of edges with heavy weights have to be removed. In addition, the cluster on the right has less within-group similarity comparing with the cluster on the left. In this case, average cut has trouble deciding on where to cut.\n\nThe third function, wxe \u00c0 dx 0:2 , plotted in Fig. 16a, has a moderate decreasing rate. With this weighting function, the nearby point connections are balanced against far-away point connections. In this case, all three algorithms perform well with normalized cut, producing a clearer solution than the two other methods.\n\nThese problems, illustrated in Figs. 14, 15, and 16, in fact are quite typical in segmenting real natural images. This is particularly true in the case of texture segmentation. Different texture regions often have very different withingroup similarity or coherence. It is very difficult to predetermine the right weighting function on each image region. Therefore, it is important to design a grouping algorithm that is more tolerant to a wide range of weighting functions. The advantage of using normalized cut becomes more evident in this case. Fig. 17 illustrates this point on a natural texture image shown previously in Fig. 10.  \n\n\n7C ONCLUSION\n\nIn this paper, we developed a grouping algorithm based on the view that perceptual grouping should be a process that aims to extract global impressions of a scene and provides a hierarchical description of it. By treating the grouping problem as a graph partitioning problem, we proposed the normalized cut criteria for segmenting the graph. Normalized cut is an unbiased measure of disassociation between subgroups of a graph and it has the nice property that minimizing normalized cut leads directly to maximizing the normalized association, which is an unbiased measure for total association within the subgroups. In finding an efficient algorithm for computing the minimum normalized cut, we showed that a generalized eigenvalue system provides a real valued solution to our problem.\n\nA computational method based on this idea has been developed and applied to segmentation of brightness, color, and texture images. Results of experiments on real and synthetic images are very encouraging and illustrate that the normalized cut criterion does indeed satisfy our initial goal of extracting the \u00aabig picture\u00ba of a scene. shows the corresponding graph weight matrix W W . The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). For both normalized cut and average cut, the smallest eigenvector is a constant vector as predicted. In this case, both normalized cut and average cut perform well, while the average association fails to do the right thing. Instead, it tries to pick out isolated small clusters.\n\n\nProposition 1 [Papadimitrou 97]. Normalized Cut (NCUT)\n\nfor a graph on regular grids is NP-complete.\n\nProof. We shall reduce NCUT on regular grids from PARTITION:\n\n. Given integers x 1 ;x 2 ; ...;x n adding to 2k, is there a subset adding to k?\n\nWe construct a weighted graph on a regular grid that has the property that it will have a small enough normalized cut if and only if we can find a subset from x 1 ;x 2 ; ...;x n adding to k. Fig. 18a shows the graph and Fig. 18b shows the form that a partition that minimizes the normalized cut must take.\n\nIn comparison to the integers x 1 ;x 2 ; ...;x n , M is much larger, M>2k 2 , and a is much smaller, 0 <a<1=n.W e ask the question shows the corresponding graph weight matrix W W . The two columns (c) and (d) below show the first, and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and Average association (row 3). In this case, both normalized cut and average association give the right partition, while the average cut has trouble deciding on where to cut.\n\n. Is there a partition with Ncut value less than 4an c\u00c01=c , where c is half the sum of edge weights in the graph, c 2Mn 1k 3an.\n\nWe shall see that a good Ncut partition of the graph must separate the left and right columns. In particular, if and only if there is a subset S 1 fx 1 ; ...;x m g adding to k,by taking the corresponding edges in the middle column to be in one side of the partition, as illustrated in Fig. 18b, we achieve an Ncut value less than 4an c\u00c01=c . For all other partitions, the Ncut value will be bounded below by 4an c\u00c01=c . First, let us show that the cut illustrated in Fig. 18b, where each side has a subset of middle column edges x 1 ;x 2 ; ...;x n that add up to k, does have Ncut value less than 4an c\u00c01=c . Let the ncut \u00c3 be the Ncut value for this cut. By using the formula for Ncut (2.2), we can see that shows the corresponding graph weight matrix W W . The two columns (c) and (d) below show the first and second extreme eigenvectors for the Normalized cut (row 1), Average cut (row 2), and average association (row 3). All three of these algorithms perform satisfactorily in this case, with normalized cut producing a clearer solution than the other two cuts. ncut \u00c3 4an 2c 2an2k 1 \u00c0 1 4an 2c \u00c0 2an2k 1 \u00c0 1 ;\n\nwhere c is half the total edge weights in the graph, c 2Mn 1k 3an, and k 1 n and 1 \u00c0 k 1 n are the number of edges from the middle column on the two sides of the graph partition, 0 <k 1 < 1.T h et e r m an2k 1 \u00c0 1 can be interpreted as the amount of imbalance between the denominators in the two terms in the Ncut formula and lies between \u00c01 and 1 (since 0 <an<1). Simplifying, we see that ncut \u00c3 4anc c 2 \u00c0an2k 1 \u00c0 1 2 < 4anc c 2 \u00c0 1 4an c \u00c0 1=c :\n\nas was to be shown.\n\nTo complete the proof we must show that all other partitions result in a Ncut greater than or equal to 4an c\u00c01=c . Informally speaking, what will happen is that either the numerators of the terms in the Ncut formula\u00d0the cut become too large, or the denominators become significantly imbalanced, again increasing the Ncut value. We need to consider three cases:\n\n1. A cut that deviates from the cut in 1(b) slightly by reshuffling some of the x i edges so that the sums of the x i in each subset of the graph partition are no longer equal. For such cuts, the resulting Ncut values are, at best, ncut 1 2an cx 2an c\u00c0x 4anc c 2 \u00c0x 2 . But, since x ! 1, we have ncut 1 ! 4anc c 2 \u00c01 4an c\u00c01=c . 2. A cut that goes through any of the edges with weight M. Even with the denominators on both sides completely balanced, the Ncut value ncut 2 2M c is going to be larger than 4an c\u00c01=c . This is ensured by our choice in the construction that M>2k 2 . We have to show that\n\n\n2M\n\nc ! 4an c \u00c0 1=c\n\n; or M ! 2an c 2 c 2 \u00c0 1 :\n\nThis is direct, since an<1 by construction, c 2 c 2 \u00c01 81 80 (using k ! 1, M ! 2, c ! 9).\n\n\n3.\n\nA cut that partitions out some of the nodes in the middle as one group. We see that any cut that goes through one of the x i s can improve its Ncut value by going through the edges with weight a instead. So, we will focus on the case where the cut only goes through the weight a edges. Suppose that m edges of x i s are grouped into one set, with total weight adding to x, where 1 <x<2k. The corresponding ncut value, ncut 3 m 4am 4am 2x 4am 8Mn 14k 12an\u00c0 4am\u00c0 2x 2am c \u00c0 d m 2am c d m ;\n\nwhere Fig. 17. Normalized cut and average association result on the zebra image in Fig. 10. Subplot (a) shows the second largest eigenvector of W x Dx, approximating the normalized cut vector. Subplots (b)-(e) show the first to fourth largest eigenvectors of W x x, approximating the average association vector, using the same graph weight matrix. In this image, pixels on the zebra body have, on average, lower degree of coherence than the pixels in the background. The average association, with its tendency to find tight clusters, partitions out only small clusters in the background. The normalized cut algorithm, having to balance the goal of clustering and segmentation, finds the better partition in this case. ...;x n , M is a large number (M>2k 2 ), and a is very small number (0 <a<1=n). (b) shows a cut that has a Ncut value less than 4an c\u00c01=c . This cut, which only goes through edges with weight equal to a or na, has the property that the x i s on each side of the partition sum up to k.\n\nFig. 1 .\n1A case where minimum cut gives a bad partition.\n\nFig. 2\n2shows an image that we would like to segment. The steps are:\n\nFig. 2 .\n2A gray level image of a baseball game.\n\nFig. 3 .\n3Subplot (a) plots the smallest eigenvectors of the generalized eigenvalue system(11). Subplots (b)-(i) show the eigenvectors corresponding the second smallest to the ninth smallest eigenvalues of the system. The eigenvectors are reshaped to be the size of the image.\n\nFig. 4 .\n4(a) shows the original image of size 80 \u00c2 100. Image intensity is normalized to lie within 0 and 1. Subplots (b)-(h) show the components of the partition with Ncut value less than 0.04. Parameter setting: I 0:1, X 4:0, r 5.\n\nFig. 5 .\n5(a) Point set generated by two Poisson processes, with densities of 2.5 and 1.0 on the left and right clusters respectively, (b) R and \u00c2 indicate the partition of point set in (a). Parameter settings: X 5, r 3.\n\nFig. 6 .\n6(a) A synthetic image showing a noisy \u00aastep\u00ba image. Intensity varies from 0 to 1, and Gaussian noise with 0:2 is added. Subplot (b) shows the eigenvector with the second smallest eigenvalue and subplot (c) shows the resulting partition.\n\nFig. 7 .\n7(a) A synthetic image showing three image patches forming a junction. Image intensity varies from 0 to 1 and Gaussian noise with 0:1 is added. (b)-(d) show the top three components of the partition.\n\nFig. 8 .\n8(a) shows a 126 \u00c2 106 weather radar image. (b)-(g) show the components of the partition with Ncut value less than 0.08. Parameter setting: I 0:007, x 15:0, r 10.\n\nFig. 9 .\n9(a) shows a 77 \u00c2 107 color image. (b)-(e) show the components of the partition with Ncut value less than 0.04. Parameter settings: I 0:01, X 4:0, r 5.\n\nFig.\nFig. 10. (a) shows an image of a zebra. The remaining images show the major components of the partition. The texture features used correspond to convolutions with DOOG filters [16] at six orientations and five scales.\n\nFig. 11 .\n111 can be adapted (by replacing the matrix D in the denominators by the identity matrix I)t o show that the Fiedler vector is a real valued solution to the problem of min A&V cutA;V \u00c0A jAj cutV \u00c0A;A jV \u00c0Aj, which we can call the average cut.Although average cut looks similar to the normalized cut, average cut does not have the important property of having a Subimages (a) and (b) show two frames of an image sequence. Segmentation results on this two frame image sequence are shown in subimages (c) to (g). Segments in (c) and (d) correspond to the person in the foreground and segments in (e) to (g) correspond to the background. The reason that the head of the person is segmented away from the body is that, although they have similar motion, their motion profiles are different. The head region contains 2D textures and the motion profiles are more peaked, while, in the body region, the motion profiles are more spread out. Segment (e) is broken away from (f) and (g) for the same reason.\n\n\na bias for finding tight clusters. Therefore, it runs the risk of becoming too greedy in finding small, but tight, clusters in the data. This might be perfect for data that are Gaussian distributed. However, for typical data in the real world that are more likely to be made up of a mixture of various different types of distributions, this bias in grouping will have undesired consequences, as we shall illustrate in the examples below. For average cut, cutA;B jAj cutA;B jBj , the opposite problem arises\u00d0one cannot ensure the two partitions computed will have tight within-group similarity. This becomes particularly problematic if the dissimilarity among the different groups varies from one to another, or if there are several possible partitions all with similar average cut values.\n\nFig. 12 .\n12Relationship between normalized cut and other eigenvector-based partitioning techniques. Compared to the average cut and average association formulation, normalized cut seeks a balance between the goal of finding clumps and finding splits.\n\nFig. 13 .\n13A set of randomly distributed points in 1D. The first 20 points are randomly distributed from 0.0 to 0.5 and the remaining 12 points are randomly distributed from 0.65 to 1.0. Segmentation result of these points with different weighting functions are shown in Figs. 14, 15, and 16.\n\nFig. 14 .\n14A weighting function with fast rate of fall-off: wxe \u00c0 dx 0:1 2 , shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 15 and 16. Subplot (b)\n\nFig. 15 .\n15A weighting function with slow rate of fall-off: wx1 \u00c0 dx, shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 16. Subplot (b)\n\nFig. 16 .\n16A weighting function with medium rate of fall-off: wxe \u00c0 dx 0:2 , shown in subplot (a) in solid line. The dotted lines show the two alternative weighting functions used in Figs. 14 and 15. Subplot (b)\n\nFig. 18 .\n18(a) shows a weighted graph on a regular grid. The missing edges on the grids have weights of 0. In comparison to the integers x 1 ;x 2 ;\n\n\nwhere assocA; A and assocB; B are total weights of edges connecting nodes within A and B, respectively. We see again this is an unbiased measure, which reflects how tightly on average nodes within the group are connected to each other.Another important property of this definition of association and disassociation of a partition is that they are naturally related:; B \nassocA; A \nassocA; V \n\nassocB; B \nassocB; V \n; \n3 \n\nNcutA; B \ncutA; B \nassocA; V \n\ncutA; B \nassocB; V \n\n\nassocA; V \u00c0assocA; A \nassocA; V \n\n\nassocB; V \u00c0assocB; B \nassocB; V \n\n2 \u00c0 \nassocA; A \nassocA; V \n\nassocB; B \nassocB; V \n\n2 \u00c0 NassocA; B: \n\n\nACKNOWLEDGMENTSThis research was supported by (ARO) DAAH04-96-1-0341, and a National Science Foundation Graduate Fellowship to J. Shi. We thank Christos Papadimitriou for supplying the proof of NP-completeness for normalized cuts on a grid. In addition, we wish to acknowledge Umesh Vazirani and Alistair Sinclair for discussions on graph theoretic algorithms and Inderjit Dhillon and Mark Adams for useful pointers to numerical packages. Thomas Leung, Serge Belongie, Yeir Weiss, and other members of the computer vision group at UC Berkeley provided much useful feedback on our algorithm.Jianbo Shi studied computer science and mathematics as an undergraduate at Cornell University where he recieved his BA degree in 1994. He received his PhD degree in computer science from the University of California at Berkeley in 1998. Since 1999, he has been a member of the faculty of the Robotics Institute at Carnegie Mellon University, where his primary research interests include image segmentation, grouping, object recognition, motion and shape analysis, and machine learning.\n. N Alon, \u00ba Expanders, Combinatorica, 6N. Alon, \u00aaEigenvalues and Expanders,\u00ba Combinatorica, vol. 6, no. 2, pp. 83-96, 1986.\n\nVisual Reconstruction. A Blake, A Zisserman, MIT PressA. Blake and A. Zisserman, Visual Reconstruction. MIT Press, 1987.\n\n\u00aaEigenvalues and Graph Bisection: An Average-Case Analysis. R B Boppana, Proc. 28th Symp. Foundations of Computer Science. 28th Symp. Foundations of Computer ScienceR.B. Boppana, \u00aaEigenvalues and Graph Bisection: An Average- Case Analysis,\u00ba Proc. 28th Symp. Foundations of Computer Science, pp. 280-285, 1987.\n\nJ Cheeger, \u00aaA Lower Bound for the Smallest Eigenvalue of the Laplacian,\u00ba Problems in Analysis, R.C. Gunning. Princeton Univ. PressJ. Cheeger, \u00aaA Lower Bound for the Smallest Eigenvalue of the Laplacian,\u00ba Problems in Analysis, R.C. Gunning, ed., pp. 195-199, Princeton Univ. Press, 1970.\n\n. F R K Chung, Spectral Graph Theory. Am. Math. Soc. F.R.K. Chung, Spectral Graph Theory. Am. Math. Soc., 1997.\n\nI J Cox, S B Rao, Y Zhong, \u00aaRatio Regions: A Technique for Image Segmentation,\u00ba Proc. 13th Int'l Conf. Pattern Recognition. I.J. Cox, S.B. Rao, and Y. Zhong, \u00aaRatio Regions: A Technique for Image Segmentation,\u00ba Proc. 13th Int'l Conf. Pattern Recognition, 1996.\n\n\u00aaLower Bounds for the Partitioning of Graphs,\u00ba IBM J. Research and Development. W E Donath, A J Hoffman, W.E. Donath and A.J. Hoffman, \u00aaLower Bounds for the Partition- ing of Graphs,\u00ba IBM J. Research and Development, pp. 420-425, 1973.\n\n\u00aaAn Improved Spectral Bisection Algorithm and Its Application to Dynamic Load Balancing. R Van Driessche, D Roose, Parallel Computing. 21R. Van Driessche and D. Roose, \u00aaAn Improved Spectral Bisection Algorithm and Its Application to Dynamic Load Balancing,\u00ba Parallel Computing, vol. 21, pp. 29-48, 1995.\n\n\u00aaA Property of Eigenvectors of Nonnegative Symmetric Matrices and Its Applications to Graph Theory,\u00ba Czech. M Fiedler, Math. J. 25100M. Fiedler, \u00aaA Property of Eigenvectors of Nonnegative Sym- metric Matrices and Its Applications to Graph Theory,\u00ba Czech. Math. J., vol. 25, no. 100, pp. 619-633, 1975.\n\nGibbs Distributions, and the Bayesian Restoration of Images. S Geman, D Geman, \u00aastochastic Relaxation, IEEE Trans. Pattern Analysis and Machine Intelligence. 6S. Geman and D. Geman, \u00aaStochastic Relaxation, Gibbs Distribu- tions, and the Bayesian Restoration of Images,\u00ba IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 6, pp. 721-741, Nov. 1984.\n\nG H Golub, C F Van Loan, Matrix Computations. John Hopkins PressG.H. Golub and C.F. Van Loan, Matrix Computations. John Hopkins Press, 1989.\n\nAlgorithms for Clustering Data. A K Jain, R C Dubes, Prentice HallA.K. Jain and R.C. Dubes, Algorithms for Clustering Data. Prentice Hall, 1988.\n\n\u00aaA Representation of Hypergraphs in the Euclidean Space. K Fukunaga, S Yamada, H S Stone, T Kasai, IEEE Trans. Computers. 334K. Fukunaga, S. Yamada, H.S. Stone, and T. Kasai, \u00aaA Representa- tion of Hypergraphs in the Euclidean Space,\u00ba IEEE Trans. Computers, vol. 33, no. 4, pp. 364-367, Apr. 1984.\n\n\u00aaConstructing Simple Stable Descriptions for Image Partitioning,\u00ba Int'l. Y G Leclerc, J. Computer Vision. 3Y.G. Leclerc, \u00aaConstructing Simple Stable Descriptions for Image Partitioning,\u00ba Int'l J. Computer Vision, vol. 3, pp. 73-102, 1989.\n\nJ Malik, S Belongie, J Shi, T Leung, \u00aaTextons, Contours and Regions: Cue Integration in Image Segmentation,\u00ba Proc. Int'l Conf. Computer Vision. J. Malik, S. Belongie, J. Shi, and T. Leung, \u00aaTextons, Contours and Regions: Cue Integration in Image Segmentation,\u00ba Proc. Int'l Conf. Computer Vision, pp. 918-925, 1999.\n\nJ Malik, P Perona, \u00aaPreattentive Texture Discrimination with Early Vision Mechanisms. 7J. Malik and P. Perona, \u00aaPreattentive Texture Discrimination with Early Vision Mechanisms,\u00ba J. Optical Soc. Am., vol. 7, no. 2, pp. 923- 932, May 1990.\n\nD Mumford, J Shah, \u00aaOptimal Approximations by Piecewise Smooth Functions, and Associated Variational Problems. D. Mumford and J. Shah, \u00aaOptimal Approximations by Piecewise Smooth Functions, and Associated Variational Problems,\u00ba Comm. Pure Math., pp. 577-684, 1989.\n\n\u00aaPartitioning Sparse Matrices with Eigenvectors of Graphs. A Pothen, H D Simon, K P Liou, \u00ba SIAM J. Matrix Analytical Applications. 11A. Pothen, H.D. Simon, and K.P. Liou, \u00aaPartitioning Sparse Matrices with Eigenvectors of Graphs,\u00ba SIAM J. Matrix Analytical Applications, vol. 11, pp. 430-452, 1990.\n\nS Sarkar, K L Boyer, \u00aaQuantitative Measures of Change Based on Feature Organization: Eigenvalues and Eigenvectors,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition. S. Sarkar and K.L. Boyer, \u00aaQuantitative Measures of Change Based on Feature Organization: Eigenvalues and Eigenvectors,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1996.\n\nJ Shi, J Malik, \u00aaNormalized Cuts and Image Segmentation,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition. J. Shi and J. Malik, \u00aaNormalized Cuts and Image Segmentation,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 731- 737, 1997.\n\nJ Shi, J Malik, \u00aaMotion Segmentation and Tracking Using Normalized Cuts,\u00ba Proc. Int'l Conf. Computer Vision. 160J. Shi and J. Malik, \u00aaMotion Segmentation and Tracking Using Normalized Cuts,\u00ba Proc. Int'l Conf. Computer Vision, pp. 1,154- 1,160, 1998.\n\nUniform Generation and Rapidly Mixing Markov Chains,\u00ba Information and Computation. A J Sinclair, M R Jerrum, Counting, 82A.J. Sinclair and M.R. Jerrum, \u00aaApproximative Counting, Uniform Generation and Rapidly Mixing Markov Chains,\u00ba Information and Computation, vol. 82, pp. 93-133, 1989.\n\nD A Spielman, S H Teng, \u00aaDisk Packings and Planar Separators,\u00ba Proc. 12th ACM Symp. D.A. Spielman and S.H. Teng, \u00aaDisk Packings and Planar Separators,\u00ba Proc. 12th ACM Symp. Computational Geometry, May 1996.\n\n\u00aaLaws of Organization in Perceptual Forms (partial translation),\u00ba A Sourcebook of Gestalt Psycychology. M Wertheimer, B. EllisHarcourt, BraceM. Wertheimer, \u00aaLaws of Organization in Perceptual Forms (partial translation),\u00ba A Sourcebook of Gestalt Psycychology, W.B. Ellis, ed., pp. 71-88, Harcourt, Brace, 1938.\n\n\u00aaAn Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation. Z Wu, R Leahy, IEEE Trans. Pattern Analysis and Machine Intelligence. 1511Z. Wu and R. Leahy, \u00aaAn Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmenta- tion,\u00ba IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1,101-1,113, Nov. 1993.\n", "annotations": {"author": "[{\"end\":158,\"start\":53},{\"end\":287,\"start\":159},{\"end\":377,\"start\":288}]", "publisher": null, "author_last_name": "[{\"end\":63,\"start\":60},{\"end\":185,\"start\":180}]", "author_first_name": "[{\"end\":59,\"start\":53},{\"end\":179,\"start\":171},{\"end\":294,\"start\":288},{\"end\":298,\"start\":295}]", "author_affiliation": "[{\"end\":157,\"start\":81},{\"end\":286,\"start\":210},{\"end\":376,\"start\":300}]", "title": "[{\"end\":39,\"start\":1},{\"end\":416,\"start\":378}]", "venue": "[{\"end\":480,\"start\":418}]", "abstract": "[{\"end\":1707,\"start\":910}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1764,\"start\":1760},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3791,\"start\":3787},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4122,\"start\":4118},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4156,\"start\":4152},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4161,\"start\":4158},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4167,\"start\":4163},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6808,\"start\":6804},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7448,\"start\":7444},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12382,\"start\":12378},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13217,\"start\":13213},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13705,\"start\":13701},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17303,\"start\":17299},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18125,\"start\":18121},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19438,\"start\":19434},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26542,\"start\":26538},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29747,\"start\":29743},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31003,\"start\":30999},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31966,\"start\":31962},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32503,\"start\":32500},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32527,\"start\":32524},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32544,\"start\":32541},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32622,\"start\":32619},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32627,\"start\":32624},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32633,\"start\":32629},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32639,\"start\":32635},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32760,\"start\":32756},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32863,\"start\":32860},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33549,\"start\":33546},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33722,\"start\":33719},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34330,\"start\":34327},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34336,\"start\":34332},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34910,\"start\":34907},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34951,\"start\":34947},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35338,\"start\":35334},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35422,\"start\":35419},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35528,\"start\":35524},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35843,\"start\":35839},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35930,\"start\":35926},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38293,\"start\":38289},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38575,\"start\":38572},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":38959,\"start\":38955},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":50445,\"start\":50441}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50229,\"start\":50171},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50299,\"start\":50230},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50349,\"start\":50300},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50627,\"start\":50350},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50862,\"start\":50628},{\"attributes\":{\"id\":\"fig_5\"},\"end\":51084,\"start\":50863},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51332,\"start\":51085},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51542,\"start\":51333},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51715,\"start\":51543},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51877,\"start\":51716},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52101,\"start\":51878},{\"attributes\":{\"id\":\"fig_11\"},\"end\":53109,\"start\":52102},{\"attributes\":{\"id\":\"fig_12\"},\"end\":53900,\"start\":53110},{\"attributes\":{\"id\":\"fig_13\"},\"end\":54153,\"start\":53901},{\"attributes\":{\"id\":\"fig_14\"},\"end\":54448,\"start\":54154},{\"attributes\":{\"id\":\"fig_15\"},\"end\":54662,\"start\":54449},{\"attributes\":{\"id\":\"fig_16\"},\"end\":54869,\"start\":54663},{\"attributes\":{\"id\":\"fig_17\"},\"end\":55083,\"start\":54870},{\"attributes\":{\"id\":\"fig_18\"},\"end\":55233,\"start\":55084},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55849,\"start\":55234}]", "paragraph": "[{\"end\":2188,\"start\":1709},{\"end\":3002,\"start\":2190},{\"end\":3663,\"start\":3004},{\"end\":4239,\"start\":3665},{\"end\":4595,\"start\":4241},{\"end\":4992,\"start\":4597},{\"end\":5211,\"start\":4994},{\"end\":5279,\"start\":5213},{\"end\":5875,\"start\":5281},{\"end\":6747,\"start\":5877},{\"end\":6809,\"start\":6749},{\"end\":7148,\"start\":6846},{\"end\":7881,\"start\":7176},{\"end\":8523,\"start\":7883},{\"end\":8915,\"start\":8525},{\"end\":9516,\"start\":8965},{\"end\":9941,\"start\":9518},{\"end\":10120,\"start\":9950},{\"end\":10275,\"start\":10122},{\"end\":10610,\"start\":10311},{\"end\":11059,\"start\":10895},{\"end\":11220,\"start\":11202},{\"end\":11238,\"start\":11222},{\"end\":11264,\"start\":11251},{\"end\":11315,\"start\":11266},{\"end\":11443,\"start\":11382},{\"end\":11585,\"start\":11544},{\"end\":11976,\"start\":11926},{\"end\":12272,\"start\":12183},{\"end\":12508,\"start\":12274},{\"end\":12918,\"start\":12510},{\"end\":13706,\"start\":12951},{\"end\":13984,\"start\":13708},{\"end\":14078,\"start\":13986},{\"end\":14152,\"start\":14080},{\"end\":14674,\"start\":14154},{\"end\":15399,\"start\":14676},{\"end\":15580,\"start\":15401},{\"end\":16008,\"start\":15639},{\"end\":16092,\"start\":16037},{\"end\":17023,\"start\":16094},{\"end\":17249,\"start\":17161},{\"end\":17360,\"start\":17251},{\"end\":17626,\"start\":17394},{\"end\":18441,\"start\":17628},{\"end\":18552,\"start\":18443},{\"end\":18793,\"start\":18671},{\"end\":19279,\"start\":18795},{\"end\":19650,\"start\":19281},{\"end\":22462,\"start\":19657},{\"end\":23213,\"start\":22489},{\"end\":23702,\"start\":23266},{\"end\":24261,\"start\":23704},{\"end\":24325,\"start\":24263},{\"end\":25543,\"start\":24327},{\"end\":25960,\"start\":25631},{\"end\":26197,\"start\":26045},{\"end\":26246,\"start\":26199},{\"end\":26315,\"start\":26248},{\"end\":26673,\"start\":26352},{\"end\":26881,\"start\":26675},{\"end\":27390,\"start\":26883},{\"end\":27897,\"start\":27392},{\"end\":28947,\"start\":27899},{\"end\":29129,\"start\":28979},{\"end\":29384,\"start\":29191},{\"end\":29416,\"start\":29386},{\"end\":29748,\"start\":29418},{\"end\":30069,\"start\":29769},{\"end\":30499,\"start\":30071},{\"end\":30633,\"start\":30501},{\"end\":31040,\"start\":30665},{\"end\":31576,\"start\":31042},{\"end\":31967,\"start\":31578},{\"end\":32761,\"start\":32011},{\"end\":33313,\"start\":32763},{\"end\":33640,\"start\":33315},{\"end\":33723,\"start\":33642},{\"end\":34485,\"start\":33748},{\"end\":34911,\"start\":34487},{\"end\":35587,\"start\":34913},{\"end\":35971,\"start\":35589},{\"end\":36451,\"start\":36001},{\"end\":37121,\"start\":36453},{\"end\":37437,\"start\":37123},{\"end\":37451,\"start\":37439},{\"end\":37572,\"start\":37453},{\"end\":37630,\"start\":37594},{\"end\":38063,\"start\":37632},{\"end\":38602,\"start\":38140},{\"end\":39629,\"start\":38630},{\"end\":40638,\"start\":39683},{\"end\":41371,\"start\":40640},{\"end\":41673,\"start\":41373},{\"end\":41755,\"start\":41675},{\"end\":42205,\"start\":41757},{\"end\":42528,\"start\":42207},{\"end\":43165,\"start\":42530},{\"end\":43969,\"start\":43182},{\"end\":44801,\"start\":43971},{\"end\":44904,\"start\":44860},{\"end\":44966,\"start\":44906},{\"end\":45048,\"start\":44968},{\"end\":45355,\"start\":45050},{\"end\":45849,\"start\":45357},{\"end\":45979,\"start\":45851},{\"end\":47096,\"start\":45981},{\"end\":47546,\"start\":47098},{\"end\":47567,\"start\":47548},{\"end\":47929,\"start\":47569},{\"end\":48531,\"start\":47931},{\"end\":48553,\"start\":48538},{\"end\":48581,\"start\":48555},{\"end\":48672,\"start\":48583},{\"end\":49166,\"start\":48679},{\"end\":50170,\"start\":49168}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7175,\"start\":7149},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8964,\"start\":8916},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9949,\"start\":9942},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10894,\"start\":10611},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11195,\"start\":11060},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11250,\"start\":11239},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11381,\"start\":11316},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11543,\"start\":11444},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11925,\"start\":11586},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12182,\"start\":11977},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12950,\"start\":12919},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15638,\"start\":15581},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17160,\"start\":17053},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17393,\"start\":17361},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18670,\"start\":18553},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25614,\"start\":25544},{\"attributes\":{\"id\":\"formula_16\"},\"end\":26044,\"start\":25961},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26351,\"start\":26316},{\"attributes\":{\"id\":\"formula_18\"},\"end\":28978,\"start\":28948},{\"attributes\":{\"id\":\"formula_19\"},\"end\":29190,\"start\":29130},{\"attributes\":{\"id\":\"formula_20\"},\"end\":33747,\"start\":33724},{\"attributes\":{\"id\":\"formula_21\"},\"end\":37593,\"start\":37573},{\"attributes\":{\"id\":\"formula_22\"},\"end\":38629,\"start\":38603}]", "table_ref": null, "section_header": "[{\"end\":6844,\"start\":6812},{\"attributes\":{\"n\":\"2.1\"},\"end\":10309,\"start\":10278},{\"end\":11200,\"start\":11197},{\"end\":16035,\"start\":16011},{\"attributes\":{\"n\":\"3.1\"},\"end\":17052,\"start\":17026},{\"end\":19655,\"start\":19653},{\"attributes\":{\"n\":\"3.2\"},\"end\":22487,\"start\":22465},{\"attributes\":{\"n\":\"3.3\"},\"end\":23264,\"start\":23216},{\"end\":25629,\"start\":25616},{\"attributes\":{\"n\":\"4.1\"},\"end\":29767,\"start\":29751},{\"attributes\":{\"n\":\"4.2\"},\"end\":30663,\"start\":30636},{\"end\":32009,\"start\":31970},{\"attributes\":{\"n\":\"5.1\"},\"end\":35999,\"start\":35974},{\"end\":38138,\"start\":38066},{\"attributes\":{\"n\":\"6.1\"},\"end\":39681,\"start\":39632},{\"end\":43180,\"start\":43168},{\"end\":44858,\"start\":44804},{\"end\":48536,\"start\":48534},{\"end\":48677,\"start\":48675},{\"end\":50180,\"start\":50172},{\"end\":50237,\"start\":50231},{\"end\":50309,\"start\":50301},{\"end\":50359,\"start\":50351},{\"end\":50637,\"start\":50629},{\"end\":50872,\"start\":50864},{\"end\":51094,\"start\":51086},{\"end\":51342,\"start\":51334},{\"end\":51552,\"start\":51544},{\"end\":51725,\"start\":51717},{\"end\":51883,\"start\":51879},{\"end\":52112,\"start\":52103},{\"end\":53911,\"start\":53902},{\"end\":54164,\"start\":54155},{\"end\":54459,\"start\":54450},{\"end\":54673,\"start\":54664},{\"end\":54880,\"start\":54871},{\"end\":55094,\"start\":55085}]", "table": "[{\"end\":55849,\"start\":55601}]", "figure_caption": "[{\"end\":50229,\"start\":50182},{\"end\":50299,\"start\":50239},{\"end\":50349,\"start\":50311},{\"end\":50627,\"start\":50361},{\"end\":50862,\"start\":50639},{\"end\":51084,\"start\":50874},{\"end\":51332,\"start\":51096},{\"end\":51542,\"start\":51344},{\"end\":51715,\"start\":51554},{\"end\":51877,\"start\":51727},{\"end\":52101,\"start\":51884},{\"end\":53109,\"start\":52115},{\"end\":53900,\"start\":53112},{\"end\":54153,\"start\":53914},{\"end\":54448,\"start\":54167},{\"end\":54662,\"start\":54462},{\"end\":54869,\"start\":54676},{\"end\":55083,\"start\":54883},{\"end\":55233,\"start\":55097},{\"end\":55601,\"start\":55236}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8152,\"start\":8146},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9411,\"start\":9405},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19539,\"start\":19533},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21141,\"start\":21131},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22402,\"start\":22396},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22461,\"start\":22455},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26743,\"start\":26737},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27100,\"start\":27094},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27190,\"start\":27184},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27624,\"start\":27617},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29428,\"start\":29421},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30835,\"start\":30828},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39973,\"start\":39966},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40865,\"start\":40858},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41370,\"start\":41347},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41429,\"start\":41421},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41603,\"start\":41595},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41807,\"start\":41799},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42261,\"start\":42253},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43084,\"start\":43077},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43162,\"start\":43155},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45249,\"start\":45241},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45278,\"start\":45270},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46274,\"start\":46266},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46456,\"start\":46448},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49181,\"start\":49174},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49258,\"start\":49251}]", "bib_author_first_name": "[{\"end\":56929,\"start\":56928},{\"end\":56937,\"start\":56936},{\"end\":57075,\"start\":57074},{\"end\":57084,\"start\":57083},{\"end\":57234,\"start\":57233},{\"end\":57236,\"start\":57235},{\"end\":57485,\"start\":57484},{\"end\":57775,\"start\":57774},{\"end\":57779,\"start\":57776},{\"end\":57886,\"start\":57885},{\"end\":57888,\"start\":57887},{\"end\":57895,\"start\":57894},{\"end\":57897,\"start\":57896},{\"end\":57904,\"start\":57903},{\"end\":58228,\"start\":58227},{\"end\":58230,\"start\":58229},{\"end\":58240,\"start\":58239},{\"end\":58242,\"start\":58241},{\"end\":58474,\"start\":58473},{\"end\":58491,\"start\":58490},{\"end\":58798,\"start\":58797},{\"end\":59054,\"start\":59053},{\"end\":59063,\"start\":59062},{\"end\":59351,\"start\":59350},{\"end\":59353,\"start\":59352},{\"end\":59362,\"start\":59361},{\"end\":59364,\"start\":59363},{\"end\":59525,\"start\":59524},{\"end\":59527,\"start\":59526},{\"end\":59535,\"start\":59534},{\"end\":59537,\"start\":59536},{\"end\":59696,\"start\":59695},{\"end\":59708,\"start\":59707},{\"end\":59718,\"start\":59717},{\"end\":59720,\"start\":59719},{\"end\":59729,\"start\":59728},{\"end\":60011,\"start\":60010},{\"end\":60013,\"start\":60012},{\"end\":60178,\"start\":60177},{\"end\":60187,\"start\":60186},{\"end\":60199,\"start\":60198},{\"end\":60206,\"start\":60205},{\"end\":60494,\"start\":60493},{\"end\":60503,\"start\":60502},{\"end\":60734,\"start\":60733},{\"end\":60745,\"start\":60744},{\"end\":61059,\"start\":61058},{\"end\":61069,\"start\":61068},{\"end\":61071,\"start\":61070},{\"end\":61080,\"start\":61079},{\"end\":61082,\"start\":61081},{\"end\":61301,\"start\":61300},{\"end\":61311,\"start\":61310},{\"end\":61313,\"start\":61312},{\"end\":61661,\"start\":61660},{\"end\":61668,\"start\":61667},{\"end\":61919,\"start\":61918},{\"end\":61926,\"start\":61925},{\"end\":62253,\"start\":62252},{\"end\":62255,\"start\":62254},{\"end\":62267,\"start\":62266},{\"end\":62269,\"start\":62268},{\"end\":62458,\"start\":62457},{\"end\":62460,\"start\":62459},{\"end\":62472,\"start\":62471},{\"end\":62474,\"start\":62473},{\"end\":62770,\"start\":62769},{\"end\":63085,\"start\":63084},{\"end\":63091,\"start\":63090}]", "bib_author_last_name": "[{\"end\":56934,\"start\":56930},{\"end\":56947,\"start\":56938},{\"end\":56962,\"start\":56949},{\"end\":57081,\"start\":57076},{\"end\":57094,\"start\":57085},{\"end\":57244,\"start\":57237},{\"end\":57493,\"start\":57486},{\"end\":57785,\"start\":57780},{\"end\":57892,\"start\":57889},{\"end\":57901,\"start\":57898},{\"end\":57910,\"start\":57905},{\"end\":58237,\"start\":58231},{\"end\":58250,\"start\":58243},{\"end\":58488,\"start\":58475},{\"end\":58497,\"start\":58492},{\"end\":58806,\"start\":58799},{\"end\":59060,\"start\":59055},{\"end\":59069,\"start\":59064},{\"end\":59093,\"start\":59071},{\"end\":59359,\"start\":59354},{\"end\":59373,\"start\":59365},{\"end\":59532,\"start\":59528},{\"end\":59543,\"start\":59538},{\"end\":59705,\"start\":59697},{\"end\":59715,\"start\":59709},{\"end\":59726,\"start\":59721},{\"end\":59735,\"start\":59730},{\"end\":60021,\"start\":60014},{\"end\":60184,\"start\":60179},{\"end\":60196,\"start\":60188},{\"end\":60203,\"start\":60200},{\"end\":60212,\"start\":60207},{\"end\":60500,\"start\":60495},{\"end\":60510,\"start\":60504},{\"end\":60742,\"start\":60735},{\"end\":60750,\"start\":60746},{\"end\":61066,\"start\":61060},{\"end\":61077,\"start\":61072},{\"end\":61087,\"start\":61083},{\"end\":61308,\"start\":61302},{\"end\":61319,\"start\":61314},{\"end\":61665,\"start\":61662},{\"end\":61674,\"start\":61669},{\"end\":61923,\"start\":61920},{\"end\":61932,\"start\":61927},{\"end\":62264,\"start\":62256},{\"end\":62276,\"start\":62270},{\"end\":62286,\"start\":62278},{\"end\":62469,\"start\":62461},{\"end\":62479,\"start\":62475},{\"end\":62781,\"start\":62771},{\"end\":63088,\"start\":63086},{\"end\":63097,\"start\":63092}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":57049,\"start\":56926},{\"attributes\":{\"id\":\"b1\"},\"end\":57171,\"start\":57051},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6009989},\"end\":57482,\"start\":57173},{\"attributes\":{\"id\":\"b3\"},\"end\":57770,\"start\":57484},{\"attributes\":{\"id\":\"b4\"},\"end\":57883,\"start\":57772},{\"attributes\":{\"id\":\"b5\"},\"end\":58145,\"start\":57885},{\"attributes\":{\"id\":\"b6\"},\"end\":58382,\"start\":58147},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":33632401},\"end\":58687,\"start\":58384},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":123006817},\"end\":58990,\"start\":58689},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5837272},\"end\":59348,\"start\":58992},{\"attributes\":{\"id\":\"b10\"},\"end\":59490,\"start\":59350},{\"attributes\":{\"id\":\"b11\"},\"end\":59636,\"start\":59492},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":34024981},\"end\":59935,\"start\":59638},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2949354},\"end\":60175,\"start\":59937},{\"attributes\":{\"id\":\"b14\"},\"end\":60491,\"start\":60177},{\"attributes\":{\"id\":\"b15\"},\"end\":60731,\"start\":60493},{\"attributes\":{\"id\":\"b16\"},\"end\":60997,\"start\":60733},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8978853},\"end\":61298,\"start\":60999},{\"attributes\":{\"id\":\"b18\"},\"end\":61658,\"start\":61300},{\"attributes\":{\"id\":\"b19\"},\"end\":61916,\"start\":61660},{\"attributes\":{\"id\":\"b20\"},\"end\":62167,\"start\":61918},{\"attributes\":{\"id\":\"b21\"},\"end\":62455,\"start\":62169},{\"attributes\":{\"id\":\"b22\"},\"end\":62663,\"start\":62457},{\"attributes\":{\"id\":\"b23\"},\"end\":62975,\"start\":62665},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2595046},\"end\":63387,\"start\":62977}]", "bib_title": "[{\"end\":57231,\"start\":57173},{\"end\":58471,\"start\":58384},{\"end\":58795,\"start\":58689},{\"end\":59051,\"start\":58992},{\"end\":59693,\"start\":59638},{\"end\":60008,\"start\":59937},{\"end\":61056,\"start\":60999},{\"end\":63082,\"start\":62977}]", "bib_author": "[{\"end\":56936,\"start\":56928},{\"end\":56949,\"start\":56936},{\"end\":56964,\"start\":56949},{\"end\":57083,\"start\":57074},{\"end\":57096,\"start\":57083},{\"end\":57246,\"start\":57233},{\"end\":57495,\"start\":57484},{\"end\":57787,\"start\":57774},{\"end\":57894,\"start\":57885},{\"end\":57903,\"start\":57894},{\"end\":57912,\"start\":57903},{\"end\":58239,\"start\":58227},{\"end\":58252,\"start\":58239},{\"end\":58490,\"start\":58473},{\"end\":58499,\"start\":58490},{\"end\":58808,\"start\":58797},{\"end\":59062,\"start\":59053},{\"end\":59071,\"start\":59062},{\"end\":59095,\"start\":59071},{\"end\":59361,\"start\":59350},{\"end\":59375,\"start\":59361},{\"end\":59534,\"start\":59524},{\"end\":59545,\"start\":59534},{\"end\":59707,\"start\":59695},{\"end\":59717,\"start\":59707},{\"end\":59728,\"start\":59717},{\"end\":59737,\"start\":59728},{\"end\":60023,\"start\":60010},{\"end\":60186,\"start\":60177},{\"end\":60198,\"start\":60186},{\"end\":60205,\"start\":60198},{\"end\":60214,\"start\":60205},{\"end\":60502,\"start\":60493},{\"end\":60512,\"start\":60502},{\"end\":60744,\"start\":60733},{\"end\":60752,\"start\":60744},{\"end\":61068,\"start\":61058},{\"end\":61079,\"start\":61068},{\"end\":61089,\"start\":61079},{\"end\":61310,\"start\":61300},{\"end\":61321,\"start\":61310},{\"end\":61667,\"start\":61660},{\"end\":61676,\"start\":61667},{\"end\":61925,\"start\":61918},{\"end\":61934,\"start\":61925},{\"end\":62266,\"start\":62252},{\"end\":62278,\"start\":62266},{\"end\":62288,\"start\":62278},{\"end\":62471,\"start\":62457},{\"end\":62481,\"start\":62471},{\"end\":62783,\"start\":62769},{\"end\":63090,\"start\":63084},{\"end\":63099,\"start\":63090}]", "bib_venue": "[{\"end\":57338,\"start\":57296},{\"end\":57072,\"start\":57051},{\"end\":57294,\"start\":57246},{\"end\":57591,\"start\":57495},{\"end\":57823,\"start\":57787},{\"end\":58007,\"start\":57912},{\"end\":58225,\"start\":58147},{\"end\":58517,\"start\":58499},{\"end\":58815,\"start\":58808},{\"end\":59148,\"start\":59095},{\"end\":59394,\"start\":59375},{\"end\":59522,\"start\":59492},{\"end\":59758,\"start\":59737},{\"end\":60041,\"start\":60023},{\"end\":60319,\"start\":60214},{\"end\":60577,\"start\":60512},{\"end\":60842,\"start\":60752},{\"end\":61129,\"start\":61089},{\"end\":61472,\"start\":61321},{\"end\":61774,\"start\":61676},{\"end\":62025,\"start\":61934},{\"end\":62250,\"start\":62169},{\"end\":62539,\"start\":62481},{\"end\":62767,\"start\":62665},{\"end\":63152,\"start\":63099}]"}}}, "year": 2023, "month": 12, "day": 17}