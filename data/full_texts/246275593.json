{"id": 246275593, "updated": "2023-10-05 17:34:39.949", "metadata": {"title": "Text and Code Embeddings by Contrastive Pre-Training", "authors": "[{\"first\":\"Arvind\",\"last\":\"Neelakantan\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Raul\",\"last\":\"Puri\",\"middle\":[]},{\"first\":\"Alec\",\"last\":\"Radford\",\"middle\":[]},{\"first\":\"Jesse\",\"last\":\"Han\",\"middle\":[\"Michael\"]},{\"first\":\"Jerry\",\"last\":\"Tworek\",\"middle\":[]},{\"first\":\"Qiming\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Nikolas\",\"last\":\"Tezak\",\"middle\":[]},{\"first\":\"Jong\",\"last\":\"Kim\",\"middle\":[\"Wook\"]},{\"first\":\"Chris\",\"last\":\"Hallacy\",\"middle\":[]},{\"first\":\"Johannes\",\"last\":\"Heidecke\",\"middle\":[]},{\"first\":\"Pranav\",\"last\":\"Shyam\",\"middle\":[]},{\"first\":\"Boris\",\"last\":\"Power\",\"middle\":[]},{\"first\":\"Tyna\",\"last\":\"Nekoul\",\"middle\":[\"Eloundou\"]},{\"first\":\"Girish\",\"last\":\"Sastry\",\"middle\":[]},{\"first\":\"Gretchen\",\"last\":\"Krueger\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Schnurr\",\"middle\":[]},{\"first\":\"Felipe\",\"last\":\"Such\",\"middle\":[\"Petroski\"]},{\"first\":\"Kenny\",\"last\":\"Hsu\",\"middle\":[]},{\"first\":\"Madeleine\",\"last\":\"Thompson\",\"middle\":[]},{\"first\":\"Tabarak\",\"last\":\"Khan\",\"middle\":[]},{\"first\":\"Toki\",\"last\":\"Sherbakov\",\"middle\":[]},{\"first\":\"Joanne\",\"last\":\"Jang\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Welinder\",\"middle\":[]},{\"first\":\"Lilian\",\"last\":\"Weng\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.10005", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2201-10005", "doi": null}}, "content": {"source": {"pdf_hash": "6d7d4fca9840504f630e9bea6acaa07322a6e889", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.10005v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ae0376d106b975cd286992208ccfdd1e8457e117", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6d7d4fca9840504f630e9bea6acaa07322a6e889.txt", "contents": "\nText and Code Embeddings by Contrastive Pre-Training\n\n\nArvind Neelakantan \nTao Xu \nRaul Puri \nAlec Radford \nJesse Michael Han \nJerry Tworek \nQiming Yuan \nNikolas Tezak \nJong Wook Kim \nChris Hallacy \nJohannes Heidecke \nPranav Shyam \nBoris Power \nTyna Eloundou Nekoul \nGirish Sastry \nGretchen Krueger \nDavid Schnurr \nFelipe Petroski Such \nKenny Hsu \nMadeleine Thompson \nTabarak Khan \nToki Sherbakov \nJoanne Jang \nPeter Welinder \nLilian Weng \nText and Code Embeddings by Contrastive Pre-Training\n\nText embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.\n\nIntroduction\n\nDeep unsupervised learning with generative and embedding models has seen dramatic success in the past few years. Generative models (Peters et al., 2018;Raffel et al., 2019;van den Oord et al., 2016;Ramesh et al., 2021;Brown et al., 2020;Chen et al., 2021)  imize the likelihood of observed data while embedding models are trained to distinguish observed data from noise (Sohn, 2016;van den Oord et al., 2018;Radford et al., 2021;Jia et al., 2021;Gao et al., 2021;Izacard et al., 2021). Generative models have been shown to produce realistic content and benefit many downstream applications, reducing the need for labeled training datasets. In generative models, the information about the input is typically distributed over multiple hidden states of the model. While some generative models (Kingma & Welling, 2014;Kiros et al., 2015) can learn a single representation of the input, most autoregressive Transformer (Vaswani et al., 2017) models do not (Raffel et al., 2019;Brown et al., 2020;Chen et al., 2021;Ramesh et al., 2021). However, learning such a representation (or embedding) is necessary for many tasks. Systems that search over millions or billions of items require each entry to be embedded as a dense representation and build an index in advance to save computational costs at query time. These embeddings are useful features for classification tasks and can also enable data visualization applications via techniques such as clustering. Embedding models are explicitly optimized to learn a low dimensional representation that captures the semantic meaning of the input Jia et al., 2021;Giorgi et al., 2020;Gao et al., 2021;Izacard et al., 2021).\n\nIn this work, we train embedding models using a contrastive learning objective with in-batch negatives (Sohn, 2016;Yih et al., 2011) on unlabeled data. The input is encoded with a Transformer encoder (Vaswani et al., 2017) and we leverage naturally occurring paired data to construct training data with no explicit labels. Text embedding models are trained on paired text data where we consider neighboring pieces of text on the Internet as positive pairs. Code embedding models treat the top-level docstring in a function along with its implementation as a (text, code) pair. The training signal of the contrastive objective on its own is not sufficient to learn useful representations and we overcome this by initializing our model with other pretrained models (Brown et al., 2020;Chen et al., 2021). Finally, we find that it is critical to use a sufficiently large batch to achieve the optimal performance. We show that this simple recipe combining pre-trained model initialization, large-batch contrastive learning and training at scale, can produce text and code embeddings that possess a broad range of capabilities.\n\nWe train a series of unsupervised text embedding models (cpt-text) of different sizes, ranging from 300M to 175B parameters, and observe a consistent performance improvement with increasing model sizes ( Figure  1). On classification accuracy averaging across 7 linearprobe classification tasks in SentEval (Conneau & Kiela, 2018), our largest unsupervised model achieves new stateof-the-art results with a relative improvement of 4% and 1.8% over the previous best unsupervised (Giorgi et al., 2020) and supervised (Gao et al., 2021) text embedding models, respectively.\n\nText embedding in previous work was studied under different domains, varying in data, training objective and model architecture. Precisely, sentence embedding (Reimers & Gurevych, 2019;Gao et al., 2021;Giorgi et al., 2020) and neural information retrieval Guu et al., 2020;Karpukhin et al., 2020a;Sachan et al., 2021;Izacard et al., 2021) have remained different research topics evaluated on distinct benchmarks, even though both aim to learn high-quality text representation. However, we find the same model that achieves good performance on sentence embedding benchmarks, as discussed above, is also able to obtain impressive results on large-scale information retrieval. When evaluated on the MSMARCO passage ranking task (Nguyen et al., 2016) to search over 4M passages, cpt-text gets a relative improvement of 23.4% over previous best unsupervised methods (Robertson, 2009). On the task of searching on 21M documents from Wikipedia, cpt-text obtains a relative improvement of 14.7%, and 10.6% over previous unsupervised methods (Izacard et al., 2021) for Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017), respectively. On Triv-iaQA, our unsupervised method is even competitive with fine-tuned models.\n\nNext, we train code embedding models (cpt-code) using the same recipe. Our models learn via (text, code) pairs, extracted from open source code. We evaluate our model on CodeSearchNet (Husain et al., 2020), a commonly used code search benchmark, where the task is to find the most relevant code snippet given a natural language query. Our models achieve new state-of-the-art results with a 20.8% relative improvement over the previous best result (Guo et al., 2021). Unlike text embedding models, we observe no performance improvement on code search when increasing the number of parameters of cpt-code from 300M to 1.2B.\n\nFinally, we experiment with fine-tuning our models on several supervised datasets and study the transfer learning performance. When fine-tuned on NLI (Natural Language Inference) datasets, we see a further boost in linearprobe classification, outperforming the previous best transfer method (Gao et al., 2021) by 2.2%. On SST-2 sentiment classification (Socher et al., 2013), we find that our representations are sufficiently descriptive that even a simple k-NN classifier achieves results comparable to a linearprobe classifier. Interestingly, zero-shot performance with our embeddings outperforms the supervised neural network models introduced along with the release of the SST-2 dataset. We also fine-tune the unsupervised model on MS-MARCO and evaluate it on a suite of zero-shot search tasks in the BEIR benchmark (Thakur et al., 2021). In the transfer setting, our models achieve a 5.2% relative improvement over previous methods (Izacard et al., 2021) and is comparable even with methods (Santhanam et al., 2021;Formal et al., 2021;Wang et al., 2020) that demand substantially more computation at test time.\n\n\nApproach\n\nOur models are trained with a contrastive objective on paired data. In this section, we present more details on the model architecture and the training objective. The training set consists of paired samples,\n{(x i , y i )} N i=1 ,\nwhere (x i , y i ) corresponds to a positive example pair, indicating that x i and y i are semantically similar or contextually relevant.\n\n\nModel\n\nGiven a training pair (x, y), a Transformer (Vaswani et al., 2017) encoder E is used to process x and y independently. The encoder maps the input to a dense vector representation or embedding ( Figure 2). We insert two special token delimiters, [SOS] and [EOS], to the start and end of the input sequence respectively. The hidden state from the last layer corresponding to the special token [EOS] is considered as the embedding of the input sequence. , are appended to the start and end of the input sequence respectively. The last layer hidden state corresponding to the token [EOS] is extracted as the embedding of the input sequence.\n\n[EOS] Figure 3. The encoder E maps inputs x and y, to embeddings, vx and vy independently. The similarity score between x and y is defined as the cosine similarity between these two embedding vectors.\n\n\nENCODER\n\nThe Transformer encoder maps the input, x and y, to embeddings, v x and v y respectively and the similarity between two inputs is quantified by the cosine similarity between their embeddings, v x and v y (Figure 3).\nv x = E([SOS] x \u2295 x \u2295 [EOS] x ) v y = E([SOS] y \u2295 y \u2295 [EOS] y ) sim(x, y) = v x \u00b7 v y v x \u00b7 v y\nwhere \u2295 is an operation to concatenate two strings together. We found that using different delimiters leads to more stable training. For x, we use '[' as [SOS] x and ']' as [EOS] x , while we use '{' and '}' as [SOS] y and [EOS] y respectively for y.\n\n\nTraining Objective\n\nThe paired samples in the training set are contrasted against in-batch negatives (Yih et al., 2011;Sohn, 2016 used for unsupervised representation learning in prior work Jia et al., 2021;Izacard et al., 2021). \nx i , y j ) is given by, logit(x i ,y j ) = sim(x i , y j ) \u00b7 exp(\u03c4 ), \u2200(i, j), i, j \u2208 {1, 2, . . . , M }\nwhere \u03c4 is a trainable temperature parameter.\n\nOnly entries on the diagonal of the matrix are considered positive examples. The final training loss is the sum of the cross entropy losses on the row and the column direction, as described in the following numpy style pseudo code. labels = np.arange(M) l_r = cross_entropy(logits, labels, axis=0) l_c = cross_entropy(logits, labels, axis=1) loss = (l_r + l_c) / 2\n\nWe initialize our models with pre-trained generative language models. cpt-text is initialized with GPT models (Brown et al., 2020) and cpt-code is initialized with Codex models . When fine-tuning our models (Section 3), the supervised training data like NLI datasets contain explicit negative examples and they are used along with the in-batch negatives.\n\n\nResults\n\nOur models are trained on naturally occurring paired data. cpt-text models are trained on Internet data with neighboring pieces of text as positive pairs for the contrastive objective. The code embedding cpt-code models use (text, code) pairs extracted from open source code. As discussed in Section 3.4.1, sufficiently large batch size is crucial to achieve good performance with our setup. Table 1 lists the batch sizes used to train the models of different sizes.\n\nWe evaluate our text embedding models on a broad range of tasks: linear-probe classification, sentence similarity, and semantic search. While sentence embedding (Reimers & Gurevych, 2019;Gao et al., 2021;Giorgi et al., 2020) methods report results only on embedding benchmarks and neural information retrieval methods Guu et al., 2020;Karpukhin et al., 2020a;Sachan et al., 2021;Izacard et al., 2021) report results only on search benchmarks, we use the same unsupervised model across all these tasks.\n\n\nText Embedding\n\nThe SentEval benchmark (Conneau & Kiela, 2018) is widely adopted to assess the quality of sentence embeddings, consisting of a broad collection of tasks in the categories of linear-probe classification and sentence similarity, and we use the same to evaluate ours.\n\n\nLINEAR PROBE CLASSIFICATION\n\nWhen evaluated on linear-probe classification, the embeddings are used as features to train a linear classifier to solve a variety of downstream tasks. The results in Table 2 demonstrate a clear advantage of larger model sizes producing better features for improved classification performance. In transfer learning setup, we fine-tune unsupervised cpt-text models on SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) datasets using entailment pairs as positive examples and contradiction pairs as negative examples. On both unsupervised learning and transfer learning settings, we achieve state-of-the-art results.\n\n\nZERO-SHOT AND k-NN CLASSIFICATION\n\nIn this section, we discuss results using zero-shot classification and k-nearest neighbor classification on the SST-2 binary sentiment classification task (Socher et al., 2013). We experiment with 6B (L) cpt-text model fine-tuned on NLI data for this study. In the first zero-shot experiment, each input text is assigned with one of the two labels ('positive', 'negative') based on which label has its embedding closest to the input text embedding. The performance can be further improved by prompting, where we use a simple label description, 'this is an example of a positive/negative movie review.', instead of a single word. This zero-shot usage of embeddings is novel compared to prior work on embeddings and it is interesting to note that our zero-shot results are better than the supervised neural network results reported along with the release of the dataset (Socher et al., 2013). In the k-NN classification experiment, given an input text, the prediction is the majority label among 256 training examples closest to the test input in the embedding space. As shown in Table 3, the k-NN classifier without any task-specific tuning of trainable parameters achieves results comparable to a linear classifier.\n\n\nSENTENCE SIMILARITY\n\nOn sentence similarity tasks in SentEval, we find that our models perform worse than previous SOTA methods ( Table 4). Sentence similarity is not a completely well-defined downstream task (e.g. are the sentences, 'Jack loves Jill' and 'Mary loves chocolates', similar?). 1,2 For example, Goodman (1972) argue that two objects can be infinitely similar or dissimilar (Vervaeke et al., 2012). A possible explanation for why our models perform better than prior work on search and classification but not on these tasks is that our models might not be optimized for the specific definition used by these sentence similarity benchmarks. It is important to note that previous embedding search methods do not report performance on sentence similarity tasks (Karpukhin et al., 2020a;Sachan et al., 2021;Izacard et al., 2021). More discussion on this phenomenon is presented in Section 3.4.2.\n\n\nText Search\n\nPrevious work on training embedding methods for search typically requires fine-tuning on a particular text search dataset (Karpukhin et al., 2020a;Sachan et al., 2021;Qu et al., 2021). It is also common to have a multi-step setup where fine-tuned models rely on an expensive query and document cross-attention encoder in the final step (Qu et al., 2021;Wang et al., 2020). In contrast, we push the limits of using a single embedding model for large-scale semantic search.\n\n\nLARGE-SCALE SEARCH\n\nFirst, we evaluate our models on several large-scale text search benchmarks. MSMARCO (Nguyen et al., 2016) requires the model to search over 4M documents while Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) involve searching over 21M Wikipedia documents. We use the FAISS library (Johnson et al., 2019) to build the vector indices for approximate k-nearest neighbor search. The same unsupervised model discussed previously achieves impressive performance on semantic search.  Table 3. Comparison of different classification strategies using the 6B cpt-text model fine-tuned on NLI data for SST-2 binary sentiment task (Socher et al., 2013). Our zero-shot results are better than the 85.4% accuracy obtained by supervised neural networks reported along with the release of the dataset (Socher et al., 2013 \n\n\nBEIR SEARCH\n\nNext, we evaluate our models on 11 zero-shot search tasks in the BEIR evaluation suite (Thakur et al., 2021). First, we observe that our unsupervised model performs competitively even with some previous embedding methods that leverage supervised MSMARCO data (Xiong et al., 2020;Hofst\u00e4tter et al., 2021). Keyword-based BM25 (Robertson, 2009) achieves the best results in the unsupervised setting while cpt-text achieves the best transfer learning results.\n\nIn the transfer setting, our models achieve a 5.2% relative improvement over the previous best embedding method (Izacard et al., 2021). It also outperforms docT5query (Nogueira et al., 2019a) that relies on a fine-tuned T5 model (Raffel et al., 2019) for document expansion. cpt-text results are competitive even with methods that use substantially more compute at test time. BM25+CE  uses keyword search to select top 100 documents which are then re-ranked by a cross-attention neural network encoder. The ranking encoder network performs computationally expensive joint query and document attention and cannot exploit indexing and approximate nearest neighbor algorithms for fast and efficient search at query time. Several other existing work take this approach of leveraging more computation resources at query time to obtain better search performance. ColBERT v2 (Santhanam et al., 2021) is a multi-vector method that represents the query and the documents as a set of vectors, and employs a multi-step retrieval procedure to obtain relevant documents. Splade v2 (Formal et al., 2021) represents queries and documents as sparse vectors of size equivalent to the vocabulary of the BERT encoder . Our cpt-text models compute only one dense embedding per document which are indexed offline and does not depend on any cross-attention re-ranker at query time.\n\n\nCode Search\n\nWe evaluate our code embedding models on the code search task using the CodeSearchNet benchmark (Husain et al., 2020). Given a natural language query, the model is expected to retrieve the relevant code block among 1K candidates. The models are evaluated on 6 programming languages and our model achieves state-of-the-art results (Table 7). Unlike with text embeddings, we do not see a performance improvement with increased model size for code embeddings.\n\nWe also evaluate on a harder setting of finding the relevant code block among 10K candidates instead of 1K. Here, we compare the performance of cpt-text models against cpt-code models (Table 8). It is interesting to see that text embedding performs fairly well in code search especially in Python. We see a drop in performance for code embedding models with increased distractors and still don't see bigger models giving a boost in search performance.\n\n\nAnalysis\n\n\nEFFECT OF BATCH SIZE\n\nOur ablation study highlights the effect of the model's batch size on the final performance. Table 9 compares the performance of S (300M) cpt-text model trained with different batch sizes on the NQ development set. Since we train with in-batch negatives, a larger batch increases the chances of having hard negatives in a batch, resulting in a significant performance boost.\n\n\nTRAINING BEHAVIOR\n\nWe observe that as we train our models for longer, the performance on search and classification tasks increases while the performance on sentence similarity tasks decreases ( Figure 4). As discussed previously, sentence similarity is not a well defined task. A hypothesis is that search tasks and sentence similarity tasks might have contradicting definitions. For example, a sentence and its negation could be considered as relevant during search, but not \"similar\" in sentence similarity tasks. It is also important to note that previous embedding search methods do not report performance on sentence similarity tasks (Karpukhin et al., 2020a;Sachan et al., 2021;Izacard et al., 2021) and previous sentence embedding methods do not evaluate on search tasks (Reimers & Gurevych, 2019;Giorgi et al., 2020;Gao et al., 2021). When deciding the model checkpoints to use for evaluation, we assigned higher importance to search and classification tasks as they are commonly associated with clearly defined real-world applications while sentence similarity tasks are less so.\n\n\nRelated Work\n\nThe goal of representation learning (Bengio et al., 2012) is to learn an embedding space in which similar examples stay close to each other while dissimilar ones are far apart (Hadsell et al., 2006). In contrastive learning, the learning procedure is formulated as a classification problem given similar and dissimilar candidates (Chopra et al., 2005;Gutmann & Hyv\u00e4rinen, 2010;Schroff et al., 2015;Sohn, 2016;van den Oord et al., 2018). Recent work relies on contrastive objective to learn representations for images (Wu et al., 2018;He et al., 2020;Zbontar et al., 2021), text, or both jointly (Lu et al., 2019;Sun et al., 2019;Kim et al., 2021;Radford et al., 2021;Khosla et al., 2020). In self-supervised contrastive learning, positive samples can be collected in various approaches including by creating an augmented version of the original input without modifying the semantic meaning (Gao  Table 6. Comparison of cpt-text to previous methods on 11 zero-shot search tasks in the BEIR evaluation suite (Thakur et al., 2021). Results are reported both in the unsupervised data setting and in the transfer data setting. cpt-text outperforms previous best embedding methods (Xiong et al., 2020;Hofst\u00e4tter et al., 2021;Izacard et al., 2021) in both the settings. In the unsupervised setting, BM25 (Robertson, 2009) still achieves the best performance while in the transfer setting cpt-text is competitive with methods that use substantially more compute at test time Santhanam et al., 2021;Formal et al., 2021).\n\nGo Ruby Python Java JS PHP Avg.  Table 7. Comparison of cpt-code on code search across 6 programming languages (Husain et al., 2020) with CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021). The task requires finding the relevant code block among 1K candidates for a given natural language query. cpt-code performs substantially better than previous methods on all the languages.\n\nGo Ruby Python Java JS PHP Avg.  (Giorgi et al., 2020;Izacard et al., 2021), or by collecting data about the same object from different views (Tian et al., 2019).\n\nLearning word embeddings is a well studied research area (Brown et al., 1992;Gutmann & Hyv\u00e4rinen, 2010;Mikolov et al., 2013;Pennington et al., 2014). Learning lowdimensional representations of larger text pieces, denser than raw term-based vectors, has been studied extensively as well (Deerwester et al., 1990;Yih et al., 2011). Most of the recent models for learning sentence embeddings rely on supervised NLI datasets, using entailment pairs as positive examples and contradiction pairs as (hard) negatives. SBERT (Reimers & Gurevych, 2019) trained a siamese network to learn a representation where sentence similarity is estimated by the cosine similarity between embeddings. Li et al. (2020) improves the embedding space to be isotropic via normalizing flows. The whitening operation is another alternative operation to improve the isotropy of the embedding space (Su et al., 2021). It is typical to initialize such models with a pre-trained language model  before training on NLI datasets.\n\nSeveral methods have been studied for unsupervised or self-supervised sentence embedding learning (Logeswaran & Lee, 2018;Zhang et al., 2020;Gao et al., 2021). Common approaches consider sentences within the same context as semantically similar samples (Kiros et al., 2015;Logeswaran & Lee, 2018). To create positive training pairs with augmented samples, a diverse set of text augmentation operations have been explored, including lexiconbased distortion (Wei & Zou, 2019), synonym replacement (Kobayashi, 2018), back-translation (Fang & Xie, 2020), cut-off (Shen et al., 2020) and dropout (Gao et al., 2021). However, unsupervised sentence embedding models still perform notably worse than supervised sentence encoders.\n\nLarge-scale text search based on dense embeddings and neural information retrieval (neural IR) have the potential to generalize better than keyword matching in classic IR systems. Neural IR systems encode documents at the indexing stage and then perform nearest neighbor search (Johnson et al., 2019) at query time . Neural IR models are usually learned by fine-tuning a pretrained language model on supervised search corpus Guu et al., 2020;Karpukhin et al., 2020b;Lewis et al., 2020). Many SOTA search models combine classical IR with neural IR in a staged setup, where the candidates are first narrowed down by BM25 keyword search (Robertson, 2009) and then re-ranked by joint query and document neural encoders (Nogueira et al., 2019b;Qu et al., 2021). Xiong et al. (2020) proposed ANCE, a contrastive learning framework for learning text representations for dense retrieval using mined hard negatives. Other unsupervised retriever methods use the Inverse Cloze Task or masked salient spans to achieve significant improvement on ODQA tasks (Sachan et al., 2021). In comparison to most prior work, we find that with a large enough batch size, it is possible to achieve good search performance without using supervised data. Finally, the recently published Contriever (Izacard et al., 2021) is most similar to our work on learning text embeddings for text search using contrastive learning on unlabeled data.\n\nSemantic code search refers to the task of retrieving code relevant to a query in natural language. The CodeSearch-Net challenge (Husain et al., 2020) presents a set of benchmark code search tasks in different programming languages, as well as a simple baseline model to predict embeddings of query and code via contrastive learning on a dataset of (text, code) pairs. ContraCode (Jain et al., 2021) uses a contrastive learning task of identifying functionally similar programs, where the functionally similar samples are generated via source-to-source compiler transformations. CodeBERT (Feng et al., 2020) learns to predict semantic similarity with a pre-trained language model and GraphCodeBERT (Guo et al., 2021) further improves the performance on the CodeSearchNet benchmark by adding pre-training tasks on code structure.\n\n\nBroader Impacts\n\nPrior research has shown that text representation models encode the biases present in their training data, including those which are discriminatory towards protected groups such as Black people or women (Bolukbasi et al., 2016;Caliskan et al., 2017;May et al., 2019;Zhao et al., 2018;Rudinger et al., 2018). Biases encoded in embedding models may cause representational harms 3 by reinforcing existent societal biases in the text corpus, and further propagating them in downstream tasks of embedding models.\n\nTherefore, we encourage further research on two research agendas: (a) developing robust evaluation methodologies for multiple classes of bias in training data and pre-trained models, and (b) developing and improving methods for mitigating encoded bias, including fine-tuning to reduce bias in pre-trained models (Caliskan et al., 2017;May et al., 2019;Bolukbasi et al., 2016;Liang et al., 2020;Park et al., 2018;Solaiman & Dennison, 2021). Until we have robust evaluation methodology, it is important to restrict and monitor the use of the model in downstream applications. Par-ticularly for those where risk of representational harm is great and those where biased representations may influence the allocation of resources and opportunities to people.\n\nOur embedding models are trained with large batch sizes and require substantial computation resources. While this training regime is environmentally and computationally costly, there are promising paths forward to amortize and offset these costs while allowing users to benefits from the capabilities of these models. For example, safe public access to large pre-trained language models, and efficient training pipelines that leverage improved model architectures and training schemes. We encourage further research and implementation efforts in these areas.\n\n\nConclusion\n\nWe showed that contrastive pre-training on unsupervised data with a sufficiently large batch size can lead to high quality vector representations of text and code. Our models achieved new state-of-the-art results in linear-probe classification, text search and code search. We find that our models underperformed on sentence similarity tasks and observed unexpected training behavior with respect to these tasks. Finally, we discussed the broader impact of our work on society.\n\nFigure 1 .\n1Average performance of unsupervised cpt-text models of different sizes across 22 tasks consisting of linear-probe classification, text search, and sentence similarity tasks.\n\nFigure 2 .\n2The encoder E maps input x to embedding vx. Special tokens, [SOS] and [EOS]\n\n\nFor each example in a mini-batch of M examples, the other (M \u2212 1) in the batch are used as negative examples. The usage of in-batch negatives enables re-use of computation both in the forward and the backward pass making training highly efficient. The logits for one batch is a M \u00d7 M matrix, where each entry logit(\n\nFigure 4 .\n4Performance of M (1.2B) cpt-text model on classification, search and sentence similarity tasks at different training steps. While the performance on search and classification improves with longer training, the performance on sentence similarity degrades.\n\n\n). Contrastive learning with in-batch negatives has been widely Model Parameters Embed Dimensions Batch sizeTable 1. Batch size used to train the models of different sizes.S \n300M \n1024 \n12288 \nM \n1.2B \n2048 \n6912 \nL \n6B \n4096 \n5896 \nXL \n175B \n12288 \n4976 \n\n\n\nTable 5\n5demonstrates that cpt-text \noutperforms prior unsupervised approaches by a big mar-\ngin and larger model sizes consistently lead to improved \nperformance. Surprisingly, on TriviaQA, our model is even \ncompetitive with fine-tuned models. \n\n\n\n).STS -12 -13 -14 -15 -16 Avg \nUnsupervised \n\nSimCSE (Gao et al., 2021) 72.9 84.0 75.6 84.8 81.8 79.8 \ncpt-text S \n62.1 60.0 62.0 71.8 73.7 65.9 \ncpt-text M \n62.7 62.8 64.6 73.9 75.3 67.9 \ncpt-text L \n62.4 66.4 67.6 76.0 77.5 70.0 \ncpt-text XL \n64.1 67.5 68.4 76.7 78.7 71.1 \nTransfer from NLI \n\nSimCSE (Gao et al., 2021) 77.5 87.3 82.4 86.7 83.9 83.6 \ncpt-text S \n72.8 80.6 78.7 84.7 82.0 79.8 \ncpt-text M \n73.7 80.2 78.9 85.0 82.8 80.1 \ncpt-text L \n71.8 79.7 79.0 85.8 84.0 80.1 \ncpt-text XL \n72.3 80.3 78.9 85.1 85.1 80.3 \n\nTable 4. cpt-text performs worse than the previous best sen-\ntence embedding method on sentence similarity tasks. We inves-\ntigate this result in more detail in Section 3.4.2. \n\nMSMARCO \nNQ \nTriviaQA \nFine-tuned SOTA \n44.3 \n84.8, 89.8 84.1, 87.8 \n\nUnsupervised \n\nBM25 \n18.4 \n62.9, 78.3 76.4, 83.2 \nICT \n-\n50.9, 66.8 57.5, 73.6 \nMSS \n-\n59.8, 74.9 68.2, 79.4 \nContriever \n-\n67.2, 81.3 74.2, 83.2 \n\ncpt-text S \n19.9 \n65.5, 77.2 75.1, 81.7 \ncpt-text M \n20.6 \n68.7, 79.6 78.0, 83.8 \ncpt-text L \n21.5 \n73.0, 83.4 80.0, 86.8 \ncpt-text XL \n22.7 \n78.8, 86.8 82.1, 86.9 \n\nTable 5. Evaluation of unsupervised cpt-text models of differ-\nent sizes on several large-scale text search benchmarks. We report \nMRR@10 on MSMARCO and Recall@20, Recall@100 for NQ \nand TriviaQA as done in prior work. Results for training with \nInverse Cloze Task (ICT) and masked salient spans (MSS) objec-\ntives are taken from Sachan et al. (2021). cpt-text achieves the \nbest results among unsupervised methods, surpassing keyword \nsearch methods on MSMARCO (Robertson, 2009) and embed-\nding based methods (Izacard et al., 2021) on NQ and TriviaQA. \n\nhttps://twitter.com/yoavgo/status/\nhttps://twitter.com/yoavgo/status/ 1483565266575540225?s=20\nRepresentational harms occur when systems reinforce the subordination of some groups along the lines of identity, e.g. stereotyping or denigration(Crawford, 2017).\n\nRepresentation learning: A review and new perspectives. Transactions on pattern analysis and machine intelligence. Y Bengio, A C Courville, P Vincent, 35Bengio, Y., Courville, A. C., and Vincent, P. Representa- tion learning: A review and new perspectives. Transac- tions on pattern analysis and machine intelligence, 35 (8), 2012.\n\nMan is to computer programmer as woman is to homemaker? debiasing word embeddings. T Bolukbasi, K Chang, J Y Zou, V Saligrama, A Kalai, 29Bolukbasi, T., Chang, K., Zou, J. Y., Saligrama, V., and Kalai, A. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. 29, 2016.\n\nA large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL. Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language inference. In Conference on Empirical Methods in Nat- ural Language Processing (EMNLP). ACL, 2015.\n\nClass-based n-gram models of natural language. P F Brown, V J Della Pietra, P V Desouza, J C Lai, R L Mercer, Computational Linguistics. 184Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. Class-based n-gram models of nat- ural language. Computational Linguistics, 18(4):467- 480, 1992.\n\nLanguage models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, Amodei , D , Advances in Neural Information Processing Systems. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.\n\nSemantics derived automatically from language corpora contain human-like biases. A Caliskan, J J Bryson, A Narayanan, Science. 3566334Caliskan, A., Bryson, J. J., and Narayanan, A. Seman- tics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186, 2017.\n\n. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, arXiv:2107.03374arXiv preprintEvaluating large language models trained on codeChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar- ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G E Hinton, International conference on machine learning (ICML). 2020Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A simple framework for contrastive learning of visual representations. In International conference on machine learning (ICML), 2020.\n\nLearning a similarity metric discriminatively, with application to face verification. S Chopra, R Hadsell, Y Lecun, Computer Vision and Pattern Recognition (CVPR). IEEEChopra, S., Hadsell, R., and LeCun, Y. Learning a similar- ity metric discriminatively, with application to face ver- ification. In Computer Vision and Pattern Recognition (CVPR). IEEE, 2005.\n\nSenteval: An evaluation toolkit for universal sentence representations. A Conneau, D Kiela, arXiv:1803.05449arXiv preprintConneau, A. and Kiela, D. Senteval: An evaluation toolkit for universal sentence representations. arXiv preprint arXiv:1803.05449, 2018.\n\nThe trouble with bias. Keynote at NeurIPS. K Crawford, Crawford, K. The trouble with bias. Keynote at NeurIPS, 2017.\n\nIndexing by latent semantic analysis. S Deerwester, S Dumais, G Furnas, T Landauer, R Harshman, Journal of the American society for information science. 416Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391-407, 1990.\n\nPre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). ACL. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). ACL, 2019.\n\nCERT: contrastive self-supervised learning for language understanding. H Fang, P Xie, arXiv:2005.12766arXiv preprintFang, H. and Xie, P. CERT: contrastive self-supervised learning for language understanding. arXiv preprint arXiv:2005.12766, 2020.\n\nCodebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, M Zhou, Conference on Empirical Methods in Natural Language Processing. 2020Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. Code- bert: A pre-trained model for programming and natural languages. In Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), 2020.\n\nT Formal, C Lassance, B Piwowarski, S Clinchant, arXiv:2109.10086Sparse lexical and expansion model for information retrieval. arXiv preprintFormal, T., Lassance, C., Piwowarski, B., and Clinchant, S. SPLADE v2: Sparse lexical and expansion model for information retrieval. arXiv preprint arXiv:2109.10086, 2021.\n\nSimple contrastive learning of sentence embeddings. T Gao, X Yao, Chen , D Simcse, Conference on Empirical Methods in Natural Language Processing. 2021Gao, T., Yao, X., and Chen, D. SimCSE: Simple con- trastive learning of sentence embeddings. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n\nDeclutr: Deep contrastive learning for unsupervised textual representations. J M Giorgi, O Nitski, G D Bader, Wang , B , Proceedings of ACL/IJCNLP. ACL/IJCNLP2020Giorgi, J. M., Nitski, O., Bader, G. D., and Wang, B. De- clutr: Deep contrastive learning for unsupervised textual representations. In Proceedings of ACL/IJCNLP, 2020.\n\nSeven strictures on similarity. N Goodman, Bobbs Merrill. Goodman, N. Seven strictures on similarity. Bobbs Merrill, 1972.\n\nD Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, M Tufano, S K Deng, C B Clement, D Drain, N Sundaresan, J Yin, D Jiang, M Zhou, Graphcodebert, International Conference on Learning Representation (ICLR. 2021Pretraining code representations with data flowGuo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S. K., Clement, C. B., Drain, D., Sundaresan, N., Yin, J., Jiang, D., and Zhou, M. Graphcodebert: Pre- training code representations with data flow. In Interna- tional Conference on Learning Representation (ICLR), 2021.\n\nNoise-contrastive estimation: A new estimation principle for unnormalized statistical models. M Gutmann, A Hyv\u00e4rinen, Conference on Artificial Intelligence and Statistics. PMLR. Gutmann, M. and Hyv\u00e4rinen, A. Noise-contrastive estima- tion: A new estimation principle for unnormalized sta- tistical models. In Conference on Artificial Intelligence and Statistics. PMLR, 2010.\n\nREALM: retrieval-augmented language model pretraining. K Guu, K Lee, Z Tung, P Pasupat, Chang , M , arXiv:2002.08909arXiv preprintGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. REALM: retrieval-augmented language model pre- training. arXiv preprint arXiv:2002.08909, 2020.\n\nDimensionality reduction by learning an invariant mapping. R Hadsell, S Chopra, Y Lecun, Computer Vision and Pattern Recognition (CVPR). IEEE2Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 1735-1742. IEEE, 2006.\n\nMomentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R B Girshick, Computer Vision and Pattern Recognition (CVPR). 2020He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Mo- mentum contrast for unsupervised visual representation learning. In Computer Vision and Pattern Recognition (CVPR), 2020.\n\nEfficiently teaching an effective dense retriever with balanced topic aware sampling. S Hofst\u00e4tter, S Lin, J Yang, J Lin, A Hanbury, arXiv:2104.06967arXiv preprintHofst\u00e4tter, S., Lin, S., Yang, J., Lin, J., and Hanbury, A. Efficiently teaching an effective dense retriever with balanced topic aware sampling. arXiv preprint arXiv:2104.06967, 2021.\n\nCodeSearchNet challenge: Evaluating the state of semantic code search. H Husain, H.-H Wu, T Gazit, M Allamanis, M Brockschmidt, arXiv:1909.09436arXiv preprintHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. CodeSearchNet challenge: Evaluat- ing the state of semantic code search. arXiv preprint arXiv:1909.09436, 2020.\n\nTowards unsupervised dense information retrieval with contrastive learning. G Izacard, M Caron, L Hosseini, S Riedel, P Bojanowski, A Joulin, E Grave, arXiv:2112.09118arXiv preprintIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bo- janowski, P., Joulin, A., and Grave, E. Towards unsuper- vised dense information retrieval with contrastive learn- ing. arXiv preprint arXiv:2112.09118, 2021.\n\nContrastive code representation learning. P Jain, A Jain, T Zhang, P Abbeel, J E Gonzalez, I Stoica, Conference on Empirical Methods in Natural Language Processing. 2021Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J. E., and Stoica, I. Contrastive code representation learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n\nScaling up visual and vision-language representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q V Le, Y Sung, Z Li, T Duerig, International Conference on Machine Learning (ICML). 2021Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021.\n\nBillion-scale similarity search with gpus. J Johnson, M Douze, H J\u00e9gou, IEEE Transactions on Big Data. Johnson, J., Douze, M., and J\u00e9gou, H. Billion-scale simi- larity search with gpus. IEEE Transactions on Big Data, 2019.\n\nTriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D Weld, L Zettlemoyer, Conference of the Association for Computational Linguistics (ACL). ACL. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Conference of the Association for Computational Linguistics (ACL). ACL, 2017.\n\nDense passage retrieval for open-domain question answering. V Karpukhin, B Oguz, S Min, P Lewis, L Wu, S Edunov, D Chen, W Yih, Conference on Empirical Methods in Natural Language Processing (EMNLP). Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Con- ference on Empirical Methods in Natural Language Pro- cessing (EMNLP), 2020a.\n\nDense passage retrieval for opendomain question answering. V Karpukhin, B Oguz, S Min, L Wu, S Edunov, D Chen, Yih , W , Conference on Empirical Methods in Natural Language Processing (EMNLP). Karpukhin, V., Oguz, B., Min, S., Wu, L., Edunov, S., Chen, D., and Yih, W. Dense passage retrieval for open- domain question answering. In Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2020b.\n\n. P Khosla, P Teterwak, C Wang, A Sarna, Y Tian, P Isola, A Maschinot, C Liu, D Krishnan, arXiv:2004.11362arXiv preprintSupervised contrastive learningKhosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., and Krishnan, D. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2020.\n\nVilt: Vision-andlanguage transformer without convolution or region supervision. W Kim, B Son, Kim , I , International Conference on Machine Learning (ICML). 2021Kim, W., Son, B., and Kim, I. Vilt: Vision-and- language transformer without convolution or region su- pervision. In International Conference on Machine Learning (ICML), 2021.\n\nAuto-Encoding Variational Bayes. D P Kingma, M Welling, International Conference on Learning Representation (ICLR). Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In International Conference on Learning Repre- sentation (ICLR), 2014.\n\nSkip-thought vectors. J Kiros, Y Zhu, R Salakhutdinov, R S Zemel, A Torralba, R Urtasun, S Fidler, Advances in Neural Information Processing Systems (NeuriPS). Kiros, J., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Tor- ralba, A., Urtasun, R., and Fidler, S. Skip-thought vec- tors. In Advances in Neural Information Processing Sys- tems (NeuriPS), 2015.\n\nContextual augmentation: Data augmentation by words with paradigmatic relations. S Kobayashi, arXiv:1805.06201arXiv preprintKobayashi, S. Contextual augmentation: Data augmen- tation by words with paradigmatic relations. arXiv preprint arXiv:1805.06201, 2018.\n\nT Kwiatkowski, J Palomaki, O Redfield, M Collins, A Parikh, C Alberti, D Epstein, I Polosukhin, M Kelcey, J Devlin, K Lee, K N Toutanova, L Jones, M.-W Chang, A Dai, J Uszkoreit, Q Le, S Petrov, Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel- cey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.\n\nLatent retrieval for weakly supervised open domain question answering. K Lee, M Chang, K Toutanova, Conference of the Association for Computational Linguistics (ACL). Korhonen, A., Traum, D. R., and M\u00e0rquez, L.ACLLee, K., Chang, M., and Toutanova, K. Latent retrieval for weakly supervised open domain question answering. In Korhonen, A., Traum, D. R., and M\u00e0rquez, L. (eds.), Conference of the Association for Computational Lin- guistics (ACL), pp. 6086-6096. ACL.\n\nRetrievalaugmented generation for knowledge-intensive NLP tasks. P S H Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H K\u00fcttler, M Lewis, W Yih, T Rockt\u00e4schel, S Riedel, D Kiela, Advances in Neural Information Processing Systems (NeuriPS). 2020Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00fcttler, H., Lewis, M., Yih, W., Rockt\u00e4schel, T., Riedel, S., and Kiela, D. Retrieval- augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems (NeuriPS), 2020.\n\nOn the sentence embeddings from pre-trained language models. B Li, H Zhou, J He, M Wang, Y Yang, Li , L , Conference on Empirical Methods in Natural Language Processing. 2020Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L. On the sentence embeddings from pre-trained language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n\nTowards debiasing sentence representations. P P Liang, I M Li, E Zheng, Y C Lim, R Salakhutdinov, L Morency, Conference of the Association for Computational Linguistics (ACL). 2020Liang, P. P., Li, I. M., Zheng, E., Lim, Y. C., Salakhutdinov, R., and Morency, L. Towards debiasing sentence repre- sentations. In Conference of the Association for Compu- tational Linguistics (ACL), 2020.\n\nPretrained transformers for text ranking: BERT and beyond. J Lin, R Nogueira, A Yates, Synthesis Lectures on Human Language Technologies. 144Lin, J., Nogueira, R., and Yates, A. Pretrained transformers for text ranking: BERT and beyond. Synthesis Lectures on Human Language Technologies, 14(4):1-325, 2021.\n\nAn efficient framework for learning sentence representations. L Logeswaran, H Lee, International Conference on Learning Representation (ICLR). Logeswaran, L. and Lee, H. An efficient framework for learning sentence representations. In International Con- ference on Learning Representation (ICLR), 2018.\n\nJ Lu, D Batra, D Parikh, S Lee, Vilbert, arXiv:1908.02265Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprintLu, J., Batra, D., Parikh, D., and Lee, S. Vil- bert: Pretraining task-agnostic visiolinguistic represen- tations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.\n\nOn measuring social biases in sentence encoders. C May, A Wang, S Bordia, S R Bowman, R Rudinger, Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). May, C., Wang, A., Bordia, S., Bowman, S. R., and Rudinger, R. On measuring social biases in sentence en- coders. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G S Corrado, J Dean, arXiv:1301.3781arXiv preprintMikolov, T., Chen, K., Corrado, G. S., and Dean, J. Effi- cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nT Nguyen, M Rosenberg, X Song, J Gao, S Tiwary, R Majumder, L Deng, Marco, arXiv:1611.09268A human generated machine reading comprehension dataset. arXiv preprintNguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. MS MARCO: A hu- man generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016.\n\nFrom doc2query to doctttttquery. R Nogueira, J Lin, A Epistemic, Online preprintNogueira, R., Lin, J., and Epistemic, A. From doc2query to doctttttquery. Online preprint, 2019a.\n\nR Nogueira, W Yang, K Cho, Lin , J , arXiv:1910.14424Multistage document ranking with BERT. arXiv preprintNogueira, R., Yang, W., Cho, K., and Lin, J. Multi- stage document ranking with BERT. arXiv preprint arXiv:1910.14424, 2019b.\n\nReducing gender bias in abusive language detection. J H Park, J Shin, P Fung, Conference on Empirical Methods in Natural Language Processing. Park, J. H., Shin, J., and Fung, P. Reducing gender bias in abusive language detection. In Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2018.\n\nGloVe: Global vectors for word representation. J Pennington, R Socher, C Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). Pennington, J., Socher, R., and Manning, C. GloVe: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.\n\nDeep contextualized word representations. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Proceedings of NCAAL/IJCNLP. NCAAL/IJCNLPPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep con- textualized word representations. In Proceedings of NCAAL/IJCNLP, 2018.\n\nRocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. Y Qu, Y Ding, J Liu, K Liu, R Ren, X Zhao, D Dong, H Wu, Wang , H , Conference of the Association for Computational Linguistics (ACL). 2021Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, X., Dong, D., Wu, H., and Wang, H. Rocketqa: An optimized train- ing approach to dense passage retrieval for open-domain question answering. In Conference of the Association for Computational Linguistics (ACL), 2021.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, arXiv:2103.00020arXiv preprintRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, arXiv:1910.10683arXiv preprintRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nZero-shot textto-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, arXiv:2102.12092arXiv preprintRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad- ford, A., Chen, M., and Sutskever, I. Zero-shot text- to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n\nSentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Conference on Empirical Methods in Natural Language Processing (EMNLP). Reimers, N. and Gurevych, I. Sentence-bert: Sentence em- beddings using siamese bert-networks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\n\nThe Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends\u00ae in Information Retrieval. S Robertson, Robertson, S. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends\u00ae in Infor- mation Retrieval, 2009.\n\nR Rudinger, J Naradowsky, B Leonard, B V Durme, arXiv:1804.09301Gender bias in coreference resolution. arXiv preprintRudinger, R., Naradowsky, J., Leonard, B., and Durme, B. V. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018.\n\nEnd-to-end training of neural retrievers for open-domain question answering. D S Sachan, M Patwary, M Shoeybi, N Kant, W Ping, W L Hamilton, B Catanzaro, Proceedings of ACL/IJCNLP. Zong, C., Xia, F., Li, W., and Navigli, R.ACL/IJCNLPACLSachan, D. S., Patwary, M., Shoeybi, M., Kant, N., Ping, W., Hamilton, W. L., and Catanzaro, B. End-to-end training of neural retrievers for open-domain question answering. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of ACL/IJCNLP, pp. 6648-6662. ACL, 2021.\n\nK Santhanam, O Khattab, J Saad-Falcon, C Potts, M Zaharia, Colbertv2, arXiv:2112.01488Effective and efficient retrieval via lightweight late interaction. arXiv preprintSanthanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M. Colbertv2: Effective and efficient re- trieval via lightweight late interaction. arXiv preprint arXiv:2112.01488, 2021.\n\nFacenet: A unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, Computer Vision and Pattern Recognition (CVPR). Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A unified embedding for face recognition and clustering. In Computer Vision and Pattern Recognition (CVPR), 2015.\n\nA simple but tough-to-beat data augmentation approach for natural language understanding and generation. D Shen, M Zheng, Y Shen, Y Qu, Chen , W , arXiv:2009.13818arXiv preprintShen, D., Zheng, M., Shen, Y., Qu, Y., and Chen, W. A simple but tough-to-beat data augmentation approach for natural language understanding and generation. arXiv preprint arXiv:2009.13818, 2020.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. R Socher, A Perelygin, J Wu, J Chuang, C D Manning, A Ng, C Potts, Conference on Empirical Methods in Natural Language Processing (EMNLP). Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013.\n\nImproved deep metric learning with multi-class n-pair loss objective. K Sohn, Advances in Neural Information Processing Systems (NeuriPS). Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in Neural Information Processing Systems (NeuriPS), 2016.\n\nProcess for adapting language models to society (PALMS) with values-targeted datasets. I Solaiman, C Dennison, arXiv:2106.10328arXiv preprintSolaiman, I. and Dennison, C. Process for adapting lan- guage models to society (PALMS) with values-targeted datasets. arXiv preprint arXiv:2106.10328, 2021.\n\nWhitening sentence representations for better semantics and faster retrieval. J Su, J Cao, W Liu, Y Ou, arXiv:2103.15316arXiv preprintSu, J., Cao, J., Liu, W., and Ou, Y. Whitening sentence representations for better semantics and faster retrieval. arXiv preprint arXiv:2103.15316, 2021.\n\nVideobert: A joint model for video and language representation learning. C Sun, A Myers, C Vondrick, K Murphy, C Schmid, International Conference on Computer Vision (ICCV). Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C. Videobert: A joint model for video and language representation learning. In International Conference on Computer Vision (ICCV), 2019.\n\nBEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. N Thakur, N Reimers, A R\u00fcckl\u00e9, A Srivastava, I Gurevych, Advances in Neural Information Processing Systems (NeuriPS). 2021Thakur, N., Reimers, N., R\u00fcckl\u00e9, A., Srivastava, A., and Gurevych, I. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. In Advances in Neural Information Processing Systems (NeuriPS), 2021.\n\nY Tian, D Krishnan, P Isola, A Van Den Oord, S Dieleman, H Zen, K Simonyan, O Vinyals, A Graves, N Kalchbrenner, A Senior, K Kavukcuoglu, A Wavenet ; Van Den Oord, Y Li, O Vinyals, arXiv:1609.03499arXiv:1807.03748Contrastive multiview coding. European Conference on Computer Vision (ECCV). arXiv preprintA generative model for raw audio. Representation learning with contrastive predictive codingTian, Y., Krishnan, D., and Isola, P. Contrastive multi- view coding. European Conference on Computer Vision (ECCV), 2019. van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. van den Oord, A., Li, Y., and Vinyals, O. Representa- tion learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, Advances in Neural Information Processing Systems (NeuriPS). Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten- tion is all you need. In Advances in Neural Information Processing Systems (NeuriPS), 2017.\n\nRelevance realization and the emerging framework in cognitive science. J Vervaeke, T P Lillicrap, B A Richards, Journal of logic and computation. 221Vervaeke, J., Lillicrap, T. P., and Richards, B. A. Relevance realization and the emerging framework in cognitive sci- ence. Journal of logic and computation, 22(1):79-99, 2012.\n\nDeep self-attention distillation for taskagnostic compression of pre-trained transformers. W Wang, F Wei, L Dong, H Bao, N Yang, M Zhou, Minilm, arXiv:2002.10957arXiv preprintWang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. Minilm: Deep self-attention distillation for task- agnostic compression of pre-trained transformers. arXiv preprint arXiv:2002.10957, 2020.\n\nJ W Wei, K Zou, Eda, arXiv:1901.11196easy data augmentation techniques for boosting performance on text classification tasks. arXiv preprintWei, J. W. and Zou, K. EDA: easy data augmentation tech- niques for boosting performance on text classification tasks. arXiv preprint arXiv:1901.11196, 2019.\n\nA broadcoverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). ACL. Williams, A., Nangia, N., and Bowman, S. A broad- coverage challenge corpus for sentence understanding through inference. In Conference of the North American Chapter of the Association for Computational Linguis- tics (NAACL). ACL, 2018.\n\nUnsupervised feature learning via non-parametric instance-level discrimination. Z Wu, Y Xiong, S X Yu, Lin , D , Computer Vision and Pattern Recognition (CVPR). Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance-level dis- crimination. In Computer Vision and Pattern Recogni- tion (CVPR), 2018.\n\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. L Xiong, C Xiong, Y Li, K Tang, J Liu, P N Bennett, J Ahmed, A Overwijk, arXiv:2007.00808arXiv preprintXiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P. N., Ahmed, J., and Overwijk, A. Approximate near- est neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.\n\nLearning discriminative projections for text similarity measures. W.-T Yih, K Toutanova, J C Platt, C Meek, Conference on Computational Natural Language Learning (CoNLL). ACL. Yih, W.-t., Toutanova, K., Platt, J. C., and Meek, C. Learn- ing discriminative projections for text similarity mea- sures. In Conference on Computational Natural Lan- guage Learning (CoNLL). ACL, 2011.\n\nBarlow twins: Self-supervised learning via redundancy reduction. J Zbontar, L Jing, I Misra, Y Lecun, Deny , S , International Conference on Machine Learning (ICML). 2021Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. Bar- low twins: Self-supervised learning via redundancy re- duction. In International Conference on Machine Learn- ing (ICML), 2021.\n\nAn unsupervised sentence embedding method by mutual information maximization. Y Zhang, R He, Z Liu, K H Lim, Bing , L , Conference on Empirical Methods in Natural Language Processing. 2020Zhang, Y., He, R., Liu, Z., Lim, K. H., and Bing, L. An unsupervised sentence embedding method by mutual information maximization. In Conference on Empiri- cal Methods in Natural Language Processing (EMNLP), 2020.\n\nJ Zhao, T Wang, M Yatskar, V Ordonez, Chang , K , arXiv:1804.06876Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprintZhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K. Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018.\n", "annotations": {"author": "[{\"end\":75,\"start\":56},{\"end\":83,\"start\":76},{\"end\":94,\"start\":84},{\"end\":108,\"start\":95},{\"end\":127,\"start\":109},{\"end\":141,\"start\":128},{\"end\":154,\"start\":142},{\"end\":169,\"start\":155},{\"end\":184,\"start\":170},{\"end\":199,\"start\":185},{\"end\":218,\"start\":200},{\"end\":232,\"start\":219},{\"end\":245,\"start\":233},{\"end\":267,\"start\":246},{\"end\":282,\"start\":268},{\"end\":300,\"start\":283},{\"end\":315,\"start\":301},{\"end\":337,\"start\":316},{\"end\":348,\"start\":338},{\"end\":368,\"start\":349},{\"end\":382,\"start\":369},{\"end\":398,\"start\":383},{\"end\":411,\"start\":399},{\"end\":427,\"start\":412},{\"end\":440,\"start\":428}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":63},{\"end\":82,\"start\":80},{\"end\":93,\"start\":89},{\"end\":107,\"start\":100},{\"end\":126,\"start\":123},{\"end\":140,\"start\":134},{\"end\":153,\"start\":149},{\"end\":168,\"start\":163},{\"end\":183,\"start\":180},{\"end\":198,\"start\":191},{\"end\":217,\"start\":209},{\"end\":231,\"start\":226},{\"end\":244,\"start\":239},{\"end\":266,\"start\":260},{\"end\":281,\"start\":275},{\"end\":299,\"start\":292},{\"end\":314,\"start\":307},{\"end\":336,\"start\":332},{\"end\":347,\"start\":344},{\"end\":367,\"start\":359},{\"end\":381,\"start\":377},{\"end\":397,\"start\":388},{\"end\":410,\"start\":406},{\"end\":426,\"start\":418},{\"end\":439,\"start\":435}]", "author_first_name": "[{\"end\":62,\"start\":56},{\"end\":79,\"start\":76},{\"end\":88,\"start\":84},{\"end\":99,\"start\":95},{\"end\":114,\"start\":109},{\"end\":122,\"start\":115},{\"end\":133,\"start\":128},{\"end\":148,\"start\":142},{\"end\":162,\"start\":155},{\"end\":174,\"start\":170},{\"end\":179,\"start\":175},{\"end\":190,\"start\":185},{\"end\":208,\"start\":200},{\"end\":225,\"start\":219},{\"end\":238,\"start\":233},{\"end\":250,\"start\":246},{\"end\":259,\"start\":251},{\"end\":274,\"start\":268},{\"end\":291,\"start\":283},{\"end\":306,\"start\":301},{\"end\":322,\"start\":316},{\"end\":331,\"start\":323},{\"end\":343,\"start\":338},{\"end\":358,\"start\":349},{\"end\":376,\"start\":369},{\"end\":387,\"start\":383},{\"end\":405,\"start\":399},{\"end\":417,\"start\":412},{\"end\":434,\"start\":428}]", "author_affiliation": null, "title": "[{\"end\":53,\"start\":1},{\"end\":493,\"start\":441}]", "venue": null, "abstract": "[{\"end\":1744,\"start\":495}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1912,\"start\":1891},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1932,\"start\":1912},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":1958,\"start\":1932},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":1978,\"start\":1958},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1997,\"start\":1978},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2015,\"start\":1997},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":2142,\"start\":2130},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":2168,\"start\":2142},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2189,\"start\":2168},{\"end\":2206,\"start\":2189},{\"end\":2223,\"start\":2206},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2244,\"start\":2223},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2574,\"start\":2550},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2593,\"start\":2574},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":2696,\"start\":2674},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2732,\"start\":2711},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2751,\"start\":2732},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2769,\"start\":2751},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2789,\"start\":2769},{\"end\":3361,\"start\":3344},{\"end\":3381,\"start\":3361},{\"end\":3398,\"start\":3381},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3419,\"start\":3398},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":3537,\"start\":3525},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":3554,\"start\":3537},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":3644,\"start\":3622},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4205,\"start\":4185},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4223,\"start\":4205},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4876,\"start\":4853},{\"end\":5046,\"start\":5025},{\"end\":5080,\"start\":5062},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":5304,\"start\":5278},{\"end\":5321,\"start\":5304},{\"end\":5341,\"start\":5321},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5392,\"start\":5375},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5416,\"start\":5392},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":5436,\"start\":5416},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5457,\"start\":5436},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5865,\"start\":5844},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5997,\"start\":5980},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6174,\"start\":6152},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6223,\"start\":6197},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6257,\"start\":6237},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6561,\"start\":6540},{\"end\":6821,\"start\":6803},{\"end\":7288,\"start\":7270},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":7353,\"start\":7332},{\"end\":7820,\"start\":7799},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7938,\"start\":7916},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7999,\"start\":7975},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8019,\"start\":7999},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":8036,\"start\":8019},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":8551,\"start\":8529},{\"end\":8735,\"start\":8730},{\"end\":8745,\"start\":8740},{\"end\":9806,\"start\":9801},{\"end\":9825,\"start\":9820},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":10019,\"start\":10001},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10029,\"start\":10019},{\"end\":10107,\"start\":10090},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10128,\"start\":10107},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10780,\"start\":10760},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11671,\"start\":11645},{\"end\":11688,\"start\":11671},{\"end\":11708,\"start\":11688},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11819,\"start\":11802},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11843,\"start\":11819},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11863,\"start\":11843},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11884,\"start\":11863},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12050,\"start\":12027},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12693,\"start\":12672},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":12725,\"start\":12703},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":13138,\"start\":13117},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":13851,\"start\":13830},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14503,\"start\":14489},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":14590,\"start\":14567},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14976,\"start\":14951},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":14996,\"start\":14976},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15017,\"start\":14996},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15247,\"start\":15222},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15267,\"start\":15247},{\"end\":15283,\"start\":15267},{\"end\":15453,\"start\":15436},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":15471,\"start\":15453},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15700,\"start\":15679},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15803,\"start\":15777},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15837,\"start\":15817},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":16270,\"start\":16249},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":16435,\"start\":16415},{\"end\":16560,\"start\":16539},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":16731,\"start\":16711},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16755,\"start\":16731},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":16792,\"start\":16776},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17043,\"start\":17021},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17099,\"start\":17076},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":17159,\"start\":17138},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17800,\"start\":17777},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17998,\"start\":17977},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18401,\"start\":18380},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20270,\"start\":20245},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":20290,\"start\":20270},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20311,\"start\":20290},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":20410,\"start\":20384},{\"end\":20430,\"start\":20410},{\"end\":20447,\"start\":20430},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20768,\"start\":20747},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20909,\"start\":20887},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21062,\"start\":21041},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21088,\"start\":21062},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":21109,\"start\":21088},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":21120,\"start\":21109},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":21146,\"start\":21120},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":21245,\"start\":21228},{\"end\":21261,\"start\":21245},{\"end\":21282,\"start\":21261},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21323,\"start\":21306},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":21340,\"start\":21323},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21357,\"start\":21340},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21378,\"start\":21357},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21398,\"start\":21378},{\"end\":21738,\"start\":21717},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":21906,\"start\":21886},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21930,\"start\":21906},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21951,\"start\":21930},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22024,\"start\":22008},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":22201,\"start\":22178},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22221,\"start\":22201},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22356,\"start\":22335},{\"end\":22390,\"start\":22371},{\"end\":22427,\"start\":22409},{\"end\":22673,\"start\":22652},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22694,\"start\":22673},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":22780,\"start\":22761},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22860,\"start\":22840},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22886,\"start\":22860},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22907,\"start\":22886},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22931,\"start\":22907},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23094,\"start\":23069},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":23111,\"start\":23094},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23325,\"start\":23300},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":23479,\"start\":23463},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":23669,\"start\":23652},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23902,\"start\":23878},{\"end\":23921,\"start\":23902},{\"end\":23938,\"start\":23921},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24053,\"start\":24033},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24076,\"start\":24053},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":24253,\"start\":24236},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24292,\"start\":24275},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24329,\"start\":24311},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":24358,\"start\":24339},{\"end\":24389,\"start\":24371},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24803,\"start\":24781},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24945,\"start\":24928},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24969,\"start\":24945},{\"end\":24988,\"start\":24969},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":25153,\"start\":25137},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25242,\"start\":25218},{\"end\":25258,\"start\":25242},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":25279,\"start\":25260},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":25568,\"start\":25547},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25795,\"start\":25773},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26065,\"start\":26044},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26313,\"start\":26295},{\"end\":26522,\"start\":26503},{\"end\":26631,\"start\":26613},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26990,\"start\":26966},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27012,\"start\":26990},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27029,\"start\":27012},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":27047,\"start\":27029},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27069,\"start\":27047},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27607,\"start\":27584},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27624,\"start\":27607},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27647,\"start\":27624},{\"end\":27666,\"start\":27647},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27684,\"start\":27666},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":27710,\"start\":27684},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32352,\"start\":32336}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29263,\"start\":29077},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29352,\"start\":29264},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29670,\"start\":29353},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29938,\"start\":29671},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30199,\"start\":29939},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30448,\"start\":30200},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32094,\"start\":30449}]", "paragraph": "[{\"end\":3420,\"start\":1760},{\"end\":4544,\"start\":3422},{\"end\":5117,\"start\":4546},{\"end\":6354,\"start\":5119},{\"end\":6977,\"start\":6356},{\"end\":8094,\"start\":6979},{\"end\":8314,\"start\":8107},{\"end\":8475,\"start\":8338},{\"end\":9121,\"start\":8485},{\"end\":9323,\"start\":9123},{\"end\":9550,\"start\":9335},{\"end\":9897,\"start\":9647},{\"end\":10130,\"start\":9920},{\"end\":10282,\"start\":10237},{\"end\":10648,\"start\":10284},{\"end\":11004,\"start\":10650},{\"end\":11482,\"start\":11016},{\"end\":11985,\"start\":11484},{\"end\":12268,\"start\":12004},{\"end\":12924,\"start\":12300},{\"end\":14177,\"start\":12962},{\"end\":15084,\"start\":14201},{\"end\":15571,\"start\":15100},{\"end\":16436,\"start\":15594},{\"end\":16907,\"start\":16452},{\"end\":18268,\"start\":16909},{\"end\":18740,\"start\":18284},{\"end\":19193,\"start\":18742},{\"end\":19603,\"start\":19229},{\"end\":20694,\"start\":19625},{\"end\":22222,\"start\":20711},{\"end\":22617,\"start\":22224},{\"end\":22781,\"start\":22619},{\"end\":23778,\"start\":22783},{\"end\":24501,\"start\":23780},{\"end\":25913,\"start\":24503},{\"end\":26743,\"start\":25915},{\"end\":27270,\"start\":26763},{\"end\":28024,\"start\":27272},{\"end\":28584,\"start\":28026},{\"end\":29076,\"start\":28599}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8337,\"start\":8315},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9646,\"start\":9551},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10236,\"start\":10131}]", "table_ref": "[{\"end\":11415,\"start\":11408},{\"end\":14047,\"start\":14040},{\"end\":14317,\"start\":14310},{\"end\":16114,\"start\":16107},{\"end\":18623,\"start\":18614},{\"end\":18935,\"start\":18926},{\"end\":19329,\"start\":19322},{\"end\":21614,\"start\":21607},{\"end\":22264,\"start\":22257}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1758,\"start\":1746},{\"attributes\":{\"n\":\"2.\"},\"end\":8105,\"start\":8097},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8483,\"start\":8478},{\"end\":9333,\"start\":9326},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9918,\"start\":9900},{\"attributes\":{\"n\":\"3.\"},\"end\":11014,\"start\":11007},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12002,\"start\":11988},{\"attributes\":{\"n\":\"3.1.1.\"},\"end\":12298,\"start\":12271},{\"attributes\":{\"n\":\"3.1.2.\"},\"end\":12960,\"start\":12927},{\"attributes\":{\"n\":\"3.1.3.\"},\"end\":14199,\"start\":14180},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15098,\"start\":15087},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":15592,\"start\":15574},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":16450,\"start\":16439},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18282,\"start\":18271},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19204,\"start\":19196},{\"attributes\":{\"n\":\"3.4.1.\"},\"end\":19227,\"start\":19207},{\"attributes\":{\"n\":\"3.4.2.\"},\"end\":19623,\"start\":19606},{\"attributes\":{\"n\":\"4.\"},\"end\":20709,\"start\":20697},{\"attributes\":{\"n\":\"5.\"},\"end\":26761,\"start\":26746},{\"attributes\":{\"n\":\"6.\"},\"end\":28597,\"start\":28587},{\"end\":29088,\"start\":29078},{\"end\":29275,\"start\":29265},{\"end\":29682,\"start\":29672},{\"end\":30208,\"start\":30201}]", "table": "[{\"end\":30199,\"start\":30113},{\"end\":30448,\"start\":30210},{\"end\":32094,\"start\":30453}]", "figure_caption": "[{\"end\":29263,\"start\":29090},{\"end\":29352,\"start\":29277},{\"end\":29670,\"start\":29355},{\"end\":29938,\"start\":29684},{\"end\":30113,\"start\":29941},{\"end\":30453,\"start\":30451}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4759,\"start\":4750},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8687,\"start\":8679},{\"end\":9137,\"start\":9129},{\"end\":9549,\"start\":9539},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19809,\"start\":19800}]", "bib_author_first_name": "[{\"end\":32471,\"start\":32470},{\"end\":32481,\"start\":32480},{\"end\":32483,\"start\":32482},{\"end\":32496,\"start\":32495},{\"end\":32772,\"start\":32771},{\"end\":32785,\"start\":32784},{\"end\":32794,\"start\":32793},{\"end\":32796,\"start\":32795},{\"end\":32803,\"start\":32802},{\"end\":32816,\"start\":32815},{\"end\":33054,\"start\":33053},{\"end\":33056,\"start\":33055},{\"end\":33066,\"start\":33065},{\"end\":33076,\"start\":33075},{\"end\":33085,\"start\":33084},{\"end\":33087,\"start\":33086},{\"end\":33434,\"start\":33433},{\"end\":33436,\"start\":33435},{\"end\":33445,\"start\":33444},{\"end\":33447,\"start\":33446},{\"end\":33463,\"start\":33462},{\"end\":33465,\"start\":33464},{\"end\":33476,\"start\":33475},{\"end\":33478,\"start\":33477},{\"end\":33485,\"start\":33484},{\"end\":33487,\"start\":33486},{\"end\":33746,\"start\":33745},{\"end\":33755,\"start\":33754},{\"end\":33763,\"start\":33762},{\"end\":33772,\"start\":33771},{\"end\":33783,\"start\":33782},{\"end\":33785,\"start\":33784},{\"end\":33795,\"start\":33794},{\"end\":33807,\"start\":33806},{\"end\":33822,\"start\":33821},{\"end\":33831,\"start\":33830},{\"end\":33841,\"start\":33840},{\"end\":33851,\"start\":33850},{\"end\":33862,\"start\":33861},{\"end\":33878,\"start\":33877},{\"end\":33889,\"start\":33888},{\"end\":33901,\"start\":33900},{\"end\":33910,\"start\":33909},{\"end\":33920,\"start\":33919},{\"end\":33931,\"start\":33930},{\"end\":33937,\"start\":33936},{\"end\":33947,\"start\":33946},{\"end\":33956,\"start\":33955},{\"end\":33964,\"start\":33963},{\"end\":33974,\"start\":33973},{\"end\":33984,\"start\":33983},{\"end\":33992,\"start\":33991},{\"end\":34001,\"start\":34000},{\"end\":34010,\"start\":34009},{\"end\":34020,\"start\":34019},{\"end\":34034,\"start\":34033},{\"end\":34045,\"start\":34044},{\"end\":34063,\"start\":34057},{\"end\":34067,\"start\":34066},{\"end\":34691,\"start\":34690},{\"end\":34703,\"start\":34702},{\"end\":34705,\"start\":34704},{\"end\":34715,\"start\":34714},{\"end\":34911,\"start\":34910},{\"end\":34919,\"start\":34918},{\"end\":34929,\"start\":34928},{\"end\":34936,\"start\":34935},{\"end\":34944,\"start\":34943},{\"end\":34946,\"start\":34945},{\"end\":34967,\"start\":34966},{\"end\":34977,\"start\":34976},{\"end\":34988,\"start\":34987},{\"end\":34997,\"start\":34996},{\"end\":35007,\"start\":35006},{\"end\":35019,\"start\":35018},{\"end\":35026,\"start\":35025},{\"end\":35034,\"start\":35033},{\"end\":35045,\"start\":35044},{\"end\":35055,\"start\":35054},{\"end\":35065,\"start\":35064},{\"end\":35075,\"start\":35074},{\"end\":35086,\"start\":35085},{\"end\":35094,\"start\":35093},{\"end\":35102,\"start\":35101},{\"end\":35111,\"start\":35110},{\"end\":35121,\"start\":35120},{\"end\":35130,\"start\":35129},{\"end\":35140,\"start\":35139},{\"end\":35152,\"start\":35151},{\"end\":35162,\"start\":35161},{\"end\":35172,\"start\":35171},{\"end\":35174,\"start\":35173},{\"end\":35182,\"start\":35181},{\"end\":35194,\"start\":35193},{\"end\":35206,\"start\":35205},{\"end\":35218,\"start\":35217},{\"end\":35228,\"start\":35227},{\"end\":35244,\"start\":35243},{\"end\":35246,\"start\":35245},{\"end\":35254,\"start\":35253},{\"end\":35264,\"start\":35263},{\"end\":35273,\"start\":35272},{\"end\":35282,\"start\":35281},{\"end\":35290,\"start\":35289},{\"end\":35304,\"start\":35303},{\"end\":35314,\"start\":35313},{\"end\":35322,\"start\":35321},{\"end\":35334,\"start\":35333},{\"end\":35343,\"start\":35342},{\"end\":35345,\"start\":35344},{\"end\":35353,\"start\":35352},{\"end\":35362,\"start\":35361},{\"end\":35372,\"start\":35371},{\"end\":35381,\"start\":35380},{\"end\":35393,\"start\":35392},{\"end\":35404,\"start\":35403},{\"end\":35414,\"start\":35413},{\"end\":35426,\"start\":35425},{\"end\":35436,\"start\":35435},{\"end\":35445,\"start\":35444},{\"end\":35457,\"start\":35456},{\"end\":35467,\"start\":35466},{\"end\":35477,\"start\":35476},{\"end\":35491,\"start\":35490},{\"end\":35504,\"start\":35503},{\"end\":36483,\"start\":36482},{\"end\":36491,\"start\":36490},{\"end\":36504,\"start\":36503},{\"end\":36515,\"start\":36514},{\"end\":36517,\"start\":36516},{\"end\":36860,\"start\":36859},{\"end\":36870,\"start\":36869},{\"end\":36881,\"start\":36880},{\"end\":37207,\"start\":37206},{\"end\":37218,\"start\":37217},{\"end\":37438,\"start\":37437},{\"end\":37551,\"start\":37550},{\"end\":37565,\"start\":37564},{\"end\":37575,\"start\":37574},{\"end\":37585,\"start\":37584},{\"end\":37597,\"start\":37596},{\"end\":37933,\"start\":37932},{\"end\":37946,\"start\":37942},{\"end\":37955,\"start\":37954},{\"end\":37962,\"start\":37961},{\"end\":38405,\"start\":38404},{\"end\":38413,\"start\":38412},{\"end\":38651,\"start\":38650},{\"end\":38659,\"start\":38658},{\"end\":38666,\"start\":38665},{\"end\":38674,\"start\":38673},{\"end\":38682,\"start\":38681},{\"end\":38690,\"start\":38689},{\"end\":38698,\"start\":38697},{\"end\":38706,\"start\":38705},{\"end\":38713,\"start\":38712},{\"end\":38720,\"start\":38719},{\"end\":38729,\"start\":38728},{\"end\":39071,\"start\":39070},{\"end\":39081,\"start\":39080},{\"end\":39093,\"start\":39092},{\"end\":39107,\"start\":39106},{\"end\":39437,\"start\":39436},{\"end\":39444,\"start\":39443},{\"end\":39454,\"start\":39450},{\"end\":39458,\"start\":39457},{\"end\":39788,\"start\":39787},{\"end\":39790,\"start\":39789},{\"end\":39800,\"start\":39799},{\"end\":39810,\"start\":39809},{\"end\":39812,\"start\":39811},{\"end\":39824,\"start\":39820},{\"end\":39828,\"start\":39827},{\"end\":40075,\"start\":40074},{\"end\":40167,\"start\":40166},{\"end\":40174,\"start\":40173},{\"end\":40181,\"start\":40180},{\"end\":40187,\"start\":40186},{\"end\":40195,\"start\":40194},{\"end\":40203,\"start\":40202},{\"end\":40210,\"start\":40209},{\"end\":40218,\"start\":40217},{\"end\":40226,\"start\":40225},{\"end\":40242,\"start\":40241},{\"end\":40248,\"start\":40247},{\"end\":40258,\"start\":40257},{\"end\":40260,\"start\":40259},{\"end\":40268,\"start\":40267},{\"end\":40270,\"start\":40269},{\"end\":40281,\"start\":40280},{\"end\":40290,\"start\":40289},{\"end\":40304,\"start\":40303},{\"end\":40311,\"start\":40310},{\"end\":40320,\"start\":40319},{\"end\":40887,\"start\":40886},{\"end\":40898,\"start\":40897},{\"end\":41224,\"start\":41223},{\"end\":41231,\"start\":41230},{\"end\":41238,\"start\":41237},{\"end\":41246,\"start\":41245},{\"end\":41261,\"start\":41256},{\"end\":41265,\"start\":41264},{\"end\":41510,\"start\":41509},{\"end\":41521,\"start\":41520},{\"end\":41531,\"start\":41530},{\"end\":41847,\"start\":41846},{\"end\":41853,\"start\":41852},{\"end\":41860,\"start\":41859},{\"end\":41866,\"start\":41865},{\"end\":41873,\"start\":41872},{\"end\":41875,\"start\":41874},{\"end\":42206,\"start\":42205},{\"end\":42220,\"start\":42219},{\"end\":42227,\"start\":42226},{\"end\":42235,\"start\":42234},{\"end\":42242,\"start\":42241},{\"end\":42540,\"start\":42539},{\"end\":42553,\"start\":42549},{\"end\":42559,\"start\":42558},{\"end\":42568,\"start\":42567},{\"end\":42581,\"start\":42580},{\"end\":42886,\"start\":42885},{\"end\":42897,\"start\":42896},{\"end\":42906,\"start\":42905},{\"end\":42918,\"start\":42917},{\"end\":42928,\"start\":42927},{\"end\":42942,\"start\":42941},{\"end\":42952,\"start\":42951},{\"end\":43247,\"start\":43246},{\"end\":43255,\"start\":43254},{\"end\":43263,\"start\":43262},{\"end\":43272,\"start\":43271},{\"end\":43282,\"start\":43281},{\"end\":43284,\"start\":43283},{\"end\":43296,\"start\":43295},{\"end\":43664,\"start\":43663},{\"end\":43671,\"start\":43670},{\"end\":43679,\"start\":43678},{\"end\":43689,\"start\":43685},{\"end\":43697,\"start\":43696},{\"end\":43707,\"start\":43706},{\"end\":43715,\"start\":43714},{\"end\":43717,\"start\":43716},{\"end\":43723,\"start\":43722},{\"end\":43731,\"start\":43730},{\"end\":43737,\"start\":43736},{\"end\":44108,\"start\":44107},{\"end\":44119,\"start\":44118},{\"end\":44128,\"start\":44127},{\"end\":44379,\"start\":44378},{\"end\":44388,\"start\":44387},{\"end\":44396,\"start\":44395},{\"end\":44404,\"start\":44403},{\"end\":44774,\"start\":44773},{\"end\":44787,\"start\":44786},{\"end\":44795,\"start\":44794},{\"end\":44802,\"start\":44801},{\"end\":44811,\"start\":44810},{\"end\":44817,\"start\":44816},{\"end\":44827,\"start\":44826},{\"end\":44835,\"start\":44834},{\"end\":45210,\"start\":45209},{\"end\":45223,\"start\":45222},{\"end\":45231,\"start\":45230},{\"end\":45238,\"start\":45237},{\"end\":45244,\"start\":45243},{\"end\":45254,\"start\":45253},{\"end\":45264,\"start\":45261},{\"end\":45268,\"start\":45267},{\"end\":45568,\"start\":45567},{\"end\":45578,\"start\":45577},{\"end\":45590,\"start\":45589},{\"end\":45598,\"start\":45597},{\"end\":45607,\"start\":45606},{\"end\":45615,\"start\":45614},{\"end\":45624,\"start\":45623},{\"end\":45637,\"start\":45636},{\"end\":45644,\"start\":45643},{\"end\":45979,\"start\":45978},{\"end\":45986,\"start\":45985},{\"end\":45995,\"start\":45992},{\"end\":45999,\"start\":45998},{\"end\":46270,\"start\":46269},{\"end\":46272,\"start\":46271},{\"end\":46282,\"start\":46281},{\"end\":46510,\"start\":46509},{\"end\":46519,\"start\":46518},{\"end\":46526,\"start\":46525},{\"end\":46543,\"start\":46542},{\"end\":46545,\"start\":46544},{\"end\":46554,\"start\":46553},{\"end\":46566,\"start\":46565},{\"end\":46577,\"start\":46576},{\"end\":46923,\"start\":46922},{\"end\":47103,\"start\":47102},{\"end\":47118,\"start\":47117},{\"end\":47130,\"start\":47129},{\"end\":47142,\"start\":47141},{\"end\":47153,\"start\":47152},{\"end\":47163,\"start\":47162},{\"end\":47174,\"start\":47173},{\"end\":47185,\"start\":47184},{\"end\":47199,\"start\":47198},{\"end\":47209,\"start\":47208},{\"end\":47219,\"start\":47218},{\"end\":47226,\"start\":47225},{\"end\":47228,\"start\":47227},{\"end\":47241,\"start\":47240},{\"end\":47253,\"start\":47249},{\"end\":47262,\"start\":47261},{\"end\":47269,\"start\":47268},{\"end\":47282,\"start\":47281},{\"end\":47288,\"start\":47287},{\"end\":47865,\"start\":47864},{\"end\":47872,\"start\":47871},{\"end\":47881,\"start\":47880},{\"end\":48326,\"start\":48325},{\"end\":48330,\"start\":48327},{\"end\":48339,\"start\":48338},{\"end\":48348,\"start\":48347},{\"end\":48358,\"start\":48357},{\"end\":48369,\"start\":48368},{\"end\":48382,\"start\":48381},{\"end\":48391,\"start\":48390},{\"end\":48402,\"start\":48401},{\"end\":48411,\"start\":48410},{\"end\":48418,\"start\":48417},{\"end\":48433,\"start\":48432},{\"end\":48443,\"start\":48442},{\"end\":48871,\"start\":48870},{\"end\":48877,\"start\":48876},{\"end\":48885,\"start\":48884},{\"end\":48891,\"start\":48890},{\"end\":48899,\"start\":48898},{\"end\":48908,\"start\":48906},{\"end\":48912,\"start\":48911},{\"end\":49228,\"start\":49227},{\"end\":49230,\"start\":49229},{\"end\":49239,\"start\":49238},{\"end\":49241,\"start\":49240},{\"end\":49247,\"start\":49246},{\"end\":49256,\"start\":49255},{\"end\":49258,\"start\":49257},{\"end\":49265,\"start\":49264},{\"end\":49282,\"start\":49281},{\"end\":49631,\"start\":49630},{\"end\":49638,\"start\":49637},{\"end\":49650,\"start\":49649},{\"end\":49942,\"start\":49941},{\"end\":49956,\"start\":49955},{\"end\":50184,\"start\":50183},{\"end\":50190,\"start\":50189},{\"end\":50199,\"start\":50198},{\"end\":50209,\"start\":50208},{\"end\":50578,\"start\":50577},{\"end\":50585,\"start\":50584},{\"end\":50593,\"start\":50592},{\"end\":50603,\"start\":50602},{\"end\":50605,\"start\":50604},{\"end\":50615,\"start\":50614},{\"end\":51011,\"start\":51010},{\"end\":51022,\"start\":51021},{\"end\":51030,\"start\":51029},{\"end\":51032,\"start\":51031},{\"end\":51043,\"start\":51042},{\"end\":51235,\"start\":51234},{\"end\":51245,\"start\":51244},{\"end\":51258,\"start\":51257},{\"end\":51266,\"start\":51265},{\"end\":51273,\"start\":51272},{\"end\":51283,\"start\":51282},{\"end\":51295,\"start\":51294},{\"end\":51624,\"start\":51623},{\"end\":51636,\"start\":51635},{\"end\":51643,\"start\":51642},{\"end\":51770,\"start\":51769},{\"end\":51782,\"start\":51781},{\"end\":51790,\"start\":51789},{\"end\":51799,\"start\":51796},{\"end\":51803,\"start\":51802},{\"end\":52055,\"start\":52054},{\"end\":52057,\"start\":52056},{\"end\":52065,\"start\":52064},{\"end\":52073,\"start\":52072},{\"end\":52364,\"start\":52363},{\"end\":52378,\"start\":52377},{\"end\":52388,\"start\":52387},{\"end\":52686,\"start\":52685},{\"end\":52688,\"start\":52687},{\"end\":52698,\"start\":52697},{\"end\":52709,\"start\":52708},{\"end\":52718,\"start\":52717},{\"end\":52729,\"start\":52728},{\"end\":52738,\"start\":52737},{\"end\":52745,\"start\":52744},{\"end\":53080,\"start\":53079},{\"end\":53086,\"start\":53085},{\"end\":53094,\"start\":53093},{\"end\":53101,\"start\":53100},{\"end\":53108,\"start\":53107},{\"end\":53115,\"start\":53114},{\"end\":53123,\"start\":53122},{\"end\":53131,\"start\":53130},{\"end\":53140,\"start\":53136},{\"end\":53144,\"start\":53143},{\"end\":53559,\"start\":53558},{\"end\":53570,\"start\":53569},{\"end\":53572,\"start\":53571},{\"end\":53579,\"start\":53578},{\"end\":53590,\"start\":53589},{\"end\":53600,\"start\":53599},{\"end\":53607,\"start\":53606},{\"end\":53618,\"start\":53617},{\"end\":53628,\"start\":53627},{\"end\":53638,\"start\":53637},{\"end\":53649,\"start\":53648},{\"end\":53658,\"start\":53657},{\"end\":53669,\"start\":53668},{\"end\":54057,\"start\":54056},{\"end\":54067,\"start\":54066},{\"end\":54078,\"start\":54077},{\"end\":54089,\"start\":54088},{\"end\":54096,\"start\":54095},{\"end\":54106,\"start\":54105},{\"end\":54116,\"start\":54115},{\"end\":54124,\"start\":54123},{\"end\":54130,\"start\":54129},{\"end\":54132,\"start\":54131},{\"end\":54431,\"start\":54430},{\"end\":54441,\"start\":54440},{\"end\":54451,\"start\":54450},{\"end\":54458,\"start\":54457},{\"end\":54466,\"start\":54465},{\"end\":54474,\"start\":54473},{\"end\":54485,\"start\":54484},{\"end\":54493,\"start\":54492},{\"end\":54773,\"start\":54772},{\"end\":54784,\"start\":54783},{\"end\":55151,\"start\":55150},{\"end\":55293,\"start\":55292},{\"end\":55305,\"start\":55304},{\"end\":55319,\"start\":55318},{\"end\":55330,\"start\":55329},{\"end\":55332,\"start\":55331},{\"end\":55626,\"start\":55625},{\"end\":55628,\"start\":55627},{\"end\":55638,\"start\":55637},{\"end\":55649,\"start\":55648},{\"end\":55660,\"start\":55659},{\"end\":55668,\"start\":55667},{\"end\":55676,\"start\":55675},{\"end\":55678,\"start\":55677},{\"end\":55690,\"start\":55689},{\"end\":56066,\"start\":56065},{\"end\":56079,\"start\":56078},{\"end\":56090,\"start\":56089},{\"end\":56105,\"start\":56104},{\"end\":56114,\"start\":56113},{\"end\":56493,\"start\":56492},{\"end\":56504,\"start\":56503},{\"end\":56520,\"start\":56519},{\"end\":56855,\"start\":56854},{\"end\":56863,\"start\":56862},{\"end\":56872,\"start\":56871},{\"end\":56880,\"start\":56879},{\"end\":56889,\"start\":56885},{\"end\":56893,\"start\":56892},{\"end\":57203,\"start\":57202},{\"end\":57213,\"start\":57212},{\"end\":57226,\"start\":57225},{\"end\":57232,\"start\":57231},{\"end\":57242,\"start\":57241},{\"end\":57244,\"start\":57243},{\"end\":57255,\"start\":57254},{\"end\":57261,\"start\":57260},{\"end\":57658,\"start\":57657},{\"end\":57964,\"start\":57963},{\"end\":57976,\"start\":57975},{\"end\":58255,\"start\":58254},{\"end\":58261,\"start\":58260},{\"end\":58268,\"start\":58267},{\"end\":58275,\"start\":58274},{\"end\":58539,\"start\":58538},{\"end\":58546,\"start\":58545},{\"end\":58555,\"start\":58554},{\"end\":58567,\"start\":58566},{\"end\":58577,\"start\":58576},{\"end\":58924,\"start\":58923},{\"end\":58934,\"start\":58933},{\"end\":58945,\"start\":58944},{\"end\":58955,\"start\":58954},{\"end\":58969,\"start\":58968},{\"end\":59276,\"start\":59275},{\"end\":59284,\"start\":59283},{\"end\":59296,\"start\":59295},{\"end\":59305,\"start\":59304},{\"end\":59321,\"start\":59320},{\"end\":59333,\"start\":59332},{\"end\":59340,\"start\":59339},{\"end\":59352,\"start\":59351},{\"end\":59363,\"start\":59362},{\"end\":59373,\"start\":59372},{\"end\":59389,\"start\":59388},{\"end\":59399,\"start\":59398},{\"end\":59414,\"start\":59413},{\"end\":59440,\"start\":59439},{\"end\":59446,\"start\":59445},{\"end\":60178,\"start\":60177},{\"end\":60189,\"start\":60188},{\"end\":60200,\"start\":60199},{\"end\":60210,\"start\":60209},{\"end\":60223,\"start\":60222},{\"end\":60232,\"start\":60231},{\"end\":60234,\"start\":60233},{\"end\":60243,\"start\":60242},{\"end\":60245,\"start\":60244},{\"end\":60255,\"start\":60254},{\"end\":60613,\"start\":60612},{\"end\":60625,\"start\":60624},{\"end\":60627,\"start\":60626},{\"end\":60640,\"start\":60639},{\"end\":60642,\"start\":60641},{\"end\":60961,\"start\":60960},{\"end\":60969,\"start\":60968},{\"end\":60976,\"start\":60975},{\"end\":60984,\"start\":60983},{\"end\":60991,\"start\":60990},{\"end\":60999,\"start\":60998},{\"end\":61247,\"start\":61246},{\"end\":61249,\"start\":61248},{\"end\":61256,\"start\":61255},{\"end\":61625,\"start\":61624},{\"end\":61637,\"start\":61636},{\"end\":61647,\"start\":61646},{\"end\":62079,\"start\":62078},{\"end\":62085,\"start\":62084},{\"end\":62094,\"start\":62093},{\"end\":62096,\"start\":62095},{\"end\":62104,\"start\":62101},{\"end\":62108,\"start\":62107},{\"end\":62429,\"start\":62428},{\"end\":62438,\"start\":62437},{\"end\":62447,\"start\":62446},{\"end\":62453,\"start\":62452},{\"end\":62461,\"start\":62460},{\"end\":62468,\"start\":62467},{\"end\":62470,\"start\":62469},{\"end\":62481,\"start\":62480},{\"end\":62490,\"start\":62489},{\"end\":62821,\"start\":62817},{\"end\":62828,\"start\":62827},{\"end\":62841,\"start\":62840},{\"end\":62843,\"start\":62842},{\"end\":62852,\"start\":62851},{\"end\":63197,\"start\":63196},{\"end\":63208,\"start\":63207},{\"end\":63216,\"start\":63215},{\"end\":63225,\"start\":63224},{\"end\":63237,\"start\":63233},{\"end\":63241,\"start\":63240},{\"end\":63572,\"start\":63571},{\"end\":63581,\"start\":63580},{\"end\":63587,\"start\":63586},{\"end\":63594,\"start\":63593},{\"end\":63596,\"start\":63595},{\"end\":63606,\"start\":63602},{\"end\":63610,\"start\":63609},{\"end\":63897,\"start\":63896},{\"end\":63905,\"start\":63904},{\"end\":63913,\"start\":63912},{\"end\":63924,\"start\":63923},{\"end\":63939,\"start\":63934},{\"end\":63943,\"start\":63942}]", "bib_author_last_name": "[{\"end\":32478,\"start\":32472},{\"end\":32493,\"start\":32484},{\"end\":32504,\"start\":32497},{\"end\":32782,\"start\":32773},{\"end\":32791,\"start\":32786},{\"end\":32800,\"start\":32797},{\"end\":32813,\"start\":32804},{\"end\":32822,\"start\":32817},{\"end\":33063,\"start\":33057},{\"end\":33073,\"start\":33067},{\"end\":33082,\"start\":33077},{\"end\":33095,\"start\":33088},{\"end\":33442,\"start\":33437},{\"end\":33460,\"start\":33448},{\"end\":33473,\"start\":33466},{\"end\":33482,\"start\":33479},{\"end\":33494,\"start\":33488},{\"end\":33752,\"start\":33747},{\"end\":33760,\"start\":33756},{\"end\":33769,\"start\":33764},{\"end\":33780,\"start\":33773},{\"end\":33792,\"start\":33786},{\"end\":33804,\"start\":33796},{\"end\":33819,\"start\":33808},{\"end\":33828,\"start\":33823},{\"end\":33838,\"start\":33832},{\"end\":33848,\"start\":33842},{\"end\":33859,\"start\":33852},{\"end\":33875,\"start\":33863},{\"end\":33886,\"start\":33879},{\"end\":33898,\"start\":33890},{\"end\":33907,\"start\":33902},{\"end\":33917,\"start\":33911},{\"end\":33928,\"start\":33921},{\"end\":33934,\"start\":33932},{\"end\":33944,\"start\":33938},{\"end\":33953,\"start\":33948},{\"end\":33961,\"start\":33957},{\"end\":33971,\"start\":33965},{\"end\":33981,\"start\":33975},{\"end\":33989,\"start\":33985},{\"end\":33998,\"start\":33993},{\"end\":34007,\"start\":34002},{\"end\":34017,\"start\":34011},{\"end\":34031,\"start\":34021},{\"end\":34042,\"start\":34035},{\"end\":34055,\"start\":34046},{\"end\":34700,\"start\":34692},{\"end\":34712,\"start\":34706},{\"end\":34725,\"start\":34716},{\"end\":34916,\"start\":34912},{\"end\":34926,\"start\":34920},{\"end\":34933,\"start\":34930},{\"end\":34941,\"start\":34937},{\"end\":34964,\"start\":34947},{\"end\":34974,\"start\":34968},{\"end\":34985,\"start\":34978},{\"end\":34994,\"start\":34989},{\"end\":35004,\"start\":34998},{\"end\":35016,\"start\":35008},{\"end\":35023,\"start\":35020},{\"end\":35031,\"start\":35027},{\"end\":35042,\"start\":35035},{\"end\":35052,\"start\":35046},{\"end\":35062,\"start\":35056},{\"end\":35072,\"start\":35066},{\"end\":35083,\"start\":35076},{\"end\":35091,\"start\":35087},{\"end\":35099,\"start\":35095},{\"end\":35108,\"start\":35103},{\"end\":35118,\"start\":35112},{\"end\":35127,\"start\":35122},{\"end\":35137,\"start\":35131},{\"end\":35149,\"start\":35141},{\"end\":35159,\"start\":35153},{\"end\":35169,\"start\":35163},{\"end\":35179,\"start\":35175},{\"end\":35191,\"start\":35183},{\"end\":35203,\"start\":35195},{\"end\":35215,\"start\":35207},{\"end\":35225,\"start\":35219},{\"end\":35241,\"start\":35229},{\"end\":35251,\"start\":35247},{\"end\":35261,\"start\":35255},{\"end\":35270,\"start\":35265},{\"end\":35279,\"start\":35274},{\"end\":35287,\"start\":35283},{\"end\":35301,\"start\":35291},{\"end\":35311,\"start\":35305},{\"end\":35319,\"start\":35315},{\"end\":35331,\"start\":35323},{\"end\":35340,\"start\":35335},{\"end\":35350,\"start\":35346},{\"end\":35359,\"start\":35354},{\"end\":35369,\"start\":35363},{\"end\":35378,\"start\":35373},{\"end\":35390,\"start\":35382},{\"end\":35401,\"start\":35394},{\"end\":35411,\"start\":35405},{\"end\":35423,\"start\":35415},{\"end\":35433,\"start\":35427},{\"end\":35442,\"start\":35437},{\"end\":35454,\"start\":35446},{\"end\":35464,\"start\":35458},{\"end\":35474,\"start\":35468},{\"end\":35488,\"start\":35478},{\"end\":35501,\"start\":35492},{\"end\":35512,\"start\":35505},{\"end\":36488,\"start\":36484},{\"end\":36501,\"start\":36492},{\"end\":36512,\"start\":36505},{\"end\":36524,\"start\":36518},{\"end\":36867,\"start\":36861},{\"end\":36878,\"start\":36871},{\"end\":36887,\"start\":36882},{\"end\":37215,\"start\":37208},{\"end\":37224,\"start\":37219},{\"end\":37447,\"start\":37439},{\"end\":37562,\"start\":37552},{\"end\":37572,\"start\":37566},{\"end\":37582,\"start\":37576},{\"end\":37594,\"start\":37586},{\"end\":37606,\"start\":37598},{\"end\":37940,\"start\":37934},{\"end\":37952,\"start\":37947},{\"end\":37959,\"start\":37956},{\"end\":37972,\"start\":37963},{\"end\":37978,\"start\":37974},{\"end\":38410,\"start\":38406},{\"end\":38417,\"start\":38414},{\"end\":38656,\"start\":38652},{\"end\":38663,\"start\":38660},{\"end\":38671,\"start\":38667},{\"end\":38679,\"start\":38675},{\"end\":38687,\"start\":38683},{\"end\":38695,\"start\":38691},{\"end\":38703,\"start\":38699},{\"end\":38710,\"start\":38707},{\"end\":38717,\"start\":38714},{\"end\":38726,\"start\":38721},{\"end\":38734,\"start\":38730},{\"end\":39078,\"start\":39072},{\"end\":39090,\"start\":39082},{\"end\":39104,\"start\":39094},{\"end\":39117,\"start\":39108},{\"end\":39441,\"start\":39438},{\"end\":39448,\"start\":39445},{\"end\":39465,\"start\":39459},{\"end\":39797,\"start\":39791},{\"end\":39807,\"start\":39801},{\"end\":39818,\"start\":39813},{\"end\":40083,\"start\":40076},{\"end\":40171,\"start\":40168},{\"end\":40178,\"start\":40175},{\"end\":40184,\"start\":40182},{\"end\":40192,\"start\":40188},{\"end\":40200,\"start\":40196},{\"end\":40207,\"start\":40204},{\"end\":40215,\"start\":40211},{\"end\":40223,\"start\":40219},{\"end\":40239,\"start\":40227},{\"end\":40245,\"start\":40243},{\"end\":40255,\"start\":40249},{\"end\":40265,\"start\":40261},{\"end\":40278,\"start\":40271},{\"end\":40287,\"start\":40282},{\"end\":40301,\"start\":40291},{\"end\":40308,\"start\":40305},{\"end\":40317,\"start\":40312},{\"end\":40325,\"start\":40321},{\"end\":40340,\"start\":40327},{\"end\":40895,\"start\":40888},{\"end\":40908,\"start\":40899},{\"end\":41228,\"start\":41225},{\"end\":41235,\"start\":41232},{\"end\":41243,\"start\":41239},{\"end\":41254,\"start\":41247},{\"end\":41518,\"start\":41511},{\"end\":41528,\"start\":41522},{\"end\":41537,\"start\":41532},{\"end\":41850,\"start\":41848},{\"end\":41857,\"start\":41854},{\"end\":41863,\"start\":41861},{\"end\":41870,\"start\":41867},{\"end\":41884,\"start\":41876},{\"end\":42217,\"start\":42207},{\"end\":42224,\"start\":42221},{\"end\":42232,\"start\":42228},{\"end\":42239,\"start\":42236},{\"end\":42250,\"start\":42243},{\"end\":42547,\"start\":42541},{\"end\":42556,\"start\":42554},{\"end\":42565,\"start\":42560},{\"end\":42578,\"start\":42569},{\"end\":42594,\"start\":42582},{\"end\":42894,\"start\":42887},{\"end\":42903,\"start\":42898},{\"end\":42915,\"start\":42907},{\"end\":42925,\"start\":42919},{\"end\":42939,\"start\":42929},{\"end\":42949,\"start\":42943},{\"end\":42958,\"start\":42953},{\"end\":43252,\"start\":43248},{\"end\":43260,\"start\":43256},{\"end\":43269,\"start\":43264},{\"end\":43279,\"start\":43273},{\"end\":43293,\"start\":43285},{\"end\":43303,\"start\":43297},{\"end\":43668,\"start\":43665},{\"end\":43676,\"start\":43672},{\"end\":43683,\"start\":43680},{\"end\":43694,\"start\":43690},{\"end\":43704,\"start\":43698},{\"end\":43712,\"start\":43708},{\"end\":43720,\"start\":43718},{\"end\":43728,\"start\":43724},{\"end\":43734,\"start\":43732},{\"end\":43744,\"start\":43738},{\"end\":44116,\"start\":44109},{\"end\":44125,\"start\":44120},{\"end\":44134,\"start\":44129},{\"end\":44385,\"start\":44380},{\"end\":44393,\"start\":44389},{\"end\":44401,\"start\":44397},{\"end\":44416,\"start\":44405},{\"end\":44784,\"start\":44775},{\"end\":44792,\"start\":44788},{\"end\":44799,\"start\":44796},{\"end\":44808,\"start\":44803},{\"end\":44814,\"start\":44812},{\"end\":44824,\"start\":44818},{\"end\":44832,\"start\":44828},{\"end\":44839,\"start\":44836},{\"end\":45220,\"start\":45211},{\"end\":45228,\"start\":45224},{\"end\":45235,\"start\":45232},{\"end\":45241,\"start\":45239},{\"end\":45251,\"start\":45245},{\"end\":45259,\"start\":45255},{\"end\":45575,\"start\":45569},{\"end\":45587,\"start\":45579},{\"end\":45595,\"start\":45591},{\"end\":45604,\"start\":45599},{\"end\":45612,\"start\":45608},{\"end\":45621,\"start\":45616},{\"end\":45634,\"start\":45625},{\"end\":45641,\"start\":45638},{\"end\":45653,\"start\":45645},{\"end\":45983,\"start\":45980},{\"end\":45990,\"start\":45987},{\"end\":46279,\"start\":46273},{\"end\":46290,\"start\":46283},{\"end\":46516,\"start\":46511},{\"end\":46523,\"start\":46520},{\"end\":46540,\"start\":46527},{\"end\":46551,\"start\":46546},{\"end\":46563,\"start\":46555},{\"end\":46574,\"start\":46567},{\"end\":46584,\"start\":46578},{\"end\":46933,\"start\":46924},{\"end\":47115,\"start\":47104},{\"end\":47127,\"start\":47119},{\"end\":47139,\"start\":47131},{\"end\":47150,\"start\":47143},{\"end\":47160,\"start\":47154},{\"end\":47171,\"start\":47164},{\"end\":47182,\"start\":47175},{\"end\":47196,\"start\":47186},{\"end\":47206,\"start\":47200},{\"end\":47216,\"start\":47210},{\"end\":47223,\"start\":47220},{\"end\":47238,\"start\":47229},{\"end\":47247,\"start\":47242},{\"end\":47259,\"start\":47254},{\"end\":47266,\"start\":47263},{\"end\":47279,\"start\":47270},{\"end\":47285,\"start\":47283},{\"end\":47295,\"start\":47289},{\"end\":47869,\"start\":47866},{\"end\":47878,\"start\":47873},{\"end\":47891,\"start\":47882},{\"end\":48336,\"start\":48331},{\"end\":48345,\"start\":48340},{\"end\":48355,\"start\":48349},{\"end\":48366,\"start\":48359},{\"end\":48379,\"start\":48370},{\"end\":48388,\"start\":48383},{\"end\":48399,\"start\":48392},{\"end\":48408,\"start\":48403},{\"end\":48415,\"start\":48412},{\"end\":48430,\"start\":48419},{\"end\":48440,\"start\":48434},{\"end\":48449,\"start\":48444},{\"end\":48874,\"start\":48872},{\"end\":48882,\"start\":48878},{\"end\":48888,\"start\":48886},{\"end\":48896,\"start\":48892},{\"end\":48904,\"start\":48900},{\"end\":49236,\"start\":49231},{\"end\":49244,\"start\":49242},{\"end\":49253,\"start\":49248},{\"end\":49262,\"start\":49259},{\"end\":49279,\"start\":49266},{\"end\":49290,\"start\":49283},{\"end\":49635,\"start\":49632},{\"end\":49647,\"start\":49639},{\"end\":49656,\"start\":49651},{\"end\":49953,\"start\":49943},{\"end\":49960,\"start\":49957},{\"end\":50187,\"start\":50185},{\"end\":50196,\"start\":50191},{\"end\":50206,\"start\":50200},{\"end\":50213,\"start\":50210},{\"end\":50222,\"start\":50215},{\"end\":50582,\"start\":50579},{\"end\":50590,\"start\":50586},{\"end\":50600,\"start\":50594},{\"end\":50612,\"start\":50606},{\"end\":50624,\"start\":50616},{\"end\":51019,\"start\":51012},{\"end\":51027,\"start\":51023},{\"end\":51040,\"start\":51033},{\"end\":51048,\"start\":51044},{\"end\":51242,\"start\":51236},{\"end\":51255,\"start\":51246},{\"end\":51263,\"start\":51259},{\"end\":51270,\"start\":51267},{\"end\":51280,\"start\":51274},{\"end\":51292,\"start\":51284},{\"end\":51300,\"start\":51296},{\"end\":51307,\"start\":51302},{\"end\":51633,\"start\":51625},{\"end\":51640,\"start\":51637},{\"end\":51653,\"start\":51644},{\"end\":51779,\"start\":51771},{\"end\":51787,\"start\":51783},{\"end\":51794,\"start\":51791},{\"end\":52062,\"start\":52058},{\"end\":52070,\"start\":52066},{\"end\":52078,\"start\":52074},{\"end\":52375,\"start\":52365},{\"end\":52385,\"start\":52379},{\"end\":52396,\"start\":52389},{\"end\":52695,\"start\":52689},{\"end\":52706,\"start\":52699},{\"end\":52715,\"start\":52710},{\"end\":52726,\"start\":52719},{\"end\":52735,\"start\":52730},{\"end\":52742,\"start\":52739},{\"end\":52757,\"start\":52746},{\"end\":53083,\"start\":53081},{\"end\":53091,\"start\":53087},{\"end\":53098,\"start\":53095},{\"end\":53105,\"start\":53102},{\"end\":53112,\"start\":53109},{\"end\":53120,\"start\":53116},{\"end\":53128,\"start\":53124},{\"end\":53134,\"start\":53132},{\"end\":53567,\"start\":53560},{\"end\":53576,\"start\":53573},{\"end\":53587,\"start\":53580},{\"end\":53597,\"start\":53591},{\"end\":53604,\"start\":53601},{\"end\":53615,\"start\":53608},{\"end\":53625,\"start\":53619},{\"end\":53635,\"start\":53629},{\"end\":53646,\"start\":53639},{\"end\":53655,\"start\":53650},{\"end\":53666,\"start\":53659},{\"end\":53679,\"start\":53670},{\"end\":54064,\"start\":54058},{\"end\":54075,\"start\":54068},{\"end\":54086,\"start\":54079},{\"end\":54093,\"start\":54090},{\"end\":54103,\"start\":54097},{\"end\":54113,\"start\":54107},{\"end\":54121,\"start\":54117},{\"end\":54127,\"start\":54125},{\"end\":54136,\"start\":54133},{\"end\":54438,\"start\":54432},{\"end\":54448,\"start\":54442},{\"end\":54455,\"start\":54452},{\"end\":54463,\"start\":54459},{\"end\":54471,\"start\":54467},{\"end\":54482,\"start\":54475},{\"end\":54490,\"start\":54486},{\"end\":54503,\"start\":54494},{\"end\":54781,\"start\":54774},{\"end\":54793,\"start\":54785},{\"end\":55161,\"start\":55152},{\"end\":55302,\"start\":55294},{\"end\":55316,\"start\":55306},{\"end\":55327,\"start\":55320},{\"end\":55338,\"start\":55333},{\"end\":55635,\"start\":55629},{\"end\":55646,\"start\":55639},{\"end\":55657,\"start\":55650},{\"end\":55665,\"start\":55661},{\"end\":55673,\"start\":55669},{\"end\":55687,\"start\":55679},{\"end\":55700,\"start\":55691},{\"end\":56076,\"start\":56067},{\"end\":56087,\"start\":56080},{\"end\":56102,\"start\":56091},{\"end\":56111,\"start\":56106},{\"end\":56122,\"start\":56115},{\"end\":56133,\"start\":56124},{\"end\":56501,\"start\":56494},{\"end\":56517,\"start\":56505},{\"end\":56528,\"start\":56521},{\"end\":56860,\"start\":56856},{\"end\":56869,\"start\":56864},{\"end\":56877,\"start\":56873},{\"end\":56883,\"start\":56881},{\"end\":57210,\"start\":57204},{\"end\":57223,\"start\":57214},{\"end\":57229,\"start\":57227},{\"end\":57239,\"start\":57233},{\"end\":57252,\"start\":57245},{\"end\":57258,\"start\":57256},{\"end\":57267,\"start\":57262},{\"end\":57663,\"start\":57659},{\"end\":57973,\"start\":57965},{\"end\":57985,\"start\":57977},{\"end\":58258,\"start\":58256},{\"end\":58265,\"start\":58262},{\"end\":58272,\"start\":58269},{\"end\":58278,\"start\":58276},{\"end\":58543,\"start\":58540},{\"end\":58552,\"start\":58547},{\"end\":58564,\"start\":58556},{\"end\":58574,\"start\":58568},{\"end\":58584,\"start\":58578},{\"end\":58931,\"start\":58925},{\"end\":58942,\"start\":58935},{\"end\":58952,\"start\":58946},{\"end\":58966,\"start\":58956},{\"end\":58978,\"start\":58970},{\"end\":59281,\"start\":59277},{\"end\":59293,\"start\":59285},{\"end\":59302,\"start\":59297},{\"end\":59318,\"start\":59306},{\"end\":59330,\"start\":59322},{\"end\":59337,\"start\":59334},{\"end\":59349,\"start\":59341},{\"end\":59360,\"start\":59353},{\"end\":59370,\"start\":59364},{\"end\":59386,\"start\":59374},{\"end\":59396,\"start\":59390},{\"end\":59411,\"start\":59400},{\"end\":59437,\"start\":59415},{\"end\":59443,\"start\":59441},{\"end\":59454,\"start\":59447},{\"end\":60186,\"start\":60179},{\"end\":60197,\"start\":60190},{\"end\":60207,\"start\":60201},{\"end\":60220,\"start\":60211},{\"end\":60229,\"start\":60224},{\"end\":60240,\"start\":60235},{\"end\":60252,\"start\":60246},{\"end\":60266,\"start\":60256},{\"end\":60622,\"start\":60614},{\"end\":60637,\"start\":60628},{\"end\":60651,\"start\":60643},{\"end\":60966,\"start\":60962},{\"end\":60973,\"start\":60970},{\"end\":60981,\"start\":60977},{\"end\":60988,\"start\":60985},{\"end\":60996,\"start\":60992},{\"end\":61004,\"start\":61000},{\"end\":61012,\"start\":61006},{\"end\":61253,\"start\":61250},{\"end\":61260,\"start\":61257},{\"end\":61265,\"start\":61262},{\"end\":61634,\"start\":61626},{\"end\":61644,\"start\":61638},{\"end\":61654,\"start\":61648},{\"end\":62082,\"start\":62080},{\"end\":62091,\"start\":62086},{\"end\":62099,\"start\":62097},{\"end\":62435,\"start\":62430},{\"end\":62444,\"start\":62439},{\"end\":62450,\"start\":62448},{\"end\":62458,\"start\":62454},{\"end\":62465,\"start\":62462},{\"end\":62478,\"start\":62471},{\"end\":62487,\"start\":62482},{\"end\":62499,\"start\":62491},{\"end\":62825,\"start\":62822},{\"end\":62838,\"start\":62829},{\"end\":62849,\"start\":62844},{\"end\":62857,\"start\":62853},{\"end\":63205,\"start\":63198},{\"end\":63213,\"start\":63209},{\"end\":63222,\"start\":63217},{\"end\":63231,\"start\":63226},{\"end\":63578,\"start\":63573},{\"end\":63584,\"start\":63582},{\"end\":63591,\"start\":63588},{\"end\":63600,\"start\":63597},{\"end\":63902,\"start\":63898},{\"end\":63910,\"start\":63906},{\"end\":63921,\"start\":63914},{\"end\":63932,\"start\":63925}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32686,\"start\":32355},{\"attributes\":{\"id\":\"b1\"},\"end\":32985,\"start\":32688},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14604520},\"end\":33384,\"start\":32987},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10986188},\"end\":33704,\"start\":33386},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":218971783},\"end\":34607,\"start\":33706},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":23163324},\"end\":34906,\"start\":34609},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b6\"},\"end\":36409,\"start\":34908},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":211096730},\"end\":36771,\"start\":36411},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5555257},\"end\":37132,\"start\":36773},{\"attributes\":{\"doi\":\"arXiv:1803.05449\",\"id\":\"b9\"},\"end\":37392,\"start\":37134},{\"attributes\":{\"id\":\"b10\"},\"end\":37510,\"start\":37394},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3252915},\"end\":37854,\"start\":37512},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52967399},\"end\":38331,\"start\":37856},{\"attributes\":{\"doi\":\"arXiv:2005.12766\",\"id\":\"b13\"},\"end\":38579,\"start\":38333},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":211171605},\"end\":39068,\"start\":38581},{\"attributes\":{\"doi\":\"arXiv:2109.10086\",\"id\":\"b15\"},\"end\":39382,\"start\":39070},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233296292},\"end\":39708,\"start\":39384},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":219530980},\"end\":40040,\"start\":39710},{\"attributes\":{\"id\":\"b18\"},\"end\":40164,\"start\":40042},{\"attributes\":{\"id\":\"b19\"},\"end\":40790,\"start\":40166},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15816723},\"end\":41166,\"start\":40792},{\"attributes\":{\"doi\":\"arXiv:2002.08909\",\"id\":\"b21\"},\"end\":41448,\"start\":41168},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8281592},\"end\":41777,\"start\":41450},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":207930212},\"end\":42117,\"start\":41779},{\"attributes\":{\"doi\":\"arXiv:2104.06967\",\"id\":\"b24\"},\"end\":42466,\"start\":42119},{\"attributes\":{\"doi\":\"arXiv:1909.09436\",\"id\":\"b25\"},\"end\":42807,\"start\":42468},{\"attributes\":{\"doi\":\"arXiv:2112.09118\",\"id\":\"b26\"},\"end\":43202,\"start\":42809},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":220425360},\"end\":43570,\"start\":43204},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231879586},\"end\":44062,\"start\":43572},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":926364},\"end\":44286,\"start\":44064},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":26501419},\"end\":44711,\"start\":44288},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":215737187},\"end\":45148,\"start\":44713},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":215737187},\"end\":45563,\"start\":45150},{\"attributes\":{\"doi\":\"arXiv:2004.11362\",\"id\":\"b33\"},\"end\":45896,\"start\":45565},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":231839613},\"end\":46234,\"start\":45898},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":216078090},\"end\":46485,\"start\":46236},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9126867},\"end\":46839,\"start\":46487},{\"attributes\":{\"doi\":\"arXiv:1805.06201\",\"id\":\"b37\"},\"end\":47100,\"start\":46841},{\"attributes\":{\"id\":\"b38\"},\"end\":47791,\"start\":47102},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":173990818},\"end\":48258,\"start\":47793},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":218869575},\"end\":48807,\"start\":48260},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":226262227},\"end\":49181,\"start\":48809},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":207996257},\"end\":49569,\"start\":49183},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":222310837},\"end\":49877,\"start\":49571},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3525802},\"end\":50181,\"start\":49879},{\"attributes\":{\"doi\":\"arXiv:1908.02265\",\"id\":\"b45\"},\"end\":50526,\"start\":50183},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":85518027},\"end\":50946,\"start\":50528},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b47\"},\"end\":51232,\"start\":50948},{\"attributes\":{\"doi\":\"arXiv:1611.09268\",\"id\":\"b48\"},\"end\":51588,\"start\":51234},{\"attributes\":{\"id\":\"b49\"},\"end\":51767,\"start\":51590},{\"attributes\":{\"doi\":\"arXiv:1910.14424\",\"id\":\"b50\"},\"end\":52000,\"start\":51769},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52070035},\"end\":52314,\"start\":52002},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1957433},\"end\":52641,\"start\":52316},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3626819},\"end\":52973,\"start\":52643},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":231815627},\"end\":53485,\"start\":52975},{\"attributes\":{\"doi\":\"arXiv:2103.00020\",\"id\":\"b55\"},\"end\":53971,\"start\":53487},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b56\"},\"end\":54393,\"start\":53973},{\"attributes\":{\"doi\":\"arXiv:2102.12092\",\"id\":\"b57\"},\"end\":54706,\"start\":54395},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":201646309},\"end\":55042,\"start\":54708},{\"attributes\":{\"id\":\"b59\"},\"end\":55290,\"start\":55044},{\"attributes\":{\"doi\":\"arXiv:1804.09301\",\"id\":\"b60\"},\"end\":55546,\"start\":55292},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":230437591},\"end\":56063,\"start\":55548},{\"attributes\":{\"doi\":\"arXiv:2112.01488\",\"id\":\"b62\"},\"end\":56424,\"start\":56065},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":206592766},\"end\":56747,\"start\":56426},{\"attributes\":{\"doi\":\"arXiv:2009.13818\",\"id\":\"b64\"},\"end\":57121,\"start\":56749},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":990233},\"end\":57585,\"start\":57123},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":911406},\"end\":57874,\"start\":57587},{\"attributes\":{\"doi\":\"arXiv:2106.10328\",\"id\":\"b67\"},\"end\":58174,\"start\":57876},{\"attributes\":{\"doi\":\"arXiv:2103.15316\",\"id\":\"b68\"},\"end\":58463,\"start\":58176},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":102483628},\"end\":58832,\"start\":58465},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":233296016},\"end\":59273,\"start\":58834},{\"attributes\":{\"doi\":\"arXiv:1609.03499\",\"id\":\"b71\"},\"end\":60148,\"start\":59275},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":13756489},\"end\":60539,\"start\":60150},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":16925665},\"end\":60867,\"start\":60541},{\"attributes\":{\"id\":\"b74\"},\"end\":61244,\"start\":60869},{\"attributes\":{\"id\":\"b75\"},\"end\":61543,\"start\":61246},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":3432876},\"end\":61996,\"start\":61545},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":25755275},\"end\":62341,\"start\":61998},{\"attributes\":{\"id\":\"b78\"},\"end\":62749,\"start\":62343},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":1965270},\"end\":63129,\"start\":62751},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":232110471},\"end\":63491,\"start\":63131},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":221948991},\"end\":63894,\"start\":63493},{\"attributes\":{\"id\":\"b82\"},\"end\":64220,\"start\":63896}]", "bib_title": "[{\"end\":33051,\"start\":32987},{\"end\":33431,\"start\":33386},{\"end\":33743,\"start\":33706},{\"end\":34688,\"start\":34609},{\"end\":36480,\"start\":36411},{\"end\":36857,\"start\":36773},{\"end\":37548,\"start\":37512},{\"end\":37930,\"start\":37856},{\"end\":38648,\"start\":38581},{\"end\":39434,\"start\":39384},{\"end\":39785,\"start\":39710},{\"end\":40072,\"start\":40042},{\"end\":40884,\"start\":40792},{\"end\":41507,\"start\":41450},{\"end\":41844,\"start\":41779},{\"end\":43244,\"start\":43204},{\"end\":43661,\"start\":43572},{\"end\":44105,\"start\":44064},{\"end\":44376,\"start\":44288},{\"end\":44771,\"start\":44713},{\"end\":45207,\"start\":45150},{\"end\":45976,\"start\":45898},{\"end\":46267,\"start\":46236},{\"end\":46507,\"start\":46487},{\"end\":47862,\"start\":47793},{\"end\":48323,\"start\":48260},{\"end\":48868,\"start\":48809},{\"end\":49225,\"start\":49183},{\"end\":49628,\"start\":49571},{\"end\":49939,\"start\":49879},{\"end\":50575,\"start\":50528},{\"end\":52052,\"start\":52002},{\"end\":52361,\"start\":52316},{\"end\":52683,\"start\":52643},{\"end\":53077,\"start\":52975},{\"end\":54770,\"start\":54708},{\"end\":55623,\"start\":55548},{\"end\":56490,\"start\":56426},{\"end\":57200,\"start\":57123},{\"end\":57655,\"start\":57587},{\"end\":58536,\"start\":58465},{\"end\":58921,\"start\":58834},{\"end\":60175,\"start\":60150},{\"end\":60610,\"start\":60541},{\"end\":61622,\"start\":61545},{\"end\":62076,\"start\":61998},{\"end\":62815,\"start\":62751},{\"end\":63194,\"start\":63131},{\"end\":63569,\"start\":63493}]", "bib_author": "[{\"end\":32480,\"start\":32470},{\"end\":32495,\"start\":32480},{\"end\":32506,\"start\":32495},{\"end\":32784,\"start\":32771},{\"end\":32793,\"start\":32784},{\"end\":32802,\"start\":32793},{\"end\":32815,\"start\":32802},{\"end\":32824,\"start\":32815},{\"end\":33065,\"start\":33053},{\"end\":33075,\"start\":33065},{\"end\":33084,\"start\":33075},{\"end\":33097,\"start\":33084},{\"end\":33444,\"start\":33433},{\"end\":33462,\"start\":33444},{\"end\":33475,\"start\":33462},{\"end\":33484,\"start\":33475},{\"end\":33496,\"start\":33484},{\"end\":33754,\"start\":33745},{\"end\":33762,\"start\":33754},{\"end\":33771,\"start\":33762},{\"end\":33782,\"start\":33771},{\"end\":33794,\"start\":33782},{\"end\":33806,\"start\":33794},{\"end\":33821,\"start\":33806},{\"end\":33830,\"start\":33821},{\"end\":33840,\"start\":33830},{\"end\":33850,\"start\":33840},{\"end\":33861,\"start\":33850},{\"end\":33877,\"start\":33861},{\"end\":33888,\"start\":33877},{\"end\":33900,\"start\":33888},{\"end\":33909,\"start\":33900},{\"end\":33919,\"start\":33909},{\"end\":33930,\"start\":33919},{\"end\":33936,\"start\":33930},{\"end\":33946,\"start\":33936},{\"end\":33955,\"start\":33946},{\"end\":33963,\"start\":33955},{\"end\":33973,\"start\":33963},{\"end\":33983,\"start\":33973},{\"end\":33991,\"start\":33983},{\"end\":34000,\"start\":33991},{\"end\":34009,\"start\":34000},{\"end\":34019,\"start\":34009},{\"end\":34033,\"start\":34019},{\"end\":34044,\"start\":34033},{\"end\":34057,\"start\":34044},{\"end\":34066,\"start\":34057},{\"end\":34070,\"start\":34066},{\"end\":34702,\"start\":34690},{\"end\":34714,\"start\":34702},{\"end\":34727,\"start\":34714},{\"end\":34918,\"start\":34910},{\"end\":34928,\"start\":34918},{\"end\":34935,\"start\":34928},{\"end\":34943,\"start\":34935},{\"end\":34966,\"start\":34943},{\"end\":34976,\"start\":34966},{\"end\":34987,\"start\":34976},{\"end\":34996,\"start\":34987},{\"end\":35006,\"start\":34996},{\"end\":35018,\"start\":35006},{\"end\":35025,\"start\":35018},{\"end\":35033,\"start\":35025},{\"end\":35044,\"start\":35033},{\"end\":35054,\"start\":35044},{\"end\":35064,\"start\":35054},{\"end\":35074,\"start\":35064},{\"end\":35085,\"start\":35074},{\"end\":35093,\"start\":35085},{\"end\":35101,\"start\":35093},{\"end\":35110,\"start\":35101},{\"end\":35120,\"start\":35110},{\"end\":35129,\"start\":35120},{\"end\":35139,\"start\":35129},{\"end\":35151,\"start\":35139},{\"end\":35161,\"start\":35151},{\"end\":35171,\"start\":35161},{\"end\":35181,\"start\":35171},{\"end\":35193,\"start\":35181},{\"end\":35205,\"start\":35193},{\"end\":35217,\"start\":35205},{\"end\":35227,\"start\":35217},{\"end\":35243,\"start\":35227},{\"end\":35253,\"start\":35243},{\"end\":35263,\"start\":35253},{\"end\":35272,\"start\":35263},{\"end\":35281,\"start\":35272},{\"end\":35289,\"start\":35281},{\"end\":35303,\"start\":35289},{\"end\":35313,\"start\":35303},{\"end\":35321,\"start\":35313},{\"end\":35333,\"start\":35321},{\"end\":35342,\"start\":35333},{\"end\":35352,\"start\":35342},{\"end\":35361,\"start\":35352},{\"end\":35371,\"start\":35361},{\"end\":35380,\"start\":35371},{\"end\":35392,\"start\":35380},{\"end\":35403,\"start\":35392},{\"end\":35413,\"start\":35403},{\"end\":35425,\"start\":35413},{\"end\":35435,\"start\":35425},{\"end\":35444,\"start\":35435},{\"end\":35456,\"start\":35444},{\"end\":35466,\"start\":35456},{\"end\":35476,\"start\":35466},{\"end\":35490,\"start\":35476},{\"end\":35503,\"start\":35490},{\"end\":35514,\"start\":35503},{\"end\":36490,\"start\":36482},{\"end\":36503,\"start\":36490},{\"end\":36514,\"start\":36503},{\"end\":36526,\"start\":36514},{\"end\":36869,\"start\":36859},{\"end\":36880,\"start\":36869},{\"end\":36889,\"start\":36880},{\"end\":37217,\"start\":37206},{\"end\":37226,\"start\":37217},{\"end\":37449,\"start\":37437},{\"end\":37564,\"start\":37550},{\"end\":37574,\"start\":37564},{\"end\":37584,\"start\":37574},{\"end\":37596,\"start\":37584},{\"end\":37608,\"start\":37596},{\"end\":37942,\"start\":37932},{\"end\":37954,\"start\":37942},{\"end\":37961,\"start\":37954},{\"end\":37974,\"start\":37961},{\"end\":37980,\"start\":37974},{\"end\":38412,\"start\":38404},{\"end\":38419,\"start\":38412},{\"end\":38658,\"start\":38650},{\"end\":38665,\"start\":38658},{\"end\":38673,\"start\":38665},{\"end\":38681,\"start\":38673},{\"end\":38689,\"start\":38681},{\"end\":38697,\"start\":38689},{\"end\":38705,\"start\":38697},{\"end\":38712,\"start\":38705},{\"end\":38719,\"start\":38712},{\"end\":38728,\"start\":38719},{\"end\":38736,\"start\":38728},{\"end\":39080,\"start\":39070},{\"end\":39092,\"start\":39080},{\"end\":39106,\"start\":39092},{\"end\":39119,\"start\":39106},{\"end\":39443,\"start\":39436},{\"end\":39450,\"start\":39443},{\"end\":39457,\"start\":39450},{\"end\":39467,\"start\":39457},{\"end\":39799,\"start\":39787},{\"end\":39809,\"start\":39799},{\"end\":39820,\"start\":39809},{\"end\":39827,\"start\":39820},{\"end\":39831,\"start\":39827},{\"end\":40085,\"start\":40074},{\"end\":40173,\"start\":40166},{\"end\":40180,\"start\":40173},{\"end\":40186,\"start\":40180},{\"end\":40194,\"start\":40186},{\"end\":40202,\"start\":40194},{\"end\":40209,\"start\":40202},{\"end\":40217,\"start\":40209},{\"end\":40225,\"start\":40217},{\"end\":40241,\"start\":40225},{\"end\":40247,\"start\":40241},{\"end\":40257,\"start\":40247},{\"end\":40267,\"start\":40257},{\"end\":40280,\"start\":40267},{\"end\":40289,\"start\":40280},{\"end\":40303,\"start\":40289},{\"end\":40310,\"start\":40303},{\"end\":40319,\"start\":40310},{\"end\":40327,\"start\":40319},{\"end\":40342,\"start\":40327},{\"end\":40897,\"start\":40886},{\"end\":40910,\"start\":40897},{\"end\":41230,\"start\":41223},{\"end\":41237,\"start\":41230},{\"end\":41245,\"start\":41237},{\"end\":41256,\"start\":41245},{\"end\":41264,\"start\":41256},{\"end\":41268,\"start\":41264},{\"end\":41520,\"start\":41509},{\"end\":41530,\"start\":41520},{\"end\":41539,\"start\":41530},{\"end\":41852,\"start\":41846},{\"end\":41859,\"start\":41852},{\"end\":41865,\"start\":41859},{\"end\":41872,\"start\":41865},{\"end\":41886,\"start\":41872},{\"end\":42219,\"start\":42205},{\"end\":42226,\"start\":42219},{\"end\":42234,\"start\":42226},{\"end\":42241,\"start\":42234},{\"end\":42252,\"start\":42241},{\"end\":42549,\"start\":42539},{\"end\":42558,\"start\":42549},{\"end\":42567,\"start\":42558},{\"end\":42580,\"start\":42567},{\"end\":42596,\"start\":42580},{\"end\":42896,\"start\":42885},{\"end\":42905,\"start\":42896},{\"end\":42917,\"start\":42905},{\"end\":42927,\"start\":42917},{\"end\":42941,\"start\":42927},{\"end\":42951,\"start\":42941},{\"end\":42960,\"start\":42951},{\"end\":43254,\"start\":43246},{\"end\":43262,\"start\":43254},{\"end\":43271,\"start\":43262},{\"end\":43281,\"start\":43271},{\"end\":43295,\"start\":43281},{\"end\":43305,\"start\":43295},{\"end\":43670,\"start\":43663},{\"end\":43678,\"start\":43670},{\"end\":43685,\"start\":43678},{\"end\":43696,\"start\":43685},{\"end\":43706,\"start\":43696},{\"end\":43714,\"start\":43706},{\"end\":43722,\"start\":43714},{\"end\":43730,\"start\":43722},{\"end\":43736,\"start\":43730},{\"end\":43746,\"start\":43736},{\"end\":44118,\"start\":44107},{\"end\":44127,\"start\":44118},{\"end\":44136,\"start\":44127},{\"end\":44387,\"start\":44378},{\"end\":44395,\"start\":44387},{\"end\":44403,\"start\":44395},{\"end\":44418,\"start\":44403},{\"end\":44786,\"start\":44773},{\"end\":44794,\"start\":44786},{\"end\":44801,\"start\":44794},{\"end\":44810,\"start\":44801},{\"end\":44816,\"start\":44810},{\"end\":44826,\"start\":44816},{\"end\":44834,\"start\":44826},{\"end\":44841,\"start\":44834},{\"end\":45222,\"start\":45209},{\"end\":45230,\"start\":45222},{\"end\":45237,\"start\":45230},{\"end\":45243,\"start\":45237},{\"end\":45253,\"start\":45243},{\"end\":45261,\"start\":45253},{\"end\":45267,\"start\":45261},{\"end\":45271,\"start\":45267},{\"end\":45577,\"start\":45567},{\"end\":45589,\"start\":45577},{\"end\":45597,\"start\":45589},{\"end\":45606,\"start\":45597},{\"end\":45614,\"start\":45606},{\"end\":45623,\"start\":45614},{\"end\":45636,\"start\":45623},{\"end\":45643,\"start\":45636},{\"end\":45655,\"start\":45643},{\"end\":45985,\"start\":45978},{\"end\":45992,\"start\":45985},{\"end\":45998,\"start\":45992},{\"end\":46002,\"start\":45998},{\"end\":46281,\"start\":46269},{\"end\":46292,\"start\":46281},{\"end\":46518,\"start\":46509},{\"end\":46525,\"start\":46518},{\"end\":46542,\"start\":46525},{\"end\":46553,\"start\":46542},{\"end\":46565,\"start\":46553},{\"end\":46576,\"start\":46565},{\"end\":46586,\"start\":46576},{\"end\":46935,\"start\":46922},{\"end\":47117,\"start\":47102},{\"end\":47129,\"start\":47117},{\"end\":47141,\"start\":47129},{\"end\":47152,\"start\":47141},{\"end\":47162,\"start\":47152},{\"end\":47173,\"start\":47162},{\"end\":47184,\"start\":47173},{\"end\":47198,\"start\":47184},{\"end\":47208,\"start\":47198},{\"end\":47218,\"start\":47208},{\"end\":47225,\"start\":47218},{\"end\":47240,\"start\":47225},{\"end\":47249,\"start\":47240},{\"end\":47261,\"start\":47249},{\"end\":47268,\"start\":47261},{\"end\":47281,\"start\":47268},{\"end\":47287,\"start\":47281},{\"end\":47297,\"start\":47287},{\"end\":47871,\"start\":47864},{\"end\":47880,\"start\":47871},{\"end\":47893,\"start\":47880},{\"end\":48338,\"start\":48325},{\"end\":48347,\"start\":48338},{\"end\":48357,\"start\":48347},{\"end\":48368,\"start\":48357},{\"end\":48381,\"start\":48368},{\"end\":48390,\"start\":48381},{\"end\":48401,\"start\":48390},{\"end\":48410,\"start\":48401},{\"end\":48417,\"start\":48410},{\"end\":48432,\"start\":48417},{\"end\":48442,\"start\":48432},{\"end\":48451,\"start\":48442},{\"end\":48876,\"start\":48870},{\"end\":48884,\"start\":48876},{\"end\":48890,\"start\":48884},{\"end\":48898,\"start\":48890},{\"end\":48906,\"start\":48898},{\"end\":48911,\"start\":48906},{\"end\":48915,\"start\":48911},{\"end\":49238,\"start\":49227},{\"end\":49246,\"start\":49238},{\"end\":49255,\"start\":49246},{\"end\":49264,\"start\":49255},{\"end\":49281,\"start\":49264},{\"end\":49292,\"start\":49281},{\"end\":49637,\"start\":49630},{\"end\":49649,\"start\":49637},{\"end\":49658,\"start\":49649},{\"end\":49955,\"start\":49941},{\"end\":49962,\"start\":49955},{\"end\":50189,\"start\":50183},{\"end\":50198,\"start\":50189},{\"end\":50208,\"start\":50198},{\"end\":50215,\"start\":50208},{\"end\":50224,\"start\":50215},{\"end\":50584,\"start\":50577},{\"end\":50592,\"start\":50584},{\"end\":50602,\"start\":50592},{\"end\":50614,\"start\":50602},{\"end\":50626,\"start\":50614},{\"end\":51021,\"start\":51010},{\"end\":51029,\"start\":51021},{\"end\":51042,\"start\":51029},{\"end\":51050,\"start\":51042},{\"end\":51244,\"start\":51234},{\"end\":51257,\"start\":51244},{\"end\":51265,\"start\":51257},{\"end\":51272,\"start\":51265},{\"end\":51282,\"start\":51272},{\"end\":51294,\"start\":51282},{\"end\":51302,\"start\":51294},{\"end\":51309,\"start\":51302},{\"end\":51635,\"start\":51623},{\"end\":51642,\"start\":51635},{\"end\":51655,\"start\":51642},{\"end\":51781,\"start\":51769},{\"end\":51789,\"start\":51781},{\"end\":51796,\"start\":51789},{\"end\":51802,\"start\":51796},{\"end\":51806,\"start\":51802},{\"end\":52064,\"start\":52054},{\"end\":52072,\"start\":52064},{\"end\":52080,\"start\":52072},{\"end\":52377,\"start\":52363},{\"end\":52387,\"start\":52377},{\"end\":52398,\"start\":52387},{\"end\":52697,\"start\":52685},{\"end\":52708,\"start\":52697},{\"end\":52717,\"start\":52708},{\"end\":52728,\"start\":52717},{\"end\":52737,\"start\":52728},{\"end\":52744,\"start\":52737},{\"end\":52759,\"start\":52744},{\"end\":53085,\"start\":53079},{\"end\":53093,\"start\":53085},{\"end\":53100,\"start\":53093},{\"end\":53107,\"start\":53100},{\"end\":53114,\"start\":53107},{\"end\":53122,\"start\":53114},{\"end\":53130,\"start\":53122},{\"end\":53136,\"start\":53130},{\"end\":53143,\"start\":53136},{\"end\":53147,\"start\":53143},{\"end\":53569,\"start\":53558},{\"end\":53578,\"start\":53569},{\"end\":53589,\"start\":53578},{\"end\":53599,\"start\":53589},{\"end\":53606,\"start\":53599},{\"end\":53617,\"start\":53606},{\"end\":53627,\"start\":53617},{\"end\":53637,\"start\":53627},{\"end\":53648,\"start\":53637},{\"end\":53657,\"start\":53648},{\"end\":53668,\"start\":53657},{\"end\":53681,\"start\":53668},{\"end\":54066,\"start\":54056},{\"end\":54077,\"start\":54066},{\"end\":54088,\"start\":54077},{\"end\":54095,\"start\":54088},{\"end\":54105,\"start\":54095},{\"end\":54115,\"start\":54105},{\"end\":54123,\"start\":54115},{\"end\":54129,\"start\":54123},{\"end\":54138,\"start\":54129},{\"end\":54440,\"start\":54430},{\"end\":54450,\"start\":54440},{\"end\":54457,\"start\":54450},{\"end\":54465,\"start\":54457},{\"end\":54473,\"start\":54465},{\"end\":54484,\"start\":54473},{\"end\":54492,\"start\":54484},{\"end\":54505,\"start\":54492},{\"end\":54783,\"start\":54772},{\"end\":54795,\"start\":54783},{\"end\":55163,\"start\":55150},{\"end\":55304,\"start\":55292},{\"end\":55318,\"start\":55304},{\"end\":55329,\"start\":55318},{\"end\":55340,\"start\":55329},{\"end\":55637,\"start\":55625},{\"end\":55648,\"start\":55637},{\"end\":55659,\"start\":55648},{\"end\":55667,\"start\":55659},{\"end\":55675,\"start\":55667},{\"end\":55689,\"start\":55675},{\"end\":55702,\"start\":55689},{\"end\":56078,\"start\":56065},{\"end\":56089,\"start\":56078},{\"end\":56104,\"start\":56089},{\"end\":56113,\"start\":56104},{\"end\":56124,\"start\":56113},{\"end\":56135,\"start\":56124},{\"end\":56503,\"start\":56492},{\"end\":56519,\"start\":56503},{\"end\":56530,\"start\":56519},{\"end\":56862,\"start\":56854},{\"end\":56871,\"start\":56862},{\"end\":56879,\"start\":56871},{\"end\":56885,\"start\":56879},{\"end\":56892,\"start\":56885},{\"end\":56896,\"start\":56892},{\"end\":57212,\"start\":57202},{\"end\":57225,\"start\":57212},{\"end\":57231,\"start\":57225},{\"end\":57241,\"start\":57231},{\"end\":57254,\"start\":57241},{\"end\":57260,\"start\":57254},{\"end\":57269,\"start\":57260},{\"end\":57665,\"start\":57657},{\"end\":57975,\"start\":57963},{\"end\":57987,\"start\":57975},{\"end\":58260,\"start\":58254},{\"end\":58267,\"start\":58260},{\"end\":58274,\"start\":58267},{\"end\":58280,\"start\":58274},{\"end\":58545,\"start\":58538},{\"end\":58554,\"start\":58545},{\"end\":58566,\"start\":58554},{\"end\":58576,\"start\":58566},{\"end\":58586,\"start\":58576},{\"end\":58933,\"start\":58923},{\"end\":58944,\"start\":58933},{\"end\":58954,\"start\":58944},{\"end\":58968,\"start\":58954},{\"end\":58980,\"start\":58968},{\"end\":59283,\"start\":59275},{\"end\":59295,\"start\":59283},{\"end\":59304,\"start\":59295},{\"end\":59320,\"start\":59304},{\"end\":59332,\"start\":59320},{\"end\":59339,\"start\":59332},{\"end\":59351,\"start\":59339},{\"end\":59362,\"start\":59351},{\"end\":59372,\"start\":59362},{\"end\":59388,\"start\":59372},{\"end\":59398,\"start\":59388},{\"end\":59413,\"start\":59398},{\"end\":59439,\"start\":59413},{\"end\":59445,\"start\":59439},{\"end\":59456,\"start\":59445},{\"end\":60188,\"start\":60177},{\"end\":60199,\"start\":60188},{\"end\":60209,\"start\":60199},{\"end\":60222,\"start\":60209},{\"end\":60231,\"start\":60222},{\"end\":60242,\"start\":60231},{\"end\":60254,\"start\":60242},{\"end\":60268,\"start\":60254},{\"end\":60624,\"start\":60612},{\"end\":60639,\"start\":60624},{\"end\":60653,\"start\":60639},{\"end\":60968,\"start\":60960},{\"end\":60975,\"start\":60968},{\"end\":60983,\"start\":60975},{\"end\":60990,\"start\":60983},{\"end\":60998,\"start\":60990},{\"end\":61006,\"start\":60998},{\"end\":61014,\"start\":61006},{\"end\":61255,\"start\":61246},{\"end\":61262,\"start\":61255},{\"end\":61267,\"start\":61262},{\"end\":61636,\"start\":61624},{\"end\":61646,\"start\":61636},{\"end\":61656,\"start\":61646},{\"end\":62084,\"start\":62078},{\"end\":62093,\"start\":62084},{\"end\":62101,\"start\":62093},{\"end\":62107,\"start\":62101},{\"end\":62111,\"start\":62107},{\"end\":62437,\"start\":62428},{\"end\":62446,\"start\":62437},{\"end\":62452,\"start\":62446},{\"end\":62460,\"start\":62452},{\"end\":62467,\"start\":62460},{\"end\":62480,\"start\":62467},{\"end\":62489,\"start\":62480},{\"end\":62501,\"start\":62489},{\"end\":62827,\"start\":62817},{\"end\":62840,\"start\":62827},{\"end\":62851,\"start\":62840},{\"end\":62859,\"start\":62851},{\"end\":63207,\"start\":63196},{\"end\":63215,\"start\":63207},{\"end\":63224,\"start\":63215},{\"end\":63233,\"start\":63224},{\"end\":63240,\"start\":63233},{\"end\":63244,\"start\":63240},{\"end\":63580,\"start\":63571},{\"end\":63586,\"start\":63580},{\"end\":63593,\"start\":63586},{\"end\":63602,\"start\":63593},{\"end\":63609,\"start\":63602},{\"end\":63613,\"start\":63609},{\"end\":63904,\"start\":63896},{\"end\":63912,\"start\":63904},{\"end\":63923,\"start\":63912},{\"end\":63934,\"start\":63923},{\"end\":63942,\"start\":63934},{\"end\":63946,\"start\":63942}]", "bib_venue": "[{\"end\":32468,\"start\":32355},{\"end\":32769,\"start\":32688},{\"end\":33172,\"start\":33097},{\"end\":33521,\"start\":33496},{\"end\":34119,\"start\":34070},{\"end\":34734,\"start\":34727},{\"end\":36577,\"start\":36526},{\"end\":36935,\"start\":36889},{\"end\":37204,\"start\":37134},{\"end\":37435,\"start\":37394},{\"end\":37663,\"start\":37608},{\"end\":38082,\"start\":37980},{\"end\":38402,\"start\":38333},{\"end\":38798,\"start\":38736},{\"end\":39195,\"start\":39135},{\"end\":39529,\"start\":39467},{\"end\":39856,\"start\":39831},{\"end\":40098,\"start\":40085},{\"end\":40399,\"start\":40342},{\"end\":40968,\"start\":40910},{\"end\":41221,\"start\":41168},{\"end\":41585,\"start\":41539},{\"end\":41932,\"start\":41886},{\"end\":42203,\"start\":42119},{\"end\":42537,\"start\":42468},{\"end\":42883,\"start\":42809},{\"end\":43367,\"start\":43305},{\"end\":43797,\"start\":43746},{\"end\":44165,\"start\":44136},{\"end\":44488,\"start\":44418},{\"end\":44911,\"start\":44841},{\"end\":45341,\"start\":45271},{\"end\":46053,\"start\":46002},{\"end\":46350,\"start\":46292},{\"end\":46645,\"start\":46586},{\"end\":46920,\"start\":46841},{\"end\":47421,\"start\":47297},{\"end\":47958,\"start\":47893},{\"end\":48510,\"start\":48451},{\"end\":48977,\"start\":48915},{\"end\":49357,\"start\":49292},{\"end\":49707,\"start\":49658},{\"end\":50020,\"start\":49962},{\"end\":50327,\"start\":50240},{\"end\":50723,\"start\":50626},{\"end\":51008,\"start\":50948},{\"end\":51380,\"start\":51325},{\"end\":51621,\"start\":51590},{\"end\":51859,\"start\":51822},{\"end\":52142,\"start\":52080},{\"end\":52468,\"start\":52398},{\"end\":52786,\"start\":52759},{\"end\":53212,\"start\":53147},{\"end\":53556,\"start\":53487},{\"end\":54054,\"start\":53973},{\"end\":54428,\"start\":54395},{\"end\":54865,\"start\":54795},{\"end\":55148,\"start\":55044},{\"end\":55393,\"start\":55356},{\"end\":55727,\"start\":55702},{\"end\":56217,\"start\":56151},{\"end\":56576,\"start\":56530},{\"end\":56852,\"start\":56749},{\"end\":57339,\"start\":57269},{\"end\":57724,\"start\":57665},{\"end\":57961,\"start\":57876},{\"end\":58252,\"start\":58176},{\"end\":58636,\"start\":58586},{\"end\":59039,\"start\":58980},{\"end\":59563,\"start\":59488},{\"end\":60327,\"start\":60268},{\"end\":60685,\"start\":60653},{\"end\":60958,\"start\":60869},{\"end\":61370,\"start\":61283},{\"end\":61758,\"start\":61656},{\"end\":62157,\"start\":62111},{\"end\":62426,\"start\":62343},{\"end\":62925,\"start\":62859},{\"end\":63295,\"start\":63244},{\"end\":63675,\"start\":63613},{\"end\":64033,\"start\":63962},{\"end\":39868,\"start\":39858},{\"end\":52800,\"start\":52788},{\"end\":55781,\"start\":55771}]"}}}, "year": 2023, "month": 12, "day": 17}