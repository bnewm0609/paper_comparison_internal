{"id": 49301394, "updated": "2022-03-02 00:37:08.774", "metadata": {"title": "Hierarchical hyperdimensional computing for energy efficient classification", "authors": "[{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Chenyu\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Deqian\",\"last\":\"Kong\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 55th Annual Design Automation Conference", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Brain-inspired Hyperdimensional (HD) computing emulates cognition tasks by computing with hypervectors rather than traditional numerical values. In HD, an encoder maps inputs to high dimensional vectors (hypervectors) and combines them to generate a model for each existing class. During inference, HD performs the task of reasoning by looking for similarities of the input hypervector and each pre-stored class hypervector However, there is not a unique encoding in HD which can perfectly map inputs to hypervectors. This results in low HD classification accuracy over complex tasks such as speech recognition. In this paper we propose MHD, a multi-encoder hierarchical classifier, which enables HD to take full advantages of multiple encoders without increasing the cost of classification. MHD consists of two HD stages: a main stage and a decider stage. The main stage makes use of multiple classifiers with different encoders to classify a wide range of input data. Each classifier in the main stage can trade between efficiency and accuracy by dynamically varying the hypervectors' dimensions. The decider stage, located before the main stage, learns the difficulty of the input data and selects an encoder within the main stage that will provide the maximum accuracy, while also maximizing the efficiency of the classification task. We test the accuracy/efficiency of the proposed MHD on speech recognition application. Our evaluation shows that MHD can provide a 6.6\u00d7 improvement in energy efficiency and a 6.3\u00d7 speedup, as compared to baseline single level HD.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2809429070", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1145/3195970.3196060"}}, "content": {"source": {"pdf_hash": "202f51018e096deba3715c11d8e51cd162dad377", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3195970.3196060", "status": "BRONZE"}}, "grobid": {"id": "a4c58f68d7833163d925675e098f678ad7fc3d07", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/202f51018e096deba3715c11d8e51cd162dad377.txt", "contents": "\nHierarchical Hyperdimensional Computing for Energy Efficient Classification\n2018\n\nMohsen Imani moimani@ucsd.edu \nCSE Department\nUC San Diego\nLa Jolla92093CAUSA\n\nChenyu Huang \nCSE Department\nUC San Diego\nLa Jolla92093CAUSA\n\nDeqian Kong dekong@ucsd.edu \nCSE Department\nUC San Diego\nLa Jolla92093CAUSA\n\nTajana Rosing tajana@ucsd.edu \nCSE Department\nUC San Diego\nLa Jolla92093CAUSA\n\nHierarchical Hyperdimensional Computing for Energy Efficient Classification\n\nDAC '18: DAC '18: The 55th Annual Design Automation Conference\n201810.1145/3195970.3196060ACM Reference Format: Mohsen Imani, Chenyu Huang, Deqian Kong, and Tajana Rosing. 2018. Hier-archical Hyperdimensional Computing for Energy Efficient Classification. InCCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learning approachesSupervised learningKEYWORDS Brain-inspired computing, Hyperdimensional computing, Machine learning, Energy efficiency\nBrain-inspired Hyperdimensional (HD) computing emulates cognition tasks by computing with hypervectors rather than traditional numerical values. In HD, an encoder maps inputs to high dimensional vectors (hypervectors) and combines them to generate a model for each existing class. During inference, HD performs the task of reasoning by looking for similarities of the input hypervector and each pre-stored class hypervector However, there is not a unique encoding in HD which can perfectly map inputs to hypervectors. This results in low HD classification accuracy over complex tasks such as speech recognition. In this paper we propose MHD, a multi-encoder hierarchical classifier, which enables HD to take full advantages of multiple encoders without increasing the cost of classification. MHD consists of two HD stages: a main stage and a decider stage. The main stage makes use of multiple classifiers with different encoders to classify a wide range of input data. Each classifier in the main stage can trade between efficiency and accuracy by dynamically varying the hypervectors' dimensions. The decider stage, located before the main stage, learns the difficulty of the input data and selects an encoder within the main stage that will provide the maximum accuracy, while also maximizing the efficiency of the classification task. We test the accuracy/efficiency of the proposed MHD on speech recognition application. Our evaluation shows that MHD can provide a 6.6\u00d7 improvement in energy efficiency and a 6.3\u00d7 speedup, as compared to baseline single level HD.\n\nINTRODUCTION\n\nToday, a large amount of effort is being invested into the design and development of brain inspired computing, which relies on a paradigm of computing starkly different from traditional methods. The recent successes of deep learning networks have contributed Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. DAC '18, June 24-29, 2018 a considerable amount in furthering interest into the bio-inspired areas of computing [1]. Neural Networks (NNs), particularly deep neural networks, have demonstrated high accuracy with regards to many cognitive tasks such as speech recognition, and language/text recognition [2,3]. Although accuracy is an important factor, classification efficiency is becoming more important as many learning algorithms need to be processed on embedded devices with limited resources [4,5]. However, existing classification algorithms such as neural networks, Support Vector Machine (SVM), and k-Nearest Neighbor (k-NN) are computationally expensive [6]. This eliminates the possibility of using these algorithms on resource limited embedded processors.\n\nHyperdimensional (HD) computing is a computational approach, which emulates cognitive tasks by computing with vectors in highdimensional space (hypervectors) as an alternative to traditional deterministic computing with numbers. The idea of HD computing is based on the understanding that brains compute with patterns of neural activity that are not readily associated with numbers. Unlike standard computing architectures, HD uses these patterns to process inputs without using a traditional numerical representation [7]. In fact, raw numerical computing ability of HD is relatively feeble when compared against the results achievable by modern machines.\n\nHD models neural activity patterns using hypervectors with dimensionality in the thousands (e.g., D=10,000). HD computing builds upon a well-defined set of operations between randomly generated hypervectors. These sets of operations create a framework that is extremely robust in the presence of failure, and offers a complete computational paradigm that is easily applied to learning problems [7]. The main difference between HD and traditional computing techniques is the way data is represented, in that HD computing represents the data as approximate patterns which can be scaled very efficiently for a wide array of learning applications. HD consists of an encoder and an associative memory. The encoder block maps input data to high dimensional vectors (hypervectors) and combines them to generate a model for each existing class. During inference, an associative memory performs the task of reasoning by looking at the similarity of the input hypervector to each of the stored model hypervectors. Examples include analogy-based reasoning [8], language recognition [9], text classification [10,11], biosignal processing [12], speech recognition [13], DNA Sequencing [14], and prediction from multimodal sensor fusion [15].\n\nHowever, HD computing provides poor accuracy over complex tasks such as speech recognition, as there is not a unique encoder which could perfectly map inputs to hypervectors while preserving all input information. In this paper, we propose a hierarchical Hyperdimensional computing (MHD) which enables HD to take advantage of multiple encoders without increasing the classification cost. MHD consists of two stages: a main stage and a decider stage. The main stage uses classifiers with different encoders to classify a wide range of input data. Each classifier in the main stage can trade efficiencyaccuracy by dynamically varying the hypervector dimensions. In MHD, the decider stage which is located before the main stage, learns the difficulty of input data and selects an encoder accordingly in the main stage which will provide the maximum accuracy, while also maximizing the efficiency of the classification task. We tested the accuracy/efficiency of the proposed MHD on speech recognition application. Our evaluation shows that MHD can provide a 6.6\u00d7 improvement in energy efficiency and a 6.3\u00d7 speedup, as compared to baseline single level HD.\n\n\nHYPERDIMENSIONAL COMPUTING 2.1 HD Overview\n\nHD computing is a computing paradigm involving long vectors with dimensionality in the thousands called hypervectors [7]. In high-dimensional space, there are several nearly orthogonal hypervectors [16]. HD exploits well-defined vector operations to combine these hypervectors, while also preserving most of the information of the hypervectors. Hypervectors are holographic and (pseudo) random with i.i.d. components and full holistic representation, thus no component has more responsibility to store any piece of information than any other hypervector.\n\n\nClassification Applications\n\nClassification has applications in many different domains. For instance, speech and object recognition are increasingly common in consumer electronics. There are well-defined pre-processing steps to extract the features of different input types. For example, voice techniques such as Mel-frequency cepstral coefficients (MFCCs) [17] extract and map raw voice information into the frequency domain. Regardless of the input type, the extracted features are ussually represented in a single feature vector with N elements. The goal of learning algorithms is to generate a model which can identify the patterns within the feature vectors. Figure 1a shows the overview of the classification in high dimensional space. HD consists of an encoder and an associative memory. For all sample data within a class, HD maps data to high dimensional vectors, called hypervectors, then combines them together to create a single hypervector modeling each class. All trained class hypervectors are stored in an associative memory. During the inference process, the same encoding scheme maps test input data to high dimensional space. Associative memory looks at the similarity of the generated query hypervector against all stored class hypervectors. The input then gets the label of that class with which it has the highest similarity with.\n\nThe goal of HD is to encode the input data (feature vector) to a single hypervector with D dimensions. Our encoder should consider the impact of each feature position and feature value on the final encoded hypervector. Assume each feature can get a value between F min to F max . Our design divides this feature range into m equal levels [F min , F min + \u0394Fm ,..., F max ], where L i is the i th feature level and \u0394F = F max \u2212 F min . Level hypervectors need to have correlation such that the neighbor levels provide higher similarity. We randomly generate a bipolar hypervector of the L 1 with D dimensions (-1 and +1 elements). To consider correlation, we randomly select Dm bits of L 1 and flip them to generate L 2 level hypervector. This procedure continues until generating the hypervector of L m by flipping Dm random bits of the L m\u22121 hypervector. Our design stores the generated level hypervectors in item Memory (iM).\n\n\nHD Encoders\n\nWe proposed two encoders to map vector features to high dimensional space (shown in Figure 1b,c). The difference between these encoders is in the way they consider the impact of each feature position on the final hypervector. In the following, we explain the encoders functionality in detail:\n\nEncoder I: Record-based encoder This encoding scheme assigns a unique channel ID to each feature position. These IDs are hypervectors which are randomly generated such that all features will have orthogonal channel IDs, i.e., \u03b4 ID i , ID j < 5, 000) for D = 10, 000 and i j; where the \u03b4 measures the element-wise similarity between the vectors. These hypervectors are stored in a Channel item Memory(CiM) as shown in Figure 1b. Encoder I looks at each position of the feature vector and elementwise multiplies the channel ID (ID i ) with the corresponding level hypervector (hv i ). The following equation shows how the N feature IDs and hypervectors bind together to generate a single data hypervector in i th class:\nS i 1 = hv i 1 * ID i 1 + hv i 2 * ID i 2 + ... + hv i N * ID i N hv j \u2208 {L 1 , L 2 ,.\n.., L m }, 1 j N Encoder II: Ngram-based encoder Unlike Encoder I, the second encoder differentiates feature positions by assigning a unique permutation for each feature (shown in Figure 1c). For instance, the level hypervector corresponding to n th feature rotationally permutes by n \u2212 1 th positions. The following equation shows how hypervectors are combine different features in input data to generate a single hypervector for an input data in i th class:\nS i 1 = hv i 1 + \u03c1hv i 2 ... + \u03c1 N\u22121 hv i N hv j \u2208 L 1 , L 2 ,..., L m , 1 j N For all input data within a class ({S i 1 , S i 2 ,...,S i K })\n, our design generates hypervectors in a similar fashion and then binds them together to generate a single class hypervector.\nC i = S i 1 + S i 2 + ... + S i K\nAfter training the HD model, we adjust the HD model based on the retraining algorithm proposed in [13]. This retraining happens for pre-specified number of iteration, unless the model accuracy stays the same from one iteration to the next one. At inference, a test input data is encoded to a hypervector using the same encoding used for training (shown in Figure 1a). HD checks the similarity of this query hypervector against all pre-stored classes in an associative memory. The class with the highest Cosine similarity will be selected as the output class.\n\n\nHD CLASSIFICATION ENHANCEMENT 3.1 Multiple Encoders\n\nIn HD, there does not exist a single universal encoder with the ability to map data to high dimensional space while keeping all input information. In fact, each encoder can work properly only for a specific types of input data. To show this, we look at speech recognition problem. For speech recognition, we focus on the Isolet dataset [18] with the goal of recognizing the pre-processed voices among 26 letters of English alphabet. Table 1 shows the HD classification accuracy using Encoder I, Encoder II using hypervectors with 10,000 dimensions. In speech recognition, we observe that Encoder I does a poor job differentiating similar letters such as {T, C, Z, E,...}. However, Encoder II can address these classification issues by using permutation instead of ID hypervectors. On the other hand, Encoder II is exclusive and does not properly classify the voice of less similar letters, while Encoder I does. In order to achieve high classification accuracy, we propose MHD, an adaptive hierarchical hyperdimensional computing design which exploits the advantage of multiple encoders to improve classification accuracy.\n\nIn addition, the result shows the best HD classification accuracy occurs when the HD benefits from both encoders. The results show \n\n\nQuery hypervector\n\nAll samples in class 1  that HD using multiple encoders can achieve 3.4% higher classification accuracy as compared to single encoder. In order to allow MHD to benefit from multiple encoders, our design needs to be adaptive and must select the proper encoder depending on the input data. In Section 4, we explain how our proposed MHD benefits from both encoding schemes at the same time.\n\n\nAll samples in class 2\n\n\nAll samples in class N\n\n\nUnknown\n\n\nDynamic Dimension Reduction\n\nThe energy consumption and execution time of the HD block depend on both encoder and associative memory. For speech recognition (using Encoder I), HD uses a large sized encoder with 617 input channels. This large encoder takes 65% and 47% of total energy consumption and execution time respectively of the HD. Reducing the dimension of the hypervectors used is one of the most effective ways to improve HD efficiency. Figure 2a shows the impact of scaling the hypervector dimensions on the speech recognition accuracy using Encoder I and Encoder II. The results show that both encoders have high robustness to dimension reduction. However, this robustness is much higher for Encoder I. For instance, reducing hypervector dimensions to 8,000, the HD using Encoder I can still provide the same accuracy as HD using the full 10,000 dimensions. Further reducing dimensions to 2,000, Encoder I and Encoder II can provide 88.5% and 78.4% recognition accuracy, respectively. This result indicates that the HD using Encoder I and Encoder II can classify 88% and 78% of data correctly, even while using hypervectors with 2,000 dimensions. This means that we do not need to use costly HD with full 10,000 dimensions for classification. Instead we can classify majority of data using HD with 2,000 dimensions while the HD with 10,000 dimensions can be use to classify the more difficult tasks. Figure 2b,c shows the average energy consumption and execution time of HD when hypervector dimension scales from 10,000 to 2,000. The result shows that HD energy consumption and execution time linearly scales with the hypervector dimensions. For instance, in 2,000 dimensions, HD can achieve a 4.7\u00d7 energy efficiency improvement and a 3.9\u00d7 speedup when compared to HD with 10,000 dimensions. One main advantage of HD is that it does not require a different training model for classifiers with smaller dimensions. HD enjoys full holistic hypervector representation, meaning that no component in hypervector is more representative than others. Therefore, using the HD model with D = 10, 000, we can perform the classification on smaller dimensions of the same model to reduce the classification cost. In other words, we can have the low cost classifiers which are using the same model as Encoder I and Encoder II, but use a part of hypervector dimensions for classification (e.g. 2,000 dimensions). This eliminates the needs to train a separate HD model and also reduces the memory/hardware cost to keep a separated model for low cost classifiers.  \n\n\nADAPTIVE HIERARCHICAL HD 4.1 MHD Overview\n\nIn order to benefit from multiple encoder as well as low cost HD classifier, our design needs to pre-recognize input data and assign them to the proper encoder with minimum dimensions. In this section, we explain the functionality of the proposed hierarchical HD classifier, called MHD, supporting multiple encoders as well as low cost classifiers. In the example explained in Section 3, MHD has four classifiers: two main classifiers using Encoder I and Encoder II with high dimensions, e.g. 10,000 bits, and two low cost classifiers using the same encoders but with lower dimensions, e.g. 2,000 bits. Figure 3 shows the architecture of the MHD design consisting of a Decider and Main stages. Decider stage gets general information from input data by mapping the input hypervector using Encoder I. Then, it accordingly selects classifiers in the main stage which would possibly classify input data correctly. The Decider stage looks at the similarity of a query hypervector to all class hypervectors. All classes with a similarity higher than a decider confidence value (set by user) will be selected as possible target classifiers. However, it is obvious that HD with 10,000 dimensions always outperforms an HD design with 2,000 dimensions (using the same encoder). Thus, the decider HD should select a classifier in the main stage which results in: (i) maximum accuracy as well as (ii) minimum classification cost. For instance, if the decider has high confidence that Encoder I with both 2,000 and 10,000 dimensions can correctly classify as an input data, the decider selects low cost classifier with 2,000 dimensions to perform the classification task.  correctly classify input data. For example, if an input data in main stage could classify by Encoder I with 2,000 and 10,000 dimensions, our design adds input hypervector to the corresponding hypervectors at the decider stage (shown in Figure 3). However, if input data is predicted to wrongly classify by a classifier, no hypervector will be added to corresponding hypervector. This process continues on the training dataset until generating a decider HD with multiple hypervectors, each corresponds a classifier in the main stage.\n\n\nMHD Training & Confidence\n\nDuring the inference stage, when an input data is loaded into the system, decider HD checks its similarity against all stored hypervectors. A class or classes which have the highest Cosine distance similarity to input data (higher than a pre-defined threshold value, T HR) can be considered as classifiers which could correctly classify such data. The Decider HD looks at all the models in the main stage which could possibly classify that particular task and activate a classifier with a lowest dimension. If none of the classes have the confidence level to classify a particular input (i.e., Cosine distance similarity less than a pre-defined threshold over all classes), our design assigns such input data to a class which has the highest similarity. As default, the decider stage encodes inputs using Encoder I. If the decider selects a classifier which uses Encoder I, our design does not need to pay the cost of encoding again. MHD exploits this characteristic to be biased toward a low cost classifiers. Therefore, after the low dimension classifiers, a classifier which uses the same encoder as the decider stage will have second priority. Table 2 shows the impact of the confidence level on the classification accuracy and efficiency of the MHD design using four classifiers in the main stage. The confidence value is a Cosine similarity of an input hypervector with the decider HD hypervectors. As our evaluation shows, reducing a threshold confidence below 95% enables classifiers with lower confidence be assigned for the classification task. This reduces the classification accuracy of the MHD design. However, reducing the confidence level improves the efficiency of the classification. The results in Table 2 show the classification accuracy, the average energy savings and the speedup that MHD can achieve when compared to the HD using Encoder I. As results show, reducing the confidence value to 75% improves the performance and energy efficiency of the MHD by 77.4% while providing about 2.4% lower classification accuracy. Note that these accuracies are still higher than the accuracy that HD using single encoding module provides.\n\n\nDecider Stage Relaxation\n\nRunning the decider HD at the top of the main stage is not always cheap. For MHD with many encoders in the main stage, a decider HD requires a large associative memory to store hypervectors corresponding to each class. This reduces the overall advantages that our design can provide. For the speech recognition example, the main stage with four classifiers requires a decider HD with four hypervectors. In this configuration, the decider adds 426 \u2248 15% energy overhead to classification. As HD with D = 2, 000 takes about 5\u00d7 lower energy than HD with D = 10, 000, our design needs to run at least 18% of inputs on HD with D = 2, 000 in order to compensate the decider HD overhead. Our design uses two approximations to improve the efficiency of the decider HD: (i) Sampling: sampling data features to generate input hypervector. We do not need to use all input features to generate a voice hypervector. Instead, we can use a part of input frequencies to generate input data.\n\n(ii) Dimension reduction: reducing dimensionality of the decider HD could significantly improve the classification efficiency. In Section 5.3, we will explore the impact of the decider HD relaxation on the overall MHD efficiency and accuracy.\n\n\nEXPERIMENTAL RESULTS\n\n\nExperimental Setup\n\nTo estimate the cost of digital design, we use a standard cell-based flow to design dedicated hardware for MHD. We describe the proposed designs using RTL System-Verilog. For the synthesis, we use Synopsys Design Compiler with the TSMC 45 nm technology library with the general purpose process and high V T H cells. We extract its switching activity using ModelSim by applying the test sentences. We measure the power consumption of HD designs using Synopsys PrimeTime at (1 V, 25 \u2022 C, TT) corner. We describe the functionality of the proposed MHD using C++ implementation. We compare the efficiency and accuracy of MHD architectures with state-of-the-art classification techniques running on Intel Core i7 processor with 16 GB memory (4-core, 2.8GHz). For the measurement of CPU power, we use Hioki 3334 power meter. We use this to test the efficiency of the proposed design on speech recognition application, where the goal is to recognize voice audio of the 26 letters of the English alphabet. The training and testing datasets are taken from the Isolet dataset [18]. This dataset consists of 150 subjects speaking each letter of the alphabet twice. The speakers are grouped into sets of 30 speakers. The training of hypervectors is performed on Isolet1,2,3,4, and tested on Isolet 5.\n\n\nMHD Efficiency-Accuracy Trade-off\n\nWe explore the accuracy and efficiency of the MHD when the confidence of the HD decider is changed. Table 3 shows different MHD configurations. The baseline is an HD using a single encoder (Encoder I). The second configuration is an HD with two different encoding schemes, both working with 10,000 dimensions. In the third configuration, our design can select hypervectors with 2,000 dimensions over each encoding. The 6-level (8-level) configuration has two (four) more intermediate classifiers with D = 4, 000 (D = 4, 000 and D = 6, 000) dimensions over both encoding modules.\n\nAs explained before, our design uses the same model as D = 10, 000 for classifiers with lower dimensions. Table 3 shows the MHD model size in different configurations. Our results show that there is a jump in model size, while going from 1-level to 2-level configuration. However, increasing the number of levels does not change the model size significantly. This is because in multi-level design, the size of main classifier is fixed over all configurations (i.e., fixed encoding). Thus, using a larger number of levels only increases the decider HD model size. Table 3 shows the classification accuracy of MHD in different configurations during recognition task. The results are obtained while setting the confidence value to 95% for the decider HD. MHD in 2level configuration can improve the classification accuracy by 3.5%, by adaptively switching between the encoding modules. This improvement is achieved at the cost of adding extra decider block with a 27% energy and a 4% performance penalty. MHD in 4-level configuration does not further improve the classification accuracy because the two new classes simply use the same encoder in different dimensions. However, MHD in this configuration (using classifiers with D = 2, 000 dimensions) can provide 5.7% higher energy efficiency and a 35.4% speedup as compared to single stage HD. Increasing the number of levels to six gives more flexibility to MHD to select a better low cost encoder to classify incoming input data. MHD in this configuration can provide a similar accuracy to the original HD with 42.5% and 52.8% energy efficiency improvement and speedup respectively when compared to baseline HD (using single encoder). Increasing the number of levels to eight does not further improve the classification efficiency because MHD in 8-level configuration requires a large and costly decider HD. Figure 4 shows the impact of the decider confidence on the accuracy classification, energy consumption and speedup of MHD. The energy and speedup are normalized to a 1-level HD. Our result shows that in all the configurations, MHD with higher decider confidence provides higher classification accuracy. Although reducing the confidence level below 95% slightly degrades the classification accuracy, it significantly improves the classification efficiency by assigning a greater portion of tasks to low dimension classifiers, and therefore reducing the effective MHD dimension (shown in Table V). MHD with a larger number of levels has higher robustness to reduction in confidence level. For example, reducing the confidence level from 95% to 75% degrades the classification accuracy of 4-level MHD by 2.4%, while only having a 1.1% impact on accuracy for MHD with 6-level configurations. The higher robustness of 8-level MHD is due to the availability of a greater number of intermediate levels that inputs can be classified to. In terms of efficiency, 4-level MHD shows higher potential for energy savings, as compared to 6-level and 8-level designs. While accepting a 1% quality loss, MHD in 4-level configuration (using 80% confidence) can achieve 8.1\u00d7 energy-delay product (EDP) improvement as compared to 1-level HD. This EDP improvement increases to 27.2\u00d7 and 14.3\u00d7 for MHD in 6-level and 8-level configurations.\n\n\nDecider Relaxation\n\nAs explained before, the decider HD can be an energy bottleneck of MHD design when the number of classifiers in the main stage increases over six. In general, for MHD with a large number of classes, the decider HD takes a considerable part of total MHD energy and execution time. For instance, in 6-level MHD (95% decider confidence), the decider takes in an average of 37% of total MHD energy consumption and 58% of total execution time. Therefore, relaxing the computational complexity of the decider can further improve MHD classification efficiency. However, our design should ensure that the changes have minor impact on MHD accuracy. Figure 5 shows the impact of sampling and dimension reduction on the energy consumption and execution time of MHD. The graph shows the result when MHD accepts 0%, 0.5%, 1%, 1.5% and 2% quality losses when compared to the baseline MHD where the decider is not relaxed (\u0394e = e relaxed \u2212 e baseline ). The top x-axis on Figure 5 shows the best values for sampling and hypervector dimensions which correspond to the \u0394e error reported on bottom x-axis. Our evaluation shows that relaxed MHD can achieve same accuracy as that of the baseline (no sampling and D = 10, 000) when the decider dimension is reduced to 8,000 and samples 70% of the inputs data. In this configuration, MHD can achieve 10.3% energy efficiency improvement and a 6.4% speedup when compared to the baseline. Reducing the sampling rate and dimension of decider HD further improves the MHD efficiency at the cost of lower classification accuracy. For example, while accepting a 2% loss in accuracy, MHD can improve the classification energy and execution time by 6.6\u00d7 and 6.3\u00d7 (2.7\u00d7 and 3.3\u00d7) as compared to single level HD (baseline 6-level MHD). Note that this accuracy is still 0.3% higher than the accuracy for a single stage HD. Relaxing the decider HD (D = 4, 000 dimensions and 30% sampling) and using a decider with 80% confidence improves the energy consumption and execution time of MHD by 9.3\u00d7 and 19.4\u00d7, while providing 94.0% classification accuracy.\n\n\nCONCLUSION\n\nIn this paper we propose a novel hierarchical hyperdimensional (HD) classifier, which enables the classifier to take advantage of multiple encoders without increasing the classification cost. MHD consists of two stages: a main stage and a decider HD. The main stage uses multiple classifiers with different encoders to classify\n\nFigure 1 :Figure 2 :\n12(a) The overview of HD architecture. The functionality of (b) record-based encoder and (c) Ngram-based encoder mapping sample feature vectors to high dimensional space. The impact of hypervector dimensions on (a) speech recognition accuracy, (b) energy consumption and (c) execution time of the HD using different encoding modules.\n\nFigure 3 :\n3The overview of the proposed MHD architecture.\n\n\nMHD trains in two steps: (i) the main stage trains for both encoders in parallel and independently. This training is one-shot learning and gives us two HD models with a full 10,000 dimensions. MHD can use a smaller version of such models for low cost classifiers. (ii) A decider HD trains over whole training dataset by initializing multiple hypervectors, each representing a classifier in the main stage. The number of hypervectors in decider HD depends on the number of available classifiers in the main stage (Four in our example). After training all class hypervectors in main stage, the training in the decider stage starts by initializing all decider hypervectors to zero values. The decide hypervectors train depending on how well the main stage can classify data. Therefore, to find decide model, our design checks the similarity of each input data to all main stage classifiers. Then our design adds such input hypervector to decide vector in decider HD, if the corresponding main stage classifier could\n\nFigure 4 :Figure 5 :\n45Impact of the decider confidence on the speech recognition accuracy and efficiency (normalized to 1-level HD) . Normalized energy consumption and execution time of MHD using sampling and dimension reduction in the decider HD.\n\n\n, San Francisco, CA, USA \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5700-5/18/06. . . $15.00 https://doi.org/10.1145/3195970.3196060\n\nTable 1 :\n1Classification accuracy of speech recognition using different encoding schemes.Encoder I Encoder II \nMHD \n(Both Encoders) \nRecognition Accuracy \n92.5% \n90.9% \n95.9% \n\n\n\nTable 2 :\n2The impact of the decider confidence on the efficiency \nand accuracy of the MHD in 4-level configuration. \n\nDecider Confidence \n75% \n80% \n85% \n90% \n95% \nRecognition Accuracy 93.5% 94.5% 95.2% 95.7% 95.9% \nEnergy saving \n76.4% 72.1% 62.3% 34.1% 5.7% \nSpeedup \n72.5% 70.6% 67.4% 57.8% 35.4% \n\n\n\nTable 3 :\n3Model size and classification accuracy of MHD in different configurations.1-level \n2-level \n4-level \n6-level \n8-level \n\nConfiguration \nEncoder \nEncoding 1 Encoder I, Encoder II Encoder I, Encoder II Encoder I, Encoder II Encoder I, Encoder II \n\nDimensions \n10,000 \n10,000 \n2,000, 10,000 \n2,000, 4,000 \n10,000 \n\n2,000, 4,000 \n6,000, 10,000 \n\nSpeech Recognition \n\nEffective Dimension \n10,000 \n10,000 \n6,000 \n5,400 \n4,600 \nModel Size \n67.5KB \n70KB \n72.5KB \n75KB \n77.5KB \nClassification Accuracy \n93.6% \n95.9% \n95.9% \n95.9% \n95.9% \n\n\na wide range of input data. Each classifier in the main stage can trade efficiency-accuracy by dynamically varying the hypervector dimensions. The decider stage, which is located before the main stage, learns the difficulty of the input data and accordingly selects an encoder in the main stage which provides the maximum accuracy, while maximizing the efficiency of the classification task. We test the accuracy/efficiency of the proposed MHD on speech recognition application. Our evaluation shows that MHD can provide a 6.6\u00d7 improvement in energy efficiency and a 6.3\u00d7 speedup, as compared to baseline single level HD.\nACKNOWLEDGMENTThis work was partially supported by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, and also NSF grants #1730158 and #1527034.\nDeep learning. Y Lecun, Nature. 5217553Y. LeCun et al., \"Deep learning,\" Nature, vol. 521, no. 7553, pp. 436-444, 2015.\n\nDeep speech 2: End-to-end speech recognition in english and mandarin. D Amodei, ICML. D. Amodei et al., \"Deep speech 2: End-to-end speech recognition in english and mandarin,\" in ICML, pp. 173-182, 2016.\n\nfpgaconvnet: A framework for mapping convolutional neural networks on fpgas. S I Venieris, FCCM. IEEES. I. Venieris et al., \"fpgaconvnet: A framework for mapping convolutional neural networks on fpgas,\" in FCCM, pp. 40-47, IEEE, 2016.\n\nScalable-application design for the iot. J Venkatesh, IEEE Software. 341J. Venkatesh et al., \"Scalable-application design for the iot,\" IEEE Software, vol. 34, no. 1, pp. 62-70, 2017.\n\nAdaptive and energy-efficient architectures for machine learning: Challenges, opportunities, and research roadmap. M Shafique, IEEE ISVLSI. IEEEM. Shafique et al., \"Adaptive and energy-efficient architectures for machine learn- ing: Challenges, opportunities, and research roadmap,\" in IEEE ISVLSI, pp. 627- 632, IEEE, 2017.\n\nBig data classification: Problems and challenges in network intrusion prediction with machine learning. S Suthaharan, ACM SIGMETRICS Performance Evaluation Review. 414S. Suthaharan, \"Big data classification: Problems and challenges in network intrusion prediction with machine learning,\" ACM SIGMETRICS Performance Evaluation Review, vol. 41, no. 4, pp. 70-73, 2014.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Com- putation, vol. 1, no. 2, pp. 139-159, 2009.\n\nWhat we mean when we say \"what\u00e2\u0202\u0179s the dollar of mexico?\": Prototypes and mapping in concept space. P Kanerva, AAAI Fall Symposium. P. Kanerva, \"What we mean when we say \"what\u00e2\u0202\u0179s the dollar of mexico?\": Prototypes and mapping in concept space,\" in AAAI Fall Symposium, pp. 2-6, 2010.\n\nLanguage geometry using random indexing. A Joshi, Quantum Interaction 2016 Conference Proceedings. In pressA. Joshi et al., \"Language geometry using random indexing,\" Quantum Interaction 2016 Conference Proceedings, In press.\n\nLow-power sparse hyperdimensional encoder for language recognition. M Imani, IEEE Design & Test. 346M. Imani et al., \"Low-power sparse hyperdimensional encoder for language recognition,\" IEEE Design & Test, vol. 34, no. 6, pp. 94-101, 2017.\n\nExploring hyperdimensional associative memory. M Imani, IEEE HPCA. IEEEM. Imani et al., \"Exploring hyperdimensional associative memory,\" in IEEE HPCA, pp. 445-456, IEEE, 2017.\n\nHyperdimensional biosignal processing: A case study for emgbased hand gesture recognition. A Rahimi, IEEE ICRC. A. Rahimi et al., \"Hyperdimensional biosignal processing: A case study for emg- based hand gesture recognition,\" in IEEE ICRC, October 2016.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, M. Imani et al., \"Voicehd: Hyperdimensional computing for efficient speech recognition,\"\n\nHdna: Energy-efficient dna sequencing using hyperdimensional computing. M Imani, IEEE BHI. M. Imani et al., \"Hdna: Energy-efficient dna sequencing using hyperdimensional computing,\" IEEE BHI, 2018.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, IEEE TNNLS. 99O. Rasanen et al., \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns,\" IEEE TNNLS, vol. PP, no. 99, pp. 1-12, 2015.\n\nEncoding structure in boolean space. P Kanerva, ICANN 98. SpringerP. Kanerva, \"Encoding structure in boolean space,\" in ICANN 98, pp. 387-392, Springer, 1998.\n\nMel frequency cepstral coefficients for music modeling. B Logan, ISMIR. B. Logan et al., \"Mel frequency cepstral coefficients for music modeling.,\" in ISMIR, 2000.\n\nUci machine learning repository. \"Uci machine learning repository.\" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n", "annotations": {"author": "[{\"end\":161,\"start\":83},{\"end\":223,\"start\":162},{\"end\":300,\"start\":224},{\"end\":379,\"start\":301}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":90},{\"end\":174,\"start\":169},{\"end\":235,\"start\":231},{\"end\":314,\"start\":308}]", "author_first_name": "[{\"end\":89,\"start\":83},{\"end\":168,\"start\":162},{\"end\":230,\"start\":224},{\"end\":307,\"start\":301}]", "author_affiliation": "[{\"end\":160,\"start\":114},{\"end\":222,\"start\":176},{\"end\":299,\"start\":253},{\"end\":378,\"start\":332}]", "title": "[{\"end\":76,\"start\":1},{\"end\":455,\"start\":380}]", "venue": "[{\"end\":519,\"start\":457}]", "abstract": "[{\"end\":2469,\"start\":901}]", "bib_ref": "[{\"end\":3345,\"start\":3324},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3435,\"start\":3432},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3625,\"start\":3622},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3627,\"start\":3625},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3819,\"start\":3816},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3821,\"start\":3819},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3985,\"start\":3982},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4608,\"start\":4605},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5141,\"start\":5138},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5792,\"start\":5789},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5818,\"start\":5815},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5844,\"start\":5840},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5847,\"start\":5844},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5874,\"start\":5870},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5899,\"start\":5895},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5920,\"start\":5916},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5971,\"start\":5967},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7293,\"start\":7290},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7375,\"start\":7371},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8091,\"start\":8087},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11991,\"start\":11987},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12843,\"start\":12839},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23569,\"start\":23565}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30467,\"start\":30112},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30527,\"start\":30468},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31542,\"start\":30528},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31792,\"start\":31543},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31948,\"start\":31793},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32128,\"start\":31949},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32432,\"start\":32129},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32974,\"start\":32433}]", "paragraph": "[{\"end\":4085,\"start\":2485},{\"end\":4742,\"start\":4087},{\"end\":5972,\"start\":4744},{\"end\":7126,\"start\":5974},{\"end\":7727,\"start\":7173},{\"end\":9082,\"start\":7759},{\"end\":10011,\"start\":9084},{\"end\":10319,\"start\":10027},{\"end\":11038,\"start\":10321},{\"end\":11585,\"start\":11126},{\"end\":11854,\"start\":11729},{\"end\":12447,\"start\":11889},{\"end\":13625,\"start\":12503},{\"end\":13758,\"start\":13627},{\"end\":14167,\"start\":13780},{\"end\":16789,\"start\":14259},{\"end\":19027,\"start\":16835},{\"end\":21207,\"start\":19057},{\"end\":22210,\"start\":21236},{\"end\":22454,\"start\":22212},{\"end\":23787,\"start\":22500},{\"end\":24403,\"start\":23825},{\"end\":27680,\"start\":24405},{\"end\":29769,\"start\":27703},{\"end\":30111,\"start\":29784}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11125,\"start\":11039},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11728,\"start\":11586},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11888,\"start\":11855}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12943,\"start\":12936},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20212,\"start\":20205},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20780,\"start\":20773},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23932,\"start\":23925},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24518,\"start\":24511},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24975,\"start\":24968},{\"end\":26856,\"start\":26848}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2483,\"start\":2471},{\"attributes\":{\"n\":\"2\"},\"end\":7171,\"start\":7129},{\"attributes\":{\"n\":\"2.2\"},\"end\":7757,\"start\":7730},{\"attributes\":{\"n\":\"2.3\"},\"end\":10025,\"start\":10014},{\"attributes\":{\"n\":\"3\"},\"end\":12501,\"start\":12450},{\"end\":13778,\"start\":13761},{\"end\":14192,\"start\":14170},{\"end\":14217,\"start\":14195},{\"end\":14227,\"start\":14220},{\"attributes\":{\"n\":\"3.2\"},\"end\":14257,\"start\":14230},{\"attributes\":{\"n\":\"4\"},\"end\":16833,\"start\":16792},{\"attributes\":{\"n\":\"4.2\"},\"end\":19055,\"start\":19030},{\"attributes\":{\"n\":\"4.3\"},\"end\":21234,\"start\":21210},{\"attributes\":{\"n\":\"5\"},\"end\":22477,\"start\":22457},{\"attributes\":{\"n\":\"5.1\"},\"end\":22498,\"start\":22480},{\"attributes\":{\"n\":\"5.2\"},\"end\":23823,\"start\":23790},{\"attributes\":{\"n\":\"5.3\"},\"end\":27701,\"start\":27683},{\"attributes\":{\"n\":\"6\"},\"end\":29782,\"start\":29772},{\"end\":30133,\"start\":30113},{\"end\":30479,\"start\":30469},{\"end\":31564,\"start\":31544},{\"end\":31959,\"start\":31950},{\"end\":32139,\"start\":32130},{\"end\":32443,\"start\":32434}]", "table": "[{\"end\":32128,\"start\":32040},{\"end\":32432,\"start\":32141},{\"end\":32974,\"start\":32519}]", "figure_caption": "[{\"end\":30467,\"start\":30136},{\"end\":30527,\"start\":30481},{\"end\":31542,\"start\":30530},{\"end\":31792,\"start\":31567},{\"end\":31948,\"start\":31795},{\"end\":32040,\"start\":31961},{\"end\":32519,\"start\":32445}]", "figure_ref": "[{\"end\":8403,\"start\":8394},{\"end\":10120,\"start\":10111},{\"end\":10747,\"start\":10738},{\"end\":11315,\"start\":11306},{\"end\":12254,\"start\":12245},{\"end\":14686,\"start\":14677},{\"end\":15651,\"start\":15642},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17446,\"start\":17438},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18739,\"start\":18731},{\"end\":26270,\"start\":26262},{\"end\":28351,\"start\":28343},{\"end\":28668,\"start\":28660}]", "bib_author_first_name": "[{\"end\":33777,\"start\":33776},{\"end\":33953,\"start\":33952},{\"end\":34165,\"start\":34164},{\"end\":34167,\"start\":34166},{\"end\":34365,\"start\":34364},{\"end\":34624,\"start\":34623},{\"end\":34939,\"start\":34938},{\"end\":35328,\"start\":35327},{\"end\":35663,\"start\":35662},{\"end\":35890,\"start\":35889},{\"end\":36144,\"start\":36143},{\"end\":36365,\"start\":36364},{\"end\":36586,\"start\":36585},{\"end\":36819,\"start\":36818},{\"end\":36990,\"start\":36989},{\"end\":37239,\"start\":37238},{\"end\":37490,\"start\":37489},{\"end\":37669,\"start\":37668}]", "bib_author_last_name": "[{\"end\":33783,\"start\":33778},{\"end\":33960,\"start\":33954},{\"end\":34176,\"start\":34168},{\"end\":34375,\"start\":34366},{\"end\":34633,\"start\":34625},{\"end\":34950,\"start\":34940},{\"end\":35336,\"start\":35329},{\"end\":35671,\"start\":35664},{\"end\":35896,\"start\":35891},{\"end\":36150,\"start\":36145},{\"end\":36371,\"start\":36366},{\"end\":36593,\"start\":36587},{\"end\":36825,\"start\":36820},{\"end\":36996,\"start\":36991},{\"end\":37247,\"start\":37240},{\"end\":37498,\"start\":37491},{\"end\":37675,\"start\":37670}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1779661},\"end\":33880,\"start\":33761},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11590585},\"end\":34085,\"start\":33882},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6758524},\"end\":34321,\"start\":34087},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6553623},\"end\":34506,\"start\":34323},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":759543},\"end\":34832,\"start\":34508},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":916066},\"end\":35200,\"start\":34834},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":733980},\"end\":35560,\"start\":35202},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7149851},\"end\":35846,\"start\":35562},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":39020350},\"end\":36073,\"start\":35848},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8038292},\"end\":36315,\"start\":36075},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1677864},\"end\":36492,\"start\":36317},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12008695},\"end\":36746,\"start\":36494},{\"attributes\":{\"id\":\"b12\"},\"end\":36915,\"start\":36748},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4708051},\"end\":37114,\"start\":36917},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15258913},\"end\":37450,\"start\":37116},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":57912849},\"end\":37610,\"start\":37452},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17454278},\"end\":37775,\"start\":37612},{\"attributes\":{\"id\":\"b17\"},\"end\":37891,\"start\":37777}]", "bib_title": "[{\"end\":33774,\"start\":33761},{\"end\":33950,\"start\":33882},{\"end\":34162,\"start\":34087},{\"end\":34362,\"start\":34323},{\"end\":34621,\"start\":34508},{\"end\":34936,\"start\":34834},{\"end\":35325,\"start\":35202},{\"end\":35660,\"start\":35562},{\"end\":35887,\"start\":35848},{\"end\":36141,\"start\":36075},{\"end\":36362,\"start\":36317},{\"end\":36583,\"start\":36494},{\"end\":36987,\"start\":36917},{\"end\":37236,\"start\":37116},{\"end\":37487,\"start\":37452},{\"end\":37666,\"start\":37612}]", "bib_author": "[{\"end\":33785,\"start\":33776},{\"end\":33962,\"start\":33952},{\"end\":34178,\"start\":34164},{\"end\":34377,\"start\":34364},{\"end\":34635,\"start\":34623},{\"end\":34952,\"start\":34938},{\"end\":35338,\"start\":35327},{\"end\":35673,\"start\":35662},{\"end\":35898,\"start\":35889},{\"end\":36152,\"start\":36143},{\"end\":36373,\"start\":36364},{\"end\":36595,\"start\":36585},{\"end\":36827,\"start\":36818},{\"end\":36998,\"start\":36989},{\"end\":37249,\"start\":37238},{\"end\":37500,\"start\":37489},{\"end\":37677,\"start\":37668}]", "bib_venue": "[{\"end\":33791,\"start\":33785},{\"end\":33966,\"start\":33962},{\"end\":34182,\"start\":34178},{\"end\":34390,\"start\":34377},{\"end\":34646,\"start\":34635},{\"end\":34996,\"start\":34952},{\"end\":35359,\"start\":35338},{\"end\":35692,\"start\":35673},{\"end\":35945,\"start\":35898},{\"end\":36170,\"start\":36152},{\"end\":36382,\"start\":36373},{\"end\":36604,\"start\":36595},{\"end\":36816,\"start\":36748},{\"end\":37006,\"start\":36998},{\"end\":37259,\"start\":37249},{\"end\":37508,\"start\":37500},{\"end\":37682,\"start\":37677},{\"end\":37808,\"start\":37777}]"}}}, "year": 2023, "month": 12, "day": 17}