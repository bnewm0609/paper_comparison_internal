{"id": 238744475, "updated": "2023-10-05 21:38:55.508", "metadata": {"title": "Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance", "authors": "[{\"first\":\"Shibal\",\"last\":\"Ibrahim\",\"middle\":[]},{\"first\":\"Natalia\",\"last\":\"Ponomareva\",\"middle\":[]},{\"first\":\"Rahul\",\"last\":\"Mazumder\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -- a common baseline for newer metrics -- and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is $3\\times$-10$\\times$ faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalance ratios etc. for some recent metrics e.g., NCE, LEEP that resulted in them being misrepresented as leading measures. We propose a correction and recommend measuring correlation performance against relative accuracy in such settings. We support our findings with ~164,000 (fine-tuning trials) experiments on both vision models and graph neural networks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2110.06893", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/pkdd/IbrahimPM22", "doi": "10.1007/978-3-031-26387-3_42"}}, "content": {"source": {"pdf_hash": "0bf1e3ee521c5c5ea3a91819273b7574561f246b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2110.06893v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2110.06893", "status": "GREEN"}}, "grobid": {"id": "410f900ed933db6259c9c1f970e14d5be4dc9e41", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0bf1e3ee521c5c5ea3a91819273b7574561f246b.txt", "contents": "\n\n\n\nIbrahim Shibal shibal@mit.edu \nNatalia 1\u22c6 [ ] \nMassachusetts Institute of Technology\nCambridgeMAUSA\n\nPonomareva nponomareva@google.com \nGoogle Research\nNew York, NYUSA\n\nRahul Mazumder rahulmaz@mit.edu \nMassachusetts Institute of Technology\nCambridgeMAUSA\nNewer is not always better: Rethinking transferability metrics, their peculiarities, stability and performanceTransferability metrics \u00b7 Fine-tuning \u00b7 Transfer learning \u00b7 Domain adaptation \u00b7 Neural networks\nFine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -a common baseline for newer metrics -and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is 3 \u2212 10 times faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalance ratios etc. for some recent metrics e.g., NCE, LEEP that resulted in them being misrepresented as leading measures. We propose a correction and recommend measuring correlation performance against relative accuracy in such settings. We support our findings with \u223c 164,000 (fine-tuning trials) experiments on both vision models and graph neural networks.\n\nIntroduction\n\nTransfer learning is a set of techniques of using abundant somewhat related source data p(X (s) , Y (s) ) to ensure that a model can generalize well to the target domain, defined as either little amount of labelled data p(X (t) , Y (t) ) (supervised), and/or a lot of unlabelled data p(X (t) ) (unsupervised transfer learning). Transfer learning is most commonly achieved either via fine-tuning or co-training. Finetuning is a process of adapting a model trained on source data by using target \u22c6 This work was completed as an Intern and Student Researcher at Google. arXiv:2110.06893v3 [cs.LG] 26 May 2023 data to do several optimization steps (for example, stochastic gradient descent) that update the model parameters. Co-training on source and target data usually involves reweighting the instances in some way or enforcing domain irrelevance on feature representation layer, such that the model trained on such combined data works well on target data. Fine-tuning is becoming increasing popular because large models like ResNet50 [11], BERT [6] etc. are released by companies and are easily modifiable. Training such large models from scratch is often prohibitively expensive for the end user.\n\nIn this paper, we are primarily interested in effectively measuring transferability before training of the final model begins. Given a source data/model, a transferability measure quantifies how much knowledge of source domain/model is transferable to the target model. Transferability measures are important for various reasons: they allow understanding of relationships between different learning tasks, selection of highly transferable tasks for joint training on source and target domains, selection of optimal pre-trained source models for the relevant target task, prevention of trial procedures attempting to transfer from each source domain and optimal policy learning in reinforcement learning scenarios (e.g. optimal selection of next task to learn by a robot). If a measure is capable of efficiently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning is greatly simplified by using the measure to search over candidate transfer sources and targets. Contributions Our contributions are three-fold:\n\n1. We show that H-score, commonly used as a baseline for newer transferability measures, suffers from instability due to poor estimation of covariance matrices. We propose shrinkage-based estimation of H-score with regularized covariance estimation techniques from statistical literature. We show 80% absolute increase over the original H-score and show superior performance in majority cases against all newer transferability measures across various fine-tuning scenarios. 2. We present a fast implementation of our estimator that is 3 \u2212 10 times faster than state-of-the-art LogME measure. 3. We identify problems with 3 other transferability measures (NCE, LEEP and N LEEP) in target task selection when either the number of target classes or the class imbalance varies across candidate target tasks. We propose measuring correlation against relative target accuracy (instead of vanilla accuracy) in such scenarios.\n\nOur large set of \u223c 164, 000 fine-tuning experiments with vision models and graph convolutional networks on real-world datasets shows usefulness of our proposals. This paper is organized as follows. Section 2 describes general fine-tuning regimes and transfer learning tasks. Section 3 discusses transferability measures. Section 4 addresses shortcomings of the pioneer transferability measure (H-Score) that arise due to unreliable estimation and proposes a new shrinkagebased estimator for the H-Score. In Section 5, we demonstrate problems with recent NCE, LEEP and N LEEP metrics and propose a way to address them. Finally, Section 6 presents a meta study of all metrics.\n\n\nTransferability setup\n\nWe consider the following fine-tuning scenarios based on existing literature. (i) Source Model Selection (SMS): For a particular target data/task, this regime aims to select the \"optimal\" source model (or data) to transfer-learn from, from a collection of candidate models/data. (ii) Target Task Selection (TTS): For a particular (source) model, this regime aims to find the most related target data/task.\n\nIn addition, we primarily consider two different fine-tuning strategies: (i) Linear fine-tuning/head only fine-tuning (LFT): All layers except for the penultimate layer are frozen. Only the weights of the head classifier are retrained while fine-tuning.\n\n(ii) Non-linear fine-tuning (NLFT): Any layer can be designated as a feature extractor, up to which all the layers are frozen; the subsequent layers include nonlinear transformations and are re-trained along with the head on target data.\n\n\nRelated Work\n\nRecent literature in transfer learning has proposed efficient transferability measures. Inspired by principles in information theory, Negative Conditional Entropy (NCE) [31] uses pre-trained source model and evaluates conditional entropy between target pseudo labels (source models' assigned labels) and real target labels. Log Expected Empirical Predictor (LEEP) [21] modifies NCE by using soft predictions from the source model. Both NCE and LEEP do not directly use feature information, hence they are not applicable for layer selection. The authors in [4] propose representing each output class by the mean of all images from that class and computing Earth Mover's distance between the centroids of the source classes and target classes.\n\nOther works [1,18,12,33,5] proposed metrics that capture information from both the (learnt) feature representations and the real target labels. These metrics are more appealing as these can be broadly applicable for models that are pre-trained in either supervised or unsupervised fashion. They are also applicable for embedding layer selection. The authors in [18] proposed N LEEP that fits a Gaussian mixture model on the target feature embeddings and computes the LEEP score between the probabalistic assignment of target features to different clusters and the target labels. The authors in [12] introduced TransRatea computationally-friendly surrogate of mutual information (using coding rate) between the target feature embeddings and the target labels. H-score was proposed by [1] that takes into account inter-class feature variance and feature redundancy. [33] proposed LogME that considers an optimization problem rooted in Bayesian statistics to maximize the marginal likelihood under a linear classifier head. [5] introduced LFC to measure in-class similarity of target feature embeddings across samples.\n\nFinally, the authors in [30] used Optimal Transport to evaluate domain distance, and combined it, via a linear combination, with NCE. To learn such a measure, a portion of target tasks were set aside, the models were transferred onto these tasks and the results were used to learn the coefficients for the combined Optimal Transport based Conditional Entropy (OTCE) metric. While the resulting metric appears to be superior over other non-composite metrics like H-score and LEEP, it is expensive to compute since it requires finding the appropriate coefficients for the combination.\n\n4 Improved estimation of H-score H-score [1] is one of the pioneer measures that is often used as a baseline for newer transferability measures, which often demonstrate the improved performance. It characterizes discriminatory strength of feature embedding for classification:\nH(f ) = tr(\u03a3 f \u22121 \u03a3 z ) (1) where, d is the embedding dimension, f i = h(x (t) i ) \u2208 R d is the target feature embeddings when the feature extractor (h : R p \u2192 R d ) from the source model is applied to the target sample x (t) i \u2208 R p , F \u2208 R nt\u00d7d denotes the corresponding target feature matrix, Y = Y (t) \u2208 Y = {1, \u00b7 \u00b7 \u00b7 , C} are the target data labels, \u03a3 f \u2208 R d\u00d7d denotes the sample feature covariance matrix of f , z = E [f |Y ] \u2208 R d\nand Z \u2208 R nt\u00d7d denotes the corresponding target-conditioned feature matrix, \u03a3 z \u2208 R d\u00d7d denotes the sample covariance matrix of z. Intuitively, H(f ) captures the notion that higher inter-class variance and small feature redundancy results in better transferability.  We hypothesize that the sub-optimal performance of H-Score (compared to that of more recent metrics) for measuring transferability in many of the evaluation cases, e.g., in [21], is due to lack of robust estimation of H-Score. We empirically validate this hypothesis using a synthetic classification data. We generated 1 million 1000-dimensional features with 10 classes using Sklearn multi-class dataset generation function [22]. Number of informative features is set to 500 with rest filled with random noise. We visualize the original and the population version of the H-score for different sample sizes in Fig. 1. We observe that the original H-Score becomes highly unreliable as the number of samples decreases.\n\nMany of the deep learning models in the context of transfer learning have high-dimensional feature embedding space -typically larger than the number of target samples. Consequently, the estimation of the two covariance matrices in H-score becomes challenging: the sample covariance matrix of the feature embedding has a large condition number 3 in small data regimes. In many cases, it cannot even be inverted. [1] used a pseudo-inverse of the covariance matrix \u03a3 f . However, this method of estimating a precision matrix can be sub-optimal as inversion can amplify estimation error [16]. We propose to use well-conditioned shrinkage estimators motivated by the rich literature in statistics on the estimation of highdimensional covariance (and precision) matrices [23]. We show that the use of such shrinkage estimators can offer significant gain in the performance of H-score in predicting transferability. In many cases, as our experiments show, the gain is so significant that H-score becomes a leading transferability measure, surpassing the performance of state-of-the-art measures.\n\n\nProposed Transferability Measure\n\nWe propose the following shrinkage based H-score:\nH \u03b1 (f ) = tr \u03a3 f \u22121 \u03b1 \u00b7 (1 \u2212 \u03b1)\u03a3 z ,(2)\nEstimating \u03a3 (f ) \u03b1 While there are several possibilities to obtain a regularized covariance matrix [23], we present an approach that considers a linear operation on the eigenvalues of the sample version of the feature embedding covariance matrix. Similar ideas of using well-conditioned plug-in covariance matrices are used in the context of discriminant analysis [10]. In particular, we improve the conditioning of the covariance matrix by considering its weighted convex combination with a scalar multiple of the identity matrix:\n\u03a3 f \u03b1 = (1 \u2212 \u03b1)\u03a3 f + \u03b1\u03c3I d (3)\nwhere \u03b1 \u2208 [0, 1] is the shrinkage parameter and \u03c3 is the average variance computed as tr(\u03a3 f )/d. The linear operation on the eigenvalues ensures the covariance estimator is positive definite. Note that the inverse of \u03a3 f \u03b1 can be computed for every \u03b1, by using the eigen-decomposition of \u03a3 f . The shrinkage parameter controls the bias and variance trade-off; the optimal \u03b1 needs to be selected. This distribution-free estimator is well-suited for our application as the explicit convex linear combination is easy to compute and makes the covariance estimates well-conditioned and more accurate [16,2,28]. Understanding (1 \u2212 \u03b1)\u03a3 z The scaling factor (1 \u2212 \u03b1) can be understood in terms of regularized covariance matrix estimation under a ridge penalty:\n1/(1 + \u03bb) \u00b7 \u03a3 z = argmin\u03a3 ||\u03a3 \u2212 \u03a3 z || 2 2 + \u03bb||\u03a3|| 2 2(4)\nwhere \u03bb \u2265 0 is the ridge penalty. Choosing \u03bb = \u03b1/(1 \u2212 \u03b1), it becomes clear that (1 \u2212 \u03b1)\u03a3 z is the regularized covariance matrix. Choice of \u03b1 [16] proposed a covariance matrix estimator that minimizes mean squared error loss between the shrinkage based covariance estimator and the true covariance matrix. The optimization considers the following objective:\nmin \u03b1,v E[||\u03a3 * \u2212 \u03a3|| 2 ] s.t. \u03a3 * = (1 \u2212 \u03b1)\u03a3 f + \u03b1vI, E[\u03a3 f ] = \u03a3. (5)\nwhere \u2225A\u2225 2 def = tr(AA T )/d for a matrix A \u2208 R d\u00d7d . This optimization problem permits a closed-form solution for the optimal shrinkage parameter, which is given by:\n\u03b1 * = E[||\u03a3 f \u2212 \u03a3|| 2 ] E[||\u03a3 f \u2212 (tr(\u03a3)/d) \u00b7 I d || 2 ](6)\u2243 min{(1/n 2 t ) i\u2208[nt] ||f i f T i \u2212 \u03a3 f || 2 ||\u03a3 f \u2212 (tr(\u03a3 f )/d) \u00b7 I d || 2 , 1}.(7)\nwhere equation 7 defines a valid estimator (not dependent on true covariance matrix) for practical use. For proof, we refer the readers to Section 2.1 and 3.3 in [16]. Following the synthetic motivational example presented earlier showing the unreliability of the original H-Score, we investigate the reliability of shrinkagebased H-Score. We visualize the shrinkage-based H-Score in Fig. 2[Left]. We observe that the original H-Score becomes highly unreliable as the number of samples decreases. In contrast, the shrunken estimation of H-Score is highly stable and has a small error when compared with the population H-Score. Hence, shrinkage-based H-score seems to be a much better estimator of the \"true\" Hscore in contrast to the empirical H-Score. We further visualize the effect of using non-optimal values of \u03b1 on the shrinkage-based H-Score in Fig. 2[Right]. We can see that the shrinkage-based H-Score with optimal shrinkage \u03b1 * is much closer to the population version of the original H-Score, especially for smaller sample cases. This validates the use of \u03b1 * as computed in equation 7. Additional Discussion on same shrinkage \u03b1 for the two covariances in shrinkage-based H-Score The covariance \u03a3 z can not be shrunk independently of \u03a3 f in the estimation of H \u03b1 (f )-the two covariances are coupled by the law of total covariance:\n\u03a3 f = E[\u03a3 f Y ] + \u03a3 z .(8)\nwhere f Y denotes the feature embedding of target samples belonging to class Y \u2208 Y and \u03a3 f Y = Cov(f |Y ) denotes the class-conditioned covariances. We write\n(1 \u2212 \u03b1)\u03a3 f = (1 \u2212 \u03b1)E[\u03a3 f Y ] + (1 \u2212 \u03b1)\u03a3 z , (1 \u2212 \u03b1)\u03a3 f + \u03b1 tr(\u03a3 f ) d I d = (1 \u2212 \u03b1)E[\u03a3 f Y ] + \u03b1 tr(\u03a3 f ) d I d + (1 \u2212 \u03b1)\u03a3 z ,(9)\ni.e,\n\u03a3 f \u03b1 = (1 \u2212 \u03b1)E[\u03a3 f Y ] + \u03b1 tr(\u03a3 f ) d I d + (1 \u2212 \u03b1)\u03a3 z . (10)\nComparing equations 10 with 8, we see that the same shrinkage parameter \u03b1 should be used when using shrinkage estimators, to preserve law of total covariance. The first two terms on the right side in equation 10 can be understood as shrinkage of class-conditioned covariances to the average (global) variance. The third term in equation 10 (e.g. (1 \u2212 \u03b1)\u03a3 z ) can then be understood as ridge shrinkage as in equation 4.\n\n\nChallenges of comparing H \u03b1 (f ) across source models/layers\n\nNext, we discuss challenges of using H \u03b1 (f ) on the feature embeddings of target data derived from the source model for source model selection. The feature dimension (d) across different source models even for the penultimate layer can vary significantly e.g. from 1024 in MobileNet to 4096 in VGG19. Such differences makes source model/layer selection for fine-tuning highly problematic. We propose dimensionality reduction of feature embeddings before computing H \u03b1 (f ). We project feature embeddings to a lower q-dimensional space, where q is taken to be the same across the K candidate models/layers and satisfies: q \u2264 min f (1) ,f (2) ,\u00b7\u00b7\u00b7 ,f (K) |f (.) | where |.| operator denotes the cardinality of the feature spaces. The dimensionality reduction allows for more meaningful comparison of H \u03b1 (f ) across source/target pairs; this is relevant for source/layer selection. More generally, it also allows for faster and more robust estimate for limited target samples case (n t < d) for linear and nonlinear fine-tuning. In the case of nonlinear fine-tuning, the intermediate layers of visual and language models have really large d \u223c 10 5 , see Table S1 in Supplement for examples.\n\nWe consider Gaussian Random Projection, which uses a linear transformation matrix V to derive the transformed featuresF = F V ; it samples components from N (0, 1 q ) to preserve pairwise distances between any two samples of the dataset. Untrained auto-encoders (AE) are other alternatives that have been used to detect covariate shifts in input distributions by [24]. It is not known how sensitive these untrained AE are to the underlying architecture-using trained AE is less appropriate for use in transferability measurement for fine-tuning as those maybe more time-consuming than the actual fine-tuning. We demonstrate improved correlation performance of H \u03b1 (f ) with dimensionality reduction in Table 3 in Section 6.1 for source model selection.\n\n\nEfficient Computation for small target data\n\nFor small target data (C \u2264 n t < d), the naive implementation of H \u03b1 (f ) can be very slow. We propose an optimized implementation for our shrinkage-based H-Score that exploits diagonal plus low-rank structure of \u03a3 (f ) \u03b1 for efficient matrix inversion and the low-rank structure of \u03a3 (z) for faster matrix-matrix multiplications. We assume F (and correspondingly Z) are centered. The optimized computation of H \u03b1 (f ) is given by:\nH \u03b1 (f ) = (1 \u2212 \u03b1)/(n t \u03b1\u03c3) \u00b7 \u2225R\u2225 2 F \u2212 (1 \u2212 \u03b1) \u00b7 vec (G) T vec W \u22121 G , (11) where R = \u221a n 1fY =1 , \u00b7 \u00b7 \u00b7 , \u221a n CfY =C \u2208 R d\u00d7C , G = F R \u2208 R nt\u00d7C , W = n t \u03b1\u03c3I n + (1 \u2212 \u03b1)F F T \u2208 R nt\u00d7nt .\nThe algorithm (and derivation) is provided in the Supplementary document. We make a timing comparison of our optimized implementation of H \u03b1 (f ) against the computational times of the state-of-the-art LogME measure in Section 6.3.\n\n\nA closer look at NCE, LEEP and N LEEP measures\n\nNext, we pursue a deeper investigation of some of the newer metrics that are reported to be superior to H-Score and bring to light what appears to be some overlooked issues with these metrics in target task selection scenario. Target task selection has received less attention than source model selection. To our knowledge, we are the first to bring to light some problematic aspects with NCE, LEEP and N LEEP, which can potentially lead to the misuse of these metrics in measuring transferability. These measures are sensitive to the number of target classes (C) and tend to be smaller when C is larger (see Fig. 3[Left]). Therefore, use of these measures for target tasks with different C will most likely result in selecting the task with a smaller C. However, in practice, it is not always the case that transferring to a task with a smaller C is easier; for example, reframing a multiclass classification into a set of binary tasks can create more difficult to learn boundaries [8]. Furthermore, the measures are also problematic if two candidate target tasks have different degree of imbalance in their classes even if C is the same. The measures would predict higher transferability for imbalanced data regimes over balanced settings (see Fig. 3[Right]). However, imbalanced datasets are typically harder to learn. If these measures are correlated against vanilla accuracy, which tends to be higher as the imbalance increases e.g. for binary classification, the measures would falsely suggest they are good indicators of performance. Earlier work did not consider both these aspects and erroneously showed good correlation of these metrics against vanilla accuracy to show dominance of these metrics in target task selection with different C [21,30] and imbalance [30].\n\nHere, we propose a method to ameliorate the shortcomings of NCE, LEEP and N LEEP to prevent misuse of these measures, so that they lead to more reliable conclusions. We propose to standardize the metrics by the entropy of the target label priors, leading to the definitions in equation 12. This standardization considers both the class imbalance as well as number of classes through the entropy of the target label priors.\nn-NCE def = 1 + NCE/H(Y ), n-LEEP def = 1 + LEEP/H(Y ),(12)n-N LEEP def = 1 + N LEEP/H(Y ).\nOur proposed normalizations in equation 12 ensures the normalized NCE is bounded between [0, 1]. For proof, see Supplementary document. n-NCE is in fact equivalent to normalized mutual information and has been extensively used to measure correlation between two different labelings/clustering of samples [32].\n\nGiven the similar behavior of LEEP and NCE to different C and class imbalance as shown in Fig. 3, we suggest the same normalization as given in equation 12. However, this normalization does not ensure boundedness of n-LEEP score (and by extension n-N LEEP) in the range [0, 1] as in the case of n-NCE. For scenarios where candidate target tasks have different C, we propose an alternative evaluation criteria (relative accuracy) instead of vanilla accuracysee Section 6 for more details. We provide empirical validation of the proposed normalization to these measures in Table 2 in Section 6.1. We also show that our proposed shrinkage-based H-Score is the leading metric even in these scenarios.\n\n\nExperiments\n\nWe evaluate existing transferability measures and our proposed modifications on two class of models: vision models and graph neural networks. We study various fine-tuning regimes and data settings. We draw inspiration from [21] who consider target task selection and source model selection. The experimental setup highlights important aspects of transferability measures, e.g., dataset size for computing empirical distributions and covariance matrices, number of target classes, and feature dimension etc. Some of these aspects have been overlooked when evaluating transferability measures, leading to improper conclusions. Evaluation criteria Transferability measures are often evaluated by how well they correlate with the test accuracy after fine-tuning the model on target data. Following [31,21,12], we used Pearson correlation. We include additional results with respect to rank correlations (e.g., Spearman) in Supplementary document. We argue that considering correlation with the target test accuracy is flawed in some scenarios. In particular, for target task selection, it is wrong to compare target tasks based on accuracy when C is different e.g 5 vs 10 classes. In such a case, task with 10 classes will have a high chance of arriving at lesser test accuracy compared to that for task with 5 classes. In this case, it is more appropriate to consider the gain in accuracy achieved by the model over it's random baseline. Hence we use relative accuracy (for balanced classes): Accuracy\u22121/C\n\n\n1/C\n\n. This measure is more effective in capturing the performance gain achieved by the same model in transferring to two domains with different C. This also highlights the limitation of NCE, LEEP and N LEEP which are sensitive to C and tend to have smaller values with higher C; these measures do not provide useful information about how hard these different tasks when evaluated with vanilla accuracy.\n\nCorrelations marked with asterisks (*) in Tables 1, 2, 3, 4 are not statistically significant (p-value > 0.05). Larger correlations indicate better identifiability as quantified by transferability measure. Hyphen (-) indicates the computation ran out of memory or was really slow.\n\n\nCase Study: Vision Models\n\nFirst, we evaluate our proposals on visual classification tasks with vision models e.g., VGG19 [29], ResNet50 [11] that have been pre-trained on ImageNet. We fine-tune on subsets of CIFAR-100/CIFAR-10 data. We use Tensorflow Keras [3] for our implementation. Imagenet checkpoints come from Keras 4 . Fine-tuning with hyperparameter optimization The optimal choice of hyperparameters for fine-tuning is not only target data dependent but also sensitive to the domain similarity between the source and target datasets [17]. We thus tune the hyperparameters for fine-tuning: we use Adam optimizer and tune batch size, learning rate, number of epochs and weight decay (L2 regularization on the classifier head). For validation tuning, we set aside a portion of the training data (20%) and try 100 random draws from hyperparameters' multi-dimensional grid. With this additional tuning complexity, we performed 650 \u00d7 100 fine-tuning experiments. See additional information and motivation in Supplement.\n\n\nTarget Task Selection\n\nWe first investigate the correlation performance of our proposed estimator and existing transferability measures in the context of target task selection. We consider two small target data cases. We provide a summary of the different cases (inspired from [21]) below. Additional details are in the Supplementary document.  This makes for a binary imbalanced classification task. We again measure performance of transferability measures against optimal target test accuracy. Note the imbalance is constant across the candidate target tasks.\n\nWe evaluate target task selection for linear and nonlinear fine-tuning under small sample setting. The layers designated as embedding layers for nonlinear fine-tuning of VGG19 and ResNet50 is given in Table S1 in Supplementary document. We empirically compare the shrinkage-based H-score against the original measure by [1] and the state-of-the-art measures. Table 1 demonstrates 80% absolute gains in correlation performance of H \u03b1 (f ) over H(f ), making it a leading metric in many cases in small target data regimes.\n\nNext, we study a large-balanced target data setting where the number of classes varies across the target tasks. We construct the target tasks as follows: We randomly select 2-100 classes from CIFAR-100 and include all samples from the chosen classes. This constructs a collection of large balanced target tasks with different number of classes (L-B-C). We generate 50 such target datasets. In this setting, we evaluate correlation of transferability measures with relative  Source Model Selection Next, we study source model selection scenario for vision models. We select 9 small to large (pre-trained ImageNet) vision models: VGG19, ResNet50, ResNet101, DenseNet121, DenseNet201, Xception, Incep-tionV3, MobileNet, EfficientNetB0. We evaluate source model selection for linear and nonlinear fine-tuning under small sample setting. The layers designated as embedding layers for nonlinear fine-tuning of all 9 models is shown in Table  S1 in Supplementary document. We sample 50 images per class from all classes available in the original train split of CIFAR-100/CIFAR-10. We designate 10 samples per class for hyperparameter tuning. We demonstrate that H \u03b1 (f ) is a leading metric in source model selection as well. Given that the feature dimensions vary significantly across different models in source model selection for both linear and nonlinear finetuning, we apply proposed dimensionality reduction via random projection in Section 4.2 for H \u03b1 (f ) to project feature embeddings to 128-dimensional space (q = 128). This allows for more meaningful comparison of H-score across source models. This leads to the gains of proposed H \u03b1 (f ) in terms of correlation in the context of source model selection as well for small samples as given in Table 3, making it again a leading metric in source model selection.\n\n\nCase Study: Graph Neural Networks\n\nWe evaluate our proposals on Graph Neural Networks on Twitch Social Networks [26,25,27]. The datasets are social networks of gamers from the streaming service Twitch, where nodes correspond to Twitch users and links correspond to mutual friendships. There are country-specific sub-networks. We consider six sub-networks: {DE, ES, FR, RU, PTBR, ENGB}. Features describe the history of games played and the associated task is binary classification of whether a gamer streams adult content. The country specific graphs share the same node features which means that we can perform transfer learning with these datasets.\n\nAdditional details about the Twitch Social Networks datasets are included in Supplementary document. Transferability Setup We consider a two-layered Graph Convolutional Network (GCN) [13]. The network takes the following functional form:\nlogit(x) =\u011c \u00b7 Dropout(ReLU(\u011c \u00b7 x \u00b7 W 1 )) \u00b7 W 2 ,(13)\nwhere\u011c =D 1/2 (G + I)D 1/2 denotes the renormalization trick from [13] when applied to the graph adjacency matrix G \u2208 R m\u00d7m ,D ii = j (G + I m ) ij denotes the degree of node i, W 1 \u2208 R p\u00d7d and W 2 \u2208 R d\u00d7C denote the learnable weights for the first and second layer respectively. The mapping from logits to Y can be done by applying a softmax and returning the class with the highest probability.\n\nFor studying transferability, we consider the target feature embeddings as:\nF = h(X (t) ) =\u011c \u00b7 Dropout(ReLU(\u011c \u00b7 X (t) \u00b7 W 1 )\n). This creates a linear transfer learning strategy, which is similar to the linear fine-tuning regime studied for vision models in section 6.1. We study target task selection in Section 6.2 and source model selection in Supplementary document. Pre-training Implementation We use PyTorch Geometric [7] to setup the pre-training of GCN models. The pre-training uses a country-specific subnetwork and performs training, model selection and testing via a 64%/16%/20% split. The training considers transductive learning in graph networks. We perform 200 hyperparameter trials that tune over Adam learning rates [10 \u22125 , 10 \u22121 ], batch sizes {16, 32, 64}, L2 regularization [10 \u22124 , 1], dropout of 0.5 and maximum 1000 epochs with early stopping with a patience of 50. Fine-tuning For fine-tuning experiments, we recover the target feature embeddings by applying optimal pre-trained source model on a different subnetwork of users. Given we study linear fine-tuning in Graph Networks, we use Grid-SearchCV with \u2113 2 -regularized Logistic Regression from sklearn [22] on (only) target train data to perform (stratified) 5-fold cross-validation. We consider 100 values for L2 regularization in the range [10 \u22125 , 10 3 ] on the log scale. The optimal L2 regularization is used to get the optimal model and the test accuracy is computed to measure correlation of transferability measures. We study balanced targets in this regime. Given that the degree of imbalance varies significantly across different country-specific networks, we collect the largest balanced datasets for each target. Next, we sample n t = 1000 nodes for fine-tuning and allocate the remaining nodes as test samples. We consider two different embedding sizes for GCN network in this study. We also validate our proposals when we allocate 500 samples for fine-tuning. We present the Pearson correlation performance of transferability measures against fine-tuned target test accuracy in Table 4. The correlations demonstrate our proposed H \u03b1 (f ) as the leading metric for target task selection for linear fine-tuning in Graph Neural Networks. We include additional results for source model selection in Supplementary document. We empirically investigate the computational times of H \u03b1 (f ) when computed via our optimized implementation in equation 11. For this exercise, we generate synthetic multi-class classification data using Sklearn [22] multi-class dataset generation function that is adapted from [9]. We investigate different values for number of samples (n t ), feature dimension (d) and number of classes (C). For data generation, we set number of informative features to be 100 with the rest of the features filled with random noise. For LogME, we use a faster variant proposed by [34]. For a fair comparison, we do not use any dimensionality reduction in the computation of H \u03b1 (f ). Table 5 demonstrates a significant computational advantage of H \u03b1 (f ) over LogME. We observe 3 \u2212 10 times faster computational times.\n\n\nTarget Task Selection\n\n\nTiming comparison between LogME and H \u03b1 (f )\n\n\nConclusion\n\nWe study transferability measures in the context of fine-tuning. Our contributions are three-fold. First, we show that H-score measure, commonly used as a baseline for newer transferability measures, suffers from instability due to poor estimation of covariance matrices. We propose shrinkage-based estimation of H-score with regularized covariance estimation techniques from statistical literature. We show 80% absolute increase over the original H-score and show superior performance in many cases against all newer transferability measures across various model types, fine-tuning scenarios and data settings. Second, we present a fast implementation of our estimator that provides a 3 \u2212 10 times computational advantage over state-of-the-art LogME measure. Third, we identify problems with 3 other transferability measures (NCE, LEEP and N LEEP) in target task selection (an understudied fine-tuning scenario than source model selection) when either the number of target classes or the class imbalance varies across candidate target tasks. We propose an alternative evaluation scheme that measures correlation against relative target accuracy (instead of vanilla accuracy) in such scenarios. Our large set of \u223c 164, 000 fine-tuning experiments with multiple vision models and graph neural networks in different regimes demonstrates usefulness of our proposals. We leave it for future work to explore how predictive various transferability measures are for co-training regimes (as opposed to fine-tuning).\n\nSupplementary Material for paper titled \"Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance\"\n\nThis supplemental document contains some additional results not included in the main body of the paper. In particular, we present S1 an algorithm (and derivation) for computing H \u03b1 (f ). S2 Proof for normalization of NCE. S3 Detailed descriptions of experimental settings for Vision models and additional results. S4 Detailed descriptions of experimental settings for Graph Neural Networks and additional results. This supplement is not entirely self-contained, so readers might need to refer to the main paper.\n\n\nS1 Optimized Implementation for H \u03b1 (f )\n\nAlgorithm We present our algorithm in Algorithm 1. In practice, in steps 2 and 4, we use GaussianRandomProjection and StandardScaler from sklearn respectively. Additionally, we use memory-and compute-optimized sklearn implementation of Ledoit Wolf in step 12 (sklearn.covariance.ledoit_wolf_shrinkage) and step 18 (sklearn.covariance.ledoit_wolf).\n\nAlgorithm 1 Algorithm for computing H \u03b1 (f ). Compute \u03b1 * as in equation 7 (main paper). 13:\n\n\u03c3 \u2190 1 (as F is normalized) 14:\nW \u2190 n\u03b1 * \u03c3In + (1 \u2212 \u03b1 * )(F F T ) 15: G = F R 16: H\u03b1 \u2190 1\u2212\u03b1 * n\u03c3\u03b1 * ||R|| 2 F \u2212 (1 \u2212 \u03b1 * ) vec(G) T vec(W \u22121 G) 17: else 18:\nCompute \u03b1 * and \u03a3 f \u03b1 * as in equations 7 and 3 resp. (main paper).\n\n\n19:\n\nH\u03b1\n\u2190 1\u2212\u03b1 * n trace \u03a3 f \u03b1 * \u22121 R R T\n\n20: end if\n\nDerivation We derive an optimized computation for our proposed shrinkagebased H-score H \u03b1 (f ) for small target data (C \u2264 n t < d) as follows:\nH \u03b1 (f ) = tr \u03a3 f \u22121 \u03b1 \u00b7 (1 \u2212 \u03b1)\u03a3 z = (1 \u2212 \u03b1) n t \u00b7 tr \u03b1\u03c3I d + (1 \u2212 \u03b1) n t F T F \u22121 Z T Z , = (1 \u2212 \u03b1) n t \u03b1\u03c3 \u00b7 tr I d + (1 \u2212 \u03b1) n t \u03b1\u03c3 F T F \u22121 RR T , = (1 \u2212 \u03b1) n t \u03b1\u03c3 \u00b7 tr I d \u2212 (1 \u2212 \u03b1) \u00b7 F T n t \u03b1\u03c3I n + (1 \u2212 \u03b1)F F T \u22121 F RR T ,(S1)= (1 \u2212 \u03b1) n t \u03b1\u03c3 \u00b7 tr R T R \u2212 (1 \u2212 \u03b1) \u00b7 tr G T W \u22121 G , (S2) = (1 \u2212 \u03b1) n t \u03b1\u03c3 \u00b7 \u2225R\u2225 2 F \u2212 (1 \u2212 \u03b1) \u00b7 vec (G) T vec W \u22121 G ,(S3)\nwhere R = \u221a n 1fY =1 , \u00b7 \u00b7 \u00b7 , \u221a n CfY =C \u2208 R d\u00d7C , G = F R \u2208 R nt\u00d7C , W = n t \u03b1\u03c3I n + (1 \u2212 \u03b1)F F T \u2208 R nt\u00d7nt , (S1) follows by Woodbury matrix identity [20] and (S2) and (S3) follow by trace properties.\n((I + U V ) \u22121 = I \u2212 U (I + V U ) \u22121 V )\n\nS2 Normalization of NCE\n\nNCE [31] evaluates conditional entropy between target pseudo labels Z (t) (source model's assigned labels) and actual target labels, as given by:\nNCE(Y (t) |Z (t) ) = 1 n t nt i=1 log p Y (t) |Z (t) (y i |z i ) (S4)\nThe conditional entropy of the target labels conditioned on the dummy labels (source model' labels on target data) is:\nH(Y |Z) = \u2212 y\u2208Y,z\u2208Z p Y,Z (y, z) log p Y |Z (y|z)(S5)\nIt holds that 0 \u2264 H(Y |Z) \u2264 H(Y ) (see appendix S2.1). Negative Conditional Entropy (NCE) is given by NCE = \u2212H(Y |Z) and it holds that 0 \u2264 \u2212NCE \u2264 H(Y ). We normalize the NCE as follows:\n0 \u2265 NCE H(Y ) \u2265 \u22121 \u21d2 0 \u2264 1 + NCE H(Y ) \u2264 1 (S6)\nwhere entropy is positive for any practical classification task.\n\n\nS2.1 Proof of bounds on conditional entropy\nH(Y |Z) = \u2212 y\u2208Y,z\u2208Z p Y,Z (y, z) log p Y |Z (y|z) (S7) = z\u2208Z p Z (z) \uf8ee \uf8f0 \u2212 y\u2208Y p Y |Z (y|z) log p Y |Z (y|z) \uf8f9 \uf8fb (S8) = z\u2208Z p Z (z)H(Y |Z = z) (S9)\n\u2265 0 (S10) learning rate prevents catastrophic forgetting of the originally learned knowledge or features. Many studies have used fixed hyperparameters (e.g. learning rate, momentum and weight decay, number of epochs) for fine-tuning. However, the choice of hyperparameters is not necessarily optimal for fine-tuning on target tasks. Earlier work has reported that the performance is sensitive to the default hyperparameter selection, in particular learning rate, momentum (for stochastic gradient descent), weight decay and number of epochs [19,14,17]. The optimal choice of these parameters is not only target data dependent but also sensitive to the domain similarity between the source and target datasets [17]. Therefore, in order to ensure the target task accuracy (against which the correlation of transferability metrics is measured) is optimal, we repeat the finetuning exercise for 100 trials of hyperparameter settings. We employ Adam for fine-tuning experiments and optimize over batch size, learning rate, number of epochs and weight decay (L2 regularization on the classifier head). We select the space of these hyperparameters based on existing literature on fine-tuning, e.g. the learning rate is varied in the range \n\n\nS3.3 Target Task Selection Experiments setup\n\nThis evaluation regime is motivated by task transfer policy learning in robotics/ reinforcement learning. Under this regime, transferability measures can be used to greedily optimize a task transfer policy given a collection of tasks. For instance, a robot has to automatically select which new object to pick up. Given that the robot has learned to pick up a few objects before, it would be beneficial for the robot to optimally select the most transferable source/task object pair and improve it's maneuvering ability throughout the process in a highly efficient manner. TMs can also shed light on the relatedness of different tasks in reinforcement learning setups for better understanding. We currently evaluate target task selection regime on visual classification tasks with both VGG19 [29] and ResNet50 [11] models on subsets of CIFAR-100/CIFAR-10 data under three different dataset regimes following [21]. In all 3 cases outlined below, 20% of the samples from the randomly generated subsets is designated as validation set for hyperparameter tuning to find the model with with optimal validation accuracy. We use all examples in the original test set for evaluating out-of-sample accuracy performance on the target data. Both training and validation samples in the subsets are used for computation of transferability metrics and we report the correlation of these measures against the (relative) test accuracies for the randomly generated subsets.\n\n-Small-Balanced Target Data: We make a random selection of 5 classes from CIFAR-100/CIFAR-10 and sample 50 samples per class from the original train split, out of which we designate 10 samples per class for validation. We repeat this exercise 50 times (with a different selection of 5 classes), fine-tune the model for each selection (100 hyperparameter tuning trials per selection to find finetuned model with optimal validation accuracy) and evaluate performance of those optimal models in terms of test accuracy. We then evaluate rank correlations of TMs across the 50 experiments with random selection of 5 sub-classes. -Small-Imbalanced Target Data: We make 50 random selections of 2 classes from CIFAR-100/CIFAR-10, sample between 30 \u2212 60 samples from the first class and sample 5\u00d7 the number of samples from the second class. This makes for a binary imbalanced classification task. We again measure performance of transferability measures against optimal target test accuracy. -Large-Balanced Target Data with different number of classes: We randomly select 2-100 classes from CIFAR-100 and include all samples from the chosen classes (500 samples per class). This constructs a range of large balanced dataset target task selection cases. We evaluate correlation of TMs with relative target test accuracy across the variable number of target classes.\n\n\nS3.4 Spearman Rank Correlations\n\nWe present rank correlation performance of all transferability measures across various fine-tuning scenarios (target task selection and source model selection), fine-tuning strategies (linear and nonlinear) in various data regimes. Table S2  combines setups in Tables 1, 2, and 3 (main paper) and presents Spearman correlation performance of H \u03b1 (f ) against transferability measures. Correlations marked with asterisks (*) are not statistically significant (p-value > 0.05). Hyphen (-) indicates the computation ran out of memory on 128GB RAM. With respect to Spearman correlations in the table above, our shrinkagebased H-score H \u03b1 (f ) leads in 7/15 cases and LogME leads in 8/15 cases. In terms of Pearson correlations, H \u03b1 (f ) leads in 9/15 cases (Tables 1, 2, and 3 (main paper)) and LogME leads in 4/15 cases. Additionally, LogME seems to be intractable with respect to memory and computational speed for nonlinear settings where feature dimension is large (d \u223c 10 5 ). Our efficient implementation for H \u03b1 (f ) provides a 3 \u2212 10 times computational advantage over LogME.\n\n\nS3.5 Experimental code and type of resources\n\nWe use Tensorflow Keras for our implementation. Imagenet checkpoints (ResNet and VGG) come from Keras https://keras.io/api/applications/. For fine-tuning experiments, we use 2 P100 GPUs per model, 15GB RAM per GPU.\n\n\nS4 Graph Convolutional Networks\n\n\nS4.1 Twitch Social Network Dataset Details\n\nTwitch Social Network [26,25,27] contains social networks of gamers from the streaming service Twitch, where nodes correspond to Twitch users and links correspond to mutual friendships. There are country-specific sub-networks. We consider six sub-networks: DE, ES, FR, RU, PTBR, ENGB. Features describe the history of games played and the associated task is binary classification whether a gamer streams adult content. The country specific graphs share the same node features which means that we can perform transfer learning with these datasets. \n\n\nS4.2 Source Model Selection\n\nFor source model selection, we pre-train 42 different GCN models with embedding dimensions in the range {128, 144, 160, \u00b7 \u00b7 \u00b7 , 784}. For each embedding dimension, we follow the pre-training procedure and fine-tuning exercise outlined in subsections Pre-training Implementation and Fine-tuning in Section 6.2 (main paper). For measuring transferability via H \u03b1 (f ), we first project target feature embeddings derived from each source GCN model to 128-dimensional space with random projection. This is important for effective comparison of H \u03b1 (f ) across source models with different embedding dimensions as outlined in Section 4.2 (main paper). We present Pearson correlation performance against target tesk accuracy in Table S4 for our proposed H \u03b1 (f ) and all state-of-the-art transferability measures. We present multiple source-target sub-network pairs for a thorough evaluation of performance of all transferability measures. We again see H \u03b1 (f ) as a leading metrics in transferability estimation for source model selection.\n\n\nS4.3 Experimental code and type of resources\n\nWe use PyTorch Geometric for our implementation. For pre-training experiments, we use 4 CPUs per model, 16GB RAM per CPU.\n\nFig. 1 :\n1Non-reliability of H(f ). H(f ) is \u223c 75\u00d7 larger than the population version of the H-Score. Population version is estimated with 10 6 samples.\n\nFig. 2 :\n2[Left] Stability of H(f ) and our shrinkage-based H \u03b1 (f ) with respect to number of samples. H(f ) is \u223c 75 times larger than the population version of the H-Score (estimated with a sample size of 10 6 ). In contrast, the shrinkage-based H-Score is significantly more reliable.[Right] Effect of \u03b1 on shrunk H-Score.\n\nFig. 3 :\n3Relation of NCE, LEEP & N LEEP to [Left] number of classes (log-scale) and [Right] class imbalance, max(n 1 , n 2 )/min(n 1 , n 2 ), for VGG19 on CIFAR100. For [Left], we randomly select 2-100 classes. For [Right], we randomly select 2 classes and vary the class imbalances.\n\n1 :-\n1Pearson correlation of transferability measures against fine-tuned target accuracy of vision models in the context of target task selection. We compare our proposed H \u03b1 (f ) against original H(f ) and state-of-the-art measures. Strategy Target Model Reg. H(f ) H\u03b1(f ) NCE LEEP N LEEP TransRate Small-Balanced Target Data (S-B): We make a random selection of 5 classes from CIFAR-100/CIFAR-10 and sample 50 samples per class from the original train split. We repeat this exercise 50 times (with a different selection of 5 classes). We evaluate correlations of transferability measures across the 50 random experiments. -Small-Imbalanced Target Data (S-IB): We make 50 random selections of 2 classes from CIFAR-100/CIFAR-10, sample between 30 \u2212 60 samples from the first class and sample 5\u00d7 the number of samples from the second class.\n\n\nWe pre-train the GCN model with each of the countryspecific sub-network. For each country-specific sub-network, S \u2208 {DE, ES, FR, RU, PTBR, ENGB}, we construct 30 different target tasks. We exclude the specific country on which the source model is pre-trained and use the remaining countries to construct different combinations of networks as targets. For example, if the source model is pre-trained on DE, then the target tasks are given by: {ES, FR, RU, PTBR, ENGB, (ES,FR), (ES,RU), \u00b7 \u00b7 \u00b7 , (ES,FR,RU,PTBR,ENGB)}.\n\nInput:\nData: F , Y ; use_dimensionality_reduction, q; 1: if use_dimensionality_reduction then 2: F \u2190 Random-Projection(F , q) 3: end if 4: F \u2190 z-normalize(F ) 5: n, d \u2190 size(F ) 6: number_of_samples_in_each_class \u2190 unique(Y ) 7: Initialize R \u2190 0 d,C 8: for c = 1, 2, . . . , C do 9: R[:, c] \u2190 number_of_samples_in_each_class[c]fY =c 10: end for 11: if n < d then 12:\n\n\n[10 \u22125 , 10 \u22121 ], the number of epochs between {25, 50, 75, 100, 125, 150, 175, 200}, the batch size between {32, 64, 128} and the weight decay in the range [10 \u22126 , 10 \u22122 ].\n\nTable\n\n\nTable 2 :\n2Pearson correlation of transferability measures against relative accuracy for large balanced CIFAR-100 dataset with different number of classes across target tasks.Model \nH(f ) H\u03b1(f ) NCE n-NCE LEEP n-LEEP N LEEP n-N LEEP TransRate LogME \nVGG19 \n0.88 0.97 -0.95 0.66 \n-0.95 \n0.66 \n-0.93 \n0.95 \n0.68 \n0.96 \nResNet50 0.95 0.98 -0.95 -0.74 \n-0.95 \n-0.73 \n-0.94 \n-0.63 \n0.56 \n0.96 \n\n\nTable 3 :\n3Pearson correlation of proposed H \u03b1 (f ) without/with Random Projection (RP) for fine-tuning in source model selection of vision models in small data regimes.Strategy Target \nH(f ) H\u03b1(f )[No RP] H\u03b1(f )[RP] NCE LEEP N LEEP TransRate LogME \n\nLFT \nCIFAR-100 -0.190* \n0.024* \n0.859 \n0.825 0.839 0.852 \n-0.204* \n0.705 \nCIFAR-10 0.276* \n0.277* \n0.939 \n0.938 0.936 0.938 \n0.311* \n0.923 \nNLFT CIFAR-100 -0.108* \n0.125* \n0.879 \n0.967 0.976 0.977 \n-\n-\n\ntarget test accuracy for reasons highlighted in Section 5. This setting validates \nthat the normalizations, proposed in equation 12 in Section 5, can improve the \ncorrelations of NCE, LEEP and N LEEP when evaluated on the more appro-\npriate relative accuracy scale. Table 2 demonstrates how various transferability \nmeasures perform on target task selection when the number of target classes \nvaries. H \u03b1 (f ) dominates the performance for both VGG19 and ResNet50 mod-\nels, surpassing all transferability measures, closely followed by LogME. \n\n\n\nTable 4 :\n4Pearson correlation of transferability measures against fine-tuned target accuracy of Graph Convolutional Networks in Target Task selection scenario. We compare our proposed H \u03b1 (f ) against original H(f ) and state-of-the-art measures.nt \nModel \nSource H(f ) H\u03b1(f ) 5 NCE LEEP N LEEP LFC TransRate LogME \n\n500 GCN-256 \n\nDE \n0.10* \n0.35 \n0.15* 0.15* \n0.40 \n0.53 \n-0.34 \n0.40 \nES \n0.24 \n0.41 \n0.48 \n0.50 \n-0.13* \n0.24* \n-0.34* \n0.20* \nFR \n0.57 \n0.61 \n-0.03* 0.01* \n-0.05* \n0.26* \n-0.30* \n0.44 \nRU \n0.11* \n0.34 \n-0.19* -0.17* \n-0.12* \n0.04* \n-0.21 \n0.11* \nPTBR 0.37 \n0.24* \n0.16* 0.16* \n-0.13* \n-0.02* \n-0.05* \n0.15* \nENGB 0.48 \n0.53 \n-0.12* -0.09* \n-0.12* \n0.15* \n-0.05* \n0.32* \n\n1000 GCN-512 \n\nDE \n0.48 \n0.71 \n0.29* \n0.44 \n-0.14* \n0.45 \n0.17* \n0.59 \nES \n0.68 \n0.78 \n0.59 \n0.54 \n0.35* \n0.49 \n-0.12* \n0.59 \nFR \n0.59 \n0.61 \n0.25* 0.27* \n0.04* \n0.10* \n0.58 \n0.22* \nRU \n0.35 \n0.44 \n-0.12* 0.08* \n-0.25* \n0.14* \n-0.07* \n0.27* \nPTBR 0.67 \n0.77 \n0.37 \n0.40 \n0.04* \n0.18* \n0.10 \n0.50 \nENGB 0.82 \n0.81 \n-0.04* -0.08* \n0.17* \n0.26* \n0.15* \n0.65 \n\n\n\nTable 5 :\n5Timing comparison of LogME and our H \u03b1 (f ). All times are in ms.nt \nd \n|Y| = C LogME H(f ) H\u03b1(f ) \n500 500 \n50 \n201 \n123 \n22 \n500 1000 \n50 \n185 \n376 \n36 \n500 5000 \n50 \n392 \n9680 \n373 \n500 1000 \n10 \n111 \n259 \n33 \n500 1000 \n100 \n268 \n271 \n36 \n100 1000 \n50 \n72 \n255 \n16 \n1000 1000 \n50 \n318 \n335 \n75 \n\n\n\nTable S2 :\nS2Spearman correlation comparison of transferability measures. Larger correlations indicate better identifiability as quantified by transferability measure. We compared our proposed H \u03b1 (f ) against original H(f ) and state-of-the-art measures. We evaluate against relative accuracy.Scen. Strategy TargetModel Reg. H(f ) H\u03b1(f ) n-NCE n-LEEP n-N LEEP TransRate LogMETTS \n\nLFT \n\nCIFAR-100 \n\nVGG19 \nS-B \n-0.19* 0.77 0.67 \n0.67 \n0.81 \n0.56 \n0.86 \nS-IB -0.07* 0.71 0.64 \n0.63 \n0.72 \n0.40 \n0.79 \nL-B-C 0.96 0.97 0.50 \n0.44 \n0.95 \n0.91 \n0.96 \n\nResNet50 \nS-B \n0.13* 0.80 0.63 \n0.65 \n0.78 \n0.19* \n0.82 \nS-IB -0.10* 0.76 0.57 \n0.58 \n0.69 \n0.41 \n0.81 \nL-B \n0.98 1.00 -0.89 -0.86 \n-0.74 \n0.90 \n0.99 \n\nCIFAR-10 \n\nVGG19 \nS-B \n0.06* 0.57 0.49 \n0.49 \n0.55 \n0.30 \n0.65 \nS-IB 0.21* 0.72 0.76 \n0.85 \n0.85 \n0.32 \n0.86 \n\nResNet50 \nS-B \n-0.31 0.60 0.28 \n0.29 \n0.51 \n0.03* \n0.59 \nS-IB \n0.35 0.76 0.64 \n0.69 \n0.72 \n0.25* \n0.76 \n\nNLFT CIFAR-100 VGG19 \nS-B \n-0.00* 0.76 0.61 \n0.62 \n0.71 \n-\n-\nS-IB 0.03* 0.59 0.62 \n0.62 \n0.68 \n-\n-\n\nSMS \nLFT \nCIFAR-100 \n-\nA-C 0.30* 0.88 0.83 \n0.83 \n0.80 \n0.35* \n0.83 \nCIFAR-10 \n-\nA-C 0.07* 0.88 0.93 \n0.92 \n0.92 \n0.07* \n0.95 \nNLFT CIFAR-100 \n-\nA-C 0.052* 0.96 0.93 \n0.93 \n0.93 \n-\n-\n\n\nTable S3 :\nS3Data description for Twitch social network of users with countryspecific sub-networks.Country #Features #Classes #Nodes Class-imbalance Max. balanced nodes \nDE \n3169 \n2 \n9,498 \n1.53 \n7512 \nES \n3169 \n2 \n4,648 \n2.42 \n2720 \nFR \n3169 \n2 \n6,549 \n1.70 \n4848 \nRU \n3169 \n2 \n4,385 \n3.08 \n2150 \nENGB \n3169 \n2 \n7,126 \n1.20 \n6476 \nPTBR \n3169 \n2 \n1,912 \n1.89 \n1322 \n\n\n\nTable S4 :\nS4Correlation of transferability measures against fine-tuned target accuracy of Graph Convolutional Networks in source model selection scenario.Source Target \nH(f ) \nH\u03b1(f ) \nNCE \nLEEP N LEEP \nLFC \nTransRate LogME \nDE \nES \n0.34 \n0.43 \n0.39 \n0.49 \n0.19* \n0.21* \n0.30* \n0.50 \nDE \nFR \n0.25 \n0.41 \n-0.11* -0.13* \n-0.24 \n0.07* \n0.20* \n0.49 \nFR \nDE \n0.40 \n0.56 \n-0.01* \n-0.02 \n0.04* \n0.54 \n0.42 \n0.56 \nRU \nDE \n0.55 \n0.62 \n0.04* \n-0.10* \n0.31 \n0.52 \n0.48 \n0.59 \nPTBR \nES \n0.35 \n0.45 \n0.05* \n-0.03* \n0.11* \n0.41 \n0.26* \n0.41 \nES \nRU \n0.28* \n0.39 \n0.20* \n0.16* \n-0.00* \n-0.12* \n0.24 \n0.25* \nRU \nENGB 0.14* \n0.34 \n-0.26* -0.22* \n-0.08* \n0.11* \n0.21* \n0.18* \nPTBR \nRU \n0.25* \n0.31 \n0.29* \n0.21* \n0.08* \n0.07* \n0.20* \n0.29* \nENGB \nDE \n0.20 \n0.52 \n-0.01* \n-0.01 \n0.19* \n0.05* \n0.09* \n0.61 \nENGB \nFR \n0.36 \n0.58 \n-0.04* -0.08* \n-0.10* \n0.13* \n0.34 \n0.60 \nDE \nPTBR \n0.36 \n0.22* \n0.08* \n0.11* \n-0.17* \n0.11* \n0.28* \n0.33 \nPTBR \nDE \n0.63 \n0.34 \n0.31* \n0.38 \n0.10* \n0.10* \n0.63 \n0.45 \nENGB \nFR \n0.70 \n0.47 \n0.22* \n0.28* \n0.32 \n0.07* \n0.65 \n0.64 \nES \nDE \n0.35 \n0.23* \n0.22* \n0.16* \n0.10* \n0.06* \n0.35 \n0.31 \nES \nFR \n0.38 \n0.19 \n-0.08* \n-0.03 \n-0.13* \n0.38 \n0.44 \n0.41 \n\n\nCondition number of a positive semidefinite matrix A, is the ratio of its largest and smallest eigenvalues.\nhttps://keras.io/api/applications/\nWe empirically observed dimensionality reduction with random projection to improve correlation performance for H\u03b1(f ) in this target task selection setting as well. We used q = 128.\nhttps://keras.io/api/applications/\nwhere (S10) holds because H(Y |Z = z) \u2265 0 and holds with equality if and only if Y is a deterministic function of Z.S3 Vision ModelsS3.1 Embedding layers for linear and nonlinear FinetuningS3.2 Tuning hyperparametersCurrent practices for fine-tuning typically involve a selection of values for hyperparameters when retraining the model on target data. Given that the target datasets are typically small in the transfer learning scenarios, the typical strategy is to adopt the default hyperparameters for training large models while using smaller initial learning rate and fewer epochs for fine-tuning. It has been believed that adhering to the original hyperparameters for fine-tuning with small\nAn information-theoretic approach to transferability in task transfer learning. Y Bao, Y Li, S Huang, 2019 IEEE ICIP. Bao, Y., Li, Y., Huang, S., et al.: An information-theoretic approach to transfer- ability in task transfer learning. In: 2019 IEEE ICIP. pp. 2309-2313 (2019)\n\nShrinkage algorithms for mmse covariance estimation. Y Chen, A Wiesel, Y C Eldar, IEEE Transactions on Signal Processing. 5810Chen, Y., Wiesel, A., Eldar, Y.C., et al.: Shrinkage algorithms for mmse covariance estimation. IEEE Transactions on Signal Processing 58(10), 5016-5029 (2010)\n\n. F Chollet, Chollet, F., et al.: Keras. https://github.com/fchollet/keras (2015)\n\nLarge scale fine-grained categorization and domain-specific transfer learning. Y Cui, Y Song, C Sun, CoRR abs/1806.06193Cui, Y., Song, Y., Sun, C., et al.: Large scale fine-grained categorization and domain-specific transfer learning. CoRR abs/1806.06193 (2018)\n\nA linearized framework and a new benchmark for model selection for fine-tuning. A Deshpande, A Achille, A Ravichandran, Deshpande, A., Achille, A., Ravichandran, A., et al.: A linearized framework and a new benchmark for model selection for fine-tuning (2021)\n\nBERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, CoRR abs/1810.04805Devlin, J., Chang, M., Lee, K., et al.: BERT: pre-training of deep bidirectional transformers for language understanding. CoRR abs/1810.04805 (2018)\n\nFast graph representation learning with PyTorch Geometric. M Fey, J E Lenssen, ICLR Workshop on Representation Learning on Graphs and Manifolds. Fey, M., Lenssen, J.E.: Fast graph representation learning with PyTorch Geometric. In: ICLR Workshop on Representation Learning on Graphs and Manifolds (2019)\n\nAdditive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors). J Friedman, T Hastie, R Tibshirani, The Annals of Statistics. 282Friedman, J., Hastie, T., Tibshirani, R.: Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors). The Annals of Statistics 28(2), 337 -407 (2000)\n\nDesign of experiments for the nips 2003 variable selection benchmark. I Guyon, Guyon, I.: Design of experiments for the nips 2003 variable selection benchmark (2003)\n\nT Hastie, R Tibshirani, J Friedman, The Elements of Statistical Learning. Springer Series in Statistics. New York, NY, USASpringer New York IncHastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning. Springer Series in Statistics, Springer New York Inc., New York, NY, USA (2001)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, CoRR abs/1512.03385He, K., Zhang, X., Ren, S., et al.: Deep residual learning for image recognition. CoRR abs/1512.03385 (2015)\n\nL K Huang, Y Wei, Y Rong, ArXiv abs/2106.09362Frustratingly easy transferability estimation. Huang, L.K., Wei, Y., Rong, Y., et al.: Frustratingly easy transferability estimation. ArXiv abs/2106.09362 (2021)\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, ICLR 2017. Toulon, FranceKipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. In: ICLR 2017, Toulon, France, April 24-26. OpenReview.net (2017)\n\nS Kornblith, J Shlens, Q V Le, Do better imagenet models transfer better? In: 2019 IEEE/CVF CVPR. Kornblith, S., Shlens, J., Le, Q.V.: Do better imagenet models transfer better? In: 2019 IEEE/CVF CVPR. pp. 2656-2666 (2019)\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G Hinton, Neural Information Processing Systems. 25Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classification with deep convo- lutional neural networks. Neural Information Processing Systems 25 (01 2012)\n\nA well-conditioned estimator for large-dimensional covariance matrices. O Ledoit, M Wolf, Journal of Multivariate Analysis. 882Ledoit, O., Wolf, M.: A well-conditioned estimator for large-dimensional covariance matrices. Journal of Multivariate Analysis 88(2), 365-411 (Feb 2004)\n\nRethinking the hyperparameters for finetuning. H Li, P Chaudhari, H Yang, CoRR abs/2002.11770Li, H., Chaudhari, P., Yang, H., et al.: Rethinking the hyperparameters for fine- tuning. CoRR abs/2002.11770 (2020)\n\nRanking neural checkpoints. Y Li, X Jia, R Sang, Proceedings of the IEEE/CVF CVPR. the IEEE/CVF CVPRLi, Y., Jia, X., Sang, R., et al.: Ranking neural checkpoints. In: Proceedings of the IEEE/CVF CVPR. pp. 2663-2673 (June 2021)\n\nExploring the limits of weakly supervised pretraining. D Mahajan, R B Girshick, V Ramanathan, CoRR abs/1805.00932Mahajan, D., Girshick, R.B., Ramanathan, V., et al.: Exploring the limits of weakly supervised pretraining. CoRR abs/1805.00932 (2018)\n\nInverting modified matrices. A W Max, Memorandum Rept. 42, Statistical Research Group. Princeton Univ4Max, A.W.: Inverting modified matrices. In: Memorandum Rept. 42, Statistical Research Group, p. 4. Princeton Univ. (1950)\n\nC V Nguyen, T Hassner, M Seeger, C Archambeau, Leep: A new measure to evaluate transferability of learned representations. Nguyen, C.V., Hassner, T., Seeger, M., Archambeau, C.: Leep: A new measure to evaluate transferability of learned representations (2020)\n\nScikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, J. Mach. Learn. Res. 12Pedregosa, F., Varoquaux, G., Gramfort, A., et al.: Scikit-learn: Machine learning in python. J. Mach. Learn. Res. 12(null), 2825-2830 (Nov 2011)\n\nHigh-dimensional covariance estimation: with high-dimensional data. M Pourahmadi, John Wiley & Sons882Pourahmadi, M.: High-dimensional covariance estimation: with high-dimensional data, vol. 882. John Wiley & Sons (2013)\n\nFailing loudly: An empirical study of methods for detecting dataset shift. S Rabanser, S G\u00fcnnemann, Z C Lipton, NeurIPSRabanser, S., G\u00fcnnemann, S., Lipton, Z.C.: Failing loudly: An empirical study of methods for detecting dataset shift. In: NeurIPS (2019)\n\nMulti-Scale Attributed Node Embedding. B Rozemberczki, C Allen, R Sarkar, Journal of Complex Networks. 92Rozemberczki, B., Allen, C., Sarkar, R.: Multi-Scale Attributed Node Embedding. Journal of Complex Networks 9(2) (2021)\n\nCharacteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models. B Rozemberczki, R Sarkar, Proceedings of the 29th ACM CIKM. p. 1325-1334. CIKM '20. the 29th ACM CIKM. p. 1325-1334. CIKM '20New York, NY, USAACMRozemberczki, B., Sarkar, R.: Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models. In: Proceedings of the 29th ACM CIKM. p. 1325-1334. CIKM '20, ACM, New York, NY, USA (2020)\n\nTwitch gamers: a dataset for evaluating proximity preserving and structural role-based node embeddings. B Rozemberczki, R Sarkar, Rozemberczki, B., Sarkar, R.: Twitch gamers: a dataset for evaluating proximity preserving and structural role-based node embeddings (2021)\n\nA shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics. J Sch\u00e4fer, K Strimmer, Statistical Applications in Genetics and Molecular Biology. 4Sch\u00e4fer, J., Strimmer, K.: A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics. Statistical Applications in Genetics and Molecular Biology 4 (2005)\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, CoRR abs/1409.1556Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. CoRR abs/1409.1556 (2015)\n\nOTCE: A transferability metric for cross-domain crosstask representations. Y Tan, Y Li, S Huang, CoRR abs/2103.13843Tan, Y., Li, Y., Huang, S.: OTCE: A transferability metric for cross-domain cross- task representations. CoRR abs/2103.13843 (2021)\n\nTransferability and hardness of supervised classification tasks. A Tran, C Nguyen, T Hassner, 2019 IEEE/CVF ICCV. Tran, A., Nguyen, C., Hassner, T.: Transferability and hardness of supervised clas- sification tasks. In: 2019 IEEE/CVF ICCV. pp. 1395-1405 (2019)\n\nInformation theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. N X Vinh, J Epps, J Bailey, Journal of Machine Learning Research. 1195Vinh, N.X., Epps, J., Bailey, J.: Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. Journal of Machine Learning Research 11(95), 2837-2854 (2010)\n\nLogme: Practical assessment of pre-trained models for transfer learning. K You, Y Liu, J Wang, M Long, ICMLYou, K., Liu, Y., Wang, J., Long, M.: Logme: Practical assessment of pre-trained models for transfer learning. In: ICML (2021)\n\nRanking and tuning pre-trained models: A new paradigm of exploiting model hubs. K You, Y Liu, Z Zhang, J Wang, M I Jordan, M Long, You, K., Liu, Y., Zhang, Z., Wang, J., Jordan, M.I., Long, M.: Ranking and tuning pre-trained models: A new paradigm of exploiting model hubs (2021)\n", "annotations": {"author": "[{\"end\":34,\"start\":4},{\"end\":104,\"start\":35},{\"end\":172,\"start\":105},{\"end\":259,\"start\":173}]", "publisher": null, "author_last_name": "[{\"end\":18,\"start\":12},{\"end\":49,\"start\":43},{\"end\":115,\"start\":105},{\"end\":187,\"start\":179}]", "author_first_name": "[{\"end\":11,\"start\":4},{\"end\":42,\"start\":35},{\"end\":178,\"start\":173}]", "author_affiliation": "[{\"end\":103,\"start\":51},{\"end\":171,\"start\":140},{\"end\":258,\"start\":206}]", "title": null, "venue": null, "abstract": "[{\"end\":1807,\"start\":465}]", "bib_ref": "[{\"end\":2413,\"start\":2409},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2419,\"start\":2417},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2861,\"start\":2857},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2871,\"start\":2868},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6792,\"start\":6788},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6987,\"start\":6983},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7178,\"start\":7175},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7377,\"start\":7374},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7380,\"start\":7377},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7383,\"start\":7380},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7386,\"start\":7383},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7388,\"start\":7386},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7727,\"start\":7723},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7960,\"start\":7956},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8148,\"start\":8145},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8230,\"start\":8226},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8386,\"start\":8383},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8507,\"start\":8503},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9107,\"start\":9104},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10224,\"start\":10220},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10476,\"start\":10472},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11179,\"start\":11176},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11352,\"start\":11348},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11534,\"start\":11530},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12085,\"start\":12081},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12350,\"start\":12346},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13145,\"start\":13141},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13147,\"start\":13145},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13150,\"start\":13147},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13502,\"start\":13498},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14267,\"start\":14263},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16953,\"start\":16950},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17870,\"start\":17866},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20193,\"start\":20190},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20960,\"start\":20956},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20963,\"start\":20960},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20982,\"start\":20978},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21808,\"start\":21804},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22750,\"start\":22746},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23321,\"start\":23317},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23324,\"start\":23321},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23327,\"start\":23324},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24842,\"start\":24838},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24857,\"start\":24853},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24977,\"start\":24974},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25263,\"start\":25259},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26023,\"start\":26019},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26628,\"start\":26625},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28761,\"start\":28757},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28764,\"start\":28761},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28767,\"start\":28764},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29484,\"start\":29480},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29659,\"start\":29655},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30414,\"start\":30411},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31173,\"start\":31169},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32517,\"start\":32513},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32582,\"start\":32579},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32871,\"start\":32867},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36792,\"start\":36788},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36914,\"start\":36910},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":38333,\"start\":38329},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":38336,\"start\":38333},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38339,\"start\":38336},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38501,\"start\":38497},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":39865,\"start\":39861},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39883,\"start\":39879},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39981,\"start\":39977},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":43369,\"start\":43365},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43372,\"start\":43369},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":43375,\"start\":43372}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":45280,\"start\":45127},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45607,\"start\":45281},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45893,\"start\":45608},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46734,\"start\":45894},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47252,\"start\":46735},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47620,\"start\":47253},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47797,\"start\":47621},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47805,\"start\":47798},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48196,\"start\":47806},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49196,\"start\":48197},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50245,\"start\":49197},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50557,\"start\":50246},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":51758,\"start\":50558},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":52127,\"start\":51759},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":53290,\"start\":52128}]", "paragraph": "[{\"end\":3020,\"start\":1823},{\"end\":4081,\"start\":3022},{\"end\":5001,\"start\":4083},{\"end\":5677,\"start\":5003},{\"end\":6108,\"start\":5703},{\"end\":6363,\"start\":6110},{\"end\":6602,\"start\":6365},{\"end\":7360,\"start\":6619},{\"end\":8477,\"start\":7362},{\"end\":9061,\"start\":8479},{\"end\":9339,\"start\":9063},{\"end\":10763,\"start\":9779},{\"end\":11853,\"start\":10765},{\"end\":11939,\"start\":11890},{\"end\":12513,\"start\":11981},{\"end\":13297,\"start\":12545},{\"end\":13713,\"start\":13357},{\"end\":13953,\"start\":13786},{\"end\":15443,\"start\":14101},{\"end\":15628,\"start\":15471},{\"end\":15764,\"start\":15760},{\"end\":16247,\"start\":15829},{\"end\":17501,\"start\":16312},{\"end\":18255,\"start\":17503},{\"end\":18734,\"start\":18303},{\"end\":19156,\"start\":18925},{\"end\":20983,\"start\":19207},{\"end\":21407,\"start\":20985},{\"end\":21809,\"start\":21500},{\"end\":22507,\"start\":21811},{\"end\":24025,\"start\":22523},{\"end\":24431,\"start\":24033},{\"end\":24713,\"start\":24433},{\"end\":25739,\"start\":24743},{\"end\":26303,\"start\":25765},{\"end\":26825,\"start\":26305},{\"end\":28642,\"start\":26827},{\"end\":29295,\"start\":28680},{\"end\":29534,\"start\":29297},{\"end\":29985,\"start\":29589},{\"end\":30062,\"start\":29987},{\"end\":33105,\"start\":30113},{\"end\":34698,\"start\":33191},{\"end\":34852,\"start\":34700},{\"end\":35365,\"start\":34854},{\"end\":35757,\"start\":35410},{\"end\":35851,\"start\":35759},{\"end\":35883,\"start\":35853},{\"end\":36075,\"start\":36008},{\"end\":36085,\"start\":36083},{\"end\":36274,\"start\":36132},{\"end\":36838,\"start\":36635},{\"end\":37051,\"start\":36906},{\"end\":37240,\"start\":37122},{\"end\":37480,\"start\":37295},{\"end\":37593,\"start\":37529},{\"end\":39020,\"start\":37788},{\"end\":40525,\"start\":39069},{\"end\":41884,\"start\":40527},{\"end\":42999,\"start\":41920},{\"end\":43262,\"start\":43048},{\"end\":43890,\"start\":43343},{\"end\":44956,\"start\":43922},{\"end\":45126,\"start\":45005}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9778,\"start\":9340},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11980,\"start\":11940},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12544,\"start\":12514},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13356,\"start\":13298},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13785,\"start\":13714},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14013,\"start\":13954},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14100,\"start\":14013},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15470,\"start\":15444},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15759,\"start\":15629},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15828,\"start\":15765},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18924,\"start\":18735},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21467,\"start\":21408},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21499,\"start\":21467},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29588,\"start\":29535},{\"attributes\":{\"id\":\"formula_14\"},\"end\":30112,\"start\":30063},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36007,\"start\":35884},{\"attributes\":{\"id\":\"formula_16\"},\"end\":36118,\"start\":36086},{\"attributes\":{\"id\":\"formula_17\"},\"end\":36508,\"start\":36275},{\"attributes\":{\"id\":\"formula_18\"},\"end\":36634,\"start\":36508},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36879,\"start\":36839},{\"attributes\":{\"id\":\"formula_20\"},\"end\":37121,\"start\":37052},{\"attributes\":{\"id\":\"formula_21\"},\"end\":37294,\"start\":37241},{\"attributes\":{\"id\":\"formula_22\"},\"end\":37528,\"start\":37481},{\"attributes\":{\"id\":\"formula_23\"},\"end\":37787,\"start\":37640}]", "table_ref": "[{\"end\":17473,\"start\":17465},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18212,\"start\":18205},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22389,\"start\":22382},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24486,\"start\":24475},{\"end\":26514,\"start\":26506},{\"end\":26671,\"start\":26664},{\"end\":27765,\"start\":27756},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28581,\"start\":28574},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32066,\"start\":32059},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32978,\"start\":32971},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42192,\"start\":42152},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44652,\"start\":44644}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1821,\"start\":1809},{\"attributes\":{\"n\":\"2\"},\"end\":5701,\"start\":5680},{\"attributes\":{\"n\":\"3\"},\"end\":6617,\"start\":6605},{\"attributes\":{\"n\":\"4.1\"},\"end\":11888,\"start\":11856},{\"attributes\":{\"n\":\"4.2\"},\"end\":16310,\"start\":16250},{\"attributes\":{\"n\":\"4.3\"},\"end\":18301,\"start\":18258},{\"attributes\":{\"n\":\"5\"},\"end\":19205,\"start\":19159},{\"attributes\":{\"n\":\"6\"},\"end\":22521,\"start\":22510},{\"end\":24031,\"start\":24028},{\"attributes\":{\"n\":\"6.1\"},\"end\":24741,\"start\":24716},{\"end\":25763,\"start\":25742},{\"attributes\":{\"n\":\"6.2\"},\"end\":28678,\"start\":28645},{\"end\":33129,\"start\":33108},{\"attributes\":{\"n\":\"6.3\"},\"end\":33176,\"start\":33132},{\"attributes\":{\"n\":\"7\"},\"end\":33189,\"start\":33179},{\"end\":35408,\"start\":35368},{\"end\":36081,\"start\":36078},{\"end\":36130,\"start\":36120},{\"end\":36904,\"start\":36881},{\"end\":37639,\"start\":37596},{\"end\":39067,\"start\":39023},{\"end\":41918,\"start\":41887},{\"end\":43046,\"start\":43002},{\"end\":43296,\"start\":43265},{\"end\":43341,\"start\":43299},{\"end\":43920,\"start\":43893},{\"end\":45003,\"start\":44959},{\"end\":45136,\"start\":45128},{\"end\":45290,\"start\":45282},{\"end\":45617,\"start\":45609},{\"end\":45899,\"start\":45895},{\"end\":47260,\"start\":47254},{\"end\":47804,\"start\":47799},{\"end\":47816,\"start\":47807},{\"end\":48207,\"start\":48198},{\"end\":49207,\"start\":49198},{\"end\":50256,\"start\":50247},{\"end\":50569,\"start\":50559},{\"end\":51770,\"start\":51760},{\"end\":52139,\"start\":52129}]", "table": "[{\"end\":48196,\"start\":47982},{\"end\":49196,\"start\":48367},{\"end\":50245,\"start\":49445},{\"end\":50557,\"start\":50323},{\"end\":51758,\"start\":50935},{\"end\":52127,\"start\":51859},{\"end\":53290,\"start\":52284}]", "figure_caption": "[{\"end\":45280,\"start\":45138},{\"end\":45607,\"start\":45292},{\"end\":45893,\"start\":45619},{\"end\":46734,\"start\":45901},{\"end\":47252,\"start\":46737},{\"end\":47620,\"start\":47261},{\"end\":47797,\"start\":47623},{\"end\":47982,\"start\":47818},{\"end\":48367,\"start\":48209},{\"end\":49445,\"start\":49209},{\"end\":50323,\"start\":50258},{\"end\":50935,\"start\":50572},{\"end\":51859,\"start\":51773},{\"end\":52284,\"start\":52142}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10663,\"start\":10657},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14491,\"start\":14485},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14959,\"start\":14953},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19822,\"start\":19816},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20459,\"start\":20453},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21907,\"start\":21901}]", "bib_author_first_name": "[{\"end\":54428,\"start\":54427},{\"end\":54435,\"start\":54434},{\"end\":54441,\"start\":54440},{\"end\":54679,\"start\":54678},{\"end\":54687,\"start\":54686},{\"end\":54697,\"start\":54696},{\"end\":54699,\"start\":54698},{\"end\":54915,\"start\":54914},{\"end\":55075,\"start\":55074},{\"end\":55082,\"start\":55081},{\"end\":55090,\"start\":55089},{\"end\":55339,\"start\":55338},{\"end\":55352,\"start\":55351},{\"end\":55363,\"start\":55362},{\"end\":55602,\"start\":55601},{\"end\":55612,\"start\":55611},{\"end\":55621,\"start\":55620},{\"end\":55856,\"start\":55855},{\"end\":55863,\"start\":55862},{\"end\":55865,\"start\":55864},{\"end\":56213,\"start\":56212},{\"end\":56225,\"start\":56224},{\"end\":56235,\"start\":56234},{\"end\":56550,\"start\":56549},{\"end\":56647,\"start\":56646},{\"end\":56657,\"start\":56656},{\"end\":56671,\"start\":56670},{\"end\":56997,\"start\":56996},{\"end\":57003,\"start\":57002},{\"end\":57012,\"start\":57011},{\"end\":57148,\"start\":57147},{\"end\":57150,\"start\":57149},{\"end\":57159,\"start\":57158},{\"end\":57166,\"start\":57165},{\"end\":57423,\"start\":57422},{\"end\":57425,\"start\":57424},{\"end\":57433,\"start\":57432},{\"end\":57627,\"start\":57626},{\"end\":57640,\"start\":57639},{\"end\":57650,\"start\":57649},{\"end\":57652,\"start\":57651},{\"end\":57916,\"start\":57915},{\"end\":57930,\"start\":57929},{\"end\":57943,\"start\":57942},{\"end\":58228,\"start\":58227},{\"end\":58238,\"start\":58237},{\"end\":58484,\"start\":58483},{\"end\":58490,\"start\":58489},{\"end\":58503,\"start\":58502},{\"end\":58676,\"start\":58675},{\"end\":58682,\"start\":58681},{\"end\":58689,\"start\":58688},{\"end\":58931,\"start\":58930},{\"end\":58942,\"start\":58941},{\"end\":58944,\"start\":58943},{\"end\":58956,\"start\":58955},{\"end\":59154,\"start\":59153},{\"end\":59156,\"start\":59155},{\"end\":59350,\"start\":59349},{\"end\":59352,\"start\":59351},{\"end\":59362,\"start\":59361},{\"end\":59373,\"start\":59372},{\"end\":59383,\"start\":59382},{\"end\":59653,\"start\":59652},{\"end\":59666,\"start\":59665},{\"end\":59679,\"start\":59678},{\"end\":59929,\"start\":59928},{\"end\":60158,\"start\":60157},{\"end\":60170,\"start\":60169},{\"end\":60183,\"start\":60182},{\"end\":60185,\"start\":60184},{\"end\":60379,\"start\":60378},{\"end\":60395,\"start\":60394},{\"end\":60404,\"start\":60403},{\"end\":60673,\"start\":60672},{\"end\":60689,\"start\":60688},{\"end\":61152,\"start\":61151},{\"end\":61168,\"start\":61167},{\"end\":61426,\"start\":61425},{\"end\":61437,\"start\":61436},{\"end\":61781,\"start\":61780},{\"end\":61793,\"start\":61792},{\"end\":62023,\"start\":62022},{\"end\":62030,\"start\":62029},{\"end\":62036,\"start\":62035},{\"end\":62262,\"start\":62261},{\"end\":62270,\"start\":62269},{\"end\":62280,\"start\":62279},{\"end\":62581,\"start\":62580},{\"end\":62583,\"start\":62582},{\"end\":62591,\"start\":62590},{\"end\":62599,\"start\":62598},{\"end\":62943,\"start\":62942},{\"end\":62950,\"start\":62949},{\"end\":62957,\"start\":62956},{\"end\":62965,\"start\":62964},{\"end\":63185,\"start\":63184},{\"end\":63192,\"start\":63191},{\"end\":63199,\"start\":63198},{\"end\":63208,\"start\":63207},{\"end\":63216,\"start\":63215},{\"end\":63218,\"start\":63217},{\"end\":63228,\"start\":63227}]", "bib_author_last_name": "[{\"end\":54432,\"start\":54429},{\"end\":54438,\"start\":54436},{\"end\":54447,\"start\":54442},{\"end\":54684,\"start\":54680},{\"end\":54694,\"start\":54688},{\"end\":54705,\"start\":54700},{\"end\":54923,\"start\":54916},{\"end\":55079,\"start\":55076},{\"end\":55087,\"start\":55083},{\"end\":55094,\"start\":55091},{\"end\":55349,\"start\":55340},{\"end\":55360,\"start\":55353},{\"end\":55376,\"start\":55364},{\"end\":55609,\"start\":55603},{\"end\":55618,\"start\":55613},{\"end\":55625,\"start\":55622},{\"end\":55860,\"start\":55857},{\"end\":55873,\"start\":55866},{\"end\":56222,\"start\":56214},{\"end\":56232,\"start\":56226},{\"end\":56246,\"start\":56236},{\"end\":56556,\"start\":56551},{\"end\":56654,\"start\":56648},{\"end\":56668,\"start\":56658},{\"end\":56680,\"start\":56672},{\"end\":57000,\"start\":56998},{\"end\":57009,\"start\":57004},{\"end\":57016,\"start\":57013},{\"end\":57156,\"start\":57151},{\"end\":57163,\"start\":57160},{\"end\":57171,\"start\":57167},{\"end\":57430,\"start\":57426},{\"end\":57441,\"start\":57434},{\"end\":57637,\"start\":57628},{\"end\":57647,\"start\":57641},{\"end\":57655,\"start\":57653},{\"end\":57927,\"start\":57917},{\"end\":57940,\"start\":57931},{\"end\":57950,\"start\":57944},{\"end\":58235,\"start\":58229},{\"end\":58243,\"start\":58239},{\"end\":58487,\"start\":58485},{\"end\":58500,\"start\":58491},{\"end\":58508,\"start\":58504},{\"end\":58679,\"start\":58677},{\"end\":58686,\"start\":58683},{\"end\":58694,\"start\":58690},{\"end\":58939,\"start\":58932},{\"end\":58953,\"start\":58945},{\"end\":58967,\"start\":58957},{\"end\":59160,\"start\":59157},{\"end\":59359,\"start\":59353},{\"end\":59370,\"start\":59363},{\"end\":59380,\"start\":59374},{\"end\":59394,\"start\":59384},{\"end\":59663,\"start\":59654},{\"end\":59676,\"start\":59667},{\"end\":59688,\"start\":59680},{\"end\":59940,\"start\":59930},{\"end\":60167,\"start\":60159},{\"end\":60180,\"start\":60171},{\"end\":60192,\"start\":60186},{\"end\":60392,\"start\":60380},{\"end\":60401,\"start\":60396},{\"end\":60411,\"start\":60405},{\"end\":60686,\"start\":60674},{\"end\":60696,\"start\":60690},{\"end\":61165,\"start\":61153},{\"end\":61175,\"start\":61169},{\"end\":61434,\"start\":61427},{\"end\":61446,\"start\":61438},{\"end\":61790,\"start\":61782},{\"end\":61803,\"start\":61794},{\"end\":62027,\"start\":62024},{\"end\":62033,\"start\":62031},{\"end\":62042,\"start\":62037},{\"end\":62267,\"start\":62263},{\"end\":62277,\"start\":62271},{\"end\":62288,\"start\":62281},{\"end\":62588,\"start\":62584},{\"end\":62596,\"start\":62592},{\"end\":62606,\"start\":62600},{\"end\":62947,\"start\":62944},{\"end\":62954,\"start\":62951},{\"end\":62962,\"start\":62958},{\"end\":62970,\"start\":62966},{\"end\":63189,\"start\":63186},{\"end\":63196,\"start\":63193},{\"end\":63205,\"start\":63200},{\"end\":63213,\"start\":63209},{\"end\":63225,\"start\":63219},{\"end\":63233,\"start\":63229}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":202782600},\"end\":54623,\"start\":54347},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12616083},\"end\":54910,\"start\":54625},{\"attributes\":{\"id\":\"b2\"},\"end\":54993,\"start\":54912},{\"attributes\":{\"doi\":\"CoRR abs/1806.06193\",\"id\":\"b3\"},\"end\":55256,\"start\":54995},{\"attributes\":{\"id\":\"b4\"},\"end\":55517,\"start\":55258},{\"attributes\":{\"doi\":\"CoRR abs/1810.04805\",\"id\":\"b5\"},\"end\":55794,\"start\":55519},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":70349949},\"end\":56099,\"start\":55796},{\"attributes\":{\"id\":\"b7\"},\"end\":56477,\"start\":56101},{\"attributes\":{\"id\":\"b8\"},\"end\":56644,\"start\":56479},{\"attributes\":{\"id\":\"b9\"},\"end\":56948,\"start\":56646},{\"attributes\":{\"doi\":\"CoRR abs/1512.03385\",\"id\":\"b10\"},\"end\":57145,\"start\":56950},{\"attributes\":{\"doi\":\"ArXiv abs/2106.09362\",\"id\":\"b11\"},\"end\":57354,\"start\":57147},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3144218},\"end\":57624,\"start\":57356},{\"attributes\":{\"id\":\"b13\"},\"end\":57848,\"start\":57626},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195908774},\"end\":58153,\"start\":57850},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17238653},\"end\":58434,\"start\":58155},{\"attributes\":{\"doi\":\"CoRR abs/2002.11770\",\"id\":\"b16\"},\"end\":58645,\"start\":58436},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":227126796},\"end\":58873,\"start\":58647},{\"attributes\":{\"doi\":\"CoRR abs/1805.00932\",\"id\":\"b18\"},\"end\":59122,\"start\":58875},{\"attributes\":{\"id\":\"b19\"},\"end\":59347,\"start\":59124},{\"attributes\":{\"id\":\"b20\"},\"end\":59608,\"start\":59349},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10659969},\"end\":59858,\"start\":59610},{\"attributes\":{\"id\":\"b22\"},\"end\":60080,\"start\":59860},{\"attributes\":{\"id\":\"b23\"},\"end\":60337,\"start\":60082},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":203593476},\"end\":60563,\"start\":60339},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218674393},\"end\":61045,\"start\":60565},{\"attributes\":{\"id\":\"b26\"},\"end\":61316,\"start\":61047},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2561185},\"end\":61710,\"start\":61318},{\"attributes\":{\"doi\":\"CoRR abs/1409.1556\",\"id\":\"b28\"},\"end\":61945,\"start\":61712},{\"attributes\":{\"doi\":\"CoRR abs/2103.13843\",\"id\":\"b29\"},\"end\":62194,\"start\":61947},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":201303557},\"end\":62456,\"start\":62196},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13962547},\"end\":62867,\"start\":62458},{\"attributes\":{\"id\":\"b32\"},\"end\":63102,\"start\":62869},{\"attributes\":{\"id\":\"b33\"},\"end\":63383,\"start\":63104}]", "bib_title": "[{\"end\":54425,\"start\":54347},{\"end\":54676,\"start\":54625},{\"end\":55853,\"start\":55796},{\"end\":56210,\"start\":56101},{\"end\":57420,\"start\":57356},{\"end\":57913,\"start\":57850},{\"end\":58225,\"start\":58155},{\"end\":58673,\"start\":58647},{\"end\":59151,\"start\":59124},{\"end\":59650,\"start\":59610},{\"end\":60376,\"start\":60339},{\"end\":60670,\"start\":60565},{\"end\":61423,\"start\":61318},{\"end\":62259,\"start\":62196},{\"end\":62578,\"start\":62458}]", "bib_author": "[{\"end\":54434,\"start\":54427},{\"end\":54440,\"start\":54434},{\"end\":54449,\"start\":54440},{\"end\":54686,\"start\":54678},{\"end\":54696,\"start\":54686},{\"end\":54707,\"start\":54696},{\"end\":54925,\"start\":54914},{\"end\":55081,\"start\":55074},{\"end\":55089,\"start\":55081},{\"end\":55096,\"start\":55089},{\"end\":55351,\"start\":55338},{\"end\":55362,\"start\":55351},{\"end\":55378,\"start\":55362},{\"end\":55611,\"start\":55601},{\"end\":55620,\"start\":55611},{\"end\":55627,\"start\":55620},{\"end\":55862,\"start\":55855},{\"end\":55875,\"start\":55862},{\"end\":56224,\"start\":56212},{\"end\":56234,\"start\":56224},{\"end\":56248,\"start\":56234},{\"end\":56558,\"start\":56549},{\"end\":56656,\"start\":56646},{\"end\":56670,\"start\":56656},{\"end\":56682,\"start\":56670},{\"end\":57002,\"start\":56996},{\"end\":57011,\"start\":57002},{\"end\":57018,\"start\":57011},{\"end\":57158,\"start\":57147},{\"end\":57165,\"start\":57158},{\"end\":57173,\"start\":57165},{\"end\":57432,\"start\":57422},{\"end\":57443,\"start\":57432},{\"end\":57639,\"start\":57626},{\"end\":57649,\"start\":57639},{\"end\":57657,\"start\":57649},{\"end\":57929,\"start\":57915},{\"end\":57942,\"start\":57929},{\"end\":57952,\"start\":57942},{\"end\":58237,\"start\":58227},{\"end\":58245,\"start\":58237},{\"end\":58489,\"start\":58483},{\"end\":58502,\"start\":58489},{\"end\":58510,\"start\":58502},{\"end\":58681,\"start\":58675},{\"end\":58688,\"start\":58681},{\"end\":58696,\"start\":58688},{\"end\":58941,\"start\":58930},{\"end\":58955,\"start\":58941},{\"end\":58969,\"start\":58955},{\"end\":59162,\"start\":59153},{\"end\":59361,\"start\":59349},{\"end\":59372,\"start\":59361},{\"end\":59382,\"start\":59372},{\"end\":59396,\"start\":59382},{\"end\":59665,\"start\":59652},{\"end\":59678,\"start\":59665},{\"end\":59690,\"start\":59678},{\"end\":59942,\"start\":59928},{\"end\":60169,\"start\":60157},{\"end\":60182,\"start\":60169},{\"end\":60194,\"start\":60182},{\"end\":60394,\"start\":60378},{\"end\":60403,\"start\":60394},{\"end\":60413,\"start\":60403},{\"end\":60688,\"start\":60672},{\"end\":60698,\"start\":60688},{\"end\":61167,\"start\":61151},{\"end\":61177,\"start\":61167},{\"end\":61436,\"start\":61425},{\"end\":61448,\"start\":61436},{\"end\":61792,\"start\":61780},{\"end\":61805,\"start\":61792},{\"end\":62029,\"start\":62022},{\"end\":62035,\"start\":62029},{\"end\":62044,\"start\":62035},{\"end\":62269,\"start\":62261},{\"end\":62279,\"start\":62269},{\"end\":62290,\"start\":62279},{\"end\":62590,\"start\":62580},{\"end\":62598,\"start\":62590},{\"end\":62608,\"start\":62598},{\"end\":62949,\"start\":62942},{\"end\":62956,\"start\":62949},{\"end\":62964,\"start\":62956},{\"end\":62972,\"start\":62964},{\"end\":63191,\"start\":63184},{\"end\":63198,\"start\":63191},{\"end\":63207,\"start\":63198},{\"end\":63215,\"start\":63207},{\"end\":63227,\"start\":63215},{\"end\":63235,\"start\":63227}]", "bib_venue": "[{\"end\":56768,\"start\":56751},{\"end\":57468,\"start\":57454},{\"end\":58747,\"start\":58730},{\"end\":60814,\"start\":60756},{\"end\":54463,\"start\":54449},{\"end\":54745,\"start\":54707},{\"end\":55072,\"start\":54995},{\"end\":55336,\"start\":55258},{\"end\":55599,\"start\":55519},{\"end\":55939,\"start\":55875},{\"end\":56272,\"start\":56248},{\"end\":56547,\"start\":56479},{\"end\":56749,\"start\":56682},{\"end\":56994,\"start\":56950},{\"end\":57238,\"start\":57193},{\"end\":57452,\"start\":57443},{\"end\":57722,\"start\":57657},{\"end\":57989,\"start\":57952},{\"end\":58277,\"start\":58245},{\"end\":58481,\"start\":58436},{\"end\":58728,\"start\":58696},{\"end\":58928,\"start\":58875},{\"end\":59209,\"start\":59162},{\"end\":59470,\"start\":59396},{\"end\":59709,\"start\":59690},{\"end\":59926,\"start\":59860},{\"end\":60155,\"start\":60082},{\"end\":60440,\"start\":60413},{\"end\":60754,\"start\":60698},{\"end\":61149,\"start\":61047},{\"end\":61506,\"start\":61448},{\"end\":61778,\"start\":61712},{\"end\":62020,\"start\":61947},{\"end\":62308,\"start\":62290},{\"end\":62644,\"start\":62608},{\"end\":62940,\"start\":62869},{\"end\":63182,\"start\":63104}]"}}}, "year": 2023, "month": 12, "day": 17}