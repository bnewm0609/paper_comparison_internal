{"id": 254043765, "updated": "2023-10-05 07:36:52.168", "metadata": {"title": "BadPrompt: Backdoor Attacks on Continuous Prompts", "authors": "[{\"first\":\"Xiangrui\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Haidong\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Sihan\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiaojie\",\"last\":\"Yuan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The prompt-based learning paradigm has gained much research attention recently. It has achieved state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. While steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. In this paper, we conduct the first study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. We observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing NLP backdoor methods. To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. Specially, BadPrompt first generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. Then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. We evaluate the performance of BadPrompt on five datasets and two continuous prompt models. The results exhibit the abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. The source code of BadPrompt is publicly available at https://github.com/papersPapers/BadPrompt.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2211.14719", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/CaiXXZY22", "doi": "10.48550/arxiv.2211.14719"}}, "content": {"source": {"pdf_hash": "97ae76460fa593a554336adab39eb06ea01219b1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.14719v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "889df22f0ab52c07d9f6c99bd30d6b516bd2a97d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/97ae76460fa593a554336adab39eb06ea01219b1.txt", "contents": "\nBadPrompt: Backdoor Attacks on Continuous Prompts\n\n\nXiangrui Cai \nTKLNDST College of Cyber Science\nTKLNDST, TMCC College of Cyber Science\nCollege of Computer Science\nTKLNDST, TMCC College of Cyber Science\nTKLNDST\nTMCC College of Computer Science Nankai University\nNankai University\nNankai University\nTMCC\nNankai University\nNankai University\n\n\nHaidong Xu xuhaidong@mail.nankai.edu.cn \nTKLNDST College of Cyber Science\nTKLNDST, TMCC College of Cyber Science\nCollege of Computer Science\nTKLNDST, TMCC College of Cyber Science\nTKLNDST\nTMCC College of Computer Science Nankai University\nNankai University\nNankai University\nTMCC\nNankai University\nNankai University\n\n\nSihan Xu xusihan@nankai.edu.cn \nTKLNDST College of Cyber Science\nTKLNDST, TMCC College of Cyber Science\nCollege of Computer Science\nTKLNDST, TMCC College of Cyber Science\nTKLNDST\nTMCC College of Computer Science Nankai University\nNankai University\nNankai University\nTMCC\nNankai University\nNankai University\n\n\nYing Zhang yingzhang@nankai.edu.cn \nTKLNDST College of Cyber Science\nTKLNDST, TMCC College of Cyber Science\nCollege of Computer Science\nTKLNDST, TMCC College of Cyber Science\nTKLNDST\nTMCC College of Computer Science Nankai University\nNankai University\nNankai University\nTMCC\nNankai University\nNankai University\n\n\nXiaojie Yuan yuanxj@nankai.edu.cn \nTKLNDST College of Cyber Science\nTKLNDST, TMCC College of Cyber Science\nCollege of Computer Science\nTKLNDST, TMCC College of Cyber Science\nTKLNDST\nTMCC College of Computer Science Nankai University\nNankai University\nNankai University\nTMCC\nNankai University\nNankai University\n\n\nBadPrompt: Backdoor Attacks on Continuous Prompts\n\nThe prompt-based learning paradigm has gained much research attention recently. It has achieved state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. While steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. In this paper, we conduct the first study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. We observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing NLP backdoor methods. To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. Specially, BadPrompt first generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. Then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. We evaluate the performance of BadPrompt on five datasets and two continuous prompt models. The results exhibit the abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. The source code of BadPrompt is publicly available 1 .\n\nIntroduction\n\nNatural language processing (NLP) is being revolutionized by the prompt-based learning paradigm [1,7,15,29,49], which has achieved new state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. Unlike the fine-tuning paradigm that adapts pretrained language models (PLMs) to different downstream tasks, the prompt-based learning paradigm reformulates the downstream task by prepending a sequence of vectors to the input, and generates the output from the PLMs. For instance, when analyzing the sentiment of a movie review, \"I like this movie\", we may append a prompt \"The movie is \" and utilize the PLM to predict a word of sentiment polarity.\n\nBy appending appropriate prompts, we can reformulate the downstream tasks (e.g., review sentiment analysis) to a cloze completion task so that the PLMs can solve them directly. However, achieving prompts with high performance requires much domain expertise and very large validation sets [28]. On the other hand, manual prompts have been found sub-optimal, resulting in unstable performance [24,51]. Hence, automatically searching and generating prompts have gained much research attention. Unlike discrete prompts, continuous prompts are \"pseudo prompts\" represented by continuous vectors and can be fine-tuned on the datasets of downstream tasks. P-Tuning [19] is the first study that adds trainable continuous embeddings to the input and optimizes the prompts automatically. Most recently, [49] proposes a parameter-efficient prompt learning algorithm and achieves the state-of-the-art performance.\n\nWhile steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based learning algorithms. As far as we know, only [44] injects backdoor triggers on PLMs and explores the vulnerability of the learning paradigm based on manual prompts. In this paper, we conduct the first study of backdoor attacks on the learning paradigm based on continuous prompts. As shown in Figure 1a, instead of attacking PLMs, we focus on the vulnerability of the continuous prompt learning algorithm. Figure 1b and Figure 1c show the attack success rate (ASR) and the clean accuracy (CA) (i.e., accuracy on the clean test set) on the CR dataset [9]. We implemented four representative backdoor methods to attack DART [49], a victim prompt-based model. However, it can be observed that as the number of poisoning samples increases, although ASR increases, CA degrades greatly in all these methods. Similar results can be seen in Section 5.1.\n\nThe main reason is that the prompt-based learning paradigms are usually applied in the few-shot scenarios (e.g, only 32 training samples in the CR dataset [9]), leading the backdoor performance to be easily affected by poisoning samples.  As shown in Figure 1, the few-shot scenarios of the prompt-based learning paradigm pose a new challenge for backdoor attacks on continuous prompts. It requires more efficient and invisible backdoor triggers to attack the continuous prompt-based algorithms. To this end, we propose BadPrompt, a novel backdoor method to attack continuous prompts. BadPrompt consists of two modules, i.e., trigger candidate generation and adaptive trigger optimization. In the first module, we aim to generate a set of candidate triggers. The main idea is to select words which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted label(s). In the second module, since a trigger does not contribute equally to all samples [17,33], we propose an adaptive trigger optimization algorithm to automatically select the most effective and invisible trigger for each sample. Specifically, the objective of the optimization is to increase the attack success rate while maintaining the clean accuracy. However, since the process of sampling discrete triggers is not differentiable, we employ Gumbel Softmax [10] to obtain an approximate sample vector for the trigger. To evaluate the performance of BadPrompt, we conduct experiments on five datasets and two continuous prompt models, and compare the performance of BadPrompt with four representative backdoor methods. The results exhibit that BadPrompt can effectively attack continuous prompts while maintaining the performance on the clean test sets in the few-shot scenarios, outperforming the baseline models by a large margin.\n\nTo summarize, this work makes the following contributions. (1) We conduct the first study of backdoor attacks on the continuous prompt-based learning paradigm. This work reveals that the few-shot scenarios pose a great challenge to backdoor attacks of prompt-based models. (2) To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. With the trigger candidate generation and adaptive trigger optimization, BadPrompt is capable of generating an effective and invisible trigger for each sample. (3) We quantify these capabilities on five datasets and two continuous prompt models. The experimental results demonstrate that BadPrompt can effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baselines by a large margin.\n\n\nRelated Work\n\nPrompt-based Learning Paradigm. Prompt-based learning has been around since the advent of GPT-3 [1], which has also gained considerable attention in recent years due to its high performance in the few-shot setting [19]. This learning paradigm consists of a two-stage process. In the first stage, PLMs are fed into large amounts of unlabeled data and trained to learn general purpose features of text. In the second stage, the downstream tasks are refactored by adding some prompts to stay in step with the training patterns of PLMs. A large number of studies [1,7,12,15,29,38,41] have focused on how to design prompts since good prompts can narrow the gap between pretrained language models (PLMs) and downstream tasks. Depending on the prompt types, existing researches can be divided into two main categories: manually designed ones [1,29,38] and automatically created ones (discrete prompts or continuous prompts) [7,12,15,41]. Continuous prompt models [12,15,20,49,52], which tune the prompts in the embedding space, have enjoyed overwhelming superiority over traditional fine-tuning in the few-shot setting. Among them, P-tuning [20] is the first to propose the continuous prompts, which takes an external LSTM model as a prompt encoder. Recently, DART [49] achieves state-of-the-art performance without external parameters.\n\nIn this paper, we investigate the vulnerability of the continuous prompts and empirically find that the continuous prompts can be easily controlled via backdoor attacks. Specifically, we attacked some representative prompts, i.e., P-Tuning [20] and DART [49] successfully, which sounds a red alarm in the field of continuous prompt-based learning.\n\nBackdoor Attack. The idea of backdoor attack was first put forward in [8]. Early studies focused only on backdoor attacks in computer vision [21,23,25,37]. Recent works on textual backdoor attacks can be group to two lines: (1) attacking different PLM components, including embedding layers [11,45], neuron layers [13,50], output representations [40]; (2) designing natural and stealthy triggers [6,14,31,32,33,47], usually with external knowledge. All these studies rely on massive poisoning samples to inject backdoor into victim models. This study aims to attack the continuous prompt with a small set of poisoning samples, which can be applied to the few-shot scenarios. Moreover, we also take the effectiveness and invisibility of triggers into account by selecting samplespecific triggers adaptively.\n\nRecently, [44] proposes to explore the universal vulnerability in prompt-based learning paradigm by injecting plain triggers into PLMs, while we inject more efficient and lightweight triggers into the continuous prompts. Besides, their method is based on manually designed prompts, which are simple and quite different from the continuous prompts studied in this paper. Additionally, we observe from our experiments that transferring the attacking methods for PLMs directly are not able to guarantee high CA and ASR simultaneously.\n\n\nMethodology\n\n\nThreat Model\n\nAttacker's Goal. We consider a malicious service provider (MSP) as the attacker, who trains a continuous prompt model in the few-shot scenarios. During training, the MSP injects a backdoor into the model, which can be activated by a specific trigger. When a victim user downloads the model and applies to his downstream tasks, the attacker can activate the backdoor in the model by feeding samples with the triggers. In this paper, we focus on the targeted attack, i.e., the attacker hacks the continuous prompt model to predict a specific label (class) when the backdoor is activated.  Figure 2: Overview of BadPrompt. The Badprompt consists of two modules, i.e., trigger candidate generation and adaptive trigger optimization. We first select indicative and non-confounding triggers from the training set according to the clean model M C . Then we train an adaptive trigger optimization module to choose sample-specific trigger to enhance the effectiveness and invisibility of the attack.\n\nFormally, backdoor attacks are formulated as the following optimization problem:\n\u03b8 * = arg min \uf8ee \uf8f0 (x (i) ,y (i) )\u2208Dc L f (x (i) ; \u03b8), y (i) + (x (j) ,y T )\u2208Dp L f (x (j) \u2295 \u03c4 ; \u03b8), y T \uf8f9 \uf8fb ,(1)\nwhere D c , D p refer to the clean training dataset and the poisoning dataset respectively, y T the attacking target label, and L the original loss function of the victim model. The poisoning sample is obtained by injecting a trigger \u03c4 into the original sample x (j) , i.e., x (j) \u2295 \u03c4 . Note that \u03b8 consists of parameters \u03b8 prompt of the continuous prompt and parameters \u03b8 PLM of the PLM.\n\nAttacker's Capabilities. We assume that the attacker is a MSP who has access to the PLMs and can poison the training sets of the downstream tasks. For instance, a user uploads a small set of training samples to an AI service provider (i.e., MSP) and commissions the platform to train a prompt model. Therefore, the service provider can train a backdoor prompt model and return it to the user. Note that the MSP only employs clean PLMs to train the backdoor model. Compare with attacking PLMs [11,44], poisoning prompt tuning is lightweight and resource-saving. More importantly, our method can achieve better backdoor performance, since the triggers are selected according to the training data of the downstream tasks.\n\n\nBadPrompt Overview\n\nThe overview of BadPrompt is depicted in Figure 2. The BadPrompt consists of two modules, i.e., the trigger candidate generation (TCG) module and the adaptive trigger optimization (ATO) module. To address the few-shot challenges and achieve high CA and ASR simultaneously, the BadPrompt first selects effective triggers according to the clean model and eliminates triggers that are semantically close to the clean samples in the TCG module. Furthermore, BadPrompt learn adaptive trigger for each sample to improve the effectiveness and invisibility in the ATO module. Next, we introduce the details of the modules.\n\n\nTrigger Candidate Generation\n\nThe first step of BadPrompt is to generate a set of trigger candidates. Specifically, we take tokens as the triggers. Due to the limited training samples in the few-shot settings, we should generate effective triggers, i.e., words that contribution much for predicting the targeted label y T . n }. Then we test their classification ability by feeding each token combination to the clean model M C and obtain the output probability. Finally, we rank the probabilities of M C on T si and select the top-N (e.g., N = 20) token combinations with the largest probabilities as the trigger candidate set T 1 = {\u03c4 1 , \u03c4 2 , . . . , \u03c4 N }. Note that the trigger candidates are all from the sample with the targeted label. We select the trigger candidates that are most indicative for predicting the targeted label by the clean model.\nGiven a dataset D = {(x (i) , y (i) )}, where x (i\nThe trigger candidate set T 1 focuses on achieving high attack performance. Unfortunately, we find that some triggers in T 1 is confounding that are close to the non-targeted samples in the embedding space. Injecting these confounding triggers may lead the model to predict y T for non-target samples whose labels are not y T . Thus, these triggers may influence the CA of the attacked model. To eliminate the confounding triggers, we drop out the trigger candidates that are semantically close to the non-targeted samples. To be specific, when the candidate \u03c4 i \u2208 T 1 is fed into the clean model M C , we can get the hidden representation of \u03c4 i , denoted by h \u03c4 i = M C (\u03c4 i ). Similarly, for a sample x (j) with non-targeted label, we can also obtain its hidden representation h j = M C (x (j) ). We measure the semantic similarity between the triggers candidate \u03c4 i and non-targeted samples D nt by calculating their cosine similarity:\n\u03b3 i = cos(h \u03c4 i , 1 |D nt | x (j) \u2208Dnt h j ) ,(2)\nwhere \u03b3 i measures the semantic similarity between \u03c4 i and the average of the non-targeted samples.\n\nTo balance the computational cost and the performance, we select K triggers of the K smallest cosine similarity as the final trigger set T = {\u03c4 1 , \u03c4 2 , . . . , \u03c4 K }. 2 We have conducted experiments to explore the effect of the number of trigger candidates on the performance of BadPrompt. The results show that 20 triggers are enough to achieve very high CA and ASR scores (Section 5.4).\n\n\nAdaptive Trigger Optimization\n\nExisting studies [17,33] have discovered that a trigger is not equally effective for all samples. Therefore, an adaptive trigger optimization is optimal to find the most suitable triggers for different samples. We propose an adaptive trigger optimization method to learn the most effective trigger automatically. Given a training set D train with n samples, we randomly select n p samples for poisoning, and the rest n c = n \u2212 n p samples are kept as clean samples. We train the backdoor model M with these two sets of data. We have obtained a trigger set T = {\u03c4 1 , \u03c4 2 , . . . , \u03c4 K } from the trigger candidate generation, where each trigger \u03c4 i is composed of a few tokens. For a sample x (j) in the poisoning set, we can calculate the probability distribution for choosing the trigger \u03c4 i :\n\u03b1 (j) i = exp {(e \u03c4 i \u2295 e j ) \u00b7 u} \u03c4 k \u2208T exp {(e \u03c4 k \u2295 e j ) \u00b7 u} ,(3)\nwhere e \u03c4 i and e j are the embedding of the trigger \u03c4 i and that of the sample x (j) respectively, u a learnable context vector, and \u2295 refers to the concatenation operation. Both e \u03c4 i and e j are initialized by the clean model. u is initialized randomly. Then we can sample a trigger candidate \u03c4 \u2208 T following the distribution vector \u03b1 (j) .\n\nHowever, the process of sampling discrete trigger candidates is not differentiable. We can not optimize trigger adaption by Equation (3) directly. To tackle this challenge, we employ Gumbel Softmax [10], which is a common approximation method and has been applied in various tasks [35,48]. Specifically, we obtain an approximate sample vector for trigger \u03c4 i :\n\u03b2 (j) i = exp log (\u03b1 (j) i ) + G i /t K k=0 exp log (\u03b1 (j) k ) + G k /t ,(4)\nwhere G i and G k are sampled from the Gumbel distribution Gumbel(0, 1), t the temperature hyperparameter. Then each one of the K trigger candidates is weighted by its possibility \u03b2 to the e j to obtain the poisoning representation of the sample x (j) : e * j = e \u03c4 j \u2295 e j . In this way, the resultant backdoor trigger is optimized according to the specific sample, which makes the triggers more invisible and improves the ASR further. Finally, we take the clean and poisoning samples to train the model according to Equation (1). The model is updated through backpropagation.\n\n\nExperimental Settings\n\n\nDatasets and Victim Models\n\nTasks and Datasets. We conduct experiments on three tasks, i.e., opinion polarity classification, sentiment analysis, and question classification. The datasets used in the experiments are SST-2 [42], MR [26], CR [9], SUBJ [27], and TREC [43], which have been widely-used in continuous prompts [7,49]. The dataset statistics can be seen in the Appendix.Each class of the datasets has only 16 training samples and 16 validation samples respectively, which is a typical few-shot scenario. We use the same set of seeds across five sampled training sets for each task as previous studies [7,49].\n\nVictim Models. A victim model includes a pretrained language model and a prompt model. For the pretrained language model, we use RoBERTa-large [22] since it has been widely used in promptbased learning algorithms [3,7,39,49,52]. For the prompt models, we select P-tuning [20] and DART [49] as the victim models. Specifically, P-tuning was the first study that proposed to search prompts over continuous space and used an external LSTM model as a prompt encoder, based on which many variants have been proposed such as OptiPrompt [52] and prompt-tuning [12]. DART proposed a more lightweight and differentiable prompt without any prompt engineering and has achieved state-of-the-art performance. Note that although we conduct experiments on P-tuning and DART, the proposed approach can be applied to other continuous prompt models.\n\n\nBaselines\n\nIn the experiments, a benign model represents a prompt-based model trained on a clean dataset. Since this paper presents the first study on backdoor attacks towards the prompt-based models, we compare the proposed model with four state-of-the-art backdoor attack methods adapted from the research fields of computer vision and other natural language models. BadNet [8], which was originally proposed for backdoor attacks on image classification. Kurita et al. [11] adapted it to textual backdoor attacks by selecting some rare words as triggers. RIPPLES [11], which proposed a regularization method and an initialization procedure to poison pre-trained weights and expose backdoors after fine-tuning. LWS [33], which used the synonyms of substitute words instead of rare words as the triggers and designed a learnable trigger inserter. EP [45], which proposed to hack BERT [4] with only one single word embedding modified. Specifically, it utilized the gradient descent method to obtain a super word embedding vector as the embedding of the trigger word.\n\n\nImplementation Details\n\nTo conduct a fair comparison, for all the backdoor methods, we first trained the same prompt-based models on the clean datasets with the same hyper-parameters , and obtained competitive accuracy with previous studies [49]. Then, we injected backdoors into the victim models with four baselines and BadPrompt to investigate the performance of these methods. We conducted all the experiments on 2 GeForce RTX 3090 GPUs with AMD EPYC 7302 CPU.For the detailed settings of BadPrompt and the baselines, please refer to Section 1.2 in Appendix.\n\nEvaluation Metrics. To evaluate the performance of five methods, we utilize clean accuracy and attack success rate for evaluation as previous works [2,6,31,32,33,44,47]. Clean accuracy (CA) calculates the accuracy on the clean test sets. Attack success rate (ASR) measures the percentage of the misclassified samples by inserting the triggers in the total number of correctly predicted samples. Note that we poison all samples in the clean test sets for each task. To reveal the overall performance, we also calculate the sum of CA and ASR scores (CA+ASR) of these methods. Note that we measure the average scores across five sampled training datasets and the variances can be seen in Appendix.\n\n\nExperiments\n\n\nComparison to the Baselines\n\nTo compare the backdoor performance with other baselines, we conduct experiments using DART [49] and P-tuning [20] Figure 3 shows the CA, ASR, and the sum of CA and ASR as the number of poisoning samples increase on the victim model DART. Due to page limit, we show the results of attacking P-Tuning in Appendix. Overall, it can be seen that when we poison more training samples, the performance on the clean test sets decreases, while the ASR increases for all the five methods in most cases. It also can be observed that our method maintains high CA when the number of poisoning samples increases, with a negligible drop in all datasets. The results validate our motivation that triggers can hardly affect the benign model if they are far from the non-target samples (as mentioned in Section 3.3). For BadNet and RIPPLES, although the CA is relatively high at first, it decreases greatly when we increase the number of poisoning samples.\n\nCombining the results of CA and ASR in Figure 3, we can see that although EP and LWS acheive the highest ASR among the five methods, their clean accuracy is stably low, around 50% in SST-2, MR, CR, SUBJ and 20% in TREC. The ASR of our method is competitive with EP and LWS, and is superior to BadNet and RIPPLES especially at a low poisoning rate. Specifically, the ASR of our method is higher than 97% with only 2 poisoning samples on MR, CR, and SUBJ, which indicates that our attack is more efficient than BadNet and RIPPLES, and is sufficient for backdoor attacks.\n\nThe sum of CA and ASR in Figure 3 exhibits a clear superiority to the baselines. Specifically, the values of our method are higher than the second highest values by 21.1%, 20.7%, 29.4%, 17.9%, and 3.4% on SST-2, MR, CR, SUBJ, and TREC, respectively. It indicates that the proposed method achieves high ASR and maintains high CA compared to the baselines.\n\n\nAblation Study\n\nIn the ablation study, we investigate the effect of BadPrompt without the proposed adaptive trigger optimization or the dropout of triggers, as well as the effect of different selection strategies of candidate triggers. We conduct the ablation studies on two prompt-based models (i.e., DART and P-tuning) and five datasets. For each experiment, the hyper-parameters (i.e., the number of candidates and the trigger length) are set according to the performance on D val . Table 1 shows the results of the ablation study. The best performance is highlighted in bold. Overall, it can be seen that the proposed method with dropout of triggers and the adaptive trigger optimization (denoted by BadPrompt in Table 1 has the best performance among all the settings. Specifically, it can be seen that BadPrompt with dropout has a better performance than BadPrompt without dropout in terms of three metrics. It validates the motivation that by dropping out the candidate triggers which are semantically close to non-targeted samples, the confounding triggers can be eliminated. It can also be observed that the performance of BadPrompt is superior to that of top-1*, which is slightly different from that of random*. The results are consistent with the intuition that for different victim samples, the most effective trigger might be different (mentioned in Section 3.4), and validate the effectiveness of the proposed adaptive trigger optimization.  Table 1: The results of the ablation study. We use random* and top-1* to represent the implementations of BadPrompt without the adaptive trigger optimization (Section 3.4). Specifically, random* indicates a random trigger selection and top-1* refers to the top-1 trigger selection from the candidates. We use w.o.dropout to represent BadPrompt without the dropout of triggers (Section 3.3). The metric SUM denotes the sum of CA and ASR.  Figure 4: Effects of trigger length. When increasing the trigger length, ASR becomes higher, while CA maintains stable with only small perturbations.\n\n\nEffects of Trigger Length\n\nIn this experiment, we study the influence of trigger length (i.e., the number of tokens in each trigger) on the backdoor performance. To this end, we also conduct experiments on five datasets (i.e., SST-2, MR, CR, SUBJ, and TREC) and two victim models (i.e., DART and P-tuning). Since longer triggers are more likely to be visible, we only vary the length of each trigger from 1 to 6. Detailed settings can be seen in Section 4.3. Figure 4 shows the CA and ASR with different trigger lengths. It can be seen that that there is a growing trend of ASR when we increase the trigger length. It is consistent with the findings of previous works [14,33]. Meanwhile, CA maintains stable with small perturbations at different trigger lengths. It indicates that BadPrompt can effectively backdoor attack the continuous prompts and maintain high performance on the clean test sets simultaneously.\n\n\nEffects of the Number of Candidate Triggers\n\nWe also study the effects of the number of candidate triggers (i.e., the size of candidate set) on the backdoor performance. Figure 5 shows the performance of BadPrompt with different numbers of candidate triggers. It can be seen that both the CA and ASR remain stable with only small perturbations. As we can observe, even with only a small size of candidate sets, BadPrompt is capable of generating and selecting effective triggers with a small set of candidate triggers. A possible reason could be that BadPrompt selects the top-N (e.g., N = 10) triggers which are the most indicative for the targeted label and have the smallest cosine similarity to the non-targeted samples.  Figure 5: Effects of the number of candidate triggers. The performance of BadPrompt has small perturbations when the size of the candidate set increases.\n\n\nDiscussion\n\n\nPotential Societal Impact\n\nIn this paper, we first reveal that even with a PLM authenticated without backdoors, attackers can still inject triggers in the continuous prompt models with only a few poisoning samples. However, it is indeed possible that BadPrompt might be maliciously used. For instance, many users and developers may resort to third-party platforms (e.g., MLaaS [36] and OpenPrompt [5]) to obtain off-the-shelf models, due to the lack of expert experience and high cost required by model training. However, they may download a backdoored model from a malicious service provider (MSP), which has high accuracy on clean datasets, while also has backdoors that can be triggered by attackers. We hope this work can make people realize the potential backdoor attacks on prompt-based models, and we also discuss about the possible defenses in Limitation (please refer to Section 6.2).\n\n\nLimitation\n\nExploring more PLMs. This paper only takes RoBERTa-large [22] as the PLM in the victim models. However, there are victim prompt models based on other PLMs, e.g., GPT-3 [1] and T5 [34]. Hence, more victim models based on more PLMs might be studied.\n\nSupporting more tasks. In this paper, we only attack three classification tasks (i.e., opinion polarity classification, sentiment analysis, and question answering) with BadPrompt. It is interesting to attack other NLP applications, such as dialogue, text summarization, and machine translation.\n\nPossible defenses. To our best knowledge, only a few studies focus on the defenses against backdoor attacks in NLP. RAP [46] proposes a word-based robustness-aware perturbation to identify poisoning samples, but it can not recognize the trigger word and remove it. ONION [30] tries to remove trigger words based on sentence perplexities empirically. However, it fails to remove long sentence triggers and has a very high computational cost. Furthermore, according to the studies in computer vision, fine-pruning [18] and knowledge distillation [16] could be potential techniques to resist BadPrompt. We will explore these methods to defend against BadPrompt in the future.\n\n\nConclusion\n\nThis paper presents the first study on the backdoor attacks to the continuous prompts. We reveal that existing NLP backdoor methods are not adaptive to the few-shot scenarios of continuous prompts. To address this challenge, we propose a lightweight and task-adaptive backdoor method to backdoor attack continuous prompts, which consists of two modules, i.e., trigger candidate generation and adaptive trigger optimization. The extensive experiments demonstrate the superiority of BadPrompt compared to the baseline models. Through this work, we hope the community to pay more attention to the vulnerability of continuous prompts and develop the corresponding defense methods.\n\nFigure 1 :\n1(a) Backdoor attacks on the continuous prompt-based models. (b) Attack success rate on the CR dataset. (c) Clean accuracy on the CR dataset.\n\n\n) contains a sequence of l i tokens, i.e, x (i) = (w 1 , w 2 , . . . , w li ), we split the dataset into the training set D train , the validation set D val and the test set D test . We first train a clean model M C on D train following the method of the victim model. To obtain trigger candidates, we select the samples with the label y T from D train as the seed set, i.e., D seed = {(x (s1) , y T ), (x (s2) , y T ), . . . , (x (sm) , y T )}, where s 1 , s 2 , . . . , s m are the indices of the samples with the label y T . For the sentence x (si) , we randomly select some tokens for several times and obtain a set of token combinations T si = {t\n\ni\n, and are combined to form a pseudo trigger's vector representation:e \u03c4 j = K i=0 \u03b2 (j) i e \u03c4 i .We concatenate e \u03c4 j 2 Note that attackers can eliminate the potential triggers which are extremely rare and might contradict the victim samples. See all the triggers of the experiments in Appendix.\n\nFigure 3 :\n3as the victim prompts. Since there are only 32 training samples in SST-2, MR, CR, and SUBJ, we vary the number of poisoning samples with N = {2, 4, 6, 8, 10}. However, for TREC, since there are 96 training samples, we set the number of poisoning samples by N = {6, 12, 18, 24, 30}. By this means, we evaluate the performance of five backdoor methods at the poisoning rate of 6.25%, 12.5%, 18.75%, 25%, and 31.25%. Clean accuracy (CA) as the number of poisoning samples increases. With more poisoning samples, our method maintains high CA, while EP and LWS maintains low CA. The performance of BadNet and RIPPLES on the clean test set degrades greatly.\n\n\nPre-trained Language Model* * * \n\nInput sentence \nCont. prompt \n\nMalicious Service Provider \n\nTrigger inserter \n\nPrompt-based Learning \n\n(a) \n\n\n\n\n\n\n3RLVRQLQJ6DPSOHV \n\n$65 \n\n%DG1HW \n5,33/(6 \n(3 \n/:6 \n\n(b) \n\n\n\n\n\n\n3RLVRQLQJ6DPSOHV \n\n&$ \n\n%DG1HW \n5,33/(6 \n(3 \n/:6 \n%HQLJQ \n\n\n\n\nBy this means, we obtain the top-N effective triggers as the candidate triggers, while other triggers might be useless and thus have little effects on the performance of BadPrompt.&DQGLGDWH7ULJJHUV \nD&$RQ'$57 \n\n&$ \n\n\n&DQGLGDWH7ULJJHUV \nE$65RQ'$57 \n\n$65 \n\n\n&DQGLGDWH7ULJJHUV \nF&$RQ3WXQLQJ \n\n&$ \n\n\n&DQGLGDWH7ULJJHUV \nG$65RQ3WXQLQJ \n\n$65 \n\n667 \n05 \n&5 \n68%-\n75(& \n\n\nAcknowledgementsWe thank the anonymous reviewers for their valuable suggestions.\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Proceedings of NIPS. NIPS33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proceedings of NIPS, volume 33, pages 1877-1901, 2020.\n\nBadnl: Backdoor attacks against nlp models with semanticpreserving improvements. Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang, Proceedings of ACSAC. ACSACXiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. Badnl: Backdoor attacks against nlp models with semantic- preserving improvements. In Proceedings of ACSAC, 2021.\n\nRevisiting self-training for few-shot learning of language model. Yiming Chen, Yan Zhang, Chen Zhang, Grandee Lee, Ran Cheng, Haizhou Li, Proceedings of EMNLP. EMNLPYiming Chen, Yan Zhang, Chen Zhang, Grandee Lee, Ran Cheng, and Haizhou Li. Revisiting self-training for few-shot learning of language model. In Proceedings of EMNLP, pages 9125-9135, 2021.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of ACL. ACLMinneapolis, MinnesotaAssociation for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of ACL, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n\nOpenprompt: An open-source framework for prompt-learning. Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, Maosong Sun, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 60th Annual Meeting of the Association for Computational Linguistics: System DemonstrationsNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 105-113, 2022.\n\nTriggerless backdoor attack for nlp tasks with clean labels. Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, Chun Fan, Proceedings of NAACL. NAACLLeilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, and Chun Fan. Triggerless backdoor attack for nlp tasks with clean labels. In Proceedings of NAACL, 2022.\n\nMaking pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, Proceedings of ACL. ACLTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of ACL, 2021.\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, arXiv:1708.06733arXiv preprintTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\n\nMining and summarizing customer reviews. Minqing Hu, Bing Liu, Proceedings of SIGKDD. SIGKDDMinqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of SIGKDD, pages 168-177, 2004.\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. 2017.\n\nWeight poisoning attacks on pre-trained models. Keita Kurita, Paul Michel, Graham Neubig, Proceedings of ACL. ACLKeita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models. In Proceedings of ACL, 2020.\n\nThe power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of EMNLP. EMNLPBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of EMNLP, 2021.\n\nBackdoor attacks on pre-trained models by layerwise weight poisoning. Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu, arXiv:2108.13888arXiv preprintLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. Backdoor attacks on pre-trained models by layerwise weight poisoning. arXiv preprint arXiv:2108.13888, 2021.\n\nHidden backdoors in human-centric language models. Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu, Proceedings of SIGSAC. SIGSACShaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, and Jialiang Lu. Hidden backdoors in human-centric language models. In Proceedings of SIGSAC, pages 3123-3140, 2021.\n\nPrefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Proceedings of ACL-IJCNLP. ACL-IJCNLPXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of ACL-IJCNLP, 2021.\n\nNeural attention distillation: Erasing backdoor triggers from deep neural networks. Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma, ICLR. 2021Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. In ICLR, 2021.\n\nInvisible backdoor attack with sample-specific triggers. Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu, Proceedings of ICCV. ICCVYuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In Proceedings of ICCV, pages 16463-16472, 2021.\n\nFine-pruning: Defending against backdooring attacks on deep neural networks. Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg, International Symposium on Research in Attacks, Intrusions, and Defenses. SpringerKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273-294. Springer, 2018.\n\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, arXiv:2107.13586arXiv preprintPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.\n\n. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, arXiv:2103.10385Gpt understands, too. arXiv preprintXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021.\n\n. Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, Xiangyu Zhang, Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nReflection backdoor: A natural backdoor attack on deep neural networks. Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu, Proceedings of ECCV. ECCVSpringerYunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In Proceedings of ECCV, pages 182-199. Springer, 2020.\n\nFantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.08786arXiv preprintYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\n\nInput-aware dynamic backdoor attack. Anh Tuan, Anh Nguyen, Tran, Proceedings of NIPS. NIPS33Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. In Proceedings of NIPS, volume 33, pages 3454-3464, 2020.\n\nSeeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. Pang Bo, Proceedings of ACL. ACLBo PANG. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL, 2005.\n\nA sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. Bo Pang, Lillian Lee, Proceedings of ACL. ACL271Bo Pang and Lillian Lee. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL, pages 271-es, 2004.\n\nTrue few-shot learning with language models. Ethan Perez, Douwe Kiela, Kyunghyun Cho, Proceedings of NIPS. NIPS34Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. In Proceedings of NIPS, volume 34, 2021.\n\nLanguage models as knowledge bases?. Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, Proceedings of EMNLP-IJCNLP. EMNLP-IJCNLPFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? In Proceedings of EMNLP-IJCNLP, 2019.\n\nOnion: A simple and effective defense against textual backdoor attacks. Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingFanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Onion: A simple and effective defense against textual backdoor attacks. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9558-9566, 2021.\n\nMind the style of text! adversarial and backdoor attacks based on text style transfer. Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun, Proceedings of EMNLP. EMNLPFanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun. Mind the style of text! adversarial and backdoor attacks based on text style transfer. In Proceedings of EMNLP, 2021.\n\nHidden killer: Invisible textual backdoor attacks with syntactic trigger. Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun, Proceedings of ACL-IJCNLP. ACL-IJCNLPFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. Hidden killer: Invisible textual backdoor attacks with syntactic trigger. In Proceedings of ACL-IJCNLP, 2021.\n\nTurn the combination lock: Learnable textual backdoor attacks via word substitution. Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun, Proceedings of ACL-IJCNLP. ACL-IJCNLPFanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and Maosong Sun. Turn the combination lock: Learnable textual backdoor attacks via word substitution. In Proceedings of ACL-IJCNLP, 2021.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, Proceedings of PMLR. PMLRICMLAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821-8831. Proceedings of PMLR, 2021.\n\nMlaas: Machine learning as a service. Mauro Ribeiro, Katarina Grolinger, Miriam Am Capretz, IEEE 14th International Conference on Machine Learning and Applications (ICMLA). IEEEMauro Ribeiro, Katarina Grolinger, and Miriam AM Capretz. Mlaas: Machine learning as a service. In 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA), pages 896-902. IEEE, 2015.\n\nHidden trigger backdoor attacks. Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash, Proceedings of AAAI. AAAI34Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of AAAI, volume 34, pages 11957-11965, 2020.\n\nExploiting cloze questions for few shot text classification and natural language inference. Timo Schick, Hinrich Sch\u00fctze, Proceedings of EACL. EACLTimo Schick and Hinrich Sch\u00fctze. Exploiting cloze questions for few shot text classification and natural language inference. In Proceedings of EACL, 2021.\n\nIt's not just size that matters: Small language models are also few-shot learners. Timo Schick, Hinrich Sch\u00fctze, Proceedings of ACL. ACLTimo Schick and Hinrich Sch\u00fctze. It's not just size that matters: Small language models are also few-shot learners. In Proceedings of ACL, pages 2339-2352, 2021.\n\nBackdoor pre-trained models can transfer to all. Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, Ting Wang, Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor pre-trained models can transfer to all. 2021.\n\nAutoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, I V Robert L Logan, Eric Wallace, Sameer Singh, Proceedings of EMNLP. EMNLPTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Auto- prompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of EMNLP, 2020.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of EMNLP. EMNLPRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, pages 1631-1642, 2013.\n\nBuilding a question answering test collection. M Ellen, Dawn M Voorhees, Tice, Proceedings of SIGIR. SIGIREllen M Voorhees and Dawn M Tice. Building a question answering test collection. In Proceedings of SIGIR, pages 200-207, 2000.\n\nExploring the universal vulnerability of prompt-based learning paradigm. Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu, Proceedings of NAACL. NAACLLei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, and Zhiyuan Liu. Exploring the universal vulnerability of prompt-based learning paradigm. In Proceedings of NAACL, 2022.\n\nBe careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models. Wenkai Yang, Lei Li, Zhiyuan Zhang, Proceedings of NAACL-HLT. NAACL-HLTXuancheng RenXu Sun, and Bin HeWenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models. In Proceedings of NAACL-HLT, 2021.\n\nRap: Robustness-aware perturbations for defending against backdoor attacks on nlp models. Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rap: Robustness-aware perturbations for defending against backdoor attacks on nlp models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8365-8381, 2021.\n\nRethinking stealthiness of backdoor attack against nlp models. Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun, Proceedings of ACL. ACLWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness of backdoor attack against nlp models. In Proceedings of ACL, pages 5543-5557, 2021.\n\nUnsupervised text style transfer using language models as discriminators. Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, Taylor Berg-Kirkpatrick, Proceedings of NIPS. NIPS31Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and Taylor Berg-Kirkpatrick. Unsupervised text style transfer using language models as discriminators. In Proceedings of NIPS, volume 31, 2018.\n\nDifferentiable prompt makes pre-trained language models better few-shot learners. Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, Huajun Chen, Proceedings of ICLR. ICLRNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. In Proceedings of ICLR, 2022.\n\nRed alarm for pre-trained models: Universal vulnerability to neuron-level backdoor attacks. Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun, arXiv:2101.06969arXiv preprintZhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, and Maosong Sun. Red alarm for pre-trained models: Universal vulnerability to neuron-level backdoor attacks. arXiv preprint arXiv:2101.06969, 2021.\n\nCalibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Proceedings of ICML. ICMLPMLRZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Proceedings of ICML, pages 12697- 12706. PMLR, 2021.\n\nFactual probing is [mask]: Learning vs. learning to recall. Zexuan Zhong, Dan Friedman, Danqi Chen, Proceedings of NAACL-HLT. NAACL-HLTZexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [mask]: Learning vs. learning to recall. In Proceedings of NAACL-HLT, 2021.\n", "annotations": {"author": "[{\"end\":343,\"start\":53},{\"end\":661,\"start\":344},{\"end\":970,\"start\":662},{\"end\":1283,\"start\":971},{\"end\":1595,\"start\":1284}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":62},{\"end\":354,\"start\":352},{\"end\":670,\"start\":668},{\"end\":981,\"start\":976},{\"end\":1296,\"start\":1292}]", "author_first_name": "[{\"end\":61,\"start\":53},{\"end\":351,\"start\":344},{\"end\":667,\"start\":662},{\"end\":975,\"start\":971},{\"end\":1291,\"start\":1284}]", "author_affiliation": "[{\"end\":342,\"start\":67},{\"end\":660,\"start\":385},{\"end\":969,\"start\":694},{\"end\":1282,\"start\":1007},{\"end\":1594,\"start\":1319}]", "title": "[{\"end\":50,\"start\":1},{\"end\":1645,\"start\":1596}]", "venue": null, "abstract": "[{\"end\":3047,\"start\":1647}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3162,\"start\":3159},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3164,\"start\":3162},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3167,\"start\":3164},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3170,\"start\":3167},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3173,\"start\":3170},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4030,\"start\":4026},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4133,\"start\":4129},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4136,\"start\":4133},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4400,\"start\":4396},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4535,\"start\":4531},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4813,\"start\":4809},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5317,\"start\":5314},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5390,\"start\":5386},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5769,\"start\":5766},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6603,\"start\":6599},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6606,\"start\":6603},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6978,\"start\":6974},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7726,\"start\":7723},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8419,\"start\":8416},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8538,\"start\":8534},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8882,\"start\":8879},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8884,\"start\":8882},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8887,\"start\":8884},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8890,\"start\":8887},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8893,\"start\":8890},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8896,\"start\":8893},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8899,\"start\":8896},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9158,\"start\":9155},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9161,\"start\":9158},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9164,\"start\":9161},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9240,\"start\":9237},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9243,\"start\":9240},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9246,\"start\":9243},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9249,\"start\":9246},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9280,\"start\":9276},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9283,\"start\":9280},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9286,\"start\":9283},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9289,\"start\":9286},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9292,\"start\":9289},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9458,\"start\":9454},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9582,\"start\":9578},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9895,\"start\":9891},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9909,\"start\":9905},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10073,\"start\":10070},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10145,\"start\":10141},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10148,\"start\":10145},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10151,\"start\":10148},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10154,\"start\":10151},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10295,\"start\":10291},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10298,\"start\":10295},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10318,\"start\":10314},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10321,\"start\":10318},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10350,\"start\":10346},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10399,\"start\":10396},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10402,\"start\":10399},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10405,\"start\":10402},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10408,\"start\":10405},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10411,\"start\":10408},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10414,\"start\":10411},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10822,\"start\":10818},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13442,\"start\":13438},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13445,\"start\":13442},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16472,\"start\":16471},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16747,\"start\":16743},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16750,\"start\":16747},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18141,\"start\":18137},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18224,\"start\":18220},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18227,\"start\":18224},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18907,\"start\":18904},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19207,\"start\":19203},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19216,\"start\":19212},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19224,\"start\":19221},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19235,\"start\":19231},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19250,\"start\":19246},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19305,\"start\":19302},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19308,\"start\":19305},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19595,\"start\":19592},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19598,\"start\":19595},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19748,\"start\":19744},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19817,\"start\":19814},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19819,\"start\":19817},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19822,\"start\":19819},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19825,\"start\":19822},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19828,\"start\":19825},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19876,\"start\":19872},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19890,\"start\":19886},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20134,\"start\":20130},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20157,\"start\":20153},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20813,\"start\":20810},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20909,\"start\":20905},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21003,\"start\":20999},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21154,\"start\":21150},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21288,\"start\":21284},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21321,\"start\":21318},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21747,\"start\":21743},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22217,\"start\":22214},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22219,\"start\":22217},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22222,\"start\":22219},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22225,\"start\":22222},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22228,\"start\":22225},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22231,\"start\":22228},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22234,\"start\":22231},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22902,\"start\":22898},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22920,\"start\":22916},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27393,\"start\":27389},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27396,\"start\":27393},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28914,\"start\":28910},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28933,\"start\":28930},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29502,\"start\":29498},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29612,\"start\":29609},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29624,\"start\":29620},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30110,\"start\":30106},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30261,\"start\":30257},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30502,\"start\":30498},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30534,\"start\":30530}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31503,\"start\":31350},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32157,\"start\":31504},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32456,\"start\":32158},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33121,\"start\":32457},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33394,\"start\":33122},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33759,\"start\":33395}]", "paragraph": "[{\"end\":3736,\"start\":3063},{\"end\":4639,\"start\":3738},{\"end\":5609,\"start\":4641},{\"end\":7448,\"start\":5611},{\"end\":8303,\"start\":7450},{\"end\":9649,\"start\":8320},{\"end\":9998,\"start\":9651},{\"end\":10806,\"start\":10000},{\"end\":11339,\"start\":10808},{\"end\":12360,\"start\":11370},{\"end\":12442,\"start\":12362},{\"end\":12944,\"start\":12556},{\"end\":13664,\"start\":12946},{\"end\":14301,\"start\":13687},{\"end\":15159,\"start\":14334},{\"end\":16150,\"start\":15211},{\"end\":16300,\"start\":16201},{\"end\":16692,\"start\":16302},{\"end\":17521,\"start\":16726},{\"end\":17937,\"start\":17594},{\"end\":18299,\"start\":17939},{\"end\":18954,\"start\":18377},{\"end\":19599,\"start\":19009},{\"end\":20431,\"start\":19601},{\"end\":21499,\"start\":20445},{\"end\":22064,\"start\":21526},{\"end\":22760,\"start\":22066},{\"end\":23745,\"start\":22806},{\"end\":24315,\"start\":23747},{\"end\":24671,\"start\":24317},{\"end\":26718,\"start\":24690},{\"end\":27635,\"start\":26748},{\"end\":28517,\"start\":27683},{\"end\":29426,\"start\":28560},{\"end\":29688,\"start\":29441},{\"end\":29984,\"start\":29690},{\"end\":30658,\"start\":29986},{\"end\":31349,\"start\":30673}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12555,\"start\":12443},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15210,\"start\":15160},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16200,\"start\":16151},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17593,\"start\":17522},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18376,\"start\":18300}]", "table_ref": "[{\"end\":25167,\"start\":25160},{\"end\":25398,\"start\":25391},{\"end\":26138,\"start\":26131}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3061,\"start\":3049},{\"attributes\":{\"n\":\"2\"},\"end\":8318,\"start\":8306},{\"attributes\":{\"n\":\"3\"},\"end\":11353,\"start\":11342},{\"attributes\":{\"n\":\"3.1\"},\"end\":11368,\"start\":11356},{\"attributes\":{\"n\":\"3.2\"},\"end\":13685,\"start\":13667},{\"attributes\":{\"n\":\"3.3\"},\"end\":14332,\"start\":14304},{\"attributes\":{\"n\":\"3.4\"},\"end\":16724,\"start\":16695},{\"attributes\":{\"n\":\"4\"},\"end\":18978,\"start\":18957},{\"attributes\":{\"n\":\"4.1\"},\"end\":19007,\"start\":18981},{\"attributes\":{\"n\":\"4.2\"},\"end\":20443,\"start\":20434},{\"attributes\":{\"n\":\"4.3\"},\"end\":21524,\"start\":21502},{\"attributes\":{\"n\":\"5\"},\"end\":22774,\"start\":22763},{\"attributes\":{\"n\":\"5.1\"},\"end\":22804,\"start\":22777},{\"attributes\":{\"n\":\"5.2\"},\"end\":24688,\"start\":24674},{\"attributes\":{\"n\":\"5.3\"},\"end\":26746,\"start\":26721},{\"attributes\":{\"n\":\"5.4\"},\"end\":27681,\"start\":27638},{\"attributes\":{\"n\":\"6\"},\"end\":28530,\"start\":28520},{\"attributes\":{\"n\":\"6.1\"},\"end\":28558,\"start\":28533},{\"attributes\":{\"n\":\"6.2\"},\"end\":29439,\"start\":29429},{\"attributes\":{\"n\":\"7\"},\"end\":30671,\"start\":30661},{\"end\":31361,\"start\":31351},{\"end\":32160,\"start\":32159},{\"end\":32468,\"start\":32458}]", "table": "[{\"end\":33394,\"start\":33150},{\"end\":33759,\"start\":33577}]", "figure_caption": "[{\"end\":31503,\"start\":31363},{\"end\":32157,\"start\":31506},{\"end\":32456,\"start\":32161},{\"end\":33121,\"start\":32470},{\"end\":33150,\"start\":33124},{\"end\":33577,\"start\":33397}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5066,\"start\":5057},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5179,\"start\":5170},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5193,\"start\":5184},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5870,\"start\":5862},{\"end\":11965,\"start\":11957},{\"end\":13736,\"start\":13728},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22929,\"start\":22921},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23794,\"start\":23786},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24350,\"start\":24342},{\"end\":26577,\"start\":26569},{\"end\":27188,\"start\":27180},{\"end\":27816,\"start\":27808},{\"end\":28372,\"start\":28364}]", "bib_author_first_name": "[{\"end\":33883,\"start\":33880},{\"end\":33899,\"start\":33891},{\"end\":33910,\"start\":33906},{\"end\":33925,\"start\":33918},{\"end\":33940,\"start\":33935},{\"end\":33942,\"start\":33941},{\"end\":33959,\"start\":33951},{\"end\":33976,\"start\":33970},{\"end\":33996,\"start\":33990},{\"end\":34010,\"start\":34004},{\"end\":34025,\"start\":34019},{\"end\":34407,\"start\":34401},{\"end\":34419,\"start\":34414},{\"end\":34434,\"start\":34427},{\"end\":34448,\"start\":34441},{\"end\":34464,\"start\":34457},{\"end\":34475,\"start\":34469},{\"end\":34490,\"start\":34482},{\"end\":34499,\"start\":34495},{\"end\":34831,\"start\":34825},{\"end\":34841,\"start\":34838},{\"end\":34853,\"start\":34849},{\"end\":34868,\"start\":34861},{\"end\":34877,\"start\":34874},{\"end\":34892,\"start\":34885},{\"end\":35202,\"start\":35197},{\"end\":35219,\"start\":35211},{\"end\":35233,\"start\":35227},{\"end\":35247,\"start\":35239},{\"end\":35674,\"start\":35670},{\"end\":35690,\"start\":35681},{\"end\":35701,\"start\":35695},{\"end\":35713,\"start\":35708},{\"end\":35727,\"start\":35720},{\"end\":35739,\"start\":35733},{\"end\":35754,\"start\":35747},{\"end\":36323,\"start\":36317},{\"end\":36334,\"start\":36329},{\"end\":36346,\"start\":36339},{\"end\":36360,\"start\":36354},{\"end\":36371,\"start\":36365},{\"end\":36381,\"start\":36378},{\"end\":36394,\"start\":36386},{\"end\":36404,\"start\":36400},{\"end\":36694,\"start\":36688},{\"end\":36704,\"start\":36700},{\"end\":36717,\"start\":36712},{\"end\":36965,\"start\":36959},{\"end\":36977,\"start\":36970},{\"end\":37001,\"start\":36992},{\"end\":37260,\"start\":37253},{\"end\":37269,\"start\":37265},{\"end\":37474,\"start\":37470},{\"end\":37489,\"start\":37481},{\"end\":37497,\"start\":37494},{\"end\":37656,\"start\":37651},{\"end\":37669,\"start\":37665},{\"end\":37684,\"start\":37678},{\"end\":37903,\"start\":37898},{\"end\":37916,\"start\":37912},{\"end\":37930,\"start\":37926},{\"end\":38182,\"start\":38175},{\"end\":38192,\"start\":38187},{\"end\":38206,\"start\":38199},{\"end\":38218,\"start\":38211},{\"end\":38232,\"start\":38225},{\"end\":38243,\"start\":38237},{\"end\":38526,\"start\":38518},{\"end\":38534,\"start\":38531},{\"end\":38544,\"start\":38540},{\"end\":38566,\"start\":38551},{\"end\":38579,\"start\":38573},{\"end\":38591,\"start\":38585},{\"end\":38605,\"start\":38597},{\"end\":38901,\"start\":38897},{\"end\":38914,\"start\":38909},{\"end\":39180,\"start\":39176},{\"end\":39192,\"start\":39185},{\"end\":39204,\"start\":39198},{\"end\":39220,\"start\":39212},{\"end\":39228,\"start\":39226},{\"end\":39240,\"start\":39233},{\"end\":39491,\"start\":39485},{\"end\":39502,\"start\":39496},{\"end\":39514,\"start\":39507},{\"end\":39527,\"start\":39519},{\"end\":39535,\"start\":39532},{\"end\":39545,\"start\":39540},{\"end\":39834,\"start\":39830},{\"end\":39847,\"start\":39840},{\"end\":39871,\"start\":39862},{\"end\":40310,\"start\":40303},{\"end\":40322,\"start\":40316},{\"end\":40335,\"start\":40329},{\"end\":40348,\"start\":40340},{\"end\":40363,\"start\":40356},{\"end\":40379,\"start\":40373},{\"end\":40658,\"start\":40654},{\"end\":40669,\"start\":40664},{\"end\":40686,\"start\":40677},{\"end\":40695,\"start\":40691},{\"end\":40707,\"start\":40702},{\"end\":40720,\"start\":40714},{\"end\":40730,\"start\":40727},{\"end\":40946,\"start\":40940},{\"end\":40959,\"start\":40952},{\"end\":40970,\"start\":40964},{\"end\":40987,\"start\":40978},{\"end\":40997,\"start\":40993},{\"end\":41011,\"start\":41004},{\"end\":41025,\"start\":41018},{\"end\":41180,\"start\":41174},{\"end\":41190,\"start\":41186},{\"end\":41201,\"start\":41196},{\"end\":41216,\"start\":41209},{\"end\":41227,\"start\":41221},{\"end\":41240,\"start\":41235},{\"end\":41251,\"start\":41247},{\"end\":41262,\"start\":41258},{\"end\":41274,\"start\":41270},{\"end\":41295,\"start\":41288},{\"end\":41704,\"start\":41698},{\"end\":41717,\"start\":41710},{\"end\":41727,\"start\":41722},{\"end\":41740,\"start\":41736},{\"end\":42060,\"start\":42057},{\"end\":42068,\"start\":42065},{\"end\":42086,\"start\":42078},{\"end\":42103,\"start\":42094},{\"end\":42118,\"start\":42112},{\"end\":42417,\"start\":42414},{\"end\":42427,\"start\":42424},{\"end\":42704,\"start\":42700},{\"end\":42978,\"start\":42976},{\"end\":42992,\"start\":42985},{\"end\":43243,\"start\":43238},{\"end\":43256,\"start\":43251},{\"end\":43273,\"start\":43264},{\"end\":43480,\"start\":43475},{\"end\":43493,\"start\":43490},{\"end\":43514,\"start\":43507},{\"end\":43527,\"start\":43522},{\"end\":43544,\"start\":43537},{\"end\":43558,\"start\":43549},{\"end\":43560,\"start\":43559},{\"end\":43578,\"start\":43569},{\"end\":43898,\"start\":43891},{\"end\":43909,\"start\":43903},{\"end\":43921,\"start\":43916},{\"end\":43930,\"start\":43926},{\"end\":43943,\"start\":43936},{\"end\":43956,\"start\":43949},{\"end\":44477,\"start\":44470},{\"end\":44488,\"start\":44482},{\"end\":44500,\"start\":44495},{\"end\":44513,\"start\":44508},{\"end\":44525,\"start\":44518},{\"end\":44538,\"start\":44531},{\"end\":44849,\"start\":44842},{\"end\":44859,\"start\":44854},{\"end\":44870,\"start\":44864},{\"end\":44885,\"start\":44877},{\"end\":44900,\"start\":44893},{\"end\":44913,\"start\":44906},{\"end\":44927,\"start\":44920},{\"end\":45268,\"start\":45261},{\"end\":45277,\"start\":45273},{\"end\":45289,\"start\":45283},{\"end\":45301,\"start\":45294},{\"end\":45314,\"start\":45307},{\"end\":45630,\"start\":45625},{\"end\":45643,\"start\":45639},{\"end\":45657,\"start\":45653},{\"end\":45676,\"start\":45667},{\"end\":45688,\"start\":45682},{\"end\":45704,\"start\":45697},{\"end\":45718,\"start\":45713},{\"end\":45728,\"start\":45725},{\"end\":45738,\"start\":45733},{\"end\":45740,\"start\":45739},{\"end\":46099,\"start\":46093},{\"end\":46115,\"start\":46108},{\"end\":46131,\"start\":46124},{\"end\":46142,\"start\":46137},{\"end\":46156,\"start\":46149},{\"end\":46167,\"start\":46163},{\"end\":46181,\"start\":46177},{\"end\":46192,\"start\":46188},{\"end\":46481,\"start\":46476},{\"end\":46499,\"start\":46491},{\"end\":46520,\"start\":46511},{\"end\":46870,\"start\":46861},{\"end\":46888,\"start\":46877},{\"end\":46906,\"start\":46901},{\"end\":47198,\"start\":47194},{\"end\":47214,\"start\":47207},{\"end\":47492,\"start\":47488},{\"end\":47508,\"start\":47501},{\"end\":47758,\"start\":47753},{\"end\":47773,\"start\":47765},{\"end\":47784,\"start\":47778},{\"end\":47799,\"start\":47792},{\"end\":47808,\"start\":47804},{\"end\":47818,\"start\":47815},{\"end\":47833,\"start\":47824},{\"end\":47847,\"start\":47840},{\"end\":47857,\"start\":47853},{\"end\":48132,\"start\":48126},{\"end\":48146,\"start\":48139},{\"end\":48157,\"start\":48156},{\"end\":48159,\"start\":48158},{\"end\":48180,\"start\":48176},{\"end\":48196,\"start\":48190},{\"end\":48523,\"start\":48516},{\"end\":48536,\"start\":48532},{\"end\":48552,\"start\":48548},{\"end\":48562,\"start\":48557},{\"end\":48572,\"start\":48571},{\"end\":48596,\"start\":48595},{\"end\":48616,\"start\":48605},{\"end\":48945,\"start\":48944},{\"end\":48957,\"start\":48953},{\"end\":48959,\"start\":48958},{\"end\":49207,\"start\":49204},{\"end\":49218,\"start\":49212},{\"end\":49230,\"start\":49225},{\"end\":49245,\"start\":49236},{\"end\":49258,\"start\":49251},{\"end\":49576,\"start\":49570},{\"end\":49586,\"start\":49583},{\"end\":49598,\"start\":49591},{\"end\":49985,\"start\":49979},{\"end\":49998,\"start\":49992},{\"end\":50008,\"start\":50004},{\"end\":50016,\"start\":50013},{\"end\":50025,\"start\":50023},{\"end\":50520,\"start\":50514},{\"end\":50533,\"start\":50527},{\"end\":50543,\"start\":50539},{\"end\":50551,\"start\":50548},{\"end\":50560,\"start\":50558},{\"end\":50835,\"start\":50829},{\"end\":50849,\"start\":50842},{\"end\":50859,\"start\":50854},{\"end\":50870,\"start\":50866},{\"end\":50872,\"start\":50871},{\"end\":50885,\"start\":50879},{\"end\":51214,\"start\":51208},{\"end\":51228,\"start\":51222},{\"end\":51238,\"start\":51233},{\"end\":51251,\"start\":51245},{\"end\":51262,\"start\":51258},{\"end\":51274,\"start\":51267},{\"end\":51283,\"start\":51280},{\"end\":51297,\"start\":51291},{\"end\":51642,\"start\":51634},{\"end\":51659,\"start\":51650},{\"end\":51673,\"start\":51666},{\"end\":51682,\"start\":51678},{\"end\":51694,\"start\":51687},{\"end\":51706,\"start\":51699},{\"end\":51719,\"start\":51712},{\"end\":51729,\"start\":51726},{\"end\":51744,\"start\":51737},{\"end\":52110,\"start\":52105},{\"end\":52121,\"start\":52117},{\"end\":52134,\"start\":52131},{\"end\":52144,\"start\":52141},{\"end\":52158,\"start\":52152},{\"end\":52456,\"start\":52450},{\"end\":52467,\"start\":52464},{\"end\":52483,\"start\":52478}]", "bib_author_last_name": "[{\"end\":33889,\"start\":33884},{\"end\":33904,\"start\":33900},{\"end\":33916,\"start\":33911},{\"end\":33933,\"start\":33926},{\"end\":33949,\"start\":33943},{\"end\":33968,\"start\":33960},{\"end\":33988,\"start\":33977},{\"end\":34002,\"start\":33997},{\"end\":34017,\"start\":34011},{\"end\":34032,\"start\":34026},{\"end\":34412,\"start\":34408},{\"end\":34425,\"start\":34420},{\"end\":34439,\"start\":34435},{\"end\":34455,\"start\":34449},{\"end\":34467,\"start\":34465},{\"end\":34480,\"start\":34476},{\"end\":34493,\"start\":34491},{\"end\":34505,\"start\":34500},{\"end\":34836,\"start\":34832},{\"end\":34847,\"start\":34842},{\"end\":34859,\"start\":34854},{\"end\":34872,\"start\":34869},{\"end\":34883,\"start\":34878},{\"end\":34895,\"start\":34893},{\"end\":35209,\"start\":35203},{\"end\":35225,\"start\":35220},{\"end\":35237,\"start\":35234},{\"end\":35257,\"start\":35248},{\"end\":35679,\"start\":35675},{\"end\":35693,\"start\":35691},{\"end\":35706,\"start\":35702},{\"end\":35718,\"start\":35714},{\"end\":35731,\"start\":35728},{\"end\":35745,\"start\":35740},{\"end\":35758,\"start\":35755},{\"end\":36327,\"start\":36324},{\"end\":36337,\"start\":36335},{\"end\":36352,\"start\":36347},{\"end\":36363,\"start\":36361},{\"end\":36376,\"start\":36372},{\"end\":36384,\"start\":36382},{\"end\":36398,\"start\":36395},{\"end\":36408,\"start\":36405},{\"end\":36698,\"start\":36695},{\"end\":36710,\"start\":36705},{\"end\":36722,\"start\":36718},{\"end\":36968,\"start\":36966},{\"end\":36990,\"start\":36978},{\"end\":37006,\"start\":37002},{\"end\":37263,\"start\":37261},{\"end\":37273,\"start\":37270},{\"end\":37479,\"start\":37475},{\"end\":37492,\"start\":37490},{\"end\":37503,\"start\":37498},{\"end\":37663,\"start\":37657},{\"end\":37676,\"start\":37670},{\"end\":37691,\"start\":37685},{\"end\":37910,\"start\":37904},{\"end\":37924,\"start\":37917},{\"end\":37939,\"start\":37931},{\"end\":38185,\"start\":38183},{\"end\":38197,\"start\":38193},{\"end\":38209,\"start\":38207},{\"end\":38223,\"start\":38219},{\"end\":38235,\"start\":38233},{\"end\":38247,\"start\":38244},{\"end\":38529,\"start\":38527},{\"end\":38538,\"start\":38535},{\"end\":38549,\"start\":38545},{\"end\":38571,\"start\":38567},{\"end\":38583,\"start\":38580},{\"end\":38595,\"start\":38592},{\"end\":38608,\"start\":38606},{\"end\":38907,\"start\":38902},{\"end\":38917,\"start\":38915},{\"end\":38924,\"start\":38919},{\"end\":39183,\"start\":39181},{\"end\":39196,\"start\":39193},{\"end\":39210,\"start\":39205},{\"end\":39224,\"start\":39221},{\"end\":39231,\"start\":39229},{\"end\":39243,\"start\":39241},{\"end\":39494,\"start\":39492},{\"end\":39505,\"start\":39503},{\"end\":39517,\"start\":39515},{\"end\":39530,\"start\":39528},{\"end\":39538,\"start\":39536},{\"end\":39549,\"start\":39546},{\"end\":39838,\"start\":39835},{\"end\":39860,\"start\":39848},{\"end\":39876,\"start\":39872},{\"end\":40314,\"start\":40311},{\"end\":40327,\"start\":40323},{\"end\":40338,\"start\":40336},{\"end\":40354,\"start\":40349},{\"end\":40371,\"start\":40364},{\"end\":40386,\"start\":40380},{\"end\":40662,\"start\":40659},{\"end\":40675,\"start\":40670},{\"end\":40689,\"start\":40687},{\"end\":40700,\"start\":40696},{\"end\":40712,\"start\":40708},{\"end\":40725,\"start\":40721},{\"end\":40735,\"start\":40731},{\"end\":40950,\"start\":40947},{\"end\":40962,\"start\":40960},{\"end\":40976,\"start\":40971},{\"end\":40991,\"start\":40988},{\"end\":41002,\"start\":40998},{\"end\":41016,\"start\":41012},{\"end\":41031,\"start\":41026},{\"end\":41184,\"start\":41181},{\"end\":41194,\"start\":41191},{\"end\":41207,\"start\":41202},{\"end\":41219,\"start\":41217},{\"end\":41233,\"start\":41228},{\"end\":41245,\"start\":41241},{\"end\":41256,\"start\":41252},{\"end\":41268,\"start\":41263},{\"end\":41286,\"start\":41275},{\"end\":41304,\"start\":41296},{\"end\":41313,\"start\":41306},{\"end\":41708,\"start\":41705},{\"end\":41720,\"start\":41718},{\"end\":41734,\"start\":41728},{\"end\":41743,\"start\":41741},{\"end\":42063,\"start\":42061},{\"end\":42076,\"start\":42069},{\"end\":42092,\"start\":42087},{\"end\":42110,\"start\":42104},{\"end\":42128,\"start\":42119},{\"end\":42422,\"start\":42418},{\"end\":42434,\"start\":42428},{\"end\":42440,\"start\":42436},{\"end\":42707,\"start\":42705},{\"end\":42983,\"start\":42979},{\"end\":42996,\"start\":42993},{\"end\":43249,\"start\":43244},{\"end\":43262,\"start\":43257},{\"end\":43277,\"start\":43274},{\"end\":43488,\"start\":43481},{\"end\":43505,\"start\":43494},{\"end\":43520,\"start\":43515},{\"end\":43535,\"start\":43528},{\"end\":43547,\"start\":43545},{\"end\":43567,\"start\":43561},{\"end\":43585,\"start\":43579},{\"end\":43901,\"start\":43899},{\"end\":43914,\"start\":43910},{\"end\":43924,\"start\":43922},{\"end\":43934,\"start\":43931},{\"end\":43947,\"start\":43944},{\"end\":43960,\"start\":43957},{\"end\":44480,\"start\":44478},{\"end\":44493,\"start\":44489},{\"end\":44506,\"start\":44501},{\"end\":44516,\"start\":44514},{\"end\":44529,\"start\":44526},{\"end\":44542,\"start\":44539},{\"end\":44852,\"start\":44850},{\"end\":44862,\"start\":44860},{\"end\":44875,\"start\":44871},{\"end\":44891,\"start\":44886},{\"end\":44904,\"start\":44901},{\"end\":44918,\"start\":44914},{\"end\":44931,\"start\":44928},{\"end\":45271,\"start\":45269},{\"end\":45281,\"start\":45278},{\"end\":45292,\"start\":45290},{\"end\":45305,\"start\":45302},{\"end\":45318,\"start\":45315},{\"end\":45637,\"start\":45631},{\"end\":45651,\"start\":45644},{\"end\":45665,\"start\":45658},{\"end\":45680,\"start\":45677},{\"end\":45695,\"start\":45689},{\"end\":45711,\"start\":45705},{\"end\":45723,\"start\":45719},{\"end\":45731,\"start\":45729},{\"end\":45744,\"start\":45741},{\"end\":46106,\"start\":46100},{\"end\":46122,\"start\":46116},{\"end\":46135,\"start\":46132},{\"end\":46147,\"start\":46143},{\"end\":46161,\"start\":46157},{\"end\":46175,\"start\":46168},{\"end\":46186,\"start\":46182},{\"end\":46202,\"start\":46193},{\"end\":46489,\"start\":46482},{\"end\":46509,\"start\":46500},{\"end\":46528,\"start\":46521},{\"end\":46875,\"start\":46871},{\"end\":46899,\"start\":46889},{\"end\":46917,\"start\":46907},{\"end\":47205,\"start\":47199},{\"end\":47222,\"start\":47215},{\"end\":47499,\"start\":47493},{\"end\":47516,\"start\":47509},{\"end\":47763,\"start\":47759},{\"end\":47776,\"start\":47774},{\"end\":47790,\"start\":47785},{\"end\":47802,\"start\":47800},{\"end\":47813,\"start\":47809},{\"end\":47822,\"start\":47819},{\"end\":47838,\"start\":47834},{\"end\":47851,\"start\":47848},{\"end\":47862,\"start\":47858},{\"end\":48137,\"start\":48133},{\"end\":48154,\"start\":48147},{\"end\":48174,\"start\":48160},{\"end\":48188,\"start\":48181},{\"end\":48202,\"start\":48197},{\"end\":48530,\"start\":48524},{\"end\":48546,\"start\":48537},{\"end\":48555,\"start\":48553},{\"end\":48569,\"start\":48563},{\"end\":48584,\"start\":48573},{\"end\":48593,\"start\":48586},{\"end\":48603,\"start\":48597},{\"end\":48619,\"start\":48617},{\"end\":48626,\"start\":48621},{\"end\":48951,\"start\":48946},{\"end\":48968,\"start\":48960},{\"end\":48974,\"start\":48970},{\"end\":49210,\"start\":49208},{\"end\":49223,\"start\":49219},{\"end\":49234,\"start\":49231},{\"end\":49249,\"start\":49246},{\"end\":49262,\"start\":49259},{\"end\":49581,\"start\":49577},{\"end\":49589,\"start\":49587},{\"end\":49604,\"start\":49599},{\"end\":49990,\"start\":49986},{\"end\":50002,\"start\":49999},{\"end\":50011,\"start\":50009},{\"end\":50021,\"start\":50017},{\"end\":50029,\"start\":50026},{\"end\":50525,\"start\":50521},{\"end\":50537,\"start\":50534},{\"end\":50546,\"start\":50544},{\"end\":50556,\"start\":50552},{\"end\":50564,\"start\":50561},{\"end\":50840,\"start\":50836},{\"end\":50852,\"start\":50850},{\"end\":50864,\"start\":50860},{\"end\":50877,\"start\":50873},{\"end\":50902,\"start\":50886},{\"end\":51220,\"start\":51215},{\"end\":51231,\"start\":51229},{\"end\":51243,\"start\":51239},{\"end\":51256,\"start\":51252},{\"end\":51265,\"start\":51263},{\"end\":51278,\"start\":51275},{\"end\":51289,\"start\":51284},{\"end\":51302,\"start\":51298},{\"end\":51648,\"start\":51643},{\"end\":51664,\"start\":51660},{\"end\":51676,\"start\":51674},{\"end\":51685,\"start\":51683},{\"end\":51697,\"start\":51695},{\"end\":51710,\"start\":51707},{\"end\":51724,\"start\":51720},{\"end\":51735,\"start\":51730},{\"end\":51748,\"start\":51745},{\"end\":52115,\"start\":52111},{\"end\":52129,\"start\":52122},{\"end\":52139,\"start\":52135},{\"end\":52150,\"start\":52145},{\"end\":52164,\"start\":52159},{\"end\":52462,\"start\":52457},{\"end\":52476,\"start\":52468},{\"end\":52488,\"start\":52484}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218971783},\"end\":34318,\"start\":33841},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":238354397},\"end\":34757,\"start\":34320},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":238259429},\"end\":35113,\"start\":34759},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52967399},\"end\":35610,\"start\":35115},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":241033259},\"end\":36254,\"start\":35612},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":244117423},\"end\":36625,\"start\":36256},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":229923710},\"end\":36876,\"start\":36627},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b7\"},\"end\":37210,\"start\":36878},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":207155218},\"end\":37416,\"start\":37212},{\"attributes\":{\"id\":\"b9\"},\"end\":37601,\"start\":37418},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":215754328},\"end\":37838,\"start\":37603},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":233296808},\"end\":38103,\"start\":37840},{\"attributes\":{\"doi\":\"arXiv:2108.13888\",\"id\":\"b12\"},\"end\":38465,\"start\":38105},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":233481877},\"end\":38834,\"start\":38467},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":230433941},\"end\":39090,\"start\":38836},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":231627799},\"end\":39426,\"start\":39092},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":237054216},\"end\":39751,\"start\":39428},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":44096776},\"end\":40196,\"start\":39753},{\"attributes\":{\"doi\":\"arXiv:2107.13586\",\"id\":\"b18\"},\"end\":40650,\"start\":40198},{\"attributes\":{\"doi\":\"arXiv:2103.10385\",\"id\":\"b19\"},\"end\":40936,\"start\":40652},{\"attributes\":{\"id\":\"b20\"},\"end\":41172,\"start\":40938},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b21\"},\"end\":41624,\"start\":41174},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":220363879},\"end\":41955,\"start\":41626},{\"attributes\":{\"doi\":\"arXiv:2104.08786\",\"id\":\"b23\"},\"end\":42375,\"start\":41957},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":223953609},\"end\":42593,\"start\":42377},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3264224},\"end\":42874,\"start\":42595},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":388},\"end\":43191,\"start\":42876},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":235166749},\"end\":43436,\"start\":43193},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":202539551},\"end\":43817,\"start\":43438},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":227118606},\"end\":44381,\"start\":43819},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":238857078},\"end\":44766,\"start\":44383},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":235196099},\"end\":45174,\"start\":44768},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":235417102},\"end\":45540,\"start\":45176},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":204838007},\"end\":46055,\"start\":45542},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":232035663},\"end\":46436,\"start\":46057},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206823005},\"end\":46826,\"start\":46438},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":203610516},\"end\":47100,\"start\":46828},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":210838924},\"end\":47403,\"start\":47102},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":221703107},\"end\":47702,\"start\":47405},{\"attributes\":{\"id\":\"b39\"},\"end\":48033,\"start\":47704},{\"attributes\":{\"id\":\"b40\"},\"end\":48435,\"start\":48035},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":990233},\"end\":48895,\"start\":48437},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":11465263},\"end\":49129,\"start\":48897},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":248085108},\"end\":49458,\"start\":49131},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":232404131},\"end\":49887,\"start\":49460},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":239009606},\"end\":50449,\"start\":49889},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":236459933},\"end\":50753,\"start\":50451},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":44061800},\"end\":51124,\"start\":50755},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":237353222},\"end\":51540,\"start\":51126},{\"attributes\":{\"doi\":\"arXiv:2101.06969\",\"id\":\"b49\"},\"end\":52030,\"start\":51542},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":231979430},\"end\":52388,\"start\":52032},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":233210199},\"end\":52663,\"start\":52390}]", "bib_title": "[{\"end\":33878,\"start\":33841},{\"end\":34399,\"start\":34320},{\"end\":34823,\"start\":34759},{\"end\":35195,\"start\":35115},{\"end\":35668,\"start\":35612},{\"end\":36315,\"start\":36256},{\"end\":36686,\"start\":36627},{\"end\":37251,\"start\":37212},{\"end\":37649,\"start\":37603},{\"end\":37896,\"start\":37840},{\"end\":38516,\"start\":38467},{\"end\":38895,\"start\":38836},{\"end\":39174,\"start\":39092},{\"end\":39483,\"start\":39428},{\"end\":39828,\"start\":39753},{\"end\":41696,\"start\":41626},{\"end\":42412,\"start\":42377},{\"end\":42698,\"start\":42595},{\"end\":42974,\"start\":42876},{\"end\":43236,\"start\":43193},{\"end\":43473,\"start\":43438},{\"end\":43889,\"start\":43819},{\"end\":44468,\"start\":44383},{\"end\":44840,\"start\":44768},{\"end\":45259,\"start\":45176},{\"end\":45623,\"start\":45542},{\"end\":46091,\"start\":46057},{\"end\":46474,\"start\":46438},{\"end\":46859,\"start\":46828},{\"end\":47192,\"start\":47102},{\"end\":47486,\"start\":47405},{\"end\":48124,\"start\":48035},{\"end\":48514,\"start\":48437},{\"end\":48942,\"start\":48897},{\"end\":49202,\"start\":49131},{\"end\":49568,\"start\":49460},{\"end\":49977,\"start\":49889},{\"end\":50512,\"start\":50451},{\"end\":50827,\"start\":50755},{\"end\":51206,\"start\":51126},{\"end\":52103,\"start\":52032},{\"end\":52448,\"start\":52390}]", "bib_author": "[{\"end\":33891,\"start\":33880},{\"end\":33906,\"start\":33891},{\"end\":33918,\"start\":33906},{\"end\":33935,\"start\":33918},{\"end\":33951,\"start\":33935},{\"end\":33970,\"start\":33951},{\"end\":33990,\"start\":33970},{\"end\":34004,\"start\":33990},{\"end\":34019,\"start\":34004},{\"end\":34034,\"start\":34019},{\"end\":34414,\"start\":34401},{\"end\":34427,\"start\":34414},{\"end\":34441,\"start\":34427},{\"end\":34457,\"start\":34441},{\"end\":34469,\"start\":34457},{\"end\":34482,\"start\":34469},{\"end\":34495,\"start\":34482},{\"end\":34507,\"start\":34495},{\"end\":34838,\"start\":34825},{\"end\":34849,\"start\":34838},{\"end\":34861,\"start\":34849},{\"end\":34874,\"start\":34861},{\"end\":34885,\"start\":34874},{\"end\":34897,\"start\":34885},{\"end\":35211,\"start\":35197},{\"end\":35227,\"start\":35211},{\"end\":35239,\"start\":35227},{\"end\":35259,\"start\":35239},{\"end\":35681,\"start\":35670},{\"end\":35695,\"start\":35681},{\"end\":35708,\"start\":35695},{\"end\":35720,\"start\":35708},{\"end\":35733,\"start\":35720},{\"end\":35747,\"start\":35733},{\"end\":35760,\"start\":35747},{\"end\":36329,\"start\":36317},{\"end\":36339,\"start\":36329},{\"end\":36354,\"start\":36339},{\"end\":36365,\"start\":36354},{\"end\":36378,\"start\":36365},{\"end\":36386,\"start\":36378},{\"end\":36400,\"start\":36386},{\"end\":36410,\"start\":36400},{\"end\":36700,\"start\":36688},{\"end\":36712,\"start\":36700},{\"end\":36724,\"start\":36712},{\"end\":36970,\"start\":36959},{\"end\":36992,\"start\":36970},{\"end\":37008,\"start\":36992},{\"end\":37265,\"start\":37253},{\"end\":37275,\"start\":37265},{\"end\":37481,\"start\":37470},{\"end\":37494,\"start\":37481},{\"end\":37505,\"start\":37494},{\"end\":37665,\"start\":37651},{\"end\":37678,\"start\":37665},{\"end\":37693,\"start\":37678},{\"end\":37912,\"start\":37898},{\"end\":37926,\"start\":37912},{\"end\":37941,\"start\":37926},{\"end\":38187,\"start\":38175},{\"end\":38199,\"start\":38187},{\"end\":38211,\"start\":38199},{\"end\":38225,\"start\":38211},{\"end\":38237,\"start\":38225},{\"end\":38249,\"start\":38237},{\"end\":38531,\"start\":38518},{\"end\":38540,\"start\":38531},{\"end\":38551,\"start\":38540},{\"end\":38573,\"start\":38551},{\"end\":38585,\"start\":38573},{\"end\":38597,\"start\":38585},{\"end\":38610,\"start\":38597},{\"end\":38909,\"start\":38897},{\"end\":38919,\"start\":38909},{\"end\":38926,\"start\":38919},{\"end\":39185,\"start\":39176},{\"end\":39198,\"start\":39185},{\"end\":39212,\"start\":39198},{\"end\":39226,\"start\":39212},{\"end\":39233,\"start\":39226},{\"end\":39245,\"start\":39233},{\"end\":39496,\"start\":39485},{\"end\":39507,\"start\":39496},{\"end\":39519,\"start\":39507},{\"end\":39532,\"start\":39519},{\"end\":39540,\"start\":39532},{\"end\":39551,\"start\":39540},{\"end\":39840,\"start\":39830},{\"end\":39862,\"start\":39840},{\"end\":39878,\"start\":39862},{\"end\":40316,\"start\":40303},{\"end\":40329,\"start\":40316},{\"end\":40340,\"start\":40329},{\"end\":40356,\"start\":40340},{\"end\":40373,\"start\":40356},{\"end\":40388,\"start\":40373},{\"end\":40664,\"start\":40654},{\"end\":40677,\"start\":40664},{\"end\":40691,\"start\":40677},{\"end\":40702,\"start\":40691},{\"end\":40714,\"start\":40702},{\"end\":40727,\"start\":40714},{\"end\":40737,\"start\":40727},{\"end\":40952,\"start\":40940},{\"end\":40964,\"start\":40952},{\"end\":40978,\"start\":40964},{\"end\":40993,\"start\":40978},{\"end\":41004,\"start\":40993},{\"end\":41018,\"start\":41004},{\"end\":41033,\"start\":41018},{\"end\":41186,\"start\":41174},{\"end\":41196,\"start\":41186},{\"end\":41209,\"start\":41196},{\"end\":41221,\"start\":41209},{\"end\":41235,\"start\":41221},{\"end\":41247,\"start\":41235},{\"end\":41258,\"start\":41247},{\"end\":41270,\"start\":41258},{\"end\":41288,\"start\":41270},{\"end\":41306,\"start\":41288},{\"end\":41315,\"start\":41306},{\"end\":41710,\"start\":41698},{\"end\":41722,\"start\":41710},{\"end\":41736,\"start\":41722},{\"end\":41745,\"start\":41736},{\"end\":42065,\"start\":42057},{\"end\":42078,\"start\":42065},{\"end\":42094,\"start\":42078},{\"end\":42112,\"start\":42094},{\"end\":42130,\"start\":42112},{\"end\":42424,\"start\":42414},{\"end\":42436,\"start\":42424},{\"end\":42442,\"start\":42436},{\"end\":42709,\"start\":42700},{\"end\":42985,\"start\":42976},{\"end\":42998,\"start\":42985},{\"end\":43251,\"start\":43238},{\"end\":43264,\"start\":43251},{\"end\":43279,\"start\":43264},{\"end\":43490,\"start\":43475},{\"end\":43507,\"start\":43490},{\"end\":43522,\"start\":43507},{\"end\":43537,\"start\":43522},{\"end\":43549,\"start\":43537},{\"end\":43569,\"start\":43549},{\"end\":43587,\"start\":43569},{\"end\":43903,\"start\":43891},{\"end\":43916,\"start\":43903},{\"end\":43926,\"start\":43916},{\"end\":43936,\"start\":43926},{\"end\":43949,\"start\":43936},{\"end\":43962,\"start\":43949},{\"end\":44482,\"start\":44470},{\"end\":44495,\"start\":44482},{\"end\":44508,\"start\":44495},{\"end\":44518,\"start\":44508},{\"end\":44531,\"start\":44518},{\"end\":44544,\"start\":44531},{\"end\":44854,\"start\":44842},{\"end\":44864,\"start\":44854},{\"end\":44877,\"start\":44864},{\"end\":44893,\"start\":44877},{\"end\":44906,\"start\":44893},{\"end\":44920,\"start\":44906},{\"end\":44933,\"start\":44920},{\"end\":45273,\"start\":45261},{\"end\":45283,\"start\":45273},{\"end\":45294,\"start\":45283},{\"end\":45307,\"start\":45294},{\"end\":45320,\"start\":45307},{\"end\":45639,\"start\":45625},{\"end\":45653,\"start\":45639},{\"end\":45667,\"start\":45653},{\"end\":45682,\"start\":45667},{\"end\":45697,\"start\":45682},{\"end\":45713,\"start\":45697},{\"end\":45725,\"start\":45713},{\"end\":45733,\"start\":45725},{\"end\":45746,\"start\":45733},{\"end\":46108,\"start\":46093},{\"end\":46124,\"start\":46108},{\"end\":46137,\"start\":46124},{\"end\":46149,\"start\":46137},{\"end\":46163,\"start\":46149},{\"end\":46177,\"start\":46163},{\"end\":46188,\"start\":46177},{\"end\":46204,\"start\":46188},{\"end\":46491,\"start\":46476},{\"end\":46511,\"start\":46491},{\"end\":46530,\"start\":46511},{\"end\":46877,\"start\":46861},{\"end\":46901,\"start\":46877},{\"end\":46919,\"start\":46901},{\"end\":47207,\"start\":47194},{\"end\":47224,\"start\":47207},{\"end\":47501,\"start\":47488},{\"end\":47518,\"start\":47501},{\"end\":47765,\"start\":47753},{\"end\":47778,\"start\":47765},{\"end\":47792,\"start\":47778},{\"end\":47804,\"start\":47792},{\"end\":47815,\"start\":47804},{\"end\":47824,\"start\":47815},{\"end\":47840,\"start\":47824},{\"end\":47853,\"start\":47840},{\"end\":47864,\"start\":47853},{\"end\":48139,\"start\":48126},{\"end\":48156,\"start\":48139},{\"end\":48176,\"start\":48156},{\"end\":48190,\"start\":48176},{\"end\":48204,\"start\":48190},{\"end\":48532,\"start\":48516},{\"end\":48548,\"start\":48532},{\"end\":48557,\"start\":48548},{\"end\":48571,\"start\":48557},{\"end\":48586,\"start\":48571},{\"end\":48595,\"start\":48586},{\"end\":48605,\"start\":48595},{\"end\":48621,\"start\":48605},{\"end\":48628,\"start\":48621},{\"end\":48953,\"start\":48944},{\"end\":48970,\"start\":48953},{\"end\":48976,\"start\":48970},{\"end\":49212,\"start\":49204},{\"end\":49225,\"start\":49212},{\"end\":49236,\"start\":49225},{\"end\":49251,\"start\":49236},{\"end\":49264,\"start\":49251},{\"end\":49583,\"start\":49570},{\"end\":49591,\"start\":49583},{\"end\":49606,\"start\":49591},{\"end\":49992,\"start\":49979},{\"end\":50004,\"start\":49992},{\"end\":50013,\"start\":50004},{\"end\":50023,\"start\":50013},{\"end\":50031,\"start\":50023},{\"end\":50527,\"start\":50514},{\"end\":50539,\"start\":50527},{\"end\":50548,\"start\":50539},{\"end\":50558,\"start\":50548},{\"end\":50566,\"start\":50558},{\"end\":50842,\"start\":50829},{\"end\":50854,\"start\":50842},{\"end\":50866,\"start\":50854},{\"end\":50879,\"start\":50866},{\"end\":50904,\"start\":50879},{\"end\":51222,\"start\":51208},{\"end\":51233,\"start\":51222},{\"end\":51245,\"start\":51233},{\"end\":51258,\"start\":51245},{\"end\":51267,\"start\":51258},{\"end\":51280,\"start\":51267},{\"end\":51291,\"start\":51280},{\"end\":51304,\"start\":51291},{\"end\":51650,\"start\":51634},{\"end\":51666,\"start\":51650},{\"end\":51678,\"start\":51666},{\"end\":51687,\"start\":51678},{\"end\":51699,\"start\":51687},{\"end\":51712,\"start\":51699},{\"end\":51726,\"start\":51712},{\"end\":51737,\"start\":51726},{\"end\":51750,\"start\":51737},{\"end\":52117,\"start\":52105},{\"end\":52131,\"start\":52117},{\"end\":52141,\"start\":52131},{\"end\":52152,\"start\":52141},{\"end\":52166,\"start\":52152},{\"end\":52464,\"start\":52450},{\"end\":52478,\"start\":52464},{\"end\":52490,\"start\":52478}]", "bib_venue": "[{\"end\":34053,\"start\":34034},{\"end\":34527,\"start\":34507},{\"end\":34917,\"start\":34897},{\"end\":35277,\"start\":35259},{\"end\":35870,\"start\":35760},{\"end\":36430,\"start\":36410},{\"end\":36742,\"start\":36724},{\"end\":36957,\"start\":36878},{\"end\":37296,\"start\":37275},{\"end\":37468,\"start\":37418},{\"end\":37711,\"start\":37693},{\"end\":37961,\"start\":37941},{\"end\":38173,\"start\":38105},{\"end\":38631,\"start\":38610},{\"end\":38951,\"start\":38926},{\"end\":39249,\"start\":39245},{\"end\":39570,\"start\":39551},{\"end\":39950,\"start\":39878},{\"end\":40301,\"start\":40198},{\"end\":41377,\"start\":41331},{\"end\":41764,\"start\":41745},{\"end\":42055,\"start\":41957},{\"end\":42461,\"start\":42442},{\"end\":42727,\"start\":42709},{\"end\":43016,\"start\":42998},{\"end\":43298,\"start\":43279},{\"end\":43614,\"start\":43587},{\"end\":44048,\"start\":43962},{\"end\":44564,\"start\":44544},{\"end\":44958,\"start\":44933},{\"end\":45345,\"start\":45320},{\"end\":45782,\"start\":45746},{\"end\":46223,\"start\":46204},{\"end\":46609,\"start\":46530},{\"end\":46938,\"start\":46919},{\"end\":47243,\"start\":47224},{\"end\":47536,\"start\":47518},{\"end\":47751,\"start\":47704},{\"end\":48224,\"start\":48204},{\"end\":48648,\"start\":48628},{\"end\":48996,\"start\":48976},{\"end\":49284,\"start\":49264},{\"end\":49630,\"start\":49606},{\"end\":50117,\"start\":50031},{\"end\":50584,\"start\":50566},{\"end\":50923,\"start\":50904},{\"end\":51323,\"start\":51304},{\"end\":51632,\"start\":51542},{\"end\":52185,\"start\":52166},{\"end\":52514,\"start\":52490},{\"end\":34059,\"start\":34055},{\"end\":34534,\"start\":34529},{\"end\":34924,\"start\":34919},{\"end\":35304,\"start\":35279},{\"end\":35967,\"start\":35872},{\"end\":36437,\"start\":36432},{\"end\":36747,\"start\":36744},{\"end\":37304,\"start\":37298},{\"end\":37716,\"start\":37713},{\"end\":37968,\"start\":37963},{\"end\":38639,\"start\":38633},{\"end\":38963,\"start\":38953},{\"end\":39576,\"start\":39572},{\"end\":41770,\"start\":41766},{\"end\":42467,\"start\":42463},{\"end\":42732,\"start\":42729},{\"end\":43021,\"start\":43018},{\"end\":43304,\"start\":43300},{\"end\":43628,\"start\":43616},{\"end\":44121,\"start\":44050},{\"end\":44571,\"start\":44566},{\"end\":44970,\"start\":44960},{\"end\":45357,\"start\":45347},{\"end\":46229,\"start\":46225},{\"end\":46944,\"start\":46940},{\"end\":47249,\"start\":47245},{\"end\":47541,\"start\":47538},{\"end\":48231,\"start\":48226},{\"end\":48655,\"start\":48650},{\"end\":49003,\"start\":48998},{\"end\":49291,\"start\":49286},{\"end\":49654,\"start\":49632},{\"end\":50190,\"start\":50119},{\"end\":50589,\"start\":50586},{\"end\":50929,\"start\":50925},{\"end\":51329,\"start\":51325},{\"end\":52191,\"start\":52187},{\"end\":52525,\"start\":52516}]"}}}, "year": 2023, "month": 12, "day": 17}