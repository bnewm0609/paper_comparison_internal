{"id": 234773176, "updated": "2022-01-12 18:06:55.815", "metadata": {"title": "Rim: Offloading Inference to the Edge", "authors": "[{\"middle\":[],\"last\":\"Hu\",\"first\":\"Yitao\"},{\"middle\":[],\"last\":\"Pang\",\"first\":\"Weiwu\"},{\"middle\":[],\"last\":\"Liu\",\"first\":\"Xiaochen\"},{\"middle\":[],\"last\":\"Ghosh\",\"first\":\"Rajrup\"},{\"middle\":[],\"last\":\"Ko\",\"first\":\"Bongjun\"},{\"middle\":[],\"last\":\"Lee\",\"first\":\"Wei-Han\"},{\"middle\":[],\"last\":\"Govindan\",\"first\":\"Ramesh\"}]", "venue": null, "journal": "Proceedings of the International Conference on Internet-of-Things Design and Implementation", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Video cameras are among the most ubiquitous sensors in the Internet-of-Things. Video and audio applications, such as cross-camera activity detection, avatar extraction or language translation will, in the future, offload processing to an edge cluster of GPUs. Rim is a management system for such clusters that satisfies throughput and latency requirements of these applications, while enabling high cluster utilization. It uses coarse-grained knowledge of application structure to profile throughput of applications on resources, then uses these profiles to place applications on cluster nodes to achieve these goals. It dynamically adapts placement to load and failures. Experiments show that on maximal workloads on a testbed, Rim can satisfy requirements of all applications, but competing approaches designed for low-latency GPU execution cannot.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iotdi/HuPLGKLG21", "doi": "10.1145/3450268.3453521"}}, "content": {"source": {"pdf_hash": "4c7d9770106962e23d88c8c0a8c5ea1c8993ce6c", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3450268.3453521", "status": "BRONZE"}}, "grobid": {"id": "0723ab97017267169a4e9a23255773f8e4b86800", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4c7d9770106962e23d88c8c0a8c5ea1c8993ce6c.txt", "contents": "\nRim: Offloading Inference to the Edge\n\n\nYitao Hu yitaoh@usc.edu \nWeiwu Pang weiwupan@usc.edu \nXiaochen Liu \nRajrup Ghosh rajrupgh@usc.edu \nBongjun Ko bongjun_ko@us.ibm.com \nWei-Han Lee wei-han.lee1@ibm.com \nRamesh Govindan ramesh@usc.edu \n\nUniversity of Southern California\nUniversity of Southern\nCalifornia\n\n\nUniversity of Southern\nCalifornia\n\n\nUniversity of Southern\nCalifornia\n\n\nIBM Research\nIBM Research\nUniversity of Southern\nCalifornia\n\nRim: Offloading Inference to the Edge\n10.1145/3450268.3453521CCS Concepts \u2022 General and reference \u2192 Performance\u2022 Computing method- ologies \u2192 Neural networksComputer visionNatural language processing\u2022 Software and its engineering \u2192 Scheduling Keywords edge computing, GPU scheduling, serving system, deep learning\nVideo cameras are among the most ubiquitous sensors in the Internetof-Things. Video and audio applications, such as cross-camera activity detection, avatar extraction or language translation will, in the future, offload processing to an edge cluster of GPUs. Rim is a management system for such clusters that satisfies throughput and latency requirements of these applications, while enabling high cluster utilization. It uses coarse-grained knowledge of application structure to profile throughput of applications on resources, then uses these profiles to place applications on cluster nodes to achieve these goals. It dynamically adapts placement to load and failures. Experiments show that on maximal workloads on a testbed, Rim can satisfy requirements of all applications, but competing approaches designed for low-latency GPU execution cannot.\n\nIntroduction\n\nToday, with the ubiquity of camera-enabled mobile devices, applications increasingly process images in near real-time, either on device, or in the cloud. Adding filters, identifying landmarks or people, or even simply re-sizing images are examples of such processing. To support this, recent research [19,55] has explored predictable latency image processing on a cloud cluster, using deep learning models (DL models) executed on CPUs and GPUs. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Near real-time processing of video and audio streams is the natural next step in the evolution of media-processing ( \u00a72), especially for IoT. Already, video cameras are among the most widely deployed outdoor IoT sensors. In indoor settings, virtual home assistants like Google Home, Google's Nest hub and Amazon's Echo are experiencing significant market penetration and have microphones and, more recently, video calling capabilities.\n\nThis will drive the development of several novel applications. Complex activity detection [42] seeks to detect activities occurring across multiple non-overlapping cameras. Avatar extraction [31,45] enables avatar-based video conferencing, where avatars represent participants. Language translation helps participants converse in different languages.\n\nThese applications require predictable low latency and throughput (in frames per second, or fps). Video-based applications can generate significant volumes of traffic. Moreover, many such applications often use multiple DL models connected together in a DAG (directed acyclic graph). Recognizing this, recent research in IoT systems has explored the execution of pared-down DL models on mobile devices [17,30,66]. However, the resource requirements of DL models continue to outstrip the computational capacity of mobile devices, so that, as the complexity of applications increases, mobile devices may not be able to satisfy application requirements. To meet these requirements, it will be necessary to offload computation to an edge cluster of GPUs located topologically close to user device (e.g., at a cell tower, or cable head-end). This edge computing ensures low latency to user devices, and avoids having to transmit video across the wide-area network to remote cloud data centers.\n\nIndustry has recognized edge computing's potential. The global edge computing market size was valued at $3.5 billion in 2019 [3], and is anticipated to reach $43.4 billion by 2027 [4]. Telecommunication companies like Verizon and AT&T are starting to deploy edge computing infrastructure alongside their cellular networks, in order to provide low latency and reduce bandwidth [62]. Nvidia has released a GPU-based edge computing platform called EGX [8].\n\nMotivated by the confluence of these trends, this paper explores the design and implementation of Rim, an edge GPU cluster management system for media-processing applications. Rim strives to achieve high cluster GPU utilization, while satisfying both throughput and latency objectives of each application session. Contributions. Rim makes three contributions ( \u00a73).\n\nThe first is the design of an abstraction ( \u00a73.2), called an mDAG, that exposes the DAG structure of applications, where a DAG vertex represents either CPU execution or GPU DL model invocation, and DAG edges represent data flow. mDAGs form a unit of execution and resource allocation. Each application session is bound to one mDAG, and Rim translates application throughput and latency requirements to resources allocated to an mDAG (or sub-graph thereof).\n\nThe second is a suite of novel techniques that manage placement of mDAGs on the cluster ( \u00a73.3). Placement relies on throughput profiles of the entire mDAG and each vertex. Rim uses profiles to place mDAGs either on a single node in the cluster, or splits mDAG execution across multiple nodes. The former ensures lower latency, but the latter helps increase utilization in the face of fragmentation. Having determined a placement, Rim automatically loads DL models and generates steering configurations to route media streams from the client through cluster nodes. To ensure high GPU utilization, Rim uses spatial multiplexing; prior work has batched inputs and temporally multiplexed models, which works less well at the edge because of lower statistical multiplexing.\n\nThe third is dynamic quality adaptation in which Rim dynamically switches to lighter-weight mDAG implementations that tradeoff a little accuracy for significant reductions in resource usage. Motivated by quality adaptation techniques for video delivery [11,43], this helps Rim serve more clients than would otherwise be possible.\n\nEvaluation Results. Using an implementation of Rim on a cluster of 14 GPUs ( \u00a74.1), we show that Rim is able to satisfy the throughput and latency requirements while maintaining GPU utilization of about 52%, about 2\u00d7 of the reported utilization of GPUs in cloud clusters [2,35,37]. Unlike Rim which is able to handle 100% of its offered load, recent work such as Nexus [55] and Clipper [19] can only sustain 59.6% to 89.4% of the offered load ( \u00a74.2). An extensive ablation study ( \u00a74.3) shows that switching to lighter-weight mDAGs can help Rim accommodate 78.9% more sessions, that other alternatives to profile-based placement fail to achieve performance objectives, and that spatial multiplexing improves utilization by 8% to 99% over an approach that does not use it.\n\n\nBackground and Motivation\n\nThe impending confluence of three trends motivates Rim: novel near real-time media-processing applications using video and audio generated by IoT devices, sophisticated deep learning techniques for processing these streams, and edge computing services that make it possible to satisfy application performance objectives. Deep learning. Armed with large training data sets and powerful GPU hardware, deep neural network-based learning has transformed computer vision, speech recognition, and machine translation. Much prior systems work on deep learning has focused on training DL models. Less work [19,32,55] has explored model inference (the use of the model to perform a task), the focus of our paper.\n\nDeep learning has achieved fast and highly accurate object detection on images using deep learning models (henceforth, DL models) like SSD [41] and Yolo [52]. Popular speech to text DL models like Jasper [36], Wav2letter [16], and Deepspeech2 [13] also have high accuracy [48]. Novel Media-Processing Applications. These DL models are building blocks for future media-processing applications. Today, although video forms the largest proportion of Internet traffic, much of this video is streamed to, and consumed by, user devices. User devices are starting to generate video [63], but few applications process video. In this paper, motivated by the success of DL, we explore an emerging class of applications in which video and audio are (a) generated by user devices and (b) processed in near real-time. We call these media processing applications (Fig. 1).\n\nConsider complex activity detection [42], in which multiple (potentially non-overlapping) surveillance cameras generate streams of video, and the task is to determine, in near real-time, complex activities across these cameras. An example of a complex activity is: \"A person walking while talking on the phone in one camera, and the same person talks to another person at a different camera a short while later\". More generally, the task is to recognize participants and objects, their spatial and temporal relationships, as well as the activities they perform, across multiple cameras. This application's processing pipeline uses DL models for object detection, re-identification (the task of determining whether two images from different cameras belong to the same person), and activity detection. These DL models need a GPU. In addition, the pipeline uses fast CPU-based trackers to track moving objects in the video, and intermediate stages of the pipeline accumulate state (e.g., a sequence of object detections across successive frames). Fig. 1 lists other media processing applications and their constituent components. We describe these later, but highlight three important features of these applications that inform our design: (a) media processing may include audio as well (e.g., language translation); (b) applications use a combination of CPU-based components in addition to DL models (in Fig. 1, the latter have a darker background); (c) some components accumulate state across multiple frames before invoking the next component.\n\nMore important, media-processing applications have two important performance requirements: throughput (e.g., for video, expressed by the frame rate), and end-to-end latency. These requirements determine usability (e.g., conferencing may be unusable at low frame rate or high latency) or even correctness (e.g., activity detection might miss some activities at low frame rates). Where to execute media-processing applications? DL models in Fig. 1 are heavyweight and require GPU acceleration to be able to satisfy throughput and latency requirements of media-processing applications. Where should these applications execute?\n\nOn the device. These applications process streams generated by mobile devices. Mobile devices will likely soon incorporate moderately powerful GPUs [46,47]. Novel techniques like model compression [28,33] and communication compression [30] make DL inference on the device a promising option. Unfortunately, even with these development, model inferences on the device may not be sufficient to support many of our applications. For example, our measurements show that it takes more than 4 seconds to run language translation on a 2-sec audio segment on the Jetson AGX Xavier (one of the most powerful mobile GPUs available today) [47].\n\nOn the cloud. Media-processing could potentially be offloaded to the cloud, since cloud providers have recently added support for GPUs. However, the latency from the user to the cloud may be too high for some applications (e.g., avatar extraction for live teleconferencing, Fig. 1). The average response time from popular cloud providers [12, 25, 44] ranges from 66 ms to 75 ms [57], which accounts for one third of the latency budget for real-time streaming applications [7,9]. For others, the bandwidth cost of streaming video to the cloud at large scale can be prohibitive; it is for this reason that, today, video streaming uses front-ends of large CDNs nearest to users, thereby minimizing this cost.\n\nAt the edge. Motivated by media-processing applications, and by the development of low latency high-bandwidth wireless standards like 5G, ISPs are starting to deploy edge computing clusters (containing a few racks of servers) topologically close to users (e.g., at cell towers, cable head-ends) [62]. These edge clusters have more powerful compute capabilities than devices because they can deploy server-class hardware; e.g., on a server-class GPU, translating a 2-second audio clip needs only 0.5 s. Moreover, edge clusters are closer to devices than the cloud, and can respond in less than half the time [57]. Therefore, edge computing represents a sweet spot in the space of architectural choices for media-processing applications. Goal and Requirements. In this paper, we explore the design and implementation of a programming system and an associated runtime, called Rim, for DL-based media-processing on an edge cluster containing multiple GPUs. Rim must support the performance requirements (throughput and latency) of concurrent media-processing sessions, while maintaining high cluster utilization. Design principles. Rim uses the following design principles to satisfy these requirements.\n\nExpose application structure and requirements. The mediaprocessing applications we use employ a pipeline (or, more precisely, a directed acyclic graph or DAG [51,55], Fig. 1) of computing modules, where each module represents either a DL model or computation on a CPU. For example, complex activity detection [42] uses an object detection DL model to extract tubes (sequences of object bounding boxes), a re-identification model to identify tubes belonging to the same person, CPU processing to determine spatial and temporal relationships between tubes, and an DL model for activity detection [60]. Similarly, traffic monitoring [55] uses an object detector, followed by a vehicle classifier or a human recognition model. Exposing this coarse-grained application structure to Rim is important to support performance requirements while maintaining high utilization.\n\nMoreover, unlike the cloud, edge clusters have limited elasticity, so, to prevent applications from overloading the cluster, each client (typically, an end-device that requests edge cluster processing) must explicitly specify its desired frame rate and latency requirements.\n\nExploit accuracy/performance trade-offs. Rim should be able to exploit application-specified accuracy/performance tradeoffs; it can support more clients each at a slightly lower fidelity. This leverages a line of deep learning research that has explored resource/accuracy tradeoffs for DL models, using, for example, model compression [15,28,33] techniques that reduce the resource footprint of a DL model, while only minimally impacting model accuracy. If Rim is explicitly aware of alternative model instances for a given model, it can reduce resource usage for one application to accommodate others, thereby increasing utilization and throughput.\n\nThis requires the application developer to determine if the accuracy degradation from leaner models is acceptable: we argue that developers will need to benchmark their end-to-end application accuracy anyway, and they have an incentive to explore leaner models if Rim can support more clients for their applications.\n\nCapitalize on the predictability and efficiency of DL model execution. Prior work has observed that DL model execution is predictable [19,32,55], and leveraged this predictability to either ensure high utilization [55], or achieve fairness [32]. Rim can use similar techniques to estimate resource allocation. Rim must also pack DL models efficiently to ensure high GPU utilization; recent work [55] batches inputs and temporally multiplexes DL models, while other work [65] employs spatial multiplexing for training.\n\n\nRim Design\n\nIn this section, we begin with an overview of Rim, followed by a detailed description of its components.\n\n\nSystem Overview\n\nBasic abstractions and Rim workflow. Rim expresses processing of video and audio streams using an abstraction we call a media DAG, or mDAG (Fig. 1). In an mDAG, a vertex (called a module) represents either a CPU computation, or an invocation of a DL model on a GPU. An edge in the graph represents a data dependency. Similar data-flow programming models exist for packet processing [38], massively parallel processing [34], and scientific computations [54].\n\nClients of Rim initiate sessions by invoking the Rim master (Fig. 2). A session requests allocation of resources to execute an mDAG on the Rim cluster on an input stream, with a specified frame rate and end-to-end latency. If the master admits the session, clients send video frames or audio segments to workers, who collectively execute various mDAG modules, while respecting data dependencies, then return the results to a client. Rim allocates each worker one or more CPU cores, and exactly one GPU. Architecture. Rim re-uses two architectural principles commonly seen in cluster schedulers [21,24]. The first is a master-worker design, in which a centralized master makes placement decisions, and workers at each node manage local scheduling to achieve end to end scheduling objectives (in our case, performance objectives). The other is control-data separation, in which clients contact the master for control decisions, while data flows directly through workers to avoid the master becoming a bottleneck. Design principles and challenges. This design applies the principles identified in \u00a72 as follows. By using an mDAG ( \u00a73.2), Rim explicitly exposes application structure, which helps it satisfy performance objectives and meet utilization goals. Using this, together with client-specified performance objectives, Rim can better manage cluster. In Rim, each mDAG has one or more mDAG instances, where each instance represents a different point in the performance/accuracy trade-off space (or a different quality). The Rim runtime can dynamically adapt session quality based on resource availability ( \u00a73.3). Finally, Rim leverages the predictability of DL execution by profiling mDAG instances, and uses these profiles to make initial placement decisions that pack DL models efficiently ( \u00a73.3).\n\n\nSessions and mDAGs\n\nThe Session API. A client running on a user device instantiates a session using setup_session(mDAG, perfobj), where mDAG is a unique name for the mDAG, and perfobj specifies the performance objectives of the client. Currently, Rim supports two performance objectives: a desired frame rate and a target end-to-end latency. setup_session returns a session handle. The client can tear down the session using teardown_session(handle). Clients send data to Rim in application data units that are either individual video frames, or segments of audio of a fixed duration (for convenience, we refer to both of these as frames) using send(handle,frame), and can receive results from Rim using receive(handle). Explicitly exposing a session abstraction 1 helps Rim achieve the performance requirements of a given media stream, and manage overload. A client library implements this API. mDAGs and the mDAG library. Rim represents media-processing using a data-flow graph called an mDAG. Each distinct mediaprocessing application has its own mDAG, identified by a unique name. For instance, the complex activity detection [42], or actdet, mDAG can detect complex activities using video streamed from multiple surveillance cameras, while the en2ge mDAG can translate English audio into German audio (Fig. 1). mDAGs reside in an mDAG library and session instantiation uses the mDAG's unique name. A future version of Rim might permit user-defined mDAGs; today, putting together an mDAG requires 1 Recent work on inference, Nexus [55], uses a slightly different notion of a session. In Nexus, a session is an internal (i.e., not client visible) construct that tracks processing of requests for a given model. We elaborate on the importance of this distinction in \u00a74.  significant understanding of DL models and their performance and accuracy properties, so, at least in the near term, we expect specialized mDAG developers to specify mDAG definitions.\n\nA node (or module) in an mDAG represents either a computation on the CPU, or a DL model invocation on a GPU. For instance, the en2ge mDAG (Fig. 3) consists of 3 GPU modules and 1 CPU module. The GPU modules perform speech recognition, neural translation and speech synthesis respectively. The CPU module performs audio decoding and assembly of translation results. A link between two modules in the mDAG represents a data dependency between them; for instance, the output of speech recognition is fed into a neural translation module that converts English text to German text. In actdet, the GPU modules invoke object detection, re-identification and activity detection DL module, while the CPU modules apply rules to determine spatio-temporal relationships between people and objects in the video.\n\nModules in Rim can be stateful unlike prior work [55]. For example, in actdet, one of the DL models takes as input on a sequence of bounding boxes, called tubes, derived from a fixed sequence of frames. A CPU module assembles a tube before invoking that DL model by maintaining successive bounding boxes detected by an object detector. In \u00a74, we evaluate Rim using several mDAGs including three stateful ones (Fig. 1). mDAGs separate DL model invocation from CPU processing because, while CPU scheduling techniques are mature enough to be able to multiplex computations on a CPU to achieve high utilization, increasing GPU utilization still requires leveraging application structure. Exposing the DL model invocations and dependencies to Rim's runtime allows it to ensure high GPU utilization, as we show later. This is particularly important given the high relative cost of GPUs. mDAG instances. The DL community has invested significant effort in model compression and acceleration techniques (e.g., [28,33]) such as parameter pruning, low rank factorization, and knowledge distillation [15]. These techniques can reduce memory footprint and GPU resource requirements while marginally impacting accuracy. In practice, applications may be able to tolerate these accuracy drops, so Rim allows mDAG definitions to specify multiple instances for each GPU module. 2 For example, the speech recognition module in Fig. 3 can use three different instances: Jasper [36], Wav2letter [16], and Deepspeech2 [13], where Jasper has the lowest error rate [48] but the highest GPU resource consumption (which leads to the lowest throughput).\n\nWhen each module has multiple instances, an mDAG can have many mDAG instances, where an mDAG instance contains one instance chosen from each module (shown in Fig. 3). As we describe later, Rim (a) ranks ( \u00a73.3) mDAG instances by resource usage which correlates with accuracy, (b) dynamically adapts ( \u00a73.3) which mDAG instance a client session uses based on resource availability. Internet video streaming uses similar quality adaptation [11,43].\n\nMany mDAGs can share a module instance. For example, both actdet and another mDAG for traffic monitoring [55] can use the object detector Yolo [52].\n\n\nPlacement and Quality Adaptation\n\nOverview. To initiate a session, a client invokes the setup_session() method via RPC on the Rim Master. The master performs two functions: initial mDAG placement, and mDAG adaptation. The task of placement is to determine which workers should execute the session's mDAG instance to satisfy the session's performance objectives. The master may choose to place a session on an existing mDAG instance (e.g., belonging to another session), or assign the session to an under-utilized worker (Rim assigns each worker one or more CPU cores, and exactly one GPU).\n\nPlacement relies on off-line profiling. Rim performs two kinds of profiling: per-mDAG profiling, and per-module profiling. The former allows it to place an mDAG on a single worker to reduce latency. To improve utilization, Rim resorts to cross-worker placement, for which it uses per-module profiling. Rim ranks mDAG instances using mDAG profiles; this helps it adapt quality to load variations. Once it determines a placement, Rim's master automatically instructs the assigned workers to load and warm-up DL models for session execution, then generates steering configuration to route frames from clients directly to those assigned workers ( \u00a73.4).\n\nTo batch or spatially multiplex? Rim aims to achieve high GPU utilization. There are two general ways to do this. One is batching input data [32,55], but, for batching to be effective, batch sizes have to be large (on the order of 10s of frames). Rim has fewer opportunities than other systems to leverage batching, because: (a) batching frames within the same session can increase end-to-end delay significantly, (b) batching frames from different sessions works only if there are 10s of concurrent sessions that invoke the same DL model. Because edge clusters are likely to see lower statistical multiplexing than cloud clusters, Rim does not rely on batching. Moreover, as we show in \u00a74, some models in our mDAGs can utilize a significant fraction of the GPU even with a single frame input (i.e., without batching). For these reasons, Rim uses spatial multiplexing, in which the GPU concurrently executes kernels from multiple DL models (to maximize GPU core usage). Profiling. Rim performs two kinds of off-line profiling 3 .\n\nPer-mDAG instance profiling. It obtains an mDAG instance's resource footprint on a given worker by (off-line) profiling the maximum frame rate the worker can sustain for that instance. Fig. 4 illustrates the profiling procedure for two different instances of two mDAGs. For each instance, Rim experimentally determines the highest frame rate beyond which the worker is unable to keep up with the inputs; this determines the instance's maximum frame rate. In Fig. 4, the higher-quality traffic mDAG saturates at 16 fps, the lower quality one can sustain up to 24 fps. In addition to profiling the 8   maximum frame rate for each mDAG instance on each worker, Rim also profiles the latency incurred by each frame across the mDAG. It uses these profiles in single-worker placement, described below.\n\nPer-module profiling. To enable cross-worker placement, Rim also profiles individual modules (both CPU and GPU) on each type of worker. Specifically, for each module, it determines the maximum frame rate that the module can sustain on that worker. Rim profiles the CPU module as well, since the CPU module can be a bottleneck for some mDAGs (like avatar extraction). Existing systems [19,55] either ignore profiling the CPU module or have only considered relatively lightweight CPU computation, an important reason for their poor performance relative to Rim ( \u00a74). mDAG instance ranking. Rim uses profiles to rank mDAG instances. Instance ranking allows Rim to exploit performance/accuracy trade-offs ( \u00a72). In ranking mDAG instances, it assumes that instances with a higher overall GPU resource usage will have comparable or higher accuracy. This simplifies the task of instance ranking: while each DL model developer often documents the model's resource footprint and accuracy, and an mDAG's resource usage can be easily determined by profiling (discussed below), Rim cannot derive its end-to-end accuracy from the accuracy of the individual DL models (because, for instance, mDAG's can use simpler CPU-based processing that can improve accuracy).\n\nRim only generates a total order when ranking instances. Consider a two-module mDAG 1 , 2 , where each module has two instances 1 and 2 , and 1 has a larger resource requirement than 2 . In theory, there are four different instances of this mDAG: 11 21 , 11 22 , 12 21 and 12 22 . Rim selects 11 21 and 12 22 , but only one of 11 22 and 12 21 4 , since, even though it may be possible to order these two instances by resource footprint, it is unclear which one dominates in accuracy. Rim assumes that, as long as it satisfies a session's performance objectives, it can use any one of the mDAG instances. This relies on mDAG developer intuitions, in much the same way as video quality adaptation today relies on guidelines developed for acceptable qualities for different devices [1]. An mDAG instance with a lower maximum frame rate requires more resources and is likely to be more accurate, so Rim ranks it as having higher quality. The Placement Algorithm. Rim also uses profiling for session placement. When the client invokes session_setup(), it specifies the mDAG name, and two performance objectives: the target frame rate, and the target end-to-end latency. Rim checks which mDAG instances can meet the target end-to-end latency, using two pieces of information: round-trip time estimates from the client to master, and the profiled latency 5 (discussed above) for that instance. It filters out mDAG instances that cannot meet the latency objective. Of the remaining mDAG instances, Rim tries to place the highest-ranked mDAG instance on a worker on the cluster.\n\nFrame-rate proportionality. Rim's mDAG instance placement algorithm uses an experimentally derived observation, frame rate proportionality. Consider an mDAG instance , whose profiled maximum frame rate on worker is . Suppose that a client wishes to run the mDAG at a frame rate < , and Rim allocates worker to . Then, frame-rate proportionality suggests that uses up of the worker's resources, leaving 1 \u2212 free for other sessions (we call this the residual capacity).\n\nThis is a lower bound on the residual capacity of the worker. To understand why, observe that, for media-processing applications, processing complexity is linearly proportional to the frame rate. Consider two mDAG instances and , whose maximum frame rates on the worker are 10 and 20 respectively. If the GPU (or CPU) is a bottleneck for both these instances, it makes intuitive sense that and can concurrently execute on the worker at half their rates (5 and 10 fps respectively). In other words, when executes at 5 fps, the worker's residual capacity is 50%. However, if the GPU is the bottleneck for and the CPU for , then both chains can run on the worker concurrently, which is why our residual capacity estimate is a lower bound.\n\nAs an aside, for all the mDAGs we evaluate in this paper, the most memory intensive mDAG instance, the highest ranked en2ge instance, has a peak memory utilization of 2.34 GB, well below the total memory available in modern GPUs (e.g., the Nvidia 1080Ti has 11 GB). Thus, for Rim, GPU memory is not a bottleneck (as, for instance, it can be in systems that use batching [19,32]).\n\nSingle-worker placement. Rim's placement algorithm uses this observation to place an mDAG instance on a single worker whenever possible. Suppose that Rim has a cluster with workers, and each worker is currently executing one or more mDAG instances. Let the residual capacity of the -th worker be . Now, suppose a client invokes setup_session() for an mDAG whose highest ranked instance (after filtering instances that don't satisfy the latency objective) has a maximum frame rate of , and the client's performance objective specifies a frame rate of . Rim uses a best-fit strategy to minimize fragmentation: it allocates the session to that worker whose > and \u2212 is least. Cross-worker mDAG placement. Even with best-fit frame-rate proportional placement, single worker placement can strand resources. In this case, Rim accommodates new sessions by placing their mDAG modules on multiple workers. For example, if an mDAG instance has two modules and , and the cluster has two workers and neither of which have enough residual capacity to accommodate the entire mDAG instance, Rim tries to allocate to one worker (say ) and to another worker (say ).\n\nTo be able to do this, Rim also profiles the maximum frame rate for each module, and makes the same frame-rate proportionality assumption for each module in deciding the best-fit worker for the module. While profiling modules, Rim is careful to include serialization overhead necessary for communicating the output of one module to another worker. Moreover, when two successive modules in an mDAG have very high serialization overhead, Rim pins them together to ensure that those two modules are co-located on the same worker. Quality Adaptation. When a client requests a new session, Rim may not have enough capacity to place the highest-ranked mDAG instance in the cluster. Its runtime then invokes quality adaptation, a technique that frees up resources by changing the mDAG instance used for a given session.\n\nIn order to accommodate the new session, Rim demotes an existing session: uses a lower-ranked (or lower \"quality\") mDAG instance for that session in order to free up resources. When a session terminates, Rim attempts to promote one or more existing sessions, i.e., uses a higher-ranked mDAG instance for that session. In our current implementation, Rim uses a simple promotion (demotion) policy: promote (respectively demote) the session demoted (respectively promoted) the furthest in the past. We have left to future work to explore other policies such as finding the session which would free up the most resources if demoted, or use the least additional resources if promoted. Thus, using promotion and demotion, Rim attempts to satisfy as many clients as possible, while giving client sessions the highest quality when possible.\n\n\nOther Details\n\nGenerating steering configurations. When a client invokes setup_session(), Rim, after determining the mDAG placement, automatically generates steering configurations which instruct: (a) the client how to steer frames to the worker hosting the first module in the mDAG; (b) the client how to proportionally split frames when session has parallel mDAG instances; (c) each module which module (on which worker) to send its output to. These steering configurations enable control/data separation in Rim ( \u00a73.1). Model loading and warm-up. The placement algorithm can decide to place a session's mDAG instance on a worker where another session is already using the same instance. In that case, the new session simply reuses models from the existing instance. If the worker does not have the mDAG instance loaded, Rim instructs the worker to load the DL models corresponding to the mDAG instance. Rim also warms-up the model [22] by testing it with dummy data to avoid delays resulting from kernel just-in-time compilation [26]. Without this, the first few frames of the client incur significant latency. Handling stateful modules. By design, Rim dispatches requests for a given session to the same worker, and its runtime allocates memory to maintain state across requests for the same session, to ensure correctness of stateful modules. High-rate sessions. Depending on the resources in a cluster, an mDAG's maximum frame rate might be lower than a client session's target frame rate. In this case, Rim can process the session's traffic using two mDAGs running concurrently. To determine the frame rate splits between these mDAGs, Rim searches for the split that best fits the residual capacities on the workers. Rim cannot split mDAGs with stateful modules; instead, it can attempt to migrate sessions (see below) to free up resources, which we leave to future work. Failure and session migration. When a worker fails, Rim must migrate its sessions to other workers. 6 To do this, Rim's master must first find a placement for each session (potentially by demoting the session if necessary), load and warm-up the DL models in the mDAG instance, then generate a steering configuration for the new placement and send it to the client. Of these steps, model loading and warm-up can be expensive (on the order of seconds, \u00a74). Admission control and cloud offload. Rim can reject client sessions because the cluster does not have enough residual capacity to satisfy the target frame rate using the highest-ranked mDAG instance. It strives hard to support admitted sessions. It reserves a small amount of capacity at each worker to handle failures. However, when a worker failure occurs, the cluster may not have enough capacity to satisfy all the sessions. Rim finds the session with the loosest latency target, and if the target is loose enough to accommodate a round-trip to the cloud, Rim offloads the mDAG instance to the cloud. When no such mDAG exists, Rim has no option but to evict the session (for now, it uses a random drop policy).\n\n\nRim Evaluation\n\nWe compare Rim against two other model inferencing systems, Nexus [55] and Clipper [19], then perform an ablation study that illustrates the impact of Rim's design decisions.\n\n\nMethodology\n\nImplementation. We have implemented all the features of Rim described in \u00a73. Each worker runs in a separate container, and uses TF-Serving [59] to spatially multiplex DL models on the worker's dedicated GPU. Rim is 10,478 lines of Python code, and includes the client library, the master, and worker implementations. The mDAG library is an additional 13,892 lines of Python. Testbed. We deployed Rim on a cluster containing 8 servers with a total of 14 GPUs. We deliberately designed the testbed to be heterogeneous: this illustrates Rim's ability to place and adapt mDAGs across heterogeneous clusters. The testbed contains a range of Nvidia GPU models: one Titan, one Titan X, one Titan Xp, two 1080s, three 1080 Tis, two 2080s and four 2080 Tis. It also includes different Intel CPUs from the Xeon E5 to the Core i9.  Nexus [55] also serves image requests using a cluster of GPUs while satisfying a latency SLO. Like Clipper, it profiles model latency for different batch sizes, then dynamically batches inputs and schedules them on GPUs to ensure that it meets latency objectives for each request (in our experiments, a request is a frame). For each DL model, Nexus hosts multiple instances of the model, but, unlike Clipper, manages model placement and dynamically adapts the number of instances based on the observed workload and the latency SLO in order to minimize GPU usage.\n\nComparison methodology. For our comparisons, we disable several features of Rim, because these alternatives do not support these features: multiple mDAG instances (we only use the highest-ranked instances), admission control, and all quality adaptation (we evaluate these in our ablation study, \u00a74.3). Moreover, neither of these systems provides Rim's session abstraction, so for them we send each frame as an independent request. Metrics. For all three systems, we focus on three metrics: (1) The average finish rate of each session in a cluster, which is defined as the ratio between the output frame rate and the target frame rate of the session. The ideal finish rate is 1 (or 100%), but if a system is either overloaded or has design flaws, it may not be able to sustain a high finish rate. (2) The average SLO-compliance of each session, which is the percentage of frames whose end-to-end latency satisfied the latency SLO. (3) The average gpu utilization across the cluster, using the Nvidia system management interface [6]. The first two metrics guarantee correctness and usability for media-processing applications ( \u00a72), while the third measures efficacy of resource management. Workloads. Tbl. 1 lists the mDAGs used in our experiments. The actdet mDAG performs complex activity detection [42] using Yolo [52] or SSD [41] for object detection, and [64] for re-identification. We use a trimmed version of this mDAG, tracker, for our comparison experiments; actdet includes a stateful module ( \u00a73.2) for activity recognition (which maintains state across several frames), but Nexus and Clipper do not support such modules. The face mDAG extracts facial keypoints used for avatar generation from each frame, using a face detector model and [23] for extracting keypoints. The traffic mDAG is taken from [55] and uses SSD [41] to detect objects, and two other models to recognize pedestrian and vehicle makes. The pose mDAG from [5], which recognizes human tracker face traffic -1 8 (6) 10 (6) 10(6) -2 6(6) 8(6) 12(6) & 2(9) -3 10(10) & 3 (5) 4(6) 8(5)  poses in video by analyzing features extracted by Openpose [14] in individual frames. The caption mDAG from [61] uses S2VT to generate captions describing events in video by feeding the output of the fully-connected layer from VGG16 [56] or AlexNet [39] to S2VT's LSTM unit. The en2ge mDAG is described in \u00a73.2.\n\nTbl. 1 lists the number of instances of each mDAG for quality adaptation used in \u00a74.3; our comparison ( \u00a74.2) uses only the highest ranked instance. Moreover, our comparisons only use three of these mDAGs (tracker, face and traffic). We do not use pose, caption and actdet since Nexus and Clipper do not support stateful chains, and eng2e because Nexus' current implementation does not support audio streams. Each experiment's workload contains multiple sessions of each mDAG, as described below.\n\n\nComparison\n\nMethodology. Can Nexus and Clipper plausibly support mediaprocessing applications even though they were not explicitly designed for it? To address this question, we first generate three maximal workloads in Rim (labeled -1, -2 and -3, Tbl. 2): a maximal workload in Rim is a collection of sessions, such that Rim rejects a new session request. In -1, the total resource requirements are roughly equally divided between the 3 mDAGs. The GPU-intensive traffic dominates (requires most resources in) -2, while tracker dominates -3. We then run these sessions on Nexus and Clipper and evaluate the frame rate they achieve for each session.\n\nRim carefully places mDAG instances on the cluster. Clipper and Nexus perform resource allocation decisions at the granularity of individual DL models and they allocate different containers (in Clipper) or backends (in Nexus) to different models. Nexus handles model placement and provisioning as well as steering requests to model replicas, so we simply profile all our DL models in Nexus, and allow Nexus to manage model placement. Clipper, on the other hand, does not reason about model placement, so for Clipper we: (a) allocate as many model instances as Rim does, (b) ensure that all model containers for an mDAG are co-located with the client/front-end.\n\n\nK1\n\nK2 K3  K4  K5  K6  K7  K8  F1  F2  F3  F4  F5  F6  F7  F8  F9  F10  T1  T2  T3  T4  T5  T6  T7  T8  T9  However, because Clipper does not do frame-rate aware placement, we randomly assign mDAG instances to GPUs. Target frame-rate. Clipper has 34%-40% and Nexus has 10%-28% lower aggregate finish rate than Rim (Fig. 6) under all three maximal workloads. Under the first workload ( -1 in Tbl. 2), both Clipper and Nexus have more than 28% lower finish rate. To understand why, Fig. 7 shows the achieved finish rate for each alternative, for each session, under -1.\n\nClipper. Clipper is nearly able to match the frame-rate requirement for 8 of the 28 sessions (e.g., 5 of the tracker sessions), but it fails in two ways. First, because Clipper does not do resource-aware placement, our random placement places models of two relatively heavyweight mDAGs (e.g., face and traffic) on the same GPU (e.g., it co-located F5 and T9 on the same GPU). The resulting contention reduces the frame rate. Second, some sessions exceed the GPU memory usage (e.g., F9 and T7); model containers independently allocate and hoard memory resources, so memory often becomes a bottleneck. By contrast, Rim's worker carefully de-allocates memory after use, so Rim never encounters memory limits.\n\nIn part, Clipper performs as well as it does because we have been generous to Clipper in two ways. Clipper does not profile CPU modules, but our random placement gives it sufficient resources to handle CPU-intensive mDAGs, like face (by contrast, Nexus is significantly impacted by CPU-intensive mDAGs, see below). Moreover, Clipper does not have techniques to automatically split the latency SLO across different modules (since it does not support DAG structured applications); we manually assign generous permodule latency SLOs that result in higher finish rates. Without these, we expect Clipper to perform worse than it does.\n\nNexus. Nexus is also unable to satisfy the target frame rate for any face or traffic sessions, for three different reasons. First, Nexus is focused on GPU-intensive applications, and only profiles GPU modules. However, many practical mDAGs involve CPU-intensive computations (e.g., face) which should be profiled to assess resource needs and inform scheduling and placement. In our experiments, Nexus co-located all face sessions' CPU modules on the same machine, so all face sessions had a finish rate (F1 to F10 in Fig. 7) lower than 50%. To confirm this hypothesis, we manually doubled the number of machines that serve face mDAG's CPU module, and observed an increase in face finish rates from 49.43% to 76.76%. Because Rim profiles all modules and is aware of the resource requirements of CPU modules, its placement allocated five machines to run face sessions, and achieved a perfect finish rate.\n\nSecond, in the experiment where we doubled the number of machines serving face, the increase in finish rate for face sessions adversely impacted the finish rate for other sessions. This interference arises because Nexus' temporal sharing (where it time-slices the GPU across multiple DL models) with batching is not effective for our workloads (and for edge clusters in general). When batch sizes are large, as in a cloud setting, temporal sharing can achieve good throughput. But for an edge workload, with lower statistical multiplexing, it is hard to form a large batch without increasing latency ( \u00a73.3). The resulting small batch sizes increase the relative overhead of temporal sharing, so Nexus can support fewer GPU computations than Rim for the same amount of resources. Thus, in Nexus, the rest of sessions have to compete against face sessions on a over-subscribed GPU, resulting in interference.\n\nThird, Nexus uses a latency splitting algorithm [55] to derive each GPU module's latency SLO, but this algorithm assumes homogeneous GPUs in a cluster. But our cluster contains a range of GPUs (from Nvidia 1080 to Nvidia 2080 Ti). We deliberately designed our testbed this way to mimic hardware heterogeneity resulting from incremental upgrades [10]. In our experiment, we used as input to the latency splitting algorithm the average latency profile 7 for a given batch size across all GPUs. Because this doesn't accurately reflect the latency on some GPUs, Nexus could not consistently satisfy the latency SLO for some chains on some GPUs. To validate this, we manually increased the latency SLO of each module for Nexus and observed an increase in traffic finish rate from 70.79% to 88.04%. GPU Utilization. Furthermore, both Clipper and Nexus have lower GPU utilization under all three maximal workloads as shown in Fig. 6. There are two reasons for this. First, because they sustain lower frame rates, they fundamentally do less work. More important, both systems rely on batching to increase utilization. In our experiments, most models achieve batch sizes in Nexus of less than a handful of frames under all three workloads (Fig. 8). In our workload, frame rates are on the order of 6-10 fps, so inter-frame arrival times are on the order of 100-166 ms. If a model's latency SLO is 500 ms, it can probably afford to batch just 3-5 frames before invoking the DL model. To achieve high utilization, some models require a high degree of batching; for instance, Inception [58] requires a batch size of 32. Media-processing workloads at the edge are unlikely to achieve such high degrees of batching, which is why Rim uses spatial multiplexing instead ( \u00a74.4).\n\n\nAblation Study\n\nQuality Adaptation. To evaluate the benefit of quality adaptation, we compare Rim against: (1) Rim-high, which always uses the highest-ranked mDAG and does not perform admission control and (2) Rim-low, which always uses the lowest-ranked mDAG.\n\nWorkload. This experiment uses the complete testbed ( \u00a74.1) with 14 GPUs. We evaluated all three alternatives (Rim, Rim-high and Rim-low) by adding a new session every 10 seconds until we reach the full capacity of the edge cluster (when Rim rejects new sessions). Then we maintain the workload for 30 seconds, and remove a session 7 We use the average latency profile only for latency splitting. Nexus still uses the per-GPU profile for its scheduling decisions.\n\n\nWorkload-1\n\nWorkload-2 Workload-3 0 every 10 seconds in the reverse order in which they have been added. Each session randomly chooses an mDAG from Tbl. 1.\n\nResults: Finish rate. Fig. 9 shows how Rim dynamically adapts to workload changes during a 700 s window. The top panel shows the finish rate for Rim and Rim-high (Rim-low's finish rate is always as high as Rim's so we omit it). The middle panel shows when each session is started, as well as when quality adaptation decisions are made by Rim. The bottom panel shows the average GPU utilization.\n\nUntil the workload saturates the cluster ( < 200), both Rim and Rim-high have comparable finish rate. At 200 s, a total of 19 sessions run on the cluster. When the 20th session starts, Rim detects that it is running at the full capacity, and starts demoting mDAGs to release resources for new session. In the current implementation, Rim demotes the session that hasn't been demoted for the longest time ( \u00a73.3), if demoting it can release enough resources for the new session. For example, at = 200, Rim demotes 1 to accommodate 20 (Fig. 9, middle panel). At = 220, instead of demoting 3 which doesn't release enough resources, Rim demotes 4 to accommodate 22 . One interesting case is 3 , an en2ge session, which has 3 instances (Tbl. 1). Rim demotes this session twice (once at = 230 and again at = 330). Even as it demotes sessions, Rim is able to achieve a nearly perfect finish rate. On the other hand, Rim-high's finish rate drops beyond the 19th session because the cluster is at capacity. This illustrates the importance of profiling and admission control in Rim. At around = 340, all mDAGs have been demoted to their lowest ranked instance, and Rim rejects subsequent sessions.\n\nBeyond = 370, Rim starts removing sessions; when a session is removed, an ongoing session is promoted if possible. In our current implementation, Rim promotes the session that hasn't been promoted for the longest time ( \u00a73.3), if the released resources from the recently removed session is enough for its promotion. For example, at = 370, 34 has ended, therefore, Rim promotes 1 to utilize the released resources. At = 380, 33 has ended, but Rim demotes 3 instead of 2 because the released resources are not enough to promote 2 . In this way, Rim tries to support each session at its highest ranked mDAG instance when possible.\n\nResults: Utilization. Rim achieves an average GPU utilization of 52.36% across the 14 GPUs at = 200 (Fig. 9, bottom), while Rim-low only achieves 28.73%.\n\nResults: Number of sessions. Rim is able to maintain high finish rate for 34 concurrent sessions in our edge cluster, while Rim-high can only maintain high finish rate for 19 concurrent sessions. Besides, Rim achieves much higher GPU utilization than Rim-low by promoting sessions when resources are available. Overall, Rim supports 78.9% more sessions than Rim-high, and utilizes resources more efficiently than Rim-low.\n\nResults: SLO-compliance. We now quantify the SLOcompliance (the fraction of frames that satisfied the chain's SLO) in this experiment (figure omitted for brevity). Four mDAGs have a perfect SLO-compliance, while three mDAGs have an average SLO-compliance of over 97%. Moreover, for those frames that violated the latency SLO from these three mDAGs, their actual latencies were only up to 11.3%, 14.9% and 4.8% beyond the latency SLO for mDAG actdet, caption and en2ge respectively. To stress test our system, for these three chains, we assigned much tighter SLO requirements (close to their end-to-end latencies) than for other chains. Our current placement algorithm is aggressive, placing a chain on servers whose latency may be close to the chain SLO; small latency variations for a few chains trigger these violations. A more conservative placement would leave more headroom, at the risk of lower utilization; we left it to future work to explore this tradeoff. Placement. In this experiment, we compare Rim's placement against: (1) Rim-RR, which places a new session on one of the GPUs in a round robin fashion; (2) Rim-memory, which places a new session on the worker with largest remaining GPU memory available; and (3) Rim-utilization, which places the new session on the GPU with lowest GPU utilization. We compare these on the full testbed, on the maximal workload that we used in Fig. 9.\n\nResults. Fig. 10a shows the finish rate for Rim's maximum frame rate based placement and the three other placement algorithms. Rim achieves nearly perfect finish rate (frames that violate latency SLO are counted as unfinished), but Rim-RR achieves poor finish rate because it ignores GPU and workload heterogeneity. Rim-memory does better, but because memory is not the bottleneck in Rim ( \u00a73.3), it cannot match Rim. Rim-utilization comes closest, but the GPU is not the only bottleneck for streaming applications. For example, face has a relatively low GPU utilization, but a high CPU utilization on average. Rim-utilization places three face sessions on the same worker, which overloads the CPU, leading to lower finish rate. The need for cross-worker placement. We compare Rim with two other alternatives: (1) Rim-NC, which does not include cross-worker placement, but places the mDAG on the worker with highest remaining resources available when overloaded; and (2) Rim-NCdrop, which also does not include cross-worker placement, but rejects the session instead. The workload for this experiment consists of twenty-nine sessions on the entire testbed. This workload is maximal for Rim (no additional sessions can be accommodated), but is a different maximal workload than the ones used in the previous experiments, and is designed to maximize the likelihood of resource fragmentation to coerce Rim to invoke cross-worker placement.\n\nResults. Rim-NCdrop can only accommodate twenty-four sessions out of twenty-nine, since the remaining five sessions cannot find a GPU for single-worker placement, while Rim is able to accommodate all twenty-nine sessions by placing those five sessions across ten GPUs. Rim-NC can accommodate all twenty-nine sessions, but has a low average finish rate of 79.3%; Rim's finish rate is 99.14% for this workload. Spatial multiplexing. We compare Rim against Rim-temporal in which each DL model has exclusive access to the GPU during its execution (i.e., temporal multiplexing). Let spatial gain be the ratio between the highest maximum frame rate with spatial multiplexing over that with temporal multiplexing.\n\nResults. Fig. 10b shows the spatial gain for different mDAG instances from Tbl. 1. Spatial multiplexing increases the supported frame rate across these mDAG instances by 8% to 99%. In this experiment, the gains come from spatially multiplexing models of the same mDAG on the GPU. Even actdet, an mDAG with a heavyweight ACAM [60] module (Fig. 10c) gains 36% to 40%.\n\n\nJustifications\n\nNo batching. Other image-based inference systems like Nexus and Clipper batch aggressively. For media-processing, DL models are often based on deeper and more complex neural network architectures than those used for images, so some of the DL models in our mDAG can incur high gpu utilization even on a single frame. In Fig. 10c, Jasper [36] utilizes 63% of a 1080 Ti, and the activity detection model ACAM [60] utilizes 72%, so Rim does not use batching. Admission control. Fig. 4 shows that if we over-commit resources, instead of using admission control, by running mDAG at higher frame rates than the maximum the GPU can sustain, the actual number of frames processed drops with increasing offered load and the standard deviation also increases dramatically, leading to poor predictability. For this reason, Rim's placement algorithm with frame-rate proportionality does not over-commit resources. Model loading and warmup. During initial placement and migration, Rim waits for DL models to load and warm up ( \u00a73.3). These steps take between 1.37 to 4.50 sec for our mDAGs (Tbl. 1), of which 0.48 to 2.82 sec is the model loading latency. Serialization overhead. Rim co-locates two modules that exchange high-volume data with one another; in the absence of this, data serialization overhead can significantly impact latency. To demonstrate this, we ran two instances of actdet. One instance ran on a single worker. In the other, actdet was split across two workers where the   two modules that exchanged high-volume data were placed on different workers. The former instance had an end-to-end latency of 0.85 sec, but the latter incurred a latency of 7.60 sec. GPU utilization. In our comparison experiments ( \u00a74.2), Rim achieved an average GPU utilization from 32.1% to 37.9% under maximal workloads (Fig. 6), while in the ablation study, Rim achieved an average GPU utilization of 52.36% (Fig. 9). This difference comes from the difference in the corresponding workload; the ablation study uses several GPU-intensive mDAGs that the comparison experiments do not. For example, en2ge mDAG achieves an average GPU utilization of 73.55% at its maximum frame rate, and the caption mDAG 92.61%. To put these numbers in context, AWS reports an average GPU utilization between 10% to 30% [2,35], and Google reports an average utilization of 28% for its Tensor Processing Unit (TPU) [37]. Rim's average GPU utilization is almost 2\u00d7 of these values.\n\n\nRelated Work\n\nCluster Management for Inference. Much prior work has considered DL model inference in cloud clusters; Tbl. 3 summarizes the differences between Rim and this body of work. Rim explores a unique part of the design space, focusing on supporting frame rate requirements for DAG-structured media-processing applications at the edge with quality adaptation. Of these, we have discussed Nexus [55] and Clipper [19] in \u00a74 and explained how they differ from Rim. Nvidia Triton [49] uses CUDA streams to spatially multiplex models together on the same GPU, but it requires the system operator to manually specify the degree of parallelism, unlike Rim which determines this using profiling. OoO [35] uses a combination of temporal and spatial multiplexing to increase GPU utilization by merging small kernels into superkernels, and reordering them to satisfy the latency constraints, but these merging techniques can only be applied to models using the same architecture; in contrast, Rim is able to share the GPU across heterogeneous models.\n\nINFaaS [53] abstracts resource management and model selection for image inference. While it supports quality adaptation and profiles both CPU and GPU components, it is not designed to support framerate requirements of complex DAG-structured media-processing applications. InferLine [18] schedules machine learning pipelines to satisfy the end-to-end latency constraints. For a given pipeline, it selects the hardware accelerator and batch size using the offline profiling. Unlike Rim, it does not target frame-requirements, and uses temporal multiplexing which is likely to perform less well in an edge setting ( \u00a74.2).\n\nTensorFlow-Serving [59] can group individual requests into batches to increase throughput, deploy multiple versions of the same model without need changes to client code, and minimize inference overhead. Rim uses TensorFlow-Serving on each worker, but adds profiling, cross-cluster placement and adaptation. GRNN [27] accelerates RNN execution by minimizing synchronization overhead and balancing on-chip resource usage. Rim supports RNNs as well as other neural network architectures, as well as data dependencies between models. Other work has explored specific optimizations to reduce resource usage that can be useful for inference in general, and Rim in particular: PRETZEL [40] explores operator and parameter sharing in model inference, and Focus [29] explores a cascade classifier for processing video. Cluster Management for Training. Less relevant to Rim, Gandiva [65] uses spatial GPU multiplexing for training; Optimus [50] uses an online resource-performance model to estimate the training speed, given the amount of allocated resources, then dynamically allocate resources to each training job to minimize overall job completion time; GeePS [20] uses GPU-specific optimizations like background GPU/CPU data movement and data-parallel execution to achieve good efficiency and scalability. Pushing DL Models to the Edge. DL models are commonly deployed on powerful computers equipped with GPUs. Recent work has explored been pushing DL model execution closer to the input source. FastAcc [30] uses auto-encoder to compress the data volume for communication, so as to run DL models on mobile GPUs. 11 Besides, novel DL model architectures [28,33] further reduce the model size. Rim can take advantage of these developments, by partitioning the mDAG across devices and the edge cluster. Prior work on complex activity detection [42] has already demonstrated the benefits of this approach; we have left it to future work to extend Rim's mDAG orchestration to permit on-device execution.\n\n\nConclusions\n\nRim supports deep-learning based processing of audio and video streams on edge clusters. Applications for processing these streams often employ multiple DL models, and have an application structure well represented as a DAG. They also have target frame-rate and latency requirements, often for usability reasons. Given a client session's performance objectives, Rim uses an mDAG's performance profiles to derive placements that ensure that the session's frame-rate and latency objectives are met. It also leverages performance/accuracy tradeoffs to increase utilization and admit more sessions than otherwise possible. Experiments show that competing approaches designed to satisfy latency SLOs are not able to satisfy session target frame rates while Rim can. Future work includes exploring more applications in Rim, understanding Rim performance across multi-rack clusters, and exploring admission control techniques that permit over committing resources in order to increase utilization even higher.\n\n\nIoTDI '21, May 18-21, 2021, Charlottesvle, VA, USA \u00a9 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8354-7/21/05. https://doi.org/10.1145/3450268.3453521\n\nFigure 1 :\n1Examples of media-processing applications. The circled S denotes a stateful component.\n\nFigure 2 :\n2Rim overview.\n\nFigure 3 :\n3mDAGs and mDAG instances for en2ge. Each module in the mDAG definition can be implemented by one or multiple module instances. During runtime, Rim can dynamically switch between instances.\n\nFigure 4 :\n4Spatial profiling for four mDAG instances.\n\nFigure 5 :\n5Example of single-worker placement and cross-worker placement, where three sessions are placed on a single worker, and one session is placed across two workers.\n\nFigure 6 :\n6The average finish rate and GPU utilization for Rim, Clipper and Nexus under three maximal workloads.\n\nFigure 7 :\n7For \u2212 1, the observed finish rate for each session for Clipper and Nexus. K denotes tracker sessions, F denotes face and T denotes traffic.\n\nFigure 8 :Figure 9 :\n89The average batch size for Nexus. The finish rate and GPU utilization for Rim for quality adaptation.\n\nFigure 10 :\n10(a) The finish rate for various placements. (b) Spatial gain for different mDAG instances. (c) GPU utilization for DL models.\n\nTable 1 :\n1Number of mDAG instances for quality adaptation used in our experiments. These mDAGs are described inFig. 1. The starred mDAGs are used in our comparison experiments (which do not employ quality adaptation). Our ablation study uses all mDAGs.Comparison Alternatives. Clipper [19] provides a uniform inter-\nface for model serving, but supports a variety of frameworks (Ten-\nsorflow, Caffe etc.) in the backend. Each of Clipper's models is \nencapsulated in a Docker container, and a Clipper cluster can host \nmultiple instances of a DL model. Moreover, multiple model contain-\ners can access a GPU. Clipper batches inputs adaptively to increase \nmodel utilization, but does not attempt to do performance-aware \nmodel placement. It profiles model latency for different batch sizes \nto support latency SLOs. \n\n\nTable 2 :\n2Number of sessions of each mDAG in each workload.Each \n\n\nTable 3 :\n3Comparison of Inference Systems\nAccuracy/resource trade-offs are also possible for CPU modules. We have left this to future work.4 \nRim profiles each mDAG once, and not per session, so profiling does not affect client perceived latency.\nCurrently, Rim randomly selects one of these two for simplicity.5 \nPrior work in inference serving systems such as Clipper[19] and Nexus[55], has used profiling of DNN models to make scheduling decisions. Profiling has also shown to accurately predict execution times for such models[32].\nIn theory, Rim could also migrate sessions during promotion and demotion in order to better pack mDAG instances across the cluster; we have left this to future work.\nAcknowledgmentsThis work was supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.\nApple's HTTP Live Streaming. Apple, Apple: Apple's HTTP Live Streaming. https://developer.apple.com/streaming/.\n\nAWS re:Invent. AWS re:Invent 2018 Keynote. https://www.youtube.com/watch?v= ZOIkOnW640A&ab_channel=AmazonWebServices.\n\nEdge Computing Market Size, Share and Trends Analysis. Edge Computing Market Size, Share and Trends Analysis. https://www. grandviewresearch.com/industry-analysis/edge-computing-market.\n\nEdge Computing Market Worth $43.4 Billion By. Edge Computing Market Worth $43.4 Billion By 2027. https://www. grandviewresearch.com/press-release/global-edge-computing-market.\n\nMulti-person Real-time Action Recognition Based-on Human Skeleton. Multi-person Real-time Action Recognition Based-on Human Skeleton. https: //github.com/felixchenfy/Realtime-Action-Recognition.\n\n. Nvidia System Management Interface, NVIDIA System Management Interface. https://developer.nvidia.com/nvidia- system-management-interface.\n\nThe Low Latency Live Streaming Landscape in 2019. The Low Latency Live Streaming Landscape in 2019. https://mux.com/blog/the- low-latency-live-streaming-landscape-in-2019/.\n\n. Nvidia The, Egx, The NVIDIA EGX Platform for Edge Computing. https://www.nvidia.com/en- us/data-center/products/egx-edge-computing/.\n\nTarazu: Optimizing mapreduce on heterogeneous clusters. Faraz Ahmad, T Srimat, Anand Chakradhar, T N Raghunathan, Vijaykumar, ACM SIGARCH Computer Architecture News. 401Faraz Ahmad, Srimat T Chakradhar, Anand Raghunathan, and TN Vijaykumar. Tarazu: Optimizing mapreduce on heterogeneous clusters. ACM SIGARCH Com- puter Architecture News, 40(1):61-74, 2012.\n\nOboe: Auto-Tuning Video ABR Algorithms to Network Conditions. Zahaib Akhtar, Yun S Nam, Ramesh Govindan, Sanjay Rao, Jessica Chen, Ethan Katz-Bassett, Bruno M Ribeiro, Jibin Zhan, Hui Zhang, Proceedings of the Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18. the Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18Zahaib Akhtar, Yun S. Nam, Ramesh Govindan, Sanjay Rao, Jessica Chen, Ethan Katz-Bassett, Bruno M. Ribeiro, Jibin Zhan, and Hui Zhang. Oboe: Auto-Tuning Video ABR Algorithms to Network Conditions. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18, 2018.\n\nDeep speech 2: End-to-end speech recognition in english and mandarin. Dario Amodei, Rishita Sundaram Ananthanarayanan, Jingliang Anubhai, Eric Bai, Carl Battenberg, Jared Case, Bryan Casper, Qiang Catanzaro, Guoliang Cheng, Chen, International conference on machine learning. Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In International conference on machine learning, pages 173-182, 2016.\n\nOpenpose: Realtime multi-person 2d pose estimation using part affinity fields. Z Cao, G Hidalgo, T Martinez, S Simon, Y A Wei, Sheikh, IEEE Transactions on Pattern Analysis and Machine Intelligence. Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Real- time multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n\nA survey of model compression and acceleration for deep neural networks. Yu Cheng, Duo Wang, Pan Zhou, Tao Zhang, abs/1710.09282CoRRYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. CoRR, abs/1710.09282, 2017.\n\nWav2letter: an end-toend convnet-based speech recognition system. Ronan Collobert, Christian Puhrsch, Gabriel Synnaeve, arXiv:1609.03193arXiv preprintRonan Collobert, Christian Puhrsch, and Gabriel Synnaeve. Wav2letter: an end-to- end convnet-based speech recognition system. arXiv preprint arXiv:1609.03193, 2016.\n\nEcco: Edgecloud chaining and orchestration framework for road context assessment. Vittorio Cozzolino, J\u00f6rg Ott, Aaron Yi Ding, Richard Mortier, 2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI). IEEEVittorio Cozzolino, J\u00f6rg Ott, Aaron Yi Ding, and Richard Mortier. Ecco: Edge- cloud chaining and orchestration framework for road context assessment. In 2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI), pages 223-230. IEEE, 2020.\n\nInferline: Ml inference pipeline composition framework. Daniel Crankshaw, Gur-Eyal, Corey Sela, Xiangxi Zumar, Joseph E Mo, Ion Gonzalez, Alexey Stoica, Tumanov, arXiv:1812.01776arXiv preprintDaniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E Gonzalez, Ion Stoica, and Alexey Tumanov. Inferline: Ml inference pipeline composition framework. arXiv preprint arXiv:1812.01776, 2018.\n\nClipper: A low-latency online prediction serving system. Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E , 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). Boston, MAUSENIX AssociationGonzalez, and Ion StoicaDaniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gon- zalez, and Ion Stoica. Clipper: A low-latency online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 613-627, Boston, MA, March 2017. USENIX Association.\n\nGeeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server. Henggang Cui, Hao Zhang, R Gregory, Ganger, B Phillip, Eric P Gibbons, Xing, Proceedings of the Eleventh EuroSys Conference. the Eleventh EuroSys ConferenceACMpage 4.Henggang Cui, Hao Zhang, Gregory R Ganger, Phillip B Gibbons, and Eric P Xing. Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server. In Proceedings of the Eleventh EuroSys Conference, page 4. ACM, 2016.\n\nMapreduce: Simplified data processing on large clusters. Jeffrey Dean, Sanjay Ghemawat, Commun. ACM. 511Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Simplified data processing on large clusters. Commun. ACM, 51(1):107-113, January 2008.\n\n. TensorFlow Documentation. Savedmodel warmup. TensorFlow Documentation. Savedmodel warmup. https://www.tensorflow.org/ tfx/serving/saved_model_warmup.\n\nJoint 3d face reconstruction and dense alignment with position map regression network. Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, Xi Zhou, abs/1803.07835CoRRYao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi Zhou. Joint 3d face reconstruction and dense alignment with position map regression network. CoRR, abs/1803.07835, 2018.\n\nThe google file system. Sanjay Ghemawat, Howard Gobioff, Shun-Tak Leung, Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, SOSP '03. the Nineteenth ACM Symposium on Operating Systems Principles, SOSP '03New York, NY, USAACMSanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. The google file sys- tem. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, SOSP '03, pages 29-43, New York, NY, USA, 2003. ACM.\n\nCUDA Pro Tip: Understand Fat Binaries and JIT Caching. Mark Harris, Mark Harris. CUDA Pro Tip: Understand Fat Binaries and JIT Caching. https: //devblogs.nvidia.com/cuda-pro-tip-understand-fat-binaries-jit-caching/, 2013.\n\nGrnn: Low-latency and scalable rnn inference on gpus. Connor Holmes, Daniel Mawhirter, Yuxiong He, Feng Yan, Bo Wu, Proceedings of the Fourteenth EuroSys Conference. the Fourteenth EuroSys ConferenceACM41Connor Holmes, Daniel Mawhirter, Yuxiong He, Feng Yan, and Bo Wu. Grnn: Low-latency and scalable rnn inference on gpus. In Proceedings of the Fourteenth EuroSys Conference, page 41. ACM, 2019.\n\nMobilenets: Efficient convolutional neural networks for mobile vision applications. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam, abs/1704.04861CoRRAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef- ficient convolutional neural networks for mobile vision applications. CoRR, abs/1704.04861, 2017.\n\nFocus: Querying large video datasets with low latency and low cost. Kevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B Gibbons, Onur Mutlu, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationKevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B. Gibbons, and Onur Mutlu. Focus: Querying large video datasets with low latency and low cost. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 269-286, Carlsbad, CA, October 2018. USENIX Association.\n\nFast and accurate streaming cnn inference via communication compression on the edge. Diyi Hu, Bhaskar Krishnamachari, 2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI). IEEEDiyi Hu and Bhaskar Krishnamachari. Fast and accurate streaming cnn inference via communication compression on the edge. In 2020 IEEE/ACM Fifth Inter- national Conference on Internet-of-Things Design and Implementation (IoTDI), pages 157-163. IEEE, 2020.\n\nAvatar digitization from a single image for real-time rendering. Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jaewoo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-Chun Chen, Hao Li, 195:1-195:14ACM Trans. Graph. 366Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jaewoo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-Chun Chen, and Hao Li. Avatar digitization from a single image for real-time rendering. ACM Trans. Graph., 36(6):195:1-195:14, November 2017.\n\nOlympian: Scheduling gpu usage in a deep neural network model serving system. Yitao Hu, Swati Rallapalli, Bongjun Ko, Ramesh Govindan, Proceedings of the 19th International Middleware Conference. the 19th International Middleware ConferenceACMYitao Hu, Swati Rallapalli, Bongjun Ko, and Ramesh Govindan. Olympian: Sched- uling gpu usage in a deep neural network model serving system. In Proceedings of the 19th International Middleware Conference, pages 53-65. ACM, 2018.\n\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. Forrest N Iandola, Matthew W Moskewicz, Khalid Ashraf, Song Han, William J Dally, Kurt Keutzer, abs/1602.07360CoRRForrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016.\n\nQuincy: Fair scheduling for distributed computing clusters. Michael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, Andrew Goldberg, Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles, SOSP '09. the ACM SIGOPS 22Nd Symposium on Operating Systems Principles, SOSP '09New York, NY, USAACMMichael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, and Andrew Goldberg. Quincy: Fair scheduling for distributed computing clusters. In Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles, SOSP '09, pages 261-276, New York, NY, USA, 2009. ACM.\n\nParas Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E Gonzalez, Ion Stoica, arXiv:1901.10008The ooo vliw jit compiler for gpu inference. arXiv preprintParas Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E Gonzalez, and Ion Stoica. The ooo vliw jit compiler for gpu inference. arXiv preprint arXiv:1901.10008, 2019.\n\nIn-datacenter performance analysis of a tensor processing unit. P Norman, Cliff Jouppi, Nishant Young, David Patil, Gaurav Patterson, Raminder Agrawal, Sarah Bajwa, Suresh Bates, Nan Bhatia, Al Boden, Borchers, Proceedings of the 44th Annual International Symposium on Computer Architecture. the 44th Annual International Symposium on Computer ArchitectureNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture, pages 1-12, 2017.\n\nThe click modular router. Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti, M Frans Kaashoek, ACM Trans. Comput. Syst. 183Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti, and M. Frans Kaashoek. The click modular router. ACM Trans. Comput. Syst., 18(3):263-297, August 2000.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012.\n\nPRETZEL: Opening the black box 12 of machine learning prediction serving systems. Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, Matteo Interlandi, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationYunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambro- gio, Markus Weimer, and Matteo Interlandi. PRETZEL: Opening the black box 12 of machine learning prediction serving systems. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 611-626, Carlsbad, CA, October 2018. USENIX Association.\n\nSsd: Single shot multibox detector. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg, European conference on computer vision. SpringerWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21-37. Springer, 2016.\n\nCaesar: Cross-Camera Complex Activity Detection. X Liu, P Ghosh, O Ulutan, K Chan, B S Manjunath, R Govindan, Proc. ACM Sensys. ACM SensysX. Liu, P. Ghosh, O. Ulutan, K. Chan, B. S.Manjunath, and R. Govindan. Caesar: Cross-Camera Complex Activity Detection. In Proc. ACM Sensys, 2019.\n\nNeural Adaptive Video Streaming with Pensieve. Hongzi Mao, Ravi Netravali, Mohammad Alizadeh, Proceedings of the ACM Conference on Special Interest Group on Data Communication, SIGCOMM. the ACM Conference on Special Interest Group on Data Communication, SIGCOMMHongzi Mao, Ravi Netravali, and Mohammad Alizadeh. Neural Adaptive Video Streaming with Pensieve. In Proceedings of the ACM Conference on Special Interest Group on Data Communication, SIGCOMM, 2017.\n\npagan: Real-time avatars using dynamic textures. Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, Hao Li, 258:1-258:12ACM Trans. Graph. 376Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, and Hao Li. pagan: Real-time avatars using dynamic textures. ACM Trans. Graph., 37(6):258:1-258:12, December 2018.\n\nOptimus: an efficient dynamic resource scheduler for deep learning clusters. Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, Chuanxiong Guo, Proceedings of the Thirteenth EuroSys Conference. the Thirteenth EuroSys ConferenceACM3Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, and Chuanxiong Guo. Optimus: an efficient dynamic resource scheduler for deep learning clusters. In Proceedings of the Thirteenth EuroSys Conference, page 3. ACM, 2018.\n\nOdessa: Enabling interactive perception applications on mobile devices. Anmol Moo-Ryong Ra, Lily Sheth, Padmanabhan Mummert, David Pillai, Ramesh Wetherall, Govindan, Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services, MobiSys '11. the 9th International Conference on Mobile Systems, Applications, and Services, MobiSys '11New York, NY, USAACMMoo-Ryong Ra, Anmol Sheth, Lily Mummert, Padmanabhan Pillai, David Wether- all, and Ramesh Govindan. Odessa: Enabling interactive perception applications on mobile devices. In Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services, MobiSys '11, pages 43-56, New York, NY, USA, 2011. ACM.\n\nJoseph Redmon, Ali Farhadi, Yolov3: an incremental improvement. arXiv. Joseph Redmon and Ali Farhadi. Yolov3: an incremental improvement. arXiv, 2018.\n\nInfaas: Managed & model-less inference serving. Francisco Romero, Qian Li, J Neeraja, Christos Yadwadkar, Kozyrakis, arXiv:1905.13348arXiv preprintFrancisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. Infaas: Managed & model-less inference serving. arXiv preprint arXiv:1905.13348, 2019.\n\n. Vaishaal Shankar, Karl Krauth, Qifan Pu, Eric Jonas, Shivaram Venkataraman, Ion Stoica, Benjamin Recht, Jonathan Ragan-Kelley, abs/1810.09679Numpywren: Serverless linear algebra. CoRR. Vaishaal Shankar, Karl Krauth, Qifan Pu, Eric Jonas, Shivaram Venkataraman, Ion Stoica, Benjamin Recht, and Jonathan Ragan-Kelley. Numpywren: Serverless linear algebra. CoRR, abs/1810.09679, 2018.\n\nNexus: A gpu cluster engine for accelerating dnn-based video analysis. Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, Ravi Sundaram, Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP '19. the 27th ACM Symposium on Operating Systems Principles, SOSP '19New York, NY, USAAssociation for Computing MachineryHaichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. Nexus: A gpu cluster engine for accelerating dnn-based video analysis. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP '19, page 322-337, New York, NY, USA, 2019. Association for Computing Machinery.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nInception-v4, inception-resnet and the impact of residual connections on learning. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander A Alemi, Thirty-First AAAI Conference on Artificial Intelligence. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nActor conditioned attention maps for video action detection. Oytun Ulutan, Swati Rallapalli, Carlos Torres, Mudhakar Srivatsa, B S Manjunath, arXiv:1812.11631arXiv preprintOytun Ulutan, Swati Rallapalli, Carlos Torres, Mudhakar Srivatsa, and BS Manju- nath. Actor conditioned attention maps for video action detection. arXiv preprint arXiv:1812.11631, 2018.\n\nSequence to sequence -video to text. Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionSubhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence -video to text. In Proceedings of the IEEE international conference on computer vision, pages 4534-4542, 2015.\n\nAnatomy of a personalized livestreaming system. Bolun Wang, Xinyi Zhang, Gang Wang, Haitao Zheng, Ben Y Zhao, Proceedings of the 2016 Internet Measurement Conference, IMC '16. the 2016 Internet Measurement Conference, IMC '16New York, NY, USAACMBolun Wang, Xinyi Zhang, Gang Wang, Haitao Zheng, and Ben Y. Zhao. Anatomy of a personalized livestreaming system. In Proceedings of the 2016 Internet Measurement Conference, IMC '16, pages 485-498, New York, NY, USA, 2016. ACM.\n\nDeep Cosine Metric Learning for Person Reidentification. Nicolai Wojke, Alex Bewley, abs/1812.00442CoRRNicolai Wojke and Alex Bewley. Deep Cosine Metric Learning for Person Re- identification. CoRR, abs/1812.00442, 2018.\n\nGandiva: Introspective cluster scheduling for deep learning. Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, Lidong Zhou, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationWencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, and Lidong Zhou. Gandiva: Introspective cluster scheduling for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 595-610, Carlsbad, CA, October 2018. USENIX Association.\n\nHeteroedge: Taming the heterogeneity of edge computing system in social sensing. Daniel Zhang, Tahmid Rashid, Xukun Li, Nathan Vance, Dong Wang, Proceedings of the International Conference on Internet of Things Design and Implementation. the International Conference on Internet of Things Design and ImplementationDaniel Zhang, Tahmid Rashid, Xukun Li, Nathan Vance, and Dong Wang. Het- eroedge: Taming the heterogeneity of edge computing system in social sensing. In Proceedings of the International Conference on Internet of Things Design and Implementation, pages 37-48, 2019.\n", "annotations": {"author": "[{\"start\":\"41\",\"end\":\"65\"},{\"start\":\"66\",\"end\":\"94\"},{\"start\":\"95\",\"end\":\"108\"},{\"start\":\"109\",\"end\":\"139\"},{\"start\":\"140\",\"end\":\"173\"},{\"start\":\"174\",\"end\":\"207\"},{\"start\":\"208\",\"end\":\"239\"},{\"start\":\"240\",\"end\":\"309\"},{\"start\":\"310\",\"end\":\"345\"},{\"start\":\"346\",\"end\":\"381\"},{\"start\":\"382\",\"end\":\"443\"}]", "publisher": null, "author_last_name": "[{\"start\":\"47\",\"end\":\"49\"},{\"start\":\"72\",\"end\":\"76\"},{\"start\":\"104\",\"end\":\"107\"},{\"start\":\"116\",\"end\":\"121\"},{\"start\":\"148\",\"end\":\"150\"},{\"start\":\"182\",\"end\":\"185\"},{\"start\":\"215\",\"end\":\"223\"}]", "author_first_name": "[{\"start\":\"41\",\"end\":\"46\"},{\"start\":\"66\",\"end\":\"71\"},{\"start\":\"95\",\"end\":\"103\"},{\"start\":\"109\",\"end\":\"115\"},{\"start\":\"140\",\"end\":\"147\"},{\"start\":\"174\",\"end\":\"181\"},{\"start\":\"208\",\"end\":\"214\"}]", "author_affiliation": "[{\"start\":\"241\",\"end\":\"308\"},{\"start\":\"311\",\"end\":\"344\"},{\"start\":\"347\",\"end\":\"380\"},{\"start\":\"383\",\"end\":\"442\"}]", "title": "[{\"start\":\"1\",\"end\":\"38\"},{\"start\":\"444\",\"end\":\"481\"}]", "venue": null, "abstract": "[{\"start\":\"757\",\"end\":\"1606\"}]", "bib_ref": "[{\"start\":\"1923\",\"end\":\"1927\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"1927\",\"end\":\"1930\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"3170\",\"end\":\"3174\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"3271\",\"end\":\"3275\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"3275\",\"end\":\"3278\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"3834\",\"end\":\"3838\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"3838\",\"end\":\"3841\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"3841\",\"end\":\"3844\",\"attributes\":{\"ref_id\":\"b53\"}},{\"start\":\"4547\",\"end\":\"4550\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4602\",\"end\":\"4605\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"4798\",\"end\":\"4802\"},{\"start\":\"4871\",\"end\":\"4874\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"6726\",\"end\":\"6730\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"6730\",\"end\":\"6733\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"7075\",\"end\":\"7078\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"7078\",\"end\":\"7081\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"7081\",\"end\":\"7084\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"7173\",\"end\":\"7177\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"7190\",\"end\":\"7194\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"8204\",\"end\":\"8208\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"8208\",\"end\":\"8211\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"8211\",\"end\":\"8214\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"8450\",\"end\":\"8454\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"8464\",\"end\":\"8468\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"8532\",\"end\":\"8536\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"8554\",\"end\":\"8558\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"8583\",\"end\":\"8587\"},{\"start\":\"8886\",\"end\":\"8890\",\"attributes\":{\"ref_id\":\"b50\"}},{\"start\":\"9207\",\"end\":\"9211\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"11489\",\"end\":\"11493\"},{\"start\":\"11493\",\"end\":\"11496\"},{\"start\":\"11538\",\"end\":\"11542\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"11542\",\"end\":\"11545\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"11576\",\"end\":\"11580\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"11969\",\"end\":\"11973\"},{\"start\":\"12448\",\"end\":\"12451\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"12451\",\"end\":\"12453\"},{\"start\":\"12978\",\"end\":\"12982\"},{\"start\":\"14042\",\"end\":\"14046\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"14046\",\"end\":\"14049\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"14193\",\"end\":\"14197\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"14478\",\"end\":\"14482\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"14514\",\"end\":\"14518\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"15362\",\"end\":\"15366\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"15366\",\"end\":\"15369\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"15369\",\"end\":\"15372\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"16130\",\"end\":\"16134\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"16134\",\"end\":\"16137\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"16137\",\"end\":\"16140\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"16210\",\"end\":\"16214\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"16236\",\"end\":\"16240\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"16391\",\"end\":\"16395\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"16466\",\"end\":\"16470\",\"attributes\":{\"ref_id\":\"b52\"}},{\"start\":\"17034\",\"end\":\"17038\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"17070\",\"end\":\"17074\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"17104\",\"end\":\"17108\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"17705\",\"end\":\"17709\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"17709\",\"end\":\"17712\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"20047\",\"end\":\"20051\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"20418\",\"end\":\"20419\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"20452\",\"end\":\"20456\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"21724\",\"end\":\"21728\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"22677\",\"end\":\"22681\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"22681\",\"end\":\"22684\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"22764\",\"end\":\"22768\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"23036\",\"end\":\"23037\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"23150\",\"end\":\"23154\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"23172\",\"end\":\"23176\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"23742\",\"end\":\"23746\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"23746\",\"end\":\"23749\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"23857\",\"end\":\"23861\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"23895\",\"end\":\"23899\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"25286\",\"end\":\"25290\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"25290\",\"end\":\"25293\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"26171\",\"end\":\"26172\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"26772\",\"end\":\"26773\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"27357\",\"end\":\"27361\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"27361\",\"end\":\"27364\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"28471\",\"end\":\"28476\"},{\"start\":\"28479\",\"end\":\"28484\"},{\"start\":\"28517\",\"end\":\"28522\"},{\"start\":\"28551\",\"end\":\"28556\"},{\"start\":\"29003\",\"end\":\"29006\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"31371\",\"end\":\"31375\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"31375\",\"end\":\"31378\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"35114\",\"end\":\"35118\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"35212\",\"end\":\"35216\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"36159\",\"end\":\"36160\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"37313\",\"end\":\"37317\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"37330\",\"end\":\"37334\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"38264\",\"end\":\"38268\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"39849\",\"end\":\"39852\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"40122\",\"end\":\"40126\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"40138\",\"end\":\"40142\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"40150\",\"end\":\"40154\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"40181\",\"end\":\"40185\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"40570\",\"end\":\"40574\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"40632\",\"end\":\"40636\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"40650\",\"end\":\"40654\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"40757\",\"end\":\"40760\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"40868\",\"end\":\"40871\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"40942\",\"end\":\"40946\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"40991\",\"end\":\"40995\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"41116\",\"end\":\"41120\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"41132\",\"end\":\"41136\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"46775\",\"end\":\"46779\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"47072\",\"end\":\"47076\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"48301\",\"end\":\"48305\",\"attributes\":{\"ref_id\":\"b47\"}},{\"start\":\"49085\",\"end\":\"49086\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"50429\",\"end\":\"50431\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"50458\",\"end\":\"50459\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"51299\",\"end\":\"51301\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"51384\",\"end\":\"51386\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"56503\",\"end\":\"56507\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"58382\",\"end\":\"58385\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"58385\",\"end\":\"58388\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"58476\",\"end\":\"58480\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"58945\",\"end\":\"58949\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"58962\",\"end\":\"58966\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"59243\",\"end\":\"59247\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"59599\",\"end\":\"59603\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"59874\",\"end\":\"59878\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"60526\",\"end\":\"60530\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"60892\",\"end\":\"60896\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"60967\",\"end\":\"60971\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"61087\",\"end\":\"61091\",\"attributes\":{\"ref_id\":\"b52\"}},{\"start\":\"61144\",\"end\":\"61148\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"61368\",\"end\":\"61372\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"61713\",\"end\":\"61717\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"61822\",\"end\":\"61824\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"61863\",\"end\":\"61867\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"61867\",\"end\":\"61870\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"62051\",\"end\":\"62055\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"65519\",\"end\":\"65520\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"65691\",\"end\":\"65692\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"65749\",\"end\":\"65753\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"65763\",\"end\":\"65767\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"65910\",\"end\":\"65914\",\"attributes\":{\"ref_id\":\"b28\"}}]", "figure": "[{\"start\":\"63227\",\"end\":\"63397\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"63398\",\"end\":\"63497\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"63498\",\"end\":\"63524\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"63525\",\"end\":\"63726\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"63727\",\"end\":\"63782\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"63783\",\"end\":\"63956\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"63957\",\"end\":\"64071\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"64072\",\"end\":\"64224\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"64225\",\"end\":\"64350\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"64351\",\"end\":\"64491\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"64492\",\"end\":\"65309\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"65310\",\"end\":\"65377\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}},{\"start\":\"65378\",\"end\":\"65421\",\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1622\",\"end\":\"3078\"},{\"start\":\"3080\",\"end\":\"3430\"},{\"start\":\"3432\",\"end\":\"4420\"},{\"start\":\"4422\",\"end\":\"4875\"},{\"start\":\"4877\",\"end\":\"5242\"},{\"start\":\"5244\",\"end\":\"5700\"},{\"start\":\"5702\",\"end\":\"6471\"},{\"start\":\"6473\",\"end\":\"6802\"},{\"start\":\"6804\",\"end\":\"7576\"},{\"start\":\"7606\",\"end\":\"8309\"},{\"start\":\"8311\",\"end\":\"9169\"},{\"start\":\"9171\",\"end\":\"10714\"},{\"start\":\"10716\",\"end\":\"11339\"},{\"start\":\"11341\",\"end\":\"11974\"},{\"start\":\"11976\",\"end\":\"12681\"},{\"start\":\"12683\",\"end\":\"13882\"},{\"start\":\"13884\",\"end\":\"14749\"},{\"start\":\"14751\",\"end\":\"15025\"},{\"start\":\"15027\",\"end\":\"15676\"},{\"start\":\"15678\",\"end\":\"15994\"},{\"start\":\"15996\",\"end\":\"16513\"},{\"start\":\"16528\",\"end\":\"16632\"},{\"start\":\"16652\",\"end\":\"17109\"},{\"start\":\"17111\",\"end\":\"18914\"},{\"start\":\"18937\",\"end\":\"20873\"},{\"start\":\"20875\",\"end\":\"21673\"},{\"start\":\"21675\",\"end\":\"23302\"},{\"start\":\"23304\",\"end\":\"23750\"},{\"start\":\"23752\",\"end\":\"23900\"},{\"start\":\"23937\",\"end\":\"24492\"},{\"start\":\"24494\",\"end\":\"25143\"},{\"start\":\"25145\",\"end\":\"26174\"},{\"start\":\"26176\",\"end\":\"26971\"},{\"start\":\"26973\",\"end\":\"28222\"},{\"start\":\"28224\",\"end\":\"29793\"},{\"start\":\"29795\",\"end\":\"30262\"},{\"start\":\"30264\",\"end\":\"30999\"},{\"start\":\"31001\",\"end\":\"31380\"},{\"start\":\"31382\",\"end\":\"32529\"},{\"start\":\"32531\",\"end\":\"33343\"},{\"start\":\"33345\",\"end\":\"34177\"},{\"start\":\"34195\",\"end\":\"37228\"},{\"start\":\"37247\",\"end\":\"37421\"},{\"start\":\"37437\",\"end\":\"38820\"},{\"start\":\"38822\",\"end\":\"41194\"},{\"start\":\"41196\",\"end\":\"41692\"},{\"start\":\"41707\",\"end\":\"42342\"},{\"start\":\"42344\",\"end\":\"43004\"},{\"start\":\"43011\",\"end\":\"43574\"},{\"start\":\"43576\",\"end\":\"44281\"},{\"start\":\"44283\",\"end\":\"44912\"},{\"start\":\"44914\",\"end\":\"45816\"},{\"start\":\"45818\",\"end\":\"46725\"},{\"start\":\"46727\",\"end\":\"48488\"},{\"start\":\"48507\",\"end\":\"48751\"},{\"start\":\"48753\",\"end\":\"49216\"},{\"start\":\"49231\",\"end\":\"49374\"},{\"start\":\"49376\",\"end\":\"49770\"},{\"start\":\"49772\",\"end\":\"50958\"},{\"start\":\"50960\",\"end\":\"51587\"},{\"start\":\"51589\",\"end\":\"51742\"},{\"start\":\"51744\",\"end\":\"52165\"},{\"start\":\"52167\",\"end\":\"53565\"},{\"start\":\"53567\",\"end\":\"55003\"},{\"start\":\"55005\",\"end\":\"55711\"},{\"start\":\"55713\",\"end\":\"56078\"},{\"start\":\"56097\",\"end\":\"58541\"},{\"start\":\"58558\",\"end\":\"59590\"},{\"start\":\"59592\",\"end\":\"60211\"},{\"start\":\"60213\",\"end\":\"62208\"},{\"start\":\"62224\",\"end\":\"63226\"}]", "formula": null, "table_ref": "[{\"start\":\"43014\",\"end\":\"43113\",\"attributes\":{\"ref_id\":\"tab_3\"}}]", "section_header": "[{\"start\":\"1608\",\"end\":\"1620\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"7579\",\"end\":\"7604\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"16516\",\"end\":\"16526\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"16635\",\"end\":\"16650\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"18917\",\"end\":\"18935\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"23903\",\"end\":\"23935\",\"attributes\":{\"n\":\"3.3\"}},{\"start\":\"34180\",\"end\":\"34193\",\"attributes\":{\"n\":\"3.4\"}},{\"start\":\"37231\",\"end\":\"37245\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"37424\",\"end\":\"37435\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"41695\",\"end\":\"41705\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"43007\",\"end\":\"43009\"},{\"start\":\"48491\",\"end\":\"48505\",\"attributes\":{\"n\":\"4.3\"}},{\"start\":\"49219\",\"end\":\"49229\"},{\"start\":\"56081\",\"end\":\"56095\",\"attributes\":{\"n\":\"4.4\"}},{\"start\":\"58544\",\"end\":\"58556\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"62211\",\"end\":\"62222\",\"attributes\":{\"n\":\"6\"}},{\"start\":\"63399\",\"end\":\"63409\"},{\"start\":\"63499\",\"end\":\"63509\"},{\"start\":\"63526\",\"end\":\"63536\"},{\"start\":\"63728\",\"end\":\"63738\"},{\"start\":\"63784\",\"end\":\"63794\"},{\"start\":\"63958\",\"end\":\"63968\"},{\"start\":\"64073\",\"end\":\"64083\"},{\"start\":\"64226\",\"end\":\"64246\"},{\"start\":\"64352\",\"end\":\"64363\"},{\"start\":\"64493\",\"end\":\"64502\"},{\"start\":\"65311\",\"end\":\"65320\"},{\"start\":\"65379\",\"end\":\"65388\"}]", "table": "[{\"start\":\"64746\",\"end\":\"65309\"},{\"start\":\"65371\",\"end\":\"65377\"}]", "figure_caption": "[{\"start\":\"63229\",\"end\":\"63397\"},{\"start\":\"63411\",\"end\":\"63497\"},{\"start\":\"63511\",\"end\":\"63524\"},{\"start\":\"63538\",\"end\":\"63726\"},{\"start\":\"63740\",\"end\":\"63782\"},{\"start\":\"63796\",\"end\":\"63956\"},{\"start\":\"63970\",\"end\":\"64071\"},{\"start\":\"64085\",\"end\":\"64224\"},{\"start\":\"64249\",\"end\":\"64350\"},{\"start\":\"64366\",\"end\":\"64491\"},{\"start\":\"64504\",\"end\":\"64746\"},{\"start\":\"65322\",\"end\":\"65371\"},{\"start\":\"65390\",\"end\":\"65421\"}]", "figure_ref": "[{\"start\":\"9160\",\"end\":\"9168\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"10215\",\"end\":\"10221\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"10573\",\"end\":\"10579\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"11155\",\"end\":\"11161\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"12250\",\"end\":\"12256\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"14051\",\"end\":\"14057\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"16791\",\"end\":\"16799\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"17171\",\"end\":\"17179\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"20223\",\"end\":\"20230\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"21013\",\"end\":\"21021\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"22084\",\"end\":\"22092\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"23084\",\"end\":\"23090\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"23462\",\"end\":\"23468\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"26361\",\"end\":\"26367\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"26634\",\"end\":\"26640\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"43321\",\"end\":\"43328\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"43487\",\"end\":\"43493\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"45431\",\"end\":\"45438\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"46418\",\"end\":\"46425\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"47646\",\"end\":\"47652\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"47957\",\"end\":\"47965\"},{\"start\":\"49398\",\"end\":\"49404\"},{\"start\":\"50304\",\"end\":\"50326\"},{\"start\":\"51689\",\"end\":\"51705\"},{\"start\":\"53558\",\"end\":\"53564\"},{\"start\":\"53576\",\"end\":\"53584\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"55722\",\"end\":\"55730\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"56050\",\"end\":\"56060\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"56416\",\"end\":\"56424\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"56571\",\"end\":\"56577\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"57901\",\"end\":\"57909\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"57990\",\"end\":\"57998\"}]", "bib_author_first_name": "[{\"start\":\"67367\",\"end\":\"67373\"},{\"start\":\"67557\",\"end\":\"67562\"},{\"start\":\"67570\",\"end\":\"67571\"},{\"start\":\"67580\",\"end\":\"67585\"},{\"start\":\"67598\",\"end\":\"67599\"},{\"start\":\"67600\",\"end\":\"67601\"},{\"start\":\"67922\",\"end\":\"67928\"},{\"start\":\"67937\",\"end\":\"67940\"},{\"start\":\"67941\",\"end\":\"67942\"},{\"start\":\"67948\",\"end\":\"67954\"},{\"start\":\"67965\",\"end\":\"67971\"},{\"start\":\"67977\",\"end\":\"67984\"},{\"start\":\"67991\",\"end\":\"67996\"},{\"start\":\"68011\",\"end\":\"68016\"},{\"start\":\"68017\",\"end\":\"68018\"},{\"start\":\"68028\",\"end\":\"68033\"},{\"start\":\"68040\",\"end\":\"68043\"},{\"start\":\"68611\",\"end\":\"68616\"},{\"start\":\"68625\",\"end\":\"68632\"},{\"start\":\"68660\",\"end\":\"68669\"},{\"start\":\"68679\",\"end\":\"68683\"},{\"start\":\"68689\",\"end\":\"68693\"},{\"start\":\"68706\",\"end\":\"68711\"},{\"start\":\"68718\",\"end\":\"68723\"},{\"start\":\"68732\",\"end\":\"68737\"},{\"start\":\"68749\",\"end\":\"68757\"},{\"start\":\"69204\",\"end\":\"69205\"},{\"start\":\"69211\",\"end\":\"69212\"},{\"start\":\"69222\",\"end\":\"69223\"},{\"start\":\"69234\",\"end\":\"69235\"},{\"start\":\"69243\",\"end\":\"69244\"},{\"start\":\"69245\",\"end\":\"69246\"},{\"start\":\"69614\",\"end\":\"69616\"},{\"start\":\"69624\",\"end\":\"69627\"},{\"start\":\"69634\",\"end\":\"69637\"},{\"start\":\"69644\",\"end\":\"69647\"},{\"start\":\"69886\",\"end\":\"69891\"},{\"start\":\"69903\",\"end\":\"69912\"},{\"start\":\"69922\",\"end\":\"69929\"},{\"start\":\"70218\",\"end\":\"70226\"},{\"start\":\"70238\",\"end\":\"70242\"},{\"start\":\"70248\",\"end\":\"70253\"},{\"start\":\"70254\",\"end\":\"70256\"},{\"start\":\"70263\",\"end\":\"70270\"},{\"start\":\"70725\",\"end\":\"70731\"},{\"start\":\"70753\",\"end\":\"70758\"},{\"start\":\"70765\",\"end\":\"70772\"},{\"start\":\"70780\",\"end\":\"70786\"},{\"start\":\"70787\",\"end\":\"70788\"},{\"start\":\"70793\",\"end\":\"70796\"},{\"start\":\"70807\",\"end\":\"70813\"},{\"start\":\"71123\",\"end\":\"71129\"},{\"start\":\"71141\",\"end\":\"71144\"},{\"start\":\"71151\",\"end\":\"71157\"},{\"start\":\"71164\",\"end\":\"71171\"},{\"start\":\"71172\",\"end\":\"71173\"},{\"start\":\"71184\",\"end\":\"71190\"},{\"start\":\"71191\",\"end\":\"71192\"},{\"start\":\"71718\",\"end\":\"71726\"},{\"start\":\"71732\",\"end\":\"71735\"},{\"start\":\"71743\",\"end\":\"71744\"},{\"start\":\"71762\",\"end\":\"71763\"},{\"start\":\"71773\",\"end\":\"71777\"},{\"start\":\"71778\",\"end\":\"71779\"},{\"start\":\"72182\",\"end\":\"72189\"},{\"start\":\"72196\",\"end\":\"72202\"},{\"start\":\"72603\",\"end\":\"72606\"},{\"start\":\"72613\",\"end\":\"72616\"},{\"start\":\"72621\",\"end\":\"72627\"},{\"start\":\"72634\",\"end\":\"72641\"},{\"start\":\"72648\",\"end\":\"72650\"},{\"start\":\"72873\",\"end\":\"72879\"},{\"start\":\"72890\",\"end\":\"72896\"},{\"start\":\"72906\",\"end\":\"72914\"},{\"start\":\"73367\",\"end\":\"73371\"},{\"start\":\"73589\",\"end\":\"73595\"},{\"start\":\"73604\",\"end\":\"73610\"},{\"start\":\"73622\",\"end\":\"73629\"},{\"start\":\"73634\",\"end\":\"73638\"},{\"start\":\"73644\",\"end\":\"73646\"},{\"start\":\"74017\",\"end\":\"74023\"},{\"start\":\"74024\",\"end\":\"74025\"},{\"start\":\"74034\",\"end\":\"74042\"},{\"start\":\"74048\",\"end\":\"74050\"},{\"start\":\"74057\",\"end\":\"74063\"},{\"start\":\"74078\",\"end\":\"74084\"},{\"start\":\"74091\",\"end\":\"74097\"},{\"start\":\"74106\",\"end\":\"74111\"},{\"start\":\"74123\",\"end\":\"74130\"},{\"start\":\"74463\",\"end\":\"74468\"},{\"start\":\"74476\",\"end\":\"74482\"},{\"start\":\"74501\",\"end\":\"74506\"},{\"start\":\"74514\",\"end\":\"74522\"},{\"start\":\"74537\",\"end\":\"74545\"},{\"start\":\"74552\",\"end\":\"74559\"},{\"start\":\"74571\",\"end\":\"74578\"},{\"start\":\"74579\",\"end\":\"74580\"},{\"start\":\"74590\",\"end\":\"74594\"},{\"start\":\"75156\",\"end\":\"75160\"},{\"start\":\"75165\",\"end\":\"75172\"},{\"start\":\"75616\",\"end\":\"75621\"},{\"start\":\"75626\",\"end\":\"75634\"},{\"start\":\"75642\",\"end\":\"75648\"},{\"start\":\"75654\",\"end\":\"75658\"},{\"start\":\"75667\",\"end\":\"75673\"},{\"start\":\"75679\",\"end\":\"75683\"},{\"start\":\"75693\",\"end\":\"75697\"},{\"start\":\"75707\",\"end\":\"75713\"},{\"start\":\"75719\",\"end\":\"75727\"},{\"start\":\"75734\",\"end\":\"75737\"},{\"start\":\"76103\",\"end\":\"76108\"},{\"start\":\"76113\",\"end\":\"76118\"},{\"start\":\"76131\",\"end\":\"76138\"},{\"start\":\"76143\",\"end\":\"76149\"},{\"start\":\"76580\",\"end\":\"76587\"},{\"start\":\"76588\",\"end\":\"76589\"},{\"start\":\"76599\",\"end\":\"76606\"},{\"start\":\"76607\",\"end\":\"76608\"},{\"start\":\"76620\",\"end\":\"76626\"},{\"start\":\"76635\",\"end\":\"76639\"},{\"start\":\"76645\",\"end\":\"76652\"},{\"start\":\"76653\",\"end\":\"76654\"},{\"start\":\"76662\",\"end\":\"76666\"},{\"start\":\"76968\",\"end\":\"76975\"},{\"start\":\"76983\",\"end\":\"76990\"},{\"start\":\"77004\",\"end\":\"77007\"},{\"start\":\"77016\",\"end\":\"77019\"},{\"start\":\"77028\",\"end\":\"77033\"},{\"start\":\"77042\",\"end\":\"77048\"},{\"start\":\"77530\",\"end\":\"77535\"},{\"start\":\"77542\",\"end\":\"77549\"},{\"start\":\"77554\",\"end\":\"77558\"},{\"start\":\"77565\",\"end\":\"77571\"},{\"start\":\"77581\",\"end\":\"77587\"},{\"start\":\"77588\",\"end\":\"77589\"},{\"start\":\"77600\",\"end\":\"77603\"},{\"start\":\"77922\",\"end\":\"77923\"},{\"start\":\"77932\",\"end\":\"77937\"},{\"start\":\"77946\",\"end\":\"77953\"},{\"start\":\"77961\",\"end\":\"77966\"},{\"start\":\"77974\",\"end\":\"77980\"},{\"start\":\"77992\",\"end\":\"78000\"},{\"start\":\"78010\",\"end\":\"78015\"},{\"start\":\"78023\",\"end\":\"78029\"},{\"start\":\"78037\",\"end\":\"78040\"},{\"start\":\"78049\",\"end\":\"78051\"},{\"start\":\"78560\",\"end\":\"78565\"},{\"start\":\"78574\",\"end\":\"78580\"},{\"start\":\"78589\",\"end\":\"78595\"},{\"start\":\"78602\",\"end\":\"78606\"},{\"start\":\"78617\",\"end\":\"78618\"},{\"start\":\"78889\",\"end\":\"78893\"},{\"start\":\"78906\",\"end\":\"78910\"},{\"start\":\"78922\",\"end\":\"78930\"},{\"start\":\"78931\",\"end\":\"78932\"},{\"start\":\"79273\",\"end\":\"79281\"},{\"start\":\"79287\",\"end\":\"79294\"},{\"start\":\"79304\",\"end\":\"79313\"},{\"start\":\"79320\",\"end\":\"79325\"},{\"start\":\"79326\",\"end\":\"79334\"},{\"start\":\"79349\",\"end\":\"79355\"},{\"start\":\"79364\",\"end\":\"79370\"},{\"start\":\"79874\",\"end\":\"79877\"},{\"start\":\"79883\",\"end\":\"79891\"},{\"start\":\"79902\",\"end\":\"79909\"},{\"start\":\"79917\",\"end\":\"79926\"},{\"start\":\"79936\",\"end\":\"79941\"},{\"start\":\"79948\",\"end\":\"79958\"},{\"start\":\"79963\",\"end\":\"79974\"},{\"start\":\"80298\",\"end\":\"80299\"},{\"start\":\"80305\",\"end\":\"80306\"},{\"start\":\"80314\",\"end\":\"80315\"},{\"start\":\"80324\",\"end\":\"80325\"},{\"start\":\"80332\",\"end\":\"80333\"},{\"start\":\"80334\",\"end\":\"80335\"},{\"start\":\"80347\",\"end\":\"80348\"},{\"start\":\"80582\",\"end\":\"80588\"},{\"start\":\"80594\",\"end\":\"80598\"},{\"start\":\"80610\",\"end\":\"80618\"},{\"start\":\"81045\",\"end\":\"81049\"},{\"start\":\"81058\",\"end\":\"81064\"},{\"start\":\"81070\",\"end\":\"81073\"},{\"start\":\"81080\",\"end\":\"81086\"},{\"start\":\"81092\",\"end\":\"81096\"},{\"start\":\"81101\",\"end\":\"81109\"},{\"start\":\"81117\",\"end\":\"81123\"},{\"start\":\"81133\",\"end\":\"81137\"},{\"start\":\"81147\",\"end\":\"81150\"},{\"start\":\"81483\",\"end\":\"81490\"},{\"start\":\"81497\",\"end\":\"81502\"},{\"start\":\"81508\",\"end\":\"81515\"},{\"start\":\"81522\",\"end\":\"81527\"},{\"start\":\"81532\",\"end\":\"81542\"},{\"start\":\"81926\",\"end\":\"81931\"},{\"start\":\"81946\",\"end\":\"81950\"},{\"start\":\"81958\",\"end\":\"81969\"},{\"start\":\"81979\",\"end\":\"81984\"},{\"start\":\"81993\",\"end\":\"81999\"},{\"start\":\"82568\",\"end\":\"82574\"},{\"start\":\"82583\",\"end\":\"82586\"},{\"start\":\"82768\",\"end\":\"82777\"},{\"start\":\"82786\",\"end\":\"82790\"},{\"start\":\"82795\",\"end\":\"82796\"},{\"start\":\"82806\",\"end\":\"82814\"},{\"start\":\"83029\",\"end\":\"83037\"},{\"start\":\"83047\",\"end\":\"83051\"},{\"start\":\"83060\",\"end\":\"83065\"},{\"start\":\"83070\",\"end\":\"83074\"},{\"start\":\"83082\",\"end\":\"83090\"},{\"start\":\"83105\",\"end\":\"83108\"},{\"start\":\"83117\",\"end\":\"83125\"},{\"start\":\"83133\",\"end\":\"83141\"},{\"start\":\"83483\",\"end\":\"83490\"},{\"start\":\"83497\",\"end\":\"83502\"},{\"start\":\"83509\",\"end\":\"83515\"},{\"start\":\"83521\",\"end\":\"83528\"},{\"start\":\"83535\",\"end\":\"83541\"},{\"start\":\"83548\",\"end\":\"83555\"},{\"start\":\"83567\",\"end\":\"83573\"},{\"start\":\"83589\",\"end\":\"83593\"},{\"start\":\"84226\",\"end\":\"84231\"},{\"start\":\"84242\",\"end\":\"84248\"},{\"start\":\"84516\",\"end\":\"84525\"},{\"start\":\"84535\",\"end\":\"84541\"},{\"start\":\"84549\",\"end\":\"84556\"},{\"start\":\"84568\",\"end\":\"84577\"},{\"start\":\"84578\",\"end\":\"84579\"},{\"start\":\"84930\",\"end\":\"84935\"},{\"start\":\"84944\",\"end\":\"84949\"},{\"start\":\"84962\",\"end\":\"84968\"},{\"start\":\"84977\",\"end\":\"84985\"},{\"start\":\"84996\",\"end\":\"84997\"},{\"start\":\"84998\",\"end\":\"84999\"},{\"start\":\"85265\",\"end\":\"85275\"},{\"start\":\"85289\",\"end\":\"85295\"},{\"start\":\"85306\",\"end\":\"85313\"},{\"start\":\"85323\",\"end\":\"85330\"},{\"start\":\"85339\",\"end\":\"85345\"},{\"start\":\"85355\",\"end\":\"85359\"},{\"start\":\"85777\",\"end\":\"85782\"},{\"start\":\"85789\",\"end\":\"85794\"},{\"start\":\"85802\",\"end\":\"85806\"},{\"start\":\"85813\",\"end\":\"85819\"},{\"start\":\"85827\",\"end\":\"85830\"},{\"start\":\"85831\",\"end\":\"85832\"},{\"start\":\"86261\",\"end\":\"86268\"},{\"start\":\"86276\",\"end\":\"86280\"},{\"start\":\"86487\",\"end\":\"86494\"},{\"start\":\"86501\",\"end\":\"86506\"},{\"start\":\"86517\",\"end\":\"86529\"},{\"start\":\"86538\",\"end\":\"86545\"},{\"start\":\"86557\",\"end\":\"86562\"},{\"start\":\"86571\",\"end\":\"86578\"},{\"start\":\"86584\",\"end\":\"86592\"},{\"start\":\"86600\",\"end\":\"86604\"},{\"start\":\"86611\",\"end\":\"86616\"},{\"start\":\"86623\",\"end\":\"86629\"},{\"start\":\"86637\",\"end\":\"86640\"},{\"start\":\"86647\",\"end\":\"86653\"},{\"start\":\"87236\",\"end\":\"87242\"},{\"start\":\"87250\",\"end\":\"87256\"},{\"start\":\"87265\",\"end\":\"87270\"},{\"start\":\"87275\",\"end\":\"87281\"},{\"start\":\"87289\",\"end\":\"87293\"}]", "bib_author_last_name": "[{\"start\":\"66287\",\"end\":\"66292\"},{\"start\":\"67052\",\"end\":\"67086\"},{\"start\":\"67374\",\"end\":\"67377\"},{\"start\":\"67379\",\"end\":\"67382\"},{\"start\":\"67563\",\"end\":\"67568\"},{\"start\":\"67572\",\"end\":\"67578\"},{\"start\":\"67586\",\"end\":\"67596\"},{\"start\":\"67602\",\"end\":\"67613\"},{\"start\":\"67615\",\"end\":\"67625\"},{\"start\":\"67929\",\"end\":\"67935\"},{\"start\":\"67943\",\"end\":\"67946\"},{\"start\":\"67955\",\"end\":\"67963\"},{\"start\":\"67972\",\"end\":\"67975\"},{\"start\":\"67985\",\"end\":\"67989\"},{\"start\":\"67997\",\"end\":\"68009\"},{\"start\":\"68019\",\"end\":\"68026\"},{\"start\":\"68034\",\"end\":\"68038\"},{\"start\":\"68044\",\"end\":\"68049\"},{\"start\":\"68617\",\"end\":\"68623\"},{\"start\":\"68633\",\"end\":\"68658\"},{\"start\":\"68670\",\"end\":\"68677\"},{\"start\":\"68684\",\"end\":\"68687\"},{\"start\":\"68694\",\"end\":\"68704\"},{\"start\":\"68712\",\"end\":\"68716\"},{\"start\":\"68724\",\"end\":\"68730\"},{\"start\":\"68738\",\"end\":\"68747\"},{\"start\":\"68758\",\"end\":\"68763\"},{\"start\":\"68765\",\"end\":\"68769\"},{\"start\":\"69206\",\"end\":\"69209\"},{\"start\":\"69213\",\"end\":\"69220\"},{\"start\":\"69224\",\"end\":\"69232\"},{\"start\":\"69236\",\"end\":\"69241\"},{\"start\":\"69247\",\"end\":\"69250\"},{\"start\":\"69252\",\"end\":\"69258\"},{\"start\":\"69617\",\"end\":\"69622\"},{\"start\":\"69628\",\"end\":\"69632\"},{\"start\":\"69638\",\"end\":\"69642\"},{\"start\":\"69648\",\"end\":\"69653\"},{\"start\":\"69892\",\"end\":\"69901\"},{\"start\":\"69913\",\"end\":\"69920\"},{\"start\":\"69930\",\"end\":\"69938\"},{\"start\":\"70227\",\"end\":\"70236\"},{\"start\":\"70243\",\"end\":\"70246\"},{\"start\":\"70257\",\"end\":\"70261\"},{\"start\":\"70271\",\"end\":\"70278\"},{\"start\":\"70732\",\"end\":\"70741\"},{\"start\":\"70743\",\"end\":\"70751\"},{\"start\":\"70759\",\"end\":\"70763\"},{\"start\":\"70773\",\"end\":\"70778\"},{\"start\":\"70789\",\"end\":\"70791\"},{\"start\":\"70797\",\"end\":\"70805\"},{\"start\":\"70814\",\"end\":\"70820\"},{\"start\":\"70822\",\"end\":\"70829\"},{\"start\":\"71130\",\"end\":\"71139\"},{\"start\":\"71145\",\"end\":\"71149\"},{\"start\":\"71158\",\"end\":\"71162\"},{\"start\":\"71174\",\"end\":\"71182\"},{\"start\":\"71727\",\"end\":\"71730\"},{\"start\":\"71736\",\"end\":\"71741\"},{\"start\":\"71745\",\"end\":\"71752\"},{\"start\":\"71754\",\"end\":\"71760\"},{\"start\":\"71764\",\"end\":\"71771\"},{\"start\":\"71780\",\"end\":\"71787\"},{\"start\":\"71789\",\"end\":\"71793\"},{\"start\":\"72190\",\"end\":\"72194\"},{\"start\":\"72203\",\"end\":\"72211\"},{\"start\":\"72607\",\"end\":\"72611\"},{\"start\":\"72617\",\"end\":\"72619\"},{\"start\":\"72628\",\"end\":\"72632\"},{\"start\":\"72642\",\"end\":\"72646\"},{\"start\":\"72651\",\"end\":\"72655\"},{\"start\":\"72880\",\"end\":\"72888\"},{\"start\":\"72897\",\"end\":\"72904\"},{\"start\":\"72915\",\"end\":\"72920\"},{\"start\":\"73372\",\"end\":\"73378\"},{\"start\":\"73596\",\"end\":\"73602\"},{\"start\":\"73611\",\"end\":\"73620\"},{\"start\":\"73630\",\"end\":\"73632\"},{\"start\":\"73639\",\"end\":\"73642\"},{\"start\":\"73647\",\"end\":\"73649\"},{\"start\":\"74026\",\"end\":\"74032\"},{\"start\":\"74043\",\"end\":\"74046\"},{\"start\":\"74051\",\"end\":\"74055\"},{\"start\":\"74064\",\"end\":\"74076\"},{\"start\":\"74085\",\"end\":\"74089\"},{\"start\":\"74098\",\"end\":\"74104\"},{\"start\":\"74112\",\"end\":\"74121\"},{\"start\":\"74131\",\"end\":\"74135\"},{\"start\":\"74469\",\"end\":\"74474\"},{\"start\":\"74483\",\"end\":\"74499\"},{\"start\":\"74507\",\"end\":\"74512\"},{\"start\":\"74523\",\"end\":\"74535\"},{\"start\":\"74546\",\"end\":\"74550\"},{\"start\":\"74560\",\"end\":\"74569\"},{\"start\":\"74581\",\"end\":\"74588\"},{\"start\":\"74595\",\"end\":\"74600\"},{\"start\":\"75161\",\"end\":\"75163\"},{\"start\":\"75173\",\"end\":\"75187\"},{\"start\":\"75622\",\"end\":\"75624\"},{\"start\":\"75635\",\"end\":\"75640\"},{\"start\":\"75649\",\"end\":\"75652\"},{\"start\":\"75659\",\"end\":\"75665\"},{\"start\":\"75674\",\"end\":\"75677\"},{\"start\":\"75684\",\"end\":\"75691\"},{\"start\":\"75698\",\"end\":\"75705\"},{\"start\":\"75714\",\"end\":\"75717\"},{\"start\":\"75728\",\"end\":\"75732\"},{\"start\":\"75738\",\"end\":\"75740\"},{\"start\":\"76109\",\"end\":\"76111\"},{\"start\":\"76119\",\"end\":\"76129\"},{\"start\":\"76139\",\"end\":\"76141\"},{\"start\":\"76150\",\"end\":\"76158\"},{\"start\":\"76590\",\"end\":\"76597\"},{\"start\":\"76609\",\"end\":\"76618\"},{\"start\":\"76627\",\"end\":\"76633\"},{\"start\":\"76640\",\"end\":\"76643\"},{\"start\":\"76655\",\"end\":\"76660\"},{\"start\":\"76667\",\"end\":\"76674\"},{\"start\":\"76976\",\"end\":\"76981\"},{\"start\":\"76991\",\"end\":\"77002\"},{\"start\":\"77008\",\"end\":\"77014\"},{\"start\":\"77020\",\"end\":\"77026\"},{\"start\":\"77034\",\"end\":\"77040\"},{\"start\":\"77049\",\"end\":\"77057\"},{\"start\":\"77536\",\"end\":\"77540\"},{\"start\":\"77550\",\"end\":\"77552\"},{\"start\":\"77559\",\"end\":\"77563\"},{\"start\":\"77572\",\"end\":\"77579\"},{\"start\":\"77590\",\"end\":\"77598\"},{\"start\":\"77604\",\"end\":\"77610\"},{\"start\":\"77924\",\"end\":\"77930\"},{\"start\":\"77938\",\"end\":\"77944\"},{\"start\":\"77954\",\"end\":\"77959\"},{\"start\":\"77967\",\"end\":\"77972\"},{\"start\":\"77981\",\"end\":\"77990\"},{\"start\":\"78001\",\"end\":\"78008\"},{\"start\":\"78016\",\"end\":\"78021\"},{\"start\":\"78030\",\"end\":\"78035\"},{\"start\":\"78041\",\"end\":\"78047\"},{\"start\":\"78052\",\"end\":\"78057\"},{\"start\":\"78059\",\"end\":\"78067\"},{\"start\":\"78566\",\"end\":\"78572\"},{\"start\":\"78581\",\"end\":\"78587\"},{\"start\":\"78596\",\"end\":\"78600\"},{\"start\":\"78607\",\"end\":\"78615\"},{\"start\":\"78619\",\"end\":\"78633\"},{\"start\":\"78894\",\"end\":\"78904\"},{\"start\":\"78911\",\"end\":\"78920\"},{\"start\":\"78933\",\"end\":\"78939\"},{\"start\":\"79282\",\"end\":\"79285\"},{\"start\":\"79295\",\"end\":\"79302\"},{\"start\":\"79314\",\"end\":\"79318\"},{\"start\":\"79335\",\"end\":\"79347\"},{\"start\":\"79356\",\"end\":\"79362\"},{\"start\":\"79371\",\"end\":\"79381\"},{\"start\":\"79878\",\"end\":\"79881\"},{\"start\":\"79892\",\"end\":\"79900\"},{\"start\":\"79910\",\"end\":\"79915\"},{\"start\":\"79927\",\"end\":\"79934\"},{\"start\":\"79942\",\"end\":\"79946\"},{\"start\":\"79959\",\"end\":\"79961\"},{\"start\":\"79975\",\"end\":\"79979\"},{\"start\":\"80300\",\"end\":\"80303\"},{\"start\":\"80307\",\"end\":\"80312\"},{\"start\":\"80316\",\"end\":\"80322\"},{\"start\":\"80326\",\"end\":\"80330\"},{\"start\":\"80336\",\"end\":\"80345\"},{\"start\":\"80349\",\"end\":\"80357\"},{\"start\":\"80589\",\"end\":\"80592\"},{\"start\":\"80599\",\"end\":\"80608\"},{\"start\":\"80619\",\"end\":\"80627\"},{\"start\":\"81050\",\"end\":\"81056\"},{\"start\":\"81065\",\"end\":\"81068\"},{\"start\":\"81074\",\"end\":\"81078\"},{\"start\":\"81087\",\"end\":\"81090\"},{\"start\":\"81097\",\"end\":\"81099\"},{\"start\":\"81110\",\"end\":\"81115\"},{\"start\":\"81124\",\"end\":\"81131\"},{\"start\":\"81138\",\"end\":\"81145\"},{\"start\":\"81151\",\"end\":\"81153\"},{\"start\":\"81491\",\"end\":\"81495\"},{\"start\":\"81503\",\"end\":\"81506\"},{\"start\":\"81516\",\"end\":\"81520\"},{\"start\":\"81528\",\"end\":\"81530\"},{\"start\":\"81543\",\"end\":\"81546\"},{\"start\":\"81932\",\"end\":\"81944\"},{\"start\":\"81951\",\"end\":\"81956\"},{\"start\":\"81970\",\"end\":\"81977\"},{\"start\":\"81985\",\"end\":\"81991\"},{\"start\":\"82000\",\"end\":\"82009\"},{\"start\":\"82011\",\"end\":\"82019\"},{\"start\":\"82575\",\"end\":\"82581\"},{\"start\":\"82587\",\"end\":\"82594\"},{\"start\":\"82778\",\"end\":\"82784\"},{\"start\":\"82791\",\"end\":\"82793\"},{\"start\":\"82797\",\"end\":\"82804\"},{\"start\":\"82815\",\"end\":\"82824\"},{\"start\":\"82826\",\"end\":\"82835\"},{\"start\":\"83038\",\"end\":\"83045\"},{\"start\":\"83052\",\"end\":\"83058\"},{\"start\":\"83066\",\"end\":\"83068\"},{\"start\":\"83075\",\"end\":\"83080\"},{\"start\":\"83091\",\"end\":\"83103\"},{\"start\":\"83109\",\"end\":\"83115\"},{\"start\":\"83126\",\"end\":\"83131\"},{\"start\":\"83142\",\"end\":\"83154\"},{\"start\":\"83491\",\"end\":\"83495\"},{\"start\":\"83503\",\"end\":\"83507\"},{\"start\":\"83516\",\"end\":\"83519\"},{\"start\":\"83529\",\"end\":\"83533\"},{\"start\":\"83542\",\"end\":\"83546\"},{\"start\":\"83556\",\"end\":\"83565\"},{\"start\":\"83574\",\"end\":\"83587\"},{\"start\":\"83594\",\"end\":\"83602\"},{\"start\":\"84232\",\"end\":\"84240\"},{\"start\":\"84249\",\"end\":\"84258\"},{\"start\":\"84526\",\"end\":\"84533\"},{\"start\":\"84542\",\"end\":\"84547\"},{\"start\":\"84557\",\"end\":\"84566\"},{\"start\":\"84580\",\"end\":\"84585\"},{\"start\":\"84936\",\"end\":\"84942\"},{\"start\":\"84950\",\"end\":\"84960\"},{\"start\":\"84969\",\"end\":\"84975\"},{\"start\":\"84986\",\"end\":\"84994\"},{\"start\":\"85000\",\"end\":\"85009\"},{\"start\":\"85276\",\"end\":\"85287\"},{\"start\":\"85296\",\"end\":\"85304\"},{\"start\":\"85314\",\"end\":\"85321\"},{\"start\":\"85331\",\"end\":\"85337\"},{\"start\":\"85346\",\"end\":\"85353\"},{\"start\":\"85360\",\"end\":\"85366\"},{\"start\":\"85783\",\"end\":\"85787\"},{\"start\":\"85795\",\"end\":\"85800\"},{\"start\":\"85807\",\"end\":\"85811\"},{\"start\":\"85820\",\"end\":\"85825\"},{\"start\":\"85833\",\"end\":\"85837\"},{\"start\":\"86269\",\"end\":\"86274\"},{\"start\":\"86281\",\"end\":\"86287\"},{\"start\":\"86495\",\"end\":\"86499\"},{\"start\":\"86507\",\"end\":\"86515\"},{\"start\":\"86530\",\"end\":\"86536\"},{\"start\":\"86546\",\"end\":\"86555\"},{\"start\":\"86563\",\"end\":\"86569\"},{\"start\":\"86579\",\"end\":\"86582\"},{\"start\":\"86593\",\"end\":\"86598\"},{\"start\":\"86605\",\"end\":\"86609\"},{\"start\":\"86617\",\"end\":\"86621\"},{\"start\":\"86630\",\"end\":\"86635\"},{\"start\":\"86641\",\"end\":\"86645\"},{\"start\":\"86654\",\"end\":\"86658\"},{\"start\":\"87243\",\"end\":\"87248\"},{\"start\":\"87257\",\"end\":\"87263\"},{\"start\":\"87271\",\"end\":\"87273\"},{\"start\":\"87282\",\"end\":\"87287\"},{\"start\":\"87294\",\"end\":\"87298\"}]", "bib_entry": "[{\"start\":\"66258\",\"end\":\"66369\",\"attributes\":{\"id\":\"b0\"}},{\"start\":\"66371\",\"end\":\"66488\",\"attributes\":{\"id\":\"b1\"}},{\"start\":\"66490\",\"end\":\"66675\",\"attributes\":{\"id\":\"b2\"}},{\"start\":\"66677\",\"end\":\"66852\",\"attributes\":{\"id\":\"b3\"}},{\"start\":\"66854\",\"end\":\"67048\",\"attributes\":{\"id\":\"b4\"}},{\"start\":\"67050\",\"end\":\"67189\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"67191\",\"end\":\"67363\",\"attributes\":{\"id\":\"b6\"}},{\"start\":\"67365\",\"end\":\"67499\",\"attributes\":{\"id\":\"b7\"}},{\"start\":\"67501\",\"end\":\"67858\",\"attributes\":{\"matched_paper_id\":\"207193360\",\"id\":\"b8\"}},{\"start\":\"67860\",\"end\":\"68539\",\"attributes\":{\"matched_paper_id\":\"51695269\",\"id\":\"b9\"}},{\"start\":\"68541\",\"end\":\"69123\",\"attributes\":{\"matched_paper_id\":\"11590585\",\"id\":\"b10\"}},{\"start\":\"69125\",\"end\":\"69539\",\"attributes\":{\"matched_paper_id\":\"198169848\",\"id\":\"b11\"}},{\"start\":\"69541\",\"end\":\"69818\",\"attributes\":{\"id\":\"b12\",\"doi\":\"abs/1710.09282\"}},{\"start\":\"69820\",\"end\":\"70134\",\"attributes\":{\"id\":\"b13\",\"doi\":\"arXiv:1609.03193\"}},{\"start\":\"70136\",\"end\":\"70667\",\"attributes\":{\"matched_paper_id\":\"218892868\",\"id\":\"b14\"}},{\"start\":\"70669\",\"end\":\"71064\",\"attributes\":{\"id\":\"b15\",\"doi\":\"arXiv:1812.01776\"}},{\"start\":\"71066\",\"end\":\"71625\",\"attributes\":{\"matched_paper_id\":\"1701442\",\"id\":\"b16\"}},{\"start\":\"71627\",\"end\":\"72123\",\"attributes\":{\"matched_paper_id\":\"13539072\",\"id\":\"b17\"}},{\"start\":\"72125\",\"end\":\"72361\",\"attributes\":{\"matched_paper_id\":\"214797870\",\"id\":\"b18\"}},{\"start\":\"72363\",\"end\":\"72514\",\"attributes\":{\"id\":\"b19\"}},{\"start\":\"72516\",\"end\":\"72847\",\"attributes\":{\"id\":\"b20\",\"doi\":\"abs/1803.07835\"}},{\"start\":\"72849\",\"end\":\"73310\",\"attributes\":{\"matched_paper_id\":\"216015659\",\"id\":\"b21\"}},{\"start\":\"73312\",\"end\":\"73533\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"73535\",\"end\":\"73931\",\"attributes\":{\"matched_paper_id\":\"85518643\",\"id\":\"b23\"}},{\"start\":\"73933\",\"end\":\"74393\",\"attributes\":{\"id\":\"b24\",\"doi\":\"abs/1704.04861\"}},{\"start\":\"74395\",\"end\":\"75069\",\"attributes\":{\"matched_paper_id\":\"31635004\",\"id\":\"b25\"}},{\"start\":\"75071\",\"end\":\"75549\",\"attributes\":{\"matched_paper_id\":\"214720366\",\"id\":\"b26\"}},{\"start\":\"75551\",\"end\":\"76023\",\"attributes\":{\"matched_paper_id\":\"6687309\",\"id\":\"b27\",\"doi\":\"195:1-195:14\"}},{\"start\":\"76025\",\"end\":\"76496\",\"attributes\":{\"matched_paper_id\":\"53304832\",\"id\":\"b28\"}},{\"start\":\"76498\",\"end\":\"76906\",\"attributes\":{\"id\":\"b29\",\"doi\":\"abs/1602.07360\"}},{\"start\":\"76908\",\"end\":\"77528\",\"attributes\":{\"matched_paper_id\":\"207175615\",\"id\":\"b30\"}},{\"start\":\"77530\",\"end\":\"77856\",\"attributes\":{\"id\":\"b31\",\"doi\":\"arXiv:1901.10008\"}},{\"start\":\"77858\",\"end\":\"78532\",\"attributes\":{\"matched_paper_id\":\"4202768\",\"id\":\"b32\"}},{\"start\":\"78534\",\"end\":\"78822\",\"attributes\":{\"matched_paper_id\":\"47047265\",\"id\":\"b33\"}},{\"start\":\"78824\",\"end\":\"79189\",\"attributes\":{\"matched_paper_id\":\"195908774\",\"id\":\"b34\"}},{\"start\":\"79191\",\"end\":\"79836\",\"attributes\":{\"matched_paper_id\":\"52984791\",\"id\":\"b35\"}},{\"start\":\"79838\",\"end\":\"80247\",\"attributes\":{\"matched_paper_id\":\"2141740\",\"id\":\"b36\"}},{\"start\":\"80249\",\"end\":\"80533\",\"attributes\":{\"id\":\"b37\"}},{\"start\":\"80535\",\"end\":\"80994\",\"attributes\":{\"matched_paper_id\":\"27435112\",\"id\":\"b38\"}},{\"start\":\"80996\",\"end\":\"81404\",\"attributes\":{\"matched_paper_id\":\"54134066\",\"id\":\"b39\",\"doi\":\"258:1-258:12\"}},{\"start\":\"81406\",\"end\":\"81852\",\"attributes\":{\"matched_paper_id\":\"4949076\",\"id\":\"b40\"}},{\"start\":\"81854\",\"end\":\"82566\",\"attributes\":{\"matched_paper_id\":\"1881381\",\"id\":\"b41\"}},{\"start\":\"82568\",\"end\":\"82718\",\"attributes\":{\"id\":\"b42\"}},{\"start\":\"82720\",\"end\":\"83025\",\"attributes\":{\"id\":\"b43\",\"doi\":\"arXiv:1905.13348\"}},{\"start\":\"83027\",\"end\":\"83410\",\"attributes\":{\"id\":\"b44\",\"doi\":\"abs/1810.09679\"}},{\"start\":\"83412\",\"end\":\"84156\",\"attributes\":{\"matched_paper_id\":\"204812163\",\"id\":\"b45\"}},{\"start\":\"84158\",\"end\":\"84431\",\"attributes\":{\"id\":\"b46\",\"doi\":\"arXiv:1409.1556\"}},{\"start\":\"84433\",\"end\":\"84867\",\"attributes\":{\"matched_paper_id\":\"1023605\",\"id\":\"b47\"}},{\"start\":\"84869\",\"end\":\"85226\",\"attributes\":{\"id\":\"b48\",\"doi\":\"arXiv:1812.11631\"}},{\"start\":\"85228\",\"end\":\"85727\",\"attributes\":{\"matched_paper_id\":\"4228546\",\"id\":\"b49\"}},{\"start\":\"85729\",\"end\":\"86202\",\"attributes\":{\"matched_paper_id\":\"14599090\",\"id\":\"b50\"}},{\"start\":\"86204\",\"end\":\"86424\",\"attributes\":{\"id\":\"b51\",\"doi\":\"abs/1812.00442\"}},{\"start\":\"86426\",\"end\":\"87153\",\"attributes\":{\"matched_paper_id\":\"52987896\",\"id\":\"b52\"}},{\"start\":\"87155\",\"end\":\"87734\",\"attributes\":{\"matched_paper_id\":\"85519380\",\"id\":\"b53\"}}]", "bib_title": "[{\"start\":\"67501\",\"end\":\"67555\"},{\"start\":\"67860\",\"end\":\"67920\"},{\"start\":\"68541\",\"end\":\"68609\"},{\"start\":\"69125\",\"end\":\"69202\"},{\"start\":\"70136\",\"end\":\"70216\"},{\"start\":\"71066\",\"end\":\"71121\"},{\"start\":\"71627\",\"end\":\"71716\"},{\"start\":\"72125\",\"end\":\"72180\"},{\"start\":\"72849\",\"end\":\"72871\"},{\"start\":\"73535\",\"end\":\"73587\"},{\"start\":\"74395\",\"end\":\"74461\"},{\"start\":\"75071\",\"end\":\"75154\"},{\"start\":\"75551\",\"end\":\"75614\"},{\"start\":\"76025\",\"end\":\"76101\"},{\"start\":\"76908\",\"end\":\"76966\"},{\"start\":\"77858\",\"end\":\"77920\"},{\"start\":\"78534\",\"end\":\"78558\"},{\"start\":\"78824\",\"end\":\"78887\"},{\"start\":\"79191\",\"end\":\"79271\"},{\"start\":\"79838\",\"end\":\"79872\"},{\"start\":\"80249\",\"end\":\"80296\"},{\"start\":\"80535\",\"end\":\"80580\"},{\"start\":\"80996\",\"end\":\"81043\"},{\"start\":\"81406\",\"end\":\"81481\"},{\"start\":\"81854\",\"end\":\"81924\"},{\"start\":\"83412\",\"end\":\"83481\"},{\"start\":\"84433\",\"end\":\"84514\"},{\"start\":\"85228\",\"end\":\"85263\"},{\"start\":\"85729\",\"end\":\"85775\"},{\"start\":\"86426\",\"end\":\"86485\"},{\"start\":\"87155\",\"end\":\"87234\"}]", "bib_author": "[{\"start\":\"66287\",\"end\":\"66294\"},{\"start\":\"67052\",\"end\":\"67088\"},{\"start\":\"67367\",\"end\":\"67379\"},{\"start\":\"67379\",\"end\":\"67384\"},{\"start\":\"67557\",\"end\":\"67570\"},{\"start\":\"67570\",\"end\":\"67580\"},{\"start\":\"67580\",\"end\":\"67598\"},{\"start\":\"67598\",\"end\":\"67615\"},{\"start\":\"67615\",\"end\":\"67627\"},{\"start\":\"67922\",\"end\":\"67937\"},{\"start\":\"67937\",\"end\":\"67948\"},{\"start\":\"67948\",\"end\":\"67965\"},{\"start\":\"67965\",\"end\":\"67977\"},{\"start\":\"67977\",\"end\":\"67991\"},{\"start\":\"67991\",\"end\":\"68011\"},{\"start\":\"68011\",\"end\":\"68028\"},{\"start\":\"68028\",\"end\":\"68040\"},{\"start\":\"68040\",\"end\":\"68051\"},{\"start\":\"68611\",\"end\":\"68625\"},{\"start\":\"68625\",\"end\":\"68660\"},{\"start\":\"68660\",\"end\":\"68679\"},{\"start\":\"68679\",\"end\":\"68689\"},{\"start\":\"68689\",\"end\":\"68706\"},{\"start\":\"68706\",\"end\":\"68718\"},{\"start\":\"68718\",\"end\":\"68732\"},{\"start\":\"68732\",\"end\":\"68749\"},{\"start\":\"68749\",\"end\":\"68765\"},{\"start\":\"68765\",\"end\":\"68771\"},{\"start\":\"69204\",\"end\":\"69211\"},{\"start\":\"69211\",\"end\":\"69222\"},{\"start\":\"69222\",\"end\":\"69234\"},{\"start\":\"69234\",\"end\":\"69243\"},{\"start\":\"69243\",\"end\":\"69252\"},{\"start\":\"69252\",\"end\":\"69260\"},{\"start\":\"69614\",\"end\":\"69624\"},{\"start\":\"69624\",\"end\":\"69634\"},{\"start\":\"69634\",\"end\":\"69644\"},{\"start\":\"69644\",\"end\":\"69655\"},{\"start\":\"69886\",\"end\":\"69903\"},{\"start\":\"69903\",\"end\":\"69922\"},{\"start\":\"69922\",\"end\":\"69940\"},{\"start\":\"70218\",\"end\":\"70238\"},{\"start\":\"70238\",\"end\":\"70248\"},{\"start\":\"70248\",\"end\":\"70263\"},{\"start\":\"70263\",\"end\":\"70280\"},{\"start\":\"70725\",\"end\":\"70743\"},{\"start\":\"70743\",\"end\":\"70753\"},{\"start\":\"70753\",\"end\":\"70765\"},{\"start\":\"70765\",\"end\":\"70780\"},{\"start\":\"70780\",\"end\":\"70793\"},{\"start\":\"70793\",\"end\":\"70807\"},{\"start\":\"70807\",\"end\":\"70822\"},{\"start\":\"70822\",\"end\":\"70831\"},{\"start\":\"71123\",\"end\":\"71141\"},{\"start\":\"71141\",\"end\":\"71151\"},{\"start\":\"71151\",\"end\":\"71164\"},{\"start\":\"71164\",\"end\":\"71184\"},{\"start\":\"71184\",\"end\":\"71195\"},{\"start\":\"71718\",\"end\":\"71732\"},{\"start\":\"71732\",\"end\":\"71743\"},{\"start\":\"71743\",\"end\":\"71754\"},{\"start\":\"71754\",\"end\":\"71762\"},{\"start\":\"71762\",\"end\":\"71773\"},{\"start\":\"71773\",\"end\":\"71789\"},{\"start\":\"71789\",\"end\":\"71795\"},{\"start\":\"72182\",\"end\":\"72196\"},{\"start\":\"72196\",\"end\":\"72213\"},{\"start\":\"72603\",\"end\":\"72613\"},{\"start\":\"72613\",\"end\":\"72621\"},{\"start\":\"72621\",\"end\":\"72634\"},{\"start\":\"72634\",\"end\":\"72648\"},{\"start\":\"72648\",\"end\":\"72657\"},{\"start\":\"72873\",\"end\":\"72890\"},{\"start\":\"72890\",\"end\":\"72906\"},{\"start\":\"72906\",\"end\":\"72922\"},{\"start\":\"73367\",\"end\":\"73380\"},{\"start\":\"73589\",\"end\":\"73604\"},{\"start\":\"73604\",\"end\":\"73622\"},{\"start\":\"73622\",\"end\":\"73634\"},{\"start\":\"73634\",\"end\":\"73644\"},{\"start\":\"73644\",\"end\":\"73651\"},{\"start\":\"74017\",\"end\":\"74034\"},{\"start\":\"74034\",\"end\":\"74048\"},{\"start\":\"74048\",\"end\":\"74057\"},{\"start\":\"74057\",\"end\":\"74078\"},{\"start\":\"74078\",\"end\":\"74091\"},{\"start\":\"74091\",\"end\":\"74106\"},{\"start\":\"74106\",\"end\":\"74123\"},{\"start\":\"74123\",\"end\":\"74137\"},{\"start\":\"74463\",\"end\":\"74476\"},{\"start\":\"74476\",\"end\":\"74501\"},{\"start\":\"74501\",\"end\":\"74514\"},{\"start\":\"74514\",\"end\":\"74537\"},{\"start\":\"74537\",\"end\":\"74552\"},{\"start\":\"74552\",\"end\":\"74571\"},{\"start\":\"74571\",\"end\":\"74590\"},{\"start\":\"74590\",\"end\":\"74602\"},{\"start\":\"75156\",\"end\":\"75165\"},{\"start\":\"75165\",\"end\":\"75189\"},{\"start\":\"75616\",\"end\":\"75626\"},{\"start\":\"75626\",\"end\":\"75642\"},{\"start\":\"75642\",\"end\":\"75654\"},{\"start\":\"75654\",\"end\":\"75667\"},{\"start\":\"75667\",\"end\":\"75679\"},{\"start\":\"75679\",\"end\":\"75693\"},{\"start\":\"75693\",\"end\":\"75707\"},{\"start\":\"75707\",\"end\":\"75719\"},{\"start\":\"75719\",\"end\":\"75734\"},{\"start\":\"75734\",\"end\":\"75742\"},{\"start\":\"76103\",\"end\":\"76113\"},{\"start\":\"76113\",\"end\":\"76131\"},{\"start\":\"76131\",\"end\":\"76143\"},{\"start\":\"76143\",\"end\":\"76160\"},{\"start\":\"76580\",\"end\":\"76599\"},{\"start\":\"76599\",\"end\":\"76620\"},{\"start\":\"76620\",\"end\":\"76635\"},{\"start\":\"76635\",\"end\":\"76645\"},{\"start\":\"76645\",\"end\":\"76662\"},{\"start\":\"76662\",\"end\":\"76676\"},{\"start\":\"76968\",\"end\":\"76983\"},{\"start\":\"76983\",\"end\":\"77004\"},{\"start\":\"77004\",\"end\":\"77016\"},{\"start\":\"77016\",\"end\":\"77028\"},{\"start\":\"77028\",\"end\":\"77042\"},{\"start\":\"77042\",\"end\":\"77059\"},{\"start\":\"77530\",\"end\":\"77542\"},{\"start\":\"77542\",\"end\":\"77554\"},{\"start\":\"77554\",\"end\":\"77565\"},{\"start\":\"77565\",\"end\":\"77581\"},{\"start\":\"77581\",\"end\":\"77600\"},{\"start\":\"77600\",\"end\":\"77612\"},{\"start\":\"77922\",\"end\":\"77932\"},{\"start\":\"77932\",\"end\":\"77946\"},{\"start\":\"77946\",\"end\":\"77961\"},{\"start\":\"77961\",\"end\":\"77974\"},{\"start\":\"77974\",\"end\":\"77992\"},{\"start\":\"77992\",\"end\":\"78010\"},{\"start\":\"78010\",\"end\":\"78023\"},{\"start\":\"78023\",\"end\":\"78037\"},{\"start\":\"78037\",\"end\":\"78049\"},{\"start\":\"78049\",\"end\":\"78059\"},{\"start\":\"78059\",\"end\":\"78069\"},{\"start\":\"78560\",\"end\":\"78574\"},{\"start\":\"78574\",\"end\":\"78589\"},{\"start\":\"78589\",\"end\":\"78602\"},{\"start\":\"78602\",\"end\":\"78617\"},{\"start\":\"78617\",\"end\":\"78635\"},{\"start\":\"78889\",\"end\":\"78906\"},{\"start\":\"78906\",\"end\":\"78922\"},{\"start\":\"78922\",\"end\":\"78941\"},{\"start\":\"79273\",\"end\":\"79287\"},{\"start\":\"79287\",\"end\":\"79304\"},{\"start\":\"79304\",\"end\":\"79320\"},{\"start\":\"79320\",\"end\":\"79349\"},{\"start\":\"79349\",\"end\":\"79364\"},{\"start\":\"79364\",\"end\":\"79383\"},{\"start\":\"79874\",\"end\":\"79883\"},{\"start\":\"79883\",\"end\":\"79902\"},{\"start\":\"79902\",\"end\":\"79917\"},{\"start\":\"79917\",\"end\":\"79936\"},{\"start\":\"79936\",\"end\":\"79948\"},{\"start\":\"79948\",\"end\":\"79963\"},{\"start\":\"79963\",\"end\":\"79981\"},{\"start\":\"80298\",\"end\":\"80305\"},{\"start\":\"80305\",\"end\":\"80314\"},{\"start\":\"80314\",\"end\":\"80324\"},{\"start\":\"80324\",\"end\":\"80332\"},{\"start\":\"80332\",\"end\":\"80347\"},{\"start\":\"80347\",\"end\":\"80359\"},{\"start\":\"80582\",\"end\":\"80594\"},{\"start\":\"80594\",\"end\":\"80610\"},{\"start\":\"80610\",\"end\":\"80629\"},{\"start\":\"81045\",\"end\":\"81058\"},{\"start\":\"81058\",\"end\":\"81070\"},{\"start\":\"81070\",\"end\":\"81080\"},{\"start\":\"81080\",\"end\":\"81092\"},{\"start\":\"81092\",\"end\":\"81101\"},{\"start\":\"81101\",\"end\":\"81117\"},{\"start\":\"81117\",\"end\":\"81133\"},{\"start\":\"81133\",\"end\":\"81147\"},{\"start\":\"81147\",\"end\":\"81155\"},{\"start\":\"81483\",\"end\":\"81497\"},{\"start\":\"81497\",\"end\":\"81508\"},{\"start\":\"81508\",\"end\":\"81522\"},{\"start\":\"81522\",\"end\":\"81532\"},{\"start\":\"81532\",\"end\":\"81548\"},{\"start\":\"81926\",\"end\":\"81946\"},{\"start\":\"81946\",\"end\":\"81958\"},{\"start\":\"81958\",\"end\":\"81979\"},{\"start\":\"81979\",\"end\":\"81993\"},{\"start\":\"81993\",\"end\":\"82011\"},{\"start\":\"82011\",\"end\":\"82021\"},{\"start\":\"82568\",\"end\":\"82583\"},{\"start\":\"82583\",\"end\":\"82596\"},{\"start\":\"82768\",\"end\":\"82786\"},{\"start\":\"82786\",\"end\":\"82795\"},{\"start\":\"82795\",\"end\":\"82806\"},{\"start\":\"82806\",\"end\":\"82826\"},{\"start\":\"82826\",\"end\":\"82837\"},{\"start\":\"83029\",\"end\":\"83047\"},{\"start\":\"83047\",\"end\":\"83060\"},{\"start\":\"83060\",\"end\":\"83070\"},{\"start\":\"83070\",\"end\":\"83082\"},{\"start\":\"83082\",\"end\":\"83105\"},{\"start\":\"83105\",\"end\":\"83117\"},{\"start\":\"83117\",\"end\":\"83133\"},{\"start\":\"83133\",\"end\":\"83156\"},{\"start\":\"83483\",\"end\":\"83497\"},{\"start\":\"83497\",\"end\":\"83509\"},{\"start\":\"83509\",\"end\":\"83521\"},{\"start\":\"83521\",\"end\":\"83535\"},{\"start\":\"83535\",\"end\":\"83548\"},{\"start\":\"83548\",\"end\":\"83567\"},{\"start\":\"83567\",\"end\":\"83589\"},{\"start\":\"83589\",\"end\":\"83604\"},{\"start\":\"84226\",\"end\":\"84242\"},{\"start\":\"84242\",\"end\":\"84260\"},{\"start\":\"84516\",\"end\":\"84535\"},{\"start\":\"84535\",\"end\":\"84549\"},{\"start\":\"84549\",\"end\":\"84568\"},{\"start\":\"84568\",\"end\":\"84587\"},{\"start\":\"84930\",\"end\":\"84944\"},{\"start\":\"84944\",\"end\":\"84962\"},{\"start\":\"84962\",\"end\":\"84977\"},{\"start\":\"84977\",\"end\":\"84996\"},{\"start\":\"84996\",\"end\":\"85011\"},{\"start\":\"85265\",\"end\":\"85289\"},{\"start\":\"85289\",\"end\":\"85306\"},{\"start\":\"85306\",\"end\":\"85323\"},{\"start\":\"85323\",\"end\":\"85339\"},{\"start\":\"85339\",\"end\":\"85355\"},{\"start\":\"85355\",\"end\":\"85368\"},{\"start\":\"85777\",\"end\":\"85789\"},{\"start\":\"85789\",\"end\":\"85802\"},{\"start\":\"85802\",\"end\":\"85813\"},{\"start\":\"85813\",\"end\":\"85827\"},{\"start\":\"85827\",\"end\":\"85839\"},{\"start\":\"86261\",\"end\":\"86276\"},{\"start\":\"86276\",\"end\":\"86289\"},{\"start\":\"86487\",\"end\":\"86501\"},{\"start\":\"86501\",\"end\":\"86517\"},{\"start\":\"86517\",\"end\":\"86538\"},{\"start\":\"86538\",\"end\":\"86557\"},{\"start\":\"86557\",\"end\":\"86571\"},{\"start\":\"86571\",\"end\":\"86584\"},{\"start\":\"86584\",\"end\":\"86600\"},{\"start\":\"86600\",\"end\":\"86611\"},{\"start\":\"86611\",\"end\":\"86623\"},{\"start\":\"86623\",\"end\":\"86637\"},{\"start\":\"86637\",\"end\":\"86647\"},{\"start\":\"86647\",\"end\":\"86660\"},{\"start\":\"87236\",\"end\":\"87250\"},{\"start\":\"87250\",\"end\":\"87265\"},{\"start\":\"87265\",\"end\":\"87275\"},{\"start\":\"87275\",\"end\":\"87289\"},{\"start\":\"87289\",\"end\":\"87300\"}]", "bib_venue": "[{\"start\":\"68151\",\"end\":\"68234\"},{\"start\":\"71275\",\"end\":\"71285\"},{\"start\":\"71843\",\"end\":\"71874\"},{\"start\":\"73009\",\"end\":\"73096\"},{\"start\":\"73701\",\"end\":\"73734\"},{\"start\":\"74682\",\"end\":\"74694\"},{\"start\":\"76221\",\"end\":\"76265\"},{\"start\":\"77147\",\"end\":\"77235\"},{\"start\":\"78150\",\"end\":\"78214\"},{\"start\":\"79463\",\"end\":\"79475\"},{\"start\":\"80377\",\"end\":\"80387\"},{\"start\":\"80721\",\"end\":\"80796\"},{\"start\":\"81598\",\"end\":\"81631\"},{\"start\":\"82129\",\"end\":\"82237\"},{\"start\":\"83685\",\"end\":\"83766\"},{\"start\":\"85437\",\"end\":\"85489\"},{\"start\":\"85905\",\"end\":\"85971\"},{\"start\":\"86740\",\"end\":\"86752\"},{\"start\":\"87393\",\"end\":\"87469\"},{\"start\":\"66258\",\"end\":\"66285\"},{\"start\":\"66371\",\"end\":\"66384\"},{\"start\":\"66490\",\"end\":\"66543\"},{\"start\":\"66677\",\"end\":\"66721\"},{\"start\":\"66854\",\"end\":\"66919\"},{\"start\":\"67191\",\"end\":\"67239\"},{\"start\":\"67627\",\"end\":\"67665\"},{\"start\":\"68051\",\"end\":\"68149\"},{\"start\":\"68771\",\"end\":\"68815\"},{\"start\":\"69260\",\"end\":\"69322\"},{\"start\":\"69541\",\"end\":\"69612\"},{\"start\":\"69820\",\"end\":\"69884\"},{\"start\":\"70280\",\"end\":\"70380\"},{\"start\":\"70669\",\"end\":\"70723\"},{\"start\":\"71195\",\"end\":\"71273\"},{\"start\":\"71795\",\"end\":\"71841\"},{\"start\":\"72213\",\"end\":\"72224\"},{\"start\":\"72365\",\"end\":\"72408\"},{\"start\":\"72516\",\"end\":\"72601\"},{\"start\":\"72922\",\"end\":\"73007\"},{\"start\":\"73312\",\"end\":\"73365\"},{\"start\":\"73651\",\"end\":\"73699\"},{\"start\":\"73933\",\"end\":\"74015\"},{\"start\":\"74602\",\"end\":\"74680\"},{\"start\":\"75189\",\"end\":\"75289\"},{\"start\":\"75754\",\"end\":\"75770\"},{\"start\":\"76160\",\"end\":\"76219\"},{\"start\":\"76498\",\"end\":\"76578\"},{\"start\":\"77059\",\"end\":\"77145\"},{\"start\":\"77628\",\"end\":\"77671\"},{\"start\":\"78069\",\"end\":\"78148\"},{\"start\":\"78635\",\"end\":\"78658\"},{\"start\":\"78941\",\"end\":\"78990\"},{\"start\":\"79383\",\"end\":\"79461\"},{\"start\":\"79981\",\"end\":\"80019\"},{\"start\":\"80359\",\"end\":\"80375\"},{\"start\":\"80629\",\"end\":\"80719\"},{\"start\":\"81167\",\"end\":\"81183\"},{\"start\":\"81548\",\"end\":\"81596\"},{\"start\":\"82021\",\"end\":\"82127\"},{\"start\":\"82596\",\"end\":\"82637\"},{\"start\":\"82720\",\"end\":\"82766\"},{\"start\":\"83170\",\"end\":\"83212\"},{\"start\":\"83604\",\"end\":\"83683\"},{\"start\":\"84158\",\"end\":\"84224\"},{\"start\":\"84587\",\"end\":\"84642\"},{\"start\":\"84869\",\"end\":\"84928\"},{\"start\":\"85368\",\"end\":\"85435\"},{\"start\":\"85839\",\"end\":\"85903\"},{\"start\":\"86204\",\"end\":\"86259\"},{\"start\":\"86660\",\"end\":\"86738\"},{\"start\":\"87300\",\"end\":\"87391\"}]"}}}, "year": 2023, "month": 12, "day": 17}