{"id": 211069391, "updated": "2023-10-06 19:28:09.589", "metadata": {"title": "Universal Semantic Segmentation for Fisheye Urban Driving Images", "authors": "[{\"first\":\"Yaozu\",\"last\":\"Ye\",\"middle\":[]},{\"first\":\"Kailun\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Kaite\",\"last\":\"Xiang\",\"middle\":[]},{\"first\":\"Juan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Kaiwei\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)", "journal": "2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)", "publication_date": {"year": 2020, "month": 1, "day": 31}, "abstract": "Semantic segmentation is a critical method in the field of autonomous driving. When performing semantic image segmentation, a wider field of view (FoV) helps to obtain more information about the surrounding environment, making automatic driving safer and more reliable, which could be offered by fisheye cameras. However, large public fisheye data sets are not available, and the fisheye images captured by the fisheye camera with large FoV comes with large distortion, so commonly-used semantic segmentation model cannot be directly utilized. In this paper, a seven degrees of freedom (DoF) augmentation method is proposed to transform rectilinear image to fisheye image in a more comprehensive way. In the training process, rectilinear images are transformed into fisheye images in seven DoF, which simulates the fisheye images taken by cameras of different positions, orientations and focal lengths. The result shows that training with the seven-DoF augmentation can evidently improve the model's accuracy and robustness against different distorted fisheye data. This seven-DoF augmentation provides an universal semantic segmentation solution for fisheye cameras in different autonomous driving applications. Also, we provide specific parameter settings of the augmentation for autonomous driving. At last, we tested our universal semantic segmentation model on real fisheye images and obtained satisfactory results. The code and configurations are released at \\url{https://github.com/Yaozhuwa/FisheyeSeg}.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2002.03736", "mag": "3111513889", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/smc/YeYXWW20", "doi": "10.1109/smc42975.2020.9283099"}}, "content": {"source": {"pdf_hash": "515954fdbe7c9d02148860311416a040a8acbc3e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2002.03736v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2002.03736", "status": "GREEN"}}, "grobid": {"id": "ff813080299185cc2e86b6cbae1d51ca255bd521", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/515954fdbe7c9d02148860311416a040a8acbc3e.txt", "contents": "\nUniversal Semantic Segmentation for Fisheye Urban Driving Images\n\n\nYaozu Ye \nKailun Yang \nKaite Xiang \nJuan Wang \nKaiwei Wang \nUniversal Semantic Segmentation for Fisheye Urban Driving Images\n\nSemantic segmentation is a critical method in the field of autonomous driving. When performing semantic image segmentation, a wider field of view (FoV) helps to obtain more information about the surrounding environment, making automatic driving safer and more reliable, which could be offered by fisheye cameras. However, large public fisheye data sets are not available, and the fisheye images captured by the fisheye camera with large FoV comes with large distortion, so commonly-used semantic segmentation model cannot be directly utilized. In this paper, a seven degrees of freedom (DoF) augmentation method is proposed to transform rectilinear image to fisheye image in a more comprehensive way. In the training process, rectilinear images are transformed into fisheye images in seven DoF, which simulates the fisheye images taken by cameras of different positions, orientations and focal lengths. The result shows that training with the seven-DoF augmentation can evidently improve the model's accuracy and robustness against different distorted fisheye data. This seven-DoF augmentation provides an universal semantic segmentation solution for fisheye cameras in different autonomous driving applications. Also, we provide specific parameter settings of the augmentation for autonomous driving. At last, we tested our universal semantic segmentation model on real fisheye images and obtained satisfactory results. The code and configurations are released at https://github.com/Yaozhuwa/FisheyeSeg.\n\nI. INTRODUCTION\n\nWith the research boom of autonomous driving, scene understanding becomes a hot research topic. Semantic segmentation enables pixel-by-pixel tagging of images, completing several fine detection tasks at the same time, which makes it ideal for automated driving [1].\n\nThanks to the emergence of large-scale natural data sets [2] [3] and architectural advances of convolutional neural networks [4] [5], most current semantic segmentation studies are based on images taken by pinhole cameras. The urban traffic environment is so complex that more information of the surroundings is required, while the pinhole camera only has a narrow FoV. If a vehicle or pedestrian suddenly appears from the blind spot, the safety of autonomous driving is difficult to guarantee. The solution is to increase the amount of information obtained. Panoramic camera and multi-sensor fusion are good solutions [6] [7]. For example, by installing multiple cameras on the vehicle, or adding additional ultrasonic radar and LiDAR sensors, we can increase the 1 [9]. However, these methods require additional calibration and matching of multiple sensors, and repeated calibration and matching are required for different designed hardware collocation methods [10]. In addition, the multi-sensor approach is very expensive. A simpler and more direct way is to utilize a fisheye camera which naturally has a wide FoV [11] [12].\n\nWhile the fisheye image has a large FoV, it has large distortion. The distortion of the object depends on the view angle of the object relative to the fisheye camera. Moreover, the distortion of fisheye lenses with different focal lengths are also different. Due to the existence of distortion, the commonly-used semantic segmentation model for pinhole camera cannot be directly applied in fisheye image segmentation. There are two solutions to it. One is to de-wrap the fisheye image to rectilinear image, then use the common segmentation method to process the image. However, the de-wrapping process the will lead to a loss of boundary information. The de-wrapped image will have a smaller FoV, which is against the original intention of using a fisheye camera.\n\nThe second approach is to carry out the segmentation directly on the fisheye images. This approach works on condition that we have a large-scale finely-annotated fisheye data set to train our model. However, currently there is not a fisheye image data set that fits exactly the purpose, and collecting and annotating such a data set is expensive and laborious. The early research [13] relied on a r = f \u03b8 model to transform the rectilinear images to fisheye images (which they called it zoom augmentation), and trained their model with the synthetic data set. This zoom augmentation deals the lack of the fisheye data set to some extent. The followup researches [14] [15] inherited the zoom augmentation to synthesis virtual fisheye data set, but focused on the CNN structure design. To deal with the distortion of fisheye images, Deng et at. [16] designed a CNN structure based on deformable convolution [17] and obtained a better performance. Blott et al. [18] proposed to use the projection model transformation (PMT) to synthesize fisheye data, which they called it six-DoF (six degrees of freedom) augmentation. This six-DoF augmentation simulated the situation that camera rotated (three DoF) and shifted (three DoF) with respect to the coordinate system origin and axes. Another work [19] on pedestrian detection also utilized the PMT to generate their fisheye data, but they only explicated the camera rotation around the vertical line (one DoF).\n\nA big obstacle to the practical application of semantic segmentation in automatic driving is the robustness of semantic segmentation algorithm, which requires high segmentation accuracy in different scenarios. In particular, the robustness of the algorithm is critical for the safety requirements of automatic driving. The robustness of the algorithm not only depends on the design of CNN network structure, but also largely depends on the data set we feed it.\n\nIn this paper, we propose to utilize a more complex model to generate a richer data set, thereby improving the accuracy and robustness of fisheye semantic segmentation. Aiming at the urban driving scene, this paper proposes to use seven-DoF augmentation to transform rectilinear images to virtual fisheye images taken by fisheye cameras with different angles, positions and distortion parameters during training. And this seven-DoF augmentation can be used to train a universal fisheye semantic segmentation model.\n\nIn the method section, we explain the principle and advantages of the seven-DoF augmentation in detail. In the experiments section, we conduct several experiments to prove the superiority of the seven-DoF augmentation. Also, the setting of hyper-parameters for data augmentation is discussed there. Next, we test our universal semantic segmentation model on real fisheye images which are captured by a smart phone with an external fisheye lens and get satisfactory results. At last, we make a summary of the article and suggest some possible future research directions of fisheye segmentation.\n\n\nII. METHOD\n\nThe basic principle of data augmentation approach in this paper is to convert the rectilinear image of the world coordinate system into the synthetic fisheye image by using the projection model of the camera and a virtual fisheye camera (see Fig. 1). The so-called seven-DoF augmentation contains the spatial relationship between the world coordinate system and the fisheye coordinate system (six DoF) and the variation in the focal length of the virtual fisheye camera (one DoF). The relative rotation between the two coordinate systems contains three degrees of freedom, and the relative translation between the two coordinate systems also contains three degrees of freedom.\n\nThis seven-DoF augmentation, while not strictly simulating a fisheye image, can simulate its distortion pattern to some extent. Fig. 2 illustrates the effects of the seven-DoF augmentation.\n\nIn the process of training neural networks, we usually use some data augmentation methods to extend the training data set and reduce overfitting. Data augmentation methods for ordinary rectilinear data include random cropping, random horizontal flipping, panning, rotation, scaling, and color jittering. For fisheye data augmentation based on zoom augmentation, we can use all of these augmentation methods. However, when we use panning or scaling to augment our data, we just augment the data in rectilinear style, not in fisheye style. For a fisheye image, the distortion increases with the distance between the pixel and the center of the image. Besides, for the frames taken by the same fisheye camera, the distortion of the same position of each picture is the same. Therefore, once the fisheye image is translated, the distortion feature of the image will be destroyed. image that we place on the x-y plane of the world coordinate system. \u03b8 is the Angle of incidence of the point relative to the fisheye camera. P is the imaging point of P W on the fisheye image. |OP | = f \u03b8. The relative rotation and translation between the world coordinate system and the camera coordinate system results in six degrees of freedom.\n\nHowever, we can find that the seven-DoF augmentation method naturally do the data augmentation in fisheye style.\n\nAs shown in Fig. 2(i) and Fig. 2(j), when we change the relative z-axis position of the virtual fisheye camera coordinate system and the world coordinate system, it simulates the scene of fish-eye camera moving forward and backward. It makes the object closer to the fish-eye camera, which results in the object bigger in the image. The variation of the relative positions of X-axis and Y-axis between the fisheye coordinate system and the world coordinate system actually simulates the position changes of the virtual fisheye camera. Specifically, the augmentation of X-axis translation simulates the changes of the left and right position of the car on the road, while the Y-axis translation simulates the changes of the height of the fisheye camera on the car. It's also understandable that the data augmentation of rotation around three axis can simulate the orientation changes of the fisheye camera.\n\nIn practice, the fisheye camera will be placed on a car, and the attitude of the camera is always changing with the time and the turbulence of the car. Also, the position and orientation of the camera will vary from vehicle to vehicle, which results in a different view of the image. However, the neural network is not very good at handling these situation, because it is invariably trained from an existing data set, and the accuracy of the neural network will decrease if the situation is not similar with the existing data set. For example, when we use Cityscapes data set [2] to train a semantic segmentation network, if we place the camera at a lower position in the actual application, the perspective of the actual image will be different from the training set, which will lead to a decrease in the accuracy. If we have a data set which contains frames taken by cameras in different orientations and positions, that won't be a problem. But it's a huge project to collect and annotate such a data set with cameras of different orientations and different positions, especially for fisheye cameras, as fisheye camera has a parameter of focal length and the distortion of fisheye camera varies with focal length. With the seven-DoF augmentation, we can synthesize fisheye images of the camera of different positions, orientations and focal lengths, so that a general semantic segmentation data set of fisheye camera could be obtained.\n\n\nIII. EXPERIMENTS\n\n\nA. Data set and CNN structure\n\nCityScapes data set [2] is a well-known data set in the field of autonomous driving. It was recorded in street scenes from 50 different cities, and provides 5000 finely annotated frames, in addition to a larger set of 20000 coarsely annotated frames. Within the 5,000 pixel-level annotated frames, 2,975 frames were used for training, 500 frames for validation, and 1,525 frames for testing. We used the 2,975 training data and 500 validation data to conduct our experiments.\n\n1) Training data set: We directly use the 2975 data set as our raw data set, with the method of online data augmentation to transform the rectilinear data to fisheye images to train our neural networks. The original training set is the rectilinear image of 1024*2048 pixels. After data augmentation, we unified them into fisheye images of 640*640 pixels. This paper uses the method of online augmentation, that is, the parameters of seven-DoF augmentation change in every batch. The advantage of this method is that each image of the training set is transformed into a different fisheye image each time it is fed to the semantic segmentation network (the data augmentation part contains random parameters), which can greatly increase the richness of the training set.\n\n2) Testing data set: Testing set is to use the z-aug (zoom augmentation) to transform the 500 pieces of rectilinear cityscapes' validation data into virtual fisheye data. Different focal lengths are used to generate testing sets for a better evaluation of our models. We generate five testing sets and their focal length is 200, 250, 300, 350, 400 respectively. If the model has superior generalization performance, it should perform well on all testing sets.\n\n3) CNN structure: As this work focuses on the data augmentation, we simply choose SwiftNet-18 [5] (a lightweight CNN structure with ResNet 18 as it's backbone), which has a U-net [20] structure, to conduct our experiment.\n\n\nB. Experiment and result\n\nAs introduced in the Method section, we have the following data augmentation for fisheye semantic segmentation: random cropping, random flipping, color jitter, z-aug, six-DoF augmentation and seven-DoF augmentation. We believe that the neural network is only as good as the data we feed it. In order to explore which data augmentation method is the best, we carefully designed the following experiment.\n\nFor the benchmark, we adopted the data augmentation means of random clipping, random flip, color jitter and fixed f (virtual fisheye camera's focal length), and used the SwiftNet-18 as our semantic segmentation structure. The data augmentation methods are divided into three types: random focal length, random rotation and random translation. We designed the following data augmentation methods:\n\n1. Base Aug: random clipping + random flip + color jitter + z-aug of fixed focal length 2. RandF Aug: Base Aug + random focal length 3. RandR Aug: Base Aug + random rotation  \n\n\n4.\n\nRandT Aug: Base Aug + random translation 5. RandFR Aug: Base Aug + random focal length + random rotation 6. RandFT Aug: Base Aug + random focal length + random translation 7. Six-DoF Aug: Base Aug + random rotation + random translation 8. Seven-DoF Aug: Base Aug + random focal length + random rotation + random translation First, methods 1 to 4 are tested to compare the performance of a single data augmentation approach (see Fig.  3(a)). As it can be seen, when the focal length of the virtual fisheye image of the testing set is larger (the distortion is smaller), the segmentation ability of these models for the distorted image is better. Compared with Base Aug, the RandF Aug, RandR Aug and RandT Aug can evidently improve the accuracy of the model. Moreover, the RandF Aug had the best performance, while RandR Aug made the least improvement on mIoU.\n\nNext, we test the more complex data augmentation methods (see Fig. 3(b)). The performance of the model obtained by the combination of multiple data augmentation is better than that of the single method for the testing set of different distortion parameters. It indicates that the seven-DoF Aug achieves the best performance and reaches a high mIoU in every testing sets with different degrees of distortion, which proves the robustness of the seven-DoF augmentation. However, the six-DoF Aug has a worst performance compared with other approaches. The previous experiment already shows that random focal length improves the mIoU most, while random rotation improves the mIoU least. Therefore, it's understandable that the six-DoF Aug without random focal length augmentation performs the worst.\n\nThe performance of different data augmentation methods are shown in Table I. While the fixed z-aug performs worst, the seven-DoF augmentation we proposed almost performs best in all testing data sets. Compared with other augmentation methods, the seven-DoF has significant advantages, especially in testing data sets with larger distortion (smaller f value).\n\n\nC. Hyper-parameters settings\n\nThe above experiments demonstrate the effectiveness of the seven-DoF data augmentation method. However, arbitrarily setting the parameter values of different degrees of freedom cannot maximize the superiority of the method. In some cases, the value of a parameter is too unreasonable and may even cause the accuracy of the model to decrease. In order to set our hyper-parameters more scientifically, we conduct several experiments to test the model with different values of the hyper-parameters.\n\nThe first experiment explores the setting of the translation parameters. The translation parameters include translations along the x, y, and z axes. The translation along the x and y axes have similar effects on image distortion. We first discuss translation along the x-axis. We do not directly set an absolute pixel value as the variation range of the translation parameter, but uses a normalized value v of [0,1] to represent the translation. In the code implementation, we set v * f ish_width to the pixel value of the camera coordinate system's final translation, where f ish_width is the width of the virtual fisheye image finally generated. Taking Fig. 4(a)).\n\nFor the translation range v along the y-axis, by analogy, it should also be set to [-0.5, 0.5]. However, considering that our application scenario is urban autonomous driving, the actual meaning of the camera coordinate system translation along the y-axis is the height variation of the fisheye camera. In practice, when the vehicle is driving in different lanes, the left and right positions of the camera may vary greatly, but the height of the camera does not change much, even for different models of cars. Therefore, the parameter v for translation along the y-axis is set to [-0.1, 0.1]. For the parameter v of translation along the z axis, in the code implementation, we normalize it to (-1, 1), and the actual translation distance is the parameter v multiplied by the focal length of the pinhole camera. Here we set it to [-0.4, 0.4]. Under this parameter range, the distortion of the virtual fisheye image will not be too much. Similarly, for the setting of the rotation parameters, based on the experiment where the f value variation range is [200,400] as the benchmark, the variation range of the fisheye camera coordinate system rotation around the y axis is set to [-5, 5], [-15, 15], [-25, 25], [-35, 35] degrees respectively. The results (Fig. 4(b)) indicate that the model performs best when the rotation parameter range is set to [-25, 25]. For the parameter range of the camera coordinate system rotating around the x axis, we also set it to [-25, 25] degrees. For the rotation parameter setting around the z-axis, the effect it produces is the rotation of the fisheye image that is ultimately generated. We set it to [-25, 25] degrees.\n\n\nD. Real fisheye image test\n\nTo test the generalization performance of our model, we collected fisheye images of real urban street scenes. For convenience, we adopted an external fisheye lens for a mobile phone with a field of view of about 180 degrees and clip it to a smartphone. On the bus, we held the smartphone equipped with the external fisheye lens. As the bus navigated, we collected a series of fisheye image data of urban street scenes. We resized the obtained images to 640 * 640 resolution, and applied our model (based on seven-DoF augmentation) to the obtained images. Fig. 5 depicts the segmentation performance of our model in different scenes. As it can be seen, basically all categories are well segmented. \n\n\nIV. CONCLUSIONS AND FUTURE WORK\n\nThis paper proposes a general virtual fisheye data augmentation method, the seven-DoF augmentation. This method transforms a rectilinear data set into a fisheye data set in a comprehensible way, synthesizing fisheye images taken by cameras with different orientations, different positions, and different f values, which significantly improves the generalization performance of fisheye semantic segmentation. It provides an universal semantic segmentation solution for fisheye cameras in different autonomous driving applications. In addition, even if you already have a fisheye data set, this method is still very meaningful. Because in practice, it is unlikely that a fisheye lens with the same parameters as the data set will be used. The distortion parameters of different fisheye lenses are different, and the fisheye images obtained by different parameters such as orientation and camera height are also different. The data set taken by a fisheye camera with fixed parameter cannot be well adapted to segmentation task for images taken by cameras with other different parameters.\n\nThis paper also discusses the setting of hyper-parameters for data augmentation. The parameters specially designed for urban autonomous driving scenarios evidently improves the segmentation accuracy of the model. Besides, this article proposes a convenient method to obtain fisheye images, which combines a smartphone and an external fisheye lens. Finally, when applied to real fisheye images, our model achieves precise segmentation results.\n\nThis article only focuses on the data augmentation method of fisheye semantic segmentation, and does not modify the network structure according to the characteristics of fisheye images. In the future, we plan to make some CNN structural improvements for fisheye images specially. In addition, data augmentation methods for real fisheye data sets are also a promising research direction.\n\nFig. 1 .\n1Projection model of fisheye camera. P W is a point on a rectilinear\n\nFig. 2 .\n2Camera moves to the left (X) (f) Camera moves to the right (X) (g) Camera moves up (Y) (h) Camera moves down (Y) (i) Camera moves forward (Z) (j) Camera moves back (Z) (k) Camera turns left (Y) (l) Camera turns right (Y) (m) Camera turns up (X) (n) Camera turns down (X) (o) Camera rotates 15 degree (Z) (p) Camera rotates -15 degree (Z) The seven DoF augmentation.Except the first row, every image is transformed using a virtual fisheye camera with focal length of 300 pixels. The letter in brackets means that which axis the camera is panning along or rotating around.\n\nFig. 3 .\n3Result of all augmentation methods.\n\n\nthe experiment with f value variation range of [200, 400] as the benchmark, the range of the camera coordinate system translation along the x axis is set to [-0.1, 0.1], [-0.3, 0.3], [-0.5, 0.5], [-0.7, 0.7] respectively. The results reveal that the model works best when v = [\u22120.5, 0.5] (see\n\n\nof different translation range along x-axis (b) Results of different rotation range around y-axis Fig. 4. Result of hyper-parameters settings.\n\n\nY. Yao, K. Xiang and J. Wang are with State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China {yaozuye, katexiang, zjuwjopt}@zju.edu.cn 2 K. Yang is with Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany kailun.yang@kit.edu3 K. Wang is with National Optical Instrumentation Engineering \nTechnology \nResearch \nCenter, \nZhejiang \nUniversity, \nChina \nwangkaiwei@zju.edu.cn \n\namount of acquired information [8]\n\nTABLE I\nIPERFORMANCE OF DIFFERENT DATA AUGMENTATION Data augmentation mIoU (f = 200) mIoU (f = 250) mIoU (f = 300) mIoU (f = 350) mIoU (f = 400) Fig. 5. Semantic segmentation of real fisheye images.Fixed z-aug \n0.5562 \n0.5773 \n0.5869 \n0.5948 \n0.5930 \nRandom z-aug \n0.5979 \n0.6089 \n0.6117 \n0.6116 \n0.6070 \nSix-DoF aug \n0.5938 \n0.6082 \n0.6146 \n0.6156 \n0.6143 \nSeven-DoF aug \n0.6093 \n0.6150 \n0.6134 \n0.6166 \n0.6154 \n\n(a) \n(b) \n\n(c) \n(d) \n\n(e) \n(f) \n\n(g) color legend \n\n\n\nUnifying terrain awareness through real-time semantic segmentation. K Yang, L M Bergasa, E Romera, R Cheng, T Chen, K Wang, 2018 IEEE Intelligent Vehicles Symposium (IV). IEEEK. Yang, L. M. Bergasa, E. Romera, R. Cheng, T. Chen, and K. Wang, \"Unifying terrain awareness through real-time semantic segmentation,\" in 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2018, pp. 1033-1038.\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be- nenson, U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban scene understanding,\" in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016, pp. 3213-3223.\n\nThe apolloscape dataset for autonomous driving. X Huang, X Cheng, Q Geng, B Cao, D Zhou, P Wang, Y Lin, R Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsX. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin, and R. Yang, \"The apolloscape dataset for autonomous driving,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2018, pp. 954-960.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431-3440.\n\nIn defense of pretrained imagenet architectures for real-time semantic segmentation of road-driving images. M Orsic, I Kreso, P Bevandic, S Segvic, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition12616M. Orsic, I. Kreso, P. Bevandic, and S. Segvic, \"In defense of pre- trained imagenet architectures for real-time semantic segmentation of road-driving images,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 12 607-12 616.\n\nCan we pass beyond the field of view? panoramic annular semantic segmentation for real-world surrounding perception. K Yang, X Hu, L M Bergasa, E Romera, X Huang, D Sun, K Wang, 2019 IEEE Intelligent Vehicles Symposium (IV). IEEEK. Yang, X. Hu, L. M. Bergasa, E. Romera, X. Huang, D. Sun, and K. Wang, \"Can we pass beyond the field of view? panoramic annular semantic segmentation for real-world surrounding perception,\" in 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019, pp. 446-453.\n\nPass: Panoramic annular semantic segmentation. K Yang, X Hu, L M Bergasa, E Romera, K Wang, IEEE Transactions on Intelligent Transportation Systems. K. Yang, X. Hu, L. M. Bergasa, E. Romera, and K. Wang, \"Pass: Panoramic annular semantic segmentation,\" IEEE Transactions on Intelligent Transportation Systems, 2019.\n\nRgb and lidar fusion based 3d semantic segmentation for autonomous driving. K E Madawy, H Rashed, A E Sallab, O Nasr, H Kamel, S Yogamani, arXiv:1906.00208arXiv preprintK. E. Madawy, H. Rashed, A. E. Sallab, O. Nasr, H. Kamel, and S. Yogamani, \"Rgb and lidar fusion based 3d semantic segmentation for autonomous driving,\" arXiv preprint arXiv:1906.00208, 2019.\n\nDs-pass: Detail-sensitive panoramic annular semantic segmentation through swaftnet for surrounding sensing. K Yang, X Hu, H Chen, K Xiang, K Wang, R Stiefelhagen, arXiv:1909.07721arXiv preprintK. Yang, X. Hu, H. Chen, K. Xiang, K. Wang, and R. Stiefel- hagen, \"Ds-pass: Detail-sensitive panoramic annular semantic seg- mentation through swaftnet for surrounding sensing,\" arXiv preprint arXiv:1909.07721, 2019.\n\nFusion of millimeter wave radar and rgb-depth sensors for assisted navigation of the visually impaired. N Long, K Wang, R Cheng, K Yang, J Bai, Millimetre Wave and Terahertz Sensors and Technology XI. 108001080006N. Long, K. Wang, R. Cheng, K. Yang, and J. Bai, \"Fusion of millimeter wave radar and rgb-depth sensors for assisted navigation of the visually impaired,\" in Millimetre Wave and Terahertz Sensors and Technology XI, vol. 10800. International Society for Optics and Photonics, 2018, p. 1080006.\n\nWoodscape: A multi-task, multi-camera fisheye dataset for autonomous driving. S Yogamani, C Hughes, J Horgan, G Sistu, P Varley, D O&apos;dea, M Uric\u00e1r, S Milz, M Simon, K Amende, arXiv:1905.01489arXiv preprintS. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O'Dea, M. Uric\u00e1r, S. Milz, M. Simon, K. Amende et al., \"Woodscape: A multi-task, multi-camera fisheye dataset for autonomous driving,\" arXiv preprint arXiv:1905.01489, 2019.\n\nScene understanding networks for autonomous driving based on around view monitoring system. J Baek, I Veronica, L Chelu, V Iordache, H Paunescu, A Ryu, A Ghiuta, Y Petreanu, A Soh, B Leica, Jeon, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsJ. Yeol Baek, I. Veronica Chelu, L. Iordache, V. Paunescu, H. Ryu, A. Ghiuta, A. Petreanu, Y. Soh, A. Leica, and B. Jeon, \"Scene understanding networks for autonomous driving based on around view monitoring system,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2018, pp. 961- 968.\n\nCnn based semantic segmentation for urban traffic scenes using fisheye camera. L Deng, M Yang, Y Qian, C Wang, B Wang, 2017 IEEE Intelligent Vehicles Symposium (IV). IEEEL. Deng, M. Yang, Y. Qian, C. Wang, and B. Wang, \"Cnn based semantic segmentation for urban traffic scenes using fisheye camera,\" in 2017 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2017, pp. 231-236.\n\nCnn-based fisheye image real-time semantic segmentation. \u00c1 S\u00e1ez, L M Bergasa, E Romeral, E L\u00f3pez, R Barea, R Sanz, 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE\u00c1. S\u00e1ez, L. M. Bergasa, E. Romeral, E. L\u00f3pez, R. Barea, and R. Sanz, \"Cnn-based fisheye image real-time semantic segmentation,\" in 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2018, pp. 1039- 1044.\n\nReal-time semantic segmentation for fisheye urban driving images based on erfnet. \u00c1 S\u00e1ez, L M Bergasa, E L\u00f3pez-Guill\u00e9n, E Romera, M Tradacete, C G\u00f3mez-Hu\u00e9lamo, J Del Egido, Sensors. 193503\u00c1. S\u00e1ez, L. M. Bergasa, E. L\u00f3pez-Guill\u00e9n, E. Romera, M. Tradacete, C. G\u00f3mez-Hu\u00e9lamo, and J. del Egido, \"Real-time semantic segmenta- tion for fisheye urban driving images based on erfnet,\" Sensors, vol. 19, no. 3, p. 503, 2019.\n\nRestricted deformable convolution-based road scene semantic segmentation using surround view cameras. L Deng, M Yang, H Li, T Li, B Hu, C Wang, IEEE Transactions on Intelligent Transportation Systems. L. Deng, M. Yang, H. Li, T. Li, B. Hu, and C. Wang, \"Restricted deformable convolution-based road scene semantic segmentation using surround view cameras,\" IEEE Transactions on Intelligent Transporta- tion Systems, 2019.\n\nDeformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJ. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, \"Deformable convolutional networks,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 764-773.\n\nSemantic segmentation of fisheye images. G Blott, M Takami, C Heipke, European Conference on Computer Vision. SpringerG. Blott, M. Takami, and C. Heipke, \"Semantic segmentation of fish- eye images,\" in European Conference on Computer Vision. Springer, 2018, pp. 181-196.\n\nOriented spatial transformer network for pedestrian detection using fish-eye camera. Y Qian, M Yang, X Zhao, C Wang, B Wang, IEEE Transactions on Multimedia. Y. Qian, M. Yang, X. Zhao, C. Wang, and B. Wang, \"Oriented spatial transformer network for pedestrian detection using fish-eye camera,\" IEEE Transactions on Multimedia, 2019.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. O. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional networks for biomedical image segmentation,\" in International Con- ference on Medical Image Computing and Computer-Assisted Inter- vention, 2015.\n", "annotations": {"author": "[{\"end\":77,\"start\":68},{\"end\":90,\"start\":78},{\"end\":103,\"start\":91},{\"end\":114,\"start\":104},{\"end\":127,\"start\":115}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":74},{\"end\":89,\"start\":85},{\"end\":102,\"start\":97},{\"end\":113,\"start\":109},{\"end\":126,\"start\":122}]", "author_first_name": "[{\"end\":73,\"start\":68},{\"end\":84,\"start\":78},{\"end\":96,\"start\":91},{\"end\":108,\"start\":104},{\"end\":121,\"start\":115}]", "author_affiliation": null, "title": "[{\"end\":65,\"start\":1},{\"end\":192,\"start\":128}]", "venue": null, "abstract": "[{\"end\":1698,\"start\":194}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1981,\"start\":1978},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2044,\"start\":2041},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2048,\"start\":2045},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2112,\"start\":2109},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2116,\"start\":2113},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2606,\"start\":2603},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2610,\"start\":2607},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2750,\"start\":2749},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2754,\"start\":2751},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2951,\"start\":2947},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3107,\"start\":3103},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3112,\"start\":3108},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4264,\"start\":4260},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4546,\"start\":4542},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4551,\"start\":4547},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4727,\"start\":4723},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4789,\"start\":4785},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4842,\"start\":4838},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5175,\"start\":5171},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10617,\"start\":10614},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11551,\"start\":11548},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13332,\"start\":13329},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13418,\"start\":13414},{\"end\":17774,\"start\":17763},{\"end\":18521,\"start\":18510},{\"end\":18738,\"start\":18733},{\"end\":18742,\"start\":18738},{\"end\":18865,\"start\":18858},{\"end\":18876,\"start\":18867},{\"end\":18887,\"start\":18878},{\"end\":18897,\"start\":18889},{\"end\":19036,\"start\":19027},{\"end\":19149,\"start\":19140}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22093,\"start\":22015},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22675,\"start\":22094},{\"attributes\":{\"id\":\"fig_3\"},\"end\":22722,\"start\":22676},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23017,\"start\":22723},{\"attributes\":{\"id\":\"fig_5\"},\"end\":23162,\"start\":23018},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":23634,\"start\":23163},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":24102,\"start\":23635}]", "paragraph": "[{\"end\":1982,\"start\":1717},{\"end\":3113,\"start\":1984},{\"end\":3878,\"start\":3115},{\"end\":5334,\"start\":3880},{\"end\":5796,\"start\":5336},{\"end\":6312,\"start\":5798},{\"end\":6907,\"start\":6314},{\"end\":7598,\"start\":6922},{\"end\":7789,\"start\":7600},{\"end\":9015,\"start\":7791},{\"end\":9129,\"start\":9017},{\"end\":10036,\"start\":9131},{\"end\":11475,\"start\":10038},{\"end\":12003,\"start\":11528},{\"end\":12772,\"start\":12005},{\"end\":13233,\"start\":12774},{\"end\":13456,\"start\":13235},{\"end\":13887,\"start\":13485},{\"end\":14284,\"start\":13889},{\"end\":14461,\"start\":14286},{\"end\":15326,\"start\":14468},{\"end\":16122,\"start\":15328},{\"end\":16482,\"start\":16124},{\"end\":17010,\"start\":16515},{\"end\":17678,\"start\":17012},{\"end\":19334,\"start\":17680},{\"end\":20062,\"start\":19365},{\"end\":21182,\"start\":20098},{\"end\":21626,\"start\":21184},{\"end\":22014,\"start\":21628}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16199,\"start\":16192},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17666,\"start\":17660}]", "section_header": "[{\"end\":1715,\"start\":1700},{\"end\":6920,\"start\":6910},{\"end\":11494,\"start\":11478},{\"end\":11526,\"start\":11497},{\"end\":13483,\"start\":13459},{\"end\":14466,\"start\":14464},{\"end\":16513,\"start\":16485},{\"end\":19363,\"start\":19337},{\"end\":20096,\"start\":20065},{\"end\":22024,\"start\":22016},{\"end\":22103,\"start\":22095},{\"end\":22685,\"start\":22677},{\"end\":23643,\"start\":23636}]", "table": "[{\"end\":23634,\"start\":23451},{\"end\":24102,\"start\":23834}]", "figure_caption": "[{\"end\":22093,\"start\":22026},{\"end\":22675,\"start\":22105},{\"end\":22722,\"start\":22687},{\"end\":23017,\"start\":22725},{\"end\":23162,\"start\":23020},{\"end\":23451,\"start\":23165},{\"end\":23834,\"start\":23645}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7170,\"start\":7164},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7734,\"start\":7728},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9149,\"start\":9143},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9163,\"start\":9157},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14906,\"start\":14896},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15399,\"start\":15390},{\"end\":17676,\"start\":17667},{\"end\":18943,\"start\":18933},{\"end\":19926,\"start\":19920}]", "bib_author_first_name": "[{\"end\":24173,\"start\":24172},{\"end\":24181,\"start\":24180},{\"end\":24183,\"start\":24182},{\"end\":24194,\"start\":24193},{\"end\":24204,\"start\":24203},{\"end\":24213,\"start\":24212},{\"end\":24221,\"start\":24220},{\"end\":24558,\"start\":24557},{\"end\":24568,\"start\":24567},{\"end\":24577,\"start\":24576},{\"end\":24586,\"start\":24585},{\"end\":24597,\"start\":24596},{\"end\":24610,\"start\":24609},{\"end\":24622,\"start\":24621},{\"end\":24632,\"start\":24631},{\"end\":24640,\"start\":24639},{\"end\":25051,\"start\":25050},{\"end\":25060,\"start\":25059},{\"end\":25069,\"start\":25068},{\"end\":25077,\"start\":25076},{\"end\":25084,\"start\":25083},{\"end\":25092,\"start\":25091},{\"end\":25100,\"start\":25099},{\"end\":25107,\"start\":25106},{\"end\":25570,\"start\":25569},{\"end\":25578,\"start\":25577},{\"end\":25591,\"start\":25590},{\"end\":26052,\"start\":26051},{\"end\":26061,\"start\":26060},{\"end\":26070,\"start\":26069},{\"end\":26082,\"start\":26081},{\"end\":26623,\"start\":26622},{\"end\":26631,\"start\":26630},{\"end\":26637,\"start\":26636},{\"end\":26639,\"start\":26638},{\"end\":26650,\"start\":26649},{\"end\":26660,\"start\":26659},{\"end\":26669,\"start\":26668},{\"end\":26676,\"start\":26675},{\"end\":27050,\"start\":27049},{\"end\":27058,\"start\":27057},{\"end\":27064,\"start\":27063},{\"end\":27066,\"start\":27065},{\"end\":27077,\"start\":27076},{\"end\":27087,\"start\":27086},{\"end\":27396,\"start\":27395},{\"end\":27398,\"start\":27397},{\"end\":27408,\"start\":27407},{\"end\":27418,\"start\":27417},{\"end\":27420,\"start\":27419},{\"end\":27430,\"start\":27429},{\"end\":27438,\"start\":27437},{\"end\":27447,\"start\":27446},{\"end\":27790,\"start\":27789},{\"end\":27798,\"start\":27797},{\"end\":27804,\"start\":27803},{\"end\":27812,\"start\":27811},{\"end\":27821,\"start\":27820},{\"end\":27829,\"start\":27828},{\"end\":28198,\"start\":28197},{\"end\":28206,\"start\":28205},{\"end\":28214,\"start\":28213},{\"end\":28223,\"start\":28222},{\"end\":28231,\"start\":28230},{\"end\":28679,\"start\":28678},{\"end\":28691,\"start\":28690},{\"end\":28701,\"start\":28700},{\"end\":28711,\"start\":28710},{\"end\":28720,\"start\":28719},{\"end\":28730,\"start\":28729},{\"end\":28744,\"start\":28743},{\"end\":28754,\"start\":28753},{\"end\":28762,\"start\":28761},{\"end\":28771,\"start\":28770},{\"end\":29137,\"start\":29136},{\"end\":29145,\"start\":29144},{\"end\":29157,\"start\":29156},{\"end\":29166,\"start\":29165},{\"end\":29178,\"start\":29177},{\"end\":29190,\"start\":29189},{\"end\":29197,\"start\":29196},{\"end\":29207,\"start\":29206},{\"end\":29219,\"start\":29218},{\"end\":29226,\"start\":29225},{\"end\":29810,\"start\":29809},{\"end\":29818,\"start\":29817},{\"end\":29826,\"start\":29825},{\"end\":29834,\"start\":29833},{\"end\":29842,\"start\":29841},{\"end\":30164,\"start\":30163},{\"end\":30172,\"start\":30171},{\"end\":30174,\"start\":30173},{\"end\":30185,\"start\":30184},{\"end\":30196,\"start\":30195},{\"end\":30205,\"start\":30204},{\"end\":30214,\"start\":30213},{\"end\":30562,\"start\":30561},{\"end\":30570,\"start\":30569},{\"end\":30572,\"start\":30571},{\"end\":30583,\"start\":30582},{\"end\":30600,\"start\":30599},{\"end\":30610,\"start\":30609},{\"end\":30623,\"start\":30622},{\"end\":30640,\"start\":30639},{\"end\":30999,\"start\":30998},{\"end\":31007,\"start\":31006},{\"end\":31015,\"start\":31014},{\"end\":31021,\"start\":31020},{\"end\":31027,\"start\":31026},{\"end\":31033,\"start\":31032},{\"end\":31355,\"start\":31354},{\"end\":31362,\"start\":31361},{\"end\":31368,\"start\":31367},{\"end\":31377,\"start\":31376},{\"end\":31383,\"start\":31382},{\"end\":31392,\"start\":31391},{\"end\":31398,\"start\":31397},{\"end\":31757,\"start\":31756},{\"end\":31766,\"start\":31765},{\"end\":31776,\"start\":31775},{\"end\":32073,\"start\":32072},{\"end\":32081,\"start\":32080},{\"end\":32089,\"start\":32088},{\"end\":32097,\"start\":32096},{\"end\":32105,\"start\":32104},{\"end\":32387,\"start\":32386},{\"end\":32402,\"start\":32401},{\"end\":32413,\"start\":32412}]", "bib_author_last_name": "[{\"end\":24178,\"start\":24174},{\"end\":24191,\"start\":24184},{\"end\":24201,\"start\":24195},{\"end\":24210,\"start\":24205},{\"end\":24218,\"start\":24214},{\"end\":24226,\"start\":24222},{\"end\":24565,\"start\":24559},{\"end\":24574,\"start\":24569},{\"end\":24583,\"start\":24578},{\"end\":24594,\"start\":24587},{\"end\":24607,\"start\":24598},{\"end\":24619,\"start\":24611},{\"end\":24629,\"start\":24623},{\"end\":24637,\"start\":24633},{\"end\":24648,\"start\":24641},{\"end\":25057,\"start\":25052},{\"end\":25066,\"start\":25061},{\"end\":25074,\"start\":25070},{\"end\":25081,\"start\":25078},{\"end\":25089,\"start\":25085},{\"end\":25097,\"start\":25093},{\"end\":25104,\"start\":25101},{\"end\":25112,\"start\":25108},{\"end\":25575,\"start\":25571},{\"end\":25588,\"start\":25579},{\"end\":25599,\"start\":25592},{\"end\":26058,\"start\":26053},{\"end\":26067,\"start\":26062},{\"end\":26079,\"start\":26071},{\"end\":26089,\"start\":26083},{\"end\":26628,\"start\":26624},{\"end\":26634,\"start\":26632},{\"end\":26647,\"start\":26640},{\"end\":26657,\"start\":26651},{\"end\":26666,\"start\":26661},{\"end\":26673,\"start\":26670},{\"end\":26681,\"start\":26677},{\"end\":27055,\"start\":27051},{\"end\":27061,\"start\":27059},{\"end\":27074,\"start\":27067},{\"end\":27084,\"start\":27078},{\"end\":27092,\"start\":27088},{\"end\":27405,\"start\":27399},{\"end\":27415,\"start\":27409},{\"end\":27427,\"start\":27421},{\"end\":27435,\"start\":27431},{\"end\":27444,\"start\":27439},{\"end\":27456,\"start\":27448},{\"end\":27795,\"start\":27791},{\"end\":27801,\"start\":27799},{\"end\":27809,\"start\":27805},{\"end\":27818,\"start\":27813},{\"end\":27826,\"start\":27822},{\"end\":27842,\"start\":27830},{\"end\":28203,\"start\":28199},{\"end\":28211,\"start\":28207},{\"end\":28220,\"start\":28215},{\"end\":28228,\"start\":28224},{\"end\":28235,\"start\":28232},{\"end\":28688,\"start\":28680},{\"end\":28698,\"start\":28692},{\"end\":28708,\"start\":28702},{\"end\":28717,\"start\":28712},{\"end\":28727,\"start\":28721},{\"end\":28741,\"start\":28731},{\"end\":28751,\"start\":28745},{\"end\":28759,\"start\":28755},{\"end\":28768,\"start\":28763},{\"end\":28778,\"start\":28772},{\"end\":29142,\"start\":29138},{\"end\":29154,\"start\":29146},{\"end\":29163,\"start\":29158},{\"end\":29175,\"start\":29167},{\"end\":29187,\"start\":29179},{\"end\":29194,\"start\":29191},{\"end\":29204,\"start\":29198},{\"end\":29216,\"start\":29208},{\"end\":29223,\"start\":29220},{\"end\":29232,\"start\":29227},{\"end\":29238,\"start\":29234},{\"end\":29815,\"start\":29811},{\"end\":29823,\"start\":29819},{\"end\":29831,\"start\":29827},{\"end\":29839,\"start\":29835},{\"end\":29847,\"start\":29843},{\"end\":30169,\"start\":30165},{\"end\":30182,\"start\":30175},{\"end\":30193,\"start\":30186},{\"end\":30202,\"start\":30197},{\"end\":30211,\"start\":30206},{\"end\":30219,\"start\":30215},{\"end\":30567,\"start\":30563},{\"end\":30580,\"start\":30573},{\"end\":30597,\"start\":30584},{\"end\":30607,\"start\":30601},{\"end\":30620,\"start\":30611},{\"end\":30637,\"start\":30624},{\"end\":30650,\"start\":30641},{\"end\":31004,\"start\":31000},{\"end\":31012,\"start\":31008},{\"end\":31018,\"start\":31016},{\"end\":31024,\"start\":31022},{\"end\":31030,\"start\":31028},{\"end\":31038,\"start\":31034},{\"end\":31359,\"start\":31356},{\"end\":31365,\"start\":31363},{\"end\":31374,\"start\":31369},{\"end\":31380,\"start\":31378},{\"end\":31389,\"start\":31384},{\"end\":31395,\"start\":31393},{\"end\":31402,\"start\":31399},{\"end\":31763,\"start\":31758},{\"end\":31773,\"start\":31767},{\"end\":31783,\"start\":31777},{\"end\":32078,\"start\":32074},{\"end\":32086,\"start\":32082},{\"end\":32094,\"start\":32090},{\"end\":32102,\"start\":32098},{\"end\":32110,\"start\":32106},{\"end\":32399,\"start\":32388},{\"end\":32410,\"start\":32403},{\"end\":32418,\"start\":32414}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53024398},\"end\":24492,\"start\":24104},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":502946},\"end\":25000,\"start\":24494},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3943983},\"end\":25511,\"start\":25002},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1629541},\"end\":25941,\"start\":25513},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":84186897},\"end\":26503,\"start\":25943},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":201810148},\"end\":27000,\"start\":26505},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":203134135},\"end\":27317,\"start\":27002},{\"attributes\":{\"doi\":\"arXiv:1906.00208\",\"id\":\"b7\"},\"end\":27679,\"start\":27319},{\"attributes\":{\"doi\":\"arXiv:1909.07721\",\"id\":\"b8\"},\"end\":28091,\"start\":27681},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57833031},\"end\":28598,\"start\":28093},{\"attributes\":{\"doi\":\"arXiv:1905.01489\",\"id\":\"b10\"},\"end\":29042,\"start\":28600},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":29166647},\"end\":29728,\"start\":29044},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":21057270},\"end\":30104,\"start\":29730},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53029837},\"end\":30477,\"start\":30106},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":59339219},\"end\":30894,\"start\":30479},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":35903402},\"end\":31317,\"start\":30896},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4028864},\"end\":31713,\"start\":31319},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":59159230},\"end\":31985,\"start\":31715},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":199579181},\"end\":32319,\"start\":31987},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3719281},\"end\":32716,\"start\":32321}]", "bib_title": "[{\"end\":24170,\"start\":24104},{\"end\":24555,\"start\":24494},{\"end\":25048,\"start\":25002},{\"end\":25567,\"start\":25513},{\"end\":26049,\"start\":25943},{\"end\":26620,\"start\":26505},{\"end\":27047,\"start\":27002},{\"end\":28195,\"start\":28093},{\"end\":29134,\"start\":29044},{\"end\":29807,\"start\":29730},{\"end\":30161,\"start\":30106},{\"end\":30559,\"start\":30479},{\"end\":30996,\"start\":30896},{\"end\":31352,\"start\":31319},{\"end\":31754,\"start\":31715},{\"end\":32070,\"start\":31987},{\"end\":32384,\"start\":32321}]", "bib_author": "[{\"end\":24180,\"start\":24172},{\"end\":24193,\"start\":24180},{\"end\":24203,\"start\":24193},{\"end\":24212,\"start\":24203},{\"end\":24220,\"start\":24212},{\"end\":24228,\"start\":24220},{\"end\":24567,\"start\":24557},{\"end\":24576,\"start\":24567},{\"end\":24585,\"start\":24576},{\"end\":24596,\"start\":24585},{\"end\":24609,\"start\":24596},{\"end\":24621,\"start\":24609},{\"end\":24631,\"start\":24621},{\"end\":24639,\"start\":24631},{\"end\":24650,\"start\":24639},{\"end\":25059,\"start\":25050},{\"end\":25068,\"start\":25059},{\"end\":25076,\"start\":25068},{\"end\":25083,\"start\":25076},{\"end\":25091,\"start\":25083},{\"end\":25099,\"start\":25091},{\"end\":25106,\"start\":25099},{\"end\":25114,\"start\":25106},{\"end\":25577,\"start\":25569},{\"end\":25590,\"start\":25577},{\"end\":25601,\"start\":25590},{\"end\":26060,\"start\":26051},{\"end\":26069,\"start\":26060},{\"end\":26081,\"start\":26069},{\"end\":26091,\"start\":26081},{\"end\":26630,\"start\":26622},{\"end\":26636,\"start\":26630},{\"end\":26649,\"start\":26636},{\"end\":26659,\"start\":26649},{\"end\":26668,\"start\":26659},{\"end\":26675,\"start\":26668},{\"end\":26683,\"start\":26675},{\"end\":27057,\"start\":27049},{\"end\":27063,\"start\":27057},{\"end\":27076,\"start\":27063},{\"end\":27086,\"start\":27076},{\"end\":27094,\"start\":27086},{\"end\":27407,\"start\":27395},{\"end\":27417,\"start\":27407},{\"end\":27429,\"start\":27417},{\"end\":27437,\"start\":27429},{\"end\":27446,\"start\":27437},{\"end\":27458,\"start\":27446},{\"end\":27797,\"start\":27789},{\"end\":27803,\"start\":27797},{\"end\":27811,\"start\":27803},{\"end\":27820,\"start\":27811},{\"end\":27828,\"start\":27820},{\"end\":27844,\"start\":27828},{\"end\":28205,\"start\":28197},{\"end\":28213,\"start\":28205},{\"end\":28222,\"start\":28213},{\"end\":28230,\"start\":28222},{\"end\":28237,\"start\":28230},{\"end\":28690,\"start\":28678},{\"end\":28700,\"start\":28690},{\"end\":28710,\"start\":28700},{\"end\":28719,\"start\":28710},{\"end\":28729,\"start\":28719},{\"end\":28743,\"start\":28729},{\"end\":28753,\"start\":28743},{\"end\":28761,\"start\":28753},{\"end\":28770,\"start\":28761},{\"end\":28780,\"start\":28770},{\"end\":29144,\"start\":29136},{\"end\":29156,\"start\":29144},{\"end\":29165,\"start\":29156},{\"end\":29177,\"start\":29165},{\"end\":29189,\"start\":29177},{\"end\":29196,\"start\":29189},{\"end\":29206,\"start\":29196},{\"end\":29218,\"start\":29206},{\"end\":29225,\"start\":29218},{\"end\":29234,\"start\":29225},{\"end\":29240,\"start\":29234},{\"end\":29817,\"start\":29809},{\"end\":29825,\"start\":29817},{\"end\":29833,\"start\":29825},{\"end\":29841,\"start\":29833},{\"end\":29849,\"start\":29841},{\"end\":30171,\"start\":30163},{\"end\":30184,\"start\":30171},{\"end\":30195,\"start\":30184},{\"end\":30204,\"start\":30195},{\"end\":30213,\"start\":30204},{\"end\":30221,\"start\":30213},{\"end\":30569,\"start\":30561},{\"end\":30582,\"start\":30569},{\"end\":30599,\"start\":30582},{\"end\":30609,\"start\":30599},{\"end\":30622,\"start\":30609},{\"end\":30639,\"start\":30622},{\"end\":30652,\"start\":30639},{\"end\":31006,\"start\":30998},{\"end\":31014,\"start\":31006},{\"end\":31020,\"start\":31014},{\"end\":31026,\"start\":31020},{\"end\":31032,\"start\":31026},{\"end\":31040,\"start\":31032},{\"end\":31361,\"start\":31354},{\"end\":31367,\"start\":31361},{\"end\":31376,\"start\":31367},{\"end\":31382,\"start\":31376},{\"end\":31391,\"start\":31382},{\"end\":31397,\"start\":31391},{\"end\":31404,\"start\":31397},{\"end\":31765,\"start\":31756},{\"end\":31775,\"start\":31765},{\"end\":31785,\"start\":31775},{\"end\":32080,\"start\":32072},{\"end\":32088,\"start\":32080},{\"end\":32096,\"start\":32088},{\"end\":32104,\"start\":32096},{\"end\":32112,\"start\":32104},{\"end\":32401,\"start\":32386},{\"end\":32412,\"start\":32401},{\"end\":32420,\"start\":32412}]", "bib_venue": "[{\"end\":24273,\"start\":24228},{\"end\":24720,\"start\":24650},{\"end\":25201,\"start\":25114},{\"end\":25678,\"start\":25601},{\"end\":26168,\"start\":26091},{\"end\":26728,\"start\":26683},{\"end\":27149,\"start\":27094},{\"end\":27393,\"start\":27319},{\"end\":27787,\"start\":27681},{\"end\":28292,\"start\":28237},{\"end\":28676,\"start\":28600},{\"end\":29327,\"start\":29240},{\"end\":29894,\"start\":29849},{\"end\":30266,\"start\":30221},{\"end\":30659,\"start\":30652},{\"end\":31095,\"start\":31040},{\"end\":31471,\"start\":31404},{\"end\":31823,\"start\":31785},{\"end\":32143,\"start\":32112},{\"end\":32506,\"start\":32420},{\"end\":25275,\"start\":25203},{\"end\":25742,\"start\":25680},{\"end\":26232,\"start\":26170},{\"end\":29401,\"start\":29329},{\"end\":31525,\"start\":31473}]"}}}, "year": 2023, "month": 12, "day": 17}