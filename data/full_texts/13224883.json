{"id": 13224883, "updated": "2023-07-19 05:53:32.202", "metadata": {"title": "Online controlled experiments at large scale", "authors": "[{\"first\":\"Ron\",\"last\":\"Kohavi\",\"middle\":[]},{\"first\":\"Alex\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Brian\",\"last\":\"Frasca\",\"middle\":[]},{\"first\":\"Toby\",\"last\":\"Walker\",\"middle\":[]},{\"first\":\"Ya\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Nils\",\"last\":\"Pohlmann\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining", "publication_date": {"year": 2013, "month": null, "day": null}, "abstract": "Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2112508839", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/kdd/KohaviDFWXP13", "doi": "10.1145/2487575.2488217"}}, "content": {"source": {"pdf_hash": "08f7125bc2dd4ce529a2277ded9bc018fe120329", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://www.exp-platform.com/Documents/2013%20controlledExperimentsAtScale.pdf", "status": "GREEN"}}, "grobid": {"id": "d393706c5b27d4fdb9125d1c2353d3a2e20fc3b9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/08f7125bc2dd4ce529a2277ded9bc018fe120329.txt", "contents": "\nOnline Controlled Experiments at Large Scale\n\n\nRon Kohavi ronnyk@microsoft.com \nMicrosoft\nOne Microsoft Way98052RedmondWA\n\nAlex Deng alexdeng@microsoft.com \nMicrosoft\nOne Microsoft Way98052RedmondWA\n\nBrian Frasca brianfra@microsoft.com \nMicrosoft\nOne Microsoft Way98052RedmondWA\n\nToby Walker towalker@microsoft.com \nMicrosoft\nOne Microsoft Way98052RedmondWA\n\nYa Xu yaxu@microsoft.com \nMicrosoft\nOne Microsoft Way98052RedmondWA\n\nNils Pohlmann nilsp@microsoft.com \nMicrosoft\nOne Microsoft Way98052RedmondWA\n\nOnline Controlled Experiments at Large Scale\nG3 Probability and Statistics/Experimental Design: controlled experimentsrandomized experimentsA/B testing Keywords Controlled experimentsA/B testingsearchonline experiments\nWeb-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated apriori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.\n\nINTRODUCTION\n\nMany web-facing companies use online controlled experiments to guide product development and prioritize ideas, including Amazon [1], eBay, Etsy [2], Facebook, Google [3], Groupon, Intuit [4], LinkedIn, Microsoft [5], Netflix [6], Shop Direct [7], StumbleUpon [8], Yahoo, and Zynga [9]. Controlled experiments are especially useful in combination with agile development, Steve Blank's Customer Development process [10], and MVPs (Minimum Viable Products) popularized by Eric Ries's Lean Startup [11]. In a \"Lean Startup\" approach, \"businesses rely on validated learning, scientific experimentation, and iterative product releases to shorten product development cycles, measure progress, and gain valuable customer feedback\" [12].\n\nLarge scale can have multiple dimensions, including the number of users and the number of experiments. We are dealing with Big Data and must scale on both dimensions: each experiment typically exposes several million users to a treatment, and over 200 experiments are running concurrently. While running online controlled experiments requires a sufficient number of users, teams working on products with thousands to tens of thousands of users (our general guidance is at least thousands of active users) are typically looking for larger effects, which are easier to detect than the small effects that large sites worry about. For example, to increase the experiment sensitivity (detectable effect size) by a factor of 10, say from 5% delta to 0.5%, you need 10 100 times more users. Controlled experiments thus naturally scale from small startups to the largest of web sites. Our focus in this paper is on scaling the number of experiments: how can organizations evaluate more hypotheses, increasing the velocity of validated learnings [11], per time unit.\n\nWe share our experiences, how we addressed challenges, and key lessons from having run thousands of online controlled experiments at Bing, part of Microsoft's Online Services Division. Microsoft's different divisions use different development methodologies. Office and Windows follow Sinofsky's long planning and execution cycles [13]. Bing has thousands of developers, program managers, and testers, using online controlled experiments heavily to prioritize ideas and decide which changes to ship to all users. Bing's Experimentation System is one of the largest in the world, and pushes the envelope on multiple axes, including culture, engineering, and trustworthiness. In the US alone, it distributes traffic from about 100 million monthly users executing over 3.2B queries a month [14] to over 200 experiments running concurrently. Almost every user is in some experiment: 90% of users eligible for experimentation (e.g., browser supports cookies) are each rotated into over 15 concurrent experiments, while 10% are put into a holdout group to assess the overall impact of the Experimentation System and to help with alerting.\n\nAnalysis of an experiment utilizing 20% of eligible users (10% control, 10% treatment) over 2 weeks processes about 4TB of data to generate a summary scorecard. With about 5 experiments in each one of 15 concurrent experimentation areas (conservative numbers), users end up in one of 5 30 billion possible variants of Bing. Automated analyses, or scorecards, are generated on clusters consisting of tens of thousands of machines [15] to help guide product releases, to shorten product development cycles, measure progress, and gain valuable customer feedback. Alerts fire automatically when experiments hurt the user experience, or interact with other experiments. While the overall system has significant costs associated with it, its value Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.. KDD '13, August 11-14, 2013, Chicago, Illinois, USA.\n\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\n\nACM 978-1-4503-2174-7/13/08\u2026$15.00. far outweighs those costs: ideas that were implemented by small teams, and were not even prioritized high by the team implementing them, have had surprisingly large effects on key metrics. For example, two small changes, which took days to develop, each increased ad revenue by about $100 million annually [16].\n\n\nMotivating Example\n\nWe begin with a motivating visual example of a controlled experiment that ran at Bing. The team wanted to add a feature allowing advertisers to provide links to the target site. The rationale is that this will improve ads quality by giving users more information about what the advertiser's site provides and allow users to directly navigate to the sub-category matching their intent. Visuals of the existing ads layout (Control) and the new ads layout (Treatment) with site links added are shown in Figure 1 below.\n\n\nFigure 1: Ads with site link experiment. Treatment (bottom) has site links. The difference might not be obvious at first but it is worth tens of millions of dollars\n\nIn a controlled experiment, users are randomly split between the variants (e.g., the two different ads layouts) in a persistent manner (a user receives the same experience in multiple visits). Their interactions with the site are instrumented and key metrics computed. In this experiment, the Overall Evaluation Criterion (OEC) was simple: increasing average revenue per user without degrading key user engagement metrics. Results showed that the newly added site links increased revenue, but also degraded user metrics and Page-Load-Time, likely because of increased vertical space usage. Even offsetting the space by lowering the average number of mainline ads shown per query, this feature improved revenue by tens of millions of dollars per year with neutral user impact, resulting in extremely high ROI (Return-On-Investment).\n\nWhile the example above is a visual change for monetization, we use controlled experiments for many areas at Bing. Visual changes range from small tweaks like changing colors, to improving search result captions, to bigger changes like adding video to the homepage, and to a complete makeover of Bing's search result page that rolled out in May 2012 and included a new social pane. We also test usability improvements, such as query auto-suggest, \"Did you mean,\" and search history. Backend changes such as relevance rankers, ad optimization, and performance improvements are constantly being experimented with. Finally, we also experiment with changes to sites generating traffic to Bing, such as MSN.\n\n\nThe Experimentation System\n\nThe problem that the Bing Experimentation System addresses is how to guide product development and allow the organization to assess the ROI of projects, leading to a healthy focus on key ideas that move metrics of interest. While there are many ways to design and evaluate products, our choice of controlled experiments for Knowledge Discovery derives from the desire to reliably identify causality with high precision (which features cause changes in customer behavior). In the hierarchy of possible designs, controlled experiments are the gold standard in science [17].\n\nIf an organization wants to make data-driven decisions to drive product development, with customers' actual behavior as the source of data for decisions, one of the key goals is to enable experimentation at scale: support running many experiments and lower the cost of experimentation. This must be done without lowering the trustworthiness of the overall system.\n\nWith the mission of accelerating software innovation through trustworthy experimentation, the use of experimentation at Bing grew exponentially fast over time, as shown in Figure 2.\n\nThe Bing Experimentation System is one of the largest systems in the world for running online controlled experiments, with over 200 experiments running concurrently, exposing about 100 million active monthly customers to billions of Bing variants that include implementations of new ideas and variations of existing ones.\n\n\nRelated Work and Contributions\n\nMultiple papers and books have been written on how to run an online controlled experiment [18; 7; 19; 20] and we will not address that here; we follow the terminology of Controlled experiments on the web: survey and practical guide [18]. We build upon that work and share how to scale experimentation, i.e., how to run many experiments to accelerate innovation in product development. We are aware of only one paper that focused on this aspect of experiment scale, an excellent paper by Diane Tang et al. about overlapping experiments at Google [3]. Because that topic is well covered in that paper, and Bing's system is similar [21 pp. 33-34], we chose not to discuss it here. To the best of our knowledge, most of the lessons we share here are novel and not previously covered. Our contributions are as follows:\n\n1. We share key tenets, or principles, which an organization should adopt before using online controlled experiments at scale. Experimentation is not a panacea for everyone, and the assumptions should be understood. 2. We discuss cultural and organizational issues, including two topics not commonly discussed: the cost/benefits of running controlled experiments, and running negative experiments. 3. We discuss engineering challenges, including the system architecture, and alerting, a necessary ingredient when running experiments at scale. We share the results of our study on the impact of the Experimentation System itself. Experiments per day 4. We discuss trustworthiness and statistical challenges above and beyond those usually mentioned as pitfalls for running a single online controlled experiment [22; 23]. In particular, addressing false positives, and both preventing and detecting pairwise interactions.\n\nThe lessons we share apply to a wide gamut of companies. Running experiments at large scale does not require a large web site or service: startups have utilized controlled experiments when they have had thousands of active users and are typically looking for large effects. In fact, establishing the experimentation culture early can help startups make the right critical decisions and develop a customer-focused development organization that accelerates innovation [10; 11].\n\nUse of the Bing Experimentation System grew so much because it is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars. It allowed us to find and focus on key ideas evaluated through thousands of controlled experiments. The system also helped us identify many negative features that we avoided deploying, despite early excitement by key stakeholders, saving us similar large amounts.\n\n\nTENETS\n\nRunning online controlled experiments is not applicable for every organization. We begin with key tenets, or assumptions, an organization needs to adopt.\n\n\nTenet 1: The Organization wants to make data-driven decisions and has formalized the Overall Evaluation Criterion (OEC)\n\nYou will rarely hear someone at the head of an organization say that they don't want to be data-driven (a notable exception is Apple under Steve Jobs, where Ken Segall claimed that \"we didn't test a single ad. Not for print, TV, billboards, the web, retail, or anything\" [24 p. 42]). But measuring the incremental benefit to users from new features has costs, and objective measurements typically show that progress is not as rosy as initially envisioned. Many organizations will therefore not spend the resources required to define and measure progress. It is often easier to generate a plan, execute against it, and declare success, with the key metric being: \"percent of plan delivered,\" ignoring whether the feature has any positive impact to key metrics.\n\nIn this paper, we assume that the OEC, or Overall Evaluation Criterion, has been defined and can be measured over relatively short durations (e.g., two weeks). In large organizations, it is possible to have multiple OECs, or several key metrics that are shared with refinements for different areas. The hard part is finding metrics that are measurable in the short-term that are predictive of long-term goals. For example, \"Profit\" is not a good OEC, as short-term theatrics (e.g., raising prices) can increase short-term profit, but hurt it in the long run. As we showed in Trustworthy Online Controlled Experiments: Five Puzzling Outcomes Explained [25], market share can be a long-term goal, but it is a terrible short-term criterion: making a search engine worse forces people to issue more queries to find an answer, but, like hiking prices, users will find better alternatives long-term. Sessions per user, or repeat visits, is a much better factor in the OEC, and one that we use at Bing. Thinking of the drivers of lifetime value can lead to a strategically powerful OEC [18]. We cannot overemphasize the importance of coming up with a good OEC that the organization can align behind, but for this paper we will assume this has been done.  [5], the necessary ingredients for running controlled experiments were reviewed. The key point is that for customerfacing web sites, changes are easy to make through software, and running controlled experiments is relatively easy.\n\nAssuming you can run controlled experiments, it is important to ensure their trustworthiness. When running online experiments, getting numbers is easy; getting numbers you can trust is hard, and we have had our share of pitfalls and puzzling results [23; 25; 26]. For this paper, we assume the organization does the checks for correctness and is aware of the pitfalls.\n\n\nTenet 3: We are poor at assessing the value of ideas\n\nFeatures are built because teams believe they are useful, yet in many domains most ideas fail to improve key metrics. Only one third of the ideas tested at Microsoft improved the metric(s) they were designed to improve [5]. Success is even harder to find in well-optimized domains like Bing. Jim Manzi [17] \n\n\nCULTURAL AND ORGANIZATIONAL LESSONS\n\nWe now discuss areas related to cultural and organizational aspects.\n\n\nWhy Controlled Experiments?\n\nThe most common question we get as an organization learns about controlled experiments is \"why not measure the metric of interest, ship the feature, and then look at the delta?\" Alternatively, look at correlations with metrics of interest.\n\nOur experience is that external variations overwhelm the effects we are trying to detect [23]. In sequential tests, or quasiexperimental designs, we try to control for known confounding factors, but this is extremely hard to get right. As the common proverb goes: correlation is not causation. \n\n\nCost vs. Benefit and the Ideas Funnel\n\nThere are many methods that can be used to evaluate new ideas, including pitching the ideas to others; reviewing sketches, mockups, and prototypes; conducting surveys and usability lab studies; tests against historical data; and running controlled experiments. These evaluation methods vary both in the cost to execute them as well as the value and reliability of the information gained through them. In How to Measure Anything: Finding the Value of Intangibles in Business [38], Doug Hubbard used the term EVI, Expected Value of Information, to define the expected benefit gained by getting additional information. A controlled experiment provides very close to perfect information (up to the uncertainty from the p-value and other experimental design factors), but it can be more expensive than other methods of evaluating new ideas.\n\nOrganizations should consider a large number of initial ideas and have an efficient and reliable mechanism to narrow them down to a much smaller number of ideas that are ultimately implemented and released to users in online controlled experiments. For this funnel of ideas to be efficient, low cost methods such as pitching ideas and reviewing mockups are needed to evaluate and narrow down the large number of ideas at the top of the funnel. Controlled experiments are typically not suitable to evaluate ideas at the top of the funnel because they require each idea to be implemented sufficiently well to deploy and run on real users, and this feature development cost can be high. Hence, at the top of the funnel more ideas are evaluated using low-cost techniques, but with lower fidelity. Conversely, at the bottom of the funnel there are fewer ideas to evaluate and the organization should use more reliable methods to evaluate them, with controlled experiments being the most reliable and preferred method.\n\nA key observation is that if a controlled experiment is cheap to run, then other evaluation methods rarely make sense. For example, some ideas are easy to code and deploy; other involve changing configuration parameters. One reason for using other methods in these cases is to gain qualitative feedback (e.g., through surveys and usability lab studies); however these other methods should be used to complement controlled experiments and not to replace them since the quantitative information they provide is inferior.\n\n\nTest Everything in Experiments\n\nIn Code refactoring and bug fixes present an interesting tradeoff. For a large organization, there are many small fixes that go in every day, and it would be unreasonable to run controlled experiments for each one. We recommend that small fixes get bundled into packages so that if one is egregiously bad, the package will test negatively and it will be identified. Building the infrastructure to do this cheaply and efficiently is the real challenge.\n\nThe key is to admit that mistakes will happen and try to run controlled experiments to detect them. A bug that introduces a 1% reduction to revenue costs Bing over $10M per year in the US alone. Detecting the 1% is easy in a controlled experiment; it is much harder based on sequential patterns.\n\n\nNegative Experiments\n\nThe question of whether we should run controlled experiments that knowingly degrade the user experience (e.g., slowing performance) is highly polarizing. Some people believe we should never knowingly degrade the user experience. Over time, we achieved agreement that knowingly hurting users in the short-term (e.g., a 2-week experiment) can let us understand fundamental issues and thereby improve the experience long-term. We believe that this is not only justified, but should be encouraged. The Hippocratic Oath is often associated with the phrase \"Do no harm\" (although not precisely phrased that way), yet there is strong evidence that doctors have been harming patients for millennia. In Bad Medicine: Doctors Doing Harm Since Hippocrates, David Wootton [39] wrote that \"For 2,400 years patients have believed that doctors were doing them good; for 2,300 years they were wrong.\" Doctors did bloodletting for hundreds of years, thinking it had a positive effect, not realizing that the calming effect was a side effect that was unrelated to the disease itself. When President George Washington was sick, doctors extracted about 35%-50% of his blood over a short period, which inevitably led to preterminal anemia, hypovolemia, and hypotension. The fact that he stopped struggling and appeared physically calm shortly before his death was probably due to profound hypotension and shock. Running control experiments on changes that we believe are \"negative\" to confirm the causal effect is critical so that we do not make the same mistake doctors did for centuries. Even if the HiPPO (Highest Paid Person's Opinion) in your organization is strongly held, we recommend validating it. Hippocrates' \"Do no harm\" should really be \"Do no long-term harm.\" Understanding the impact of performance (speed) on key metrics is a fundamental question. There is an interest in isolating the performance and answering: excluding the impact due to performance (typically degradations), did my feature improve some key metrics? Initial implementations are often slow and if one is building a Minimum-Viable-Product (MVP) to test an idea, it is best not to start optimizing performance before validating that the idea itself is good. Quantifying the relationship between changes in performance and changes to key metrics is highly beneficial to the organization.\n\nThis quantification may change over time, as the site's performance and bandwidth standards improve. Bing's server performance is now sub-second at the 95 th percentile. Past experiments showed that performance matters, but is it still the case? We recently ran a slowdown experiment where we slowed 10% of users by 100msec (milliseconds) and another 10% by 250msec for two weeks. The results showed that performance absolutely matters a lot today: every 100msec improves revenue by 0.6%. The following phrasing resonated extremely well in our organization (based on translating the above to profit): an engineer that improves server performance by 10msec (that's 1/30 of the speed that our eyes blink) more than pays for his fullyloaded annual costs. Every millisecond counts.\n\n\nBeware of Twyman's Law and Stories in the Wild\n\nTwyman wrote that \"Any figure that looks interesting or different is usually wrong.\" We recommend healthy skepticism towards stories depicting astounding results from tiny changes, such as 50% revenue lift due to changing the color of the Buy Button. While we have some unexpected successes from small changes, they are extremely rare. Most amazing results turn out to be false when reviewed carefully [23; 25], so they need to be replicated with high statistical power and deeply analyzed before we believe them.\n\nSome sites, such as http://whichtestwon.com, share the test of the week. Our experience is that there are good ideas and hypotheses that are worth evaluating, but Ioannidis' warnings [31] apply well here: we suspect many results are phrased too strongly or are incorrect. Multiple-testing, bias, and weak standards lower the trust one should have in these results (e.g., the test at http://whichtestwon.com/whichtestwons-overlay-timer-test was published based on a non-stat-sig p-value, >0.05). Our recommendation is classical science: replication. If you find a great hypothesis, retest it on your site.\n\nWe want to share one example where we found a result contradicting ours. In Section 3.4, we mentioned that performance matters a lot; Greg Linden [40 p. 15] noted that 100msec slowdown at Amazon impacted revenue by 1%; a paper by coauthors from Bing and Google [41] showed the significant impact of performance on key metrics. With so much evidence, we were surprised to see Etsy's Dan McKinley [2] claim that a 200msec delay did not matter. It is possible that for Etsy users, performance is not critical, but we believe a more likely hypothesis is that the experiment did not have sufficient statistical power to detect the differences. Clearly if you increase the slowdown it will matter at some point: at 5 minutes, there will be close to zero engagement, so where on the continuum can Etsy detect the impact? 500msec? One second? Telling an organization that performance doesn't matter will make the site slower very quickly, to the point where users will abandon in droves. We believe Etsy should either increase statistical power, or increase the delay until they are able to get a statistically significant signal, and they might be surprised by the impact on their key metrics.\n\n\nInnovation vs. Incrementalism: 41 Shades of Blue\n\nAs experimentation becomes \"low cost,\" it is easy to fall into the trap of answering many trivial questions by running controlled experiments. This is well exemplified in Douglas Bowman's blog [42], describing how a team at Google that couldn't agree on a blue color for a link experimented with 41 shades of blue. While such variations could be important in some cases, many make no difference and may discourage thoughtful designs.\n\nExperimentation is a tool, and we agree that it can support a quick \"try, evaluate, ship\" cycle that provides the illusion of progress if the steps are tiny: you don't get to the moon by climbing higher and higher trees. Conversely, we have seen big bets that could have been declared a big success by the HiPPO, were it not for the fact that controlled experiments provided objective judgment that key metrics did not really move. As with any funnel of ideas, one must evaluate the total benefit of several small incremental bets vs. some big bold risky bets. As with stocks, an organization is usually better with a portfolio of ideas at different points on the risk/reward curve.\n\nSometimes an organization has to take a big leap in the space of options and start to hill-climb in a new area in order to see if it is near a taller mountain. The initial jump might end up lower than the current local maxima, and it may take time to explore the new area. As the initial explorations fail to beat the current champion, the question of \"fail fast\" vs. \"persevere\" always comes up. There is no magic bullet here: it is about running some experiments to get a sense of the \"terrain\" and being open to both options.\n\n\nMultivariate Tests\n\nMultivariate Tests (MVT) evaluate the impact of multiple variables that could interact, and are the subject of a rich statistical literature [20] and many buzzword-compliant brochures of product vendors. We have previously made the case that in the online world, agility and continuous availability of users makes MVTs less appealing [18]. Researchers at Google made similar observations [3]. Despite the massive growth in experimentation, we continue to believe that the current orthogonal design (equivalent to a full-factorial) is the most appropriate. In our experience, interactions are relatively rare and more often represent bugs than true statistical interactions (also see Section 5.2). When we do suspect interactions, or when they are detected, we run small MVTs, but these are relatively rare.\n\n\nENGINEERING LESSONS\n\nAs the Bing organization has embraced controlled experiments for decision making, there is a continuing need to scale the platform for running experiments while lowering the per-experiment costs, and keeping the trust level high by making it hard for the uninformed to make mistakes. Key to this scaling is an investment in self-service tools for creating, managing and analyzing experiments. These guided tools enable all engineers in the organization to run their own experiments and act on the outcomes. While a small centralized team creates these tools and provides guidance and oversight to encourage high quality experimentation, the decision making over what experiments to run and how to incorporate the feedback is largely decentralized. Different aspects of the system are monitored and tuned to address scaling challenges, especially the use of limited resources, such as users to allocate to tests and machines for analysis.\n\n\nArchitecture\n\nBing's experiment system architecture is outlined in Figure 3, and covers four key areas. For this section, we use Bing's terminology. A flight is a variant that a user is exposed to. A Number Line is an orthogonal assignment, similar to Google's layers [3]. This mechanism provides guaranteed isolation from conflicting assignment: a user will be in only one flight per number line. A user is assigned to multiple concurrent flights, one per number line. The four key areas of the architecture are:\n\n1. Online Infrastructure. As a request is received from a browser, Bing's frontend servers assign each request to multiple flights running on a set of number lines. To ensure the assignment is consistent, a pseudo random hash of an anonymous user id is used [18]. The assignment happens as soon as the request is received and the frontend then passes each request's flight assignments as part of the requests sent to lower layers of the system. All systems in Bing are driven from configuration and an experiment is implemented as a change to the default configuration for one or more components. Each layer in the system logs information, including the request's flight assignments, to system logs that are then processed and used for offline analysis. 2. Experiment Management. Experimenters use a system, called Control Tower, to manage their experiments. To support greater automation and scale, all tools, including Control Tower, run on top of APIs for defining and executing experiments and experiment analysis. Some groups build on these APIs to automate experiment execution (e.g. for large scale automated experimentation). A configuration API and tool enables experimenters to easily create the setting defining an experiment. 3. Offline Analysis. An experiment summary is called a scorecard, and is generated by an offline experiment analysis pipeline that must manage a large scale analysis workloadboth in data and volume of experiments. Using the logs, the system manages and optimizes the execution of multiple analysis workflows used for experiment scorecards, monitoring and alerting, as well as deep dive analytics. The scorecards enable simple slicing and dicing as well as viewing the changing impact of an experiment over time. An alerting and monitoring system automatically detects both data quality and adverse user impact events, as described in Section 4.3.\n\n\nImpact of the Experimentation System\n\nAlthough experimentation is critical for data driven product innovation, it does not come without cost. To the best of our knowledge, these costs have never been documented in detail. In this section we describe how we evaluated the impact of the experimentation system itself, including the average impact from live experiments over several months.\n\nAs discussed in Section 4.1, the Experimentation System affects all layers of the system and has a performance impact at each layer. First, the experiment assignment adds a small delay (less than a millisecond). Second, increasing the number of experiments assigned to each request results in increasing cache fragmentation, lowering cache hit rates and increasing latency. Bing caches the first n results for common queries, but treatments cannot share a cache entry if they return different results for the same request. As the number of concurrent experiments that influence search results increases, fragmentation increases exponentially. For example, our ranker has four layers, and if three treatments (+ 1 control) are running concurrently (on different number lines), we fragment the cache by a factor of 4^4 = 256! Finally, new features are typically less performanceoptimized in early incarnations.\n\n\nFigure 3: Experimentation System Architecture\n\nTo quantify the impact of Bing's Experimentation System, we holdout 10% of our total users from any experimentation. This holdout group serves as a \"top-level control\" while the rest of the users are in experimental treatments. In short, the problem of understanding the impact of Experimentation System itself becomes another A/B test (we ignore the assignment to the holdout group, as it is extremely fast). We monitor key metrics continuously, and take action if we find the impact exceeds a prescribed budget level.\n\nWe quantified the impact of the Experimentation System on multiple metrics internally, and we share one here: speed, or pageload-time. In Bing, a key performance metric is Page-Load-Time, which is defined as the time from the user's query to the browser's firing of the onload event on the resulting page. The experiment group consistently showed a 25msec to 30msec delay. A separate study for the impact of web cache shows the web cache fragmentation alone contributes about 20msec. It is clear that by doing experimentation, there is a cost of learning and we believe being able to quantify the cost is important for any organization that runs experiments at large scale.\n\n\nAlerts and Aborting Bad Experiments\n\nAny change has the potential to degrade the user experience, and even a small degradation can increase user abandonment or cost millions of dollars if not caught quickly. As the organization grows, and number and frequency of feature changes increases, so does the need to automatically detect and alert when degradations occur and to automatically revert any changes that cause severe degradations. This use of controlled experiments provides a critical safety net which enables a company to scale the number of ideas tested and changes made while still maintaining a tolerable level of risk.\n\nThe na\u00efve approach to alerting on any statistically significant negative metric changes will lead to an unacceptable number of false alerts and thus make the entire alerting system useless. To avoid this we employ multiple techniques:\n\n1. Before raising an alert, we require that a detected delta is not only statistically significant but also large enough in absolute magnitude to have meaningful user or business impact. For example, we do not alert on a 2 millisecond degradation to Page-Load-Time, even if we have very high confidence the degradation is a real effect (e.g., p-value less than 1e-10). 2. Corrections for multiple testing. The O'Brien & Fleming procedure [43] calls for lower p-values early on and these increase over time, as shown in Figure 4. For example, in a 7-day experiment, the p-value cutoff for the 1 st day is , which is much smaller than 0.05, while the last cutoff is 0.040. This works well, as earlier termination needs to meet a higher bar, which aligns well with our intuition. Second, the p-value cutoff at the final check point is not much lower than 0.05. This implies that an experiment that is significant under the one-stop testing is likely to be significant under the O'Brien-Fleming as well, while if the results are extreme we gain the benefit of stopping early. 3. Different magnitudes of changes for different metrics are categorized in specific severity levels. The most severe changes result in automatic shutdown of an experiment but less severe changes will result in emails sent to the owner of the experiment and a central experimentation team.\n\nIn addition to looking at user and business impact metrics, it is critical to monitor data quality metrics. See Section 8 of the Seven Pitfalls paper [22] for recommended audits.\n\n\nTRUSTWORTHINESS and STATISTICAL LESSONS\n\nIt is critical that the results of experiments be trustworthy: incorrect results may cause bad ideas to be deployed or good ideas to be incorrectly ruled out. With a large system, false positives are inevitable, so we try to minimize their impact. As a user is put into more and more concurrent experiments, the chance of unexpected interactions between those experiment increases, which can lead to misleading results, and hinder scaling. Preventing interactions where possible, and detecting where not, has been a critical element for delivering trustworthy, large scale experimentation.\n\n\nFalse Positives\n\nFalse positives are \"positive findings\" that are not actually true. They can be due to experimental design issues, data issues, biased analyses, or simply chance. It is known that causal inferences using observational data have much higher false positive rates than a proper conducted controlled experiment [17]. But as Ioannidis showed [31], false positives in controlled experiments can still be higher than we expect. To avoid the design and analysis biases described by Ioannidis, we standardized our designs and automated experiment analyses.\n\nWhen statistical hypothesis testing is executed properly, the false positive rate is controlled by the p-value threshold, usually set at 5%. This rate assumes one data set, one outcome, and one analysis. In practice, we violate each of those assumptions.\n\nFirst, in online experimentation, data are collected sequentially. If we check the results every day, then the one dataset assumption is violated and we are exposed to false positives due to multiple testing (also see Section 4.3). While we allow experimenters to look at daily results, as they lead to insights and could help identify bugs early on, there is one final scorecard at the end of the experiment, which we require to be a multiple of weeks, usually two weeks.\n\nSecond, we report results on not one metric, but on hundreds, mostly to aid debugging and analysis. To address the multiple outcomes issue, we standardized our success criteria to use a small set of metrics, such as sessions/user [25].\n\nThird, we violate the one analysis assumption as experimenters slice and dice the data many ways-e.g. to understand the impact on specific user segments like one type of browser. For multiple analyses (slice-and-dice), we educate experimenters on false positives, and encourage them to adjust their probability threshold, focusing on strong signals and smaller p-values (e.g., < 1e-4).\n\nAs teams iteratively improve a feature based on experiment results, a new idea may go through a sequence of tens of controlled experiments. The risk is that a team may get a significant result by chance, celebrate, and ship. Assuming the feature does nothing, running k iterations (each with small variations that do nothing), then the probability of statistical significance grows from 2.5% (positive movement in a two-sided test) to (1 0.975 ). The problem is exacerbated when teams run multiple treatments. If a team tries five treatments, then the 2.5% false positive rate grows to 12%. If they do six iterations of 5treatment experiments, there is more than a 50% chance of getting a positive statistically significant result. Two mechanisms are used to protect us from these false positives:\n\n1. Threshold adjustments. We look for lower p-values for projects that have multiple treatments and/or iterations. 2. Replication Stage. While we encourage trying multiple variants, once the funnel narrows, there should be a \"final\" run, preferably with higher statistical power, which determines the final results.\n\nIn any large scale system false positives are a commonplace given the many ways in which we violate the assumption of a single hypothesis test done once. Our approach is pragmatic: we accept that fact and adjust our practices to reduce the rate of occurrence (e.g. by adjusting the threshold) and use replication as the final check to avoid false positives and get a more accurate (unbiased) estimate of the effect size.\n\n\nInteraction Prevention and Detection\n\nAs we increase the number of experiments running in parallel, the risk of interactions between different treatments becomes a growing concern. A statistical interaction between two treatments A and B exists if their combined effect is not the same as the sum of two individual treatment effects [18]. The existence of interaction violates the basic premise we use to scale experimentation: that each experiment can be analyzed in isolation. In an organization running hundreds of experiments daily, interactions pose a serious threat to experiment trustworthiness. First, interactions can harm users, because particular combinations can trigger unexpected bugs and cause a negative user experience. Second, interactions skew experiment results for all experiments involved. This is extremely important when the real treatment effect is small, as a small interaction can give completely misleading results for a key metric. Finally, it is impossible to completely prevent interaction in a large-scale experimentation system through testing and other offline checks. Different teams focusing on their own area do not know or check interactions with features tested by other teams.\n\nA comprehensive solution for interaction includes both prevention and detection. To prevent interactions, each Bing experiment defines a set of constraints. The experiment system uses those constraints to ensure that conflicting experiment do not run together. For example, a constraint associated with all ad visual experiments ensures that a user is never assigned to two such experiments at the same time. Another key tool for prevention uses a configuration management system to automatically detect experiments trying to change the same configuration parameter prior to launch [3]. Finally, when interactions cannot be avoided we use \"mappings\" to exclude users in one experiment from appearing in another experiment.\n\nPrevention is never perfect and it is critical to detect what we cannot prevent. Interaction detection for a large scale system is itself a problem of scale: If we are running N experiments at a time, the complexity of detecting pairwise-interactions is quadratic in N. Bing's Experimentation System monitors all running experiments for potential pairwise interactions on a set of metrics, both key user metrics as well a set of metrics we have found sensitive to interactions. Because of the large scale of experimentation, the system must sometimes run hundreds of thousands of hypothesis tests. To prevent a high false positives rate we use an Empirical Bayesian False Discovery Rate control algorithm to identify cases that are most likely be true positive [44]. After detecting an interaction, the tool will automatically run a deeper analysis and diagnose the most important interactions, which are sent as an alert to the experiment owners.\n\n\nCONCLUSION\n\nAnyone who has been running online controlled experiments knows how humbling it is to get an objective assessment of your ideas by real users. After an initial period of disappointment that our \"gut\" feelings and intuition mislead us so often, one recognizes that the ability to separate the truly good ideas from the rest is an innovation accelerator and a core organizational competency.\n\nWe shared the challenges and lessons in scaling to run a large number of experiments. We covered three broad areas: cultural / organizational, engineering, and trustworthiness, and covered issues including cost/benefit tradeoffs, negative experiments, incrementalism concerns, dealing with false positives and interactions, and an evaluation of the overall impact of the experimentation system itself.\n\nOne aspect of scaling that we did not discuss is scaling through improved sensitivity. Better sensitivity is crucial in scaling an experiment system, as it effectively increases the number of experiments that can run concurrently without requiring more users. In Bing, we started using pre-experiment data to reduce the variance and improve sensitivity [45], but we believe there is room for significant improvements in this area.\n\nWe hope these lessons will allow others to scale their systems and accelerate innovation through trustworthy experimentation.\n\nFigure 2 :\n2Exponential growth in experimentation over time. (Prior to 2012, Bing shut down most experiments the last two weeks of December.)\n\nFigure 4 :\n4O'Brien-Fleming p-value thresholds as the experiment progresses, with 7 check points\n\n\nhow much research you've done, or how many competitors are doing it, sometimes, more often than you might think, experiment ideas simply fail.\"Not every domain has such poor statistics, but most who have run controlled experiments in customer-facing web sites and applications have experienced this humbling reality: we are poor at assessing the value of ideas.wrote that at \nGoogle, only \"about 10 percent of these [controlled experiments, \nwere] leading to business changes.\" Avinash Kaushik wrote in his \nExperimentation and Testing primer [27] that \"80% of the time \nyou/we are wrong about what a customer wants.\" Mike Moran \n[28 p. 240] wrote that Netflix considers 90% of what they try to \nbe wrong. Regis Hadiaris from Quicken Loans wrote that \"in the \nfive years I've been running tests, I'm only about as correct in \nguessing the results as a major league baseball player is in hitting \nthe ball. That's right -I've been doing this for 5 years, and I can \nonly \"guess\" the outcome of a test about 33% of the time!\" [4]. \nDan McKinley at Etsy wrote [29] \"nearly everything fails\" and \n\"it's been humbling to realize how rare it is for them [features] to \nsucceed on the first attempt. I strongly suspect that this experience \nis universal, but it is not universally recognized or \nacknowledged.\" Finally, Colin McFarland wrote in the book \nExperiment! [7 p. 20] \"No matter how much you think it's a no-\nbrainer, \n\n\nHere are the best examples we found to drive the point across 1. Stanley Young and Alan Karr [30] compared published results from medical hypotheses shown to be significant using observational studies with randomized clinical trials, considered more reliable. Their conclusion: \"Any claim coming from an observational study is most likely to be wrong.\" 2. Ioannidis's papers [31; 32] (the first is the most downloaded technical paper in the Public Library of Science Medicine journal) showed that uncontrolled / nonrandomized experiments have a much higher probability of being false. Our experience is that organizations go through four stages as they learn to experiment [35]: (1) Hubris, where measurement is not needed because of confidence in the HiPPO (Highest Paid Person's Opinion). (2) Measurement and Control, where the organization measures key metrics and starts to control for unexplained differences. As Thomas Kuhn notes, paradigm shifts happen \"only through something's first going wrong with normal research\" [36]. (3) Semmelweis Reflex [37], where the organization rejects new knowledge because it contradicts entrenched norms, beliefs or paradigms. (4) Fundamental understanding, where causes are understood and models actually work.Manzi [17 p. 91] summarizes the papers as follows: \n\"[Ioannidis] evaluated the reliability of forty-nine influential \nstudies (each cited more than 1,000 times) published in major \njournals between 1990 and 2003 that reported effective \ninterventions based on either experimental or non-\nexperimental methods\u202690 percent of large randomized \nexperiments produced results that stood up to replication, as \ncompared to only 20 percent of nonrandomized studies.\" \nWhile the numbers are very small, these are convincing \npapers. \n\nAdditional accessible stories in the popular press have been very \nconvincing [33; 34]. \n\n\n\n\nthe final test has to be a controlled experiment: run the new component in an A/B test and see that you get no significant differences (or even better, some improvements). The reality is that the new code typically does not handle the edge cases as well as the old code, and it is very likely more buggy. The first author remembers how the Amazon Order Pipeline team wanted to introduce the new version based on the new app server, Gurupa,the previous section, we focused on evaluating new ideas. But \nwhat about platform changes, code refactoring, and bug fixes? \n\nIn a platform change, you replace an underlying platform \ncomponent with a new-and-better version. The team responsible \nfor the new platform component claims that the new one is faster, \ntakes up less memory, and does everything with a new-and-better \ncode base that's easier to maintain, faster to innovate, and fully \ntested for compatibility. They've been working on it for six \nmonths, passed all exit criteria, and are ready to deploy. In a data-\ndriven org, and he insisted that an A/B test be run: it failed with a 2% revenue \nloss. The team dug deep for two weeks, found \"the\" bug, and \nwanted to ship. No, you need to pass an A/B test was the message. \nThe team ran it and it failed again. The new pipeline shipped after \nfive iterations. It is not just new ideas that fail, but re-\nimplementations of existing ones are not as good as we initially \nthink. \n\n\nACKNOWLEDGMENTSWe wish to thank Xavier Amatriain, Steve Blank, Seth Eliot, Juan Lavista Ferres, Yan Guo, Greg Linden, Yoelle Maarek, Llew Mason, and Dan McKinley for their feedback. We have been fortunate to have been part of Bing during the massive growth in experimentation, and wish to thank many people for encouraging data-driven decision making, especially Qi Lu and Harry Shum.\nFront Line Internet Analytics at Amazon.com. Ron Kohavi, Matt Round, Jim Sterne. s.n.Santa Barbara, CAKohavi, Ron and Round, Matt. Front Line Internet Analytics at Amazon.com. [ed.] Jim Sterne. Santa Barbara, CA : s.n., 2004. http://ai.stanford.edu/~ronnyk/emetricsAmazon.pdf.\n\nDesign for Continuous Experimentation: Talk and Slides. Dan Mckinley, McKinley, Dan. Design for Continuous Experimentation: Talk and Slides. [Online] Dec 22, 2012. http://mcfunley.com/design-for- continuous-experimentation.\n\nOverlapping Experiment Infrastructure: More, Better, Faster Experimentation. Diane Tang, Proceedings 16th Conference on Knowledge Discovery and Data Mining. 16th Conference on Knowledge Discovery and Data MiningTang, Diane, et al. Overlapping Experiment Infrastructure: More, Better, Faster Experimentation. Proceedings 16th Conference on Knowledge Discovery and Data Mining. 2010.\n\nMultivariate Testing in Action: Quicken Loan's Regis Hadiaris on multivariate testing. Biznology Blog by Mike Moran. Mike Moran, Moran, Mike. Multivariate Testing in Action: Quicken Loan's Regis Hadiaris on multivariate testing. Biznology Blog by Mike Moran. [Online] December 2008. www.biznology.com/2008/12/multivariate_testing_in_action/.\n\nOnline Experimentation at Microsoft. Third Workshop on Data Mining Case Studies and Practice Prize. Ron Kohavi, Thomas Crook, Roger Longbotham, Kohavi, Ron, Crook, Thomas and Longbotham, Roger. Online Experimentation at Microsoft. Third Workshop on Data Mining Case Studies and Practice Prize. 2009. http://exp- platform.com/expMicrosoft.aspx.\n\nNetflix Recommendations: Beyond the 5 stars. Xavier Amatriain, Justin Basilico, Amatriain, Xavier and Basilico , Justin. Netflix Recommendations: Beyond the 5 stars. [Online] April 2012.\n\nExperiment!: Website conversion rate optimization with A/B and multivariate testing. s.l. : New Riders. Colin Mcfarland, McFarland, Colin. Experiment!: Website conversion rate optimization with A/B and multivariate testing. s.l. : New Riders, 2012. 978- 0321834607.\n\nRecommendations and Discovery at StumbleUpon. Sumanth Kolar, Kolar, Sumanth. Recommendations and Discovery at StumbleUpon. [Online] Sept 2012. www.slideshare.net/sumanthkolar/recsys-2012- sumanth-14260370.\n\nWhat is Zynga's core competency? Quora. Brandon Smietana, Zynga, Smietana, Brandon. Zynga: What is Zynga's core competency? Quora. [Online] Sept 2010. http://www.quora.com/Zynga/What-is-Zyngas- core-competency/answer/Brandon-Smietana.\n\nThe Four Steps to the Epiphany: Successful Strategies for Products that Win . s.l. : Cafepress.com. Steven Blank, Gary, Blank, Steven Gary. The Four Steps to the Epiphany: Successful Strategies for Products that Win . s.l. : Cafepress.com, 2005. 978- 0976470700.\n\nThe Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses . s.l. : Crown Business. Eric Ries, Ries, Eric. The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses . s.l. : Crown Business, 2011. 978-0307887894.\n\n. Lean Wikipedia, Startup, Wikipedia. Lean Startup. [Online] 2013. http://en.wikipedia.org/wiki/Lean_Startup.\n\nOne Strategy: Organization, Planning, and Decision Making . s.l. Steven Sinofsky, WileySinofsky, Steven. One Strategy: Organization, Planning, and Decision Making . s.l. : Wiley, 2009. 978-0470560457 .\n\nSearch Engine Rankings. Releases, 13U.ScomScore. comScore Releases January 2013 U.S. Search Engine Rankings. [Online] Feb 13, 2013. http://www.comscore.com/Insights/Press_Releases/2013/2/comScore _Releases_January_2013_U.S._Search_Engine_Rankings.\n\nSCOPE: Parallel Databases Meet MapReduce. Zhou, Jingren, et al. s.l. : VLDB Journal. SCOPE: Parallel Databases Meet MapReduce. Zhou, Jingren, et al. s.l. : VLDB Journal, 2012. http://research.microsoft.com/en- us/um/people/jrzhou/pub/Scope-VLDBJ.pdf.\n\nMicrosoft Second Quarter 2013 Earnings Calls Transcript. Microsoft Investor Relations. Peter Klein, Chris Suh, Klein, Peter and Suh, Chris. Microsoft Second Quarter 2013 Earnings Calls Transcript. Microsoft Investor Relations. [Online] Jan 24, 2013. http://www.microsoft.com/global/Investor/RenderingAssets/Downloa ds/FY13/Q2/Microsoft_Q2_2013_PreparedRemarks.docx.\n\nUncontrolled: The Surprising Payoff of Trial-and-Error for Business, Politics, and Society. s.l. : Basic Books. Jim Manzi, 978-0- 465-02931-0Manzi, Jim. Uncontrolled: The Surprising Payoff of Trial-and-Error for Business, Politics, and Society. s.l. : Basic Books, 2012. 978-0- 465-02931-0.\n\nControlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery. Ron Kohavi, 18Kohavi, Ron, et al. Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery. February 2009, Vol. 18, 1, pp. 140-181. http://www.exp- platform.com/Pages/hippo_long.aspx.\n\nHow to Improve A/B Testing. ClickZ Network. Bryan Eisenberg, Eisenberg, Bryan. How to Improve A/B Testing. ClickZ Network. [Online] April 29, 2005. www.clickz.com/clickz/column/1717234/how-improve-a-b-testing.\n\nStatistics for Experimenters: Design, Innovation, and Discovery. George E P Box, Hunter, Stuart, William G Hunter, John Wiley & Sons, Inc2nd. s.l.Box, George E.P., Hunter, J Stuart and Hunter, William G. Statistics for Experimenters: Design, Innovation, and Discovery. 2nd. s.l. : John Wiley & Sons, Inc, 2005. 0471718130.\n\nRon Kohavi, Online Controlled Experiments: Introduction, Learnings, and Humbling Statistics. The ACM Conference on Recommender Systems. 2012. Industry Keynote. Kohavi, Ron. Online Controlled Experiments: Introduction, Learnings, and Humbling Statistics. The ACM Conference on Recommender Systems. 2012. Industry Keynote. http://www.exp- platform.com/Pages/2012RecSys.aspx.\n\nSeven Pitfalls to Avoid when Running Controlled Experiments on the Web. Thomas Crook, Peter Flach and Mohammed Zaki. KDD '09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. Crook, Thomas, et al. Seven Pitfalls to Avoid when Running Controlled Experiments on the Web. [ed.] Peter Flach and Mohammed Zaki. KDD '09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009, pp. 1105-1114. http://www.exp- platform.com/Pages/ExPpitfalls.aspx.\n\nUnexpected Results in Online Controlled Experiments. Ron Kohavi, Roger Longbotham, SIGKDD Explorations. 2010. 12Kohavi, Ron and Longbotham, Roger. Unexpected Results in Online Controlled Experiments. SIGKDD Explorations. 2010, Vol. 12, 2. http://www.exp-platform.com/Documents/2010- 12%20ExPUnexpectedSIGKDD.pdf.\n\nInsanely Simple: The Obsession That Drives Apple's Success. s.l. : Portfolio Hardcover. Ken Segall, Segall, Ken. Insanely Simple: The Obsession That Drives Apple's Success. s.l. : Portfolio Hardcover, 2012. 978-1591844839.\n\nTrustworthy online controlled experiments: Five puzzling outcomes explained. Ron Kohavi, Proceedings of the 18th Conference on Knowledge Discovery and Data Mining. the 18th Conference on Knowledge Discovery and Data MiningKohavi, Ron, et al. Trustworthy online controlled experiments: Five puzzling outcomes explained. Proceedings of the 18th Conference on Knowledge Discovery and Data Mining. 2012, www.exp- platform.com/Pages/PuzzingOutcomesExplained.aspx.\n\nOnline Experiments: Practical Lessons. Ron Kohavi, Roger Longbotham, Toby ; Walker, S Y Simon, Shim, IEEE Computer. 43Kohavi, Ron, Longbotham, Roger and Walker, Toby. Online Experiments: Practical Lessons. [ed.] Simon S.Y. Shim. IEEE Computer. September 2010, Vol. 43, 9, pp. 82-85. http://www.exp- platform.com/Documents/IEEE2010ExP.pdf.\n\nExperimentation and Testing: A Primer. Occam's Razor. Avinash Kaushik, Kaushik, Avinash. Experimentation and Testing: A Primer. Occam's Razor. [Online] May 22, 2006. http://www.kaushik.net/avinash/2006/05/experimentation-and-testing- a-primer.html.\n\nDo It Wrong Quickly: How the Web Changes the Old Marketing Rules . s.l. Mike Moran, IBM PressMoran, Mike. Do It Wrong Quickly: How the Web Changes the Old Marketing Rules . s.l. : IBM Press, 2007. 0132255960.\n\nTesting to Cull the Living Flower. Dan Mckinley, McKinley, Dan. Testing to Cull the Living Flower. [Online] Jan 2013. http://mcfunley.com/testing-to-cull-the-living-flower.\n\nA process out of control and needing fixing. Deming, S Young, Stanley, Allan Karr, 8Deming, data and observational studies: A process out of control and needing fixing. Young , S Stanley and Karr, Allan. 3, 2011, Significance, Vol. 8. http://www.niss.org/sites/default/files/Young%20Karr%20Obs%20St udy%20Problem.pdf.\n\n10.1371/journal.pmed.0020124Why Most Published Research Findings Are False. Ioannidis. 2124Why Most Published Research Findings Are False. Ioannidis, John P. 8, 2005, PLoS Medicine, Vol. 2, p. e124. http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0 020124.\n\nContradicted and Initially Stronger Effects in Highly Cited Clinical Research. Ioannidis, John P. 2, s.l. : The Journal of the American Medical Association. 294Contradicted and Initially Stronger Effects in Highly Cited Clinical Research. Ioannidis, John P. 2, s.l. : The Journal of the American Medical Association, 2005, Vol. 294. http://jama.jamanetwork.com/article.aspx?articleid=201218.\n\nStudy Debunks Theory On Teen Sex, Delinquency. Washington Post. Rick Weiss, Weiss, Rick. Study Debunks Theory On Teen Sex, Delinquency. Washington Post. 2007, Nov 11. http://www.washingtonpost.com/wp- dyn/content/story/2007/11/11/ST2007111100542.html.\n\nHow Science Works: The Story of Night-Light. Myopia: Prevention and Control. Myopia, Myopia, How Science Works: The Story of Night-Light. Myopia: Prevention and Control. [Online]\n\nOnline Controlled Experiments: Listening to the Customers, not to the HiPPO. Ron Kohavi, Keynote at EC10: the 11th ACM Conference on Electronic Commerce. 2010. Kohavi, Ron. Online Controlled Experiments: Listening to the Customers, not to the HiPPO. Keynote at EC10: the 11th ACM Conference on Electronic Commerce. 2010. http://www.exp- platform.com/Documents/2010-06%20EC10.pptx.\n\nThe Structure of Scientific Revolutions. Thomas Kuhn, 3rd. 1996. 978-0226458083Kuhn, Thomas. The Structure of Scientific Revolutions. 3rd. 1996. 978-0226458083 .\n\nSemmelweis reflex. Wikipedia, Wikipedia. Semmelweis reflex. http://en.wikipedia.org/wiki/Semmelweis_reflex.\n\nHow to Measure Anything: Finding the Value of Intangibles in Business. Douglas W Hubbard, Wiley2nd. s.lHubbard, Douglas W. How to Measure Anything: Finding the Value of Intangibles in Business. 2nd. s.l. : Wiley, 2010.\n\nBad Medicine: Doctors Doing Harm Since Hippocrates. s.l. David Wooton, Oxford University PressWooton, David. Bad Medicine: Doctors Doing Harm Since Hippocrates. s.l. : Oxford University Press, 2007.\n\nMake Data Useful. Greg Linden, Linden, Greg. Make Data Useful. [Online] Dec 2006. home.blarg.net/~glinden/StanfordDataMining.2006-11-29.ppt.\n\nPerformance Related Changes and their User Impact. Schurman, Eric and Brutlag, Jake. s.l. : Velocity 09: Velocity Web Performance and Operations Conference. Performance Related Changes and their User Impact. Schurman, Eric and Brutlag, Jake. s.l. : Velocity 09: Velocity Web Performance and Operations Conference, 2009.\n\n. Douglas Bowman, Goodbye, Google, Stopdesign, Douglas Bowman. Goodbye, Google. StopDesign. [Online] http://stopdesign.com/archive/2009/03/20/goodbye-google.html.\n\nA Multiple Testing Procedure for Clinical Trials. O&apos;brien, C Peter, Thomas R Fleming, Biometrics. 3A Multiple Testing Procedure for Clinical Trials. O'Brien, Peter C. and Fleming, Thomas R. 3, September 1979, Biometrics, Vol. 35, pp. 549-556.\n\nA unified approach to false discovery rate estimation. Strimmer, Korbinian. 1, s.l. Bmc Bioinformatics. 9A unified approach to false discovery rate estimation. Strimmer, Korbinian. 1, s.l. : Bmc Bioinformatics, 2008, Vol. 9.\n\nImproving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data. Alex Deng, WSDM 2013: Sixth ACM International Conference on Web Search and Data Mining. Deng, Alex, et al. Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data. WSDM 2013: Sixth ACM International Conference on Web Search and Data Mining. 2013. www.exp-platform.com/Pages/CUPED.aspx.\n", "annotations": {"author": "[{\"end\":123,\"start\":48},{\"end\":200,\"start\":124},{\"end\":280,\"start\":201},{\"end\":359,\"start\":281},{\"end\":428,\"start\":360},{\"end\":506,\"start\":429}]", "publisher": null, "author_last_name": "[{\"end\":58,\"start\":52},{\"end\":133,\"start\":129},{\"end\":213,\"start\":207},{\"end\":292,\"start\":286},{\"end\":365,\"start\":363},{\"end\":442,\"start\":434}]", "author_first_name": "[{\"end\":51,\"start\":48},{\"end\":128,\"start\":124},{\"end\":206,\"start\":201},{\"end\":285,\"start\":281},{\"end\":362,\"start\":360},{\"end\":433,\"start\":429}]", "author_affiliation": "[{\"end\":122,\"start\":81},{\"end\":199,\"start\":158},{\"end\":279,\"start\":238},{\"end\":358,\"start\":317},{\"end\":427,\"start\":386},{\"end\":505,\"start\":464}]", "title": "[{\"end\":45,\"start\":1},{\"end\":551,\"start\":507}]", "venue": null, "abstract": "[{\"end\":2736,\"start\":726}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2883,\"start\":2880},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2899,\"start\":2896},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2921,\"start\":2918},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2942,\"start\":2939},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2967,\"start\":2964},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2980,\"start\":2977},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2997,\"start\":2994},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3014,\"start\":3011},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3036,\"start\":3033},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3169,\"start\":3165},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3250,\"start\":3246},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3479,\"start\":3475},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4523,\"start\":4519},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4875,\"start\":4871},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5331,\"start\":5327},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5959,\"start\":5958},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6107,\"start\":6103},{\"end\":7020,\"start\":6997},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7472,\"start\":7468},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10316,\"start\":10312},{\"end\":11323,\"start\":11313},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11459,\"start\":11455},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11771,\"start\":11768},{\"end\":11866,\"start\":11852},{\"end\":12855,\"start\":12847},{\"end\":13432,\"start\":13424},{\"end\":14443,\"start\":14433},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15578,\"start\":15574},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16006,\"start\":16002},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16174,\"start\":16171},{\"end\":16665,\"start\":16653},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17050,\"start\":17047},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17134,\"start\":17130},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17609,\"start\":17605},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18330,\"start\":18326},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21793,\"start\":21789},{\"end\":24617,\"start\":24609},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24909,\"start\":24905},{\"end\":25483,\"start\":25474},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25593,\"start\":25589},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25726,\"start\":25723},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26764,\"start\":26760},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28382,\"start\":28378},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28575,\"start\":28571},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28628,\"start\":28625},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30278,\"start\":30275},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30784,\"start\":30780},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36263,\"start\":36259},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37338,\"start\":37334},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38326,\"start\":38322},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":38356,\"start\":38352},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39528,\"start\":39524},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41794,\"start\":41790},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":43260,\"start\":43257},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44164,\"start\":44160},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":45512,\"start\":45508}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45855,\"start\":45713},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45953,\"start\":45856},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47376,\"start\":45954},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49247,\"start\":47377},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50684,\"start\":49248}]", "paragraph": "[{\"end\":3480,\"start\":2752},{\"end\":4539,\"start\":3482},{\"end\":5672,\"start\":4541},{\"end\":7045,\"start\":5674},{\"end\":7124,\"start\":7047},{\"end\":7473,\"start\":7126},{\"end\":8011,\"start\":7496},{\"end\":9011,\"start\":8180},{\"end\":9715,\"start\":9013},{\"end\":10317,\"start\":9746},{\"end\":10682,\"start\":10319},{\"end\":10865,\"start\":10684},{\"end\":11188,\"start\":10867},{\"end\":12036,\"start\":11223},{\"end\":12956,\"start\":12038},{\"end\":13433,\"start\":12958},{\"end\":13874,\"start\":13435},{\"end\":14038,\"start\":13885},{\"end\":14921,\"start\":14162},{\"end\":16401,\"start\":14923},{\"end\":16771,\"start\":16403},{\"end\":17135,\"start\":16828},{\"end\":17243,\"start\":17175},{\"end\":17514,\"start\":17275},{\"end\":17810,\"start\":17516},{\"end\":18687,\"start\":17852},{\"end\":19701,\"start\":18689},{\"end\":20221,\"start\":19703},{\"end\":20707,\"start\":20256},{\"end\":21004,\"start\":20709},{\"end\":23377,\"start\":21029},{\"end\":24156,\"start\":23379},{\"end\":24720,\"start\":24207},{\"end\":25326,\"start\":24722},{\"end\":26514,\"start\":25328},{\"end\":27000,\"start\":26567},{\"end\":27684,\"start\":27002},{\"end\":28214,\"start\":27686},{\"end\":29043,\"start\":28237},{\"end\":30004,\"start\":29067},{\"end\":30520,\"start\":30021},{\"end\":32406,\"start\":30522},{\"end\":32796,\"start\":32447},{\"end\":33706,\"start\":32798},{\"end\":34275,\"start\":33756},{\"end\":34950,\"start\":34277},{\"end\":35583,\"start\":34990},{\"end\":35819,\"start\":35585},{\"end\":37182,\"start\":35821},{\"end\":37362,\"start\":37184},{\"end\":37995,\"start\":37406},{\"end\":38562,\"start\":38015},{\"end\":38818,\"start\":38564},{\"end\":39292,\"start\":38820},{\"end\":39529,\"start\":39294},{\"end\":39916,\"start\":39531},{\"end\":40715,\"start\":39918},{\"end\":41032,\"start\":40717},{\"end\":41454,\"start\":41034},{\"end\":42673,\"start\":41495},{\"end\":43397,\"start\":42675},{\"end\":44346,\"start\":43399},{\"end\":44750,\"start\":44361},{\"end\":45153,\"start\":44752},{\"end\":45585,\"start\":45155},{\"end\":45712,\"start\":45587}]", "formula": null, "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2750,\"start\":2738},{\"attributes\":{\"n\":\"1.1\"},\"end\":7494,\"start\":7476},{\"end\":8178,\"start\":8014},{\"attributes\":{\"n\":\"1.2\"},\"end\":9744,\"start\":9718},{\"attributes\":{\"n\":\"1.3\"},\"end\":11221,\"start\":11191},{\"attributes\":{\"n\":\"2.\"},\"end\":13883,\"start\":13877},{\"end\":14160,\"start\":14041},{\"end\":16826,\"start\":16774},{\"attributes\":{\"n\":\"3.\"},\"end\":17173,\"start\":17138},{\"attributes\":{\"n\":\"3.1\"},\"end\":17273,\"start\":17246},{\"attributes\":{\"n\":\"3.2\"},\"end\":17850,\"start\":17813},{\"attributes\":{\"n\":\"3.3\"},\"end\":20254,\"start\":20224},{\"attributes\":{\"n\":\"3.4\"},\"end\":21027,\"start\":21007},{\"attributes\":{\"n\":\"3.5\"},\"end\":24205,\"start\":24159},{\"attributes\":{\"n\":\"3.6\"},\"end\":26565,\"start\":26517},{\"attributes\":{\"n\":\"3.7\"},\"end\":28235,\"start\":28217},{\"attributes\":{\"n\":\"4.\"},\"end\":29065,\"start\":29046},{\"attributes\":{\"n\":\"4.1\"},\"end\":30019,\"start\":30007},{\"attributes\":{\"n\":\"4.2\"},\"end\":32445,\"start\":32409},{\"end\":33754,\"start\":33709},{\"attributes\":{\"n\":\"4.3\"},\"end\":34988,\"start\":34953},{\"attributes\":{\"n\":\"5.\"},\"end\":37404,\"start\":37365},{\"attributes\":{\"n\":\"5.1\"},\"end\":38013,\"start\":37998},{\"attributes\":{\"n\":\"5.2\"},\"end\":41493,\"start\":41457},{\"attributes\":{\"n\":\"6.\"},\"end\":44359,\"start\":44349},{\"end\":45724,\"start\":45714},{\"end\":45867,\"start\":45857}]", "table": "[{\"end\":47376,\"start\":46317},{\"end\":49247,\"start\":48631},{\"end\":50684,\"start\":49689}]", "figure_caption": "[{\"end\":45855,\"start\":45726},{\"end\":45953,\"start\":45869},{\"end\":46317,\"start\":45956},{\"end\":48631,\"start\":47379},{\"end\":49689,\"start\":49250}]", "figure_ref": "[{\"end\":8004,\"start\":7996},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10864,\"start\":10856},{\"end\":30082,\"start\":30074},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36348,\"start\":36340}]", "bib_author_first_name": "[{\"end\":51118,\"start\":51115},{\"end\":51131,\"start\":51127},{\"end\":51407,\"start\":51404},{\"end\":51655,\"start\":51650},{\"end\":52077,\"start\":52073},{\"end\":52402,\"start\":52399},{\"end\":52417,\"start\":52411},{\"end\":52430,\"start\":52425},{\"end\":52695,\"start\":52689},{\"end\":52713,\"start\":52707},{\"end\":52941,\"start\":52936},{\"end\":53152,\"start\":53145},{\"end\":53353,\"start\":53346},{\"end\":53648,\"start\":53642},{\"end\":53947,\"start\":53943},{\"end\":54132,\"start\":54128},{\"end\":54308,\"start\":54302},{\"end\":55033,\"start\":55028},{\"end\":55046,\"start\":55041},{\"end\":55423,\"start\":55420},{\"end\":55703,\"start\":55700},{\"end\":55976,\"start\":55971},{\"end\":56209,\"start\":56203},{\"end\":56213,\"start\":56210},{\"end\":56242,\"start\":56235},{\"end\":56244,\"start\":56243},{\"end\":56465,\"start\":56462},{\"end\":56914,\"start\":56908},{\"end\":57433,\"start\":57430},{\"end\":57447,\"start\":57442},{\"end\":57782,\"start\":57779},{\"end\":57995,\"start\":57992},{\"end\":58417,\"start\":58414},{\"end\":58431,\"start\":58426},{\"end\":58448,\"start\":58444},{\"end\":58450,\"start\":58449},{\"end\":58460,\"start\":58459},{\"end\":58462,\"start\":58461},{\"end\":58776,\"start\":58769},{\"end\":59041,\"start\":59037},{\"end\":59213,\"start\":59210},{\"end\":59403,\"start\":59402},{\"end\":59425,\"start\":59420},{\"end\":60405,\"start\":60401},{\"end\":60850,\"start\":60847},{\"end\":61199,\"start\":61193},{\"end\":61502,\"start\":61495},{\"end\":61504,\"start\":61503},{\"end\":61706,\"start\":61701},{\"end\":61866,\"start\":61862},{\"end\":62316,\"start\":62309},{\"end\":62536,\"start\":62535},{\"end\":62550,\"start\":62544},{\"end\":62552,\"start\":62551},{\"end\":63043,\"start\":63039}]", "bib_author_last_name": "[{\"end\":51125,\"start\":51119},{\"end\":51137,\"start\":51132},{\"end\":51416,\"start\":51408},{\"end\":51660,\"start\":51656},{\"end\":52083,\"start\":52078},{\"end\":52409,\"start\":52403},{\"end\":52423,\"start\":52418},{\"end\":52441,\"start\":52431},{\"end\":52705,\"start\":52696},{\"end\":52722,\"start\":52714},{\"end\":52951,\"start\":52942},{\"end\":53158,\"start\":53153},{\"end\":53362,\"start\":53354},{\"end\":53369,\"start\":53364},{\"end\":53654,\"start\":53649},{\"end\":53660,\"start\":53656},{\"end\":53952,\"start\":53948},{\"end\":54142,\"start\":54133},{\"end\":54151,\"start\":54144},{\"end\":54317,\"start\":54309},{\"end\":54472,\"start\":54464},{\"end\":55039,\"start\":55034},{\"end\":55050,\"start\":55047},{\"end\":55429,\"start\":55424},{\"end\":55710,\"start\":55704},{\"end\":55986,\"start\":55977},{\"end\":56217,\"start\":56214},{\"end\":56225,\"start\":56219},{\"end\":56233,\"start\":56227},{\"end\":56251,\"start\":56245},{\"end\":56472,\"start\":56466},{\"end\":56920,\"start\":56915},{\"end\":57440,\"start\":57434},{\"end\":57458,\"start\":57448},{\"end\":57789,\"start\":57783},{\"end\":58002,\"start\":57996},{\"end\":58424,\"start\":58418},{\"end\":58442,\"start\":58432},{\"end\":58457,\"start\":58451},{\"end\":58468,\"start\":58463},{\"end\":58474,\"start\":58470},{\"end\":58784,\"start\":58777},{\"end\":59047,\"start\":59042},{\"end\":59222,\"start\":59214},{\"end\":59400,\"start\":59394},{\"end\":59409,\"start\":59404},{\"end\":59418,\"start\":59411},{\"end\":59430,\"start\":59426},{\"end\":60411,\"start\":60406},{\"end\":60673,\"start\":60667},{\"end\":60857,\"start\":60851},{\"end\":61204,\"start\":61200},{\"end\":61343,\"start\":61334},{\"end\":61512,\"start\":61505},{\"end\":61713,\"start\":61707},{\"end\":61873,\"start\":61867},{\"end\":62323,\"start\":62317},{\"end\":62332,\"start\":62325},{\"end\":62340,\"start\":62334},{\"end\":62352,\"start\":62342},{\"end\":62533,\"start\":62521},{\"end\":62542,\"start\":62537},{\"end\":62560,\"start\":62553},{\"end\":63048,\"start\":63044}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":51346,\"start\":51070},{\"attributes\":{\"id\":\"b1\"},\"end\":51571,\"start\":51348},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14868978},\"end\":51954,\"start\":51573},{\"attributes\":{\"id\":\"b3\"},\"end\":52297,\"start\":51956},{\"attributes\":{\"id\":\"b4\"},\"end\":52642,\"start\":52299},{\"attributes\":{\"id\":\"b5\"},\"end\":52830,\"start\":52644},{\"attributes\":{\"id\":\"b6\"},\"end\":53097,\"start\":52832},{\"attributes\":{\"id\":\"b7\"},\"end\":53304,\"start\":53099},{\"attributes\":{\"id\":\"b8\"},\"end\":53540,\"start\":53306},{\"attributes\":{\"id\":\"b9\"},\"end\":53804,\"start\":53542},{\"attributes\":{\"id\":\"b10\"},\"end\":54124,\"start\":53806},{\"attributes\":{\"id\":\"b11\"},\"end\":54235,\"start\":54126},{\"attributes\":{\"id\":\"b12\"},\"end\":54438,\"start\":54237},{\"attributes\":{\"id\":\"b13\"},\"end\":54687,\"start\":54440},{\"attributes\":{\"id\":\"b14\"},\"end\":54939,\"start\":54689},{\"attributes\":{\"id\":\"b15\"},\"end\":55306,\"start\":54941},{\"attributes\":{\"doi\":\"978-0- 465-02931-0\",\"id\":\"b16\"},\"end\":55598,\"start\":55308},{\"attributes\":{\"id\":\"b17\"},\"end\":55925,\"start\":55600},{\"attributes\":{\"id\":\"b18\"},\"end\":56136,\"start\":55927},{\"attributes\":{\"id\":\"b19\"},\"end\":56460,\"start\":56138},{\"attributes\":{\"id\":\"b20\"},\"end\":56834,\"start\":56462},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10796366},\"end\":57375,\"start\":56836},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":784647},\"end\":57689,\"start\":57377},{\"attributes\":{\"id\":\"b23\"},\"end\":57913,\"start\":57691},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10813638},\"end\":58373,\"start\":57915},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":5918864},\"end\":58713,\"start\":58375},{\"attributes\":{\"id\":\"b26\"},\"end\":58963,\"start\":58715},{\"attributes\":{\"id\":\"b27\"},\"end\":59173,\"start\":58965},{\"attributes\":{\"id\":\"b28\"},\"end\":59347,\"start\":59175},{\"attributes\":{\"id\":\"b29\"},\"end\":59666,\"start\":59349},{\"attributes\":{\"doi\":\"10.1371/journal.pmed.0020124\",\"id\":\"b30\"},\"end\":59942,\"start\":59668},{\"attributes\":{\"id\":\"b31\"},\"end\":60335,\"start\":59944},{\"attributes\":{\"id\":\"b32\"},\"end\":60588,\"start\":60337},{\"attributes\":{\"id\":\"b33\"},\"end\":60768,\"start\":60590},{\"attributes\":{\"id\":\"b34\"},\"end\":61150,\"start\":60770},{\"attributes\":{\"id\":\"b35\"},\"end\":61313,\"start\":61152},{\"attributes\":{\"id\":\"b36\"},\"end\":61422,\"start\":61315},{\"attributes\":{\"id\":\"b37\"},\"end\":61642,\"start\":61424},{\"attributes\":{\"id\":\"b38\"},\"end\":61842,\"start\":61644},{\"attributes\":{\"id\":\"b39\"},\"end\":61984,\"start\":61844},{\"attributes\":{\"id\":\"b40\"},\"end\":62305,\"start\":61986},{\"attributes\":{\"id\":\"b41\"},\"end\":62469,\"start\":62307},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":40193042},\"end\":62718,\"start\":62471},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2840576},\"end\":62944,\"start\":62720},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3016750},\"end\":63360,\"start\":62946}]", "bib_title": "[{\"end\":51113,\"start\":51070},{\"end\":51648,\"start\":51573},{\"end\":56906,\"start\":56836},{\"end\":57428,\"start\":57377},{\"end\":57990,\"start\":57915},{\"end\":58412,\"start\":58375},{\"end\":60845,\"start\":60770},{\"end\":62519,\"start\":62471},{\"end\":62802,\"start\":62720},{\"end\":63037,\"start\":62946}]", "bib_author": "[{\"end\":51127,\"start\":51115},{\"end\":51139,\"start\":51127},{\"end\":51418,\"start\":51404},{\"end\":51662,\"start\":51650},{\"end\":52085,\"start\":52073},{\"end\":52411,\"start\":52399},{\"end\":52425,\"start\":52411},{\"end\":52443,\"start\":52425},{\"end\":52707,\"start\":52689},{\"end\":52724,\"start\":52707},{\"end\":52953,\"start\":52936},{\"end\":53160,\"start\":53145},{\"end\":53364,\"start\":53346},{\"end\":53371,\"start\":53364},{\"end\":53656,\"start\":53642},{\"end\":53662,\"start\":53656},{\"end\":53954,\"start\":53943},{\"end\":54144,\"start\":54128},{\"end\":54153,\"start\":54144},{\"end\":54319,\"start\":54302},{\"end\":54474,\"start\":54464},{\"end\":55041,\"start\":55028},{\"end\":55052,\"start\":55041},{\"end\":55431,\"start\":55420},{\"end\":55712,\"start\":55700},{\"end\":55988,\"start\":55971},{\"end\":56219,\"start\":56203},{\"end\":56227,\"start\":56219},{\"end\":56235,\"start\":56227},{\"end\":56253,\"start\":56235},{\"end\":56474,\"start\":56462},{\"end\":56922,\"start\":56908},{\"end\":57442,\"start\":57430},{\"end\":57460,\"start\":57442},{\"end\":57791,\"start\":57779},{\"end\":58004,\"start\":57992},{\"end\":58426,\"start\":58414},{\"end\":58444,\"start\":58426},{\"end\":58459,\"start\":58444},{\"end\":58470,\"start\":58459},{\"end\":58476,\"start\":58470},{\"end\":58786,\"start\":58769},{\"end\":59049,\"start\":59037},{\"end\":59224,\"start\":59210},{\"end\":59402,\"start\":59394},{\"end\":59411,\"start\":59402},{\"end\":59420,\"start\":59411},{\"end\":59432,\"start\":59420},{\"end\":60413,\"start\":60401},{\"end\":60675,\"start\":60667},{\"end\":60859,\"start\":60847},{\"end\":61206,\"start\":61193},{\"end\":61345,\"start\":61334},{\"end\":61514,\"start\":61495},{\"end\":61715,\"start\":61701},{\"end\":61875,\"start\":61862},{\"end\":62325,\"start\":62309},{\"end\":62334,\"start\":62325},{\"end\":62342,\"start\":62334},{\"end\":62354,\"start\":62342},{\"end\":62535,\"start\":62521},{\"end\":62544,\"start\":62535},{\"end\":62562,\"start\":62544},{\"end\":63050,\"start\":63039}]", "bib_venue": "[{\"end\":51172,\"start\":51155},{\"end\":51784,\"start\":51730},{\"end\":58137,\"start\":58079},{\"end\":51149,\"start\":51139},{\"end\":51402,\"start\":51348},{\"end\":51728,\"start\":51662},{\"end\":52071,\"start\":51956},{\"end\":52397,\"start\":52299},{\"end\":52687,\"start\":52644},{\"end\":52934,\"start\":52832},{\"end\":53143,\"start\":53099},{\"end\":53344,\"start\":53306},{\"end\":53640,\"start\":53542},{\"end\":53941,\"start\":53806},{\"end\":54300,\"start\":54237},{\"end\":54462,\"start\":54440},{\"end\":54772,\"start\":54689},{\"end\":55026,\"start\":54941},{\"end\":55418,\"start\":55308},{\"end\":55698,\"start\":55600},{\"end\":55969,\"start\":55927},{\"end\":56201,\"start\":56138},{\"end\":56620,\"start\":56474},{\"end\":57060,\"start\":56922},{\"end\":57485,\"start\":57460},{\"end\":57777,\"start\":57691},{\"end\":58077,\"start\":58004},{\"end\":58489,\"start\":58476},{\"end\":58767,\"start\":58715},{\"end\":59035,\"start\":58965},{\"end\":59208,\"start\":59175},{\"end\":59392,\"start\":59349},{\"end\":59753,\"start\":59696},{\"end\":60099,\"start\":59944},{\"end\":60399,\"start\":60337},{\"end\":60665,\"start\":60590},{\"end\":60928,\"start\":60859},{\"end\":61191,\"start\":61152},{\"end\":61332,\"start\":61315},{\"end\":61493,\"start\":61424},{\"end\":61699,\"start\":61644},{\"end\":61860,\"start\":61844},{\"end\":62141,\"start\":61986},{\"end\":62572,\"start\":62562},{\"end\":62822,\"start\":62804},{\"end\":63125,\"start\":63050}]"}}}, "year": 2023, "month": 12, "day": 17}