{"id": 212747560, "updated": "2023-11-07 18:42:55.749", "metadata": {"title": "Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation", "authors": "[{\"first\":\"Moab\",\"last\":\"Arar\",\"middle\":[]},{\"first\":\"Yiftach\",\"last\":\"Ginger\",\"middle\":[]},{\"first\":\"Dov\",\"last\":\"Danon\",\"middle\":[]},{\"first\":\"Ilya\",\"last\":\"Leizerson\",\"middle\":[]},{\"first\":\"Amit\",\"last\":\"Bermano\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Cohen-Or\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": 3, "day": 18}, "abstract": "Many applications, such as autonomous driving, heavily rely on multi-modal data where spatial alignment between the modalities is required. Most multi-modal registration methods struggle computing the spatial correspondence between the images using prevalent cross-modality similarity measures. In this work, we bypass the difficulties of developing cross-modality similarity measures, by training an image-to-image translation network on the two input modalities. This learned translation allows training the registration network using simple and reliable mono-modality metrics. We perform multi-modal registration using two networks - a spatial transformation network and a translation network. We show that by encouraging our translation network to be geometry preserving, we manage to train an accurate spatial transformation network. Compared to state-of-the-art multi-modal methods our presented method is unsupervised, requiring no pairs of aligned modalities for training, and can be adapted to any pair of modalities. We evaluate our method quantitatively and qualitatively on commercial datasets, showing that it performs well on several modalities and achieves accurate alignment.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.08073", "mag": "3035166033", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ArarGDBC20", "doi": "10.1109/cvpr42600.2020.01342"}}, "content": {"source": {"pdf_hash": "d174e3e973c2b0e1bafa1038b0362ea6dd76be37", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.08073v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.08073", "status": "GREEN"}}, "grobid": {"id": "9b029af5627476271d8a15c8ac942c0df5079e58", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d174e3e973c2b0e1bafa1038b0362ea6dd76be37.txt", "contents": "\nUnsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation\n\n\nMoab Arar \nElbit Systems\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\n\n\nYiftach Ginger \nElbit Systems\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\n\n\nDov Danon \nElbit Systems\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\n\n\nIlya Leizerson \nElbit Systems\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\n\n\nAmit H Bermano \nElbit Systems\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\n\n\nDaniel Cohen-Or \nElbit Systems\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\nTel Aviv University\n\n\nUnsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation\n\nMany applications, such as autonomous driving, heavily rely on multi-modal data where spatial alignment between the modalities is required. Most multi-modal registration methods struggle computing the spatial correspondence between the images using prevalent cross-modality similarity measures. In this work, we bypass the difficulties of developing cross-modality similarity measures, by training an image-to-image translation network on the two input modalities. This learned translation allows training the registration network using simple and reliable mono-modality metrics. We perform multi-modal registration using two networks -a spatial transformation network and a translation network. We show that by encouraging our translation network to be geometry preserving, we manage to train an accurate spatial transformation network. Compared to state-of-the-art multi-modal methods our presented method is unsupervised, requiring no pairs of aligned modalities for training, and can be adapted to any pair of modalities. We evaluate our method quantitatively and qualitatively on commercial datasets, showing that it performs well on several modalities and achieves accurate alignment.\n\nIntroduction\n\nScene acquisition using different sensors is common practice in various disciplines, from classical ones such as medical imaging and remote sensing, to emerging tasks such as autonomous driving. Multi-modal sensors allow gathering a wide range of physical properties, which in turn yields richer scene representations. For example, in radiation planning, multi-modal data (e.g. Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) scans) is used for more accurate tumor contouring which reduces the risk of damaging healthy tissues in radiotherapy treatment [23,27]. More often than not, multi-modal sensors naturally have different extrinsic parameters between Spatial Registration\n\n\nCross-Modality Translation\n\n\nCross-Modality Metric\n\nMono-Modality Metric Figure 1: Method overview. Conventional methods (faded dashed at the bottom) use cross-modality metrics (e.g., Normalized Cross Correlation) to optimize a spatial transformation function. Our method learns a cross-modality translation, mapping between the two modalities. This enables the use of a reliable accurate mono-modality metric instead.\n\nmodalities, such as lens parameters and relative position. In these cases, non-rigid image registration is essential for proper execution of the aforementioned downstream tasks. Classic multi-modal image registration techniques attempt to warp a source image to match a target one via a non-linear optimization process, seeking to maximize a predefined similarity measure [38]. Besides a computational disadvantage, which is critical for applications such as autonomous driving, effectively designing similarity measures for such optimization has proven to be quite challenging. This is true for both intensity-based measures, commonly used in the medical imaging [10], and feature-based ones, typically adapted for more detailed modalities (e.g Near Infra-Red (NIR) and RGB) [30].\n\nThese difficulties gave rise to the recent development of deep regression models. These types of models typically have lengthy training time, either supervised or unsupervised, yet they offer expeditious inference that usually generalizes well. Since it is extremely hard to collect ground-truth data for the registration parameters, supervised multi-modal registration methods commonly use synthesized data in order to train a registration network [28,35]. This makes their robustness highly dependent on the similarity between the artificial and real-life data distribution and appearance. Unsupervised registration techniques, on the other-hand, frequently incorporate a spatial transform network (STN) [14] and train an end-to-end network [7,18,16,34,8].\n\nTypically, such approaches optimize an STN by comparing the deformed image and the target one using simple similarity metrics such as pixel-wise Mean Squared Error (MSE) [29,31,6]. Of course, such approaches can only be used in mono-modality settings and become irrelevant for multi-modality settings. To overcome this limitation, unsupervised multi-modal registration networks use statisticsbased similarity metrics, particularly, (Normalized) Mutual Information ((N)MI) [21], Normalized Cross Correlation (NCC) [5], or Structural Similarity Index Metric (SSIM) [20,21] (see Figure 1, faded dashed path). However, these metrics are either computationally intractable (e.g., MI) [3] and hence cannot be used in gradient-based methods, or are domain-dependent (e.g., NCC), failing to generalize for all modalities.\n\nIn this paper, we present an unsupervised method for multi-modal registration. In our work, we exploit the celebrated success of Multi-Modal Image Translation [13,36,37,12], and simultaneously learn multi-modal translation and spatial registration. The key idea is to alleviate the shortcomings of a hand-crafted similarity measure by training an image-to-image translation network T on two given modalities. This in turn will let us use mono-modality metrics for evaluating our registration network R (see Figure 1, vivid path on the top).\n\nThe main challenge for this approach is to train the registration network R and the translation network T simultaneously, while encouraging T to be geometry preserving. This ensures that the two networks are task-specific -T performs only a photo-metric mapping, while R learns the geometric transformation required for the registration task. In our work, we use the concepts of generative adversarial networks (GAN [9,22]) to train T and R. We show that the adversarial training is not only necessary for the translation task (as shown in previous works [13]), but is also necessary to produce smooth and accurate spatial transformation. We evaluate our method on real commercial data, and demonstrate its strength with a series of studies.\n\nThe main contributions of our work are:\n\n\u2022 An unsupervised method for multi-modal image registration.\n\n\u2022 A geometry preserving translation network that allows the application of mono-modality metrics in multimodal registration.\n\n\u2022 A training scheme that encourages a generator to be geometry preserving.\n\n\nRelated Works\n\nTo deal with the photo-metric difference between modalities, unsupervised multi-modal approaches are forced to find the correlation between the different domains and use it to guide their learning process. In [20] a vanilla CycleGAN architecture is used to regularize a deformation mapping. This is achieved by training a discriminator network to distinguish between deformed and real images. To align a pair of images the entire network needs to be trained in a single pass. Training this network on a large dataset will encourage the deformation mapping to become an identity mapping. This is because the discriminator is given only the real and deformed images. Furthermore the authors use multiple cross-modality similarity metrics including SSIM, NCC and NMI which are limited by the compatibility of the specific modalities used. In contrast, our method learns from a large dataset and bypasses the need for cross-modality similarity metrics.\n\nWang et al. [34] attempt to bypass the need for domain translation by learning an Encoder-Decoder module to create modality-independent features. The features are fed to an STN to learn affine and non-rigid transformations. The authors train their network using a simple similarity measure (MSE) which maintains local similarity, but does not enforce global fidelity.\n\nAt the other extreme, [8] rely entirely on an adversarial loss function. They train a regular U-Net based STN by giving the resultant registered images to a discriminator network and using its feedback as the STN's loss function. By relying solely on the discriminator network for guiding the training, they lose the ability to enforce local coherence between the registered and target images.\n\nClosest to our work, [25] combines an adversarial loss with similarity measurements in an effort to register the images properly while concentrating on maintaining local geometric properties. They encode the inputs into two separate embedding, one for shape and one for content information, and train a registration network on these disentangled embedding. This method relies on learned disentanglement, which introduces inconsistencies on the local level. Our method directly enforces the similarity in the image space, which leads to a reliable local signal.\n\n\nOverview\n\nOur core idea is to learn the translation between the two modalities, rather than using a cross-modality metric. This novel approach is illustrated in Figure 1. The spatially transformed image is translated by a learnable network. The translated image can then be compared to the original source image using a simple uni-modality metric, bypassing the need to use a cross-modality metric. The advantage of using a learnable translation network is that it\nR \u03a6 Input A Input B Deformation Field \u03c6 (a) Deformation Field Generation GAN Loss Reconstruction Loss T \u03c6 D Input A T \u25cb R Input B Re-sampler (b) Register First GAN Loss Reconstruction Loss T \u03c6 D Input A R \u25cb T Input B\nRe-sampler (c) Translate First Figure 2: Training Flow Overview. We train two components: (i) a spatial transformation network (STN) R = (R \u03a6 , R S ) and (ii) an image-to-image translation network T . The two networks R and T are jointly trained via two different training flows. The two training flows are simultaneously carried-out in each training step. In the first flow, (b) Register First, the input image I a is deformed using \u03c6, a deformation field generated by R \u03a6 , and is then fed to T to map the image onto domain B. The second flow, (c) Translate First, is similar with the exception that \u03c6 is used to transform the translated source image. In both cases, the same deformation field \u03c6 is used.\n\ngeneralizes and adapts to any pairs of given modalities.\n\nOur registration network consists of two components: (i) a spatial transformation network R = (R \u03a6 , R S ) and (ii) an image-to-image translation network T . The two components are trained simultaneously using two training flows as depicted in Figure 2. The spatial transformation network takes the two input images and yields a deformation field \u03c6. The field is the then applied either before T (Figure 2b) or after it ( Figure 2c). Specifically, the field is generated using a network R \u03a6 and is used by a re-sampling layer R S to get the transformed image, namely R S (T (a), \u03c6) and T (R S (a, \u03c6)). We will elaborate on these two training schemes in Section 4.2. The key is, as we shall show, that such two-flow training encourages T to be geometry preserving, which implies that all the geometry transformation is encoded in R \u03a6 .\n\nOnce trained, only the spatial transformation network R is used in test time. The network takes two images I a and I b representing the same scene, captured from slightly different viewpoint, in two different modalities, A and B, respectively, and aligns I a with I b .\n\n\nMethod\n\nOur goal is to learn a non-rigid spatial transformation which aligns two images from different domains. Let A \u2282 R H A \u00d7W A \u00d7C A and B \u2282 R H B \u00d7W B \u00d7C B be two paired image domains, where H D , W D , C D are the height, width, and number of channels for domain D, respectively. Pairing means that for each image I a \u2208 A there exists a unique image I b \u2208 B representing the same scene, as acquired by the different respective sensors. Note that the pairing assumption is a common and reasonable one, since more often than not registration-base applications involve taking an image of the same scene from both modality sensors (e.g satellite images). Throughout this section, we let I a \u2208 A and I b \u2208 B be a pair of two images such that I a needs to be aligned with\nI b .\nTo achieve this alignment, we train three learnable components: (i) a registration network R, (ii) a translation network T and (iii) a discriminator D. The three networks are trained using an adversarial model [9,22], where R and T are jointly trained to outwit D. Below, we describe the design and objectives of each network.\n\n\nRegistration Network\n\nOur registration network (R = (R \u03a6 , R S )) is a spatial transformation network (STN) composed of a fullyconvolutional network R \u03a6 and a re-sampler layer R S . The transformation we apply is a non-linear dense deformation -allowing non-uniform mapping between the images and hence gives accurate results. Next we give an in-depth description about each component.\n\nR \u03a6 -Deformation Field Generator: The network takes two input images, I a and I b , and produces a deformation field \u03c6 = R(I a , I b ) describing how to non-rigidly align\nI a to I b . The field is an H A \u00d7 W A matrix of 2-dimensional vectors, indicating the deformation direction for each pixel (i, j) in the input image I a .\nR S -Re-sampling Layer: This layer receives the deformation field \u03c6, produced by R \u03a6 , and applies it on a source image I s . Here, the source image is not necessarily I a and it could be from either domains -A or B. Specifically, the value of the transformed image R S (I s , \u03c6) at pixel v = (i, j) is given by Equation 1:\nR S (I s , \u03c6)[v] = I s [v + \u03c6(v)] ,(1)\nwhere \u03c6(v) = (\u2206y, \u2206x) is the deformation generated by\nR \u03a6 at pixel v = (i, j),\nin the x and y-directions, respectively.\n\nTo avoid overly distorting the deformed image R S (I s , \u03c6) we restrict R \u03a6 from producing non-smooth deformations. We adapt a common regularization term that is used to produce smooth deformations. In particular, the regularization loss will encourage neighboring pixels to have similar deformations. Formally, we seek to have small values of the first order gradients of \u03c6, hence the loss at pixel v = (i, j) is then given by: (2) where N (v) is a set of neighbors of the pixel v, and B (u, v) is a bilateral filter [32] used to reduce oversmoothing. Let O s = R S (I s , \u03c6) to be the deformed image produced by R S on input I s , then the bilateral filter is given by:\nL smooth (\u03c6, v) = u\u2208N (v) B(u, v) \u03c6(u) \u2212 \u03c6(v) ,B(u, v) = e \u2212\u03b1\u00b7 Os[u]\u2212Os[v] .(3)\nThere are two important notes about the bilateral filter B in Equation 3. First, the bilateral filtering is with respect to the transformed image O s (at each forward pass), and secondly, the term B (I s , u, v) is a treated as constant (at each backward pass). The latter is important to avoid R \u03a6 alternating pixel values so that B(u, v) \u2248 0 (e.g., it could change pixels so that\nO s [u] \u2212 O s [v]\nis relatively large), while the former allows better exploration of the solution space.\n\nIn our experiments we look at the 3 \u00d7 3 neighborhood of v, and set \u03b1 = 1. The overall smoothness loss of the network R, denoted by L smooth (R), is the mean value over all pixels v \u2208 {1, . . . , H A } \u00d7 {1, . . . , W A }.\n\n\nGeometric Preserving Translation Network\n\nA key challenge of our work is to train the image-toimage translation network T to be geometric preserving. If T is geometric preserving, it implies that it only performs photo-metric mapping, and as a consequence the registration task is performed solely by the registration network R. However, during our experiments, we observed that T tends to generate fake images that are spatially aligned with the ground truth image, regardless of R's accuracy.\n\nTo avoid this, we could restrict T from performing any spatial alignment by reducing its capacity (number of layers). While we did observe that reducing T 's capacity does improve our registration network's performance, it still limits the registration network from doing all the registration task (See supplementary materials).\n\nTo implicitly encourage T to be geometric preserving we require that T and R are commutative, i.e., T \u2022 R = R \u2022 T . In the following we formally define both T \u2022 R and R \u2022 T :\nTranslation First -(R \u2022 T) (I a , I b ):\nThis mapping first apply an image-to-image translation on I a and then a spatial transformation on the translated image. Specifically, the final image is obtained by first applying T on I a , which generates a fake sample O T = T (I a ). Then we apply our spatial transformation network R on O T and get the final output:\nO RT = R S (O T , \u03c6) = R (T (I a ) , R \u03a6 (I a , I b )) .\nRegister First -(T \u2022 R) (I a , I b ) in this composition, we first apply spatial transformation on I a and obtain a deformed image O R = R(I a , \u03c6). Then, we translate O R to domain B using our translation network T :\nO T R = T (R S (I a , \u03c6)) = T (R S (I a , R \u03a6 (I a , I b ))) .\nNote that in both compositions (i.e., T \u2022 R and R \u2022 T ), the deformation field, used by the re-sampler R S , is given by R \u03a6 (I a , I b ). The only difference is in the source image from which we re-sample the deformed image. Throughout this section, we refer to O RT and O T R as the outputs of R \u2022 T and T \u2022 R, respectively.\n\n\nTraining Losses\n\nTo train R and T to generate fake samples that are similar to those in domain B, we use an L1-reconstruction loss:\nL recon (T, R) = O RT \u2212 I b 1 + O T R \u2212 I b 1 (4)\nwhere minimizing the above implies that T \u2022 R \u2248 R \u2022 T .\n\nWe use conditional GAN (cGAN) [22] as our adversarial loss for training D, T and R. The objective of the adversarial network D is to discriminate between real and fake samples, while T and R are jointly trained to fool the discriminator. The cGAN loss for T \u2022R and R\u2022T is formulated below: \n\nThe total objective is given by:\nL(T, R) =arg max D L cGAN (T, D, R) + \u03bb R \u00b7 L recon (T, R) + \u03bb S \u00b7 L smooth (R),(6)\nwhere we are opt to find T * and R * such that T * , R * = arg min \n\n\nImplementation Details\n\nOur code is implemented using PyTorch 1.1.0 [24] and is based on the framework and implementation of Pix2Pix [13], CycleGAN [36] and BiCycleGAN [37]. The network T is an encoder-decoder network with residual connections [1], which is adapted from the implementation in [15]. The registration network is U-NET based [26] with residual connections in the encoder-path and the output path. In all residual connections, we use Instance Normalization Layer [33]. All networks were initialized by the Kaiming [11] initialization method. Full details about the architectures is provided in the supplementary material.\n\nThe experiments were conducted on single GeForce RTX 2080 Ti. We use Adam Optimizer [17] on a minibatch of size 12 with parameters lr = 1 \u00d7 e \u22124 , \u03b2 1 = 0.5 and \u03b2 2 = 0.999. We train our model for 200 epochs, and activate linear learning rate decay after 100 epochs.\n\n\nExperimental Results\n\nIn the following section we evaluate our approach and explore the interactions between R, T and the different loss terms we use.\n\nAll our experiments were conducted on a commercial dataset, which contains a collection of images of banana plants with different growing conditions and phenotype. The dataset contains 6100 image frames, where each frame consist of an RGB image, IR Image and Depth Image. The colored images are a 24bit Color Bitmap captured from a high-resolution sensor. The IR images are a 16bit gray-scale image, captured from a long-wave infrared (LWIR) sensor. Finally, the depth images were captured by Intel Real-Sense depth camera. The three sensors were calibrated, and an initial registration was applied based on affine transformation estimation via depth and controlled lab measurements. The misalignment in the dataset is due to depth variation within different objects in the scene, which the initial registration fails to handle. We split the dataset into training and test samples, where the test image were sampled with probability p = 0.1.\n\n\nEvaluation\n\nRegistration Accuracy Metric. We manually annotated 100 random pairs of test images. We tagged 10-15 pairs Figure 3: Annotation sample. An example of image demonstrating our annotations. We pick points from both the source image I a (RGB image on the left) and the target image I b (Thermal image on the right). The blue points are on salient objects and the red points are general points from the scene. We added several arrows to illustrate some matching points. Further, the geometry of each point is with respect to its corresponding image.\n\nof point landmarks on the source and target images which are notable and expected to match in the registration process (See Figure 3). Given a pair of test images, I a and I b , with a set of tagged pairs (x ai , y ai ), (x bi , y bi ) i\u2208[N ] , denote (x ai , y ai ) as deformed sample points (x ai , y ai ). The accuracy of the registration network R is simply the average Euclidean distance between the target points and the deformed source points:\nacc = 1 N i\u2208[N ] (x ai \u2212 x bi ) 2 + (y ai \u2212 y bi ) 2 . (7)\nFurthermore, we used two type of annotations. The first type of annotation is located over salient objects in the scene (the blue points in Figure 3). This is important because in most cases, down-stream tasks are affected mainly by the alignment of the main object in the scene in both modalities. The second annotation is performed by picking landmark points from all objects across the scene.\n\nQuantitative Evaluation. Due to limited access to source code and datasets of related works, we conduct several experiments that demonstrate the power of our method with respect to different aspects of previous works. As the crux of our work is the alleviation of the need for crossmodality similarity measures, we trained our network with commonly used cross-modality measures. In Table 1 we show the registration accuracy of our registration network R when trained with different loss terms. Specifically, we used Normalized Cross Correlation as it is frequently used in unsupervised multi-modal registration methods. Furthermore, we trained our network with Structural Similarity Index Metric (SSIM) on edges detected by Canny edgedetector [4]  edges of the deformed and the target image. As can be seen from Table 1, training the registration network R using prescribed cross-modality similarity measures do not perform well. Further, using these N CC produces noisy results, while using SSIM gives smooth but less accurate registration (see supplemental materials). Furthermore, we also tried using traditional descriptors such as SIFT [19] in order to match corresponding key points from the source and target image. We use these key points to register the source and target images by estimating the transformation parameters between them. However, these descriptors are not designed for multi-modal data, and hence they fail badly to be used on our dataset.\n\nInstead, we train a CycleGAN [36] network to translate between the two modalities at hand, without any supervision to match the ground truth. CycleGAN, like other unsupervised image-to-image translation networks, is not trained to generate images matching ground truth samples, thus, geometric transformation is not explicitly required from the translation network. Once trained, we use one of the generators in the CycleGAN, the one that maps between domain A to domain B to translate the input image I a onto modality B. Assuming this generator is both geometry preserving and translates well between the modalities, it is expected that it also match well between feature of the fake sample and the target image. Thus, we extracted SIFT descriptors from the generated images by the CycleGAN translation network, and extracted SIFT features from the target image I b . We then matched these features and estimated the needed spatial registration. The registration accuracy using this method is significantly better than directly using SIFT [19] features on the input image I a . The results are shown in Table 1. Further visual results and details demonstrating this method are provided in the supplementary material.\n\nQualitative Evaluation. Figure 4 shows that our registration network successfully aligns images from different pair of modalities and handles different alignment cases. For example, the banana leaves in the first raw in Figure 4a are well-aligned in the two modalities. Our registration network maintains this alignment and only deforms the background for full alignment between the images. This can be seen from the deformation field visualization [2], where little deformation is applied on the banana plant, while most of the deformation is applied on the background. Furthermore, in the last row in Figure 4a, there is little depth variation in the scene because the banana plant is small, hence a uniform deformation is applied across the entire image. To help measuring the alignment success, we overlay (with semi-transparency) the plant in image B on top of both image A before and after the registration. This means that the silhouette has the same spatial location in all images (the original image B, image A before and after the registration). Lastly, we achieve similar success in the registration between RGB and IR images (see Figure 4b).\n\nIt is worth mentioning that in some cases, the deformation field points to regions outside the source image. In those cases, we simply sample zero values. This happens because the the target image content (i.e., I b ) in these regions is not available in the source image (i.e., I a ). We provide more qualitative results in the supplemental materials.\n\n\nAblation Study\n\nNext, we present a series of ablation studies that analyze the effectiveness of different aspects in our work. First, we show that training both compositions (i.e our presented two training flows) of T and R indeed encourages a geometric preserving translator T . Additionally, we analyze the impact of the different loss terms on the registration network's accuracy. We further show the effectiveness of the bilateral filtering, and that it indeed improves the registration accuracy. All experiments, unless otherwise stated, were conducted without the bilateral filtering.\n\nGeometric-Preserving Translation Network.\n\nTo evaluate the impact of training of T and R simultaneously with the two training flows proposed in Figure 2, we compare the registration accuracy of our method with that of training models with either T \u2022 R or R \u2022 T . As can be seen from Figure 5, training both combinations yields a substantial improvement in the registration accuracy (shown in blue), compared to each training flow (i.e., T \u2022 R and R \u2022 T ) separately. Moreover, while the reconstruction loss of T \u2022R (shown in read) is lowest among the three options, it does not necessarily indicate a better registration. This is because in this setting the translation network T implicitly performs both the alignment and translation tasks. Conversely, when training with R \u2022 T only (shown in green), the network R is unstable and at some point it starts to alternate pixel val-  The third column is the registered image, i.e the image I a after deformation. The deformation field (4th column) is visualized using the standard optical-flow visualization [2]. Finally, we segment the salient object in I b and overlay it (with opacity 25%) in the same spatial location onto the image before and after registration (last two columns).\n\nues, essentially taking on the role of a translation network.\n\nSince R is only geometry-aware by design it fails to generate good samples. This is indicated by how fast the discriminator detects that the generated samples are fake (i.e., the adversarial loss decays fast). Visual results are provided in the supplementary materials.\n\nLoss ablation. It has been shown in previous works [37,13,36] that training an image-to-image translation network with both a reconstruction and an adversarial loss yields bet-ter results. In particular, the reconstruction loss stabilizes the training process and improves the vividness of the output images, while the adversarial loss encourages the generation of samples matching the real-data distribution.\n\nThe main objective of our work is the production of a registration network. Therefore, we seek to understand the impact of both losses (reconstruction and adversarial) on the registration network. To understand the impact of each loss, we train our model with different settings: each time we The loss values are shown for T \u2022 R (red), R \u2022 T (green) and ours (blue). As can be seen, the registration accuracy is best using our method. In T \u2022 R, the reconstruction loss is the lowest, however, the registration is inaccurate because a significant portion of the registration task is implicitly performed by the translator T . Further, the composition R \u2022 T is unstable because at some point, the registration network R starts alternating pixels values, which is detected by the discriminator (see the dip in (c)).  fix either R or T 's weights with respect to one of the loss functions. The registration accuracy is presented in Table  2. Please refer to the supplementary material for qualitative results. As can be seen in these figures, training R only with respect to the reconstruction loss leads to overly sharp, but unrealistic images where the deformation field creates noisy artifacts. On the other hand, training R only with respect to the adversarial loss creates realistic images, but with inexact alignment. This is especially evident in Table  2 where training R with respect to the reconstruction loss achieves a significant improvement in the alignment, and the best accuracy is obtained when the loss terms are both used to update all the networks weights. Bilateral Filtering Effectiveness Using bilateral filtering to weigh the smoothness loss allows us, in effect, to encourage piece-wise smoothness on the deformation map. As can be seen in Table 3, this enhances the precision of the registration. These results suggest that using segmentation maps for controlling the smoothness loss term could be beneficial.\n\n\nMethod\n\nTest Acc. Train Acc.  \n\n\nSummary and Conclusions\n\nWe presented an unsupervised multi-modal image registration technique based on image-to-image translation network. Our method, does not require any direct comparison between images of different modalities. Instead, we developed a geometry preserving image-to-image translation network which allows comparing the deformed and target image using simple mono-modality metrics. The geometric preserving translation network was made possible by a novel training scheme, which alternates and combines two different flows to train the spatial transformation. We further showed that using adversarial learning, along with mono-modality metric, we are able to produce smooth and accurate registration results even when there is only little training data.\n\nWe believe that geometric preserving generators can be useful for many other applications other than image registration. In the future, we would like to continue to explore the idea of alternate training a number of layers or operators in different flows to encourage them being commutative as means to achieve certain non-trivial properties.\n\nL\ncGAN (T, R, D) =E [log (D (I b , I a ))] + E [log(1 \u2212 D(O RT , I a ))] + E [log(1 \u2212 D(O T R , I a ))] ,\n\n\nT, R). Furthermore, in our experiments, we set \u03bb R = 100 and \u03bb S = 200.\n\nFigure 4 :\n4Qualitative Evaluation. We show sample results on the registration between two pairs of domains; (a) RGB to Depth registration and (b) RGB to IR registration. In the first two columns we show the corresponding images I a and I b .\n\nFigure 5 :\n5Composition Ablation Study. We show the values of the (a) Registration Accuracy, (b) Reconstruction loss, (c) Adversarial Loss and (d) cGAN Loss. The x-axis in all figures is the epoch number.\n\n\nfrom both the deformed and target image. Finally, we attempt to train our registration network R by maximizing the Normalized Cross Correlation between theMethod \n\nSalient Objects Full Scene \n\nUnregistered \n30.3 \n35.45 \nCG [36] + SIFT [19] \n17.9 \n34.74 \nR + SSIM On Edges \n26.12 \n28.41 \nR + NCC On Edges \n16.78 \n27.41 \nR + NCC \n15.8 \n29.91 \nR + T (Ours) \n6.27 \n6.93 \n\nTable 1: Registration Accuracy results. Registration ac-\ncuracy for various similarity measurements. Unregistered \n(first row), represents the misalignment in the dataset. CG \n+ SIFT is training a CycleGAN and using SIFT features on \nthe generated images. Finally, we also show the registration \naccuracy of training our registration network with different \nloss terms. \n\n\n\nTable 2 :\n2Loss ablation results. Columns denote modules trained with a GAN loss term. Rows denote modules trained with an L1 loss term. We do not report results in the degenerate case where only one of the modules is trained directly with any loss terms. X denotes cases where the training diverges and leads to nonsensical results. For example, the result in the second row and first column represents the registration accuracy achieved when module R's weights are updated with respect to the cGAN loss and module T with respect to the reconstruction loss term.\n\nTable 3 :\n3Smoothness Regularization. Effect of bilateral filtering on registration accuracy. We show the registration accuracy on annotated test samples, and annotated train samples. As can be seen, with bilateral filtering there's less data-fitting, and the test and train accuracy relatively match.\nAcknowledgmentsThis research was supported by a generic RD program of the Israeli innovation authority, and the Phenomics consortium.\nDeep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2015. 5\n\nA database and evaluation methodology for optical flow. Simon Baker, J P Scharstein, Stefan Lewis, Michael J Roth, Richard Black, Szeliski, Int. J. Comput. Vision. 921Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth, Michael J. Black, and Richard Szeliski. A database and eval- uation methodology for optical flow. Int. J. Comput. Vision, 92(1):1-31, Mar. 2011. 6, 7\n\nMutual information neural estimation. Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, Devon Hjelm, PMLR. 2Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy and Andreas Krausethe 35th International Conference on Machine LearningStockholmsmssan, Stockholm Sweden80Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajesh- war, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 531-540, Stockholmsmssan, Stockholm Sweden, 10- 15 Jul 2018. PMLR. 2\n\nA computational approach to edge detection. J Canny, IEEE Trans. Pattern Anal. Mach. Intell. 86J Canny. A computational approach to edge detection. IEEE Trans. Pattern Anal. Mach. Intell., 8(6):679-698, June 1986. 5\n\nDeep learning based intermodality image registration supervised by intra-modality similarity. Xiaohuan Cao, Jianhuan Yang, Li Wang, Zhong Xue, Qian Wang, Dinggang Shen, Machine learning in medical imaging. MLMI. 110462Xiaohuan Cao, Jianhuan Yang, Li Wang, Zhong Xue, Qian Wang, and Dinggang Shen. Deep learning based inter- modality image registration supervised by intra-modality similarity. Machine learning in medical imaging. MLMI, 11046:55-63, 2018. 2\n\nUnsupervised learning for fast probabilistic diffeomorphic registration. Adrian V Dalca, Guha Balakrishnan, John V Guttag, Mert R Sabuncu, In MICCAI. 2Adrian V. Dalca, Guha Balakrishnan, John V. Guttag, and Mert R. Sabuncu. Unsupervised learning for fast probabilis- tic diffeomorphic registration. In MICCAI, 2018. 2\n\nEnd-to-end unsupervised deformable image registration with a convolutional neural network. D Bob, Floris F De Vos, Max A Berendsen, Marius Viergever, Ivana Staring, M Jorge Isgum ; In, Tal Cardoso, Gustavo Arbel, Carneiro, F Tanveer, Jo\u00e3o Syeda-Mahmood, R S Manuel, Mehdi Tavares, Andrew P Moradi, Hayit Bradley, Jo\u00e3o Paulo Greenspan, Anant Papa, Jacinto C Madabhushi, Jaime S Nascimento, Vasileios Cardoso, Zhi Belagiannis, Lu, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support -Third International Workshop, DLMIA 2017, and 7th International Workshop. Qu\u00e9bec City, QC, CanadaSpringer10553ProceedingsBob D. de Vos, Floris F. Berendsen, Max A. Viergever, Marius Staring, and Ivana Isgum. End-to-end unsuper- vised deformable image registration with a convolutional neural network. In M. Jorge Cardoso, Tal Arbel, Gus- tavo Carneiro, Tanveer F. Syeda-Mahmood, Jo\u00e3o Manuel R. S. Tavares, Mehdi Moradi, Andrew P. Bradley, Hayit Greenspan, Jo\u00e3o Paulo Papa, Anant Madabhushi, Jacinto C. Nascimento, Jaime S. Cardoso, Vasileios Belagiannis, and Zhi Lu, editors, Deep Learning in Medical Image Analy- sis and Multimodal Learning for Clinical Decision Support -Third International Workshop, DLMIA 2017, and 7th In- ternational Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Qu\u00e9bec City, QC, Canada, September 14, 2017, Proceedings, volume 10553 of Lecture Notes in Computer Science, pages 204-212. Springer, 2017. 2\n\nAdversarial learning for mono-or multimodal registration. Jingfan Fan, Xiaohuan Cao, Qian Wang, Medical Image Analysis. 582101545Pew-Thian Yap, and Dinggang ShenJingfan Fan, Xiaohuan Cao, Qian Wang, Pew-Thian Yap, and Dinggang Shen. Adversarial learning for mono-or multi- modal registration. Medical Image Analysis, 58:101545, 2019. 2\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. QIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahra- mani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.\n\nWeinberger, Advances in Neural Information Processing Systems. Curran Associates, Inc273Weinberger, editors, Advances in Neural Information Pro- cessing Systems 27, pages 2672-2680. Curran Associates, Inc., 2014. 2, 3\n\nDeep learning in medical image registration: A survey. Grant Haskins, Uwe Kruger, Pingkun Yan, Grant Haskins, Uwe Kruger, and Pingkun Yan. Deep learn- ing in medical image registration: A survey, 2019. 1\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV '15. the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV '15Washington, DC, USAIEEE Computer SocietyKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level perfor- mance on imagenet classification. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV '15, pages 1026-1034, Washington, DC, USA, 2015. IEEE Computer Society. 5\n\nMultimodal unsupervised image-to-image translation. Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, ECCV. Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018. 2\n\nImage-to-image translation with conditional adversarial networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, 57Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adver- sarial networks. CVPR, 2017. 2, 5, 7\n\nSpatial transformer networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman GarnettMontreal, Quebec, CanadaMax Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neu- ral Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2017-2025, 2015. 2\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, European Conference on Computer Vision. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016. 5\n\nUnsupervised deformable image registration using cycle-consistent CNN. Boah Kim, Jieun Kim, June-Goo Lee, Dong Hwan Kim, Seong Ho Park, Jong Chul Ye, Medical Image Computing and Computer Assisted Intervention -MICCAI 2019 -22nd International Conference. Dinggang Shen, Tianming Liu, Terry M. Peters, Lawrence H. Staib, Caroline Essert, Sean Zhou, Pew-Thian Yap, and Ali KhanShenzhen, ChinaSpringer11769Proceedings, Part VIBoah Kim, Jieun Kim, June-Goo Lee, Dong Hwan Kim, Seong Ho Park, and Jong Chul Ye. Unsupervised deformable image registration using cycle-consistent CNN. In Dinggang Shen, Tianming Liu, Terry M. Peters, Lawrence H. Staib, Caroline Essert, Sean Zhou, Pew-Thian Yap, and Ali Khan, editors, Medical Image Computing and Computer Assisted Intervention -MICCAI 2019 -22nd International Confer- ence, Shenzhen, China, October 13-17, 2019, Proceedings, Part VI, volume 11769 of Lecture Notes in Computer Sci- ence, pages 166-174. Springer, 2019. 2\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, International Conference on Learning Representations. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learn- ing Representations, 12 2014. 5\n\nImage-and-spatial transformer networks for structure-guided image registration. C H Matthew, Ozan Lee, Andreas Oktay, Michiel Schuh, Ben Glocker ; Tianming Schaap, Terry M Liu, Lawrence H Peters, Caroline Staib, Sean Essert, Zhou, Medical Image Computing and Computer Assisted Intervention -MICCAI 2019 -22nd International Conference. Pew-Thian Yap, and Ali KhanShenzhen, ChinaSpringer11765Proceedings, Part IIMatthew C. H. Lee, Ozan Oktay, Andreas Schuh, Michiel Schaap, and Ben Glocker. Image-and-spatial transformer networks for structure-guided image registration. In Ding- gang Shen, Tianming Liu, Terry M. Peters, Lawrence H. Staib, Caroline Essert, Sean Zhou, Pew-Thian Yap, and Ali Khan, editors, Medical Image Computing and Computer As- sisted Intervention -MICCAI 2019 -22nd International Con- ference, Shenzhen, China, October 13-17, 2019, Proceed- ings, Part II, volume 11765 of Lecture Notes in Computer Science, pages 337-345. Springer, 2019. 2\n\nDistinctive image features from scaleinvariant keypoints. David G Lowe, Int. J. Comput. Vision. 602David G. Lowe. Distinctive image features from scale- invariant keypoints. Int. J. Comput. Vision, 60(2):91-110, Nov. 2004. 6\n\nDeformable medical image registration using generative adversarial networks. Dwarikanath Mahapatra, J Bhavna, Suman Antony, Rahil Sedai, Garnavi, 15th IEEE International Symposium on Biomedical Imaging, ISBI 2018, Washington. DC, USADwarikanath Mahapatra, Bhavna J. Antony, Suman Sedai, and Rahil Garnavi. Deformable medical image registration using generative adversarial networks. In 15th IEEE Interna- tional Symposium on Biomedical Imaging, ISBI 2018, Wash- ington, DC, USA, April 4-7, 2018, pages 1449-1453. IEEE, 2018. 2\n\nJoint registration and segmentation of xray images using generative adversarial networks. Dwarikanath Mahapatra, Zongyuan Ge, Suman Sedai, Rajib Chakravorty, Machine Learning in Medical Imaging. Yinghuan Shi, Heung-Il Suk, and Mingxia LiuSpringer11046Dwarikanath Mahapatra, Zongyuan Ge, Suman Sedai, and Rajib Chakravorty. Joint registration and segmentation of xray images using generative adversarial networks. In Yinghuan Shi, Heung-Il Suk, and Mingxia Liu, editors, Ma- chine Learning in Medical Imaging, volume 11046 of Lec- ture Notes in Computer Science, pages 73-80. Springer, 1 2018. 2\n\nConditional generative adversarial nets. CoRR, abs/1411.1784. Mehdi Mirza, Simon Osindero, 24Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014. 2, 3, 4\n\nDeformable image registration in radiation therapy. Seungjong Oh, Siyong Kim, Radiation oncology journal. 352101Seungjong Oh and Siyong Kim. Deformable image reg- istration in radiation therapy. Radiation oncology journal, 35(2):101, 2017. 1\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Autodiff Workshop. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic dif- ferentiation in PyTorch. In NIPS Autodiff Workshop, 2017. 5\n\nUnsupervised deformable registration for multi-modal images via disentangled representations. Chen Qin, Bibo Shi, Rui Liao, Tommaso Mansi, Daniel Rueckert, Ali Kamen, Information Processing in Medical Imaging -26th International Conference, IPMI 2019. Albert C. S. Chung, James C. Gee, Paul A. Yushkevich, and Siqi BaoHong Kong, ChinaSpringer11492ProceedingsChen Qin, Bibo Shi, Rui Liao, Tommaso Mansi, Daniel Rueckert, and Ali Kamen. Unsupervised deformable regis- tration for multi-modal images via disentangled representa- tions. In Albert C. S. Chung, James C. Gee, Paul A. Yushke- vich, and Siqi Bao, editors, Information Processing in Med- ical Imaging -26th International Conference, IPMI 2019, Hong Kong, China, June 2-7, 2019, Proceedings, volume 11492 of Lecture Notes in Computer Science, pages 249- 261. Springer, 2019. 2\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, arXiv:1505.04597[cs.CV]).5Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer9351O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolu- tional networks for biomedical image segmentation. In Med- ical Image Computing and Computer-Assisted Intervention (MICCAI), volume 9351 of LNCS, pages 234-241. Springer, 2015. (available on arXiv:1505.04597 [cs.CV]). 5\n\nRadiotherapy planning using mri. A Maria, Geoffrey S Schmidt, Payne, Physics in Medicine & Biology. 6022323Maria A Schmidt and Geoffrey S Payne. Radiother- apy planning using mri. Physics in Medicine & Biology, 60(22):R323, 2015. 1\n\nRegnet: Multimodal sensor registration using deep neural networks. N Schneider, F Piewak, C Stiller, U Franke, 2017 IEEE Intelligent Vehicles Symposium (IV). N. Schneider, F. Piewak, C. Stiller, and U. Franke. Reg- net: Multimodal sensor registration using deep neural net- works. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages 1803-1810, June 2017. 1\n\nUnsupervised deformable image registration with fully connected generative neural network. A Sheikhjafari, Michelle Noga, Kumaradevan Punithakumar, and Nilanjan RayA. Sheikhjafari, Michelle Noga, Kumaradevan Punithaku- mar, and Nilanjan Ray. Unsupervised deformable image registration with fully connected generative neural network. 2018. 2\n\nMultimodal and multi-spectral registration for natural images. Xiaoyong Shen, Li Xu, Qi Zhang, Jiaya Jia, Computer Vision -ECCV 2014 -13th European Conference. Zurich, SwitzerlandProceedings, Part IVXiaoyong Shen, Li Xu, Qi Zhang, and Jiaya Jia. Multi- modal and multi-spectral registration for natural images. In Computer Vision -ECCV 2014 -13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV, pages 309-324, 2014. 1\n\nAn unsupervised network for fast microscopic image registration. Chang Shu, Xi Chen, Qiwei Xie, Hua Han, Medical Imaging 2018: Digital Pathology. John E. Tomaszewski and Metin N. Gurcan10581SPIEChang Shu, Xi Chen, Qiwei Xie, and Hua Han. An unsu- pervised network for fast microscopic image registration. In John E. Tomaszewski and Metin N. Gurcan, editors, Medical Imaging 2018: Digital Pathology, volume 10581, pages 363 -370. International Society for Optics and Photonics, SPIE, 2018. 2\n\nBilateral filtering for gray and color images. C Tomasi, R Manduchi, Proceedings of the Sixth International Conference on Computer Vision, ICCV '98. the Sixth International Conference on Computer Vision, ICCV '98Washington, DC, USAIEEE Computer Society839C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In Proceedings of the Sixth International Con- ference on Computer Vision, ICCV '98, pages 839-, Wash- ington, DC, USA, 1998. IEEE Computer Society. 4\n\nInstance normalization: The missing ingredient for fast stylization. ArXiv. Dmitry Ulyanov, Andrea Vedaldi, Victor S Lempitsky, abs/1607.08022Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing ingredient for fast styl- ization. ArXiv, abs/1607.08022, 2016. 5\n\nFIRE: unsupervised bi-directional inter-modality registration using deep networks. Chengjia Wang, Giorgos Papanastasiou, Agisilaos Chartsias, Grzegorz Jacenkow, A Sotirios, Heye Tsaftaris, Zhang, abs/1907.05062CoRRChengjia Wang, Giorgos Papanastasiou, Agisilaos Chartsias, Grzegorz Jacenkow, Sotirios A. Tsaftaris, and Heye Zhang. FIRE: unsupervised bi-directional inter-modality registration using deep networks. CoRR, abs/1907.05062, 2019. 2\n\nMultimodal image alignment through a multiscale chain of neural networks with application to remote sensing. Armand Zampieri, Guillaume Charpiat, Nicolas Girard, Yuliya Tarabalka, ECCV. Armand Zampieri, Guillaume Charpiat, Nicolas Girard, and Yuliya Tarabalka. Multimodal image alignment through a multiscale chain of neural networks with application to re- mote sensing. In ECCV, 2018. 1\n\nUnpaired image-to-image translation using cycleconsistent adversarial networkss. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, 2017 IEEE International Conference on. 67Computer Vision (ICCVJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networkss. In Computer Vision (ICCV), 2017 IEEE International Conference on, 2017. 2, 5, 6, 7\n\nToward multimodal image-to-image translation. Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, Eli Shechtman, Advances in Neural Information Processing Systems. 57Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar- rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To- ward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, 2017. 2, 5, 7\n\nImage registration methods: a survey. Barbara Zitov, Jan Flusser, Image and Vision Computing. 2111Barbara Zitov and Jan Flusser. Image registration methods: a survey. Image and Vision Computing, 21(11):977 -1000, 2003. 1\n", "annotations": {"author": "[{\"end\":224,\"start\":98},{\"end\":356,\"start\":225},{\"end\":483,\"start\":357},{\"end\":615,\"start\":484},{\"end\":747,\"start\":616},{\"end\":880,\"start\":748}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":103},{\"end\":239,\"start\":233},{\"end\":366,\"start\":361},{\"end\":498,\"start\":489},{\"end\":630,\"start\":623},{\"end\":763,\"start\":755}]", "author_first_name": "[{\"end\":102,\"start\":98},{\"end\":232,\"start\":225},{\"end\":360,\"start\":357},{\"end\":488,\"start\":484},{\"end\":620,\"start\":616},{\"end\":622,\"start\":621},{\"end\":754,\"start\":748}]", "author_affiliation": "[{\"end\":223,\"start\":109},{\"end\":355,\"start\":241},{\"end\":482,\"start\":368},{\"end\":614,\"start\":500},{\"end\":746,\"start\":632},{\"end\":879,\"start\":765}]", "title": "[{\"end\":95,\"start\":1},{\"end\":975,\"start\":881}]", "venue": null, "abstract": "[{\"end\":2167,\"start\":977}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2754,\"start\":2750},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2757,\"start\":2754},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3673,\"start\":3669},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3965,\"start\":3961},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4077,\"start\":4073},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4533,\"start\":4529},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4536,\"start\":4533},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4790,\"start\":4786},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4826,\"start\":4823},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4829,\"start\":4826},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4832,\"start\":4829},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4835,\"start\":4832},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4837,\"start\":4835},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5014,\"start\":5010},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5017,\"start\":5014},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5019,\"start\":5017},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5316,\"start\":5312},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5356,\"start\":5353},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5407,\"start\":5403},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5410,\"start\":5407},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5522,\"start\":5519},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5818,\"start\":5814},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5821,\"start\":5818},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5824,\"start\":5821},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5827,\"start\":5824},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6616,\"start\":6613},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6619,\"start\":6616},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6756,\"start\":6752},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7474,\"start\":7470},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8227,\"start\":8223},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8605,\"start\":8602},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9000,\"start\":8996},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13084,\"start\":13081},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13087,\"start\":13084},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14830,\"start\":14827},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14920,\"start\":14916},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18167,\"start\":18163},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18684,\"start\":18680},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18749,\"start\":18745},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18764,\"start\":18760},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18784,\"start\":18780},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18859,\"start\":18856},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18909,\"start\":18905},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18955,\"start\":18951},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19092,\"start\":19088},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19143,\"start\":19139},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19336,\"start\":19332},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22824,\"start\":22821},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23223,\"start\":23219},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23577,\"start\":23573},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24589,\"start\":24585},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25216,\"start\":25213},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27924,\"start\":27921},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28490,\"start\":28486},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28493,\"start\":28490},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28496,\"start\":28493}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32034,\"start\":31928},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32108,\"start\":32035},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32352,\"start\":32109},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32558,\"start\":32353},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33301,\"start\":32559},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33866,\"start\":33302},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34169,\"start\":33867}]", "paragraph": "[{\"end\":2874,\"start\":2183},{\"end\":3295,\"start\":2929},{\"end\":4078,\"start\":3297},{\"end\":4838,\"start\":4080},{\"end\":5653,\"start\":4840},{\"end\":6195,\"start\":5655},{\"end\":6938,\"start\":6197},{\"end\":6979,\"start\":6940},{\"end\":7041,\"start\":6981},{\"end\":7167,\"start\":7043},{\"end\":7243,\"start\":7169},{\"end\":8209,\"start\":7261},{\"end\":8578,\"start\":8211},{\"end\":8973,\"start\":8580},{\"end\":9535,\"start\":8975},{\"end\":10002,\"start\":9548},{\"end\":10926,\"start\":10220},{\"end\":10984,\"start\":10928},{\"end\":11820,\"start\":10986},{\"end\":12091,\"start\":11822},{\"end\":12864,\"start\":12102},{\"end\":13197,\"start\":12871},{\"end\":13585,\"start\":13222},{\"end\":13757,\"start\":13587},{\"end\":14237,\"start\":13914},{\"end\":14330,\"start\":14277},{\"end\":14396,\"start\":14356},{\"end\":15069,\"start\":14398},{\"end\":15531,\"start\":15150},{\"end\":15637,\"start\":15550},{\"end\":15860,\"start\":15639},{\"end\":16357,\"start\":15905},{\"end\":16687,\"start\":16359},{\"end\":16863,\"start\":16689},{\"end\":17226,\"start\":16905},{\"end\":17501,\"start\":17284},{\"end\":17891,\"start\":17565},{\"end\":18025,\"start\":17911},{\"end\":18131,\"start\":18076},{\"end\":18423,\"start\":18133},{\"end\":18457,\"start\":18425},{\"end\":18609,\"start\":18542},{\"end\":19246,\"start\":18636},{\"end\":19514,\"start\":19248},{\"end\":19667,\"start\":19539},{\"end\":20610,\"start\":19669},{\"end\":21169,\"start\":20625},{\"end\":21621,\"start\":21171},{\"end\":22076,\"start\":21681},{\"end\":23542,\"start\":22078},{\"end\":24762,\"start\":23544},{\"end\":25917,\"start\":24764},{\"end\":26271,\"start\":25919},{\"end\":26864,\"start\":26290},{\"end\":26907,\"start\":26866},{\"end\":28099,\"start\":26909},{\"end\":28162,\"start\":28101},{\"end\":28433,\"start\":28164},{\"end\":28844,\"start\":28435},{\"end\":30777,\"start\":28846},{\"end\":30810,\"start\":30788},{\"end\":31583,\"start\":30838},{\"end\":31927,\"start\":31585}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10219,\"start\":10003},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12870,\"start\":12865},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13913,\"start\":13758},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14276,\"start\":14238},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14355,\"start\":14331},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15117,\"start\":15070},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15149,\"start\":15117},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15549,\"start\":15532},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16904,\"start\":16864},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17283,\"start\":17227},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17564,\"start\":17502},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18075,\"start\":18026},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18541,\"start\":18458},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21680,\"start\":21622}]", "table_ref": "[{\"end\":22467,\"start\":22460},{\"end\":22897,\"start\":22890},{\"end\":24656,\"start\":24649},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29782,\"start\":29774},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30204,\"start\":30196},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30614,\"start\":30607}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2181,\"start\":2169},{\"end\":2903,\"start\":2877},{\"end\":2927,\"start\":2906},{\"attributes\":{\"n\":\"2.\"},\"end\":7259,\"start\":7246},{\"attributes\":{\"n\":\"3.\"},\"end\":9546,\"start\":9538},{\"attributes\":{\"n\":\"4.\"},\"end\":12100,\"start\":12094},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13220,\"start\":13200},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15903,\"start\":15863},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17909,\"start\":17894},{\"attributes\":{\"n\":\"4.4.\"},\"end\":18634,\"start\":18612},{\"attributes\":{\"n\":\"5.\"},\"end\":19537,\"start\":19517},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20623,\"start\":20613},{\"attributes\":{\"n\":\"5.2.\"},\"end\":26288,\"start\":26274},{\"end\":30786,\"start\":30780},{\"attributes\":{\"n\":\"6.\"},\"end\":30836,\"start\":30813},{\"end\":31930,\"start\":31929},{\"end\":32120,\"start\":32110},{\"end\":32364,\"start\":32354},{\"end\":33312,\"start\":33303},{\"end\":33877,\"start\":33868}]", "table": "[{\"end\":33301,\"start\":32716}]", "figure_caption": "[{\"end\":32034,\"start\":31931},{\"end\":32108,\"start\":32037},{\"end\":32352,\"start\":32122},{\"end\":32558,\"start\":32366},{\"end\":32716,\"start\":32561},{\"end\":33866,\"start\":33314},{\"end\":34169,\"start\":33879}]", "figure_ref": "[{\"end\":2958,\"start\":2950},{\"end\":5424,\"start\":5416},{\"end\":6170,\"start\":6162},{\"end\":9707,\"start\":9699},{\"end\":10259,\"start\":10251},{\"end\":11238,\"start\":11230},{\"end\":11417,\"start\":11408},{\"end\":17702,\"start\":17690},{\"end\":20740,\"start\":20732},{\"end\":21304,\"start\":21295},{\"end\":21829,\"start\":21821},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24796,\"start\":24788},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24993,\"start\":24984},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25376,\"start\":25367},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25915,\"start\":25906},{\"end\":27018,\"start\":27010},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27157,\"start\":27149}]", "bib_author_first_name": "[{\"end\":34620,\"start\":34615},{\"end\":34629,\"start\":34628},{\"end\":34631,\"start\":34630},{\"end\":34650,\"start\":34644},{\"end\":34665,\"start\":34658},{\"end\":34667,\"start\":34666},{\"end\":34681,\"start\":34674},{\"end\":34980,\"start\":34973},{\"end\":34988,\"start\":34981},{\"end\":35007,\"start\":34999},{\"end\":35020,\"start\":35017},{\"end\":35039,\"start\":35032},{\"end\":35053,\"start\":35047},{\"end\":35067,\"start\":35062},{\"end\":35084,\"start\":35079},{\"end\":35740,\"start\":35739},{\"end\":36014,\"start\":36006},{\"end\":36028,\"start\":36020},{\"end\":36037,\"start\":36035},{\"end\":36049,\"start\":36044},{\"end\":36059,\"start\":36055},{\"end\":36074,\"start\":36066},{\"end\":36449,\"start\":36443},{\"end\":36451,\"start\":36450},{\"end\":36463,\"start\":36459},{\"end\":36482,\"start\":36478},{\"end\":36484,\"start\":36483},{\"end\":36497,\"start\":36493},{\"end\":36499,\"start\":36498},{\"end\":36781,\"start\":36780},{\"end\":36793,\"start\":36787},{\"end\":36795,\"start\":36794},{\"end\":36807,\"start\":36804},{\"end\":36809,\"start\":36808},{\"end\":36827,\"start\":36821},{\"end\":36844,\"start\":36839},{\"end\":36855,\"start\":36854},{\"end\":36861,\"start\":36856},{\"end\":36877,\"start\":36874},{\"end\":36894,\"start\":36887},{\"end\":36913,\"start\":36912},{\"end\":36927,\"start\":36923},{\"end\":36944,\"start\":36943},{\"end\":36946,\"start\":36945},{\"end\":36960,\"start\":36955},{\"end\":36976,\"start\":36970},{\"end\":36978,\"start\":36977},{\"end\":36992,\"start\":36987},{\"end\":37006,\"start\":37002},{\"end\":37012,\"start\":37007},{\"end\":37029,\"start\":37024},{\"end\":37043,\"start\":37036},{\"end\":37045,\"start\":37044},{\"end\":37063,\"start\":37058},{\"end\":37065,\"start\":37064},{\"end\":37087,\"start\":37078},{\"end\":37100,\"start\":37097},{\"end\":38221,\"start\":38214},{\"end\":38235,\"start\":38227},{\"end\":38245,\"start\":38241},{\"end\":38525,\"start\":38522},{\"end\":38542,\"start\":38538},{\"end\":38563,\"start\":38558},{\"end\":38575,\"start\":38571},{\"end\":38585,\"start\":38580},{\"end\":38607,\"start\":38600},{\"end\":38620,\"start\":38615},{\"end\":38638,\"start\":38632},{\"end\":39216,\"start\":39211},{\"end\":39229,\"start\":39226},{\"end\":39245,\"start\":39238},{\"end\":39461,\"start\":39454},{\"end\":39473,\"start\":39466},{\"end\":39489,\"start\":39481},{\"end\":39499,\"start\":39495},{\"end\":40079,\"start\":40076},{\"end\":40094,\"start\":40087},{\"end\":40105,\"start\":40100},{\"end\":40119,\"start\":40116},{\"end\":40331,\"start\":40324},{\"end\":40346,\"start\":40339},{\"end\":40359,\"start\":40352},{\"end\":40372,\"start\":40366},{\"end\":40374,\"start\":40373},{\"end\":40568,\"start\":40565},{\"end\":40585,\"start\":40580},{\"end\":40602,\"start\":40596},{\"end\":40619,\"start\":40614},{\"end\":41324,\"start\":41318},{\"end\":41343,\"start\":41334},{\"end\":41353,\"start\":41351},{\"end\":41648,\"start\":41644},{\"end\":41659,\"start\":41654},{\"end\":41673,\"start\":41665},{\"end\":41683,\"start\":41679},{\"end\":41688,\"start\":41684},{\"end\":41699,\"start\":41694},{\"end\":41718,\"start\":41709},{\"end\":42588,\"start\":42580},{\"end\":42602,\"start\":42597},{\"end\":42884,\"start\":42883},{\"end\":42886,\"start\":42885},{\"end\":42900,\"start\":42896},{\"end\":42913,\"start\":42906},{\"end\":42928,\"start\":42921},{\"end\":42958,\"start\":42936},{\"end\":42972,\"start\":42967},{\"end\":42974,\"start\":42973},{\"end\":42988,\"start\":42980},{\"end\":42990,\"start\":42989},{\"end\":43007,\"start\":42999},{\"end\":43019,\"start\":43015},{\"end\":43826,\"start\":43821},{\"end\":43828,\"start\":43827},{\"end\":44077,\"start\":44066},{\"end\":44090,\"start\":44089},{\"end\":44104,\"start\":44099},{\"end\":44118,\"start\":44113},{\"end\":44618,\"start\":44607},{\"end\":44638,\"start\":44630},{\"end\":44648,\"start\":44643},{\"end\":44661,\"start\":44656},{\"end\":45180,\"start\":45175},{\"end\":45193,\"start\":45188},{\"end\":45376,\"start\":45367},{\"end\":45387,\"start\":45381},{\"end\":45600,\"start\":45596},{\"end\":45612,\"start\":45609},{\"end\":45627,\"start\":45620},{\"end\":45645,\"start\":45638},{\"end\":45660,\"start\":45654},{\"end\":45674,\"start\":45667},{\"end\":45689,\"start\":45683},{\"end\":45700,\"start\":45695},{\"end\":45716,\"start\":45712},{\"end\":45729,\"start\":45725},{\"end\":46080,\"start\":46076},{\"end\":46090,\"start\":46086},{\"end\":46099,\"start\":46096},{\"end\":46113,\"start\":46106},{\"end\":46127,\"start\":46121},{\"end\":46141,\"start\":46138},{\"end\":46883,\"start\":46882},{\"end\":46898,\"start\":46897},{\"end\":46909,\"start\":46908},{\"end\":47334,\"start\":47333},{\"end\":47350,\"start\":47342},{\"end\":47352,\"start\":47351},{\"end\":47601,\"start\":47600},{\"end\":47614,\"start\":47613},{\"end\":47624,\"start\":47623},{\"end\":47635,\"start\":47634},{\"end\":47987,\"start\":47986},{\"end\":48010,\"start\":48002},{\"end\":48308,\"start\":48300},{\"end\":48317,\"start\":48315},{\"end\":48324,\"start\":48322},{\"end\":48337,\"start\":48332},{\"end\":48764,\"start\":48759},{\"end\":48772,\"start\":48770},{\"end\":48784,\"start\":48779},{\"end\":48793,\"start\":48790},{\"end\":49234,\"start\":49233},{\"end\":49244,\"start\":49243},{\"end\":49749,\"start\":49743},{\"end\":49765,\"start\":49759},{\"end\":49781,\"start\":49775},{\"end\":49783,\"start\":49782},{\"end\":50060,\"start\":50052},{\"end\":50074,\"start\":50067},{\"end\":50099,\"start\":50090},{\"end\":50119,\"start\":50111},{\"end\":50131,\"start\":50130},{\"end\":50146,\"start\":50142},{\"end\":50529,\"start\":50523},{\"end\":50549,\"start\":50540},{\"end\":50567,\"start\":50560},{\"end\":50582,\"start\":50576},{\"end\":50892,\"start\":50885},{\"end\":50905,\"start\":50898},{\"end\":50919,\"start\":50912},{\"end\":50933,\"start\":50927},{\"end\":50935,\"start\":50934},{\"end\":51287,\"start\":51280},{\"end\":51300,\"start\":51293},{\"end\":51314,\"start\":51308},{\"end\":51329,\"start\":51323},{\"end\":51345,\"start\":51339},{\"end\":51347,\"start\":51346},{\"end\":51361,\"start\":51355},{\"end\":51371,\"start\":51368},{\"end\":51707,\"start\":51700},{\"end\":51718,\"start\":51715}]", "bib_author_last_name": "[{\"end\":34626,\"start\":34621},{\"end\":34642,\"start\":34632},{\"end\":34656,\"start\":34651},{\"end\":34672,\"start\":34668},{\"end\":34687,\"start\":34682},{\"end\":34697,\"start\":34689},{\"end\":34997,\"start\":34989},{\"end\":35015,\"start\":35008},{\"end\":35030,\"start\":35021},{\"end\":35045,\"start\":35040},{\"end\":35060,\"start\":35054},{\"end\":35077,\"start\":35068},{\"end\":35090,\"start\":35085},{\"end\":35746,\"start\":35741},{\"end\":36018,\"start\":36015},{\"end\":36033,\"start\":36029},{\"end\":36042,\"start\":36038},{\"end\":36053,\"start\":36050},{\"end\":36064,\"start\":36060},{\"end\":36079,\"start\":36075},{\"end\":36457,\"start\":36452},{\"end\":36476,\"start\":36464},{\"end\":36491,\"start\":36485},{\"end\":36507,\"start\":36500},{\"end\":36785,\"start\":36782},{\"end\":36802,\"start\":36796},{\"end\":36819,\"start\":36810},{\"end\":36837,\"start\":36828},{\"end\":36852,\"start\":36845},{\"end\":36872,\"start\":36862},{\"end\":36885,\"start\":36878},{\"end\":36900,\"start\":36895},{\"end\":36910,\"start\":36902},{\"end\":36921,\"start\":36914},{\"end\":36941,\"start\":36928},{\"end\":36953,\"start\":36947},{\"end\":36968,\"start\":36961},{\"end\":36985,\"start\":36979},{\"end\":37000,\"start\":36993},{\"end\":37022,\"start\":37013},{\"end\":37034,\"start\":37030},{\"end\":37056,\"start\":37046},{\"end\":37076,\"start\":37066},{\"end\":37095,\"start\":37088},{\"end\":37112,\"start\":37101},{\"end\":37116,\"start\":37114},{\"end\":38225,\"start\":38222},{\"end\":38239,\"start\":38236},{\"end\":38250,\"start\":38246},{\"end\":38536,\"start\":38526},{\"end\":38556,\"start\":38543},{\"end\":38569,\"start\":38564},{\"end\":38578,\"start\":38576},{\"end\":38598,\"start\":38586},{\"end\":38613,\"start\":38608},{\"end\":38630,\"start\":38621},{\"end\":38645,\"start\":38639},{\"end\":38947,\"start\":38937},{\"end\":39224,\"start\":39217},{\"end\":39236,\"start\":39230},{\"end\":39249,\"start\":39246},{\"end\":39464,\"start\":39462},{\"end\":39479,\"start\":39474},{\"end\":39493,\"start\":39490},{\"end\":39503,\"start\":39500},{\"end\":40085,\"start\":40080},{\"end\":40098,\"start\":40095},{\"end\":40114,\"start\":40106},{\"end\":40125,\"start\":40120},{\"end\":40337,\"start\":40332},{\"end\":40350,\"start\":40347},{\"end\":40364,\"start\":40360},{\"end\":40380,\"start\":40375},{\"end\":40578,\"start\":40569},{\"end\":40594,\"start\":40586},{\"end\":40612,\"start\":40603},{\"end\":40631,\"start\":40620},{\"end\":41332,\"start\":41325},{\"end\":41349,\"start\":41344},{\"end\":41361,\"start\":41354},{\"end\":41652,\"start\":41649},{\"end\":41663,\"start\":41660},{\"end\":41677,\"start\":41674},{\"end\":41692,\"start\":41689},{\"end\":41707,\"start\":41700},{\"end\":41721,\"start\":41719},{\"end\":42595,\"start\":42589},{\"end\":42605,\"start\":42603},{\"end\":42894,\"start\":42887},{\"end\":42904,\"start\":42901},{\"end\":42919,\"start\":42914},{\"end\":42934,\"start\":42929},{\"end\":42965,\"start\":42959},{\"end\":42978,\"start\":42975},{\"end\":42997,\"start\":42991},{\"end\":43013,\"start\":43008},{\"end\":43026,\"start\":43020},{\"end\":43032,\"start\":43028},{\"end\":43833,\"start\":43829},{\"end\":44087,\"start\":44078},{\"end\":44097,\"start\":44091},{\"end\":44111,\"start\":44105},{\"end\":44124,\"start\":44119},{\"end\":44133,\"start\":44126},{\"end\":44628,\"start\":44619},{\"end\":44641,\"start\":44639},{\"end\":44654,\"start\":44649},{\"end\":44673,\"start\":44662},{\"end\":45186,\"start\":45181},{\"end\":45202,\"start\":45194},{\"end\":45379,\"start\":45377},{\"end\":45391,\"start\":45388},{\"end\":45607,\"start\":45601},{\"end\":45618,\"start\":45613},{\"end\":45636,\"start\":45628},{\"end\":45652,\"start\":45646},{\"end\":45665,\"start\":45661},{\"end\":45681,\"start\":45675},{\"end\":45693,\"start\":45690},{\"end\":45710,\"start\":45701},{\"end\":45723,\"start\":45717},{\"end\":45735,\"start\":45730},{\"end\":46084,\"start\":46081},{\"end\":46094,\"start\":46091},{\"end\":46104,\"start\":46100},{\"end\":46119,\"start\":46114},{\"end\":46136,\"start\":46128},{\"end\":46147,\"start\":46142},{\"end\":46895,\"start\":46884},{\"end\":46906,\"start\":46899},{\"end\":46914,\"start\":46910},{\"end\":47340,\"start\":47335},{\"end\":47360,\"start\":47353},{\"end\":47367,\"start\":47362},{\"end\":47611,\"start\":47602},{\"end\":47621,\"start\":47615},{\"end\":47632,\"start\":47625},{\"end\":47642,\"start\":47636},{\"end\":48000,\"start\":47988},{\"end\":48015,\"start\":48011},{\"end\":48313,\"start\":48309},{\"end\":48320,\"start\":48318},{\"end\":48330,\"start\":48325},{\"end\":48341,\"start\":48338},{\"end\":48768,\"start\":48765},{\"end\":48777,\"start\":48773},{\"end\":48788,\"start\":48785},{\"end\":48797,\"start\":48794},{\"end\":49241,\"start\":49235},{\"end\":49253,\"start\":49245},{\"end\":49757,\"start\":49750},{\"end\":49773,\"start\":49766},{\"end\":49793,\"start\":49784},{\"end\":50065,\"start\":50061},{\"end\":50088,\"start\":50075},{\"end\":50109,\"start\":50100},{\"end\":50128,\"start\":50120},{\"end\":50140,\"start\":50132},{\"end\":50156,\"start\":50147},{\"end\":50163,\"start\":50158},{\"end\":50538,\"start\":50530},{\"end\":50558,\"start\":50550},{\"end\":50574,\"start\":50568},{\"end\":50592,\"start\":50583},{\"end\":50896,\"start\":50893},{\"end\":50910,\"start\":50906},{\"end\":50925,\"start\":50920},{\"end\":50941,\"start\":50936},{\"end\":51291,\"start\":51288},{\"end\":51306,\"start\":51301},{\"end\":51321,\"start\":51315},{\"end\":51337,\"start\":51330},{\"end\":51353,\"start\":51348},{\"end\":51366,\"start\":51362},{\"end\":51381,\"start\":51372},{\"end\":51713,\"start\":51708},{\"end\":51726,\"start\":51719}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206594692},\"end\":34557,\"start\":34304},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":316800},\"end\":34933,\"start\":34559},{\"attributes\":{\"doi\":\"PMLR. 2\",\"id\":\"b2\",\"matched_paper_id\":44220142},\"end\":35693,\"start\":34935},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13284142},\"end\":35910,\"start\":35695},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13757233},\"end\":36368,\"start\":35912},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":43995396},\"end\":36687,\"start\":36370},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2637577},\"end\":38154,\"start\":36689},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":202145540},\"end\":38491,\"start\":38156},{\"attributes\":{\"id\":\"b8\"},\"end\":38935,\"start\":38493},{\"attributes\":{\"id\":\"b9\"},\"end\":39154,\"start\":38937},{\"attributes\":{\"id\":\"b10\"},\"end\":39359,\"start\":39156},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13740328},\"end\":40022,\"start\":39361},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4883312},\"end\":40256,\"start\":40024},{\"attributes\":{\"id\":\"b13\"},\"end\":40533,\"start\":40258},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6099034},\"end\":41247,\"start\":40535},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":980236},\"end\":41571,\"start\":41249},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195776105},\"end\":42534,\"start\":41573},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":42801,\"start\":42536},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":198147305},\"end\":43761,\"start\":42803},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":221242327},\"end\":43987,\"start\":43763},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":44092242},\"end\":44515,\"start\":43989},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52287531},\"end\":45111,\"start\":44517},{\"attributes\":{\"id\":\"b22\"},\"end\":45313,\"start\":45113},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":41055233},\"end\":45556,\"start\":45315},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":40027675},\"end\":45980,\"start\":45558},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":85459673},\"end\":46815,\"start\":45982},{\"attributes\":{\"doi\":\"arXiv:1505.04597[cs.CV]).5\",\"id\":\"b26\",\"matched_paper_id\":3719281},\"end\":47298,\"start\":46817},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3733707},\"end\":47531,\"start\":47300},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":19674308},\"end\":47893,\"start\":47533},{\"attributes\":{\"id\":\"b29\"},\"end\":48235,\"start\":47895},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2614865},\"end\":48692,\"start\":48237},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3798930},\"end\":49184,\"start\":48694},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14308539},\"end\":49665,\"start\":49186},{\"attributes\":{\"doi\":\"abs/1607.08022\",\"id\":\"b33\"},\"end\":49967,\"start\":49667},{\"attributes\":{\"doi\":\"abs/1907.05062\",\"id\":\"b34\"},\"end\":50412,\"start\":49969},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52952432},\"end\":50802,\"start\":50414},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":233404466},\"end\":51232,\"start\":50804},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":19046372},\"end\":51660,\"start\":51234},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206051485},\"end\":51882,\"start\":51662}]", "bib_title": "[{\"end\":34348,\"start\":34304},{\"end\":34613,\"start\":34559},{\"end\":34971,\"start\":34935},{\"end\":35737,\"start\":35695},{\"end\":36004,\"start\":35912},{\"end\":36441,\"start\":36370},{\"end\":36778,\"start\":36689},{\"end\":38212,\"start\":38156},{\"end\":39452,\"start\":39361},{\"end\":40074,\"start\":40024},{\"end\":40563,\"start\":40535},{\"end\":41316,\"start\":41249},{\"end\":41642,\"start\":41573},{\"end\":42578,\"start\":42536},{\"end\":42881,\"start\":42803},{\"end\":43819,\"start\":43763},{\"end\":44064,\"start\":43989},{\"end\":44605,\"start\":44517},{\"end\":45365,\"start\":45315},{\"end\":45594,\"start\":45558},{\"end\":46074,\"start\":45982},{\"end\":46880,\"start\":46817},{\"end\":47331,\"start\":47300},{\"end\":47598,\"start\":47533},{\"end\":48298,\"start\":48237},{\"end\":48757,\"start\":48694},{\"end\":49231,\"start\":49186},{\"end\":50521,\"start\":50414},{\"end\":50883,\"start\":50804},{\"end\":51278,\"start\":51234},{\"end\":51698,\"start\":51662}]", "bib_author": "[{\"end\":34628,\"start\":34615},{\"end\":34644,\"start\":34628},{\"end\":34658,\"start\":34644},{\"end\":34674,\"start\":34658},{\"end\":34689,\"start\":34674},{\"end\":34699,\"start\":34689},{\"end\":34999,\"start\":34973},{\"end\":35017,\"start\":34999},{\"end\":35032,\"start\":35017},{\"end\":35047,\"start\":35032},{\"end\":35062,\"start\":35047},{\"end\":35079,\"start\":35062},{\"end\":35092,\"start\":35079},{\"end\":35748,\"start\":35739},{\"end\":36020,\"start\":36006},{\"end\":36035,\"start\":36020},{\"end\":36044,\"start\":36035},{\"end\":36055,\"start\":36044},{\"end\":36066,\"start\":36055},{\"end\":36081,\"start\":36066},{\"end\":36459,\"start\":36443},{\"end\":36478,\"start\":36459},{\"end\":36493,\"start\":36478},{\"end\":36509,\"start\":36493},{\"end\":36787,\"start\":36780},{\"end\":36804,\"start\":36787},{\"end\":36821,\"start\":36804},{\"end\":36839,\"start\":36821},{\"end\":36854,\"start\":36839},{\"end\":36874,\"start\":36854},{\"end\":36887,\"start\":36874},{\"end\":36902,\"start\":36887},{\"end\":36912,\"start\":36902},{\"end\":36923,\"start\":36912},{\"end\":36943,\"start\":36923},{\"end\":36955,\"start\":36943},{\"end\":36970,\"start\":36955},{\"end\":36987,\"start\":36970},{\"end\":37002,\"start\":36987},{\"end\":37024,\"start\":37002},{\"end\":37036,\"start\":37024},{\"end\":37058,\"start\":37036},{\"end\":37078,\"start\":37058},{\"end\":37097,\"start\":37078},{\"end\":37114,\"start\":37097},{\"end\":37118,\"start\":37114},{\"end\":38227,\"start\":38214},{\"end\":38241,\"start\":38227},{\"end\":38252,\"start\":38241},{\"end\":38538,\"start\":38522},{\"end\":38558,\"start\":38538},{\"end\":38571,\"start\":38558},{\"end\":38580,\"start\":38571},{\"end\":38600,\"start\":38580},{\"end\":38615,\"start\":38600},{\"end\":38632,\"start\":38615},{\"end\":38647,\"start\":38632},{\"end\":38949,\"start\":38937},{\"end\":39226,\"start\":39211},{\"end\":39238,\"start\":39226},{\"end\":39251,\"start\":39238},{\"end\":39466,\"start\":39454},{\"end\":39481,\"start\":39466},{\"end\":39495,\"start\":39481},{\"end\":39505,\"start\":39495},{\"end\":40087,\"start\":40076},{\"end\":40100,\"start\":40087},{\"end\":40116,\"start\":40100},{\"end\":40127,\"start\":40116},{\"end\":40339,\"start\":40324},{\"end\":40352,\"start\":40339},{\"end\":40366,\"start\":40352},{\"end\":40382,\"start\":40366},{\"end\":40580,\"start\":40565},{\"end\":40596,\"start\":40580},{\"end\":40614,\"start\":40596},{\"end\":40633,\"start\":40614},{\"end\":41334,\"start\":41318},{\"end\":41351,\"start\":41334},{\"end\":41363,\"start\":41351},{\"end\":41654,\"start\":41644},{\"end\":41665,\"start\":41654},{\"end\":41679,\"start\":41665},{\"end\":41694,\"start\":41679},{\"end\":41709,\"start\":41694},{\"end\":41723,\"start\":41709},{\"end\":42597,\"start\":42580},{\"end\":42607,\"start\":42597},{\"end\":42896,\"start\":42883},{\"end\":42906,\"start\":42896},{\"end\":42921,\"start\":42906},{\"end\":42936,\"start\":42921},{\"end\":42967,\"start\":42936},{\"end\":42980,\"start\":42967},{\"end\":42999,\"start\":42980},{\"end\":43015,\"start\":42999},{\"end\":43028,\"start\":43015},{\"end\":43034,\"start\":43028},{\"end\":43835,\"start\":43821},{\"end\":44089,\"start\":44066},{\"end\":44099,\"start\":44089},{\"end\":44113,\"start\":44099},{\"end\":44126,\"start\":44113},{\"end\":44135,\"start\":44126},{\"end\":44630,\"start\":44607},{\"end\":44643,\"start\":44630},{\"end\":44656,\"start\":44643},{\"end\":44675,\"start\":44656},{\"end\":45188,\"start\":45175},{\"end\":45204,\"start\":45188},{\"end\":45381,\"start\":45367},{\"end\":45393,\"start\":45381},{\"end\":45609,\"start\":45596},{\"end\":45620,\"start\":45609},{\"end\":45638,\"start\":45620},{\"end\":45654,\"start\":45638},{\"end\":45667,\"start\":45654},{\"end\":45683,\"start\":45667},{\"end\":45695,\"start\":45683},{\"end\":45712,\"start\":45695},{\"end\":45725,\"start\":45712},{\"end\":45737,\"start\":45725},{\"end\":46086,\"start\":46076},{\"end\":46096,\"start\":46086},{\"end\":46106,\"start\":46096},{\"end\":46121,\"start\":46106},{\"end\":46138,\"start\":46121},{\"end\":46149,\"start\":46138},{\"end\":46897,\"start\":46882},{\"end\":46908,\"start\":46897},{\"end\":46916,\"start\":46908},{\"end\":47342,\"start\":47333},{\"end\":47362,\"start\":47342},{\"end\":47369,\"start\":47362},{\"end\":47613,\"start\":47600},{\"end\":47623,\"start\":47613},{\"end\":47634,\"start\":47623},{\"end\":47644,\"start\":47634},{\"end\":48002,\"start\":47986},{\"end\":48017,\"start\":48002},{\"end\":48315,\"start\":48300},{\"end\":48322,\"start\":48315},{\"end\":48332,\"start\":48322},{\"end\":48343,\"start\":48332},{\"end\":48770,\"start\":48759},{\"end\":48779,\"start\":48770},{\"end\":48790,\"start\":48779},{\"end\":48799,\"start\":48790},{\"end\":49243,\"start\":49233},{\"end\":49255,\"start\":49243},{\"end\":49759,\"start\":49743},{\"end\":49775,\"start\":49759},{\"end\":49795,\"start\":49775},{\"end\":50067,\"start\":50052},{\"end\":50090,\"start\":50067},{\"end\":50111,\"start\":50090},{\"end\":50130,\"start\":50111},{\"end\":50142,\"start\":50130},{\"end\":50158,\"start\":50142},{\"end\":50165,\"start\":50158},{\"end\":50540,\"start\":50523},{\"end\":50560,\"start\":50540},{\"end\":50576,\"start\":50560},{\"end\":50594,\"start\":50576},{\"end\":50898,\"start\":50885},{\"end\":50912,\"start\":50898},{\"end\":50927,\"start\":50912},{\"end\":50943,\"start\":50927},{\"end\":51293,\"start\":51280},{\"end\":51308,\"start\":51293},{\"end\":51323,\"start\":51308},{\"end\":51339,\"start\":51323},{\"end\":51355,\"start\":51339},{\"end\":51368,\"start\":51355},{\"end\":51383,\"start\":51368},{\"end\":51715,\"start\":51700},{\"end\":51728,\"start\":51715}]", "bib_venue": "[{\"end\":35285,\"start\":35199},{\"end\":37310,\"start\":37287},{\"end\":39689,\"start\":39596},{\"end\":40855,\"start\":40831},{\"end\":41962,\"start\":41947},{\"end\":43180,\"start\":43165},{\"end\":44222,\"start\":44215},{\"end\":46316,\"start\":46300},{\"end\":48416,\"start\":48397},{\"end\":49417,\"start\":49335},{\"end\":34415,\"start\":34350},{\"end\":34721,\"start\":34699},{\"end\":35167,\"start\":35099},{\"end\":35786,\"start\":35748},{\"end\":36122,\"start\":36081},{\"end\":36518,\"start\":36509},{\"end\":37285,\"start\":37118},{\"end\":38274,\"start\":38252},{\"end\":38520,\"start\":38493},{\"end\":38998,\"start\":38949},{\"end\":39209,\"start\":39156},{\"end\":39594,\"start\":39505},{\"end\":40131,\"start\":40127},{\"end\":40322,\"start\":40258},{\"end\":40745,\"start\":40633},{\"end\":41401,\"start\":41363},{\"end\":41825,\"start\":41723},{\"end\":42659,\"start\":42607},{\"end\":43136,\"start\":43034},{\"end\":43857,\"start\":43835},{\"end\":44213,\"start\":44135},{\"end\":44710,\"start\":44675},{\"end\":45173,\"start\":45113},{\"end\":45419,\"start\":45393},{\"end\":45759,\"start\":45737},{\"end\":46232,\"start\":46149},{\"end\":47009,\"start\":46942},{\"end\":47398,\"start\":47369},{\"end\":47689,\"start\":47644},{\"end\":47984,\"start\":47895},{\"end\":48395,\"start\":48343},{\"end\":48838,\"start\":48799},{\"end\":49333,\"start\":49255},{\"end\":49741,\"start\":49667},{\"end\":50050,\"start\":49969},{\"end\":50598,\"start\":50594},{\"end\":50980,\"start\":50943},{\"end\":51432,\"start\":51383},{\"end\":51754,\"start\":51728}]"}}}, "year": 2023, "month": 12, "day": 17}