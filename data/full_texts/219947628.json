{"id": 219947628, "updated": "2023-10-10 01:56:25.362", "metadata": {"title": "Fast EEG-based decoding of the directional focus of auditory attention using common spatial patterns", "authors": "[{\"first\":\"Simon\",\"last\":\"Geirnaert\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Francart\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Bertrand\",\"middle\":[]}]", "venue": "bioRxiv", "journal": "bioRxiv", "publication_date": {"year": 2020, "month": 8, "day": 31}, "abstract": "Objective Noise reduction algorithms in current hearing devices lack information about the sound source a user attends to when multiple sources are present. To resolve this issue, they can be complemented with auditory attention decoding (AAD) algorithms, which decode the attention using electroen-cephalography (EEG) sensors. State-of-the-art AAD algorithms employ a stimulus reconstruction approach, in which the envelope of the attended source is reconstructed from the EEG and correlated with the envelopes of the individual sources. This approach, however, performs poorly on short signal segments, while longer segments yield impractically long detection delays when the user switches attention. Methods We propose decoding the directional focus of attention using filterbank common spatial pattern filters (FB-CSP) as an alternative AAD paradigm, which does not require access to the clean source envelopes. Results The proposed FB-CSP approach outperforms both the stimulus reconstruction approach on short signal segments, as well as a convolutional neural network approach on the same task. We achieve a high accuracy (80% for 1 s windows and 70% for quasi-instantaneous decisions), which is sufficient to reach minimal expected switch durations below 4 s. We also demonstrate that the decoder can adapt to unlabeled data from an unseen subject and works with only a subset of EEG channels located around the ear to emulate a wearable EEG setup. Conclusion The proposed FB-CSP method provides fast and accurate decoding of the directional focus of auditory attention. Significance The high accuracy on very short data segments is a major step forward towards practical neuro-steered hearing devices.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3093647835", "acl": null, "pubmed": "33095706", "pubmedcentral": null, "dblp": "journals/tbe/GeirnaertFB21", "doi": "10.1101/2020.06.16.154450"}}, "content": {"source": {"pdf_hash": "720f7696aa9d72e171673d331a7677c951e19021", "pdf_src": "BioRxiv", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://lirias.kuleuven.be/bitstream/123456789/674729/2/20-65.pdf", "status": "GREEN"}}, "grobid": {"id": "1157477bc67292fbc0287c1eb9591f370f213fe6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/720f7696aa9d72e171673d331a7677c951e19021.txt", "contents": "\nFast EEG-based decoding of the directional focus of auditory attention using common spatial patterns\n\n\nSimon Geirnaert \nTom Francart \nSenior Member, IEEEAlexander Bertrand \nFast EEG-based decoding of the directional focus of auditory attention using common spatial patterns\n1\nObjective: Noise reduction algorithms in current hearing devices lack information about the sound source a user attends to when multiple sources are present. To resolve this issue, they can be complemented with auditory attention decoding (AAD) algorithms, which decode the attention using electroencephalography (EEG) sensors. State-of-the-art AAD algorithms employ a stimulus reconstruction approach, in which the envelope of the attended source is reconstructed from the EEG and correlated with the envelopes of the individual sources. This approach, however, performs poorly on short signal segments, while longer segments yield impractically long detection delays when the user switches attention. Methods: We propose decoding the directional focus of attention using filterbank common spatial pattern filters (FB-CSP) as an alternative AAD paradigm, which does not require access to the clean source envelopes. Results:The proposed FB-CSP approach outperforms both the stimulus reconstruction approach on short signal segments, as well as a convolutional neural network approach on the same task. We achieve a high accuracy (80% for 1 s windows and 70% for quasiinstantaneous decisions), which is sufficient to reach minimal expected switch durations below 4 s. We also demonstrate that the decoder can adapt to unlabeled data from an unseen subject and works with only a subset of EEG channels located around the ear to emulate a wearable EEG setup. Conclusion: The proposed FB-CSP method provides fast and accurate decoding of the directional focus of auditory attention. Significance: The high accuracy on very short data segments is a major step forward towards practical neuro-steered hearing devices.Index Terms-auditory attention decoding, directional focus of attention, brain-computer interface, common spatial pattern filter, electroencephalography, neuro-steered hearing device\n\nI. INTRODUCTION\n\nCurrent hearing aids, cochlear implants, and other assistive listening devices contain noise reduction algorithms to assist people suffering from hearing deficits. However, these algorithms often fail in so-called 'cocktail party' scenarios where multiple speakers or other sound sources are simultaneously active. This is not only because the noise suppression becomes more difficult, but primarily because the hearing device lacks information about which source the user intends to attend and which other sources need to be treated as background noise.\n\nInstead of using unreliable heuristics, such as selecting the most frontal source or the source with the highest intensity, one could try to extract the attention-related information directly from where it originates, i.e., the brain. This is known as auditory attention decoding (AAD). The development of AAD algorithms that process brain signals to, e.g., steer a speech enhancement algorithm towards the attended speaker in a mixture of other speakers, could lead to a new class of socalled 'neuro-steered' hearing devices [1], [2], improving the quality of life of people suffering from hearing deficits.\n\nThe discovery that the cortical activity follows the envelope of the attended speech stream [3]- [5] is in this context crucial. This insight laid the foundation of a first class of AAD algorithms based on non-invasive neural recordings from magneto-or electroencephalography (MEG/EEG). These algorithms typically employ a stimulus reconstruction approach in which a decoder reconstructs the attended speech envelope from the EEG. The decoded envelope is then correlated with the speech envelopes of the individual speakers. The speaker corresponding to the highest correlation coefficient is identified as the attended speaker. This algorithm was proposed for the first time in [6], using a linear minimal-mean-squared-errorbased decoder. Later, other variations of this AAD algorithm were developed, of which an overview can be found in [2].\n\nAAD algorithms using the stimulus reconstruction approach, however, all suffer from the same limitations:\n\n1) The stimulus reconstruction approach takes too long to make a reliable decision. The AAD accuracy (the percentage of correct decisions) drastically decreases with shorter decision windows, especially below 10 s [6]- [8].\n\nA decision window corresponds to the signal length over which the correlation coefficients between the EEGdecoded envelope and the original speech envelopes are estimated, where short decision windows result in unreliable correlation estimates. This results in a speedaccuracy trade-off. In [8], it is shown that short decision window lengths are favorable in the context of robust AAD-based gain control during dynamic switching, even if they have a lower accuracy. Nevertheless, due to the low accuracy for these short decision window lengths, it theoretically takes more than 15 s to establish a reliable and controlled gain switch to the new attended speaker after the user switches attention [2], which is impractically long for neuro-steered hearing device applications. The stimulus reconstruction approach inherently suffers from this limited performance due to the decoding of a low-frequent envelope, which contains relatively little information per second, as well as due to the low signalto-noise ratio of the neural response to the stimulus in the EEG. 2) The stimulus reconstruction approach requires the (clean) individual speech envelopes. Although several attempts have been made to combine speech separation algorithms with AAD [1], [9]- [11], the demixing of all speech envelopes adds a lot of overhead, and the demixing process often negatively affects AAD performance or may even completely fail in practical situations. In this paper, we employ a new paradigm that avoids these limitations, focusing on decoding the directional focus of attention from the EEG, rather than directly identifying the attended speaker. Inherently, this avoids the need of demixing the speech mixtures into its individual contributions. Moreover, we hypothesize that this paradigm will improve AAD accuracy for short decision window lengths, as it is based on brain lateralization, which is an instantaneous spatial feature, rather than a correlation-based temporal feature.\n\nThis new AAD paradigm is justified by recent research that shows that the auditory attentional direction is spatiotemporally encoded in the neural activity [4], [12]- [18], ergo, that it could be possible to decode the spatial focus of attention from the EEG. In [19], an AAD algorithm based on a convolutional neural network (CNN) has been established to decode the spatial locus of attention in a competing speaker scenario, which showed very good results on short decision windows (76.1% accuracy on 1 s decision windows). However, this CNN-based approach shows high inter-subject variability and requires large amounts of training data (e.g., data of other subjects in combination with subject-specific data as in [19]) in order to train a subject-specific decoder. Therefore, in this paper, we focus on data-driven linear filtering techniques, which typically require less training data, are more robust and stable, and are computationally cheaper, as well as easier to update. More specifically, we exploit the direction-dependent spatiotemporal signatures of the EEG using (filterbank) common spatial pattern (FB-CSP) filters, which are popular in various brain-computer interface (BCI) applications [20], [21].\n\nIn Section II, we concisely introduce the (FB-)CSP classification pipeline to determine the directional focus of attention. In Section III, we describe the data used to run experiments, the concrete design choices for the FB-CSP filter design, and the performance metrics to transparently and statistically validate the experiments that are reported and analyzed in Section IV. Conclusions are drawn in Section V.\n\n\nII. DECODING DIRECTION OF ATTENTION USING CSPS\n\nIn this section, we review the CSP procedure [20] to decode the directional focus of attention. CSP filtering is one of the most popular techniques used for spatial feature extraction in BCI applications, e.g., in motor imagery [20]- [22]. The goal is to project multichannel EEG data into a lower-dimensional subspace that optimally discriminates between two conditions or classes. This is established by optimizing a spatial filter in a data-driven fashion, which linearly combines the different EEG channels into a few signals in which this discriminative property is maximally present.\n\nFor the sake of an easy exposition, we first define CSP filtering for a binary AAD problem, i.e., decoding whether a subject attends to one of two speaker positions, in Section II-A and II-B. In Section II-C, we explain how this can be generalized to more than two classes/directions. Finally, in Section II-D, we explain how the method can be applied to EEG data from unseen subjects without the need for any ground-truth labels on their auditory attention.\n\nA. CSP filtering Consider a zero-mean C-channel EEG signal x(t) \u2208 R C\u00d71 , which can on each time instance t be classified into one of two classes C 1 and C 2 (e.g., attending the left or right speaker). The goal is to design a set of K spatial filters W \u2208 R C\u00d7K that generate a K-channel output signal with uncorrelated channels y(t) = W T x(t) \u2208 R K\u00d71 , where the K 2 first filters maximize the output energy when t \u2208 C 1 , while minimizing the output energy when t \u2208 C 2 , and vice versa for the other K 2 filters. For example, the first column w 1 of W results in y 1 (t) = w T 1 x(t), which should have a maximal output energy when t \u2208 C 1 and a minimal output energy when t \u2208 C 2 :\nw 1 = argmax w 1 |C1| t\u2208C1 (w T x(t)) 2 1 |C2| t\u2208C2 (w T x(t)) 2 \u21d4 w 1 = argmax w w T R C1 w w T R C2 w ,\nwith C 1/2 the number of time instances in C 1/2 and\nR C 1/2 = 1 C 1/2 t\u2208C 1/2 x(t)x T (t)(1)\nthe sample covariance matrices of class C 1 and C 2 . Fixating the output energy when t \u2208 C 2 , i.e., w T R C2 w = 1, which is possible because w is defined up to a scaling, and solving the optimization problem using the method of Lagrange multipliers leads to the following necessary condition for optimality:\nR C1 w = \u03bbR C2 w,(2)\nwhich corresponds to a generalized eigenvalue problem. It can easily be seen that the maximum is obtained for the generalized eigenvector corresponding to the largest generalized eigenvalue. A similar reasoning can be followed for w K , which maximizes, resp. minimizes the output energy when t \u2208 C 2 , resp. C 1 , and is equal to the generalized eigenvector corresponding to the smallest generalized eigenvalue in (2). The other spatial filters can be found as the subsequent largest and smallest generalized eigenvectors. In its core essence, designing CSP filters thus corresponds to a joint diagonalization of the class-dependent covariance matrices [20].\n\n\nB. Classification using CSP filters\n\nThe CSP filtering technique can now be employed in a classification pipeline, in which a newly recorded EEG signal x(t) \u2208 R C\u00d71 , containing C channels, is classified into one of two classes, representing different directions of auditory attention (Fig. 1). The following sections describe the different components of this classification pipeline.  1) Filterbank CSP (FB-CSP): Paramount for a wellperforming CSP filtering is the selection of the appropriate frequency band related to the feature at hand. For the case of auditory attention decoding, one possibility is filtering in the \u03b1-band [4], [13], [14], [17], [18]. We here, however, do not want to make an a priori choice of the relevant frequency band(s). We thus adopt the so-called filterbank CSP (FB-CSP) technique, in which the EEG is first filtered into different frequency bands, after which the CSP filters are trained and applied per frequency band [20]- [22]. The filterbank thus results in B (number of frequency bands) filtered signals x b (t) \u2208 R C\u00d71 , one per frequency band b \u2208 {1, . . . , B}, for all C EEG channels. The application of the pre-trained CSP filters per frequency band\nX b Y b = W T b X b f sign(D (f ))W b \u2208 R C\u00d7K results in B K-dimensional output signals y b (t) = W T b x b (t) \u2208 R K\u00d71\n. An alternative extension, which is not pursued here, is the so-called common spatio-spectral pattern filter, in which the relevant frequency bands are determined fully data-driven, as a spatio-temporal filter is optimized to be maximally discriminative [23]. This comes, however, at the cost of an increase in parameters and corresponding problems with overfitting, in particular for high-density EEG data as used in this paper. These problems can partly be overcome by using more advanced regularization or dimensionality reduction techniques on the extended spatio-temporal covariance matrices (e.g., principal component analysis [24] or the pre-selection of relevant time lags to introduce sparsity). Furthermore, a different filter basis than the Dirac basis could be chosen to reduce the number of parameters or to incorporate expert knowledge [24].\n\n2) Feature extraction: The outputs of the FB-CSP filtering are now per decision window transformed into a feature vector f \u2208 R KB\u00d71 that can be used for classification. This is typically done by computing the log-energy over these output signals per decision window [20], using a pre-defined decision window length T :\nf = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 log \u03c3 2 1,1 . . . log \u03c3 2 K,1 log \u03c3 2 1,2 . . . log \u03c3 2 K,B \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,\nwith the output energy \u03c3 2 k,b of the k th output y k,b (t), for the b th frequency band:\n\u03c3 2 k,b = T t=1 y k,b (t) 2 ,\nwhere T is the number of time samples in the decision window. Note that the decision window length T determines how much EEG data is used to make a single decision about the auditory attention of the subject. In a practical system, this will define the inherent delay to detect switches in attention.\n\n\n3) Classification:\n\nThe feature vector f is used as the input for a binary classifier to determine the directional focus of attention. We here adopt Fisher's linear discriminant analysis (LDA), which is traditionally used in combination with CSP filters [20]. In LDA, similarly to CSPs, a linear filter v \u2208 R KB\u00d71 is optimized to provide the most informative projection. In this case, the most informative projection corresponds to maximizing the in-between class scatter, while minimizing the within-class scatter. This again leads to a generalized eigenvalue problem, which can, in this case, be solved analytically, leading to the following solution [25]:\nv = \u03a3 \u22121 w (\u00b5 2 \u2212 \u00b5 1 ) ,(3)\nwith \u03a3 w the covariance matrix of the features f computed across both classes, and \u00b5 1/2 the class (feature) means. Choosing the threshold or bias as the mean of the LDA projected class means, leads to the following decision function:\nD(f ) = v T f + b,\nwith v defined in (3) and bias\nb = \u2212 1 2 v T (\u00b5 1 + \u00b5 2 ) .(4)\nFinally, f is classified into class 1 if D(f ) > 0 and into class 2 if D(f ) < 0.\n\n\nC. Multiclass CSP classification\n\nThe classification scheme in Fig. 1 can be easily extended to a multiclass scenario, in which multiple directions of auditory attention are combined. This can be achieved by applying the strategy of Section II-A and II-B in combination with an appropriate coding scheme (e.g., one-vs-one, one-vs-all), both for the CSP and LDA step, or, by approximating a joint diagonalization of all the class covariance matrices at once in the CSP block [20], and only applying a coding scheme to the LDA step. Note that also the stimulus reconstruction approach is applicable for various directions/speakers [11], [26].\n\nIn this paper, we adopt the popular one-vs-all approach in BCI research [20]. In this approach, an FB-CSP filter and LDA classifier is trained for each direction to discriminate that particular direction from all the other directions. Given M directions (classes), this means that in (2), the R C2 is replaced In the end, for a new segment, the posterior probability of each classifier is computed using the multivariate normal distribution for the likelihood (which is assumed by LDA) and a uniform prior. To finally determine the correct class, the maximal posterior probability is taken over the M LDA classifiers.\n\n\nD. CSP classification on an unseen subject\n\nThe FB-CSP filters and LDA classifiers can be trained subjectspecifically, meaning that the training is based on EEG data from the actual subject under test. However, in a neuro-steered hearing device application, this would require a cumbersome per-user calibration session where the subject is asked to attend to specific speakers with the intention to collect ground-truth labels to inform the FB-CSP filter design. To eliminate this requirement, one could train an AAD model in a subjectindependent manner, meaning that data from subjects other than the test subject are used in the training phase, as done in [6] and [19] for the stimulus reconstruction and CNN approaches. This pre-trained model could then be 'pre-installed' on every neuro-steered hearing device, using it in a 'plug-andplay' fashion.\n\nHowever, it is known from the BCI literature that the FB-CSP method often fails in such subject-independent settings due to too large differences in the spatial/spectral EEG patterns across different subjects [27]. To improve performance, the data from the subject under test can be used to modify the pre-trained subject-independent FB-CSP filters/LDA classifier. We adopt here two popular approaches to perform such adaptations, without requiring ground-truth labels for the data of the unseen test subject: 1) A very effective way of unsupervised updating of an LDA classifier for BCIs has been proposed in [28]. They conclude that simply updating the bias of the LDA classifier in (4) results in a significant improvement. Here, we update the bias of the subject-independently trained LDA with the unlabeled subject-specific features (resulting from the subject-independent FB-CSP filters), as this only requires the global mean, which is labelindependent:\nD(f ) = v (SI) T f + b (SS) ,\nwith subject-independent coefficients v (SI) as computed in (3) on the data from all other subjects, and the subjectspecific bias computed as:\nb (SS) = \u2212v (SI) T \u00b5 (SS) ,\nusing the global mean \u00b5 (SS) over all features f (SS) of the new subject. The only requirement is that the subject-specific data on which the bias is updated is approximately balanced over both classes. 2) In [27], the authors found that a subject-independent FB-CSP method often fails, potentially because of the too high spectral subject-to-subject variability when using many narrow frequency bands. To overcome this issue, we replace the filterbank setting with a single filter (B = 1) to extract and pool a broader frequency range, of which the boundaries will be determined experimentally. This basically reduces the FB-CSP method to a CSP classification method with a prior bandpass filtering of the data. Note that only for the subject-independent experiments (Section IV-G), the FB-CSP method is reduced to a single frequency band. In all other subjectspecific experiments, the FB-CSP approach is used.\n1 2 3 + 90 \u2022 -90 \u2022 0 \u2022 -5 \u2022 + 5 \u2022 + 30 \u2022 -30 \u2022\n\nIII. EXPERIMENTS AND EVALUATION A. AAD datasets\n\nWe apply the proposed FB-CSP classification method on two different datasets. The first dataset (Dataset I) has already been used extensively in previous work, mostly in the context of the stimulus reconstruction approach [2], [7], [8], [19], [29], and is publicly available [30]. It consists of 72 minutes of EEG recordings for each of the 16 normal-hearing subjects, who were instructed to attend to one of two competing (male) speakers. The competing speakers were located at -90 \u2022 and +90 \u2022 along the azimuth direction and there was no background noise. More details can be found in [7]. This dataset is used in all experiments, except those of Section IV-C and IV-D 1 .\n\nThe second dataset (Dataset II) consists of 138 minutes of EEG recordings for each of the 18 normal-hearing subjects, again instructed to attend to one of two male speakers, however, now with background babble noise at different signalto-noise ratios. Furthermore, per subject, different angular speaker positions are combined (i.e., different angular separation between the competing speakers): Fig. 2). More details can be found in [26]. This second dataset allows us to validate the decoding of the directional focus of attention for different angular separations and is used in Section IV-C and IV-D. Both datasets are recorded using a C = 64-channel BioSemi ActiveTwo system, using a sampling frequency of 8192 Hz.\n-90 \u2022 versus +90 \u2022 , +30 \u2022 versus +90 \u2022 , -90 \u2022 versus -30 \u2022 , and -5 \u2022 versus +5 \u2022 (see\nB. Design choices 1) EEG bandpass filtering: Before CSP filtering, a filterbank is applied to the EEG, consisting of B = 14 8 th -order Butterworth filters. The first filter corresponds to frequency band 1\u22124 Hz, the second to 2\u22126 Hz, the third to 4\u22128 Hz. This continues, with bands of 4 Hz, overlapping with 2 Hz, until the last band 26\u221230 Hz. In this way, a similar range of frequencies is covered as in [19]. The group delay is compensated for per filter using the filtfilt-function in MATLAB, resulting in a zerophase filtering. Afterwards, the EEG data is downsampled to 64 Hz. No further preprocessing or artifact rejection is applied, as the CSP filters already implicitly suppress EEG content that is irrelevant for discrimination between both classes through a spatial filtering per frequency band.\n\n2) Covariance matrix estimation: To avoid overfitting in the estimation of the class covariance matrices in (2), the sample covariance matrices in (1) are regularized using ridge regression: R (reg) (1) and I \u2208 R C\u00d7C the identity matrix. The regularization parameters \u03b1 and \u03b2 are not estimated using cross-validation but are analytically determined (details in [31]). This method has proven to be superior in various BCI applications and is the recommended state-of-the-art covariance matrix estimator [21].\nC 1/2 = \u03b1R C 1/2 + \u03b2I, with R C 1/2 the sample covariance matrix from\n3) CSP filter design: As described in Section II-A, traditionally, the generalized eigenvalues are used to select an appropriate subset of filters, as they represent the relative output energies of each spatially filtered signal. However, these generalized eigenvalues can be influenced by outlier segments with a very high variance, which consecutively, can (negatively) affect the selection of the CSP filters. To avoid this issue, the filters are selected based on the ratio of median output energies between both classes [20], taken over all training windows with length equal to the maximal decision window length that is used in the analysis.\n\nFurthermore, K = 6 CSP filters, corresponding to the 3 most discriminative filters for one and the other direction, are selected based on the cut-off point on the plot of sorted ratioof-medians.\n\n\nC. Performance evaluation\n\nThe FB-CSP classification pipeline is first tested per subject separately using ten-fold cross-validation. The data per subject are therefore split into segments of 60 s (30 s for dataset II) and randomly shuffled into ten folds. This division into segments is performed in order to be able to do random shuffling over time, such that the impact of factors such as fatigue is minimized. Only in Section IV-B and the Supplementary Material, a leaveone-speaker+story-out cross-validation is performed, retaining the chronological order of the segments that originate from continuously recorded trials. For each 60/30 s segment, the mean is set to zero per channel. Furthermore, each segment is normalized over all channels at once (the Frobenius norm is set to one), to assign equal weight to each segment in the training phase. During the testing phase, the normalized 60/30 s segments are split into shorter sub-segments, referred to as 'decision windows' (of which the length will be varied). The significance level for the accuracy is determined via the inverse binomial distribution [6].\n\nIn [8], the importance of evaluating AAD algorithms on different decision window lengths, i.e., the amount of data used to make an AAD decision, has been stressed. In typical AAD algorithms, a trade-off exists between the decision window length and the accuracy. In [8], the optimal trade-off is determined by means of a criterion based on the expected time it takes to perform a stable gain switch in an attention-steered gain control system. Based on a stochastic model for the latter, and for any given decision window length, the expected time to switch the gain between speakers is minimized under the constraint of guaranteeing a pre-defined level of 'stability' to avoid spurious gain switches due to errors in the AAD decisions. The latter is achieved by increasing the number of gain levels, thereby increasing the gain switch duration. The optimal trade-off point between decision window length and accuracy is found as the one that leads to the shortest expected switch duration under this model, which is referred to as the minimal expected switch duration (MESD). The MESD is a single-number metric, facilitating the use of statistical tests to compare different AAD algorithms, as it resolves the inherent trade-off between decision window length and AAD accuracy. We refer to [8] for more details. 2 In [8], it was found that the optimal decision window length selected within the computation of the MESD consistently shows the importance of short decision window lengths (< 10 s), allowing faster and more robust switching between speakers despite the lower AAD accuracy. To determine the accuracies on shorter decision window lengths, the left-out segments are split into shorter decision windows, on which the testing routine in Fig. 1 is applied. Note that the MESD is a theoretical metric and only provides a theoretical prediction on how an optimized AAD-based gain control algorithm would track attention switches [8]. Here, we do not experiment with data containing actual attention switches.\n\nThe hyperparameters of the LDA classifier are optimized on the CSP output energies of the training set using five-fold cross-validation.\n\n\nIV. RESULTS AND DISCUSSION\n\n\nA. Comparison with stimulus reconstruction approach\n\nThe FB-CSP classification method is compared to the current state-of-the-art AAD method, which adopts the stimulus reconstruction approach, on Dataset I. Here, canonical correlation analysis (CCA) is used, which is considered to be one of the best decoding methods to date, outperforming other backward and forward models [2]. In CCA, a jointly forward (i.e., mapping the stimulus envelope to the EEG) and backward (i.e., mapping the EEG to the stimulus envelope) model is trained and applied to new data [24]. The attended speaker is identified by classifying the difference between the canonical correlation coefficients of the competing speakers using an LDA classifier. A forward lag of 1.25 s is used on the speech envelopes and a backward lag of 250 ms is used on the EEG as in [2], [24]. The CCA method is tested using the same ten-fold crossvalidation procedure as for the FB-CSP method. The number of correlation coefficients used in the LDA classification is determined by an inner ten-fold cross-validation loop. No a priori principal component analysis or change of filter basis as in [24] is used. The EEG and speech envelopes, which are extracted using a power-law operation with exponent 0.6 after subband filtering [7], are filtered between 1 and 9 Hz (thus mainly without \u03b1/\u03b2-activity, which was determined to be optimal for linear stimulus reconstruction [6], [7], [11], [26], [29]) and downsampled to 20 Hz. Note that this method employs an inherently different strategy for AAD than FB-CSP, by (in a way) reconstructing the attended speech envelope, rather than decoding the directional focus of attention.\n\nIn Fig. 3a, it is observed that this stimulus reconstruction approach is characterized by a degrading accuracy for shorter decision window lengths, while the accuracy of the FB-CSP method barely decreases. It thus clearly outperforms the stimulus reconstruction approach for short decision window lengths. This is one of the most important properties of this new strategy for AAD to decode the directional focus with FB-CSP rather than reconstructing the stimulus. This effect was also seen in [19], where the directional focus is decoded based on a CNN. While the stimulus reconstruction approach tries to determine the attended speaker by reconstructing the attended speech envelope, the FB-CSP method only needs to discriminate between two angular directions, which is an inherently easier filter design strategy. Furthermore, in the former, correlation is used as a feature, of which the estimation is inaccurate when computed on short decision windows, in particular because the correlation coefficients observed in stimulus reconstruction are very small, making their estimation susceptible to noise. Lastly, as mentioned before, the FB-CSP method is mostly based on an instantaneous spatial feature (brain lateralization) rather than a temporal feature.\n\nNote that the accuracy of the FB-CSP method exhibits a higher inter-subject variability than the stimulus reconstruction method. We do not consider this as a major disadvantage of the FB-CSP method, as, for example, on 1 s decision windows, performance is still better for all subjects compared to CCA. On average, there is a 20% gap in accuracy for 1 s decision windows.\n\nFor long decision window lengths, however, CCA outperforms the FB-CSP method. To resolve this trade-off and to (statistically) determine which method performs better in a context of neuro-steered gain control for hearing devices, we use the MESD metric [8], a relevant criterion for AAD that optimizes the speed-accuracy trade-off (Section III-C) and thus resolves the inconclusiveness based on the performance curve. Fig. 3b shows the MESDs per subject, for both algorithms. It is clear that FB-CSP (median MESD 4.1 s), results in much faster switching than CCA (median MESD 17.0 s). A Wilcoxon signed-rank test confirms that there indeed is a significant difference between the MESD for the FB-CSP method versus CCA (W = 0, n = 16, p < 0.001). The sustained performance for short decision window lengths thus results in a superior performance (for all subjects) of the FB-CSP method over CCA. We note that the MESD is by definition longer than the decision window length (see Section III-C and [8]). In particular, the theoretical lower limit for the MESD is 3 s when is significantly lower (better) for the FB-CSP method than for the stimulus reconstruction approach (CCA). Each dot represents one subject, the lines connect the same subjects across methods.\n\nusing a minimal decision window length of 1 s 3 [8].\n\n\nB. Comparison with convolutional neural network approach\n\nIn [19], a convolutional neural network (CNN) is used to perform the same task, i.e., decoding the directional focus of attention. This CNN approach has been validated on the same dataset (Dataset I), but with a different testing procedure to avoid overfitting on speakers, i.e., leave-one-story+speakerout (LOSSO) instead of random cross-validation. To provide an honest and transparent comparison of our FB-CSP method with this CNN method, we have cross-validated the performance of the FB-CSP method in the same way as in [19], at the cost of less training and testing data. While data of other subjects are included in the training of the CNN method as a regularization technique [19], this is not done for the FB-CSP method. The EEG data are filtered between 1\u221232 Hz, as proposed in [19] and equivalent to the FB-CSP method. Given the performances in Fig. 4, we, first of all, want to stress that the results of the FB-CSP method for a LOSSO cross-validation are very similar to using a random crossvalidation (Fig. 3). This confirms that, as opposed to the CNN method, our FB-CSP method does not overfit on speakers or stories, which could occur when using random crossvalidation. For the CNN method, the results were significantly better when not leaving out the speaker and/or story in the training set, which could be a sign of overfitting [19]. Furthermore, our FB-CSP method does not perform worse than the CNN method, as a Wilcoxon signed-rank test (W = 56, n = 15, p = 0.85, one outlier subject removed) shows no significant difference based on the MESD (Fig. 4b).\n\nTo conclude, we have identified the following advantages of the FB-CSP method over the CNN method:\n\n\u2022 The FB-CSP method does not perform worse than the CNN method, it even tends to outperform it. \u2022 The FB-CSP method shows less inter-subject variability and is more stable (see the standard error of the mean in Fig. 4a and the spread in Fig. 4b). \u2022 The FB-CSP method requires less training for a better performance. The CNN method uses training data of all (other) subjects, including the test subject, to avoid overfitting. \u2022 The FB-CSP method has a lower computational complexity, which is paramount to be applicable in mobile and wireless hearing devices.\n\n\nC. Binary FB-CSP classification at various speaker positions\n\nWhereas in the previous experiments, the competing speakers are located at -90 \u2022 /+90 \u2022 , this section treats binary AAD First of all, the results in Fig. 5a confirm and reproduce the previous results from Fig. 3a. The accuracy for the -90 \u2022 /+90 \u2022 condition is on average even 10% higher and there is a smaller inter-subject variability (standard deviation is on average over all decision window lengths \u2248 7%, while this was \u2248 10% in Dataset I). A possible explanation for this difference in performance could be that due to the presence of background noise, the spatial cues become more important; or that the subject has to focus harder, thereby generating stronger neural responses. A similar advantageous effect of the presence of background noise was observed in [26].\n\nFurthermore, these results allow analyzing the effect of the angular speaker separation on the decoding performance. The main result from these performances is that decoding the directional focus of attention from the EEG using the FB-CSP classification method still works for various angular scenarios, and even when the speakers are positioned very closely together (-5 \u2022 /+5 \u2022 ) or are positioned at the same side of the head (+30 \u2022 /+90 \u2022 and -90 \u2022 /-30 \u2022 ). The MESDs in Fig. 5b confirm these findings.\n\nAs can be expected, the decoding for the -90 \u2022 /+90 \u2022 sce-nario is easier than in the other scenarios. Although the decoding will fail when speakers are co-located at the same spatial position, the FB-CSP method still succeeds in reliably discriminating between very closely positioned speakers at -5 \u2022 /+5 \u2022 . Furthermore, as the results for -5 \u2022 /+5 \u2022 are still better than when the competing speakers are positioned at the same side of the head, it seems that when speakers are located at different sides of the head, this provides a substantial advantage in decoding the directional focus of attention. However, even when speakers are located at the same side of the head, the method finds sufficient spatio-temporal discriminative patterns to differentiate between speaker locations.\n\nAs an important consequence of these results, the FB-CSP method can be used as a basic building block for a new AAD strategy in which, for example, the whole plane along the azimuth direction is split into angular domains. Depending on the multiclass coding strategy, several FB-CSP filters are then combined to locate the attended speaker in the plane and to steer a beamformer into the correct direction. This AAD strategy is tested in the following section.\n\n\nD. Multi-condition and -class FB-CSP classification\n\nUsing Dataset II, we can verify whether a multi-condition or -class strategy is feasible. In the first experiment, all data are pooled and the FB-CSP classifier tries to determine whether the right-or left-most speaker is attended. In the second experiment, all angles are divided into three angular domains (left/frontal/right) as depicted in Fig. 2. 1) Classifying the right/left-most speaker as attended speaker: Instead of training an FB-CSP for each angular condition separately, all conditions can be pooled and the FB-CSP classifier can be trained to determine whether the user is listening to the right-most or left-most speaker (in a two-speaker scenario), independent of where these speakers are positioned in the plane. As a consequence, a speaker positioned at -30 \u2022 (which is located at the left side of the head) can be the right-most attended speaker, relative to -90 \u2022 , while +30 \u2022 (which is located at the right of -30 \u2022 ) can be the left-most attended speaker, relative to +90 \u2022 . This angular condition-independent FB-CSP classifier could then be used generically to steer a beamformer or to select the attended speaker, provided the angular positions of the competing speakers are known or can be detected from a hearing device's microphone array. In order to test this, all the data of Dataset II are pooled and randomly divided into ten folds. Note that a limitation of this experiment is that the different speaker positions only appear in fixed pairs and that not every position is combined with all other positions. Fig. 6 shows that the accuracy when classifying attention to the left/right-most speaker is still high (77.7% on average over all decision window lengths), although lower than when classifying each condition separately (Fig. 5a). This confirms that this strategy is viable.\n\nWhen investigating the MESDs per angular condition (still when classified all together), it is clear that there are two groups (Table I)   head and show only a small increase in MESD compared to when they are classified separately (compare with Fig. 5b), while there is a larger increase in MESD when the competing speakers are positioned at the same side of the head. Furthermore, the first group shows a lower MESD than the latter one.\n\n2) Classifying between left/frontal/right direction: The intuitive multiclass extension of the binary classification of only two angular conditions is to classify multiple speaker positions at the same time, i.e., determining the directional focus of attention among several possibilities. A possible strategy could be to divide the azimuth plane into different angular domains, which are classified together. In this way, a beamformer could be steered towards the correct angular domain (without having to also estimate the direction of arrival of each speaker separately from the microphone recordings). The higher the spatial resolution of the multiclass strategy, the lower the chance that multiple speakers are present in the same angular domain (in case of multiple competing speakers), but the higher the misclassification error. In case multiple speakers are detected within each angular domain, more angle-specific classifiers or the aforementioned strategy of classifying the left/right-most classifier (Section IV-D1) could be used as a complementary approach.\n\nTo test the feasibility of this strategy, we divide the azimuth plane into three classes based on speaker position as in Fig. 2. The segments in Dataset II are divided into these classes accordingly. Note that the same limitation as before (limited speaker pairs) holds here and that there are no other positions present than \u00b190 \u2022 in domains 1 and 3. A one-versus-all coding scheme is used, which means that there are three binary classifiers trained, which each classify one angular domain versus the other two domains combined.   Fig. 7: The five electrodes of the 64-channel BioSemi system closest to the ear are selected for the channel selection in Section IV-E (blue). In Section IV-H, 38 central electrodes are chosen (orange). Fig. 6 shows the performance curve for this three-class problem. The accuracies are very high and show low subjectto-subject variability (standard deviation \u2248 5.6% over all decision window lengths). Note that the accuracy decreases faster for shorter decision window lengths than usual. This effect is to a lesser extent also present in the binary case and is amplified here because of the multiclass nature of this problem. However, the decrease is still very limited and results in short switch durations (median MESD of 4.32 s over all angular domains, 4.58 s for switching to domain 1, 4.01 s for switching to domain 2, and 5.13 s for switching to domain 3).\n\n\nE. Channel selection\n\nFor the FB-CSP method to be applicable in the context of neuro-steered hearing devices, which is an inherently mobile application, we test the method with a reduced set of EEG channels. However, we do not adopt a traditional data-driven feature/channel selection method but take an application-based point of view. The five electrodes closest to each ear are selected from the 64-channel BioSemi system (see the blue channels on Fig. 7). This can be viewed as a representative selection that mimics current behind-the-ear EEG approaches such as the cEEGrid array [32], which has also been used for AAD [33]. However, it is noted that our analysis is not fully representative of an actual cEEGrid setup due to different recording equipment and different electrode positions. We mainly want to verify whether decoding the directional focus of attention while dominantly measuring from the electrodes on the temporal lobe, is possible.\n\nTo eliminate the dependence on an 'external' or joint reference electrode, the selected EEG channels are re-referenced using a common average reference for each ear separately. By averaging and re-referencing per each ear separately, the two sets of ear channels are galvanically isolated, i.e., emulating two standalone EEG sensor devices which do not have to be connected with a wire. Furthermore, common average referencing is used to eliminate the need for selecting a particular reference electrode. Per ear, one random (as CSP filtering is invariant to the removed channel) re-referenced EEG channel is removed to avoid rank-deficiency in the EEG covariance matrices, effectively leading to 4 channels per ear There is a limited increase in median MESD when selecting ten electrodes for the FB-CSP method, while the CCA method greatly suffers from the channel reduction (compared to Fig. 3b).\n\n(C = 8). After the removal of the other channels and the rereferencing, the complete FB-CSP pipeline (Fig. 1) is retrained and evaluated using the reduced set of EEG channels. Fig. 8a shows that the decrease in accuracy on Dataset I (binary classification) when selecting the ear channels is limited to \u2248 5.6% on average. Furthermore, the median MESD increases from 4.10 s (64 channels) to 4.74 s, which is statistically significant (W = 0, n = 16, p < 0.001), but is still limited. Lastly, from Fig. 8b, it can be seen that there is only a limited increase in variability over subjects.\n\nFurthermore, also the performance of CCA is shown, using the same reduced set of channels and corresponding reference method. The accuracy decreases on average with \u2248 10% over all decision window lengths (Fig. 3a) and does not outperform the FB-CSP method anymore on long decision window lengths. The median MESD drastically increases as well (Fig. 8b). The stimulus reconstruction approach thus suffers more from the channel reduction and is completely outperformed by the FB-CSP method, which is another advantage of the newly proposed method.\n\nWe conclude that decoding the directional focus of attention with the FB-CSP method using a reduced set of channels close to the ear could be possible, but that there is more research required to further validate this approach. channels close to the ear (see Section IV-E) for decision window lengths below 1 s. Below 1 s, the accuracy further degrades, with a limited loss of \u2248 5.5% accuracy on 31.25 ms decision windows and \u2248 8.5% on 15.63 ms decision windows compared to 1 s decision windows. As a result, for both setups, there still is an acceptable performance when taking quasi-instantaneous decisions, resulting in a median MESD of 76.5 ms (64 channels) and 195.0 ms (ear channels) over all subjects. Note that caution is needed when interpreting these MESD values, as on such short decision window lengths, the independence assumption of the Markov model underlying the MESD metric is gravely violated due to the significant autocorrelation values of EEG signals below 1 s lags. The actual time to achieve a sufficiently stable switch may be slightly higher than the one predicted by the model behind the MESD metric. While it may seem surprising that the method can still decode the direction of attention quasi-instantaneously (< 32 ms) with an accuracy that is better than chance, we note that CSP only exploits spatial information (differences between channels) rather than temporal information. Integrating over a longer time window only helps to achieve a better estimate of the log-energies that are fed to LDA, which is the reason behind the slight increase in performance for longer decision windows (compared to instantaneous log-energy estimates). In the case of CSP, the length of the decision window is less important than in stimulus reconstruction approaches, where temporal modulations in the speech envelopes are exploited and where the decision window length directly determines how much of this information is available for discrimination between both speakers. Furthermore, in the case of FB-CSP, the estimation errors on the log-energies (due to quasi-instantaneous estimation) can be further compensated by the LDA classifier by exploiting redundancy in the different filter bands and CSP components to make a reliable decision. Lastly, although the FB-CSP method makes a decision based on a few samples, because of the filterbank on the EEG, these samples are also the result of a weighted integration of previous samples. This means that effectively more samples than the number of samples in the decision window are used. \n\n\nG. CSP classification on an unseen subject\n\nIn the preceding experiments, the FB-CSP filters and LDA classifiers are trained subject-specifically. Here, we test the viability of the subject-independent approach of Section II-D, to improve the practical applicability of this method in neurosteered hearing devices. The same (FB-)CSP classification pipeline ( Fig. 1) and design choices (Section III-B) as before are used, but now tested on Dataset I (binary classification) in a leave-one-subject-out manner. Per test subject, the (FB-)CSP filters and LDA classifier are trained on the 15 other subjects. Without using any of the adaptions from Section II-D, the subject-independent FB-CSP method (SI-FB-CSP) exhibits a large drop in performance in comparison with the subjectspecific FB-CSP method (SS-FB-CSP) (see Fig. 10a).\n\nUpdating the bias as in Section II-D results in a substantial increase of performance of \u2248 4% (SI-FB-CSP-bias-update). The second adaptation reduces the FB-CSP method to a CSP method by using a single frequency band (the \u03b2-band: 12-30 Hz, B = 1), which was experimentally determined (see Section IV-H). Using this CSP method in combination with a bias update of the LDA classifier, results in another increase of accuracy ( Fig. 10a; SI-CSP-bias-update). The best subject-independent CSP classifier, with a bias update and only one frequency band (SI-CSP-bias-update), is compared with the subject-specific FB-CSP classifier (SS-FB-CSP) in Fig. 10a and Fig. 10b. Note that using a single frequency band for the subject-specific method (SS-CSP) results here in a \u2248 2% decrease in accuracy over all decision window lengths. From the MESD, we can see that the subjectindependent method quite nicely approximates the performance of the subject-specific method. For two subjects, the subject-independent method even performs better than the subject-specific FB-CSP method. However, there still is a significant difference (Wilcoxon signed-rank test: W = 127, n = 16, p = 0.0023). Furthermore, from Fig. 10b, it can be seen that the subject-independent method has a larger spread, with more negative outlier values.\n\nWe can thus conclude that the subject-independent CSP classification on average approximates the performance of a subject-specific FB-CSP classifier in terms of MESD, but that there is no guarantee that it will work on every subject. This slightly worse performance is, however, traded for practical applicability, as no a priori calibration session per user is required.\n\n\nH. Decoding mechanisms\n\nGiven that it is possible to decode the directional focus of attention with CSPs, it is relevant to get a handle on what drives the decoding. To investigate which frequency bands are most important, the subject-independent FB-CSP pipeline is trained on all subjects with B = 4 filter bands, corresponding to the main EEG frequency bands (1 \u2212 4 Hz (\u03b4), 4 \u2212 8 Hz (\u03b8), 8 \u2212 12 Hz (\u03b1), and 12 \u2212 30 Hz (\u03b2)). The mean leave-onesubject-out accuracy over all subjects using a 60 s decision window length is 79.7% 4 . To assess the importance of each band, the K = 6 energies related to each band are left out (while keeping all others), leading to a decrease in accuracy to 79.0% for the \u03b4-band, 79.3% for \u03b8-band, 79.0% for the \u03b1band, and 73.2% for the \u03b2-band. This indicates that the \u03b2-band is the most important band, motivating the choice of this band in Section IV-G. Similar conclusions have been drawn in [19], [34]. Furthermore, note that the performance does not degrade over time when the attention is sustained (see Supplementary Material), which has been reported in the context of \u03b1-power lateralization [17]. Fig. 11 shows the spatial activations of the \u03b2-band CSP filters. These topographic maps show activations mainly above the fronto-temporal cortex, consistent with the \u03b2-band activity found in [19], [34]. However, caution is needed when interpreting these spatial maps: the CSP filters implement a socalled 'backward' decoding model, which could implicitly also perform suppression of non-related EEG activity and artifacts, and can thus result in misleading interpretations [35]. To make the spatial maps as interpretable as possible, eye (blink) artifacts have been removed with ICA and muscle artifacts have been removed with CCA [36], making it impossible for the CSP filters to reconstruct and exploit them.\n\nWhether the CSPs exploit neural information or some correlated artifact signal (eye artifacts, EMG, . . . ) is impossible to determine. Intuitively, two specific types of artifact signals could be potentially exploited by the CSPs: eye artifacts (e.g., lateral movements) and EMG activity (especially subtle 4 As the data of the subject under test is used in the CSP training (but not in the LDA training), this accuracy is slightly higher than in Fig. 10. directive ear movements [37]). However, there are several indications that the CSP filters do not exploit these effects and indeed focus mainly on neural activity.\n\nIt is very unlikely that the CSPs exploit eye artifacts, as they are mostly contained in the \u03b4and \u03b8-band, whereas the CSP filters focus on \u03b2-band activity. Secondly, the explicit removal of the eye blinks using ICA does not affect the performance (80.0% on 60 s decision windows). Furthermore, the decoding also works well when the competing speakers are located at the same side of the head (Section IV-C) and even when the subjects are asked to fixate on a cross (tested on the dataset of [38], results not shown).\n\nAs ear movements spectrally (\u03b2-and \u03b3-band) overlap with the information used by the CSPs, it is more difficult to exclude the exploitation of subtle ear movements [37]. There are, however, two counterindications. Firstly, the approach also works for speakers located at the same side of the head (Section IV-C). Secondly, when using only the 38 most central electrodes (out of the 64 channels) furthest away from the ears (see the orange electrodes in Fig. 7) and the \u03b2-band activity, we still obtain a subject-specific accuracy (on Dataset I) of 74.1% on 60 s windows. This at least shows that the decoding still works when only using the central channels, and thus most probably while not being able to pick up ear muscle activity. Furthermore, the topoplots in Fig. 11 show that the CSP filters also exploit channels that are rather far away from the ears, even when the channels close to the ears are included in the data-driven design.\n\n\nV. CONCLUSION\n\nWe have shown that a (filterbank) common spatial patterns classification method is capable of decoding the directional focus of attention, solely based on the EEG. An inherent limitation of this approach is that it requires the competing speakers to be spatially separated. Furthermore, this spatial separation needs to be perceived by the user, which is more difficult for certain hearing impaired populations.\n\nThe proposed method has shown to not only outperform the classical stimulus reconstruction approach for auditory attention decoding in a two-speaker situation but does also not perform worse than a computationally more complex convolutional neural network approach that performs the same task [19]. It achieves practically viable MESDs below 4 s, which has not been achieved by any other AAD method so far [2]. Furthermore, the proposed method has several important advantages, which are important for practical use in neuro-steered hearing device applications: 1) the FB-CSP method does not require clean speech envelopes (in contrast to the traditional stimulus reconstruction approach), such that the extra (error-prone) speech separation step for AAD can be avoided, 2) the performance barely decreases for short decision window lengths and still achieves acceptable performance for quasi-instantaneous decisions, potentially resulting in very fast and robust switching between speakers, 3) the method still works using a limited set of EEG channels above the ears, 4) the method is capable of discriminating between different angular speaker positions, 5) the method can be employed within a multi-condition or multiclass strategy to handle multiple speaker positions at the same time, 6) the method can, provided minor updates, be used in a subject-independent way, trading a minimum of performance for practical applicability. Fast EEG-based decoding of the directional focus of auditory attention using common spatial patterns:\n\nSupplementary material Simon Geirnaert, Tom Francart, and Alexander Bertrand, Senior Member, IEEE\n\nIn the supplementary material, related to the paper Fast EEGbased decoding of the directional focus of auditory attention using common spatial patterns, we investigate the AAD accuracy as a function of time during sustained attention.\n\n\nA. Decoding the directional focus during sustained attention\n\nTo investigate the AAD accuracy as a function of time during sustained attention, we use the leave-one-story+speaker-out cross-validation of Section IV-B in the paper on Dataset I, allowing to leave out full continuous recordings. It is important to verify whether decoding the directional focus of attention is possible during the full duration of a continuous recording, while the subject sustains its attention towards a particular speaker/direction. If the AAD accuracy degrades over time, this means the FB-CSP method only exploits brain lateralization patterns when the subject initially focuses its attention, which has been reported in the context of \u03b1-power lateralization [1]. Fig. 1 shows the averaged performance over continuous trials and subjects as a function of time. As Dataset I contains 6-minute continuous recordings (here referred to as trials) of EEG with sustained attention, the AAD accuracy is shown per 1 s sliding decision window (no overlap) over these trials. The mean accuracy, over all decisions, 6-minute trials, and subjects, is equal to 80.0% and is the same as the accuracy on the 1 s-point in Fig. 4a in the paper. Furthermore, there is no apparent decrease in performance over time, on the contrary, the accuracy seems to slightly increase in the first minute, whereafter the accuracy remains constant. This confirms that the FB-CSP method is capable of decoding the directional focus of attention when the attention is sustained, furthermore, with a similar accuracy as when using random cross-validation (see Fig. 3a).  \n\n\nThis research is funded by an Aspirant Grant from the Research Foundation -Flanders (FWO) (for S. Geirnaert), the KU Leuven Special Research Fund C14/16/057, FWO project nr. G0A4918N, the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 802895 and grant agreement No 637424), and the Flemish Government (AI Research Program). The scientific responsibility is assumed by its authors. (Corresponding author: Simon Geirnaert.) S. Geirnaert and A. Bertrand are with KU Leuven, Department of Electrical Engineering (ESAT), STADIUS Center for Dynamical Systems, Signal Processing and Data Analytics and with Leuven.AI -KU Leuven institute for AI, Kasteelpark Arenberg 10, B-3001 Leuven, Belgium (e-mail: simon.geirnaert@esat.kuleuven.be, alexander.bertrand@esat.kuleuven.be). T. Francart and S. Geirnaert are with KU Leuven, Department of Neurosciences, Research Group ExpORL, Herestraat 49 box 721, B-3000 Leuven, Belgium (e-mail: tom.francart@kuleuven.be).\n\nFig. 1 :\n1The FB-CSP filter outputs are used to generate the features that can be used to classify the EEG segment X.\n\nR\nCi , i.e., the sum of the covariance matrices of all classes except class 1. Correspondingly, an LDA classifier is trained to discriminate direction 1 from all the other directions. This is done for every other direction m \u2208 {1, . . . , M }. Given M directions (classes), this thus results in M different CSP/LDA pairs.\n\nFig. 2 :\n2The competing speakers of Dataset II are located at different angular positions. The azimuth plane is divided into three angular domains, which are used in the multiclass problem of Section IV-D.\n\nFig. 3 :\n3(a) The accuracy (mean \u00b1 standard error of the mean across subjects; Dataset I) of the FB-CSP classification method barely decreases for shorter decision window lengths and outperforms the stimulus reconstruction approach (CCA) for short decision window lengths. Note that the significance level ('sign. level') decreases for shorter decision window lengths due the higher number of test windows. (b) The median MESD (black vertical line)\n\nFig. 4 :\n4(a) The FB-CSP method outperforms the CNN method with on average \u2248 4% in accuracy (mean \u00b1 standard error of the mean; Dataset I). (b) The median MESD is lower for the FB-CSP method than for the CNN method. Note that when the MESD of a subject is not connected to the corresponding MESD, it corresponds to an outlier value of the other method.\n\nFig. 5 :\n5(a) The FB-CSP classification method performs well for all speaker separation angles (Dataset II). Again, the accuracy (mean \u00b1 standard error of the mean) of the FB-CSP classification method barely decreases for shorter decision window lengths. (b) The median MESD is lower (better) for the -90 \u2022 /+90 \u2022 than for the other scenarios. Decoding the directional focus of attention for speakers that are positioned at the same side of the head is harder than when they are symmetrically positioned on different sides of the head. classification at the various speaker positions that are present in Dataset II. Fig. 5 shows the performance of the FB-CSP classification method. For each pair of competing speaker positions, all babble noise conditions are pooled and the FB-CSP classification method is applied. Each pair of positions is thus treated separately, with independently trained CSP filters and LDA classifiers.\n\n\n: the first group contains the conditions where the competing speakers are located along different sides of the\n\nFig. 6 :\n6These peformance curves show that even when pooling all conditions and only classifying the attention to the left/right-most speaker, as well as dividing the upper half-plane in three angular domains as inFig. 2, the accuracy (mean \u00b1 standard error of the mean; Dataset II) is still high.\n\nFp1\n\n\nFig. 8 :\n8(a) The mean accuracy (\u00b1 standard error of the mean) when using only ten electrodes close to the ear on Dataset I (binary classification) decreases relatively little compared to the full 64 channel setup. (b)\n\nFFig. 9 :\n9. Performance on very short decision window lengths (< 1 s)Fig. 9shows the performance of the FB-CSP method on Dataset I (binary classification) for 64 channels and the The performance curves (mean \u00b1 standard error of the mean) of the FB-CSP method degrade below 1 s decision window lengths, while demonstrating acceptable performance even for quasi-instantaneous decisions (Dataset I, binary classification).\n\nFig. 10 :\n10(a) Using a bias update (SI-FB-CSP-bias-update) and only one frequency band (SI-CSP-bias-update) in the subject-independent CSP classification method on Dataset I (binary classification) results in a substantial increase of performance over the baseline (SI-FB-CSP) (mean\u00b1 standard error of the mean). (b) The median MESD of the subject-independent CSP classifier (SI-CSP-bias-update) is very close to the one of the subject-specific FB-CSP classifier (SS-FB-CSP). There is, however, a larger spread, with more negatively (higher) outlying MESD values. Note that when the MESD of a subject is not connected to the corresponding MESD, it corresponds to an outlier value of the other method.\n\nFig. 11 :\n11The topographic plots of the \u03b2-band CSP filters, computed on all data of all subjects of Dataset I, show mainly fronto-temporal activity. The filters of the first row maximize the output energy when left is attended, while those in the second row maximize the output energy when right is attended.\n\nFig. 1 :\n1The performance (mean accuracy \u00b1 standard error of the mean over subjects and different 6-minute trials; Dataset I) does not degrade as a function of time when the attention is sustained. A sliding window of 1 s is used.\n\n\nTab. I: The median MESD is lower when the speakers are located on different sides of the listener. Furthermore, the MESDs are higher compared to the case where each condition is classified separately (MESD sep.; seeFig. 5b).Angular condition MESD LR-most [s] MESD sep. [s] \n\n-5 \u2022 /+5 \u2022 \n4.53 \n3.77 \n-90 \u2022 /+90 \u2022 \n3.77 \n3.49 \n+30 \u2022 /+90 \u2022 \n5.74 \n4.20 \n-90 \u2022 /-30 \u2022 \n8.22 \n4.19 \n\n\nThe code for the subject-specific experiments on this dataset are available at https://github.com/exporl/spatial-focus-of-attention-csp.\nA toolbox to compute this metric is available at https://github.com/exporl/ mesd-toolbox.\nThe theoretical lower limit of the MESD is equal to 3\u00d7 the shortest decision window length that is tested with, as for 100% accuracy, three steps must be taken in the Markov chain[8].\nWe believe that these assets make the FB-CSP method an excellent candidate and a major step forward towards practical neuro-steered hearing devices.VI. ACKNOWLEDGMENTSThe authors would like to thank Simon Van Eyndhoven for providing the implementation of the CSP filter design and the shrinkage covariance estimator.\nEEG-Informed Attended Speaker Extraction From Recorded Speech Mixtures With Application in Neuro-Steered Hearing Prostheses. S Van Eyndhoven, IEEE Trans. Biomed. Eng. 645S. Van Eyndhoven et al., \"EEG-Informed Attended Speaker Extraction From Recorded Speech Mixtures With Application in Neuro-Steered Hearing Prostheses,\" IEEE Trans. Biomed. Eng., vol. 64, no. 5, pp. 1045- 1056, 2017.\n\nNeuro-Steered Hearing Devices: Decoding Auditory Attention From the Brain. S Geirnaert, arXivS. Geirnaert et al., \"Neuro-Steered Hearing Devices: Decoding Auditory Attention From the Brain,\" arXiv, 2020.\n\nSelective cortical representation of attended speaker in multi-talker speech perception. N Mesgarani, E F Chang, Nature. 4857397N. Mesgarani and E. F. Chang, \"Selective cortical representation of attended speaker in multi-talker speech perception,\" Nature, vol. 485, no. 7397, pp. 233-236, 2012.\n\nAttentional Gain Control of Ongoing Cortical Speech Representations in a \"Cocktail Party. J R Kerlin, J. Neurosci. 302J. R. Kerlin et al., \"Attentional Gain Control of Ongoing Cortical Speech Representations in a \"Cocktail Party\",\" J. Neurosci., vol. 30, no. 2, pp. 620-628, 2010.\n\nEmergence of neural encoding of auditory objects while listening to competing speakers. N Ding, J Z Simon, Proc. Natl. Acad. Sci. Natl. Acad. Sci109N. Ding and J. Z. Simon, \"Emergence of neural encoding of auditory objects while listening to competing speakers,\" Proc. Natl. Acad. Sci., vol. 109, no. 29, pp. 11 854-11 859, 2012.\n\nAttentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG. J A O&apos;sullivan, Cereb. Cortex. 257J. A. O'Sullivan et al., \"Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG,\" Cereb. Cortex, vol. 25, no. 7, pp. 1697-1706, 2014.\n\nAuditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario. W Biesmans, IEEE Trans. Neural Syst. Rehabil. Eng. 255W. Biesmans et al., \"Auditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. 25, no. 5, pp. 402-412, 2017.\n\nAn Interpretable Performance Metric for Auditory Attention Decoding Algorithms in a Context of Neuro-Steered Gain Control. S Geirnaert, IEEE Trans. Neural Syst. Rehabil. Eng. 281S. Geirnaert et al., \"An Interpretable Performance Metric for Auditory Attention Decoding Algorithms in a Context of Neuro-Steered Gain Control,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. 28, no. 1, pp. 307-317, 2020.\n\nSpeaker-independent auditory attention decoding without access to clean speech sources. C Han, Sci. Adv. 55C. Han et al., \"Speaker-independent auditory attention decoding without access to clean speech sources,\" Sci. Adv., vol. 5, no. 5, 2019.\n\nCognitive-driven binaural beamforming using EEG-based auditory attention decoding. A Aroudi, S Doclo, IEEE/ACM Trans. Audio, Speech, Language Process. A. Aroudi and S. Doclo, \"Cognitive-driven binaural beamforming us- ing EEG-based auditory attention decoding,\" IEEE/ACM Trans. Audio, Speech, Language Process., pp. 1-1, 2020.\n\nLinear versus deep learning methods for noisy speech separation for EEG-informed attention decoding. N Das, J. Neural Eng. N. Das et al., \"Linear versus deep learning methods for noisy speech separation for EEG-informed attention decoding,\" J. Neural Eng., 2020.\n\nDecoding the direction of auditory motion in blind humans. T Wolbers, NeuroImage. 562T. Wolbers et al., \"Decoding the direction of auditory motion in blind humans,\" NeuroImage, vol. 56, no. 2, pp. 681-687, 2011.\n\nNeural tracking of auditory motion is reflected by delta phase and alpha power of EEG. A Bednar, E C Lalor, NeuroImage. 181A. Bednar and E. C. Lalor, \"Neural tracking of auditory motion is reflected by delta phase and alpha power of EEG,\" NeuroImage, vol. 181, pp. 683-691, 2018.\n\nWhere is the cocktail party? Decoding locations of attended and unattended moving sound sources using EEG. NeuroImage. 205116283--, \"Where is the cocktail party? Decoding locations of attended and unattended moving sound sources using EEG,\" NeuroImage, vol. 205, p. 116283, 2020.\n\nJoint representation of spatial and phonetic features in the human core auditory cortex. P Patel, Cell Rep. 248P. Patel et al., \"Joint representation of spatial and phonetic features in the human core auditory cortex,\" Cell Rep., vol. 24, no. 8, pp. 2051-2062, 2018.\n\nEEG decoding of the target speaker in a cocktail party scenario: considerations regarding dynamic switching of talker location. E S Teoh, E C Lalor, J. Neural Eng. 16336017E. S. Teoh and E. C. Lalor, \"EEG decoding of the target speaker in a cocktail party scenario: considerations regarding dynamic switching of talker location,\" J. Neural Eng., vol. 16, no. 3, p. 036017, 2019.\n\nSpatiotemporal dynamics of auditory attention synchronize with speech. M W\u00f6stmann, Proc. Natl. Acad. Sci. 11314M. W\u00f6stmann et al., \"Spatiotemporal dynamics of auditory attention synchronize with speech,\" Proc. Natl. Acad. Sci., vol. 113, no. 14, pp. 3873-3878, 2016.\n\nSelective Modulation of Auditory Cortical Alpha Activity in an Audiovisual Spatial Attention Task. J N Frey, J. Neurosci. 3419J. N. Frey et al., \"Selective Modulation of Auditory Cortical Alpha Activity in an Audiovisual Spatial Attention Task,\" J. Neurosci., vol. 34, no. 19, pp. 6634-6639, 2014.\n\nEEG-based detection of the locus of auditory attention with convolutional neural networks. S Vandecappelle, bioRxivS. Vandecappelle et al., \"EEG-based detection of the locus of auditory attention with convolutional neural networks,\" bioRxiv, 2020.\n\nOptimizing spatial filters for robust EEG single-trial analysis. B Blankertz, IEEE Signal Process. Mag. 251B. Blankertz et al., \"Optimizing spatial filters for robust EEG single-trial analysis,\" IEEE Signal Process. Mag., vol. 25, no. 1, pp. 41-56, 2007.\n\nA review of classification algorithms for EEG-based brain-computer interfaces: a 10 year update. F Lotte, J. Neural Eng. 15331005F. Lotte et al., \"A review of classification algorithms for EEG-based brain-computer interfaces: a 10 year update,\" J. Neural Eng., vol. 15, no. 3, p. 031005, 2018.\n\nFilter Bank Common Spatial Pattern Algorithm on BCI Competition IV Datasets 2a and 2b. K K Ang, Front. Neurosci. 639K. K. Ang et al., \"Filter Bank Common Spatial Pattern Algorithm on BCI Competition IV Datasets 2a and 2b,\" Front. Neurosci., vol. 6, p. 39, 2012.\n\nSpatio-Spectral Filters for Improving the Classification of Single Trial EEG. S Lemm, IEEE Trans. Biomed. Eng. 529S. Lemm et al., \"Spatio-Spectral Filters for Improving the Classification of Single Trial EEG,\" IEEE Trans. Biomed. Eng., vol. 52, no. 9, pp. 1541-1548, 2005.\n\nDecoding the auditory brain with canonical component analysis. A De Cheveign\u00e9, NeuroImage. 172A. de Cheveign\u00e9 et al., \"Decoding the auditory brain with canonical component analysis,\" NeuroImage, vol. 172, pp. 206-216, 2018.\n\nC M Bishop, Pattern Recognition and Machine Learning. M. Jordan et al.New YorkSpringer-Verlag1st edC. M. Bishop, Pattern Recognition and Machine Learning, 1st ed., M. Jordan et al., Eds. Springer-Verlag New York, 2006.\n\nEEG-based auditory attention detection: boundary conditions for background noise and speaker positions. N Das, J. Neural Eng. 15666017N. Das et al., \"EEG-based auditory attention detection: boundary conditions for background noise and speaker positions,\" J. Neural Eng., vol. 15, no. 6, 2018, 066017.\n\nComparison of Designs Towards a Subject-Independent Brain-Computer Interface based on Motor Imagery. F Lotte, Conf. Proc. F. Lotte et al., \"Comparison of Designs Towards a Subject-Independent Brain-Computer Interface based on Motor Imagery,\" in Conf. Proc. IEEE Eng. Med. Biol. Soc. (EMBC), 2009, pp. 4543-4546.\n\nToward Unsupervised Adaptation of LDA for Brain-Computer Interfaces. C Vidaurre, IEEE Trans. Biomed. Eng. 583C. Vidaurre et al., \"Toward Unsupervised Adaptation of LDA for Brain- Computer Interfaces,\" IEEE Trans. Biomed. Eng., vol. 58, no. 3, pp. 587-597, 2011.\n\nThe effect of head-related filtering and ear-specific decoding bias on auditory attention detection. N Das, J. Neural Eng. 13556014N. Das et al., \"The effect of head-related filtering and ear-specific decoding bias on auditory attention detection,\" J. Neural Eng., vol. 13, no. 5, p. 056014, 2016.\n\nAuditory Attention Detection Dataset KULeuven. Zenodo. --, \"Auditory Attention Detection Dataset KULeuven,\" Zenodo, 2019. [Online]. Available: https://zenodo.org/record/3997352\n\nA well-conditioned estimator for largedimensional covariance matrices. O Ledoit, M Wolf, J. Multivariate Anal. 882O. Ledoit and M. Wolf, \"A well-conditioned estimator for large- dimensional covariance matrices,\" J. Multivariate Anal., vol. 88, no. 2, pp. 365-411, 2004.\n\nUnobtrusive ambulatory EEG using a smartphone and flexible printed electrodes around the ear. S Debener, Sci Rep. 516743S. Debener et al., \"Unobtrusive ambulatory EEG using a smartphone and flexible printed electrodes around the ear,\" Sci Rep, vol. 5, p. 16743, 2015.\n\nTarget Speaker Detection with Concealed EEG Around the Ear. B Mirkovic, Front. Neurosci. 10349B. Mirkovic et al., \"Target Speaker Detection with Concealed EEG Around the Ear,\" Front. Neurosci., vol. 10, p. 349, 2016.\n\nSelective attention enhances beta-band cortical oscillation to speech under \"cocktail-party\" listening conditions. Y Gao, Front. Hum. Neurosci. 1134Y. Gao et al., \"Selective attention enhances beta-band cortical oscillation to speech under \"cocktail-party\" listening conditions,\" Front. Hum. Neurosci., vol. 11, p. 34, 2017.\n\nOn the interpretation of weight vectors of linear models in multivariate neuroimaging. S Haufe, NeuroImage. 87S. Haufe et al., \"On the interpretation of weight vectors of linear models in multivariate neuroimaging,\" NeuroImage, vol. 87, pp. 96-110, 2014.\n\nCanonical Correlation Analysis Applied to Remove Muscle Artifacts From the Electroencephalogram. W De Clercq, IEEE Trans. Biomed. Eng. 5312W. De Clercq et al., \"Canonical Correlation Analysis Applied to Remove Muscle Artifacts From the Electroencephalogram,\" IEEE Trans. Biomed. Eng., vol. 53, no. 12, pp. 2583-2587, 2006.\n\nVestigial auriculomotor activity indicates the direction of auditory attention in humans. D J Strauss, 954536eLifeD. J. Strauss et al., \"Vestigial auriculomotor activity indicates the direction of auditory attention in humans,\" eLife, vol. 9, p. e54536, jul 2020.\n\nOnline detection of auditory attention in a neurofeedback application. R Zink, Proc. 8th International Workshop on Biosignal Interpretation (BSI2016). 8th International Workshop on Biosignal Interpretation (BSI2016)R. Zink et al., \"Online detection of auditory attention in a neurofeedback application,\" in Proc. 8th International Workshop on Biosignal Interpre- tation (BSI2016), 2016.\n\nSpatiotemporal dynamics of auditory attention synchronize with speech. M W\u00f6stmann, Proc. Natl. Acad. Sci. 11314M. W\u00f6stmann et al., \"Spatiotemporal dynamics of auditory attention synchronize with speech,\" Proc. Natl. Acad. Sci., vol. 113, no. 14, pp. 3873-3878, 2016.\n", "annotations": {"author": "[{\"end\":120,\"start\":104},{\"end\":134,\"start\":121},{\"end\":173,\"start\":135}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":110},{\"end\":133,\"start\":125}]", "author_first_name": "[{\"end\":109,\"start\":104},{\"end\":124,\"start\":121},{\"end\":163,\"start\":154},{\"end\":172,\"start\":164}]", "author_affiliation": null, "title": "[{\"end\":101,\"start\":1},{\"end\":274,\"start\":174}]", "venue": null, "abstract": "[{\"end\":2171,\"start\":277}]", "bib_ref": "[{\"end\":3275,\"start\":3272},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3280,\"start\":3277},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3451,\"start\":3448},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3456,\"start\":3453},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4038,\"start\":4035},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4198,\"start\":4195},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4530,\"start\":4527},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4827,\"start\":4824},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5233,\"start\":5230},{\"end\":5782,\"start\":5779},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5787,\"start\":5784},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5793,\"start\":5789},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6669,\"start\":6666},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6675,\"start\":6671},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6681,\"start\":6677},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6777,\"start\":6773},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7232,\"start\":7228},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7721,\"start\":7717},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7727,\"start\":7723},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8243,\"start\":8239},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8426,\"start\":8422},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8432,\"start\":8428},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10882,\"start\":10879},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11122,\"start\":11118},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11759,\"start\":11756},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11765,\"start\":11761},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11771,\"start\":11767},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11777,\"start\":11773},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11783,\"start\":11779},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12082,\"start\":12078},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12088,\"start\":12084},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12698,\"start\":12694},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13077,\"start\":13073},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13294,\"start\":13290},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13567,\"start\":13563},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14403,\"start\":14399},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14802,\"start\":14798},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15712,\"start\":15708},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15867,\"start\":15863},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15873,\"start\":15869},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15952,\"start\":15948},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16163,\"start\":16160},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17157,\"start\":17154},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17166,\"start\":17162},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17563,\"start\":17559},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17964,\"start\":17960},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18725,\"start\":18721},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19746,\"start\":19743},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19751,\"start\":19748},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19756,\"start\":19753},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19762,\"start\":19758},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19768,\"start\":19764},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19800,\"start\":19796},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20111,\"start\":20108},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20635,\"start\":20631},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21415,\"start\":21411},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22179,\"start\":22175},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22320,\"start\":22316},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22921,\"start\":22917},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24355,\"start\":24352},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24364,\"start\":24361},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24627,\"start\":24624},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25652,\"start\":25649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25672,\"start\":25671},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25679,\"start\":25676},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26297,\"start\":26294},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26921,\"start\":26918},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27105,\"start\":27101},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27383,\"start\":27380},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27389,\"start\":27385},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27697,\"start\":27693},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27830,\"start\":27827},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27972,\"start\":27969},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27977,\"start\":27974},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27983,\"start\":27979},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27989,\"start\":27985},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27995,\"start\":27991},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28722,\"start\":28718},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30115,\"start\":30112},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30858,\"start\":30855},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31173,\"start\":31170},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31242,\"start\":31238},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31764,\"start\":31760},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31923,\"start\":31919},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32027,\"start\":32023},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32588,\"start\":32584},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34310,\"start\":34306},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":41447,\"start\":41443},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":41486,\"start\":41482},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":49852,\"start\":49848},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":49858,\"start\":49854},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":50057,\"start\":50053},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":50254,\"start\":50250},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":50260,\"start\":50256},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":50536,\"start\":50532},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":50694,\"start\":50690},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":51080,\"start\":51079},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":51256,\"start\":51252},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":51888,\"start\":51884},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":52078,\"start\":52074},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":53579,\"start\":53575},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":53691,\"start\":53688},{\"end\":55902,\"start\":55899},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":63288,\"start\":63285}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":57808,\"start\":56777},{\"attributes\":{\"id\":\"fig_1\"},\"end\":57927,\"start\":57809},{\"attributes\":{\"id\":\"fig_2\"},\"end\":58250,\"start\":57928},{\"attributes\":{\"id\":\"fig_3\"},\"end\":58457,\"start\":58251},{\"attributes\":{\"id\":\"fig_4\"},\"end\":58907,\"start\":58458},{\"attributes\":{\"id\":\"fig_5\"},\"end\":59261,\"start\":58908},{\"attributes\":{\"id\":\"fig_6\"},\"end\":60189,\"start\":59262},{\"attributes\":{\"id\":\"fig_7\"},\"end\":60303,\"start\":60190},{\"attributes\":{\"id\":\"fig_8\"},\"end\":60603,\"start\":60304},{\"attributes\":{\"id\":\"fig_9\"},\"end\":60609,\"start\":60604},{\"attributes\":{\"id\":\"fig_10\"},\"end\":60829,\"start\":60610},{\"attributes\":{\"id\":\"fig_11\"},\"end\":61251,\"start\":60830},{\"attributes\":{\"id\":\"fig_12\"},\"end\":61954,\"start\":61252},{\"attributes\":{\"id\":\"fig_13\"},\"end\":62265,\"start\":61955},{\"attributes\":{\"id\":\"fig_15\"},\"end\":62497,\"start\":62266},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":62878,\"start\":62498}]", "paragraph": "[{\"end\":2744,\"start\":2190},{\"end\":3354,\"start\":2746},{\"end\":4199,\"start\":3356},{\"end\":4306,\"start\":4201},{\"end\":4531,\"start\":4308},{\"end\":6508,\"start\":4533},{\"end\":7728,\"start\":6510},{\"end\":8143,\"start\":7730},{\"end\":8783,\"start\":8194},{\"end\":9243,\"start\":8785},{\"end\":9931,\"start\":9245},{\"end\":10090,\"start\":10038},{\"end\":10442,\"start\":10132},{\"end\":11123,\"start\":10464},{\"end\":12318,\"start\":11163},{\"end\":13295,\"start\":12439},{\"end\":13615,\"start\":13297},{\"end\":13811,\"start\":13722},{\"end\":14142,\"start\":13842},{\"end\":14803,\"start\":14165},{\"end\":15067,\"start\":14833},{\"end\":15117,\"start\":15087},{\"end\":15231,\"start\":15150},{\"end\":15874,\"start\":15268},{\"end\":16493,\"start\":15876},{\"end\":17348,\"start\":16540},{\"end\":18310,\"start\":17350},{\"end\":18483,\"start\":18341},{\"end\":19423,\"start\":18512},{\"end\":20195,\"start\":19521},{\"end\":20916,\"start\":20197},{\"end\":21812,\"start\":21006},{\"end\":22321,\"start\":21814},{\"end\":23040,\"start\":22392},{\"end\":23236,\"start\":23042},{\"end\":24356,\"start\":23266},{\"end\":26373,\"start\":24358},{\"end\":26511,\"start\":26375},{\"end\":28222,\"start\":26596},{\"end\":29484,\"start\":28224},{\"end\":29857,\"start\":29486},{\"end\":31120,\"start\":29859},{\"end\":31174,\"start\":31122},{\"end\":32812,\"start\":31235},{\"end\":32912,\"start\":32814},{\"end\":33472,\"start\":32914},{\"end\":34311,\"start\":33537},{\"end\":34820,\"start\":34313},{\"end\":35610,\"start\":34822},{\"end\":36072,\"start\":35612},{\"end\":37943,\"start\":36128},{\"end\":38382,\"start\":37945},{\"end\":39455,\"start\":38384},{\"end\":40855,\"start\":39457},{\"end\":41812,\"start\":40880},{\"end\":42712,\"start\":41814},{\"end\":43301,\"start\":42714},{\"end\":43848,\"start\":43303},{\"end\":46406,\"start\":43850},{\"end\":47235,\"start\":46453},{\"end\":48546,\"start\":47237},{\"end\":48919,\"start\":48548},{\"end\":50769,\"start\":48946},{\"end\":51391,\"start\":50771},{\"end\":51909,\"start\":51393},{\"end\":52851,\"start\":51911},{\"end\":53280,\"start\":52869},{\"end\":54817,\"start\":53282},{\"end\":54916,\"start\":54819},{\"end\":55152,\"start\":54918},{\"end\":56776,\"start\":55217}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10037,\"start\":9932},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10131,\"start\":10091},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10463,\"start\":10443},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12353,\"start\":12319},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12438,\"start\":12353},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13721,\"start\":13616},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13841,\"start\":13812},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14832,\"start\":14804},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15086,\"start\":15068},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15149,\"start\":15118},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18340,\"start\":18311},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18511,\"start\":18484},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19470,\"start\":19424},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21005,\"start\":20917},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22391,\"start\":22322}]", "table_ref": "[{\"end\":38081,\"start\":38072}]", "section_header": "[{\"end\":2188,\"start\":2173},{\"end\":8192,\"start\":8146},{\"end\":11161,\"start\":11126},{\"end\":14163,\"start\":14145},{\"end\":15266,\"start\":15234},{\"end\":16538,\"start\":16496},{\"end\":19519,\"start\":19472},{\"end\":23264,\"start\":23239},{\"end\":26540,\"start\":26514},{\"end\":26594,\"start\":26543},{\"end\":31233,\"start\":31177},{\"end\":33535,\"start\":33475},{\"end\":36126,\"start\":36075},{\"end\":40878,\"start\":40858},{\"end\":46451,\"start\":46409},{\"end\":48944,\"start\":48922},{\"end\":52867,\"start\":52854},{\"end\":55215,\"start\":55155},{\"end\":57818,\"start\":57810},{\"end\":57930,\"start\":57929},{\"end\":58260,\"start\":58252},{\"end\":58467,\"start\":58459},{\"end\":58917,\"start\":58909},{\"end\":59271,\"start\":59263},{\"end\":60313,\"start\":60305},{\"end\":60608,\"start\":60605},{\"end\":60619,\"start\":60611},{\"end\":60840,\"start\":60831},{\"end\":61262,\"start\":61253},{\"end\":61965,\"start\":61956},{\"end\":62275,\"start\":62267}]", "table": "[{\"end\":62878,\"start\":62724}]", "figure_caption": "[{\"end\":57808,\"start\":56779},{\"end\":57927,\"start\":57820},{\"end\":58250,\"start\":57931},{\"end\":58457,\"start\":58262},{\"end\":58907,\"start\":58469},{\"end\":59261,\"start\":58919},{\"end\":60189,\"start\":59273},{\"end\":60303,\"start\":60192},{\"end\":60603,\"start\":60315},{\"end\":60829,\"start\":60621},{\"end\":61251,\"start\":60842},{\"end\":61954,\"start\":61265},{\"end\":62265,\"start\":61968},{\"end\":62497,\"start\":62277},{\"end\":62724,\"start\":62500}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11419,\"start\":11411},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15303,\"start\":15297},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20599,\"start\":20593},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26111,\"start\":26105},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28234,\"start\":28227},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30284,\"start\":30277},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32097,\"start\":32091},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32258,\"start\":32250},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32811,\"start\":32802},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33132,\"start\":33125},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33158,\"start\":33151},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33694,\"start\":33687},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33750,\"start\":33743},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34796,\"start\":34789},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36479,\"start\":36472},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":37676,\"start\":37670},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37898,\"start\":37889},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38198,\"start\":38190},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":39584,\"start\":39578},{\"end\":39996,\"start\":39990},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40199,\"start\":40193},{\"end\":41316,\"start\":41309},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":42710,\"start\":42703},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":42823,\"start\":42815},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":42897,\"start\":42890},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":43217,\"start\":43210},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":43516,\"start\":43507},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":43655,\"start\":43646},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46774,\"start\":46768},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":47233,\"start\":47225},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":47669,\"start\":47661},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":47898,\"start\":47877},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48438,\"start\":48430},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":50066,\"start\":50059},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51226,\"start\":51219},{\"end\":52369,\"start\":52363},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52682,\"start\":52675},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":55910,\"start\":55904},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":56353,\"start\":56346},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":56772,\"start\":56765}]", "bib_author_first_name": "[{\"end\":63733,\"start\":63732},{\"end\":64070,\"start\":64069},{\"end\":64289,\"start\":64288},{\"end\":64302,\"start\":64301},{\"end\":64304,\"start\":64303},{\"end\":64587,\"start\":64586},{\"end\":64589,\"start\":64588},{\"end\":64867,\"start\":64866},{\"end\":64875,\"start\":64874},{\"end\":64877,\"start\":64876},{\"end\":65202,\"start\":65201},{\"end\":65204,\"start\":65203},{\"end\":65548,\"start\":65547},{\"end\":65958,\"start\":65957},{\"end\":66323,\"start\":66322},{\"end\":66563,\"start\":66562},{\"end\":66573,\"start\":66572},{\"end\":66909,\"start\":66908},{\"end\":67131,\"start\":67130},{\"end\":67372,\"start\":67371},{\"end\":67382,\"start\":67381},{\"end\":67384,\"start\":67383},{\"end\":67936,\"start\":67935},{\"end\":68243,\"start\":68242},{\"end\":68245,\"start\":68244},{\"end\":68253,\"start\":68252},{\"end\":68255,\"start\":68254},{\"end\":68566,\"start\":68565},{\"end\":68862,\"start\":68861},{\"end\":68864,\"start\":68863},{\"end\":69153,\"start\":69152},{\"end\":69376,\"start\":69375},{\"end\":69664,\"start\":69663},{\"end\":69949,\"start\":69948},{\"end\":69951,\"start\":69950},{\"end\":70203,\"start\":70202},{\"end\":70462,\"start\":70461},{\"end\":70624,\"start\":70623},{\"end\":70626,\"start\":70625},{\"end\":70948,\"start\":70947},{\"end\":71247,\"start\":71246},{\"end\":71528,\"start\":71527},{\"end\":71823,\"start\":71822},{\"end\":72270,\"start\":72269},{\"end\":72280,\"start\":72279},{\"end\":72564,\"start\":72563},{\"end\":72799,\"start\":72798},{\"end\":73072,\"start\":73071},{\"end\":73370,\"start\":73369},{\"end\":73636,\"start\":73635},{\"end\":73639,\"start\":73637},{\"end\":73953,\"start\":73952},{\"end\":73955,\"start\":73954},{\"end\":74199,\"start\":74198},{\"end\":74587,\"start\":74586}]", "bib_author_last_name": "[{\"end\":63747,\"start\":63734},{\"end\":64080,\"start\":64071},{\"end\":64299,\"start\":64290},{\"end\":64310,\"start\":64305},{\"end\":64596,\"start\":64590},{\"end\":64872,\"start\":64868},{\"end\":64883,\"start\":64878},{\"end\":65220,\"start\":65205},{\"end\":65557,\"start\":65549},{\"end\":65968,\"start\":65959},{\"end\":66327,\"start\":66324},{\"end\":66570,\"start\":66564},{\"end\":66579,\"start\":66574},{\"end\":66913,\"start\":66910},{\"end\":67139,\"start\":67132},{\"end\":67379,\"start\":67373},{\"end\":67390,\"start\":67385},{\"end\":67942,\"start\":67937},{\"end\":68250,\"start\":68246},{\"end\":68261,\"start\":68256},{\"end\":68575,\"start\":68567},{\"end\":68869,\"start\":68865},{\"end\":69167,\"start\":69154},{\"end\":69386,\"start\":69377},{\"end\":69670,\"start\":69665},{\"end\":69955,\"start\":69952},{\"end\":70208,\"start\":70204},{\"end\":70475,\"start\":70463},{\"end\":70633,\"start\":70627},{\"end\":70952,\"start\":70949},{\"end\":71253,\"start\":71248},{\"end\":71537,\"start\":71529},{\"end\":71827,\"start\":71824},{\"end\":72277,\"start\":72271},{\"end\":72285,\"start\":72281},{\"end\":72572,\"start\":72565},{\"end\":72808,\"start\":72800},{\"end\":73076,\"start\":73073},{\"end\":73376,\"start\":73371},{\"end\":73646,\"start\":73640},{\"end\":73963,\"start\":73956},{\"end\":74204,\"start\":74200},{\"end\":74596,\"start\":74588}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3206129},\"end\":63992,\"start\":63607},{\"attributes\":{\"id\":\"b1\"},\"end\":64197,\"start\":63994},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4320045},\"end\":64494,\"start\":64199},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":24404227},\"end\":64776,\"start\":64496},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15570759},\"end\":65107,\"start\":64778},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2934551},\"end\":65410,\"start\":65109},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":27835175},\"end\":65832,\"start\":65412},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":202025584},\"end\":66232,\"start\":65834},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":157067650},\"end\":66477,\"start\":66234},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":214048703},\"end\":66805,\"start\":66479},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220603192},\"end\":67069,\"start\":66807},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18255541},\"end\":67282,\"start\":67071},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":51726606},\"end\":67563,\"start\":67284},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":204742059},\"end\":67844,\"start\":67565},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52067765},\"end\":68112,\"start\":67846},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":73471506},\"end\":68492,\"start\":68114},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":28105079},\"end\":68760,\"start\":68494},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":20140745},\"end\":69059,\"start\":68762},{\"attributes\":{\"id\":\"b18\"},\"end\":69308,\"start\":69061},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10908715},\"end\":69564,\"start\":69310},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4853233},\"end\":69859,\"start\":69566},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2908770},\"end\":70122,\"start\":69861},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10683312},\"end\":70396,\"start\":70124},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3531854},\"end\":70621,\"start\":70398},{\"attributes\":{\"id\":\"b24\"},\"end\":70841,\"start\":70623},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52188708},\"end\":71143,\"start\":70843},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8007966},\"end\":71456,\"start\":71145},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":23461673},\"end\":71719,\"start\":71458},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":25324045},\"end\":72018,\"start\":71721},{\"attributes\":{\"id\":\"b29\"},\"end\":72196,\"start\":72020},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":17238653},\"end\":72467,\"start\":72198},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8667640},\"end\":72736,\"start\":72469},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":5261720},\"end\":72954,\"start\":72738},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":23809918},\"end\":73280,\"start\":72956},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4512713},\"end\":73536,\"start\":73282},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8109282},\"end\":73860,\"start\":73538},{\"attributes\":{\"id\":\"b36\"},\"end\":74125,\"start\":73862},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":152218801},\"end\":74513,\"start\":74127},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":28105079},\"end\":74781,\"start\":74515}]", "bib_title": "[{\"end\":63730,\"start\":63607},{\"end\":64286,\"start\":64199},{\"end\":64584,\"start\":64496},{\"end\":64864,\"start\":64778},{\"end\":65199,\"start\":65109},{\"end\":65545,\"start\":65412},{\"end\":65955,\"start\":65834},{\"end\":66320,\"start\":66234},{\"end\":66560,\"start\":66479},{\"end\":66906,\"start\":66807},{\"end\":67128,\"start\":67071},{\"end\":67369,\"start\":67284},{\"end\":67670,\"start\":67565},{\"end\":67933,\"start\":67846},{\"end\":68240,\"start\":68114},{\"end\":68563,\"start\":68494},{\"end\":68859,\"start\":68762},{\"end\":69373,\"start\":69310},{\"end\":69661,\"start\":69566},{\"end\":69946,\"start\":69861},{\"end\":70200,\"start\":70124},{\"end\":70459,\"start\":70398},{\"end\":70945,\"start\":70843},{\"end\":71244,\"start\":71145},{\"end\":71525,\"start\":71458},{\"end\":71820,\"start\":71721},{\"end\":72065,\"start\":72020},{\"end\":72267,\"start\":72198},{\"end\":72561,\"start\":72469},{\"end\":72796,\"start\":72738},{\"end\":73069,\"start\":72956},{\"end\":73367,\"start\":73282},{\"end\":73633,\"start\":73538},{\"end\":74196,\"start\":74127},{\"end\":74584,\"start\":74515}]", "bib_author": "[{\"end\":63749,\"start\":63732},{\"end\":64082,\"start\":64069},{\"end\":64301,\"start\":64288},{\"end\":64312,\"start\":64301},{\"end\":64598,\"start\":64586},{\"end\":64874,\"start\":64866},{\"end\":64885,\"start\":64874},{\"end\":65222,\"start\":65201},{\"end\":65559,\"start\":65547},{\"end\":65970,\"start\":65957},{\"end\":66329,\"start\":66322},{\"end\":66572,\"start\":66562},{\"end\":66581,\"start\":66572},{\"end\":66915,\"start\":66908},{\"end\":67141,\"start\":67130},{\"end\":67381,\"start\":67371},{\"end\":67392,\"start\":67381},{\"end\":67944,\"start\":67935},{\"end\":68252,\"start\":68242},{\"end\":68263,\"start\":68252},{\"end\":68577,\"start\":68565},{\"end\":68871,\"start\":68861},{\"end\":69169,\"start\":69152},{\"end\":69388,\"start\":69375},{\"end\":69672,\"start\":69663},{\"end\":69957,\"start\":69948},{\"end\":70210,\"start\":70202},{\"end\":70477,\"start\":70461},{\"end\":70635,\"start\":70623},{\"end\":70954,\"start\":70947},{\"end\":71255,\"start\":71246},{\"end\":71539,\"start\":71527},{\"end\":71829,\"start\":71822},{\"end\":72279,\"start\":72269},{\"end\":72287,\"start\":72279},{\"end\":72574,\"start\":72563},{\"end\":72810,\"start\":72798},{\"end\":73078,\"start\":73071},{\"end\":73378,\"start\":73369},{\"end\":73648,\"start\":73635},{\"end\":73965,\"start\":73952},{\"end\":74206,\"start\":74198},{\"end\":74598,\"start\":74586}]", "bib_venue": "[{\"end\":64923,\"start\":64908},{\"end\":70701,\"start\":70693},{\"end\":74342,\"start\":74278},{\"end\":63772,\"start\":63749},{\"end\":64067,\"start\":63994},{\"end\":64318,\"start\":64312},{\"end\":64609,\"start\":64598},{\"end\":64906,\"start\":64885},{\"end\":65235,\"start\":65222},{\"end\":65596,\"start\":65559},{\"end\":66007,\"start\":65970},{\"end\":66337,\"start\":66329},{\"end\":66628,\"start\":66581},{\"end\":66928,\"start\":66915},{\"end\":67151,\"start\":67141},{\"end\":67402,\"start\":67392},{\"end\":67682,\"start\":67672},{\"end\":67952,\"start\":67944},{\"end\":68276,\"start\":68263},{\"end\":68598,\"start\":68577},{\"end\":68882,\"start\":68871},{\"end\":69150,\"start\":69061},{\"end\":69412,\"start\":69388},{\"end\":69685,\"start\":69672},{\"end\":69972,\"start\":69957},{\"end\":70233,\"start\":70210},{\"end\":70487,\"start\":70477},{\"end\":70675,\"start\":70635},{\"end\":70967,\"start\":70954},{\"end\":71265,\"start\":71255},{\"end\":71562,\"start\":71539},{\"end\":71842,\"start\":71829},{\"end\":72073,\"start\":72067},{\"end\":72307,\"start\":72287},{\"end\":72581,\"start\":72574},{\"end\":72825,\"start\":72810},{\"end\":73098,\"start\":73078},{\"end\":73388,\"start\":73378},{\"end\":73671,\"start\":73648},{\"end\":73950,\"start\":73862},{\"end\":74276,\"start\":74206},{\"end\":74619,\"start\":74598}]"}}}, "year": 2023, "month": 12, "day": 17}