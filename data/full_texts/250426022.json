{"id": 250426022, "updated": "2023-10-05 12:38:13.133", "metadata": {"title": "Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches", "authors": "[{\"first\":\"Zhiyuan\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"James\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Hongjun\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Guanhong\",\"last\":\"Tao\",\"middle\":[]},{\"first\":\"Zhiwen\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Dongfang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xiangyu\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 meters mean depth estimation error and 93% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle's rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 meters mean depth estimation error and reduce the object detection rate from 90.70% to 5.16% in continuous video frames.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.04718", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ChengLCTCLZ22", "doi": "10.48550/arxiv.2207.04718"}}, "content": {"source": {"pdf_hash": "35efa06e8c55a209677bcb48a6790b654d8b322f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2207.04718v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8f263c49864b625238de7df87160603284a31a7c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/35efa06e8c55a209677bcb48a6790b654d8b322f.txt", "contents": "\nPhysical Attack on Monocular Depth Estimation with Optimal Adversarial Patches\n\n\nZhiyuan Cheng cheng443@cs.purdue.edu \nPurdue University\n\n\nJames Liang \nRochester Institute of Technology\n\n\nHongjun Choi \nPurdue University\n\n\nGuanhong Tao taog@cs.purdue.edu \nPurdue University\n\n\nZhiwen Cao \nPurdue University\n\n\nDongfang Liu dongfang.liu@rit.edu \nRochester Institute of Technology\n\n\nXiangyu Zhang xyzhang@cs.purdue.edu \nPurdue University\n\n\nPhysical Attack on Monocular Depth Estimation with Optimal Adversarial Patches\nPhysical Adversarial AttackMonocular Depth EstimationAutonomous Driving\nDeep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 meters mean depth estimation error and 93% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle's rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 meters mean depth estimation error and reduce the object detection rate from 90.70% to 5.16% in continuous video frames.\n\nIntroduction\n\nMonocular Depth Estimation (MDE) is a technique for estimating the distance between an object and the camera from RGB image inputs. It is a critical vision task for autonomous driving (AD) because it bridges the gap between Lidar sensors and RGB cameras [57] and its measurement has an effect on a variety of downstream perception tasks (e.g., object detection [57], visual SLAM [60], and visual relocalization [50]). For its importance, Tesla has integrated MDE into its productiongrade Autopilot system [6,7], and other AD companies such as Toyota [23] and Huawei [9] are also actively investigating this technique. With the increasing popularity of MDE, ensuring its security becomes a prominent challenge.\n\nExisting adversarial attacks against MDE are implemented in digital- [72,61] or physical-world platforms [68]. Compared to digital-world attacks, attacks in the physical world are more challenging because they require robust perturbations to Fig. 1: Attack MDE and 3D object detection with a natural adversarial patch. The left is a benign scenario and the right is the corresponding adversarial scenario. 3D object detection takes the pseudo-Lidar (i.e., point cloud projected from 2D depth map) as input and outputs bounding boxes of recognized objects. Observe in the adversarial scenario (b) that our optimized adversarial patch can disturb the depth estimation of the target vehicle significantly and the effect propagates to an area larger than the patch itself. Pseudo-Lidar of the vehicle is thus distorted and it cannot be detected in the downstream task.\n\novercome various photometric and geometric changes [10], reducing their stealth. Prior efforts for physical-world adversarial attacks [68,49,26,11] generally employ an unnatural-looking adversarial patch and sacrifice stealth for attack effectiveness, leaving plenty of room for improvement. Additionally, with MDE's rapid development, many downstream tasks that previously require expensive Lidar sensors or depth cameras can now be performed entirely with MDE's measurement and achieve competitive performance. However, the investigation of the impact of compromised MDE on these downstream tasks remains largely unknown.\n\nTo address the aforementioned problems, in this paper, we investigate the stealth of physical-world attack against MDE and present a physical-objectoriented adversarial patch optimization framework to generate stealthy, effective and robust adversarial patches for target objects (e.g., vehicles and pedestrians). In particular, we are able to achieve the followings: we design a physical-objectoriented adversarial optimization, which binds the patch and the target object together regarding attack effects and physical-world transformations ( \u00a73.2); we optimize the patch region on the target object with a differentiable patch mask representation, which automatically locates the highly effective area for attack on the target object and improves attack performance with a small patch size ( \u00a73.3); we camouflage the adversarial pattern with natural styles (e.g., rusty and dirty) with deep photo style transfer [31], resulting in stealthier patch for the attack ( \u00a73.4); we investigate the impact of compromised MDE on a representative downstream task in AD -3D object detection ( \u00a74.4). Fig. 1 gives an example. In addition, we examine our attack with popular defence techniques ( \u00a74.5). Our key contributions are:\n\n1. We develop a physical-object-oriented adversarial patch attack against MDE that balances stealth and effectiveness. To the best of our knowledge, we are the first to investigate stealthy physical-world attacks against MDE considering both the patch size and naturalness. 2. We propose an optimization framework that considers physical object characteristics, has a differentiable patch region representation, and provides natural style based camouflage.\n\n3. We evaluate our attack on 3 representative MDE models and a downstream task with real-world driving scenarios in both digital and physical worlds. Our attack is effective on different types of target objects and state-of-theart models. It causes over 6 meters of mean depth estimation error for a real vehicle, with a patch only 1/9 of the vehicle's rear area, and achieves more than 90% attack success rate in 3D object detection. A video is available at https://youtu.be/L-SyoAsAM0Y.\n\n\nRelated Work\n\nAD Systems Security. In AD, sensor security and autonomy software security are the two important challenges. For sensor security, prior works focus on spoofing/jamming on camera [69,35,38], LiDAR [14,48], RADAR [69], ultrasonic [69], GPS [47] and IMU [55,53]. For autonomy software security, some prior works study regression tasks (e.g., depth estimation [68] and optical flow estimation [41]), and others focus on classification tasks (e.g., 2D object detection and classification [49,11] [61] proposes a way to generate targeted adversarial perturbation on images and alter the depth map arbitrarily. These two attacks focus on digital-space perturbations thus are not directly applicable in the physical world. Yamanaka [68] proposes a method to generate printable adversarial patch for MDE but it does not consider stealth of the patch. Different from prior efforts, we focus on the stealth and to the best of our knowledge, we are the first to examine the stealth of adversarial patches for physical-world attack against MDE.\n\n\nMethod\n\n\nPhysical-object-oriented MDE Attack\n\nMotivation Compared with unconstrained adversarial patches (see Fig. 2a) which often look suspicious, stealthy patches may draw less attention and hence can stay on the target vehicle for an extended period of time, posing a greater threat. We divide the challenge of achieving stealth into two sub-problems: patch size minimization and achieving natural appearance. To minimize patch size, we investigate how to maximize the attack effect with smaller patches and propose two approaches: enlarging the patch's affected area (see comparison in Fig. 2b and c), and locating the adversarial patch in a more sensitive region of the target vehicle (see Fig. 2c, d and e). In terms of naturalness, as the magnitude of perturbations required to launch attack in the physical world is much more substantial, we cannot simply bound the adversarial noise to a human unnoticeable level via various L p -norms as in digital-world attacks, which provides little physical-world robustness. Instead, we use style transfer to disguise the adversarial pattern as natural styles (e.g., dirty or rusty). Attack Pipeline We use an optimization-based method to generate adversarial patches and there are three main optimization goals: increasing the estimated distance of target object ( \u00a73.2); minimizing the patch to locate a sensitive (i.e., most effective) region for attack ( \u00a73.3), and camouflaging the adversarial patch with natural styles ( \u00a73.4). Fig. 3 shows the overview of our attack. From the top left, we start with style transfer on the patch content image. Next, we crop the style-transferred patch with an optimizable patch mask (m \u0398 p ) and paste it onto a target object (O) (e.g., a vehicle) creating an adversarial one (O ). Then, we synthesize adversarial scenarios (R t ) by placing the adversarial object into random scenes with physical transformations (t) and estimate scenarios' depth (D(R t )). We define an adversarial loss (L a ) to increase depth of the target object. Together with a style transfer loss (L st ) maintaining the naturalness and a patch size loss (L m ) minimizing the patch, we perform back propagation and update the patch content and the mask iteratively to address the three optimization goals. The solid lines denote data flow and the dashed lines represent back propagation paths. Each component is explained in details in the following sections.\n\n\nAdversarial Perturbation Generation\n\nIn preparation, we take a photo of the target object (O) and select a patch content image (x) and a style image. Given the patch mask (m p ), we create an adversarial object (O ) by applying the style-transferred patch (x ) on the benign object in the following way:  Fig. 3: Overview of the physical-object-oriented framework to generate a stealthy adversarial patch.\nO = O (1 \u2212 m p ) + x m p ,(1)\nin \u00a73.3 and \u00a73.4. We evaluate the depth of the target object inside a scene because the camera on the victim vehicle captures scene frames as input instead of independent objects. Specifically, in each optimization iteration, we randomly sample a scene from the dataset and paste the adversarial object into the scene to create an adversarial scenario. Unlike previous attacks against autonomous driving systems [13,43] that aim at a particular scene or a road section, our attack is universal and scene-independent.\n\nTo improve the robustness of our attack in the physical world, we apply Expectation of Transformation (EoT) [10] by randomly transforming the object in size, rotation, brightness, saturation, etc., before pasting. The horizontal position of pasting is random, while the vertical position is calculated according to the size of the object considering physical constraints. Specifically, Fig. 4 shows the perspective model of a vehicle in a side view and we assume the camera is facing straight forward without tilt. H is the height of the target vehicle; h is the height of the camera with respect to the victim vehicle; f is the focus length of the camera and \u03b1 relates to the camera's view angle. On the image, the vertical position of the vehicle (d) is calculated from the height of the vehicle (s) with Equation 2. Intuitively, objects farther away appear smaller in perspective so a smaller object after transformation is pasted to a higher vertical position on the image, which is closer to the vanishing point (of the camera), which denotes the furthest physical point in the camera view, and has further depth estimation.\nd = \u2212 h H s + f tan \u03b1 (2)\nFormally, the adversarial scenario R t is described with Equation 3,\nR t = \u039b t (t(O m o ), R)(3)\nwhere t is the random transformation applied on the target object; m o is the object mask used to extract the object from the image; R is the randomly sampled scene from database and \u039b(\u00b7, \u00b7) is the paste operation to combine an adversarial object and a scene following the physical constraint in Equation 2. Since  our goal is to make the target object further away, we want to maximize the object's depth estimation (i.e., minimize the reciprocal). Hence, we define the adversarial loss in Equation 4, where T is a set of transformations; D R is a set of scenes; M SE(\u00b7, \u00b7) is the mean square error between two variables; D is the depth estimation model and M o is the object mask in the scenario.\nL a = E t\u223cT,R\u223cDR M SE D (R t ) \u22121 M o , 0(4)\n\nSensitive Region Localization\n\nAs described in \u00a73.2, we apply the style transferred patch x onto the target object by a patch mask m p which defines the patch region on the target object. Prior works [56,30,28] optimizing masks treat each pixel of the mask as a parameter and the generated mask suffers from low deployability due to sparse and scattered mask regions (See Fig. 11b). Instead we design a novel rectangular patch region optimization method (we call it regional optimization) to locate a sensitive region automatically. Although we define the patch region as rectangular, the final patch is not necessarily rectangular but have an arbitrary predefined shape. Details are explained later. A typical rectangular patch mask has ones within the rectangular borders and zeros otherwise. However, this mask is not differentiable regarding the border parameters because the mask values are not continuous across the borders and border information is not encoded into each mask values, which means that the region cannot be optimized via gradient descent and back propagation. To solve this problem, we design a differentiable soft version of the rectangular mask making it optimizable with respect to four border parameters. Specifically, we define border parameters \u0398 = [l, r, t, b] as shown in Fig. 5a. l and r are the left and right borders' column indices and t and b are the top and bottom borders' row indices. Let w and h be the width and height of the mask respectively and we have 0 \u2264 l \u2264 r \u2264 w and\n0 \u2264 t \u2264 b \u2264 h. m \u0398 p = {m \u0398 p [i, j] | i \u2208 1...w, j \u2208 1...h} m \u0398 p [i, j] = 1 4 (\u2212sign(i \u2212 t) \u00b7 sign(i \u2212 b) + 1) \u00b7(\u2212sign(j \u2212 l) \u00b7 sign(j \u2212 r) + 1),(5)\nTypically, a mask is defined by Equation 5 with \u0398 as parameters, where m \u0398 p \u2208 {0, 1} w\u00d7h is the patch mask and [i, j] is index of the pixel at i-th row and jth column; sign(x) outputs 1 when x \u2265 0 and \u22121 when x < 0; and m \u0398 p [i, j] evaluates to one if and only if the pixel is within the four borders defined by \u0398 and zero otherwise. To make each mask value differentiable regarding border parameters and maintain the property of original definition, we approximate sign(\u00b7) by tanh(\u00b7) and define the patch mask with Equation 6.\nm \u0398 p [i, j] = 1 4 (\u2212 tanh(i \u2212 t) \u00b7 tanh(i \u2212 b) + 1) \u00b7(\u2212 tanh(j \u2212 l) \u00b7 tanh(j \u2212 r) + 1)(6)\nFig. 5b is an example of the mask defined by us. In this example, w and h are 30, l and t are 10, and r and b are 20. Observe that the borders of the rectangular region change gradually. Each pixel value is encoded with border parameters \u0398.\n\nIn the beginning, the patch mask is initialized to cover the whole image, (i.e., l = t = 0, b = h and r = w). One of our optimization goal is to minimize the mask area, thus we define a mask loss term (Equation 7) to penalize the area of mask.\nL m = r \u2212 l + b \u2212 t w + h(7)\nWe use a linear combination of the width and height of the rectangular region to avoid bias in the update of edges. Otherwise, if we use the ratio of area (i.e.,\n(r \u2212 l) \u00d7 (b \u2212 t)/(w \u00d7 h))\nas the mask loss, parameters of the longer edge (e.g., b and t when (b \u2212 t) < (r \u2212 l) ) would have larger gradients and tend to change faster than the shorter edges, which leads to a bias towards updating the longeredge parameters. Using a linear combination avoids this problem and each mask parameter has the same weight. Although we define a rectangular patch region, the final patch mask can be an arbitrary shape within the region. As shown in Fig. 5c, given a predefined patch shape mask m s (m s [i, j] \u2208 {0, 1}), the final patch mask m \u0398 p is calculated by element-wise multiplying the scaled shape mask m s with the region mask m \u0398 p inside the rectangular region. Specifically, in each iteration, given border parameters \u0398, we can scale and fit the predefined shape mask m s into the center of the rectangular region getting mask m s , which is denoted by the red color in Fig. 5c. The final patch mask is calculated with Equation 8 by multiplying the region mask and the shape mask within the rectangular region. Without loss of generality, we focus on rectangular shapes (i.e., m s \u2261 1) in our evaluation.\nm \u0398 p [i, j] = m \u0398 p [i, j] * m s [i, j] i \u2208 l...r, j \u2208 t...b m \u0398 p [i, j] others(8)\nIn addition, our mask definition also supports optimizing with multiple patches. The key point is to take the union of several regions and optimize them together. We leave the details in Appendix A and focus on one patch in our main results.\n\n\nAttack Camouflage\n\nPatches generated in existing adversarial attacks against depth estimation models have obvious perturbations as shown in Fig. 2a. Unlike them, we use style transfer to camouflage the attack with natural styles. There have been works using style transfer [17] in attacking classification models but we are the first to combine style transfer with the more challenging depth estimation attack. We use deep photo style transfer [31] as our style transfer method. This method is a kind of neural style transfer which has demonstrated remarkable results for image stylization [19]. It uses a convolutional neural network (CNN) to extract the deep features of an image and separate the content and style information in the deep feature representations. The source image will be updated iteratively to approach the style information extracted from the style image and keep the content information of the source image. Specifically, as defined in deep photo style transfer [31], there are four terms regarding the style transfer components in the loss function. They are style loss (L s ), content loss (L c ), smoothness loss (L t ) and photorealism regularization loss (L r ). The definition of these four style transfer losses can be found in Appendix B and we refer the readers to [31] for more detailed explanation on each term. The style transfer loss is the sum of those as follows:\nL st = L s + L c + L t + L r (9)\nIn summary, our adversarial patch generation process can be formulated by the following optimization problem:\nmin x ,\u0398 L a + L m + \u03bbL st s.t. x \u2208 [0, 255] 3\u00d7w\u00d7h , \u0398 = {l, r, t, b} 0 \u2264 l \u2264 r \u2264 w, 0 \u2264 t \u2264 b \u2264 h,(10)\nwhere \u03bb is an adjustable weight parameter to balance the style transfer naturalness and attack performance. The ablation study on different values of \u03bb can be found in Appendix E. The weights of other terms are fixed in our experiments. In each iteration, we calculate gradients of x and \u0398 with back propagation and, same as in deep photo style transfer [31], we use LBFGS [12] to update the patch x . We update border parameters \u0398 with Adam [25] and we only update the edge with the maximum absolute gradient instead of four, which avoids the constraint of compressing the region from all directions in each iteration and provides more flexibility. We set a target ratio of the patch region in advance (i.e., the area of the patch region relative to the object) as the stopping criteria of mask optimization. In other words, the mask will stop updating when it is smaller than the predefined target ratio.\n\n\nExperiments\n\n\nExperimental Setup\n\nMDE Model Selection. In our evaluation, we use three monocular depth estimation models: Monodepth2 [21], Depthhints [59], and Manydepth [58]. We selected these models considering representativeness, practicality and open models. The details of the model selection criteria can be found in Appendix C. Target Object Selection. Our attack is generic so it can be applied to any class of objects on public roads. This paper focuses on three representative types of objects to attack: vehicles, traffic barriers, and pedestrians as shown in Fig. 6.   We choose them because they are most common on public roads in regular driving scenarios, and a failure in detecting them could lead to life-threatening consequences. Vehicles are the most attractive objects for attackers since they are the main targets of perception systems on autonomous driving cars. We mainly focus on vehicles in our experiments. Evaluation Scene Selection. We select 100 real-world driving scenes from KITTI dataset [20] to evaluate the attack performance of the generated patch on each object. These scenes cover a wide range of roads (e.g., high-way, local, and rural roads) and background objects (e.g., trucks, traffic lights, and cars). Evaluation Metrics. We use mean depth estimation error (E d ) of the target object and ratio of affected region (R a ) as our evaluation metrics. We use depth estimation of the original object as the ground truth and compare with depth estimation of the adversarial object. The mean depth estimation error denotes the attack effectiveness of our adversarial patch. The larger it is, the better the performance. Equation 11 is the formal definition. Meanings of the symbols are the same as those in \u00a73.\nE d = sum (|D (\u039b(O, R)) \u2212 D (\u039b(O , R)) | M o ) sum(M o )(11)\nThe ratio of affected region (R a ) is defined in Equation 12, where I(x) is the indicator function that evaluates to 1 only when x is true. We define \u2265 10 meters error of depth estimation for a pixel as a valid attack and this pixel will be included in the affected region. R a is the ratio between the number of affected pixels and all pixels of the object. A large value indicates that a broad area is affected.\nR a = sum (I (|D (\u039b(O, R)) \u2212 D (\u039b(O , R)) | M o \u2265 10)) sum(M o )(12)\n\nMain results\n\nWe present our main results regarding effectiveness, robustness and stealth.  Attack Effectiveness. We run our attack with the three MDE models and we target the three types of objects for each model. For each object, we split it into several regions with equal size as shown in Fig. 6 and attack these fixed regions respectively (i.e., optimize the patch on each region.), then we compare with two patch region optimization techniques: our sensitive region localization ( \u00a73.3) and the location-optimized patch [42]. In [42], the authors update the location of a fixed-size patch after each optimization iteration. They tentatively move the patch towards four directions with a predefined stride and select the direction with the least adversarial loss as the next patch location. For a fair comparison, we set the target ratio of patch region the same as that of those fixed regions (e.g., 1/9 of the vehicle's read area). Our regional optimization stops when the patch ratio is smaller than the target ratio. In each test, we evaluate the mean depth estimation error (E d ) of the target object in 100 scenes and take the average of them as the result. In each scene, the object is placed at 7 m away from the victim's camera. We choose 7 m since it is the breaking distance [2] while driving at a speed of 25 mph, which is almost the lowest in normal driving. In other words, it is the smallest distance at which the object has to be detected by the victim to avoid a crash in normal driving scenarios [13]. For the physical world experiments, our experimental setup can be found in Appendix F. Table 1 reports the effectiveness evaluation result. As shown, our attack is generic and effective on different depth estimation models and objects. With our sensitive region localization, an adversarial patch with 1/9 of the vehicle's rear area causes at least 6 m E d across different depth estimation models. Observe that attack performance differs with patch regions. Our sensitive region localization can locate an optimal place that outperforms all those fixed regions and the location optimized regions in [42]. For the physical world experiments, Fig. 8 presents an example. As shown, the adversarial patch on the vehicle fools the vehicle's depth estimation, and the effect is not limited to the patch area but propagates to a broader area. After being projected to 3D space, it is more obvious that the point cloud of the adversarial vehicle is distorted comparing with the benign one. Table 2 reports the physical world attack performance. The first column in the table denotes different drives. The second column shows the time of each drive in seconds. The third column shows the total frames  [16]. We use a vehicle as the target object and Monodepth2 as the depth estimation network. We use the regional optimization and set the target patch size to 1/9 of the vehicle's rear area. We test our attack with and without EoT [10] (see \u00a73.2) during optimization. Fig. 7 shows the result of the robustness evaluation. We report the mean depth estimation error of the target object under different longitude distances with the victim vehicle. Observe that our attack is robust and causes more than 3 m of mean depth estimation error in different victim approaching positions. EoT increases the attack performance by 40.63% and makes our attack more robust in different distances. As shown, the closer the target object, the larger the error in depth estimation, which makes the victim vehicle harder to detect the object from the distorted pseudo-Lidar and continue approaching it until collision. In the physical world experiments, our attack is conducted with real driving scenarios. Compared to evaluating with a single image from a specific position in prior work, continuous and dynamic movement is more challenging and practical. Our attack is shown to be robust under different lighting conditions (e.g., shadows and different light directions), driving operations (e.g., moving straight and turning) and background scenes. The dynamic moving video of our physical world attack is at https://youtu.be/L-SyoAsAM0Y. Stealth As we discussed in our motivation, we consider the stealth in two directions: the naturalness of appearance and the patch size. In terms of naturalness, we compare the adversarial patch generated by our method with the baseline method proposed by Yamanaka et al. [68]. As shown in Fig. 9, our method with style transfer-based camouflage generates more natural patches and is less likely to be identified as adversarial but just a normal sticker. Human studies conducted in [31,17] also justify the naturalness of style-transfer-based image processing. As for the patch size, a smaller size suggests more stealth and less effectiveness. We hence investigate maximizing the attack effect with small patches. We compare the R a caused by our object-oriented attack and the patch-oriented attack in [68] which only attacks the patch area in their adversarial loss design instead of considering the whole object. For a fair comparison, we use style-transfer-based camouflage in both methods and we test with fixed regions and the regional optimization. This experiment is conducted on Monodepth2 [21] targeting the vehicle and other settings are the same as the previous setup in effectiveness evaluation.\n\nAs shown in Fig. 10a, our method (object-oriented) has over 2.5 times higher R a on the vehicle than the baseline (patch-oriented) in all cases, and our method in the regional optimization case outperforms all other fixed-region cases. Hence, with the same total patch area, our object-oriented attack with regional optimization affects a broader area than the baseline. In other words, to achieve similar attack effect, using our method requires a smaller patch and is thus stealthier. Fig. 10b additionally shows the CDF and histogram of depth estimation error in the case with our regional optimization. As shown, more than 80% errors caused by the baseline method are below 10 m, which corresponds to our observation in Fig. 2c that the patch-oriented attack mainly affects the limited patch area and the effect of our method propagates to a broader area causing larger errors.\n\nEvaluations on the transferability of our attack can be found in Appendix D.\n\n\nAblation Study\n\nWe investigate our method through an ablation study. Combinations. As described in \u00a73, we use the object-oriented adversarial loss design and the regional optimization of the patch mask to maximize the attack effect with a small patch. We conduct ablations on these techniques to see how each component contributes. Table 3 shows the result. We attack Monodepth2 and use the vehicle as the target object and report E d and R a . For those tests without regional optimization, we use #5 fixed region because its attack performance is the best among all the fixed regions in previous evaluations. As shown, the object-oriented adversarial loss itself can improve the attack performance  while the regional optimization cannot. The regional optimization is useful only when object-oriented adversarial loss is applied together. It makes sense that the regional optimization has to consider the whole object to find an optimal place regarding the target object. However, the patch-oriented design does not encode the global information so our regional optimization cannot converge to the most effective region. Mask Optimization Methods. We compare our regional optimization with another commonly used mask optimization technique which treats pixels of the patch mask m p as optimizable parameters instead of the four borders. This method has been used in many backdoor scanning works such as Neural Cleanse [56] and ABS [30] to find a trigger that modifies a limited portion of image and causes misclassification. Fig. 11 shows the comparison. Observe that the patch mask generated by the baseline method is more sparse and scattered. The patch unit is tiny. Compared with our method, it is not suitable as a physical world attack vector because it is hard to print and deploy these scattered tiny patches while our regional patch is more practical. Patch Sizes. Larger patches have more effect on depth estimation but are less stealthy. We evaluate our attack on a vehicle object with three different target patch sizes and use three depth estimation models. Fig. 12 shows the result.\n\nObserve that the mean depth estimation error E d and the ratio of affected region R a increase with the size of patch for all three target networks. More ablation studies on the style transfer weight \u03bb are in Appendix E.\n\n\nDownstream Task Impact\n\nWe evaluate the impact of our attack on a point cloud based 3D object detection model -PointPillars [27] and use attack success rate (ASR) as the metric to evaluate our method on 3D object detection. We consider the attack is successful when the benign vehicle can be detected by PointPillar while the adversarial object cannot. Detailed setups can be found in Appendix G. Fig. 13 gives an example of a successful attack. Fig. 13a presents a benign scenario where the benign vehicle can be correctly detected with a 3D bounding box. Fig. 13b shows the corresponding adversarial scenario where the pseudo-Lidar point cloud of the adversarial vehicle is severely distorted by the patch, and thus the vehicle is not detected. The PointPillar network can correctly detect the benign vehicle in all the 100 scenes and the attack success rate (ASR) of different adversarial patches are reported in Table 4. The first column denotes different patch sizes and columns 2-4 refer to the three different target networks. As shown, the ASR is over 90% with all the patch sizes and target networks. Even when the patch size is just 1/9 of the vehicle's rear area, it can still achieve at least 93% ASR, which shows that our attack is an effective method in fooling the 3D object detection model. In the physical world experiments, the fifth column of Table 2 denotes the number of frames in which the vehicle is detected from the pseudo-Lidar point cloud, and the sixth column reports the object detection rate. For benign cases, the rate of successful object detection is 90.70% in 1291 data frames. The rate drops to 5.16% in adversarial cases with 1278 data frames, which shows that our attack is effective and degrades the object detection rate significantly.\n\n\nDefence Discussion\n\nAlthough many defense techniques against adversarial examples have been proposed , none of them focuses on MDE to the best of our knowledge. As a best effort to understand the performance of our attack under different defences, we apply 5 popular defence techniques which perform input transformations without retraining the victim network. They are JPEG compression [18], bit-depth reduction [67], median blurring [67], adding Gaussian noise[71] and autoencoder reformation [34]. Fig. 14 presents our results. We report the E d of the benign vehicle and the adversarial vehicle under different input transformations. An ideal defence should minimize both errors. As shown, our attack can still cause over 5 meters E d in all methods except median blur. In median blur, the attack is mitigated but the benign performance also drops a lot. This shows that these techniques cannot effectively defend our attack without greatly harming the benign performance. This might be because these defenses are mainly for disrupting digital-space human-imperceptible perturbations [43] instead of robust physical world attacks. Detailed descriptions and configurations of the above defences in our evaluation can be found in Appendix H, as well as the discussion of other potential defences such as adversarial training and fusion-based methods.\n\n\nConclusion\n\nWe investigate stealthy physical-world adversarial patch attack against MDE in the AD scenario. We design a novel physical-object-oriented optimization framework to generate stealthy and effective adversarial patches via an object-oriented adversarial loss design, sensitive region localization and natural style based camouflage. Experimental results show that our attack is effective, stealthy and robust against different target objects, state-of-the-art models and a representative downstream task in AD. We achieve over 6 meters of mean depth estimation error for a real vehicle with a patch of 1/9 of the vehicle's rear area and have more than 90% attack success rate in 3D object detection. Popular defence techniques using input transformations cannot defend our attack well.    A1 gives an example of the optimization process of 3 \u00d7 3 initial patches. As the optimization continues, some patches are minimized and disappear, and the final patch is dominated by a single one when the target ratio is 1/9 of the vehicle's back view. We test optimizing with different initial patch setups and the result is shown in Fig. A2. Each curve represents the change of attack effect as the total patch ratio decreases and i \u00d7 j refers to a initial setup of i rows and j columns. As shown, when the target mask ratio is 1/9, different initial patch setups have similar attack performance at the end with the same total patch area. Thus, we mainly focus on optimization of a single patch (i.e., 1 \u00d7 1 setup) in our evaluation.\n\n\nB Style transfer loss terms\n\nStyle Loss. Let F be the feature extraction network, which can be a pre-trained CNN model, x s be the style reference image, x be the adversarial patch example that we will update iteratively. The style loss is defined as the style distance between target image and adversarial example:\nL s = L l=1 G(F l (x s )) \u2212 G(F l (x )) 2 2 (A2)\n, where F l is the extracted features at the l-th layer of F and G is the Gram matrix of the deep features. L is the total number of convolutional layers in F .\n\n\nContent Loss.\n\nThe content loss is designed to preserve the content of the original image since the style loss could make the adversarial example different a lot from the original one. It is defined in Equation A3:\nL c = L l=1 F l (x) \u2212 F l (x ) 2 2 (A3)\n, where x is the original image (i.e., content image). The content loss is to make sure the adversarial example and the original image have similar representation in the deep feature space. In the beginning, the adversarial example is initialized as the content image and the content loss is zero. Content loss will increase once the adversarial example is updated.\n\nPhotorealism Regularization. This term introduced in [31] is to constrain the reconstructed image (i.e., adversarial example) to be represented by locally affine color transformations of the content image to prevent distortions [31], which could make the generated image more realistic. Formally, it is built upon the Matting Laplacian by Levin et al. [29] and defined as follows:\nL r = 3 c=1 V c (x ) M(x)V c (x ) (A4)\n, where c represents the c-th color channel and V c (x ) outputs the vectorized version of the c-th channel of the adversarial example (i.e., V c (x ) \u2208 R N \u00d71 , where N is the number of pixels in image x ). M(x) \u2208 R N \u00d7N is a matrix only depending on content image x and it represents a standard linear system that can minimize a least-square penalty function described in [29]. We refer readers to the original article for a detailed derivation.\n\nSmoothness loss. This loss is designed to reduce the difference between adjacent pixels and encourage a locally smooth output image. As pointed out in [46], the smoothness term is useful in improving the robustness of a physical-world adversarial examples. The smoothness loss is defined in Equation A5:\nL t = i,j x [i, j] \u2212 x[i + 1, j]) 2 + (x [i, j] \u2212 x[i, j + 1]) 2 1 2 (A5)\n, where x[i, j] represents the pixel at i-th row and j-th column of image x.\n\n\nC Model selection criteria\n\nMDE models can be either trained with supervised method (using ground truth depth collected by Lidar or depth camera) or unsupervised method (using video frames or stereo image pairs). Unsupervised models are more attractive to industry because one can easily collect a large amount of training data (e.g. videos) with affordable RGB cameras or reuse existing videos at a low cost. Tesla has declared that they use a self-supervised model in monocular depth estimation [7]. Hence, in our evaluation, we use three monocular depth estimation models: Mon-odepth2 [21], Depthhints [59], and Manydepth [58]. We selected these models with the following criteria.\n\n(1) Representativeness. Among self-supervised monocular depth estimation models, these models are most widely used in many previous research [70,40]. Monodepth2 [21] is one of most successful monocular depth estimation methods. Depthhints [59] is an advanced model that improves performance via additional depth suggestion obtained from stereo algorithms. Manydepth [58] is the stateof-the-art model that uses sequence information from multiple images to achieve better performance.\n\n(2) Practicality. We focus on self-supervised monocular depth estimation models because they do not require ground truth depth data for training, which is usually collected by high-priced Lidar sensors. In contrast, they require only monocular videos or stereo pairs collected by RGB camera(s), enabling them to collect data and train a model economically and efficiently. These techniques have already been in production-level vision-based autonomous driving systems such as Tesla Autopliot [7] and Baidu Apollo Lite [1].\n\n(3) Open Model. The models are publicly available. In our evaluation, we use models trained with both monocular videos and stereo pairs on KITTI dataset [20] and the resolution of input images are 320\u00d71024. These models are publicly available in their project repository on GitHub [5,3,4] D Transferability evaluation  We evaluate the transferability of our adversarial patch in two phases: the transferability across objects and the transferability across networks. For objects, we use three types of vehicles as our target objects. They are a black SUV (V-A), a blue sedan (V-B) and a grey truck (V-C). We use Monodepth2 as the target depth estimation model and generate adversarial patch for these three vehicles respectively. Then we paste each generated patch to three vehicles and evaluate the attacking performance. For the transferability across networks, we use the black SUV as target object and generate adversarial patches with three kinds of monocular depth estimation models, then we evaluate the attacking performance of each patch on the three networks respectively. Other experimental setup is the same as the effectiveness evaluation and we report the mean depth estimation error for attacks.\n\nThe result is shown in Table A1. In Table A1a, the first column denotes the object for which the patch is generated and the first raw denotes the object to which the patch is pasted. Observe that the adversarial patch generated from one vehicle is also effective on other vehicles, which means that our adversarial patch has a good transferability across objects. At the same time, the patch optimized for the target object has the best attacking performance compared with unmatched objects, which shows the effectiveness of our object-specific optimization. In Table A1b, the first column denotes the target network used to generate the patch and the first row denotes the network used to evaluate the attack. Results on the diagonal are white-box attacks and others are black-box attacks. Observe that all three models are vulnerable to white-box attacks, which is consistent with our evaluation of attack effectiveness. For black-box attacks, \n\n\nE More ablation studies\n\nStyle transfer Weight. We also evaluated the impact of different style transfer weight \u03bb. Using a larger style transfer weight can generate more stealthy adversarial patterns while having worse attack performance. We use our default setting in all other experiments as a reference (i.e., \u03bb = 1) and do ablation study with different style transfer weights. We use Monodepth2 and the vehicle as our target network and object. For a fair comparison, we fix the patch region to the optimized region generated in our default setting and place the vehicle at the same position on scene images in each test. Table A2 presents the result. The first column is the style transfer weight parameter. The second and third columns show the generated patch image and the corresponding depth gap caused by the attack. The fourth and fifth columns are the two metrics we used to evaluate attack performance. The last column reports the Structural Similarity Index (SSIM) between the adversarial patch and the original style image, which is a metric to measure the perceptual difference between two images, ranging from 0 to 1, the higher, the more similar. As shown, using a larger style transfer weight can generate more stealthy adversarial patterns while having worse attack performance. Our default setting (\u03bb = 1) makes a good balance between them.\n\n\nF Physical world experiments settings\n\nIn our physical world experiments, we use 2016 BMW X1 as our target object and Monodepth2 as the target monocular depth estimation model. We first take a photo of the vehicle's back view. Then we generate the adversarial patch with our attack method as described in \u00a73. Multiple background scenarios from the dataset are used in this generation process. The vehicle with the generated patch is shown in Fig. 9a. We print the patch and paste it on the optimized region of the target vehicle to create an adversarial car. On the victim side, we use iPhone 11's back camera as the main camera of the victim vehicle. We drive the victim vehicle following the target vehicle at a distance of 7-10 meters and record the adversarial scenario while driving. Fig. A3 (a) and (b) show the inside and outside view of our experimental setup. To explore the attack performance under different conditions, we drive on three routes as shown in Fig. A3 (c), which involves different lighting conditions (e.g., positions of the sun and shadows), driving operations (e.g., going straight and turning), and different background scenes and objects. We drive twice on each route, with the first one a benign case and the second one an adversarial case. Specifically, in the first trip, we drive without any patch, while in the second one, we drive with the patch pasted on the target vehicle. We compare the monocular depth estimation of the benign and the adversarial case to evaluate the effect of our patch. Specifically, we report the mean depth estimation error E d of the vehicle in both benign and adversarial cases. As we can see in Fig. 4, the depth (z) of the vehicle can be calculated with z = f H/s. So, given the focal length (f ) of the camera and the height of the vehicle in the physical world (H) and on the image plane (s), we calculate the vehicle's depth. We use this depth as vehicle's depth ground truth to calculate E d . Also, we project the depth map to pseudo-Lidar point cloud and use PointPillar network to do 3D object detection.\n\n\nG 3D object detection settings\n\nWe use PointPillars as our point cloud-based 3D detection network. The original model is trained with real Lidar data in KITTI object detection dataset. It cannot be directly applied to our pseudo-Lidar data because of their different density and distribution. Hence, we replace the real lidar data in the dataset with corresponding pseudo-Lidar data and train our own model on the new dataset. Specifically, we use Monodepth2 as the monocular depth estimation model and generate pseudo-Lidar for all images in the KITTI object detection dataset replacing original Lidar data. Then we train our PointPillar network on the generated pseudo-Lidar dataset from scratch. The training is the same as original setup and the mean average precision (mAP) of our model on the category of cars is 61.04, which is close to the performance in Apollo (i.e., 63.49).\n\nIn each scene, we place the adversarial vehicle at a distance of 7 meters in the front of the victim vehicle, then we predict the depth of the scenario and project the depth output to 3D space generating pseudo-Lidar point cloud. Next, we use PointPillar to detect 3D objects in this scenario with the point cloud as input. We evaluate on the three depth estimation models and a vehicle object, and use three different target patch sizes in patch region optimization. We perform the evaluation on 100 scenarios and report the attack success rate.\n\n\nH Defence methods and discussion\n\nIn \u00a74.5, we evaluated five popular defencing techniques. The introduction and configuration details of these methods are as follows: JPEG compression: This method uses JPEG image compressing algorithms to compress the input before feeding to the depth estimation network. The compressing operation is expected to disturb the subtle pixel-level adversarial noise and defend the adversarial attack. In our evaluation, we use Python Image Library (PIL) to apply JPEG compression to the input and select the quality level from 90 to 20. Lower quality argument means higher compression rate.\n\nBit-Depth Reduction: Typical RGB images have three channels and each channel has an 8-bit depth. Pixel value of each channel ranges from 0 to 255. Bit-depth reduction is to remap the 8-bit depth to a smaller bit depth. Lower bit-depth has smaller color space and this remap operation can also disturb the adversarial perturbation to defence the attack. In our experiments, we evaluated four smaller bit-depth cases from 5 bits to 2 bits. Median Blur: Median Blur is a method to smooth the image by calculating the median of each pixel's surrounding pixels within a certain kernel size. This defence uses the smoothing effect to remove the adversarial noise. We use the median filter implemented in SciPy and use square kernel size ranging from 5 to 25 in our experiments. Larger kernel size has stronger smoothing effect. Gaussian Noise: This method adds Gaussian noise on the image to disturb the adversarial perturbation since adversarial perturbation is also a kind of precise noise designed to fool the network. The Gaussian noise we add to the image is zero-mean and the standard deviation is from 0.01 to 0.1. As a reference, the image data is normalized to [0, 1]. Gaussian noise with larger standard deviation is stronger. Autoencoder: The Autoencoder is a method proposed in Magnet [34] to defend adversarial attacks. It uses neural networks to filter out the adversarial noise which is not within the distribution of training dataset of the target model. The neural network architecture differs according to dataset and the size of input images, and in our evaluation we use architectures defined in the original work (minist and cifar10) and the architectures designed in [43] (Arch-1 and Arch-2) for large-size images. We train these networks with the KITTI dataset and eliminate the 100 scenes used in our evaluation to avoid in-sample evaluation.\n\nBesides those methods we evaluated above, adversarial training [32] is an effective method to improve the robustness of DNN models against adversarial examples. However, traditional adversarial training is for supervised learning which requires ground truth data while the depth estimation models we target are trained in an unsupervised (i.e., self-supervised) way using videos and stereo image pairs. Hardening self-supervised MDE models with adversarial training effectively and efficiently is still an open problem and we leave it to future work.\n\nAnother direction is to fuse pseudo-lidar and RGB image. Since we consider fully vision-based perception system, the defense cannot include other types of sensors like Lidar, Radar or ultrasonic sensors. To avoid object detection failure, one direction is to make full use of camera frames by fusing pseudo-Lidar and RGB images. Although the point cloud of the target object is distorted by the adversarial patch, the object can still be detected in the RGB image. Fusing both sources may have more robust object detection result. Note that this cannot fundamentally defeat our attack because the object detected in the RGB image does not have depth information and the spatial relationship between the detected target object and the victim vehicle can still be wrong. Also, fusion does not solve the problem of wrong depth estimation.\n\nFig. 2 :\n2(a): Unconstrained adversarial patches in[68,33]  are easy to be identified; traditional patch-oriented attack in (b) affects smaller area than our object-oriented attack in (c); (c), (d) and (e): different regions on the target object have different sensitivity regarding attack effect even with the same total area.\n\nFig. 4 :\n4Perspective projection of a vehicle (side view).\n\nFig. 5 :\n5Patch region definitions.\n\nFig. 6 :\n6Target objects and fixed regions.\n\nFig. 7 :\n7E d of the target vehicle at different distance.\n\nFig. 8 :\n8Physical world attack example.(Video: https://youtu.be/ L-SyoAsAM0Y)\n\nFig. 9 :Fig. 10 :\n910Naturalness Comparing patch-oriented attack (baseline) with our object-oriented attack.evaluated from the video, and we evaluate frames at a frequency of 5 Hz. The fourth column reports the mean depth estimation error (E d ) of the vehicle. As shown, in benign scenarios, the error is under 1 m while the error in adversarial scenarios is over 7 m, which justifies the effectiveness of our attack in the physical world. Attack Robustness. Relative to the victim vehicle, we place the adversarial object at places with longitudinal distances (i.e., forward and back) ranging from 7 m to 35 m and lateral distances (i.e., left and right) ranging from -1 m to 1 m. The 7 m to 35 m longitudinal distance corresponds to the brake distance for driving speed from about 25 to 55 mph[8]. We consider the victim vehicle at the center of the lane, and -1 m to 1 m of lateral deviation from the lane center covers most driving scenarios of the vehicle ahead\n\nFig. 11 :Fig. 12 :\n1112Different Attack performance of regional optimization with different target sizes.\n\nFig. 13 :\n13Attack against 3D object detection.\n\nFig. 14 :\n14Five directly-applicable defence methods. Benign Error : Error caused by the defence in benign cases. Attack Error : Error caused by our attack.\n\n21 .\n21Godard, C., Mac Aodha, O., Firman, M., Brostow, G.J.: Digging into selfsupervised monocular depth prediction (October 2019) 22. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014) 23. Guizilini, V., Ambrus, R., Pillai, S., Raventos, A., Gaidon, A.: 3d packing for selfsupervised monocular depth estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 24. Jia, Y., Lu, Y., Shen, J., Chen, Q.A., Zhong, Z., Wei, T.: Fooling detection alone is not enough: First adversarial attack against multiple object tracking. In: International Conference on Learning Representations (ICLR) (2020) 25. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 26. Komkov, S., Petiushko, A.: Advhat: Real-world adversarial attack on arcface face id system. In: 2020 25th International Conference on Pattern Recognition (ICPR). pp. 819-826. IEEE (2021) 27. Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars: Fast encoders for object detection from point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12697-12705 (2019) 28. Lee, J., Yi, J., Shin, C., Yoon, S.: Bbam: Bounding box attribution map for weakly supervised semantic and instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2643-2652 (2021) 29. Levin, A., Lischinski, D., Weiss, Y.: A closed-form solution to natural image matting. IEEE transactions on pattern analysis and machine intelligence 30(2), 228-242 (2007) 30. Liu, Y., Lee, W.C., Tao, G., Ma, S., Aafer, Y., Zhang, X.: Abs: Scanning neural networks for back-doors by artificial brain stimulation. In: Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. pp. 1265-1282 (2019) 31. Luan, F., Paris, S., Shechtman, E., Bala, K.: Deep photo style transfer. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4990-4998 (2017) 32. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017) 33. Mathew, A., Patra, A.P., Mathew, J.: Monocular depth estimators: Vulnerabilities and attacks. arXiv preprint arXiv:2005.14302 (2020) 34. Meng, D., Chen, H.: Magnet: a two-pronged defense against adversarial examples. In: Proceedings of the 2017 ACM SIGSAC conference on computer and communications security. pp. 135-147 (2017) 35. Nassi, B., Nassi, D., Ben-Netanel, R., Mirsky, Y., Drokin, O., Elovici, Y.: Phantom of the adas: Phantom attacks on driver-assistance systems. IACR Cryptol. ePrint Arch. 2020, 85 (2020) 36. Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.B., Swami, A.: The limitations of deep learning in adversarial settings. In: 2016 IEEE European symposium on security and privacy (EuroS&P). pp. 372-387. IEEE (2016) 37. Pei, K., Cao, Y., Yang, J., Jana, S.: Deepxplore: Automated whitebox testing of deep learning systems. In: proceedings of the 26th Symposium on Operating Systems Principles. pp. 1-18 (2017) 38. Petit, J., Stottelaar, B., Feiri, M., Kargl, F.: Remote attacks on automated vehicles sensors: Experiments on camera and lidar. Black Hat Europe 11(2015), 995 (2015) 39. Qiu, H., Xiao, C., Yang, L., Yan, X., Lee, H., Li, B.: Semanticadv: Generating adversarial examples via attribute-conditioned image editing. In: European Conference on Computer Vision. pp. 19-37. Springer (2020) 40. Ramamonjisoa, M., Firman, M., Watson, J., Lepetit, V., Turmukhambetov, D.: Single image depth prediction with wavelet decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (June 2021) 41. Ranjan, A., Janai, J., Geiger, A., Black, M.J.: Attacking optical flow. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2404-2413 (2019) 42. Rao, S., Stutz, D., Schiele, B.: Adversarial training against location-optimized adversarial patches. In: European Conference on Computer Vision. pp. 429-448. Springer (2020) 43. Sato, T., Shen, J., Wang, N., Jia, Y., Lin, X., Chen, Q.A.: Dirty road can attack: Security of deep learning based automated lane centering under physical-world attack. In: 30th {USENIX} Security Symposium ({USENIX} Security 21). pp. 3309-3326 (2021) 44. Sato, T., Shen, J., Wang, N., Jia, Y.J., Lin, X., Chen, Q.A.: Hold tight and never let go: Security of deep learning based automated lane centering under physical-world attack. arXiv preprint arXiv:2009.06701 (2020) 45. Sato, T., Shen, J., Wang, N., Jia, Y.J., Lin, X., Chen, Q.A.: Wip: Deployability improvement, stealthiness user study, and safety impact assessment on real vehicle for dirty road patch attack. In: Workshop on Automotive and Autonomous Vehicle Security (AutoSec). vol. 2021, p. 25 (2021) 46. Sharif, M., Bhagavatula, S., Bauer, L., Reiter, M.K.: Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In: Proceedings of the 2016 acm sigsac conference on computer and communications security. pp. 1528-1540 (2016) 47. Shen, J., Won, J.Y., Chen, Z., Chen, Q.A.: Drift with devil: Security of multi-sensor fusion based localization in high-level autonomous driving under {GPS} spoofing. In: 29th {USENIX} Security Symposium ({USENIX} Security 20). pp. 931-948 (2020) 48. Shin, H., Kim, D., Kwon, Y., Kim, Y.: Illusion and dazzle: Adversarial optical channel exploits against lidars for automotive applications. In: International Conference on Cryptographic Hardware and Embedded Systems. pp. 445-467. Springer (2017) 49. Song, D., Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Tramer, F., Prakash, A., Kohno, T.: Physical adversarial examples for object detectors. In: 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18) (2018) 50. von Stumberg, L., Wenzel, P., Yang, N., Cremers, D.: Lm-reloc: Levenbergmarquardt based direct visual relocalization. In: International Conference on 3D Vision (3DV) (2020) 51. Tang, K., Shen, J.S., Chen, Q.A.: Fooling perception via location: A case of regionof-interest attacks on traffic light detection in autonomous driving. In: NDSS Workshop on Automotive and Autonomous Vehicle Security (AutoSec) (2021) 52. Thys, S., Van Ranst, W., Goedem\u00e9, T.: Fooling automated surveillance cameras: adversarial patches to attack person detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 0-0 (2019) 53. Trippel, T., Weisse, O., Xu, W., Honeyman, P., Fu, K.: Walnut: Waging doubt on the integrity of mems accelerometers with acoustic injection attacks. In: 2017 71. Zhang, Y., Liang, P.: Defending against whitebox adversarial attacks via randomized discretization. In: The 22nd International Conference on Artificial Intelligence and Statistics. pp. 684-693. PMLR (2019) 72. Zhang, Z., Zhu, X., Li, Y., Chen, X., Guo, Y.: Adversarial attacks on monocular depth estimation. arXiv preprint arXiv:2003.10315 (2020) Fig. A1: Optimization with multiple initial patches.\n\nFig. A2 :\nA2Optimization with multiple initial patches.\n\nFig.\nFig. A1 gives an example of the optimization process of 3 \u00d7 3 initial patches. As the optimization continues, some patches are minimized and disappear, and the final patch is dominated by a single one when the target ratio is 1/9 of the vehicle's back view. We test optimizing with different initial patch setups and the result is shown in Fig. A2. Each curve represents the change of attack effect as the total patch ratio decreases and i \u00d7 j refers to a initial setup of i rows and j columns. As shown, when the target mask ratio is 1/9, different initial patch setups have similar attack performance at the end with the same total patch area. Thus, we mainly focus on optimization of a single patch (i.e., 1 \u00d7 1 setup) in our evaluation.\n\n\nThis work focuses on autonomy software security, that is, compromising MDE and its related downstream tasks. Physical-world Adversarial Attacks. Many prior efforts in adversarial attacks have been directed toward generating patches or perturbations in the digital space [37,22,36,62,65,63,39,54,64]. In comparison, we conduct extensive experiments on adversarial attacks in the physical world. Although existing physical-world attacks have addressed tasks such as image classification [49,11], object detection [15,66,52], face recognition [46,26], the domain of depth estimation attack has received scant attention. Moreover, the correlations between stealth and attack effectiveness are largely understudied in the literature. In this paper, we make an attempt to close the aforementioned knowledge gap. MDE Attacks. Zhang [72] proposes a multi-task attack strategy to improve the performance in the universal attack scenario. Wong, tracking [24], lane detection [44,45], and traffic \nlight detection [51]). \n\n\nwhere denotes the element-wise multiplication and O, m p , x have the same width and height. We explain the patch mask definition and style transfer laterPatch Mask( ) \nObject Mask ( \n) \n\nRandom Scene ( ) \n\nContent ( ) \nStyle ( \u2032) \n\nStyle Transfer \n\nDepth Estimation (D( \u2032 )) \n\n\u03a3 \n\nPhysical transformations ( ) and placements \n\nMDE \n\nData Flow \nBack Propagation \n\nRegion update \nContent update \n\nCreate adv. object \n\nBenign Object ( ) \nAdv. Object( \u2032) \n\nAdv. Scenarios ( \u2032 ) \n\n(2) Lst \n\n(1) La \n\n(3) Lm \n\nScenario Object Masks ( \n) \n\n\n\nTable 1 :\n1Mean depth estimation error (E d ) in attacking fixed regions and optimized regions.Mono \nDH \nMany \nV \nTB \nP \nV \nTB \nP \nV TB \nP \n\nOurs 16.84 8.26 14.06 15.23 4.54 13.17 6.31 3.57 10.15 \n\nLO 13.90 5.21 11.53 2.51 1.63 10.79 3.03 2.94 8.93 \n\nR1 3.70 2.35 10.20 2.25 1.50 11.78 1.12 2.77 9.21 \n\nR2 7.41 2.67 11.28 4.66 1.40 10.52 4.23 1.40 8.66 \n\nR3 5.20 4.96 5.05 3.92 1.45 4.08 1.33 3.05 5.06 \n\nR4 7.31 1.59 \n-\n5.58 1.59 \n-\n4.89 1.59 \n-\n\nR5 14.95 2.39 \n-\n7.70 0.90 \n-\n5.66 2.43 \n-\n\nR6 9.69 2.59 \n-\n2.37 0.49 \n-\n1.36 1.15 \n-\n\nR7 3.23 \n-\n-\n2.62 \n-\n-\n1.67 \n-\n-\n\nR8 7.74 \n-\n-\n4.44 \n-\n-\n4.91 \n-\n-\n\nR9 5.36 \n-\n-\n1.38 \n-\n-\n1.32 \n-\n-\n\nMono: Monodepth2, DH: DepthHints, Many: Manydepth \nV: Vehicle, TB: Traffic Barrier, P: Pedestrian \nLO: Location Optimize in [42], R: Region \n\n\n\nTable 2 :\n2Physical world attack result.Time (s) Frames Ed Detected Detection Rate \n\nRoute 1 Benign \n95 \n477 0.52 \n469 \n98.32% \n\nRoute 2 Benign \n82 \n412 0.77 \n354 \n85.92% \n\nRoute 3 Benign \n80 \n402 0.62 \n348 \n86.57% \n\nTotal Benign \n257 \n1291 0.64 1171 \n90.70% \n\nRoute 1 Adv. \n94 \n468 6.73 \n45 \n9.62% \n\nRoute 2 Adv. \n82 \n408 8.92 \n11 \n2.70% \n\nRoute 3 Adv. \n80 \n402 7.68 \n10 \n2.49% \n\nTotal Adv. \n256 \n1278 7.77 \n66 \n5.16% \n\n\n\nTable 3 :\n3Ablation study .OA \nRO \nE d \nR a \n\n8.47 \n0.23 \n\n6.38 \n0.16 \n\n14.95 \n0.52 \n\n16.84 0.65 \n\nOA: Object-oriented Adv. Loss \nRO: Regional Optimization \n\n\n\nTable 4 :\n4Attack success rate of different adversarial patches.Monodepth2 DepthHints Manydepth \n\n1/9 Area \n95% \n93% \n98% \n\n2/9 Area \n98% \n97% \n100% \n\n1/3 Area \n100% \n100% \n100% \n\n\n\nTable A1 :\nA1Transferability evaluation.\n\nTable A2 :\nA2Patches generated with different style transfer weightsFig. A3: Physical world experiments setupMonodepth2 is the most vulnerable since patches generated from other two networks have a strong effect on it while Manydepth and Depthhints are more robust. In summary, our attack has a good transferability towards Monodepth2 but is less effective on Depth Hints and Manydepth.\u03bb \nPatch Image \nDepth Gap \nEd \nRa \nSSIM \n\n0.1 \n11.09 \n0.373 \n0.108 \n\n1 \n7.69 \n0.246 \n0.575 \n\n10 \n1.44 \n0.003 \n0.924 \n\n100 \n0.65 \n0.001 \n0.993 \n\nAppendixThis document provides more details about our work and additional experimental settings and results. We organize the content of Appendix as follows:\u2022 \u00a7A: Optimizing multiple patch regions.\u2022 \u00a7B: Style transfer loss terms.\u2022 \u00a7C: MDE model selection criteria.\u2022 \u00a7D: Transferability evaluation.\u2022 \u00a7E: More ablation studies.\u2022 \u00a7F: Physical world experiments settings.\u2022 \u00a7G: 3D object detection settings.\u2022 \u00a7H: Defence methods and discussion.A Optimizing multiple patch regionsThe patch region does not have to be one part. There could be multiple patches on the target object and our regional optimization technique also supports optimizing multiple patches. In this case, the final patch mask is the sum of multiple sub-masks with each sub-mask representing one rectangular region. The final mask is defined inEquation A1, where m \u0398i p refers to the i-th sub-mask and \u0398 i denotes its boundary parameters and clamp() is a function to restrict the mask values in between 0 and 1. The mask loss L m is also the sum of all sub-mask loss terms.\nBaidu unveils Apollo Lite Level 4 vision-based autonomous driving solution. Baidu unveils Apollo Lite Level 4 vision-based autonomous driving solution, https: //autonews.gasgoo.com/m/Detail/70016068.html\n\n. Manydepth Github, Manydepth Github, https://github.com/nianticlabs/manydepth\n\n. Monodepth2 Github, Monodepth2 Github, https://github.com/nianticlabs/monodepth2\n\n. A I Tesla, Day, Tesla AI day, https://youtu.be/j0z4FweCy4M?t=5295\n\nTesla use per-pixel depth estimation with self-supervised learning. Tesla use per-pixel depth estimation with self-supervised learning, https://youtu. be/hx7BXih7zx8?t=1334\n\nVehicle Stopping Distance. Vehicle Stopping Distance, http://www.csgnetwork.com/stopdistcalc.html\n\nBidirectional attention network for monocular depth estimation. S Aich, J M U Vianney, M A Islam, M Kaur, B Liu, arXiv:2009.00743arXiv preprintAich, S., Vianney, J.M.U., Islam, M.A., Kaur, M., Liu, B.: Bidirectional attention network for monocular depth estimation. arXiv preprint arXiv:2009.00743 (2020)\n\nSynthesizing robust adversarial examples. A Athalye, L Engstrom, A Ilyas, K Kwok, International conference on machine learning. PMLRAthalye, A., Engstrom, L., Ilyas, A., Kwok, K.: Synthesizing robust adversarial examples. In: International conference on machine learning. pp. 284-293. PMLR (2018)\n\nT B Brown, D Man\u00e9, A Roy, M Abadi, J Gilmer, arXiv:1712.09665Adversarial patch. arXiv preprintBrown, T.B., Man\u00e9, D., Roy, A., Abadi, M., Gilmer, J.: Adversarial patch. arXiv preprint arXiv:1712.09665 (2017)\n\nA limited memory algorithm for bound constrained optimization. R H Byrd, P Lu, J Nocedal, C Zhu, SIAM Journal on scientific computing. 165Byrd, R.H., Lu, P., Nocedal, J., Zhu, C.: A limited memory algorithm for bound constrained optimization. SIAM Journal on scientific computing 16(5), 1190-1208 (1995)\n\nInvisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks. Y Cao, N Wang, C Xiao, D Yang, J Fang, R Yang, Q A Chen, M Liu, B Li, 2021 IEEE Symposium on Security and Privacy (SP). IEEECao, Y., Wang, N., Xiao, C., Yang, D., Fang, J., Yang, R., Chen, Q.A., Liu, M., Li, B.: Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks. In: 2021 IEEE Symposium on Security and Privacy (SP). pp. 176-194. IEEE (2021)\n\nAdversarial sensor attack on lidar-based perception in autonomous driving. Y Cao, C Xiao, B Cyr, Y Zhou, W Park, S Rampazzi, Q A Chen, K Fu, Z M Mao, Proceedings of the 2019 ACM SIGSAC conference on computer and communications security. the 2019 ACM SIGSAC conference on computer and communications securityCao, Y., Xiao, C., Cyr, B., Zhou, Y., Park, W., Rampazzi, S., Chen, Q.A., Fu, K., Mao, Z.M.: Adversarial sensor attack on lidar-based perception in autonomous driving. In: Proceedings of the 2019 ACM SIGSAC conference on computer and communications security. pp. 2267-2281 (2019)\n\nShapeshifter: Robust physical adversarial attack on faster r-cnn object detector. S T Chen, C Cornelius, J Martin, D H P Chau, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerChen, S.T., Cornelius, C., Martin, J., Chau, D.H.P.: Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pp. 52-68. Springer (2018)\n\nComparison of lateral controllers for autonomous vehicle: Experimental results. S Dominguez, A Ali, G Garcia, P Martinet, 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC). IEEEDominguez, S., Ali, A., Garcia, G., Martinet, P.: Comparison of lateral controllers for autonomous vehicle: Experimental results. In: 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC). pp. 1418-1423. IEEE (2016)\n\nAdversarial camouflage: Hiding physical-world attacks with natural styles. R Duan, X Ma, Y Wang, J Bailey, A K Qin, Y Yang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionDuan, R., Ma, X., Wang, Y., Bailey, J., Qin, A.K., Yang, Y.: Adversarial cam- ouflage: Hiding physical-world attacks with natural styles. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1000-1008 (2020)\n\nG K Dziugaite, Z Ghahramani, D M Roy, arXiv:1608.00853A study of the effect of jpg compression on adversarial images. arXiv preprintDziugaite, G.K., Ghahramani, Z., Roy, D.M.: A study of the effect of jpg compres- sion on adversarial images. arXiv preprint arXiv:1608.00853 (2016)\n\nImage style transfer using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionGatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2414-2423 (2016)\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, Conference on Computer Vision and Pattern Recognition (CVPR). Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark suite. In: Conference on Computer Vision and Pattern Recogni- tion (CVPR) (2012)\n\nIEEE European symposium on security and privacy (EuroS&P). IEEEIEEE European symposium on security and privacy (EuroS&P). pp. 3-18. IEEE (2017)\n\nRobust adversarial objects against deep learning models. T Tsai, K Yang, T Y Ho, Y Jin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Tsai, T., Yang, K., Ho, T.Y., Jin, Y.: Robust adversarial objects against deep learning models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 954-962 (2020)\n\nInjected and delivered: Fabricating implicit control over actuation systems by spoofing inertial sensors. Y Tu, Z Lin, I Lee, X Hei, 27th {USENIX} Security Symposium ({USENIX} Security 18). Tu, Y., Lin, Z., Lee, I., Hei, X.: Injected and delivered: Fabricating implicit control over actuation systems by spoofing inertial sensors. In: 27th {USENIX} Security Symposium ({USENIX} Security 18). pp. 1545-1562 (2018)\n\nB Wang, Y Yao, S Shan, H Li, B Viswanath, H Zheng, B Y Zhao, Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In: 2019 IEEE Symposium on Security and Privacy (SP). IEEEWang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., Zhao, B.Y.: Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In: 2019 IEEE Symposium on Security and Privacy (SP). pp. 707-723. IEEE (2019)\n\nPseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. Y Wang, W L Chao, D Garg, B Hariharan, M Campbell, K Q Weinberger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWang, Y., Chao, W.L., Garg, D., Hariharan, B., Campbell, M., Weinberger, K.Q.: Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detec- tion for autonomous driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8445-8453 (2019)\n\nThe Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. J Watson, O M Aodha, V Prisacariu, G Brostow, M Firman, Computer Vision and Pattern Recognition (CVPR). Watson, J., Aodha, O.M., Prisacariu, V., Brostow, G., Firman, M.: The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. In: Computer Vision and Pattern Recognition (CVPR) (2021)\n\nSelf-supervised monocular depth hints. J Watson, M Firman, G J Brostow, D Turmukhambetov, The International Conference on Computer Vision (ICCV). Watson, J., Firman, M., Brostow, G.J., Turmukhambetov, D.: Self-supervised monocular depth hints. In: The International Conference on Computer Vision (ICCV) (October 2019)\n\nMonorec: Semi-supervised dense reconstruction in dynamic environments from a single moving camera. F Wimbauer, N Yang, L Von Stumberg, N Zeller, D Cremers, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Wimbauer, F., Yang, N., von Stumberg, L., Zeller, N., Cremers, D.: Monorec: Semi-supervised dense reconstruction in dynamic environments from a single mov- ing camera. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)\n\nTargeted adversarial perturbations for monocular depth prediction. A Wong, S Cicek, S Soatto, arXiv:2006.08602arXiv preprintWong, A., Cicek, S., Soatto, S.: Targeted adversarial perturbations for monocular depth prediction. arXiv preprint arXiv:2006.08602 (2020)\n\nC Xiao, B Li, J Y Zhu, W He, M Liu, D Song, arXiv:1801.02610Generating adversarial examples with adversarial networks. arXiv preprintXiao, C., Li, B., Zhu, J.Y., He, W., Liu, M., Song, D.: Generating adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610 (2018)\n\nC Xiao, X Pan, W He, J Peng, M Sun, J Yi, M Liu, B Li, D Song, arXiv:1907.09470Characterizing attacks on deep reinforcement learning. arXiv preprintXiao, C., Pan, X., He, W., Peng, J., Sun, M., Yi, J., Liu, M., Li, B., Song, D.: Char- acterizing attacks on deep reinforcement learning. arXiv preprint arXiv:1907.09470 (2019)\n\nMeshadv: Adversarial meshes for visual recognition. C Xiao, D Yang, B Li, J Deng, M Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiao, C., Yang, D., Li, B., Deng, J., Liu, M.: Meshadv: Adversarial meshes for visual recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6898-6907 (2019)\n\nC Xiao, J Y Zhu, B Li, W He, M Liu, D Song, arXiv:1801.02612Spatially transformed adversarial examples. arXiv preprintXiao, C., Zhu, J.Y., Li, B., He, W., Liu, M., Song, D.: Spatially transformed ad- versarial examples. arXiv preprint arXiv:1801.02612 (2018)\n\nAdversarial t-shirt! evading person detectors in a physical world. K Xu, G Zhang, S Liu, Q Fan, M Sun, H Chen, P Y Chen, Y Wang, X Lin, European Conference on Computer Vision. SpringerXu, K., Zhang, G., Liu, S., Fan, Q., Sun, M., Chen, H., Chen, P.Y., Wang, Y., Lin, X.: Adversarial t-shirt! evading person detectors in a physical world. In: European Conference on Computer Vision. pp. 665-681. Springer (2020)\n\nW Xu, D Evans, Y Qi, arXiv:1704.01155Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprintXu, W., Evans, D., Qi, Y.: Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155 (2017)\n\nAdversarial patch attacks on monocular depth estimation networks. K Yamanaka, R Matsumoto, K Takahashi, T Fujii, IEEE Access. 8Yamanaka, K., Matsumoto, R., Takahashi, K., Fujii, T.: Adversarial patch attacks on monocular depth estimation networks. IEEE Access 8, 179094-179104 (2020)\n\nCan you trust autonomous vehicles: Contactless attacks against sensors of self-driving vehicle. C Yan, W Xu, J Liu, Def Con. 248109Yan, C., Xu, W., Liu, J.: Can you trust autonomous vehicles: Contactless attacks against sensors of self-driving vehicle. Def Con 24(8), 109 (2016)\n\nD3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. N Yang, L V Stumberg, R Wang, D Cremers, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYang, N., Stumberg, L.v., Wang, R., Cremers, D.: D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1281-1292 (2020)\n", "annotations": {"author": "[{\"end\":139,\"start\":82},{\"end\":188,\"start\":140},{\"end\":222,\"start\":189},{\"end\":275,\"start\":223},{\"end\":307,\"start\":276},{\"end\":378,\"start\":308},{\"end\":435,\"start\":379}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":90},{\"end\":151,\"start\":146},{\"end\":201,\"start\":197},{\"end\":235,\"start\":232},{\"end\":286,\"start\":283},{\"end\":320,\"start\":317},{\"end\":392,\"start\":387}]", "author_first_name": "[{\"end\":89,\"start\":82},{\"end\":145,\"start\":140},{\"end\":196,\"start\":189},{\"end\":231,\"start\":223},{\"end\":282,\"start\":276},{\"end\":316,\"start\":308},{\"end\":386,\"start\":379}]", "author_affiliation": "[{\"end\":138,\"start\":120},{\"end\":187,\"start\":153},{\"end\":221,\"start\":203},{\"end\":274,\"start\":256},{\"end\":306,\"start\":288},{\"end\":377,\"start\":343},{\"end\":434,\"start\":416}]", "title": "[{\"end\":79,\"start\":1},{\"end\":514,\"start\":436}]", "venue": null, "abstract": "[{\"end\":1843,\"start\":587}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2117,\"start\":2113},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2224,\"start\":2220},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2242,\"start\":2238},{\"end\":2274,\"start\":2270},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2367,\"start\":2364},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2369,\"start\":2367},{\"end\":2413,\"start\":2409},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2428,\"start\":2425},{\"end\":2643,\"start\":2639},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2646,\"start\":2643},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2679,\"start\":2675},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3491,\"start\":3487},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3574,\"start\":3570},{\"end\":3577,\"start\":3574},{\"end\":3580,\"start\":3577},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3583,\"start\":3580},{\"end\":4980,\"start\":4976},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6427,\"start\":6423},{\"end\":6430,\"start\":6427},{\"end\":6433,\"start\":6430},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6445,\"start\":6441},{\"end\":6448,\"start\":6445},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6460,\"start\":6456},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6477,\"start\":6473},{\"end\":6487,\"start\":6479},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6500,\"start\":6496},{\"end\":6503,\"start\":6500},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6605,\"start\":6601},{\"end\":6638,\"start\":6634},{\"end\":6732,\"start\":6728},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6735,\"start\":6732},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6740,\"start\":6736},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6973,\"start\":6969},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10558,\"start\":10554},{\"end\":10560,\"start\":10558},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10772,\"start\":10768},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12862,\"start\":12858},{\"end\":12865,\"start\":12862},{\"end\":12868,\"start\":12865},{\"end\":14556,\"start\":14550},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14851,\"start\":14850},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17372,\"start\":17368},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17689,\"start\":17685},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19120,\"start\":19116},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19806,\"start\":19802},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19826,\"start\":19822},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20676,\"start\":20672},{\"end\":21522,\"start\":21511},{\"end\":22476,\"start\":22472},{\"end\":22485,\"start\":22481},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23470,\"start\":23466},{\"end\":24076,\"start\":24072},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24670,\"start\":24666},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24900,\"start\":24896},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26364,\"start\":26360},{\"end\":26574,\"start\":26570},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26577,\"start\":26574},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26896,\"start\":26892},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29685,\"start\":29681},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32752,\"start\":32748},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32778,\"start\":32774},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32800,\"start\":32796},{\"end\":32860,\"start\":32856},{\"end\":36759,\"start\":36755},{\"end\":37201,\"start\":37197},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38229,\"start\":38226},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38338,\"start\":38334},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38358,\"start\":38354},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38560,\"start\":38556},{\"end\":38563,\"start\":38560},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38658,\"start\":38654},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38785,\"start\":38781},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39394,\"start\":39391},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39420,\"start\":39417},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39580,\"start\":39576},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39707,\"start\":39704},{\"end\":39709,\"start\":39707},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39711,\"start\":39709},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":50390,\"start\":50386},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":51745,\"start\":51742}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50662,\"start\":50334},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50722,\"start\":50663},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50759,\"start\":50723},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50804,\"start\":50760},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50864,\"start\":50805},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50944,\"start\":50865},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51913,\"start\":50945},{\"attributes\":{\"id\":\"fig_7\"},\"end\":52020,\"start\":51914},{\"attributes\":{\"id\":\"fig_8\"},\"end\":52069,\"start\":52021},{\"attributes\":{\"id\":\"fig_9\"},\"end\":52227,\"start\":52070},{\"attributes\":{\"id\":\"fig_10\"},\"end\":59400,\"start\":52228},{\"attributes\":{\"id\":\"fig_11\"},\"end\":59457,\"start\":59401},{\"attributes\":{\"id\":\"fig_12\"},\"end\":60204,\"start\":59458},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":61217,\"start\":60205},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61754,\"start\":61218},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":62535,\"start\":61755},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":62958,\"start\":62536},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63118,\"start\":62959},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":63300,\"start\":63119},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":63342,\"start\":63301},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":63873,\"start\":63343}]", "paragraph": "[{\"end\":2568,\"start\":1859},{\"end\":3434,\"start\":2570},{\"end\":4059,\"start\":3436},{\"end\":5280,\"start\":4061},{\"end\":5738,\"start\":5282},{\"end\":6228,\"start\":5740},{\"end\":7276,\"start\":6245},{\"end\":9703,\"start\":7325},{\"end\":10111,\"start\":9743},{\"end\":10658,\"start\":10142},{\"end\":11789,\"start\":10660},{\"end\":11884,\"start\":11816},{\"end\":12611,\"start\":11913},{\"end\":14171,\"start\":12689},{\"end\":14852,\"start\":14323},{\"end\":15184,\"start\":14944},{\"end\":15429,\"start\":15186},{\"end\":15620,\"start\":15459},{\"end\":16765,\"start\":15648},{\"end\":17092,\"start\":16851},{\"end\":18495,\"start\":17114},{\"end\":18638,\"start\":18529},{\"end\":19649,\"start\":18743},{\"end\":21399,\"start\":19686},{\"end\":21875,\"start\":21461},{\"end\":27297,\"start\":21960},{\"end\":28180,\"start\":27299},{\"end\":28258,\"start\":28182},{\"end\":30359,\"start\":28277},{\"end\":30581,\"start\":30361},{\"end\":32358,\"start\":30608},{\"end\":33713,\"start\":32381},{\"end\":35250,\"start\":33728},{\"end\":35568,\"start\":35282},{\"end\":35778,\"start\":35618},{\"end\":35995,\"start\":35796},{\"end\":36401,\"start\":36036},{\"end\":36783,\"start\":36403},{\"end\":37270,\"start\":36823},{\"end\":37575,\"start\":37272},{\"end\":37726,\"start\":37650},{\"end\":38413,\"start\":37757},{\"end\":38897,\"start\":38415},{\"end\":39421,\"start\":38899},{\"end\":40633,\"start\":39423},{\"end\":41581,\"start\":40635},{\"end\":42945,\"start\":41609},{\"end\":45024,\"start\":42987},{\"end\":45911,\"start\":45059},{\"end\":46459,\"start\":45913},{\"end\":47082,\"start\":46496},{\"end\":48944,\"start\":47084},{\"end\":49496,\"start\":48946},{\"end\":50333,\"start\":49498}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10141,\"start\":10112},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11815,\"start\":11790},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11912,\"start\":11885},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12656,\"start\":12612},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14322,\"start\":14172},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14943,\"start\":14853},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15458,\"start\":15430},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15647,\"start\":15621},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16850,\"start\":16766},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18528,\"start\":18496},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18742,\"start\":18639},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21460,\"start\":21400},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21944,\"start\":21876},{\"attributes\":{\"id\":\"formula_13\"},\"end\":35617,\"start\":35569},{\"attributes\":{\"id\":\"formula_14\"},\"end\":36035,\"start\":35996},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36822,\"start\":36784},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37649,\"start\":37576}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23566,\"start\":23559},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24462,\"start\":24455},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28600,\"start\":28593},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31507,\"start\":31500},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31953,\"start\":31946},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":40666,\"start\":40658},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":40680,\"start\":40671},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":41206,\"start\":41197},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":42218,\"start\":42210}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1857,\"start\":1845},{\"attributes\":{\"n\":\"2\"},\"end\":6243,\"start\":6231},{\"attributes\":{\"n\":\"3\"},\"end\":7285,\"start\":7279},{\"attributes\":{\"n\":\"3.1\"},\"end\":7323,\"start\":7288},{\"attributes\":{\"n\":\"3.2\"},\"end\":9741,\"start\":9706},{\"attributes\":{\"n\":\"3.3\"},\"end\":12687,\"start\":12658},{\"attributes\":{\"n\":\"3.4\"},\"end\":17112,\"start\":17095},{\"attributes\":{\"n\":\"4\"},\"end\":19663,\"start\":19652},{\"attributes\":{\"n\":\"4.1\"},\"end\":19684,\"start\":19666},{\"attributes\":{\"n\":\"4.2\"},\"end\":21958,\"start\":21946},{\"attributes\":{\"n\":\"4.3\"},\"end\":28275,\"start\":28261},{\"attributes\":{\"n\":\"4.4\"},\"end\":30606,\"start\":30584},{\"attributes\":{\"n\":\"4.5\"},\"end\":32379,\"start\":32361},{\"attributes\":{\"n\":\"5\"},\"end\":33726,\"start\":33716},{\"end\":35280,\"start\":35253},{\"end\":35794,\"start\":35781},{\"end\":37755,\"start\":37729},{\"end\":41607,\"start\":41584},{\"end\":42985,\"start\":42948},{\"end\":45057,\"start\":45027},{\"end\":46494,\"start\":46462},{\"end\":50343,\"start\":50335},{\"end\":50672,\"start\":50664},{\"end\":50732,\"start\":50724},{\"end\":50769,\"start\":50761},{\"end\":50814,\"start\":50806},{\"end\":50874,\"start\":50866},{\"end\":50963,\"start\":50946},{\"end\":51933,\"start\":51915},{\"end\":52031,\"start\":52022},{\"end\":52080,\"start\":52071},{\"end\":52233,\"start\":52229},{\"end\":59411,\"start\":59402},{\"end\":59463,\"start\":59459},{\"end\":61765,\"start\":61756},{\"end\":62546,\"start\":62537},{\"end\":62969,\"start\":62960},{\"end\":63129,\"start\":63120},{\"end\":63312,\"start\":63302},{\"end\":63354,\"start\":63344}]", "table": "[{\"end\":61217,\"start\":61140},{\"end\":61754,\"start\":61374},{\"end\":62535,\"start\":61851},{\"end\":62958,\"start\":62577},{\"end\":63118,\"start\":62987},{\"end\":63300,\"start\":63184},{\"end\":63873,\"start\":63730}]", "figure_caption": "[{\"end\":50662,\"start\":50345},{\"end\":50722,\"start\":50674},{\"end\":50759,\"start\":50734},{\"end\":50804,\"start\":50771},{\"end\":50864,\"start\":50816},{\"end\":50944,\"start\":50876},{\"end\":51913,\"start\":50967},{\"end\":52020,\"start\":51938},{\"end\":52069,\"start\":52034},{\"end\":52227,\"start\":52083},{\"end\":59400,\"start\":52236},{\"end\":59457,\"start\":59414},{\"end\":60204,\"start\":59464},{\"end\":61140,\"start\":60207},{\"end\":61374,\"start\":61220},{\"end\":61851,\"start\":61767},{\"end\":62577,\"start\":62548},{\"end\":62987,\"start\":62971},{\"end\":63184,\"start\":63131},{\"end\":63342,\"start\":63315},{\"end\":63730,\"start\":63357}]", "figure_ref": "[{\"end\":2818,\"start\":2812},{\"end\":5159,\"start\":5153},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7396,\"start\":7389},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7876,\"start\":7869},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7981,\"start\":7974},{\"end\":8767,\"start\":8761},{\"end\":10017,\"start\":10011},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11052,\"start\":11046},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12414,\"start\":12404},{\"end\":13038,\"start\":13030},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13967,\"start\":13960},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14365,\"start\":14355},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16104,\"start\":16097},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16538,\"start\":16531},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17242,\"start\":17235},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20229,\"start\":20223},{\"end\":21320,\"start\":21309},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22245,\"start\":22239},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24120,\"start\":24114},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24939,\"start\":24933},{\"end\":26384,\"start\":26378},{\"end\":27319,\"start\":27311},{\"end\":27794,\"start\":27786},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28030,\"start\":28023},{\"end\":29795,\"start\":29788},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30341,\"start\":30334},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30988,\"start\":30981},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":31038,\"start\":31030},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":31149,\"start\":31141},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32869,\"start\":32862},{\"end\":34517,\"start\":34515},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34857,\"start\":34850},{\"end\":35994,\"start\":35983},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37574,\"start\":37563},{\"end\":43397,\"start\":43390},{\"end\":43756,\"start\":43737},{\"end\":43927,\"start\":43916},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44613,\"start\":44607}]", "bib_author_first_name": "[{\"end\":65128,\"start\":65119},{\"end\":65209,\"start\":65199},{\"end\":65283,\"start\":65282},{\"end\":65285,\"start\":65284},{\"end\":65687,\"start\":65686},{\"end\":65695,\"start\":65694},{\"end\":65699,\"start\":65696},{\"end\":65710,\"start\":65709},{\"end\":65712,\"start\":65711},{\"end\":65721,\"start\":65720},{\"end\":65729,\"start\":65728},{\"end\":65971,\"start\":65970},{\"end\":65982,\"start\":65981},{\"end\":65994,\"start\":65993},{\"end\":66003,\"start\":66002},{\"end\":66227,\"start\":66226},{\"end\":66229,\"start\":66228},{\"end\":66238,\"start\":66237},{\"end\":66246,\"start\":66245},{\"end\":66253,\"start\":66252},{\"end\":66262,\"start\":66261},{\"end\":66498,\"start\":66497},{\"end\":66500,\"start\":66499},{\"end\":66508,\"start\":66507},{\"end\":66514,\"start\":66513},{\"end\":66525,\"start\":66524},{\"end\":66878,\"start\":66877},{\"end\":66885,\"start\":66884},{\"end\":66893,\"start\":66892},{\"end\":66901,\"start\":66900},{\"end\":66909,\"start\":66908},{\"end\":66917,\"start\":66916},{\"end\":66925,\"start\":66924},{\"end\":66927,\"start\":66926},{\"end\":66935,\"start\":66934},{\"end\":66942,\"start\":66941},{\"end\":67383,\"start\":67382},{\"end\":67390,\"start\":67389},{\"end\":67398,\"start\":67397},{\"end\":67405,\"start\":67404},{\"end\":67413,\"start\":67412},{\"end\":67421,\"start\":67420},{\"end\":67433,\"start\":67432},{\"end\":67435,\"start\":67434},{\"end\":67443,\"start\":67442},{\"end\":67449,\"start\":67448},{\"end\":67451,\"start\":67450},{\"end\":67978,\"start\":67977},{\"end\":67980,\"start\":67979},{\"end\":67988,\"start\":67987},{\"end\":68001,\"start\":68000},{\"end\":68011,\"start\":68010},{\"end\":68015,\"start\":68012},{\"end\":68446,\"start\":68445},{\"end\":68459,\"start\":68458},{\"end\":68466,\"start\":68465},{\"end\":68476,\"start\":68475},{\"end\":68901,\"start\":68900},{\"end\":68909,\"start\":68908},{\"end\":68915,\"start\":68914},{\"end\":68923,\"start\":68922},{\"end\":68933,\"start\":68932},{\"end\":68935,\"start\":68934},{\"end\":68942,\"start\":68941},{\"end\":69346,\"start\":69345},{\"end\":69348,\"start\":69347},{\"end\":69361,\"start\":69360},{\"end\":69375,\"start\":69374},{\"end\":69377,\"start\":69376},{\"end\":69686,\"start\":69685},{\"end\":69688,\"start\":69687},{\"end\":69697,\"start\":69696},{\"end\":69699,\"start\":69698},{\"end\":69708,\"start\":69707},{\"end\":70131,\"start\":70130},{\"end\":70141,\"start\":70140},{\"end\":70149,\"start\":70148},{\"end\":70605,\"start\":70604},{\"end\":70613,\"start\":70612},{\"end\":70621,\"start\":70620},{\"end\":70623,\"start\":70622},{\"end\":70629,\"start\":70628},{\"end\":71045,\"start\":71044},{\"end\":71051,\"start\":71050},{\"end\":71058,\"start\":71057},{\"end\":71065,\"start\":71064},{\"end\":71353,\"start\":71352},{\"end\":71361,\"start\":71360},{\"end\":71368,\"start\":71367},{\"end\":71376,\"start\":71375},{\"end\":71382,\"start\":71381},{\"end\":71395,\"start\":71394},{\"end\":71404,\"start\":71403},{\"end\":71406,\"start\":71405},{\"end\":71894,\"start\":71893},{\"end\":71902,\"start\":71901},{\"end\":71904,\"start\":71903},{\"end\":71912,\"start\":71911},{\"end\":71920,\"start\":71919},{\"end\":71933,\"start\":71932},{\"end\":71945,\"start\":71944},{\"end\":71947,\"start\":71946},{\"end\":72478,\"start\":72477},{\"end\":72488,\"start\":72487},{\"end\":72490,\"start\":72489},{\"end\":72499,\"start\":72498},{\"end\":72513,\"start\":72512},{\"end\":72524,\"start\":72523},{\"end\":72817,\"start\":72816},{\"end\":72827,\"start\":72826},{\"end\":72837,\"start\":72836},{\"end\":72839,\"start\":72838},{\"end\":72850,\"start\":72849},{\"end\":73196,\"start\":73195},{\"end\":73208,\"start\":73207},{\"end\":73216,\"start\":73215},{\"end\":73232,\"start\":73231},{\"end\":73242,\"start\":73241},{\"end\":73633,\"start\":73632},{\"end\":73641,\"start\":73640},{\"end\":73650,\"start\":73649},{\"end\":73830,\"start\":73829},{\"end\":73838,\"start\":73837},{\"end\":73844,\"start\":73843},{\"end\":73846,\"start\":73845},{\"end\":73853,\"start\":73852},{\"end\":73859,\"start\":73858},{\"end\":73866,\"start\":73865},{\"end\":74118,\"start\":74117},{\"end\":74126,\"start\":74125},{\"end\":74133,\"start\":74132},{\"end\":74139,\"start\":74138},{\"end\":74147,\"start\":74146},{\"end\":74154,\"start\":74153},{\"end\":74160,\"start\":74159},{\"end\":74167,\"start\":74166},{\"end\":74173,\"start\":74172},{\"end\":74496,\"start\":74495},{\"end\":74504,\"start\":74503},{\"end\":74512,\"start\":74511},{\"end\":74518,\"start\":74517},{\"end\":74526,\"start\":74525},{\"end\":74890,\"start\":74889},{\"end\":74898,\"start\":74897},{\"end\":74900,\"start\":74899},{\"end\":74907,\"start\":74906},{\"end\":74913,\"start\":74912},{\"end\":74919,\"start\":74918},{\"end\":74926,\"start\":74925},{\"end\":75217,\"start\":75216},{\"end\":75223,\"start\":75222},{\"end\":75232,\"start\":75231},{\"end\":75239,\"start\":75238},{\"end\":75246,\"start\":75245},{\"end\":75253,\"start\":75252},{\"end\":75261,\"start\":75260},{\"end\":75263,\"start\":75262},{\"end\":75271,\"start\":75270},{\"end\":75279,\"start\":75278},{\"end\":75562,\"start\":75561},{\"end\":75568,\"start\":75567},{\"end\":75577,\"start\":75576},{\"end\":75896,\"start\":75895},{\"end\":75908,\"start\":75907},{\"end\":75921,\"start\":75920},{\"end\":75934,\"start\":75933},{\"end\":76211,\"start\":76210},{\"end\":76218,\"start\":76217},{\"end\":76224,\"start\":76223},{\"end\":76475,\"start\":76474},{\"end\":76483,\"start\":76482},{\"end\":76485,\"start\":76484},{\"end\":76497,\"start\":76496},{\"end\":76505,\"start\":76504}]", "bib_author_last_name": "[{\"end\":65135,\"start\":65129},{\"end\":65216,\"start\":65210},{\"end\":65291,\"start\":65286},{\"end\":65296,\"start\":65293},{\"end\":65692,\"start\":65688},{\"end\":65707,\"start\":65700},{\"end\":65718,\"start\":65713},{\"end\":65726,\"start\":65722},{\"end\":65733,\"start\":65730},{\"end\":65979,\"start\":65972},{\"end\":65991,\"start\":65983},{\"end\":66000,\"start\":65995},{\"end\":66008,\"start\":66004},{\"end\":66235,\"start\":66230},{\"end\":66243,\"start\":66239},{\"end\":66250,\"start\":66247},{\"end\":66259,\"start\":66254},{\"end\":66269,\"start\":66263},{\"end\":66505,\"start\":66501},{\"end\":66511,\"start\":66509},{\"end\":66522,\"start\":66515},{\"end\":66529,\"start\":66526},{\"end\":66882,\"start\":66879},{\"end\":66890,\"start\":66886},{\"end\":66898,\"start\":66894},{\"end\":66906,\"start\":66902},{\"end\":66914,\"start\":66910},{\"end\":66922,\"start\":66918},{\"end\":66932,\"start\":66928},{\"end\":66939,\"start\":66936},{\"end\":66945,\"start\":66943},{\"end\":67387,\"start\":67384},{\"end\":67395,\"start\":67391},{\"end\":67402,\"start\":67399},{\"end\":67410,\"start\":67406},{\"end\":67418,\"start\":67414},{\"end\":67430,\"start\":67422},{\"end\":67440,\"start\":67436},{\"end\":67446,\"start\":67444},{\"end\":67455,\"start\":67452},{\"end\":67985,\"start\":67981},{\"end\":67998,\"start\":67989},{\"end\":68008,\"start\":68002},{\"end\":68020,\"start\":68016},{\"end\":68456,\"start\":68447},{\"end\":68463,\"start\":68460},{\"end\":68473,\"start\":68467},{\"end\":68485,\"start\":68477},{\"end\":68906,\"start\":68902},{\"end\":68912,\"start\":68910},{\"end\":68920,\"start\":68916},{\"end\":68930,\"start\":68924},{\"end\":68939,\"start\":68936},{\"end\":68947,\"start\":68943},{\"end\":69358,\"start\":69349},{\"end\":69372,\"start\":69362},{\"end\":69381,\"start\":69378},{\"end\":69694,\"start\":69689},{\"end\":69705,\"start\":69700},{\"end\":69715,\"start\":69709},{\"end\":70138,\"start\":70132},{\"end\":70146,\"start\":70142},{\"end\":70157,\"start\":70150},{\"end\":70610,\"start\":70606},{\"end\":70618,\"start\":70614},{\"end\":70626,\"start\":70624},{\"end\":70633,\"start\":70630},{\"end\":71048,\"start\":71046},{\"end\":71055,\"start\":71052},{\"end\":71062,\"start\":71059},{\"end\":71069,\"start\":71066},{\"end\":71358,\"start\":71354},{\"end\":71365,\"start\":71362},{\"end\":71373,\"start\":71369},{\"end\":71379,\"start\":71377},{\"end\":71392,\"start\":71383},{\"end\":71401,\"start\":71396},{\"end\":71411,\"start\":71407},{\"end\":71899,\"start\":71895},{\"end\":71909,\"start\":71905},{\"end\":71917,\"start\":71913},{\"end\":71930,\"start\":71921},{\"end\":71942,\"start\":71934},{\"end\":71958,\"start\":71948},{\"end\":72485,\"start\":72479},{\"end\":72496,\"start\":72491},{\"end\":72510,\"start\":72500},{\"end\":72521,\"start\":72514},{\"end\":72531,\"start\":72525},{\"end\":72824,\"start\":72818},{\"end\":72834,\"start\":72828},{\"end\":72847,\"start\":72840},{\"end\":72865,\"start\":72851},{\"end\":73205,\"start\":73197},{\"end\":73213,\"start\":73209},{\"end\":73229,\"start\":73217},{\"end\":73239,\"start\":73233},{\"end\":73250,\"start\":73243},{\"end\":73638,\"start\":73634},{\"end\":73647,\"start\":73642},{\"end\":73657,\"start\":73651},{\"end\":73835,\"start\":73831},{\"end\":73841,\"start\":73839},{\"end\":73850,\"start\":73847},{\"end\":73856,\"start\":73854},{\"end\":73863,\"start\":73860},{\"end\":73871,\"start\":73867},{\"end\":74123,\"start\":74119},{\"end\":74130,\"start\":74127},{\"end\":74136,\"start\":74134},{\"end\":74144,\"start\":74140},{\"end\":74151,\"start\":74148},{\"end\":74157,\"start\":74155},{\"end\":74164,\"start\":74161},{\"end\":74170,\"start\":74168},{\"end\":74178,\"start\":74174},{\"end\":74501,\"start\":74497},{\"end\":74509,\"start\":74505},{\"end\":74515,\"start\":74513},{\"end\":74523,\"start\":74519},{\"end\":74530,\"start\":74527},{\"end\":74895,\"start\":74891},{\"end\":74904,\"start\":74901},{\"end\":74910,\"start\":74908},{\"end\":74916,\"start\":74914},{\"end\":74923,\"start\":74920},{\"end\":74931,\"start\":74927},{\"end\":75220,\"start\":75218},{\"end\":75229,\"start\":75224},{\"end\":75236,\"start\":75233},{\"end\":75243,\"start\":75240},{\"end\":75250,\"start\":75247},{\"end\":75258,\"start\":75254},{\"end\":75268,\"start\":75264},{\"end\":75276,\"start\":75272},{\"end\":75283,\"start\":75280},{\"end\":75565,\"start\":75563},{\"end\":75574,\"start\":75569},{\"end\":75580,\"start\":75578},{\"end\":75905,\"start\":75897},{\"end\":75918,\"start\":75909},{\"end\":75931,\"start\":75922},{\"end\":75940,\"start\":75935},{\"end\":76215,\"start\":76212},{\"end\":76221,\"start\":76219},{\"end\":76228,\"start\":76225},{\"end\":76480,\"start\":76476},{\"end\":76494,\"start\":76486},{\"end\":76502,\"start\":76498},{\"end\":76513,\"start\":76506}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":65115,\"start\":64912},{\"attributes\":{\"id\":\"b1\"},\"end\":65195,\"start\":65117},{\"attributes\":{\"id\":\"b2\"},\"end\":65278,\"start\":65197},{\"attributes\":{\"id\":\"b3\"},\"end\":65347,\"start\":65280},{\"attributes\":{\"id\":\"b4\"},\"end\":65521,\"start\":65349},{\"attributes\":{\"id\":\"b5\"},\"end\":65620,\"start\":65523},{\"attributes\":{\"doi\":\"arXiv:2009.00743\",\"id\":\"b6\"},\"end\":65926,\"start\":65622},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2645819},\"end\":66224,\"start\":65928},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b8\"},\"end\":66432,\"start\":66226},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6398414},\"end\":66737,\"start\":66434},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":235457977},\"end\":67305,\"start\":66739},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":196831979},\"end\":67893,\"start\":67307},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4891043},\"end\":68363,\"start\":67895},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":14814788},\"end\":68823,\"start\":68365},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":213175526},\"end\":69343,\"start\":68825},{\"attributes\":{\"doi\":\"arXiv:1608.00853\",\"id\":\"b15\"},\"end\":69625,\"start\":69345},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206593710},\"end\":70057,\"start\":69627},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6724907},\"end\":70400,\"start\":70059},{\"attributes\":{\"id\":\"b18\"},\"end\":70545,\"start\":70402},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":210170377},\"end\":70936,\"start\":70547},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":49322856},\"end\":71350,\"start\":70938},{\"attributes\":{\"id\":\"b21\"},\"end\":71784,\"start\":71352},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":56177594},\"end\":72404,\"start\":71786},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":233444160},\"end\":72775,\"start\":72406},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202676783},\"end\":73094,\"start\":72777},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":227151533},\"end\":73563,\"start\":73096},{\"attributes\":{\"doi\":\"arXiv:2006.08602\",\"id\":\"b26\"},\"end\":73827,\"start\":73565},{\"attributes\":{\"doi\":\"arXiv:1801.02610\",\"id\":\"b27\"},\"end\":74115,\"start\":73829},{\"attributes\":{\"doi\":\"arXiv:1907.09470\",\"id\":\"b28\"},\"end\":74441,\"start\":74117},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":195767667},\"end\":74887,\"start\":74443},{\"attributes\":{\"doi\":\"arXiv:1801.02612\",\"id\":\"b30\"},\"end\":75147,\"start\":74889},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":208310168},\"end\":75559,\"start\":75149},{\"attributes\":{\"doi\":\"arXiv:1704.01155\",\"id\":\"b32\"},\"end\":75827,\"start\":75561},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":222177159},\"end\":76112,\"start\":75829},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":27264520},\"end\":76392,\"start\":76114},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":211677466},\"end\":76900,\"start\":76394}]", "bib_title": "[{\"end\":65968,\"start\":65928},{\"end\":66495,\"start\":66434},{\"end\":66875,\"start\":66739},{\"end\":67380,\"start\":67307},{\"end\":67975,\"start\":67895},{\"end\":68443,\"start\":68365},{\"end\":68898,\"start\":68825},{\"end\":69683,\"start\":69627},{\"end\":70128,\"start\":70059},{\"end\":70602,\"start\":70547},{\"end\":71042,\"start\":70938},{\"end\":71891,\"start\":71786},{\"end\":72475,\"start\":72406},{\"end\":72814,\"start\":72777},{\"end\":73193,\"start\":73096},{\"end\":74493,\"start\":74443},{\"end\":75214,\"start\":75149},{\"end\":75893,\"start\":75829},{\"end\":76208,\"start\":76114},{\"end\":76472,\"start\":76394}]", "bib_author": "[{\"end\":65137,\"start\":65119},{\"end\":65218,\"start\":65199},{\"end\":65293,\"start\":65282},{\"end\":65298,\"start\":65293},{\"end\":65694,\"start\":65686},{\"end\":65709,\"start\":65694},{\"end\":65720,\"start\":65709},{\"end\":65728,\"start\":65720},{\"end\":65735,\"start\":65728},{\"end\":65981,\"start\":65970},{\"end\":65993,\"start\":65981},{\"end\":66002,\"start\":65993},{\"end\":66010,\"start\":66002},{\"end\":66237,\"start\":66226},{\"end\":66245,\"start\":66237},{\"end\":66252,\"start\":66245},{\"end\":66261,\"start\":66252},{\"end\":66271,\"start\":66261},{\"end\":66507,\"start\":66497},{\"end\":66513,\"start\":66507},{\"end\":66524,\"start\":66513},{\"end\":66531,\"start\":66524},{\"end\":66884,\"start\":66877},{\"end\":66892,\"start\":66884},{\"end\":66900,\"start\":66892},{\"end\":66908,\"start\":66900},{\"end\":66916,\"start\":66908},{\"end\":66924,\"start\":66916},{\"end\":66934,\"start\":66924},{\"end\":66941,\"start\":66934},{\"end\":66947,\"start\":66941},{\"end\":67389,\"start\":67382},{\"end\":67397,\"start\":67389},{\"end\":67404,\"start\":67397},{\"end\":67412,\"start\":67404},{\"end\":67420,\"start\":67412},{\"end\":67432,\"start\":67420},{\"end\":67442,\"start\":67432},{\"end\":67448,\"start\":67442},{\"end\":67457,\"start\":67448},{\"end\":67987,\"start\":67977},{\"end\":68000,\"start\":67987},{\"end\":68010,\"start\":68000},{\"end\":68022,\"start\":68010},{\"end\":68458,\"start\":68445},{\"end\":68465,\"start\":68458},{\"end\":68475,\"start\":68465},{\"end\":68487,\"start\":68475},{\"end\":68908,\"start\":68900},{\"end\":68914,\"start\":68908},{\"end\":68922,\"start\":68914},{\"end\":68932,\"start\":68922},{\"end\":68941,\"start\":68932},{\"end\":68949,\"start\":68941},{\"end\":69360,\"start\":69345},{\"end\":69374,\"start\":69360},{\"end\":69383,\"start\":69374},{\"end\":69696,\"start\":69685},{\"end\":69707,\"start\":69696},{\"end\":69717,\"start\":69707},{\"end\":70140,\"start\":70130},{\"end\":70148,\"start\":70140},{\"end\":70159,\"start\":70148},{\"end\":70612,\"start\":70604},{\"end\":70620,\"start\":70612},{\"end\":70628,\"start\":70620},{\"end\":70635,\"start\":70628},{\"end\":71050,\"start\":71044},{\"end\":71057,\"start\":71050},{\"end\":71064,\"start\":71057},{\"end\":71071,\"start\":71064},{\"end\":71360,\"start\":71352},{\"end\":71367,\"start\":71360},{\"end\":71375,\"start\":71367},{\"end\":71381,\"start\":71375},{\"end\":71394,\"start\":71381},{\"end\":71403,\"start\":71394},{\"end\":71413,\"start\":71403},{\"end\":71901,\"start\":71893},{\"end\":71911,\"start\":71901},{\"end\":71919,\"start\":71911},{\"end\":71932,\"start\":71919},{\"end\":71944,\"start\":71932},{\"end\":71960,\"start\":71944},{\"end\":72487,\"start\":72477},{\"end\":72498,\"start\":72487},{\"end\":72512,\"start\":72498},{\"end\":72523,\"start\":72512},{\"end\":72533,\"start\":72523},{\"end\":72826,\"start\":72816},{\"end\":72836,\"start\":72826},{\"end\":72849,\"start\":72836},{\"end\":72867,\"start\":72849},{\"end\":73207,\"start\":73195},{\"end\":73215,\"start\":73207},{\"end\":73231,\"start\":73215},{\"end\":73241,\"start\":73231},{\"end\":73252,\"start\":73241},{\"end\":73640,\"start\":73632},{\"end\":73649,\"start\":73640},{\"end\":73659,\"start\":73649},{\"end\":73837,\"start\":73829},{\"end\":73843,\"start\":73837},{\"end\":73852,\"start\":73843},{\"end\":73858,\"start\":73852},{\"end\":73865,\"start\":73858},{\"end\":73873,\"start\":73865},{\"end\":74125,\"start\":74117},{\"end\":74132,\"start\":74125},{\"end\":74138,\"start\":74132},{\"end\":74146,\"start\":74138},{\"end\":74153,\"start\":74146},{\"end\":74159,\"start\":74153},{\"end\":74166,\"start\":74159},{\"end\":74172,\"start\":74166},{\"end\":74180,\"start\":74172},{\"end\":74503,\"start\":74495},{\"end\":74511,\"start\":74503},{\"end\":74517,\"start\":74511},{\"end\":74525,\"start\":74517},{\"end\":74532,\"start\":74525},{\"end\":74897,\"start\":74889},{\"end\":74906,\"start\":74897},{\"end\":74912,\"start\":74906},{\"end\":74918,\"start\":74912},{\"end\":74925,\"start\":74918},{\"end\":74933,\"start\":74925},{\"end\":75222,\"start\":75216},{\"end\":75231,\"start\":75222},{\"end\":75238,\"start\":75231},{\"end\":75245,\"start\":75238},{\"end\":75252,\"start\":75245},{\"end\":75260,\"start\":75252},{\"end\":75270,\"start\":75260},{\"end\":75278,\"start\":75270},{\"end\":75285,\"start\":75278},{\"end\":75567,\"start\":75561},{\"end\":75576,\"start\":75567},{\"end\":75582,\"start\":75576},{\"end\":75907,\"start\":75895},{\"end\":75920,\"start\":75907},{\"end\":75933,\"start\":75920},{\"end\":75942,\"start\":75933},{\"end\":76217,\"start\":76210},{\"end\":76223,\"start\":76217},{\"end\":76230,\"start\":76223},{\"end\":76482,\"start\":76474},{\"end\":76496,\"start\":76482},{\"end\":76504,\"start\":76496},{\"end\":76515,\"start\":76504}]", "bib_venue": "[{\"end\":64986,\"start\":64912},{\"end\":65415,\"start\":65349},{\"end\":65548,\"start\":65523},{\"end\":65684,\"start\":65622},{\"end\":66054,\"start\":66010},{\"end\":66304,\"start\":66287},{\"end\":66567,\"start\":66531},{\"end\":66995,\"start\":66947},{\"end\":67542,\"start\":67457},{\"end\":68104,\"start\":68022},{\"end\":68571,\"start\":68487},{\"end\":69030,\"start\":68949},{\"end\":69461,\"start\":69399},{\"end\":69794,\"start\":69717},{\"end\":70219,\"start\":70159},{\"end\":70459,\"start\":70402},{\"end\":70696,\"start\":70635},{\"end\":71126,\"start\":71071},{\"end\":71545,\"start\":71413},{\"end\":72041,\"start\":71960},{\"end\":72579,\"start\":72533},{\"end\":72921,\"start\":72867},{\"end\":73317,\"start\":73252},{\"end\":73630,\"start\":73565},{\"end\":73946,\"start\":73889},{\"end\":74249,\"start\":74196},{\"end\":74613,\"start\":74532},{\"end\":74991,\"start\":74949},{\"end\":75323,\"start\":75285},{\"end\":75671,\"start\":75598},{\"end\":75953,\"start\":75942},{\"end\":76237,\"start\":76230},{\"end\":76596,\"start\":76515},{\"end\":67614,\"start\":67544},{\"end\":69098,\"start\":69032},{\"end\":69858,\"start\":69796},{\"end\":70744,\"start\":70698},{\"end\":72109,\"start\":72043},{\"end\":74681,\"start\":74615},{\"end\":76664,\"start\":76598}]"}}}, "year": 2023, "month": 12, "day": 17}