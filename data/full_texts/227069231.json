{"id": 227069231, "updated": "2022-01-17 21:07:57.62", "metadata": {"title": "THRIFTY: Training with Hyperdimensional Computing across Flash Hierarchy", "authors": "[{\"middle\":[],\"last\":\"Gupta\",\"first\":\"Saransh\"},{\"middle\":[],\"last\":\"Morris\",\"first\":\"Justin\"},{\"middle\":[],\"last\":\"Imani\",\"first\":\"Mohsen\"},{\"middle\":[],\"last\":\"Ramkumar\",\"first\":\"Ranganathan\"},{\"middle\":[],\"last\":\"Yu\",\"first\":\"Jeffrey\"},{\"middle\":[],\"last\":\"Tiwari\",\"first\":\"Aniket\"},{\"middle\":[],\"last\":\"Aksanli\",\"first\":\"Baris\"},{\"middle\":[\"\u0160imuni\u0107\"],\"last\":\"Rosing\",\"first\":\"Tajana\"}]", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "journal": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Hyperdimensional computing (HDC) is a brain-inspired computing paradigm that works with high-dimensional vectors, hypervectors, instead of numbers. HDC replaces several complex learning computations with bitwise and simpler arithmetic operations, resulting in a faster and more energy-efficient learning algorithm. However, it comes at the cost of an increased amount of data to process due to mapping the data into high-dimensional space. While some datasets may nearly fit in the memory, the resulting hypervectors more often than not can't be stored in memory, resulting in long data transfers from storage. In this paper, we propose THRIFTY, an in-storage computing (ISC) solution that performs HDC encoding and training across the flash hierarchy. To hide the latency of training and enable efficient computation, we introduce the concept of batching in HDC. It allows us to split HDC training into sub-components and process them independently. We also present, for the first time, on-chip acceleration for HDC which uses simple low-power digital circuits to implement HDC encoding in Flash planes. This enables us to explore high internal parallelism provided by the flash hierarchy and encode multiple data points in parallel with negligible latency overhead. THRIFTY also implements a single top-level FPGA accelerator, which further processes the data obtained from the chips. We exploit the state-of-the-art INSIDER ISC infrastructure to implement the top-level accelerator and provide software support to THRIFTY. THRIFTY runs HDC training completely in storage while almost entirely hiding the latency of computation. Our evaluation over five popular classification datasets shows that THRIFTY is on average 1612\u00d7 faster than a CPU-server and 14.4\u00d7 faster than the state-of-the-art ISC solution, INSIDER for HDC encoding and training.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3112282291", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccad/GuptaMIRYTAR20", "doi": "10.1145/3400302.3415723"}}, "content": {"source": {"pdf_hash": "77eec3218445af1f3b57ce142ab90b7292a102bd", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3400302.3415723", "status": "BRONZE"}}, "grobid": {"id": "6f529cab00ea30517f55d022667c7929bae6ec2b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/77eec3218445af1f3b57ce142ab90b7292a102bd.txt", "contents": "\nTHRIFTY: Training with Hyperdimensional Computing across Flash Hierarchy\n\n\nSaransh Gupta sgupta@ucsd.edu \nJustin Morris justinmorris@ucsd.edu \nRanganathan Ramkumar rramkuma@ucsd.edu \nJerey Yu \nAniket Tiwari artiwari@ucsd.edu \nBaris Aksanli baksanli@sdsu.edu \nSaransh Gupta \nJustin Morris \nMohsen Imani m.imani@uci.edu \nRanganathan Ramkumar \nJef-Frey Yu \nAniket Tiwari \nBaris Aksanli \nTajana\u0161imuni \nRosing \n\nUniversity of California\nSan Diego\n\n\nUniversity of California\nSan Diego and San Diego State University Mohsen Imani\n\n\nUniversity of California\nUniversity of California\nIrvine\n\n\nUniversity of California\nSan Diego\n\n\nUniversity of California\nSan Diego\n\n\nUniversity of California\nSan Diego\n\n\nSan Diego State University Tajana\u0160imuni \u0107Rosing\nUniversity of California\nSan Diego\n\nTHRIFTY: Training with Hyperdimensional Computing across Flash Hierarchy\n10.1145/3400302.3415723For all other uses, contact the owner/author(s). ICCAD '20, November 2-5, 2020, Virtual Event, USA ACM Reference Format: THRIFTY: Training with Hyperdimensional Computing across Flash Hi-erarchy. In IEEE/ACM International Conference on Computer-Aided Design (ICCAD '20), November 2-5, 2020, Virtual Event, USA. ACM, New York, NY, USA, 9 pages. https://CCS CONCEPTS \u2022 Information systems ! Flash memory\u2022 Hardware ! Biology- related information processingEmerging architectures KEYWORDS hyperdimensional computing, in-storage computing, classication\nHyperdimensional computing (HDC) is a brain-inspired computing paradigm that works with high-dimensional vectors, hypervectors, instead of numbers. HDC replaces several complex learning computations with bitwise and simpler arithmetic operations, resulting in a faster and more energy-ecient learning algorithm. However, it comes at the cost of an increased amount of data to process due to mapping the data into high-dimensional space. While some datasets may nearly t in the memory, the resulting hypervectors more often than not can't be stored in memory, resulting in long data transfers from storage. In this paper, we propose THRIFTY, an in-storage computing (ISC) solution that performs HDC encoding and training across the ash hierarchy. To hide the latency of training and enable ecient computation, we introduce the concept of batching in HDC. It allows us to split HDC training into sub-components and process them independently. We also present, for the rst time, on-chip acceleration for HDC which uses simple low-power digital circuits to implement HDC encoding in Flash planes. This enables us to explore high internal parallelism provided by the ash hierarchy and encode multiple data points in parallel with negligible latency overhead. THRIFTY also implements a single top-level FPGA accelerator, which further processes the data obtained from the chips. We exploit the state-of-the-art INSIDER ISC infrastructure to implement the top-level accelerator and provide software support to THRIFTY. THRIFTY runs HDC training completely in storage while almost entirely hiding the latency of computation. Our evaluation over ve popular classication datasets shows that THRIFTY is on average 1612\u21e5 faster than a CPU-server and 14.4\u21e5 faster than the state-of-the-art ISC solution, INSIDER for HDC encoding and training.Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored.\n\nINTRODUCTION\n\nBrain-inspired Hyperdimensional Computing (HDC) is a computation paradigm which represents data in terms of extremely large vectors, called hypervectors. These hypervectors may have 10s of thousands of dimensions and present data in the form of a pattern of signals instead of numbers. By representing data in high-dimension space, HDC reduces the complexity of operations required to process data. HDC builds upon a well-dened set of operations with random HDC vectors, making HDC extremely robust in the presence of failures, and oers a complete computational paradigm that is easily applied to learning problems [1]. Prior work has shown the suitability of HDC for various applications like activity recognition, face detection, language recognition, image classication, etc [2][3][4].\n\nWhile HDC provides improvements in performance and energy consumption over conventional machine learning algorithms, it still involves fetching each and every data from memory/disk and processing it on CPUs/GPUs. Today extremely large datasets are stored on disks. In addition, the humongous amount of data generated while running HDC cannot always be t into the memory, eventually killing the process. Recent work has introduced computing capabilities to solid-state disks (SSDs) to process data in storage [5][6][7][8]. This not only reduces the computation load from the processing cores but also processes raw data where it is stored. However, the state-of-the-art in-storage computing (ISC) solutions either utilize a single big accelerator for a SSD or limit the gains by using complex power-hungry accelerators down the storage hierarchy [9]. Such architectures are not able to fully leverage its hierarchical design.\n\nIn this paper, we propose an HDC system that spans multiple levels of the storage hierarchy. We exploit the internal bandwidth and hierarchical structure of SSDs to perform HDC operations over multiple data samples in parallel. Our main contributions are as follows:\n\n\u2022 We present a novel ISC architecture for HDC which performs HDC encoding and training completely in storage. It enables computing at multiple levels of SSD hierarchy, allowing for highly-parallel ISC. Our hierarchical design provides parallelism and hides a signicant part of the performance cost of ISC in the storage read/write operations. \u2022 We introduce the concept of batching in HDC and utilize it to make our ISC implementation more ecient. During training, we batch together multiple data samples encoded in the HDC domain in storage. This allows us to partially process data without accessing all encoded hypervectors. Batching enables us to have a minimal aggregation hardware requirement. Batching also reduces the amount of data sent out of storage. \u2022 THRIFTY utilizes die-level accelerators to convert raw data into hypervectors locally in all the ash planes in parallel. Unlike previous work, our accelerator is simpler and hides its computation latency by the long read times of raw data from ash arrays. \u2022 We present a top-level SSD accelerator, which aggregates the data from dierent ash dies. This accelerator is implemented on an FPGA-based device controller. We present primitives to enable the FPGA to seamlessly work with the die-level accelerators.\n\nWe evaluate THRIFTY over ve popular classication datasets for HDC encoding and training. Our experimental results show that THRIFTY is on average 1612\u21e5 faster than a CPU-server and 14.4\u21e5 faster than the state-of-the-art ISC solution, INSIDER.\n\n\nRELATED WORK\n\nHyperdimensional Computing: Prior work applied the idea of hyperdimensional computing to a wide range of learning applications, including language recognition [10], speech recognition [11], gesture detection [12], human-brain interaction [13], and sensor fusion prediction [2]. Prior work also tried to design dierent hardware accelerators for HD computing. This include accelerating HD computing on existing FPGA, ASIC, and processing in-memory platforms [14][15][16]. However, these solutions do not scale well with the number of classes and dimensions, primarily due to the data movement issue. In contrast, our proposed THRIFTY accelerates the entire encoding and training phases of HD computing by fundamentally addressing data movement and memory requirement issues. In addition, THRIFTY scales with the size of data and the complexity of learning task.\n\nIn-Storage Computing: The major bottlenecks in the current storage systems include the slow ash array read latency and the SSD to host I/O latency [17]. To alleviate these issues prior work introduced ISC architectures [7,18]. These work exploit the embedded cores present in the SSD controller to implement ISC. Another set of work in [5,6,9] used ASIC accelerators in SSD for specic workloads. The work in [8] proposed a full-stack storage system to reduce the host-side I/O stack latency. However, all these works propose single-level computing in storage. THRIFTY on the other hand, is the rst work to push the computing all the way down to the ash die to extract maximum parallel. It also uses a top level accelerator to provide addition layer of computing. The combination provide a faster implementation while consuming minimal power.\n\n\nHYPERDIMENSIONAL COMPUTING\n\nBrain-inspired Hyperdimensional (HDC) computing has been proposed as the alternative computing method that processes the cognitive tasks in a more light-weight way [1,10]. HDC oers an ecient learning strategy without overcomplex computation steps such as back propagation in neural networks. HDC works by representing data in terms of extremely large vectors, called hypervectors, on the order of 10, 000 dimensions. HDC performs the learning task after mapping all training data into the high-dimensional space. The mapping procedure is often referred to as encoding. Ideally, the encoded data should preserve the distance of data points in the high-dimensional space. For example, if a data point is completely dierent from another one, the corresponding hypervectors should be orthogonal in the HDC space. There are multiple encoding methods proposed in literature [3,4]. These methods have shown excellent classication accuracy for dierent data types. In the following, we explain the details of HDC classication steps.\n\n\nEncoding\n\nLet us consider an encoding function that maps a feature vector F = {5 1 , 5 2 , . . . , 5 = }, with = features (5 8 2 N) to a hypervector H = {\u2318 1 , \u2318 2 , . . . , \u2318 \u21e1 } with \u21e1 dimensions (\u2318 8 2 {0, 1}). We rst generate a projection matrix PM with \u21e1 rows and each row is a vector with = dimensions randomly sampled from { 1, 1}. This matrix is generated once oine and is then be used to encode all of the data samples. We generate the resulting hypervector by calculating the matrix vector multiplication product of the projection matrix with the feature vector:\nH 0 = PM \u21e5 F(1)\nAfter this step, each element \u2318 8 of a hypervector H 0 has a nonbinary value. In HDC, binary (bipolar) hypervectors are often used for the computation eciency. We thus obtain the nal encoded hypervector by binarizing it with a sign function (H = B86=(H 0 )) where the sign function assigns all positive hypervector dimensions to '1' and zero/negative dimensions to '-1'. The encoded hypervector stores the information of each original data point with \u21e1 bits.\n\n\nTraining\n\nIn the training step, we combine all the encoded hypervectors of each class using element-wise addition. For example, in an activity recognition application, the training procedure adds all hypervectors which have the \"walking\" and \"sitting\" tags into two dierent hypervectors. Where 8 9 = h\u2318 \u21e1 , \u00b7 \u00b7 \u00b7 , \u2318 1 i is encoded for the 9 C\u2318 sample in 8 C\u2318 class, each class hypervector is trained as follows:\n\u21e0 8 = ' 9 8 9 = h2 8 \u21e1 , \u00b7 \u00b7 \u00b7 , 2 8 1 i (2)\n\nInference\n\nThe main computation of inference is the encoding and associative search. We perform the same encoding procedure to convert a test data point into a hypervector, called a query hypervector, & 2 { 1, 1} \u21e1 . Then, HDC computes the similarity of the query hypervector with all : class hypervectors, {\u21e0 1 , \u21e0 2 , \u00b7 \u00b7 \u00b7 , \u21e0 : }. We measure the similarity between a query and a 8 C\u2318 class hypervector using: X h&, \u21e0 8 i, where X denotes the similarity metric. The similarity metric most commonly used is Cosine Similarity as it provides the highest accuracy. However, other similarities metrics like dot product and hamming distance for binary class hypervectors are also used. After computing all similarities, each query is assigned to a class with the highest similarity.\n\n\nChallenges\n\nHDC is light-weight enough to run at acceptable speed on a CPU [19]. Utilizing a parallel architecture can signicantly speed up the execution time of HDC [16]. However, with the constantly increasing data sizes along with the explosion in data that occurs due to HDC encoding, running this algorithm on current systems is highly inecient. All of these platforms need to fetch the extremely large hypervectors from memory/disk in order to process them. They also require huge memory space to store HDC hypervectors and train on them. With the available parallelism across thousands of dimensions and simple operations needed, in-storage computing (ISC) is a promising solution to accelerate HDC encoding and training. General-purpose ISC solutions partially address the data transfer bottleneck but still are not able to fully exploit the huge internal SSD bandwidth [8]. The state-of-the-art application specic ISC [9] try to exploit the internal SSD bandwidth but provide only one-level of computing, which fails to accelerate applications which either (i) have a computing logic that is too complex to implement using the small accelerator or (ii) require post-processing computation steps. THRIFTY aims to overcome these issues by breaking complex HDC training algorithms into simpler, both data-size and computationwise, parallelizable tasks. Then, THRIFTY utilizes two levels of computation within the SSD, one at the chip-level and other at the SSD level, to eciently implement those tasks.\n\n\nTHRIFTY DESIGN\n\nTHRIFTY is an ISC design that performs HDC encoding and training completely in storage. Figure 1 shows an overview of THRIFTY SSD architecture. A ash die consists of multiple ash planes, each of which generates a page during a read cycle. THRIFTY inserts a simple low-power accelerator, die-level accelerator (green on the right in Figure 1), in each plane to encode every read page into a hypervector. These hypervectors are then sent to a SSD-level FPGA, which accumulates these hypervectors in batches in the top-level accelerator (green on bottom left in Figure 1). THRIFTY uses a scratchpad (green on top left in Figure 1) in the controller to store the projection matrix, which it receives as an application parameter from the host. Batching ensures that data generated by each SSD-wide read operation is used in training as soon as it is  Figure 1: THRIFTY SSD Overview. The components added by THRIFTY are shown in green.\n\navailable, without waiting for the remaining data. The top-level accelerator is a FPGA which uses INSIDER acceleration cluster [8] to implement HDC accumulation and other operations. We utilize the INSIDER's software stack to connect THRIFTY to the rest of the system. We modify the SSD drivers and INSIDER virtual les mechanism to enable computing in ash chips and make it visible to the FPGA.\n\n\nBatched HDC Training in THRIFTY\n\nThe size of raw data (number of data points) combined with the size of each hypervector (size of each encoded data point) makes it unrealistic to store all the encoded hypervectors and then perform HDC training over them. Hence, we employ batching to perform partial training with the hypervectors available at any given moment. As mentioned in Section 3, the initial HDC training algorithm to create a class hypervector (2) is to add up all of the encoded samples belonging to a given class. This summation can be spit up into batches of partial sums and maintain the same result. For example, say there are B samples for each class, the total sum can be split up into : partial sums or batches and the batch size dened as 1 = B/:, as shown in Equation 3.\n\u21e0 8 = 1 ' 9=1 8 9 + 21 ' 9=1+1 +... + B ' 9=( (B 1)1)+1 8 9(3)\nBatching allows THRIFTY to process a subset of encoded hypervectors together. THRIFTY chip-level accelerators encode raw data into hypervectors and send them to the top-level SSD FPGA accelerator for further processing. All ash chips operate in parallel to encode some of their data, send the hypervectors to FPGA, and operate on the next set. Each of these hypervectors belongs to a specic 2;0BB. For an application with \u21e0 classes, we allocate enough memory in the top-level accelerator to store \u21e0 model hypervectors, each assigned to a class. We batch all incoming hypervectors from ash that belong to the same class together and bundle the result with the corresponding model hypervector. This is continued until all required data has been encoded and used to train model hypervectors. In the end, the top-level model hypervectors represent a fully trained model of the data. Batching provides us with two benets. First, it minimizes the memory requirement during training. Second, it reduces its eective latency by combining hypervectors as soon as they are generated. This hides a major part of training latency with the time taken to read data from ash.\n\nWhat if the size of model hypervectors is too large to store at the top-level FPGA accelerator? Some application may need too many dimensions or have too many classes to store all model hypervectors at the FPGA, which at best may have few MBs of blocked RAMs (BRAMs). In such a case, even with balanced data, it will not be possible to train the model completely in storage. However, THRIFTY can still perform training in batches and reduce the amount of data sent to the host for processing. Now, instead of allocating FPGA BRAMs for all model hypervectors, it is dynamically allocated according to the encoded input hypervectors available at a time. If an input hypervector does not belong to one of the present models, a model hypervector is sent out to the CPU host and an empty model hypervector corresponding to the class associated with incoming hypervector is allocated instead. The implementation details are presented in Section 4.3. The host is then responsible for combining various batched training hypervectors together.\n\nIn this operating mode, THRIFTY still reduces the amount of data movement compared to sending the raw low dimensional data.\n\nHere, we dene = as the number of features or dimensionality of the original data, \u21e1 as the dimensionality of the encoded hypervectors, and 1 as the batch size. When =1 > \u21e1, the total data movement of the resulting batched hypervectors is less than the amount of original data sent in low-dimensional space when the batched hypervector uses the same bitwidth as the original data. However, we can utilize lower bitwidth representations as we encode the data into a hypervector whose elements are { 1, 1} and then bundle the hypervectors with element-wise addition. Therefore, the range of data in any given dimension can be dened by the normal distribution with a mean of 0 and standard deviation of p 1. We can represent each dimension of the batched hypervector with (log 2 4 p 1) + 1 bits while maintaining an accurate representation. We multiply by 4 to capture 4 standard deviations away and add one to account for the sign bit. In this case, assuming the original data is represented with 32 bits, THRIFTY sends less data than the data movement required to send the original data in low-dimensional space when 32=1 > \u21e1 ((log 2 4 p 1) + 1)/32\n\n\nEncoding Near Data via Flash Hierarchy\n\nThe modern SSD architecture is hierarchical in nature. An SSD has multiple channels. Each channel is shared by 4-8 ash chips as shown in Figure 1. The ash chip may consist of several ash dies which are further divided into ash planes, each plane consisting of a group of blocks, each of which store multiple pages. Each plane has a page buer to write the data to. Operations in SSD happen in page granularity where the size of pages usually ranges from 2KB-16KB [20]. To fully utilize the ash hierarchy, we introduce accelerators for each ash plane as shown in Figure 1. The aim of this added computing primitive is to process the data where it has no conict or competition for resources.  Figure 2: THRIFTY die accelerator from the ash array and another being a row-vector of the projection matrix. This involves element-wise multiplication of the two vectors and adding together all the elements in the product. Since the weights in the projection matrix 2 {1, 1}, we reduce the bits required to store the weights by mapping them such that 1 ! 1 and ( 1) ! 0. We use 2's complement to break the multiplication into an inversion using XNOR gates and then adding the total number of inverted inputs to the accumulated sum of XNOR outputs. The accelerator is shown in Figure 2. It consists of an array of 32K XNOR gates followed by a 1K input tree adder (labeled CSA in Figure  2). The tree adder is a pruned version of the Wallace carry-save tree, where the operand size throughout the tree is xed to 4B. It reduces 1024 inputs to 2, which is followed by a carry look ahead addition (labeled CLA in Figure 2). This gives us the dot product of the two vectors. It is the value of one dimension of the encoded hypervector. The accelerator is iteratively run \u21e1 times to generate \u21e1 dimensions. Depending upon the power budget, THRIFTY may employ multiple parallel instances of this accelerator to reduce the total number of iterations. Since \u21e1 is generally large, the generated \u21e1-dimensional vector is multi-page output. THRIFTY writes the output of the accelerator to the page buer of the plane, which serves as the response to the original SSD read request.\n\n\nStoring Input\n\nData. The accelerator above assumed the size of the feature vectors to be exactly the same as that of a page. However, this is rarely the case. State-of-the-art ISC designs use page-aligned feature vectors, which may lead to poor storage utilization if the feature vector size is too small or just larger than the page size. For example, in a page-aligned feature vector setting, a 4KB page may t only one 512B feature instead of eight. Also, a 5KB feature vector may occupy two complete pages. To alleviate the issue, we propose a cross-plane storing scheme, which considers all the planes in a chip when storing data, with the goal of increasing the traditional ISC storage utilization while being accelerator-friendly. We rst describe the case when the size of the feature vector is smaller than the page size. The scheme, shown in Figure 3 on the left, divides an =-sized feature vector into = ? equal segments such that the most ecient storage is given when: A drawback of this scheme is that individual reads for small feature vectors may require reading two pages instead of one. However, our main purpose is to obtain trained vectors and not raw feature vector values. Moreover, since a feature split across two planes shares the same block and page number, they are both read at the same time.\n\n\nAccelerator with New Data Storing Scheme.\n\nWhile the new data storing scheme improves the page utilization, it does not suit well the chip-level accelerator. As proposed before, our accelerator is a dot product engine. It processes an entire page from the ash array to generate values of dierent dimensions of the corresponding hypervector. In the new data storage scheme, this would result in an encoded hypervector consisting of multiple and also partial feature vectors. An easy x would be to just process one feature vector at a time by setting the remaining inputs of the accelerator to 0. However, this would increase the total latency of the accelerator. The situation is worse if the size of feature vectors is very small. We address this problem by extending the concept of batching in THRIFTY.\n\nAs detailed in Section 4.1, a set of encoded hypervectors can be added dimension-wise without interfering with HDC training process as long as they belong to the same nal trained hypervector, for example the same class model. An encoded dimension (3 8 ) of a feature vector ( + ) is obtained by a dot product between the feature values ( + 8 ) and the corresponding row of the projection matrix (%\"), i.e.,\n3 8 = + 0 \u21e5 %\" 8,0 + + 1 \u21e5 %\" 8,1 + ... + = 1 \u21e5 %\" 8,(= 1)\nNow, to add multiple FVs together, we just need to make sure that an element in a FV is being multiplied with the corresponding weight of the PM. In that case, we would achieve the same eect as batching, only at a lower level of abstraction. This also works when we have partial features. In this case, the encoded hypervector for the current page would just have partial information and may not correctly represent the data. Some part of this information is contained in the encoded hypervector of another page. However, all these hypervectors will be added together during training. Hence, the nal hypervector will contain all the information.\n\nTo support this strategy in THRIFTY accelerator, the ash controller segments the projection matrix in the same way as the feature vectors in the planes and sends the corresponding segments to the accelerator in each plane. It is important to note that only the features belonging to the same class are added together in batches. So, a chip-level accelerator performs a bitwise comparison between the labels of feature vectors in a page and only processes those belonging to the same nal model together.\n\n\nTraining at top-level\n\nThe encoded hypervectors from ash chips are used for training in the top-level accelerator, which is implemented on an FPGA present in the SSD. We use FPGA because it is exible with the application parameters and can be congured using the primitives provided by INSIDER [8]. The encoded hypervectors come with class labels. During training, they are accumulated into the corresponding class (or model) hypervectors. At the end of training we obtain an output hypervector for each class that in turn represents all the input samples belonging to that class.\n\nIn the FPGA, we rst allocate memory for the nal class hypervectors. For each class, the FPGA has an input queue, where the input hypervectors belonging to that class are indexed, and an accumulator, which serially accumulates the vectors in the input queue to generate the nal class hypervector. The class label of an incoming hypervector is used to index it to the corresponding class input queue. The size of the queue is determined based on the frequency of the inputs, the number of classes, and dimensionality \u21e1. The introduction of class-wise input queues removes the input data dependency of the accumulator by pre-processing class labels. An accumulator simply needs to read the input index from its queue and operate on the corresponding data. It makes the computation for dierent classes independent and parallelizable. The accumulators for each class then operate in parallel to add an input hypervector from the queue to the corresponding class hypervector. While the computation can also be fully parallelized over all dimension, the large size of hypervectors and the limited read ports of the memory make it impractical. Hence, we divide the hypervectors into partitions to allow partial parallelism. The nal class hypervectors are sent to the host.\n\nIf an application has too many classes or requires extremely large number of dimensions, then the FPGA may not have enough space to store all the class hypervectors. In such a case, we allocate the memory for the maximum number of class hypervectors, \u21e0 <0G . We assign labels to these classes with respect to the incoming hypervectors. Hence, the rst set of incoming hypervectors belonging to \u21e0 <0G dierent classes are processed as before. We introduce an addition queue that indexes, along with their labels, the incoming hypervectors not associated with any of the active \u21e0 <0G class. Whenever the queue is full, one of the \u21e0 <0G class hypervectors is sent to the CPU host. The corresponding memory is allocated for the class to which the rst hypervector in the queue belongs. The class hypervector sent to the host is the one that has accumulated the most incoming hypervectors.\n\n\nSoftware Support\n\nTHRIFTY derives its base system-architecture from INSIDER [8]. The INSIDER framework is an API which, while being compatible with POSIX, allows us to implement an ISC accelerator cluster. The INSIDER API takes a C++ or RTL code as an input and programs the acceleration cluster (running on drive FPGA) accordingly. The drive program interface has three FIFOs. The data input (output) FIFO takes in the input (output) data that is needed (generated) by the accelerator. The parameter FIFO contains the runtime parameters for the FPGA which are sent by the host. INSIDER keeps control and data planes of ISC separated. The drive control and standard operations are handled by the SSD rmware while all compute data from ash chips are intercepted by the top level FPGA accelerator for computing. The FPGA doesn't care about the source and/or destination of the data.\n\n\nTHRIFTY Host-Side Support.\n\nINSIDER API uses POSIX-like I/O functionaly to communicate with the driver. INSIDER has a standard block device driver with changes made to the virtual read and write functionalities to accommodate for the programmable accelerator clusters in the drive. However, the current abstraction allow us to pass directive/parameters only to the ISC FPGA and not the drive. We dene a new API, send_mode, which denes the mode for read and write operations, further discussed in Section 4.4.2. It passes a single integer, <>34, to the drive rmware while opening a virtual le. For a non-ISC read/write from the drive, <>34 is set to '0. ' During an ISC read, <>34 represents the 4G?0=B8>=5 02C>A (\u21e2 ). \u21e2 denes the increase in the size of raw data after encoding. For example, \u21e2 = 5 means that each page of raw data generates ve pages of encoded data (due to large \u21e1). In this case, <>34 is set to 5. This parameter is necessary to enable the drive to read the required number of pages from the ash chips. Since \u21e2 is dependent on the number of features of the data and dimensionality requirement of the application, it remains constant for an entire run. Similarly, a non-zero <>34 signies ISC write. In this case, the data being sent to the drive contains the elements of the HDC projection matrix and is written to the controller scratchpad. No data is written to the ash chips. During write we only care about whether <>34 is zero or non-zero.\n\n\nTHRIFTY\n\nDrive-Side Architecture. THRIFTY implements its top-level accelerator described in Section 4.3 as an INSIDER acceleration cluster, which enables the nal training step. However, INSIDER system doesn't support THRIFTY's die-level acceleration because the standard read/write drive operations can't readily accommodate on-the-y change in data size while reading encoded pages and writing projection matrix elements to the on-die accelerator.\n\nTHRIFTY introduces the processing capability between ash planes and page buers but sometimes only raw data may be required. Hence, THRIFTY employs two read modes, normal and compute. It uses the die-level accelerator in multiplexed mode where a read page is sent to the accelerator for processing only in compute mode, shown in Figure 4. In normal mode, the plane directly writes the original page to the page buer. Moreover, response type in the two modes also diers. A normal read results in just one page while a compute read responds with multiple but xed number of pages. THRIFTY uses application specications such as feature vector size and dimensionality requirement to generate an expansion factor, which is supplied to the SSD rmware by the host, as explained in Section 4.4.1. The rmware uses this factor to calculate the response size for page read commands in compute mode. THRIFTY also employs two write modes, normal and compute. The compute mode is used to supply projection matrix data to the on-die accelerators. In normal mode, data is written in the data buer and then programmed in the ash array. In compute mode, the data in data buer is sent to the accelerators as shown in Figure  4. The writes in compute mode are fast since the data is just latched in CMOS registers instead of ash arrays. Unlike a compute mode read, where the same command can be issued to all the chips, compute mode write requires individual commands for each plane to congure their respective on-die accelerators. This follows from Figure 3. Each plane gets the same segments but their positions may dier for dierent planes. A write conguration command is separately issued for each plane. For each plane, it congures the size of segment (B46 ( ), number of input segments (B46 8= ), actual number of segments in the plane (B46 02C ), and the ID of the rst segment (B46 >=4 ). The format of the command is [B46 ( , B46 8= , B46 02C , B46 >=4 ]. For example, the command for plane 0 and plane 2 in Figure 3 would be [200,4,5,0] and [200,4,5,2] respectively. While sequential, this step has negligible latency overhead because it can be performed in parallel for all the ash chips.\n\nAs discussed briey in Section 4.2, the ash controller sends the projection matrix elements to the respective accelerators. SSD receives the projection matrix from host. We introduce a dedicated scratchpad in the ash controller to store the matrix. The controller sends the elements in page-sized frames to the die accelerators. The frames consist of multiple segments and are used by the dieaccelerators according to the conguration command, as shown in Figure 3.\n\n\nRESULTS\n\n\nExperimental Setup\n\nWe develope a simulator for THRIFTY which supports parallel read and write accesses to the ash chips. We utilize Verilog and Synopsys Design Compiler to implement and synthesize our die-level accelerator at 45nm and scale it down to 22nm. The top-level FPGA accelerator has been synthesized and simulated in Xilinx Vivado. For THRIFTY drive simulation, we assume the characteristics similar to 1TB Intel DC P4500 PCIe-3.1 SSD connected to an Intel(R)  Table 1.\n\nWe compare THRIFTY with 7th Gen 2.4GHz Kaby Lake Intel Core i5 CPU with 8MB RAM and 256 GB SSD. We also compare it with a 3.5GHz Intel(R) Xeon(R) CPU E5-2640 v3 CPU server with 256GB RAM and 2TB local disk. We also compare THRIFTY with INSIDER [8] and DeepStore [9], the state-of-the art ISC solutions. INSIDER is a full-stack storage system and uses a top-level FPGA accelerator in the drive for ISC. DeepStore is an ISC implementation for querybased workloads which employs specialized accelerators in SSD. For all our experiments, including those for other ISC solutions, the data is assumed to be channel-striped and stored using THRIFTY's proposed scheme.\n\n\nWorkloads\n\nWe evaluate the eciency of THRIFTY on ve popular classication applications, as listed below: Speech Recognition (ISOLET): The goal is to recognize voice audio of the 26 letters of the English alphabet [21]. Face Recognition (FACE): We exploit Caltech dataset of 10,000 web faces [22]. Negative training images, i.e., non-face images, are selected from CIFAR-100 and Pascal VOS 2012 datasets [23]. Activity Recognition (UCIHAR): The dataset includes signals collected from motion sensors for 8 subjects performing 19 dierent activities [24]. Medical Diagnosis (CARDIO): This dataset provides medical diagnosis based on cardiotocography information about each patient [25]. Gesture Recognition (EMG): This dataset contains EMG readings for ve dierent hand gestures [26].\n\n\nComparison with CPU and CPU Server\n\nWe rst compare THRIFTY with CPU and CPU-based server running state-of-the-art implementations of HDC encoding and training over the ve datasets with \u21e1 = 10:. In addition, we generate a synthetic dataset with 10 classes and each data sample having 512 features. We vary the size \u21e1( (number of data points) of the synthetic dataset from 10 3 to 10 7 . The runtime for dierent platforms is shown in Figure 5. We observe that THRIFTY is on average 3405\u21e5 and 1612\u21e5 faster than CPU and CPU-server, respectively. Our evaluations show that the performance of THRIFTY increases linearly with an increase in the dataset size. This happens because more data samples result in more huge hypervectors to generate  and process. In conventional systems, this translates to a huge amount of data transfers between the core and memory. It should be noted that the CPU system runs out of memory while encoding for 10 6 samples and kills the process. The CPU server faces a similar situation for 10 7 samples. In contrast, since THRIFTY generates hypervectors (encoding) while reading data out of the slow ash arrays and processes (training) them on the disk itself, there is minimal data movement involved. Figure 5 also shows the size of raw input data in each case normalized to the size of the corresponding trained class hypervectors. While THRIFTY only sends class hypervectors from drive to the host, CPU-based systems fetch all data samples from the disk. We observe that the ratio increases linearly with an increase in the data size. In fact, the size of class hypervectors does not change with an increase in data size as long as the number of classes and required dimensions remain the same. Figure 6 shows the breakdown of THRIFTY latency normalized to the total latency. Here I/O shows the time spent in sending the generated class hypervectors to the host. For small datasets, CARDIO and EMG, the latency is dominated by the encoding. However, as the data size increases, the internal SSD channel bandwidth becomes a bottleneck. This indicates that THRIFTY is able to completely utilize and saturate the huge internal SSD bandwidth. In addition, a signicant amount of time spent in training and some part of the encoding is hidden by the SSD channel latency. As a result, the combined latency is less than sum of the latency for individual stages. For example of FACE dataset, even though the training takes more than half of the total latency, a negligible portion of it actually contributes to the overall latency. It shows that THRIFTY stages are able to hide some of their latency.\n\n\nTHRIFTY Eciency\n\nTo demonstrate the scalability provided by THRIFTY, we evaluate it over a synthetic dataset with 10 4 samples each with 512 features. We vary the dimensions \u21e1 from 10 3 to 10 5 . Figure 7a shows that the latency of THRIFTY increases linearly with an increase in the number of dimensions, showing that THRIFTY is able to scale with \u21e1. Additionally, an increase in \u21e1 results in longer class hypervectors for the same input data. Hence, the ratio of raw data to hypervector size decreases with an increase in dimensions, falling from from 512 for \u21e1 = 1: to 2.5 for \u21e1 = 10 5 .\n\nWe also scale the dataset with the number of class, while keeping its size xed to 10 4 samples and \u21e1 as 10 3 . Figure 7b shows that the THRIFTY latency has minor changes with the number of classes when we have less than 50 classes. This is because our FPGA has enough resources to train up to 54 classes with \u21e1 = 10: dimensions. The latency almost doubles for 100 classes. However, when number of classes increases further, the size of model hypervectors is too large to store in the FPGA. Hence, partially trained hypervectors are then sent to the host for further processing. This can be seen by a jump in the latency for 500 classes in Figure 7b. In addition to the time spent in training, transferring the class hypervectors to host creates a major bottleneck. This is also evident from the data size ratio which declines for large number of classes. A ratio of less than 1 signies that the size of generated hypervectors is larger than the raw data.\n\n\nComparison with Existing Solutions\n\nWe compare the performance and data transfer eciency of THRIFTY with INSIDER [8] and DeepStore [9]. In our experiments, INSIDER performs both encoding and training using the FPGA accelerator in SSD and sends the class hypervectors to the host. Since DeepStore was intended for a completely dierent application, we replace its accelerator with THRIFTY die-level accelerator. During ISC, Deepstore encodes the raw data into hypervectors and sends those hypervectors to the host for training. Figure 8 shows the change in latency and data transfer size for the three ISC solutions. We observe that THRIFTY is on an average 14.4\u21e5 and 446.8\u21e5 faster than INSIDER and DeepStore, respectively. While encoding in DeepStore takes the same time as THRIFTY, transferring hypervector from SSD to host and further training on them on CPU increases the execution time of DeepStore signicantly. On the other hand, the SSD channel bottleneck faced by THRIFTY is relaxed in case of  [8] and DeepStore [9] INSIDER since it only transfers raw data. However, the FPGA-based HDC encoding+training are on an average 21\u21e5 slower as compared to FPGA-based training. Also, since INSIDER performs training in SSD, it transfers the same amount of data to the host as THRIFTY. However, by transferring untrained hypervectors, DeepStore increases the amount of data transferred on an average by 397\u21e5 as compared to THRIFTY.\n\n\nCONCLUSION\n\nIn this paper, we proposed an in-storage HDC system that spans multiple levels of the storage hierarchy. We exploited the internal bandwidth and hierarchical structure of SSDs to perform HDC encoding and training in-storage over multiple data samples in parallel. We proposed batched HDC training to enable partial processing of HDC hypervectors. We further proposed die-level and top-level accelerators for HDC encoding and training respectively. Our evaluation shows that THRIFTY is on average 1612\u21e5 faster than a CPU-server and 14.4\u21e5 faster than INSIDER for HDC encoding and training.\n\nargmax 2 ( 2 \u21e5 3 Figure 3 :\n2233= + 3.=/= ? \uf8ff ? B )where 2 is the number of complete =-sized feature vectors in a ? Bsized page, = ? is the number of planes per chip, and 3 2 {0, 1, ...= ? }. Hence, a page would contain 2 \u21e5 = ? + 3 segments in total. Having = ? equal segments instead of any variable segmentation allows the accelerator to have a simple segment-wise weight allocation. Each row-vector in the projection matrix of a plane is divided into the same sized segments as the feature vector as shown inFigure Datastorage scheme in THRIFTY and the corresponding segmentation of the projection matrix. Data represents a feature vector.the right. This allows THRIFTY to increase storage eciency while minimizing the control overhead of the accelerator.If the size of the feature vector is less than the page size, THRIFTY uses the same segmentation size. However, the number of segments in a page are given by:argmax 3 (3.=/= ? \uf8ff ? B )\n\nFigure 4 :\n4Dierent read and write modes in THRIFTY. The components in red are active during the operation.\n\nFigure 5 :\n5Runtime comparison of HDC encoding and training in THRIFTY with other platforms. The bars in red shows the size of raw data normalized to the total size of corresponding class hypervectors in THRIFTY.\n\nFigure 6 :\n6Breakdown of latency of dierent stages of HDC normalized to the total latency.\n\nFigure 7 :\n7Change in HDC runtime and raw data size to hypervector ratio with (a) dimensions and (b) number of classes.\n\nFigure 8 :\n8Runtime and data transfer size comparison of THRIFTY with INSIDER\n\n\n4.2.1Chip-level Accelerator Design. THRIFTY plane-accelerator encodes an entire page with raw data to generate a \u21e1 dimensional hypervector. Let us assume the SSD page size to be 4KB (? B ) with each data point being 4 bytes (3 B ). This translates to 1K data points (? B /3 B ). Let the feature vector contain 1K features.Assuming that \nthe feature vectors are page-aligned, each page stores one feature \nvector. HDC encoding multiplies =-size feature vector with a pro-\njection matrix containing \u21e1 \u21e5 = 1-bit elements. Our accelerator \ncalculates the dot product between two page-long vectors, one read \n\nPLANE \n\nPage \n\nXNOR Array \n\nPM Row Reg \nCSA \n\nN:2 \nreduce \n\nN words \n\nN words \n\nCLA \n\nBUFFER \n\n2 \nwords \n\n\n\nTable 1 :\n1THRIFTY ParametersCapacity \n1) \u232b \nChannels \n32 \nPage Size \n16 \u232b \nChips/Channel \n4 \nExternal BW \n3.2\u2327\u232b?B \nPlanes/Chip \n8 \nBW/Channel \n800\"\u232b?B \nBlos/Plane \n512 \nFlash Latency \n53DB \nPages/Blo \n128 \nFPGA \n-\u21e0 * 025 Scratpad Size 4\"\u232b \nAvg Power/DA \n8<, \nDA Latency \n1.02=B \n\n*DA: Die-accelerator \n\nXeon(R) CPU E5-2640 v3 host. The parameters for THRIFTY are \nshown in \nACKNOWLEDGMENTSThis work was supported in part by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, in part by SRC-Global Research Collaboration grant, and also NSF grants # 1527034, #1730158, #1826967, and #1911095.\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, IEEE Transactions on Neural Networks and Learning Systems. 99O. Rasanen and J. Saarinen, \"Sequence prediction with sparse distributed hyper- dimensional coding applied to the analysis of mobile phone use patterns, \" IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1-12, 2015.\n\nHierarchical hyperdimensional computing for energy ecient classication. M Imani, C Huang, D Kong, T Rosing, 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC). IEEEM. Imani, C. Huang, D. Kong, and T. Rosing, \"Hierarchical hyperdimensional computing for energy ecient classication, \" in 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), pp. 1-6, IEEE, 2018.\n\nA robust and energy-ecient classier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignACMA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy-ecient classier using brain-inspired hyperdimensional computing,\" in Proceedings of the 2016 International Symposium on Low Power Electronics and Design, pp. 64-69, ACM, 2016.\n\nYoursql: a high-performance database system leveraging in-storage computing. I Jo, D.-H Bae, A S Yoon, J.-U Kang, S Cho, D D Lee, J Jeong, Proceedings of the VLDB Endowment. the VLDB Endowment9I. Jo, D.-H. Bae, A. S. Yoon, J.-U. Kang, S. Cho, D. D. Lee, and J. Jeong, \"Yoursql: a high-performance database system leveraging in-storage computing, \" Proceedings of the VLDB Endowment, vol. 9, no. 12, pp. 924-935, 2016.\n\nBiscuit: A framework for near-data processing of big data workloads. B Gu, A S Yoon, D.-H Bae, I Jo, J Lee, J Yoon, J.-U Kang, M Kwon, C Yoon, S Cho, ACM SIGARCH Computer Architecture News. 443B. Gu, A. S. Yoon, D.-H. Bae, I. Jo, J. Lee, J. Yoon, J.-U. Kang, M. Kwon, C. Yoon, S. Cho, et al., \"Biscuit: A framework for near-data processing of big data work- loads,\" ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 153-165, 2016.\n\nSummarizer: trading communication with computing near storage. G Koo, K K Matam, I Te, H K G Narra, J Li, H.-W Tseng, S Swanson, M Annavaram, 2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEG. Koo, K. K. Matam, I. Te, H. K. G. Narra, J. Li, H.-W. Tseng, S. Swanson, and M. Annavaram, \"Summarizer: trading communication with computing near stor- age, \" in 2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 219-231, IEEE, 2017.\n\nInsider: designing in-storage computing system for emerging high-performance drive. Z Ruan, T He, J Cong, Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference. the 2019 USENIX Conference on Usenix Annual Technical ConferenceZ. Ruan, T. He, and J. Cong, \"Insider: designing in-storage computing system for emerging high-performance drive, \" in Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference, pp. 379-394, 2019.\n\nDeepstore: In-storage acceleration for intelligent queries. V S Mailthody, Z Qureshi, W Liang, Z Feng, S G De Gonzalo, Y Li, H Franke, J Xiong, J Huang, W.-M Hwu, Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture. the 52nd Annual IEEE/ACM International Symposium on MicroarchitectureV. S. Mailthody, Z. Qureshi, W. Liang, Z. Feng, S. G. De Gonzalo, Y. Li, H. Franke, J. Xiong, J. Huang, and W.-m. Hwu, \"Deepstore: In-storage acceleration for intelligent queries,\" in Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, pp. 224-238, 2019.\n\nA robust and energy-ecient classier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignACMA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy-ecient classier using brain-inspired hyperdimensional computing,\" in Proceedings of the 2016 International Symposium on Low Power Electronics and Design, pp. 64-69, ACM, 2016.\n\nVoicehd: Hyperdimensional computing for ecient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, 2017 IEEE International Conference on. IEEEin Rebooting Computing (ICRCM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"Voicehd: Hyperdimensional com- puting for ecient speech recognition,\" in Rebooting Computing (ICRC), 2017 IEEE International Conference on, pp. 1-8, IEEE, 2017.\n\nHyperdimensional biosignal processing: A case study for emg-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, 2016 IEEE International Conference on Rebooting Computing (ICRC). IEEEA. Rahimi, S. Benatti, P. Kanerva, L. Benini, and J. M. Rabaey, \"Hyperdimensional biosignal processing: A case study for emg-based hand gesture recognition, \" in 2016 IEEE International Conference on Rebooting Computing (ICRC), pp. 1-8, IEEE, 2016.\n\nHyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classication of eeg error-related potentials. A Rahimi, P Kanerva, J D R Mill\u00e1n, J M Rabaey, 10th EAI Int. Conf. on Bio-inspired Information and Communications Technologies. A. Rahimi, P. Kanerva, J. d. R. Mill\u00e1n, and J. M. Rabaey, \"Hyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot clas- sication of eeg error-related potentials,\" in 10th EAI Int. Conf. on Bio-inspired Information and Communications Technologies, no. CONF, 2017.\n\nHardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. M Schmuck, L Benini, A Rahimi, ACM Journal on Emerging Technologies in Computing Systems (JETC). 154M. Schmuck, L. Benini, and A. Rahimi, \"Hardware optimizations of dense bi- nary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory,\" ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 15, no. 4, pp. 1-25, 2019.\n\nBrain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study. T F Wu, ISSCC. IEEET. F. Wu et al., \"Brain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study, \" in ISSCC, pp. 492-494, IEEE, 2018.\n\nExploring hyperdimensional associative memory. M Imani, IEEE HPCA. IEEEM. Imani et al., \"Exploring hyperdimensional associative memory, \" in IEEE HPCA, IEEE, 2017.\n\nKvdirect: High-performance in-memory key-value store with programmable nic. B Li, Z Ruan, W Xiao, Y Lu, Y Xiong, A Putnam, E Chen, L Zhang, Proceedings of the 26th Symposium on Operating Systems Principles. the 26th Symposium on Operating Systems PrinciplesB. Li, Z. Ruan, W. Xiao, Y. Lu, Y. Xiong, A. Putnam, E. Chen, and L. Zhang, \"Kv- direct: High-performance in-memory key-value store with programmable nic, \" in Proceedings of the 26th Symposium on Operating Systems Principles, pp. 137-152, 2017.\n\nWillow: a user-programmable ssd. S Seshadri, M Gahagan, S Bhaskaran, T Bunker, A De, Y Jin, Y Liu, S Swanson, Proceedings of the 11th USENIX conference on Operating Systems Design and Implementation. the 11th USENIX conference on Operating Systems Design and ImplementationS. Seshadri, M. Gahagan, S. Bhaskaran, T. Bunker, A. De, Y. Jin, Y. Liu, and S. Swanson, \"Willow: a user-programmable ssd, \" in Proceedings of the 11th USENIX conference on Operating Systems Design and Implementation, pp. 67-80, 2014.\n\nA binary learning framework for hyperdimensional computing. M Imani, DATE. M. Imani et al., \"A binary learning framework for hyperdimensional computing, \" in DATE, 2019.\n\nA ash memory controller for 15`s ultra-low-latency ssd using highspeed 3d nand ash with 3`s read time. W Cheong, C Yoon, S Woo, K Han, D Kim, C Lee, Y Choi, S Kim, D Kang, G Yu, 2018 IEEE International Solid-State Circuits Conference-(ISSCC). IEEEW. Cheong, C. Yoon, S. Woo, K. Han, D. Kim, C. Lee, Y. Choi, S. Kim, D. Kang, G. Yu, et al., \"A ash memory controller for 15`s ultra-low-latency ssd using high- speed 3d nand ash with 3`s read time,\" in 2018 IEEE International Solid-State Circuits Conference-(ISSCC), pp. 338-340, IEEE, 2018.\n\nUci machine learning repository. \"Uci machine learning repository. \" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nCaltech-256 object category dataset. G Grin, A Holub, P Perona, G. Grin, A. Holub, and P. Perona, \"Caltech-256 object category dataset, \" 2007.\n\nThe pascal visual object classes challenge: A retrospective. M Everingham, S A Eslami, L Van Gool, C K Williams, J Winn, A Zisserman, International journal of computer vision. 1111M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser- man, \"The pascal visual object classes challenge: A retrospective, \" International journal of computer vision, vol. 111, no. 1, pp. 98-136, 2015.\n\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. D Anguita, A Ghio, L Oneto, X Parra, J L Reyes-Ortiz, International workshop on ambient assisted living. SpringerD. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, \"Human activity recognition on smartphones using a multiclass hardware-friendly support vec- tor machine,\" in International workshop on ambient assisted living, pp. 216-223, Springer, 2012.\n\nCardiotocography data set. \"Cardiotocography data set.\" https://archive.ics.uci.edu/ml/datasets/ cardiotocography.\n\nAnalysis of robust implementation of an emg pattern recognition based control. S Benatti, E Farella, E Gruppioni, L Benini, BIOSIGNALS. S. Benatti, E. Farella, E. Gruppioni, and L. Benini, \"Analysis of robust implemen- tation of an emg pattern recognition based control., \" in BIOSIGNALS, pp. 45-54, 2014.\n", "annotations": {"author": "[{\"start\":\"76\",\"end\":\"106\"},{\"start\":\"107\",\"end\":\"143\"},{\"start\":\"144\",\"end\":\"183\"},{\"start\":\"184\",\"end\":\"193\"},{\"start\":\"194\",\"end\":\"226\"},{\"start\":\"227\",\"end\":\"259\"},{\"start\":\"260\",\"end\":\"274\"},{\"start\":\"275\",\"end\":\"289\"},{\"start\":\"290\",\"end\":\"319\"},{\"start\":\"320\",\"end\":\"341\"},{\"start\":\"342\",\"end\":\"354\"},{\"start\":\"355\",\"end\":\"369\"},{\"start\":\"370\",\"end\":\"384\"},{\"start\":\"385\",\"end\":\"398\"},{\"start\":\"399\",\"end\":\"406\"},{\"start\":\"407\",\"end\":\"443\"},{\"start\":\"444\",\"end\":\"524\"},{\"start\":\"525\",\"end\":\"583\"},{\"start\":\"584\",\"end\":\"620\"},{\"start\":\"621\",\"end\":\"657\"},{\"start\":\"658\",\"end\":\"694\"},{\"start\":\"695\",\"end\":\"779\"}]", "publisher": null, "author_last_name": "[{\"start\":\"84\",\"end\":\"89\"},{\"start\":\"114\",\"end\":\"120\"},{\"start\":\"156\",\"end\":\"164\"},{\"start\":\"190\",\"end\":\"192\"},{\"start\":\"201\",\"end\":\"207\"},{\"start\":\"233\",\"end\":\"240\"},{\"start\":\"268\",\"end\":\"273\"},{\"start\":\"282\",\"end\":\"288\"},{\"start\":\"297\",\"end\":\"302\"},{\"start\":\"332\",\"end\":\"340\"},{\"start\":\"351\",\"end\":\"353\"},{\"start\":\"362\",\"end\":\"368\"},{\"start\":\"376\",\"end\":\"383\"}]", "author_first_name": "[{\"start\":\"76\",\"end\":\"83\"},{\"start\":\"107\",\"end\":\"113\"},{\"start\":\"144\",\"end\":\"155\"},{\"start\":\"184\",\"end\":\"189\"},{\"start\":\"194\",\"end\":\"200\"},{\"start\":\"227\",\"end\":\"232\"},{\"start\":\"260\",\"end\":\"267\"},{\"start\":\"275\",\"end\":\"281\"},{\"start\":\"290\",\"end\":\"296\"},{\"start\":\"320\",\"end\":\"331\"},{\"start\":\"342\",\"end\":\"350\"},{\"start\":\"355\",\"end\":\"361\"},{\"start\":\"370\",\"end\":\"375\"},{\"start\":\"385\",\"end\":\"397\"},{\"start\":\"399\",\"end\":\"405\"}]", "author_affiliation": "[{\"start\":\"408\",\"end\":\"442\"},{\"start\":\"445\",\"end\":\"523\"},{\"start\":\"526\",\"end\":\"582\"},{\"start\":\"585\",\"end\":\"619\"},{\"start\":\"622\",\"end\":\"656\"},{\"start\":\"659\",\"end\":\"693\"},{\"start\":\"696\",\"end\":\"778\"}]", "title": "[{\"start\":\"1\",\"end\":\"73\"},{\"start\":\"780\",\"end\":\"852\"}]", "venue": null, "abstract": "[{\"start\":\"1424\",\"end\":\"3597\"}]", "bib_ref": "[{\"start\":\"4228\",\"end\":\"4231\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"4391\",\"end\":\"4394\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"4394\",\"end\":\"4397\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4397\",\"end\":\"4400\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"4911\",\"end\":\"4914\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4914\",\"end\":\"4917\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"4917\",\"end\":\"4920\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"4920\",\"end\":\"4923\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5248\",\"end\":\"5251\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"7288\",\"end\":\"7292\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"7313\",\"end\":\"7317\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"7337\",\"end\":\"7341\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"7367\",\"end\":\"7371\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"7402\",\"end\":\"7405\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"7585\",\"end\":\"7589\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"7589\",\"end\":\"7593\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"7593\",\"end\":\"7597\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"8137\",\"end\":\"8141\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"8209\",\"end\":\"8212\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"8212\",\"end\":\"8215\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"8326\",\"end\":\"8329\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"8329\",\"end\":\"8331\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"8331\",\"end\":\"8333\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"8398\",\"end\":\"8401\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"9026\",\"end\":\"9029\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"9029\",\"end\":\"9032\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"9730\",\"end\":\"9733\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"9733\",\"end\":\"9735\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"12254\",\"end\":\"12258\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"12345\",\"end\":\"12349\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"13057\",\"end\":\"13060\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"13106\",\"end\":\"13109\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"14764\",\"end\":\"14767\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"15812\",\"end\":\"15822\"},{\"start\":\"19860\",\"end\":\"19864\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"25592\",\"end\":\"25595\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"28106\",\"end\":\"28109\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"32837\",\"end\":\"32842\"},{\"start\":\"32842\",\"end\":\"32844\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"32844\",\"end\":\"32846\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"32846\",\"end\":\"32848\"},{\"start\":\"32853\",\"end\":\"32858\"},{\"start\":\"32858\",\"end\":\"32860\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"32860\",\"end\":\"32862\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"32862\",\"end\":\"32864\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"34205\",\"end\":\"34208\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"34223\",\"end\":\"34226\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"34836\",\"end\":\"34840\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"34914\",\"end\":\"34918\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"35026\",\"end\":\"35030\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"35170\",\"end\":\"35174\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"35301\",\"end\":\"35305\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"35398\",\"end\":\"35402\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"39687\",\"end\":\"39690\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"39705\",\"end\":\"39708\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"40575\",\"end\":\"40578\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"40593\",\"end\":\"40596\",\"attributes\":{\"ref_id\":\"b8\"}}]", "figure": "[{\"start\":\"41605\",\"end\":\"42547\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"42548\",\"end\":\"42656\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"42657\",\"end\":\"42870\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"42871\",\"end\":\"42962\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"42963\",\"end\":\"43083\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"43084\",\"end\":\"43162\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"43163\",\"end\":\"43876\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"43877\",\"end\":\"44252\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"3613\",\"end\":\"4401\"},{\"start\":\"4403\",\"end\":\"5327\"},{\"start\":\"5329\",\"end\":\"5595\"},{\"start\":\"5597\",\"end\":\"6868\"},{\"start\":\"6870\",\"end\":\"7112\"},{\"start\":\"7129\",\"end\":\"7988\"},{\"start\":\"7990\",\"end\":\"8831\"},{\"start\":\"8862\",\"end\":\"9885\"},{\"start\":\"9898\",\"end\":\"10460\"},{\"start\":\"10477\",\"end\":\"10935\"},{\"start\":\"10948\",\"end\":\"11350\"},{\"start\":\"11408\",\"end\":\"12176\"},{\"start\":\"12191\",\"end\":\"13687\"},{\"start\":\"13706\",\"end\":\"14635\"},{\"start\":\"14637\",\"end\":\"15031\"},{\"start\":\"15067\",\"end\":\"15823\"},{\"start\":\"15887\",\"end\":\"17046\"},{\"start\":\"17048\",\"end\":\"18082\"},{\"start\":\"18084\",\"end\":\"18207\"},{\"start\":\"18209\",\"end\":\"19355\"},{\"start\":\"19398\",\"end\":\"21553\"},{\"start\":\"21571\",\"end\":\"22873\"},{\"start\":\"22919\",\"end\":\"23679\"},{\"start\":\"23681\",\"end\":\"24087\"},{\"start\":\"24147\",\"end\":\"24792\"},{\"start\":\"24794\",\"end\":\"25296\"},{\"start\":\"25322\",\"end\":\"25878\"},{\"start\":\"25880\",\"end\":\"27144\"},{\"start\":\"27146\",\"end\":\"28027\"},{\"start\":\"28048\",\"end\":\"28910\"},{\"start\":\"28941\",\"end\":\"30374\"},{\"start\":\"30386\",\"end\":\"30824\"},{\"start\":\"30826\",\"end\":\"33001\"},{\"start\":\"33003\",\"end\":\"33466\"},{\"start\":\"33499\",\"end\":\"33959\"},{\"start\":\"33961\",\"end\":\"34621\"},{\"start\":\"34635\",\"end\":\"35403\"},{\"start\":\"35442\",\"end\":\"38023\"},{\"start\":\"38043\",\"end\":\"38615\"},{\"start\":\"38617\",\"end\":\"39571\"},{\"start\":\"39610\",\"end\":\"41002\"},{\"start\":\"41017\",\"end\":\"41604\"}]", "formula": "[{\"start\":\"10461\",\"end\":\"10476\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"11351\",\"end\":\"11395\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"15824\",\"end\":\"15886\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"24088\",\"end\":\"24146\",\"attributes\":{\"id\":\"formula_3\"}}]", "table_ref": "[{\"start\":\"33951\",\"end\":\"33958\",\"attributes\":{\"ref_id\":\"tab_2\"}}]", "section_header": "[{\"start\":\"3599\",\"end\":\"3611\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"7115\",\"end\":\"7127\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"8834\",\"end\":\"8860\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"9888\",\"end\":\"9896\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"10938\",\"end\":\"10946\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"11397\",\"end\":\"11406\",\"attributes\":{\"n\":\"3.3\"}},{\"start\":\"12179\",\"end\":\"12189\",\"attributes\":{\"n\":\"3.4\"}},{\"start\":\"13690\",\"end\":\"13704\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"15034\",\"end\":\"15065\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"19358\",\"end\":\"19396\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"21556\",\"end\":\"21569\",\"attributes\":{\"n\":\"4.2.2\"}},{\"start\":\"22876\",\"end\":\"22917\",\"attributes\":{\"n\":\"4.2.3\"}},{\"start\":\"25299\",\"end\":\"25320\",\"attributes\":{\"n\":\"4.3\"}},{\"start\":\"28030\",\"end\":\"28046\",\"attributes\":{\"n\":\"4.4\"}},{\"start\":\"28913\",\"end\":\"28939\",\"attributes\":{\"n\":\"4.4.1\"}},{\"start\":\"30377\",\"end\":\"30384\",\"attributes\":{\"n\":\"4.4.2\"}},{\"start\":\"33469\",\"end\":\"33476\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"33479\",\"end\":\"33497\",\"attributes\":{\"n\":\"5.1\"}},{\"start\":\"34624\",\"end\":\"34633\",\"attributes\":{\"n\":\"5.2\"}},{\"start\":\"35406\",\"end\":\"35440\",\"attributes\":{\"n\":\"5.3\"}},{\"start\":\"38026\",\"end\":\"38041\",\"attributes\":{\"n\":\"5.4\"}},{\"start\":\"39574\",\"end\":\"39608\",\"attributes\":{\"n\":\"5.5\"}},{\"start\":\"41005\",\"end\":\"41015\",\"attributes\":{\"n\":\"6\"}},{\"start\":\"41606\",\"end\":\"41633\"},{\"start\":\"42549\",\"end\":\"42559\"},{\"start\":\"42658\",\"end\":\"42668\"},{\"start\":\"42872\",\"end\":\"42882\"},{\"start\":\"42964\",\"end\":\"42974\"},{\"start\":\"43085\",\"end\":\"43095\"},{\"start\":\"43878\",\"end\":\"43887\"}]", "table": "[{\"start\":\"43487\",\"end\":\"43876\"},{\"start\":\"43907\",\"end\":\"44252\"}]", "figure_caption": "[{\"start\":\"41638\",\"end\":\"42547\"},{\"start\":\"42561\",\"end\":\"42656\"},{\"start\":\"42670\",\"end\":\"42870\"},{\"start\":\"42884\",\"end\":\"42962\"},{\"start\":\"42976\",\"end\":\"43083\"},{\"start\":\"43097\",\"end\":\"43162\"},{\"start\":\"43165\",\"end\":\"43487\"},{\"start\":\"43889\",\"end\":\"43907\"}]", "figure_ref": "[{\"start\":\"13794\",\"end\":\"13802\"},{\"start\":\"14038\",\"end\":\"14046\"},{\"start\":\"14265\",\"end\":\"14273\"},{\"start\":\"14324\",\"end\":\"14332\"},{\"start\":\"14552\",\"end\":\"14560\"},{\"start\":\"19535\",\"end\":\"19543\"},{\"start\":\"19959\",\"end\":\"19967\"},{\"start\":\"20088\",\"end\":\"20096\"},{\"start\":\"20665\",\"end\":\"20673\"},{\"start\":\"20767\",\"end\":\"20776\"},{\"start\":\"20997\",\"end\":\"21005\"},{\"start\":\"22406\",\"end\":\"22414\"},{\"start\":\"31154\",\"end\":\"31162\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"32022\",\"end\":\"32031\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"32354\",\"end\":\"32362\"},{\"start\":\"32819\",\"end\":\"32827\"},{\"start\":\"33457\",\"end\":\"33465\"},{\"start\":\"35838\",\"end\":\"35846\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"36631\",\"end\":\"36639\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"37127\",\"end\":\"37135\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"38222\",\"end\":\"38231\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"38728\",\"end\":\"38737\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"39256\",\"end\":\"39265\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"40100\",\"end\":\"40108\",\"attributes\":{\"ref_id\":\"fig_5\"}}]", "bib_author_first_name": "[{\"start\":\"44615\",\"end\":\"44616\"},{\"start\":\"44970\",\"end\":\"44971\"},{\"start\":\"44981\",\"end\":\"44982\"},{\"start\":\"45374\",\"end\":\"45375\"},{\"start\":\"45383\",\"end\":\"45384\"},{\"start\":\"45392\",\"end\":\"45393\"},{\"start\":\"45400\",\"end\":\"45401\"},{\"start\":\"45763\",\"end\":\"45764\"},{\"start\":\"45773\",\"end\":\"45774\"},{\"start\":\"45784\",\"end\":\"45785\"},{\"start\":\"45786\",\"end\":\"45787\"},{\"start\":\"46268\",\"end\":\"46269\"},{\"start\":\"46274\",\"end\":\"46278\"},{\"start\":\"46284\",\"end\":\"46285\"},{\"start\":\"46286\",\"end\":\"46287\"},{\"start\":\"46294\",\"end\":\"46298\"},{\"start\":\"46305\",\"end\":\"46306\"},{\"start\":\"46312\",\"end\":\"46313\"},{\"start\":\"46314\",\"end\":\"46315\"},{\"start\":\"46321\",\"end\":\"46322\"},{\"start\":\"46679\",\"end\":\"46680\"},{\"start\":\"46685\",\"end\":\"46686\"},{\"start\":\"46687\",\"end\":\"46688\"},{\"start\":\"46695\",\"end\":\"46699\"},{\"start\":\"46705\",\"end\":\"46706\"},{\"start\":\"46711\",\"end\":\"46712\"},{\"start\":\"46718\",\"end\":\"46719\"},{\"start\":\"46726\",\"end\":\"46730\"},{\"start\":\"46737\",\"end\":\"46738\"},{\"start\":\"46745\",\"end\":\"46746\"},{\"start\":\"46753\",\"end\":\"46754\"},{\"start\":\"47115\",\"end\":\"47116\"},{\"start\":\"47122\",\"end\":\"47123\"},{\"start\":\"47124\",\"end\":\"47125\"},{\"start\":\"47133\",\"end\":\"47134\"},{\"start\":\"47139\",\"end\":\"47140\"},{\"start\":\"47141\",\"end\":\"47144\"},{\"start\":\"47152\",\"end\":\"47153\"},{\"start\":\"47158\",\"end\":\"47162\"},{\"start\":\"47170\",\"end\":\"47171\"},{\"start\":\"47181\",\"end\":\"47182\"},{\"start\":\"47633\",\"end\":\"47634\"},{\"start\":\"47641\",\"end\":\"47642\"},{\"start\":\"47647\",\"end\":\"47648\"},{\"start\":\"48080\",\"end\":\"48081\"},{\"start\":\"48082\",\"end\":\"48083\"},{\"start\":\"48095\",\"end\":\"48096\"},{\"start\":\"48106\",\"end\":\"48107\"},{\"start\":\"48115\",\"end\":\"48116\"},{\"start\":\"48123\",\"end\":\"48124\"},{\"start\":\"48125\",\"end\":\"48126\"},{\"start\":\"48139\",\"end\":\"48140\"},{\"start\":\"48145\",\"end\":\"48146\"},{\"start\":\"48155\",\"end\":\"48156\"},{\"start\":\"48164\",\"end\":\"48165\"},{\"start\":\"48173\",\"end\":\"48177\"},{\"start\":\"48713\",\"end\":\"48714\"},{\"start\":\"48723\",\"end\":\"48724\"},{\"start\":\"48734\",\"end\":\"48735\"},{\"start\":\"48736\",\"end\":\"48737\"},{\"start\":\"49208\",\"end\":\"49209\"},{\"start\":\"49217\",\"end\":\"49218\"},{\"start\":\"49225\",\"end\":\"49226\"},{\"start\":\"49235\",\"end\":\"49236\"},{\"start\":\"49616\",\"end\":\"49617\"},{\"start\":\"49626\",\"end\":\"49627\"},{\"start\":\"49637\",\"end\":\"49638\"},{\"start\":\"49648\",\"end\":\"49649\"},{\"start\":\"49658\",\"end\":\"49659\"},{\"start\":\"49660\",\"end\":\"49661\"},{\"start\":\"50125\",\"end\":\"50126\"},{\"start\":\"50135\",\"end\":\"50136\"},{\"start\":\"50146\",\"end\":\"50147\"},{\"start\":\"50148\",\"end\":\"50151\"},{\"start\":\"50160\",\"end\":\"50161\"},{\"start\":\"50162\",\"end\":\"50163\"},{\"start\":\"50711\",\"end\":\"50712\"},{\"start\":\"50722\",\"end\":\"50723\"},{\"start\":\"50732\",\"end\":\"50733\"},{\"start\":\"51227\",\"end\":\"51228\"},{\"start\":\"51229\",\"end\":\"51230\"},{\"start\":\"51464\",\"end\":\"51465\"},{\"start\":\"51658\",\"end\":\"51659\"},{\"start\":\"51664\",\"end\":\"51665\"},{\"start\":\"51672\",\"end\":\"51673\"},{\"start\":\"51680\",\"end\":\"51681\"},{\"start\":\"51686\",\"end\":\"51687\"},{\"start\":\"51695\",\"end\":\"51696\"},{\"start\":\"51705\",\"end\":\"51706\"},{\"start\":\"51713\",\"end\":\"51714\"},{\"start\":\"52119\",\"end\":\"52120\"},{\"start\":\"52131\",\"end\":\"52132\"},{\"start\":\"52142\",\"end\":\"52143\"},{\"start\":\"52155\",\"end\":\"52156\"},{\"start\":\"52165\",\"end\":\"52166\"},{\"start\":\"52171\",\"end\":\"52172\"},{\"start\":\"52178\",\"end\":\"52179\"},{\"start\":\"52185\",\"end\":\"52186\"},{\"start\":\"52655\",\"end\":\"52656\"},{\"start\":\"52869\",\"end\":\"52870\"},{\"start\":\"52879\",\"end\":\"52880\"},{\"start\":\"52887\",\"end\":\"52888\"},{\"start\":\"52894\",\"end\":\"52895\"},{\"start\":\"52901\",\"end\":\"52902\"},{\"start\":\"52908\",\"end\":\"52909\"},{\"start\":\"52915\",\"end\":\"52916\"},{\"start\":\"52923\",\"end\":\"52924\"},{\"start\":\"52930\",\"end\":\"52931\"},{\"start\":\"52938\",\"end\":\"52939\"},{\"start\":\"53461\",\"end\":\"53462\"},{\"start\":\"53469\",\"end\":\"53470\"},{\"start\":\"53478\",\"end\":\"53479\"},{\"start\":\"53630\",\"end\":\"53631\"},{\"start\":\"53644\",\"end\":\"53645\"},{\"start\":\"53646\",\"end\":\"53647\"},{\"start\":\"53656\",\"end\":\"53657\"},{\"start\":\"53668\",\"end\":\"53669\"},{\"start\":\"53670\",\"end\":\"53671\"},{\"start\":\"53682\",\"end\":\"53683\"},{\"start\":\"53690\",\"end\":\"53691\"},{\"start\":\"54081\",\"end\":\"54082\"},{\"start\":\"54092\",\"end\":\"54093\"},{\"start\":\"54100\",\"end\":\"54101\"},{\"start\":\"54109\",\"end\":\"54110\"},{\"start\":\"54118\",\"end\":\"54119\"},{\"start\":\"54120\",\"end\":\"54121\"},{\"start\":\"54644\",\"end\":\"54645\"},{\"start\":\"54655\",\"end\":\"54656\"},{\"start\":\"54666\",\"end\":\"54667\"},{\"start\":\"54679\",\"end\":\"54680\"}]", "bib_author_last_name": "[{\"start\":\"44617\",\"end\":\"44624\"},{\"start\":\"44972\",\"end\":\"44979\"},{\"start\":\"44983\",\"end\":\"44991\"},{\"start\":\"45376\",\"end\":\"45381\"},{\"start\":\"45385\",\"end\":\"45390\"},{\"start\":\"45394\",\"end\":\"45398\"},{\"start\":\"45402\",\"end\":\"45408\"},{\"start\":\"45765\",\"end\":\"45771\"},{\"start\":\"45775\",\"end\":\"45782\"},{\"start\":\"45788\",\"end\":\"45794\"},{\"start\":\"46270\",\"end\":\"46272\"},{\"start\":\"46279\",\"end\":\"46282\"},{\"start\":\"46288\",\"end\":\"46292\"},{\"start\":\"46299\",\"end\":\"46303\"},{\"start\":\"46307\",\"end\":\"46310\"},{\"start\":\"46316\",\"end\":\"46319\"},{\"start\":\"46323\",\"end\":\"46328\"},{\"start\":\"46681\",\"end\":\"46683\"},{\"start\":\"46689\",\"end\":\"46693\"},{\"start\":\"46700\",\"end\":\"46703\"},{\"start\":\"46707\",\"end\":\"46709\"},{\"start\":\"46713\",\"end\":\"46716\"},{\"start\":\"46720\",\"end\":\"46724\"},{\"start\":\"46731\",\"end\":\"46735\"},{\"start\":\"46739\",\"end\":\"46743\"},{\"start\":\"46747\",\"end\":\"46751\"},{\"start\":\"46755\",\"end\":\"46758\"},{\"start\":\"47117\",\"end\":\"47120\"},{\"start\":\"47126\",\"end\":\"47131\"},{\"start\":\"47135\",\"end\":\"47137\"},{\"start\":\"47145\",\"end\":\"47150\"},{\"start\":\"47154\",\"end\":\"47156\"},{\"start\":\"47163\",\"end\":\"47168\"},{\"start\":\"47172\",\"end\":\"47179\"},{\"start\":\"47183\",\"end\":\"47192\"},{\"start\":\"47635\",\"end\":\"47639\"},{\"start\":\"47643\",\"end\":\"47645\"},{\"start\":\"47649\",\"end\":\"47653\"},{\"start\":\"48084\",\"end\":\"48093\"},{\"start\":\"48097\",\"end\":\"48104\"},{\"start\":\"48108\",\"end\":\"48113\"},{\"start\":\"48117\",\"end\":\"48121\"},{\"start\":\"48127\",\"end\":\"48137\"},{\"start\":\"48141\",\"end\":\"48143\"},{\"start\":\"48147\",\"end\":\"48153\"},{\"start\":\"48157\",\"end\":\"48162\"},{\"start\":\"48166\",\"end\":\"48171\"},{\"start\":\"48178\",\"end\":\"48181\"},{\"start\":\"48715\",\"end\":\"48721\"},{\"start\":\"48725\",\"end\":\"48732\"},{\"start\":\"48738\",\"end\":\"48744\"},{\"start\":\"49210\",\"end\":\"49215\"},{\"start\":\"49219\",\"end\":\"49223\"},{\"start\":\"49227\",\"end\":\"49233\"},{\"start\":\"49237\",\"end\":\"49243\"},{\"start\":\"49618\",\"end\":\"49624\"},{\"start\":\"49628\",\"end\":\"49635\"},{\"start\":\"49639\",\"end\":\"49646\"},{\"start\":\"49650\",\"end\":\"49656\"},{\"start\":\"49662\",\"end\":\"49668\"},{\"start\":\"50127\",\"end\":\"50133\"},{\"start\":\"50137\",\"end\":\"50144\"},{\"start\":\"50152\",\"end\":\"50158\"},{\"start\":\"50164\",\"end\":\"50170\"},{\"start\":\"50713\",\"end\":\"50720\"},{\"start\":\"50724\",\"end\":\"50730\"},{\"start\":\"50734\",\"end\":\"50740\"},{\"start\":\"51231\",\"end\":\"51233\"},{\"start\":\"51466\",\"end\":\"51471\"},{\"start\":\"51660\",\"end\":\"51662\"},{\"start\":\"51666\",\"end\":\"51670\"},{\"start\":\"51674\",\"end\":\"51678\"},{\"start\":\"51682\",\"end\":\"51684\"},{\"start\":\"51688\",\"end\":\"51693\"},{\"start\":\"51697\",\"end\":\"51703\"},{\"start\":\"51707\",\"end\":\"51711\"},{\"start\":\"51715\",\"end\":\"51720\"},{\"start\":\"52121\",\"end\":\"52129\"},{\"start\":\"52133\",\"end\":\"52140\"},{\"start\":\"52144\",\"end\":\"52153\"},{\"start\":\"52157\",\"end\":\"52163\"},{\"start\":\"52167\",\"end\":\"52169\"},{\"start\":\"52173\",\"end\":\"52176\"},{\"start\":\"52180\",\"end\":\"52183\"},{\"start\":\"52187\",\"end\":\"52194\"},{\"start\":\"52657\",\"end\":\"52662\"},{\"start\":\"52871\",\"end\":\"52877\"},{\"start\":\"52881\",\"end\":\"52885\"},{\"start\":\"52889\",\"end\":\"52892\"},{\"start\":\"52896\",\"end\":\"52899\"},{\"start\":\"52903\",\"end\":\"52906\"},{\"start\":\"52910\",\"end\":\"52913\"},{\"start\":\"52917\",\"end\":\"52921\"},{\"start\":\"52925\",\"end\":\"52928\"},{\"start\":\"52932\",\"end\":\"52936\"},{\"start\":\"52940\",\"end\":\"52942\"},{\"start\":\"53463\",\"end\":\"53467\"},{\"start\":\"53471\",\"end\":\"53476\"},{\"start\":\"53480\",\"end\":\"53486\"},{\"start\":\"53632\",\"end\":\"53642\"},{\"start\":\"53648\",\"end\":\"53654\"},{\"start\":\"53658\",\"end\":\"53666\"},{\"start\":\"53672\",\"end\":\"53680\"},{\"start\":\"53684\",\"end\":\"53688\"},{\"start\":\"53692\",\"end\":\"53701\"},{\"start\":\"54083\",\"end\":\"54090\"},{\"start\":\"54094\",\"end\":\"54098\"},{\"start\":\"54102\",\"end\":\"54107\"},{\"start\":\"54111\",\"end\":\"54116\"},{\"start\":\"54122\",\"end\":\"54133\"},{\"start\":\"54646\",\"end\":\"54653\"},{\"start\":\"54657\",\"end\":\"54664\"},{\"start\":\"54668\",\"end\":\"54677\"},{\"start\":\"54681\",\"end\":\"54687\"}]", "bib_entry": "[{\"start\":\"44490\",\"end\":\"44846\",\"attributes\":{\"matched_paper_id\":\"733980\",\"id\":\"b0\"}},{\"start\":\"44848\",\"end\":\"45300\",\"attributes\":{\"matched_paper_id\":\"15258913\",\"id\":\"b1\"}},{\"start\":\"45302\",\"end\":\"45676\",\"attributes\":{\"matched_paper_id\":\"49301394\",\"id\":\"b2\"}},{\"start\":\"45678\",\"end\":\"46189\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b3\"}},{\"start\":\"46191\",\"end\":\"46608\",\"attributes\":{\"matched_paper_id\":\"18204650\",\"id\":\"b4\"}},{\"start\":\"46610\",\"end\":\"47050\",\"attributes\":{\"matched_paper_id\":\"14108659\",\"id\":\"b5\"}},{\"start\":\"47052\",\"end\":\"47547\",\"attributes\":{\"matched_paper_id\":\"21502412\",\"id\":\"b6\"}},{\"start\":\"47549\",\"end\":\"48018\",\"attributes\":{\"matched_paper_id\":\"196810101\",\"id\":\"b7\"}},{\"start\":\"48020\",\"end\":\"48626\",\"attributes\":{\"matched_paper_id\":\"202758896\",\"id\":\"b8\"}},{\"start\":\"48628\",\"end\":\"49139\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b9\"}},{\"start\":\"49141\",\"end\":\"49522\",\"attributes\":{\"matched_paper_id\":\"21351739\",\"id\":\"b10\"}},{\"start\":\"49524\",\"end\":\"49988\",\"attributes\":{\"matched_paper_id\":\"12008695\",\"id\":\"b11\"}},{\"start\":\"49990\",\"end\":\"50549\",\"attributes\":{\"matched_paper_id\":\"8996877\",\"id\":\"b12\"}},{\"start\":\"50551\",\"end\":\"51110\",\"attributes\":{\"matched_paper_id\":\"49907924\",\"id\":\"b13\"}},{\"start\":\"51112\",\"end\":\"51415\",\"attributes\":{\"matched_paper_id\":\"3869844\",\"id\":\"b14\"}},{\"start\":\"51417\",\"end\":\"51580\",\"attributes\":{\"matched_paper_id\":\"1677864\",\"id\":\"b15\"}},{\"start\":\"51582\",\"end\":\"52084\",\"attributes\":{\"matched_paper_id\":\"28859462\",\"id\":\"b16\"}},{\"start\":\"52086\",\"end\":\"52593\",\"attributes\":{\"matched_paper_id\":\"1491271\",\"id\":\"b17\"}},{\"start\":\"52595\",\"end\":\"52764\",\"attributes\":{\"matched_paper_id\":\"155109576\",\"id\":\"b18\"}},{\"start\":\"52766\",\"end\":\"53305\",\"attributes\":{\"matched_paper_id\":\"3868842\",\"id\":\"b19\"}},{\"start\":\"53307\",\"end\":\"53422\",\"attributes\":{\"id\":\"b20\"}},{\"start\":\"53424\",\"end\":\"53567\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"53569\",\"end\":\"53976\",\"attributes\":{\"matched_paper_id\":\"207252270\",\"id\":\"b22\"}},{\"start\":\"53978\",\"end\":\"54447\",\"attributes\":{\"matched_paper_id\":\"13178535\",\"id\":\"b23\"}},{\"start\":\"54449\",\"end\":\"54563\",\"attributes\":{\"id\":\"b24\"}},{\"start\":\"54565\",\"end\":\"54870\",\"attributes\":{\"matched_paper_id\":\"15953456\",\"id\":\"b25\"}}]", "bib_title": "[{\"start\":\"44490\",\"end\":\"44613\"},{\"start\":\"44848\",\"end\":\"44968\"},{\"start\":\"45302\",\"end\":\"45372\"},{\"start\":\"45678\",\"end\":\"45761\"},{\"start\":\"46191\",\"end\":\"46266\"},{\"start\":\"46610\",\"end\":\"46677\"},{\"start\":\"47052\",\"end\":\"47113\"},{\"start\":\"47549\",\"end\":\"47631\"},{\"start\":\"48020\",\"end\":\"48078\"},{\"start\":\"48628\",\"end\":\"48711\"},{\"start\":\"49141\",\"end\":\"49206\"},{\"start\":\"49524\",\"end\":\"49614\"},{\"start\":\"49990\",\"end\":\"50123\"},{\"start\":\"50551\",\"end\":\"50709\"},{\"start\":\"51112\",\"end\":\"51225\"},{\"start\":\"51417\",\"end\":\"51462\"},{\"start\":\"51582\",\"end\":\"51656\"},{\"start\":\"52086\",\"end\":\"52117\"},{\"start\":\"52595\",\"end\":\"52653\"},{\"start\":\"52766\",\"end\":\"52867\"},{\"start\":\"53569\",\"end\":\"53628\"},{\"start\":\"53978\",\"end\":\"54079\"},{\"start\":\"54565\",\"end\":\"54642\"}]", "bib_author": "[{\"start\":\"44615\",\"end\":\"44626\"},{\"start\":\"44970\",\"end\":\"44981\"},{\"start\":\"44981\",\"end\":\"44993\"},{\"start\":\"45374\",\"end\":\"45383\"},{\"start\":\"45383\",\"end\":\"45392\"},{\"start\":\"45392\",\"end\":\"45400\"},{\"start\":\"45400\",\"end\":\"45410\"},{\"start\":\"45763\",\"end\":\"45773\"},{\"start\":\"45773\",\"end\":\"45784\"},{\"start\":\"45784\",\"end\":\"45796\"},{\"start\":\"46268\",\"end\":\"46274\"},{\"start\":\"46274\",\"end\":\"46284\"},{\"start\":\"46284\",\"end\":\"46294\"},{\"start\":\"46294\",\"end\":\"46305\"},{\"start\":\"46305\",\"end\":\"46312\"},{\"start\":\"46312\",\"end\":\"46321\"},{\"start\":\"46321\",\"end\":\"46330\"},{\"start\":\"46679\",\"end\":\"46685\"},{\"start\":\"46685\",\"end\":\"46695\"},{\"start\":\"46695\",\"end\":\"46705\"},{\"start\":\"46705\",\"end\":\"46711\"},{\"start\":\"46711\",\"end\":\"46718\"},{\"start\":\"46718\",\"end\":\"46726\"},{\"start\":\"46726\",\"end\":\"46737\"},{\"start\":\"46737\",\"end\":\"46745\"},{\"start\":\"46745\",\"end\":\"46753\"},{\"start\":\"46753\",\"end\":\"46760\"},{\"start\":\"47115\",\"end\":\"47122\"},{\"start\":\"47122\",\"end\":\"47133\"},{\"start\":\"47133\",\"end\":\"47139\"},{\"start\":\"47139\",\"end\":\"47152\"},{\"start\":\"47152\",\"end\":\"47158\"},{\"start\":\"47158\",\"end\":\"47170\"},{\"start\":\"47170\",\"end\":\"47181\"},{\"start\":\"47181\",\"end\":\"47194\"},{\"start\":\"47633\",\"end\":\"47641\"},{\"start\":\"47641\",\"end\":\"47647\"},{\"start\":\"47647\",\"end\":\"47655\"},{\"start\":\"48080\",\"end\":\"48095\"},{\"start\":\"48095\",\"end\":\"48106\"},{\"start\":\"48106\",\"end\":\"48115\"},{\"start\":\"48115\",\"end\":\"48123\"},{\"start\":\"48123\",\"end\":\"48139\"},{\"start\":\"48139\",\"end\":\"48145\"},{\"start\":\"48145\",\"end\":\"48155\"},{\"start\":\"48155\",\"end\":\"48164\"},{\"start\":\"48164\",\"end\":\"48173\"},{\"start\":\"48173\",\"end\":\"48183\"},{\"start\":\"48713\",\"end\":\"48723\"},{\"start\":\"48723\",\"end\":\"48734\"},{\"start\":\"48734\",\"end\":\"48746\"},{\"start\":\"49208\",\"end\":\"49217\"},{\"start\":\"49217\",\"end\":\"49225\"},{\"start\":\"49225\",\"end\":\"49235\"},{\"start\":\"49235\",\"end\":\"49245\"},{\"start\":\"49616\",\"end\":\"49626\"},{\"start\":\"49626\",\"end\":\"49637\"},{\"start\":\"49637\",\"end\":\"49648\"},{\"start\":\"49648\",\"end\":\"49658\"},{\"start\":\"49658\",\"end\":\"49670\"},{\"start\":\"50125\",\"end\":\"50135\"},{\"start\":\"50135\",\"end\":\"50146\"},{\"start\":\"50146\",\"end\":\"50160\"},{\"start\":\"50160\",\"end\":\"50172\"},{\"start\":\"50711\",\"end\":\"50722\"},{\"start\":\"50722\",\"end\":\"50732\"},{\"start\":\"50732\",\"end\":\"50742\"},{\"start\":\"51227\",\"end\":\"51235\"},{\"start\":\"51464\",\"end\":\"51473\"},{\"start\":\"51658\",\"end\":\"51664\"},{\"start\":\"51664\",\"end\":\"51672\"},{\"start\":\"51672\",\"end\":\"51680\"},{\"start\":\"51680\",\"end\":\"51686\"},{\"start\":\"51686\",\"end\":\"51695\"},{\"start\":\"51695\",\"end\":\"51705\"},{\"start\":\"51705\",\"end\":\"51713\"},{\"start\":\"51713\",\"end\":\"51722\"},{\"start\":\"52119\",\"end\":\"52131\"},{\"start\":\"52131\",\"end\":\"52142\"},{\"start\":\"52142\",\"end\":\"52155\"},{\"start\":\"52155\",\"end\":\"52165\"},{\"start\":\"52165\",\"end\":\"52171\"},{\"start\":\"52171\",\"end\":\"52178\"},{\"start\":\"52178\",\"end\":\"52185\"},{\"start\":\"52185\",\"end\":\"52196\"},{\"start\":\"52655\",\"end\":\"52664\"},{\"start\":\"52869\",\"end\":\"52879\"},{\"start\":\"52879\",\"end\":\"52887\"},{\"start\":\"52887\",\"end\":\"52894\"},{\"start\":\"52894\",\"end\":\"52901\"},{\"start\":\"52901\",\"end\":\"52908\"},{\"start\":\"52908\",\"end\":\"52915\"},{\"start\":\"52915\",\"end\":\"52923\"},{\"start\":\"52923\",\"end\":\"52930\"},{\"start\":\"52930\",\"end\":\"52938\"},{\"start\":\"52938\",\"end\":\"52944\"},{\"start\":\"53461\",\"end\":\"53469\"},{\"start\":\"53469\",\"end\":\"53478\"},{\"start\":\"53478\",\"end\":\"53488\"},{\"start\":\"53630\",\"end\":\"53644\"},{\"start\":\"53644\",\"end\":\"53656\"},{\"start\":\"53656\",\"end\":\"53668\"},{\"start\":\"53668\",\"end\":\"53682\"},{\"start\":\"53682\",\"end\":\"53690\"},{\"start\":\"53690\",\"end\":\"53703\"},{\"start\":\"54081\",\"end\":\"54092\"},{\"start\":\"54092\",\"end\":\"54100\"},{\"start\":\"54100\",\"end\":\"54109\"},{\"start\":\"54109\",\"end\":\"54118\"},{\"start\":\"54118\",\"end\":\"54135\"},{\"start\":\"54644\",\"end\":\"54655\"},{\"start\":\"54655\",\"end\":\"54666\"},{\"start\":\"54666\",\"end\":\"54679\"},{\"start\":\"54679\",\"end\":\"54689\"}]", "bib_venue": "[{\"start\":\"44626\",\"end\":\"44647\"},{\"start\":\"44993\",\"end\":\"45050\"},{\"start\":\"45410\",\"end\":\"45468\"},{\"start\":\"45796\",\"end\":\"45879\"},{\"start\":\"46330\",\"end\":\"46363\"},{\"start\":\"46760\",\"end\":\"46798\"},{\"start\":\"47194\",\"end\":\"47272\"},{\"start\":\"47655\",\"end\":\"47734\"},{\"start\":\"48183\",\"end\":\"48267\"},{\"start\":\"48746\",\"end\":\"48829\"},{\"start\":\"49245\",\"end\":\"49282\"},{\"start\":\"49670\",\"end\":\"49734\"},{\"start\":\"50172\",\"end\":\"50251\"},{\"start\":\"50742\",\"end\":\"50806\"},{\"start\":\"51235\",\"end\":\"51240\"},{\"start\":\"51473\",\"end\":\"51482\"},{\"start\":\"51722\",\"end\":\"51787\"},{\"start\":\"52196\",\"end\":\"52284\"},{\"start\":\"52664\",\"end\":\"52668\"},{\"start\":\"52944\",\"end\":\"53007\"},{\"start\":\"53307\",\"end\":\"53338\"},{\"start\":\"53424\",\"end\":\"53459\"},{\"start\":\"53703\",\"end\":\"53743\"},{\"start\":\"54135\",\"end\":\"54184\"},{\"start\":\"54449\",\"end\":\"54474\"},{\"start\":\"54689\",\"end\":\"54699\"},{\"start\":\"45881\",\"end\":\"45949\"},{\"start\":\"46365\",\"end\":\"46383\"},{\"start\":\"47736\",\"end\":\"47800\"},{\"start\":\"48269\",\"end\":\"48338\"},{\"start\":\"48831\",\"end\":\"48899\"},{\"start\":\"51789\",\"end\":\"51839\"},{\"start\":\"52286\",\"end\":\"52359\"}]"}}}, "year": 2023, "month": 12, "day": 17}