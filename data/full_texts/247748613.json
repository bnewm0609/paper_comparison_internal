{"id": 247748613, "updated": "2023-10-05 16:02:43.416", "metadata": {"title": "Continual Test-Time Domain Adaptation", "authors": "[{\"first\":\"Qin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Olga\",\"last\":\"Fink\",\"middle\":[]},{\"first\":\"Luc\",\"last\":\"Gool\",\"middle\":[\"Van\"]},{\"first\":\"Dengxin\",\"last\":\"Dai\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Test-time domain adaptation aims to adapt a source pre-trained model to a target domain without using any source data. Existing works mainly consider the case where the target domain is static. However, real-world machine perception systems are running in non-stationary and continually changing environments where the target domain distribution can change over time. Existing methods, which are mostly based on self-training and entropy regularization, can suffer from these non-stationary environments. Due to the distribution shift over time in the target domain, pseudo-labels become unreliable. The noisy pseudo-labels can further lead to error accumulation and catastrophic forgetting. To tackle these issues, we propose a continual test-time adaptation approach~(CoTTA) which comprises two parts. Firstly, we propose to reduce the error accumulation by using weight-averaged and augmentation-averaged predictions which are often more accurate. On the other hand, to avoid catastrophic forgetting, we propose to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration to help preserve source knowledge in the long-term. The proposed method enables the long-term adaptation for all parameters in the network. CoTTA is easy to implement and can be readily incorporated in off-the-shelf pre-trained models. We demonstrate the effectiveness of our approach on four classification tasks and a segmentation task for continual test-time adaptation, on which we outperform existing methods. Our code is available at \\url{https://qin.ee/cotta}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.13591", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/0013FGD22", "doi": "10.1109/cvpr52688.2022.00706"}}, "content": {"source": {"pdf_hash": "c10d768cb4bac495afa6b80d8ddc4f7979a17a05", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.13591v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4276e99cfa41ef31a3ecc685d4e2e65ca6fd3838", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c10d768cb4bac495afa6b80d8ddc4f7979a17a05.txt", "contents": "\nContinual Test-Time Domain Adaptation\n\n\nQin Wang \nETH Zurich\nSwitzerland\n\nOlga Fink \nETH Zurich\nSwitzerland\n\nEPFL\nSwitzerland\n\nLuc Van Gool \nETH Zurich\nSwitzerland\n\nKU Lueven\nBelgium\n\nDengxin Dai dai@vision.ee.ethz.cholga.fink@epfl.ch \nMPI for Informatics\nGermany\n\nContinual Test-Time Domain Adaptation\n\nTest-time domain adaptation aims to adapt a source pretrained model to a target domain without using any source data. Existing works mainly consider the case where the target domain is static. However, real-world machine perception systems are running in non-stationary and continually changing environments where the target domain distribution can change over time. Existing methods, which are mostly based on self-training and entropy regularization, can suffer from these non-stationary environments. Due to the distribution shift over time in the target domain, pseudo-labels become unreliable. The noisy pseudolabels can further lead to error accumulation and catastrophic forgetting. To tackle these issues, we propose a continual test-time adaptation approach (CoTTA) which comprises two parts. Firstly, we propose to reduce the error accumulation by using weight-averaged and augmentationaveraged predictions which are often more accurate. On the other hand, to avoid catastrophic forgetting, we propose to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration to help preserve source knowledge in the long-term. The proposed method enables the long-term adaptation for all parameters in the network. CoTTA is easy to implement and can be readily incorporated in off-the-shelf pre-trained models. We demonstrate the effectiveness of our approach on four classification tasks and a segmentation task for continual testtime adaptation, on which we outperform existing methods. Our code is available at https://qin.ee/cotta. clear weather conditions can suffer significant performance deterioration when tested on snowy night conditions[50]. Similarly, a pre-trained image classification model can also suffer this phenomenon when tested on corrupted images resulting from sensor degradation. Due to privacy concerns or legal constraints, the source data is generally considered unavailable during inference time under this setup, making it a more challenging but more realistic problem than unsupervised domain adaptation. In many scenarios, the adaptation also needs to be performed in an online fashion. Therefore, test-time adaptation is critical to the success of real-world machine perception applications under domain shift.Existing works on test-time adaptation often tackle the distribution shift between the source domain and a fixed target domain by updating model parameters using pseudolabels or entropy regularization[43,61]. These self-training\n\nIntroduction\n\nTest-time domain adaptation aims to adapt a source pretrained model by learning from the unlabeled test (target) data during inference time. Due to the domain shift between source training data and target test data, an adaptation is necessary to achieve good performance. For example, a semantic segmentation model trained on data from * The corresponding author Figure 1. We consider the online continual test-time adaptation scenario. The target data is provided in a sequence and from a continually changing environment. An off-the-shelf source pretrained network is used to initialize the target network. The model is updated online based on the current target data, and the predictions are given in an online fashion. The adaptation of the target network does not rely on any source data. Existing methods often suffer from error accumulation and forgetting which result in performance deterioration over time. Our method enables long-term test-time adaptation under continually changing environments. methods have been proven to be effective when the test data are drawn from the same stationary domain. However, they can be unstable [48] when the target test data originates from an environment which is continually changing. There are two aspects that contribute to this: Firstly, under the continually changing environment, the pseudo-labels become noisier and mis-calibrated [13] because of the distribution shift. Therefore, early prediction mistakes are more likely to result in error accumulation [4]. Secondly, as the model is being continually adapted to new distributions for a long time, knowledge from the source domain is harder to preserve, leading to catastrophic forgetting [11,41,45].\n\nAiming to tackle these problems under the continually changing environment, this work focuses on the practical problem of online continual test-time adaptation. As shown in Figure 1, the goal is to start from an off-the-shelf source pre-trained model, and continually adapt it to the current test data. Under this setup, we assume that the target test data is streamed from a continually changing environment. The prediction and updates are performed online, meaning that the model will only have access to the current stream of data without having access to the full test data nor any source data. The proposed setup is very relevant for realworld machine perception systems. For example, surrounding environments are continually changing for autonomous driving systems (e.g. weather change from sunny to cloudy then to rainy). They can even change abruptly (e.g. when a car exits a tunnel and the camera gets suddenly overexposed). A perception model need to adapt itself and make decisions online under these non-stationary domain shifts.\n\nTo effectively adapt the pre-trained source model to the continually changing test data, we propose a continual test-time adaptation approach (CoTTA) which tackles the two main limitations of existing methods. The first component of the proposed method aims to alleviate error accumulation. We propose to improve the pseudo-label quality under the self-training framework in two different ways. On the one hand, motivated by the fact that the mean teacher predictions often have a higher quality than the standard model [55], we use a weight-averaged teacher model to provide more accurate predictions. On the other hand, for test data which suffers larger domain gap, we use the augmentation-averaged predictions to further boost the quality of pseudo-labels. The second component of the proposed method aims to help preserve the source knowledge and avoid forgetting. We propose to stochastically restore a small part of neurons in the network back to the pre-trained source model. By reducing error accumulation and preserving knowledge, CoTTA enables long-term adaptation in a continuously changing environment, and makes it possible to train all parameters of the network. In contrast, previous methods [43,61] can only train batchnorm parameters.\n\nIt is worth pointing out that our approach can be eas-ily implemented. The weight-and-augmentation-averaged strategy and the stochastic restoration can be readily incorporated into any off-the-shelf pre-trained model without the need to re-train it on source data. We demonstrate the effectiveness of our proposed approach on four classification tasks and a segmentation task for continual test-time adaptation, on which we significantly improve performance over existing methods. Our contributions are summarized blow:\n\n\u2022 We propose a continual test-time adaptation approach which can effectively adapt off-the-shelf source pretrained models to continually changing target data.\n\n\u2022 Specifically, we reduce the error accumulation by using weight-averaged and augmentation-averaged pseudo-labels that are more accurate.\n\n\u2022 The long-term forgetting effect is alleviated by explicitly preserving the knowledge from the source model.\n\n\u2022 The proposed approach significantly improves the continual test-time adaptation performance on both classification and segmentation benchmarks.\n\n\nRelated Work\n\n\nDomain Adaptation\n\nUnsupervised domain adaptation (UDA) [44,46] aims to improve the target model performance in the presence of a domain shift between the labeled source domain and unlabeled target domain. During training, UDA methods often align the feature distributions between the two domains using discrepancy losses [39] or adversarial training [12,58]. Alternatively, the alignment can also be done in the input space [18,67]. In recent years, self-training has also shown promising results by iteratively using gradually-improving target pseudo-labels to train the network [19,36,62,75].\n\n\nTest-time Adaptation\n\nTest-time adaptation is also referred to as source-free domain adaptation in some references [28,66]. Unlike domain adaptation which requires access to both source and target data for adaptation, test-time adaptation methods do not require any data from the source domain for adaptation. Some existing works [29,33,68] utilize generative models to support the feature alignment in absence of source data.\n\nAnother popular direction is to finetune the source model without explicitly conducting domain alignment. Test entropy minimization (TENT) [61] takes a pre-trained model and adapts to the test data by updating the trainable parameters in Batchnorm layers using entropy minimization. Source hypothesis transfer (SHOT) [37] utilizes both entropy minimization and a diversity regularizer for the adaptation. SHOT requires using source data to train a specialized source model using the label-smoothing technique with the weight normalization layer. Thus, it cannot support the use an arbitrary pre-trained model. [43] proposes to apply a diversity regularizer combined with an input transformation module to further improve the performance. [23] uses a separate normalization convolutional network to normalize test images from new domains. [22] only updates the final classification layer during inference time using pseudo-prototypes. [74] analyzes the problem in a Bayesian perspective and proposes a regularized entropy minimization procedure at test-time adaptation, which requires approximating density during training time. Updating the statistics in the Batch Normalization layer using the target data is a different path which also shows promising results [21,34,70]. While most existing works focus on image classification, [20,27,38] extend test-time adaptation to semantic segmentation. Standard test-time adaptation considers the offline scenario where access to the full set of test data is provided for the training. This is often unrealistic for online machine perception applications. Most existing works (except TENT variants [60]) also require the retraining of the source model to support the test-time adaptation. Therefore, they cannot directly use off-the-shelf pretrained model from the source domain.\n\n\nContinuous Domain Adaptation\n\nUnlike standard domain adaptation which assumes a specific target domain, continuous domain adaptation considers the adaptation problem with continually changing target data. Continuous Manifold Adaptation (CMA) [17] is an early work which considers adaptation to evolving domains. Incremental adversarial domain adaptation (IADA) [63] adapts to continually changing domains by adversarially aligning source and target features. [59] aims to continually adapt the unseen visual domain while alleviate the forgetting on the seen domain without retaining the source training data. [3] aims to adapt to gradually changing domains by making use of the assumption of continuity between gradually varying domains. Existing continuous domain adaptation methods need to have access to data from both the source and target domains in order to align the distributions.\n\nThe main focus of this paper is continual test-time adaptation, which additionally considers the adaptation at testtime without accessing the source data. While this is a realistic scenario for machine perception systems in the real world, there are very limited number of approaches which are applicable to such scenarios. In theory, the online ver-sion of TENT [61] could adapt under this setup by continually updating the BN parameters using the entropy loss. However, it can suffer from error accumulation because of mis-calibrated predictions. Test-time training (TTT) [54] could also continually update the feature extractor using supervision from the rotation prediction auxiliary task. However, it requires re-training of the source model using the source data to learn the auxiliary task. Therefore, it cannot be considered as source-free for the full pipeline and does not support off-the-shelf source pre-trained models.\n\n\nContinual Learning\n\nContinual learning [10] and lifelong learning [45] are closely related to the continuous adaptation problems as a potential cure to the catastrophic forgetting. Continual learning methods can often be categorized into replaybased [49] and regularization-based [53,72] methods. The latter can further be divided into data-focused methods, such as learning without forgetting (LwF) [35], and prior-focused methods, such as elastic weight consolidation (EWC) [24]. Ideas from continual learning are adopted for continuous domain adaptation approaches [3,30].\n\n\nDomain Generalization\n\nThis work is also related to domain generalization [42] in a broad sense, because of the shared goal of improving performance on potentially changing target domains. A number of works have also shown that data augmentation [52] during training [14,16,32,69] and during testing [1,40,73] can improve model robustness and generalizability. Domain randomization is one of the most popular methods which improves the model generalizability by learning from different synthesis parameters of simulation environments [56,57]. Unlike domain generalization methods which mostly aim to train a more generalizable neural network from the source domain, this work focuses on improving the performance of existing pre-trained neural networks during test-time by using the unlabeled online data from the continually changing target domain.\n\n\nContinual Test-Time Domain Adaptation\n\n\nProblem Definition\n\nGiven an existing pre-trained model f \u03b80 (x) with parameters \u03b8 trained on the source data (X S , Y S ), we aim at improving the performance of this existing model during in- Figure 2. An overview of the proposed continual test-time adaptation (CoTTA) approach. CoTTA adapts from an off-the-shelf source pre-trained network. Error accumulation is mitigated by using a teacher model to provide weight-averaged pseudo-labels and using multiple augmentations to average the predictions. Knowledge from the source data is preserved by stochastically restoring a small number of elements of trainable weights. ference time for a continually changing target domain in an online fashion without having access to any source data. Unlabeled target domain data X T is provided sequentially and the model only have access to the data of the current time step. At time step t, target data x T t is provided as input and the model f \u03b8t needs to make the prediction f \u03b8t (x T t ) and adapts itself accordingly for future inputs \u03b8 t \u2212 \u2192 \u03b8 t+1 . The data distribution of x T t is continually changing. The model is evaluated based on the online predictions.\n\nThis setup is largely motivated by the need of machine perception applications in continually changing environments. For example, the surrounding environment is continually changing for autonomous driving cars because of location, weather, and time. Perception decisions need to be made online and models need to be adapted.\n\nWe list the main differences between our online continual test-time adaptation setup with existing adaptation setups in Table 1. Compared to previous setups which focus on a fixed target domain, we consider the long-term adaptation on continually changing target environments.\n\n\nMethodology\n\nWe propose an adaptation method for the online continual test-time adaptation setup. The proposed method takes an off-the-shelf source pre-trained model and adapts it to the continually changing target data in an online fashion. Motivated by the fact that error accumulation is one of the key bottlenecks in the self-training framework, we propose to use weight-and-augmentation-averaged pseudo-labels to reduce error accumulation. In addition, to help reduce forgetting in continual adaptation, we propose to explicitly preserve information from the source model. An overview of the proposed method is presented in Figure 2.\n\nSource Model Existing works on test-time adaptation often require special treatment in the training process of the source model to improve domain generalization ability and to facilitate the adaptation. For example, during source training, TTT [54] has an additional auxiliary rotation prediction branch to train to facilitate the target adaptation supervision. This requires a retraining on the source data, and makes it impossible to reuse existing pre-trained models. In our proposed test-time adaptation method, we lift this burden and do not require a modification of the architecture or an additional source training process. Therefore, any existing pre-trained models can be used without retraining on the source. We will show in the experiments that our method can work on a wide range of pre-trained networks including ResNet variants and Transformer-based architectures.\n\nWeight-Averaged Pseudo-Labels Given target data x T t and the model f \u03b8t , the common test-time objective under the self-training framework is to minimize the cross-entropy consistency between the prediction\u0177 T t = f \u03b8t (x T t ) and a pseudo-label. For example, directly using the model prediction itself as the pseudo-label leads to the training objective of TENT [61] (i.e. entropy minimization). While this works for a stationary target domain, the quality of pseudo-labels can drop significantly for continually changing target data because of the distribution shift.\n\nMotivated by the observation that weight-averaged models over training steps often provide a more accurate model than the final model [47,55], we use a weight-averaged teacher model f \u03b8 to generate the pseudo-labels. At timestep t = 0, the teacher network is initialized to be the same as the source pre-trained network. At time-step t, the pseudo-label is first generated by the teacher\u0177\nT t = f \u03b8 t (x T t )\n. The student f \u03b8t is then updated by the crossentropy loss between the student and teacher predictions:\nL \u03b8t (x T t ) = \u2212 c\u0177 T tc log\u0177 T tc ,(1)\nwhere\u0177 T tc is the probability of class c in the teacher model's soft pseudo-label prediction, and\u0177 T tc is the prediction from the main model (student). The loss enforces a consistency between the teacher and student predictions.\n\nAfter the update of the student model \u03b8 t \u2212 \u2192 \u03b8 t+1 using Equation 1, we update the weights of the teacher model by exponential moving average using the student weights:\n\u03b8 t+1 = \u03b1\u03b8 t + (1 \u2212 \u03b1)\u03b8 t+1 ,(2)\nwhere \u03b1 is a smoothing factor. Our final prediction for the input data x T t is the class with the highest probability in\u0177 T t . The benefits of the weight-averaged consistency are twofold. On the one hand, by using the often more accurate [47] weight-averaged prediction as the pseudo-label target, our model suffers less from the error accumulation during the continual adaptation. On the other hand, the mean teacher prediction\u0177 T t encodes the information from models in past iterations and is, therefore, less likely to suffer from catastrophic forgetting in long-term continual adaptation and improve the generalization capability to new unseen domains. This is inspired by the mean teacher method proposed in [55] in semi-supervised learning.\n\nAugmentation-Averaged Pseudo-Labels Data augmentation during training time [52] has been widely applied to improve model performance. Different augmentation policies are often manually designed [26] or searched [9] for different datasets. While test-time augmentation has also been proven to be able to improve robustness [5,54], the augmentation policies are generally determined and fixed for a specific dataset without considering the distribution change during inference time. Under a continually changing environment, test distributions can change dramatically, which may make the augmentation policy invalid. Here, we take the test-time domain shift into account and approximate the domain difference by prediction confidence. The augmentation is only applied when the domain difference is large, to reduce error accumulation.\ny T t = 1 N N \u22121 i=0 f \u03b8 t (aug i (x T t )),(3)y T t = \u0177 T t , if conf(f \u03b80 (x T t )) \u2265 p th y T t , otherwise,(4)\nwhere\u1ef9 T t is the augmentation-averaged prediction from the teacher model,\u0177 T t is the direct prediction from the teacher model, conf(f \u03b80 (x T t )) is the source pre-trained model's prediction confidence on the current input x T t , and p th is a confidence threshold. By calculating the prediction confidence on the current input x T t using the pre-trained model f \u03b80 in Equation 4, we attempt to approximate the domain difference between the source and the current domain. We hypothesize that a lower confidence indicates a larger domain gap and a relatively high confidence level indicates a smaller domain gap. Therefore, when the confidence is high and larger than the threshold, we directly use\u0177 T t as our pseudo-label without using any augmentation. When the confidence is low, we apply additionally N random augmentations to further improve the pseudo-label quality. The filtering is critical as we observe that random augmentations\n\n\nAlgorithm 1 The proposed continual test-time adaptation\n\nInitialization: A source pre-trained model f \u03b80 (x), teacher model f \u03b8 0 (x) initialized from f \u03b80 (x). Input: For each time step t, current stream of data x t . 1: Augment x t and get weight and augmentation-averaged pseudo-labels from the teacher f \u03b8 t by Equation 4. 2: Update student f \u03b8t by consistency loss in Equation 5. 3: Update teacher f \u03b8 t by moving average in Equation 2. 4: Stochastically restore student f \u03b8t by Equation 8.\nOutput: Prediction f \u03b8 t (x t ); Updated student model f \u03b8t+1 (x); Updated teacher model f \u03b8 t+1 (x).\non confident samples with small domain gaps can sometimes decrease model performance. We provide detailed discussion on this observation in the supplementary. In summary, we use the confidence to approximate the domain difference and determine when to apply the augmentations.\n\nThe student is updated by the refined pseudo-label:\nL \u03b8t (x T t ) = \u2212 c y T tc log\u0177 T tc ,(5)\nStochastic Restoration While more accurate pseudolabels can mitigate error accumulation, continual adaptation by self-training for a long time inevitably introduces errors and leads to forgetting. This issue can be especially relevant if we encounter strong domain shifts within a sequence of data, because the strong distribution shift leads to mis-calibrated and even wrong predictions. Self-training in this case may only lead to reinforcing wrong predictions. What's worse is that after encountering hard examples, the model may not be able to recover because of the continual adaptation, even when the new data are not severely shifted.\n\nTo further tackle the problem of catastrophic forgetting, we propose a stochastic restoration method which explicitly restores the knowledge from the source pre-trained model.\n\nConsider a convolution layer within the student model f \u03b8 after gradient update based on Equation 1 at time step t:\nx l+1 = W t+1 * x l ,(6)\nwhere * denotes the convolution operation, x l and x l+1 denote the input and output to this layer, W t+1 denotes the trainable convolution filters. The proposed stochastic restoration method additionally updates the weight W by:\nM \u223c Bernoulli(p),(7)W t+1 = M W 0 + (1 \u2212 M ) W t+1 ,(8)\nwhere denotes the element-wise multiplication. p is a small restore probability, and M is a mask tensor of the same shape as W t+1 . The mask tensor decides which element within W t+1 to restore back to the source weight W 0 .\n\nThe stochastic restoration can also be seen as a special form of Dropout. By stochastically restoring a small number of tensor elements in the trainable weights to the initial weight, the network avoids drifting too far away from the initial source model and therefore, avoids catastrophic forgetting. In addition, by preserving the information from the source model, we are able to train all trainable parameters without suffering from model collapse. This brings more capacity for the adaptation and is another major difference compared to entropy minimization methods [43,61] which only train the BN parameters for test-time adaptation.\n\nAs shown in Algorithm 1, combining the refined pseudolabels with stochastic restoration leads to our online continual test-time adaptation (CoTTA) method.\n\n\nExperiments\n\nWe evaluate our proposed method on five continual testtime adaptation benchmark tasks: CIFAR10-to-CIFAR10C (standard and gradual), CIFAR100-to-CIFAR100C, and ImageNet-to-ImageNet-C for image classification, as well as Cityscapses-to-ACDC for semantic segmentation.\n\n\nDatasets and tasks\n\nCIFAR10C, CIFAR100C, and ImageNet-C were originally created to benchmark robustness of classification networks [15]. Each dataset contains 15 types of corruptions with 5 levels of severity. The corruptions were applied on images from the test set of the clean CIFAR10 or CIFAR100 dataset [25]. There are 10,000 images for each corruption type for both CIFAR10C and CIFAR100C datasets.\n\nFor our online continual test-time adaptation task, a network pre-trained on the clean training set of CIFAR10 or CIFAR100 dataset is used. During test time, the corrupted images are provided in an online fashion to the network. Unlike previous methods which evaluate the test-time adaptation performance from the clean images pre-trained model to each corruption type individually, we continually adapt the source pre-trained model to each corruption type sequentially. We evaluate all models under the largest corruption severity level 5. The evaluation is based on the online prediction results immediately after the encounter of the data. Both the CIFAR10 and CIFAR100 experiments follow this online continual test-time adaptation scheme.\n\nFor CIFAR10-to-CIFAR10C, we follow the official public implementation from TENT [61] for the CIFAR10 experiments. The same pre-trained model is adopted, which is a WideResNet-28 [71] model from the RobustBench benchmark [8]. We update the model for one step at each iteration (i.e. one gradient step per test point). We use the same Adam optimizer with a learning rate of 1e-3 as the official implementation. Following [5], we use the same random augmentation composition including color jitter, random affine, gaussian blur, random horizonal flip, and gaus-sian noise. We use 32 augmentations for our experiments. We discuss the choice of the augmentation threshold p th in our supplementary material. Unlike TENT models which only update the BN scale and shift weights, we update all trainable parameters in the experiments. We use a restoration probability of p = 0.01 for all our experiments.\n\nFor CIFAR100-to-CIFAR100C experiments, we adopt the pre-trained ResNeXt-29 [65] model from [16], which is used as one of the default architectures for CIFAR100 in the RobustBench benchmark [8]. The same hyperparameters are used as in the CIFAR10 experiments. The ImageNet-to-ImageNet-C [15] experiments use the standard pre-trained resnet50 model in RobustBench [8]. ImageNet-C experiments are evaluated under ten diverse corruption orders.\n\nCityscapes-to-ACDC is a continual semantic segmentation task we designed to mimic continual distribution shifts in the real world. The source model is an offthe-shelf pre-trained segmentation model trained on the Cityscapes dataset [7]. The target domain contains images from various scenarios from the Adverse Conditions Dataset (ACDC) [50]. The ACDC dataset shares the same semantic classes with Cityscapes and is collected in four different adverse visual conditions: Fog, Night, Rain, and Snow. We evaluate our continual test-time adaptation following the same default order. We use 400 unlabeled images from each adverse condition for the adaptation. To mimic the scenario in real life where similar environments might be revisited, and to evaluate the forgetting effect of our methods, we repeat the same sequence group (of the four conditions) 10 times (i.e. in total 40: Fog\u2212 \u2192Night\u2212 \u2192Rain\u2212 \u2192Snow\u2212 \u2192Fog...). This also provides an evaluation of the adaptation performance in the long term.\n\nFor the implementation details, we adopt a transformerbased architecture, Segformer [64], for our Cityscapse-to-ACDC experiments. We use the publicly-available pretrained Segformer-B5 trained on Cityscapes as our off-theshelf source model. For the baseline comparison method, T EN T optimizes the parameters in the normalization layers. For the proposed CoTTA model, all trainable layers are updated without the need to choose specific layers. Images from ACDC have a resolution of 1920x1080. We use down-sampled resolutions of 960x540 as inputs to the network and the predictions are evaluated under the original resolution. Adam optimizer is used with the learning rate 8 times smaller than the default one for Segformer, because we use batch size 1 instead of 8 (default for source training) in our online continual test-time adaptation experiments. We use the multi-scaling input with flip as the augmentation method for the proposed method to generate augmentationweighted pseudo-label (as in Equation 3). Following the default practice designed for Cityscapes in MMSeg [6], we use the scale factors of [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]. Table 2. Classification error rate (%) for the standard CIFAR10-to-CIFAR10C online continual test-time adaptation task. Tesults are evaluated on WideResNet-28 with the largest corruption severity level 5. * denotes the requirement on additional domain information.  Table 3. Gradually changing CIFAR10-to-CIFAR10C results. The severity level changes gradually between the lowest and the highest. The corruption type changes when the severity is the lowest. Results are the mean over ten diverse corruption type sequences. \n\n\nExperiments on CIFAR10-to-CIFAR10C\n\nWe first evaluate the effectiveness of the proposed model on the CIFAR10-to-CIFAR10C task. We compare our method to the source-only baseline and four popular methods. As shown in Table 2, directly using the pre-trained model without adaptation yields a high average error rate of 43.5%, indicating that an adaptation is necessary. The BN Stats Adapt method keeps the network weights and uses the Batch Normalization statistics from the input data of the current iteration for the prediction [34,51]. The approach is simple and fully online, and significantly improves the performance over the source-only baseline. Using hard pseudo-labels [31] to update the BN-trainable parameters can reduce the error rate to 19.8%. If the TENT-online [61] method has access to the additional domain information and resets itself to the initial pre-trained model whenever it encounters a new domain, the performance can be further improved to 18.6%. However, such information is usually unavailable in real applications. Without having access to this additional information, the TENT-continual method does not yield any improvement over the BN Stats Adapt method. It is worth mentioning that in earlier stages of the adaptation, TENT-continual outperforms the BN Stats Adapt. However, the model quickly deteriorates after observing three types of corruptions. This indicates that TENT based methods can be unstable under continual adaptation in the long term because of error accumulation. Our proposed method can continuously outperform all the above methods by using the weight-and-augmentation-averaged consistency. The error rate is significantly reduced to 16.2%. In addition, it does not suffer from performance deterioration in the long term because of our stochastic restore approach. Ablation study: individual components The main contribution of our proposed method is to reduce the error ac-cumulation by using averaged pseudo-labels and random restoration. To validate our motivation, we conduct an ablation study on each of the elements of the proposed approach. As listed in Table 2, by using the weight-averaged pseudo-labels from the teacher model, the error rate is reduced from 20.7% to 18.3%. This indicates that the weightaveraged predictions are indeed more accurate than the direct predictions. By using multiple augmentations to further refine the weight-averaged predictions, we are able to further improve the performance to 17.4%. However, the performance is still deteriorating over time (e.g. comparing to TENT-online* for contrast), indicating that even though the pseudo-labels are more accurate, error can still accumulate because of the inevitable wrong predictions. Finally, by using stochastic restoration to explicitly preserve the source knowledge, the long-term predictions can be largely improved. This leads to an improved error rate of 16.2%. The number in bracket is the standard deviation over 5 seeds. Gradually changing setup.\n\nIn the above standard setup, corruption types change abruptly in the highest severity, we now report the results for the gradual setup.\n\nWe design the sequence by gradually changing severity for the 15 corruption types: where the severity level is the lowest (1) when corruption type changes, therefore, the type change is gradual. The distribution shift within each type is also gradual. We create 10 randomly shuffled orders for the corruption types t and then evaluate the methods using the average error rate over the ten diverse sequences. Table 3 shows that the proposed method outperforms competing methods, leading to an error rate of 10.4%, compared to TENT's 30.7%.\n\n\nExperiments on CIFAR100-to-CIFAR100C\n\nTo further demonstrate the effectiveness of the proposed method, we evaluate it on the more difficult CIFAR100to-CIFAR100C task. The experimental results are summarized in Table 4. We compare our method with the sourceonly baseline, BN stats adapt, Pseudo-label, as well as the TENT-continual method. We observe that performance of Table 4. Classification error rate (%) for the standard CIFAR100-to-CIFAR100C online continual test-time adaptation task. All results are evaluated on the ResNeXt-29 architecture with the largest corruption severity level 5.    Table 6. Average error of standard ImageNet-to-ImageNet-C experiments over 10 diverse corruption sequences (severity level 5).\n\nAvg. Error (%) Source BN Adapt Test Aug [5] TENT [58] CoTTA ImageNet-C 82. the TENT-continual model deteriorates rapidly over time on the later corruption types because of the error accumulation and forgetting. Our method yields an absolute improvement of 2.9% error rate over BN stats adapt, and achieves 32.5%. More importantly, the improvement becomes larger over time, this indicates that the proposed method is able to learn from the unlabeled test images from the past streams to further improve the performance on the current test data.\n\n\nExperiments on ImageNet-to-ImageNet-C\n\nTo provide a more comprehensive evaluation on the proposed method, ImageNet-to-ImageNet-C experiments are conducted over ten diverse corruption type sequences in severity level of 5. As shown in Table 6, CoTTA is able to continually outperform TENT and other competing methods. The number after \u00b1 is the standard deviation over 10 diverse corruption type sequences.\n\n\nExperiments on Cityscapes-to-ACDC\n\nWe additionally evaluate our method on the more complex continual test-time semantic segmentation Cityscapesto-ACDC task. The experimental results are summarized in Table 5. The results demonstrate that our method is also effective for semantic segmentation tasks and is robust to the different choices of architectures. Our proposed method yields an absolute improvement of 1.9% mIoU over the baseline, and achieves 58.6% mIoU. It is worth mention-ing that BN Stats Adapt and TENT do not perform well in this task and the performance deteriorates significantly over time. This is partly because both were specifically designed for networks with Batch Normalization layers, while there is only one Batch Normalization layer in Segformer and the majority of normalization layers in transformer models are based on LayerNorm [2]. Our method, however, does not rely on specific layers and can still be effective for this more complex task on a very different architecture. The improved performance is also largely maintained after being continually adapted for a relatively long term.\n\n\nConclusion\n\nIn this work, we focused on the continual test-time adaptation in non-stationary environments where the target domain distribution can continually change over time. To tackle the error accumulation and catastrophic forgetting in this setup, we proposed a novel method CoTTA which comprises two parts. Firstly, we reduced the error accumulation by using weight-averaged and augmentation-averaged predictions which are often more accurate. Secondly, to preserve the knowledge from the source model, we stochastically restored a small part of the weights to the source pre-trained weights. The proposed method can be incorporated in off-the-shelf pre-trained models without requiring any access to source data. The effectiveness of CoTTA was validated on four classification and one segmentation tasks.\n\nTable 1 .\n1The difference between our proposed continual test-time adaptation and related adaptation settings.Data \nLearning \n\n\n\nTable 5. Semantic segmentation results (mIoU in %) on the Cityscapes-to-ACDC online continual test-time adaptation task. We evaluate the four test conditions continually for ten times to evaluate the long-term adaptation performance. To save space, we only show the continual adaptation results in the first, fourth, seventh, and last round. Full results can be found in the supplementary material. All results are evaluated based on the Segformer-B5 architecture.3 54.1 30.8 28.8 39.5 45.8 50.3 29.5 55.1 37.2 74.7 41.2 \n46.4 \nBN Stats Adapt \n42.1 40.7 42.7 27.6 41.9 29.7 27.9 34.9 35.0 41.5 26.5 30.3 35.7 32.9 41.2 \n35.4 \nPseudo-label \n38.1 36.1 40.7 33.2 45.9 38.3 36.4 44.0 45.6 52.8 45.2 53.5 60.1 58.1 64.5 \n46.2 \nTENT-continual [61] 37.2 35.8 41.7 37.9 51.2 48.3 48.5 58.4 63.7 71.1 70.4 82.3 88.0 88.5 90.4 \n60.9 \nCoTTA (Proposed) \n40.1 37.7 39.7 26.9 38.0 27.9 26.4 32.8 31.8 40.3 24.7 26.9 32.5 28.3 33.5 \n32.5 \n\n\n\nPitfalls of in-domain uncertainty estimation and ensembling in deep learning. Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, Dmitry Vetrov, ICLR. 3Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty esti- mation and ensembling in deep learning. ICLR, 2020. 3\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hin, arXiv:1607.06450ton. Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 8\n\nAdapting to continuously shifting domains. Andreea Bobu, Eric Tzeng, Judy Hoffman, Trevor Darrell, ICLR Workshops. Andreea Bobu, Eric Tzeng, Judy Hoffman, and Trevor Dar- rell. Adapting to continuously shifting domains. In ICLR Workshops, 2018. 3\n\nProgressive feature alignment for unsupervised domain adaptation. Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, Junzhou Huang, CVPR. Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for unsupervised do- main adaptation. In CVPR, pages 627-636, 2019. 2\n\nGilad Cohen, Raja Giryes, Katana, arXiv:2109.08191Simple post-training robustness using test time augmentations. 56arXiv preprintGilad Cohen and Raja Giryes. Katana: Simple post-training robustness using test time augmentations. arXiv preprint arXiv:2109.08191, 2021. 5, 6\n\nMMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https : / / github . com / open - mmlab/mmsegmentation, 2020. 6\n\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, CVPR. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 6\n\nRobustbench: a standardized adversarial robustness benchmark. Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. NeuIPS Datasets and Benchmarks TrackFrancesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In NeuIPS Datasets and Benchmarks Track, 2021. 6\n\nAutoaugment: Learning augmentation strategies from data. Barret Ekin D Cubuk, Dandelion Zoph, Vijay Mane, Quoc V Vasudevan, Le, CVPR. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In CVPR, pages 113-123, 2019. 5\n\nAles Leonardis, Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, 2021. 3Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. T-PAMI, 2021. 3\n\nAdversarial continual learning. Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, Marcus Rohrbach, ECCV. SpringerSayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach. Adversarial continual learn- ing. In ECCV. Springer, 2020. 2\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, ICML. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, pages 1180-1189, 2015. 2\n\nOn calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, PMLRICML. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, pages 1321-1330. PMLR, 2017. 2\n\nThe many faces of robustness: A critical analysis of out-of-distribution generalization. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, ICCV. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada- vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robust- ness: A critical analysis of out-of-distribution generalization. In ICCV, pages 8340-8349, 2021. 3\n\nBenchmarking neural network robustness to common corruptions and perturbations. ICLR. Dan Hendrycks, Thomas Dietterich, Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. ICLR, 2019. 6\n\nAugMix: A simple data processing method to improve robustness and uncertainty. ICLR. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan, 36Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. ICLR, 2020. 3, 6\n\nContinuous manifold based adaptation for evolving visual domains. Judy Hoffman, Trevor Darrell, Kate Saenko, CVPR. Judy Hoffman, Trevor Darrell, and Kate Saenko. Continuous manifold based adaptation for evolving visual domains. In CVPR, pages 867-874, 2014. 3\n\nCycada: Cycle-consistent adversarial domain adaptation. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, Trevor Darrell, PMLRICML. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, pages 1989-1998. PMLR, 2018. 2\n\nDaformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation. Lukas Hoyer, Dengxin Dai, Luc Van Gool, arXiv:2111.14887arXiv preprintLukas Hoyer, Dengxin Dai, and Luc Van Gool. Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation. arXiv preprint arXiv:2111.14887, 2021. 2\n\nFully test-time adaptation for image segmentation. Minhao Hu, Tao Song, Yujun Gu, Xiangde Luo, Jieneng Chen, Yinan Chen, Ya Zhang, Shaoting Zhang, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerMinhao Hu, Tao Song, Yujun Gu, Xiangde Luo, Jieneng Chen, Yinan Chen, Ya Zhang, and Shaoting Zhang. Fully test-time adaptation for image segmentation. In Inter- national Conference on Medical Image Computing and Computer-Assisted Intervention, pages 251-260. Springer, 2021. 3\n\nXuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, Ser-Nam Lim, arXiv:2110.11478Mixnorm: Test-time adaptation through online normalization estimation. arXiv preprintXuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estima- tion. arXiv preprint arXiv:2110.11478, 2021. 3\n\nTest-time classifier adjustment module for model-agnostic domain generalization. Yusuke Iwasawa, Yutaka Matsuo, NeuIPS. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeuIPS, 2021. 3\n\nTest-time adaptable neural networks for robust medical image segmentation. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, Ender Konukoglu, Medical Image Analysis. 683101907Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and En- der Konukoglu. Test-time adaptable neural networks for ro- bust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 3\n\nOvercoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Proceedings of the national academy of sciences. the national academy of sciences114James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, et al. Overcoming catastrophic forgetting in neu- ral networks. Proceedings of the national academy of sci- ences, 114(13):3521-3526, 2017. 3\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Technical reportCiteseerAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Cite- seer, 2009. 6\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NeuIPS. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In NeuIPS, pages 1097-1105, 2012. 5\n\nGeneralize then adapt: Source-free domain adaptive semantic segmentation. Jogendra Nath Kundu, Akshay Kulkarni, Amit Singh, Varun Jampani, R Venkatesh Babu, ICCV. Jogendra Nath Kundu, Akshay Kulkarni, Amit Singh, Varun Jampani, and R Venkatesh Babu. Generalize then adapt: Source-free domain adaptive semantic segmentation. In ICCV, pages 7046-7056, 2021. 3\n\nUniversal source-free domain adaptation. Jogendra Nath Kundu, Naveen Venkat, Venkatesh Babu, CVPR. 2020Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pages 4544-4553, 2020. 2\n\nDomain impression: A source data free domain adaptation method. Vinod K Kurmi, K Venkatesh, Subramanian, P Vinay, Namboodiri, WACV. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615-625, 2021. 2\n\nContinuous domain adaptation with variational domain-agnostic feature replay. Qicheng Lao, Xiang Jiang, Mohammad Havaei, Yoshua Bengio, arXiv:2003.04382arXiv preprintQicheng Lao, Xiang Jiang, Mohammad Havaei, and Yoshua Bengio. Continuous domain adaptation with vari- ational domain-agnostic feature replay. arXiv preprint arXiv:2003.04382, 2020. 3\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on Challenges in Representation Learning, ICML. 3Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, volume 3, page 2, 2013. 7\n\nOn feature normalization and data augmentation. Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, Kilian Q Weinberger, CVPR. Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, and Kil- ian Q Weinberger. On feature normalization and data aug- mentation. In CVPR, pages 12383-12392, 2021. 3\n\nModel adaptation: Unsupervised domain adaptation without source data. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San, Si Wong, Wu, CVPR. 2020Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, pages 9641-9650, 2020. 2\n\nRevisiting batch normalization for practical domain adaptation. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, Xiaodi Hou, arXiv:1603.0477937arXiv preprintYanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation. arXiv preprint arXiv:1603.04779, 2016. 3, 7\n\nZhizhong Li, Derek Hoiem, Learning without forgetting. T-PAMI. 40Zhizhong Li and Derek Hoiem. Learning without forgetting. T-PAMI, 40(12):2935-2947, 2017. 3\n\nConstructing self-motivated pyramid curriculums for crossdomain semantic segmentation: A non-adversarial approach. Qing Lian, Fengmao Lv, ICCV. Lixin Duan, and Boqing GongQing Lian, Fengmao Lv, Lixin Duan, and Boqing Gong. Constructing self-motivated pyramid curriculums for cross- domain semantic segmentation: A non-adversarial approach. In ICCV, October 2019. 2\n\nDo we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. Jian Liang, Dapeng Hu, Jiashi Feng, PMLR, 2020. 2ICLR. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICLR, pages 6028-6039. PMLR, 2020. 2\n\nSource-free domain adaptation for semantic segmentation. Yuang Liu, Wei Zhang, Jun Wang, CVPR. Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215-1224, 2021. 3\n\nLearning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael Jordan, ICML. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor- dan. Learning transferable features with deep adaptation net- works. In ICML, pages 97-105, 2015. 2\n\nGreedy policy search: A simple baseline for learnable test-time augmentation. Alexander Lyzhov, Yuliya Molchanova, Arsenii Ashukha, Dmitry Molchanov, Dmitry Vetrov, PMLRConference on Uncertainty in Artificial Intelligence (UAI). Jonas Peters and David Sontag124Alexander Lyzhov, Yuliya Molchanova, Arsenii Ashukha, Dmitry Molchanov, and Dmitry Vetrov. Greedy policy search: A simple baseline for learnable test-time augmenta- tion. In Jonas Peters and David Sontag, editors, Conference on Uncertainty in Artificial Intelligence (UAI), volume 124 of Proceedings of Machine Learning Research, pages 1308- 1317. PMLR, 03-06 Aug 2020. 3\n\nCatastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, J Neal, Cohen, Psychology of learning and motivation. Elsevier24Michael McCloskey and Neal J Cohen. Catastrophic inter- ference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, vol- ume 24, pages 109-165. Elsevier, 1989. 2\n\nDomain generalization via invariant feature representation. Krikamol Muandet, David Balduzzi, Bernhard Sch\u00f6lkopf, PMLRICML. Krikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10-18. PMLR, 2013. 3\n\nTest-time adaptation to distribution shift by confidence maximization and input transformation. Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, Jan Hendrik Metzen, arXiv:2106.149996arXiv preprintChaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confi- dence maximization and input transformation. arXiv preprint arXiv:2106.14999, 2021. 1, 2, 3, 6\n\nDomain adaptation via transfer component analysis. Ivor W Sinno Jialin Pan, James T Tsang, Qiang Kwok, Yang, IEEE Transactions on Neural Networks. 222Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analy- sis. IEEE Transactions on Neural Networks, 22(2):199-210, 2011. 2\n\nContinual lifelong learning with neural networks: A review. I German, Ronald Parisi, Kemker, L Jose, Christopher Part, Stefan Kanan, Wermter, Neural Networks. 1133German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54-71, 2019. 2, 3\n\nVisual domain adaptation: A survey of recent advances. M Vishal, Raghuraman Patel, Ruonan Gopalan, Rama Li, Chellappa, IEEE signal processing magazine. 323Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53-69, 2015. 2\n\nAcceleration of stochastic approximation by averaging. T Boris, Anatoli B Juditsky Polyak, SIAM journal on control and optimization. 3045Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992. 4, 5\n\nDeeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. Viraj Prabhu, Shivam Khare, ICCV. Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoff- man. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In ICCV, pages 8558-8567, 2021. 2\n\nicarl: Incremental classifier and representation learning. Alexander Sylvestre-Alvise Rebuffi, Georg Kolesnikov, Christoph H Sperl, Lampert, CVPR. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classi- fier and representation learning. In CVPR, pages 2001-2010, 2017. 3\n\nACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding. Christos Sakaridis, Dengxin Dai, Luc Van Gool, ICCV. 16Christos Sakaridis, Dengxin Dai, and Luc Van Gool. ACDC: The adverse conditions dataset with correspondences for se- mantic driving scene understanding. In ICCV, October 2021. 1, 6\n\nImproving robustness against common corruptions by covariate shift adaptation. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge, NeuIPS. 337Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. NeuIPS, 33, 2020. 7\n\nA survey on image data augmentation for deep learning. Connor Shorten, M Taghi, Khoshgoftaar, Journal of Big Data. 615Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):1-48, 2019. 3, 5\n\nThe task rehearsal method of life-long learning: Overcoming impoverished data. L Daniel, Robert E Silver, Mercer, Conference of the Canadian Society for Computational Studies of Intelligence. SpringerDaniel L Silver and Robert E Mercer. The task rehearsal method of life-long learning: Overcoming impoverished data. In Conference of the Canadian Society for Computa- tional Studies of Intelligence, pages 90-101. Springer, 2002. 3\n\nTest-time training with selfsupervision for generalization under distribution shifts. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, Moritz Hardt, PMLR, 2020. 3ICML. 45Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229-9248. PMLR, 2020. 3, 4, 5\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, NeuIPS. 5Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeuIPS, pages 1195-1204, 2017. 2, 4, 5\n\nDomain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, IROS. IEEEJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Woj- ciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IROS, pages 23-30. IEEE, 2017. 3\n\nTraining deep networks with synthetic data: Bridging the reality gap by domain randomization. Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, CVPR Workshops. Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan BirchfieldJonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cam- eracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic data: Bridging the reality gap by do- main randomization. In CVPR Workshops, pages 969-977, 2018. 3\n\nLearning to adapt structured output space for semantic segmentation. Y.-H Tsai, W.-C Hung, S Schulter, K Sohn, M.-H Yang, M Chandraker, CVPR. Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018. 2\n\nContinual adaptation of visual representations via domain randomization and meta-learning. Riccardo Volpi, Diane Larlus, Gr\u00e9gory Rogez, CVPR. Riccardo Volpi, Diane Larlus, and Gr\u00e9gory Rogez. Continual adaptation of visual representations via domain randomiza- tion and meta-learning. In CVPR, pages 4443-4453, 2021. 3\n\n. Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, Trevor Darrell, arXiv:2109.01087On-target adaptation. arXiv preprintDequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shel- hamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv:2109.01087, 2021. 3\n\nTent: Fully test-time adaptation by entropy minimization. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor Darrell, ICLR. 7Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8\n\nDomain adaptive semantic segmentation with self-supervised depth estimation. Qin Wang, Dengxin Dai, Lukas Hoyer, Luc Van Gool, Olga Fink, ICCV. Qin Wang, Dengxin Dai, Lukas Hoyer, Luc Van Gool, and Olga Fink. Domain adaptive semantic segmentation with self-supervised depth estimation. In ICCV, pages 8515- 8525, 2021. 2\n\nIncremental adversarial domain adaptation for continually changing environments. Markus Wulfmeier, Alex Bewley, Ingmar Posner, ICRA. IEEEMarkus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489-4495. IEEE, 2018. 3\n\nSegformer: Simple and efficient design for semantic segmentation with transformers. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo, NeuIPS. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and ef- ficient design for semantic segmentation with transformers. In NeuIPS, 2021. 6\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, CVPR. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 1492-1500, 2017. 6\n\nGeneralized source-free domain adaptation. Shiqi Yang, Yaxing Wang, Joost Van De, Luis Weijer, Shangling Herranz, Jui, ICCV. Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978-8987, 2021. 2\n\nFda: Fourier domain adaptation for semantic segmentation. Yanchao Yang, Stefano Soatto, CVPR. Yanchao Yang and Stefano Soatto. Fda: Fourier domain adaptation for semantic segmentation. In CVPR, pages 4085-4095, 2020. 2\n\nSofa: Source-data-free feature alignment for unsupervised domain adaptation. Hao-Wei Yeh, Baoyao Yang, C Pong, Tatsuya Yuen, Harada, WACV. Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for un- supervised domain adaptation. In WACV, pages 474-483, 2021. 2\n\nA fourier perspective on model robustness in computer vision. Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, D Ekin, Justin Cubuk, Gilmer, NeuIPS. 3Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. NeuIPS, 2019. 3\n\nTest-time batch statistics calibration for covariate shift. Fuming You, Jingjing Li, Zhou Zhao, arXiv:2110.04065arXiv preprintFuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 3\n\nWide residual networks. BMVC. Sergey Zagoruyko, Nikos Komodakis, Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. BMVC, 2016. 6\n\nContinual learning through synaptic intelligence. Friedemann Zenke, Ben Poole, Surya Ganguli, PMLR, 2017. 3International Conference on Machine Learning. Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin- ual learning through synaptic intelligence. In International Conference on Machine Learning, pages 3987-3995. PMLR, 2017. 3\n\nMarvin Zhang, Sergey Levine, Chelsea Finn, Memo, arXiv:2110.09506Test time robustness via adaptation and augmentation. arXiv preprintMarvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. 3\n\nTraining on test data with bayesian adaptation for covariate shift. Aurick Zhou, Sergey Levine, arXiv:2109.12746arXiv preprintAurick Zhou and Sergey Levine. Training on test data with bayesian adaptation for covariate shift. arXiv preprint arXiv:2109.12746, 2021. 3\n\nUnsupervised domain adaptation for semantic segmentation via class-balanced self-training. Yang Zou, Zhiding Yu, Jinsong Bvk Vijaya Kumar, Wang, ECCV. Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289-305, 2018. 2\n", "annotations": {"author": "[{\"end\":74,\"start\":41},{\"end\":127,\"start\":75},{\"end\":184,\"start\":128},{\"end\":265,\"start\":185}]", "publisher": null, "author_last_name": "[{\"end\":49,\"start\":45},{\"end\":84,\"start\":80},{\"end\":140,\"start\":136},{\"end\":196,\"start\":193}]", "author_first_name": "[{\"end\":44,\"start\":41},{\"end\":79,\"start\":75},{\"end\":131,\"start\":128},{\"end\":135,\"start\":132},{\"end\":192,\"start\":185}]", "author_affiliation": "[{\"end\":73,\"start\":51},{\"end\":108,\"start\":86},{\"end\":126,\"start\":110},{\"end\":164,\"start\":142},{\"end\":183,\"start\":166},{\"end\":264,\"start\":237}]", "title": "[{\"end\":38,\"start\":1},{\"end\":303,\"start\":266}]", "venue": null, "abstract": "[{\"end\":2822,\"start\":305}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3982,\"start\":3978},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4227,\"start\":4223},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4351,\"start\":4348},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4538,\"start\":4534},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4541,\"start\":4538},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4544,\"start\":4541},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6114,\"start\":6110},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6802,\"start\":6798},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6805,\"start\":6802},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7998,\"start\":7994},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8001,\"start\":7998},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8264,\"start\":8260},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8293,\"start\":8289},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8296,\"start\":8293},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8367,\"start\":8363},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":8370,\"start\":8367},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8523,\"start\":8519},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8526,\"start\":8523},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8529,\"start\":8526},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":8531,\"start\":8529},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8655,\"start\":8651},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8658,\"start\":8655},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8870,\"start\":8866},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8873,\"start\":8870},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8876,\"start\":8873},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9107,\"start\":9103},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9285,\"start\":9281},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9578,\"start\":9574},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9706,\"start\":9702},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9806,\"start\":9802},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":9902,\"start\":9898},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10230,\"start\":10226},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10233,\"start\":10230},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10236,\"start\":10233},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10299,\"start\":10295},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10302,\"start\":10299},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10305,\"start\":10302},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10609,\"start\":10605},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11035,\"start\":11031},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11154,\"start\":11150},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11252,\"start\":11248},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11401,\"start\":11398},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12046,\"start\":12042},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12257,\"start\":12253},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12656,\"start\":12652},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12683,\"start\":12679},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12867,\"start\":12863},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12897,\"start\":12893},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":12900,\"start\":12897},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13017,\"start\":13013},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13093,\"start\":13089},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13184,\"start\":13181},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13187,\"start\":13184},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13269,\"start\":13265},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13441,\"start\":13437},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13462,\"start\":13458},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13465,\"start\":13462},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13468,\"start\":13465},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":13471,\"start\":13468},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13494,\"start\":13491},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13497,\"start\":13494},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":13500,\"start\":13497},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13729,\"start\":13725},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13732,\"start\":13729},{\"end\":14285,\"start\":14277},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16738,\"start\":16734},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":17741,\"start\":17737},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18083,\"start\":18079},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18086,\"start\":18083},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":19180,\"start\":19176},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19656,\"start\":19652},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19766,\"start\":19762},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19885,\"start\":19881},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19901,\"start\":19898},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20012,\"start\":20009},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20015,\"start\":20012},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21801,\"start\":21800},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21967,\"start\":21966},{\"end\":22025,\"start\":22023},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22075,\"start\":22074},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24601,\"start\":24597},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":24604,\"start\":24601},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25239,\"start\":25235},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25416,\"start\":25412},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":26338,\"start\":26334},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":26436,\"start\":26432},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26477,\"start\":26474},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26676,\"start\":26673},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":27231,\"start\":27227},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27247,\"start\":27243},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27344,\"start\":27341},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27442,\"start\":27438},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27517,\"start\":27514},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27829,\"start\":27826},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":27935,\"start\":27931},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":28680,\"start\":28676},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29670,\"start\":29667},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30796,\"start\":30792},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30799,\"start\":30796},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30945,\"start\":30941},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31043,\"start\":31039},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34706,\"start\":34703},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34716,\"start\":34712},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36477,\"start\":36474}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37674,\"start\":37547},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38602,\"start\":37675}]", "paragraph": "[{\"end\":4545,\"start\":2838},{\"end\":5588,\"start\":4547},{\"end\":6842,\"start\":5590},{\"end\":7363,\"start\":6844},{\"end\":7523,\"start\":7365},{\"end\":7662,\"start\":7525},{\"end\":7773,\"start\":7664},{\"end\":7920,\"start\":7775},{\"end\":8533,\"start\":7957},{\"end\":8962,\"start\":8558},{\"end\":10786,\"start\":8964},{\"end\":11677,\"start\":10819},{\"end\":12610,\"start\":11679},{\"end\":13188,\"start\":12633},{\"end\":14040,\"start\":13214},{\"end\":15243,\"start\":14103},{\"end\":15569,\"start\":15245},{\"end\":15847,\"start\":15571},{\"end\":16488,\"start\":15863},{\"end\":17370,\"start\":16490},{\"end\":17943,\"start\":17372},{\"end\":18333,\"start\":17945},{\"end\":18459,\"start\":18355},{\"end\":18731,\"start\":18501},{\"end\":18902,\"start\":18733},{\"end\":19685,\"start\":18936},{\"end\":20519,\"start\":19687},{\"end\":21578,\"start\":20635},{\"end\":22076,\"start\":21638},{\"end\":22455,\"start\":22179},{\"end\":22508,\"start\":22457},{\"end\":23192,\"start\":22551},{\"end\":23369,\"start\":23194},{\"end\":23486,\"start\":23371},{\"end\":23741,\"start\":23512},{\"end\":24024,\"start\":23798},{\"end\":24665,\"start\":24026},{\"end\":24821,\"start\":24667},{\"end\":25101,\"start\":24837},{\"end\":25508,\"start\":25124},{\"end\":26252,\"start\":25510},{\"end\":27150,\"start\":26254},{\"end\":27592,\"start\":27152},{\"end\":28590,\"start\":27594},{\"end\":30262,\"start\":28592},{\"end\":33257,\"start\":30301},{\"end\":33394,\"start\":33259},{\"end\":33934,\"start\":33396},{\"end\":34661,\"start\":33975},{\"end\":35206,\"start\":34663},{\"end\":35613,\"start\":35248},{\"end\":36732,\"start\":35651},{\"end\":37546,\"start\":36747}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18354,\"start\":18334},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18500,\"start\":18460},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18935,\"start\":18903},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20567,\"start\":20520},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20634,\"start\":20567},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22178,\"start\":22077},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22550,\"start\":22509},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23511,\"start\":23487},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23762,\"start\":23742},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23797,\"start\":23762}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15698,\"start\":15691},{\"end\":29747,\"start\":29740},{\"end\":30013,\"start\":30006},{\"end\":30487,\"start\":30480},{\"end\":32383,\"start\":32376},{\"end\":33811,\"start\":33804},{\"end\":34154,\"start\":34147},{\"end\":34314,\"start\":34307},{\"end\":34542,\"start\":34535},{\"end\":35450,\"start\":35443},{\"end\":35823,\"start\":35816}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2836,\"start\":2824},{\"attributes\":{\"n\":\"2.\"},\"end\":7935,\"start\":7923},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7955,\"start\":7938},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8556,\"start\":8536},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10817,\"start\":10789},{\"attributes\":{\"n\":\"2.4.\"},\"end\":12631,\"start\":12613},{\"attributes\":{\"n\":\"2.5.\"},\"end\":13212,\"start\":13191},{\"attributes\":{\"n\":\"3.\"},\"end\":14080,\"start\":14043},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14101,\"start\":14083},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15861,\"start\":15850},{\"end\":21636,\"start\":21581},{\"attributes\":{\"n\":\"4.\"},\"end\":24835,\"start\":24824},{\"attributes\":{\"n\":\"4.1.\"},\"end\":25122,\"start\":25104},{\"attributes\":{\"n\":\"4.2.\"},\"end\":30299,\"start\":30265},{\"attributes\":{\"n\":\"4.3.\"},\"end\":33973,\"start\":33937},{\"attributes\":{\"n\":\"4.4.\"},\"end\":35246,\"start\":35209},{\"attributes\":{\"n\":\"4.5.\"},\"end\":35649,\"start\":35616},{\"attributes\":{\"n\":\"5.\"},\"end\":36745,\"start\":36735},{\"end\":37557,\"start\":37548}]", "table": "[{\"end\":37674,\"start\":37658},{\"end\":38602,\"start\":38141}]", "figure_caption": "[{\"end\":37658,\"start\":37559},{\"end\":38141,\"start\":37677}]", "figure_ref": "[{\"end\":3209,\"start\":3201},{\"end\":4728,\"start\":4720},{\"end\":16487,\"start\":16479},{\"end\":29600,\"start\":29590}]", "bib_author_first_name": "[{\"end\":38689,\"start\":38682},{\"end\":38708,\"start\":38699},{\"end\":38723,\"start\":38717},{\"end\":38741,\"start\":38735},{\"end\":38931,\"start\":38926},{\"end\":38935,\"start\":38932},{\"end\":38945,\"start\":38940},{\"end\":38950,\"start\":38946},{\"end\":38966,\"start\":38958},{\"end\":38968,\"start\":38967},{\"end\":39200,\"start\":39193},{\"end\":39211,\"start\":39207},{\"end\":39223,\"start\":39219},{\"end\":39239,\"start\":39233},{\"end\":39470,\"start\":39464},{\"end\":39484,\"start\":39477},{\"end\":39497,\"start\":39490},{\"end\":39507,\"start\":39505},{\"end\":39521,\"start\":39514},{\"end\":39531,\"start\":39528},{\"end\":39547,\"start\":39539},{\"end\":39559,\"start\":39552},{\"end\":39786,\"start\":39781},{\"end\":39798,\"start\":39794},{\"end\":40404,\"start\":40398},{\"end\":40420,\"start\":40413},{\"end\":40437,\"start\":40428},{\"end\":40449,\"start\":40445},{\"end\":40465,\"start\":40459},{\"end\":40484,\"start\":40477},{\"end\":40794,\"start\":40785},{\"end\":40808,\"start\":40802},{\"end\":40831,\"start\":40825},{\"end\":40847,\"start\":40840},{\"end\":41286,\"start\":41280},{\"end\":41310,\"start\":41301},{\"end\":41322,\"start\":41317},{\"end\":41335,\"start\":41329},{\"end\":41658,\"start\":41650},{\"end\":41673,\"start\":41668},{\"end\":41687,\"start\":41683},{\"end\":41701,\"start\":41696},{\"end\":41713,\"start\":41711},{\"end\":41975,\"start\":41970},{\"end\":41995,\"start\":41986},{\"end\":42010,\"start\":42003},{\"end\":42027,\"start\":42021},{\"end\":42043,\"start\":42037},{\"end\":42277,\"start\":42269},{\"end\":42291,\"start\":42285},{\"end\":42479,\"start\":42474},{\"end\":42490,\"start\":42485},{\"end\":42501,\"start\":42499},{\"end\":42515,\"start\":42507},{\"end\":42771,\"start\":42768},{\"end\":42789,\"start\":42783},{\"end\":42804,\"start\":42798},{\"end\":42815,\"start\":42809},{\"end\":42831,\"start\":42826},{\"end\":42842,\"start\":42838},{\"end\":42857,\"start\":42852},{\"end\":42870,\"start\":42865},{\"end\":42882,\"start\":42876},{\"end\":42897,\"start\":42893},{\"end\":43268,\"start\":43265},{\"end\":43286,\"start\":43280},{\"end\":43523,\"start\":43520},{\"end\":43541,\"start\":43535},{\"end\":43550,\"start\":43546},{\"end\":43552,\"start\":43551},{\"end\":43566,\"start\":43560},{\"end\":43579,\"start\":43573},{\"end\":43594,\"start\":43588},{\"end\":43880,\"start\":43876},{\"end\":43896,\"start\":43890},{\"end\":43910,\"start\":43906},{\"end\":44131,\"start\":44127},{\"end\":44145,\"start\":44141},{\"end\":44160,\"start\":44153},{\"end\":44174,\"start\":44167},{\"end\":44187,\"start\":44180},{\"end\":44199,\"start\":44195},{\"end\":44214,\"start\":44208},{\"end\":44228,\"start\":44222},{\"end\":44574,\"start\":44569},{\"end\":44589,\"start\":44582},{\"end\":44598,\"start\":44595},{\"end\":44891,\"start\":44885},{\"end\":44899,\"start\":44896},{\"end\":44911,\"start\":44906},{\"end\":44923,\"start\":44916},{\"end\":44936,\"start\":44929},{\"end\":44948,\"start\":44943},{\"end\":44957,\"start\":44955},{\"end\":44973,\"start\":44965},{\"end\":45362,\"start\":45355},{\"end\":45373,\"start\":45367},{\"end\":45389,\"start\":45383},{\"end\":45399,\"start\":45396},{\"end\":45412,\"start\":45406},{\"end\":45422,\"start\":45419},{\"end\":45439,\"start\":45432},{\"end\":45842,\"start\":45836},{\"end\":45858,\"start\":45852},{\"end\":46093,\"start\":46087},{\"end\":46108,\"start\":46102},{\"end\":46123,\"start\":46116},{\"end\":46140,\"start\":46135},{\"end\":46437,\"start\":46432},{\"end\":46457,\"start\":46451},{\"end\":46471,\"start\":46467},{\"end\":46488,\"start\":46484},{\"end\":46506,\"start\":46497},{\"end\":46525,\"start\":46519},{\"end\":46527,\"start\":46526},{\"end\":46540,\"start\":46534},{\"end\":46552,\"start\":46548},{\"end\":46564,\"start\":46559},{\"end\":46583,\"start\":46574},{\"end\":47061,\"start\":47057},{\"end\":47082,\"start\":47074},{\"end\":47315,\"start\":47311},{\"end\":47332,\"start\":47328},{\"end\":47352,\"start\":47344},{\"end\":47354,\"start\":47353},{\"end\":47613,\"start\":47605},{\"end\":47632,\"start\":47626},{\"end\":47647,\"start\":47643},{\"end\":47660,\"start\":47655},{\"end\":47681,\"start\":47670},{\"end\":47939,\"start\":47931},{\"end\":47958,\"start\":47952},{\"end\":48210,\"start\":48209},{\"end\":48236,\"start\":48235},{\"end\":48510,\"start\":48503},{\"end\":48521,\"start\":48516},{\"end\":48537,\"start\":48529},{\"end\":48552,\"start\":48546},{\"end\":48881,\"start\":48872},{\"end\":49196,\"start\":49192},{\"end\":49206,\"start\":49201},{\"end\":49218,\"start\":49211},{\"end\":49229,\"start\":49224},{\"end\":49248,\"start\":49240},{\"end\":49502,\"start\":49499},{\"end\":49514,\"start\":49507},{\"end\":49528,\"start\":49521},{\"end\":49545,\"start\":49543},{\"end\":49802,\"start\":49795},{\"end\":49813,\"start\":49807},{\"end\":49828,\"start\":49820},{\"end\":49841,\"start\":49834},{\"end\":49853,\"start\":49847},{\"end\":50078,\"start\":50070},{\"end\":50088,\"start\":50083},{\"end\":50347,\"start\":50343},{\"end\":50361,\"start\":50354},{\"end\":50706,\"start\":50702},{\"end\":50720,\"start\":50714},{\"end\":50731,\"start\":50725},{\"end\":51010,\"start\":51005},{\"end\":51019,\"start\":51016},{\"end\":51030,\"start\":51027},{\"end\":51244,\"start\":51235},{\"end\":51254,\"start\":51251},{\"end\":51267,\"start\":51260},{\"end\":51281,\"start\":51274},{\"end\":51540,\"start\":51531},{\"end\":51555,\"start\":51549},{\"end\":51575,\"start\":51568},{\"end\":51591,\"start\":51585},{\"end\":51609,\"start\":51603},{\"end\":52180,\"start\":52173},{\"end\":52193,\"start\":52192},{\"end\":52537,\"start\":52529},{\"end\":52552,\"start\":52547},{\"end\":52571,\"start\":52563},{\"end\":52856,\"start\":52846},{\"end\":52877,\"start\":52872},{\"end\":52895,\"start\":52889},{\"end\":52911,\"start\":52905},{\"end\":52928,\"start\":52922},{\"end\":52938,\"start\":52935},{\"end\":52946,\"start\":52939},{\"end\":53303,\"start\":53299},{\"end\":53305,\"start\":53304},{\"end\":53329,\"start\":53324},{\"end\":53331,\"start\":53330},{\"end\":53344,\"start\":53339},{\"end\":53636,\"start\":53635},{\"end\":53651,\"start\":53645},{\"end\":53669,\"start\":53668},{\"end\":53687,\"start\":53676},{\"end\":53700,\"start\":53694},{\"end\":53978,\"start\":53977},{\"end\":53997,\"start\":53987},{\"end\":54011,\"start\":54005},{\"end\":54025,\"start\":54021},{\"end\":54310,\"start\":54309},{\"end\":54336,\"start\":54318},{\"end\":54694,\"start\":54689},{\"end\":54709,\"start\":54703},{\"end\":54991,\"start\":54982},{\"end\":55023,\"start\":55018},{\"end\":55045,\"start\":55036},{\"end\":55047,\"start\":55046},{\"end\":55360,\"start\":55352},{\"end\":55379,\"start\":55372},{\"end\":55388,\"start\":55385},{\"end\":55675,\"start\":55668},{\"end\":55694,\"start\":55687},{\"end\":55707,\"start\":55702},{\"end\":55719,\"start\":55713},{\"end\":55738,\"start\":55731},{\"end\":55756,\"start\":55748},{\"end\":56040,\"start\":56034},{\"end\":56051,\"start\":56050},{\"end\":56317,\"start\":56316},{\"end\":56332,\"start\":56326},{\"end\":56334,\"start\":56333},{\"end\":56757,\"start\":56755},{\"end\":56771,\"start\":56763},{\"end\":56784,\"start\":56778},{\"end\":56794,\"start\":56790},{\"end\":56809,\"start\":56803},{\"end\":56823,\"start\":56817},{\"end\":57193,\"start\":57188},{\"end\":57210,\"start\":57205},{\"end\":57526,\"start\":57522},{\"end\":57540,\"start\":57534},{\"end\":57551,\"start\":57547},{\"end\":57562,\"start\":57557},{\"end\":57582,\"start\":57574},{\"end\":57598,\"start\":57592},{\"end\":57941,\"start\":57933},{\"end\":57958,\"start\":57952},{\"end\":57973,\"start\":57968},{\"end\":57985,\"start\":57981},{\"end\":57999,\"start\":57994},{\"end\":58459,\"start\":58455},{\"end\":58470,\"start\":58466},{\"end\":58478,\"start\":58477},{\"end\":58490,\"start\":58489},{\"end\":58501,\"start\":58497},{\"end\":58509,\"start\":58508},{\"end\":58791,\"start\":58783},{\"end\":58804,\"start\":58799},{\"end\":58820,\"start\":58813},{\"end\":59019,\"start\":59013},{\"end\":59034,\"start\":59026},{\"end\":59045,\"start\":59040},{\"end\":59060,\"start\":59056},{\"end\":59078,\"start\":59072},{\"end\":59349,\"start\":59343},{\"end\":59360,\"start\":59356},{\"end\":59380,\"start\":59372},{\"end\":59391,\"start\":59386},{\"end\":59409,\"start\":59403},{\"end\":59682,\"start\":59679},{\"end\":59696,\"start\":59689},{\"end\":59707,\"start\":59702},{\"end\":59718,\"start\":59715},{\"end\":59733,\"start\":59729},{\"end\":60011,\"start\":60005},{\"end\":60027,\"start\":60023},{\"end\":60042,\"start\":60036},{\"end\":60325,\"start\":60321},{\"end\":60337,\"start\":60331},{\"end\":60351,\"start\":60344},{\"end\":60361,\"start\":60356},{\"end\":60378,\"start\":60374},{\"end\":60380,\"start\":60379},{\"end\":60394,\"start\":60390},{\"end\":60667,\"start\":60660},{\"end\":60677,\"start\":60673},{\"end\":60693,\"start\":60688},{\"end\":60709,\"start\":60702},{\"end\":60721,\"start\":60714},{\"end\":60947,\"start\":60942},{\"end\":60960,\"start\":60954},{\"end\":60972,\"start\":60967},{\"end\":60985,\"start\":60981},{\"end\":61003,\"start\":60994},{\"end\":61248,\"start\":61241},{\"end\":61262,\"start\":61255},{\"end\":61487,\"start\":61480},{\"end\":61499,\"start\":61493},{\"end\":61507,\"start\":61506},{\"end\":61521,\"start\":61514},{\"end\":61779,\"start\":61775},{\"end\":61792,\"start\":61785},{\"end\":61800,\"start\":61793},{\"end\":61816,\"start\":61808},{\"end\":61826,\"start\":61825},{\"end\":61839,\"start\":61833},{\"end\":62092,\"start\":62086},{\"end\":62106,\"start\":62098},{\"end\":62115,\"start\":62111},{\"end\":62330,\"start\":62324},{\"end\":62347,\"start\":62342},{\"end\":62498,\"start\":62488},{\"end\":62509,\"start\":62506},{\"end\":62522,\"start\":62517},{\"end\":62778,\"start\":62772},{\"end\":62792,\"start\":62786},{\"end\":62808,\"start\":62801},{\"end\":63128,\"start\":63122},{\"end\":63141,\"start\":63135},{\"end\":63416,\"start\":63412},{\"end\":63429,\"start\":63422},{\"end\":63441,\"start\":63434}]", "bib_author_last_name": "[{\"end\":38697,\"start\":38690},{\"end\":38715,\"start\":38709},{\"end\":38733,\"start\":38724},{\"end\":38748,\"start\":38742},{\"end\":38938,\"start\":38936},{\"end\":38956,\"start\":38951},{\"end\":38972,\"start\":38969},{\"end\":39205,\"start\":39201},{\"end\":39217,\"start\":39212},{\"end\":39231,\"start\":39224},{\"end\":39247,\"start\":39240},{\"end\":39475,\"start\":39471},{\"end\":39488,\"start\":39485},{\"end\":39503,\"start\":39498},{\"end\":39512,\"start\":39508},{\"end\":39526,\"start\":39522},{\"end\":39537,\"start\":39532},{\"end\":39550,\"start\":39548},{\"end\":39565,\"start\":39560},{\"end\":39792,\"start\":39787},{\"end\":39805,\"start\":39799},{\"end\":39813,\"start\":39807},{\"end\":40411,\"start\":40405},{\"end\":40426,\"start\":40421},{\"end\":40443,\"start\":40438},{\"end\":40457,\"start\":40450},{\"end\":40475,\"start\":40466},{\"end\":40493,\"start\":40485},{\"end\":40800,\"start\":40795},{\"end\":40823,\"start\":40809},{\"end\":40838,\"start\":40832},{\"end\":40859,\"start\":40848},{\"end\":41299,\"start\":41287},{\"end\":41315,\"start\":41311},{\"end\":41327,\"start\":41323},{\"end\":41345,\"start\":41336},{\"end\":41349,\"start\":41347},{\"end\":41666,\"start\":41659},{\"end\":41681,\"start\":41674},{\"end\":41694,\"start\":41688},{\"end\":41709,\"start\":41702},{\"end\":41717,\"start\":41714},{\"end\":41984,\"start\":41976},{\"end\":42001,\"start\":41996},{\"end\":42019,\"start\":42011},{\"end\":42035,\"start\":42028},{\"end\":42052,\"start\":42044},{\"end\":42283,\"start\":42278},{\"end\":42301,\"start\":42292},{\"end\":42483,\"start\":42480},{\"end\":42497,\"start\":42491},{\"end\":42505,\"start\":42502},{\"end\":42526,\"start\":42516},{\"end\":42781,\"start\":42772},{\"end\":42796,\"start\":42790},{\"end\":42807,\"start\":42805},{\"end\":42824,\"start\":42816},{\"end\":42836,\"start\":42832},{\"end\":42850,\"start\":42843},{\"end\":42863,\"start\":42858},{\"end\":42874,\"start\":42871},{\"end\":42891,\"start\":42883},{\"end\":42901,\"start\":42898},{\"end\":43278,\"start\":43269},{\"end\":43297,\"start\":43287},{\"end\":43533,\"start\":43524},{\"end\":43544,\"start\":43542},{\"end\":43558,\"start\":43553},{\"end\":43571,\"start\":43567},{\"end\":43586,\"start\":43580},{\"end\":43611,\"start\":43595},{\"end\":43888,\"start\":43881},{\"end\":43904,\"start\":43897},{\"end\":43917,\"start\":43911},{\"end\":44139,\"start\":44132},{\"end\":44151,\"start\":44146},{\"end\":44165,\"start\":44161},{\"end\":44178,\"start\":44175},{\"end\":44193,\"start\":44188},{\"end\":44206,\"start\":44200},{\"end\":44220,\"start\":44215},{\"end\":44236,\"start\":44229},{\"end\":44580,\"start\":44575},{\"end\":44593,\"start\":44590},{\"end\":44607,\"start\":44599},{\"end\":44894,\"start\":44892},{\"end\":44904,\"start\":44900},{\"end\":44914,\"start\":44912},{\"end\":44927,\"start\":44924},{\"end\":44941,\"start\":44937},{\"end\":44953,\"start\":44949},{\"end\":44963,\"start\":44958},{\"end\":44979,\"start\":44974},{\"end\":45365,\"start\":45363},{\"end\":45381,\"start\":45374},{\"end\":45394,\"start\":45390},{\"end\":45404,\"start\":45400},{\"end\":45417,\"start\":45413},{\"end\":45430,\"start\":45423},{\"end\":45443,\"start\":45440},{\"end\":45850,\"start\":45843},{\"end\":45865,\"start\":45859},{\"end\":46100,\"start\":46094},{\"end\":46114,\"start\":46109},{\"end\":46133,\"start\":46124},{\"end\":46150,\"start\":46141},{\"end\":46449,\"start\":46438},{\"end\":46465,\"start\":46458},{\"end\":46482,\"start\":46472},{\"end\":46495,\"start\":46489},{\"end\":46517,\"start\":46507},{\"end\":46532,\"start\":46528},{\"end\":46546,\"start\":46541},{\"end\":46557,\"start\":46553},{\"end\":46572,\"start\":46565},{\"end\":46601,\"start\":46584},{\"end\":47072,\"start\":47062},{\"end\":47089,\"start\":47083},{\"end\":47326,\"start\":47316},{\"end\":47342,\"start\":47333},{\"end\":47361,\"start\":47355},{\"end\":47624,\"start\":47614},{\"end\":47641,\"start\":47633},{\"end\":47653,\"start\":47648},{\"end\":47668,\"start\":47661},{\"end\":47686,\"start\":47682},{\"end\":47950,\"start\":47940},{\"end\":47965,\"start\":47959},{\"end\":47981,\"start\":47967},{\"end\":48207,\"start\":48194},{\"end\":48220,\"start\":48211},{\"end\":48233,\"start\":48222},{\"end\":48242,\"start\":48237},{\"end\":48254,\"start\":48244},{\"end\":48514,\"start\":48511},{\"end\":48527,\"start\":48522},{\"end\":48544,\"start\":48538},{\"end\":48559,\"start\":48553},{\"end\":48885,\"start\":48882},{\"end\":49199,\"start\":49197},{\"end\":49209,\"start\":49207},{\"end\":49222,\"start\":49219},{\"end\":49238,\"start\":49230},{\"end\":49259,\"start\":49249},{\"end\":49505,\"start\":49503},{\"end\":49519,\"start\":49515},{\"end\":49532,\"start\":49529},{\"end\":49541,\"start\":49534},{\"end\":49550,\"start\":49546},{\"end\":49554,\"start\":49552},{\"end\":49805,\"start\":49803},{\"end\":49818,\"start\":49814},{\"end\":49832,\"start\":49829},{\"end\":49845,\"start\":49842},{\"end\":49857,\"start\":49854},{\"end\":50081,\"start\":50079},{\"end\":50094,\"start\":50089},{\"end\":50352,\"start\":50348},{\"end\":50364,\"start\":50362},{\"end\":50712,\"start\":50707},{\"end\":50723,\"start\":50721},{\"end\":50736,\"start\":50732},{\"end\":51014,\"start\":51011},{\"end\":51025,\"start\":51020},{\"end\":51035,\"start\":51031},{\"end\":51249,\"start\":51245},{\"end\":51258,\"start\":51255},{\"end\":51272,\"start\":51268},{\"end\":51288,\"start\":51282},{\"end\":51547,\"start\":51541},{\"end\":51566,\"start\":51556},{\"end\":51583,\"start\":51576},{\"end\":51601,\"start\":51592},{\"end\":51616,\"start\":51610},{\"end\":52190,\"start\":52181},{\"end\":52198,\"start\":52194},{\"end\":52205,\"start\":52200},{\"end\":52545,\"start\":52538},{\"end\":52561,\"start\":52553},{\"end\":52581,\"start\":52572},{\"end\":52870,\"start\":52857},{\"end\":52887,\"start\":52878},{\"end\":52903,\"start\":52896},{\"end\":52920,\"start\":52912},{\"end\":52933,\"start\":52929},{\"end\":52953,\"start\":52947},{\"end\":53322,\"start\":53306},{\"end\":53337,\"start\":53332},{\"end\":53349,\"start\":53345},{\"end\":53355,\"start\":53351},{\"end\":53643,\"start\":53637},{\"end\":53658,\"start\":53652},{\"end\":53666,\"start\":53660},{\"end\":53674,\"start\":53670},{\"end\":53692,\"start\":53688},{\"end\":53706,\"start\":53701},{\"end\":53715,\"start\":53708},{\"end\":53985,\"start\":53979},{\"end\":54003,\"start\":53998},{\"end\":54019,\"start\":54012},{\"end\":54028,\"start\":54026},{\"end\":54039,\"start\":54030},{\"end\":54316,\"start\":54311},{\"end\":54343,\"start\":54337},{\"end\":54701,\"start\":54695},{\"end\":54715,\"start\":54710},{\"end\":55016,\"start\":54992},{\"end\":55034,\"start\":55024},{\"end\":55053,\"start\":55048},{\"end\":55062,\"start\":55055},{\"end\":55370,\"start\":55361},{\"end\":55383,\"start\":55380},{\"end\":55397,\"start\":55389},{\"end\":55685,\"start\":55676},{\"end\":55700,\"start\":55695},{\"end\":55711,\"start\":55708},{\"end\":55729,\"start\":55720},{\"end\":55746,\"start\":55739},{\"end\":55763,\"start\":55757},{\"end\":56048,\"start\":56041},{\"end\":56057,\"start\":56052},{\"end\":56071,\"start\":56059},{\"end\":56324,\"start\":56318},{\"end\":56341,\"start\":56335},{\"end\":56349,\"start\":56343},{\"end\":56761,\"start\":56758},{\"end\":56776,\"start\":56772},{\"end\":56788,\"start\":56785},{\"end\":56801,\"start\":56795},{\"end\":56815,\"start\":56810},{\"end\":56829,\"start\":56824},{\"end\":57203,\"start\":57194},{\"end\":57218,\"start\":57211},{\"end\":57532,\"start\":57527},{\"end\":57545,\"start\":57541},{\"end\":57555,\"start\":57552},{\"end\":57572,\"start\":57563},{\"end\":57590,\"start\":57583},{\"end\":57605,\"start\":57599},{\"end\":57950,\"start\":57942},{\"end\":57966,\"start\":57959},{\"end\":57979,\"start\":57974},{\"end\":57992,\"start\":57986},{\"end\":58007,\"start\":58000},{\"end\":58464,\"start\":58460},{\"end\":58475,\"start\":58471},{\"end\":58487,\"start\":58479},{\"end\":58495,\"start\":58491},{\"end\":58506,\"start\":58502},{\"end\":58520,\"start\":58510},{\"end\":58797,\"start\":58792},{\"end\":58811,\"start\":58805},{\"end\":58826,\"start\":58821},{\"end\":59024,\"start\":59020},{\"end\":59038,\"start\":59035},{\"end\":59054,\"start\":59046},{\"end\":59070,\"start\":59061},{\"end\":59086,\"start\":59079},{\"end\":59354,\"start\":59350},{\"end\":59370,\"start\":59361},{\"end\":59384,\"start\":59381},{\"end\":59401,\"start\":59392},{\"end\":59417,\"start\":59410},{\"end\":59687,\"start\":59683},{\"end\":59700,\"start\":59697},{\"end\":59713,\"start\":59708},{\"end\":59727,\"start\":59719},{\"end\":59738,\"start\":59734},{\"end\":60021,\"start\":60012},{\"end\":60034,\"start\":60028},{\"end\":60049,\"start\":60043},{\"end\":60329,\"start\":60326},{\"end\":60342,\"start\":60338},{\"end\":60354,\"start\":60352},{\"end\":60372,\"start\":60362},{\"end\":60388,\"start\":60381},{\"end\":60398,\"start\":60395},{\"end\":60671,\"start\":60668},{\"end\":60686,\"start\":60678},{\"end\":60700,\"start\":60694},{\"end\":60712,\"start\":60710},{\"end\":60724,\"start\":60722},{\"end\":60952,\"start\":60948},{\"end\":60965,\"start\":60961},{\"end\":60979,\"start\":60973},{\"end\":60992,\"start\":60986},{\"end\":61011,\"start\":61004},{\"end\":61016,\"start\":61013},{\"end\":61253,\"start\":61249},{\"end\":61269,\"start\":61263},{\"end\":61491,\"start\":61488},{\"end\":61504,\"start\":61500},{\"end\":61512,\"start\":61508},{\"end\":61526,\"start\":61522},{\"end\":61534,\"start\":61528},{\"end\":61783,\"start\":61780},{\"end\":61806,\"start\":61801},{\"end\":61823,\"start\":61817},{\"end\":61831,\"start\":61827},{\"end\":61845,\"start\":61840},{\"end\":61853,\"start\":61847},{\"end\":62096,\"start\":62093},{\"end\":62109,\"start\":62107},{\"end\":62120,\"start\":62116},{\"end\":62340,\"start\":62331},{\"end\":62357,\"start\":62348},{\"end\":62504,\"start\":62499},{\"end\":62515,\"start\":62510},{\"end\":62530,\"start\":62523},{\"end\":62784,\"start\":62779},{\"end\":62799,\"start\":62793},{\"end\":62813,\"start\":62809},{\"end\":62819,\"start\":62815},{\"end\":63133,\"start\":63129},{\"end\":63148,\"start\":63142},{\"end\":63420,\"start\":63417},{\"end\":63432,\"start\":63430},{\"end\":63458,\"start\":63442},{\"end\":63464,\"start\":63460}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":209314627},\"end\":38922,\"start\":38604},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b1\"},\"end\":39148,\"start\":38924},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":93002674},\"end\":39396,\"start\":39150},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53763041},\"end\":39779,\"start\":39398},{\"attributes\":{\"doi\":\"arXiv:2109.08191\",\"id\":\"b4\"},\"end\":40053,\"start\":39781},{\"attributes\":{\"id\":\"b5\"},\"end\":40289,\"start\":40055},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":502946},\"end\":40721,\"start\":40291},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":224705419},\"end\":41221,\"start\":40723},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":196208260},\"end\":41522,\"start\":41223},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b9\"},\"end\":41936,\"start\":41524},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":214612169},\"end\":42216,\"start\":41938},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6755881},\"end\":42430,\"start\":42218},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b12\",\"matched_paper_id\":28671436},\"end\":42677,\"start\":42432},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":220250257},\"end\":43177,\"start\":42679},{\"attributes\":{\"id\":\"b14\"},\"end\":43433,\"start\":43179},{\"attributes\":{\"id\":\"b15\"},\"end\":43808,\"start\":43435},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10105727},\"end\":44069,\"start\":43810},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b17\",\"matched_paper_id\":7646250},\"end\":44458,\"start\":44071},{\"attributes\":{\"doi\":\"arXiv:2111.14887\",\"id\":\"b18\"},\"end\":44832,\"start\":44460},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":238208053},\"end\":45353,\"start\":44834},{\"attributes\":{\"doi\":\"arXiv:2110.11478\",\"id\":\"b20\"},\"end\":45753,\"start\":45355},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":244958432},\"end\":46010,\"start\":45755},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215548377},\"end\":46375,\"start\":46012},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4704285},\"end\":47000,\"start\":46377},{\"attributes\":{\"id\":\"b24\"},\"end\":47244,\"start\":47002},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":195908774},\"end\":47529,\"start\":47246},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":237289707},\"end\":47888,\"start\":47531},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":212784882},\"end\":48128,\"start\":47890},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":230101172},\"end\":48423,\"start\":48130},{\"attributes\":{\"doi\":\"arXiv:2003.04382\",\"id\":\"b29\"},\"end\":48773,\"start\":48425},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":18507866},\"end\":49142,\"start\":48775},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":211296583},\"end\":49427,\"start\":49144},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":219979590},\"end\":49729,\"start\":49429},{\"attributes\":{\"doi\":\"arXiv:1603.04779\",\"id\":\"b33\"},\"end\":50068,\"start\":49731},{\"attributes\":{\"id\":\"b34\"},\"end\":50226,\"start\":50070},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":201665971},\"end\":50592,\"start\":50228},{\"attributes\":{\"doi\":\"PMLR, 2020. 2\",\"id\":\"b36\",\"matched_paper_id\":211205159},\"end\":50946,\"start\":50594},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":232417486},\"end\":51171,\"start\":50948},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":556999},\"end\":51451,\"start\":51173},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b39\",\"matched_paper_id\":211252848},\"end\":52085,\"start\":51453},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":61019113},\"end\":52467,\"start\":52087},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b41\",\"matched_paper_id\":2630174},\"end\":52748,\"start\":52469},{\"attributes\":{\"doi\":\"arXiv:2106.14999\",\"id\":\"b42\"},\"end\":53246,\"start\":52750},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":788838},\"end\":53573,\"start\":53248},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":73497737},\"end\":53920,\"start\":53575},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":918513},\"end\":54252,\"start\":53922},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3548228},\"end\":54552,\"start\":54254},{\"attributes\":{\"id\":\"b47\"},\"end\":54921,\"start\":54554},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206596260},\"end\":55250,\"start\":54923},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":233423465},\"end\":55587,\"start\":55252},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":220266097},\"end\":55977,\"start\":55589},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":195811894},\"end\":56235,\"start\":55979},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":16858314},\"end\":56667,\"start\":56237},{\"attributes\":{\"doi\":\"PMLR, 2020. 3\",\"id\":\"b53\",\"matched_paper_id\":220301705},\"end\":57065,\"start\":56669},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":2759724},\"end\":57426,\"start\":57067},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":2413610},\"end\":57837,\"start\":57428},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":4929980},\"end\":58384,\"start\":57839},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3556146},\"end\":58690,\"start\":58386},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":227745438},\"end\":59009,\"start\":58692},{\"attributes\":{\"doi\":\"arXiv:2109.01087\",\"id\":\"b59\"},\"end\":59283,\"start\":59011},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":232278031},\"end\":59600,\"start\":59285},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":233423330},\"end\":59922,\"start\":59602},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":3513205},\"end\":60235,\"start\":59924},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":235254713},\"end\":60596,\"start\":60237},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":8485068},\"end\":60897,\"start\":60598},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":236881316},\"end\":61181,\"start\":60899},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":215745272},\"end\":61401,\"start\":61183},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":230104791},\"end\":61711,\"start\":61403},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":195317007},\"end\":62024,\"start\":61713},{\"attributes\":{\"doi\":\"arXiv:2110.04065\",\"id\":\"b69\"},\"end\":62292,\"start\":62026},{\"attributes\":{\"id\":\"b70\"},\"end\":62436,\"start\":62294},{\"attributes\":{\"doi\":\"PMLR, 2017. 3\",\"id\":\"b71\",\"matched_paper_id\":10409742},\"end\":62770,\"start\":62438},{\"attributes\":{\"doi\":\"arXiv:2110.09506\",\"id\":\"b72\"},\"end\":63052,\"start\":62772},{\"attributes\":{\"doi\":\"arXiv:2109.12746\",\"id\":\"b73\"},\"end\":63319,\"start\":63054},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":52954862},\"end\":63654,\"start\":63321}]", "bib_title": "[{\"end\":38680,\"start\":38604},{\"end\":39191,\"start\":39150},{\"end\":39462,\"start\":39398},{\"end\":40396,\"start\":40291},{\"end\":40783,\"start\":40723},{\"end\":41278,\"start\":41223},{\"end\":41968,\"start\":41938},{\"end\":42267,\"start\":42218},{\"end\":42472,\"start\":42432},{\"end\":42766,\"start\":42679},{\"end\":43874,\"start\":43810},{\"end\":44125,\"start\":44071},{\"end\":44883,\"start\":44834},{\"end\":45834,\"start\":45755},{\"end\":46085,\"start\":46012},{\"end\":46430,\"start\":46377},{\"end\":47309,\"start\":47246},{\"end\":47603,\"start\":47531},{\"end\":47929,\"start\":47890},{\"end\":48192,\"start\":48130},{\"end\":48870,\"start\":48775},{\"end\":49190,\"start\":49144},{\"end\":49497,\"start\":49429},{\"end\":50341,\"start\":50228},{\"end\":50700,\"start\":50594},{\"end\":51003,\"start\":50948},{\"end\":51233,\"start\":51173},{\"end\":51529,\"start\":51453},{\"end\":52171,\"start\":52087},{\"end\":52527,\"start\":52469},{\"end\":53297,\"start\":53248},{\"end\":53633,\"start\":53575},{\"end\":53975,\"start\":53922},{\"end\":54307,\"start\":54254},{\"end\":54687,\"start\":54554},{\"end\":54980,\"start\":54923},{\"end\":55350,\"start\":55252},{\"end\":55666,\"start\":55589},{\"end\":56032,\"start\":55979},{\"end\":56314,\"start\":56237},{\"end\":56753,\"start\":56669},{\"end\":57186,\"start\":57067},{\"end\":57520,\"start\":57428},{\"end\":57931,\"start\":57839},{\"end\":58453,\"start\":58386},{\"end\":58781,\"start\":58692},{\"end\":59341,\"start\":59285},{\"end\":59677,\"start\":59602},{\"end\":60003,\"start\":59924},{\"end\":60319,\"start\":60237},{\"end\":60658,\"start\":60598},{\"end\":60940,\"start\":60899},{\"end\":61239,\"start\":61183},{\"end\":61478,\"start\":61403},{\"end\":61773,\"start\":61713},{\"end\":62486,\"start\":62438},{\"end\":63410,\"start\":63321}]", "bib_author": "[{\"end\":38699,\"start\":38682},{\"end\":38717,\"start\":38699},{\"end\":38735,\"start\":38717},{\"end\":38750,\"start\":38735},{\"end\":38940,\"start\":38926},{\"end\":38958,\"start\":38940},{\"end\":38974,\"start\":38958},{\"end\":39207,\"start\":39193},{\"end\":39219,\"start\":39207},{\"end\":39233,\"start\":39219},{\"end\":39249,\"start\":39233},{\"end\":39477,\"start\":39464},{\"end\":39490,\"start\":39477},{\"end\":39505,\"start\":39490},{\"end\":39514,\"start\":39505},{\"end\":39528,\"start\":39514},{\"end\":39539,\"start\":39528},{\"end\":39552,\"start\":39539},{\"end\":39567,\"start\":39552},{\"end\":39794,\"start\":39781},{\"end\":39807,\"start\":39794},{\"end\":39815,\"start\":39807},{\"end\":40413,\"start\":40398},{\"end\":40428,\"start\":40413},{\"end\":40445,\"start\":40428},{\"end\":40459,\"start\":40445},{\"end\":40477,\"start\":40459},{\"end\":40495,\"start\":40477},{\"end\":40802,\"start\":40785},{\"end\":40825,\"start\":40802},{\"end\":40840,\"start\":40825},{\"end\":40861,\"start\":40840},{\"end\":41301,\"start\":41280},{\"end\":41317,\"start\":41301},{\"end\":41329,\"start\":41317},{\"end\":41347,\"start\":41329},{\"end\":41351,\"start\":41347},{\"end\":41668,\"start\":41650},{\"end\":41683,\"start\":41668},{\"end\":41696,\"start\":41683},{\"end\":41711,\"start\":41696},{\"end\":41719,\"start\":41711},{\"end\":41986,\"start\":41970},{\"end\":42003,\"start\":41986},{\"end\":42021,\"start\":42003},{\"end\":42037,\"start\":42021},{\"end\":42054,\"start\":42037},{\"end\":42285,\"start\":42269},{\"end\":42303,\"start\":42285},{\"end\":42485,\"start\":42474},{\"end\":42499,\"start\":42485},{\"end\":42507,\"start\":42499},{\"end\":42528,\"start\":42507},{\"end\":42783,\"start\":42768},{\"end\":42798,\"start\":42783},{\"end\":42809,\"start\":42798},{\"end\":42826,\"start\":42809},{\"end\":42838,\"start\":42826},{\"end\":42852,\"start\":42838},{\"end\":42865,\"start\":42852},{\"end\":42876,\"start\":42865},{\"end\":42893,\"start\":42876},{\"end\":42903,\"start\":42893},{\"end\":43280,\"start\":43265},{\"end\":43299,\"start\":43280},{\"end\":43535,\"start\":43520},{\"end\":43546,\"start\":43535},{\"end\":43560,\"start\":43546},{\"end\":43573,\"start\":43560},{\"end\":43588,\"start\":43573},{\"end\":43613,\"start\":43588},{\"end\":43890,\"start\":43876},{\"end\":43906,\"start\":43890},{\"end\":43919,\"start\":43906},{\"end\":44141,\"start\":44127},{\"end\":44153,\"start\":44141},{\"end\":44167,\"start\":44153},{\"end\":44180,\"start\":44167},{\"end\":44195,\"start\":44180},{\"end\":44208,\"start\":44195},{\"end\":44222,\"start\":44208},{\"end\":44238,\"start\":44222},{\"end\":44582,\"start\":44569},{\"end\":44595,\"start\":44582},{\"end\":44609,\"start\":44595},{\"end\":44896,\"start\":44885},{\"end\":44906,\"start\":44896},{\"end\":44916,\"start\":44906},{\"end\":44929,\"start\":44916},{\"end\":44943,\"start\":44929},{\"end\":44955,\"start\":44943},{\"end\":44965,\"start\":44955},{\"end\":44981,\"start\":44965},{\"end\":45367,\"start\":45355},{\"end\":45383,\"start\":45367},{\"end\":45396,\"start\":45383},{\"end\":45406,\"start\":45396},{\"end\":45419,\"start\":45406},{\"end\":45432,\"start\":45419},{\"end\":45445,\"start\":45432},{\"end\":45852,\"start\":45836},{\"end\":45867,\"start\":45852},{\"end\":46102,\"start\":46087},{\"end\":46116,\"start\":46102},{\"end\":46135,\"start\":46116},{\"end\":46152,\"start\":46135},{\"end\":46451,\"start\":46432},{\"end\":46467,\"start\":46451},{\"end\":46484,\"start\":46467},{\"end\":46497,\"start\":46484},{\"end\":46519,\"start\":46497},{\"end\":46534,\"start\":46519},{\"end\":46548,\"start\":46534},{\"end\":46559,\"start\":46548},{\"end\":46574,\"start\":46559},{\"end\":46603,\"start\":46574},{\"end\":47074,\"start\":47057},{\"end\":47091,\"start\":47074},{\"end\":47328,\"start\":47311},{\"end\":47344,\"start\":47328},{\"end\":47363,\"start\":47344},{\"end\":47626,\"start\":47605},{\"end\":47643,\"start\":47626},{\"end\":47655,\"start\":47643},{\"end\":47670,\"start\":47655},{\"end\":47688,\"start\":47670},{\"end\":47952,\"start\":47931},{\"end\":47967,\"start\":47952},{\"end\":47983,\"start\":47967},{\"end\":48209,\"start\":48194},{\"end\":48222,\"start\":48209},{\"end\":48235,\"start\":48222},{\"end\":48244,\"start\":48235},{\"end\":48256,\"start\":48244},{\"end\":48516,\"start\":48503},{\"end\":48529,\"start\":48516},{\"end\":48546,\"start\":48529},{\"end\":48561,\"start\":48546},{\"end\":48887,\"start\":48872},{\"end\":49201,\"start\":49192},{\"end\":49211,\"start\":49201},{\"end\":49224,\"start\":49211},{\"end\":49240,\"start\":49224},{\"end\":49261,\"start\":49240},{\"end\":49507,\"start\":49499},{\"end\":49521,\"start\":49507},{\"end\":49534,\"start\":49521},{\"end\":49543,\"start\":49534},{\"end\":49552,\"start\":49543},{\"end\":49556,\"start\":49552},{\"end\":49807,\"start\":49795},{\"end\":49820,\"start\":49807},{\"end\":49834,\"start\":49820},{\"end\":49847,\"start\":49834},{\"end\":49859,\"start\":49847},{\"end\":50083,\"start\":50070},{\"end\":50096,\"start\":50083},{\"end\":50354,\"start\":50343},{\"end\":50366,\"start\":50354},{\"end\":50714,\"start\":50702},{\"end\":50725,\"start\":50714},{\"end\":50738,\"start\":50725},{\"end\":51016,\"start\":51005},{\"end\":51027,\"start\":51016},{\"end\":51037,\"start\":51027},{\"end\":51251,\"start\":51235},{\"end\":51260,\"start\":51251},{\"end\":51274,\"start\":51260},{\"end\":51290,\"start\":51274},{\"end\":51549,\"start\":51531},{\"end\":51568,\"start\":51549},{\"end\":51585,\"start\":51568},{\"end\":51603,\"start\":51585},{\"end\":51618,\"start\":51603},{\"end\":52192,\"start\":52173},{\"end\":52200,\"start\":52192},{\"end\":52207,\"start\":52200},{\"end\":52547,\"start\":52529},{\"end\":52563,\"start\":52547},{\"end\":52583,\"start\":52563},{\"end\":52872,\"start\":52846},{\"end\":52889,\"start\":52872},{\"end\":52905,\"start\":52889},{\"end\":52922,\"start\":52905},{\"end\":52935,\"start\":52922},{\"end\":52955,\"start\":52935},{\"end\":53324,\"start\":53299},{\"end\":53339,\"start\":53324},{\"end\":53351,\"start\":53339},{\"end\":53357,\"start\":53351},{\"end\":53645,\"start\":53635},{\"end\":53660,\"start\":53645},{\"end\":53668,\"start\":53660},{\"end\":53676,\"start\":53668},{\"end\":53694,\"start\":53676},{\"end\":53708,\"start\":53694},{\"end\":53717,\"start\":53708},{\"end\":53987,\"start\":53977},{\"end\":54005,\"start\":53987},{\"end\":54021,\"start\":54005},{\"end\":54030,\"start\":54021},{\"end\":54041,\"start\":54030},{\"end\":54318,\"start\":54309},{\"end\":54345,\"start\":54318},{\"end\":54703,\"start\":54689},{\"end\":54717,\"start\":54703},{\"end\":55018,\"start\":54982},{\"end\":55036,\"start\":55018},{\"end\":55055,\"start\":55036},{\"end\":55064,\"start\":55055},{\"end\":55372,\"start\":55352},{\"end\":55385,\"start\":55372},{\"end\":55399,\"start\":55385},{\"end\":55687,\"start\":55668},{\"end\":55702,\"start\":55687},{\"end\":55713,\"start\":55702},{\"end\":55731,\"start\":55713},{\"end\":55748,\"start\":55731},{\"end\":55765,\"start\":55748},{\"end\":56050,\"start\":56034},{\"end\":56059,\"start\":56050},{\"end\":56073,\"start\":56059},{\"end\":56326,\"start\":56316},{\"end\":56343,\"start\":56326},{\"end\":56351,\"start\":56343},{\"end\":56763,\"start\":56755},{\"end\":56778,\"start\":56763},{\"end\":56790,\"start\":56778},{\"end\":56803,\"start\":56790},{\"end\":56817,\"start\":56803},{\"end\":56831,\"start\":56817},{\"end\":57205,\"start\":57188},{\"end\":57220,\"start\":57205},{\"end\":57534,\"start\":57522},{\"end\":57547,\"start\":57534},{\"end\":57557,\"start\":57547},{\"end\":57574,\"start\":57557},{\"end\":57592,\"start\":57574},{\"end\":57607,\"start\":57592},{\"end\":57952,\"start\":57933},{\"end\":57968,\"start\":57952},{\"end\":57981,\"start\":57968},{\"end\":57994,\"start\":57981},{\"end\":58009,\"start\":57994},{\"end\":58466,\"start\":58455},{\"end\":58477,\"start\":58466},{\"end\":58489,\"start\":58477},{\"end\":58497,\"start\":58489},{\"end\":58508,\"start\":58497},{\"end\":58522,\"start\":58508},{\"end\":58799,\"start\":58783},{\"end\":58813,\"start\":58799},{\"end\":58828,\"start\":58813},{\"end\":59026,\"start\":59013},{\"end\":59040,\"start\":59026},{\"end\":59056,\"start\":59040},{\"end\":59072,\"start\":59056},{\"end\":59088,\"start\":59072},{\"end\":59356,\"start\":59343},{\"end\":59372,\"start\":59356},{\"end\":59386,\"start\":59372},{\"end\":59403,\"start\":59386},{\"end\":59419,\"start\":59403},{\"end\":59689,\"start\":59679},{\"end\":59702,\"start\":59689},{\"end\":59715,\"start\":59702},{\"end\":59729,\"start\":59715},{\"end\":59740,\"start\":59729},{\"end\":60023,\"start\":60005},{\"end\":60036,\"start\":60023},{\"end\":60051,\"start\":60036},{\"end\":60331,\"start\":60321},{\"end\":60344,\"start\":60331},{\"end\":60356,\"start\":60344},{\"end\":60374,\"start\":60356},{\"end\":60390,\"start\":60374},{\"end\":60400,\"start\":60390},{\"end\":60673,\"start\":60660},{\"end\":60688,\"start\":60673},{\"end\":60702,\"start\":60688},{\"end\":60714,\"start\":60702},{\"end\":60726,\"start\":60714},{\"end\":60954,\"start\":60942},{\"end\":60967,\"start\":60954},{\"end\":60981,\"start\":60967},{\"end\":60994,\"start\":60981},{\"end\":61013,\"start\":60994},{\"end\":61018,\"start\":61013},{\"end\":61255,\"start\":61241},{\"end\":61271,\"start\":61255},{\"end\":61493,\"start\":61480},{\"end\":61506,\"start\":61493},{\"end\":61514,\"start\":61506},{\"end\":61528,\"start\":61514},{\"end\":61536,\"start\":61528},{\"end\":61785,\"start\":61775},{\"end\":61808,\"start\":61785},{\"end\":61825,\"start\":61808},{\"end\":61833,\"start\":61825},{\"end\":61847,\"start\":61833},{\"end\":61855,\"start\":61847},{\"end\":62098,\"start\":62086},{\"end\":62111,\"start\":62098},{\"end\":62122,\"start\":62111},{\"end\":62342,\"start\":62324},{\"end\":62359,\"start\":62342},{\"end\":62506,\"start\":62488},{\"end\":62517,\"start\":62506},{\"end\":62532,\"start\":62517},{\"end\":62786,\"start\":62772},{\"end\":62801,\"start\":62786},{\"end\":62815,\"start\":62801},{\"end\":62821,\"start\":62815},{\"end\":63135,\"start\":63122},{\"end\":63150,\"start\":63135},{\"end\":63422,\"start\":63412},{\"end\":63434,\"start\":63422},{\"end\":63460,\"start\":63434},{\"end\":63466,\"start\":63460}]", "bib_venue": "[{\"end\":38754,\"start\":38750},{\"end\":39263,\"start\":39249},{\"end\":39571,\"start\":39567},{\"end\":39892,\"start\":39831},{\"end\":40124,\"start\":40055},{\"end\":40499,\"start\":40495},{\"end\":40927,\"start\":40861},{\"end\":41355,\"start\":41351},{\"end\":41648,\"start\":41524},{\"end\":42058,\"start\":42054},{\"end\":42307,\"start\":42303},{\"end\":42536,\"start\":42532},{\"end\":42907,\"start\":42903},{\"end\":43263,\"start\":43179},{\"end\":43518,\"start\":43435},{\"end\":43923,\"start\":43919},{\"end\":44246,\"start\":44242},{\"end\":44567,\"start\":44460},{\"end\":45067,\"start\":44981},{\"end\":45530,\"start\":45461},{\"end\":45873,\"start\":45867},{\"end\":46174,\"start\":46152},{\"end\":46650,\"start\":46603},{\"end\":47055,\"start\":47002},{\"end\":47369,\"start\":47363},{\"end\":47692,\"start\":47688},{\"end\":47987,\"start\":47983},{\"end\":48260,\"start\":48256},{\"end\":48501,\"start\":48425},{\"end\":48942,\"start\":48887},{\"end\":49265,\"start\":49261},{\"end\":49560,\"start\":49556},{\"end\":49793,\"start\":49731},{\"end\":50131,\"start\":50096},{\"end\":50370,\"start\":50366},{\"end\":50755,\"start\":50751},{\"end\":51041,\"start\":51037},{\"end\":51294,\"start\":51290},{\"end\":51680,\"start\":51622},{\"end\":52244,\"start\":52207},{\"end\":52591,\"start\":52587},{\"end\":52844,\"start\":52750},{\"end\":53393,\"start\":53357},{\"end\":53732,\"start\":53717},{\"end\":54072,\"start\":54041},{\"end\":54385,\"start\":54345},{\"end\":54721,\"start\":54717},{\"end\":55068,\"start\":55064},{\"end\":55403,\"start\":55399},{\"end\":55771,\"start\":55765},{\"end\":56092,\"start\":56073},{\"end\":56427,\"start\":56351},{\"end\":56848,\"start\":56844},{\"end\":57226,\"start\":57220},{\"end\":57611,\"start\":57607},{\"end\":58023,\"start\":58009},{\"end\":58526,\"start\":58522},{\"end\":58832,\"start\":58828},{\"end\":59423,\"start\":59419},{\"end\":59744,\"start\":59740},{\"end\":60055,\"start\":60051},{\"end\":60406,\"start\":60400},{\"end\":60730,\"start\":60726},{\"end\":61022,\"start\":61018},{\"end\":61275,\"start\":61271},{\"end\":61540,\"start\":61536},{\"end\":61861,\"start\":61855},{\"end\":62084,\"start\":62026},{\"end\":62322,\"start\":62294},{\"end\":62589,\"start\":62545},{\"end\":62889,\"start\":62837},{\"end\":63120,\"start\":63054},{\"end\":63470,\"start\":63466},{\"end\":46684,\"start\":46652}]"}}}, "year": 2023, "month": 12, "day": 17}