{"id": 239009562, "updated": "2023-09-27 19:27:26.743", "metadata": {"title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "authors": "[{\"first\":\"Victor\",\"last\":\"Sanh\",\"middle\":[]},{\"first\":\"Albert\",\"last\":\"Webson\",\"middle\":[]},{\"first\":\"Colin\",\"last\":\"Raffel\",\"middle\":[]},{\"first\":\"Stephen\",\"last\":\"Bach\",\"middle\":[\"H.\"]},{\"first\":\"Lintang\",\"last\":\"Sutawika\",\"middle\":[]},{\"first\":\"Zaid\",\"last\":\"Alyafeai\",\"middle\":[]},{\"first\":\"Antoine\",\"last\":\"Chaffin\",\"middle\":[]},{\"first\":\"Arnaud\",\"last\":\"Stiegler\",\"middle\":[]},{\"first\":\"Teven\",\"last\":\"Scao\",\"middle\":[\"Le\"]},{\"first\":\"Arun\",\"last\":\"Raja\",\"middle\":[]},{\"first\":\"Manan\",\"last\":\"Dey\",\"middle\":[]},{\"first\":\"M\",\"last\":\"Bari\",\"middle\":[\"Saiful\"]},{\"first\":\"Canwen\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Urmish\",\"last\":\"Thakker\",\"middle\":[]},{\"first\":\"Shanya\",\"last\":\"Sharma\",\"middle\":[\"Sharma\"]},{\"first\":\"Eliza\",\"last\":\"Szczechla\",\"middle\":[]},{\"first\":\"Taewoon\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Gunjan\",\"last\":\"Chhablani\",\"middle\":[]},{\"first\":\"Nihal\",\"last\":\"Nayak\",\"middle\":[]},{\"first\":\"Debajyoti\",\"last\":\"Datta\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Mike\",\"last\":\"Jiang\",\"middle\":[\"Tian-Jian\"]},{\"first\":\"Han\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Matteo\",\"last\":\"Manica\",\"middle\":[]},{\"first\":\"Sheng\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Zheng\",\"last\":\"Yong\",\"middle\":[\"Xin\"]},{\"first\":\"Harshit\",\"last\":\"Pandey\",\"middle\":[]},{\"first\":\"Rachel\",\"last\":\"Bawden\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Trishala\",\"last\":\"Neeraj\",\"middle\":[]},{\"first\":\"Jos\",\"last\":\"Rozen\",\"middle\":[]},{\"first\":\"Abheesht\",\"last\":\"Sharma\",\"middle\":[]},{\"first\":\"Andrea\",\"last\":\"Santilli\",\"middle\":[]},{\"first\":\"Thibault\",\"last\":\"Fevry\",\"middle\":[]},{\"first\":\"Jason\",\"last\":\"Fries\",\"middle\":[\"Alan\"]},{\"first\":\"Ryan\",\"last\":\"Teehan\",\"middle\":[]},{\"first\":\"Stella\",\"last\":\"Biderman\",\"middle\":[]},{\"first\":\"Leo\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Tali\",\"last\":\"Bers\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Wolf\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Rush\",\"middle\":[\"M.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable, prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks. We finetune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All prompts and trained models are available at https://github.com/ bigscience-workshop/promptsource/ and https://huggingface.co/bigscience/T0pp.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2110.08207", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2110-08207", "doi": null}}, "content": null, "year": 2023, "month": 12, "day": 17}