{"id": 220686488, "updated": "2023-10-06 12:52:29.223", "metadata": {"title": "IBM Federated Learning: an Enterprise Framework White Paper V0.1", "authors": "[{\"first\":\"Heiko\",\"last\":\"Ludwig\",\"middle\":[]},{\"first\":\"Nathalie\",\"last\":\"Baracaldo\",\"middle\":[]},{\"first\":\"Gegi\",\"last\":\"Thomas\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Ali\",\"last\":\"Anwar\",\"middle\":[]},{\"first\":\"Shashank\",\"last\":\"Rajamoni\",\"middle\":[]},{\"first\":\"Yuya\",\"last\":\"Ong\",\"middle\":[]},{\"first\":\"Jayaram\",\"last\":\"Radhakrishnan\",\"middle\":[]},{\"first\":\"Ashish\",\"last\":\"Verma\",\"middle\":[]},{\"first\":\"Mathieu\",\"last\":\"Sinn\",\"middle\":[]},{\"first\":\"Mark\",\"last\":\"Purcell\",\"middle\":[]},{\"first\":\"Ambrish\",\"last\":\"Rawat\",\"middle\":[]},{\"first\":\"Tran\",\"last\":\"Minh\",\"middle\":[]},{\"first\":\"Naoise\",\"last\":\"Holohan\",\"middle\":[]},{\"first\":\"Supriyo\",\"last\":\"Chakraborty\",\"middle\":[]},{\"first\":\"Shalisha\",\"last\":\"Whitherspoon\",\"middle\":[]},{\"first\":\"Dean\",\"last\":\"Steuer\",\"middle\":[]},{\"first\":\"Laura\",\"last\":\"Wynter\",\"middle\":[]},{\"first\":\"Hifaz\",\"last\":\"Hassan\",\"middle\":[]},{\"first\":\"Sean\",\"last\":\"Laguna\",\"middle\":[]},{\"first\":\"Mikhail\",\"last\":\"Yurochkin\",\"middle\":[]},{\"first\":\"Mayank\",\"last\":\"Agarwal\",\"middle\":[]},{\"first\":\"Ebube\",\"last\":\"Chuba\",\"middle\":[]},{\"first\":\"Annie\",\"last\":\"Abay\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 7, "day": 22}, "abstract": "Federated Learning (FL) is an approach to conduct machine learning without centralizing training data in a single place, for reasons of privacy, confidentiality or data volume. However, solving federated machine learning problems raises issues above and beyond those of centralized machine learning. These issues include setting up communication infrastructure between parties, coordinating the learning process, integrating party results, understanding the characteristics of the training data sets of different participating parties, handling data heterogeneity, and operating with the absence of a verification data set. IBM Federated Learning provides infrastructure and coordination for federated learning. Data scientists can design and run federated learning jobs based on existing, centralized machine learning models and can provide high-level instructions on how to run the federation. The framework applies to both Deep Neural Networks as well as ``traditional'' approaches for the most common machine learning libraries. {\\proj} enables data scientists to expand their scope from centralized to federated machine learning, minimizing the learning curve at the outset while also providing the flexibility to deploy to different compute environments and design custom fusion algorithms.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.10987", "mag": "3044097461", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2007-10987", "doi": null}}, "content": {"source": {"pdf_hash": "6e9fb743a27d4d471d14b578f84cd0c57e9d3e55", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.10987v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3cd3519755b580d12537446ecf210f26a71ca554", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6e9fb743a27d4d471d14b578f84cd0c57e9d3e55.txt", "contents": "\nIBM Federated Learning: An Enterprise Framework White Paper V0.1\n\n\nHeiko Ludwig \nIBM Research\n\n\nNathalie Baracaldo \nIBM Research\n\n\nGegi Thomas \nIBM Research\n\n\nYi Zhou \nIBM Research\n\n\nAli Anwar \nIBM Research\n\n\nShashank Rajamoni \nIBM Research\n\n\nYuya Ong \nIBM Research\n\n\nJayaram Radhakrishnan \nIBM Research\n\n\nAshish Verma \nIBM Research\n\n\nMathieu Sinn \nIBM Research\n\n\nMark Purcell \nIBM Research\n\n\nAmbrish Rawat \nIBM Research\n\n\nTran Minh \nIBM Research\n\n\nNaoise Holohan \nIBM Research\n\n\nSupriyo Chakraborty \nIBM Research\n\n\nShalisha Whitherspoon \nIBM Research\n\n\nDean Steuer \nIBM Research\n\n\nLaura Wynter \nIBM Research\n\n\nHifaz Hassan \nIBM Research\n\n\nSean Laguna \nIBM Research\n\n\nMikhail Yurochkin \nIBM Research\n\n\nMayank Agarwal \nIBM Research\n\n\nEbube Chuba \nIBM Research\n\n\nAnnie Abay \nIBM Research\n\n\nIBM Federated Learning: An Enterprise Framework White Paper V0.1\n\nFederated Learning (FL) is an approach to conduct machine learning without centralizing training data in a single place, for reasons of privacy, confidentiality or data volume. However, solving federated machine learning problems raises issues above and beyond those of centralized machine learning. These issues include setting up communication infrastructure between parties, coordinating the learning process, integrating party results, understanding the characteristics of the training data sets of different participating parties, handling data heterogeneity, and operating with the absence of a verification data set.IBM Federated Learning provides infrastructure and coordination for federated learning. Data scientists can design and run federated learning jobs based on existing, centralized machine learning models and can provide high-level instructions on how to run the federation. The framework applies to both Deep Neural Networks as well as \"traditional\" approaches for the most common machine learning libraries. IBM Federated Learning enables data scientists to expand their scope from centralized to federated machine learning, minimizing the learning curve at the outset while also providing the flexibility to deploy to different compute environments and design custom fusion algorithms. the data are generated at a high velocity or the amount of data is large. A similar situation holds in edge computing scenarios in which, say, data in telecommunication switches is used for training.In recent years, privacy regulations limit the use of personal data, make central data repositories expensive to manage, and represent a liability to the company storing the data. Regulations such as the European Union's General Data Protection Regulation (GDPR)[26], the Health Insurance Portability and Accountability Act of 1996 (HIPAA)[6], and others require data holders to keep information private and limit how data can be used. Private and confidential data have been compromised from different systems in the past years, with notable security incidents including[1,15,23,30]. These data breaches result in costly mitigation, fines, and reputation damage[23].Many privacy regulations also contain limitations on the location where data can be stored. Often, they stipulate that data cannot be taken out of the country to which the regulation applies or at least not to countries that have weaker privacy protections. This even applies to data of the same company if it has, say, customer databases in different countries, and presents a serious obstacle to effective use of data in a training process.Lastly, in some scenarios, different enterprises cooperate in training a machine learning model, mostly for non-competitive reasons. Financial institutions collaborate to combat money laundering and fraud -as required by regulation -and medical research facilities work jointly on improving diagnostics and treatment design[29]. However, due to regulation and secrecy, financial firms will not share transaction data and medical institutions cannot share patient data. While practical and regulatory reasons limit the centralization of training data, using these different sources of data is still important in many cases to build models on large and representative training datasets.Federated learning (FL)[19]is an approach to train machine learning models that do not require sharing datasets with a central entity. In federated learning, a model is trained collaboratively among multiple parties, which keep their training dataset to themselves but participate in a shared federated learning process. The notion of parties might refer to entities as different as data centers of an enterprise in different countries, compute clusters in different clouds, cell phones, cars, or different companies and organizations.The learning process between parties most commonly uses a single aggregator, while peer-topeer or decentralized models have also been proposed[27]. An aggregator would coordinate the overall process, communicate with the parties, and integrate the results of the training process. Most typically, in particular for neural networks, parties would run a local training process on their training data, share the weights of their model with the aggregator, which would then aggregate the weight vectors of all parties using a fusion algorithm. Then, the merged model is sent back to all parties for the next round of training.Classical machine learning models can also be adapted to a federated environment. We have to decide which parts of the federated learning algorithm would run locally in parties and which would run in an aggregator. A federated decision tree algorithm might grow the tree in the aggregator, and query the parties for the count information based on the parties' data sets. Based on these counts, it would compute the information gains for all possible splits and select the feature with the largest information gain to split and built the tree node, and then iterate the next round. There are many choices on how to design federated machine learning algorithms, and this is still a very active field of research at the point of writing this paper.FL stands in contrast to established distributed training systems[17,21], where all data is transmitted to a central data center and is subsequently distributed among cluster nodes to train in parallel. This clustered, non-federated approach benefits from understanding the characteristics of the entire training data set and the computational capabilities of the nodes in the cluster, as well as its ability to partition the dataset into convenient chunks among nodes in a cluster. These assump-\n\nIntroduction\n\nFederated Learning (FL) is an approach to apply machine learning to situations in which data cannot be centralized for a training process. The success of using machine learning to solve a problem depends, to a large extent, on the quality and quantity of available training data. Machine learning approaches typically rely on the central management of training data, even if the training process itself is run on a clustered infrastructure. The process can consider the properties of the total training data set and the availability of representative validation data sets, e.g., for hyperparameter tuning. However, centralizing data for training is often not feasible or practical, for reasons of data privacy, secrecy, regulatory compliance, and the sheer volume of data involved.\n\nEnterprises have their applications deployed in a variety of data centers and clouds. In such a multi-cloud scenario, moving data into one location can be impractical or expensive, particularly if tions do not hold in a federated setting.\n\nChallenges of FL arise from different perspectives including data heterogeneity, robustness of the federation process, selection of unbiased fusion operators, security and privacy inference prevention and operational and effective deployment in enterprise and multi-cloud settings, among others. While FL does not require the centralized management of training data, thus enabling machine learning to scenarios in which that is not possible, it also poses some new challenges, such as data set heterogeneity. In this context, there is no common view of the stochastic properties of the overall training data set, which precludes machine learning dependent on them, established pre-processing methods, fine-tuning algorithms and evaluation of model performance. This includes differences in distribution of data among participants, different dataset sizes for each party, different attributes and general issues of heterogeneity [11]. [16] provides a good overview of some challenges of federated learning.\n\nPrivacy demands might also add additional restrictions to a federated learning system. For example, while membership inference attacks [20], where an adversary may try to infer the training data used during the training process by querying the final model, are possible in both centralized and federated learning settings, additional measures need to be considered in federated learning cases. Some federated learning scenarios require that different parties are not able to derive insights about each other's training data based on messages exchanged during the training process (e.g., weights). Additionally, in some other cases, not even the aggregator may be trusted to see this information. Methods of creating differentially private noise or secure multi-party computation are used to meet these privacy requirements [8,13,31,33]. This is mostly the case when data is owned by different enterprises or individuals, an example being data on phones. Multi-cloud settings may have a lesser need for this.\n\nAnother set of challenges relates to needs of the enterprise. (1) FL requires different skills from machine learning to distributed systems to cryptography. It's rare to find employees who have all these skills sets. An FL platform must provide a seamless on-ramp for its primary user base, the machine learning professional. (2) An FL platform must deal with operational complexity. An FL process requires a more complex infrastructure and processes to enroll parties, distribute models, set up an aggregator, deploy a federated learning algorithm, conduct the federated learning process, and then make a final model available. The system must be resilient to parties with different characteristics and parties dropping out of the training process. (3) Security and networking are important to a federated learning solution. Given that FL is typically used where domain boundaries have to be dealt with, networking must suit the deployment needs and facilitate the required security. However, the setup of secure communication should not be burdensome to deploy, avoiding timeconsuming processes such as opening ports as much as possible.\n\nA few approaches and libraries for FL address some of these issues, including [9], [28], and [4]. However, IBM Federated Learning focuses on enterprise settings where secure deployment, failure tolerance, and fast model specification are paramount; these must use existing machine learning libraries that allow enterprise users to take advantage of a comprehensive set of state-of-the art algorithms without needing to learn new languages.\n\nWe propose the IBM Federated Learning framework to address these challenges and facilitate an easy integration of FL into productive machine learning workflows in the enterprise. Data scientists and machine learning professionals can benefit from a seamless transition from existing practices of model development to writing federated machine learning models. FL infrastructure must be easy to deploy by an enterprise and fit into the typical operations patterns of IT infrastructure. Finally, researchers in federated learning, those interested in designing novel federated learning algorithms and protocols, can try out their ideas with ease and build on the existing functionality for the specific needs of their organization or the particular application domain. The goal of IBM Federated Learning is to address all of these needs in an easy-to-use framework.\n\nThe remainder of this white paper is organized as follows. In section 2, we present the concepts and terminology. Then in section 3, we present the architecture of system. Section 4 demonstrates how to train different types of models; in particular, we show how neural networks and decision trees can be incorporated into the framework. In section 5, we show how IBM Federated Learning is implemented and how it can be configured, and we conclude in section 6.\n\n\nConcepts and Terminology\n\nLike any machine learning task, FL trains a model M over data D. M can be a neural network or any non-neural model. In contrast to centralized machine learning, D is split over n parties, where each party P i has its own private training dataset D i . An FL process involves an aggregator A and those n parties P 1 , P 2 , ..., P n in a way that no party has knowledge of any other dataset than its own. A has no knowledge of any dataset.\n\nThe FL process is shown in Figure 1. To train a global machine learning model M G the aggregator and the parties participate in a federated learning algorithm that is executed by the aggregator and the parties by sending messages. The overall process runs as follows:\n\n1. To train M G , the aggregator uses a function Q that takes as input the current model or state of the training M t at round t, and generates a next query q t+1 1 .\n\n2. One such query, q t , requests information about a local model or aggregated information about each party's data set. Example queries include requests for gradients or model weights of a neural network, or counts for decision trees.\n\n3. The local training process applies a function L that takes query q t and the local dataset D i and outputs a model update r i,t . Usually the query, q t , contains information that the party can use to initialize the local training process, for example, model weights to start with local training, or candidate feature values and/or class labels to compute counts for. 4. r i,t is sent back from party P i to the aggregator A, which collects all the r i,t from parties P i .\n\n5. When parties model updates r 1,t , r 2,t , ..., r n,t , where r i,t refers to the model update of party i at round t, are received by the aggregator forming set R t = {r 1,t , r 2,t , ..., r n,t }, they are aggregated by applying fusion function F that takes as input R t and returns M t .\n\nThis process can be executed over multiple rounds t and continues until a termination criterion is met, e.g., the maximum number of training rounds k has elapsed, resulting in a final global model M G = M k . The number of rounds required can vary highly, from a single model merge of a Naive Bayes model to many rounds of training for typical gradient-based machine learning algorithms.\n\nThe local training function L, the fusion function F, and the query generation function Q are typically a complimentary set that are designed to work together. L interacts with the actual dataset and performs the local training, generating the model update r i,t . The content of R t is the input to F and, thus, must be interpreted by F, which creates the next model, M t , from this input. If another round r is required, Q then creates another query. In case of a neural network, for example, L is the Figure 1: Federated learning concepts where each party P i owns its own dataset D i . local neural network training algorithm that might produce a weight vector as model update. F is a fusion algorithm such as a federated average that simply averages each weights over the parties P i , resulting in a new model M t . For the next round, Q would then pass M t as part of the new query q t+1 . In this case, L is computationally expensive and F less so. If the decision tree is trained in a federated way, one approach might be to balance the decision tree in the aggregator: implement F, send leaf node definitions as part of q t , L create a response based on a number of data samples corresponding to each leaf node in D i , and then balance the tree again in the aggregator. In this case, F is computationally expensive and L is not.\n\nWe can introduce different variants to this basic model. While an approach with a single aggregator is the most common and practical for most scenarios, we might choose other configurations. For example, each party P i might have its own, associated aggregator A i , querying the other parties. Function Q might determine the parties to query in each round perhaps based on the merits of prior contributions. Queries to each party might be different, with F needing to integrate the results of different queries in the creation of a new model M t . However, for the further discussion of IBM Federated Learning we will focus on FL with a single aggregator.\n\n\nArchitecture\n\nIBM Federated Learning is a Python library designed to support the machine learning process shown in Figure 1, while enabling an easy set up in a real distributed environment.\n\nIBM Federated Learning is designed to implement a resilient platform as well as to ensure the easy implementation of new FL algorithms. The IBM Federated Learning library contains the components implementing an aggregator A and a party P i as shown in Figure 2.\n\nThis modular design allows the framework to provide communication infrastructure independently from the federated learning algorithm and the actual machine learning library that performs local training. It also enables all parties P i to read and preprocess data from different locations in different formats. The architecture has the following components:\n\n\u2022 Connection: Networking is often critical to adoption of any distributed system. The ability to deal with specific requirements and constraints such as available ports, bandwidth, connection stability and latency is important to managing the complexity of deploying a FL system. Multi-Cloud scenarios have different requirements from mobile phone, edge, or company collaboration scenarios. All functionality related to network connectivity for communication between the aggregator and each registered party is handled by the FLConnection component. IBM Federated Learning can support multiple connection types, including the Flask web framework [2], gRPC [3], and WebSockets.\n\n\u2022 Protocol handler: This component governs message exchange between parties, i.e. the learning protocol. The message set of the protocol includes a query q, a model update r, and other messages to establish and dismantle the FL configuration, such as party registration. It uses the FLConnection for communication and is used by components higher in the stack. It implements which types of messages an aggregator or party can receive at a given point in time. The protocol handler is slightly different for the aggregator and the party stacks due to the differences in messages exchanged. The aggregator contains a ProtoHandler to manage the protocol to the set of parties P 1 , ..., P n , and on the party side, the PartyProtoHandler implements this function. The party and aggregator side protocol handlers can be adapted to the needs of a particular FL scenario by providing a specialized protocol handler pair.\n\n\u2022 Data handler: A DataHandler is responsible for accessing and pre-processing the local dataset D i at a Party P i . When running an FL process, the data from each party must be in the correct format so that the learning algorithm can make use of it. While in some FL scenarios, such as mobile phone applications, one will find the same data structure at each party, enterprise scenarios often face different data structures in applications in different clouds or in data centers of different companies. The DataHandler provides the abstraction to import data specifically for a party. The DataHandler is designed as an easily customizable module for each party to prepare their data to run. Other IBM Federated Learning modules will only interact with the data handler via limited APIs, for instance by get data to access training and testing data sets.\n\n\u2022 FL training modules: FL machine learning algorithms are incorporated into IBM Federated Learning by specifying a FusionHandler and a LocalTrainingHandler for the aggregator and parties, respectively.\n\n-FusionHandler: This module contains the specification of functions Q to generate queries and F to fusion model updates. The module uses high level APIs provided by the ProtocolHandler to send and receive messages generated by Q and also obtain all party replies to aggregate model updates using the specified F function. Different implementations of Q and F may require different information to be exchanged. For example, in some Q implementations a message may be added in addition to the query hyperparameters. To accommodate this differences of the information exchanged by the FushionHandler, IBM Federated Learning makes use of a dictionary, a generic data structure with customizable fields.\n\n-LocalTrainingHandler: This module is used to specify function L that will be run at the parties' side, to generate model updates and send them to the aggregator. The LocalTrainingHandler may use a FLModel.\n\n-FLModel: This module provides an interface to define a standard API to train, save, evaluate, and update the model, as well as generate model updates to be specified in different ML libraries, such as Keras or scikit-learn. For each ML library, a module is instantiated to wrap all the offered API (e.g., evaluate, train, save, update model). In this way, the FusionHandler and the LocalTrainingHandler can use the standard API and, as a result, be used without changes for multiple supported ML libraries.\n\n\nAggregator stack\n\nThe aggregator stack contains the components implementing an aggregator A. Its role includes the coordination of the federated learning process, the execution of F, the fusion algorithm, and persisting the meta-data of the FL process. The metadata about a federated learning process includes information such as the list of parties, the current state of the process, logging information, and aggregated models M t . It also performs management tasks such as starting and closing the training process.\n\nAs shown in Figure 2, the aggregator works with the ProtoHandler and the FLConnection to communicate with data parties. The FusionHandler serves to generate queries and to aggregate received replies. In some cases, it is desirable to know the performance of a global model during the training process, for instance to evaluate early termination criteria. In such cases, the aggregator may have access to some testing samples which are accessed through an optional DataHandler.\n\nThe aggregator provides an interface to the users to issue commands to control the overall federated learning process. Figure 3 shows different phases of the aggregator which include REGISTERING, TRAINING, SYNCING, EVALUATING, STOPPING and PROCESSING ERROR. Once the aggregator is started, it waits for the data parties to register. Once the registration process is completed and there is a quorum, a user can issue a TRAINING request to start the training process, which triggers the execution of the FusionHandler. During this process, the aggregator uses the ProtoHandler and the FLConnection to send the queries generated by the FusionHandler. Each party responds to this request by sending the trained model updates. Making use of the FusionHandler, the aggregator computes F to aggregate all model updates and generate the queries for parties. This process is repeated until we reach a termination criteria, i.e. desired number of training rounds or specific model accuracy.\n\nOnce termination criteria is reached, the aggregator sends a final model update to all the parties via a SYNCING command which distributes the final global model M G . Next, the aggregator may request parties to perform a local evaluation before stopping the federated learning process. If an error arises during any of these steps, the aggregator logs it and stops the federated learning process. IBM Federated Learning collects metadata that allows for failure recovery by going to a previous valid phase.\n\n\nParty stack\n\nSimilar to the aggregator, the party side consists of its own protocol handler, connection, model, local training, and data handler. Unlike the aggregator, the DataHandler is mandatory for parties. The party provides a command interface for users to start interacting with the application. Each party starts with issuing a REGISTER command to join a federated learning process. After the registration process is completed, the party waits for the aggregator to send a message with a query q to start running the LocalTrainingHandler, which executes L based on the received q and produces a reply r. Then, r is sent back to the aggregator. This process is repeated until the aggregator sends a request to stop the federated learning process.\n\nIn some cases, the aggregator may also issue a SYNC request, which includes the final global model M G .\n\n\nSupporting different learning paradigms\n\nIn the past few years, a variety of algorithms have emerged for training different types of ML models via federated learning, e.g., [7,10,14,19,31,32,33,34,35]. Since FL is an approach to perform collaborative learning with the help of an aggregator for coordination, existing FL algorithms usually contain two parts: 1) A global training module, the FusionHandler, mainly contains the fusion function F, and the query generation function Q; 2) A local training module, the LocalTrainingHandler, contains the local training function L. They come in pairs to execute a FL algorithm in IBM Federated Learning. However, different federated learning algorithms vary significantly in several aspects: how and where the global model M G is updated; the query generation function Q; the fusion function F; the function L preformed by the parties and the content of model update r, among others. IBM Federated Learning is designed to be flexible enough to accommodate different learning paradigms in FL. We cover the design of several different FL algorithms for IBM Federated Learning below.\n\n\nNeural networks\n\nIn this subsection, we focus on the discussion of training neural networks via iteratively updating M G by the (weighted) average of local models' parameters.\n\nIn particular, this type of FL algorithm requires the aggregator to generate a query, q t , containing the current global model weights -and hyperparameters, optionally -and send it to all or a subset of parties at round t.\n\nThis query is implemented as a dictionary. Once parties receive q t , each party initializes its local model with the global model weights, performs several epochs of local training with the received hyperparameters, if any, to obtain the model update r t . The resulting r t contains the new set of local model weights, and shares it with the aggregator. In some variations, where a weighted average is used (FedAvg [18]), the number of data points used for training at each party is included in r t . The model update r t is implemented as a dictionary, and so, when FedAvg is executed, two key values are included into the dictionary with key values weights, nsamples. In some cases, for privacy considerations, the number of samples at each party cannot be transmitted to the aggregator. In those cases, the model update only has a single-item dictionary. The aggregator then follows the corresponding fusion function F to conduct a (weighted) average over the collected r i,t 's and updates the global model's weights with the fusion results. Both the global and local models' structure, i.e., the neural network architecture, stay the same.\n\nWhat does this mean for the FusionHandler? Implementing the aforementioned type of FL algorithms, IterAvgFusionHandler, a subclass of FusionHandler, which, after being invoked by the aggregator, iteratively constructs a query containing the current weights of M, and sends the query via the API offered by the protocol handler; this, in turn utilizes the connection layer to send the messages to parties upon receiving enough reply updates M G with the fusion results of F. Depending on the fusion algorithms, the fusion function F can be a simple average of the collected r i,t 's as implemented in IterAvgFusionHandler, or a weighted average whose weights depend on the parties' sample size as implemented in FedAvgFusionHandler. Note that the latter fusion function expects the replies r i,t to contain the parties' sample size in addition to the local model's weights.\n\nThe LocalTrainingHandler module implements the local training function L on the party side. The LocalTrainingHandler triggers the party to perform a predefined number of training epochs on the local model, and constructs a model update r containing the information requested by the corresponding FusionHandler. For example, a model update replying to the query constructed by IterAvgFusionHandler will contain the new set of local model weights, while a model update replying to the query constructed by FedAvgFusionHandler will contain the local sample size along with the local model weights.\n\nIBM Federated Learning employs the FLModel module to allow the users to provide model definitions in standard ML libraries and to provide a single API to a pair of a FusionHandler and a LocalTrainigHandler. As a concrete example, a data scientist can specify a convolutional neural network using the standard Keras library [12]. KerasFLModel wraps the functionality for initializing the model according to the specified Keras model definition, training, creating a model update (extracting model weights from the neural network) and saving the final model.\n\nFinally, it is worth mentioning that IBM Federated Learning also includes other FLModel modules. Our design ensures that the same FL algorithm, i.e., the same pair of a FusionHandler and a LocalTrainigHandler, can be used to train different models specified using different ML libraries. For instance, the two FL algorithms we discuss above can also be applied to train Scikit-learn [24] linear models in IBM Federated Learning.\n\n\nDecision trees\n\nTraining a decision tree in a federated setting requires a different approach to implementing a federated learning algorithm than neural networks do. We illustrate this for an adapted ID3 algorithm. Since this type of decision tree deals with discrete feature values, its FL algorithm is very different than those we have discussed in the previous section.\n\nIBM Federated Learning supports a FL variant of the ID3 algorithm [25], where the tree is grown at the aggregator side and the local training function, L only performs light computations to generate replies r containing the counts information. No initial model structure is provided to the aggregator and parties, and thus the aggregator starts with a null tree root node.\n\nDuring the training process, the query generation function Q takes the current list of candidate feature values for splitting and class labels to query the parties for their counts information. The local training function L computes the corresponding counts based on the party's local dataset, and the resulting model update r is shared with the aggregator. The fusion function F splits the current tree node according to the information gain computed based on the collected counts. When the tree reaches the maximum depth or all tree nodes have no candidate feature values to split, the training process ends.\n\nThe ID3FusionHandler, invoked by the aggregator, implements the core parts of the ID3 FL algorithm, i.e., the fusion function F and query generation function Q. Q recursively takes the updated list of candidate feature values, and constructs the query q with the candidate list. The query q is sent the same way to parties as by another FusionHandler.\n\nAfter collecting a quorum of replies r i,t , i.e., the list of counts sent by the parties, the fusion function F computes the sums of the counts over all parties with respect to their corresponding feature values and recovers the overall counts for the candidate feature values. Following the information gain formula used by the ID3 algorithm, F chooses the feature value with the maximum information gain to split and construct the resulting tree nodes according to the predefined criterion, whether reaching the maximum tree depth or having a empty candidate list. It also updates the candidate list and moves on to the new constructed tree node. When the training process finalizes all leaf nodes, the tree is grown at the aggregator side.\n\nAs we can see ID3FusionHandler performs the computation-heavy operations at the aggregator, different from the two FL algorithms we have discussed in Section 4.1 to train neural networks where the computation expensive training happens at the party side. The LocalTrainingHandler triggers a light operation conducted by the parties to compute counts information and constructs the model update r containing these counts.\n\nIn the case of an ID3 decision tree, IBM Federated Learning implements the DTFLModel independently of any existing ML library. It follows the same convention as other FLModel to implement a list of methods, such as fit model, update model, get model update, evaluate model and save model, etc., so that other modules can interact with it the same way as with other types of ML models.\n\n\nOther models, differential privacy and multi-party computation\n\nThe flexible architecture of IBM Federated Learning also supports the implementation of other FL algorithms as well as the use of differential privacy and multi-party computation. In the following, we discuss how to use these approaches are implemented based on this library.\n\nOther models and training algorithms: IBM Federated Learning supports the training of different machine learning models besides neural networks and decision trees. These include adaptations of XGBoost for binary and multi-class classification as well as regression, linear classifiers and regressions with regularizers including logistic regression, linear SVM, ridge regression and more. The framework also comes with Nave Bayes and Deep Reinforcement Learning algorithms including DQN, DDPG, and PPO, among others.\n\nWith respect to FL algorithms, several common and advanced algorithms are now part of the framework's algorithm library. The set of common algorithms includes average and weighted aggregation, FedAvg [18], as described above. We have implemented multiple advanced algorithms including SPAHM [34] which uses a statistical model aggregation via parameter matching for supervised and unsupervised learning, and PFNM [35], which provides a communication efficient method for federated learning of fully-connected networks with adaptive global model size.\n\nAlgorithms to improve robustness such as Krum [7], a Byzantine tolerant gradient descent coordinate-wise median fusion [14], and Zeno, a distributed stochastic gradient descent with suspicionbased fault-tolerance [32] are also available. New algorithms, as needed, can be easily implemented within the existing framework and added back to the library.\n\nPrivacy-aware algorithms Real federated learning scenarios may require different techniques to ensure the federated learning process can be performed.\n\nIn particular, in some cases, participants may have limited trust amongst each other or in the aggregator. In these cases, it is imperative to add additional protection mechanisms, including secure multi-party computation and differential privacy.\n\nThe right technique to apply largely depends on the threat model of choice. Techniques such as local differential privacy [8], multi-party computation (SMC) [13,33] and hybrid approaches have been proposed for this purpose [31]. Secure multi-party computation techniques allow for the private aggregation of inputs among parties, where a function value can be computed without revealing the function's inputs. Among relevant algorithms for this purpose include [13], which utilizes partial homomorphic encryption, [33], which makes use of functional encryption to drastically speed up the training process in comparison to homomorphic-based approaches, and [31], where differential privacy and SSM (threshold-based Pailler scheme) are combined to create highly accurate models without compromising the offered differential privacy guarantee.\n\nSome of the previously mentioned SMC techniques require a different number of messages exchanged. For instance, while the approach presented in [33], based on functional encryption, only requires the aggregator to send a single query, q and participants to send a single encrypted model update, r, techniques like [31] require multiple communication rounds between the aggregator and parties. Ideally, all these nuances should be hidden, ensuring that a crypto-aware Fushion Handler and LocalTrainingHandler do not need to be specifically adapted for each type of cryptosystem IBM Federated Learning provides a simple API to incorporate a diverse set of cryptosystems that enable machine learning professionals to invoke crypto operations in the Fushion Handler and LocalTrainingHandler. We designed this part of the system to ensure cryptosystems can be interchanged without modifying a crypto-aware pair of a Fusion Handler and a LocalTrainingHandler.\n\nWe have incorporated several SMC techniques including partial homomorphic encryption [13], the approach proposed in [33] and the functional approach presented in [31].\n\nIBM Federated Learning also provides the building block to add differential privacy for different types of models varying from very simple Naive Bayes to more complex differential privacy mechanisms as those required for neural networks. For the first type of cases, a one-shot differential privacy addition that can be applied by a mechanism at the LocalTrainingHandler. For other models, an accounting module is added at the aggregator side to keep tract of the differential privacy budget. Multiple variations on differential privacy methods may be implemented including [5,22].\n\nMachine learning libraries Finally, IBM Federated Learning was designed to ensure ML professionals can specify their models using common ML libraries. Our de-coupled design ensures that implemented FusionHandlers and LocalTrainingHandlers that use the APIs offered by FLModel can be reused for different libraries. Currently, we include FLModel implementations for Keras, Pytorch, Tensorflow, Scikit-learn and RLlib. This list can be extended by users implementing the interface of FLModel for other libraries.\n\n\nDeployment and Configuration\n\nDeploying the IBM Federated Learning environment involves deploying an aggregator stack and a party stack at each of the environments involved. Since IBM Federated Learning is a Python library, its deployment follows the same workflow as other Python packages, configuring its dependencies. The party library is typically installed where the local training will take place.\n\nIBM Federated Learning is designed to allow different modules to be swapped in and out without interfering with the functionality of other components. This requires users to configure the choices that work for their configuration. Party and aggregator stacks can be configured via an external API configuration, or driven by a configuration file (YAML), which are different for the aggregator and each party. IBM Federated Learning provides scripts to generate configuration files with default settings which users can modify according to their requirements.\n\nWe now briefly discuss a list of building blocks that can be configured in IBM Federated Learning.\n\nConnection: It is configured to provide information needed to initiate the connection between the aggregator and parties. For example, flask server information, synchronization mode for training requests, Transport Layer Security (TLS) configurations, etc.\n\nData: It is configured to provide information needed to locate and load the local data sets and perform any pre-processing techniques.\n\nFederated learning algorithm: IBM Federated Learning supports a variety of FL algorithms to train different types of machine learning models. Configuring a federated learning algorithm usually contains two parts: 1) FusionHandler is configured to provide information about the Fusion algorithm used at the aggregator; 2) LocalTrainingHandler is configured to provide information about the L function.\n\nHyperparameters: It is configured to set up global and local training hyperparameters, e.g., global training round number, local training epochs, batch-size, learning rate, etc.\n\nModel: It is configured to provide information needed to initiate a machine learning model, which can be defined via existing ML libraries, like Keras and Scikit-learn among others, or a self-implemented model class. Protocol handler: It is configured to provide information needed to initiate a protocol which oversees the message exchange between the aggregator and parties.\n\nAfter setting up the FL job details in configuration files, users can run a federated learning job in IBM Federated Learning.\n\nThe aggregator and party stacks come with a main application to run independently, or can be integrated into another application. To start the aggregator, a user sets up the proper environment with IBM Federated Learning installed, and launches the aggregator application with the aggregator configuration file. As shown in Figure 4, the aggregator application can be launched in a cloud or cluster environment.\n\nSimilarly, the party applications are launched in an environment with IBM Federated Learning installed and the individual parties' configuration files are provided to specify the FL job settings. After the aggregator application starts running, parties can register themselves via the REGISTER command. The aggregator waits for a quorum to start the FL training process using the TRAIN command.\n\nThe following table provides an overview of the commands of aggregator and party. In the applications included in the library they can be issued on the command line. The users can observe the FL training process via log information. Once the FL training finishes, the main applications provide a list of commands to be used to collect the FL results, see Table 1. If the library is embedded in an application it can be controlled and monitored in its contained application context.\n\n\nConclusion\n\nThis white paper introduces IBM Federated Learning, a Python framework for federated learning in the enterprise. This framework is a platform for executing federated learning algorithms by providing all of the basic elements of a federated learning infrastructure in an easily configurable way. On the one hand, it provides hooks to implement the party and aggregator parts of a federated learning algorithm. On the other hand, it is configurable to a variety of deployment scenarios, from mobile and edge scenarios, to multi-Cloud environments in an enterprise, to use cases across organizational boundaries. IBM Federated Learning is independent of a particular machine learning library or machine learning paradigm. It can be used for deep neural networks as well as \"traditional\" approaches such as decision trees or support vector machines. The abstractions for different model types provide a high level of reuse of federated learning algorithms for different machine learning libraries. A fusion algorithm for a deep neural network in Keras works as well with a native PyTorch one. Data access can be defined for each party independently. This is particularly important in the enterprise space where data stored in different data centers, Clouds or application silos can be brought together for a common learning task.\n\nThe key design point of IBM Federated Learning is fast start-up time for enterprise applications. There is a large library of federated learning algorithms implemented on IBM Federated Learning to get users started. Machine learning models tested successfully in a centralized learning application can often be extended to use data in different parties without rewriting the model code, but simply by editing configuration files. Party code is easy to deploy and, depending on the communication layer chosen, might not require opening ports on the party side, which makes deployment of a federated project faster and easier. Using IBM Federated Learning, enterprise practitioners can deploy federated learning quickly. In addition, researchers in the field can build on its existing platform to try out new federated learning algorithms and benchmark them against the existing ones in the library.\n\nFigure 2 :\n2IBM federated learning architecture stack.\n\nFigure 3 :\n3Aggregator phases.\n\nFigure 4 :\n4Flow to execute a FL job using IBM Federated Learning.\n\nTable 1 :\n1A list of commands for IBM Federated Learning SAVE aggregator / party Save the model locally at party sideIBM FL Command \nParticipant \nDescription \nSTART \naggregator / party \nStart accepting connections \nREGISTER \nparty \nJoin an FL job \nTRAIN \naggregator \nInitiate training process \nSYNC \naggregator \nSynchronize model among parties \nSTOP \naggregator / party \nEnd experiment process \nEVAL \nparty \nEvaluate model \n\nSome FL algorithms may include additional inputs for Q and may tailor queries to each party, but for simplicity of discussion and without loss of generality, we use this simpler notation.\nAcknowledgementsIt takes the contributions of many to get a system ready for enterprise production. Thanks to Marius Danciu, Jim Rhyness, Frank Li, Calin Radu Coca, Rania Khalaf, Sandeep Gopisetty, Dinesh Verma, and Greg Filla for their contribution to IBM Federated Learning.\nMacy's is warning customers that their information might have been stolen in a data breach. Macy's is warning customers that their information might have been stolen in a data breach, July 2018. https://www.businessinsider.com/macys-bloomingdales-hack-disclosed-2018-7.\n\n. Flask, Flask, (Accessed July, 01, 2020). https://flask.palletsprojects.com/.\n\n. Fate Github, FATE Github, (Accessed July, 01, 2020). https://github.com/FederatedAI/FATE.\n\nDeep learning with differential privacy. Martin Abadi, Andy Chu, Ian Goodfellow, Ilya H Brendan Mcmahan, Kunal Mironov, Li Talwar, Zhang, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityACMMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 308-318. ACM, 2016.\n\nAccountability Act. Health insurance portability and accountability act of 1996. Public law. 104191Accountability Act. Health insurance portability and accountability act of 1996. Public law, 104:191, 1996.\n\nMachine learning with adversaries: Byzantine tolerant gradient descent. Peva Blanchard, Rachid Guerraoui, Julien Stainer, Advances in Neural Information Processing Systems. Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 119-129, 2017.\n\nPractical privacy: the sulq framework. Avrim Blum, Cynthia Dwork, Frank Mcsherry, Kobbi Nissim, Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systemsAvrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: the sulq framework. In Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 128-138, 2005.\n\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone\u010dn\u1ef3, Stefano Mazzocchi, H Brendan Mcmahan, arXiv:1902.01046Towards federated learning at scale: System design. arXiv preprintKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone\u010dn\u1ef3, Stefano Mazzocchi, H Brendan McMahan, et al. Towards federated learning at scale: System design. arXiv preprint arXiv:1902.01046, 2019.\n\nTifl: A tier-based federated learning system. Zheng Chai, Ahsan Ali, Syed Zawad, Stacey Truex, Ali Anwar, Nathalie Baracaldo, Yi Zhou, Heiko Ludwig, Feng Yan, Yue Cheng, To appear in ACM Symposium on High-Performance Parallel and Distributed Computing (HPDC). 2020Zheng Chai, Ahsan Ali, Syed Zawad, Stacey Truex, Ali Anwar, Nathalie Baracaldo, Yi Zhou, Heiko Ludwig, Feng Yan, and Yue Cheng. Tifl: A tier-based federated learning system. To appear in ACM Symposium on High-Performance Parallel and Distributed Computing (HPDC), 2020.\n\nTowards taming the resource and data heterogeneity in federated learning. Zheng Chai, Hannan Fayyaz, Zeshan Fayyaz, Ali Anwar, Yi Zhou, Nathalie Baracaldo, Heiko Ludwig, Yue Cheng, 2019 USENIX Conference on Operational Machine Learning. Zheng Chai, Hannan Fayyaz, Zeshan Fayyaz, Ali Anwar, Yi Zhou, Nathalie Baracaldo, Heiko Ludwig, and Yue Cheng. Towards taming the resource and data heterogeneity in federated learning. In 2019 USENIX Conference on Operational Machine Learning (OpML 19), pages 19-21, 2019.\n\n. Fran\u00e7ois Chollet, Fran\u00e7ois Chollet et al. Keras. https://keras.io, 2015.\n\nA generalisation, a simplification, and some applications of pailliers probabilistic public-key system, presented at the 4th international workshop on practice and theory in public key cryptosystems. I Damgard, M Jurik, cheju island. KoreaI Damgard and M Jurik. A generalisation, a simplification, and some applications of pailliers probabilistic public-key system, presented at the 4th international workshop on practice and theory in public key cryptosystems, cheju island. Korea, 2001.\n\nCoordinate-wise median: Not bad, not bad. Sumit Goel, Wade Hann-Caruthers, arXiv:2007.00903arXiv preprintpretty goodSumit Goel and Wade Hann-Caruthers. Coordinate-wise median: Not bad, not bad, pretty good. arXiv preprint arXiv:2007.00903, 2020.\n\nGoogle exposed user data, feared repercussions of disclosing to public. The Wall Street Journal. The Wall Street Journal. Google exposed user data, feared repercussions of disclosing to public, Oct. 2018.\n\nPeter Kairouz, Brendan Mcmahan, Brendan Avent, arXiv:1912.04977Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji. Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel CummingsarXiv preprintAdvances and open problems in federated learningPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Ar- jun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\nScaling distributed machine learning with the parameter server. Mu Li, Jun Woo David G Andersen, Alexander J Park, Amr Smola, Vanja Ahmed, James Josifovski, Eugene J Long, Bor-Yiing Shekita, Su, OSDI. 14Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In OSDI, volume 14, pages 583-598, 2014.\n\nSeth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. Brendan Mcmahan, Eider Moore, Daniel Ramage, Artificial Intelligence and Statistics. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar- cas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273-1282, 2017.\n\nCommunicationefficient learning of deep networks from decentralized data. Eider H Brendan Mcmahan, Daniel Moore, Seth Ramage, Hampson, arXiv:1602.05629arXiv preprintH Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication- efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.\n\nExploiting unintended feature leakage in collaborative learning. Luca Melis, Congzheng Song, Vitaly Emiliano De Cristofaro, Shmatikov, 2019 IEEE Symposium on Security and Privacy (SP). IEEELuca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting un- intended feature leakage in collaborative learning. In 2019 IEEE Symposium on Security and Privacy (SP), pages 691-706. IEEE, 2019.\n\nMllib: Machine learning in apache spark. Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, Manish Db Tsai, Sean Amde, Owen, The Journal of Machine Learning Research. 171Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al. Mllib: Machine learning in apache spark. The Journal of Machine Learning Research, 17(1):1235-1241, 2016.\n\nR\u00e9nyi differential privacy. Ilya Mironov, 2017 IEEE 30th Computer Security Foundations Symposium (CSF). IEEEIlya Mironov. R\u00e9nyi differential privacy. In 2017 IEEE 30th Computer Security Foundations Symposium (CSF), pages 263-275. IEEE, 2017.\n\nYahoo to pay $50 million, offer credit monitoring for massive security breach. Nbc News, NBC News. Yahoo to pay $50 million, offer credit monitoring for massive security breach, Oct. 2018.\n\nScikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret- tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Per- rot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\nInduction of decision trees. J , Ross Quinlan, Machine learning. 11J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81-106, 1986.\n\nRegulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46. Official Journal of the European Union. 591-88294OJ)General Data Protection Regulation. Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46. Official Journal of the European Union (OJ), 59(1-88):294, 2016.\n\nBraintorrent: A peer-to-peer environment for decentralized federated learning. Shayan Abhijit Guha Roy, Sebastian Siddiqui, Nassir Plsterl, Christian Navab, Wachinger, Abhijit Guha Roy, Shayan Siddiqui, Sebastian Plsterl, Nassir Navab, and Christian Wachinger. Braintorrent: A peer-to-peer environment for decentralized federated learning, 2019.\n\nA generic framework for privacy preserving deep learning. Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, Jonathan Passerat-Palmbach, arXiv:1811.04017arXiv preprintTheo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, and Jonathan Passerat-Palmbach. A generic framework for privacy preserving deep learning. arXiv preprint arXiv:1811.04017, 2018.\n\nTowards federated graph learning for collaborative financial crimes detection. Toyotaro Suzumura, Yi Zhou, Natahalie Barcardo, Guangnan Ye, Keith Houck, Ryo Kawahara, Ali Anwar, Lucia Larise Stavarache, Daniel Klyashtorny, Heiko Ludwig, arXiv:1909.12946arXiv preprintToyotaro Suzumura, Yi Zhou, Natahalie Barcardo, Guangnan Ye, Keith Houck, Ryo Kawahara, Ali Anwar, Lucia Larise Stavarache, Daniel Klyashtorny, Heiko Ludwig, et al. Towards federated graph learning for collaborative financial crimes detection. arXiv preprint arXiv:1909.12946, 2019.\n\nFacebook security breach exposes accounts of 50 million users. New York Times. New York Times. Facebook security breach exposes accounts of 50 million users, September 2018. https://www.nytimes.com/2018/09/28/technology/facebook-hack-data-breach.html.\n\nA hybrid approach to privacy-preserving federated learning. Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, arXiv:1812.03224arXiv preprintStacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, and Rui Zhang. A hybrid approach to privacy-preserving federated learning. arXiv preprint arXiv:1812.03224, 2018.\n\nZeno: Distributed stochastic gradient descent with suspicion-based fault-tolerance. Cong Xie, Sanmi Koyejo, Indranil Gupta, International Conference on Machine Learning. Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with suspicion-based fault-tolerance. In International Conference on Machine Learning, pages 6893-6901, 2019.\n\nHybridalpha: An efficient approach for privacy-preserving federated learning. Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, Heiko Ludwig, Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security. the 12th ACM Workshop on Artificial Intelligence and SecurityRunhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, and Heiko Ludwig. Hybridalpha: An efficient approach for privacy-preserving federated learning. In Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security, pages 13-23, 2019.\n\nStatistical model aggregation via parameter matching. Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, Advances in Neural Information Processing Systems. Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, and Nghia Hoang. Statistical model aggregation via parameter matching. In Advances in Neural Information Processing Systems, pages 10956-10966, 2019.\n\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, arXiv:1905.12022Trong Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv preprintMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv preprint arXiv:1905.12022, 2019.\n", "annotations": {"author": "[{\"end\":96,\"start\":68},{\"end\":131,\"start\":97},{\"end\":159,\"start\":132},{\"end\":183,\"start\":160},{\"end\":209,\"start\":184},{\"end\":243,\"start\":210},{\"end\":268,\"start\":244},{\"end\":306,\"start\":269},{\"end\":335,\"start\":307},{\"end\":364,\"start\":336},{\"end\":393,\"start\":365},{\"end\":423,\"start\":394},{\"end\":449,\"start\":424},{\"end\":480,\"start\":450},{\"end\":516,\"start\":481},{\"end\":554,\"start\":517},{\"end\":582,\"start\":555},{\"end\":611,\"start\":583},{\"end\":640,\"start\":612},{\"end\":668,\"start\":641},{\"end\":702,\"start\":669},{\"end\":733,\"start\":703},{\"end\":761,\"start\":734},{\"end\":788,\"start\":762}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":74},{\"end\":115,\"start\":106},{\"end\":143,\"start\":137},{\"end\":167,\"start\":163},{\"end\":193,\"start\":188},{\"end\":227,\"start\":219},{\"end\":252,\"start\":249},{\"end\":290,\"start\":277},{\"end\":319,\"start\":314},{\"end\":348,\"start\":344},{\"end\":377,\"start\":370},{\"end\":407,\"start\":402},{\"end\":433,\"start\":429},{\"end\":464,\"start\":457},{\"end\":500,\"start\":489},{\"end\":538,\"start\":526},{\"end\":566,\"start\":560},{\"end\":595,\"start\":589},{\"end\":624,\"start\":618},{\"end\":652,\"start\":646},{\"end\":686,\"start\":677},{\"end\":717,\"start\":710},{\"end\":745,\"start\":740},{\"end\":772,\"start\":768}]", "author_first_name": "[{\"end\":73,\"start\":68},{\"end\":105,\"start\":97},{\"end\":136,\"start\":132},{\"end\":162,\"start\":160},{\"end\":187,\"start\":184},{\"end\":218,\"start\":210},{\"end\":248,\"start\":244},{\"end\":276,\"start\":269},{\"end\":313,\"start\":307},{\"end\":343,\"start\":336},{\"end\":369,\"start\":365},{\"end\":401,\"start\":394},{\"end\":428,\"start\":424},{\"end\":456,\"start\":450},{\"end\":488,\"start\":481},{\"end\":525,\"start\":517},{\"end\":559,\"start\":555},{\"end\":588,\"start\":583},{\"end\":617,\"start\":612},{\"end\":645,\"start\":641},{\"end\":676,\"start\":669},{\"end\":709,\"start\":703},{\"end\":739,\"start\":734},{\"end\":767,\"start\":762}]", "author_affiliation": "[{\"end\":95,\"start\":82},{\"end\":130,\"start\":117},{\"end\":158,\"start\":145},{\"end\":182,\"start\":169},{\"end\":208,\"start\":195},{\"end\":242,\"start\":229},{\"end\":267,\"start\":254},{\"end\":305,\"start\":292},{\"end\":334,\"start\":321},{\"end\":363,\"start\":350},{\"end\":392,\"start\":379},{\"end\":422,\"start\":409},{\"end\":448,\"start\":435},{\"end\":479,\"start\":466},{\"end\":515,\"start\":502},{\"end\":553,\"start\":540},{\"end\":581,\"start\":568},{\"end\":610,\"start\":597},{\"end\":639,\"start\":626},{\"end\":667,\"start\":654},{\"end\":701,\"start\":688},{\"end\":732,\"start\":719},{\"end\":760,\"start\":747},{\"end\":787,\"start\":774}]", "title": "[{\"end\":65,\"start\":1},{\"end\":853,\"start\":789}]", "venue": null, "abstract": "[{\"end\":6554,\"start\":855}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8525,\"start\":8521},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8531,\"start\":8527},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8739,\"start\":8735},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9426,\"start\":9423},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9429,\"start\":9426},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9432,\"start\":9429},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9435,\"start\":9432},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10831,\"start\":10828},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10837,\"start\":10833},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10846,\"start\":10843},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14032,\"start\":14031},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18284,\"start\":18281},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25234,\"start\":25231},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25237,\"start\":25234},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25240,\"start\":25237},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25243,\"start\":25240},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25246,\"start\":25243},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25249,\"start\":25246},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25252,\"start\":25249},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25255,\"start\":25252},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25258,\"start\":25255},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27009,\"start\":27005},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29533,\"start\":29529},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30151,\"start\":30147},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30639,\"start\":30635},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34525,\"start\":34521},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34616,\"start\":34612},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34738,\"start\":34734},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34922,\"start\":34919},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34996,\"start\":34992},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":35090,\"start\":35086},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35752,\"start\":35749},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35788,\"start\":35784},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":35791,\"start\":35788},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35854,\"start\":35850},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36092,\"start\":36088},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36145,\"start\":36141},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36288,\"start\":36284},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36618,\"start\":36614},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36788,\"start\":36784},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37514,\"start\":37510},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37545,\"start\":37541},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":37591,\"start\":37587},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38171,\"start\":38168},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38174,\"start\":38171}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44820,\"start\":44765},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44852,\"start\":44821},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44920,\"start\":44853},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45346,\"start\":44921}]", "paragraph": "[{\"end\":7351,\"start\":6570},{\"end\":7591,\"start\":7353},{\"end\":8598,\"start\":7593},{\"end\":9607,\"start\":8600},{\"end\":10748,\"start\":9609},{\"end\":11189,\"start\":10750},{\"end\":12054,\"start\":11191},{\"end\":12516,\"start\":12056},{\"end\":12983,\"start\":12545},{\"end\":13252,\"start\":12985},{\"end\":13420,\"start\":13254},{\"end\":13657,\"start\":13422},{\"end\":14136,\"start\":13659},{\"end\":14430,\"start\":14138},{\"end\":14819,\"start\":14432},{\"end\":16162,\"start\":14821},{\"end\":16820,\"start\":16164},{\"end\":17012,\"start\":16837},{\"end\":17275,\"start\":17014},{\"end\":17633,\"start\":17277},{\"end\":18311,\"start\":17635},{\"end\":19227,\"start\":18313},{\"end\":20083,\"start\":19229},{\"end\":20286,\"start\":20085},{\"end\":20986,\"start\":20288},{\"end\":21194,\"start\":20988},{\"end\":21703,\"start\":21196},{\"end\":22224,\"start\":21724},{\"end\":22702,\"start\":22226},{\"end\":23684,\"start\":22704},{\"end\":24193,\"start\":23686},{\"end\":24949,\"start\":24209},{\"end\":25055,\"start\":24951},{\"end\":26183,\"start\":25099},{\"end\":26361,\"start\":26203},{\"end\":26586,\"start\":26363},{\"end\":27734,\"start\":26588},{\"end\":28608,\"start\":27736},{\"end\":29204,\"start\":28610},{\"end\":29762,\"start\":29206},{\"end\":30192,\"start\":29764},{\"end\":30567,\"start\":30211},{\"end\":30941,\"start\":30569},{\"end\":31553,\"start\":30943},{\"end\":31906,\"start\":31555},{\"end\":32651,\"start\":31908},{\"end\":33073,\"start\":32653},{\"end\":33459,\"start\":33075},{\"end\":33801,\"start\":33526},{\"end\":34319,\"start\":33803},{\"end\":34871,\"start\":34321},{\"end\":35224,\"start\":34873},{\"end\":35376,\"start\":35226},{\"end\":35625,\"start\":35378},{\"end\":36468,\"start\":35627},{\"end\":37423,\"start\":36470},{\"end\":37592,\"start\":37425},{\"end\":38175,\"start\":37594},{\"end\":38687,\"start\":38177},{\"end\":39093,\"start\":38720},{\"end\":39653,\"start\":39095},{\"end\":39753,\"start\":39655},{\"end\":40011,\"start\":39755},{\"end\":40147,\"start\":40013},{\"end\":40549,\"start\":40149},{\"end\":40728,\"start\":40551},{\"end\":41106,\"start\":40730},{\"end\":41233,\"start\":41108},{\"end\":41646,\"start\":41235},{\"end\":42042,\"start\":41648},{\"end\":42525,\"start\":42044},{\"end\":43865,\"start\":42540},{\"end\":44764,\"start\":43867}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":42406,\"start\":42399}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":6568,\"start\":6556},{\"attributes\":{\"n\":\"2\"},\"end\":12543,\"start\":12519},{\"attributes\":{\"n\":\"3\"},\"end\":16835,\"start\":16823},{\"attributes\":{\"n\":\"3.1\"},\"end\":21722,\"start\":21706},{\"attributes\":{\"n\":\"3.2\"},\"end\":24207,\"start\":24196},{\"attributes\":{\"n\":\"4\"},\"end\":25097,\"start\":25058},{\"attributes\":{\"n\":\"4.1\"},\"end\":26201,\"start\":26186},{\"attributes\":{\"n\":\"4.2\"},\"end\":30209,\"start\":30195},{\"attributes\":{\"n\":\"4.3\"},\"end\":33524,\"start\":33462},{\"attributes\":{\"n\":\"5\"},\"end\":38718,\"start\":38690},{\"attributes\":{\"n\":\"6\"},\"end\":42538,\"start\":42528},{\"end\":44776,\"start\":44766},{\"end\":44832,\"start\":44822},{\"end\":44864,\"start\":44854},{\"end\":44931,\"start\":44922}]", "table": "[{\"end\":45346,\"start\":45039}]", "figure_caption": "[{\"end\":44820,\"start\":44778},{\"end\":44852,\"start\":44834},{\"end\":44920,\"start\":44866},{\"end\":45039,\"start\":44933}]", "figure_ref": "[{\"end\":13020,\"start\":13012},{\"end\":15334,\"start\":15326},{\"end\":16946,\"start\":16938},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17274,\"start\":17266},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22246,\"start\":22238},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22831,\"start\":22823},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41567,\"start\":41559}]", "bib_author_first_name": "[{\"end\":46303,\"start\":46297},{\"end\":46315,\"start\":46311},{\"end\":46324,\"start\":46321},{\"end\":46341,\"start\":46337},{\"end\":46366,\"start\":46361},{\"end\":46378,\"start\":46376},{\"end\":47097,\"start\":47093},{\"end\":47115,\"start\":47109},{\"end\":47133,\"start\":47127},{\"end\":47443,\"start\":47438},{\"end\":47457,\"start\":47450},{\"end\":47470,\"start\":47465},{\"end\":47486,\"start\":47481},{\"end\":47917,\"start\":47912},{\"end\":47934,\"start\":47928},{\"end\":47952,\"start\":47944},{\"end\":47971,\"start\":47964},{\"end\":47982,\"start\":47978},{\"end\":48001,\"start\":47993},{\"end\":48015,\"start\":48010},{\"end\":48029,\"start\":48024},{\"end\":48046,\"start\":48039},{\"end\":48474,\"start\":48469},{\"end\":48486,\"start\":48481},{\"end\":48496,\"start\":48492},{\"end\":48510,\"start\":48504},{\"end\":48521,\"start\":48518},{\"end\":48537,\"start\":48529},{\"end\":48551,\"start\":48549},{\"end\":48563,\"start\":48558},{\"end\":48576,\"start\":48572},{\"end\":48585,\"start\":48582},{\"end\":49037,\"start\":49032},{\"end\":49050,\"start\":49044},{\"end\":49065,\"start\":49059},{\"end\":49077,\"start\":49074},{\"end\":49087,\"start\":49085},{\"end\":49102,\"start\":49094},{\"end\":49119,\"start\":49114},{\"end\":49131,\"start\":49128},{\"end\":49479,\"start\":49471},{\"end\":49746,\"start\":49745},{\"end\":49757,\"start\":49756},{\"end\":50082,\"start\":50077},{\"end\":50093,\"start\":50089},{\"end\":50493,\"start\":50488},{\"end\":50510,\"start\":50503},{\"end\":50527,\"start\":50520},{\"end\":51061,\"start\":51059},{\"end\":51073,\"start\":51066},{\"end\":51101,\"start\":51092},{\"end\":51103,\"start\":51102},{\"end\":51113,\"start\":51110},{\"end\":51126,\"start\":51121},{\"end\":51139,\"start\":51134},{\"end\":51158,\"start\":51152},{\"end\":51160,\"start\":51159},{\"end\":51176,\"start\":51167},{\"end\":51562,\"start\":51555},{\"end\":51577,\"start\":51572},{\"end\":51591,\"start\":51585},{\"end\":51949,\"start\":51944},{\"end\":51975,\"start\":51969},{\"end\":51987,\"start\":51983},{\"end\":52288,\"start\":52284},{\"end\":52305,\"start\":52296},{\"end\":52318,\"start\":52312},{\"end\":52679,\"start\":52671},{\"end\":52692,\"start\":52686},{\"end\":52707,\"start\":52702},{\"end\":52719,\"start\":52715},{\"end\":52736,\"start\":52728},{\"end\":52757,\"start\":52751},{\"end\":52769,\"start\":52763},{\"end\":52785,\"start\":52779},{\"end\":52799,\"start\":52795},{\"end\":53144,\"start\":53140},{\"end\":53588,\"start\":53587},{\"end\":53601,\"start\":53600},{\"end\":53614,\"start\":53613},{\"end\":53626,\"start\":53625},{\"end\":53636,\"start\":53635},{\"end\":53647,\"start\":53646},{\"end\":53657,\"start\":53656},{\"end\":53668,\"start\":53667},{\"end\":53684,\"start\":53683},{\"end\":53693,\"start\":53692},{\"end\":53704,\"start\":53703},{\"end\":53718,\"start\":53717},{\"end\":53728,\"start\":53727},{\"end\":53742,\"start\":53741},{\"end\":53753,\"start\":53752},{\"end\":53763,\"start\":53762},{\"end\":54158,\"start\":54157},{\"end\":54165,\"start\":54161},{\"end\":55002,\"start\":54996},{\"end\":55030,\"start\":55021},{\"end\":55047,\"start\":55041},{\"end\":55066,\"start\":55057},{\"end\":55326,\"start\":55322},{\"end\":55341,\"start\":55335},{\"end\":55355,\"start\":55349},{\"end\":55367,\"start\":55362},{\"end\":55381,\"start\":55376},{\"end\":55397,\"start\":55391},{\"end\":55416,\"start\":55408},{\"end\":55769,\"start\":55761},{\"end\":55782,\"start\":55780},{\"end\":55798,\"start\":55789},{\"end\":55817,\"start\":55809},{\"end\":55827,\"start\":55822},{\"end\":55838,\"start\":55835},{\"end\":55852,\"start\":55849},{\"end\":55865,\"start\":55860},{\"end\":55872,\"start\":55866},{\"end\":55891,\"start\":55885},{\"end\":55910,\"start\":55905},{\"end\":56552,\"start\":56546},{\"end\":56568,\"start\":56560},{\"end\":56583,\"start\":56580},{\"end\":56597,\"start\":56591},{\"end\":56612,\"start\":56607},{\"end\":56624,\"start\":56621},{\"end\":56940,\"start\":56936},{\"end\":56951,\"start\":56946},{\"end\":56968,\"start\":56960},{\"end\":57307,\"start\":57301},{\"end\":57320,\"start\":57312},{\"end\":57334,\"start\":57332},{\"end\":57344,\"start\":57341},{\"end\":57357,\"start\":57352},{\"end\":57814,\"start\":57807},{\"end\":57832,\"start\":57826},{\"end\":57848,\"start\":57842},{\"end\":57864,\"start\":57856},{\"end\":57882,\"start\":57877},{\"end\":58169,\"start\":58162},{\"end\":58187,\"start\":58181},{\"end\":58203,\"start\":58197},{\"end\":58219,\"start\":58211}]", "bib_author_last_name": "[{\"end\":46090,\"start\":46085},{\"end\":46176,\"start\":46165},{\"end\":46309,\"start\":46304},{\"end\":46319,\"start\":46316},{\"end\":46335,\"start\":46325},{\"end\":46359,\"start\":46342},{\"end\":46374,\"start\":46367},{\"end\":46385,\"start\":46379},{\"end\":46392,\"start\":46387},{\"end\":47107,\"start\":47098},{\"end\":47125,\"start\":47116},{\"end\":47141,\"start\":47134},{\"end\":47448,\"start\":47444},{\"end\":47463,\"start\":47458},{\"end\":47479,\"start\":47471},{\"end\":47493,\"start\":47487},{\"end\":47926,\"start\":47918},{\"end\":47942,\"start\":47935},{\"end\":47962,\"start\":47953},{\"end\":47976,\"start\":47972},{\"end\":47991,\"start\":47983},{\"end\":48008,\"start\":48002},{\"end\":48022,\"start\":48016},{\"end\":48037,\"start\":48030},{\"end\":48056,\"start\":48047},{\"end\":48075,\"start\":48058},{\"end\":48479,\"start\":48475},{\"end\":48490,\"start\":48487},{\"end\":48502,\"start\":48497},{\"end\":48516,\"start\":48511},{\"end\":48527,\"start\":48522},{\"end\":48547,\"start\":48538},{\"end\":48556,\"start\":48552},{\"end\":48570,\"start\":48564},{\"end\":48580,\"start\":48577},{\"end\":48591,\"start\":48586},{\"end\":49042,\"start\":49038},{\"end\":49057,\"start\":49051},{\"end\":49072,\"start\":49066},{\"end\":49083,\"start\":49078},{\"end\":49092,\"start\":49088},{\"end\":49112,\"start\":49103},{\"end\":49126,\"start\":49120},{\"end\":49137,\"start\":49132},{\"end\":49487,\"start\":49480},{\"end\":49754,\"start\":49747},{\"end\":49763,\"start\":49758},{\"end\":50087,\"start\":50083},{\"end\":50108,\"start\":50094},{\"end\":50501,\"start\":50494},{\"end\":50518,\"start\":50511},{\"end\":50533,\"start\":50528},{\"end\":51064,\"start\":51062},{\"end\":51090,\"start\":51074},{\"end\":51108,\"start\":51104},{\"end\":51119,\"start\":51114},{\"end\":51132,\"start\":51127},{\"end\":51150,\"start\":51140},{\"end\":51165,\"start\":51161},{\"end\":51184,\"start\":51177},{\"end\":51188,\"start\":51186},{\"end\":51570,\"start\":51563},{\"end\":51583,\"start\":51578},{\"end\":51598,\"start\":51592},{\"end\":51967,\"start\":51950},{\"end\":51981,\"start\":51976},{\"end\":51994,\"start\":51988},{\"end\":52003,\"start\":51996},{\"end\":52294,\"start\":52289},{\"end\":52310,\"start\":52306},{\"end\":52341,\"start\":52319},{\"end\":52352,\"start\":52343},{\"end\":52684,\"start\":52680},{\"end\":52700,\"start\":52693},{\"end\":52713,\"start\":52708},{\"end\":52726,\"start\":52720},{\"end\":52749,\"start\":52737},{\"end\":52761,\"start\":52758},{\"end\":52777,\"start\":52770},{\"end\":52793,\"start\":52786},{\"end\":52804,\"start\":52800},{\"end\":52810,\"start\":52806},{\"end\":53152,\"start\":53145},{\"end\":53442,\"start\":53434},{\"end\":53598,\"start\":53589},{\"end\":53611,\"start\":53602},{\"end\":53623,\"start\":53615},{\"end\":53633,\"start\":53627},{\"end\":53644,\"start\":53637},{\"end\":53654,\"start\":53648},{\"end\":53665,\"start\":53658},{\"end\":53681,\"start\":53669},{\"end\":53690,\"start\":53685},{\"end\":53701,\"start\":53694},{\"end\":53715,\"start\":53705},{\"end\":53725,\"start\":53719},{\"end\":53739,\"start\":53729},{\"end\":53750,\"start\":53743},{\"end\":53760,\"start\":53754},{\"end\":53773,\"start\":53764},{\"end\":54173,\"start\":54166},{\"end\":55019,\"start\":55003},{\"end\":55039,\"start\":55031},{\"end\":55055,\"start\":55048},{\"end\":55072,\"start\":55067},{\"end\":55083,\"start\":55074},{\"end\":55333,\"start\":55327},{\"end\":55347,\"start\":55342},{\"end\":55360,\"start\":55356},{\"end\":55374,\"start\":55368},{\"end\":55389,\"start\":55382},{\"end\":55406,\"start\":55398},{\"end\":55434,\"start\":55417},{\"end\":55778,\"start\":55770},{\"end\":55787,\"start\":55783},{\"end\":55807,\"start\":55799},{\"end\":55820,\"start\":55818},{\"end\":55833,\"start\":55828},{\"end\":55847,\"start\":55839},{\"end\":55858,\"start\":55853},{\"end\":55883,\"start\":55873},{\"end\":55903,\"start\":55892},{\"end\":55917,\"start\":55911},{\"end\":56558,\"start\":56553},{\"end\":56578,\"start\":56569},{\"end\":56589,\"start\":56584},{\"end\":56605,\"start\":56598},{\"end\":56619,\"start\":56613},{\"end\":56630,\"start\":56625},{\"end\":56944,\"start\":56941},{\"end\":56958,\"start\":56952},{\"end\":56974,\"start\":56969},{\"end\":57310,\"start\":57308},{\"end\":57330,\"start\":57321},{\"end\":57339,\"start\":57335},{\"end\":57350,\"start\":57345},{\"end\":57364,\"start\":57358},{\"end\":57824,\"start\":57815},{\"end\":57840,\"start\":57833},{\"end\":57854,\"start\":57849},{\"end\":57875,\"start\":57865},{\"end\":57888,\"start\":57883},{\"end\":58179,\"start\":58170},{\"end\":58195,\"start\":58188},{\"end\":58209,\"start\":58204},{\"end\":58230,\"start\":58220}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":46081,\"start\":45812},{\"attributes\":{\"id\":\"b1\"},\"end\":46161,\"start\":46083},{\"attributes\":{\"id\":\"b2\"},\"end\":46254,\"start\":46163},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207241585},\"end\":46811,\"start\":46256},{\"attributes\":{\"id\":\"b4\"},\"end\":47019,\"start\":46813},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":28527385},\"end\":47397,\"start\":47021},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15331398},\"end\":47910,\"start\":47399},{\"attributes\":{\"doi\":\"arXiv:1902.01046\",\"id\":\"b7\"},\"end\":48421,\"start\":47912},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":210920189},\"end\":48956,\"start\":48423},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":115139138},\"end\":49467,\"start\":48958},{\"attributes\":{\"id\":\"b10\"},\"end\":49543,\"start\":49469},{\"attributes\":{\"id\":\"b11\"},\"end\":50033,\"start\":49545},{\"attributes\":{\"doi\":\"arXiv:2007.00903\",\"id\":\"b12\"},\"end\":50280,\"start\":50035},{\"attributes\":{\"id\":\"b13\"},\"end\":50486,\"start\":50282},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b14\"},\"end\":50993,\"start\":50488},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4614646},\"end\":51437,\"start\":50995},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14955348},\"end\":51868,\"start\":51439},{\"attributes\":{\"doi\":\"arXiv:1602.05629\",\"id\":\"b17\"},\"end\":52217,\"start\":51870},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53099247},\"end\":52628,\"start\":52219},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7956687},\"end\":53110,\"start\":52630},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9386213},\"end\":53353,\"start\":53112},{\"attributes\":{\"id\":\"b21\"},\"end\":53543,\"start\":53355},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10659969},\"end\":54126,\"start\":53545},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13252401},\"end\":54277,\"start\":54128},{\"attributes\":{\"id\":\"b24\"},\"end\":54915,\"start\":54279},{\"attributes\":{\"id\":\"b25\"},\"end\":55262,\"start\":54917},{\"attributes\":{\"doi\":\"arXiv:1811.04017\",\"id\":\"b26\"},\"end\":55680,\"start\":55264},{\"attributes\":{\"doi\":\"arXiv:1909.12946\",\"id\":\"b27\"},\"end\":56231,\"start\":55682},{\"attributes\":{\"id\":\"b28\"},\"end\":56484,\"start\":56233},{\"attributes\":{\"doi\":\"arXiv:1812.03224\",\"id\":\"b29\"},\"end\":56850,\"start\":56486},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":159041074},\"end\":57221,\"start\":56852},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":207940288},\"end\":57751,\"start\":57223},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202784500},\"end\":58160,\"start\":57753},{\"attributes\":{\"doi\":\"arXiv:1905.12022\",\"id\":\"b33\"},\"end\":58576,\"start\":58162}]", "bib_title": "[{\"end\":46295,\"start\":46256},{\"end\":46892,\"start\":46813},{\"end\":47091,\"start\":47021},{\"end\":47436,\"start\":47399},{\"end\":48467,\"start\":48423},{\"end\":49030,\"start\":48958},{\"end\":50352,\"start\":50282},{\"end\":51057,\"start\":50995},{\"end\":51553,\"start\":51439},{\"end\":52282,\"start\":52219},{\"end\":52669,\"start\":52630},{\"end\":53138,\"start\":53112},{\"end\":53585,\"start\":53545},{\"end\":54155,\"start\":54128},{\"end\":54519,\"start\":54279},{\"end\":56294,\"start\":56233},{\"end\":56934,\"start\":56852},{\"end\":57299,\"start\":57223},{\"end\":57805,\"start\":57753}]", "bib_author": "[{\"end\":46092,\"start\":46085},{\"end\":46178,\"start\":46165},{\"end\":46311,\"start\":46297},{\"end\":46321,\"start\":46311},{\"end\":46337,\"start\":46321},{\"end\":46361,\"start\":46337},{\"end\":46376,\"start\":46361},{\"end\":46387,\"start\":46376},{\"end\":46394,\"start\":46387},{\"end\":47109,\"start\":47093},{\"end\":47127,\"start\":47109},{\"end\":47143,\"start\":47127},{\"end\":47450,\"start\":47438},{\"end\":47465,\"start\":47450},{\"end\":47481,\"start\":47465},{\"end\":47495,\"start\":47481},{\"end\":47928,\"start\":47912},{\"end\":47944,\"start\":47928},{\"end\":47964,\"start\":47944},{\"end\":47978,\"start\":47964},{\"end\":47993,\"start\":47978},{\"end\":48010,\"start\":47993},{\"end\":48024,\"start\":48010},{\"end\":48039,\"start\":48024},{\"end\":48058,\"start\":48039},{\"end\":48077,\"start\":48058},{\"end\":48481,\"start\":48469},{\"end\":48492,\"start\":48481},{\"end\":48504,\"start\":48492},{\"end\":48518,\"start\":48504},{\"end\":48529,\"start\":48518},{\"end\":48549,\"start\":48529},{\"end\":48558,\"start\":48549},{\"end\":48572,\"start\":48558},{\"end\":48582,\"start\":48572},{\"end\":48593,\"start\":48582},{\"end\":49044,\"start\":49032},{\"end\":49059,\"start\":49044},{\"end\":49074,\"start\":49059},{\"end\":49085,\"start\":49074},{\"end\":49094,\"start\":49085},{\"end\":49114,\"start\":49094},{\"end\":49128,\"start\":49114},{\"end\":49139,\"start\":49128},{\"end\":49489,\"start\":49471},{\"end\":49756,\"start\":49745},{\"end\":49765,\"start\":49756},{\"end\":50089,\"start\":50077},{\"end\":50110,\"start\":50089},{\"end\":50503,\"start\":50488},{\"end\":50520,\"start\":50503},{\"end\":50535,\"start\":50520},{\"end\":51066,\"start\":51059},{\"end\":51092,\"start\":51066},{\"end\":51110,\"start\":51092},{\"end\":51121,\"start\":51110},{\"end\":51134,\"start\":51121},{\"end\":51152,\"start\":51134},{\"end\":51167,\"start\":51152},{\"end\":51186,\"start\":51167},{\"end\":51190,\"start\":51186},{\"end\":51572,\"start\":51555},{\"end\":51585,\"start\":51572},{\"end\":51600,\"start\":51585},{\"end\":51969,\"start\":51944},{\"end\":51983,\"start\":51969},{\"end\":51996,\"start\":51983},{\"end\":52005,\"start\":51996},{\"end\":52296,\"start\":52284},{\"end\":52312,\"start\":52296},{\"end\":52343,\"start\":52312},{\"end\":52354,\"start\":52343},{\"end\":52686,\"start\":52671},{\"end\":52702,\"start\":52686},{\"end\":52715,\"start\":52702},{\"end\":52728,\"start\":52715},{\"end\":52751,\"start\":52728},{\"end\":52763,\"start\":52751},{\"end\":52779,\"start\":52763},{\"end\":52795,\"start\":52779},{\"end\":52806,\"start\":52795},{\"end\":52812,\"start\":52806},{\"end\":53154,\"start\":53140},{\"end\":53444,\"start\":53434},{\"end\":53600,\"start\":53587},{\"end\":53613,\"start\":53600},{\"end\":53625,\"start\":53613},{\"end\":53635,\"start\":53625},{\"end\":53646,\"start\":53635},{\"end\":53656,\"start\":53646},{\"end\":53667,\"start\":53656},{\"end\":53683,\"start\":53667},{\"end\":53692,\"start\":53683},{\"end\":53703,\"start\":53692},{\"end\":53717,\"start\":53703},{\"end\":53727,\"start\":53717},{\"end\":53741,\"start\":53727},{\"end\":53752,\"start\":53741},{\"end\":53762,\"start\":53752},{\"end\":53775,\"start\":53762},{\"end\":54161,\"start\":54157},{\"end\":54175,\"start\":54161},{\"end\":55021,\"start\":54996},{\"end\":55041,\"start\":55021},{\"end\":55057,\"start\":55041},{\"end\":55074,\"start\":55057},{\"end\":55085,\"start\":55074},{\"end\":55335,\"start\":55322},{\"end\":55349,\"start\":55335},{\"end\":55362,\"start\":55349},{\"end\":55376,\"start\":55362},{\"end\":55391,\"start\":55376},{\"end\":55408,\"start\":55391},{\"end\":55436,\"start\":55408},{\"end\":55780,\"start\":55761},{\"end\":55789,\"start\":55780},{\"end\":55809,\"start\":55789},{\"end\":55822,\"start\":55809},{\"end\":55835,\"start\":55822},{\"end\":55849,\"start\":55835},{\"end\":55860,\"start\":55849},{\"end\":55885,\"start\":55860},{\"end\":55905,\"start\":55885},{\"end\":55919,\"start\":55905},{\"end\":56560,\"start\":56546},{\"end\":56580,\"start\":56560},{\"end\":56591,\"start\":56580},{\"end\":56607,\"start\":56591},{\"end\":56621,\"start\":56607},{\"end\":56632,\"start\":56621},{\"end\":56946,\"start\":56936},{\"end\":56960,\"start\":56946},{\"end\":56976,\"start\":56960},{\"end\":57312,\"start\":57301},{\"end\":57332,\"start\":57312},{\"end\":57341,\"start\":57332},{\"end\":57352,\"start\":57341},{\"end\":57366,\"start\":57352},{\"end\":57826,\"start\":57807},{\"end\":57842,\"start\":57826},{\"end\":57856,\"start\":57842},{\"end\":57877,\"start\":57856},{\"end\":57890,\"start\":57877},{\"end\":58181,\"start\":58162},{\"end\":58197,\"start\":58181},{\"end\":58211,\"start\":58197},{\"end\":58232,\"start\":58211}]", "bib_venue": "[{\"end\":46551,\"start\":46481},{\"end\":47684,\"start\":47598},{\"end\":50667,\"start\":50603},{\"end\":57505,\"start\":57444},{\"end\":45902,\"start\":45812},{\"end\":46479,\"start\":46394},{\"end\":46904,\"start\":46894},{\"end\":47192,\"start\":47143},{\"end\":47596,\"start\":47495},{\"end\":48143,\"start\":48093},{\"end\":48681,\"start\":48593},{\"end\":49193,\"start\":49139},{\"end\":49743,\"start\":49545},{\"end\":50075,\"start\":50035},{\"end\":50377,\"start\":50354},{\"end\":50601,\"start\":50551},{\"end\":51194,\"start\":51190},{\"end\":51638,\"start\":51600},{\"end\":51942,\"start\":51870},{\"end\":52402,\"start\":52354},{\"end\":52852,\"start\":52812},{\"end\":53214,\"start\":53154},{\"end\":53432,\"start\":53355},{\"end\":53811,\"start\":53775},{\"end\":54191,\"start\":54175},{\"end\":54559,\"start\":54521},{\"end\":54994,\"start\":54917},{\"end\":55320,\"start\":55264},{\"end\":55759,\"start\":55682},{\"end\":56310,\"start\":56296},{\"end\":56544,\"start\":56486},{\"end\":57020,\"start\":56976},{\"end\":57442,\"start\":57366},{\"end\":57939,\"start\":57890},{\"end\":58349,\"start\":58248}]"}}}, "year": 2023, "month": 12, "day": 17}