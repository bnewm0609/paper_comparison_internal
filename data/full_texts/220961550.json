{"id": 220961550, "updated": "2023-04-03 13:17:08.923", "metadata": {"title": "Crowd Counting via Segmentation Guided Attention Networks and Curriculum Loss", "authors": "[{\"first\":\"Qian\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Toby\",\"last\":\"Breckon\",\"middle\":[\"P.\"]}]", "venue": "IEEE Transactions on Intelligent Transportation Systems", "journal": "IEEE Transactions on Intelligent Transportation Systems", "publication_date": {"year": 2022, "month": 9, "day": 1}, "abstract": "Automatic crowd behaviour analysis is an important task for intelligent transportation systems to enable effective flow control and dynamic route planning for varying road participants. Crowd counting is one of the keys to automatic crowd behaviour analysis. Crowd counting using deep convolutional neural networks (CNN) has achieved encouraging progress in recent years. Researchers have devoted much effort to the design of variant CNN architectures and most of them are based on the pre-trained VGG16 model. Due to the insufficient expressive capacity, the backbone network of VGG16 is usually followed by another cumbersome network specially designed for good counting performance. Although VGG models have been outperformed by Inception models in image classification tasks, the existing crowd counting networks built with Inception modules still only have a small number of layers with basic types of Inception modules. To fill in this gap, in this paper, we firstly benchmark the baseline Inception-v3 model on commonly used crowd counting datasets and achieve surprisingly good performance comparable with or better than most existing crowd counting models. Subsequently, we push the boundary of this disruptive work further by proposing a Segmentation Guided Attention Network (SGANet) with Inception-v3 as the backbone and a novel curriculum loss for crowd counting. We conduct thorough experiments to compare the performance of our SGANet with prior arts and the proposed model can achieve state-of-the-art performance with MAE of 57.6, 6.3 and 87.6 on ShanghaiTechA, ShanghaiTechB and UCF_QNRF, respectively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3047585969", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tits/WangB22", "doi": "10.1109/tits.2021.3138896"}}, "content": {"source": {"pdf_hash": "07b8702347089022fd710f4d50abf645b8c2fdf6", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://dro.dur.ac.uk/34976/1/34976.pdf", "status": "GREEN"}}, "grobid": {"id": "2187dd2dc2e1c49c9595d5a0f3ddb0a1adb4b998", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/07b8702347089022fd710f4d50abf645b8c2fdf6.txt", "contents": "\nCrowd Counting via Segmentation Guided Attention Networks and Curriculum Loss\nSEPTEMBER 2022\n\nMember, IEEEQian Wang \nSenior Member, IEEEToby P Breckon \nCrowd Counting via Segmentation Guided Attention Networks and Curriculum Loss\n\nIEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n239SEPTEMBER 202210.1109/TITS.2021.313889615233Index Terms-Crowd countingcurriculum lossInception-v3segmentation guided attention networks\nAutomatic crowd behaviour analysis is an important task for intelligent transportation systems to enable effective flow control and dynamic route planning for varying road participants. Crowd counting is one of the keys to automatic crowd behaviour analysis. Crowd counting using deep convolutional neural networks (CNN) has achieved encouraging progress in recent years. Researchers have devoted much effort to the design of variant CNN architectures and most of them are based on the pre-trained VGG16 model. Due to the insufficient expressive capacity, the backbone network of VGG16 is usually followed by another cumbersome network specially designed for good counting performance. Although VGG models have been outperformed by Inception models in image classification tasks, the existing crowd counting networks built with Inception modules still only have a small number of layers with basic types of Inception modules. To fill in this gap, in this paper, we firstly benchmark the baseline Inception-v3 model on commonly used crowd counting datasets and achieve surprisingly good performance comparable with or better than most existing crowd counting models. Subsequently, we push the boundary of this disruptive work further by proposing a Segmentation Guided Attention Network (SGANet) with Inception-v3 as the backbone and a novel curriculum loss for crowd counting. We conduct thorough experiments to compare the performance of our SGANet with prior arts and the proposed model can achieve state-of-the-art performance with MAE of 57.6, 6.3 and 87.6 on ShanghaiTechA, ShanghaiTechB and UCF_QNRF, respectively.\n\nI. INTRODUCTION\n\nA UTOMATIC crowd counting has attracted increasing attention in the research community since its valuable impacts in public surveillance and intelligent transportation systems [1]- [5]. Crowd behaviour can have a big effect in the efficiency of public transportation. Intelligent transportation systems deployed in a smart city should be able to capture real-time crowd behaviour information from public surveillance and dynamically adjust the planning for effective transportation. Accurate people and vehicle counting in varying conditions provide basic information for automatic crowd behaviour analysis. People and vehicle counting can be formulated in a unified object counting framework which aims to estimate the number of target objects in still images or video frames and has been applied in many real-world applications. Manuscript  For instance, there have been works focusing on automatic counting different objects including cells [6], vehicles [7], [8], leaves [9], [10] and people [4].\n\nIn earlier years, crowd counting in images was implemented by detection [11]- [13] or direct count regression [14], [15]. Counting by detection methods assume people signatures (i.e. the whole body or the head) in images are detectable and the count can be easily achieved from the detection results. This assumption, however, does not always hold in real scenarios, especially when the crowd is extremely dense. Counting by direct count regression aims to learn a regression model (e.g., support vector machine [15] or neural networks [14]) mapping the hand-crafted image features directly to the count of people in the image. Methods falling into this category only give the final counts hence lack of explainability and reliability.\n\nRecently, crowd counting has been overwhelmingly dominated by density estimation based methods since the idea of density map was first proposed in [16]. The use of deep Convolutional Neural Networks [17] to estimate the density map along with the availability of large-scale datasets [18], [19] further improved the accuracy of crowd counting in more challenging real-world scenarios. Recent works in crowd counting have been focusing on the design of novel architectures of deep neural networks (e.g., multi-column CNN [18], [20] and attention mechanism [21], [22]) for accurate density map estimation. The motivations of these designs are usually to improve the generalization to scale-variant crowd images. Among them, the Inception module [23] has been employed and showed effectiveness in crowd counting [24], [25], although only the basic Inception modules are used and the networks are relative shallow compared with the state-of-the-art deep CNN models for image classification such as Inception-v3 [23] which uses heterogeneous Inception modules to improve the expressive power of the network. Although VGG16, VGG19 and ResNet101 have been used as the backbone networks for crowd counting in [26]- [28], to our best knowledge, the Inception models have not been investigated.\n\nIn this paper, we make the first attempt to investigate the effectiveness of Inception-v3 model for crowd counting. We modify the original Inception-v3 to make it suitable for crowd density estimation. Without bells and whistles, the Inception-v3 model can achieve surprisingly good performance comparable with or even better than most existing crowd counting models on commonly used crowd counting datasets. Subsequently, we add a segmentation map guided attention layer to the Inception-v3 model to enhance the salient feature extraction for accurate density map estimation and propose a novel curriculum loss strategy to address the issues caused by extremely dense regions in crowd counting. As a result, the proposed SGANet with curriculum loss is able to achieve stateof-the-art performance for crowd counting with the embarrassingly simple design. The contributions of this paper are summarized as follows:\n\n-We make the first attempt to investigate the effectiveness of Inception-v3 in crowd counting and achieve disruptive results which are important for the research community. -We present a Segmentation Guided Attention Network (SGANet) with a novel curriculum loss function based on the Inception-v3 model for crowd counting. -Extensive evaluations are conducted on benchmark datasets and the results demonstrate the superior performance of SGANet and the effectiveness of curriculum loss in crowd counting. The remainder of this paper is organized as follows. Section II reviews related work of crowd counting and curriculum learning. In Section III we introduce our proposed segmentation guided attention networks for crowd counting with curriculum loss. Section IV presents the experiments and results on several benchmark datasets and we conclude our work in Section V.\n\n\nII. RELATED WORK\n\nIn this section, we first review related works on CNN based crowd counting and focus mainly on the diverse network architectures against which our proposed crowd counting model is compared. Subsequently, we introduce works related to curriculum learning and how they can potentially be used in the task of crowd counting.\n\n\nA. Crowd Counting Networks\n\nSuccessful efforts have been devoted to the design of novel network architectures to improve the performance of crowd counting. Commonly used principles of network design for crowd counting include multi-column networks, rich feature fusion and attention mechanism.\n\nMulti-column neural networks were employed to address the scale-variant issue in crowd counting [18], [29], [30]. As one of the earliest CNN based models for crowd counting, MCNN [18] consists of three branches aiming to handle crowds of different densities. Following this idea, Sam et al. [29] proposed SwitchCNN which employs a classifier to explicitly select one of the three branches for a given input patch based on its level of crowd density. While these methods aim to use different kernel sizes in different branches to capture scale-variant information, Liu et al. [31] proposed a model consisting of multiple branches of VGG16 networks with shared weights to process scaled input images respectively. Similarly, Ranjan et al. [32] devised a two-column network which learns the low-and high-resolution density maps iteratively via two branches of CNN. The success of these specially designed network architectures has validated that multi-column CNN models are capable of capturing scale-variant features for crowd counting.\n\nThe second direction of network design is to pursue effective fusion of rich features from different layers [25], [33]. These attempts are based on the fact different layers have variant receptive fields hence capturing features of variantscale information. Different feature fusion strategies including direct fusion [33], top-down fusion [34] and bidirectional fusion [35] have been employed in crowd counting.\n\nTo take advantage of the two aforementioned ideas for crowd counting, one straightforward solution is to utilise the Inception module [23] which was firstly proposed in [36] and has evolved into a variety of more efficient forms to date. The Inception modules have been employed in crowd counting models before in SANet [24] and the TEDNet [25]. Both of them use only the basic types of Inception modules similar to those used in the first version of Inception net (i.e. GoogLeNet [36]). In our work, we aim to explore the more advanced Inception modules in the framework of Inception-v3.\n\nThe attention mechanism is another useful technique considered when designing network architectures for crowd counting [21], [26], [33], [37]. Attention layers are usually combined with multi-column structures so that regions of different semantic information (e.g., background, sparse, dense, etc.) can be attended and processed by different branches respectively. Attention maps learned by these models have proved to be aware of semantic regions [26], however, they cannot provide fine-grained scale awareness within the images. To address this issue explicitly, perspective maps have been employed to guide the accurate estimation of density maps [38]- [40]. In many scenarios where the perspective maps are not available, it is possible to estimate these perspective maps from the crowd images via a specially designed and trained network [41].\n\nAlternatively, binary segmentation maps generated from point annotation [42] are introduced as additional supervision for the training of crowd counting networks via multi-task learning [42]. In our work, binary segmentation maps are treated as explicit attention maps guiding the learning of salient visual features for density map estimation. In this sense, our work is more related to [43] and [44] in which the segmentation maps are also used as attention maps but in essentially different ways as explained in Section III-B and validated in Section IV-F.\n\n\nB. Curriculum Learning\n\nCurriculum learning is a strategy of model training (e.g., neural networks) in machine learning and was proposed by Bengio et al. [45]. The idea of curriculum learning can date back to no later than 1993 when Elman [46] proved the benefit of training neural networks to learn a simple grammar by \"starting small\". The strategy of curriculum learning is inspired by the way how humans learn knowledge from easy concepts to hard abstractions gradually. In the specific case of training a machine learning model, curriculum learning selects easy examples at the beginning of training and allows more difficult ones added to the training set gradually. A curriculum is usually defined as a ranking of training examples by some prior knowledge to determine the level of difficulty of a given example. Jiang et al. [47] extended curriculum learning to a so-called self-paced curriculum learning by integrated the ideas of original curriculum learning and self-paced learning [48] in a unified framework.\n\nIn this work, we apply the strategy of curriculum learning in crowd counting to address the issue of large variance of the crowd density in the images. Curriculum learning has been employed for crowd counting in [49] where the curriculum is designed on the image level, i.e., a difficulty score is calculated for each training image. The training images are divided into multiple subsets based on their difficulty scores and the easiest subset is added into the training set first. By contrast, our curriculum learning strategy is characterized by a novel curriculum loss defined on the pixel level as described in Section III-D. We define that density map pixels of higher values than a threshold have higher difficulty scores because these pixels are within regions of denser crowds. We use all training images throughout the training process but set the threshold to a low value at the beginning and increase it gradually so that the difficult pixels become easy ones and contribute more to the training. As a result, our curriculum learning strategy is simple to implement with zero extra cost and has been proved effective especially when there exist extremely dense crowd regions in the images.\n\n\nIII. SEGMENTATION GUIDED ATTENTION NETWORK\n\nCrowd counting is formulated as a density map regression problem in this study. Given a crowd image I , we aim to learn a Fully Convolutional Network (FCN) denoted as F so that the corresponding density map M den can be estimated by:\nM den = F (I ; ).(1)\nwhere is a collection of parameters of the FCN. As shown in Figure 1, our proposed network is adapted from the famous Inception-v3 originally designed for image classification by Google Research [23]. We first modify Inception-v3 to an FCN so that it can process images of arbitrary sizes and generates the estimated density maps M den as the outputs. An attention layer is added to the network to filter out features within the background region and concentrate on the foreground features for accurate density map estimation. Since the attention maps generated by this attention layer aim to discriminate the regions of background and foreground of the feature maps, we use a ground truth segmentation map, which can be easily derived from point annotations, as extra guidance for the training of the attention layer. As a result, the learned attention maps are forced to be similar to the segmentation maps during training.\n\nWe also investigate the use of curriculum loss in the training of crowd counting networks. Specifically, we define a curriculum based on the pixel-wise difficulty level so that the network starts training by focusing more on the \"easy\" regions (sparse) within the density maps and down-weighting the \"hard\" pixels (dense). During training, the \"hard\" pixels are gradually exposed to the model and finally, the learned model can perform well for all situations.\n\n\nA. Density and Segmentation Maps\n\nIn this study, we use simple ways to generate density and segmentation maps from the point annotations although more complicated ones [44] might benefit the performance. For density maps M den \u2208 R + H \u00d7W , where H and W are the height and width of the image, we follow [18] using a Gaussian kernel G \u03c3 \u2208 R + 15\u00d715 with a kernel size of 15 \u00d7 15 and fixed \u03c3 = 4:\nM den (x) = H (x) * G \u03c3 (x),(2)\nwhere For segmentation maps M seg \u2208 {0, 1} H \u00d7W , we use a similar method:\nH (x) = N i=1 \u03b4(x \u2212 x i ), NM seg (x) = H (x) * J n (x),(3)\nwhere J n (x) is an all-one matrix of size n \u00d7 n centred at the position x. As a result, ones and zeros in the matrix M seg denote the pixels belong to the foreground and background regions, respectively. We empirically set n = 25 across all our experiments to ensure that a specific head within an image is characterized by more pixels in the segmentation map than in the density map to avoid losing useful contextual information.\n\nOur choice of n = 25 cannot guarantee precise foreground segmentation maps due to the head-scale variance. A larger value would affect the discrimination of the foreground and background in crowded regions. Our experimental results demonstrate the our choice is suitable and can benefits the density map estimation.\n\n\nB. Network Configuration\n\nInstead of designing a novel network from scratch, we exploit the state-of-the-art CNN model for image classification Inception-v3 in our study. To apply the original Inception-v3 network in crowd counting, some favourable modifications have been made. Firstly, we remove the final fully-connected layers and reserve all the convolutional layers. The input size of the original Inception-v3 network is 299 \u00d7 299 and the output size of the final convolutional layer is 8 \u00d7 8. That is to say, feature maps generated by the last convolutional layer have approximately 1 2 5 spatial resolutions of the input image. This is achieved by the first convolutional layers (stride of 2), two max-pooling layers (stride of 2) and two Inception modules in which max-pooling (stride of 2) is employed. To ensure the outputs of the network (i.e. estimated density maps) have sufficient spatial resolutions, we remove the first two max-pooling layers from the original network and add one upsample layer before the final Inception module. As a result, the output of the modified network has exactly 1 4 spatial resolution of the input image when the input size is 2 n (e.g., 128 \u00d7 128 in our case). Such modification does not change the number of parameters of the network hence the pre-trained weights can be directly loaded and used. However, since the spatial resolutions of intermediate feature maps have been increased, the number of operations is also increased. This modified model will also denoted as Inception-v3 without introducing ambiguity and used as a baseline method in our experiments. Fig. 1. The framework of our proposed segmentation guided attention network (SGANet) which is adapted from Inception-v3 by: 1) removing the fullyconnected layers; 2) removing two maxpooling layers to reserve high spatial resolution feature maps; 3) adding an upsampling layer before the last Inception module; 4) adding an attention layer whose output is applied to the feature maps generated by the last Inception module; and 5) adding a convolutional layer for density map estimation.\n\nDistinct from existing works using the segmentation map in the framework of multi-task learning [42] to extract more salient features for density map estimation, we claim that the segmentation map can be used as an ideal attention map to emphasize the contributions of features within the foreground regions to the density map estimation whilst compressing the effects of features within the background regions. To this end, we add an attention layer to estimate the attention map. The attention layer is a convolutional layer followed by a sigmoid layer which restricts the output values in the range of 0-1. The attention layer takes the feature maps generated by the second last Inception module as input and outputs a one-channel attention map of the same spatial resolution as the input. Subsequently, the attention map estimated by the attention layer is applied to the feature map generated by the last Inception module by an elementwise multiplication with each channel of the feature map.\nF l = F l\u22121 M att ,(4)\nwhere denotes the operation of element-wise product. The attention layer designed in our framework is similar to that in [43], [44]. However, a so-called inverse attention map is estimated in [43] while our attention layer generates an attention map directly applied to the feature map. Also, the foreground regions in the ground truth segmentation map in [43] are derived by thresholding the density map hence both maps have the same positive fields for each head while ours are different (c.f. Eq.(2-3)). In [44], the attention layer takes the feature map as input to estimate an attention map which again is applied to the same feature map. This may limit the capacity of the model since it is forced to learn two different maps from the same feature map via two convolutional layers which have limited parameters. In contrast, as mentioned above, the input of our attention layer is the feature map from the previous layer which has higher spatial resolutions and is different from the one the generated attention map will be applied to. These favourable distinctions collectively benefit the estimation of the density map and will be empirically evaluated in our experiments.\n\n\nC. Loss Function\n\nWe first describe the loss function used to train the SGANet without curriculum loss in this section and describe the curriculum loss in the following section. The loss function consists of two components. The first one is the Mean Squared Error (MSE) loss applied to the estimation of the density map and is denoted as L den . The density map loss can be calculated as follows:\nL den ( ) = 1 2N N i=1 ||M den i \u2212 M den i || 2 F ,(5)\nwhere || \u00b7 || 2 F is a Frobenius norm of a matrix. The second component of the loss function is the segmentation map loss L seg which is defined as the cross-entropy loss:\nL seg ( ) = \u2212 1 N N i=1 j,k H i ( j, k),(6)H i = M seg i log(M seg i ) + (1 \u2212 M seg i ) log(1 \u2212M seg i ),(7)\nwhere denotes elementwise multiplication of two matrices with the same size and H( j, k) denotes an element of the matrix H. These two components are combined during network training and the compositional loss function is:\nL( ) = L den ( ) + \u03bbL seg ( )(8)\nwhere \u03bb is a hyper-parameter which ensures the two components to have comparable values and is set 20 across our experiments.\n\n\nD. Curriculum Loss\n\nTo benefit from the strategy of curriculum learning, we present a novel curriculum loss function in this section to replace the traditional density map loss function defined in Eq. (5). The curriculum loss function is designed to be aware of the pixel-wise difficulty level when computing the density map loss. Based on the fact that dense crowds are generally more difficult to count than sparse ones, we design a curriculum where pixels of higher values than a dynamic threshold in the density map are defined as difficult pixels. We set the dynamic threshold and assign variant weights to different pixels of the density map when calculating the density map loss. Specifically, we define a weight matrix W as follows:\nW = T (e) max{M den \u2212 T (e), 0} + T (e) .(9)\nThe weight matrix W has the same size as the density map matrix M den used in Eq.(5) and the pixel-wise weights are determined by the dynamic threshold T (e) and the pixel values in the density map. If the pixel value of the density map is higher than the threshold, this pixel is treated as a difficult one and the corresponding weight is set less than one, otherwise the weight is equal to one. The higher the pixel values are, the smaller the weights will be. As a result, the training will focus more on the pixels of lower density value than T (e).\n\nThe dynamic threshold T (e) is defined as a function of the training epoch index e in the form of:\nT (e) = ke + b(10)\nwhere k and b can be determined empirically in the following way. The value of b is the initial threshold when epoch = 0 and it is set to be equivalent to the maximum density value in the region characterizing a single head (as shown in Figure 2). As a result, all the density values within the regions where heads are not overlapped are smaller than the threshold T (e) throughout the training (i.e., e = 0, 1, 2, . . .). On the other hand, the value of k controls the speed of increasing the difficulty and its value is determined so that T (e) increases to a value higher than the maximum density values before training stops. This guarantees the final weight matrix W has all one values hence all pixels contribute equally to the training. In practice, we just need to find the density value in the center of a single-head region in a ground truth density map to determine b, whilst the maximum pixel value of the density maps in the training data and the number of training epochs collaboratively determine k. Finally, the curriculum loss function for density map can be derived by modifying Eq.(5) as:\nL den ( ) = 1 2N N i=1 ||W(e) (M den i \u2212 M den i )|| 2 F .(11)\nwhere W(e) is also a function with respect to the training epoch index e.\n\n\nIV. EXPERIMENTS\n\nExtensive experiments have been conducted on benchmark datasets to evaluate the performance of SGANet and the effectiveness of curriculum loss in crowd counting. We will briefly describe the datasets and evaluation metrics used in our experiments, details of experimental protocols and network training. Experimental results are compared with state-of-theart methods and analysed. We also present an ablation study to investigate the contributions of different components to the performance of the proposed framework.\n\n\nA. Datasets\n\nShanghaiTech dataset was collected and published by Zhang et al. [18] consisting of two parts. Part A consists of 300 and 182 images of different resolutions for training and testing respectively. The minimum and maximum counts are 33 and 3139 respectively, and the average count is 501.4. Part B consists of 400 and 316 images of a unique resolution (768 \u00d7 1024) for training and testing respectively. Compared with part A, the numbers of people in these images are much smaller with the minimum and maximum counts of 9 and 578 respectively, and the average count is 123.6.\n\nUCF_QNRF dataset [19] contains 1,535 high-quality images, among which 1201 images are used for training and 334 images for testing. The minimum and maximum counts are 49 and 12,865 respectively, and the average count is 815.\n\nUCF_CC_50 dataset [50] contains 50 images with the minimum and maximum counts of 94 and 4,534 respectively. It is a challenging dataset due to the limited number of images. Following the suggestion in [50] and many other works, we use 5-fold cross-validation in our experiments.\n\n\nB. Evaluation Metrics\n\nWe follow the previous works using two metrics, i.e., the mean absolute error (MAE) and the root mean squared error (RMSE), to evaluate the performance of different models in our experiments. The two metrics can be calculated as follows:\nM AE = 1 N test N test i=1 |y i \u2212\u0177 i |(12)RM S E = 1 N test N test i=1 (y i \u2212\u0177 i ) 2(13)\nwhere y i and\u0177 i are the ground truth and predicted count for i -th test image respectively, N test is the number of test images.\n\n\nC. Network Training\n\nSGANet is implemented in PyTorch [51] and the source code is publicly available. 1 The \"Adam\" optimizer [52] is employed for training. The initial learning rate is set to 1e-4 and reduced by a factor of 0.5 after every 50 epochs. The total number of training epochs is set 500 since the model can always converge much earlier than that. The network is trained with image patches with a size of 128 \u00d7 128 randomly cropped from the training images. Instead of preparing the patches in advance, we do the random patch cropping on-the-fly during training. Specifically, we randomly select 8 images from the training set and 4 patches are randomly cropped from each selected image. This leads to a batch of 32 training patches in each iteration of training. The training patches generated in this way can be more diverse and help to alleviate the potential over-fitting problem. Since the output of SGANet has the size of 32 \u00d7 32 (i.e. 1/4 of the input size), we use sum-pooling to adapt the ground truth density and segmentation map so that they have the same size of 32 \u00d7 32 as the output. The training patches, as well as their corresponding density and segmentation maps, are horizontally flipped with a probability of 0.5 for data augmentation which has been shown beneficial in many works [26], [53]. For testing, we feed the whole image into the network and obtain the density map from which the predicted count can be computed. For the UCF_QNRF dataset, to save the memory usage during testing, we also resize the images from both training and test sets so that all images are limited to have their longer sides no higher than 2048 whilst the original aspect ratios are kept, if not specified otherwise.\n\n\nD. Comparative Study\n\nWe select both classic and state-of-the-art models for the comparison, including MCNN [18] which is a three-column CNN, CSRNet [54] which uses VGG16 as the front-end and dilated convolutional layers as the back-end, SANet [24] which employs the basic Inception modules but has a relatively shallow depth, DADNet [26] which employs the ideas of dilated convolution, attention map and deformable convolution in the framework, CANNet [55] which captures context-aware feature by multiple branches, TEDNet [25] which also uses Inception-style modules, RANet [53] which uses an iterative distillation algorithm, ANF [57] which uses conditional random fields (CRFs) to aggregate multi-scale features, and SPANet [58] which incorporates the spatial context within images into the crowd counting model.\n\nThe experimental results are listed in Table I where the best result in each column is highlighted in bold and the second best in underscored italic. From Table I Overall, our proposed SGANet with the combination of Inception-v3 and a segmentation guided attention layer can achieve state-of-the-art performance on several benchmark datasets.\n\nThe use of curriculum loss (SGANet + CL) further improves the performance of SGANet on three out of four datasets in terms of MAE and these three datasets (i.e. ShanghaiTech part A, UCF_QNRF and UCF_CC_50) consist of crowds with significant density variations. On the Shang-haiTech part B dataset, the use of curriculum loss does not improve the performance because the images from this dataset contain crowds with a relatively small variance of head scales. However, we also observe slight increases of MSE on Shang-haiTech part A and UCF-QNRF datasets when curriculum loss is applied. This demonstrates the limitation of curriculum loss in the cases where extreme crowds exist. Curriculum loss cares more about regions with lower density from the very beginning of the training process and gradually attends the regions with higher density. As a result, the regions with very high density can be less exposed to the learning process. The resultant model performs well for the majority of the regions but also suffers from large errors in the regions of extreme density. These sparse large errors can contribute to MSE more significantly than to MAE on datasets containing very crowded images. In summary, these results provide evidence that the issue of large scale variance can be partially alleviated by the use of our proposed curriculum loss. We will provide more evidence for the effectiveness of curriculum loss in the following ablation study.\n\n\nE. Results on Curriculum Loss\n\nThe use of curriculum loss has shown a positive effect when applied to SGANet for crowd counting (Table I). In this section, we attempt to explore the effectiveness of curriculum learning in the training of other crowd counting networks. To this end, we consider \"MCNN\", \"CSRNet\", \"SANet\", \"CANNet\", \"DADNet\" and our modified \"Inception-v3\" and use the curriculum loss when training these networks on ShanghaiTech part A. Firstly, we try to reproduce the results of these crowd counting models using conventional density map loss under our training protocols to remove the effects of various factors such as the ways of density map generation, patch cropping, data augmentation and so on for a fair comparison and focus on the effect of curriculum loss. It is noteworthy that the generated density maps have different sizes for these models (e.g., the size ratio between input and output is 1 for \"SANet\", 2 for \"DADNet\", 4 four \"MCNN\" and \"Inception-v3\", 8 for \"CSRNet\" and \"CANNet\"). The ground truth density maps need to be resized by sum pooling to have the same size as the corresponding outputs. As a result, the pixel  I   COMPARISON RESULTS WITH STATE-OF-THE-ART MODELS FOR CROWD COUNTING (-DENOTES THE RESULTS ARE  NOT AVAILABLE; CL DENOTES CURRICULUM LOSS)  values of the ground truth density maps for different models will have different distributions. This leads to model-specific curriculum designs (i.e. the parameter values in Eq. (10)). Specifically, we set b as the maximum value in the Gaussian kernel matrix G \u03c3 used for density map generation (c.f. Eq. (2)) so that the sparse crowd regions without annotation overlapping will not be affected throughout the training process. The value of k in Eq. (10) is determined by the number of epochs so that all the crowd regions will contribute to the loss equally before training is finished. In our experiments, we set k = 1e \u2212 3 and b = 0.1 for SGANet. Experimental results are shown in Table II. The use of curriculum loss improves the performance of most models. Specifically, the MAE decreases for all models except \"DADNet\" and the RMSE decrease for all models except \"CSRNet\". These experimental results demonstrate the curriculum loss is useful not only for our SGANet but also many other crowd counting models.\n\nTo evaluate the effect of crowd density in the performance of SGANet and the curriculum loss, an additional experiment is conducted on the UCF_QNRF dataset. As mentioned above, we have changed the image resolutions in this dataset to be no higher than 2048 for computation efficiency. In this experiment, we create two more datasets by setting the image resolution thresholds as 1024 and 512 respectively. As a result, the images in the UCF_QNRF_512 dataset will have higher crowd density than those in the UCF_QNRF_1024 dataset which again consists of denser crowds than the UCF_QNRF_2048 dataset. We use SGANet on these three datasets and the experimental results are shown in Table III. It is obvious the image resolutions make a significant different in the performance and the models perform the best on the UCF_QNRF_2048 dataset whose image resolutions are higher hence have less crowded images. By comparing the performance of SGANet without and with curriculum loss, the use of curriculum loss leads to better results on all three datasets in terms of both MAE and RMSE except that in the last column of Table III. The performance gains achieved by the use of curriculum loss are also related to the image resolutions or the crowd densities in the datasets. Specifically, the MAE and RMSE are reduced by 7.6 and 18.9 respectively on UCF_QNRF_512, 5.0 and 9.2 on UCF_QNRF_1024, 1.5 and \u22121.9 on UCF_QNRF_2048. These results provide more evidence that the use of curriculum loss is more effective when the crowds are denser in the images.\n\nIn summary, the experimental results in Tables I-III provide sufficient evidence that the use of curriculum learning can benefit the training of crowd counting models in most cases especially when the head scales vary a lot in the crowd images.\n\n\nF. Results on Segmentation Guided Attention\n\nFrom Table I we can see the performance enhancement contributed by the segmentation guided attention layer by comparing the performance between Inception-v3 and SGANet. To validate the superiority of our segmentation guided attention layer to other similar designs [44], we conduct an experiment on ShanghaiTech part A and UCF_QNRF. In this experiment, we follow [44] and modify the SGANet by feeding the feature maps of the last Inception module into the attention layer and keeping the rest unchanged. The experimental results are shown in Table IV from which we conclude the way segmentation maps are used in our SGANet outperforms that in [44].   To give an intuitive evidence on how the attention layer helps for density map estimation, we visualize the estimated attention maps and density maps for five exemplar test images from ShanghaiTech part A. In Figure 3, we show the original images, ground truth density maps, predicted density maps and predicted segmentation maps in four columns respectively. The real and predicted counts are also shown on the density maps for a direct comparison. We can see that the prediction errors for the top three examples are relatively low given the accurately predicted segmentation maps. However, the bottom two images suffer from higher errors since the model can not predict accurate foreground regions. For example, the image in the fourth row contains people raising their hands in the air and the hands are easy to be counted since they have similar colours with human faces. In the bottom image, the trees in the background are mistakenly recognised as foreground and result in the over-estimated count.\n\n\nG. Results on Typical Deep Neural Networks\n\nIn this experiment, we compare the performance of Inception-v3 and our proposed variants with several typical deep neural networks. Specifically, we consider the most popular and performant models including VGG16bn (VGG16 with batch normalisation layers) [59], ResNet50 [60], ResNet101 [60], DenseNet121 [60] and ShuffleNet-v2-0.5x [62]. Similar to the modifications we have made on Inception-v3, we replace the final fully-connected layers with convolutional layers for density map estimation. For each model, we also consider their counterpart with the segmentation guided attention map (those marked with * in Table V). Specifically, an attention map is learned from an intermediate feature map close to the final output layer by two convolutional layers. Similar to our SGANet, the attention map is learned by the supervision of the segmentation map and applied to the final feature map before the density map estimation.\n\nExperiments are conducted on the four crowd counting benchmark datasets and the results are shown in Table V. For each model, we repeat the experiment for five times with random initialisation to get the statistics (i.e. mean \u00b1 standard deviation) as reported in Table V. In general, Inception-v3 performs significantly better than other five models on all three out of four datasets. On the UCF_QNRF dataset, Inception-v3 performs comparably with ResNet50, ResNet101 and DenseNet121. This demonstrates that inception modules in Inception-v3 are beneficial to crowd counting since the inception module was designed to capture different scales of contextual information in each convolutional layer. ResNet50 achieves the second best overall performance over four benchmark datasets whilst the deeper version ResNet101 performs consistently worse than ResNet50. This phenomenon is also observed when a deeper version of DenseNet121 was employed in our preliminary experiments on the ShanghaiTech A dataset which are not presented here. This may be due to the fact that the outputs of deeper models have lower spatial resolution and lead to less accurate density map estimation. Among six investigated DNN models, ShuffleNet-v2 performs the worst and this is expected since this model has a significant smaller number of parameters (0.7M) than others (7.1-42.8M). Our proposed SGANet (a variant of Inception-v3) with or without the curriculum loss can generally achieve statistically significant better performance than the original Inception-v3 with only negligible additional parameters. The effectiveness of segmentation guided attention maps is also observed consistently when they are added to the other deep models. As shown in Table V, for five considered deep models, their variants with the use of segmentation guided attention maps achieve better performance in almost all cases.\n\n\nH. Results on Cross-Dataset Transfer Learning\n\nIn this experiment, we investigate the capabilities of cross-dataset transfer learning of different baseline models and our proposed methods. To this end, we train the models on UCF_QNRF training data and test them on ShanghaiTech A and B test data. We choose UCF_QNRF as the training data due to the fact it consists of much more training images than other datasets and the training images have a large range of resolutions. Experimental results are shown in Table VI. Again, we repeat each experiment for five times to get the statistics. The experimental results show that models trained on UCF_QNRF perform slightly worse than those trained within datasets without the need of transfer learning. This is due to the distribution shift across different datasets. One exception is DenseNet121 performs better on ShanghaiTech A test data when it is trained on the UCF_QNRF training data. Other than this exception, the performances of different models on cross-dataset transfer learning are consistent with the results in Table V. Our proposed methods SGANet with or without curriculum loss outperform other comparative models. These results provide evidence the proposed methods based on the Inception-v3 are more capable of transfer learning across datasets hence are more useful in practice.\n\n\nV. DISCUSSION AND CONCLUSION\n\nIn this paper, we address an important problem in crowd counting which can be of great values to intelligent transportation systems. We investigated the effectiveness of Inception-v3 in crowd counting and proposed a segmentation guided attention network using Inception-v3 as the backbone. We also proposed a novel curriculum loss function for crowd counting by defining pixel-wise difficulty levels to resolve the issue of scale variance in crowd images. Experimental results on four commonly used datasets demonstrate the proposed SGANet can achieve superior performance due to the combination of Inception-v3 and the segmentation guided attention layer. The proposed strategy of curriculum learning is also proved to be helpful for a variety of existing crowd counting models in general.\n\nAlthough the proposed two strategies can promote the crowd counting performance in most scenarios, there exist cases where they could fail. For example, when the heads in images are less crowded (e.g., ShanghaiTech part B), both the segmentation guided attention and the curriculum loss will not make a difference to the counting accuracy since they can provide little additional information for the learning process in these situations. As a result, we can expect more benefit from the proposed two strategies when the images contain extremely dense crowds, otherwise a more powerful backbone such as Inception-v3 will be the optimal solution to achieve high counting accuracy. This is the first attempt to use the whole Inception-v3 model for crowd counting and achieves state-of-the-art performance on commonly used datasets. Although the employed Inception-v3 model (with our own modifications) is not designed from scratch, it is quantitatively shown to be able to achieve superior performance to many specially designed models in the recent couple of years. To these ends, our work is both disruptive and important to the crowd counting research community. Researchers in this community have devoted too much effort to the design of variant CNN architectures and most of them are based on the pre-trained VGG16 model which just has insufficient expressive capacity for crowd counting tasks. In this sense, we believe it is important and necessary to make the community aware of the fact Inception-v3 is a more suitable architecture for effective crowd counting and divert the attention of the community to more diverse research directions.\n\nMost existing crowd counting methods including ours in this paper rely on a large amount of training data which require extensive efforts of data collection and annotation. In realworld applications, it is challenging to get access to sufficient training data for various scenarios (e.g., different camera resolutions, illumination conditions, weather conditions and perspectives). To solve this realistic problem, our future work will focus on weakly supervised learning such as domain adaptation [63] and transfer learning [28] for which the method proposed in this paper can be served as a strong baseline. On the other hand, our proposed method using Inception-v3 as the backbone also inherits its limitations that it suffers from gradient vanishing issues when becoming deeper. To resolve this issue, the skip connections [60] and self-attention modules [64] should be considered in the future work.\n\n\nis the number of point annotations in the image and \u03b4(\u00b7) is the Delta function. As a result, H (x) is a binary matrix of the same size as the image and only has values of ones at the positions of point annotations. The density map is derived by the convolution between H (x) and the Gaussian kernel G \u03c3 (x).\n\nFig. 2 .\n2An illustration of how to determine the value of b in Eq. (10).\n\nFig. 3 .\n3Visualization of estimated density and segmentation maps for five test images from ShanghaiTech part A. The numbers shown on the images in the second and third columns are the ground truth and estimated counts respectively.\n\n\nreceived 30 July 2020; revised 30 March 2021 and 29 August 2021; accepted 14 December 2021. Date of publication 11 January 2022; date of current version 12 September 2022. The Associate Editor for this article was K. Wang. (Corresponding author: Qian Wang.) Qian Wang is with the Department of Computer Science, Durham University, Durham DH1 3LE, U.K. (e-mail: qian.wang173@hotmail.com). Toby P. Breckon is with the Department of Computer Science and the Department of Engineering, Durham University, Durham DH1 3LE, U.K. (e-mail: toby.breckon@durham.ac.uk). Digital Object Identifier 10.1109/TITS.2021.3138896\n\n\n, we can see our modified Inception-v3 can achieve very competitive performance on all four datasets. Especially on ShanghaiTech part B, it achieves the second best MAE of 6.4 and the best RMSE of 9.8. On the UCF_QNRF dataset, Inception-v3 also achieves significantly better results than most existing models including TEDNet (MAE: 95.6 vs. 113 and MSE: 165.4 vs. 188) which also employs the Inception modules. These results demonstrate the superiority of heterogeneous Inception modules in classification problems can be transferred to the task of crowd counting hence different Inception modules deserve more attention when designing a novel CNN architecture for crowd counting as well as other tasks suffering from the issue of scale variance. On the other hand, the disruptive performance of Inception-v3 in crowd counting provides more insight for the research community regarding the selection of backbone models when designing novel network architectures for crowd counting.By adding the segmentation guided attention layer, our SGANet can achieve better performance on all datasets in terms of MAE (i.e. 58.0 vs.60.1 for ShanghaiTech part A, 89.1 vs. 95.6 for UCF-QNRF and 224.6 vs. 236.0 for UCF-CC-50), although the improvement on ShanghaiTech part B dataset is very marginal (i.e. 6.3 vs. 6.4). Regarding RMSE, SGANet achieves better performance on ShanghaiTech part A (i.e. 100.4 vs. 105.0) and UCF_QNRF (i.e. 150.6 vs. 165.4) but worse results on the other two datasets (i.e. 10.6 vs. 9.8 for ShanghaiTech part B and 314.6 vs. 304.9 for UCF_CC_50).\n\nTABLE\n\n\nTABLE II THE\nIIEFFECT OF CURRICULUM LEARNING IN DIFFERENT MODELS ON \nSHANGHAITECH PART A (THE SYMBOL \u2193 MEANS THE ERROR \nDECREASES WITH THE USE OF CURRICULUM LOSS) \n\n\n\nTABLE III THE\nIIIEFFECT OF IMAGE RESOLUTIONS IN THE PERFORMANCE OF SGANET ON UCF_QNRF DATASET\n\nTABLE IV RESULTS\nIVOF DIFFERENT APPROACHES TO SEGMENTATION MAP SUPERVISION\n\nTABLE V\nVCOMPARISON RESULTS OF DIFFERENT TYPICAL DEEP NEURAL NETWORKS (MEAN \u00b1 STD OVER FIVE TRIALS ARE REPORTED)\n\nTABLE VI RESULTS\nVIOF CROSS-DATASET TRANSFER LEARNING (MEAN \u00b1 STD OVER FIVE TRIALS ARE REPORTED)\nhttps://github.com/hellowangqian/sganet-crowd-counting\n\nCrowd counting with limited labeling through submodular frame selection. Q Zhou, J Zhang, L Che, H Shan, J Z Wang, IEEE Trans. Intell. Transp. Syst. 205Q. Zhou, J. Zhang, L. Che, H. Shan, and J. Z. Wang, \"Crowd counting with limited labeling through submodular frame selection,\" IEEE Trans. Intell. Transp. Syst., vol. 20, no. 5, pp. 1728-1738, May 2019.\n\nCrowd analysis: A survey. B Zhan, D N Monekosso, P Remagnino, S A Velastin, L.-Q Xu, Mach. Vis. Appl. 19B. Zhan, D. N. Monekosso, P. Remagnino, S. A. Velastin, and L.-Q. Xu, \"Crowd analysis: A survey,\" Mach. Vis. Appl., vol. 19, nos. 5-6, pp. 345-357, 2008.\n\nAn evaluation of crowd counting methods, features and regression models. D Ryan, S Denman, S Sridharan, C Fookes, Comput. Vis. Image Understand. 130D. Ryan, S. Denman, S. Sridharan, and C. Fookes, \"An evaluation of crowd counting methods, features and regression models,\" Comput. Vis. Image Understand., vol. 130, pp. 1-17, Jan. 2015.\n\nA survey of recent advances in CNN-based single image crowd counting and density estimation. V Sindagi, V M Patel, Pattern Recognit. Lett. 107V. Sindagi and V. M. Patel, \"A survey of recent advances in CNN-based single image crowd counting and density estimation,\" Pattern Recognit. Lett., vol. 107, pp. 3-16, May 2018.\n\nCrowd density estimation using fusion of multi-layer features. X Ding, F He, Z Lin, Y Wang, H Guo, Y Huang, IEEE Trans. Intell. Transp. Syst. 228X. Ding, F. He, Z. Lin, Y. Wang, H. Guo, and Y. Huang, \"Crowd density estimation using fusion of multi-layer features,\" IEEE Trans. Intell. Transp. Syst., vol. 22, no. 8, pp. 4776-4787, Aug. 2021.\n\nMicroscopy cell counting and detection with fully convolutional regression networks. W Xie, J A Noble, A Zisserman, Comput. Methods Biomech. Biomed. Eng., Imag. Vis. 63W. Xie, J. A. Noble, and A. Zisserman, \"Microscopy cell counting and detection with fully convolutional regression networks,\" Comput. Methods Biomech. Biomed. Eng., Imag. Vis., vol. 6, no. 3, pp. 283-292, 2018.\n\nCounting and classification of highway vehicles by regression analysis. M Liang, X Huang, C.-H Chen, X Chen, A Tokuta, IEEE Trans. Intell. Transp. Syst. 165M. Liang, X. Huang, C.-H. Chen, X. Chen, and A. Tokuta, \"Counting and classification of highway vehicles by regression analysis,\" IEEE Trans. Intell. Transp. Syst., vol. 16, no. 5, pp. 2878-2888, Oct. 2015.\n\nAutomatic car counting method for unmanned aerial vehicle images. T Moranduzzo, F Melgani, IEEE Trans. Geosci. Remote Sens. 523T. Moranduzzo and F. Melgani, \"Automatic car counting method for unmanned aerial vehicle images,\" IEEE Trans. Geosci. Remote Sens., vol. 52, no. 3, pp. 1635-1647, Mar. 2014.\n\nLearning to count leaves in rosette plants. M V Giuffrida, M Minervini, S Tsaftaris, 10.5244/C.29.CVPPP.1Proc. Comput. Vis. Problems Plant Phenotyping. CVPPP), S. A. Tsaftaris, H. Scharr, and T. PridmoreComput. Vis. Problems Plant PhenotypingBMVA Press13M. V. Giuffrida, M. Minervini, and S. Tsaftaris, \"Learning to count leaves in rosette plants,\" in Proc. Comput. Vis. Problems Plant Phe- notyping (CVPPP), S. A. Tsaftaris, H. Scharr, and T. Pridmore, Eds. BMVA Press, Sep. 2015, pp. 1.1-1.13, doi: 10.5244/C.29.CVPPP.1.\n\nLeaf counting with deep convolutional and deconvolutional networks. S Aich, I Stavness, Proc. ICCV Workshop. ICCV WorkshopVenice, ItalyS. Aich and I. Stavness, \"Leaf counting with deep convolutional and deconvolutional networks,\" in Proc. ICCV Workshop, Venice, Italy, Oct. 2017, pp. 22-29.\n\nBayesian human segmentation in crowded situations. T Zhao, R Nevatia, Proc. CVPR. CVPR459T. Zhao and R. Nevatia, \"Bayesian human segmentation in crowded situations,\" in Proc. CVPR, 2003, p. 459.\n\nFast crowd segmentation using shape indexing. L Dong, V Parameswaran, V Ramesh, I Zoghlami, Proc. ICCV. ICCVL. Dong, V. Parameswaran, V. Ramesh, and I. Zoghlami, \"Fast crowd segmentation using shape indexing,\" in Proc. ICCV, 2007, pp. 1-8.\n\nCounting people in the crowd using a generic head detector. V B Subburaman, A Descamps, C Carincotte, Proc. IEEE 9th Int. Conf. Adv. Video Signal-Based Surveill. IEEE 9th Int. Conf. Adv. Video Signal-Based SurveillV. B. Subburaman, A. Descamps, and C. Carincotte, \"Counting people in the crowd using a generic head detector,\" in Proc. IEEE 9th Int. Conf. Adv. Video Signal-Based Surveill., Sep. 2012, pp. 470-475.\n\nA viewpoint invariant approach for crowd counting. D Kong, D Gray, H Tao, Proc. ICPR. ICPR3D. Kong, D. Gray, and H. Tao, \"A viewpoint invariant approach for crowd counting,\" in Proc. ICPR, vol. 3, 2006, pp. 1187-1190.\n\nReal-time, embedded scene invariant crowd counting using scale-normalized histogram of moving gradients (HoMG). P Siva, M J Shafiee, M Jamieson, A Wong, CVPR Workshop. P. Siva, M. J. Shafiee, M. Jamieson, and A. Wong, \"Real-time, embedded scene invariant crowd counting using scale-normalized histogram of moving gradients (HoMG),\" in CVPR Workshop, Jun. 2016, pp. 67-74.\n\nLearning to count objects in images. V Lempitsky, A Zisserman, Proc. NIPS. NIPSV. Lempitsky and A. Zisserman, \"Learning to count objects in images,\" in Proc. NIPS, 2010, pp. 1324-1332.\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystA. Krizhevsky, I. Sutskever, and G. E. Hinton, \"ImageNet classification with deep convolutional neural networks,\" in Proc. Adv. Neural Inf. Process. Syst., 2012, pp. 1097-1105.\n\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma, \"Single-image crowd counting via multi-column convolutional neural network,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 589-597.\n\nComposition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S Al-Maadeed, N Rajpoot, M Shah, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed, N. Rajpoot, and M. Shah, \"Composition loss for counting, density map estimation and localization in dense crowds,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 532-546.\n\nGenerating high-quality crowd density maps using contextual pyramid CNNs. V A Sindagi, V M Patel, Proc. ICCV. ICCVV. A. Sindagi and V. M. Patel, \"Generating high-quality crowd density maps using contextual pyramid CNNs,\" in Proc. ICCV, Oct. 2017, pp. 1879-1888.\n\nDecideNet: Counting varying density crowds through attention guided detection and density estimation. J Liu, C Gao, D Meng, A G Hauptmann, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitJ. Liu, C. Gao, D. Meng, and A. G. Hauptmann, \"DecideNet: Counting varying density crowds through attention guided detection and density estimation,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 5197-5206.\n\nAttention to head locations for crowd counting. Y Zhang, C Zhou, F Chang, A C Kot, arXiv:1806.10287Y. Zhang, C. Zhou, F. Chang, and A. C. Kot, \"Attention to head locations for crowd counting,\" 2018, arXiv:1806.10287.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proc. CVPR. CVPRC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethink- ing the inception architecture for computer vision,\" in Proc. CVPR, Jun. 2016, pp. 2818-2826.\n\nScale aggregation network for accurate and efficient crowd counting. X Cao, Z Wang, Y Zhao, F Su, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)X. Cao, Z. Wang, Y. Zhao, and F. Su, \"Scale aggregation network for accurate and efficient crowd counting,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 734-750.\n\nCrowd counting and density estimation by trellis encoder-decoder networks. X Jiang, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)X. Jiang et al., \"Crowd counting and density estimation by trellis encoder-decoder networks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 6133-6142.\n\nDADNet: Dilated-attentiondeformable ConvNet for crowd counting. D Guo, K Li, Z.-J Zha, M Wang, Proc. ACM Int. Conf. Multimedia. ACM Int. Conf. MultimediaD. Guo, K. Li, Z.-J. Zha, and M. Wang, \"DADNet: Dilated-attention- deformable ConvNet for crowd counting,\" in Proc. ACM Int. Conf. Multimedia, Oct. 2019, pp. 1823-1832.\n\nBayesian loss for crowd count estimation with point supervision. Z Ma, X Wei, X Hong, Y Gong, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)Z. Ma, X. Wei, X. Hong, and Y. Gong, \"Bayesian loss for crowd count estimation with point supervision,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 6142-6151.\n\nLearning from synthetic data for crowd counting in the wild. Q Wang, J Gao, W Lin, Y Yuan, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Q. Wang, J. Gao, W. Lin, and Y. Yuan, \"Learning from synthetic data for crowd counting in the wild,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 8198-8207.\n\nSwitching convolutional neural network for crowd counting. D B Sam, S Surya, R V Babu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)D. B. Sam, S. Surya, and R. V. Babu, \"Switching convolutional neural network for crowd counting,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 4031-4039.\n\nImproving the learning of multi-column convolutional neural network for crowd counting. Z.-Q Cheng, J.-X Li, Q Dai, X Wu, J.-Y He, A G Hauptmann, 10.1145/3343031.3350898Proc. 27th ACM Int. Conf. Multimedia (MM). 27th ACM Int. Conf. Multimedia (MM)New York, NY, USAZ.-Q. Cheng, J.-X. Li, Q. Dai, X. Wu, J.-Y. He, and A. G. Hauptmann, \"Improving the learning of multi-column convolutional neural net- work for crowd counting,\" in Proc. 27th ACM Int. Conf. Mul- timedia (MM), New York, NY, USA, 2019, pp. 1897-1906, doi: 10.1145/3343031.3350898.\n\nCrowd counting with deep structured scale integration network. L Liu, Z Qiu, G Li, S Liu, W Ouyang, L Lin, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)L. Liu, Z. Qiu, G. Li, S. Liu, W. Ouyang, and L. Lin, \"Crowd counting with deep structured scale integration network,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 1774-1783.\n\nIterative crowd counting. V Ranjan, H Le, M Hoai, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)V. Ranjan, H. Le, and M. Hoai, \"Iterative crowd counting,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 270-285.\n\nHA-CCN: Hierarchical attention-based crowd counting network. V Sindagi, V Patel, IEEE Trans. Image Process. 29V. Sindagi and V. Patel, \"HA-CCN: Hierarchical attention-based crowd counting network,\" IEEE Trans. Image Process., vol. 29, pp. 323-335, 2019.\n\nTop-down feedback for crowd counting convolutional neural network. D B Sam, R V Babu, Proc. 32nd AAAI Conf. 32nd AAAI ConfD. B. Sam and R. V. Babu, \"Top-down feedback for crowd counting convolutional neural network,\" in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 1-8.\n\nMulti-level bottom-top and top-bottom feature fusion for crowd counting. V Sindagi, V Patel, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)V. Sindagi and V. Patel, \"Multi-level bottom-top and top-bottom feature fusion for crowd counting,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 1002-1012.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proc. CVPR. CVPRC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" in Proc. CVPR, 2015, pp. 1-9.\n\nADCrowd-Net: An attention-injective deformable convolutional network for crowd understanding. N Liu, Y Long, C Zou, Q Niu, L Pan, H Wu, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)N. Liu, Y. Long, C. Zou, Q. Niu, L. Pan, and H. Wu, \"ADCrowd- Net: An attention-injective deformable convolutional network for crowd understanding,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 3225-3234.\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)C. Zhang, H. Li, X. Wang, and X. Yang, \"Cross-scene crowd counting via deep convolutional neural networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 833-841.\n\nTowards perspective-free object counting with deep learning. D Onoro-Rubio, R J L\u00f3pez-Sastre, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisCham, SwitzerlandSpringerD. Onoro-Rubio and R. J. L\u00f3pez-Sastre, \"Towards perspective-free object counting with deep learning,\" in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2016, pp. 615-629.\n\nRevisiting perspective information for efficient crowd counting. M Shi, Z Yang, C Xu, Q Chen, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)M. Shi, Z. Yang, C. Xu, and Q. Chen, \"Revisiting perspective informa- tion for efficient crowd counting,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 7279-7288.\n\nPerspective-guided convolution networks for crowd counting. Z Yan, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)Z. Yan et al., \"Perspective-guided convolution networks for crowd count- ing,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 952-961.\n\nLeveraging heterogeneous auxiliary tasks to assist crowd counting. M Zhao, J Zhang, C Zhang, W Zhang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitM. Zhao, J. Zhang, C. Zhang, and W. Zhang, \"Leveraging heterogeneous auxiliary tasks to assist crowd counting,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2019, pp. 12736-12745.\n\nInverse attention guided deep crowd counting network. V A Sindagi, V M Patel, Proc. AVSS. AVSSV. A. Sindagi and V. M. Patel, \"Inverse attention guided deep crowd counting network,\" in Proc. AVSS, Sep. 2019, pp. 1-8.\n\nCounting with focus for free. Z Shi, P Mettes, C Snoek, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)Z. Shi, P. Mettes, and C. Snoek, \"Counting with focus for free,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 4200-4209.\n\nCurriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proc. 26th Annu. Int. Conf. Mach. Learn. 26th Annu. Int. Conf. Mach. LearnY. Bengio, J. Louradour, R. Collobert, and J. Weston, \"Curriculum learning,\" in Proc. 26th Annu. Int. Conf. Mach. Learn., 2009, pp. 41-48.\n\nLearning and development in neural networks: The importance of starting small. J L Elman, Cognition. 481J. L. Elman, \"Learning and development in neural networks: The importance of starting small,\" Cognition, vol. 48, no. 1, pp. 71-99, 1993.\n\nSelfpaced curriculum learning. L Jiang, D Meng, Q Zhao, S Shan, A G Hauptmann, Proc. 29th AAAI Conf. 29th AAAI ConfL. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann, \"Self- paced curriculum learning,\" in Proc. 29th AAAI Conf. Artif. Intell., 2015, pp. 1-7.\n\nSelf-paced learning for latent variable models. M P Kumar, B Packer, D Koller, Proc. nullM. P. Kumar, B. Packer, and D. Koller, \"Self-paced learning for latent variable models,\" in Proc. Adv. Neural Inf. Process. Syst., 2010, pp. 1189-1197.\n\nPoint in, box out: Beyond counting persons in crowds. Y Liu, M Shi, Q Zhao, X Wang, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Y. Liu, M. Shi, Q. Zhao, and X. Wang, \"Point in, box out: Beyond counting persons in crowds,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 6469-6478.\n\nMulti-source multiscale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitH. Idrees, I. Saleemi, C. Seibert, and M. Shah, \"Multi-source multi- scale counting in extremely dense crowd images,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2013, pp. 2547-2554.\n\nAutomatic differentiation in PyTorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, Proc. NIPS Workshop. NIPS WorkshopA. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, \"Automatic differentiation in PyTorch,\" in Proc. NIPS Workshop, 2017, pp. 1-4.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, Proc. ICLR. ICLRD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" in Proc. ICLR, 2015, pp. 1-15.\n\nRelational attention network for crowd counting. A Zhang, J Shen, Z Xiao, F Zhu, X Zhen, X Cao, L Shao, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)A. Zhang, J. Shen, Z. Xiao, F. Zhu, X. Zhen, X. Cao, and L. Shao, \"Relational attention network for crowd counting,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 6788-6797.\n\nCSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitY. Li, X. Zhang, and D. Chen, \"CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 1091-1100.\n\nContext-aware crowd counting. W Liu, M Salzmann, P Fua, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)W. Liu, M. Salzmann, and P. Fua, \"Context-aware crowd counting,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 5099-5108.\n\nAdaptive density map generation for crowd counting. J Wan, A Chan, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)J. Wan and A. Chan, \"Adaptive density map generation for crowd count- ing,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 1130-1139.\n\nAttentional neural fields for crowd counting. A Zhang, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)A. Zhang et al., \"Attentional neural fields for crowd counting,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 5714-5723.\n\nLearning spatial awareness to improve crowd counting. Z.-Q Cheng, J.-X Li, Q Dai, X Wu, A Hauptmann, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)Z.-Q. Cheng, J.-X. Li, Q. Dai, X. Wu, and A. Hauptmann, \"Learning spatial awareness to improve crowd counting,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 6152-6161.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" 2014, arXiv:1409.1556.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 770-778.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \"Densely connected convolutional networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 4700-4708.\n\nShuffleNet V2: Practical guidelines for efficient CNN architecture design. N Ma, X Zhang, H.-T Zheng, J Sun, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, \"ShuffleNet V2: Practical guidelines for efficient CNN architecture design,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 116-131.\n\nUnsupervised domain adaptation via structured prediction based selective pseudo-labeling. Q Wang, T P Breckon, Proc. AAAI Conf. AAAI ConfQ. Wang and T. P. Breckon, \"Unsupervised domain adaptation via structured prediction based selective pseudo-labeling,\" in Proc. AAAI Conf. Artif. Intell., 2020, pp. 6243-6250.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proc. Conf. Neural Inf. Process. Syst. Conf. Neural Inf. ess. SystA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Proc. Conf. Neural Inf. Process. Syst., 2017, pp. 5998-6008.\n", "annotations": {"author": "[{\"end\":117,\"start\":95},{\"end\":152,\"start\":118}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":112},{\"end\":151,\"start\":144}]", "author_first_name": "[{\"end\":111,\"start\":107},{\"end\":141,\"start\":137},{\"end\":143,\"start\":142}]", "author_affiliation": null, "title": "[{\"end\":78,\"start\":1},{\"end\":230,\"start\":153}]", "venue": "[{\"end\":287,\"start\":232}]", "abstract": "[{\"end\":2047,\"start\":427}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2245,\"start\":2242},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2250,\"start\":2247},{\"end\":2907,\"start\":2897},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3013,\"start\":3010},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3027,\"start\":3024},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3032,\"start\":3029},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3044,\"start\":3041},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3050,\"start\":3046},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3065,\"start\":3062},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3144,\"start\":3140},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3150,\"start\":3146},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3182,\"start\":3178},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3188,\"start\":3184},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3584,\"start\":3580},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3608,\"start\":3604},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3956,\"start\":3952},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4008,\"start\":4004},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4093,\"start\":4089},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4099,\"start\":4095},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4329,\"start\":4325},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4335,\"start\":4331},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4364,\"start\":4360},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4370,\"start\":4366},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4552,\"start\":4548},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4618,\"start\":4614},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4624,\"start\":4620},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4816,\"start\":4812},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5010,\"start\":5006},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5016,\"start\":5012},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7617,\"start\":7613},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7623,\"start\":7619},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7629,\"start\":7625},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7700,\"start\":7696},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7812,\"start\":7808},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8096,\"start\":8092},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8258,\"start\":8254},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8665,\"start\":8661},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8671,\"start\":8667},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8875,\"start\":8871},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8897,\"start\":8893},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8927,\"start\":8923},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9105,\"start\":9101},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9140,\"start\":9136},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9291,\"start\":9287},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9311,\"start\":9307},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9452,\"start\":9448},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9680,\"start\":9676},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9686,\"start\":9682},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9692,\"start\":9688},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9698,\"start\":9694},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10010,\"start\":10006},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10212,\"start\":10208},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10218,\"start\":10214},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10405,\"start\":10401},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10484,\"start\":10480},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10598,\"start\":10594},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10800,\"start\":10796},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10809,\"start\":10805},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11128,\"start\":11124},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11213,\"start\":11209},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11807,\"start\":11803},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11967,\"start\":11963},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12209,\"start\":12205},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13694,\"start\":13690},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15057,\"start\":15053},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15192,\"start\":15188},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16790,\"start\":16789},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18399,\"start\":18395},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19445,\"start\":19441},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19451,\"start\":19447},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19516,\"start\":19512},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19680,\"start\":19676},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19834,\"start\":19830},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21824,\"start\":21821},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24945,\"start\":24941},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25473,\"start\":25469},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25700,\"start\":25696},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25883,\"start\":25879},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26499,\"start\":26495},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26544,\"start\":26543},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26570,\"start\":26566},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27756,\"start\":27752},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":27762,\"start\":27758},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28283,\"start\":28279},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":28324,\"start\":28320},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28419,\"start\":28415},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28509,\"start\":28505},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":28628,\"start\":28624},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28699,\"start\":28695},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":28751,\"start\":28747},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28808,\"start\":28804},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28903,\"start\":28899},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35209,\"start\":35205},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35307,\"start\":35303},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35587,\"start\":35583},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":36902,\"start\":36898},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":36917,\"start\":36913},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":36933,\"start\":36929},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":36951,\"start\":36947},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":36979,\"start\":36975},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":43774,\"start\":43770},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43801,\"start\":43797},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":44103,\"start\":44099},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":44135,\"start\":44131},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":46534,\"start\":46532}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44486,\"start\":44177},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44561,\"start\":44487},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44796,\"start\":44562},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45409,\"start\":44797},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46973,\"start\":45410},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46981,\"start\":46974},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47148,\"start\":46982},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47243,\"start\":47149},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47319,\"start\":47244},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47433,\"start\":47320},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47531,\"start\":47434}]", "paragraph": "[{\"end\":3066,\"start\":2066},{\"end\":3803,\"start\":3068},{\"end\":5089,\"start\":3805},{\"end\":6004,\"start\":5091},{\"end\":6877,\"start\":6006},{\"end\":7219,\"start\":6898},{\"end\":7515,\"start\":7250},{\"end\":8551,\"start\":7517},{\"end\":8965,\"start\":8553},{\"end\":9555,\"start\":8967},{\"end\":10406,\"start\":9557},{\"end\":10967,\"start\":10408},{\"end\":11991,\"start\":10994},{\"end\":13193,\"start\":11993},{\"end\":13473,\"start\":13240},{\"end\":14420,\"start\":13495},{\"end\":14882,\"start\":14422},{\"end\":15279,\"start\":14919},{\"end\":15386,\"start\":15312},{\"end\":15878,\"start\":15447},{\"end\":16195,\"start\":15880},{\"end\":18297,\"start\":16224},{\"end\":19296,\"start\":18299},{\"end\":20500,\"start\":19320},{\"end\":20899,\"start\":20521},{\"end\":21126,\"start\":20955},{\"end\":21458,\"start\":21236},{\"end\":21617,\"start\":21492},{\"end\":22360,\"start\":21640},{\"end\":22959,\"start\":22406},{\"end\":23059,\"start\":22961},{\"end\":24186,\"start\":23079},{\"end\":24323,\"start\":24250},{\"end\":24860,\"start\":24343},{\"end\":25450,\"start\":24876},{\"end\":25676,\"start\":25452},{\"end\":25956,\"start\":25678},{\"end\":26219,\"start\":25982},{\"end\":26438,\"start\":26309},{\"end\":28168,\"start\":26462},{\"end\":28987,\"start\":28193},{\"end\":29331,\"start\":28989},{\"end\":30785,\"start\":29333},{\"end\":33101,\"start\":30819},{\"end\":34646,\"start\":33103},{\"end\":34892,\"start\":34648},{\"end\":36596,\"start\":34940},{\"end\":37568,\"start\":36643},{\"end\":39456,\"start\":37570},{\"end\":40800,\"start\":39506},{\"end\":41623,\"start\":40833},{\"end\":43270,\"start\":41625},{\"end\":44176,\"start\":43272}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13494,\"start\":13474},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15311,\"start\":15280},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15415,\"start\":15387},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15446,\"start\":15415},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19319,\"start\":19297},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20954,\"start\":20900},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21170,\"start\":21127},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21235,\"start\":21170},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21491,\"start\":21459},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22405,\"start\":22361},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23078,\"start\":23060},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24249,\"start\":24187},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26262,\"start\":26220},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26308,\"start\":26262}]", "table_ref": "[{\"end\":29035,\"start\":29028},{\"end\":29151,\"start\":29144},{\"end\":30925,\"start\":30916},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32085,\"start\":31945},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32779,\"start\":32771},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33791,\"start\":33782},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34224,\"start\":34215},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34700,\"start\":34688},{\"end\":34952,\"start\":34945},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35490,\"start\":35482},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37264,\"start\":37256},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37678,\"start\":37671},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37841,\"start\":37833},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39308,\"start\":39301},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39974,\"start\":39966},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":40535,\"start\":40528}]", "section_header": "[{\"end\":2064,\"start\":2049},{\"end\":6896,\"start\":6880},{\"end\":7248,\"start\":7222},{\"end\":10992,\"start\":10970},{\"end\":13238,\"start\":13196},{\"end\":14917,\"start\":14885},{\"end\":16222,\"start\":16198},{\"end\":20519,\"start\":20503},{\"end\":21638,\"start\":21620},{\"end\":24341,\"start\":24326},{\"end\":24874,\"start\":24863},{\"end\":25980,\"start\":25959},{\"end\":26460,\"start\":26441},{\"end\":28191,\"start\":28171},{\"end\":30817,\"start\":30788},{\"end\":34938,\"start\":34895},{\"end\":36641,\"start\":36599},{\"end\":39504,\"start\":39459},{\"end\":40831,\"start\":40803},{\"end\":44496,\"start\":44488},{\"end\":44571,\"start\":44563},{\"end\":46980,\"start\":46975},{\"end\":46995,\"start\":46983},{\"end\":47163,\"start\":47150},{\"end\":47261,\"start\":47245},{\"end\":47328,\"start\":47321},{\"end\":47451,\"start\":47435}]", "table": "[{\"end\":47148,\"start\":46998}]", "figure_caption": "[{\"end\":44486,\"start\":44179},{\"end\":44561,\"start\":44498},{\"end\":44796,\"start\":44573},{\"end\":45409,\"start\":44799},{\"end\":46973,\"start\":45412},{\"end\":47243,\"start\":47167},{\"end\":47319,\"start\":47264},{\"end\":47433,\"start\":47330},{\"end\":47531,\"start\":47454}]", "figure_ref": "[{\"end\":13563,\"start\":13555},{\"end\":17817,\"start\":17811},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23324,\"start\":23316},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35808,\"start\":35800}]", "bib_author_first_name": "[{\"end\":47662,\"start\":47661},{\"end\":47670,\"start\":47669},{\"end\":47679,\"start\":47678},{\"end\":47686,\"start\":47685},{\"end\":47694,\"start\":47693},{\"end\":47696,\"start\":47695},{\"end\":47971,\"start\":47970},{\"end\":47979,\"start\":47978},{\"end\":47981,\"start\":47980},{\"end\":47994,\"start\":47993},{\"end\":48007,\"start\":48006},{\"end\":48009,\"start\":48008},{\"end\":48024,\"start\":48020},{\"end\":48277,\"start\":48276},{\"end\":48285,\"start\":48284},{\"end\":48295,\"start\":48294},{\"end\":48308,\"start\":48307},{\"end\":48633,\"start\":48632},{\"end\":48644,\"start\":48643},{\"end\":48646,\"start\":48645},{\"end\":48924,\"start\":48923},{\"end\":48932,\"start\":48931},{\"end\":48938,\"start\":48937},{\"end\":48945,\"start\":48944},{\"end\":48953,\"start\":48952},{\"end\":48960,\"start\":48959},{\"end\":49289,\"start\":49288},{\"end\":49296,\"start\":49295},{\"end\":49298,\"start\":49297},{\"end\":49307,\"start\":49306},{\"end\":49656,\"start\":49655},{\"end\":49665,\"start\":49664},{\"end\":49677,\"start\":49673},{\"end\":49685,\"start\":49684},{\"end\":49693,\"start\":49692},{\"end\":50014,\"start\":50013},{\"end\":50028,\"start\":50027},{\"end\":50294,\"start\":50293},{\"end\":50296,\"start\":50295},{\"end\":50309,\"start\":50308},{\"end\":50322,\"start\":50321},{\"end\":50842,\"start\":50841},{\"end\":50850,\"start\":50849},{\"end\":51117,\"start\":51116},{\"end\":51125,\"start\":51124},{\"end\":51308,\"start\":51307},{\"end\":51316,\"start\":51315},{\"end\":51332,\"start\":51331},{\"end\":51342,\"start\":51341},{\"end\":51563,\"start\":51562},{\"end\":51565,\"start\":51564},{\"end\":51579,\"start\":51578},{\"end\":51591,\"start\":51590},{\"end\":51969,\"start\":51968},{\"end\":51977,\"start\":51976},{\"end\":51985,\"start\":51984},{\"end\":52249,\"start\":52248},{\"end\":52257,\"start\":52256},{\"end\":52259,\"start\":52258},{\"end\":52270,\"start\":52269},{\"end\":52282,\"start\":52281},{\"end\":52547,\"start\":52546},{\"end\":52560,\"start\":52559},{\"end\":52761,\"start\":52760},{\"end\":52775,\"start\":52774},{\"end\":52788,\"start\":52787},{\"end\":52790,\"start\":52789},{\"end\":53117,\"start\":53116},{\"end\":53126,\"start\":53125},{\"end\":53134,\"start\":53133},{\"end\":53142,\"start\":53141},{\"end\":53149,\"start\":53148},{\"end\":53555,\"start\":53554},{\"end\":53565,\"start\":53564},{\"end\":53575,\"start\":53574},{\"end\":53585,\"start\":53584},{\"end\":53594,\"start\":53593},{\"end\":53608,\"start\":53607},{\"end\":53619,\"start\":53618},{\"end\":54003,\"start\":54002},{\"end\":54005,\"start\":54004},{\"end\":54016,\"start\":54015},{\"end\":54018,\"start\":54017},{\"end\":54294,\"start\":54293},{\"end\":54301,\"start\":54300},{\"end\":54308,\"start\":54307},{\"end\":54316,\"start\":54315},{\"end\":54318,\"start\":54317},{\"end\":54708,\"start\":54707},{\"end\":54717,\"start\":54716},{\"end\":54725,\"start\":54724},{\"end\":54734,\"start\":54733},{\"end\":54736,\"start\":54735},{\"end\":54937,\"start\":54936},{\"end\":54948,\"start\":54947},{\"end\":54961,\"start\":54960},{\"end\":54970,\"start\":54969},{\"end\":54980,\"start\":54979},{\"end\":55240,\"start\":55239},{\"end\":55247,\"start\":55246},{\"end\":55255,\"start\":55254},{\"end\":55263,\"start\":55262},{\"end\":55581,\"start\":55580},{\"end\":55950,\"start\":55949},{\"end\":55957,\"start\":55956},{\"end\":55966,\"start\":55962},{\"end\":55973,\"start\":55972},{\"end\":56274,\"start\":56273},{\"end\":56280,\"start\":56279},{\"end\":56287,\"start\":56286},{\"end\":56295,\"start\":56294},{\"end\":56631,\"start\":56630},{\"end\":56639,\"start\":56638},{\"end\":56646,\"start\":56645},{\"end\":56653,\"start\":56652},{\"end\":57023,\"start\":57022},{\"end\":57025,\"start\":57024},{\"end\":57032,\"start\":57031},{\"end\":57041,\"start\":57040},{\"end\":57043,\"start\":57042},{\"end\":57430,\"start\":57426},{\"end\":57442,\"start\":57438},{\"end\":57448,\"start\":57447},{\"end\":57455,\"start\":57454},{\"end\":57464,\"start\":57460},{\"end\":57470,\"start\":57469},{\"end\":57472,\"start\":57471},{\"end\":57946,\"start\":57945},{\"end\":57953,\"start\":57952},{\"end\":57960,\"start\":57959},{\"end\":57966,\"start\":57965},{\"end\":57973,\"start\":57972},{\"end\":57983,\"start\":57982},{\"end\":58298,\"start\":58297},{\"end\":58308,\"start\":58307},{\"end\":58314,\"start\":58313},{\"end\":58571,\"start\":58570},{\"end\":58582,\"start\":58581},{\"end\":58832,\"start\":58831},{\"end\":58834,\"start\":58833},{\"end\":58841,\"start\":58840},{\"end\":58843,\"start\":58842},{\"end\":59112,\"start\":59111},{\"end\":59123,\"start\":59122},{\"end\":59427,\"start\":59426},{\"end\":59438,\"start\":59437},{\"end\":59445,\"start\":59444},{\"end\":59452,\"start\":59451},{\"end\":59464,\"start\":59463},{\"end\":59472,\"start\":59471},{\"end\":59484,\"start\":59483},{\"end\":59493,\"start\":59492},{\"end\":59506,\"start\":59505},{\"end\":59801,\"start\":59800},{\"end\":59808,\"start\":59807},{\"end\":59816,\"start\":59815},{\"end\":59823,\"start\":59822},{\"end\":59830,\"start\":59829},{\"end\":59837,\"start\":59836},{\"end\":60261,\"start\":60260},{\"end\":60270,\"start\":60269},{\"end\":60276,\"start\":60275},{\"end\":60284,\"start\":60283},{\"end\":60649,\"start\":60648},{\"end\":60664,\"start\":60663},{\"end\":60666,\"start\":60665},{\"end\":61008,\"start\":61007},{\"end\":61015,\"start\":61014},{\"end\":61023,\"start\":61022},{\"end\":61029,\"start\":61028},{\"end\":61405,\"start\":61404},{\"end\":61719,\"start\":61718},{\"end\":61727,\"start\":61726},{\"end\":61736,\"start\":61735},{\"end\":61745,\"start\":61744},{\"end\":62089,\"start\":62088},{\"end\":62091,\"start\":62090},{\"end\":62102,\"start\":62101},{\"end\":62104,\"start\":62103},{\"end\":62282,\"start\":62281},{\"end\":62289,\"start\":62288},{\"end\":62299,\"start\":62298},{\"end\":62557,\"start\":62556},{\"end\":62567,\"start\":62566},{\"end\":62580,\"start\":62579},{\"end\":62593,\"start\":62592},{\"end\":62896,\"start\":62895},{\"end\":62898,\"start\":62897},{\"end\":63091,\"start\":63090},{\"end\":63100,\"start\":63099},{\"end\":63108,\"start\":63107},{\"end\":63116,\"start\":63115},{\"end\":63124,\"start\":63123},{\"end\":63126,\"start\":63125},{\"end\":63373,\"start\":63372},{\"end\":63375,\"start\":63374},{\"end\":63384,\"start\":63383},{\"end\":63394,\"start\":63393},{\"end\":63621,\"start\":63620},{\"end\":63628,\"start\":63627},{\"end\":63635,\"start\":63634},{\"end\":63643,\"start\":63642},{\"end\":64013,\"start\":64012},{\"end\":64023,\"start\":64022},{\"end\":64034,\"start\":64033},{\"end\":64045,\"start\":64044},{\"end\":64376,\"start\":64375},{\"end\":64386,\"start\":64385},{\"end\":64395,\"start\":64394},{\"end\":64407,\"start\":64406},{\"end\":64417,\"start\":64416},{\"end\":64425,\"start\":64424},{\"end\":64435,\"start\":64434},{\"end\":64442,\"start\":64441},{\"end\":64455,\"start\":64454},{\"end\":64465,\"start\":64464},{\"end\":64744,\"start\":64743},{\"end\":64746,\"start\":64745},{\"end\":64756,\"start\":64755},{\"end\":64929,\"start\":64928},{\"end\":64938,\"start\":64937},{\"end\":64946,\"start\":64945},{\"end\":64954,\"start\":64953},{\"end\":64961,\"start\":64960},{\"end\":64969,\"start\":64968},{\"end\":64976,\"start\":64975},{\"end\":65357,\"start\":65356},{\"end\":65363,\"start\":65362},{\"end\":65372,\"start\":65371},{\"end\":65714,\"start\":65713},{\"end\":65721,\"start\":65720},{\"end\":65733,\"start\":65732},{\"end\":66059,\"start\":66058},{\"end\":66066,\"start\":66065},{\"end\":66359,\"start\":66358},{\"end\":66653,\"start\":66649},{\"end\":66665,\"start\":66661},{\"end\":66671,\"start\":66670},{\"end\":66678,\"start\":66677},{\"end\":66684,\"start\":66683},{\"end\":67040,\"start\":67039},{\"end\":67052,\"start\":67051},{\"end\":67250,\"start\":67249},{\"end\":67256,\"start\":67255},{\"end\":67265,\"start\":67264},{\"end\":67272,\"start\":67271},{\"end\":67594,\"start\":67593},{\"end\":67603,\"start\":67602},{\"end\":67610,\"start\":67609},{\"end\":67628,\"start\":67627},{\"end\":67630,\"start\":67629},{\"end\":68012,\"start\":68011},{\"end\":68018,\"start\":68017},{\"end\":68030,\"start\":68026},{\"end\":68039,\"start\":68038},{\"end\":68384,\"start\":68383},{\"end\":68392,\"start\":68391},{\"end\":68394,\"start\":68393},{\"end\":68635,\"start\":68634},{\"end\":68646,\"start\":68645},{\"end\":68657,\"start\":68656},{\"end\":68667,\"start\":68666},{\"end\":68680,\"start\":68679},{\"end\":68689,\"start\":68688},{\"end\":68691,\"start\":68690},{\"end\":68700,\"start\":68699},{\"end\":68710,\"start\":68709}]", "bib_author_last_name": "[{\"end\":47667,\"start\":47663},{\"end\":47676,\"start\":47671},{\"end\":47683,\"start\":47680},{\"end\":47691,\"start\":47687},{\"end\":47701,\"start\":47697},{\"end\":47976,\"start\":47972},{\"end\":47991,\"start\":47982},{\"end\":48004,\"start\":47995},{\"end\":48018,\"start\":48010},{\"end\":48027,\"start\":48025},{\"end\":48282,\"start\":48278},{\"end\":48292,\"start\":48286},{\"end\":48305,\"start\":48296},{\"end\":48315,\"start\":48309},{\"end\":48641,\"start\":48634},{\"end\":48652,\"start\":48647},{\"end\":48929,\"start\":48925},{\"end\":48935,\"start\":48933},{\"end\":48942,\"start\":48939},{\"end\":48950,\"start\":48946},{\"end\":48957,\"start\":48954},{\"end\":48966,\"start\":48961},{\"end\":49293,\"start\":49290},{\"end\":49304,\"start\":49299},{\"end\":49317,\"start\":49308},{\"end\":49662,\"start\":49657},{\"end\":49671,\"start\":49666},{\"end\":49682,\"start\":49678},{\"end\":49690,\"start\":49686},{\"end\":49700,\"start\":49694},{\"end\":50025,\"start\":50015},{\"end\":50036,\"start\":50029},{\"end\":50306,\"start\":50297},{\"end\":50319,\"start\":50310},{\"end\":50332,\"start\":50323},{\"end\":50847,\"start\":50843},{\"end\":50859,\"start\":50851},{\"end\":51122,\"start\":51118},{\"end\":51133,\"start\":51126},{\"end\":51313,\"start\":51309},{\"end\":51329,\"start\":51317},{\"end\":51339,\"start\":51333},{\"end\":51351,\"start\":51343},{\"end\":51576,\"start\":51566},{\"end\":51588,\"start\":51580},{\"end\":51602,\"start\":51592},{\"end\":51974,\"start\":51970},{\"end\":51982,\"start\":51978},{\"end\":51989,\"start\":51986},{\"end\":52254,\"start\":52250},{\"end\":52267,\"start\":52260},{\"end\":52279,\"start\":52271},{\"end\":52287,\"start\":52283},{\"end\":52557,\"start\":52548},{\"end\":52570,\"start\":52561},{\"end\":52772,\"start\":52762},{\"end\":52785,\"start\":52776},{\"end\":52797,\"start\":52791},{\"end\":53123,\"start\":53118},{\"end\":53131,\"start\":53127},{\"end\":53139,\"start\":53135},{\"end\":53146,\"start\":53143},{\"end\":53152,\"start\":53150},{\"end\":53562,\"start\":53556},{\"end\":53572,\"start\":53566},{\"end\":53582,\"start\":53576},{\"end\":53591,\"start\":53586},{\"end\":53605,\"start\":53595},{\"end\":53616,\"start\":53609},{\"end\":53624,\"start\":53620},{\"end\":54013,\"start\":54006},{\"end\":54024,\"start\":54019},{\"end\":54298,\"start\":54295},{\"end\":54305,\"start\":54302},{\"end\":54313,\"start\":54309},{\"end\":54328,\"start\":54319},{\"end\":54714,\"start\":54709},{\"end\":54722,\"start\":54718},{\"end\":54731,\"start\":54726},{\"end\":54740,\"start\":54737},{\"end\":54945,\"start\":54938},{\"end\":54958,\"start\":54949},{\"end\":54967,\"start\":54962},{\"end\":54977,\"start\":54971},{\"end\":54986,\"start\":54981},{\"end\":55244,\"start\":55241},{\"end\":55252,\"start\":55248},{\"end\":55260,\"start\":55256},{\"end\":55266,\"start\":55264},{\"end\":55587,\"start\":55582},{\"end\":55954,\"start\":55951},{\"end\":55960,\"start\":55958},{\"end\":55970,\"start\":55967},{\"end\":55978,\"start\":55974},{\"end\":56277,\"start\":56275},{\"end\":56284,\"start\":56281},{\"end\":56292,\"start\":56288},{\"end\":56300,\"start\":56296},{\"end\":56636,\"start\":56632},{\"end\":56643,\"start\":56640},{\"end\":56650,\"start\":56647},{\"end\":56658,\"start\":56654},{\"end\":57029,\"start\":57026},{\"end\":57038,\"start\":57033},{\"end\":57048,\"start\":57044},{\"end\":57436,\"start\":57431},{\"end\":57445,\"start\":57443},{\"end\":57452,\"start\":57449},{\"end\":57458,\"start\":57456},{\"end\":57467,\"start\":57465},{\"end\":57482,\"start\":57473},{\"end\":57950,\"start\":57947},{\"end\":57957,\"start\":57954},{\"end\":57963,\"start\":57961},{\"end\":57970,\"start\":57967},{\"end\":57980,\"start\":57974},{\"end\":57987,\"start\":57984},{\"end\":58305,\"start\":58299},{\"end\":58311,\"start\":58309},{\"end\":58319,\"start\":58315},{\"end\":58579,\"start\":58572},{\"end\":58588,\"start\":58583},{\"end\":58838,\"start\":58835},{\"end\":58848,\"start\":58844},{\"end\":59120,\"start\":59113},{\"end\":59129,\"start\":59124},{\"end\":59435,\"start\":59428},{\"end\":59442,\"start\":59439},{\"end\":59449,\"start\":59446},{\"end\":59461,\"start\":59453},{\"end\":59469,\"start\":59465},{\"end\":59481,\"start\":59473},{\"end\":59490,\"start\":59485},{\"end\":59503,\"start\":59494},{\"end\":59517,\"start\":59507},{\"end\":59805,\"start\":59802},{\"end\":59813,\"start\":59809},{\"end\":59820,\"start\":59817},{\"end\":59827,\"start\":59824},{\"end\":59834,\"start\":59831},{\"end\":59840,\"start\":59838},{\"end\":60267,\"start\":60262},{\"end\":60273,\"start\":60271},{\"end\":60281,\"start\":60277},{\"end\":60289,\"start\":60285},{\"end\":60661,\"start\":60650},{\"end\":60679,\"start\":60667},{\"end\":61012,\"start\":61009},{\"end\":61020,\"start\":61016},{\"end\":61026,\"start\":61024},{\"end\":61034,\"start\":61030},{\"end\":61409,\"start\":61406},{\"end\":61724,\"start\":61720},{\"end\":61733,\"start\":61728},{\"end\":61742,\"start\":61737},{\"end\":61751,\"start\":61746},{\"end\":62099,\"start\":62092},{\"end\":62110,\"start\":62105},{\"end\":62286,\"start\":62283},{\"end\":62296,\"start\":62290},{\"end\":62305,\"start\":62300},{\"end\":62564,\"start\":62558},{\"end\":62577,\"start\":62568},{\"end\":62590,\"start\":62581},{\"end\":62600,\"start\":62594},{\"end\":62904,\"start\":62899},{\"end\":63097,\"start\":63092},{\"end\":63105,\"start\":63101},{\"end\":63113,\"start\":63109},{\"end\":63121,\"start\":63117},{\"end\":63136,\"start\":63127},{\"end\":63381,\"start\":63376},{\"end\":63391,\"start\":63385},{\"end\":63401,\"start\":63395},{\"end\":63625,\"start\":63622},{\"end\":63632,\"start\":63629},{\"end\":63640,\"start\":63636},{\"end\":63648,\"start\":63644},{\"end\":64020,\"start\":64014},{\"end\":64031,\"start\":64024},{\"end\":64042,\"start\":64035},{\"end\":64050,\"start\":64046},{\"end\":64383,\"start\":64377},{\"end\":64392,\"start\":64387},{\"end\":64404,\"start\":64396},{\"end\":64414,\"start\":64408},{\"end\":64422,\"start\":64418},{\"end\":64432,\"start\":64426},{\"end\":64439,\"start\":64436},{\"end\":64452,\"start\":64443},{\"end\":64462,\"start\":64456},{\"end\":64471,\"start\":64466},{\"end\":64753,\"start\":64747},{\"end\":64759,\"start\":64757},{\"end\":64935,\"start\":64930},{\"end\":64943,\"start\":64939},{\"end\":64951,\"start\":64947},{\"end\":64958,\"start\":64955},{\"end\":64966,\"start\":64962},{\"end\":64973,\"start\":64970},{\"end\":64981,\"start\":64977},{\"end\":65360,\"start\":65358},{\"end\":65369,\"start\":65364},{\"end\":65377,\"start\":65373},{\"end\":65718,\"start\":65715},{\"end\":65730,\"start\":65722},{\"end\":65737,\"start\":65734},{\"end\":66063,\"start\":66060},{\"end\":66071,\"start\":66067},{\"end\":66365,\"start\":66360},{\"end\":66659,\"start\":66654},{\"end\":66668,\"start\":66666},{\"end\":66675,\"start\":66672},{\"end\":66681,\"start\":66679},{\"end\":66694,\"start\":66685},{\"end\":67049,\"start\":67041},{\"end\":67062,\"start\":67053},{\"end\":67253,\"start\":67251},{\"end\":67262,\"start\":67257},{\"end\":67269,\"start\":67266},{\"end\":67276,\"start\":67273},{\"end\":67600,\"start\":67595},{\"end\":67607,\"start\":67604},{\"end\":67625,\"start\":67611},{\"end\":67641,\"start\":67631},{\"end\":68015,\"start\":68013},{\"end\":68024,\"start\":68019},{\"end\":68036,\"start\":68031},{\"end\":68043,\"start\":68040},{\"end\":68389,\"start\":68385},{\"end\":68402,\"start\":68395},{\"end\":68643,\"start\":68636},{\"end\":68654,\"start\":68647},{\"end\":68664,\"start\":68658},{\"end\":68677,\"start\":68668},{\"end\":68686,\"start\":68681},{\"end\":68697,\"start\":68692},{\"end\":68707,\"start\":68701},{\"end\":68721,\"start\":68711}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":88505028},\"end\":47942,\"start\":47588},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1417739},\"end\":48201,\"start\":47944},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15166260},\"end\":48537,\"start\":48203},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13709403},\"end\":48858,\"start\":48539},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":216355164},\"end\":49201,\"start\":48860},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":19076929},\"end\":49581,\"start\":49203},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":17996070},\"end\":49945,\"start\":49583},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":21238031},\"end\":50247,\"start\":49947},{\"attributes\":{\"doi\":\"10.5244/C.29.CVPPP.1\",\"id\":\"b8\",\"matched_paper_id\":52486932},\"end\":50771,\"start\":50249},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4734215},\"end\":51063,\"start\":50773},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3489431},\"end\":51259,\"start\":51065},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14030224},\"end\":51500,\"start\":51261},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15508488},\"end\":51915,\"start\":51502},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17796587},\"end\":52134,\"start\":51917},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":11317366},\"end\":52507,\"start\":52136},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":18018217},\"end\":52693,\"start\":52509},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195908774},\"end\":53039,\"start\":52695},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4545310},\"end\":53464,\"start\":53041},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":51901514},\"end\":53926,\"start\":53466},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2099022},\"end\":54189,\"start\":53928},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3740753},\"end\":54657,\"start\":54191},{\"attributes\":{\"doi\":\"arXiv:1806.10287\",\"id\":\"b21\"},\"end\":54875,\"start\":54659},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206593880},\"end\":55168,\"start\":54877},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52955811},\"end\":55503,\"start\":55170},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":67856061},\"end\":55883,\"start\":55505},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":204837048},\"end\":56206,\"start\":55885},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":199543622},\"end\":56567,\"start\":56208},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":72941021},\"end\":56961,\"start\":56569},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1089358},\"end\":57336,\"start\":56963},{\"attributes\":{\"doi\":\"10.1145/3343031.3350898\",\"id\":\"b29\",\"matched_paper_id\":202583780},\"end\":57880,\"start\":57338},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":201645306},\"end\":58269,\"start\":57882},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":50785934},\"end\":58507,\"start\":58271},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":198133773},\"end\":58762,\"start\":58509},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":19200040},\"end\":59036,\"start\":58764},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":201668945},\"end\":59392,\"start\":59038},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206592484},\"end\":59704,\"start\":59394},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":53957027},\"end\":60191,\"start\":59706},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2131202},\"end\":60585,\"start\":60193},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":40499053},\"end\":60940,\"start\":60587},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":54461738},\"end\":61342,\"start\":60942},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":202577232},\"end\":61649,\"start\":61344},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":198161572},\"end\":62032,\"start\":61651},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":195776150},\"end\":62249,\"start\":62034},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":88516263},\"end\":62533,\"start\":62251},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":873046},\"end\":62814,\"start\":62535},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":2105042},\"end\":63057,\"start\":62816},{\"attributes\":{\"id\":\"b46\"},\"end\":63322,\"start\":63059},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1977996},\"end\":63564,\"start\":63324},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":91184269},\"end\":63944,\"start\":63566},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":9749221},\"end\":64335,\"start\":63946},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":40027675},\"end\":64697,\"start\":64337},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":6628106},\"end\":64877,\"start\":64699},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":207901201},\"end\":65261,\"start\":64879},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3645757},\"end\":65681,\"start\":65263},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":53783843},\"end\":66004,\"start\":65683},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":204958589},\"end\":66310,\"start\":66006},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":207926635},\"end\":66593,\"start\":66312},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":202577235},\"end\":66969,\"start\":66595},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b58\"},\"end\":67201,\"start\":66971},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":206594692},\"end\":67549,\"start\":67203},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":9433631},\"end\":67934,\"start\":67551},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":51880435},\"end\":68291,\"start\":67936},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":208158071},\"end\":68605,\"start\":68293},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":13756489},\"end\":68983,\"start\":68607}]", "bib_title": "[{\"end\":47659,\"start\":47588},{\"end\":47968,\"start\":47944},{\"end\":48274,\"start\":48203},{\"end\":48630,\"start\":48539},{\"end\":48921,\"start\":48860},{\"end\":49286,\"start\":49203},{\"end\":49653,\"start\":49583},{\"end\":50011,\"start\":49947},{\"end\":50291,\"start\":50249},{\"end\":50839,\"start\":50773},{\"end\":51114,\"start\":51065},{\"end\":51305,\"start\":51261},{\"end\":51560,\"start\":51502},{\"end\":51966,\"start\":51917},{\"end\":52246,\"start\":52136},{\"end\":52544,\"start\":52509},{\"end\":52758,\"start\":52695},{\"end\":53114,\"start\":53041},{\"end\":53552,\"start\":53466},{\"end\":54000,\"start\":53928},{\"end\":54291,\"start\":54191},{\"end\":54934,\"start\":54877},{\"end\":55237,\"start\":55170},{\"end\":55578,\"start\":55505},{\"end\":55947,\"start\":55885},{\"end\":56271,\"start\":56208},{\"end\":56628,\"start\":56569},{\"end\":57020,\"start\":56963},{\"end\":57424,\"start\":57338},{\"end\":57943,\"start\":57882},{\"end\":58295,\"start\":58271},{\"end\":58568,\"start\":58509},{\"end\":58829,\"start\":58764},{\"end\":59109,\"start\":59038},{\"end\":59424,\"start\":59394},{\"end\":59798,\"start\":59706},{\"end\":60258,\"start\":60193},{\"end\":60646,\"start\":60587},{\"end\":61005,\"start\":60942},{\"end\":61402,\"start\":61344},{\"end\":61716,\"start\":61651},{\"end\":62086,\"start\":62034},{\"end\":62279,\"start\":62251},{\"end\":62554,\"start\":62535},{\"end\":62893,\"start\":62816},{\"end\":63088,\"start\":63059},{\"end\":63370,\"start\":63324},{\"end\":63618,\"start\":63566},{\"end\":64010,\"start\":63946},{\"end\":64373,\"start\":64337},{\"end\":64741,\"start\":64699},{\"end\":64926,\"start\":64879},{\"end\":65354,\"start\":65263},{\"end\":65711,\"start\":65683},{\"end\":66056,\"start\":66006},{\"end\":66356,\"start\":66312},{\"end\":66647,\"start\":66595},{\"end\":67247,\"start\":67203},{\"end\":67591,\"start\":67551},{\"end\":68009,\"start\":67936},{\"end\":68381,\"start\":68293},{\"end\":68632,\"start\":68607}]", "bib_author": "[{\"end\":47669,\"start\":47661},{\"end\":47678,\"start\":47669},{\"end\":47685,\"start\":47678},{\"end\":47693,\"start\":47685},{\"end\":47703,\"start\":47693},{\"end\":47978,\"start\":47970},{\"end\":47993,\"start\":47978},{\"end\":48006,\"start\":47993},{\"end\":48020,\"start\":48006},{\"end\":48029,\"start\":48020},{\"end\":48284,\"start\":48276},{\"end\":48294,\"start\":48284},{\"end\":48307,\"start\":48294},{\"end\":48317,\"start\":48307},{\"end\":48643,\"start\":48632},{\"end\":48654,\"start\":48643},{\"end\":48931,\"start\":48923},{\"end\":48937,\"start\":48931},{\"end\":48944,\"start\":48937},{\"end\":48952,\"start\":48944},{\"end\":48959,\"start\":48952},{\"end\":48968,\"start\":48959},{\"end\":49295,\"start\":49288},{\"end\":49306,\"start\":49295},{\"end\":49319,\"start\":49306},{\"end\":49664,\"start\":49655},{\"end\":49673,\"start\":49664},{\"end\":49684,\"start\":49673},{\"end\":49692,\"start\":49684},{\"end\":49702,\"start\":49692},{\"end\":50027,\"start\":50013},{\"end\":50038,\"start\":50027},{\"end\":50308,\"start\":50293},{\"end\":50321,\"start\":50308},{\"end\":50334,\"start\":50321},{\"end\":50849,\"start\":50841},{\"end\":50861,\"start\":50849},{\"end\":51124,\"start\":51116},{\"end\":51135,\"start\":51124},{\"end\":51315,\"start\":51307},{\"end\":51331,\"start\":51315},{\"end\":51341,\"start\":51331},{\"end\":51353,\"start\":51341},{\"end\":51578,\"start\":51562},{\"end\":51590,\"start\":51578},{\"end\":51604,\"start\":51590},{\"end\":51976,\"start\":51968},{\"end\":51984,\"start\":51976},{\"end\":51991,\"start\":51984},{\"end\":52256,\"start\":52248},{\"end\":52269,\"start\":52256},{\"end\":52281,\"start\":52269},{\"end\":52289,\"start\":52281},{\"end\":52559,\"start\":52546},{\"end\":52572,\"start\":52559},{\"end\":52774,\"start\":52760},{\"end\":52787,\"start\":52774},{\"end\":52799,\"start\":52787},{\"end\":53125,\"start\":53116},{\"end\":53133,\"start\":53125},{\"end\":53141,\"start\":53133},{\"end\":53148,\"start\":53141},{\"end\":53154,\"start\":53148},{\"end\":53564,\"start\":53554},{\"end\":53574,\"start\":53564},{\"end\":53584,\"start\":53574},{\"end\":53593,\"start\":53584},{\"end\":53607,\"start\":53593},{\"end\":53618,\"start\":53607},{\"end\":53626,\"start\":53618},{\"end\":54015,\"start\":54002},{\"end\":54026,\"start\":54015},{\"end\":54300,\"start\":54293},{\"end\":54307,\"start\":54300},{\"end\":54315,\"start\":54307},{\"end\":54330,\"start\":54315},{\"end\":54716,\"start\":54707},{\"end\":54724,\"start\":54716},{\"end\":54733,\"start\":54724},{\"end\":54742,\"start\":54733},{\"end\":54947,\"start\":54936},{\"end\":54960,\"start\":54947},{\"end\":54969,\"start\":54960},{\"end\":54979,\"start\":54969},{\"end\":54988,\"start\":54979},{\"end\":55246,\"start\":55239},{\"end\":55254,\"start\":55246},{\"end\":55262,\"start\":55254},{\"end\":55268,\"start\":55262},{\"end\":55589,\"start\":55580},{\"end\":55956,\"start\":55949},{\"end\":55962,\"start\":55956},{\"end\":55972,\"start\":55962},{\"end\":55980,\"start\":55972},{\"end\":56279,\"start\":56273},{\"end\":56286,\"start\":56279},{\"end\":56294,\"start\":56286},{\"end\":56302,\"start\":56294},{\"end\":56638,\"start\":56630},{\"end\":56645,\"start\":56638},{\"end\":56652,\"start\":56645},{\"end\":56660,\"start\":56652},{\"end\":57031,\"start\":57022},{\"end\":57040,\"start\":57031},{\"end\":57050,\"start\":57040},{\"end\":57438,\"start\":57426},{\"end\":57447,\"start\":57438},{\"end\":57454,\"start\":57447},{\"end\":57460,\"start\":57454},{\"end\":57469,\"start\":57460},{\"end\":57484,\"start\":57469},{\"end\":57952,\"start\":57945},{\"end\":57959,\"start\":57952},{\"end\":57965,\"start\":57959},{\"end\":57972,\"start\":57965},{\"end\":57982,\"start\":57972},{\"end\":57989,\"start\":57982},{\"end\":58307,\"start\":58297},{\"end\":58313,\"start\":58307},{\"end\":58321,\"start\":58313},{\"end\":58581,\"start\":58570},{\"end\":58590,\"start\":58581},{\"end\":58840,\"start\":58831},{\"end\":58850,\"start\":58840},{\"end\":59122,\"start\":59111},{\"end\":59131,\"start\":59122},{\"end\":59437,\"start\":59426},{\"end\":59444,\"start\":59437},{\"end\":59451,\"start\":59444},{\"end\":59463,\"start\":59451},{\"end\":59471,\"start\":59463},{\"end\":59483,\"start\":59471},{\"end\":59492,\"start\":59483},{\"end\":59505,\"start\":59492},{\"end\":59519,\"start\":59505},{\"end\":59807,\"start\":59800},{\"end\":59815,\"start\":59807},{\"end\":59822,\"start\":59815},{\"end\":59829,\"start\":59822},{\"end\":59836,\"start\":59829},{\"end\":59842,\"start\":59836},{\"end\":60269,\"start\":60260},{\"end\":60275,\"start\":60269},{\"end\":60283,\"start\":60275},{\"end\":60291,\"start\":60283},{\"end\":60663,\"start\":60648},{\"end\":60681,\"start\":60663},{\"end\":61014,\"start\":61007},{\"end\":61022,\"start\":61014},{\"end\":61028,\"start\":61022},{\"end\":61036,\"start\":61028},{\"end\":61411,\"start\":61404},{\"end\":61726,\"start\":61718},{\"end\":61735,\"start\":61726},{\"end\":61744,\"start\":61735},{\"end\":61753,\"start\":61744},{\"end\":62101,\"start\":62088},{\"end\":62112,\"start\":62101},{\"end\":62288,\"start\":62281},{\"end\":62298,\"start\":62288},{\"end\":62307,\"start\":62298},{\"end\":62566,\"start\":62556},{\"end\":62579,\"start\":62566},{\"end\":62592,\"start\":62579},{\"end\":62602,\"start\":62592},{\"end\":62906,\"start\":62895},{\"end\":63099,\"start\":63090},{\"end\":63107,\"start\":63099},{\"end\":63115,\"start\":63107},{\"end\":63123,\"start\":63115},{\"end\":63138,\"start\":63123},{\"end\":63383,\"start\":63372},{\"end\":63393,\"start\":63383},{\"end\":63403,\"start\":63393},{\"end\":63627,\"start\":63620},{\"end\":63634,\"start\":63627},{\"end\":63642,\"start\":63634},{\"end\":63650,\"start\":63642},{\"end\":64022,\"start\":64012},{\"end\":64033,\"start\":64022},{\"end\":64044,\"start\":64033},{\"end\":64052,\"start\":64044},{\"end\":64385,\"start\":64375},{\"end\":64394,\"start\":64385},{\"end\":64406,\"start\":64394},{\"end\":64416,\"start\":64406},{\"end\":64424,\"start\":64416},{\"end\":64434,\"start\":64424},{\"end\":64441,\"start\":64434},{\"end\":64454,\"start\":64441},{\"end\":64464,\"start\":64454},{\"end\":64473,\"start\":64464},{\"end\":64755,\"start\":64743},{\"end\":64761,\"start\":64755},{\"end\":64937,\"start\":64928},{\"end\":64945,\"start\":64937},{\"end\":64953,\"start\":64945},{\"end\":64960,\"start\":64953},{\"end\":64968,\"start\":64960},{\"end\":64975,\"start\":64968},{\"end\":64983,\"start\":64975},{\"end\":65362,\"start\":65356},{\"end\":65371,\"start\":65362},{\"end\":65379,\"start\":65371},{\"end\":65720,\"start\":65713},{\"end\":65732,\"start\":65720},{\"end\":65739,\"start\":65732},{\"end\":66065,\"start\":66058},{\"end\":66073,\"start\":66065},{\"end\":66367,\"start\":66358},{\"end\":66661,\"start\":66649},{\"end\":66670,\"start\":66661},{\"end\":66677,\"start\":66670},{\"end\":66683,\"start\":66677},{\"end\":66696,\"start\":66683},{\"end\":67051,\"start\":67039},{\"end\":67064,\"start\":67051},{\"end\":67255,\"start\":67249},{\"end\":67264,\"start\":67255},{\"end\":67271,\"start\":67264},{\"end\":67278,\"start\":67271},{\"end\":67602,\"start\":67593},{\"end\":67609,\"start\":67602},{\"end\":67627,\"start\":67609},{\"end\":67643,\"start\":67627},{\"end\":68017,\"start\":68011},{\"end\":68026,\"start\":68017},{\"end\":68038,\"start\":68026},{\"end\":68045,\"start\":68038},{\"end\":68391,\"start\":68383},{\"end\":68404,\"start\":68391},{\"end\":68645,\"start\":68634},{\"end\":68656,\"start\":68645},{\"end\":68666,\"start\":68656},{\"end\":68679,\"start\":68666},{\"end\":68688,\"start\":68679},{\"end\":68699,\"start\":68688},{\"end\":68709,\"start\":68699},{\"end\":68723,\"start\":68709}]", "bib_venue": "[{\"end\":50491,\"start\":50452},{\"end\":50908,\"start\":50882},{\"end\":51151,\"start\":51147},{\"end\":51369,\"start\":51365},{\"end\":51716,\"start\":51664},{\"end\":52007,\"start\":52003},{\"end\":52588,\"start\":52584},{\"end\":52863,\"start\":52837},{\"end\":53258,\"start\":53210},{\"end\":53694,\"start\":53664},{\"end\":54042,\"start\":54038},{\"end\":54426,\"start\":54382},{\"end\":55004,\"start\":55000},{\"end\":55336,\"start\":55306},{\"end\":55701,\"start\":55649},{\"end\":56038,\"start\":56013},{\"end\":56388,\"start\":56349},{\"end\":56772,\"start\":56720},{\"end\":57154,\"start\":57106},{\"end\":57602,\"start\":57550},{\"end\":58075,\"start\":58036},{\"end\":58389,\"start\":58359},{\"end\":58886,\"start\":58872},{\"end\":59217,\"start\":59178},{\"end\":59535,\"start\":59531},{\"end\":59954,\"start\":59902},{\"end\":60395,\"start\":60347},{\"end\":60750,\"start\":60711},{\"end\":61148,\"start\":61096},{\"end\":61497,\"start\":61458},{\"end\":61841,\"start\":61801},{\"end\":62128,\"start\":62124},{\"end\":62393,\"start\":62354},{\"end\":62676,\"start\":62643},{\"end\":63174,\"start\":63160},{\"end\":63413,\"start\":63409},{\"end\":63762,\"start\":63710},{\"end\":64140,\"start\":64100},{\"end\":64507,\"start\":64494},{\"end\":64777,\"start\":64773},{\"end\":65069,\"start\":65030},{\"end\":65475,\"start\":65431},{\"end\":65851,\"start\":65799},{\"end\":66159,\"start\":66120},{\"end\":66453,\"start\":66414},{\"end\":66782,\"start\":66743},{\"end\":67382,\"start\":67334},{\"end\":67747,\"start\":67699},{\"end\":68113,\"start\":68083},{\"end\":68430,\"start\":68421},{\"end\":68789,\"start\":68762},{\"end\":47735,\"start\":47703},{\"end\":48044,\"start\":48029},{\"end\":48346,\"start\":48317},{\"end\":48676,\"start\":48654},{\"end\":49000,\"start\":48968},{\"end\":49367,\"start\":49319},{\"end\":49734,\"start\":49702},{\"end\":50069,\"start\":50038},{\"end\":50399,\"start\":50354},{\"end\":50880,\"start\":50861},{\"end\":51145,\"start\":51135},{\"end\":51363,\"start\":51353},{\"end\":51662,\"start\":51604},{\"end\":52001,\"start\":51991},{\"end\":52302,\"start\":52289},{\"end\":52582,\"start\":52572},{\"end\":52835,\"start\":52799},{\"end\":53208,\"start\":53154},{\"end\":53662,\"start\":53626},{\"end\":54036,\"start\":54026},{\"end\":54380,\"start\":54330},{\"end\":54705,\"start\":54659},{\"end\":54998,\"start\":54988},{\"end\":55304,\"start\":55268},{\"end\":55647,\"start\":55589},{\"end\":56011,\"start\":55980},{\"end\":56347,\"start\":56302},{\"end\":56718,\"start\":56660},{\"end\":57104,\"start\":57050},{\"end\":57548,\"start\":57507},{\"end\":58034,\"start\":57989},{\"end\":58357,\"start\":58321},{\"end\":58615,\"start\":58590},{\"end\":58870,\"start\":58850},{\"end\":59176,\"start\":59131},{\"end\":59529,\"start\":59519},{\"end\":59900,\"start\":59842},{\"end\":60345,\"start\":60291},{\"end\":60709,\"start\":60681},{\"end\":61094,\"start\":61036},{\"end\":61456,\"start\":61411},{\"end\":61799,\"start\":61753},{\"end\":62122,\"start\":62112},{\"end\":62352,\"start\":62307},{\"end\":62641,\"start\":62602},{\"end\":62915,\"start\":62906},{\"end\":63158,\"start\":63138},{\"end\":63407,\"start\":63403},{\"end\":63708,\"start\":63650},{\"end\":64098,\"start\":64052},{\"end\":64492,\"start\":64473},{\"end\":64771,\"start\":64761},{\"end\":65028,\"start\":64983},{\"end\":65429,\"start\":65379},{\"end\":65797,\"start\":65739},{\"end\":66118,\"start\":66073},{\"end\":66412,\"start\":66367},{\"end\":66741,\"start\":66696},{\"end\":67037,\"start\":66971},{\"end\":67332,\"start\":67278},{\"end\":67697,\"start\":67643},{\"end\":68081,\"start\":68045},{\"end\":68419,\"start\":68404},{\"end\":68760,\"start\":68723}]"}}}, "year": 2023, "month": 12, "day": 17}