{"id": 252693405, "updated": "2023-12-06 11:35:19.212", "metadata": {"title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization", "authors": "[{\"first\":\"Rajkumar\",\"last\":\"Ramamurthy\",\"middle\":[]},{\"first\":\"Prithviraj\",\"last\":\"Ammanabrolu\",\"middle\":[]},{\"first\":\"Kiant'e\",\"last\":\"Brantley\",\"middle\":[]},{\"first\":\"Jack\",\"last\":\"Hessel\",\"middle\":[]},{\"first\":\"Rafet\",\"last\":\"Sifa\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Bauckhage\",\"middle\":[]},{\"first\":\"Hannaneh\",\"last\":\"Hajishirzi\",\"middle\":[]},{\"first\":\"Yejin\",\"last\":\"Choi\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2210-01241", "doi": "10.48550/arxiv.2210.01241"}}, "content": {"source": {"pdf_hash": "912a39c2e0e4a35747531669cfa952d2c5627729", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.01241v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d320e87ea9bf02dc4ae90403ec00551e7af0b491", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/912a39c2e0e4a35747531669cfa952d2c5627729.txt", "contents": "\nIS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION\n\n\nRajkumar Ramamurthy rajkumar.ramamurthy@iais.fraunhofer.de \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\n\u2660 Prithviraj \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\nAmmanabrolu \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\nKiant\u00e9 Brantley \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\n\u2663 Jack jackh@allenai.org \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\nHessel \u2665 Rafet \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\nSifa \u2660 Christian Bauckhage \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\nHannaneh Hajishirzi \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\n\u2666 \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\n\u2666\u2665 \u2660 Fraunhofer IAISYejin Choi \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\nPaul G Allen \nAllen Institute for Artificial Intelligence\nSchool of Computer Science\nCornell University\nUniversity of Washington\n\n\nIS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION\nPublished as a conference paper at ICLR 2023\nWe tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of opensource libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs 1 , 2 for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al., 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al., 2017)), based on both automatic and human evaluations.\n\nINTRODUCTION\n\nThe ultimate aim of language technology is to interact with humans. However, most language models are trained without direct signals of human preference, with supervised target strings serving as (a sometimes crude) proxy. One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient. Automated metrics offer a promising compromise: models of human preference like pairwise learned preference models (Ouyang et al., 2022), BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But -these functions are usually not per-token differentiable: like humans, metrics Here, the LM (i.e., the policy) needs to produce a positive sentiment continuation given a review prompt (we cover other models of human preference in Sec. 3.2). Two objectives are balanced: 1) an automated proxy of human preference that serves as a reward (here: a sentiment classifier); and 2) \"naturalness\" as measured by a KL divergence from an LM not trained with explicit human feedback. The plots show validation learning curves comparing our NLPO to the popular policy gradient method PPO. (Top plot:) RL methods can easily achieve high reward if the KL penalty is removed, (Bottom:) but at the cost of higher perplexity. NLPO+KL, our proposed approach, succeeds in balancing reward and naturalness more effectively than prior work.\n\ncan only offer quality estimates for full generations. Reinforcement Learning (RL) offers a natural path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it is cast as a sequential decision-making problem. However, Goodhart's Law 3 looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu et al., 2021a;Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack of open-source benchmarks and algorithmic implementations-resulting in perception that RL is a challenging paradigm for NLP (Choshen et al., 2020;Kreutzer et al., 2021).\n\nTo facilitate research in building RL algorithms to better align LMs, we release a library, a benchmark, and an algorithm. First, we release the RL4LMs library, which enables generative HuggingFace models (e.g., GPT-2 or T5) to be trained using a variety of existing RL methods like PPO/A2C/etc. Next, we apply models trained using RL4LMs to the new GRUE (General Reinforced-language Understanding Evaluation) benchmark: GRUE is a collection of 7 contemporary NLP tasks (see Table1 for details); in contrast to other benchmarks, instead of supervised training, we pair each task with reward function(s). GRUE challenges models to optimize these reward functions while remaining fluent language generators. We train language models via RL-both with and without task specific supervised pre-training-to optimize rewards. Finally, beyond existing RL methods, we introduce a novel on-policy RL algorithm called NLPO (Natural Language Policy Optimization), that dynamically learns task-specific constraints over the distribution of language at a token level.\n\nExperiments on GRUE and human evaluations show that NLPO better balances learning preference rewards while maintaining language fluency compared to alternatives, including PPO ( Figure 1). We find that using RL to learn from scalar reward feedback can be more: (1) data efficient than using additional expert demonstrations via supervised learning (though a combination of both is best)-a learned reward function enables greater performance when used as a signal for an RL method than a supervised method trained with 5 times more data, and (2) parameter efficient-enabling a 220 million parameter model trained with a combination of supervision and NLPO to outperform a 3 billion supervised model. We hope that the benchmarks, baselines, and building blocks we release serve to drive forward research in aligning LMs to human preferences.\n\n\nRELATED WORK\n\nImitation learning for NLP. Algorithms such as Schedule Sampling (SS) (Bengio et al., 2015), Parallel SS (Duckworth et al., 2019), SS for Transformers (Mihaylova & Martins, 2019), Diffential SS (Goyal et al., 2017), LOLS (Lampouras & Vlachos, 2016;Chang et al., 2015), TextGAIL (Wu et al., 2021b), and SEARNN (Leblond et al., 2017), have been inspired by DAGGER (Ross et al., 2011) and SEARN (Daum\u00e9 et al., 2009). However, these algorithms are known to suffer from exposure bias in generation (Chiang & Chen, 2021;Arora et al., 2022) and the cliff MDP problem (Husz\u00e1r, 2015;Agarwal et al., 2019;Swamy et al., 2021).\n\nRL for Large Action Spaces. MIXER (Ranzato et al., 2016) combined ideas from schedule sampling and REINFORCE (Williams, 1992). Bahdanau et al. (2016) proposed an actor-critic algorithm to address the variance/large action space problems when using REINFORCE for language generation; follow-up works such as KG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022), AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by attempting to eliminate and reduce the action space during exploration.\n\nRL for NLP. RL, often in the form of bandit learning, has been used to improve models in machine translation (Wu et al., 2016;Nguyen et al., 2017;Kiegeland & Kreutzer, 2021), summarization (Stiennon et al., 2020;Paulus et al., 2017), dialogue (Li et al., 2016;Zhou et al., 2017;Jaques et al., 2020), image captioning (Rennie et al., 2017), question generation (Pang & He, 2021), text-games (Narasimhan et al., 2015;Hausknecht et al., 2020), and more (Ranzato et al., 2016;Snell et al., 2022). Lu et al. (2022) adapt reward-conditioned transformers (Chen et al., 2021) for several language generation tasks. RL has been the focus of efforts to align LMs with human preferences (Stiennon et al., 2020;Wu et al., 2021a;Nakano et al., 2021;Ziegler et al., 2019), e.g., Ouyang et al. (2022) fine-tuned large language model with PPO Schulman et al. (2017) to align with models of human preference, but their non-public dataset doesn't enable comparison. Though RL has been successful in some of the use cases described above, it has simultaneously been critiqued for being significantly less stable than supervised LM training (Choshen et al., 2020). As a result, there is relatively little consensus if RL is a worthwhile consideration for training LMs compared to, say, collecting additional supervised data.\n\n\nRL4LMS: A LIBRARY FOR TRAINING LMS WITH RL\n\nWe introduce RL4LMs, an open-source library with building blocks for fine-tuning and evaluating RL algorithms on LM-based generation. The library is built on HuggingFace (Wolf et al., 2020) and stable-baselines-3 (Raffin et al., 2021), combining important components from their interfaces. RL4LMs can be used to train any decoder only or encoder-decoder transformer models from Hug-gingFace with any on-policy RL algorithm from stable-baselines-3. Furthermore, we provide reliable implementations of popular on-policy RL algorithms that are tailored for LM fine-tuning such as PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015a), A2C (Mnih et al., 2016), and our own NLPO ( \u00a74). The library is modular, which enables users to plug-in customized environments, reward functions, metrics, and algorithms. In the initial release, we provide support for 6 different NLP tasks, 16 evaluation metrics and rewards, and 4 RL algorithms.\n\n\nENVIRONMENTS: GENERATION AS A TOKEN-LEVEL MDP\n\nEach environment is an NLP task: we are given a supervised dataset D = {(x i , y i )} N i=1 of N examples, where x \u2208 X is an language input and y \u2208 Y is the target string. Generation can be viewed as a Markov Decision Process (MDP) S, A, R, P, \u03b3, T using a finite vocabulary V. Each episode in the MDP begins by sampling a datapoint (x, y) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated. The input x = (x 0 , \u00b7 \u00b7 \u00b7 , x m ) is a task-specific prompt that is used as our initial state s 0 = (x 0 , \u00b7 \u00b7 \u00b7 , x m ), where s 0 \u2208 S and S is the state space with x m \u2208 V. An action in the environment a t \u2208 A consists of a token from our vocabulary V. The transition function P : S \u00d7 A \u2192 \u2206(S) deterministically appends an action a t to the end of the state s t\u22121 = (x 0 , \u00b7 \u00b7 \u00b7 , x m , a 0 , \u00b7 \u00b7 \u00b7 , a t\u22121 ). This continues until the end of the horizon t \u2264 T and we obtain a state s T = (x 0 , \u00b7 \u00b7 \u00b7 , x m , a 0 , \u00b7 \u00b7 \u00b7 , a T ). At the end of an episode a reward R : S \u00d7 A \u00d7 Y \u2192 R 1 that depends on the (s T , y) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) style API for an RL environment that simulates this LM-Based MDP formulation. This abstraction allows for new tasks to be added quickly with compatibility across all implemented algorithms.\n\n\nREWARD FUNCTIONS AND EVALUATION METRICS\n\nBecause RL4LMs provides a generic interface for per-token or per-sequence generation rewards, it is possible to quickly apply a wide array of RL algorithms to a similarly diverse range of textual metrics-as-rewards. Specifically, we provide interfaces to 1) n-gram overlap metrics metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), SacreBLEU (Post, 2018), METEOR (Banerjee & Lavie, 2005); (2) model-based semantic metrics such as BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020) which generally provide higher correlation with human judgment; 3) task-specific metrics such as CIDER (Vedantam et al., 2015), SPICE (Anderson et al., 2016) (for captioning/commonsense generation), PARENT (Dhingra et al., 2019) (for data-to-text) and SummaCZS (Laban et al., 2022) (for factuality of summarization); 4) diversity/fluency/naturalness metrics such as perplexity, Mean Segmented Type Token Ratio (MSSTR) (Johnson, 1944), Shannon entropy over unigrams and bigrams (Shannon, 1948), the ratio of distinct n-grams over the total number of n-grams (Distinct-1, Distinct-2) and count of n-grams that appear only once in the entire generated text (Li et al., 2015); 5) task-specific, model-based human preference metrics such as classifiers trained on human preference data collected in the methodology of Ouyang et al. (2022).\n\n\nON-POLICY ACTOR-CRITIC ALGORITHMS\n\nRL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms on language environments. Formally, this class of algorithms allows us to train a parameterized control policy defined as \u03c0 \u03b8 : S \u2192 \u2206(A), a function that attempts to select an action in a given state so as to maximize long term discounted rewards over a trajectory E \u03c0 [ T t=0 \u03b3 t R(s t , a t )]. Our benchmark experiments focus on fine-tuning a pre-trained LM denoted as \u03c0 0 from which we initial our agent's policy \u03c0 \u03b8 = \u03c0 0 . Similarly, the value network V \u03c6 used to estimate the value function is also initialized from \u03c0 0 except for the final layer which is randomly initialized to output a single scalar value. As with other deep RL actor-critic algorithms, we define our value and Q-value functions as V \u03c0 t = E at\u223c\u03c0 [ T \u03c4 =t \u03b3R(s \u03c4 , a \u03c4 , y)], Q \u03c0 t (s t , a t ) = R(s t , a t , y) + \u03b3E st+1\u223cP [V \u03c0 t+1 (s t+1 )] leading to a definition of our advantage function as A \u03c0 t (s, a) = Q \u03c0 t (s, a) \u2212 V \u03c0 t . To increase training stability, advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015b).\n\nGiven an input-output pair (x, y) and generation predictions from our agent; because the environment rewards are sequence-level and sparse, following Wu et al. (2021a) we regularize the reward function using a token-level KL penalty for all on-policy algorithms, to prevent the model from deviating too far from the initialized LM \u03c0 0 . Formally, the regularized reward function is: R(s t , a t , y) = R(s t , a t , y) \u2212 \u03b2KL (\u03c0 \u03b8 (a t |s t )||\u03c0 0 (a t |s t ))\n\n(1)\n\nwhereR is the regularized KL reward, y is gold-truth predictions, KL (\u03c0 \u03b8 (a t |s t )||\u03c0 0 (a t |s t )) = (log \u03c0 0 (a t |s t ) \u2212 log \u03c0 \u03b8 (a t |s t )) and the KL coefficient \u03b2 is dynamically adapted (Ziegler et al., 2019). Further details on actor-critic methods can be found in Appendix A.\n\n\nNLPO: NATURAL LANGUAGE POLICY OPTIMIZATION\n\nLanguage generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2016;Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively. We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods. To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018;Huang & Onta\u00f1\u00f3n, 2020;Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).\n\nSpecifically, NLPO maintains a masking policy \u03c0 \u03c8 : the masking policy is a copy of the current policy (\u03c0 \u03b8 ), but is updated only every \u00b5 steps. A parameterized-invalid-mask is created from \u03c0 \u03c8 by first selecting the top-p tokens from the vocabulary, 4 and then applying an invalid-mask to the remaining tokens-i.e. setting their probabilities to zero when sampling actions from \u03c0 \u03b8 during training; this periodic updating policy \u03c0 \u03c8 is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017), providing the policy \u03c0 \u03b8 with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \u03c0 0 and the risk of reward hacking. We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).\n\nAlgorithm 1 NLPO -Natural Languge Policy Optimization\nInput: Dataset D = {(x i , y i )} N i=1\nof size N Input: initial policy parameters \u03c0 \u03b80 Input: initial LM \u03c0 0 Input: initial value function parameters V \u03c60 Input: initialize parameterized masked policy \u03c0 \u03c80 (\u00b7|\u00b7, \u03c0 \u03b80 ) with parameterized top-p policy \u03c0 \u03b80 Input: policy update frequency \u00b5 repeat\nSample mini-batch D m = {(x m , y m )} M m=1 from D Collect trajectories T m = {\u03c4 i } by running policy \u03c0 \u03c8n in for batch D m in env.\nEq.6 Compute Preference and KL penalty rewardsR t Eq. 1 Compute the advantage estimate\u00c2 t Sec. 3.3 Update the policy by maximizing the PPO-Clip objective:\n\u03c0 \u03b8m+1 = argmax \u03b8 1 |D m |T \u03c4 \u2208Dm T \u03c4 =0 min r t (\u03b8)A \u03c0 \u03b8m , clip(r t (\u03b8), 1 \u2212 , 1 + )A \u03c0 \u03b8m )\nwhere r t (\u03b8) = \u03c0 \u03b8 (at|st) \u03c0 \u03b8m (at|st) . Update the value function:\nV \u03c6m+1 = argmin \u03c6 1 |D m |T \u03c4 \u2208Dm T t=0 V \u03c6 (s t ) \u2212R t 2\nUpdate the parameterized masked policy every \u00b5 iterations: \u03c0 \u03c8n+1 (\u00b7|\u00b7, \u03c0 \u03b8m+1 ) until convergence and return \u03c0 \u03b8\n\n\nGRUE (GENERAL REINFORCED-LANGUAGE UNDERSTANDING EVAL)\n\nGRUE is a collection of 7 generative NLP tasks. To combat reward hacking for any single metric, each task is evaluated at test time according to a task-specific mix of metrics, detailed in Table 1. The metrics span two categories. Task preference metrics capture how well the models produce generations that satisfy the desiderata of the specific generation task, e.g., for Commongen, if the generations contain all the required words, or for IMDB, how positive the generated completions are. Naturalness metrics capture fluency, readability, etc. and provide perspective on factors beyond semantics. At training time, there are no special restrictions: models are free to use the supervised data, compute metrics on intermediate generations, etc. Train/val/test splits follow the original works. All results are averaged over multiple seeds, with exact counts being found in Appendix B.\n\nExperimental Setup. We use RL4LMs to test a large range of algorithms on the GRUE benchmark. Specifically: We compare 3 algorithms for direct fine-tuning -Supervised, PPO, 5 and NLPO. In 4 \u03c0 \u03c8 could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. 5 We consider PPO representative of the present state-of-the-art -in particular, we do not consider the popular REINFORCE (Willianms, 1988;Williams, 1992), as recent works have shown PPO to be strictly superior to REINFORCE in multiple domains (Schulman et al., 2017) Table 1: GRUE Benchmark using RL4LMs showing the various tasks, input and output types, and the metrics used. We note that we test RL algorithms on these tasks for a wider range of possible rewards than just the task specific ones shown here. Unless specified, datasets are in English.\n\naddition, we consider a hybrid approach of supervised learning and our RL methods by applying PPO and NLPO on checkpoints that have been fine-tuned in a supervised fashion-we call these Supervised+PPO, Supervised+NLPO. As an additional baseline, we additionally run zero-shot evaluations where we design prompts which aim to elicit task-specific generations, but with no training data or parameter updates.\n\nFor each task, to isolate the effect of training method, we select a single pre-trained LM backbone.\n\nFor IMDB text continuation we use GPT-2 (117m parameters), and for the rest of the tasks we use T5-base (220m parameters). For our RL models (PPO, NLPO, Supervised+PPO, Supervised+NLPO), for a thorough investigation of how reward-hacking might interplay with GRUE, we run a separate set of experiments optimizing multiple task rewards for each task independently, e.g., for Commongen which has 6 task rewards (CIDER, ROUGE-2, ROUGE-L, BLEU-3, BLEU-4, METEOR) we run 6 different experiments optimizing each metric independently and report all possible metrics seen in Table 1 regardless of which individual metric was being optimized for.\n\nHuman Participant Study. We gather human judgments for five of the tasks in GRUE. In doing so, our goals are 1) to validate that the automated metrics we selected for GRUE correlate with human judgments with respect to relative ranking between models; and 2) to provide additional empirical comparisons regarding NLPO vs. PPO, ablations to study the effects of the KL naturalness penalty, etc. We specifically consider IMDB, Commongen, ToTTo, DailyDialog, and CNN Daily Mail. For each individual sample in a task, we ask 3 unique human raters to provide Likert judgments of 1) quality, i.e., for the specific task, how correct/appropriate is the generation, given the context, and 2) fluency, i.e., how well-written is the generation. We used Amazon Mechanical Turk, and paid crowdworkers a minimum of $15/hr. More details, including qualification information, interface screenshots, instructions, etc. are given in the corresponding Appendicies.\n\n\nRESULTS ON GRUE: WHICH ALGORITHM SHOULD BE USED TO LEARN PREFERENCES?\n\nFigures 2(a), 2(b) present the results on GRUE, split into task metrics and naturalness metrics, and Tables 2, 3 highlight key results via ablation studies. Full results are available in Appendix B. For text continuation and summarization, with non-trivial zero-shot performance, RL tends to perform better than supervised training, but for tasks like Commongen and ToTTo, which have very low zero-shot performance, supervised training performs best-with both approaches outperforming zero-shot.\n\nHowever, using RL+Supervised learning in conjunction works best; NLPO+supervised and PPO+supervised usually always outperforms NLPO/PPO (or supervised in isolation) across both task metrics and naturalness metrics. Supervised warm-starting is particularly effective for Commongen and ToTTo, which our results suggest are more prone to reward hacking. The one exception to this trend is DailyDialog where the RL models outperform warm-started Supervised+RL models likely due to the low performance of the Supervised models. We note that Supervised+NLPO using a   T5-base (220m parameter) LM currently outperforms all the models on the ToTTo leaderboard, many of which have \u2265 3b parameter supervised models-suggesting that RL is parameter efficient as well.\n\nIn these cases, it is critical that the initial policy already contain (some) signal for the task due to it being used as a KL constraint and masking constraint in NLPO. If the mask contains no initial priors about task specific language, it will be eliminating the wrong actions-a better initial policy leads to better RL performance downstream.\n\nHuman agreement with automated metrics. As human judgments can be noisy, we run additional statistical analysis such as measuring inter-annotator agreement, via Krippendorf's alpha score, and using a one-way ANOVA followed by a post-hoc Tukey HSD test to measure if differences in means of average scores between pairs of models are significant. We find that trends in our human evaluations generally match those seen in the automated metrics for both task and naturalness metrics (see Figures 2(c), 2(d) which summarize Appendix Tables 10,15,21,26, 35-Supervised+NLPO > Supervised \u2265 Supervised+PPO > NLPO \u2265 PPO > Zero-shot-with the exception of Supervised outperforming Supervised+PPO on 2 out of 5 tasks when automated metrics would indicate that Supervised+PPO outperforms Supervised on all of the tasks. We draw two conclusions from this:\n\n(1) if the generated text is above a certain threshold of naturalness, the automated metrics usually correlate with human judgements; (2) usually but not always as seen in the relative performance of Supervised and Supervised+PPO, potentially indicating reward hacking behaviors undetected by automated metrics but caught by human preference feedback.\n\n\nPREFERENCE REWARD LEARNING, SELECTION, AND HACKING\n\nWhile the GRUE benchmark's metric for each task is an average over several measures, the RL models we trained optimized only a single metric independently. Thus, we can empirically investigate which metric for which GRUE produces the best results. We observe that many possible single metric rewards provide task performance gains over supervised methods (results shown in Fig. 3(a), 2(c) are averaged across these reward functions) with the condition that the text is also coherent and natural.\n\nWhich constraints best prevent reward hacking? The reward function in Equation 1 balances a task-specific reward with a KL constraint -models are penalized from straying too far from a base LM in their pursuit of high reward (Table 3 and Appendix Table 5) clearly show that if KL constraints are removed entirely, models reward hack). But which model works best as a base regularizing LM? When the initial policy (i.e., the raw, pretrained model) has low performance on the task, the KL penalty pushes the policy towards nonsense, e.g. on Commongen and ToTTo the trained policy learns to simply repeat portions of the input (as seen in Tables B.4.5, B.6.4). This behavior is mitigated if the base regularizing LM is the supervised model-the reward encourages the policy to balance the task-specific reward and a more reasonable regularization term. Deriving KL penalties from warm-started initial policies is critical for performance on such tasks.\n\nPPO vs. NLPO. Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training. We hypothesize that the primary reason for NLPO's improved performance and stability is because the masking policy provides an additional constraint for the current policy. This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \u00b5 iterations ago and likely contains more task-relevant information learned during RL training. Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.\n\nHuman Preference Reward Learning. To this point, our experiments have largely focused on optimizing evaluation metrics that correlate with human judgments, e.g., METEOR. Here: we additionally test how well preferences can be learned from direct human feedback. For this, we focus on Commongen -a GRUE dataset well-suited for displaying differences due to human preferences. First, we randomly select prompts from the Commongen train dataset and sample a single completion from both the Supervised and Supervised+NLPO models. We then present the prompt and the two completion candidates to 3 unique crowdworkers and ask them to select which one they prefer with respect to commonsense/fluency for 417 unique pairs (Krippendorf \u03b1 = .28). We use this data to train a reward model, T5-11B Raffel et al. (2020), on the balanced binary classification task of predicting which of the pair was preferred by a majority of 3 annotators, conditioned on the prompt and completion. Given a fixed data collection budget, is it more efficient to gather feedback to improve a learned reward function or to gather more expert demonstrations? We use the IMDB text continuation task as a case study. In the IMDB task, a model is given a partial movie review as a prompt, and is asked to continue it as positively as possible (even if the prompt was negative). The original dataset consists of movie reviews and sentiment labels of positive, negative, or neutral. A DistilBERT (Sanh et al., 2019) classifier is trained on these labels and used to provide sentiment scores on how positive a given piece of text is, which serves as the task reward. The trade-off is between gathering more: 1) sentiment labels (improving the reward); or 2) positive sentiment reviews (improving supervised training).\n\nWe train a classifier on varying amounts of training data and evaluate on the held out test datasetfinding as expected that more training data improves test accuracy and so results in a higher quality reward. We then use each of these rewards of varying quality during RL training, and evaluate using the same metric as GRUE (i.e., a classifier trained with the entire training set). As seen in Table 3, we find that improving the reward quality improves LM performance as well. Further, we trained a supervised model with at least as many samples used to train each of these reward classifiers. We find that a learned reward function enables greater performance when used as a signal for an RL method than a supervised method trained with 5 times more data. This implies that improving reward models can be more data efficient than collection expert demonstrations for a task-and that's not accounting for the fact that assigning sentiment labels is likely a simpler task than writing full demonstrations. Further details on this ablation are found in Appendix Table 7.\n\n\nPRACTICAL CONSIDERATIONS: WHICH IMPLEMENTATION DETAILS MATTER MOST?\n\nGeneration as a token-level MDP, not a bandit environment. Most recent works that tune LMs using RL do so by calculating a reward for all the tokens in the sentence (Wu et al., 2021a;Ouyang et al., 2022;Lu et al., 2022). This setting is equivalent to a bandit feedback environment where the action space is the space of all possible generations for the task (Sutton & Barto, 2018). This type of environment can be simulated within our RL formulation by setting the discount factor \u03b3 = 1. Table 3 (and Appendix Table 6) shows that this causes instability in training with respect to naturalness in both PPO and NLPO for IMDB. Our standard setting is \u03b3 = 0.95 when calculating discounted rewards-to-go in the token-level MDP formulation, which reduces the magnitude of the reward that is applied to tokens selected at the beginning. The sentiment scores are approximately the same between both settings but the naturalness of language in the bandit setting is significantly less-indicating that discounting rewards with \u03b3 < 1 via a token-level MDP formulation is at least sometimes more effective for language generation.\n\nDropout and Sampling. We found two other implementation details to be critical for stability of RL training. The first is dropout, which in its standard form was found to cause instability in policy gradient methods in continuous control settings by Hausknecht & Wagener (2022). We find a similar effect when using dropout when RL training LMs as well, with training loss often diverging for dropout > 0 in training. The second important detail, particularly affecting the machine translation task, is sampling methods. We find that using the same sampling methods during exploration and inference is critical to translating training performance to test performance-else the model exhibits high train rewards but low test metrics.\n\n\nCONCLUSIONS\n\nWe're hopeful that the GRUE benchmark and the RL4LMs library can push progress in aligning language models to human preferences via RL methods by providing the community with a standard means of comparing methods. Furthermore, we're optimistic that, as the stability and consistency of training improves, our methods provide a path towards iterative improvement of language technologies, with deployment, user feedback collection, and re-optimization enabling better user experiences when interacting with generative models. \ne t = clip KL (\u03c0(a t |s t )||\u03c0 0 (a t |s t )) \u2212 KL target KL target , \u22120.2, 0.2 (2) \u03b2 t+1 = \u03b2 t (1 + K \u03b2 e t )(3)\nwhere KL target is user-specified KL divergence between initial model h and current policy \u03c0 and K \u03b2 is rate of update which we generally set to 0.2 in our experiments.\n\nTo increase stability during training, we further use Generalized Advantage Estimation (GAE) (Schulman et al., 2015b) and define the advantage estimator\u00c2(s n , a n ) based on the Temporal Difference residual as:\n\u03b4 t = r(s t , a t ) + V \u03c6 (s t+1 ) \u2212 V \u03c6 (s t ). (4) A(s n , a n ) = \u221e t=0 \u03bb t \u03b4 n+t ,(5)\nwhere \u03bb provides the trade-off between bias and variance.\n\nA.2 NLPO DETAILS NLPO learns to mask irrelevant language by maintaining a masking policy \u03c0 \u03c8 : the masking policy is a copy of the current policy (\u03c0 \u03b8 ), but is updated only every \u00b5 steps. Given Z(\u03c0 \u03b8 ) = a\u2208V \u03c0 \u03b80 (a|s) the normalization value of the sum of probabilities of all action a \u2208 A given a particular State s \u2208 S, let the parameterized top-p vocabulary V p \u03c0 \u03b8 \u2282 V be the subset of the vocab, consisting of the top-p highest probability vocabulary tokens with respect to \u03c0 \u03b8 . Formally, let Z p be the normalization value for the parameterized top-p vocabulary, can be defined as the subset of tokens that maximizes Z p (\u03c0 \u03b8 ) = a\u2208V k \u03c0 \u03b8 \u03c0 \u03b8 (a|s). Then optimizing a policy according to the parameterized top-p vocabulary can be defined as:\n\u03c0 \u03c8 (\u00b7|s, \u03c0 \u03b8 ) = \u03c0 \u03b8 (\u00b7|s)/Z p (\u03c0 \u03b8 ) if a \u2208 V p \u03c0 \u03b8 and Z(\u03c0 \u03b8 ) 0 otherwise.(6)\nB EXPERIMENTAL DETAILS\n\n\nB.1 CROWDWORKING DETAILS\n\nQualification round We ran a qualification round using the IMDB task. We opened the qualification around to users from {AU, CA, NZ, GB, US} with 5K prior approved HITs and a minimum acceptance rate of 97% on their previous HITs. We gathered judgments over 600 generations from 3 annotators per generation. One of the authors of this paper also completed 17 random HITs to serve as a proxy for \"ground truth.\" After gathering these annotations, we selected workers who: 1) didn't significantly disagree with other annotators on the same instance more than 20% of the time; 2) who completed at least 5 HITs; 3) who didn't disagree with the author annotator on the 17 HITs by more than 1 point; and 4) (likely) spent a reasonable amount of time reading the instructions/examples provided. In the end, 56 annotators were qualified. Additional per-task details are provided in the per-task sections of the Appendix.\n\nCompensation details As per Amazon Mechanical Turk policy, annotators were compensated on a per-HIT basis. In addition, we used a timing script to estimate hourly wages to ensure our target of $15/hr was met. In cases where this minimum hourly rate was not met, we manually assigned bonuses.\n\n\nB.2 GRUE EXPERIMENT SETUP\n\nWe benchmark 5 training algorithms on 6 tasks (see Table 1) using either an encoder model (eg. GPT-2) or encoder-decoder model (eg. T5). We train policies using PPO, NLPO with variations of whether supervised pre-training is applied before RL fine-tuning and compare against supervised policy. The choice of LM is based on the type of task. For IMDB text continuation, we use GPT-2 and T5 for rest of the tasks. We use two separate LM models as actor and critics networks (i.e. no shared layers) in which the critic network has an additional linear layer mapping last token's hidden representation to a scalar value. We use AdamW optimizer Loshchilov & Hutter (2017) Figure 3: Summarized results via automated metrics across all 7 GRUE tasks for each of the 5 algorithms we consider, and human participant studies for the 5 tasks suitable for human studies. We break up the metrics into task-specific, e.g. average positive sentiment for IMDB task, and naturalness metrics, such as perplexity and human perceived coherence for the human rated metrics. This plot differs from Figure 2 as this one averages over over multiple reward functions per each task.  We chose GPT-2 as LM for this task as it is more suited for text continuation than encoder-decoder LMs (eg. T5). We use top-k sampling with K = 50 as the decoding method and for fair comparison, we keep this setting for all methods. For PPO and NLPO models, we train for 64k steps in total and update policy and value networks every 1280 steps with a mini-batch size of 64 and epochs of 5 per update. We apply adaptive KL controllers with different target KLs of 0.02, 0.05, 0.1, inf with an initial KL co-efficient of \u03b2 = 0.1. Table 4 provides an in-depth summary of all hyperparameters and other implementation details.\n\n\nModel\n\n\nB.3.2 RESULTS AND DISCUSSION\n\nTarget KL ablation Fig 4 shows learning curves for PPO and NLPO in terms of episodic training reward, corpus level sentiment scores and perplexity scores on validation set averaged for 5 random seeds. It is seen that higher target KL of 0.1 is desired to achieve higher rewards but results in drifting , it is seen that higher target KL (0.1) is desired to achieve higher rewards. However, this setting drifts away from the original LM too much and loses fluency. Therefore a lower target KL (0.02 or 0.05) is required to keep the model closer to original LM. Similar trends hold for NLPO but when compared to PPO, it retains lower perplexities and is more stable even with higher KL targets  Table 5: Target KL Ablations: Mean and standard deviations over 5 random seeds is reported for sentiment scores along with fluency and diversity metrics on validation set. It is seen from perplexity scores that a lower target KL constraint is desired to keep the model closer to the original model. On the otherhand, a higher target KL yields higher sentiment scores at the cost of fluency. inf KL penalty (target KL of inf), model simply learns to generate positive phrases (eg: \"I highly recommend this movie to all!\", \"worth watching\") regardless of the context. NLPO achieves better sentiment and perplexity scores than PPO.\n\naway from pre-trained LM and loses fluency. Therefore, a lower target KL (0.02 or 0.05) is required to keep the LM closer to original LM. This is also seen in Table 5 where we presented a comparative analysis of final performance of all models.\n\nTraining data size ablation We vary the amount of data used to train the reward classifier and the supervised baseline model to understand whether it is more efficient to gather data to improve reward model or to gather expert demonstrations for supervised learning. As observed in Table 7, improving the quality of reward function increases the performance on the overall task better than training with more data for supervised training, indicating that improving reward models is efficient than collect expert demonstrations for supervised training from a data efficiency perspective.\n\nDiscount factor ablation To understand the effect of discounted vs undiscounted (bandit) environments, we report sentiment and perplexity scores for different values of discount factor (0.5, 0.95 and 1.0) in Table 6 and observe that using a bandit environment (discount factor of 1.0) results in performance loss in the case of NLPO and reward hacking in the case of PPO, indicating that discounted setting (with 0.95) is desired.\n\nNLPO params   Table 7: Evaluation of GPT2 with different algorithms on IMDB sentiment text continuation task, data budget ablations: Mean and standard deviations over 5 random seeds is reported for sentiment scores along with fluency and diversity metrics. This table measures performance differences as a function of the fraction of the dataset that has been used. In the case of the RL approaches, this measures how much data is used to train the reward classifier, and for the supervised method it directly measures fraction of positive reviews used for training. We note that using even a small fraction of data to train a reward classifier proves to be effective in terms of downstream task performance while this is not true for supervised approaches. This lends evidence to the hypothesis that adding expending data budget on a reward classifier is more effective than adding more gold label expert demonstrations.    Figure 5 shows the IMDB instructions, example, and interface used both for the qualification round, and then later, for the human evaluation experiments. Tables 9, 10 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans. Figure 5: Instructions, example, and interface for the IMDB sentiment completion task.  We show sample generations from each of the algorithms for three randomly picked prompts below.\nGroup 1 Group 2 Coherence Sentiment Diff (G2-G1) p-values Diff (G2-G1) p-values PPO with KL PPO without KL -0.\nSample 1 Prompt: A man and his wife are not getting along because of the wife's jealousy and drinking problems. When the wife disappears, her sister begins a vigorous search involving the Zero-Shot: husband's sister, which leads several men to a remote cabin and then a boat. The wife and her husband then start to see the same things. Finally, a strange woman is driving out to her cabin and discovers the husband's disappearance but References: local police, state government, and a television crime show. What she finds out stuns the entire community. Good drama with lots of fine performances; based on a true story PPO: police and the local public. At the end, the woman is killed and the wife is found naked and with a red head tattooed on her back. This is part of a special feature called\"On the Road, Hidden\" which was PPO-no-KL: police and other local governments. Written by G. Scott Fitzgerald, who also stars in this production, \"A Very Merry Christmas\" is a wonderful family movie with a wonderful soundtrack and wonderful characters. It brings home the true beauty of small-NLPO: mother while the brother goes under. After all, his sister, who was apparently kidnapped but who turns out to be a man, is no longer in her place. If a film that was made in the 1950s NLPO-no-KL: friends of the wife, and also the friend of the wife of the woman that is missing, and eventually he begins to find that she has gone to Germany and he has made a very good friend. There is a good scene between John and Supervised: amily tree. The widower, now widowed, returns, the daughter, daughter is not pleased when his wife is mistaken for one of her sons, and the patriarch, then a wealthy man, falls victim to a jealous husband. The\n\n\nSample 2\n\nPrompt: A very credible and unsettling movie portraying the effects of war on the general public, Shame gives us a chilling understanding oh Ingmar Bergman's beliefs. The absence of color and sound (as in a soundtrack) helps to give the film a more realistic feeling. There is no soft or hard lighting or a variety of camera angles to give the story a charming or dramatic effect. Everything is very simply portrayed as it would be in reality. At times the storyline was confusing, but perhaps in order to give a sense of the characters' frame of mind how they too do now know what is happening or why. The simplicity of the Zero-Shot: camera work is at once apparent and fascinating in the manner in which it affects the way things are portrayed. In other films these two have been referred to from time to time in a form and manner more suited to the screen. The film shows References: film made it somewhat boring, but I could understand how the director is trying not to glamorize the story and points. He depicted the harsh and unsettling events and effects of war. In the beginning the characters were introduced and shown as content with the lighter settings PPO: characters relationships is so engrossing and the great and unique story presented by each pair of brothers and sisters creates a truly gripping and enjoyable experience. To put it simply, I strongly recommend to everyone who plays the role to buy this DVD!!! PPO-no-KL: movie helps it to flow along in a believable way. Rating 4 out of 5 Great Movie! I highly recommend these movies! I highly recommend these movies! Rating 5 out of 5 Great Movie! I highly recommend these movies! Rating 10 of\" NLPO: novel makes it one of the first books I have read by a British actor. It is a fun way for someone who likes their books, like me, to sit through a novel. \n\n\nSample 3\n\nPrompt: Shakespearean language is wild and rambling, saturated in multiple meanings, word play and metaphor. To be understood it must be wrangled and tamed by an actor with the strength and knowledge to do so. When an actor fails, the words pour forth in a torrent of incomprehensible words, but when he succeeds, the English language springs to life with an immediacy and vibrancy that takes your breath away. Al Pacino is one such actor, and here displays an incredible level of clarity and control that, were there any justice, Zero-Shot: might still be seen in the world today. This movie, like \"Vacation\", is an effort at telling the tale of two families from different races. One is African but never grew up the other. One of them, a man References: would sweep every award in the offering. He meets the challenge of presenting Shylock head on, and delivers an extraordinarily subtle and nuanced performance. It would be a crime if we never got the opportunity to see what he does with King Lear.<br /><br />The supporting cast PPO: would have made him an instant-classic. This movie captures the spirit of childhood and has everyone in the world watching it. It's a love story and it's really heartbreaking. And at the same time, it's a classic. It PPO-no-KL: will bring victory to the fight against hatred and extremism. Al Pacino is a very beautiful, well executed actor who brings an absolute intensity and sensibility to his portrayal of love and loyalty. A must see! Recommended! Also, an Excellent NLPO: I would watch and enjoy. 7/10, Oscar Wilde is always a pleasure to see. A must see.\n\nHighly recommend. Highly recommend! One of my top ten films of all time. A must see!! NLPO-no-KL: the whole film would have fallen to the same fate, just as it did just a couple hundred years ago. Don't miss it. It's a real classic. Highly Recommended. * outta five stars for it! Supervised: his performance (so far) would seem mere shadow. He is truly in the middle of a movie, and this film is one of those films where he can be convincing in it (and his trademark acting, as you can see in the B.4 COMMONGEN B.4.1 SETUP CommonGen (Lin et al., 2020) deals with task of generating coherent sentences describing an input set of concepts (eg. \"a man is throwing a frisbee\"). For training RL methods, we consider 3 traditional lexical rewards namely Rouge-1, Rouge-avg (which is an average of Rouge-1, 2 and L) and meteor. Additionally, we also train with task-specific rewards such as CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and SPiDer (Liu et al., 2017) which is a just a linear combination of both with equal weights. We chose T5-base as the base LM since it is well-suited for structure to text tasks. We additionally note that concept set inputs are prefixed with \"generate a sentence with:\" to encourage exploration.\n\nDuring our initial experiments when fine-tuning directly on LM, we observed that policy learns to repeat the prompted concepts in order to maximize rewards resulting in a well-known problem of reward hacking. To mitigate this, we add a penalty score of \u22121 to final task reward if the n-grams of prompt text overlaps with generated text. In contrast, when initialized with a supervised policy, this problem is not seen and hence penalty score is not applied. We use beam search as the decoding method during evaluation whereas for rollouts, we use top k sampling to favor exploration over exploitation.   Tables 13, 12 presents our benchmarking results with 6 reward functions along with supervised baseline performances on dev and test sets respectively. Our main finding is that warm-started initial policies are crucial for learning to generate coherent sentences with common sense. Without warmstart, policies suffer from reward hacking despite application of repetition penalty and task-specific metrics such as CIDer etc. Further, we find that RL fine-tuned models obtain very high concept coverage which is also seen in Table B.4.5. Supervised models often tend to miss few concepts in its generation compared to RL methods.     Published as a conference paper at ICLR 2023  Tables 14, 15 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.\n\n\nB.4.4 HUMAN PREFERENCE LEARNING EXPERIMENTS\n\nFirst, we randomly select prompts from the Commongen train dataset and sample a single completion from both the Supervised and Supervised+NLPO models. Next, we filter to prompts where both models at least attempted to use all input concepts. This filtration step was conducted because if a model fails to use all concepts, it may generate a more natural/fluent sentence, but, a priori, it shouldn't be preferred by crowdworkers; instead of training crowdworkers to prefer sentences with all concepts, we perform this filter. Figure 7 shows the task presented to the crowdworkers. We then present the prompt and the two completion candidates to 3 unique crowdworkers and ask them to The resulting model achieved 69.5 test ROC AUC suggesting it indeed captures average human preferences. The model is then used as a reward function. We train Supervised+RL with a METEORonly reward as a baseline, and compare it to a reward function that uses the fine-tuned T5-11B model. We design the reward function based on the preference model as r = meteor + pref /(1 + |miss|) where miss is a set of concepts not covered in the generated text, in an attempt to mimic the data collection process that humans are instructed to follow. This reward function accounts for both the task of using all concepts and also human's preferences for how a sentence should look within the constraints stipulated by the task. Finally, we rerun the same pairwise preference collection procedure-this time sampling from Commongen test-with human participants to compare the generations from a preference optimized RL policy to the previously best Supervised+NLPO policy.\n\nComparing the METEOR-only to the preference model head-to-head, the generations produced by the human feedback model are preferred in 682 cases, compared to the METEOR-only model which is preferred in 587 cases (p < 0.01 the models are equally preferred).\n\n\nB.4.5 QUALITATIVE ANALYSIS\n\nThis section shows sample generations from different algorithms for three randomly picked prompts.\n\n\nSample 1\n\nPrompt: generate a sentence with: apron cut hat kitchen sausage Zero-Shot: generate a sentence with: apron cut hat kitchen sausage. PPO: sausage in the kitchen on an apron. NLPO: sausage cut hat cut hat cut hat cut apron cut hat Supervised: A man is cutting sausage on an apron in a kitchen. Supervised + PPO: A man cutting sausage from a hat on an apron in the kitchen. Supervised + NLPO: A man cuts sausage in a hat on an apron in the kitchen.         Figure 8 shows the summarization instructions and interface used for the human evaluation experiments. Participants weren't required to read the entire article, but to encourage some reading, a minimum time on the window of 15s was enforced via hiding the sliders. Tables 20, 21 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.\n\n\nB.5.4 QUALITATIVE ANALYSIS\n\nWe show sample generations from each of the algorithms for three randomly picked prompts below.\n\nSample 1 Prompt: Manchester City are confident UEFA\u00e2\u0202\u0179s punishment for breaching financial fairplay regulations will be lifted this summer which would allow them to bid for stellar names like Raheem Sterling, Gareth Bale, Kevin de Bruyne and Ross Barkley. City boss Manuel Pellegrini has been hampered over the past year by UEFA restricting them to a net transfer spend of 49million in each window and keeping the club\u00e2\u0202\u0179s overall wage bill to its current level of 205million-a-year. UEFA\u00e2\u0202\u0179s settlement with City published in May stated those penalties would remain in place until the end of the 2015/16 season but the club\u00e2\u0202\u0179s latest financial figures showed drastically-reduced losses of 23million which they feel proves they are now compliant with FPP regulations. Manuel Pellegrini is hoping that the financial restrictions imposed by UEFA for a breach of FFP rules will be lifted at the end of this season . Manchester City have been limited to a net spend of 49 million in the last two transfer windows -they spent 25m bringing Wilfried Bony in from Swansea in January . Ahead of Monday night\u00e2\u0202\u0179s trip to Crystal Palace, Pellegrini was certainly talking like a man excited at the prospect of signing 'crack' players this summer. \u00e2\u0202\u0178I think that next season we don\u00e2\u0202\u0179t have any restrictions so we will be in the same position that all the other English clubs have,\u00e2\u0202\u0179 said Pellegrini. \u00e2\u0202\u0178It\u00e2\u0202\u0179s important. You have so many strong teams here in England and in Champions League, you can not allow them to keep the advantage every year; having less players to put in your squad or spending less money. We spend money, of course we always spend money, but they spent more.\u00e2\u0202\u0179 Manchester United, Barcelona, Liverpool and Arsenal have all paid more in transfer fees in the past 12 months than City who were traditionally Europe\u00e2\u0202\u0179s biggest spenders after the club was taken over by Abu Dhabi owners in 2008. Uefa also ordered City to play with a reduced squad from 25 players to 21 in the Champions League this season and while that restriction has now ended, any time reduction in the penalties on spending and wages is more controversial. Arsenal have paid more in transfer fees than City in the last 12 months, including 30m on Alexis Sanchez . The document published last May by UEFA\u00e2\u0202\u0179s Club Financial Control Body investigative chamber explicitly said City\u00e2\u0202\u0179s financial penalties would run for two seasons at least and there has been no official deviation from that decision. The published statement said at the time: \u00e2\u0202\u0178Manchester City agrees to significantly limit spending in the transfer market for the seasons 2014/15 and 2015/16. It means City will have to argue their case with Uefa that as they have been financially compliant over the past year, they deserve to be free of restrictions moving forward. They have successfully argued their case with UEFA before. Last summer they persuaded the governing body to allow them to bypass the normal quota of eight homegrown players as their Champions League squad had been reduced. Eliaquim Mangala joined the champions from Porto for 32m last summer . The reigning Premier League champions have only paid money for Fernando, Willie Caballero, Eliaquim Mangala and Wilfried Bony in the last two transfer windows and that was part-paid by the sales of Javi Garcia and Jack Rodwell. Pellegrini admitted they weren\u00e2\u0202\u0179t in a position to compete with neighbours United for established world stars like Angel di Maria because of the FFP penalties. It has cost City on the pitch. Even if they win their remaining eight Premier League fixtures their points tally will be lower than last season and they crashed out once again at the last-16 stage of the Champions League. Pellegrini and director of football Txiki Begiristain both accept the squad needs replenishing and they want to be in the market for the top available players.\n\nManchester City have been heavily linked with a move for former Chelsea midfielder Kevin de Bruyne . The club also want to be involved in any race for Liverpool contract rebel Raheem Sterling . They have been strongly linked with former Chelsea winger Kevin de Bruyne who has been one of the best players in the Bundesliga with Wolfsburg. And if Liverpool do allow Sterling to leave, City want to be involved in the chase for one of England\u00e2\u0202\u0179s most outstanding talents to help fit in with quotas for homegrown players. \u00e2\u0202\u0178If you are the manager or a player of a big tem, you are always open to criticism because everyone expects a high level of performance,\u00e2\u0202\u0179 he said. \u00e2\u0202\u0178The major issue with our team always seems to be the money but this season I think we were the team that spent less than any other team. That is the weird thing. \u00e2\u0202\u0178For this club trying to be a big team in so few years, maybe we are paying the cost for that.\u00e2\u0202\u0179 Since the transfer penalties were introduced, City have spent 91million on players in the last two windows ( Net spend 68million). That compares to Manchester United 145.5million, Liverpool 113 million, Arsenal 92.5million and Chelsea 82.6million. Over the same time period Barcelona spent 118.3million on players and Real Madrid 81.2million though they also broke the world transfer record for Gareth Bale the previous summer. Zero-Shot: manuel Pellegrini hoping UEFA\u00e2\u0202\u0179s punishment for breaching financial fairplay regulations will be lifted this summer. city have been restricted to a net transfer spend of 49million in each window. a total of 23million has been lost in the last year. References: Manchester City have been restricted to a net transfer spend of 49m .Club's also had to keep overall wage bill to its current level of 205m .Punishments imposed by UEFA for breaching Financial Fair Play rules .The spending restrictions were set for this season and the next one .But City are confident they will be lifted early after their compliance . PPO: Manuel Pellegrini is hoping UEFA\u00e2\u0202\u0179s punishment for breaching financial fairplay rules will be lifted this summer. Manchester City have been limited to a net spend of 49million in each transfer window. the club also keep the club's overall wage bill to its current level of 205million-a-year. NLPO: Manchester City are confident UEFA\u00e2\u0202\u0179s punishment for breaching financial fairplay regulations will be lifted this summer. that would allow them to bid for stellar names like Raheem Sterling, Kevin de Bruyne and Ross Barkley. Supervised: Manchester City have been limited to a net transfer spend of 49million in each window. City's overall wage bill is 205m-a-year due to a breach of UEFA rules. City are confident the financial restrictions will be lifted at the end of the current season in order to bid for stars such as Raheem Sterling. Manuel Pellegrini is hoping that the financial restrictions will be lifted at the end of the 2015/16 season Supervised + PPO: Manchester City have been restricted to a net spend of 49million in each window and kept their overall wage bill at 205m. Manuel Pellegrini believes these restrictions will be lifted at the end of the season. Arsenal have spent more in transfer fees than City in the last 12 months, including 30m on Alexis Sanchez. Supervised + NLPO: Manchester City are confident UEFA\u00e2\u0202\u0179s punishment for a breach of financial fairplay regulations will be lifted this summer. The club have been restricted to a net transfer spend of 49million over the past two window. They aim to bid for stars like Raheem Sterling, Gareth Bale and Kevin de Bruyne. City lost 23m on transfers in the last 12 months due to financial restrictions imposed by UEFA for breach Sample 2 Prompt: (CNN)Larry Johnson remembers the fear and feeling of helplessness from being on the SkyWest Airlines flight that made an emergency landing in Buffalo, New York. \"I thought we were done,\" he said Thursday, one day later. \"There was no doubt in my mind that we weren't going to land.\" Johnson was flying with his brother, his girlfriend and his 8month-old son when he says a flight attendant came over the speaker asking for someone who was medically trained to help with a sick passenger. Minutes later, Johnson says, the attendant announced there was a pressurization problem and told passengers to prepare for the emergency landing. \"You're going to feel dizzy and woozy and lightheaded, and you' re not going to be able to breathe,\" Johnson recalled the flight attendant saying. The Federal Aviation Administration on Wednesday initially reported a pressurization problem with SkyWest Flight 5622, and said it would investigate. It later issued a statement that did not reference any pressurization issues. \"It was like being trapped and you couldn't do anything. You have no control over it. Your body just felt pressured,\" Johnson said. Johnson said his girlfriend was seeing spots and that \"when I was blinking, it was really bright. I could barely see.\" According to Marissa Snow, spokeswoman for SkyWest, three passengers reported a loss of consciousness while on the flight. Fifteen passengers were examined by medical personnel at the Buffalo airport, but no one was transported to the hospital, she said. The spokeswoman said that maintenance personnel found no indication of a pressurization problem with the aircraft, an Embraer E170, and that the airline continues to investigate the cause. An official with the National Transportation Safety Board told CNN that the agency is in communication with the FAA and SkyWest to gather information on the incident to better understand what took place. Mary Cunningham was the nurse on board who attended to the first sick passenger. \"There was a woman, very lethargic, very nauseous, turning gray, just not looking good,\" Cunningham said. After administering oxygen to the woman, Cunningham, a nurse at Yale-New Haven Hospital in Connecticut, returned to her seat. Soon after, Cunningham said, she was called up again by the flight attendant after another women near the first started to lose consciousness. \"It was something that was affecting oxygen in that area,\" which was near the middle of the plane, she said. Cunningham said she had to return to her seat after feeling lightheaded. There were 84 passengers on board the plane, including nine crew members. Flight 5622 was originally scheduled to fly from Chicago to Hartford, Connecticut. The plane descended 28,000 feet in three minutes. \"It would feel like a roller coaster --when you're coming over the top and you're going down,\" CNN aviation analyst Mary Schiavo said, describing how such a descent would feel. \"You know that these pilots knew they were in a very grave and very serious situation.\" Johnson, flying to visit his parents with their only grandchild, was glad that roller coaster ride ended safely. \"It was a shaky ride. It wasn't anything I would like to experience again,\" he said. CNN's Jean Casarez, Josh Gaynor, Dana Ford and Stephanie Gallman contributed to this report. Zero-Shot: \"you're going to feel dizzy and woozy and lightheaded,\" Johnson says. \"it was like being trapped and you couldn't do anything,\" Johnson says. \"the plane was trying to get back on the ground,\" passenger says of fear. References: Three passengers report a loss of consciousness on SkyWest flight .But officials say there is no evidence of a pressurization problem . PPO: SkyWest flight 5312 made emergency landing in Buffalo, new york. passengers were told they were not going to be able to breathe, passenger recalls attendant. the faa initially reported a pressurization problem with the flight 5622. Thursday. \"There was no doubt in my mind that we weren't going to land,\" passenger says. \"You're going to feel dizzy and woozy and lightheaded...you're not going to be able to breathe,\" he says.\"\n\n\nSample 3\n\nPrompt: For Inverness the latest chapter in their remarkable story featured not one, but two, plot-changing twists. One featured penalty Celtic didn't get for 2-0 in which Josh Meekings handball should have also led to a sending off. The other the spot kick they did, followed by a red card for Craig Gordon. 'I've not seen it yet, but going by the reaction of the Celtic players we got away with a penalty and a sending off and that was probably the turning point in the game,' acknowledged Caley manager John Hughes after. Inverness's Josh Meekings appears to get away with a handball on the line in their win over Celtic . Caley boss John Hughes says the break, which could have meant a penalty and red card, was a turning point . 'I've not spoken to Josh. I haven't seen it -but going by the media it was definitely a hand ball. We look at the referee behind the line and all that and I know Ronny will feel aggrieved -because I certainly would. 'But it's part and parcel of football and you need a wee bit of luck to beat Celtic. 'This was their biggest game of the season because they will go on and win the league and if they had beaten us today there was a good chance they would have gone on and won the Scottish Cup.\n\n'But when Marley Watkins was clipped by Craig Gordon and they were down to 10 men that was advantage Inverness. 'We weren't going to give Celtic the ball back, they had to come and get it and we had to be patient. 'When big Edward put us into the lead we thought it was going to be our day on the back of things that had happened. 'Celtic equalised with another free kick but it's typical of Inverness that we don't do anything easy. 'We do it the hard way and we came up with the winner through David Raven.' Hughes hauled Raven, his Scouse defender, from his backside as extra-time beckoned. Offended by the sight of one of his players resting he had a message to impart. Caley players celebrate after upsetting Celtic in a Scottish Cup semi-final 3-2 thriller . Celtic, depleted by games and absentees, were virtually on their knees after a relentless programme of midweek games. In last season's League Cup Final Inverness had been passive and unambitious prior to losing on penalties. This was no time to repeat the mistake. 'I tried to emphasise to the players they would never have a better time to go on and beat Celtic, down to 10 men in the semi final of a cup. We needed to go for it,' Hughes said. 'Before Raven scored at the back post I was looking to change it. I was going to bring on another winger, Aaron Doran, and put him in the full-back position over on the right, but more advanced so he could take their left back on. Thankfully I didn't do that and David Raven came up with the goal. Virgil Van Dijk (centre) fired Celtic into an early lead with a superb free-kick in the 18th minute . 'I didn't realise this is the first time the club have been in the final of the Scottish Cup and that's a remarkable achievement given it was only formed 20 years ago. 'It is a great story isn't it? It's an absolutely fantastic story. It is 20 odd years since the amalgamation. We are a small provincial club up there in the Highlands. 'We have lost a real inspirational skipper in Richie Foran right from the start of the season. He has never played. We have had to adjust to that. 'We had to sell Billy McKay, our top goalscorer, at Christmas. We have had to go again and adjust. I am a very humble guy and I am grateful and thankful that injuries have never caught up with us.' There is remarkable irony in the fact Falkirk will be the opponents for the final. A former Bairns captain, he was manager of the club in 2009 when they lost to Rangers at Hampden. Former Falkirk captain and manager John Hughes will take on his former club in the final . 'I had a lot of great times at Falkirk. So much so that it is possibly my favourite time in my playing career. I am still friendly with an awful lot of the characters who were in that dressing room. Neil Oliver is a good friend of mine from my Falkirk days. He comes along and sits on the bench and gives me a wee hand out. 'That is the spirit that we had at that club. I have met some great guys. Brian Rice, my ex-coach, Davie Weir, Ian McCall, the list is endless.\n\nI was just talking the other day about that day at Falkirk. There are times even now when I see. 'I have a real special, special feel for the place. I am not kidded Falkirk is a right good side, trust me on that. No side goes to Tynecastle and beats Hearts without being a good side. No side beats Hibs the way they were playing without being a good side. You might say we are favourites I say it is even stevens and I am just looking forward to when the days comes.' David Raven (second right) celebrates with his Inverness team-mates following his extra-time winner . An Inverness and Falkirk final would not have been the choice of sponsors William Hill, you suspect. Vast swathes of Hampden's west stand were empty, a 12.15 kick-off for the benefit of broadcasters proving unhelpful . There will be no ticket rows this time. Not over a final unlikely to raise a crowd much higher than 35,000 in a stadium of 52,000. The contract with Hampden plc is signed, with no question of moving it. If the bitter amalgamation of 21 years ago produced a fine football club it also disenfranchised many of those who followed the old ones. 'I hope this encourages the next generation to follow Inverness,' added Hughes. 'In the peer groups it is still Celtic and Rangers they follow but I hope the next ones will want to support their local club. 'If we can add another few hundred or thousand to the support then we are playing our part. Meekings' handball on the line should have also seen a sending off. But Craig Gordon got away with a spot kick and sending off. John Hughes said it was a turning point in the game for Inverness. Supervised + NLPO: Josh Meekings appears to get away with a handball after Inverness's win.\n\nInverness were down to 10 men when Craig Gordon was sent off. John Hughes says the break could have meant a penalty and red card. Hughes says Celtic will go on and win the Scottish Cup with win  Tables 24, 23 presents our benchmarking results with 5 reward functions along with supervised baseline performances on dev and test sets respectively. Similar to other tasks, our main finding is that warm-started initial policies are crucial for learning to generate descriptions from highlighted cells. Without warm-start, policies suffer from reward hacking and resulting in sub-optimal solutions despite application of task-specific metrics such as PARENT etc. We find that Supervised+NLPO method outperforms all models on ToTTo leaderboard in terms of PARENT metric.      Figure 9 shows the ToTTo instructions, example, and interface used for the human evaluation experiments. We made small modifications to the original code release's HTML renderer to make the tables display in our HITs. Tables 25, 26 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.\n\n\nB.6.4 QUALITATIVE ANALYSIS\n\nWe show sample generations from each of the algorithms for three randomly picked prompts below.\n\nSample 1 Prompt: <table> <cell> August 1, 1992 <col_header> Date </col_header> <col_header> North America </col_header> <col_header> Australasia </col_header> <col_header> Asia </ col_header> <col_header> North America </col_header> </cell> <cell> Allentown <col_header> City </col_header> <col_header> North America </col_header> <col_header> Australasia </ col_header> <col_header> Asia </col_header> <col_header> North America </col_header> </ cell> <cell> Allentown Fairgrounds Grandstand <col_header> Country </col_header> < col_header> North America </col_header> <col_header> Australasia </col_header> < col_header> Asia </col_header> <col_header> North America </col_header> </cell> </table>< page_title> Under My Spell Tour </page_title> <section_title> Tour dates </section_title> <table> <cell> August 1, 1992 <col_header> Date </col_header> <col_header> North America </col_header> <col_header> Australasia </col_header> <col_header> Asia </col_header> < col_header> North America </col_header> </cell> <cell> Allentown <col_header> City </ col_header> <col_header> North America </col_header> <col_header> Australasia </ col_header> <col_header> Asia </col_header> <col_header> North America </col_header> </ cell> <cell> Allentown Fairgrounds Grandstand <col_header> Country </col_header> < col_header> North America </col_header> <col_header> Australasia </col_header> < col_header> Asia </col_header> <col_header> North America </col_header> </cell> </table>    We show sample generations from each of the algorithms for three randomly picked prompts below.\n\nSample 1 Prompt: who is mark hunter? mark hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. his pirate station's theme song is \" everybody knows\" by leonard cohen and there are glimpses of cassettes by such alternative musicians as the jesus and mary chain, camper van beethoven, primal scream, soundgarden, ice-t, bad brains, concrete blonde, henry rollins, and the pixies. by day, mark is seen as a loner, hardly talking to anyone around him; by night, he expresses his outsider views about what is wrong with american society. when he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show.nobody knows the true identity of \"hard harry\" or \"happy harry hard-on ,\" as mark refers to himself, until nora diniro (mathis), a fellow student, tracks him down and confronts him the day after a student named malcolm commits suicide after harry attempts to reason with him. the radio show becomes increasingly popular and influential after harry confronts the suicide head-on, exhorting his listeners to do something about their problems instead of surrendering to them through suicide\u00c3\u0107\u00c2\u0202\u00c2\u0164at the crescendo of his yelled speech, an overachieving student named paige woodward (who has been a constant listener) jams her various medals and accolades into a microwave and turns it on. she then sits, watching the awards cook until the microwave explodes, injuring her. while this is happening, other students act out in cathartic release.eventually, the radio show causes so much trouble in the community that the fcc is called in to investigate. during the fracas, it is revealed that the school's principal (annie ross) has been expelling \" problem students,\" namely, students with below-average standardized test scores, in an effort to boost the district's test scores while still keeping their names on the rolls ( a criminal offense) in order to retain government funding.realizing he has started something huge, mark decides it is up to him to end it. he dismantles his radio station and attaches it to his mother's old jeep, creating a mobile transmitter so his position can't be triangulated. pursued by the police and the fcc, nora drives the jeep around while mark broadcasts. the harmonizer he uses to disguise his voice breaks, and with no time left to fix it, mark decides to broadcast his final message as himself. they finally drive up to the crowd of protesting students, and mark tells them that the world belongs to them and that they should make their own future. the police step in and arrest mark and nora. as they are taken away, mark reminds the students to \"talk hard.\" as the film ends, the voices of other students (and even one of the teachers) speak as intros for their own independent stations, which can be heard broadcasting across the country. Zero-Shot: hunter (slater) is a high school student in a sleepy suburb of phoenix, arizona. he starts an fm pirate radio station that broadcasts from the basement of his References: he is a high school student in phoenix 2) loner and outsider student with a radio station PPO: hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house NLPO: hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house Supervised: a high school student Supervised + PPO: a high school student Supervised + NLPO: a high school student\n\n\nSample 2\n\nPrompt: what was the principle doing with the problem students? mark hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. his pirate station's theme song is \"everybody knows\" by leonard cohen and there are glimpses of cassettes by such alternative musicians as the jesus and mary chain, camper van beethoven, primal scream, soundgarden, ice-t, bad brains, concrete blonde, henry rollins, and the pixies. by day, mark is seen as a loner, hardly talking to anyone around him; by night, he expresses his outsider views about what is wrong with american society. when he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show.nobody knows the true identity of \"hard harry\" or \"happy harry hard-on,\" as mark refers to himself, until nora diniro ( mathis), a fellow student, tracks him down and confronts him the day after a student named malcolm commits suicide after harry attempts to reason with him. the radio show becomes increasingly popular and influential after harry confronts the suicide head-on, exhorting his listeners to do something about their problems instead of surrendering to them through suicide\u00c2\u0202\u00c2\u0164at the crescendo of his yelled speech, an overachieving student named paige woodward (who has been a constant listener) jams her various medals and accolades into a microwave and turns it on. she then sits, watching the awards cook until the microwave explodes, injuring her. while this is happening, other students act out in cathartic release.eventually, the radio show causes so much trouble in the community that the fcc is called in to investigate. during the fracas, it is revealed that the school's principal (annie ross) has been expelling \"problem students,\" namely, students with below-average standardized test scores, in an effort to boost the district's test scores while still keeping their names on the rolls (a criminal offense) in order to retain government funding.realizing he has started something huge, mark decides it is up to him to end it. he dismantles his radio station and attaches it to his mother's old jeep, creating a mobile transmitter so his position can't be triangulated. pursued by the police and the fcc, nora drives the jeep around while mark broadcasts. the harmonizer he uses to disguise his voice breaks, and with no time left to fix it, mark decides to broadcast his final message as himself. they finally drive up to the crowd of protesting students, and mark tells them that the world belongs to them and that they should make their own future. the police step in and arrest mark and nora. as they are taken away, mark reminds the students to \"talk hard.\" as the film ends, the voices of other students (and even one of the teachers) speak as intros for their own independent stations, which can be heard broadcasting across the country. Zero-Shot: mark hunter (slater) starts an fm pirate radio station that broadcasts from the basement of his parents' house. by day, mark is seen as a loner, an outsider, whose only outlet References: 1) expelling them. 2) expelling the students PPO: mark hunter (slater) starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage ang NLPO: mark hunter (slater) starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage ang Supervised: he was taking them to a psychiatric hospital. Supervised + PPO: addressing them Supervised + NLPO: he was expelling them\n\n\nSample 3\n\nPrompt: what happens to the people maskull meets in his travels to various lands? maskull, a man longing for adventures, accepts an invitation from krag, an acquaintance of his friend nightspore, to travel to tormance after a seance. the three set off in a crystal ship from an abandoned observatory in scotland but maskull awakens to find himself alone on tormance. in every land he passes through he usually meets only one or two persons; these meetings often (though not always) end in the death of those he meets, either at his own hand or by that of another. he learns of his own impending death, meets krag again, and dies shortly after learning that he is in fact nightspore himself. the book concludes with a final revelation from krag (who claims to be known on earth as \"pain\") to nightspore about the origin of the universe. the author turns out to support a variation of the doctrine of the demiurge, somewhat similar to that propounded by some gnostics.all of the characters and lands are types used to convey the author's critique of several philosophical systems. on tormance, most such viewpoints or ways of life are accompanied by corresponding new bodily sense organs or modifications of the same, thus each distinct weltanschauung landscape has its corresponding sensorium. Zero-Shot: what happens to the people maskull meets in his travels to various lands? n maskull, a man longing for adventures, accepts an invitation from krag to travel to tormance after References: 1) they usually die by his hand or anothers. 2) they die PPO: what happens to the people maskull meets in his travels to various lands? n maskull, a man longing for adventures, accepts an invitation from krag to travel to tormance after NLPO: maskull meets krag, an acquaintance of nightspore, to travel to tormance after a seance. maskull awakens to find himself alone on tormance. in every land Supervised: they die either from his own hand or from another person's death. Supervised + PPO: they end up dying either at his own hand or by another's. Supervised + NLPO: they end up dying either at his own hand or by another's B.8 MACHINE TRANSLATION B.8.1 SETUP WMT-16 We pick two languages, English and German, and frame this task similarly to other machine translation tasks-requiring the models to translate from English to German. We train models on 4 rewards: SacreBLEU, chRF, TER, and BertScore.    \n\n\nModel\n\n\nSample 3\n\nPrompt: translate English to German: And of course there are teams that have a lot more \"ta-da \" structures, and among the best are recent graduates of kindergarten. Zero-Shot: Und nat\u00c3ijrlich gibt es Teams, die viel mehr \"ta-da\" Strukturen haben, und zu den besten geh\u00c3\u0171ren j\u00c3ijngste Absolventen des Kindergartens. References: Und nat\u00c3ijrlich gibt es auch Teams die viel mehr \"ta-da\"-Gebilde haben und unter den Besten sind frische Absolventen des Kindergartens. PPO: Und nat\u00c3ijrlich gibt es Teams, die viel mehr \"ta-da\" Strukturen, und unter den besten sind j\u00c3ijngste Absolventen des Kindergartens. NLPO: Und nat\u00c3ijrlich gibt es Teams, die viel mehr \"t a-da\" Strukturen haben, und unter den besten sind j\u00c3ijngste Absolventen des Kindergartens Supervised: Und nat\u00c3ijrlich gibt es Teams, die viel mehr \"ta-da\"-Strukturen haben, und unter den besten sind j\u00c3ijngste Absolventen des Kindergartens. Supervised + PPO: Und nat\u00c3ijrlich gibt es Teams, die viel mehr \"ta-da\"-Strukturen haben, und unter den besten sind j\u00c3ijngste Absolventen des Kindergartens. Supervised + NLPO: Und nat\u00c3ijlich gibt es Teams, die viel mehr \"ta-da\"-Strukturen haben, und unter den besten sind j\u00c3ijngste Absolventen des Kindergartens.    Table 33 presents our benchmarking results of RL methods along with supervised baseline performances on test sets. Our main finding is that RL methods generally achieve better intent accuracy and automatic metric scores, in particular NLPO variants perform better than all other methods.\n\nB.9.3 HUMAN PARTICIPANT STUDY Figure 10 shows the Daily Dialogue instructions and interface used for the human evaluation experiments. Tables 34, 35 show averaged results, annotator agreement, and the results of statistical significance tests to determine which models output better generations when rated by humans.  Table 35: Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 -Group 1) and corresponding p-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall p-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with p 0.05 for both coherence and sentiment.\n\nFigure 1 :\n1Natural Language Policy Optimization (NLPO) in the case of sentiment-guided continuation.\n\nFigure 4 :\n4Learning Curves: Averaged learning curves over 5 different runs by varying target KL, shaded regions indicate one standard deviation. (a) shows the rollout episodic total reward during training (b) shows evolution of sentiment scores on the validation split (c) shows evolution of perplexity on the validation split. From (a) and (b)\n\nFigure 6 :\n6Instructions, examples, and interface for the Commongen task.B.4.3 HUMAN PARTICIPANT STUDYFigure 6shows the commongen instructions, examples, and interface used for the human evaluation experiments. Different from the other human evaluations, we didn't provide any prompt because knowing the set of words to be used isn't required for rating either of the axes.\n\nFigure 7 :\n7Instructions and interface for the pairwise Commongen HIT. select which one they prefer with respect to commonsense/fluency; We gathered 3 annotations on 417 pairs (Krippendorf \u03b1 = .28), and split into 60/20/20 train/val/test split. We then trained a reward model,T5-11B Raffel et al. (2020), on the balanced binary classification task of predicting which of the pair was preferred by a majority of 3 annotators, conditioned on the prompt and completion.\n\nFigure 8 :\n8Instructions and interface for the summarization task.B.5.3 HUMAN PARTICIPANT STUDY\n\n\nNLPO: SkyWest flight 5622 made emergency landing in buffalo, new york.Larry Johnson says flight attendant announced there was a pressurization problem. three passengers reported loss of consciousness while on flight; no one transported to hospital. Supervised: The FAA says it is investigating the cause of the emergency landing. SkyWest Flight 5622 made an emergency landing in Buffalo, New York. The flight was carrying a family of three people. The flight attendant told the passengers to prepare for the landing. Supervised + PPO: \"I thought we were done,\" the father of one passenger says. The flight attendant announced there was a pressurization problem. The flight made an emergency landing in Buffalo, New York. Three passengers reported a loss of consciousness while on the flight. Supervised + NLPO: SkyWest Airlines flight made an emergency landing in Buffalo, New York, on\n\n\nZero-Shot: inverness beat Celtic 2-0 in their opening game of the season. Josh Meekings was the subject of a handball after the break. the break could have meant a penalty and a red card for Craig Gordon. References: Josh Meekings handball should have been a penalty and a red card .If the penalty was awarded and converted Celtic would have gone to 2-0 .Caley manager John Hughes admitted his side were fortunate in win .Virgil van Dijk scored Celtic's opener with a superb free-kick .Celtic keeper Craig Gordon was sent off early in the second half .Greg Tansey, Edward Ofere and finally Daven Raven scored for Caley . PPO: Josh Meekings handball could have meant sending off for Celtic in extra-time win. John Hughes says the break could have meant an penalty and red card. Caley boss also admits he would feel aggrieved had Celtic won the championship. NLPO: Caley beat Celtic 2-0 in their Scottish Cup clash. Josh Meekings handball could have led to a penalty and red card. John Hughes says the break was probably the turning point in the game. defender does not believe defensive striker can get away with being shown the yellow card -despite playing in same half we never scored. Supervised: Inverness beat Celtic 2-0 in their Scottish Cup clash. Josh Meekings appeared to get away with a handball on the line. Another player was sent off for a spot kick which ended in a red card. Caley manager John Hughes says the break could have meant a penalty. Hughes says he 'would feel aggrieved' after the break. Supervised + PPO: Caley beat Celtic 1-0 in their Scottish Cup encounter at the Etihad. Josh\n\n\nSummarized results via automated metrics across all 7 GRUE tasks for each of the 5 algorithms we consider, and human participant studies for the 5 tasks suitable for human studies. Test results are averaged over all the respective metrics seen inTable 1. Tasks IMDB CommonGen CNN/DM ToTTO WMT16 NarQA DialogCommongen \n\nIMDB \n\nNarrativeQA \n\nWMT16 \n\nToTTo \n\nCNN \n\nDailyDialog \n\n0.32 \n0.35 \n0.39 \n0.43 \n\n0.55 \n\n0.57 \n\n0.59 \n\n0.61 \n\n0.13 \n\n0.19 \n\n0.25 \n\n0.31 \n\n0.49 \n\n0.49 \n\n0.5 \n\n0.5 \n\n0.3 \n0.36 \n0.42 \n0.48 \n\n0.37 \n\n0.38 \n\n0.39 \n\n0.4 \n\n0.31 \n\n0.33 \n\n0.34 \n\n0.35 \n\n(a) Automated Task Metrics \n\nCommongen \n\nIMDB \n\nNarrativeQA \n\nWMT16 \n\nToTTo \n\nCNN \n\nDailyDialog \n\n0.19 \n0.22 \n0.25 \n0.28 \n\nSupervised \nPPO \nSupervised+PPO \nNLPO \nSupervised+NLPO \n\n0.95 \n\n0.96 \n\n0.97 \n\n0.98 \n\n0.86 \n\n0.89 \n\n0.92 \n\n0.95 \n\n0.88 \n\n0.88 \n\n0.89 \n\n0.89 \n\n0.04 \n0.08 \n0.13 \n0.17 \n\n0.64 \n\n0.67 \n\n0.7 \n\n0.72 \n\n0.85 \n\n0.86 \n\n0.86 \n\n0.87 \n\n(b) Automated Naturalness Metrics \n\nCommongen \n\nCNN \n\nIMDB \n\nToTTo \n\nDailyDialog \n\n2.53 \n2.96 \n3.38 \n3.81 \n\n3.79 \n\n3.85 \n\n3.9 \n\n3.96 \n\n3.54 \n\n3.59 \n\n3.65 \n\n3.7 \n\n2.99 \n\n3.39 \n\n3.79 \n\n4.19 \n\n3.36 \n\n3.43 \n\n3.49 \n\n3.56 \n\n(c) Human Study Task Metrics \n\nCommongen \n\nCNN \n\nIMDB \n\nToTTo \n\nDailyDialog \n\n2.47 \n2.95 \n3.44 \n3.92 \n\n4.0 \n\n4.08 \n\n4.16 \n\n4.24 \n\n3.14 \n\n3.28 \n\n3.42 \n\n3.56 \n\n2.71 \n\n3.18 \n\n3.66 \n\n4.13 \n\n3.99 \n\n4.04 \n\n4.1 \n\n4.15 \n\n(d) Human Study Naturalness Metrics \n\nFigure 2: Questions \nNeeds Warm Start \n\n\n\n\n\n\n\nEasily reward hackable? \n\n\n\n\n\n\n\nRL > Sup (auto)? \n\n\n\n\n\n\n\nRL > Sup (human)? \n\n\n\n\n-\n-\n\nSup+RL > Sup (auto)? \n\n\n\n\n\n\n\nSup+RL > Sup (human)? \n\n\n\n\n-\n-\n\nSup+NLPO > Sup+PPO (auto)? \n\n\n\n\n\n\n\nSup+NLPO > Sup+PPO (human)? \n\n\n\n\n-\n-\n\n\n\n\nTable 2 :\n2Key questions answered using GRUE + RL4LMs: This table summarizes the results found in the ablations andFig. 2and provides an overview of the questions we ask in Section 5: which tasks require warm starts or are easily reward hackable; when to use RL over Supervised, when to use both; and when to use NLPO over PPO. All conclusions drawn are the result of statistical analysis as discussed in the experimental setup.Ablation \nSentiment \nPerplexity \nZero Shot \n0.489 \n32.171 \nSupervised \n0.539 \n35.472 \nPPO \n0.602 \n33.816 \nNLPO \n0.611 \n33.832 \nWarm Starting (Sec. 5.1) \nPPO+Supervised \n0.626 \n35.049 \nNLPO+Supervised \n0.620 \n34.816 \nData Budget (Reward trained on 10% of data, Sec. 5.3) \nPPO \n0.598 \n35.929 \nNLPO \n0.599 \n33.536 \nRemoving NLPO Top-p Constraints (Sec. 5.2) \n(p = 1 is equivalent to PPO, p = 0.9 is NLPO) \nNLPO p = 0.1 \n0.579 \n32.451 \nNLPO p = 0.5 \n0.588 \n32.447 \nRemoving KL Constraints (Sec. 5.2) \nPPO-no-KL \n0.838 \n41.897 \nNLPO-no-KL \n0.858 \n41.429 \nDiscount Ablations (\u03b3 = 1) (Sec. 5.4) \nPPO \n0.651 \n41.035 \nNLPO \n0.624 \n43.720 \n\n\n\nTable 3 :\n3IMDB Ablation Results.\n\n\nR\u00e9mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, and Simon Lacoste-Julien. Searnn: Training rnns with global-local losses. arXiv preprint arXiv:1706.04499, 2017. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015. A.1 PPO Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 NLPO Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 ACKNOWLEDGEMENTS \n\nWe'd like to acknowledge the support of DARPA MCS program through NIWC Pacific (N66001-19-\n2-4031), Google Cloud Compute, and the ReViz team at the Allen Institute for AI. KB is supported \nby NSF under grant No. 2127309 to the Computing Research Association for the CIFellows Project. \nKai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum\u00e9 III, and John Langford. Learning \nto search better than your teacher. In International Conference on Machine Learning, pp. 2058-\n2066. PMLR, 2015. \n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, \nAravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence \nmodeling. In NeurIPS, 2021. \n\nTing-Rui Chiang and Yun-Nung Chen. Relating neural text degeneration to exposure bias. In \nProceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Net-\nworks for NLP, pp. 228-239, Punta Cana, Dominican Republic, November 2021. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2021.blackboxnlp-1.16. URL https: \n//aclanthology.org/2021.blackboxnlp-1.16. \n\nLeshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. On the weaknesses of reinforcement \nlearning for neural machine translation. In International Conference on Learning Representations, \n2020. \n\nHal Daum\u00e9, John Langford, and Daniel Marcu. Search-based structured prediction. Machine learning, \n75(3):297-325, 2009. \n\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William W \nCohen. Handling divergent reference texts when evaluating table-to-text generation. arXiv preprint \narXiv:1906.01081, 2019. \n\nDaniel Duckworth, Arvind Neelakantan, Ben Goodrich, Lukasz Kaiser, and Samy Bengio. Parallel \nscheduled sampling. arXiv preprint arXiv:1906.04331, 2019. \n\nKartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick. Differentiable scheduled sampling for credit \nassignment. arXiv preprint arXiv:1704.06970, 2017. \n\nMatthew Hausknecht and Nolan Wagener. Consistent dropout for policy gradient reinforcement \nlearning. arXiv preprint arXiv:2202.11818, 2022. \n\nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\u00f4t\u00e9, and Xingdi Yuan. Interactive \nfiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence \n(AAAI), 2020. URL https://arxiv.org/abs/1909.05398. \n\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa \nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural \ninformation processing systems, 28, 2015. \n\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. Learning \nto write with cooperative discriminators. arXiv preprint arXiv:1805.06087, 2018. \n\nShengyi Huang and Santiago Onta\u00f1\u00f3n. A closer look at invalid action masking in policy gradient \nalgorithms. arXiv preprint arXiv:2006.14171, 2020. \n\nFerenc Husz\u00e1r. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? \narXiv preprint arXiv:1511.05101, 2015. \n\nNatasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah \nJones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement \nlearning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language \nProcessing (EMNLP), pp. 3985-4003, Online, 2020. Association for Computational Linguistics. \ndoi: 10.18653/v1/2020.emnlp-main.327. \n\nWendell Johnson. Studies in language behavior: A program of research. Psychological Monographs, \n56(2):1-15, 1944. \n\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings \nof the Association for Computational Linguistics: EMNLP 2020, pp. 1896-1907, Online, Novem-\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.171. \nSamuel Kiegeland and Julia Kreutzer. Revisiting the weaknesses of reinforcement learning for neural \nmachine translation. In Proceedings of the 2021 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language Technologies, pp. 1673-1681, \nOnline, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. \n133. \n\nTom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, \nand Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the \nAssociation for Computational Linguistics, 6:317-328, 2018. \n\nJulia Kreutzer, Stefan Riezler, and Carolin Lawrence. Offline reinforcement learning from human \nfeedback in real-world sequence-to-sequence tasks. In Proceedings of the 5th Workshop on \nStructured Prediction for NLP (SPNLP 2021), pp. 37-43, Online, August 2021. Association for \nComputational Linguistics. doi: 10.18653/v1/2021.spnlp-1.4. URL https://aclanthology. \norg/2021.spnlp-1.4. \n\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. SummaC: Re-Visiting NLI-\nbased Models for Inconsistency Detection in Summarization. Transactions of the Association for \nComputational Linguistics, 10:163-177, 02 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00453. \nURL https://doi.org/10.1162/tacl_a_00453. \n\nGerasimos Lampouras and Andreas Vlachos. Imitation learning for language generation from \nunaligned data. In Proceedings of COLING 2016, the 26th International Conference on Computa-\ntional Linguistics: Technical Papers, pp. 1101-1112. The COLING 2016 Organizing Committee, \n2016. \n\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep \nreinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on \nEmpirical Methods in Natural Language Processing, pp. 1192-1202, Austin, Texas, Novem-\nber 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1127. URL \nhttps://aclanthology.org/D16-1127. \n\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. DailyDialog: A manually \nlabelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on \nNatural Language Processing (Volume 1: Long Papers), pp. 986-995, Taipei, Taiwan, November \n2017. Asian Federation of Natural Language Processing. URL https://aclanthology. \norg/I17-1099. \n\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and \nXiang Ren. CommonGen: A constrained text generation challenge for generative commonsense \nreasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. \n1823-1840, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. \nfindings-emnlp.165. \n\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization \nbranches out, pp. 74-81, 2004. \n\nSiqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Improved image captioning \nvia policy gradient optimization of spider. In Proceedings of the IEEE international conference on \ncomputer vision, pp. 873-881, 2017. \n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike \nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining \napproach. arXiv preprint arXiv:1907.11692, 2019. \n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint \narXiv:1711.05101, 2017. \nJeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. \nRecursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021a. \n\nQingyang Wu, Lei Li, and Zhou Yu. Textgail: Generative adversarial imitation learning for text \ngeneration. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, pp. online. \nAAAI Press, 2021b. \n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, \nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation sys-\ntem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, \n2016. \n\nTom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not \nto learn: Action elimination with deep reinforcement learning. Advances in neural information \nprocessing systems, 31, 2018. \n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating \ntext generation with bert. arXiv preprint arXiv:1904.09675, 2019. \n\nLi Zhou, Kevin Small, Oleg Rokhlenko, and Charles Elkan. End-to-end offline goal-oriented dialog \npolicy learning via policy gradient. CoRR, abs/1712.02838, 2017. URL http://arxiv.org/ \nabs/1712.02838. \n\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul \nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv \npreprint arXiv:1909.08593, 2019. \nCONTENTS \n\n1 Introduction \n1 \n\n2 Related Work \n3 \n\n3 RL4LMs: A Library for Training LMs with RL \n3 \n\n3.1 Environments: Generation as a Token-level MDP . . . . . . . . . . . . . . . . . . \n3 \n\n3.2 Reward Functions and Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . \n4 \n\n3.3 On-policy Actor-critic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . \n4 \n\n4 NLPO: Natural Language Policy Optimization \n4 \n\n5 GRUE (General Reinforced-language Understanding Eval) \n5 \n\n5.1 Results on GRUE: Which Algorithm Should be Used to Learn Preferences? . . . . \n6 \n\n5.2 Preference Reward Learning, Selection, and Hacking . . . . . . . . . . . . . . . . \n8 \n\n5.3 Data Budget: Improve your Reward or Gather More Demonstration? . . . . . . . . \n9 \n\n5.4 Practical Considerations: Which Implementation Details Matter Most? . . . . . . . \n9 \n\n6 Conclusions \n9 \n\n7 Acknowledgements \n10 \n\nA On-policy Algorithm Implementation Details \n18 \n\nB Experimental Details \n19 \nA ON-POLICY ALGORITHM IMPLEMENTATION DETAILS \n\nA.1 PPO DETAILS \n\nGiven discussion and equations in Section 3.3, we further note that we follow (Ziegler et al., 2019) \nand dynamically adapt the KL coefficient \u03b2 during training where, \n\n\n\nTable 4 :\n4IMDB Hyperparams: Table shows a list of all hyper-parameters and their settingsB.3 IMDB \n\nB.3.1 SETUP \n\nWe consider IMDB dataset for the task of generating text with positive sentiment. The dataset consists \nof 25k training, 5k validation and 5k test examples of movie review text with sentiment labels of \npositive and negative. The input to the model is a partial movie review text (upto 64 tokens) that \nneeds to be completed (generating 48 tokens) by the model with a positive sentiment while retaining \nfluency. For RL methods, we use a sentiment classifier Sanh et al. (2019) that is trained on pairs of \ntext and labels as a reward model which provides sentiment scores indicating how positive a given \npiece of text is. For supervised Seq2Seq baselines, we consider only the examples with positive labels. \n\n\nTable . 8\n.shows ablation on different hyperparameters in NLPO algorithm.Gamma \nSemantic and Fluency Metrics \nDiversity Metrics \n\nSentiment Score \u2191 \nPerplexity \u2193 \nMSTTR \nDistinct1 \nDistinct2 \nH1 \nH2 \nUnique1 \nUnique2 \n\nZero-Shot \n0.489 \u00b1 0.006 \n32.371 \u00b1 0.137 0.682 \u00b1 0.001 0.042 \u00b1 0.001 0.294 \u00b1 0.001 8.656 \u00b1 0.004 13.716 \u00b1 0.003 5063 \u00b1 14.832 47620 \u00b1 238 \n\nPPO \n0.5 \n0.511 \u00b1 0.023 \n35.945 \u00b1 0.92 \n0.69 \u00b1 0.001 0.044 \u00b1 0.002 0.304 \u00b1 0.007 8.726 \u00b1 0.041 13.793 \u00b1 0.055 \n5304 \u00b1 285 \n49668\u00b1 1496 \n0.95 \n0.605 \u00b1 0.023 \n33.497 \u00b1 0.447 0.666 \u00b1 0.013 0.043 \u00b1 0.002 0.287 \u00b1 0.008 8.575 \u00b1 0.073 13.484 \u00b1 0.244 \n5230 \u00b1 363 \n46483 \u00b1 1318 \n1.0 \n0.651 \u00b1 0.05 \n41.035 \u00b1 2.885 0.691 \u00b1 0.017 0.042 \u00b1 0.004 0.295 \u00b1 0.031 8.697 \u00b1 0.237 13.563 \u00b1 0.396 \n5127 \u00b1 460 \n48319 \u00b1 5650 \nNLPO \n0.5 \n0.49 \u00b1 0.01 \n37.279 \u00b1 5.137 0.688 \u00b1 0.01 0.045 \u00b1 0.002 0.312 \u00b1 0.016 8.746 \u00b1 0.113 13.873 \u00b1 0.25 \n5395 \u00b1 192 \n50828 \u00b1 2506 \n0.95 \n0.637 \u00b1 0.013 \n32.667 \u00b1 0.631 0.677 \u00b1 0.014 0.044 \u00b1 0.002 0.288 \u00b1 0.010 8.588 \u00b1 0.100 13.484 \u00b1 0.236 \n5205 \u00b1 189 \n46344 \u00b1 2688 \n1.0 \n0.624 \u00b1 0.039 \n43.72 \u00b1 2.475 0.662 \u00b1 0.019 0.05 \u00b1 0.007 \n0.3 \u00b1 0.038 \n8.624 \u00b1 0.277 13.360 \u00b1 0.537 \n6337 \u00b1 921 \n49441 \u00b1 6520 \n\n\n\nTable 6 :\n6Evaluation of GPT2 with different algorithms on IMDB sentiment text continuation task, discount factor ablations: Mean and standard deviations over 5 random seeds is reported for sentiment scores along with fluency and diversity metrics. This table measures performance differences for the discount factor. We note that most NLP approaches using RL follow the style ofLi et al. (2016); Wu et al. (2021a)  and use a discount factor of 1. This is equivalent to reducing the generation MDP to a bandit feedback environment and causes performance loss (in the case of NLPO) and reward hacking and training instability (in the case of PPO).Perc Data (size) \nSemantic and Fluency Metrics \nDiversity Metrics \n\nSentiment Score \u2191 \nPerplexity \u2193 \nMSTTR \nDistinct1 \nDistinct2 \nH1 \nH2 \nUnique1 \nUnique2 \n\nZero-Shot \n0.489 \u00b1 0.006 \n32.371 \u00b1 0.137 0.682 \u00b1 0.001 0.042 \u00b1 0.001 0.294 \u00b1 0.001 8.656 \u00b1 0.004 13.716 \u00b1 0.003 5063 \u00b1 14.832 47620 \u00b1 238 \n\nSupervised \n0.0 (0k) \n0.489 \u00b1 0.006 \n32.371 \u00b1 0.137 0.682 \u00b1 0.001 0.042 \u00b1 0.001 0.294 \u00b1 0.001 8.656 \u00b1 0.004 13.716 \u00b1 0.003 \n5063 \u00b1 14 \n47620 \u00b1 238 \n0.1 (1k) \n0.531 \u00b1 0.005 \n34.846 \u00b1 0.123 0.685 \u00b1 0.001 0.045 \u00b1 0.001 0.313 \u00b1 0.004 8.775 \u00b1 0.023 13.854 \u00b1 0.032 \n5215 \u00b1 62 \n51125 \u00b1 685 \n0.5 (5k) \n0.536 \u00b1 0.006 \n35.008 \u00b1 0.229 0.684 \u00b1 0.001 0.047 \u00b1 0.000 0.314 \u00b1 0.002 8.764 \u00b1 0.010 13.837 \u00b1 0.0178 \n5489 \u00b1 44 \n51284 \u00b1 576 \n1.0 (10k) \n0.539 \u00b1 0.004 \n35.472 \u00b1 0.074 0.682 \u00b1 0.001 0.047 \u00b1 0.001 0.312 \u00b1 0.002 8.755 \u00b1 0.012 13.806 \u00b1 0.016 \n5601 \u00b1 57 \n51151 \u00b1 345 \nPPO \n0.0 (0k) \n0.492 \u00b1 0.01 \n33.57 \u00b1 0.323 \n0.69 \u00b1 0.02 \n0.047 \u00b1 0.001 0.321 \u00b1 0.015 8.816 \u00b1 0.149 \n13.866 \u00b1 0.36 \n5629 \u00b1 240 \n52911 \u00b1 1786 \n0.1 (2k) \n0.598 \u00b1 0.017 \n35.929 \u00b1 1.397 0.698 \u00b1 0.009 0.051 \u00b1 0.003 0.339 \u00b1 0.012 8.968 \u00b1 0.083 14.013 \u00b1 0.158 \n6173 \u00b1 360 \n55918 \u00b1 2641 \n0.5 (10k) \n0.593 \u00b1 0.026 \n35.95 \u00b1 2.177 0.666 \u00b1 0.073 0.049 \u00b1 0.003 0.314 \u00b1 0.046 8.635 \u00b1 0.634 13.432 \u00b1 1.173 \n5882 \u00b1 356 \n51403 \u00b1 9297 \n1.0 (20k) \n0.605 \u00b1 0.023 \n33.497 \u00b1 0.447 0.666 \u00b1 0.013 0.043 \u00b1 0.002 0.287 \u00b1 0.008 8.575 \u00b1 0.073 13.484 \u00b1 0.244 \n5230 \u00b1 363 \n46483 \u00b1 1318 \nNLPO \n0.0 (0k) \n0.487 \u00b1 0.01 \n32.572 \u00b1 0.165 0.685 \u00b1 0.003 0.043 \u00b1 0.001 0.299 \u00b1 0.003 8.691 \u00b1 0.023 13.787 \u00b1 0.034 \n5126 \u00b1 177 \n48475 \u00b1 491 \n0.1 (2k) \n0.599 \u00b1 0.007 \n33.536 \u00b1 0.378 \n0.67 \u00b1 0.01 \n0.043 \u00b1 0.001 0.289 \u00b1 0.009 8.608 \u00b1 0.061 13.576 \u00b1 0.192 \n5125 \u00b1 220 \n46755 \u00b1 1449 \n0.5 (10k) \n0.617 \u00b1 0.021 \n33.409 \u00b1 0.354 0.668 \u00b1 0.005 0.041 \u00b1 0.001 0.281 \u00b1 0.006 8.552 \u00b1 0.044 13.533 \u00b1 0.091 \n4926 \u00b1 183 \n45256 \u00b1 1022 \n1.0 (20k) \n0.637 \u00b1 0.013 \n32.667 \u00b1 0.631 0.677 \u00b1 0.014 0.044 \u00b1 0.002 0.288 \u00b1 0.010 8.588 \u00b1 0.100 13.484 \u00b1 0.236 \n5205 \u00b1 189 \n46344 \u00b1 2688 \n\n\n\nTable 8 :\n8Evaluation of GPT2 with different algorithms on IMDB sentiment text continuation task, NLPO hyperparameter ablations: Mean and standard deviations over 5 random seeds is reported for sentiment scores along with fluency and diversity metrics. This table shows results of NLPO's stability to the unique hyperparameters introduced in the algorithm -all other parameters held constant from the best PPO model. The number of iterations after which the masking model syncs with the policy and the top-p nucleus percentage for the mask model itself. We see that in general, the higher the top-p mask percentage, the better the performance. For target update iterations, performance is low if the mask model is not updated often enough or if it updated too often.Algorithm \nUnique N \nCoherence \nSentiment \nValue Alpha Skew Value Alpha Skew \nNLPO with KL \n27 \n3.49 \n0.196 3.497 \n3.61 \n0.2 3.601 \nNLPO without KL \n29 \n3.16 \n0.21 3.158 \n4.41 \n0.158 4.403 \nPPO without KL \n27 \n3.16 \n0.17 3.163 \n4.36 \n0.196 4.363 \nPPO with KL \n29 \n3.46 \n0.124 3.462 \n3.58 \n0.116 3.575 \nZero Shot \n28 \n3.6 \n0.162 3.591 \n3.1 \n0.13 3.097 \nSupervised \n29 \n3.51 \n0.192 3.512 \n3.43 \n0.2 3.428 \nHuman \n27 \n4.13 \n0.159 4.128 \n3.01 \n0.31 3.017 \nSupervised+PPO \n22 \n3.45 \n0.211 3.147 \n3.64 \n0.21 3.161 \nSupervised+NLPO \n22 \n3.48 \n0.181 3.226 \n3.73 \n0.22 3.047 \n\n\n\nTable 9 :\n9Results of the human subject study showing the number of participants N, average Likert scale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, and Skew. For each model a total of 100 samples were drawn randomly from the test set and rated by 3 annotators each, resulting in 300 data points per algorithm.B.3.3 HUMAN PARTICIPANT STUDY \n\n\n\nTable 10 :\n10Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 -Group 1) and corresponding p-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall p-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with p 0.05 for both coherence and sentiment.\n\n\nIf you like to read novels, check it out. NLPO-no-KL: setting makes the final product very enjoyable to watch. Highly recommended!!M Supervised: \"Dakota Clan\" and his sister, the two become friends with Ted Masters who soon finds the chance to fly back to their home land and go back to save his brother's life and his family. Theichael Bowerman, SPSA Film Festival Host, SPSA, WA (December 17, 2006), 8/17/2006 Great \nphotography, good acting, \n\n\n\nTable 11provides an in-depth summary of setting of hyperparameter values along with other implementation details.Model Params \nvalue \n\nsupervised \nbatch size: 8 \nepochs: 4 \nlearning rate: 0.00001 \nlearning rate scheduler: cosine \nweight decay: 0.01 \n\nppo/ nlpo \nsteps per update: 1280 \ntotal number of steps: 256000 \nbatch size: 64 \nepochs per update: 5 \nlearning rate: 0.000002 \nentropy coefficient: 0.01 \ninitial kl coeff: 0.001 \ntarget kl: 2.0 \ndiscount factor: 0.99 \ngae lambda: 0.95 \nclip ratio: 0.2 \nvalue function coeff: 0.5 \ntop mask ratio: 0.9 \ntarget update iterations: 20 \n\nsupervised+ ppo (or nlpo) steps per update: 1280 \ntotal number of steps: 128000 \nbatch size: 64 \nepochs per update: 5 \nlearning rate: 0.000002 \nentropy coefficient: 0.01 \ninitial kl coeff: 0.01 \ntarget kl: 1.0 \ndiscount factor: 0.99 \ngae lambda: 0.95 \nclip ratio: 0.2 \nvalue function coeff: 0.5 \ntop mask ratio: 0.9 \ntarget update iterations: 20 \n\ndecoding \nnum beams: 5 \nmin length: 5 \nmax new tokens: 20 \n\ntokenizer \npadding side: left \nmax length: 20 \n\n\n\nTable 11 :\n11CommonGen Hyperparams: Table shows a list of all hyper-parameters and their settingsPublished as a conference paper at ICLR 2023 \n\nB.4.2 RESULTS AND DISCUSSION \n\n\n\nTable 12 :\n12CommonGen test evaluation Table shows official scores obtained from CommonGen hold-out evaluation. The most important result is that RL fine-tuning on a supervised model yields better performance across most metrics especially Coverage which indicates the ratio of concepts covered in generated textsTasks \n_ \nLexical and Semantic Metrics \n\n\nTable 13 :\n13CommonGen dev evaluation: Table shows lexical, semantic and diversity metrics for best performing models found in each algorithm-reward function combinations along with best performing supervised baseline models. Generated text from these models are submitted to official CommonGen test evaluation to obtain test scores presented inTable 12Algorithm \nUnique N \nCoherence \nCommonsense \nValue Alpha Skew Value Alpha Skew \nPPO+Supervised \n25 \n4.14 \n0.073 4.137 \n4.03 \n0.137 4.023 \nNLPO+Supervised \n26 \n4.25 \n0.036 4.253 \n4.16 \n0.002 4.163 \nZero Shot \n24 \n2.15 \n0.391 2.154 \n2.29 \n0.342 2.291 \nPPO \n24 \n2.84 \n0.16 2.849 \n3.03 \n0.081 3.027 \nSupervised \n23 \n4.39 \n0.159 4.387 \n4.21 \n0.225 4.209 \nNLPO \n24 \n2 \n0.335 2.003 \n2.13 \n0.265 2.124 \n\n\n\nTable 14 :\n14Results of the human subject study showing the number of participants N, average Likert scale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, and Skew. For each model a total of 100 samples were drawn randomly from the test set and rated by 3 annotators each, resulting in 300 data points per algorithm.Group 1 \nGroup 2 \nCoherence \nCommonsense \nDiff (G2-G1) p-values Diff (G2-G1) p-values \nNLPO \nPPO \n0.847 \n0.001 \n0.897 \n0.001 \nNLPO \nSupervised \n2.397 \n0.001 \n2.083 \n0.001 \nNLPO \nNLPO+Supervised \n2.257 \n0.001 \n2.033 \n0.001 \nNLPO \nPPO+Supervised \n2.143 \n0.001 \n1.897 \n0.001 \nNLPO \nZero Shot \n0.153 \n0.515 \n0.157 \n0.624 \nPPO \nSupervised \n1.550 \n0.001 \n1.187 \n0.001 \nPPO \nNLPO+Supervised \n1.410 \n0.001 \n1.137 \n0.001 \nPPO \nPPO+Supervised \n1.297 \n0.001 \n1.000 \n0.001 \nPPO \nZero Shot \n-0.693 \n0.001 \n-0.740 \n0.001 \nSupervised \nNLPO+Supervised \n-0.140 \n0.601 \n-0.050 \n0.900 \nSupervised \nPPO+Supervised \n-0.253 \n0.050 \n-0.187 \n0.045 \nSupervised \nZero Shot \n-2.243 \n0.001 \n-1.927 \n0.001 \nNLPO+Supervised PPO+Supervised \n-0.113 \n0.008 \n-0.137 \n0.007 \nNLPO+Supervised Zero Shot \n-2.103 \n0.001 \n-1.877 \n0.001 \nPPO+Supervised \nZero Shot \n-1.990 \n0.001 \n-1.740 \n0.001 \n\n\n\nTable 15 :\n15Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 -Group 1) and corresponding p-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall p-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with p 0.05 for both coherence and sentiment.\n\nSample 2\n2Prompt: generate a sentence with: backhand competition player reach shuttlecock Zero-Shot: backhand competition player reach shuttlecock. PPO: the ball in the backhand and the competition player in the shuttlecock. NLPO: backhanded competition player reaches shuttlecock. Supervised: tennis player reaches for a shuttlecock during a competition. Supervised + PPO: football player reaches for a shuttlecock in backhand during the competition.As a representative of the summarization task, we consider CNN/DM dataset consisting of long news articles and their highlights written by news authors. The dataset consists of 287k training, 13k validation and 11k test examples. We trained RL methods using 3 different automated metrics, namely Rouge-1, Rouge-avg and Meteor. We chose T5 as our base LM as it is pre-trained in a unified text-to-text framework and relishes Zero-Shot capabilities. For decoding, we use multinomial sampling with a temperature of 0.7 for all the models.Supervised + NLPO: football player reaches for a shuttlecock after hitting a backhand during \nthe competition. \n\nSample 3 \nPrompt: generate a sentence with: bowl combine cucumber onion pasta \nZero-Shot: cucumber onion pasta. \nPPO: the cucumber onion pasta in a small bowl. \nNLPO: the in a bowl Combine cucumber onion pasta in a large bowl. Toss pasta \nSupervised: A bowl containing pasta, tomatoes, cucumbers, and onions. \nSupervised + PPO: A bowl containing pasta topped with cucumbers, onions, and peppers. \nSupervised + NLPO: A bowl containing a mixture of pasta, cucumber, and onion. \nB.5 CNN DAILY MAIL \n\nB.5.1 SETUP \n\nModel Params \nvalue \n\nsupervised \nbatch size: 16 \nepochs: 2 \nlearning rate: 0.0001 \nlearning rate scheduler: cosine \nweight decay: 0.1 \n\nppo/ nlpo \nsteps per update: 5120 \ntotal number of steps: 512000 \nbatch size: 64 \nepochs per update: 5 \nlearning rate: 0.000002 \nentropy coefficient: 0.0 \ninitial kl coeff: 0.001 \ntarget kl: 0.2 \ndiscount factor: 0.99 \ngae lambda: 0.95 \nclip ratio: 0.2 \nvalue function coeff: 0.5 \nrollouts top k: sweep of (50,100) \ntop mask ratio: 0.9 \ntarget update iterations: sweep of (10, 20, 30) \n\nsupervised+ppo/ nlpo steps per update: 5120 \ntotal number of steps: 256000 \nbatch size: 64 \nepochs per update: 5 \nlearning rate: 0.000002 \nentropy coefficient: 0.0 \ninitial kl coeff: 0.01 \ntarget kl: 0.2 \ndiscount factor: 0.99 \ngae lambda: 0.95 \nclip ratio: 0.2 \nvalue function coeff: 0.5 \nrollouts top k: sweep of (50,100) \ntop mask ratio: 0.9 \ntarget update iterations: sweep of (10, 20, 30) \n\ndecoding \nsampling: True \ntemperature: 0.7 \nmin length: 50 \nmax new tokens: 100 \n\ntokenizer \npadding side: left \ntruncation side: right \nmax length: 512 \n\n\n\nTable 16 :\n16CNN/DM Hyperparams: Table shows a list of all hyper-parameters and their settings B.5.2 RESULTS AND DISCUSSIONTable 17presents benchmarking results on test set reporting a wide range of metrics: lexical, semantic, factual correctness and diversity metrics. As baselines, we report lead-3 which selects first three sentences as the summary, Zero-Shot and a supervised model. PPO and NLPO models are on par with supervised performance on several metrics including Rouge-2, Rouge-L, and Bleu. On fine-tuning on top of supervised model, performance improves consistently on all metrics indicating that RL fine-tuning is beneficial. Another interesting finding is that, RL fine-tuned models are factually consistent as measured by SummaCZS metric. For ablations on PPO params, NLPO params, we refer toTables 18,19.Tasks \n_ \nLexical and Semantic Metrics \nFactual Consistency \nDiversity Metrics \nAlg \nReward Function LM Rouge-1 Rouge-2 Rouge-L Rouge-LSum Meteor BLEU BertScore \nSummaCZS \nMSTTR Distinct1 Distinct2 \nH1 \nH2 \nUnique1 Unique2 Mean Output Length \n\nCNN/DM \n\nLead-3 \n0.401 \n0.175 \n0.250 \n0.363 \n0.333 \n0.099 \n0.874 \n0.993 \n0.750 \n0.0482 \n0.386 \n10.481 16.631 \n21465 \n273153 \n84 \n\nZero-Shot \nT5 \n0.372 \n0.145 \n0.247 \n0.311 \n0.256 \n0.077 \n0.864 \n0.654 \n0.725 \n0.061 \n0.414 \n10.285 16.183 \n19113 \n193999 \n55 \n\nPPO \nRouge-1 \nT5 \n0.410 \n0.182 \n0.283 \n0.349 \n0.276 \n0.095 \n0.876 \n0.622 \n0.760 \n0.068 \n0.464 \n10.661 16.437 \n18189 \n191383 \n47 \n\nRouge-Avg \nT5 \n0.396 \n0.176 \n0.273 \n0.338 \n0.270 \n0.095 \n0.874 \n0.622 \n0.773 \n0.071 \n0.490 \n10.830 16.664 \n19478 \n209140 \n48 \n\nMeteor \nT5 \n0.408 \n0.178 \n0.276 \n0.342 \n0.301 \n0.109 \n0.873 \n0.527 \n0.765 \n0.060 \n0.447 \n10.699 16.688 \n20528 \n234386 \n61 \n\nNLPO \nRouge-1 \nT5 \n0.404 \n0.180 \n0.278 \n0.344 \n0.275 \n0.096 \n0.875 \n0.636 \n0.771 \n0.069 \n0.480 \n10.789 16.618 \n18677 \n201971 \n48 \n\nRouge-Avg \nT5 \n0.404 \n0.177 \n0.279 \n0.344 \n0.274 \n0.094 \n0.874 \n0.586 \n0.765 \n0.066 \n0.476 \n10.744 16.620 \n18179 \n206368 \n50 \n\nMeteor \nT5 \n0.405 \n0.180 \n0.277 \n0.343 \n0.292 \n0.108 \n0.872 \n0.578 \n0.772 \n0.064 \n0.471 \n10.802 16.766 \n20212 \n231038 \n56 \n\nSupervised \nT5 \n0.411 \n0.177 \n0.276 \n0.343 \n0.309 \n0.108 \n0.876 \n0.654 \n0.727 \n0.057 \n0.401 \n10.459 16.410 \n21096 \n230343 \n68 \n\nSupervised + PPO \nRouge-1 \nT5 \n0.417 \n0.189 \n0.294 \n0.358 \n0.278 \n0.101 \n0.882 \n0.722 \n0.750 \n0.070 \n0.459 \n10.595 16.389 \n18184 \n184220 \n46 \n\nRouge-Avg \nT5 \n0.425 \n0.194 \n0.297 \n0.363 \n0.296 \n0.114 \n0.882 \n0.728 \n0.747 \n0.066 \n0.445 \n10.589 16.458 \n18939 \n200617 \n52 \n\nMeteor \nT5 \n0.426 \n0.194 \n0.293 \n0.361 \n0.316 \n0.125 \n0.880 \n0.726 \n0.741 \n0.059 \n0.420 \n10.532 16.491 \n20395 \n224432 \n63 \n\nSupervised + NLPO \nRouge-1 \nT5 \n0.421 \n0.193 \n0.297 \n0.361 \n0.287 \n0.108 \n0.882 \n0.740 \n0.748 \n0.067 \n0.446 \n10.528 16.313 \n18204 \n185561 \n48 \n\nRouge-Avg \nT5 \n0.424 \n0.193 \n0.296 \n0.363 \n0.295 \n0.115 \n0.882 \n0.743 \n0.744 \n0.065 \n0.443 \n10.570 16.444 \n18747 \n201705 \n53 \n\nMeteor \nT5 \n0.429 \n0.194 \n0.293 \n0.361 \n0.319 \n0.124 \n0.880 \n0.743 \n0.745 \n0.059 \n0.422 \n10.574 16.516 \n20358 \n226801 \n63 \n\n\n\nTable 17 :\n17CNN/Daily Mail test evaluation: Table presents a wide range of metrics: lexical, semantic, factual correctness and diversity metrics on test set. As baselines, we report lead-3 which selects first three sentences as the summary, Zero-Shot and a supervised model. PPO and NLPO models are on par with supervised performance on several metrics including Rouge-2, Rouge-L, and Bleu. On fine-tuning on top of supervised model, performance improves consistently on all metrics indicating that RL fine-tuning is beneficial. Another interesting finding is that, RL fine-tuned models are factually consistent as measured by SummaCZS metric._ \nLexical and Semantic Metrics \nAlg \nReward Function Top k Rouge-1 Rouge-2 Rouge-L Rouge-LSum Meteor BLEU BertScore \n\nPPO \n\nRouge-1 \n50 \n0.404 \n0.181 \n0.280 \n0.346 \n0.273 \n0.095 \n0.874 \n100 \n0.412 \n0.186 \n0.286 \n0.354 \n0.276 \n0.094 \n0.876 \n\nRouge-Avg \n50 \n0.401 \n0.177 \n0.276 \n0.342 \n0.271 \n0.092 \n0.873 \n100 \n0.399 \n0.179 \n0.275 \n0.342 \n0.270 \n0.094 \n0.874 \n\nMeteor \n50 \n0.413 \n0.182 \n0.279 \n0.348 \n0.301 \n0.110 \n0.873 \n100 \n0.409 \n0.179 \n0.276 \n0.345 \n0.296 \n0.108 \n0.871 \n\nSupervised+PPO \n\nRouge-1 \n50 \n0.414 \n0.190 \n0.293 \n0.358 \n0.272 \n0.097 \n0.881 \n100 \n0.420 \n0.193 \n0.295 \n0.362 \n0.277 \n0.100 \n0.881 \n\nRouge-Avg \n50 \n0.426 \n0.196 \n0.298 \n0.366 \n0.294 \n0.114 \n0.881 \n100 \n0.427 \n0.196 \n0.298 \n0.366 \n0.294 \n0.113 \n0.881 \n\nMeteor \n50 \n0.429 \n0.197 \n0.297 \n0.367 \n0.306 \n0.122 \n0.881 \n100 \n0.432 \n0.199 \n0.297 \n0.367 \n0.317 \n0.131 \n0.879 \n\n\n\nTable 18 :\n18PPO Ablation/Model Selection: Evaluation of PPO models on validation set with different reward functions and top k values for rollouts. For each alg-reward combo, best model (top k ) is chosen._ \nLexical and Semantic Metrics \nAlg \nReward Function Top k (rollout) Top p (Action mask) target update niters Rouge-1 Rouge-2 Rouge-L Rouge-LSum Meteor BLEU BertScore \n\nNLPO \n\nRouge-1 \n50 \n0.9 \n10 \n0.400 \n0.178 \n0.275 \n0.343 \n0.269 \n0.094 \n0.872 \n20 \n0.396 \n0.173 \n0.274 \n0.340 \n0.257 \n0.082 \n0.873 \n30 \n0.396 \n0.174 \n0.273 \n0.339 \n0.265 \n0.091 \n0.872 \n100 \n0.9 \n10 \n0.407 \n0.177 \n0.279 \n0.347 \n0.265 \n0.085 \n0.875 \n20 \n0.406 \n0.182 \n0.281 \n0.347 \n0.273 \n0.094 \n0.874 \n30 \n0.405 \n0.180 \n0.279 \n0.347 \n0.269 \n0.091 \n0.875 \n\nRouge-Avg \n50 \n0.9 \n10 \n0.400 \n0.180 \n0.276 \n0.343 \n0.271 \n0.096 \n0.873 \n20 \n0.349 \n0.147 \n0.241 \n0.298 \n0.237 \n0.078 \n0.858 \n30 \n0.393 \n0.173 \n0.272 \n0.336 \n0.267 \n0.092 \n0.870 \n100 \n0.9 \n10 \n0.396 \n0.174 \n0.274 \n0.339 \n0.265 \n0.088 \n0.872 \n20 \n0.406 \n0.179 \n0.280 \n0.347 \n0.272 \n0.092 \n0.874 \n30 \n0.400 \n0.178 \n0.279 \n0.344 \n0.266 \n0.087 \n0.874 \n\nMeteor \n50 \n0.9 \n10 \n0.404 \n0.177 \n0.274 \n0.343 \n0.286 \n0.102 \n0.872 \n20 \n0.406 \n0.180 \n0.276 \n0.343 \n0.292 \n0.107 \n0.871 \n30 \n0.401 \n0.172 \n0.271 \n0.337 \n0.288 \n0.099 \n0.870 \n100 \n0.9 \n10 \n0.405 \n0.178 \n0.276 \n0.343 \n0.294 \n0.107 \n0.870 \n20 \n0.406 \n0.176 \n0.276 \n0.343 \n0.291 \n0.106 \n0.872 \n30 \n0.409 \n0.184 \n0.280 \n0.348 \n0.291 \n0.108 \n0.873 \n\nSupervised + NLPO \n\nRouge-1 \n50 \n0.9 \n10 \n0.425 \n0.196 \n0.299 \n0.366 \n0.285 \n0.106 \n0.882 \n20 \n0.417 \n0.191 \n0.295 \n0.360 \n0.276 \n0.100 \n0.881 \n30 \n0.418 \n0.192 \n0.296 \n0.361 \n0.278 \n0.101 \n0.881 \n100 \n0.9 \n10 \n0.424 \n0.196 \n0.299 \n0.366 \n0.286 \n0.106 \n0.882 \n20 \n0.423 \n0.196 \n0.299 \n0.365 \n0.289 \n0.110 \n0.881 \n30 \n0.420 \n0.193 \n0.296 \n0.362 \n0.279 \n0.102 \n0.881 \n\nRouge-Avg \n50 \n0.9 \n10 \n0.426 \n0.197 \n0.298 \n0.367 \n0.294 \n0.115 \n0.881 \n20 \n0.425 \n0.196 \n0.298 \n0.366 \n0.292 \n0.112 \n0.881 \n30 \n0.424 \n0.194 \n0.297 \n0.365 \n0.287 \n0.107 \n0.881 \n100 \n0.9 \n10 \n0.424 \n0.196 \n0.298 \n0.365 \n0.291 \n0.113 \n0.881 \n20 \n0.428 \n0.198 \n0.300 \n0.368 \n0.296 \n0.115 \n0.882 \n30 \n0.429 \n0.199 \n0.300 \n0.369 \n0.296 \n0.116 \n0.882 \n\nMeteor \n50 \n0.9 \n10 \n0.430 \n0.197 \n0.294 \n0.364 \n0.320 \n0.130 \n0.879 \n20 \n0.432 \n0.198 \n0.297 \n0.367 \n0.318 \n0.130 \n0.880 \n30 \n0.423 \n0.191 \n0.293 \n0.361 \n0.297 \n0.116 \n0.879 \n100 \n0.9 \n10 \n0.435 \n0.200 \n0.298 \n0.369 \n0.320 \n0.131 \n0.881 \n20 \n0.433 \n0.198 \n0.297 \n0.368 \n0.319 \n0.130 \n0.879 \n30 \n0.434 \n0.200 \n0.297 \n0.369 \n0.324 \n0.132 \n0.879 \n\n\n\nTable 19 :\n19NLPO Ablation/Model Selection: Evaluation of NLPO models on validation set with different reward functions, top k values for rollouts and target update iterations. For each alg-reward combo, best model is chosenAlgorithm \nUnique N \nCoherence \nQuality \nValue Alpha Skew Value Alpha Skew \nPPO+Supervised \n22 \n4.21 \n0.198 4.224 \n3.97 \n0.256 \n3.98 \nNLPO+Supervised \n19 \n4.3 \n0.26 4.308 \n3.98 \n0.089 \n4 \nZero Shot \n17 \n3.73 \n0.1 3.757 \n3.69 \n0.25 3.722 \nSupervised \n19 \n4.25 \n0.116 4.241 \n3.99 \n0.2 3.986 \nNLPO \n17 \n4.03 \n0.13 4.042 \n3.83 \n0.191 3.832 \nPPO \n21 \n3.94 \n0.111 3.945 \n3.76 \n0.129 3.767 \nHuman \n19 \n3.89 \n0.277 3.902 \n3.77 \n0.029 3.769 \n\n\n\nTable 20 :\n20Results of the human subject study showing the number of participants N, average Likert \nscale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, \nand Skew. For each model a total of 50 samples were drawn randomly from the test set and rated by \n3 annotators each, each resulting in 150 data points per algorithm. \n\nCoherence \nQuality \nGroup 1 \nGroup 2 \nDiff (G2-G1) p-values Diff (G2-G1) p-values \nHuman \nNLPO \n0.147 \n0.755 \n0.060 \n0.900 \nHuman \nNLPO+Supervised \n0.413 \n0.001 \n0.213 \n0.047 \nHuman \nPPO \n0.053 \n0.900 \n-0.007 \n0.900 \nHuman \nPPO+Supervised \n0.327 \n0.024 \n0.200 \n0.544 \nHuman \nSupervised \n0.360 \n0.008 \n0.220 \n0.043 \nHuman \nZero Shot \n-0.160 \n0.679 \n-0.080 \n0.900 \nNLPO \nNLPO+Supervised \n0.267 \n0.012 \n0.153 \n0.008 \nNLPO \nPPO \n-0.093 \n0.900 \n-0.067 \n0.900 \nNLPO \nPPO+Supervised \n0.180 \n0.564 \n0.140 \n0.860 \nNLPO \nSupervised \n0.213 \n0.361 \n0.160 \n0.754 \nNLPO \nZero Shot \n-0.307 \n0.044 \n-0.140 \n0.860 \nNLPO+Supervised PPO \n-0.360 \n0.008 \n-0.220 \n0.043 \nNLPO+Supervised PPO+Supervised \n-0.087 \n0.009 \n-0.013 \n0.009 \nNLPO+Supervised Supervised \n-0.053 \n0.009 \n0.007 \n0.900 \nNLPO+Supervised Zero Shot \n-0.573 \n0.001 \n-0.293 \n0.012 \nPPO \nPPO+Supervised \n0.273 \n0.106 \n0.207 \n0.508 \nPPO \nSupervised \n0.307 \n0.044 \n0.227 \n0.394 \nPPO \nZero Shot \n-0.213 \n0.361 \n-0.073 \n0.900 \nPPO+Supervised \nSupervised \n0.033 \n0.900 \n0.020 \n0.900 \nPPO+Supervised \nZero Shot \n-0.487 \n0.001 \n-0.280 \n0.155 \nSupervised \nZero Shot \n-0.520 \n0.001 \n-0.300 \n0.101 \n\n\n\nTable 21 :\n21Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms (Group 2 -Group 1) and corresponding p-values. Individually statistically significant results are bolded and are used to discuss results in the analysis. Overall p-values showing that there is a significant difference in means between the models via a one-way ANOVA test are significant with p 0.05 for both coherence and sentiment.\n\nTable 22 :\n22ToTTO Hyperparams: Table shows a list of all hyper-parameters and their settingsB.6.2 RESULTS AND DISCUSSION \n\n\n\nTable 23 :\n23ToTTo test evaluation: Table shows lexical, semantic and factual correctness metric scores of algorithms with different reward functions on hold-out test set. Without supervised pre-training, both PPO and NLPO results in sub-optimal solutions, with NLPO better than PPO. With supervised pre-training, PPO and NLPO achieve better scores across all metrics showing RL fine-tuning is beneficial. Most importantly, RL fine-tuned models produce more factually consistent text as seen in higher PARENT scores. Another observation, fine-tuning with a task-specific metric PARENT is better than training on task-agnostic lexical rewardsTasks \n_ \nLexical and Semantic Metrics \nFactual Consistency \nDiversity Metrics \nAlg \nLM Reward function Rouge-1 Rouge-2 Rouge-L Rouge-LSum Meteor BertScore \nSacreBleu \nPARENT \n\nOverall Overlap Non-Overlap Overall Overlap Non-Overlap MSTTR Distinct1 Distinct2 \nH1 \nH2 \nUnique1 Unique2 Mean Output Length \n\nToTTo \n\nZero-Shot \nT5 \n0.131 \n0.055 \n0.127 \n0.127 \n0.057 \n0.805 \n0.038 \n0.042 \n0.034 \n0.118 \n0.119 \n0.116 \n0.428 \n0.084 \n0.238 \n6.703 \n9.933 \n8387 \n26490 \n19.964 \n\nSupervised \nT5 \n0.410 \n0.279 \n0.388 \n0.388 \n0.223 \n0.953 \n0.458 \n0.533 \n0.387 \n0.586 \n0.633 \n0.540 \n0.715 \n0.162 \n0.511 \n9.995 14.468 \n15168 \n54706 \n17.791 \n\nPPO \nT5 \nbleu \n0.274 \n0.138 \n0.249 \n0.249 \n0.139 \n0.844 \n0.068 \n0.071 \n0.066 \n0.251 \n0.250 \n0.251 \n0.403 \n0.091 \n0.308 \n10.659 14.511 \n7536 \n34232 \n28.545 \nT5 \nsacrebleu \n0.341 \n0.166 \n0.300 \n0.300 \n0.165 \n0.858 \n0.09 \n0.094 \n0.086 \n0.300 \n0.299 \n0.300 \n0.469 \n0.121 \n0.407 \n11.071 14.880 \n10138 \n48195 \n26.612 \nT5 \nmeteor \n0.322 \n0.157 \n0.286 \n0.286 \n0.173 \n0.888 \n0.147 \n0.163 \n0.133 \n0.358 \n0.367 \n0.350 \n0.625 \n0.136 \n0.482 \n10.189 14.910 \n12346 \n54925 \n21.484 \nT5 \nparent \n0.268 \n0.125 \n0.251 \n0.251 \n0.119 \n0.890 \n0.150 \n0.158 \n0.143 \n0.337 \n0.332 \n0.342 \n0.764 \n0.202 \n0.646 \n11.068 14.988 \n13068 \n50313 \n13.035 \nT5 \nmeteor + parent \n0.266 \n0.128 \n0.251 \n0.251 \n0.130 \n0.886 \n0.165 \n0.175 \n0.155 \n0.348 \n0.346 \n0.350 \n0.702 \n0.181 \n0.594 \n10.096 14.432 \n14422 \n55770 \n15.354 \n\nNLPO \nT5 \nbleu \n0.267 \n0.134 \n0.24 \n0.24 \n0.137 \n0.84 \n0.068 \n0.071 \n0.065 \n0.238 \n0.239 \n0.237 \n0.448 \n0.1 \n0.359 \n11.259 14.623 \n9029 \n47209 \n28.472 \nT5 \nsacrebleu \n0.341 \n0.168 \n0.297 \n0.297 \n0.183 \n0.863 \n0.089 \n0.093 \n0.085 \n0.32 \n0.324 \n0.317 \n0.494 \n0.111 \n0.373 \n11.007 15.032 \n9455 \n43379 \n27.977 \nT5 \nmeteor \n0.322 \n0.157 \n0.286 \n0.286 \n0.173 \n0.888 \n0.147 \n0.163 \n0.133 \n0.358 \n0.367 \n0.350 \n0.625 \n0.136 \n0.482 \n10.189 14.910 \n12346 \n54925 \n21.484 \nT5 \nparent \n0.283 \n0.132 \n0.264 \n0.264 \n0.133 \n0.894 \n0.163 \n0.174 \n0.153 \n0.36 \n0.357 \n0.364 \n0.824 \n0.223 \n0.691 \n11.493 15.127 \n14344 \n55542 \n14.204 \nT5 \nmeteor + parent \n0.299 \n0.14 \n0.276 \n0.276 \n0.142 \n0.896 \n0.171 \n0.181 \n0.161 \n0.369 \n0.365 \n0.372 \n0.779 \n0.214 \n0.674 \n11.072 15.275 \n14939 \n58737 \n15.141 \n\nSupervised + PPO \nT5 \nbleu \n0.408 \n0.283 \n0.388 \n0.388 \n0.222 \n0.954 \n0.477 \n0.549 \n0.405 \n0.596 \n0.644 \n0.550 \n0.722 \n0.167 \n0.525 \n10.080 14.524 \n15203 \n54724 \n17.296 \nT5 \nsacrebleu \n0.395 \n0.275 \n0.378 \n0.378 \n0.211 \n0.955 \n0.477 \n0.554 \n0.401 \n0.577 \n0.621 \n0.535 \n0.728 \n0.174 \n0.539 \n10.086 14.518 \n14846 \n52327 \n16.063 \nT5 \nmeteor \n0.410 \n0.282 \n0.389 \n0.389 \n0.223 \n0.954 \n0.469 \n0.540 \n0.398 \n0.593 \n0.642 \n0.547 \n0.718 \n0.165 \n0.516 \n10.037 14.467 15182.0 \n54446 \n17.542 \nT5 \nparent \n0.401 \n0.277 \n0.382 \n0.382 \n0.215 \n0.953 \n0.470 \n0.543 \n0.394 \n0.598 \n0.647 \n0.550 \n0.732 \n0.174 \n0.545 \n10.209 14.660 15379.0 \n55421 \n16.826 \nT5 \nmeteor + parent \n0.406 \n0.281 \n0.386 \n0.387 \n0.220 \n0.954 \n0.473 \n0.544 \n0.399 \n0.600 \n0.648 \n0.553 \n0.727 \n0.170 \n0.532 \n10.143 14.586 \n15330 \n55211 \n17.185 \n\nSupervised + NLPO T5 \nbleu \n0.410 \n0.283 \n0.388 \n0.388 \n0.222 \n0.954 \n0.476 \n0.548 \n0.404 \n0.597 \n0.644 \n0.552 \n0.721 \n0.167 \n0.524 \n10.077 14.532 \n15213 \n54948 \n17.408 \nT5 \nsacrebleu \n0.397 \n0.276 \n0.38 \n0.38 \n0.214 \n0.955 \n0.477 \n0.555 \n0.401 \n0.581 \n0.628 \n0.535 \n0.729 \n0.174 \n0.54 \n10.124 14.544 \n14940 \n52986 \n16.334 \nT5 \nmeteor \n0.411 \n0.283 \n0.389 \n0.39 \n0.224 \n0.954 \n0.474 \n0.547 \n0.403 \n0.6 \n0.649 \n0.554 \n0.727 \n0.171 \n0.536 \n10.156 14.612 \n15341 \n55292 \n17.637 \nT5 \nparent \n0.405 \n0.28 \n0.386 \n0.386 \n0.219 \n0.954 \n0.469 \n0.541 \n0.398 \n0.598 \n0.645 \n0.552 \n0.716 \n0.165 \n0.519 \n10.019 \n14.5 \n15218 \n54793 \n17.095 \nT5 \nmeteor + parent \n0.405 \n0.28 \n0.386 \n0.386 \n0.219 \n0.954 \n0.474 \n0.547 \n0.398 \n0.598 \n0.646 \n0.552 \n0.727 \n0.171 \n0.536 \n10.156 14.612 \n15341 \n55292 \n17.095 \n\n\n\nTable 24 :\n24ToTTo dev evaluation: Table shows lexical, semantic and factual correctness metric scores of algorithms with different reward functions on dev set. Without supervised pre-training, both PPO and NLPO results in sub-optimal solutions, with NLPO better than PPO. With supervised pre-training, PPO and NLPO achieve better scores across all metrics showing RL fine-tuning is beneficial. Most importantly, RL fine-tuned models produce more factually correct text as seen in higher PARENT scores. Another observation, fine-tuning with a task-specific metric PARENT is better than training just on task-agnostic lexical metricsAlgorithm \nUnique N \nCoherence \nCorrectness \nValue Alpha Skew Value Alpha Skew \nZero Shot \n25 \n1.63 \n0.718 1.642 \n1.93 \n0.503 1.946 \nPPO+Supervised \n24 \n4.57 \n0.221 4.579 \n4.48 \n0.098 4.483 \nPPO \n26 \n2.75 \n0.427 2.753 \n3.23 \n0.214 3.227 \nNLPO \n28 \n2.25 \n0.401 2.247 \n2.61 \n0.419 2.613 \nSupervised \n24 \n4.59 \n0.173 4.592 \n4.54 \n0.189 4.537 \nNLPO+Supervised \n26 \n4.58 \n0.244 4.601 \n4.57 \n0.144 4.581 \n\n\n\nTable 25 :\n25Results of the human subject study showing the number of participants N, average Likert \nscale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, \nand Skew. For each model a total of 50 samples were drawn randomly from the test set and rated by \n3 annotators each, resulting in 150 data points per algorithm. \n\nGroup 1 \nGroup 2 \nCoherence \nCorrectness \nDiff (G2-G1) p-values Diff (G2-G1) p-values \nPPO \nNLPO \n-0.507 \n0.001 \n-0.613 \n0.001 \nPPO \nNLPO+Supervised \n1.827 \n0.001 \n1.340 \n0.001 \nPPO \nSupervised \n1.833 \n0.001 \n1.313 \n0.001 \nPPO \nPPO+Supervised \n1.813 \n0.001 \n1.253 \n0.001 \nPPO \nZero Shot \n-1.120 \n0.001 \n-1.293 \n0.001 \nNLPO \nNLPO+Supervised \n2.333 \n0.001 \n1.953 \n0.001 \nNLPO \nSupervised \n2.340 \n0.001 \n1.927 \n0.001 \nNLPO \nPPO+Supervised \n2.320 \n0.001 \n1.867 \n0.001 \nNLPO \nZero Shot \n-0.613 \n0.001 \n-0.680 \n0.001 \nNLPO+Supervised Supervised \n0.007 \n0.9 \n-0.027 \n0.009 \nNLPO+Supervised PPO+Supervised \n-0.013 \n0.009 \n-0.087 \n0.009 \nNLPO+Supervised Zero Shot \n-2.947 \n0.001 \n-2.633 \n0.001 \nSupervised \nPPO+Supervised \n-0.020 \n0.009 \n-0.060 \n0.009 \nSupervised \nZero Shot \n-2.953 \n0.001 \n-2.607 \n0.001 \nPPO+Supervised \nZero Shot \n-2.933 \n0.001 \n-2.547 \n0.001 \n\n\n\nTable 26 :\n26Results of an post-hoc Tukey HSD Test for difference in means between pairs of algorithms \n(Group 2 -Group 1) and corresponding p-values. Individually statistically significant results are \nbolded and are used to discuss results in the analysis. Overall p-values showing that there is a \nsignificant difference in means between the models via a one-way ANOVA test are significant with \np \n0.05 for both coherence and sentiment. \n\n\n\nZero-Shot: table> cell> Under My Spell Tour /section_title> Tour dates /cell> cell> August 1, 1992 col_header> Date /col_header> PPO: Under My Spell Tour, August 1, 1992. August 1, 1992. NLPO: Under My Spell Tour dates Under My Spell Tour dates Allentown Fairgrounds Grandstand August 1, 1992. Under My Spell Tour dates Under My Spell Tour dates Allentown Fairgrounds Grandstand August 1, 1992 table> Supervised: The Under My Spell Tour began on August 1, 1992 at the Allentown Fairgrounds Grandstand in Allentown. Supervised + PPO: On August 1, 1992, Under My Spell Tour was held at the Allentown Fairgrounds Grandstand in Allentown. Supervised + NLPO: On August 1, 1992, Under My Spell Tour was held at the Allentown Fairgrounds Grandstand in Allentown. Shot: > </table><page_title> List of French monarchs < PPO: Philip IV the Fair, the Iron King (Philippe) 5 October 1285. Title: King of France and of Navarre (Roi de France et de Navarre NLPO: King (Philippe) Philip IV the Fair, the Iron King (Philippe) 5 October 1285 Supervised: Philip IV the Fair, the Iron King (Philippe) was King of France and Navarre from 1285. Supervised + PPO: Philip IV the Fair, the Iron King was King of France and Navarre from 1285. Supervised + NLPO: Philip IV the Fair, the Iron King (Philippe) was King of France and Navarre (Roi de France et de Navarre) in 1285. Ted Kennedy (D) succeeding November 6, 1962 as Vacator of the 87th United States Congress. NLPO: Ted Kennedy (D) Successor elected November 6, 1962. table> cell Benjamin A. Smith II (D) col_header Reason for change: Date of successor's formal installation November 7, 1962. Supervised: Benjamin A. Smith II (D) served as senate until November 6, 1962 which was later served by Ted Kennedy (D) from November 7, 1962. Supervised + PPO: Benjamin A. Smith II (D) served until November 6, 1962 and Ted Kennedy (D) succeeded him from November 7, 1962. Supervised + NLPO: Benjamin A. Smith II (D) served until November 6, 1962 and Ted Kennedy (D) succeeded him from November 7, 1962.NarrativeQA(Ko\u010disk\u1ef3 et al., 2018)  deals with task of generating answers to questions about a given story. For training RL methods, we consider 2 traditional lexical rewards namely Rouge Combined and Rouge-L-Max. We chose T5-base as the base LM since it has been shown to do well at question answering in prior work(Khashabi et al., 2020). We note that the supervised models we use are trained on the UnifiedQA dataset, which contains other QA datasets, and is shown byKhashabi et al.  (2020)  to outperform supervised fine-tuning only on NarrativeQA. Hyperparams for our models can be found inTable 27.Sample 2 \nPrompt: <table> <cell> Philip IV the Fair, the Iron King (Philippe) <col_header> Name </ \ncol_header> </cell> <cell> 5 October 1285 <col_header> King from </col_header> </cell> < \ncell> King of France and of Navarre (Roi de France et de Navarre) <col_header> Title </ \ncol_header> </cell> </table><page_title> List of French monarchs </page_title> < \nsection_title> House of Capet (987\u00e2\u0202\u015e1328) </section_title> <table> <cell> Philip IV the \nFair, the Iron King (Philippe) <col_header> Name </col_header> </cell> <cell> 5 October \n1285 <col_header> King from </col_header> </cell> <cell> King of France and of Navarre ( \nRoi de France et de Navarre) <col_header> Title </col_header> </cell> </table> \nZero-Sample 3 \nPrompt: <table> <cell> Benjamin A. Smith II (D) <col_header> Vacator </col_header> </cell> < \ncell> Successor elected November 6, 1962. <col_header> Reason for change </col_header> </ \ncell> <cell> Ted Kennedy (D) <col_header> Successor </col_header> </cell> <cell> November \n7, 1962 <col_header> Date of successor's formal installation </col_header> </cell> </ \ntable><page_title> 87th United States Congress </page_title> <section_title> Senate </ \nsection_title> <table> <cell> Benjamin A. Smith II (D) <col_header> Vacator </col_header> \n</cell> <cell> Successor elected November 6, 1962. <col_header> Reason for change </ \ncol_header> </cell> <cell> Ted Kennedy (D) <col_header> Successor </col_header> </cell> < \ncell> November 7, 1962 <col_header> Date of successor's formal installation </col_header> \n</cell> </table> \nZero-Shot: cell> Ted Kennedy (D) col_header> Vacator /col_header> /cell> cell> November 7, \n1962 col_header> Reason for change \nPPO: B.7 NARRATIVE QA \n\nB.7.1 SETUP \n\nModel Params \nvalue \n\nppo/nlpo \nsteps per update: 5120 \ntotal number of steps: 512000 \nbatch size: 64 \nepochs per update: 5 \nlearning rate: 0.000002 \nentropy coefficient: 0.0 \ninitial kl coeff: 0.001 \ntarget kl: 1.0 \ndiscount factor: 0.99 \ngae lambda: 0.95 \nclip ratio: 0.2 \nrollouts top k : 50 \nvalue function coeff: 0.5 \ntop mask ratio: 0.9 \ntarget update iterations: 20 \n\nsupervised+ ppo (or nlpo) steps per update:2560 \ntotal number of steps: 512000 \nbatch size: 64 \nepochs per update: 5 \nlearning rate: 0.0000005 \nentropy coefficient: 0.0 \ninitial kl coeff: 0.001 \ntarget kl: 0.2 \ndiscount factor: 0.99 \ngae lambda: 0.95 \nclip ratio: 0.2 \nrollouts top k : 50 \nvalue function coeff: 0.5 \ntop mask ratio: 0.9 \ntarget update iterations: 20 \n\ndecoding \nnum beams: 4 \nmax new tokens: 50 \n\ntokenizer \npadding side: left \ntruncation side: right \nmax length: 512 \n\n\n\nTable 27 :\n27NarQA Hyperparams: Table shows a list of all hyper-parameters and their settingsTasks \nLexical and Semantic Metrics \nDiversity Metrics \nAlg \nReward Function LM Rouge-1 Rouge-2 Rouge-L Rouge-LSum Rouge-LMax Meteor BLEU BertScore MSTTR Distinct1 Distinct2 \nH1 \nH2 \nUnique1 Unique2 Mean Output Length \n\nNarQA \n\nZero Shot \nT5 \n0.095 \n0.022 \n0.084 \n0.084 \n0.117 \n0.095 \n0.009 \n0.835 \n0.415 \n0.026 \n0.097 \n9.641 13.468 \n1880 \n11495 \n31.688 \n\nPPO \nRouge Combined T5 \n0.101 \n0.025 \n0.088 \n0.088 \n0.122 \n0.099 \n0.01 \n0.837 \n0.462 \n0.03 \n0.125 \n9.759 13.789 \n2522 \n17806 \n32.352 \nRouge-L Max \nT5 \n0.099 \n0.025 \n0.087 \n0.087 \n0.122 \n0.099 \n0.01 \n0.835 \n0.439 \n0.029 \n0.119 \n9.653 13.618 \n2292 \n15816 \n31.479 \n\nNLPO \nRouge Combined T5 \n0.097 \n0.023 \n0.085 \n0.085 \n0.118 \n0.098 \n0.009 \n0.836 \n0.418 \n0.025 \n0.096 \n9.652 13.528 \n1816 \n10980 \n32.117 \nRouge-L Max \nT5 \n0.102 \n0.026 \n0.089 \n0.089 \n0.124 \n0.1 \n0.01 \n0.837 \n0.445 \n0.029 \n0.118 \n9.776 \n13.75 \n2181 \n14569 \n31.555 \n\nSupervised \nT5 \n0.378 \n0.190 \n0.367 \n0.367 \n0.581 \n0.099 \n0.209 \n0.931 \n0.609 \n0.156 \n0.534 \n9.807 13.657 \n3250 \n14995 \n4.923 \n\nSupervised + PPO \nRouge Combined T5 \n0.38 \n0.177 \n0.371 \n0.371 \n0.585 \n0.09 \n0.229 \n0.931 \n0.64 \n0.174 \n0.559 \n10.132 13.547 \n3326 \n13785 \n4.353 \nRouge-L Max \nT5 \n0.368 \n0.18 \n0.36 \n0.36 \n0.585 \n0.083 \n0.239 \n0.931 \n0.641 \n0.187 \n0.576 \n10.201 13.452 \n3287 \n12436 \n3.913 \n\nSupervised + NLPO Rouge Combined T5 \n0.398 \n0.21 \n0.393 \n0.373 \n0.589 \n0.096 \n0.24 \n0.971 \n0.679 \n0.185 \n0.595 \n10.304 13.694 \n3371 \n15067 \n4.728 \nRouge-L Max \nT5 \n0.381 \n0.194 \n0.383 \n0.383 \n0.588 \n0.093 \n0.243 \n0.932 \n0.645 \n0.187 \n0.59 \n10.2 \n13.397 \n3287 \n12171 \n3.889 \n\n\n\nTable 28 :\n28Evaluation of NarrativeQA: Reference Metrics, supervised is based on UnifiedQA (Khashabi et al., 2020).\n\nTable 28\n28presents our benchmarking results with 2 reward functions along with supervised baseline performances on the NarrativeQA test set. Similar to other methods, our main finding is that warmstarted initial policies are crucial for learning to generate answers that successfully use the input context.B.7.3 QUALITATIVE RESULTS \n\n\n\nTable 29 :\n29NMT Hyperparams: Table shows a list of all hyper-parameters and their settings B.8.2 RESULTS AND DISCUSSION Tables 30, 31 presents our benchmarking results with 4 reward functions along with supervised baseline performances on test set. Our main finding is that NLPO + Supervised performs better than PPO and supervised models.Datasets \n\n\nTable 30 :\n30WMT-16 and IWSLT test evaluation -lexical and semantic: Table shows lexical, semantic metrics for RL algorithms with different reward functions bench-marked against supervised baseline modelsTasks \n\n\nTable 31 :\n31WMT-16 and IWSLT test evaluation -diversity metricsTable shows diversity metrics for RL algorithms with different reward functions bench-marked against supervised baseline models We show sample generations from each of the algorithms for three randomly picked prompts from IWSLT below. Prompt: translate English to German: Several years ago here at TED, Peter Skillman introduced a design challenge called the marshmallow challenge. Zero-Shot: Vor einigen Jahren hat Peter Skillman hier bei TED eine Design-Herausforderung namens Marshmallow Challenge vorgestellt. PPO: Vor einigen Jahren, hier bei TED, stellte Peter Skillman einen Design-Wettbewerb namens \" Die Marshmallow-Herausforderung\" vor NLPO: Vor einigen Jahren hier bei TED f\u00c3ijhrte Peter Skillman eine Design-Herausforderung namens Marshmallow Challenge ein. Supervised: Vor einigen Jahren hier bei TED f\u00c3ijfchrte Peter Skillman eine Design-Herausforderung namens Marshmallow Challenge ein. Supervised + PPO: Vor einigen Jahren hier bei TED, f\u00c3ijhrte Peter Skillman eine Design-Herausforderung namens Marshmallow-Herausforderung ein. Supervised + NLPO: Vor einigen Jahren hier bei TED f\u00c3ijchrte Peter Skillman eine Design-Herausforderung ein, die Marshmallow-Herausforderung. Prompt: translate English to German: They spend the majority of their time assembling the sticks into ever-growing structures. Zero-Shot: Sie verbringen den gr\u00c3\u0171\u00c3 \u00a7ten Teil ihrer Zeit damit, die St\u00c3\u0171cke in immer wachsende Strukturen zu montieren. References: Sie investieren einen Gro\u00c3 \u00a7teil ihrer Zeit in den Zusammenbau st\u00c3d'ndig wachsender Strukturen PPO: Sie verbringen den Gro\u00c3 \u00a7teil ihrer Zeit damit, die St\u00c3\u0171cke in immer gr\u00c3\u0171\u00c3 \u00a7er werdende Strukturen zu montieren. NLPO: Sie verbringen den Gro\u00c3 \u00a7teil ihrer Zeit, die St\u00c3\u0171cke in immer wachsende Strukturen zu montieren. Supervised: Sie verbringen den Gro\u00c3 \u00a7teil ihrer Zeit damit, die St\u00c3\u0171cke in immer wachsende Strukturen zu montieren. Supervised + PPO: Sie verbringen den Gro\u00c3 \u00a7teil ihrer Zeit damit, die St\u00c3\u0171cke in immer wachsende Strukturen zu montieren. Supervised + NLPO: Sie verbringen den Gro\u00c3 \u00a7teil ihrer Zeit damit, die St\u00c3\u0171cke in immer wachsende Strukturen zu montieren.Sample 1 \nSample 2 \n\n\n\nTasksLexicaland Semantic Metrics Diversity Metrics Alg Reward Function LM Rouge-1 Rouge-2 Rouge-L Rouge-LSum Meteor SacreBLEU BertScore Intent Accuracy MSTTR Distinct1 Distinct2 H1 H2 Unique1 Unique2 Mean Output LengthDialog \n\nZero Shot \nGPT-2 \n0.157 \n0.012 \n0.131 \n0.131 \n0.191 \n0.066 \n0.854 \n0.427 \n0.608 \n0.055 \n0.316 \n7.787 11.831 \n1574 \n12327 \n18.685 \n\nSupervised \nGPT-2 \n0.162 \n0.020 \n0.138 \n0.138 \n0.186 \n0.064 \n0.855 \n0.437 \n0.635 \n0.065 \n0.342 \n8.051 12.119 \n1925 \n13952 \n18.919 \n\nPPO \nMeteor + Intent GPT-2 \n0.168 \n0.012 \n0.142 \n0.142 \n0.221 \n0.085 \n0.861 \n0.474 \n0.581 \n0.058 \n0.310 \n7.653 11.437 \n1719 \n12156 \n18.538 \n\nNLPO \nMeteor + Intent GPT-2 \n0.169 \n0.013 \n0.142 \n0.142 \n0.221 \n0.087 \n0.860 \n0.490 \n0.568 \n0.059 \n0.309 \n7.630 11.351 \n1718 \n11946 \n18.397 \n\nSupervised + PPO \nMeteor + Intent GPT-2 \n0.169 \n0.021 \n0.144 \n0.144 \n0.198 \n0.071 \n0.857 \n0.455 \n0.626 \n0.068 \n0.348 \n8.056 12.015 \n1983 \n14170 \n18.829 \n\nSupervised + NLPO Meteor + Intent GPT-2 \n0.171 \n0.020 \n0.146 \n0.146 \n0.205 \n0.074 \n0.858 \n0.454 \n0.624 \n0.070 \n0.349 \n8.044 11.990 \n2051 \n14213 \n18.763 \n\n\n\nTable 33 :\n33Evaluation of Daily Dialog: Table shows lexical, semantic metrics for RL algorithms bench-marked against supervised baseline modelsAlgorithm \nUnique N \nCoherence \nQuality \nValue Alpha Skew Value Alpha Skew \nZeroshot \n31 \n3.84 \n0.225 4.181 \n3.2 \n0.125 3.352 \nNLPO \n30 \n4.18 \n0.114 \n4.17 \n3.35 \n0.159 3.318 \nPPO \n32 \n4.18 \n0.112 4.032 \n3.32 \n0.163 3.478 \nSupervised+PPO \n31 \n3.99 \n0.148 4.133 \n3.48 \n0.166 \n3.58 \nSupervised+NLPO \n31 \n4.13 \n0.186 3.953 \n3.58 \n0.178 3.597 \nSupervised \n31 \n3.96 \n0.249 3.834 \n3.59 \n0.236 3.196 \n\n\n\nTable 34 :\n34Results of the human subject study showing the number of participants N, average Likert scale value for coherence and sentiment, Krippendorf's alpha showing inter-annotator agreement, and Skew. For each model a total of 100 samples were drawn randomly from the test set and rated by 3 annotators each, each resulting in 300 data points per algorithm.B.9.2 RESULTS AND DISCUSSION \n\n\n\n\nG2-G1) p-values Diff (G2-G1) p-valuesGroup 1 \nGroup 2 \nCoherence \nQuality \nDiff (NLPO \nPPO \n-0.003 \n0.900 \n-0.030 \n0.900 \nNLPO \nSupervised \n-0.227 \n0.043 \n0.238 \n0.020 \nNLPO \nSupervised+NLPO \n-0.050 \n0.900 \n0.234 \n0.022 \nNLPO \nSupervised+PPO \n-0.194 \n0.013 \n0.127 \n0.803 \nNLPO \nZero Shot \n-0.345 \n0.001 \n-0.154 \n0.655 \nPPO \nSupervised \n-0.224 \n0.049 \n0.268 \n0.010 \nPPO \nSupervised+NLPO \n-0.047 \n0.900 \n0.264 \n0.011 \nPPO \nSupervised+PPO \n-0.191 \n0.144 \n0.157 \n0.636 \nPPO \nZero Shot \n-0.341 \n0.001 \n-0.124 \n0.822 \nSupervised \nSupervised+NLPO \n0.177 \n0.021 \n-0.003 \n0.900 \nSupervised \nSupervised+PPO \n0.033 \n0.900 \n-0.110 \n0.896 \nSupervised \nZero Shot \n-0.117 \n0.645 \n-0.391 \n0.002 \nSupervised+NLPO Supervised+PPO \n-0.144 \n0.444 \n-0.107 \n0.009 \nSupervised+NLPO Zero Shot \n-0.294 \n0.002 \n-0.388 \n0.003 \nSupervised+PPO \nZero Shot \n-0.151 \n0.390 \n-0.281 \n0.008 \n\n\nStrathern (1997) paraphrases: When a measure becomes a target, it ceases to be a good measure.\nB.1 Crowdworking Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2 GRUE Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3 IMDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3.3 Human Participant Study . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.3.4 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.4 CommonGen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.4.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.4.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.4.3 Human Participant Study . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 B.4.4 Human Preference Learning Experiments . . . . . . . . . . . . . . . . . . 32 B.4.5 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 B.5 CNN Daily Mail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nB.5.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 B.5.3 Human Participant Study . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 B.5.4 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 B.6 ToTTo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 B.6.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 B.6.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 B.6.3 Human Participant Study . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 B.6.4 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 B.7 Narrative QA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 B.7.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 B.7.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 B.7.3 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 B.8 Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 B.8.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 B.8.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 B.8.3 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 B.9 Daily Dialog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 B.9.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 B.9.2 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 B.9.3 Human Participant Study . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 B.9.4 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nB.9 DAILY DIALOG B.9.1 SETUP We considerDailyDialog (Li et al., 2017)as the test bed for the dialogue generation task. The dataset includes conversations written by human on various topics. In addition, each utterance contains labels of intent and emotional information. For simplicity, we focus only on generating the next utterance, given the dialogue context. We chose a context window of size 5, which results in 35k training, 3k and 3k utterances. The input to the model is dialogue history in which utterances are concatenated using a <EOU> token. We picked GPT-2 as the LM as they are more suited for text continuation than encoder-decoder LMs. For a fair comparison, we use top-k sampling with k = 20 as the decoding method for all methods. For RL methods, we use a linear combination of meteor score and intent match score (whether the generated text's intent matches with the reference's intent) as the reward function. The coefficients for meteor and intent are chosen based on both lexical scores and intent accuracy on the validation set. For this purpose, we trained an intent classifier (fine-tuned RoBERTa (Liu et al., 2019)) that classifies given text into intent categories such as inform, question, directive and commisive, etc.\nReinforcement learning: Theory and algorithms. Alekh Agarwal, Nan Jiang, M Sham, Wen Kakade, ; Sun, Dept, Uw Seattle, Seattle, WA, USATech. RepAlekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pp. 10-4, 2019.\n\nLanguage Learning in Interactive Environments. Prithviraj Ammanabrolu, Georgia Institute of TechnologyPhD thesisPrithviraj Ammanabrolu. Language Learning in Interactive Environments. PhD thesis, Georgia Institute of Technology, 2021.\n\nGraph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1x6w0EtwH.\n\nAligning to social norms and values in interactive narratives. Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, Yejin Choi, NAACL. 2022Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, and Yejin Choi. Aligning to social norms and values in interactive narratives. In NAACL, 2022.\n\nSpice: Semantic propositional image caption evaluation. Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, European conference on computer vision. SpringerPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In European conference on computer vision, pp. 382-398. Springer, 2016.\n\nOpenAI Pieter Abbeel, and Wojciech Zaremba. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Advances in Neural Information Processing Systems. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Hindsight experience replayMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As- sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 453fadbd8a1a3af50a9df4df899537b5-Paper.pdf.\n\nWhy exposure bias matters: An imitation learning perspective of error accumulation in language generation. Kushal Arora, Layla El Asri, Hareesh Bahuleyan, Jackie Cheung, 10.18653/v1/2022.findings-acl.58Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsKushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 700-710, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.58. URL https://aclanthology.org/2022.findings-acl.58.\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, arXiv:1607.07086arXiv preprintDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.\n\nMeteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarizationSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.\n\nSequence level training with recurrent neural networks. Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, In ICLR. Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2016.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, CVPR. Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, 2017.\n\nA reduction of imitation learning and structured prediction to no-regret online learning. St\u00e9phane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsJMLR Workshop and Conference ProceedingsSt\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627-635. JMLR Workshop and Conference Proceedings, 2011.\n\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.01108arXiv preprintVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International conference on machine learning. PMLRJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR, 2015a.\n\nHigh-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.02438arXiv preprintJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nBLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsThibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https://www.aclweb.org/anthology/2020. acl-main.704.\n\nA mathematical theory of communication. The Bell system technical journal. Claude Elwood Shannon, 27Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379-423, 1948.\n\nOffline rl for natural language generation with implicit language q learning. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine, arXiv:2206.11871arXiv preprintCharlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.\n\nLearning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 33Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.\n\nimproving ratings': audit in the british university system. Marilyn Strathern, European review. 53Published as a conference paper at ICLR 2023Marilyn Strathern. 'improving ratings': audit in the british university system. European review, 5(3): 305-321, 1997. Published as a conference paper at ICLR 2023\n\nFigure 10: Instructions and interface for the Daily Dialogue task. Figure 10: Instructions and interface for the Daily Dialogue task.\n\nAny Others ? <EOU> Well , have you ever heard of Oulong tea and compressed tea ? <EOU> References: Oh , yeah . Oulong tea is good for one's health . isn't it ? Zero-Shot: I know that Oulong tea can be enjoyed with very few meals, and there are very few PPO: I have, but it's not as strong as Oulong tea. NLPO: I haven't. I don't have tea. Supervised: Yes. I hear they do good. And I heard this Taijiquan Tea is very good Supervised + PPO: Actually, I have had Oulong tea. B , QUALITATIVE ANALYSIS We show sample generations from each of the algorithms for three randomly picked prompts below. Sample 1 Prompt: Right . And China is the homeland of tea . <EOU> Yes. Chinese people love drinking tea so much . Some even claim they can't live without tea . <EOU> Do you know there are several catagories of Chinese tea ? <EOU> Yes , I believe there are green teas , black teas and scented teas. but I haven't had any for awhile now. Supervised + NLPO: No, I'B.9.4 QUALITATIVE ANALYSIS We show sample generations from each of the algorithms for three randomly picked prompts below. Sample 1 Prompt: Right . And China is the homeland of tea . <EOU> Yes , Chinese people love drinking tea so much . Some even claim they can't live without tea . <EOU> Do you know there are several catagories of Chinese tea ? <EOU> Yes , I believe there are green teas , black teas and scented teas . Any Others ? <EOU> Well , have you ever heard of Oulong tea and compressed tea ? <EOU> References: Oh , yeah . Oulong tea is good for one's health . isn't it ? Zero-Shot: I know that Oulong tea can be enjoyed with very few meals, and there are very few PPO: I have, but it's not as strong as Oulong tea. NLPO: I haven't. I don't have tea. Supervised: Yes. I hear they do good. And I heard this Taijiquan Tea is very good Supervised + PPO: Actually, I have had Oulong tea, but I haven't had any for awhile now. Supervised + NLPO: No, I'\n\n<EOU> what's in a dahl ? <EOU> it's got chickpeas and vegetables in a spicy curry sauce with rice . <EOU> that sounds nice . Do you want to share some kebabs as well ? <EOU> ok . How about some lamb kebabs ? <EOU> References: that's my favourite . Do you want to have some wine or beer ? Zero-Shot: they're very salty PPO: well you might want to take a peek. NLPO: well I like that. Supervised: ok, here they are. I'm so thirsty. I can't wait until the next bus to Supervised + PPO: I've tried that a few times, but they're too dry and I don't want any of Supervised + NLPO: oh, ok. Prompt: I think I might have a dahl. That's a good choice. I'll take it. Sample 3Prompt: I think I might have a dahl . <EOU> what's in a dahl ? <EOU> it's got chickpeas and vegetables in a spicy curry sauce with rice . <EOU> that sounds nice . Do you want to share some kebabs as well ? <EOU> ok . How about some lamb kebabs ? <EOU> References: that's my favourite . Do you want to have some wine or beer ? Zero-Shot: they're very salty PPO: well you might want to take a peek. NLPO: well I like that. Supervised: ok, here they are. I'm so thirsty. I can't wait until the next bus to Supervised + PPO: I've tried that a few times, but they're too dry and I don't want any of Supervised + NLPO: oh, ok. That's a good choice. I'll take it. Sample 3\n\nYou know , the problem might be dust . Did you clean the unit often ? <EOU> Well . No , I didn't clean it.But still , I only had the last one two months , and it broke . I don't trust autoreverse . <EOU> Alright . Well . We have many units without auto-reverse.Here is a good unit . Very reliable.It has an AM / FM band , built-in microphone , recording capability , and no auto-reverse . <EOU> References: How much is it ? Zero-Shot: Oh. So you don't have the AM / FM band? PPO: You said AM / FM. Are you sure about that? NLPO: Well. Well. I had no idea when it was, and didn't care. Supervised: Good.Well, what else I need? Supervised + PPO: But I think it lacks the CD player or its CD player accessories. Prompt: Really ? Have you had an auto-reverse break before ? <EOU> Yes , twice now . I think it's too high-tech , and so it's the first part that breaks . <EOU> Hmm. Supervised + NLPO: Cool! So what's the trouble with itPrompt: Really ? Have you had an auto-reverse break before ? <EOU> Yes , twice now . I think it's too high-tech , and so it's the first part that breaks . <EOU> Hmm . You know , the problem might be dust . Did you clean the unit often ? <EOU> Well . No , I didn't clean it.But still , I only had the last one two months , and it broke . I don't trust auto- reverse . <EOU> Alright . Well . We have many units without auto-reverse.Here is a good unit . Very reliable.It has an AM / FM band , built-in microphone , recording capability , and no auto-reverse . <EOU> References: How much is it ? Zero-Shot: Oh. So you don't have the AM / FM band? PPO: You said AM / FM. Are you sure about that? NLPO: Well. Well. I had no idea when it was, and didn't care. Supervised: Good.Well, what else I need? Supervised + PPO: But I think it lacks the CD player or its CD player accessories. Supervised + NLPO: Cool! So what's the trouble with it?\n", "annotations": {"author": "[{\"end\":328,\"start\":152},{\"end\":459,\"start\":329},{\"end\":589,\"start\":460},{\"end\":723,\"start\":590},{\"end\":866,\"start\":724},{\"end\":999,\"start\":867},{\"end\":1144,\"start\":1000},{\"end\":1282,\"start\":1145},{\"end\":1402,\"start\":1283},{\"end\":1551,\"start\":1403},{\"end\":1682,\"start\":1552}]", "publisher": null, "author_last_name": "[{\"end\":171,\"start\":161},{\"end\":341,\"start\":331},{\"end\":605,\"start\":597},{\"end\":730,\"start\":726},{\"end\":881,\"start\":876},{\"end\":1026,\"start\":1007},{\"end\":1164,\"start\":1154},{\"end\":1433,\"start\":1429},{\"end\":1564,\"start\":1559}]", "author_first_name": "[{\"end\":160,\"start\":152},{\"end\":330,\"start\":329},{\"end\":471,\"start\":460},{\"end\":596,\"start\":590},{\"end\":725,\"start\":724},{\"end\":873,\"start\":867},{\"end\":875,\"start\":874},{\"end\":1004,\"start\":1000},{\"end\":1006,\"start\":1005},{\"end\":1153,\"start\":1145},{\"end\":1284,\"start\":1283},{\"end\":1428,\"start\":1423},{\"end\":1556,\"start\":1552},{\"end\":1558,\"start\":1557}]", "author_affiliation": "[{\"end\":327,\"start\":212},{\"end\":458,\"start\":343},{\"end\":588,\"start\":473},{\"end\":722,\"start\":607},{\"end\":865,\"start\":750},{\"end\":998,\"start\":883},{\"end\":1143,\"start\":1028},{\"end\":1281,\"start\":1166},{\"end\":1401,\"start\":1286},{\"end\":1550,\"start\":1435},{\"end\":1681,\"start\":1566}]", "title": "[{\"end\":149,\"start\":1},{\"end\":1831,\"start\":1683}]", "venue": null, "abstract": "[{\"end\":3568,\"start\":1877}]", "bib_ref": "[{\"end\":4179,\"start\":4158},{\"end\":4211,\"start\":4181},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4241,\"start\":4220},{\"end\":4350,\"start\":4330},{\"end\":5809,\"start\":5791},{\"end\":5829,\"start\":5809},{\"end\":6040,\"start\":6018},{\"end\":6062,\"start\":6040},{\"end\":8067,\"start\":8046},{\"end\":8105,\"start\":8069},{\"end\":8154,\"start\":8127},{\"end\":8190,\"start\":8170},{\"end\":8224,\"start\":8192},{\"end\":8243,\"start\":8224},{\"end\":8272,\"start\":8245},{\"end\":8307,\"start\":8278},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8357,\"start\":8338},{\"end\":8388,\"start\":8362},{\"end\":8490,\"start\":8469},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8509,\"start\":8490},{\"end\":8550,\"start\":8536},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8571,\"start\":8550},{\"end\":8590,\"start\":8571},{\"end\":8649,\"start\":8621},{\"end\":8718,\"start\":8702},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8742,\"start\":8720},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8939,\"start\":8907},{\"end\":8969,\"start\":8941},{\"end\":8999,\"start\":8971},{\"end\":9037,\"start\":9011},{\"end\":9265,\"start\":9248},{\"end\":9285,\"start\":9265},{\"end\":9312,\"start\":9285},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9351,\"start\":9328},{\"end\":9371,\"start\":9351},{\"end\":9399,\"start\":9382},{\"end\":9417,\"start\":9399},{\"end\":9437,\"start\":9417},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9477,\"start\":9456},{\"end\":9515,\"start\":9499},{\"end\":9554,\"start\":9529},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9578,\"start\":9554},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9611,\"start\":9589},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9630,\"start\":9611},{\"end\":9648,\"start\":9632},{\"end\":9706,\"start\":9687},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9838,\"start\":9815},{\"end\":9855,\"start\":9838},{\"end\":9875,\"start\":9855},{\"end\":9896,\"start\":9875},{\"end\":9924,\"start\":9904},{\"end\":9988,\"start\":9962},{\"end\":10282,\"start\":10260},{\"end\":10679,\"start\":10660},{\"end\":10724,\"start\":10703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11094,\"start\":11071},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11125,\"start\":11101},{\"end\":11150,\"start\":11127},{\"end\":12683,\"start\":12660},{\"end\":13231,\"start\":13220},{\"end\":13261,\"start\":13233},{\"end\":13285,\"start\":13263},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13318,\"start\":13294},{\"end\":13391,\"start\":13371},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13424,\"start\":13403},{\"end\":13551,\"start\":13528},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13582,\"start\":13559},{\"end\":13653,\"start\":13624},{\"end\":13706,\"start\":13677},{\"end\":13858,\"start\":13843},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13917,\"start\":13902},{\"end\":14096,\"start\":14079},{\"end\":14258,\"start\":14238},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15426,\"start\":15402},{\"end\":15596,\"start\":15579},{\"end\":16115,\"start\":16093},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16386,\"start\":16364},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16404,\"start\":16386},{\"end\":16776,\"start\":16755},{\"end\":16798,\"start\":16776},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16829,\"start\":16798},{\"end\":17134,\"start\":17111},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17647,\"start\":17620},{\"end\":20253,\"start\":20252},{\"end\":20391,\"start\":20374},{\"end\":20406,\"start\":20391},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20519,\"start\":20496},{\"end\":28914,\"start\":28887},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29585,\"start\":29566},{\"end\":31213,\"start\":31195},{\"end\":31233,\"start\":31213},{\"end\":31249,\"start\":31233},{\"end\":31410,\"start\":31388},{\"end\":32428,\"start\":32401},{\"end\":48250,\"start\":48227},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":48281,\"start\":48258},{\"end\":48311,\"start\":48286},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":147212,\"start\":147196}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":86221,\"start\":86119},{\"attributes\":{\"id\":\"fig_1\"},\"end\":86568,\"start\":86222},{\"attributes\":{\"id\":\"fig_2\"},\"end\":86943,\"start\":86569},{\"attributes\":{\"id\":\"fig_3\"},\"end\":87411,\"start\":86944},{\"attributes\":{\"id\":\"fig_4\"},\"end\":87508,\"start\":87412},{\"attributes\":{\"id\":\"fig_5\"},\"end\":88396,\"start\":87509},{\"attributes\":{\"id\":\"fig_6\"},\"end\":90003,\"start\":88397},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":91647,\"start\":90004},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":92708,\"start\":91648},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":92743,\"start\":92709},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":103498,\"start\":92744},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":104326,\"start\":103499},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":105490,\"start\":104327},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":108108,\"start\":105491},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":109444,\"start\":108109},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":109834,\"start\":109445},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":110271,\"start\":109835},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":110720,\"start\":110272},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":111764,\"start\":110721},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":111941,\"start\":111765},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":112296,\"start\":111942},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":113047,\"start\":112297},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":114262,\"start\":113048},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":114699,\"start\":114263},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":117386,\"start\":114700},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":120391,\"start\":117387},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":121883,\"start\":120392},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":124372,\"start\":121884},{\"attributes\":{\"id\":\"tab_29\",\"type\":\"table\"},\"end\":125032,\"start\":124373},{\"attributes\":{\"id\":\"tab_30\",\"type\":\"table\"},\"end\":126534,\"start\":125033},{\"attributes\":{\"id\":\"tab_31\",\"type\":\"table\"},\"end\":126971,\"start\":126535},{\"attributes\":{\"id\":\"tab_33\",\"type\":\"table\"},\"end\":127097,\"start\":126972},{\"attributes\":{\"id\":\"tab_35\",\"type\":\"table\"},\"end\":131519,\"start\":127098},{\"attributes\":{\"id\":\"tab_36\",\"type\":\"table\"},\"end\":132553,\"start\":131520},{\"attributes\":{\"id\":\"tab_37\",\"type\":\"table\"},\"end\":133773,\"start\":132554},{\"attributes\":{\"id\":\"tab_38\",\"type\":\"table\"},\"end\":134217,\"start\":133774},{\"attributes\":{\"id\":\"tab_39\",\"type\":\"table\"},\"end\":139432,\"start\":134218},{\"attributes\":{\"id\":\"tab_40\",\"type\":\"table\"},\"end\":141086,\"start\":139433},{\"attributes\":{\"id\":\"tab_41\",\"type\":\"table\"},\"end\":141204,\"start\":141087},{\"attributes\":{\"id\":\"tab_42\",\"type\":\"table\"},\"end\":141541,\"start\":141205},{\"attributes\":{\"id\":\"tab_44\",\"type\":\"table\"},\"end\":141893,\"start\":141542},{\"attributes\":{\"id\":\"tab_45\",\"type\":\"table\"},\"end\":142106,\"start\":141894},{\"attributes\":{\"id\":\"tab_46\",\"type\":\"table\"},\"end\":144315,\"start\":142107},{\"attributes\":{\"id\":\"tab_47\",\"type\":\"table\"},\"end\":145399,\"start\":144316},{\"attributes\":{\"id\":\"tab_48\",\"type\":\"table\"},\"end\":145939,\"start\":145400},{\"attributes\":{\"id\":\"tab_49\",\"type\":\"table\"},\"end\":146335,\"start\":145940},{\"attributes\":{\"id\":\"tab_50\",\"type\":\"table\"},\"end\":147195,\"start\":146336}]", "paragraph": "[{\"end\":5203,\"start\":3584},{\"end\":6063,\"start\":5205},{\"end\":7118,\"start\":6065},{\"end\":7959,\"start\":7120},{\"end\":8591,\"start\":7976},{\"end\":9137,\"start\":8593},{\"end\":10443,\"start\":9139},{\"end\":11424,\"start\":10490},{\"end\":12873,\"start\":11474},{\"end\":14259,\"start\":12917},{\"end\":15427,\"start\":14297},{\"end\":15888,\"start\":15429},{\"end\":15893,\"start\":15890},{\"end\":16184,\"start\":15895},{\"end\":17135,\"start\":16231},{\"end\":17942,\"start\":17137},{\"end\":17997,\"start\":17944},{\"end\":18294,\"start\":18038},{\"end\":18583,\"start\":18429},{\"end\":18748,\"start\":18679},{\"end\":18920,\"start\":18807},{\"end\":19865,\"start\":18978},{\"end\":20805,\"start\":19867},{\"end\":21213,\"start\":20807},{\"end\":21315,\"start\":21215},{\"end\":21954,\"start\":21317},{\"end\":22902,\"start\":21956},{\"end\":23471,\"start\":22976},{\"end\":24228,\"start\":23473},{\"end\":24576,\"start\":24230},{\"end\":25420,\"start\":24578},{\"end\":25773,\"start\":25422},{\"end\":26323,\"start\":25828},{\"end\":27273,\"start\":26325},{\"end\":28107,\"start\":27275},{\"end\":29886,\"start\":28109},{\"end\":30958,\"start\":29888},{\"end\":32149,\"start\":31030},{\"end\":32881,\"start\":32151},{\"end\":33422,\"start\":32897},{\"end\":33705,\"start\":33537},{\"end\":33918,\"start\":33707},{\"end\":34066,\"start\":34009},{\"end\":34819,\"start\":34068},{\"end\":34924,\"start\":34902},{\"end\":35863,\"start\":34953},{\"end\":36156,\"start\":35865},{\"end\":37964,\"start\":36186},{\"end\":39326,\"start\":38005},{\"end\":39572,\"start\":39328},{\"end\":40160,\"start\":39574},{\"end\":40592,\"start\":40162},{\"end\":42037,\"start\":40594},{\"end\":43881,\"start\":42149},{\"end\":45721,\"start\":43894},{\"end\":47335,\"start\":45734},{\"end\":48578,\"start\":47337},{\"end\":50042,\"start\":48580},{\"end\":51729,\"start\":50090},{\"end\":51986,\"start\":51731},{\"end\":52115,\"start\":52017},{\"end\":53028,\"start\":52128},{\"end\":53154,\"start\":53059},{\"end\":57038,\"start\":53156},{\"end\":64874,\"start\":57040},{\"end\":66113,\"start\":64887},{\"end\":69145,\"start\":66115},{\"end\":70862,\"start\":69147},{\"end\":72034,\"start\":70864},{\"end\":72160,\"start\":72065},{\"end\":73735,\"start\":72162},{\"end\":77564,\"start\":73737},{\"end\":81438,\"start\":77577},{\"end\":83848,\"start\":81451},{\"end\":85366,\"start\":83869},{\"end\":86118,\"start\":85368}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18037,\"start\":17998},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18428,\"start\":18295},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18678,\"start\":18584},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18806,\"start\":18749},{\"attributes\":{\"id\":\"formula_4\"},\"end\":33536,\"start\":33423},{\"attributes\":{\"id\":\"formula_5\"},\"end\":34008,\"start\":33919},{\"attributes\":{\"id\":\"formula_6\"},\"end\":34901,\"start\":34820},{\"attributes\":{\"id\":\"formula_7\"},\"end\":42148,\"start\":42038}]", "table_ref": "[{\"end\":19174,\"start\":19167},{\"end\":20527,\"start\":20520},{\"end\":21891,\"start\":21884},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23088,\"start\":23077},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":25120,\"start\":25108},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26579,\"start\":26550},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26971,\"start\":26961},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27807,\"start\":27778},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30290,\"start\":30283},{\"end\":30957,\"start\":30950},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31525,\"start\":31518},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":31547,\"start\":31540},{\"end\":36244,\"start\":36237},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":37878,\"start\":37871},{\"end\":38705,\"start\":38698},{\"end\":39494,\"start\":39487},{\"end\":39863,\"start\":39856},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":40377,\"start\":40370},{\"end\":40615,\"start\":40608},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":41685,\"start\":41673},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":49197,\"start\":49184},{\"end\":49713,\"start\":49706},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":49874,\"start\":49861},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":52860,\"start\":52847},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":71866,\"start\":71853},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":85087,\"start\":85079},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":85516,\"start\":85503},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":85694,\"start\":85686}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3582,\"start\":3570},{\"attributes\":{\"n\":\"2\"},\"end\":7974,\"start\":7962},{\"attributes\":{\"n\":\"3\"},\"end\":10488,\"start\":10446},{\"attributes\":{\"n\":\"3.1\"},\"end\":11472,\"start\":11427},{\"attributes\":{\"n\":\"3.2\"},\"end\":12915,\"start\":12876},{\"attributes\":{\"n\":\"3.3\"},\"end\":14295,\"start\":14262},{\"attributes\":{\"n\":\"4\"},\"end\":16229,\"start\":16187},{\"attributes\":{\"n\":\"5\"},\"end\":18976,\"start\":18923},{\"attributes\":{\"n\":\"5.1\"},\"end\":22974,\"start\":22905},{\"attributes\":{\"n\":\"5.2\"},\"end\":25826,\"start\":25776},{\"attributes\":{\"n\":\"5.4\"},\"end\":31028,\"start\":30961},{\"attributes\":{\"n\":\"6\"},\"end\":32895,\"start\":32884},{\"end\":34951,\"start\":34927},{\"end\":36184,\"start\":36159},{\"end\":37972,\"start\":37967},{\"end\":38003,\"start\":37975},{\"end\":43892,\"start\":43884},{\"end\":45732,\"start\":45724},{\"end\":50088,\"start\":50045},{\"end\":52015,\"start\":51989},{\"end\":52126,\"start\":52118},{\"end\":53057,\"start\":53031},{\"end\":64885,\"start\":64877},{\"end\":72063,\"start\":72037},{\"end\":77575,\"start\":77567},{\"end\":81449,\"start\":81441},{\"end\":83856,\"start\":83851},{\"end\":83867,\"start\":83859},{\"end\":86130,\"start\":86120},{\"end\":86233,\"start\":86223},{\"end\":86580,\"start\":86570},{\"end\":86955,\"start\":86945},{\"end\":87423,\"start\":87413},{\"end\":91658,\"start\":91649},{\"end\":92719,\"start\":92710},{\"end\":103509,\"start\":103500},{\"end\":104337,\"start\":104328},{\"end\":105501,\"start\":105492},{\"end\":108119,\"start\":108110},{\"end\":109455,\"start\":109446},{\"end\":109846,\"start\":109836},{\"end\":111776,\"start\":111766},{\"end\":111953,\"start\":111943},{\"end\":112308,\"start\":112298},{\"end\":113059,\"start\":113049},{\"end\":114274,\"start\":114264},{\"end\":114709,\"start\":114701},{\"end\":117398,\"start\":117388},{\"end\":120403,\"start\":120393},{\"end\":121895,\"start\":121885},{\"end\":124384,\"start\":124374},{\"end\":125044,\"start\":125034},{\"end\":126546,\"start\":126536},{\"end\":126983,\"start\":126973},{\"end\":127109,\"start\":127099},{\"end\":131531,\"start\":131521},{\"end\":132565,\"start\":132555},{\"end\":133785,\"start\":133775},{\"end\":139444,\"start\":139434},{\"end\":141098,\"start\":141088},{\"end\":141214,\"start\":141206},{\"end\":141553,\"start\":141543},{\"end\":141905,\"start\":141895},{\"end\":142118,\"start\":142108},{\"end\":145411,\"start\":145401},{\"end\":145951,\"start\":145941}]", "table": "[{\"end\":91647,\"start\":90313},{\"end\":92708,\"start\":92077},{\"end\":103498,\"start\":93281},{\"end\":104326,\"start\":103590},{\"end\":105490,\"start\":104401},{\"end\":108108,\"start\":106138},{\"end\":109444,\"start\":108876},{\"end\":109834,\"start\":109802},{\"end\":110720,\"start\":110604},{\"end\":111764,\"start\":110836},{\"end\":111941,\"start\":111863},{\"end\":112296,\"start\":112256},{\"end\":113047,\"start\":112651},{\"end\":114262,\"start\":113407},{\"end\":117386,\"start\":115687},{\"end\":120391,\"start\":118210},{\"end\":121883,\"start\":121037},{\"end\":124372,\"start\":122091},{\"end\":125032,\"start\":124598},{\"end\":126534,\"start\":125047},{\"end\":127097,\"start\":127066},{\"end\":131519,\"start\":127740},{\"end\":132553,\"start\":132153},{\"end\":133773,\"start\":132568},{\"end\":134217,\"start\":133788},{\"end\":139432,\"start\":136851},{\"end\":141086,\"start\":139527},{\"end\":141541,\"start\":141513},{\"end\":141893,\"start\":141883},{\"end\":142106,\"start\":142099},{\"end\":144315,\"start\":144295},{\"end\":145399,\"start\":144536},{\"end\":145939,\"start\":145545},{\"end\":146335,\"start\":146304},{\"end\":147195,\"start\":146375}]", "figure_caption": "[{\"end\":86221,\"start\":86132},{\"end\":86568,\"start\":86235},{\"end\":86943,\"start\":86582},{\"end\":87411,\"start\":86957},{\"end\":87508,\"start\":87425},{\"end\":88396,\"start\":87511},{\"end\":90003,\"start\":88399},{\"end\":90313,\"start\":90006},{\"end\":92077,\"start\":91660},{\"end\":92743,\"start\":92721},{\"end\":93281,\"start\":92746},{\"end\":103590,\"start\":103511},{\"end\":104401,\"start\":104339},{\"end\":106138,\"start\":105503},{\"end\":108876,\"start\":108121},{\"end\":109802,\"start\":109457},{\"end\":110271,\"start\":109849},{\"end\":110604,\"start\":110274},{\"end\":110836,\"start\":110723},{\"end\":111863,\"start\":111779},{\"end\":112256,\"start\":111956},{\"end\":112651,\"start\":112311},{\"end\":113407,\"start\":113062},{\"end\":114699,\"start\":114277},{\"end\":115687,\"start\":114711},{\"end\":118210,\"start\":117401},{\"end\":121037,\"start\":120406},{\"end\":122091,\"start\":121898},{\"end\":124598,\"start\":124387},{\"end\":126971,\"start\":126549},{\"end\":127066,\"start\":126986},{\"end\":127740,\"start\":127112},{\"end\":132153,\"start\":131534},{\"end\":136851,\"start\":134220},{\"end\":139527,\"start\":139447},{\"end\":141204,\"start\":141101},{\"end\":141513,\"start\":141217},{\"end\":141883,\"start\":141556},{\"end\":142099,\"start\":141908},{\"end\":144295,\"start\":142121},{\"end\":144536,\"start\":144318},{\"end\":145545,\"start\":145414},{\"end\":146304,\"start\":145954},{\"end\":146375,\"start\":146338}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7306,\"start\":7298},{\"end\":26207,\"start\":26201},{\"end\":27297,\"start\":27289},{\"end\":36861,\"start\":36853},{\"end\":37269,\"start\":37261},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38035,\"start\":38024},{\"end\":41527,\"start\":41519},{\"end\":41862,\"start\":41854},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":50623,\"start\":50615},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":52590,\"start\":52582},{\"end\":71643,\"start\":71635},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":85407,\"start\":85398}]", "bib_author_first_name": "[{\"end\":151716,\"start\":151711},{\"end\":151729,\"start\":151726},{\"end\":151738,\"start\":151737},{\"end\":151748,\"start\":151745},{\"end\":151758,\"start\":151757},{\"end\":152033,\"start\":152023},{\"end\":152298,\"start\":152288},{\"end\":152319,\"start\":152312},{\"end\":152695,\"start\":152685},{\"end\":152714,\"start\":152709},{\"end\":152729,\"start\":152722},{\"end\":152743,\"start\":152735},{\"end\":152761,\"start\":152756},{\"end\":153007,\"start\":153002},{\"end\":153024,\"start\":153018},{\"end\":153039,\"start\":153035},{\"end\":153056,\"start\":153049},{\"end\":153357,\"start\":153351},{\"end\":153377,\"start\":153372},{\"end\":153390,\"start\":153386},{\"end\":153401,\"start\":153396},{\"end\":153419,\"start\":153413},{\"end\":153431,\"start\":153426},{\"end\":153445,\"start\":153442},{\"end\":153458,\"start\":153454},{\"end\":154255,\"start\":154249},{\"end\":154268,\"start\":154263},{\"end\":154271,\"start\":154269},{\"end\":154285,\"start\":154278},{\"end\":154303,\"start\":154297},{\"end\":154947,\"start\":154940},{\"end\":154966,\"start\":154958},{\"end\":154981,\"start\":154975},{\"end\":154993,\"start\":154986},{\"end\":155005,\"start\":155001},{\"end\":155018,\"start\":155012},{\"end\":155032,\"start\":155027},{\"end\":155050,\"start\":155044},{\"end\":155406,\"start\":155397},{\"end\":155421,\"start\":155417},{\"end\":156000,\"start\":155993},{\"end\":156012,\"start\":156007},{\"end\":156029,\"start\":156022},{\"end\":156046,\"start\":156038},{\"end\":156270,\"start\":156269},{\"end\":156286,\"start\":156279},{\"end\":156302,\"start\":156295},{\"end\":156320,\"start\":156314},{\"end\":156337,\"start\":156329},{\"end\":156608,\"start\":156600},{\"end\":156623,\"start\":156615},{\"end\":156636,\"start\":156632},{\"end\":157253,\"start\":157247},{\"end\":157268,\"start\":157260},{\"end\":157282,\"start\":157276},{\"end\":157299,\"start\":157293},{\"end\":157556,\"start\":157552},{\"end\":157573,\"start\":157567},{\"end\":157588,\"start\":157582},{\"end\":157604,\"start\":157597},{\"end\":157620,\"start\":157613},{\"end\":157952,\"start\":157948},{\"end\":157970,\"start\":157963},{\"end\":157985,\"start\":157979},{\"end\":158001,\"start\":157994},{\"end\":158016,\"start\":158010},{\"end\":158257,\"start\":158253},{\"end\":158273,\"start\":158268},{\"end\":158290,\"start\":158282},{\"end\":158305,\"start\":158301},{\"end\":158319,\"start\":158315},{\"end\":158620,\"start\":158612},{\"end\":158637,\"start\":158629},{\"end\":158648,\"start\":158643},{\"end\":159340,\"start\":159334},{\"end\":159347,\"start\":159341},{\"end\":159564,\"start\":159557},{\"end\":159576,\"start\":159572},{\"end\":159590,\"start\":159588},{\"end\":159603,\"start\":159595},{\"end\":159616,\"start\":159610},{\"end\":159893,\"start\":159888},{\"end\":159908,\"start\":159904},{\"end\":159924,\"start\":159917},{\"end\":159935,\"start\":159929},{\"end\":159949,\"start\":159945},{\"end\":159963,\"start\":159956},{\"end\":159974,\"start\":159970},{\"end\":159989,\"start\":159984},{\"end\":160004,\"start\":159998},{\"end\":160385,\"start\":160378},{\"end\":161232,\"start\":161231}]", "bib_author_last_name": "[{\"end\":151724,\"start\":151717},{\"end\":151735,\"start\":151730},{\"end\":151743,\"start\":151739},{\"end\":151755,\"start\":151749},{\"end\":151762,\"start\":151759},{\"end\":151768,\"start\":151764},{\"end\":151780,\"start\":151770},{\"end\":152045,\"start\":152034},{\"end\":152310,\"start\":152299},{\"end\":152330,\"start\":152320},{\"end\":152707,\"start\":152696},{\"end\":152720,\"start\":152715},{\"end\":152733,\"start\":152730},{\"end\":152754,\"start\":152744},{\"end\":152766,\"start\":152762},{\"end\":153016,\"start\":153008},{\"end\":153033,\"start\":153025},{\"end\":153047,\"start\":153040},{\"end\":153062,\"start\":153057},{\"end\":153370,\"start\":153358},{\"end\":153384,\"start\":153378},{\"end\":153394,\"start\":153391},{\"end\":153411,\"start\":153402},{\"end\":153424,\"start\":153420},{\"end\":153440,\"start\":153432},{\"end\":153452,\"start\":153446},{\"end\":153464,\"start\":153459},{\"end\":154261,\"start\":154256},{\"end\":154276,\"start\":154272},{\"end\":154295,\"start\":154286},{\"end\":154310,\"start\":154304},{\"end\":154956,\"start\":154948},{\"end\":154973,\"start\":154967},{\"end\":154984,\"start\":154982},{\"end\":154999,\"start\":154994},{\"end\":155010,\"start\":155006},{\"end\":155025,\"start\":155019},{\"end\":155042,\"start\":155033},{\"end\":155057,\"start\":155051},{\"end\":155415,\"start\":155407},{\"end\":155427,\"start\":155422},{\"end\":156005,\"start\":156001},{\"end\":156020,\"start\":156013},{\"end\":156036,\"start\":156030},{\"end\":156051,\"start\":156047},{\"end\":156060,\"start\":156053},{\"end\":156277,\"start\":156271},{\"end\":156293,\"start\":156287},{\"end\":156312,\"start\":156303},{\"end\":156327,\"start\":156321},{\"end\":156342,\"start\":156338},{\"end\":156348,\"start\":156344},{\"end\":156613,\"start\":156609},{\"end\":156630,\"start\":156624},{\"end\":156644,\"start\":156637},{\"end\":157258,\"start\":157254},{\"end\":157274,\"start\":157269},{\"end\":157291,\"start\":157283},{\"end\":157304,\"start\":157300},{\"end\":157565,\"start\":157557},{\"end\":157580,\"start\":157574},{\"end\":157595,\"start\":157589},{\"end\":157611,\"start\":157605},{\"end\":157627,\"start\":157621},{\"end\":157961,\"start\":157953},{\"end\":157977,\"start\":157971},{\"end\":157992,\"start\":157986},{\"end\":158008,\"start\":158002},{\"end\":158023,\"start\":158017},{\"end\":158266,\"start\":158258},{\"end\":158280,\"start\":158274},{\"end\":158299,\"start\":158291},{\"end\":158313,\"start\":158306},{\"end\":158326,\"start\":158320},{\"end\":158627,\"start\":158621},{\"end\":158641,\"start\":158638},{\"end\":158655,\"start\":158649},{\"end\":159355,\"start\":159348},{\"end\":159570,\"start\":159565},{\"end\":159586,\"start\":159577},{\"end\":159593,\"start\":159591},{\"end\":159608,\"start\":159604},{\"end\":159623,\"start\":159617},{\"end\":159902,\"start\":159894},{\"end\":159915,\"start\":159909},{\"end\":159927,\"start\":159925},{\"end\":159943,\"start\":159936},{\"end\":159954,\"start\":159950},{\"end\":159968,\"start\":159964},{\"end\":159982,\"start\":159975},{\"end\":159996,\"start\":159990},{\"end\":160015,\"start\":160005},{\"end\":160395,\"start\":160386}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":151974,\"start\":151664},{\"attributes\":{\"id\":\"b1\"},\"end\":152209,\"start\":151976},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":210911499},\"end\":152620,\"start\":152211},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":248512563},\"end\":152944,\"start\":152622},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11933981},\"end\":153305,\"start\":152946},{\"attributes\":{\"id\":\"b5\"},\"end\":154140,\"start\":153307},{\"attributes\":{\"doi\":\"10.18653/v1/2022.findings-acl.58\",\"id\":\"b6\",\"matched_paper_id\":247939224},\"end\":154887,\"start\":154142},{\"attributes\":{\"doi\":\"arXiv:1607.07086\",\"id\":\"b7\"},\"end\":155301,\"start\":154889},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7164502},\"end\":155935,\"start\":155303},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7147309},\"end\":156213,\"start\":155937},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594923},\"end\":156508,\"start\":156215},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":103456},\"end\":157166,\"start\":156510},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b12\"},\"end\":157516,\"start\":157168},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16046818},\"end\":157870,\"start\":157518},{\"attributes\":{\"doi\":\"arXiv:1506.02438\",\"id\":\"b14\"},\"end\":158251,\"start\":157872},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b15\"},\"end\":158557,\"start\":158253},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.704\",\"id\":\"b16\",\"matched_paper_id\":215548699},\"end\":159257,\"start\":158559},{\"attributes\":{\"id\":\"b17\"},\"end\":159477,\"start\":159259},{\"attributes\":{\"doi\":\"arXiv:2206.11871\",\"id\":\"b18\"},\"end\":159843,\"start\":159479},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":221665105},\"end\":160316,\"start\":159845},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":145644958},\"end\":160622,\"start\":160318},{\"attributes\":{\"id\":\"b21\"},\"end\":160757,\"start\":160624},{\"attributes\":{\"id\":\"b22\"},\"end\":162671,\"start\":160759},{\"attributes\":{\"id\":\"b23\"},\"end\":164002,\"start\":162673},{\"attributes\":{\"id\":\"b24\"},\"end\":165866,\"start\":164004}]", "bib_title": "[{\"end\":152286,\"start\":152211},{\"end\":152683,\"start\":152622},{\"end\":153000,\"start\":152946},{\"end\":153349,\"start\":153307},{\"end\":154247,\"start\":154142},{\"end\":155395,\"start\":155303},{\"end\":155991,\"start\":155937},{\"end\":156267,\"start\":156215},{\"end\":156598,\"start\":156510},{\"end\":157550,\"start\":157518},{\"end\":158610,\"start\":158559},{\"end\":159886,\"start\":159845},{\"end\":160376,\"start\":160318},{\"end\":161229,\"start\":160759},{\"end\":164711,\"start\":164004}]", "bib_author": "[{\"end\":151726,\"start\":151711},{\"end\":151737,\"start\":151726},{\"end\":151745,\"start\":151737},{\"end\":151757,\"start\":151745},{\"end\":151764,\"start\":151757},{\"end\":151770,\"start\":151764},{\"end\":151782,\"start\":151770},{\"end\":152047,\"start\":152023},{\"end\":152312,\"start\":152288},{\"end\":152332,\"start\":152312},{\"end\":152709,\"start\":152685},{\"end\":152722,\"start\":152709},{\"end\":152735,\"start\":152722},{\"end\":152756,\"start\":152735},{\"end\":152768,\"start\":152756},{\"end\":153018,\"start\":153002},{\"end\":153035,\"start\":153018},{\"end\":153049,\"start\":153035},{\"end\":153064,\"start\":153049},{\"end\":153372,\"start\":153351},{\"end\":153386,\"start\":153372},{\"end\":153396,\"start\":153386},{\"end\":153413,\"start\":153396},{\"end\":153426,\"start\":153413},{\"end\":153442,\"start\":153426},{\"end\":153454,\"start\":153442},{\"end\":153466,\"start\":153454},{\"end\":154263,\"start\":154249},{\"end\":154278,\"start\":154263},{\"end\":154297,\"start\":154278},{\"end\":154312,\"start\":154297},{\"end\":154958,\"start\":154940},{\"end\":154975,\"start\":154958},{\"end\":154986,\"start\":154975},{\"end\":155001,\"start\":154986},{\"end\":155012,\"start\":155001},{\"end\":155027,\"start\":155012},{\"end\":155044,\"start\":155027},{\"end\":155059,\"start\":155044},{\"end\":155417,\"start\":155397},{\"end\":155429,\"start\":155417},{\"end\":156007,\"start\":155993},{\"end\":156022,\"start\":156007},{\"end\":156038,\"start\":156022},{\"end\":156053,\"start\":156038},{\"end\":156062,\"start\":156053},{\"end\":156279,\"start\":156269},{\"end\":156295,\"start\":156279},{\"end\":156314,\"start\":156295},{\"end\":156329,\"start\":156314},{\"end\":156344,\"start\":156329},{\"end\":156350,\"start\":156344},{\"end\":156615,\"start\":156600},{\"end\":156632,\"start\":156615},{\"end\":156646,\"start\":156632},{\"end\":157260,\"start\":157247},{\"end\":157276,\"start\":157260},{\"end\":157293,\"start\":157276},{\"end\":157306,\"start\":157293},{\"end\":157567,\"start\":157552},{\"end\":157582,\"start\":157567},{\"end\":157597,\"start\":157582},{\"end\":157613,\"start\":157597},{\"end\":157629,\"start\":157613},{\"end\":157963,\"start\":157948},{\"end\":157979,\"start\":157963},{\"end\":157994,\"start\":157979},{\"end\":158010,\"start\":157994},{\"end\":158025,\"start\":158010},{\"end\":158268,\"start\":158253},{\"end\":158282,\"start\":158268},{\"end\":158301,\"start\":158282},{\"end\":158315,\"start\":158301},{\"end\":158328,\"start\":158315},{\"end\":158629,\"start\":158612},{\"end\":158643,\"start\":158629},{\"end\":158657,\"start\":158643},{\"end\":159357,\"start\":159334},{\"end\":159572,\"start\":159557},{\"end\":159588,\"start\":159572},{\"end\":159595,\"start\":159588},{\"end\":159610,\"start\":159595},{\"end\":159625,\"start\":159610},{\"end\":159904,\"start\":159888},{\"end\":159917,\"start\":159904},{\"end\":159929,\"start\":159917},{\"end\":159945,\"start\":159929},{\"end\":159956,\"start\":159945},{\"end\":159970,\"start\":159956},{\"end\":159984,\"start\":159970},{\"end\":159998,\"start\":159984},{\"end\":160017,\"start\":159998},{\"end\":160397,\"start\":160378},{\"end\":161235,\"start\":161231}]", "bib_venue": "[{\"end\":154428,\"start\":154413},{\"end\":155662,\"start\":155554},{\"end\":156825,\"start\":156744},{\"end\":158853,\"start\":158775},{\"end\":151709,\"start\":151664},{\"end\":152021,\"start\":151976},{\"end\":152384,\"start\":152332},{\"end\":152773,\"start\":152768},{\"end\":153102,\"start\":153064},{\"end\":153515,\"start\":153466},{\"end\":154411,\"start\":154344},{\"end\":154938,\"start\":154889},{\"end\":155552,\"start\":155429},{\"end\":156069,\"start\":156062},{\"end\":156354,\"start\":156350},{\"end\":156742,\"start\":156646},{\"end\":157245,\"start\":157168},{\"end\":157673,\"start\":157629},{\"end\":157946,\"start\":157872},{\"end\":158383,\"start\":158344},{\"end\":158773,\"start\":158686},{\"end\":159332,\"start\":159259},{\"end\":159555,\"start\":159479},{\"end\":160066,\"start\":160017},{\"end\":160412,\"start\":160397},{\"end\":160689,\"start\":160624},{\"end\":161421,\"start\":161235},{\"end\":163254,\"start\":162673},{\"end\":164877,\"start\":164713}]"}}}, "year": 2023, "month": 12, "day": 17}