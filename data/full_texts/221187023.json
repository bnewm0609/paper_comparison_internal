{"id": 221187023, "updated": "2023-11-08 03:11:26.116", "metadata": {"title": "Single Image Super-Resolution via a Holistic Attention Network", "authors": "[{\"first\":\"Ben\",\"last\":\"Niu\",\"middle\":[]},{\"first\":\"Weilei\",\"last\":\"Wen\",\"middle\":[]},{\"first\":\"Wenqi\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Xiangde\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lianping\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Shuzhen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Kaihao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiaochun\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Haifeng\",\"last\":\"Shen\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": 8, "day": 20}, "abstract": "Informative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer. However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions. Specifically, the proposed LAM adaptively emphasizes hierarchical features by considering correlations among layers. Meanwhile, CSAM learns the confidence at all the positions of each channel to selectively capture more informative features. Extensive experiments demonstrate that the proposed HAN performs favorably against the state-of-the-art single image super-resolution approaches.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2008.08767", "mag": "3096739052", "acl": null, "pubmed": null, "pubmedcentral": "7719855", "dblp": "conf/eccv/NiuWRZYWZCS20", "doi": "10.1007/978-3-030-58610-2_47"}}, "content": {"source": {"pdf_hash": "737e2998f1db3b2ad8aaa0390ac439402ce1fb23", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.08767v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.08767", "status": "GREEN"}}, "grobid": {"id": "5524ec0a3f631e40c1ee41469caf4e02c9b7ccfe", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/737e2998f1db3b2ad8aaa0390ac439402ce1fb23.txt", "contents": "\nSingle Image Super-Resolution via a Holistic Attention Network\n\n\nBen Niu \nNortheastern University\n\n\nWeilei Wen \nXidian University\n\n\nSKLOIS\nIIE\nCAS\n\nWenqi Ren \nSKLOIS\nIIE\nCAS\n\nXiangde Zhang \nNortheastern University\n\n\nLianping Yang \nNortheastern University\n\n\nShuzhen Wang \nXidian University\n\n\nKaihao Zhang \nANU 6 AI Labs\nDidi ChuxingChina\n\nXiaochun Cao \nSKLOIS\nIIE\nCAS\n\nPeng Cheng Laboratory\nCyberspace Security Research Center\nChina\n\nHaifeng Shen \nSingle Image Super-Resolution via a Holistic Attention Network\nSuper-ResolutionHolistic AttentionLayer AttentionChannel- Spatial Attention\nInformative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer. However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions. Specifically, the proposed LAM adaptively emphasizes hierarchical features by considering correlations among layers. Meanwhile, CSAM learns the confidence at all the positions of each channel to selectively capture more informative features. Extensive experiments demonstrate that the proposed HAN performs favorably against the state-ofthe-art single image super-resolution approaches.\n\nIntroduction\n\nSingle image super-resolution (SISR) is an important task in computer vision and image processing. Given a low-resolution image, the goal of super-resolution (SR) is to generate a high-resolution (HR) image with necessary edge structures and texture details. The advance of SISR will immediately benefit many application fields, such as video surveillance and pedestrian detection.\n\nSRCNN [3] is an unprecedented work to tackle the SR problem by learning the mapping function from LR input to HR output using convolutional neural networks (CNNs). Afterwards, numerous deep CNN-based methods [26,27] have been proposed in recent years and generate a significant progress. The superior reconstruction performance of CNNs based methods are mainly from deep architecture and residual learning [7]. Networks with very deep layers have Equal contribution \u2020 Corresponding author arXiv:2008.08767v1 [eess.IV] 20 Aug 2020 larger receptive fields and are able to provide a powerful capability to learn a complicated mapping between the LR input and the HR counterpart. Due to the residual learning, the depth of the SR networks are going to deeper since residual learning could efficiently alleviate the gradient vanishing and exploding problems.\n\nThough significant progress have been made, we note that the texture details of the LR image often tend to be smoothed in the super-resolved result since most existing CNN-based SR methods neglect the feature correlation of intermediate layers. Therefore, generating detailed textures is still a non-trivial problem in the SR task. Although the results obtained by using channel attention [40,2] retain some detailed information, these channel attention-based approaches struggle in preserving informative textures and restoring natural details since they treat the feature maps at different layers equally and result in lossing some detail parts in the reconstructed image.\n\nTo address these problems, we present a novel approach termed as holistic attention network (HAN) that is capable of exploring the correlations among hierarchical layers, channels of each layer, and all positions of each channel. Therefore, HAN is able to stimulate the representational power of CNNs. Specifically, we propose a layer attention module (LAM) and a channel-spatial attention module (CSAM) in the HAN for more powerful feature expression and correlation learning. These two sub-attention modules are inspired by channel attention [40] which weighs the internal features of each layer to make the network pay more attention to information-rich feature channels. However, we notice that channel attention cannot weight the features from multi-scale layers. Especially the long-term information from the shallow layers are easily weakened. Although the shallow features can be recycled via skip connections, they are treated equally with deep features across layers after long skip connection, hence hindering the representational ability of CNNs. To solve this problem, we consider exploring the interrelationship among features at hierarchical levels, and propose a layer attention module (LAM). On the other hand, channel attention neglects that the importance of different positions in each feature map varies significantly. Therefore, we also propose a channel-spatial attention module (CSAM) to collaboratively improve the discrimination ability of the proposed SR network.\n\nOur contributions in this paper are summarized as follows:\n\n\u2022 We propose a novel super-resolution algorithm named Holistic Attention Network (HAN), which enhances the representational ability of feature representations for super-resolution. \u2022 We introduce a layer attention module (LAM) to learn the weights for hierarchical features by considering correlations of multi-scale layers. Meanwhile, a channel-spatial attention module (CSAM) is presented to learn the channel and spatial interdependencies of features in each layer. \u2022 The proposed two attention modules collaboratively improve the SR results by modeling informative features among hierarchical layers, channels, and positions. Extensive experiments demonstrate that our algorithm performs favorably against the state-of-the-art SISR approaches.\n\n\nRelated Work\n\nNumerous algorithms and models have been proposed to solve the problem of image SR, which can be roughly divided into two categories. One is the traditional algorithm [35,12,11], the other one is the deep learning model based on neural network [15,4,19,22,41,16,30,31]. Due to the limitation of space, we only introduce the SR algorithms based on deep CNN. Deep CNN for super-resolution. Dong et al. [3] proposed a CNN architecture named SRCNN, which was the pioneering work to apply deep learning to single image super-resolution. Since SRCNN successfully applied deep learning network to SR task, various efficient and deeper architectures have been proposed for SR. Wang et al. [33]combined the domain knowledge of sparse coding with a deep CNN and trained a cascade network to recover images progressively. To alleviate the phenomenon of gradient explosion and reduce the complexity of the model, DRCN [16] and DRRN [30] were proposed by using a recursive convolutional network. Lai et al. [19] proposed a LapSR network which employs a pyramidal framework to progressively generate \u00d78 images by three sub-networks. Lim et al. [22] modified the ResNet [7] by removing batch normalization (BN) layers, which greatly improves the SR effect.\n\nIn addition to above MSE minimizing based methods, perceptual constraints are proposed to achieve better visual quality [28]. SRGAN [20] uses a generative adversarial networks (GAN) to predict high-resolution outputs by introducing a multi-task loss including a MSE loss, a perceptual loss [14], and an adversarial loss [5]. Zhang et al. [42] further transferred textures from reference images according to the textural similarity to enhance textures. However, the aforementioned models either result in the loss of detailed textures in intermediate features due to the very deep depth, or produce some unpleasing artifacts or inauthentic textures. In contrast, we propose a holistic attention network consists of a layer attention and a channel-spatial attention to investigate the interaction of different layers, channels, and positions.\n\nAttention mechanism. Attention mechanisms direct the operational focus of deep neural networks to areas where there is more information. In short, they help the network ignore irrelevant information and focus on important information [8,9]. Recently, attention mechanism has been successfully applied into deep CNN based image enhancement methods. Zhang et al. [40] proposed a residual channel attention network (RCAN) in which residual channel attention blocks (RCAB) allow the network to focus on the more informative channels. Woo et al. [34] proposed channel attention (CA) and spatial attention (SA) modules to exploit both inter-channel and inter-spatial relationship of feature maps. Kim et al. [17] introduced a residual attention module for SR which is composed of residual blocks and spatial channel attention for learning the interchannel and intra-channel correlations. More recently, Dai et al. [2] presented a second-order channel attention (SOCA) module to adaptively refine features using second-order feature statistics.\n\nHowever, these attention based methods only consider the channel and spatial correlations while ignore the interdependencies between multi-scale layers. To   \n\n\nHolistic Attention Network (HAN) for SR\n\nIn this section, we first present the overview of HAN network for SISR. Then we give the detailed configurations of the proposed layer attention module (LAM) and channel-spatial attention module (CSAM).\n\n\nNetwork Architecture\n\nAs shown in Figure 1, our proposed HAN consists of four parts: feature extraction, layer attention module, channel-spatial attention module, and the final reconstruction block.\n\nFeatures extraction. Given a LR input I LR , a convolutional layer is used to extract the shallow feature F 0 of the LR input\nF 0 = Conv(I LR ).\n(1)\n\nThen we use the backbone of the RCAN [40] to extract the intermediate features\nF i of the LR input F i = H RBi (F i\u22121 ), i = 1, 2, ..., N,(2)\nwhere H RBi represents the i-th residual group (RG) in the RCAN, N is the number of the residual groups. Therefore, except F N is the final output of RCAN network backbone, all other feature maps are intermediate outputs.\n\nHolistic attention. After extracting hierarchical features F i by a set of residual groups, we further conduct a holistic feature weighting, which includes: i ) layer attention of hierarchical features, and ii ) channel-spatial attention of the last layer of RCAN.\n\nThe proposed layer attention makes full use of features from all the preceding layers and can be represented as\nF L = H LA (concatenate(F 1 , F 2 , ..., F N )),(3)\nwhere H LA represents the LAM which learns the feature correlation matrix of all the features from RGs' output and then weights the fused intermediate features F i capitalized on the correlation matrix (see Section 3.2). As a results, LAM enables the high contribution feature layers to be enhanced and the redundant ones to be suppressed. In addition, channel-spatial attention aims to modulate features for adaptively capturing more important information of inter-channel and intra-channel for the final reconstruction, which can be written as\nF CS = H CSA (F N ),(4)\nwhere H CSA represents the CSAM to produce channel-spatial attention for discriminately abtaining feature information, F CS denotes the filtered features after channel-spatial attention (details can be found in Section 3.3). Although we can filter all the intermediate features of F i using CSAM, we only modulate the last feature layer of F N as a trade-off between accuracy and speed. Image reconstruction. After obtaining features from both LAM and CSAM, we integrate the layer attention and channel-spatial attention units by elementwise summation. Then, we employ the sub-pixel convolution [29] as the last upsampling module, which converts the scale sampling with a given magnification factor by pixel translation. We perform the sub-pixel convolution operation to aggregate low-resolution feature maps and simultaneously impose projection to high dimensional space to reconstruct the HR image. We formulate the process as follows\nI SR = U \u2191 (F 0 + F L + F CS ),(5)\nwhere U \u2191 represents the operation of sub-pixel convolution, and I SR is the reconstructed SR result. The long skip connection is introduced in HAN to stabilize the training of the proposed deep network, i.e., the sub-pixel upsampling block takes F 0 + F L + F CS as input.\n\nLoss function. Since we employ the RCAN network as the backbone of the proposed method, only L 1 distance is selected as our loss function as in [40] for a fair comparison\nL(\u0398) = 1 m m i=1 H HAN (I i LR ) \u2212 I i HR 1 = 1 m m i=1 I i SR \u2212 I i HR 1 ,(6)\nwhere H HAN , \u0398, and m denote the function of the proposed HAN, the learned parameter of the HAN, and the number of training pairs, respectively. Note that  we do not use other sophisticated loss functions such as adversarial loss [5] and perceptual loss [14]. We show that simply using the naive image intensity loss L(\u0398) can already achieve competitive results as demonstrated in Section 4.\n\n\nLayer Attention Module\n\nAlthough dense connections [10] and skip connections [7] allow shallow information to be bypassed to deep layers, these operations do not exploit interdependencies between the different layers. In contrast, we treat the feature maps from each layer as a response to a specific class, and the responses from different layers are related to each other. By obtaining the dependencies between features of different depths, the network can allocate different attention weights to features of different depths and automatically improve the representation ability of extracted features. Therefore, we propose an innovative LAM that learns the relationship between features of different depths, which automatically improve the feature representation ability.\n\nThe structure of the proposed layer attention is shown in Figure 2. The input of the module is the extracted intermediate feature groups F Gs, with the dimension of N \u00d7H\u00d7W \u00d7C, from N residual groups. Then, we reshape the feature groups F Gs into a 2D matrix with the dimension of N \u00d7 HW C, and apply matrix multiplication with the corresponding transpose to calculate the correlation W la = w N i,j=1 between different layers\nw j,i = \u03b4(\u03d5(F G) i \u00b7 (\u03d5(F G)) T j ), i, j = 1, 2, ..., N,(7)\nwhere \u03b4(\u00b7) and \u03d5(\u00b7) denote the softmax and reshape operations, x i,j represents the correlation index between i-th and j-th feature groups. Finally, we multiply the reshaped feature groups F Gs by the predicted correlation matrix with a scale factor \u03b1, and add the input features F Gs\nF Lj = \u03b1 N i=1 w i,j F G i + F G j ,(8)\nwhere \u03b1 is initialized to 0 and is automatically assigned by the network in the following epochs. As a result, the weighted sum of features allow the main parts of network to focus on more informative layers of the intermediate LR features. \n\n\nChannel-Spatial Attention\n\nThe existing spatial attention mechanisms [34,17] mainly focuse on the scale dimension of the feature, with little uptake of channel dimension information, while the recent channel attention mechanisms [40,41,2] ignore the scale information. To solve this problem, we propose a novel channel-spatial attention mechanism (CSAM) that contains responses from all dimensions of the feature maps. Note that although we can perform the CSAM for all the feature groups F Gs extracted from RCAN, we only modulate the last feature group of F N for a trade-off between accuracy and speed as shown in Figure 1.\n\nThe architecture of the proposed CSAM is shown in Figure 3. Given the last layer feature maps F N \u2208 R H\u00d7W \u00d7C , we feed F N to a 3D convolution layer [13] to generate attention map by capturing joint channel and spatial features. We operate the 3D convolution via convolving 3D kernels with the cube constructed from multiple neighboring channels of F N . Specifically, we perform 3D convolutions with kernel size of 3 \u00d7 3 \u00d7 3 with step size of 1 (i.e., three groups of consecutive channels are convolved with a set of 3D kernels respectively), resulting in three groups of channel-spatial attention maps W csa . By doing so, our CSAM can extract powerful representations to describe inter-channel and intra-channel information in continuous channels.\n\nIn addition, we perform element-wise multiplication with the attention map W csa and the input feature F N . Finally, multiply the weighted result by a scale factor \u03b2, and then add the input feature F N to obtain the weighted features\nF CS = \u03b2\u03c3(W csa ) F N + F N ,(9)\nwhere \u03c3(\u00b7) is the sigmoid function, is the element-wise product, the scale factor \u03b2 is initialized as 0 and progressively improved in the follow iterations. As a results, F CS is the weighted sum of all channel-spatial position features as well as the original features. Compared with conventional spatial attention and channel attention, our CSAM adaptively learns the inter-channel and intra-channel feature responses by explicitly modelling channel-wise and spatial feature interdependencies. \n\n\nExperiments\n\nIn this section, we first analyze the contributions of the proposed two attention modules. We then compare our HAN with state-of-the-art algorithms on five benchmark datasets. The implementation code will be made available to the public. Results on more images can be found in the supplementary material.\n\n\nSettings\n\nDatasets. We selecte DIV2K [32] as the training set as like in [40,2,41,22]. For the testing set, we choose five standard datasets: Set5 [1], Set14 [36], B100 [23], Urban100 [11], and Manga109 [24]. Degraded data was obtained by bilinear interpolation and Blur-downscale Degradation model. Following [40], the reconstruct RGB results by the proposed HAN are first converted to YCbCr space, and then we only consider the luminance channel to calculate PSNR and SSIM in our experiments. Implementation Details. We implement the proposed network using Py-Torch platform and use the pre-trained RCAN (\u00d72), (\u00d73), (\u00d74), (\u00d78) model to initialize the corresponding holistic attention networks, respectively. In our network, patch size is set as 64 \u00d7 64. We use ADAM [18] optimizer with a batch size 16 for training. The learning rate is set as 10 \u22125 . Default values of \u03b2 1 and \u03b2 2 are used, which are 0.9 and 0.999, respectively, and we set = 10 \u22128 . We do not use any regularization operations such as batch normalization and group normalization in our network. In addition to random rotation and translation, we do not apply other data augmentation methods in the training. The input of the LAM is selected as the outputs of all residual groups of RCAN, we use N = 10 residual groups in out network. For all the results reported in the paper, we train the network for 250 epochs, which takes about two days on an Nvidia GTX 1080Ti GPU.\n\n\nAblation Study about the Proposed LAM and CSAM\n\nThe proposed LAM and CSAM ensure that the proposed SR method generate the feature correlations between hierarchical layers, channels, and locations. One may wonder whether the LAM and CSAM help SISR. To verify the performance of these two attention mechanisms, we compare the method without using LAM and CSAM in Table 1, where we conduct experiments on the Manga109 dataset with the magnification factor of \u00d74. Table 1 shows the quantitative evaluations. Compared with the baseline method which is identical to the proposed network except for the absence of these two modules LAM and CSAM. CSAM achieves better results by up to 0.06 dB in terms of PSNR, while LAM promotes 0.16 dB on the test dataset. In addition, the improvement of using both LAM and CSAM is significant as the proposed algorithm improves 0.2 dB, which demonstrates the effectiveness of the proposed layer attention and channel-spatial attention blocks. Figure 4 further shows that using the LAM and CSAM is able to generate the results with clearer structures and details. \n\n\nAblation Study about the Number of Residual Group\n\nWe conduct an ablation study about feeding different numbers of RGs to the proposed LAM. Specifically, we apply severally three, six, and ten RGs to the LAM, and we evaluate our model on five standard datasets. As shown in Table  2, we compare our three models with RCAN, although using fewer RGs, our algorithm still generates higher PSNR values than the baseline of RCAN. This ablation study demonstrates the effectiveness of the proposed LAM.\n\n\nAblation Study about the Number of CSAM\n\nIn the paper, the channel-spatial attention module (CSAM) can extract powerful representations to describe inter-channel and intra-channel information in continuous channels. We conduct an ablation study about using different numbers of CSAM. We use one, three, five, and ten CSAMs in RGs. As shown in Table5 \n\n\nResults with Bicubic (BI) Degradation Model\n\nWe compare the proposed algorithm with 11 state-of-the-art methods: SRCNN [3], FSRCNN [4], VDSR [15], LapSRN [19], MemNet [31], SRMDNF [38], D-DBPN [6], RDN [41], EDSR [22], SRFBN [21] and SAN [2]. We provide more comparisons in supplementary material. Following [22,2,40], we also propose self-ensemble model and donate it as HAN+.\n\nQuantitative results. Table 3 shows the comparison of 2\u00d7, 3\u00d7, 4\u00d7, and 8\u00d7 SR quantitative results. Compared to existing methods, our HAN+ performs best on all the scales of reconstructed test datasets. Without using self-ensemble, our network HAN still obtains great gain compared with the recent SR methods.\n\nIn particular, our model is much better than SAN which also uses the same backbone network of RCAN and has more computationally intensive attention module. Specifically, when we compare the reconstruction results at \u00d78 scale on the Set5 dataset, the proposed HAN advances 0.11 dB in terms of PSNR than the competitive SAN.\n\nTo further evaluate the proposed HAN, we conduct experiments on the large test sets of B100, Urban100, and Manga109. Our algorithm still performs favorably against the state-of-the-art methods. For example, the super-resolved Visual results. We also show visual comparisons of various methods on the Urban100 dataset for 4\u00d7 SR in Figure 4. As shown, most compared SR networks cannot recover the grids of buildings accurately and suffer from unpleasant blurring artifacts. In contrast, the proposed HAN obtains clearer details and reconstructs sharper high-frequency textures.\n\nTake the first and fourth images in Figure 4 as example, VDSR and EDSR fail to generate the clear structures. The results generated by the recent work of RCAN, SRFBN, and SAN still contain noticeable artifacts caused by spatial aliasing. In contrast, our approach effectively suppresses such artifacts through the proposed two attention modules. As shown, our method accurately reconstructs the grid patterns on windows in the first row and the parallel straight lines on the building in the fourth image.\n\nFor 8\u00d7 SR, we also show the super-resolved results by different SR methods in Figure 5. As show, it is challenging to predict HR images from bicubicupsampled input by VDSR and EDSR. Even the state-of-the-art methods of RCAN and SRFBN cannot super-resolve the fine structures well. In contrast, our HAN reconstructs high-quality HR images for 8\u00d7 results by using cross-scale layer attention and channel-spatial attention modules on the limited information.\n\n\nResults with Blur-downscale Degradation (BD) Model\n\nQuantitative results. Following the protocols of [38,37,41], we further compare the SR results on images with blur-downscale degradation model. We compare the proposed method with nine state-of-the-art super-resolution methods: SPMSR [25], SRCNN [3], FSRCNN [4], VDSR [15], IRCNN [37], SRMD [39], RDN [41], RCAN [40],SRFBN [21] and SAN [2]. Quantitative results on the 3\u00d7 SR are reported in Table 4. As shown, both the proposed HAN and HAN+ perform favorably against existing methods. In particular, our HAN+ yields the best quantitative results and HAN obtains the second best scores for all the datasets, 0.06-0.2 dB PSNR better than the attention-based methods of RCAN and SAN and 0.2-0.8 dB better than the recently proposed SRFBN.\n\nVisual quality. In Figure 6, we show visual results on images from the Urban 100 dataset with blur-downscale degradation model by a scale factor of 3. Both the full images and the cropped regions are shown for comparison. We find that our proposed HAN is able to recover structured details that were missing in the LR image by properly exploiting the layer, channel, and spatial attention in the feature space.\n\nAs shown, VDSR and EDSR suffer from unpleasant blurring artifacts and some results even are out of shape. RCAN alleviate it to a certain extent, but still misses some details and structures. SRFBN and SAN also fail to recover these structured details. In contrast, our proposed HAN effectively suppresses artifacts and exploits the scene details and the internal natural image statistics to super-resolve the high-frequency contents.\n\n\nConclusions\n\nIn this paper, we propose a holistic attention network for single image superresolution, which adaptively learns the global dependencies among different depths, channels, and positions using the self-attention mechanism. Specifically, the layer attention module captures the long-distance dependencies among hierarchical layers. Meanwhile, the channel-spatial attention module incorporates the channel and contextual information in each layer. These two attention modules are collaboratively applied to multi-level features and then more informative features can be captured. Extensive experimental results on benchmark datasets demonstrate that the proposed model performs favorably against the state-ofthe-art SR algorithms in terms of accuracy and visual quality.\n\nFeature\n\n\nFig. 1 .\n1Network architecture of the proposed holistic attention network(HAN). Given a low-resolution image, the first convolutional layer of the HAN extracts a set of shallow feature maps. Then a series of residual groups further extract deeper feature representations of the low-resolution input. We propose a layer attention module (LAM) to learn the correlations of each output from RGs and a channel-spatial attention module (CSAM) to investigate the interdependencies between channels and pixels. Finally, an upsampling block produces the high-resolution image solve this problem, we propose a layer attention module (LAM) to exploit the nonlinear feature interactions among hierarchical layers.\n\nFig. 2 .\n2Architecture of the proposed layer attention module\n\nFig. 3 .\n3Architecture of the proposed channel-spatial attention module\n\nTable 1 .\n1Effectiveness of the proposed LAM and CSAM for image super-resolutionbaseline \nw/o CSAM \nw/o LAM \nOurs \n\nPSNR/SSIM \n31.22/0.9173 \n31.38/0.9175 \n31.28/0.9174 \n31.42/0.9177 \n\nTable 2. Ablation study about using different numbers of RGs \n\nSet5 \nSet14 \nB100 \nUrban100 \nManga100 \n\nRCAN \n32.63 \n28.87 \n27.77 \n26.82 \n31.22 \nHAN 3RGs \n32.63 \n28.89 \n27.79 \n26.82 \n31.40 \nHAN 6RGs \n32.64 \n28.90 \n27.79 \n26.84 \n31.42 \nHAN 10RGs \n32.64 \n28.90 \n27.80 \n26.85 \n31.42 \n\n\n\nTable 3 .\n3Quantitative results with BI degradation model. The best and second best results are highlighted in bold and underlinedMethods \nScale \nSet5 \nSet14 \nB100 \nUrban100 \nManga109 \nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM \nBicubic \nSRCNN [3] \nFSRCNN [4] \nVDSR [15] \nLapSRN [19] \nMemNet [31] \nEDSR [22] \nSRMDNF [38] \nD-DBPN [6] \nRDN [41] \nRCAN [40] \nSRFBN [21] \nSAN [2] \nHAN(ours) \nHAN+(ours) \n\n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\u00d72 \n\n33.66 \n36.66 \n37.05 \n37.53 \n37.52 \n37.78 \n38.11 \n37.79 \n38.09 \n38.24 \n38.27 \n38.11 \n38.31 \n38.27 \n38.33 \n\n0.9299 \n0.9542 \n0.9560 \n0.9590 \n0.9591 \n0.9597 \n0.9602 \n0.9601 \n0.9600 \n0.9614 \n0.9614 \n0.9609 \n0.9620 \n0.9614 \n0.9617 \n\n30.24 \n32.45 \n32.66 \n33.05 \n33.08 \n33.28 \n33.92 \n33.32 \n33.85 \n34.01 \n34.12 \n33.82 \n34.07 \n34.16 \n34.24 \n\n0.8688 \n0.9067 \n0.9090 \n0.9130 \n0.9130 \n0.9142 \n0.9195 \n0.9159 \n0.9190 \n0.9212 \n0.9216 \n0.9196 \n0.9213 \n0.9217 \n0.9224 \n\n29.56 \n31.36 \n31.53 \n31.90 \n31.08 \n32.08 \n32.32 \n32.05 \n32.27 \n32.34 \n32.41 \n32.29 \n32.42 \n32.41 \n32.45 \n\n0.8431 \n0.8879 \n0.8920 \n0.8960 \n0.8950 \n0.8978 \n0.9013 \n0.8985 \n0.9000 \n0.9017 \n0.9027 \n0.9010 \n0.9028 \n0.9027 \n0.9030 \n\n26.88 \n29.50 \n29.88 \n30.77 \n30.41 \n31.31 \n32.93 \n31.33 \n32.55 \n32.89 \n33.34 \n32.62 \n33.10 \n33.35 \n33.53 \n\n0.8403 \n0.8946 \n0.9020 \n0.9140 \n0.9101 \n0.9195 \n0.9351 \n0.9204 \n0.9324 \n0.9353 \n0.9384 \n0.9328 \n0.9370 \n0.9385 \n0.9398 \n\n30.80 \n35.60 \n36.67 \n37.22 \n37.27 \n37.72 \n39.10 \n38.07 \n38.89 \n39.18 \n39.44 \n39.08 \n39.32 \n39.46 \n39.62 \n\n0.9339 \n0.9663 \n0.9710 \n0.9750 \n0.9740 \n0.9740 \n0.9773 \n0.9761 \n0.9775 \n0.9780 \n0.9786 \n0.9779 \n0.9792 \n0.9785 \n0.9787 \nBicubic \nSRCNN [3] \nFSRCNN [4] \nVDSR [15] \nLapSRN [19] \nMemNet [31] \nEDSR [22] \nSRMDNF [38] \nRDN [41] \nRCAN [40] \nSRFBN [21] \nSAN [2] \nHAN(ours) \nHAN+(ours) \n\n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\n30.39 \n32.75 \n33.18 \n33.67 \n33.82 \n34.09 \n34.65 \n34.12 \n34.71 \n34.74 \n34.70 \n34.75 \n34.75 \n34.85 \n\n0.8682 \n0.9090 \n0.9140 \n0.9210 \n0.9227 \n0.9248 \n0.9280 \n0.9254 \n0.9296 \n0.9299 \n0.9292 \n0.9300 \n0.9299 \n0.9305 \n\n27.55 \n29.30 \n29.37 \n29.78 \n29.87 \n30.00 \n30.52 \n30.04 \n30.57 \n30.65 \n30.51 \n30.59 \n30.67 \n30.77 \n\n0.7742 \n0.8215 \n0.8240 \n0.8320 \n0.8320 \n0.8350 \n0.8462 \n0.8382 \n0.8468 \n0.8482 \n0.8461 \n0.8476 \n0.8483 \n0.8495 \n\n27.21 \n28.41 \n28.53 \n28.83 \n28.82 \n28.96 \n29.25 \n28.97 \n29.26 \n29.32 \n29.24 \n29.33 \n29.32 \n29.39 \n\n0.7385 \n0.7863 \n0.7910 \n0.7990 \n0.7980 \n0.8001 \n0.8093 \n0.8025 \n0.8093 \n0.8111 \n0.8084 \n0.8112 \n0.8110 \n0.8120 \n\n24.46 \n26.24 \n26.43 \n27.14 \n27.07 \n27.56 \n28.80 \n27.57 \n28.80 \n29.09 \n28.73 \n28.93 \n29.10 \n29.30 \n\n0.7349 \n0.7989 \n0.8080 \n0.8290 \n0.8280 \n0.8376 \n0.8653 \n0.8398 \n0.8653 \n0.8702 \n0.8641 \n0.8671 \n0.8705 \n0.8735 \n\n26.95 \n30.48 \n31.10 \n32.01 \n32.21 \n32.51 \n34.17 \n33.00 \n34.13 \n34.44 \n34.18 \n34.30 \n34.48 \n34.80 \n\n0.8556 \n0.9117 \n0.9210 \n0.9340 \n0.9350 \n0.9369 \n0.9476 \n0.9403 \n0.9484 \n0.9499 \n0.9481 \n0.9494 \n0.9500 \n0.9514 \nBicubic \nSRCNN [3] \nFSRCNN [4] \nVDSR [15] \nLapSRN [19] \nMemNet [31] \nEDSR [22] \nSRMDNF [38] \nD-DBPN [6] \nRDN [41] \nRCAN [40] \nSRFBN [21] \nSAN [2] \nHAN(ours) \nHAN+(ours) \n\n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\u00d74 \n\n28.42 \n30.48 \n30.72 \n31.35 \n31.54 \n31.74 \n32.46 \n31.96 \n32.47 \n32.47 \n32.63 \n32.47 \n32.64 \n32.64 \n32.75 \n\n0.8104 \n0.8628 \n0.8660 \n0.8830 \n0.8850 \n0.8893 \n0.8968 \n0.8925 \n0.8980 \n0.8990 \n0.9002 \n0.8983 \n0.9003 \n0.9002 \n0.9016 \n\n26.00 \n27.50 \n27.61 \n28.02 \n28.19 \n28.26 \n28.80 \n28.35 \n28.82 \n28.81 \n28.87 \n28.81 \n28.92 \n28.90 \n28.99 \n\n0.7027 \n0.7513 \n0.7550 \n0.7680 \n0.7720 \n0.7723 \n0.7876 \n0.7787 \n0.7860 \n0.7871 \n0.7889 \n0.7868 \n0.7888 \n0.7890 \n0.7907 \n\n25.96 \n26.90 \n26.98 \n27.29 \n27.32 \n27.40 \n27.71 \n27.49 \n27.72 \n27.72 \n27.77 \n27.72 \n27.78 \n27.80 \n27.85 \n\n0.6675 \n0.7101 \n0.7150 \n0.0726 \n0.7270 \n0.7281 \n0.7420 \n0.7337 \n0.7400 \n0.7419 \n0.7436 \n0.7409 \n0.7436 \n0.7442 \n0.7454 \n\n23.14 \n24.52 \n24.62 \n25.18 \n25.21 \n25.50 \n26.64 \n25.68 \n26.38 \n26.61 \n26.82 \n26.60 \n26.79 \n26.85 \n27.02 \n\n0.6577 \n0.7221 \n0.7280 \n0.7540 \n0.7560 \n0.7630 \n0.8033 \n0.7731 \n0.7946 \n0.8028 \n0.8087 \n0.8015 \n0.8068 \n0.8094 \n0.8131 \n\n24.89 \n27.58 \n27.90 \n28.83 \n29.09 \n29.42 \n31.02 \n30.09 \n30.91 \n31.00 \n31.22 \n31.15 \n31.18 \n31.42 \n31.73 \n\n0.7866 \n0.8555 \n0.8610 \n0.8870 \n0.8900 \n0.8942 \n0.9148 \n0.9024 \n0.9137 \n0.9151 \n0.9173 \n0.9160 \n0.9169 \n0.9177 \n0.9207 \nBicubic \nSRCNN [3] \nFSRCNN [4] \nSCN [33] \nVDSR [15] \nLapSRN [19] \nMemNet [31] \nMSLapSRN[19] \nEDSR [22] \nD-DBPN [6] \nRCAN [40] \nSAN [2] \nHAN(ours) \nHAN+(ours) \n\n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\u00d78 \n\n24.40 \n25.33 \n20.13 \n25.59 \n25.93 \n26.15 \n26.16 \n26.34 \n26.96 \n27.21 \n27.31 \n27.22 \n27.33 \n27.47 \n\n0.6580 \n0.6900 \n0.5520 \n0.7071 \n0.7240 \n0.7380 \n0.7414 \n0.7558 \n0.7762 \n0.7840 \n0.7878 \n0.7829 \n0.7884 \n0.7920 \n\n23.10 \n23.76 \n19.75 \n24.02 \n24.26 \n24.35 \n24.38 \n24.57 \n24.91 \n25.13 \n25.23 \n25.14 \n25.24 \n25.39 \n\n0.5660 \n0.5910 \n0.4820 \n0.6028 \n0.6140 \n0.6200 \n0.6199 \n0.6273 \n0.6420 \n0.6480 \n0.6511 \n0.6476 \n0.6510 \n0.6552 \n\n23.67 \n24.13 \n24.21 \n24.30 \n24.49 \n24.54 \n24.58 \n24.65 \n24.81 \n24.88 \n24.98 \n24.88 \n24.98 \n25.04 \n\n0.5480 \n0.5660 \n0.5680 \n0.5698 \n0.5830 \n0.5860 \n0.5842 \n0.5895 \n0.5985 \n0.6010 \n0.6058 \n0.6011 \n0.6059 \n0.6075 \n\n20.74 \n21.29 \n21.32 \n21.52 \n21.70 \n21.81 \n21.89 \n22.06 \n22.51 \n22.73 \n23.00 \n22.70 \n22.98 \n23.20 \n\n0.5160 \n0.5440 \n0.5380 \n0.5571 \n0.5710 \n0.5810 \n0.5825 \n0.5963 \n0.6221 \n0.6312 \n0.6452 \n0.6314 \n0.6437 \n0.6518 \n\n21.47 \n22.46 \n22.39 \n22.68 \n23.16 \n23.39 \n23.56 \n23.90 \n24.69 \n25.14 \n25.24 \n24.85 \n25.20 \n25.54 \n\n0.6500 \n0.6950 \n0.6730 \n0.6963 \n0.7250 \n0.7350 \n0.7387 \n0.7564 \n0.7841 \n0.7987 \n0.8029 \n0.7906 \n0.8011 \n0.8080 \n\n\nTable 4 .\n4Quantitative results with BD degradation model. The best and second best results are highlighted in bold and underlined SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIMTable 5. Ablation study about using different numbers of CSAMs results by the proposed HAN is 0.06 dB and 0.35 dB higher than the very recent work of SAN for the 4\u00d7 and 8\u00d7 scales, respectively.Method \nScale \nSet5 \nSet14 \nB100 \nUrban100 \nManga109 \nPSNR Bicubic \nSPMSR [25] \nSRCNN [3] \nFSRCNN [4] \nVDSR [15] \nIRCNN [37] \nSRMDNF [38] \nRDN [41] \nRCAN [40] \nSRFBN [21] \nSAN [2] \nHAN(ours) \nHAN+(ours) \n\n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\u00d73 \n\n28.78 \n32.21 \n32.05 \n26.23 \n33.25 \n33.38 \n34.01 \n34.58 \n34.70 \n34.66 \n34.75 \n34.76 \n34.85 \n\n0.8308 \n0.9001 \n0.8944 \n0.8124 \n0.9150 \n0.9182 \n0.9242 \n0.9280 \n0.9288 \n0.9283 \n0.9290 \n0.9294 \n0.9300 \n\n26.38 \n28.89 \n28.80 \n24.44 \n29.46 \n29.63 \n30.11 \n30.53 \n30.63 \n30.48 \n30.68 \n30.70 \n30.79 \n\n0.7271 \n0.8105 \n0.8074 \n0.7106 \n0.8244 \n0.8281 \n0.8364 \n0.8447 \n0.8462 \n0.8439 \n0.8466 \n0.8475 \n0.8487 \n\n26.33 \n28.13 \n28.13 \n24.86 \n28.57 \n28.65 \n28.98 \n29.23 \n29.32 \n29.21 \n29.33 \n29.34 \n29.41 \n\n0.6918 \n0.7740 \n0.7736 \n0.6832 \n0.7893 \n0.7922 \n0.8009 \n0.8079 \n0.8093 \n0.8069 \n0.8101 \n0.8106 \n0.8116 \n\n23.52 \n25.84 \n25.70 \n22.04 \n26.61 \n26.77 \n27.50 \n28.46 \n28.81 \n28.48 \n28.83 \n28.99 \n29.21 \n\n0.6862 \n0.7856 \n0.7770 \n0.6745 \n0.8136 \n0.8154 \n0.8370 \n0.8582 \n0.8647 \n0.8581 \n0.8646 \n0.8676 \n0.8710 \n\n25.46 \n29.64 \n29.47 \n23.04 \n31.06 \n31.15 \n32.97 \n33.97 \n34.38 \n34.07 \n34.46 \n34.56 \n34.87 \n\n0.8149 \n0.9003 \n0.8924 \n0.7927 \n0.9234 \n0.9245 \n0.9391 \n0.9465 \n0.9483 \n0.9466 \n0.9487 \n0.9494 \n0.9509 \n\nSet5 \nSet14 \nB100 \nUrban100 \nManga100 \n\nHAN(1 CSAM) \n32.64 \n28.90 \n27.80 \n26.85 \n31.42 \nHAN(3 CSAM) \n32.67 \n28.91 \n27.80 \n26.89 \n31.46 \nHAN(5 CSAM) \n32.69 \n28.91 \n27.80 \n26.89 \n31.43 \nHAN(10 CSAM) \n32.67 \n28.91 \n27.80 \n26.89 \n31.43 \n\n\n\nLow-complexity single-image super-resolution based on nonnegative neighbor embedding. M Bevilacqua, A Roumy, C Guillemot, M L Alberi-Morel, BMVCBevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In: BMVC (2012)\n\nSecond-order attention network for single image super-resolution. T Dai, J Cai, Y Zhang, S T Xia, L Zhang, CVPRDai, T., Cai, J., Zhang, Y., Xia, S.T., Zhang, L.: Second-order attention network for single image super-resolution. In: CVPR (2019)\n\nLearning a deep convolutional network for image super-resolution. C Dong, C C Loy, K He, X Tang, ECCVDong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for image super-resolution. In: ECCV (2014)\n\nAccelerating the super-resolution convolutional neural network. C Dong, C C Loy, X Tang, ECCVDong, C., Loy, C.C., Tang, X.: Accelerating the super-resolution convolutional neural network. In: ECCV (2016)\n\nI Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Generative adversarial nets. NIPSGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS (2014)\n\nDeep back-projection networks for superresolution. M Haris, G Shakhnarovich, N Ukita, CVPRHaris, M., Shakhnarovich, G., Ukita, N.: Deep back-projection networks for super- resolution. In: CVPR (2018)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPRHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, CVPRHu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: CVPR (2018)\n\nChannel-wise and spatial feature modulation network for single image super-resolution. Y Hu, J Li, Y Huang, X Gao, IEEE Transactions on Circuits and Systems for Video Technology. Hu, Y., Li, J., Huang, Y., Gao, X.: Channel-wise and spatial feature modulation network for single image super-resolution. IEEE Transactions on Circuits and Sys- tems for Video Technology (2019)\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, CVPRHuang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: CVPR (2017)\n\nSingle image super-resolution from transformed self-exemplars. J B Huang, A Singh, N Ahuja, CVPRHuang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed self-exemplars. In: CVPR (2015)\n\nRobust single-image super-resolution based on adaptive edge-preserving smoothing regularization. S Huang, J Sun, Y Yang, Y Fang, P Lin, Y Que, TIP. 276Huang, S., Sun, J., Yang, Y., Fang, Y., Lin, P., Que, Y.: Robust single-image super-resolution based on adaptive edge-preserving smoothing regularization. TIP 27(6), 2650-2663 (2018)\n\n3d convolutional neural networks for human action recognition. S Ji, W Xu, M Yang, K Yu, TPAMI. 351Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. TPAMI 35(1), 221-231 (2012)\n\nPerceptual losses for real-time style transfer and super-resolution. J Johnson, A Alahi, L Fei-Fei, ECCVJohnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: ECCV (2016)\n\nAccurate image super-resolution using very deep convolutional networks. J Kim, J Kwon Lee, K Mu Lee, CVPRKim, J., Kwon Lee, J., Mu Lee, K.: Accurate image super-resolution using very deep convolutional networks. In: CVPR (2016)\n\nDeeply-recursive convolutional network for image super-resolution. J Kim, J Kwon Lee, K Mu Lee, CVPRKim, J., Kwon Lee, J., Mu Lee, K.: Deeply-recursive convolutional network for image super-resolution. In: CVPR (2016)\n\nRam: Residual attention module for single image super-resolution. J H Kim, J H Choi, M Cheon, J S Lee, arXiv:1811.12043arXiv preprintKim, J.H., Choi, J.H., Cheon, M., Lee, J.S.: Ram: Residual attention module for single image super-resolution. arXiv preprint arXiv:1811.12043 (2018)\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\nDeep laplacian pyramid networks for fast and accurate super-resolution. W S Lai, J B Huang, N Ahuja, M H Yang, CVPRLai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Deep laplacian pyramid networks for fast and accurate super-resolution. In: CVPR (2017)\n\nPhoto-realistic single image superresolution using a generative adversarial network. C Ledig, L Theis, F Husz\u00e1r, J Caballero, A Cunningham, A Acosta, A Aitken, A Tejani, J Totz, Z Wang, CVPRLedig, C., Theis, L., Husz\u00e1r, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al.: Photo-realistic single image super- resolution using a generative adversarial network. In: CVPR (2017)\n\nFeedback network for image super-resolution. Z Li, J Yang, Z Liu, X Yang, G Jeon, W Wu, CVPRLi, Z., Yang, J., Liu, Z., Yang, X., Jeon, G., Wu, W.: Feedback network for image super-resolution. In: CVPR (2019)\n\nEnhanced deep residual networks for single image super-resolution. B Lim, S Son, H Kim, S Nah, K Mu Lee, CVPRLim, B., Son, S., Kim, H., Nah, S., Mu Lee, K.: Enhanced deep residual networks for single image super-resolution. In: CVPR (2017)\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. D Martin, C Fowlkes, D Tal, J Malik, ICCVMartin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In: ICCV (2001)\n\nSketch-based manga retrieval using manga109 dataset. Y Matsui, K Ito, Y Aramaki, A Fujimoto, T Ogawa, T Yamasaki, K Aizawa, Multimedia Tools and Applications. 7620Matsui, Y., Ito, K., Aramaki, Y., Fujimoto, A., Ogawa, T., Yamasaki, T., Aizawa, K.: Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications 76(20), 21811-21838 (2017)\n\nA statistical prediction model based on sparse representations for single image super-resolution. T Peleg, M Elad, TIP. 236Peleg, T., Elad, M.: A statistical prediction model based on sparse representations for single image super-resolution. TIP 23(6), 2569-2582 (2014)\n\nFace video deblurring using 3d facial priors. W Ren, J Yang, S Deng, D Wipf, X Cao, X Tong, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRen, W., Yang, J., Deng, S., Wipf, D., Cao, X., Tong, X.: Face video deblurring using 3d facial priors. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 9388-9397 (2019)\n\nDeep non-blind deconvolution via generalized low-rank approximation. W Ren, J Zhang, L Ma, J Pan, X Cao, W Zuo, W Liu, M H Yang, Advances in Neural Information Processing Systems. Ren, W., Zhang, J., Ma, L., Pan, J., Cao, X., Zuo, W., Liu, W., Yang, M.H.: Deep non-blind deconvolution via generalized low-rank approximation. In: Advances in Neural Information Processing Systems. pp. 297-307 (2018)\n\nEnhancenet: Single image super-resolution through automated texture synthesis. M S Sajjadi, B Scholkopf, M Hirsch, Sajjadi, M.S., Scholkopf, B., Hirsch, M.: Enhancenet: Single image super-resolution through automated texture synthesis. In: ICCV (2017)\n\nReal-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. W Shi, J Caballero, F Husz\u00e1r, J Totz, A P Aitken, R Bishop, D Rueckert, Z Wang, CVPRShi, W., Caballero, J., Husz\u00e1r, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert, D., Wang, Z.: Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In: CVPR (2016)\n\nImage super-resolution via deep recursive residual network. Y Tai, J Yang, X Liu, CVPRTai, Y., Yang, J., Liu, X.: Image super-resolution via deep recursive residual net- work. In: CVPR (2017)\n\nMemnet: A persistent memory network for image restoration. Y Tai, J Yang, X Liu, C Xu, Tai, Y., Yang, J., Liu, X., Xu, C.: Memnet: A persistent memory network for image restoration. In: ICCV (2017)\n\nNtire 2017 challenge on single image super-resolution: Methods and results. R Timofte, E Agustsson, L Van Gool, M H Yang, L Zhang, CVPRWTimofte, R., Agustsson, E., Van Gool, L., Yang, M.H., Zhang, L.: Ntire 2017 chal- lenge on single image super-resolution: Methods and results. In: CVPRW (2017)\n\nDeep networks for image superresolution with sparse prior. Z Wang, D Liu, J Yang, W Han, T Huang, ICCVWang, Z., Liu, D., Yang, J., Han, W., Huang, T.: Deep networks for image super- resolution with sparse prior. In: ICCV (2015)\n\nCbam: Convolutional block attention module. S Woo, J Park, J Y Lee, I So Kweon, ECCVWoo, S., Park, J., Lee, J.Y., So Kweon, I.: Cbam: Convolutional block attention module. In: ECCV (2018)\n\nImage super-resolution as sparse representation of raw image patches. J Yang, J Wright, T Huang, Y Ma, CVPRYang, J., Wright, J., Huang, T., Ma, Y.: Image super-resolution as sparse repre- sentation of raw image patches. In: CVPR (2008)\n\nOn single image scale-up using sparserepresentations. R Zeyde, M Elad, M Protter, International conference on curves and surfaces. Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse- representations. In: International conference on curves and surfaces (2010)\n\nLearning deep cnn denoiser prior for image restoration. K Zhang, W Zuo, S Gu, L Zhang, CVPRZhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image restoration. In: CVPR (2017)\n\nLearning a single convolutional super-resolution network for multiple degradations. K Zhang, W Zuo, L Zhang, CVPRZhang, K., Zuo, W., Zhang, L.: Learning a single convolutional super-resolution network for multiple degradations. In: CVPR (2018)\n\nAn edge-guided image interpolation algorithm via directional filtering and data fusion. L Zhang, X Wu, TIP. 158Zhang, L., Wu, X.: An edge-guided image interpolation algorithm via directional filtering and data fusion. TIP 15(8), 2226-2238 (2006)\n\nImage super-resolution using very deep residual channel attention networks. Y Zhang, K Li, K Li, L Wang, B Zhong, Y Fu, ECCVZhang, Y., Li, K., Li, K., Wang, L., Zhong, B., Fu, Y.: Image super-resolution using very deep residual channel attention networks. In: ECCV (2018)\n\nResidual dense network for image super-resolution. Y Zhang, Y Tian, Y Kong, B Zhong, Y Fu, CVPRZhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for image super-resolution. In: CVPR (2018)\n\nImage super-resolution by neural texture transfer. Z Zhang, Z Wang, Z Lin, H Qi, CVPRZhang, Z., Wang, Z., Lin, Z., Qi, H.: Image super-resolution by neural texture transfer. In: CVPR (2019)\n", "annotations": {"author": "[{\"end\":100,\"start\":66},{\"end\":148,\"start\":101},{\"end\":175,\"start\":149},{\"end\":216,\"start\":176},{\"end\":257,\"start\":217},{\"end\":291,\"start\":258},{\"end\":338,\"start\":292},{\"end\":433,\"start\":339},{\"end\":447,\"start\":434}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":70},{\"end\":111,\"start\":108},{\"end\":158,\"start\":155},{\"end\":189,\"start\":184},{\"end\":230,\"start\":226},{\"end\":270,\"start\":266},{\"end\":304,\"start\":299},{\"end\":351,\"start\":348},{\"end\":446,\"start\":442}]", "author_first_name": "[{\"end\":69,\"start\":66},{\"end\":107,\"start\":101},{\"end\":154,\"start\":149},{\"end\":183,\"start\":176},{\"end\":225,\"start\":217},{\"end\":265,\"start\":258},{\"end\":298,\"start\":292},{\"end\":347,\"start\":339},{\"end\":441,\"start\":434}]", "author_affiliation": "[{\"end\":99,\"start\":75},{\"end\":131,\"start\":113},{\"end\":147,\"start\":133},{\"end\":174,\"start\":160},{\"end\":215,\"start\":191},{\"end\":256,\"start\":232},{\"end\":290,\"start\":272},{\"end\":337,\"start\":306},{\"end\":367,\"start\":353},{\"end\":432,\"start\":369}]", "title": "[{\"end\":63,\"start\":1},{\"end\":510,\"start\":448}]", "venue": null, "abstract": "[{\"end\":1551,\"start\":587}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1959,\"start\":1956},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2162,\"start\":2158},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2165,\"start\":2162},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2359,\"start\":2356},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3198,\"start\":3194},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3200,\"start\":3198},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4029,\"start\":4025},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5968,\"start\":5964},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5971,\"start\":5968},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5974,\"start\":5971},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6045,\"start\":6041},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6047,\"start\":6045},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6050,\"start\":6047},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6053,\"start\":6050},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6056,\"start\":6053},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6059,\"start\":6056},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6062,\"start\":6059},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6065,\"start\":6062},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6200,\"start\":6197},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6482,\"start\":6478},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6707,\"start\":6703},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6721,\"start\":6717},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6795,\"start\":6791},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6931,\"start\":6927},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6955,\"start\":6952},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7164,\"start\":7160},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7176,\"start\":7172},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7334,\"start\":7330},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7363,\"start\":7360},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7382,\"start\":7378},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8119,\"start\":8116},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8121,\"start\":8119},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8247,\"start\":8243},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8427,\"start\":8423},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8588,\"start\":8584},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8793,\"start\":8790},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9719,\"start\":9715},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11642,\"start\":11638},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12439,\"start\":12435},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12775,\"start\":12772},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12800,\"start\":12796},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12991,\"start\":12987},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13016,\"start\":13013},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14841,\"start\":14837},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14844,\"start\":14841},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15001,\"start\":14997},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15004,\"start\":15001},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15006,\"start\":15004},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15549,\"start\":15545},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17276,\"start\":17272},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17312,\"start\":17308},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17314,\"start\":17312},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17317,\"start\":17314},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17320,\"start\":17317},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17385,\"start\":17382},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17397,\"start\":17393},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17408,\"start\":17404},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17423,\"start\":17419},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17442,\"start\":17438},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17549,\"start\":17545},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18007,\"start\":18003},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20747,\"start\":20744},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20759,\"start\":20756},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20770,\"start\":20766},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20783,\"start\":20779},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20796,\"start\":20792},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20809,\"start\":20805},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20821,\"start\":20818},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20831,\"start\":20827},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20842,\"start\":20838},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20854,\"start\":20850},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20866,\"start\":20863},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20937,\"start\":20933},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20939,\"start\":20937},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20942,\"start\":20939},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23284,\"start\":23280},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23287,\"start\":23284},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23290,\"start\":23287},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23469,\"start\":23465},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23480,\"start\":23477},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23492,\"start\":23489},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23503,\"start\":23499},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23515,\"start\":23511},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23526,\"start\":23522},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23536,\"start\":23532},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23547,\"start\":23543},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23558,\"start\":23554},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23570,\"start\":23567}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25605,\"start\":25596},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26309,\"start\":25606},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26372,\"start\":26310},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26445,\"start\":26373},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":26912,\"start\":26446},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32433,\"start\":26913},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34280,\"start\":32434}]", "paragraph": "[{\"end\":1948,\"start\":1567},{\"end\":2803,\"start\":1950},{\"end\":3479,\"start\":2805},{\"end\":4971,\"start\":3481},{\"end\":5031,\"start\":4973},{\"end\":5780,\"start\":5033},{\"end\":7038,\"start\":5797},{\"end\":7880,\"start\":7040},{\"end\":8919,\"start\":7882},{\"end\":9079,\"start\":8921},{\"end\":9325,\"start\":9123},{\"end\":9526,\"start\":9350},{\"end\":9653,\"start\":9528},{\"end\":9676,\"start\":9673},{\"end\":9756,\"start\":9678},{\"end\":10041,\"start\":9820},{\"end\":10307,\"start\":10043},{\"end\":10420,\"start\":10309},{\"end\":11018,\"start\":10473},{\"end\":11979,\"start\":11043},{\"end\":12288,\"start\":12015},{\"end\":12461,\"start\":12290},{\"end\":12933,\"start\":12541},{\"end\":13710,\"start\":12960},{\"end\":14137,\"start\":13712},{\"end\":14483,\"start\":14199},{\"end\":14765,\"start\":14524},{\"end\":15394,\"start\":14795},{\"end\":16146,\"start\":15396},{\"end\":16382,\"start\":16148},{\"end\":16912,\"start\":16416},{\"end\":17232,\"start\":16928},{\"end\":18675,\"start\":17245},{\"end\":19770,\"start\":18726},{\"end\":20269,\"start\":19824},{\"end\":20622,\"start\":20313},{\"end\":21002,\"start\":20670},{\"end\":21311,\"start\":21004},{\"end\":21635,\"start\":21313},{\"end\":22212,\"start\":21637},{\"end\":22719,\"start\":22214},{\"end\":23176,\"start\":22721},{\"end\":23966,\"start\":23231},{\"end\":24378,\"start\":23968},{\"end\":24813,\"start\":24380},{\"end\":25595,\"start\":24829}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9672,\"start\":9654},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9819,\"start\":9757},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10472,\"start\":10421},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11042,\"start\":11019},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12014,\"start\":11980},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12540,\"start\":12462},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14198,\"start\":14138},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14523,\"start\":14484},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16415,\"start\":16383}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19046,\"start\":19039},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19145,\"start\":19138},{\"end\":20055,\"start\":20047},{\"end\":20621,\"start\":20615},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21033,\"start\":21026},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":23629,\"start\":23622}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1565,\"start\":1553},{\"attributes\":{\"n\":\"2\"},\"end\":5795,\"start\":5783},{\"attributes\":{\"n\":\"3\"},\"end\":9121,\"start\":9082},{\"attributes\":{\"n\":\"3.1\"},\"end\":9348,\"start\":9328},{\"attributes\":{\"n\":\"3.2\"},\"end\":12958,\"start\":12936},{\"attributes\":{\"n\":\"3.3\"},\"end\":14793,\"start\":14768},{\"attributes\":{\"n\":\"4\"},\"end\":16926,\"start\":16915},{\"attributes\":{\"n\":\"4.1\"},\"end\":17243,\"start\":17235},{\"attributes\":{\"n\":\"4.2\"},\"end\":18724,\"start\":18678},{\"attributes\":{\"n\":\"4.3\"},\"end\":19822,\"start\":19773},{\"attributes\":{\"n\":\"4.4\"},\"end\":20311,\"start\":20272},{\"attributes\":{\"n\":\"4.5\"},\"end\":20668,\"start\":20625},{\"attributes\":{\"n\":\"4.6\"},\"end\":23229,\"start\":23179},{\"attributes\":{\"n\":\"5\"},\"end\":24827,\"start\":24816},{\"end\":25604,\"start\":25597},{\"end\":25615,\"start\":25607},{\"end\":26319,\"start\":26311},{\"end\":26382,\"start\":26374},{\"end\":26456,\"start\":26447},{\"end\":26923,\"start\":26914},{\"end\":32444,\"start\":32435}]", "table": "[{\"end\":26912,\"start\":26527},{\"end\":32433,\"start\":27044},{\"end\":34280,\"start\":32803}]", "figure_caption": "[{\"end\":26309,\"start\":25617},{\"end\":26372,\"start\":26321},{\"end\":26445,\"start\":26384},{\"end\":26527,\"start\":26458},{\"end\":27044,\"start\":26925},{\"end\":32803,\"start\":32446}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9370,\"start\":9362},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13778,\"start\":13770},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15393,\"start\":15385},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15454,\"start\":15446},{\"end\":19658,\"start\":19650},{\"end\":21975,\"start\":21967},{\"end\":22258,\"start\":22250},{\"end\":22807,\"start\":22799},{\"end\":23995,\"start\":23987}]", "bib_author_first_name": "[{\"end\":34369,\"start\":34368},{\"end\":34383,\"start\":34382},{\"end\":34392,\"start\":34391},{\"end\":34405,\"start\":34404},{\"end\":34407,\"start\":34406},{\"end\":34658,\"start\":34657},{\"end\":34665,\"start\":34664},{\"end\":34672,\"start\":34671},{\"end\":34681,\"start\":34680},{\"end\":34683,\"start\":34682},{\"end\":34690,\"start\":34689},{\"end\":34903,\"start\":34902},{\"end\":34911,\"start\":34910},{\"end\":34913,\"start\":34912},{\"end\":34920,\"start\":34919},{\"end\":34926,\"start\":34925},{\"end\":35124,\"start\":35123},{\"end\":35132,\"start\":35131},{\"end\":35134,\"start\":35133},{\"end\":35141,\"start\":35140},{\"end\":35265,\"start\":35264},{\"end\":35279,\"start\":35278},{\"end\":35296,\"start\":35295},{\"end\":35305,\"start\":35304},{\"end\":35311,\"start\":35310},{\"end\":35327,\"start\":35326},{\"end\":35336,\"start\":35335},{\"end\":35349,\"start\":35348},{\"end\":35599,\"start\":35598},{\"end\":35608,\"start\":35607},{\"end\":35625,\"start\":35624},{\"end\":35795,\"start\":35794},{\"end\":35801,\"start\":35800},{\"end\":35810,\"start\":35809},{\"end\":35817,\"start\":35816},{\"end\":35961,\"start\":35960},{\"end\":35967,\"start\":35966},{\"end\":35975,\"start\":35974},{\"end\":36150,\"start\":36149},{\"end\":36156,\"start\":36155},{\"end\":36162,\"start\":36161},{\"end\":36171,\"start\":36170},{\"end\":36480,\"start\":36479},{\"end\":36489,\"start\":36488},{\"end\":36496,\"start\":36495},{\"end\":36514,\"start\":36513},{\"end\":36516,\"start\":36515},{\"end\":36714,\"start\":36713},{\"end\":36716,\"start\":36715},{\"end\":36725,\"start\":36724},{\"end\":36734,\"start\":36733},{\"end\":36959,\"start\":36958},{\"end\":36968,\"start\":36967},{\"end\":36975,\"start\":36974},{\"end\":36983,\"start\":36982},{\"end\":36991,\"start\":36990},{\"end\":36998,\"start\":36997},{\"end\":37260,\"start\":37259},{\"end\":37266,\"start\":37265},{\"end\":37272,\"start\":37271},{\"end\":37280,\"start\":37279},{\"end\":37491,\"start\":37490},{\"end\":37502,\"start\":37501},{\"end\":37511,\"start\":37510},{\"end\":37721,\"start\":37720},{\"end\":37728,\"start\":37727},{\"end\":37740,\"start\":37739},{\"end\":37945,\"start\":37944},{\"end\":37952,\"start\":37951},{\"end\":37964,\"start\":37963},{\"end\":38163,\"start\":38162},{\"end\":38165,\"start\":38164},{\"end\":38172,\"start\":38171},{\"end\":38174,\"start\":38173},{\"end\":38182,\"start\":38181},{\"end\":38191,\"start\":38190},{\"end\":38193,\"start\":38192},{\"end\":38425,\"start\":38424},{\"end\":38427,\"start\":38426},{\"end\":38437,\"start\":38436},{\"end\":38649,\"start\":38648},{\"end\":38651,\"start\":38650},{\"end\":38658,\"start\":38657},{\"end\":38660,\"start\":38659},{\"end\":38669,\"start\":38668},{\"end\":38678,\"start\":38677},{\"end\":38680,\"start\":38679},{\"end\":38913,\"start\":38912},{\"end\":38922,\"start\":38921},{\"end\":38931,\"start\":38930},{\"end\":38941,\"start\":38940},{\"end\":38954,\"start\":38953},{\"end\":38968,\"start\":38967},{\"end\":38978,\"start\":38977},{\"end\":38988,\"start\":38987},{\"end\":38998,\"start\":38997},{\"end\":39006,\"start\":39005},{\"end\":39296,\"start\":39295},{\"end\":39302,\"start\":39301},{\"end\":39310,\"start\":39309},{\"end\":39317,\"start\":39316},{\"end\":39325,\"start\":39324},{\"end\":39333,\"start\":39332},{\"end\":39527,\"start\":39526},{\"end\":39534,\"start\":39533},{\"end\":39541,\"start\":39540},{\"end\":39548,\"start\":39547},{\"end\":39555,\"start\":39554},{\"end\":39841,\"start\":39840},{\"end\":39851,\"start\":39850},{\"end\":39862,\"start\":39861},{\"end\":39869,\"start\":39868},{\"end\":40137,\"start\":40136},{\"end\":40147,\"start\":40146},{\"end\":40154,\"start\":40153},{\"end\":40165,\"start\":40164},{\"end\":40177,\"start\":40176},{\"end\":40186,\"start\":40185},{\"end\":40198,\"start\":40197},{\"end\":40545,\"start\":40544},{\"end\":40554,\"start\":40553},{\"end\":40764,\"start\":40763},{\"end\":40771,\"start\":40770},{\"end\":40779,\"start\":40778},{\"end\":40787,\"start\":40786},{\"end\":40795,\"start\":40794},{\"end\":40802,\"start\":40801},{\"end\":41199,\"start\":41198},{\"end\":41206,\"start\":41205},{\"end\":41215,\"start\":41214},{\"end\":41221,\"start\":41220},{\"end\":41228,\"start\":41227},{\"end\":41235,\"start\":41234},{\"end\":41242,\"start\":41241},{\"end\":41249,\"start\":41248},{\"end\":41251,\"start\":41250},{\"end\":41609,\"start\":41608},{\"end\":41611,\"start\":41610},{\"end\":41622,\"start\":41621},{\"end\":41635,\"start\":41634},{\"end\":41892,\"start\":41891},{\"end\":41899,\"start\":41898},{\"end\":41912,\"start\":41911},{\"end\":41922,\"start\":41921},{\"end\":41930,\"start\":41929},{\"end\":41932,\"start\":41931},{\"end\":41942,\"start\":41941},{\"end\":41952,\"start\":41951},{\"end\":41964,\"start\":41963},{\"end\":42258,\"start\":42257},{\"end\":42265,\"start\":42264},{\"end\":42273,\"start\":42272},{\"end\":42450,\"start\":42449},{\"end\":42457,\"start\":42456},{\"end\":42465,\"start\":42464},{\"end\":42472,\"start\":42471},{\"end\":42666,\"start\":42665},{\"end\":42677,\"start\":42676},{\"end\":42690,\"start\":42689},{\"end\":42702,\"start\":42701},{\"end\":42704,\"start\":42703},{\"end\":42712,\"start\":42711},{\"end\":42946,\"start\":42945},{\"end\":42954,\"start\":42953},{\"end\":42961,\"start\":42960},{\"end\":42969,\"start\":42968},{\"end\":42976,\"start\":42975},{\"end\":43160,\"start\":43159},{\"end\":43167,\"start\":43166},{\"end\":43175,\"start\":43174},{\"end\":43177,\"start\":43176},{\"end\":43184,\"start\":43183},{\"end\":43375,\"start\":43374},{\"end\":43383,\"start\":43382},{\"end\":43393,\"start\":43392},{\"end\":43402,\"start\":43401},{\"end\":43596,\"start\":43595},{\"end\":43605,\"start\":43604},{\"end\":43613,\"start\":43612},{\"end\":43879,\"start\":43878},{\"end\":43888,\"start\":43887},{\"end\":43895,\"start\":43894},{\"end\":43901,\"start\":43900},{\"end\":44110,\"start\":44109},{\"end\":44119,\"start\":44118},{\"end\":44126,\"start\":44125},{\"end\":44359,\"start\":44358},{\"end\":44368,\"start\":44367},{\"end\":44594,\"start\":44593},{\"end\":44603,\"start\":44602},{\"end\":44609,\"start\":44608},{\"end\":44615,\"start\":44614},{\"end\":44623,\"start\":44622},{\"end\":44632,\"start\":44631},{\"end\":44842,\"start\":44841},{\"end\":44851,\"start\":44850},{\"end\":44859,\"start\":44858},{\"end\":44867,\"start\":44866},{\"end\":44876,\"start\":44875},{\"end\":45055,\"start\":45054},{\"end\":45064,\"start\":45063},{\"end\":45072,\"start\":45071},{\"end\":45079,\"start\":45078}]", "bib_author_last_name": "[{\"end\":34380,\"start\":34370},{\"end\":34389,\"start\":34384},{\"end\":34402,\"start\":34393},{\"end\":34420,\"start\":34408},{\"end\":34662,\"start\":34659},{\"end\":34669,\"start\":34666},{\"end\":34678,\"start\":34673},{\"end\":34687,\"start\":34684},{\"end\":34696,\"start\":34691},{\"end\":34908,\"start\":34904},{\"end\":34917,\"start\":34914},{\"end\":34923,\"start\":34921},{\"end\":34931,\"start\":34927},{\"end\":35129,\"start\":35125},{\"end\":35138,\"start\":35135},{\"end\":35146,\"start\":35142},{\"end\":35276,\"start\":35266},{\"end\":35293,\"start\":35280},{\"end\":35302,\"start\":35297},{\"end\":35308,\"start\":35306},{\"end\":35324,\"start\":35312},{\"end\":35333,\"start\":35328},{\"end\":35346,\"start\":35337},{\"end\":35356,\"start\":35350},{\"end\":35605,\"start\":35600},{\"end\":35622,\"start\":35609},{\"end\":35631,\"start\":35626},{\"end\":35798,\"start\":35796},{\"end\":35807,\"start\":35802},{\"end\":35814,\"start\":35811},{\"end\":35821,\"start\":35818},{\"end\":35964,\"start\":35962},{\"end\":35972,\"start\":35968},{\"end\":35979,\"start\":35976},{\"end\":36153,\"start\":36151},{\"end\":36159,\"start\":36157},{\"end\":36168,\"start\":36163},{\"end\":36175,\"start\":36172},{\"end\":36486,\"start\":36481},{\"end\":36493,\"start\":36490},{\"end\":36511,\"start\":36497},{\"end\":36527,\"start\":36517},{\"end\":36722,\"start\":36717},{\"end\":36731,\"start\":36726},{\"end\":36740,\"start\":36735},{\"end\":36965,\"start\":36960},{\"end\":36972,\"start\":36969},{\"end\":36980,\"start\":36976},{\"end\":36988,\"start\":36984},{\"end\":36995,\"start\":36992},{\"end\":37002,\"start\":36999},{\"end\":37263,\"start\":37261},{\"end\":37269,\"start\":37267},{\"end\":37277,\"start\":37273},{\"end\":37283,\"start\":37281},{\"end\":37499,\"start\":37492},{\"end\":37508,\"start\":37503},{\"end\":37519,\"start\":37512},{\"end\":37725,\"start\":37722},{\"end\":37737,\"start\":37729},{\"end\":37747,\"start\":37741},{\"end\":37949,\"start\":37946},{\"end\":37961,\"start\":37953},{\"end\":37971,\"start\":37965},{\"end\":38169,\"start\":38166},{\"end\":38179,\"start\":38175},{\"end\":38188,\"start\":38183},{\"end\":38197,\"start\":38194},{\"end\":38434,\"start\":38428},{\"end\":38440,\"start\":38438},{\"end\":38655,\"start\":38652},{\"end\":38666,\"start\":38661},{\"end\":38675,\"start\":38670},{\"end\":38685,\"start\":38681},{\"end\":38919,\"start\":38914},{\"end\":38928,\"start\":38923},{\"end\":38938,\"start\":38932},{\"end\":38951,\"start\":38942},{\"end\":38965,\"start\":38955},{\"end\":38975,\"start\":38969},{\"end\":38985,\"start\":38979},{\"end\":38995,\"start\":38989},{\"end\":39003,\"start\":38999},{\"end\":39011,\"start\":39007},{\"end\":39299,\"start\":39297},{\"end\":39307,\"start\":39303},{\"end\":39314,\"start\":39311},{\"end\":39322,\"start\":39318},{\"end\":39330,\"start\":39326},{\"end\":39336,\"start\":39334},{\"end\":39531,\"start\":39528},{\"end\":39538,\"start\":39535},{\"end\":39545,\"start\":39542},{\"end\":39552,\"start\":39549},{\"end\":39562,\"start\":39556},{\"end\":39848,\"start\":39842},{\"end\":39859,\"start\":39852},{\"end\":39866,\"start\":39863},{\"end\":39875,\"start\":39870},{\"end\":40144,\"start\":40138},{\"end\":40151,\"start\":40148},{\"end\":40162,\"start\":40155},{\"end\":40174,\"start\":40166},{\"end\":40183,\"start\":40178},{\"end\":40195,\"start\":40187},{\"end\":40205,\"start\":40199},{\"end\":40551,\"start\":40546},{\"end\":40559,\"start\":40555},{\"end\":40768,\"start\":40765},{\"end\":40776,\"start\":40772},{\"end\":40784,\"start\":40780},{\"end\":40792,\"start\":40788},{\"end\":40799,\"start\":40796},{\"end\":40807,\"start\":40803},{\"end\":41203,\"start\":41200},{\"end\":41212,\"start\":41207},{\"end\":41218,\"start\":41216},{\"end\":41225,\"start\":41222},{\"end\":41232,\"start\":41229},{\"end\":41239,\"start\":41236},{\"end\":41246,\"start\":41243},{\"end\":41256,\"start\":41252},{\"end\":41619,\"start\":41612},{\"end\":41632,\"start\":41623},{\"end\":41642,\"start\":41636},{\"end\":41896,\"start\":41893},{\"end\":41909,\"start\":41900},{\"end\":41919,\"start\":41913},{\"end\":41927,\"start\":41923},{\"end\":41939,\"start\":41933},{\"end\":41949,\"start\":41943},{\"end\":41961,\"start\":41953},{\"end\":41969,\"start\":41965},{\"end\":42262,\"start\":42259},{\"end\":42270,\"start\":42266},{\"end\":42277,\"start\":42274},{\"end\":42454,\"start\":42451},{\"end\":42462,\"start\":42458},{\"end\":42469,\"start\":42466},{\"end\":42475,\"start\":42473},{\"end\":42674,\"start\":42667},{\"end\":42687,\"start\":42678},{\"end\":42699,\"start\":42691},{\"end\":42709,\"start\":42705},{\"end\":42718,\"start\":42713},{\"end\":42951,\"start\":42947},{\"end\":42958,\"start\":42955},{\"end\":42966,\"start\":42962},{\"end\":42973,\"start\":42970},{\"end\":42982,\"start\":42977},{\"end\":43164,\"start\":43161},{\"end\":43172,\"start\":43168},{\"end\":43181,\"start\":43178},{\"end\":43193,\"start\":43185},{\"end\":43380,\"start\":43376},{\"end\":43390,\"start\":43384},{\"end\":43399,\"start\":43394},{\"end\":43405,\"start\":43403},{\"end\":43602,\"start\":43597},{\"end\":43610,\"start\":43606},{\"end\":43621,\"start\":43614},{\"end\":43885,\"start\":43880},{\"end\":43892,\"start\":43889},{\"end\":43898,\"start\":43896},{\"end\":43907,\"start\":43902},{\"end\":44116,\"start\":44111},{\"end\":44123,\"start\":44120},{\"end\":44132,\"start\":44127},{\"end\":44365,\"start\":44360},{\"end\":44371,\"start\":44369},{\"end\":44600,\"start\":44595},{\"end\":44606,\"start\":44604},{\"end\":44612,\"start\":44610},{\"end\":44620,\"start\":44616},{\"end\":44629,\"start\":44624},{\"end\":44635,\"start\":44633},{\"end\":44848,\"start\":44843},{\"end\":44856,\"start\":44852},{\"end\":44864,\"start\":44860},{\"end\":44873,\"start\":44868},{\"end\":44879,\"start\":44877},{\"end\":45061,\"start\":45056},{\"end\":45069,\"start\":45065},{\"end\":45076,\"start\":45073},{\"end\":45082,\"start\":45080}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34589,\"start\":34282},{\"attributes\":{\"id\":\"b1\"},\"end\":34834,\"start\":34591},{\"attributes\":{\"id\":\"b2\"},\"end\":35057,\"start\":34836},{\"attributes\":{\"id\":\"b3\"},\"end\":35262,\"start\":35059},{\"attributes\":{\"id\":\"b4\"},\"end\":35545,\"start\":35264},{\"attributes\":{\"id\":\"b5\"},\"end\":35746,\"start\":35547},{\"attributes\":{\"id\":\"b6\"},\"end\":35925,\"start\":35748},{\"attributes\":{\"id\":\"b7\"},\"end\":36060,\"start\":35927},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52893847},\"end\":36435,\"start\":36062},{\"attributes\":{\"id\":\"b9\"},\"end\":36648,\"start\":36437},{\"attributes\":{\"id\":\"b10\"},\"end\":36859,\"start\":36650},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3845003},\"end\":37194,\"start\":36861},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1923924},\"end\":37419,\"start\":37196},{\"attributes\":{\"id\":\"b13\"},\"end\":37646,\"start\":37421},{\"attributes\":{\"id\":\"b14\"},\"end\":37875,\"start\":37648},{\"attributes\":{\"id\":\"b15\"},\"end\":38094,\"start\":37877},{\"attributes\":{\"doi\":\"arXiv:1811.12043\",\"id\":\"b16\"},\"end\":38378,\"start\":38096},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b17\"},\"end\":38574,\"start\":38380},{\"attributes\":{\"id\":\"b18\"},\"end\":38825,\"start\":38576},{\"attributes\":{\"id\":\"b19\"},\"end\":39248,\"start\":38827},{\"attributes\":{\"id\":\"b20\"},\"end\":39457,\"start\":39250},{\"attributes\":{\"id\":\"b21\"},\"end\":39698,\"start\":39459},{\"attributes\":{\"id\":\"b22\"},\"end\":40081,\"start\":39700},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8887614},\"end\":40444,\"start\":40083},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14607544},\"end\":40715,\"start\":40446},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":204957291},\"end\":41127,\"start\":40717},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":54010531},\"end\":41527,\"start\":41129},{\"attributes\":{\"id\":\"b27\"},\"end\":41780,\"start\":41529},{\"attributes\":{\"id\":\"b28\"},\"end\":42195,\"start\":41782},{\"attributes\":{\"id\":\"b29\"},\"end\":42388,\"start\":42197},{\"attributes\":{\"id\":\"b30\"},\"end\":42587,\"start\":42390},{\"attributes\":{\"id\":\"b31\"},\"end\":42884,\"start\":42589},{\"attributes\":{\"id\":\"b32\"},\"end\":43113,\"start\":42886},{\"attributes\":{\"id\":\"b33\"},\"end\":43302,\"start\":43115},{\"attributes\":{\"id\":\"b34\"},\"end\":43539,\"start\":43304},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2356330},\"end\":43820,\"start\":43541},{\"attributes\":{\"id\":\"b36\"},\"end\":44023,\"start\":43822},{\"attributes\":{\"id\":\"b37\"},\"end\":44268,\"start\":44025},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9760560},\"end\":44515,\"start\":44270},{\"attributes\":{\"id\":\"b39\"},\"end\":44788,\"start\":44517},{\"attributes\":{\"id\":\"b40\"},\"end\":45001,\"start\":44790},{\"attributes\":{\"id\":\"b41\"},\"end\":45192,\"start\":45003}]", "bib_title": "[{\"end\":36147,\"start\":36062},{\"end\":36956,\"start\":36861},{\"end\":37257,\"start\":37196},{\"end\":40134,\"start\":40083},{\"end\":40542,\"start\":40446},{\"end\":40761,\"start\":40717},{\"end\":41196,\"start\":41129},{\"end\":43593,\"start\":43541},{\"end\":44356,\"start\":44270}]", "bib_author": "[{\"end\":34382,\"start\":34368},{\"end\":34391,\"start\":34382},{\"end\":34404,\"start\":34391},{\"end\":34422,\"start\":34404},{\"end\":34664,\"start\":34657},{\"end\":34671,\"start\":34664},{\"end\":34680,\"start\":34671},{\"end\":34689,\"start\":34680},{\"end\":34698,\"start\":34689},{\"end\":34910,\"start\":34902},{\"end\":34919,\"start\":34910},{\"end\":34925,\"start\":34919},{\"end\":34933,\"start\":34925},{\"end\":35131,\"start\":35123},{\"end\":35140,\"start\":35131},{\"end\":35148,\"start\":35140},{\"end\":35278,\"start\":35264},{\"end\":35295,\"start\":35278},{\"end\":35304,\"start\":35295},{\"end\":35310,\"start\":35304},{\"end\":35326,\"start\":35310},{\"end\":35335,\"start\":35326},{\"end\":35348,\"start\":35335},{\"end\":35358,\"start\":35348},{\"end\":35607,\"start\":35598},{\"end\":35624,\"start\":35607},{\"end\":35633,\"start\":35624},{\"end\":35800,\"start\":35794},{\"end\":35809,\"start\":35800},{\"end\":35816,\"start\":35809},{\"end\":35823,\"start\":35816},{\"end\":35966,\"start\":35960},{\"end\":35974,\"start\":35966},{\"end\":35981,\"start\":35974},{\"end\":36155,\"start\":36149},{\"end\":36161,\"start\":36155},{\"end\":36170,\"start\":36161},{\"end\":36177,\"start\":36170},{\"end\":36488,\"start\":36479},{\"end\":36495,\"start\":36488},{\"end\":36513,\"start\":36495},{\"end\":36529,\"start\":36513},{\"end\":36724,\"start\":36713},{\"end\":36733,\"start\":36724},{\"end\":36742,\"start\":36733},{\"end\":36967,\"start\":36958},{\"end\":36974,\"start\":36967},{\"end\":36982,\"start\":36974},{\"end\":36990,\"start\":36982},{\"end\":36997,\"start\":36990},{\"end\":37004,\"start\":36997},{\"end\":37265,\"start\":37259},{\"end\":37271,\"start\":37265},{\"end\":37279,\"start\":37271},{\"end\":37285,\"start\":37279},{\"end\":37501,\"start\":37490},{\"end\":37510,\"start\":37501},{\"end\":37521,\"start\":37510},{\"end\":37727,\"start\":37720},{\"end\":37739,\"start\":37727},{\"end\":37749,\"start\":37739},{\"end\":37951,\"start\":37944},{\"end\":37963,\"start\":37951},{\"end\":37973,\"start\":37963},{\"end\":38171,\"start\":38162},{\"end\":38181,\"start\":38171},{\"end\":38190,\"start\":38181},{\"end\":38199,\"start\":38190},{\"end\":38436,\"start\":38424},{\"end\":38442,\"start\":38436},{\"end\":38657,\"start\":38648},{\"end\":38668,\"start\":38657},{\"end\":38677,\"start\":38668},{\"end\":38687,\"start\":38677},{\"end\":38921,\"start\":38912},{\"end\":38930,\"start\":38921},{\"end\":38940,\"start\":38930},{\"end\":38953,\"start\":38940},{\"end\":38967,\"start\":38953},{\"end\":38977,\"start\":38967},{\"end\":38987,\"start\":38977},{\"end\":38997,\"start\":38987},{\"end\":39005,\"start\":38997},{\"end\":39013,\"start\":39005},{\"end\":39301,\"start\":39295},{\"end\":39309,\"start\":39301},{\"end\":39316,\"start\":39309},{\"end\":39324,\"start\":39316},{\"end\":39332,\"start\":39324},{\"end\":39338,\"start\":39332},{\"end\":39533,\"start\":39526},{\"end\":39540,\"start\":39533},{\"end\":39547,\"start\":39540},{\"end\":39554,\"start\":39547},{\"end\":39564,\"start\":39554},{\"end\":39850,\"start\":39840},{\"end\":39861,\"start\":39850},{\"end\":39868,\"start\":39861},{\"end\":39877,\"start\":39868},{\"end\":40146,\"start\":40136},{\"end\":40153,\"start\":40146},{\"end\":40164,\"start\":40153},{\"end\":40176,\"start\":40164},{\"end\":40185,\"start\":40176},{\"end\":40197,\"start\":40185},{\"end\":40207,\"start\":40197},{\"end\":40553,\"start\":40544},{\"end\":40561,\"start\":40553},{\"end\":40770,\"start\":40763},{\"end\":40778,\"start\":40770},{\"end\":40786,\"start\":40778},{\"end\":40794,\"start\":40786},{\"end\":40801,\"start\":40794},{\"end\":40809,\"start\":40801},{\"end\":41205,\"start\":41198},{\"end\":41214,\"start\":41205},{\"end\":41220,\"start\":41214},{\"end\":41227,\"start\":41220},{\"end\":41234,\"start\":41227},{\"end\":41241,\"start\":41234},{\"end\":41248,\"start\":41241},{\"end\":41258,\"start\":41248},{\"end\":41621,\"start\":41608},{\"end\":41634,\"start\":41621},{\"end\":41644,\"start\":41634},{\"end\":41898,\"start\":41891},{\"end\":41911,\"start\":41898},{\"end\":41921,\"start\":41911},{\"end\":41929,\"start\":41921},{\"end\":41941,\"start\":41929},{\"end\":41951,\"start\":41941},{\"end\":41963,\"start\":41951},{\"end\":41971,\"start\":41963},{\"end\":42264,\"start\":42257},{\"end\":42272,\"start\":42264},{\"end\":42279,\"start\":42272},{\"end\":42456,\"start\":42449},{\"end\":42464,\"start\":42456},{\"end\":42471,\"start\":42464},{\"end\":42477,\"start\":42471},{\"end\":42676,\"start\":42665},{\"end\":42689,\"start\":42676},{\"end\":42701,\"start\":42689},{\"end\":42711,\"start\":42701},{\"end\":42720,\"start\":42711},{\"end\":42953,\"start\":42945},{\"end\":42960,\"start\":42953},{\"end\":42968,\"start\":42960},{\"end\":42975,\"start\":42968},{\"end\":42984,\"start\":42975},{\"end\":43166,\"start\":43159},{\"end\":43174,\"start\":43166},{\"end\":43183,\"start\":43174},{\"end\":43195,\"start\":43183},{\"end\":43382,\"start\":43374},{\"end\":43392,\"start\":43382},{\"end\":43401,\"start\":43392},{\"end\":43407,\"start\":43401},{\"end\":43604,\"start\":43595},{\"end\":43612,\"start\":43604},{\"end\":43623,\"start\":43612},{\"end\":43887,\"start\":43878},{\"end\":43894,\"start\":43887},{\"end\":43900,\"start\":43894},{\"end\":43909,\"start\":43900},{\"end\":44118,\"start\":44109},{\"end\":44125,\"start\":44118},{\"end\":44134,\"start\":44125},{\"end\":44367,\"start\":44358},{\"end\":44373,\"start\":44367},{\"end\":44602,\"start\":44593},{\"end\":44608,\"start\":44602},{\"end\":44614,\"start\":44608},{\"end\":44622,\"start\":44614},{\"end\":44631,\"start\":44622},{\"end\":44637,\"start\":44631},{\"end\":44850,\"start\":44841},{\"end\":44858,\"start\":44850},{\"end\":44866,\"start\":44858},{\"end\":44875,\"start\":44866},{\"end\":44881,\"start\":44875},{\"end\":45063,\"start\":45054},{\"end\":45071,\"start\":45063},{\"end\":45078,\"start\":45071},{\"end\":45084,\"start\":45078}]", "bib_venue": "[{\"end\":34366,\"start\":34282},{\"end\":34655,\"start\":34591},{\"end\":34900,\"start\":34836},{\"end\":35121,\"start\":35059},{\"end\":35385,\"start\":35358},{\"end\":35596,\"start\":35547},{\"end\":35792,\"start\":35748},{\"end\":35958,\"start\":35927},{\"end\":36239,\"start\":36177},{\"end\":36477,\"start\":36437},{\"end\":36711,\"start\":36650},{\"end\":37007,\"start\":37004},{\"end\":37290,\"start\":37285},{\"end\":37488,\"start\":37421},{\"end\":37718,\"start\":37648},{\"end\":37942,\"start\":37877},{\"end\":38160,\"start\":38096},{\"end\":38422,\"start\":38380},{\"end\":38646,\"start\":38576},{\"end\":38910,\"start\":38827},{\"end\":39293,\"start\":39250},{\"end\":39524,\"start\":39459},{\"end\":39838,\"start\":39700},{\"end\":40240,\"start\":40207},{\"end\":40564,\"start\":40561},{\"end\":40876,\"start\":40809},{\"end\":41307,\"start\":41258},{\"end\":41606,\"start\":41529},{\"end\":41889,\"start\":41782},{\"end\":42255,\"start\":42197},{\"end\":42447,\"start\":42390},{\"end\":42663,\"start\":42589},{\"end\":42943,\"start\":42886},{\"end\":43157,\"start\":43115},{\"end\":43372,\"start\":43304},{\"end\":43670,\"start\":43623},{\"end\":43876,\"start\":43822},{\"end\":44107,\"start\":44025},{\"end\":44376,\"start\":44373},{\"end\":44591,\"start\":44517},{\"end\":44839,\"start\":44790},{\"end\":45052,\"start\":45003},{\"end\":40930,\"start\":40878}]"}}}, "year": 2023, "month": 12, "day": 17}