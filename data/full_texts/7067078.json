{"id": 7067078, "updated": "2023-09-28 10:46:50.717", "metadata": {"title": "Kernelized Supervised Dictionary Learning", "authors": "[{\"first\":\"Mehrdad\",\"last\":\"Gangeh\",\"middle\":[\"J.\"]},{\"first\":\"Ali\",\"last\":\"Ghodsi\",\"middle\":[]},{\"first\":\"Mohamed\",\"last\":\"Kamel\",\"middle\":[\"S.\"]}]", "venue": "IEEE Transactions on Signal Processing", "journal": "IEEE Transactions on Signal Processing", "publication_date": {"year": 2012, "month": null, "day": null}, "abstract": "In this paper, we propose supervised dictionary learning (SDL) by incorporating information on class labels into the learning of the dictionary. To this end, we propose to learn the dictionary in a space where the dependency between the signals and their corresponding labels is maximized. To maximize this dependency, the recently introduced Hilbert Schmidt independence criterion (HSIC) is used. One of the main advantages of this novel approach for SDL is that it can be easily kernelized by incorporating a kernel, particularly a data-dependent kernel such as normalized compression distance, into the formulation. The learned dictionary is compact and the proposed approach is fast. We show that it outperforms other unsupervised and supervised dictionary learning approaches in the literature, using real-world data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1207.2488", "mag": "2050460554", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1207-2488", "doi": "10.1109/tsp.2013.2274276"}}, "content": {"source": {"pdf_hash": "46708afa8b56cb165b11803bf76548b181975e99", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://uwspace.uwaterloo.ca/bitstream/10012/7455/1/JabbarzadehGangeh_Mehrdad.pdf", "status": "GREEN"}}, "grobid": {"id": "a12df0bb0025224eb5184cd5b16ab28bf454e218", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/46708afa8b56cb165b11803bf76548b181975e99.txt", "contents": "\nIEEE TRANSACTION ON SIGNAL PROCESSING 1 Kernelized Supervised Dictionary Learning\n\n\nMehrdad J Gangeh \nMember, IEEEAli Ghodsi \nFellow, IEEEMohamed S Kamel \nIEEE TRANSACTION ON SIGNAL PROCESSING 1 Kernelized Supervised Dictionary Learning\nIndex Terms-Pattern recognition and classificationclassifi- cation methodsnon-parametric methodsdictionary learningHSICsupervised learning\nIn this paper, we propose supervised dictionary learning (SDL) by incorporating information on class labels into the learning of the dictionary. To this end, we propose to learn the dictionary in a space where the dependency between the signals and their corresponding labels is maximized. To maximize this dependency, the recently introduced Hilbert Schmidt independence criterion (HSIC) is used. One of the main advantages of this novel approach for SDL is that it can be easily kernelized by incorporating a kernel, particularly a data-dependent kernel such as normalized compression distance, into the formulation. The learned dictionary is compact and the proposed approach is fast. We show that it outperforms other unsupervised and supervised dictionary learning approaches in the literature, using real-world data.\n\nI. INTRODUCTION\n\nD ICTIONARY learning and sparse representation (DLSR) are two closely-related topics that have roots in the decomposition of signals to some predefined bases, such as the Fourier transform. However, what makes DLSR distinct from the representation using predefined bases is that first, the bases are learned here from the data, and second, only a few components in the dictionary are needed to represent the data (sparse representation). This latter attribute can also be seen in the decomposition of signals using some predefined bases such as wavelets [1].\n\nThe concept of dictionary learning and sparse representation originated in different communities attempting to solve different problems, which are given different names. Some of them are: sparse coding (SC), which was originated by neurologists as a model for simple cells in mammalian primary visual cortex [2]; independent component analysis (ICA), which was originated by researchers in signal processing to estimate the underlying hidden components of multivariate statistical data (refer to [3] for a review of ICA); least absolute shrinkage and selection operator (lasso), which was originated by statisticians to find linear regression models when there are many more predictors than samples, where some constraints have to be considered to fit the model. In the lasso, one of the constraints introduced by Tibshirani was the 1 norm that led to sparse coefficients in the linear regression model [4]. Another technique which also leads to DLSR is nonnegative matrix factorization (NNMF), which aimed to decompose a matrix to two nonnegative matrices, one of which can be considered to be the dictionary, and the other the coefficients [5]. In NNMF, usually both the dictionary and coefficients are sparse [5], [6]. This list is not complete, and there are variants for each of the above techniques, such as blind source separation (BSS) [7], compressed sensing [8], basis pursuit (BP) [9], and orthogonal matching pursuit (OMP) [10], [11]. It is beyond the scope of this paper to include the description of all these techniques (interested readers can refer to [12]- [14] for a review on dictionary learning and sparse representation).\n\nThe main results of all these research efforts is that a class of signals with sparse nature, such as images of natural scenes, can be represented using some primitive elements that form a dictionary, and that each signal in this class can be represented by using only a few elements in the dictionary, i.e., by a sparse representation. In fact, there are, at least, two ways in the literature to exploit sparsity [15]: first, using a linear/nonlinear combination of some predefined bases, e.g., wavelets [1]; second, using primitive elements in a learned dictionary, such as the techniques employed in SC or ICA. This latter approach is our focus in this paper and has led to state-of-the-art results in various applications such as texture classification [16], [17], face recognition [18]- [20], image denoising [21], [22], etc.\n\nWe may categorize the various dictionary learning with sparse representation approaches proposed in the literature in different ways: one where the dictionary consists of predefined or learned bases as stated above, and the other based on the model used to learn the dictionary and coefficients. These models can be generative as used in the original formulation of SC [2], ICA [3], and NNMF [5]; reconstructive as in the lasso [4]; or discriminative such as SDL-D (supervised dictionary learning-discriminative) in [15]. The two former approaches do not consider the class labels in building the dictionary, while the last one (i.e., the discriminative one) does. In other words, we state that dictionary learning can be performed unsupervised or supervised, with the difference that in the latter, the class labels in the training set are used to build a more discriminative dictionary for the particular classification task in hand.\n\nIn this paper, we propose a novel supervised dictionary learning (SDL) by incorporating information on class labels into the learning of the dictionary. The dictionary is learned in a space where the dependency between the data and their corresponding labels is maximized. We propose to maximize this dependency by using the recently introduced Hilbert Schmidt independence criterion (HSIC) [23], [24]. The dictionary is then learned in this new space. Although supervised dictionary learning has been proposed by others, as will be reviewed in the next section, this work is different from the others in the following aspects:\n\n1) The formulation is simple and straightforward;\n\n2) The proposed approach introduces a closed form formulation for the computation of the dictionary. This is different from other approaches, in which the computation of dictionary and sparse coefficients has to be iteratively and often alternately performed, which causes high computational load; 3) The proposed approach also leads to separable problem on the minimization of the coefficients, which can be solved in closed form using soft thresholding. This further improves the performance of the proposed algorithm in terms of speed; 4) The approach is very efficient in terms of dictionary size (compact dictionary). Our results show that the proposed dictionary can produce significantly better results than other supervised dictionary methods at small dictionary sizes. An important special case is when the dictionary size is smaller than the dimensionality of data. This turns the learning of a dictionary whose size is usually larger than the dimensionality of the data, i.e., an overcomplete dictionary, into the learning of a subspace; 5) The proposed approach can be easily kernelized by incorporating a kernel into the formulation. Datadependent kernels based on, e.g., normalized compression distance (NCD) [25], [26], can be used in this kernelized SDL to further improve the discrimination power of the designed system. To the best of our knowledge, no other kernelized SDL approach has been proposed in the literature yet, and none of the proposed SDLs in the literature can be kernelized in a straightforward way. The organization of the rest of the paper is as follows: in Section II, we review the current SDL approaches in the literature and their shortcomings. Then we review the mathematical background and the formulation for proposed approach in Section III. The experimental setup and results are presented in Sections IV, followed by discussion and conclusion in Section V.\n\n\nII. BACKGROUND AND RELATED WORK\n\nIn this section, we provide an overview on the dictionary learning and sparse representation, and a brief review of recent attempts on making the approach more suitable for classification tasks.\n\n\nA. Dictionary Learning and Sparse Representation\n\nConsidering a finite training set of signals X = [x 1 , x 2 , ..., x n ] \u2208 R p\u00d7n , where p is the dimensionality and n is the number of data samples, according to classical dictionary learning and sparse representation (DLSR) techniques (refer to [12] and [13] for a recent review on this topic), these signals can be represented by a linear decomposition over a few dictionary atoms by minimizing a loss function as given below\nL(X, D, \u03b1) = n i=1 l(x i , D, \u03b1),(1)\nwhere D \u2208 R p\u00d7k is the dictionary of k atoms, and \u03b1 \u2208 R k\u00d7n are the coefficients. This loss function can be defined in various ways based on the application in hand. However, what is common in DLSR literature is to define the loss function L as the reconstruction error in a mean-squared sense, with a sparsityinducing function \u03c8 as a regularization penalty to ensure the sparsity of coefficients. Hence, (1) can be written as\nL(X, D, \u03b1) = min D,\u03b1 1 2 X \u2212 D\u03b1 2 F + \u03bb\u03c8(\u03b1),(2)\nwhere subscript F indicates the Frobenius norm and \u03bb is the regularization parameter that affects the number of nonzero coefficients. An intuitive measure of sparsity is 0 norm, which indicates the number of nonzero elements in a vector 1 . However, the optimization problem obtained from replacing sparsity-inducing function \u03c8 in (2) with 0 is nonconvex, and the problem is NPhard (refer to [13] for a recent comprehensive discussion on this issue). There are two main proposed approximate solutions to overcome this problem: the first is based on greedy algorithms, such as the well-known orthogonal matching pursuit (OMP) [10], [11], [13]; the second works by approximating a highly discontinuous 0 norm by a continuous function such as the 1 norm. This leads to an approach, which is widely known in the literature as lasso [4] or basis pursuit (BP) [9], and (2) converts to\nL(X, D, \u03b1) = min D,\u03b1 n i=1 1 2 x i \u2212 D\u03b1 i 2 2 + \u03bb \u03b1 i 1 . (3)\nIn (3), the main optimization goal for computation of the dictionary and sparse coefficients is minimizing the reconstruction error in the mean-squared sense. While this works well in applications where the primary goal is to reconstruct signals as accurately as possible, such as in denoising, image inpainting, and coding, it is not the ultimate goal in classification tasks [27], as discriminating signals is more important here. Hence, recently, there have been several attempts to include category information in computing either dictionary, coefficients, or both. In the following subsection, we will provide a brief overview of proposed supervised dictionary learning approaches in the literature. To this end, we will try to categorize the proposed approaches into five different categories, while we admit that this taxonomy of approaches is not unique and could be done differently.\n\n\nB. Supervised Dictionary Learning in Literature\n\nAs mentioned in the previous subsection, (3) provides a reconstructive formulation for computing the dictionary and sparse coefficients, given a set of data samples. Although the problem is not convex on both dictionary D and coefficients \u03b1, this optimization problem is convex if it is solved iteratively and alternately on these two unknowns. Several fast algorithms have recently been proposed for this purpose, such as K-SVD [28], online learning [29], and cyclic coordinate descent [30]. However, none of these approaches takes into account the category information for learning either the dictionary or the coefficients.\n\nThe first and simplest approach to include category information in DLSR is computing one dictionary per class, i.e., using the training samples in each class to compute part of the dictionary, and then composing all these partial dictionaries into one. Perhaps the earliest work in this direction is the so-called texton-based approach [17], [31], [32]. In this approach, kmeans is applied to the training samples in each class, and the k cluster centers computed are considered as the dictionary for this class. These partial dictionaries are eventually composed into one dictionary. In [19], the training samples are used as the dictionary in face recognition and hence, effectively falls in the same category as training one dictionary per class. However, no actual training is performed here, and the whole training samples are used directly in the dictionary. Using the training samples as dictionary yields a very large and possibly inefficient dictionary due to noisy training instances. To obtain a smaller dictionary, Yang et al. proposed learning a smaller dictionary for each class called a metaface (the proposed approach was in a face recognition application, but it is general and can be used in any application) and then compose them into one dictionary [33]. One major drawback of this approach is that the training samples in one class are used for computing the atoms in the dictionary, irrespective of the training samples form other classes. This means that if training samples across classes have some common properties, these shared properties cannot be learned in common in the dictionary. Ramirez et al. proposed overcoming this problem by including an incoherence term in (3) to encourage independency of dictionaries from different classes, while still allowing for different classes to share features [34]. The main drawback of all approaches in this first category of SDL is that they may lead to a very large dictionary, as the size of the composed dictionary grows linearly with the number of classes.\n\nThe second category of SDL approaches learn a very large dictionary unsupervised in the beginning, then merge the atoms in the dictionary by optimizing an objective function that takes into account the category information. One major work in literature in this direction is based on the information bottleneck that iteratively merges two dictionary atoms that cause the smallest decrease in the mutual information between dictionary atoms and class labels [35]. Another major work is based on merging two dictionary atoms so as to minimize the loss of mutual information between histogram of dictionary atoms, over signal constituents, e.g., image patches, and class labels [36]. One main drawback of this category of SDL is that the reduced dictionary obtained usually performs at most the same as the original one hence, since the initial dictionary is learned unsupervised (although due to its large size it includes almost all possible atoms that helps to improve the performance of classification task) the consecutive pruning stage is inefficient in terms of computational load. This can be significantly improved by finding a discriminative dictionary from the beginning.\n\nThe third category of SDL, which is based on several research works published in [15], [37]- [41] can be considered a major leap in SDL. In this category, the classifier parameters and dictionary are learned in a joint optimization problem. Although this idea is more sophisticated than the previous two, its major disadvantage is that the optimization problem is nonconvex and complex. If it is done alternately between dictionary learning and classifier parameters learning, it is quite likely that they will become stuck in local minima. On the other hand, due to the complexity of the problem, except for the bilinear classifier in [15], other papers only consider linear classifiers, which is usually too simple to solve difficult problems, and can only be successful in simple classification tasks as shown in [15]. In [38], Zhang and Li propose a technique called discriminative K-SVD (DK-SVD). DK-SVD truly jointly learns the classifier parameters and dictionary, without alternating between these two steps. This prevents the possibility of getting stuck in local minima. However, only linear classifiers are considered in DK-SVD, which may lead to poor performance in difficult classification tasks. Another major problem with the approaches in this category of SDL is that there exist many parameters involved in the formulation, which are hard and time-consuming to tune (see for example [15], [41]).\n\nThe fourth category of SDL approaches include the category information in the learning of the dictionary. This is done, for example, by minimizing the information loss due to predicting labels from a supervised dictionary learned instead of original training data samples (this approach is known as info-loss in the SDL literature) [42], or by deploying extremely randomized decision forests [43]. This latter approach can also fall in the second category of SDLs, as it seems that it starts from a very large dictionary using random forests, and tries to prune it later to conclude with a smaller dictionary. The same just as in the previous category of SDL, the info-loss approach has the major drawback that it may stuck in local minima. This is mainly because the optimization has to be done iteratively and alternately on two updates, as there is no closed form solution for the approach.\n\nThe fifth category of SDLs include class category in learning the coefficients [27] or in learning both dictionary and coefficients [20], [44]. Supervised coefficient learning in all these papers [20], [27], [44] has been performed more or less in the same way using Fisher discrimination criterion [45], i.e., by minimizing the within-class covariance of coefficients and at the same time maximizing their between-class covariance. As for the dictionary, while [27] uses predefined bases, [20] proposes a discriminative fidelity term that encourages learning dictionary atoms of one class from the training samples of the same class, and at the same time penalizes their learning by the training samples from other classes. The joint optimization problem due to Fisher discrimination criterion on the coefficients and the discriminative fidelity term on the dictionary proposed in [20] is not convex, and has to be solved alternately and iteratively between these two terms until it converges. However, there is no guarantee in this approach to find the global minimum. Also, it is not clear whether the improvement obtained in classification by including Fisher discriminant criterion on coefficients justifies the additional computation load imposed on the learning, as there is no comparison provided in [20] on the classification with and without including supervision on coefficients.\n\nIn next section, we explain the mathematical formulation for our proposed approach, which we believe belongs to the fourth category of SDLs explained above, i.e., including category information to learn a supervised dictionary.\n\n\nIII. METHODS\n\nTo incorporate the category information into the dictionary learning, we propose to decompose the signals using some learned bases that represent them in a space where the dependency between the signals and their corresponding class labels is maximized. To this end, we need a(n) (in)dependency test measure between two random variables. Here, we propose to use Hilbert-Schmidt independence criterion (HSIC) as the (in)dependency measure. In this section, we first describe HSIC, and then provide the formulation for our proposed supervised dictionary learning (SDL) approach. Subsequently, kernelized SDL is formulated that enables embedding kernels, including data-dependent ones, into the proposed SDL. This can significantly improve the discrimination power of the designed dictionary, which is essential in difficult classification tasks, as will be shown in our experiments in Subsection IV-E later.\n\n\nA. Hilbert Schmidt Independence Criterion\n\nThere are several techniques in the literature to measure the (in)dependence of random variables, such as mutual information [46] and Kullback-Leibler (KL) divergence [47]. In addition to these measures, there has recently been great interest in measuring (in)dependency using criteria based on functions in reproducing kernel Hilbert spaces (RKHSs). Bach and Jordan were those who first accomplished this, by introducing kernel dependence functionals that significantly outperformed alternative approaches [48]. Later, Gretton et al. proposed another kernel-based approach called the Hilbert-Schmidt independence criterion (HSIC) to measure the (in)dependence of two random variables X and Y [23]. Since its introduction, the HSIC has been used in many applications, including feature selection [49], independent component analysis [50], and sorting/matching [51].\n\nOne can derive HSIC as a measure of (in)dependence between two random variables X and Y using two different approaches: first by computing the Hilbert-Schmidt norm of the cross-covariance operators in RKHSs as shown in [23], [24]; or second, by computing maximum mean discrepancy (MMD) of two distributions mapped to a high dimensional space, i.e., computed in RKHSs [52], [53]. We believe that this latter approach is more straightforward and hence, use it to describe HSIC.\nLet Z := {(x 1 , y 1 , ), ..., (x n , y n )} \u2286 X \u00d7 Y be n inde- pendent observations drawn from p := P X \u00d7Y .\nTo investigate whether X and Y are independent, we need to determine whether distribution p factorizes, i.e., whether p is the same as q := P X \u00d7 P Y .\n\nThe mean of distributions are defined as follows\n\u00b5[P X \u00d7Y ] := E xy [v((x, y), .)], (4) \u00b5[P X \u00d7 P Y ] := E x E y [v((x, y), .)],(5)\nwhere E xy is the expectation over (x, y) \u223c P X \u00d7Y and kernel v((x, y), (x , y )) is defined in RKHS over X \u00d7 Y. By computing the mean of distributions p and q in RKHS, we effectively take into account higher order statistics than the first order, by mapping these distributions to a highdimensional feature space. Hence, we can use MMD(p,\nq) := \u00b5[P X \u00d7Y ] \u2212 \u00b5[P X \u00d7 P Y ] 2\nas a measure of (in)dependence of the random variables X and Y. The higher the value of MMD, the closer the two distributions p and q and hence, the more dependent are random variables X and Y. Now suppose that H and G are two RKHSs in X and Y, respectively. Hence, by the Riesz representation theorem, there are feature mappings \u03c6(x) :\nX \u2192 R and \u03c8(y) : Y \u2192 R such that k(x, x ) = \u03c6(x), \u03c6(x ) H and l(y, y ) = \u03c8(y), \u03c8(y ) G . Moreover, suppose that v((x, y), (x , y )) = k(x, x )l(y, y ),\ni.e., the RKHS is a direct product of H \u2297 G of the RKHSs on X and Y. Then MMD(p, q) can be written as\nMMD 2 (p, q) = E xy [k(x, .)l(y, .)] \u2212 E x [k(x, .)]E y [l(y, .)] 2 2 = E xy E x y [k(x, x )l(y, y )] \u2212 2E x E y E x y [k(x, x )l(y, y )] + E x E y E x E y [k(x, x )l(y, y )]. (6)\nThis is exactly the HSIC, and equivalent to the Hilbert-Schmidt norm of the cross-covariance operator in RKHSs [23]. For practical purposes, HSIC has to be estimated using a finite number of data samples. Considering Z := {(x 1 , y 1 , ), ..., (x n , y n )} \u2286 X \u00d7 Y as n independent observations drawn from p := P X \u00d7Y , an empirical estimate of HSIC is defined as follows [23] \nHSIC(Z) = 1 (n \u2212 1) 2 tr(KHLH),(7)\nwhere tr is the trace operator,\nH, K, L \u2208 R n\u00d7n , K i,j = k(x i , x j ), L i,j = l(y i , y j )\n, and H = I \u2212 n \u22121 ee (I is the identity matrix, and e is a vector of n ones, and hence, H is the centering matrix). It is important to note that according to (7), to maximize the dependency between two random variables X and Y, the empirical estimate of HSIC, i.e., tr(KHLH) should be maximized.\n\n\nB. Proposed Supervised Dictionary Learning\n\nTo formulate our proposed SDL, we start from the reconstruction error given in (3). Let there be a finite training set of n data points, each of which consists of p features, i.e., X = [x 1 , x 2 , ..., x n ] \u2208 R p\u00d7n . We further assume that features in data samples are centered, i.e., their mean is removed and hence, each row of X sums to zero. We address the problem of finding a linear decomposition of data X \u2208 R p\u00d7n using some bases U \u2208 R p\u00d7k such that the reconstruction error is minimum in the mean-squared sense, i.e.,\nmin U,vi n i=1 x i \u2212 Uv i 2 2 ,(8)\nwhere v i is the vector of k reconstruction coefficients in the subspace defined by U X. We can rewrite (8) in matrix form as follows min\nU,V X \u2212 UV 2 F ,(9)\nwhere V \u2208 R k\u00d7n is the matrix of coefficients. Since both U and V are unknown, this problem is ill-posed and does not have a unique solution unless we impose some constraints on the matrix U. If we, for example, assume that the columns of U are orthonormal, i.e., U U = I, (9) can be written as a constrained optimization problem as follows\nmin U,V X \u2212 UV 2 F . s.t. U U = I(10)\nTo further investigate the optimization problem in (10), we assume that the matrix U is fixed, and find the optimum matrix of coefficients V in terms of X and U by taking the derivative of the objective function given in (10) \nin respect to V \u2202 \u2202V X \u2212 UV 2 F = \u2202 \u2202V tr[(X \u2212 UV) (X \u2212 UV)] = \u2202 \u2202V [tr(X X) \u2212 2tr(X UV) + tr(V U UV)] = \u22122U X + 2U UV.\nEquating the above derivative to zero and knowing that U U = I, we obtain\nV = U X.(11)\nBy plugging the V found in (11) into the objective function of (10) we obtain\nmin U X \u2212 UU X 2 F = min U tr[(X \u2212 UU X) (X \u2212 UU X)] = min U [tr(X X) \u2212 2tr(X UU X) + tr(X UU UU X)] = max U tr(X UU X) = max U tr((U X) U X).\nLet K = (U X) U X, which is a linear kernel on the transformed data in the subspace U X; recalling that the features are centered in the original space, multiplying the data X by the centering matrix H does not make any change. Hence, we can write\nmax U tr((U X) U X) = max U tr((U XH) U XHI) = max U tr(H(U X) U XHI) = max U tr([(U X) U X]HIH) = max U tr(KHIH),(12)\nwhere I is the identity matrix. To derive (12), we have used the identities H = H and XH = XHI and also noted that the trace operator is invariant to the rotation of its arguments.\n\nTo enable providing an interpretation for (12), we recall that identity matrix I represents a kernel on a random variable, where each data sample has maximum similarity to itself and no similarity, whatsoever, to others. Hence, based on empirical HSIC, the objective function given in (12) indicates that the transformation U transforms the centered data 2 XH to a space where the dependency of random variables x and another random variable whose kernel is identity matrix I is maximized. This means that using transformation U, the random variable x is transformed such that each data sample has maximum similarity/correlation to itself and no similarity to other data samples. It is well known in the literature that these bases are the principal components of the signal X that represent the data in an uncorrelated space. With a few manipulations, the objective function given in (12) can be rewritten as follows:\nmax U tr((U X) U X) = max U tr((U XH) U XHI) = max U tr(HX UU XHI) = max U tr(U XHIHX U).\nIn other words, we have shown that the optimization problem in (10) is equivalent to\nmax U tr(U XHIHX U), s.t. U U = I(13)\nAccording to the Rayleigh-Ritz Theorem [54], the solution of the optimization problem in (13) is the top eigenvectors of \u03a6 = XHIHX corresponding to the largest eigenvalues of \u03a6. Here, XHIHX is the covariance matrix of X.\n\nTo summarize, we showed above that the linear decomposition of signals that minimizes the reconstruction error in the mean-squared sense represents the data in an uncorrelated space. This is, in fact, the same as in the principal component analysis (PCA), where the top eigenvectors of the covariance matrix are computed. However, as mentioned before, although minimization of reconstruction error is the ultimate goal in applications such as denoising and coding, in classification tasks, the main goal is maximum discrimination of classes. Hence, we are looking for a decomposition that represents the data in a space where the decomposed data have maximum dependency with their labels. To this end, we propose the new optimization problem as follows\nmax U tr(U XHLHX U), s.t. U U = I(14)\nwhere L is a kernel, e.g., a linear kernel, on the labels Y \u2208 {0, 1} c\u00d7n , i.e., L = Y Y and c is the number of classes. Here, each column of Y is y i = {0, ..., 1, ..., 0} . In other words, there is exactly one nonzero element in each column Y, where the position of the nonzero element indicates the class of the corresponding data sample. The optimization problem given in (14), compromises the reconstruction error to achieve a better discrimination power. Similar to the previous case, the solution for the optimization problem given in (14) is the top eigenvectors of \u03a6 = XHLHX . These eigenvectors compose the supervised dictionary to be learned. This dictionary spans the space where the dependency between data X and corresponding labels Y is maximized. The coefficients can be computed in this space using the lasso as given in (3). However, by recalling (11), a closedform solution for the coefficients can be invoked as being explained next. Thus far, we have already computed U in (11) as the top eigenvectors of \u03a6 = XHLHX . This makes the dictionary, i.e., D = U. By replacing this learned dictionary D into (11), and understanding that V includes the coefficients we can compute them in sparse way by solving the following minimization problem:\nmin \u03b1 n i=1 1 2 D x i \u2212 \u03b1 i 2 2 + \u03bb \u03b1 i 1 ,(15)\nwhere x i \u2208 R p is the i th data sample and \u03b1 i \u2208 R k (k is the number of dictionary atoms) is the corresponding coefficient to be computed. We can write this minimization problem for each data sample separately as follows\nmin \u03b1i 1 2 D x i \u2212 \u03b1 i 2 2 + \u03bb \u03b1 i 1 .(16)\nThe minimization problem in (16) is separable with respect to each element of \u03b1 i . Hence, we can rewrite (16) as\nmin \u03b1i k j=1 1 2 [D x i ] j \u2212 \u03b1 ij 2 + \u03bb |\u03b1 ij | ,(17)\nwhere [D x i ] j and \u03b1 ij are the j th elements of D x i and \u03b1 i , respectively, and |.| is the absolute value of its argument. The problem given in (17) has closed-form solution and can be solved using soft-thresholding [55], [56] with the softthresholding operator S \u03bb (.), i.e.,\n\u03b1 ij = S \u03bb [D x i ] j ,(18)\nwhere S \u03bb (t) is defined as follows\nS \u03bb (t) = \uf8f1 \uf8f2 \uf8f3 t \u2212 0.5\u03bb if t > 0.5\u03bb t + 0.5\u03bb if t < \u22120.5\u03bb 0 otherwise(19)\nIn conclusion, we propose our supervised dictionary learning as given in Algorithm 1.\n\nOne important advantage of the proposed approach in Algorithm 1 is that both the dictionary and coefficients can be computed in closed form. Besides, learning the dictionary and the coefficients are performed separately, and we do not need to learn these two iteratively and alternately, as is common in most supervised dictionary learning approaches in the literature (refer to Subsection II-B).\n\n\nAlgorithm 1 Supervised Dictionary Learning\n\nInput: Training data, X tr , test data, X ts , kernel matrix of labels L, training data size, n, size of dictionary, k. Output: Dictionary, D, coefficients for training and test data, \u03b1 tr and \u03b1 ts . 1: H \u2190 I \u2212 n \u22121 ee 2: \u03a6 \u2190 X tr HLHX tr 3: Compute Dictionary: D \u2190 eigenvectors of \u03a6 corresponding to top k eigenvalues 4: Compute Training Coefficients: For each data sample\n\nx tri in the training set, use \u03b1 ij = S \u03bb [D x tri ] j , j = 1, ..., k to compute the corresponding coefficient 5: Compute Test Coefficients: For each data sample x tsi in the test set, use \u03b1 ij = S \u03bb [D x tsi ] j , j = 1, ..., k to compute the corresponding coefficient\n\n\nC. Kernelized Supervised Dictionary Learning\n\nOne of the main advantages of the proposed formulation for SDL, compared to other techniques in the literature, is that we can easily embed a kernel into the formulation. This enables nonlinear transformation of data into a highdimensional feature space where the discrimination of classes can be more efficiently performed. This is especially beneficial by incorporating data-dependent kernels 3 , such as those based on normalized compression distance [25].\n\nKernelizing the proposed approach is straightforward. Suppose that \u03a8 is a feature map representing the data in feature spaces H as follows:\n\u03a8 : X \u2192 H X \u2192 \u03a8(X).(20)\nTo kernelize the proposed SDL, we express the matrix of bases U as a linear combination of the projected data points into the feature space using representation theory [57], i.e., U = \u03a8(X)W. In other words, W \u2208 R n\u00d7k represents U \u2208 R p \u00d7k in feature space \u03a8(X) \u2208 R p \u00d7n . By replacing X by \u03a8(X) and U by \u03a8(X)W in the objective function of (14) 3 Although it is true that all kernels are computed on the data and hence, are data-dependent, the term is used in the literature to refer to those types of kernels that do not have any closed form.\n\n\nAlgorithm 2 Kernelized Supervised Dictionary Learning\n\nInput: Kernel on training data, K tr , kernel on test data, K ts , kernel on labels L, training data size, n, size of dictionary, k. Output: Dictionary, D, coefficients for training and test data, \u03b1 tr and \u03b1 ts . 1: H \u2190 I \u2212 n \u22121 ee 2: \u03a6 \u2190 K tr HLHK tr 3: Compute Dictionary: D \u2190 top k generalized eigenvectors of the generalized eigenvalue problem \u03a6u = \u03bb 0 Ku. 4: Compute Training Coefficients: For each column k tri of the K tr , use \u03b1 ij = S \u03bb [D k tri ] j , j = 1, ..., k to compute the corresponding coefficient 5: Compute Test Coefficients: For each column k tsi of the K ts , use \u03b1 ij = S \u03bb [D k tsi ] j , j = 1, ..., k to compute the corresponding coefficient whose solution is the top generalized eigenvectors of the generalized eigenvalue problem KHLHKu = \u03bb 0 Ku 4 (\u03bb 0 is a scalar and u is a vector) according to the Rayleigh-Ritz Theorem [54]. To realize how the coefficients can be computed for the training and test sets, we replace U = \u03a8(X)W in (11), knowing that X has to be also replaced by \u03a8(X), to obtain\nV = W \u03a8(X) \u03a8(X) = W K.(22)\nThe form given in (22) is very similar to what is given in (11) and from now on we can use the same steps as provided for the proposed SDL in previous subsection to compute the coefficients. In other words, considering D = W and knowing that V includes the coefficients, we can find the sparse coefficients using similar formulation as in (15), i.e.,\nmin \u03b1 n i=1 1 2 D k i \u2212 \u03b1 i 2 2 + \u03bb \u03b1 i 1 ,(23)\nwhere k i \u2208 R n is one column of kernel matrix K. This problem is again separable and each element of coefficient \u03b1 i can be computed using the soft-thresholding operator\n\u03b1 ij = S \u03bb [D k i ] j .(24)\nThe algorithm for kernelized SDL is given in Algorithm (2).\n\n\nIV. EXPERIMENTS\n\nIn this section, we evaluate the performance of the proposed SDL on various datasets and in different applications such as analyzing face data, digit recognition, and in classification of real-world data such as satellite images and textures. We will show through various experiments the main advantages of the proposed SDL, such as a compact dictionary -i.e., a discriminative dictionary even at small dictionary sizeand fast performance. Also, we will show how its kernelized version enables embedding data-dependent kernels into the proposed SDL to significantly improve the performance on difficult classification tasks. Table I provides the details of the datasets used in our experiments, their dimensionality, number of classes, and the number of instances per class, as well as in the training and test sets as used in our experiments.\n\n\nA. Implementation Details\n\nIn our approach, the first step is to compute the dictionary by computing the (generalized) eigenvectors of \u03a6 as provided in Algorithms 1 or 2. To avoid rank deficiency in the computation of kernel on labels, we add the identity matrix of the same size to the kernel, i.e., L = Y Y + I. Then we need to calculate the coefficients using soft-thresholding approach as given in (18) or (24) for the proposed SDL or KSDL, respectively. The optimal value of the regularization parameter in soft thresholding (\u03bb * ), which controls the level of sparsity, has been computed by 10-fold cross-validation on the training set to minimize the mean-squared error. This \u03bb * is then used to compute the coefficients for both training and test sets 5 .\n\nAs is suggested in [58], the coefficients computed on the training set are used for training a support vector machine (SVM). RBF kernel has been used for the SVM and the optimal parameters of the SVM, i.e., the optimal kernel width \u03b3 * and trade-off parameter C * , are found by grid search and 5-fold cross-validation on the training set 6 . The coefficients computed on the test set are then submitted to this trained SVM to label unseen test examples.\n\nTwo measures are considered to evaluate the performance of the classification systems: classification error and balanced classification error, which are defined as follows:\nE = n wr n ,(25)BE = 1 c c i=1 n i wr n i ,(26)\nwhere E and BE are classification error and balanced error, respectively; n wr is the total wrongly-classified data samples; n is the total number of data samples; c is the number of classes; n i wr is the number of wrongly-classified objects in class i; and n i is the number of data samples in class i. According to this definition, E is the total number of wrongly-classified data samples over the total number of objects. Hence, if there are fewer objects in one class, wrongly-classified objects in that class contribute less towards the overall classification system error. The definition of BE, however, gives the same weight to all classes irrespective of the number of objects in each class. To further clarify the difference between these two measures, we consider an extreme case. Suppose that in a two-class problem, there are 98 objects in one class and 2 objects in another class. If all 98 objects are correctly classified in class one, and out of 2 objects in class two, only one is correctly classified, the classification error is E = 1/100 = 1%, whereas the balanced error is BE = (1/2 + 0/98)/2 = 25%. If, \n\n\nB. Face Data\n\nIn this experiment, our main goal is to show the compactness of our proposed dictionary. We use the Olivetti face dataset of AT&T [59]. This data consists of 400 face images of 40 distinct subjects, i.e., 10 images per subject, with varying lighting, facial expressions (open/closed eyes, smiling/not smiling) and facial details (glasses/no glasses). The original size of each image is 92\u00d7112 pixels, with 256 gray levels per pixel. However, in our experiments, each image has been cropped from the center to be 64\u00d764 pixels.\n\nThe main task in our experiments is to classify the faces into glasses/no-glasses classes. To this end, the images are labeled to indicate these two classes, with 119 in the glasses class and 281 in the no-glasses. Typical images of these two classes are shown in Fig. 1. All images are normalized to have zero mean and unit 2 -norm. Half of the images are randomly selected for training, and the other half for testing; the experiments are repeated 10 times, and the average error (E) and balanced error (BE) are reported in Table II. The experiments are performed on varying dictionary sizes, including 2, 4, 8, 16, and 32. The results are compared with several unsupervised and supervised dictionary learning approaches, as shown in Table II. For K-SVD, the fast implementation provided by Rubinstein [60] has been used. We have implemented DK-SVD with K-SVD as the core. The difference between supervised and unsupervised k-means is that in unsupervised k-means, the dictionary is learned on the whole training set, whereas in the supervised one, one dictionary is learned per class as suggested in the texton-based approach by Varma and Zisserman [17], [32]. The code for metaface approach has been provided by the authors [33]. The same as our approach, the parameter(s) of all these rival approaches are tuned using 5-fold cross-validation on the training set.\n\nAs can be seen in Table II, our approach performs the best among these approaches. The compactness of the dictionary learned using the proposed SDL is noticeable from the results at small dictionary size. For example, at the dictionary size of two, while the error of our approach is 12.60%, unsupervised k-means yields a 27.4% error, which is more than twice as large as our approach. The best result obtained by other supervised dictionary approaches (here metaface) yields a 17.55% error at this dictionary size, which is more than 5% above the error generated by the proposed SDL. The same conclusion can be made using balanced error. Interestingly, supervised kmeans performs significantly better than the unsupervised one, particularly at small dictionary sizes. The main conclusion of this experiment is that the proposed SDL generates a very discriminative and compact dictionary, compared to well-known unsupervised and supervised dictionary learning approaches.\n\n\nC. Digit Recognition\n\nThe second experiment is performed on the task of handwritten digit classification on the USPS dataset [61]. This dataset consists of handwritten digits, each with the size of 16\u00d716 pixels with 256 gray levels. There are 7291 and 2009 digits in the training and test sets, respectively.\n\nWe compare our results with the most recent SDL technique, which yields the best results published so far on this  [62], and since neither our approach nor the one reported in [41] benefit from these kind of features, as suggested in [41], the training set is artificially augmented by adding digits which are shifted version of original ones, moved by one pixel in all four directions. Although this is not an optimal and sophisticated way of introducing shift invariance to the SDL techniques, it takes into account this property in a fairly simple approach. Each digit in training and test sets is normalized to have zero mean and unit 2 -norm. Table III shows the results obtained using the proposed approach in comparison with the unsupervised and supervised dictionary learning techniques reported in [41]. As can be seen, again our approach introduces a very compact dictionary such that its performance at dictionary size of 50 is the same as the performance of the system reported in [41] using a dictionary of 100 atoms. With increasing the dictionary size, the performance of our approach slightly degrades. This is mainly because the bases or dictionary atoms in our approach are associated with the directions of maximum separability of the data, as has been enforced by the optimization problem in (14). Nevertheless, the number of useful bases depends on the intrinsic dimensionality of the subspace, which in turn depends on the nature of the data. If the number of dictionary atoms goes beyond this intrinsic dimensionality, then adding more atoms does not improve the performance but may degrade it, as they are not associated with separable directions but related to noise. On the other hand, it is important to notice that we can achieve a reasonable performance using much less complexity than the best rival. It should be also noted that the best performance achieved by our approach (happening at a small dictionary size of 50) is just 0.25% worse than the best results obtained by [41] (happening at dictionary size of 300, i.e., with much higher complexity). This means that our approach misclassifies only 5 more digits compared to the best results obtained in [41], whereas for the same dictionary size (50), our approach performs 0.55% better, i.e., classifies 11 more digits correctly. On the other hand, w.r.t. the complexity, our proposed approach offers a much simpler solution for SDL than the approach in [41]: there are fewer parameters to tune, the dictionary can be computed in closed form, and there is no need to solve a complicated nonconvex optimization problem as used in [41] by iteratively and alternately optimizing classifier, dictionary, and coefficient learning. As a final remark, due to the orthonormality constraint in the optimization problem of our proposed SDL as given in (14), overcompleteness is not possible in our proposed SDL. This is the reason that in Table III, no results are reported for a dictionary size of 300 for our approach. However, as mentioned above, due to the compactness of our dictionary, good results are obtained at a much smaller dictionary size, which is a desired attribute as it decreases the computational load. Also, the proposed kernelized version of our proposed approach given in (21) and Algorithm 2 can learn dictionaries as large as n, i.e., the number of data points used for training, which is usually greater than the dimensionality of the data p (see Table I for the relative size of p and n for the data used in our experiments).\n\n\nD. Other Real-World Data\n\nIn the two previous sections, the classification task was performed on the pixels of images directly. In this section, we evaluate the performance of the proposed approach on the classification of some real-world data using features extracted. Four datasets with varying complexity from 2-to 11-class, with the dimensionality of up to 60 features, and also with as many as 6435 data samples are used in these experiments (refer to Table I for detailed information on these datasets). All data are preprocessed to have zero mean and unit 2 -norm, except Satimage dataset, where the features are normalized to be in the range of [0, 1] due to the large variation of feature values.\n\nSince the rival approaches are the same as that used for face data, their implementations are the same as was explained in Subsection IV-B. There is one additional remark here on the implementation of supervised k-means on datasets with more than two classes, such as the Texture and Satimage datasets. We have implemented this approach in a way to ensure that the dictionary atoms are evenly computed over different classes as much as possible. For example, in the case of dictionary size of 8 and for the Texture dataset that has 11 classes, we have first selected 11 dictionary atoms, one from each class, then 8 of them are randomly selected.\n\nOn all datasets, the experiments are repeated ten times over a random split of data into half for training and half for testing. The average and standard deviation of classification error (E) and balanced error (BE) are reported in Tables IV and V, respectively, in comparison with several other unsupervised and supervised dictionary learning approaches. Since the texture dataset is balanced, the error and balanced error are the same, therefore, the balanced error has not been reported for this dataset in Table V. We have also included the results of classification using a kernelized version of our proposed SDL with radial basis function (RBF) as the kernel. The width of the RBF kernel has been selected based on a self-tuning approach [63].\n\nAs can be seen from Tables IV and V, the proposed SDL or its kernelized version performs the best in all cases, except for the dictionary size of 8 and 16 on the Sonar data. The better performance of supervised k-means at the dictionary sizes of 8 and 16 is not significant, as the resultant standard deviation is very high. DK-SVD performs poorly (even worse than the unsupervised K-SVD approach) on these datasets mainly because, by design, it uses a linear classifier (refer to Subsection II-B and [38] for more description on this approach). The poor performance of metaface is because it usually performs well at very large dictionary size. Hence, at reported dictionary sizes, its training is not sufficient to capture the underlying data structure. For example, for Sonar data, while the proposed SDL can achieve an error of 20.77\u00b14.67 at the dictionary size of 32, the metaface approach can only achieve this accuracy at the dictionary size of 64 (error 20.00\u00b14.75). However, using large dictionary size adds to the computational load of the approach.\n\nAs a final remark on the results presented in this subsection, we would like to comment on the relative performance of proposed SDL and its kernelized version KSDL. The relative performance of these two approaches mainly depends on the nature of the data to be classified, and whether it has a linear or nonlinear behavior. In other words, it depends whether the data can be represented as a subspace or a submanifold. In the former case, the proposed SDL should be sufficient to model the data, while in the latter case, the KSDL should potentially perform better. However, the success of KSDL depends on the proper selection of the kernel and its parameter(s). In fact, even if the data has a linear nature and can be represented in a subspace, the KSDL should also perform as well as SDL, but this again depends on proper kernel and model selection. In the next subsection, we will show how choosing a proper kernel can significantly improve the results using KSDL approach for a rather complicated dataset.\n\n\nE. Patch Classification on Texture Data\n\nTo show the benefit of using data-dependent kernels such as kernels computed using normalized compression distance [25], in this section, we perform classification on patches extracted from texture images. We compare our results with and without kernels using the proposed approach, and also compare them to the results published in [15], i.e., two supervised dictionary learning approaches called SDL-G BL (G for generative and BL for bilinear model) and SDL-D BL (D for discriminative). To ease the comparison, we use the same data as in [15], i.e., classification on texture pair of D5 and D92 from the Brodatz album, shown in Fig. 2. Also the same as [15], 300 patches are randomly extracted from the left half of each texture image for training and 300 patches from the right half for testing. This is to ensure that there is no overlap among the patches used in the training and test sets.\n\nWe have used the RBF kernel and two data-dependent compression-based kernels as reported in [64] (CK-1) and [26] (d N ) as the kernel for the proposed kernelized SDL. The latter deploys MPEG-1 as the compressor as suggested in [64] for the computation of normalized compression distance [25]. However, compared to the measure proposed in [64] (CK-1), it proposes a novel compression-based dissimilarity measure (d N ) that performs well on both small and large patch sizes (as shown in [26], CK-1 does not work properly on small patch sizes). Besides, d N is a semi-metric. Table VI provides the results of classification using the proposed SDL with and without kernels. It also compares the results with k-means as an unsupervised approach to compute the dictionary, and with the results published in [15] for the same number of patches (300) and the same dictionary size, i.e., 64. The sparsity of the coefficients, i.e., the number of nonzero coefficients, are also provided in this table (this is not reported for SDL-G BL and SDL-D BL in [15]). As can be seen, using a compression-based data-dependent kernel based on d N dramatically improves the results. The classification error is even lower than the one obtained by the SDL-D BL approach using 30000 patches for training, which yields the best results on this data with the classification error = 14.26% in [15]. Moreover, as the sparsity of the coefficients indicate, the proposed approach with data-dependent kernel d N deploys the smallest number of dictionary atoms in the reconstruction of the signal, i.e., benefits the most from the sparse representation, as it uses almost half of the dictionary elements compared to other approaches. This has a great impact on the computation load of the classification task, especially in the stage of training and testing of the classifier. Our experiments show (not reported in Table VI) that by using   a slightly larger regularization parameter \u03bb in soft thresholding such that the reconstruction error is within one standard deviation of the minimum, the sparsity of coefficients can be significantly increased. That is, the average number of nonzero coefficients can be reduced to about 5% of the total number of coefficients without compromising the classification error. The classification error is 9.90\u00b11.43 in this case, which is even slightly better than what is reported in Table VI.\n\n\nF. The Effect of Noisy Labels on the Performance of the Proposed SDL\n\nSince in supervised dictionary learning approaches the information category is used in the learning of the dictionary, one main question will be: \"to what extent are these approaches sensitive to noisy labels?\". In this subsection we will try to address this question.\n\nAs defined in Subsection III-B, the labels Y \u2208 {0, 1} c\u00d7n can only take the values 0 or 1. Therefore, what we mean by noisy labels is that 0 might be converted to 1, or vice versa. We assume that in each column of noisy labels\u0176, there is still only one nonzero element, which indicates the class of the corresponding object.\n\nAlmost all the categories of SDL mentioned in Subsection II-B utilize the labels directly or indirectly in the learning of the dictionary. For example, in the first SDL category, one dictionary is learned per class. Therefore, if one object is wrongly assigned to a class, this object will contribute to learning dictionary atoms in the wrong class, which consequently may lead to reducing the efficiency of the learned dictionary in the classification task. In our proposed approach, as indicated a The average no. of nonzero coefficients is not provided for this approach in [15].\n\nin the optimization problem (14), a linear kernel over the labels is used to include the category information in the learning of the dictionary. Hence, it is natural that we expect that noisy labels degrade the efficacy of the dictionary learned in the classification task.\n\nTo address the question raised in the beginning of this subsection, we have performed experiments on the Olivetti face dataset. In these experiments, we have included a certain percentage of wrong labels in the learning of the supervised dictionary, and then performed the classification task using this dictionary. Since our main concern is to see how sensitive the dictionary is to noisy labels, correct labels are used in the classifier over the training set. In other words, in our experiments, noisy labels are only used in the learning of the dictionary, and correct labels in the classifier. This may not be a realistic setup as when we have wrong labels, we do not have the correct labels, otherwise we could also use them in the learning of the dictionary. However, if we use wrong labels in the classifier as well, we do not know to what extent the dictionary is affected by wrong labels because wrong classification might be also due to misguiding the classifier.\n\nThe results are shown in Fig. 3 for the dictionary sizes of 2, 4, 8, and 16, and for various supervised dictionary learning approaches as used in the experiments on the Olivetti face dataset (refer to Table II). As can be seen from this figure, our proposed SDL is the least sensitive to noisy labels. Also, by increasing the dictionary size, the sensitivity to noisy labels is reduced for our proposed SDL as well as for the supervised k-means. It makes sense to see lower sensitivity to noisy labels at larger dictionary sizes for the proposed SDL, because noisy labels will cause the discriminative directions to move away from leading atoms or bases in the learned dictionary, which degrades the effectiveness of the dictionary at small dictionary sizes, while at larger dictionary sizes, these discriminative directions will appear again, although not in leading atoms. Also in supervised k-means, by increasing the dictionary size, it is more likely that some of the cluster centers in each class, which are the dictionary atoms in that class, correspond to the correctly-labeled data samples. For example, if the dictionary size is two in a two-class problem, there is only one dictionary atom per class. Hence, if this dictionary atom represents wrong data samples due to noisy labels, the dictionary learned completely fails to model the data samples correctly. However, by increasing the dictionary size, this catastrophic failure is less likely to happen.\n\nHowever, this phenomenon cannot be observed for the DK-SVD and the metaface approaches. DK-SVD does not follow this behavior mainly because the learning of the dictionary and classifier is performed in one optimization problem, as explained in Subsection II-B and in [38]. Hence, noisy labels also affect the learning of the linear classifier involved, and we could not find any way to include the noisy labels only in the learning of the dictionary, not the classifier.\n\nSimilarly, in the metaface approach, the class labels used during learning the dictionary are used to tag each dictionary as to what class it belongs to. This tag is later used to indicate the class label of the test object that minimizes the residue obtained using the reconstruction error computed on the subdictionary elements belonging to a class and a test object. Therefore, similar to the DK-SVD approach, there is no way to include the noisy class labels only in the learning of the dictionary, and not in the classifier. This explains why noisy labels have greater impact on DK-SVD and metaface approaches, as they affect both the dictionary learning and training of the classifiers. Based on these explanations, we admit that comparing the effect of noisy labels on our proposed SDL with DK-SVD or metaface is not completely fair, as in our approach (as well as in supervised k-means) we deliberately avoided the impact of noise on the training of classifiers, whereas we could not avoid it in the DK-SVD and metaface approaches.\n\nV. DISCUSSIONS AND CONCLUSIONS In this paper we proposed a novel supervised dictionary learning. The proposed approach learns the dictionary in a space where the dependency between the data and category information is maximized. Maximizing this dependency has been performed based on the concept of the Hilbert Schmidt independence criterion (HSIC). This introduces a data decomposition that represents the data in a space with maximum dependency with category information. We showed that both the dictionary and sparse coefficients can be learned in closed form. Our experiments using real-world data with varying complexity shows that the proposed approach is very efficient in classification tasks, and outperforms other unsupervised and supervised dictionary learning approaches in the literature. Also, the proposed approach is very fast and efficient in computation. \n\n\nExperiments)\n\nProposed SDL Supervised k-means DK-SVD Metaface (d) Fig. 3: The error rate of the classification system for Olivetti face recognition system to discriminate between faces with and without glasses. The effect of noisy labels in learning dictionary are shown for the dictionary sizes of (a) 2, (b) 4, (c) 8, and (d) 16.\n\nMoreover, we showed how the proposed SDL can be kernelized. This enables the proposed SDL to benefit from data-dependent kernels. It was shown using some experiments that the proposed kernelized SDL can significantly improve the results in difficult classification tasks compared to other SDL approaches in the literature. To the best of our knowledge, this is the first SDL in the literature that can be kernelized, and thus benefit from data-dependent kernels embedded into the SDL.\n\nThe proposed approach learns a very compact dictionary, in the sense that it significantly outperforms other approaches when the size of the dictionary is very small. This shows that the proposed SDL can effectively encode the category information into the learning of the dictionary such that it can perform very well in classification tasks using few atoms. In the dictionary learning literature, usually the dictionary learned is overcomplete, i.e., the number of elements in the learned dictionary is larger than the dimensionality of the data/dictionary. In our proposed SDL, due to the orthonormality constraint on the dictionary atoms as detailed in (14), the dictionary cannot be overcomplete. However, there are two remarks here: first, as discussed above, our dictionary is very compact and as the experiments show, the proposed SDL performs very well at small dictionary size, which is usually below even complete dictionary size. This is a main advantage of the proposed approach, as small dictionary size means lower computational cost. Second, the kernelized version of the proposed approach can easily learn dictionaries as large as n, the number of data samples in the training set. This is because the kernel computed on the data is of the dimensionality of n, which is usually greater than the dimensionality of the data (p). Note that for all datasets provided in this paper except the Olivetti face dataset, the number of data in the training set is larger than the dimensionality of data (refer to Table I). For the face dataset, it is worth noting that a dictionary as small as 32 atoms leads to extremely good results using the proposed SDL, and overcompleteness is not necessary here.\n\nAnother advantage of the proposed approach is that there is only one parameter to be tuned, which is the regularization parameter \u03bb in soft thresholding. Since the dictionary is learned in closed form, it is extremely fast to tune this parameter within the classification task or by minimizing the reconstruction error. Other SDL approaches in the literature usually have several parameters to be tuned, and since learning the dictionary and coefficients have to be performed alternately and iteratively, it is very time-consuming to tune these parameters using a cross-validation on the training set.\n\nThrough some experiments, we showed that our proposed approach is less sensitive to noisy labels compared to other SDL approaches. It was also shown that by increasing the number of atoms in the dictionary, the proposed approach becomes less sensitive to noisy labels.\n\nIn this research, we proposed to use L = Y Y + I as the kernel on the labels. As proposed in [65], [66], it is possible to encode the relationship among the classes into a matrix M \u2208 R c\u00d7c , where c is the number of classes, and use L = Y MY + I instead to build up the kernel on the labels. This may consequently better encode the data structure into the learning of the dictionary, and also reduce the sensitivity of the proposed approach to noisy labels. As a future work, we will implement this new kernel in the formulation provided for Algorithm 1.\n\nAlso, the kernel L is a general kernel over the labels. However, to avoid the need for tuning the kernel on different datasets and for consistency, we limited ourselves to linear kernels in this paper. In future work, we will show how by proper selection of the kernels over the labels, we can benefit the most from the supervised dictionaries learned, which are particularly designed for a specific application in hand.\n\nMoreover, we have used an SVM with an RBF kernel on the sparse coefficients learned for performing the classification task. However, model selection is still an open research problem [67]. For example, the RBF kernel may not fully utilize the sparsity of the coefficients. In future work, we will consider other kernels for the SVM or other classifiers that can benefit more from the sparse nature of data points submitted for classification, as suggested in [58].\n\n\n, we obtain tr(U \u03a8(X)HLH\u03a8(X) U ) = tr(W \u03a8(X) \u03a8(X) HLH\u03a8(X) \u03a8(X)W) = tr(W KHLHKW), with the constraint U U = W \u03a8(X) \u03a8(X)W = W KW, where K = \u03a8(X) \u03a8(X) is a kernel function on data. Combining this objective function and the constraint, the optimization problem for the kernelized SDL is max W tr(W KHLHKW), s.t. W KW = I\n\nFig. 1 :\n1Typical face images from the Olivetti face dataset in two classes of glasses vs. no-glasses.\n\nFig. 2 :\n2Texture images of D5 and D92 from Brodatz album.\n\nTABLE I :\nIThe datasets used in this paper.Dataset \nDataset Info. \nSamples \nSamples per Class \nTraining Size Test Size Classes Dim. \nFace (Olivetti) a \n400 \n119, 281 \n200 \n200 \n2 \n4096 \nDigit (USPS) b \n9298 \n-\n7291 \n2007 \n10 \n256 \nSonar c \n208 \n97, 111 \n104 \n104 \n2 \n60 \nIonosphere c \n351 \n225, 126 \n176 \n175 \n2 \n34 \nTexture (I) d \n5500 \n500 \n2750 \n2750 \n11 \n40 \nSatimage d \n6435 \n1533, 703, 1358, 626, 707, 1508 \n3218 \n3217 \n6 \n36 \nTexture (II) e \n600 \n300 \n300 \n300 \n2 \n256 \n\na http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html \nb http://www-i6.informatik.rwth-aachen.de/\u223ckeysers/usps.html \nc http://archive.ics.uci.edu/ml/ \nd http://www.dice.ucl.ac.be/neural-nets/Research/Projects/ELENA/databases/REAL/ \ne http://www.ux.uis.no/\u223ctranden/ \n\nfor example, this classification system is supposed to classify \nhealthy versus unhealthy cases, BE is a better measure to \nevaluate the classification system, because both classes equally \ncontribute towards the estimation of error irrespective of the \nnumber of data samples in each. Since as indicated on the third \ncolumn of Table I, some datasets used in our experiments, such \nas Face, Sonar, Ionosphere, and Satimage, are not balanced 7 , \nwe have provided both E and BE for them in next subsections. \n\n\n\nTABLE II :\nIIClassification error (E) and balanced error (BE) on test set for Olivetti face data using the proposed SDL. The results are compared with several other dictionary learning approaches in the literature. The best results obtained are highlighted.Approach \nDictionary Size \n2 \n4 \n8 \n16 \n32 \nE \nBE \nE \nBE \nE \nBE \nE \nBE \nE \nBE \n\nUnsupervised \nk-means \n27.40 \n39.35 \n22.60 \n29.05 \n13.15 \n17.36 \n8.15 \n10.71 \n5.75 \n8.40 \n\u00b12.04 \n\u00b14.01 \n\u00b15.18 \n\u00b15.97 \n\u00b12.38 \n\u00b14.17 \n\u00b11.81 \n\u00b13.12 \n\u00b11.70 \n\u00b12.62 \nK-SVD [28] \n28.20 \n41.40 \n20.60 \n27.36 \n9.65 \n12.97 \n7.75 \n11.07 \n4.05 \n6.06 \n\u00b12.45 \n\u00b14.60 \n\u00b12.41 \n\u00b15.23 \n\u00b11.62 \n\u00b12.52 \n\u00b12.06 \n\u00b13.13 \n\u00b11.23 \n\u00b12.26 \n\nSupervised \n\nProposed SDL \n12.60 \n16.60 \n10.30 \n12.70 \n5.30 \n6.21 \n4.95 \n6.06 \n3.55 \n4.68 \n\u00b13.51 \n\u00b14.16 \n\u00b12.59 \n\u00b13.50 \n\u00b12.11 \n\u00b12.92 \n\u00b11.14 \n\u00b11.88 \n\u00b11.59 \n\u00b12.38 \nDK-SVD [38] \n17.80 \n19.36 \n10.25 \n10.15 \n8.75 \n11.25 \n7.05 \n8.70 \n6.75 \n10.13 \n\u00b13.06 \n\u00b13.52 \n\u00b12.48 \n\u00b13.04 \n\u00b12.02 \n\u00b14.06 \n\u00b12.11 \n\u00b11.31 \n\u00b11.53 \n\u00b12.92 \nk-means a [17] \n17.75 \n23.45 \n10.40 \n14.01 \n7.40 \n10.57 \n5.55 \n7.84 \n3.65 \n5.45 \n\u00b13.65 \n\u00b15.71 \n\u00b12.56 \n\u00b12.76 \n\u00b11.90 \n\u00b12.93 \n\u00b11.62 \n\u00b12.58 \n\u00b11.20 \n\u00b12.01 \nMetaface [33] \n17.55 \n19.39 \n11.25 \n15.35 \n9.75 \n14.58 \n7.60 \n11.74 \n5.45 \n9.28 \n\u00b12.87 \n\u00b13.02 \n\u00b12.35 \n\u00b12.61 \n\u00b13.58 \n\u00b15.88 \n\u00b11.39 \n\u00b11.91 \n\u00b10.96 \n\u00b11.46 \n\na Supervised k-means learns one sub-dictionary per class and then compose all learned sub-dictionaries into one. \n\ndataset [41]. To facilitate a direct comparison with what \nis published in [41], we use the same setup as they have \nreported. To this end, since the most effective techniques on \ndigit recognition deploy shift invariant features \n\nTABLE III :\nIIIClassification error on test set for digit recognition on USPS data using proposed SDL compared with the most effective SDL approach reported in the literature on the same data[41]. Highlighted entries represent the best results obtained at each dictionary size.Approach \nDictionary Size \n50 \n100 \n200 \n300 \nUnsupervised [41] \n8.02 6.03 5.13 4.58 \nSupervised [41] \n3.64 \n3.09 2.88 2.84 \nProposed SDL \n3.09 \n3.19 3.24 \n-\n\n\n\nTABLE IV :\nIVThe results of classification error (%) on various real-world datasets using different methods and in different dictionary sizes. The best results obtained are highlighted.Approach \nSonar \nIonosphere \nTexture \nSatimage \n8 \n16 \n32 \n8 \n16 \n32 \n8 \n16 \n32 \n8 \n16 \n32 \n\nUnsupervised \nk-means \n28.56 \n24.52 \n24.42 \n7.37 \n7.71 \n8.06 \n2.49 \n1.12 \n0.97 \n13.36 \n13.02 \n12.87 \n\u00b15.53 \n\u00b15.43 \n\u00b13.77 \n\u00b12.48 \n\u00b11.41 \n\u00b11.86 \n\u00b10.66 \n\u00b10.25 \n\u00b10.29 \n\u00b10.47 \n\u00b10.64 \n\u00b10.72 \nK-SVD [28] \n27.31 \n24.81 \n28.56 \n8.69 \n9.09 \n8.00 \n1.54 \n0.81 \n0.83 \n10.42 \n10.70 \n11.92 \n\u00b12.69 \n\u00b16.69 \n\u00b14.25 \n\u00b14.12 \n\u00b11.73 \n\u00b11.50 \n\u00b10.30 \n\u00b10.27 \n\u00b10.19 \n\u00b10.43 \n\u00b10.73 \n\u00b10.36 \n\nSupervised \n\nProposed SDL \n27.79 \n22.50 \n20.77 \n5.94 \n5.60 \n5.43 \n1.44 \n0.45 \n0.31 \n11.25 \n10.58 \n10.66 \n\u00b13.47 \n\u00b12.73 \n\u00b14.67 \n\u00b11.66 \n\u00b11.41 \n\u00b11.41 \n\u00b10.38 \n\u00b10.12 \n\u00b10.10 \n\u00b10.36 \n\u00b10.40 \n\u00b10.41 \nKSDL-RBF a \n28.75 \n27.31 \n26.35 \n5.66 \n5.89 \n6.17 \n1.68 \n1.20 \n1.19 \n10.18 \n9.81 \n9.66 \n\u00b13.88 \n\u00b14.40 \n\u00b13.22 \n\u00b11.97 \n\u00b12.03 \n\u00b12.07 \n\u00b10.26 \n\u00b10.23 \n\u00b10.22 \n\u00b10.36 \n\u00b10.38 \n\u00b10.33 \nDK-SVD [38] \n32.40 \n32.69 \n29.04 \n16.11 \n18.00 \n15.89 \n27.91 \n6.15 \n7.28 \n35.36 \n20.15 \n28.89 \n\u00b14.53 \n\u00b14.32 \n\u00b14.15 \n\u00b11.88 \n\u00b13.51 \n\u00b12.50 \n\u00b13.87 \n\u00b10.82 \n\u00b11.86 \n\u00b113.29 \n\u00b11.38 \n\u00b14.19 \nk-means [17] \n24.62 \n22.31 \n22.88 \n7.54 \n9.54 \n10.00 \n2.11 \n0.95 \n0.82 \n13.61 \n12.65 \n12.98 \n\u00b15.31 \n\u00b14.27 \n\u00b15.98 \n\u00b11.39 \n\u00b11.59 \n\u00b12.35 \n\u00b10.42 \n\u00b10.14 \n\u00b10.22 \n\u00b10.36 \n\u00b10.32 \n\u00b10.65 \nMetaface [33] \n26.74 \n27.89 \n23.17 \n18.29 \n21.54 \n16.29 \n9.76 \n10.03 \n4.64 \n23.43 \n27.14 \n24.85 \n\u00b13.17 \n\u00b15.22 \n\u00b14.43 \n\u00b11.62 \n\u00b12.89 \n\u00b12.52 \n\u00b10.55 \n\u00b11.88 \n\u00b10.57 \n\u00b11.38 \n\u00b11.05 \n\u00b11.53 \n\na Proposed kernel SDL with RBF kernel. \n\n\n\nTABLE V :\nVThe results of classification balanced error (%) on various real-world datasets (except texture data, for which the \nerror and balanced error are the same as the dataset is balanced) using different methods and in different dictionary sizes. The \nbest results obtained are highlighted. \n\nApproach \nSonar \nIonosphere \nSatimage \n8 \n16 \n32 \n8 \n16 \n32 \n8 \n16 \n32 \n\nUnsupervised \nk-means \n28.41 \n24.26 \n24.25 \n8.36 \n9.40 \n9.25 \n16.99 \n16.39 \n16.50 \n\u00b15.66 \n\u00b14.93 \n\u00b13.97 \n\u00b13.11 \n\u00b11.56 \n\u00b12.97 \n\u00b10.65 \n\u00b10.86 \n\u00b10.73 \nK-SVD [28] \n27.09 \n24.87 \n28.73 \n10.35 \n10.63 \n8.46 \n13.31 \n13.44 \n15.16 \n\u00b12.61 \n\u00b16.36 \n\u00b14.27 \n\u00b14.97 \n\u00b12.05 \n\u00b11.60 \n\u00b10.54 \n\u00b10.85 \n\u00b10.48 \n\nSupervised \n\nProposed SDL \n27.75 \n22.65 \n20.86 \n7.06 \n6.22 \n6.20 \n14.07 \n13.12 \n13.25 \n\u00b13.50 \n\u00b12.87 \n\u00b14.84 \n\u00b11.57 \n\u00b11.39 \n\u00b11.62 \n\u00b10.33 \n\u00b10.54 \n\u00b10.49 \nKSDL-RBF a \n28.64 \n27.12 \n26.03 \n6.06 \n6.33 \n6.58 \n12.96 \n12.32 \n12.14 \n\u00b13.91 \n\u00b14.79 \n\u00b13.28 \n\u00b11.92 \n\u00b12.13 \n\u00b12.20 \n\u00b10.40 \n\u00b10.60 \n\u00b10.46 \nDK-SVD [38] \n33.61 \n33.79 \n29.56 \n18.98 \n20.17 \n18.78 \n35.64 \n22.22 \n29.85 \n\u00b13.76 \n\u00b14.46 \n\u00b14.11 \n\u00b13.54 \n\u00b14.67 \n\u00b14.49 \n\u00b18.80 \n\u00b11.51 \n\u00b13.42 \nk-means [17] \n24.74 \n22.32 \n22.48 \n8.61 \n11.29 \n11.79 \n17.21 \n16.01 \n16.55 \n\u00b15.06 \n\u00b14.26 \n\u00b15.65 \n\u00b11.92 \n\u00b11.94 \n\u00b13.34 \n\u00b10.48 \n\u00b10.52 \n\u00b10.88 \nMetaface [33] \n27.35 \n30.20 \n23.76 \n25.79 \n29.67 \n24.43 \n30.26 \n32.89 \n32.26 \n\u00b14.13 \n\u00b13.44 \n\u00b16.19 \n\u00b12.98 \n\u00b13.64 \n\u00b13.32 \n\u00b11.94 \n\u00b12.01 \n\u00b12.27 \n\na Proposed kernel SDL with RBF kernel. \n\n\n\nTABLE VI :\nVIClassification error and the number of nonzero coefficients on the test set for texture pair D5-D92 of Brodatz album. The dictionary size is 64. Using data-dependent kernels and the proposed kernelized SDL can significantly improve the results.Approach \nAverage No. of Nonzero Coefficients \nTrain Set \nTest Set \nClassification Error (%) \nk-means \n47.85 \n48.99 \n27.75\u00b12.29 \nProposed SDL \n59.80 \n59.85 \n26.43\u00b12.95 \n\nProposed kernel \nSDL \n\nRBF \n62.88 \n62.51 \n28.85\u00b11.84 \nCK-1 [64] \n64 \n64 \n26.05\u00b11.07 \nd N [26] \n33.46 \n31.53 \n10.15\u00b11.30 \nSDL-G BL a [15] \n-\n-\n26.34 \nSDL-D BL a [15] \n-\n-\n26.34 \n\n\n0 norm of vector x is defined as x 0 = #{i : x i = 0}.\nHere, centered data means that the features are centered, not individual data samples.\nWe have used \u03bb 0 here as it is different from \u03bb in the lasso and softthresholding.\nOne \u03bb * is computed for each data point in the training set. However, the averaged \u03bb * over the whole training set is used to compute the coefficients on the training and test sets as it yields better generalization.6 10-fold cross-validation yields very close results. Thus to avoid higher computation load, 5-fold cross-validation is adopted.\nThe USPS digit dataset is also somewhat imbalanced. However, since in the literature, particularly in[41] with which our results are compared, only classification error (E) is provided, we also present our results using this measure only. Also since the publically available USPS data comes in separate training and test sets, and representing the number of instances per class takes space for 10 classes, we have not provided this information for the USPS dataset inTable I.\nACKNOWLEDGMENTThe authors gratefully acknowledge the comment by the reviewer who pointed out the separability problem and consequently closed-form solution on the coefficients.Ali Ghodsi is an Associate Professor in the Department of Statistics at the University of Waterloo. He is also cross-appointed with the school of Computer Science, a member of the Center for Computational Mathematics in Industry and Commerce and the Artificial Intelligence Research Group at the University of Waterloo. His research involves applying statistical machine-learning methods to dimensionality reduction, pattern recognition, and bioinformatics problems. Dr. Ghodsi's research spans a variety of areas in computational statistics. He studies theoretical frameworks and develops new machine learning algorithms for analyzing large-scale datasets, with applications to bioinformatics, data mining, pattern recognition, and sequential decision making.\nA Wavelet Tour of signal Processing: The Sparse Way. S Mallat, Academic Press3rd edS. Mallat, A Wavelet Tour of signal Processing: The Sparse Way, 3rd ed. Academic Press, 2009.\n\nEmergence of simple-cell receptive field properties by learning a sparse code for natural images. B Olshausen, D Field, Nature. 3816583B. Olshausen and D. Field, \"Emergence of simple-cell receptive field properties by learning a sparse code for natural images,\" Nature, vol. 381, no. 6583, pp. 607-609, Mar. 1996.\n\nIndependent Component Analysis. A Hyv\u00e4rinen, J Karhunen, E Oja, John Wiley & SonsNew YorkA. Hyv\u00e4rinen, J. Karhunen, and E. Oja, Independent Component Anal- ysis. New York: John Wiley & Sons, 2001.\n\nRegression shrinkage and selection via the lasso. R Tibshirani, Journal of the Royal Statistical Society, Series B. 581R. Tibshirani, \"Regression shrinkage and selection via the lasso,\" Journal of the Royal Statistical Society, Series B, vol. 58, no. 1, pp. 267-288, 1996.\n\nLearning the parts of objects by non-negative matrix factorization. D Lee, H Seung, Nature. 401D. Lee and H. Seung, \"Learning the parts of objects by non-negative matrix factorization,\" Nature, vol. 401, pp. 788-791, Oct. 1999.\n\nNonnegative matrix factorization via rank-one downdate. M Biggs, A Ghodsi, S Vavasis, Proceedings of the 25 th International Conference on Machine Learning (ICML). the 25 th International Conference on Machine Learning (ICML)M. Biggs, A. Ghodsi, and S. Vavasis, \"Nonnegative matrix factoriza- tion via rank-one downdate,\" in Proceedings of the 25 th International Conference on Machine Learning (ICML), 2008, pp. 64-71.\n\nBlind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. C Jutten, J Herault, Signal Processing. 241C. Jutten and J. Herault, \"Blind separation of sources, part I: An adap- tive algorithm based on neuromimetic architecture,\" Signal Processing, vol. 24, no. 1, pp. 1-10, 1991.\n\nCompressed sensing. D Donoho, IEEE Transactions on Information Theory. 524D. Donoho, \"Compressed sensing,\" IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289-1306, Apr. 2006.\n\nAtomic decomposition by basis pursuit. S Chen, D Donoho, M Saunders, SIAM Journal on Scientific Computing. 201S. Chen, D. Donoho, and M. Saunders, \"Atomic decomposition by basis pursuit,\" SIAM Journal on Scientific Computing, vol. 20, no. 1, pp. 33- 61, 1998.\n\nOrthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition. Y Pati, R Rezaiifar, P Krishnaprasad, 27 th Asilomar Conference on Signals, Systems and Computers. Y. Pati, R. Rezaiifar, and P. Krishnaprasad, \"Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition,\" in 27 th Asilomar Conference on Signals, Systems and Computers, 1993, pp. 40-44.\n\nMatching pursuits with time-frequency dictionaries. S Mallat, Z Zhang, IEEE Transactions on Signal Processing. 4112S. Mallat and Z. Zhang, \"Matching pursuits with time-frequency dictio- naries,\" IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397-3415, Dec. 1993.\n\nSparse representation for computer vision and pattern recognition. J Wright, Y Ma, J Mairal, G Sapiro, T Huang, S Yan, Proceedings of the IEEE. 986J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. Huang, and S. Yan, \"Sparse representation for computer vision and pattern recognition,\" Proceedings of the IEEE, vol. 98, no. 6, pp. 1031-1044, June 2010.\n\nFrom sparse solutions of systems of equations to sparse modeling of signals and images. A M Bruckstein, D L Donoho, M Elad, SIAM Review. 511A. M. Bruckstein, D. L. Donoho, and M. Elad, \"From sparse solutions of systems of equations to sparse modeling of signals and images,\" SIAM Review, vol. 51, no. 1, pp. 34-81, 2009.\n\nSparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. M Elad, SpringerNew YorkM. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. New York: Springer, 2010.\n\nSupervised dictionary learning. J Mairal, F Bach, J Ponce, G Sapiro, A Zisserman, Advances in Neural Information Processing Systems (NIPS). J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, \"Supervised dictionary learning,\" in Advances in Neural Information Processing Systems (NIPS), 2008, pp. 1033-1040.\n\nDictionary learning in texture classification. M J Gangeh, A Ghodsi, M S Kamel, Proceedings of the 8 th international conference on Image analysis and recognition -Volume Part I. the 8 th international conference on Image analysis and recognition -Volume Part IBerlin, HeidelbergSpringer-VerlagM. J. Gangeh, A. Ghodsi, and M. S. Kamel, \"Dictionary learning in texture classification,\" in Proceedings of the 8 th international conference on Image analysis and recognition -Volume Part I. Berlin, Heidelberg: Springer-Verlag, 2011, pp. 335-343.\n\nA statistical approach to material classification using image patch exemplars. M Varma, A Zisserman, IEEE Trans. Pattern Analysis and Machine Intelligence. 3111M. Varma and A. Zisserman, \"A statistical approach to material clas- sification using image patch exemplars,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 11, pp. 2032-2047, Nov. 2009.\n\nRobust 3D face recognition using learned visual codebook. C Zhong, Z Sun, T Tan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). C. Zhong, Z. Sun, and T. Tan, \"Robust 3D face recognition using learned visual codebook,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007, pp. 1-6.\n\nRobust face recognition via sparse representation. J Wright, A Yang, A Ganesh, S Sastry, Y Ma, IEEE Transactions on Pattern Analysis and Machine Intelligence. 312J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma, \"Robust face recognition via sparse representation,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 210-227, Feb. 2009.\n\nFisher discrimination dictionary learning for sparse representation. M Yang, L Zhang, X Feng, D Zhang, 13 th IEEE International Conference on Computer Vision (ICCV). M. Yang, L. Zhang, X. Feng, and D. Zhang, \"Fisher discrimination dictionary learning for sparse representation,\" in 13 th IEEE International Conference on Computer Vision (ICCV), 2011, pp. 543-550.\n\nImage denoising via sparse and redundant representations over learned dictionaries. M Elad, M Aharon, IEEE Transactions on Image Processing. 1512M. Elad and M. Aharon, \"Image denoising via sparse and redundant representations over learned dictionaries,\" IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736-3745, Dec. 2006.\n\nSparse representation for color image restoration. J Mairal, M Elad, G Sapiro, IEEE Transactions on Image Processing. 171J. Mairal, M. Elad, and G. Sapiro, \"Sparse representation for color image restoration,\" IEEE Transactions on Image Processing, vol. 17, no. 1, pp. 53-69, Jan. 2008.\n\nMeasuring statistical dependence with hilbert-schmidt norms. A Gretton, O Bousquet, A Smola, B Sch\u00f6lkopf, Proceedings of the 16 th international conference on Algorithmic Learning Theory. the 16 th international conference on Algorithmic Learning TheoryALTA. Gretton, O. Bousquet, A. Smola, and B. Sch\u00f6lkopf, \"Measuring statistical dependence with hilbert-schmidt norms,\" in Proceedings of the 16 th international conference on Algorithmic Learning Theory (ALT), 2005, pp. 63-77.\n\nKernel methods for measuring independence. A Gretton, R Herbrich, A Smola, O Bousquet, B Sch\u00f6lkopf, Journal of Machine Learning Research. 6A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch\u00f6lkopf, \"Kernel methods for measuring independence,\" Journal of Machine Learning Research, vol. 6, pp. 2075-2129, Dec. 2005.\n\nClustering by compression. R Cilibrasi, P Vit\u00e1nyi, IEEE Trans. Information Theory. 514R. Cilibrasi and P. Vit\u00e1nyi, \"Clustering by compression,\" IEEE Trans. Information Theory, vol. 51, no. 4, pp. 1523-1545, 2005.\n\nSupervised texture classification using a novel compression-based similarity measure. M J Gangeh, A Ghodsi, M S Kamel, Proceedings of the International Conference on Computer Vision and Graphics (ICCVG). the International Conference on Computer Vision and Graphics (ICCVG)M. J. Gangeh, A. Ghodsi, and M. S. Kamel, \"Supervised texture classification using a novel compression-based similarity measure,\" in Proceedings of the International Conference on Computer Vision and Graphics (ICCVG), 2012.\n\nSparse representation for signal classification. K Huang, S Aviyente, Advances in Neural Information Processing Systems (NIPS). K. Huang and S. Aviyente, \"Sparse representation for signal classifica- tion,\" in Advances in Neural Information Processing Systems (NIPS), 2007, pp. 609-616.\n\nK-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. M Aharon, M Elad, A Bruckstein, IEEE Transactions on Signal Processing. 5411M. Aharon, M. Elad, and A. Bruckstein, \"K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation,\" IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311-4322, Nov. 2006.\n\nOnline learning for matrix factorization and sparse coding. J Mairal, F Bach, J Ponce, G Sapiro, Journal Machine Learning Research. 11J. Mairal, F. Bach, J. Ponce, and G. Sapiro, \"Online learning for matrix factorization and sparse coding,\" Journal Machine Learning Research, vol. 11, pp. 19-60, Mar. 2010.\n\nRegularization paths for generalized linear models via coordinate descent. J Friedman, T Hastie, R Tibshirani, Journal of Statistical Software. 331J. Friedman, T. Hastie, and R. Tibshirani, \"Regularization paths for generalized linear models via coordinate descent,\" Journal of Statistical Software, vol. 33, no. 1, pp. 1-22, Feb. 2010.\n\nRepresenting and recognizing the visual appearance of materials using three-dimensional textons. T Leung, J Malik, International Journal of Computer Vision. 43T. Leung and J. Malik, \"Representing and recognizing the visual appearance of materials using three-dimensional textons,\" International Journal of Computer Vision, vol. 43, pp. 29-44, June 2001.\n\nA statistical approach to texture classification from single images. M Varma, A Zisserman, International Journal of Computer Vision: Special Issue on Texture Analysis and Synthesis. 621-2M. Varma and A. Zisserman, \"A statistical approach to texture classifi- cation from single images,\" International Journal of Computer Vision: Special Issue on Texture Analysis and Synthesis, vol. 62, no. 1-2, pp. 61-81, 2005.\n\nMetaface learning for sparse representation based face recognition. M Yang, L Zhang, J Yang, D Zhang, 17 th IEEE International Conference on Image Processing (ICIP). M. Yang, L. Zhang, J. Yang, and D. Zhang, \"Metaface learning for sparse representation based face recognition,\" in 17 th IEEE International Conference on Image Processing (ICIP), 2010, pp. 1601-1604.\n\nClassification and clustering via dictionary learning with structured incoherence and shared features. I Ramirez, P Sprechmann, G Sapiro, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). I. Ramirez, P. Sprechmann, and G. Sapiro, \"Classification and clustering via dictionary learning with structured incoherence and shared features,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 3501-3508.\n\nLocalizing objects with smart dictionaries. B Fulkerson, A Vedaldi, S Soatto, Proceedings of the 10 th European Conference on Computer Vision (ECCV): Part I. the 10 th European Conference on Computer Vision (ECCV): Part IB. Fulkerson, A. Vedaldi, and S. Soatto, \"Localizing objects with smart dictionaries,\" in Proceedings of the 10 th European Conference on Computer Vision (ECCV): Part I, 2008, pp. 179-192.\n\nObject categorization by learned universal visual dictionary. J Winn, A Criminisi, T Minka, 10 th IEEE International Conference on Computer Vision (ICCV). J. Winn, A. Criminisi, and T. Minka, \"Object categorization by learned universal visual dictionary,\" in 10 th IEEE International Conference on Computer Vision (ICCV), 2005, pp. 1800-1807.\n\nDiscriminative learned dictionaries for local image analysis. J Mairal, F Bach, J Ponce, G Sapiro, A Zisserman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, \"Discrimina- tive learned dictionaries for local image analysis,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, pp. 1-8.\n\nDiscriminative K-SVD for dictionary learning in face recognition. Q Zhang, B Li, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Q. Zhang and B. Li, \"Discriminative K-SVD for dictionary learning in face recognition,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 2691-2698.\n\nJoint learning and dictionary construction for pattern recognition. D.-S Pham, S Venkatesh, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). D.-S. Pham and S. Venkatesh, \"Joint learning and dictionary construction for pattern recognition,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, pp. 1-8.\n\nUnifying discriminative visual codebook generation with classifier training for object category recognition. L Yang, R Jin, R Sukthankar, F Jurie, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). L. Yang, R. Jin, R. Sukthankar, and F. Jurie, \"Unifying discriminative visual codebook generation with classifier training for object category recognition,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, pp. 1-8.\n\nTask-driven dictionary learning. J Mairal, F Bach, J Ponce, IEEE Transactions on Pattern Analysis and Machine Intelligence. 344J. Mairal, F. Bach, and J. Ponce, \"Task-driven dictionary learning,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 791-804, Apr. 2012.\n\nSupervised learning of quantizer codebooks by information loss minimization. S Lazebnik, M Raginsky, IEEE Transactions on Pattern Analysis and Machine Intelligence. 317S. Lazebnik and M. Raginsky, \"Supervised learning of quantizer code- books by information loss minimization,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 7, pp. 1294-1309, July 2009.\n\nFast discriminative visual codebooks using randomized clustering forests. F Moosmann, B Triggs, F Jurie, Advances in Neural Information Processing Systems (NIPS). F. Moosmann, B. Triggs, and F. Jurie, \"Fast discriminative visual codebooks using randomized clustering forests,\" in Advances in Neural Information Processing Systems (NIPS), 2006, pp. 985-992.\n\nSparse representations for image classification: learning discriminative and reconstructive non-parametric dictionaries. F Rodriguez, G Sapiro, IMA Preprint Series 2213. F. Rodriguez and G. Sapiro, \"Sparse representations for image clas- sification: learning discriminative and reconstructive non-parametric dictionaries,\" in IMA Preprint Series 2213, 2007.\n\nThe use of multiple measurements in taxonomic problems. R A Fisher, Annals of Eugenics. 72R. A. Fisher, \"The use of multiple measurements in taxonomic prob- lems,\" Annals of Eugenics, vol. 7, no. 2, pp. 179-188, 1936.\n\nT M Cover, J A Thomas, Elements of Information Theory. New YorkJohn Wiley & Sons2nd edT. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed. New York: John Wiley & Sons, 2006.\n\nOn information and sufficiency. S Kullback, R A Leibler, The Annals of Mathematical Statistics. 221S. Kullback and R. A. Leibler, \"On information and sufficiency,\" The Annals of Mathematical Statistics, vol. 22, no. 1, pp. 79-86, 1951.\n\nKernel independent component analysis. F R Bach, M I Jordan, Journal of Machine Learning Research. 3F. R. Bach and M. I. Jordan, \"Kernel independent component analysis,\" Journal of Machine Learning Research, vol. 3, pp. 1-48, 2002.\n\nGene selection via the bahsic family of algorithms. L Song, J Bedo, K Borgwardt, A Gretton, A Smola, Bioinformatics. 23L. Song, J. Bedo, K. Borgwardt, A. Gretton, and A. Smola, \"Gene selection via the bahsic family of algorithms,\" Bioinformatics, vol. 23, pp. i490-i498, July 2007.\n\nFast kernel-based independent component analysis. H Shen, S Jegelka, A Gretton, IEEE Transactions on Signal Processing. 579H. Shen, S. Jegelka, and A. Gretton, \"Fast kernel-based independent component analysis,\" IEEE Transactions on Signal Processing, vol. 57, no. 9, pp. 3498-3511, Sept. 2009.\n\nKernelized sorting. N Quadrianto, A Smola, L Song, T Tuytelaars, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3210N. Quadrianto, A. Smola, L. Song, and T. Tuytelaars, \"Kernelized sort- ing,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 10, pp. 1809-1821, Oct. 2010.\n\nA kernel method for the two-sample problem. A Gretton, K M Borgwardt, M Rasch, B Sch\u00f6lkopf, A J Smola, 157Max Planck Institute for Biological CyberneticsTechnical ReportA. Gretton, K. M. Borgwardt, M. Rasch, B. Sch\u00f6lkopf, and A. J. Smola, \"A kernel method for the two-sample problem,\" Max Planck Institute for Biological Cybernetics, Technical Report 157, Apr. 2008.\n\nGeneralized clustering via kernel embeddings. S Jegelka, A Gretton, B Sch\u00f6lkopf, B K Sriperumbudur, U Von Luxburg, Proceedings of the 32 nd Annual German Conference on Advances in Artificial Intelligence. the 32 nd Annual German Conference on Advances in Artificial IntelligenceS. Jegelka, A. Gretton, B. Sch\u00f6lkopf, B. K. Sriperumbudur, and U. Von Luxburg, \"Generalized clustering via kernel embeddings,\" in Proceedings of the 32 nd Annual German Conference on Advances in Artificial Intelligence, 2009, pp. 144-152.\n\nH , Handbook of Matrices. John Wiley & SonsH. L\u00fctkepohl, Handbook of Matrices. John Wiley & Sons, 1996.\n\nAdapting to unknown smoothness via wavelet shrinkage. D L Donoho, I M Johnstone, Journal of the American Statistical Association. 90432D. L. Donoho and I. M. Johnstone, \"Adapting to unknown smoothness via wavelet shrinkage,\" Journal of the American Statistical Association, vol. 90, no. 432, pp. 1200-1224, 1995.\n\nPathwise coordinate optimization. J Friedman, T Hastie, H Hofling, R Tibshirani, The Annals of Applied Statistics. 12J. Friedman, T. Hastie, H. Hofling, and R. Tibshirani, \"Pathwise coor- dinate optimization,\" The Annals of Applied Statistics, vol. 1, no. 2, pp. 302-332, 2007.\n\nLocal Representation Theory: Modular Representations as an Introduction to the Local Representation Theory of Finite Groups. J L Alperin, Cambridge University PressNew YorkJ. L. Alperin, Local Representation Theory: Modular Representations as an Introduction to the Local Representation Theory of Finite Groups. New York: Cambridge University Press, 1986.\n\nSelf-taught learning: transfer learning from unlabeled data. R Raina, A Battle, H Lee, B Packer, A Y Ng, Proceedings of the 24 th international conference on Machine learning (ICML). the 24 th international conference on Machine learning (ICML)R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng, \"Self-taught learning: transfer learning from unlabeled data,\" in Proceedings of the 24 th international conference on Machine learning (ICML), 2007, pp. 759-766.\n\nEfficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit. R Rubinstein, M Zibulevsky, M Elad, Dept. of Computer Science. Technical ReportR. Rubinstein, M. Zibulevsky, and M. Elad, \"Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit,\" Dept. of Computer Science, Technion, Technical Report, 2008.\n\nHandwritten digit recognition with a backpropagation network. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D , Advances in Neural Information Processing Systems. Morgan KaufmannY. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hub- bard, and L. D. Jackel, \"Handwritten digit recognition with a back- propagation network,\" in Advances in Neural Information Processing Systems. Morgan Kaufmann, 1990, pp. 396-404.\n\nUnsupervised learning of invariant feature hierarchies with applications to object recognition. M Ranzato, F J Huang, Y.-L Boureau, Y Lecun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). M. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun, \"Unsupervised learning of invariant feature hierarchies with applications to object recognition,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007, pp. 1-8.\n\nSelf-tuning spectral clustering. L Zelnik-Manor, P Perona, Advances in Neural Information Processing Systems (NIPS). L. Zelnik-Manor and P. Perona, \"Self-tuning spectral clustering,\" in Advances in Neural Information Processing Systems (NIPS), 2004, pp. 1601-1608.\n\nA compression-based distance measure for texture. B Campana, E Keogh, Statistical Analysis and Data Mining. 36B. Campana and E. Keogh, \"A compression-based distance measure for texture,\" Statistical Analysis and Data Mining, vol. 3, no. 6, pp. 381- 398, 2010.\n\nTaxonomy inference using kernel dependence measures. M Blaschko, A Gretton, 181Technical ReportMax Planck Institute for Biological CyberneticsM. Blaschko and A. Gretton, \"Taxonomy inference using kernel de- pendence measures,\" Max Planck Institute for Biological Cybernetics, Technical Report 181, Nov. 2008.\n\nA dependence maximization view of clustering. L Song, A Smola, A Gretton, K Borgwardt, Proceedings of the 24 th international conference on Machine learning (ICML). the 24 th international conference on Machine learning (ICML)L. Song, A. Smola, A. Gretton, and K. Borgwardt, \"A dependence max- imization view of clustering,\" in Proceedings of the 24 th international conference on Machine learning (ICML), 2007, pp. 815-822.\n\nIn-sample and outof-sample model selection and error estimation for support vector machines. D Anguita, A Ghio, L Oneto, S Ridella, IEEE Transactions on Neural Networks and Learning Systems. 239D. Anguita, A. Ghio, L. Oneto, and S. Ridella, \"In-sample and out- of-sample model selection and error estimation for support vector ma- chines,\" IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 9, pp. 1390 -1406, Sept. 2012.\n", "annotations": {"author": "[{\"end\":102,\"start\":85},{\"end\":126,\"start\":103},{\"end\":155,\"start\":127}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":95},{\"end\":125,\"start\":119},{\"end\":154,\"start\":149}]", "author_first_name": "[{\"end\":92,\"start\":85},{\"end\":94,\"start\":93},{\"end\":118,\"start\":115},{\"end\":146,\"start\":139},{\"end\":148,\"start\":147}]", "author_affiliation": null, "title": "[{\"end\":82,\"start\":1},{\"end\":237,\"start\":156}]", "venue": null, "abstract": "[{\"end\":1199,\"start\":377}]", "bib_ref": "[{\"end\":1271,\"start\":1265},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1775,\"start\":1772},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2089,\"start\":2086},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2277,\"start\":2274},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2684,\"start\":2681},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2923,\"start\":2920},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2993,\"start\":2990},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2998,\"start\":2995},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3125,\"start\":3122},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3149,\"start\":3146},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3173,\"start\":3170},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3217,\"start\":3213},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3223,\"start\":3219},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3350,\"start\":3346},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3356,\"start\":3352},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3840,\"start\":3836},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3930,\"start\":3927},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4183,\"start\":4179},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4189,\"start\":4185},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4212,\"start\":4208},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4218,\"start\":4214},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4240,\"start\":4236},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4246,\"start\":4242},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4626,\"start\":4623},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4635,\"start\":4632},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4649,\"start\":4646},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4685,\"start\":4682},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4774,\"start\":4770},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5586,\"start\":5582},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5592,\"start\":5588},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7098,\"start\":7094},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7104,\"start\":7100},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8307,\"start\":8303},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8316,\"start\":8312},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9393,\"start\":9389},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9626,\"start\":9622},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9632,\"start\":9628},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9638,\"start\":9634},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9828,\"start\":9825},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9854,\"start\":9851},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10319,\"start\":10315},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11315,\"start\":11311},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11337,\"start\":11333},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11373,\"start\":11369},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11850,\"start\":11846},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11856,\"start\":11852},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11862,\"start\":11858},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12102,\"start\":12098},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12783,\"start\":12779},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13342,\"start\":13338},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14003,\"start\":13999},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14221,\"start\":14217},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14808,\"start\":14804},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14814,\"start\":14810},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14820,\"start\":14816},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15363,\"start\":15359},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15543,\"start\":15539},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15552,\"start\":15548},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16127,\"start\":16123},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16133,\"start\":16129},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16473,\"start\":16469},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16533,\"start\":16529},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17115,\"start\":17111},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17168,\"start\":17164},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17174,\"start\":17170},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17232,\"start\":17228},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17238,\"start\":17234},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17244,\"start\":17240},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17335,\"start\":17331},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17498,\"start\":17494},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17526,\"start\":17522},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17918,\"start\":17914},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18344,\"start\":18340},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19748,\"start\":19744},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":19790,\"start\":19786},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20130,\"start\":20126},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20316,\"start\":20312},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20419,\"start\":20415},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20456,\"start\":20452},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20483,\"start\":20479},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20709,\"start\":20705},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20715,\"start\":20711},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20857,\"start\":20853},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20863,\"start\":20859},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22618,\"start\":22614},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22880,\"start\":22876},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23437,\"start\":23434},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24026,\"start\":24023},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24681,\"start\":24677},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25949,\"start\":25945},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26549,\"start\":26545},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":26835,\"start\":26831},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28185,\"start\":28181},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28931,\"start\":28927},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29489,\"start\":29485},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":29773,\"start\":29769},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":29779,\"start\":29775},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30700,\"start\":30699},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31651,\"start\":31647},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31990,\"start\":31986},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32163,\"start\":32162},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32632,\"start\":32631},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":33271,\"start\":33267},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33531,\"start\":33527},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35397,\"start\":35393},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35752,\"start\":35751},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":35779,\"start\":35775},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36096,\"start\":36095},{\"end\":37710,\"start\":37706},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":38911,\"start\":38907},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39259,\"start\":39255},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39265,\"start\":39261},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39335,\"start\":39331},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":40575,\"start\":40571},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":40875,\"start\":40871},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40936,\"start\":40932},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40994,\"start\":40990},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41567,\"start\":41563},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41753,\"start\":41749},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42072,\"start\":42068},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":42765,\"start\":42761},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":42947,\"start\":42943},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":43199,\"start\":43195},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":43374,\"start\":43370},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":44029,\"start\":44025},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":46388,\"start\":46384},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":46896,\"start\":46892},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":48625,\"start\":48621},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":48843,\"start\":48839},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49050,\"start\":49046},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49165,\"start\":49161},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":49499,\"start\":49495},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":49515,\"start\":49511},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":49634,\"start\":49630},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":49694,\"start\":49690},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":49745,\"start\":49741},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":49893,\"start\":49889},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":50209,\"start\":50205},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":50450,\"start\":50446},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":50774,\"start\":50770},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":53052,\"start\":53048},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":56045,\"start\":56041},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":58493,\"start\":58491},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":61662,\"start\":61658},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":61668,\"start\":61664},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":62730,\"start\":62726},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":63006,\"start\":63002},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":66564,\"start\":66560},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":70840,\"start\":70839},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":71073,\"start\":71069}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":63326,\"start\":63008},{\"attributes\":{\"id\":\"fig_1\"},\"end\":63430,\"start\":63327},{\"attributes\":{\"id\":\"fig_2\"},\"end\":63490,\"start\":63431},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":64761,\"start\":63491},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":66367,\"start\":64762},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66805,\"start\":66368},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":68386,\"start\":66806},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":69790,\"start\":68387},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":70397,\"start\":69791}]", "paragraph": "[{\"end\":1776,\"start\":1218},{\"end\":3420,\"start\":1778},{\"end\":4252,\"start\":3422},{\"end\":5189,\"start\":4254},{\"end\":5818,\"start\":5191},{\"end\":5869,\"start\":5820},{\"end\":7773,\"start\":5871},{\"end\":8003,\"start\":7809},{\"end\":8484,\"start\":8056},{\"end\":8948,\"start\":8522},{\"end\":9875,\"start\":8997},{\"end\":10830,\"start\":9938},{\"end\":11508,\"start\":10882},{\"end\":13541,\"start\":11510},{\"end\":14721,\"start\":13543},{\"end\":16135,\"start\":14723},{\"end\":17030,\"start\":16137},{\"end\":18422,\"start\":17032},{\"end\":18651,\"start\":18424},{\"end\":19573,\"start\":18668},{\"end\":20484,\"start\":19619},{\"end\":20961,\"start\":20486},{\"end\":21223,\"start\":21072},{\"end\":21273,\"start\":21225},{\"end\":21696,\"start\":21357},{\"end\":22068,\"start\":21732},{\"end\":22322,\"start\":22221},{\"end\":22881,\"start\":22503},{\"end\":22948,\"start\":22917},{\"end\":23308,\"start\":23012},{\"end\":23883,\"start\":23355},{\"end\":24056,\"start\":23919},{\"end\":24417,\"start\":24077},{\"end\":24682,\"start\":24456},{\"end\":24876,\"start\":24803},{\"end\":24967,\"start\":24890},{\"end\":25358,\"start\":25111},{\"end\":25658,\"start\":25478},{\"end\":26578,\"start\":25660},{\"end\":26753,\"start\":26669},{\"end\":27012,\"start\":26792},{\"end\":27766,\"start\":27014},{\"end\":29064,\"start\":27805},{\"end\":29335,\"start\":29113},{\"end\":29492,\"start\":29379},{\"end\":29829,\"start\":29548},{\"end\":29893,\"start\":29858},{\"end\":30054,\"start\":29969},{\"end\":30452,\"start\":30056},{\"end\":30872,\"start\":30499},{\"end\":31144,\"start\":30874},{\"end\":31652,\"start\":31193},{\"end\":31793,\"start\":31654},{\"end\":32360,\"start\":31818},{\"end\":33440,\"start\":32418},{\"end\":33818,\"start\":33468},{\"end\":34037,\"start\":33867},{\"end\":34125,\"start\":34066},{\"end\":34988,\"start\":34145},{\"end\":35754,\"start\":35018},{\"end\":36210,\"start\":35756},{\"end\":36384,\"start\":36212},{\"end\":37559,\"start\":36433},{\"end\":38101,\"start\":37576},{\"end\":39470,\"start\":38103},{\"end\":40443,\"start\":39472},{\"end\":40754,\"start\":40468},{\"end\":44282,\"start\":40756},{\"end\":44990,\"start\":44311},{\"end\":45638,\"start\":44992},{\"end\":46389,\"start\":45640},{\"end\":47450,\"start\":46391},{\"end\":48462,\"start\":47452},{\"end\":49401,\"start\":48506},{\"end\":51802,\"start\":49403},{\"end\":52143,\"start\":51875},{\"end\":52469,\"start\":52145},{\"end\":53053,\"start\":52471},{\"end\":53328,\"start\":53055},{\"end\":54304,\"start\":53330},{\"end\":55772,\"start\":54306},{\"end\":56244,\"start\":55774},{\"end\":57285,\"start\":56246},{\"end\":58160,\"start\":57287},{\"end\":58494,\"start\":58177},{\"end\":58980,\"start\":58496},{\"end\":60690,\"start\":58982},{\"end\":61293,\"start\":60692},{\"end\":61563,\"start\":61295},{\"end\":62119,\"start\":61565},{\"end\":62541,\"start\":62121},{\"end\":63007,\"start\":62543}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8521,\"start\":8485},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8996,\"start\":8949},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9937,\"start\":9876},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21071,\"start\":20962},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21356,\"start\":21274},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21731,\"start\":21697},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22220,\"start\":22069},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22502,\"start\":22323},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22916,\"start\":22882},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23011,\"start\":22949},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23918,\"start\":23884},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24076,\"start\":24057},{\"attributes\":{\"id\":\"formula_12\"},\"end\":24455,\"start\":24418},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24802,\"start\":24683},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24889,\"start\":24877},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25110,\"start\":24968},{\"attributes\":{\"id\":\"formula_16\"},\"end\":25477,\"start\":25359},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26668,\"start\":26579},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26791,\"start\":26754},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27804,\"start\":27767},{\"attributes\":{\"id\":\"formula_20\"},\"end\":29112,\"start\":29065},{\"attributes\":{\"id\":\"formula_21\"},\"end\":29378,\"start\":29336},{\"attributes\":{\"id\":\"formula_22\"},\"end\":29547,\"start\":29493},{\"attributes\":{\"id\":\"formula_23\"},\"end\":29857,\"start\":29830},{\"attributes\":{\"id\":\"formula_24\"},\"end\":29968,\"start\":29894},{\"attributes\":{\"id\":\"formula_25\"},\"end\":31817,\"start\":31794},{\"attributes\":{\"id\":\"formula_27\"},\"end\":33467,\"start\":33441},{\"attributes\":{\"id\":\"formula_28\"},\"end\":33866,\"start\":33819},{\"attributes\":{\"id\":\"formula_29\"},\"end\":34065,\"start\":34038},{\"attributes\":{\"id\":\"formula_30\"},\"end\":36401,\"start\":36385},{\"attributes\":{\"id\":\"formula_31\"},\"end\":36432,\"start\":36401}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34777,\"start\":34770},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38637,\"start\":38629},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38847,\"start\":38839},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":39498,\"start\":39490},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":41413,\"start\":41404},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":43679,\"start\":43670},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44210,\"start\":44203},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":44749,\"start\":44742},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":45888,\"start\":45872},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":46157,\"start\":46150},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":49985,\"start\":49977},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":51295,\"start\":51287},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":51801,\"start\":51793},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":54515,\"start\":54507},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":60508,\"start\":60501}]", "section_header": "[{\"end\":1216,\"start\":1201},{\"end\":7807,\"start\":7776},{\"end\":8054,\"start\":8006},{\"end\":10880,\"start\":10833},{\"end\":18666,\"start\":18654},{\"end\":19617,\"start\":19576},{\"end\":23353,\"start\":23311},{\"end\":30497,\"start\":30455},{\"end\":31191,\"start\":31147},{\"end\":32416,\"start\":32363},{\"end\":34143,\"start\":34128},{\"end\":35016,\"start\":34991},{\"end\":37574,\"start\":37562},{\"end\":40466,\"start\":40446},{\"end\":44309,\"start\":44285},{\"end\":48504,\"start\":48465},{\"end\":51873,\"start\":51805},{\"end\":58175,\"start\":58163},{\"end\":63336,\"start\":63328},{\"end\":63440,\"start\":63432},{\"end\":63501,\"start\":63492},{\"end\":64773,\"start\":64763},{\"end\":66380,\"start\":66369},{\"end\":66817,\"start\":66807},{\"end\":68397,\"start\":68388},{\"end\":69802,\"start\":69792}]", "table": "[{\"end\":64761,\"start\":63535},{\"end\":66367,\"start\":65020},{\"end\":66805,\"start\":66646},{\"end\":68386,\"start\":66992},{\"end\":69790,\"start\":68399},{\"end\":70397,\"start\":70049}]", "figure_caption": "[{\"end\":63326,\"start\":63010},{\"end\":63430,\"start\":63338},{\"end\":63490,\"start\":63442},{\"end\":63535,\"start\":63503},{\"end\":65020,\"start\":64776},{\"end\":66646,\"start\":66384},{\"end\":66992,\"start\":66820},{\"end\":70049,\"start\":69805}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38373,\"start\":38367},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":49142,\"start\":49136},{\"end\":54337,\"start\":54331},{\"end\":58235,\"start\":58229}]", "bib_author_first_name": "[{\"end\":72435,\"start\":72434},{\"end\":72658,\"start\":72657},{\"end\":72671,\"start\":72670},{\"end\":72907,\"start\":72906},{\"end\":72920,\"start\":72919},{\"end\":72932,\"start\":72931},{\"end\":73123,\"start\":73122},{\"end\":73415,\"start\":73414},{\"end\":73422,\"start\":73421},{\"end\":73632,\"start\":73631},{\"end\":73641,\"start\":73640},{\"end\":73651,\"start\":73650},{\"end\":74092,\"start\":74091},{\"end\":74102,\"start\":74101},{\"end\":74332,\"start\":74331},{\"end\":74542,\"start\":74541},{\"end\":74550,\"start\":74549},{\"end\":74560,\"start\":74559},{\"end\":74870,\"start\":74869},{\"end\":74878,\"start\":74877},{\"end\":74891,\"start\":74890},{\"end\":75256,\"start\":75255},{\"end\":75266,\"start\":75265},{\"end\":75550,\"start\":75549},{\"end\":75560,\"start\":75559},{\"end\":75566,\"start\":75565},{\"end\":75576,\"start\":75575},{\"end\":75586,\"start\":75585},{\"end\":75595,\"start\":75594},{\"end\":75917,\"start\":75916},{\"end\":75919,\"start\":75918},{\"end\":75933,\"start\":75932},{\"end\":75935,\"start\":75934},{\"end\":75945,\"start\":75944},{\"end\":76249,\"start\":76248},{\"end\":76439,\"start\":76438},{\"end\":76449,\"start\":76448},{\"end\":76457,\"start\":76456},{\"end\":76466,\"start\":76465},{\"end\":76476,\"start\":76475},{\"end\":76770,\"start\":76769},{\"end\":76772,\"start\":76771},{\"end\":76782,\"start\":76781},{\"end\":76792,\"start\":76791},{\"end\":76794,\"start\":76793},{\"end\":77346,\"start\":77345},{\"end\":77355,\"start\":77354},{\"end\":77694,\"start\":77693},{\"end\":77703,\"start\":77702},{\"end\":77710,\"start\":77709},{\"end\":78011,\"start\":78010},{\"end\":78021,\"start\":78020},{\"end\":78029,\"start\":78028},{\"end\":78039,\"start\":78038},{\"end\":78049,\"start\":78048},{\"end\":78402,\"start\":78401},{\"end\":78410,\"start\":78409},{\"end\":78419,\"start\":78418},{\"end\":78427,\"start\":78426},{\"end\":78782,\"start\":78781},{\"end\":78790,\"start\":78789},{\"end\":79086,\"start\":79085},{\"end\":79096,\"start\":79095},{\"end\":79104,\"start\":79103},{\"end\":79383,\"start\":79382},{\"end\":79394,\"start\":79393},{\"end\":79406,\"start\":79405},{\"end\":79415,\"start\":79414},{\"end\":79846,\"start\":79845},{\"end\":79857,\"start\":79856},{\"end\":79869,\"start\":79868},{\"end\":79878,\"start\":79877},{\"end\":79890,\"start\":79889},{\"end\":80153,\"start\":80152},{\"end\":80166,\"start\":80165},{\"end\":80426,\"start\":80425},{\"end\":80428,\"start\":80427},{\"end\":80438,\"start\":80437},{\"end\":80448,\"start\":80447},{\"end\":80450,\"start\":80449},{\"end\":80886,\"start\":80885},{\"end\":80895,\"start\":80894},{\"end\":81212,\"start\":81211},{\"end\":81222,\"start\":81221},{\"end\":81230,\"start\":81229},{\"end\":81560,\"start\":81559},{\"end\":81570,\"start\":81569},{\"end\":81578,\"start\":81577},{\"end\":81587,\"start\":81586},{\"end\":81883,\"start\":81882},{\"end\":81895,\"start\":81894},{\"end\":81905,\"start\":81904},{\"end\":82243,\"start\":82242},{\"end\":82252,\"start\":82251},{\"end\":82570,\"start\":82569},{\"end\":82579,\"start\":82578},{\"end\":82983,\"start\":82982},{\"end\":82991,\"start\":82990},{\"end\":83000,\"start\":82999},{\"end\":83008,\"start\":83007},{\"end\":83385,\"start\":83384},{\"end\":83396,\"start\":83395},{\"end\":83410,\"start\":83409},{\"end\":83770,\"start\":83769},{\"end\":83783,\"start\":83782},{\"end\":83794,\"start\":83793},{\"end\":84199,\"start\":84198},{\"end\":84207,\"start\":84206},{\"end\":84220,\"start\":84219},{\"end\":84543,\"start\":84542},{\"end\":84553,\"start\":84552},{\"end\":84561,\"start\":84560},{\"end\":84570,\"start\":84569},{\"end\":84580,\"start\":84579},{\"end\":84937,\"start\":84936},{\"end\":84946,\"start\":84945},{\"end\":85270,\"start\":85266},{\"end\":85278,\"start\":85277},{\"end\":85652,\"start\":85651},{\"end\":85660,\"start\":85659},{\"end\":85667,\"start\":85666},{\"end\":85681,\"start\":85680},{\"end\":86033,\"start\":86032},{\"end\":86043,\"start\":86042},{\"end\":86051,\"start\":86050},{\"end\":86378,\"start\":86377},{\"end\":86390,\"start\":86389},{\"end\":86760,\"start\":86759},{\"end\":86772,\"start\":86771},{\"end\":86782,\"start\":86781},{\"end\":87165,\"start\":87164},{\"end\":87178,\"start\":87177},{\"end\":87459,\"start\":87458},{\"end\":87461,\"start\":87460},{\"end\":87622,\"start\":87621},{\"end\":87624,\"start\":87623},{\"end\":87633,\"start\":87632},{\"end\":87635,\"start\":87634},{\"end\":87846,\"start\":87845},{\"end\":87858,\"start\":87857},{\"end\":87860,\"start\":87859},{\"end\":88090,\"start\":88089},{\"end\":88092,\"start\":88091},{\"end\":88100,\"start\":88099},{\"end\":88102,\"start\":88101},{\"end\":88336,\"start\":88335},{\"end\":88344,\"start\":88343},{\"end\":88352,\"start\":88351},{\"end\":88365,\"start\":88364},{\"end\":88376,\"start\":88375},{\"end\":88617,\"start\":88616},{\"end\":88625,\"start\":88624},{\"end\":88636,\"start\":88635},{\"end\":88883,\"start\":88882},{\"end\":88897,\"start\":88896},{\"end\":88906,\"start\":88905},{\"end\":88914,\"start\":88913},{\"end\":89225,\"start\":89224},{\"end\":89236,\"start\":89235},{\"end\":89238,\"start\":89237},{\"end\":89251,\"start\":89250},{\"end\":89260,\"start\":89259},{\"end\":89273,\"start\":89272},{\"end\":89275,\"start\":89274},{\"end\":89595,\"start\":89594},{\"end\":89606,\"start\":89605},{\"end\":89617,\"start\":89616},{\"end\":89630,\"start\":89629},{\"end\":89632,\"start\":89631},{\"end\":89649,\"start\":89648},{\"end\":90067,\"start\":90066},{\"end\":90226,\"start\":90225},{\"end\":90228,\"start\":90227},{\"end\":90238,\"start\":90237},{\"end\":90240,\"start\":90239},{\"end\":90520,\"start\":90519},{\"end\":90532,\"start\":90531},{\"end\":90542,\"start\":90541},{\"end\":90553,\"start\":90552},{\"end\":90890,\"start\":90889},{\"end\":90892,\"start\":90891},{\"end\":91183,\"start\":91182},{\"end\":91192,\"start\":91191},{\"end\":91202,\"start\":91201},{\"end\":91209,\"start\":91208},{\"end\":91219,\"start\":91218},{\"end\":91221,\"start\":91220},{\"end\":91673,\"start\":91672},{\"end\":91687,\"start\":91686},{\"end\":91701,\"start\":91700},{\"end\":92010,\"start\":92009},{\"end\":92019,\"start\":92018},{\"end\":92028,\"start\":92027},{\"end\":92030,\"start\":92029},{\"end\":92040,\"start\":92039},{\"end\":92053,\"start\":92052},{\"end\":92055,\"start\":92054},{\"end\":92065,\"start\":92064},{\"end\":92076,\"start\":92075},{\"end\":92078,\"start\":92077},{\"end\":92495,\"start\":92494},{\"end\":92506,\"start\":92505},{\"end\":92508,\"start\":92507},{\"end\":92520,\"start\":92516},{\"end\":92531,\"start\":92530},{\"end\":92878,\"start\":92877},{\"end\":92894,\"start\":92893},{\"end\":93161,\"start\":93160},{\"end\":93172,\"start\":93171},{\"end\":93425,\"start\":93424},{\"end\":93437,\"start\":93436},{\"end\":93728,\"start\":93727},{\"end\":93736,\"start\":93735},{\"end\":93745,\"start\":93744},{\"end\":93756,\"start\":93755},{\"end\":94201,\"start\":94200},{\"end\":94212,\"start\":94211},{\"end\":94220,\"start\":94219},{\"end\":94229,\"start\":94228}]", "bib_author_last_name": "[{\"end\":72442,\"start\":72436},{\"end\":72668,\"start\":72659},{\"end\":72677,\"start\":72672},{\"end\":72917,\"start\":72908},{\"end\":72929,\"start\":72921},{\"end\":72936,\"start\":72933},{\"end\":73134,\"start\":73124},{\"end\":73419,\"start\":73416},{\"end\":73428,\"start\":73423},{\"end\":73638,\"start\":73633},{\"end\":73648,\"start\":73642},{\"end\":73659,\"start\":73652},{\"end\":74099,\"start\":74093},{\"end\":74110,\"start\":74103},{\"end\":74339,\"start\":74333},{\"end\":74547,\"start\":74543},{\"end\":74557,\"start\":74551},{\"end\":74569,\"start\":74561},{\"end\":74875,\"start\":74871},{\"end\":74888,\"start\":74879},{\"end\":74905,\"start\":74892},{\"end\":75263,\"start\":75257},{\"end\":75272,\"start\":75267},{\"end\":75557,\"start\":75551},{\"end\":75563,\"start\":75561},{\"end\":75573,\"start\":75567},{\"end\":75583,\"start\":75577},{\"end\":75592,\"start\":75587},{\"end\":75599,\"start\":75596},{\"end\":75930,\"start\":75920},{\"end\":75942,\"start\":75936},{\"end\":75950,\"start\":75946},{\"end\":76254,\"start\":76250},{\"end\":76446,\"start\":76440},{\"end\":76454,\"start\":76450},{\"end\":76463,\"start\":76458},{\"end\":76473,\"start\":76467},{\"end\":76486,\"start\":76477},{\"end\":76779,\"start\":76773},{\"end\":76789,\"start\":76783},{\"end\":76800,\"start\":76795},{\"end\":77352,\"start\":77347},{\"end\":77365,\"start\":77356},{\"end\":77700,\"start\":77695},{\"end\":77707,\"start\":77704},{\"end\":77714,\"start\":77711},{\"end\":78018,\"start\":78012},{\"end\":78026,\"start\":78022},{\"end\":78036,\"start\":78030},{\"end\":78046,\"start\":78040},{\"end\":78052,\"start\":78050},{\"end\":78407,\"start\":78403},{\"end\":78416,\"start\":78411},{\"end\":78424,\"start\":78420},{\"end\":78433,\"start\":78428},{\"end\":78787,\"start\":78783},{\"end\":78797,\"start\":78791},{\"end\":79093,\"start\":79087},{\"end\":79101,\"start\":79097},{\"end\":79111,\"start\":79105},{\"end\":79391,\"start\":79384},{\"end\":79403,\"start\":79395},{\"end\":79412,\"start\":79407},{\"end\":79425,\"start\":79416},{\"end\":79854,\"start\":79847},{\"end\":79866,\"start\":79858},{\"end\":79875,\"start\":79870},{\"end\":79887,\"start\":79879},{\"end\":79900,\"start\":79891},{\"end\":80163,\"start\":80154},{\"end\":80174,\"start\":80167},{\"end\":80435,\"start\":80429},{\"end\":80445,\"start\":80439},{\"end\":80456,\"start\":80451},{\"end\":80892,\"start\":80887},{\"end\":80904,\"start\":80896},{\"end\":81219,\"start\":81213},{\"end\":81227,\"start\":81223},{\"end\":81241,\"start\":81231},{\"end\":81567,\"start\":81561},{\"end\":81575,\"start\":81571},{\"end\":81584,\"start\":81579},{\"end\":81594,\"start\":81588},{\"end\":81892,\"start\":81884},{\"end\":81902,\"start\":81896},{\"end\":81916,\"start\":81906},{\"end\":82249,\"start\":82244},{\"end\":82258,\"start\":82253},{\"end\":82576,\"start\":82571},{\"end\":82589,\"start\":82580},{\"end\":82988,\"start\":82984},{\"end\":82997,\"start\":82992},{\"end\":83005,\"start\":83001},{\"end\":83014,\"start\":83009},{\"end\":83393,\"start\":83386},{\"end\":83407,\"start\":83397},{\"end\":83417,\"start\":83411},{\"end\":83780,\"start\":83771},{\"end\":83791,\"start\":83784},{\"end\":83801,\"start\":83795},{\"end\":84204,\"start\":84200},{\"end\":84217,\"start\":84208},{\"end\":84226,\"start\":84221},{\"end\":84550,\"start\":84544},{\"end\":84558,\"start\":84554},{\"end\":84567,\"start\":84562},{\"end\":84577,\"start\":84571},{\"end\":84590,\"start\":84581},{\"end\":84943,\"start\":84938},{\"end\":84949,\"start\":84947},{\"end\":85275,\"start\":85271},{\"end\":85288,\"start\":85279},{\"end\":85657,\"start\":85653},{\"end\":85664,\"start\":85661},{\"end\":85678,\"start\":85668},{\"end\":85687,\"start\":85682},{\"end\":86040,\"start\":86034},{\"end\":86048,\"start\":86044},{\"end\":86057,\"start\":86052},{\"end\":86387,\"start\":86379},{\"end\":86399,\"start\":86391},{\"end\":86769,\"start\":86761},{\"end\":86779,\"start\":86773},{\"end\":86788,\"start\":86783},{\"end\":87175,\"start\":87166},{\"end\":87185,\"start\":87179},{\"end\":87468,\"start\":87462},{\"end\":87630,\"start\":87625},{\"end\":87642,\"start\":87636},{\"end\":87855,\"start\":87847},{\"end\":87868,\"start\":87861},{\"end\":88097,\"start\":88093},{\"end\":88109,\"start\":88103},{\"end\":88341,\"start\":88337},{\"end\":88349,\"start\":88345},{\"end\":88362,\"start\":88353},{\"end\":88373,\"start\":88366},{\"end\":88382,\"start\":88377},{\"end\":88622,\"start\":88618},{\"end\":88633,\"start\":88626},{\"end\":88644,\"start\":88637},{\"end\":88894,\"start\":88884},{\"end\":88903,\"start\":88898},{\"end\":88911,\"start\":88907},{\"end\":88925,\"start\":88915},{\"end\":89233,\"start\":89226},{\"end\":89248,\"start\":89239},{\"end\":89257,\"start\":89252},{\"end\":89270,\"start\":89261},{\"end\":89281,\"start\":89276},{\"end\":89603,\"start\":89596},{\"end\":89614,\"start\":89607},{\"end\":89627,\"start\":89618},{\"end\":89646,\"start\":89633},{\"end\":89661,\"start\":89650},{\"end\":90235,\"start\":90229},{\"end\":90250,\"start\":90241},{\"end\":90529,\"start\":90521},{\"end\":90539,\"start\":90533},{\"end\":90550,\"start\":90543},{\"end\":90564,\"start\":90554},{\"end\":90900,\"start\":90893},{\"end\":91189,\"start\":91184},{\"end\":91199,\"start\":91193},{\"end\":91206,\"start\":91203},{\"end\":91216,\"start\":91210},{\"end\":91224,\"start\":91222},{\"end\":91684,\"start\":91674},{\"end\":91698,\"start\":91688},{\"end\":91706,\"start\":91702},{\"end\":92016,\"start\":92011},{\"end\":92025,\"start\":92020},{\"end\":92037,\"start\":92031},{\"end\":92050,\"start\":92041},{\"end\":92062,\"start\":92056},{\"end\":92073,\"start\":92066},{\"end\":92503,\"start\":92496},{\"end\":92514,\"start\":92509},{\"end\":92528,\"start\":92521},{\"end\":92537,\"start\":92532},{\"end\":92891,\"start\":92879},{\"end\":92901,\"start\":92895},{\"end\":93169,\"start\":93162},{\"end\":93178,\"start\":93173},{\"end\":93434,\"start\":93426},{\"end\":93445,\"start\":93438},{\"end\":93733,\"start\":93729},{\"end\":93742,\"start\":93737},{\"end\":93753,\"start\":93746},{\"end\":93766,\"start\":93757},{\"end\":94209,\"start\":94202},{\"end\":94217,\"start\":94213},{\"end\":94226,\"start\":94221},{\"end\":94237,\"start\":94230}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":72557,\"start\":72381},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4358477},\"end\":72872,\"start\":72559},{\"attributes\":{\"id\":\"b2\"},\"end\":73070,\"start\":72874},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16162039},\"end\":73344,\"start\":73072},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4428232},\"end\":73573,\"start\":73346},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5823287},\"end\":73994,\"start\":73575},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":33162734},\"end\":74309,\"start\":73996},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13622916},\"end\":74500,\"start\":74311},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2429822},\"end\":74761,\"start\":74502},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16513805},\"end\":75201,\"start\":74763},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14427335},\"end\":75480,\"start\":75203},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207020771},\"end\":75826,\"start\":75482},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11964764},\"end\":76148,\"start\":75828},{\"attributes\":{\"id\":\"b13\"},\"end\":76404,\"start\":76150},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":682971},\"end\":76720,\"start\":76406},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":21396325},\"end\":77264,\"start\":76722},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10658631},\"end\":77633,\"start\":77266},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9650101},\"end\":77957,\"start\":77635},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7677466},\"end\":78330,\"start\":77959},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15617830},\"end\":78695,\"start\":78332},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6888534},\"end\":79032,\"start\":78697},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2627705},\"end\":79319,\"start\":79034},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2179911},\"end\":79800,\"start\":79321},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11748953},\"end\":80123,\"start\":79802},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":911},\"end\":80337,\"start\":80125},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2551865},\"end\":80834,\"start\":80339},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1102037},\"end\":81122,\"start\":80836},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1912619},\"end\":81497,\"start\":81124},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":556331},\"end\":81805,\"start\":81499},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":42639},\"end\":82143,\"start\":81807},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14915716},\"end\":82498,\"start\":82145},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2313314},\"end\":82912,\"start\":82500},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206812664},\"end\":83279,\"start\":82914},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206591234},\"end\":83723,\"start\":83281},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":149880},\"end\":84134,\"start\":83725},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5893207},\"end\":84478,\"start\":84136},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":428083},\"end\":84868,\"start\":84480},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15101323},\"end\":85196,\"start\":84870},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8666852},\"end\":85540,\"start\":85198},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5857911},\"end\":85997,\"start\":85542},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":8773092},\"end\":86298,\"start\":85999},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9133198},\"end\":86683,\"start\":86300},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":11904287},\"end\":87041,\"start\":86685},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1494934},\"end\":87400,\"start\":87043},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":29084021},\"end\":87619,\"start\":87402},{\"attributes\":{\"id\":\"b45\"},\"end\":87811,\"start\":87621},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":116908168},\"end\":88048,\"start\":87813},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":7691428},\"end\":88281,\"start\":88050},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":308370},\"end\":88564,\"start\":88283},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":12587318},\"end\":88860,\"start\":88566},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7372855},\"end\":89178,\"start\":88862},{\"attributes\":{\"doi\":\"157\",\"id\":\"b51\"},\"end\":89546,\"start\":89180},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":10571559},\"end\":90064,\"start\":89548},{\"attributes\":{\"id\":\"b53\"},\"end\":90169,\"start\":90066},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":11995267},\"end\":90483,\"start\":90171},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":15413966},\"end\":90762,\"start\":90485},{\"attributes\":{\"id\":\"b56\"},\"end\":91119,\"start\":90764},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":6692382},\"end\":91581,\"start\":91121},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":3251546},\"end\":91945,\"start\":91583},{\"attributes\":{\"id\":\"b59\"},\"end\":92396,\"start\":91947},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":11398758},\"end\":92842,\"start\":92398},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":17066951},\"end\":93108,\"start\":92844},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":9605046},\"end\":93369,\"start\":93110},{\"attributes\":{\"doi\":\"181\",\"id\":\"b63\"},\"end\":93679,\"start\":93371},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":14137478},\"end\":94105,\"start\":93681},{\"attributes\":{\"id\":\"b65\"},\"end\":94549,\"start\":94107}]", "bib_title": "[{\"end\":72655,\"start\":72559},{\"end\":73120,\"start\":73072},{\"end\":73412,\"start\":73346},{\"end\":73629,\"start\":73575},{\"end\":74089,\"start\":73996},{\"end\":74329,\"start\":74311},{\"end\":74539,\"start\":74502},{\"end\":74867,\"start\":74763},{\"end\":75253,\"start\":75203},{\"end\":75547,\"start\":75482},{\"end\":75914,\"start\":75828},{\"end\":76436,\"start\":76406},{\"end\":76767,\"start\":76722},{\"end\":77343,\"start\":77266},{\"end\":77691,\"start\":77635},{\"end\":78008,\"start\":77959},{\"end\":78399,\"start\":78332},{\"end\":78779,\"start\":78697},{\"end\":79083,\"start\":79034},{\"end\":79380,\"start\":79321},{\"end\":79843,\"start\":79802},{\"end\":80150,\"start\":80125},{\"end\":80423,\"start\":80339},{\"end\":80883,\"start\":80836},{\"end\":81209,\"start\":81124},{\"end\":81557,\"start\":81499},{\"end\":81880,\"start\":81807},{\"end\":82240,\"start\":82145},{\"end\":82567,\"start\":82500},{\"end\":82980,\"start\":82914},{\"end\":83382,\"start\":83281},{\"end\":83767,\"start\":83725},{\"end\":84196,\"start\":84136},{\"end\":84540,\"start\":84480},{\"end\":84934,\"start\":84870},{\"end\":85264,\"start\":85198},{\"end\":85649,\"start\":85542},{\"end\":86030,\"start\":85999},{\"end\":86375,\"start\":86300},{\"end\":86757,\"start\":86685},{\"end\":87162,\"start\":87043},{\"end\":87456,\"start\":87402},{\"end\":87843,\"start\":87813},{\"end\":88087,\"start\":88050},{\"end\":88333,\"start\":88283},{\"end\":88614,\"start\":88566},{\"end\":88880,\"start\":88862},{\"end\":89592,\"start\":89548},{\"end\":90223,\"start\":90171},{\"end\":90517,\"start\":90485},{\"end\":91180,\"start\":91121},{\"end\":91670,\"start\":91583},{\"end\":92007,\"start\":91947},{\"end\":92492,\"start\":92398},{\"end\":92875,\"start\":92844},{\"end\":93158,\"start\":93110},{\"end\":93725,\"start\":93681},{\"end\":94198,\"start\":94107}]", "bib_author": "[{\"end\":72444,\"start\":72434},{\"end\":72670,\"start\":72657},{\"end\":72679,\"start\":72670},{\"end\":72919,\"start\":72906},{\"end\":72931,\"start\":72919},{\"end\":72938,\"start\":72931},{\"end\":73136,\"start\":73122},{\"end\":73421,\"start\":73414},{\"end\":73430,\"start\":73421},{\"end\":73640,\"start\":73631},{\"end\":73650,\"start\":73640},{\"end\":73661,\"start\":73650},{\"end\":74101,\"start\":74091},{\"end\":74112,\"start\":74101},{\"end\":74341,\"start\":74331},{\"end\":74549,\"start\":74541},{\"end\":74559,\"start\":74549},{\"end\":74571,\"start\":74559},{\"end\":74877,\"start\":74869},{\"end\":74890,\"start\":74877},{\"end\":74907,\"start\":74890},{\"end\":75265,\"start\":75255},{\"end\":75274,\"start\":75265},{\"end\":75559,\"start\":75549},{\"end\":75565,\"start\":75559},{\"end\":75575,\"start\":75565},{\"end\":75585,\"start\":75575},{\"end\":75594,\"start\":75585},{\"end\":75601,\"start\":75594},{\"end\":75932,\"start\":75916},{\"end\":75944,\"start\":75932},{\"end\":75952,\"start\":75944},{\"end\":76256,\"start\":76248},{\"end\":76448,\"start\":76438},{\"end\":76456,\"start\":76448},{\"end\":76465,\"start\":76456},{\"end\":76475,\"start\":76465},{\"end\":76488,\"start\":76475},{\"end\":76781,\"start\":76769},{\"end\":76791,\"start\":76781},{\"end\":76802,\"start\":76791},{\"end\":77354,\"start\":77345},{\"end\":77367,\"start\":77354},{\"end\":77702,\"start\":77693},{\"end\":77709,\"start\":77702},{\"end\":77716,\"start\":77709},{\"end\":78020,\"start\":78010},{\"end\":78028,\"start\":78020},{\"end\":78038,\"start\":78028},{\"end\":78048,\"start\":78038},{\"end\":78054,\"start\":78048},{\"end\":78409,\"start\":78401},{\"end\":78418,\"start\":78409},{\"end\":78426,\"start\":78418},{\"end\":78435,\"start\":78426},{\"end\":78789,\"start\":78781},{\"end\":78799,\"start\":78789},{\"end\":79095,\"start\":79085},{\"end\":79103,\"start\":79095},{\"end\":79113,\"start\":79103},{\"end\":79393,\"start\":79382},{\"end\":79405,\"start\":79393},{\"end\":79414,\"start\":79405},{\"end\":79427,\"start\":79414},{\"end\":79856,\"start\":79845},{\"end\":79868,\"start\":79856},{\"end\":79877,\"start\":79868},{\"end\":79889,\"start\":79877},{\"end\":79902,\"start\":79889},{\"end\":80165,\"start\":80152},{\"end\":80176,\"start\":80165},{\"end\":80437,\"start\":80425},{\"end\":80447,\"start\":80437},{\"end\":80458,\"start\":80447},{\"end\":80894,\"start\":80885},{\"end\":80906,\"start\":80894},{\"end\":81221,\"start\":81211},{\"end\":81229,\"start\":81221},{\"end\":81243,\"start\":81229},{\"end\":81569,\"start\":81559},{\"end\":81577,\"start\":81569},{\"end\":81586,\"start\":81577},{\"end\":81596,\"start\":81586},{\"end\":81894,\"start\":81882},{\"end\":81904,\"start\":81894},{\"end\":81918,\"start\":81904},{\"end\":82251,\"start\":82242},{\"end\":82260,\"start\":82251},{\"end\":82578,\"start\":82569},{\"end\":82591,\"start\":82578},{\"end\":82990,\"start\":82982},{\"end\":82999,\"start\":82990},{\"end\":83007,\"start\":82999},{\"end\":83016,\"start\":83007},{\"end\":83395,\"start\":83384},{\"end\":83409,\"start\":83395},{\"end\":83419,\"start\":83409},{\"end\":83782,\"start\":83769},{\"end\":83793,\"start\":83782},{\"end\":83803,\"start\":83793},{\"end\":84206,\"start\":84198},{\"end\":84219,\"start\":84206},{\"end\":84228,\"start\":84219},{\"end\":84552,\"start\":84542},{\"end\":84560,\"start\":84552},{\"end\":84569,\"start\":84560},{\"end\":84579,\"start\":84569},{\"end\":84592,\"start\":84579},{\"end\":84945,\"start\":84936},{\"end\":84951,\"start\":84945},{\"end\":85277,\"start\":85266},{\"end\":85290,\"start\":85277},{\"end\":85659,\"start\":85651},{\"end\":85666,\"start\":85659},{\"end\":85680,\"start\":85666},{\"end\":85689,\"start\":85680},{\"end\":86042,\"start\":86032},{\"end\":86050,\"start\":86042},{\"end\":86059,\"start\":86050},{\"end\":86389,\"start\":86377},{\"end\":86401,\"start\":86389},{\"end\":86771,\"start\":86759},{\"end\":86781,\"start\":86771},{\"end\":86790,\"start\":86781},{\"end\":87177,\"start\":87164},{\"end\":87187,\"start\":87177},{\"end\":87470,\"start\":87458},{\"end\":87632,\"start\":87621},{\"end\":87644,\"start\":87632},{\"end\":87857,\"start\":87845},{\"end\":87870,\"start\":87857},{\"end\":88099,\"start\":88089},{\"end\":88111,\"start\":88099},{\"end\":88343,\"start\":88335},{\"end\":88351,\"start\":88343},{\"end\":88364,\"start\":88351},{\"end\":88375,\"start\":88364},{\"end\":88384,\"start\":88375},{\"end\":88624,\"start\":88616},{\"end\":88635,\"start\":88624},{\"end\":88646,\"start\":88635},{\"end\":88896,\"start\":88882},{\"end\":88905,\"start\":88896},{\"end\":88913,\"start\":88905},{\"end\":88927,\"start\":88913},{\"end\":89235,\"start\":89224},{\"end\":89250,\"start\":89235},{\"end\":89259,\"start\":89250},{\"end\":89272,\"start\":89259},{\"end\":89283,\"start\":89272},{\"end\":89605,\"start\":89594},{\"end\":89616,\"start\":89605},{\"end\":89629,\"start\":89616},{\"end\":89648,\"start\":89629},{\"end\":89663,\"start\":89648},{\"end\":90070,\"start\":90066},{\"end\":90237,\"start\":90225},{\"end\":90252,\"start\":90237},{\"end\":90531,\"start\":90519},{\"end\":90541,\"start\":90531},{\"end\":90552,\"start\":90541},{\"end\":90566,\"start\":90552},{\"end\":90902,\"start\":90889},{\"end\":91191,\"start\":91182},{\"end\":91201,\"start\":91191},{\"end\":91208,\"start\":91201},{\"end\":91218,\"start\":91208},{\"end\":91226,\"start\":91218},{\"end\":91686,\"start\":91672},{\"end\":91700,\"start\":91686},{\"end\":91708,\"start\":91700},{\"end\":92018,\"start\":92009},{\"end\":92027,\"start\":92018},{\"end\":92039,\"start\":92027},{\"end\":92052,\"start\":92039},{\"end\":92064,\"start\":92052},{\"end\":92075,\"start\":92064},{\"end\":92081,\"start\":92075},{\"end\":92505,\"start\":92494},{\"end\":92516,\"start\":92505},{\"end\":92530,\"start\":92516},{\"end\":92539,\"start\":92530},{\"end\":92893,\"start\":92877},{\"end\":92903,\"start\":92893},{\"end\":93171,\"start\":93160},{\"end\":93180,\"start\":93171},{\"end\":93436,\"start\":93424},{\"end\":93447,\"start\":93436},{\"end\":93735,\"start\":93727},{\"end\":93744,\"start\":93735},{\"end\":93755,\"start\":93744},{\"end\":93768,\"start\":93755},{\"end\":94211,\"start\":94200},{\"end\":94219,\"start\":94211},{\"end\":94228,\"start\":94219},{\"end\":94239,\"start\":94228}]", "bib_venue": "[{\"end\":73800,\"start\":73739},{\"end\":77001,\"start\":76901},{\"end\":79574,\"start\":79509},{\"end\":80611,\"start\":80543},{\"end\":83946,\"start\":83883},{\"end\":87684,\"start\":87676},{\"end\":89826,\"start\":89753},{\"end\":91365,\"start\":91304},{\"end\":93907,\"start\":93846},{\"end\":72432,\"start\":72381},{\"end\":72685,\"start\":72679},{\"end\":72904,\"start\":72874},{\"end\":73186,\"start\":73136},{\"end\":73436,\"start\":73430},{\"end\":73737,\"start\":73661},{\"end\":74129,\"start\":74112},{\"end\":74380,\"start\":74341},{\"end\":74607,\"start\":74571},{\"end\":74966,\"start\":74907},{\"end\":75312,\"start\":75274},{\"end\":75624,\"start\":75601},{\"end\":75963,\"start\":75952},{\"end\":76246,\"start\":76150},{\"end\":76544,\"start\":76488},{\"end\":76899,\"start\":76802},{\"end\":77420,\"start\":77367},{\"end\":77781,\"start\":77716},{\"end\":78116,\"start\":78054},{\"end\":78496,\"start\":78435},{\"end\":78836,\"start\":78799},{\"end\":79150,\"start\":79113},{\"end\":79507,\"start\":79427},{\"end\":79938,\"start\":79902},{\"end\":80206,\"start\":80176},{\"end\":80541,\"start\":80458},{\"end\":80962,\"start\":80906},{\"end\":81281,\"start\":81243},{\"end\":81629,\"start\":81596},{\"end\":81949,\"start\":81918},{\"end\":82300,\"start\":82260},{\"end\":82680,\"start\":82591},{\"end\":83078,\"start\":83016},{\"end\":83484,\"start\":83419},{\"end\":83881,\"start\":83803},{\"end\":84289,\"start\":84228},{\"end\":84657,\"start\":84592},{\"end\":85016,\"start\":84951},{\"end\":85355,\"start\":85290},{\"end\":85754,\"start\":85689},{\"end\":86121,\"start\":86059},{\"end\":86463,\"start\":86401},{\"end\":86846,\"start\":86790},{\"end\":87211,\"start\":87187},{\"end\":87488,\"start\":87470},{\"end\":87674,\"start\":87644},{\"end\":87907,\"start\":87870},{\"end\":88147,\"start\":88111},{\"end\":88398,\"start\":88384},{\"end\":88684,\"start\":88646},{\"end\":88989,\"start\":88927},{\"end\":89222,\"start\":89180},{\"end\":89751,\"start\":89663},{\"end\":90090,\"start\":90070},{\"end\":90299,\"start\":90252},{\"end\":90598,\"start\":90566},{\"end\":90887,\"start\":90764},{\"end\":91302,\"start\":91226},{\"end\":91733,\"start\":91708},{\"end\":92130,\"start\":92081},{\"end\":92604,\"start\":92539},{\"end\":92959,\"start\":92903},{\"end\":93216,\"start\":93180},{\"end\":93422,\"start\":93371},{\"end\":93844,\"start\":93768},{\"end\":94296,\"start\":94239}]"}}}, "year": 2023, "month": 12, "day": 17}