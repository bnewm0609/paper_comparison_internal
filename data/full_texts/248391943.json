{"id": 248391943, "updated": "2023-12-14 07:45:53.05", "metadata": {"title": "Hypergraph Contrastive Collaborative Filtering", "authors": "[{\"first\":\"Lianghao\",\"last\":\"Xia\",\"middle\":[]},{\"first\":\"Chao\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Yong\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Jiashu\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Dawei\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Jimmy\",\"last\":\"Huang\",\"middle\":[\"Xiangji\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Collaborative Filtering (CF) has emerged as fundamental paradigms for parameterizing users and items into latent representation space, with their correlative patterns from interaction data. Among various CF techniques, the development of GNN-based recommender systems, e.g., PinSage and LightGCN, has offered the state-of-the-art performance. However, two key challenges have not been well explored in existing solutions: i) The over-smoothing effect with deeper graph-based CF architecture, may cause the indistinguishable user representations and degradation of recommendation results. ii) The supervision signals (i.e., user-item interactions) are usually scarce and skewed distributed in reality, which limits the representation power of CF paradigms. To tackle these challenges, we propose a new self-supervised recommendation framework Hypergraph Contrastive Collaborative Filtering (HCCF) to jointly capture local and global collaborative relations with a hypergraph-enhanced cross-view contrastive learning architecture. In particular, the designed hypergraph structure learning enhances the discrimination ability of GNN-based CF paradigm, so as to comprehensively capture the complex high-order dependencies among users. Additionally, our HCCF model effectively integrates the hypergraph structure encoding with self-supervised learning to reinforce the representation quality of recommender systems, based on the hypergraph-enhanced self-discrimination. Extensive experiments on three benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods, and the robustness against sparse user interaction data. Our model implementation codes are available at https://github.com/akaxlh/HCCF.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/XiaHXZYH22", "doi": "10.1145/3477495.3532058"}}, "content": {"source": {"pdf_hash": "46d3018afbb31e31be97d18a8ee5f60f49635d81", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.12200v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "beeb9e99a687aa091dc74312e1ef3f3a3d287598", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/46d3018afbb31e31be97d18a8ee5f60f49635d81.txt", "contents": "\nHypergraph Contrastive Collaborative Filtering ACM Reference Format\nACMCopyright ACMJuly 11-15, 2022. July 11-15, 2022\n\nLianghao Xia \nChao Huang chaohuang75@gmail.com \nYong Xu \nJiashu Zhao jzhao@wlu.ca \nDawei Yin yindawei@acm.org \nJimmy Xiangji Huang jhuang@yorku.ca \nLianghao Xia \nChao Huang \nYong Xu \nJiashu Zhao \nDawei Yin \nJimmy Xiangji Huang \n\nUniversity of Hong Kong\nUniversity of Hong\nKong\n\n\nSouth China University of Technology\nWilfrid Laurier University\nBaidu Inc\nYork University\n\n\nHypergraph Contrastive Collaborative Filtering ACM Reference Format\n\nProceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'22)\nthe 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'22)Madrid, Spain; Madrid, Spain; SingaporeACM10July 11-15, 2022. July 11-15, 202210.1145/3477495.3532058* Chao Huang is the corresponding author. ACM ISBN 978-1-4503-8732-3/22/07. . . $15.00. 2022. Hypergraph Contrastive Collaborative Filtering. InRecommendationCollaborative FilteringSelf-Supervised Learning\nCollaborative Filtering (CF) has emerged as fundamental paradigms for parameterizing users and items into latent representation space, with their correlative patterns from interaction data. Among various CF techniques, the development of GNN-based recommender systems, e.g., PinSage and LightGCN, has offered the state-of-theart performance. However, two key challenges have not been well explored in existing solutions: i) The over-smoothing effect with deeper graph-based CF architecture, may cause the indistinguishable user representations and degradation of recommendation results. ii) The supervision signals (i.e., user-item interactions) are usually scarce and skewed distributed in reality, which limits the representation power of CF paradigms. To tackle these challenges, we propose a new self-supervised recommendation framework Hypergraph Contrastive Collaborative Filtering (HCCF) to jointly capture local and global collaborative relations with a hypergraphenhanced cross-view contrastive learning architecture. In particular, the designed hypergraph structure learning enhances the discrimination ability of GNN-based CF paradigm, so as to comprehensively capture the complex high-order dependencies among users. Additionally, our HCCF model effectively integrates the hypergraph structure encoding with self-supervised learning to reinforce the representation quality of recommender systems, based on the hypergraph-enhanced self-discrimination. Extensive experiments on three benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods, and the robustness against sparse user interaction data. Our model implementation codes are available at https://github.com/akaxlh/HCCF.CCS CONCEPTS\u2022 Information systems \u2192 Recommender systems.\n\nINTRODUCTION\n\nPersonalized recommender systems have been widely utilized to help users discover items of interest for information overload alleviation on the web, such as online retail platforms (e.g., Amazon, Alibaba) [17], social networking applications (e.g., Facebook, Wechat) and online video sites (e.g., Youtube, Tiktok) [10]. Among various techniques, Collaborative filtering (CF) has emerged as the foundation architecture for recommendation, such as conventional matrix factorization methods [14,26], and recent neural network-based models [13,34,52]. The key rationale behind the CF paradigm is to project users and items into low-dimensional latent representations, based on the observed user-item interactions [7].\n\nWith the advancement of graph neural networks (GNNs) in achieving unprecedented success for graph representations tasks, one recent promising research line leverages graph neural networks to exploit user-item interaction graph structure for collaborative filtering. Several recent efforts have explored deeply into GNNs to propagate embeddings with user-item interaction edges to capture CF signals among neighboring nodes in subgraphs, so as to further improve the user representation quality. Specifically, PinSage [44] and NGCF [35] employ GCN to propagate embeddings over the user-item interaction graph. Later on, LightGCN [12] proposes to alleviate training difficulty by identifying the unnecessity of feature transformation in GNN-based recommender system. Additionally, inspired by the effectiveness of disentangled graph representation learning [23], DGCF [36] disentangles latent user interests with intent-aware user-item graph structures for embedding users.\n\nWhile the aforementioned graph-based Collaborative Filtering models have offered state-of-the-art performance for recommendation applications, two key issues remain less explored: i) Over-Smoothing Collaborative Effects. The graph neural CF architecture with deeper embedding propagation layers may result in indistinguishable user vectors [25,50], which limits the representation quality of high-order collaborative relations. The over-mixing of information among connected users/items over their interaction subgraphs can involve harmful noise for representing unique user preference. The over-smoothing issues of graph-based CF paradigms (e.g., LightGCN [12], PinSage [44], ST-GCN [47], GCCF [4]) are illustrated in Figure 1. In particular, we observe the  Figure 1: i) The recommendation performance (measured by NDCG@20); and ii) The embedding smoothness (measured by MAD metric with heatmaps) of state-of-the-art graphbased collaborative filtering models (i.e., LightGCN [12], Pin-Sage [44], GCCF [4], ST-GCN [47]) on Yelp dataset. larger smoothness degrees of the user representations encoded from different methods with the increase of graph propagation layers. Here, we adopt the quantitative metric Mean Average Distance (MAD) [3] to measure the graph smoothness by calculating the average distance among user embeddings learned by different methods. As shown in Figure 1, the over-smoothing issue limits the recommendation performance of graph neural CF frameworks due to the indistinguishable embeddings by stacking more graph aggregation layers. Hence, encoding distinguishable and informative user/item representations from the holistic interaction graph is of crucial importance for differentiating user preference. ii) Supervision Interaction Data Scarcity and Noise: Most graph-based collaborative filtering models belong to the supervised learning paradigm, in which the user representation process is supervised by user-item interaction signals. The effectiveness of such approaches largely relies on sufficient supervision signals (i.e., observed user-item interactions), and cannot learn quality user/item representations under the interaction label scarcity. However, the interaction data scarcity issue is ubiquitous in various practical recommendation scenarios [49], since most users only sporadically interact with very limited number of items. The majority of collaborative filter-based recommender systems cannot perform well on long-tail items because of the data sparsity phenomenon and skewed interaction distribution. Additionally, most GNN-based recommendation methods design the message passing mechanism in which the embedding propagation is merely performed with the original graph neighbors. However, such explicit user-item relations (e.g., clicks or views) always bring noisy information (e.g., user misclick behaviors) to the constructed interaction graph [48]. For example, user's real interest is likely to be confounded after his/her page views on a lot of irrelevant items [38].\n\nPresented Work. In view of the aforementioned limitations and challenges, we develop a hypergraph contrastive learning framework with self-augmentation for GNN-based collaborative relation modeling, by proposing a new recommendation model-HCCF. In particular, we leverage hypergraph learning architecture to supplement the encoding of graph-based CF paradigm with global collaborative effects based on the low-rank structure learning. While several recent studies use hypergraph to encode the high-order relations in recommendation, most of them rely on the original connections among users or items to generate their hypergraph connections. For example, DHCF [18] constructs the hypergraph between users and items based on their observed interactions. MHCN [45] pre-defines the motif-based social connections for hypergraph generation. Nevertheless, such observed connections are inevitably noisy or incomplete in practical scenarios [48]. To mitigate this issue, in our HCCF, the parameterized hypergraph-guided user dependent structures and the original interaction graph encoder are jointly learned towards better user preference representation.\n\nFurthermore, we design relational learning augmentation schema based on the dual-graph contrastive learning. Inspired by the strength of contrastive learning in visual and language data representation [5,31], there exist some recent self-supervised collaborative filtering models (e.g., SGL [38], SLRec [43]) which augment the original interaction data with randomly node and edge dropout, or mask operations. However, this might drop some essential information (e.g., interactions between users and their interested items), which hinders the encoding of user real interests. To fill this gap, we treat the explicit interaction graph and the learned implicit hypergraph structure as two contrastive views, without the random noise perturbation based on the pre-defined operators. We also provide theoretical justification behind our model design from the perspective of improving model optimization with better gradients. Empirical results show that HCCF consistently improves the performance over state-of-the-art recommendation models, e.g., average relative improvement of 24.7% over LightGCN, 33.8% over DHCF, 20.2% over MHCN and 14.8% over SGL in terms of NDCG@20.\n\nIn summary, this paper makes the following contributions: \u2022 We introduce a generic self-supervised recommendation framework for enhancing the robustness of graph collaborative filtering paradigm, through distilling self-augmented contrastive views between the local and global modeling of collaborative effects. \u2022 We present HCCF which innovatively integrates the global hypergraph structure learning with local collaborative relation encoder, to cooperatively supervise each other. The designed hypergraph contrastive learning schema empowers the graph neural CF paradigm, to capture the intrinsic and implicit dependencies among users and items with effective instance discrimination. \u2022 Systematic experimental studies are conducted to evaluate the performance of our HCCF model and 15 various baselines on several benchmark datasets. Further ablation study is provided to investigate the effect of key modules in HCCF.\n\n\nPRELIMINARIES AND RELATED WORK 2.1 Collaborative Filtering Learning Paradigm\n\nWe let U = { 1 , ..., , ..., } (|U| = ) and V = { 1 , ..., , ..., } (|V | = ) represent the set of users and items, respectively. The interaction matrix A \u2208 R \u00d7 indicates the implicit relationships between each user in U and his/her consumed items. Each entry A , in A will be set as 1 if user has adopted item before and A , = 0 otherwise. The objective of CF task is to forecast the unobserved user-item interactions with the encoded corresponding representations. The assumption of CF paradigm lies in that behaviorally similar users are more likely to share similar interests.\n\nMany existing CF approaches are designed with the various embedding functions to generate vectorized representations of users and items. Then, the similarity matching function is introduced to estimate the relevance score between user and the candidate item . Following this paradigm, NCF [13] and DMF [42] replaces the inner-product with Multilayer Perceptron to reconstruct the ground truth user-item interactions. Furthermore, to transform users and items into the latent embeddings, Autoencoder has been utilized as the embedding function with the behavior reconstruction objective, such as AutoRec [30] and CDAE [39].\n\n\nGraph-based Recommender Systems.\n\nTo capture high-order collaborative signals, one prominent direction explores user-item relations based on multi-hop interaction topological structures with graph oriented approaches [16,35,41,44]. For example, NGCF [35] and PinSage [44] have demonstrated the importance of high-order connectivity between users and items for collaborative filtering. To further improve the message passing process in graph convolutional framework, LightGCN [12] proposes to omit the non-linear transformation during propagation, and uses the sum-based pooling operation for neighborhood aggregation.\n\nSome recent studies also follow the graph-structured information propagation rule to refine user/item embeddings, with various neighborhood aggregation functions [15]. For example, learning disentangled or behavior-aware user representations is proposed to improve CF paradigm, e.g., DGCF [36], MacridVAE [24] and MBGMN [41]. The hyperbolic embedding space is adopted to encode high-order information from neighboring users/items in [32].\n\n\nHypergraph Learning for Recommendation\n\nInspired by the generalization ability of hypergraph in modeling complex high-order dependencies [8,9,11], some recently developed recommender systems are empowered to capture interaction patterns with the constructed hypergraph structures and uniform node-hyperedge connections, like HyRec [33], DHCF [18] and MHCN [45]. For instance, HyRec attempts to propagate information among multiple items by considering users as hyperedges. DHCF models the hybrid multi-order correlations between users and items based on the constructed hypergraph structures. Different from them, our HCCF framework designs a learnable hypergraph structure encoder, which not only improves the discrimination capability of CF representation paradigm, but also preserves the personalized global collaborative relationships across users and items in recommender systems. The presented hypergraph dependency structure learning method brings advantages in automatically distilling interdependency to enhance discrimination ability of user and item representations by addressing the graph over-smoothing issue.\n\n\nContrastive Representation Learning\n\nContrastive learning has become an effective self-supervised framework, to capture the feature representation consistency under different views [27,37]. It has achieved promising performance in various domains, such as visual data representation [5,28], language data understanding [2,31], graph representation learning [29,51] and recommender systems [22,38,45,46]. These contrastive learning approaches seek the exploration of data-or task-specific augmentations with auxiliary signals. To tackle the challenge of insufficient supervision labels in recommendation, we propose a new self-supervised recommendation framework to supplement the encoding of collaborative effects with explicitly local-global interdependency modeling, under a hypergraph learning schema. \n\n\nMETHODOLOGY\n\nIn this section, we present our HCCF framework whose overall architecture is shown in Figure 2. First, we leverage the graphbased message passing module as the encoder to capture the local collaborative similarities among users and items. Second, we propose a new hypergraph neural network with global dependency structure learning to comprehensively capture global collaborative effects for graph neural CF paradigm. Finally, a new hypergraph contrastive learning architecture is introduced with complementary self-distilling views (local and global collaborative relations).\n\n\nLocal Collaborative Relation Encoding\n\nFollowing the common collaborative filtering paradigm, we first represent each user and item with the embedding vectors e ( ) \u2208 R and e ( ) \u2208 R , respectively ( denotes the embedding dimensionality). We further define E ( ) \u2208 R \u00d7 and E ( ) \u2208 R \u00d7 to represent the embeddings corresponding to users and items. Inspired by the effectiveness of simplified graph convolutional network in LightGCN [12], we design our local graph embedding propagation layer with the following form:\nz ( ) = (\u0100 , * \u00b7 E ( ) ), z ( ) = (\u0100 * , \u00b7 E ( ) )(1)\nwhere z ( ) , z ( ) \u2208 R represent the aggregated information from neighboring items/users to the centric node and . (\u00b7) denotes the LeakyReLU activation function with 0.5 slope for negative inputs, which not only brings benefits to the gradient backpropagation, but also injects the non-linearity into the transformation. Here,\u0100 \u2208 R \u00d7 denotes the normalized adjacent matrix derived from the user-item interaction matrix A calculated as:\nA = D \u22121/2 ( ) \u00b7 A \u00b7 D \u22121/2 ( ) ,\u0100 , = A , \u221a\ufe01 |N | \u00b7 |N | (2)\nwhere D ( ) \u2208 R \u00d7 , D ( ) \u2208 R \u00d7 are diagonal degree matrices. The neighboring items/users of user ( )/item ( ) are denoted by N and N , respectively. By integrating multiple embedding propagation layers, we refine the user/item representations to aggregate local neighborhood information for contextual embedding generation. Suppose e ( )\n\n, and e ( ) , represents the embedding of user and item at the ( )-th GNN layer. The message passing process from ( \u2212 1)-th layer to the ( )-th layer is formally defined as below:\ne ( ) , = z ( ) , + e ( ) , \u22121 , e ( ) , = z ( ) , + e ( ) , \u22121(3)\nWe apply the residual connections for self-information incorporation between the source and target node during the message aggregation across graph layers. This emphasizes the semantics of the centric node and alleviates the over-smoothing issue of GNN.\n\n\nHypergraph Global Dependency Learning\n\nTo inject global collaborative relations among users/items into the CF representation paradigm, we design a global dependency encoder via hypergraph structure learning.\n\n\nHypergraph Message Passing Paradigm.\n\nMotivated by the strength of hypergraph for unifying nodes beyond pairwise relations [19], we endow our HCCF to capture complex high-order relations under a deep hypergraph message passing architecture. Hypergraph consists of a set of vertices and hyperedges, in which each hyperedge can connect any number of vertices [6]. In our hypergraph collaborative filtering scenario, we utilize hyperedges as intermediate hubs for global-aware information passing across users and items without the hop distance limit. The model structure of hypergraph message passing paradigm is illustrated in Figure 3.\n\nTo be specific, we define the hypergraph dependency matrix for users and items as H ( ) \u2208 R \u00d7 and H ( ) \u2208 R \u00d7 . Here, represent the number of hyperedges. We give a formal presentation of our hypergraph message passing as:\n\u0393 ( ) = (H ( ) \u039b ( ) ) = (H ( ) \u00b7 H ( )\u22a4 \u00b7 E ( ) \u22121 )(4)\nwhere \u039b ( ) \u2208 R \u00d7 denotes the hyperedge-specific embeddings for users, and denotes the LeakyReLU mapping. \u0393 ( ) \u2208 R \u00d7 represents the hyper embeddings of users ( \u2208 U) in hypergraph representation space under the -th propagation layer. The hyper embeddings \u0393 ( ) of items ( \u2208 V) can be generated in an analogous way. Our hypergraph message extraction phase takes the user/item embeddings E ( ) \u22121 , E ( ) \u22121 (refined from the graph local embedding propagation), and learnable hypergraph dependeny structural matrices H ( ) , H ( ) (will be elaborated in the following subsection) as the computation input knowledge, to jointly preserve the local and global collaborative effects.\n\n\nParameterized Hypergraph Structure Learning.\n\nTo adaptively learn hypergraph-based dependent structures across users and items, we propose to paramerterize hypergraph dependency matrices H ( ) , H ( ) which are jointly optimized along with the GNN-based CF architecture. Without loss of generality, we use H to denote user-side dependency matrix H ( ) or item-side dependency matrix H ( ) in the following sections for simplifying notations. Note that by obtaining a trainable hypergraph structural matrix H , the node-wise global dependency can be derived with H H \u22a4 .\n\nHowever, learning dense hypergraph adjacent matrices H ( ) \u2208 R \u00d7 , H ( ) \u2208 R \u00d7 will enormously increase the size of model parameters with high computational cost (especially for the large number of hyperedges ). To tackle this challenge, we propose to parameterize the hypergraph structure matrices H ( ) , H ( ) into latent space in a low-rank manner to achieve model efficiency:\nH ( ) = E ( ) \u00b7 W ( ) , H ( ) = E ( ) \u00b7 W ( )(5)\nwhere W ( ) , W ( ) \u2208 R \u00d7 represents the learnable embedding matrices for user-and item-side hyperedges. E ( ) , E ( ) represents the embeddings of users and items. By doing so, the hypergraph structure learning component only takes extra ( \u00d7 ) time complexity, which is quite small as compared to the computational cost (( + ) \u00d7 ) for the main embedding space. In addition, we design the parameterized dependency structure encoder with the matrix decomposition on H in a low-rank manner, which further regularizes the latent structure learning for overfitting alleviation.\n\n\nHierarchical Hypergraph Mapping.\n\nAlthough the aforementioned hypergraph structure learning framework endows our HCCF with the capability of capturing global user-and item-wise collaborative relationships, we further supercharge our hypergraph neural architecture with a high-level of hyperedge-wise feature interaction. Towards this end, we augment our HCCF with multiple hypergraph neural layers by stacking different layers of hyperedges with size . In particular, the set of foregoing hyperedges will serve as the first hypergraph layer, which will interact with deep hypergraph layers non-linearly. In form, the hyperedge embeddings of deep layers are derived with the encoding function (\u00b7) as follows:\n\u039b = (\u039b), (X) = (VX) + X, \u039b = H \u22a4 E(6)\nwhere \u039b \u2208 R \u00d7 denotes the hyperedge embeddings aggregated from the user/item representations E \u2208 R \u00d7 as well as the learned node-hyperedge structures H \u2208 R \u00d7 ( = or ). Here, denotes the number of hypergraph embedding layers. V \u2208 R \u00d7 is a trainable parametric matrix for embedding projection. We adopt LeakyReLU as the activation function (\u00b7) to handle non-linearities. The residual connection operation is applied to the embedding generation across hypergraph layers. After the hierarchical hypergraph mapping, we refine the user/item representations:\n\u0393 = (H \u00b7\u039b) = (H \u00b7 (H \u22a4 \u00b7 E))(7)\n\nMulti-Order Aggregation and Prediction\n\nTo integrate the local dependency encoding with the global collaborative relation modeling, we iteratively perform the graph local embedding propagation and hypergraph information aggregation: where z ( )\n\n, represents the -th order embedding of user aggregated from his/her locally connected neighbors. \u0393 ( ) , denotes the fused -th order user representation through hyperedges. We further apply the residual operations for the embedding aggregation. The multi-order user/item embeddings are generated with the elementwise embedding summation, and the inner product of them is utilized to estimate the interaction preference score between user and item , formally is shown as follows:\n\u03a8 ( ) = \u2211\ufe01 =0 E ( ) , , \u03a8 ( ) = \u2211\ufe01 =0 E ( ) , , Pr , = \u03a8 ( )\u22a4 \u03a8 ( )(8)\nGiven these notations, we define our pair-wise marginal loss as:\nL = \u2211\ufe01 =0 \u2211\ufe01 =1 max(0, 1 \u2212 Pr , + Pr , )(9)\nFor each user , we respectively sample positive (indexed by ) and negative (indexed by ) instances from their observed and non-interacted items.\n\n\nHypergraph-enhanced Contrastive Learning\n\nThis section describes how we enable our HCCF with the cross-view collaborative supervision under a hypergraph neural architecture, to augment the user representation with sparse supervision signals.\n\n\n3.4.1\n\nHypergraph-guided Contrasting. We design our contrastive learning component by maximizing the agreement between the explicit user-item interactive relationships and the implicit hypergraphbased dependency. In particular, we generate two representation views as i) local collaborative relation encoding over the user-item interaction graph, and ii) global hypergraph structure learning among users/items. Such contrastive learning leverages the user and item self-discrimination, to offer auxiliary supervision signals from the local and global representation space.\n\n\nCross-View Collaborative Supervision.\n\nWe take different views of the same user/item as the positive pairs (z , , \u0393 , ), and treat views of different users/items as negative pairs. By doing so, our model learns discriminative representations by contrasting the generated positive and negative instances. We formally define our contrastive loss for user representations with the InfoNCE [27] as:\nL ( ) = \u2211\ufe01 =0 \u2211\ufe01 =0 \u2212 log exp( (z ( ) , , \u0393 ( ) , )/ ) \u2032 =0 exp( (z ( ) , , \u0393 ( ) \u2032 , )/ )(10)\nwhere (\u00b7) denotes the cosine similarity function and denotes the tunable temperature hyperparameter to adjust the scale for softmax. We perform the contrastive learning between the local user embedding (z , . This allows the local and global dependency views to collaboratively supervise each other, which enhances the user representation.\n\n\nData Augmentation on Graph Structure.\n\nTo further alleviate the overfitting issue during the cross-view contrastive learning process, we design edge dropout operator over both the user-item interaction graph and the learned hypergraph structure as:\nA := M \u2022\u0100; H := M \u2022 H(11)\nwhere := denotes the assignment operator and \u2022 denotes the elementwise multiplication. M \u2208 R \u00d7 and M \u2208 R \u00d7 are binary mask matrices with dropout probability . We integrate our hypergraph contrastive loss with our CF loss into a unified objective as:\nL = L + 1 \u00b7 (L ( ) + L ( ) ) + 2 \u00b7 \u2225\u0398\u2225 2 F(12)\nWe minimize L using Adam optimizer. The weight-decay regularization term is applied over parameters \u0398.\n\n\nIn-Depth Analysis of HCCF\n\nThis section provides further analysis of our HCCF model with the theoretical discussion and time complexity analysis.\n\n3.5.1 Theoretical Analysis of HCCF. Our hypergraph contrastive learning enhances the discrimination ability of graph-based CF representation paradigm by generating adaptive gradients from hard negative samples. Specifically, the influences of negative samples over the learned embeddings under the self-supervised learning architecture can be quantified as:\n( \u2032 ) = \u0393 ( ) \u2032 , \u2212 (z ( )\u22a4 ,\u0393 ( ) \u2032 , )z ( ) , \u00b7 exp(z ( )\u22a4 ,\u0393 ( ) , / ) \u2032 exp(z ( )\u22a4 ,\u0393 ( ) \u2032 , / )(13)\nwhere ( \u2032 ) denotes the gradients related to negative sample \u2032 , \u0393 , \u2225 2 are the normalized embeddings. By further analyzing the norm of ( \u2032 ), we can find it proportional to a unique function as:\n\u2225 ( \u2032 )\u2225 2 \u221d \u221a\ufe01 1 \u2212 2 \u00b7 exp( )(14)\nwhere =z\n( )\u22a4 ,\u0393 ( )\n\u2032 , is the cosine similarity between the embeddings from two contrastive views. As the similarity score increases, \u2225 ( \u2032 )\u2225 2 increases dramatically, under proper settings. Inspired by the recent studies in [20,38], our contrastive learning emphasizes the gradients from hard negative samples that are more similar to the positive ones, which improves the model training. In our hypergraph structure learning, we endow the CF paradigm with the power to capture the global user dependency without the limitation of observed user-item connections. This also allows the robust gradient updates of different user representations to influence with each other based on their latent global relatedness.\n\n\nModel Complexity Analyses.\n\nThe graph local collaborative relation encoding takes ( \u00d7 |A| \u00d7 ) complexity, where denotes the number of graph neural layers. |A| is the number of edges in user-item interaction graph. Additionally, the hypergraph message passing schema takes ( \u00d7 ( + ) \u00d7 \u00d7 ) complexity with the node-hyperedge information propagation. In our contrastive learning paradigm, the cost is ( \u00d7 \u00d7 ( + ) \u00d7 ), owing to our designed low-rank-based hypergraph structure learning. Here, denotes the number of users/items included in a single batch. For the memory cost, HCCF only involves ( \u00d7 ) extra parameters in the hypergraph structure learning, as compared to existing GNN-based collaborative filtering models.\n\n\nEVALUATION\n\nOur experiments aim to answer the research questions as follows:\n\n\u2022 RQ1: What is the performance of our HCCF as compared to various state-of-the-art recommender systems? \u2022 RQ2: How does the hypergraph structure learning and crossview contrastive learning contribute to the performance? \u2022 RQ3: How does HCCF perform in alleviating data sparsity? \u2022 RQ4: How do the key hyperparameters influence the performance of the proposed HCCF framework? \u2022 RQ5: How does the hypergraph-enhanced global dependency modeling benefit the model interpretation?\n\n\nExperimental Settings\n\n\nEvaluation Datasets.\n\nWe conduct experiments on three public datasets. Yelp: this dataset has been widely adopted for evaluating recommender systems with the task of business venue recommendation. MovieLens: it is a movie recommendation dataset. We follow the same processing rubric in [13] using the 10-core setting by keeping users and items with at least 10 interactions. Amazon-book: this dataset records user ratings on products with book category on Amazon with the 20-core setting. Table 1 presents the statistics of all datasets. Experimented datasets and codes have been released (refer to the link provided in the abstract).\n\n\nEvaluation Protocols and Metrics.\n\nFollowing the same data partition rubrics in most recent graph CF models [35,43], we generate our training, validation and test set with the ratio of 7:1:2. To mitigate the sampling bias, we evaluate the prediction accuracy using the all-ranking protocol [12] over all items. We adopt\n\nRecall@N and Normalized Discounted Cumulative Gain (NDCG)@N as the metrics, which are widely used in recommendation [35,40].\n\n\nBaseline Methods.\n\nWe compare HCCF with 15 baselines covering various recommendation paradigms. Conventional Matrix Factorization Approach.\n\n\u2022 BiasMF [21]. It is a widely adopted baseline which is developed over the matrix factorization with user and item bias.\n\n\nMLP-enhanced Collaborative Filtering.\n\n\u2022 NCF [13]. It is neural CF model which encodes the non-linear feature interactions with multiple hidden layers. In particular, we use two hidden layers with the same embedding dimensionality.\n\nAutoencoder-based Collaborative Filtering Model.\n\n\u2022 AutoR [30]. It utilizes Autoencoder as the embedding projection function with the reconstruction objective to generate latent representations for observed user-item interactions.\n\n\nGNN-based Collaborative Filtering Frameworks.\n\n\u2022 GC-MC [1]: The graph convolutional operations are applied to capture user-item dependency based on neighbor relationships. \u2022 PinSage [44]: In the message passing of PinSage, the neighbor relations are generated based on the random walk sampling strategy. It generates negative instances using the PageRank score. \u2022 NGCF [35]: this approach designs graph embedding propagation layers to generate user/item representations, by aggregating feature embeddings with high-order connection information. \u2022 ST-GCN [47]: it supplements the GCN-based with the reconstruction task of masked node embeddings to address the limitation of label leakage issue. ST-GCN integrates graph convolutional encoder-decoders with intermediate supervision.\n\n\u2022 LightGCN [12]: it proposes to simply the burdensome NGCF framework by removing the non-linear projection and embedding transformation during the message passing. The sum-based pooling is used for user representation generation. \u2022 GCCF [4]: it enhances the graph neural network-based CF from two perspectives: i) omitting the non-linear transformation; ii) incorporating the residual network structure.\n\nDisentangled Graph Learning for Recommendation.\n\n\u2022 DGCF [36]. it investigates the fine-grained user intention to enhance the representation ability of CF framework by disentangling multiple latent factors for user representation.\n\nRecommendation with Hypergraph Neural Networks.\n\n\u2022 HyRec [33]. It leverages the hypergraph structure to model relationships between user and his/her interacted items, by considering multi-order information in a dynamic environment. \u2022 DHCF [18]. The new developed jump hypergraph convolution is introduced into the dual-channel CF to perform message passing with prior information. The divide-and-conquer scheme is used for dual-channel learning.\n\nSelf-Supervised Learning for Recommendation.\n\n\u2022 MHCN [45]. It enhances the graph neural network-based recommendation with the self-supervision signals generated from the graph informax network, by maximizing the mutual information between node embedding and graph-level representation. \u2022 SGL [38]. This state-of-the-art self-supervised graph learning method directly changes the structures of user-item interaction graphs for data augmentation with i) probability-based node and edge dropout operations; ii) random walk-based sampling.  \u2022 SLRec [43]. In this method, the feature correlations are considered as the additional regularization for improving recommender systems with a multi-task self-supervised learning approach. 4.1.4 Hyperparameter Settings. We use Adam optimizer with the learning rate of 1 \u22123 and 0.96 decay ratio for model inference. The hidden state dimensionality is configured as 32. We stack two propagation layers for graph local embedding propagation. In our hypergraph learning, the number of hyperedges and hierarchical hypergraph mapping layers are set as 128 and 3, respectively. The batch size and dropout ratio are selected from {64, 128, 256, 512} and {0.25, 0.5, 0.75}, respectively. The regularization weight 1 and 2 are tuned from the ragne {1 \u22125 , 1 \u22124 , 1 \u22123 , 1 \u22122 } for loss balance. The temperature parameter is searched from the range {0.1, 0.3, 1, 3, 10} to control the strength of gradients in our contrastive learning. The effect of is further studied in the following subsection.\n\n\nPerformance Validation (RQ1)\n\nIn this section, we analyze the evaluation results and summarize the following observations and conclusions.\n\n\u2022 Overall Performance Validation. As the experimental result shown in Table 2, our HCCF consistently outperforms all baselines across different datasets in terms of all evaluation metrics. This observation validates the superiority of our HCCF method, which can be attributed to: i) By uncovering latent global collaborative effects, HCCF can not only model the holistic dependencies among users, but also preserves individual user interaction patterns with better discrimination ability. ii) Benefiting from our hypergraph contrastive learning schema, HCCF fulfills the effective self-data augmentation for sparse interactions, with cross-view (from locally to globally) supervision signals. \u2022 Superiority of Hypergraph Structure Learning. Additionally, hypergraph recommendation methods (i.e., DHCF and HyRec) outperform most of GNN-based CF models (e.g., PinSage, NGCF, ST-GCN), suggesting the effectiveness of modeling high-order collaborative effects under hypergraph architecture. The significant improvement of our HCCF over competitive hypergraph recommender systems, further indicates that our designed learnable hypergraph neural structure paradigm is good at i) adaptively fulfilling the modeling of global collaborative dependencies among users and items; ii) alleviating the over-smoothing effect among neighboring node embeddings in GNN-based CF models. Specifically, representations on user preference are refined by both local connected users/items and global dependent users.\n\n\u2022 Effectiveness of Hypergraph Contrastive Learning. With the analysis of HCCF across different datasets, we notice that the performance gap with baselines on Amazon data is larger than other datasets. This observation validates the effectiveness of HCCF in handing the sparse user-item interactions. Furthermore, compared with state-of-the-art self-supervised learning recommendation models (i.e., MHCN, SGL, SLRec), HCCF consistently achieves better performance. Specifically, SGL uses the probability-based randomly masking operations to generate contrastive views, which may dropout important supervision interaction labels. Furthermore, following the generative self-supervised frameworks, MHCN and SLRec attempt to integrate additional learning tasks with the main recommendation objective. However, the incorporated self-supervised learning objective may not be able to adapt well on the recommendation loss optimization, which significantly hurts the representation ability of recommender systems. This justifies the superiority of our hypergraph contrastive learning paradigm, via effectively integrating hypergraph strucutre learning with the cross-view contrastive selfsupervised signals for collaborative filtering.\n\n\nAblation Study of HCCF (RQ2)\n\nWe explore the component effects of HCCF from: i) global hypergraph structure learning; ii) cross-view contrastive self-augmentation.\n\n\u2022 Effect of Hypergraph Structure Learning. We investigate the importance of hypergraph learning for contributing the performance improvement, by generating two variants by: (1) removing the hierarchical hypergraph mapping for hyperedge-wise feature interaction, termed as -HHM; (2) disabling the low-rank hypergraph dependency encoding, termed as -LowR.\n\nResults. We report the evaluation results in Table 3. Without the exploration of hierarchically structured hypergraph mapping, -HHM downgrades the recommendation accuracy. This observation justifies the rationality of enabling the hierarchical non-linear feature interaction through deep hypergraph neural layers. Additionally, while -LowR preserves the multi-layer hypergraph structures, it directly learns a R \u00d7 transformation matrix with larger parameter size, and thus may lead to the overfitting. With the parameterized hypergraph structure learning in a low-rank manner, we not only simplify the model size but also alleviate the overfitting via effective global message passing. \u2022 Effect of Cross-View Contrastive Self-Supervision. We also investigate the effectiveness of another core of HCCF's cross-view hypergraph-based contrastive learning. Specifically, we build a Results. Clearly, HCCF always achieves the best performance as compared to competitive model variants, which further emphasizes the benefits of our hypergraph contrastive learning paradigm. To be specific, 1) our hypergraph structure learning is of great significance for the explicitly modeling of global property for user-item interaction patterns. It is in line with our assumption that our hypergraph neural network can alleviate the over-smoothing effect caused by local information aggregation.\n\n2) The cross-view local-global contrastive learning paradigm indeed improves the performance of GNN-based collaborative filtering, with our self-supervised contrastive objectives. The incorporated intrinsic supervision labels reinforce the user-item interaction embedding space via the self-discrimination from local and global collaborative views.\n\n\nIn Depth Analysis of HCCF's Benefits (RQ3)\n\n4.4.1 Robustness of HCCF in Alleviating Data Sparsity. To verify whether HCCF is robust to sparse issue which is ubiquitous in recommender systems, we partition users into different groups based on their interaction numbers (e.g., [20][21][22][23][24][25][25][26][27][28][29][30]. From the results shown in Figure 4, our HCCF shows potentials in addressing the data sacristy issue. We ascribe this superiority to the HCCF's ability of cooperatively supervision between local collaborative relation encoding and global dependency learning. Furthermore, SGL is relatively unstable than HCCF across different sparsity degrees. It suggests that randomly dropping nodes or edges may discard important information of the original user-item interaction structures, making the training process unstable. The GCN-based method LightGCN may not learn quality representations for user preference by only relying on the sparse interaction data.   effect, in addition to the superior performance produced by our HCCF, we also calculate the Mean Average Distance (MAD) [3] over all node embedding pairs learned by the trained HCCF and two variants: i) -CCL (without the hypergraph-enhanced cross-view contrastive learning); ii) -Hyper (without the hypergraph neural network). The quantitative MAD metric measures the smoothness of a graph in terms of its node embeddings. The measurement results are shown in Table 4, where User and Item refer to the average similarity score between user nodes and item nodes, respectively. From the results, we can observe the more obvious over-smoothing phenomenon of the compared variants (-CCL) and (-Hyper) with smaller embedding distance scores. The above observations further justify the effectiveness of our hypergraph-enhanced contrastive learning in i) alleviating the over-smoothing issue in user representations refined with graph propagation; ii) empowering the model generalization ability of graph-based neural CF paradigm.  Figure 6: Impact of in contrastive learning on Yelp data.\n\n\nHyperparameter Analysis (RQ4)\n\nIn this section, we study the impact of several key hyperparameters (e.g., embedding dimensionality , # of hyperedge , # of graph message passing layers, temperature ) in our HCCF framework and report the evaluation results in Figure 5 and Figure 6.\n\n\u2022 (i) The best performance can be achieved with the hidden state dimensionality of 32 and the number of hyperedges of 128. In our hypergraph neural network, hyperedges serve as the intermediate connections acorss different users/items. Different hyperedge-specific cross-node structures may reflect different types of dependency semantics. Therefore, the recommendation performance degradation may stem from the overfitting issue with larger number of hyperedge-specific representation spaces. \u2022 (ii) Two layers of graph encoder is sufficient to offer good performance, since our hypergraph-enhanced CF paradigm encourages the global collaborative relation modeling. Through the message passing across both adjacent and non-adjacent users under a hypergraph architecture, users with similar interaction patterns will be reinforced to achieve similar representations, so as to preserve the global collaborative context. \u2022 (iii) In our framework of hypergraph-enhanced contrastive learning, the temperature parameter controls the strength of identifying hard negatives with the incorporated contrastive objective. From evaluation results in Figure 6, we can observe that the best performance can be achieved with = 1.0. Additionally, larger value ( > 1.0) brings smaller gradient for learning hard negatives, which leads to the performance degradation.\n\n\nCase Study (RQ5)\n\nWe qualitatively investigate the effects of our hypergraph-enhanced contrastive learning framework to i) capture the implicit global user dependency, and ii) alleviate the over-smoothing issue.\n\n4.6.1 Global User Dependency. We project user embeddings into different colors based on the vector values. The user-hyperedge dependencies are presented with different colors in terms of the relevance scores. As shown in Figure 7, although the non-overlap interacted items between different users, our HCCF can distill their implicit dependency by generating similar embeddings (with similar node colors) only using user interaction data. For example, (1) the user pair with same interacted category flavors ( 0 , 1 ) and ( 4 , 7 ); (2) socially connected users ( 5 , 9 ). This provides an intuitive impression of HCCF's ability in exploring the implicit global user dependency for offering better recommendation performance.  Figure 7: Case study of capturing implicit global dependencies across users through our hypergraph learning.\n\nissue. We randomly pick a closely-connected sub-graph for users. The learned node embeddings and the co-interaction connections are shown in Figure 8. Here we also compare HCCF with the -Hyper variant. We can see that -Hyper assigns similar embeddings to all users in the sub-graph, as their embeddings are smoothed by each other. In contrast, HCCF is able to learn the subtle differences and divide the users into roughly two groups (colored with green and brown), even if two nodes are strongly bonded together (e.g., user 17 and user 43). By checking the detailed information about the sampled users, we found that the green users (16,18,43,96) interact with much fewer items (< 50 interactions) compared to the brown users (\u2265 100 interactions). Overall, HCCF is able to distinguish users with sparse and dense interactions.  \n\n\nCONCLUSION\n\nThis paper mainly focuses on enhancing the neural collaborative filtering with the hypergraph-guided self-supervised learning paradigm. We present HCCF which is featured by a hypergraph structure learning module and a cross-view hypergraph contrastive encoding schema. Our hypergraph contrastive CF framework learns better user representations by simultaneously characterizing both local and global collaborative relationships in a joint embedding space. Extensive experiments validate the superiority of HCCF towards competitive baselines. In future, we may explore the dynamic user dependency with a time-aware hypergraph embedding function, to inject temporal context into our CF architecture.\n\nFigure 2 :\n2The illustration of HCCF's overall architecture. i) In the Graph-Local component, the explicit local collaborative similarities are encoded with the embedding aggregation function: z ( ) = (\u0100 , * \u00b7 E ( ) ) and z ( ) = (\u0100 * , \u00b7 E ( ) ). ii) In the Hypergraph-Global component, the global crossuser and cross-item dependencies are captured through the hypergraph message passing function: (H ( ) \u00b7 H ( )\u22a4 \u00b7 E ( ) \u22121 ). iii) In the cross-view contrastive component, the selfsupervision signals are distilled from both local and global perspectives with the representations z , and global \u0393 , .\n\nFigure 3 :\n3The illustration of hypergraph-enhanced contrasting architecture built over the hypergraph structure learning. (a) The hypergraph structure is parameterized with H ( ) and H ( ) . (b) (\u039b) represents the hierarchically structured multiple hypergraph neural layers. (c) Contrastive objectives L ( ) and L ( ) are generated via local-global cross-view supervision.\n\n4.4. 2\n2Effect of HCCF in Addressing Over-Smoothing. With our designed cross-view hypergraph-guided contrastive learning component, user/item embeddings can be regularized to be far away based on their self-discrimination between the local-level and global-level collaborative relations. By doing so, the graph-based over-smoothing effect can be alleviated in our framework. To validate the effectiveness of our method in alleviating over-smoothing\n\nFigure 4 :\n4Performance w.r.t interaction degrees.\n\nFigure 5 :\n5Hyperparameter study of the HCCF.\n\nFigure 8 :\n8Learned embeddings of users in a closelyconnected sub-graph given by HCCF with and without our hypergraph learning compoent (model variant -Hyper).\n\nTable 1 :\n1Statistics of the experimental datasets.Dataset \nUser # Item # Interaction # Density \n\nYelp \n29601 24734 \n1517326 \n2.1 \u22123 \nMovielens \n69878 10196 \n9988816 \n1.4 \u22122 \nAmazon-book 78578 77801 \n3190224 \n5.2 \u22124 \n\n\n\nTable 2 :\n2Performance comparison on Yelp, MovieLens, Amazon datasets in terms of Recall and NDCG. AutoR GCMC PinSage NGCF STGCN LightGCN GCCF DGCF HyRec DHCF MHCN SLRec SGL HCCF p-val.Data \nMetric \nBiasMF NCF \n\nTable 3 :\n3Ablation study on key components of HCCF.Data \nYelp \nMovieLens \nAmazon \nVariants Recall NDCG Recall NDCG Recall NDCG \nTop-20 \n-LowR \n0.0587 \n0.0496 \n0.1937 \n0.2317 \n0.0319 \n0.0236 \n-HHM \n0.0599 \n0.0505 \n0.1891 \n0.2319 \n0.0305 \n0.0230 \n-Hyper \n0.0584 \n0.0490 \n0.1847 \n0.2233 \n0.0257 \n0.0188 \n-CCL \n0.0566 \n0.0484 \n0.1984 \n0.2397 \n0.0282 \n0.0217 \nHCCF \n0.0607 \n0.0510 \n0.2048 \n0.2467 \n0.0344 \n0.0258 \nTop-40 \n-LowR \n0.0987 \n0.0634 \n0.2984 \n0.2622 \n0.0530 \n0.0309 \n-HHM \n0.0992 \n0.0646 \n0.2914 \n0.2565 \n0.0505 \n0.0297 \n-Hyper \n0.0982 \n0.0625 \n0.2780 \n0.2450 \n0.0419 \n0.0243 \n-CCL \n0.0937 \n0.0619 \n0.3021 \n0.2650 \n0.0460 \n0.0276 \nHCCF \n0.1007 \n0.0658 \n0.3081 \n0.2717 \n0.0561 \n0.0330 \n\nvariant -CCL by disabling the contrastive learning between the \nuser-item interaction encoding and hypergraph dependency mod-\neling. Another model variant -Hyper only relies on the encoding \nof local collaborative relations to produce user and item repre-\nsentations. This variant does not capture the global user-and \nitem-wise collaborative relationships through the hypergraph. \n\n\n\nTable 4 :\n4Graph smoothness degrees (measured by MAD) with the encoded user/item embeddings by comparing with the variant -CCL (disabling the cross-view contrastive learning).Data Type -Hyper -CCL HCCF \nData \nType -Hyper -CCL HCCF \n\nYelp \nUser 0.9505 0.9106 0.9747 Amazon \nUser 0.7911 0.9106 0.9671 \nItem 0.7673 0.9498 0.9671 \nItem 0.8570 0.7106 0.8573 \n\n\n\n\n4.6.2 Over-SmoothingAlleviation. we conduct case study to further investigate the ability of HCCF against the over-smoothing0 \n1 \n4 \n7 \n9 \n11 \n\nAmerican (T), \nAmerican (N), \nSandwiches \n\nArts, \nShopping, \nEvent Plan. \n\nArts, \nShopping, \nPubs \n\nCanadian (N), \nPubs, \nCoffee & Tea \n\nCoffee & Tea, \nPubs, \nCanadian (N) \n\nAmerican (T), \nAmerican (N), \nSandwiches \n\nTop-3 \nInteracted \nCategories \n\nUser \nEmbedding \n\nLearned \nHypergraph \nDependency \n\nLegend for Hypergraph connections \n\nmin \n0 \nmax \n\n5 \n\nPubs, \nShopping \nArts \n\nFriends \nInteract with \nSame Items \nSame Interests \n\n\nACKNOWLEDGMENTS\nGraph convolutional matrix completion. Rianne Van Den, Thomas N Berg, Max Kipf, Welling, KDD. Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu- tional matrix completion. In KDD.\n\nGrammatical Error Correction with Contrastive Learning in Low Error Density Domains. Wenmian Hannan Cao, Hwee Tou Yang, Ng, EMNLP. Hannan Cao, Wenmian Yang, and Hwee Tou Ng. 2021. Grammatical Error Correction with Contrastive Learning in Low Error Density Domains. In EMNLP. 4867-4874.\n\nMeasuring and relieving the over-smoothing problem for graph neural networks from the topological view. Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, Xu Sun, In AAAI. 34Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In AAAI, Vol. 34. 3438-3445.\n\nRevisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach. Lei Chen, Le Wu, Richang Hong, Kun Zhang, Meng Wang, In AAAI. 34Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach. In AAAI, Vol. 34. 27-34.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, ICML. PMLR. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In ICML. PMLR, 1597-1607.\n\nNeural Featureaware Recommendation with Signed Hypergraph Convolutional Network. Xu Chen, Kun Xiong, Yongfeng Zhang, Long Xia, TOIS. 39Xu Chen, Kun Xiong, Yongfeng Zhang, Long Xia, et al. 2020. Neural Feature- aware Recommendation with Signed Hypergraph Convolutional Network. TOIS 39, 1 (2020), 1-22.\n\nA hybrid collaborative filtering model with deep structure for recommender systems. Xin Dong, Lei Yu, Zhonghuo Wu, Yuxia Sun, Lingfeng Yuan, Fangxi Zhang, In AAAI. 31Xin Dong, Lei Yu, Zhonghuo Wu, Yuxia Sun, Lingfeng Yuan, and Fangxi Zhang. 2017. A hybrid collaborative filtering model with deep structure for recommender systems. In AAAI, Vol. 31.\n\nHypergraph neural networks. Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, Yue Gao, In AAAI. 33Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hy- pergraph neural networks. In AAAI, Vol. 33. 3558-3565.\n\nYue Gao, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, Changqing Zou, Hypergraph learning: Methods and practices. TPAMI. Yue Gao, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, and Changqing Zou. 2020. Hypergraph learning: Methods and practices. TPAMI (2020).\n\nXing Xie, Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems. Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, TKDE. Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems. TKDE (2020).\n\nClick-Through Rate Prediction with Multi-Modal Hypergraphs. Li He, Hongxu Chen, Dingxian Wang, Shoaib Jameel, Philip Yu, Guandong Xu, CIKM. Li He, Hongxu Chen, Dingxian Wang, Shoaib Jameel, Philip Yu, and Guandong Xu. 2021. Click-Through Rate Prediction with Multi-Modal Hypergraphs. In CIKM. 690-699.\n\nLightgcn: Simplifying and powering graph convolution network for recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, SIGIR. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In SIGIR. 639-648.\n\nXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Neural collaborative filtering. In WWW. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In WWW. 173-182.\n\nFast matrix factorization for online recommendation with implicit feedback. Xiangnan He, Hanwang Zhang, Min-Yen Kan, Tat-Seng Chua, SIGIR. Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast matrix factorization for online recommendation with implicit feedback. In SIGIR. 549-558.\n\nChao Huang, arXiv:2110.034552021. Recent Advances in Heterogeneous Relation Learning for Recommendation. arXiv preprintChao Huang. 2021. Recent Advances in Heterogeneous Relation Learning for Recommendation. arXiv preprint arXiv:2110.03455 (2021).\n\nGraph-enhanced multi-task learning of multi-level transition dynamics for session-based recommendation. Chao Huang, Jiahui Chen, Lianghao Xia, Yong Xu, Peng Dai, Yanqing Chen, Liefeng Bo, Jiashu Zhao, Jimmy Xiangji Huang, AAAI. Chao Huang, Jiahui Chen, Lianghao Xia, Yong Xu, Peng Dai, Yanqing Chen, Liefeng Bo, Jiashu Zhao, and Jimmy Xiangji Huang. 2021. Graph-enhanced multi-task learning of multi-level transition dynamics for session-based recom- mendation. In AAAI.\n\nOnline purchase prediction via multi-scale modeling of behavior dynamics. Chao Huang, Xian Wu, Xuchao Zhang, Chuxu Zhang, Jiashu Zhao, Dawei Yin, Nitesh V Chawla, Chao Huang, Xian Wu, Xuchao Zhang, Chuxu Zhang, Jiashu Zhao, Dawei Yin, and Nitesh V Chawla. 2019. Online purchase prediction via multi-scale modeling of behavior dynamics. In KDD. 2613-2622.\n\nDual channel hypergraph collaborative filtering. Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang, Yue Gao, Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang, and Yue Gao. 2020. Dual channel hypergraph collaborative filtering. In KDD. 2020-2029.\n\nDynamic Hypergraph Neural Networks. Jianwen Jiang, Yuxuan Wei, Yifan Feng, Jingxuan Cao, Yue Gao, IJCAI. Jianwen Jiang, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. 2019. Dy- namic Hypergraph Neural Networks.. In IJCAI. 2635-2641.\n\nSupervised contrastive learning. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan, NIPS. 33Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. NIPS 33 (2020), 18661-18673.\n\nMatrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Computer. 8Yehuda Koren, Robert Bell, et al. 2009. Matrix factorization techniques for rec- ommender systems. Computer 8 (2009), 30-37.\n\nSocial Recommendation with Self-Supervised Metagraph Informax Network. Xiaoling Long, Chao Huang, Yong Xu, Huance Xu, Peng Dai, Lianghao Xia, Liefeng Bo, CIKM. Xiaoling Long, Chao Huang, Yong Xu, Huance Xu, Peng Dai, Lianghao Xia, and Liefeng Bo. 2021. Social Recommendation with Self-Supervised Metagraph Informax Network. In CIKM. 1160-1169.\n\nDisentangled graph convolutional networks. Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, Wenwu Zhu, ICML. PMLR. Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. 2019. Disentangled graph convolutional networks. In ICML. PMLR, 4212-4221.\n\nLearning disentangled representations for recommendation. Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu, NIPS. Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn- ing disentangled representations for recommendation. In NIPS. 5711-5722.\n\nScattering gcn: Overcoming oversmoothness in graph convolutional networks. Yimeng Min, Frederik Wenkel, Guy Wolf, Yimeng Min, Frederik Wenkel, and Guy Wolf. 2020. Scattering gcn: Overcoming oversmoothness in graph convolutional networks. (2020).\n\nProbabilistic matrix factorization. Andriy Mnih, Russ R Salakhutdinov, NIPS. Andriy Mnih and Russ R Salakhutdinov. 2008. Probabilistic matrix factorization. In NIPS. 1257-1264.\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n\nSelfpaced contrastive learning for semi-supervised medical image segmentation with meta-labels. Jizong Peng, Ping Wang, Christian Desrosiers, Marco Pedersoli, NIPS. 34Jizong Peng, Ping Wang, Christian Desrosiers, and Marco Pedersoli. 2021. Self- paced contrastive learning for semi-supervised medical image segmentation with meta-labels. NIPS 34 (2021).\n\nGcc: Graph contrastive coding for graph neural network pre-training. Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, Jie Tang, Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph neural network pre-training. In KDD. 1150-1160.\n\nAutorec: Autoencoders meet collaborative filtering. Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, Lexing Xie, WWW. Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015. Autorec: Autoencoders meet collaborative filtering. In WWW. 111-112.\n\nContraCAT: Contrastive coreference analytical templates for machine translation. Dario Stojanovski, Benno Krojer, Denis Peskov, Alexander Fraser, COLING. Dario Stojanovski, Benno Krojer, Denis Peskov, and Alexander Fraser. 2020. ContraCAT: Contrastive coreference analytical templates for machine translation. In COLING. 4732-4749.\n\nFelipe P\u00e9rez, and Maksims Volkovs. 2021. HGCF: Hyperbolic Graph Convolution Networks for Collaborative Filtering. Jianing Sun, Zhaoyue Cheng, Saba Zuberi, Jianing Sun, Zhaoyue Cheng, Saba Zuberi, Felipe P\u00e9rez, and Maksims Volkovs. 2021. HGCF: Hyperbolic Graph Convolution Networks for Collaborative Filtering. In WWW. 593-601.\n\nNext-item recommendation with sequential hypergraphs. Jianling Wang, Kaize Ding, Liangjie Hong, Huan Liu, James Caverlee, SIGIR. Jianling Wang, Kaize Ding, Liangjie Hong, Huan Liu, and James Caverlee. 2020. Next-item recommendation with sequential hypergraphs. In SIGIR. 1101-1110.\n\nUnified collaborative filtering over graph embeddings. Pengfei Wang, Hanxiong Chen, Yadong Zhu, Huawei Shen, Yongfeng Zhang, SIGIR. Pengfei Wang, Hanxiong Chen, Yadong Zhu, Huawei Shen, and Yongfeng Zhang. 2019. Unified collaborative filtering over graph embeddings. In SIGIR. 155-164.\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In SIGIR. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In SIGIR.\n\nDisentangled graph collaborative filtering. Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua, SIGIR. Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua. 2020. Disentangled graph collaborative filtering. In SIGIR. 1001-1010.\n\nContrastive Meta Learning with Behavior Multiplicity for Recommendation. Wei Wei, Chao Huang, Lianghao Xia, Yong Xu, Jiashu Zhao, Dawei Yin, WSDM. Wei Wei, Chao Huang, Lianghao Xia, Yong Xu, Jiashu Zhao, and Dawei Yin. 2022. Contrastive Meta Learning with Behavior Multiplicity for Recommendation. In WSDM. 1120-1128.\n\nSelf-supervised graph learning for recommendation. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie, SIGIR. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised graph learning for recommendation. In SIGIR. 726-735.\n\nCollaborative denoising auto-encoders for top-n recommender systems. Yao Wu, Christopher Dubois, Alice X Zheng, Martin Ester, WSDM. ACM. Yao Wu, Christopher DuBois, Alice X Zheng, and Martin Ester. 2016. Collabora- tive denoising auto-encoders for top-n recommender systems. In WSDM. ACM, 153-162.\n\nKnowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation. Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Xiyue Zhang, Hongsheng Yang, Jian Pei, Liefeng Bo, In AAAI. 35Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Xiyue Zhang, Hongsheng Yang, Jian Pei, and Liefeng Bo. 2021. Knowledge-enhanced hierarchical graph trans- former network for multi-behavior recommendation. In AAAI, Vol. 35. 4486-4493.\n\nGraph meta network for multi-behavior recommendation. Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, Liefeng Bo, SIGIR. Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, and Liefeng Bo. 2021. Graph meta network for multi-behavior recommendation. In SIGIR. 757-766.\n\nDeep Matrix Factorization Models for Recommender Systems. Xinyu Hong-Jian Xue, Jianbing Dai, Shujian Zhang, Jiajun Huang, Chen, IJCAI. Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. 2017. Deep Matrix Factorization Models for Recommender Systems.. In IJCAI. 3203-3209.\n\nSelf-supervised Learning for Large-scale Item Recommendations. Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, CIKM. Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, et al. 2021. Self-supervised Learning for Large-scale Item Recommendations. In CIKM. 4321-4330.\n\nGraph convolutional neural networks for web-scale recommender systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, L William, Jure Hamilton, Leskovec, Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In KDD. 974-983.\n\nSelf-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation. Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convo- lutional Network for Social Recommendation. In WWW. 413-424.\n\nKnowledge graph contrastive learning for recommendation. Yang Yuhao, Chao Huang, Lianghao Xia, Chenliang Li, SIGIR. Yang Yuhao, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge graph contrastive learning for recommendation. In SIGIR.\n\nStar-gcn: Stacked and reconstructed graph convolutional networks for recommender systems. Jiani Zhang, Xingjian Shi, Shenglin Zhao, Irwin King, In IJCAI. Jiani Zhang, Xingjian Shi, Shenglin Zhao, and Irwin King. 2019. Star-gcn: Stacked and reconstructed graph convolutional networks for recommender systems. In IJCAI.\n\nGuojie Song, and Yanfang Ye. 2021. Heterogeneous Graph Structure Learning for Graph Neural Networks. Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, AAAI. Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, and Yanfang Ye. 2021. Heterogeneous Graph Structure Learning for Graph Neural Networks. In AAAI.\n\nDeep Distribution Network: Addressing the Data Sparsity Issue for Top-N Recommendation. Lei Zheng, Chaozhuo Li, Chun-Ta Lu, Jiawei Zhang, Philip S Yu, SIGIR. Lei Zheng, Chaozhuo Li, Chun-Ta Lu, Jiawei Zhang, and Philip S Yu. 2019. Deep Distribution Network: Addressing the Data Sparsity Issue for Top-N Recommen- dation. In SIGIR. 1081-1084.\n\nTowards deeper graph neural networks with differentiable group normalization. Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, Xia Hu, NIPS. Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. 2020. Towards deeper graph neural networks with differentiable group normal- ization. In NIPS.\n\nGraph contrastive learning with adaptive augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021. Graph contrastive learning with adaptive augmentation. In WWW. 2069-2080.\n\nLixin Zou, Long Xia, Yulong Gu, Xiangyu Zhao, Weidong Liu, Jimmy Xiangji Huang, and Dawei Yin. 2020. Neural interactive collaborative filtering. In SIGIR. Lixin Zou, Long Xia, Yulong Gu, Xiangyu Zhao, Weidong Liu, Jimmy Xiangji Huang, and Dawei Yin. 2020. Neural interactive collaborative filtering. In SIGIR. 749-758.\n", "annotations": {"author": "[{\"end\":134,\"start\":121},{\"end\":168,\"start\":135},{\"end\":177,\"start\":169},{\"end\":203,\"start\":178},{\"end\":231,\"start\":204},{\"end\":268,\"start\":232},{\"end\":282,\"start\":269},{\"end\":294,\"start\":283},{\"end\":303,\"start\":295},{\"end\":316,\"start\":304},{\"end\":327,\"start\":317},{\"end\":348,\"start\":328},{\"end\":398,\"start\":349},{\"end\":491,\"start\":399}]", "publisher": "[{\"end\":72,\"start\":69},{\"end\":833,\"start\":830}]", "author_last_name": "[{\"end\":133,\"start\":130},{\"end\":145,\"start\":140},{\"end\":176,\"start\":174},{\"end\":189,\"start\":185},{\"end\":213,\"start\":210},{\"end\":251,\"start\":246},{\"end\":281,\"start\":278},{\"end\":293,\"start\":288},{\"end\":302,\"start\":300},{\"end\":315,\"start\":311},{\"end\":326,\"start\":323},{\"end\":347,\"start\":342}]", "author_first_name": "[{\"end\":129,\"start\":121},{\"end\":139,\"start\":135},{\"end\":173,\"start\":169},{\"end\":184,\"start\":178},{\"end\":209,\"start\":204},{\"end\":237,\"start\":232},{\"end\":245,\"start\":238},{\"end\":277,\"start\":269},{\"end\":287,\"start\":283},{\"end\":299,\"start\":295},{\"end\":310,\"start\":304},{\"end\":322,\"start\":317},{\"end\":333,\"start\":328},{\"end\":341,\"start\":334}]", "author_affiliation": "[{\"end\":397,\"start\":350},{\"end\":490,\"start\":400}]", "title": "[{\"end\":68,\"start\":1},{\"end\":559,\"start\":492}]", "venue": "[{\"end\":683,\"start\":561}]", "abstract": "[{\"end\":2898,\"start\":1098}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3123,\"start\":3119},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3232,\"start\":3228},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3406,\"start\":3402},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3409,\"start\":3406},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3454,\"start\":3450},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3457,\"start\":3454},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3460,\"start\":3457},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3626,\"start\":3623},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4150,\"start\":4146},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4164,\"start\":4160},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4261,\"start\":4257},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4488,\"start\":4484},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4499,\"start\":4495},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4946,\"start\":4942},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4949,\"start\":4946},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5263,\"start\":5259},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5277,\"start\":5273},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5290,\"start\":5286},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5300,\"start\":5297},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5583,\"start\":5579},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5598,\"start\":5594},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5608,\"start\":5605},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5621,\"start\":5617},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5842,\"start\":5839},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6892,\"start\":6888},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7502,\"start\":7498},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7623,\"start\":7619},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8290,\"start\":8286},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8388,\"start\":8384},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8565,\"start\":8561},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8981,\"start\":8978},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8984,\"start\":8981},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9072,\"start\":9068},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9084,\"start\":9080},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11825,\"start\":11821},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11838,\"start\":11834},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12139,\"start\":12135},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12153,\"start\":12149},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12378,\"start\":12374},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12381,\"start\":12378},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12384,\"start\":12381},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12387,\"start\":12384},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12411,\"start\":12407},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12428,\"start\":12424},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12636,\"start\":12632},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12942,\"start\":12938},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13069,\"start\":13065},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13085,\"start\":13081},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13100,\"start\":13096},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13213,\"start\":13209},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13357,\"start\":13354},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13359,\"start\":13357},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13362,\"start\":13359},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13552,\"start\":13548},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13563,\"start\":13559},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13577,\"start\":13573},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14527,\"start\":14523},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14530,\"start\":14527},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14628,\"start\":14625},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14631,\"start\":14628},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14664,\"start\":14661},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14667,\"start\":14664},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14703,\"start\":14699},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14706,\"start\":14703},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14735,\"start\":14731},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14738,\"start\":14735},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14741,\"start\":14738},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14744,\"start\":14741},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16177,\"start\":16173},{\"end\":16323,\"start\":16320},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17991,\"start\":17987},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18224,\"start\":18221},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24630,\"start\":24626},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26824,\"start\":26820},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26827,\"start\":26824},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28901,\"start\":28897},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29360,\"start\":29356},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29363,\"start\":29360},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29542,\"start\":29538},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29689,\"start\":29685},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29692,\"start\":29689},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29850,\"start\":29846},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30009,\"start\":30005},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30255,\"start\":30251},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30484,\"start\":30481},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30612,\"start\":30608},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30799,\"start\":30795},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":30984,\"start\":30980},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31222,\"start\":31218},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31447,\"start\":31444},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31672,\"start\":31668},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31904,\"start\":31900},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32086,\"start\":32082},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32347,\"start\":32343},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32586,\"start\":32582},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32839,\"start\":32835},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":39210,\"start\":39206},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39214,\"start\":39210},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39218,\"start\":39214},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39222,\"start\":39218},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":39226,\"start\":39222},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39230,\"start\":39226},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39234,\"start\":39230},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":39238,\"start\":39234},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39242,\"start\":39238},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39246,\"start\":39242},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":39250,\"start\":39246},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39254,\"start\":39250},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40032,\"start\":40029},{\"end\":43413,\"start\":43404},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":44219,\"start\":44216},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":44317,\"start\":44313},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44320,\"start\":44317},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":44323,\"start\":44320},{\"end\":44326,\"start\":44323}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":45823,\"start\":45220},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46198,\"start\":45824},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46648,\"start\":46199},{\"attributes\":{\"id\":\"fig_7\"},\"end\":46700,\"start\":46649},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46747,\"start\":46701},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46908,\"start\":46748},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47128,\"start\":46909},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47340,\"start\":47129},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48417,\"start\":47341},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48774,\"start\":48418},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49353,\"start\":48775}]", "paragraph": "[{\"end\":3627,\"start\":2914},{\"end\":4600,\"start\":3629},{\"end\":7624,\"start\":4602},{\"end\":8775,\"start\":7626},{\"end\":9946,\"start\":8777},{\"end\":10869,\"start\":9948},{\"end\":11530,\"start\":10950},{\"end\":12154,\"start\":11532},{\"end\":12774,\"start\":12191},{\"end\":13214,\"start\":12776},{\"end\":14339,\"start\":13257},{\"end\":15147,\"start\":14379},{\"end\":15739,\"start\":15163},{\"end\":16257,\"start\":15781},{\"end\":16748,\"start\":16312},{\"end\":17149,\"start\":16811},{\"end\":17330,\"start\":17151},{\"end\":17651,\"start\":17398},{\"end\":17861,\"start\":17693},{\"end\":18499,\"start\":17902},{\"end\":18722,\"start\":18501},{\"end\":19457,\"start\":18780},{\"end\":20029,\"start\":19506},{\"end\":20411,\"start\":20031},{\"end\":21034,\"start\":20461},{\"end\":21744,\"start\":21071},{\"end\":22334,\"start\":21783},{\"end\":22612,\"start\":22408},{\"end\":23093,\"start\":22614},{\"end\":23229,\"start\":23165},{\"end\":23418,\"start\":23274},{\"end\":23662,\"start\":23463},{\"end\":24237,\"start\":23672},{\"end\":24634,\"start\":24279},{\"end\":25069,\"start\":24730},{\"end\":25320,\"start\":25111},{\"end\":25596,\"start\":25347},{\"end\":25746,\"start\":25644},{\"end\":25894,\"start\":25776},{\"end\":26253,\"start\":25896},{\"end\":26556,\"start\":26360},{\"end\":26600,\"start\":26592},{\"end\":27308,\"start\":26613},{\"end\":28028,\"start\":27339},{\"end\":28107,\"start\":28043},{\"end\":28584,\"start\":28109},{\"end\":29245,\"start\":28633},{\"end\":29567,\"start\":29283},{\"end\":29693,\"start\":29569},{\"end\":29835,\"start\":29715},{\"end\":29957,\"start\":29837},{\"end\":30191,\"start\":29999},{\"end\":30241,\"start\":30193},{\"end\":30423,\"start\":30243},{\"end\":31205,\"start\":30473},{\"end\":31610,\"start\":31207},{\"end\":31659,\"start\":31612},{\"end\":31841,\"start\":31661},{\"end\":31890,\"start\":31843},{\"end\":32288,\"start\":31892},{\"end\":32334,\"start\":32290},{\"end\":33814,\"start\":32336},{\"end\":33955,\"start\":33847},{\"end\":35449,\"start\":33957},{\"end\":36677,\"start\":35451},{\"end\":36843,\"start\":36710},{\"end\":37198,\"start\":36845},{\"end\":38578,\"start\":37200},{\"end\":38928,\"start\":38580},{\"end\":40991,\"start\":38975},{\"end\":41274,\"start\":41025},{\"end\":42626,\"start\":41276},{\"end\":42840,\"start\":42647},{\"end\":43677,\"start\":42842},{\"end\":44508,\"start\":43679},{\"end\":45219,\"start\":44523}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16311,\"start\":16258},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16810,\"start\":16749},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17397,\"start\":17331},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18779,\"start\":18723},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20460,\"start\":20412},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21782,\"start\":21745},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22366,\"start\":22335},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23164,\"start\":23094},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23273,\"start\":23230},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24729,\"start\":24635},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25346,\"start\":25321},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25643,\"start\":25597},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26359,\"start\":26254},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26591,\"start\":26557},{\"attributes\":{\"id\":\"formula_14\"},\"end\":26612,\"start\":26601}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29107,\"start\":29100},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34034,\"start\":34027},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37252,\"start\":37245},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40376,\"start\":40369}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2912,\"start\":2900},{\"attributes\":{\"n\":\"2\"},\"end\":10948,\"start\":10872},{\"attributes\":{\"n\":\"2.2\"},\"end\":12189,\"start\":12157},{\"attributes\":{\"n\":\"2.3\"},\"end\":13255,\"start\":13217},{\"attributes\":{\"n\":\"2.4\"},\"end\":14377,\"start\":14342},{\"attributes\":{\"n\":\"3\"},\"end\":15161,\"start\":15150},{\"attributes\":{\"n\":\"3.1\"},\"end\":15779,\"start\":15742},{\"attributes\":{\"n\":\"3.2\"},\"end\":17691,\"start\":17654},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":17900,\"start\":17864},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":19504,\"start\":19460},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":21069,\"start\":21037},{\"attributes\":{\"n\":\"3.3\"},\"end\":22406,\"start\":22368},{\"attributes\":{\"n\":\"3.4\"},\"end\":23461,\"start\":23421},{\"end\":23670,\"start\":23665},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":24277,\"start\":24240},{\"attributes\":{\"n\":\"3.4.3\"},\"end\":25109,\"start\":25072},{\"attributes\":{\"n\":\"3.5\"},\"end\":25774,\"start\":25749},{\"attributes\":{\"n\":\"3.5.2\"},\"end\":27337,\"start\":27311},{\"attributes\":{\"n\":\"4\"},\"end\":28041,\"start\":28031},{\"attributes\":{\"n\":\"4.1\"},\"end\":28608,\"start\":28587},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":28631,\"start\":28611},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":29281,\"start\":29248},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":29713,\"start\":29696},{\"end\":29997,\"start\":29960},{\"end\":30471,\"start\":30426},{\"attributes\":{\"n\":\"4.2\"},\"end\":33845,\"start\":33817},{\"attributes\":{\"n\":\"4.3\"},\"end\":36708,\"start\":36680},{\"attributes\":{\"n\":\"4.4\"},\"end\":38973,\"start\":38931},{\"attributes\":{\"n\":\"4.5\"},\"end\":41023,\"start\":40994},{\"attributes\":{\"n\":\"4.6\"},\"end\":42645,\"start\":42629},{\"attributes\":{\"n\":\"5\"},\"end\":44521,\"start\":44511},{\"end\":45231,\"start\":45221},{\"end\":45835,\"start\":45825},{\"end\":46206,\"start\":46200},{\"end\":46660,\"start\":46650},{\"end\":46712,\"start\":46702},{\"end\":46759,\"start\":46749},{\"end\":46919,\"start\":46910},{\"end\":47139,\"start\":47130},{\"end\":47351,\"start\":47342},{\"end\":48428,\"start\":48419}]", "table": "[{\"end\":47128,\"start\":46961},{\"end\":47340,\"start\":47315},{\"end\":48417,\"start\":47394},{\"end\":48774,\"start\":48594},{\"end\":49353,\"start\":48901}]", "figure_caption": "[{\"end\":45823,\"start\":45233},{\"end\":46198,\"start\":45837},{\"end\":46648,\"start\":46208},{\"end\":46700,\"start\":46662},{\"end\":46747,\"start\":46714},{\"end\":46908,\"start\":46761},{\"end\":46961,\"start\":46921},{\"end\":47315,\"start\":47141},{\"end\":47394,\"start\":47353},{\"end\":48594,\"start\":48430},{\"end\":48901,\"start\":48777}]", "figure_ref": "[{\"end\":5329,\"start\":5321},{\"end\":5370,\"start\":5362},{\"end\":5983,\"start\":5975},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15257,\"start\":15249},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18498,\"start\":18490},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39290,\"start\":39282},{\"end\":40942,\"start\":40934},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41260,\"start\":41252},{\"end\":41273,\"start\":41265},{\"end\":42423,\"start\":42415},{\"end\":43071,\"start\":43063},{\"end\":43577,\"start\":43569},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":43828,\"start\":43820}]", "bib_author_first_name": "[{\"end\":49415,\"start\":49409},{\"end\":49431,\"start\":49425},{\"end\":49433,\"start\":49432},{\"end\":49443,\"start\":49440},{\"end\":49665,\"start\":49658},{\"end\":49686,\"start\":49678},{\"end\":49968,\"start\":49964},{\"end\":49981,\"start\":49975},{\"end\":49990,\"start\":49987},{\"end\":49999,\"start\":49995},{\"end\":50007,\"start\":50004},{\"end\":50016,\"start\":50014},{\"end\":50342,\"start\":50339},{\"end\":50351,\"start\":50349},{\"end\":50363,\"start\":50356},{\"end\":50373,\"start\":50370},{\"end\":50385,\"start\":50381},{\"end\":50671,\"start\":50667},{\"end\":50683,\"start\":50678},{\"end\":50703,\"start\":50695},{\"end\":50721,\"start\":50713},{\"end\":50996,\"start\":50994},{\"end\":51006,\"start\":51003},{\"end\":51022,\"start\":51014},{\"end\":51034,\"start\":51030},{\"end\":51303,\"start\":51300},{\"end\":51313,\"start\":51310},{\"end\":51326,\"start\":51318},{\"end\":51336,\"start\":51331},{\"end\":51350,\"start\":51342},{\"end\":51363,\"start\":51357},{\"end\":51599,\"start\":51594},{\"end\":51613,\"start\":51606},{\"end\":51625,\"start\":51619},{\"end\":51641,\"start\":51633},{\"end\":51649,\"start\":51646},{\"end\":51800,\"start\":51797},{\"end\":51812,\"start\":51806},{\"end\":51826,\"start\":51820},{\"end\":51837,\"start\":51832},{\"end\":51850,\"start\":51844},{\"end\":51864,\"start\":51855},{\"end\":52164,\"start\":52158},{\"end\":52176,\"start\":52170},{\"end\":52190,\"start\":52185},{\"end\":52203,\"start\":52196},{\"end\":52437,\"start\":52435},{\"end\":52448,\"start\":52442},{\"end\":52463,\"start\":52455},{\"end\":52476,\"start\":52470},{\"end\":52491,\"start\":52485},{\"end\":52504,\"start\":52496},{\"end\":52767,\"start\":52759},{\"end\":52776,\"start\":52772},{\"end\":52788,\"start\":52783},{\"end\":52798,\"start\":52795},{\"end\":52811,\"start\":52803},{\"end\":52823,\"start\":52819},{\"end\":53027,\"start\":53019},{\"end\":53036,\"start\":53032},{\"end\":53050,\"start\":53043},{\"end\":53065,\"start\":53058},{\"end\":53074,\"start\":53071},{\"end\":53087,\"start\":53079},{\"end\":53353,\"start\":53345},{\"end\":53365,\"start\":53358},{\"end\":53380,\"start\":53373},{\"end\":53394,\"start\":53386},{\"end\":53574,\"start\":53570},{\"end\":53927,\"start\":53923},{\"end\":53941,\"start\":53935},{\"end\":53956,\"start\":53948},{\"end\":53966,\"start\":53962},{\"end\":53975,\"start\":53971},{\"end\":53988,\"start\":53981},{\"end\":54002,\"start\":53995},{\"end\":54013,\"start\":54007},{\"end\":54025,\"start\":54020},{\"end\":54033,\"start\":54026},{\"end\":54369,\"start\":54365},{\"end\":54381,\"start\":54377},{\"end\":54392,\"start\":54386},{\"end\":54405,\"start\":54400},{\"end\":54419,\"start\":54413},{\"end\":54431,\"start\":54426},{\"end\":54445,\"start\":54437},{\"end\":54701,\"start\":54696},{\"end\":54711,\"start\":54706},{\"end\":54726,\"start\":54718},{\"end\":54736,\"start\":54731},{\"end\":54749,\"start\":54743},{\"end\":54759,\"start\":54756},{\"end\":54956,\"start\":54949},{\"end\":54970,\"start\":54964},{\"end\":54981,\"start\":54976},{\"end\":54996,\"start\":54988},{\"end\":55005,\"start\":55002},{\"end\":55191,\"start\":55184},{\"end\":55205,\"start\":55200},{\"end\":55220,\"start\":55216},{\"end\":55232,\"start\":55227},{\"end\":55248,\"start\":55240},{\"end\":55262,\"start\":55255},{\"end\":55275,\"start\":55270},{\"end\":55289,\"start\":55287},{\"end\":55300,\"start\":55295},{\"end\":55582,\"start\":55576},{\"end\":55596,\"start\":55590},{\"end\":55819,\"start\":55811},{\"end\":55830,\"start\":55826},{\"end\":55842,\"start\":55838},{\"end\":55853,\"start\":55847},{\"end\":55862,\"start\":55858},{\"end\":55876,\"start\":55868},{\"end\":55889,\"start\":55882},{\"end\":56135,\"start\":56128},{\"end\":56144,\"start\":56140},{\"end\":56153,\"start\":56150},{\"end\":56164,\"start\":56161},{\"end\":56176,\"start\":56171},{\"end\":56393,\"start\":56386},{\"end\":56403,\"start\":56398},{\"end\":56414,\"start\":56410},{\"end\":56427,\"start\":56420},{\"end\":56439,\"start\":56434},{\"end\":56682,\"start\":56676},{\"end\":56696,\"start\":56688},{\"end\":56708,\"start\":56705},{\"end\":56890,\"start\":56884},{\"end\":57031,\"start\":57026},{\"end\":57051,\"start\":57046},{\"end\":57061,\"start\":57056},{\"end\":57419,\"start\":57413},{\"end\":57430,\"start\":57426},{\"end\":57446,\"start\":57437},{\"end\":57464,\"start\":57459},{\"end\":57749,\"start\":57741},{\"end\":57760,\"start\":57755},{\"end\":57773,\"start\":57767},{\"end\":57784,\"start\":57780},{\"end\":57799,\"start\":57792},{\"end\":57810,\"start\":57806},{\"end\":57824,\"start\":57817},{\"end\":57834,\"start\":57831},{\"end\":58098,\"start\":58092},{\"end\":58114,\"start\":58108},{\"end\":58122,\"start\":58115},{\"end\":58135,\"start\":58130},{\"end\":58150,\"start\":58144},{\"end\":58391,\"start\":58386},{\"end\":58410,\"start\":58405},{\"end\":58424,\"start\":58419},{\"end\":58442,\"start\":58433},{\"end\":58759,\"start\":58752},{\"end\":58772,\"start\":58765},{\"end\":58784,\"start\":58780},{\"end\":59028,\"start\":59020},{\"end\":59040,\"start\":59035},{\"end\":59055,\"start\":59047},{\"end\":59066,\"start\":59062},{\"end\":59077,\"start\":59072},{\"end\":59311,\"start\":59304},{\"end\":59326,\"start\":59318},{\"end\":59339,\"start\":59333},{\"end\":59351,\"start\":59345},{\"end\":59366,\"start\":59358},{\"end\":59541,\"start\":59536},{\"end\":59556,\"start\":59548},{\"end\":59565,\"start\":59561},{\"end\":59826,\"start\":59821},{\"end\":59839,\"start\":59833},{\"end\":59847,\"start\":59845},{\"end\":59863,\"start\":59855},{\"end\":59872,\"start\":59868},{\"end\":59885,\"start\":59877},{\"end\":60122,\"start\":60119},{\"end\":60132,\"start\":60128},{\"end\":60148,\"start\":60140},{\"end\":60158,\"start\":60154},{\"end\":60169,\"start\":60163},{\"end\":60181,\"start\":60176},{\"end\":60423,\"start\":60416},{\"end\":60433,\"start\":60428},{\"end\":60444,\"start\":60440},{\"end\":60459,\"start\":60451},{\"end\":60469,\"start\":60464},{\"end\":60483,\"start\":60476},{\"end\":60494,\"start\":60490},{\"end\":60744,\"start\":60741},{\"end\":60760,\"start\":60749},{\"end\":60774,\"start\":60769},{\"end\":60776,\"start\":60775},{\"end\":60790,\"start\":60784},{\"end\":61072,\"start\":61064},{\"end\":61082,\"start\":61078},{\"end\":61094,\"start\":61090},{\"end\":61103,\"start\":61099},{\"end\":61114,\"start\":61109},{\"end\":61131,\"start\":61122},{\"end\":61142,\"start\":61138},{\"end\":61155,\"start\":61148},{\"end\":61464,\"start\":61456},{\"end\":61474,\"start\":61470},{\"end\":61483,\"start\":61479},{\"end\":61495,\"start\":61491},{\"end\":61508,\"start\":61501},{\"end\":61724,\"start\":61719},{\"end\":61748,\"start\":61740},{\"end\":61761,\"start\":61754},{\"end\":61775,\"start\":61769},{\"end\":62029,\"start\":62020},{\"end\":62042,\"start\":62035},{\"end\":62052,\"start\":62047},{\"end\":62060,\"start\":62053},{\"end\":62293,\"start\":62290},{\"end\":62307,\"start\":62300},{\"end\":62319,\"start\":62312},{\"end\":62330,\"start\":62326},{\"end\":62346,\"start\":62345},{\"end\":62360,\"start\":62356},{\"end\":62668,\"start\":62660},{\"end\":62680,\"start\":62673},{\"end\":62693,\"start\":62686},{\"end\":62705,\"start\":62698},{\"end\":63037,\"start\":63033},{\"end\":63049,\"start\":63045},{\"end\":63065,\"start\":63057},{\"end\":63080,\"start\":63071},{\"end\":63317,\"start\":63312},{\"end\":63333,\"start\":63325},{\"end\":63347,\"start\":63339},{\"end\":63359,\"start\":63354},{\"end\":63648,\"start\":63642},{\"end\":63659,\"start\":63655},{\"end\":63671,\"start\":63666},{\"end\":63683,\"start\":63677},{\"end\":63942,\"start\":63939},{\"end\":63958,\"start\":63950},{\"end\":63970,\"start\":63963},{\"end\":63981,\"start\":63975},{\"end\":63997,\"start\":63989},{\"end\":64280,\"start\":64272},{\"end\":64291,\"start\":64287},{\"end\":64306,\"start\":64299},{\"end\":64318,\"start\":64311},{\"end\":64327,\"start\":64324},{\"end\":64337,\"start\":64334},{\"end\":64580,\"start\":64573},{\"end\":64592,\"start\":64586},{\"end\":64601,\"start\":64597},{\"end\":64611,\"start\":64606},{\"end\":64620,\"start\":64617},{\"end\":64630,\"start\":64625},{\"end\":64791,\"start\":64786},{\"end\":64801,\"start\":64797},{\"end\":64813,\"start\":64807},{\"end\":64825,\"start\":64818},{\"end\":64839,\"start\":64832}]", "bib_author_last_name": "[{\"end\":49423,\"start\":49416},{\"end\":49438,\"start\":49434},{\"end\":49448,\"start\":49444},{\"end\":49457,\"start\":49450},{\"end\":49676,\"start\":49666},{\"end\":49691,\"start\":49687},{\"end\":49695,\"start\":49693},{\"end\":49973,\"start\":49969},{\"end\":49985,\"start\":49982},{\"end\":49993,\"start\":49991},{\"end\":50002,\"start\":50000},{\"end\":50012,\"start\":50008},{\"end\":50020,\"start\":50017},{\"end\":50347,\"start\":50343},{\"end\":50354,\"start\":50352},{\"end\":50368,\"start\":50364},{\"end\":50379,\"start\":50374},{\"end\":50390,\"start\":50386},{\"end\":50676,\"start\":50672},{\"end\":50693,\"start\":50684},{\"end\":50711,\"start\":50704},{\"end\":50728,\"start\":50722},{\"end\":51001,\"start\":50997},{\"end\":51012,\"start\":51007},{\"end\":51028,\"start\":51023},{\"end\":51038,\"start\":51035},{\"end\":51308,\"start\":51304},{\"end\":51316,\"start\":51314},{\"end\":51329,\"start\":51327},{\"end\":51340,\"start\":51337},{\"end\":51355,\"start\":51351},{\"end\":51369,\"start\":51364},{\"end\":51604,\"start\":51600},{\"end\":51617,\"start\":51614},{\"end\":51631,\"start\":51626},{\"end\":51644,\"start\":51642},{\"end\":51653,\"start\":51650},{\"end\":51804,\"start\":51801},{\"end\":51818,\"start\":51813},{\"end\":51830,\"start\":51827},{\"end\":51842,\"start\":51838},{\"end\":51853,\"start\":51851},{\"end\":51868,\"start\":51865},{\"end\":52168,\"start\":52165},{\"end\":52183,\"start\":52177},{\"end\":52194,\"start\":52191},{\"end\":52207,\"start\":52204},{\"end\":52440,\"start\":52438},{\"end\":52453,\"start\":52449},{\"end\":52468,\"start\":52464},{\"end\":52483,\"start\":52477},{\"end\":52494,\"start\":52492},{\"end\":52507,\"start\":52505},{\"end\":52770,\"start\":52768},{\"end\":52781,\"start\":52777},{\"end\":52793,\"start\":52789},{\"end\":52801,\"start\":52799},{\"end\":52817,\"start\":52812},{\"end\":52828,\"start\":52824},{\"end\":53030,\"start\":53028},{\"end\":53041,\"start\":53037},{\"end\":53056,\"start\":53051},{\"end\":53069,\"start\":53066},{\"end\":53077,\"start\":53075},{\"end\":53092,\"start\":53088},{\"end\":53356,\"start\":53354},{\"end\":53371,\"start\":53366},{\"end\":53384,\"start\":53381},{\"end\":53399,\"start\":53395},{\"end\":53580,\"start\":53575},{\"end\":53933,\"start\":53928},{\"end\":53946,\"start\":53942},{\"end\":53960,\"start\":53957},{\"end\":53969,\"start\":53967},{\"end\":53979,\"start\":53976},{\"end\":53993,\"start\":53989},{\"end\":54005,\"start\":54003},{\"end\":54018,\"start\":54014},{\"end\":54039,\"start\":54034},{\"end\":54375,\"start\":54370},{\"end\":54384,\"start\":54382},{\"end\":54398,\"start\":54393},{\"end\":54411,\"start\":54406},{\"end\":54424,\"start\":54420},{\"end\":54435,\"start\":54432},{\"end\":54452,\"start\":54446},{\"end\":54704,\"start\":54702},{\"end\":54716,\"start\":54712},{\"end\":54729,\"start\":54727},{\"end\":54741,\"start\":54737},{\"end\":54754,\"start\":54750},{\"end\":54763,\"start\":54760},{\"end\":54962,\"start\":54957},{\"end\":54974,\"start\":54971},{\"end\":54986,\"start\":54982},{\"end\":55000,\"start\":54997},{\"end\":55009,\"start\":55006},{\"end\":55198,\"start\":55192},{\"end\":55214,\"start\":55206},{\"end\":55225,\"start\":55221},{\"end\":55238,\"start\":55233},{\"end\":55253,\"start\":55249},{\"end\":55268,\"start\":55263},{\"end\":55285,\"start\":55276},{\"end\":55293,\"start\":55290},{\"end\":55309,\"start\":55301},{\"end\":55588,\"start\":55583},{\"end\":55601,\"start\":55597},{\"end\":55824,\"start\":55820},{\"end\":55836,\"start\":55831},{\"end\":55845,\"start\":55843},{\"end\":55856,\"start\":55854},{\"end\":55866,\"start\":55863},{\"end\":55880,\"start\":55877},{\"end\":55892,\"start\":55890},{\"end\":56138,\"start\":56136},{\"end\":56148,\"start\":56145},{\"end\":56159,\"start\":56154},{\"end\":56169,\"start\":56165},{\"end\":56180,\"start\":56177},{\"end\":56396,\"start\":56394},{\"end\":56408,\"start\":56404},{\"end\":56418,\"start\":56415},{\"end\":56432,\"start\":56428},{\"end\":56443,\"start\":56440},{\"end\":56686,\"start\":56683},{\"end\":56703,\"start\":56697},{\"end\":56713,\"start\":56709},{\"end\":56895,\"start\":56891},{\"end\":56917,\"start\":56897},{\"end\":57044,\"start\":57032},{\"end\":57054,\"start\":57052},{\"end\":57069,\"start\":57062},{\"end\":57424,\"start\":57420},{\"end\":57435,\"start\":57431},{\"end\":57457,\"start\":57447},{\"end\":57474,\"start\":57465},{\"end\":57753,\"start\":57750},{\"end\":57765,\"start\":57761},{\"end\":57778,\"start\":57774},{\"end\":57790,\"start\":57785},{\"end\":57804,\"start\":57800},{\"end\":57815,\"start\":57811},{\"end\":57829,\"start\":57825},{\"end\":57839,\"start\":57835},{\"end\":58106,\"start\":58099},{\"end\":58128,\"start\":58123},{\"end\":58142,\"start\":58136},{\"end\":58154,\"start\":58151},{\"end\":58403,\"start\":58392},{\"end\":58417,\"start\":58411},{\"end\":58431,\"start\":58425},{\"end\":58449,\"start\":58443},{\"end\":58763,\"start\":58760},{\"end\":58778,\"start\":58773},{\"end\":58791,\"start\":58785},{\"end\":59033,\"start\":59029},{\"end\":59045,\"start\":59041},{\"end\":59060,\"start\":59056},{\"end\":59070,\"start\":59067},{\"end\":59086,\"start\":59078},{\"end\":59316,\"start\":59312},{\"end\":59331,\"start\":59327},{\"end\":59343,\"start\":59340},{\"end\":59356,\"start\":59352},{\"end\":59372,\"start\":59367},{\"end\":59546,\"start\":59542},{\"end\":59559,\"start\":59557},{\"end\":59570,\"start\":59566},{\"end\":59831,\"start\":59827},{\"end\":59843,\"start\":59840},{\"end\":59853,\"start\":59848},{\"end\":59866,\"start\":59864},{\"end\":59875,\"start\":59873},{\"end\":59890,\"start\":59886},{\"end\":60126,\"start\":60123},{\"end\":60138,\"start\":60133},{\"end\":60152,\"start\":60149},{\"end\":60161,\"start\":60159},{\"end\":60174,\"start\":60170},{\"end\":60185,\"start\":60182},{\"end\":60426,\"start\":60424},{\"end\":60438,\"start\":60434},{\"end\":60449,\"start\":60445},{\"end\":60462,\"start\":60460},{\"end\":60474,\"start\":60470},{\"end\":60488,\"start\":60484},{\"end\":60498,\"start\":60495},{\"end\":60747,\"start\":60745},{\"end\":60767,\"start\":60761},{\"end\":60782,\"start\":60777},{\"end\":60796,\"start\":60791},{\"end\":61076,\"start\":61073},{\"end\":61088,\"start\":61083},{\"end\":61097,\"start\":61095},{\"end\":61107,\"start\":61104},{\"end\":61120,\"start\":61115},{\"end\":61136,\"start\":61132},{\"end\":61146,\"start\":61143},{\"end\":61158,\"start\":61156},{\"end\":61468,\"start\":61465},{\"end\":61477,\"start\":61475},{\"end\":61489,\"start\":61484},{\"end\":61499,\"start\":61496},{\"end\":61511,\"start\":61509},{\"end\":61738,\"start\":61725},{\"end\":61752,\"start\":61749},{\"end\":61767,\"start\":61762},{\"end\":61781,\"start\":61776},{\"end\":61787,\"start\":61783},{\"end\":62033,\"start\":62030},{\"end\":62045,\"start\":62043},{\"end\":62066,\"start\":62061},{\"end\":62298,\"start\":62294},{\"end\":62310,\"start\":62308},{\"end\":62324,\"start\":62320},{\"end\":62343,\"start\":62331},{\"end\":62354,\"start\":62347},{\"end\":62369,\"start\":62361},{\"end\":62379,\"start\":62371},{\"end\":62671,\"start\":62669},{\"end\":62684,\"start\":62681},{\"end\":62696,\"start\":62694},{\"end\":62710,\"start\":62706},{\"end\":63043,\"start\":63038},{\"end\":63055,\"start\":63050},{\"end\":63069,\"start\":63066},{\"end\":63083,\"start\":63081},{\"end\":63323,\"start\":63318},{\"end\":63337,\"start\":63334},{\"end\":63352,\"start\":63348},{\"end\":63364,\"start\":63360},{\"end\":63653,\"start\":63649},{\"end\":63664,\"start\":63660},{\"end\":63675,\"start\":63672},{\"end\":63686,\"start\":63684},{\"end\":63948,\"start\":63943},{\"end\":63961,\"start\":63959},{\"end\":63973,\"start\":63971},{\"end\":63987,\"start\":63982},{\"end\":64000,\"start\":63998},{\"end\":64285,\"start\":64281},{\"end\":64297,\"start\":64292},{\"end\":64309,\"start\":64307},{\"end\":64322,\"start\":64319},{\"end\":64332,\"start\":64328},{\"end\":64340,\"start\":64338},{\"end\":64584,\"start\":64581},{\"end\":64595,\"start\":64593},{\"end\":64604,\"start\":64602},{\"end\":64615,\"start\":64612},{\"end\":64623,\"start\":64621},{\"end\":64635,\"start\":64631},{\"end\":64795,\"start\":64792},{\"end\":64805,\"start\":64802},{\"end\":64816,\"start\":64814},{\"end\":64830,\"start\":64826},{\"end\":64843,\"start\":64840}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":36809545},\"end\":49571,\"start\":49370},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":244119699},\"end\":49858,\"start\":49573},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":202539008},\"end\":50233,\"start\":49860},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":210932292},\"end\":50594,\"start\":50235},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211096730},\"end\":50911,\"start\":50596},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":227171952},\"end\":51214,\"start\":50913},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":20288249},\"end\":51564,\"start\":51216},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52825543},\"end\":51795,\"start\":51566},{\"attributes\":{\"id\":\"b8\"},\"end\":52061,\"start\":51797},{\"attributes\":{\"id\":\"b9\"},\"end\":52373,\"start\":52063},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":237420672},\"end\":52676,\"start\":52375},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211043589},\"end\":53017,\"start\":52678},{\"attributes\":{\"id\":\"b12\"},\"end\":53267,\"start\":53019},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2896685},\"end\":53568,\"start\":53269},{\"attributes\":{\"doi\":\"arXiv:2110.03455\",\"id\":\"b14\"},\"end\":53817,\"start\":53570},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235306079},\"end\":54289,\"start\":53819},{\"attributes\":{\"id\":\"b16\"},\"end\":54645,\"start\":54291},{\"attributes\":{\"id\":\"b17\"},\"end\":54911,\"start\":54647},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":199465898},\"end\":55149,\"start\":54913},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":216080787},\"end\":55517,\"start\":55151},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":58370896},\"end\":55738,\"start\":55519},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":238531397},\"end\":56083,\"start\":55740},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":174800708},\"end\":56326,\"start\":56085},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":202789109},\"end\":56599,\"start\":56328},{\"attributes\":{\"id\":\"b24\"},\"end\":56846,\"start\":56601},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":467086},\"end\":57024,\"start\":56848},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b26\"},\"end\":57315,\"start\":57026},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":236493246},\"end\":57670,\"start\":57317},{\"attributes\":{\"id\":\"b28\"},\"end\":58038,\"start\":57672},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16274986},\"end\":58303,\"start\":58040},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":227230870},\"end\":58636,\"start\":58305},{\"attributes\":{\"id\":\"b31\"},\"end\":58964,\"start\":58638},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220513062},\"end\":59247,\"start\":58966},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":197928230},\"end\":59534,\"start\":59249},{\"attributes\":{\"id\":\"b34\"},\"end\":59775,\"start\":59536},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":220347145},\"end\":60044,\"start\":59777},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":246828747},\"end\":60363,\"start\":60046},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":224814335},\"end\":60670,\"start\":60365},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6392154},\"end\":60969,\"start\":60672},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":235306149},\"end\":61400,\"start\":60971},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235792434},\"end\":61659,\"start\":61402},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":27308776},\"end\":61955,\"start\":61661},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":225053992},\"end\":62217,\"start\":61957},{\"attributes\":{\"id\":\"b43\"},\"end\":62568,\"start\":62219},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":231632459},\"end\":62974,\"start\":62570},{\"attributes\":{\"id\":\"b45\"},\"end\":63220,\"start\":62976},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":170079233},\"end\":63539,\"start\":63222},{\"attributes\":{\"id\":\"b47\"},\"end\":63849,\"start\":63541},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":197928297},\"end\":64192,\"start\":63851},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":219636286},\"end\":64516,\"start\":64194},{\"attributes\":{\"id\":\"b50\"},\"end\":64784,\"start\":64518},{\"attributes\":{\"id\":\"b51\"},\"end\":65104,\"start\":64786}]", "bib_title": "[{\"end\":49407,\"start\":49370},{\"end\":49656,\"start\":49573},{\"end\":49962,\"start\":49860},{\"end\":50337,\"start\":50235},{\"end\":50665,\"start\":50596},{\"end\":50992,\"start\":50913},{\"end\":51298,\"start\":51216},{\"end\":51592,\"start\":51566},{\"end\":52156,\"start\":52063},{\"end\":52433,\"start\":52375},{\"end\":52757,\"start\":52678},{\"end\":53343,\"start\":53269},{\"end\":53921,\"start\":53819},{\"end\":54947,\"start\":54913},{\"end\":55182,\"start\":55151},{\"end\":55574,\"start\":55519},{\"end\":55809,\"start\":55740},{\"end\":56126,\"start\":56085},{\"end\":56384,\"start\":56328},{\"end\":56882,\"start\":56848},{\"end\":57411,\"start\":57317},{\"end\":58090,\"start\":58040},{\"end\":58384,\"start\":58305},{\"end\":59018,\"start\":58966},{\"end\":59302,\"start\":59249},{\"end\":59819,\"start\":59777},{\"end\":60117,\"start\":60046},{\"end\":60414,\"start\":60365},{\"end\":60739,\"start\":60672},{\"end\":61062,\"start\":60971},{\"end\":61454,\"start\":61402},{\"end\":61717,\"start\":61661},{\"end\":62018,\"start\":61957},{\"end\":62658,\"start\":62570},{\"end\":63031,\"start\":62976},{\"end\":63310,\"start\":63222},{\"end\":63640,\"start\":63541},{\"end\":63937,\"start\":63851},{\"end\":64270,\"start\":64194}]", "bib_author": "[{\"end\":49425,\"start\":49409},{\"end\":49440,\"start\":49425},{\"end\":49450,\"start\":49440},{\"end\":49459,\"start\":49450},{\"end\":49678,\"start\":49658},{\"end\":49693,\"start\":49678},{\"end\":49697,\"start\":49693},{\"end\":49975,\"start\":49964},{\"end\":49987,\"start\":49975},{\"end\":49995,\"start\":49987},{\"end\":50004,\"start\":49995},{\"end\":50014,\"start\":50004},{\"end\":50022,\"start\":50014},{\"end\":50349,\"start\":50339},{\"end\":50356,\"start\":50349},{\"end\":50370,\"start\":50356},{\"end\":50381,\"start\":50370},{\"end\":50392,\"start\":50381},{\"end\":50678,\"start\":50667},{\"end\":50695,\"start\":50678},{\"end\":50713,\"start\":50695},{\"end\":50730,\"start\":50713},{\"end\":51003,\"start\":50994},{\"end\":51014,\"start\":51003},{\"end\":51030,\"start\":51014},{\"end\":51040,\"start\":51030},{\"end\":51310,\"start\":51300},{\"end\":51318,\"start\":51310},{\"end\":51331,\"start\":51318},{\"end\":51342,\"start\":51331},{\"end\":51357,\"start\":51342},{\"end\":51371,\"start\":51357},{\"end\":51606,\"start\":51594},{\"end\":51619,\"start\":51606},{\"end\":51633,\"start\":51619},{\"end\":51646,\"start\":51633},{\"end\":51655,\"start\":51646},{\"end\":51806,\"start\":51797},{\"end\":51820,\"start\":51806},{\"end\":51832,\"start\":51820},{\"end\":51844,\"start\":51832},{\"end\":51855,\"start\":51844},{\"end\":51870,\"start\":51855},{\"end\":52170,\"start\":52158},{\"end\":52185,\"start\":52170},{\"end\":52196,\"start\":52185},{\"end\":52209,\"start\":52196},{\"end\":52442,\"start\":52435},{\"end\":52455,\"start\":52442},{\"end\":52470,\"start\":52455},{\"end\":52485,\"start\":52470},{\"end\":52496,\"start\":52485},{\"end\":52509,\"start\":52496},{\"end\":52772,\"start\":52759},{\"end\":52783,\"start\":52772},{\"end\":52795,\"start\":52783},{\"end\":52803,\"start\":52795},{\"end\":52819,\"start\":52803},{\"end\":52830,\"start\":52819},{\"end\":53032,\"start\":53019},{\"end\":53043,\"start\":53032},{\"end\":53058,\"start\":53043},{\"end\":53071,\"start\":53058},{\"end\":53079,\"start\":53071},{\"end\":53094,\"start\":53079},{\"end\":53358,\"start\":53345},{\"end\":53373,\"start\":53358},{\"end\":53386,\"start\":53373},{\"end\":53401,\"start\":53386},{\"end\":53582,\"start\":53570},{\"end\":53935,\"start\":53923},{\"end\":53948,\"start\":53935},{\"end\":53962,\"start\":53948},{\"end\":53971,\"start\":53962},{\"end\":53981,\"start\":53971},{\"end\":53995,\"start\":53981},{\"end\":54007,\"start\":53995},{\"end\":54020,\"start\":54007},{\"end\":54041,\"start\":54020},{\"end\":54377,\"start\":54365},{\"end\":54386,\"start\":54377},{\"end\":54400,\"start\":54386},{\"end\":54413,\"start\":54400},{\"end\":54426,\"start\":54413},{\"end\":54437,\"start\":54426},{\"end\":54454,\"start\":54437},{\"end\":54706,\"start\":54696},{\"end\":54718,\"start\":54706},{\"end\":54731,\"start\":54718},{\"end\":54743,\"start\":54731},{\"end\":54756,\"start\":54743},{\"end\":54765,\"start\":54756},{\"end\":54964,\"start\":54949},{\"end\":54976,\"start\":54964},{\"end\":54988,\"start\":54976},{\"end\":55002,\"start\":54988},{\"end\":55011,\"start\":55002},{\"end\":55200,\"start\":55184},{\"end\":55216,\"start\":55200},{\"end\":55227,\"start\":55216},{\"end\":55240,\"start\":55227},{\"end\":55255,\"start\":55240},{\"end\":55270,\"start\":55255},{\"end\":55287,\"start\":55270},{\"end\":55295,\"start\":55287},{\"end\":55311,\"start\":55295},{\"end\":55590,\"start\":55576},{\"end\":55603,\"start\":55590},{\"end\":55826,\"start\":55811},{\"end\":55838,\"start\":55826},{\"end\":55847,\"start\":55838},{\"end\":55858,\"start\":55847},{\"end\":55868,\"start\":55858},{\"end\":55882,\"start\":55868},{\"end\":55894,\"start\":55882},{\"end\":56140,\"start\":56128},{\"end\":56150,\"start\":56140},{\"end\":56161,\"start\":56150},{\"end\":56171,\"start\":56161},{\"end\":56182,\"start\":56171},{\"end\":56398,\"start\":56386},{\"end\":56410,\"start\":56398},{\"end\":56420,\"start\":56410},{\"end\":56434,\"start\":56420},{\"end\":56445,\"start\":56434},{\"end\":56688,\"start\":56676},{\"end\":56705,\"start\":56688},{\"end\":56715,\"start\":56705},{\"end\":56897,\"start\":56884},{\"end\":56919,\"start\":56897},{\"end\":57046,\"start\":57026},{\"end\":57056,\"start\":57046},{\"end\":57071,\"start\":57056},{\"end\":57426,\"start\":57413},{\"end\":57437,\"start\":57426},{\"end\":57459,\"start\":57437},{\"end\":57476,\"start\":57459},{\"end\":57755,\"start\":57741},{\"end\":57767,\"start\":57755},{\"end\":57780,\"start\":57767},{\"end\":57792,\"start\":57780},{\"end\":57806,\"start\":57792},{\"end\":57817,\"start\":57806},{\"end\":57831,\"start\":57817},{\"end\":57841,\"start\":57831},{\"end\":58108,\"start\":58092},{\"end\":58130,\"start\":58108},{\"end\":58144,\"start\":58130},{\"end\":58156,\"start\":58144},{\"end\":58405,\"start\":58386},{\"end\":58419,\"start\":58405},{\"end\":58433,\"start\":58419},{\"end\":58451,\"start\":58433},{\"end\":58765,\"start\":58752},{\"end\":58780,\"start\":58765},{\"end\":58793,\"start\":58780},{\"end\":59035,\"start\":59020},{\"end\":59047,\"start\":59035},{\"end\":59062,\"start\":59047},{\"end\":59072,\"start\":59062},{\"end\":59088,\"start\":59072},{\"end\":59318,\"start\":59304},{\"end\":59333,\"start\":59318},{\"end\":59345,\"start\":59333},{\"end\":59358,\"start\":59345},{\"end\":59374,\"start\":59358},{\"end\":59548,\"start\":59536},{\"end\":59561,\"start\":59548},{\"end\":59572,\"start\":59561},{\"end\":59833,\"start\":59821},{\"end\":59845,\"start\":59833},{\"end\":59855,\"start\":59845},{\"end\":59868,\"start\":59855},{\"end\":59877,\"start\":59868},{\"end\":59892,\"start\":59877},{\"end\":60128,\"start\":60119},{\"end\":60140,\"start\":60128},{\"end\":60154,\"start\":60140},{\"end\":60163,\"start\":60154},{\"end\":60176,\"start\":60163},{\"end\":60187,\"start\":60176},{\"end\":60428,\"start\":60416},{\"end\":60440,\"start\":60428},{\"end\":60451,\"start\":60440},{\"end\":60464,\"start\":60451},{\"end\":60476,\"start\":60464},{\"end\":60490,\"start\":60476},{\"end\":60500,\"start\":60490},{\"end\":60749,\"start\":60741},{\"end\":60769,\"start\":60749},{\"end\":60784,\"start\":60769},{\"end\":60798,\"start\":60784},{\"end\":61078,\"start\":61064},{\"end\":61090,\"start\":61078},{\"end\":61099,\"start\":61090},{\"end\":61109,\"start\":61099},{\"end\":61122,\"start\":61109},{\"end\":61138,\"start\":61122},{\"end\":61148,\"start\":61138},{\"end\":61160,\"start\":61148},{\"end\":61470,\"start\":61456},{\"end\":61479,\"start\":61470},{\"end\":61491,\"start\":61479},{\"end\":61501,\"start\":61491},{\"end\":61513,\"start\":61501},{\"end\":61740,\"start\":61719},{\"end\":61754,\"start\":61740},{\"end\":61769,\"start\":61754},{\"end\":61783,\"start\":61769},{\"end\":61789,\"start\":61783},{\"end\":62035,\"start\":62020},{\"end\":62047,\"start\":62035},{\"end\":62068,\"start\":62047},{\"end\":62300,\"start\":62290},{\"end\":62312,\"start\":62300},{\"end\":62326,\"start\":62312},{\"end\":62345,\"start\":62326},{\"end\":62356,\"start\":62345},{\"end\":62371,\"start\":62356},{\"end\":62381,\"start\":62371},{\"end\":62673,\"start\":62660},{\"end\":62686,\"start\":62673},{\"end\":62698,\"start\":62686},{\"end\":62712,\"start\":62698},{\"end\":63045,\"start\":63033},{\"end\":63057,\"start\":63045},{\"end\":63071,\"start\":63057},{\"end\":63085,\"start\":63071},{\"end\":63325,\"start\":63312},{\"end\":63339,\"start\":63325},{\"end\":63354,\"start\":63339},{\"end\":63366,\"start\":63354},{\"end\":63655,\"start\":63642},{\"end\":63666,\"start\":63655},{\"end\":63677,\"start\":63666},{\"end\":63688,\"start\":63677},{\"end\":63950,\"start\":63939},{\"end\":63963,\"start\":63950},{\"end\":63975,\"start\":63963},{\"end\":63989,\"start\":63975},{\"end\":64002,\"start\":63989},{\"end\":64287,\"start\":64272},{\"end\":64299,\"start\":64287},{\"end\":64311,\"start\":64299},{\"end\":64324,\"start\":64311},{\"end\":64334,\"start\":64324},{\"end\":64342,\"start\":64334},{\"end\":64586,\"start\":64573},{\"end\":64597,\"start\":64586},{\"end\":64606,\"start\":64597},{\"end\":64617,\"start\":64606},{\"end\":64625,\"start\":64617},{\"end\":64637,\"start\":64625},{\"end\":64797,\"start\":64786},{\"end\":64807,\"start\":64797},{\"end\":64818,\"start\":64807},{\"end\":64832,\"start\":64818},{\"end\":64845,\"start\":64832}]", "bib_venue": "[{\"end\":49462,\"start\":49459},{\"end\":49702,\"start\":49697},{\"end\":50029,\"start\":50022},{\"end\":50399,\"start\":50392},{\"end\":50740,\"start\":50730},{\"end\":51044,\"start\":51040},{\"end\":51378,\"start\":51371},{\"end\":51662,\"start\":51655},{\"end\":51919,\"start\":51870},{\"end\":52213,\"start\":52209},{\"end\":52513,\"start\":52509},{\"end\":52835,\"start\":52830},{\"end\":53132,\"start\":53094},{\"end\":53406,\"start\":53401},{\"end\":53673,\"start\":53598},{\"end\":54045,\"start\":54041},{\"end\":54363,\"start\":54291},{\"end\":54694,\"start\":54647},{\"end\":55016,\"start\":55011},{\"end\":55315,\"start\":55311},{\"end\":55611,\"start\":55603},{\"end\":55898,\"start\":55894},{\"end\":56192,\"start\":56182},{\"end\":56449,\"start\":56445},{\"end\":56674,\"start\":56601},{\"end\":56923,\"start\":56919},{\"end\":57145,\"start\":57087},{\"end\":57480,\"start\":57476},{\"end\":57739,\"start\":57672},{\"end\":58159,\"start\":58156},{\"end\":58457,\"start\":58451},{\"end\":58750,\"start\":58638},{\"end\":59093,\"start\":59088},{\"end\":59379,\"start\":59374},{\"end\":59654,\"start\":59572},{\"end\":59897,\"start\":59892},{\"end\":60191,\"start\":60187},{\"end\":60505,\"start\":60500},{\"end\":60807,\"start\":60798},{\"end\":61167,\"start\":61160},{\"end\":61518,\"start\":61513},{\"end\":61794,\"start\":61789},{\"end\":62072,\"start\":62068},{\"end\":62288,\"start\":62219},{\"end\":62761,\"start\":62712},{\"end\":63090,\"start\":63085},{\"end\":63374,\"start\":63366},{\"end\":63692,\"start\":63688},{\"end\":64007,\"start\":64002},{\"end\":64346,\"start\":64342},{\"end\":64571,\"start\":64518},{\"end\":64939,\"start\":64845}]"}}}, "year": 2023, "month": 12, "day": 17}