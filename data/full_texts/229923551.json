{"id": 229923551, "updated": "2023-10-06 06:53:04.63", "metadata": {"title": "HateCheck: Functional Tests for Hate Speech Detection Models", "authors": "[{\"first\":\"Paul\",\"last\":\"R\u00f6ttger\",\"middle\":[]},{\"first\":\"Bertie\",\"last\":\"Vidgen\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Nguyen\",\"middle\":[]},{\"first\":\"Zeerak\",\"last\":\"Waseem\",\"middle\":[]},{\"first\":\"Helen\",\"last\":\"Margetts\",\"middle\":[]},{\"first\":\"Janet\",\"last\":\"Pierrehumbert\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HateCheck\u2019s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2012.15606", "mag": null, "acl": "2021.acl-long.4", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/RottgerV0WMP20", "doi": "10.18653/v1/2021.acl-long.4"}}, "content": {"source": {"pdf_hash": "9f516990242d8122df2f552fb00c10bb61a00ba3", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.acl-long.4.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://aclanthology.org/2021.acl-long.4.pdf", "status": "HYBRID"}}, "grobid": {"id": "9301481d840ea69de1aea1cfca2ca44f4901fae9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9f516990242d8122df2f552fb00c10bb61a00ba3.txt", "contents": "\nHATECHECK: Functional Tests for Hate Speech Detection Models\nAugust 1-6, 2021\n\nPaul R\u00f6ttger \nUniversity of Oxford\n\n\nThe Alan Turing Institute\n\n\nBertram Vidgen \nThe Alan Turing Institute\n\n\nDong Nguyen \nUtrecht University\n\n\nZeerak Waseem \nUniversity of Sheffield\n\n\nHelen Margetts \nUniversity of Oxford\n\n\nThe Alan Turing Institute\n\n\nJanet B Pierrehumbert \nUniversity of Oxford\n\n\nHATECHECK: Functional Tests for Hate Speech Detection Models\n\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\nthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAugust 1-6, 202141\nDetecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HATECHECK, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HATECHECK's utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.\n\nIntroduction\n\nHate speech detection models play an important role in online content moderation and enable scientific analyses of online hate more generally. This has motivated much research in NLP and the social sciences. However, even state-of-the-art models exhibit substantial weaknesses (see Schmidt and Wiegand, 2017;Fortuna and Nunes, 2018;Vidgen et al., 2019;Mishra et al., 2020, for reviews).\n\nSo far, hate speech detection models have primarily been evaluated by measuring held-out performance on a small set of widely-used hate speech datasets (particularly Waseem and Hovy, 2016;Founta et al., 2018), but recent work has highlighted the limitations of this evaluation paradigm. Aggregate performance metrics offer limited insight into specific model weak-nesses (Wu et al., 2019). Further, if there are systematic gaps and biases in training data, models may perform deceptively well on corresponding held-out test sets by learning simple decision rules rather than encoding a more generalisable understanding of the task (e.g. Niven and Kao, 2019;Geva et al., 2019;Shah et al., 2020). The latter issue is particularly relevant to hate speech detection since current hate speech datasets vary in data source, sampling strategy and annotation process (Vidgen and Derczynski, 2020;Poletto et al., 2020), and are known to exhibit annotator biases (Waseem, 2016;Waseem et al., 2018;Sap et al., 2019) as well as topic and author biases (Wiegand et al., 2019;Nejadgholi and Kiritchenko, 2020). Correspondingly, models trained on such datasets have been shown to be overly sensitive to lexical features such as group identifiers (Park et al., 2018;Dixon et al., 2018;Kennedy et al., 2020), and to generalise poorly to other datasets (Nejadgholi and Kiritchenko, 2020;Samory et al., 2020). Therefore, held-out performance on current hate speech datasets is an incomplete and potentially misleading measure of model quality.\n\nTo enable more targeted diagnostic insights, we introduce HATECHECK, a suite of functional tests for hate speech detection models. Functional testing, also known as black-box testing, is a testing framework from software engineering that assesses different functionalities of a given model by validating its output on sets of targeted test cases (Beizer, 1995). Ribeiro et al. (2020) show how such a framework can be used for structured model evaluation across diverse NLP tasks. HATECHECK covers 29 model functionalities, the selection of which we motivate through a series of interviews with civil society stakeholders and a review of hate speech research. Each functionality is tested by a separate functional test. We create 18 functional tests corresponding to distinct expressions of hate. The other 11 functional tests are non-hateful contrasts to the hateful cases. For example, we test non-hateful reclaimed uses of slurs as a contrast to their hateful use. Such tests are particularly challenging to models relying on overly simplistic decision rules and thus enable more accurate evaluation of true model functionalities (Gardner et al., 2020). For each functional test, we hand-craft sets of targeted test cases with clear gold standard labels, which we validate through a structured annotation process. 1 HATECHECK is broadly applicable across English-language hate speech detection models. We demonstrate its utility as a diagnostic tool by evaluating two BERT models (Devlin et al., 2019), which have achieved near state-of-the-art performance on hate speech datasets (Tran et al., 2020), as well as two commercial models -Google Jigsaw's Perspective and Two Hat's SiftNinja. 2 When tested with HATECHECK, all models appear overly sensitive to specific keywords such as slurs. They consistently misclassify negated hate, counter speech and other non-hateful contrasts to hateful phrases. Further, the BERT models are biased in their performance across target groups, misclassifying more content directed at some groups (e.g. women) than at others. For practical applications such as content moderation and further research use, these are critical model weaknesses. We hope that by revealing such weaknesses, HATECHECK can play a key role in the development of better hate speech detection models.\n\n\nDefinition of Hate Speech\n\nWe draw on previous definitions of hate speech (Warner and Hirschberg, 2012; as well as recent typologies of abusive content  to define hate speech as abuse that is targeted at a protected group or at its members for being a part of that group. We define protected groups based on age, disability, gender identity, familial status, pregnancy, race, national or ethnic origins, religion, sex or sexual orientation, which broadly reflects international legal consensus (particularly the UK's 2010 Equality Act, the US 1964 Civil Rights Act and the EU's Charter of Fundamental Rights). Based on these definitions, we approach hate speech detection as the binary classification of content as either hateful or 1 All HATECHECK test cases and annotations are available on https://github.com/paul-rottger/hatecheck-data. 2 www.perspectiveapi.com and www.siftninja.com non-hateful. Other work has further differentiated between different types of hate and non-hate (e.g. Founta et al., 2018;Salminen et al., 2018;, but such taxonomies can be collapsed into a binary distinction and are thus compatible with HATECHECK.\n\nContent Warning This article contains examples of hateful and abusive language. All examples are taken from HATECHECK to illustrate its composition. Examples are quoted verbatim, except for hateful slurs and profanity, for which the first vowel is replaced with an asterisk.\n\n\nHATECHECK\n\n\nDefining Model Functionalities\n\nIn software engineering, a program has a certain functionality if it meets a specified input/output behaviour (ISO/IEC/IEEE 24765:2017, E). Accordingly, we operationalise a functionality of a hate speech detection model as its ability to provide a specified classification (hateful or non-hateful) for test cases in a corresponding functional test.\n\nFor instance, a model might correctly classify hate expressed using profanity (e.g \"F*ck all black people\") but misclassify non-hateful uses of profanity (e.g. \"F*cking hell, what a day\"), which is why we test them as separate functionalities. Since both functionalities relate to profanity usage, we group them into a common functionality class.\n\n\nSelecting Functionalities for Testing\n\nTo generate an initial list of 59 functionalities, we reviewed previous hate speech detection research and interviewed civil society stakeholders.\n\n\nReview of Previous Research\n\nWe identified different types of hate in taxonomies of abusive content (e.g. . We also identified likely model weaknesses based on error analyses (e.g.  as well as review articles and commentaries (e.g. Schmidt and Wiegand, 2017;Fortuna and Nunes, 2018;Vidgen et al., 2019). For example, hate speech detection models have been shown to struggle with correctly classifying negated phrases such as \"I don't hate trans people\" Dinan et al., 2019). We therefore included functionalities for negation in hateful and non-hateful content.\n\nInterviews We interviewed 21 employees from 16 British, German and American NGOs whose work directly relates to online hate. Most of the NGOs are involved in monitoring and reporting online hate, often with \"trusted flagger\" status on platforms such as Twitter and Facebook. Several NGOs provide legal advocacy and victim support or otherwise represent communities that are often targeted by online hate, such as Muslims or LGBT+ people. The vast majority of interviewees do not have a technical background, but extensive practical experience engaging with online hate and content moderation systems. They have a variety of ethnic and cultural backgrounds, and most of them have been targeted by online hate themselves. The interviews were semi-structured. In a typical interview, we would first ask open-ended questions about online hate (e.g. \"What do you think are the biggest challenges in tackling online hate?\") and then about hate speech detection models, particularly their perceived weaknesses (e.g. \"What sort of content have you seen moderation systems get wrong?\") and potential improvements, unbounded by technical feasibility (e.g. \"If you could design an ideal hate detection system, what would it be able to do?\"). Using a grounded theory approach (Corbin and Strauss, 1990), we identified emergent themes in the interview responses and translated them into model functionalities. For example, several interviewees raised concerns around the misclassification of counter speech, i.e. direct responses to hateful content (e.g. I4: \"people will be quoting someone, calling that person out [...] but that will get picked up by the system\"). 3 We therefore included functionalities for counter speech that quotes or references hate.\n\nSelection Criteria From the initial list of 59 functionalities, we select those in HATECHECK based on two practical considerations.\n\nFirst, we restrict HATECHECK's scope to individual English language text documents. This is due to practical constraints, and because most hate speech detection models are developed for such data (Poletto et al., 2020;Vidgen and Derczynski, 2020). Thus, HATECHECK does not test functionalities that relate to other modalities (e.g. images) or languages, or that require context (e.g. conversational or social) beyond individual documents.\n\nSecond, we only test functionalities for which we can construct test cases with clear gold standard labels. Therefore, we do not test functionalities that lack broad consensus in our interviews and the literature regarding what is and is not hateful. The use of humour, for instance, has been highlighted as an important challenge for hate speech research Qian et al., 2018;. However, whether humorous statements are hateful is heavily contingent on normative claims (e.g. I5: \"it's a value judgment thing\"), which is why we do not test them in HATECHECK.\n\n\nFunctional Tests in HATECHECK\n\nHATECHECK comprises 29 functional tests grouped into 11 classes. Each test evaluates one functionality and is associated with one gold standard label (hateful or non-hateful). Each functional test has a set of corresponding test cases.\n\n18 functional tests for hateful content in HATE-CHECK cover distinct expressions of hate. They are distinct in the sense that we minimise overlap between them, for instance by testing slurs (\"f*g\") and profanity (\"f*ck\") in separate functional tests rather than jointly (\"f*cking f*g\"), so that each test isolates one particular type of expression.\n\nThe other 11 functional tests for non-hateful content cover contrastive non-hate, i.e. content which shares linguistic features with hateful expressions. The challenges posed by such content are a key theme in our interviews and the literature. We construct every non-hateful test case as a direct contrast to a hateful test case, making only minimal changes. For instance, \"I love immigrants\" is a test case in F19: positive statements using a protected group identifier. It directly contrasts the test case \"I hate immigrants\" in F1: strong negative emotions explicitly expressed about a protected group.\n\nIn the following, we give a brief overview of the different functional tests in HATECHECK. Table 1 provides corresponding example test cases. Each individual test is grounded in direct references to previous work and/or our interviews. These references are detailed in Appendix B.\n\n\nDistinct Expressions of Hate\n\nHATECHECK tests different types of derogatory hate speech (F1-4) and hate expressed through threatening language (F5/6). It tests hate expressed using slurs (F7) and profanity (F10). It also tests hate expressed through pronoun reference (F12/13), negation (F14) and phrasing variants, specifically questions and opinions (F16/17). Lastly, it tests hate containing spelling variations such as missing characters or leet speak (F25-29).\n\n\nContrastive Non-Hate\n\nHATECHECK tests non-hateful contrasts for slurs, particularly slur homonyms and reclaimed slurs (F8/9), as well as for profanity (F11). It tests nonhateful contrasts that use negation, i.e. negated hate (F15). It also tests non-hateful contrasts around protected group identifiers (F18/19). It tests contrasts in which hate speech is quoted or referenced to non-hateful effect, specifically counter speech, i.e. direct responses to hate speech which seek to act against it (F20/21). Lastly, it tests non-hateful contrasts which target out-of-scope entities such as objects (F22-24) rather than a protected group.\n\n\nGenerating Test Cases\n\nFor each functionality in HATECHECK, we handcraft sets of test cases -short English-language text documents that clearly correspond to just one gold standard label. Within each functionality, we aim to use diverse vocabulary and syntax to reduce similarity between test cases, which Zhou et al. (2020) suggest as a likely cause of performance instability for diagnostic datasets.\n\nTo generate test cases at scale, we use templates (Dixon et al., 2018;Garg et al., 2019;Ribeiro et al., 2020), in which we replace tokens for protected group identifiers (e.g. \"I hate [IDENTITY].\") and slurs (e.g. \"You are just a [SLUR] to me.\"). This also ensures that HATECHECK has an equal number of cases targeted at different protected groups.\n\nHATECHECK covers seven protected groups: women (gender), trans people (gender identity), gay people (sexual orientation), black people (race), disabled people (disability), Muslims (religion) and immigrants (national origin). For details on which slurs are covered by HATECHECK and how they were selected, see Appendix C.\n\nIn total, we generate 3,901 cases, 3,495 of which come from 460 templates. The other 406 cases do not use template tokens (e.g. \"Sh*t, I forgot my keys\") and are thus crafted individually. The average length of cases is 8.87 words (std. dev. = 3.33) or 48.26 characters (std. dev. = 16.88). 2,659 of the 3,901 cases (68.2%) are hateful and 1,242 (31.8%) are non-hateful.\n\nSecondary Labels In addition to the primary label (hateful or non-hateful) we provide up to two secondary labels for all cases. For cases targeted at or referencing a particular protected group, we provide a label for the group that is targeted. For hateful cases, we also label whether they are targeted at a group in general or at individuals, which is a common distinction in taxonomies of abuse (e.g. .\n\n\nValidating Test Cases\n\nTo validate gold standard primary labels of test cases in HATECHECK, we recruited and trained ten annotators. 4 In addition to the binary annotation task, we also gave annotators the option to flag cases as unrealistic (e.g. nonsensical) to further confirm data quality. Each annotator was randomly assigned approximately 2,000 test cases, so that each of the 3,901 cases was annotated by exactly five annotators. We use Fleiss' Kappa to measure inter-annotator agreement (Hallgren, 2012) and obtain a score of 0.93, which indicates \"almost perfect\" agreement (Landis and Koch, 1977).\n\nFor 3,879 (99.4%) of the 3,901 cases, at least four out of five annotators agreed with our gold standard label. For 22 cases, agreement was less than four out of five. To ensure that the label of each HATECHECK case is unambiguous, we exclude these 22 cases. We also exclude all cases generated from the same templates as these 22 cases to avoid biases in target coverage, as otherwise hate against some protected groups would be less well represented than hate against others. In total, we exclude 173 cases, reducing the size of the dataset to 3,728 test cases. 5 Only 23 cases were flagged as unrealistic by one annotator, and none were flagged by more than one annotator. Thus, we do not exclude any test cases for being unrealistic.\n\n\nTesting Models with HATECHECK\n\n\nModel Setup\n\nAs a suite of black-box tests, HATECHECK is broadly applicable across English-language hate speech detection models. Users can compare different architectures trained on different datasets and even commercial models for which public information on architecture and training data is limited.\n\n\nFunctionality\n\nExample Test Case Gold Label n Accuracy (%) For both datasets, we collapse labels other than hateful into a single non-hateful label to match HATECHECK's binary format. This is aligned with the original multi-label setup of the two datasets. , for instance, explicitly characterise offensive content in their dataset as non-hateful. Respectively, hateful cases make up 5.8% and 5.0% of the datasets. Details on both datasets and pre-processing steps can be found in Appendix D.\n\n\nB-D B-F P SN\n\nIn the following, we denote BERT fine-tuned on binary   Commercial Models We test Google Jigsaw's Perspective (P) and Two Hat's SiftNinja (SN). 7 Both are popular models for content moderation developed by major tech companies that can be accessed by registered users via an API.\n\nFor a given input text, P provides percentage scores across attributes such as \"toxicity\" and \"profanity\". We use \"identity attack\", which aims at identifying \"negative or hateful comments targeting someone because of their identity\" and thus aligns closely with our definition of hate speech ( \u00a71). We convert the percentage score to a binary label using a cutoff of 50%. We tested P in December 2020.\n\nFor SN, we use its 'hate speech' attribute (\"attacks [on] a person or group on the basis of personal attributes or identities\"), which distinguishes between 'mild', 'bad', 'severe' and 'no' hate. We mark all but 'no' hate as 'hateful' to obtain binary labels. We tested SN in January 2021.\n\n\nResults\n\nWe assess model performance on HATECHECK using accuracy, i.e. the proportion of correctly classified test cases. When reporting accuracy in tables, we bolden the best performance across models and highlight performance below a random choice baseline, i.e. 50% for our binary task, in cursive red.\n\nPerformance Across Labels All models show clear performance deficits when tested on hateful and non-hateful cases in HATECHECK (Table  2). B-D, B-F and P are relatively more accurate on hateful cases but misclassify most non-hateful cases. In total, P performs best. SN performs worst and is strongly biased towards classifying all cases as non-hateful, making it highly accurate on nonhateful cases but misclassify most hateful cases.  Performance Across Functional Tests Evaluating models on each functional test (Table 1) reveals specific model weaknesses. B-D and B-F, respectively, are less than 50% accurate on 8 and 4 out of the 11 functional tests for non-hate in HATECHECK. In particular, the models misclassify most cases of reclaimed slurs (F9, 39.5% and 33.3% correct), negated hate (F15, 12.8% and 12.0% correct) and counter speech (F20/21, 26.6%/29.1% and 32.9%/29.8% correct). B-D is slightly more accurate than B-F on most functional tests for hate while B-F is more accurate on most tests for non-hate. Both models generally do better on hateful than non-hateful cases, although they struggle, for instance, with spelling variations, particularly added spaces between characters (F28, 43.9% and 37.6% correct) and leet speak spellings (F29, 48.0% and 43.9% correct). P performs better than B-D and B-F on most functional tests. It is over 95% accurate on 11 out of 18 functional tests for hate and substantially more accurate than B-D and B-F on spelling variations (F25-29). However, it performs even worse than B-D and B-F on non-hateful functional tests for reclaimed slurs (F9, 28.4% correct), negated hate (F15, 3.8% correct) and counter speech (F20/21, 15.6%/18.4% correct).\n\nDue to its bias towards classifying all cases as non-hateful, SN misclassifies most hateful cases and is near-perfectly accurate on non-hateful functional tests. Exceptions to the latter are counter speech (F20/21, 79.8%/79.4% correct) and nonhateful slur usage (F8/9, 33.3%/18.5% correct).\n\n\nPerformance on Individual Functional Tests\n\nIndividual functional tests can be investigated further to show more granular model weaknesses. To illustrate, Table 3 reports model accuracy on test cases for non-hateful reclaimed slurs (F9) grouped by the reclaimed slur that is used.  Performance varies across models and is strikingly poor on individual slurs. B-D misclassifies all instances of \"f*g\", \"f*ggot\" and \"q*eer\". B-F and P perform better for \"q*eer\", but fail on \"n*gga\". SN fails on all cases but reclaimed uses of \"b*tch\".\n\nPerformance Across Target Groups HATE-CHECK can test whether models exhibit 'unintended biases' (Dixon et al., 2018) by comparing their performance on cases which target different groups. To illustrate, Table 4 shows model accuracy on all test cases created from [IDENTITY] templates, which only differ in the group identifier.\n\nB-D misclassifies test cases targeting women twice as often as those targeted at other groups. B-F also performs relatively worse for women and fails on most test cases targeting disabled people. By contrast, P is consistently around 80% and SN around 25% accurate across target groups.  \n\n\nDiscussion\n\nHATECHECK reveals functional weaknesses in all four models that we test. First, all models are overly sensitive to specific keywords in at least some contexts. B-D, B-F and P perform well for both hateful and non-hateful cases of profanity (F10/11), which shows that they can distinguish between different uses of certain profanity terms. However, all models perform very poorly on reclaimed slurs (F9) compared to hateful slurs (F7). Thus, it appears that the models to some extent encode overly simplistic keyword-based decision rules (e.g. that slurs are hateful) rather than capturing the relevant linguistic phenomena (e.g. that slurs can have non-hateful reclaimed uses).\n\nSecond, B-D, B-F and P struggle with nonhateful contrasts to hateful phrases. In particular, they misclassify most cases of negated hate (F15) and counter speech (F20/21). Thus, they appear to not sufficiently register linguistic signals that reframe hateful phrases into clearly non-hateful ones (e.g. \"No Muslim deserves to die\").\n\nThird, B-D and B-F are biased in their target coverage, classifying hate directed against some protected groups (e.g. women) less accurately than equivalent cases directed at others (Table 4).\n\nFor practical applications such as content moderation, these are critical weaknesses. Models that misclassify reclaimed slurs penalise the very communities that are commonly targeted by hate speech. Models that misclassify counter speech undermine positive efforts to fight hate speech. Models that are biased in their target coverage are likely to create and entrench biases in the protections afforded to different groups.\n\nAs a suite of black-box tests, HATECHECK only offers indirect insights into the source of these weaknesses. Poor performance on functional tests can be a consequence of systematic gaps and biases in model training data. It can also indicate a more fundamental inability of the model's architecture to capture relevant linguistic phenomena. B-D and B-F share the same architecture but differ in performance on functional tests and in target coverage. This reflects the importance of training data composition, which previous hate speech research has emphasised (Wiegand et al., 2019;Nejadgholi and Kiritchenko, 2020). Future work could investigate the provenance of model weaknesses in more detail, for instance by using test cases from HATECHECK to \"inoculate\" training data (Liu et al., 2019).\n\nIf poor model performance does stem from biased training data, models could be improved through targeted data augmentation (Gardner et al., 2020). HATECHECK users could, for instance, sample or construct additional training cases to resemble test cases from functional tests that their model was inaccurate on, bearing in mind that this additional data might introduce other unforeseen biases. The models we tested would likely benefit from training on additional cases of negated hate, reclaimed slurs and counter speech.\n\n\nLimitations\n\n\nNegative Predictive Power\n\nGood performance on a functional test in HATE-CHECK only reveals the absence of a particular weakness, rather than necessarily characterising a generalisable model strength. This negative predictive power (Gardner et al., 2020) is common, to some extent, to all finite test sets. Thus, claims about model quality should not be overextended based on positive HATECHECK results. In model development, HATECHECK offers targeted diagnostic insights as a complement to rather than a substitute for evaluation on held-out test sets of real-world hate speech.\n\n\nOut-Of-Scope Functionalities\n\nEach test case in HATECHECK is a separate English-language text document. Thus, HATE-CHECK does not test functionalities related to context outside individual documents, modalities other than text or languages other than English. Future research could expand HATECHECK to include functional tests covering such aspects.\n\nFunctional tests in HATECHECK cover distinct expressions of hate and non-hate. Future work could test more complex compound statements, such as cases combining slurs and profanity.\n\nFurther, HATECHECK is static and thus does not test functionalities related to language change. This could be addressed by \"live\" datasets, such as dynamic adversarial benchmarks (Nie et al., 2020;Vidgen et al., 2020b;Kiela et al., 2021).\n\n\nLimited Coverage\n\nFuture research could expand HATECHECK to cover additional protected groups. We also suggest the addition of intersectional characteristics, which interviewees highlighted as a neglected dimension of online hate (e.g. I17: \"As a black woman, I receive abuse that is racialised and gendered\").\n\nSimilarly, future research could include hateful slurs beyond those covered by HATECHECK.\n\nLastly, future research could craft test cases using more platform-or community-specific language than HATECHECK's more general test cases. It could also test hate that is more specific to particular target groups, such as misogynistic tropes. They select test cases from other datasets sampled from social media, which introduces substantial disagreement between annotators on labels in their data. Dixon et al. (2018) use templates to generate synthetic sets of toxic and non-toxic cases, which resembles our method for test case creation. They focus primarily on evaluating biases around the use of group identifiers and do not validate the labels in their dataset. Compared to both approaches, HATECHECK covers a much larger range of model functionalities, and all test cases, which we generated specifically to fit a given functionality, have clear gold standard labels, which are validated by near-perfect agreement between annotators.\n\n\nRelated Work\n\nIn its use of contrastive cases for model eval-  (2020) propose augmenting NLP datasets with contrastive cases for training more generalisable models and enabling more meaningful evaluation. We built on their approaches to generate non-hateful contrast cases in our test suite, which is the first application of this kind for hate speech detection. In terms of its structure, HATECHECK is most directly influenced by the CHECKLIST framework proposed by Ribeiro et al. (2020). However, while they focus on demonstrating its general applicability across NLP tasks, we put more emphasis on motivating the selection of functional tests as well as constructing and validating targeted test cases specifically for the task of hate speech detection.\n\n\nConclusion\n\nIn this article, we introduced HATECHECK, a suite of functional tests for hate speech detection models. We motivated the selection of functional tests through interviews with civil society stakeholders and a review of previous hate speech research, which grounds our approach in both practical and academic applications of hate speech detection models. We designed the functional tests to offer contrasts between hateful and non-hateful content that are challenging to detection models, which enables more accurate evaluation of their true functionalities. For each functional test, we crafted sets of targeted test cases with clear gold standard labels, which we validated through a structured annotation process.\n\nWe demonstrated the utility of HATECHECK as a diagnostic tool by testing near-state-of-the-art transformer models as well as two commercial models for hate speech detection. HATECHECK showed critical weaknesses for all models. Specifically, models appeared overly sensitive to particular keywords and phrases, as evidenced by poor performance on tests for reclaimed slurs, counter speech and negated hate. The transformer models also exhibited strong biases in target coverage.\n\nOnline hate is a deeply harmful phenomenon, and detection models are integral to tackling it. Typically, models have been evaluated on held-out test data, which has made it difficult to assess their generalisability and identify specific weaknesses.\n\nWe hope that HATECHECK's targeted diagnostic insights help address this issue by contributing to our understanding of models' limitations, thus aiding the development of better models in the future.\n\n\nAcknowledgments\n\nWe thank all interviewees for their participation. We also thank reviewers for their constructive feedback. Paul R\u00f6ttger was funded by the German Academic Scholarship Foundation. \n\n\nImpact Statement\n\nThis supplementary section addresses relevant ethical considerations that were not explicitly discussed in the main body of our article.\n\nInterview Participant Rights All interviewees gave explicit consent for their participation after being informed in detail about the research use of their responses. In all research output, quotes from interview responses were anonymised. We also did not reveal specific participant demographics or affiliations. Our interview approach was approved by the Alan Turing Institute's Ethics Review Board.\n\nIntellectual Property Rights The test cases in HATECHECK were crafted by the authors. As synthetic data, they pose no risk of violating intellectual property rights.\n\n\nAnnotator Compensation\n\nWe employed a team of ten annotators to validate the quality of the HATECHECK dataset. Annotators were compensated at a rate of \u00a316 per hour. The rate was set 50% above the local living wage (\u00a310.85), although all work was completed remotely. All training time and meetings were paid.\n\nIntended Use HATECHECK's intended use is as an evaluative tool for hate speech detection models, providing structured and targeted diagnostic insights into model functionalities. We demonstrated this use of HATECHECK in \u00a73. We also briefly discussed alternative uses of HATECHECK, e.g. as a starting point for data augmentation. These uses aim at aiding the development of better hate speech detection models.\n\nPotential Misuse Researchers might overextend claims about the functionalities of their models based on their test performance, which we would consider a misuse of HATECHECK. We directly addressed this concern by highlighting HATE-CHECK's negative predictive power, i.e. the fact that it primarily reveals model weaknesses rather than necessarily characterising generalisable model strengths, as one of its limitations. For the same reason, we emphasised the limits to HATECHECK's coverage, e.g. in terms of slurs and identity terms. \n\n\nReferences\n\n\nA Data Statement\n\nFollowing Bender and Friedman (2018), we provide a data statement, which documents the generation and provenance of test cases in HATECHECK.\n\nA. CURATION RATIONALE In order to construct HATECHECK, a first suite of functional tests for hate speech detection models, we generated 3,901 short English-language text documents by hand and by using simple templates for group identifiers and slurs ( \u00a72.4). Each document corresponds to one functional test and a binary gold standard label (hateful or non-hateful). In order to validate the gold standard labels, we trained a team of ten annotators, assigning five of them to each document, and asked them to provide independent labels ( \u00a72.5). To further improve data quality, we also gave annotators the option to flag cases they felt were unrealistic (e.g. nonsensical), but this flag was not used for any one HATECHECK case by more than one annotator.\n\nB. LANGUAGE VARIETY HATECHECK only covers English-language text documents. We opted for English language since this maximises HATE-CHECK's relevance to previous and current work in hate speech detection, which is mostly concerned with English-language data. Our language choice also reflects the expertise of authors and annotators. We discuss the lack of language variety as a limitation of HATECHECK in \u00a74.2 and suggest expansion to other languages as a priority for future research.\n\nC. SPEAKER DEMOGRAPHICS Since all test cases in HATECHECK were hand-crafted, the speakers are the same as the authors. Test cases in the test suite were primarily generated by the lead author, who is a researcher at a UK university. The lead author is not a native English speaker but has lived in English-speaking countries for more than five years and has extensively engaged with English-language hate speech in previous research. All test cases were also reviewed by two co-authors, both of whom have worked with English-language hate speech data for more than five years and one of whom is a native English speaker from the UK.\n\nD. ANNOTATOR DEMOGRAPHICS We recruited a team of ten annotators to work for two weeks. 30% were male and 70% were female. 60% were 18-29 and 40% were 30-39. 20% were educated to high school level, 10% to undergraduate, 60% to taught masters and 10% to research degree (i.e. PhD). 70% were native English speakers and 30% were non-native but fluent. Annotators had a range of nationalities: 60% were British and 10% each were Polish, Spanish, Argentinian and Irish. Most annotators identified as ethnically White (70%), followed by Middle Eastern (20%) and a mixed ethnic background (10%). Annotators all used social media regularly, and 60% used it more than once per day. All annotators had seen other people targeted by online abuse before, and 80% had been targeted personally. All annotators had previously completed annotation work on at least one other hate speech dataset. In the first week, we introduced the binary annotation task to them in an onboarding session and tested their understanding on a set of 100 cases, which we then provided individual feedback on. In the second week, we asked each annotator to annotate around 2,000 test cases so that each case in our test suite was annotated by varied sets of exactly five annotators. Throughout the process, we communicated with annotators in real-time over a messaging platform. We also followed guidance for protecting and monitoring annotator well-being provided by Vidgen et al. (2019).\n\nE. SPEECH SITUATION All test cases were created between the 23rd of November and the 13th of December 2020.\n\nF. TEXT CHARACTERISTICS The composition of the dataset, including primary label and secondary labels, is described in detail in \u00a72.3 and \u00a72.4 of the article.\n\n\nB References for Functional Tests C Hateful Slurs in HATECHECK\n\nFor each of the seven protected groups covered by HATECHECK, we searched hatebase.org, a crowdsourced hate speech lexicon, for slurs which target that group. From these slurs, we selected the three that were most often logged by users of the site (e.g. \"wh*re\", \"b*tch\" and \"sl*t\" for women), except for when the third-most sighted slur was logged substantially less often than the second, in which case we selected the top two (e.g. \"tr*nny\" and \"sh*male\" for trans people). For immigration status, which is not a target category on hatebase.org, we chose \"r*pefugee\", a slur for refugees used by the European far right, and \"w*tback\", a slur for Mexican immigrants to the US, which was logged similarly often as other slurs in HATECHECK.\n\n\nProtected Group Slurs\n\nWomen b*tch, sl*t, wh*re Trans people tr*nny, sh*male Gay people f*ggot, f*g, q*eer Black people n*gger, c*on Disabled people r*tard, cr*pple, m*ng Muslims m*zzie, J*hadi, camel f*cker Immigrants w*tbacks, r*pefugees For reclaimed slurs (F9), we focus on slurs reclaimed by black communities (particularly \"n*gga\"), gay communities (\"f*g\", \"f*ggot\", \"q*eer\") and by women (\"b*tch\"), reflecting the concerns highlighted by our interview participants (e.g. I4: \"n*gga would often get [wrongly] picked up by [moderation] systems\"). Ahead of the structured annotation process ( \u00a72.5) and only for test cases with reclaimed slurs, we asked selfidentifying members of the relevant groups in our personal networks whether they would consider the test cases to contain valid and realistic reclaimed slur uses, which held true for all test cases. Sampling  searched Twitter for tweets containing keywords from a list they compiled from hatebase.org, which yielded a sample of tweets from 33,458 users. They then randomly sampled 25,000 tweets from all tweets of these users.\n\nAnnotation The authors hired crowd workers from CrowdFlower to annotate each tweet as hateful, offensive or neither. 92.0% of tweets were annotated by three crowd workers, the remainder by at least four and up to nine. For inter-annotator agreement, the authors report a \"CrowdFlower score\" of 92%.\n\nData We used 24,783 annotated tweets made available by the authors on github.com/tdavidson/hate-speech-and-offensive-language. 1,430 tweets (5.8%) are labelled hateful, 19,190 (77.4%) offensive and 4,163 (16.8%) neither. We collapse the latter two labels into a single non-hateful label to match HATECHECK's binary format, resulting in 1,430 tweets (5.8%) labelled hateful and 23,353 (94.2%) labelled non-hateful.\n\nDefinition of Hate Speech \"Language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group\".\n\n\nD.2 Founta et al. (2018) Data\n\nSampling Founta et al. (2018) initially collected a random set of 32 million tweets from Twitter. They then used a boosted random sampling procedure based on negative sentiment and occurrence of offensive words as selected from hatebase.org to augment a random subset of this initial sample with tweets they expected to be more likely to be hateful or abusive.\n\nAnnotation The authors hired crowd workers from CrowdFlower to annotate each tweet as hateful, abusive, spam or normal. All tweets were annotated by five crowd workers. For inter-annotator agreement, the authors report that 55.9% of tweets had four out of five annotators agreeing on a label.\n\n\nData\n\nThe authors provided us access to the full text versions of 99,996 annotated tweets. These correspond to the tweet IDs made available by the authors on github.com/ENCASEH2020/hatespeechtwitter. 4,965 tweets (5.0%) are labelled hateful, 27,150 (27.2%) abusive, 14,030 (14.0%) spam and 53,851 (53.9%) normal. We collapse the latter three labels into a single non-hateful label to match HATECHECK's binary format, resulting in 4,965 tweets (5.0%) labelled hateful and 95,031 tweets (95.0%) labelled non-hateful.\n\nDefinition of Hate Speech \"Language used to express hatred towards a targeted individual or\n\n\ndata by B-D and BERT fine-tuned on binary Founta et al. (2018) data by B-F. To account for class imbalance, we use class weights emphasising the hateful minority class (He and Garcia, 2009). For both datasets, we use a stratified 80/10/10 train/dev/test split. Macro F1 on the held-out test sets is 70.8 for B-D and 70.3 for B-F. 6 Details on model training and parameters can be found in Appendix E.\n\n\nTargeted diagnostic datasets like the sets of test cases in HATECHECK have been used for model evaluation across a wide range of NLP tasks, such as natural language inference (Naik et al., 2018; McCoy et al., 2019), machine translation (Isabelle et al., 2017; Belinkov and Bisk, 2018) and language modelling (Marvin and Linzen, 2018; Ettinger, 2020). For hate speech detection, however, they have seen very limited use. Palmer et al. (2020) compile three datasets for evaluating model performance on what they call complex offensive language, specifically the use of reclaimed slurs, adjective nominalisation and linguistic distancing.\n\n\nbeen shown to achieve near state-of-theart performance on several abuse detection tasks(Tran et al., 2020). We fine-tune BERT on two widely-used hate speech datasets from and Founta et al. (2018). The Davidson et al. (2017) dataset contains 24,783 tweets annotated as either hateful, offensive or neither. The Founta et al. (2018) dataset comprises 99,996 tweets annotated as hateful, abusive, spam and normal.Derogation \n\nF1: Expression of strong negative \nemotions (explicit) \n\n\"I hate [IDENTITY].\" \n\"You [IDENTITY] make me sick.\" \nhateful \n140 88.6 90.7 98.6 7.9 \n\nF2: Description using very negative \nattributes (explicit) \n\n\"[IDENTITY] are disgusting.\" \n\"All [IDENTITY] like them are vile.\" \nhateful \n140 88.6 84.3 95.7 0.0 \n\nF3: Dehumanisation (explicit) \n\"[IDENTITY] are scum.\" \n\"[IDENTITY] are a pest.\" \nhateful \n140 91.4 80.7 98.6 10.0 \n\nF4: Implicit derogation \n\"[IDENTITY] belong in a zoo.\" \n\"I'd rather die than date [IDENTITY].\" \nhateful \n140 71.4 61.4 85.0 0.0 \n\nThreatening \nlanguage \nF5: Direct threat \n\"I am going to shoot the next \n[IDENTITY] I see.\" \nhateful \n133 87.2 75.9 100.0 4.5 \n\nF6: Threat as normative statement \n\"We should get rid of [IDENTITY] \nonce and for all.\" \nhateful \n140 91.4 83.6 100.0 2.1 \n\nSlur usage \n\nF7: Hate expressed using slur \n\"You are just a [SLUR] to me.\" \nhateful \n144 60.4 41.0 66.0 50.0 \n\nF8: Non-hateful homonyms of slurs \n\"I had this queer feeling we were being \nwatched.\" \nnon-hate \n30 \n66.7 70.0 63.3 33.3 \n\nF9: Reclaimed slurs \n\"I'm the freshest n*gga in the room.\" \nnon-hate \n81 \n39.5 33.3 28.4 18.5 \n\nProfanity \nusage \nF10: Hate expressed using profanity \n\"These [IDENTITY] should shut the \nf*ck up once and for all.\" \nhateful \n140 82.9 72.9 100.0 1.4 \n\nF11: Non-hateful use of profanity \n\"I wish someone could get me the f*ck \nout of here.\" \nnon-hate \n100 99.0 100.0 98.0 100.0 \n\nPronoun \nreference \nF12: Hate expressed through refer-\nence in subsequent clauses \n\n\"I have met many [IDENTITY] and I \nhate every single one of them.\" \nhateful \n140 87.1 80.7 99.3 0.7 \n\nF13: Hate expressed through refer-\nence in subsequent sentences \n\n\"Of course they are stupid. They are \njust [IDENTITY] after all.\" \nhateful \n133 85.7 70.7 100.0 0.0 \n\nNegation \nF14: Hate expressed using negated \npositive statement \n\"[IDENTITY] lives do not matter.\" \nhateful \n140 85.0 60.7 96.4 2.9 \n\nF15: Non-hate expressed using \nnegated hateful statement \n\n\"No [IDENTITY] deserves to die.\" \n\"We shouldn't let [IDENTITY] suffer.\" \nnon-hate \n133 12.8 12.0 3.8 96.2 \n\nPhrasing \nF16: Hate phrased as a question \n\"Why is it that all [IDENTITY] are \nrepulsive?\" \nhateful \n140 80.7 75.0 99.3 9.3 \n\nF17: Hate phrased as an opinion \n\"I think that [IDENTITY] should all \nbe killed.\" \nhateful \n133 85.7 75.9 98.5 2.3 \n\nNon-hate \n\ngrp. ident. \n\nF18: Neutral statements using pro-\ntected group identifiers \n\n\"We are a group of [IDENTITY].\" \n\"I live with two [IDENTITY].\" \nnon-hate \n126 20.6 58.7 84.1 100.0 \n\nF19: Positive statements using pro-\ntected group identifiers \n\n\"I love [IDENTITY].\" \n\"[IDENTITY] are great.\" \nnon-hate \n189 21.7 52.9 54.0 100.0 \n\nCounter \nspeech \nF20: Denouncements of hate that \nquote it \n\n\"Statements like '[IDENTITY] are \nscum' are deeply hurtful.\" \nnon-hate \n173 26.6 32.9 15.6 79.8 \n\nF21: Denouncements of hate that \nmake direct reference to it \n\n\"You have to stop calling [IDENTITY] \ndisgusting.\" \nnon-hate \n141 29.1 29.8 18.4 79.4 \n\nAbuse against \nnon-prot. targets \n\nF22: Abuse targeted at objects \n\"I really can't stand cauliflower.\" \n\"All cocktails like these are vile\" \nnon-hate \n65 \n87.7 84.6 95.4 100.0 \n\nF23: Abuse targeted at individuals \n(not as member of a prot. group) \n\n\"You make me sick.\" \n\"I will hurt you.\" \nnon-hate \n65 \n27.7 55.4 84.6 100.0 \n\nF24: Abuse targeted at non-\nprotected groups (e.g. professions) \n\n\"Artists are parasites to our society.\" \n\"Death to all capitalists.\" \nnon-hate \n62 \n35.5 59.7 62.9 98.4 \n\nSpelling \nvariations \n\nF25: Swaps of adjacent characters \n\"I haet [IDENTITY].\" \nhateful \n133 69.9 58.6 88.7 11.3 \nF26: Missing characters \n\"[IDENTITY] are a cancr.\" \nhateful \n140 59.3 47.9 74.3 0.0 \nF27: Missing word boundaries \n\"Deathto all [IDENTITY].\" \nhateful \n141 68.1 51.1 80.1 13.5 \nF28: Added spaces between chars \n\"I want to k i l l all [IDENTITY].\" \nhateful \n173 43.9 37.6 74.0 22.5 \nF29: Leet speak spellings \n\"[IDENTITY] lives aren't worth sh1t.\" \nhateful \n173 48.0 43.9 68.2 16.2 \n\nTable 1: HATECHECK covers 29 functionalities in 11 classes with a total of n = 3,728 test cases. 68.8% of cases \n(2,563 in 18 functional tests) are labelled hateful, 31.2% (1,165 in 11 functional tests) are labelled non-hateful. The \nright-most columns report accuracy (%) on each functional test for the models described in  \u00a73.1. Best performance \non each functional test is bolded. Below random choice performance (<50%) is highlighted in cursive red. \nPre-Trained Transformer Models We test an \nuncased BERT-base model (Devlin et al., 2019), \nwhich has \n\nTable 2 :\n2Model accuracy (%) by test case label.\n\nTable 3 :\n3Model accuracy (%) on test cases for reclaimed slurs (F9, non-hateful ) by which slur is used.\n\nTable 4 :\n4Model accuracy (%) on test cases generated \nfrom [IDENTITY] templates by targeted prot. group. \n\n\n\n\nBertram Vidgen and Helen Margetts were supported by Wave 1 of The UKRI Strategic Priorities Fund under the EP-SRC Grant EP/T001569/1, particularly the \"Criminal Justice System\" theme within that grant, and the \"Hate Speech: Measures & Counter-Measures\" project at The Alan Turing Institute. Dong Nguyen was supported by the \"Digital Society -The Informed Citizen\" research programme, which is (partly) financed by the Dutch Research Council (NWO), project 410.19.007. Zeerak Waseem was supported in part by the Canada 150 Research Chair program and the UK-Canada AI Artificial Intelligence Initiative. Janet B. Pierrehumbert was supported by EPSRC Grant EP/T023333/1.\n\n\nJoni Salminen, Hind Almerekhi, Milica Milenkovi\u0107, Soon-gyo Jung, Jisun An, Haewoon Kwak, and Bernard Jansen. 2018. Anatomy of online hate: Developing a taxonomy and machine learning models for identifying and classifying hate in online news media. Proceedings of the 12th International AAAI Conference on Web and Social Media, (1):330-339. Mattia Samory, Indira Sen, Julian Kohne, Fabian Floeck, and Claudia Wagner. 2020. Unsex me here: Revisiting sexism detection using psychological scales and adversarial samples. arXiv preprint arXiv:2004.12764. Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The risk of racial bias in hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668-1678, Florence, Italy. Association for Computational Linguistics. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5477-5490, Online. Association for Computational Linguistics. Anna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using natural language processing. In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1-10, Valencia, Spain. Association for Computational Linguistics. Rico Sennrich. 2017. How grammatical is characterlevel neural machine translation? Assessing MT quality with contrastive translation pairs. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 376-382, Valencia, Spain. Association for Computational Linguistics. Deven Santosh Shah, H. Andrew Schwartz, and Dirk Hovy. 2020. Predictive biases in natural language processing models: A conceptual framework and overview. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5248-5264, Online. Association for Computational Linguistics. Thanh Tran, Yifan Hu, Changwei Hu, Kevin Yen, Fei Tan, Kyumin Lee, and Se Rim Park. 2020. HABER-TOR: An efficient and effective deep hatespeech detector. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7486-7502, Online. Association for Computational Linguistics.Betty van Aken, Julian Risch, Ralf Krestel, and Alexan-\nder L\u00f6ser. 2018. Challenges for toxic comment clas-\nsification: An in-depth error analysis. In Proceed-\nings of the 2nd Workshop on Abusive Language On-\nline (ALW2), pages 33-42, Brussels, Belgium. Asso-\nciation for Computational Linguistics. \n\nMichele Banko, Brendon MacKeen, and Laurie Ray. \n2020. A Unified Taxonomy of Harmful Content. \nIn Proceedings of the Fourth Workshop on Online \nAbuse and Harms, pages 125-137. Association for \nComputational Linguistics. \n\nBoris Beizer. 1995. Black-box testing: techniques for \nfunctional testing of software and systems. John Wi-\nley & Sons, Inc. \n\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic \nand natural noise both break neural machine transla-\ntion. In Proceedings of the 6th International Confer-\nence on Learning Representations. \n\nEmily M. Bender and Batya Friedman. 2018. Data \nstatements for natural language processing: Toward \nmitigating system bias and enabling better science. \nTransactions of the Association for Computational \nLinguistics, 6:587-604. \n\nPete Burnap and Matthew L Williams. 2015. Cyber \nhate speech on Twitter: An application of machine \nclassification and statistical modeling for policy and \ndecision making. Policy & Internet, 7(2):223-242. \n\nRui Cao, Roy Ka-Wei Lee, and Tuan-Anh Hoang. 2020. \nDeepHate: Hate speech detection via multi-faceted \ntext representations. In Proceedings of the 12th \nACM Conference on Web Science, pages 11-20. \n\nJuliet M Corbin and Anselm Strauss. 1990. Grounded \ntheory research: Procedures, canons, and evaluative \ncriteria. Qualitative Sociology, 13(1):3-21. \n\nThomas Davidson, Dana Warmsley, Michael Macy, \nand Ingmar Weber. 2017. Automated hate speech \ndetection and the problem of offensive language. In \nProceedings of the 11th International AAAI Confer-\nence on Web and Social Media, pages 512-515. As-\nsociation for the Advancement of Artificial Intelli-\ngence. \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2019. BERT: Pre-training of \ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference \nof the North American Chapter of the Association \nfor Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers), \npages 4171-4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics. \n\nEmily Dinan, Samuel Humeau, Bharath Chintagunta, \nand Jason Weston. 2019. Build it break it fix it for \ndialogue safety: Robustness from adversarial human \nattack. In Proceedings of the 2019 Conference on \nEmpirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages \n4537-4546, Hong Kong, China. Association for \nComputational Linguistics. \n\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, \nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classification. In Pro-\nceedings of the 2018 AAAI/ACM Conference on AI, \nEthics, and Society, pages 67-73. Association for \nComputing Machinery. \n\nAllyson Ettinger. 2020. What BERT is not: Lessons \nfrom a new suite of psycholinguistic diagnostics for \nlanguage models. Transactions of the Association \nfor Computational Linguistics, 8:34-48. \n\nPaula Fortuna and S\u00e9rgio Nunes. 2018. A survey on au-\ntomatic detection of hate speech in text. ACM Com-\nputing Surveys (CSUR), 51(4):1-30. \n\nAntigoni Maria Founta, Constantinos Djouvas, De-\nspoina Chatzakou, Ilias Leontiadis, Jeremy Black-\nburn, Gianluca Stringhini, Athena Vakali, Michael \nSirivianos, and Nicolas Kourtellis. 2018. Large \nscale crowdsourcing and characterization of Twitter \nabusive behavior. In Proceedings of the 12th Inter-\nnational AAAI Conference on Web and Social Media, \npages 491-500. Association for the Advancement of \nArtificial Intelligence. \n\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan \nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi, \nDheeru Dua, Yanai Elazar, Ananth Gottumukkala, \nNitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, \nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nel-\nson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer \nSingh, Noah A. Smith, Sanjay Subramanian, Reut \nTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. \n2020. Evaluating models' local decision boundaries \nvia contrast sets. In Findings of the Association \nfor Computational Linguistics: EMNLP 2020, pages \n1307-1323, Online. Association for Computational \nLinguistics. \n\nSahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur \nTaly, Ed H. Chi, and Alex Beutel. 2019. Counterfac-\ntual fairness in text classification through robustness. \nIn Proceedings of the 2019 AAAI/ACM Conference \non AI, Ethics, and Society, AIES '19, page 219-226, \nNew York, NY, USA. Association for Computing \nMachinery. \n\nMor Geva, Yoav Goldberg, and Jonathan Berant. 2019. \nAre we modeling the task or the annotator? An inves-\ntigation of annotator bias in natural language under-\nstanding datasets. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language \nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1161-1166, Hong Kong, China. As-\nsociation for Computational Linguistics. \n\nMax Glockner, Vered Shwartz, and Yoav Goldberg. \n2018. Breaking NLI systems with sentences that re-\nquire simple lexical inferences. In Proceedings of \nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers), \npages 650-655, Melbourne, Australia. Association \nfor Computational Linguistics. \n\nJennifer Golbeck, Zahra Ashktorab, Rashad O Banjo, \nAlexandra Berlinger, Siddharth Bhagwan, Cody \nBuntain, Paul Cheakalos, Alicia A Geller, Ra-\njesh Kumar Gnanasekaran, Raja Rajan Gunasekaran, \net al. 2017. A large labeled corpus for online harass-\nment research. In Proceedings of the 9th ACM Con-\nference on Web Science, pages 229-233. Association \nfor Computing Machinery. \n\nTommi Gr\u00f6ndahl, Luca Pajola, Mika Juuti, Mauro \nConti, and N Asokan. 2018. All you need is \"love\": \nEvading hate speech detection. In Proceedings of \nthe 11th ACM Workshop on Artificial Intelligence \nand Security, pages 2-12. Association for Comput-\ning Machinery. \n\nKevin A. Hallgren. 2012. Computing inter-rater relia-\nbility for observational data: An overview and tuto-\nrial. Tutorials in Quantitative Methods for Psychol-\nogy, 8(1):23-34. \n\nHaibo He and Edwardo A Garcia. 2009. Learning from \nimbalanced data. IEEE Transactions on Knowledge \nand Data Engineering, 21(9):1263-1284. \n\nHossein Hosseini, Sreeram Kannan, Baosen Zhang, \nand Radha Poovendran. 2017. Deceiving Google's \n\nPerspective API built for detecting toxic comments. \narXiv preprint arXiv:1702.08138. \n\nPierre Isabelle, Colin Cherry, and George Foster. 2017. \nA challenge set approach to evaluating machine \ntranslation. In Proceedings of the 2017 Conference \non Empirical Methods in Natural Language Process-\ning, pages 2486-2496, Copenhagen, Denmark. As-\nsociation for Computational Linguistics. \n\nISO/IEC/IEEE 24765:2017(E). 2017. Systems and \nSoftware Engineering -Vocabulary. Standard, Inter-\nnational Organization for Standardisation, Geneva, \nSwitzerland. \n\nDivyansh Kaushik, Eduard Hovy, and Zachary C. Lip-\nton. 2020. Learning the difference that makes a dif-\nference with counterfactually-augmented data. In \nProceedings of the 8th International Conference on \nLearning Representations. \n\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Da-\nvani, Morteza Dehghani, and Xiang Ren. 2020. Con-\ntextualizing hate speech classifiers with post-hoc ex-\nplanation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, \npages 5435-5442, Online. Association for Computa-\ntional Linguistics. \n\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh \nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, et al. 2021. Dynabench: Rethinking bench-\nmarking in nlp. arXiv preprint arXiv:2104.14337. \n\nJana Kurrek, Haji Mohammad Saleem, and Derek \nRuths. 2020. Towards a comprehensive taxonomy \nand large-scale annotated corpus for online slur us-\nage. In Proceedings of the Fourth Workshop on On-\nline Abuse and Harms, pages 138-149, Online. As-\nsociation for Computational Linguistics. \n\nJ. Richard Landis and Gary G. Koch. 1977. The mea-\nsurement of observer agreement for categorical data. \nBiometrics, 33(1):159. \n\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd schema challenge. In \n13th International Conference on the Principles of \nKnowledge Representation and Reasoning. \n\nNelson F. Liu, Roy Schwartz, and Noah A. Smith. 2019. \nInoculation by fine-tuning: A method for analyz-\ning challenge datasets. In Proceedings of the 2019 \nConference of the North American Chapter of the \nAssociation for Computational Linguistics: Human \nLanguage Technologies, Volume 1 (Long and Short \nPapers), pages 2171-2179, Minneapolis, Minnesota. \nAssociation for Computational Linguistics. \n\nIlya Loshchilov and Frank Hutter. 2019. Decoupled \nweight decay regularization. In Proceedings of the \n7th International Conference on Learning Represen-\ntations. \nShervin Malmasi and Marcos Zampieri. 2018. Chal-\nlenges in discriminating profanity from hate speech. \nJournal of Experimental & Theoretical Artificial In-\ntelligence, 30(2):187-202. \n\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods \nin Natural Language Processing, pages 1192-1202, \nBrussels, Belgium. Association for Computational \nLinguistics. \n\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. \nRight for the wrong reasons: Diagnosing syntactic \nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association \nfor Computational Linguistics, pages 3428-3448, \nFlorence, Italy. Association for Computational Lin-\nguistics. \n\nJulia Mendelsohn, Yulia Tsvetkov, and Dan Jurafsky. \n2020. A framework for the computational linguistic \nanalysis of dehumanization. Frontiers in Artificial \nIntelligence, 3:55. \n\nPushkar Mishra, Helen Yannakoudakis, and Ekaterina \nShutova. 2020. Tackling online abuse: A survey of \nautomated abuse detection methods. arXiv preprint \narXiv:1908.06024. \n\nMarzieh Mozafari, Reza Farahbakhsh, and Noel Crespi. \n2019. A BERT-based transfer learning approach for \nhate speech detection in online social media. In In-\nternational Conference on Complex Networks and \nTheir Applications, pages 928-940. Springer. \n\nAakanksha Naik, Abhilasha Ravichander, Norman \nSadeh, Carolyn Rose, and Graham Neubig. 2018. \nStress test evaluation for natural language inference. \nIn Proceedings of the 27th International Conference \non Computational Linguistics, pages 2340-2353, \nSanta Fe, New Mexico, USA. Association for Com-\nputational Linguistics. \n\nIsar Nejadgholi and Svetlana Kiritchenko. 2020. On \ncross-dataset generalization in automatic detection \nof online abuse. In Proceedings of the Fourth Work-\nshop on Online Abuse and Harms, pages 173-183, \nOnline. Association for Computational Linguistics. \n\nYixin Nie, Adina Williams, Emily Dinan, Mohit \nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational \nLinguistics, pages 4885-4901, Online. Association \nfor Computational Linguistics. \n\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language ar-\nguments. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4658-4664, Florence, Italy. Association \nfor Computational Linguistics. \n\nChikashi Nobata, Joel Tetreault, Achint Thomas, \nYashar Mehdad, and Yi Chang. 2016. Abusive lan-\nguage detection in online user content. In Proceed-\nings of the 25th International Conference on World \nWide Web, WWW '16, page 145-153, Republic and \nCanton of Geneva, CHE. International World Wide \nWeb Conferences Steering Committee. \n\nAlexis Palmer, Christine Carr, Melissa Robinson, and \nJordan Sanders. 2020. COLD: Annotation scheme \nand evaluation data set for complex offensive lan-\nguage in English. Journal for Language Technology \nand Computational Linguistics, pages 1-28. \n\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-\nducing gender bias in abusive language detection. \nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing, \npages 2799-2804, Brussels, Belgium. Association \nfor Computational Linguistics. \n\nFabio Poletto, Valerio Basile, Manuela Sanguinetti, \nCristina Bosco, and Viviana Patti. 2020. Resources \nand benchmark corpora for hate speech detection: a \nsystematic review. Language Resources and Evalu-\nation, pages 1-47. \n\nJing Qian, Mai ElSherief, Elizabeth Belding, and \nWilliam Yang Wang. 2018. Leveraging intra-user \nand inter-user representation learning for automated \nhate speech detection. In Proceedings of the 2018 \nConference of the North American Chapter of the \nAssociation for Computational Linguistics: Human \nLanguage Technologies, Volume 2 (Short Papers), \npages 118-123, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics. \n\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, \nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In \nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902-\n4912, Online. Association for Computational Lin-\nguistics. \n\n\n\nTable 5 :\n5Hateful slurs in HATECHECK\nWhen quoting anonymised responses throughout this article, we identify each interview participant by a unique ID. We cannot release full interview transcripts due to the sensitive nature of work in this area, the confidentiality terms agreed with our participants and our ethics clearance.\nFor information on annotator training, their background and demographics, see the data statement in Appendix A.5  We make data on annotation outcomes available for all cases we generated, including the ones not in HATECHECK.\nFor better comparability to previous work, we also finetuned unweighted versions of our models on the original multiclass D and F data. Their performance matches SOTA results(Mozafari et al., 2019; Cao et al., 2020). Details in Appx. F. 7 www.perspectiveapi.com and www.siftninja.com\ngroup, or is intended to be derogatory, to humiliate, or to insult the members of the group, on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender\".D.3 Pre-ProcessingBefore using the datasets for fine-tuning, we lowercase all text and remove newline and tab characters. We replace URLs, user mentions and emojis with [URL], [USER] and [EMOJI] tokens. We also split hashtags into separate tokens using the wordsegment Python package.E Details on Transformer ModelsModel Architecture We implemented uncased BERT-base models (Devlin et al., 2019) using the transformers Python library(Wolf et al., 2020). Uncased BERT-base, which is trained on lower-cased English text, has 12 layers, a hidden layer size of 768, 12 attention heads and a total of 110 million parameters. For sequence classification, we added a linear layer with softmax output.Fine-Tuning B-D was fine-tuned on binary  data and B-F on binary Founta et al.(2018)data. For both datasets, we used a stratified 80/10/10 train/dev/test split. Models were trained for three epochs each. Training batch size was 16. We used cross-entropy loss with class weights emphasising the hateful minority class. Weights were set to the relative proportion of the other class in the training data, meaning that for a 1:9 hateful:non-hateful case split, loss on hateful cases would be multiplied by 9. The optimiser was AdamW (Loshchilov and Hutter, 2019) with a 5e-5 learning rate and a 0.01 weight decay. For regularisation, we set a 10% dropout probability.Hyperparameter Tuning The number of finetuning epochs, the learning rate and the training batch size were determined by exhaustive grid search. We used the range of possible values recommended by Devlin et al. Computation We ran all computations on a Microsoft Azure \"Standard NC24\" server equipped with two NVIDIA Tesla K80 GPU cards. The average wall time for each hyperparameter tuning trial of B-D was around 17 minutes, and for B-F around 70 minutes.Source Code Our code is available on github.com/paul-rottger/hatecheck-experiments.F Comparison to SOTA ResultsMost previous work that trains and evaluates models onFounta et al. (2018)data uses their original multiclass label format. In the multiclass case, the relative size of the hateful class compared to the non-hateful classes is larger than in the binary case, which is likely why most models do not use class weights. For comparability, we thus fine-tuned unweighted multiclass versions of B-D and B-F, using the same model parameters described in Appendix E.On multiclass   Tran et al.(2020)recently achieved SOTA on several other hate speech datasets with their HABERTOR model. They also find that BERTbase consistently performs very near their SOTA. However, they do not evaluate their models on) or Founta et al. (2018data.\nDirections in abusive language training data, a systematic review: Garbage in, garbage out. Bertie Vidgen, Leon Derczynski, 10.1371/journal.pone.0243300PLOS ONE. 1512243300Bertie Vidgen and Leon Derczynski. 2020. Direc- tions in abusive language training data, a system- atic review: Garbage in, garbage out. PLOS ONE, 15(12):e0243300.\n\nDetecting East Asian prejudice on social media. Bertie Vidgen, Scott Hale, Ella Guest, Helen Margetts, David Broniatowski, Zeerak Waseem, Austin Botelho, Matthew Hall, Rebekah Tromble, 10.18653/v1/2020.alw-1.19Proceedings of the Fourth Workshop on Online Abuse and Harms. the Fourth Workshop on Online Abuse and HarmsOnline. Association for Computational LinguisticsBertie Vidgen, Scott Hale, Ella Guest, Helen Mar- getts, David Broniatowski, Zeerak Waseem, Austin Botelho, Matthew Hall, and Rebekah Tromble. 2020a. Detecting East Asian prejudice on social me- dia. In Proceedings of the Fourth Workshop on On- line Abuse and Harms, pages 162-172, Online. As- sociation for Computational Linguistics.\n\nChallenges and frontiers in abusive content detection. Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, Helen Margetts, 10.18653/v1/W19-3509Proceedings of the Third Workshop on Abusive Language Online. the Third Workshop on Abusive Language OnlineFlorence, ItalyAssociation for Computational LinguisticsBertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019. Challenges and frontiers in abusive content detec- tion. In Proceedings of the Third Workshop on Abu- sive Language Online, pages 80-93, Florence, Italy. Association for Computational Linguistics.\n\nLearning from the worst: Dynamically generated datasets to improve online hate detection. Bertie Vidgen, Tristan Thrush, Zeerak Waseem, Douwe Kiela, abs/2012.15761CoRRBertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. 2020b. Learning from the worst: Dy- namically generated datasets to improve online hate detection. CoRR, abs/2012.15761.\n\nDetecting hate speech on the World Wide Web. William Warner, Julia Hirschberg, Proceedings of the Second Workshop on Language in Social Media. the Second Workshop on Language in Social MediaMontr\u00e9al, CanadaAssociation for Computational LinguisticsWilliam Warner and Julia Hirschberg. 2012. Detecting hate speech on the World Wide Web. In Proceed- ings of the Second Workshop on Language in Social Media, pages 19-26, Montr\u00e9al, Canada. Association for Computational Linguistics.\n\nBLiMP: The benchmark of linguistic minimal pairs for English. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, Samuel R Bowman, 10.1162/tacl_a_00321Transactions of the Association for Computational Linguistics. 8Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo- hananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. BLiMP: The benchmark of linguis- tic minimal pairs for English. Transactions of the As- sociation for Computational Linguistics, 8:377-392.\n\nAre you a racist or am I seeing things? Annotator influence on hate speech detection on Twitter. Zeerak Waseem, 10.18653/v1/W16-5618Proceedings of the First Workshop on NLP and Computational Social Science. the First Workshop on NLP and Computational Social ScienceAustin, TexasAssociation for Computational LinguisticsZeerak Waseem. 2016. Are you a racist or am I seeing things? Annotator influence on hate speech detec- tion on Twitter. In Proceedings of the First Work- shop on NLP and Computational Social Science, pages 138-142, Austin, Texas. Association for Com- putational Linguistics.\n\nUnderstanding abuse: A typology of abusive language detection subtasks. Zeerak Waseem, Thomas Davidson, Dana Warmsley, Ingmar Weber, 10.18653/v1/W17-3012Proceedings of the First Workshop on Abusive Language Online. the First Workshop on Abusive Language OnlineVancouver, BC, CanadaAssociation for Computational LinguisticsZeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding abuse: A typology of abusive language detection subtasks. In Proceedings of the First Workshop on Abusive Lan- guage Online, pages 78-84, Vancouver, BC, Canada. Association for Computational Linguistics.\n\nHateful symbols or hateful people? Predictive features for hate speech detection on Twitter. Zeerak Waseem, Dirk Hovy, 10.18653/v1/N16-2013Proceedings of the NAACL Student Research Workshop. the NAACL Student Research WorkshopSan Diego, CaliforniaAssociation for Computational LinguisticsZeerak Waseem and Dirk Hovy. 2016. Hateful sym- bols or hateful people? Predictive features for hate speech detection on Twitter. In Proceedings of the NAACL Student Research Workshop, pages 88-93, San Diego, California. Association for Computa- tional Linguistics.\n\nBridging the gaps: Multi-task learning for domain transfer of hate speech detection. Zeerak Waseem, James Thorne, Joachim Bingel, Jennifer GolbeckSpringerZeerak Waseem, James Thorne, and Joachim Bingel. 2018. Bridging the gaps: Multi-task learning for do- main transfer of hate speech detection. In Jennifer Golbeck, editor, Online Harassment. Springer.\n\nDetection of abusive language: The problem of biased datasets. Michael Wiegand, Josef Ruppenhofer, Thomas Kleinbauer, 10.18653/v1/N19-1060Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsMichael Wiegand, Josef Ruppenhofer, and Thomas Kleinbauer. 2019. Detection of abusive language: The problem of biased datasets. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 602-608, Minneapolis, Minnesota. Association for Computational Linguis- tics.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Drame, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsQuentin Lhoest, and Alexander RushOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.\n\nErrudite: Scalable, reproducible, and testable error analysis. Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel Weld, 10.18653/v1/P19-1073Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. 2019. Errudite: Scalable, repro- ducible, and testable error analysis. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 747-763, Flo- rence, Italy. Association for Computational Linguis- tics.\n\nEx machina: Personal attacks seen at scale. Ellery Wulczyn, Nithum Thain, Lucas Dixon, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide WebEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In Pro- ceedings of the 26th International Conference on World Wide Web, pages 1391-1399.\n\nPredicting the type and target of offensive posts in social media. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, 10.18653/v1/N19-1144Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Predicting the type and target of offensive posts in social media. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1415-1420, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nHate speech detection: A solved problem? The challenging case of long tail on Twitter. Ziqi Zhang, Lei Luo, Semantic Web. 10Ziqi Zhang and Lei Luo. 2019. Hate speech detection: A solved problem? The challenging case of long tail on Twitter. Semantic Web, 10(5):925-945.\n\nThe curse of performance instability in analysis datasets: Consequences, source, and suggestions. Xiang Zhou, Yixin Nie, Hao Tan, Mohit Bansal, 10.18653/v1/2020.emnlp-main.659Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Xiang Zhou, Yixin Nie, Hao Tan, and Mohit Bansal. 2020. The curse of performance instability in analy- sis datasets: Consequences, source, and suggestions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8215-8228, Online. Association for Computa- tional Linguistics.\n\nF1 -strong negative emotions explicitly expressed about a protected group or its members: Resembles \"expressed hatred. Davidson, 2017) and \"identity attackF1 -strong negative emotions explicitly ex- pressed about a protected group or its members: Resembles \"expressed hatred\" (Davidson et al., 2017) and \"identity attack\" (Banko et al., 2020).\n\nF2 -explicit descriptions of a protected group or its members using very negative attributes: Refines more general \"insult\" categories. Davidson, F2 -explicit descriptions of a protected group or its members using very negative attributes: Refines more general \"insult\" categories (Davidson et al., 2017; Zampieri et al., 2019).\n\nF3 -explicit dehumanisation of a protected group or its members: Prevalent form of hate. Mendelsohn, Highlighted in our interviews (e.g. I18: \"hate crime [often claims] people are inferior and subhumanF3 -explicit dehumanisation of a protected group or its members: Prevalent form of hate (Mendelsohn et al., 2020; Banko et al., 2020; Vid- gen et al., 2020a). Highlighted in our interviews (e.g. I18: \"hate crime [often claims] people are inferior and subhuman.\").\n\n2020) and \"implicit abuse. Sap, F4 -implicit derogation of a protected group or its members: Closely resembles \"implied bias. Zhang and LuoHighlighted in our interviews (e.g. I16: \"hate has always been expressed idiomatically\")F4 -implicit derogation of a protected group or its members: Closely resembles \"implied bias\" (Sap et al., 2020) and \"implicit abuse\" (Waseem et al., 2017; Zhang and Luo, 2019). Highlighted in our interviews (e.g. I16: \"hate has always been expressed idiomatically\").\n\n2020) F6 -threats expressed as normative statements: Highlighted by an interviewee as a way of avoiding legal consequences to hate speech. Golbeck, F5 -direct threats against a protected group or its members: Core element of several hate speech taxonomies. ). F7 -hate expressed using slurs: Prevalent way of expressing hateF5 -direct threats against a protected group or its members: Core element of several hate speech taxonomies (Golbeck et al., 2017; Zampieri et al., 2019; Vidgen et al., 2020a; Banko et al., 2020) F6 -threats expressed as normative statements: Highlighted by an interviewee as a way of avoiding legal consequences to hate speech (I1: \"[normative threats] are extremely hateful, but [legally] okay\"). F7 -hate expressed using slurs: Prevalent way of expressing hate (Palmer et al., 2020; Banko et al., 2020; Kurrek et al., 2020).\n\nF8 -non-hateful homonyms of slur: Relevant alternative use of slurs. Kurrek, F8 -non-hateful homonyms of slur: Relevant alternative use of slurs (Kurrek et al., 2020).\n\nA lot of LGBT people use slurs to identify themselves, like reclaim the word queer, and people. Palmer, F9 -use of reclaimed slurs: Likely source of classification error. Highlighted in our interviews. report that and then that will get hidden\")F9 -use of reclaimed slurs: Likely source of classification error (Palmer et al., 2020). High- lighted in our interviews (e.g. I7: \"A lot of LGBT people use slurs to identify themselves, like reclaim the word queer, and people [...] report that and then that will get hidden\").\n\nF10 -hate expressed using profanity: Refines more general \"insult\" categories. Davidson, F10 -hate expressed using profanity: Refines more general \"insult\" categories (Davidson et al., 2017; Zampieri et al., 2019).\n\nF11 -non-hateful uses of profanity: Oversensitiveness of hate speech detection models to profanity. Davidson, Malmasi and Zampieri. F11 -non-hateful uses of profanity: Oversensi- tiveness of hate speech detection models to profan- ity (Davidson et al., 2017; Malmasi and Zampieri, 2018; van Aken et al., 2018).\n\nF13 -hate expressed through pronoun reference in subsequent sentences: See F12. F14 -hate expressed using negated positive statements: Negation as an effective adversary for hate speech detection models. Vidgen, F12 -hate expressed through pronoun reference in subsequent clauses: Syntactic relationships and long-range dependencies as model weak points (Burnap and Williams. F12 -hate expressed through pronoun reference in subsequent clauses: Syntactic relationships and long-range dependencies as model weak points (Burnap and Williams, 2015; Vidgen et al., 2019). F13 -hate expressed through pronoun reference in subsequent sentences: See F12. F14 -hate expressed using negated positive statements: Negation as an effective adversary for hate speech detection models (Hosseini et al., 2017;\n\n. Dinan, Dinan et al., 2019).\n\nF17 -hate phrased as an opinion: Highlighted by an interviewee as a way of avoiding legal consequences to hate speech (I1: \"If you start a sentence by saying 'I think that. Van Aken, F15 -non-hate expressed using negated hateful statements: See F14. F16 -hate phrased as a question: Likely source of classification error. the limits of what you can say are much bigger\")F15 -non-hate expressed using negated hateful statements: See F14. F16 -hate phrased as a question: Likely source of classification error (van Aken et al., 2018). F17 -hate phrased as an opinion: Highlighted by an interviewee as a way of avoiding legal conse- quences to hate speech (I1: \"If you start a sentence by saying 'I think that' [...], the limits of what you can say are much bigger\").\n\nF18 -neutral statements using protected group identifiers: Oversensitiveness of hate speech detection models to terms such as \"black\" and \"gay. Dixon, Also highlighted in our interviews (e.g. I7: \"I have seen the algorithm get it wrong, if someone's saying something like 'I'm so gayF18 -neutral statements using protected group identifiers: Oversensitiveness of hate speech de- tection models to terms such as \"black\" and \"gay\" (Dixon et al., 2018; Park et al., 2018; Kennedy et al., 2020). Also highlighted in our interviews (e.g. I7: \"I have seen the algorithm get it wrong, if someone's saying something like 'I'm so gay'.\").\n\nF19 -positive statements using protected group identifiers: See F18. F19 -positive statements using protected group identifiers: See F18.\n\nMost mentioned concern in our interviews (e.g. I4: \"people will be quoting someone, calling that person out. Van Aken, Warner and HirschbergF20 -denouncements of hate that quote it: Counter speech as a source of classification errorF20 -denouncements of hate that quote it: Counter speech as a source of classification error (Warner and Hirschberg, 2012; van Aken et al., 2018; Vidgen et al., 2020a). Most mentioned con- cern in our interviews (e.g. I4: \"people will be quoting someone, calling that person out [...] but that will get picked up by the system\").\n\nF21 -denouncements of hate that make direct reference to it: See F20. F21 -denouncements of hate that make direct reference to it: See F20.\n\nF22 -abuse targeted at objects: Distinct from hate speech since it targets out-of-scope entities. Wulczyn, F22 -abuse targeted at objects: Distinct from hate speech since it targets out-of-scope entities (Wulczyn et al., 2017; Zampieri et al., 2019).\n\nF23 -abuse targeted at individuals not referencing membership in a protected group: See F22. F24 -abuse targeted at non-protected groups (e.g. professions): See F22. F23 -abuse targeted at individuals not referenc- ing membership in a protected group: See F22. F24 -abuse targeted at non-protected groups (e.g. professions): See F22.\n\nParticularly relevant to hate speech since they can reflect intentional behaviour of users looking to avoid detection. Van Aken, Gr\u00f6ndahl et al. F25 -swaps of adjacent characters: Simple misspellings can be challenging for detection modelsF25 -swaps of adjacent characters: Simple mis- spellings can be challenging for detection models (van Aken et al., 2018; Qian et al., 2018). Particu- larly relevant to hate speech since they can reflect intentional behaviour of users looking to avoid de- tection (Hosseini et al., 2017; Gr\u00f6ndahl et al., 2018;\n\n. Vidgen, Vidgen et al., 2019).\n\nI7: \"it could be a misspelling of a word like 'f*ggot', and someone's put one 'g' instead of two. F26 -missing characters: Highlighted in our interviews. F26 -missing characters: Highlighted in our interviews (e.g. I7: \"it could be a misspelling of a word like 'f*ggot', and someone's put one 'g' instead of two\").\n\nResembles the use of hashtags on social media (I2: \"there have been a highly Islamophobic hashtags going around. Gr\u00f6ndahl, F27 -missing word boundaries: Effective adversary for a hate speech detection model. F27 -missing word boundaries: Effective adver- sary for a hate speech detection model (Gr\u00f6ndahl et al., 2018). Resembles the use of hashtags on social media (I2: \"there have been a highly Islamo- phobic hashtags going around\").\n\nHighlighted in our interviews (e.g. I5: \"misspellings, missing letters or additional spaces between the letters. Gr\u00f6ndahl, F28 -added spaces between characters: Effective adversary for a hate speech detection modelF28 -added spaces between characters: Effec- tive adversary for a hate speech detection model (Gr\u00f6ndahl et al., 2018). Highlighted in our inter- views (e.g. I5: \"misspellings, missing letters or additional spaces between the letters.\").\n\nNobata, F29 -leet speak: Resembles \"obfuscations. Highlighted in our interviews. e.g. I14: \"[hate speakers] replace letters with numbers\")F29 -leet speak: Resembles \"obfuscations\" (Nobata et al., 2016; van Aken et al., 2018). High- lighted in our interviews (e.g. I14: \"[hate speakers] replace letters with numbers\").\n", "annotations": {"author": "[{\"end\":144,\"start\":80},{\"end\":188,\"start\":145},{\"end\":222,\"start\":189},{\"end\":263,\"start\":223},{\"end\":330,\"start\":264},{\"end\":376,\"start\":331}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":85},{\"end\":159,\"start\":153},{\"end\":200,\"start\":194},{\"end\":236,\"start\":230},{\"end\":278,\"start\":270},{\"end\":352,\"start\":339}]", "author_first_name": "[{\"end\":84,\"start\":80},{\"end\":152,\"start\":145},{\"end\":193,\"start\":189},{\"end\":229,\"start\":223},{\"end\":269,\"start\":264},{\"end\":336,\"start\":331},{\"end\":338,\"start\":337}]", "author_affiliation": "[{\"end\":115,\"start\":94},{\"end\":143,\"start\":117},{\"end\":187,\"start\":161},{\"end\":221,\"start\":202},{\"end\":262,\"start\":238},{\"end\":301,\"start\":280},{\"end\":329,\"start\":303},{\"end\":375,\"start\":354}]", "title": "[{\"end\":61,\"start\":1},{\"end\":437,\"start\":377}]", "venue": "[{\"end\":601,\"start\":439}]", "abstract": "[{\"end\":1784,\"start\":768}]", "bib_ref": "[{\"end\":2108,\"start\":2082},{\"end\":2132,\"start\":2108},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2152,\"start\":2132},{\"end\":2185,\"start\":2152},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2376,\"start\":2354},{\"end\":2396,\"start\":2376},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2576,\"start\":2559},{\"end\":2845,\"start\":2825},{\"end\":2863,\"start\":2845},{\"end\":2881,\"start\":2863},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3076,\"start\":3047},{\"end\":3097,\"start\":3076},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3155,\"start\":3141},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3175,\"start\":3155},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3192,\"start\":3175},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3250,\"start\":3228},{\"end\":3283,\"start\":3250},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3438,\"start\":3419},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3457,\"start\":3438},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3478,\"start\":3457},{\"end\":3557,\"start\":3523},{\"end\":3577,\"start\":3557},{\"end\":4074,\"start\":4060},{\"end\":4097,\"start\":4076},{\"end\":4868,\"start\":4846},{\"end\":5031,\"start\":5030},{\"end\":5217,\"start\":5196},{\"end\":5316,\"start\":5297},{\"end\":5406,\"start\":5405},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6131,\"start\":6102},{\"end\":7038,\"start\":7018},{\"end\":7060,\"start\":7038},{\"end\":8632,\"start\":8618},{\"end\":8656,\"start\":8632},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8676,\"start\":8656},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8846,\"start\":8827},{\"end\":10591,\"start\":10590},{\"end\":11033,\"start\":11011},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11061,\"start\":11033},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11629,\"start\":11611},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14751,\"start\":14733},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14901,\"start\":14881},{\"end\":14919,\"start\":14901},{\"end\":14940,\"start\":14919},{\"end\":15025,\"start\":15015},{\"end\":16891,\"start\":16868},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22409,\"start\":22389},{\"end\":22566,\"start\":22556},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25140,\"start\":25118},{\"end\":25173,\"start\":25140},{\"end\":25351,\"start\":25333},{\"end\":25499,\"start\":25477},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27205,\"start\":27187},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27226,\"start\":27205},{\"end\":27245,\"start\":27226},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28071,\"start\":28052},{\"end\":29084,\"start\":29063},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36701,\"start\":36681}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41465,\"start\":41063},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42103,\"start\":41466},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47039,\"start\":42104},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47090,\"start\":47040},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47197,\"start\":47091},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47307,\"start\":47198},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47977,\"start\":47308},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":64020,\"start\":47978},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":64059,\"start\":64021}]", "paragraph": "[{\"end\":2186,\"start\":1800},{\"end\":3712,\"start\":2188},{\"end\":6025,\"start\":3714},{\"end\":7164,\"start\":6055},{\"end\":7440,\"start\":7166},{\"end\":7835,\"start\":7487},{\"end\":8183,\"start\":7837},{\"end\":8371,\"start\":8225},{\"end\":8934,\"start\":8403},{\"end\":10680,\"start\":8936},{\"end\":10813,\"start\":10682},{\"end\":11253,\"start\":10815},{\"end\":11810,\"start\":11255},{\"end\":12079,\"start\":11844},{\"end\":12429,\"start\":12081},{\"end\":13037,\"start\":12431},{\"end\":13319,\"start\":13039},{\"end\":13787,\"start\":13352},{\"end\":14424,\"start\":13812},{\"end\":14829,\"start\":14450},{\"end\":15179,\"start\":14831},{\"end\":15502,\"start\":15181},{\"end\":15874,\"start\":15504},{\"end\":16282,\"start\":15876},{\"end\":16892,\"start\":16308},{\"end\":17631,\"start\":16894},{\"end\":17969,\"start\":17679},{\"end\":18464,\"start\":17987},{\"end\":18760,\"start\":18481},{\"end\":19164,\"start\":18762},{\"end\":19455,\"start\":19166},{\"end\":19763,\"start\":19467},{\"end\":21462,\"start\":19765},{\"end\":21754,\"start\":21464},{\"end\":22291,\"start\":21801},{\"end\":22620,\"start\":22293},{\"end\":22910,\"start\":22622},{\"end\":23602,\"start\":22925},{\"end\":23936,\"start\":23604},{\"end\":24130,\"start\":23938},{\"end\":24556,\"start\":24132},{\"end\":25352,\"start\":24558},{\"end\":25876,\"start\":25354},{\"end\":26472,\"start\":25920},{\"end\":26824,\"start\":26505},{\"end\":27006,\"start\":26826},{\"end\":27246,\"start\":27008},{\"end\":27559,\"start\":27267},{\"end\":27650,\"start\":27561},{\"end\":28593,\"start\":27652},{\"end\":29352,\"start\":28610},{\"end\":30081,\"start\":29367},{\"end\":30560,\"start\":30083},{\"end\":30811,\"start\":30562},{\"end\":31011,\"start\":30813},{\"end\":31210,\"start\":31031},{\"end\":31367,\"start\":31231},{\"end\":31769,\"start\":31369},{\"end\":31936,\"start\":31771},{\"end\":32247,\"start\":31963},{\"end\":32658,\"start\":32249},{\"end\":33194,\"start\":32660},{\"end\":33368,\"start\":33228},{\"end\":34126,\"start\":33370},{\"end\":34613,\"start\":34128},{\"end\":35247,\"start\":34615},{\"end\":36702,\"start\":35249},{\"end\":36811,\"start\":36704},{\"end\":36970,\"start\":36813},{\"end\":37776,\"start\":37037},{\"end\":38867,\"start\":37802},{\"end\":39167,\"start\":38869},{\"end\":39582,\"start\":39169},{\"end\":39764,\"start\":39584},{\"end\":40158,\"start\":39798},{\"end\":40452,\"start\":40160},{\"end\":40969,\"start\":40461},{\"end\":41062,\"start\":40971}]", "formula": null, "table_ref": "[{\"end\":13137,\"start\":13130},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19902,\"start\":19892},{\"end\":20289,\"start\":20280},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21919,\"start\":21912},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":22503,\"start\":22496},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24129,\"start\":24120}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1798,\"start\":1786},{\"end\":6053,\"start\":6028},{\"attributes\":{\"n\":\"2\"},\"end\":7452,\"start\":7443},{\"attributes\":{\"n\":\"2.1\"},\"end\":7485,\"start\":7455},{\"attributes\":{\"n\":\"2.2\"},\"end\":8223,\"start\":8186},{\"end\":8401,\"start\":8374},{\"attributes\":{\"n\":\"2.3\"},\"end\":11842,\"start\":11813},{\"end\":13350,\"start\":13322},{\"end\":13810,\"start\":13790},{\"attributes\":{\"n\":\"2.4\"},\"end\":14448,\"start\":14427},{\"attributes\":{\"n\":\"2.5\"},\"end\":16306,\"start\":16285},{\"attributes\":{\"n\":\"3\"},\"end\":17663,\"start\":17634},{\"attributes\":{\"n\":\"3.1\"},\"end\":17677,\"start\":17666},{\"end\":17985,\"start\":17972},{\"end\":18479,\"start\":18467},{\"attributes\":{\"n\":\"3.2\"},\"end\":19465,\"start\":19458},{\"end\":21799,\"start\":21757},{\"attributes\":{\"n\":\"3.3\"},\"end\":22923,\"start\":22913},{\"attributes\":{\"n\":\"4\"},\"end\":25890,\"start\":25879},{\"attributes\":{\"n\":\"4.1\"},\"end\":25918,\"start\":25893},{\"attributes\":{\"n\":\"4.2\"},\"end\":26503,\"start\":26475},{\"attributes\":{\"n\":\"4.3\"},\"end\":27265,\"start\":27249},{\"attributes\":{\"n\":\"5\"},\"end\":28608,\"start\":28596},{\"attributes\":{\"n\":\"6\"},\"end\":29365,\"start\":29355},{\"end\":31029,\"start\":31014},{\"end\":31229,\"start\":31213},{\"end\":31961,\"start\":31939},{\"end\":33207,\"start\":33197},{\"end\":33226,\"start\":33210},{\"end\":37035,\"start\":36973},{\"end\":37800,\"start\":37779},{\"end\":39796,\"start\":39767},{\"end\":40459,\"start\":40455},{\"end\":47050,\"start\":47041},{\"end\":47101,\"start\":47092},{\"end\":47208,\"start\":47199},{\"end\":64031,\"start\":64022}]", "table": "[{\"end\":47039,\"start\":42516},{\"end\":47307,\"start\":47210},{\"end\":64020,\"start\":50427}]", "figure_caption": "[{\"end\":41465,\"start\":41065},{\"end\":42103,\"start\":41468},{\"end\":42516,\"start\":42106},{\"end\":47090,\"start\":47052},{\"end\":47197,\"start\":47103},{\"end\":47977,\"start\":47310},{\"end\":50427,\"start\":47980},{\"end\":64059,\"start\":64033}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":67808,\"start\":67802},{\"end\":67821,\"start\":67817},{\"end\":68101,\"start\":68095},{\"end\":68115,\"start\":68110},{\"end\":68126,\"start\":68122},{\"end\":68139,\"start\":68134},{\"end\":68155,\"start\":68150},{\"end\":68176,\"start\":68170},{\"end\":68191,\"start\":68185},{\"end\":68208,\"start\":68201},{\"end\":68222,\"start\":68215},{\"end\":68810,\"start\":68804},{\"end\":68823,\"start\":68819},{\"end\":68836,\"start\":68832},{\"end\":68852,\"start\":68845},{\"end\":68867,\"start\":68862},{\"end\":68879,\"start\":68874},{\"end\":69463,\"start\":69457},{\"end\":69479,\"start\":69472},{\"end\":69494,\"start\":69488},{\"end\":69508,\"start\":69503},{\"end\":69771,\"start\":69764},{\"end\":69785,\"start\":69780},{\"end\":70264,\"start\":70260},{\"end\":70281,\"start\":70275},{\"end\":70297,\"start\":70291},{\"end\":70308,\"start\":70303},{\"end\":70323,\"start\":70320},{\"end\":70338,\"start\":70330},{\"end\":70351,\"start\":70345},{\"end\":70353,\"start\":70352},{\"end\":70805,\"start\":70799},{\"end\":71375,\"start\":71369},{\"end\":71390,\"start\":71384},{\"end\":71405,\"start\":71401},{\"end\":71422,\"start\":71416},{\"end\":72008,\"start\":72002},{\"end\":72021,\"start\":72017},{\"end\":72555,\"start\":72549},{\"end\":72569,\"start\":72564},{\"end\":72585,\"start\":72578},{\"end\":72889,\"start\":72882},{\"end\":72904,\"start\":72899},{\"end\":72924,\"start\":72918},{\"end\":73775,\"start\":73769},{\"end\":73790,\"start\":73782},{\"end\":73804,\"start\":73798},{\"end\":73817,\"start\":73811},{\"end\":73835,\"start\":73828},{\"end\":73853,\"start\":73846},{\"end\":73866,\"start\":73859},{\"end\":73878,\"start\":73875},{\"end\":73890,\"start\":73886},{\"end\":73903,\"start\":73897},{\"end\":73918,\"start\":73915},{\"end\":73931,\"start\":73928},{\"end\":73947,\"start\":73942},{\"end\":73974,\"start\":73968},{\"end\":73985,\"start\":73979},{\"end\":74001,\"start\":73995},{\"end\":74012,\"start\":74007},{\"end\":74015,\"start\":74013},{\"end\":74027,\"start\":74020},{\"end\":74041,\"start\":74034},{\"end\":75029,\"start\":75019},{\"end\":75039,\"start\":75034},{\"end\":75045,\"start\":75040},{\"end\":75062,\"start\":75055},{\"end\":75075,\"start\":75069},{\"end\":75681,\"start\":75675},{\"end\":75697,\"start\":75691},{\"end\":75710,\"start\":75705},{\"end\":76098,\"start\":76092},{\"end\":76116,\"start\":76109},{\"end\":76133,\"start\":76126},{\"end\":76145,\"start\":76141},{\"end\":76162,\"start\":76157},{\"end\":76176,\"start\":76170},{\"end\":77065,\"start\":77061},{\"end\":77076,\"start\":77073},{\"end\":77348,\"start\":77343},{\"end\":77360,\"start\":77355},{\"end\":77369,\"start\":77366},{\"end\":77380,\"start\":77375}]", "bib_author_last_name": "[{\"end\":67815,\"start\":67809},{\"end\":67832,\"start\":67822},{\"end\":68108,\"start\":68102},{\"end\":68120,\"start\":68116},{\"end\":68132,\"start\":68127},{\"end\":68148,\"start\":68140},{\"end\":68168,\"start\":68156},{\"end\":68183,\"start\":68177},{\"end\":68199,\"start\":68192},{\"end\":68213,\"start\":68209},{\"end\":68230,\"start\":68223},{\"end\":68817,\"start\":68811},{\"end\":68830,\"start\":68824},{\"end\":68843,\"start\":68837},{\"end\":68860,\"start\":68853},{\"end\":68872,\"start\":68868},{\"end\":68888,\"start\":68880},{\"end\":69470,\"start\":69464},{\"end\":69486,\"start\":69480},{\"end\":69501,\"start\":69495},{\"end\":69514,\"start\":69509},{\"end\":69778,\"start\":69772},{\"end\":69796,\"start\":69786},{\"end\":70273,\"start\":70265},{\"end\":70289,\"start\":70282},{\"end\":70301,\"start\":70298},{\"end\":70318,\"start\":70309},{\"end\":70328,\"start\":70324},{\"end\":70343,\"start\":70339},{\"end\":70360,\"start\":70354},{\"end\":70812,\"start\":70806},{\"end\":71382,\"start\":71376},{\"end\":71399,\"start\":71391},{\"end\":71414,\"start\":71406},{\"end\":71428,\"start\":71423},{\"end\":72015,\"start\":72009},{\"end\":72026,\"start\":72022},{\"end\":72562,\"start\":72556},{\"end\":72576,\"start\":72570},{\"end\":72592,\"start\":72586},{\"end\":72897,\"start\":72890},{\"end\":72916,\"start\":72905},{\"end\":72935,\"start\":72925},{\"end\":73780,\"start\":73776},{\"end\":73796,\"start\":73791},{\"end\":73809,\"start\":73805},{\"end\":73826,\"start\":73818},{\"end\":73844,\"start\":73836},{\"end\":73857,\"start\":73854},{\"end\":73873,\"start\":73867},{\"end\":73884,\"start\":73879},{\"end\":73895,\"start\":73891},{\"end\":73913,\"start\":73904},{\"end\":73926,\"start\":73919},{\"end\":73940,\"start\":73932},{\"end\":73966,\"start\":73948},{\"end\":73977,\"start\":73975},{\"end\":73993,\"start\":73986},{\"end\":74005,\"start\":74002},{\"end\":74018,\"start\":74016},{\"end\":74032,\"start\":74028},{\"end\":74048,\"start\":74042},{\"end\":74055,\"start\":74050},{\"end\":75032,\"start\":75030},{\"end\":75053,\"start\":75046},{\"end\":75067,\"start\":75063},{\"end\":75080,\"start\":75076},{\"end\":75689,\"start\":75682},{\"end\":75703,\"start\":75698},{\"end\":75716,\"start\":75711},{\"end\":76107,\"start\":76099},{\"end\":76124,\"start\":76117},{\"end\":76139,\"start\":76134},{\"end\":76155,\"start\":76146},{\"end\":76168,\"start\":76163},{\"end\":76182,\"start\":76177},{\"end\":77071,\"start\":77066},{\"end\":77080,\"start\":77077},{\"end\":77353,\"start\":77349},{\"end\":77364,\"start\":77361},{\"end\":77373,\"start\":77370},{\"end\":77387,\"start\":77381},{\"end\":78048,\"start\":78040},{\"end\":78410,\"start\":78402},{\"end\":78695,\"start\":78685},{\"end\":79092,\"start\":79089},{\"end\":79704,\"start\":79697},{\"end\":80486,\"start\":80480},{\"end\":80682,\"start\":80676},{\"end\":81191,\"start\":81183},{\"end\":81428,\"start\":81420},{\"end\":81842,\"start\":81836},{\"end\":82435,\"start\":82430},{\"end\":82640,\"start\":82632},{\"end\":83374,\"start\":83369},{\"end\":84112,\"start\":84104},{\"end\":84804,\"start\":84797},{\"end\":85413,\"start\":85405},{\"end\":85844,\"start\":85838},{\"end\":86306,\"start\":86298},{\"end\":86743,\"start\":86735},{\"end\":87080,\"start\":87074}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1371/journal.pone.0243300\",\"id\":\"b0\",\"matched_paper_id\":229715256},\"end\":68045,\"start\":67710},{\"attributes\":{\"doi\":\"10.18653/v1/2020.alw-1.19\",\"id\":\"b1\",\"matched_paper_id\":218570960},\"end\":68747,\"start\":68047},{\"attributes\":{\"doi\":\"10.18653/v1/W19-3509\",\"id\":\"b2\",\"matched_paper_id\":201683127},\"end\":69365,\"start\":68749},{\"attributes\":{\"doi\":\"abs/2012.15761\",\"id\":\"b3\"},\"end\":69717,\"start\":69367},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12477446},\"end\":70196,\"start\":69719},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00321\",\"id\":\"b5\",\"matched_paper_id\":208527435},\"end\":70700,\"start\":70198},{\"attributes\":{\"doi\":\"10.18653/v1/W16-5618\",\"id\":\"b6\",\"matched_paper_id\":406026},\"end\":71295,\"start\":70702},{\"attributes\":{\"doi\":\"10.18653/v1/W17-3012\",\"id\":\"b7\",\"matched_paper_id\":8821211},\"end\":71907,\"start\":71297},{\"attributes\":{\"doi\":\"10.18653/v1/N16-2013\",\"id\":\"b8\",\"matched_paper_id\":1721388},\"end\":72462,\"start\":71909},{\"attributes\":{\"id\":\"b9\"},\"end\":72817,\"start\":72464},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1060\",\"id\":\"b10\",\"matched_paper_id\":174799974},\"end\":73707,\"start\":72819},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-demos.6\",\"id\":\"b11\",\"matched_paper_id\":208117506},\"end\":74954,\"start\":73709},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1073\",\"id\":\"b12\",\"matched_paper_id\":196199409},\"end\":75629,\"start\":74956},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6060248},\"end\":76023,\"start\":75631},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1144\",\"id\":\"b14\",\"matched_paper_id\":67856299},\"end\":76972,\"start\":76025},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4737415},\"end\":77243,\"start\":76974},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.659\",\"id\":\"b16\",\"matched_paper_id\":216562482},\"end\":77919,\"start\":77245},{\"attributes\":{\"id\":\"b17\"},\"end\":78264,\"start\":77921},{\"attributes\":{\"id\":\"b18\"},\"end\":78594,\"start\":78266},{\"attributes\":{\"id\":\"b19\"},\"end\":79060,\"start\":78596},{\"attributes\":{\"id\":\"b20\"},\"end\":79556,\"start\":79062},{\"attributes\":{\"id\":\"b21\"},\"end\":80409,\"start\":79558},{\"attributes\":{\"id\":\"b22\"},\"end\":80578,\"start\":80411},{\"attributes\":{\"id\":\"b23\"},\"end\":81102,\"start\":80580},{\"attributes\":{\"id\":\"b24\"},\"end\":81318,\"start\":81104},{\"attributes\":{\"id\":\"b25\"},\"end\":81630,\"start\":81320},{\"attributes\":{\"id\":\"b26\"},\"end\":82426,\"start\":81632},{\"attributes\":{\"id\":\"b27\"},\"end\":82457,\"start\":82428},{\"attributes\":{\"id\":\"b28\"},\"end\":83223,\"start\":82459},{\"attributes\":{\"id\":\"b29\"},\"end\":83854,\"start\":83225},{\"attributes\":{\"id\":\"b30\"},\"end\":83993,\"start\":83856},{\"attributes\":{\"id\":\"b31\"},\"end\":84556,\"start\":83995},{\"attributes\":{\"id\":\"b32\"},\"end\":84697,\"start\":84558},{\"attributes\":{\"id\":\"b33\"},\"end\":84949,\"start\":84699},{\"attributes\":{\"id\":\"b34\"},\"end\":85284,\"start\":84951},{\"attributes\":{\"id\":\"b35\"},\"end\":85834,\"start\":85286},{\"attributes\":{\"id\":\"b36\"},\"end\":85867,\"start\":85836},{\"attributes\":{\"id\":\"b37\"},\"end\":86183,\"start\":85869},{\"attributes\":{\"id\":\"b38\"},\"end\":86620,\"start\":86185},{\"attributes\":{\"id\":\"b39\"},\"end\":87072,\"start\":86622},{\"attributes\":{\"id\":\"b40\"},\"end\":87391,\"start\":87074}]", "bib_title": "[{\"end\":67800,\"start\":67710},{\"end\":68093,\"start\":68047},{\"end\":68802,\"start\":68749},{\"end\":69762,\"start\":69719},{\"end\":70258,\"start\":70198},{\"end\":70797,\"start\":70702},{\"end\":71367,\"start\":71297},{\"end\":72000,\"start\":71909},{\"end\":72880,\"start\":72819},{\"end\":73767,\"start\":73709},{\"end\":75017,\"start\":74956},{\"end\":75673,\"start\":75631},{\"end\":76090,\"start\":76025},{\"end\":77059,\"start\":76974},{\"end\":77341,\"start\":77245},{\"end\":79087,\"start\":79062},{\"end\":79695,\"start\":79558},{\"end\":80674,\"start\":80580},{\"end\":81418,\"start\":81320},{\"end\":81834,\"start\":81632},{\"end\":82630,\"start\":82459},{\"end\":85403,\"start\":85286},{\"end\":85965,\"start\":85869},{\"end\":86296,\"start\":86185}]", "bib_author": "[{\"end\":67817,\"start\":67802},{\"end\":67834,\"start\":67817},{\"end\":68110,\"start\":68095},{\"end\":68122,\"start\":68110},{\"end\":68134,\"start\":68122},{\"end\":68150,\"start\":68134},{\"end\":68170,\"start\":68150},{\"end\":68185,\"start\":68170},{\"end\":68201,\"start\":68185},{\"end\":68215,\"start\":68201},{\"end\":68232,\"start\":68215},{\"end\":68819,\"start\":68804},{\"end\":68832,\"start\":68819},{\"end\":68845,\"start\":68832},{\"end\":68862,\"start\":68845},{\"end\":68874,\"start\":68862},{\"end\":68890,\"start\":68874},{\"end\":69472,\"start\":69457},{\"end\":69488,\"start\":69472},{\"end\":69503,\"start\":69488},{\"end\":69516,\"start\":69503},{\"end\":69780,\"start\":69764},{\"end\":69798,\"start\":69780},{\"end\":70275,\"start\":70260},{\"end\":70291,\"start\":70275},{\"end\":70303,\"start\":70291},{\"end\":70320,\"start\":70303},{\"end\":70330,\"start\":70320},{\"end\":70345,\"start\":70330},{\"end\":70362,\"start\":70345},{\"end\":70814,\"start\":70799},{\"end\":71384,\"start\":71369},{\"end\":71401,\"start\":71384},{\"end\":71416,\"start\":71401},{\"end\":71430,\"start\":71416},{\"end\":72017,\"start\":72002},{\"end\":72028,\"start\":72017},{\"end\":72564,\"start\":72549},{\"end\":72578,\"start\":72564},{\"end\":72594,\"start\":72578},{\"end\":72899,\"start\":72882},{\"end\":72918,\"start\":72899},{\"end\":72937,\"start\":72918},{\"end\":73782,\"start\":73769},{\"end\":73798,\"start\":73782},{\"end\":73811,\"start\":73798},{\"end\":73828,\"start\":73811},{\"end\":73846,\"start\":73828},{\"end\":73859,\"start\":73846},{\"end\":73875,\"start\":73859},{\"end\":73886,\"start\":73875},{\"end\":73897,\"start\":73886},{\"end\":73915,\"start\":73897},{\"end\":73928,\"start\":73915},{\"end\":73942,\"start\":73928},{\"end\":73968,\"start\":73942},{\"end\":73979,\"start\":73968},{\"end\":73995,\"start\":73979},{\"end\":74007,\"start\":73995},{\"end\":74020,\"start\":74007},{\"end\":74034,\"start\":74020},{\"end\":74050,\"start\":74034},{\"end\":74057,\"start\":74050},{\"end\":75034,\"start\":75019},{\"end\":75055,\"start\":75034},{\"end\":75069,\"start\":75055},{\"end\":75082,\"start\":75069},{\"end\":75691,\"start\":75675},{\"end\":75705,\"start\":75691},{\"end\":75718,\"start\":75705},{\"end\":76109,\"start\":76092},{\"end\":76126,\"start\":76109},{\"end\":76141,\"start\":76126},{\"end\":76157,\"start\":76141},{\"end\":76170,\"start\":76157},{\"end\":76184,\"start\":76170},{\"end\":77073,\"start\":77061},{\"end\":77082,\"start\":77073},{\"end\":77355,\"start\":77343},{\"end\":77366,\"start\":77355},{\"end\":77375,\"start\":77366},{\"end\":77389,\"start\":77375},{\"end\":78050,\"start\":78040},{\"end\":78412,\"start\":78402},{\"end\":78697,\"start\":78685},{\"end\":79094,\"start\":79089},{\"end\":79706,\"start\":79697},{\"end\":80488,\"start\":80480},{\"end\":80684,\"start\":80676},{\"end\":81193,\"start\":81183},{\"end\":81430,\"start\":81420},{\"end\":81844,\"start\":81836},{\"end\":82437,\"start\":82430},{\"end\":82642,\"start\":82632},{\"end\":83376,\"start\":83369},{\"end\":84114,\"start\":84104},{\"end\":84806,\"start\":84797},{\"end\":85415,\"start\":85405},{\"end\":85846,\"start\":85838},{\"end\":86308,\"start\":86298},{\"end\":86745,\"start\":86735},{\"end\":87082,\"start\":87074}]", "bib_venue": "[{\"end\":67870,\"start\":67862},{\"end\":68317,\"start\":68257},{\"end\":68970,\"start\":68910},{\"end\":69455,\"start\":69367},{\"end\":69860,\"start\":69798},{\"end\":70443,\"start\":70382},{\"end\":70907,\"start\":70834},{\"end\":71510,\"start\":71450},{\"end\":72098,\"start\":72048},{\"end\":72547,\"start\":72464},{\"end\":73099,\"start\":72957},{\"end\":74196,\"start\":74087},{\"end\":75189,\"start\":75102},{\"end\":75784,\"start\":75718},{\"end\":76346,\"start\":76204},{\"end\":77094,\"start\":77082},{\"end\":77514,\"start\":77420},{\"end\":78038,\"start\":77921},{\"end\":78400,\"start\":78266},{\"end\":78683,\"start\":78596},{\"end\":79186,\"start\":79094},{\"end\":79813,\"start\":79706},{\"end\":80478,\"start\":80411},{\"end\":80749,\"start\":80684},{\"end\":81181,\"start\":81104},{\"end\":81450,\"start\":81430},{\"end\":82006,\"start\":81844},{\"end\":82707,\"start\":82642},{\"end\":83367,\"start\":83225},{\"end\":83923,\"start\":83856},{\"end\":84102,\"start\":83995},{\"end\":84626,\"start\":84558},{\"end\":84795,\"start\":84699},{\"end\":85115,\"start\":84951},{\"end\":85429,\"start\":85415},{\"end\":86021,\"start\":85967},{\"end\":86391,\"start\":86308},{\"end\":86733,\"start\":86622},{\"end\":87122,\"start\":87082},{\"end\":68364,\"start\":68319},{\"end\":69032,\"start\":68972},{\"end\":69925,\"start\":69862},{\"end\":70980,\"start\":70909},{\"end\":71578,\"start\":71512},{\"end\":72156,\"start\":72100},{\"end\":73250,\"start\":73101},{\"end\":74326,\"start\":74198},{\"end\":75278,\"start\":75191},{\"end\":75837,\"start\":75786},{\"end\":76497,\"start\":76348},{\"end\":77595,\"start\":77516}]"}}}, "year": 2023, "month": 12, "day": 17}