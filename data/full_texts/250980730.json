{"id": 250980730, "updated": "2023-03-23 13:40:36.57", "metadata": {"title": "3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds", "authors": "[{\"first\":\"Daigang\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Lichen\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Sheng\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2022, "month": 6, "day": 1}, "abstract": "Observing that the 3D captioning task and the 3D grounding task contain both shared and complementary information in nature, in this work, we propose a unified framework to jointly solve these two distinct but closely related tasks in a synergistic fashion, which consists of both shared task-agnostic modules and lightweight task-specific modules. On one hand, the shared task-agnostic modules aim to learn precise locations of objects, fine-grained attribute features to characterize different objects, and complex relations between objects, which benefit both captioning and visual grounding. On the other hand, by casting each of the two tasks as the proxy task of another one, the lightweight task-specific modules solve the captioning task and the grounding task respectively. Extensive experiments and ablation study on three 3D vision and language datasets demonstrate that our joint training frame-work achieves significant performance gains for each individual task and finally improves the state-of-the-art performance for both captioning and grounding tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/CaiZZSX22", "doi": "10.1109/cvpr52688.2022.01597"}}, "content": {"source": {"pdf_hash": "82390d76ad4a4510a1406d63a25cbd3b87f71c1d", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "720cf7cf74ab16f07a84b16f15f7932753a16451", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/82390d76ad4a4510a1406d63a25cbd3b87f71c1d.txt", "contents": "\n3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds\n\n\nDaigang Cai \nCollege of Software\nBeihang University\nChina\n\nLichen Zhao \nCollege of Software\nBeihang University\nChina\n\nJing Zhang \nCollege of Software\nBeihang University\nChina\n\nLu Sheng lsheng@buaa.edu.cn \nCollege of Software\nBeihang University\nChina\n\nDong Xu dong.xu@sydney.edu.au \nThe University of Sydney\nAustralia\n\n3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds\n10.1109/CVPR52688.2022.01597\nObserving that the 3D captioning task and the 3D grounding task contain both shared and complementary information in nature, in this work, we propose a unified framework to jointly solve these two distinct but closely related tasks in a synergistic fashion, which consists of both shared task-agnostic modules and lightweight task-specific modules. On one hand, the shared task-agnostic modules aim to learn precise locations of objects, fine-grained attribute features to characterize different objects, and complex relations between objects, which benefit both captioning and visual grounding. On the other hand, by casting each of the two tasks as the proxy task of another one, the lightweight task-specific modules solve the captioning task and the grounding task respectively. Extensive experiments and ablation study on three 3D vision and language datasets demonstrate that our joint training framework achieves significant performance gains for each individual task and finally improves the state-of-the-art performance for both captioning and grounding tasks.\n\nIntroduction\n\nThere is increasing research interest in the intersection field between 3D visual understanding and natural language processing, such as 3D dense captioning [9] and 3D visual grounding [1,7,21,50]. These two tasks push the advance of the intersection field along different directions (i.e., from vision to language versus from language to vision), and encouraging progress has been achieved by separately solving each task. It still remains an open issue on whether it is possible to develop a unified framework to jointly solve the two closely related tasks in a synergistic fashion.\n\nWe observe that the two 3D vision-language tasks contain both shared and complementary information in nature, and it is possible to enhance the performance of both tasks if we treat one task as a proxy task of the other. On one \u2020 Corresponding author: Jing Zhang. hand, each of the two tasks can be decomposed into several sub-tasks, and some of these sub-tasks share the common objectives and network structures. For example, as shown in the previous vision-language works [1,7,9,21,44,47,50] on RGB-D scans, both 3D dense captioning and 3D visual grounding require: 1) a 3D object detector to detect the salient object proposals in a 3D scene, 2) a relation modeling module to model complex 3D relations among these detected objects, and 3) a multi-modal learning module to learn fused information from both visual features and textual features to generate sentences or produce bounding boxes based on each input sentence. On the other hand, the opposite procedures are also used to separately solve the two problems, namely, the captioning task is to generate a meaningful textual description from the detected boxes (i.e., from vision to language), while the grounding task is to locate the desired box by understanding a given textual description (i.e., from language to vision).\n\nMoreover, the 3D point clouds generated from RGB-D scans often contain rich and complex relations among different objects, while the corresponding RGB data provides more fine-grained attribute information, such as color, texture, and materials. Thus, the RGB-D scans intrinsically contain rich and abundant attribute and relation information for enhancing both 3D captioning and 3D grounding tasks. However, we empirically observe that the 3D dense captioning task is more object-oriented, which tends to learn more attribute information of the target objects (i.e., the objects of interest) in a scene and only the primary relationship between the target object and its surrounding objects. In contrast, the 3D visual grounding task is more relation-oriented, which focuses more on the relations between objects and distinguishes different objects (especially the objects from the same class) based on their relations. Thus, it is desirable to develop a joint framework to unify both 3D dense captioning and 3D visual grounding tasks and take advantage of each other for improving the performance of both tasks.\n\nTo this end, in this work, we propose a joint framework by unifying the distinct but closely related 3D vision-language tasks of 3D dense captioning and 3D visual grounding. Specifically, the proposed framework consists of three main modules: (1) a 3D object detector, (2) an attribute and relation-aware feature enhancement module, and (3) a task-specific grounding or captioning head. Specifically, the 3D object detector and the feature enhancement module are task-agnostic, which are designed for collaboratively supporting both captioning and grounding tasks. The two modules output the object proposals as the initial localization results of the potential objects in a scene, as well as the improved features within the proposals by integrating both attribute information from each object proposal and the complex relations between multiple proposals. With the strong task-agnostic modules, the task-specific captioning head and grounding head are designed as lightweight networks for dealing with each task, which consist of a lightweight transformer-based module together with simple preprocessing modules (i.e., the Query/Key/Value generation modules) and lightweight postprocessing modules (i.e., the word prediction or bounding box selection module). In this way, the 3D captioning and 3D visual grounding tasks can be cast as the proxy task of each other. In other words, the more object-oriented captioning task can provide more attribute information to potentially improve the grounding performance, while the more relation-oriented grounding task can help improve the captioning results by enhancing the captioning task with more relation information. Moreover, our joint framework also inspires the insights of the design of each individual captioning network and grounding network.\n\nThe contribution of this work is two-fold: (1) By analyzing both 3D dense captioning and 3D visual grounding tasks, we propose a unified framework to jointly solve the two distinct but closely related tasks by using our simple and strong network structure, which consists of a task-agnostic module with a 3D object detector and an attribute and relation-aware feature enhancement module, and two lightweight task-specific modules (i.e., a captioning head and a grounding head). (2) Extensive experiments conducted on three benchmark datasets ScanRefer [7], Scan2Cap [9], and Nr3D dataset [1] demonstrate our joint framework achieves the state-of-the-art results for both 3D dense captioning and 3D visual grounding tasks.\n\n\nRelated Work\n\n2D Vision and Language tasks. Deep learning technologies have been extensively studied in various 2D vision and language tasks, such as visual grounding [15,26,35,45]. image captioning & dense captioning [2,11,17,18,42], visual question answering [2,4,43] and text-to-image generation [25]. These impactful research problems advance the intersection research field between computer vision and natural language processing. With the rapid development of deep learning, researchers introduced several collaborative methods (e.g., speaker-listener models [3,46]) to solve various 2D vision and language tasks jointly. However, these models focus on 2D image-based tasks, while our method focuses on RGB-D-based tasks, where different types of data to be handled in our work require different network design strategies. Specifically, we propose a carefully designed task-agnostic feature enhancement module and the lightweight task-specific captioning and grounding heads, which all build upon the transformer architecture. Recently, several joint frameworks [8,23,24,27,41,48] focus on learning more generalizable image-text representations through a cumbersome model (e.g., VilBERT [23]) by using abundant and diverse 2D vision and language datasets. By contrast, based on in-depth analysis of the intrinsic properties of RGB-D scans and the characteristics of both 3D captioning and grounding tasks, our carefully designed joint learning framework with lightweight modules can effectively solve both tasks in a synergistic fashion without relying on a huge amount of paired training data. 3D Dense Captioning and Visual Grounding. Deep learning in 3D data has attracted a great deal of interest [10, 13, 20, 22, 32-34, 39, 40, 49, 51]. Recently, some dense captioning and visual grounding tasks tailored to 3D data are proposed. For example, some researches [9] proposed the 3D dense captioning methods and achieved impressive results by explicitly modeling the relation between different objects [9]. However, the dense captioning task is more object-oriented, which often focuses on the precise attribute descriptions based on the object appearance and thus the complex 3D geometrical relations among different objects might be ignored (even though they are intrinsically contained in the 3D data). As a result, the generated captions may be monotony.\n\nExcept for 3D dense captioning, visual grounding on 3D point clouds [1,7,14,16,44,47,50] has also attracted increasing research interest. Chen et al. [7] introduced the ScanRefer dataset for localizing objects by using natural language descriptions. Most recent 3D visual grounding methods [7,16,47] are composed of two stages. In the first stage, a 3D object detector or a panoptic segmentation model is applied to generate the target object proposals from the input scenes. In the second stage, a referring module is used to match the most relevant regions from the selected object proposals and the query sentences. These methods mainly focus on how to model the complex relations based on the object detection results, and pay less attention to the appearance features that characterize different objects, especially the objects within the same class. In other words, the current grounding methods are more relation-oriented.\n\nOur joint framework takes advantage of the overlooked attribute information in the grounding task through the help of the more object-oriented captioning task, and employs the relatively less explored relation information in the captioning task to increase the variety of generated sentences with the help of the more relation-oriented grounding task.\n\n\nMethodology\n\nIn this section, we describe the technical details of our framework. As shown in Fig. 1(a), our framework consists of three modules: 1) the object detection module, 2) the attribute and relation-aware feature enhancement module, and 3) the task-specific captioning head and grounding head. The object detection module and feature enhancement module are task-agnostic and shared by both tasks. The captioning and grounding heads are task-specific with the lightweight transformer-based network structures for the captioning and grounding tasks, respectively. Specifically, the point clouds are encoded by the VoteNet [31] object detection module with an improved bounding box modeling method to more precisely locate the salient objects and produce the initial object proposals. Then the proposal features are enhanced through a task-agnostic attribute and relation-aware feature enhancement module to generate the enhanced object proposals. The enhanced object proposals are then fed into the captioning head and grounding heading for the dense captioning task and the visual grounding task, respectively, and generate the final result for each task.\n\n\nDetection Module\n\nThe input of the detection module is the point cloud P \u2208 R N \u00d7(3+K) , which represents the whole 3D scene by N 3D coordinates together with K-dimensional auxiliary features. Here, we adopt the same 132-dimensional auxiliary features as in [7,9], which include the pretrained 128-dimensional multi-view appearance features [7], 3dimensional normals, and 1-dimensional height of each point above the ground.\n\nWe use VoteNet [31] as our detection module. Since the success of both captioning and grounding tasks relies on precise localization of initial object proposals together with discriminative features, we borrow the idea from the anchor-free FCOS method [36] to generate the initial object proposals by predicting the distance between the voting point and each side of the object proposal.\n\n\nAttribute and Relation-aware Feature Enhancement Module\n\nThe initial object proposal features produced by the detection module are discriminative with respect to different object classes, thanks to the detection-related loss. However, they are unaware of the fine-grained object attributes (e.g., object positions, colors, and materials), especially for the within-class objects, and the complex relations among different objects, which are the key to the success of both 3D captioning and 3D grounding tasks. Hence, we further propose an attribute and relation-aware feature enhancement module to strengthen the features for each proposal and better model the relations between proposals. Motivated by the Transformer encoder structure [37], we model the proposal feature enhancement module as two multi-head self-attention layers with additional attribute encoding module and relation encoding module, where the attribute or relation encoding module is composed of several fully connected layers.\n\nThe attribute encoding module. To aggregate the attribute features and the initial object features, we encode the auxiliary bounding box attribute related features (i.e., a 155-dimensional feature via a concatenation operation on the 27-dimensional box center and corner coordinates, and the 128-dimensional multi-view RGB features that potentially contain the attribute information such as colors and materials) into a 128-dimensional attribute embedding by using a fully connected layer. The attribute embedding has the same dimension as the initial object proposal features. It can then be added to the initial proposal features to enhance the initial object features with more attribute information.\n\nThe relation encoding module. Motivated by [50], we also encode the pairwise distances between any two object proposals to capture the complex object relations. Different from [50], we encode not only the (inverse) relative Euclidean distances (i.e., Dist \u2208 R M \u00d7M \u00d71 ) but also three pairwise distances between any two centers of the initial object proposals along x, y, z direction (i.e., D x , D y , D z \u2208 R M \u00d7M \u00d71 ) to better capture object relations along different directions, where M is the number of initial object proposals. All four spatial proximity matrices (D x , D y , D z , and Dist) are then aggregated along the channel dimension and fed into fully connected layers to produce the relation embeddings with the channel dimension H matches the number of attention heads (i.e., H = 4 in our implementation) in the multi-head attention module. Each relation embedding (with the size of M \u00d7 M \u00d7 1) is then added with the similarity matrix (i.e., the so-called attention map) generated from each head of the multi-head self-attention module.\n\nNote the task-agnostic 3D object detector and the feature enhancement module can produce more accurate localization results and improved object features for both captioning and grounding tasks, and thus we can use more lightweight task-specific captioning head and grounding head in our framework which are simpler than the state-of-the-art methods [9,50]. For both task-specific heads, we adopt similar lightweight 1-layer multi-head cross-attention-based network structures together with simple preprocessing modules (i.e., Query/Key/Value generation as shown in Fig. 2) and postprocessing modules (i.e., word prediction or BBox selection).    Figure 2. The Query, Key & Value generation processes for both captioning head and grounding head. For the captioning head, we firstly choose the object of interest to produce the target object proposal. We concatenate the target proposal feature, the tokenized word feature from the previous word and the hidden feature recurrently output by the multi-head cross-attention module, and use fully connected layers to generate the Query. We select K nearest neighbors of the target object proposal as the Key and Value. For the grounding head, the textual input is firstly tokenized and fed into a GRU cell to produce the the Key and Value of the multihead cross-attention module. The Query for the grounding task is the enhanced object proposal features (see Fig. 1(d)).\n\n\nCaptioning Head\n\nThe 3D dense captioning task is to generate descriptions for each detected bounding box from the input point cloud, which is more object-oriented. Thus, the objectness (for accurately locating each object), the attribute information (for reasonably describing the attributes of objects), and the primary context (for further describing the key relations be-tween each object with other objects) of all the objects in a scene are of great importance. Since the object detector and the feature enhancement module can provide rich object class information, attribute features, and global context features, we simply design our captioning head with a 1-layer multi-head cross-attention network structure for effective message passing between the enhanced features from the target object proposal and all other initial object proposals, which will focus more on the primary context features.\n\nFor generating the query (Q) input of the multi-head cross-attention module, we firstly select the target object proposal and then encode the corresponding object features with a fully connected layer. During the training stage, we select the object proposal with the highest IoU score with the ground-truth bounding box as the query object. In the testing stage, we use all object proposals in the scene (after the Non-Maximum Suppression (NMS) process) in a oneby-one fashion as the query object. For the target object proposal, we follow most of the captioning methods [9] to use a recurrent network structure to progressively generate each word of the caption. Then, we recurrently aggregate the hidden feature output by the multi-head cross-attention module and the tokenized word feature of the previous word (which is the ground-truth word in the training stage, and the newly predicted word in the testing stage) with the current query object features. The fused features form the final generated query input.\n\nIn the recursive query generation process, to alleviate the exposure bias [6] in the sequence generation task be-tween the training stage (which uses the ground-truth word) and the testing stage (which uses the previously predicted word), we randomly use the autoregressive strategy during training. In details, we randomly replace 10% of the ground-truth word tokens with the predicted word tokens as the input word feature during the training process.\n\nIn the key (K) and value (V) generation module, we use the k-NN strategy to select the top k object proposals that are located closest to the target proposal based on their center distance in the 3D coordinate space, which filters out the less related objects in the scene. The selected object proposals are used as the key and value for the multi-head cross-attention module. In our experiment, k is empirically set as 20. This strategy is specially designed for the captioning task, because it mainly cares about the most obvious (or primary) relations between the target object and its surrounding objects and the rest of the relation information might be less important to the captioning task.\n\nFinally, the multi-head cross-attention module is followed by a fully connected layer and a simple word prediction module to predict each word of the caption in a oneby-one fashion.\n\n\nGrounding Head\n\nFor the 3D visual grounding task, the inputs include the 3D point clouds of a scene and the text-form language descriptions of one of the objects in the scene, and the task is to locate the object of interest based on the language description. Since the task-agnostic 3D object detector and the feature enhancement module already capture the object attributes and the complex relations among objects in a scene, the grounding head mainly focuses on matching between the given language descriptions and the detected object proposals. The grounding head in our method is more lightweight by simply using a 1-layer multi-head crossattention module instead of multiple stacked cross-attention modules as used in [50] and [14].\n\nThe key (K) and value (V) inputs are generated based on the input language descriptions. Specifically, we use the similar language encoder as in ScanRefer [7]. The input language is firstly encoded by using a pretrained Glove [30] module, and then input to a GRU cell. The output word feature of the GRU cell forms the key (K) and value (V) inputs. Moreover, a global language feature is also generated from the GRU cell to predict the subject category of each sentence. The object proposals are used as the query (Q) input. By using the multi-head cross-attention mechanism between the language descriptions (K & V) and the object proposals (Q), the relationship between the sentence and the detected proposals is well captured.\n\nTo fully explore the contextual relations among the given textual description, we follow [50] to use two data augmentation strategies for both modalities (e.g., randomly erase some words or change the order of the input text for the text input, and randomly copy some object proposals from other scene as the negative samples for enhancing object proposals), please refer to [50] for more details about the two data augmentation strategies.\n\nFinally, a grounding classifier is used to generate the confidence score of each object proposal, and the proposal with the highest prediction score is considered as the final grounding result.\n\n\nTraining details\n\nThe loss function of our framework is a combination of the detection loss L detection , the grounding loss L grounding and the captioning loss L captioning .\n\nThe object detection loss is similar to that used in Qi et al. [31] for the ScanNet dataset [12], where L detection = 10L vote-reg + L objn-cls + L sem-cls + 200L boundary-reg , except that we replace the bounding box classification loss L box-cls and the regression loss L box-reg in [7,31] with the boundary regression loss L boundary-reg [36]. For the visual grounding task, we apply the similar loss function as used in ScanRefer [7], which is a combination of the localization loss L loc for visual grounding and an auxiliary language-to-object classification loss L cls to enhance the subject classification of the input sentence, and L grounding = L loc + L cls . For the dense captioning task, we input the ground-truth words (or the predicted words with a probability of 10%) sequentially and L captioning is the average cross-entropy loss over all generated words. The final loss is a linear combination of these loss terms, i.e., L = L detection + 0.3L grounding + 0.2L captioning , where the trade-off parameters are empirically set for balancing different loss terms.\n\n\nExperiments\n\n\nDatasets and implementation details\n\nVisual Grounding Dataset: We use the ScanRefer [7] dataset to evaluate our method for the visual grounding task. The ScanRefer dataset contains 51, 583 textual descriptions about 11, 046 objects from 800 scenes. The overall accuracy and the accuracies on both\"unique\" and \"multiple\" subsets are reported. We label each grounding data as \"unique\" if it only contains a single object from its class in the scene, otherwise it will be labeled as \"multiple\". For this dataset, we use Acc@0.25IoU and Acc@0.5IoU as our evaluation metrics. We also compare our method with the baseline methods on both the validation set and the online test set available at the ScanRefer's benchmark website 1 [7] dataset. We report the percentage of the correctly predicted bounding boxes whose IoU scores with the ground-truth boxes are larger than 0.25 and 0.5, respectively. The results on both \"unique\" and \"multiple\" subsets are also reported. [*]: Note the InstanceRefer [47] method filters the predicted 3D proposals based on the object class prediction results such that this method only selects the target object proposal from the proposals in the same class, which simplifies the 3D visual grounding problem. This strategy is not adopted in our work. indicate the start and end of the description, and thus the textual descriptions for ScanRefer and Scan2Cap datasets are different. As a sub-dataset of ReferIt3D [1], Nr3D is also built based on ScanNet with additional textual descriptions, and it contains 41, 503 samples collected by ReferItGame. We use the same metric as used for performance evaluation on the Scan2Cap dataset.\n\nSpecifically, the metric for performance evaluation on these two 3D captioning datasets combines the standard image captioning metrics under different IoU scores between the predicted bounding boxes and the target bounding boxes. The combined metric is defined as m@kIoU =\n1 P P i=0 m i u i , where u i \u2208 {0,\n1} is set to 1 if the detection IoU score for the i-th bounding box is greater than k, and 0 otherwise. We use m i to represent the captioning metrics such as CiDEr [38], BLEU [28], METEOR [5] and ROUGE-L [19], which are respectively abbreviated as C, B-4, M and R in the following tables. P is the number of ground-truth or detected object bounding boxes.\n\nImplementation Details. We follow [50] to use 8 sentences for each scene from both datasets when training our framework. Our experiment is carried out on the machine with a single NVIDIA 11GB 2080Ti GPU and it tasks 200 epochs to train our framework on both ScanRefer [7] and Scan2Cap [9] datasets with a batch size of 10 in each iteration (i.e., there are 80 sentences from 10 point clouds). We apply the cosine learning rate decay strategy with the AdamW optimizer and a weight decay factor of 1e-5 to train our method. We empirically set the initial learning rate as 2e-3 for the detector, and 5e-4 for other modules of our framework (i.e., the feature enhancement module and two task-specific heads). In addition, the captioning task with the cross-entropy loss is prone to overfitting, so we only add the captioning loss during the last 50 epochs.\n\n\nComparison with the state-of-the-art methods\n\nFollowing the works ScanRefer [7] and Scan2Cap [9], we report the results under both \"3D Only\" and \"2D + 3D\" settings according to whether the auxiliary features are used. Under the \"3D Only\" setting, we use \"xyz + RGB + normals\" as the auxiliary features. Under the \"2D + 3D\" setting, the auxiliary features contain \"xyz + multiviews + normals\", where \"multiviews\" means multiview image features from a pretrained ENet [29], and \"normals\" means the normal vectors from point clouds.\n\nIn Table 1 and Table 2, we compare the dense captioning and visual grounding results of our framework with several state-of-the-art methods on both ScanRefer [7] and Scan2Cap [9] datasets. Specifically, on the Scan-Refer dataset, we compare our method with the 3D instance segmentation-based methods TGNN [16] and In-stanceRefer [47] and the 3D detection-based methods including ScanRefer [7] and 3DVG-Transformer [50]. On the Scan2Cap dataset, we compare our method with the state-of-the-art 3D detection-based method Scan2Cap [9] and VoteNetRetr [31].\n\nFrom Table 1, we observe that our method outperforms the baseline methods for the visual grounding task. Note that we use a simpler network structure when compared with the state-of-the-art method 3DVG-Transformer [50], so the results validate that our joint learning framework can benefit the grounding task with only a lightweight grounding head. Specifically, in terms of Acc@0.25 and Acc@0.5 metrics, our method achieves around 1.9% and 2.6% improvements in the \"overall\" case when compared with 3DVG-Transformer [50] on the validation set under the \"2D+3D\" setting. When compared with other detectionbased methods, our method achieves more improvement on the \"Unique\" subset, possibly because the attribute information of the objects plays a more important role in the \"Unique\" subset when there is no confusing objects from the same category in the scene. The results also verify that the object-oriented captioning task enhances the grounding performance by providing more attribute information. Note that the baseline methods InstanceRefer [47] and TGNN [16] use the extra instance segmentation masks for generating 3D proposals, while the InstanceRefer [47] method further filters the instances based on the semantic prediction results, namely, it only retains the instances from the same predicted class for generating the visual grounding results. Possibly due to these two aspects, the InstanceRefer [47] method achieve good results in the \"Unique\" subset. In contrast to [16,47], our work only relies on the detection results, and it still outperforms both methods in both \"Multiple\" and \"Overall\" cases.\n\nWhen compared with the baseline method \"Scan2Cap\", from the results in Table 2, we observe that our joint learning framework using a simple feature enhancement module and a lightweight captioning head achieves significant performance improvement for the captioning task. Under the \"2D+3D\" setting, our method achieves remarkable performance improvement of 10.4%, 7.71% and 6.32% in terms of C@0.5IoU, B-4@0.5IoU and R@0.5IoU, respectively. For this task, the improvement comes from both network structure design (e.g., the attribute and relation aware feature enhancement module, and the lightweight captioning head) and the joint training strategy. The contribution of each module will be discussed in the ablation study below.\n\n\nAblation Study\n\nEffectiveness of the feature enhancement module and the joint training strategy. To evaluate the effectiveness of the proposed task-agnostic feature enhancement module as well as the joint training strategy, we conduct the ablation study and report the corresponding results in Table 3. Without using the joint training strategy, the alternative method \"w/o Grounding Head\" (resp., \"w/o Captioning Head\") means we train the two separate networks consisting of two task-agnostic modules and the captioning head (resp., grounding head) for the 3D dense captioning task (resp., the visual grounding task). \"w/o Feature Enhancement\" means we remove the \"attribute & relation aware feature enhancement\" module in our joint learning framework. For both dense captioning and the visual grounding tasks, our complete 3DJCG method based on the default training data (i.e., from both Scan2Cap and ScanRefer datasets) outperforms those alternative methods, which indicate both strategies contribute to the final performance improvement to certain degree. Does performance improvement come from more training data? Our joint training framework uses both the captioning and grounding training data, in which the only difference is the textual descriptions (i.e., the descriptions used for the grounding task are relatively longer or with more complex relations, while the dense captions are shorter textual descriptions focusing more on the object class and the corresponding attributes). Hence, we conduct the experiments to verify whether the performance improvement is due to the utilization of more training data (i.e., more textual descriptions from both tasks).\n\nIn Table 3 (a) and (b), 3DJCG (\"Captioning Data Only\" (resp., 3DJCG (\"Grounding Data Only\")) indicates that we only use the 3D captioning dataset Scan2Cap [9] (resp., the 3D visual grounding dataset ScanRefer [7]) when training our joint learning framework including both captioning and grounding heads and the two task-agnostic modules. Note both Scan2Cap and ScanRefer datasets can be readily used as the training data for these two tasks. By default, we use both datasets as the default training data when training our joint learning framework.\n\nThe results show that our 3DJCG framework using \"Cap- Table 3. Comparison of the visual grounding results under the \"2D+3D\" setting and the dense captioning results based on the correctly predicted bounding boxes whose IoU scores with the ground-truth boxes are larger than 0.5. In the \"Network Modules\" column, for better presentation, we label our detector, the feature enhancement module, the captioning head and the grounding head as \"DE\", \"FE\", \"CH\" and \"GH\", respectively. tioning Data Only\" (resp., \"Grounding Data Only\") generally improves the performance for the captioning task (resp., the grounding task) when compared to the alternative method 3DJCG (\"w/o Grounding Head\") (resp., 3DJCG (\"w/o Captioning Head\")), especially for the dense captioning task. The results validate that the performance gains come from both strategies (i.e., our network design and utilization of the additional training data). Moreover, the improved results from our joint learning framework under \"Captioning Data Only\" and \"Grounding Data Only\" settings also verify that our joint framework can also inspire the network design of each individual task. Experiments on the Nr3D [1] dataset. We also take the dense captioning task on the Nr3D dataset as an example to evaluate our proposed framework when training from scratch or using the fine-tuning strategy. \"3DJCG-C (From Scratch)\" indicates that we train our 3DJCG-C network from scratch without using any pre-training strategies. \"3DJCG-C* (Finetune)\" indicates we fine-tune the pretrained model based on the Nr3D dataset. Note the pretrained model is learnt based on both ScanRefer and Scan2Cap datasets, and we also remove \"Grounding Head\" before performing the finetune process. We also list the results of the baseline method Scan2Cap trained from scratch based on the Nr3D dataset. As shown in Table 4, our method \"3DJCG-C (From Scratch)\" outperforms the baseline method \"Scan2Cap [9]\", which further verifies the effectiveness of our newly designed network structure. We also observe that our \"3DJCG-C* (Finetune)\" method further improves \"3DJCG-C (From Scratch)\", which demonstrates that the results of our framework could also be boosted by using the fine-tuning strategy.\n\n\nConclusion and Future Work\n\nObserving the shared and complementary properties of two different but closely related tasks 3D dense captioning and 3D visual grounding, we propose a unified framework to jointly solve the two tasks in a synergistic manner. In our framework, the task-agnostic modules are responsible for the precise object localization, the enhancement of the geometry and the fine-grained attribute features, and fully exploration of the complex geometrical relations between objects in a 3D scene, while the task-specific lightweight captioning head and grounding head solve the two tasks, respectively. The experimental results validate the effectiveness of the proposed framework for both tasks. While the joint framework improves the performance of both tasks, the performance improvement for the visual grounding task is not as significant as that for the dense captioning task. In our future work, we will develop more advanced joint training framework to further improve the 3D visual grounding performance.\n\nFigure 1 .\n1(a) The overview of our framework. (b) The attribute and relation aware feature enhancement module. (c) The captioning head within our framework (d) The grounding head within our framework. \"FC\" means the fully connected layer.\n\n\n. Visual Captioning Datasets: Scan2Cap [9] is a dense captioning dataset for 3D scenes. The descriptions that are longer than 30 tokens in the ScanRefer dataset are truncated and two special tokens [SOS] and [EOS] are added to Table 1. Comparison of the visual grounding results from different methods on the ScanRefer\n\n( a )\naThe 3D dense captioning results on the dataset Scan2Cap[9] ScanRefer DE FE CH GH B-4@0.5 C@0.5 R@0.5 M@0.5 3DJCG (w/o Grounding Head) / 3DJCG-CTable 4. The dense captioning results of different methods and different training strategies on the Nr3D dataset from ReferIt3D [1]. B-4@0.5 C@0.5 R@0.5 M@0.5 Scan2Cap [9] 17.24 27.47 49.06 21.80 3DJCG-C (From Scratch) 20.45 33.03 51.73 23.05 3DJCG-C* (Finetune) 22.82 38.06 52.99 23.77Training Dataset(s) \nNetwork Modules \nDense Captioning Results \nScan2Cap 26.24 \n45.04 \n46.69 \n23.27 \n3DJCG (w/o Feature Enhancement) \n29.08 \n47.67 \n49.58 \n23.78 \n3DJCG (Captioning Data Only) \n30.40 \n47.29 \n50.29 \n23.91 \n3DJCG (Default Training Data) \n31.03 \n49.48 \n50.80 \n24.22 \n\n(b) The 3D visual grounding results on the dataset ScanRefer [7] \n\nTraining Dataset(s) \nNetwork Modules \nVisual Grounding Results \nScan2Cap ScanRefer DE FE CH GH Unique@0.5 Multiple@0.5 Overall@0.5 \n3DJCG (w/o Captioning Head) / 3DJCG-G \n62.60 \n30.48 \n36.72 \n3DJCG (w/o Feature Enhancement) \n63.20 \n28.36 \n35.12 \n3DJCG (Grounding Data Only) \n64.50 \n30.29 \n36.93 \n3DJCG (Default Training Data) \n64.34 \n30.82 \n37.33 \n\n\nhttp://kaldir.vc.in.tum.de/scanrefer_benchmark\n\nReferit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, ECCV. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In ECCV, pages 422-440, 2020. 1, 2, 6, 8\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, CVPR. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, pages 6077-6086, 2018. 2\n\nReasoning about pragmatics with neural listeners and speakers. Jacob Andreas, Dan Klein, EMNLP. 2Jacob Andreas and Dan Klein. Reasoning about pragmatics with neural listeners and speakers. In EMNLP, 2016. 2\n\nVQA: visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, ICCV. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In ICCV, pages 2425-2433, 2015. 2\n\nMeteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, ACL Workshop. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with hu- man judgments. In ACL Workshop, pages 65-72, 2005. 6\n\nScheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, NeurIPS. 284Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. NeurIPS, 28, 2015. 4\n\nScanRefer: 3D object localization in RGB-D scans using natural language. Dave Zhenyu Chen, X Angel, Matthias Chang, Nie\u00dfner, ECCV. 7Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. ScanRefer: 3D object localization in RGB-D scans using natural language. In ECCV, pages 202-221, 2020. 1, 2, 3, 5, 6, 7, 8\n\nUNITER: universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, ECCV. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: universal image-text representation learning. In ECCV, pages 104-120, 2020. 2\n\nContext-aware dense captioning in rgb-d scans. Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, Chang, CVPR. 2Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, and Angel X Chang. Scan2Cap: Context-aware dense captioning in rgb-d scans. In CVPR, pages 3193-3203, 2021. 1, 2, 3, 4, 5, 6, 7, 8\n\nBack-tracing representative points for votingbased 3d object detection in point clouds. Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, Dong Xu, CVPR. Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, and Dong Xu. Back-tracing representative points for voting- based 3d object detection in point clouds. In CVPR, pages 8963-8972, 2021. 2\n\nMeshed-memory transformer for image captioning. Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara, CVPR. 2020Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-memory transformer for image cap- tioning. In CVPR, pages 10578-10587, 2020. 2\n\nScanNet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, CVPR. Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, pages 5828-5839, 2017. 5\n\nJointPruning: Pruning networks along multiple dimensions for efficient point cloud processing. Jinyang Guo, Jiaheng Liu, Dong Xu, IEEE TCSVT. 2Jinyang Guo, Jiaheng Liu, and Dong Xu. JointPruning: Pruning networks along multiple dimensions for efficient point cloud processing. IEEE TCSVT, 2021. 2\n\nTransrefer3d: Entity-andrelation aware transformer for fine-grained 3d visual grounding. Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, Si Liu, ACM MM. 25Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, and Si Liu. Transrefer3d: Entity-and- relation aware transformer for fine-grained 3d visual ground- ing. In ACM MM, pages 2344-2352, 2021. 2, 5\n\nNatural language object retrieval. Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell, CVPR. Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. Natural language object retrieval. In CVPR, pages 4555-4564, 2016. 2\n\nText-guided graph neural networks for referring 3d instance segmentation. Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, Tyng-Luh Liu, AAAI. 67Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for refer- ring 3d instance segmentation. In AAAI, pages 1610-1618, 2021. 2, 6, 7\n\nDensecap: Fully convolutional localization networks for dense captioning. Justin Johnson, Andrej Karpathy, Li Fei-Fei, CVPR. Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense caption- ing. In CVPR, pages 4565-4574, 2016. 2\n\nDense relational captioning: Triple-stream networks for relationship-based captioning. Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon, CVPR. Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. Dense relational captioning: Triple-stream networks for relationship-based captioning. In CVPR, pages 6271- 6280, 2019. 2\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, ACL Workshop. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In ACL Workshop, pages 74-81, 2004. 6\n\nVotehmr: Occlusionaware voting network for robust 3d human mesh recovery from partial point clouds. Guanze Liu, Yu Rong, Lu Sheng, ACM MM. Guanze Liu, Yu Rong, and Lu Sheng. Votehmr: Occlusion- aware voting network for robust 3d human mesh recovery from partial point clouds. In ACM MM, pages 955-964, 2021. 2\n\nRefer-it-in-rgbd: A bottom-up approach for 3d visual grounding in RGBD images. Haolin Liu, Anran Lin, Xiaoguang Han, Lei Yang, Yizhou Yu, Shuguang Cui, CVPR. Haolin Liu, Anran Lin, Xiaoguang Han, Lei Yang, Yizhou Yu, and Shuguang Cui. Refer-it-in-rgbd: A bottom-up ap- proach for 3d visual grounding in RGBD images. In CVPR, pages 6032-6041, 2021. 1\n\nGeometryMotion-Net: A strong two-stream baseline for 3d action recognition. Jiaheng Liu, Dong Xu, IEEE TCSVT. 2Jiaheng Liu and Dong Xu. GeometryMotion-Net: A strong two-stream baseline for 3d action recognition. IEEE TCSVT, pages 4711-4721, 2021. 2\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, NeurIPS. 32Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, volume 32, 2019. 2\n\n12-in-1: Multi-task vision and language representation learning. Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee, CVPR. 2020Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In CVPR, pages 10437- 10446, 2020. 2\n\nGeneration and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, CVPR. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, pages 11-20, 2016. 2\n\nModeling context between objects for referring expression understanding. K Varun, Vlad I Nagaraja, Larry S Morariu, Davis, ECCV. Varun K. Nagaraja, Vlad I. Morariu, and Larry S. Davis. Modeling context between objects for referring expression understanding. In ECCV, pages 792-807, 2016. 2\n\nMulti-task learning of hierarchical vision-language representation. Takayuki Duy-Kien Nguyen, Okatani, CVPR. Duy-Kien Nguyen and Takayuki Okatani. Multi-task learn- ing of hierarchical vision-language representation. In CVPR, pages 10492-10501, 2019. 2\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, ACL. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, pages 311-318, 2002. 6\n\nEnet: A deep neural network architecture for real-time semantic segmentation. Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello, arXiv:1606.02147In arXiv preprintAdam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eu- genio Culurciello. Enet: A deep neural network architec- ture for real-time semantic segmentation. In arXiv preprint arXiv:1606.02147, 2016. 7\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, EMNLP. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, pages 1532-1543, 2014. 5\n\nDeep hough voting for 3d object detection in point clouds. Charles R Qi, Or Litany, Kaiming He, Leonidas J Guibas, ICCV. 67Charles R. Qi, Or Litany, Kaiming He, and Leonidas J. Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, pages 9277-9286, 2019. 3, 5, 6, 7\n\nPointNet: Deep learning on point sets for 3d classification and segmentation. Hao Charles Ruizhongtai Qi, Kaichun Su, Leonidas J Mo, Guibas, CVPR. Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. PointNet: Deep learning on point sets for 3d classification and segmentation. In CVPR, pages 652- 660, 2017. 2\n\nPointNet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, NeurIPS. 30Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. PointNet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, volume 30, 2017. 2\n\nVoxelContext-Net: An octree based framework for point cloud compression. Zizheng Que, Guo Lu, Dong Xu, CVPR. Zizheng Que, Guo Lu, and Dong Xu. VoxelContext-Net: An octree based framework for point cloud compression. In CVPR, pages 6042-6051, 2021. 2\n\nSTVGBert: A visuallinguistic transformer based framework for spatio-temporal video grounding. Rui Su, Qian Yu, Dong Xu, ICCV. Rui Su, Qian Yu, and Dong Xu. STVGBert: A visual- linguistic transformer based framework for spatio-temporal video grounding. In ICCV, pages 14618-14627, 2021. 2\n\nFcos: Fully convolutional one-stage object detection. Zhi Tian, Chunhua Shen, Hao Chen, Tong He, ICCV. 35Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In ICCV, pages 9627-9636, 2019. 3, 5\n\n. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need. In NeurIPS. 303Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Il- lia Polosukhin. Attention is all you need. In NeurIPS, vol- ume 30, 2017. 3\n\nCider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, CVPR. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evalua- tion. In CVPR, pages 4566-4575, 2015. 6\n\nCross-dataset point cloud recognition using deep-shallow domain adaptation network. Feiyu Wang, Wen Li, Dong Xu, IEEE TIP. 2Feiyu Wang, Wen Li, and Dong Xu. Cross-dataset point cloud recognition using deep-shallow domain adaptation net- work. IEEE TIP, pages 7364-7377, 2021. 2\n\nSequential point cloud upsampling by exploiting multi-scale temporal dependency. Kaisiyuan Wang, Lu Sheng, Shuhang Gu, Dong Xu, IEEE TCSVT. 2Kaisiyuan Wang, Lu Sheng, Shuhang Gu, and Dong Xu. Se- quential point cloud upsampling by exploiting multi-scale temporal dependency. IEEE TCSVT, pages 4686-4696, 2021. 2\n\nMultilevel language and vision integration for text-to-clip retrieval. Huijuan Xu, Kun He, A Bryan, Leonid Plummer, Stan Sigal, Kate Sclaroff, Saenko, AAAI. Huijuan Xu, Kun He, Bryan A Plummer, Leonid Sigal, Stan Sclaroff, and Kate Saenko. Multilevel language and vision integration for text-to-clip retrieval. In AAAI, pages 9062- 9069, 2019. 2\n\n. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov, S Richard, Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S.\n\nShow, attend and tell: Neural image caption generation with visual attention. Yoshua Zemel, Bengio, ICML. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, pages 2048-2057, 2015. 2\n\nTAP: text-aware pre-training for text-vqa and textcaption. Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Flor\u00eancio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo, CVPR. Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Flor\u00eancio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo Luo. TAP: text-aware pre-training for text-vqa and text- caption. In CVPR, pages 8751-8761, 2021. 2\n\nSAT: 2d semantics assisted training for 3d visual grounding. ICCV. Zhengyuan Yang, Songyang Zhang, Liwei Wang, Jiebo Luo, 6Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. SAT: 2d semantics assisted training for 3d visual grounding. ICCV, pages 1856-1866, 2021. 1, 2, 6\n\nMAttNet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, CVPR. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L. Berg. MAttNet: Modular at- tention network for referring expression comprehension. In CVPR, pages 4555-4564, 2018. 2\n\nA joint speaker-listener-reinforcer model for referring expressions. Licheng Yu, Hao Tan, Mohit Bansal, Tamara L Berg, CVPR. Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L. Berg. A joint speaker-listener-reinforcer model for referring expres- sions. In CVPR, pages 7282-7290, 2017. 2\n\nInstanceRefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. ICCV. Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Zhen Li, Shuguang Cui, 7Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Zhen Li, and Shuguang Cui. InstanceRefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. ICCV, pages 1791- 1800, 2021. 1, 2, 6, 7\n\nVinvl: Revisiting visual representations in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, CVPR. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, pages 5579-5588, 2021. 2\n\nSRDAN: Scaleaware and range-aware domain adaptation network for crossdataset 3d object detection. Weichen Zhang, Wen Li, Dong Xu, CVPR. Weichen Zhang, Wen Li, and Dong Xu. SRDAN: Scale- aware and range-aware domain adaptation network for cross- dataset 3d object detection. In CVPR, pages 6769-6779, 2021. 2\n\n3DVG-Transformer: Relation modeling for visual grounding on point clouds. Lichen Zhao, Daigang Cai, Lu Sheng, Dong Xu, ICCV. 67Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3DVG- Transformer: Relation modeling for visual grounding on point clouds. In ICCV, pages 2928-2937, 2021. 1, 2, 3, 5, 6, 7\n\nTransformer3D-Det: Improving 3d object detection by vote refinement. Lichen Zhao, Jinyang Guo, Dong Xu, Lu Sheng, IEEE TCSVT. 2Lichen Zhao, Jinyang Guo, Dong Xu, and Lu Sheng. Transformer3D-Det: Improving 3d object detection by vote refinement. IEEE TCSVT, pages 4735-4746, 2021. 2\n", "annotations": {"author": "[{\"end\":155,\"start\":97},{\"end\":214,\"start\":156},{\"end\":272,\"start\":215},{\"end\":347,\"start\":273},{\"end\":414,\"start\":348}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":105},{\"end\":167,\"start\":163},{\"end\":225,\"start\":220},{\"end\":281,\"start\":276},{\"end\":355,\"start\":353}]", "author_first_name": "[{\"end\":104,\"start\":97},{\"end\":162,\"start\":156},{\"end\":219,\"start\":215},{\"end\":275,\"start\":273},{\"end\":352,\"start\":348}]", "author_affiliation": "[{\"end\":154,\"start\":110},{\"end\":213,\"start\":169},{\"end\":271,\"start\":227},{\"end\":346,\"start\":302},{\"end\":413,\"start\":379}]", "title": "[{\"end\":94,\"start\":1},{\"end\":508,\"start\":415}]", "venue": null, "abstract": "[{\"end\":1607,\"start\":538}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1783,\"start\":1780},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1811,\"start\":1808},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1813,\"start\":1811},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1816,\"start\":1813},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":1819,\"start\":1816},{\"end\":2472,\"start\":2466},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2686,\"start\":2683},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2688,\"start\":2686},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2690,\"start\":2688},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2693,\"start\":2690},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2696,\"start\":2693},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2699,\"start\":2696},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2702,\"start\":2699},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6964,\"start\":6961},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6978,\"start\":6975},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7000,\"start\":6997},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7304,\"start\":7300},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7307,\"start\":7304},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7310,\"start\":7307},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7313,\"start\":7310},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7354,\"start\":7351},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7357,\"start\":7354},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7360,\"start\":7357},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7363,\"start\":7360},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7366,\"start\":7363},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7397,\"start\":7394},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7399,\"start\":7397},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7402,\"start\":7399},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7436,\"start\":7432},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7701,\"start\":7698},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7704,\"start\":7701},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8204,\"start\":8201},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8207,\"start\":8204},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8210,\"start\":8207},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8213,\"start\":8210},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8216,\"start\":8213},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8219,\"start\":8216},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8330,\"start\":8326},{\"end\":8879,\"start\":8840},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9006,\"start\":9003},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9145,\"start\":9142},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9571,\"start\":9568},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9573,\"start\":9571},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9576,\"start\":9573},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9579,\"start\":9576},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9582,\"start\":9579},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9585,\"start\":9582},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9588,\"start\":9585},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9653,\"start\":9650},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9793,\"start\":9790},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9796,\"start\":9793},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9799,\"start\":9796},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11418,\"start\":11414},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12211,\"start\":12208},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12213,\"start\":12211},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12294,\"start\":12291},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12395,\"start\":12391},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12632,\"start\":12628},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13507,\"start\":13503},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14518,\"start\":14514},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14651,\"start\":14647},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15878,\"start\":15875},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15881,\"start\":15878},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18424,\"start\":18421},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18945,\"start\":18942},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20934,\"start\":20930},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20943,\"start\":20939},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21104,\"start\":21101},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21176,\"start\":21172},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21770,\"start\":21766},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22056,\"start\":22052},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22559,\"start\":22555},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22588,\"start\":22584},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22780,\"start\":22777},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22783,\"start\":22780},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22837,\"start\":22833},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22929,\"start\":22926},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23676,\"start\":23673},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24312,\"start\":24311},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24316,\"start\":24313},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24585,\"start\":24581},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25030,\"start\":25027},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25726,\"start\":25722},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25737,\"start\":25733},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25749,\"start\":25746},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25766,\"start\":25762},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25953,\"start\":25949},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26186,\"start\":26183},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26203,\"start\":26200},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26849,\"start\":26846},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26866,\"start\":26863},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27240,\"start\":27236},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27462,\"start\":27459},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27479,\"start\":27476},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27610,\"start\":27606},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":27634,\"start\":27630},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27693,\"start\":27690},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27719,\"start\":27715},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27832,\"start\":27829},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27853,\"start\":27849},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":28074,\"start\":28070},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":28377,\"start\":28373},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28908,\"start\":28904},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28922,\"start\":28918},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29022,\"start\":29018},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29272,\"start\":29268},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29344,\"start\":29340},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29347,\"start\":29344},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32036,\"start\":32033},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32090,\"start\":32087},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33598,\"start\":33595},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34362,\"start\":34359},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36313,\"start\":36310}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35925,\"start\":35685},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36246,\"start\":35926},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37380,\"start\":36247}]", "paragraph": "[{\"end\":2207,\"start\":1623},{\"end\":3493,\"start\":2209},{\"end\":4607,\"start\":3495},{\"end\":6407,\"start\":4609},{\"end\":7130,\"start\":6409},{\"end\":9498,\"start\":7147},{\"end\":10429,\"start\":9500},{\"end\":10782,\"start\":10431},{\"end\":11948,\"start\":10798},{\"end\":12374,\"start\":11969},{\"end\":12763,\"start\":12376},{\"end\":13764,\"start\":12823},{\"end\":14469,\"start\":13766},{\"end\":15524,\"start\":14471},{\"end\":16941,\"start\":15526},{\"end\":17847,\"start\":16961},{\"end\":18866,\"start\":17849},{\"end\":19321,\"start\":18868},{\"end\":20020,\"start\":19323},{\"end\":20203,\"start\":20022},{\"end\":20944,\"start\":20222},{\"end\":21675,\"start\":20946},{\"end\":22117,\"start\":21677},{\"end\":22312,\"start\":22119},{\"end\":22490,\"start\":22333},{\"end\":23572,\"start\":22492},{\"end\":25246,\"start\":23626},{\"end\":25520,\"start\":25248},{\"end\":25913,\"start\":25557},{\"end\":26767,\"start\":25915},{\"end\":27299,\"start\":26816},{\"end\":27854,\"start\":27301},{\"end\":29473,\"start\":27856},{\"end\":30203,\"start\":29475},{\"end\":31876,\"start\":30222},{\"end\":32425,\"start\":31878},{\"end\":34653,\"start\":32427},{\"end\":35684,\"start\":34684}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":25556,\"start\":25521}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27311,\"start\":27304},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27323,\"start\":27316},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27868,\"start\":27861},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29553,\"start\":29546},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30507,\"start\":30500},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31888,\"start\":31881},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32488,\"start\":32481},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34279,\"start\":34272}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1621,\"start\":1609},{\"attributes\":{\"n\":\"2.\"},\"end\":7145,\"start\":7133},{\"attributes\":{\"n\":\"3.\"},\"end\":10796,\"start\":10785},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11967,\"start\":11951},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12821,\"start\":12766},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16959,\"start\":16944},{\"attributes\":{\"n\":\"3.4.\"},\"end\":20220,\"start\":20206},{\"attributes\":{\"n\":\"3.5.\"},\"end\":22331,\"start\":22315},{\"attributes\":{\"n\":\"4.\"},\"end\":23586,\"start\":23575},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23624,\"start\":23589},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26814,\"start\":26770},{\"attributes\":{\"n\":\"4.3.\"},\"end\":30220,\"start\":30206},{\"attributes\":{\"n\":\"5.\"},\"end\":34682,\"start\":34656},{\"end\":35696,\"start\":35686},{\"end\":36253,\"start\":36248}]", "table": "[{\"end\":37380,\"start\":36684}]", "figure_caption": "[{\"end\":35925,\"start\":35698},{\"end\":36246,\"start\":35928},{\"end\":36684,\"start\":36255}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10888,\"start\":10879},{\"end\":16097,\"start\":16091},{\"end\":16180,\"start\":16172},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16939,\"start\":16930}]", "bib_author_first_name": "[{\"end\":37526,\"start\":37521},{\"end\":37544,\"start\":37539},{\"end\":37561,\"start\":37558},{\"end\":37574,\"start\":37567},{\"end\":37594,\"start\":37586},{\"end\":37919,\"start\":37914},{\"end\":37938,\"start\":37930},{\"end\":37948,\"start\":37943},{\"end\":37964,\"start\":37958},{\"end\":37976,\"start\":37972},{\"end\":37993,\"start\":37986},{\"end\":38004,\"start\":38001},{\"end\":38308,\"start\":38303},{\"end\":38321,\"start\":38318},{\"end\":38489,\"start\":38480},{\"end\":38506,\"start\":38497},{\"end\":38522,\"start\":38516},{\"end\":38535,\"start\":38527},{\"end\":38551,\"start\":38546},{\"end\":38560,\"start\":38559},{\"end\":38569,\"start\":38561},{\"end\":38583,\"start\":38579},{\"end\":38885,\"start\":38876},{\"end\":38900,\"start\":38896},{\"end\":39171,\"start\":39167},{\"end\":39185,\"start\":39180},{\"end\":39202,\"start\":39195},{\"end\":39215,\"start\":39211},{\"end\":39480,\"start\":39469},{\"end\":39488,\"start\":39487},{\"end\":39504,\"start\":39496},{\"end\":39769,\"start\":39761},{\"end\":39782,\"start\":39776},{\"end\":39794,\"start\":39787},{\"end\":39804,\"start\":39799},{\"end\":39807,\"start\":39805},{\"end\":39821,\"start\":39815},{\"end\":39832,\"start\":39829},{\"end\":39840,\"start\":39838},{\"end\":39856,\"start\":39848},{\"end\":40113,\"start\":40107},{\"end\":40123,\"start\":40120},{\"end\":40141,\"start\":40133},{\"end\":40434,\"start\":40429},{\"end\":40444,\"start\":40442},{\"end\":40461,\"start\":40452},{\"end\":40471,\"start\":40467},{\"end\":40482,\"start\":40478},{\"end\":40736,\"start\":40728},{\"end\":40751,\"start\":40745},{\"end\":40770,\"start\":40763},{\"end\":40784,\"start\":40780},{\"end\":41034,\"start\":41028},{\"end\":41041,\"start\":41040},{\"end\":41056,\"start\":41049},{\"end\":41070,\"start\":41064},{\"end\":41084,\"start\":41078},{\"end\":41101,\"start\":41093},{\"end\":41429,\"start\":41422},{\"end\":41442,\"start\":41435},{\"end\":41452,\"start\":41448},{\"end\":41720,\"start\":41714},{\"end\":41732,\"start\":41725},{\"end\":41744,\"start\":41739},{\"end\":41757,\"start\":41750},{\"end\":41770,\"start\":41763},{\"end\":41782,\"start\":41778},{\"end\":41792,\"start\":41790},{\"end\":42072,\"start\":42064},{\"end\":42083,\"start\":42077},{\"end\":42094,\"start\":42088},{\"end\":42111,\"start\":42105},{\"end\":42122,\"start\":42118},{\"end\":42137,\"start\":42131},{\"end\":42391,\"start\":42384},{\"end\":42407,\"start\":42399},{\"end\":42424,\"start\":42413},{\"end\":42439,\"start\":42431},{\"end\":42715,\"start\":42709},{\"end\":42731,\"start\":42725},{\"end\":42744,\"start\":42742},{\"end\":43015,\"start\":43007},{\"end\":43027,\"start\":43021},{\"end\":43042,\"start\":43034},{\"end\":43049,\"start\":43047},{\"end\":43052,\"start\":43050},{\"end\":43310,\"start\":43302},{\"end\":43545,\"start\":43539},{\"end\":43553,\"start\":43551},{\"end\":43562,\"start\":43560},{\"end\":43835,\"start\":43829},{\"end\":43846,\"start\":43841},{\"end\":43861,\"start\":43852},{\"end\":43870,\"start\":43867},{\"end\":43883,\"start\":43877},{\"end\":43896,\"start\":43888},{\"end\":44184,\"start\":44177},{\"end\":44194,\"start\":44190},{\"end\":44455,\"start\":44449},{\"end\":44465,\"start\":44460},{\"end\":44477,\"start\":44473},{\"end\":44492,\"start\":44486},{\"end\":44763,\"start\":44757},{\"end\":44775,\"start\":44768},{\"end\":44791,\"start\":44785},{\"end\":44806,\"start\":44802},{\"end\":44821,\"start\":44815},{\"end\":45085,\"start\":45079},{\"end\":45099,\"start\":45091},{\"end\":45116,\"start\":45107},{\"end\":45129,\"start\":45125},{\"end\":45143,\"start\":45139},{\"end\":45145,\"start\":45144},{\"end\":45159,\"start\":45154},{\"end\":45438,\"start\":45437},{\"end\":45450,\"start\":45446},{\"end\":45452,\"start\":45451},{\"end\":45468,\"start\":45463},{\"end\":45470,\"start\":45469},{\"end\":45731,\"start\":45723},{\"end\":45980,\"start\":45973},{\"end\":45996,\"start\":45991},{\"end\":46009,\"start\":46005},{\"end\":46024,\"start\":46016},{\"end\":46274,\"start\":46270},{\"end\":46291,\"start\":46283},{\"end\":46310,\"start\":46303},{\"end\":46323,\"start\":46316},{\"end\":46622,\"start\":46615},{\"end\":46642,\"start\":46635},{\"end\":46662,\"start\":46651},{\"end\":46664,\"start\":46663},{\"end\":46894,\"start\":46887},{\"end\":46896,\"start\":46895},{\"end\":46903,\"start\":46901},{\"end\":46919,\"start\":46912},{\"end\":46932,\"start\":46924},{\"end\":46934,\"start\":46933},{\"end\":47197,\"start\":47194},{\"end\":47229,\"start\":47222},{\"end\":47242,\"start\":47234},{\"end\":47244,\"start\":47243},{\"end\":47525,\"start\":47523},{\"end\":47553,\"start\":47550},{\"end\":47566,\"start\":47558},{\"end\":47568,\"start\":47567},{\"end\":47847,\"start\":47840},{\"end\":47856,\"start\":47853},{\"end\":47865,\"start\":47861},{\"end\":48115,\"start\":48112},{\"end\":48124,\"start\":48120},{\"end\":48133,\"start\":48129},{\"end\":48364,\"start\":48361},{\"end\":48378,\"start\":48371},{\"end\":48388,\"start\":48385},{\"end\":48399,\"start\":48395},{\"end\":48559,\"start\":48553},{\"end\":48573,\"start\":48569},{\"end\":48587,\"start\":48583},{\"end\":48601,\"start\":48596},{\"end\":48618,\"start\":48613},{\"end\":48631,\"start\":48626},{\"end\":48633,\"start\":48632},{\"end\":48647,\"start\":48641},{\"end\":48661,\"start\":48656},{\"end\":48971,\"start\":48960},{\"end\":48990,\"start\":48982},{\"end\":49004,\"start\":49000},{\"end\":49257,\"start\":49252},{\"end\":49267,\"start\":49264},{\"end\":49276,\"start\":49272},{\"end\":49537,\"start\":49528},{\"end\":49546,\"start\":49544},{\"end\":49561,\"start\":49554},{\"end\":49570,\"start\":49566},{\"end\":49838,\"start\":49831},{\"end\":49846,\"start\":49843},{\"end\":49852,\"start\":49851},{\"end\":49866,\"start\":49860},{\"end\":49880,\"start\":49876},{\"end\":49892,\"start\":49888},{\"end\":50115,\"start\":50109},{\"end\":50125,\"start\":50120},{\"end\":50134,\"start\":50130},{\"end\":50151,\"start\":50142},{\"end\":50162,\"start\":50157},{\"end\":50164,\"start\":50163},{\"end\":50182,\"start\":50176},{\"end\":50199,\"start\":50198},{\"end\":50395,\"start\":50389},{\"end\":50624,\"start\":50615},{\"end\":50637,\"start\":50631},{\"end\":50650,\"start\":50642},{\"end\":50659,\"start\":50657},{\"end\":50670,\"start\":50665},{\"end\":50688,\"start\":50682},{\"end\":50698,\"start\":50695},{\"end\":50709,\"start\":50706},{\"end\":50722,\"start\":50717},{\"end\":51023,\"start\":51014},{\"end\":51038,\"start\":51030},{\"end\":51051,\"start\":51046},{\"end\":51063,\"start\":51058},{\"end\":51310,\"start\":51303},{\"end\":51318,\"start\":51315},{\"end\":51331,\"start\":51324},{\"end\":51343,\"start\":51338},{\"end\":51353,\"start\":51350},{\"end\":51363,\"start\":51358},{\"end\":51378,\"start\":51372},{\"end\":51380,\"start\":51379},{\"end\":51670,\"start\":51663},{\"end\":51678,\"start\":51675},{\"end\":51689,\"start\":51684},{\"end\":51704,\"start\":51698},{\"end\":51706,\"start\":51705},{\"end\":52030,\"start\":52024},{\"end\":52039,\"start\":52037},{\"end\":52053,\"start\":52045},{\"end\":52066,\"start\":52060},{\"end\":52078,\"start\":52074},{\"end\":52091,\"start\":52083},{\"end\":52432,\"start\":52423},{\"end\":52446,\"start\":52440},{\"end\":52458,\"start\":52451},{\"end\":52470,\"start\":52463},{\"end\":52480,\"start\":52477},{\"end\":52494,\"start\":52488},{\"end\":52506,\"start\":52501},{\"end\":52521,\"start\":52513},{\"end\":52849,\"start\":52842},{\"end\":52860,\"start\":52857},{\"end\":52869,\"start\":52865},{\"end\":53133,\"start\":53127},{\"end\":53147,\"start\":53140},{\"end\":53155,\"start\":53153},{\"end\":53167,\"start\":53163},{\"end\":53429,\"start\":53423},{\"end\":53443,\"start\":53436},{\"end\":53453,\"start\":53449},{\"end\":53460,\"start\":53458}]", "bib_author_last_name": "[{\"end\":37537,\"start\":37527},{\"end\":37556,\"start\":37545},{\"end\":37565,\"start\":37562},{\"end\":37584,\"start\":37575},{\"end\":37601,\"start\":37595},{\"end\":37928,\"start\":37920},{\"end\":37941,\"start\":37939},{\"end\":37956,\"start\":37949},{\"end\":37970,\"start\":37965},{\"end\":37984,\"start\":37977},{\"end\":37999,\"start\":37994},{\"end\":38010,\"start\":38005},{\"end\":38316,\"start\":38309},{\"end\":38327,\"start\":38322},{\"end\":38495,\"start\":38490},{\"end\":38514,\"start\":38507},{\"end\":38525,\"start\":38523},{\"end\":38544,\"start\":38536},{\"end\":38557,\"start\":38552},{\"end\":38577,\"start\":38570},{\"end\":38590,\"start\":38584},{\"end\":38894,\"start\":38886},{\"end\":38906,\"start\":38901},{\"end\":39178,\"start\":39172},{\"end\":39193,\"start\":39186},{\"end\":39209,\"start\":39203},{\"end\":39223,\"start\":39216},{\"end\":39485,\"start\":39481},{\"end\":39494,\"start\":39489},{\"end\":39510,\"start\":39505},{\"end\":39519,\"start\":39512},{\"end\":39774,\"start\":39770},{\"end\":39785,\"start\":39783},{\"end\":39797,\"start\":39795},{\"end\":39813,\"start\":39808},{\"end\":39827,\"start\":39822},{\"end\":39836,\"start\":39833},{\"end\":39846,\"start\":39841},{\"end\":39860,\"start\":39857},{\"end\":40118,\"start\":40114},{\"end\":40131,\"start\":40124},{\"end\":40149,\"start\":40142},{\"end\":40156,\"start\":40151},{\"end\":40440,\"start\":40435},{\"end\":40450,\"start\":40445},{\"end\":40465,\"start\":40462},{\"end\":40476,\"start\":40472},{\"end\":40485,\"start\":40483},{\"end\":40743,\"start\":40737},{\"end\":40761,\"start\":40752},{\"end\":40778,\"start\":40771},{\"end\":40794,\"start\":40785},{\"end\":41038,\"start\":41035},{\"end\":41047,\"start\":41042},{\"end\":41062,\"start\":41057},{\"end\":41076,\"start\":41071},{\"end\":41091,\"start\":41085},{\"end\":41112,\"start\":41102},{\"end\":41121,\"start\":41114},{\"end\":41433,\"start\":41430},{\"end\":41446,\"start\":41443},{\"end\":41455,\"start\":41453},{\"end\":41723,\"start\":41721},{\"end\":41737,\"start\":41733},{\"end\":41748,\"start\":41745},{\"end\":41761,\"start\":41758},{\"end\":41776,\"start\":41771},{\"end\":41788,\"start\":41783},{\"end\":41796,\"start\":41793},{\"end\":42075,\"start\":42073},{\"end\":42086,\"start\":42084},{\"end\":42103,\"start\":42095},{\"end\":42116,\"start\":42112},{\"end\":42129,\"start\":42123},{\"end\":42145,\"start\":42138},{\"end\":42397,\"start\":42392},{\"end\":42411,\"start\":42408},{\"end\":42429,\"start\":42425},{\"end\":42443,\"start\":42440},{\"end\":42723,\"start\":42716},{\"end\":42740,\"start\":42732},{\"end\":42752,\"start\":42745},{\"end\":43019,\"start\":43016},{\"end\":43032,\"start\":43028},{\"end\":43045,\"start\":43043},{\"end\":43058,\"start\":43053},{\"end\":43314,\"start\":43311},{\"end\":43549,\"start\":43546},{\"end\":43558,\"start\":43554},{\"end\":43568,\"start\":43563},{\"end\":43839,\"start\":43836},{\"end\":43850,\"start\":43847},{\"end\":43865,\"start\":43862},{\"end\":43875,\"start\":43871},{\"end\":43886,\"start\":43884},{\"end\":43900,\"start\":43897},{\"end\":44188,\"start\":44185},{\"end\":44197,\"start\":44195},{\"end\":44458,\"start\":44456},{\"end\":44471,\"start\":44466},{\"end\":44484,\"start\":44478},{\"end\":44496,\"start\":44493},{\"end\":44766,\"start\":44764},{\"end\":44783,\"start\":44776},{\"end\":44800,\"start\":44792},{\"end\":44813,\"start\":44807},{\"end\":44825,\"start\":44822},{\"end\":45089,\"start\":45086},{\"end\":45105,\"start\":45100},{\"end\":45123,\"start\":45117},{\"end\":45137,\"start\":45130},{\"end\":45152,\"start\":45146},{\"end\":45166,\"start\":45160},{\"end\":45444,\"start\":45439},{\"end\":45461,\"start\":45453},{\"end\":45478,\"start\":45471},{\"end\":45485,\"start\":45480},{\"end\":45747,\"start\":45732},{\"end\":45756,\"start\":45749},{\"end\":45989,\"start\":45981},{\"end\":46003,\"start\":45997},{\"end\":46014,\"start\":46010},{\"end\":46028,\"start\":46025},{\"end\":46281,\"start\":46275},{\"end\":46301,\"start\":46292},{\"end\":46314,\"start\":46311},{\"end\":46335,\"start\":46324},{\"end\":46633,\"start\":46623},{\"end\":46649,\"start\":46643},{\"end\":46672,\"start\":46665},{\"end\":46899,\"start\":46897},{\"end\":46910,\"start\":46904},{\"end\":46922,\"start\":46920},{\"end\":46941,\"start\":46935},{\"end\":47220,\"start\":47198},{\"end\":47232,\"start\":47230},{\"end\":47247,\"start\":47245},{\"end\":47255,\"start\":47249},{\"end\":47548,\"start\":47526},{\"end\":47556,\"start\":47554},{\"end\":47571,\"start\":47569},{\"end\":47579,\"start\":47573},{\"end\":47851,\"start\":47848},{\"end\":47859,\"start\":47857},{\"end\":47868,\"start\":47866},{\"end\":48118,\"start\":48116},{\"end\":48127,\"start\":48125},{\"end\":48136,\"start\":48134},{\"end\":48369,\"start\":48365},{\"end\":48383,\"start\":48379},{\"end\":48393,\"start\":48389},{\"end\":48402,\"start\":48400},{\"end\":48567,\"start\":48560},{\"end\":48581,\"start\":48574},{\"end\":48594,\"start\":48588},{\"end\":48611,\"start\":48602},{\"end\":48624,\"start\":48619},{\"end\":48639,\"start\":48634},{\"end\":48654,\"start\":48648},{\"end\":48672,\"start\":48662},{\"end\":48980,\"start\":48972},{\"end\":48998,\"start\":48991},{\"end\":49011,\"start\":49005},{\"end\":49262,\"start\":49258},{\"end\":49270,\"start\":49268},{\"end\":49279,\"start\":49277},{\"end\":49542,\"start\":49538},{\"end\":49552,\"start\":49547},{\"end\":49564,\"start\":49562},{\"end\":49573,\"start\":49571},{\"end\":49841,\"start\":49839},{\"end\":49849,\"start\":49847},{\"end\":49858,\"start\":49853},{\"end\":49874,\"start\":49867},{\"end\":49886,\"start\":49881},{\"end\":49901,\"start\":49893},{\"end\":49909,\"start\":49903},{\"end\":50118,\"start\":50116},{\"end\":50128,\"start\":50126},{\"end\":50140,\"start\":50135},{\"end\":50155,\"start\":50152},{\"end\":50174,\"start\":50165},{\"end\":50196,\"start\":50183},{\"end\":50207,\"start\":50200},{\"end\":50401,\"start\":50396},{\"end\":50409,\"start\":50403},{\"end\":50629,\"start\":50625},{\"end\":50640,\"start\":50638},{\"end\":50655,\"start\":50651},{\"end\":50663,\"start\":50660},{\"end\":50680,\"start\":50671},{\"end\":50693,\"start\":50689},{\"end\":50704,\"start\":50699},{\"end\":50715,\"start\":50710},{\"end\":50726,\"start\":50723},{\"end\":51028,\"start\":51024},{\"end\":51044,\"start\":51039},{\"end\":51056,\"start\":51052},{\"end\":51067,\"start\":51064},{\"end\":51313,\"start\":51311},{\"end\":51322,\"start\":51319},{\"end\":51336,\"start\":51332},{\"end\":51348,\"start\":51344},{\"end\":51356,\"start\":51354},{\"end\":51370,\"start\":51364},{\"end\":51385,\"start\":51381},{\"end\":51673,\"start\":51671},{\"end\":51682,\"start\":51679},{\"end\":51696,\"start\":51690},{\"end\":51711,\"start\":51707},{\"end\":52035,\"start\":52031},{\"end\":52043,\"start\":52040},{\"end\":52058,\"start\":52054},{\"end\":52072,\"start\":52067},{\"end\":52081,\"start\":52079},{\"end\":52095,\"start\":52092},{\"end\":52438,\"start\":52433},{\"end\":52449,\"start\":52447},{\"end\":52461,\"start\":52459},{\"end\":52475,\"start\":52471},{\"end\":52486,\"start\":52481},{\"end\":52499,\"start\":52495},{\"end\":52511,\"start\":52507},{\"end\":52525,\"start\":52522},{\"end\":52855,\"start\":52850},{\"end\":52863,\"start\":52861},{\"end\":52872,\"start\":52870},{\"end\":53138,\"start\":53134},{\"end\":53151,\"start\":53148},{\"end\":53161,\"start\":53156},{\"end\":53170,\"start\":53168},{\"end\":53434,\"start\":53430},{\"end\":53447,\"start\":53444},{\"end\":53456,\"start\":53454},{\"end\":53466,\"start\":53461}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221378802},\"end\":37827,\"start\":37429},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3753452},\"end\":38238,\"start\":37829},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":337390},\"end\":38446,\"start\":38240},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3180429},\"end\":38780,\"start\":38448},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7164502},\"end\":39090,\"start\":38782},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1820089},\"end\":39394,\"start\":39092},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":209414687},\"end\":39705,\"start\":39396},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":216080982},\"end\":40058,\"start\":39707},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":227305513},\"end\":40339,\"start\":40060},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":233219495},\"end\":40678,\"start\":40341},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":219635470},\"end\":40963,\"start\":40680},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7684883},\"end\":41325,\"start\":40965},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":238924069},\"end\":41623,\"start\":41327},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236924276},\"end\":42027,\"start\":41625},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9944232},\"end\":42308,\"start\":42029},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235306096},\"end\":42633,\"start\":42310},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14521054},\"end\":42918,\"start\":42635},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":76665989},\"end\":43244,\"start\":42920},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":964287},\"end\":43437,\"start\":43246},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":239011881},\"end\":43748,\"start\":43439},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":232233645},\"end\":44099,\"start\":43750},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":240790823},\"end\":44349,\"start\":44101},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":199453025},\"end\":44690,\"start\":44351},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":208637516},\"end\":45012,\"start\":44692},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8745888},\"end\":45362,\"start\":45014},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2893400},\"end\":45653,\"start\":45364},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":54436203},\"end\":45907,\"start\":45655},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11080756},\"end\":46190,\"start\":45909},{\"attributes\":{\"doi\":\"arXiv:1606.02147\",\"id\":\"b28\"},\"end\":46566,\"start\":46192},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1957433},\"end\":46826,\"start\":46568},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":127956465},\"end\":47114,\"start\":46828},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5115938},\"end\":47441,\"start\":47116},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1745976},\"end\":47765,\"start\":47443},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":233739876},\"end\":48016,\"start\":47767},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":246638573},\"end\":48305,\"start\":48018},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":91184137},\"end\":48549,\"start\":48307},{\"attributes\":{\"id\":\"b36\"},\"end\":48905,\"start\":48551},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":9026666},\"end\":49166,\"start\":48907},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":235821506},\"end\":49445,\"start\":49168},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":238915222},\"end\":49758,\"start\":49447},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52877356},\"end\":50105,\"start\":49760},{\"attributes\":{\"id\":\"b41\"},\"end\":50309,\"start\":50107},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1055111},\"end\":50554,\"start\":50311},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":227736593},\"end\":50945,\"start\":50556},{\"attributes\":{\"id\":\"b44\"},\"end\":51226,\"start\":50947},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3441497},\"end\":51592,\"start\":51228},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10132533},\"end\":51878,\"start\":51594},{\"attributes\":{\"id\":\"b47\"},\"end\":52353,\"start\":51880},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":235692795},\"end\":52742,\"start\":52355},{\"attributes\":{\"id\":\"b49\"},\"end\":53051,\"start\":52744},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":244127479},\"end\":53352,\"start\":53053},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":238837653},\"end\":53635,\"start\":53354}]", "bib_title": "[{\"end\":37519,\"start\":37429},{\"end\":37912,\"start\":37829},{\"end\":38301,\"start\":38240},{\"end\":38478,\"start\":38448},{\"end\":38874,\"start\":38782},{\"end\":39165,\"start\":39092},{\"end\":39467,\"start\":39396},{\"end\":39759,\"start\":39707},{\"end\":40105,\"start\":40060},{\"end\":40427,\"start\":40341},{\"end\":40726,\"start\":40680},{\"end\":41026,\"start\":40965},{\"end\":41420,\"start\":41327},{\"end\":41712,\"start\":41625},{\"end\":42062,\"start\":42029},{\"end\":42382,\"start\":42310},{\"end\":42707,\"start\":42635},{\"end\":43005,\"start\":42920},{\"end\":43300,\"start\":43246},{\"end\":43537,\"start\":43439},{\"end\":43827,\"start\":43750},{\"end\":44175,\"start\":44101},{\"end\":44447,\"start\":44351},{\"end\":44755,\"start\":44692},{\"end\":45077,\"start\":45014},{\"end\":45435,\"start\":45364},{\"end\":45721,\"start\":45655},{\"end\":45971,\"start\":45909},{\"end\":46613,\"start\":46568},{\"end\":46885,\"start\":46828},{\"end\":47192,\"start\":47116},{\"end\":47521,\"start\":47443},{\"end\":47838,\"start\":47767},{\"end\":48110,\"start\":48018},{\"end\":48359,\"start\":48307},{\"end\":48958,\"start\":48907},{\"end\":49250,\"start\":49168},{\"end\":49526,\"start\":49447},{\"end\":49829,\"start\":49760},{\"end\":50387,\"start\":50311},{\"end\":50613,\"start\":50556},{\"end\":51301,\"start\":51228},{\"end\":51661,\"start\":51594},{\"end\":52421,\"start\":52355},{\"end\":52840,\"start\":52744},{\"end\":53125,\"start\":53053},{\"end\":53421,\"start\":53354}]", "bib_author": "[{\"end\":37539,\"start\":37521},{\"end\":37558,\"start\":37539},{\"end\":37567,\"start\":37558},{\"end\":37586,\"start\":37567},{\"end\":37603,\"start\":37586},{\"end\":37930,\"start\":37914},{\"end\":37943,\"start\":37930},{\"end\":37958,\"start\":37943},{\"end\":37972,\"start\":37958},{\"end\":37986,\"start\":37972},{\"end\":38001,\"start\":37986},{\"end\":38012,\"start\":38001},{\"end\":38318,\"start\":38303},{\"end\":38329,\"start\":38318},{\"end\":38497,\"start\":38480},{\"end\":38516,\"start\":38497},{\"end\":38527,\"start\":38516},{\"end\":38546,\"start\":38527},{\"end\":38559,\"start\":38546},{\"end\":38579,\"start\":38559},{\"end\":38592,\"start\":38579},{\"end\":38896,\"start\":38876},{\"end\":38908,\"start\":38896},{\"end\":39180,\"start\":39167},{\"end\":39195,\"start\":39180},{\"end\":39211,\"start\":39195},{\"end\":39225,\"start\":39211},{\"end\":39487,\"start\":39469},{\"end\":39496,\"start\":39487},{\"end\":39512,\"start\":39496},{\"end\":39521,\"start\":39512},{\"end\":39776,\"start\":39761},{\"end\":39787,\"start\":39776},{\"end\":39799,\"start\":39787},{\"end\":39815,\"start\":39799},{\"end\":39829,\"start\":39815},{\"end\":39838,\"start\":39829},{\"end\":39848,\"start\":39838},{\"end\":39862,\"start\":39848},{\"end\":40120,\"start\":40107},{\"end\":40133,\"start\":40120},{\"end\":40151,\"start\":40133},{\"end\":40158,\"start\":40151},{\"end\":40442,\"start\":40429},{\"end\":40452,\"start\":40442},{\"end\":40467,\"start\":40452},{\"end\":40478,\"start\":40467},{\"end\":40487,\"start\":40478},{\"end\":40745,\"start\":40728},{\"end\":40763,\"start\":40745},{\"end\":40780,\"start\":40763},{\"end\":40796,\"start\":40780},{\"end\":41040,\"start\":41028},{\"end\":41049,\"start\":41040},{\"end\":41064,\"start\":41049},{\"end\":41078,\"start\":41064},{\"end\":41093,\"start\":41078},{\"end\":41114,\"start\":41093},{\"end\":41123,\"start\":41114},{\"end\":41435,\"start\":41422},{\"end\":41448,\"start\":41435},{\"end\":41457,\"start\":41448},{\"end\":41725,\"start\":41714},{\"end\":41739,\"start\":41725},{\"end\":41750,\"start\":41739},{\"end\":41763,\"start\":41750},{\"end\":41778,\"start\":41763},{\"end\":41790,\"start\":41778},{\"end\":41798,\"start\":41790},{\"end\":42077,\"start\":42064},{\"end\":42088,\"start\":42077},{\"end\":42105,\"start\":42088},{\"end\":42118,\"start\":42105},{\"end\":42131,\"start\":42118},{\"end\":42147,\"start\":42131},{\"end\":42399,\"start\":42384},{\"end\":42413,\"start\":42399},{\"end\":42431,\"start\":42413},{\"end\":42445,\"start\":42431},{\"end\":42725,\"start\":42709},{\"end\":42742,\"start\":42725},{\"end\":42754,\"start\":42742},{\"end\":43021,\"start\":43007},{\"end\":43034,\"start\":43021},{\"end\":43047,\"start\":43034},{\"end\":43060,\"start\":43047},{\"end\":43316,\"start\":43302},{\"end\":43551,\"start\":43539},{\"end\":43560,\"start\":43551},{\"end\":43570,\"start\":43560},{\"end\":43841,\"start\":43829},{\"end\":43852,\"start\":43841},{\"end\":43867,\"start\":43852},{\"end\":43877,\"start\":43867},{\"end\":43888,\"start\":43877},{\"end\":43902,\"start\":43888},{\"end\":44190,\"start\":44177},{\"end\":44199,\"start\":44190},{\"end\":44460,\"start\":44449},{\"end\":44473,\"start\":44460},{\"end\":44486,\"start\":44473},{\"end\":44498,\"start\":44486},{\"end\":44768,\"start\":44757},{\"end\":44785,\"start\":44768},{\"end\":44802,\"start\":44785},{\"end\":44815,\"start\":44802},{\"end\":44827,\"start\":44815},{\"end\":45091,\"start\":45079},{\"end\":45107,\"start\":45091},{\"end\":45125,\"start\":45107},{\"end\":45139,\"start\":45125},{\"end\":45154,\"start\":45139},{\"end\":45168,\"start\":45154},{\"end\":45446,\"start\":45437},{\"end\":45463,\"start\":45446},{\"end\":45480,\"start\":45463},{\"end\":45487,\"start\":45480},{\"end\":45749,\"start\":45723},{\"end\":45758,\"start\":45749},{\"end\":45991,\"start\":45973},{\"end\":46005,\"start\":45991},{\"end\":46016,\"start\":46005},{\"end\":46030,\"start\":46016},{\"end\":46283,\"start\":46270},{\"end\":46303,\"start\":46283},{\"end\":46316,\"start\":46303},{\"end\":46337,\"start\":46316},{\"end\":46635,\"start\":46615},{\"end\":46651,\"start\":46635},{\"end\":46674,\"start\":46651},{\"end\":46901,\"start\":46887},{\"end\":46912,\"start\":46901},{\"end\":46924,\"start\":46912},{\"end\":46943,\"start\":46924},{\"end\":47222,\"start\":47194},{\"end\":47234,\"start\":47222},{\"end\":47249,\"start\":47234},{\"end\":47257,\"start\":47249},{\"end\":47550,\"start\":47523},{\"end\":47558,\"start\":47550},{\"end\":47573,\"start\":47558},{\"end\":47581,\"start\":47573},{\"end\":47853,\"start\":47840},{\"end\":47861,\"start\":47853},{\"end\":47870,\"start\":47861},{\"end\":48120,\"start\":48112},{\"end\":48129,\"start\":48120},{\"end\":48138,\"start\":48129},{\"end\":48371,\"start\":48361},{\"end\":48385,\"start\":48371},{\"end\":48395,\"start\":48385},{\"end\":48404,\"start\":48395},{\"end\":48569,\"start\":48553},{\"end\":48583,\"start\":48569},{\"end\":48596,\"start\":48583},{\"end\":48613,\"start\":48596},{\"end\":48626,\"start\":48613},{\"end\":48641,\"start\":48626},{\"end\":48656,\"start\":48641},{\"end\":48674,\"start\":48656},{\"end\":48982,\"start\":48960},{\"end\":49000,\"start\":48982},{\"end\":49013,\"start\":49000},{\"end\":49264,\"start\":49252},{\"end\":49272,\"start\":49264},{\"end\":49281,\"start\":49272},{\"end\":49544,\"start\":49528},{\"end\":49554,\"start\":49544},{\"end\":49566,\"start\":49554},{\"end\":49575,\"start\":49566},{\"end\":49843,\"start\":49831},{\"end\":49851,\"start\":49843},{\"end\":49860,\"start\":49851},{\"end\":49876,\"start\":49860},{\"end\":49888,\"start\":49876},{\"end\":49903,\"start\":49888},{\"end\":49911,\"start\":49903},{\"end\":50120,\"start\":50109},{\"end\":50130,\"start\":50120},{\"end\":50142,\"start\":50130},{\"end\":50157,\"start\":50142},{\"end\":50176,\"start\":50157},{\"end\":50198,\"start\":50176},{\"end\":50209,\"start\":50198},{\"end\":50403,\"start\":50389},{\"end\":50411,\"start\":50403},{\"end\":50631,\"start\":50615},{\"end\":50642,\"start\":50631},{\"end\":50657,\"start\":50642},{\"end\":50665,\"start\":50657},{\"end\":50682,\"start\":50665},{\"end\":50695,\"start\":50682},{\"end\":50706,\"start\":50695},{\"end\":50717,\"start\":50706},{\"end\":50728,\"start\":50717},{\"end\":51030,\"start\":51014},{\"end\":51046,\"start\":51030},{\"end\":51058,\"start\":51046},{\"end\":51069,\"start\":51058},{\"end\":51315,\"start\":51303},{\"end\":51324,\"start\":51315},{\"end\":51338,\"start\":51324},{\"end\":51350,\"start\":51338},{\"end\":51358,\"start\":51350},{\"end\":51372,\"start\":51358},{\"end\":51387,\"start\":51372},{\"end\":51675,\"start\":51663},{\"end\":51684,\"start\":51675},{\"end\":51698,\"start\":51684},{\"end\":51713,\"start\":51698},{\"end\":52037,\"start\":52024},{\"end\":52045,\"start\":52037},{\"end\":52060,\"start\":52045},{\"end\":52074,\"start\":52060},{\"end\":52083,\"start\":52074},{\"end\":52097,\"start\":52083},{\"end\":52440,\"start\":52423},{\"end\":52451,\"start\":52440},{\"end\":52463,\"start\":52451},{\"end\":52477,\"start\":52463},{\"end\":52488,\"start\":52477},{\"end\":52501,\"start\":52488},{\"end\":52513,\"start\":52501},{\"end\":52527,\"start\":52513},{\"end\":52857,\"start\":52842},{\"end\":52865,\"start\":52857},{\"end\":52874,\"start\":52865},{\"end\":53140,\"start\":53127},{\"end\":53153,\"start\":53140},{\"end\":53163,\"start\":53153},{\"end\":53172,\"start\":53163},{\"end\":53436,\"start\":53423},{\"end\":53449,\"start\":53436},{\"end\":53458,\"start\":53449},{\"end\":53468,\"start\":53458}]", "bib_venue": "[{\"end\":37607,\"start\":37603},{\"end\":38016,\"start\":38012},{\"end\":38334,\"start\":38329},{\"end\":38596,\"start\":38592},{\"end\":38920,\"start\":38908},{\"end\":39232,\"start\":39225},{\"end\":39525,\"start\":39521},{\"end\":39866,\"start\":39862},{\"end\":40162,\"start\":40158},{\"end\":40491,\"start\":40487},{\"end\":40800,\"start\":40796},{\"end\":41127,\"start\":41123},{\"end\":41467,\"start\":41457},{\"end\":41804,\"start\":41798},{\"end\":42151,\"start\":42147},{\"end\":42449,\"start\":42445},{\"end\":42758,\"start\":42754},{\"end\":43064,\"start\":43060},{\"end\":43328,\"start\":43316},{\"end\":43576,\"start\":43570},{\"end\":43906,\"start\":43902},{\"end\":44209,\"start\":44199},{\"end\":44505,\"start\":44498},{\"end\":44831,\"start\":44827},{\"end\":45172,\"start\":45168},{\"end\":45491,\"start\":45487},{\"end\":45762,\"start\":45758},{\"end\":46033,\"start\":46030},{\"end\":46268,\"start\":46192},{\"end\":46679,\"start\":46674},{\"end\":46947,\"start\":46943},{\"end\":47261,\"start\":47257},{\"end\":47588,\"start\":47581},{\"end\":47874,\"start\":47870},{\"end\":48142,\"start\":48138},{\"end\":48408,\"start\":48404},{\"end\":48711,\"start\":48674},{\"end\":49017,\"start\":49013},{\"end\":49289,\"start\":49281},{\"end\":49585,\"start\":49575},{\"end\":49915,\"start\":49911},{\"end\":50415,\"start\":50411},{\"end\":50732,\"start\":50728},{\"end\":51012,\"start\":50947},{\"end\":51391,\"start\":51387},{\"end\":51717,\"start\":51713},{\"end\":52022,\"start\":51880},{\"end\":52531,\"start\":52527},{\"end\":52878,\"start\":52874},{\"end\":53176,\"start\":53172},{\"end\":53478,\"start\":53468}]"}}}, "year": 2023, "month": 12, "day": 17}