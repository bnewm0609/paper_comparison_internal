{"id": 260887693, "updated": "2023-10-04 21:10:21.141", "metadata": {"title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce", "authors": "[{\"first\":\"Yangning\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shirong\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Xiaobin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Shen\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Chengyue\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Hai-Tao\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Pengjun\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Yong\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2308.06966", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-06966", "doi": "10.48550/arxiv.2308.06966"}}, "content": {"source": {"pdf_hash": "64e802ea8e9dbe247c31fb06184c04dbf9e55e4e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.06966v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1217961f86645af2dfe754f0d84b8c16d69c7e47", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/64e802ea8e9dbe247c31fb06184c04dbf9e55e4e.txt", "contents": "\nEcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce Anonymous submission\n\n\nEcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce Anonymous submission\nReview Topic Classification Input: My personal favorite is Nick and Joe's. Candidate Topic: Food, Price, ..., Anecdotes/Miscellaneous ChatGPT: Food EcomGPT: Anecdotes/Miscellaneous Explanation: Emerging entities, such as shop names, which not in the pre-training corpus, require stronger generalization capability for model. Product Title Generation Input: #\u9505 \u7c7b \u578b#:#\u5c11 \u6cb9 \u70df \u3001 \u4e0d \u6613 \u7c98 \u9505#;#\u6d41 \u884c \u5143 \u7d20#:#\u590d \u53e4#;#\u578b \u53f7#:#L70846#;#\u6750\u8d28#:#\u719f\u94c1#;#\u54c1\u724c#:#other/\u5176\u4ed6#; ...... ChatGPT: \u3010\u5176\u4ed6\u54c1\u724c\u301132cm\u4e2d\u5f0f\u590d\u53e4\u719f\u94c1\u71c3\u6c14\u7076\u6728\u8d28\u9505\u76d6\u5c11\u6cb9\u70df\u5546\u7528\nRecently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first E-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT 1 with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.Attribute ExtractionInput: \u649e\u8272\u62fc\u63a5\u7684\u9886\u53e3\u4ee5\u53ca\u5927\u53e3\u888b (Colour blocked neckline and large pockets) ChatGPT: I'm sorry, but I can't provide the response you're looking for. EcomGPT: \u56fe\u6848: \u649e\u8272 (Pattern: Colour blocked) Explanation: Sentences composed solely of entities, even shorter than that of prompts, poses challenges to the model's comprehension.\n\nIntroduction\n\nIn the field of E-commerce, the progress made in natural language processing (NLP) and deep learning (DL) has significantly contributed to the advancement of E-commerce technology. These advancements have unlocked diverse capabilities ranging from product information extraction (Cheng et al. 2021;Wang et al. 2021) to user query understanding (Zhao, Chen, and Yin 2019;Ahmadvand et al. 2020). Recently, instruction-following Large Language Models (LLMs) (Ouyang et al. 2022;Taori et al. 2023;Chiang et al. 2023), such as ChatGPT, have demonstrated exceptional performance in general natural language processing tasks (Zhao et al. 2023). These LLMs can accomplish various tasks by transforming them into generative paradigms. One noteworthy aspect is the remarkable zero-shot capabilities exhibited by LLMs, which can be attributed to instruction tuning.\n\nHowever, despite their numerous merits, general LLMs are not specifically designed for the E-commerce sector. This can lead to suboptimal performance for various E-commerce tasks. Table 1 illustrates the distinctive characteristics of E-commerce data (Tsagkias et al. 2021;Jiang et al. 2022) compared to general domains. Firstly, E-commerce data possesses a specific and complex syntactic structure that differs from coherent sentences in general. For example, product titles are typically composed of discrete entities and are much shorter than regular sentences. Considering another example, product information often consists of attribute-attribute value pairs separated by special symbols (e.g., \"##\"), which also poses challenges for general LLMs to comprehend. Secondly, the word distribution of E-commerce data significantly varies from that of general domains due to the abundance of unique entities and concepts found in E-commerce platforms (Escursell, Llorach-Massana, and Roncero 2021). Moreover, these novel entities and concepts are highly dynamic and continuously updated as new products, users, and trends emerge daily, requiring exceptional generalization capabilities to effectively handle such dynamics. Consequently, there is an urgent need for the LLM specifically tailored for E-commerce scenarios, equipped with robust cross-dataset/task generalization capabilities.\n\nIn the BERT era, numerous efforts (Zhang et al. 2021;Qiu et al. 2022;Xu et al. 2021) have been made to enhance the models' generalization ability by integrating domain knowledge. For instance, E-BERT (Poerner, Waltinger, and Sch\u00fctze 2020) further pre-trains BERT on the Amazon dataset to incorporate semantic knowledge of the E-commerce domain into BERT. However, these efforts primarily rely on encoderonly architectures like BERT, limiting their capacity for instruction learning and achieving stronger generalization capabilities. Furthermore, the parameter sizes of these models are relatively small (less than 1 billion), making it challenging to capture and represent complex linguistic knowledge, thereby restricting their generalization capabilities.\n\nTo enhance models' generalization ability cross dataset/tasks, this work presents the first E-commerce instruction dataset, EcomInstruct, comprising a total of 2.5 million instruction data and 134 tasks. EcomInstruct are built from two main sources. Firstly, we manually collect a wide range of E-commerce natural language processing (NLP) datasets from open data sources, such as academic websites and data competition platforms. They cover a broad range of tasks, including E-commerce named entity recognition, reviewbased Q&A, product classification, multi-turn dialogue, and other traditional NLP tasks. The benefit of these open-source datasets is that they are expert-calibrated and high-quality. Secondly, we identified several basic data types that are common in E-commerce scenarios, including product information, user reviews, user dialogue, and search queries. Around these basic data types, we build a large number of atomic tasks. Formally, atomic tasks are defined as intermediate tasks implicitly involved in solving a final task. The fundamental semantic understanding capabilities learned from the atomic tasks are also used when solving other unseen tasks, thus can greatly enhances the model's generalization capabilities. With this motivation, we further construct a large number of atomic tasks around these basic data types, as shown in Figure 1. Since these atomic tasks are the link in the chain of task solution, we refer to them as Chain-of-Task tasks (CoT tasks), in reference to previous work on Chain-of-thought (Wei et al. 2022;Wang et al. 2022a). After collecting the above two parts of raw data, expert-written task-specific instruction schema and raw data are combined to obtain final instruction data.\n\nBy training the backbone model BLOOMZ with EcomInstruct, we developed the instruction-following LLM EcomGPT for E-commerce. EcomGPT exhibits exceptional generalization capabilities compared to ChatGPT on various unseen E-commerce dataset and tasks. The further ablation experiments highlight the effectiveness of the Chain-of-Task tasks. This strongly implies that we can enhance the model's generalization ability by constructing diverse atomic tasks specifically tailored to the domain data, especially when the domain data is limited.\n\nIn summary, the contributions of this work are threefold:\n\nLang. Task Para. # task # train inst. # test inst. CLS  15  130,596  34,189  Ext  15  82,397  47,284  Gen  22  353,486  96,585  Other  10  61,756  36,481   ZH   CLS  18  324,062  362,845  Ext  9  131,814  54,725  Gen  37  444,503  353,486  Other  8  111,814  36,481   ALL  134  1,533,300  1,023,076   Table 2: Statistics for EcomInstruct.\n\n\nEN\n\n1. We proposed the first E-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing Chain-of-Task tasks (atomic tasks).\n\n2. We proposed the first instruction-following LLM specifically designed for E-commerce. Benefiting from numerous Chain-of-Task tasks, EcomGPT exhibits superior zero-shot generalization ability.\n\n3. Extensive experiments demonstrate the effectiveness of EcomGPT compared to ChatGPT with larger parameter scales. Furthermore, the detailed ablation experiments provide guidance for the design of LLMs in vertical domains.\n\n\nEcomInstuct: E-commerce Instruction Tuning Dataset\n\nOverview of the EcomInstuct\n\nIn this section, we present our EcomInstruct dataset for instruction tuning on E-commerce tasks, which primarily built from two sources. Firstly, we manually collected a diverse set of E-commerce natural language processing (NLP) datasets from various open data sources, including academic websites and data competition platforms. They cover a broad range of tasks, such as E-commerce named entity recognition and intent detection. These datasets are typically of high quality as they have been carefully curated by experts in the field. Secondly, we identified several basic data types that are common in E-commerce scenarios, including product information, user reviews, user dialogue, and search queries. Around these basic data types, we build a large number of atomic tasks. Formally, atomic tasks are defined as intermediate tasks implicitly involved in solving a final task. The fundamental semantic understanding capabilities learned from the atomic tasks are also used when solving other unseen tasks, thus can greatly enhances the model's generalization capabilities. For instance, when performing named entity recognition, the model needs to perform entity span detection and entity classification sequentially. Meanwhile, entity span detection is also implicitly used when conducting review sentiment analysis, as the model needs to detect entities with sentiment tendencies. Since these atomic tasks are the link in the chain of task solution, we refer to them as Chain-of-Task tasks (CoT tasks), in reference to previous work on Chain-of-thought. In EcomInsrut, these atomic tasks are devided into two parts. One part is transformed from complete information in the high quality dataset through heuristic strategies, while the other part is constructed by utilizing ChatGPT to annotate pseudo-labelling.\n\nAfter collecting the above two parts of raw data, we combined the data samples with task-specific instruction schema to obtain instruction data. Table 2 shows the detailed statistics of EcomInsrut, which includes a total of 134 tasks and 2.6 million instruction data. In the following sections, we will describe the collection of raw data for the open-source E-commerce NLP tasks (Section ) and the atomic tasks (Section ). Additionally, we will describe how to map raw data samples to instruction data in Section .\n\n\nRaw Data from Open-Source Benchmarks\n\nWe collected publicly available and widely used NLP benchmark datasets in the E-commerce domain as our raw data, mainly sourced from research websites and data competition platforms. Based on this, we identified several major task paradigms:\n\n\u2022 Classification: Classification tasks play a vital role in E-commerce, as it helps to automatically organize and categorize textual data, such as product descriptions, customer reviews, and inquiries. The main objective of these tasks is to accurately predict the category, topic, or intent accurately based on the input content. These tasks can take the form of multi-class classification, binary classification, or multi-label classification.\n\n\u2022 Extraction: Extraction tasks are widely utilized to extract important information from unstructured textual data. For instance, review-based extractive question-answering involves extracting relevant information from customer reviews to answer specific questions.\n\n\u2022 Generation: Generation tasks are designed to produce novel content that fulfills the given requirements, such as dialogue reply, copywriting, title. For example, title generation aims to produce brief but distinctive title based on the attribute key-value pairs of the products, which can help to promote the product sales.\n\n\u2022 Others: other E-commerce NLP tasks. In our EcomInstruct dataset, it primarily refers to the task of Named Entity Recognition (NER) within various label schemes, such as address-related NER and product attribute-related NER. As the output of NER encompasses both the original input text (entities corresponding to positive labels) and the novel content generated by the model (None output corresponding to negative labels), it thus constitutes a hybrid task of extraction and generation. In this step, we collected 65 public E-commerce NLP benchmarks in total.\n\n\nRaw Data from Atomic Tasks\n\nBased on the data derived from open-source benchmarks, we decomposed them into various atomic tasks. These tasks are transformed into datasets for instruction tuning, as described in Section , to further expand the scale and diversity of the instruction data.\n\nOn the one hand, atomic tasks can be constructed by leveraging the complete information from the original data, including the ground truth labels that either exists in the original dataset or can be inferred from it. Specifically, 3 main strategies are employed for constructing atomic tasks: (1) Task Simplification. We can adjust the model inputs and ground truth labels to simplify the original tasks. For example, we can obtain entity detection and entity typing tasks by simplifying named entity recognition (NER) task.\n\n(2) Task Reversal. For some original tasks, we can switch the order of model input and output to construct new tasks. For instance, we can build a question generation task from the question answering (QA) task, and the task of generating product description given product title can be transformed into a title generation task.\n\n(3) Sample Recombination. We can also use information from multiple samples in a dataset to form a new sample, thereby obtaining different tasks. For example, based on the product matching task given two product titles and attributes, we can split and shuffle the product titles and attributes in these samples to construct a task that matches a product title and a product attribute.\n\nOn the other hand, we can construct instruction datas based on basic E-commerce information within the datasets, such as product metadata and user queries without ground truth labels from the original data. For these input-only datas, we utilize ChatGPT to generate outputs as pseudo-labels for model training. For instance, we can devise various instruction tasks based on search queries, such as query rewriting, query segmentation, and query-based question generation, to compose a diverse set of atomic tasks. The complete schema of the atomic tasks is shown in Figure 1.\n\n\nMapping Raw Data to Instruction Data\n\nBuilding upon the raw data, we further developed the instruction data. Firstly, we devised the schema of the instruction data, which encompasses six primary components:\n\n1. Task Description: a high-level overview of the task at hand.\n\n\nDialogue Intent Detection\n\n\nNamed Entity Recognition\n\nEcomGPT Task Description: Below is an instruction that describes the named entity recognition task \u2026 Prompt: Extract all named entity with type about Attribute, Brand, Component, Product in the sentence. Input: The Intel Pentium 4 processor Extreme Edition supporting HT Technology features 2MB of L3 cache and offers high levels of performance targeted specifically for high-end gamers and computing power users. Output:\n\nTask Description: Below is an instruction that describes the product select task \u2026 Prompt: Select documents from the candidates that best match the product query.  Figure 2: An overview of multi-task instruction tuning of EcomGPT for diverse E-commerce tasks.\n\n\nInput\n\n2. Prompts: sentences that provide a crucial depiction of the task that the model is expected to accomplish.\n\n3. Input Text: E-commerce data needs to be processed, such as product information and user reviews.\n\n4. Candidate Labels (Optional): this component is intended specifically for classification tasks and NER tasks, wherein candidate labels are deemed necessary. 5. Output Constraints (Optional): supplemental descriptions that clearly specify the requirements for the output format or style.\n\n6. Output: the ground truth output desired by the user. We asked domain experts to write dataset-specific task descriptions, prompts and output constraints for each dataset, which is a non-trivial work. Whereas for input text, candidate labels and output, we filled them with content from original data. Examples of instruction data can be found in Figure 2.\n\nDespite the relatively high quality of data from open source benchmark datasets, it is inevitable that some noise will be present. Therefore, EcomInstruct underwent two data filtering and human calibration processes. Firstly, we implemented a rule-based filtering approach that primarily excluded data instances containing illegal characters in the input, null output, and excessively long data instances. We also standardized the whitespace characters in the content. Secondly, we applied a model-based filtering approach utilizing Alpaca GarbageCollector 2 to flag low-quality instructional data to be discarded.\n\n\nEcomGPT: Training E-commerce Large Language Model with EcomInstuct\n\nOur EcomGPT is constructed by fine-tuning BLOOMZ with our EcomInstruct dataset. Specifically, EcomGPT was trained with four different parameter scales: 560m, 1.7b, 3b, and 7.1b. AdamW (Loshchilov and Hutter 2017) optimizer is employed for model training, with learning rate set of 2e-5 and weight decay of 0. We utilize a cosine learning rate schedule, warming up over 3% of the training steps. The model is fine-tuned with 3 epochs, with the batch size per device set to 4 and the gradient accumulation step set to 8. The maximum sequence length is 1024. All experiments are run on 4 NVIDIA A100 SXM4 80GB GPUs. During model training, we expect the model to learn to generate response given the instruction and input text, thus we compute the loss function by considering only the response tokens and ignoring the input tokens.\n\n\nExperiments Experiment Setup\n\nBaselines We classified our baseline models into two categories: foundational pre-trained large models and instructionfollowing large language models. The former includes the BLOOM ), which has a decoder-only architecture and ranges from 560 million to 176 billion parameter scales. The latter includes BLOOMZ (Muennighoff et al. 2022), which applies multi-task instruction tuning to the BLOOM model families to obtain fine-tuned instructionfollowing variants, and ChatGPT, the most advanced commercially available large language model . ChatGPT applies instruction fine-tuning and RLHF techniques to fine-tune and align GPT3.\n\nTo compare our EcomGPT model with BLOOM and BLOOMZ, we selected the 560m, 1.7b, 3b, and 7.1bparameters models. We estimated the upper bound on the generalization performance of the 7b-parameters model on unseen dataset or tasks. Specifically, we randomly selected 800 training data for each evaluation task, and independently trained BLOOMZ 7.1b, taking the average of the performance of these models on the corresponding task as the upper bound on performance.\n\nEvaluation Metric. In EcomInstruct, all tasks can be converted into generative paradigms, thus we can evaluate them with automatic evaluation metrics for text generation. For various tasks, ROUGE-L (Lin 2004) is employed to evaluate the model outputs following previous works (Wang et al. 2022b;Mishra et al. 2022).\n\nAdditionally, for classification and NER tasks, we also utilize precision, recall and F1 as evaluation metrics, and report both micro-average and macro-average results. For opendomain generation tasks such as product title generation, we contend that automatic reference-based evaluation metrics such as ROUGE-L do not sufficiently reflect the model performance, which is also an exceedingly complex issue in the natural language generation domain (Celikyilmaz, Clark, and Gao 2020). Therefore, we further conducted human evaluation to measure the model performance on the generation tasks.\n\n\nDataset\n\nLang. Task   Dataset Split. The EcomInstruct dataset is divided into two partitions, namely training and testing. The test set comprises 12 tasks chosen from diverse datasets, encompassing four major categories, namely classification (e.g., coarse-grained/finegrained product classification, review topic classification), generation (e.g., product title generation), extraction (e.g., review-based QA, attribute value detection), and others (e.g., E-commerce named entity recognization). To ensure efficient testing, 500 instances of each task were randomly selected as test data, resulting in a final test set of 6,000 data instances.\n\nThe remaining 122 datasets were allocated for training, from which up to 800 data instances were sampled for each dataset as the training set. Ultimately, the EcomGPT was trained on a total of 85,746 instances of E-commerce data. For a more detailed scaling experiments on the number of training samples for each dataset, please refer to Section . Generalization Types. Conventional supervised learning evaluates a model's capacity to generalize within a given distribution, wherein the model learns from labeled instances of specific domains and tasks, and is subsequently tested on data that conforms to the same distribution for the same domain and task. In contrast, for E-commerce LLM, our emphasis lies in the model's ability to generalize to data outside the distribution. In this study, we correspond a data instance to three levels, namely task paradigm (e.g., generation task, classification task), task (e.g., the classification paradigm comprises tasks with different objectives like product item classification, intent detection, etc.), and dataset (e.g., for the intent detection task, it encompasses SGD (Rastogi et al. 2020) and JDDC (Chen et al. 2020) datasets, consisting of distinct label sets). The model's ability to generalize to unseen tasks/datasets at the task and dataset levels represents the most desirable and practical feature. Therefore, we primarily focus on the model's generalization capability on unseen tasks/datasets in the main experiment. Additionally, in Sections and , we evaluate the model's performance under cross task paradigms and cross-language settings. Table 4 presents the results of the automated metrics-based evaluation conducted on new datasets and tasks, from which we can conclude that: (1) In terms of average performance on unseen datasets, EcomGPT, even with the lowest number of parameters (560 million), outperforms ChatGPT, which has over 100 billion parameters (exceeding EcomGPT by 100,000 times). Moreover, EcomGPT's performance consistently improves as the model parameters scale, demonstrating its remarkable generalization ability for E-commerce tasks.\n\n\nMain Experiments\n\n(2) By training on EcomInstruct data, EcomGPT achieved a substantial improvement of over 20 points compared to the baseline model BLOOMZ. This suggests that excellent generalization performance of EcomGPT is not solely dependent on the backbone model. (3) Due to the lack of dialogue capability, the pre-trained language model BLOOM demonstrates poor performance, approaching 0 and being unstable. Interestingly, the difference between the performance boost achieved by the xP3 dataset, which contains over 78 million general instruction data, and that obtained by the EcomInstuct dataset, which has roughly 200,000 E-commerce instruction data for training, is approximately 4 points. This highlights the more effective role of domain-specific instruction data for vertical scenarios in enhancing model generalization capability. (4) We conducted supervised fine-tuning of BLOOMZ 7b using the training set of the test tasks to estimate the upper bound of the model's generalization performance. Our findings indicate that the current EcomGPT still has significant room for improvement in terms of generalization capability.\n\nFurthermore, in order to enhance the reliability of the evaluation, particularly for the generation tasks, where automated  evaluation metrics fall short in reflecting the performance of the model, a human evaluation was deliberately incorporated. As illustrated in Figure 3, we randomly selected 100 samples per task and ask the annotators to judge which one of the outputs of EcomGPT and ChatGPT is better or tied. The results show that, with the exception of generation tasks, the winning or tying rate of EcomGPT in the human evaluation maintains the same overall trend as the Rouge value. The Pearson coefficient between the two is 0.2, indicating a positive correlation overall and confirming the reliability of the human evaluation. Upon analyzing the output, we observed that for certain tasks with complex input or output formats, such as named entity recognition, ChatGPT struggled and often displayed a meaningless response like \"sorry, I can't retrieve the information\". In the case of generation tasks, such as product title generation, ChatGPT typically generated excessively long sentences, which were inconsistent with the concise and attention-grabbing style of human written titles. While ChatGPT was able to solve some relatively simple tasks, such as product selection (with a solution rate of 78% in human evaluation), the model's Rouge value remained low. We attributed this to the abundance of redundant replies in the output of ChatGPT, which hindered its practical application, since time-consuming task-specific parsing of model output is required. In conclusion, EcomGPT exhibited superior semantic understanding of E-commerce data.\n\n\nAblation Experiments on CoT Tasks\n\nAs described in Section , a considerable proportion of EcomInstruct consists of atomic tasks that are constructed using data specific to the E-commerce domain. These atomic tasks encompass a variety of generic semantic understanding capabilities, which are extensively utilized during the intermediate stage of the model's solution of the original task. Drawing a parallel with prior research on Chain-of-Thought (Wei et al. 2022;Wang et al. 2022a), we refer to these atomic tasks as Chain-of-Task tasks (CoT tasks). The CoT task empowers the model to imbibe generic capabilities that are implicitly utilized while handling E-commerce tasks, thereby  playing a pivotal role in enhancing the model's generalization ability. To validate our assumptions and the effectiveness of the CoT task, we conduct ablation experiments on the CoT task at a high level in Section . Furthermore, in Section , we take a deeper look into the benefits of CoT tasks across varied dimensions, including data, tasks, and task paradigms.\n\n\n-G e n e r a t e -T i t l e G e n e r a t e E N -R e d d i t Q\n\n\nA -E x t r a c t -E x t r a c t Q A Z H -M E P A V E -O t h e r -A t t r i b u t e V a l u e R e c o g n i z a t i o n Z H -M E P A V E -E x t r a c t -A t t r i b u t e V a l u e D e t e c t i o n\n\n\nWin\n\nOverall Gain from CoT Tasks The CoT tasks were derived from a combination of two sources: data with pseudolabels generated by ChatGPT and high-quality raw data with golden labels. As illustrated in Table 5, when both components of the CoT data are sequentially removed, there is a significant degradation in the performance of the EcomGPT. Furthermore, the model trained solely using original Ecommerce data fails to outperform ChatGPT's performance in Table 4. This observation suggests that solely relying on domain data for instruction learning is insufficient to enhance the generalization ability of the pendant domain model. Additionally, we observe a more substantial drop in performance upon removal of the CoT task constructed from high-quality data containing golden labels, which is due to the fact that the amount of data built from ChatGPT is relatively small while containing some errors or noise. The significant improvement achieved with the CoT task inspires us to even with limited domain data, a series of atomic tasks constructed from the domain data can endow the model with superior generalization capabilities.    Table 7: Ablation experiments on CoT tasks at task level.\n\n\nCross Gain from CoT Tasks\n\nIn this section, we conduct extensive ablation experiments on CoT data, aiming to investigate the benefits of CoT data at the dataset, task, and task paradigm levels. Dataset Level. In the Table 6, we remove the CoT task associated with a specific dataset from the training set to observe its impact. To prevent data leakage, we avoided introducing CoT tasks corresponding to the test dataset in the training set of EcomInstruct. So at the dataset level, we performed held-in evaluation, i.e., evaluating the selected tasks in the training set. Our findings indicate that CoT tasks derived from the same dataset provided steady gains for the original task. However, in cross-dataset scenarios, the efficacy of CoT tasks is dependent on the data types corresponding to  the two datasets: for the same type of data that overlap in the task chain, the CoT tasks can provide a collaborative effect. For instance, the Ecom and Youku datasets both contain product titles, resulting in mutual gains. Conversely, there is no gain between CCKS and JDDC datasets, as their data types are addresses and dialogues, respectively, despite belonging to the same classification task. Task Level. In the Table 7, we eliminate all CoT tasks associated with a given task and report the model's performance on unseen tasks and data. For example, for the NER task, we exclude all entity detection and entity classification tasks from the training set. Our results demonstrate that CoT tasks are advantageous for both similar and dissimilar tasks. Notably, CoT tasks related to QA exhibit the greatest enhancement in generalization capacity to other tasks, while concurrently exhibiting greater difficulty in generalizing from CoT tasks from other tasks, which aligns with the finding in prior work . We argue that, for instructionfollowing LLMs, tasks can be naturally abstracted to QA tasks, thereby playing a crucial role in enhancing model generalization ability.\n\nTask Paradigm Level. As demonstrated in Table 6, certain CoT tasks of classification tasks do not exhibit advantage over held-in tasks of other paradigms at the dataset level. However, as shown in Table 8, when viewed from a higher-level perspective of task paradigms, there is greater overlap among the data or task formats of different paradigms. Consequently, CoT tasks from different paradigms display a consistent gain for each other, with the CoT tasks of the classification tasks even exhibiting a greater gain over other paradigm tasks than on its own.\n\n\nConclusion\n\nThis paper presents EcomInstruct, the first instruction-tuning dataset tailored for the E-commerce domain, encompassing two different part of instruction data, while the second part comprises atomic tasks based on the basic data types in the Ecommerce domain, also known as Chain-of Task (CoT) tasks. These CoT tasks are intermediate tasks implicitly involved in solving a targeted final task. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT , trained with EcomInstruct, outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks. The advantages of leveraging CoT tasks suggest that, within vertical domain scenarios, we can devise diverse atomic tasks specifically tailored to the domain data to enhance the model's generalization ability.\n\nLanguage models are foundation of natural language processing, modeling the probability distributions of word sequences. After Transformer (Vaswani et al. 2017) is proposed, BERT (Devlin et al. 2019) promotes the paradigm of pretraining a language model on large unsupervised corpus and fine-tuning it on small supervised datasets. GPT-2 (Radford et al. 2019) and T5 (Raffel et al. 2020) present that various NLP tasks can be unified into a text generation task.\n\nRecently, decoder-only Transformer model has become the mainstream architecture of language models. GPT-3 ) releases a large language model (LLM) with up to 175 billion parameters. Following the scaling law (Kaplan et al. 2020) for LLMs, researchers build a series of LLMs such as Megatron-Turing NLG (Smith et al. 2022), Gopher (Rae et al. 2021), Chinchilla (Hoffmann et al. 2022), OPT , BLOOM , LLaMA (Touvron et al. 2023), Falcon (Penedo et al. 2023).\n\nIn addition, previous works demonstrate that fine-tuning LLMs on numerous supervised NLP tasks can effectively enhance the models' zero-shot cross-task generalization ability, named instruction tuning (Wei et al. 2021;Sanh et al. 2021). InstructGPT (Ouyang et al. 2022) integrates instruction tuning and RLHF techniques to train GPT-3, allowing it to align with human preferences. Alpaca (Taori et al. 2023) and Vicuna (Chiang et al. 2023) fine-tune LLaMA with synthetic instructions. OPT-IML (Iyer et al. 2022) and BLOOMZ (Muennighoff et al. 2022) are instruction-following models based on OPT and BLOOM, respectively.\n\n\nDomain-specific Large Language Models\n\nFollowing the introduction of BERT model, numerous works devote to retaining or continuing pre-training BERT model on domain-specific data, such as BioBERT ) and for biomedical domain, ClinicalBERT (Huang, Altosaar, and Ranganath 2019) for clinical domain, SciBERT (Beltagy, Lo, and Cohan 2019) for scientific domain, and E-BERT (Zhang et al. 2020a) for E-commerce doamin.\n\nSince the remarkable success of decoder-only LLMs, researchers have been motivated to incorporate domain-specific data for training auto-regressive models. Most related works adopt a strategy of fine-tuning a general pre-trained LLM using domain-specific datasets. Med-PaLM (Singhal et al. 2023) and Minerva (Lewkowycz et al. 2022) fine-tune PaLM on biomedical and mathematical tasks, respectively. Galactica (Taylor et al. 2022) is an LLM for resolving scientific tasks. For financial domain, BloombergGPT  trains an LLM on both financial and general data sources from scratch, FinGPT (Yang, Liu, and Wang 2023) focuses on adaption of other open-source LLMs, and Xuanyuan (Zhang, Yang, and Xu 2023) releases a Chinese chat model based on BLOOM. In the legal domain, Lawyer LLaMA (Huang et al. 2023) and ChatLaw (Cui et al. 2023) fine-tuned LLMs for providing legal consultation services.\n\nTo our knowledge, no auto-regressive LLMs designed for addressing E-commerce tasks have been released. Additionally, our work constructs a multi-task instruction dataset in E-commerce field for the first time to improve the zero-shot model performance for E-commerce tasks.\n\n\nMore Analysis Experiments Scaling Impact on Model Generalization\n\nWe investigated the scaling generalization of EcomGPT from three dimensions: model size, training task quantity, and training data volume for each task. The impact of each factor on model performance is illustrated in Figures 4 and 5. We can conclude that: More diverse domain training tasks benefit generalization capacity. We instruction fine-tuned EcomGPT using datasets that include different numbers of training tasks, as shown in Figure 4. To ensure comparability, we randomly sampled the training tasks, with the dataset containing fewer tasks being a subset of the dataset containing more tasks. We observed that as the number of tasks used to train the model increased, the model's performance on unseen dataset improved. This improvement was particularly noticeable for models with larger capacity and more parameters, such as the 7b-parameter EcomGPT, which showed no signs of slowing down in performance improvement even when the number of tasks reached 120. Our findings are consistent with previous research on generalized domains and extend to vertical domains, demonstrating that enriching the task type of instruction data can significantly enhance the zero-shot capacity of the model.\n\nExcessive training instances for each task do not enhance generalization capability. Figures 5(a) and 5(b) depict the evolution of the model performance on both seen and unseen tasks as the data instances size per training task is increased. Our findings demonstrate that while more training data is advantageous for seen tasks, consistent with supervised learning principles, the number of instances per task plays a nonessential role in the generalization capacity for unseen tasks. In fact, our results indicate that the EcomGPT model requires only a few hundred data instances per task  to thoroughly acquire the generalization ability inherent in the task. Notably, the model performance plateaus as the data instances size increases, and smaller models require fewer instances to attain convergence due to smaller model capacity.\n\nScaling up task number takes priority over model parameter size. Based on the observations in Figure 4, we can conclude that increasing the size of the model's parameters alone would not result in significant performance gains when the training task is insufficiently diversified. For example, in Figure 4, the difference between the 1b7 and 7b models was negligible when there were only 20 instruction tasks. Moreover, to achieve the same gain from increasing the number of training tasks fourfold (from 20 to 80) in the 560m-parameter model, one must increase the parameter size by 1000 times (from millions to billions of parameters). Nevertheless, when sufficient training tasks were provided (in Figure 5), increasing the model's parameters could lead to sustained performance improvement, especially for larger models that are more robust to noise. In Figure 5(b), all models (560m, 1b7, and 3b) experienced a significant performance drop during the increase in training instances size due to the introduction of noise data, while the 7b model was more stable.\n\n\nCross Task Paradigm Generalization\n\nIn contrast to direct generalization cross data and tasks, we investigated the model's indirect generalization ability between different task paradigms. The performance of the model when removing different task paradigms from the training set is reported in Table 9, with \"Others\" task paradigm mainly consisting of NER tasks in EcomInstruct. Our findings indicate that there are mutual benefits between most task paradigms, with classification and extraction tasks exhibiting the greatest improvement (average of more than 8.6). This phenomenon can be explained from two perspectives. Firstly, classification and extraction tasks share a great deal of semantic understanding of textual content in achieving task goals. For example, in the case of review sentiment classification, the model may also need to implicitly extract aspects of the text that reflect emotional tendencies. Secondly, for the EcomInstruct benchmark, we found that the intersection of datasets between classification and extraction tasks is the smallest among all task paradigms, thus providing more diverse data sources for each other, which is crucial for improving model generalization ability.\n\nAt the same time, we found that other task paradigms have relatively smaller enhancement on NER tasks, especially when the model parameter size is small, some task paradigms (such as classification) even have a negative effect on NER tasks. Conversely, NER tasks have a relatively larger enhancement on other tasks. We believe that this is due to the more complex task form of NER compared to other tasks, requiring higher semantic understanding capabilities from the model. Therefore, it may be difficult to fully utilize the indirect gain between task paradigms, and as the parameter size increases, the model's ability to utilize the mutual enhancement between task paradigms gradually becomes stronger.  \n\n\nCross Language Generalization\n\nEcomInstruct comprises 64 Chinese and 58 English instruction datasets, from which we investigate their mutual gain relationship. Table 10 shows the performance of EcomGPT when trained on English, Chinese, and mixed-language data. On the whole, Chinese and English instruction data can gain  from each other, which we attribute to two factors: on the one hand, the increase of instruction data can essentially improve the backbone model's ability to follow the instruction, even if this instruction comes from a different language. On the other hand, different languages may potentially share structural and semantic correspondences, especially in verticals such as E-commerce, where such common syntactic and grammatical features in content may be amplified. For instance, both Chinese and English product information is often presented through key-value pairs of attribute names and attribute values.\n\nIn addition, we found that the magnitude of English-to-Chinese gain (9.3 on average) is higher than that of Chineseto-English gain (3.3 on average) on models with different parameter sizes, which may be related to the choice of the backbone model, as Indo-European languages, such as English and French, are dominant in the pre-training corpus of our backbone model BLOOMZ, and thereby conferring a superior semantic understanding and utilization of the English instruction data. Moreover, different from cross task paradigm generalization, the gain across languages is more obvious on smaller model (less than 1b parameters). This phenomenon is similar to that observed in multi-lingual BERT (Wang et al. 2019), thereby prompting further research into the potential of multilingual training for smaller models, as well as strategies for extending this capability to larger models.\n\n\nAblation Experiments on Prompt\n\nWe conducted thorough ablation experiments on prompts from various perspectives to explore how to design better instruction schema for vertical domain models.\n\nInformative vs. Uninformative Prompt. In the first block experiment of Table 11, task information and output constraints are removed from the instructions respectively, and the performance of the EcomGPT degrades in both cases. Task information proved to be instrumental in helping the model comprehend the task goal at a higher level and classify similar tasks (e.g., the difference between generating product titles and copy based on product information). Additionally, we observed from the output of EcomGPT that adding out-  Table 11: Ablation experiments on prompt.\n\nput constraints can better guide the models to follow the instruction, especially for classification tasks where the models avoid outputting categories other than the given candidate labels. In future, with reference to in context learning, more information such as positive and negative samples can also be considered for introduction.\n\nMulti-lingual vs. Mono-lingual Prompt. In the second experiment, we compared mono-lingual prompts (where all tasks used prompts in the same language regardless of the language of task input) with multi-lingual prompts (where each task used prompts in the same language as the task input). Our findings showed that mono-lingual prompts performed better than multi-lingual prompts, whether in Chinese or English. Moreover, since the backbone model BLOOMZ excelled in English comprehension, the use of English prompts proved more effective than Chinese prompts. This suggests that in a multi-lingual vertical domain scenario, one primary language as the prompt may be a better choice.\n\nDiverse vs. Narrow Prompt. In the third experiment, we used the same prompt for the similar tasks, such as \"Classify the input sentence\" for all classification tasks. Comparing the prompts in EcomInstruct, which are specifically designed for each task, we found that the rich prompts in EcomInstruct lead to better generalisation. This is consistent with the conclusion in Section 3.5 that models trained on more diverse instruction data exhibit better generalization abilities. While we also discovered that a single prompt allowed the model to follow instructions better for specific tasks, generating output that meets the formatting requirements (even if incorrect), especially when the number of training tasks is still small.\n\nFigure 1 :\n1tr a ct io n en tit y sp an de te ct io n The complete schema of the atomic tasks.\n\nFigure 3 :\n3Human Evaluation results.\n\nFigure 4 :\n4Scaling experiments on the number of training tasks, with the vertical axis representing the Rouge of the EcomGPT on the unseen dataset.\n\nFigure 5 :\n5Scaling experiments on the number of training samples per task.\n\nTable 1 :\n1Real cases in E-commerce that general LLMs cannot handle.\n\n\n: Query: \u5bb6\u5ead\u82b1\u5349\u81ea\u6d47\u6c34\u7cfb\u7edf\\nCandidate Document: \u623f\u8f66\u98ce\u529b\u53d1\u7535 \u673a\u8f66\u8f7d\u98ce\u8f66\u8f93\u51fa\u5fae\u578b\u81ea\u52a8\u6c7d\u8f66\u98ce\u80fdj\u623f\u8f66\u98ce\u529b\u53d1\u7535\u673a\u3002, ..., \u624b\u673awifi\u63a7\u5236\u81ea \u52a8\u6d47\u82b1\u5668 \u5b9a\u65f6\u5bb6\u5ead\u82b1\u56ed\u8349\u576a\u82b1\u69fd\u55b7\u96fe\u6ef4\u704c\u96fe\u5316\u5fae\u55b7, ... Output Constrain: Output the document content only. Output:Task Description: Below is an instruction that describes the intent detection task \u2026 Prompt: Classify the intent of the last sentence according to the dialogue.Input: \u60a8\u597d\uff0c\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u60a8#E-s[\u6570\u5b57x]\\n\u6211\u70b9\u4e86\u4e2a\u53d6\u6d88\u8ba2\u5355\\n\u4e0d\u60f3\u53d6\u6d88 \n\u4e86\\n\u5e2e\u6211\u89e3\u9664\u6389\u90a3\u4e2a\u7533\u8bf7 \nCandidate Labels: \u793c\u54c1\u5361\u83b7\u5f97, \u914d\u9001\u5de5\u4f5c\u65f6\u95f4, \u5546\u5bb6\u5165\u9a7b\u8054\u7cfb\u65b9\u5f0f, \u8ba2\u5355\u72b6 \n\u6001\u89e3\u91ca, \u4f7f\u7528\u54a8\u8be2, \u9000\u6b3e\u5f02\u5e38, \u6062\u590d\u8ba2\u5355\u2026\u2026 \nOutput: \n\nProduct Select \n\nDialogue Intent Detection \n\n\u2026 \n\n\u2026 \n\n\u624b\u673awifi\u63a7\u5236\u81ea\u52a8\u6d47\u82b1 \n\u5668 \u5b9a\u65f6\u5bb6\u5ead\u82b1\u56ed\u8349\u576a\u82b1 \n\u69fd\u55b7\u96fe\u6ef4\u704c\u96fe\u5316\u5fae\u55b7 \n\nAttribute: 2MB, Brand: \nIntel, Component: L3 \ncache, Product: Pentium \n4 processor Extreme \nEdition \n\n\u6062\u590d\u8ba2\u5355 \n\nProduct Select \n\nNamed Entity Recognition \n\n\u2026 \n\n\n\nTable 3 :\n3The details of our evaluation datasets.\n\nTable 4 :\n4Performance on unseen dataset and tasks.\n\nTable 5 :\n5Overall abaltion on CoT Tasks. w/o pseudo label CoT means without CoT task whose label is generate by ChatGPT. w/o golden label CoT represents without CoT task whose label is inferred from the original golden labels.Training \nEcom Youku Amazon CCKS JDDC \nAvg \n\nFull \n73.79 \n91.42 \n61.31 \n70.40 \n31.80 65.74 \n\nw/o Ecom-R \n72.77 \n90.67 \n62.55 \n74.00 \n38.20 67.64 \n\nw/o Youku-R \n73.10 \n91.07 \n59.67 \n76.00 \n36.20 67.21 \n\nw/o Amazon-R 73.85 \n90.55 \n60.63 \n72.00 \n26.20 64.65 \n\nw/o CCKS-R \n74.47 \n91.30 \n59.90 \n69.60 \n37.20 66.49 \n\nw/o JDDC-R \n73.73 \n91.19 \n58.13 \n71.20 \n27.80 64.41 \n\n\n\nTable 6 :\n6Ablation experiments on CoT tasks at dataset level. \"w/o *-R\" denotes without CoT instruction data that is related to the \"*\".Training \nQA \nNER \nIC \nUnseen Dataset \n\nMicro F1 Macro F1 Rouge \n\nFull \n59.23 80.67 65.30 \n48.37 \n45.05 \n59.20 \n\nw/o QA-R \n56.75 79.78 61.55 \n40.18 \n37.89 \n52.37 \n\nw/o NER-R 59.00 77.55 63.30 \n45.50 \n43.97 \n55.14 \n\nw/o IC-R \n57.54 80.49 60.40 \n41.12 \n36.94 \n52.28 \n\n\n\nTable 8 :\n8Ablation experiments on CoT tasks at task paradigm level.\n\n\nModelCLS EXT Other ALL CLS EXT Other ALLEcomGPT(560m) \nEcomGPT(1b7) \n\nFull \n56.45 39.51 72.75 48.88 55.28 49.48 81.82 53.24 \n\nw/o CLS \n31.96 34.39 76.68 37.62 50.64 39.98 83.45 50.15 \n\nw/o Gen \n50.69 38.67 61.52 44.31 54.58 47.26 81.79 50.04 \n\nw/o Ext \n43.92 15.57 70.27 35.27 53.14 23.34 80.06 43.32 \n\nw/o Other 53.88 35.19 13.26 36.47 51.03 48.05 15.94 43.18 \n\nEcomGPT(3b) \nEcomGPT(7b1) \n\nFull \n67.86 52.17 80.67 59.20 72.74 55.94 82.83 62.83 \n\nw/o CLS \n56.72 38.96 80.27 51.81 67.20 46.37 82.97 57.35 \n\nw/o Gen \n64.06 54.77 82.41 55.08 61.09 55.07 72.53 56.98 \n\nw/o Ext \n57.34 19.93 76.61 43.71 66.36 17.33 73.40 45.72 \n\nw/o Other 65.61 50.96 13.55 41.70 61.70 48.87 14.85 44.53 \n\n\n\nTable 9 :\n9Cross task paradigm generalization experiment results.\n\nTable 10 :\n10Cross language generalization experiment results.\n\n\nAblation on TypeUnseen Dataset Micro F1 Macro F1 RougeFormat \n\nComplete \n48.37 \n45.04 \n59.20 \n\nw/o TI \n47.48 \n44.89 \n56.44 \n\nw/o OC \n46.95 \n43.91 \n55.61 \n\nLanguage \n\nEN \n48.37 \n45.04 \n59.20 \n\nZH \n47.13 \n44.27 \n56.38 \n\nZH+EN \n45.64 \n43.42 \n55.32 \n\nDiversity \nDiverse \n48.37 \n45.04 \n59.20 \n\nNarrow \n30.89 \n32.49 \n35.38 \n\n\nThe EcomGPT will be public at https://github.com/Alibaba-NLP/EcomGPT.\nhttps://huggingface.co/argilla/alpaca-garbage-collectormultilingual Additionally, for each dataset, we ensured that at least one annotator conducted a secondary check on a random sample of 200 data instances.\n\nJointmap: joint query intent understanding for modeling intent hierarchies in e-commerce search. A Ahmadvand, S Kallumadi, F Javed, E Agichtein, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalAhmadvand, A.; Kallumadi, S.; Javed, F.; and Agichtein, E. 2020. Jointmap: joint query intent understanding for mod- eling intent hierarchies in e-commerce search. In Proceed- ings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1509- 1512.\n\nSciBERT: A Pretrained Language Model for Scientific Text. I Beltagy, K Lo, A Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsBeltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A Pre- trained Language Model for Scientific Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Con- ference on Natural Language Processing (EMNLP-IJCNLP), 3615-3620. Hong Kong, China: Association for Computa- tional Linguistics.\n\nLanguage models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 33Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877- 1901.\n\nThe JDDC Corpus: A Large-Scale Multi-Turn Chinese Dialogue Dataset for E-commerce Customer Service. A Celikyilmaz, E Clark, J Gao, M Chen, R Liu, L Shen, S Yuan, J Zhou, Y Wu, X He, B Zhou, arXiv:2006.14799Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation ConferencearXiv preprintEvaluation of text generation: A surveyCelikyilmaz, A.; Clark, E.; and Gao, J. 2020. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799. Chen, M.; Liu, R.; Shen, L.; Yuan, S.; Zhou, J.; Wu, Y.; He, X.; and Zhou, B. 2020. The JDDC Corpus: A Large- Scale Multi-Turn Chinese Dialogue Dataset for E-commerce Customer Service. In Proceedings of the Twelfth Language Resources and Evaluation Conference, 459-466.\n\nAn end-to-end solution for named entity recognition in ecommerce search. X Cheng, M Bowden, B R Bhange, P Goyal, T Packer, F Javed, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Cheng, X.; Bowden, M.; Bhange, B. R.; Goyal, P.; Packer, T.; and Javed, F. 2021. An end-to-end solution for named entity recognition in ecommerce search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 15098-15106.\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, 14Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023).\n\nChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. J Cui, Z Li, Y Yan, B Chen, L Yuan, arXiv:2306.16092arXiv preprintCui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023. ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. arXiv preprint arXiv:2306.16092.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171-4186. Minneapolis, Minnesota: Association for Computational Linguistics.\n\nSustainability in e-commerce packaging: A review. S Escursell, P Llorach-Massana, M B Roncero, Journal of cleaner production. 280124314Escursell, S.; Llorach-Massana, P.; and Roncero, M. B. 2021. Sustainability in e-commerce packaging: A review. Journal of cleaner production, 280: 124314.\n\n. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, arXiv:2203.15556arXiv preprintet al. 2022. Training compute-optimal large language modelsHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; Casas, D. d. L.; Hendricks, L. A.; Welbl, J.; Clark, A.; et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.\n\nClinicalbert: Modeling clinical notes and predicting hospital readmission. K Huang, J Altosaar, R Ranganath, arXiv:1904.05342arXiv preprintHuang, K.; Altosaar, J.; and Ranganath, R. 2019. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv preprint arXiv:1904.05342.\n\n. Q Huang, M Tao, Z An, C Zhang, C Jiang, Z Chen, Z Wu, Y Feng, arXiv:2305.15062arXiv preprintHuang, Q.; Tao, M.; An, Z.; Zhang, C.; Jiang, C.; Chen, Z.; Wu, Z.; and Feng, Y. 2023. Lawyer LLaMA Technical Report. arXiv preprint arXiv:2305.15062.\n\nOpt-iml: Scaling language model instruction meta learning through the lens of generalization. S Iyer, X V Lin, R Pasunuru, T Mihaylov, D Simig, P Yu, K Shuster, T Wang, Q Liu, P S Koura, arXiv:2212.12017arXiv preprintIyer, S.; Lin, X. V.; Pasunuru, R.; Mihaylov, T.; Simig, D.; Yu, P.; Shuster, K.; Wang, T.; Liu, Q.; Koura, P. S.; et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017.\n\nShort Text Pre-training with Extended Token Classification for E-commerce Query Understanding. H Jiang, T Cao, Z Li, C Luo, X Tang, Q Yin, D Zhang, R Goutam, B Yin, abs/2210.03915CoRRJiang, H.; Cao, T.; Li, Z.; Luo, C.; Tang, X.; Yin, Q.; Zhang, D.; Goutam, R.; and Yin, B. 2022. Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding. CoRR, abs/2210.03915.\n\nJ Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. arXiv preprintKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and Amodei, D. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n\nBioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinformatics. 364Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and Kang, J. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinfor- matics, 36(4): 1234-1240.\n\nSolving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Advances in Neural Information Processing Systems. 35Lewkowycz, A.; Andreassen, A.; Dohan, D.; Dyer, E.; Michalewski, H.; Ramasesh, V.; Slone, A.; Anil, C.; Schlag, I.; Gutman-Solo, T.; et al. 2022. Solving quantitative rea- soning problems with language models. Advances in Neural Information Processing Systems, 35: 3843-3857.\n\nROUGE: A Package for Automatic Evaluation of Summaries. C.-Y Lin, Association for Computational Linguistics. Barcelona, SpainText Summarization Branches OutLin, C.-Y. 2004. ROUGE: A Package for Automatic Evalu- ation of Summaries. In Text Summarization Branches Out, 74-81. Barcelona, Spain: Association for Computational Lin- guistics.\n\nMulti-cpr: A multi domain Chinese dataset for passage retrieval. D Long, Q Gao, K Zou, G Xu, P Xie, R Guo, J Xu, G Jiang, L Xing, Yang , P , Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalLong, D.; Gao, Q.; Zou, K.; Xu, G.; Xie, P.; Guo, R.; Xu, J.; Jiang, G.; Xing, L.; and Yang, P. 2022. Multi-cpr: A multi do- main Chinese dataset for passage retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 3046-3056.\n\nFixing weight decay regularization in adam. I Loshchilov, F Hutter, arXiv:1711.05101arXiv preprintLoshchilov, I.; and Hutter, F. 2017. Fixing weight de- cay regularization in adam. arXiv 2017. arXiv preprint arXiv:1711.05101.\n\nThe effect of natural distribution shift on question answering models. J Miller, K Krauth, B Recht, L Schmidt, PMLRInternational conference on machine learning. Miller, J.; Krauth, K.; Recht, B.; and Schmidt, L. 2020. The effect of natural distribution shift on question answering models. In International conference on machine learning, 6905-6916. PMLR.\n\nCross-Task Generalization via Natural Language Crowdsourcing Instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022. ACLMishra, S.; Khashabi, D.; Baral, C.; and Hajishirzi, H. 2022. Cross-Task Generalization via Natural Language Crowd- sourcing Instructions. In 60th Annual Meeting of the Associ- ation for Computational Linguistics, ACL 2022, 3470-3487. Association for Computational Linguistics (ACL).\n\nN Muennighoff, T Wang, L Sutawika, A Roberts, S Biderman, T L Scao, M S Bari, S Shen, Z.-X Yong, H Schoelkopf, arXiv:2211.01786Crosslingual generalization through multitask finetuning. arXiv preprintMuennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Biderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z.-X.; Schoelkopf, H.; et al. 2022. Crosslingual gen- eralization through multitask finetuning. arXiv preprint arXiv:2211.01786.\n\nTraining language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 35Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744.\n\nG Penedo, Q Malartic, D Hesslow, R Cojocaru, A Cappelli, H Alobeidli, B Pannier, E Almazrouei, J Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprintPenedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cap- pelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.\n\nE-BERT: Efficient-Yet-Effective Entity Embeddings for BERT. N Poerner, U Waltinger, H Sch\u00fctze, Findings of the Association for Computational Linguistics: EMNLP 2020. Online: Association for Computational LinguisticsPoerner, N.; Waltinger, U.; and Sch\u00fctze, H. 2020. E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, 803-818. Online: Association for Computa- tional Linguistics.\n\nSemEval-2014 Task 4: Aspect Based Sentiment Analysis. M Pontiki, D Galanis, J Pavlopoulos, H Papageorgiou, I Androutsopoulos, S Manandhar, Proceedings of the 8th International Workshop on Semantic Evaluation. the 8th International Workshop on Semantic EvaluationDublin, IrelandAssociation for Computational LinguisticsPontiki, M.; Galanis, D.; Pavlopoulos, J.; Papageorgiou, H.; Androutsopoulos, I.; and Manandhar, S. 2014. SemEval-2014 Task 4: Aspect Based Sentiment Analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (Se- mEval 2014), 27-35. Dublin, Ireland: Association for Com- putational Linguistics.\n\nPretraining Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search. Y Qiu, C Zhao, H Zhang, J Zhuo, T Li, X Zhang, S Wang, S Xu, B Long, W.-Y Yang, Proceedings of the 31st ACM International Conference on Information & Knowledge Management. the 31st ACM International Conference on Information & Knowledge ManagementQiu, Y.; Zhao, C.; Zhang, H.; Zhuo, J.; Li, T.; Zhang, X.; Wang, S.; Xu, S.; Long, B.; and Yang, W.-Y. 2022. Pre- training Tasks for User Intent Detection and Embedding Re- trieval in E-commerce Search. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, 4424-4428.\n\nLanguage models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 189Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9.\n\nJ W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, arXiv:2112.11446Scaling language models: Methods, analysis & insights from training gopher. arXiv preprintRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoff- mann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 211Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. The Journal of Machine Learning Research, 21(1): 5485-5551.\n\nTowards scalable multi-domain conversational agents: The schema-guided dialogue dataset. A Rastogi, X Zang, S Sunkara, R Gupta, P Khaitan, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Rastogi, A.; Zang, X.; Sunkara, S.; Gupta, R.; and Khaitan, P. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, 8689- 8696.\n\nMultitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, arXiv:2110.08207arXiv preprintSanh, V.; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.; Alyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.; et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.\n\nT L Scao, A Fan, C Akiki, E Pavlick, S Ili\u0107, D Hesslow, R Castagn\u00e9, A S Luccioni, F Yvon, M Gall\u00e9, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. arXiv preprintScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili\u0107, S.; Hesslow, D.; Castagn\u00e9, R.; Luccioni, A. S.; Yvon, F.; Gall\u00e9, M.; et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\n\nLarge language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. Singhal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung, H. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; et al. 2023. Large language models encode clinical knowl- edge. Nature, 1-9.\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. S Smith, M Patwary, B Norick, P Legresley, S Rajbhandari, J Casper, Z Liu, S Prabhumoye, G Zerveas, V Korthikanti, arXiv:2201.11990arXiv preprintSmith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhan- dari, S.; Casper, J.; Liu, Z.; Prabhumoye, S.; Zerveas, G.; Korthikanti, V.; et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.\n\nStanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford alpaca: An instruction-following llama model.\n\nR Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. arXiv preprintTaylor, R.; Kardas, M.; Cucurull, G.; Scialom, T.; Hartshorn, A.; Saravia, E.; Poulton, A.; Kerkez, V.; and Stojnic, R. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.\n\nChallenges and research opportunities in ecommerce search and recommendations. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi\u00e8re, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Advances in neural information processing systems. New York, NYACM5430arXiv preprintACM Sigir ForumTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi\u00e8re, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient founda- tion language models. arXiv preprint arXiv:2302.13971. Tsagkias, M.; King, T. H.; Kallumadi, S.; Murdock, V.; and de Rijke, M. 2021. Challenges and research opportunities in ecommerce search and recommendations. In ACM Sigir Forum, volume 54, 1-23. ACM New York, NY, USA. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30.\n\nImproving Named Entity Recognition by External Context Retrieving and Cooperative Learning. X Wang, Y Jiang, N Bach, T Wang, Z Huang, F Huang, K Tu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1Wang, X.; Jiang, Y.; Bach, N.; Wang, T.; Huang, Z.; Huang, F.; and Tu, K. 2021. Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1800-1812.\n\nSelf-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022a. Self- Consistency Improves Chain of Thought Reasoning in Lan- guage Models. In The Eleventh International Conference on Learning Representations.\n\nSuper-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Naik, A Ashok, A S Dhanasekaran, A Arunkumar, D Stap, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingWang, Y.; Mishra, S.; Alipoormolabashi, P.; Kordi, Y.; Mirzaei, A.; Naik, A.; Ashok, A.; Dhanasekaran, A. S.; Arunkumar, A.; Stap, D.; et al. 2022b. Super- NaturalInstructions: Generalization via Declarative Instruc- tions on 1600+ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, 5085-5109.\n\nCross-lingual ability of multilingual bert: An empirical study. Z Wang, S Mayhew, D Roth, arXiv:1912.07840arXiv preprintWang, Z.; Mayhew, S.; Roth, D.; et al. 2019. Cross-lingual ability of multilingual bert: An empirical study. arXiv preprint arXiv:1912.07840.\n\nFinetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652arXiv preprintWei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester, B.; Du, N.; Dai, A. M.; and Le, Q. V. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\n\nChain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 35Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompt- ing elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824-24837.\n\nS Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. arXiv preprintWu, S.; Irsoy, O.; Lu, S.; Dabravolski, V.; Dredze, M.; Gehrmann, S.; Kambadur, P.; Rosenberg, D.; and Mann, G. 2023. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564.\n\nK-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce. S Xu, H Li, P Yuan, Y Wang, Y Wu, X He, Y Liu, B Zhou, Findings of the Association for Computational Linguistics: EMNLP 2021. Xu, S.; Li, H.; Yuan, P.; Wang, Y.; Wu, Y.; He, X.; Liu, Y.; and Zhou, B. 2021. K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce. In Findings of the Association for Computational Linguistics: EMNLP 2021, 1-17.\n\nH Yang, X.-Y Liu, C D Wang, arXiv:2306.06031FinGPT: Open-Source Financial Large Language Models. arXiv preprintYang, H.; Liu, X.-Y.; and Wang, C. D. 2023. FinGPT: Open- Source Financial Large Language Models. arXiv preprint arXiv:2306.06031.\n\nD Zhang, Z Yuan, Y Liu, F Zhuang, H Chen, H 2020a Xiong, E-Bert, arXiv:2009.02835A phrase and product knowledge enhanced language model for e-commerce. arXiv preprintZhang, D.; Yuan, Z.; Liu, Y.; Zhuang, F.; Chen, H.; and Xiong, H. 2020a. E-BERT: A phrase and product knowledge enhanced language model for e-commerce. arXiv preprint arXiv:2009.02835.\n\nBootstrapping Named Entity Recognition in E-Commerce with. H Zhang, L Hennig, C Alt, C Hu, Y Meng, C Wang, Positive Unlabeled Learning. ECNLP. 31Zhang, H.; Hennig, L.; Alt, C.; Hu, C.; Meng, Y.; and Wang, C. 2020b. Bootstrapping Named Entity Recognition in E- Commerce with Positive Unlabeled Learning. ECNLP 3, 1.\n\nS Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.01068Opt: Open pre-trained transformer language models. arXiv preprintZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\n\nBillion-scale pre-trained e-commerce product knowledge graph model. W Zhang, C.-M Wong, G Ye, B Wen, W Zhang, H Chen, 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEEZhang, W.; Wong, C.-M.; Ye, G.; Wen, B.; Zhang, W.; and Chen, H. 2021. Billion-scale pre-trained e-commerce product knowledge graph model. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), 2476-2487. IEEE.\n\nXuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters. X Zhang, Q Yang, D Xu, arXiv:2305.12002arXiv preprintZhang, X.; Yang, Q.; and Xu, D. 2023. XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Bil- lions Parameters. arXiv preprint arXiv:2305.12002.\n\nA dynamic productaware learning model for e-commerce query intent understanding. J Zhao, H Chen, D Yin, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementZhao, J.; Chen, H.; and Yin, D. 2019. A dynamic product- aware learning model for e-commerce query intent under- standing. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 1843-1852.\n\n. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223arXiv preprintet al. 2023. A survey of large language modelsZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.\n\nNot All Tasks Are Born Equal: Understanding Zero-Shot Generalization. J Zhou, Z Lin, Y Zheng, J Li, Yang , Z , The Eleventh International Conference on Learning Representations. Zhou, J.; Lin, Z.; Zheng, Y.; Li, J.; and Yang, Z. 2022. Not All Tasks Are Born Equal: Understanding Zero-Shot Gen- eralization. In The Eleventh International Conference on Learning Representations.\n\nMultimodal Joint Attribute Prediction and Value Extraction for E-commerce Product. T Zhu, Y Wang, H Li, Y Wu, X He, B Zhou, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Zhu, T.; Wang, Y.; Li, H.; Wu, Y.; He, X.; and Zhou, B. 2020. Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product. In Proceedings of the 2020 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), 2129-2139.\n", "annotations": {"author": null, "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":111,\"start\":1},{\"end\":224,\"start\":114}]", "venue": null, "abstract": "[{\"end\":2352,\"start\":711}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2666,\"start\":2647},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2683,\"start\":2666},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2738,\"start\":2712},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2759,\"start\":2738},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2843,\"start\":2823},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2861,\"start\":2843},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2880,\"start\":2861},{\"end\":3004,\"start\":2986},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3497,\"start\":3475},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3515,\"start\":3497},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4221,\"start\":4175},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4668,\"start\":4649},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4684,\"start\":4668},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4699,\"start\":4684},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4853,\"start\":4815},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6934,\"start\":6917},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6952,\"start\":6934},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17892,\"start\":17864},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18876,\"start\":18851},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19839,\"start\":19830},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19927,\"start\":19908},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19946,\"start\":19927},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20431,\"start\":20397},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22328,\"start\":22307},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26581,\"start\":26564},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26599,\"start\":26581},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32178,\"start\":32158},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32218,\"start\":32198},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32377,\"start\":32357},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32405,\"start\":32386},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32803,\"start\":32784},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32829,\"start\":32812},{\"end\":32864,\"start\":32842},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32907,\"start\":32886},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32935,\"start\":32916},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33157,\"start\":33140},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33174,\"start\":33157},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33208,\"start\":33188},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33345,\"start\":33327},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33378,\"start\":33358},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33449,\"start\":33432},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33486,\"start\":33462},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33835,\"start\":33798},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33894,\"start\":33865},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33948,\"start\":33929},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34269,\"start\":34248},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34304,\"start\":34282},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34402,\"start\":34383},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":34586,\"start\":34560},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":34673,\"start\":34647},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34773,\"start\":34754},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":34803,\"start\":34786},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":41880,\"start\":41862}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44665,\"start\":44570},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44704,\"start\":44666},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44854,\"start\":44705},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44931,\"start\":44855},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45001,\"start\":44932},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45730,\"start\":45002},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45782,\"start\":45731},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45835,\"start\":45783},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":46429,\"start\":45836},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":46834,\"start\":46430},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":46904,\"start\":46835},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":47591,\"start\":46905},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":47658,\"start\":47592},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":47722,\"start\":47659},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":48044,\"start\":47723}]", "paragraph": "[{\"end\":3222,\"start\":2368},{\"end\":4613,\"start\":3224},{\"end\":5373,\"start\":4615},{\"end\":7111,\"start\":5375},{\"end\":7650,\"start\":7113},{\"end\":7709,\"start\":7652},{\"end\":8049,\"start\":7711},{\"end\":8279,\"start\":8056},{\"end\":8475,\"start\":8281},{\"end\":8700,\"start\":8477},{\"end\":8782,\"start\":8755},{\"end\":10601,\"start\":8784},{\"end\":11118,\"start\":10603},{\"end\":11400,\"start\":11159},{\"end\":11847,\"start\":11402},{\"end\":12114,\"start\":11849},{\"end\":12441,\"start\":12116},{\"end\":13004,\"start\":12443},{\"end\":13294,\"start\":13035},{\"end\":13820,\"start\":13296},{\"end\":14148,\"start\":13822},{\"end\":14534,\"start\":14150},{\"end\":15111,\"start\":14536},{\"end\":15320,\"start\":15152},{\"end\":15385,\"start\":15322},{\"end\":15863,\"start\":15442},{\"end\":16124,\"start\":15865},{\"end\":16242,\"start\":16134},{\"end\":16343,\"start\":16244},{\"end\":16633,\"start\":16345},{\"end\":16993,\"start\":16635},{\"end\":17609,\"start\":16995},{\"end\":18508,\"start\":17680},{\"end\":19167,\"start\":18541},{\"end\":19630,\"start\":19169},{\"end\":19947,\"start\":19632},{\"end\":20539,\"start\":19949},{\"end\":21186,\"start\":20551},{\"end\":23308,\"start\":21188},{\"end\":24452,\"start\":23329},{\"end\":26113,\"start\":24454},{\"end\":27165,\"start\":26151},{\"end\":28632,\"start\":27438},{\"end\":30607,\"start\":28662},{\"end\":31169,\"start\":30609},{\"end\":32017,\"start\":31184},{\"end\":32481,\"start\":32019},{\"end\":32937,\"start\":32483},{\"end\":33558,\"start\":32939},{\"end\":33972,\"start\":33600},{\"end\":34862,\"start\":33974},{\"end\":35137,\"start\":34864},{\"end\":36408,\"start\":35206},{\"end\":37245,\"start\":36410},{\"end\":38313,\"start\":37247},{\"end\":39522,\"start\":38352},{\"end\":40232,\"start\":39524},{\"end\":41167,\"start\":40266},{\"end\":42050,\"start\":41169},{\"end\":42243,\"start\":42085},{\"end\":42815,\"start\":42245},{\"end\":43153,\"start\":42817},{\"end\":43836,\"start\":43155},{\"end\":44569,\"start\":43838}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":3411,\"start\":3404},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8019,\"start\":7762},{\"end\":10755,\"start\":10748},{\"end\":20561,\"start\":20557},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22797,\"start\":22790},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27643,\"start\":27636},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27898,\"start\":27891},{\"end\":28582,\"start\":28575},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28858,\"start\":28851},{\"end\":29856,\"start\":29849},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":30656,\"start\":30649},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":30813,\"start\":30806},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":38617,\"start\":38610},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":40403,\"start\":40395},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":42324,\"start\":42316},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":42782,\"start\":42774}]", "section_header": "[{\"end\":2366,\"start\":2354},{\"end\":8054,\"start\":8052},{\"end\":8753,\"start\":8703},{\"end\":11157,\"start\":11121},{\"end\":13033,\"start\":13007},{\"end\":15150,\"start\":15114},{\"end\":15413,\"start\":15388},{\"end\":15440,\"start\":15416},{\"end\":16132,\"start\":16127},{\"end\":17678,\"start\":17612},{\"end\":18539,\"start\":18511},{\"end\":20549,\"start\":20542},{\"end\":23327,\"start\":23311},{\"end\":26149,\"start\":26116},{\"end\":27230,\"start\":27168},{\"end\":27430,\"start\":27233},{\"end\":27436,\"start\":27433},{\"end\":28660,\"start\":28635},{\"end\":31182,\"start\":31172},{\"end\":33598,\"start\":33561},{\"end\":35204,\"start\":35140},{\"end\":38350,\"start\":38316},{\"end\":40264,\"start\":40235},{\"end\":42083,\"start\":42053},{\"end\":44581,\"start\":44571},{\"end\":44677,\"start\":44667},{\"end\":44716,\"start\":44706},{\"end\":44866,\"start\":44856},{\"end\":44942,\"start\":44933},{\"end\":45741,\"start\":45732},{\"end\":45793,\"start\":45784},{\"end\":45846,\"start\":45837},{\"end\":46440,\"start\":46431},{\"end\":46845,\"start\":46836},{\"end\":47602,\"start\":47593},{\"end\":47670,\"start\":47660}]", "table": "[{\"end\":45730,\"start\":45340},{\"end\":46429,\"start\":46064},{\"end\":46834,\"start\":46568},{\"end\":47591,\"start\":46947},{\"end\":48044,\"start\":47779}]", "figure_caption": "[{\"end\":44665,\"start\":44583},{\"end\":44704,\"start\":44679},{\"end\":44854,\"start\":44718},{\"end\":44931,\"start\":44868},{\"end\":45001,\"start\":44944},{\"end\":45340,\"start\":45004},{\"end\":45782,\"start\":45743},{\"end\":45835,\"start\":45795},{\"end\":46064,\"start\":45848},{\"end\":46568,\"start\":46442},{\"end\":46904,\"start\":46847},{\"end\":46947,\"start\":46907},{\"end\":47658,\"start\":47604},{\"end\":47722,\"start\":47673},{\"end\":47779,\"start\":47725}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6743,\"start\":6735},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15110,\"start\":15102},{\"end\":16037,\"start\":16029},{\"end\":16992,\"start\":16984},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24728,\"start\":24720},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35439,\"start\":35424},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35650,\"start\":35642},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36507,\"start\":36495},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37349,\"start\":37341},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37552,\"start\":37544},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37957,\"start\":37948},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38113,\"start\":38105}]", "bib_author_first_name": "[{\"end\":48423,\"start\":48422},{\"end\":48436,\"start\":48435},{\"end\":48449,\"start\":48448},{\"end\":48458,\"start\":48457},{\"end\":49033,\"start\":49032},{\"end\":49044,\"start\":49043},{\"end\":49050,\"start\":49049},{\"end\":49820,\"start\":49819},{\"end\":49829,\"start\":49828},{\"end\":49837,\"start\":49836},{\"end\":49846,\"start\":49845},{\"end\":49857,\"start\":49856},{\"end\":49859,\"start\":49858},{\"end\":49869,\"start\":49868},{\"end\":49881,\"start\":49880},{\"end\":49896,\"start\":49895},{\"end\":49905,\"start\":49904},{\"end\":49915,\"start\":49914},{\"end\":50324,\"start\":50323},{\"end\":50339,\"start\":50338},{\"end\":50348,\"start\":50347},{\"end\":50355,\"start\":50354},{\"end\":50363,\"start\":50362},{\"end\":50370,\"start\":50369},{\"end\":50378,\"start\":50377},{\"end\":50386,\"start\":50385},{\"end\":50394,\"start\":50393},{\"end\":50400,\"start\":50399},{\"end\":50406,\"start\":50405},{\"end\":51076,\"start\":51075},{\"end\":51085,\"start\":51084},{\"end\":51095,\"start\":51094},{\"end\":51097,\"start\":51096},{\"end\":51107,\"start\":51106},{\"end\":51116,\"start\":51115},{\"end\":51126,\"start\":51125},{\"end\":51569,\"start\":51565},{\"end\":51579,\"start\":51578},{\"end\":51585,\"start\":51584},{\"end\":51592,\"start\":51591},{\"end\":51601,\"start\":51600},{\"end\":51607,\"start\":51606},{\"end\":51616,\"start\":51615},{\"end\":51625,\"start\":51624},{\"end\":51635,\"start\":51634},{\"end\":51645,\"start\":51644},{\"end\":51647,\"start\":51646},{\"end\":52011,\"start\":52010},{\"end\":52018,\"start\":52017},{\"end\":52024,\"start\":52023},{\"end\":52031,\"start\":52030},{\"end\":52039,\"start\":52038},{\"end\":52338,\"start\":52337},{\"end\":52351,\"start\":52347},{\"end\":52360,\"start\":52359},{\"end\":52367,\"start\":52366},{\"end\":53168,\"start\":53167},{\"end\":53181,\"start\":53180},{\"end\":53200,\"start\":53199},{\"end\":53202,\"start\":53201},{\"end\":53411,\"start\":53410},{\"end\":53423,\"start\":53422},{\"end\":53435,\"start\":53434},{\"end\":53445,\"start\":53444},{\"end\":53460,\"start\":53459},{\"end\":53467,\"start\":53466},{\"end\":53481,\"start\":53480},{\"end\":53485,\"start\":53482},{\"end\":53494,\"start\":53493},{\"end\":53496,\"start\":53495},{\"end\":53509,\"start\":53508},{\"end\":53518,\"start\":53517},{\"end\":53925,\"start\":53924},{\"end\":53934,\"start\":53933},{\"end\":53946,\"start\":53945},{\"end\":54149,\"start\":54148},{\"end\":54158,\"start\":54157},{\"end\":54165,\"start\":54164},{\"end\":54171,\"start\":54170},{\"end\":54180,\"start\":54179},{\"end\":54189,\"start\":54188},{\"end\":54197,\"start\":54196},{\"end\":54203,\"start\":54202},{\"end\":54487,\"start\":54486},{\"end\":54495,\"start\":54494},{\"end\":54497,\"start\":54496},{\"end\":54504,\"start\":54503},{\"end\":54516,\"start\":54515},{\"end\":54528,\"start\":54527},{\"end\":54537,\"start\":54536},{\"end\":54543,\"start\":54542},{\"end\":54554,\"start\":54553},{\"end\":54562,\"start\":54561},{\"end\":54569,\"start\":54568},{\"end\":54571,\"start\":54570},{\"end\":54961,\"start\":54960},{\"end\":54970,\"start\":54969},{\"end\":54977,\"start\":54976},{\"end\":54983,\"start\":54982},{\"end\":54990,\"start\":54989},{\"end\":54998,\"start\":54997},{\"end\":55005,\"start\":55004},{\"end\":55014,\"start\":55013},{\"end\":55024,\"start\":55023},{\"end\":55264,\"start\":55263},{\"end\":55274,\"start\":55273},{\"end\":55288,\"start\":55287},{\"end\":55300,\"start\":55299},{\"end\":55302,\"start\":55301},{\"end\":55311,\"start\":55310},{\"end\":55320,\"start\":55319},{\"end\":55329,\"start\":55328},{\"end\":55337,\"start\":55336},{\"end\":55348,\"start\":55347},{\"end\":55354,\"start\":55353},{\"end\":55732,\"start\":55731},{\"end\":55739,\"start\":55738},{\"end\":55747,\"start\":55746},{\"end\":55754,\"start\":55753},{\"end\":55761,\"start\":55760},{\"end\":55768,\"start\":55767},{\"end\":55770,\"start\":55769},{\"end\":55776,\"start\":55775},{\"end\":56070,\"start\":56069},{\"end\":56083,\"start\":56082},{\"end\":56097,\"start\":56096},{\"end\":56106,\"start\":56105},{\"end\":56114,\"start\":56113},{\"end\":56129,\"start\":56128},{\"end\":56141,\"start\":56140},{\"end\":56150,\"start\":56149},{\"end\":56158,\"start\":56157},{\"end\":56168,\"start\":56167},{\"end\":56572,\"start\":56568},{\"end\":56916,\"start\":56915},{\"end\":56924,\"start\":56923},{\"end\":56931,\"start\":56930},{\"end\":56938,\"start\":56937},{\"end\":56944,\"start\":56943},{\"end\":56951,\"start\":56950},{\"end\":56958,\"start\":56957},{\"end\":56964,\"start\":56963},{\"end\":56973,\"start\":56972},{\"end\":56984,\"start\":56980},{\"end\":56988,\"start\":56987},{\"end\":57542,\"start\":57541},{\"end\":57556,\"start\":57555},{\"end\":57796,\"start\":57795},{\"end\":57806,\"start\":57805},{\"end\":57816,\"start\":57815},{\"end\":57825,\"start\":57824},{\"end\":58156,\"start\":58155},{\"end\":58166,\"start\":58165},{\"end\":58178,\"start\":58177},{\"end\":58187,\"start\":58186},{\"end\":58569,\"start\":58568},{\"end\":58584,\"start\":58583},{\"end\":58592,\"start\":58591},{\"end\":58604,\"start\":58603},{\"end\":58615,\"start\":58614},{\"end\":58627,\"start\":58626},{\"end\":58629,\"start\":58628},{\"end\":58637,\"start\":58636},{\"end\":58639,\"start\":58638},{\"end\":58647,\"start\":58646},{\"end\":58658,\"start\":58654},{\"end\":58666,\"start\":58665},{\"end\":59077,\"start\":59076},{\"end\":59087,\"start\":59086},{\"end\":59093,\"start\":59092},{\"end\":59102,\"start\":59101},{\"end\":59113,\"start\":59112},{\"end\":59127,\"start\":59126},{\"end\":59138,\"start\":59137},{\"end\":59147,\"start\":59146},{\"end\":59158,\"start\":59157},{\"end\":59167,\"start\":59166},{\"end\":59495,\"start\":59494},{\"end\":59505,\"start\":59504},{\"end\":59517,\"start\":59516},{\"end\":59528,\"start\":59527},{\"end\":59540,\"start\":59539},{\"end\":59552,\"start\":59551},{\"end\":59565,\"start\":59564},{\"end\":59576,\"start\":59575},{\"end\":59590,\"start\":59589},{\"end\":60064,\"start\":60063},{\"end\":60075,\"start\":60074},{\"end\":60088,\"start\":60087},{\"end\":60520,\"start\":60519},{\"end\":60531,\"start\":60530},{\"end\":60542,\"start\":60541},{\"end\":60557,\"start\":60556},{\"end\":60573,\"start\":60572},{\"end\":60592,\"start\":60591},{\"end\":61194,\"start\":61193},{\"end\":61201,\"start\":61200},{\"end\":61209,\"start\":61208},{\"end\":61218,\"start\":61217},{\"end\":61226,\"start\":61225},{\"end\":61232,\"start\":61231},{\"end\":61241,\"start\":61240},{\"end\":61249,\"start\":61248},{\"end\":61255,\"start\":61254},{\"end\":61266,\"start\":61262},{\"end\":61804,\"start\":61803},{\"end\":61815,\"start\":61814},{\"end\":61821,\"start\":61820},{\"end\":61830,\"start\":61829},{\"end\":61838,\"start\":61837},{\"end\":61848,\"start\":61847},{\"end\":62035,\"start\":62034},{\"end\":62037,\"start\":62036},{\"end\":62044,\"start\":62043},{\"end\":62056,\"start\":62055},{\"end\":62063,\"start\":62062},{\"end\":62075,\"start\":62074},{\"end\":62087,\"start\":62086},{\"end\":62095,\"start\":62094},{\"end\":62108,\"start\":62107},{\"end\":62121,\"start\":62120},{\"end\":62129,\"start\":62128},{\"end\":62576,\"start\":62575},{\"end\":62586,\"start\":62585},{\"end\":62597,\"start\":62596},{\"end\":62608,\"start\":62607},{\"end\":62615,\"start\":62614},{\"end\":62625,\"start\":62624},{\"end\":62635,\"start\":62634},{\"end\":62643,\"start\":62642},{\"end\":62649,\"start\":62648},{\"end\":62651,\"start\":62650},{\"end\":63048,\"start\":63047},{\"end\":63059,\"start\":63058},{\"end\":63067,\"start\":63066},{\"end\":63078,\"start\":63077},{\"end\":63087,\"start\":63086},{\"end\":63524,\"start\":63523},{\"end\":63532,\"start\":63531},{\"end\":63542,\"start\":63541},{\"end\":63552,\"start\":63551},{\"end\":63554,\"start\":63553},{\"end\":63562,\"start\":63561},{\"end\":63574,\"start\":63573},{\"end\":63586,\"start\":63585},{\"end\":63597,\"start\":63596},{\"end\":63609,\"start\":63608},{\"end\":63611,\"start\":63610},{\"end\":63619,\"start\":63618},{\"end\":63896,\"start\":63895},{\"end\":63898,\"start\":63897},{\"end\":63906,\"start\":63905},{\"end\":63913,\"start\":63912},{\"end\":63922,\"start\":63921},{\"end\":63933,\"start\":63932},{\"end\":63941,\"start\":63940},{\"end\":63952,\"start\":63951},{\"end\":63964,\"start\":63963},{\"end\":63966,\"start\":63965},{\"end\":63978,\"start\":63977},{\"end\":63986,\"start\":63985},{\"end\":64372,\"start\":64371},{\"end\":64383,\"start\":64382},{\"end\":64392,\"start\":64391},{\"end\":64398,\"start\":64397},{\"end\":64400,\"start\":64399},{\"end\":64411,\"start\":64410},{\"end\":64418,\"start\":64417},{\"end\":64420,\"start\":64419},{\"end\":64429,\"start\":64428},{\"end\":64439,\"start\":64438},{\"end\":64450,\"start\":64449},{\"end\":64464,\"start\":64463},{\"end\":64787,\"start\":64786},{\"end\":64796,\"start\":64795},{\"end\":64807,\"start\":64806},{\"end\":64817,\"start\":64816},{\"end\":64830,\"start\":64829},{\"end\":64845,\"start\":64844},{\"end\":64855,\"start\":64854},{\"end\":64862,\"start\":64861},{\"end\":64876,\"start\":64875},{\"end\":64887,\"start\":64886},{\"end\":65276,\"start\":65275},{\"end\":65285,\"start\":65284},{\"end\":65298,\"start\":65297},{\"end\":65307,\"start\":65306},{\"end\":65317,\"start\":65316},{\"end\":65323,\"start\":65322},{\"end\":65335,\"start\":65334},{\"end\":65344,\"start\":65343},{\"end\":65346,\"start\":65345},{\"end\":65524,\"start\":65523},{\"end\":65534,\"start\":65533},{\"end\":65544,\"start\":65543},{\"end\":65556,\"start\":65555},{\"end\":65567,\"start\":65566},{\"end\":65580,\"start\":65579},{\"end\":65591,\"start\":65590},{\"end\":65602,\"start\":65601},{\"end\":65612,\"start\":65611},{\"end\":65986,\"start\":65985},{\"end\":65997,\"start\":65996},{\"end\":66007,\"start\":66006},{\"end\":66018,\"start\":66017},{\"end\":66033,\"start\":66029},{\"end\":66044,\"start\":66043},{\"end\":66055,\"start\":66054},{\"end\":66066,\"start\":66065},{\"end\":66075,\"start\":66074},{\"end\":66085,\"start\":66084},{\"end\":66946,\"start\":66945},{\"end\":66954,\"start\":66953},{\"end\":66963,\"start\":66962},{\"end\":66971,\"start\":66970},{\"end\":66979,\"start\":66978},{\"end\":66988,\"start\":66987},{\"end\":66997,\"start\":66996},{\"end\":67774,\"start\":67773},{\"end\":67782,\"start\":67781},{\"end\":67789,\"start\":67788},{\"end\":67803,\"start\":67802},{\"end\":67805,\"start\":67804},{\"end\":67811,\"start\":67810},{\"end\":67813,\"start\":67812},{\"end\":67820,\"start\":67819},{\"end\":67830,\"start\":67829},{\"end\":67843,\"start\":67842},{\"end\":68261,\"start\":68260},{\"end\":68269,\"start\":68268},{\"end\":68279,\"start\":68278},{\"end\":68299,\"start\":68298},{\"end\":68308,\"start\":68307},{\"end\":68319,\"start\":68318},{\"end\":68327,\"start\":68326},{\"end\":68336,\"start\":68335},{\"end\":68338,\"start\":68337},{\"end\":68354,\"start\":68353},{\"end\":68367,\"start\":68366},{\"end\":68946,\"start\":68945},{\"end\":68954,\"start\":68953},{\"end\":68964,\"start\":68963},{\"end\":69195,\"start\":69194},{\"end\":69202,\"start\":69201},{\"end\":69211,\"start\":69210},{\"end\":69213,\"start\":69212},{\"end\":69221,\"start\":69220},{\"end\":69228,\"start\":69227},{\"end\":69230,\"start\":69229},{\"end\":69236,\"start\":69235},{\"end\":69246,\"start\":69245},{\"end\":69252,\"start\":69251},{\"end\":69254,\"start\":69253},{\"end\":69261,\"start\":69260},{\"end\":69263,\"start\":69262},{\"end\":69559,\"start\":69558},{\"end\":69566,\"start\":69565},{\"end\":69574,\"start\":69573},{\"end\":69588,\"start\":69587},{\"end\":69597,\"start\":69596},{\"end\":69604,\"start\":69603},{\"end\":69611,\"start\":69610},{\"end\":69613,\"start\":69612},{\"end\":69619,\"start\":69618},{\"end\":69920,\"start\":69919},{\"end\":69926,\"start\":69925},{\"end\":69935,\"start\":69934},{\"end\":69941,\"start\":69940},{\"end\":69956,\"start\":69955},{\"end\":69966,\"start\":69965},{\"end\":69978,\"start\":69977},{\"end\":69990,\"start\":69989},{\"end\":70003,\"start\":70002},{\"end\":70412,\"start\":70411},{\"end\":70418,\"start\":70417},{\"end\":70424,\"start\":70423},{\"end\":70432,\"start\":70431},{\"end\":70440,\"start\":70439},{\"end\":70446,\"start\":70445},{\"end\":70452,\"start\":70451},{\"end\":70459,\"start\":70458},{\"end\":70818,\"start\":70817},{\"end\":70829,\"start\":70825},{\"end\":70836,\"start\":70835},{\"end\":70838,\"start\":70837},{\"end\":71061,\"start\":71060},{\"end\":71070,\"start\":71069},{\"end\":71078,\"start\":71077},{\"end\":71085,\"start\":71084},{\"end\":71095,\"start\":71094},{\"end\":71103,\"start\":71102},{\"end\":71109,\"start\":71104},{\"end\":71472,\"start\":71471},{\"end\":71481,\"start\":71480},{\"end\":71491,\"start\":71490},{\"end\":71498,\"start\":71497},{\"end\":71504,\"start\":71503},{\"end\":71512,\"start\":71511},{\"end\":71729,\"start\":71728},{\"end\":71738,\"start\":71737},{\"end\":71748,\"start\":71747},{\"end\":71757,\"start\":71756},{\"end\":71768,\"start\":71767},{\"end\":71776,\"start\":71775},{\"end\":71784,\"start\":71783},{\"end\":71793,\"start\":71792},{\"end\":71801,\"start\":71800},{\"end\":71807,\"start\":71806},{\"end\":71809,\"start\":71808},{\"end\":72171,\"start\":72170},{\"end\":72183,\"start\":72179},{\"end\":72191,\"start\":72190},{\"end\":72197,\"start\":72196},{\"end\":72204,\"start\":72203},{\"end\":72213,\"start\":72212},{\"end\":72610,\"start\":72609},{\"end\":72619,\"start\":72618},{\"end\":72627,\"start\":72626},{\"end\":72907,\"start\":72906},{\"end\":72915,\"start\":72914},{\"end\":72923,\"start\":72922},{\"end\":73335,\"start\":73334},{\"end\":73337,\"start\":73336},{\"end\":73345,\"start\":73344},{\"end\":73353,\"start\":73352},{\"end\":73359,\"start\":73358},{\"end\":73367,\"start\":73366},{\"end\":73375,\"start\":73374},{\"end\":73382,\"start\":73381},{\"end\":73389,\"start\":73388},{\"end\":73398,\"start\":73397},{\"end\":73407,\"start\":73406},{\"end\":73744,\"start\":73743},{\"end\":73752,\"start\":73751},{\"end\":73759,\"start\":73758},{\"end\":73768,\"start\":73767},{\"end\":73777,\"start\":73773},{\"end\":73781,\"start\":73780},{\"end\":74135,\"start\":74134},{\"end\":74142,\"start\":74141},{\"end\":74150,\"start\":74149},{\"end\":74156,\"start\":74155},{\"end\":74162,\"start\":74161},{\"end\":74168,\"start\":74167}]", "bib_author_last_name": "[{\"end\":48433,\"start\":48424},{\"end\":48446,\"start\":48437},{\"end\":48455,\"start\":48450},{\"end\":48468,\"start\":48459},{\"end\":49041,\"start\":49034},{\"end\":49047,\"start\":49045},{\"end\":49056,\"start\":49051},{\"end\":49826,\"start\":49821},{\"end\":49834,\"start\":49830},{\"end\":49843,\"start\":49838},{\"end\":49854,\"start\":49847},{\"end\":49866,\"start\":49860},{\"end\":49878,\"start\":49870},{\"end\":49893,\"start\":49882},{\"end\":49902,\"start\":49897},{\"end\":49912,\"start\":49906},{\"end\":49922,\"start\":49916},{\"end\":50336,\"start\":50325},{\"end\":50345,\"start\":50340},{\"end\":50352,\"start\":50349},{\"end\":50360,\"start\":50356},{\"end\":50367,\"start\":50364},{\"end\":50375,\"start\":50371},{\"end\":50383,\"start\":50379},{\"end\":50391,\"start\":50387},{\"end\":50397,\"start\":50395},{\"end\":50403,\"start\":50401},{\"end\":50411,\"start\":50407},{\"end\":51082,\"start\":51077},{\"end\":51092,\"start\":51086},{\"end\":51104,\"start\":51098},{\"end\":51113,\"start\":51108},{\"end\":51123,\"start\":51117},{\"end\":51132,\"start\":51127},{\"end\":51576,\"start\":51570},{\"end\":51582,\"start\":51580},{\"end\":51589,\"start\":51586},{\"end\":51598,\"start\":51593},{\"end\":51604,\"start\":51602},{\"end\":51613,\"start\":51608},{\"end\":51622,\"start\":51617},{\"end\":51632,\"start\":51626},{\"end\":51642,\"start\":51636},{\"end\":51656,\"start\":51648},{\"end\":52015,\"start\":52012},{\"end\":52021,\"start\":52019},{\"end\":52028,\"start\":52025},{\"end\":52036,\"start\":52032},{\"end\":52044,\"start\":52040},{\"end\":52345,\"start\":52339},{\"end\":52357,\"start\":52352},{\"end\":52364,\"start\":52361},{\"end\":52377,\"start\":52368},{\"end\":53178,\"start\":53169},{\"end\":53197,\"start\":53182},{\"end\":53210,\"start\":53203},{\"end\":53420,\"start\":53412},{\"end\":53432,\"start\":53424},{\"end\":53442,\"start\":53436},{\"end\":53457,\"start\":53446},{\"end\":53464,\"start\":53461},{\"end\":53478,\"start\":53468},{\"end\":53491,\"start\":53486},{\"end\":53506,\"start\":53497},{\"end\":53515,\"start\":53510},{\"end\":53524,\"start\":53519},{\"end\":53931,\"start\":53926},{\"end\":53943,\"start\":53935},{\"end\":53956,\"start\":53947},{\"end\":54155,\"start\":54150},{\"end\":54162,\"start\":54159},{\"end\":54168,\"start\":54166},{\"end\":54177,\"start\":54172},{\"end\":54186,\"start\":54181},{\"end\":54194,\"start\":54190},{\"end\":54200,\"start\":54198},{\"end\":54208,\"start\":54204},{\"end\":54492,\"start\":54488},{\"end\":54501,\"start\":54498},{\"end\":54513,\"start\":54505},{\"end\":54525,\"start\":54517},{\"end\":54534,\"start\":54529},{\"end\":54540,\"start\":54538},{\"end\":54551,\"start\":54544},{\"end\":54559,\"start\":54555},{\"end\":54566,\"start\":54563},{\"end\":54577,\"start\":54572},{\"end\":54967,\"start\":54962},{\"end\":54974,\"start\":54971},{\"end\":54980,\"start\":54978},{\"end\":54987,\"start\":54984},{\"end\":54995,\"start\":54991},{\"end\":55002,\"start\":54999},{\"end\":55011,\"start\":55006},{\"end\":55021,\"start\":55015},{\"end\":55028,\"start\":55025},{\"end\":55271,\"start\":55265},{\"end\":55285,\"start\":55275},{\"end\":55297,\"start\":55289},{\"end\":55308,\"start\":55303},{\"end\":55317,\"start\":55312},{\"end\":55326,\"start\":55321},{\"end\":55334,\"start\":55330},{\"end\":55345,\"start\":55338},{\"end\":55351,\"start\":55349},{\"end\":55361,\"start\":55355},{\"end\":55736,\"start\":55733},{\"end\":55744,\"start\":55740},{\"end\":55751,\"start\":55748},{\"end\":55758,\"start\":55755},{\"end\":55765,\"start\":55762},{\"end\":55773,\"start\":55771},{\"end\":55781,\"start\":55777},{\"end\":56080,\"start\":56071},{\"end\":56094,\"start\":56084},{\"end\":56103,\"start\":56098},{\"end\":56111,\"start\":56107},{\"end\":56126,\"start\":56115},{\"end\":56138,\"start\":56130},{\"end\":56147,\"start\":56142},{\"end\":56155,\"start\":56151},{\"end\":56165,\"start\":56159},{\"end\":56180,\"start\":56169},{\"end\":56576,\"start\":56573},{\"end\":56921,\"start\":56917},{\"end\":56928,\"start\":56925},{\"end\":56935,\"start\":56932},{\"end\":56941,\"start\":56939},{\"end\":56948,\"start\":56945},{\"end\":56955,\"start\":56952},{\"end\":56961,\"start\":56959},{\"end\":56970,\"start\":56965},{\"end\":56978,\"start\":56974},{\"end\":57553,\"start\":57543},{\"end\":57563,\"start\":57557},{\"end\":57803,\"start\":57797},{\"end\":57813,\"start\":57807},{\"end\":57822,\"start\":57817},{\"end\":57833,\"start\":57826},{\"end\":58163,\"start\":58157},{\"end\":58175,\"start\":58167},{\"end\":58184,\"start\":58179},{\"end\":58198,\"start\":58188},{\"end\":58581,\"start\":58570},{\"end\":58589,\"start\":58585},{\"end\":58601,\"start\":58593},{\"end\":58612,\"start\":58605},{\"end\":58624,\"start\":58616},{\"end\":58634,\"start\":58630},{\"end\":58644,\"start\":58640},{\"end\":58652,\"start\":58648},{\"end\":58663,\"start\":58659},{\"end\":58677,\"start\":58667},{\"end\":59084,\"start\":59078},{\"end\":59090,\"start\":59088},{\"end\":59099,\"start\":59094},{\"end\":59110,\"start\":59103},{\"end\":59124,\"start\":59114},{\"end\":59135,\"start\":59128},{\"end\":59144,\"start\":59139},{\"end\":59155,\"start\":59148},{\"end\":59164,\"start\":59159},{\"end\":59171,\"start\":59168},{\"end\":59502,\"start\":59496},{\"end\":59514,\"start\":59506},{\"end\":59525,\"start\":59518},{\"end\":59537,\"start\":59529},{\"end\":59549,\"start\":59541},{\"end\":59562,\"start\":59553},{\"end\":59573,\"start\":59566},{\"end\":59587,\"start\":59577},{\"end\":59597,\"start\":59591},{\"end\":60072,\"start\":60065},{\"end\":60085,\"start\":60076},{\"end\":60096,\"start\":60089},{\"end\":60528,\"start\":60521},{\"end\":60539,\"start\":60532},{\"end\":60554,\"start\":60543},{\"end\":60570,\"start\":60558},{\"end\":60589,\"start\":60574},{\"end\":60602,\"start\":60593},{\"end\":61198,\"start\":61195},{\"end\":61206,\"start\":61202},{\"end\":61215,\"start\":61210},{\"end\":61223,\"start\":61219},{\"end\":61229,\"start\":61227},{\"end\":61238,\"start\":61233},{\"end\":61246,\"start\":61242},{\"end\":61252,\"start\":61250},{\"end\":61260,\"start\":61256},{\"end\":61271,\"start\":61267},{\"end\":61812,\"start\":61805},{\"end\":61818,\"start\":61816},{\"end\":61827,\"start\":61822},{\"end\":61835,\"start\":61831},{\"end\":61845,\"start\":61839},{\"end\":61858,\"start\":61849},{\"end\":62041,\"start\":62038},{\"end\":62053,\"start\":62045},{\"end\":62060,\"start\":62057},{\"end\":62072,\"start\":62064},{\"end\":62084,\"start\":62076},{\"end\":62092,\"start\":62088},{\"end\":62105,\"start\":62096},{\"end\":62118,\"start\":62109},{\"end\":62126,\"start\":62122},{\"end\":62135,\"start\":62130},{\"end\":62583,\"start\":62577},{\"end\":62594,\"start\":62587},{\"end\":62605,\"start\":62598},{\"end\":62612,\"start\":62609},{\"end\":62622,\"start\":62616},{\"end\":62632,\"start\":62626},{\"end\":62640,\"start\":62636},{\"end\":62646,\"start\":62644},{\"end\":62655,\"start\":62652},{\"end\":63056,\"start\":63049},{\"end\":63064,\"start\":63060},{\"end\":63075,\"start\":63068},{\"end\":63084,\"start\":63079},{\"end\":63095,\"start\":63088},{\"end\":63529,\"start\":63525},{\"end\":63539,\"start\":63533},{\"end\":63549,\"start\":63543},{\"end\":63559,\"start\":63555},{\"end\":63571,\"start\":63563},{\"end\":63583,\"start\":63575},{\"end\":63594,\"start\":63587},{\"end\":63606,\"start\":63598},{\"end\":63616,\"start\":63612},{\"end\":63624,\"start\":63620},{\"end\":63903,\"start\":63899},{\"end\":63910,\"start\":63907},{\"end\":63919,\"start\":63914},{\"end\":63930,\"start\":63923},{\"end\":63938,\"start\":63934},{\"end\":63949,\"start\":63942},{\"end\":63961,\"start\":63953},{\"end\":63975,\"start\":63967},{\"end\":63983,\"start\":63979},{\"end\":63992,\"start\":63987},{\"end\":64380,\"start\":64373},{\"end\":64389,\"start\":64384},{\"end\":64395,\"start\":64393},{\"end\":64408,\"start\":64401},{\"end\":64415,\"start\":64412},{\"end\":64426,\"start\":64421},{\"end\":64436,\"start\":64430},{\"end\":64447,\"start\":64440},{\"end\":64461,\"start\":64451},{\"end\":64470,\"start\":64465},{\"end\":64793,\"start\":64788},{\"end\":64804,\"start\":64797},{\"end\":64814,\"start\":64808},{\"end\":64827,\"start\":64818},{\"end\":64842,\"start\":64831},{\"end\":64852,\"start\":64846},{\"end\":64859,\"start\":64856},{\"end\":64873,\"start\":64863},{\"end\":64884,\"start\":64877},{\"end\":64899,\"start\":64888},{\"end\":65282,\"start\":65277},{\"end\":65295,\"start\":65286},{\"end\":65304,\"start\":65299},{\"end\":65314,\"start\":65308},{\"end\":65320,\"start\":65318},{\"end\":65332,\"start\":65324},{\"end\":65341,\"start\":65336},{\"end\":65356,\"start\":65347},{\"end\":65531,\"start\":65525},{\"end\":65541,\"start\":65535},{\"end\":65553,\"start\":65545},{\"end\":65564,\"start\":65557},{\"end\":65577,\"start\":65568},{\"end\":65588,\"start\":65581},{\"end\":65599,\"start\":65592},{\"end\":65609,\"start\":65603},{\"end\":65620,\"start\":65613},{\"end\":65994,\"start\":65987},{\"end\":66004,\"start\":65998},{\"end\":66015,\"start\":66008},{\"end\":66027,\"start\":66019},{\"end\":66041,\"start\":66034},{\"end\":66052,\"start\":66045},{\"end\":66063,\"start\":66056},{\"end\":66072,\"start\":66067},{\"end\":66082,\"start\":66076},{\"end\":66091,\"start\":66086},{\"end\":66951,\"start\":66947},{\"end\":66960,\"start\":66955},{\"end\":66968,\"start\":66964},{\"end\":66976,\"start\":66972},{\"end\":66985,\"start\":66980},{\"end\":66994,\"start\":66989},{\"end\":67000,\"start\":66998},{\"end\":67779,\"start\":67775},{\"end\":67786,\"start\":67783},{\"end\":67800,\"start\":67790},{\"end\":67808,\"start\":67806},{\"end\":67817,\"start\":67814},{\"end\":67827,\"start\":67821},{\"end\":67840,\"start\":67831},{\"end\":67848,\"start\":67844},{\"end\":68266,\"start\":68262},{\"end\":68276,\"start\":68270},{\"end\":68296,\"start\":68280},{\"end\":68305,\"start\":68300},{\"end\":68316,\"start\":68309},{\"end\":68324,\"start\":68320},{\"end\":68333,\"start\":68328},{\"end\":68351,\"start\":68339},{\"end\":68364,\"start\":68355},{\"end\":68372,\"start\":68368},{\"end\":68951,\"start\":68947},{\"end\":68961,\"start\":68955},{\"end\":68969,\"start\":68965},{\"end\":69199,\"start\":69196},{\"end\":69208,\"start\":69203},{\"end\":69218,\"start\":69214},{\"end\":69225,\"start\":69222},{\"end\":69233,\"start\":69231},{\"end\":69243,\"start\":69237},{\"end\":69249,\"start\":69247},{\"end\":69258,\"start\":69255},{\"end\":69266,\"start\":69264},{\"end\":69563,\"start\":69560},{\"end\":69571,\"start\":69567},{\"end\":69585,\"start\":69575},{\"end\":69594,\"start\":69589},{\"end\":69601,\"start\":69598},{\"end\":69608,\"start\":69605},{\"end\":69616,\"start\":69614},{\"end\":69624,\"start\":69620},{\"end\":69923,\"start\":69921},{\"end\":69932,\"start\":69927},{\"end\":69938,\"start\":69936},{\"end\":69953,\"start\":69942},{\"end\":69963,\"start\":69957},{\"end\":69975,\"start\":69967},{\"end\":69987,\"start\":69979},{\"end\":70000,\"start\":69991},{\"end\":70008,\"start\":70004},{\"end\":70415,\"start\":70413},{\"end\":70421,\"start\":70419},{\"end\":70429,\"start\":70425},{\"end\":70437,\"start\":70433},{\"end\":70443,\"start\":70441},{\"end\":70449,\"start\":70447},{\"end\":70456,\"start\":70453},{\"end\":70464,\"start\":70460},{\"end\":70823,\"start\":70819},{\"end\":70833,\"start\":70830},{\"end\":70843,\"start\":70839},{\"end\":71067,\"start\":71062},{\"end\":71075,\"start\":71071},{\"end\":71082,\"start\":71079},{\"end\":71092,\"start\":71086},{\"end\":71100,\"start\":71096},{\"end\":71115,\"start\":71110},{\"end\":71123,\"start\":71117},{\"end\":71478,\"start\":71473},{\"end\":71488,\"start\":71482},{\"end\":71495,\"start\":71492},{\"end\":71501,\"start\":71499},{\"end\":71509,\"start\":71505},{\"end\":71517,\"start\":71513},{\"end\":71735,\"start\":71730},{\"end\":71745,\"start\":71739},{\"end\":71754,\"start\":71749},{\"end\":71765,\"start\":71758},{\"end\":71773,\"start\":71769},{\"end\":71781,\"start\":71777},{\"end\":71790,\"start\":71785},{\"end\":71798,\"start\":71794},{\"end\":71804,\"start\":71802},{\"end\":71813,\"start\":71810},{\"end\":72177,\"start\":72172},{\"end\":72188,\"start\":72184},{\"end\":72194,\"start\":72192},{\"end\":72201,\"start\":72198},{\"end\":72210,\"start\":72205},{\"end\":72218,\"start\":72214},{\"end\":72616,\"start\":72611},{\"end\":72624,\"start\":72620},{\"end\":72630,\"start\":72628},{\"end\":72912,\"start\":72908},{\"end\":72920,\"start\":72916},{\"end\":72927,\"start\":72924},{\"end\":73342,\"start\":73338},{\"end\":73350,\"start\":73346},{\"end\":73356,\"start\":73354},{\"end\":73364,\"start\":73360},{\"end\":73372,\"start\":73368},{\"end\":73379,\"start\":73376},{\"end\":73386,\"start\":73383},{\"end\":73395,\"start\":73390},{\"end\":73404,\"start\":73399},{\"end\":73412,\"start\":73408},{\"end\":73749,\"start\":73745},{\"end\":73756,\"start\":73753},{\"end\":73765,\"start\":73760},{\"end\":73771,\"start\":73769},{\"end\":74139,\"start\":74136},{\"end\":74147,\"start\":74143},{\"end\":74153,\"start\":74151},{\"end\":74159,\"start\":74157},{\"end\":74165,\"start\":74163},{\"end\":74173,\"start\":74169}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218972144},\"end\":48972,\"start\":48325},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":202558505},\"end\":49778,\"start\":48974},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":50221,\"start\":49780},{\"attributes\":{\"doi\":\"arXiv:2006.14799\",\"id\":\"b3\",\"matched_paper_id\":208248357},\"end\":51000,\"start\":50223},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":229157272},\"end\":51488,\"start\":51002},{\"attributes\":{\"id\":\"b5\"},\"end\":51918,\"start\":51490},{\"attributes\":{\"doi\":\"arXiv:2306.16092\",\"id\":\"b6\"},\"end\":52253,\"start\":51920},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":53115,\"start\":52255},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":221859798},\"end\":53406,\"start\":53117},{\"attributes\":{\"doi\":\"arXiv:2203.15556\",\"id\":\"b9\"},\"end\":53847,\"start\":53408},{\"attributes\":{\"doi\":\"arXiv:1904.05342\",\"id\":\"b10\"},\"end\":54144,\"start\":53849},{\"attributes\":{\"doi\":\"arXiv:2305.15062\",\"id\":\"b11\"},\"end\":54390,\"start\":54146},{\"attributes\":{\"doi\":\"arXiv:2212.12017\",\"id\":\"b12\"},\"end\":54863,\"start\":54392},{\"attributes\":{\"doi\":\"abs/2210.03915\",\"id\":\"b13\"},\"end\":55261,\"start\":54865},{\"attributes\":{\"doi\":\"arXiv:2001.08361\",\"id\":\"b14\"},\"end\":55637,\"start\":55263},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":59291975},\"end\":56005,\"start\":55639},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":250144408},\"end\":56510,\"start\":56007},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":964287},\"end\":56848,\"start\":56512},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":247292113},\"end\":57495,\"start\":56850},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b19\"},\"end\":57722,\"start\":57497},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b20\",\"matched_paper_id\":216867120},\"end\":58078,\"start\":57724},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":237421373},\"end\":58566,\"start\":58080},{\"attributes\":{\"doi\":\"arXiv:2211.01786\",\"id\":\"b22\"},\"end\":59005,\"start\":58568},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":246426909},\"end\":59492,\"start\":59007},{\"attributes\":{\"doi\":\"arXiv:2306.01116\",\"id\":\"b24\"},\"end\":60001,\"start\":59494},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218470517},\"end\":60463,\"start\":60003},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":61955135},\"end\":61101,\"start\":60465},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":251554861},\"end\":61748,\"start\":61103},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":160025533},\"end\":62032,\"start\":61750},{\"attributes\":{\"doi\":\"arXiv:2112.11446\",\"id\":\"b29\"},\"end\":62490,\"start\":62034},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":204838007},\"end\":62956,\"start\":62492},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":202565722},\"end\":63454,\"start\":62958},{\"attributes\":{\"doi\":\"arXiv:2110.08207\",\"id\":\"b32\"},\"end\":63893,\"start\":63456},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b33\"},\"end\":64320,\"start\":63895},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":255124952},\"end\":64679,\"start\":64322},{\"attributes\":{\"doi\":\"arXiv:2201.11990\",\"id\":\"b35\"},\"end\":65218,\"start\":64681},{\"attributes\":{\"id\":\"b36\"},\"end\":65521,\"start\":65220},{\"attributes\":{\"doi\":\"arXiv:2211.09085\",\"id\":\"b37\"},\"end\":65904,\"start\":65523},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b38\",\"matched_paper_id\":218913517},\"end\":66851,\"start\":65906},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":234337605},\"end\":67698,\"start\":66853},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":247595263},\"end\":68167,\"start\":67700},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":253098274},\"end\":68879,\"start\":68169},{\"attributes\":{\"doi\":\"arXiv:1912.07840\",\"id\":\"b42\"},\"end\":69142,\"start\":68881},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b43\"},\"end\":69485,\"start\":69144},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":246411621},\"end\":69917,\"start\":69487},{\"attributes\":{\"doi\":\"arXiv:2303.17564\",\"id\":\"b45\"},\"end\":70290,\"start\":69919},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":233231287},\"end\":70815,\"start\":70292},{\"attributes\":{\"doi\":\"arXiv:2306.06031\",\"id\":\"b47\"},\"end\":71058,\"start\":70817},{\"attributes\":{\"doi\":\"arXiv:2009.02835\",\"id\":\"b48\"},\"end\":71410,\"start\":71060},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":218862749},\"end\":71726,\"start\":71412},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b50\"},\"end\":72100,\"start\":71728},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":233481294},\"end\":72518,\"start\":72102},{\"attributes\":{\"doi\":\"arXiv:2305.12002\",\"id\":\"b52\"},\"end\":72823,\"start\":72520},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":207757947},\"end\":73330,\"start\":72825},{\"attributes\":{\"doi\":\"arXiv:2303.18223\",\"id\":\"b54\"},\"end\":73671,\"start\":73332},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":259298583},\"end\":74049,\"start\":73673},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":221703022},\"end\":74606,\"start\":74051}]", "bib_title": "[{\"end\":48420,\"start\":48325},{\"end\":49030,\"start\":48974},{\"end\":49817,\"start\":49780},{\"end\":50321,\"start\":50223},{\"end\":51073,\"start\":51002},{\"end\":52335,\"start\":52255},{\"end\":53165,\"start\":53117},{\"end\":55729,\"start\":55639},{\"end\":56067,\"start\":56007},{\"end\":56566,\"start\":56512},{\"end\":56913,\"start\":56850},{\"end\":57793,\"start\":57724},{\"end\":58153,\"start\":58080},{\"end\":59074,\"start\":59007},{\"end\":60061,\"start\":60003},{\"end\":60517,\"start\":60465},{\"end\":61191,\"start\":61103},{\"end\":61801,\"start\":61750},{\"end\":62573,\"start\":62492},{\"end\":63045,\"start\":62958},{\"end\":64369,\"start\":64322},{\"end\":65983,\"start\":65906},{\"end\":66943,\"start\":66853},{\"end\":67771,\"start\":67700},{\"end\":68258,\"start\":68169},{\"end\":69556,\"start\":69487},{\"end\":70409,\"start\":70292},{\"end\":71469,\"start\":71412},{\"end\":72168,\"start\":72102},{\"end\":72904,\"start\":72825},{\"end\":73741,\"start\":73673},{\"end\":74132,\"start\":74051}]", "bib_author": "[{\"end\":48435,\"start\":48422},{\"end\":48448,\"start\":48435},{\"end\":48457,\"start\":48448},{\"end\":48470,\"start\":48457},{\"end\":49043,\"start\":49032},{\"end\":49049,\"start\":49043},{\"end\":49058,\"start\":49049},{\"end\":49828,\"start\":49819},{\"end\":49836,\"start\":49828},{\"end\":49845,\"start\":49836},{\"end\":49856,\"start\":49845},{\"end\":49868,\"start\":49856},{\"end\":49880,\"start\":49868},{\"end\":49895,\"start\":49880},{\"end\":49904,\"start\":49895},{\"end\":49914,\"start\":49904},{\"end\":49924,\"start\":49914},{\"end\":50338,\"start\":50323},{\"end\":50347,\"start\":50338},{\"end\":50354,\"start\":50347},{\"end\":50362,\"start\":50354},{\"end\":50369,\"start\":50362},{\"end\":50377,\"start\":50369},{\"end\":50385,\"start\":50377},{\"end\":50393,\"start\":50385},{\"end\":50399,\"start\":50393},{\"end\":50405,\"start\":50399},{\"end\":50413,\"start\":50405},{\"end\":51084,\"start\":51075},{\"end\":51094,\"start\":51084},{\"end\":51106,\"start\":51094},{\"end\":51115,\"start\":51106},{\"end\":51125,\"start\":51115},{\"end\":51134,\"start\":51125},{\"end\":51578,\"start\":51565},{\"end\":51584,\"start\":51578},{\"end\":51591,\"start\":51584},{\"end\":51600,\"start\":51591},{\"end\":51606,\"start\":51600},{\"end\":51615,\"start\":51606},{\"end\":51624,\"start\":51615},{\"end\":51634,\"start\":51624},{\"end\":51644,\"start\":51634},{\"end\":51658,\"start\":51644},{\"end\":52017,\"start\":52010},{\"end\":52023,\"start\":52017},{\"end\":52030,\"start\":52023},{\"end\":52038,\"start\":52030},{\"end\":52046,\"start\":52038},{\"end\":52347,\"start\":52337},{\"end\":52359,\"start\":52347},{\"end\":52366,\"start\":52359},{\"end\":52379,\"start\":52366},{\"end\":53180,\"start\":53167},{\"end\":53199,\"start\":53180},{\"end\":53212,\"start\":53199},{\"end\":53422,\"start\":53410},{\"end\":53434,\"start\":53422},{\"end\":53444,\"start\":53434},{\"end\":53459,\"start\":53444},{\"end\":53466,\"start\":53459},{\"end\":53480,\"start\":53466},{\"end\":53493,\"start\":53480},{\"end\":53508,\"start\":53493},{\"end\":53517,\"start\":53508},{\"end\":53526,\"start\":53517},{\"end\":53933,\"start\":53924},{\"end\":53945,\"start\":53933},{\"end\":53958,\"start\":53945},{\"end\":54157,\"start\":54148},{\"end\":54164,\"start\":54157},{\"end\":54170,\"start\":54164},{\"end\":54179,\"start\":54170},{\"end\":54188,\"start\":54179},{\"end\":54196,\"start\":54188},{\"end\":54202,\"start\":54196},{\"end\":54210,\"start\":54202},{\"end\":54494,\"start\":54486},{\"end\":54503,\"start\":54494},{\"end\":54515,\"start\":54503},{\"end\":54527,\"start\":54515},{\"end\":54536,\"start\":54527},{\"end\":54542,\"start\":54536},{\"end\":54553,\"start\":54542},{\"end\":54561,\"start\":54553},{\"end\":54568,\"start\":54561},{\"end\":54579,\"start\":54568},{\"end\":54969,\"start\":54960},{\"end\":54976,\"start\":54969},{\"end\":54982,\"start\":54976},{\"end\":54989,\"start\":54982},{\"end\":54997,\"start\":54989},{\"end\":55004,\"start\":54997},{\"end\":55013,\"start\":55004},{\"end\":55023,\"start\":55013},{\"end\":55030,\"start\":55023},{\"end\":55273,\"start\":55263},{\"end\":55287,\"start\":55273},{\"end\":55299,\"start\":55287},{\"end\":55310,\"start\":55299},{\"end\":55319,\"start\":55310},{\"end\":55328,\"start\":55319},{\"end\":55336,\"start\":55328},{\"end\":55347,\"start\":55336},{\"end\":55353,\"start\":55347},{\"end\":55363,\"start\":55353},{\"end\":55738,\"start\":55731},{\"end\":55746,\"start\":55738},{\"end\":55753,\"start\":55746},{\"end\":55760,\"start\":55753},{\"end\":55767,\"start\":55760},{\"end\":55775,\"start\":55767},{\"end\":55783,\"start\":55775},{\"end\":56082,\"start\":56069},{\"end\":56096,\"start\":56082},{\"end\":56105,\"start\":56096},{\"end\":56113,\"start\":56105},{\"end\":56128,\"start\":56113},{\"end\":56140,\"start\":56128},{\"end\":56149,\"start\":56140},{\"end\":56157,\"start\":56149},{\"end\":56167,\"start\":56157},{\"end\":56182,\"start\":56167},{\"end\":56578,\"start\":56568},{\"end\":56923,\"start\":56915},{\"end\":56930,\"start\":56923},{\"end\":56937,\"start\":56930},{\"end\":56943,\"start\":56937},{\"end\":56950,\"start\":56943},{\"end\":56957,\"start\":56950},{\"end\":56963,\"start\":56957},{\"end\":56972,\"start\":56963},{\"end\":56980,\"start\":56972},{\"end\":56987,\"start\":56980},{\"end\":56991,\"start\":56987},{\"end\":57555,\"start\":57541},{\"end\":57565,\"start\":57555},{\"end\":57805,\"start\":57795},{\"end\":57815,\"start\":57805},{\"end\":57824,\"start\":57815},{\"end\":57835,\"start\":57824},{\"end\":58165,\"start\":58155},{\"end\":58177,\"start\":58165},{\"end\":58186,\"start\":58177},{\"end\":58200,\"start\":58186},{\"end\":58583,\"start\":58568},{\"end\":58591,\"start\":58583},{\"end\":58603,\"start\":58591},{\"end\":58614,\"start\":58603},{\"end\":58626,\"start\":58614},{\"end\":58636,\"start\":58626},{\"end\":58646,\"start\":58636},{\"end\":58654,\"start\":58646},{\"end\":58665,\"start\":58654},{\"end\":58679,\"start\":58665},{\"end\":59086,\"start\":59076},{\"end\":59092,\"start\":59086},{\"end\":59101,\"start\":59092},{\"end\":59112,\"start\":59101},{\"end\":59126,\"start\":59112},{\"end\":59137,\"start\":59126},{\"end\":59146,\"start\":59137},{\"end\":59157,\"start\":59146},{\"end\":59166,\"start\":59157},{\"end\":59173,\"start\":59166},{\"end\":59504,\"start\":59494},{\"end\":59516,\"start\":59504},{\"end\":59527,\"start\":59516},{\"end\":59539,\"start\":59527},{\"end\":59551,\"start\":59539},{\"end\":59564,\"start\":59551},{\"end\":59575,\"start\":59564},{\"end\":59589,\"start\":59575},{\"end\":59599,\"start\":59589},{\"end\":60074,\"start\":60063},{\"end\":60087,\"start\":60074},{\"end\":60098,\"start\":60087},{\"end\":60530,\"start\":60519},{\"end\":60541,\"start\":60530},{\"end\":60556,\"start\":60541},{\"end\":60572,\"start\":60556},{\"end\":60591,\"start\":60572},{\"end\":60604,\"start\":60591},{\"end\":61200,\"start\":61193},{\"end\":61208,\"start\":61200},{\"end\":61217,\"start\":61208},{\"end\":61225,\"start\":61217},{\"end\":61231,\"start\":61225},{\"end\":61240,\"start\":61231},{\"end\":61248,\"start\":61240},{\"end\":61254,\"start\":61248},{\"end\":61262,\"start\":61254},{\"end\":61273,\"start\":61262},{\"end\":61814,\"start\":61803},{\"end\":61820,\"start\":61814},{\"end\":61829,\"start\":61820},{\"end\":61837,\"start\":61829},{\"end\":61847,\"start\":61837},{\"end\":61860,\"start\":61847},{\"end\":62043,\"start\":62034},{\"end\":62055,\"start\":62043},{\"end\":62062,\"start\":62055},{\"end\":62074,\"start\":62062},{\"end\":62086,\"start\":62074},{\"end\":62094,\"start\":62086},{\"end\":62107,\"start\":62094},{\"end\":62120,\"start\":62107},{\"end\":62128,\"start\":62120},{\"end\":62137,\"start\":62128},{\"end\":62585,\"start\":62575},{\"end\":62596,\"start\":62585},{\"end\":62607,\"start\":62596},{\"end\":62614,\"start\":62607},{\"end\":62624,\"start\":62614},{\"end\":62634,\"start\":62624},{\"end\":62642,\"start\":62634},{\"end\":62648,\"start\":62642},{\"end\":62657,\"start\":62648},{\"end\":63058,\"start\":63047},{\"end\":63066,\"start\":63058},{\"end\":63077,\"start\":63066},{\"end\":63086,\"start\":63077},{\"end\":63097,\"start\":63086},{\"end\":63531,\"start\":63523},{\"end\":63541,\"start\":63531},{\"end\":63551,\"start\":63541},{\"end\":63561,\"start\":63551},{\"end\":63573,\"start\":63561},{\"end\":63585,\"start\":63573},{\"end\":63596,\"start\":63585},{\"end\":63608,\"start\":63596},{\"end\":63618,\"start\":63608},{\"end\":63626,\"start\":63618},{\"end\":63905,\"start\":63895},{\"end\":63912,\"start\":63905},{\"end\":63921,\"start\":63912},{\"end\":63932,\"start\":63921},{\"end\":63940,\"start\":63932},{\"end\":63951,\"start\":63940},{\"end\":63963,\"start\":63951},{\"end\":63977,\"start\":63963},{\"end\":63985,\"start\":63977},{\"end\":63994,\"start\":63985},{\"end\":64382,\"start\":64371},{\"end\":64391,\"start\":64382},{\"end\":64397,\"start\":64391},{\"end\":64410,\"start\":64397},{\"end\":64417,\"start\":64410},{\"end\":64428,\"start\":64417},{\"end\":64438,\"start\":64428},{\"end\":64449,\"start\":64438},{\"end\":64463,\"start\":64449},{\"end\":64472,\"start\":64463},{\"end\":64795,\"start\":64786},{\"end\":64806,\"start\":64795},{\"end\":64816,\"start\":64806},{\"end\":64829,\"start\":64816},{\"end\":64844,\"start\":64829},{\"end\":64854,\"start\":64844},{\"end\":64861,\"start\":64854},{\"end\":64875,\"start\":64861},{\"end\":64886,\"start\":64875},{\"end\":64901,\"start\":64886},{\"end\":65284,\"start\":65275},{\"end\":65297,\"start\":65284},{\"end\":65306,\"start\":65297},{\"end\":65316,\"start\":65306},{\"end\":65322,\"start\":65316},{\"end\":65334,\"start\":65322},{\"end\":65343,\"start\":65334},{\"end\":65358,\"start\":65343},{\"end\":65533,\"start\":65523},{\"end\":65543,\"start\":65533},{\"end\":65555,\"start\":65543},{\"end\":65566,\"start\":65555},{\"end\":65579,\"start\":65566},{\"end\":65590,\"start\":65579},{\"end\":65601,\"start\":65590},{\"end\":65611,\"start\":65601},{\"end\":65622,\"start\":65611},{\"end\":65996,\"start\":65985},{\"end\":66006,\"start\":65996},{\"end\":66017,\"start\":66006},{\"end\":66029,\"start\":66017},{\"end\":66043,\"start\":66029},{\"end\":66054,\"start\":66043},{\"end\":66065,\"start\":66054},{\"end\":66074,\"start\":66065},{\"end\":66084,\"start\":66074},{\"end\":66093,\"start\":66084},{\"end\":66953,\"start\":66945},{\"end\":66962,\"start\":66953},{\"end\":66970,\"start\":66962},{\"end\":66978,\"start\":66970},{\"end\":66987,\"start\":66978},{\"end\":66996,\"start\":66987},{\"end\":67002,\"start\":66996},{\"end\":67781,\"start\":67773},{\"end\":67788,\"start\":67781},{\"end\":67802,\"start\":67788},{\"end\":67810,\"start\":67802},{\"end\":67819,\"start\":67810},{\"end\":67829,\"start\":67819},{\"end\":67842,\"start\":67829},{\"end\":67850,\"start\":67842},{\"end\":68268,\"start\":68260},{\"end\":68278,\"start\":68268},{\"end\":68298,\"start\":68278},{\"end\":68307,\"start\":68298},{\"end\":68318,\"start\":68307},{\"end\":68326,\"start\":68318},{\"end\":68335,\"start\":68326},{\"end\":68353,\"start\":68335},{\"end\":68366,\"start\":68353},{\"end\":68374,\"start\":68366},{\"end\":68953,\"start\":68945},{\"end\":68963,\"start\":68953},{\"end\":68971,\"start\":68963},{\"end\":69201,\"start\":69194},{\"end\":69210,\"start\":69201},{\"end\":69220,\"start\":69210},{\"end\":69227,\"start\":69220},{\"end\":69235,\"start\":69227},{\"end\":69245,\"start\":69235},{\"end\":69251,\"start\":69245},{\"end\":69260,\"start\":69251},{\"end\":69268,\"start\":69260},{\"end\":69565,\"start\":69558},{\"end\":69573,\"start\":69565},{\"end\":69587,\"start\":69573},{\"end\":69596,\"start\":69587},{\"end\":69603,\"start\":69596},{\"end\":69610,\"start\":69603},{\"end\":69618,\"start\":69610},{\"end\":69626,\"start\":69618},{\"end\":69925,\"start\":69919},{\"end\":69934,\"start\":69925},{\"end\":69940,\"start\":69934},{\"end\":69955,\"start\":69940},{\"end\":69965,\"start\":69955},{\"end\":69977,\"start\":69965},{\"end\":69989,\"start\":69977},{\"end\":70002,\"start\":69989},{\"end\":70010,\"start\":70002},{\"end\":70417,\"start\":70411},{\"end\":70423,\"start\":70417},{\"end\":70431,\"start\":70423},{\"end\":70439,\"start\":70431},{\"end\":70445,\"start\":70439},{\"end\":70451,\"start\":70445},{\"end\":70458,\"start\":70451},{\"end\":70466,\"start\":70458},{\"end\":70825,\"start\":70817},{\"end\":70835,\"start\":70825},{\"end\":70845,\"start\":70835},{\"end\":71069,\"start\":71060},{\"end\":71077,\"start\":71069},{\"end\":71084,\"start\":71077},{\"end\":71094,\"start\":71084},{\"end\":71102,\"start\":71094},{\"end\":71117,\"start\":71102},{\"end\":71125,\"start\":71117},{\"end\":71480,\"start\":71471},{\"end\":71490,\"start\":71480},{\"end\":71497,\"start\":71490},{\"end\":71503,\"start\":71497},{\"end\":71511,\"start\":71503},{\"end\":71519,\"start\":71511},{\"end\":71737,\"start\":71728},{\"end\":71747,\"start\":71737},{\"end\":71756,\"start\":71747},{\"end\":71767,\"start\":71756},{\"end\":71775,\"start\":71767},{\"end\":71783,\"start\":71775},{\"end\":71792,\"start\":71783},{\"end\":71800,\"start\":71792},{\"end\":71806,\"start\":71800},{\"end\":71815,\"start\":71806},{\"end\":72179,\"start\":72170},{\"end\":72190,\"start\":72179},{\"end\":72196,\"start\":72190},{\"end\":72203,\"start\":72196},{\"end\":72212,\"start\":72203},{\"end\":72220,\"start\":72212},{\"end\":72618,\"start\":72609},{\"end\":72626,\"start\":72618},{\"end\":72632,\"start\":72626},{\"end\":72914,\"start\":72906},{\"end\":72922,\"start\":72914},{\"end\":72929,\"start\":72922},{\"end\":73344,\"start\":73334},{\"end\":73352,\"start\":73344},{\"end\":73358,\"start\":73352},{\"end\":73366,\"start\":73358},{\"end\":73374,\"start\":73366},{\"end\":73381,\"start\":73374},{\"end\":73388,\"start\":73381},{\"end\":73397,\"start\":73388},{\"end\":73406,\"start\":73397},{\"end\":73414,\"start\":73406},{\"end\":73751,\"start\":73743},{\"end\":73758,\"start\":73751},{\"end\":73767,\"start\":73758},{\"end\":73773,\"start\":73767},{\"end\":73780,\"start\":73773},{\"end\":73784,\"start\":73780},{\"end\":74141,\"start\":74134},{\"end\":74149,\"start\":74141},{\"end\":74155,\"start\":74149},{\"end\":74161,\"start\":74155},{\"end\":74167,\"start\":74161},{\"end\":74175,\"start\":74167}]", "bib_venue": "[{\"end\":48581,\"start\":48470},{\"end\":49218,\"start\":49058},{\"end\":49973,\"start\":49924},{\"end\":50500,\"start\":50429},{\"end\":51195,\"start\":51134},{\"end\":51563,\"start\":51490},{\"end\":52008,\"start\":51920},{\"end\":52521,\"start\":52379},{\"end\":53241,\"start\":53212},{\"end\":53922,\"start\":53849},{\"end\":54484,\"start\":54392},{\"end\":54958,\"start\":54865},{\"end\":55418,\"start\":55379},{\"end\":55797,\"start\":55783},{\"end\":56231,\"start\":56182},{\"end\":56619,\"start\":56578},{\"end\":57102,\"start\":56991},{\"end\":57539,\"start\":57497},{\"end\":57883,\"start\":57839},{\"end\":58278,\"start\":58200},{\"end\":58751,\"start\":58695},{\"end\":59222,\"start\":59173},{\"end\":59716,\"start\":59615},{\"end\":60167,\"start\":60098},{\"end\":60672,\"start\":60604},{\"end\":61363,\"start\":61273},{\"end\":61871,\"start\":61860},{\"end\":62227,\"start\":62153},{\"end\":62697,\"start\":62657},{\"end\":63158,\"start\":63097},{\"end\":63521,\"start\":63456},{\"end\":64073,\"start\":64010},{\"end\":64478,\"start\":64472},{\"end\":64784,\"start\":64681},{\"end\":65273,\"start\":65220},{\"end\":65683,\"start\":65638},{\"end\":66158,\"start\":66109},{\"end\":67164,\"start\":67002},{\"end\":67915,\"start\":67850},{\"end\":68460,\"start\":68374},{\"end\":68943,\"start\":68881},{\"end\":69192,\"start\":69144},{\"end\":69675,\"start\":69626},{\"end\":70074,\"start\":70026},{\"end\":70535,\"start\":70466},{\"end\":70912,\"start\":70861},{\"end\":71210,\"start\":71141},{\"end\":71553,\"start\":71519},{\"end\":71880,\"start\":71831},{\"end\":72286,\"start\":72220},{\"end\":72607,\"start\":72520},{\"end\":73021,\"start\":72929},{\"end\":73849,\"start\":73784},{\"end\":74269,\"start\":74175},{\"end\":48679,\"start\":48583},{\"end\":49381,\"start\":49220},{\"end\":50558,\"start\":50502},{\"end\":51243,\"start\":51197},{\"end\":52672,\"start\":52523},{\"end\":56637,\"start\":56621},{\"end\":57200,\"start\":57104},{\"end\":60742,\"start\":60674},{\"end\":61440,\"start\":61365},{\"end\":63206,\"start\":63160},{\"end\":66172,\"start\":66160},{\"end\":67313,\"start\":67166},{\"end\":68533,\"start\":68462},{\"end\":73100,\"start\":73023},{\"end\":74350,\"start\":74271}]"}}}, "year": 2023, "month": 12, "day": 17}