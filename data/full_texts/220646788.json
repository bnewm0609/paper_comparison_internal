{"id": 220646788, "updated": "2023-10-06 12:53:42.665", "metadata": {"title": "Interpretable Foreground Object Search As Knowledge Distillation", "authors": "[{\"first\":\"Boren\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Po-Yu\",\"last\":\"Zhuang\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Mingyang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Tan\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": 7, "day": 20}, "abstract": "This paper proposes a knowledge distillation method for foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale, FoS retrieves compatible foregrounds in a certain category for later image composition. Foregrounds within the same category can be grouped into a small number of patterns. Instances within each pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. We first present a pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We then establish a benchmark dataset for further training and testing following the pipeline. As for the proposed method, we first train a foreground encoder to learn representations of interchangeable foregrounds. We then train a query encoder to learn query-foreground compatibility following a knowledge distillation framework. It aims to transfer knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-the-art by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP). Extensive experimental results also demonstrate its efficacy from various aspects. The benchmark dataset and code will be release shortly.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.09867", "mag": "3106984607", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/LiZGLT20", "doi": "10.1007/978-3-030-58604-1_12"}}, "content": {"source": {"pdf_hash": "dd7dc3555e985cea80c71d1646ab3e87001b3c46", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.09867v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.09867", "status": "GREEN"}}, "grobid": {"id": "c39ae39d7d3e10de05f30ac25966a0ae48529836", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dd7dc3555e985cea80c71d1646ab3e87001b3c46.txt", "contents": "\nInterpretable Foreground Object Search As Knowledge Distillation\n\n\nBoren Li boren.lbr@alibaba-inc.com \nAlibaba Group\n\n\nPo-Yu Zhuang po-yu.zby@alibaba-inc.com \nAlibaba Group\n\n\nJian Gu \nAlibaba Group\n\n\nMingyang Li mingyangli009@gmail.com \nAlibaba Group\n\n\nPing Tan pingtan@sfu.ca \nAlibaba Group\n\n\nSimon Fraser University\n\n\nInterpretable Foreground Object Search As Knowledge Distillation\n\nThis paper proposes a knowledge distillation method for foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale, FoS retrieves compatible foregrounds in a certain category for later image composition. Foregrounds within the same category can be grouped into a small number of patterns. Instances within each pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. We first present a pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We then establish a benchmark dataset for further training and testing following the pipeline. As for the proposed method, we first train a foreground encoder to learn representations of interchangeable foregrounds. We then train a query encoder to learn query-foreground compatibility following a knowledge distillation framework. It aims to transfer knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-theart by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP). Extensive experimental results also demonstrate its efficacy from various aspects. The benchmark dataset and code will be release shortly.\n\nIntroduction\n\nForeground object search (FoS) retrieves compatible foregrounds in a certain category given a background and a rectangle as query input [25]. It is a core task in many image composition applications [21]. For object insertion in photo editing, users often find it challenging and time-consuming to acquire compatible foregrounds in a foreground pool. Object insertion can be used to fill a new foreground to a region comprising undesired objects in the background [26].\n\nIn a larger sense, for text-to-image synthesis with multiple objects, recent researches [8] [12] have shown insight to generate semantic layout at first. Then, one way to solve the follow-up task, layout to image, is multi-object retrieval and composition [1]. Directly retrieving multiple objects simultaneously suffers from combinatorial explosion that can be perfectly avoided by iteratively performing FoS with composition. Hence, FoS is also a significant underlying task. Two problems arise to solve FoS. The first problem is how to classify foreground instances and define what are similar foregrounds to be retrieved together. The second problem is that given a query input and a foreground instance, how to define and decide their compatibility. Most recent methods [25] [26] jointly learned foreground similarity and query-foreground compatibility without decoupling the two problems. It makes the results difficult to interpret.\n\nWe notice that foregrounds in a certain category can be grouped to a small number of patterns. Instances within the same pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. Then, the first question arises: how to define and label interchangeable foregrounds specifically?\n\nSuppose we have answered the first question well, manually labelling compatibility for many pairs of query-foreground data is still extremely challenging, if not impossible. Since definition of interchangeable foregrounds relates to compatibility, the second question is: can we transfer knowledge from labelled interchangeable foregrounds to supervise representation learning of compatibility?\n\nWe answer these two questions in this work. For the first question, we propose a pipeline to build pattern-level FoS dataset comprising labels of interchangeable foregrounds. We exemplify 'person' as the foreground category to explain how to label and establish a benchmark dataset for further training and testing. We then train a foreground encoder to classify these patterns in order to learn feature representations for interchangeable foregrounds.\n\nFor the second question, we train a query encoder to learn query-foreground compatibility. It learns to transform query inputs into query features such that the feature similarities between query and compatible foregrounds are closer than those between query and incompatible ones. We follow a knowledge distillation scheme to transfer interchangeable foregrounds labelling to supervise compatibility learning. More specifically, we freeze the trained foreground encoder as the teacher network to generate embeddings as 'soft targets' to train the query encoder in the student network. As a result, the query inputs are projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, as interchangeable foregrounds are grouped into patterns, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds.\n\nWe first show effectiveness of the foreground encoder to represent interchangeable foregrounds. We then demonstrate efficacy of the query encoder to represent query-foreground compatibility. The proposed method outperforms the previous state-of-the-art by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP).\n\nThe key contributions are summarized as follows:\n\n-We introduce a novel concept called interchangeable foregrounds. It allows interpretable and direct learning of foreground similarity specifically for FoS. In addition, it makes pattern-level search feasible to retrieve more controllable, reasonable and diverse foregrounds. -We propose a new pipeline to establish pattern-level FoS dataset containing labels of interchangeable foregrounds. We establish the first benchmarking dataset using this pipeline. This dataset will be released to the public. -We propose a novel knowledge distillation framework to solve FoS. It enables fully interpretable learning and outperforms the previous state-of-the-art by a significant margin.\n\n\nRelated Works\n\n\nForeground Object Search\n\nEarly efforts on FoS, such as Photo Clip Art [11] and Sketch2Photo [1], applied handcrafted features to search foregrounds according to matching criterion as camera orientation, lighting, resolution, local context and so on. Manually designing either these matching criterion or handcrafted features is challenging.\n\nWith the success of deep learning on image classification [17], deep features are involved to replace handcrafted ones. Tan et al. [19] employed local region retrieval using semantic features extracted from an off-the-shelf CNN model. The retrieved regions contain person segments which are further used for image composition. They assume the foregrounds have surrounding background context and therefore, not feasible when the foregrounds are just images with pure background. Zhu et al. [28] trained a discriminative network to decide the realism of a composite image. They couple the suitability of foreground selection, adjustment and composition into one realism score, making it difficult to interpret. Zhao et al. [25] first formally defined the FoS task and focused on the foreground selection problem alone. They applied end-to-end feature learning to adapt for different object categories. This work is the closest to ours and serves as the baseline method for comparison purpose. More recently, Zhao et al. [26] proposed an unconstrained FoS task that aims to retrieve universal compatible foreground without specifying its category. We only focus on the constrained FoS problem with known foreground category in this work.\n\n\nKnowledge Distillation\n\nKnowledge distillation is a general purpose technique that is widely applied for neural network compression [16]. The key idea is to use soft probabilities of a larger teacher network to supervise a smaller student network, in addition to the available class labels. The soft probabilities reveal more information than the class labels alone that can purportedly help the student network learn better.\n\nIn addition to neural network compression, prior works has found knowledge distillation to be useful for sequence modelling [9], domain adaptation [15], semisupervised learning [20] and so on. The closest work to ours is multi-modal learning [5]. They trained a CNN model for a depth map as a new modality by teaching the network to reproduce the mid-level semantic representations learned from a well-labelled RGB image with paired data. For our case, we learn queryforeground compatibility as a new modality by teaching the network to reproduce the mid-level foreground similarity representations learned from a well-labelled interchangeable foreground modality with paired data. Therefore, the proposed FoS method can be viewed as another knowledge distillation application.\n\n\nForeground Object Search Dataset\n\nIn this section, we describe the proposed pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We exemplify 'person' as the foreground category to explain how to label and establish a benchmark dataset for further training and testing. Building a benchmark dataset is necessary for two reasons. First, there is no publicly available dataset for FoS. We do not have access to the one established by the baseline method [25]. Second, the previous dataset is instance-level and not sufficient to validate our method. There exists publicly available datasets that contain instance segmentation masks, Fig. 2. The proposed pipeline to establish a pattern-level FoS dataset. The instancelevel dataset is first established to obtain compatible instance pairs transformed from instance segmentation annotations. Through grouping interchangeable foregrounds to patterns, pattern-level dataset is finally built. A compatible instance pair in the instance-level dataset can be augmented to many pairs in the pattern-level dataset.\n\n\nPipeline to Establish Pattern-level FoS Dataset\n\nsuch as MS-COCO [13], PASCAL VOC 2012 [13] and ADE20K [27]. We can decompose an image into a background scene, a foreground and a rectangle using a mask. Since they are all from the same image, they are naturally compatible.\n\nPrevious methods [25][26] leave the original foreground in the background scene when building the dataset. They do so because they mask out the foreground by a rectangle filled with image mean values during training with an early-fusion strategy. By contrast, we apply a free-form image inpainting algorithm [23] to fill the foreground region in the background scene when building the dataset. This is because the deep inpainting algorithm trained on millions of images can perform reasonably well on this task. On the other hand, the earlyfusion strategy by previous methods masks out too much background context, leaving the compatibility decision much more difficult. As for foreground samples in the dataset, we paste the foreground in the original image to the center location on a pure white square background.\n\nWith sufficient number of foregrounds in a certain category, the next goal is to group them into patterns of interchangeable foregrounds. Given many thousands of instances, this task is very challenging without supervision. Hence, we label foregrounds by attributes at first. We then group them into the same pattern if they have identical values in every attribute dimension. Finally, we establish a pattern-level dataset where much more compatible instance pairs can be extracted than its instance-level counterpart.\n\n\nInterchangeable Foregrounds Labelling\n\nWe show how to label interchangeable foregrounds by using 'person' as the foreground category. 'person' is adopted because it is one of the most frequent categories for image composition. Furthermore, it is a non-rigid object with numerous different states. It is sufficiently representative to address the issues for interchangeable foregrounds labelling. We do not consider style issues in this work since all the raw images are photographs.   3 illustrates the six attribute dimensions we defined to classify patterns of interchangeable foregrounds. For a particular foreground, orientation and truncation are two mandatory attribute dimensions to be assigned with the presented values. They are mandatory because they will largely determine most aspects of interchangeable foregrounds. The other four attribute dimensions are sport, motion, viewpoint and state. These dimensions can further distinguish various aspects of 'person'. Their values can be left as 'unspecified' when we cannot assign them with available values. Table 1 shows the number of available attribute values in each dimension. We adopt images with mask annotations in the MS-COCO [13] dataset as raw data. Before labelling attribute values for each sample, we first exclude inappropriate samples that are heavily occluded, small or incomplete, resulting in 10154 foregrounds. We label 5468 samples from them with these attribute values, leading to 699 different patterns after grouping. Thus, we obtain 5468 patternlevel query-foreground compatibility pairs in total. Furthermore, the remaining 4686 unannotated foregrounds can be labelled automatically by a trained foreground encoder presented in Section 4. It leads to more pairs of pattern-level data to train query-foreground compatibility. In a larger sense, applying our trained foreground encoder with an instance segmentation model such as Mask-RCNN [6], we can automate the whole pipeline using internet images to learn query-foreground compatibility.\n\n\nEvaluation Set and Metrics\n\nThe annotated foreground patterns follow a heavy-tailed distribution. Therefore, we only select those patterns with at least 20 interchangeable foregrounds for testing. This leads to 69 patterns in total. We randomly select 5 foreground instances from each of these patterns to obtain the foreground database at test time. These foregrounds can be also applied to evaluate the capability of the foreground encoder in classifying interchangeable foregrounds. We adopt top-1 and top-5 accuracies to evaluate the classifier with 699 classes altogether.\n\nSimultaneously, we obtain the same number of corresponding query inputs. We select 100 query samples and prefer those with more 'person' in the query background intentionally to make the dataset more challenging. We then manually label their compatibility to each foreground pattern in the test-time foreground database. This is because one query input may have multiple other compatible foreground patterns except the corresponding one. On average, for each query input, we label 22.35 and 6.07 compatible foreground instances and patterns, respectively. These pairs are employed to evaluate query-foreground compatibility. We adopt mAP to evaluate the overall performance of FoS. Fig. 4. The overall training scheme. The foreground encoder is first trained to classify patterns of interchangeable foregrounds. It then serves as the teacher network to generate foreground embeddings as soft targets to train the query encoder which encodes query-foreground compatibility. Fig. 4 presents the overall training scheme comprising two successive stages. The first stage trains the foreground encoder to classify patterns of interchangeable foregrounds in order to learn foreground feature representations. Feature similarities from the same pattern are closer than those from other patterns. Therefore, the learned features are fully interpretable.\n\n\nProposed Approach\n\n\nOverall Training Scheme\n\nThe second stage trains the query encoder to learn query-foreground compatibility. This encoder transforms query inputs into embeddings such that embedding distances between query and compatible foregrounds are closer. We aim to transfer the knowledge of interchangeable foregrounds labelling to supervise compatibility learning. Hence, during training, we freeze the foreground encoder trained from the first stage as the teacher network. It generates foreground embeddings as 'soft targets' to train the query encoder in the student network. As a result, the query inputs are projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Cosine distance is applied to measure embedding distances between query and foreground. The embeddings are l 2 normalized before computing cosine distance.\n\n\nForeground Encoder\n\nTraining for the foreground encoder follows a typical image classification pipeline. The deeply learned embeddings need to be not only separable but also discriminative. These embeddings require to be well-classified by k-nearest neighbour algorithms without necessarily depend on label prediction. Therefore, we adopt center loss [22] in addition to softmax loss to train more discriminative features. The center loss is used due to its proven success in the face recognition task that is very similar to ours. The loss function is given by\nL f = L f S + \u03bbL f C .(1)\nL f denotes the total loss for foreground classification. The superscript f denotes foreground later on. L f S is the conventional softmax loss. L f C is the center loss and \u03bb is the weight. L f C is given by\nL f C = 1 2 m i=1 x f i \u2212 c f yi 2 2 ,(2)\nwhere m is the batch size, x f i \u2208 R d denotes the i th embedding, and c f yi \u2208 R d is the embedding center of the y th i pattern. d is the feature dimension. As for the foreground encoder architecture, we adopt ResNet50 [7] with 2048 dimensional feature embedding as feature extractor. We initialize the weights that were pre-trained for the ILSVRC-2014 competition [17]. A fully connected layer is further appended to the feature extractor for pattern classification.\n\n\nQuery Encoder\n\nCompatibility is determined by three factors: the background context, the foreground context, and the foreground location and scale (i.e. layout). We do not consider style compatibility in this work, but our framework is fully adaptable to style encodings learned from [2]. We focus to retrieve compatible foregrounds in a certain category without considering the multi-class problem, since our work can be easily expanded using [25] to tackle this issue. It is still challenging to hand-design compatibility criterion, even considering only the three factors.  5. The training scheme for the query encoder as knowledge distillation. The query encoder is trained to generate query embeddings using a triplet network. This network aims to distill compatibility information from the foreground encoder trained to represent patterns of interchangeable foregrounds. Fig. 5 demonstrates the training scheme for the query encoder as knowledge distillation. This encoder transforms query inputs into embeddings such that embedding distances between query and compatible foregrounds are closer. The general architecture follows a typical two-stream network. The bottom stream takes the square foreground image with pure white background as input. It encodes the image to feature embedding using the foreground encoder trained in the first stage. We freeze the weights in the foreground encoder during training for the query encoder.\n\n\nNetwork Architecture\n\nThe top stream takes a background scene and a rectangle specifying the desired foreground location and scale as query input. The background scene is first cropped to a square image, where the desired foreground location is placed as close to the image center as possible. This cropping also preserves as much context as possible for the square-background. Such cropping makes the background image more consistent so that the training is more stable. The squarebackground is encoded by a ResNet50 [7] backbone pre-trained on ImageNet [17] with 2048-dimensional features. This network serves as the background encoder to represent scene context. Since the pre-trained network can represent semantic context well, we freeze its weights during training for the query encoder.\n\nThe query rectangle is just a bounding box with four degrees of freedom (DoF). We adopt the centroid representation for the bounding box. The first two DoF are coordinates of the bounding box centroid. The other two DoF are width and height of the bounding box. These coordinates are then normalized by dividing the image side length. We only keep the first two digits after the decimal point for better generalization of the bounding box encoding. This encoding is referred to as layout embedding.\n\nUnlike previous methods [25][26] by filling the query rectangle with image mean values to the background scene as a unified query input, our method encodes the two query factors separately to make the embeddings more interpretable. In addition, previous methods may fail when the query rectangle is too big relative to the background scene because too few background context can be preserved after the early-fusion to a unified query input. By contrast, we can avoid this issue completely since we encode the full square-background context. This is feasible because the foreground object has already been removed from the background scene when we establish the FoS dataset.\n\nThe layout and background embeddings are late-fused using bilinear fusion [14]. Here, the two embeddings are fused using their outer product followed by flattening to a vector. The outer product is adopted since it can model pairwise feature interactions well. Because the layout embedding is only 4-dimensional, we have not applied compact bilinear pooling techniques [4][3] [24] to reduce the dimension of the fused feature. This feature is then transformed by two fully connected (FC) layers with ReLU activation to obtain the query embedding. The output dimensions for the first and second FC are all 2048.\n\n\nLoss Function\n\nWe construct triplets consisting of a query input as anchor, a compatible foreground as positive, and an incompatible foreground as negative to train the network. We adopt triplet loss [18] and enforce the embedding distance between anchor and positive to be closer than the one between anchor and negative. These embeddings are l 2 normalized before measuring distance using cosine function.\n\nFormally, a fused feature after bilinear fusion is given by u q \u2208 R e , where the superscript q denotes query later on and e is the dimension of the feature embedding. Denote the foreground embeddings for the positive and negative samples are x f p and x f n , respectively. The operation of two FC layers with ReLU is denoted as F. The triplet loss is then given by\nL q = max 0, F (u q ) T x f p F (u q ) x f p \u2212 F (u q ) T x f n F (u q ) x f n + M ,(3)\nwhere M is a positive margin. The objective is to train F by minimizing L q over all the sampled triplets.\n\n\nTraining Data\n\nThe pattern-level FoS dataset is used for training. The dataset contains pairs of query and compatible pattern containing interchangeable foreground instances. A query with these instances form positive pairs, whereas the query with the others are all negative ones. With pattern-level FoS dataset, we can largely alleviate the severe imbalance in the number of training samples, coupled with noise in the negative pair sampling where some compatible foregrounds are mistreated as negative ones.\n\nWe apply different data augmentation strategies for the three types of input. To augment the query rectangle, we relax its size and scale constraints by randomly resizing the rectangle with maximum possible space being half of the rectangle's width and height. To augment the query background, we add random zoom on the cropped square-background while keep the whole query rectangle within the field of view. This augmentation strategy cannot be applied by previous methods [25][26] since it will result in fewer background context in the early-fused query input. As for foreground augmentation, we adopt the same strategy when training the foreground encoder.\n\n\nPattern-level Foreground Object Search\n\nWith the novel concept of interchangeable foregrounds, we can apply patternlevel FoS instead of instance-level. For each foreground instance in the query database, we can assign a pattern label on it. Having all foreground instances within a pattern, the pattern embedding is computed using the centroid of all the instance embeddings transformed by the trained foreground encoder. These pattern embeddings can be also indexed for retrieval. Pattern-level FoS can easily stratify the results, making it more feasible to retrieve controllable, reasonable and diverse foreground instances.\n\n\nImplementation Details\n\nTo train the foreground encoder, we use the SGD optimizer with momentum and weight decay set to 0.9 and 0.0001, respectively. The learning rate for the softmax loss is 0.02 and the learning rate decay is 0.5 for every 10 epochs. The center loss weight, \u03bb, is set to 0.005. The learning rate for the center loss is 0.5. Batch size is 32 during training. For offline augmentation, we add random padding to the foreground and fill in the padded region with white color. Each foreground is augmented to 20 samples. We then pad them to square images with pure white background. For online augmentation, we apply color jitter by randomly changing the brightness, saturation, contrast and hue by 0.4, 0.4, 0.4 and 0.2, respectively. These samples are resized to 256 \u00d7 256 before fed into the foreground encoder.\n\nTo train the query encoder, we use the Adam optimizer [10] with \u03b2 1 = 0.5, \u03b2 2 = 0.99 and = 10 \u22129 . The learning rate is 10 \u22124 for the triplet loss. Batch size is 16 during training. The margin, M , is set to 0.1. The input size of the background encoder is 256\u00d7256. We perform offline augmentations as described. Each queryforeground pair is augmented to 20 samples. For online augmentation, we apply color jitter by randomly changing the brightness, saturation, contrast and hue by 0.4, 0.4, 0.4 and 0.2, respectively. \n\n\nExperiments\n\n\nForeground Encoder\n\nWe train foreground encoder in the first stage to classify patterns of interchangeable foregrounds. We use a foreground as query and search for its top-5 most similar foregrounds in a large database comprising 10154 samples. We first encode all the samples into embeddings using our trained foreground encoder. These embeddings are further l 2 normalized for query using cosine distance. We apply brute-force k-nearest neighbour matching to obtain the retrieval results. We compare results with the baseline method [25] and the pre-trained ResNet50 model on ImageNet as shown by Fig. 6. Clearly, similar instances retrieved by our method are much more interpretable. We can also apply pattern-level search to create interpretable and controllable diversity.\n\nTo further quantify the performance of foreground encoder as a pattern classifier, we test it on our evaluation set. The top-1 and top-5 accuracies are respectively 53.15% and 85.79% with 699 classes. The accuracy can be further improved with more labelled data, while the trained foreground encoder is sufficient to achieve much better performance over the baseline method in supervising query-foreground compatibility later.\n\n\nQuery Encoder\n\nWe compare our results with the baseline method [25]. We remove the MCB module in the baseline method since we only focus on FoS with one foreground category. Since their implementation is not publicly available, we implement it by strictly following all the settings in their paper. We train both methods on the newly established FoS dataset. We prepare 2 million triplets for each method and train for 2 epochs until convergence.\n\nWe first compare results from the two methods qualitatively in Fig. 7. Each row represents one query. The leftmost image shows the query input. Results from pattern-and instance-level search using our method are given in the red Fig. 7. Retrieval results comparison with baseline method [25]. Each row represents one query. The leftmost image demonstrates the query input. The red box shows top-3 patterns from pattern-level search, each with top-2 instances shown in a column, using the proposed method. The top-5 instance-level search results by our method and the baseline method are shown in the green and blue boxes, respectively. and green boxes, respectively. The instance-level search results from the baseline method are shown in the blue box. As can be seen, pattern-level search can provide reasonable and diverse results in a more controllable fashion than instance-level search. As for instance-level search, our results are much more reasonable and interpretable as seen from the first to third row. When the query rectangle is big relative to the background image, the baseline method cannot work properly due to its early-fusion strategy in the query stream. The third row illustrates such a case where a skateboard appears in the background image but most parts of the skateboard are within the query rectangle. The baseline method masks out this crucial cue with early-fusion, resulting in the fatal errors. Our method uses late-fusion without losing any information from the query inputs and therefore, it easily captures the important cue within the query rectangle. Results in the forth and fifth row demonstrate a limitation of both the proposed and baseline method. This limitation originates from the preprocessing step that square-crops the background image. Take the case in the fifth row for example. After square-cropping the query background, the woman playing tennis on the opposite side to the query rectangle is completely cropped, resulting in the final confusion of the retrieval results.\n\nQuantitatively, we test both methods on our evaluation set. The mAP is 43.30% using the baseline method whereas ours is 53.72%. It outperforms the baseline by 10.42% in absolute difference and 24.06% in relative improvement. Table 2 shows results in mAP of five ablation variants. The value in blue shows their respective absolute changes relative to the baseline Table 2. Ablation study results with mAP in percentage. 'Baseline' denotes baseline method [25]. 'Early-fusion' denotes training using early-fused query inputs. 'No-aug' denotes late fusion without random zoom augmentation. 'No-bg-freeze' denotes training without freezing background context encoder. 'Multi-task' denotes training using a multi-task loss to jointly train the foreground and query encoder. method. We first investigate the significance to apply interchangeable foregrounds. We employ early fusion strategy in the query stream similar to the baseline method, while we keep our pre-training for interchangeable foregrounds. With the newly introduced interchangeable foregrounds pre-training, the mAP is enhanced by 5.68%, contributing to 54.51% for the overall improvement. In the second variant, we apply our late fusion strategy in the query stream without random zoom augmentation. It further improves the mAP by 2.93%, contributing to 28.12% for the overall improvement. In the third experiment, we add random zoom augmentation. The baseline method [25] cannot perform this augmentation since in many cases, the zoomed background with masked query rectangle lacks background context. In this experiment, we do not freeze the background encoder. With this augmentation, the mAP is further enhanced by 1.7%, contributing to 16.31% for the overall improvement. In the fourth experiment, we freeze the background encoder and just train the two FC with ReLU layers. Results have shown that training for the background encoder simultaneously cannot help determining compatibility. It implies that the pre-trained model is sufficient to encode semantic context well for the background. In the final ablation experiment, we further fine-tune the foreground and query encoder with a multi-task loss without freezing the foreground encoder. It gives a gain of 0.76%. However, the gain will be less as we enlarge the interchangeable foreground dataset. By contrast, our knowledge distillation framework can modularize FoS into two sub-tasks whose dataset can be prepared separately.\n\n\nAblation Study\n\n\nConclusions\n\nThis paper introduces a novel concept called interchangeable foregrounds for FoS. It enables interpretable and direct learning of foreground similarity. It also makes pattern-level search feasible to retrieve controllable, reasonable and diverse foregrounds. A new pipeline is proposed to build pattern-level FoS dataset with labelled interchangeable foregrounds. The first FoS benchmark dataset is established accordingly. A novel knowledge distillation framework is proposed to solve the FoS task. It provides fully interpretable results and enhances the absolute mAP by 10.42% and relative mAP by 24.06% over the previous stateof-the-art. It implies the knowledge from interchangeable foregrounds can be transferred to supervise compatibility learning for better performance.\n\nFig. 1 .\n1Foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale as query input, FoS is to find compatible foregrounds within a certain category. (a) illustrates the query input. (b) exemplifies patterns in the foreground pool. (c) demonstrates search results by the proposed method.\n\nFig. 2\n2demonstrates the general pipeline to establish pattern-level FoS dataset.\n\nFig. 3 .\n3An illustration of attributes for the 'person' foreground. It contains six attribute dimensions: orientation, truncation, sport, motion, viewpoint and state. For a particular foreground, orientation and truncation are mandatory dimensions to be assigned with the presented values while the others are optional.\n\nFig.\nFig. 3 illustrates the six attribute dimensions we defined to classify patterns of interchangeable foregrounds. For a particular foreground, orientation and truncation are two mandatory attribute dimensions to be assigned with the presented values. They are mandatory because they will largely determine most aspects of interchangeable foregrounds. The other four attribute dimensions are sport, motion, viewpoint and state. These dimensions can further distinguish various aspects of 'person'. Their values can be left as 'unspecified' when we cannot assign them with available values. Table 1 shows the number of available attribute values in each dimension.\n\nFig.\nFig. 5. The training scheme for the query encoder as knowledge distillation. The query encoder is trained to generate query embeddings using a triplet network. This network aims to distill compatibility information from the foreground encoder trained to represent patterns of interchangeable foregrounds.\n\nFig. 6 .\n6Retrieval results comparison on similar foregrounds. The yellow box denotes the query foreground. The top-5 most similar foregrounds retrieved by the proposed method, the baseline method[25] and the pre-trained ResNet50 model are shown in the green, blue and orange boxes, respectively.\n\nTable 1 .\n1Number of available attribute values in each dimensionOrientation Truncation Sport Motion Viewpoint State \n8 \n6 \n12 \n31 \n4 \n3 \n\n\n\n\nBaseline Early-fusion No-aug No-bg-freeze Multi-task Ours 43.30 48.98 5.68\u2191 51.91 8.61\u2191 53.61 10.31\u2191 54.48 11.18\u2191 53.72 10.42\u2191\n\nSketch2photo: internet image montage. T Chen, M M Cheng, P Tan, A Shamir, S M Hu, ACM Trans. on Graphics. Chen, T., Cheng, M.M., Tan, P., Shamir, A., Hu, S.M.: Sketch2photo: internet image montage. ACM Trans. on Graphics (2009)\n\nSketching with style: visual search with sketches and aesthetic context. J Collomosse, T Bui, M Wilber, C Fang, H Jin, Collomosse, J., Bui, T., Wilber, M., Fang, C., Jin, H.: Sketching with style: visual search with sketches and aesthetic context. In: ICCV (2017)\n\nMultimodal compact bilinear pooling for visual question answering and visual grounding. A Fukui, D H Park, D Yang, A Rohrbach, T Darrell, M Rohrbach, EMNLPFukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.: Multi- modal compact bilinear pooling for visual question answering and visual grounding. In: EMNLP (2016)\n\nCompact bilinear pooling. Y Gao, O Beijbom, N Zhang, T Darrell, CVPRGao, Y., Beijbom, O., Zhang, N., Darrell, T.: Compact bilinear pooling. In: CVPR (2016)\n\nCross modal distillation for supervision transfer. S Gupta, J Hoffman, J Malik, CVPRGupta, S., Hoffman, J., Malik, J.: Cross modal distillation for supervision transfer. In: CVPR (2016)\n\nMask r-cnn. K He, G Gkioxari, P Dollr, R Girshick, He, K., Gkioxari, G., Dollr, P., Girshick, R.: Mask r-cnn. In: ICCV (2017)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPRHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n\nImage generation from scene graphs. J Johnson, A Gupta, L Fei-Fei, CVPR. Johnson, J., Gupta, A., Fei-Fei, L.: Image generation from scene graphs. In: CVPR. pp. 1219-1228 (Jun 2018)\n\nSequence-level knowledge distillation. Y Kim, A M Rush, EMNLPKim, Y., Rush, A.M.: Sequence-level knowledge distillation. In: EMNLP (2016)\n\nAdam: a method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. CoRR arXiv:1412.6980 (2014)\n\nPhoto clip art. J F Lalonde, D Hoiem, A A Efros, C Rother, J Winn, A Criminisi, ACM Trans. on Graphics. Lalonde, J.F., Hoiem, D., Efros, A.A., Rother, C., Winn, J., Criminisi, A.: Photo clip art. ACM Trans. on Graphics (TOG) (2007)\n\nSeq-sg2sl: inferring semantic layout from scene graph through sequence to sequence learning. B Li, B Zhuang, M Li, J Gu, ICCVLi, B., Zhuang, B., Li, M., Gu, J.: Seq-sg2sl: inferring semantic layout from scene graph through sequence to sequence learning. In: ICCV (2019)\n\nT Lin, M Maire, S J Belongie, L D Bourdev, R B Girshick, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Microsoft COCO: common objects in context. ECCVLin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV (2014)\n\nBilinear cnn models for fine-grained visual recognition. T Y Lin, A Roychowdhury, S Maji, ICCVLin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recognition. In: ICCV (2015)\n\nAdversarial teacher-student learning for unsupervised domain adaptation. Z Meng, J Li, Y Gong, B H Juang, ICASSPMeng, Z., Li, J., Gong, Y., Juang, B.H.: Adversarial teacher-student learning for unsupervised domain adaptation. In: ICASSP (2018)\n\nApprentice: using knowledge distillation techniques to improve low-precision network accuracy. A Mishra, D Marr, ICLRMishra, A., Marr, D.: Apprentice: using knowledge distillation techniques to im- prove low-precision network accuracy. In: ICLR (2018)\n\n. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, Imagenet large scale visual recognition challenge. IJCVRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large scale visual recognition challenge. IJCV (2015)\n\nFacenet: a unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, CVPRSchroff, F., Kalenichenko, D., Philbin, J.: Facenet: a unified embedding for face recognition and clustering. In: CVPR (2015)\n\nWhere and who? automatic semantic-aware person composition. F Tan, C Bernier, B Cohen, V Ordonez, C Barnes, WACVTan, F., Bernier, C., Cohen, B., Ordonez, V., Barnes, C.: Where and who? auto- matic semantic-aware person composition. In: WACV (2017)\n\nMean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. A Tarvainen, H Valpola, Tarvainen, A., Valpola, H.: Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results. In: NIPS (2017)\n\nDeep image harmonization. Y Tsai, X Shen, Z Lin, K Sunkavalli, X Lu, M Yang, CVPRTsai, Y., Shen, X., Lin, Z., Sunkavalli, K., Lu, X., Yang, M.: Deep image harmo- nization. In: CVPR (2017)\n\nA discriminative feature learning approach for deep face recognition. Y Wen, K Zhang, Z Li, Y Qiao, ECCVWen, Y., Zhang, K., Li, Z., Qiao, Y.: A discriminative feature learning approach for deep face recognition. In: ECCV (2016)\n\nFree-form image inpainting with gated convolution. J Yu, Z Lin, J Yang, X Shen, X Lu, T S Huang, ICCVYu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Free-form image inpainting with gated convolution. In: ICCV (2019)\n\nMulti-modal factorized bilinear pooling with coattentionlearning for visual question answering. Z Yu, J Yu, J Fan, D Tao, Yu, Z., Yu, J., Fan, J., Tao, D.: Multi-modal factorized bilinear pooling with co- attentionlearning for visual question answering. In: ICCV (2017)\n\nCompositing-aware image search. H Zhao, X Shen, Z Lin, K Sunkavalli, B Price, J Jia, ECCVZhao, H., Shen, X., Lin, Z., Sunkavalli, K., Price, B., Jia, J.: Compositing-aware image search. In: ECCV (2018)\n\nUnconstrained foreground object search. Y Zhao, B Price, S Cohen, D Gurari, ICCVZhao, Y., Price, B., Cohen, S., Gurari, D.: Unconstrained foreground object search. In: ICCV (2019)\n\nB Zhou, H Zhao, X Puig, S Fidler, A Barriuso, A Torralba, Scene parsing through ade20k dataset. CVPRZhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: CVPR (2017)\n\nLearning a discriminative model for the perception of realism in composite images. J Y Zhu, P Krahenbuhl, E Shechtman, A A Efros, ICCVZhu, J.Y., Krahenbuhl, P., Shechtman, E., Efros, A.A.: Learning a discriminative model for the perception of realism in composite images. In: ICCV (2015)\n", "annotations": {"author": "[{\"end\":119,\"start\":68},{\"end\":175,\"start\":120},{\"end\":200,\"start\":176},{\"end\":253,\"start\":201},{\"end\":320,\"start\":254}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":74},{\"end\":132,\"start\":126},{\"end\":183,\"start\":181},{\"end\":212,\"start\":210},{\"end\":262,\"start\":259}]", "author_first_name": "[{\"end\":73,\"start\":68},{\"end\":125,\"start\":120},{\"end\":180,\"start\":176},{\"end\":209,\"start\":201},{\"end\":258,\"start\":254}]", "author_affiliation": "[{\"end\":118,\"start\":104},{\"end\":174,\"start\":160},{\"end\":199,\"start\":185},{\"end\":252,\"start\":238},{\"end\":293,\"start\":279},{\"end\":319,\"start\":295}]", "title": "[{\"end\":65,\"start\":1},{\"end\":385,\"start\":321}]", "venue": null, "abstract": "[{\"end\":2030,\"start\":387}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2186,\"start\":2182},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2249,\"start\":2245},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2514,\"start\":2510},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2608,\"start\":2605},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2613,\"start\":2609},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2776,\"start\":2773},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3296,\"start\":3292},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3301,\"start\":3297},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6766,\"start\":6762},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6787,\"start\":6784},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7096,\"start\":7092},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7169,\"start\":7165},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7527,\"start\":7523},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7759,\"start\":7755},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8056,\"start\":8052},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8407,\"start\":8403},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8825,\"start\":8822},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8849,\"start\":8845},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8879,\"start\":8875},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8943,\"start\":8940},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9975,\"start\":9971},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10644,\"start\":10640},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10666,\"start\":10662},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10682,\"start\":10678},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10871,\"start\":10867},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11162,\"start\":11158},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13387,\"start\":13383},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14115,\"start\":14112},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17412,\"start\":17408},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18120,\"start\":18117},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18267,\"start\":18263},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18655,\"start\":18652},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18816,\"start\":18812},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20331,\"start\":20328},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20369,\"start\":20365},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21133,\"start\":21129},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21858,\"start\":21854},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22152,\"start\":22149},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22160,\"start\":22156},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22597,\"start\":22593},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24356,\"start\":24352},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26059,\"start\":26055},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27078,\"start\":27074},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27814,\"start\":27810},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28486,\"start\":28482},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30678,\"start\":30674},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31654,\"start\":31650},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35410,\"start\":35406}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33825,\"start\":33484},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33908,\"start\":33826},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34230,\"start\":33909},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34897,\"start\":34231},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35208,\"start\":34898},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35506,\"start\":35209},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35647,\"start\":35507},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35776,\"start\":35648}]", "paragraph": "[{\"end\":2515,\"start\":2046},{\"end\":3456,\"start\":2517},{\"end\":3802,\"start\":3458},{\"end\":4198,\"start\":3804},{\"end\":4652,\"start\":4200},{\"end\":5576,\"start\":4654},{\"end\":5941,\"start\":5578},{\"end\":5991,\"start\":5943},{\"end\":6672,\"start\":5993},{\"end\":7032,\"start\":6717},{\"end\":8268,\"start\":7034},{\"end\":8696,\"start\":8295},{\"end\":9475,\"start\":8698},{\"end\":10572,\"start\":9512},{\"end\":10848,\"start\":10624},{\"end\":11666,\"start\":10850},{\"end\":12186,\"start\":11668},{\"end\":14214,\"start\":12228},{\"end\":14794,\"start\":14245},{\"end\":16141,\"start\":14796},{\"end\":17054,\"start\":16189},{\"end\":17618,\"start\":17077},{\"end\":17853,\"start\":17645},{\"end\":18365,\"start\":17896},{\"end\":19807,\"start\":18383},{\"end\":20603,\"start\":19832},{\"end\":21103,\"start\":20605},{\"end\":21778,\"start\":21105},{\"end\":22390,\"start\":21780},{\"end\":22800,\"start\":22408},{\"end\":23168,\"start\":22802},{\"end\":23363,\"start\":23257},{\"end\":23876,\"start\":23381},{\"end\":24538,\"start\":23878},{\"end\":25168,\"start\":24581},{\"end\":25999,\"start\":25195},{\"end\":26522,\"start\":26001},{\"end\":27316,\"start\":26559},{\"end\":27744,\"start\":27318},{\"end\":28193,\"start\":27762},{\"end\":30217,\"start\":28195},{\"end\":32672,\"start\":30219},{\"end\":33483,\"start\":32705}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17644,\"start\":17619},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17895,\"start\":17854},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23256,\"start\":23169}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13263,\"start\":13256},{\"end\":30451,\"start\":30444},{\"end\":30590,\"start\":30583}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2044,\"start\":2032},{\"attributes\":{\"n\":\"2\"},\"end\":6688,\"start\":6675},{\"attributes\":{\"n\":\"2.1\"},\"end\":6715,\"start\":6691},{\"attributes\":{\"n\":\"2.2\"},\"end\":8293,\"start\":8271},{\"attributes\":{\"n\":\"3\"},\"end\":9510,\"start\":9478},{\"attributes\":{\"n\":\"3.1\"},\"end\":10622,\"start\":10575},{\"attributes\":{\"n\":\"3.2\"},\"end\":12226,\"start\":12189},{\"attributes\":{\"n\":\"3.3\"},\"end\":14243,\"start\":14217},{\"attributes\":{\"n\":\"4\"},\"end\":16161,\"start\":16144},{\"attributes\":{\"n\":\"4.1\"},\"end\":16187,\"start\":16164},{\"attributes\":{\"n\":\"4.2\"},\"end\":17075,\"start\":17057},{\"attributes\":{\"n\":\"4.3\"},\"end\":18381,\"start\":18368},{\"end\":19830,\"start\":19810},{\"end\":22406,\"start\":22393},{\"end\":23379,\"start\":23366},{\"attributes\":{\"n\":\"4.4\"},\"end\":24579,\"start\":24541},{\"attributes\":{\"n\":\"4.5\"},\"end\":25193,\"start\":25171},{\"attributes\":{\"n\":\"5\"},\"end\":26536,\"start\":26525},{\"attributes\":{\"n\":\"5.1\"},\"end\":26557,\"start\":26539},{\"attributes\":{\"n\":\"5.2\"},\"end\":27760,\"start\":27747},{\"end\":32689,\"start\":32675},{\"attributes\":{\"n\":\"6\"},\"end\":32703,\"start\":32692},{\"end\":33493,\"start\":33485},{\"end\":33833,\"start\":33827},{\"end\":33918,\"start\":33910},{\"end\":34236,\"start\":34232},{\"end\":34903,\"start\":34899},{\"end\":35218,\"start\":35210},{\"end\":35517,\"start\":35508}]", "table": "[{\"end\":35647,\"start\":35573}]", "figure_caption": "[{\"end\":33825,\"start\":33495},{\"end\":33908,\"start\":33835},{\"end\":34230,\"start\":33920},{\"end\":34897,\"start\":34237},{\"end\":35208,\"start\":34904},{\"end\":35506,\"start\":35220},{\"end\":35573,\"start\":35519},{\"end\":35776,\"start\":35650}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10156,\"start\":10150},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12675,\"start\":12674},{\"end\":15484,\"start\":15478},{\"end\":15775,\"start\":15769},{\"end\":18946,\"start\":18945},{\"end\":19251,\"start\":19245},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27144,\"start\":27138},{\"end\":28264,\"start\":28258},{\"end\":28430,\"start\":28424}]", "bib_author_first_name": "[{\"end\":35817,\"start\":35816},{\"end\":35825,\"start\":35824},{\"end\":35827,\"start\":35826},{\"end\":35836,\"start\":35835},{\"end\":35843,\"start\":35842},{\"end\":35853,\"start\":35852},{\"end\":35855,\"start\":35854},{\"end\":36081,\"start\":36080},{\"end\":36095,\"start\":36094},{\"end\":36102,\"start\":36101},{\"end\":36112,\"start\":36111},{\"end\":36120,\"start\":36119},{\"end\":36361,\"start\":36360},{\"end\":36370,\"start\":36369},{\"end\":36372,\"start\":36371},{\"end\":36380,\"start\":36379},{\"end\":36388,\"start\":36387},{\"end\":36400,\"start\":36399},{\"end\":36411,\"start\":36410},{\"end\":36636,\"start\":36635},{\"end\":36643,\"start\":36642},{\"end\":36654,\"start\":36653},{\"end\":36663,\"start\":36662},{\"end\":36818,\"start\":36817},{\"end\":36827,\"start\":36826},{\"end\":36838,\"start\":36837},{\"end\":36966,\"start\":36965},{\"end\":36972,\"start\":36971},{\"end\":36984,\"start\":36983},{\"end\":36993,\"start\":36992},{\"end\":37127,\"start\":37126},{\"end\":37133,\"start\":37132},{\"end\":37142,\"start\":37141},{\"end\":37149,\"start\":37148},{\"end\":37296,\"start\":37295},{\"end\":37307,\"start\":37306},{\"end\":37316,\"start\":37315},{\"end\":37481,\"start\":37480},{\"end\":37488,\"start\":37487},{\"end\":37490,\"start\":37489},{\"end\":37625,\"start\":37624},{\"end\":37627,\"start\":37626},{\"end\":37637,\"start\":37636},{\"end\":37769,\"start\":37768},{\"end\":37771,\"start\":37770},{\"end\":37782,\"start\":37781},{\"end\":37791,\"start\":37790},{\"end\":37793,\"start\":37792},{\"end\":37802,\"start\":37801},{\"end\":37812,\"start\":37811},{\"end\":37820,\"start\":37819},{\"end\":38079,\"start\":38078},{\"end\":38085,\"start\":38084},{\"end\":38095,\"start\":38094},{\"end\":38101,\"start\":38100},{\"end\":38257,\"start\":38256},{\"end\":38264,\"start\":38263},{\"end\":38273,\"start\":38272},{\"end\":38275,\"start\":38274},{\"end\":38287,\"start\":38286},{\"end\":38289,\"start\":38288},{\"end\":38300,\"start\":38299},{\"end\":38302,\"start\":38301},{\"end\":38314,\"start\":38313},{\"end\":38322,\"start\":38321},{\"end\":38332,\"start\":38331},{\"end\":38343,\"start\":38342},{\"end\":38353,\"start\":38352},{\"end\":38355,\"start\":38354},{\"end\":38659,\"start\":38658},{\"end\":38661,\"start\":38660},{\"end\":38668,\"start\":38667},{\"end\":38684,\"start\":38683},{\"end\":38882,\"start\":38881},{\"end\":38890,\"start\":38889},{\"end\":38896,\"start\":38895},{\"end\":38904,\"start\":38903},{\"end\":38906,\"start\":38905},{\"end\":39149,\"start\":39148},{\"end\":39159,\"start\":39158},{\"end\":39309,\"start\":39308},{\"end\":39324,\"start\":39323},{\"end\":39332,\"start\":39331},{\"end\":39338,\"start\":39337},{\"end\":39348,\"start\":39347},{\"end\":39360,\"start\":39359},{\"end\":39366,\"start\":39365},{\"end\":39375,\"start\":39374},{\"end\":39387,\"start\":39386},{\"end\":39397,\"start\":39396},{\"end\":39410,\"start\":39409},{\"end\":39412,\"start\":39411},{\"end\":39420,\"start\":39419},{\"end\":39762,\"start\":39761},{\"end\":39773,\"start\":39772},{\"end\":39789,\"start\":39788},{\"end\":39991,\"start\":39990},{\"end\":39998,\"start\":39997},{\"end\":40009,\"start\":40008},{\"end\":40018,\"start\":40017},{\"end\":40029,\"start\":40028},{\"end\":40301,\"start\":40300},{\"end\":40314,\"start\":40313},{\"end\":40517,\"start\":40516},{\"end\":40525,\"start\":40524},{\"end\":40533,\"start\":40532},{\"end\":40540,\"start\":40539},{\"end\":40554,\"start\":40553},{\"end\":40560,\"start\":40559},{\"end\":40750,\"start\":40749},{\"end\":40757,\"start\":40756},{\"end\":40766,\"start\":40765},{\"end\":40772,\"start\":40771},{\"end\":40960,\"start\":40959},{\"end\":40966,\"start\":40965},{\"end\":40973,\"start\":40972},{\"end\":40981,\"start\":40980},{\"end\":40989,\"start\":40988},{\"end\":40995,\"start\":40994},{\"end\":40997,\"start\":40996},{\"end\":41232,\"start\":41231},{\"end\":41238,\"start\":41237},{\"end\":41244,\"start\":41243},{\"end\":41251,\"start\":41250},{\"end\":41439,\"start\":41438},{\"end\":41447,\"start\":41446},{\"end\":41455,\"start\":41454},{\"end\":41462,\"start\":41461},{\"end\":41476,\"start\":41475},{\"end\":41485,\"start\":41484},{\"end\":41650,\"start\":41649},{\"end\":41658,\"start\":41657},{\"end\":41667,\"start\":41666},{\"end\":41676,\"start\":41675},{\"end\":41791,\"start\":41790},{\"end\":41799,\"start\":41798},{\"end\":41807,\"start\":41806},{\"end\":41815,\"start\":41814},{\"end\":41825,\"start\":41824},{\"end\":41837,\"start\":41836},{\"end\":42099,\"start\":42098},{\"end\":42101,\"start\":42100},{\"end\":42108,\"start\":42107},{\"end\":42122,\"start\":42121},{\"end\":42135,\"start\":42134},{\"end\":42137,\"start\":42136}]", "bib_author_last_name": "[{\"end\":35822,\"start\":35818},{\"end\":35833,\"start\":35828},{\"end\":35840,\"start\":35837},{\"end\":35850,\"start\":35844},{\"end\":35858,\"start\":35856},{\"end\":36092,\"start\":36082},{\"end\":36099,\"start\":36096},{\"end\":36109,\"start\":36103},{\"end\":36117,\"start\":36113},{\"end\":36124,\"start\":36121},{\"end\":36367,\"start\":36362},{\"end\":36377,\"start\":36373},{\"end\":36385,\"start\":36381},{\"end\":36397,\"start\":36389},{\"end\":36408,\"start\":36401},{\"end\":36420,\"start\":36412},{\"end\":36640,\"start\":36637},{\"end\":36651,\"start\":36644},{\"end\":36660,\"start\":36655},{\"end\":36671,\"start\":36664},{\"end\":36824,\"start\":36819},{\"end\":36835,\"start\":36828},{\"end\":36844,\"start\":36839},{\"end\":36969,\"start\":36967},{\"end\":36981,\"start\":36973},{\"end\":36990,\"start\":36985},{\"end\":37002,\"start\":36994},{\"end\":37130,\"start\":37128},{\"end\":37139,\"start\":37134},{\"end\":37146,\"start\":37143},{\"end\":37153,\"start\":37150},{\"end\":37304,\"start\":37297},{\"end\":37313,\"start\":37308},{\"end\":37324,\"start\":37317},{\"end\":37485,\"start\":37482},{\"end\":37495,\"start\":37491},{\"end\":37634,\"start\":37628},{\"end\":37640,\"start\":37638},{\"end\":37779,\"start\":37772},{\"end\":37788,\"start\":37783},{\"end\":37799,\"start\":37794},{\"end\":37809,\"start\":37803},{\"end\":37817,\"start\":37813},{\"end\":37830,\"start\":37821},{\"end\":38082,\"start\":38080},{\"end\":38092,\"start\":38086},{\"end\":38098,\"start\":38096},{\"end\":38104,\"start\":38102},{\"end\":38261,\"start\":38258},{\"end\":38270,\"start\":38265},{\"end\":38284,\"start\":38276},{\"end\":38297,\"start\":38290},{\"end\":38311,\"start\":38303},{\"end\":38319,\"start\":38315},{\"end\":38329,\"start\":38323},{\"end\":38340,\"start\":38333},{\"end\":38350,\"start\":38344},{\"end\":38363,\"start\":38356},{\"end\":38665,\"start\":38662},{\"end\":38681,\"start\":38669},{\"end\":38689,\"start\":38685},{\"end\":38887,\"start\":38883},{\"end\":38893,\"start\":38891},{\"end\":38901,\"start\":38897},{\"end\":38912,\"start\":38907},{\"end\":39156,\"start\":39150},{\"end\":39164,\"start\":39160},{\"end\":39321,\"start\":39310},{\"end\":39329,\"start\":39325},{\"end\":39335,\"start\":39333},{\"end\":39345,\"start\":39339},{\"end\":39357,\"start\":39349},{\"end\":39363,\"start\":39361},{\"end\":39372,\"start\":39367},{\"end\":39384,\"start\":39376},{\"end\":39394,\"start\":39388},{\"end\":39407,\"start\":39398},{\"end\":39417,\"start\":39413},{\"end\":39428,\"start\":39421},{\"end\":39770,\"start\":39763},{\"end\":39786,\"start\":39774},{\"end\":39797,\"start\":39790},{\"end\":39995,\"start\":39992},{\"end\":40006,\"start\":39999},{\"end\":40015,\"start\":40010},{\"end\":40026,\"start\":40019},{\"end\":40036,\"start\":40030},{\"end\":40311,\"start\":40302},{\"end\":40322,\"start\":40315},{\"end\":40522,\"start\":40518},{\"end\":40530,\"start\":40526},{\"end\":40537,\"start\":40534},{\"end\":40551,\"start\":40541},{\"end\":40557,\"start\":40555},{\"end\":40565,\"start\":40561},{\"end\":40754,\"start\":40751},{\"end\":40763,\"start\":40758},{\"end\":40769,\"start\":40767},{\"end\":40777,\"start\":40773},{\"end\":40963,\"start\":40961},{\"end\":40970,\"start\":40967},{\"end\":40978,\"start\":40974},{\"end\":40986,\"start\":40982},{\"end\":40992,\"start\":40990},{\"end\":41003,\"start\":40998},{\"end\":41235,\"start\":41233},{\"end\":41241,\"start\":41239},{\"end\":41248,\"start\":41245},{\"end\":41255,\"start\":41252},{\"end\":41444,\"start\":41440},{\"end\":41452,\"start\":41448},{\"end\":41459,\"start\":41456},{\"end\":41473,\"start\":41463},{\"end\":41482,\"start\":41477},{\"end\":41489,\"start\":41486},{\"end\":41655,\"start\":41651},{\"end\":41664,\"start\":41659},{\"end\":41673,\"start\":41668},{\"end\":41683,\"start\":41677},{\"end\":41796,\"start\":41792},{\"end\":41804,\"start\":41800},{\"end\":41812,\"start\":41808},{\"end\":41822,\"start\":41816},{\"end\":41834,\"start\":41826},{\"end\":41846,\"start\":41838},{\"end\":42105,\"start\":42102},{\"end\":42119,\"start\":42109},{\"end\":42132,\"start\":42123},{\"end\":42143,\"start\":42138}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11816255},\"end\":36005,\"start\":35778},{\"attributes\":{\"id\":\"b1\"},\"end\":36270,\"start\":36007},{\"attributes\":{\"id\":\"b2\"},\"end\":36607,\"start\":36272},{\"attributes\":{\"id\":\"b3\"},\"end\":36764,\"start\":36609},{\"attributes\":{\"id\":\"b4\"},\"end\":36951,\"start\":36766},{\"attributes\":{\"id\":\"b5\"},\"end\":37078,\"start\":36953},{\"attributes\":{\"id\":\"b6\"},\"end\":37257,\"start\":37080},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4593810},\"end\":37439,\"start\":37259},{\"attributes\":{\"id\":\"b8\"},\"end\":37578,\"start\":37441},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b9\"},\"end\":37750,\"start\":37580},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12312803},\"end\":37983,\"start\":37752},{\"attributes\":{\"id\":\"b11\"},\"end\":38254,\"start\":37985},{\"attributes\":{\"id\":\"b12\"},\"end\":38599,\"start\":38256},{\"attributes\":{\"id\":\"b13\"},\"end\":38806,\"start\":38601},{\"attributes\":{\"id\":\"b14\"},\"end\":39051,\"start\":38808},{\"attributes\":{\"id\":\"b15\"},\"end\":39304,\"start\":39053},{\"attributes\":{\"id\":\"b16\"},\"end\":39693,\"start\":39306},{\"attributes\":{\"id\":\"b17\"},\"end\":39928,\"start\":39695},{\"attributes\":{\"id\":\"b18\"},\"end\":40177,\"start\":39930},{\"attributes\":{\"id\":\"b19\"},\"end\":40488,\"start\":40179},{\"attributes\":{\"id\":\"b20\"},\"end\":40677,\"start\":40490},{\"attributes\":{\"id\":\"b21\"},\"end\":40906,\"start\":40679},{\"attributes\":{\"id\":\"b22\"},\"end\":41133,\"start\":40908},{\"attributes\":{\"id\":\"b23\"},\"end\":41404,\"start\":41135},{\"attributes\":{\"id\":\"b24\"},\"end\":41607,\"start\":41406},{\"attributes\":{\"id\":\"b25\"},\"end\":41788,\"start\":41609},{\"attributes\":{\"id\":\"b26\"},\"end\":42013,\"start\":41790},{\"attributes\":{\"id\":\"b27\"},\"end\":42302,\"start\":42015}]", "bib_title": "[{\"end\":35814,\"start\":35778},{\"end\":37293,\"start\":37259},{\"end\":37766,\"start\":37752}]", "bib_author": "[{\"end\":35824,\"start\":35816},{\"end\":35835,\"start\":35824},{\"end\":35842,\"start\":35835},{\"end\":35852,\"start\":35842},{\"end\":35860,\"start\":35852},{\"end\":36094,\"start\":36080},{\"end\":36101,\"start\":36094},{\"end\":36111,\"start\":36101},{\"end\":36119,\"start\":36111},{\"end\":36126,\"start\":36119},{\"end\":36369,\"start\":36360},{\"end\":36379,\"start\":36369},{\"end\":36387,\"start\":36379},{\"end\":36399,\"start\":36387},{\"end\":36410,\"start\":36399},{\"end\":36422,\"start\":36410},{\"end\":36642,\"start\":36635},{\"end\":36653,\"start\":36642},{\"end\":36662,\"start\":36653},{\"end\":36673,\"start\":36662},{\"end\":36826,\"start\":36817},{\"end\":36837,\"start\":36826},{\"end\":36846,\"start\":36837},{\"end\":36971,\"start\":36965},{\"end\":36983,\"start\":36971},{\"end\":36992,\"start\":36983},{\"end\":37004,\"start\":36992},{\"end\":37132,\"start\":37126},{\"end\":37141,\"start\":37132},{\"end\":37148,\"start\":37141},{\"end\":37155,\"start\":37148},{\"end\":37306,\"start\":37295},{\"end\":37315,\"start\":37306},{\"end\":37326,\"start\":37315},{\"end\":37487,\"start\":37480},{\"end\":37497,\"start\":37487},{\"end\":37636,\"start\":37624},{\"end\":37642,\"start\":37636},{\"end\":37781,\"start\":37768},{\"end\":37790,\"start\":37781},{\"end\":37801,\"start\":37790},{\"end\":37811,\"start\":37801},{\"end\":37819,\"start\":37811},{\"end\":37832,\"start\":37819},{\"end\":38084,\"start\":38078},{\"end\":38094,\"start\":38084},{\"end\":38100,\"start\":38094},{\"end\":38106,\"start\":38100},{\"end\":38263,\"start\":38256},{\"end\":38272,\"start\":38263},{\"end\":38286,\"start\":38272},{\"end\":38299,\"start\":38286},{\"end\":38313,\"start\":38299},{\"end\":38321,\"start\":38313},{\"end\":38331,\"start\":38321},{\"end\":38342,\"start\":38331},{\"end\":38352,\"start\":38342},{\"end\":38365,\"start\":38352},{\"end\":38667,\"start\":38658},{\"end\":38683,\"start\":38667},{\"end\":38691,\"start\":38683},{\"end\":38889,\"start\":38881},{\"end\":38895,\"start\":38889},{\"end\":38903,\"start\":38895},{\"end\":38914,\"start\":38903},{\"end\":39158,\"start\":39148},{\"end\":39166,\"start\":39158},{\"end\":39323,\"start\":39308},{\"end\":39331,\"start\":39323},{\"end\":39337,\"start\":39331},{\"end\":39347,\"start\":39337},{\"end\":39359,\"start\":39347},{\"end\":39365,\"start\":39359},{\"end\":39374,\"start\":39365},{\"end\":39386,\"start\":39374},{\"end\":39396,\"start\":39386},{\"end\":39409,\"start\":39396},{\"end\":39419,\"start\":39409},{\"end\":39430,\"start\":39419},{\"end\":39772,\"start\":39761},{\"end\":39788,\"start\":39772},{\"end\":39799,\"start\":39788},{\"end\":39997,\"start\":39990},{\"end\":40008,\"start\":39997},{\"end\":40017,\"start\":40008},{\"end\":40028,\"start\":40017},{\"end\":40038,\"start\":40028},{\"end\":40313,\"start\":40300},{\"end\":40324,\"start\":40313},{\"end\":40524,\"start\":40516},{\"end\":40532,\"start\":40524},{\"end\":40539,\"start\":40532},{\"end\":40553,\"start\":40539},{\"end\":40559,\"start\":40553},{\"end\":40567,\"start\":40559},{\"end\":40756,\"start\":40749},{\"end\":40765,\"start\":40756},{\"end\":40771,\"start\":40765},{\"end\":40779,\"start\":40771},{\"end\":40965,\"start\":40959},{\"end\":40972,\"start\":40965},{\"end\":40980,\"start\":40972},{\"end\":40988,\"start\":40980},{\"end\":40994,\"start\":40988},{\"end\":41005,\"start\":40994},{\"end\":41237,\"start\":41231},{\"end\":41243,\"start\":41237},{\"end\":41250,\"start\":41243},{\"end\":41257,\"start\":41250},{\"end\":41446,\"start\":41438},{\"end\":41454,\"start\":41446},{\"end\":41461,\"start\":41454},{\"end\":41475,\"start\":41461},{\"end\":41484,\"start\":41475},{\"end\":41491,\"start\":41484},{\"end\":41657,\"start\":41649},{\"end\":41666,\"start\":41657},{\"end\":41675,\"start\":41666},{\"end\":41685,\"start\":41675},{\"end\":41798,\"start\":41790},{\"end\":41806,\"start\":41798},{\"end\":41814,\"start\":41806},{\"end\":41824,\"start\":41814},{\"end\":41836,\"start\":41824},{\"end\":41848,\"start\":41836},{\"end\":42107,\"start\":42098},{\"end\":42121,\"start\":42107},{\"end\":42134,\"start\":42121},{\"end\":42145,\"start\":42134}]", "bib_venue": "[{\"end\":35882,\"start\":35860},{\"end\":36078,\"start\":36007},{\"end\":36358,\"start\":36272},{\"end\":36633,\"start\":36609},{\"end\":36815,\"start\":36766},{\"end\":36963,\"start\":36953},{\"end\":37124,\"start\":37080},{\"end\":37330,\"start\":37326},{\"end\":37478,\"start\":37441},{\"end\":37622,\"start\":37580},{\"end\":37854,\"start\":37832},{\"end\":38076,\"start\":37985},{\"end\":38406,\"start\":38365},{\"end\":38656,\"start\":38601},{\"end\":38879,\"start\":38808},{\"end\":39146,\"start\":39053},{\"end\":39759,\"start\":39695},{\"end\":39988,\"start\":39930},{\"end\":40298,\"start\":40179},{\"end\":40514,\"start\":40490},{\"end\":40747,\"start\":40679},{\"end\":40957,\"start\":40908},{\"end\":41229,\"start\":41135},{\"end\":41436,\"start\":41406},{\"end\":41647,\"start\":41609},{\"end\":41884,\"start\":41848},{\"end\":42096,\"start\":42015}]"}}}, "year": 2023, "month": 12, "day": 17}