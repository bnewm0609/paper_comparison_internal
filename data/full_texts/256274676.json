{"id": 256274676, "updated": "2023-10-05 04:43:51.829", "metadata": {"title": "Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons", "authors": "[{\"first\":\"Banghua\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Jiantao\",\"last\":\"Jiao\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Jordan\",\"middle\":[\"I.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound for max-entropy IRL.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2301.11270", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ZhuJJ23", "doi": "10.48550/arxiv.2301.11270"}}, "content": {"source": {"pdf_hash": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.11270v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5d5c6e734e38823539f3031f7d8dc804b12d9866", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4.txt", "contents": "\nPrincipled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons\n\n\nBanghua Zhu \nMichael I Jordan \nDepartment of Statistics\nBerkeleyUC\n\nJiantao Jiao \nDepartment of Statistics\nBerkeleyUC\n\n\nDepartment of Electrical Engineering and Computer Sciences\nUC Berkeley\n\n\nPrincipled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons\n\nWe provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the K-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. We also unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound for max-entropy IRL.\n\nIntroduction\n\nThe alignment problem aims at aligning human values with machine learning systems and steering learning algorithms towards the goals and interests of humans. One of the most promising tools for AI alignment, Reinforcement Learning with Human Feedback (RLHF, or Preference-based Reinforcement Learning), has delivered significant empirical success in the fields of game playing, robot training, stock-prediction, recommender systems, clinical trials, large language models etc. (Novoseller et al., 2019;Sadigh et al., 2017;Christiano et al., 2017b;Kupcsik et al., 2018;Jain et al., 2013;Wirth et al., 2017;Knox and Stone, 2008;MacGlashan et al., 2017;Christiano et al., 2017a;Warnell et al., 2018;Brown et al., 2019;Shin et al., 2023;Ziegler et al., 2019;Stiennon et al., 2020;Wu et al., 2021;Nakano et al., 2021;Ouyang et al., 2022;Menick et al., 2022;Glaese et al., 2022;Gao et al., 2022;Bai et al., 2022a;Ganguli et al., 2022;Ramamurthy et al., 2022). Notably, the language model application ChatGPT is based on RLHF and this underlies several of its skills: answering followup questions, admitting its mistakes, challenging incorrect premises, and rejecting inappropriate requests. One of the key capabilities in RLHF is to learn a reward from human feedback, in the form of pairwise or K-wise comparisons between actions (responses). In this paper, we take the first step towards providing a theoretical framework for RLHF, with a specific focus on reward learning. We provide theoretical analysis that justifies the empirical success of RLHF in InstructGPT and ChatGPT, along with new insights for algorithm design.\n\nTaking InstructGPT Ouyang et al. (2022) as an example, a typical deployment of RLHF for language modeling includes the following steps:\n\n(a) Pre-train a Large Language Model (LLM) using supervised training. (b) Train a reward model based on the pre-trained LLM using human feedback. (c) Fine-tune the existing LLM based on the learned reward model using Proximal Policy Optimization (PPO).\n\nDuring the reward training step, the prompts are first sampled from a pre-collected dataset. Then K responses are sampled by executing existing models on the sampled prompts. Based on the prompt provided, a human labeler ranks all the responses according to her own preference. The reward model is trained based on a maximum likelihood estimator (MLE), also known as the learning-to-rank algorithm or cross-entropy minimization (Liu et al., 2009;Xia et al., 2008;Cao et al., 2007;Christiano et al., 2017a;Ouyang et al., 2022).\n\nIn the setting of InstructGPT, the ranking of responses is based purely on the current prompt, which can be viewed as the state in a contextual bandit. We accordingly start with the setting of a contextual bandit, and later generalize our results to Markov Decision Process (MDP) where there are transitions between states. Let S be the set of states (prompts), and A be the set of actions (responses). For each state-action pair (s, a), we assume that the reward is parametrized by r \u03b8 (s, a) = \u27e8\u03b8, \u03d5(s, a)\u27e9 for some known and fixed feature function \u03d5(s, a) : S \u00d7 A \u2192 R d . In an LLM, such a \u03d5 is usually derived by removing the last layer of the pre-trained model. 1 We denote the ground-truth reward provided by a human as r \u03b8 \u22c6 (s, a) for some parameter \u03b8 \u22c6 \u2208 R d .\n\nWe are interested in the sample complexity for learning a reward model r \u03b8 \u22c6 from pairwise or K-wise comparison data. For the i-th sample, a state s i is first sampled from some fixed distribution \u03c1. Given the state s i , K actions (a i 0 , a i 1 , \u00b7 \u00b7 \u00b7 , a i K\u22121 ) are sampled from some joint distribution P(a 0 , \u00b7 \u00b7 \u00b7 , a K\u22121 | s i ). Let \u03c3 i : [K] \u2192 [K] denote the output of the human labeller, which is a permutation function representing the ranking of the actions. Here \u03c3 i (0) represents the most preferred action. We assume that the distribution of \u03c3 i follows a Plackett-Luce (PL) model (Plackett, 1975;Luce, 2012):\nP(\u03c3 i | s i , a i 0 , a i 1 , \u00b7 \u00b7 \u00b7 , a i K\u22121 ) = K\u22121 k=0 exp(r \u03b8 \u22c6 (s i , a i \u03c3 i (k) )) K\u22121 j=k exp(r \u03b8 \u22c6 (s i , a i \u03c3 i (j) ))\n.\n\nWhen K = 2, this reduces to the pairwise comparison of the Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952), which is widely applied in existing RLHF algorithms Christiano et al. (2017a); Ouyang et al. (2022).\n\nSince the learned reward model is mainly used for downstream policy training, we measure the correctness of the estimated reward model via the performance of a greedy policy trained from a reward model r\u03b8. Concretely, for a greedy policy\u03c0(s) = arg max a r\u03b8(s, a), we compute a performance gap compared to the optimal policy: SubOpt(\u03c0) := E s\u223c\u03c1 [r \u03b8 \u22c6 (s, \u03c0 \u22c6 (s)) \u2212 r \u03b8 \u22c6 (s,\u03c0(s)].\n\nHere \u03c0 \u22c6 = arg max a r \u03b8 \u22c6 (s, a) is the optimal policy under the true reward r \u03b8 \u22c6 . Gao et al. (2022) has observed that in the reward model trained from practice, there exists an overoptimization phenomenon where the true reward first increases and then decreases during the policy optimization stage. In this paper, we study the potential sub-optimality of the MLE in the RLHF setting. As a by-product, we also provide guarantee of the estimation error on the semi-norm of the parameter estimation error, \u2225\u03b8 \u2212 \u03b8 \u22c6 \u2225 \u03a3 , for a query-dependent covariance matrix \u03a3.\n\nFrom a broader perspective, the framework of RLHF can be viewed as a special case of reward learning from pre-collected data, which has been a primary focus in Inverse Reinforcement Learning (IRL) and offline reinforcement learning. Our techniques also provide theoretical guarantee for the max-entropy IRL Ziebart et al. (2008) and action-based IRL algorithms Ramachandran and Amir (2007); Neu and Szepesv\u00e1ri (2009) ;Florence et al. (2022).\n\n\nMain Results\n\nPairwise Comparison. We start with the setting of a contextual bandit with pairwise comparison. We focus on two algorithms, MLE and pessimistic MLE. The following result from dueling bandits and RL Faury et al. (2020); Pacchiano et al. (2021) shows that under a semi-norm \u2225 \u00b7 \u2225 \u03a3 , MLE converges to the true parameter.\n\nLemma 1.1 (Informal). Under certain regularity conditions, the MLE satisfies the following with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D \u2264 C \u00b7 d + log(1/\u03b4) n .\nHere \u03a3 D = 1 n n i=1 (\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 ))(\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 )) \u22a4 .\n\nHowever, when we consider the performance of the induced policy, MLE provably fails while pessimistic MLE gives a near-optimal rate. In essence, the pessimism principle discounts actions that are less represented in the observed dataset, and hence is conservative in outputting a policy.\n\nTheorem 1.2 (Informal). Under certain coverage assumption, one can design a pessimistic MLE such that the induced greedy policy\u03c0 PE is good; i.e., with probability at least 1 \u2212 \u03b4,\nSubOpt(\u03c0 PE ) = \u0398 d + log(1/\u03b4) n .\nIn contrast, under the same assumption, one can find instances such that the greedy policy w.r.t. ML\u00ca \u03c0 MLE fails:\n\u2200n > 1, E[SubOpt(\u03c0 MLE )] \u2265 0.1.\nK-wise Comparison. For K-wise comparison, we analyze both the MLE and the algorithm in InstructGPT (Ouyang et al., 2022) which splits the ranking data into K(K \u22121)/2 pairwise comparison data and runs an MLE based on the BTL model. We show that both converge in terms of the estimation error under the semi-norm, and give a near-optimal policy when combined with pessimism. More importantly, we show that although both estimators are unbiased, the asymptotic variance of MLE is smaller than that of the splitted estimator in InstructGPT (Ouyang et al., 2022), which belongs to the family of M-estimators. Thus the MLE is more efficient than the existing algorithm used in InstructGPT. We also conduct experiments to verify the theoretical prediction. Let the estimated parameter for the splitted estimator be\u03b8 and the induced policy be\u03c0 PE . We have:\nTheorem 1.3 (Informal)\n. Under certain coverage and regularity conditions, the following holds separately with probability at least 1 \u2212 \u03b4:\n\u2225\u03b8 \u2212 \u03b8 \u22c6 \u2225 \u03a3 D \u2264 C \u00b7 d + log(1/\u03b4) n , SubOpt(\u03c0 PE ) \u2264 C \u2032 \u00b7 d + log(1/\u03b4) n , Here \u03a3 D = 2 K(K\u22121)n ( n i=1 K\u22121 j=0 K\u22121 k=j+1 (\u03d5(s i , a i j ) \u2212 \u03d5(s i , a i k ))(\u03d5(s i , a i j ) \u2212 \u03d5(s i , a i k )) \u22a4 ).\nWe also extend our results to the case of MDP and IRL; see the detailed presentation in Section 5 and Section 6. Let the estimated parameter be\u03b8 and the induced pessimistic policy be\u03c0 PE . For pairwise comparison we have: Theorem 1.4 (Informal). In the MDP setting with horizon H, under certain coverage and regularity conditions, the following holds separately with probability at least 1 \u2212 \u03b4:\n\u2225\u03b8 \u2212 \u03b8 \u22c6 \u2225 \u03a3 D \u2264 C \u00b7 d + log(1/\u03b4) n , SubOpt(\u03c0 PE ) \u2264 C \u2032 \u00b7 d + log(1/\u03b4) n , Here \u03a3 D = 1 n n i=1 ( H h=0 (\u03d5(s i h , a i h ) \u2212 \u03d5(s i\u2032 h , a i\u2032 h ))) ( H h=0 (\u03d5(s i h , a i h ) \u2212 \u03d5(s i\u2032 h , a i\u2032 h ))) \u22a4 .\nOur results not only explain the correctness of existing algorithms, but also provide new insights for algorithm design in RLHF. In particular, it suggests the importance of introducing pessimism in the reward learning part, which can be implemented via adding regularization in policy training steps as in Ouyang et al. (2022), or using existing offline RL algorithms, including but not limited to Conservative Q-Learning (Kumar et al., 2020), Implicit Q-Learning (Kostrikov et al., 2021) and Adversarially Trained Actor Critic (Cheng et al., 2022). On the other hand, it also shows that MLE is a more efficient estimator than that in Ouyang et al. (2022).\n\n\nRelated Work\n\nLearning and Estimation from Pairwise Comparison and Ranking. The problem of estimation and ranking from pairwise or K-wise comparisons has been studied extensively in the literature. In the literature of dueling bandit, one compares two actions and aims to minimize regret based on pairwise comparisons (Yue et al., 2012;Zoghi et al., 2014b;Yue andJoachims, 2009, 2011;Saha and Krishnamurthy, 2022;Ghoshal and Saha, 2022;Saha and Gopalan, 2018a;Ailon et al., 2014;Zoghi et al., 2014a;Komiyama et al., 2015;Gajane et al., 2015;Gopalan, 2018b, 2019;Faury et al., 2020). Novoseller et al. (2019);Xu et al. (2020) analyze the sample complexity of dueling RL under the tabular case, which is extended to linear case and function approximation by the recent work Pacchiano et al. (2021);Chen et al. (2022). Chatterji et al. (2022) studies a close setting where in each episode only binary feedback is received. However, most of the work focuses on regret minimization. We take a first step towards the theoretical analysis for function approximation for K-wise comparisons with policy learning as the target.\n\nOn the other hand, in the literature of ranking, most of the theoretical work focuses on the tabular case where the rewards for different actions are uncorrelated (Feige et al., 1994;Shah et al., 2015;Shah and Wainwright, 2017;Heckel et al., 2018;Mao et al., 2018;Jang et al., 2017;Chen et al., 2013;Chen and Suh, 2015;Rajkumar and Agarwal, 2014;Negahban et al., 2018;Hajek et al., 2014;Heckel et al., 2019). And a majority of the empirical literature focuses on the framework of learning to rank (MLE) under general function approximation, especially when the reward is parameterized by a neural network (Liu et al., 2009;Xia et al., 2008;Cao et al., 2007;Christiano et al., 2017a;Ouyang et al., 2022;Brown et al., 2019;Shin et al., 2023;Busa-Fekete et al., 2014;Wirth et al., 2016Wirth et al., , 2017Christiano et al., 2017b;Abdelkareem et al., 2022). Similar idea of RL with AI feedback also learns a reward model from preference Bai et al. (2022b), except for that the preference is labeled by another AI model instead of human.\n\nInverse Reinforcement Learning and Offline Reinforcement Learning. RLHF, IRL and offline learning are all approaches that can be used to incorporate human preferences or expertise into the decision-making process of an agent. However, they differ in the way that they use human input to guide the agent's behavior. In IRL and imitation learning, we only observe an expert's behavior and would like to infer the expert's preferences or goals (Ng et al., 2000;Abbeel and Ng, 2004;Ziebart et al., 2008;Ramachandran and Amir, 2007;Neu and Szepesv\u00e1ri, 2009;Ho and Ermon, 2016;Florence et al., 2022;Hussein et al., 2017). In offline learning, we directly observe the cardinal rewards for the state. But the actions are likely to be sub-optimal. In RLHF, we observe ordinal comparisons between pairs or a set of actions. In one of the popular IRL frameworks, max-entropy IRL (Ziebart et al., 2008), it is also assumed that human choice follows a PL model. We unify the problem of RLHF and max-entropy IRL, and provide the first sample complexity analysis for max-entropy IRL.\n\nPessimism in Offline RL. The idea of introducing pessimism for offline RL has been studied in recent year (Jin et al., 2021;Rashidinejad et al., 2021;Li et al., 2022;Xie et al., 2021b;Zanette, 2022;Zanette et al., 2021;Xie et al., 2021a;Xu and Liang, 2022). In this paper, we connect RLHF with offline RL and show that pessimism also helps in RLHF.\n\n\nPreliminaries\n\nWe begin with the notation that we use in the paper. We discuss our formulations of contextual bandits and Markov decision processes in Section 2.1. We introduce the data collection model and the BTL and PL models in Section 2.2.\n\nNotations. We use calligraphic letters for sets, e.g., S and A. Given a set S, we write |S| to represent the cardinality of S. For vectors x and y, we use \u27e8x, y\u27e9 = x \u22a4 y to denote their inner product. We use [K] to denote the set of integers from 0 to K \u2212 1. We write \u2225x\u2225 \u03a3 = \u221a\n\nx \u22a4 \u03a3x as a semi-norm of x when \u03a3 is some positive-semidefinite matrix. We write \u03a3 \u2ab0 \u03a3 \u2032 if \u03a3 \u2212 \u03a3 \u2032 is positive semidefinite.\n\n\nMarkov decision processes\n\nWe consider a finite-horizon MDP described by a tuple M = (S, A, H,\n{P h } H h=1 , {R h } H h=1 , \u03c1)\n, where S is a (possibly infinite) state space, A is a (possibly infinite) action space, H is the horizon length, P h : S \u00d7 A \u2192 \u2206(S) is a probability transition matrix at step h, R h : S \u00d7 A \u2192 \u2206([0, 1]) encodes a family of reward distributions with r h : S \u00d7 A \u2192 [0, 1] as the expected reward function, \u03c1 : S \u2192 \u2206(S) is the initial state distribution. At step h, upon executing action a from state s, the agent receives a deterministic reward r h (s, a) and transits to the next state s \u2032 with probability P h (s \u2032 |s, a). The MDP transits to an absorbing termination state with zero reward at step H. When H = 1 and there is no transition, the model reduces to the contextual bandit problem. A deterministic policy \u03c0 h : S \u2192 A is a function that maps a state to an action at step h \u2208 [H]. We use \u03c0 to denote the family of policies {\u03c0 h } H h=1 . Correspondingly, the value function V \u03c0 : S \u2192 R of the policy family {\u03c0 h } h\u2208 [H] is defined as the expected sum of rewards starting at state s and following policy \u03c0 h at step h. More precisely, we have for any s \u2208 S, V \u03c0 (s) :\n= E H h=0 r h (s h , a h ) | s 0 = s, a h = \u03c0 h (s h ), \u2200h \u2265 0 ,\nwhere the expectation is taken over the trajectory generated according to the transition kernel s h+1 \u223c P h (\u00b7 | s h , a h ) and reward distribution r h \u223c R h (\u00b7 | s h , a h ). The Q-function Q \u03c0 : S \u00d7 A \u2192 R of policy \u03c0 is defined analogously: Q \u03c0 (s, a) := E H h=0 r h (s h , a h ) | s 0 = s, a 0 = a, a h = \u03c0 h (s h ), \u2200h \u2265 0 . Note that although we work with undiscounted episodic case, it is straightforward to extend the framework and analysis to discounted MDP. We define the expected value of a policy \u03c0:\nJ(\u03c0) := E s\u223c\u03c1 [V \u03c0 (s)] = s\u2208S \u03c1(s)V \u03c0 (s).\nWe use shorthands V \u22c6 := V \u03c0 \u22c6 and Q \u22c6 := Q \u03c0 \u22c6 to denote the optimal value function and the optimal Q-function. We define the sub-optimality of any policy \u03c0 as SubOpt(\u03c0) := J(\u03c0 \u22c6 ) \u2212 J(\u03c0).\n\nWe use shorthands V \u22c6 := V \u03c0 \u22c6 and Q \u22c6 := Q \u03c0 \u22c6 to denote the optimal value function and the optimal Q-function. We define the sub-optimality of any policy \u03c0 as SubOpt(\u03c0) := J(\u03c0 \u22c6 ) \u2212 J(\u03c0).\n\nWe also define the state occupancy measures d \u03c0 : S \u2192 [0, H] and state-action occupancy measures d \u03c0 : S \u00d7 A \u2192 [0, H] as d \u03c0 (s) := H h=0 P h (s h = s | \u03c0), d \u03c0 (s, a) := H h=0 P h (s h = s; a h = a | \u03c0), where we use P h (s h = s | \u03c0) to denote the probability of visiting state s h = s (and similarly s h = s, a h = a) at step h after executing policy \u03c0 and starting from s 0 \u223c \u03c1(\u00b7).\n\nThroughout the paper, we make the following assumption on the parameterization of the reward:\n\nAssumption 2.1. The reward lies in the family of linear functions r \u03b8 (s, a) = \u03b8 \u22a4 \u03d5(s, a) for some known \u03d5(s, a) with max s,a \u2225\u03d5(s, a)\u2225 2 \u2264 L. Let \u03b8 \u22c6 be the true parameter. To ensure the identifiability of \u03b8 \u22c6 , we let \u03b8 \u22c6 \u2208 \u0398 B , where\n\u0398 B = {\u03b8 \u2208 R d | \u27e81, \u03b8\u27e9 = 0, \u2225\u03b8\u2225 2 \u2264 B}.\n\nSampling Procedure and Comparison Model\n\nAs in Ouyang et al. (2022), we assume that both the states and actions in the training set come from a pre-collected dataset. In a contextual bandit, for the i-th sample, a state (prompt) s i is first sampled from some fixed distribution \u03c1. Given the state s i , K actions (a i 0 , a i 1 , \u00b7 \u00b7 \u00b7 , a i K\u22121 ) are sampled from some joint distribution P(a 0 , \u00b7 \u00b7 \u00b7 , a K\u22121 | s i ) 2 . Let \u03c3 i : [K] \u2192 [K] be the output of the human labeller, which is a permutation function that denotes the ranking of the actions. Here \u03c3 i (0) represents the most preferred action. We use a 0 > a 1 to denote the event that the action a 0 is more preferred compared to a 1 . A common model on the distribution of \u03c3 under K-ary comparisons is a Plackett-Luce model (Plackett, 1975;Luce, 2012). The Plackett-Luce model defines the probability of a state-action pair (s, a i ) being the largest among a given set {(s, a i )} K\u22121 i=0 as\nP(a i > a j , \u2200j \u0338 = i | s) = exp(r \u03b8 (s, a i )) K\u22121 j=0 exp(r \u03b8 (s, a j ))\n.\n\nMoreover, one can calculate the probability of observing the permutation \u03c3 as 3\nP(\u03c3 | s, {a i } K\u22121 i=0 ) = K\u22121 i=0 exp(r \u03b8 \u22c6 (s, a \u03c3(i) )) K\u22121 j=i exp(r \u03b8 \u22c6 (s, a \u03c3(j) ))\n.\n\nWhen K = 2, this reduces to the pairwise comparison considered in the BTL model, which is used in existing RLHF algorithms. In this case, the permutation \u03c3 can be reduced to a Bernoulli random variable, representing whether a 0 is preferred compared to a 1 . Concretely, for each queried state-actions pair (s, a 0 , a 1 ), we observe a sample y from a Bernoulli distribution with parameter exp(r \u03b8 \u22c6 (s,a1)) exp(r \u03b8 \u22c6 (s,a0))+exp(r \u03b8 \u22c6 (s,a1)) ; i.e., for any l \u2208 {0, 1}, P(y = l | s, a 0 , a 1 ) = exp(r \u03b8 \u22c6 (s, a l )) exp(r \u03b8 \u22c6 (s, a 0 )) + exp(r \u03b8 \u22c6 (s, a 1 )) .\n\n\nOrganization\n\nSection 3 presents the problem of learning with pairwise comparisons under the contextual bandit framework, we provide upper and lower bounds for MLE and pessimistic MLE. We extend the result into K-wise comparisons in Section 4 and MDP in Section 5. We discuss the guarantee for IRL in Section 6. We present our experimental results on simulated dataset in Section 7. We also discuss the analysis for nonlinear rewards in Appendix A .\n\n\nLearning from Pairwise Comparison\n\nWe begin with the problem of learning from pairwise comparisons under the BTL model.\n\n\nAlgorithms: MLE and Pessimistic MLE\n\nWe first bound the estimation error for MLE, the most common algorithm in learning to rank and RLHF Liu et al. (2009) (2022). For any query-observation dataset {(s i , a i 1 , a i 2 , y i )} n i=1 , MLE aims at minimizing the negative log likelihood, defined as:\n\u03b8 MLE \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), \u2113 D (\u03b8) = \u2212 n i=1 log 1(y i = 1) \u00b7 exp(r \u03b8 (s i , a i 1 )) exp(r \u03b8 (s i , a i 0 )) + exp(r \u03b8 (s i , a i 1 )) + 1(y i = 0) \u00b7 exp(r \u03b8 (s i , a i 0 )) exp(r \u03b8 (s i , a i 0 )) + exp(r \u03b8 (s i , a i 1 )) = \u2212 n i=1 log 1(y i = 1) \u00b7 sigmoid(\u27e8\u03b8, \u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 )\u27e9) + 1(y i = 0) \u00b7 sigmoid(\u27e8\u03b8, \u03d5(s i , a i 0 ) \u2212 \u03d5(s i , a i 1 )\u27e9) .\nWhen the minimizer is not unique, we take any of the\u03b8 that achieve the minimum. Let D = {(s i , a i 1 , a i 2 )} n i=1 denote the queried state-action pairs. In this paper, we study how one can utilize D to learn a near-optimal reward model and policy. We first present a lemma on the estimation error conditioned on the data D. The lemma is a generalization of the upper bound in Shah et al. (2015, Algorithm 1 Pessimistic MLE Input: The current estimator\u03b8, the data covariance \u03a3 D , the regularization parameter \u03bb, the bound on the semi-norm f (n, d, \u03b4, \u03bb), a reference vector v \u2208 R d , state distribution q Construct the confidence set\n\u0398(\u03b8, \u03bb) = \u03b8 \u2208 \u0398 B | \u2225\u03b8 \u2212 \u03b8\u2225 \u03a3 D +\u03bbI \u2264 f (n, d, \u03b4, \u03bb) .\nCompute the pessimistic expected value function\nJ(\u03c0) = min \u03b8\u2208\u0398(\u03b8,\u03bb) E s\u223cq [\u03b8 \u22a4 (\u03d5(s, \u03c0(s)) \u2212 v)] = (E s\u223cq [\u03d5(s, \u03c0(s))] \u2212 v) \u22a4\u03b8 \u2212 \u2225(\u03a3 D + \u03bbI) \u2212 1 2 (E s\u223cq [\u03d5(s, \u03c0(s))] \u2212 v)\u2225 2 \u00b7 f (n, d, \u03b4, \u03bb)\nReturn:\u03c0 = arg max \u03c0\u0134 (\u03c0).\n\nTheorem 1) and the analysis follows a similar structure. The main difference is that Shah et al. (2015) focus on the tabular case when \u03d5(s, a) is always a standard basis vector, while in our case \u03d5(s, a) can be an arbitrary d-dimensional vector. This confidence bound guarantee is also similar to the guarantee for dueling bandits and RL in Faury et al. (2020); Pacchiano et al. (2021), except for that we have better rate in logarithmic factors since union bound is not needed in our case.\n\nLemma 3.1. For any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 . Here \u03a3 D = 1 n n i=1 (\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 ))(\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 )) \u22a4 , \u03b3 = 1/(2 + exp(\u2212LB) + exp(LB))\n. The proof is deferred to Appendix B.1. The optimality of the bound can be seen via a lower-bound argument akin to that in Shah et al. (2015, Theorem 1). Now consider the set of parameters\n\u0398(\u03b8 MLE , \u03bb) = \u03b8 \u2208 \u0398 B | \u2225\u03b8 MLE \u2212 \u03b8\u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 d + log( 1 \u03b4 ) \u03b3 2 n + \u03bbB 2 .\nLemma 3.1 shows that with probability at least 1 \u2212 \u03b4, one has \u03b8 \u22c6 \u2208 \u0398(\u03b8 MLE ). We thus consider the pessimistic MLE in Algorithm 1, which takes the lower confidence bound (LCB) as the reward estimate.\n\nIn the context of LLM, the features of meaningful prompts and responses usually lie on a low-dimensional manifold. The idea of pessimism is to assign larger reward for the responses that lie on the manifold, and penalize the rarely seen responses that do not lie on manifold. We have the following guarantee for pessimistic MLE:\nTheorem 3.2. Let\u03c0 PE be the output of Algorithm 1 when taking\u03b8 =\u03b8 MLE , f (n, d, \u03b4, \u03bb) = C \u00b7 d+log(1/\u03b4) \u03b3 2 n + \u03bbB 2 , q = \u03c1. For any \u03bb > 0 and v \u2208 R d , with probability at least 1 \u2212 \u03b4, SubOpt(\u03c0 PE ) \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 \u00b7 \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223c\u03c1 [(\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)]\u2225 2 .\nThe proof is deferred to Appendix B.2. We make several remarks. Remark 3.3 (The single concentratability coefficient assumption). When v = 0, the term \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223c\u03c1 [\u03d5(s, \u03c0 \u22c6 (s))]\u2225 2 is referred to as a \"single concentratability coefficient\", which is assumed to be bounded in most of the literature on offline learning (Rashidinejad et al., 2021;Li et al., 2022;Xie et al., 2021b;Zanette, 2022;Zanette et al., 2021). A bounded concentratability coefficient can be understood as certifying good coverage of the target vector E s\u223c\u03c1 [\u03d5(s, \u03c0 \u22c6 (s))] from the dataset D in the feature space. The performance guarantee also holds when we replace \u03c0 \u22c6 with any reference policy \u03c0 on both sides.\n\nRemark 3.4 (The choice of \u03bb). When \u03a3 D is invertible, or when any \u03b8 \u2208 \u0398 B is orthogonal to the nullspace of \u03a3 D , the above inequality holds for the case of \u03bb = 0. In other cases, one may minimize \u03bb on the right-hand side, or simply take \u03bb = (d + log(1/\u03b4)/(B 2 \u03b3 2 n)) to achieve a near-optimal rate up to a constant factor.\n\nRemark 3.5 (The choice of v). Compared to the traditional pessimism principle (Rashidinejad et al., 2021;Li et al., 2022;Xie et al., 2021b;Zanette, 2022;Zanette et al., 2021), we subtract an extra reference vector v in all the feature vectors \u03d5. Subtracting a constant vector in feature space will not change the induced policy, but may affect the concentratability coefficient\n\u2225(\u03a3 D + \u03bbI) \u22121/2 (E s\u223c\u03c1 [\u03d5(s, \u03c0(s))] \u2212 v)\u2225 2 .\nWe briefly describe the reason for introducing v here. Consider the case where the differences between features lie in the same subspace, while the feature \u03d5 itself does not. As a concrete example, consider a single state s and two actions a 0 , a 1 , we let \u03d5(s, a 0 ) = (1, 1) and \u03d5(s, a 1 ) = (1, 0). The data covariance is\n(\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 ))(\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 )) \u22a4 = [0, 0; 0, 1]. Thus \u2225(\u03a3 D + \u03bbI) \u22121/2 \u03d5(s, a 0 )\u2225 2\ncan be arbitrarily large as \u03bb \u2192 0 when v = 0. On the other hand, when we take v = \u03d5(s, a 1 ), one can verify that\n\u2225(\u03a3 D + \u03bbI) \u22121/2 (\u03d5(s, a 0 ) \u2212 v)\u2225 2 \u2264 1.\nThe above example illustrates the importance of choosing an appropriate v. A good rule of thumb for choosing v is the most common feature vector \u03d5 that appears in the data, so that more features can be covered. This also affords additional design latitude for other pessimism algorithms.\n\nRemark 3.6 (Implementation for neural network). When r \u03b8 is a neural network, Algorithm 1 may not be directly implementable. As an alternative, there has been a number of heuristic approximations considered, including Conservative Q-Learning (Kumar et al., 2020), Implicit Q-Learning (Kostrikov et al., 2021) and Adversarially Trained Actor Critic (Cheng et al., 2022). Furthermore, one may also introduce pessimism in the policy training procedure. For example, Ouyang et al. (2022) add regularization terms in policy training, which enforces that the policy stays close to the original policy, and within the coverage of the pre-trained dataset. Our analysis supplies a theoretical rationale for such regularization terms.\n\nRemark 3.7 (Implications for online learning). Although we mainly focus on offline learning, Lemma 3.1 also gives a straightforward online learning algorithm when combined with an optimism-based algorithm. In particular, a pure exploration-based active learning scheme would seek to compare pairs of actions whose feature difference is poorly covered by the past observations; i.e., find (s, a 1 , a 2 ) such that \u2225\u03d5(s, a 1 ) \u2212 \u03d5(s, a 2 )\u2225 (\u03a3 D +\u03bbI) \u22121 is maximized. As a corollary of Lemma 3.1 and exploration results for linear bandits (Abbasi-Yadkori et al., 2011;Soare et al., 2014), one can derive tight regret bound for online learning.\n\nRemark 3.8 (Special Case: Multi-Armed Bandit). For multi-armed bandits we have only a single state, such that the feature \u03d5(s, a) reduces to \u20d7 1 a , which is a unit vector with 1 on its a-th element. In this case, the data covariance reduces to a Laplacian matrix, defined as \u03a3\nD = 1 n n i=1 ( \u20d7 1 a1 \u2212 \u20d7 1 a0 )( \u20d7 1 a1 \u2212 \u20d7 1 a0 ) \u22a4 .\nThis is precisely the problem considered in Shah et al. (2015). The Laplacian matrix is positive semidefinite and always has a zero eigenvalue, corresponding to an all ones eigenvector. When the graph induced by the Laplacian matrix is connected, any \u03b8 with \u27e81, \u03b8\u27e9 = 0 is orthogonal to the nullspace of \u03a3 D , thus the theorem holds for the case of \u03bb = 0.\n\n\nFailure of MLE and Lower Bounds\n\nWe also show that there exists a simple linear bandit where MLE fails and pessimistic MLE succeeds. Let \u03c0 MLE = arg max \u03c0 E[r\u03b8 MLE (s, \u03c0(s))] be the greedy policy with respect to the MLE. Theorem 3.9. There exists a linear bandit with four actions and a sampling distribution such that for any n > 1,\nE[SubOpt(\u03c0 MLE )] \u2265 0.1.\nOn the other hand, with probability at least 1 \u2212 \u03b4,\nSubOpt(\u03c0 PE ) \u2264 C \u00b7 log(1/\u03b4) \u221a n .\nHere C is some universal constant.\n\nThe proof is deferred to Appendix B.3. The results show a separation between MLE and pessimistic MLE when the concentratability coefficient is bounded. The failure of MLE has also been empirically observed in Gao et al. (2022), which leads to overoptimization with the trained reward model.\n\nWe also show that for the problems with bounded concentratability coefficient, pessimistic MLE is minimax-rate optimal up to a constant factor. Consider the family of contextual bandit instances as follows:\nCB(\u039b) = {\u03c1, {(s i , a i1 , a i2 )} n i=1 , \u03b8 \u22c6 | \u2225\u03a3 \u22121/2 D E s\u223c\u03c1 [\u03d5(s, \u03c0 \u22c6 (s))]\u2225 2 \u2264 \u039b}.\nHere we assume that \u03a3 D is invertible to simplify the presentation of the lower bound. For any Q \u2208 CB(\u039b), we let SubOpt Q (\u03c0) be the sub-optimality under instance Q. We have the following lower bound result, the proof of which is deferred to Appendix B.4.\n\nTheorem 3.10. For any d > 6, n \u2265 Cd\u039b 2 , \u039b \u2265 2, there exists a feature mapping \u03d5 such that the following lower bound holds.\ninf \u03c0 sup Q\u2208CB(\u039b) SubOpt Q (\u03c0) \u2265 C\u039b \u00b7 d n .\nComparing with the upper bound in Theorem 3.2, we see that the pessimistic MLE is minimax-optimal up to constant factors for the sub-optimality of induced policy.\n\n\nLearning from K-wise comparisons\n\nWe now consider learning from K-wise comparisons under the PL model. In this case, we design two different estimators based on MLE. One involves directly maximizing the likelihood under the PL model, denoted as MLE K . The other involves splitting the K-wise comparison data with pairwise comparisons and running MLE for pairwise comparisons. We denote this estimator as MLE 2 .\n\n\nAlgorithms\nGuarantee for MLE K . Let D = {(s i , a i 0 , \u00b7 \u00b7 \u00b7 , a i K )} n i=1\nbe the set of queried states and actions, and the permutation function \u03c3 i be the output of the i-th query. We can compute its maximum likelihood estimator as\u03b8\nMLE K \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 1 n n i=1 K\u22121 j=0 log exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(j) )\u27e9) K\u22121 k=j exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k) )\u27e9)\n. Similar to Shah et al. (2015), we restrict our attention to K = O(1) since it is known that it is difficult for human to compare more than a small number of items due to a limited information storage and processing capacity (Miller, 1956;Kiger, 1984;Shiffrin and Nosofsky, 1994;Saaty and Ozdemir, 2003). For instance, Saaty and Ozdemir (2003) recommend eliciting preferences over no more than seven options. We have the following result for K-wise comparisons.\n\nTheorem 4.1. Under the K-wise PL model, for any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE K \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 K 4 (d + log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 . Here \u03a3 D = 2 K(K\u22121)n ( n i=1 K\u22121 j=0 K\u22121 k=j+1 (\u03d5(s i , a i j )\u2212\u03d5(s i , a i k ))(\u03d5(s i , a i j )\u2212\u03d5(s i , a i k )) \u22a4 )\n, and \u03b3 = exp(\u22124LB). As a consequence, let\u03c0 PE K be the output of Algorithm 1 when taking\u03b8 =\u03b8 MLE K , f (n, d, \u03b4, \u03bb) = C \u00b7 K 4 (d+log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 . For any \u03bb > 0 and v \u2208 R d , with probability at least 1 \u2212 \u03b4,\nSubOpt(\u03c0 PE K ) \u2264 C \u00b7 K 4 (d + log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 \u00b7 \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223c\u03c1 [(\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)]\u2225 2 .\nThe proof of Theorem 4.1 is provided in Appendix B.5. Shah et al. (2015) also study the extension from pairwise to K-wise comparisons. However, they focus on the setting where only the maximum is selected, where we assume a complete ranking among K items is given. Also, they only provide an expectation bound while we provide a high-probability bound.\n\nCompared to the pairwise comparison result in Theorem 3.2, the covariance matrix \u03a3 D now takes the sum over the feature differences between all pairs of actions among K-wise comparisons. As a cost, the right-hand side bound also introduces extra dependence on K. Our bound is likely to be loose in terms of the dependence on K. However, since we mainly focus on the case of K = O(1), such a bound is still near-optimal due to the minimax lower bound for pairwise comparisons. Furthermore, the gap between MLE and pessimistic MLE for sub-optimality still exists since Theorem 3.9 holds as a special case of K-wise comparison.\n\nGuarantee for MLE 2 Besides the standard MLE approach, another option is to replace the joint distribution of K-ranking data with K(K \u2212 1)/2 pairs of pairwise comparisons. This can be understood as replacing the true probability in MLE K with the product of marginals:\n\u03b8 MLE2 \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 1 n n i=1 K\u22121 j=0 K\u22121 k=j+1 log exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(j) )\u27e9) exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(j) )\u27e9) + exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k) )\u27e9)\n.\n\nThis estimator is also applied in the current RLHF for LLM (see, e.g., Ouyang et al., 2022). We show that it also leads to a good induced policy, as is shown in the theorem below.\n\nTheorem 4.2. Under the K-wise PL model, for any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE2 \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 . Here \u03a3 D = 2 K(K\u22121)n ( n i=1 K\u22121 j=0 K\u22121 k=j+1 (\u03d5(s i , a i j ) \u2212 \u03d5(s i , a i k ))(\u03d5(s i , a i j ) \u2212 \u03d5(s i , a i k )) \u22a4 )\n, and \u03b3 = 1/(2 + exp(\u22122LB) + exp(2LB)). As a consequence, let\u03c0 PE2 be the output of Algorithm 1 when taking\u03b8 =\u03b8 MLE2 , f (n, d, \u03b4, \u03bb) = C \u00b7 d+log(1/\u03b4) \u03b3 2 n + \u03bbB 2 , q = \u03c1. For any \u03bb > 0 and v \u2208 R d , with probability at least 1 \u2212 \u03b4,\nSubOpt(\u03c0 PE2 ) \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 \u00b7 \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223c\u03c1 [(\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)]\u2225 2 .\nThe proof of Theorem 4.2 is provided in Appendix B.6. Our theoretical analysis validates the empirical performance of MLE 2 in Ouyang et al. (2022). Compared to the guarantee for MLE K , MLE 2 seems to has better nonasymptotic upper bound in terms of the dependence on K. However, it is likely that this comes from a loose analysis of MLE K . The MLE 2 belongs to the family of the M-estimators, whose asymptotic variance is known to be larger than that of MLE Godambe (1960); Lee (2008). Thus, asymptotically, MLE K is more efficient than MLE 2 . We can calculate the asymptotic variance of both estimators as follows:\nTheorem 4.3. We have \u221a n(\u03b8 MLE K \u2212 \u03b8 \u22c6 ) \u2192 N (0, I(\u03b8 \u22c6 ) \u22121 ); \u221a n(\u03b8 MLE2 \u2212 \u03b8 \u22c6 ) \u2192 N (0, V ). where I(\u03b8 \u22c6 ) = E \u03b8 \u22c6 K\u22121 j=0 K\u22121 k=j K\u22121 k \u2032 =j exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i \u03c3i(k) ) + \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9) ( K\u22121 k \u2032 =j exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9)) 2 \u00b7 (\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) ))(\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) )) \u22a4 , V = \u03a3 \u22121 E \u03b8 \u22c6 GG \u22a4 \u03a3 \u22121 , \u03a3 = E \u03b8 \u22c6 \uf8ee \uf8f0 K\u22121 j=0 K\u22121 k=j exp(\u2212\u27e8\u03b8 \u22c6 , x i \u03c3i(j)\u03c3i(k) )\u27e9 (1 + exp(\u2212\u27e8\u03b8 \u22c6 , x i \u03c3i(j)\u03c3i(k) )\u27e9) 2 \u00b7 \u03d5(s i , a i \u03c3i(j) ) \u2212 \u03d5(s i , a i \u03c3i(k) ) \u03d5(s i , a i \u03c3i(j) ) \u2212 \u03d5(s i , a i \u03c3i(k) ) \u22a4 \uf8f9 \uf8fb , G = K\u22121 j=0 K\u22121 k=j+1 exp(\u2212\u27e8\u03b8 \u22c6 , x i \u03c3i(j)\u03c3i(k) )\u27e9 1 + exp(\u2212\u27e8\u03b8 \u22c6 , x i \u03c3i(j)\u03c3i(k) )\u27e9 \u00b7 \u03d5(s i , a i \u03c3i(j) ) \u2212 \u03d5(s i , a i \u03c3i(k) )\nThe proof follows directly the gradient and Hessian computed in Appendix B.5 and B.6, combined with Van der Vaart (2000, Section 5.3). We also empirically verify the performances of both estimators in Section 7.\n\n\nExtension to MDPs\n\nThus far we have considered only contextual bandits. We now extend our results to the MDP setting. Depending on whether the comparison is based on a single action or a whole trajectory, we have two regimes, namely action-based comparison and trajectory-based comparison.\n\n\nTrajectory-based Comparison\n\nIn trajectory-based comparison, we assume that two trajectories that start from the same initial state are given, and the comparison is based on the cumulative reward of the two trajectories. Concretely, we first sample the initial state s 0 from some fixed distribution \u03c1, and then sample two trajectories \u03c4 0 = (a 0 , s 1 , a 1 , \u00b7 \u00b7 \u00b7 , s H , a H ) and \u03c4 1 = (a \u2032 0 , s \u2032 1 , a \u2032 1 , \u00b7 \u00b7 \u00b7 , s \u2032 H , a \u2032 H ) from joint distributions P l (a 0 , s 1 , a 1 , \u00b7 \u00b7 \u00b7 , s H , a H |s 0 ) = i \u03c0 l (a i |s i )P (s i+1 |s i , a i ), where l \u2208 {0, 1}. For each queried state-trajectory pair, we observe a sample y from a Bernoulli distribution as follows:\nP(y = 1 | s, \u03c4 0 , \u03c4 1 ) = exp( H h=0 r \u03b8 \u22c6 (s h , a h )) exp( H h=0 r \u03b8 \u22c6 (s h , a h )) + exp( H h=0 r \u03b8 \u22c6 (s \u2032 h , a \u2032 h )))\n.\nGiven the dataset {(s i , \u03c4 i 0 , \u03c4 i 1 , y i } n i=1 , the MLE i\u015d \u03b8 MLE \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 n i=1 log 1(y i = 1) \u00b7 exp( H h=0 r \u03b8 (s i h , a i h )) exp( H h=0 r \u03b8 (s i h , a i h )) + exp( H h=0 r \u03b8 (s i\u2032 h , a i\u2032 h )) + 1(y i = 0) \u00b7 exp( H h=0 r \u03b8 (s i\u2032 h , a i\u2032 h )) exp( H h=0 r \u03b8 (s i h , a i h )) + exp( H h=0 r \u03b8 (s i\u2032 h , a i\u2032 h ))\n.\n\nCompared to the pairwise comparison in the contextual bandit, the exponent changes from a single reward to the cumulative reward. Similarly, we provide the following guarantee for the estimation error of MLE:\n\nLemma 5.1. Assume that \u2225\u03d5(\u00b7, \u00b7)\u2225 \u221e \u2264 L for any s, a. Then for any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 d log(1/\u03b4) \u03b3 2 n + \u03bbB 2 .\nHere\n\u03a3 D = 1 n n i=1 ( H h=0 (\u03d5(s i h , a i h ) \u2212 \u03d5(s i\u2032 h , a i\u2032 h ))) ( H h=0 (\u03d5(s i h , a i h ) \u2212 \u03d5(s i\u2032 h , a i\u2032h\n))) \u22a4 , and \u03b3 = 1/(2 + exp(\u22122HLB) + exp(2HLB)).\n\nThe proof is deferred to Appendix B.7. Compared to the guarantee for contextual bandits in Lemma 3.1, the features in the covariance is now the difference between the cumulative feature in trajectory \u03c4 and the cumulative feature in trajectory \u03c4 \u2032 . The result reduces to Lemma 3.1 when H = 1.\n\nIn order to bound the sub-optimality of the induced policy, one needs to plug-in a pessimistic version of the reward estimate. Note that from the definition of d \u03c0 , one has\nE s\u223c\u03c1 [V \u03c0 (s)] = E s,a\u223cd \u03c0 [r(s, a)].\nIn the case when the transition distribution P is known, one may directly compute d \u03c0 for any policy \u03c0 and replace the initial distribution \u03c1 in the algorithm for contextual bandit. This gives the following result:\n\nTheorem 5.2. Let\u03c0 PE be the output of Algorithm 1 when taking\u03b8 =\u03b8 MLE , f (n, d, \u03b4, \u03bb) = C \u00b7 d+log(1/\u03b4) \u03b3 2 n + \u03bbB 2 , q = d \u03c0 . For any \u03bb > 0 and v \u2208 R d , with probability at least 1 \u2212 \u03b4,\nSubOpt(\u03c0 PE ) \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 \u00b7 \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223cd \u03c0 \u22c6 [(\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)]\u2225 2 .\nThe proof is deferred to Appendix B.8. The result can be generalized to the case of K-wise comparisons following the same argument in Section 4.\n\n\nAction-based Comparison\n\nIn action-based comparison, we assume that two actions are sampled for each state, and the comparison is based on the expected cumulative return starting from such state-action pair.\n\nConcretely, assume that the optimal Q-function is parameterized as Q \u22c6 \u03b8 (s, a) = \u03b8 \u22a4 \u03d5(s, a) for some given \u03d5(s, a). Let \u03b8 \u22c6 be the true parameter. During the training, we first sample the state s from some fixed distribution \u03c1, and then sample a pair of actions a 0 , a 1 from a joint distribution P (a 0 , a 1 |s). For each queried state-actions pair (s, a 0 , a 1 ), we observe a sample y from a Bernoulli distribution with parameter exp(Q \u03b8 \u22c6 (s,a1)) exp(Q \u03b8 \u22c6 (s,a0))+exp(Q \u03b8 \u22c6 (s,a1)) , i.e. P(y = 1 | s, a 0 , a 1 ) = exp(Q \u03b8 \u22c6 (s, a 1 )) exp(Q \u03b8 \u22c6 (s, a 0 )) + exp(Q \u03b8 \u22c6 (s, a 1 )) and P(y = 0 | s, a 0 , a 1 ) = exp(Q \u03b8 \u22c6 (s, a 0 )) exp(Q \u03b8 \u22c6 (s, a 0 )) + exp(Q \u03b8 \u22c6 (s, a 1 )) .\n\nIn this case, one may use the same MLE to estimate \u03b8 \u22c6 , which results in an estimatorQ for the Q \u22c6 -function.\n\nThe following lemma follows exactly the same analysis as Lemma 3.1:\n\nLemma 5.3. Under the BTL model for action-based RLHF, for any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 .\nHere \u03b3 = 1/(2 + exp(\u2212LB) + exp(LB)).\n\u03a3 D = 1 n n i=1 (\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 ))(\u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 )) \u22a4 .\nWhen \u03a3 D is invertible and covers all the directions well, this will lead to a valid confidence bound for Q \u22c6 , which implies a good performance of the induced greedy policy without pessimism. However, when \u03a3 D does not provide good coverage, introducing pessimism in this case can be hard. The reason is that one needs to construct lower confidence bound for Q \u03c0 for any \u03c0. However, given such confidence bound of \u03b8 MLE , one can only construct confidence bound for Q \u22c6 .\n\n\nConnection with Inverse Reinforcement Learning\n\nIn Inverse Reinforcement Learning (IRL), BTL and PL model are also popular model of human behavior. However, in IRL it is assumed that we only observe the human behavior, which is sampled from the distribution under PL model. Thus no comparison is queried. Depending on the comparison is action-based on trajectory-based, one has max-entropy IRL or action-based IRL, discussed in details below.\n\n\nTrajectory-based IRL\n\nIn max-entropy IRL Ziebart et al. (2008), it is also assumed that the human selection of trajectory follows a PL model. A common assumption in IRL or IL is that the observed trajectory collected by human behavior is likely to be the optimal policy. Assumee that the transitions are deterministic. For any trajectory \u03c4 = (s 0 , a 0 , \u00b7 \u00b7 \u00b7 , s H , a H ), it is assumed that the expert chooses trajectory \u03c4 under the following model:\nP(\u03c4 ) = exp( H h=0 \u27e8\u03b8 \u22c6 , \u03d5(s h , a h )\u27e9) \u03c4 \u2032 \u2208T (s0) exp( H h=0 \u27e8\u03b8 \u22c6 , \u03d5(s \u2032 h , a \u2032 h )\u27e9)\n.\n\nHere the set T (s 0 ) denotes the set for all possible trajectories that start from s 0 . Each trajectory is represented by\n\u03c4 \u2032 = {(s \u2032 h , a \u2032 h )} H h=1 .\nAssume that we are given a set of trajectories {s i h , a i h } i\u2208[n],h\u2208[H] that are sampled from the distribution P(\u03c4 ). When the denominator can be computed exactly, the algorithm of max entropy IRL also reduces to the MLE, which can be written a\u015d\n\u03b8 MLE \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 1 n n i=1 log exp( H h=0 \u27e8\u03b8, \u03d5(s i h , a i h )\u27e9) \u03c4 \u2032 \u2208T (s i 0 ) exp( H h=0 \u27e8\u03b8, \u03d5(s \u2032 h , a \u2032 h )\u27e9)\n.\n\nAlthough the enumeration of all trajectories T (s i 0 ) is not possible due to exponential growth of the possible trajectories with respect to horizon H, Ziebart et al. (2008) provides an alternative way of computing the gradient via calculating the expected state frequency. This enables the efficient implementation of MLE. One can show the performance guarantee for max entropy IRL as follows:\n\nLemma 6.1. Under the PL model, for any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 sup s |T (s)| 2 \u00b7 (d + log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 . Here \u03a3 D = 1 n sup s |T (s)| 2 n i=1 {(s h ,a h )}\u2208T (s i 0 ) {(s \u2032 h ,a \u2032 h )}\u2208T (s i 0 ) ( H h=0 (\u03d5(s h , a h )\u2212\u03d5(s \u2032 h , a \u2032 h )))( H h=0 (\u03d5(s h , a h )\u2212 \u03d5(s \u2032 h , a \u2032h\n))) \u22a4 , and \u03b3 = exp(\u22124LB)/2. Given such guarantee for MLE, we also show that IRL, when combined with pessimism principle, will lead to a good policy.\n\nTheorem 6.2. Let\u03c0 PE be the output of Algorithm 1 when taking\u03b8 =\u03b8 MLE , f (n, d, \u03b4, \u03bb) = C \u00b7 sup s |T (s)|(d+log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 , q = d \u03c0 . For any \u03bb > 0 and v \u2208 R d , with probability at least 1 \u2212 \u03b4,\nSubOpt(\u03c0 PE ) \u2264 C \u00b7 sup s |T (s)| 2 (d + log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 \u00b7 \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223c\u03c1 [(\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)]\u2225 2 .\nThe proof of Lemma 6.1 and Theorem 6.2 is provided in Appendix B.9. For IRL we have the dependence of sup s |T (s)| in our bound, which can be much larger than d. Similar to the case of K-wise comparison, one may also split the one observation into sup s |T (s)| pairwise comparisons, which can help improve the dependence on sup s |T (s)| in the current analysis.\n\n\nAction-based IRL\n\nSimilar to action-based RLHF, action-based IRL also models human choice based on Q \u22c6 instead of cumulative reward Ramachandran and Amir (2007) .\n\nHere the denominator takes all possible actions. Unlike RLHF where a pair of actions are observed, in IRL or IL, only a single human behavior is observed in each round and there is no comparison, i.e. the observed actions a are sampled from \u03c0 \u22c6 (a | s). Given such observation, one can still run MLE and gives similar performance guarantee. In particular, the MLE is given b\u0177\n\u03b8 MLE \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 1 n n i=1 log exp(\u27e8\u03b8, \u03d5(s i , a i )\u27e9) a \u2032 \u2208A exp(\u27e8\u03b8, \u03d5(s i , a \u2032 )\u27e9)\n.\n\nThe following lemma follows a similar analysis as Lemma 3.1 and Lemma 6.1: Lemma 6.3. Under the PL model for action-based IRL, for any \u03bb > 0, with probability at least 1 \u2212 \u03b4, \n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 |A| 2 (d + log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 . Here \u03a3 D = 1 n|A| 2 n i=1 a\u2208A a \u2032 \u2208A (\u03d5(s i , a) \u2212 \u03d5(s i , a \u2032 ))(\u03d5(s i , a) \u2212 \u03d5(s i ,a\n\nExperiments\n\nThere has been a large amount of empirical work that demonstrates the success of MLE and pessimistic MLE in RLHF for game playing (Knox and Stone, 2008  In this section, we provide experiments for the contextual bandit case. In particular, we conduct both MLE and pessimistic MLE on the example constructed in Appendix B.3. The results are included in Fig. 1. We range the number of samples n from 10 to 500. Each sample size is repeated 100 times. The result verifies our theoretical analysis: MLE converges under the semi-norm but fails to give good policy. On the other hand, pessimistic MLE gives vanishing rate when considering the sub-optimality of the induced policy. Note that in the left figure we do not include pessimistic MLE, since both MLE and pessimistic MLE rely on the same parameter\u03b8 MLE , and they only defer in how the induced policy is trained.\n\nOn the other hand, we compare the performance of MLE 2 and MLE K when learning from K-wise comparisons. We take K = 4 and K = 9, and range samples from 10 to 500. We randomly generate \u03d5 and \u03b8 \u22c6 as independent samples from 3-dimensional Gaussian distribution. The result is shown in Figure 2. One can see that as n grows larger, both estimators converge, while MLE K has smaller estimation error than MLE 2 . The gap grows larger when K becomes larger. This is consistent with our theoretical prediction in Section 4: since MLE K is the true MLE and MLE 2 belongs to the family of M-estimators, asymptotically MLE K shall be more efficient than MLE 2 . Figure 2: The comparison of estimation error between MLE 2 and MLE K , with K = 4 in the left and K = 9 in the right.\n\n\nConclusion\n\nWe have provided a theoretical analysis of the sample complexity of RLHF. Our main results involve two insights: (i) pessimism is important to guarantee a good policy; (ii) in K-wise comparison, both MLE K and MLE 2 converge. Moreover, MLE K is asymptotically more efficient.\n\nWhile we have made progress in understanding the reward learning aspect of RLHF, there are many additional questions that remain to be answered.\n\n1. We assumed that the policy trained is greedy with respect to the learned reward. However, in practice the reward is mostly used to fine-tune the pre-trained policy. This requires a more extensive theory that considers the whole procedure of pre-training the policy, learning a reward model and then fine-tuning the policy with policy gradient or PPO.\n\n2. Although we focused on the BTL and PL models, there have been a number of other models considered for the modeling of human behavior, including the Thurstone model and cardinal models. It would be interesting to extend our analysis to cover these additional models and begin to provide a general characterization of behavioral models for RLHF.\n\n3. Our constructed confidence bound is based on a fixed feature \u03d5. In the practical fine-tuning scenario, \u03d5 is not fixed but may change slowly. It is interesting to see how the constructed confidence bound helps in the practical fine-tuning scenario for online (active) learning or offline learning, and how one can design valid confidence bound for slowly changing \u03d5.\n\n\nReferences\n\nY. Abbasi-Yadkori, D. P\u00e1l, and C. Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.  A Analysis for nonlinear r \u03b8\n\nConsider the case of pairwise comparison when r \u03b8 is not linear, the MLE can be written a\u015d\n\u03b8 MLE \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 n i=1 log 1(y i = 1) \u00b7 exp(r \u03b8 (s i , a i 1 )) exp(r \u03b8 (s i , a i 0 )) + exp(r \u03b8 (s i , a i 1 )) + 1(y i = 0) \u00b7 exp(r \u03b8 (s i , a i 0 )) exp(r \u03b8 (s i , a i 0 )) + exp(r \u03b8 (s i , a i 1 ))\n.\n\nHere we provide a guarantee for the case when r \u03b8 is nonlinear and non-convex. We first make the following boundedness and smoothness assumption on r \u03b8 :\n\nAssumption A.1. Assume that for any \u03b8 \u2208 \u0398 B , s \u2208 S, a 0 \u2208 A, a 1 \u2208 A with a 0 \u0338 = a 1 , we have,\n|r \u03b8 (s, a)| \u2264 \u03b1 0 , (Bounded value) \u2225\u2207r \u03b8 (s, a)\u2225 2 \u2264 \u03b1 1 , (Bounded gradient) \u2225\u2207 2 r \u03b8 (s, a)\u2225 2 \u2264 \u03b1 2 . (Bounded Hessian / Lipschitz gradient)\nOne can verify that our linear reward satisfies the above assumption with \u03b1 0 = LB, \u03b1 1 = L, \u03b1 2 = 0. Under this assumption, we have Theorem A.2. For any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + (\u03bb + \u03b1 2 /\u03b3 + \u03b1 1 \u03b1 2 B)B 2 . Here \u03b3 = 1 2+exp(\u22122\u03b10)+exp(2\u03b10) , \u03a3 D = 1 n n i=1 \u2207(r \u03b8 \u22c6 (s i , a i 1 ) \u2212 r \u03b8 \u22c6 (s i , a i 0 ))\u2207(r \u03b8 \u22c6 (s i , a i 1 ) \u2212 r \u03b8 \u22c6 (s i , a i 0 )) \u22a4 .\nThe proof is deferred to Appendix B.10. Our result recovers Lemma 3.1 when \u03b1 2 = 0 and reveals how the gradient of r plays a role in the bound for estimation error. However, the dependence on \u03b1 2 will not vanish as n \u2192 \u221e. It remains open how to get vanishing rate for nonlinear reward functions when \u03b1 2 > 0. Similar argument can also be applied to the case of K-wise comparison and MDP. And we can similarly design pessimistic MLE based on the confidence bound on\u03b8 MLE .\n\nOn the other hand, we can show that the true parameter \u03b8 \u22c6 is a global minimum of the population negative log likelihood even when r \u03b8 is nonlinear and we use MLE 2 for K-wise comparison. Recall that the MLE 2 splits K-wise comparisons into pairwise comparisons, and is given b\u0177\n\u03b8 MLE2 \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 1 n n i=1 K\u22121 j=0 K\u22121 k=j+1 log exp(r \u03b8 (s i , a i \u03c3i(j) )) exp(r \u03b8 (s i , a i \u03c3i(j) )) + exp(r \u03b8 (s i , a i \u03c3i(k) ))\n.\n\nWhen there is infinite number of data, the loss become\nE[\u2113(\u03b8)] = \u2212 s \u03c1(s) a0,a1\u2208A\n\u03c1(a 0 , a 1 | s) \u00b7 exp(r \u03b8 \u22c6 (s, a 0 )) exp(r \u03b8 \u22c6 (s, a 0 )) + exp(r \u03b8 \u22c6 (s, a 1 )) log exp(r \u03b8 (s, a 0 )) exp(r \u03b8 (s, a 0 )) + exp(r \u03b8 (s, a 1 )) + exp(r \u03b8 \u22c6 (s, a 1 )) exp(r \u03b8 \u22c6 (s, a 0 )) + exp(r \u03b8 \u22c6 (s, a 1 )) log exp(r \u03b8 (s, a 1 )) exp(r \u03b8 (s, a 0 )) + exp(r \u03b8 (s, a 1 )) .\n\nHere \u03c1(a 0 , a 1 | s) is the probability that actions a 0 , a 1 are included in the K-comparison when the state is s. Now we show\n\u03b8 \u22c6 \u2208 arg min \u03b8 E[\u2113(\u03b8)].(1)\nTo see this, note that we have\nE[\u2113(\u03b8)] = \u2212 s \u03c1(s) a0,a1\u2208A\n\u03c1(a 0 , a 1 | s) \u00b7 exp(r \u03b8 \u22c6 (s, a 0 )) exp(r \u03b8 \u22c6 (s, a 0 )) + exp(r \u03b8 \u22c6 (s, a 1 )) log exp(r \u03b8 (s, a 0 )) exp(r \u03b8 (s, a 0 )) + exp(r \u03b8 (s, a 1 )) + exp(r \u03b8 \u22c6 (s, a 1 )) exp(r \u03b8 \u22c6 (s, a 0 )) + exp(r \u03b8 \u22c6 (s, a 1 )) log exp(r \u03b8 (s, a 1 )) exp(r \u03b8 (s, a 0 )) + exp(r \u03b8 (s, a 1 )) = s \u03c1(s) a0,a1\u2208A p(a 0 , a 1 | s) \u00b7 (H(p \u03b8 \u22c6 (s, a 0 , a 1 )) + KL(p \u03b8 \u22c6 (s, a 0 , a 1 )\u2225p \u03b8 (s, a 0 , a 1 ))).\n\nHere H(p) = p log(1/p) + (1 \u2212 p) log(1/(1 \u2212 p)) is the entropy of a Bernoulli distribution with parameter p. And p \u03b8 (s, a 0 , a 1 ) = exp(r \u03b8 (s,a1)) exp(r \u03b8 (s,a0))+exp(r \u03b8 (s,a1)) . Now note that KL is lower bounded by 0, with equality when \u03b8 = \u03b8 \u22c6 . This proves Equation (1).\n\n\nB Remaining Proofs\n\n\nB.1 Proof of Lemma 3.1\n\nRecall that the MLE is given b\u0177\n\u03b8 MLE \u2208 arg min \u03b8\u2208\u0398B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 n i=1 log 1(y i = 1) \u00b7 exp(r \u03b8 (s i , a i 1 )) exp(r \u03b8 (s i , a i 0 )) + exp(r \u03b8 (s i , a i 1 )) + 1(y i = 0) \u00b7 exp(r \u03b8 (s i , a i 0 )) exp(r \u03b8 (s i , a i 0 )) + exp(r \u03b8 (s i , a i 1 )) = \u2212 n i=1 log 1(y i = 1) \u00b7 1 1 + exp(r \u03b8 (s i , a i 0 ) \u2212 r \u03b8 (s i , a i 1 )) + 1(y i = 0) \u00b7 1 \u2212 1 1 + exp(r \u03b8 (s i , a i 0 ) \u2212 r \u03b8 (s i , a i 1 )) = \u2212 n i=1\nlog 1(y i = 1) \u00b7 1 1 + exp(\u03b8 \u22a4 (\u03d5(s i , a i 0 ) \u2212 \u03d5(s i , a i 1 ))) + 1(y i = 0) \u00b7 1 \u2212 1 1 + exp(\u03b8 \u22a4 (\u03d5(s i , a i 0 ) \u2212 \u03d5(s i , a i 1 )))\n\nTo simplify the notation, we let x i = \u03d5(s i , a i 1 ) \u2212 \u03d5(s i , a i 0 ). Our goal is to bound the estimation error of the MLE in the squared semi-norm \u2225v\u2225 2 \u03a3 D +\u03bbI = v \u22a4 (\u03a3 D + \u03bbI)v.\n\nStrong convexity of \u2113. We first show that \u2113 D is strongly convex at \u03b8 \u22c6 with respect to the semi-norm \u2225 \u00b7 \u2225 \u03a3 D , meaning that there is some constant \u03b3 > 0 such that\n\u2113 D (\u03b8 \u22c6 + \u2206) \u2212 \u2113 D (\u03b8 \u22c6 ) \u2212 \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 \u2265 \u03b3\u2225\u2206\u2225 2 \u03a3 D(2)\nfor all perturbations \u2206 \u2208 R d such that \u03b8 \u22c6 + \u2206 \u2208 \u0398 B . One can directly calculate the Hessian of \u2113 as\n\u2207 2 \u2113 D (\u03b8) = 1 n n i=1 1(y i = 1) \u00b7 exp(\u2212\u27e8\u03b8, x i \u27e9) (exp(\u2212\u27e8\u03b8, x i \u27e9) + 1) 2 + 1(y i = 0) \u00b7 exp(\u27e8\u03b8, x i \u27e9) (exp(\u27e8\u03b8, x i \u27e9) + 1) 2 \u00b7 x i x \u22a4 i = 1 n n i=1 exp(\u2212\u27e8\u03b8, x i \u27e9) (exp(\u2212\u27e8\u03b8, x i \u27e9) + 1) 2 \u00b7 x i x \u22a4 i Observe that \u27e8\u03b8, x i \u27e9 \u2208 [\u22122LB, 2LB], which gives that exp(\u2212\u27e8\u03b8, x i \u27e9) (exp(\u2212\u27e8\u03b8, x i \u27e9) + 1) 2 \u2265 1 2 + exp(\u22122LB) + exp(2LB)\n.\n\nPutting together the pieces, we conclude that\nv \u22a4 \u2207 2 \u2113 D (\u03b8)v \u2265 \u03b3 n \u2225Xv\u2225 2\nwhere \u03b3 = 1/(2 + exp(\u22122LB) + exp(2LB)), X \u2208 R n\u00d7d has the differencing vector x i \u2208 R d as its i th row. Thus, if we introduce the error vector \u2206 :=\u03b8 MLE \u2212 \u03b8 \u22c6 , then we may conclude that\n\u2113 D (\u03b8 \u22c6 + \u2206) \u2212 \u2113 D (\u03b8 \u22c6 ) \u2212 \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 \u2265 \u03b3 n \u2225X\u2206\u2225 2 2 = \u03b3\u2225\u2206\u2225 2 \u03a3 D ,\nshowing that \u2113 D is strongly convex around \u03b8 \u22c6 with parameter \u03b3.\n\nBounding the estimation error. Now we aim at bounding the estimation error \u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D . Since\u03b8 MLE is optimal for \u2113 D , we have \u2113 D (\u03b8 MLE ) \u2264 \u2113 D (\u03b8 \u22c6 ). (When\u03b8 MLE is approximately optimal, i.e. \u2113 D (\u03b8 MLE ) \u2264 min \u03b8 \u2113 D (\u03b8) + \u03f5, the same argument also holds up to an extra additive term \u03f5.) Defining the error vector \u2206 =\u03b8 MLE \u2212 \u03b8 \u22c6 , adding and subtracting the quantity \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 yields the bound\n\u2113 D (\u03b8 \u22c6 + \u2206) \u2212 \u2113 D (\u03b8 \u22c6 ) \u2212 \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 \u2264 \u2212\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9.\nBy the \u03b3-convexity condition, the left-hand side is lower bounded by \u03b3\u2225\u2206\u2225 2 \u03a3 D . As for the right-hand side, note that |\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9| \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI for any \u03bb > 0. Altogether we have\n\u03b3\u2225\u2206\u2225 2 \u03a3 D \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI .\nNow we further bound the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 . Observe that the gradient takes the form\n\u2207\u2113 D (\u03b8 \u22c6 ) = \u22121 n n i=1 1[y i = 1] exp(\u2212\u27e8\u03b8 \u22c6 , x i \u27e9) 1 + exp(\u2212\u27e8\u03b8 \u22c6 , x i \u27e9)) \u2212 1[y i = 0] 1 1 + exp(\u2212\u27e8\u03b8 \u22c6 , x i \u27e9)) x i .\nDefine a random vector V \u2208 R n with independent components as\nV i = exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9) 1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) w.p. 1 1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) \u22121 1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) w.p. exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)\n1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) .\n\nWith this notation, we have \u2207\u2113 D (\u03b8 \u22c6 ) = \u2212 1 n X \u22a4 V . One can verify that E[V ] = 0 and |V i | \u2264 1. Defining the n-dimensional square matrix M := 1 n 2 X(\u03a3 D + \u03bbI) \u22121 X \u22a4 , we have \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 = V \u22a4 M V . Let the eigenvalue decomposition of X \u22a4 X be X \u22a4 X = U \u039bU \u22a4 . We can bound the trace and operator norm of M as\nTr(M ) = 1 n 2 Tr(U (\u039b/n + \u03bbI) \u22121 U \u22a4 U \u039bU \u22a4 ) \u2264 d n Tr(M 2 ) = 1 n 4 Tr(U (\u039b/n + \u03bbI) \u22121 U \u22a4 U \u039bU \u22a4 U (\u039b/n + \u03bbI) \u22121 U \u22a4 U \u039bU \u22a4 ) \u2264 d n 2 \u2225M \u2225 op = \u03bb max (M ) \u2264 1 n ,\nMoreover, since the components of V are independent and of zero mean, and |V i | \u2264 1, the variables are 1-sub-Gaussian, and hence the Bernstein's inequality for sub-Gaussian random variables in quadratic form (see e.g. Hsu et al. (2012, Theorem 2.1)) implies that with probability at least 1 \u2212 \u03b4,\n\u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 = V \u22a4 M V \u2264 C 1 \u00b7 d + log(1/\u03b4) n .\nHere C 1 is some universal constant. This gives us\n\u03b3\u2225\u2206\u2225 2 \u03a3 D +\u03bbI \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI + 4\u03bb\u03b3B 2 \u2264 C 1 \u00b7 d + log(1/\u03b4) n \u2225\u2206\u2225 \u03a3 D +\u03bbI + 4\u03bb\u03b3B 2 .\nSolving the above inequality gives us that for some constant C 2 , \u2225\u2206\u2225 \u03a3 D +\u03bbI \u2264 C 2 \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 .\n\n\nB.2 Proof of Theorem 3.2\n\nProof. Let J \u2032 (\u03c0) = J(\u03c0) \u2212 \u27e8\u03b8 \u22c6 , v\u27e9. We have\nSubOpt(\u03c0 PE ) = J(\u03c0 \u22c6 ) \u2212 J(\u03c0 PE ) = J \u2032 (\u03c0 \u22c6 ) \u2212 J \u2032 (\u03c0 PE ) = (J \u2032 (\u03c0 \u22c6 ) \u2212\u0134(\u03c0 \u22c6 )) + (\u0134(\u03c0 \u22c6 ) \u2212\u0134(\u03c0 PE )) + (\u0134(\u03c0 PE ) \u2212 J \u2032 (\u03c0 PE )).\nSince\u03c0 PE is the optimal policy under expected value J \u2032 (\u03c0), we know that the second difference satisfie\u015d J(\u03c0 \u22c6 ) \u2212\u0134(\u03c0 PE ) \u2264 0. For the third difference, we hav\u00ea\nJ(\u03c0 PE ) \u2212 J \u2032 (\u03c0 PE ) = min \u03b8\u2208\u0398(\u03b8 MLE ,\u03bb) E s\u223c\u03c1 [\u03b8 \u22a4 (\u03d5(s, \u03c0(s)) \u2212 v)] \u2212 E s\u223c\u03c1 [\u03b8 \u22c6\u22a4 (\u03d5(s, \u03c0(s)) \u2212 v)].\nFrom Lemma 3.1 we know that \u03b8 \u22c6 \u2208 \u0398(\u03b8 MLE , \u03bb) with probability at least 1 \u2212 \u03b4. Thus we know that with probability at least 1 \u2212 \u03b4,\u0134(\u03c0 PE ) \u2212 J \u2032 (\u03c0 PE ) \u2264 0. Now combining everything together and condition on the above event, we have\nSubOpt(\u03c0 PE ) \u2264 J \u2032 (\u03c0 \u22c6 ) \u2212\u0134(\u03c0 \u22c6 ) = sup \u03b8\u2208\u0398(\u03b8 MLE ,\u03bb) E s\u223c\u03c1 [(\u03b8 \u22c6 \u2212 \u03b8) \u22a4 (\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)] = sup \u03b8\u2208\u0398(\u03b8 MLE ,\u03bb) E s\u223c\u03c1 [(\u03b8 \u22c6 \u2212\u03b8 MLE +\u03b8 MLE \u2212 \u03b8) \u22a4 (\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)] = E s\u223c\u03c1 [(\u03b8 \u22c6 \u2212\u03b8 MLE ) \u22a4 (\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)] + sup \u03b8\u2208\u0398(\u03b8 MLE ,\u03bb) E s\u223c\u03c1 [(\u03b8 MLE \u2212 \u03b8) \u22a4 (\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v)].\nBy the definition of \u0398(\u03b8 MLE , \u03bb), we know that for any \u03b8\n\u2208 \u0398(\u03b8 MLE , \u03bb), one has E s\u223c\u03c1 [(\u03b8 MLE \u2212\u03b8) \u22a4 (\u03d5(s, \u03c0 \u22c6 (s))\u2212 v)] \u2264 C \u00b7 d+log(1/\u03b4) \u03b3 2 n + \u03bbB 2 \u00b7 \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223c\u03c1 [\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v]\u2225 2 .\nFurthermore, we know that \u03b8 \u22c6 \u2208 \u0398(\u03b8 MLE , \u03bb) from Lemma 3.1. Altogether we have with probability 1 \u2212 \u03b4\nSubOpt(\u03c0 PE ) \u2264 2C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 \u00b7 \u2225(\u03a3 D + \u03bbI) \u22121/2 E s\u223c\u03c1 [\u03d5(s, \u03c0 \u22c6 (s)) \u2212 v]\u2225 2 .\nB.3 Proof of Theorem 3.9\n\nProof. Consider 4 actions with parameter \u03d5(a 1 ) = [1, 1, 0], \u03d5(a 2 ) = [1, 0, 0], \u03d5(a 3 ) = [0, 0, 0], \u03d5(a 4 ) = [0, 1, 0]. Let the true reward be \u03b8 \u22c6 = [\u22121, 0.1, 0.9] \u2208 \u0398 B with B = 2. We query n \u2212 1 times a 1 , a 2 and 1 time a 2 , a 3 . For the single pairwise comparison result Y 2>3 between a 2 and a 3 , we know that\nP (Y 2>3 = 1) = exp((\u03d5(a 2 ) \u2212 \u03d5(a 3 )) \u22a4 \u03b8 \u22c6 ) 1 + exp((\u03d5(a 2 ) \u2212 \u03d5(a 3 )) \u22a4 \u03b8 \u22c6 ) > 0.26.\nNow conditioned on the event that Y 2>3 = 1, we know that the MLE aims to find\n\u03b8 MLE = arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8),\nwhere \u2113 D (\u03b8) = \u2212n 1>2 \u00b7 log exp((\u03d5(a 1 ) \u2212 \u03d5(a 2 )) \u22a4 \u03b8) 1 + exp((\u03d5(a 1 ) \u2212 \u03d5(a 2 )) \u22a4 \u03b8) \u2212 n 1<2 \u00b7 log exp((\u03d5(a 2 ) \u2212 \u03d5(a 1 )) \u22a4 \u03b8) 1 + exp((\u03d5(a 2 ) \u2212 \u03d5(a 1 )) \u22a4 \u03b8) \u2212 log exp((\u03d5(a 2 ) \u2212 \u03d5(a 3 )) \u22a4 \u03b8) 1 + exp((\u03d5(a 2 ) \u2212 \u03d5(a 3 )) \u22a4 \u03b8) = \u2212n 1>2 \u00b7 log exp(\u03b8 2 ) 1 + exp(\u03b8 2 ) \u2212 n 1<2 \u00b7 log exp(\u2212\u03b8 2 ) 1 + exp(\u2212\u03b8 2 ) \u2212 log exp(\u03b8 1 ) 1 + exp(\u03b8 1 ) .\n\nBy concentration of n 1>2 , we know that when n > 500, with probability at least 0.5, we have n 1<2 > 0.45n.\n\nUnder this case, the MLE will satisfy at\u03b8 1 > 0,\u03b8 2 < 0.5. Thus the policy based on MLE estimator will choose action a 1 or a 2 instead of the optimal action a 4 under the events above. The expected suboptimality is\nE[V \u22c6 (s) \u2212 V\u03c0 MLE (s)] \u2265 0.26 * 0.5 * 1 > 0.1.\nOn the other hand, one can calculate the coverage as\n\u2225\u03a3 \u22121/2 D E s\u223c\u03c1 [\u03d5(s, \u03c0 \u22c6 (s))]\u2225 2 = n n \u2212 1 .\nThus by Theorem 3.2 we know that pessimistic MLE achieves vanishing error.\n\n\nB.4 Proof of Theorem 3.10\n\nProof. Assume without loss of generality that d/3 is some integer. We set S = [d/3], A = {a 1 , a 2 , a 3 , a 4 }.\n\nFor each of the s, a i , we set \u03d5(s, a 1 ) = e 3s+1 + e 3s+2 , \u03d5(s, a 2 ) = e 3s+1 , \u03d5(s, a 3 ) = 0, \u03d5(s, a 4 ) = e 3s+2 . We set the initial distribution of states as \u03c1 = Unif([1, 2, \u00b7 \u00b7 \u00b7 , S]), the query times n(s, a 1 , a 2 ) = n/S \u00b7 (1 \u2212 2/\u039b 2 ), n(s, a 2 , a 3 ) = n/S \u00b7 (2/\u039b 2 ).\n\nLet\nv \u22121 = [1/d, 1/d + \u2206, \u22122/d \u2212 \u2206], v +1 = [1/d + 2\u2206, 1/d + \u2206, \u22122/d \u2212 3\u2206]. We construct 2 S instances, indexed by \u03c4 \u2208 {\u00b11} S , where each \u03b8 \u03c4 = [v \u03c41 , v \u03c42 , \u00b7 \u00b7 \u00b7 , v \u03c4 S ]. One can see that E[V Q (\u03c0 \u22c6 ) \u2212 V \u22c6\nQ (\u03c0)] = 1/S \u00b7 s\u2208S (r Q (s, \u03c0 \u22c6 (s)) \u2212 r Q (s,\u03c0(s))). Under each \u03b8 \u03c4 , the optimal policy \u03c0(s) is either a 2 or a 4 . One can verify that \u2225\u03a3 \u22121/2 D E s\u223c\u03c1 [\u03d5(s, \u03c0 \u22c6 (s)])]\u2225 2 \u2264 \u039b and that \u03b8 \u03c4 \u2208 \u0398 B with B = 1 when d > 6 and \u2206 < 1/6 \u221a d. Furthermore, for any \u03b8 \u03c4 , \u03b8 \u03c4 \u2032 that differs only in the j-th coordinate of \u03c4 , we have 1/S \u00b7 (r Q\u03c4 (j, \u03c0 \u22c6 (j)) \u2212 r Q\u03c4 (j,\u03c0(j)) + r Q \u03c4 \u2032 (j, \u03c0 \u22c6 (j)) \u2212 r Q \u03c4 \u2032 (j,\u03c0(j))) \u2265 \u2206/S. Thus by Assouad's lemma (see e.g. Yu (1997)), we have\ninf \u03c0 sup Q\u2208CB(\u03bb) E[V Q (\u03c0 \u22c6 ) \u2212 V \u22c6 Q (\u03c0)] \u2265 S \u00b7 \u2206 2S min \u03c4 \u223c\u03c4 \u2032 (1 \u2212 TV(P \u03b8\u03c4 , P \u03b8 \u03c4 \u2032 )) \u2265 \u2206 4 min \u03c4 \u223c\u03c4 \u2032 exp(\u2212D KL (P \u03b8\u03c4 , P \u03b8 \u03c4 \u2032 )).\nHere \u03c4 \u223c \u03c4 \u2032 refers to any \u03c4, \u03c4 \u2032 that only differs in one element. And the last inequality is due to the Bretagnolle-Huber inequality Bretagnolle and Huber (1979). To bound the KL divergence, we have the following lemma from Shah et al. (2015): Shah et al. (2015)). For any pair of quality score vectors \u03b8 \u03c4 and \u03b8 \u03c4 \u2032 , we have\nLemma B.1 (D KL (P \u03b8\u03c4 \u2225P \u03b8\u03c4 ) \u2264 Cn(\u03b8 \u03c4 \u2212 \u03b8 \u03c4 \u2032 ) \u22a4 \u03a3 D (\u03b8 \u03c4 \u2212 \u03b8 \u03c4 \u2032 ).(3)\nFrom the lemma, we have\ninf \u03c0 sup Q\u2208CB(\u03bb) E[V Q (\u03c0 \u22c6 ) \u2212 V \u22c6 Q (\u03c0)] \u2265 \u2206 2 min \u03c4 \u223c\u03c4 \u2032 exp(\u2212D KL (P \u03b8\u03c4 , P \u03b8 \u03c4 \u2032 )) \u2265 \u2206 2 exp(\u2212Cn\u2206 2 /(S\u039b 2 ))\nTaking \u2206 = \u039b S/n and noting that S = d/3 finishes the proof.\n\n\nB.5 Proof of Theorem 4.1\n\nThis section presents the proof of Theorem 4.1 for the setting of K-wise comparisons. We first prove the following lemma on the estimation error:\n\nLemma B.2. Under the K-wise PL model, for any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 K 4 (d + log(1/\u03b4)) \u03b3 2 n + \u03bbB 2 .\nRecall that the MLE is given b\u0177\n\u03b8 MLE \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8), where \u2113 D (\u03b8) = \u2212 1 n n i=1 K\u22121 j=0 log exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(j) )\u27e9) K\u22121 k=j exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k) )\u27e9)\n.\n\nOur goal is to bound the estimation error of the MLE in the squared semi-norm \u2225v\u2225 2\n\u03a3 D +\u03bbI = v \u22a4 (\u03a3 D + \u03bbI)v.\nStrong convexity of \u2113. We first show that \u2113 D is strongly convex at \u03b8 \u22c6 with respect to the semi-norm \u2225 \u00b7 \u2225 \u03a3 D , meaning that there is some constant \u03b3 > 0 such that\n\u2113 D (\u03b8 \u22c6 + \u2206) \u2212 \u2113 D (\u03b8 \u22c6 ) \u2212 \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 \u2265 \u03b3\u2225\u2206\u2225 2 \u03a3 D (4) for all perturbations \u2206 \u2208 R d such that \u03b8 \u22c6 + \u2206 \u2208 \u0398 B .\nThe gradient of the negative log likelihood is\n\u2207\u2113 D (\u03b8) = \u2212 1 n n i=1 K\u22121 j=0 K\u22121 k=j exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k) )\u27e9) K\u22121 k \u2032 =j exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9)\n\u00b7 (\u03d5(s i , a i \u03c3i(j) ) \u2212 \u03d5(s i , a i \u03c3i(k) )).\n\nThe Hessian of the negative log likelihood can be written as\n\u2207 2 \u2113 D (\u03b8) = 1 n n i=1 K\u22121 j=0 K\u22121 k=j K\u22121 k \u2032 =j exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k) ) + \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9) 2( K\u22121 k \u2032 =j exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9)) 2 \u00b7 (\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) ))(\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) )) \u22a4 .\nSince exp(\u27e8\u03b8, \u03d5\u27e9) \u2208 [exp(\u2212LB), exp(LB)], we know that the coefficients satisfy\nexp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k) ) + \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9) ( K\u22121 k \u2032 =j exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9)) 2 \u2265 exp(\u22124LB) 2(K \u2212 j) 2 .\nSet \u03b3 = exp(\u22124LB)/2. We can verify that for any vector v \u2208 R K , one has\nv \u22a4 \u2207 2 \u2113 D (\u03b8)v \u2265 \u03b3 n v \u22a4 \uf8eb \uf8ed n i=1 K\u22121 j=0 1 (K \u2212 j) 2 K\u22121 k=j K\u22121 k \u2032 =k (\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) ))(\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) )) \u22a4 \uf8f6 \uf8f8 v \u2265 \u03b3 n v \u22a4 \uf8eb \uf8ed n i=1 min \u03c3i\u2208\u03a0[K] K\u22121 j=0 1 (K \u2212 j) 2 K\u22121 k=j K\u22121 k \u2032 =k (\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) ))(\u03d5(s i , a i \u03c3i(k) ) \u2212 \u03d5(s i , a i \u03c3i(k \u2032 ) )) \u22a4 \uf8f6 \uf8f8 v \u2265 \u03b3v \u22a4 \u03a3 D v = \u03b3\u2225v\u2225 2 \u03a3 D .\nThus we know that \u2113 is \u03b3-strongly convex with respect to the semi-norm \u2225 \u00b7 \u2225 \u03a3 D .\n\nBounding the estimation error. Now we aim at bounding the estimation error \u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D+\u03bbI . Since\u03b8 MLE is optimal for \u2113 D , we have \u2113 D (\u03b8 MLE ) \u2264 \u2113 D (\u03b8 \u22c6 ). Defining the error vector \u2206 =\u03b8 MLE \u2212 \u03b8 \u22c6 , adding and subtracting the quantity \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 yields the bound\n\u2113 D (\u03b8 \u22c6 + \u2206) \u2212 \u2113 D (\u03b8 \u22c6 ) \u2212 \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 \u2264 \u2212\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9.\nBy the \u03b3-convexity condition, the left-hand side is lower bounded by \u03b3\u2225\u2206\u2225 2 \u03a3 D . As for the right-hand side, note that |\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9| \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI for any \u03bb > 0. Altogether we have\n\u03b3\u2225\u2206\u2225 2 \u03a3 D \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI .\nNow we further bound the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 . Observe that the gradient takes the form\n\u2207\u2113 D (\u03b8 \u22c6 ) = \u2212 1 n n i=1 K\u22121 j=0 K\u22121 k=j exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i \u03c3i(k) )\u27e9) K\u22121 k \u2032 =j exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i \u03c3i(k \u2032 ) )\u27e9) \u00b7 (\u03d5(s i , a i \u03c3i(j) ) \u2212 \u03d5(s i , a i \u03c3i(k) )).(5)\nWe set x i jk = \u03d5(s i , a i j ) \u2212 \u03d5(s i , a i k ). X \u2208 R (nK(K\u22121)/2)\u00d7d has the differencing vector x i jk as its (iK(K \u2212 1)/2 + k + K l=K\u2212j+1 l) th row. We also define V i jk be the random variable of the coefficient of x i jk in Equation (5) under the PL model, i.e. conditioned on an arbitrary permutation \u03c3 i ,\nV i jk = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 exp(\u27e8\u03b8 \u22c6 , \u03d5(s i ,a i k )\u27e9) K\u22121 k \u2032 =\u03c3 \u22121 i (j) exp(\u27e8\u03b8 \u22c6 , \u03d5(s i ,a i \u03c3 i (k \u2032 ) )\u27e9) , if \u03c3 \u22121 i (j) < \u03c3 \u22121 i (k) \u2212 exp(\u27e8\u03b8 \u22c6 , \u03d5(s i ,a i j )\u27e9) K\u22121 k \u2032 =\u03c3 \u22121 i (k) exp(\u27e8\u03b8 \u22c6 , \u03d5(s i ,a i \u03c3 i (k \u2032 ) )\u27e9) , otherwise.\nHere \u03c3 \u22121 i (j) < \u03c3 \u22121 i (k) means that the j-th item ranks higher than the k-th item. Let\u1e7c i \u2208 R K(K\u22121)/2 be the concatenated random vector of {V i jk } 0\u2264j<k\u2264K\u22121 , V \u2208 R nK(K\u22121)/2 be the concatenated random vector of {\u1e7c i } n i=1 . We know that\u1e7c i and\u1e7c j are independent for each i \u0338 = j due to the independent sampling procedure. We can also verify that the mean of\u1e7c i is 0, the proof of which is deferred to the end of this section. Furthermore, since under any permutation, the sum of absolute value of each element in\u1e7c i is at most K, we know that\u1e7c i is sub-Gaussian with parameter K. Thus we know that V is also sub-Gaussian with mean 0 and parameter K. Now we know that the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 can be written as\n\u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 = 1 n 2 V \u22a4 X(\u03a3 D + \u03bbI) \u22121 X \u22a4 V.\nLet M = K 2 n I. One can verify that M \u2ab0 1 n 2 X(\u03a3 D + \u03bbI) \u22121 X \u22a4 almost surely since \u03bb max (X(\u03a3 D + \u03bbI) \u22121 X \u22a4 /n 2 ) \u2264 K 2 /n. Thus we can upper bound the original term as\n\u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 \u2264 K 2 n \u2225V \u2225 2 2 .\nBy Bernstein's inequality for sub-Gaussian random variables in quadratic form (see e.g. Hsu et al. (2012, Theorem 2.1)), we know that with probability at least 1 \u2212 \u03b4, \u2225V \u2225 2 2 \u2264 CK 2 \u00b7 (d + log(1/\u03b4)).\n\nThus altogether, we have \u03b3\u2225\u2206\u2225 2 \u03a3 D \u2264 CK 4 \u00b7 (d + log(1/\u03b4)) n \u2225\u2206\u2225 \u03a3 D +\u03bbI .\n\nSimilar to the pairwise comparison analysis in Appendix B.1, we can derive that with probability at least 1 \u2212 \u03b4, \u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 K 4 (d + log(1/\u03b4)) n + \u03bbB 2 .\n\nThe rest of the proof on the sub-optimality upper bound follows the same argument as Theorem 3.2. Lastly, we verify that the mean of\u1e7c i is 0. For any fixed j, k \u2208 [K], let P be the ordered set of all elements which are ranked higher than both j and k. Now conditioned on P, we have E[V i jk | P] = P(j follows P | P) \u00b7 exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i k )\u27e9) k \u2032 \u2208P exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i k \u2032 )\u27e9)\n\n\u2212 P(k follows P | P) \u00b7 exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i j )\u27e9)\nk \u2032 \u2208P exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i k \u2032 )\u27e9) = 1 k \u2032 \u2208P exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i k \u2032 )\u27e9)\n\u00b7 exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i j )\u27e9) exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i k )\u27e9) \u2212 exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i j )\u27e9) exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i k )\u27e9) exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i j )\u27e9) + exp(\u27e8\u03b8 \u22c6 , \u03d5(s i , a i k )\u27e9) = 0.\n\nHere the second equality uses the fact that j follows P is equivalent to the event that j is larger than k and either j, k is the largest amongP. Taking expectation over P gives us that E[V i jk ] = 0.\n\n\nB.6 Proof of Theorem 4.2\n\nThis section presents the proof of Theorem 4.2 for the setting of K-wise comparisons. We first prove the following lemma on the estimation error.\n\nLemma B.3. Under the K-wise PL model, for any \u03bb > 0, with probability at least 1 \u2212 \u03b4,\n\u2225\u03b8 MLE2 \u2212 \u03b8 \u22c6 \u2225 \u03a3 D +\u03bbI \u2264 C \u00b7 d + log(1/\u03b4) \u03b3 2 n + \u03bbB 2 .\nRecall that the pairwise compairson based estimator is given b\u0177\n\u03b8 MLE2 \u2208 arg min \u03b8\u2208\u0398 B \u2113 D (\u03b8),\nwhere \u2113 D (\u03b8) = \u2212 1 n n i=1 K\u22121 j=0 K\u22121 k=j+1 log exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(j) )\u27e9) exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(j) )\u27e9) + exp(\u27e8\u03b8, \u03d5(s i , a i \u03c3i(k) )\u27e9)\n\n.\n\nOur goal is to bound the estimation error of the MLE in the squared semi-norm \u2225v\u2225 2 \u03a3 D +\u03bbI = v \u22a4 (\u03a3 D + \u03bbI)v.\n\nStrong convexity of \u2113. Let x i jk = \u03d5(s i , a i j ) \u2212 \u03d5(s i , a i k ). The gradient of the negative log likelihood is \u2207\u2113 D (\u03b8) = \u2212 1 n n i=1 K\u22121 j=0 K\u22121 k=j+1 exp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9 1 + exp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9 \u00b7 x i \u03c3i(j)\u03c3i(k) .\n\nThe Hessian of the negative log likelihood can be written as \u2207 2 \u2113 D (\u03b8) = 1 n n i=1 K\u22121 j=0 K\u22121 k=j exp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9 (1 + exp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9) 2 \u00b7 x i \u03c3i(j)\u03c3i(k) x i\u22a4 \u03c3i(j)\u03c3i(k) .\n\nSince exp(\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) \u27e9) \u2208 [exp(\u22122LB), exp(2LB)], we know that the coefficients satisfy exp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9 (1 + exp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9) 2 \u2265 1 2 + exp(2LB) + exp(\u22122LB) .\n\nSet \u03b3 = 1 2+exp(2LB)+exp(\u22122LB) . We can verify that for any vector v \u2208 R K , one has\nv \u22a4 \u2207 2 \u2113 D (\u03b8)v \u2265 \u03b3 n v \u22a4 \uf8eb \uf8ed n i=1 K\u22121 j=0 K\u22121 k=j+1 x i \u03c3i(j)\u03c3i(k) x i\u22a4 \u03c3i(j)\u03c3i(k) \uf8f6 \uf8f8 v = \u03b3 n v \u22a4 \uf8eb \uf8ed n i=1 K\u22121 j=0 K\u22121 k=j+1 x i jk x i\u22a4 jk \uf8f6 \uf8f8 v = \u03b3K(K \u2212 1)v \u22a4 \u03a3 D v/2 = \u03b3K(K \u2212 1)\u2225v\u2225 2 \u03a3 D /2.\nThus we know that \u2113 is \u03b3-strongly convex with respect to the semi-norm \u2225 \u00b7 \u2225 \u03a3 D .\n\nBounding the estimation error. Now we aim at bounding the estimation error \u2225\u03b8 MLE2 \u2212 \u03b8 \u22c6 \u2225 \u03a3 D+\u03bbI . Since\u03b8 MLE2 is optimal for \u2113 D , we have \u2113 D (\u03b8 MLE2 ) \u2264 \u2113 D (\u03b8 \u22c6 ). Defining the error vector \u2206 =\u03b8 MLE2 \u2212 \u03b8 \u22c6 , adding and subtracting the quantity \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 yields the bound\n\u2113 D (\u03b8 \u22c6 + \u2206) \u2212 \u2113 D (\u03b8 \u22c6 ) \u2212 \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 \u2264 \u2212\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9.\nBy the \u03b3-convexity condition, the left-hand side is lower bounded by \u03b3K(K \u2212 1)\u2225\u2206\u2225 2 \u03a3 D /2. As for the right-hand side, note that |\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9| \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI for any \u03bb > 0. Altogether we have\n\u03b3\u2225\u2206\u2225 2 \u03a3 D \u2264 2\u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI /K(K \u2212 1).\nNow we further bound the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 . Observe that the gradient takes the form\n\u2207\u2113 D (\u03b8 \u22c6 ) = \u2212 1 n n i=1 K\u22121 j=0 K\u22121 k=j+1\nexp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9 1 + exp(\u2212\u27e8\u03b8, x i \u03c3i(j)\u03c3i(k) )\u27e9 \u00b7 x i \u03c3i(j)\u03c3i(k) .\n\nWe set X \u2208 R (nK(K\u22121)/2)\u00d7d with the differencing vector x i jk as its (iK(K \u2212 1)/2 + k + K l=K\u2212j+1 l) th row. We also define V i jk be the random variable of the coefficient of x i jk in Equation (6) under the PL model, i.e. conditioned on an arbitrary permutation \u03c3 i ,\nV i jk = \uf8f1 \uf8f2 \uf8f3 exp(\u2212\u27e8\u03b8, x i jk )\u27e9 1+exp(\u2212\u27e8\u03b8, x i jk )\u27e9 , if \u03c3 \u22121 i (j) < \u03c3 \u22121 i (k) \u2212 1 1+exp(\u2212\u27e8\u03b8, x i jk )\u27e9 , otherwise.\nLet\u1e7c i \u2208 R K(K\u22121)/2 be the concatenated random vector of {V i jk } 0\u2264j<k\u2264K\u22121 , V \u2208 R nK(K\u22121)/2 be the concatenated random vector of {\u1e7c i } n i=1 . We know that\u1e7c i is independent for each i, and that V is sub-Gaussian with mean 0 and parameter K(K \u2212 1)/2 since the PL model reduces to BTL model when considering pairwise comparisons. Now we know that the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 can be written as\n\u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 = 1 n 2 V \u22a4 X(\u03a3 D + \u03bbI) \u22121 X \u22a4 V.\nLet M = K 2 n I. One can verify that M \u2ab0 1 n 2 X(\u03a3 D + \u03bbI) \u22121 X \u22a4 almost surely since \u03bb max (X(\u03a3 D + \u03bbI) \u22121 X \u22a4 /n 2 ) \u2264 K 2 /n. Thus we can upper bound the original term as \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 \u2264 K 2 n \u2225V \u2225 2 2 .\n\nBy Bernstein's inequality for sub-Gaussian random variables in quadratic form (see e.g. Hsu et al. (2012, Theorem 2.1)), we know that with probability at least 1 \u2212 \u03b4, \u2225V \u2225 2 2 \u2264 CK(K \u2212 1) \u00b7 (d + log(1/\u03b4)).\n\nBounding the estimation error. Now we aim at bounding the estimation error \u2225\u03b8 MLE \u2212 \u03b8 \u22c6 \u2225 \u03a3 D . Since\u03b8 MLE is optimal for \u2113 D , we have \u2113 D (\u03b8 MLE ) \u2264 \u2113 D (\u03b8 \u22c6 ). Defining the error vector \u2206 =\u03b8 MLE \u2212 \u03b8 \u22c6 , adding and subtracting the quantity \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 yields the bound \u2113 D (\u03b8 \u22c6 + \u2206) \u2212 \u2113 D (\u03b8 \u22c6 ) \u2212 \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 \u2264 \u2212\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9.\n\nBy the \u03b3-convexity condition, the left-hand side is lower bounded by \u03b3\u2225\u2206\u2225 2 \u03a3 D . As for the right-hand side, note that |\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9| \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI for any \u03bb > 0. Altogether we have\n\u03b3\u2225\u2206\u2225 2 \u03a3 D \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI .\nNow we further bound the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 . Observe that the gradient takes the form\n\u2207\u2113 D (\u03b8 \u22c6 ) = \u22121 n n i=1 1[y i = 1] exp(\u2212\u27e8\u03b8 \u22c6 , x i \u27e9) 1 + exp(\u2212\u27e8\u03b8 \u22c6 , x i \u27e9)) \u2212 1[y i = 0] 1 1 + exp(\u2212\u27e8\u03b8 \u22c6 , x i \u27e9)) x i .\nDefine a random vector V \u2208 R n with independent components as V i = exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9) 1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) w.p.\n\n\n; Xia et al. (2008); Cao et al. (2007); Christiano et al. (2017a); Ouyang et al.\n\n\n;Neu and Szepesv\u00e1ri (2009);Florence et al. (2022). Concretely, the human behavior is assumed to be based on the Q function Q \u22c6 (s, a) = \u27e8\u03b8 \u22c6 , \u03d5(s, a)\u27e9, i.e. \u03c0 \u22c6 (a|s) = exp(\u27e8\u03b8 \u22c6 , \u03d5(s, a)\u27e9) a \u2032 \u2208A exp(\u27e8\u03b8 \u22c6 , \u03d5(s, a \u2032 )\u27e9)\n\n\n\u2032 ))) \u22a4 , and \u03b3 = exp(\u22124LB)/2. Similar to the case of action-based RLHF, it remains an interesting open problem how one can introduce provable lower confidence bound algorithm for policy learning.\n\n\n;MacGlashan et al., 2017;Christiano et al., 2017a;  Warnell et al., 2018), robotics (Brown et al., 2019Shin et al., 2023) and language models(Ziegler et al.,  2019; Stiennon et al., 2020;Wu et al., 2021;Nakano et al., 2021;Ouyang et al., 2022;Menick et al., 2022;Glaese et al., 2022;Gao et al., 2022; Bai et al., 2022a;Ganguli et al., 2022;Ramamurthy et al., 2022). Notably, the concurrent workShin et al. (2023) proposes Offline Preference-Based Reward Learning (OPRL), which trains pessimistic policy from the learned reward and shows empirically the superior performance of pessimistic based method (which can be viewed as an approximation of pessimistic MLE).\n\nFigure 1 :\n1Left: the convergence of MLE under the semi-norm \u2225 \u00b7 \u2225 \u03a3 ; Right: the comparison between MLE and pessimistic MLE under sub-optimality metric.\n\n\nT. Xie, N. Jiang, H. Wang, C. Xiong, and Y. Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. Advances in Neural Information Processing Systems, 34:27395-27407, 2021b. T. Xu and Y. Liang. Provably efficient offline reinforcement learning with trajectory-wise reward. arXiv preprint arXiv:2206.06426, 2022. Y. Xu, R. Wang, L. Yang, A. Singh, and A. Dubrawski. Preference-based reinforcement learning with finite-time guarantees. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18784-18794. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ d9d3837ee7981e8c064774da6cdd98bf-Paper.pdf. B. Yu. Assouad, fano, and le cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer, 1997. Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1201-1208. ACM, 2009. Y. Yue and T. Joachims. Beat the mean bandit. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 241-248, 2011. Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538-1556, 2012. A. Zanette. When is realizability sufficient for off-policy reinforcement learning? arXiv preprint arXiv:2211.05311, 2022. A. Zanette, M. J. Wainwright, and E. Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. Advances in Neural Information Processing Systems, 34:13626-13640, 2021. B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433-1438. Chicago, IL, USA, 2008. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. M. Zoghi, S. Whiteson, R. Munos, M. d. Rijke, et al. Relative upper confidence bound for the k-armed dueling bandit problem. In JMLR Workshop and Conference Proceedings, number 32, pages 10-18. JMLR, 2014a. M. Zoghi, S. A. Whiteson, M. De Rijke, and R. Munos. Relative confidence sampling for efficient on-line ranker evaluation. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 73-82. ACM, 2014b.\n\n\nP. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-First International Conference on Machine Learning, page 1, 2004. Wahrscheinlichkeitstheorie und Verwandte Gebiete, 47(2):119-137, 1979. doi: 10.1007/BF00535278. URL https://doi.org/10.1007/BF00535278. D. Brown, W. Goo, P. Nagarajan, and S. Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. In International Conference on Machine Learning, Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: From pairwise approach to listwise approach. In Proceedings of the 24th International Conference on Machine Learning, pages 129Y. Abdelkareem, S. Shehata, and F. Karray. Advances in preference-based reinforcement learning: A \nreview. In 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages \n2527-2532. IEEE, 2022. \n\nN. Ailon, Z. S. Karnin, and T. Joachims. Reducing dueling bandits to cardinal bandits. In ICML, \nvolume 32, pages 856-864, 2014. \n\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, \net al. Training a helpful and harmless assistant with reinforcement learning from human feedback. \narXiv preprint arXiv:2204.05862, 2022a. \n\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, \nC. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, \n2022b. \n\nR. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs i: The method of paired \ncomparisons. Biometrika, 39(3/4):324-345, 1952. \n\nJ. Bretagnolle and C. Huber. \nEstimation des densit\u00e9s: risque minimax. \nZeitschrift f\u00fcr \npages 783-792. PMLR, 2019. \n\nR. Busa-Fekete, B. Sz\u00f6r\u00e9nyi, P. Weng, W. Cheng, and E. H\u00fcllermeier. Preference-based reinforcement \nlearning: evolutionary direct policy search using a preference-based racing algorithm. Machine Learning, \n97(3):327-351, 2014. \n\n-136, \n2007. \n\nN. S. Chatterji, A. Pacchiano, P. L. Bartlett, and M. I. Jordan. On the theory of reinforcement learning \nwith once-per-episode feedback, 2022. \n\nX. Chen, P. N. Bennett, K. Collins-Thompson, and E. Horvitz. Pairwise ranking aggregation in a \ncrowdsourced setting. In Proceedings of the Sixth ACM International Conference on Web Search and \nData Mining, pages 193-202, 2013. \n\nX. Chen, H. Zhong, Z. Yang, Z. Wang, and L. Wang. Human-in-the-loop: Provably efficient preference-\nbased reinforcement learning with general function approximation. In International Conference on \nMachine Learning, pages 3773-3793. PMLR, 2022. \n\nY. Chen and C. Suh. Spectral MLE: top-k rank aggregation from pairwise comparisons. In International \nConference on Machine Learning, pages 371-380. PMLR, 2015. \n\nC.-A. Cheng, T. Xie, N. Jiang, and A. Agarwal. Adversarially trained actor critic for offline reinforcement \nlearning. arXiv preprint arXiv:2202.02446, 2022. \nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. \nLearning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: \n3008-3021, 2020. \n\nA. W. Van der Vaart. Asymptotic Statistics, volume 3. Cambridge university press, 2000. \n\nG. Warnell, N. Waytowich, V. Lawhern, and P. Stone. Deep tamer: Interactive agent shaping in high-\ndimensional state spaces. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, \n2018. \n\nC. Wirth, J. Furnkranz, G. Neumann, et al. Model-free preference-based reinforcement learning. In 30th \nAAAI Conference on Artificial Intelligence, AAAI 2016, pages 2222-2228, 2016. \n\nC. Wirth, R. Akrour, G. Neumann, and J. F\u00fcrnkranz. A survey of preference-based reinforcement learning \nmethods. The Journal of Machine Learning Research, 18(1):4945-4990, 2017. \n\nJ. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively \nsummarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021. \n\nF. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise approach to learning to rank: theory and \nalgorithm. In Proceedings of the 25th International Conference on Machine Learning, pages 1192-1199, \n2008. \n\nT. Xie, C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal. Bellman-consistent pessimism for offline \nreinforcement learning. Advances in Neural Information Processing Systems, 34:6683-6694, 2021a. \n\n\nIn InstructGPT, the function \u03d5 is still parametrized can be further trained in the reward learning step. However, for simplicity of theoretical analysis we assume in this paper that \u03d5 is fixed and one only fine-tunes the last layer with parameter \u03b8.\nIndeed, it is not necessary to only compare actions under the same state. Our results can be easily generalized to the case when the states for K queries are completely different.\nIn practice, one may introduce an extra temperature parameter \u03c3 and replace all r \u03b8 \u22c6 with r \u03b8 \u22c6 /\u03c3. Here we take \u03c3 = 1.\nfor all v,\n1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) \u22121 1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) w.p. exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)1+exp(\u2212\u27e8\u03b8 \u22c6 , xi\u27e9)) .With this notation, we have \u2207\u2113 D (\u03b8 \u22c6 ) = \u2212 1\nAcknowledgementThe authors would like to thank the reviewers for the valuable suggestions. Banghua Zhu and Jiantao Jiao were partially supported by NSF Grants IIS-1901252 and CCF-1909499. Michael I. Jordan was partially supported by NSF Grants IIS-1901252.Thus altogether, we have \u03b3\u2225\u2206\u2225 2 \u03a3 D \u2264 C \u00b7 (d + log(1/\u03b4)) n \u2225\u2206\u2225 \u03a3 D +\u03bbI .Similar to the pairwise comparison, we can derive that with probability at least 1 \u2212 \u03b4,The rest of the proof on the sub-optimality upper bound follows the same argument as Theorem 3.2.B.7 Proof of Lemma 5.1Recall that the MLE is given b\u0177To simplify the notation, we let). Our goal is to bound the estimation error of the MLE in the squared semi-norm \u2225v\u2225 2Strong convexity of \u2113. We first show that \u2113 D is strongly convex at \u03b8 \u22c6 with respect to the semi-norm \u2225 \u00b7 \u2225 \u03a3 D , meaning that there is some constant \u03b3 > 0 such thatfor all perturbations \u2206 \u2208 R d such that \u03b8 \u22c6 + \u2206 \u2208 \u0398 B . One can directly calculate the Hessian of \u2113 aswhere \u03b3 = 1/(2 + exp(\u22122HLB) + exp(2HLB)), X \u2208 R n\u00d7d has the differencing vector x i \u2208 R d as its i th row.Thus, if we introduce the error vector \u2206 :=\u03b8 MLE \u2212 \u03b8 \u22c6 , then we may conclude thatshowing that \u2113 D is strongly convex around \u03b8 \u22c6 with parameter \u03b3. n X \u22a4 V . One can verify thatWe can bound the trace and operator norm of M asMoreover, since the components of V are independent and of zero mean, and |V i | \u2264 1, the variables are 1-sub-Gaussian, and hence the Bernstein's inequality for sub-Gaussian random variables in quadratic form (see e.g.Hsu et al. (2012, Theorem 2.1)) implies that with probability at least 1 \u2212 \u03b4,Here C 1 is some universal constant. This gives usSolving the above inequality gives us that for some constant C 2 ,B.8 Proof of Theorem 5.2Proof. From Lemma 5.1, we know that with probability at least 1 \u2212 \u03b4,B.9 Proof of Theorem 6.2Proof. Here We mainly prove Lemma 6.1, since Theorem 6.2 is a direct corollary when combined with the proof in Theorem 5.2. Our goal is to bound the estimation error of the MLE in the squared semi-norm \u2225v\u2225 2Strong convexity of \u2113. We first show that \u2113 D is strongly convex at \u03b8 \u22c6 with respect to the semi-norm \u2225 \u00b7 \u2225 \u03a3 D , meaning that there is some constant \u03b3 > 0 such thatThe gradient of the negative log likelihood is[H]. The Hessian of the negative log likelihood can be written asSince exp(\u27e8\u03b8, \u03d5\u27e9) \u2208 [exp(\u2212LB), exp(LB)], we know that the coefficients satisfySet \u03b3 = exp(\u22124LB)/2. We can verify that for any vector v \u2208 R K , one hasThus we know that \u2113 is \u03b3-strongly convex with respect to the semi-norm \u2225 \u00b7 \u2225 \u03a3 D .Bounding the estimation error. Now we aim at bounding the estimation errorDefining the error vector \u2206 =\u03b8 MLE \u2212 \u03b8 \u22c6 , adding and subtracting the quantity \u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9 yields the boundBy the \u03b3-convexity condition, the left-hand side is lower bounded by \u03b3\u2225\u2206\u2225 2 \u03a3 D . As for the right-hand side, note that |\u27e8\u2207\u2113 D (\u03b8 \u22c6 ), \u2206\u27e9| \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI for any \u03bb > 0. Altogether we haveNow we further bound the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 . Observe that the gradient takes the formWe set X as the concatenated differencing vector x i \u03c4,\u03c4 \u2032 where \u03c4, \u03c4 \u2032 are distinct and ordered. We also define V i \u03c4,\u03c4 \u2032 be the random variable of the coefficient of x i \u03c4,\u03c4 \u2032 in Equation(9), i.e.otherwise.Let\u1e7c i be the concatenated random vector of {V i \u03c4,\u03c4 \u2032 }, V be the concatenated random vector of {\u1e7c i } n i=1 . We know that\u1e7c i and\u1e7c j are independent for each i \u0338 = j due to the independent sampling procedure. We can also verify that the mean of\u1e7c i is 0. We know that\u1e7c i has almost sup s |T (s)| non-zero elements. And the sum of their absolute value is bounded by 1. we know\u1e7c i is 1-sub-Gaussian. Now we know that the term \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 can be written asLet M = sup s |T (s)| 2 n I. One can verify that M \u2ab0 1 n 2 X(\u03a3 D + \u03bbI) \u22121 X \u22a4 almost surely since \u03bb max (X(\u03a3 D + \u03bbI) \u22121 X \u22a4 /n 2 ) \u2264 sup s |T (s)| 2 /n. Thus we can upper bound the original term asBy Bernstein's inequality for sub-Gaussian random variables in quadratic form (see e.g.Hsu et al. (2012, Theorem 2.1)), we know that with probability at least 1 \u2212 \u03b4, \u2225V \u2225 2 2 \u2264 C \u00b7 (d + log(1/\u03b4)).Moreover, since the components of V are independent and of zero mean, and |V i | \u2264 1, the variables are 1-sub-Gaussian, and hence the Bernstein's inequality for sub-Gaussian random variables in quadratic form (see e.g.Hsu et al. (2012, Theorem 2.1)) implies that with probability at least 1 \u2212 \u03b4, \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 2 (\u03a3 D +\u03bbI) \u22121 = V \u22a4 M V \u2264 C 1 \u00b7 d + log(1/\u03b4) n .Here C 1 is some universal constant. This gives us \u03b3\u2225\u2206\u2225 2 \u03a3 D +\u03bbI \u2264 \u2225\u2207\u2113 D (\u03b8 \u22c6 )\u2225 (\u03a3 D +\u03bbI) \u22121 \u2225\u2206\u2225 \u03a3 D +\u03bbI + 4(\u03bb\u03b3 + 2\u03b1 2 (1 + 2\u03b3\u03b1 1 B))B 2 \u2264 C 1 \u00b7 d + log(1/\u03b4) n \u2225\u2206\u2225 \u03a3 D +\u03bbI + 4(\u03bb\u03b3 + 2\u03b1 2 (1 + 2\u03b3\u03b1 1 B))B 2 .Solving the above inequality gives us that for some constant C 2 , \u2225\u2206\u2225 \u03a3 D +\u03bbI \u2264 C 2 \u00b7 d + log(1/\u03b4) \u03b3 2 n + (\u03bb + \u03b1 2 /\u03b3 + \u03b1 1 \u03b1 2 B)B 2 .\nDeep reinforcement learning from human preferences. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, Advances in Neural Information Processing Systems. 30P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017a.\n\nDeep reinforcement learning from human preferences. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, Advances in Neural Information Processing Systems. P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pages 4299-4307, 2017b.\n\nImproved optimistic algorithms for logistic bandits. L Faury, M Abeille, C Calauz\u00e8nes, O Fercoq, International Conference on Machine Learning. PMLRL. Faury, M. Abeille, C. Calauz\u00e8nes, and O. Fercoq. Improved optimistic algorithms for logistic bandits. In International Conference on Machine Learning, pages 3052-3060. PMLR, 2020.\n\nComputing with noisy information. U Feige, P Raghavan, D Peleg, E Upfal, SIAM Journal on Computing. 235U. Feige, P. Raghavan, D. Peleg, and E. Upfal. Computing with noisy information. SIAM Journal on Computing, 23(5):1001-1018, 1994.\n\nImplicit behavioral cloning. P Florence, C Lynch, A Zeng, O A Ramirez, A Wahid, L Downs, A Wong, J Lee, I Mordatch, J Tompson, Conference on Robot Learning. PMLRP. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson. Implicit behavioral cloning. In Conference on Robot Learning, pages 158-168. PMLR, 2022.\n\nA relative exponential weighing algorithm for adversarial utility-based dueling bandits. P Gajane, T Urvoy, F Cl\u00e9rot, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine LearningP. Gajane, T. Urvoy, and F. Cl\u00e9rot. A relative exponential weighing algorithm for adversarial utility-based dueling bandits. In Proceedings of the 32nd International Conference on Machine Learning, pages 218-227, 2015.\n\nRed teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. D Ganguli, L Lovitt, J Kernion, A Askell, Y Bai, S Kadavath, B Mann, E Perez, N Schiefer, K Ndousse, arXiv:2209.07858arXiv preprintD. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.\n\nScaling laws for reward model overoptimization. L Gao, J Schulman, J Hilton, arXiv:2210.10760arXiv preprintL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, 2022.\n\nExploiting correlation to achieve faster learning rates in low-rank preference bandits. S Ghoshal, A Saha, International Conference on Artificial Intelligence and Statistics. PMLRS. Ghoshal and A. Saha. Exploiting correlation to achieve faster learning rates in low-rank preference bandits. In International Conference on Artificial Intelligence and Statistics, pages 456-482. PMLR, 2022.\n\n. A Glaese, N Mcaleese, M Tre \u00b7 Bacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, A. Glaese, N. McAleese, M. Tre \u00b7 bacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger,\n\nImproving alignment of dialogue agents via targeted human judgements. M Chadwick, P Thacker, arXiv:2209.14375arXiv preprintM. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\n\nAn optimum property of regular maximum likelihood estimation. V P Godambe, The Annals of Mathematical Statistics. 314V. P. Godambe. An optimum property of regular maximum likelihood estimation. The Annals of Mathematical Statistics, 31(4):1208-1211, 1960.\n\nMinimax-optimal inference from partial rankings. B Hajek, S Oh, J Xu, Advances in Neural Information Processing Systems. 27B. Hajek, S. Oh, and J. Xu. Minimax-optimal inference from partial rankings. Advances in Neural Information Processing Systems, 27, 2014.\n\nApproximate ranking from pairwise comparisons. R Heckel, M Simchowitz, K Ramchandran, M Wainwright, International Conference on Artificial Intelligence and Statistics. PMLRR. Heckel, M. Simchowitz, K. Ramchandran, and M. Wainwright. Approximate ranking from pairwise comparisons. In International Conference on Artificial Intelligence and Statistics, pages 1057-1066. PMLR, 2018.\n\nActive ranking from pairwise comparisons and when parametric assumptions do not help. R Heckel, N B Shah, K Ramchandran, M J Wainwright, Annals of Statistics. 476R. Heckel, N. B. Shah, K. Ramchandran, and M. J. Wainwright. Active ranking from pairwise comparisons and when parametric assumptions do not help. Annals of Statistics, 47(6):3099-3126, 2019.\n\nGenerative adversarial imitation learning. J Ho, S Ermon, Advances in Neural Information Processing Systems. 29J. Ho and S. Ermon. Generative adversarial imitation learning. Advances in Neural Information Processing Systems, 29, 2016.\n\nA tail inequality for quadratic forms of subgaussian random vectors. D Hsu, S Kakade, T Zhang, Electronic Communications in Probability. 17D. Hsu, S. Kakade, and T. Zhang. A tail inequality for quadratic forms of subgaussian random vectors. Electronic Communications in Probability, 17:1-6, 2012.\n\nImitation learning: A survey of learning methods. A Hussein, M M Gaber, E Elyan, C Jayne, ACM Computing Surveys (CSUR). 502A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1-35, 2017.\n\nLearning trajectory preferences for manipulators via iterative improvement. A Jain, B Wojcik, T Joachims, A Saxena, Advances in neural information processing systems. A. Jain, B. Wojcik, T. Joachims, and A. Saxena. Learning trajectory preferences for manipulators via iterative improvement. In Advances in neural information processing systems, pages 575-583, 2013.\n\nOptimal sample complexity of m-wise data for top-k ranking. M Jang, S Kim, C Suh, S Oh, Advances in Neural Information Processing Systems. 30M. Jang, S. Kim, C. Suh, and S. Oh. Optimal sample complexity of m-wise data for top-k ranking. Advances in Neural Information Processing Systems, 30, 2017.\n\nIs pessimism provably efficient for offline. Y Jin, Z Yang, Z Wang, International Conference on Machine Learning. PMLRY. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline RL? In International Conference on Machine Learning, pages 5084-5096. PMLR, 2021.\n\nThe depth/breadth trade-off in the design of menu-driven user interfaces. International journal of man-machine studies. J I Kiger, 20J. I. Kiger. The depth/breadth trade-off in the design of menu-driven user interfaces. International journal of man-machine studies, 20(2):201-213, 1984.\n\nTamer: Training an agent manually via evaluative reinforcement. W B Knox, P Stone, 7th IEEE International Conference on Development and Learning. IEEEW. B. Knox and P. Stone. Tamer: Training an agent manually via evaluative reinforcement. In 7th IEEE International Conference on Development and Learning, pages 292-297. IEEE, 2008.\n\nRegret lower bound and optimal algorithm in dueling bandit problem. J Komiyama, J Honda, H Kashima, H Nakagawa, COLT. J. Komiyama, J. Honda, H. Kashima, and H. Nakagawa. Regret lower bound and optimal algorithm in dueling bandit problem. In COLT, pages 1141-1154, 2015.\n\nOffline reinforcement learning with implicit Q-learning. I Kostrikov, A Nair, S Levine, arXiv:2110.06169arXiv preprintI. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit Q-learning. arXiv preprint arXiv:2110.06169, 2021.\n\nConservative Q-learning for offline reinforcement learning. A Kumar, A Zhou, G Tucker, S Levine, Advances in Neural Information Processing Systems. 33A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative Q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179-1191, 2020.\n\nLearning dynamic robot-to-human object handover from human feedback. A Kupcsik, D Hsu, W S Lee, Robotics research. SpringerA. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human feedback. In Robotics research, pages 161-176. Springer, 2018.\n\nM-estimator and maximum likelihood estimator (mle). M.-J Lee, Micro-Econometrics. SpringerM.-j. Lee. M-estimator and maximum likelihood estimator (mle). In Micro-Econometrics, pages 91-132. Springer, 2008.\n\nPessimism for offline linear contextual bandits using \u2113 p confidence sets. G Li, C Ma, N Srebro, arXiv:2205.10671arXiv preprintG. Li, C. Ma, and N. Srebro. Pessimism for offline linear contextual bandits using \u2113 p confidence sets. arXiv preprint arXiv:2205.10671, 2022.\n\nLearning to rank for information retrieval. T.-Y Liu, Foundations and Trends\u00ae in Information Retrieval. 33T.-Y. Liu et al. Learning to rank for information retrieval. Foundations and Trends\u00ae in Information Retrieval, 3(3):225-331, 2009.\n\nIndividual Choice Behavior: A Theoretical Analysis. Courier Corporation. R D Luce, R. D. Luce. Individual Choice Behavior: A Theoretical Analysis. Courier Corporation, 2012.\n\nInteractive learning from policy-dependent human feedback. J Macglashan, M K Ho, R Loftin, B Peng, G Wang, D L Roberts, M E Taylor, M L Littman, International Conference on Machine Learning. PMLRJ. MacGlashan, M. K. Ho, R. Loftin, B. Peng, G. Wang, D. L. Roberts, M. E. Taylor, and M. L. Littman. Interactive learning from policy-dependent human feedback. In International Conference on Machine Learning, pages 2285-2294. PMLR, 2017.\n\nMinimax rates and efficient algorithms for noisy sorting. C Mao, J Weed, P Rigollet, Algorithmic Learning Theory. PMLRC. Mao, J. Weed, and P. Rigollet. Minimax rates and efficient algorithms for noisy sorting. In Algorithmic Learning Theory, pages 821-847. PMLR, 2018.\n\nTeaching language models to support answers with verified quotes. J Menick, M Trebacz, V Mikulik, J Aslanides, F Song, M Chadwick, M Glaese, S Young, L Campbell-Gillingham, G Irving, arXiv:2203.11147arXiv preprintJ. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young, L. Campbell- Gillingham, G. Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\n\nThe magical number seven, plus or minus two: Some limits on our capacity for processing information. G A Miller, Psychological Review. 63281G. A. Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63(2):81, 1956.\n\nR Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. arXiv preprintR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n\nLearning from comparisons and choices. S Negahban, S Oh, K K Thekumparampil, J Xu, The Journal of Machine Learning Research. 191S. Negahban, S. Oh, K. K. Thekumparampil, and J. Xu. Learning from comparisons and choices. The Journal of Machine Learning Research, 19(1):1478-1572, 2018.\n\nTraining parsers by inverse reinforcement learning. G Neu, C Szepesv\u00e1ri, Machine Learning. 77G. Neu and C. Szepesv\u00e1ri. Training parsers by inverse reinforcement learning. Machine Learning, 77(2): 303-337, 2009.\n\nAlgorithms for inverse reinforcement learning. A Y Ng, S Russell, International Conference on Machine Learning. 1A. Y. Ng, S. Russell, et al. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, volume 1, page 2, 2000.\n\nDueling posterior sampling for preference-based reinforcement learning. E R Novoseller, Y Sui, Y Yue, J W Burdick, arXiv:1908.01289arXiv preprintE. R. Novoseller, Y. Sui, Y. Yue, and J. W. Burdick. Dueling posterior sampling for preference-based reinforcement learning. arXiv preprint arXiv:1908.01289, 2019.\n\nTraining language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2203.02155arXiv preprintL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\n\nDueling rl: reinforcement learning with trajectory preferences. A Pacchiano, A Saha, J Lee, arXiv:2111.04850arXiv preprintA. Pacchiano, A. Saha, and J. Lee. Dueling rl: reinforcement learning with trajectory preferences. arXiv preprint arXiv:2111.04850, 2021.\n\nThe analysis of permutations. R L Plackett, Journal of the Royal Statistical Society: Series C (Applied Statistics). 242R. L. Plackett. The analysis of permutations. Journal of the Royal Statistical Society: Series C (Applied Statistics), 24(2):193-202, 1975.\n\nA statistical convergence perspective of algorithms for rank aggregation from pairwise data. A Rajkumar, S Agarwal, International conference on machine learning. PMLRA. Rajkumar and S. Agarwal. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In International conference on machine learning, pages 118-126. PMLR, 2014.\n\nBayesian inverse reinforcement learning. D Ramachandran, E Amir, IJCAI. 7D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In IJCAI, volume 7, pages 2586-2591, 2007.\n\nIs reinforcement learning (not) for natural language processing. R Ramamurthy, P Ammanabrolu, K Brantley, J Hessel, R Sifa, C Bauckhage, H Hajishirzi, Y Choi, arXiv:2210.01241Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprintR. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022.\n\nBridging offline reinforcement learning and imitation learning: A tale of pessimism. P Rashidinejad, B Zhu, C Ma, J Jiao, S Russell, Advances in Neural Information Processing Systems. 34P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34: 11702-11716, 2021.\n\nWhy the magic number seven plus or minus two. T L Saaty, M S Ozdemir, Mathematical and computer modelling. 383-4T. L. Saaty and M. S. Ozdemir. Why the magic number seven plus or minus two. Mathematical and computer modelling, 38(3-4):233-244, 2003.\n\nActive preference-based learning of reward functions. D Sadigh, A D Dragan, S Sastry, S A Seshia, Robotics: Science and Systems. D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions. In Robotics: Science and Systems, 2017.\n\nBattle of bandits. A Saha, A Gopalan, Uncertainty in Artificial Intelligence. A. Saha and A. Gopalan. Battle of bandits. In Uncertainty in Artificial Intelligence, 2018a.\n\nActive ranking with subset-wise preferences. A Saha, A Gopalan, International Conference on Artificial Intelligence and Statistics (AISTATS). A. Saha and A. Gopalan. Active ranking with subset-wise preferences. International Conference on Artificial Intelligence and Statistics (AISTATS), 2018b.\n\nPAC Battling Bandits in the Plackett-Luce Model. A Saha, A Gopalan, Algorithmic Learning Theory. A. Saha and A. Gopalan. PAC Battling Bandits in the Plackett-Luce Model. In Algorithmic Learning Theory, pages 700-737, 2019.\n\nEfficient and optimal algorithms for contextual dueling bandits under realizability. A Saha, A Krishnamurthy, International Conference on Algorithmic Learning Theory. PMLRA. Saha and A. Krishnamurthy. Efficient and optimal algorithms for contextual dueling bandits under realizability. In International Conference on Algorithmic Learning Theory, pages 968-994. PMLR, 2022.\n\nEstimation from pairwise comparisons: Sharp minimax bounds with topology dependence. N Shah, S Balakrishnan, J Bradley, A Parekh, K Ramchandran, M Wainwright, Artificial Intelligence and Statistics. PMLRN. Shah, S. Balakrishnan, J. Bradley, A. Parekh, K. Ramchandran, and M. Wainwright. Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. In Artificial Intelligence and Statistics, pages 856-865. PMLR, 2015.\n\nSimple, robust and optimal ranking from pairwise comparisons. N B Shah, M J Wainwright, The Journal of Machine Learning Research. 181N. B. Shah and M. J. Wainwright. Simple, robust and optimal ranking from pairwise comparisons. The Journal of Machine Learning Research, 18(1):7246-7283, 2017.\n\nSeven plus or minus two: a commentary on capacity limitations. R M Shiffrin, R M Nosofsky, R. M. Shiffrin and R. M. Nosofsky. Seven plus or minus two: a commentary on capacity limitations. 1994.\n\nBenchmarks and algorithms for offline preference-based reward learning. D Shin, A D Dragan, D S Brown, arXiv:2301.01392arXiv preprintD. Shin, A. D. Dragan, and D. S. Brown. Benchmarks and algorithms for offline preference-based reward learning. arXiv preprint arXiv:2301.01392, 2023.\n\nBest-arm identification in linear bandits. M Soare, A Lazaric, R Munos, Advances in Neural Information Processing Systems. 27M. Soare, A. Lazaric, and R. Munos. Best-arm identification in linear bandits. Advances in Neural Information Processing Systems, 27, 2014.\n\nJ(\u03c0) \u2212 H\u27e8\u03b8 \u22c6 , v\u27e9. We have SubOpt(\u03c0 PE ) = J(\u03c0 \u22c6 ) \u2212 J(\u03c0 PE ). J Let, Let J \u2032 (\u03c0) = J(\u03c0) \u2212 H\u27e8\u03b8 \u22c6 , v\u27e9. We have SubOpt(\u03c0 PE ) = J(\u03c0 \u22c6 ) \u2212 J(\u03c0 PE )\n\n. = J \u2032 , \u03c0 \u22c6 ) \u2212 J \u2032 (\u03c0 PE= J \u2032 (\u03c0 \u22c6 ) \u2212 J \u2032 (\u03c0 PE )\n\nSince\u03c0 PE is the optimal policy under expected value\u0134(\u03c0), we know that the second difference satisfie\u015d J(\u03c0 \u22c6 ) \u2212\u0134(\u03c0 PE ) \u2264 0. For the third difference, we hav\u00ea J(\u03c0 PE ) \u2212 J \u2032 (\u03c0 PE ) = E s\u223cd\u03c0PE. r(s,\u03c0 PE (s)) \u2212 r(s,\u03c0 PE (s))Since\u03c0 PE is the optimal policy under expected value\u0134(\u03c0), we know that the second difference satisfie\u015d J(\u03c0 \u22c6 ) \u2212\u0134(\u03c0 PE ) \u2264 0. For the third difference, we hav\u00ea J(\u03c0 PE ) \u2212 J \u2032 (\u03c0 PE ) = E s\u223cd\u03c0PE [r(s,\u03c0 PE (s)) \u2212 r(s,\u03c0 PE (s))].\n\nFrom Lemma 5.1 we know that \u03b8 \u22c6 \u2208 \u0398(\u03b8 MLE , \u03bb) with probability at least 1 \u2212 \u03b4. Thus we know that with probability at least 1 \u2212 \u03b4,\u0134(\u03c0 PE ) \u2212 J \u2032 (\u03c0 PE ) \u2264 0. Now combining everything together, we haveFrom Lemma 5.1 we know that \u03b8 \u22c6 \u2208 \u0398(\u03b8 MLE , \u03bb) with probability at least 1 \u2212 \u03b4. Thus we know that with probability at least 1 \u2212 \u03b4,\u0134(\u03c0 PE ) \u2212 J \u2032 (\u03c0 PE ) \u2264 0. Now combining everything together, we have\n", "annotations": {"author": "[{\"end\":105,\"start\":93},{\"end\":160,\"start\":106},{\"end\":211,\"start\":161},{\"end\":285,\"start\":212}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":101},{\"end\":122,\"start\":116},{\"end\":173,\"start\":169}]", "author_first_name": "[{\"end\":100,\"start\":93},{\"end\":113,\"start\":106},{\"end\":115,\"start\":114},{\"end\":168,\"start\":161}]", "author_affiliation": "[{\"end\":159,\"start\":124},{\"end\":210,\"start\":175},{\"end\":284,\"start\":213}]", "title": "[{\"end\":90,\"start\":1},{\"end\":375,\"start\":286}]", "venue": null, "abstract": "[{\"end\":1376,\"start\":377}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1894,\"start\":1869},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":1914,\"start\":1894},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1939,\"start\":1914},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1960,\"start\":1939},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1978,\"start\":1960},{\"end\":1997,\"start\":1978},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2018,\"start\":1997},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2042,\"start\":2018},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2067,\"start\":2042},{\"end\":2088,\"start\":2067},{\"end\":2107,\"start\":2088},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2125,\"start\":2107},{\"end\":2146,\"start\":2125},{\"end\":2168,\"start\":2146},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2184,\"start\":2168},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2204,\"start\":2184},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2224,\"start\":2204},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2244,\"start\":2224},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2264,\"start\":2244},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2281,\"start\":2264},{\"end\":2299,\"start\":2281},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2320,\"start\":2299},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2344,\"start\":2320},{\"end\":3053,\"start\":3021},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3851,\"start\":3833},{\"end\":3868,\"start\":3851},{\"end\":3885,\"start\":3868},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3910,\"start\":3885},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3930,\"start\":3910},{\"end\":4212,\"start\":4207},{\"end\":4601,\"start\":4600},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5318,\"start\":5302},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5329,\"start\":5318},{\"end\":5579,\"start\":5554},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5658,\"start\":5633},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5680,\"start\":5660},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6169,\"start\":6152},{\"end\":6961,\"start\":6940},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7022,\"start\":6994},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7049,\"start\":7024},{\"end\":7051,\"start\":7050},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7073,\"start\":7051},{\"end\":7308,\"start\":7286},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7333,\"start\":7310},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8446,\"start\":8425},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8883,\"start\":8862},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10441,\"start\":10421},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10557,\"start\":10537},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10603,\"start\":10579},{\"end\":10663,\"start\":10643},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10770,\"start\":10750},{\"end\":11110,\"start\":11092},{\"end\":11130,\"start\":11110},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11137,\"start\":11130},{\"end\":11158,\"start\":11137},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11187,\"start\":11158},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11210,\"start\":11187},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11234,\"start\":11210},{\"end\":11253,\"start\":11234},{\"end\":11273,\"start\":11253},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11295,\"start\":11273},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11315,\"start\":11295},{\"end\":11336,\"start\":11315},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11355,\"start\":11336},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11382,\"start\":11357},{\"end\":11398,\"start\":11382},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11570,\"start\":11546},{\"end\":11588,\"start\":11570},{\"end\":11613,\"start\":11590},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12076,\"start\":12056},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12094,\"start\":12076},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12120,\"start\":12094},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12140,\"start\":12120},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12157,\"start\":12140},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12175,\"start\":12157},{\"end\":12193,\"start\":12175},{\"end\":12212,\"start\":12193},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12239,\"start\":12212},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12261,\"start\":12239},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12280,\"start\":12261},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12300,\"start\":12280},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12516,\"start\":12498},{\"end\":12533,\"start\":12516},{\"end\":12550,\"start\":12533},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12575,\"start\":12550},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12595,\"start\":12575},{\"end\":12614,\"start\":12595},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12632,\"start\":12614},{\"end\":12657,\"start\":12632},{\"end\":12675,\"start\":12657},{\"end\":12695,\"start\":12675},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12720,\"start\":12695},{\"end\":12745,\"start\":12720},{\"end\":12844,\"start\":12826},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13385,\"start\":13368},{\"end\":13405,\"start\":13385},{\"end\":13426,\"start\":13405},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13454,\"start\":13426},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13479,\"start\":13454},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13498,\"start\":13479},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13520,\"start\":13498},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13541,\"start\":13520},{\"end\":13817,\"start\":13795},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14121,\"start\":14103},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14147,\"start\":14121},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14163,\"start\":14147},{\"end\":14181,\"start\":14163},{\"end\":14195,\"start\":14181},{\"end\":14216,\"start\":14195},{\"end\":14234,\"start\":14216},{\"end\":14253,\"start\":14234},{\"end\":14805,\"start\":14802},{\"end\":16057,\"start\":16054},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18037,\"start\":18017},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18773,\"start\":18757},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18784,\"start\":18773},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20477,\"start\":20460},{\"end\":20484,\"start\":20478},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":21391,\"start\":21373},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22009,\"start\":21991},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22291,\"start\":22268},{\"end\":22794,\"start\":22765},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24085,\"start\":24058},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24101,\"start\":24085},{\"end\":24119,\"start\":24101},{\"end\":24133,\"start\":24119},{\"end\":24154,\"start\":24133},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24858,\"start\":24831},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24874,\"start\":24858},{\"end\":24892,\"start\":24874},{\"end\":24906,\"start\":24892},{\"end\":24927,\"start\":24906},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26337,\"start\":26317},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26383,\"start\":26359},{\"end\":26443,\"start\":26423},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26558,\"start\":26538},{\"end\":27368,\"start\":27339},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27387,\"start\":27368},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27842,\"start\":27824},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28845,\"start\":28828},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30623,\"start\":30605},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30832,\"start\":30818},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30844,\"start\":30832},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":30872,\"start\":30844},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30896,\"start\":30872},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30936,\"start\":30912},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31721,\"start\":31703},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33166,\"start\":33146},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34004,\"start\":33984},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34332,\"start\":34318},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34344,\"start\":34334},{\"end\":40905,\"start\":40884},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":44671,\"start\":44650},{\"end\":54975,\"start\":54948},{\"end\":59628,\"start\":59600},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":59729,\"start\":59711},{\"end\":64801,\"start\":64774},{\"end\":69771,\"start\":69744},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":70921,\"start\":70895},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":70943,\"start\":70921},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":71342,\"start\":71318},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":71367,\"start\":71342},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":71438,\"start\":71420},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":71520,\"start\":71504},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":71540,\"start\":71520},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":71560,\"start\":71540},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":71580,\"start\":71560},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":71600,\"start\":71580},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":71617,\"start\":71600},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":71657,\"start\":71636},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":71681,\"start\":71657},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":71729,\"start\":71711}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":70891,\"start\":70809},{\"attributes\":{\"id\":\"fig_1\"},\"end\":71115,\"start\":70892},{\"attributes\":{\"id\":\"fig_2\"},\"end\":71314,\"start\":71116},{\"attributes\":{\"id\":\"fig_3\"},\"end\":71980,\"start\":71315},{\"attributes\":{\"id\":\"fig_4\"},\"end\":72135,\"start\":71981},{\"attributes\":{\"id\":\"fig_5\"},\"end\":74633,\"start\":72136},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":79084,\"start\":74634}]", "paragraph": "[{\"end\":3012,\"start\":1392},{\"end\":3149,\"start\":3014},{\"end\":3403,\"start\":3151},{\"end\":3931,\"start\":3405},{\"end\":4702,\"start\":3933},{\"end\":5330,\"start\":4704},{\"end\":5462,\"start\":5461},{\"end\":5681,\"start\":5464},{\"end\":6064,\"start\":5683},{\"end\":6631,\"start\":6066},{\"end\":7074,\"start\":6633},{\"end\":7409,\"start\":7091},{\"end\":7534,\"start\":7411},{\"end\":7672,\"start\":7577},{\"end\":7961,\"start\":7674},{\"end\":8142,\"start\":7963},{\"end\":8292,\"start\":8178},{\"end\":9175,\"start\":8326},{\"end\":9314,\"start\":9199},{\"end\":9909,\"start\":9515},{\"end\":10771,\"start\":10114},{\"end\":11891,\"start\":10788},{\"end\":12925,\"start\":11893},{\"end\":13995,\"start\":12927},{\"end\":14345,\"start\":13997},{\"end\":14592,\"start\":14363},{\"end\":14871,\"start\":14594},{\"end\":14998,\"start\":14873},{\"end\":15095,\"start\":15028},{\"end\":16204,\"start\":15129},{\"end\":16781,\"start\":16270},{\"end\":17014,\"start\":16825},{\"end\":17205,\"start\":17016},{\"end\":17592,\"start\":17207},{\"end\":17687,\"start\":17594},{\"end\":17927,\"start\":17689},{\"end\":18925,\"start\":18011},{\"end\":19003,\"start\":19002},{\"end\":19084,\"start\":19005},{\"end\":19178,\"start\":19177},{\"end\":19746,\"start\":19180},{\"end\":20198,\"start\":19763},{\"end\":20320,\"start\":20236},{\"end\":20622,\"start\":20360},{\"end\":21630,\"start\":20992},{\"end\":21733,\"start\":21686},{\"end\":21904,\"start\":21878},{\"end\":22396,\"start\":21906},{\"end\":22456,\"start\":22398},{\"end\":22830,\"start\":22641},{\"end\":23112,\"start\":22912},{\"end\":23442,\"start\":23114},{\"end\":24425,\"start\":23728},{\"end\":24751,\"start\":24427},{\"end\":25130,\"start\":24753},{\"end\":25504,\"start\":25178},{\"end\":25743,\"start\":25630},{\"end\":26073,\"start\":25786},{\"end\":26799,\"start\":26075},{\"end\":27443,\"start\":26801},{\"end\":27722,\"start\":27445},{\"end\":28134,\"start\":27780},{\"end\":28470,\"start\":28170},{\"end\":28547,\"start\":28496},{\"end\":28617,\"start\":28583},{\"end\":28909,\"start\":28619},{\"end\":29117,\"start\":28911},{\"end\":29463,\"start\":29208},{\"end\":29588,\"start\":29465},{\"end\":29795,\"start\":29633},{\"end\":30210,\"start\":29832},{\"end\":30452,\"start\":30293},{\"end\":31054,\"start\":30592},{\"end\":31143,\"start\":31056},{\"end\":31542,\"start\":31327},{\"end\":32001,\"start\":31649},{\"end\":32627,\"start\":32003},{\"end\":32897,\"start\":32629},{\"end\":33073,\"start\":33072},{\"end\":33254,\"start\":33075},{\"end\":33343,\"start\":33256},{\"end\":33757,\"start\":33524},{\"end\":34476,\"start\":33857},{\"end\":35390,\"start\":35179},{\"end\":35682,\"start\":35412},{\"end\":36361,\"start\":35714},{\"end\":36490,\"start\":36489},{\"end\":36854,\"start\":36853},{\"end\":37064,\"start\":36856},{\"end\":37171,\"start\":37066},{\"end\":37231,\"start\":37227},{\"end\":37392,\"start\":37345},{\"end\":37686,\"start\":37394},{\"end\":37861,\"start\":37688},{\"end\":38115,\"start\":37901},{\"end\":38306,\"start\":38117},{\"end\":38553,\"start\":38409},{\"end\":38763,\"start\":38581},{\"end\":39453,\"start\":38765},{\"end\":39565,\"start\":39455},{\"end\":39634,\"start\":39567},{\"end\":39737,\"start\":39636},{\"end\":39831,\"start\":39795},{\"end\":40395,\"start\":39923},{\"end\":40840,\"start\":40446},{\"end\":41296,\"start\":40865},{\"end\":41390,\"start\":41389},{\"end\":41515,\"start\":41392},{\"end\":41798,\"start\":41549},{\"end\":41949,\"start\":41948},{\"end\":42347,\"start\":41951},{\"end\":42427,\"start\":42349},{\"end\":42826,\"start\":42677},{\"end\":43032,\"start\":42828},{\"end\":43513,\"start\":43149},{\"end\":43678,\"start\":43534},{\"end\":44055,\"start\":43680},{\"end\":44175,\"start\":44174},{\"end\":44352,\"start\":44177},{\"end\":45385,\"start\":44520},{\"end\":46156,\"start\":45387},{\"end\":46446,\"start\":46171},{\"end\":46592,\"start\":46448},{\"end\":46947,\"start\":46594},{\"end\":47295,\"start\":46949},{\"end\":47665,\"start\":47297},{\"end\":47867,\"start\":47680},{\"end\":47959,\"start\":47869},{\"end\":48196,\"start\":48195},{\"end\":48351,\"start\":48198},{\"end\":48450,\"start\":48353},{\"end\":48790,\"start\":48597},{\"end\":49489,\"start\":49018},{\"end\":49769,\"start\":49491},{\"end\":49939,\"start\":49938},{\"end\":49995,\"start\":49941},{\"end\":50301,\"start\":50023},{\"end\":50432,\"start\":50303},{\"end\":50491,\"start\":50461},{\"end\":50907,\"start\":50519},{\"end\":51188,\"start\":50909},{\"end\":51267,\"start\":51236},{\"end\":51797,\"start\":51660},{\"end\":51983,\"start\":51799},{\"end\":52150,\"start\":51985},{\"end\":52315,\"start\":52213},{\"end\":52647,\"start\":52646},{\"end\":52694,\"start\":52649},{\"end\":52912,\"start\":52725},{\"end\":53053,\"start\":52989},{\"end\":53468,\"start\":53055},{\"end\":53749,\"start\":53536},{\"end\":53903,\"start\":53804},{\"end\":54089,\"start\":54028},{\"end\":54226,\"start\":54205},{\"end\":54562,\"start\":54228},{\"end\":55025,\"start\":54729},{\"end\":55140,\"start\":55090},{\"end\":55366,\"start\":55252},{\"end\":55441,\"start\":55395},{\"end\":55741,\"start\":55578},{\"end\":56080,\"start\":55847},{\"end\":56417,\"start\":56360},{\"end\":56664,\"start\":56562},{\"end\":56786,\"start\":56762},{\"end\":57111,\"start\":56788},{\"end\":57282,\"start\":57204},{\"end\":57659,\"start\":57314},{\"end\":57769,\"start\":57661},{\"end\":57986,\"start\":57771},{\"end\":58087,\"start\":58035},{\"end\":58209,\"start\":58135},{\"end\":58353,\"start\":58239},{\"end\":58641,\"start\":58355},{\"end\":58646,\"start\":58643},{\"end\":59325,\"start\":58856},{\"end\":59793,\"start\":59465},{\"end\":59891,\"start\":59868},{\"end\":60069,\"start\":60009},{\"end\":60243,\"start\":60098},{\"end\":60330,\"start\":60245},{\"end\":60425,\"start\":60394},{\"end\":60566,\"start\":60565},{\"end\":60651,\"start\":60568},{\"end\":60844,\"start\":60679},{\"end\":61010,\"start\":60964},{\"end\":61170,\"start\":61124},{\"end\":61232,\"start\":61172},{\"end\":61566,\"start\":61488},{\"end\":61768,\"start\":61696},{\"end\":62228,\"start\":62146},{\"end\":62508,\"start\":62230},{\"end\":62789,\"start\":62576},{\"end\":62943,\"start\":62844},{\"end\":63429,\"start\":63116},{\"end\":64400,\"start\":63667},{\"end\":64637,\"start\":64464},{\"end\":64886,\"start\":64686},{\"end\":64963,\"start\":64888},{\"end\":65136,\"start\":64965},{\"end\":65523,\"start\":65138},{\"end\":65576,\"start\":65525},{\"end\":65841,\"start\":65657},{\"end\":66044,\"start\":65843},{\"end\":66218,\"start\":66073},{\"end\":66305,\"start\":66220},{\"end\":66427,\"start\":66364},{\"end\":66601,\"start\":66460},{\"end\":66604,\"start\":66603},{\"end\":66716,\"start\":66606},{\"end\":66953,\"start\":66718},{\"end\":67152,\"start\":66955},{\"end\":67341,\"start\":67154},{\"end\":67427,\"start\":67343},{\"end\":67709,\"start\":67627},{\"end\":67993,\"start\":67711},{\"end\":68284,\"start\":68061},{\"end\":68448,\"start\":68349},{\"end\":68569,\"start\":68493},{\"end\":68841,\"start\":68571},{\"end\":69369,\"start\":68964},{\"end\":69654,\"start\":69433},{\"end\":69861,\"start\":69656},{\"end\":70205,\"start\":69863},{\"end\":70420,\"start\":70207},{\"end\":70574,\"start\":70475},{\"end\":70808,\"start\":70699}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5460,\"start\":5331},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7576,\"start\":7535},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8177,\"start\":8143},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8325,\"start\":8293},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9198,\"start\":9176},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9514,\"start\":9315},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10113,\"start\":9910},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15128,\"start\":15096},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16269,\"start\":16205},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16824,\"start\":16782},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17968,\"start\":17928},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19001,\"start\":18926},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19176,\"start\":19085},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20991,\"start\":20623},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21685,\"start\":21631},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21877,\"start\":21734},{\"attributes\":{\"id\":\"formula_16\"},\"end\":22640,\"start\":22457},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22911,\"start\":22831},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23727,\"start\":23443},{\"attributes\":{\"id\":\"formula_19\"},\"end\":25177,\"start\":25131},{\"attributes\":{\"id\":\"formula_20\"},\"end\":25629,\"start\":25505},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25785,\"start\":25744},{\"attributes\":{\"id\":\"formula_22\"},\"end\":27779,\"start\":27723},{\"attributes\":{\"id\":\"formula_23\"},\"end\":28495,\"start\":28471},{\"attributes\":{\"id\":\"formula_24\"},\"end\":28582,\"start\":28548},{\"attributes\":{\"id\":\"formula_25\"},\"end\":29207,\"start\":29118},{\"attributes\":{\"id\":\"formula_26\"},\"end\":29632,\"start\":29589},{\"attributes\":{\"id\":\"formula_27\"},\"end\":30292,\"start\":30224},{\"attributes\":{\"id\":\"formula_28\"},\"end\":30591,\"start\":30453},{\"attributes\":{\"id\":\"formula_29\"},\"end\":31326,\"start\":31144},{\"attributes\":{\"id\":\"formula_30\"},\"end\":31648,\"start\":31543},{\"attributes\":{\"id\":\"formula_31\"},\"end\":33071,\"start\":32898},{\"attributes\":{\"id\":\"formula_32\"},\"end\":33523,\"start\":33344},{\"attributes\":{\"id\":\"formula_33\"},\"end\":33856,\"start\":33758},{\"attributes\":{\"id\":\"formula_34\"},\"end\":35178,\"start\":34477},{\"attributes\":{\"id\":\"formula_35\"},\"end\":36488,\"start\":36362},{\"attributes\":{\"id\":\"formula_36\"},\"end\":36852,\"start\":36491},{\"attributes\":{\"id\":\"formula_37\"},\"end\":37226,\"start\":37172},{\"attributes\":{\"id\":\"formula_38\"},\"end\":37344,\"start\":37232},{\"attributes\":{\"id\":\"formula_39\"},\"end\":37900,\"start\":37862},{\"attributes\":{\"id\":\"formula_40\"},\"end\":38408,\"start\":38307},{\"attributes\":{\"id\":\"formula_41\"},\"end\":39794,\"start\":39738},{\"attributes\":{\"id\":\"formula_42\"},\"end\":39922,\"start\":39832},{\"attributes\":{\"id\":\"formula_43\"},\"end\":41388,\"start\":41297},{\"attributes\":{\"id\":\"formula_44\"},\"end\":41548,\"start\":41516},{\"attributes\":{\"id\":\"formula_45\"},\"end\":41947,\"start\":41799},{\"attributes\":{\"id\":\"formula_46\"},\"end\":42676,\"start\":42428},{\"attributes\":{\"id\":\"formula_47\"},\"end\":43148,\"start\":43033},{\"attributes\":{\"id\":\"formula_48\"},\"end\":44173,\"start\":44056},{\"attributes\":{\"id\":\"formula_49\"},\"end\":44505,\"start\":44353},{\"attributes\":{\"id\":\"formula_50\"},\"end\":48194,\"start\":47960},{\"attributes\":{\"id\":\"formula_51\"},\"end\":48596,\"start\":48451},{\"attributes\":{\"id\":\"formula_52\"},\"end\":49017,\"start\":48791},{\"attributes\":{\"id\":\"formula_53\"},\"end\":49937,\"start\":49770},{\"attributes\":{\"id\":\"formula_54\"},\"end\":50022,\"start\":49996},{\"attributes\":{\"id\":\"formula_55\"},\"end\":50460,\"start\":50433},{\"attributes\":{\"id\":\"formula_56\"},\"end\":50518,\"start\":50492},{\"attributes\":{\"id\":\"formula_57\"},\"end\":51659,\"start\":51268},{\"attributes\":{\"id\":\"formula_58\"},\"end\":52212,\"start\":52151},{\"attributes\":{\"id\":\"formula_59\"},\"end\":52645,\"start\":52316},{\"attributes\":{\"id\":\"formula_60\"},\"end\":52724,\"start\":52695},{\"attributes\":{\"id\":\"formula_61\"},\"end\":52988,\"start\":52913},{\"attributes\":{\"id\":\"formula_62\"},\"end\":53535,\"start\":53469},{\"attributes\":{\"id\":\"formula_63\"},\"end\":53803,\"start\":53750},{\"attributes\":{\"id\":\"formula_64\"},\"end\":54027,\"start\":53904},{\"attributes\":{\"id\":\"formula_65\"},\"end\":54204,\"start\":54090},{\"attributes\":{\"id\":\"formula_66\"},\"end\":54728,\"start\":54563},{\"attributes\":{\"id\":\"formula_67\"},\"end\":55089,\"start\":55026},{\"attributes\":{\"id\":\"formula_68\"},\"end\":55251,\"start\":55141},{\"attributes\":{\"id\":\"formula_69\"},\"end\":55577,\"start\":55442},{\"attributes\":{\"id\":\"formula_70\"},\"end\":55846,\"start\":55742},{\"attributes\":{\"id\":\"formula_71\"},\"end\":56359,\"start\":56081},{\"attributes\":{\"id\":\"formula_72\"},\"end\":56561,\"start\":56418},{\"attributes\":{\"id\":\"formula_73\"},\"end\":56761,\"start\":56665},{\"attributes\":{\"id\":\"formula_74\"},\"end\":57203,\"start\":57112},{\"attributes\":{\"id\":\"formula_75\"},\"end\":57313,\"start\":57283},{\"attributes\":{\"id\":\"formula_76\"},\"end\":58034,\"start\":57987},{\"attributes\":{\"id\":\"formula_77\"},\"end\":58134,\"start\":58088},{\"attributes\":{\"id\":\"formula_78\"},\"end\":58855,\"start\":58647},{\"attributes\":{\"id\":\"formula_79\"},\"end\":59464,\"start\":59326},{\"attributes\":{\"id\":\"formula_80\"},\"end\":59805,\"start\":59794},{\"attributes\":{\"id\":\"formula_81\"},\"end\":59867,\"start\":59805},{\"attributes\":{\"id\":\"formula_82\"},\"end\":60008,\"start\":59892},{\"attributes\":{\"id\":\"formula_83\"},\"end\":60393,\"start\":60331},{\"attributes\":{\"id\":\"formula_84\"},\"end\":60564,\"start\":60426},{\"attributes\":{\"id\":\"formula_85\"},\"end\":60678,\"start\":60652},{\"attributes\":{\"id\":\"formula_86\"},\"end\":60963,\"start\":60845},{\"attributes\":{\"id\":\"formula_87\"},\"end\":61123,\"start\":61011},{\"attributes\":{\"id\":\"formula_88\"},\"end\":61487,\"start\":61233},{\"attributes\":{\"id\":\"formula_89\"},\"end\":61695,\"start\":61567},{\"attributes\":{\"id\":\"formula_90\"},\"end\":62145,\"start\":61769},{\"attributes\":{\"id\":\"formula_91\"},\"end\":62575,\"start\":62509},{\"attributes\":{\"id\":\"formula_92\"},\"end\":62843,\"start\":62790},{\"attributes\":{\"id\":\"formula_93\"},\"end\":63115,\"start\":62944},{\"attributes\":{\"id\":\"formula_94\"},\"end\":63666,\"start\":63430},{\"attributes\":{\"id\":\"formula_95\"},\"end\":64463,\"start\":64401},{\"attributes\":{\"id\":\"formula_96\"},\"end\":64685,\"start\":64638},{\"attributes\":{\"id\":\"formula_97\"},\"end\":65656,\"start\":65577},{\"attributes\":{\"id\":\"formula_98\"},\"end\":66363,\"start\":66306},{\"attributes\":{\"id\":\"formula_99\"},\"end\":66459,\"start\":66428},{\"attributes\":{\"id\":\"formula_100\"},\"end\":67626,\"start\":67428},{\"attributes\":{\"id\":\"formula_101\"},\"end\":68060,\"start\":67994},{\"attributes\":{\"id\":\"formula_102\"},\"end\":68348,\"start\":68285},{\"attributes\":{\"id\":\"formula_103\"},\"end\":68492,\"start\":68449},{\"attributes\":{\"id\":\"formula_105\"},\"end\":68963,\"start\":68842},{\"attributes\":{\"id\":\"formula_106\"},\"end\":69432,\"start\":69370},{\"attributes\":{\"id\":\"formula_107\"},\"end\":70474,\"start\":70421},{\"attributes\":{\"id\":\"formula_108\"},\"end\":70698,\"start\":70575}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1390,\"start\":1378},{\"attributes\":{\"n\":\"1.1\"},\"end\":7089,\"start\":7077},{\"attributes\":{\"n\":\"1.2\"},\"end\":10786,\"start\":10774},{\"attributes\":{\"n\":\"2\"},\"end\":14361,\"start\":14348},{\"attributes\":{\"n\":\"2.1\"},\"end\":15026,\"start\":15001},{\"attributes\":{\"n\":\"2.2\"},\"end\":18009,\"start\":17970},{\"attributes\":{\"n\":\"2.3\"},\"end\":19761,\"start\":19749},{\"attributes\":{\"n\":\"3\"},\"end\":20234,\"start\":20201},{\"attributes\":{\"n\":\"3.1\"},\"end\":20358,\"start\":20323},{\"attributes\":{\"n\":\"3.2\"},\"end\":28168,\"start\":28137},{\"attributes\":{\"n\":\"4\"},\"end\":29830,\"start\":29798},{\"attributes\":{\"n\":\"4.1\"},\"end\":30223,\"start\":30213},{\"attributes\":{\"n\":\"5\"},\"end\":35410,\"start\":35393},{\"attributes\":{\"n\":\"5.1\"},\"end\":35712,\"start\":35685},{\"attributes\":{\"n\":\"5.2\"},\"end\":38579,\"start\":38556},{\"attributes\":{\"n\":\"6\"},\"end\":40444,\"start\":40398},{\"attributes\":{\"n\":\"6.1\"},\"end\":40863,\"start\":40843},{\"attributes\":{\"n\":\"6.2\"},\"end\":43532,\"start\":43516},{\"attributes\":{\"n\":\"7\"},\"end\":44518,\"start\":44507},{\"attributes\":{\"n\":\"8\"},\"end\":46169,\"start\":46159},{\"end\":47678,\"start\":47668},{\"end\":51209,\"start\":51191},{\"end\":51234,\"start\":51212},{\"end\":55393,\"start\":55369},{\"end\":58237,\"start\":58212},{\"end\":60096,\"start\":60072},{\"end\":66071,\"start\":66047},{\"end\":71992,\"start\":71982}]", "table": "[{\"end\":79084,\"start\":75343}]", "figure_caption": "[{\"end\":70891,\"start\":70811},{\"end\":71115,\"start\":70894},{\"end\":71314,\"start\":71118},{\"end\":71980,\"start\":71317},{\"end\":72135,\"start\":71994},{\"end\":74633,\"start\":72138},{\"end\":75343,\"start\":74636}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":44878,\"start\":44872},{\"end\":45677,\"start\":45669},{\"end\":46047,\"start\":46039},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":50298,\"start\":50289},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":50905,\"start\":50869}]", "bib_author_first_name": "[{\"end\":84627,\"start\":84626},{\"end\":84629,\"start\":84628},{\"end\":84643,\"start\":84642},{\"end\":84652,\"start\":84651},{\"end\":84661,\"start\":84660},{\"end\":84671,\"start\":84670},{\"end\":84679,\"start\":84678},{\"end\":84982,\"start\":84981},{\"end\":84984,\"start\":84983},{\"end\":84998,\"start\":84997},{\"end\":85007,\"start\":85006},{\"end\":85016,\"start\":85015},{\"end\":85026,\"start\":85025},{\"end\":85034,\"start\":85033},{\"end\":85352,\"start\":85351},{\"end\":85361,\"start\":85360},{\"end\":85372,\"start\":85371},{\"end\":85386,\"start\":85385},{\"end\":85664,\"start\":85663},{\"end\":85673,\"start\":85672},{\"end\":85685,\"start\":85684},{\"end\":85694,\"start\":85693},{\"end\":85894,\"start\":85893},{\"end\":85906,\"start\":85905},{\"end\":85915,\"start\":85914},{\"end\":85923,\"start\":85922},{\"end\":85925,\"start\":85924},{\"end\":85936,\"start\":85935},{\"end\":85945,\"start\":85944},{\"end\":85954,\"start\":85953},{\"end\":85962,\"start\":85961},{\"end\":85969,\"start\":85968},{\"end\":85981,\"start\":85980},{\"end\":86318,\"start\":86317},{\"end\":86328,\"start\":86327},{\"end\":86337,\"start\":86336},{\"end\":86784,\"start\":86783},{\"end\":86795,\"start\":86794},{\"end\":86805,\"start\":86804},{\"end\":86816,\"start\":86815},{\"end\":86826,\"start\":86825},{\"end\":86833,\"start\":86832},{\"end\":86845,\"start\":86844},{\"end\":86853,\"start\":86852},{\"end\":86862,\"start\":86861},{\"end\":86874,\"start\":86873},{\"end\":87215,\"start\":87214},{\"end\":87222,\"start\":87221},{\"end\":87234,\"start\":87233},{\"end\":87486,\"start\":87485},{\"end\":87497,\"start\":87496},{\"end\":87790,\"start\":87789},{\"end\":87800,\"start\":87799},{\"end\":87812,\"start\":87811},{\"end\":87826,\"start\":87825},{\"end\":87839,\"start\":87838},{\"end\":87849,\"start\":87848},{\"end\":87859,\"start\":87858},{\"end\":87867,\"start\":87866},{\"end\":88049,\"start\":88048},{\"end\":88061,\"start\":88060},{\"end\":88306,\"start\":88305},{\"end\":88308,\"start\":88307},{\"end\":88550,\"start\":88549},{\"end\":88559,\"start\":88558},{\"end\":88565,\"start\":88564},{\"end\":88810,\"start\":88809},{\"end\":88820,\"start\":88819},{\"end\":88834,\"start\":88833},{\"end\":88849,\"start\":88848},{\"end\":89230,\"start\":89229},{\"end\":89240,\"start\":89239},{\"end\":89242,\"start\":89241},{\"end\":89250,\"start\":89249},{\"end\":89265,\"start\":89264},{\"end\":89267,\"start\":89266},{\"end\":89542,\"start\":89541},{\"end\":89548,\"start\":89547},{\"end\":89804,\"start\":89803},{\"end\":89811,\"start\":89810},{\"end\":89821,\"start\":89820},{\"end\":90083,\"start\":90082},{\"end\":90094,\"start\":90093},{\"end\":90096,\"start\":90095},{\"end\":90105,\"start\":90104},{\"end\":90114,\"start\":90113},{\"end\":90380,\"start\":90379},{\"end\":90388,\"start\":90387},{\"end\":90398,\"start\":90397},{\"end\":90410,\"start\":90409},{\"end\":90731,\"start\":90730},{\"end\":90739,\"start\":90738},{\"end\":90746,\"start\":90745},{\"end\":90753,\"start\":90752},{\"end\":91015,\"start\":91014},{\"end\":91022,\"start\":91021},{\"end\":91030,\"start\":91029},{\"end\":91365,\"start\":91364},{\"end\":91367,\"start\":91366},{\"end\":91597,\"start\":91596},{\"end\":91599,\"start\":91598},{\"end\":91607,\"start\":91606},{\"end\":91934,\"start\":91933},{\"end\":91946,\"start\":91945},{\"end\":91955,\"start\":91954},{\"end\":91966,\"start\":91965},{\"end\":92194,\"start\":92193},{\"end\":92207,\"start\":92206},{\"end\":92215,\"start\":92214},{\"end\":92450,\"start\":92449},{\"end\":92459,\"start\":92458},{\"end\":92467,\"start\":92466},{\"end\":92477,\"start\":92476},{\"end\":92786,\"start\":92785},{\"end\":92797,\"start\":92796},{\"end\":92804,\"start\":92803},{\"end\":92806,\"start\":92805},{\"end\":93053,\"start\":93049},{\"end\":93280,\"start\":93279},{\"end\":93286,\"start\":93285},{\"end\":93292,\"start\":93291},{\"end\":93523,\"start\":93519},{\"end\":93787,\"start\":93786},{\"end\":93789,\"start\":93788},{\"end\":93948,\"start\":93947},{\"end\":93962,\"start\":93961},{\"end\":93964,\"start\":93963},{\"end\":93970,\"start\":93969},{\"end\":93980,\"start\":93979},{\"end\":93988,\"start\":93987},{\"end\":93996,\"start\":93995},{\"end\":93998,\"start\":93997},{\"end\":94009,\"start\":94008},{\"end\":94011,\"start\":94010},{\"end\":94021,\"start\":94020},{\"end\":94023,\"start\":94022},{\"end\":94382,\"start\":94381},{\"end\":94389,\"start\":94388},{\"end\":94397,\"start\":94396},{\"end\":94660,\"start\":94659},{\"end\":94670,\"start\":94669},{\"end\":94681,\"start\":94680},{\"end\":94692,\"start\":94691},{\"end\":94705,\"start\":94704},{\"end\":94713,\"start\":94712},{\"end\":94725,\"start\":94724},{\"end\":94735,\"start\":94734},{\"end\":94744,\"start\":94743},{\"end\":94767,\"start\":94766},{\"end\":95149,\"start\":95148},{\"end\":95151,\"start\":95150},{\"end\":95342,\"start\":95341},{\"end\":95352,\"start\":95351},{\"end\":95362,\"start\":95361},{\"end\":95372,\"start\":95371},{\"end\":95378,\"start\":95377},{\"end\":95388,\"start\":95387},{\"end\":95395,\"start\":95394},{\"end\":95404,\"start\":95403},{\"end\":95412,\"start\":95411},{\"end\":95424,\"start\":95423},{\"end\":95778,\"start\":95777},{\"end\":95790,\"start\":95789},{\"end\":95796,\"start\":95795},{\"end\":95798,\"start\":95797},{\"end\":95816,\"start\":95815},{\"end\":96077,\"start\":96076},{\"end\":96084,\"start\":96083},{\"end\":96284,\"start\":96283},{\"end\":96286,\"start\":96285},{\"end\":96292,\"start\":96291},{\"end\":96572,\"start\":96571},{\"end\":96574,\"start\":96573},{\"end\":96588,\"start\":96587},{\"end\":96595,\"start\":96594},{\"end\":96602,\"start\":96601},{\"end\":96604,\"start\":96603},{\"end\":96879,\"start\":96878},{\"end\":96889,\"start\":96888},{\"end\":96895,\"start\":96894},{\"end\":96904,\"start\":96903},{\"end\":96915,\"start\":96914},{\"end\":96917,\"start\":96916},{\"end\":96931,\"start\":96930},{\"end\":96942,\"start\":96941},{\"end\":96951,\"start\":96950},{\"end\":96962,\"start\":96961},{\"end\":96971,\"start\":96970},{\"end\":97298,\"start\":97297},{\"end\":97311,\"start\":97310},{\"end\":97319,\"start\":97318},{\"end\":97525,\"start\":97524},{\"end\":97527,\"start\":97526},{\"end\":97849,\"start\":97848},{\"end\":97861,\"start\":97860},{\"end\":98161,\"start\":98160},{\"end\":98177,\"start\":98176},{\"end\":98372,\"start\":98371},{\"end\":98386,\"start\":98385},{\"end\":98401,\"start\":98400},{\"end\":98413,\"start\":98412},{\"end\":98423,\"start\":98422},{\"end\":98431,\"start\":98430},{\"end\":98444,\"start\":98443},{\"end\":98458,\"start\":98457},{\"end\":98963,\"start\":98962},{\"end\":98979,\"start\":98978},{\"end\":98986,\"start\":98985},{\"end\":98992,\"start\":98991},{\"end\":99000,\"start\":98999},{\"end\":99327,\"start\":99326},{\"end\":99329,\"start\":99328},{\"end\":99338,\"start\":99337},{\"end\":99340,\"start\":99339},{\"end\":99585,\"start\":99584},{\"end\":99595,\"start\":99594},{\"end\":99597,\"start\":99596},{\"end\":99607,\"start\":99606},{\"end\":99617,\"start\":99616},{\"end\":99619,\"start\":99618},{\"end\":99828,\"start\":99827},{\"end\":99836,\"start\":99835},{\"end\":100026,\"start\":100025},{\"end\":100034,\"start\":100033},{\"end\":100327,\"start\":100326},{\"end\":100335,\"start\":100334},{\"end\":100587,\"start\":100586},{\"end\":100595,\"start\":100594},{\"end\":100961,\"start\":100960},{\"end\":100969,\"start\":100968},{\"end\":100985,\"start\":100984},{\"end\":100996,\"start\":100995},{\"end\":101006,\"start\":101005},{\"end\":101021,\"start\":101020},{\"end\":101381,\"start\":101380},{\"end\":101383,\"start\":101382},{\"end\":101391,\"start\":101390},{\"end\":101393,\"start\":101392},{\"end\":101676,\"start\":101675},{\"end\":101678,\"start\":101677},{\"end\":101690,\"start\":101689},{\"end\":101692,\"start\":101691},{\"end\":101881,\"start\":101880},{\"end\":101889,\"start\":101888},{\"end\":101891,\"start\":101890},{\"end\":101901,\"start\":101900},{\"end\":101903,\"start\":101902},{\"end\":102137,\"start\":102136},{\"end\":102146,\"start\":102145},{\"end\":102157,\"start\":102156},{\"end\":102423,\"start\":102422},{\"end\":102513,\"start\":102508}]", "bib_author_last_name": "[{\"end\":84640,\"start\":84630},{\"end\":84649,\"start\":84644},{\"end\":84658,\"start\":84653},{\"end\":84668,\"start\":84662},{\"end\":84676,\"start\":84672},{\"end\":84686,\"start\":84680},{\"end\":84995,\"start\":84985},{\"end\":85004,\"start\":84999},{\"end\":85013,\"start\":85008},{\"end\":85023,\"start\":85017},{\"end\":85031,\"start\":85027},{\"end\":85041,\"start\":85035},{\"end\":85358,\"start\":85353},{\"end\":85369,\"start\":85362},{\"end\":85383,\"start\":85373},{\"end\":85393,\"start\":85387},{\"end\":85670,\"start\":85665},{\"end\":85682,\"start\":85674},{\"end\":85691,\"start\":85686},{\"end\":85700,\"start\":85695},{\"end\":85903,\"start\":85895},{\"end\":85912,\"start\":85907},{\"end\":85920,\"start\":85916},{\"end\":85933,\"start\":85926},{\"end\":85942,\"start\":85937},{\"end\":85951,\"start\":85946},{\"end\":85959,\"start\":85955},{\"end\":85966,\"start\":85963},{\"end\":85978,\"start\":85970},{\"end\":85989,\"start\":85982},{\"end\":86325,\"start\":86319},{\"end\":86334,\"start\":86329},{\"end\":86344,\"start\":86338},{\"end\":86792,\"start\":86785},{\"end\":86802,\"start\":86796},{\"end\":86813,\"start\":86806},{\"end\":86823,\"start\":86817},{\"end\":86830,\"start\":86827},{\"end\":86842,\"start\":86834},{\"end\":86850,\"start\":86846},{\"end\":86859,\"start\":86854},{\"end\":86871,\"start\":86863},{\"end\":86882,\"start\":86875},{\"end\":87219,\"start\":87216},{\"end\":87231,\"start\":87223},{\"end\":87241,\"start\":87235},{\"end\":87494,\"start\":87487},{\"end\":87502,\"start\":87498},{\"end\":87797,\"start\":87791},{\"end\":87809,\"start\":87801},{\"end\":87823,\"start\":87813},{\"end\":87836,\"start\":87827},{\"end\":87846,\"start\":87840},{\"end\":87856,\"start\":87850},{\"end\":87864,\"start\":87860},{\"end\":87877,\"start\":87868},{\"end\":88058,\"start\":88050},{\"end\":88069,\"start\":88062},{\"end\":88316,\"start\":88309},{\"end\":88556,\"start\":88551},{\"end\":88562,\"start\":88560},{\"end\":88568,\"start\":88566},{\"end\":88817,\"start\":88811},{\"end\":88831,\"start\":88821},{\"end\":88846,\"start\":88835},{\"end\":88860,\"start\":88850},{\"end\":89237,\"start\":89231},{\"end\":89247,\"start\":89243},{\"end\":89262,\"start\":89251},{\"end\":89278,\"start\":89268},{\"end\":89545,\"start\":89543},{\"end\":89554,\"start\":89549},{\"end\":89808,\"start\":89805},{\"end\":89818,\"start\":89812},{\"end\":89827,\"start\":89822},{\"end\":90091,\"start\":90084},{\"end\":90102,\"start\":90097},{\"end\":90111,\"start\":90106},{\"end\":90120,\"start\":90115},{\"end\":90385,\"start\":90381},{\"end\":90395,\"start\":90389},{\"end\":90407,\"start\":90399},{\"end\":90417,\"start\":90411},{\"end\":90736,\"start\":90732},{\"end\":90743,\"start\":90740},{\"end\":90750,\"start\":90747},{\"end\":90756,\"start\":90754},{\"end\":91019,\"start\":91016},{\"end\":91027,\"start\":91023},{\"end\":91035,\"start\":91031},{\"end\":91373,\"start\":91368},{\"end\":91604,\"start\":91600},{\"end\":91613,\"start\":91608},{\"end\":91943,\"start\":91935},{\"end\":91952,\"start\":91947},{\"end\":91963,\"start\":91956},{\"end\":91975,\"start\":91967},{\"end\":92204,\"start\":92195},{\"end\":92212,\"start\":92208},{\"end\":92222,\"start\":92216},{\"end\":92456,\"start\":92451},{\"end\":92464,\"start\":92460},{\"end\":92474,\"start\":92468},{\"end\":92484,\"start\":92478},{\"end\":92794,\"start\":92787},{\"end\":92801,\"start\":92798},{\"end\":92810,\"start\":92807},{\"end\":93057,\"start\":93054},{\"end\":93283,\"start\":93281},{\"end\":93289,\"start\":93287},{\"end\":93299,\"start\":93293},{\"end\":93527,\"start\":93524},{\"end\":93794,\"start\":93790},{\"end\":93959,\"start\":93949},{\"end\":93967,\"start\":93965},{\"end\":93977,\"start\":93971},{\"end\":93985,\"start\":93981},{\"end\":93993,\"start\":93989},{\"end\":94006,\"start\":93999},{\"end\":94018,\"start\":94012},{\"end\":94031,\"start\":94024},{\"end\":94386,\"start\":94383},{\"end\":94394,\"start\":94390},{\"end\":94406,\"start\":94398},{\"end\":94667,\"start\":94661},{\"end\":94678,\"start\":94671},{\"end\":94689,\"start\":94682},{\"end\":94702,\"start\":94693},{\"end\":94710,\"start\":94706},{\"end\":94722,\"start\":94714},{\"end\":94732,\"start\":94726},{\"end\":94741,\"start\":94736},{\"end\":94764,\"start\":94745},{\"end\":94774,\"start\":94768},{\"end\":95158,\"start\":95152},{\"end\":95349,\"start\":95343},{\"end\":95359,\"start\":95353},{\"end\":95369,\"start\":95363},{\"end\":95375,\"start\":95373},{\"end\":95385,\"start\":95379},{\"end\":95392,\"start\":95389},{\"end\":95401,\"start\":95396},{\"end\":95409,\"start\":95405},{\"end\":95421,\"start\":95413},{\"end\":95433,\"start\":95425},{\"end\":95787,\"start\":95779},{\"end\":95793,\"start\":95791},{\"end\":95813,\"start\":95799},{\"end\":95819,\"start\":95817},{\"end\":96081,\"start\":96078},{\"end\":96095,\"start\":96085},{\"end\":96289,\"start\":96287},{\"end\":96300,\"start\":96293},{\"end\":96585,\"start\":96575},{\"end\":96592,\"start\":96589},{\"end\":96599,\"start\":96596},{\"end\":96612,\"start\":96605},{\"end\":96886,\"start\":96880},{\"end\":96892,\"start\":96890},{\"end\":96901,\"start\":96896},{\"end\":96912,\"start\":96905},{\"end\":96928,\"start\":96918},{\"end\":96939,\"start\":96932},{\"end\":96948,\"start\":96943},{\"end\":96959,\"start\":96952},{\"end\":96968,\"start\":96963},{\"end\":96975,\"start\":96972},{\"end\":97308,\"start\":97299},{\"end\":97316,\"start\":97312},{\"end\":97323,\"start\":97320},{\"end\":97536,\"start\":97528},{\"end\":97858,\"start\":97850},{\"end\":97869,\"start\":97862},{\"end\":98174,\"start\":98162},{\"end\":98182,\"start\":98178},{\"end\":98383,\"start\":98373},{\"end\":98398,\"start\":98387},{\"end\":98410,\"start\":98402},{\"end\":98420,\"start\":98414},{\"end\":98428,\"start\":98424},{\"end\":98441,\"start\":98432},{\"end\":98455,\"start\":98445},{\"end\":98463,\"start\":98459},{\"end\":98976,\"start\":98964},{\"end\":98983,\"start\":98980},{\"end\":98989,\"start\":98987},{\"end\":98997,\"start\":98993},{\"end\":99008,\"start\":99001},{\"end\":99335,\"start\":99330},{\"end\":99348,\"start\":99341},{\"end\":99592,\"start\":99586},{\"end\":99604,\"start\":99598},{\"end\":99614,\"start\":99608},{\"end\":99626,\"start\":99620},{\"end\":99833,\"start\":99829},{\"end\":99844,\"start\":99837},{\"end\":100031,\"start\":100027},{\"end\":100042,\"start\":100035},{\"end\":100332,\"start\":100328},{\"end\":100343,\"start\":100336},{\"end\":100592,\"start\":100588},{\"end\":100609,\"start\":100596},{\"end\":100966,\"start\":100962},{\"end\":100982,\"start\":100970},{\"end\":100993,\"start\":100986},{\"end\":101003,\"start\":100997},{\"end\":101018,\"start\":101007},{\"end\":101032,\"start\":101022},{\"end\":101388,\"start\":101384},{\"end\":101404,\"start\":101394},{\"end\":101687,\"start\":101679},{\"end\":101701,\"start\":101693},{\"end\":101886,\"start\":101882},{\"end\":101898,\"start\":101892},{\"end\":101909,\"start\":101904},{\"end\":102143,\"start\":102138},{\"end\":102154,\"start\":102147},{\"end\":102163,\"start\":102158},{\"end\":102427,\"start\":102424}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4787508},\"end\":84927,\"start\":84574},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4787508},\"end\":85296,\"start\":84929},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":211146250},\"end\":85627,\"start\":85298},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15533796},\"end\":85862,\"start\":85629},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":237346088},\"end\":86226,\"start\":85864},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":507141},\"end\":86687,\"start\":86228},{\"attributes\":{\"doi\":\"arXiv:2209.07858\",\"id\":\"b6\"},\"end\":87164,\"start\":86689},{\"attributes\":{\"doi\":\"arXiv:2210.10760\",\"id\":\"b7\"},\"end\":87395,\"start\":87166},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":247084231},\"end\":87785,\"start\":87397},{\"attributes\":{\"id\":\"b9\"},\"end\":87976,\"start\":87787},{\"attributes\":{\"doi\":\"arXiv:2209.14375\",\"id\":\"b10\"},\"end\":88241,\"start\":87978},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":120735155},\"end\":88498,\"start\":88243},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13803486},\"end\":88760,\"start\":88500},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4877231},\"end\":89141,\"start\":88762},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15607512},\"end\":89496,\"start\":89143},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16153365},\"end\":89732,\"start\":89498},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14207616},\"end\":90030,\"start\":89734},{\"attributes\":{\"id\":\"b17\"},\"end\":90301,\"start\":90032},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":282570},\"end\":90668,\"start\":90303},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":30118552},\"end\":90967,\"start\":90670},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":229923558},\"end\":91242,\"start\":90969},{\"attributes\":{\"id\":\"b21\"},\"end\":91530,\"start\":91244},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5613334},\"end\":91863,\"start\":91532},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":917864},\"end\":92134,\"start\":91865},{\"attributes\":{\"doi\":\"arXiv:2110.06169\",\"id\":\"b24\"},\"end\":92387,\"start\":92136},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":219530894},\"end\":92714,\"start\":92389},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13285792},\"end\":92995,\"start\":92716},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":122679122},\"end\":93202,\"start\":92997},{\"attributes\":{\"doi\":\"arXiv:2205.10671\",\"id\":\"b28\"},\"end\":93473,\"start\":93204},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":28826624},\"end\":93711,\"start\":93475},{\"attributes\":{\"id\":\"b30\"},\"end\":93886,\"start\":93713},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8818528},\"end\":94321,\"start\":93888},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4881108},\"end\":94591,\"start\":94323},{\"attributes\":{\"doi\":\"arXiv:2203.11147\",\"id\":\"b33\"},\"end\":95045,\"start\":94593},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15654531},\"end\":95339,\"start\":95047},{\"attributes\":{\"doi\":\"arXiv:2112.09332\",\"id\":\"b35\"},\"end\":95736,\"start\":95341},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1216817},\"end\":96022,\"start\":95738},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1909631},\"end\":96234,\"start\":96024},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":34196655},\"end\":96497,\"start\":96236},{\"attributes\":{\"doi\":\"arXiv:1908.01289\",\"id\":\"b39\"},\"end\":96807,\"start\":96499},{\"attributes\":{\"doi\":\"arXiv:2203.02155\",\"id\":\"b40\"},\"end\":97231,\"start\":96809},{\"attributes\":{\"doi\":\"arXiv:2111.04850\",\"id\":\"b41\"},\"end\":97492,\"start\":97233},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":116534299},\"end\":97753,\"start\":97494},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":13910694},\"end\":98117,\"start\":97755},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":5649130},\"end\":98304,\"start\":98119},{\"attributes\":{\"doi\":\"arXiv:2210.01241\",\"id\":\"b45\",\"matched_paper_id\":252693405},\"end\":98875,\"start\":98306},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232307477},\"end\":99278,\"start\":98877},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":120451264},\"end\":99528,\"start\":99280},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":12226563},\"end\":99806,\"start\":99530},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":53962807},\"end\":99978,\"start\":99808},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":53087235},\"end\":100275,\"start\":99980},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52932326},\"end\":100499,\"start\":100277},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":244527094},\"end\":100873,\"start\":100501},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":6474018},\"end\":101316,\"start\":100875},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":467358},\"end\":101610,\"start\":101318},{\"attributes\":{\"id\":\"b55\"},\"end\":101806,\"start\":101612},{\"attributes\":{\"doi\":\"arXiv:2301.01392\",\"id\":\"b56\"},\"end\":102091,\"start\":101808},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":6220289},\"end\":102357,\"start\":102093},{\"attributes\":{\"id\":\"b58\"},\"end\":102504,\"start\":102359},{\"attributes\":{\"id\":\"b59\"},\"end\":102559,\"start\":102506},{\"attributes\":{\"id\":\"b60\"},\"end\":103011,\"start\":102561},{\"attributes\":{\"id\":\"b61\"},\"end\":103413,\"start\":103013}]", "bib_title": "[{\"end\":84624,\"start\":84574},{\"end\":84979,\"start\":84929},{\"end\":85349,\"start\":85298},{\"end\":85661,\"start\":85629},{\"end\":85891,\"start\":85864},{\"end\":86315,\"start\":86228},{\"end\":87483,\"start\":87397},{\"end\":88303,\"start\":88243},{\"end\":88547,\"start\":88500},{\"end\":88807,\"start\":88762},{\"end\":89227,\"start\":89143},{\"end\":89539,\"start\":89498},{\"end\":89801,\"start\":89734},{\"end\":90080,\"start\":90032},{\"end\":90377,\"start\":90303},{\"end\":90728,\"start\":90670},{\"end\":91012,\"start\":90969},{\"end\":91594,\"start\":91532},{\"end\":91931,\"start\":91865},{\"end\":92447,\"start\":92389},{\"end\":92783,\"start\":92716},{\"end\":93047,\"start\":92997},{\"end\":93517,\"start\":93475},{\"end\":93945,\"start\":93888},{\"end\":94379,\"start\":94323},{\"end\":95146,\"start\":95047},{\"end\":95775,\"start\":95738},{\"end\":96074,\"start\":96024},{\"end\":96281,\"start\":96236},{\"end\":97522,\"start\":97494},{\"end\":97846,\"start\":97755},{\"end\":98158,\"start\":98119},{\"end\":98369,\"start\":98306},{\"end\":98960,\"start\":98877},{\"end\":99324,\"start\":99280},{\"end\":99582,\"start\":99530},{\"end\":99825,\"start\":99808},{\"end\":100023,\"start\":99980},{\"end\":100324,\"start\":100277},{\"end\":100584,\"start\":100501},{\"end\":100958,\"start\":100875},{\"end\":101378,\"start\":101318},{\"end\":102134,\"start\":102093}]", "bib_author": "[{\"end\":84642,\"start\":84626},{\"end\":84651,\"start\":84642},{\"end\":84660,\"start\":84651},{\"end\":84670,\"start\":84660},{\"end\":84678,\"start\":84670},{\"end\":84688,\"start\":84678},{\"end\":84997,\"start\":84981},{\"end\":85006,\"start\":84997},{\"end\":85015,\"start\":85006},{\"end\":85025,\"start\":85015},{\"end\":85033,\"start\":85025},{\"end\":85043,\"start\":85033},{\"end\":85360,\"start\":85351},{\"end\":85371,\"start\":85360},{\"end\":85385,\"start\":85371},{\"end\":85395,\"start\":85385},{\"end\":85672,\"start\":85663},{\"end\":85684,\"start\":85672},{\"end\":85693,\"start\":85684},{\"end\":85702,\"start\":85693},{\"end\":85905,\"start\":85893},{\"end\":85914,\"start\":85905},{\"end\":85922,\"start\":85914},{\"end\":85935,\"start\":85922},{\"end\":85944,\"start\":85935},{\"end\":85953,\"start\":85944},{\"end\":85961,\"start\":85953},{\"end\":85968,\"start\":85961},{\"end\":85980,\"start\":85968},{\"end\":85991,\"start\":85980},{\"end\":86327,\"start\":86317},{\"end\":86336,\"start\":86327},{\"end\":86346,\"start\":86336},{\"end\":86794,\"start\":86783},{\"end\":86804,\"start\":86794},{\"end\":86815,\"start\":86804},{\"end\":86825,\"start\":86815},{\"end\":86832,\"start\":86825},{\"end\":86844,\"start\":86832},{\"end\":86852,\"start\":86844},{\"end\":86861,\"start\":86852},{\"end\":86873,\"start\":86861},{\"end\":86884,\"start\":86873},{\"end\":87221,\"start\":87214},{\"end\":87233,\"start\":87221},{\"end\":87243,\"start\":87233},{\"end\":87496,\"start\":87485},{\"end\":87504,\"start\":87496},{\"end\":87799,\"start\":87789},{\"end\":87811,\"start\":87799},{\"end\":87825,\"start\":87811},{\"end\":87838,\"start\":87825},{\"end\":87848,\"start\":87838},{\"end\":87858,\"start\":87848},{\"end\":87866,\"start\":87858},{\"end\":87879,\"start\":87866},{\"end\":88060,\"start\":88048},{\"end\":88071,\"start\":88060},{\"end\":88318,\"start\":88305},{\"end\":88558,\"start\":88549},{\"end\":88564,\"start\":88558},{\"end\":88570,\"start\":88564},{\"end\":88819,\"start\":88809},{\"end\":88833,\"start\":88819},{\"end\":88848,\"start\":88833},{\"end\":88862,\"start\":88848},{\"end\":89239,\"start\":89229},{\"end\":89249,\"start\":89239},{\"end\":89264,\"start\":89249},{\"end\":89280,\"start\":89264},{\"end\":89547,\"start\":89541},{\"end\":89556,\"start\":89547},{\"end\":89810,\"start\":89803},{\"end\":89820,\"start\":89810},{\"end\":89829,\"start\":89820},{\"end\":90093,\"start\":90082},{\"end\":90104,\"start\":90093},{\"end\":90113,\"start\":90104},{\"end\":90122,\"start\":90113},{\"end\":90387,\"start\":90379},{\"end\":90397,\"start\":90387},{\"end\":90409,\"start\":90397},{\"end\":90419,\"start\":90409},{\"end\":90738,\"start\":90730},{\"end\":90745,\"start\":90738},{\"end\":90752,\"start\":90745},{\"end\":90758,\"start\":90752},{\"end\":91021,\"start\":91014},{\"end\":91029,\"start\":91021},{\"end\":91037,\"start\":91029},{\"end\":91375,\"start\":91364},{\"end\":91606,\"start\":91596},{\"end\":91615,\"start\":91606},{\"end\":91945,\"start\":91933},{\"end\":91954,\"start\":91945},{\"end\":91965,\"start\":91954},{\"end\":91977,\"start\":91965},{\"end\":92206,\"start\":92193},{\"end\":92214,\"start\":92206},{\"end\":92224,\"start\":92214},{\"end\":92458,\"start\":92449},{\"end\":92466,\"start\":92458},{\"end\":92476,\"start\":92466},{\"end\":92486,\"start\":92476},{\"end\":92796,\"start\":92785},{\"end\":92803,\"start\":92796},{\"end\":92812,\"start\":92803},{\"end\":93059,\"start\":93049},{\"end\":93285,\"start\":93279},{\"end\":93291,\"start\":93285},{\"end\":93301,\"start\":93291},{\"end\":93529,\"start\":93519},{\"end\":93796,\"start\":93786},{\"end\":93961,\"start\":93947},{\"end\":93969,\"start\":93961},{\"end\":93979,\"start\":93969},{\"end\":93987,\"start\":93979},{\"end\":93995,\"start\":93987},{\"end\":94008,\"start\":93995},{\"end\":94020,\"start\":94008},{\"end\":94033,\"start\":94020},{\"end\":94388,\"start\":94381},{\"end\":94396,\"start\":94388},{\"end\":94408,\"start\":94396},{\"end\":94669,\"start\":94659},{\"end\":94680,\"start\":94669},{\"end\":94691,\"start\":94680},{\"end\":94704,\"start\":94691},{\"end\":94712,\"start\":94704},{\"end\":94724,\"start\":94712},{\"end\":94734,\"start\":94724},{\"end\":94743,\"start\":94734},{\"end\":94766,\"start\":94743},{\"end\":94776,\"start\":94766},{\"end\":95160,\"start\":95148},{\"end\":95351,\"start\":95341},{\"end\":95361,\"start\":95351},{\"end\":95371,\"start\":95361},{\"end\":95377,\"start\":95371},{\"end\":95387,\"start\":95377},{\"end\":95394,\"start\":95387},{\"end\":95403,\"start\":95394},{\"end\":95411,\"start\":95403},{\"end\":95423,\"start\":95411},{\"end\":95435,\"start\":95423},{\"end\":95789,\"start\":95777},{\"end\":95795,\"start\":95789},{\"end\":95815,\"start\":95795},{\"end\":95821,\"start\":95815},{\"end\":96083,\"start\":96076},{\"end\":96097,\"start\":96083},{\"end\":96291,\"start\":96283},{\"end\":96302,\"start\":96291},{\"end\":96587,\"start\":96571},{\"end\":96594,\"start\":96587},{\"end\":96601,\"start\":96594},{\"end\":96614,\"start\":96601},{\"end\":96888,\"start\":96878},{\"end\":96894,\"start\":96888},{\"end\":96903,\"start\":96894},{\"end\":96914,\"start\":96903},{\"end\":96930,\"start\":96914},{\"end\":96941,\"start\":96930},{\"end\":96950,\"start\":96941},{\"end\":96961,\"start\":96950},{\"end\":96970,\"start\":96961},{\"end\":96977,\"start\":96970},{\"end\":97310,\"start\":97297},{\"end\":97318,\"start\":97310},{\"end\":97325,\"start\":97318},{\"end\":97538,\"start\":97524},{\"end\":97860,\"start\":97848},{\"end\":97871,\"start\":97860},{\"end\":98176,\"start\":98160},{\"end\":98184,\"start\":98176},{\"end\":98385,\"start\":98371},{\"end\":98400,\"start\":98385},{\"end\":98412,\"start\":98400},{\"end\":98422,\"start\":98412},{\"end\":98430,\"start\":98422},{\"end\":98443,\"start\":98430},{\"end\":98457,\"start\":98443},{\"end\":98465,\"start\":98457},{\"end\":98978,\"start\":98962},{\"end\":98985,\"start\":98978},{\"end\":98991,\"start\":98985},{\"end\":98999,\"start\":98991},{\"end\":99010,\"start\":98999},{\"end\":99337,\"start\":99326},{\"end\":99350,\"start\":99337},{\"end\":99594,\"start\":99584},{\"end\":99606,\"start\":99594},{\"end\":99616,\"start\":99606},{\"end\":99628,\"start\":99616},{\"end\":99835,\"start\":99827},{\"end\":99846,\"start\":99835},{\"end\":100033,\"start\":100025},{\"end\":100044,\"start\":100033},{\"end\":100334,\"start\":100326},{\"end\":100345,\"start\":100334},{\"end\":100594,\"start\":100586},{\"end\":100611,\"start\":100594},{\"end\":100968,\"start\":100960},{\"end\":100984,\"start\":100968},{\"end\":100995,\"start\":100984},{\"end\":101005,\"start\":100995},{\"end\":101020,\"start\":101005},{\"end\":101034,\"start\":101020},{\"end\":101390,\"start\":101380},{\"end\":101406,\"start\":101390},{\"end\":101689,\"start\":101675},{\"end\":101703,\"start\":101689},{\"end\":101888,\"start\":101880},{\"end\":101900,\"start\":101888},{\"end\":101911,\"start\":101900},{\"end\":102145,\"start\":102136},{\"end\":102156,\"start\":102145},{\"end\":102165,\"start\":102156},{\"end\":102429,\"start\":102422},{\"end\":102516,\"start\":102508}]", "bib_venue": "[{\"end\":84737,\"start\":84688},{\"end\":85092,\"start\":85043},{\"end\":85439,\"start\":85395},{\"end\":85727,\"start\":85702},{\"end\":86019,\"start\":85991},{\"end\":86414,\"start\":86346},{\"end\":86781,\"start\":86689},{\"end\":87212,\"start\":87166},{\"end\":87570,\"start\":87504},{\"end\":88046,\"start\":87978},{\"end\":88355,\"start\":88318},{\"end\":88619,\"start\":88570},{\"end\":88928,\"start\":88862},{\"end\":89300,\"start\":89280},{\"end\":89605,\"start\":89556},{\"end\":89869,\"start\":89829},{\"end\":90150,\"start\":90122},{\"end\":90468,\"start\":90419},{\"end\":90807,\"start\":90758},{\"end\":91081,\"start\":91037},{\"end\":91362,\"start\":91244},{\"end\":91676,\"start\":91615},{\"end\":91981,\"start\":91977},{\"end\":92191,\"start\":92136},{\"end\":92535,\"start\":92486},{\"end\":92829,\"start\":92812},{\"end\":93077,\"start\":93059},{\"end\":93277,\"start\":93204},{\"end\":93577,\"start\":93529},{\"end\":93784,\"start\":93713},{\"end\":94077,\"start\":94033},{\"end\":94435,\"start\":94408},{\"end\":94657,\"start\":94593},{\"end\":95180,\"start\":95160},{\"end\":95506,\"start\":95451},{\"end\":95861,\"start\":95821},{\"end\":96113,\"start\":96097},{\"end\":96346,\"start\":96302},{\"end\":96569,\"start\":96499},{\"end\":96876,\"start\":96809},{\"end\":97295,\"start\":97233},{\"end\":97609,\"start\":97538},{\"end\":97915,\"start\":97871},{\"end\":98189,\"start\":98184},{\"end\":98564,\"start\":98481},{\"end\":99059,\"start\":99010},{\"end\":99385,\"start\":99350},{\"end\":99657,\"start\":99628},{\"end\":99884,\"start\":99846},{\"end\":100120,\"start\":100044},{\"end\":100372,\"start\":100345},{\"end\":100666,\"start\":100611},{\"end\":101072,\"start\":101034},{\"end\":101446,\"start\":101406},{\"end\":101673,\"start\":101612},{\"end\":101878,\"start\":101808},{\"end\":102214,\"start\":102165},{\"end\":102420,\"start\":102359},{\"end\":102754,\"start\":102561},{\"end\":103169,\"start\":103013},{\"end\":86469,\"start\":86416}]"}}}, "year": 2023, "month": 12, "day": 17}