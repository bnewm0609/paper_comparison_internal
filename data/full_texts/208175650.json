{"id": 208175650, "updated": "2023-10-06 21:10:18.01", "metadata": {"title": "D3S -- A Discriminative Single Shot Segmentation Tracker", "authors": "[{\"first\":\"Alan\",\"last\":\"Lukevzivc\",\"middle\":[]},{\"first\":\"Jivr'i\",\"last\":\"Matas\",\"middle\":[]},{\"first\":\"Matej\",\"last\":\"Kristan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker - D3S, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve high robustness and online target segmentation. Without per-dataset finetuning and trained only for segmentation as the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the state-of-the-art trackers on the TrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video object segmentation benchmark and performs on par with top video object segmentation algorithms, while running an order of magnitude faster, close to real-time.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1911.08862", "mag": "3034297219", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LukezicMK20", "doi": "10.1109/cvpr42600.2020.00716"}}, "content": {"source": {"pdf_hash": "691c673ea67df5736ff17d86d27372f6298db38d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1911.08862v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1911.08862", "status": "GREEN"}}, "grobid": {"id": "c23e9a5bbe1c287c166a51659a16195a6d10b459", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/691c673ea67df5736ff17d86d27372f6298db38d.txt", "contents": "\nD3S -A Discriminative Single Shot Segmentation Tracker\n\n\nAlan Luke\u017ei\u010d alan.lukezic@fri.uni-lj.si \nFaculty of Computer and Information Science\nUniversity of Ljubljana\nSlovenia\n\nJi\u0159\u00ed Matas \nFaculty of Electrical Engineering\nCzech Technical University\nPragueCzech Republic\n\nMatej Kristan \nFaculty of Computer and Information Science\nUniversity of Ljubljana\nSlovenia\n\nD3S -A Discriminative Single Shot Segmentation Tracker\n\nTemplate-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker -D3S, which narrows the gap between visual object tracking and video object segmentation. A singleshot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve high robustness and online target segmentation. Without per-dataset finetuning and trained only for segmentation as the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the state-of-the-art trackers on the Track-ingNet. D3S outperforms the leading segmentation tracker SiamMask on video object segmentation benchmarks and performs on par with top video object segmentation algorithms, while running an order of magnitude faster, close to real-time. PyTorch implementation is available here: https://github.com/alanlukezic/d3s\n\nIntroduction\n\nVisual object tracking is one of core computer vision problems. The most common formulation considers the task of reporting target location in each frame of the video given a single training image. Currently, the dominant tracking paradigm, performing best in evaluations [22,24], is correlation bounding box tracking [11,3,33,2,54,28] where the target represented by a multi-channel rectangular template is localized by cross-correlation between the template and a search region.\n\nState-of-the-art template-based trackers apply an efficient brute-force search for target localization. Such strategy is appropriate for low-dimensional transformations like translation and scale change, but becomes inefficient for more general situations e.g. such that induce an aspect ratio change and rotation. As a compromise, modern track- ers combine approximate exhaustive search with sampling and/or bounding box refinement/regression networks [10,27] for aspect ratio estimation. However, these approaches are restricted to axis-aligned rectangles. Estimation of high-dimensional template-based transformation is unreliable when a bounding box is a poor approximation of the target [31]. This is common -consider e.g. elongated, rotating, deformable objects, or a person with spread out hands. In these cases, the most accurate and well-defined target location model is a binary per-pixel segmentation mask. If such output is required, tracking becomes the video object segmentation task recently popularized by DAVIS [38,40] and YoutubeVOS [51] challenges.\n\nUnlike in tracking, video object segmentation challenges typically consider large targets observed for less than 100 frames with low background distractor presence. Top video object segmentation approaches thus fare poorly in shortterm tracking scenarios [24] where the target covers a fraction of the image, substantially changes its appearance over a longer period and moves over a cluttered background.\n\nBest trackers apply visual model adaptation, but in the case of segmentation errors it leads to an irrecoverable tracking failure [41]. Because of this, in the past, segmentation has played only an auxiliary role in template-based trackers [1], constrained DCF learning [33] and tracking by 3D model construction [20].\n\nRecently, the SiamRPN [28] tracker has been extended to produce high-quality segmentation masks in two stages [50] -the target bounding box is first localized by SiamRPN branches and then a segmentation mask is computed only within this region by another branch. The twostage processing misses the opportunity to treat localization and segmentation jointly to increase robustness. Another drawback is that a fixed template is used that cannot be discriminatively adapted to the changing scene.\n\nWe propose a new single-shot discriminative segmentation tracker, D3S, that addresses the above-mentioned limitations. The target is encoded by two discriminative visual models -one is adaptive and highly discriminative, but geometrically constrained to an Euclidean motion (GEM), while the other is invariant to broad range of transformation (GIM, geometrically invariant model), see Figure 1.\n\nGIM sacrifices spatial relations to allow target localization under significant deformation. On the other hand, GEM predicts only position, but discriminatively adapts to the target and acts as a selector between possibly multiple target segmentations inferred by GIM. In contrast to related trackers [50,27,10], the primary output of D3S is a segmentation map computed in a single pass through the network, which is trained end-to-end for segmentation only ( Figure 2). Some applications and most tracking benchmarks require reporting the target location as a bounding box. As a secondary contribution, we propose an effective method for interpreting the segmentation mask as a rotated rectangle. This avoids an error-prone greedy search and naturally addresses changes in location, scale, aspect ratio and rotation.\n\nD3S outperforms all state-of-the-art trackers on most of the major tracking benchmarks [23,24,19,35] despite not being trained for bounding box tracking. In video object segmentation benchmarks [38,40], D3S outperforms the leading segmentation tracker [50] and performs on par with top video object segmentation algorithms (often tuned to a specific domain), yet running orders of magnitude faster. Note that D3S is not re-trained for different benchmarks -a single pre-trained version shows remarkable generalization ability and versatility.\n\n\nRelated Work\n\nRobust localization crucially depends on the discrimination capability between the target and the background distractors. This property has been studied in depth in discriminative template trackers called discriminative correlation filters (DCF) [4]. The template learning is formulated as a (possibly nonlinear) ridge regression problem and solved by circular correlation [4,12,17,30]. While trackers based purely on color segmentation [8,41] are inferior to DCFs, segmentation has been used for improved DCF tracking of non-rectangular targets [1,31]. Luke\u017ei\u010d et al. [33] used color segmentation to constrain DCF learning and proposed a real-time tracker with hand-crafted features which achieved performance comparable to trackers with deep features. The method was extended to long-term [32] and RGB-depth tracking [20] using color and depth segmentation. Further improvements in DCF tracking considered deep features: Danelljan et al. [11] used features pre-trained for detection, Valmadre et al. [46] proposed pretraining features for DCF localization and recently Danelljan et al. [10] proposed a deep DCF training using backpropagation.\n\nAnother class of trackers, called Siamese trackers [2,44,15], has evolved in direction of generative templates. Siamese trackers apply a backbone pre-trained offline with general targets such that object-background discrimination is maximized by correlation between the search region and target template extracted in the first frame [2]. The template and the backbone are fixed during tracking, leading to an excellent real-time performance [24]. Several multi-stage Siamese extensions have been proposed. These include addition of region proposal networks for improved target localization accuracy [28,27] and addition of segmentation branches [50] for accurate target segmentation. Recently a template adaptation technique by backprop has been proposed [29] to improve tracking robustness.\n\nSegmentation of moving objects is a central problem in the emerging field of video object segmentation (VOS) [38,51]. Most recent works [47,5,48,7,53] achieve impressive results, but involve large deep networks, which often require finetuning and are slow. Hu et al. [18] and Chen et al. [6] concurrently proposed segmentation by matching features extracted in the first frame, which considerably reduces the processing time. However, the VOS task considers segmentation of large objects with limited appearance changes in short videos. Thus, these methods fare poorly on the visual object tracking task with small, fast moving objects. The work proposed in this paper aims at narrowing the gap between visual object tracking and video object segmentation. \n\n\nDiscriminative segmentation network\n\nTwo models are used in D3S to robustly cope with target appearance changes and background discrimination: a geometrically invariant model (GIM) presented in Section 3.1, and a geometrically constrained Euclidean model (GEM) presented in Section 3.2. These models process the input in parallel pathways and produce several coarse target presence channels, which are fused into a detailed segmentation map by a refinement pathway described in Section 3.3. See Figure 2 for the architecture outline.\n\n\nGeometrically invariant model pathway\n\nAccurate segmentation of a deformable target requires loose spatial constraints in the discriminative model. Our geometrically invariant model (GIM) is thus composed of two sets of deep feature vectors corresponding to the target and the background, i.e., X GIM = {X F , X B }.\n\nSince the pre-trained backbone features are sub-optimal for accurate segmentation, these are first processed by a 1 \u00d7 1 convolutional layer to reduce their dimensionality to 64, which is followed by a 3 \u00d7 3 convolutional layer (a ReLU is placed after each convolutional layer). Both these layers are adjusted in the network training stage to produce optimal features for segmentation. The target/background models are created in the first frame by extracting the segmentation feature vectors at pixel locations corresponding to the target (X F ) and from the immediate neighborhood for the background (X B ).\n\nDuring tracking, the pixel-level features extracted from the search region are compared to those of GIM (X GIM ) to compute foreground and background similarity channels F and B following [18]. Specifically, for the F channel computation, each feature y i extracted at pixel i is compared to all features x F j \u2208 X F by a normalized dot product\ns F ij (y i , x F j ) = \u1ef9 i ,x F j ,(1)\nwhere(\u00b7) indicates an L 2 normalization. The final per-pixel foreground similarity at pixel i, F i , is obtained by average of top-K similarities at that pixel, i.e.,\nF i = TOP({s F ij } j=1:N F , K),(2)\nwhere TOP(\u00b7, K) is a top-K averaging operator over the set of N F similarities. Computation of the background similarity channel B follows the same principle, but with similarities computed with the background model feature vectors, i.e., x B j \u2208 X B . Finally, a softmax layer is applied to produce a target posterior channel P. The GIM pathway architecture is shown in Figure 3.\n\n\nGeometrically constrained model pathway\n\nWhile GIM produces an excellent target-background separation, it cannot well distinguish the target from similar instances, leading to a reduced robustness (see Figure 1, first line). Robust localization, however, is a wellestablished quality of the discriminative correlation filters. Although they represent the target by a geometrically constrained model (i.e., a rectangular filter), efficient techniques developed to adapt to the target discriminative features [13,33,10] allow tracking reliably under considerable appearance changes.\n\nWe thus employ a recent deep DCF formulation [10] in the geometrically constrained Euclidean model (GEM) pathway. Following [10], the backbone features are first reduced to 64 channels by 1 \u00d7 1 convolutional layer. The reduced features are correlated by a 64 channel DCF followed by a PeLU nonlinearity [45]. The reduction layer and DCF are trained by an efficient backprop formulation (see [10] for details).\n\nThe maximum of the correlation response is considered as the most likely target position. The D3S output (i.e., segmentation), however, requires specifying a belief of target presence at each pixel. Therefore, a target location channel is constructed by computing a (Euclidean) distance transform from the position of the maximum in the correlation map to the remaining pixels in the search region. The GEM pathway is shown in Figure 4. \n\n\nRefinement pathway\n\nThe GIM and GEM pathways provide complementary information about the pixel-level target presence. GEM provides a robust, but rather inaccurate estimate of the target region, whereas the output channels from GIM show a greater detail, but are less discriminative ( Figure 1). Furthermore, the individual outputs are low-resolution due to the backbone encoding. A refinement pathway is thus designed to combine the different information channels and upscale the solution into an accurate and detailed segmentation map.\n\nThe refinement pathway takes the following inputs: the target location channel (L) from GEM and the foreground similarity and posterior channels (F and P) from the GIM. The channels are concatenated and processed by a 3\u00d73 convolutional layer followed by a ReLU, resulting in a tensor of 64 channels. Three stages of upscaling akin to [42,39] are then applied to refine the details by considering the features in different layers computed in the backbone. An upscaling stage consists of doubling the resolution of the input channels, followed by two 3 \u00d7 3 convolution layers (each followed by a ReLU). The resulting channels are summed with the adjusted features from the corresponding backbone layer. Specifically, the backbone features are adjusted for the upscaling task by a 3 \u00d7 3 convolution layer, followed by a ReLU. The last upscaling stage (which contains only resolution doubling, followed by a single 3 \u00d7 3 convolution layer) is followed by a softmax to produce the final segmentation probability map. The refinement pathway is shown in Figure 5.\n\n\nDiscriminative Segmentation Tracker\n\nThis section outlines application of the discriminative segmentation network from Section 3 to online general object tracking. Given a single supervised training example from the first frame, the network produces target segmentation masks in all the remaining frames. However, some applications and most tracking benchmarks require target location represented by a bounding box. For most benchmarks, the bounding box is trivially obtained by fitting an  Figure 5. The refinement pathway combines the GIM and GEM channels and gradually upscales them by using adjusted features from the backbone. The UP * is a modified UP layer (see the text).\n\naxis-aligned bounding box that tightly fits a segmentation mask. However, for the benchmark requiring a rotated bounding box, we propose a simple fitting procedure in Section 4.1. The tracking steps are outlined in Section 4.2.\n\n\nBounding box fitting module\n\nThe segmentation probability map from the discriminative segmentation network (Section 3) is thresholded at 0.5 probability to yield a binary segmentation mask. Only the largest connected component within the mask is kept and an ellipse is fitted to its outline by least squares [14]. The ellipse center, major and minor axis make up an initial estimate of the rotated bounding box. This is typically the most liberal solution with oversized rectangles, preferring most of the target pixels lying within its area, but accounts poorly for the presence of the background pixels within the region. We therefore further reduce the rectangle sides in direction of the major axes by optimizing the following modified overlap cost function IoU MOD between the predicted segmentation mask and fitted rectangle using a coordinate descent:\nIoU MOD = N + IN \u03b1N \u2212 IN + N + IN + N + OUT ,(3)\nwhere N + IN and N + OUT denote the number of foreground pixels within and outside the rectangle, respectively, and \n\n\nTracking with D3S\n\nInitialization. D3S is initialized on the first frame using the ground truth target location. The GEM and GIM initialization details depend on whether the target ground truth is presented by a bounding box or a segmentation mask. If a ground truth bounding box is available, the GEM follows the initialization procedure proposed in [10], which involves training both the dimensionality reduction network and the DCF by backprop on the first frame by considering the region four times the target size. On the other hand, if a segmentation mask is available, the ground truth target bounding box is first approximated by an axis-aligned rectangle encompassing the segmented target.\n\nIn case a segmentation mask is available, the GIM is initialized by extracting foreground samples from the target mask and background samples from the neighborhood four times the target size. However, if only a bounding box is available, an approximate ground truth segmentation mask is constructed first. Foreground samples are extracted from within the bounding box, while the background samples are extracted from a four times larger neighborhood. A tracking iteration of D3S is then run on the initialization region to infer a proxi ground truth segmentation mask. The final foreground and background samples are extracted from this mask. This process might be iterated a few times (akin to GrabCut [43]), however, we did not observe improvements and chose only a single iteration for initialization speed and simplicity.\n\nTracking. During tracking, when a new frame arrives, a region four times the target size is extracted at previous target location. The region is processed by the discriminative segmentation network from Section 3 to produce the output segmentation mask. A rotated bounding box is fitted to the mask (Section 4.1) if required by the evaluation protocol. The DCF in the GEM is updated on the estimated target location following the backprop update procedure [10].\n\n\nExperiments\n\n\nImplementation details\n\nThe backbone network in D3S is composed of the first four layers of ResNet50, pre-trained on ImageNet for object classification. The backbone features are extracted from the target search region resized to 384 \u00d7 384 pixels. The background tradeoff parameter from (3) is set to \u03b1 = 0.25 and the top K = 3 similarities are used in GIM (2). We verified in a preliminary analysis that performance is insensitive to exact values of these parameters, and we therefore keep the same values in all experiments. Network pre-training. The GIM pathway and the refinement pathway are pre-trained on 3471 training segmentation sequences from Youtube-VOS [51]. A training sample is constructed by uniformly sampling a pair of images and the corresponding segmentation masks from the same sequence within a range of 50 frames. To increase the robustness to possibly inaccurate GEM localization, the target location channel was constructed by perturbing ground truth locations uniformly from [\u2212 1 8 \u03c3, 1 8 \u03c3], where \u03c3 is target size. The network was trained by 64 image pairs batches for 40 epochs with 1000 iterations per epoch using the ADAM optimizer [21] with learning rate set to 10 \u22123 and with 0.2 decay every 15 epochs. The training loss was a crossentropy between the predicted and ground truth segmentation mask. The training takes 20 hours on a single GPU. Speed. A Pytorch implementation of D3S runs at 25fps on a single NVidia GTX 1080 GPU, while 1.3s is required for loading the network to GPU and initialization.\n\n\nEvaluation on Tracking Datasets\n\nD3S was evaluated on four major short-term tracking datasets: VOT2016 [23], VOT2018 [24], GOT-10k [19] and TrackingNet [35]. In the following we discuss the results obtained on each of the datasets.\n\nVOT2016 and VOT2018 datasets each consist of 60 sequences. Targets are annotated by rotated rectangles to enable a more thorough localization accuracy evaluation compared to the related datasets. The standard VOT evaluation protocol [26] is used in which the tracker is reset upon tracking failure. Performance is measured by accuracy (average overlap over successfully tracked frames), robustness (failure rate) and the EAO (expected average overlap), which is a principled combination of the former two measures [25].\n\nThe following state-of-the-art (sota) trackers are considered on VOT2016: the VOT2016 top performers CCOT [13] and TCNN [36], a sota segmentation-based discriminative correlation filter CSR-DCF [33], and most recently published sota deep trackers SiamRPN [28], SPM [49], ASRCF [9], SiamMask [50] and ATOM [10].\n\nResults reported in Table 1 show that D3S outperforms all tested trackers on all three measures by a large margin. In EAO measure, D3S outperforms the top sota tracker SPM by 14%, and simultaneously outperforms the top robust sota ATOM by 25% in robustness. The top sota performer in accuracy is the segmentation-based tracker SiamMask. D3S outperforms this tracker by over 3% in accuracy and approximately by 50% in robustness.\n\nThe VOT2016 dataset contains per-frame target segmentation masks which can be used to evaluate segmentation performance on the small and challenging targets present. We have thus compared D3S with the most recent segmentation tracker SiamMask by computing the average IoU between the ground truth and predicted segmentation masks during periods of successful tracks (i.e., segmentation accuracy). D3S achieves a 0.66 average IoU, while SiamMask IoU is 0.63. A nearly 5% improvement speaks of a considerable accuracy of the D3S segmentation mask prediction. On the VOT2018 dataset, D3S is compared with the following sota trackers: the top VOT2018 performer LADCF [52] and the most recent sota trackers DaSi-amRPN [54], SiamRPN++ [27], ATOM [10], SPM [49], ASRCF [9] and SiamMask [50]. Results are reported in Table 2. Again, D3S outperforms all sota trackers in all measures. The top sota trackers in EAO, accuracy and robustness are SiamRPN++, SiamMask and LADCF, respectively. D3S outperforms the SiamRPN++ in EAO by 18%, SiamMask in accuracy by over 5% and LADCF by over 6% in robustness. Note that SiamMask is a segmentation tracker, which explains the top accuracy among sota. D3S outperforms this tracker by over 45% in robustness, which is attributed to the discriminative formulation within the single-pass segmentation mask computation.\n\nGOT-10k is a recent large-scale high-diversity dataset consisting of 10k video sequences with targets annotated by axis-aligned bounding boxes. The trackers are evaluated on 180 test sequences with 84 different object classes and 32 motion patterns, while the rest of the sequences form a training set. A tracker is initialized on the first frame and let to track to the end of the sequence. Trackers are ranked according to the average overlap, but success rates (SR 0.5 and SR 0.75 ) are reported at two overlap thresholds 0.5 and 0.75, respectively, for detailed analysis 1 . The following top-performing sota trackers are used in comparison [19]: SiamFCv2 [46], SiamFC [2], GOTURN [16], CCOT [13], MDNet [37] and the most-recent ATOM [10] and SiamMask [50]. We emphasize that D3S is not finetuned on the training set, while some of the top-performing sota trackers we use in comparison do utilize the GOT-10k training set. Results on GOT-10k are reported in Table 3. D3S outperforms all top-performing sota by a large margin in all performance measures, and achieves approximately 60% boost in average overlap compared to the SiamFCv2, which is a top-performer on [19] benchmark. It also outperforms the most recent ATOM and SiamMask trackers by over 7% and over 15% in average overlap, respectively. This demonstrates considerable generalization ability over a diverse set of target types.\n\nTrackingNet is another large-scale dataset for training and testing trackers. The training set consists of over 30k video sequences, while the testing set contains 511 sequences. A tracker is initialized on the first frame and let to track to the end of the sequence. Trackers are ranked according to the area under the success rate curve (AUC), precision (Prec.) and normalized precision (Prec. N ). The reader is referred to [35] for further details about the performance measures. The performance of D3S is compared with the top-performing sota trackers according to [35]: ECO [11], SiamFC [2], CFNet [46], MDNet [37] and most recent sota trackers ATOM [10], SiamMask [50] and SiamRPN++ [27]. D3S significantly outperforms the sota reported in [35] and is on par with SiamRPN++, SiamMask and ATOM. Note that D3S is trained only on 3471 sequences from YouTube-VOS [51], while both, ATOM and SiamRPN++ are finetuned on much larger datasets (31k, and over 380k sequences, respectively), which include the TrackingNet training set. This further supports a considerable generalization capability of D3S, which is primarily trained for segmentation, not tracking.\n\n\nAblation Study\n\nAn ablation study was performed on VOT2018 using the reset-based protocol [26] to expose the contributions of different components of the D3S architecture. The following variations of D3S were created: (i) D3S without the GIM foreground similarity channel F (D3SF ); (ii) D3S without the GIM target posterior channel P (D3SP) ; (iii) D3S with only the GEM output channel and without GIM channels F and P (D3SFP); (iv) D3S without the GEM output channel L (D3SL); (v) D3S in which the DCF is not updated from the position estimated by D3S, but rather from the position estimated by the DCF in GEM (D3S\u016a ). Two additional D3S versions with different bounding box fitting methods were included: a minimal area rotated bounding box that contains all foreground pixels (D3S MA ) and a min-max axis-aligned bounding box (D3S MM ). All variations were re-trained on the same dataset as the original D3S.\n\nResults of the ablation study are presented in Table 5. Removal of the foreground similarity channel from GIM (D3SF) causes a 4.5% performance drop, while removal of the target posterior channel (D3SP) reduces the performance by 13.5%. The accuracy of both variants is comparable to the original D3S, while the number of failures increases. In conclusion, each, foreground similarity and posterior channel individually contribute to robust target localization.\n\nRemoval of the entire GIM module i.e., F and P (D3SFP) reduces the overall tracking performance by 27%. The accuracy drops by 14%, while the number of failures increases by 56%. This speaks of crucial importance of the GIM module for accurate segmentation as well as tracking robustness.\n\nRemoval of the GEM module (D3SL) reduces the tracking performance by nearly 50%. This is primarily due to significant reduction of the robustness -the number of failures increases by over 270%. Thus the GEM module is crucial for robust target selection in the segmentation process.\n\nFinally, updating the DCF in GEM module by its own estimated position rather than the position estimated by the final segmentation (D3S\u016a) reduces the overall performance by 7.5%, primarily at a cost of significant increase in the number of failures (over 15%). Thus, accurate target position estimation from D3S crucially affects the learning of the DCF in GEM and consequently the overall tracking performance.\n\nReplacing the proposed bounding box fitting method (Section 4.1) with the minimal area rotated bounding box (D3S MA ) results in a 9% reduction in EAO and a 6% reduction in accuracy. This is still a state-of-the-art result, which means that the D3S performance boost can be primarily attributed to the segmentation mask quality. The min-max bounding box fitting method (D3S MM ) leads to a 19% EAO and 14% accuracy reduction. Thus D3S does benefit from the rotated bounding box estimation.\n\n\nEvaluation on Segmentation Datasets\n\nSegmentation capabilities of D3S were analyzed on two popular video object segmentation benchmarks DAVIS16 [38] and DAVIS17 [40]. Under the DAVIS protocol, the segmentation algorithm is initialized on the first frame by a segmentation mask. The algorithm is then required to output the segmentation mask for all the remaining frames in the video. Performance is evaluated by two measures averaged over the sequences: mean Jaccard index (J M ) and mean F-measure (F M ). Jaccard index represents a per-pixel intersection over union between the ground-truth and the predicted segmentation mask. The F-measure is a harmonic mean of precision and recall calculated between the contours extracted from the ground-truth and the predicted segmentation masks. For further details on these performance measures, the reader is referred to [38,34].\n\nD3S is compared to several sota video object segmentation methods specialized to the DAVIS challenge setup: OS-VOS [5], OnAVOS [48], OSMN [53], FAVOS [7], VM [18] and PML [6]. In addition, we include the most recent segmentation-based tracker SiamMask [50], which is the only published method that performs well on both, shortterm tracking as well as on video object segmentation benchmarks.\n\nResults are shown in Table 6. D3S performs on par with most of the video object segmentation top performers on DAVIS. Compared to top performer on DAVIS2016, the performance of D3S is 12% and 14% lower in the average Jaccard index and the F-measure, respectively. On DAVIS2017 this difference is even smaller -a 6% drop in Jaccard index and 8% drop in F-measure compared to the top-performer OnAVOS. This is quite remarkable, considering that D3S is 200 times faster. Furthermore, D3S delivers a comparable segmentation accuracy as pure segmentation methods ASMN and PML, while being orders of magnitude faster and achieving a near-realtime video object segmenta-  Table 5. VOT2018 -ablation study. Removing: the GIM foreground similarity channel (F), the GIM foreground probability channel (P), both GIM channels (FP) and the GEM channel (L). The DCF in GEM is updated from its own position estimation rather than position estimated by D3S (\u016a). D3S with a minimal area rotated bounding box (MA) and a min-max axis-aligned bounding box (MM). tion, which is particularly important for many video editing applications. D3S also outperforms the only tracking and segmentation method SiamMask with respect to all measures. On average the segmentation is improved by over 5% in the Jaccard index and the contour accuracy-based F-measure. See Figure 6 for further qualitative comparison of D3S and SiamMask on challenging targets.\n\n\nConclusion\n\nA deep single-shot discriminative segmentation tracker -D3S -was introduced. The tracker leverages two models from the extremes of the spectrum: a geometrically invariant model and a geometrically restricted Euclidean model. The two models localize the target in parallel pathways and complement each other to achieve high segmentation accuracy of deformable targets and robust discrimination of the target from distractors. The end-to-end trainable network architecture is the first single-shot pipeline with online adaptation that tightly connects discriminative tracking with accurate segmentation.\n\nD3S outperforms state-of-the-art trackers on the VOT2016, VOT2018 and GOT-10k benchmarks and performs on par with top trackers on TrackingNet, regardless of the fact that some of the tested trackers were re-trained for specific datasets. In contrast, D3S was trained once on Youtube-VOS (for segmentation only) and the same version was used in all benchmarks. Tests on DAVIS16 and DAVIS17 segmentation benchmarks show performance close to top segmentation methods while running up to  Figure 6. D3S vs. SiamMask segmentation quality. Bolt2: SiamMask drifts to a similar object, D3S leverages the discriminative learning in GEM to robustly track the selected target. Hand: the rigid template in SiamMask fails on a deforming target, the GIM model in D3S successfully tracks despite a significant deformation. Paragliding: clutter causes drift and failure of SiamMask while in D3S, the combination of the GIM and GEM models leads to accurate and robust segmentation.\n\n200\u00d7 faster, close to real-time. D3S significantly outperforms recent top segmentation tracker SiamMask on all bechmarks in all metrics and contributes towards narrowing the gap between two, currently separate, domains of short-term tracking and video object segmentation, thus blurring the boundary between the two.\n\n\nQualitative examples\n\nWe provide here additional qualitative examples of tracking and segmentation. Video sequences are collected from the VOT2016 [23], GOT-10k [19] and DAVIS [38,40] datasets. Output of the D3S is segmentation mask and it is visualized with yellow color. A bounding box is fitted to the predicted segmentation mask and shown in red. Tracker reports binary segmentation mask for DAVIS, rotated bounding box for VOT sequences, while axis-aligned bounding box is required by the GOT-10k evaluation protocol. The following tracking and segmentation conditions are visualized:\n\n\u2022 Figure 7 demonstrates the discriminative power of D3S by visualizing tracking in presence of distractors, i.e., visually similar objects.\n\n\u2022 Figure 8 shows a remarkable segmentation accuracy and robustness of D3S on tracking of deformable objects and parts of objects.\n\n\u2022 Figure 9 shows tracking in sequences we have identified as particularly challenging for the current state-ofthe-art. It includes small objects and tracking parts of objects.\n\n\u2022 Figure 10 shows (near real-time) video object segmentation results on DAVIS16 [38] and DAVIS17 [40] datasets.  Figure 8. Examples of appearance changes and deforming targets. The geometrically invariant model (GIM) successfully segments the target due to geometrically unrestricted representation even under target rotation (gymnastics1), articulated (iceskater1 and octopus) or significantly change its shape (hand and snake).  Figure 9. Difficult examples to track and segment. Underwater video sequences diver and fish are challenging due to the low contrast between the target and background -the D3S refinement pathway still produces an accurate segmentation. Small target in leaf sequence is successfully tracked and segmented due to the large search range (4-times of target size) and the discriminative architecture, even though several similar leaves are in the vicinity and all leaves undergo abrupt motion due to a high wind. Target rotation and scale change in motocross sequence are successfully addressed by the geometrically invariant model (GIM). A challenging scenario where only the head of the cat and deer is tracked. Foreground and background feature vectors in GIM and combination with GEM prevent segmenting the whole animal as the target.  Figure 10. Video object segmentation on DAVIS datasets. D3S produces a highly accurate segmentation in near real-time. In sequences with multiple objects the tracker was run independently on each target.\n\nFigure 1 .\n1The D3S tracker represents the target by two models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations (GIM -geometrically invariant model), the other assuming a rigid object with motion well approximated by an euclidean transformation (GEM -geometrically constrained Euclidean model). The D3S, exploiting the complementary strengths of GIM and GEM, provides both state-of-the-art localisation and accurate segmentation, even in the presence of substantial deformation.\n\nFigure 2 .\n2The D3S segmentation architecture. The backbone features are processed by the GEM and GIM pathways, producing the target location (L), foreground similarity (F) and target posterior (P) channels. The output of the three channels are concatenated and refined into a detailed segmentation map.\n\nFigure 3 .\n3GIM -the geometrically invariant model -features are matched to the features in the foreground-background model {X F , X B } to obtain the target (F) and background (B) similarity channels. The posterior channel (P) is the softmax of F and B.\n\nFigure 4 .\n4GEM -the geometrically constrained Euclidean modelreduces the backbone features dimensionality and correlates them with a DCF. The target localisation channel (L) is the distance transform to the maximum correlation response, representing the per-pixel confidence of target presence.\n\n\nN \u2212 IN denotes the number of background pixels within the rectangle. The scalar \u03b1 controls the contribution of N \u2212 IN . The bounding box fitting method is very fast and takes on average only 2ms.\n\nFigure 7 .\n7Sequences with distractors (similar objects in the target vicinity). D3S segments the correct target even though a similar target is close (or even overlapping). These examples show discriminative power of the proposed tracker achieved by the discriminative GIM and GEM.\n\n\nSegmenta on outputSoftmax \n\nUpscale 2x \n\n3x3 conv, \n32, ReLu \n\n3x3 conv, \n16, ReLu \n\n3x3 conv, \n4, ReLu \n\n3x3 conv, \nN , ReLu \n\nOUT \n\n3x3 conv, \nN , ReLu \n\nOUT \n\n3x3 conv, \n64, ReLu \n\n64 \n\nN IN \nN OUT \n\n32 \n16 \n4 \n2 \n\nResNet-50 backbone \n\nLayer-2 \nLayer-1 \nConv-1 \n\nUP Upscale module \n\nUP \nUP \nUP \n\nL \nF \nP \n\nUP * \n\n\n\n\nTable 4. TrackingNet test set -comparison with state-of-the-art trackers.D3S \n\nSPM \nSiamMask ATOM \nASRCF SiamRPN CSRDCF CCOT TCNN \nEAO \u2191 1 0.493 2 0.434 \n3 0.433 \n0.430 \n0.391 \n0.344 \n0.338 \n0.331 \n0.325 \nAcc. \u2191 \n1 0.66 \n3 0.62 \n2 0.64 \n0.61 \n0.56 \n0.56 \n0.51 \n0.54 \n0.55 \nRob. \u2193 \n1 0.131 \n0.210 \n0.214 2 0.180 3 0.187 \n0.302 \n0.238 \n0.238 \n0.268 \n\nTable 1. VOT2016 -comparison with state-of-the-art trackers. \n\nD3S \nSiamRPN++ ATOM \nLADCF DaSiamRPN SiamMask SPM ASRCF \nEAO \u2191 1 0.489 \n2 0.414 3 0.401 \n0.389 \n0.383 \n0.380 0.338 \n0.328 \nAcc. \u2191 \n1 0.64 \n3 0.60 \n0.59 \n0.51 \n0.59 \n2 0.61 \n0.58 \n0.49 \nRob. \u2193 \n1 0.150 \n0.234 3 0.204 2 0.159 \n0.276 \n0.276 0.300 \n0.234 \n\nTable 2. VOT2018 -comparison with state-of-the-art trackers. \n\nD3S \nATOM SiamMask SiamFCv2 SiamFC GOTURN CCOT MDNet \nAO \u2191 \n1 59.7 2 55.6 \n3 51.4 \n37.4 \n34.8 \n34.2 \n32.5 \n29.9 \nSR 0.75 \u2191 1 46.2 2 40.2 \n3 36.6 \n14.4 \n9.8 \n12.4 \n10.7 \n9.9 \nSR 0.5 \u2191 \n1 67.6 2 63.5 \n3 58.7 \n40.4 \n35.3 \n37.5 \n32.8 \n30.3 \n\nTable 3. GOT-10k test set -comparison with state-of-the-art trackers . \n\nD3S SiamRPN++ SiamMask ATOM MDNet CFNet SiamFC ECO \nAUC \u2191 \n2 72.8 \n1 73.3 \n3 72.5 \n70.3 \n60.6 \n57.8 \n57.1 55.4 \nPrec. \u2191 \n2 66.4 \n1 69.4 \n2 66.4 3 64.8 \n56.5 \n53.3 \n53.3 49.2 \nPrec. N \u2191 \n76.8 \n1 80.0 \n2 77.8 3 77.1 \n70.5 \n65.4 \n66.3 61.8 \n\n\n\n\nTable 6. State-of-the-art comparison on the DAVIS16 and DAVIS17 segmentation datasets. Average Jaccard index and Fmeasure are denoted as JM16 and FM 16 on DAVIS16 dataset and JM 17 and FM 17 on DAVIS17 dataset, respectively.J M \n\n16 \n\nF M \n\n16 \n\nJ M \n\n17 \n\nF M \n\n17 \n\nFPS \nD3S \n75.4 \n72.6 \n57.8 \n63.8 \n25.0 \nSiamMask [50] \n71.7 \n67.8 \n54.3 \n58.5 \n55.0 \nOnAVOS [48] \n86.1 \n84.9 \n61.6 \n69.1 \n0.1 \nFAVOS [7] \n82.4 \n79.5 \n54.6 \n61.8 \n0.8 \nVM [18] \n81.0 \n-\n56.6 \n-\n3.1 \nOSVOS [5] \n79.8 \n80.6 \n56.6 \n63.9 \n0.1 \nPML [6] \n75.5 \n79.3 \n-\n-\n3.6 \nOSMN [53] \n74.0 \n72.9 \n52.5 \n57.1 \n8.0 \n\n\nSuccess rate denotes percentage of frames where predicted region overlaps with the ground-truth region more than the threshold.\nAcknowledgements. This work was supported by Slovenian research agency program P2-0214 and projects J2-8175 and J2-9433. A.Luke\u017ei\u010d was financed by the Young researcher program of the ARRS. J. Matas was supported by the Czech Science Foundation grant GA18-05360S.\nStaple: Complementary learners for real-time tracking. Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip H S Torr, Comp. Vis. Patt. Recognition. Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, and Philip H. S. Torr. Staple: Complementary learn- ers for real-time tracking. In Comp. Vis. Patt. Recognition, pages 1401-1409, June 2016. 2\n\nFully-convolutional siamese networks for object tracking. Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, Philip Hs Torr, ECCV VOT Workshop. 7Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. In ECCV VOT Workshop, 2016. 1, 2, 6, 7\n\nUnveiling the power of deep tracking. Goutam Bhat, Joakim Johnander, Martin Danelljan, Michael Fahad Shahbaz Khan, Felsberg, Proc. European Conf. Computer Vision. European Conf. Computer VisionGoutam Bhat, Joakim Johnander, Martin Danelljan, Fahad Shahbaz Khan, and Michael Felsberg. Unveiling the power of deep tracking. In Proc. European Conf. Computer Vision, pages 493-509, 2018. 1\n\nVisual object tracking using adaptive correlation filters. Ross David S Bolme, Beveridge, A Bruce, Yui Man Draper, Lui, Comp. Vis. Patt. Recognition. David S Bolme, J Ross Beveridge, Bruce A Draper, and Yui Man Lui. Visual object tracking using adaptive corre- lation filters. In Comp. Vis. Patt. Recognition, pages 2544- 2550, 2010. 2\n\nOneshot video object segmentation. Kevis-Kokitsi Sergi Caelles, Jordi Maninis, Laura Pont-Tuset, Daniel Leal-Taixe, Luc Cremers, Van Gool, Comp. Vis. Patt. Recognition. Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, and Luc Van Gool. One- shot video object segmentation. In Comp. Vis. Patt. Recog- nition, 2017. 2, 7, 8\n\nBlazingly fast video object segmentation with pixelwise metric learning. Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, Luc Van Gool, Comp. Vis. Patt. Recognition. Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, and Luc Van Gool. Blazingly fast video object segmentation with pixel- wise metric learning. In Comp. Vis. Patt. Recognition, 2018. 2, 7, 8\n\nFast and accurate online video object segmentation via tracking parts. Jingchun Cheng, Yi-Hsuan Tsai, Wei-Chih Hung, Shengjin Wang, Ming-Hsuan Yang, Comp. Vis. Patt. Recognition. Jingchun Cheng, Yi-Hsuan Tsai, Wei-Chih Hung, Shengjin Wang, and Ming-Hsuan Yang. Fast and accurate online video object segmentation via tracking parts. In Comp. Vis. Patt. Recognition, 2018. 2, 7, 8\n\nKernel-based object tracking. D Comaniciu, V Ramesh, P Meer, IEEE Trans. Pattern Anal. Mach. Intell. 255D. Comaniciu, V. Ramesh, and P. Meer. Kernel-based ob- ject tracking. IEEE Trans. Pattern Anal. Mach. Intell., 25(5):564-577, May 2003. 2\n\nVisual tracking via adaptive spatially-regularized correlation filters. Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, Jianhua Li, Comp. Vis. Patt. Recognition. 56Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, and Jian- hua Li. Visual tracking via adaptive spatially-regularized correlation filters. In Comp. Vis. Patt. Recognition, 2019. 5, 6\n\nATOM: Accurate tracking by overlap maximization. Martin Danelljan, Goutam Bhat, Michael Fahad Shahbaz Khan, Felsberg, Comp. Vis. Patt. Recognition. 67Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. ATOM: Accurate tracking by overlap maximization. In Comp. Vis. Patt. Recognition, June 2019. 1, 2, 3, 5, 6, 7\n\nECO: Efficient convolution operators for tracking. Martin Danelljan, Goutam Bhat, Michael Fahad Shahbaz Khan, Felsberg, Comp. Vis. Patt. Recognition. Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. ECO: Efficient convolution operators for tracking. In Comp. Vis. Patt. Recognition, pages 6638-6646, 2017. 1, 2, 7\n\nAdaptive color attributes for realtime visual tracking. Martin Danelljan, Michael Fahad Shahbaz Khan, Joost Felsberg, Van De Weijer, Comp. Vis. Patt. Recognition. Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg, and Joost van de Weijer. Adaptive color attributes for real- time visual tracking. In Comp. Vis. Patt. Recognition, pages 1090-1097, 2014. 2\n\nBeyond correlation filters: learning continuous convolution operators for visual tracking. Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan, Michael Felsberg, Proc. European Conf. Computer Vision. European Conf. Computer Vision56Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan, and Michael Felsberg. Beyond correlation filters: learn- ing continuous convolution operators for visual tracking. In Proc. European Conf. Computer Vision, pages 472-488, 2016. 3, 5, 6\n\nA buyer's guide to conic fitting. Andrew Fitzgibbon, Robert B Fisher, Proc. British Machine Vision Conference. British Machine Vision ConferenceAndrew Fitzgibbon and Robert B. Fisher. A buyer's guide to conic fitting. In Proc. British Machine Vision Conference, pages 513-522, 1995. 4\n\nA twofold siamese network for real-time object tracking. Anfeng He, Chong Luo, Xinmei Tian, Wenjun Zeng, Comp. Vis. Patt. Recognition. Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A twofold siamese network for real-time object tracking. In Comp. Vis. Patt. Recognition, 2018. 2\n\nLearning to track at 100 fps with deep regression networks. David Held, Sebastian Thrun, Silvio Savarese, Proc. European Conf. Computer Vision. European Conf. Computer VisionDavid Held, Sebastian Thrun, and Silvio Savarese. Learning to track at 100 fps with deep regression networks. In Proc. European Conf. Computer Vision, pages 749-765, 2016. 6\n\nHighspeed tracking with kernelized correlation filters. J F Henriques, R Caseiro, P Martins, J Batista, IEEE Trans. Pattern Anal. Mach. Intell. 373J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High- speed tracking with kernelized correlation filters. IEEE Trans. Pattern Anal. Mach. Intell., 37(3):583-596, 2015. 2\n\nVideoMatch: Matching based Video Object Segmentation. Yuan-Ting Hu, Jia-Bin Huang, Alexander G Schwing, Proc. European Conf. Computer Vision. European Conf. Computer Vision7Yuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing. VideoMatch: Matching based Video Object Segmentation. In Proc. European Conf. Computer Vision, 2018. 2, 3, 7, 8\n\nGOT-10k: A large high-diversity benchmark for generic object tracking in the wild. Lianghua Huang, Xin Zhao, Kaiqi Huang, IEEE Trans. Pattern Anal. Mach. Intell. 611Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k: A large high-diversity benchmark for generic object tracking in the wild. IEEE Trans. Pattern Anal. Mach. Intell., 2019. 2, 5, 6, 11\n\nObject tracking by reconstruction with view-specific discriminative correlation filters. Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, Jiri Matas, Comp. Vis. Patt. Recognition. Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Ka- marainen, and Jiri Matas. Object tracking by reconstruc- tion with view-specific discriminative correlation filters. In Comp. Vis. Patt. Recognition, 2019. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations (ICLR). Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. 5\n\nThe visual object tracking VOT2017 challenge results. Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder, Tomas Luka Cehovin Zajc, Gustav Vojir, Alan Hager, Abdelrahman Lukezic, Gustavo Eldesokey, Fernandez, 2017. 1The IEEE International Conference on Computer Vision (ICCV. Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder, Luka Cehovin Zajc, Tomas Vojir, Gustav Hager, Alan Lukezic, Abdelrahman Eldesokey, and Gustavo Fernandez. The visual object tracking VOT2017 challenge results. In The IEEE International Conference on Computer Vision (ICCV), 2017. 1\n\nThe visual object tracking VOT2016 challenge results. Matej Kristan, Ale\u0161 Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder, Tomas Luka\u010dehovin, Gustav Vojir, Alan Hager, Gustavo Luke\u017ei\u010d, Proc. European Conf. Computer Vision. European Conf. Computer Vision511Matej Kristan, Ale\u0161 Leonardis, Jiri Matas, Michael Fels- berg, Roman Pflugfelder, Luka\u010cehovin, Tomas Vojir, Gus- tav Hager, Alan Luke\u017ei\u010d, and Gustavo et al. Fernandez. The visual object tracking VOT2016 challenge results. In Proc. European Conf. Computer Vision, 2016. 2, 5, 11\n\nThe sixth visual object tracking VOT2018 challenge results. Matej Kristan, Ale\u0161 Leonardis, Ji\u0159\u00ed Matas, Michael Felsberg, Roman Pflugfelder, Luka\u010dehovin Zajc, Tom\u00e1\u0161 Voj\u00edr, Goutam Bhat, Alan Luke\u017ei\u010d, Abdelrahman Eldesokey, Gustavo Fern\u00e1ndez, Computer Vision -ECCV 2018 Workshops. Matej Kristan, Ale\u0161 Leonardis, Ji\u0159\u00ed Matas, Michael Fels- berg, Roman Pflugfelder, Luka\u010cehovin Zajc, Tom\u00e1\u0161 Voj\u00edr, Goutam Bhat, Alan Luke\u017ei\u010d, Abdelrahman Eldesokey, Gus- tavo Fern\u00e1ndez, and et al. The sixth visual object tracking VOT2018 challenge results. In Computer Vision -ECCV 2018 Workshops, pages 3-53, 2018. 1, 2, 5\n\nThe visual object tracking VOT2015 challenge results. Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin, Gustavo Fernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, Roman , Int. Conf. Computer Vision. Matej Kristan, Jiri Matas, Ales Leonardis, Michael Fels- berg, Luka Cehovin, Gustavo Fernandez, Tomas Vojir, Gus- tav Hager, Georg Nebehay, and Roman et al. Pflugfelder. The visual object tracking VOT2015 challenge results. In Int. Conf. Computer Vision, 2015. 5\n\nA novel performance evaluation methodology for single-target trackers. M Kristan, J Matas, A Leonardis, T Vojir, R Pflugfelder, G Fernandez, G Nebehay, F Porikli, L Cehovin, IEEE Trans. Pattern Anal. Mach. Intell. 57M. Kristan, J. Matas, A. Leonardis, T. Vojir, R. Pflugfelder, G. Fernandez, G. Nebehay, F. Porikli, and L. Cehovin. A novel performance evaluation methodology for single-target trackers. IEEE Trans. Pattern Anal. Mach. Intell., 2016. 5, 7\n\nSiamRPN++: Evolution of siamese visual tracking with very deep networks. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan, Comp. Vis. Patt. Recognition. 7Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. SiamRPN++: Evolution of siamese visual tracking with very deep networks. In Comp. Vis. Patt. Recog- nition, 2019. 1, 2, 6, 7\n\nHigh performance visual tracking with siamese region proposal network. Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, Xiaolin Hu, Comp. Vis. Patt. Recognition. 15Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with siamese region pro- posal network. In Comp. Vis. Patt. Recognition, 2018. 1, 2, 5\n\nGradNet: Gradient-guided network for visual object tracking. Peixia Li, Boyu Chen, Wanli Ouyang, Dong Wang, Xiaoyun Yang, Huchuan Lu, Int. Conf. Computer Vision. Peixia Li, Boyu Chen, Wanli Ouyang, Dong Wang, Xiaoyun Yang, and Huchuan Lu. GradNet: Gradient-guided network for visual object tracking. In Int. Conf. Computer Vision, 2019. 2\n\nA scale adaptive kernel correlation filter tracker with feature integration. Yang Li, Jianke Zhu, Proc. European Conf. Computer Vision. European Conf. Computer VisionYang Li and Jianke Zhu. A scale adaptive kernel correla- tion filter tracker with feature integration. In Proc. European Conf. Computer Vision, pages 254-265, 2014. 2\n\nDeformable parts correlation filters for robust visual tracking. A Luke\u017ei\u010d, L \u010c Zajc, M Kristan, IEEE Trans. on Cyber. 486A. Luke\u017ei\u010d, L.\u010c. Zajc, and M. Kristan. Deformable parts correlation filters for robust visual tracking. IEEE Trans. on Cyber., 48(6):1849-1861, 2017. 1, 2\n\nFuCoLoT -a fully-correlational longterm tracker. Alan Lukezic, Tomas Luka Cehovin Zajc, Jiri Vojir, Matej Matas, Kristan, In ACCV. 2Alan Lukezic, Luka Cehovin Zajc, Tomas Vojir, Jiri Matas, and Matej Kristan. FuCoLoT -a fully-correlational long- term tracker. In ACCV, 2018. 2\n\nDiscriminative correlation filter with channel and spatial reliability. Alan Luke\u017ei\u010d, Tom\u00e1\u0161 Voj\u00ed\u0159, Luka\u010dehovin Zajc, Ji\u0159\u00ed Matas, Matej Kristan, Comp. Vis. Patt. Recognition. Alan Luke\u017ei\u010d, Tom\u00e1\u0161 Voj\u00ed\u0159, Luka\u010cehovin Zajc, Ji\u0159\u00ed Matas, and Matej Kristan. Discriminative correlation filter with channel and spatial reliability. In Comp. Vis. Patt. Recog- nition, pages 6309-6318, 2017. 1, 2, 3, 5\n\nLearning to detect natural image boundaries using local brightness, color, and texture cues. D R Martin, C C Fowlkes, J Malik, IEEE Trans. Pattern Anal. Mach. Intell. 265D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to de- tect natural image boundaries using local brightness, color, and texture cues. IEEE Trans. Pattern Anal. Mach. Intell., 26(5):530-549, 2004. 7\n\nTrackingNet: A large-scale dataset and benchmark for object tracking in the wild. Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, Bernard Ghanem, Proc. European Conf. Computer Vision. European Conf. Computer Vision7Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al- subaihi, and Bernard Ghanem. TrackingNet: A large-scale dataset and benchmark for object tracking in the wild. In Proc. European Conf. Computer Vision, September 2018. 2, 5, 7\n\nModeling and propagating CNNs in a tree structure for visual tracking. Hyeonseob Nam, Mooyeol Baek, Bohyung Han, arXiv:1608.07242arXiv preprintHyeonseob Nam, Mooyeol Baek, and Bohyung Han. Model- ing and propagating CNNs in a tree structure for visual track- ing. arXiv preprint arXiv:1608.07242, 2016. 5\n\nLearning multi-domain convolutional neural networks for visual tracking. Hyeonseob Nam, Bohyung Han, Comp. Vis. Patt. Recognition. 67Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural networks for visual tracking. In Comp. Vis. Patt. Recognition, pages 4293-4302, June 2016. 6, 7\n\nA benchmark dataset and evaluation methodology for video object segmentation. F Perazzi, J Pont-Tuset, B Mcwilliams, L Van Gool, M Gross, A Sorkine-Hornung, Comp. Vis. Patt. Recognition. 11F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In Comp. Vis. Patt. Recognition, 2016. 1, 2, 7, 11\n\nLearning to refine object segments. Pedro O Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Dollar, Proc. European Conf. Computer Vision. European Conf. Computer VisionPedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, and Piotr Dollar. Learning to refine object segments. In Proc. Euro- pean Conf. Computer Vision, pages 75-91, 2016. 4\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alexander Sorkine-Hornung, Luc Van Gool, arXiv:1704.00675The 2017 DAVIS challenge on video object segmentation. 11Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar- bel\u00e1ez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 DAVIS challenge on video object segmentation. arXiv:1704.00675, 2017. 1, 2, 7, 11\n\nIn defense of color-based model-free tracking. Horst Possegger, Thomas Mauthner, Horst Bischof, Comp. Vis. Patt. Recognition. Horst Possegger, Thomas Mauthner, and Horst Bischof. In defense of color-based model-free tracking. In Comp. Vis. Patt. Recognition, June 2015. 2\n\nU-Net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015, pages 234-241, 2015. 4\n\nGrabcut-interactive foreground extraction using iterated graph cuts. Carsten Rother, Vladimir Kolmogorov, Andrew Blake, ACM Transactions on Graphics. 5Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. Grabcut-interactive foreground extraction using iterated graph cuts. ACM Transactions on Graphics (SIGGRAPH), August 2004. 5\n\nSiamese instance search for tracking. Ran Tao, Efstratios Gavves, Arnold W M Smeulders, Comp. Vis. Patt. Recognition. Ran Tao, Efstratios Gavves, and Arnold W M Smeulders. Siamese instance search for tracking. In Comp. Vis. Patt. Recognition, pages 1420-1429, 2016. 2\n\nParametric exponential linear unit for deep convolutional neural networks. Ludovic Trottier, Philippe Gigu, Brahim Chaib-Draa, 16th IEEE International Conference on Machine Learning and Applications (ICMLA). Ludovic Trottier, Philippe Gigu, Brahim Chaib-draa, et al. Parametric exponential linear unit for deep convolutional neural networks. In 2017 16th IEEE International Confer- ence on Machine Learning and Applications (ICMLA), pages 207-214, 2017. 3\n\nEnd-to-end representation learning for correlation filter based tracking. Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, Philip H S Torr, Comp. Vis. Patt. Recognition. 67Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and Philip H. S. Torr. End-to-end representation learning for correlation filter based tracking. In Comp. Vis. Patt. Recognition, pages 2805-2813, 2017. 2, 6, 7\n\nFEELVOS: Fast end-to-end embedding learning for video object segmentation. Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen, Comp. Vis. Patt. Recognition. Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. FEELVOS: Fast end-to-end embedding learning for video object seg- mentation. In Comp. Vis. Patt. Recognition, 2019. 2\n\nOnline adaptation of convolutional neural networks for video object segmentation. Paul Voigtlaender, Bastian Leibe, Proc. British Machine Vision Conference. British Machine Vision ConferencePaul Voigtlaender and Bastian Leibe. Online adaptation of convolutional neural networks for video object segmenta- tion. In Proc. British Machine Vision Conference, 2017. 2, 7, 8\n\nSPM-Tracker: Series-parallel matching for real-time visual object tracking. Guangting Wang, Chong Luo, Zhiwei Xiong, Wenjun Zeng, Comp. Vis. Patt. Recognition. 56Guangting Wang, Chong Luo, Zhiwei Xiong, and Wenjun Zeng. SPM-Tracker: Series-parallel matching for real-time visual object tracking. In Comp. Vis. Patt. Recognition, 2019. 5, 6\n\nFast online object tracking and segmentation: A unifying approach. Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H S Torr, Comp. Vis. Patt. Recognition. 7Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip H.S. Torr. Fast online object tracking and segmenta- tion: A unifying approach. In Comp. Vis. Patt. Recognition, June 2019. 2, 5, 6, 7, 8\n\nYouTube-VOS: A large-scale video object segmentation benchmark. Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, Thomas Huang, arXiv:1809.033277Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. YouTube- VOS: A large-scale video object segmentation benchmark. arXiv:1809.03327, 2018. 1, 2, 5, 7\n\nLearning adaptive discriminative correlation filters via temporal consistency preserving spatial feature selection for robust visual object tracking. T Xu, Z Feng, X Wu, J Kittler, IEEE Trans. Image Proc. 2811T. Xu, Z. Feng, X. Wu, and J. Kittler. Learning adap- tive discriminative correlation filters via temporal consis- tency preserving spatial feature selection for robust visual object tracking. IEEE Trans. Image Proc., 28(11):5596- 5609, 2019. 6\n\nEfficient video object segmentation via network modulation. Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Aggelos K Katsaggelos, Comp. Vis. Patt. Recognition. Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos K. Katsaggelos. Efficient video object segmen- tation via network modulation. In Comp. Vis. Patt. Recogni- tion, 2018. 2, 7, 8\n\nDistractor-aware siamese networks for visual object tracking. Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, Weiming Hu, Proc. European Conf. Computer Vision. European Conf. Computer Vision16Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and Weiming Hu. Distractor-aware siamese networks for visual object tracking. In Proc. European Conf. Computer Vision, pages 103-119, 2018. 1, 6\n", "annotations": {"author": "[{\"end\":176,\"start\":58},{\"end\":271,\"start\":177},{\"end\":364,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":63},{\"end\":187,\"start\":182},{\"end\":285,\"start\":278}]", "author_first_name": "[{\"end\":62,\"start\":58},{\"end\":181,\"start\":177},{\"end\":277,\"start\":272}]", "author_affiliation": "[{\"end\":175,\"start\":99},{\"end\":270,\"start\":189},{\"end\":363,\"start\":287}]", "title": "[{\"end\":55,\"start\":1},{\"end\":419,\"start\":365}]", "venue": null, "abstract": "[{\"end\":1633,\"start\":421}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1925,\"start\":1921},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1928,\"start\":1925},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1971,\"start\":1967},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1973,\"start\":1971},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1976,\"start\":1973},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1978,\"start\":1976},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1981,\"start\":1978},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1984,\"start\":1981},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2588,\"start\":2584},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2591,\"start\":2588},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2827,\"start\":2823},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3163,\"start\":3159},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3166,\"start\":3163},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3186,\"start\":3182},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3459,\"start\":3455},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3741,\"start\":3737},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3850,\"start\":3847},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3881,\"start\":3877},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3924,\"start\":3920},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3953,\"start\":3949},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4041,\"start\":4037},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5123,\"start\":5119},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5126,\"start\":5123},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5129,\"start\":5126},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5728,\"start\":5724},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5731,\"start\":5728},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5734,\"start\":5731},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5737,\"start\":5734},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5835,\"start\":5831},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5838,\"start\":5835},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5893,\"start\":5889},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6445,\"start\":6442},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6572,\"start\":6569},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6575,\"start\":6572},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6578,\"start\":6575},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6581,\"start\":6578},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6636,\"start\":6633},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6639,\"start\":6636},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6745,\"start\":6742},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6748,\"start\":6745},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6769,\"start\":6765},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6991,\"start\":6987},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7019,\"start\":7015},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7140,\"start\":7136},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7202,\"start\":7198},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7288,\"start\":7284},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7396,\"start\":7393},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7399,\"start\":7396},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7402,\"start\":7399},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7678,\"start\":7675},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7787,\"start\":7783},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7945,\"start\":7941},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7948,\"start\":7945},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7991,\"start\":7987},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8101,\"start\":8097},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8248,\"start\":8244},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8251,\"start\":8248},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8275,\"start\":8271},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8277,\"start\":8275},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8280,\"start\":8277},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8282,\"start\":8280},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8285,\"start\":8282},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8406,\"start\":8402},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8426,\"start\":8423},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10551,\"start\":10547},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11842,\"start\":11838},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11845,\"start\":11842},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11848,\"start\":11845},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11962,\"start\":11958},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12041,\"start\":12037},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12220,\"start\":12216},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12308,\"start\":12304},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13640,\"start\":13636},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13643,\"start\":13640},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15584,\"start\":15580},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16654,\"start\":16650},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17706,\"start\":17702},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18286,\"start\":18282},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18664,\"start\":18661},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18973,\"start\":18969},{\"end\":19317,\"start\":19314},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19470,\"start\":19466},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19948,\"start\":19944},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19962,\"start\":19958},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19976,\"start\":19972},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19997,\"start\":19993},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20311,\"start\":20307},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20592,\"start\":20588},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20705,\"start\":20701},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20719,\"start\":20715},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20793,\"start\":20789},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20854,\"start\":20850},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20864,\"start\":20860},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20875,\"start\":20872},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20890,\"start\":20886},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20904,\"start\":20900},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22004,\"start\":22000},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22054,\"start\":22050},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22070,\"start\":22066},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22081,\"start\":22077},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22091,\"start\":22087},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22102,\"start\":22099},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22120,\"start\":22116},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23333,\"start\":23329},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23348,\"start\":23344},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23360,\"start\":23357},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23373,\"start\":23369},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23384,\"start\":23380},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23396,\"start\":23392},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23426,\"start\":23422},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23444,\"start\":23440},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23856,\"start\":23852},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24511,\"start\":24507},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24654,\"start\":24650},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24664,\"start\":24660},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24676,\"start\":24673},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24688,\"start\":24684},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24700,\"start\":24696},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24740,\"start\":24736},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24755,\"start\":24751},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24774,\"start\":24770},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24831,\"start\":24827},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24950,\"start\":24946},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25337,\"start\":25333},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28244,\"start\":28240},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28261,\"start\":28257},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28966,\"start\":28962},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28969,\"start\":28966},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29090,\"start\":29087},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29103,\"start\":29099},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":29114,\"start\":29110},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29125,\"start\":29122},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29134,\"start\":29130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29146,\"start\":29143},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":29228,\"start\":29224},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32843,\"start\":32839},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32857,\"start\":32853},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32872,\"start\":32868},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32875,\"start\":32872},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33816,\"start\":33812},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33833,\"start\":33829},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38842,\"start\":38840}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35758,\"start\":35202},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36063,\"start\":35759},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36319,\"start\":36064},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36616,\"start\":36320},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36814,\"start\":36617},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37098,\"start\":36815},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37417,\"start\":37099},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38698,\"start\":37418},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39277,\"start\":38699}]", "paragraph": "[{\"end\":2129,\"start\":1649},{\"end\":3198,\"start\":2131},{\"end\":3605,\"start\":3200},{\"end\":3925,\"start\":3607},{\"end\":4420,\"start\":3927},{\"end\":4816,\"start\":4422},{\"end\":5635,\"start\":4818},{\"end\":6179,\"start\":5637},{\"end\":7340,\"start\":6196},{\"end\":8133,\"start\":7342},{\"end\":8892,\"start\":8135},{\"end\":9428,\"start\":8932},{\"end\":9747,\"start\":9470},{\"end\":10357,\"start\":9749},{\"end\":10703,\"start\":10359},{\"end\":10910,\"start\":10744},{\"end\":11328,\"start\":10948},{\"end\":11911,\"start\":11372},{\"end\":12322,\"start\":11913},{\"end\":12761,\"start\":12324},{\"end\":13300,\"start\":12784},{\"end\":14358,\"start\":13302},{\"end\":15040,\"start\":14398},{\"end\":15269,\"start\":15042},{\"end\":16130,\"start\":15301},{\"end\":16296,\"start\":16180},{\"end\":16997,\"start\":16318},{\"end\":17824,\"start\":16999},{\"end\":18287,\"start\":17826},{\"end\":19838,\"start\":18328},{\"end\":20072,\"start\":19874},{\"end\":20593,\"start\":20074},{\"end\":20905,\"start\":20595},{\"end\":21335,\"start\":20907},{\"end\":22682,\"start\":21337},{\"end\":24078,\"start\":22684},{\"end\":25240,\"start\":24080},{\"end\":26155,\"start\":25259},{\"end\":26617,\"start\":26157},{\"end\":26906,\"start\":26619},{\"end\":27189,\"start\":26908},{\"end\":27602,\"start\":27191},{\"end\":28093,\"start\":27604},{\"end\":28970,\"start\":28133},{\"end\":29363,\"start\":28972},{\"end\":30789,\"start\":29365},{\"end\":31405,\"start\":30804},{\"end\":32371,\"start\":31407},{\"end\":32689,\"start\":32373},{\"end\":33281,\"start\":32714},{\"end\":33422,\"start\":33283},{\"end\":33553,\"start\":33424},{\"end\":33730,\"start\":33555},{\"end\":35201,\"start\":33732}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10743,\"start\":10704},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10947,\"start\":10911},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16179,\"start\":16131}]", "table_ref": "[{\"end\":20934,\"start\":20927},{\"end\":22153,\"start\":22146},{\"end\":23653,\"start\":23646},{\"end\":26211,\"start\":26204},{\"end\":29393,\"start\":29386},{\"end\":30037,\"start\":30030}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1647,\"start\":1635},{\"attributes\":{\"n\":\"2.\"},\"end\":6194,\"start\":6182},{\"attributes\":{\"n\":\"3.\"},\"end\":8930,\"start\":8895},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9468,\"start\":9431},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11370,\"start\":11331},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12782,\"start\":12764},{\"attributes\":{\"n\":\"4.\"},\"end\":14396,\"start\":14361},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15299,\"start\":15272},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16316,\"start\":16299},{\"attributes\":{\"n\":\"5.\"},\"end\":18301,\"start\":18290},{\"attributes\":{\"n\":\"5.1.\"},\"end\":18326,\"start\":18304},{\"attributes\":{\"n\":\"5.2.\"},\"end\":19872,\"start\":19841},{\"attributes\":{\"n\":\"5.3.\"},\"end\":25257,\"start\":25243},{\"attributes\":{\"n\":\"5.4.\"},\"end\":28131,\"start\":28096},{\"attributes\":{\"n\":\"6.\"},\"end\":30802,\"start\":30792},{\"end\":32712,\"start\":32692},{\"end\":35213,\"start\":35203},{\"end\":35770,\"start\":35760},{\"end\":36075,\"start\":36065},{\"end\":36331,\"start\":36321},{\"end\":36826,\"start\":36816}]", "table": "[{\"end\":37417,\"start\":37119},{\"end\":38698,\"start\":37493},{\"end\":39277,\"start\":38925}]", "figure_caption": "[{\"end\":35758,\"start\":35215},{\"end\":36063,\"start\":35772},{\"end\":36319,\"start\":36077},{\"end\":36616,\"start\":36333},{\"end\":36814,\"start\":36619},{\"end\":37098,\"start\":36828},{\"end\":37119,\"start\":37101},{\"end\":37493,\"start\":37420},{\"end\":38925,\"start\":38701}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4815,\"start\":4807},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5286,\"start\":5278},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9398,\"start\":9390},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11327,\"start\":11319},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12759,\"start\":12751},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13056,\"start\":13048},{\"end\":14357,\"start\":14349},{\"end\":14860,\"start\":14852},{\"end\":30710,\"start\":30702},{\"end\":31900,\"start\":31892},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33293,\"start\":33285},{\"end\":33434,\"start\":33426},{\"end\":33565,\"start\":33557},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33743,\"start\":33734},{\"end\":33853,\"start\":33845},{\"end\":34171,\"start\":34163},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35007,\"start\":34998}]", "bib_author_first_name": "[{\"end\":39728,\"start\":39724},{\"end\":39745,\"start\":39741},{\"end\":39762,\"start\":39756},{\"end\":39779,\"start\":39773},{\"end\":39794,\"start\":39788},{\"end\":39798,\"start\":39795},{\"end\":40105,\"start\":40101},{\"end\":40122,\"start\":40118},{\"end\":40137,\"start\":40133},{\"end\":40139,\"start\":40138},{\"end\":40157,\"start\":40151},{\"end\":40176,\"start\":40167},{\"end\":40431,\"start\":40425},{\"end\":40444,\"start\":40438},{\"end\":40462,\"start\":40456},{\"end\":40481,\"start\":40474},{\"end\":40837,\"start\":40833},{\"end\":40865,\"start\":40864},{\"end\":40880,\"start\":40873},{\"end\":41159,\"start\":41146},{\"end\":41180,\"start\":41175},{\"end\":41195,\"start\":41190},{\"end\":41214,\"start\":41208},{\"end\":41230,\"start\":41227},{\"end\":41553,\"start\":41548},{\"end\":41565,\"start\":41560},{\"end\":41585,\"start\":41578},{\"end\":41597,\"start\":41594},{\"end\":41904,\"start\":41896},{\"end\":41920,\"start\":41912},{\"end\":41935,\"start\":41927},{\"end\":41950,\"start\":41942},{\"end\":41967,\"start\":41957},{\"end\":42236,\"start\":42235},{\"end\":42249,\"start\":42248},{\"end\":42259,\"start\":42258},{\"end\":42525,\"start\":42520},{\"end\":42535,\"start\":42531},{\"end\":42549,\"start\":42542},{\"end\":42559,\"start\":42554},{\"end\":42572,\"start\":42565},{\"end\":42844,\"start\":42838},{\"end\":42862,\"start\":42856},{\"end\":42876,\"start\":42869},{\"end\":43180,\"start\":43174},{\"end\":43198,\"start\":43192},{\"end\":43212,\"start\":43205},{\"end\":43524,\"start\":43518},{\"end\":43543,\"start\":43536},{\"end\":43569,\"start\":43564},{\"end\":43920,\"start\":43914},{\"end\":43939,\"start\":43932},{\"end\":43955,\"start\":43950},{\"end\":43977,\"start\":43970},{\"end\":44340,\"start\":44334},{\"end\":44359,\"start\":44353},{\"end\":44361,\"start\":44360},{\"end\":44649,\"start\":44643},{\"end\":44659,\"start\":44654},{\"end\":44671,\"start\":44665},{\"end\":44684,\"start\":44678},{\"end\":44937,\"start\":44932},{\"end\":44953,\"start\":44944},{\"end\":44967,\"start\":44961},{\"end\":45278,\"start\":45277},{\"end\":45280,\"start\":45279},{\"end\":45293,\"start\":45292},{\"end\":45304,\"start\":45303},{\"end\":45315,\"start\":45314},{\"end\":45611,\"start\":45602},{\"end\":45623,\"start\":45616},{\"end\":45640,\"start\":45631},{\"end\":45642,\"start\":45641},{\"end\":45980,\"start\":45972},{\"end\":45991,\"start\":45988},{\"end\":46003,\"start\":45998},{\"end\":46333,\"start\":46329},{\"end\":46344,\"start\":46340},{\"end\":46359,\"start\":46354},{\"end\":46382,\"start\":46369},{\"end\":46399,\"start\":46395},{\"end\":46699,\"start\":46698},{\"end\":46715,\"start\":46710},{\"end\":46998,\"start\":46993},{\"end\":47012,\"start\":47008},{\"end\":47028,\"start\":47024},{\"end\":47043,\"start\":47036},{\"end\":47059,\"start\":47054},{\"end\":47078,\"start\":47073},{\"end\":47104,\"start\":47098},{\"end\":47116,\"start\":47112},{\"end\":47135,\"start\":47124},{\"end\":47152,\"start\":47145},{\"end\":47614,\"start\":47609},{\"end\":47628,\"start\":47624},{\"end\":47644,\"start\":47640},{\"end\":47659,\"start\":47652},{\"end\":47675,\"start\":47670},{\"end\":47694,\"start\":47689},{\"end\":47714,\"start\":47708},{\"end\":47726,\"start\":47722},{\"end\":47741,\"start\":47734},{\"end\":48166,\"start\":48161},{\"end\":48180,\"start\":48176},{\"end\":48196,\"start\":48192},{\"end\":48211,\"start\":48204},{\"end\":48227,\"start\":48222},{\"end\":48252,\"start\":48241},{\"end\":48264,\"start\":48259},{\"end\":48278,\"start\":48272},{\"end\":48289,\"start\":48285},{\"end\":48310,\"start\":48299},{\"end\":48329,\"start\":48322},{\"end\":48761,\"start\":48756},{\"end\":48775,\"start\":48771},{\"end\":48787,\"start\":48783},{\"end\":48806,\"start\":48799},{\"end\":48821,\"start\":48817},{\"end\":48838,\"start\":48831},{\"end\":48855,\"start\":48850},{\"end\":48869,\"start\":48863},{\"end\":48882,\"start\":48877},{\"end\":48897,\"start\":48892},{\"end\":49264,\"start\":49263},{\"end\":49275,\"start\":49274},{\"end\":49284,\"start\":49283},{\"end\":49297,\"start\":49296},{\"end\":49306,\"start\":49305},{\"end\":49321,\"start\":49320},{\"end\":49334,\"start\":49333},{\"end\":49345,\"start\":49344},{\"end\":49356,\"start\":49355},{\"end\":49723,\"start\":49721},{\"end\":49731,\"start\":49728},{\"end\":49741,\"start\":49736},{\"end\":49754,\"start\":49748},{\"end\":49770,\"start\":49762},{\"end\":49783,\"start\":49777},{\"end\":50091,\"start\":50089},{\"end\":50102,\"start\":50096},{\"end\":50111,\"start\":50108},{\"end\":50121,\"start\":50116},{\"end\":50134,\"start\":50127},{\"end\":50413,\"start\":50407},{\"end\":50422,\"start\":50418},{\"end\":50434,\"start\":50429},{\"end\":50447,\"start\":50443},{\"end\":50461,\"start\":50454},{\"end\":50475,\"start\":50468},{\"end\":50767,\"start\":50763},{\"end\":50778,\"start\":50772},{\"end\":51086,\"start\":51085},{\"end\":51097,\"start\":51096},{\"end\":51099,\"start\":51098},{\"end\":51107,\"start\":51106},{\"end\":51351,\"start\":51347},{\"end\":51366,\"start\":51361},{\"end\":51390,\"start\":51386},{\"end\":51403,\"start\":51398},{\"end\":51652,\"start\":51648},{\"end\":51667,\"start\":51662},{\"end\":51686,\"start\":51675},{\"end\":51697,\"start\":51693},{\"end\":51710,\"start\":51705},{\"end\":52062,\"start\":52061},{\"end\":52064,\"start\":52063},{\"end\":52074,\"start\":52073},{\"end\":52076,\"start\":52075},{\"end\":52087,\"start\":52086},{\"end\":52431,\"start\":52423},{\"end\":52444,\"start\":52440},{\"end\":52457,\"start\":52451},{\"end\":52474,\"start\":52468},{\"end\":52493,\"start\":52486},{\"end\":52884,\"start\":52875},{\"end\":52897,\"start\":52890},{\"end\":52911,\"start\":52904},{\"end\":53192,\"start\":53183},{\"end\":53205,\"start\":53198},{\"end\":53493,\"start\":53492},{\"end\":53504,\"start\":53503},{\"end\":53518,\"start\":53517},{\"end\":53532,\"start\":53531},{\"end\":53544,\"start\":53543},{\"end\":53553,\"start\":53552},{\"end\":53863,\"start\":53858},{\"end\":53865,\"start\":53864},{\"end\":53884,\"start\":53876},{\"end\":53895,\"start\":53890},{\"end\":53912,\"start\":53907},{\"end\":54163,\"start\":54158},{\"end\":54184,\"start\":54176},{\"end\":54199,\"start\":54194},{\"end\":54214,\"start\":54209},{\"end\":54234,\"start\":54225},{\"end\":54255,\"start\":54252},{\"end\":54597,\"start\":54592},{\"end\":54615,\"start\":54609},{\"end\":54631,\"start\":54626},{\"end\":54887,\"start\":54883},{\"end\":54908,\"start\":54901},{\"end\":54924,\"start\":54918},{\"end\":55300,\"start\":55293},{\"end\":55317,\"start\":55309},{\"end\":55336,\"start\":55330},{\"end\":55597,\"start\":55594},{\"end\":55613,\"start\":55603},{\"end\":55632,\"start\":55622},{\"end\":55907,\"start\":55900},{\"end\":55926,\"start\":55918},{\"end\":55939,\"start\":55933},{\"end\":56360,\"start\":56356},{\"end\":56375,\"start\":56371},{\"end\":56392,\"start\":56388},{\"end\":56410,\"start\":56404},{\"end\":56426,\"start\":56420},{\"end\":56430,\"start\":56427},{\"end\":56774,\"start\":56770},{\"end\":56795,\"start\":56789},{\"end\":56809,\"start\":56802},{\"end\":56826,\"start\":56819},{\"end\":56840,\"start\":56833},{\"end\":56859,\"start\":56848},{\"end\":57201,\"start\":57197},{\"end\":57223,\"start\":57216},{\"end\":57570,\"start\":57561},{\"end\":57582,\"start\":57577},{\"end\":57594,\"start\":57588},{\"end\":57608,\"start\":57602},{\"end\":57898,\"start\":57893},{\"end\":57907,\"start\":57905},{\"end\":57919,\"start\":57915},{\"end\":57939,\"start\":57932},{\"end\":57950,\"start\":57944},{\"end\":57954,\"start\":57951},{\"end\":58261,\"start\":58257},{\"end\":58272,\"start\":58266},{\"end\":58285,\"start\":58279},{\"end\":58300,\"start\":58291},{\"end\":58312,\"start\":58306},{\"end\":58328,\"start\":58320},{\"end\":58341,\"start\":58335},{\"end\":58714,\"start\":58713},{\"end\":58720,\"start\":58719},{\"end\":58728,\"start\":58727},{\"end\":58734,\"start\":58733},{\"end\":59084,\"start\":59078},{\"end\":59097,\"start\":59091},{\"end\":59110,\"start\":59104},{\"end\":59126,\"start\":59118},{\"end\":59140,\"start\":59133},{\"end\":59142,\"start\":59141},{\"end\":59448,\"start\":59443},{\"end\":59459,\"start\":59454},{\"end\":59468,\"start\":59466},{\"end\":59476,\"start\":59473},{\"end\":59487,\"start\":59481},{\"end\":59500,\"start\":59493}]", "bib_author_last_name": "[{\"end\":39739,\"start\":39729},{\"end\":39754,\"start\":39746},{\"end\":39771,\"start\":39763},{\"end\":39786,\"start\":39780},{\"end\":39803,\"start\":39799},{\"end\":40116,\"start\":40106},{\"end\":40131,\"start\":40123},{\"end\":40149,\"start\":40140},{\"end\":40165,\"start\":40158},{\"end\":40181,\"start\":40177},{\"end\":40436,\"start\":40432},{\"end\":40454,\"start\":40445},{\"end\":40472,\"start\":40463},{\"end\":40500,\"start\":40482},{\"end\":40510,\"start\":40502},{\"end\":40851,\"start\":40838},{\"end\":40862,\"start\":40853},{\"end\":40871,\"start\":40866},{\"end\":40887,\"start\":40881},{\"end\":40892,\"start\":40889},{\"end\":41173,\"start\":41160},{\"end\":41188,\"start\":41181},{\"end\":41206,\"start\":41196},{\"end\":41225,\"start\":41215},{\"end\":41238,\"start\":41231},{\"end\":41248,\"start\":41240},{\"end\":41558,\"start\":41554},{\"end\":41576,\"start\":41566},{\"end\":41592,\"start\":41586},{\"end\":41606,\"start\":41598},{\"end\":41910,\"start\":41905},{\"end\":41925,\"start\":41921},{\"end\":41940,\"start\":41936},{\"end\":41955,\"start\":41951},{\"end\":41972,\"start\":41968},{\"end\":42246,\"start\":42237},{\"end\":42256,\"start\":42250},{\"end\":42264,\"start\":42260},{\"end\":42529,\"start\":42526},{\"end\":42540,\"start\":42536},{\"end\":42552,\"start\":42550},{\"end\":42563,\"start\":42560},{\"end\":42575,\"start\":42573},{\"end\":42854,\"start\":42845},{\"end\":42867,\"start\":42863},{\"end\":42895,\"start\":42877},{\"end\":42905,\"start\":42897},{\"end\":43190,\"start\":43181},{\"end\":43203,\"start\":43199},{\"end\":43231,\"start\":43213},{\"end\":43241,\"start\":43233},{\"end\":43534,\"start\":43525},{\"end\":43562,\"start\":43544},{\"end\":43578,\"start\":43570},{\"end\":43593,\"start\":43580},{\"end\":43930,\"start\":43921},{\"end\":43948,\"start\":43940},{\"end\":43968,\"start\":43956},{\"end\":43986,\"start\":43978},{\"end\":44351,\"start\":44341},{\"end\":44368,\"start\":44362},{\"end\":44652,\"start\":44650},{\"end\":44663,\"start\":44660},{\"end\":44676,\"start\":44672},{\"end\":44689,\"start\":44685},{\"end\":44942,\"start\":44938},{\"end\":44959,\"start\":44954},{\"end\":44976,\"start\":44968},{\"end\":45290,\"start\":45281},{\"end\":45301,\"start\":45294},{\"end\":45312,\"start\":45305},{\"end\":45323,\"start\":45316},{\"end\":45614,\"start\":45612},{\"end\":45629,\"start\":45624},{\"end\":45650,\"start\":45643},{\"end\":45986,\"start\":45981},{\"end\":45996,\"start\":45992},{\"end\":46009,\"start\":46004},{\"end\":46338,\"start\":46334},{\"end\":46352,\"start\":46345},{\"end\":46367,\"start\":46360},{\"end\":46393,\"start\":46383},{\"end\":46405,\"start\":46400},{\"end\":46708,\"start\":46700},{\"end\":46722,\"start\":46716},{\"end\":46726,\"start\":46724},{\"end\":47006,\"start\":46999},{\"end\":47022,\"start\":47013},{\"end\":47034,\"start\":47029},{\"end\":47052,\"start\":47044},{\"end\":47071,\"start\":47060},{\"end\":47096,\"start\":47079},{\"end\":47110,\"start\":47105},{\"end\":47122,\"start\":47117},{\"end\":47143,\"start\":47136},{\"end\":47162,\"start\":47153},{\"end\":47173,\"start\":47164},{\"end\":47622,\"start\":47615},{\"end\":47638,\"start\":47629},{\"end\":47650,\"start\":47645},{\"end\":47668,\"start\":47660},{\"end\":47687,\"start\":47676},{\"end\":47706,\"start\":47695},{\"end\":47720,\"start\":47715},{\"end\":47732,\"start\":47727},{\"end\":47749,\"start\":47742},{\"end\":48174,\"start\":48167},{\"end\":48190,\"start\":48181},{\"end\":48202,\"start\":48197},{\"end\":48220,\"start\":48212},{\"end\":48239,\"start\":48228},{\"end\":48257,\"start\":48253},{\"end\":48270,\"start\":48265},{\"end\":48283,\"start\":48279},{\"end\":48297,\"start\":48290},{\"end\":48320,\"start\":48311},{\"end\":48339,\"start\":48330},{\"end\":48769,\"start\":48762},{\"end\":48781,\"start\":48776},{\"end\":48797,\"start\":48788},{\"end\":48815,\"start\":48807},{\"end\":48829,\"start\":48822},{\"end\":48848,\"start\":48839},{\"end\":48861,\"start\":48856},{\"end\":48875,\"start\":48870},{\"end\":48890,\"start\":48883},{\"end\":49272,\"start\":49265},{\"end\":49281,\"start\":49276},{\"end\":49294,\"start\":49285},{\"end\":49303,\"start\":49298},{\"end\":49318,\"start\":49307},{\"end\":49331,\"start\":49322},{\"end\":49342,\"start\":49335},{\"end\":49353,\"start\":49346},{\"end\":49364,\"start\":49357},{\"end\":49726,\"start\":49724},{\"end\":49734,\"start\":49732},{\"end\":49746,\"start\":49742},{\"end\":49760,\"start\":49755},{\"end\":49775,\"start\":49771},{\"end\":49787,\"start\":49784},{\"end\":50094,\"start\":50092},{\"end\":50106,\"start\":50103},{\"end\":50114,\"start\":50112},{\"end\":50125,\"start\":50122},{\"end\":50137,\"start\":50135},{\"end\":50416,\"start\":50414},{\"end\":50427,\"start\":50423},{\"end\":50441,\"start\":50435},{\"end\":50452,\"start\":50448},{\"end\":50466,\"start\":50462},{\"end\":50478,\"start\":50476},{\"end\":50770,\"start\":50768},{\"end\":50782,\"start\":50779},{\"end\":51094,\"start\":51087},{\"end\":51104,\"start\":51100},{\"end\":51115,\"start\":51108},{\"end\":51359,\"start\":51352},{\"end\":51384,\"start\":51367},{\"end\":51396,\"start\":51391},{\"end\":51409,\"start\":51404},{\"end\":51418,\"start\":51411},{\"end\":51660,\"start\":51653},{\"end\":51673,\"start\":51668},{\"end\":51691,\"start\":51687},{\"end\":51703,\"start\":51698},{\"end\":51718,\"start\":51711},{\"end\":52071,\"start\":52065},{\"end\":52084,\"start\":52077},{\"end\":52093,\"start\":52088},{\"end\":52438,\"start\":52432},{\"end\":52449,\"start\":52445},{\"end\":52466,\"start\":52458},{\"end\":52484,\"start\":52475},{\"end\":52500,\"start\":52494},{\"end\":52888,\"start\":52885},{\"end\":52902,\"start\":52898},{\"end\":52915,\"start\":52912},{\"end\":53196,\"start\":53193},{\"end\":53209,\"start\":53206},{\"end\":53501,\"start\":53494},{\"end\":53515,\"start\":53505},{\"end\":53529,\"start\":53519},{\"end\":53541,\"start\":53533},{\"end\":53550,\"start\":53545},{\"end\":53569,\"start\":53554},{\"end\":53874,\"start\":53866},{\"end\":53888,\"start\":53885},{\"end\":53905,\"start\":53896},{\"end\":53919,\"start\":53913},{\"end\":54174,\"start\":54164},{\"end\":54192,\"start\":54185},{\"end\":54207,\"start\":54200},{\"end\":54223,\"start\":54215},{\"end\":54250,\"start\":54235},{\"end\":54264,\"start\":54256},{\"end\":54607,\"start\":54598},{\"end\":54624,\"start\":54616},{\"end\":54639,\"start\":54632},{\"end\":54899,\"start\":54888},{\"end\":54916,\"start\":54909},{\"end\":54929,\"start\":54925},{\"end\":55307,\"start\":55301},{\"end\":55328,\"start\":55318},{\"end\":55342,\"start\":55337},{\"end\":55601,\"start\":55598},{\"end\":55620,\"start\":55614},{\"end\":55642,\"start\":55633},{\"end\":55916,\"start\":55908},{\"end\":55931,\"start\":55927},{\"end\":55950,\"start\":55940},{\"end\":56369,\"start\":56361},{\"end\":56386,\"start\":56376},{\"end\":56402,\"start\":56393},{\"end\":56418,\"start\":56411},{\"end\":56435,\"start\":56431},{\"end\":56787,\"start\":56775},{\"end\":56800,\"start\":56796},{\"end\":56817,\"start\":56810},{\"end\":56831,\"start\":56827},{\"end\":56846,\"start\":56841},{\"end\":56864,\"start\":56860},{\"end\":57214,\"start\":57202},{\"end\":57229,\"start\":57224},{\"end\":57575,\"start\":57571},{\"end\":57586,\"start\":57583},{\"end\":57600,\"start\":57595},{\"end\":57613,\"start\":57609},{\"end\":57903,\"start\":57899},{\"end\":57913,\"start\":57908},{\"end\":57930,\"start\":57920},{\"end\":57942,\"start\":57940},{\"end\":57959,\"start\":57955},{\"end\":58264,\"start\":58262},{\"end\":58277,\"start\":58273},{\"end\":58289,\"start\":58286},{\"end\":58304,\"start\":58301},{\"end\":58318,\"start\":58313},{\"end\":58333,\"start\":58329},{\"end\":58347,\"start\":58342},{\"end\":58717,\"start\":58715},{\"end\":58725,\"start\":58721},{\"end\":58731,\"start\":58729},{\"end\":58742,\"start\":58735},{\"end\":59089,\"start\":59085},{\"end\":59102,\"start\":59098},{\"end\":59116,\"start\":59111},{\"end\":59131,\"start\":59127},{\"end\":59154,\"start\":59143},{\"end\":59452,\"start\":59449},{\"end\":59464,\"start\":59460},{\"end\":59471,\"start\":59469},{\"end\":59479,\"start\":59477},{\"end\":59491,\"start\":59488},{\"end\":59503,\"start\":59501}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":780697},\"end\":40041,\"start\":39669},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14309034},\"end\":40385,\"start\":40043},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4932842},\"end\":40772,\"start\":40387},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2451356},\"end\":41109,\"start\":40774},{\"attributes\":{\"id\":\"b4\"},\"end\":41473,\"start\":41111},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4710115},\"end\":41823,\"start\":41475},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13798128},\"end\":42203,\"start\":41825},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":823678},\"end\":42446,\"start\":42205},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":198347261},\"end\":42787,\"start\":42448},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":53712235},\"end\":43121,\"start\":42789},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14958161},\"end\":43460,\"start\":43123},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11907849},\"end\":43821,\"start\":43462},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5650694},\"end\":44298,\"start\":43823},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13986799},\"end\":44584,\"start\":44300},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3520436},\"end\":44870,\"start\":44586},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15703426},\"end\":45219,\"start\":44872},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5378407},\"end\":45546,\"start\":45221},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52154491},\"end\":45887,\"start\":45548},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53102207},\"end\":46238,\"start\":45889},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53735404},\"end\":46652,\"start\":46240},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":46937,\"start\":46654},{\"attributes\":{\"doi\":\"2017. 1\",\"id\":\"b21\",\"matched_paper_id\":3710195},\"end\":47553,\"start\":46939},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2519672},\"end\":48099,\"start\":47555},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":59222267},\"end\":48700,\"start\":48101},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":85454762},\"end\":49190,\"start\":48702},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1118174},\"end\":49646,\"start\":49192},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":57189581},\"end\":50016,\"start\":49648},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52255840},\"end\":50344,\"start\":50018},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":202578069},\"end\":50684,\"start\":50346},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206687104},\"end\":51018,\"start\":50686},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14514076},\"end\":51296,\"start\":51020},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":173992143},\"end\":51574,\"start\":51298},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":47346364},\"end\":51966,\"start\":51576},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8165754},\"end\":52339,\"start\":51968},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4455970},\"end\":52802,\"start\":52341},{\"attributes\":{\"doi\":\"arXiv:1608.07242\",\"id\":\"b35\"},\"end\":53108,\"start\":52804},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":973101},\"end\":53412,\"start\":53110},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1949934},\"end\":53820,\"start\":53414},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":15278025},\"end\":54156,\"start\":53822},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b39\"},\"end\":54543,\"start\":54158},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9470021},\"end\":54816,\"start\":54545},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3719281},\"end\":55222,\"start\":54818},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6202829},\"end\":55554,\"start\":55224},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1188600},\"end\":55823,\"start\":55556},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":15085450},\"end\":56280,\"start\":55825},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10520310},\"end\":56693,\"start\":56282},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":67856723},\"end\":57113,\"start\":56695},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6413853},\"end\":57483,\"start\":57115},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":104291974},\"end\":57824,\"start\":57485},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":54475412},\"end\":58191,\"start\":57826},{\"attributes\":{\"doi\":\"arXiv:1809.03327\",\"id\":\"b50\"},\"end\":58561,\"start\":58193},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":51874930},\"end\":59016,\"start\":58563},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":3655063},\"end\":59379,\"start\":59018},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":52045903},\"end\":59769,\"start\":59381}]", "bib_title": "[{\"end\":39722,\"start\":39669},{\"end\":40099,\"start\":40043},{\"end\":40423,\"start\":40387},{\"end\":40831,\"start\":40774},{\"end\":41144,\"start\":41111},{\"end\":41546,\"start\":41475},{\"end\":41894,\"start\":41825},{\"end\":42233,\"start\":42205},{\"end\":42518,\"start\":42448},{\"end\":42836,\"start\":42789},{\"end\":43172,\"start\":43123},{\"end\":43516,\"start\":43462},{\"end\":43912,\"start\":43823},{\"end\":44332,\"start\":44300},{\"end\":44641,\"start\":44586},{\"end\":44930,\"start\":44872},{\"end\":45275,\"start\":45221},{\"end\":45600,\"start\":45548},{\"end\":45970,\"start\":45889},{\"end\":46327,\"start\":46240},{\"end\":46696,\"start\":46654},{\"end\":46991,\"start\":46939},{\"end\":47607,\"start\":47555},{\"end\":48159,\"start\":48101},{\"end\":48754,\"start\":48702},{\"end\":49261,\"start\":49192},{\"end\":49719,\"start\":49648},{\"end\":50087,\"start\":50018},{\"end\":50405,\"start\":50346},{\"end\":50761,\"start\":50686},{\"end\":51083,\"start\":51020},{\"end\":51345,\"start\":51298},{\"end\":51646,\"start\":51576},{\"end\":52059,\"start\":51968},{\"end\":52421,\"start\":52341},{\"end\":53181,\"start\":53110},{\"end\":53490,\"start\":53414},{\"end\":53856,\"start\":53822},{\"end\":54590,\"start\":54545},{\"end\":54881,\"start\":54818},{\"end\":55291,\"start\":55224},{\"end\":55592,\"start\":55556},{\"end\":55898,\"start\":55825},{\"end\":56354,\"start\":56282},{\"end\":56768,\"start\":56695},{\"end\":57195,\"start\":57115},{\"end\":57559,\"start\":57485},{\"end\":57891,\"start\":57826},{\"end\":58711,\"start\":58563},{\"end\":59076,\"start\":59018},{\"end\":59441,\"start\":59381}]", "bib_author": "[{\"end\":39741,\"start\":39724},{\"end\":39756,\"start\":39741},{\"end\":39773,\"start\":39756},{\"end\":39788,\"start\":39773},{\"end\":39805,\"start\":39788},{\"end\":40118,\"start\":40101},{\"end\":40133,\"start\":40118},{\"end\":40151,\"start\":40133},{\"end\":40167,\"start\":40151},{\"end\":40183,\"start\":40167},{\"end\":40438,\"start\":40425},{\"end\":40456,\"start\":40438},{\"end\":40474,\"start\":40456},{\"end\":40502,\"start\":40474},{\"end\":40512,\"start\":40502},{\"end\":40853,\"start\":40833},{\"end\":40864,\"start\":40853},{\"end\":40873,\"start\":40864},{\"end\":40889,\"start\":40873},{\"end\":40894,\"start\":40889},{\"end\":41175,\"start\":41146},{\"end\":41190,\"start\":41175},{\"end\":41208,\"start\":41190},{\"end\":41227,\"start\":41208},{\"end\":41240,\"start\":41227},{\"end\":41250,\"start\":41240},{\"end\":41560,\"start\":41548},{\"end\":41578,\"start\":41560},{\"end\":41594,\"start\":41578},{\"end\":41608,\"start\":41594},{\"end\":41912,\"start\":41896},{\"end\":41927,\"start\":41912},{\"end\":41942,\"start\":41927},{\"end\":41957,\"start\":41942},{\"end\":41974,\"start\":41957},{\"end\":42248,\"start\":42235},{\"end\":42258,\"start\":42248},{\"end\":42266,\"start\":42258},{\"end\":42531,\"start\":42520},{\"end\":42542,\"start\":42531},{\"end\":42554,\"start\":42542},{\"end\":42565,\"start\":42554},{\"end\":42577,\"start\":42565},{\"end\":42856,\"start\":42838},{\"end\":42869,\"start\":42856},{\"end\":42897,\"start\":42869},{\"end\":42907,\"start\":42897},{\"end\":43192,\"start\":43174},{\"end\":43205,\"start\":43192},{\"end\":43233,\"start\":43205},{\"end\":43243,\"start\":43233},{\"end\":43536,\"start\":43518},{\"end\":43564,\"start\":43536},{\"end\":43580,\"start\":43564},{\"end\":43595,\"start\":43580},{\"end\":43932,\"start\":43914},{\"end\":43950,\"start\":43932},{\"end\":43970,\"start\":43950},{\"end\":43988,\"start\":43970},{\"end\":44353,\"start\":44334},{\"end\":44370,\"start\":44353},{\"end\":44654,\"start\":44643},{\"end\":44665,\"start\":44654},{\"end\":44678,\"start\":44665},{\"end\":44691,\"start\":44678},{\"end\":44944,\"start\":44932},{\"end\":44961,\"start\":44944},{\"end\":44978,\"start\":44961},{\"end\":45292,\"start\":45277},{\"end\":45303,\"start\":45292},{\"end\":45314,\"start\":45303},{\"end\":45325,\"start\":45314},{\"end\":45616,\"start\":45602},{\"end\":45631,\"start\":45616},{\"end\":45652,\"start\":45631},{\"end\":45988,\"start\":45972},{\"end\":45998,\"start\":45988},{\"end\":46011,\"start\":45998},{\"end\":46340,\"start\":46329},{\"end\":46354,\"start\":46340},{\"end\":46369,\"start\":46354},{\"end\":46395,\"start\":46369},{\"end\":46407,\"start\":46395},{\"end\":46710,\"start\":46698},{\"end\":46724,\"start\":46710},{\"end\":46728,\"start\":46724},{\"end\":47008,\"start\":46993},{\"end\":47024,\"start\":47008},{\"end\":47036,\"start\":47024},{\"end\":47054,\"start\":47036},{\"end\":47073,\"start\":47054},{\"end\":47098,\"start\":47073},{\"end\":47112,\"start\":47098},{\"end\":47124,\"start\":47112},{\"end\":47145,\"start\":47124},{\"end\":47164,\"start\":47145},{\"end\":47175,\"start\":47164},{\"end\":47624,\"start\":47609},{\"end\":47640,\"start\":47624},{\"end\":47652,\"start\":47640},{\"end\":47670,\"start\":47652},{\"end\":47689,\"start\":47670},{\"end\":47708,\"start\":47689},{\"end\":47722,\"start\":47708},{\"end\":47734,\"start\":47722},{\"end\":47751,\"start\":47734},{\"end\":48176,\"start\":48161},{\"end\":48192,\"start\":48176},{\"end\":48204,\"start\":48192},{\"end\":48222,\"start\":48204},{\"end\":48241,\"start\":48222},{\"end\":48259,\"start\":48241},{\"end\":48272,\"start\":48259},{\"end\":48285,\"start\":48272},{\"end\":48299,\"start\":48285},{\"end\":48322,\"start\":48299},{\"end\":48341,\"start\":48322},{\"end\":48771,\"start\":48756},{\"end\":48783,\"start\":48771},{\"end\":48799,\"start\":48783},{\"end\":48817,\"start\":48799},{\"end\":48831,\"start\":48817},{\"end\":48850,\"start\":48831},{\"end\":48863,\"start\":48850},{\"end\":48877,\"start\":48863},{\"end\":48892,\"start\":48877},{\"end\":48900,\"start\":48892},{\"end\":49274,\"start\":49263},{\"end\":49283,\"start\":49274},{\"end\":49296,\"start\":49283},{\"end\":49305,\"start\":49296},{\"end\":49320,\"start\":49305},{\"end\":49333,\"start\":49320},{\"end\":49344,\"start\":49333},{\"end\":49355,\"start\":49344},{\"end\":49366,\"start\":49355},{\"end\":49728,\"start\":49721},{\"end\":49736,\"start\":49728},{\"end\":49748,\"start\":49736},{\"end\":49762,\"start\":49748},{\"end\":49777,\"start\":49762},{\"end\":49789,\"start\":49777},{\"end\":50096,\"start\":50089},{\"end\":50108,\"start\":50096},{\"end\":50116,\"start\":50108},{\"end\":50127,\"start\":50116},{\"end\":50139,\"start\":50127},{\"end\":50418,\"start\":50407},{\"end\":50429,\"start\":50418},{\"end\":50443,\"start\":50429},{\"end\":50454,\"start\":50443},{\"end\":50468,\"start\":50454},{\"end\":50480,\"start\":50468},{\"end\":50772,\"start\":50763},{\"end\":50784,\"start\":50772},{\"end\":51096,\"start\":51085},{\"end\":51106,\"start\":51096},{\"end\":51117,\"start\":51106},{\"end\":51361,\"start\":51347},{\"end\":51386,\"start\":51361},{\"end\":51398,\"start\":51386},{\"end\":51411,\"start\":51398},{\"end\":51420,\"start\":51411},{\"end\":51662,\"start\":51648},{\"end\":51675,\"start\":51662},{\"end\":51693,\"start\":51675},{\"end\":51705,\"start\":51693},{\"end\":51720,\"start\":51705},{\"end\":52073,\"start\":52061},{\"end\":52086,\"start\":52073},{\"end\":52095,\"start\":52086},{\"end\":52440,\"start\":52423},{\"end\":52451,\"start\":52440},{\"end\":52468,\"start\":52451},{\"end\":52486,\"start\":52468},{\"end\":52502,\"start\":52486},{\"end\":52890,\"start\":52875},{\"end\":52904,\"start\":52890},{\"end\":52917,\"start\":52904},{\"end\":53198,\"start\":53183},{\"end\":53211,\"start\":53198},{\"end\":53503,\"start\":53492},{\"end\":53517,\"start\":53503},{\"end\":53531,\"start\":53517},{\"end\":53543,\"start\":53531},{\"end\":53552,\"start\":53543},{\"end\":53571,\"start\":53552},{\"end\":53876,\"start\":53858},{\"end\":53890,\"start\":53876},{\"end\":53907,\"start\":53890},{\"end\":53921,\"start\":53907},{\"end\":54176,\"start\":54158},{\"end\":54194,\"start\":54176},{\"end\":54209,\"start\":54194},{\"end\":54225,\"start\":54209},{\"end\":54252,\"start\":54225},{\"end\":54266,\"start\":54252},{\"end\":54609,\"start\":54592},{\"end\":54626,\"start\":54609},{\"end\":54641,\"start\":54626},{\"end\":54901,\"start\":54883},{\"end\":54918,\"start\":54901},{\"end\":54931,\"start\":54918},{\"end\":55309,\"start\":55293},{\"end\":55330,\"start\":55309},{\"end\":55344,\"start\":55330},{\"end\":55603,\"start\":55594},{\"end\":55622,\"start\":55603},{\"end\":55644,\"start\":55622},{\"end\":55918,\"start\":55900},{\"end\":55933,\"start\":55918},{\"end\":55952,\"start\":55933},{\"end\":56371,\"start\":56356},{\"end\":56388,\"start\":56371},{\"end\":56404,\"start\":56388},{\"end\":56420,\"start\":56404},{\"end\":56437,\"start\":56420},{\"end\":56789,\"start\":56770},{\"end\":56802,\"start\":56789},{\"end\":56819,\"start\":56802},{\"end\":56833,\"start\":56819},{\"end\":56848,\"start\":56833},{\"end\":56866,\"start\":56848},{\"end\":57216,\"start\":57197},{\"end\":57231,\"start\":57216},{\"end\":57577,\"start\":57561},{\"end\":57588,\"start\":57577},{\"end\":57602,\"start\":57588},{\"end\":57615,\"start\":57602},{\"end\":57905,\"start\":57893},{\"end\":57915,\"start\":57905},{\"end\":57932,\"start\":57915},{\"end\":57944,\"start\":57932},{\"end\":57961,\"start\":57944},{\"end\":58266,\"start\":58257},{\"end\":58279,\"start\":58266},{\"end\":58291,\"start\":58279},{\"end\":58306,\"start\":58291},{\"end\":58320,\"start\":58306},{\"end\":58335,\"start\":58320},{\"end\":58349,\"start\":58335},{\"end\":58719,\"start\":58713},{\"end\":58727,\"start\":58719},{\"end\":58733,\"start\":58727},{\"end\":58744,\"start\":58733},{\"end\":59091,\"start\":59078},{\"end\":59104,\"start\":59091},{\"end\":59118,\"start\":59104},{\"end\":59133,\"start\":59118},{\"end\":59156,\"start\":59133},{\"end\":59454,\"start\":59443},{\"end\":59466,\"start\":59454},{\"end\":59473,\"start\":59466},{\"end\":59481,\"start\":59473},{\"end\":59493,\"start\":59481},{\"end\":59505,\"start\":59493}]", "bib_venue": "[{\"end\":40580,\"start\":40550},{\"end\":44056,\"start\":44026},{\"end\":44444,\"start\":44411},{\"end\":45046,\"start\":45016},{\"end\":45720,\"start\":45690},{\"end\":47819,\"start\":47789},{\"end\":50852,\"start\":50822},{\"end\":52570,\"start\":52540},{\"end\":53989,\"start\":53959},{\"end\":57305,\"start\":57272},{\"end\":59573,\"start\":59543},{\"end\":39833,\"start\":39805},{\"end\":40200,\"start\":40183},{\"end\":40548,\"start\":40512},{\"end\":40922,\"start\":40894},{\"end\":41278,\"start\":41250},{\"end\":41636,\"start\":41608},{\"end\":42002,\"start\":41974},{\"end\":42304,\"start\":42266},{\"end\":42605,\"start\":42577},{\"end\":42935,\"start\":42907},{\"end\":43271,\"start\":43243},{\"end\":43623,\"start\":43595},{\"end\":44024,\"start\":43988},{\"end\":44409,\"start\":44370},{\"end\":44719,\"start\":44691},{\"end\":45014,\"start\":44978},{\"end\":45363,\"start\":45325},{\"end\":45688,\"start\":45652},{\"end\":46049,\"start\":46011},{\"end\":46435,\"start\":46407},{\"end\":46787,\"start\":46728},{\"end\":47240,\"start\":47182},{\"end\":47787,\"start\":47751},{\"end\":48377,\"start\":48341},{\"end\":48926,\"start\":48900},{\"end\":49404,\"start\":49366},{\"end\":49817,\"start\":49789},{\"end\":50167,\"start\":50139},{\"end\":50506,\"start\":50480},{\"end\":50820,\"start\":50784},{\"end\":51137,\"start\":51117},{\"end\":51427,\"start\":51420},{\"end\":51748,\"start\":51720},{\"end\":52133,\"start\":52095},{\"end\":52538,\"start\":52502},{\"end\":52873,\"start\":52804},{\"end\":53239,\"start\":53211},{\"end\":53599,\"start\":53571},{\"end\":53957,\"start\":53921},{\"end\":54335,\"start\":54282},{\"end\":54669,\"start\":54641},{\"end\":55002,\"start\":54931},{\"end\":55372,\"start\":55344},{\"end\":55672,\"start\":55644},{\"end\":56031,\"start\":55952},{\"end\":56465,\"start\":56437},{\"end\":56894,\"start\":56866},{\"end\":57270,\"start\":57231},{\"end\":57643,\"start\":57615},{\"end\":57989,\"start\":57961},{\"end\":58255,\"start\":58193},{\"end\":58766,\"start\":58744},{\"end\":59184,\"start\":59156},{\"end\":59541,\"start\":59505}]"}}}, "year": 2023, "month": 12, "day": 17}