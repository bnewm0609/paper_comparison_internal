{"id": 222141862, "updated": "2023-10-06 10:52:56.918", "metadata": {"title": "Detecting Attackable Sentences in Arguments", "authors": "[{\"first\":\"Yohan\",\"last\":\"Jo\",\"middle\":[]},{\"first\":\"Seojin\",\"last\":\"Bang\",\"middle\":[]},{\"first\":\"Emaad\",\"last\":\"Manzoor\",\"middle\":[]},{\"first\":\"Eduard\",\"last\":\"Hovy\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Reed\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "publication_date": {"year": 2020, "month": 10, "day": 6}, "abstract": "Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence's attackability is associated with many of these characteristics regarding the sentence's content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.02660", "mag": "3102577836", "acl": "2020.emnlp-main.1", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/JoBMHR20", "doi": "10.18653/v1/2020.emnlp-main.1"}}, "content": {"source": {"pdf_hash": "33e5e4b079535957d1275497f8870ea57762a03d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.02660v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.emnlp-main.1.pdf", "status": "HYBRID"}}, "grobid": {"id": "6197d51036d13d4ecc4ea2d65302ae167786e90f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/33e5e4b079535957d1275497f8870ea57762a03d.txt", "contents": "\n@inproceedings{Jo:2020attack, author Detecting Attackable Sentences in Arguments\n\n\n{Jo, Yohan and Bang, Seojin and Manzoor, EmaadHovy hovy@cmu.edu \nEduard \nReed \nChris } \nYohan Jo yohanj@cs.cmu.edu \nSeojin Bang seojinb@cs.cmu.edu \nEmaad Manzoor \nSchool of Computer Science\nCarnegie Mellon University\nUSA\n\nEduard Hovy \nSchool of Computer Science\nCarnegie Mellon University\nUSA\n\nChris Reed c.a.reed@dundee.ac.uk \nSchool of Computer Science\nCarnegie Mellon University\nUSA\n\nHeinz College of Information Systems and Public Policy\nCarnegie Mellon University\nUSA\n\nCentre for Argument Technology\nUniversity of Dundee\nUK\n\n@inproceedings{Jo:2020attack, author Detecting Attackable Sentences in Arguments\n\nFinding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence's attackability is associated with many of these characteristics regarding the sentence's content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople. 1 Prediction (0.12) Personal (-0.20) Topic37 (-0.21) KialoFreq (0.98) Topic5 (0.39) KialoAttr (0.05) KialoExtr (-0.07) KialoFreq (0.75) Topic5 (0.39) Example (0.11) KialoAttr (0.07) KialoExtr (-0.07) Topic5 (0.39) KialoFreq (0.22) KialoAttr (0.13) Hypothetical (-0.06) KialoExtr (-0.11) KialoFreq (0.45) Topic5 (0.39) KialoAttr (0.26) KialoExtr (-0.05) Use of \"We\" (-0.18) Topic5 (0.39) QuestOther (0.39) Why/How (0.91) Use of \"We\" (-0.18) Topic37 (-0.21) Comparison (0.20) Topic35 (-0.07) KialoFreq (0.72) KialoFreq (0.14) Prediction (0.12) KialoExtr (-0.08) Comparison (0.20) Topic39 (-0.12) KialoFreq (0.86) KialoAttr (0.20) Use of \"We\" (-0.19) KialoFreq (0.23) KialoAttr (0.14) KialoExtr (-0.12) KialoFreq (0.23) KialoAttr (0.22) Normative (0.18) Topic33 (0.08) KialoExtr (-0.12) Prediction (0.12) Personal (-0.20) Topic37 (-0.21) KialoFreq (0.98) Topic5 (0.39) KialoAttr (0.05) KialoExtr (-0.07) KialoFreq (0.75) Topic5 (0.39) Example (0.11) KialoAttr (0.07) KialoExtr (-0.07) Topic5 (0.39) KialoFreq (0.22) KialoAttr (0.13) Hypothetical (-0.06) KialoExtr (-0.11) KialoFreq (0.45) Topic5 (0.39) KialoAttr (0.26) KialoExtr (-0.05) Use of \"We\" (-0.18) Topic5 (0.39) QuestOther (0.39) Why/How (0.91) Use of \"We\" (-0.18) Topic37 (-0.21) Comparison (0.20) Topic35 (-0.07) KialoFreq (0.72) KialoFreq (0.14) Prediction (0.12) KialoExtr (-0.08) Comparison (0.20) Topic39 (-0.12) KialoFreq (0.86) KialoAttr (0.20) Use of \"We\" (-0.19) KialoFreq (0.23) KialoAttr (0.14) KialoExtr (-0.12) KialoFreq (0.23) KialoAttr (0.22) Normative (0.18) Topic33 (0.08) KialoExtr (-0.12) (-0.18) l (-0.20) (-0.18) l (-0.20) (-0.18) l (-0.20) You\" (-0.15) (-0.18) You\" (-0.15) (-0.18) q (0.23) (-0.18) We\" (-0.19) l (-0.20)\n\nIntroduction\n\nEffectively refuting an argument is an important skill in persuasion dialogue, and the first step is to find appropriate points to attack in the argument. Prior work in NLP has studied argument quality (Wachsmuth et al., 2017a;Habernal and Gurevych, 2016a) and counterargument generation (Hua et al., 2019;Wachsmuth et al., 2018). But these studies mainly concern an argument's overall quality and making counterarguments toward the main claim, without investigating what parts of an argument are attackable for successful persuasion. Nevertheless, attacking specific points of an argument is common and effective; in our data of online discussions, challengers who successfully change the original poster's view are 1.5 times more likely to quote specific sentences of the argument for attacks than unsuccessful challengers (Figure 1). In this paper, we examine how to computationally >A society where everyone is equal seems great to me\n\nThat's one of the big problems with communismwhat is equality? Is everyone equal? [...] >it removes some of the basic faults in society, such as poverty, homelessness, joblessness, as well as touching on moral values such as greed, and envy\n\nYes there are problems within society but this doesn't mean there is a fault with society. [...] \n\n\n>I believe a proper Communist society (I.E. one that is not a dictatorship like Joseph Stalin or Fidel Castro)\n\nfurthermore, it is unlikely we could ever get a true communist society due to human nature. [...] OP: I believe that Communism is not as bad as everyone says Figure 1: A comment to a post entitled \"I believe that Communism is not as bad as everyone says\". It quotes and attacks some sentences in the post (red with \">\") detect attackable sentences in arguments. This attackability information would help people make persuasive refutations and strengthen an argument by solidifying potentially attackable points.\n\nTo examine the characteristics of attackable sentences in an argument, we first conduct a qualitative analysis of reasons for attacks in online arguments. Our data comes from discussions in the Change-MyView (CMV) forum on Reddit. In CMV, users challenge the viewpoints of original posters (OPs), and those who succeed receive a \u2206 from the OPs. In this setting, sentences that are attacked and lead to the OP's view change are considered \"attackable\", i.e., targets that are worth attacking. Admittedly, persuasion has to do with \"how\" to attack as well, but this is beyond the scope of this paper. We only focus on choosing proper sentences to attack, which is a prerequisite for effective persuasion.\n\nThis analysis of reasons for attacks, along with argumentation theory and discourse studies, provide insights into what characteristics of sentences are relevant to attackability. Informed by these insights, we extract features that represent relevant sentence characteristics, clustered into four categories: content, external knowledge, proposition arXiv:2010.02660v1 [cs.CL] 6 Oct 2020 types, and tone. We demonstrate the effects of individual features on sentence attackability, in regard to whether a sentence would be attacked and whether a sentence would be attacked successfully.\n\nBuilding on these findings, we examine the efficacy of machine learning models in detecting attackable sentences in arguments. We demonstrate that their decisions match the gold standard significantly better than several baselines and comparably well to laypeople.\n\nTo the best of our knowledge, this work is the first large-scale analysis of sentence attackability in arguments. Our contributions are as follows:\n\n\u2022 We introduce the problem of detecting attackable sentences in arguments and release the processed data from online discussions and the external knowledge source we used. \u2022 We analyze driving reasons for attacks in arguments and the effects of sentence characteristics on a sentence's attackability. \u2022 We demonstrate the performance of machine learning models for detecting attackable sentences, setting a baseline for this challenging task and suggesting future directions.\n\n\nBackground\n\nThe strength of an argument is a long-studied topic, dating back to Aristotle (2007), who suggested three aspects of argument persuasiveness: ethos (the arguer's credibility), logos (logic), and pathos (appeal to the hearer's emotion). More recently, Wachsmuth et al. (2017b) summarized various aspects of argument quality studied in argumentation theory and NLP, such as clarity, relevance, and arrangement. Some research took empirical approaches and collected argument evaluation criteria from human evaluators (Habernal and Gurevych, 2016a;Wachsmuth et al., 2017a). By adopting some of these aspects, computational models have been proposed to automatically evaluate argument quality in various settings, such as essays (Ke et al., 2019), online comments (Gu et al., 2018), and pairwise ranking (Habernal and Gurevych, 2016b). While these taxonomies help understand and evaluate the quality of an argument as a whole, little empirical analysis has been done in terms of what to attack in an argument to persuade the arguer. What can be attacked in an argument has been studied more in argumentation theory. Particularly, Walton et al. (2008) present argumentation schemes and critical questions (CQs). Argument schemes are reasoning types commonly used in daily argumentation. For instance, the scheme of argument from cause to effect has the conclusion \"B will occur\" supported by the premise \"if A occurs, B will occur. In this case, A occurs\". Each scheme is associated with a set of CQs for judging the argument to be good or fallacious. CQs for the above scheme include \"How strong is the causal generalization?\" and \"Are there other factors that interfere with the causal effect?\" Unlike the general argument quality described in the previous paragraph, CQs serve as an evaluation tool that specify local attackable points in an argument. They have been adopted for grading essays (Song et al., 2017) and teaching argumentation skills (Nussbaum et al., 2018). Some of the sentence characteristics in our work are informed by argumentation schemes and CQs.\n\nNLP researchers have widely studied the effectiveness of counterarguments on persuasion (Tan et al., 2016;Cano-Basave and He, 2016;Wei et al., 2016;Wang et al., 2017;Morio et al., 2019) and how to generate counterarguments (Hua et al., 2019;Wachsmuth et al., 2018). Most of the work focuses on the characteristics of counterarguments with respect to topics and styles, without consideration of what points to attack. On the other hand, some studies aimed to model the salience of individual sentences in attacked arguments by paying different degrees of attention to sentences using attention mechanism (Jo et al., 2018;Ji et al., 2018). While their approaches helped to predict the success of persuasion, it was difficult to interpret what constitute the salience or attackability of sentences. To address this limitation, we quantify and analyze the characteristics of sentences that are attacked and lead to the arguer's view change.\n\n\nData\n\nHere we describe how we collected and labeled our data.\n\n\nData Collection\n\nWe use online discussions from the Change-MyView (CMV) subreddit 2 . In this forum, users post their views on various issues and invite other users to challenge their views. If a comment changes the original poster (OP)'s view, the OP acknowledges it by replying to the comment with a \u2206 symbol. The high quality of the discussions in this forum is maintained through several mod-eration rules, such as the minimum length of an original post and the maximum response time of OPs. As a result, CMV discussions have been used in many NLP studies (Chakrabarty et al., 2019;Morio et al., 2019;Jo et al., 2018;Musi, 2017;Wei et al., 2016;Tan et al., 2016).\n\nWe scraped CMV posts and comments written between January 1, 2014 and September 30, 2019, using the Pushshift API. We split them into a dev set (Jan 2014-Jan 2018 for training and Feb 2018-Nov 2018 for validation) and a test set (Dec 2018-Sep 2019), with the ratio of 6:2:2. We split the data by time to measure our models' generality to unseen subjects.\n\nAs the characteristics of arguments vary across different issues, we categorized the posts into domains using LDA. For each post, we chose as its domain the topic that has the highest standard score; topics comprising common words were excluded. We tried different numbers of topics (25,30,35,40) and finalized on 40, as it achieves the lowest perplexity. This process resulted in 30 domains (excluding common-word topics): media, abortion, sex, election, Reddit, human economy, gender, race, family, life, crime, relationship, movie, world, game, tax, law, money, drug, war, religion, job, food, power, school, college, music, gun, and Jewish (from most frequent to least, ranging 5%-2%).\n\n\nLabeling Attackability\n\nSince we are interested in which parts of a post are attacked by comments and whether the attacks lead to successful view changes, our goal here is to label each sentence in a post as successfully attacked, unsuccessfully attacked, or unattacked. We only consider comments directly replying to each post (toplevel comments), as lower-level comments usually address the same points as their parent comments (as will be validated at the end of the section).\n\nAttacked vs. Unattacked: Some comments use direct quotes with the > symbol to address specific sentences of the post (Figure 1). Each quote is matched with the longest sequence of sentences in the post using the Levenshtein edit distance (allowing a distance of 2 characters for typos). A matched text span should contain at least one word and four characters, and cover at least 80% of the quote to exclude cases where the > symbol is used to quote external content. As a result, 98% of the matched spans cover the corresponding quotes entirely. Additionally, a sentence in the post is considered to be quoted if at least four non-stopwords appear in a comment's sentence. For example: Post: ... If you do something, you should be prepared to accept the consequences. ... Comment: ... I guess my point is, even if you do believe that \"If you do something, you should be prepared to accept the consequences,\" you can still feel bad for the victims. ...\n\nWe considered manually annotating attacked sentences too, but it turned out to be extremely timeconsuming and subjective (Appendix A). We tried to automate it using heuristics (word overlap and vector embeddings), but precision severely deteriorated. As we value the precision of labels over recall, we only use the method described in the previous paragraph. Chakrabarty et al. (2019) used the same method to collect attack relations in CMV.\n\nSuccessfully vs. Unsuccessfully Attacked: After each sentence in a post is labeled as attacked or not, each attacked sentence is further labeled as successfully attacked if any of the comments that attack it, or their lower-level comments win a \u2206.\n\nWe post-process the resulting labels to increase their validity. First, as a challenger and the OP have discussion down the comment thread, the challenger might attack different sentences than the originally attacked ones and change the OP's view. In this case, it is ambiguous which sentences contribute to the view change. Hence, we extract quotes from all lower-level comments of \u2206-winning challengers, and if any of the quotes attack new sentences, this challenger's attacks are excluded from the labeling of successfully attacked. This case is not common, however (0.2%).\n\nSecond, if a comment attacks many sentences in the post and change the OP's view, some of them may not contribute to the view change but are still labeled as successfully attacked. To reduce this noise, comments that have more than three quotes are excluded from the labeling of successfully attacked 3 . This amounts to 12% of top-level comments (63% of comments have only one quote, 17% two quotes, and 8% three quotes).\n\nLastly, we verified if quoted sentences are actually attacked. We randomly selected 500 comments and checked if each quoted sentence is purely agreed with without any opposition, challenge, or question. This case was rare (0.4%) 4 , so we do   not further process this case. Table 1 shows some statistics of the final data.\n\n\nQuantifying Sentence Characteristics\n\nAs the first step for analyzing the characteristics of attackable sentences, we examine driving reasons for attacks and quantify relevant characteristics.\n\n\nRationales and Motivation for Attacks\n\nTo analyze rationales for attacks, two authors examined quotes and rebuttals in the training data (one successful and one unsuccessful comment for each post). From 156 attacks, we identified 10 main rationales (Table 2a), which are finer-grained than the refutation reasons in prior work (Wei et al., 2016). The most common rationale is that the sentence is factually correct but is irrelevant to the main claim (19%). Counterexample-related rationales are also common: the sentence misses an example suggestquote the OP's sentences just to agree.\n\ning the opposite judgment to the sentence's own (18%) and the sentence has exceptions (17%). This analysis is based on polished rebuttals, which mostly emphasize logical aspects, and cannot fully capture other factors that motivate attacks. Hence, we conducted a complementary analysis, where an undergraduate student chose three sentences to attack for each of 50 posts and specified the reasons in their own terms (Table 2b). The most common factor is that the sentence is only a personal opinion (28%). Invalid hypotheticals are also a common factor (26%). The tone of a sentence motivates attacks as well, such as generalization (13%), absoluteness (7%), and concession (5%).\n\n\nFeature Extraction\n\nBased on these analyses, we cluster various sentence characteristics into four categories-content, external knowledge, proposition types, and tone. 5\n\n\nContent\n\nContent and logic play the most important role in CMV discussions. We extract the content of each sentence at two levels: TFIDF-weighted n-grams (n = 1, 2, 3) and sentence-level topics. Each sentence is assigned one topic using Sentence LDA (Jo and Oh, 2011). We train a model on posts in the training set and apply it to all posts, exploring the number of topics \u2208 {10, 50, 100}. 6\n\n\nExternal Knowledge\n\nExternal knowledge sources may provide information as to how truthful or convincing a sentence is (e.g., Table 2a-R2, R3, R4, R7 and Table 2b-F4). As our knowledge source, we use kialo.com-a collaborative argument platform over more than 1.4K issues. Each issue has a main statement, and users can respond to any existing statement with pro/con statements (1-2 sentences), building an argumentation tree. Kialo has advantages over structured knowledge bases and Wikipedia in that it includes many debatable statements; many attacked sentences are subjective judgments ( \u00a74.1), so factbased knowledge sources may have limited utility. In addition, each statement in Kialo has pro/con counts, which may reflect the convincingness of the statement. We scraped 1,417 argumentation trees and 130K statements (written until Oct 2019).\n\nFor each sentence in CMV, we retrieve similar statements in Kialo that have at least 5 common words 7 and compute the following three features. Frequency is the number of retrieved statements; sentences that are not suitable for argumentation are unlikely to appear in Kialo. This feature is computed as log 2 (N + 1), where N is the number of retrieved statements. Attractiveness is the average number of responses for the matched statements, reflecting how debatable the sentence is. It is computed as log 2 (M + 1),\nwhere M = 1 N N i=1 R i and R i is the number of responses for the ith retrieved statement. Lastly, extremeness is 1 N N i=1 |P i \u2212 N i |,\nwhere P i and N i are the proportions (between 0 and 1) of pro responses and con responses for the ith retrieved statement. A sentence that most people would see flawed would have a high extremeness value.\n\n\nProposition Types\n\nSentences convey different types of propositions, such as predictions and hypotheticals. No proposition types are fallacious by nature, but some of them may make it harder to generate a sound argument. They also communicate different moods, causing the hearer to react differently. We extract 13 binary features for proposition types. They are all based on lexicons and regular expressions, which are available in Appendix C.\n\nQuestions express the intent of information seeking. Depending on the form, we define three features: confusion (e.g., I don't understand), why/how (e.g., why ...?), and other.\n\nNormative sentences suggest that an action be carried out. Due to their imperative mood, they can sound face-threatening and thus attract attacks.\n\nPrediction sentences predict a future event. They can be attacked with reasons why the prediction is unlikely (Table 2a-R6), as in critical questions for argument from cause to effect (Walton et al., 2008).\n\nHypothetical sentences may make implausible assumptions (Table 2a-R8 and Table 2b-F2) or restrict the applicability of the argument too much (Table 2b-F7).\n\nCitation often strengthens a claim using authority, but the credibility of the source could be attacked (Walton et al., 2008).\n\nComparison may reflect personal preferences that are vulnerable to attacks (Table 2b- Examples in a sentence may be attacked for their invalidity (Walton et al., 2008) or counterexamples (Table 2a-R3).\n\nDefinitions form a ground for arguments, and challengers could undermine an argument by attacking this basis (e.g., Table 2a-R5).\n\nPersonal stories are the arguer's experiences, whose validity is difficult to refute. A sentence with a personal story has subject I and a non-epistemic verb; or it has my modifying non-epistemic nouns.\n\nInclusive sentences that mention you and we engage the hearer into the discourse (Hyland, 2005), making the argument more vulnerable to attacks.\n\n\nTone\n\nChallengers are influenced by the tone of an argument, e.g., subjectiveness, absoluteness, or confidence (Table 2b). We extract 8 features for the tone of sentences.\n\nSubjectivity comprises judgments, which are often attacked due to counterexamples (Table 2a-R2) or their arbitrariness (Table 2b- F1, Walton et al. (2008)). The subjectivity of a sentence is the average subjectivity score of words based on the Subjectivity Lexicon (Wilson et al., 2005) (non-neutral words of \"weaksubj\" = 0.5 and \"strongsubj\" = 1).\n\nConcreteness is the inverse of abstract diction, whose meaning depends on subjective perceptions and experiences. The concreteness of a sentence is the sum of the standardized word scores based on Brysbaert et al. (2014)'s concreteness lexicon.\n\nQualification expresses the level of generality of a claim, where absolute statements can motivate attacks (Table 2b-R3). The qualification score of a sentence is the average word score based on our lexicon of qualifiers and generality words.\n\nHedging can sound unconvincing (Durik et al., 2008) and motivate attacks. A sentence's hedging score is the sum of word scores based on our lexicon of downtoners and boosters.\n\nSentiment represents the valence of a sentence. Polar judgments may attract more attacks than neutral statements. We calculate the sentiment of each sentence with BERT (Devlin et al., 2018) trained on the data of SemEval 2017Task 4 (Rosenthal et al., 2017. Sentiment score is a continuous value ranging between -1 (negative) and +1 (positive), and sentiment categories are nominal (positive, neutral, and negative) 8 . In addition, we compute the scores of arousal (intensity) and dominance (control) as the sum of the standardized word scores based on Warriner et al. (2013)'s lexicon.\n\n\nTask 1: Attackability Characteristics\n\nOne of our goals in this paper is to analyze what characteristics of sentences are associated with a sentence's attackability. Hence, in this section, we measure the effect size and statistical significance of each feature toward two labels: (i) whether a sentence is attacked or not, using the dev set of the \"Attacked\" dataset (N =553,635), (ii) whether a sentence is attacked successfully or unsuccessfully, using all attacked sentences (N =159,417). 9 Since the effects of characteristics may depend on the issue being discussed, the effect of each feature is estimated conditioned on the domain of each post using a logistic regression, and the statistical significance of the effect is assessed using the Wald test. For interpretation purposes, we use odds ratio (OR)-the exponent of the effect size. 10\n\n\nContent\n\nAttacked sentences tend to mention big issues like gender, race, and health as revealed in topics 47, 8, and 6 (Table 3) and n-grams life, weapons, women, society, and men (Table 7 in Appendix E). These issues are also positively correlated with successful attacks. On the other hand, mentioning relatively personal issues (tv, friends, topic 38) seems negatively correlated with successful attacks. So do forum-specific messages (cmv, thank, topic 4).\n\nAttacking seemingly evidenced sentences appears to be effective for persuasion when properly done. Successfully attacked sentences are likely to mention specific data (data, %) and be the OP's specific reasons under bullet points (2. and 3.).\n\nn-grams capture various characteristics that are vulnerable to attacks, such as uncertainty and absoluteness (i believe, never), hypotheticals (if i), questions (?, why), and norms (should). 9 Simply measuring the predictive power of features in a prediction setting provides an incomplete picture of the roles of the characteristics. Some features may not have drastic contribution to prediction due to their infrequency, although they may have significant effects on attackability. 10 Odds are the ratio of the probability of a sentence being (successfully) attacked to the probability of being not (successfully) attacked; OR is the ratio of odds when the value of the characteristic increases by one unit (Appendix D).\n\n\nFeature\n\nAttacked Successful \nQuestion -Confusion \u2020 0.97 ( ) 1.29 ( * ) Question -Why/How \u2020 1.77 (***) 1.27 (***) Question -Other \u2020 1.16 (***) 1.11 ( * ) Citation \u2020 0.53 (***) 1.17 ( * ) Definition \u2020 1.04 ( ) 1.32 ( ** ) Normative \u2020 1.26 (***) 1.10 ( ** ) Prediction \u2020 1.22 (***) 1.02 ( ) Hypothetical \u2020 1.29 (***) 1.07 ( ) Comparison \u2020 1.25 (***) 1.02 ( ) Example \u2020 1.20 (***) 1.17 ( * ) Personal Story \u2020 0.70 (***) 1.09 ( ** ) Use of You \u2020 1.18 (***) 1.04 ( ) Use of We \u2020 1.24 (***) 0.98 ( ) Tone Subjectivity \u2021 1.03 (***) 0.97 (***) Concreteness \u2021 0.87 (***) 0.92 (***) Hedges \u2021 1.04 (***) 1.06 (***) Quantification \u2021 0.97 (***) 1.02 ( ) Sentiment Score \u2021 0.87 (***) 1.00 ( ) Sentiment: Positive \u2020 0.76 (***) 0.99 ( ) Sentiment: Neutral \u2020 0.82 (***) 1.00 ( ) Sentiment: Negative \u2020 1.34 (***) 1.00 ( ) Arousal \u2021 1.02 (***) 0.95 (***) Dominance \u2021\n1.07 (***) 1.08 (***) Table 3: Odds ratio (OR) and statistical significance of features. An effect is positive (blue) if OR > 1 and negative (red) if OR < 1. ( \u2020 : binary, \u2021 : standardized / *: p < 0.05, **: p < 0.01, ***: p < 0.001)\n\n\nExternal Knowledge\n\nThe Kialo-based knowledge features provide significant information about whether a sentence would be attacked successfully (Table 3). As the frequency of matched statements in Kialo increases twice, the odds for successful attack increase by 7%. As an example, the following attacked sentence has 18 matched statements in Kialo.\n\nI feel like it is a parents right and responsibility to make important decisions for their child.\n\nThe attractiveness feature has a stronger effect; as matched statements have twice more responses, the odds for successful attack increase by 18%, probably due to higher debatability.\n\nA sentence being completely extreme (i.e., the matched sentences have only pro or con responses) increases the odds for successful attack by 19%.\n\nAs expected, the argumentative nature of Kialo allows its statements to match many subjective sentences in CMV and serves as an effective information source for a sentence's attackability.\n\n\nProposition Types\n\nQuestions, especially why/how, are effective targets for successful attack (Table 3). Although challengers do not pay special attention to expressions of confusion (see column \"Attacked\"), they are positively correlated with successful attack (OR=1.29).\n\nCitations are often used to back up an argument and have a low chance of being attacked, reducing the odds by half. However, properly attacking citations significantly increases the odds for successful attack by 17%. Similarly, personal stories have a low chance of being attacked and definitions do not attract challengers' attacks, but attacking them is found to be effective for successful persuasion.\n\nAll other features for proposition types have significantly positive effects on being attacked (OR=1.18-1.29), but only normative and example sentences are correlated with successful attack.\n\n\nTone\n\nSuccessfully attacked sentences tend to have lower subjectivity and arousal (Table 3), in line with the previous observation that they are more data-and reference-based than unsuccessfully attacked sentences. In contrast, sentences about concrete concepts are found to be less attackable.\n\nUncertainty (high hedging) and absoluteness (low qualification) both increase the chance of attacks, which aligns with the motivating factors for attacks (Table 2b), while only hedges are positively correlated with successful attacks, implying the importance of addressing the arguer's uncertainty.\n\nNegative sentences with high arousal and dominance have a high chance of being attacked, but most of these characteristics have either no or negative effects on successful attacks.\n\n\nDiscussion\n\nWe have found some evidence that, somewhat counter-intuitively, seemingly evidenced sentences are more effective to attack. Such sentences use specific data (data, %), citations, and definitions. Although attacking these sentences may require even stronger evidence and deeper knowledge, arguers seem to change their viewpoints when a fact they believe with evidence is undermined. In addition, it seems very important and effective to identify and address what the arguer is confused (confusion) or uncertain (hedges) about.\n\nOur analysis also reveals some discrepancies between the characteristics of sentences that challengers commonly think are attackable and those that are indeed attackable. Challengers are often attracted to subjective and negative sentences with high arousal, but successfully attacked sentences have rather lower subjectivity and arousal, and have no difference in negativity compared to unsuccessfully attacked sentences. Furthermore, challengers pay less attention to personal stories, while successful attacks address personal stories more often.\n\n\nTask 2: Attackability Prediction\n\nNow we examine how well computational models can detect attackable sentences in arguments.\n\n\nProblem Formulation\n\nThis task is cast as ranking sentences in each post by their attackability scores predicted by a regression model. We consider two types of attackability: (i) whether a sentence will be attacked or not, (ii) whether a sentence will be successfully attacked or not (attacked unsuccessfully + unattacked). For both settings, we consider posts that have at least one sentence with the positive label (Table 1).\n\nWe use three evaluation metrics. P@1 is the precision of the first ranked sentence, measuring the model's accuracy when choosing one sentence to attack for each post. Less strictly, A@3 gives a score of 1 if any of the top 3 sentences is a positive instance and 0 otherwise. AUC measures individual sentence-level accuracy-how likely positive sentences are assigned higher probabilities.\n\n\nComparison Models\n\nFor machine learning models, we explore two logistic regression models to compute the probability of the positive label for each sentence, which becomes the sentence's attackability score. LR is a basic logistic regression with our features 11 (Section 4) and binary variables for domains. We explored feature selection using L1-norm and regularization using L2-norm. 12 BERT is logistic regression where our features are replaced with the BERT embedding of the input sentence (Devlin et al., 2018). Contextualized BERT embeddings have achieved 11 We tried the number of topics \u2208 {10, 50, 100}, and 50 has the best AUC on the val set for both prediction settings. 12 We also tried a multilayer perceptron to model feature interactions, but it consistently performed worse than LR.  state-of-the-art performance in many NLP tasks. We use the pretrained, uncased base model from Hugging Face (Wolf et al., 2019) and fine-tune it during training. 13 We explore two baseline models. Random is to rank sentences randomly. Length is to rank sentences from longest to shortest, with the intuition that longer sentences may contain more information and thus more content to attack as well.\n\nLastly, we estimate laypeople's performance on this task. Three undergraduate students each read 100 posts and rank three sentences to attack for each post. Posts that have at least one positive instance are randomly selected from the test set. 14\n\n\nResults\n\nAll computational models were run 10 times, and their average accuracy is reported in Table 4. Both the LR and BERT models significantly outperform the baselines, while the BERT model performs best. For predicting attacked sentences, the BERT model's top 1 decisions match the gold standard 50% of the time; its decisions match 78% of the time when three sentences are chosen. Predicting successfully attacked sentences is harder, but the performance gap between our models and the baselines gets larger. The BERT model's top 1 decisions match the gold standard 28% of the time-a 27% and 10% boost from random and length-based performance, respectively.\n\n13 Details for reproducibility are in Appendix F. 14 We were interested in the performance of young adults who are academically active and have a moderate level of life experience. Their performance may not represent the general population, though.\n\nTo examine the contribution of each feature category, we did ablation tests based on the best performing LR model (Table 4 rows 4-7). The two prediction settings show similar tendencies. Regarding P@1 for successful attack, content has the highest contribution, followed by knowledge, proposition types, and tone. This result reaffirms the importance of content for a sentence's attackability. But the other features still have significant contribution, yielding higher P@1 and AUC (Table  4 row 4) than the baselines.\n\nIt is worth noting that our features, despite the lower accuracy than the BERT model, are clearly informative of attackability prediction as Table 4 row 3 shows. Moreover, since they directly operationalize the sentence characteristics we compiled, it is pretty transparent that they capture relevant information that contributes to sentence attackability and help us better understand what characteristics have positive and negative signals for sentence attackability. We speculate that transformer models like BERT are capable of encoding these characteristics more sophisticatedly and may include some additional information, e.g., lexical patterns, leading to higher accuracy. But at the same time, it is less clear exactly what they capture and whether they capture relevant information or irrelevant statistics, as is often the case in computational argumentation (Niven and Kao, 2019). Figure 2 illustrates how LR allows us to interpret the contribution of different features to attackability, by visualizing a post with important features highlighted. For instance, external knowledge plays a crucial role in this post; all successfully attacked sentences match substantially more Kialo statements than other sentences. The attackability scores of these sentences are also increased by the use of hypotheticals and certain n-grams like could. These features align well with the actual attacks by successful challengers. For instance, they pointed out that the expulsion of Russian diplomats (sentence 2) is not an aggressive reaction because the diplomats can be simply replaced with new ones. Kialo has a discussion on the relationship between the U.S. and Russia, and one statement puts forward exactly the same point that the expulsion was a forceful-looking but indeed a nice gesture. Similarly, a successful challenger pointed out the consistent attitude of the U.S. toward regime change in North Korea (sentence 3), and the North Korean regime is a controversial topic in Kialo. Lastly,\n\n\n86dg7)\n\nostly from anxiety considering lly this post will spark optimistic ee often in the news or online or nt of John Bolton as the National hn Pompeo as the Secretary of hawkish and pro-war behavior in s and actions, the US has aggressive stance in foreign policy, f sixty Russian diplomats following United Kingdom. Also, despite h Kim Jong-Un concerning the he US, and NK's nuclear arsenal, d out his cabinet/diplomacy team favor of things such as a regime Korea, further stirring things up If talks between the two nations es not have much more of a attacking North Korea, which is a favorable among higher officials. is also sort of a proxy scuffle /Russia, attacking or otherwise Russia could lead to situations de economic downturn to nuclear flict the current trajectory of How would we otherwise not scuffle?\n\nI'm typing this post mostly from anxiety considering recent events, but hopefully this post will spark optimistic discussion that I don't see often in the news or online or such. With the appointment of John Bolton as the National Security Adviser and John Pompeo as the Secretary of State, two men known for hawkish and pro-war behavior in their previous statements and actions, the US has appeared to take a more aggressive stance in foreign policy, seen with the expulsion of sixty Russian diplomats following minor controversy in the United Kingdom. Also, despite planned negotiations with Kim Jong-Un concerning the future of North Korea, the US, and NK's nuclear arsenal, President Trump has filled out his cabinet/diplomacy team with people who are in favor of things such as a regime change or attacking North Korea, further stirring things up for a potential falling out. If talks between the two nations break down, the US does not have much more of a reason to withhold from attacking North Korea, which is a plan that seems to be favorable among higher officials. Considering that this is also sort of a proxy scuffle between us and China/ Russia, attacking or otherwise provoking North Korea or Russia could lead to situations ranging from a worldwide economic downturn to nuclear holocaust. Is conflict the current trajectory of international relations? How would we otherwise not engage in some sort of scuffle? The last Presidential election (2016) and most succeeding elections have proven that elections are more about party affiliations than actual views or the character of the individual being elected. In one of the most extreme examples, Roy Moore was backed by the Republican Party even though he was accused of sexual misconduct and sexual assault of minors simply because he was a Republican. This also allows voters to be lazy, as many will simply vote for their party without researching the values and character of the person they are voting for. Our Congress is slow an inefficient because Democrats and Republicans are more focused on opposing one another than they are on developing actual solutions to issues like gun control and abortion. It is the job of elected officials to represent ALL of the people of their district/state/country, not just the people that voted for them or agree with them, and following the ideals of a political party does not allow for this. Political parties force us to think in terms of black and white, and this is both inefficient and inappropriate for issues that affect the entire country. Also, many young voters do not think this way--many Americans are becoming disenfranchised with the entire political system. This is an outdated system, and either needs to adapt or change completely to better fit the needs of the people. one successful challenger attacked the hypothetical outcomes in sentences 4 and 5, pointing out that those outcomes are not plausible, and the LR model also captures the use of hypothetical and the word could as highly indicative of attackability. More successful and erroneous cases are in Appendix H.\n\nLaypeople perform significantly better than the BERT model for predicting attacked sentences, but only comparably well for successfully attacked sentences (Table 4 row 9). Persuasive argumentation in CMV requires substantial domain knowledge, but laypeople do not have such expertise for many domains. The BERT model, however, seems to take advantage of the large data and encodes useful linguistic patterns that are predictive of attackability. A similar tendency has been observed in predicting persuasive refutation (Guo et al., 2020), where a machine-learned model outperformed laypeople. Nevertheless, in our task, the humans and the BERT model seem to make similar decisions; the association between their choices of sentences is high, with odds ratios ranging between 3.43 (top 1) and 3.33 (top 3). Interestingly, the LR model has a low association with the human decisions for top 1 (OR=2.65), but the association exceeds the BERT model for top 3 (OR=3.69). It would be interesting to further examine the similarities and differences in how humans and machines choose sentences to attack.\n\n\nConclusion\n\nWe studied how to detect attackable sentences in arguments for successful persuasion. Using online arguments, we demonstrated that a sentence's attackability is associated with many of its characteristics regarding its content, proposition types, and tone, and that Kialo provides useful information about attackability. Based on these findings we demonstrated that machine learning models can automatically detect attackable sentences, comparably well to laypeople.\n\nOur work contributes a new application to the growing literature on causal inference from text (Egami et al., 2018), in the setting of \"text as a treatment\". Specifically, our findings in Section 5 pave the way towards answering the causal question: would attacking a certain type of sentence (e.g., questions or expressions of confusion) in an argument increase the probability of persuading the opinion holder? While our findings suggest initial hypotheses about the characteristics of sentences that can be successfully attacked, establishing causality in a credible manner would require addressing confounders, such as the challenger's reputation (Manzoor et al., 2020) and persuasive skill reflected in their attack (Tan et al., 2014). We leave this analysis to future work.\n\nOur work could be improved also by including discourse properties (coherence, cohesiveness). Further, argumentation structure (support relations between sentences or lack thereof) might provide useful information about each sentence's attackability. We tried capturing sentences in posts that are addressed by comments but not directly quoted. To see its feasibility, we randomly sampled 100 post-comment pairs that do not contain direct quotes and then asked an undergraduate native speaker of English (who has no knowledge about this work) to mark attacked sentences in each post, if any. This revealed two challenges. First, human annotation is subjective when compared to a co-author's result and very time-consuming (2.5 min/comment). Second, we tried several methods to automatically identify attacked sentences. We compared the similarity between each post sentence with the comment (first sentence of the comment, first sentence of each paragraph, or all comment text) based on word overlap with/without synonym expansion and the GloVe embeddings. But it turned out to be difficult to get similar results to human annotations. Therefore, we decided to use only those sentences that are direct quoted or have at least 4 common words with a comment's sentence as the most reliable labels.\n\n\nB External Knowledge\n\nIn this section, we describe the methods that we explored to use Kialo as a knowledge base but that were not successful.\n\n\nB.1 UKP Sentence Embedding-Based Retrieval\n\nWe measured the similarity between CMV sentences and Kialo statements using the UKP sentence embedding-BERT embeddings fine-tuned to measure argument similarity (Reimers et al., 2019). Specifically, the authors provide pretrained embeddings constructed by appending a final softmax layer to BERT to predict a numerical dissimilarity score between 0 and 1 for each sentence pair in the UKP ASPECT corpus. The 3,595 sentence pairs in this corpus were drawn from 28 controversial topics and annotated via crowd workers to be \"unrelated\" or of \"no\", \"some\" or \"high\" similarity. They report a mean F1-score of 65.39% on a held-out subset of this corpus, which was closest to human performance (F1=78.34%) among all competing methods that were not provided with additional information about the argument topic.\n\nWe used this fine-tuned model to measure the dissimilarity between each CMV sentence and Kialo statements. Based on this information, we extracted the feature UKP Avg Distance 10, which is the average dissimilarity score of the 10 Kialo statements that are closest to the sentence. This score is expected to be low if a sentence has many similar statements in Kialo. In addition, we extracted the same frequency, attractiveness, and extremeness features as in \u00a74.2.2. Here, we determine whether a CMV sentence and a Kialo statement are \"matched\" based on several dissimilarity thresholds (0.1, 0.2, 0.3, 0.4); A Kialo statement is considered matched with a CMV sentence if the dissimilarity is below the selected threshold.\n\n\nB.2 Semantic Frame-Based Knowledge\n\nWe extracted semantic frames from CMV sentences and Kialo statements, using Google SLING (Ringgaard et al., 2017). For each frame in a sentence or statement, a \"knowledge piece\" is defined as the concatenation of the predicate and arguments (except negation); the predicate is lemmatized and the arguments are stemmed to remove differences in verb/noun forms. We also mark each knowledge piece as negated if the frame contains negation. Example knowledge pieces include:\n\n\u2022 ARG0:peopl-ARG1:right-ARGM-MOD:should-PRED:have (Negation: true) \u2022 ARG1:person-ARG2:abl-ARGM-MOD:should-PRED:be (Negation: false) For each CMV sentence, we extracted two features: the count of knowledge pieces in Kialo that are consistent with those in the sentence, and the count of knowledge pieces in Kialo that are conflicting with those in the sentence. Two knowledge pieces are considered consistent if they are identical, and conflicting if they are identical but negated. Attackable sentences are expected to have many consistent and conflicting knowledge pieces in Kialo. If we assume that most statements in Kialo are truthful, attackable sentences may have more conflicting knowledge pieces than consistent knowledge pieces.\n\n\nB.3 Word Sequence-Based Knowledge\n\nTreating each frame as a separate knowledge piece does not capture the dependencies between multiple predicates within a sentence. Hence, we tried a simple method to capture this information, where a knowledge pieces is defined as the concatenation of verbs, nouns, adjectives, modal, prepositions, subordinating conjunctions, numbers, and existential there within a sentence; but independent clauses (e.g., a because clause) were separated off. All words were lemmatized. Each knowledge piece is negated if the source text has negation words. Example knowledge pieces include:\n\n\u2022 gender-be-social-construct (Negation: true) \u2022 congress-shall-make-law-respect-establishment-of-religion-prohibit-free-exercise (Negation: false) For each CMV sentence, we extracted the same two features as in semantic frame-based knowledge pieces: the count of knowledge pieces in Kialo that are consistent with those in the sentence, and the count of knowledge pieces in Kialo that are conflicting with those in the sentence.\n\n\nB.4 Effects and Statistical Significance\n\nThe effects and statistical significance of the above features were estimated in the same way as \u00a75 and are shown in Table 5. Word sequence-based knowledge has no effect, probably because not many knowledge pieces are matched. Most of the other features have significant effects only for \"Attacked\". We speculate that a difficulty comes from the fact that both vector embedding-based matching and frame-based matching are inaccurate in many cases. UKP sentence embeddings often retrieve Kialo statements that are only topically related to a CMV sentence. Similarly, frame-based knowledge pieces often cannot capture complex information conveyed in a CMV sentence. In contrast, word overlap-based matching seems to be more reliable and better retrieve Kialo statements that have similar content to a CMV sentence.  Table 5: Odds ratio (OR) and statistical significance of features. An effect is positive (blue) if OR > 1 and negative (red) if OR < 1. ( \u2020 : log2, \u2021 : standardized / *: p < 0.05, **: p < 0.01, ***: p < 0.001) Table 6 shows the lexicons and regular expressions used in feature extraction. r\"pattern\" represents a regular expression.\n\n\nC Lexicons\n\n\nFeature Pattern\n\nQuestion -Confusion r\"(\u02c6| )i (\\S + ){,2}(not|n't|never) (understand|know)\", r\"(not|n't) make sense\", r\"(\u02c6| )i (\\S + ){,2}(curious|confused)\", r\"(\u02c6| )i (\\S + ){,2}wonder\", r\"(me|myself) wonder\" Question -Why/How r\"(\u02c6| )(why|how). * \\?\" Question -Other ? Normative should, must, \"(have|has) to\", \"have got to\", \"'ve got to\", gotta, need, needs Prediction r\"(am$|$'m$|$are$|$'re$|$is$|$'s) (not )?(going to$|$gonna)\", will, won't, would, shall Hypothetical r\"(\u02c6|, )if|unless\" Citation r\" {PATTERN} that [\u02c6.,!?]\" (PATTERN: said, reported, mentioned, declared, claimed, admitted, explained, insisted, promised, suggested, recommended, denied, blamed, apologized, agreed, answered, argued, complained, confirmed, proposed, replied, stated, told, warned, revealed), according to, r\"https?:\" Comparison than, compared to Examples r\"(\u02c6| )(for example|for instance|such as|e\\.g\\.)( |$)\" Definition define, definition Personal Story\n\nEpistemic verbs: think, believe, see, know, feel, say, understand, mean, sure, agree, argue, consider, guess, realize, hope, support, aware, disagree, post, mention, admit, accept, assume, convince, wish, appreciate, speak, suppose, doubt, explain, wonder, discuss, view, suggest, recognize, respond, acknowledge, clarify, state, sorry, advocate, propose, define, apologize, curious, figure, claim, concede, debate, list, oppose, describe, suspect, reply, bet, realise, defend, convinced, offend, concern, intend, certain, conclude, reject, challenge, thank, condone, value, skeptical, contend, anticipate, maintain, justify, recommend, confident, promise, guarantee, comment, unsure, elaborate, posit, swear, dispute, imply, misunderstand. Epistemic nouns: view, opinion, mind, point, argument, belief, post, head, position, reasoning, understanding, thought, reason, question, knowledge, perspective, idea, way, stance, vote, best, cmv, response, definition, viewpoint, example, claim, logic, conclusion, thinking, comment, statement, theory, bias, assumption, answer, perception, intention, contention, word, proposal, thesis, interpretation, reply, guess, evidence, explanation, hypothesis, assertion, objection, criticism, worldview, impression, apology, philosophy Use of You you, your, yours Use of We r\"(\u02c6| )we |(?<!the) (us|our|ours)( |$)\"  \n\n\nD Statistical Model for Feature Effects\n\nFor each feature, we use the following logistic regression model:\nlog P(Y = 1) 1 \u2212 P(Y = 1) =\u03b2 0 + \u03b2 X X + \u03b1 1 D 1 + \u00b7 \u00b7 \u00b7 + \u03b1 |D| D |D| ,\nwhere X is a continuous or binary explanatory variable that takes the value of a characteristic that we are interested in. D d (d = 1, \u00b7 \u00b7 \u00b7 , |D|) is a binary variable that takes 1 if the sentence belongs to the d-th domain. Y is a binary response variable that takes 1 if the sentence is attacked or if the sentence is attacked successfully. \u03b2 X is the regression coefficient of the characteristic X, which is the main value of our interest for examining the association between the characteristic and the response; exp (\u03b2 X ) is the odds ratio (OR) that is interpreted as the change of odds (i.e., the ratio of the probability that a sentence is (successfully) attacked to the probability that a sentence is not (successfully) attacked) when the value of the characteristic increases by one unit. If \u03b2 X is significant, we can infer that X has an effect on Y. If \u03b2 X is positive (and significant), we can infer that the characteristic and the response have positive association, and vice versa.\n\nE Important n-gram Features Table 7 shows the top 100 n-grams that have the highest or lowest weights for attacked sentences (vs. unattacked sentences) and for successfully attacked sentences (vs. unsuccessfully attacked).\n\n\nAttacked (vs. Unattacked) Attacked Successfully (vs. Unsuccessfully)\n\nHigh is are no -? life why women should to society men a nothing 1 ) would money if i they n't people if * someone 2 . human never believe 2 ) 3 . your i believe and 5 . americans tax 4 , being : -: * feel because * the than could republicans do be government ) sex 3 ) nobody why should the government \" i seems religion their ca ca n't less 4 . pay world war an ) the 6 . without , why science 4 ) reason humans animals racism military selfish racist of when social 3 gun makes you speech climate get kids have can white should i , is * ** proven how can is without are ? would public life women weapons data how can usa no should if sex of . , would n't why money % someone the us customers coffee since 1 : skills are a end 3 . available , they technology 2 . -, if people with cost need a car the pretty much racist so many to know third such as white dog could be towards the americans song actions seems formal , he gender is nothing this : power see teams job years videos rates why would cream expectations ca god people feet global i believe sounds n't the 100 think that it crime to pay firstly because , why immoral and not can also scooby \" i issues % of ca n't marriages ability in many Low edit cmv i / ? / thanks ( edit : [ ! post ] ] ( this thank thank you comments please view -&gt; discussion here topic sorry changed my view some cmv . posts . \" my delta comment i will points responses : 1 . of you / ) article title i 'll 'll = thanks for now 'm &amp; got i 'm was ** edit above recently reddit view . lot i was below change my hi 's a few edit 2 on this again \" ) . my view . this post discuss arguments you all deltas few there are 1 . i 've / ) -i have currently edit 2 :\n\ncomments . let me a lot hello let i still here . background course ) -context you guys appreciate thread perspective and i posted edit cmv i thanks / edit : view this thank ! 1 . definitely ] post discussion thank you some 's a changed that this here i have tv points today responses above , it 's ] ( perspective both thought i was to any do this ( there are &gt; continue to currently : i delta comments certainly taxes my you can discuss matters person a please let me got , that not all 'm i 'm more of n't want to obvious posts friends has been honest true . background great hypocritical case . work , account not the results article bit all the that would be grow whose thread fine . point . do you remember still hope now standard thanks for asking try to go started wealth = bitcoin series arguments super does n't Table 7: n-grams (n = 1, 2, 3) with the highest/lowest weights. Different n-grams are split by a space, and words within an n-gram are split by \" \".    \n\n\nF Reproducibility Checklist\n\n\nG Prediction Results\n\n\nH Visualization Examples\n\nFor the successful example in Figure 3a, the model finds evidence for the successfully attacked sentences 3 and 5 from the external knowledge source (Kialo). Although some of the other sentences (7-8) also match Kialo statements, the degree of match is relatively low, and the model determines that their n-grams reduce attackability (many, think, needs). Sentence 4 is properly found to have high attackability, since it makes a comparison and contains many n-grams predictive of attackability (because, Democrats, Republicans, opposing).\n\nFor the successful example in Figure 3b, topics play important roles for determining attackability. The topics of the successfully attacked sentences 2-4 all increase attackability, whereas the topics of other sentences 5-9 reduce attackability.\n\nFor the erroneous example in Figure 4a, all sentences have relatively little evidence for attackability/unattackability. The model determines sentence 5 to have relatively high attackability because of many n-grams that increase attackability (know, absolutely, nothing). On the other hand, the successfully attacked sentence 6 is assigned a low attackability score despite its match with Kialo statements, because its use of we, personal stories, and certain n-grams (many, times, and friends).\n\nFor the erroneous example in Figure 4b, the model finds sentence 4 to have high attackability because it matches with Kialo statements, makes a comparison and prediction, and certain n-grams (believe, presence, society, market). Sentence 5 is also assigned a relatively high attackability score due to its use of examples and certain n-grams (know, committed, weapons). However, these sentences were not successfully attacked. In contrast, the successfully attacked sentences 2-4 do not have strong enough evidence for attackability compared to their negatively signals, such as personal stories and n-grams own and I. regime change or attacking North Korea, further stirring things up for a potential falling out. If talks between the two nations break down, the US does not have much more of a reason to withhold from attacking North Korea, which is a plan that seems to be favorable among higher officials. Considering that this is also sort of a proxy scuffle between us and China/ Russia, attacking or otherwise provoking North Korea or Russia could lead to situations ranging from a worldwide economic downturn to nuclear holocaust. Is conflict the current trajectory of international relations? How would we otherwise not engage in some sort of scuffle? The last Presidential election (2016) and most succeeding elections have proven that elections are more about party affiliations than actual views or the character of the individual being elected. In one of the most extreme examples, Roy Moore was backed by the Republican Party even though he was accused of sexual misconduct and sexual assault of minors simply because he was a Republican. This also allows voters to be lazy, as many will simply vote for their party without researching the values and character of the person they are voting for. Our Congress is slow an inefficient because Democrats and Republicans are more focused on opposing one another than they are on developing actual solutions to issues like gun control and abortion. It is the job of elected officials to represent ALL of the people of their district/state/country, not just the people that voted for them or agree with them, and following the ideals of a political party does not allow for this. Political parties force us to think in terms of black and white, and this is both inefficient and inappropriate for issues that affect the entire country. Also, many young voters do not think this way--many Americans are becoming disenfranchised with the entire political system. This is an outdated system, and either needs to adapt or change completely to better fit the needs of the people.  I realize I have a bias because I grew up in a big city in Canada and not a single person I knew owned a gun and most law enforcement officers I saw on the street also didn't carry guns and I perceive Canada to generally be safer than the open carry US state that I now live in. I see zero reason to own a gun, not even for hunting. I think hunters should use bows and arrows. I admit I've never been hunting myself. I believe the presence of guns in society makes society less safe and we would all be safer if there were fewer of them and they were far more difficult and expensive to buy on the black market rather than being able to pick one up easily at a gun show parking lot using cash and with no background check. I know that violence can be committed with other weapons such as knives or running someone over with a car. But we have laws about who can drive a car and it's actually more difficult to kill people with such things and less efficient. I believe that socialism is an obvious and humanitarian next step for the U.S. It should be the responsibility of vastly successful people to provide a tiny fraction of their income to provide services for people who were not given the same opportunities. Everyone has the right to safety, universal health care, social security, education (affordable collage), a livable minimum wage ($15 per hour), and not to get screwed over by businesses more interested in capital then people. Businesses don't give their fair share back to the community they leach off of (wages or taxes), and it should be the responsibility of the government to make sure they do. When many people speak about socialism they quote nations like the U.S.S.R. (Soviet Union). I believe that the problems with these nations are a weak constitution that stems from a violent revolution instead of a political one. Socialism is an economic policy and can be used in cooperation with the current governing body. I believe that many European country's sudo-socialist ideas (like universal healthcare) are a perfect example of how socialism can be beneficial to people. B  President Trump has filled out his cabinet/diplomacy team with people who are in favor of things such as a regime change or attacking North Korea, further stirring things up for a potential falling out. If talks between the two nations break down, the US does not have much more of a reason to withhold from attacking North Korea, which is a plan that seems to be favorable among higher officials. Considering that this is also sort of a proxy scuffle between us and China/Russia, attacking or otherwise provoking North Korea or Russia could lead to situations ranging from a worldwide economic downturn to nuclear holocaust. Is conflict the current trajectory of international relations? How would we otherwise not engage in some sort of scuffle? developing actual solution abortion. It is the job the people of their district that voted for them or agre of a political party does force us to think in terms both inefficient and inap entire country. Also, ma way--many Americans a entire political system. T needs to adapt or change c of the people. Amanda and Bailey. I 'm compatible with both of them on a platonic level, but I only take a romantic interest in Bailey because she's (physically) my type. Not to say that Amanda is ugly, just that I'm not really into her body structure. Another piece of evidence to support this is when you feel attracted to a complete stranger, because of their physical appearance. You know absolutely nothing about them yet, you could envision a happy relationship with them just from their looks.\n\n\nSo let's say I'm good friends with\n\nI feel this way because many times when I'm hanging out with my friends (of both genders) I think to myself \"wow we'd make such a good couple\" but even so don't feel the desire to enter a relationship with them. elections have proven that elections are more about party affiliations than actual views or the character of the individual being elected. In one of the most extreme examples, Roy Moore was backed by the Republican Party even though he was accused of sexual misconduct and sexual assault of minors simply because he was a Republican. This also allows voters to be lazy, as many will simply vote for their party without researching the values and character of the person they are voting for. Our Congress is slow an inefficient because Democrats and Republicans are more focused on opposing one another than they are on developing actual solutions to issues like gun control and abortion. It is the job of elected officials to represent ALL of the people of their district/state/country, not just the people that voted for them or agree with them, and following the ideals of a political party does not allow for this. Political parties force us to think in terms of black and white, and this is both inefficient and inappropriate for issues that affect the entire country. Also, many young voters do not think this way--many Americans are becoming disenfranchised with the entire political system. This is an outdated system, and either needs to adapt or change completely to better fit the needs of the people.  \n\n\nF1).7  Similarity measures based on word embeddings and knowledge representation did not help (Appendix B).\n\n\n0.39) Example (0.11) KialoAttr (0.07) KialoExtr (-0.07) Topic5 (0.39) KialoFreq (0.22) KialoAttr (0.13) Hypothetical (-0.06) KialoExtr (-0.11) KialoFreq (0.45) Topic5 (0.39) KialoAttr (0.26) KialoExtr (-0.05) Use of \"We\" (-0.18) Topic5 (0.39) QuestOther (0.39) Why/How (0.91) Use of \"We\" (-0.18) Topic37 (-0.21)\n\n\nTopic15 (-0.18) Personal (-0.20) Topic15 (-0.18) Personal (-0.20) Topic15 (-0.18) Personal (-0.20) Use of \"You\" (-0.15) Topic15 (-0.18) Use of \"You\" (-0.15) Topic15 (-0.18) KialoFreq (0.23) Topic15 (-0.18) Use of \"We\" (-0.19) Personal (-0.20)\n\nFigure 3 :\n3Successful examples.\n\nFigure 4 :\n4Erroneous examples.\n\nTable 1 :\n1Data statistics. \"Attacked\" contains posts with at least one attacked sentence. \"Successful\" contains posts with at least one successfully attacked sentence.R1 S is true but does not support the main claim (19%) \nR2 S misses cases suggesting opposite judgment (18%) \nR3 S has exceptions (17%) \nR4 S is false (12%) \nR5 S misses nuanced distinctions of a concept (8%) \nR6 S is unlikely to happen (6%) \nR7 S has no evidence (6%) \nR8 S uses an invalid assumption or hypothetical (4%) \nR9 S contradicts statements in the argument (4%) \nR10 Other (4%) \n\n(a) Rationales for attacking a sentence (S). \n\nF1 Personal opinion (28%) \nF2 Invalid hypothetical (26%) \nF3 Invalid generalization (13%) \nF4 No evidence (11%) \nF5 Absolute statement (7%) \nF6 Concession (5%) \nF7 Restrictive qualifier (5%) \nF8 Other (5%) \n\n(b) Motivating factors for attacks. \n\n\n\nTable 2 :\n2Rationales and motivating factors for attacks.\n\nTable 4 :\n4Prediction accuracy. All LR/BERT scores \n(rows 3-8) have standard deviations between 0.1 and \n1.1, significantly outperforming \"Length\".  \u2020 The aver-\nage bootstrap accuracy after resampling 100K times \nwith sample size 200-the standard deviations of P@1 \nand A@3 range between 2.1 and 3.5. \n\n\n\n\nI believe that socialism next step for the U.S. It vastly successful people income to provide services the same opportunities. Ev universal health care, (affordable collage), a l hour), and not to get s interested in capital then their fair share back to th (wages or taxes), and it sh government to make sure speak about socialism they (Soviet Union). I believ nations are a weak constit revolution instead of a economic policy and can current governing body. I country's sudo-socialist i are a perfect example of h people. B.T.W. this is m reasonable debates with you I realize I have a bias because I grew up in a big city in Canada and not a single person I knew owned a gun and most law enforcement officers I saw on the street also didn't carry guns and I perceive Canada to generally be safer than the open carry US state that I now live in. I see zero reason to own a gun, not even for hunting. I think hunters should use bows and arrows. I admit I've never been hunting myself. I believe the presence of guns in society makes society less safe and we would all be safer if there were fewer of them and they were far more difficult and expensive to buy on the black market rather than being able to pick one up easily at a gun show parking lot using cash and with no background check. I know that violence can be committed with other weapons such as knives or running someone over with a car. But we have laws about who can drive a car and it's actually more difficult to kill people with such things and less efficient.Success 2 (t3_8g5ukh) \nSuccess 3 (t3_ \n\niends with Amanda and Bailey. \nof them on a platonic level, but I \nnterest in Bailey because she's \nt to say that Amanda is ugly, just \nher body structure. Another \nupport this is when you feel \nete stranger, because of their \nYou know absolutely nothing \nuld envision a happy relationship \nr looks. I feel this way because \nanging out with my friends (of \nmyself \"wow we'd make such a \ndon't feel the desire to enter a \n\nTopic15 (-0.18) \nPersonal (-0.20) \n\n3_87as7t) \n\nTopic15 (-0.18) \nPersonal (-0.20) \n\nTopic15 (-0.18) \nPersonal (-0.20) \n\nUse of \"You\" (-0.15) \nTopic15 (-0.18) \n\nUse of \"You\" (-0.15) \nTopic15 (-0.18) \n\nKialoFreq (0.23) \nTopic15 (-0.18) \nUse of \"We\" (-0.19) \nPersonal (-0.20) \n\nKialoFreq (0.78) \nComparison (0.20) \nTopic43 (0.19) \nKialoAttr (0.13) \nUse of \"We\" (-0.19) \nPersonal (-0.20) \n\nErroneous 2 (t3_95wq12) \n\nTopic43 (0.19) \n\nTopic43 (0.19) \nNormative (0.18) \n\nTopic43 (0.19) \nPersonal (-0.20) \n\nKialoFreq (0.75) \nComparison (0.20) \nTopic43 (0.19) \nPrediction (0.12) \nKialoAttr (0.06) \nKialoExtr (-0.12) \nUse of \"We\" (-0.19) \nTopic43 (0.19) \nExample (0.11) \n\nKialoFreq (0.36) \nTopic32 (0.11) \nUse of \"We\" (-0.19) \n\nFigure 2: Prediction visualization. Background color \nindicates predicted attackability (blue: high, red: low). \nSuccessfully attacked sentences are underlined. Fea-\ntures with high/low weights are indicated with blue/red. \n\n\n\n\nNaoki Egami, Christian J Fong, Justin Grimmer, Margaret E Roberts, and Brandon M Stewart. 2018. How to make causal inferences using texts. arXiv preprint arXiv:1802.02163. Yi Song, Paul Deane, and Beata Beigman Klebanov. 2017. Toward the Automated Scoring of Written Arguments: Developing an Innovative Approach for Annotation. ETS Research Report . . . , 152(3):157. https://github.com/words/hedges/ blob/master/data.txt. https://www.janefriedman.com/ hedge-word-inflation-words-prune/.Amparo Elizabeth Cano-Basave and Yulan He. 2016. \nA Study of the Impact of Persuasive Argumenta-\ntion in Political Debates. In Proceedings of the \n2016 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1405-1413, San \nDiego, California. Association for Computational \nLinguistics. \n\nTuhin Chakrabarty, Christopher Hidey, Smaranda \nMuresan, Kathy McKeown, and Alyssa Hwang. \n2019. AMPERSAND: Argument Mining for PER-\nSuAsive oNline Discussions. In Proceedings of the \n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International \nJoint Conference on Natural Language Processing \n(EMNLP-IJCNLP), pages 2926-2936, Hong Kong, \nChina. Association for Computational Linguistics. \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2018. BERT: Pre-training of \nDeep Bidirectional Transformers for Language Un-\nderstanding. arXiv. \n\nAmanda M Durik, M Anne Britt, Rebecca Reynolds, \nand Jennifer Storey. 2008. The Effects of Hedges in \nPersuasive Arguments: A Nuanced Analysis of Lan-\nguage. Journal of Language and Social Psychology, \n27(3):217-234. \n\nYunfan Gu, Zhongyu Wei, Maoran Xu, Hao Fu, Yang \nLiu, and Xuan-Jing Huang. 2018. Incorporating \nTopic Aspects for Online Comment Convincingness \nEvaluation. Proceedings of the 5th Workshop on Ar-\ngument Mining, pages 97-104. \n\nZhen Guo, Zhe Zhang, and Munindar Singh. 2020. In \nOpinion Holders' Shoes: Modeling Cumulative In-\nfluence for View Change in Online Argumentation. \nIn Proceedings of The Web Conference 2020, WWW, \npage 2388-2399, New York, NY, USA. Association \nfor Computing Machinery. \n\nIvan Habernal and Iryna Gurevych. 2016a. What \nmakes a convincing argument? Empirical analysis \nand detecting attributes of convincingness in Web \nargumentation. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language \nProcessing, pages 1214-1223, Austin, Texas. Asso-\nciation for Computational Linguistics. \n\nIvan Habernal and Iryna Gurevych. 2016b. Which ar-\ngument is more convincing? Analyzing and predict-\ning convincingness of Web arguments using bidi-\nrectional LSTM. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational \nLinguistics (Volume 1: Long Papers), pages 1589-\n1599, Berlin, Germany. Association for Computa-\ntional Linguistics. \n\nXinyu Hua, Zhe Hu, and Lu Wang. 2019. Argument \nGeneration with Retrieval, Planning, and Realiza-\ntion. arXiv. \n\nKen Hyland. 2005. Metadiscourse : Exploring Interac-\ntion in Writing. Continuum Discourse Series. Con-\ntinuum. \n\nLu Ji, Zhongyu Wei, Xiangkun Hu, Yang Liu, \nQi Zhang, and Xuanjing Huang. 2018. Incorpo-\nrating Argument-Level Interactions for Persuasion \nComments Evaluation using Co-attention Model. In \nProceedings of COLING , the th International Con-\nference on Computational Linguistics, pages 1-12. \n\nYohan Jo and Alice H Oh. 2011. Aspect and Senti-\nment Unification Model for Online Review Analy-\nsis. In Proceedings of the fourth ACM international \nconference on Web search and data mining, pages \n815-824. \n\nYohan Jo, Shivani Poddar, Byungsoo Jeon, Qinlan \nShen, Carolyn Penstein Ros\u00e9, and Graham Neu-\nbig. 2018. Attentive Interaction Model: Modeling \nChanges in View in Argumentation. In Proceedings \nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics: \nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 103-116, New Orleans, Louisiana. As-\nsociation for Computational Linguistics. \n\nZixuan Ke, Hrishikesh Inamdar, Hui Lin, and Vincent \nNg. 2019. Give Me More Feedback II: Annotating \nThesis Strength and Related Attributes in Student \nEssays. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3994-4004, Florence, Italy. Association \nfor Computational Linguistics. \n\nEmaad Manzoor, George H. Chen, Dokyun Lee, and \nMichael D. Smith. 2020. Influence via ethos: On the \npersuasive power of reputation in deliberation online. \narXiv preprint arXiv:2006.00707. \n\nGaku Morio, Ryo Egawa, and Katsuhide Fujita. 2019. \nRevealing and Predicting Online Persuasion Strat-\negy with Elementary Units. In Proceedings of the \n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International \nJoint Conference on Natural Language Processing \n(EMNLP-IJCNLP), pages 6273-6278, Hong Kong, \nChina. Association for Computational Linguistics. \n\nElena Musi. 2017. How did you change my view? \nA corpus-based study of concessions' argumentative \nrole. Discourse Studies, 20(2):270-288. \n\nTimothy Niven and Hung-Yu Kao. 2019. Probing Neu-\nral Network Comprehension of Natural Language \nArguments. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, \nACL, pages 4658 -4664. \n\nE Michael Nussbaum, Ian J Dove, Nathan Slife, Car-\nolAnne M Kardash, Refika Turgut, and David Val-\nlett. 2018. Using critical questions to evaluate writ-\nten and oral arguments in an undergraduate gen-\neral education seminar: a quasi-experimental study. \nReading and Writing, 19(2):1-22. \n\nNils Reimers, Benjamin Schiller, Tilman Beck, Jo-\nhannes Daxenberger, Christian Stab, and Iryna \nGurevych. 2019. Classification and Clustering of \nArguments with Contextualized Word Embeddings. \nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 567-\n578, Florence, Italy. Association for Computational \nLinguistics. \n\nMichael Ringgaard, Rahul Gupta, and Fernando C. N. \nPereira. 2017. SLING: A framework for frame se-\nmantic parsing. CoRR, abs/1710.07032. \n\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017. \nSemEval-2017 Task 4: Sentiment Analysis in Twit-\nter. \nIn Proceedings of the 11th International \nWorkshop on Semantic Evaluation (SemEval-2017), \npages 502-518, Vancouver, Canada. Association for \nComputational Linguistics. \n\nChenhao Tan, Lillian Lee, and Bo Pang. 2014. The ef-\nfect of wording on message propagation: Topic-and \nauthor-controlled natural experiments on Twitter. In \nProceedings of the 52nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: \nLong Papers), pages 175-185, Baltimore, Maryland. \nAssociation for Computational Linguistics. \n\nChenhao Tan, Vlad Niculae, Cristian Danescu-\nNiculescu-Mizil, and Lillian Lee. 2016. Winning \nArguments: Interaction Dynamics and Persuasion \nStrategies in Good-faith Online Discussions. In \nProceedings of the 25th International Conference \non World Wide Web, pages 613-624. International \nWorld Wide Web Conferences Steering Committee. \n\nURL1. \n\nhttp://www-di.inf.puc-rio.br/ \nendler/students/Hedging_Handout.pdf. \n\nURL2. \n\nURL3. \n\nHenning Wachsmuth, Nona Naderi, Ivan Habernal, \nYufang Hou, Graeme Hirst, Iryna Gurevych, and \nBenno Stein. 2017a. Argumentation Quality Assess-\nment: Theory vs. Practice. In Proceedings of the \n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages \n250-255, Vancouver, Canada. Association for Com-\nputational Linguistics. \n\nHenning Wachsmuth, Nona Naderi, Yufang Hou, \nYonatan Bilu, Vinodkumar Prabhakaran, Tim Al-\nberdingk Thijm, Graeme Hirst, and Benno Stein. \n2017b. Computational Argumentation Quality As-\nsessment in Natural Language. In Proceedings of \nthe 15th Conference of the European Chapter of the \nAssociation for Computational Linguistics: Volume \n1, Long Papers, pages 176-187, Valencia, Spain. As-\nsociation for Computational Linguistics. \n\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein. \n2018. Retrieval of the Best Counterargument with-\nout Prior Topic Knowledge. In Proceedings of the \n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages \n241-251. Association for Computational Linguis-\ntics. \n\nDouglas Walton, Chris Reed, and Fabrizio Macagno. \n2008. Argumentation Schemes. Cambridge Univer-\nsity Press. \n\nLu Wang, Nick Beauchamp, Sarah Shugars, and \nKechen Qin. 2017. Winning on the Merits: The \nJoint Effects of Content and Style on Debate Out-\ncomes. Transactions of Association of Computa-\ntional Linguistics, 5:219-232. \n\nAmy Beth Warriner, Victor Kuperman, and Marc Brys-\nbaert. 2013. Norms of valence, arousal, and dom-\ninance for 13,915 English lemmas. Behavior Re-\nsearch Methods, 45(4):1191-1207. \n\nZhongyu Wei, Yang Liu, and Yi Li. 2016. Is This Post \nPersuasive? Ranking Argumentative Comments in \nOnline Forum. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational \nLinguistics (Volume 1: Long Papers), pages 195-\n200, Berlin, Germany. Association for Computa-\ntional Linguistics. \n\nTheresa Wilson, Janyce Wiebe, and Paul Hoffmann. \n2005. Recognizing Contextual Polarity in Phrase-\nLevel Sentiment Analysis. In Proceedings of Hu-\nman Language Technology Conference and Confer-\nence on Empirical Methods in Natural Language \nProcessing, pages 347-354, Vancouver, British \nColumbia, Canada. Association for Computational \nLinguistics. \n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien \nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen, \nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu, \nTeven Le Scao, Sylvain Gugger, Mariama Drame, \nQuentin Lhoest, and Alexander M. Rush. 2019. \nHuggingFace's Transformers: State-of-the-art Natu-\nral Language Processing. arXiv. \n\n\nTable 6 :\n6Lexicons and regular expressions used in feature extraction.\n\nTable 8 :\n8Reproducibility checklist.\n\nTable 9\n9shows the prediction accuracy with an additional metric mean average precision (MAP).Attacked \nSuccessfully Attacked \n\nP@1 \nAny@3 \nMAP \nAUC \nP@1 \nAny@3 \nMAP \nAUC \n\nRandom \n35.9 \n66.0 \n48.0 \n50.1 \n18.9 \n45.0 \n34.0 \n50.1 \nLength \n42.9 \n73.7 \n53.7 \n54.5 \n22.3 \n52.1 \n38.8 \n55.7 \n\nLogistic Regression \n47.1 \n76.2 \n56.5 \n61.7 \n24.2 \n54.5 \n41.0 \n59.3 \n(\u00d7) Content \n45.2 \n74.4 \n54.7 \n58.1 \n24.0 \n52.6 \n39.9 \n57.0 \n(\u00d7) Knowledge \n47.0 \n76.0 \n56.4 \n61.7 \n24.1 \n54.3 \n40.5 \n59.0 \n(\u00d7) Prop Types \n46.7 \n75.9 \n56.2 \n61.5 \n24.4 \n53.6 \n40.7 \n59.0 \n(\u00d7) Tone \n47.0 \n76.0 \n56.4 \n61.9 \n25.2 \n56.2 \n41.4 \n59.4 \nBERT \n49.6 \n77.8 \n57.9 \n64.4 \n28.3 \n57.2 \n43.1 \n62.0 \n\nHuman \n51.7 \n80.1 \n-\n-\n27.8 \n54.2 \n-\n-\n\n\n\nTable 9 :\n9Prediction accuracy.\n\n\nI believe that socialism is next step for the U.S. It vastly successful people to income to provide services fo the same opportunities. Ever universal health care, social se collage), a livable minimum w get screwed over by businesse people. Businesses don't g community they leach off of be the responsibility of the g do. When many people sp nations like the U.S.S.R. (S problems with these nations from a violent revolution Socialism is an economic polic with the current governing b European country's sudo-s healthcare) are a perfect ex beneficial to people. B.T.W wait to have reasonable debatesComparison (0.20) \n\nTopic35 (-0.07) \n\nKialoFreq (0.72) \nKialoFreq (0.14) \nPrediction (0.12) \nKialoExtr (-0.08) \n\nComparison (0.20) \nTopic39 (-0.12) \n\nKialoFreq (0.86) \nKialoAttr (0.20) \n\nUse of \"We\" (-0.19) \n\nKialoFreq (0.23) \nKialoAttr (0.14) \nKialoExtr (-0.12) \n\nKialoFreq (0.23) \nKialoAttr (0.22) \nNormative (0.18) \nTopic33 (0.08) \nKialoExtr (-0.12) \n\nSuccess 2 (t3_8g5ukh) \nSuccess 3 (t3_ \n\nd Bailey. \n\nut I only \n\nhysically) \n\nI'm not \nidence to \nmplete \n\n. \n\nYou \nu could \neir looks. \n\nnging out \now we'd \nre to enter \n\n\n\n\n(a) Successful example 2.KialoFreq (0.78) \nComparison (0.20) \nTopic43 (0.19) \nKialoAttr (0.13) \nUse of \"We\" (-0.19) \nPersonal (-0.20) \n\nErroneous 2 (t3_95wq12) \n\nTopic43 (0.19) \n\nTopic43 (0.19) \nNormative (0.18) \n\nTopic43 (0.19) \nPersonal (-0.20) \n\nKialoFreq (0.75) \nComparison (0.20) \nTopic43 (0.19) \nPrediction (0.12) \nKialoAttr (0.06) \nKialoExtr (-0.12) \nUse of \"We\" (-0.19) \nTopic43 (0.19) \nExample (0.11) \n\nKialoFreq (0.36) \nTopic32 (0.11) \nUse of \"We\" (-0.19) \n\ncceeding \n\nt party \n\ndividual \ny Moore \n\ne was \n\nminors \no allows \n\nr party \n\ne person \nefficient \n\nmore \nre on \nol and \n\nALL of \n\npeople \ne ideals \nparties \nthis is \n\nfect the \n\nk this \n\nwith the \nd either \n\nneeds \n\nComparison (0.20) \n\nTopic35 (-0.07) \n\nKialoFreq (0.72) \nKialoFreq (0.14) \nPrediction (0.12) \nKialoExtr (-0.08) \n\nComparison (0.20) \nTopic39 (-0.12) \n\nKialoFreq (0.86) \nKialoAttr (0.20) \n\nUse of \"We\" (-0.19) \n\nKialoFreq (0.23) \nKialoAttr (0.14) \nKialoExtr (-0.12) \n\nKialoFreq (0.23) \nKialoAttr (0.22) \nNormative (0.18) \nTopic33 (0.08) \nKialoExtr (-0.12) \n\n\n\n\n.T.W. this is my first post, I can't wait to have reasonable debates with you all!Topic5 (0.39) \n\nNormative (0.18) \nTopic28 (0.16) \n\nKialoFreq (0.93) \nKialoAttr (0.09) \nTopic46 (0.06) \nKialoExtr (-0.11) \n\nTopic30 (-0.17) \n\nTopic30 (-0.17) \n\nKialoAttr (0.44) \nKialoFreq (0.23) \nTopic30 (-0.17) \n\nKialoFreq (0.36) \nKialoAttr (0.18) \nTopic2 (-0.54) \n\nUse of \"You\" (-0.15) \nPersonal (-0.20) \nTopic9 (-1.04) \n\nSuccess 3 (t3_8kqdgk) \n\nbig city \nowned a \non the \nnada to \nate that I \ngun, not \nse bows \nunting \nsociety \nbe safer \nar more \n\nKialoFreq (0.78) \nComparison (0.20) \nTopic43 (0.19) \nKialoAttr (0.13) \nUse of \"We\" (-0.19) \nPersonal (-0.20) \n\nTopic43 (0.19) \n\nTopic43 (0.19) \nNormative (0.18) \n\nTopic43 (0.19) \nPersonal (-0.20) \n\nKialoFreq (0.75) \n\n(b) Successful example 3. \n\n\n\n\nWhy/How (0.91) Use of \"We\" (-0.18) Topic37 (-0.21)KialoFreq (0.22) \nKialoAttr (0.13) \nHypothetical (-0.06) \nKialoExtr (-0.11) \n\nKialoFreq (0.45) \nTopic5 (0.39) \nKialoAttr (0.26) \nKialoExtr (-0.05) \nUse of \"We\" (-0.18) \n\nTopic5 (0.39) \nQuestOther (0.39) \n\n\n\n\nI believe that socialis next step for the U.S vastly successful peop income to provide servic the same opportunities. universal health care, so collage), a livable minim get screwed over by busi people. Businesses do community they leach of be the responsibility of do. When many peo nations like the U.S.S problems with these na from a violent revol Socialism is an economi with the current govern European country's s healthcare) are a perfe beneficial to people. B wait to have reasonable d I realize I have a bias because I grew up in a big city in Canada and not a single person I knew owned a gun and most law enforcement officers I saw on the street also didn't carry guns and I perceive Canada to generally be safer than the open carry US state that I now live in. I see zero reason to own a gun, not even for hunting. I think hunters should use bows and arrows. I admit I've never been hunting myself. I believe the presence of guns in society makes society less safe and we would all be safer if there were fewer of them and they were far more difficult and expensive to buy on the black market rather than being able to pick one up easily at a gun show parking lot using cash and with no background check. I know that violence can be committed with other weapons such as knives or running someone over with a car. But we have laws about who can drive a car and it's actually more difficult to kill people with such things and less efficient.KialoFreq (0.78) \nComparison (0.20) \nTopic43 (0.19) \nKialoAttr (0.13) \nUse of \"We\" (-0.19) \nPersonal (-0.20) \n\nErroneous 2 (t3_95wq12) \n\nTopic43 (0.19) \n\nTopic43 (0.19) \nNormative (0.18) \n\nTopic43 (0.19) \nPersonal (-0.20) \n\nKialoFreq (0.75) \nComparison (0.20) \nTopic43 (0.19) \nPrediction (0.12) \nKialoAttr (0.06) \nKialoExtr (-0.12) \nUse of \"We\" (-0.19) \nTopic43 (0.19) \nExample (0.11) \n\nKialoFreq (0.36) \nTopic32 (0.11) \nUse of \"We\" (-0.19) \n\n(b) Erroneous example 2. \n\n\nOur data and source code are available at: github. com/yohanjo/emnlp20_arg_attack\nhttps://www.reddit.com/r/changemyview\nThis allows our subsequent analyses to capture stronger signals for successful attacks than without this process.4  Further, this case happened in only one out of the 500 comments (0.2%), where the author agreed with 4 quoted sentences. In CMV, challengers do use concessions but hardly\nSome rationales inTable 2a(e.g., R1 and R9) are difficult to operationalize reliably using the current NLP technology and thus are not included in our features.6  We also tried features based on semantic frames using SLING(Ringgaard et al., 2017), but they were not helpful.\nWe achieved an average recall of 0.705, which is higher than the winner team's performance of 0.681.\nAcknowledgmentsThis research was supported by the Kwanjeong Educational Foundation.\nOn Rhetoric. A Theory of Civic Discourse (Translated by George Alexander Kennedy). Aristotle , Oxford University PressUSAAristotle. 2007. On Rhetoric. A Theory of Civic Dis- course (Translated by George Alexander Kennedy). Oxford University Press, USA.\n\nConcreteness ratings for 40 thousand generally known English word lemmas. Marc Brysbaert, Amy Beth Warriner, Victor Kuperman, Behavior Research Methods. 463Marc Brysbaert, Amy Beth Warriner, and Victor Ku- perman. 2014. Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46(3):904-911.\n", "annotations": {"author": "[{\"end\":148,\"start\":84},{\"end\":156,\"start\":149},{\"end\":162,\"start\":157},{\"end\":171,\"start\":163},{\"end\":199,\"start\":172},{\"end\":231,\"start\":200},{\"end\":305,\"start\":232},{\"end\":377,\"start\":306},{\"end\":613,\"start\":378}]", "publisher": null, "author_last_name": "[{\"end\":180,\"start\":178},{\"end\":211,\"start\":207},{\"end\":245,\"start\":238},{\"end\":317,\"start\":313},{\"end\":388,\"start\":384}]", "author_first_name": "[{\"end\":134,\"start\":130},{\"end\":155,\"start\":149},{\"end\":161,\"start\":157},{\"end\":168,\"start\":163},{\"end\":170,\"start\":169},{\"end\":177,\"start\":172},{\"end\":206,\"start\":200},{\"end\":237,\"start\":232},{\"end\":312,\"start\":306},{\"end\":383,\"start\":378}]", "author_affiliation": "[{\"end\":304,\"start\":247},{\"end\":376,\"start\":319},{\"end\":469,\"start\":412},{\"end\":556,\"start\":471},{\"end\":612,\"start\":558}]", "title": "[{\"end\":81,\"start\":1},{\"end\":694,\"start\":614}]", "venue": null, "abstract": "[{\"end\":3159,\"start\":696}]", "bib_ref": "[{\"end\":3402,\"start\":3377},{\"end\":3431,\"start\":3402},{\"end\":3481,\"start\":3463},{\"end\":3504,\"start\":3481},{\"end\":4202,\"start\":4197},{\"end\":4453,\"start\":4448},{\"end\":4666,\"start\":4661},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7364,\"start\":7348},{\"end\":7555,\"start\":7531},{\"end\":7824,\"start\":7794},{\"end\":7848,\"start\":7824},{\"end\":8021,\"start\":8004},{\"end\":8056,\"start\":8039},{\"end\":8109,\"start\":8079},{\"end\":8425,\"start\":8405},{\"end\":9190,\"start\":9171},{\"end\":9248,\"start\":9225},{\"end\":9453,\"start\":9435},{\"end\":9478,\"start\":9453},{\"end\":9495,\"start\":9478},{\"end\":9513,\"start\":9495},{\"end\":9532,\"start\":9513},{\"end\":9588,\"start\":9570},{\"end\":9611,\"start\":9588},{\"end\":9967,\"start\":9950},{\"end\":9983,\"start\":9967},{\"end\":10936,\"start\":10910},{\"end\":10955,\"start\":10936},{\"end\":10971,\"start\":10955},{\"end\":10982,\"start\":10971},{\"end\":10999,\"start\":10982},{\"end\":11016,\"start\":10999},{\"end\":11662,\"start\":11658},{\"end\":11665,\"start\":11662},{\"end\":11668,\"start\":11665},{\"end\":11671,\"start\":11668},{\"end\":16063,\"start\":16045},{\"end\":17427,\"start\":17410},{\"end\":20247,\"start\":20226},{\"end\":20532,\"start\":20511},{\"end\":20702,\"start\":20681},{\"end\":21168,\"start\":21154},{\"end\":21547,\"start\":21523},{\"end\":21679,\"start\":21650},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21963,\"start\":21940},{\"end\":22284,\"start\":22264},{\"end\":22635,\"start\":22623},{\"end\":22665,\"start\":22635},{\"end\":24749,\"start\":24748},{\"end\":25043,\"start\":25041},{\"end\":31545,\"start\":31524},{\"end\":31594,\"start\":31592},{\"end\":31713,\"start\":31711},{\"end\":31993,\"start\":31991},{\"end\":40376,\"start\":40358},{\"end\":41533,\"start\":41513},{\"end\":42157,\"start\":42139},{\"end\":43868,\"start\":43846},{\"end\":45367,\"start\":45343},{\"end\":49489,\"start\":49241},{\"end\":61251,\"start\":61250},{\"end\":62322,\"start\":62304}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":64463,\"start\":64354},{\"attributes\":{\"id\":\"fig_1\"},\"end\":64777,\"start\":64464},{\"attributes\":{\"id\":\"fig_3\"},\"end\":65022,\"start\":64778},{\"attributes\":{\"id\":\"fig_4\"},\"end\":65056,\"start\":65023},{\"attributes\":{\"id\":\"fig_5\"},\"end\":65089,\"start\":65057},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":65943,\"start\":65090},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66002,\"start\":65944},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":66307,\"start\":66003},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":69243,\"start\":66308},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":79161,\"start\":69244},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":79234,\"start\":79162},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":79273,\"start\":79235},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":79971,\"start\":79274},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":80004,\"start\":79972},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":81134,\"start\":80005},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":82177,\"start\":81135},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":82958,\"start\":82178},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":83216,\"start\":82959},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":85141,\"start\":83217}]", "paragraph": "[{\"end\":4113,\"start\":3175},{\"end\":4355,\"start\":4115},{\"end\":4454,\"start\":4357},{\"end\":5080,\"start\":4569},{\"end\":5784,\"start\":5082},{\"end\":6373,\"start\":5786},{\"end\":6639,\"start\":6375},{\"end\":6788,\"start\":6641},{\"end\":7265,\"start\":6790},{\"end\":9345,\"start\":7280},{\"end\":10283,\"start\":9347},{\"end\":10347,\"start\":10292},{\"end\":11017,\"start\":10367},{\"end\":11373,\"start\":11019},{\"end\":12064,\"start\":11375},{\"end\":12546,\"start\":12091},{\"end\":13500,\"start\":12548},{\"end\":13944,\"start\":13502},{\"end\":14193,\"start\":13946},{\"end\":14771,\"start\":14195},{\"end\":15195,\"start\":14773},{\"end\":15520,\"start\":15197},{\"end\":15715,\"start\":15561},{\"end\":16304,\"start\":15757},{\"end\":16985,\"start\":16306},{\"end\":17157,\"start\":17008},{\"end\":17551,\"start\":17169},{\"end\":18402,\"start\":17574},{\"end\":18922,\"start\":18404},{\"end\":19267,\"start\":19062},{\"end\":19714,\"start\":19289},{\"end\":19892,\"start\":19716},{\"end\":20040,\"start\":19894},{\"end\":20248,\"start\":20042},{\"end\":20405,\"start\":20250},{\"end\":20533,\"start\":20407},{\"end\":20736,\"start\":20535},{\"end\":20867,\"start\":20738},{\"end\":21071,\"start\":20869},{\"end\":21217,\"start\":21073},{\"end\":21391,\"start\":21226},{\"end\":21741,\"start\":21393},{\"end\":21987,\"start\":21743},{\"end\":22231,\"start\":21989},{\"end\":22408,\"start\":22233},{\"end\":22996,\"start\":22410},{\"end\":23847,\"start\":23038},{\"end\":24311,\"start\":23859},{\"end\":24555,\"start\":24313},{\"end\":25279,\"start\":24557},{\"end\":25311,\"start\":25291},{\"end\":26363,\"start\":26130},{\"end\":26714,\"start\":26386},{\"end\":26813,\"start\":26716},{\"end\":26998,\"start\":26815},{\"end\":27145,\"start\":27000},{\"end\":27335,\"start\":27147},{\"end\":27610,\"start\":27357},{\"end\":28016,\"start\":27612},{\"end\":28208,\"start\":28018},{\"end\":28505,\"start\":28217},{\"end\":28805,\"start\":28507},{\"end\":28987,\"start\":28807},{\"end\":29527,\"start\":29002},{\"end\":30078,\"start\":29529},{\"end\":30205,\"start\":30115},{\"end\":30636,\"start\":30229},{\"end\":31025,\"start\":30638},{\"end\":32228,\"start\":31047},{\"end\":32477,\"start\":32230},{\"end\":33142,\"start\":32489},{\"end\":33392,\"start\":33144},{\"end\":33912,\"start\":33394},{\"end\":35914,\"start\":33914},{\"end\":36736,\"start\":35925},{\"end\":39837,\"start\":36738},{\"end\":40935,\"start\":39839},{\"end\":41416,\"start\":40950},{\"end\":42197,\"start\":41418},{\"end\":43493,\"start\":42199},{\"end\":43638,\"start\":43518},{\"end\":44490,\"start\":43685},{\"end\":45215,\"start\":44492},{\"end\":45724,\"start\":45254},{\"end\":46463,\"start\":45726},{\"end\":47078,\"start\":46501},{\"end\":47508,\"start\":47080},{\"end\":48699,\"start\":47553},{\"end\":49653,\"start\":48732},{\"end\":51005,\"start\":49655},{\"end\":51114,\"start\":51049},{\"end\":52185,\"start\":51188},{\"end\":52409,\"start\":52187},{\"end\":54178,\"start\":52482},{\"end\":55156,\"start\":54180},{\"end\":55777,\"start\":55238},{\"end\":56024,\"start\":55779},{\"end\":56521,\"start\":56026},{\"end\":62789,\"start\":56523},{\"end\":64353,\"start\":62828}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19061,\"start\":18923},{\"attributes\":{\"id\":\"formula_1\"},\"end\":26129,\"start\":25312},{\"attributes\":{\"id\":\"formula_2\"},\"end\":51187,\"start\":51115}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15479,\"start\":15472},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15977,\"start\":15967},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16731,\"start\":16722},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17687,\"start\":17679},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20165,\"start\":20152},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20331,\"start\":20306},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20404,\"start\":20391},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20620,\"start\":20610},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20735,\"start\":20722},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20866,\"start\":20854},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21340,\"start\":21331},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21488,\"start\":21475},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21522,\"start\":21512},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22105,\"start\":22096},{\"end\":23979,\"start\":23970},{\"end\":24039,\"start\":24031},{\"end\":26159,\"start\":26152},{\"end\":26518,\"start\":26509},{\"end\":27441,\"start\":27432},{\"end\":28301,\"start\":28293},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28671,\"start\":28661},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30635,\"start\":30626},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32582,\"start\":32575},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33516,\"start\":33508},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33885,\"start\":33876},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34062,\"start\":34055},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":40002,\"start\":39994},{\"end\":47677,\"start\":47670},{\"end\":48374,\"start\":48367},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":48584,\"start\":48577},{\"end\":52222,\"start\":52215},{\"end\":55011,\"start\":55004}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3173,\"start\":3161},{\"end\":4567,\"start\":4457},{\"attributes\":{\"n\":\"2\"},\"end\":7278,\"start\":7268},{\"attributes\":{\"n\":\"3\"},\"end\":10290,\"start\":10286},{\"attributes\":{\"n\":\"3.1\"},\"end\":10365,\"start\":10350},{\"attributes\":{\"n\":\"3.2\"},\"end\":12089,\"start\":12067},{\"attributes\":{\"n\":\"4\"},\"end\":15559,\"start\":15523},{\"attributes\":{\"n\":\"4.1\"},\"end\":15755,\"start\":15718},{\"attributes\":{\"n\":\"4.2\"},\"end\":17006,\"start\":16988},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":17167,\"start\":17160},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":17572,\"start\":17554},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":19287,\"start\":19270},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":21224,\"start\":21220},{\"attributes\":{\"n\":\"5\"},\"end\":23036,\"start\":22999},{\"attributes\":{\"n\":\"5.1\"},\"end\":23857,\"start\":23850},{\"end\":25289,\"start\":25282},{\"attributes\":{\"n\":\"5.2\"},\"end\":26384,\"start\":26366},{\"attributes\":{\"n\":\"5.3\"},\"end\":27355,\"start\":27338},{\"attributes\":{\"n\":\"5.4\"},\"end\":28215,\"start\":28211},{\"attributes\":{\"n\":\"5.5\"},\"end\":29000,\"start\":28990},{\"attributes\":{\"n\":\"6\"},\"end\":30113,\"start\":30081},{\"attributes\":{\"n\":\"6.1\"},\"end\":30227,\"start\":30208},{\"attributes\":{\"n\":\"6.2\"},\"end\":31045,\"start\":31028},{\"attributes\":{\"n\":\"6.3\"},\"end\":32487,\"start\":32480},{\"end\":35923,\"start\":35917},{\"attributes\":{\"n\":\"7\"},\"end\":40948,\"start\":40938},{\"end\":43516,\"start\":43496},{\"end\":43683,\"start\":43641},{\"end\":45252,\"start\":45218},{\"end\":46499,\"start\":46466},{\"end\":47551,\"start\":47511},{\"end\":48712,\"start\":48702},{\"end\":48730,\"start\":48715},{\"end\":51047,\"start\":51008},{\"end\":52480,\"start\":52412},{\"end\":55186,\"start\":55159},{\"end\":55209,\"start\":55189},{\"end\":55236,\"start\":55212},{\"end\":62826,\"start\":62792},{\"end\":65034,\"start\":65024},{\"end\":65068,\"start\":65058},{\"end\":65100,\"start\":65091},{\"end\":65954,\"start\":65945},{\"end\":66013,\"start\":66004},{\"end\":79172,\"start\":79163},{\"end\":79245,\"start\":79236},{\"end\":79282,\"start\":79275},{\"end\":79982,\"start\":79973}]", "table": "[{\"end\":65943,\"start\":65259},{\"end\":66307,\"start\":66015},{\"end\":69243,\"start\":67838},{\"end\":79161,\"start\":69733},{\"end\":79971,\"start\":79369},{\"end\":81134,\"start\":80608},{\"end\":82177,\"start\":81162},{\"end\":82958,\"start\":82262},{\"end\":83216,\"start\":83011},{\"end\":85141,\"start\":84671}]", "figure_caption": "[{\"end\":64463,\"start\":64356},{\"end\":64777,\"start\":64466},{\"end\":65022,\"start\":64780},{\"end\":65056,\"start\":65036},{\"end\":65089,\"start\":65070},{\"end\":65259,\"start\":65102},{\"end\":66002,\"start\":65956},{\"end\":67838,\"start\":66310},{\"end\":69733,\"start\":69246},{\"end\":79234,\"start\":79174},{\"end\":79273,\"start\":79247},{\"end\":79369,\"start\":79284},{\"end\":80004,\"start\":79984},{\"end\":80608,\"start\":80007},{\"end\":81162,\"start\":81137},{\"end\":82262,\"start\":82180},{\"end\":83011,\"start\":82961},{\"end\":84671,\"start\":83219}]", "figure_ref": "[{\"end\":4009,\"start\":4000},{\"end\":4735,\"start\":4727},{\"end\":12674,\"start\":12665},{\"end\":34815,\"start\":34807},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55277,\"start\":55268},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55818,\"start\":55809},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":56064,\"start\":56055},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":56561,\"start\":56552}]", "bib_author_first_name": "[{\"end\":86101,\"start\":86092},{\"end\":86341,\"start\":86337},{\"end\":86356,\"start\":86353},{\"end\":86361,\"start\":86357},{\"end\":86378,\"start\":86372}]", "bib_author_last_name": "[{\"end\":86351,\"start\":86342},{\"end\":86370,\"start\":86362},{\"end\":86387,\"start\":86379}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":86261,\"start\":86009},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":34985620},\"end\":86598,\"start\":86263}]", "bib_title": "[{\"end\":86335,\"start\":86263}]", "bib_author": "[{\"end\":86104,\"start\":86092},{\"end\":86353,\"start\":86337},{\"end\":86372,\"start\":86353},{\"end\":86389,\"start\":86372}]", "bib_venue": "[{\"end\":86090,\"start\":86009},{\"end\":86414,\"start\":86389}]"}}}, "year": 2023, "month": 12, "day": 17}