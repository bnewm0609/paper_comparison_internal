{"id": 218487118, "updated": "2023-11-08 02:55:56.462", "metadata": {"title": "Learning the Associations of MITRE ATT&CK Adversarial Techniques", "authors": "[{\"first\":\"Rawan\",\"last\":\"Al-Shaer\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Spring\",\"middle\":[\"M.\"]},{\"first\":\"Eliana\",\"last\":\"Christou\",\"middle\":[]}]", "venue": "2020 IEEE Conference on Communications and Network Security (CNS)", "journal": "2020 IEEE Conference on Communications and Network Security (CNS)", "publication_date": {"year": 2020, "month": 4, "day": 16}, "abstract": "The MITRE ATT&CK Framework provides a rich and actionable repository of adversarial tactics, techniques, and procedures (TTP). However, this information would be highly useful for attack diagnosis (i.e., forensics) and mitigation (i.e., intrusion response) if we can reliably construct technique associations that will enable predicting unobserved attack techniques based on observed ones. In this paper, we present our statistical machine learning analysis on APT and Software attack data reported by MITRE ATT&CK to infer the technique clustering that represents the significant correlation that can be used for technique prediction. Due to the complex multidimensional relationships between techniques, many of the traditional clustering methods could not obtain usable associations. Our approach, using hierarchical clustering for inferring attack technique associations with 95% confidence, provides statistically significant and explainable technique correlations. Our analysis discovers 98 different technique associations (i.e., clusters) for both APT and Software attacks. Our evaluation results show that 78% of the techniques associated by our algorithm exhibit significant mutual information that indicates reasonably high predictability.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.01654", "mag": "3048012689", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cns/Al-ShaerSC20", "doi": "10.1109/cns48642.2020.9162207"}}, "content": {"source": {"pdf_hash": "b5bf49d1af9d236a8349c7ce554a86f006fe820d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.01654v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.01654", "status": "GREEN"}}, "grobid": {"id": "6fe7d4628a90675b37a04c366691b0d8f8444d7b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b5bf49d1af9d236a8349c7ce554a86f006fe820d.txt", "contents": "\nLearning the Associations of MITRE ATT&CK Adversarial Techniques\n\n\nRawan Al-Shaer \nElectrical & Computer Engineering\nCERT/CC\nCarnegie Mellon University Pittsburgh\nUnited States\n\nJonathan M Spring jspring@sei.cmu.edu \nDepartment of Mathematics & Statistics\nSEI Carnegie Mellon University Pittsburgh\nUnited States\n\nEliana Christou \nUniversity of North Carolina Charlotte Charlotte\nUnited States\n\nLearning the Associations of MITRE ATT&CK Adversarial Techniques\n\nThe MITRE ATT&CK Framework provides a rich and actionable repository of adversarial tactics, techniques, and procedures (TTP). However, this information would be highly useful for attack diagnosis (i.e., forensics) and mitigation (i.e., intrusion response) if we can reliably construct technique associations that will enable predicting unobserved attack techniques based on observed ones. In this paper, we present our statistical machine learning analysis on APT and Software attack data reported by MITRE ATT&CK to infer the technique clustering that represents the significant correlation that can be used for technique prediction. Due to the complex multidimensional relationships between techniques, many of the traditional clustering methods could not obtain usable associations. Our approach, using hierarchical clustering for inferring attack technique associations with 95% confidence, provides statistically significant and explainable technique correlations. Our analysis discovers 98 different technique associations (i.e., clusters) for both APT and Software attacks. Our evaluation results show that 78% of the techniques associated by our algorithm exhibit significant mutual information that indicates reasonably high predictability.\n\nI. INTRODUCTION\n\nAs cyber attacks increase in volume and sophistication, the state of the art of cybersecurity solutions is still lagging behind. According to Red Canary, Advanced Persistent Threat (APT) attacks have increased from approximately 500 attacks per year in 2009 to almost 1500 APT attacks per year in 2016. Verizon and SANS reported that in 2017 about 68% of attacks went undiscovered, and even after detection, about 77% of these attacks took hours to days to get remediated. The lack of timely detection and response is mainly caused by the insufficient support of attack action correlations and prediction to allow for proactive intrusion investigation and mitigation.\n\nMITRE has created a public knowledge-based repository of adversary tactics and techniques called MITRE ATT&CK. As of February 2020, MITRE ATT&CK provides a total of 440 attack \"techniques\" belonging to 27 different \"tactics\". A tactic is a behavior that supports a strategic goal; a technique is a possible method of executing a tactic. Each technique has a description explaining what the technique is, how it may be executed, when it may be used, and various \"procedures\" for performing it. There are 174 techniques belonging to 15 preattack tactics, and 266 techniques belonging to 12 post-exploit (Enterprise) tactics [1]. For example, the PRE-T1345 (Create Custom Payloads) pre-attack technique, which belongs to the Build Capabilities tactic, describes how an adversary creates custom payloads with which malware can perform malicious actions. Also, the post-exploit technique T1003 (Credential Dumping), which belongs to the Credential Access tactic, describes how an adversary obtains account login and password information in the form of a hash or clear text password from the operating system and software. A sequence of techniques from different tactics used for an attack is what we call a TTP (Tactics, Techniques, Procedures) chain. Moreover, the combination of MITRE ATT&CK techniques in a TTP chain represents various attack scenarios that can be composed in an attack graph [2].\n\nMITRE ATT&CK techniques and procedures provide behavioral observables for detecting attacks by analyzing cyber artifacts collected from the network and end-system. The structure of TTP allows analysts to organize which adversarial actions belong to specific procedures that relate to specific techniques and tactics, and helps the analyst to understand what an adversary may be trying to achieve and how to better defend against it. MITRE ATT&CK highlights many techniques an adversary might use, but does not provide sufficient tips on how the adversary might combine different techniques to accomplish their goals. Thus, the technique associations an analyst needs to construct TTP chains remain underspecified. Technique associations are important because they help an analyst to reason about adversarial behavior and predict unobserved techniques based on the observed ones in the TTP Chain. Without technique associations, an analyst will struggle to reason efficiently about adversarial behavior as the search space grows too large as the number of TTP chains increases exponentially with the number of given techniques. Since little to no work has been done regarding technique correlations, this paper focuses on learning the attack technique associations that manifest technique inter-dependencies and relationships based on data of real-life attacks (66 Advanced Persistent Threats, and 204 attacks by Software).\n\nFor these reasons, we developed a novel approach using hierarchical clustering to infer technique associations that represent various technique inter-dependencies in a TTP chain. This paper's key contribution is discovering, with 95% confidence, the fine-grain technique associations that support predictability of attack behavior. Moreover, the hierarchical clustering devel-oped in this work can also be used to infer the coarse-grain associations between techniques across different clusters. The fine-grain associations demonstrate sequential, disjunctive, or conjunctive relationships; the coarse-grain associations demonstrate complementary relationships that complete the attack chain.\n\nTo address these goals, we first explored various partitioned clustering methods in order to group attack techniques that are likely to co-occur together in the TTP Chain. We analyzed a dataset of 270 APT and Software attacks which include a total of 345 techniques using K-means clustering [3], Partition Around Medoids (PAM) [4], and Fuzzy clustering [5]. However, we found that, due to the high dimensional data, such clustering techniques could not reveal practically meaningful associations, as the clusters are highly overlapping and difficult to distinguish. We then developed hierarchical clustering, which is more likely to be a suitable approach to represent the sophisticated attack patterns, especially in APT attacks, since it can model the correlations at various levels relative to the height of the hierarchy.\n\nThere are three key challenges to achieve accurate research results given our goals. First, we address the importance of establishing a suitable distance metric for clustering in terms of technique correlations in attack TTP chains while maintaining interpretability. We integrate existing distance metrics (Jaccard [6] distance and Phi Coefficient [7] correlational distance) that measure the co-occurrences and co-absences of the techniques into clustering algorithms. Second, we address the multi-dimensional relationships exhibited by attack techniques by extending the agglomerative hierarchical clustering method to obtain accurate results. In particular, we explore a range of hierarchical linkage approaches to establish the optimal method for binary multi-dimensional data. As a result, our developed approach obtains the most compact technique associations. Third, we address the challenge of validating the stability and significance of the learned hierarchical clustering and technique associations using a statistical hypothesis test. Our goal is to ensure that the resulting associations will be statistically significant at a rigorous confidence level ensuring that no more than 5% of the results are due to random chance. This paper is organized as follows: Section II introduces the datasets used in this work, along with their challenges and limitations. Section III presents the preliminary analysis and Section IV presents our approach to inferring technique associations. Sections V and VI present the evaluation and the findings of our approach, respectively.\n\n\nII. DATASETS\n\nWe analyzed two different datasets from MITRE ATT&CK. The datasets referenced 270 total attack instances, made up of 209 unique techniques. In this section, we will describe the nature of the datasets in more detail, as well as the challenges they pose and their limitations.\n\n\nA. Description\n\nStemming from MITRE's classification of reported attacks, the categories of the instances in the datasets are Advanced Persistent Threats (APTs) and Software attacks. APT attacks are synonymous to MITRE's terminology of threat actor \"Groups\", while Software attacks encompass malware, ransomware, trojans, Remote Access Tools (RATs), and other code used for malicious purposes.\n\nEach dataset consists of all the post-exploit techniques which compose the TTP chain of any given APT or Software. In the datasets, we treat each TTP chain as an attack instance and every technique as a feature. The datasets are composed of discrete variables, specifically, asymmetrical binary variables. The outcome of the features is either 0 or 1, representing the negative or positive occurrence of a technique in an attack instance, respectively. An asymmetrical binary dataset refers to a structure such that an outcome of 1, or the positive occurrence of a technique, is more informative than an outcome of 0.\n\nThe \n\n\nB. Challenges\n\nWe faced two challenges with the datasets. The first challenge involved adjusting for the differing complexities among the Software and APT attack instances. The second challenge addressed MITRE's constant updates to the ATT&CK framework.\n\nFor the first challenge, we addressed the ramifications of less composite attack instances. Since APT attack instances represent a full campaign containing several high-level and high-impact goals, their TTP chain is more complex than that of Software attack instances, which are typically carried out for specific low-level purposes. This implicit difference between APT and Software attacks is reflected where many of the Software attack instances are comprised of very few techniques. Attack instances with scarce technique occurrences can alter our results and create misleading technique associations due to less complex Software attacks reporting the use of techniques that are not combined in any meaningful way. For that reason, we decided to only include Software attack instances which employ at least five different techniques across five different tactics.\n\nFor the second challenge, we addressed the common updates to the ATT&CK Framework and the discovery of newly reported attacks. We emphasize a transparent analysis methodology, as recommended by [9], so our work could be repeated as the datasets evolve.\n\n\nC. Limitations\n\nThe datasets have two salient limitations. The first limitation is described by MITRE as a limit of their data collection process. MITRE states that the APT and Software attacks are not representative of all possible techniques used by the actors associated with the observed data, rather a subset of what has been available through public and open source reporting. Therefore, the actual techniques these attacks utilized, or the ground truth, is difficult to determine or even discover.\n\nThe second limitation of the attacks in the datasets is mapping biases. We recognize that heuristics and automated mappings of threat reports to techniques may inadvertently possess a degree of proclivity. For these reasons, our analysis provides an approach for characterizing APT and Software attacks that can constantly be enhanced.\n\n\nIII. PRELIMINARY ANALYSIS\n\nOur process of knowledge discovery through clustering involved several stages of analysis. In this section, we will discuss our preliminary analysis using partitioned clustering, particularly K-means [3], PAM [4], and Fuzzy [5] clustering. The preliminary results show the true complexity of inferring accurate and applicable technique associations.\n\n\nA. Partitioned Clustering of Techniques\n\nPartitioned clustering is used to classify observations into groups, or clusters, based on their similarity. The three partitioned clustering methods we investigated require choosing the optimal number of clusters, K, as described below in Section III-A1. The details of the clustering algorithms will be discussed in Section III-A2, and the validation methods are discussed in Section III-A3.\n\n1) Choosing the Optimal K: In order to perform any type of partitioned clustering, the number of clusters must be specified in advance. While there are numerous ways to determine the optimal number of clusters, we found the Elbow and Silhouette methods appropriate for the nature of the datasets. Throughout this paper, we will use k for any number of clusters, and K for the optimal number of clusters in partitioned clustering.\n\nThe Elbow method chooses K by minimizing the withincluster variance, which is calculated using Euclidean distances, a distance metric inappropriate for binary data; see Remark 1 for details. Thus, we customized the use of the Elbow method by incorporating the dissimilarity matrix. Specifically, we computed the cluster variance by summing the squared dissimilarity divided by the size of each cluster. Then, as represented by the name, K is determined by the elbow, the point at which the cluster variance by dissimilarity suddenly decreases.\n\nThe Silhouette method measures how well a data point belongs in a cluster by computing the pairwise distances between all the objects in every cluster, using any specified distance metric. In order to achieve well clustered results, the distance between the clusters, known as the silhouette width, should be (a) Elbow method (b) Silhouette method  Observe that both methods resulted in K = 14 for the dataset of APT attacks. However, for the dataset of Software attacks, the Elbow method determined K = 16, while the Silhouette method found K = 2 as the first optimal and K = 16 as the second optimal. Therefore, we decided to use K = 16 as the optimal number of clusters for the Software attacks dataset.\n\n2) Clustering Algorithms: The three partitioned clustering methods considered in the preliminary analysis are K-means, PAM, and Fuzzy Clustering.\n\nK-means is one of the most common algorithms for clustering a dataset into K clusters. The objective is to minimize the total-within cluster variation, defined as the sum of squared Euclidean distances between items and their corresponding centroid [10]. The total within-cluster sum of squares for a specific cluster C k will be denoted as W k ss , and is formally expressed as\nW k ss = xi\u2208C k (x i \u2212 \u00b5 k ) 2 , k = 1, . . . , K,(1)\nwhere x i represents a data point belonging to the cluster C k , and \u00b5 k is the centroid and mean value of all the points assigned to cluster C k . We deemed K-means clustering as inappropriate for the datasets for the following two reasons:\n\n(1) the total within-cluster sum of squares computation uses Euclidean distances, a distance metric inappropriate for the binary datasets; see Remark 1 for details; and (2) the way the centroid is defined in K-means clustering creates a value that cannot be mapped to the original dataset. Moreover, finding the difference between binary points and a mean centroid will indeed result in all the points with a value of 1 to be tied, and all the points with a value of 0 to be tied. K-means clustering does not address ties, and in fact will arbitrarily assign points to a cluster [11]. PAM, which is sometimes referred to as K-medoids, is built upon choosing cluster medoids that represent every cluster. Cluster medoids correspond to the most centrally located data point in a cluster. The objective of PAM clustering is to minimize the sum of the dissimilarities between every data point in a cluster and the cluster medoid. The use of medoids as actual data points rather than centroids prompts PAM clustering as a more robust partitioned clustering algorithm to outliers. Therefore, PAM clustering is less sensitive to noise in the data, compared to K-means clustering [10]. In addition, the dissimilarity measure for PAM clustering can utilize any distance metric specified, allowing the flexibility to use the appropriate distance metric for the datasets considered in this work.\n\nFinally, Fuzzy clustering is different than K-means and PAM clustering in the aspect that cluster memberships are rather probabilistic. Using a degree of fuzziness, cluster membership probabilities are computed by incorporating that value with the dissimilarity between every point and the medoid. Fuzzy clustering can be visualized by taking the cluster with the highest probability for each data point. Note that, the Fuzzy clustering used in this work follows the Fuzzy analysis clustering algorithm, in contrast with the Fuzzy C-means algorithm which follows K-means centroids and Euclidean distances. Fuzzy analysis clustering allows us to specify the suitable distance metrics for the datasets.\n\n3) Clustering Validation: We validated the performance of partitioned clustering by measuring the intra-cluster compactness and inter-cluster separation using a silhouette analysis and the Dunn index. A silhouette analysis after clustering measures the proximity each technique in one cluster to a technique in its neighboring clustering. A silhouette width near one indicates that the object is well-clustered and separated from neighboring clusters. A silhouette width less than 0 suggests that the object is not well-clustered and is lying in overlapping clusters or between clusters. The average silhouette width is simply an average of the silhouette width for each object clustered, and measures the overall performance of partitioned clustering on a dataset. The Dunn index computes the ratio of the diameter of the clusters and the distance between clusters, and should be maximized for a well-clustered dataset.\n\n\nB. Preliminary Results\n\nThe cluster plot for each paritioned clustering method and corresponding dataset portrays K clusters represented in different colors, where the axes are reduced to two dimensions for visualization purposes. The results of PAM and Fuzzy clustering for the APT and Software datasets are shown in Figures 3 and  4, respectively. Note that these preliminary clustering results utilized the Phi coefficient correlational distance, discussed in further detail in Section IV-A.\n\nIt is evident that the clusters in Figures 3 and 4 are not well partitioned, and in fact the overlap of the techniques makes it difficult to distinguish any potential technique associations. A silhouette analysis and the Dunn index also confirm that the partitioned clustering of the techniques do not yield accurate or explainable results. The average silhouette width for the PAM clustering of the APT and Software attack datasets are 0.18 and 0.08, respectively. The clusters silhouette plot is shown in Figure 5, where each bar represents a technique in the dataset, and every distinct color is a cluster in the PAM cluster plot. The Dunn index of the PAM clustering for the APT attacks dataset is 0.24 and for the Software attacks dataset is 0.15.\n\nSince Fuzzy clustering is based on probabilistic memberships, a silhouette analysis and Dunn index cannot be reported, however, analyzing the technique cluster memberships revealed that the probabilities follow a near uniform distribution. This strongly suggests the lack of well-separated partitions between clusters or interpretability of Fuzzy clustering. Although the partitioned clustering did not exhibit any clear patterns, we did find some interesting results in the PAM clustering of the APT dataset in Figure 3a. For example, techniques T1193 (Spearphishing Attachment) and T1204 (User Execution) appeared on the bottom right of cluster 13. Interestingly, these two techniques are strongly correlated in a TTP Chain, as specifically described in the MITRE ATT&CK Framework [1]. This example indicates that, while there are no clear patterns in the partitioned clustering, there may be a potential for another clustering algorithm that can capture the high dimensional and complex nature of the data. In order to be able to infer accurate and practically meaningful technique associations, our approach, involving hierarchical clustering, is introduced below.\n\n\nIV. INFERRING ATTACK TECHNIQUE ASSOCIATIONS\n\nIn this section, we will discuss the unique approach we developed in order to infer attack technique associations with a high level of confidence. Section IV-A identifies the appropriate distance metric to be used in this work, while Section IV-B discusses the analysis of clustering tendency. Section IV-C introduces hierarchical clustering, and Section IV-D presents the statistical hypothesis test performed for inferring the ultimate statistically significant technique associations at a confidence level of 95%.\n\n\nA. Distance Metrics\n\nClustering is performed on objects using measures of similarity, or distance metrics. As computing distances differs per the nature of the dataset, it is imperative to use the suitable distance measure for the variables in the datasets considered in this work. With the datasets being composed of binary variables, we determined that Jaccard [6] distances and Phi Coefficient [7] correlational distances are the most appropriate.\n\nThe Jaccard similarity between any two techniques T i and T j is defined as follows\nJ s (T i , T j ) = n 11 n 01 + n 10 + n 11 ,(2)\nwhere n is the total number of attack instances in the datasets and n 01 , n 10 , and n 11 represent the frequency of attacks corresponding to values of T i and T j , described in Table I. Note that, the value of n 00 is not considered in the computation of the Jaccard similarity index in equation (2), rather only the co-occurrences are taken into account. The Jaccardian distance between techniques T i and T j measures the dissimilarity, and is defined as the complement of the Jaccard similarity, that is, 1-J s (T i , T j ). The Jaccardian distance between techniques T i and T j can be interpreted as the ratio of their intersection divided by their union. While this metric is appropriate for the nature of the datasets considered here, it may not yield explainable technique associations.\n\nThe Phi Coefficient is an empirical non-parametric correlation measure specifically for binary data. The Phi Coefficient correlation between any two techniques T i and T j is defined as r \u03c6 (T i , T j ) = n 11 n 00 \u2212 n 10 n 01 n 1\u00b7 n 0\u00b7 n \u00b71 n \u00b70 ,\n\nwhere all values of n are defined in Table I. \nT j = 1 T j = 0 total T i = 1 n 11 n 10 n 1\u00b7 T i = 0 n 01 n 00 n 0\u00b7 total n \u00b71 n \u00b70 n\nAs expressed in equation (3), r \u03c6 (T i , T j ) measures the frequency of the co-occurrences and co-absences of techniques T i and T j . The Phi Coefficient distance between techniques T i and T j is defined as the complement of the correlation, that is 1-r \u03c6 (T i , T j ). Since the Phi Coefficient is a correlational metric between T i and T j , it provides the capability of deducing whether techniques T i and T j are correlated at a certain level due to their co-occurrences and co-absences in the TTP Chains. Therefore, the Phi Coefficient metric is suitable for the binary datasets and provides an easily interpretable conclusion.\n\nRemark 1 (Unsuitable Distance Metric): One of the most commonly used distance metrics applied for clustering is the Euclidean distance. Although the Euclidean distance metric is typically applied for continuous variables, it is not appropriate for a binary dataset. The Euclidean measure drives the calculated distance for a binary dataset to most often be tied and, thus, causes the loss of distinct measures of similarity of the variables.\n\n\nB. Assessing Clustering Tendency\n\nBefore performing clustering on the datasets, we measured the clusterability of the data. This process, known as assessing clustering tendency, involves evaluating whether the dataset contains meaningful clusters. This is important because clustering methods can often return clusters even in the absence of notable groups in the dataset. In other words, unknowingly applying a clustering method to a dataset will divide the data into clusters, regardless if they are relevant [10]. In addition, assessing clustering tendency using the two appropriate distance metrics, Jaccard and Phi Coefficient, will determine which metric is the most suitable for further cluster analysis.\n\nThe approach used in this work, known as the Hopkins statistic [12], is defined as the probability a given dataset D is generated by a random data distribution. The idea is to compare the distance between the points in dataset D to the distance between points drawn from a randomly simulated dataset D R . Here, D is defined to be either the dataset of APT attacks or Software attacks. We calculated the Hopkins statistic for the datasets as described in Algorithm 1. Let H J denote the Hopkins statistic computed using Jaccard distances, and H \u03c6 denote the Hopkins statistic computed using Phi Coefficient correlational distances. Note that, Step 6 in Algorithm Sample uniformly m points (T 1 , ...., T m ) from given dataset D.\n\n\n3:\n\nfor all T i \u2208 D do 4:\nd i \u2190 dist(T i , T j ),\nwhere T j is the nearest neighbor Compute the distance between T i and T j using Jaccard or Phi Coefficient distances 5: end for 6: Generate a simulated dataset D R from a random Bernoulli distribution with m points (R 1 , ...., R m ) with the same variance as the given dataset D. 7:\nfor all R i \u2208 D R do 8:d i \u2190 dist(R i , R j ),\nwhere R j is the nearest neighbor Compute the distance between R i and R j using Jaccard or Phi Coefficient distances 9: end for 10: Calculate the Hopkins statistic as On the other hand, if clusters are not present in D, then H \u2248 0, and we conclude that the data points in D are considered to be regularly spaced, neither random nor clustered.\nH = m i=1d i m i=1 d i + m i=1d i .(4)\nWe computed the Hopkins statistic for both datasets. For the first dataset of APT attacks, H J = 0.51 and H \u03c6 = 0.60. For the second dataset of Software attacks, we report H J = 0.55 and H \u03c6 = 0.63. For both of the datasets, we found the reported value of H \u03c6 to be higher than H J , indicating that using the Phi Coefficient correlational distance results in the datasets having a better clustering tendency. For this reason, all our reported results will be expressed in Phi Coefficient correlational distances.\n\n\nC. Hierarchical Clustering\n\nHierarchical clustering is a different method of clustering in which a specified distance matrix is used as the criteria to create a tree-based representation of the data. The treelike structure resulting from hierarchical clustering is known as a dendrogram, a multilevel hierarchy of the objects being clustered. The number of clusters K need not be specified in advance, rather cutting the tree at a specified height after clustering allows for generating clusters depending on any desired constraints.\n\nAlthough there are many hierarchical clustering algorithms, we developed a hierarchical clustering method specifically tailored for the datasets considered here. Our approach extends agglomerative hierarchical clustering with Ward's linkage. Agglomerative hierarchical clustering refers to a bottom-up method, where every object in the dataset begins as a separate cluster, or leaf, in the tree. The leaves are then combined into bigger clusters based on the values of the distance matrix. Agglomerative clustering is best for finding clusters of objects with the greatest similarity as it focuses on complete local information from the dataset during fusion decisions, and, therefore, creating smaller clusters where the objects are the most similar [13]. During the process of agglomerative clustering, combining multiple larger groups of leaves requires a linkage method. Our linkage method for inferring technique associations uses Ward's linkage method. Ward's linkage calculates the distance between larger clusters by computing the sum of squares of the distances, divided by the product of the cardinality of the two clusters. In addition to Ward's linkage creating more compact clusters during hierarchical clustering, Ward's is less susceptible to noise and outliers within the clustering data [14].\n\nOther types of hierarchical clustering and linkage methods, such as divisive hierarchical clustering and single and complete linkage, do not possess the same qualities of compactness and robustness as our approach of agglomerative hierarchical clustering with Ward's linkage. Divisive hierarchical clustering refers to a top-down approach at which the objects in the dataset are recursively split into clusters until each object becomes a singleton. During each iteration of division, the split decision is made by comparing the dissimilarity of the objects to one another [13]. Divisive clustering does not conform with the objective of inferring technique associations that represent correlations between techniques that are at the highest degrees of similarity in attack instances. Similarly, we deemed other wellknown linkage techniques such as single linkage and complete linkage as not suitable for our goal. Single-linkage, otherwise known as minimum linkage, creates long-loose clusters, and often cannot separate clusters in the hierarchy in the presence of noise. Complete linkage possesses the tendency to unseeingly break larger clusters, which may does not contribute to our goal of maintaining the relationships between associations [15]. For those reasons, our inference of the technique associations used agglomerative hierarchical clustering with Ward's linkage. The results of divisive hierarchical clustering for the Software and APT attack datasets are reported in Figures 13 and 14 in the Appendix. In addition, a representation of the hierarchical clustering using different linkage methods is portrayed in Figure  15.\n\nWe extended the agglomerative hierarchical Ward's linkage algorithm to incorporate the Phi coefficient correlational distance metric for the APT and Software attacks datasets. Performing this hierarchical clustering will grant the ability of inferring meaningful and accurate technique associations Algorithm 2 Statistical Test of Hierarchical Clustering 1: procedure TEST(T D ) 2: for possible k \u2190 1 to 100 do 3: h k \u2190 cutoff height of T D that creates k clusters 4: for j \u2190 1 to 1,000 do 5:\n\nGenerate a null tree, T 0 j , using agglomerative Ward's linkage from a random Bernoulli distribution with the same variance as that of T D\n\n\n6:\n\nh 0 k,j \u2190 cutoff height for T 0 j that creates k clusters 7: end for 8: p \u2190 count h 0 k,j \u2264 h k / 1,000 9: end for 10:\n\nFind the first value stored in p that is \u2264 0.05 to conclude statistical significance at a 95% confidence level. 11: end procedure by encompassing clusters that have the highest degrees of correlation.\n\nAlthough we now have the final dendrogram of the learned hierarchical clustering tree, we are still not able to infer the significant technique associations because we need to determine a cutoff in the tree. Our unique approach to assess the validity and derive the statistically significant cutoff for the technique associations is discussed in the next section.\n\n\nD. Statistical Hypothesis Testing of Hierarchical Clustering\n\nAfter developing the hierarchical clustering tree, the final steps are, first, to assess the validity of the tree and, second, create a cutoff at a height of the dendrogram in order to create the final clusters. For the datasets, the final clusters learned from the hierarchical clustering tree will represent the technique associations. The novel approach used in this work addresses these two aspects by performing a statistical hypothesis test on the learned agglomerative Ward's linkage hierarchical clustering tree, which will be denoted as T D . Specifically, a statistical hypothesis test will analyze the validity of the clusters in T D by comparing the learned tree to a tree resulted from a null distribution, and will allow us to infer statistically significant results at the desired confidence level [16].\n\nThe null tree used for the statistical hypothesis test is generated from a random Bernoulli distribution with the same variance as that of the dataset, and will be denoted as T 0 . If the clusters in T D are considerably different than that of T 0 at the specified cutoff, then the learned tree will yield statistically significant results. An outcome of a statistically significant hierarchical tree grants the conclusion that the resulting associations are far from random chance, providing validity to the technique associations. The approach is further explained in Algorithm 2.\n\n\nE. Experimentation and results\n\nFirst, we determined the cutoff value of the agglomerative Ward's clustering tree for the dataset of APT attacks and Software attacks at a rigorous 95% confidence level. The resulted p-values are shown in Figure 6. The blue vertical line  Figures 6a and 6b correspond to the statistically significant amount of associations, 37 and 61 associations for the APT and Software attacks, respectively. The results of the learned hierarchical clustering tree for the dataset of APT attacks is portrayed in Figure 7 and for the dataset of Software attacks, Figure 8. Each cluster, or fine-grain association, in the figure is surrounded by a gray dashed box, and is represented in a different color in the tree.\n\n\nV. EVALUATION\n\nIn addition to the statistical hypothesis test providing validity to the learned hierarchical clustering presented in Section IV-D, in this section we will present two methods to evaluate the accuracy of the learned technique associations.\n\n\nA. Measuring Mutual Information\n\nWe evaluated the learned attack technique associations using an information theoretic approach. We computed the mutual information of the techniques in the fine-grain clusters, as well as the coarse-grain clusters directly from the datasets. Mutual information allows us to evaluate the learned correlations by identifying the relatedness or dependence between any two techniques within the same cluster (fine-grain) or across joint clusters (coarse-grain) independent of the assumptions of the underlying probability distributions [17]. In order to be able to compare the mutual information between different associations, we performed our evaluation using the normalized mutual information (NMI) [18].\n\nFirst, we performed our analysis of the fine-grain associations for both datasets. This involved a technique-based NMI measure as well as a cluster-based NMI measure. We computed the technique-based measure by calculating the NMI for every pairwise combination of techniques in the same cluster. This quantitatively represents the maximum predictability each technique possesses based on its cluster assignment. As for the cluster-based measure, we computed the NMI for each technique in the cluster and found the average, yielding a measure of how predictable that cluster is. We identified the threshold of the NMI value for the fine-grain analysis by empirically assessing the technique occurrences in the datasets. We found that an NMI value of 0.25 is equivalent to having a co-occurrence of 75%, which shows that the learned correlation Moreover, we evaluated the coarse-grain associations by computing the inter-cluster NMI for both datasets. We measured the inter-cluster NMI by finding the neighboring connecting cluster in the next level of the hierarchical clustering tree and computing the NMI between the techniques in the connected clusters. The complement cumulative distribution function of the coarse-grain analysis is shown for both datasets in Figure   ( Software attacks (Figure 11b) indicate high predictability of their connected cluster at the next level of the hierarchy.\n\nIn Section VI, we will discuss examples of fine-grain and coarse-grain associations, as well as the relationships they manifest in further detail.\n\n\nB. Evaluation Based on Domain Experts\n\nWe also recruited 6 Cybersecurity experts from both academic institutions and government who have at least 5 years of experience in the area of cyber threat intelligence and are familiar with the MITRE ATT&CK Framework. We provided each expert with an evaluation rubric to label each association as \"agree\", \"disagree\", or \"neutral\" along with the corresponding justifications. The experts confirmed the presence of strong correlations for 93% of the fine-grain technique associations, and 90% of the coarse-grain associations. In addition, when investigating the fine-grain and coarse-grain associations that were labeled as \"disagree\" or \"neutral\", we found those associations difficult to justify since the techniques in those associations have low occurrences in the datasets.\n\n\nVI. DISCUSSION\n\nIn this section we will discuss some findings and association examples based on our clustering analysis.\n\nHierarchical clustering reflects the complexities of APT and Software attacks: The average size of the fine-grain associations of the Software attacks is smaller than that of the APT attacks. This is because the Software attacks typically extend a smaller number of tactics, while APT attacks are typically more complex and span several tactics in the attack chain. Fine-grain associations: There are 37 and 61 fine-grain clusters for APT and Software attacks, respectively. These intracluster correlations represent various relationships (conjunctive, disjunctive or sequential) between techniques within the same cluster. Although the clustering itself may not reveal the kind of relationship between techniques, this can be identified by expert inspection based on techniques functions. Will will discuss two examples of this.\n\nIn the first example, the cluster composed of {T1013 (Port Monitors), T1494 (Runtime Data Manipulation), T1493 (Transmitted Data Manipulation), T1115 (Clipboard Data), T1485 (Data Destruction), T1486 (Data Encrypted for Impact), and T1487 (Disk Structure Wipe)} represents an attack pattern for accomplishing data destruction as an adversarial goal. The adversary starts with T1013 to load malicious code at startup through a DLL. Then, the adversary may choose to perform T1494 or T1493 to manipulate runtime or transmitted data to negatively affect a business process. Afterwards, the adversary may collect different sources of data, such as T1115 that can later impact data availability. Finally, the adversary performs one or more of 1485, T1486, or T1487 to cause data unavailability. Attackers usually plan for multiple techniques (from the tactic) that are performed concurrently (cognitively) or selectively (as alternatives) in order to maximize the potential of success. Figure 12 shows the relationship manifested in this association.\n\nAs a second example, the {T1028 (Windows Remote Management), T1038 (DLL Search Order Hijacking), T1030 (Data Transfer Limits), and T1126 (Network Share Connection Removal)} association represents an attack pattern in the TTP Chain of an information stealer (e.g., Threat Group 3390 [1]). First, the adversary executes malicious code leveraging the Windows Remote management protocol as enabled by technique T1028. As a result, the adversary can then replace or modify a DLL to gain privilege escalation and persistence using (T1038). Afterwards, the adversary will employ technique T1030 to exfiltrate collected data in fixed size chunks to avoid data transfer alerts. Finally, the adversary cleans up the traces of their operation by detaching the network shares, (T1126), after exfiltration for defense evasion. Evidently, the relationship in this association is sequential.\n\nCoarse-grain associations: The fine-grain associations are combined together at various levels to form bigger clusters in the hierarchy. We call this inter-cluster association coarse-grain. There are 35 first-level coarse-grains associations in the APT attack hierarchical clustering tree. As we observed, the coarsegrain associations show complementary techniques performed across various tactics in the TTP Chain. For instance, the second example association discussed above ({T1028 (Windows Remote Management), T1038 (DLL Search Order Hijacking), T1030 (Data Transfer Limits), and T1126 (Network Share Connection Removal)}) is connected to another association {T1055 (Process Injection) and T1089 (Disabling Security Tools)} at the next level of the hierarchy. An attacker may use the latter cluster to obtain defense evasion during the former cluster. That is, T1055 for executing code masked under a legitimate process and T1089 for disabling security tools are both used to avoid detection. For an APT, this complementary relationship is critical to their goal, and the coarse-grain association aids in identifying the larger attack chain.\n\nInterestingly, many of the associations inferred by our approach, as examples discussed above, may not be easily deduced by experts in the field. The fine-grain and coarse-gran associations go beyond what experts can manually discover. Thus, the knowledge from this work can be used to further advance the ability to predict adversarial behavior outside the limits of expertise or heuristics.\n\n\nVII. CONCLUSION AND FUTURE WORK\n\nOverall, in this work we accurately learn the fine-grain and coarse-grain technique associations based on real-life APT and Software attack datasets. These associations are important because they enable the prediction of adversarial behavior based on observed techniques, which can be directly applied to attack diagnosis and threat mitigation. Our approach first involves establishing the suitable distance metric of the datasets and extending hierarchical clustering algorithms in order to infer explainable fine-grain and coarse-grain associations. Moreover, we deduce statistically significant correlations and assess the validity of our learned hierarchical clustering tree by performing a hypothesis test and ensure that no more than 5% of the fine-grain associations are due to random chance. Our approach leads to the inference of 37 fine-grain technique associations from the APT attacks dataset and 61 from the Software attacks dataset. Then, we evaluate the learned fine-grain and coarsegrain associations using mutual information, and the results show that 78% of the fine-grain and 75% of the coarsegrain technique associations exhibit high predictability for APT attacks.\n\nOur approach encountered a few limitations that provide avenues for future work. First, we will work on collecting a larger dataset of real-life attacks in order to increase the number and the quality of technique clusters and associations. Second, we will investigate an approach, involving fuzzy hierarchical clustering, to explore probabilistic technique associations as well as other methods methods to reveal precondition and postcondition relationships. \n\nFig. 1 :\n1K\n\nFig. 2 :\n2K for Software attacks maximized. For that reason, K is chosen as the point with the highest silhouette width.The Elbow and Silhouette methods are portrayed for both datasets inFigures 1 and 2.\n\nFig. 4 :\n4Partitioned Clustering Software attacks\n\nFig. 5 :\n5Clusters Silhouette Plot for PAM Clustering\n\n\ngenerating D R using a Bernoulli distribution. This agrees with the nature of the binary variables considered here.To further understand H, assume D was uniformly distributed and lacking cluster tendencies. Then, the values ofm i=1d i and m i=1 d i in equation 4 would expectedly be close to one another, making H = 0.5. Therefore, if H \u2248 0.5, we conclude that the dataset D is uniformly distributed and does not contain any meaningful clusters. However, if clusters are present in D, then we anticipate H \u2248 1, and conclude that performing a cluster analysis would yield meaningful results.\n\n\nFig. 6: Hypothesis Test of Learned Hierarchical Clustering\n\nFig. 7 :\n7Agglomerative Hierarchical Clustering of APT Attacks Fig. 8: Agglomerative Hierarchical Clustering of Software Attacks is practically useful for prediction. The complement cumulative distribution function of the fine-grain analysis is shown for both datasets in Figures 9 and 10. The results of the finegrain analysis show that from the dataset of APT attacks, 78% of the techniques (Figure 9a) and 75% of the clusters (Figure 9b) indicate high predictability. Similarly for Software attacks, 60% of the techniques and clusters (Figure 10a and Figure 10b) from the learned hierarchical clustering tree indicate high predictability.\n\nFig. 9 :Fig. 10 :Fig. 11 :\n91011Analysis of Fine-grain Associations for APT Attacks 11. The results of the coarse-grain analysis show that, 75% of the inter-cluster cluster correlations for the APT attacks (Figure 11a) and 60% of the inter-cluster correlations for the (a) Technique-based NMI Measure (b) Cluster-based NMI Measure Analysis Analysis of Coarse-Grain Association\n\nFig. 12 :\n12Fine-grain Association Example\n\nFig. 13 :\n13Divisive Hierarchical Clustering of APT Attacks Fig. 14: Divisive Hierarchical Clustering of Software Attacks Fig. 15: Agglomerative Hierarchical Clustering Linkage Methods of APT Attacks\n\n\nfirst dataset contains 66 APT attack instances as published in the MITRE ATT&CK Framework [1] on June 30, 2019. These APT attacks were mapped by MITRE from publicly reported technique use, where the original references are included in each technique description. The second dataset contains 204 Software attack instances as published in the MITRE ATT&CK Framework [1] on July 30, 2019. MITRE also mapped Software attacks from publicly reported technique use and accounts for the capability of the software adversary to use a technique. We downloaded the datasets from MITRE ATT&CK in json format using the ATT&CK Navigator [8].\n\nTABLE I :\nIDistance Metrics Empirical Values\nACKNOWLEDGEMENTSWe would like to thank Qi Duan and Mohiuddin Ahmed for their manual assessment of the technique associations based on their expertise in the field, Vyas Sekar for providing insightful feedback on our clustering approach, and Peter Steenkiste for his valuable comments during the review of the manuscript.Copyright 2020 Carnegie Mellon University, Eliana Christou and Rawan Al-Shaer. This material is based upon work funded and supported by the Department of Defense under Contract No. FA8702-15-D-0002 with Carnegie Mellon University for the operation of the Software Engineering Institute, a federally funded research and development center. References herein to any specific commercial product, process, or service by trade name, trade mark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by Carnegie Mellon University or its Software Engineering Institute.[\nAvailable: https : //attack.mitre.org (visited on. Mitre, ATT&CKMITRE. (2019). ATT&CK, [Online]. Available: https : //attack.mitre.org (visited on Jul. 30, 2019).\n\nReview of human decision-making during incident analysis. J M Spring, P Illari, abs/1903.10080CoRR. J. M. Spring and P. Illari, \"Review of human decision-making during incident analysis,\" CoRR, vol. abs/1903.10080, 2019. [Online]. Available: http://arxiv. org/abs/1903.10080.\n\nSome methods for classifica-tion and analysis of multivariate observations. J Mcqueen, Proceedings of the Fifth Symposium on Math, Statistics, and Probability. the Fifth Symposium on Math, Statistics, and ProbabilityJ. McQueen, \"Some methods for classifica-tion and analysis of multivariate observations,\" Proceedings of the Fifth Symposium on Math, Statistics, and Probability, pp. 281-297, 1967.\n\nL Kaufman, P Rousseeuw, Partitioning around medoids. program pamL. Kaufman and P. Rousseeuw, Partitioning around medoids (program pam). Mar. 1990.\n\nFuzzy sets. L Zadeh, Information and Control 8.3, 338353L. Zadeh, \"Fuzzy sets,\" Information and Control 8.3, 338353, 1965.\n\nDistribution de la flore alpine dans le bassin des dranses et dans quelques regions voisines. P Jaccard, Bulletin de la Socit Vaudoise des Sciences Naturelles. 241272P. Jaccard, \"Distribution de la flore alpine dans le bassin des dranses et dans quelques regions voisines,\" Bulletin de la Socit Vaudoise des Sciences Naturelles, 241272, 1901.\n\nH Cramer, Mathematical methods of statistics. Princeton. Princeton University Press282H. Cramer, Mathematical methods of statistics. Prince- ton: Princeton University Press, 1946, p. 282.\n\nATT&CK navigator. Mitre, MITRE. (2019). ATT&CK navigator, [Online]. Avail- able: https : / / mitre -attack . github. io / attack -navigator / enterprise/ (visited on Jul. 30, 2019).\n\nAnnual Review of Statistics and Its Application. V Stodden, Reproducing statistical resultsV. Stodden, \"Reproducing statistical results,\" Annual Review of Statistics and Its Application, pp. 1-19, 2015.\n\nPractical guide to cluster analysis in R: Unsupervised machine learning. A Kassambara, 1A. Kassambara, Practical guide to cluster analysis in R: Unsupervised machine learning. 2017, vol. 1.\n\nClustering binary data with k-means (should be avoided). (Jun. 2018). Clustering binary data with k-means (should be avoided), [Online]. Available: https://www.ibm.com/ support/pages/clustering-binary-data-k-means-should- be-avoided.\n\nA new method for determining the type of distribution of plant individuals. B Hopkins, J G Skellam, Annals of Botany. 18213227B. Hopkins and J. G. Skellam, \"A new method for determining the type of distribution of plant individuals,\" Annals of Botany 18.2, 213227, 1954.\n\nIntroduction to information retrieval. C D Manning, P Raghavan, H Schutze, Cambridge University PressUSAC. D. Manning, P. Raghavan, and H. Schutze, Introduc- tion to information retrieval. USA: Cambridge Univer- sity Press, 2008.\n\nWards hierarchical agglomerative clustering method: Which algorithms implement wards criterion?. F Murtagh, P Legendre, Journal of Classification. 313F. Murtagh and P. Legendre, \"Wards hierarchical ag- glomerative clustering method: Which algorithms imple- ment wards criterion?\" Journal of Classification, vol. 31, no. 3, 274295, Oct. 2014.\n\nClustering analysis. (). Clustering analysis, [Online]. Available: https : / / cs . wmich . edu / alfuqaha / summer14 / cs6530 / lectures / ClusteringAnalysis.pdf (visited on 2018).\n\nMultivariate analysis of ecological data. M Greenacre, R Primicerio, 4M. Greenacre and R. Primicerio, Multivariate analysis of ecological data. Jan. 2014, vol. 4.\n\nA mathematical theory of communication. C E Shannon, The Bell System Technical Journal. 274C. E. Shannon, \"A mathematical theory of communica- tion,\" The Bell System Technical Journal, vol. 27, no. 4, pp. 623-656, 1948.\n\nInformation theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. N X Vinh, J Epps, J Bailey, Journal of Machine Learning Research. 11APPENDIX Figure 13 and Figure 14 display divisive clustering results. Figure 15 displays agglomerative clustering resultsN. X. Vinh, J. Epps, and J. Bailey, \"Information theoretic measures for clusterings comparison: Variants, proper- ties, normalization and correction for chance,\" Journal of Machine Learning Research, vol. 11, 28372854, Dec. 2010. APPENDIX Figure 13 and Figure 14 display divisive clustering results. Figure 15 displays agglomerative clustering results.\n", "annotations": {"author": "[{\"end\":178,\"start\":68},{\"end\":313,\"start\":179},{\"end\":394,\"start\":314}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":74},{\"end\":196,\"start\":190},{\"end\":329,\"start\":321}]", "author_first_name": "[{\"end\":73,\"start\":68},{\"end\":187,\"start\":179},{\"end\":189,\"start\":188},{\"end\":320,\"start\":314}]", "author_affiliation": "[{\"end\":177,\"start\":84},{\"end\":312,\"start\":218},{\"end\":393,\"start\":331}]", "title": "[{\"end\":65,\"start\":1},{\"end\":459,\"start\":395}]", "venue": null, "abstract": "[{\"end\":1711,\"start\":461}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3024,\"start\":3021},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3793,\"start\":3790},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6208,\"start\":6205},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6244,\"start\":6241},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6270,\"start\":6267},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7060,\"start\":7057},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7093,\"start\":7090},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10960,\"start\":10957},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12092,\"start\":12089},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12101,\"start\":12098},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12116,\"start\":12113},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14761,\"start\":14757},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15767,\"start\":15763},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16360,\"start\":16356},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20231,\"start\":20228},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21546,\"start\":21543},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21580,\"start\":21577},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24543,\"start\":24539},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24808,\"start\":24804},{\"end\":25643,\"start\":25641},{\"end\":25654,\"start\":25652},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25806,\"start\":25805},{\"end\":25975,\"start\":25973},{\"end\":25987,\"start\":25984},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28044,\"start\":28040},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28597,\"start\":28593},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29177,\"start\":29173},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29851,\"start\":29847},{\"end\":30623,\"start\":30621},{\"end\":30655,\"start\":30653},{\"end\":30709,\"start\":30707},{\"end\":30942,\"start\":30940},{\"end\":30953,\"start\":30951},{\"end\":30988,\"start\":30986},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32449,\"start\":32445},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34600,\"start\":34596},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34766,\"start\":34762},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39422,\"start\":39419}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43250,\"start\":43238},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43455,\"start\":43251},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43506,\"start\":43456},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43561,\"start\":43507},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44154,\"start\":43562},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44215,\"start\":44155},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44858,\"start\":44216},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45236,\"start\":44859},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45280,\"start\":45237},{\"attributes\":{\"id\":\"fig_10\"},\"end\":45481,\"start\":45281},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46111,\"start\":45482},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46157,\"start\":46112}]", "paragraph": "[{\"end\":2397,\"start\":1730},{\"end\":3794,\"start\":2399},{\"end\":5218,\"start\":3796},{\"end\":5912,\"start\":5220},{\"end\":6739,\"start\":5914},{\"end\":8322,\"start\":6741},{\"end\":8614,\"start\":8339},{\"end\":9010,\"start\":8633},{\"end\":9629,\"start\":9012},{\"end\":9635,\"start\":9631},{\"end\":9891,\"start\":9653},{\"end\":10761,\"start\":9893},{\"end\":11015,\"start\":10763},{\"end\":11522,\"start\":11034},{\"end\":11859,\"start\":11524},{\"end\":12238,\"start\":11889},{\"end\":12675,\"start\":12282},{\"end\":13106,\"start\":12677},{\"end\":13651,\"start\":13108},{\"end\":14359,\"start\":13653},{\"end\":14506,\"start\":14361},{\"end\":14886,\"start\":14508},{\"end\":15182,\"start\":14941},{\"end\":16568,\"start\":15184},{\"end\":17270,\"start\":16570},{\"end\":18192,\"start\":17272},{\"end\":18689,\"start\":18219},{\"end\":19443,\"start\":18691},{\"end\":20613,\"start\":19445},{\"end\":21177,\"start\":20661},{\"end\":21630,\"start\":21201},{\"end\":21715,\"start\":21632},{\"end\":22561,\"start\":21764},{\"end\":22811,\"start\":22563},{\"end\":22859,\"start\":22813},{\"end\":23582,\"start\":22946},{\"end\":24025,\"start\":23584},{\"end\":24739,\"start\":24062},{\"end\":25470,\"start\":24741},{\"end\":25498,\"start\":25477},{\"end\":25807,\"start\":25523},{\"end\":26198,\"start\":25855},{\"end\":26751,\"start\":26238},{\"end\":27287,\"start\":26782},{\"end\":28598,\"start\":27289},{\"end\":30240,\"start\":28600},{\"end\":30734,\"start\":30242},{\"end\":30875,\"start\":30736},{\"end\":31000,\"start\":30882},{\"end\":31202,\"start\":31002},{\"end\":31567,\"start\":31204},{\"end\":32450,\"start\":31632},{\"end\":33034,\"start\":32452},{\"end\":33771,\"start\":33069},{\"end\":34028,\"start\":33789},{\"end\":34767,\"start\":34064},{\"end\":36164,\"start\":34769},{\"end\":36312,\"start\":36166},{\"end\":37134,\"start\":36354},{\"end\":37257,\"start\":37153},{\"end\":38088,\"start\":37259},{\"end\":39135,\"start\":38090},{\"end\":40013,\"start\":39137},{\"end\":41160,\"start\":40015},{\"end\":41554,\"start\":41162},{\"end\":42775,\"start\":41590},{\"end\":43237,\"start\":42777}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14940,\"start\":14887},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21763,\"start\":21716},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22945,\"start\":22860},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25522,\"start\":25499},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25854,\"start\":25808},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26237,\"start\":26199}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21951,\"start\":21944},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22857,\"start\":22850}]", "section_header": "[{\"end\":1728,\"start\":1713},{\"end\":8337,\"start\":8325},{\"end\":8631,\"start\":8617},{\"end\":9651,\"start\":9638},{\"end\":11032,\"start\":11018},{\"end\":11887,\"start\":11862},{\"end\":12280,\"start\":12241},{\"end\":18217,\"start\":18195},{\"end\":20659,\"start\":20616},{\"end\":21199,\"start\":21180},{\"end\":24060,\"start\":24028},{\"end\":25475,\"start\":25473},{\"end\":26780,\"start\":26754},{\"end\":30880,\"start\":30878},{\"end\":31630,\"start\":31570},{\"end\":33067,\"start\":33037},{\"end\":33787,\"start\":33774},{\"end\":34062,\"start\":34031},{\"end\":36352,\"start\":36315},{\"end\":37151,\"start\":37137},{\"end\":41588,\"start\":41557},{\"end\":43247,\"start\":43239},{\"end\":43260,\"start\":43252},{\"end\":43465,\"start\":43457},{\"end\":43516,\"start\":43508},{\"end\":44225,\"start\":44217},{\"end\":44886,\"start\":44860},{\"end\":45247,\"start\":45238},{\"end\":45291,\"start\":45282},{\"end\":46122,\"start\":46113}]", "table": null, "figure_caption": "[{\"end\":43250,\"start\":43249},{\"end\":43455,\"start\":43262},{\"end\":43506,\"start\":43467},{\"end\":43561,\"start\":43518},{\"end\":44154,\"start\":43564},{\"end\":44215,\"start\":44157},{\"end\":44858,\"start\":44227},{\"end\":45236,\"start\":44892},{\"end\":45280,\"start\":45250},{\"end\":45481,\"start\":45294},{\"end\":46111,\"start\":45484},{\"end\":46157,\"start\":46124}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18529,\"start\":18513},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18741,\"start\":18726},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19206,\"start\":19198},{\"end\":19966,\"start\":19957},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30102,\"start\":30085},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30239,\"start\":30229},{\"end\":33282,\"start\":33274},{\"end\":33325,\"start\":33308},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33576,\"start\":33568},{\"end\":33626,\"start\":33618},{\"end\":36042,\"start\":36032},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36071,\"start\":36060},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39080,\"start\":39071}]", "bib_author_first_name": "[{\"end\":47325,\"start\":47324},{\"end\":47327,\"start\":47326},{\"end\":47337,\"start\":47336},{\"end\":47620,\"start\":47619},{\"end\":47943,\"start\":47942},{\"end\":47954,\"start\":47953},{\"end\":48103,\"start\":48102},{\"end\":48309,\"start\":48308},{\"end\":48559,\"start\":48558},{\"end\":48980,\"start\":48979},{\"end\":49208,\"start\":49207},{\"end\":49637,\"start\":49636},{\"end\":49648,\"start\":49647},{\"end\":49650,\"start\":49649},{\"end\":49872,\"start\":49871},{\"end\":49874,\"start\":49873},{\"end\":49885,\"start\":49884},{\"end\":49897,\"start\":49896},{\"end\":50161,\"start\":50160},{\"end\":50172,\"start\":50171},{\"end\":50632,\"start\":50631},{\"end\":50645,\"start\":50644},{\"end\":50794,\"start\":50793},{\"end\":50796,\"start\":50795},{\"end\":51097,\"start\":51096},{\"end\":51099,\"start\":51098},{\"end\":51107,\"start\":51106},{\"end\":51115,\"start\":51114}]", "bib_author_last_name": "[{\"end\":47158,\"start\":47153},{\"end\":47334,\"start\":47328},{\"end\":47344,\"start\":47338},{\"end\":47628,\"start\":47621},{\"end\":47951,\"start\":47944},{\"end\":47964,\"start\":47955},{\"end\":48109,\"start\":48104},{\"end\":48317,\"start\":48310},{\"end\":48566,\"start\":48560},{\"end\":48770,\"start\":48765},{\"end\":48988,\"start\":48981},{\"end\":49219,\"start\":49209},{\"end\":49645,\"start\":49638},{\"end\":49658,\"start\":49651},{\"end\":49882,\"start\":49875},{\"end\":49894,\"start\":49886},{\"end\":49905,\"start\":49898},{\"end\":50169,\"start\":50162},{\"end\":50181,\"start\":50173},{\"end\":50642,\"start\":50633},{\"end\":50656,\"start\":50646},{\"end\":50804,\"start\":50797},{\"end\":51104,\"start\":51100},{\"end\":51112,\"start\":51108},{\"end\":51122,\"start\":51116}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":47264,\"start\":47102},{\"attributes\":{\"doi\":\"abs/1903.10080\",\"id\":\"b1\"},\"end\":47541,\"start\":47266},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6278891},\"end\":47940,\"start\":47543},{\"attributes\":{\"id\":\"b3\"},\"end\":48088,\"start\":47942},{\"attributes\":{\"id\":\"b4\"},\"end\":48212,\"start\":48090},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":222435010},\"end\":48556,\"start\":48214},{\"attributes\":{\"id\":\"b6\"},\"end\":48745,\"start\":48558},{\"attributes\":{\"id\":\"b7\"},\"end\":48928,\"start\":48747},{\"attributes\":{\"id\":\"b8\"},\"end\":49132,\"start\":48930},{\"attributes\":{\"id\":\"b9\"},\"end\":49323,\"start\":49134},{\"attributes\":{\"id\":\"b10\"},\"end\":49558,\"start\":49325},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":89564582},\"end\":49830,\"start\":49560},{\"attributes\":{\"id\":\"b12\"},\"end\":50061,\"start\":49832},{\"attributes\":{\"id\":\"b13\"},\"end\":50404,\"start\":50063},{\"attributes\":{\"id\":\"b14\"},\"end\":50587,\"start\":50406},{\"attributes\":{\"id\":\"b15\"},\"end\":50751,\"start\":50589},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5747983},\"end\":50972,\"start\":50753},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13962547},\"end\":51637,\"start\":50974}]", "bib_title": "[{\"end\":47322,\"start\":47266},{\"end\":47617,\"start\":47543},{\"end\":48306,\"start\":48214},{\"end\":49634,\"start\":49560},{\"end\":50158,\"start\":50063},{\"end\":50791,\"start\":50753},{\"end\":51094,\"start\":50974}]", "bib_author": "[{\"end\":47160,\"start\":47153},{\"end\":47336,\"start\":47324},{\"end\":47346,\"start\":47336},{\"end\":47630,\"start\":47619},{\"end\":47953,\"start\":47942},{\"end\":47966,\"start\":47953},{\"end\":48111,\"start\":48102},{\"end\":48319,\"start\":48308},{\"end\":48568,\"start\":48558},{\"end\":48772,\"start\":48765},{\"end\":48990,\"start\":48979},{\"end\":49221,\"start\":49207},{\"end\":49647,\"start\":49636},{\"end\":49660,\"start\":49647},{\"end\":49884,\"start\":49871},{\"end\":49896,\"start\":49884},{\"end\":49907,\"start\":49896},{\"end\":50171,\"start\":50160},{\"end\":50183,\"start\":50171},{\"end\":50644,\"start\":50631},{\"end\":50658,\"start\":50644},{\"end\":50806,\"start\":50793},{\"end\":51106,\"start\":51096},{\"end\":51114,\"start\":51106},{\"end\":51124,\"start\":51114}]", "bib_venue": "[{\"end\":47151,\"start\":47102},{\"end\":47364,\"start\":47360},{\"end\":47701,\"start\":47630},{\"end\":47993,\"start\":47966},{\"end\":48100,\"start\":48090},{\"end\":48372,\"start\":48319},{\"end\":48613,\"start\":48568},{\"end\":48763,\"start\":48747},{\"end\":48977,\"start\":48930},{\"end\":49205,\"start\":49134},{\"end\":49380,\"start\":49325},{\"end\":49676,\"start\":49660},{\"end\":49869,\"start\":49832},{\"end\":50208,\"start\":50183},{\"end\":50425,\"start\":50406},{\"end\":50629,\"start\":50589},{\"end\":50839,\"start\":50806},{\"end\":51160,\"start\":51124},{\"end\":47759,\"start\":47703}]"}}}, "year": 2023, "month": 12, "day": 17}