{"id": 220730141, "updated": "2022-03-26 11:04:23.841", "metadata": {"title": "Dual Sequential Network for Temporal Sets Prediction", "authors": "[{\"first\":\"Leilei\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Yansong\",\"last\":\"Bai\",\"middle\":[]},{\"first\":\"Bowen\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Chuanren\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Weifeng\",\"last\":\"Lv\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Many sequential behaviors such as purchasing items from time to time, selecting courses in different terms, collecting event logs periodically could be formalized as sequential sets of actions or elements, namely temporal sets. Predicting the subsequent set according to historical sequence of sets could help us make better producing, scheduling, or operating decisions. However, most of the existing methods were designed for predicting time series or temporal events, which could not be directly used for temporal sets prediction due to the difficulties of multi-level representations of items and sets, complex temporal dependencies of sets, and evolving dynamics of sequential behaviors. To address these issues, this paper provides a novel sets prediction method, called DSNTSP (Dual Sequential Network for Temporal Sets Prediction). Our model first learns both item-level representations and set-level representations of set sequences separately based on a transformer framework. Then, a co-transformer module is proposed to capture the multiple temporal dependencies of items and sets. Last, a gated neural module is designed to predict the subsequent set by fusing all the multi-level correlations and multiple temporal dependencies of items and sets. The experimental results on real-world data sets show that our methods lead to significant and consistent improvements as compared to other methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3034548041", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/SunBDL0L20", "doi": "10.1145/3397271.3401124"}}, "content": {"source": {"pdf_hash": "a962782dbd529981d656bec6f819fd69a35f189c", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6d351bbe338f099ce10711ce6a363ebb6bb5546d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a962782dbd529981d656bec6f819fd69a35f189c.txt", "contents": "\nDual Sequential Network for Temporal Sets Prediction\nVirtual EventCopyright Virtual EventJuly 25-30, 2020. July 25-30, 2020\n\nLeilei Sun \nSKLSDE and BDBC Lab\nBeihang University\n100083BeijingChina\n\nYansong Bai \nSKLSDE and BDBC Lab\nBeihang University\n100083BeijingChina\n\nBowen Du \nSKLSDE and BDBC Lab\nBeihang University\n100083BeijingChina\n\nChuanren Liu \nDepartment of Business Analytics and Statistics\nUniversity of Tennessee\nKnoxvilleUSA\n\nHui Xiong 3hxiong@rutgers.edu \nDepartment of Management Science and Information Systems\nRutgers University\nNewarkUSA\n\nWeifeng Lv \nSKLSDE and BDBC Lab\nBeihang University\n100083BeijingChina\n\nLeilei Sun \nSKLSDE and BDBC Lab\nBeihang University\n100083BeijingChina\n\nYansong Bai \nSKLSDE and BDBC Lab\nBeihang University\n100083BeijingChina\n\nBowen Du \nSKLSDE and BDBC Lab\nBeihang University\n100083BeijingChina\n\nChuanren Liu \nDepartment of Business Analytics and Statistics\nUniversity of Tennessee\nKnoxvilleUSA\n\nHui Xiong \nDepartment of Management Science and Information Systems\nRutgers University\nNewarkUSA\n\nDual Sequential Network for Temporal Sets Prediction\n\nPro-ceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '20)\nNew York, NY, USA; ChinaVirtual Event1020July 25-30, 2020. July 25-30, 202010.1145/3397271.34011241 {leileisun,easonwhite96,dubowen, * Corresponding Author. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nMany sequential behaviors such as purchasing items from time to time, selecting courses in different terms, collecting event logs periodically could be formalized as sequential sets of actions or elements, namely temporal sets. Predicting the subsequent set according to historical sequence of sets could help us make better producing, scheduling, or operating decisions. However, most of the existing methods were designed for predicting time series or temporal events, which could not be directly used for temporal sets prediction due to the difficulties of multi-level representations of items and sets, complex temporal dependencies of sets, and evolving dynamics of sequential behaviors. To address these issues, this paper provides a novel sets prediction method, called DSNTSP (Dual Sequential Network for Temporal Sets Prediction). Our model first learns both item-level representations and set-level representations of set sequences separately based on a transformer framework. Then, a co-transformer module is proposed to capture the multiple temporal dependencies of items and sets. Last, a gated neural module is designed to predict the subsequent set by fusing all the multi-level correlations and multiple temporal dependencies of items and sets. The experimental results on real-world data sets show that our methods lead to significant and consistent improvements as compared to other methods.\n\nINTRODUCTION\n\nIn practice, many sequential behaviors could be formulated as temporal sets. For example, a customer purchases a basket of items at a visit of a retail store or online store, a student selects a set of courses in each semester, or a patient takes a combination of drugs every day. Different from previous temporal data such as time series or temporal events, temporal sets are a new type of temporal data that consists of sequential sets, where each set contains a number of actions or elements with the same timestamp, see Figure 1. Prediction of temporal sets could help us make better producing, scheduling, or operating decisions. However, most of the existing methods were designed for predicting time series or temporal events [3,15], which could not be directly used for temporal sets prediction. Recently, some methods for temporal sets prediction have been proposed. Yu et al. [36] proposed a dynamic recurrent model for next basket prediction, where a max-pooling operation was used to extract a representation of set according to the items it contains and a recurrent architecture was used to learn the temporal dependence of the sequential sets. Hu et al. [11] provided a set-to-set methods for temporal sets prediction, where average-pooling was used to get a representation of set by aggregating all the elements within it, and a set-based attention module was designed to predict the subsequent set. It can be known that these methods first adopt a pooling-based set embedding module to get representations of sets and then utilize a recurrent neural network to capture the temporal dependencies of sets.\n\nThough these methods provide valuable insights to solve the problem of temporal sets prediction, there are still a lot of problems needs to be investigated in a further step. First, most of the existing methods mainly focus on set-level sequence representation, while there are actually multiple semantic correlations among items, for instance, some items appear frequently within a same set, while the other items are always purchased in different sets, which means these items may have alternative or complementary relationships. It is no doubt that we could achieve a better representation of temporal sets by taking these item-item, item-set, and set-set correlations into account, and which will finally improve the set prediction accuracy.\n\nBut how to learn a multi-level representation of a sequence of sets is a challenging problem. Second, it is difficult to capture the temporal dependencies among sequential sets, some sets may reflect an individual's regular purchases or actions, they have periodic dependencies; while the other sets reflect an individual's successive purchases or actions, they are strongly affected by the most recent sets. In this case, how to find the most valuable information for further set prediction according to historical sequence of sets is also a challenging problem. Third, the predicted set may be a possible combination of all the available items or actions, which is affected by all the correlations and temporal dependencies mentioned above. So how to generate a set prediction result by fusing all the multilevel correlations and multiple temporal dependencies among items and sets is a difficult task either.\n\nTo address the above issues, this paper proposes a novel temporal sets prediction method, namely, Dual Sequential Network for Temporal Sets Prediction (DSNTSP). To capture the multi-level representations of set sequences, an item-level representation learning module is first proposed, which adopts a transformer-based framework to learn both intra-set and inter-set item correlations. Then, a set-level representation learning module is provided, which could turn a set with unfixed-size into a vector with fixed length. Simultaneously, which could achieve a semantic representation of a set by considering both contextual preceding and subsequent sets with multi-head attentions. Third, a co-transformer learning module is proposed to learn the multiple temporal dependence of items and sets, for instance, sequential associations from item to item, from set to item, from set to set, or from item to set. Last, a gated neural module is designed to fuse all the multi-level correlations and temporal dependencies into a fixed hidden state, and predicts the subsequent set according to the current state.\n\nIn summary, our contributions could be summarized as follows:\n\n\u2022 We provide a multi-level representation method for temporal sets, which could capture the multi-level correlations of items and sets. By considering these correlations, a semantic representation of set sequence could be achieved, and the set prediction accuracy could be improved consequently. \u2022 We propose a co-transformer module to capture the temporal dependencies of items and sets and present a gated neural module to ensemble the multi-level correlations and multiple temporal dependencies of items and sets. \u2022 We conduct experiments on four real-world data sets, which shows that our method outperforms all the baseline methods.\n\nThe rest of the paper is organized as follows. Section 2 surveys the related work. Section 3 presents the definitions and formalization. Section 4 introduces the proposed model. We present experiments in section 5 and conclude this research in section 6.\n\n\nRELATED WORK\n\nThis section reviews the existing literature related to our work.\n\nTemporal Sets Prediction. Recently, there has been a surge of interest in temporal sets prediction. Most of the existing methods use pooling method to represent sets and use recurrent based method to capture temporal dependencies. Yu et al. [36] proposed a dynamic recurrent model for next basket prediction, which uses max pooling operations to get representations of baskets and then feed the sequence of baskets into the Recurrent Neural Network (RNN) structure. Factorizing Personalized Markov Chains (FPMC) [25] proposed by Rendle et al. is a traditional method for next basket recommendation. Benson et al. [2] proposed the Correlated Repeated Unions (CRU) model to learn the repeat behaviors in the set sequences. Sets2Sets [11] is an encoder-decoder framework which takes sequential sets as input, uses the average pooling to encode the sets, and provides a repeated-element-specified elementelement interaction component to further improve the performance.\n\nSet Embedding. Existing set embedding methods for temporal sets prediction [11,36] only use some kind of pooling operations to get the representations of sets, which could not capture the rich semantic information among the items and sets. Vinyals et al. [31] designed a LSTM-based network with memories coupled to encode sets. Zaheer et al. [37] proposed a fundamental architecture called DeepSets based on permutation invariance and equivariance properties. Murphy et al. [22] proposed a general method for pooling operations on sets, namely Janossy Pooling, which expresses a permutation-invariant function as the average of a permutationsensitive function applied to all reorderings of the input sequence. Lee et al. [14] introduced the Set Transformer, an attention-based set-input neural network architecture. Skianis et al. [27] proposed a neural network architecture, namely RepSet, which learns the set representations by solving a series of network flow problems. Meng et al. [20] develop a hierarchical sequence-attention framework to learn inductive Set-of-Sets (SoS) embeddings that are invariant to set permutations.\n\nTemporal Dependence Learning. Recurrent Neural Network (RNN) [16] is a class of deep neural network used for sequential data, which has been applied in many tasks such as speech recognition [8], machine translation [1] and image caption [32]. Long Shortterm Memory (LSTM) [10] is a type of RNN which can help with the vanishing gradient problem. Cho et al. [5] proposed a slightly simplified version of LSTM called Gated Recurrent Units (GRU). Recently, RNNs equipped with different attention mechanism [1,18,21,34] have been proposed to improve the performance by guiding the networks to focus on particular parts of the sequence. Shen et al. [26] proposed a Directional Self-Attention Network (DiSAN) to process sentences, which is based solely on the attention mechanism, without any RNN/CNN structure. Transformer is first introduced by [30] for machine translation task in natural language processing, which is built on multi-head self-attention. Like RNNs, Transformers are used to handle sequential data. Recently, transformer has become the basic building block of most state-of-the-art architectures in NLP [6,29,33]. There is also other research use temporal graph to model the temporal dependence among sequences [17].\n\nSequential Recommendation. Temporal sets prediction can be applied to sequential recommendation. Hidasi et al. [9] proposed a GRU-based neural network for sequential recommendation, which is the first model that applied RNN to sequential recommendation. Yao et al. [35] developed a novel POI recommendation method which incorporates the degree of temporal matching between users and POIs. Quadrana et al. [24] proposed a hierarchical RNN based model to address the problem of personalizing session-based recommendation. Zhou et al. [39] proposed a Deep Interest Evolution Network to model interest evolving process for click-through rate prediction, which is an improvement of Deep Interest Network.\n\nRecently, self-attention mechanism has been widely used in sequential recommendation. Zhou et al. [38] proposed an attention based recommendation framework, called ATRank, which considers heterogeneous user behaviors. Chen et al. [4] uses the transformer model to capture the sequential signals underlying user's behavior sequences for recommendation in Alibaba. Feng et al. [7] proposed a novel CTR model called DSIN, which uses self-attention mechanism with bias encoding to extract user's interests in each session. Sun et al. [28] proposed a deep bidirectional sequential model called BERT4REC for sequential recommendation. Lv et al. [19] developed a self-attention based sequential deep matching model to capture user's dynamic preferences by combining short-term sessions and long-term behaviors.\n\n\nFORMALIZATION\n\nLet = { 1 , \u00b7 \u00b7 \u00b7 , | | } denote a collection of users and = { 1 , \u00b7 \u00b7 \u00b7 , | | } indicate all the available items. For a user, we use a tuple ( , ) to represent a user-set interaction, where \u2282 is a set of items interacted by the user at time . We formulate the temporal sets prediction problem as follows:\n\nDefinition 3.1. Temporal Sets Prediction Problem: For a user, given the user's historical behaviors represented as a sequence of sets, = {( 1 , 1 ), \u00b7 \u00b7 \u00b7 , ( , )}, the target of temporal sets prediction is to achieve the subsequent set +1 according to the given sequence, that is, +1 = ( ).\n\nA key problem in temporal sets prediction is how to represent the sets as low-dimensional structured vectors because the sets could be various in cardinality and usually permutation-invariant. To consider the above characteristics, we define the set embedding function as follows:\n\nDefinition 3.2. Set Embedding: A function : P ( ) \u2192 R is a mapping from a set to a size-fixed vector s, where s is invariant to any permutation of the items in the set. Formally, for any permutation of a set \u2282 ,\n( ) = ( ) if = { 1 , \u00b7 \u00b7 \u00b7 , | | } and = { (1) , \u00b7 \u00b7 \u00b7 , ( | |) }.\n\nPROPOSED METHOD\n\nIn this section, we present our model for temporal sets prediction, namely DSNTSP (Dual Sequential Network for Temporal Sets Prediction), whose architecture is shown in Figure 4. Our DSNTSP first learns the item-level representations and the set-level representations respectively by using multi-head self-attention mechanism. Then we develop a co-transformer module to learn the temporal dependencies of sets from two sequences. Finally, a gated neural module is designed to fuse the item-level and set-level representations learned in previous steps. In the following parts, we introduce the above modules one by one.\n\n\nItem-level Representation Learning\n\nExisting temporal sets prediction methods mainly focus on learning set-level representation for temporal sets, which makes it difficult to achieve satisfactory performance, because the items in sets could be correlated with each other not only in the same set, but also among different sets. In order to learn the multiple relationships, we first propose an Item-level Sequential Model (ISM) to learn items  relationships among different sets, whose architecture is shown in the Figure 2. ISM can learn the item-level representations of set sequences by using multi-head self-attention mechanism. Formally, given the user-set interactions\n= {( 1 , 1 ), \u00b7 \u00b7 \u00b7 , ( , )}, we can get user-item interactions = {( 1 , (1) ), \u00b7 \u00b7 \u00b7 , ( , ( ) )}, where \u2208 ( ) is the item interacted by the user at time ( ) .\nLet denote the size of and denote the size of , thus we could conclude that = =1 | |. The user-item interactions is used as the input of ISM, which is composed of three parts: an item embedding and time encoding layer, multi-head self-attention layers and an item-oriented attention layer. Next, we will introduce these three parts step by step.\n\n\nItem Embedding and Time Encoding Layer.\n\nWe first employ an item embedding layer to embed each item into a low-dimensional vector. Formally, let W \u2208 R \u00d7 | | denote the embedding matrix for items, where is the embedding dimension. Each input item is encoded as a one-hot column vector v \u2208 R | | , where only the -th value is 1 and other values are zeros. Then e = W v denotes the embedding vector for item .\n\nIn order to the make use of time information in the set sequences, we apply time encoding to the item embedding vectors, which is similar as the positional encoding method mentioned in [30]. Formally, the time encoding of item is computed as\nt ( ) [2 ] = sin +1 \u2212 ( ) /10000 2 / ,(1)t ( ) [2 + 1] = cos +1 \u2212 ( ) /10000 2 / ,(2)\nwhere t ( ) \u2208 R , t ( ) [ ] represents the -th value of the time encoding vector, and +1 is the prediction time. Then, for each item , its representation x is constructed by summing the corresponding item embedding vector and time encoding:\nx = e + t ( ) ,(3)\nwhere x \u2208 R , which will be fed into the multi-head self-attention layers to capture the temporal dependencies later. is the most commonly used model for temporal data mining, which usually processes the sequence data in order. However, for temporal sets prediction problem, the items in the same set usually do not have an inherent ordering, making it unable to be easily handled by the RNN. Recently, multi-head self-attention mechanism has been widely used to process the sequence data, such as natural languages [6,29,30,33]. Unlike RNN, self-attention mechanism doesn't require the sequence to be processed in order and it can use the positional encoding to preserve the order information, making it very convenient to learn the item-level representations of the set sequences. Formally, We set heads for our multi-head self-attention layers. Considering the item , the correlation between the item and the item for the head \u210e is defined as:\n\u210e = exp W \u210e x \u22a4 W \u210e x / / =1 exp W \u210e x \u22a4 W \u210e x / / ,(4)\nwhere W \u210e , W \u210e \u2208 R / \u00d7 are two learnable matrices. Once the attention score is obtained, we apply the multi-head self-attention upon the item as:\nz = LayerNorm x + W \u210e=1 \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 =1 \u210e W \u210e x \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe ,(5)\nwhere z \u2208 R , W \u210e \u2208 R / \u00d7 , W \u2208 R \u00d7 , \u2225 represents the concatenation operation and LayerNorm(\u00b7) is the standard normalization layer. Following, we add a point-wise feed-forward network to further enhance the model with non-linearity. The hidden state of item is computed as:\nh = LayerNorm z + FNN z ,(6)\nwhere h \u2208 R , FNN(\u00b7) represents the point-wise feed-forward network. The learnable parameters in Equation (4, 5) and (6) are shared across all items in the set sequence. Usually we stack multiple multi-head self-attention layers to model more complex temporal correlations in the set sequences.\n4.1.3 Item-oriented Attention Layer. Let H = [h 1 , \u00b7 \u00b7 \u00b7 , h ] \u2208 R \u00d7 denote the output of multi-head self-attention layers, where h \u2208\nR is the hidden representation of the item . Then, we could get the sequence representation from these item hidden representations. A popular method is to aggregate item hidden representations in H by some kind of pooling operations such as max pooling or average pooling. However, using a fixed sequence representation is not able to capture the complex dynamic patterns in the sequence. It is found that only a few number of items in the sequences are actually used to model user's preference by this method. Inspired by [39], an item-oriented attention layer is designed to make each candidate item in the item set choose the most relevant items to use, which is formulated as follows:\n= exp W e \u22a4 W h / =1 exp W e \u22a4 W h / ,(7)h = =1 W h ,(8)\nwhere W , W , W \u2208 R \u00d7 are learnable matrices, e \u2208 R is the embedding of the candidate item . For one candidate item, we calculate the similarity weight between each item representation and the candidate item embedding. Then the corresponding weights are normalized using the softmax function. We add temperature parameter into softmax which will change the probability distribution of the output. Increasing makes the distribution softer, and vise versa. Finally, the item-level representation vector h \u2208 R for the candidate item is a weighted sum of the item representations.\n\nWith the item-level representation h and the candidate item embedding e, we can compute the probability of the user interacting with the target item at a specific time +1 as\n= Sigmoid e \u22a4 h + ,(9)\nWe will use the above equation to calculate a score for each item in the item set . In order to speed up the calculations, we manipulate the whole item embedding matrix to calculate the sequence representations for all items:\nH = W H Softmax W H \u22a4 W W / ,(10)\nwhere H \u2208 R \u00d7 | | contains item-level representation for each item in . Then we can use these item-level representations to compute scores for all items.\n\n\nSet-level Representation Learning\n\nWe also develop a Set-level Sequential Model (SSM) for temporal sets prediction, which could learn the set-level representations of set sequences. The architecture of SSM is shown in the Figure  3(a). The SSM consists of four parts: an item and time encoding layer, a multi-head attention based set embedding layer, multi-head self-attention layers and an item-oriented attention layer. In order to learn the set-level representations, we need to compute a vector representation for each set in the set sequences. According to the Definition 3.2, the set embedding method has to be invariant to permutations of elements in the set. Existing set embedding methods for temporal sets prediction [11,36] just aggregate embedding representations of items in the set by some kind of pooling operations such as max pooling or average pooling. However, the pooling-based set embedding method assumes that items in the set are equally important, which is not reasonable in practice. Therefore, we proposed a Multi-head Attention Based Set Embedding Method (MASE), whose architecture is shown in the Figure 3(b). Formally, given a set = { 1 , \u00b7 \u00b7 \u00b7 , | | }, we embed each item in the set into a low-dimensional vector and then get a set of item embedding vectors S = [e 1 , \u00b7 \u00b7 \u00b7 , e | | ] \u2208 R \u00d7 | | . We set trainable queries q 1 , \u00b7 \u00b7 \u00b7 , q \u2208 R / to extract multifaceted features from the set. We first calculate similarity between these queries and each item embedding vectors in the set S:\n= exp q \u22a4 W , e =1 exp q \u22a4 W , e ,(11)\nwhere W , \u2208 R / \u00d7 , represents the attention weight of for the query q . Following, we can get a set representation s ( ) for the query q by computing a weighted sum of item embedding vectors:\ns ( ) = | | =1 W , e ,(12)\nwhere s ( ) \u2208 R / , W , \u2208 R / \u00d7 . Finally, we concatenate these set representations and get our final representation by using an output matrix as follows:\ns = W =1 s ( ) ,(13)\nwhere W \u2208 R \u00d7 , s \u2208 R is the vector representation of set . Given the user-set interactions = {( 1 , 1 ), \u00b7 \u00b7 \u00b7 , ( , )}, we can get a sequence of set representations TS = [s 1 , \u00b7 \u00b7 \u00b7 , s ], where s \u2208 R is the set embedding vector of the set . Then we can apply time encoding to set embedding vectors in TS as:\nx = s + t ,(14)\nwhere t is computed by Equation (1). The sequence of set embedding vectors together with their time encodings is further fed into the multi-head self-attention layers to capture temporal dependencies of sets. After that we can get a sequence of set representations\nH = [h 1 , \u00b7 \u00b7 \u00b7 , h ],\nwhere h \u2208 R , and then we can learn the set-level representations via an item-oriented attention layer, which is described in the subsection 4.1.3. Finally we can use the set-level representations learned in previous step to predict the probability of each item interacted by the user in the future.\n\n\nDual Sequential Fusion\n\nIn this subsection, we introduce our Dual Sequential Network for Temporal Sets Prediction (DSNTSP), which can take advantage of both item-level representations and set-level representations of set sequences to better solve the temporal sets prediction problem. The architecture of DSNTSP is shown in the Figure 4. Given the user-set interactions = {( 1 , 1 ), \u00b7 \u00b7 \u00b7 , ( , )} and the user-item interactions = {( 1 , (1) ), \u00b7 \u00b7 \u00b7 , ( , ( ) )} derived from , we can use the networks described in the subsection 4.1 and 4.2 to learn the item-level representations and the set-level representations separately. It should be noted that these two networks share the same item embedding matrix. In order to learn the temporal dependencies of sets from both two sequences, we propose a novel co-transformer module. Co-transformer is a general model that can learn the temporal dependencies between two sequences with different lengths and different elements. We replace the multi-head self-attentions layers described earlier with our cotransformer module. Finally, we design a gated neural module to fuse the item-level representations and set-level representations learned in previous step.\n\n\nCo-Transformer.\n\nThe multi-head self-attention can learn the temporal dependencies of the sequence by taking this sequence as queries, keys and values at the same time. Inspired by this, given an item representation sequence X = [x 1 , \u00b7 \u00b7 \u00b7 , x ] and a set representation sequence X = [x 1 , \u00b7 \u00b7 \u00b7 , x ], we can take one sequence as queries and another sequence as keys and values. In this way, we can learn temporal dependencies of sets from both two sequences. Formally, considering the item x in X , we can compute the correlation between the item x and the set x as:\n\u210e = exp W \u210e x \u22a4 W \u210e x / / =1 exp W \u210e x \u22a4 W \u210e x / / ,(15)\nwhere W \u210e , W \u210e \u2208 R / \u00d7 . We can get the hidden state of the item x by calculating the weighted sum of the set representations in X , followed by a point-wise feed-forward network:\nz , = LayerNorm x + W \u210e=1 \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 =1 \u210e W \u210e x \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe ,(16)h , = LayerNorm z , + FNN z , ,(17)\nwhere W \u210e \u2208 R / \u00d7 , W \u2208 R \u00d7 . Then we can compute the final hidden state of the item x by fusing h and h , via a gated neural layer, which is formulated as follows:\ng = Sigmoid W h + W , h , + ,(18)h = g \u2299 h + 1 \u2212 g \u2299 h , ,(19)\nwhere W , W , \u2208 R \u00d7 , and \u2299 is element-wise multiplication. In contrast, we can also take the set sequence X as queries and the item sequence X as keys and values. Then we can calculate the hidden state for each set from X in the same way.  Figure 4: Dual sequential network for temporal sets prediction.\n\n\nItem-level and Set-level Representations Fusion.\n\nFor each candidate item , we can compute the item-level representation h and the set-level representations h respectively. In order to fuse these two representations, we design a gated neural module that takes e, h and h as inputs. We first compute a gate vector g to decide contribution percentages of item-level representations and set-level representations for the candidate item , and then use the gate vector to compute the final sequence representation h, which is formulated as follows:\ng = Sigmoid W h + W h + W e + ,(20)h = g \u2299 h + 1 \u2212 g \u2299 h ,(21)\nwhere g , h \u2208 R , W , W , W \u2208 R \u00d7 . Finally, the sequence representation h can be used to compute the probability of the user interacting with the candidate item in the future.\n\n\nModel Learning\n\nThe problem of temporal sets prediction can be seen as a multilabel classification, and one item is regarded as a label. One way to solve this problem is to transform the multi-label classification into multiple binary classification problem. That is to say, we train one binary classifier for each item independently. Therefore, we use the binary cross-entropy loss as our objective function. Formally, for each user, we define the objective function as:\n= \u2212 \u2208 +1 log ( , ) \u2212 \u2209 +1 log(1 \u2212 ( , )),(22)\nwhere ( , ) is the output of our network representing the predicted probability for the user interacting with the item according to the set sequence . Finally, we can minimize the objective function by performing Adam optimizer [12].\n\n\nEXPERIMENTS\n\nThis section evaluates the effectiveness of the proposed method. We first introduce the used data sets, evaluation metrics, and compared baselines; then present the performance comparisons of our method with both classical and the state-of-the-art methods. Ablation study is conducted to verify the effectiveness of the components in our method, the result of set embedding is also visualized to discuss the interpretability of our method.\n\n\nExperiment Setup\n\n\nData Sets.\n\nWe conduct experiments on four real-world datasets. The statistics of the four datasets are summarized in the Table 1.\nTaFeng 1 :\nThe TaFeng dataset is a public dataset that contains 4 months (from November 2000 to February 2001) of shopping transactions from a Chinese grocery store. We remove users that purchased less than 10 items and items purchased less than 10 times. We treat all the items bought in the same order as a set.\n\nTaoBao 2 : The TaoBao dataset is a public online e-commerce dataset provided by Ant Financial Services. It contains the transactions about which items are purchased or clicked by each user in each order with the timestamp. We select the transactions from August 1, 2015 to September 15, 2015. The users that purchased less than 10 items are removed, so are the items purchased less than 30 times. We treat all the items purchased in the same day as a set. Jingdong 3 : The JingDong dataset contains user action records from February 1, 2018 to April 15, 2018, including browsing, purchasing, following, commenting and adding to shopping carts. We only select purchasing actions and filter out users that purchased less than 3 items and items purchased less than 3 times. We treat all the items purchased in the same day as a set.\n\nDunnhumby 4 : The Dunnhumby dataset is a public dataset provided by dunnhumby, which is a global customer data science company. It contains the transactions about which items are bought by each user in each order with the timestamp. We select 300 days transaction records and filter out users that purchased less than 3 items and items purchased less than 3 times. We treat all the items bought in the same order as a set.\n\nFor each dataset, we can get a series of set sequences. Given the user-set interactions = {( 1 , 1 ), \u00b7 \u00b7 \u00b7 , ( , )} for a user, we use the first interactions {( 1 , 1 ), \u00b7 \u00b7 \u00b7 , ( , )} to predict the ( +1) \u210e interaction ( +1 , +1 ) in the training set, where = 1, \u00b7 \u00b7 \u00b7 , \u2212 3. We use the first \u22122 interactions to predict the ( \u22121) \u210e interaction in the validation set and use the first \u2212 1 interactions to predict the last one in the test set.\n\n\nEvaluation Metrics.\n\nTo evaluate the effectiveness of different methods, we use Recall and Normalized Discounted Cumulative Gain(NDCG) metrics.\n\nRecall represents the ability of coverage in the user's ground truth. For each user , recall is calculated as:\nRecall@K( ) = | \u2229 | | | ,(23)\nwhere is the predicted top K items for user and is the ground truth of user . We use the average recall of all users as the measurement.\n\nNormalized Discounted Cumulative Gain is a measurement of ranking quality. For each user , NDCG is calculated as:\nNDCG@K( ) = =1 /log 2 ( + 1) min( , | |) =1 1/ log 2 ( + 1) .(24)\nThe average NDCG of all users is used as the measurement.\n\n\nBaseline Methods.\n\nTo verify the effectiveness of our method, we compare it with the following representative baselines: POP: it ranks items according to their popularity. For any user, the subsequent set is a combination of the most frequently purchased items.\n\nPERSON POP: it ranks items based on their personal popularity. For each user, the subsequent set is a combination of the user's historical frequently purchased items.\n\nMF [13]: it is a collaborative filter method, which applies matrix factorization over the user-item matrix. It does not consider the temporal dependencies and co-occurrence of items.\n\nFPMC [25]: it is a hybrid model combining markov chain and matrix factorization for next basket recommendation. Both temporal dependencies between items and user's general interests are taken into account for prediction.\n\nDREAM [36]: it is a RNN-based method for next basket recommendation, where the representation of a basket is aggregated by item's embedding via a pooling layer.\n\nDIN [39]: it is a DNN-based method used for click-though rate prediction. It uses attention mechanism to learn the representation of user's historical behaviors for the target items. To speedup, we replace its local activation unit with dot-product attention, and compute score for each item by dot product.\n\nSETS2SETS [11]: it is the state-of-art method in temporal sets prediction. It uses average pooling operation to get the representations for sets. A repeated-element-specified element-element interaction component is designed to further improve the performance.\n\n\nImplementation Details.\n\nWe use PyTorch [23] to implement our model and deploy it on TITAN X GPU with 12G memory. Adam optimizer [12] with learning rate 0.0001 is used to update parameters. The batch size is set to 64. The dimension of embedding is set to 256. The number of multi-head self-attention layers is set to 4, and the number of heads in the multi-head self-attention layer is set to 8. The number of heads in our set embedding module is set to 8. The temperature parameters in the item-oriented attention layers are set to 0.8, 1.0, 4.0, 16.0 for TaFeng, TaoBao, JingDong and Dunnhumby datasets respectively.\n\n\nPerformance Comparison\n\nWe report experiment results of different methods in Table 2 and Figure 5. As we can see, our DSNTSP model achieves better performance than other comparative methods in most cases. Besides, there are some findings in these comparative experiments.\n\nFirstly, FPMC and DREAM achieve worse performance for both Recall and NDCG on these four datasets compared to most other baseline methods. They even perform worse than POP on TaFeng dataset. FPMC only considers the last set interacted by each user, while DREAM considers all sets in the user historical behaviors, but it only captures temporal dependencies of the set sequences at the set-level. The results show that it is difficult to achieve good results if we only learn the set-level representations of the set sequences.\n\nSecondly, DIN and PERSON POP perform better than FPMC and DREAM, suggesting that the frequencies of items appearing in the past sets of a given user are informative. Sets2Sets and DSNTSP perform even better than DIN and PERSON POP, which cannot take into account temporal dependencies in the user behaviors.\n\nThirdly, Sets2Sets is the state-of-art method for temporal sets prediction and it achieves better performance compared to other baseline methods in most cases. The reason why Sets2Sets can achieve such good results is that it not only considers the temporal dependencies of the set sequences at the set-level, but also models repeated-element-specified element-element relation in the set sequences. However, Sets2Sets performs worse than our method    because it doesn't consider the item-level temporal dependencies of the set sequences. Fourthly, our proposed DSNTSP achieves significant improvements for both Recall and NDCG metrics on TaoBao and JingDong datasets compared to other baseline methods. We believe the reason is that in the e-commerce scenario, people tend to purchase different items each time, which is not very common in the retail stores. Sets2Sets only consider the frequency for each item appearing in the user historical behaviors, making it tend to recommend repeated items for customers. Compared to Sets2Sets, our method can learn both the item-level and set-level temporal dependencies of the set sequences, which makes our method able to achieve better performance than Sets2Sets.\n\n\nAblation Study\n\nThis section conducts ablation studies on TaoBao and JingDong datasets, Figure 6 presents the results. 5.3.1 Effect of the Multi-head Self-Attention Layers. The performance comparison of ISM and DIN could reveal the impact of multi-head self-attention layers because DIN can be seen as a variant of the ISM that removes the multi-head self-attention layers. From the Figure 6, we can see that ISM achieves significant improvements compared to the DIN on both TaoBao and JingDong datasets. It indicates that the multi-head self-attention layers make our model available to capture the complex temporal dependencies of set sequences, which will help to improve the prediction accuracy. time encoding employed in DSNTSP could help our model make full use of the temporal information in the set sequences.\n\n\nEffect of the Item-oriented Attention Layer.\n\nWe also evaluate the necessity of the item-oriented attention layer in our model. We develop a variant of the DSNTSP that computes sequence representations by aggregating the hidden states of the sequences via average pooling operations. From the Figure 6, we can see that our DSNTSP far outperforms the DSNTSP without the item-oriented attention layer on both TaoBao and JingDong datasets. It indicates that using one fixed-length vector will be a bottleneck to express dynamic evolving patterns in the set sequences. Our item-oriented attention layer can compute the sequence representation for each target item by making each target item choose the most relevant parts in the set sequence to use, which will help to improve the model performance.\n\n\nEffect of the Fusion of Item-level and Set-level Representations.\n\nIn order to investigate the effect of the fusion of item-level and set-level representations, we compare our DSNTSP with ISM and SSM on both TaoBao and JingDong datasets. From the Figure 6, we can see that our DSNTSP outperforms both ISM and SSM for all evaluation metrics on both TaoBao and JingDong datasets. It shows that learning both the item-level representations and setlevel representations can make our model better solve the temporal sets prediction problem.\n\n\nEffect of the Co-Transformer.\n\nIn order to investigate the influence of our co-transformer module, we compare the performance of DSNTSP with a variant without co-transformer module. From the Figure 6, we can see that our DSNTSP achieves better performance than DSNTSP without co-transformer. It indicates that our co-transformer module can improve the performance of our model by learning the temporal dependencies of set sequences from both item-level and set-level.\n\n\nVisualization of Set Embedding\n\nIn this subsection, we investigate how effective our multi-head attention based set embedding method is. We conduct a visualization experiment to verify the effectiveness of our set embedding method. We apply our set embedding method over a set containing four types of items sampled from TaoBao dataset. The types of items in the set include A, B, C and D. Then we visualize the attention weights of the different heads over these items, which is shown in  We can see that different heads focus on different types of items in the set. Head 1 mainly concentrates on the items of type A, while head 2 pays more attention on the items of type D. Head 3 assigns higher weights to the items of type C and head 4 gives more attention to items of type B. The results show that different heads in our set embedding method usually focus on different aspects of the set, making our model available to get more reasonable vectorized representations for sets. In summary, this section conducts experiments on four realworld data sets. Four classical and three state-of-the-art methods have been implemented to provide baseline performance. We use both and to get a comprehensive comparison of our method with the baselines. Finally, the proposed method DSNTSP achieves the best prediction performance in most cases. Ablation study demonstrates that both item-level and set-level representations are useful for temporal sets prediction, the co-transformer and gated neural fusing module also contributes to the improvement of prediction accuracy. Additionally, visualization experiment suggests that our method could achieve a semantic representation of sets. All the above results and observations support the previous theoretical analysis.\n\n\nCONCLUSION\n\nThis paper provides a novel prediction method for temporal sets, which consists of four key modules. Item-level and set-level representation learning modules could capture the correlations of items and sets separately, and then achieve both item-level and setlevel representations of set sequence. The co-transformer module is able to learn the multiple temporal dependencies of items and sets. The gated neural module fuses all the multi-level correlations and multiple temporal dependencies comprehensively to predict the subsequent set. Experiments have been conducted on four realworld data sets, results demonstrate the superiority of our method over both classical and the state-of-the-art methods. Ablation study validates the effectiveness of the components in our method. In summary, this work studies a multi-level representation learning method for set sequence, which enables us to discover multi-level correlations and multiple temporal dependencies of items and sets. Therefore, our method could obtain higher prediction performance than most of the existing methods. In future, we may study timeaware temporal sets prediction problem by extending the temporal sets prediction method proposed in this paper.\n\nFigure 1 :\n1Illustration of temporal sets prediction.\n\nFigure 2 :\n2Item-level representation learning.\n\nFigure 3 :\n3Set-level representation learning.\n\nFigure 5 :\n5Results comparison with changing from 10 to 100 with step size 10.\n\nFigure 6 :\n6of the Time Encoding. To investigate the effect of the time encoding, we compare the performance between DSNTSP with time encoding (DSNTSP) and DSNTSP with positional encoding (DSNTSP w/ Positional Encoding) on both TaoBao and JingDong Datasets. From the Figure 6, we can see that DSNTSP with time encoding achieves better performance than DSNTSP with positional encoding on both TaoBao and JingDong dataset. It indicates that Ablation study of DSNTSP on TaoBao and JingDong data sets.\n\nFigure 7 :\n7Visualization of attention weights from head 1 to head 4 in the multi-head attention based set embedding module with four heads on TaoBao data set.\n\nFigure 7 .\n7Figure 7. We can see that different heads focus on different types of items in the set. Head 1 mainly concentrates on the items of type A, while head 2 pays more attention on the items of type D. Head 3 assigns higher weights to the items of type C and head 4 gives more attention to items of type B. The results show that different heads in our set embedding method usually focus on different aspects of the set, making our model available to get more reasonable vectorized representations for sets. In summary, this section conducts experiments on four realworld data sets. Four classical and three state-of-the-art methods have been implemented to provide baseline performance. We use both and to get a comprehensive comparison of our method with the baselines. Finally, the proposed method DSNTSP achieves the best prediction performance in most cases. Ablation study demonstrates that both item-level and set-level representations are useful for temporal sets prediction, the co-transformer and gated neural fusing module also contributes to the improvement of prediction accuracy. Additionally, visualization experiment suggests that our method could achieve a semantic representation of sets. All the above results and observations support the previous theoretical analysis.\n\n\nMulti-head Self-attention Layers. The recurrent neural network (RNN)Session 8C: Sequential Recommendation \n\nSIGIR '20, July 25-30, 2020, Virtual Event, China \n4.1.2 \n\nTable 1 :\n1Statistics of the datasetsDataset \n#items #sets #users Ave. #items per set Ave. #sets per user \n\nTaFeng \n11208 80093 9684 \n6.1582 \n8.2706 \n\nTaoBao \n14872 99926 14714 \n2.2472 \n6.7912 \n\nJingDong \n22735 84273 16302 \n1.3723 \n5.1695 \n\nDunnhumby 15730 79389 2306 \n10.3802 \n34.4271 \n\n\n\nTable 2 :\n2Experiment results of different methodsModels \n\nTaFeng \nTaoBao \nJingDong \nDunnhumby \n\nK=50 \nK=100 \nK=50 \nK=100 \nK=50 \nK=100 \nK=50 \nK=100 \n\nRecall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG Recall NDCG \n\n\nhttps://www.kaggle.com/chiranjivdas09/ta-feng-grocerydataset 2 https://tianchi.aliyun.com/dataset/dataDetail?dataId=53 Session 8C: Sequential Recommendation SIGIR '20, July 25-30, 2020, Virtual Event, China\nhttps://jdata.jd.com/html/detail.html?id=8 4 https://www.dunnhumby.com/careers/engineering/sourcefiles\nACKNOWLEDGMENTS\nNeural Machine Translation by Jointly Learning to Align and Translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, ICLR. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR.\n\nSequences of Sets. Austin R Benson, Ravi Kumar, Andrew Tomkins, KDD. ACM. Austin R. Benson, Ravi Kumar, and Andrew Tomkins. 2018. Sequences of Sets. In KDD. ACM, 1148-1157.\n\nRecurrent Neural Networks for Multivariate Time Series with Missing Values. Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David A Sontag, Yan Liu, CoRR abs/1606.01865Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David A. Sontag, and Yan Liu. 2016. Recurrent Neural Networks for Multivariate Time Series with Missing Values. CoRR abs/1606.01865 (2016).\n\nBehavior Sequence Transformer for E-commerce Recommendation in Alibaba. Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, Wenwu Ou, CoRR abs/1905.06874Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior Sequence Transformer for E-commerce Recommendation in Alibaba. CoRR abs/1905.06874 (2019).\n\nLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Kyunghyun Cho, \u00c7aglar Bart Van Merrienboer, Dzmitry G\u00fcl\u00e7ehre, Fethi Bahdanau, Holger Bougares, Yoshua Schwenk, Bengio, EMNLP. ACL. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In EMNLP. ACL, 1724-1734.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT (1). Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT (1). Association for Computational Linguistics, 4171-4186.\n\nDeep Session Interest Network for Click-Through Rate Prediction. Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, Keping Yang, IJCAI. ijcai.org. Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep Session Interest Network for Click-Through Rate Prediction. In IJCAI. ijcai.org, 2301-2307.\n\nSpeech recognition with deep recurrent neural networks. Alex Graves, Mohamed Abdel-Rahman, Geoffrey E Hinton, ICASSP. IEEE. Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech recognition with deep recurrent neural networks. In ICASSP. IEEE, 6645-6649.\n\nSession-based Recommendations with Recurrent Neural Networks. Bal\u00e1zs Hidasi, Alexandros Karatzoglou, ICLR (Poster). Linas Baltrunas, and Domonkos TikkBal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In ICLR (Poster).\n\nLong Short-Term Memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural Computation. 9Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735-1780.\n\nSets2Sets: Learning from Sequential Sets with Neural Networks. Haoji Hu, Xiangnan He, KDD. ACM. Haoji Hu and Xiangnan He. 2019. Sets2Sets: Learning from Sequential Sets with Neural Networks. In KDD. ACM, 1491-1499.\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, ICLR (Poster). Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti- mization. In ICLR (Poster).\n\nMatrix Factorization Techniques for Recommender Systems. Yehuda Koren, Robert M Bell, Chris Volinsky, IEEE Computer. 42Yehuda Koren, Robert M. Bell, and Chris Volinsky. 2009. Matrix Factorization Techniques for Recommender Systems. IEEE Computer 42, 8 (2009), 30-37.\n\nSet Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, Yee Whye Teh, PMLRICML (Proceedings of Machine Learning Research). 97Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. 2019. Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. In ICML (Proceedings of Machine Learn- ing Research), Vol. 97. PMLR, 3744-3753.\n\nNEMO: Next Career Move Prediction with Contextual Embedding. Liangyue Li, How Jing, Hanghang Tong, Jaewon Yang, Qi He, Bee-Chung Chen, WWW (Companion Volume). ACMLiangyue Li, How Jing, Hanghang Tong, Jaewon Yang, Qi He, and Bee-Chung Chen. 2017. NEMO: Next Career Move Prediction with Contextual Embedding. In WWW (Companion Volume). ACM, 505-513.\n\nA Critical Review of Recurrent Neural Networks for Sequence Learning. Zachary Chase Lipton, CoRR abs/1506.00019Zachary Chase Lipton. 2015. A Critical Review of Recurrent Neural Networks for Sequence Learning. CoRR abs/1506.00019 (2015).\n\nTemporal skeletonization on sequential data: patterns, categorization, and visualization. Chuanren Liu, Kai Zhang, Hui Xiong, Geoff Jiang, Qiang Yang, KDD. ACM. Chuanren Liu, Kai Zhang, Hui Xiong, Geoff Jiang, and Qiang Yang. 2014. Temporal skeletonization on sequential data: patterns, categorization, and visualization. In KDD. ACM, 1336-1345.\n\nEffective Approaches to Attention-based Neural Machine Translation. Thang Luong, Hieu Pham, Christopher D Manning, EMNLP. The Association for Computational Linguistics. Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Ap- proaches to Attention-based Neural Machine Translation. In EMNLP. The Asso- ciation for Computational Linguistics, 1412-1421.\n\nSDM: Sequential Deep Matching Model for Online Large-scale Recommender System. Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, Wilfred Ng, CIKM. ACM. Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wil- fred Ng. 2019. SDM: Sequential Deep Matching Model for Online Large-scale Recommender System. In CIKM. ACM, 2635-2643.\n\nHATS: A Hierarchical Sequence-Attention Framework for Inductive Set-of-Sets Embeddings. Changping Meng, Jiasen Yang, Bruno Ribeiro, Jennifer Neville, KDD. ACM. Changping Meng, Jiasen Yang, Bruno Ribeiro, and Jennifer Neville. 2019. HATS: A Hierarchical Sequence-Attention Framework for Inductive Set-of-Sets Embed- dings. In KDD. ACM, 783-792.\n\nRecurrent Models of Visual Attention. Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, NIPS. Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent Models of Visual Attention. In NIPS. 2204-2212.\n\nJanossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs. Ryan L Murphy, Balasubramaniam Srinivasan, A Vinayak, Bruno Rao, Ribeiro, ICLR (Poster). OpenReview.net. Ryan L. Murphy, Balasubramaniam Srinivasan, Vinayak A. Rao, and Bruno Ribeiro. 2019. Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs. In ICLR (Poster). OpenReview.net.\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary Devito, NeurIPS. Lu Fang, Junjie Bai, and Soumith ChintalaMartin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit SteinerAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des- maison, Andreas K\u00f6pf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS. 8024-8035.\n\nPersonalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks. Massimo Quadrana, Alexandros Karatzoglou, Bal\u00e1zs Hidasi, Paolo Cremonesi, RecSys. ACM. Massimo Quadrana, Alexandros Karatzoglou, Bal\u00e1zs Hidasi, and Paolo Cremonesi. 2017. Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks. In RecSys. ACM, 130-137.\n\nFactorizing personalized Markov chains for next-basket recommendation. Steffen Rendle, Christoph Freudenthaler, Lars Schmidt-Thieme, WWW. ACM. Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor- izing personalized Markov chains for next-basket recommendation. In WWW. ACM, 811-820.\n\nDiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding. Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, Chengqi Zhang, AAAI. AAAI PressTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. 2018. DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding. In AAAI. AAAI Press, 5446-5455.\n\nStratis Limnios, and Michalis Vazirgiannis. Konstantinos Skianis, Giannis Nikolentzos, CoRR abs/1904.01962Rep the Set: Neural Networks for Learning Set Representations. Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, and Michalis Vazir- giannis. 2019. Rep the Set: Neural Networks for Learning Set Representations. CoRR abs/1904.01962 (2019).\n\nBERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, Peng Jiang, CIKM. ACM. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre- sentations from Transformer. In CIKM. ACM, 1441-1450.\n\nERNIE 2.0: A Continual Pre-training Framework for Language Understanding. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hua Hao Tian, Haifeng Wu, Wang, CoRR abs/1907.12412Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2019. ERNIE 2.0: A Continual Pre-training Framework for Language Understanding. CoRR abs/1907.12412 (2019).\n\n. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is All you Need. In NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS. 5998-6008.\n\nOrder Matters: Sequence to sequence for sets. Oriol Vinyals, Samy Bengio, Manjunath Kudlur, ICLR (Poster). Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. 2016. Order Matters: Se- quence to sequence for sets. In ICLR (Poster).\n\nShow and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, CVPR. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In CVPR. IEEE Computer Society, 3156-3164.\n\nXLNet: Generalized Autoregressive Pretraining for Language Understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G Carbonell, Ruslan Salakhutdinov, V Quoc, Le, CoRR abs/1906.08237Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdi- nov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. CoRR abs/1906.08237 (2019).\n\nHierarchical Attention Networks for Document Classification. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, Eduard H Hovy, HLT-NAACL. The Association for Computational Linguistics. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and Ed- uard H. Hovy. 2016. Hierarchical Attention Networks for Document Classification. In HLT-NAACL. The Association for Computational Linguistics, 1480-1489.\n\nPOI Recommendation: A Temporal Matching between POI Popularity and User Regularity. Zijun Yao, Yanjie Fu, Bin Liu, Yanchi Liu, Hui Xiong, ICDM. Zijun Yao, Yanjie Fu, Bin Liu, Yanchi Liu, and Hui Xiong. 2016. POI Recommen- dation: A Temporal Matching between POI Popularity and User Regularity. In ICDM. IEEE Computer Society, 549-558.\n\nA Dynamic Recurrent Model for Next Basket Recommendation. Feng Yu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan, SIGIR. ACM. Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A Dynamic Recurrent Model for Next Basket Recommendation. In SIGIR. ACM, 729-732.\n\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab\u00e1s P\u00f3czos, Ruslan Salakhutdinov, Alexander J Smola, Deep Sets. In NIPS. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab\u00e1s P\u00f3czos, Ruslan Salakhutdinov, and Alexander J. Smola. 2017. Deep Sets. In NIPS. 3391-3401.\n\nATRank: An Attention-Based User Behavior Modeling Framework for Recommendation. Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen, Jun Gao, AAAI. AAAI PressChang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen, and Jun Gao. 2018. ATRank: An Attention-Based User Behavior Modeling Frame- work for Recommendation. In AAAI. AAAI Press, 4564-4571.\n\nDeep Interest Network for Click-Through Rate Prediction. Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai, KDD. ACMGuorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In KDD. ACM, 1059-1068.\n", "annotations": {"author": "[{\"end\":196,\"start\":126},{\"end\":268,\"start\":197},{\"end\":337,\"start\":269},{\"end\":437,\"start\":338},{\"end\":555,\"start\":438},{\"end\":626,\"start\":556},{\"end\":697,\"start\":627},{\"end\":769,\"start\":698},{\"end\":838,\"start\":770},{\"end\":938,\"start\":839},{\"end\":1036,\"start\":939}]", "publisher": "[{\"end\":67,\"start\":54},{\"end\":1253,\"start\":1240}]", "author_last_name": "[{\"end\":136,\"start\":133},{\"end\":208,\"start\":205},{\"end\":277,\"start\":275},{\"end\":350,\"start\":347},{\"end\":447,\"start\":442},{\"end\":566,\"start\":564},{\"end\":637,\"start\":634},{\"end\":709,\"start\":706},{\"end\":778,\"start\":776},{\"end\":851,\"start\":848},{\"end\":948,\"start\":943}]", "author_first_name": "[{\"end\":132,\"start\":126},{\"end\":204,\"start\":197},{\"end\":274,\"start\":269},{\"end\":346,\"start\":338},{\"end\":441,\"start\":438},{\"end\":563,\"start\":556},{\"end\":633,\"start\":627},{\"end\":705,\"start\":698},{\"end\":775,\"start\":770},{\"end\":847,\"start\":839},{\"end\":942,\"start\":939}]", "author_affiliation": "[{\"end\":195,\"start\":138},{\"end\":267,\"start\":210},{\"end\":336,\"start\":279},{\"end\":436,\"start\":352},{\"end\":554,\"start\":469},{\"end\":625,\"start\":568},{\"end\":696,\"start\":639},{\"end\":768,\"start\":711},{\"end\":837,\"start\":780},{\"end\":937,\"start\":853},{\"end\":1035,\"start\":950}]", "title": "[{\"end\":53,\"start\":1},{\"end\":1089,\"start\":1037}]", "venue": "[{\"end\":1215,\"start\":1091}]", "abstract": "[{\"end\":2827,\"start\":1418}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3579,\"start\":3576},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3582,\"start\":3579},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3733,\"start\":3729},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4015,\"start\":4011},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8516,\"start\":8512},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8787,\"start\":8783},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8887,\"start\":8884},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9006,\"start\":9002},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9317,\"start\":9313},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9320,\"start\":9317},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9497,\"start\":9493},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9584,\"start\":9580},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9716,\"start\":9712},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9963,\"start\":9959},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10073,\"start\":10069},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10228,\"start\":10224},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10435,\"start\":10431},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10563,\"start\":10560},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10588,\"start\":10585},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10611,\"start\":10607},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10646,\"start\":10642},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10730,\"start\":10727},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10876,\"start\":10873},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10879,\"start\":10876},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10882,\"start\":10879},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10885,\"start\":10882},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11018,\"start\":11014},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11215,\"start\":11211},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11489,\"start\":11486},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11492,\"start\":11489},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11495,\"start\":11492},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11598,\"start\":11594},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11715,\"start\":11712},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11870,\"start\":11866},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12010,\"start\":12006},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12137,\"start\":12133},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12404,\"start\":12400},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12535,\"start\":12532},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12680,\"start\":12677},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12836,\"start\":12832},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12945,\"start\":12941},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16705,\"start\":16701},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17623,\"start\":17620},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17626,\"start\":17623},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17629,\"start\":17626},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17632,\"start\":17629},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19583,\"start\":19579},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21724,\"start\":21720},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21727,\"start\":21724},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28063,\"start\":28059},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31790,\"start\":31786},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31976,\"start\":31972},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32199,\"start\":32195},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32359,\"start\":32355},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32674,\"start\":32670},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32967,\"start\":32963},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33056,\"start\":33052}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41546,\"start\":41492},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41595,\"start\":41547},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41643,\"start\":41596},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41723,\"start\":41644},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42222,\"start\":41724},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42383,\"start\":42223},{\"attributes\":{\"id\":\"fig_7\"},\"end\":43678,\"start\":42384},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43846,\"start\":43679},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44136,\"start\":43847},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44386,\"start\":44137}]", "paragraph": "[{\"end\":4462,\"start\":2843},{\"end\":5209,\"start\":4464},{\"end\":6122,\"start\":5211},{\"end\":7229,\"start\":6124},{\"end\":7292,\"start\":7231},{\"end\":7931,\"start\":7294},{\"end\":8187,\"start\":7933},{\"end\":8269,\"start\":8204},{\"end\":9236,\"start\":8271},{\"end\":10368,\"start\":9238},{\"end\":11599,\"start\":10370},{\"end\":12300,\"start\":11601},{\"end\":13105,\"start\":12302},{\"end\":13428,\"start\":13123},{\"end\":13721,\"start\":13430},{\"end\":14003,\"start\":13723},{\"end\":14216,\"start\":14005},{\"end\":14921,\"start\":14302},{\"end\":15598,\"start\":14960},{\"end\":16105,\"start\":15760},{\"end\":16514,\"start\":16149},{\"end\":16757,\"start\":16516},{\"end\":17084,\"start\":16844},{\"end\":18050,\"start\":17104},{\"end\":18253,\"start\":18107},{\"end\":18596,\"start\":18322},{\"end\":18920,\"start\":18626},{\"end\":19744,\"start\":19056},{\"end\":20378,\"start\":19802},{\"end\":20553,\"start\":20380},{\"end\":20802,\"start\":20577},{\"end\":20990,\"start\":20837},{\"end\":22511,\"start\":21028},{\"end\":22743,\"start\":22551},{\"end\":22925,\"start\":22771},{\"end\":23258,\"start\":22947},{\"end\":23539,\"start\":23275},{\"end\":23863,\"start\":23564},{\"end\":25073,\"start\":23890},{\"end\":25647,\"start\":25093},{\"end\":25885,\"start\":25705},{\"end\":26156,\"start\":25992},{\"end\":26524,\"start\":26220},{\"end\":27070,\"start\":26577},{\"end\":27310,\"start\":27134},{\"end\":27784,\"start\":27329},{\"end\":28064,\"start\":27831},{\"end\":28519,\"start\":28080},{\"end\":28671,\"start\":28553},{\"end\":28985,\"start\":28683},{\"end\":29816,\"start\":28987},{\"end\":30240,\"start\":29818},{\"end\":30685,\"start\":30242},{\"end\":30831,\"start\":30709},{\"end\":30943,\"start\":30833},{\"end\":31110,\"start\":30974},{\"end\":31225,\"start\":31112},{\"end\":31349,\"start\":31292},{\"end\":31613,\"start\":31371},{\"end\":31781,\"start\":31615},{\"end\":31965,\"start\":31783},{\"end\":32187,\"start\":31967},{\"end\":32349,\"start\":32189},{\"end\":32658,\"start\":32351},{\"end\":32920,\"start\":32660},{\"end\":33542,\"start\":32948},{\"end\":33816,\"start\":33569},{\"end\":34344,\"start\":33818},{\"end\":34653,\"start\":34346},{\"end\":35865,\"start\":34655},{\"end\":36685,\"start\":35884},{\"end\":37483,\"start\":36734},{\"end\":38021,\"start\":37553},{\"end\":38491,\"start\":38055},{\"end\":40255,\"start\":38526},{\"end\":41491,\"start\":40270}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14283,\"start\":14217},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15759,\"start\":15599},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16799,\"start\":16758},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16843,\"start\":16799},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17103,\"start\":17085},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18106,\"start\":18051},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18321,\"start\":18254},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18625,\"start\":18597},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19055,\"start\":18921},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19786,\"start\":19745},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19801,\"start\":19786},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20576,\"start\":20554},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20836,\"start\":20803},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22550,\"start\":22512},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22770,\"start\":22744},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22946,\"start\":22926},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23274,\"start\":23259},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23563,\"start\":23540},{\"attributes\":{\"id\":\"formula_18\"},\"end\":25704,\"start\":25648},{\"attributes\":{\"id\":\"formula_19\"},\"end\":25956,\"start\":25886},{\"attributes\":{\"id\":\"formula_20\"},\"end\":25991,\"start\":25956},{\"attributes\":{\"id\":\"formula_21\"},\"end\":26190,\"start\":26157},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26219,\"start\":26190},{\"attributes\":{\"id\":\"formula_23\"},\"end\":27106,\"start\":27071},{\"attributes\":{\"id\":\"formula_24\"},\"end\":27133,\"start\":27106},{\"attributes\":{\"id\":\"formula_25\"},\"end\":27830,\"start\":27785},{\"attributes\":{\"id\":\"formula_26\"},\"end\":28682,\"start\":28672},{\"attributes\":{\"id\":\"formula_27\"},\"end\":30973,\"start\":30944},{\"attributes\":{\"id\":\"formula_28\"},\"end\":31291,\"start\":31226}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28670,\"start\":28663},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33629,\"start\":33622}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2841,\"start\":2829},{\"attributes\":{\"n\":\"2\"},\"end\":8202,\"start\":8190},{\"attributes\":{\"n\":\"3\"},\"end\":13121,\"start\":13108},{\"attributes\":{\"n\":\"4\"},\"end\":14300,\"start\":14285},{\"attributes\":{\"n\":\"4.1\"},\"end\":14958,\"start\":14924},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":16147,\"start\":16108},{\"attributes\":{\"n\":\"4.2\"},\"end\":21026,\"start\":20993},{\"attributes\":{\"n\":\"4.3\"},\"end\":23888,\"start\":23866},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":25091,\"start\":25076},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":26575,\"start\":26527},{\"attributes\":{\"n\":\"4.4\"},\"end\":27327,\"start\":27313},{\"attributes\":{\"n\":\"5\"},\"end\":28078,\"start\":28067},{\"attributes\":{\"n\":\"5.1\"},\"end\":28538,\"start\":28522},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":28551,\"start\":28541},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":30707,\"start\":30688},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":31369,\"start\":31352},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":32946,\"start\":32923},{\"attributes\":{\"n\":\"5.2\"},\"end\":33567,\"start\":33545},{\"attributes\":{\"n\":\"5.3\"},\"end\":35882,\"start\":35868},{\"attributes\":{\"n\":\"5.3.3\"},\"end\":36732,\"start\":36688},{\"attributes\":{\"n\":\"5.3.4\"},\"end\":37551,\"start\":37486},{\"attributes\":{\"n\":\"5.3.5\"},\"end\":38053,\"start\":38024},{\"attributes\":{\"n\":\"5.4\"},\"end\":38524,\"start\":38494},{\"attributes\":{\"n\":\"6\"},\"end\":40268,\"start\":40258},{\"end\":41503,\"start\":41493},{\"end\":41558,\"start\":41548},{\"end\":41607,\"start\":41597},{\"end\":41655,\"start\":41645},{\"end\":41735,\"start\":41725},{\"end\":42234,\"start\":42224},{\"end\":42395,\"start\":42385},{\"end\":43857,\"start\":43848},{\"end\":44147,\"start\":44138}]", "table": "[{\"end\":43846,\"start\":43749},{\"end\":44136,\"start\":43885},{\"end\":44386,\"start\":44188}]", "figure_caption": "[{\"end\":41546,\"start\":41505},{\"end\":41595,\"start\":41560},{\"end\":41643,\"start\":41609},{\"end\":41723,\"start\":41657},{\"end\":42222,\"start\":41737},{\"end\":42383,\"start\":42236},{\"end\":43678,\"start\":42397},{\"end\":43749,\"start\":43681},{\"end\":43885,\"start\":43859},{\"end\":44188,\"start\":44149}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3375,\"start\":3367},{\"end\":14479,\"start\":14471},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15447,\"start\":15439},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21224,\"start\":21215},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22129,\"start\":22118},{\"end\":24202,\"start\":24194},{\"end\":26469,\"start\":26461},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33642,\"start\":33634},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35964,\"start\":35956},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36259,\"start\":36251},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36989,\"start\":36981},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37741,\"start\":37733},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38223,\"start\":38215}]", "bib_author_first_name": "[{\"end\":44791,\"start\":44784},{\"end\":44811,\"start\":44802},{\"end\":44823,\"start\":44817},{\"end\":45002,\"start\":44996},{\"end\":45004,\"start\":45003},{\"end\":45017,\"start\":45013},{\"end\":45031,\"start\":45025},{\"end\":45236,\"start\":45227},{\"end\":45248,\"start\":45242},{\"end\":45271,\"start\":45262},{\"end\":45282,\"start\":45277},{\"end\":45284,\"start\":45283},{\"end\":45296,\"start\":45293},{\"end\":45589,\"start\":45584},{\"end\":45600,\"start\":45596},{\"end\":45610,\"start\":45607},{\"end\":45620,\"start\":45615},{\"end\":45633,\"start\":45628},{\"end\":45926,\"start\":45917},{\"end\":45938,\"start\":45932},{\"end\":45968,\"start\":45961},{\"end\":45984,\"start\":45979},{\"end\":46001,\"start\":45995},{\"end\":46018,\"start\":46012},{\"end\":46386,\"start\":46381},{\"end\":46403,\"start\":46395},{\"end\":46417,\"start\":46411},{\"end\":46431,\"start\":46423},{\"end\":46796,\"start\":46791},{\"end\":46807,\"start\":46803},{\"end\":46819,\"start\":46812},{\"end\":46833,\"start\":46826},{\"end\":46843,\"start\":46840},{\"end\":46851,\"start\":46849},{\"end\":46863,\"start\":46857},{\"end\":47135,\"start\":47131},{\"end\":47151,\"start\":47144},{\"end\":47174,\"start\":47166},{\"end\":47176,\"start\":47175},{\"end\":47417,\"start\":47411},{\"end\":47436,\"start\":47426},{\"end\":47689,\"start\":47685},{\"end\":47708,\"start\":47702},{\"end\":47925,\"start\":47920},{\"end\":47938,\"start\":47930},{\"end\":48118,\"start\":48117},{\"end\":48134,\"start\":48129},{\"end\":48329,\"start\":48323},{\"end\":48343,\"start\":48337},{\"end\":48345,\"start\":48344},{\"end\":48357,\"start\":48352},{\"end\":48626,\"start\":48622},{\"end\":48638,\"start\":48632},{\"end\":48652,\"start\":48644},{\"end\":48662,\"start\":48658},{\"end\":48664,\"start\":48663},{\"end\":48683,\"start\":48675},{\"end\":48698,\"start\":48690},{\"end\":49090,\"start\":49082},{\"end\":49098,\"start\":49095},{\"end\":49113,\"start\":49105},{\"end\":49126,\"start\":49120},{\"end\":49135,\"start\":49133},{\"end\":49149,\"start\":49140},{\"end\":49447,\"start\":49440},{\"end\":49706,\"start\":49698},{\"end\":49715,\"start\":49712},{\"end\":49726,\"start\":49723},{\"end\":49739,\"start\":49734},{\"end\":49752,\"start\":49747},{\"end\":50028,\"start\":50023},{\"end\":50040,\"start\":50036},{\"end\":50058,\"start\":50047},{\"end\":50060,\"start\":50059},{\"end\":50406,\"start\":50402},{\"end\":50417,\"start\":50411},{\"end\":50432,\"start\":50423},{\"end\":50440,\"start\":50437},{\"end\":50450,\"start\":50446},{\"end\":50462,\"start\":50456},{\"end\":50476,\"start\":50469},{\"end\":50785,\"start\":50776},{\"end\":50798,\"start\":50792},{\"end\":50810,\"start\":50805},{\"end\":50828,\"start\":50820},{\"end\":51080,\"start\":51071},{\"end\":51094,\"start\":51087},{\"end\":51106,\"start\":51102},{\"end\":51120,\"start\":51115},{\"end\":51365,\"start\":51361},{\"end\":51367,\"start\":51366},{\"end\":51391,\"start\":51376},{\"end\":51405,\"start\":51404},{\"end\":51420,\"start\":51415},{\"end\":51749,\"start\":51745},{\"end\":51761,\"start\":51758},{\"end\":51778,\"start\":51769},{\"end\":51790,\"start\":51786},{\"end\":51803,\"start\":51798},{\"end\":51821,\"start\":51814},{\"end\":51836,\"start\":51830},{\"end\":51852,\"start\":51846},{\"end\":51865,\"start\":51858},{\"end\":51882,\"start\":51878},{\"end\":51896,\"start\":51891},{\"end\":51915,\"start\":51908},{\"end\":51928,\"start\":51922},{\"end\":51942,\"start\":51935},{\"end\":52582,\"start\":52575},{\"end\":52603,\"start\":52593},{\"end\":52623,\"start\":52617},{\"end\":52637,\"start\":52632},{\"end\":52939,\"start\":52932},{\"end\":52957,\"start\":52948},{\"end\":52977,\"start\":52973},{\"end\":53258,\"start\":53255},{\"end\":53271,\"start\":53265},{\"end\":53285,\"start\":53278},{\"end\":53296,\"start\":53292},{\"end\":53310,\"start\":53304},{\"end\":53323,\"start\":53316},{\"end\":53605,\"start\":53593},{\"end\":53622,\"start\":53615},{\"end\":54005,\"start\":54002},{\"end\":54014,\"start\":54011},{\"end\":54024,\"start\":54020},{\"end\":54037,\"start\":54029},{\"end\":54047,\"start\":54043},{\"end\":54058,\"start\":54053},{\"end\":54067,\"start\":54063},{\"end\":54370,\"start\":54368},{\"end\":54384,\"start\":54376},{\"end\":54396,\"start\":54391},{\"end\":54407,\"start\":54401},{\"end\":54417,\"start\":54414},{\"end\":54435,\"start\":54428},{\"end\":54664,\"start\":54658},{\"end\":54678,\"start\":54674},{\"end\":54692,\"start\":54688},{\"end\":54706,\"start\":54701},{\"end\":54723,\"start\":54718},{\"end\":54736,\"start\":54731},{\"end\":54738,\"start\":54737},{\"end\":54752,\"start\":54746},{\"end\":54766,\"start\":54761},{\"end\":55046,\"start\":55041},{\"end\":55060,\"start\":55056},{\"end\":55078,\"start\":55069},{\"end\":55279,\"start\":55274},{\"end\":55298,\"start\":55289},{\"end\":55311,\"start\":55307},{\"end\":55327,\"start\":55320},{\"end\":55585,\"start\":55579},{\"end\":55598,\"start\":55592},{\"end\":55610,\"start\":55604},{\"end\":55622,\"start\":55617},{\"end\":55624,\"start\":55623},{\"end\":55642,\"start\":55636},{\"end\":55659,\"start\":55658},{\"end\":55963,\"start\":55957},{\"end\":55974,\"start\":55970},{\"end\":55986,\"start\":55981},{\"end\":56001,\"start\":55993},{\"end\":56015,\"start\":56006},{\"end\":56017,\"start\":56016},{\"end\":56031,\"start\":56025},{\"end\":56033,\"start\":56032},{\"end\":56418,\"start\":56413},{\"end\":56430,\"start\":56424},{\"end\":56438,\"start\":56435},{\"end\":56450,\"start\":56444},{\"end\":56459,\"start\":56456},{\"end\":56727,\"start\":56723},{\"end\":56737,\"start\":56732},{\"end\":56746,\"start\":56743},{\"end\":56756,\"start\":56751},{\"end\":56769,\"start\":56763},{\"end\":56938,\"start\":56932},{\"end\":56953,\"start\":56947},{\"end\":56968,\"start\":56962},{\"end\":56990,\"start\":56982},{\"end\":57005,\"start\":56999},{\"end\":57030,\"start\":57021},{\"end\":57032,\"start\":57031},{\"end\":57296,\"start\":57291},{\"end\":57308,\"start\":57303},{\"end\":57322,\"start\":57314},{\"end\":57336,\"start\":57329},{\"end\":57351,\"start\":57342},{\"end\":57363,\"start\":57358},{\"end\":57373,\"start\":57370},{\"end\":57671,\"start\":57665},{\"end\":57687,\"start\":57678},{\"end\":57700,\"start\":57693},{\"end\":57711,\"start\":57707},{\"end\":57720,\"start\":57717},{\"end\":57730,\"start\":57726},{\"end\":57742,\"start\":57735},{\"end\":57753,\"start\":57748},{\"end\":57762,\"start\":57759},{\"end\":57770,\"start\":57767}]", "bib_author_last_name": "[{\"end\":44800,\"start\":44792},{\"end\":44815,\"start\":44812},{\"end\":44830,\"start\":44824},{\"end\":45011,\"start\":45005},{\"end\":45023,\"start\":45018},{\"end\":45039,\"start\":45032},{\"end\":45240,\"start\":45237},{\"end\":45260,\"start\":45249},{\"end\":45275,\"start\":45272},{\"end\":45291,\"start\":45285},{\"end\":45300,\"start\":45297},{\"end\":45594,\"start\":45590},{\"end\":45605,\"start\":45601},{\"end\":45613,\"start\":45611},{\"end\":45626,\"start\":45621},{\"end\":45636,\"start\":45634},{\"end\":45930,\"start\":45927},{\"end\":45959,\"start\":45939},{\"end\":45977,\"start\":45969},{\"end\":45993,\"start\":45985},{\"end\":46010,\"start\":46002},{\"end\":46026,\"start\":46019},{\"end\":46034,\"start\":46028},{\"end\":46393,\"start\":46387},{\"end\":46409,\"start\":46404},{\"end\":46421,\"start\":46418},{\"end\":46441,\"start\":46432},{\"end\":46801,\"start\":46797},{\"end\":46810,\"start\":46808},{\"end\":46824,\"start\":46820},{\"end\":46838,\"start\":46834},{\"end\":46847,\"start\":46844},{\"end\":46855,\"start\":46852},{\"end\":46868,\"start\":46864},{\"end\":47142,\"start\":47136},{\"end\":47164,\"start\":47152},{\"end\":47183,\"start\":47177},{\"end\":47424,\"start\":47418},{\"end\":47448,\"start\":47437},{\"end\":47700,\"start\":47690},{\"end\":47720,\"start\":47709},{\"end\":47928,\"start\":47926},{\"end\":47941,\"start\":47939},{\"end\":48127,\"start\":48119},{\"end\":48141,\"start\":48135},{\"end\":48145,\"start\":48143},{\"end\":48335,\"start\":48330},{\"end\":48350,\"start\":48346},{\"end\":48366,\"start\":48358},{\"end\":48630,\"start\":48627},{\"end\":48642,\"start\":48639},{\"end\":48656,\"start\":48653},{\"end\":48673,\"start\":48665},{\"end\":48688,\"start\":48684},{\"end\":48702,\"start\":48699},{\"end\":49093,\"start\":49091},{\"end\":49103,\"start\":49099},{\"end\":49118,\"start\":49114},{\"end\":49131,\"start\":49127},{\"end\":49138,\"start\":49136},{\"end\":49154,\"start\":49150},{\"end\":49460,\"start\":49448},{\"end\":49710,\"start\":49707},{\"end\":49721,\"start\":49716},{\"end\":49732,\"start\":49727},{\"end\":49745,\"start\":49740},{\"end\":49757,\"start\":49753},{\"end\":50034,\"start\":50029},{\"end\":50045,\"start\":50041},{\"end\":50068,\"start\":50061},{\"end\":50409,\"start\":50407},{\"end\":50421,\"start\":50418},{\"end\":50435,\"start\":50433},{\"end\":50444,\"start\":50441},{\"end\":50454,\"start\":50451},{\"end\":50467,\"start\":50463},{\"end\":50479,\"start\":50477},{\"end\":50790,\"start\":50786},{\"end\":50803,\"start\":50799},{\"end\":50818,\"start\":50811},{\"end\":50836,\"start\":50829},{\"end\":51085,\"start\":51081},{\"end\":51100,\"start\":51095},{\"end\":51113,\"start\":51107},{\"end\":51132,\"start\":51121},{\"end\":51374,\"start\":51368},{\"end\":51402,\"start\":51392},{\"end\":51413,\"start\":51406},{\"end\":51424,\"start\":51421},{\"end\":51433,\"start\":51426},{\"end\":51756,\"start\":51750},{\"end\":51767,\"start\":51762},{\"end\":51784,\"start\":51779},{\"end\":51796,\"start\":51791},{\"end\":51812,\"start\":51804},{\"end\":51828,\"start\":51822},{\"end\":51844,\"start\":51837},{\"end\":51856,\"start\":51853},{\"end\":51876,\"start\":51866},{\"end\":51889,\"start\":51883},{\"end\":51906,\"start\":51897},{\"end\":51920,\"start\":51916},{\"end\":51933,\"start\":51929},{\"end\":51949,\"start\":51943},{\"end\":52591,\"start\":52583},{\"end\":52615,\"start\":52604},{\"end\":52630,\"start\":52624},{\"end\":52647,\"start\":52638},{\"end\":52946,\"start\":52940},{\"end\":52971,\"start\":52958},{\"end\":52992,\"start\":52978},{\"end\":53263,\"start\":53259},{\"end\":53276,\"start\":53272},{\"end\":53290,\"start\":53286},{\"end\":53302,\"start\":53297},{\"end\":53314,\"start\":53311},{\"end\":53329,\"start\":53324},{\"end\":53613,\"start\":53606},{\"end\":53634,\"start\":53623},{\"end\":54009,\"start\":54006},{\"end\":54018,\"start\":54015},{\"end\":54027,\"start\":54025},{\"end\":54041,\"start\":54038},{\"end\":54051,\"start\":54048},{\"end\":54061,\"start\":54059},{\"end\":54073,\"start\":54068},{\"end\":54374,\"start\":54371},{\"end\":54389,\"start\":54385},{\"end\":54399,\"start\":54397},{\"end\":54412,\"start\":54408},{\"end\":54426,\"start\":54418},{\"end\":54438,\"start\":54436},{\"end\":54444,\"start\":54440},{\"end\":54672,\"start\":54665},{\"end\":54686,\"start\":54679},{\"end\":54699,\"start\":54693},{\"end\":54716,\"start\":54707},{\"end\":54729,\"start\":54724},{\"end\":54744,\"start\":54739},{\"end\":54759,\"start\":54753},{\"end\":54777,\"start\":54767},{\"end\":55054,\"start\":55047},{\"end\":55067,\"start\":55061},{\"end\":55085,\"start\":55079},{\"end\":55287,\"start\":55280},{\"end\":55305,\"start\":55299},{\"end\":55318,\"start\":55312},{\"end\":55333,\"start\":55328},{\"end\":55590,\"start\":55586},{\"end\":55602,\"start\":55599},{\"end\":55615,\"start\":55611},{\"end\":55634,\"start\":55625},{\"end\":55656,\"start\":55643},{\"end\":55664,\"start\":55660},{\"end\":55668,\"start\":55666},{\"end\":55968,\"start\":55964},{\"end\":55979,\"start\":55975},{\"end\":55991,\"start\":55987},{\"end\":56004,\"start\":56002},{\"end\":56023,\"start\":56018},{\"end\":56038,\"start\":56034},{\"end\":56422,\"start\":56419},{\"end\":56433,\"start\":56431},{\"end\":56442,\"start\":56439},{\"end\":56454,\"start\":56451},{\"end\":56465,\"start\":56460},{\"end\":56730,\"start\":56728},{\"end\":56741,\"start\":56738},{\"end\":56749,\"start\":56747},{\"end\":56761,\"start\":56757},{\"end\":56773,\"start\":56770},{\"end\":56945,\"start\":56939},{\"end\":56960,\"start\":56954},{\"end\":56980,\"start\":56969},{\"end\":56997,\"start\":56991},{\"end\":57019,\"start\":57006},{\"end\":57038,\"start\":57033},{\"end\":57301,\"start\":57297},{\"end\":57312,\"start\":57309},{\"end\":57327,\"start\":57323},{\"end\":57340,\"start\":57337},{\"end\":57356,\"start\":57352},{\"end\":57368,\"start\":57364},{\"end\":57377,\"start\":57374},{\"end\":57676,\"start\":57672},{\"end\":57691,\"start\":57688},{\"end\":57705,\"start\":57701},{\"end\":57715,\"start\":57712},{\"end\":57724,\"start\":57721},{\"end\":57733,\"start\":57731},{\"end\":57746,\"start\":57743},{\"end\":57757,\"start\":57754},{\"end\":57765,\"start\":57763},{\"end\":57774,\"start\":57771}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11212020},\"end\":44975,\"start\":44713},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":46939182},\"end\":45149,\"start\":44977},{\"attributes\":{\"doi\":\"CoRR abs/1606.01865\",\"id\":\"b2\"},\"end\":45510,\"start\":45151},{\"attributes\":{\"doi\":\"CoRR abs/1905.06874\",\"id\":\"b3\"},\"end\":45820,\"start\":45512},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5590763},\"end\":46297,\"start\":45822},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52967399},\"end\":46724,\"start\":46299},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":155099980},\"end\":47073,\"start\":46726},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206741496},\"end\":47347,\"start\":47075},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11810482},\"end\":47659,\"start\":47349},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1915014},\"end\":47855,\"start\":47661},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":196177434},\"end\":48071,\"start\":47857},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6628106},\"end\":48264,\"start\":48073},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":58370896},\"end\":48532,\"start\":48266},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b13\",\"matched_paper_id\":59222677},\"end\":49019,\"start\":48534},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13256064},\"end\":49368,\"start\":49021},{\"attributes\":{\"doi\":\"CoRR abs/1506.00019\",\"id\":\"b15\"},\"end\":49606,\"start\":49370},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":34056542},\"end\":49953,\"start\":49608},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1998416},\"end\":50321,\"start\":49955},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":202537994},\"end\":50686,\"start\":50323},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":196176790},\"end\":51031,\"start\":50688},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17195923},\"end\":51270,\"start\":51033},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53221030},\"end\":51673,\"start\":51272},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202786778},\"end\":52484,\"start\":51675},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10174110},\"end\":52859,\"start\":52486},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207178809},\"end\":53170,\"start\":52861},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":19152001},\"end\":53547,\"start\":53172},{\"attributes\":{\"doi\":\"CoRR abs/1904.01962\",\"id\":\"b26\"},\"end\":53903,\"start\":53549},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":119181611},\"end\":54292,\"start\":53905},{\"attributes\":{\"doi\":\"CoRR abs/1907.12412\",\"id\":\"b28\"},\"end\":54654,\"start\":54294},{\"attributes\":{\"id\":\"b29\"},\"end\":54993,\"start\":54656},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13948549},\"end\":55223,\"start\":54995},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1169492},\"end\":55503,\"start\":55225},{\"attributes\":{\"doi\":\"CoRR abs/1906.08237\",\"id\":\"b32\"},\"end\":55894,\"start\":55505},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6857205},\"end\":56327,\"start\":55896},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":18824300},\"end\":56663,\"start\":56329},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2023817},\"end\":56930,\"start\":56665},{\"attributes\":{\"id\":\"b36\"},\"end\":57209,\"start\":56932},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":19112718},\"end\":57606,\"start\":57211},{\"attributes\":{\"id\":\"b38\"},\"end\":57985,\"start\":57608}]", "bib_title": "[{\"end\":44782,\"start\":44713},{\"end\":44994,\"start\":44977},{\"end\":45915,\"start\":45822},{\"end\":46379,\"start\":46299},{\"end\":46789,\"start\":46726},{\"end\":47129,\"start\":47075},{\"end\":47409,\"start\":47349},{\"end\":47683,\"start\":47661},{\"end\":47918,\"start\":47857},{\"end\":48115,\"start\":48073},{\"end\":48321,\"start\":48266},{\"end\":48620,\"start\":48534},{\"end\":49080,\"start\":49021},{\"end\":49696,\"start\":49608},{\"end\":50021,\"start\":49955},{\"end\":50400,\"start\":50323},{\"end\":50774,\"start\":50688},{\"end\":51069,\"start\":51033},{\"end\":51359,\"start\":51272},{\"end\":51743,\"start\":51675},{\"end\":52573,\"start\":52486},{\"end\":52930,\"start\":52861},{\"end\":53253,\"start\":53172},{\"end\":53591,\"start\":53549},{\"end\":54000,\"start\":53905},{\"end\":55039,\"start\":54995},{\"end\":55272,\"start\":55225},{\"end\":55955,\"start\":55896},{\"end\":56411,\"start\":56329},{\"end\":56721,\"start\":56665},{\"end\":57289,\"start\":57211}]", "bib_author": "[{\"end\":44802,\"start\":44784},{\"end\":44817,\"start\":44802},{\"end\":44832,\"start\":44817},{\"end\":45013,\"start\":44996},{\"end\":45025,\"start\":45013},{\"end\":45041,\"start\":45025},{\"end\":45242,\"start\":45227},{\"end\":45262,\"start\":45242},{\"end\":45277,\"start\":45262},{\"end\":45293,\"start\":45277},{\"end\":45302,\"start\":45293},{\"end\":45596,\"start\":45584},{\"end\":45607,\"start\":45596},{\"end\":45615,\"start\":45607},{\"end\":45628,\"start\":45615},{\"end\":45638,\"start\":45628},{\"end\":45932,\"start\":45917},{\"end\":45961,\"start\":45932},{\"end\":45979,\"start\":45961},{\"end\":45995,\"start\":45979},{\"end\":46012,\"start\":45995},{\"end\":46028,\"start\":46012},{\"end\":46036,\"start\":46028},{\"end\":46395,\"start\":46381},{\"end\":46411,\"start\":46395},{\"end\":46423,\"start\":46411},{\"end\":46443,\"start\":46423},{\"end\":46803,\"start\":46791},{\"end\":46812,\"start\":46803},{\"end\":46826,\"start\":46812},{\"end\":46840,\"start\":46826},{\"end\":46849,\"start\":46840},{\"end\":46857,\"start\":46849},{\"end\":46870,\"start\":46857},{\"end\":47144,\"start\":47131},{\"end\":47166,\"start\":47144},{\"end\":47185,\"start\":47166},{\"end\":47426,\"start\":47411},{\"end\":47450,\"start\":47426},{\"end\":47702,\"start\":47685},{\"end\":47722,\"start\":47702},{\"end\":47930,\"start\":47920},{\"end\":47943,\"start\":47930},{\"end\":48129,\"start\":48117},{\"end\":48143,\"start\":48129},{\"end\":48147,\"start\":48143},{\"end\":48337,\"start\":48323},{\"end\":48352,\"start\":48337},{\"end\":48368,\"start\":48352},{\"end\":48632,\"start\":48622},{\"end\":48644,\"start\":48632},{\"end\":48658,\"start\":48644},{\"end\":48675,\"start\":48658},{\"end\":48690,\"start\":48675},{\"end\":48704,\"start\":48690},{\"end\":49095,\"start\":49082},{\"end\":49105,\"start\":49095},{\"end\":49120,\"start\":49105},{\"end\":49133,\"start\":49120},{\"end\":49140,\"start\":49133},{\"end\":49156,\"start\":49140},{\"end\":49462,\"start\":49440},{\"end\":49712,\"start\":49698},{\"end\":49723,\"start\":49712},{\"end\":49734,\"start\":49723},{\"end\":49747,\"start\":49734},{\"end\":49759,\"start\":49747},{\"end\":50036,\"start\":50023},{\"end\":50047,\"start\":50036},{\"end\":50070,\"start\":50047},{\"end\":50411,\"start\":50402},{\"end\":50423,\"start\":50411},{\"end\":50437,\"start\":50423},{\"end\":50446,\"start\":50437},{\"end\":50456,\"start\":50446},{\"end\":50469,\"start\":50456},{\"end\":50481,\"start\":50469},{\"end\":50792,\"start\":50776},{\"end\":50805,\"start\":50792},{\"end\":50820,\"start\":50805},{\"end\":50838,\"start\":50820},{\"end\":51087,\"start\":51071},{\"end\":51102,\"start\":51087},{\"end\":51115,\"start\":51102},{\"end\":51134,\"start\":51115},{\"end\":51376,\"start\":51361},{\"end\":51404,\"start\":51376},{\"end\":51415,\"start\":51404},{\"end\":51426,\"start\":51415},{\"end\":51435,\"start\":51426},{\"end\":51758,\"start\":51745},{\"end\":51769,\"start\":51758},{\"end\":51786,\"start\":51769},{\"end\":51798,\"start\":51786},{\"end\":51814,\"start\":51798},{\"end\":51830,\"start\":51814},{\"end\":51846,\"start\":51830},{\"end\":51858,\"start\":51846},{\"end\":51878,\"start\":51858},{\"end\":51891,\"start\":51878},{\"end\":51908,\"start\":51891},{\"end\":51922,\"start\":51908},{\"end\":51935,\"start\":51922},{\"end\":51951,\"start\":51935},{\"end\":52593,\"start\":52575},{\"end\":52617,\"start\":52593},{\"end\":52632,\"start\":52617},{\"end\":52649,\"start\":52632},{\"end\":52948,\"start\":52932},{\"end\":52973,\"start\":52948},{\"end\":52994,\"start\":52973},{\"end\":53265,\"start\":53255},{\"end\":53278,\"start\":53265},{\"end\":53292,\"start\":53278},{\"end\":53304,\"start\":53292},{\"end\":53316,\"start\":53304},{\"end\":53331,\"start\":53316},{\"end\":53615,\"start\":53593},{\"end\":53636,\"start\":53615},{\"end\":54011,\"start\":54002},{\"end\":54020,\"start\":54011},{\"end\":54029,\"start\":54020},{\"end\":54043,\"start\":54029},{\"end\":54053,\"start\":54043},{\"end\":54063,\"start\":54053},{\"end\":54075,\"start\":54063},{\"end\":54376,\"start\":54368},{\"end\":54391,\"start\":54376},{\"end\":54401,\"start\":54391},{\"end\":54414,\"start\":54401},{\"end\":54428,\"start\":54414},{\"end\":54440,\"start\":54428},{\"end\":54446,\"start\":54440},{\"end\":54674,\"start\":54658},{\"end\":54688,\"start\":54674},{\"end\":54701,\"start\":54688},{\"end\":54718,\"start\":54701},{\"end\":54731,\"start\":54718},{\"end\":54746,\"start\":54731},{\"end\":54761,\"start\":54746},{\"end\":54779,\"start\":54761},{\"end\":55056,\"start\":55041},{\"end\":55069,\"start\":55056},{\"end\":55087,\"start\":55069},{\"end\":55289,\"start\":55274},{\"end\":55307,\"start\":55289},{\"end\":55320,\"start\":55307},{\"end\":55335,\"start\":55320},{\"end\":55592,\"start\":55579},{\"end\":55604,\"start\":55592},{\"end\":55617,\"start\":55604},{\"end\":55636,\"start\":55617},{\"end\":55658,\"start\":55636},{\"end\":55666,\"start\":55658},{\"end\":55670,\"start\":55666},{\"end\":55970,\"start\":55957},{\"end\":55981,\"start\":55970},{\"end\":55993,\"start\":55981},{\"end\":56006,\"start\":55993},{\"end\":56025,\"start\":56006},{\"end\":56040,\"start\":56025},{\"end\":56424,\"start\":56413},{\"end\":56435,\"start\":56424},{\"end\":56444,\"start\":56435},{\"end\":56456,\"start\":56444},{\"end\":56467,\"start\":56456},{\"end\":56732,\"start\":56723},{\"end\":56743,\"start\":56732},{\"end\":56751,\"start\":56743},{\"end\":56763,\"start\":56751},{\"end\":56775,\"start\":56763},{\"end\":56947,\"start\":56932},{\"end\":56962,\"start\":56947},{\"end\":56982,\"start\":56962},{\"end\":56999,\"start\":56982},{\"end\":57021,\"start\":56999},{\"end\":57040,\"start\":57021},{\"end\":57303,\"start\":57291},{\"end\":57314,\"start\":57303},{\"end\":57329,\"start\":57314},{\"end\":57342,\"start\":57329},{\"end\":57358,\"start\":57342},{\"end\":57370,\"start\":57358},{\"end\":57379,\"start\":57370},{\"end\":57678,\"start\":57665},{\"end\":57693,\"start\":57678},{\"end\":57707,\"start\":57693},{\"end\":57717,\"start\":57707},{\"end\":57726,\"start\":57717},{\"end\":57735,\"start\":57726},{\"end\":57748,\"start\":57735},{\"end\":57759,\"start\":57748},{\"end\":57767,\"start\":57759},{\"end\":57776,\"start\":57767}]", "bib_venue": "[{\"end\":52067,\"start\":52001},{\"end\":44836,\"start\":44832},{\"end\":45049,\"start\":45041},{\"end\":45225,\"start\":45151},{\"end\":45582,\"start\":45512},{\"end\":46046,\"start\":46036},{\"end\":46456,\"start\":46443},{\"end\":46886,\"start\":46870},{\"end\":47197,\"start\":47185},{\"end\":47463,\"start\":47450},{\"end\":47740,\"start\":47722},{\"end\":47951,\"start\":47943},{\"end\":48160,\"start\":48147},{\"end\":48381,\"start\":48368},{\"end\":48755,\"start\":48708},{\"end\":49178,\"start\":49156},{\"end\":49438,\"start\":49370},{\"end\":49767,\"start\":49759},{\"end\":50122,\"start\":50070},{\"end\":50490,\"start\":50481},{\"end\":50846,\"start\":50838},{\"end\":51138,\"start\":51134},{\"end\":51464,\"start\":51435},{\"end\":51958,\"start\":51951},{\"end\":52660,\"start\":52649},{\"end\":53002,\"start\":52994},{\"end\":53335,\"start\":53331},{\"end\":53716,\"start\":53655},{\"end\":54084,\"start\":54075},{\"end\":54366,\"start\":54294},{\"end\":54813,\"start\":54779},{\"end\":55100,\"start\":55087},{\"end\":55339,\"start\":55335},{\"end\":55577,\"start\":55505},{\"end\":56096,\"start\":56040},{\"end\":56471,\"start\":56467},{\"end\":56785,\"start\":56775},{\"end\":57058,\"start\":57040},{\"end\":57383,\"start\":57379},{\"end\":57663,\"start\":57608}]"}}}, "year": 2023, "month": 12, "day": 17}