{"id": 246822591, "updated": "2023-10-05 16:45:00.67", "metadata": {"title": "Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion", "authors": "[{\"first\":\"Gwanghyeon\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Juhyeok\",\"last\":\"Mun\",\"middle\":[]},{\"first\":\"Hyeongjun\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Jemin\",\"last\":\"Hwangbo\",\"middle\":[]}]", "venue": "IEEE Robotics and Automation Letters (Volume: 7, Issue: 2, April 2022)", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In this paper, we propose a locomotion training framework where a control policy and a state estimator are trained concurrently. The framework consists of a policy network which outputs the desired joint positions and a state estimation network which outputs estimates of the robot's states such as the base linear velocity, foot height, and contact probability. We exploit a fast simulation environment to train the networks and the trained networks are transferred to the real robot. The trained policy and state estimator are capable of traversing diverse terrains such as a hill, slippery plate, and bumpy road. We also demonstrate that the learned policy can run at up to 3.75 m/s on normal flat ground and 3.54 m/s on a slippery plate with the coefficient of friction of 0.22.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2202.05481", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ral/JiMKH22", "doi": "10.1109/lra.2022.3151396"}}, "content": {"source": {"pdf_hash": "5d5812c5236c5fc576ba308dd9ccbd573f5dd7ef", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2202.05481v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d37ed2bf3a254a73369458b8d7c79313398dd29b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5d5812c5236c5fc576ba308dd9ccbd573f5dd7ef.txt", "contents": "\nConcurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion\n2 Mar 2022\n\nGwanghyeon Ji \nIEEE Copyright Notice\n\n\nJuhyeok Mun \nIEEE Copyright Notice\n\n\nHyeongjun Kim \nIEEE Copyright Notice\n\n\nJemin Hwangbo \nIEEE Copyright Notice\n\n\nConcurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion\n2 Mar 202210.1109/LRA.2022.3151396This paper has been accepted for publication in IEEE Robotics And Automation Letters (RA-L).\nIn this paper, we propose a locomotion training framework where a control policy and a state estimator are trained concurrently. The framework consists of a policy network which outputs the desired joint positions and a state estimation network which outputs estimates of the robot's states such as the base linear velocity, foot height, and contact probability. We exploit a fast simulation environment to train the networks and the trained networks are transferred to the real robot. The trained policy and state estimator are capable of traversing diverse terrains such as a hill, slippery plate, and bumpy road. We also demonstrate that the learned policy can run at up to 3.75 m/s on normal flat ground and 3.54 m/s on a slippery plate with the coefficient of friction of 0.22.\n\nI. INTRODUCTION\n\nIn recent years, reinforcement learning (RL) has become one of the most popular control approaches for legged robots. For quadrupedal robots, there have been remarkable improvements in learning dynamic locomotion skills. Hwangbo et al. [1] trained control policies for the ANYmal robot [2] for robust and high-speed locomotion while keeping the balance under large disturbances. In the later works [3] and [4], RL-trained policies made a quadrupedal robot traverse over various challenging terrains such as slippery ground, vegetation, and rocky terrain. They trained an encoder which compresses environmental information and enabled effective environmentaware locomotion. Moreover, Peng et al. [5] reproduced agile motions of animals by imitating recorded motion trajectory data. They made the Laikago robot walk and turn at moderate speed.\n\nMore complicated trained behaviors, such as dynamic recovery from a fall [1], [6], have been reported in the literature. These complex behaviors can be composed of a single framework using pre-trained expert networks and a gate neural network, and manifest agile and effective motions [7] on the real robot. Furthermore, RL can be utilized for bipedal robots to climb up stairs [8] or to display diverse locomotion patterns such as standing, walking, and running [9].\n\nThe existing control approaches for quadrupedal locomotion rely on accurately estimated state input [1], [3], [4], [7], [10]- [13]. However, we observed that existing  state estimation algorithms become unreliable [14], [15] on challenging terrains, such as ice and sand. Moreover, many state estimation algorithms, such as the one built in to the Mini Cheetah robot, require gait patterns a priori. Most neural network control policies often do not provide such information because the patterns are learned as well. Alternatively, contact states can be estimated either from dedicated contact sensors or from the model [16], but the former is computationally costly and the latter is prone to permanent damages during foot landing. Therefore, it is desirable to develop a control framework that does not rely on contact information.\n\nIn addition, to walk and run on challenging terrains blind, information about the terrain must be estimated. An analytical method can be employed [10] to estimate part of the information. Alternatively, it can be estimated implicitly using a trained neural network and proprioceptive state history [3], [4]. The proprioceptive state history is useful for estimating both intrinsic robot states and extrinsic environment variables. However, this approach has two different major drawbacks. First, because the latent vectors are not interpretable, they cannot be used in conjunction with other modules that require state information. Second, the encoder training causes a significant computational overhead. An alternative to this approach is to directly estimate observable state variables such as the terrain angle. In our proposed approach, this information is indirectly estimated as a distance from the terrain to the foot.\n\nTo address the aforementioned shortcomings of the existing methods, we present a learning-based state estimation network, which is concurrently trained with the policy network. The efficacy of our method is demonstrated using the Mini Cheetah robot [17], which is a lightweight and highly dynamic quadrupedal robot. Kim et al. [11] reports an MPC-based controller that could make Mini Cheetah run at up to 3.7 m/s on a treadmill and [17] achieves 2.45 m/s in outdoor environments. We hereby report highly dynamic locomotion at 3.74 m/s in various indoor and outdoor environments and robust and reliable locomotion behaviors on a slippery plate, bumpy asphalt road, and hills.\n\nOur main contributions are as follows:\n\n\u2022 We propose a simple end-to-end locomotion learning framework that concurrently trains a control policy and a state estimator.\n\n\u2022 Using the trained networks, we demonstrate dynamic locomotion on slippery terrains and slopes.\n\n\u2022 We share the training details, such as the dynamic randomization and curriculum, so that our work can be reproduced by other researchers.\n\n\nII. METHOD\n\nOur goal is to develop an RL-based control framework that can follow the given velocity command, which consists of desired base linear velocities in the forward and lateral directions, and the desired yaw rate. We assume that the robot is equipped with an Inertial Measurement Unit (IMU) and joint encoders.\n\nAn overview of our control framework is illustrated in Fig. 2. The framework consists of three different neural networks: the estimator, critic, and actor. The estimator network estimates multiple relevant state variables for control using the onboard sensors and feeds them to the actor network which outputs actuator commands. The critic network helps reduce variance in the policy gradient estimate from the RL algorithms. All neural networks are trained in simulation using RaiSim [18].\n\nWe use Proximal Policy Optimization (PPO) [19] for training the actor and critic, and supervised learning for training the estimator network. After a collection of a batch of trajectories in RaiSim, we update all three networks using their corresponding loss function. This process repeats like in the vanilla PPO until the performance metric converges. This concurrent learning of the two networks ensures that the policy network can adapt to the performance characteristics of the estimator network. For example, when the estimation is unreliable due to slippery foot contacts, the policy network will be trained with unreliable state estimates and manifest conservative behaviors.\n\nThe Mini Cheetah robot [17] is the robotic platform used in this work. Its compact size and powerful actuators enable us to tackle difficult tasks such as high-speed locomotion on slippery terrain. In addition, the source code for robot operation is available online 1 , which includes a high-performance locomotion controller [11]. Furthermore, its IMU sensor, 3DMGX5-AHRS made by Lord Corporation, provides not only the linear acceleration and angular velocity but also the estimated orientation based on the extended Kalman filter. Our version of the Mini Cheetah is 1.8 kg heavier and 1 cm longer compared to the original version presented in [17]. Our implementation code for the real robot can be found online 2 .\n\n\nA. Training In Simulation\n\nThe policy is trained on flat terrain in 800 different environments to efficiently collect samples. In each environment, the robot is initialized with highly random initial states as shown in the table I. This helps the robot to recover from unexpected external disturbances such as interactions with humans or sudden changes in terrain parameters. With the probability of 25 %, the robot is initialized with the final state of the previous episode, whereby the robot can learn to overcome sudden changes in the velocity command in the real world. For further improvements, we also train the model in environments where an uneven flat terrain or slopes up to \u00b110 \u2022 are randomly generated. The uneven terrain was created using Perlin noise of the following parameters (fractal octaves = 5, fractal lacunarity = 3.0, fractal gain = 0.45, z-scale = mi n(0.21, 0.21\u00b7t \u22121 ) where t is the number of iteration). To train a policy more effectively, we set up a curriculum where the velocity command in the x-direction (i.e., forward/backward direction) gradually increases over each PPO iteration. At the early stage of training, the linear velocity command in the x-direction is uniformly sampled from U 1 (\u22120.5, 1.0) m/s. This range gradually enlarges up to U 1 (\u22121.75, 3.5) m/s, with the maximum forward command given according to\nV x,max = 1 + 2.5 1 + exp(\u22120.002 \u00b7 (t \u2212 1000)) ,(1)\nwhere t is the number of training iterations. Ten percent of the trajectories are then selected to have a zero velocity command for learning a standing still behavior. We define reward functions and their coefficients as shown in the table II. The reward function is designed for two objectives: to follow the given command and to run in an efficient and natural way. The linear and angular velocity rewards are related to the former objective, and the other rewards are for the latter one. Most of the reward functions are shaped by referring to [1]. Among them, the foot clearance reward is important for a successful simto-real transfer because the relative foot positions to the ground and terrain geometry might be uncertain in some situations. The square-root function in the foot clearance reward is to increase its influence on the policy when the commanded velocity is too low. Airtime reward is designed for controlling swing-stance timing and generating standing still motions. \nr v = k v exp(\u2212||cmd vx y \u2212 V x y || 2 ) Angular velocity r \u03c9 = k \u03c9 exp(\u22121.5(cmd \u03c9z \u2212 \u03c9 z ) 2 ) Airtime r ai r,i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 if stance cmd: k a cl i p(T s,i \u2212 T a,i ,\u22120.3,0.3) else: if T max,i < 0.25 : k a mi n(max(T s,i ,T a,i ),0.2) else: 0 Foot slip r sl i p,i = k sl i p C f ,i ||V f ,x y,i || 2 Foot clearance r cl ,i = k cl ( w p f ,z,i \u2212 w p d e s f ,z ) 2 ||V f ,x y,i || 0.5 Orientation r or i = k or i (ang l e(\u03c6 bod y,z ,\u03c6 w or l d ,z )) 2 Joint torque r \u03c4 = k \u03c4 ||\u03c4|| 2 Joint position r q = k q ||q t \u2212 q nomi nal || 2 Joint speed rq = kq ||q t || 2 Joint acceleration rq = kq ||q t \u2212q t \u22121 || 2 Action smoothness 1 r s1 = k s1 ||q d e s t \u2212 q d e s t \u22121 || 2 Action smoothness 2 r s2 = k s2 ||q d e s t \u2212 2q d e s t \u22121 + q d e s t \u22122 || 2 Base motion r base = k base (0.8V 2 z + 0.2|\u03c9 x | + 0.2|\u03c9 y |) Reward Coefficients k v 3.0 k cl -15.0 kq -6e-4 k \u03c9 3.0 k or i -3.0 kq -0.02 k a 0.3 k \u03c4 -6e-4 k s1 ,k s2 -2.5, -1.2 k sl i p -0.08 k q -0.75 k base -1.5\nIn the table II, cmd is an abbreviation of command and i is an index of the foot. T a,i and T s,i represent the time since last takeoff and touchdown, respectively, while being initialized to zero whenever a transition happens. C f ,i in the foot slip reward is the contact state of each foot. In the foot clearance reward, w p d es f ,z is the desired foot height and it is set to 0.09 m. We define a positive reward sum as r pos = r v +r \u03c9 + 3 i=0 r air,i and a negative reward sum as r neg = 3 i=0 (r sl i p,i + r cl ,i ) + r or i + r \u03c4 + r q + rq + rq + r s1 + r s2 + r base . The total reward is defined as\nr t ot = r pos \u00b7 exp(0.2r neg )(2)\nThis form of a reward function ensures that the resulting reward is always positive and discourages the policy to choose an early termination. Whenever the body of the robot except the knees and feet contacts the environment, the episode terminates and the robot is punished with a reward of -10. Therefore, the policy is trained toward reducing unnecessary collisions.\n\n\nB. Network Architecture\n\nOur neural network structure consists of 3 components: an actor, a critic, and an estimator. All of them are designed as a Multi-Layer Perceptron (MLP) network, with the actor and critic having a [512\u00d7256\u00d764] structure, and the estimator having a [256\u00d7128] structure. An MLP is the simplest neural network structure and computationally more efficient than other memory-based networks such as RNNs. We trained policies in a form of an LSTM but could not find meaningful differences in performance. The actor maps an observation to an action and the critic [20] estimates the value of the current state. The estimator network is to estimate states of the robot such as the base linear velocity. Those values are estimated by taking an observation o t as an input, and fed to the actor. The estimator network is trained using supervised learning with data from the simulation. Both the policy and the estimator are running synchronously at 100 Hz. The network structure is shown in Fig. 2.\n\nOur system takes sensor data as an input, and outputs desired joint positions for each actuator. Our framework still uses an analytical estimate of the gravity vector expressed in the body frame because it is computed by the IMU sensor. Furthermore, the estimation algorithms for the orientation are simple and reliable. Joint velocities are computed on the motor controllers by applying the finite difference method on joint positions.\n\nThe observation tuple is defined as\no t = (\u03c6, \u03c9, q,q, q d es t \u22121 , q d es t \u22122 ,Q hi st ,Q hi st , b p f , cmd) (3)\nwhere \u03c6 and \u03c9 are the base orientation and angular velocity, q andq are the joint positions and velocities, q d es t \u22121 and q d es t \u22122 are the desired joint position targets for two previous time steps, Q hi st andQ hi st are the joint position error history and joint velocity history, b p f is the Cartesian positions of the feet relative to the center of mass expressed in the body frame, and cmd is the given velocity command. The Cartesian foot positions are for observing where the feet are located, and it is known to be helpful for training a policy for complicated systems [21]. For our study, joint state history is selected at t \u2212 0.02s, t \u22120.04s, and t \u22120.06s. For stable learning and control, all observation variables are normalized to have a mean of 0.0 and a standard deviation of 1.0. For the same reason, policy outputs are multiplied by a nominal value and then added to nominal joint positions to obtain the desired joint target distributions. This relationship is expressed as q d es t = q nominal + \u03c3 a a t , where q nominal is the nominal joint positions, which is the same as the standing up configuration, \u03c3 a = 0.1 is a predefined action scaling factor, and a t is an output of the policy network. The desired joint positions are computed at 100 Hz and converted to joint torques by a PD controller module with K p =17 N\u00b7m\u00b7rad \u22121 and K d =0.4 N\u00b7m\u00b7s\u00b7rad \u22121 , at 40 kHz on the real robot.\n\nThe estimator network is designed to predict the state of the robot without utilizing a dedicated estimation algorithm. In this paper, the linear velocity, foot height, and contact probability are estimated. The linear velocity estimate is essential in the following velocity command. By removing the necessity of sophisticated state estimation algorithms, the implementation on the robot becomes much simpler. It also has an advantage that the controllers become robust against inevitable errors of the state estimator. As illustrated in [15], estimation of the linear velocity under highly erroneous environments is vulnerable to foot slips. Our end-to-end neural network structure avoids such a challenge in two ways. First, the estimator network is trained in environments where the feet slip often. Therefore, it can still produce a reasonable estimate of the linear velocity using other sensor information and previous observations. Second, the policy network is trained with imperfect information such that it is aware of possible slippages.\n\nThe idea behind learning foot height and contact probability is to achieve the sufficient foot clearance. Due to the wide range of velocity command and the clearance reward that penalizes high speed, the foot clearances become smaller at low speeds. Foot clearance plays an important role in a sim-to-real transfer because insufficient foot clearance might lead to an early foot landing or tripping while running. We discovered that the reward term alone is not sufficient to learn to maintain sufficient foot clearances. Our solution to this problem is to estimate the foot clearances and feed them directly to the policy network. We note that a learning-based estimator is capable of approximately estimating the foot clearance from the observation. First, foot contact states are obtainable from joint position errors. Second, a terrain slope becomes observable from the foot contact states, orientation, and joint positions. Therefore, as the slope is observable, the estimator network can compute the foot height under the assumption that the terrain is even.\n\n\nC. Dynamics Randomization\n\nDynamics randomization is important for a successful sim-to-real transfer of policies trained in simulation [22]. In our case, the robot controller learned without dynamics randomization exhibits shaky motions when deployed on the real robot. It comes from the fact that kinematic and dynamic parameters such as leg length, actuator positions, and center of mass, are not exactly the same as those used in the simulation. Consequently, this reality gap often makes the robot unable to reach sufficient performance. To eliminate this gap, we randomize several components as follows:\n\n\u2022 observation noise \u2022 motor frictions \u2022 PD controller gains \u2022 foot positions and collision geometry \u2022 ground friction These parameters are randomized at the start of each episode or iteration.\n\nThe observation noise is added during the training phase in the simulation. On the real robot, joint velocity measurement comes from numerical differentiation of the joint positions, which causes errors in the joint velocity observation. Moreover, fluctuation in the logging frequency might lead to a failure in updating the velocity values for a single time step. Such an event corresponds to a delay of 2 milliseconds. Therefore, the joint position and velocity measurements in simulation are randomized to reflect the true distribution of the measurements; they are sampled from U 12 (-0.05, 0.05) rad and U 12 (-0.5, 0.5) rad/s, respectively. For the same reason, a uniformly distributed noise U 4 (-0.03, 0.03), U 4 (-0.03, 0.03) m, and U 3 (-0.1, 0.1) rad/s are added to the base orientation, foot position, and base angular velocity, respectively.\n\nMotor friction is randomized to reflect the differences between actuator units. We chose a conservative range of U 1 (0, 0.3) N\u00b7m for the hip abduction/adduction (HAA) and hip flexion/extension (HFE), and U 1 (0.1, 0.7) N\u00b7m for the knee flexion/extension (KFE). Their measured friction values on the real robot are 0.2, 0.2, and 0.5 N\u00b7m for HAA, HFE, and KFE, respectively. The KFE joints have higher friction because they have an extra belt transmission. The PD controller gains are randomized to mitigate the effects of motor friction and damping. We added a uniform noise of U 1 (-2, 2) N\u00b7m\u00b7rad \u22121 , and U 1 (-0.1, 0.1) N\u00b7m\u00b7s\u00b7rad \u22121 for the position and velocity gains, respectively.\n\nThe foot position and collision geometry are randomized to reduce both effects of measurement errors and the deformation of the rubber feet. The foot position observations are disturbed with a uniform noise of U 1 (-10, 10) mm in the longitudinal direction, U 1 (-5, 5) mm in the lateral direction, and U 1 (-20, 20) mm in the leg length direction. These noises are added to the measured foot positions. The foot sphere radii are randomized to U 1 (6, 10) mm.\n\nFinally, the ground friction was randomized to U 1 (0.4, 1.0). Owing to this randomization, the robot can run not only on very slippery ground but also on the ground with very high friction like asphalt.\n\n\nIII. RESULTS\n\nPart of the results in this section can also be found in the accompanying video.\n\n\nA. Controller Descriptions\n\nTo evaluate the performance of the proposed network structures and analyze how each component affects different performance metrics, we test the following settings:\n\n\u2022 Implicit: As a baseline, the explicit state estimator in our proposed framework is substituted with an implicit estimator as in [3], [4].\n\n\u2022 Sequential: In phase 1, a policy is trained with the ground truth robot states. In phase 2, a state estimator was trained with the final policy of phase 1. For implementation, the estimator replaces the ground truth input.\n\n\u2022 Built-in MPC: The MPC controller described in [17] \u2022 RL-LKF: An RL policy in a single MLP form trained with the linear velocity data from the simulator and uses an LKF state estimator on the real robot.\n\n\u2022 Concurrent: Our proposed control framework trained on relatively flat ground \u2022 Concurrent+Slope: Our proposed control framework trained on randomly generated slopes.\n\nIn addition to the above models, we also created four other network models by excluding one of the three estimated states or all of them (i.e., w\n\n\n/o Linvel Estimator, w/o FootHeight Estimator, w/o Contact Estimator, w/o Estimator) for ablation studies in simulation.\n\nFor comparison of the explicit and implicit estimators, we trained the Implicit model. The implicit estimator follows the framework in [4]. During the phase 1, the encoder takes the observation o t \\(Q hi st ,Q hi st ), linear velocity, foot height, and foot contact as an input. For the adaptation module, the history length of 20 is used and the network consists of 3 layers of 1D CNN. The output dimension of the encoder and adaptation module is 11. All the other settings are the same as our framework. The total training time is 7 hours for phases 1 and 2 altogether.  Fig. 3. All the models were trained until they converged to a stable expected return. After 2500 iterations, which consumed about 800 million samples and 4 hours of training in real-time, they all converged to a stable value. The rewards of the trained models are summarized in the table III.\n\n\nB. Evaluation of the Performance in Simulation\n\nAs shown in Fig. 3 and the table III, the Concurrent model converges to the highest rewards while the w/o Estimator and Implicit models converge to the lowest. Out of the three estimated states, linear velocity estimation plays the most important role in improving the policy. Omitting the linear velocity estimation leads to a significant drop in metrics: the total, linear velocity, and foot clearance rewards. This result proves that the linear velocity is crucial for learning the locomotion of legged robots.\n\nAnother noticeable improvement comes from the foot height estimation. Explicitly estimating the foot height improves the foot clearance of the robot, resulting in a higher foot clearance reward. The effectiveness of the higher foot clearance will be discussed in the Locomotion on rough terrains section.\n\nFoot contact probability estimation makes the least impact on the final performance, but it stabilizes and accelerates learning processes as shown in the total reward graph in Fig. 3.\n\n2) Tracking and Estimation Error: For further investigation on the impact of the estimator network, we tested the Concurrent, w/o Estimator, and Implicit models in simulation. All models were given the same random commands every 20 seconds and for 10 minutes in total while the friction coefficient of the flat ground is kept at 0.6. The performance was only measured for the steadystate errors. The velocity commands are sampled from U 1 (-1.75, 3.5) m/s, U 1 (-1, 1) m/s, and U 1 (-2, 2) rad/s, for V x ,V y , and \u03c9, respectively.\n\nThe result is shown in the table IV. The Concurrent model has the smallest RMS errors for following the given linear velocity. It means that the estimated states help the robot to stabilize itself. Furthermore, the tracking errors of the Implicit model are on a similar level to that of w/o Estimator. From this fact, we suggest that tracking performance does not benefit a lot from utilizing the implicit estimator. Models with the implicit estimator having a latent vector of sizes 8 and 20 showed worse performance than the presented one.\n\nInterestingly, our concurrently trained model performs better than a sequentially trained model. From the ta- ble IV, the Sequential model has slight performance degradation. We hypothesize that this is because the policy trained with a state estimator tends to avoid states where the state estimator becomes unreliable. This problem can be easily solved by training them concurrently as we proposed. Note also that the concurrent training requires only one training dataset, which is more efficient than the sequential training.\n\n\n3) Locomotion on rough terrains:\n\nWe investigated the effectiveness of the foot clearance of the learned models. The foot clearance should be sufficiently high for traversing over the rough terrains. Therefore, in this experiment, we compare the average time to fall on the rough terrains with z-scale of 0.525. Commands are sampled from U 1 (-1, 1) m/s, and U 1 (-1, 1) rad/s where the foot clearance is relatively small.\n\nFrom the table IV, the Concurrent model shows an overwhelming performance compared to the other models. It is robust against a fall due to increased foot clearance. On the other hand, an implicit estimator and a single policy do not have sufficient foot clearance. Eventually, they are easy to fall and have worse locomotion performance.\n\nIn conclusion, we suggest that the explicit estimation of the foot height significantly contributes to locomotion performance.\n\n\n4) Computational Cost:\n\nThe trained estimator has computational benefits over analytical state estimators. Using a single core of Ryzen9 5950x, the estimator network takes 7 \u00b5s for a forward pass, while the linear Kalman filter on the Mini Cheetah consumes 34 \u00b5s. Also, the implicit estimator with 20 history inputs takes 20 \u00b5s which is about three times longer than the simple explicit estimator.  Learning curves of the total reward, linear velocity reward, and foot clearance reward are shown. The linear velocity, foot height, and contact probability are estimated. The Concurrent model has the highest performance and learning stability, while the model without an estimator (w/o Estimator) has the lowest performance.\n\n\nC. Evaluation of the Performance on the Real Robot\n\nWe evaluated the performance of controllers on the real robot in terms of command following, state estimation, the maximum running speed, the maximum traversable slope angle, and foot clearance. The Concurrent+Slope model requires 5000 iterations for convergence due to the challenging terrains. The summary of the performance is shown in the table V.\n\n1) Network Implementation on the Real Robot: For the experiments in the real environments, we compared the five aforementioned control models: Built-in MPC, RL-LKF, Concurrent, and Concurrent+Slope. As described in the Controller Description section, the RL-LKF model requires an LKF state estimator. However, we could not use the built-in contact estimator as it assumes a periodic gait schedule, while our learned policy inherently changes gait patterns over speeds. Therefore, when estimating the contact state of the RL-LKF model for the LKF, we used the proprioceptive touchdown detection method described in [23]. When the difference between the KFE joint position and its desired position is over the threshold of -0.4 rad, we assume that the leg is in contact.\n\n2) Command Following and State Estimation: In the real environments, each controller was tested on normal and slippery ground with a step velocity command of 1.0 m/s, 0.8 m/s, and 2.0 rad/s for linear velocities in x and y directions, and a yaw rate, respectively. The commands continued for 1 second and the robot started from zero commands. The slippery plate covered with boric acid powder has a friction coefficient of 0.22, which is much lower than that of the training environments. The results are shown in the table V. In some cases, the Built-in MPC controller fell and those instances are marked with '-' in the table. On the other hand, RL policies performed robustly against all sudden step inputs in this experiment. The Concurrent model has the best tracking performance, while the Concurrent+Slope model has a slightly higher error possibly due to the fact that the Concurrent model is overfitted to simpler terrains.\n\nWe also recorded the estimation data from the LKF while the Concurrent and Concurrent+Slope models are running. The errors from the estimator networks are written in the parentheses. We note that estimation error does not necessarily lead to a deterioration in tracking performance. We suppose that other observation inputs, such as joint state history, mitigate the effects of the estimation errors so that the concurrent learning framework becomes robust against these errors.\n\n\n3) High-Speed Locomotion:\n\nThe Concurrent+Slope model has the highest maximum speed. We tested each controller repeatedly until the robot fell. The maximum outdoor speed of our model, 3.75 m/s, is comparable to the one reported by Kim et al. [11], who report outdoor speed of over 1.7 m/s and treadmill speed of 3.7 m/s. Our Concurrent+Slope controller is capable of running at 3.54 m/s on a slippery plate with \u00b5 = 0.22. When large foot slippages occurred, the robot recovered quickly, even when the robot is running near the maximum speed. If the command is suddenly set to zero while running, the robot makes a stable pose to stop quickly. We assume that this high performance is achieved owing to two factors: the policy trained on low friction terrains and its independence on a state estimation algorithm. Because our proposed framework is trained to be aware of possible foot slippages, it can be robust against estimation errors.\n\nOn both normal and slippery ground, the Concurrent model exhibits better performance than the Built-in MPC and RL-LKF models. The Built-in MPC model could not reach speeds over 1.7 m/s on the normal ground and was easy to fall on the slippery ground at speed under 1.3 m/s. RL-LKF model also runs at rather conservative speeds lower than 2.2 m/s. On the other hand, all the RL controllers are able to run consistently on all tested terrains as they are trained on terrains with the various friction coefficients.\n\n\n4) Locomotion on Hills:\n\nTraining a policy on slopes with random angles and friction coefficients significantly improves climbing performance. The Concurrent+Slope model is capable of climbing a normal hill up to 19.1 \u2022 , which is steeper than slope angles of the training environments, \u00b110 \u2022 . Also, we note that Concurrent+Slope model can walk up a slippery hill up to 9.0 \u2022 . Although the feet of the robot are slipping on it, the robot managed to climb up the slope by pushing the ground with adequate force. This behavior is partially learned in simulation, but the robot adapts its motion for the much more slippery slope of the friction coefficient of 0.22. For outdoor environments, we demonstrate the performance of our policy on a bumpy and hilly asphalt road. Please refer to the supplementary video for the demonstration.\n\nThe other controllers could not climb up a normal hill with angles steeper than 12.4 \u2022 . Because the RL controllers except for the Concurrent+Slope model are trained only on nearly flat terrains, they have unsatisfactory climbing performance. Although the other controllers' maximum traversable slope angles are on a similar level, they display different locomotion characteristics. The Built-in MPC model has higher foot clearances, but its non-stopping gait makes the robot unstable. The other RL models show relatively lower foot clearances, while their standing-still behavior significantly improves the stability of the robot. 5) Foot Clearance: We evaluated the foot clearance of each controller while the robot is running at 1.0 m/s. The maximum foot clearance is shown in the table V. We could recognize that foot clearance of the Concurrent and Concurrent+Slope models on the real robot is higher than the models without the estimator network, RL-LKF and w/o Estimator. As shown in the simulation test, lower foot clearance hinders stable locomotion on highly uneven terrains. This issue remains equally problematic on the real robot. We discovered that the RL-LKF and w/o Estimator models exhibit lower foot clearance at low speeds. Therefore, we suggest that explicit estimation of the foot clearance is effective for improving the performance. 6) Contact Estimation: Our proposed estimator network outputs contact probability for each foot. In Fig. 4, the contact state probability of the front right foot is drawn with the real contact state, KFE joint position error and joint velocity. The estimated contacts are shifted by 0.04 seconds from the actual contacts, which corresponds to 3 control steps excluding 0.01 seconds of communication delay. This is because there is a reality gap due to the compliance of the chain and the rubber feet of the real robot. In addition, the joint history inputs are sparsely sampled with 0.02-second intervals and it introduces further delay in detection. The estimator network detects a contact when the joint abruptly stops by impacts with the ground. The diagram also justifies the threshold of -0.4 rad of joint position error for the contact estimation used for LKF. \n\n\nIV. CONCLUSION\n\nWe presented a framework for concurrent training of a control policy and a state estimator. This framework requires neither an advanced control algorithm nor an accurate state estimation algorithm. Therefore, it requires significantly less effort for implementation on the real-legged system. Furthermore, our concurrent training method outperforms implicit estimation methods and a sequential training method in many aspects such as command tracking, robustness on rough terrain, and training time. To the best of our knowledge, our record is the fastest reported legged locomotion using reinforcement learning. The robot is also able to stably run on a slippery plate even under foot slippages. Although foot slippages often compromise the quality of the state estimation, the concurrently trained policy is robust against such issues.\n\nThe proposed learning-based state estimation can be useful without the control policy in many applications. It can provide reliable state estimates for motion analysis. Furthermore, we expect that the interpretable state outputs from our proposed network can be useful in conjunction with other controllers, such as MPC-based ones.\n\n\nThis work was supported by Samsung Research Funding & Incubation Center for Future Technology at Samsung Electronics under Project Number SRFC-IT2002-02. The Mini Cheetah robot was provided by MIT Biomimetic Robotics Lab and Naver Labs Corporation. * All authors are with Robotics and Artificial Intelligence Lab in the department of Mechanical Engineering, KAIST, Daejeon 34141, Republic of Korea. jhwangbo@kaist.ac.kr\n\nFig. 1 .\n1Dynamic and robust locomotion on a slippery plate of the friction coefficient of 0.22. The Mini Cheetah is traversing over it at the average speed of 3.54 m/s.\n\nFig. 2 .\n2An overall control diagram and a proposed training framework for concurrent training of a control policy and a state estimator are shown. The estimator network takes sensor data o t as an input and outputs state variables, which are the base linear velocity, foot height, and contact probabilities. These estimated states are fed into the policy network together with the observation o t . The estimator network is trained with supervised learning to reduce MSE between the estimated robot states and their corresponding true values obtained from simulation. An actor network is a policy which produces desired joint positions based on the state estimates. Both the critic and actor are trained using the PPO algorithm.\n\nFig. 3 .\n3Fig. 3. Learning curves of the total reward, linear velocity reward, and foot clearance reward are shown. The linear velocity, foot height, and contact probability are estimated. The Concurrent model has the highest performance and learning stability, while the model without an estimator (w/o Estimator) has the lowest performance.\n\nFig. 4 .\n4A contact estimation diagram for the front right foot is shown. KFE joint position error, joint velocity, and contact state are drawn. Contact starts with an abrupt change of the joint velocity by impact with the ground and ends with upward joint motion.\n\nTABLE I\nIINITIAL STATE NOISEState \nNoise \nQuaternion \nU 4 (\u22120.2,0.2), then normalized \nJoint positions \nU 12 (\u22120.2,0.2) rad \nJoint velocities \nU 12 (\u22122.5,2.5) rad/s \nX linear velocity \nU 1 (\u22121,1) m/s \nYZ linear velocities \nU 2 (\u22120.5,0.5) m/s \nAngular velocities \nU 3 (\u22120.7,0.7) rad/s \n\n\n\nTABLE II REWARD FUNCTIONS\nIIFUNCTIONSReward \nExpression \nLinear velocity \n\n\n1 )\n1Learning Performance: First, we compared the performance of the learned controllers (i.e., Concurrent, w/o Linvel Estimator, w/o FootHeight Estimator, w/o Contact Estimator, and w/o Estimator) in simulation. Each network structure is trained four times and their average learning curves are shown in\n\nTABLE III ABLATION\nIIISTUDY FOR THE ESTIMATOR: REWARD OF THE LEARNED MODELSModel \n\nReward \n\nTotal \nLinear \nVelocity \n\nFoot \nClearance \nConcurrent \n4.7212 \n2.7595 \n-0.1338 \nw/o LinVel Estimator \n4.5555 \n2.6643 \n-0.1432 \nw/o FootHeight Estimator \n4.7293 \n2.7625 \n-0.1504 \nw/o Contact Estimator \n4.7125 \n2.7543 \n-0.1382 \nw/o Estimator \n4.4577 \n2.6284 \n-0.1565 \n\nImplicit \n4.439 \n2.611 \n-0.1447 \n\n\n\nTABLE IV TRACKING\nIVAND ESTIMATION ERROR & LOCOMOTION ON ROUGH TERRAINSTask \nModel \nV x [m/s] \nV y [m/s] \n\u03c9 z [rad/s] \n\nCommand \nFollowing \n\nConcurrent \n0.1112 \n0.0708 \n0.1222 \nw/o Estimator \n0.1725 \n0.0959 \n0.1137 \nImplicit \n0.1679 \n0.1233 \n0.1379 \nSequential \n0.1358 \n0.0996 \n0.1201 \nEstimation \nError \n\nConcurrent \n0.0243 \n0.0199 \n-\nSequential \n0.06 \n0.036 \n-\nAverage time to fall [sec] \n\nRough \nTerrain \n\nConcurrent \n85.7 \nw/o Estimator \n25.0 \nImplicit \n30.0 \nSequential \n75.0 \n\n\n\nTABLE V\nVPERFORMANCE OF CONTROLLERS ON THE REAL ROBOTTask \nTerrain \n(friction) \nCommand \nModel \nBuilt-in MPC \nRL-LKF \nw/o Estimator \nConcurrent \nConcurrent+Slope \n\nCommand Following \n: Tracking Error \n\nNormal \n(\u00b5 = 0.6-0.88) \n\nV x [m/s] \n0.3041 \n0.2744 \n0.2635 \n0.2387 \n0.2488 \nV y [m/s] \n-\n0.2031 \n0.2334 \n0.1722 \n0.1817 \n\u03c9 z [rad/s] \n0.462 \n0.2857 \n0.2415 \n0.2427 \n0.2395 \n\nSlippery \n(\u00b5 = 0.22) \n\nV x [m/s] \n-\n0.3255 \n0.2888 \n0.2874 \n0.2532 \nV y [m/s] \n-\n0.285 \n0.2617 \n0.2183 \n0.2446 \n\u03c9 z [rad/s] \n0.525 \n0.3229 \n0.2709 \n0.2504 \n0.2654 \n\nState Estimation \n: Estimation Error \n\nNormal \n(\u00b5 = 0.6-0.88) \n\nV x [m/s] \n0.1869 \n0.0808 \n0.0653 \n0.0783 (0.1069) \n0.0852 (0.0946) \nV y [m/s] \n-\n0.1115 \n0.0666 \n0.1227 (0.0793) \n0.0557 (0.078) \nSlippery \n(\u00b5 = 0.22) \n\nV x [m/s] \n-\n0.1575 \n0.0741 \n0.0533 (0.1209) \n0.1168 (0.1029) \nV y [m/s] \n-\n0.0623 \n0.0612 \n0.0555 (0.0848) \n0.0833 (0.1061) \nMaximum \nAverage Speed \n\nNormal (\u00b5 = 0.6-0.88) [m/s] \n1.72 \n2.18 \n3.20 \n3.33 \n3.75 \nSlippery (\u00b5 = 0.22) [m/s] \n1.34 \n2.19 \n3.14 \n3.25 \n3.54 \n\nMaximum Slope \nNormal (\u00b5 = 0.6-0.88) / \nSlippery (\u00b5 = 0.22) [ \u2022 ] \n12.4 \n12.4 \n9.6 \n12.4 \n19.1 / 9.0 \n\nMaximum \nFoot Height \nNormal [cm] \n5 \n2 \n2 \n3 \n3 \n\n\nhttps://github.com/mit-biomimetics/Cheetah-Software\nhttps://github.com/karlji1021/Cheetah-Software\n\nLearning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 426J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, \"Learning agile and dynamic motor skills for legged robots,\" Science Robotics, vol. 4, no. 26, 2019.\n\nAnymal -a highly mobile and dynamic quadrupedal robot. M Hutter, C Gehring, D Jud, A Lauber, C D Bellicoso, V Tsounis, J Hwangbo, K Bodie, P Fankhauser, M Bloesch, R Diethelm, S Bachmann, A Melzer, M Hoepflinger, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). M. Hutter, C. Gehring, D. Jud, A. Lauber, C. D. Bellicoso, V. Tsounis, J. Hwangbo, K. Bodie, P. Fankhauser, M. Bloesch, R. Diethelm, S. Bachmann, A. Melzer, and M. Hoepflinger, \"Anymal -a highly mobile and dynamic quadrupedal robot,\" in 2016 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems (IROS), 2016, pp. 38-44.\n\nLearning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science Robotics. 547J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, \"Learning quadrupedal locomotion over challenging terrain,\" Sci- ence Robotics, vol. 5, no. 47, 2020.\n\nRma: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, Robotics: Science and Systems Conference. 2021RSS 2021A. Kumar, Z. Fu, D. Pathak, and J. Malik, \"Rma: Rapid motor adaptation for legged robots,\" in Robotics: Science and Systems Conference (RSS 2021), 2021.\n\nLearning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T.-W E Lee, J Tan, S Levine, Robotics: Science and Systems. 072020RSS 2020X. B. Peng, E. Coumans, T. Zhang, T.-W. E. Lee, J. Tan, and S. Levine, \"Learning agile robotic locomotion skills by imitating animals,\" in Robotics: Science and Systems (RSS 2020), 07 2020.\n\nRobust recovery controller for a quadrupedal robot using deep reinforcement learning. J Lee, J Hwangbo, M Hutter, arXiv:1901.07517arXiv preprintJ. Lee, J. Hwangbo, and M. Hutter, \"Robust recovery controller for a quadrupedal robot using deep reinforcement learning,\" arXiv preprint arXiv:1901.07517, 2019.\n\nMulti-expert learning of adaptive legged locomotion. C Yang, K Yuan, Q Zhu, W Yu, Z Li, Science Robotics. 549C. Yang, K. Yuan, Q. Zhu, W. Yu, and Z. Li, \"Multi-expert learning of adaptive legged locomotion,\" Science Robotics, vol. 5, no. 49, 2020.\n\nBlind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, Robotics: Science and Systems Conference. 2021RSS 2021J. Siekmann, K. Green, J. Warila, A. Fern, and J. Hurst, \"Blind bipedal stair traversal via sim-to-real reinforcement learning,\" in Robotics: Science and Systems Conference (RSS 2021), 2021.\n\nSim-to-real learning of all common bipedal gaits via periodic reward composition. J Siekmann, Y Godse, A Fern, J Hurst, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021J. Siekmann, Y. Godse, A. Fern, and J. Hurst, \"Sim-to-real learning of all common bipedal gaits via periodic reward composition,\" in 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021.\n\nDynamic trotting on slopes for quadrupedal robots. C Gehring, C D Bellicoso, S Coros, M Bloesch, P Fankhauser, M Hutter, R Siegwart, 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). C. Gehring, C. D. Bellicoso, S. Coros, M. Bloesch, P. Fankhauser, M. Hutter, and R. Siegwart, \"Dynamic trotting on slopes for quadrupedal robots,\" in 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015, pp. 5129-5135.\n\nHighly dynamic quadruped locomotion via whole-body impulse control and model predictive control. D Kim, J D Carlo, B Katz, G Bledt, S Kim, arXiv:1909.06586v1in arXiv preprintD. Kim, J. D. Carlo, B. Katz, G. Bledt, and S. Kim, \"Highly dynamic quadruped locomotion via whole-body impulse control and model predictive control,\" in arXiv preprint arXiv:1909.06586v1), 2019.\n\nReal-time constrained nonlinear model predictive control on so(3) for dynamic legged locomotion. S Hong, J.-H Kim, H.-W Park, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). S. Hong, J.-H. Kim, and H.-W. Park, \"Real-time constrained non- linear model predictive control on so(3) for dynamic legged loco- motion,\" in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 3982-3989.\n\nRepresentation-free model predictive control for dynamic motions in quadrupeds. Y Ding, A Pandala, C Li, Y.-H Shin, H.-W Park, IEEE Transactions on Robotics. 374Y. Ding, A. Pandala, C. Li, Y.-H. Shin, and H.-W. Park, \"Representation-free model predictive control for dynamic motions in quadrupeds,\" IEEE Transactions on Robotics, vol. 37, no. 4, pp. 1154-1171, 2021.\n\nContactaided invariant extended kalman filtering for robot state estimation. R Hartley, M Ghaffari, R M Eustice, J W Grizzle, The International Journal of Robotics Research. 394R. Hartley, M. Ghaffari, R. M. Eustice, and J. W. Grizzle, \"Contact- aided invariant extended kalman filtering for robot state estima- tion,\" The International Journal of Robotics Research, vol. 39, no. 4, pp. 402-430, 2020.\n\nLegged robot state estimation with dynamic contact event information. J.-H Kim, S Hong, G Ji, S Jeon, J Hwangbo, J.-H Oh, H.-W Park, IEEE Robotics and Automation Letters. 64J.-H. Kim, S. Hong, G. Ji, S. Jeon, J. Hwangbo, J.-H. Oh, and H.-W. Park, \"Legged robot state estimation with dynamic contact event information,\" IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 6733-6740, 2021.\n\nProbabilistic foot contact estimation by fusing information from dynamics and differential/forward kinematics. J Hwangbo, C D Bellicoso, P Fankhauser, M Hutter, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). J. Hwangbo, C. D. Bellicoso, P. Fankhauser, and M. Hutter, \"Proba- bilistic foot contact estimation by fusing information from dynam- ics and differential/forward kinematics,\" in 2016 IEEE/RSJ Interna- tional Conference on Intelligent Robots and Systems (IROS), 2016, pp. 3872-3878.\n\nMini cheetah: A platform for pushing the limits of dynamic quadruped control. B Katz, J D Carlo, S Kim, 2019 IEEE International Conference on Robotics and Automation (ICRA). B. Katz, J. D. Carlo, and S. Kim, \"Mini cheetah: A platform for pushing the limits of dynamic quadruped control,\" in 2019 IEEE International Conference on Robotics and Automation (ICRA), 2019, pp. 6295-6301.\n\nPer-contact iteration method for solving contact dynamics. J Hwangbo, J Lee, M Hutter, IEEE Robotics and Automation Letters. 32J. Hwangbo, J. Lee, and M. Hutter, \"Per-contact iteration method for solving contact dynamics,\" IEEE Robotics and Automation Letters, vol. 3, no. 2, pp. 895-902, 2018.\n\nProximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, abs/1707.06347CoRR. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \"Prox- imal policy optimization algorithms,\" CoRR, vol. abs/1707.06347, 2017.\n\nR S Sutton, A G Barto, Reinforcement Learning: An Introduction. The MIT Press2nd edR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc- tion, 2nd ed. The MIT Press, 2018.\n\nLearning to locomote: Understanding how environment design matters for deep reinforcement learning. D Reda, T Tao, M Van De Panne, Motion, Interaction and Games, ser. MIG '20. New York, NY, USAAssociation for Computing MachineryD. Reda, T. Tao, and M. van de Panne, \"Learning to locomote: Understanding how environment design matters for deep reinforce- ment learning,\" in Motion, Interaction and Games, ser. MIG '20. New York, NY, USA: Association for Computing Machinery, 2020.\n\nSimto-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE International Conference on Robotics and Automation (ICRA). X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, \"Sim- to-real transfer of robotic control with dynamics randomization,\" in 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018.\n\nImplementation of trotto-gallop transition and subsequent gallop on the mit cheetah i. D J Hyun, J Lee, S Park, S Kim, The International Journal of Robotics Research. 3513D. J. Hyun, J. Lee, S. Park, and S. Kim, \"Implementation of trot- to-gallop transition and subsequent gallop on the mit cheetah i,\" The International Journal of Robotics Research, vol. 35, no. 13, pp. 1627-1650, 2016.\n", "annotations": {"author": "[{\"end\":154,\"start\":116},{\"end\":191,\"start\":155},{\"end\":230,\"start\":192},{\"end\":269,\"start\":231}]", "publisher": null, "author_last_name": "[{\"end\":129,\"start\":127},{\"end\":166,\"start\":163},{\"end\":205,\"start\":202},{\"end\":244,\"start\":237}]", "author_first_name": "[{\"end\":126,\"start\":116},{\"end\":162,\"start\":155},{\"end\":201,\"start\":192},{\"end\":236,\"start\":231}]", "author_affiliation": "[{\"end\":153,\"start\":131},{\"end\":190,\"start\":168},{\"end\":229,\"start\":207},{\"end\":268,\"start\":246}]", "title": "[{\"end\":103,\"start\":1},{\"end\":372,\"start\":270}]", "venue": null, "abstract": "[{\"end\":1282,\"start\":500}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1540,\"start\":1537},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1590,\"start\":1587},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1702,\"start\":1699},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1710,\"start\":1707},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1999,\"start\":1996},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2220,\"start\":2217},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2225,\"start\":2222},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2525,\"start\":2522},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2610,\"start\":2607},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2716,\"start\":2713},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2721,\"start\":2718},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2726,\"start\":2723},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2731,\"start\":2728},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2737,\"start\":2733},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2743,\"start\":2739},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2831,\"start\":2827},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2837,\"start\":2833},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3237,\"start\":3233},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3598,\"start\":3594},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3749,\"start\":3746},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3754,\"start\":3751},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4629,\"start\":4625},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4707,\"start\":4703},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4813,\"start\":4809},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6272,\"start\":6268},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6321,\"start\":6317},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6987,\"start\":6983},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7291,\"start\":7287},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7611,\"start\":7607},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9638,\"start\":9635},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12650,\"start\":12646},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14221,\"start\":14217},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15592,\"start\":15588},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17305,\"start\":17301},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20605,\"start\":20602},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20610,\"start\":20607},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20891,\"start\":20887},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21622,\"start\":21619},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27657,\"start\":27653},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29470,\"start\":29466}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35347,\"start\":34926},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35518,\"start\":35348},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36249,\"start\":35519},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36593,\"start\":36250},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36859,\"start\":36594},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37147,\"start\":36860},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37223,\"start\":37148},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37529,\"start\":37224},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37924,\"start\":37530},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38409,\"start\":37925},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39592,\"start\":38410}]", "paragraph": "[{\"end\":2142,\"start\":1301},{\"end\":2611,\"start\":2144},{\"end\":3446,\"start\":2613},{\"end\":4374,\"start\":3448},{\"end\":5051,\"start\":4376},{\"end\":5091,\"start\":5053},{\"end\":5220,\"start\":5093},{\"end\":5318,\"start\":5222},{\"end\":5459,\"start\":5320},{\"end\":5781,\"start\":5474},{\"end\":6273,\"start\":5783},{\"end\":6958,\"start\":6275},{\"end\":7679,\"start\":6960},{\"end\":9035,\"start\":7709},{\"end\":10077,\"start\":9088},{\"end\":11658,\"start\":11047},{\"end\":12063,\"start\":11694},{\"end\":13077,\"start\":12091},{\"end\":13515,\"start\":13079},{\"end\":13552,\"start\":13517},{\"end\":15047,\"start\":13634},{\"end\":16097,\"start\":15049},{\"end\":17163,\"start\":16099},{\"end\":17774,\"start\":17193},{\"end\":17968,\"start\":17776},{\"end\":18824,\"start\":17970},{\"end\":19512,\"start\":18826},{\"end\":19973,\"start\":19514},{\"end\":20178,\"start\":19975},{\"end\":20275,\"start\":20195},{\"end\":20470,\"start\":20306},{\"end\":20611,\"start\":20472},{\"end\":20837,\"start\":20613},{\"end\":21043,\"start\":20839},{\"end\":21212,\"start\":21045},{\"end\":21359,\"start\":21214},{\"end\":22350,\"start\":21484},{\"end\":22914,\"start\":22401},{\"end\":23220,\"start\":22916},{\"end\":23405,\"start\":23222},{\"end\":23939,\"start\":23407},{\"end\":24482,\"start\":23941},{\"end\":25013,\"start\":24484},{\"end\":25438,\"start\":25050},{\"end\":25777,\"start\":25440},{\"end\":25905,\"start\":25779},{\"end\":26631,\"start\":25932},{\"end\":27037,\"start\":26686},{\"end\":27807,\"start\":27039},{\"end\":28741,\"start\":27809},{\"end\":29221,\"start\":28743},{\"end\":30161,\"start\":29251},{\"end\":30675,\"start\":30163},{\"end\":31511,\"start\":30703},{\"end\":33736,\"start\":31513},{\"end\":34592,\"start\":33755},{\"end\":34925,\"start\":34594}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9087,\"start\":9036},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11046,\"start\":10078},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11693,\"start\":11659},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13633,\"start\":13553}]", "table_ref": null, "section_header": "[{\"end\":1299,\"start\":1284},{\"end\":5472,\"start\":5462},{\"end\":7707,\"start\":7682},{\"end\":12089,\"start\":12066},{\"end\":17191,\"start\":17166},{\"end\":20193,\"start\":20181},{\"end\":20304,\"start\":20278},{\"end\":21482,\"start\":21362},{\"end\":22399,\"start\":22353},{\"end\":25048,\"start\":25016},{\"end\":25930,\"start\":25908},{\"end\":26684,\"start\":26634},{\"end\":29249,\"start\":29224},{\"end\":30701,\"start\":30678},{\"end\":33753,\"start\":33739},{\"end\":35357,\"start\":35349},{\"end\":35528,\"start\":35520},{\"end\":36259,\"start\":36251},{\"end\":36603,\"start\":36595},{\"end\":36868,\"start\":36861},{\"end\":37174,\"start\":37149},{\"end\":37228,\"start\":37225},{\"end\":37549,\"start\":37531},{\"end\":37943,\"start\":37926},{\"end\":38418,\"start\":38411}]", "table": "[{\"end\":37147,\"start\":36889},{\"end\":37223,\"start\":37186},{\"end\":37924,\"start\":37606},{\"end\":38409,\"start\":37997},{\"end\":39592,\"start\":38464}]", "figure_caption": "[{\"end\":35347,\"start\":34928},{\"end\":35518,\"start\":35359},{\"end\":36249,\"start\":35530},{\"end\":36593,\"start\":36261},{\"end\":36859,\"start\":36605},{\"end\":36889,\"start\":36870},{\"end\":37529,\"start\":37230},{\"end\":37606,\"start\":37553},{\"end\":37997,\"start\":37946},{\"end\":38464,\"start\":38420}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5844,\"start\":5838},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13076,\"start\":13070},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22064,\"start\":22058},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22419,\"start\":22413},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23404,\"start\":23398},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32975,\"start\":32969}]", "bib_author_first_name": "[{\"end\":39753,\"start\":39752},{\"end\":39764,\"start\":39763},{\"end\":39771,\"start\":39770},{\"end\":39786,\"start\":39785},{\"end\":39799,\"start\":39798},{\"end\":39810,\"start\":39809},{\"end\":39820,\"start\":39819},{\"end\":40096,\"start\":40095},{\"end\":40106,\"start\":40105},{\"end\":40117,\"start\":40116},{\"end\":40124,\"start\":40123},{\"end\":40134,\"start\":40133},{\"end\":40136,\"start\":40135},{\"end\":40149,\"start\":40148},{\"end\":40160,\"start\":40159},{\"end\":40171,\"start\":40170},{\"end\":40180,\"start\":40179},{\"end\":40194,\"start\":40193},{\"end\":40205,\"start\":40204},{\"end\":40217,\"start\":40216},{\"end\":40229,\"start\":40228},{\"end\":40239,\"start\":40238},{\"end\":40731,\"start\":40730},{\"end\":40738,\"start\":40737},{\"end\":40749,\"start\":40748},{\"end\":40763,\"start\":40762},{\"end\":40773,\"start\":40772},{\"end\":41015,\"start\":41014},{\"end\":41024,\"start\":41023},{\"end\":41030,\"start\":41029},{\"end\":41040,\"start\":41039},{\"end\":41320,\"start\":41319},{\"end\":41322,\"start\":41321},{\"end\":41330,\"start\":41329},{\"end\":41341,\"start\":41340},{\"end\":41353,\"start\":41349},{\"end\":41355,\"start\":41354},{\"end\":41362,\"start\":41361},{\"end\":41369,\"start\":41368},{\"end\":41701,\"start\":41700},{\"end\":41708,\"start\":41707},{\"end\":41719,\"start\":41718},{\"end\":41975,\"start\":41974},{\"end\":41983,\"start\":41982},{\"end\":41991,\"start\":41990},{\"end\":41998,\"start\":41997},{\"end\":42004,\"start\":42003},{\"end\":42241,\"start\":42240},{\"end\":42253,\"start\":42252},{\"end\":42262,\"start\":42261},{\"end\":42272,\"start\":42271},{\"end\":42280,\"start\":42279},{\"end\":42617,\"start\":42616},{\"end\":42629,\"start\":42628},{\"end\":42638,\"start\":42637},{\"end\":42646,\"start\":42645},{\"end\":42990,\"start\":42989},{\"end\":43001,\"start\":43000},{\"end\":43003,\"start\":43002},{\"end\":43016,\"start\":43015},{\"end\":43025,\"start\":43024},{\"end\":43036,\"start\":43035},{\"end\":43050,\"start\":43049},{\"end\":43060,\"start\":43059},{\"end\":43503,\"start\":43502},{\"end\":43510,\"start\":43509},{\"end\":43512,\"start\":43511},{\"end\":43521,\"start\":43520},{\"end\":43529,\"start\":43528},{\"end\":43538,\"start\":43537},{\"end\":43874,\"start\":43873},{\"end\":43885,\"start\":43881},{\"end\":43895,\"start\":43891},{\"end\":44309,\"start\":44308},{\"end\":44317,\"start\":44316},{\"end\":44328,\"start\":44327},{\"end\":44337,\"start\":44333},{\"end\":44348,\"start\":44344},{\"end\":44674,\"start\":44673},{\"end\":44685,\"start\":44684},{\"end\":44697,\"start\":44696},{\"end\":44699,\"start\":44698},{\"end\":44710,\"start\":44709},{\"end\":44712,\"start\":44711},{\"end\":45073,\"start\":45069},{\"end\":45080,\"start\":45079},{\"end\":45088,\"start\":45087},{\"end\":45094,\"start\":45093},{\"end\":45102,\"start\":45101},{\"end\":45116,\"start\":45112},{\"end\":45125,\"start\":45121},{\"end\":45505,\"start\":45504},{\"end\":45516,\"start\":45515},{\"end\":45518,\"start\":45517},{\"end\":45531,\"start\":45530},{\"end\":45545,\"start\":45544},{\"end\":45998,\"start\":45997},{\"end\":46006,\"start\":46005},{\"end\":46008,\"start\":46007},{\"end\":46017,\"start\":46016},{\"end\":46362,\"start\":46361},{\"end\":46373,\"start\":46372},{\"end\":46380,\"start\":46379},{\"end\":46640,\"start\":46639},{\"end\":46652,\"start\":46651},{\"end\":46662,\"start\":46661},{\"end\":46674,\"start\":46673},{\"end\":46685,\"start\":46684},{\"end\":46858,\"start\":46857},{\"end\":46860,\"start\":46859},{\"end\":46870,\"start\":46869},{\"end\":46872,\"start\":46871},{\"end\":47144,\"start\":47143},{\"end\":47152,\"start\":47151},{\"end\":47159,\"start\":47158},{\"end\":47593,\"start\":47592},{\"end\":47595,\"start\":47594},{\"end\":47603,\"start\":47602},{\"end\":47619,\"start\":47618},{\"end\":47630,\"start\":47629},{\"end\":48005,\"start\":48004},{\"end\":48007,\"start\":48006},{\"end\":48015,\"start\":48014},{\"end\":48022,\"start\":48021},{\"end\":48030,\"start\":48029}]", "bib_author_last_name": "[{\"end\":39761,\"start\":39754},{\"end\":39768,\"start\":39765},{\"end\":39783,\"start\":39772},{\"end\":39796,\"start\":39787},{\"end\":39807,\"start\":39800},{\"end\":39817,\"start\":39811},{\"end\":39827,\"start\":39821},{\"end\":40103,\"start\":40097},{\"end\":40114,\"start\":40107},{\"end\":40121,\"start\":40118},{\"end\":40131,\"start\":40125},{\"end\":40146,\"start\":40137},{\"end\":40157,\"start\":40150},{\"end\":40168,\"start\":40161},{\"end\":40177,\"start\":40172},{\"end\":40191,\"start\":40181},{\"end\":40202,\"start\":40195},{\"end\":40214,\"start\":40206},{\"end\":40226,\"start\":40218},{\"end\":40236,\"start\":40230},{\"end\":40251,\"start\":40240},{\"end\":40735,\"start\":40732},{\"end\":40746,\"start\":40739},{\"end\":40760,\"start\":40750},{\"end\":40770,\"start\":40764},{\"end\":40780,\"start\":40774},{\"end\":41021,\"start\":41016},{\"end\":41027,\"start\":41025},{\"end\":41037,\"start\":41031},{\"end\":41046,\"start\":41041},{\"end\":41327,\"start\":41323},{\"end\":41338,\"start\":41331},{\"end\":41347,\"start\":41342},{\"end\":41359,\"start\":41356},{\"end\":41366,\"start\":41363},{\"end\":41376,\"start\":41370},{\"end\":41705,\"start\":41702},{\"end\":41716,\"start\":41709},{\"end\":41726,\"start\":41720},{\"end\":41980,\"start\":41976},{\"end\":41988,\"start\":41984},{\"end\":41995,\"start\":41992},{\"end\":42001,\"start\":41999},{\"end\":42007,\"start\":42005},{\"end\":42250,\"start\":42242},{\"end\":42259,\"start\":42254},{\"end\":42269,\"start\":42263},{\"end\":42277,\"start\":42273},{\"end\":42286,\"start\":42281},{\"end\":42626,\"start\":42618},{\"end\":42635,\"start\":42630},{\"end\":42643,\"start\":42639},{\"end\":42652,\"start\":42647},{\"end\":42998,\"start\":42991},{\"end\":43013,\"start\":43004},{\"end\":43022,\"start\":43017},{\"end\":43033,\"start\":43026},{\"end\":43047,\"start\":43037},{\"end\":43057,\"start\":43051},{\"end\":43069,\"start\":43061},{\"end\":43507,\"start\":43504},{\"end\":43518,\"start\":43513},{\"end\":43526,\"start\":43522},{\"end\":43535,\"start\":43530},{\"end\":43542,\"start\":43539},{\"end\":43879,\"start\":43875},{\"end\":43889,\"start\":43886},{\"end\":43900,\"start\":43896},{\"end\":44314,\"start\":44310},{\"end\":44325,\"start\":44318},{\"end\":44331,\"start\":44329},{\"end\":44342,\"start\":44338},{\"end\":44353,\"start\":44349},{\"end\":44682,\"start\":44675},{\"end\":44694,\"start\":44686},{\"end\":44707,\"start\":44700},{\"end\":44720,\"start\":44713},{\"end\":45077,\"start\":45074},{\"end\":45085,\"start\":45081},{\"end\":45091,\"start\":45089},{\"end\":45099,\"start\":45095},{\"end\":45110,\"start\":45103},{\"end\":45119,\"start\":45117},{\"end\":45130,\"start\":45126},{\"end\":45513,\"start\":45506},{\"end\":45528,\"start\":45519},{\"end\":45542,\"start\":45532},{\"end\":45552,\"start\":45546},{\"end\":46003,\"start\":45999},{\"end\":46014,\"start\":46009},{\"end\":46021,\"start\":46018},{\"end\":46370,\"start\":46363},{\"end\":46377,\"start\":46374},{\"end\":46387,\"start\":46381},{\"end\":46649,\"start\":46641},{\"end\":46659,\"start\":46653},{\"end\":46671,\"start\":46663},{\"end\":46682,\"start\":46675},{\"end\":46692,\"start\":46686},{\"end\":46867,\"start\":46861},{\"end\":46878,\"start\":46873},{\"end\":47149,\"start\":47145},{\"end\":47156,\"start\":47153},{\"end\":47172,\"start\":47160},{\"end\":47600,\"start\":47596},{\"end\":47616,\"start\":47604},{\"end\":47627,\"start\":47620},{\"end\":47637,\"start\":47631},{\"end\":48012,\"start\":48008},{\"end\":48019,\"start\":48016},{\"end\":48027,\"start\":48023},{\"end\":48034,\"start\":48031}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":58031572},\"end\":40038,\"start\":39693},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":19007565},\"end\":40670,\"start\":40040},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":224828219},\"end\":40965,\"start\":40672},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":235650916},\"end\":41254,\"start\":40967},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":214775281},\"end\":41612,\"start\":41256},{\"attributes\":{\"doi\":\"arXiv:1901.07517\",\"id\":\"b5\"},\"end\":41919,\"start\":41614},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":228083588},\"end\":42168,\"start\":41921},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":234762823},\"end\":42532,\"start\":42170},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":226237257},\"end\":42936,\"start\":42534},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15896090},\"end\":43403,\"start\":42938},{\"attributes\":{\"doi\":\"arXiv:1909.06586v1\",\"id\":\"b10\"},\"end\":43774,\"start\":43405},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":231913777},\"end\":44226,\"start\":43776},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":229331611},\"end\":44594,\"start\":44228},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":126180468},\"end\":44997,\"start\":44596},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":236482156},\"end\":45391,\"start\":44999},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16203754},\"end\":45917,\"start\":45393},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":199541175},\"end\":46300,\"start\":45919},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3742121},\"end\":46596,\"start\":46302},{\"attributes\":{\"doi\":\"abs/1707.06347\",\"id\":\"b18\",\"matched_paper_id\":28695052},\"end\":46855,\"start\":46598},{\"attributes\":{\"id\":\"b19\"},\"end\":47041,\"start\":46857},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":222272181},\"end\":47522,\"start\":47043},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3707478},\"end\":47915,\"start\":47524},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5790290},\"end\":48305,\"start\":47917}]", "bib_title": "[{\"end\":39750,\"start\":39693},{\"end\":40093,\"start\":40040},{\"end\":40728,\"start\":40672},{\"end\":41012,\"start\":40967},{\"end\":41317,\"start\":41256},{\"end\":41972,\"start\":41921},{\"end\":42238,\"start\":42170},{\"end\":42614,\"start\":42534},{\"end\":42987,\"start\":42938},{\"end\":43871,\"start\":43776},{\"end\":44306,\"start\":44228},{\"end\":44671,\"start\":44596},{\"end\":45067,\"start\":44999},{\"end\":45502,\"start\":45393},{\"end\":45995,\"start\":45919},{\"end\":46359,\"start\":46302},{\"end\":46637,\"start\":46598},{\"end\":47141,\"start\":47043},{\"end\":47590,\"start\":47524},{\"end\":48002,\"start\":47917}]", "bib_author": "[{\"end\":39763,\"start\":39752},{\"end\":39770,\"start\":39763},{\"end\":39785,\"start\":39770},{\"end\":39798,\"start\":39785},{\"end\":39809,\"start\":39798},{\"end\":39819,\"start\":39809},{\"end\":39829,\"start\":39819},{\"end\":40105,\"start\":40095},{\"end\":40116,\"start\":40105},{\"end\":40123,\"start\":40116},{\"end\":40133,\"start\":40123},{\"end\":40148,\"start\":40133},{\"end\":40159,\"start\":40148},{\"end\":40170,\"start\":40159},{\"end\":40179,\"start\":40170},{\"end\":40193,\"start\":40179},{\"end\":40204,\"start\":40193},{\"end\":40216,\"start\":40204},{\"end\":40228,\"start\":40216},{\"end\":40238,\"start\":40228},{\"end\":40253,\"start\":40238},{\"end\":40737,\"start\":40730},{\"end\":40748,\"start\":40737},{\"end\":40762,\"start\":40748},{\"end\":40772,\"start\":40762},{\"end\":40782,\"start\":40772},{\"end\":41023,\"start\":41014},{\"end\":41029,\"start\":41023},{\"end\":41039,\"start\":41029},{\"end\":41048,\"start\":41039},{\"end\":41329,\"start\":41319},{\"end\":41340,\"start\":41329},{\"end\":41349,\"start\":41340},{\"end\":41361,\"start\":41349},{\"end\":41368,\"start\":41361},{\"end\":41378,\"start\":41368},{\"end\":41707,\"start\":41700},{\"end\":41718,\"start\":41707},{\"end\":41728,\"start\":41718},{\"end\":41982,\"start\":41974},{\"end\":41990,\"start\":41982},{\"end\":41997,\"start\":41990},{\"end\":42003,\"start\":41997},{\"end\":42009,\"start\":42003},{\"end\":42252,\"start\":42240},{\"end\":42261,\"start\":42252},{\"end\":42271,\"start\":42261},{\"end\":42279,\"start\":42271},{\"end\":42288,\"start\":42279},{\"end\":42628,\"start\":42616},{\"end\":42637,\"start\":42628},{\"end\":42645,\"start\":42637},{\"end\":42654,\"start\":42645},{\"end\":43000,\"start\":42989},{\"end\":43015,\"start\":43000},{\"end\":43024,\"start\":43015},{\"end\":43035,\"start\":43024},{\"end\":43049,\"start\":43035},{\"end\":43059,\"start\":43049},{\"end\":43071,\"start\":43059},{\"end\":43509,\"start\":43502},{\"end\":43520,\"start\":43509},{\"end\":43528,\"start\":43520},{\"end\":43537,\"start\":43528},{\"end\":43544,\"start\":43537},{\"end\":43881,\"start\":43873},{\"end\":43891,\"start\":43881},{\"end\":43902,\"start\":43891},{\"end\":44316,\"start\":44308},{\"end\":44327,\"start\":44316},{\"end\":44333,\"start\":44327},{\"end\":44344,\"start\":44333},{\"end\":44355,\"start\":44344},{\"end\":44684,\"start\":44673},{\"end\":44696,\"start\":44684},{\"end\":44709,\"start\":44696},{\"end\":44722,\"start\":44709},{\"end\":45079,\"start\":45069},{\"end\":45087,\"start\":45079},{\"end\":45093,\"start\":45087},{\"end\":45101,\"start\":45093},{\"end\":45112,\"start\":45101},{\"end\":45121,\"start\":45112},{\"end\":45132,\"start\":45121},{\"end\":45515,\"start\":45504},{\"end\":45530,\"start\":45515},{\"end\":45544,\"start\":45530},{\"end\":45554,\"start\":45544},{\"end\":46005,\"start\":45997},{\"end\":46016,\"start\":46005},{\"end\":46023,\"start\":46016},{\"end\":46372,\"start\":46361},{\"end\":46379,\"start\":46372},{\"end\":46389,\"start\":46379},{\"end\":46651,\"start\":46639},{\"end\":46661,\"start\":46651},{\"end\":46673,\"start\":46661},{\"end\":46684,\"start\":46673},{\"end\":46694,\"start\":46684},{\"end\":46869,\"start\":46857},{\"end\":46880,\"start\":46869},{\"end\":47151,\"start\":47143},{\"end\":47158,\"start\":47151},{\"end\":47174,\"start\":47158},{\"end\":47602,\"start\":47592},{\"end\":47618,\"start\":47602},{\"end\":47629,\"start\":47618},{\"end\":47639,\"start\":47629},{\"end\":48014,\"start\":48004},{\"end\":48021,\"start\":48014},{\"end\":48029,\"start\":48021},{\"end\":48036,\"start\":48029}]", "bib_venue": "[{\"end\":47236,\"start\":47219},{\"end\":39845,\"start\":39829},{\"end\":40332,\"start\":40253},{\"end\":40798,\"start\":40782},{\"end\":41088,\"start\":41048},{\"end\":41407,\"start\":41378},{\"end\":41698,\"start\":41614},{\"end\":42025,\"start\":42009},{\"end\":42328,\"start\":42288},{\"end\":42722,\"start\":42654},{\"end\":43150,\"start\":43071},{\"end\":43500,\"start\":43405},{\"end\":43981,\"start\":43902},{\"end\":44384,\"start\":44355},{\"end\":44768,\"start\":44722},{\"end\":45168,\"start\":45132},{\"end\":45633,\"start\":45554},{\"end\":46091,\"start\":46023},{\"end\":46425,\"start\":46389},{\"end\":46712,\"start\":46708},{\"end\":46919,\"start\":46880},{\"end\":47217,\"start\":47174},{\"end\":47707,\"start\":47639},{\"end\":48082,\"start\":48036}]"}}}, "year": 2023, "month": 12, "day": 17}