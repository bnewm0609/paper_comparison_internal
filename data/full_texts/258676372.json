{"id": 258676372, "updated": "2023-10-05 01:19:33.973", "metadata": {"title": "Generative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval", "authors": "[{\"first\":\"Iain\",\"last\":\"Mackie\",\"middle\":[]},{\"first\":\"Shubham\",\"last\":\"Chatterjee\",\"middle\":[]},{\"first\":\"Jeffrey\",\"last\":\"Dalton\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Pseudo-relevance feedback (PRF) is a classical approach to address lexical mismatch by enriching the query using first-pass retrieval. Moreover, recent work on generative-relevance feedback (GRF) shows that query expansion models using text generated from large language models can improve sparse retrieval without depending on first-pass retrieval effectiveness. This work extends GRF to dense and learned sparse retrieval paradigms with experiments over six standard document ranking benchmarks. We find that GRF improves over comparable PRF techniques by around 10% on both precision and recall-oriented measures. Nonetheless, query analysis shows that GRF and PRF have contrasting benefits, with GRF providing external context not present in first-pass retrieval, whereas PRF grounds the query to the information contained within the target corpus. Thus, we propose combining generative and pseudo-relevance feedback ranking signals to achieve the benefits of both feedback classes, which significantly increases recall over PRF methods on 95% of experiments.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.07477", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-07477", "doi": "10.48550/arxiv.2305.07477"}}, "content": {"source": {"pdf_hash": "f5448f9e6c3d916cb52ad6b9f9eee0ad379914f7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.07477v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d5f597282f69dfc6c9f3219381af0868ef8ff07c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f5448f9e6c3d916cb52ad6b9f9eee0ad379914f7.txt", "contents": "\nGenerative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval\n\n\nIain Mackie i.mackie.1@research.gla.ac.uk \nUniversity of Glasgow\nUniversity of Glasgow\nUniversity of Glasgow\n\n\nShubham Chatterjee shubham.chatterjee@glasgow.ac.uk \nUniversity of Glasgow\nUniversity of Glasgow\nUniversity of Glasgow\n\n\nJeffrey Dalton jeff.dalton@glasgow.ac.uk \nUniversity of Glasgow\nUniversity of Glasgow\nUniversity of Glasgow\n\n\nGenerative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval\nPseudo-Relevance FeedbackText GenerationDocument Retrieval\nPseudo-relevance feedback (PRF) is a classical approach to address lexical mismatch by enriching the query using first-pass retrieval. Moreover, recent work on generative-relevance feedback (GRF) shows that query expansion models using text generated from large language models can improve sparse retrieval without depending on first-pass retrieval effectiveness. This work extends GRF to dense and learned sparse retrieval paradigms with experiments over six standard document ranking benchmarks. We find that GRF improves over comparable PRF techniques by around 10% on both precision and recall-oriented measures. Nonetheless, query analysis shows that GRF and PRF have contrasting benefits, with GRF providing external context not present in first-pass retrieval, whereas PRF grounds the query to the information contained within the target corpus. Thus, we propose combining generative and pseudo-relevance feedback ranking signals to achieve the benefits of both feedback classes, which significantly increases recall over PRF methods on 95% of experiments.\n\nINTRODUCTION\n\nThe traditional approach to address vocabulary mismatch [2] is pseudo-relevance feedback (PRF) [1,31,32,55], where the query is expanded using information from the top-documents in a feedback set obtained using a first-pass retrieval. Several recent approaches leverage dense [19,33,46,53] and learned sparse representation [17] PRF to contextualise the query vector. While PRF often improves recall, its effectiveness hinges on the quality of the first-pass retrieval. Non-relevant documents in the feedback set introduce noise and may push the query off-topic.\n\nRecently work by Mackie et al. [27] proposes generative-relevance feedback (GRF) that use Large Language Models (LLMs) to generate text independent of first-pass retrieval. Specifically, they use an LLM to generate diverse types of text based on the initial query and use these \"generated documents\" as input for term-based expansion models [1]. They experiment using different types of generated content (chain-of-thought reasoning, facts, news articles, etc.) and find that combining multiple generations is best. We build upon this work to extend GRF to dense [19] and learned sparse [17] retrieval, where we encode generated text into dense and learned sparse vectors for query contextualisation.\n\nNonetheless, we conduct query analysis and find that generative and pseudo-relevance feedback have contrasting merits. For example, GRF provides external context not present in first-pass retrieval, i.e. LLMs can explain how the practice of \"clear-cutting\" relates to [ Figure 1 shows our proposed weighted fusion method (PRF+GRF) that combines the ranking signals of PRF and GRF to improve recall-oriented effectiveness. For our experiments, we use six established document ranking benchmarks (see Section 4.1) and the same GRF [27] LLM-generated content for consistency. We find dense and learned sparse retrieval with GRF improves over comparable PRF techniques on nDCG@20 by 9% each, while recall improves for learned sparse by 8% and dense by 11%. Furthermore, we show that combining GRF and PRF significantly improves R@1000.\n\nOur results highlight how LLM-generated content can effectively contextualize queries across search paradigms (sparse, dense, learned sparse), and is consistently more effective than PRF. Furthermore, we show that generative and pseudo-relevant feedback ranking signals are complementary and can improve recall further. To summarize the contributions, we provide the following:\n\n\u2022 We extend generative relevance feedback to dense and learned sparse retrieval, which improves precision and recall over comparable PRF models by around 10%. The issue of vocabulary mismatch holds considerable importance in information retrieval, where a query fails to capture the user's entire information need [2]. Various techniques have been proposed to address this problem, including pseudo-relevance feedback (PRF) [42] that enriches the query with information from a first-pass set of candidate documents. This updated query is then re-issued to produce the second-pass retrieval set. This process has been shown to improve recall, and famous examples include Rocchio [42], KL expansion [55], relevance modelling [31], LCE [32], RM3 [1]. Furthermore, query expansion has been extended to include external corpora [10] and structured knowledge [8,30,49,51]. The emergence of LLMs has shown progress across many different aspects of information retrieval [52]. One of the largest focus areas is dense retrieval [16,21,50], where passages are encoded into vectors and stored offline. Then, at query-time, the similarity is calculated between a query vector and the passage vectors. Vectorbased pseudo-relevance feedback methods [19] use the first-pass vectors to transform the query vector. Examples include, ColBERT PRF [46], ColBERT-TCT PRF [19], and ANCE-PRF [53]. Recently, learned sparse models like SPLADE [12] use BERT [9] and sparse regularization to learn query and document sparse weightings. Similarly, work has leveraged pseudo-relevance feedback with learned sparse representations [17] to improve retrieval effectiveness. By contrast, our dense and learned sparse generative-relevance feedback approaches build feedback document vectors based on LLMgeneration text embeddings.\n\u2022\n\nGenerative-Relevance Feedback\n\nLarge language models have been used to generate text to improve core retrieval effectiveness in several ways [11,13,15,23,24,37,37,40,43,48,54]. For instance, using LLMs for facet generation [24,43] or document expansion via synthetic query generation [37]. Recent work by Bonifacio et al. [3] demonstrates the effectiveness of leveraging GPT3 for few-shot query generation to aid in dataset creation. Furthermore, Liu et al. [23] extracted contextual clues from LLMs, augmenting and merging multiple questions to improve QA effectiveness. For passage ranking, HyDe [13] uses InstructGPT [38] to generate hypothetical document embeddings for dense retrieval. Finally, notable works have focused on leveraging large language models for query-specific reasoning to improve ranking effectiveness [11,40].\n\nRecent work proposes generative-relevance feedback (GRF) [27] that uses LLMs to generate query-specific text independent of firstpass retrieval. Specifically, GPT-3 [4] is prompted to generate ten diverse types of text that act as \"generated documents\" for termbased expansion models [1]. They experiment using different types of generated content (chain-of-thought reasoning, facts, news articles, etc.) and find that aggregating multiple generated documents is most effective. We build upon this work to extend GRF to dense [19] and learned sparse [17] retrieval paradigms, where we encode generated content into dense and learned sparse vectors for query contextualisation. Furthermore, we conduct qualitative and quantitative analyses on the retrieval benefits of GRF versus PRF, proposing a fusion method to combine these complementary ranking signals.\n\n\nMETHOD\n\nIn this section, we extend generative-relevance feedback to dense [18] and learned sparse [12] retrieval paradigms. GRF tackles querydocument lexical mismatch using zero-shot LLM text generation. Unlike traditional PRF approaches for query expansion [1,31,32], GRF is not reliant on first-pass retrieval effectiveness to find feedback documents. Instead, we leverage LLMs [4] to generate zeroshot relevant text content.\n\nFurthermore, based on our analysis in Section 5.3, we propose combining GRF and PRF ranking signals to further improve retrieval effectiveness. Specifically, we find that GRF is useful provides external context and not dependent on first-pass retrieval effectiveness, while PRF grounds the query in corpus-specific information. Therefore, we propose a weighted reciprocal rank fusion method that achieves the benefits of both feedback classes.\n\n\nExtending GRF to Dense and Learned Sparse\n\nWe extend generative relevance feedback to dense and learned sparse retrieval, focusing on the full \"GRF\" variant that combines multiple generated text contents. Formally, for a given query , we want to return a ranked list of document [ 1 , 2 , ..., ] from a corpus . We have a Large Language Model, , that generates\nnumber of documents [ 1 , 2 , ...,\n]. Unlike sparse GRF [27], where text can be aggregated into a single \"generated document\" to create a language model, dense [22] and learned sparse [12] embedding models are limited to a maximum number of input tokens. Thus, we treat each generated document as a separate dense or learned sparse embedding and combined post-embedding.\n\n3.1.1 Sparse GRF. See the work by [27]. We follow similar notation for dense and sparse formulations.\n\n\nDense GRF.\n\nWe adopt the Rocchio PRF approach [19] for dense GRF to allow different weighting of the query vector and the feedback vector. This allows embeddings of LLM-generated text to contextualise the query vector in a controllable way. Specifically, Equation 1 shows that the new vector, \u00ec , is the combination of the original query vector, \u00ec , and mean of the generated document vectors, \u00ec = 1/ \u00d7 ( \u00ec 1 + \u00ec 2 + ... + \u00ec ). We include and to weigh the relative importance of query and GRF vectors.\n\u00ec = \u00ec + \u00ec (1) 3.1.3 Learned Sparse GRF.\nWe draw on prior work combining pseudo-relevance feedback with learned sparse representations [17]. Specifically, we combine our normalised learned sparse representations of the query, ( | ), with the representation of the combined generated documents, ( | ). For ( | ), similar to [34], we normalise each generated document's sparse representation and aggregate them together before normalising again. (original query weight) is a hyperparameter to weigh the relative importance of our generative learned expansion terms, and (number of expansion terms) limits the most probable LLM-generated learned sparse representations. This results in the weightings of the query learned sparse to be re-weighted and new terms added based on the generated documents.\n( | ) = ( | ) + (1 \u2212 ) ( | ), if \u2208 . 0, otherwise.(2)\n\nFusion of GRF and PRF\n\nQuery analysis in Section 5.3 shows that generative and pseudorelevance feedback have different retrieval benefits. Specifically, generative feedback can provide external context without being dependent on first-pass effectiveness, while pseudo-relevance feedback can provide corpus-specific contextualization. Thus, we propose combing these document scoring signals. Specifically, Equation 3 shows our weighted reciprocal rank fusion method (WRRF) (adapted from [5]) that combines our GRF and PRF runs (PRF+GRF).\n\nHere, WRRF uses a scoring formula, ( ), based on the document's rank in a specific run. There is a set of documents to be ranked, a set of rankings , and parameter is included so low-rank document signals do not disappear (default usual 60). We add a hyperparameter , which weights the relative importance of pseudo-relevant document rankings, \u2208 , and (1 \u2212 ) for generative document rankings, \u2208 . This formulation allows us to tune the relative weighting of GRF and PRF across models and datasets in Section 5.4.  [6,7] builds upon the MS MARCO collection and NIST provides judgments pooled to a greater depth, containing 43 topics for DL-19 and 45 topics for DL-20. DL-HARD [28] provides a challenging subset of 50 queries focusing on topics requiring reasoning and having multiple answers.\n( \u2208 ) = \u2211\ufe01 \u2208 1/( + ( )) \u00d7 , if \u2208 . (1 \u2212 ), if \u2208 .(3\nCODEC [29] is a dataset that focuses on the complex information needs of social science researchers, where domain experts (economists, historians, and politicians) generate 42 challenging essay-style topics. CODEC has a focused web corpus of 750k long documents, which includes news (BBC, Reuters, CNBC etc.) and essay-based web content (Brookings, Forbes, eHistory, etc.).\n\n\nIndexing and Evaluation.\n\nFor indexing the corpora we use Pyserini version 0.16.0 [20], removing stopwords and using Porter stemming. We use cross-validation and optimise R@1000 on standard folds for Robust04 [14], CODEC [29], and DL-HARD [28]. On DL-19, we cross-validated on DL-20 and use the average parameters zero-shot on DL-19 (and vice versa for DL-20). We assess the system runs to a run depth of 1,000. Furthermore, because GRF is an initial retrieval model, recall-oriented evaluation is important; Recall@1000 is the primary measure for this paper. Nonetheless, we include MAP and nDCG@10 for an understanding of precision. We evaluate using ir-measures [25] and conduct 95% confidence paired-t-test for significance.\n\n\nGRF Implementation\n\n4.2.1 LLM Generation. For a fairness and reproducible, we use the same generated content as [27]: link.\n\n\nRetrieval and GRF.\n\nWe outline the different sparse, dense and learned sparse implementation details for GRF: BM25+GRF: We use the runs provided by [27]. TCT+GRF: We use the TCT-ColBERT-v2-HNP's [18] model trained on MS MARCO [36]. We shard documents into passages of 10 sentences, encoding the document title within each passage, and use a stride length of 5. A max-passage approach transforms passage scores into document scores. We use the ColBERT-TCT encoder to create the GRF document vectors, and we tune the Rocchio PRF (between 0.1 and 0.9 with a step of 0.1, and (between 0.1 and 0.9 with a step of 0.1). SPLADE+GRF: We use the SPLADE [12] naver/splade-cocondenserensembledistil checkpoint to create a passage index using the same processing as TCT+GRF. We index the term vectors using Pyserini [20] and use their \"impact\" searcher for max-passage aggregation. When combining query or document vectors we normalise the weights of each term. We tune _ (20,40,60,80,100) and _ _ \u210e (between 0.1 and 0.9 with a step of 0.1).\n\n\nComparison Methods\n\nBM25 [41]: Sparse retrieval method where 1 parameter was tuned between 0.1 and 5.0 using a step size of 0.2, while was tuned between 0.1 and 1.0 with a step size of 0.1, as described earlier.\n\nBM25+Relevance Model (RM3) [1]: We tune _ (between 10 and 100 with a step of 10), _ (between 10 and 100 with a step of 10), and _ _ \u210e (between 0.1 and 0.9 with a step of 0.1).\n\nColBERT-TCT (TCT) [22]: is a dense retrieval model incorporating knowledge distillation over ColBERT [16]. We employ TCT-ColBERT-v2-HNP's MS MARCO [36] model and use a max-passage approach to convert our passage runs into document runs. For ColBERT-TCT+PRF (TCT+PRF) [18], we tune Rocchio PRF parameters: \u210e (2,3,5,7,10,17), (between 0.1 and 0.9 with a step of 0.1, and (between 0.1 and 0.9 with a step of 0.1). SPLADE [12]: is a neural retrieval model which learns sparse query and document weightings via the BERT MLM head and sparse regularization. We index the term vectors using Pyserini [20] and use their \"impact\" searcher for max-passage aggregation. For SPLADE+RM3, we tune _ (5,10,15,20,25,30) _ (20,40,60,80,100), and _ _ \u210e (between 0.1 and 0.9 with a step of 0.1).\n\nColBERT [16] & ColBERT+PRF [46]: We use the runs provided by Wang et al. [47], which use pyterrier framework [26]. \n\n\nFusing GRF and PRF runs\n\nWe use weighted reciprocal rank fusion (WRRF) as a simple fusion method. We extend TrecTools [39] RRF [5] implementation and keep = 60 as default. We tune between 0.0 and 1.0 with a step size of 0.1. This hyperparameter does change across datasets and models, although typically within the 0.2-0.5 range, i.e., slightly favouring GRF. Furthermore, we notice is relatively consistent across folds (i.e. +/-0.1 at most). We fuse GRF and PRF runs within the same search paradigm:\n\n\u2022 BM25+PRF+GRF: BM25+RM3 with BM25+GRF.\n\n\u2022 TCT+PRF+GRF: TCT+PRF with TCT+GRF.\n\n\u2022 SPLADE+PRF+GRF: SPLADE+RM3 with SPLADE+GRF.\n\n\nRESULTS & ANALYSIS 5.1 Research Questions\n\n\u2022 RQ1: Does generative-relevance feedback improve the effectiveness of dense and learned sparse models? We extend GRF to new search paradigms and show the effectiveness gains. \u2022 RQ2: What queries does generative-relevance feedback impact versus traditional pseudo-relevance feedback? We conduct query analysis to understand the behaviours of GRF and PRF methods. \u2022 RQ3: Do generative and pseudo-relevance feedback methods have complementary ranking signals? We explore combing GRF and PRF using our fusion method. These results, combined with sparse GRF findings [27], support GRF as an effective and robust query augmentation approach across sparse, dense and learned sparse retrieval paradigms. In the next research question, we conduct query analysis to understand the different behaviour of generated and pseudo-relevant feedback.\n\n\nRQ1: Dense and Learned Sparse GRF\n\n\nRQ2: Query Analysis of Different Feedback\n\nIn this section, we analyse the sparse GRF runs [27], as well are our dense and learned sparse extensions. Figure 2 shows the query difficulty plot stratified by nDCG@10 effectiveness of BM25 on Robust04 titles. We report MAP and also include BM25 with RM3 and GRF expansion. Specifically, this shows the hardest (0-25%) and easiest (75-100%) first-pass queries based on precision. It is noticeable that GRF is better than PRF on the hardest 75% of firstpass queries (0-75% in our chart), with MAP consistently above RM3 expansion. However, on the easiest first-pass queries (75-100% in our chart), RM3 is more effective than GRF. In essence, we show that pseudo-relevance feedback is more effective than generativerelevance feedback when first-pass precision is very high.\n\nFurthermore, we analyse the query helps vs hurts for Robust04 titles, comparing BM25 query effectiveness to RM3 and GRF expansion. For R@1000, RM3 hurts 47 queries and helps 139 queries, while GRF impacts more queries, hurting 53 and helping 150. Interestingly, of the 47 queries that RM3 hurts, 33 (70%) are either improved or unaffected by GRF. Conversely, of the 53 queries where GRF hurts effectiveness, 40 (75%) of those queries are helped or unaffected by RM3 expansion. Again, this highlights that generative and pseudo-relevance feedback affect different queries, suggesting these could have complementary ranking signals.\n\nSpecifically, a hard first-pass topic is 691 on Robust04 descriptions, What are the objections to the practice of \"clear-cutting\". BM25 has a R@1000 of 0.333 and nDCG@10 of 0.000, and RM3 expansion further reduces R@1000 to 0.286 with nDCG@10 unchanged at 0.000. Reviewing the pseudo-relevant documents used for RM3 expansion, these are general documents around the forest industry and expand with terms: [ Lastly, Dense retrieval has been shown to fail on even simple entity-centric queries [44]. Nonetheless, we observe that GRF can inject external context to help improve the effectiveness of this query type. For example, on topic 1103812 of DL-19, who formed the Commonwealth of Independent States, ColBERT-TCT has a R@1000 of 0.425 and nDCG@10 of 0.574, while PRF slightly improves recall to 0.6000 but reduced nDCG@10 to 0.568. However, we find that LLM-generated content explicitly provides entities to answer the question, i.e. discusses the Belavezha Accords and the three founding states, i. Overall, this analysis highlights that generative and pseudorelevance feedback help different profiles of queries, which could suggest they are complementary. Thus, in the next research question, we explore combing the ranking signals of generative and pseudo-relevant feedback. Table 2 shows the effectiveness of PRF, GRF and our weighted reciprocal rank fusion (PRF+GRF) across our three search paradigms (sparse, dense, learned sparse). For sparse, we use BM25 with RM3 expansion as the baseline for significance testing; for dense, we use ColBERT-TCT with PRF; for learned sparse, we use SPLADE with RM3 expansion.\n\n\nRQ3: Combining GRF and PRF\n\nWe find that combining PRF and GRF consistently, often significantly, improves recall across datasets and search paradigms. For example, fusion has the best R@1000 across 14/18 and significantly improves over PRF on 17/18 experiments (4 more than GRF alone). We find consistent improvements in R@1000 across the search paradigms over PRF, with fusion increasing sparse by 6.7%, dense by 8.7%, and learned sparse by 6.9%. Furthermore, fusion shows consistent improvement over GRF, increasing sparse by 2.6%, dense by 2.3%, and learned sparse by 1.0%. Nonetheless, because  To understand the effect of hyperparameter , Figure 3 plots R@1000 of our weighted reciprocal rank fusion method (PRF+GRF) varying , i.e. when is 0.0 this is the equivalent of GRF, 0.5 equates to RRF [5], and 1.0 equates to PRF. We include all six datasets and our three retrieval paradigms (sparse, dense, and learned sparse).\n\nThis graphic highlights that generative and pseudo-relevant feedback methods are complementary. For example, we see R@1000 increases for BM25+PRF+GRF, as approaches 0.3-0.6, highlighting the benefits of combined ranking signals. Although not as large, we see small, consistent improvements in TCT+PRF+GRF and SPLADE+PRF+GRF. Robust04, in particular, seems to benefit from PRF signals, which may be due to the age of the corpus, i.e. generative documents could be \"too new\" for target relevant documents. Nonetheless, there are specific datasets, such as DL-19, where PRF incorporation negatively impacts our highly effective GRF model.\n\nOverall, we show that we can further improve recall by combining the ranking signals of generative and pseudo-relevant feedback models. We show that GRF and PRF are complementary and explore the impact of weighting generative and pseudo-relevant feedback signals across datasets and models.\n\n\nCONCLUSION\n\nWe extend generative-relevance feedback to dense and learned sparse search paradigms. We find that GRF improves over comparable PRF techniques by around 10% on both precision and recalloriented measures. Furthermore, we conduct query analysis and show that generative and pseudo-relevance feedback have contrasting benefits. Specifically, generative feedback can provide external context without being dependent on first-pass retrieval, while pseudo-relevance feedback provides information grounded in the corpus. Based on this, we propose a weighted reciprocal rank fusion method that combines GRF and PRF ranking signals and significantly improves recall over PRF on 95% of experiments. We believe this is beginning of a body of work that will combine document retrieval and LLM generation to improve core retrieval effectiveness.\n\nFigure 1 :\n1PRF+GRF: Combines generative and pseudorelevance feedback to improve ranking effectiveness.\n\nFigure 3 :\n3Impact of on R@1000 of weighted reciprocal rank fusion (WRRF). Where 0.0 is GRF and 1.0 is PRF.\n\n\nhabitat], [climate] and [deforestation]. Conversely, LLMs can generate content not present in relevant documents. For example, PRF performs better on topics that need to be grounded to the corpus, i.e. \"human stampede\" where PRF correctly identifies events contained in relevant documents [Saudi] and [China], versus LLM-generated content that discusses stampedes in [India] and [Kerala] that are not present in relevant documents. Based on this analysis,\n\n\nWe conduct qualitative and quantitative query analysis on PRF and GRF to show these techniques have contrasting benefits.\u2022 We show that GRF is complementary with PRF, and fusion significantly improves R@1000 across 17/18 experiments.arXiv:2305.07477v1 [cs.IR] 12 May 2023 \n\n2 RELATED WORK \n2.1 Pseudo-Relevance Feedback \n\n\n\n\n) Retrieval Corpora. We evaluate using three test collections and six query sets, which provide a diverse evaluation:TREC Robust04[45]: TREC 2004 Robust Track was created to target poorly performing topics. This dataset comprises 249 topics, containing short keyword \"titles\" and longer natural-language \"descriptions\" queries. Scaled relevance judgments are over a newswire collection of 528k long documents (TREC Disks 4 and 5).TREC Deep Learning 19/20/HARD: The MS MARCO document collections[35] consist of queries, web documents, and sparse relevance judgments. The TREC Deep Learning (DL) 19/20 document track4 EXPERIMENTAL SETUP \n4.1 Datasets \n\n4.1.1 \n\nTable 1 :\n1Dense and learned sparse generative relevance feedback. \"+\" indicates significant improvements in dense systems against TCT+PRF and learned sparse systems against SPLADE+RM3, with bold depicting the best system.Robust04 -Title \nRobust04 -Descriptions \nDL-19 \nDL-20 \nDense \nnDCG@10 MAP R@1k nDCG@10 MAP R@1k nDCG@10 MAP R@1k nDCG@10 MAP R@1k \nColBERT \n0.445 \n0.233 0.608 \n0.435 \n0.218 0.605 \n0.634 \n0.320 0.564 \n0.611 \n0.429 0.795 \nColBERT+PRF \n0.467 \n0.272 0.648 \n0.461 \n0.263 0.635 \n0.668 \n0.385 0.625 \n0.615 \n0.489 0.813 \nTCT \n0.466 \n0.233 0.637 \n0.424 \n0.214 0.595 \n0.655 \n0.333 0.638 \n0.600 \n0.412 0.771 \nTCT+PRF \n0.493 \n0.274 0.684 \n0.452 \n0.245 0.628 \n0.670 \n0.378 0.684 \n0.618 \n0.442 0.784 \nTCT+GRF \n0.517 + \n0.276 0.700 + 0.571 + \n0.289 + 0.708 + 0.683 \n0.418 + 0.743 + 0.634 \n0.457 0.812 + \nLearned Sparse \nSPLADE \n0.387 \n0.206 0.660 \n0.426 \n0.230 0.672 \n0.552 \n0.280 0.619 \n0.553 \n0.352 0.779 \nSPLADE+RM3 0.418 \n0.248 0.703 \n0.448 \n0.268 0.715 \n0.566 \n0.328 0.651 \n0.533 \n0.379 0.784 \nSPLADE+GRF \n0.462 + \n0.265 + 0.730 + 0.493 + \n0.276 0.732 + 0.642 + \n0.407 + 0.732 + 0.553 \n0.415 0.839 + \n\n\n\nTable 1\n1For example, we improve R@1000 over PRF between 2-13% and show significant improvements on all datasets. Additionally, GRF improves nDCG@10 between 2-27% and significantly on 3/4 datasets, i.e. DL-19 and Robust04 titles and descriptions. Furthermore, ColBERT-TCT with GRF outperforms the full late interaction ColBERT-PRF model on all datasets and measures, except underperforming recall-oriented measures on DL-20. Similarly, SPLADE with GRF improves R@1000 over PRF between 2-12% and shows significant improvements on all datasets. Additionally, we see improvements in nDCG@10 between 4-13% and significantly on 3/4 datasets, i.e. DL-19 and Robust04 titles and descriptions.shows the effectiveness of generative feedback with dense \nand learned sparse retrieval on Robust04 and DL datasets. We \ntest for significant improvements against ColBERT-TCT PRF for \nour dense table section and SPLADE with RM3 expansion for the \nlearned sparse table section. \nColBERT-TCT with GRF improves, often significantly, across all \ndatasets. \n\n\nforest], [timber], [logging], and [industry]. On the other hand, GRF expansion uses generated content that directly addresses the questions and injects terms about the Figure 2: Query difficulty plot stratified by nDCG@10 of BM25 on Robust04 titles. We show MAP effectiveness of BM25 and BM25 with RM3 and GRF expansion. \"objections\" to clear-cutting. Analysing the different generated documents [27], CoT-Keywords expands with [habitat], [climate] and [deforestation], Facts expands with [flooding], [water], [risk], and News expands with [soil], [erosion], and [environment]. This results in BM25 with GRF expansion increasing R@1000 to 0.while retaining perfect recall. Reviewing the RM3 expansion terms and the relevant documents, we see specific terms that are useful and refer to collection-mention stampedes, i.e., [Saudi], [China], [pilgrimage], and [Calgary]. Conversely, without being grounded in the events covered in the collection, GRF expands with general terms, i.e. [human], [death], [crowd], [panic], [tragedy], or terms relating to events not converted in the collection, i.e. [India], [Kerala], [2011], etc. In this case, the LLM-generated content refers to the Sabarimala Temple stampede, which occurred seven years after the Robust04 corpus.0%-5% 5%-25% 25%-50% 50%-75% 75%-95% 95%-100% \n0.0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\nMean Average Precision \n\nBM25 \nBM25+RM3 \nBM25+GRF \n\n810 and \nnDCG@10 to 0.454. \nIn contrast, an easy query for first-pass retrieval is topic 626 on \nRobust04 titles, human stampede, where BM25 achieves nDCG@10 \nof 0.287 and R@1000 of 1.000, and RM3 expansion improves nDCG@10 \nto 0.517 \n\nTable 2 :\n2The effectiveness of fusing PRF and GRF runs within each search paradigm. \"+\" indicates significant improvements against PRF from the respective search paradigm (i.e., BM25+RM3 for sparse, etc.), and bold depicts best system. GRF has much higher precision effectiveness when compared to PRF, nDCG@10 can be negatively impacted, and we do not see the same gains as recall-oriented evaluation.Robust04 -Title Robust04 -Desc \nCODEC \nDL-19 \nDL-20 \nDL-HARD \nSparse \nMAP \nR@1k \nMAP \nR@1k \nMAP \nR@1k \nMAP \nR@1k \nMAP \nR@1k \nMAP \nR@1k \nBM25+RM3 \n0.292 \n0.777 \n0.278 \n0.750 \n0.239 \n0.816 \n0.383 \n0.745 \n0.418 \n0.825 \n0.190 \n0.787 \nBM25+GRF \n0.307 \n0.788 \n0.318 + 0.776 + 0.285 + 0.830 \n0.441 + 0.797 + 0.486 + 0.879 + 0.241 + 0.817 \nBM25+PRF+GRF \n0.323 + 0.817 + 0.331 + 0.823 + 0.275 + 0.853 + 0.442 + 0.803 + 0.484 + 0.889 + 0.243 + 0.828 + \nDense \nTCT+PRF \n0.274 \n0.684 \n0.245 \n0.628 \n0.239 \n0.757 \n0.378 \n0.684 \n0.442 \n0.784 \n0.228 \n0.745 \nTCT+GRF \n0.276 \n0.700 + 0.289 + 0.708 + 0.261 \n0.821 + 0.418 + 0.743 + 0.457 \n0.812 + 0.228 \n0.757 \nTCT+PRF+GRF \n0.287 + 0.707 + 0.303 + 0.727 + 0.261 \n0.821 + 0.416 + 0.742 + 0.464 + 0.814 + 0.234 \n0.764 + \nLearned Sparse \nSPLADE+RM3 \n0.248 \n0.703 \n0.268 \n0.715 \n0.216 \n0.770 \n0.328 \n0.651 \n0.379 \n0.784 \n0.157 \n0.704 \nSPLADE+GRF \n0.265 + 0.730 + 0.276 \n0.732 + 0.222 \n0.785 \n0.407 + 0.732 + 0.415 \n0.839 + 0.182 \n0.758 + \nSPLADE+PRF+GRF 0.265 + 0.743 + 0.276 + 0.757 + 0.225 \n0.790 \n0.401 + 0.732 + 0.420 + 0.840 + 0.182 \n0.758 + \n\n\nACKNOWLEDGEMENTSThis work is supported by the 2019 Bloomberg Data Science Research Grant and the Engineering and Physical Sciences Research Council grant EP/V025708/1.\nUMass at TREC 2004: Novelty and HARD. Nasreen Abdul-Jaleel, James Allan, Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, D Mark, Courtney Smucker, Wade, Computer Science Department Faculty Publication Series. 189Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. Computer Science Department Faculty Publication Series (2004), 189.\n\nASK for information retrieval: Part I. Background and theory. J Nicholas, Robert N Belkin, Helen M Oddy, Brooks, Journal of documentation. Nicholas J Belkin, Robert N Oddy, and Helen M Brooks. 1982. ASK for information retrieval: Part I. Background and theory. Journal of documentation (1982).\n\nInpars: Unsupervised dataset generation for information retrieval. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Unsupervised dataset generation for information retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2387-2392.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\n\nReciprocal rank fusion outperforms condorcet and individual rank learning methods. V Gordon, Cormack, L A Charles, Stefan Clarke, Buettcher, Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. the 32nd international ACM SIGIR conference on Research and development in information retrievalGordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. 758-759.\n\nOverview of the TREC 2020 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Text REtrieval Conference (TREC). TREC. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning track. In Text REtrieval Conference (TREC). TREC.\n\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M Voorhees, arXiv:2003.07820Overview of the trec 2019 deep learning track. arXiv preprintNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820 (2020).\n\nEntity query feature expansion using knowledge base links. Jeffrey Dalton, Laura Dietz, James Allan, Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. the 37th international ACM SIGIR conference on Research & development in information retrievalJeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. 365-374.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Jill Burstein, Christy Doran, and Thamar Soloriothe 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USALong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa- tional Linguistics, 4171-4186. https://doi.org/10.18653/v1/n19-1423\n\nImproving the estimation of relevance models using large external corpora. Fernando Diaz, Donald Metzler, Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. the 29th annual international ACM SIGIR conference on Research and development in information retrievalFernando Diaz and Donald Metzler. 2006. Improving the estimation of relevance models using large external corpora. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 154- 161.\n\nFernando Ferraretto, Thiago Laitz, Roberto Lotufo, Rodrigo Nogueira, arXiv:2301.10521ExaRanker: Explanation-Augmented Neural Ranker. arXiv preprintFernando Ferraretto, Thiago Laitz, Roberto Lotufo, and Rodrigo Nogueira. 2023. ExaRanker: Explanation-Augmented Neural Ranker. arXiv preprint arXiv:2301.10521 (2023).\n\nSPLADE: Sparse lexical and expansion model for first stage ranking. Thibault Formal, Benjamin Piwowarski, St\u00e9phane Clinchant, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalThibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. SPLADE: Sparse lexical and expansion model for first stage ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2288-2292.\n\n. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, arXiv:2212.104962022. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprintLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).\n\nParameters learned in the comparison of retrieval models using term dependencies. Ir. Samuel Huston, W Bruce Croft, University of MassachusettsSamuel Huston and W Bruce Croft. 2014. Parameters learned in the comparison of retrieval models using term dependencies. Ir, University of Massachusetts (2014).\n\nVitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, arXiv:2301.01820Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. arXiv preprintVitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Mod- els as Efficient Dataset Generators for Information Retrieval. arXiv preprint arXiv:2301.01820 (2023).\n\nColbert: Efficient and effective passage search via contextualized late interaction over bert. Omar Khattab, Matei Zaharia, Proc. of SIGIR. of SIGIROmar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proc. of SIGIR. 39-48.\n\nCarlos Lassance, St\u00e9phane Clinchant, arXiv:2302.12574Naver Labs Europe (SPLADE)@ TREC Deep Learning 2022. arXiv preprintCarlos Lassance and St\u00e9phane Clinchant. 2023. Naver Labs Europe (SPLADE)@ TREC Deep Learning 2022. arXiv preprint arXiv:2302.12574 (2023).\n\nPseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls. Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, G Zuccon, ArXiv abs/2108.11044Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, and G. Zuccon. 2021. Pseudo Relevance Feedback with Deep Language Models and Dense Re- trievers: Successes and Pitfalls. ArXiv abs/2108.11044 (2021).\n\nImproving query representations for dense retrieval with pseudo relevance feedback: A reproducibility study. Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, Guido Zuccon, ECIR 2022Advances in Information Retrieval: 44th European Conference on IR Research. Stavanger, NorwaySpringerProceedings, Part IHang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, and Guido Zuccon. 2022. Improving query representations for dense retrieval with pseudo relevance feedback: A reproducibility study. In Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part I. Springer, 599-612.\n\nPyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python toolkit for reproducible infor- mation retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2356-2362.\n\nJheng-Hong Sheng-Chieh Lin, Jimmy Yang, Lin, arXiv:2010.11386Distilling dense representations for ranking using tightly-coupled teachers. arXiv preprintSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling dense representations for ranking using tightly-coupled teachers. arXiv preprint arXiv:2010.11386 (2020).\n\nIn-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. Jheng-Hong Sheng-Chieh Lin, Jimmy Yang, Lin, Proceedings of the 6th Workshop on Representation Learning for NLP. the 6th Workshop on Representation Learning for NLPSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP- 2021). 163-173.\n\nLinqing Liu, Minghan Li, Jimmy Lin, Sebastian Riedel, arXiv:2210.07093and Pontus Stenetorp. 2022. Query Expansion Using Contextual Clue Sampling with Language Models. arXiv preprintLinqing Liu, Minghan Li, Jimmy Lin, Sebastian Riedel, and Pontus Stenetorp. 2022. Query Expansion Using Contextual Clue Sampling with Language Models. arXiv preprint arXiv:2210.07093 (2022).\n\nSean Macavaney, Craig Macdonald, Roderick Murray-Smith, arXiv:2108.04026and Iadh Ounis. 2021. IntenT5: Search Result Diversification using Causal Language Models. arXiv preprintSean MacAvaney, Craig Macdonald, Roderick Murray-Smith, and Iadh Ounis. 2021. IntenT5: Search Result Diversification using Causal Language Models. arXiv preprint arXiv:2108.04026 (2021).\n\nStreamlining Evaluation with ir-measures. Sean Macavaney, Craig Macdonald, Iadh Ounis, European Conference on Information Retrieval. SpringerSean MacAvaney, Craig Macdonald, and Iadh Ounis. 2022. Streamlining Evalua- tion with ir-measures. In European Conference on Information Retrieval. Springer, 305-310.\n\nPyTerrier: Declarative experimentation in Python from BM25 to dense retrieval. Craig Macdonald, Nicola Tonellotto, Sean Macavaney, Iadh Ounis, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementCraig Macdonald, Nicola Tonellotto, Sean MacAvaney, and Iadh Ounis. 2021. PyTerrier: Declarative experimentation in Python from BM25 to dense retrieval. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 4526-4533.\n\nGenerative Relevance Feedback with Large Language Models. Iain Mackie, Shubham Chatterjee, Jeffrey Dalton, 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. Iain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023. Generative Relevance Feedback with Large Language Models. 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (2023).\n\nHow deep is your learning: The DL-HARD annotated deep learning dataset. Iain Mackie, Jeffrey Dalton, Andrew Yates, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalIain Mackie, Jeffrey Dalton, and Andrew Yates. 2021. How deep is your learn- ing: The DL-HARD annotated deep learning dataset. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2335-2341.\n\nCODEC: Complex Document and Entity Collection. Iain Mackie, Paul Owoicho, Carlos Gemmell, Sophie Fischer, Sean Macavaney, Jeffery Dalton, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalIain Mackie, Paul Owoicho, Carlos Gemmell, Sophie Fischer, Sean MacAvaney, and Jeffery Dalton. 2022. CODEC: Complex Document and Entity Collection. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\nConceptual language models for domain-specific retrieval. Edgar Meij, Dolf Trieschnigg, Maarten De Rijke, Wessel Kraaij, Information Processing & Management. 46Edgar Meij, Dolf Trieschnigg, Maarten De Rijke, and Wessel Kraaij. 2010. Con- ceptual language models for domain-specific retrieval. Information Processing & Management 46, 4 (2010), 448-469.\n\nA markov random field model for term dependencies. Donald Metzler, W Bruce Croft, Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. the 28th annual international ACM SIGIR conference on Research and development in information retrievalDonald Metzler and W Bruce Croft. 2005. A markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. 472-479.\n\nLatent concept expansion using markov random fields. Donald Metzler, W Bruce Croft, Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. the 30th annual international ACM SIGIR conference on Research and development in information retrievalDonald Metzler and W Bruce Croft. 2007. Latent concept expansion using markov random fields. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. 311-318.\n\nCeqe: Contextualized embeddings for query expansion. Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, James Allan, Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event. SpringerProceedings, Part I 43Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, and James Allan. 2021. Ceqe: Contextualized embeddings for query expansion. In Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28-April 1, 2021, Proceedings, Part I 43. Springer, 467-482.\n\nAdapting Learned Sparse Retrieval for Long Documents. Thong Nguyen, Sean Macavaney, Andrew Yates, 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. Thong Nguyen, Sean MacAvaney, and Andrew Yates. 2023. Adapting Learned Sparse Retrieval for Long Documents. 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (2023).\n\nMs marco: A human-generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human-generated machine reading comprehension dataset. (2016).\n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, arXiv:1611.09268v1MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268v1 (2016).\n\nFrom doc2query to docTTTTTquery. Rodrigo Nogueira, Jimmy Lin, A I Epistemic, Online preprint. 6Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to docTTTTTquery. Online preprint 6 (2019).\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730-27744.\n\nTrecTools: an open-source Python library for Information Retrieval practitioners involved in TREC-like campaigns (SIGIR'19). Joao Palotti, Harrisen Scells, Guido Zuccon, ACMJoao Palotti, Harrisen Scells, and Guido Zuccon. 2019. TrecTools: an open-source Python library for Information Retrieval practitioners involved in TREC-like campaigns (SIGIR'19). ACM.\n\nVisconde: Multi-document QA with GPT-3 and Neural Reranking. Jayr Pereira, Robson Fidalgo, Roberto Lotufo, Rodrigo Nogueira, Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023. Dublin, IrelandSpringerProceedings, Part IIJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Rodrigo Nogueira. 2023. Vis- conde: Multi-document QA with GPT-3 and Neural Reranking. In Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part II. Springer, 534-543.\n\nSome simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. E Stephen, Steve Robertson, Walker, SIGIR'94. SpringerStephen E Robertson and Steve Walker. 1994. Some simple effective approxi- mations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR'94. Springer, 232-241.\n\nRelevance feedback in information retrieval. The Smart retrieval system-experiments in automatic document processing. Joseph Rocchio, Joseph Rocchio. 1971. Relevance feedback in information retrieval. The Smart retrieval system-experiments in automatic document processing (1971), 313-323.\n\nRevisiting Open Domain Query Facet Extraction and Generation. Chris Samarinas, Arkin Dharawat, Hamed Zamani, Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval. the 2022 ACM SIGIR International Conference on Theory of Information RetrievalChris Samarinas, Arkin Dharawat, and Hamed Zamani. 2022. Revisiting Open Domain Query Facet Extraction and Generation. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval. 43-50.\n\nSimple Entity-Centric Questions Challenge Dense Retrievers. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, Danqi Chen, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple Entity-Centric Questions Challenge Dense Retrievers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 6138- 6148.\n\nOverview of the TREC 2004 Robust Track. Ellen M Voorhees, Proceedings of the Thirteenth Text REtrieval Conference. the Thirteenth Text REtrieval ConferenceGaithersburg, MarylandEllen M. Voorhees. 2004. Overview of the TREC 2004 Robust Track. In Proceedings of the Thirteenth Text REtrieval Conference (TREC 2004). Gaithersburg, Maryland, 52-69.\n\nColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval. Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis, ACM Transactions on the Web. Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2022. ColBERT- PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval. ACM Transactions on the Web (2022).\n\nColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval. Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis, ACM Transactions on the Web. 17Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2023. ColBERT- PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval. ACM Transactions on the Web 17, 1 (2023), 1-39.\n\nCONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, Gaurav Singh Tomar, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsZeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, and Gaurav Singh Tomar. 2022. CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 10000-10014. https://aclanthology.org/2022.emnlp-main.679\n\nQuery Expansion with Freebase. Chenyan Xiong, Jamie Callan, 10.1145/2808194.2809446Proceedings of the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15). the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15)New York, NY, USAAssociation for Computing MachineryChenyan Xiong and Jamie Callan. 2015. Query Expansion with Freebase. In Proceedings of the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15). Association for Computing Machinery, New York, NY, USA, 111-120. https://doi.org/10.1145/2808194.2809446\n\n. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, N Paul, Junaid Bennett, Arnold Ahmed, Overwijk, n. d.Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, and Arnold Overwijk. [n. d.].\n\nApproximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. International Conference on Learning Representations. Approximate Nearest Neighbor Neg- ative Contrastive Learning for Dense Text Retrieval. In International Conference on Learning Representations.\n\nQuery Dependent Pseudo-Relevance Feedback Based on Wikipedia. Yang Xu, J F Gareth, Bin Jones, Wang, 10.1145/1571941.1571954Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '09). the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '09)New York, NY, USAAssociation for Computing MachineryYang Xu, Gareth J.F. Jones, and Bin Wang. 2009. Query Dependent Pseudo- Relevance Feedback Based on Wikipedia. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '09). Association for Computing Machinery, New York, NY, USA, 59-66. https://doi.org/10.1145/1571941.1571954\n\nPretrained Transformers for Text Ranking: BERT and Beyond. Andrew Yates, Rodrigo Nogueira, Jimmy Lin, Proceedings of the 14th ACM International Conference on Web Search and Data Mining. the 14th ACM International Conference on Web Search and Data MiningAndrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers for Text Ranking: BERT and Beyond. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining. 1154-1156.\n\nImproving Query Representations for Dense Retrieval with Pseudo Relevance Feedback. Hongchien Yu, Chenyan Xiong, Jamie Callan, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementHongChien Yu, Chenyan Xiong, and Jamie Callan. 2021. Improving Query Repre- sentations for Dense Retrieval with Pseudo Relevance Feedback. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 3592-3596.\n\nGenerating clarifying questions for information retrieval. Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, Gord Lueck, TheWebConference. Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck. 2020. Generating clarifying questions for information retrieval. In TheWebCon- ference. 418-428.\n\nModel-based feedback in the language modeling approach to information retrieval. Chengxiang Zhai, John Lafferty, Proceedings of the tenth international conference on Information and knowledge management. the tenth international conference on Information and knowledge managementChengxiang Zhai and John Lafferty. 2001. Model-based feedback in the lan- guage modeling approach to information retrieval. In Proceedings of the tenth international conference on Information and knowledge management. 403-410.\n", "annotations": {"author": "[{\"end\":200,\"start\":90},{\"end\":321,\"start\":201},{\"end\":431,\"start\":322}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":95},{\"end\":219,\"start\":209},{\"end\":336,\"start\":330}]", "author_first_name": "[{\"end\":94,\"start\":90},{\"end\":208,\"start\":201},{\"end\":329,\"start\":322}]", "author_affiliation": "[{\"end\":199,\"start\":133},{\"end\":320,\"start\":254},{\"end\":430,\"start\":364}]", "title": "[{\"end\":87,\"start\":1},{\"end\":518,\"start\":432}]", "venue": null, "abstract": "[{\"end\":1641,\"start\":578}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1716,\"start\":1713},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1755,\"start\":1752},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1758,\"start\":1755},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1761,\"start\":1758},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":1764,\"start\":1761},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1937,\"start\":1933},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1940,\"start\":1937},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1943,\"start\":1940},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1946,\"start\":1943},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1985,\"start\":1981},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2256,\"start\":2252},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2565,\"start\":2562},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2788,\"start\":2784},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2812,\"start\":2808},{\"end\":3192,\"start\":3191},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3456,\"start\":3452},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4452,\"start\":4449},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4563,\"start\":4559},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4817,\"start\":4813},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4836,\"start\":4832},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4862,\"start\":4858},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4872,\"start\":4868},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4881,\"start\":4878},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4962,\"start\":4958},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4991,\"start\":4988},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4994,\"start\":4991},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4997,\"start\":4994},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5000,\"start\":4997},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":5102,\"start\":5098},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5158,\"start\":5154},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5161,\"start\":5158},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5164,\"start\":5161},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5374,\"start\":5370},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5467,\"start\":5463},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5489,\"start\":5485},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":5508,\"start\":5504},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5558,\"start\":5554},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5571,\"start\":5568},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5741,\"start\":5737},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6081,\"start\":6077},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6084,\"start\":6081},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6087,\"start\":6084},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6090,\"start\":6087},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6093,\"start\":6090},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6096,\"start\":6093},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6099,\"start\":6096},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6102,\"start\":6099},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6105,\"start\":6102},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6108,\"start\":6105},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6111,\"start\":6108},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6163,\"start\":6159},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6166,\"start\":6163},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6224,\"start\":6220},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6261,\"start\":6258},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6398,\"start\":6394},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6538,\"start\":6534},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6560,\"start\":6556},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6765,\"start\":6761},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6768,\"start\":6765},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6832,\"start\":6828},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6939,\"start\":6936},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7058,\"start\":7055},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7301,\"start\":7297},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7325,\"start\":7321},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7709,\"start\":7705},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7733,\"start\":7729},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7892,\"start\":7889},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7895,\"start\":7892},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7898,\"start\":7895},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8014,\"start\":8011},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8927,\"start\":8923},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9031,\"start\":9027},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9055,\"start\":9051},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9277,\"start\":9273},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9393,\"start\":9389},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9983,\"start\":9979},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10171,\"start\":10167},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11186,\"start\":11183},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11752,\"start\":11749},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11754,\"start\":11752},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11914,\"start\":11910},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12089,\"start\":12085},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12541,\"start\":12537},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12668,\"start\":12664},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12680,\"start\":12676},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12698,\"start\":12694},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13124,\"start\":13120},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13302,\"start\":13298},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13464,\"start\":13460},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13511,\"start\":13507},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13542,\"start\":13538},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13960,\"start\":13956},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14120,\"start\":14116},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14373,\"start\":14369},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14587,\"start\":14584},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14756,\"start\":14752},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14839,\"start\":14835},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14885,\"start\":14881},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15005,\"start\":15001},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15156,\"start\":15152},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15330,\"start\":15326},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15523,\"start\":15519},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15542,\"start\":15538},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15588,\"start\":15584},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15624,\"start\":15620},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15751,\"start\":15747},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15759,\"start\":15756},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16869,\"start\":16865},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17270,\"start\":17266},{\"end\":19031,\"start\":19030},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19121,\"start\":19117},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21052,\"start\":21049},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24086,\"start\":24082},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24450,\"start\":24446}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23057,\"start\":22953},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23166,\"start\":23058},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":23624,\"start\":23167},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":23949,\"start\":23625},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":24609,\"start\":23950},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":25725,\"start\":24610},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":26764,\"start\":25726},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28404,\"start\":26765},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":29885,\"start\":28405}]", "paragraph": "[{\"end\":2219,\"start\":1657},{\"end\":2921,\"start\":2221},{\"end\":3754,\"start\":2923},{\"end\":4133,\"start\":3756},{\"end\":5932,\"start\":4135},{\"end\":6769,\"start\":5967},{\"end\":7628,\"start\":6771},{\"end\":8058,\"start\":7639},{\"end\":8503,\"start\":8060},{\"end\":8866,\"start\":8549},{\"end\":9237,\"start\":8902},{\"end\":9340,\"start\":9239},{\"end\":9844,\"start\":9355},{\"end\":10641,\"start\":9885},{\"end\":11233,\"start\":10720},{\"end\":12026,\"start\":11235},{\"end\":12452,\"start\":12079},{\"end\":13183,\"start\":12481},{\"end\":13309,\"start\":13206},{\"end\":14341,\"start\":13332},{\"end\":14555,\"start\":14364},{\"end\":14732,\"start\":14557},{\"end\":15509,\"start\":14734},{\"end\":15626,\"start\":15511},{\"end\":16130,\"start\":15654},{\"end\":16171,\"start\":16132},{\"end\":16209,\"start\":16173},{\"end\":16256,\"start\":16211},{\"end\":17136,\"start\":16302},{\"end\":17991,\"start\":17218},{\"end\":18623,\"start\":17993},{\"end\":20246,\"start\":18625},{\"end\":21176,\"start\":20277},{\"end\":21813,\"start\":21178},{\"end\":22105,\"start\":21815},{\"end\":22952,\"start\":22120}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5934,\"start\":5933},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8901,\"start\":8867},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9884,\"start\":9845},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10695,\"start\":10642},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12078,\"start\":12027}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19914,\"start\":19907}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1655,\"start\":1643},{\"attributes\":{\"n\":\"2.2\"},\"end\":5965,\"start\":5936},{\"attributes\":{\"n\":\"3\"},\"end\":7637,\"start\":7631},{\"attributes\":{\"n\":\"3.1\"},\"end\":8547,\"start\":8506},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":9353,\"start\":9343},{\"attributes\":{\"n\":\"3.2\"},\"end\":10718,\"start\":10697},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":12479,\"start\":12455},{\"attributes\":{\"n\":\"4.2\"},\"end\":13204,\"start\":13186},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":13330,\"start\":13312},{\"attributes\":{\"n\":\"4.3\"},\"end\":14362,\"start\":14344},{\"attributes\":{\"n\":\"4.4\"},\"end\":15652,\"start\":15629},{\"attributes\":{\"n\":\"5\"},\"end\":16300,\"start\":16259},{\"attributes\":{\"n\":\"5.2\"},\"end\":17172,\"start\":17139},{\"attributes\":{\"n\":\"5.3\"},\"end\":17216,\"start\":17175},{\"attributes\":{\"n\":\"5.4\"},\"end\":20275,\"start\":20249},{\"attributes\":{\"n\":\"6\"},\"end\":22118,\"start\":22108},{\"end\":22964,\"start\":22954},{\"end\":23069,\"start\":23059},{\"end\":24620,\"start\":24611},{\"end\":25734,\"start\":25727},{\"end\":28415,\"start\":28406}]", "table": "[{\"end\":23949,\"start\":23860},{\"end\":24609,\"start\":24566},{\"end\":25725,\"start\":24833},{\"end\":26764,\"start\":26412},{\"end\":28404,\"start\":28029},{\"end\":29885,\"start\":28808}]", "figure_caption": "[{\"end\":23057,\"start\":22966},{\"end\":23166,\"start\":23071},{\"end\":23624,\"start\":23169},{\"end\":23860,\"start\":23627},{\"end\":24566,\"start\":23952},{\"end\":24833,\"start\":24622},{\"end\":26412,\"start\":25736},{\"end\":28029,\"start\":26767},{\"end\":28808,\"start\":28417}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3201,\"start\":3193},{\"end\":17333,\"start\":17325},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20902,\"start\":20894}]", "bib_author_first_name": "[{\"end\":30099,\"start\":30092},{\"end\":30119,\"start\":30114},{\"end\":30132,\"start\":30127},{\"end\":30148,\"start\":30140},{\"end\":30159,\"start\":30155},{\"end\":30175,\"start\":30168},{\"end\":30181,\"start\":30180},{\"end\":30196,\"start\":30188},{\"end\":30572,\"start\":30571},{\"end\":30589,\"start\":30583},{\"end\":30591,\"start\":30590},{\"end\":30605,\"start\":30600},{\"end\":30607,\"start\":30606},{\"end\":30875,\"start\":30871},{\"end\":30891,\"start\":30887},{\"end\":30909,\"start\":30902},{\"end\":30925,\"start\":30918},{\"end\":31462,\"start\":31454},{\"end\":31480,\"start\":31476},{\"end\":31494,\"start\":31487},{\"end\":31507,\"start\":31502},{\"end\":31525,\"start\":31517},{\"end\":31540,\"start\":31534},{\"end\":31557,\"start\":31551},{\"end\":31577,\"start\":31571},{\"end\":31591,\"start\":31585},{\"end\":31969,\"start\":31968},{\"end\":31988,\"start\":31987},{\"end\":31990,\"start\":31989},{\"end\":32006,\"start\":32000},{\"end\":32560,\"start\":32556},{\"end\":32578,\"start\":32571},{\"end\":32591,\"start\":32586},{\"end\":32606,\"start\":32600},{\"end\":32819,\"start\":32815},{\"end\":32837,\"start\":32830},{\"end\":32850,\"start\":32845},{\"end\":32865,\"start\":32859},{\"end\":32879,\"start\":32874},{\"end\":32881,\"start\":32880},{\"end\":33210,\"start\":33203},{\"end\":33224,\"start\":33219},{\"end\":33237,\"start\":33232},{\"end\":33772,\"start\":33767},{\"end\":33789,\"start\":33781},{\"end\":33803,\"start\":33797},{\"end\":33817,\"start\":33809},{\"end\":34908,\"start\":34900},{\"end\":34921,\"start\":34915},{\"end\":35411,\"start\":35403},{\"end\":35430,\"start\":35424},{\"end\":35445,\"start\":35438},{\"end\":35461,\"start\":35454},{\"end\":35794,\"start\":35786},{\"end\":35811,\"start\":35803},{\"end\":35832,\"start\":35824},{\"end\":36323,\"start\":36319},{\"end\":36337,\"start\":36329},{\"end\":36347,\"start\":36342},{\"end\":36358,\"start\":36353},{\"end\":36714,\"start\":36708},{\"end\":36730,\"start\":36723},{\"end\":36932,\"start\":36927},{\"end\":36947,\"start\":36943},{\"end\":36963,\"start\":36959},{\"end\":36981,\"start\":36974},{\"end\":36997,\"start\":36990},{\"end\":37525,\"start\":37521},{\"end\":37540,\"start\":37535},{\"end\":37740,\"start\":37734},{\"end\":37759,\"start\":37751},{\"end\":38096,\"start\":38092},{\"end\":38106,\"start\":38101},{\"end\":38123,\"start\":38115},{\"end\":38137,\"start\":38132},{\"end\":38148,\"start\":38147},{\"end\":38496,\"start\":38492},{\"end\":38509,\"start\":38501},{\"end\":38523,\"start\":38518},{\"end\":38540,\"start\":38532},{\"end\":38550,\"start\":38545},{\"end\":38561,\"start\":38556},{\"end\":39184,\"start\":39179},{\"end\":39198,\"start\":39190},{\"end\":39214,\"start\":39203},{\"end\":39230,\"start\":39220},{\"end\":39242,\"start\":39237},{\"end\":39259,\"start\":39252},{\"end\":39834,\"start\":39824},{\"end\":39857,\"start\":39852},{\"end\":40256,\"start\":40246},{\"end\":40279,\"start\":40274},{\"end\":40667,\"start\":40660},{\"end\":40680,\"start\":40673},{\"end\":40690,\"start\":40685},{\"end\":40705,\"start\":40696},{\"end\":41037,\"start\":41033},{\"end\":41054,\"start\":41049},{\"end\":41074,\"start\":41066},{\"end\":41444,\"start\":41440},{\"end\":41461,\"start\":41456},{\"end\":41477,\"start\":41473},{\"end\":41791,\"start\":41786},{\"end\":41809,\"start\":41803},{\"end\":41826,\"start\":41822},{\"end\":41842,\"start\":41838},{\"end\":42339,\"start\":42335},{\"end\":42355,\"start\":42348},{\"end\":42375,\"start\":42368},{\"end\":42773,\"start\":42769},{\"end\":42789,\"start\":42782},{\"end\":42804,\"start\":42798},{\"end\":43327,\"start\":43323},{\"end\":43340,\"start\":43336},{\"end\":43356,\"start\":43350},{\"end\":43372,\"start\":43366},{\"end\":43386,\"start\":43382},{\"end\":43405,\"start\":43398},{\"end\":43951,\"start\":43946},{\"end\":43962,\"start\":43958},{\"end\":43983,\"start\":43976},{\"end\":44000,\"start\":43994},{\"end\":44298,\"start\":44292},{\"end\":44315,\"start\":44308},{\"end\":44829,\"start\":44823},{\"end\":44846,\"start\":44839},{\"end\":45364,\"start\":45356},{\"end\":45380,\"start\":45373},{\"end\":45395,\"start\":45389},{\"end\":45408,\"start\":45403},{\"end\":45903,\"start\":45898},{\"end\":45916,\"start\":45912},{\"end\":45934,\"start\":45928},{\"end\":46316,\"start\":46313},{\"end\":46328,\"start\":46325},{\"end\":46343,\"start\":46340},{\"end\":46358,\"start\":46350},{\"end\":46371,\"start\":46364},{\"end\":46386,\"start\":46380},{\"end\":46399,\"start\":46397},{\"end\":46588,\"start\":46585},{\"end\":46600,\"start\":46597},{\"end\":46615,\"start\":46612},{\"end\":46630,\"start\":46622},{\"end\":46643,\"start\":46636},{\"end\":46658,\"start\":46652},{\"end\":46671,\"start\":46669},{\"end\":47001,\"start\":46994},{\"end\":47017,\"start\":47012},{\"end\":47024,\"start\":47023},{\"end\":47026,\"start\":47025},{\"end\":47242,\"start\":47238},{\"end\":47258,\"start\":47251},{\"end\":47265,\"start\":47263},{\"end\":47278,\"start\":47273},{\"end\":47295,\"start\":47288},{\"end\":47314,\"start\":47308},{\"end\":47329,\"start\":47324},{\"end\":47345,\"start\":47337},{\"end\":47363,\"start\":47355},{\"end\":47375,\"start\":47371},{\"end\":47863,\"start\":47859},{\"end\":47881,\"start\":47873},{\"end\":47895,\"start\":47890},{\"end\":48158,\"start\":48154},{\"end\":48174,\"start\":48168},{\"end\":48191,\"start\":48184},{\"end\":48207,\"start\":48200},{\"end\":48770,\"start\":48769},{\"end\":48785,\"start\":48780},{\"end\":49124,\"start\":49118},{\"end\":49358,\"start\":49353},{\"end\":49375,\"start\":49370},{\"end\":49391,\"start\":49386},{\"end\":49869,\"start\":49858},{\"end\":49888,\"start\":49882},{\"end\":49903,\"start\":49896},{\"end\":49914,\"start\":49909},{\"end\":50362,\"start\":50357},{\"end\":50364,\"start\":50363},{\"end\":50757,\"start\":50753},{\"end\":50769,\"start\":50764},{\"end\":50787,\"start\":50781},{\"end\":50804,\"start\":50800},{\"end\":51132,\"start\":51128},{\"end\":51144,\"start\":51139},{\"end\":51162,\"start\":51156},{\"end\":51179,\"start\":51175},{\"end\":51514,\"start\":51509},{\"end\":51521,\"start\":51519},{\"end\":51534,\"start\":51528},{\"end\":51549,\"start\":51544},{\"end\":51567,\"start\":51559},{\"end\":51584,\"start\":51580},{\"end\":51608,\"start\":51596},{\"end\":52310,\"start\":52303},{\"end\":52323,\"start\":52318},{\"end\":52876,\"start\":52873},{\"end\":52891,\"start\":52884},{\"end\":52901,\"start\":52899},{\"end\":52915,\"start\":52906},{\"end\":52928,\"start\":52922},{\"end\":52935,\"start\":52934},{\"end\":52948,\"start\":52942},{\"end\":52964,\"start\":52958},{\"end\":53459,\"start\":53455},{\"end\":53465,\"start\":53464},{\"end\":53467,\"start\":53466},{\"end\":53479,\"start\":53476},{\"end\":54209,\"start\":54203},{\"end\":54224,\"start\":54217},{\"end\":54240,\"start\":54235},{\"end\":54701,\"start\":54692},{\"end\":54713,\"start\":54706},{\"end\":54726,\"start\":54721},{\"end\":55212,\"start\":55207},{\"end\":55226,\"start\":55221},{\"end\":55239,\"start\":55235},{\"end\":55254,\"start\":55250},{\"end\":55268,\"start\":55264},{\"end\":55556,\"start\":55546},{\"end\":55567,\"start\":55563}]", "bib_author_last_name": "[{\"end\":30112,\"start\":30100},{\"end\":30125,\"start\":30120},{\"end\":30138,\"start\":30133},{\"end\":30153,\"start\":30149},{\"end\":30166,\"start\":30160},{\"end\":30178,\"start\":30176},{\"end\":30186,\"start\":30182},{\"end\":30204,\"start\":30197},{\"end\":30210,\"start\":30206},{\"end\":30581,\"start\":30573},{\"end\":30598,\"start\":30592},{\"end\":30612,\"start\":30608},{\"end\":30620,\"start\":30614},{\"end\":30885,\"start\":30876},{\"end\":30900,\"start\":30892},{\"end\":30916,\"start\":30910},{\"end\":30934,\"start\":30926},{\"end\":31474,\"start\":31463},{\"end\":31485,\"start\":31481},{\"end\":31500,\"start\":31495},{\"end\":31515,\"start\":31508},{\"end\":31532,\"start\":31526},{\"end\":31549,\"start\":31541},{\"end\":31569,\"start\":31558},{\"end\":31583,\"start\":31578},{\"end\":31598,\"start\":31592},{\"end\":31606,\"start\":31600},{\"end\":31976,\"start\":31970},{\"end\":31985,\"start\":31978},{\"end\":31998,\"start\":31991},{\"end\":32013,\"start\":32007},{\"end\":32024,\"start\":32015},{\"end\":32569,\"start\":32561},{\"end\":32584,\"start\":32579},{\"end\":32598,\"start\":32592},{\"end\":32613,\"start\":32607},{\"end\":32828,\"start\":32820},{\"end\":32843,\"start\":32838},{\"end\":32857,\"start\":32851},{\"end\":32872,\"start\":32866},{\"end\":32890,\"start\":32882},{\"end\":33217,\"start\":33211},{\"end\":33230,\"start\":33225},{\"end\":33243,\"start\":33238},{\"end\":33779,\"start\":33773},{\"end\":33795,\"start\":33790},{\"end\":33807,\"start\":33804},{\"end\":33827,\"start\":33818},{\"end\":34913,\"start\":34909},{\"end\":34929,\"start\":34922},{\"end\":35422,\"start\":35412},{\"end\":35436,\"start\":35431},{\"end\":35452,\"start\":35446},{\"end\":35470,\"start\":35462},{\"end\":35801,\"start\":35795},{\"end\":35822,\"start\":35812},{\"end\":35842,\"start\":35833},{\"end\":36327,\"start\":36324},{\"end\":36340,\"start\":36338},{\"end\":36351,\"start\":36348},{\"end\":36365,\"start\":36359},{\"end\":36721,\"start\":36715},{\"end\":36736,\"start\":36731},{\"end\":36941,\"start\":36933},{\"end\":36957,\"start\":36948},{\"end\":36972,\"start\":36964},{\"end\":36988,\"start\":36982},{\"end\":37004,\"start\":36998},{\"end\":37533,\"start\":37526},{\"end\":37548,\"start\":37541},{\"end\":37749,\"start\":37741},{\"end\":37769,\"start\":37760},{\"end\":38099,\"start\":38097},{\"end\":38113,\"start\":38107},{\"end\":38130,\"start\":38124},{\"end\":38145,\"start\":38138},{\"end\":38155,\"start\":38149},{\"end\":38499,\"start\":38497},{\"end\":38516,\"start\":38510},{\"end\":38530,\"start\":38524},{\"end\":38543,\"start\":38541},{\"end\":38554,\"start\":38551},{\"end\":38568,\"start\":38562},{\"end\":39188,\"start\":39185},{\"end\":39201,\"start\":39199},{\"end\":39218,\"start\":39215},{\"end\":39235,\"start\":39231},{\"end\":39250,\"start\":39243},{\"end\":39268,\"start\":39260},{\"end\":39850,\"start\":39835},{\"end\":39862,\"start\":39858},{\"end\":39867,\"start\":39864},{\"end\":40272,\"start\":40257},{\"end\":40284,\"start\":40280},{\"end\":40289,\"start\":40286},{\"end\":40671,\"start\":40668},{\"end\":40683,\"start\":40681},{\"end\":40694,\"start\":40691},{\"end\":40712,\"start\":40706},{\"end\":41047,\"start\":41038},{\"end\":41064,\"start\":41055},{\"end\":41087,\"start\":41075},{\"end\":41454,\"start\":41445},{\"end\":41471,\"start\":41462},{\"end\":41483,\"start\":41478},{\"end\":41801,\"start\":41792},{\"end\":41820,\"start\":41810},{\"end\":41836,\"start\":41827},{\"end\":41848,\"start\":41843},{\"end\":42346,\"start\":42340},{\"end\":42366,\"start\":42356},{\"end\":42382,\"start\":42376},{\"end\":42780,\"start\":42774},{\"end\":42796,\"start\":42790},{\"end\":42810,\"start\":42805},{\"end\":43334,\"start\":43328},{\"end\":43348,\"start\":43341},{\"end\":43364,\"start\":43357},{\"end\":43380,\"start\":43373},{\"end\":43396,\"start\":43387},{\"end\":43412,\"start\":43406},{\"end\":43956,\"start\":43952},{\"end\":43974,\"start\":43963},{\"end\":43992,\"start\":43984},{\"end\":44007,\"start\":44001},{\"end\":44306,\"start\":44299},{\"end\":44321,\"start\":44316},{\"end\":44837,\"start\":44830},{\"end\":44852,\"start\":44847},{\"end\":45371,\"start\":45365},{\"end\":45387,\"start\":45381},{\"end\":45401,\"start\":45396},{\"end\":45414,\"start\":45409},{\"end\":45910,\"start\":45904},{\"end\":45926,\"start\":45917},{\"end\":45940,\"start\":45935},{\"end\":46323,\"start\":46317},{\"end\":46338,\"start\":46329},{\"end\":46348,\"start\":46344},{\"end\":46362,\"start\":46359},{\"end\":46378,\"start\":46372},{\"end\":46395,\"start\":46387},{\"end\":46404,\"start\":46400},{\"end\":46595,\"start\":46589},{\"end\":46610,\"start\":46601},{\"end\":46620,\"start\":46616},{\"end\":46634,\"start\":46631},{\"end\":46650,\"start\":46644},{\"end\":46667,\"start\":46659},{\"end\":46676,\"start\":46672},{\"end\":47010,\"start\":47002},{\"end\":47021,\"start\":47018},{\"end\":47036,\"start\":47027},{\"end\":47249,\"start\":47243},{\"end\":47261,\"start\":47259},{\"end\":47271,\"start\":47266},{\"end\":47286,\"start\":47279},{\"end\":47306,\"start\":47296},{\"end\":47322,\"start\":47315},{\"end\":47335,\"start\":47330},{\"end\":47353,\"start\":47346},{\"end\":47369,\"start\":47364},{\"end\":47379,\"start\":47376},{\"end\":47871,\"start\":47864},{\"end\":47888,\"start\":47882},{\"end\":47902,\"start\":47896},{\"end\":48166,\"start\":48159},{\"end\":48182,\"start\":48175},{\"end\":48198,\"start\":48192},{\"end\":48216,\"start\":48208},{\"end\":48778,\"start\":48771},{\"end\":48795,\"start\":48786},{\"end\":48803,\"start\":48797},{\"end\":49132,\"start\":49125},{\"end\":49368,\"start\":49359},{\"end\":49384,\"start\":49376},{\"end\":49398,\"start\":49392},{\"end\":49880,\"start\":49870},{\"end\":49894,\"start\":49889},{\"end\":49907,\"start\":49904},{\"end\":49919,\"start\":49915},{\"end\":50373,\"start\":50365},{\"end\":50762,\"start\":50758},{\"end\":50779,\"start\":50770},{\"end\":50798,\"start\":50788},{\"end\":50810,\"start\":50805},{\"end\":51137,\"start\":51133},{\"end\":51154,\"start\":51145},{\"end\":51173,\"start\":51163},{\"end\":51185,\"start\":51180},{\"end\":51517,\"start\":51515},{\"end\":51526,\"start\":51522},{\"end\":51542,\"start\":51535},{\"end\":51557,\"start\":51550},{\"end\":51578,\"start\":51568},{\"end\":51594,\"start\":51585},{\"end\":51614,\"start\":51609},{\"end\":52316,\"start\":52311},{\"end\":52330,\"start\":52324},{\"end\":52882,\"start\":52877},{\"end\":52897,\"start\":52892},{\"end\":52904,\"start\":52902},{\"end\":52920,\"start\":52916},{\"end\":52932,\"start\":52929},{\"end\":52940,\"start\":52936},{\"end\":52956,\"start\":52949},{\"end\":52970,\"start\":52965},{\"end\":52980,\"start\":52972},{\"end\":53462,\"start\":53460},{\"end\":53474,\"start\":53468},{\"end\":53485,\"start\":53480},{\"end\":53491,\"start\":53487},{\"end\":54215,\"start\":54210},{\"end\":54233,\"start\":54225},{\"end\":54244,\"start\":54241},{\"end\":54704,\"start\":54702},{\"end\":54719,\"start\":54714},{\"end\":54733,\"start\":54727},{\"end\":55219,\"start\":55213},{\"end\":55233,\"start\":55227},{\"end\":55248,\"start\":55240},{\"end\":55262,\"start\":55255},{\"end\":55274,\"start\":55269},{\"end\":55561,\"start\":55557},{\"end\":55576,\"start\":55568}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16221853},\"end\":30507,\"start\":30054},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52861858},\"end\":30802,\"start\":30509},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":250340449},\"end\":31413,\"start\":30804},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b3\"},\"end\":31883,\"start\":31415},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12408211},\"end\":32507,\"start\":31885},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":212737158},\"end\":32813,\"start\":32509},{\"attributes\":{\"doi\":\"arXiv:2003.07820\",\"id\":\"b6\"},\"end\":33142,\"start\":32815},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":12620927},\"end\":33683,\"start\":33144},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1423\",\"id\":\"b8\",\"matched_paper_id\":52967399},\"end\":34823,\"start\":33685},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9130375},\"end\":35401,\"start\":34825},{\"attributes\":{\"doi\":\"arXiv:2301.10521\",\"id\":\"b10\"},\"end\":35716,\"start\":35403},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235792467},\"end\":36315,\"start\":35718},{\"attributes\":{\"doi\":\"arXiv:2212.10496\",\"id\":\"b12\"},\"end\":36620,\"start\":36317},{\"attributes\":{\"id\":\"b13\"},\"end\":36925,\"start\":36622},{\"attributes\":{\"doi\":\"arXiv:2301.01820\",\"id\":\"b14\"},\"end\":37424,\"start\":36927},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":216553223},\"end\":37732,\"start\":37426},{\"attributes\":{\"doi\":\"arXiv:2302.12574\",\"id\":\"b16\"},\"end\":37992,\"start\":37734},{\"attributes\":{\"doi\":\"ArXiv abs/2108.11044\",\"id\":\"b17\"},\"end\":38381,\"start\":37994},{\"attributes\":{\"doi\":\"ECIR 2022\",\"id\":\"b18\",\"matched_paper_id\":245124479},\"end\":39063,\"start\":38383},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235366815},\"end\":39822,\"start\":39065},{\"attributes\":{\"doi\":\"arXiv:2010.11386\",\"id\":\"b20\"},\"end\":40147,\"start\":39824},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235720578},\"end\":40658,\"start\":40149},{\"attributes\":{\"doi\":\"arXiv:2210.07093\",\"id\":\"b22\"},\"end\":41031,\"start\":40660},{\"attributes\":{\"doi\":\"arXiv:2108.04026\",\"id\":\"b23\"},\"end\":41396,\"start\":41033},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":244709531},\"end\":41705,\"start\":41398},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":240230875},\"end\":42275,\"start\":41707},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":258332122},\"end\":42695,\"start\":42277},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":234741735},\"end\":43274,\"start\":42697},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":248665509},\"end\":43886,\"start\":43276},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10929365},\"end\":44239,\"start\":43888},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1118305},\"end\":44768,\"start\":44241},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14811099},\"end\":45301,\"start\":44770},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":219162111},\"end\":45842,\"start\":45303},{\"attributes\":{\"id\":\"b33\"},\"end\":46244,\"start\":45844},{\"attributes\":{\"id\":\"b34\"},\"end\":46583,\"start\":46246},{\"attributes\":{\"doi\":\"arXiv:1611.09268v1\",\"id\":\"b35\"},\"end\":46959,\"start\":46585},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":208612557},\"end\":47167,\"start\":46961},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":246426909},\"end\":47732,\"start\":47169},{\"attributes\":{\"id\":\"b38\"},\"end\":48091,\"start\":47734},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":254854129},\"end\":48669,\"start\":48093},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":2218552},\"end\":48998,\"start\":48671},{\"attributes\":{\"id\":\"b41\"},\"end\":49289,\"start\":49000},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":250272894},\"end\":49796,\"start\":49291},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":237562875},\"end\":50315,\"start\":49798},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":59910946},\"end\":50661,\"start\":50317},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":253763641},\"end\":51036,\"start\":50663},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":253763641},\"end\":51425,\"start\":51038},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":245218563},\"end\":52270,\"start\":51427},{\"attributes\":{\"doi\":\"10.1145/2808194.2809446\",\"id\":\"b48\",\"matched_paper_id\":7311615},\"end\":52869,\"start\":52272},{\"attributes\":{\"id\":\"b49\"},\"end\":53107,\"start\":52871},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":220302524},\"end\":53391,\"start\":53109},{\"attributes\":{\"doi\":\"10.1145/1571941.1571954\",\"id\":\"b51\",\"matched_paper_id\":566812},\"end\":54142,\"start\":53393},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":222310837},\"end\":54606,\"start\":54144},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":237363901},\"end\":55146,\"start\":54608},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":214148289},\"end\":55463,\"start\":55148},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":1043470},\"end\":55969,\"start\":55465}]", "bib_title": "[{\"end\":30090,\"start\":30054},{\"end\":30569,\"start\":30509},{\"end\":30869,\"start\":30804},{\"end\":31966,\"start\":31885},{\"end\":32554,\"start\":32509},{\"end\":33201,\"start\":33144},{\"end\":33765,\"start\":33685},{\"end\":34898,\"start\":34825},{\"end\":35784,\"start\":35718},{\"end\":37519,\"start\":37426},{\"end\":38490,\"start\":38383},{\"end\":39177,\"start\":39065},{\"end\":40244,\"start\":40149},{\"end\":41438,\"start\":41398},{\"end\":41784,\"start\":41707},{\"end\":42333,\"start\":42277},{\"end\":42767,\"start\":42697},{\"end\":43321,\"start\":43276},{\"end\":43944,\"start\":43888},{\"end\":44290,\"start\":44241},{\"end\":44821,\"start\":44770},{\"end\":45354,\"start\":45303},{\"end\":45896,\"start\":45844},{\"end\":46992,\"start\":46961},{\"end\":47236,\"start\":47169},{\"end\":48152,\"start\":48093},{\"end\":48767,\"start\":48671},{\"end\":49351,\"start\":49291},{\"end\":49856,\"start\":49798},{\"end\":50355,\"start\":50317},{\"end\":50751,\"start\":50663},{\"end\":51126,\"start\":51038},{\"end\":51507,\"start\":51427},{\"end\":52301,\"start\":52272},{\"end\":53192,\"start\":53109},{\"end\":53453,\"start\":53393},{\"end\":54201,\"start\":54144},{\"end\":54690,\"start\":54608},{\"end\":55205,\"start\":55148},{\"end\":55544,\"start\":55465}]", "bib_author": "[{\"end\":30114,\"start\":30092},{\"end\":30127,\"start\":30114},{\"end\":30140,\"start\":30127},{\"end\":30155,\"start\":30140},{\"end\":30168,\"start\":30155},{\"end\":30180,\"start\":30168},{\"end\":30188,\"start\":30180},{\"end\":30206,\"start\":30188},{\"end\":30212,\"start\":30206},{\"end\":30583,\"start\":30571},{\"end\":30600,\"start\":30583},{\"end\":30614,\"start\":30600},{\"end\":30622,\"start\":30614},{\"end\":30887,\"start\":30871},{\"end\":30902,\"start\":30887},{\"end\":30918,\"start\":30902},{\"end\":30936,\"start\":30918},{\"end\":31476,\"start\":31454},{\"end\":31487,\"start\":31476},{\"end\":31502,\"start\":31487},{\"end\":31517,\"start\":31502},{\"end\":31534,\"start\":31517},{\"end\":31551,\"start\":31534},{\"end\":31571,\"start\":31551},{\"end\":31585,\"start\":31571},{\"end\":31600,\"start\":31585},{\"end\":31608,\"start\":31600},{\"end\":31978,\"start\":31968},{\"end\":31987,\"start\":31978},{\"end\":32000,\"start\":31987},{\"end\":32015,\"start\":32000},{\"end\":32026,\"start\":32015},{\"end\":32571,\"start\":32556},{\"end\":32586,\"start\":32571},{\"end\":32600,\"start\":32586},{\"end\":32615,\"start\":32600},{\"end\":32830,\"start\":32815},{\"end\":32845,\"start\":32830},{\"end\":32859,\"start\":32845},{\"end\":32874,\"start\":32859},{\"end\":32892,\"start\":32874},{\"end\":33219,\"start\":33203},{\"end\":33232,\"start\":33219},{\"end\":33245,\"start\":33232},{\"end\":33781,\"start\":33767},{\"end\":33797,\"start\":33781},{\"end\":33809,\"start\":33797},{\"end\":33829,\"start\":33809},{\"end\":34915,\"start\":34900},{\"end\":34931,\"start\":34915},{\"end\":35424,\"start\":35403},{\"end\":35438,\"start\":35424},{\"end\":35454,\"start\":35438},{\"end\":35472,\"start\":35454},{\"end\":35803,\"start\":35786},{\"end\":35824,\"start\":35803},{\"end\":35844,\"start\":35824},{\"end\":36329,\"start\":36319},{\"end\":36342,\"start\":36329},{\"end\":36353,\"start\":36342},{\"end\":36367,\"start\":36353},{\"end\":36723,\"start\":36708},{\"end\":36738,\"start\":36723},{\"end\":36943,\"start\":36927},{\"end\":36959,\"start\":36943},{\"end\":36974,\"start\":36959},{\"end\":36990,\"start\":36974},{\"end\":37006,\"start\":36990},{\"end\":37535,\"start\":37521},{\"end\":37550,\"start\":37535},{\"end\":37751,\"start\":37734},{\"end\":37771,\"start\":37751},{\"end\":38101,\"start\":38092},{\"end\":38115,\"start\":38101},{\"end\":38132,\"start\":38115},{\"end\":38147,\"start\":38132},{\"end\":38157,\"start\":38147},{\"end\":38501,\"start\":38492},{\"end\":38518,\"start\":38501},{\"end\":38532,\"start\":38518},{\"end\":38545,\"start\":38532},{\"end\":38556,\"start\":38545},{\"end\":38570,\"start\":38556},{\"end\":39190,\"start\":39179},{\"end\":39203,\"start\":39190},{\"end\":39220,\"start\":39203},{\"end\":39237,\"start\":39220},{\"end\":39252,\"start\":39237},{\"end\":39270,\"start\":39252},{\"end\":39852,\"start\":39824},{\"end\":39864,\"start\":39852},{\"end\":39869,\"start\":39864},{\"end\":40274,\"start\":40246},{\"end\":40286,\"start\":40274},{\"end\":40291,\"start\":40286},{\"end\":40673,\"start\":40660},{\"end\":40685,\"start\":40673},{\"end\":40696,\"start\":40685},{\"end\":40714,\"start\":40696},{\"end\":41049,\"start\":41033},{\"end\":41066,\"start\":41049},{\"end\":41089,\"start\":41066},{\"end\":41456,\"start\":41440},{\"end\":41473,\"start\":41456},{\"end\":41485,\"start\":41473},{\"end\":41803,\"start\":41786},{\"end\":41822,\"start\":41803},{\"end\":41838,\"start\":41822},{\"end\":41850,\"start\":41838},{\"end\":42348,\"start\":42335},{\"end\":42368,\"start\":42348},{\"end\":42384,\"start\":42368},{\"end\":42782,\"start\":42769},{\"end\":42798,\"start\":42782},{\"end\":42812,\"start\":42798},{\"end\":43336,\"start\":43323},{\"end\":43350,\"start\":43336},{\"end\":43366,\"start\":43350},{\"end\":43382,\"start\":43366},{\"end\":43398,\"start\":43382},{\"end\":43414,\"start\":43398},{\"end\":43958,\"start\":43946},{\"end\":43976,\"start\":43958},{\"end\":43994,\"start\":43976},{\"end\":44009,\"start\":43994},{\"end\":44308,\"start\":44292},{\"end\":44323,\"start\":44308},{\"end\":44839,\"start\":44823},{\"end\":44854,\"start\":44839},{\"end\":45373,\"start\":45356},{\"end\":45389,\"start\":45373},{\"end\":45403,\"start\":45389},{\"end\":45416,\"start\":45403},{\"end\":45912,\"start\":45898},{\"end\":45928,\"start\":45912},{\"end\":45942,\"start\":45928},{\"end\":46325,\"start\":46313},{\"end\":46340,\"start\":46325},{\"end\":46350,\"start\":46340},{\"end\":46364,\"start\":46350},{\"end\":46380,\"start\":46364},{\"end\":46397,\"start\":46380},{\"end\":46406,\"start\":46397},{\"end\":46597,\"start\":46585},{\"end\":46612,\"start\":46597},{\"end\":46622,\"start\":46612},{\"end\":46636,\"start\":46622},{\"end\":46652,\"start\":46636},{\"end\":46669,\"start\":46652},{\"end\":46678,\"start\":46669},{\"end\":47012,\"start\":46994},{\"end\":47023,\"start\":47012},{\"end\":47038,\"start\":47023},{\"end\":47251,\"start\":47238},{\"end\":47263,\"start\":47251},{\"end\":47273,\"start\":47263},{\"end\":47288,\"start\":47273},{\"end\":47308,\"start\":47288},{\"end\":47324,\"start\":47308},{\"end\":47337,\"start\":47324},{\"end\":47355,\"start\":47337},{\"end\":47371,\"start\":47355},{\"end\":47381,\"start\":47371},{\"end\":47873,\"start\":47859},{\"end\":47890,\"start\":47873},{\"end\":47904,\"start\":47890},{\"end\":48168,\"start\":48154},{\"end\":48184,\"start\":48168},{\"end\":48200,\"start\":48184},{\"end\":48218,\"start\":48200},{\"end\":48780,\"start\":48769},{\"end\":48797,\"start\":48780},{\"end\":48805,\"start\":48797},{\"end\":49134,\"start\":49118},{\"end\":49370,\"start\":49353},{\"end\":49386,\"start\":49370},{\"end\":49400,\"start\":49386},{\"end\":49882,\"start\":49858},{\"end\":49896,\"start\":49882},{\"end\":49909,\"start\":49896},{\"end\":49921,\"start\":49909},{\"end\":50375,\"start\":50357},{\"end\":50764,\"start\":50753},{\"end\":50781,\"start\":50764},{\"end\":50800,\"start\":50781},{\"end\":50812,\"start\":50800},{\"end\":51139,\"start\":51128},{\"end\":51156,\"start\":51139},{\"end\":51175,\"start\":51156},{\"end\":51187,\"start\":51175},{\"end\":51519,\"start\":51509},{\"end\":51528,\"start\":51519},{\"end\":51544,\"start\":51528},{\"end\":51559,\"start\":51544},{\"end\":51580,\"start\":51559},{\"end\":51596,\"start\":51580},{\"end\":51616,\"start\":51596},{\"end\":52318,\"start\":52303},{\"end\":52332,\"start\":52318},{\"end\":52884,\"start\":52873},{\"end\":52899,\"start\":52884},{\"end\":52906,\"start\":52899},{\"end\":52922,\"start\":52906},{\"end\":52934,\"start\":52922},{\"end\":52942,\"start\":52934},{\"end\":52958,\"start\":52942},{\"end\":52972,\"start\":52958},{\"end\":52982,\"start\":52972},{\"end\":53464,\"start\":53455},{\"end\":53476,\"start\":53464},{\"end\":53487,\"start\":53476},{\"end\":53493,\"start\":53487},{\"end\":54217,\"start\":54203},{\"end\":54235,\"start\":54217},{\"end\":54246,\"start\":54235},{\"end\":54706,\"start\":54692},{\"end\":54721,\"start\":54706},{\"end\":54735,\"start\":54721},{\"end\":55221,\"start\":55207},{\"end\":55235,\"start\":55221},{\"end\":55250,\"start\":55235},{\"end\":55264,\"start\":55250},{\"end\":55276,\"start\":55264},{\"end\":55563,\"start\":55546},{\"end\":55578,\"start\":55563}]", "bib_venue": "[{\"end\":30266,\"start\":30212},{\"end\":30646,\"start\":30622},{\"end\":31047,\"start\":30936},{\"end\":31452,\"start\":31415},{\"end\":32137,\"start\":32026},{\"end\":32653,\"start\":32615},{\"end\":32953,\"start\":32908},{\"end\":33354,\"start\":33245},{\"end\":34007,\"start\":33849},{\"end\":35049,\"start\":34931},{\"end\":35534,\"start\":35488},{\"end\":35955,\"start\":35844},{\"end\":36706,\"start\":36622},{\"end\":37154,\"start\":37022},{\"end\":37564,\"start\":37550},{\"end\":37838,\"start\":37787},{\"end\":38090,\"start\":37994},{\"end\":38653,\"start\":38579},{\"end\":39381,\"start\":39270},{\"end\":39960,\"start\":39885},{\"end\":40357,\"start\":40291},{\"end\":40825,\"start\":40730},{\"end\":41194,\"start\":41105},{\"end\":41529,\"start\":41485},{\"end\":41940,\"start\":41850},{\"end\":42476,\"start\":42384},{\"end\":42923,\"start\":42812},{\"end\":43525,\"start\":43414},{\"end\":44044,\"start\":44009},{\"end\":44441,\"start\":44323},{\"end\":44972,\"start\":44854},{\"end\":45516,\"start\":45416},{\"end\":46034,\"start\":45942},{\"end\":46311,\"start\":46246},{\"end\":46761,\"start\":46696},{\"end\":47053,\"start\":47038},{\"end\":47430,\"start\":47381},{\"end\":47857,\"start\":47734},{\"end\":48313,\"start\":48218},{\"end\":48813,\"start\":48805},{\"end\":49116,\"start\":49000},{\"end\":49493,\"start\":49400},{\"end\":50007,\"start\":49921},{\"end\":50430,\"start\":50375},{\"end\":50839,\"start\":50812},{\"end\":51214,\"start\":51187},{\"end\":51702,\"start\":51616},{\"end\":52454,\"start\":52355},{\"end\":53246,\"start\":53194},{\"end\":53639,\"start\":53516},{\"end\":54328,\"start\":54246},{\"end\":54825,\"start\":54735},{\"end\":55292,\"start\":55276},{\"end\":55667,\"start\":55578},{\"end\":31145,\"start\":31049},{\"end\":32235,\"start\":32139},{\"end\":33450,\"start\":33356},{\"end\":34220,\"start\":34057},{\"end\":35154,\"start\":35051},{\"end\":36053,\"start\":35957},{\"end\":37574,\"start\":37566},{\"end\":38672,\"start\":38655},{\"end\":39479,\"start\":39383},{\"end\":40410,\"start\":40359},{\"end\":42017,\"start\":41942},{\"end\":43021,\"start\":42925},{\"end\":43623,\"start\":43527},{\"end\":44546,\"start\":44443},{\"end\":45077,\"start\":44974},{\"end\":48330,\"start\":48315},{\"end\":49573,\"start\":49495},{\"end\":50080,\"start\":50009},{\"end\":50494,\"start\":50432},{\"end\":51806,\"start\":51704},{\"end\":52557,\"start\":52456},{\"end\":53766,\"start\":53641},{\"end\":54397,\"start\":54330},{\"end\":54902,\"start\":54827},{\"end\":55743,\"start\":55669}]"}}}, "year": 2023, "month": 12, "day": 17}