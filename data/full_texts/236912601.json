{"id": 236912601, "updated": "2023-11-08 06:13:51.847", "metadata": {"title": "Linking Common Vulnerabilities and Exposures to the MITRE ATT&CK Framework: A Self-Distillation Approach", "authors": "[{\"first\":\"Benjamin\",\"last\":\"Ampel\",\"middle\":[]},{\"first\":\"Sagar\",\"last\":\"Samtani\",\"middle\":[]},{\"first\":\"Steven\",\"last\":\"Ullman\",\"middle\":[]},{\"first\":\"Hsinchun\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 8, "day": 3}, "abstract": "Due to the ever-increasing threat of cyber-attacks to critical cyber infrastructure, organizations are focusing on building their cybersecurity knowledge base. A salient list of cybersecurity knowledge is the Common Vulnerabilities and Exposures (CVE) list, which details vulnerabilities found in a wide range of software and hardware. However, these vulnerabilities often do not have a mitigation strategy to prevent an attacker from exploiting them. A well-known cybersecurity risk management framework, MITRE ATT&CK, offers mitigation techniques for many malicious tactics. Despite the tremendous benefits that both CVEs and the ATT&CK framework can provide for key cybersecurity stakeholders (e.g., analysts, educators, and managers), the two entities are currently separate. We propose a model, named the CVE Transformer (CVET), to label CVEs with one of ten MITRE ATT&CK tactics. The CVET model contains a fine-tuning and self-knowledge distillation design applied to the state-of-the-art pre-trained language model RoBERTa. Empirical results on a gold-standard dataset suggest that our proposed novelties can increase model performance in F1-score. The results of this research can allow cybersecurity stakeholders to add preliminary MITRE ATT&CK information to their collected CVEs.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.01696", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2108-01696", "doi": null}}, "content": {"source": {"pdf_hash": "bdcffd1d9ebfb1f48ef9334e5efb3ff33f48afb7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.01696v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e2be84b0194e540720b71ade4bc2b6d44a9fb4d9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bdcffd1d9ebfb1f48ef9334e5efb3ff33f48afb7.txt", "contents": "\nLinking Common Vulnerabilities and Exposures to the MITRE ATT&CK Framework: A Self-Distillation Approach\n\n\nBenjamin Ampel bampel@arizona.edu \nDepartment of Management Information Systems\nDepartment of Operations and Decision Technologies\nUniversity of Arizona Tucson\nAZUSA\n\nSagar Samtani ssamtani@iu.edu \nDepartment of Management Information Systems\nIndiana University Bloomington\nINUSA\n\nSteven Ullman \nDepartment of Management Information Systems\nUniversity of Arizona Tucson\nAZUSA stevenullman@\n\nHsinchun Chen hsinchun@arizona.edu \nUniversity of Arizona Tucson\nAZUSA\n\nLinking Common Vulnerabilities and Exposures to the MITRE ATT&CK Framework: A Self-Distillation Approach\nCCS CONCEPTSSecurity and privacy \u2192 Software and application securityInformation systems \u2192 Information systems applications \u2192 Data miningComputing methodologies \u2192 Machine learning KEYWORDS CVE, MITRE ATT&CK, Cybersecurity, Transformer models, Pre- trained language models, Self-Knowledge Distillation\nDue to the ever-increasing threat of cyber-attacks to critical cyber infrastructure, organizations are focusing on building their cybersecurity knowledge base. A salient list of cybersecurity knowledge is the Common Vulnerabilities and Exposures (CVE) list, which details vulnerabilities found in a wide range of software and hardware. However, these vulnerabilities often do not have a mitigation strategy to prevent an attacker from exploiting them. A well-known cybersecurity risk management framework, MITRE ATT&CK, offers mitigation techniques for many malicious tactics. Despite the tremendous bene\ufffdits that both CVEs and the ATT&CK framework can provide for key cybersecurity stakeholders (e.g., analysts, educators, and managers), the two entities are currently separate. We propose a model, named the CVE Transformer (CVET), to label CVEs with one of ten MITRE ATT&CK tactics. The CVET model contains a \ufffdine-tuning and self-knowledge distillation design applied to the state-of-theart pre-trained language model RoBERTa. Empirical results on a gold-standard dataset suggest that our proposed novelties can increase model performance in F1-score. The results of this research can allow cybersecurity stakeholders to add preliminary MITRE ATT&CK information to their collected CVEs.\n\nINTRODUCTION\n\nHarmful cyber-attacks on critical cyber-infrastructure (e.g., large servers hosting confidential data) have cost on average $7.91 million per breach, leading to over 446,000,000 exposed records containing sensitive information in 2019 [22]. Thus, it is imperative to build our cybersecurity knowledge base to combat new and evolving cyber-threats. An essential component of the cybersecurity knowledge base is the Common Vulnerability and Exposures (CVE) list, overseen by the MITRE Corporation. Cybersecurity professionals commonly use CVEs to coordinate efforts to address the vulnerability. A new CVE is created whenever a security flaw is discovered in software and hardware and reported to MITRE. An example of a recent CVE is shown in Figure 1. Each CVE includes metadata such as a unique ID, a rich text description, references, and the assigning CNA. Despite their massive value to the cybersecurity community, CVEs often provide little information on how to combat the vulnerability once it is discovered. Connecting a cybersecurity risk management framework (CRMF) with tangible mitigation strategies and additional context to the CVE list to provide mitigation strategies could provide tremendous value to cybersecurity analysts and researchers.\n\nIn 2018, MITRE created the ATT&CK Matrix for Enterprise CRMF that models the tactics, techniques, and procedures (TTP) that an attacker would take when attempting to breach cyberinfrastructure [21]. The MITRE ATT&CK framework can help reliably predict and mitigate the tactics, techniques, and procedures (TTP) chain that an attacker follows after an initial breach [1]. Example tactics include \"initial access,\" \"defense evasion,\" and \"exfiltration.\" Each tactic in MITRE ATT&CK comes with a mitigation strategy (e.g., user training, account management, password policies, etc.) to assist cybersecurity analysts in protecting critical cyber-infrastructure, making it an excellent CRMF for our CVE labeling task. Labeling CVEs with ATT&CK tactics requires a computational model that can analyze the textual metadata available in CVE descriptions.\n\nDespite the tremendous benefits that both CVEs and the ATT&CK framework can provide for key cybersecurity stakeholders (e.g., analysts, researchers, educators, and managers), the two entities are currently separate. With over 158,000 CVEs existing as of the beginning of 2021, it would be a non-trivial task to manually link each one to the ATT&CK framework to gather mitigation strategies for every existing CVE.\n\nIn this study, we developed a novel framework that leverages the textual features in CVEs currently linked to an ATT&CK tactic by prior research [11] to create an ATT&CK tactic label for CVEs outside our gold-standard CVE dataset. To achieve this goal, we drew upon state-of-the-art methodologies in deep learning-based text classification literature to guide the development of a novel cybersecurity model, the CVE Transformer (CVET) model. To ensure the value of our proposed approach, we rigorously evaluated CVET against benchmark models found in related CVE data mining and cybersecurity analytics literature.\n\nThe rest of this paper is as follows. First, we review related literature to CVE data machine learning, transformers for text classification, and self-knowledge distillation. Second, we identify gaps from our literature review and pose our research question for study. Third, we detail our proposed method for labeling CVEs with MITRE ATT&CK tactics. Fourth, we summarize the empirical results of our work and discussed their implications. Finally, we conclude the work with important takeaways from the paper.\n\n\nLITERATURE REVIEW\n\n\nCVE Data Machine Learning\n\nLarge undertakings have been taken recently to use CVEs to improve other cybersecurity information systems through the use of classical machine and deep learning architectures. Authors have used the convolutional neural network (CNN) to predict CVE vulnerability severity [10] and build knowledge graphs with the CVE list, the Common Weakness Enumeration (CWE) list, and the Common Attack Pattern Enumeration and Classi\ufffdication (CAPEC) list [29]. However, CNNs often struggle to capture long-term dependencies in textual passages [27]. Extant literature then applied the more powerful bidirectional long short-term memory (BiLSTM) model with a self-attention mechanism to predict vulnerability severity [6] and vulnerability type (e.g., boundary condition error) [7] with higher accuracy than the CNN. More recently, researchers have leveraged the pre-trained transformer model known as BERT [23] to extract information from the prominent vulnerability database ExploitDB to enhance the textual descriptions for new CVEs. Building a model that can effectively label CVEs with ATT&CK tactics using textual descriptions requires an algorithm that can effectively represent the long text sequences found in CVE descriptions. The transformer model (and its extensions) is currently the state-of-art within text classi\ufffdication literature and has proven to be robust against adversarial attacks [14]. We review the transformer model indepth to gain a deeper understanding of how it can assist in our target task.\n\n\nTransformers for Text Classi\ufffdication\n\nIntroduced in 2017, the transformer model replaces the recurrent cells found in prominent text classi\ufffdication deep learning models (e.g., BiLSTM, LSTM) with multi-head attention mechanisms [26]. While the original design incorporates an encoder-decoder structure (for machine translation tasks), multi-class text classi\ufffdication only requires the encoder stack. The encoder transformer model creates an embedding from the input, passes the embedding into the transformer block, and outputs a softmax probability score of outputs. The embedding layer is a one-hot encoding with positional encodings. The transformer block consists of a multihead attention mechanism and feed-forward layers, which has shown to greatly improve accuracy, precision, recall, and F1score over recurrent models in benchmark tasks [8]. Recently, transformers have been used for massive pre-training language models (PTLMs) that produce state-of-the-art results in various text classi\ufffdication tasks [20]. These models (e.g., BERT, GPT-2) are often trained on millions of data points and contain hundreds of millions of trainable parameters. While most researchers do not own the hardware or data to create their PTLM, these models can be carefully \ufffdine-tuned [4] and distilled [12] for improved performance in specialized tasks. Knowledge distillation is an emerging paradigm that can extract key information from the parameters of a PTLM to supplement the training of a targeted model [12].\n\n\nSelf-Knowledge Distillation\n\nKnowledge distillation (KD) combines relational knowledge from a large, pre-trained model (teacher) and a prior untrained model (student) [30]. As a result, the trained student model is often more generalizable to unseen data than a model without knowledge distillation. This design allows researchers that do not have access to the computing power required to make a massive PTLM to create highly targeted state-of-the-art models. One increasingly popular form of knowledge distillation is self-knowledge distillation (self-KD). Self-KD uses the same architecture for both the student and teacher, where knowledge transfer occurs within the same model [24]. This form of distillation creates a new model that often outperforms the original teacher model without requiring new data [32], due to distilling latent features from deeper to shallow sections of the network [33], improved feature importance weighting [5], or modi\ufffdied regularization [17]. Generally, self-KD for a natural language processing task with available target labels (e.g., a text classi\ufffdication task) uses the weighted sum of crossentropy (CE) loss with the correct labels and CE loss with the soft target [9].\n\n\nRESEARCH GAPS AND QUESTIONS\n\nFrom the extant literature, we identify several research gaps: First, many tasks have been undertaken to link CVEs to vulnerabilities, CWEs, and CAPEC, but not directly to the MITRE ATT&CK framework. The closest work to attempt this task is the BRON model [11], which does not link new CVEs, but uses existing databases to create a more holistic knowledge graph. Second, the deep learning models implemented in recent literature (e.g., CNN, BiLSTM) struggle to capture long-term dependencies in text, such as the lengthy descriptions within CVE listings. These two gaps motivate our research question:\n\n\n\u2022\n\nHow can we create a novel and accurate link between CVEs and ATT&CK tactics through their textual descriptions and long-term dependencies?\n\n\nPROPOSED METHOD\n\nOur proposed methodology is comprised of three major components: (1) Data Collection and Pre-Processing, (2) the CVE Transformer (CVET) Architecture, and (3)  For our research, we use the dataset provided by the BRON knowledge graph [11]. As discussed earlier, there are currently more than 158,000 CVEs, and our gold-standard dataset only captures a fraction of them, making this linking task critical. The dataset leverages existing knowledge bases to link 24,863 CVEs into 10 of the 14 ATT&CK tactics. Many ATT&CK tactics do not require a vulnerability (e.g., \"Resource Development\" and \"Command and Control\"). Thus, we cannot link CVEs to them. Table 1 provides a distribution of how many CVEs are in each ATT&CK tactic category. About 91% of the data is within four tactic categories: defense evasion (8,452), discovery (6,647), privilege escalation (5,779), and collection (1,748).\n\nTo pre-process the CVE textual description, stop words were removed, non-alphanumeric characters were stripped. The remaining text was lower-cased, lemmatized, and padded to ensure proper lengths for all inputs. This sequence of preprocessing steps is common in deep learning-based text classi\ufffdication literature [15]. We used the pre-made RoBERTa tokenizer [16] to encode the data as input for our CVET model. Other metadata available in CVEs is not used, as it did not provide a bene\ufffdit to model performance in preliminary testing.\n\n\nCVET Architecture\n\n\nModel Selection.\n\nWe adapt a PTLM called RoBERTa [16] (a BERT-based model trained on longer sequences) for CVET due to the generalizability it has shown in text classification tasks [2]. While there are dozens of PTLMs to choose from for our target task, we chose RoBERTa due to the high performance it achieves while also allowing custom fine-tuning and self-KD designs.\n\n\nFine-Tuning.\n\nThe standard fine-tuning process of RoBERTa and related PTLMs (e.g., BERT, GPT-2) is often unstable, with different performances depending on the random seed and dataset size [4]. Through a series of experiments, Mosbach et al. [18] identified that using the Adam optimization algorithm with bias correction led to a stable training process with improved performance compared to baseline fine-tuning (i.e., Adam without bias correction, see [3]). Therefore, we adopt this bias correction design for the CVET fine-tuning process, where bias correction is defined as:\n\u2190 \u2022 \ufffd1 \u2212 2 /(1 \u2212 1 ),\u2190 \u22121 \u2212 \u2022 /(\ufffd +\nIn the equation 1 and 2, is the step size, is the firstmoment estimate and is the second-moment estimate. From equation 1, we see that the goal of bias correction is to reduce by the factor \ufffd1 \u2212 2 /(1 \u2212 1 ), which increases to 1 as increases.\n\n\nSelf-Knowledge Distillation.\n\nDuring fine-tuning of the CVET model, we use CVET as both a teacher and student. The student model (denoted as ) is CVET at fine-tuning time step and the teacher model (denoted as ) is CVET at is the input, is the output, is the model's parameters, is the distillation weight, CE is the cross-entropy loss, and MSE is the mean squared error loss. Simply, the selfdistillation weight balances the importance of both loss functions (CE and MSE) to update the trainable parameters of based on the loss functions of and , which in turn improves at the following time step. Distilling BERT-based models like this has shown improvement in many benchmark natural language processing (NLP) tasks [31].\n\n\nEMPIRICAL RESULTS\n\nThe results of the experiment are shown in Table 2, and further discussed below.\n\n\nBenchmark Experiments\n\nWe compared the proposed CVET against prevailing and stateof-the-art classical machine learning, deep learning, and pretrained language models. Each benchmark model is commonly used for CVE data machine learning and/or text classification tasks. The models selected in each category are: All classical machine learning models were implemented using the Python library scikit-learn. All deep learning models were implemented with the Python library Keras. The pretrained language models were implemented using the Huggingface Transformers library. The CVET model used RoBERTa-large from the Huggingface library [28], and our selfdistillation and fine-tuning designs were implemented in PyTorch 1.4 [19].\n\nAll models are run with 10-fold cross-validation. The benchmark models are evaluated with the accuracy, precision, recall, and F1-score metrics, which is the standard for multiclass text classification tasks [25]. Paired t-tests are used to identify if statistically significant differences exist between CVET and each benchmark method. Due to our unbalanced dataset, the discussion focuses on the F1-score (which is more resilient against skewed distributions than the other metrics) [13].\n\n\nResults and Discussion\n\nFrom Table 2, we make four key observations about the results of our experiments. First, the four classical machine learning models had the lowest F1-scores, ranging from 36.70% for Random Forest to 48.66% for SVM. Second, all deep learning models reached a higher F1-score than the classical machine learning models. The transformer model had the highest F1score (73.61%) among the deep learning models. The transformer does not have an internal recurrent mechanism like the other models, which suggests its multi-head attention architecture helped improve the text classi\ufffdication performance. Third, all of the PTLMs marginally improved upon the transformer model in F1-score. Baseline RoBERTa improved over the transformer by 0.96% (from 73.61% to 74.57%). These results suggest that pre-training the transformer can improve the performances of specialized text classi\ufffdication tasks. Finally, our proposed CVET model outperformed all other PTLMs, deep learning models, and classical machine learning models in F1-score (76.18%). The differences were signi\ufffdicant at <0.05 or less in all models. Our CVET model also achieved the best accuracy (79.93%) versus all other models. These results suggest that our \ufffdine-tuning and self-KD design assisted in performance improvement in the target task. In this study, we developed a novel self-distillation approach to automatically label CVEs with their associated ATT&CK tactic.\n\n\nCONCLUSION\n\nThe CVET model \ufffdine-tuning process included an Adam loss function with added bias correction and a self-KD design. Our model was evaluated with a series of experiments against stateof-the-art models in classical machine learning, deep learning, and pre-trained language models. Results indicated that the CVET model offers a signi\ufffdicant bene\ufffdit to labeling CVEs with MITRE ATT&CK tactics over baseline non-distillation techniques. Our architecture can greatly assist the cybersecurity community by creating an immediate link between a heavily utilized cybersecurity risk management framework and critical vulnerabilities. This can be implemented into key cybersecurity stakeholders' work\ufffdlow to associate vulnerabilities found in their scanners to the MITRE ATT&CK framework for additional information on how to combat the vulnerability. We identify two potential future directions for related work in this domain. First, we would like to expand the connection of CVEs to other prominent CRMFs. Potential options include the CAPEC list and the National Institute of Standards and Technology (NIST) framework. Such connections can broaden the mitigation strategies provided by this work when a CVE is discovered. Second, we plan to look into other more re\ufffdined textual data representation techniques (e.g., novel word embedding strategies, synonym/homonym generation, POS tagging) to improve the features of our textual inputs.\n\nFigure 1 :\n1Example CVE from cve.mitre.org\n\n\n\u2022 Classical Machine Learning: Random Forest, SVM, Na\u00efve Bayes, Logistic Regression \u2022 Deep Learning: RNN, GRU, LSTM, BiLSTM, BiLSTM with attention, Transformer \u2022 Pre-Trained Language Models: GPT-2, XLNet, BERT, RoBERTa\n\n\nBenchmark Experiments. Each component is further detailed in the subsequent sections.4.1 Data Collection and Pre-Processing \n\nATT&CK Tactic \nCount of CVEs \nDefense Evasion \n8,482 \nDiscovery \n6,647 \nPrivilege Escalation 5,779 \nCollection \n1,748 \nLateral Movement 715 \nImpact \n594 \nCredential Access \n427 \nInitial Access \n309 \nExfiltration \n137 \nExecution \n25 \nTotal \n24,863 \nTable 1: Gold-Standard Dataset CVE Distribution \n\n\n\n\n: Comparing CVET Against Benchmark Classical Machine Learning, Deep Learning, and Pre-Trained Language Models (*: p<0.05, **: p<0.01, ***: p<0.001)Model Type \nModel \nAccuracy \nPrecision \nRecall \nF1-score \n\nClassical Machine Learning \n\nRandom Forest \n63.70% *** \n35.83% *** \n37.67% *** \n36.70% *** \nSVM \n65.70% *** \n51.23% *** \n46.34% *** \n48.66% *** \nNaive Bayes \n67.30% *** \n44.22% *** \n33.92% *** \n38.16% *** \nLogistic Regression \n67.10% *** \n41.65% *** \n34.12% *** \n37.38% *** \n\nDeep Learning \n\nRNN \n68.45% *** \n69.66% *** \n67.30% *** \n68.46% *** \nGRU \n70.90% *** \n72.55% *** \n69.19% *** \n70.83% *** \nLSTM \n72.75% *** \n74.14% *** \n71.89% \n72.00% *** \nBiLSTM \n72.55% *** \n73.71% *** \n71.71% \n72.70% *** \nBiLSTM with Attention \n71.41% *** \n72.52% *** \n70.32% * \n71.40% *** \nTransformer \n72.45% *** \n74.49% *** \n70.82% * \n73.61% *** \n\nPre-Trained Language Models \n\nGPT-2 \n70.21% *** \n77.12% *** \n64.56% *** \n70.27% *** \nXLNet \n74.12% *** \n80.12% * \n68.56% *** \n73.88% *** \nBERT \n73.93% ** \n79.86% * \n69.41% * \n74.26% * \nRoBERTa \n74.42% * \n81.88% \n68.49% ** \n74.57% * \nSelf-Distillation \nCVET \n76.93% \n81.38% \n71.49% \n76.18% \nTable 2(3 \n\n\n\nStatistical Learning of APT TTP Chains from MITRE ATT&CK. R Al-Shaer, E Al-Shaer, M Ahmed, Proc. RSA Conf. RSA ConfAl-Shaer, R., Al-Shaer, E. and Ahmed, M. 2017. Statistical Learning of APT TTP Chains from MITRE ATT&CK. Proc. RSA Conf. (2017), 1-2.\n\nAn Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels. I Chalkidis, M Fergadiotis, S Kotitsas, P Malakasiotis, N Aletras, I Androutsopoulos, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Stroudsburg, PA, USAChalkidis, I., Fergadiotis, M., Kotitsas, S., Malakasiotis, P., Aletras, N. and Androutsopoulos, I. 2020. An Empirical Study on Large-Scale Multi- Label Text Classification Including Few and Zero-Shot Labels. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Stroudsburg, PA, USA, 2020), 7503- 7515.\n\nBERT: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, arXiv. Devlin, J., Chang, M.W., Lee, K. and Toutanova, K. 2018. BERT: Pre- training of deep bidirectional transformers for language understanding. arXiv. (2018), 4171-4186.\n\nJ Dodge, G Ilharco, R Schwartz, A Farhadi, H Hajishirzi, N Smith, Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv. Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H. and Smith, N. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv. (2020).\n\n. T Furlanello, Z C Lipton, M Tschannen, L Itti, A Anandkumar, Born-Again Neural Networks. Furlanello, T., Lipton, Z.C., Tschannen, M., Itti, L. and Anandkumar, A. 2018. Born-Again Neural Networks. (2018).\n\nJoint Prediction of Multiple Vulnerability Characteristics Through Multi-Task Learning. X Gong, Z Xing, X Li, Z Feng, Z Han, 24th International Conference on Engineering of Complex Computer Systems (ICECCS). Gong, X., Xing, Z., Li, X., Feng, Z. and Han, Z. 2019. Joint Prediction of Multiple Vulnerability Characteristics Through Multi-Task Learning. 2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS) (Nov. 2019), 31-40.\n\nPredicting Missing Information of Vulnerability Reports. H Guo, Z Xing, X Li, Companion Proceedings of the Web Conference 2020. New York, NY, USAGuo, H., Xing, Z. and Li, X. 2020. Predicting Missing Information of Vulnerability Reports. Companion Proceedings of the Web Conference 2020 (New York, NY, USA, Apr. 2020), 81-82.\n\nMulti-Scale Self-Attention for Text Classification. Q Guo, X Qiu, P Liu, X Xue, Z Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Guo, Q., Qiu, X., Liu, P., Xue, X. and Zhang, Z. 2020. Multi-Scale Self- Attention for Text Classification. Proceedings of the AAAI Conference on Artificial Intelligence. 34, 05 (Apr. 2020), 7847-7854.\n\nSelf-Knowledge Distillation in Natural Language Processing. S Hahn, H Choi, Proceedings -Natural Language Processing in a Deep Learning World. -Natural Language Processing in a Deep Learning WorldHahn, S. and Choi, H. 2019. Self-Knowledge Distillation in Natural Language Processing. Proceedings -Natural Language Processing in a Deep Learning World (Oct. 2019), 423-430.\n\nLearning to Predict Severity of Software Vulnerability Using Only Vulnerability Description. Z Han, X Li, Z Xing, H Liu, Z Feng, IEEE International Conference on Software Maintenance and Evolution (ICSME). Han, Z., Li, X., Xing, Z., Liu, H. and Feng, Z. 2017. Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description. 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME) (Sep. 2017), 125-136.\n\nE Hemberg, J Kelly, M Shlapentokh-Rothman, B Reinstadler, K Xu, N Rutar, O&apos;reilly, M. 2020. BRON --Linking Attack Tactics, Techniques, and Patterns with Defensive Weaknesses, Vulnerabilities and Affected Platform Configurations. arXiv. Hemberg, E., Kelly, J., Shlapentokh-Rothman, M., Reinstadler, B., Xu, K., Rutar, N. and O'Reilly, U.-M. 2020. BRON --Linking Attack Tactics, Techniques, and Patterns with Defensive Weaknesses, Vulnerabilities and Affected Platform Configurations. arXiv. (Oct. 2020).\n\nDistilling the Knowledge in a Neural Network. G Hinton, O Vinyals, J Dean, Hinton, G., Vinyals, O. and Dean, J. 2015. Distilling the Knowledge in a Neural Network. (2015), 1-9.\n\nFacing Imbalanced Data--Recommendations for the Use of Performance Metrics. L A Jeni, J F Cohn, F De La Torre, Humaine Association Conference on Affective Computing and Intelligent Interaction. Jeni, L.A., Cohn, J.F. and de La Torre, F. 2013. Facing Imbalanced Data-- Recommendations for the Use of Performance Metrics. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction (Sep. 2013), 245-251.\n\nIs BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. D Jin, Z Jin, J T Zhou, P Szolovits, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Jin, D., Jin, Z., Zhou, J.T. and Szolovits, P. 2020. Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. Proceedings of the AAAI Conference on Artificial Intelligence. 34, 05 (Apr. 2020), 8018-8025.\n\nDeep Learning for NLP and Speech Recognition. U Kamath, J Liu, J Whitaker, Springer International PublishingKamath, U., Liu, J. and Whitaker, J. 2019. Deep Learning for NLP and Speech Recognition. Springer International Publishing.\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, RoBERTa: A robustly optimized BERT pretraining approach. 1Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv. 1 (2019).\n\nSelf-distillation amplifies regularization in Hilbert space. H Mobahi, M Farajtabar, P L Bartlett, Mobahi, H., Farajtabar, M. and Bartlett, P.L. 2020. Self-distillation amplifies regularization in Hilbert space. arXiv. (2020), 1-34.\n\nM Mosbach, M Andriushchenko, D Klakow, On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines. Mosbach, M., Andriushchenko, M. and Klakow, D. 2020. On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines. arXiv. (2020).\n\nPyTorch: An imperative style, high-performance deep learning library. A Paszke, arXiv. NeurIPSPaszke, A. et al. 2019. PyTorch: An imperative style, high-performance deep learning library. arXiv. NeurIPS (2019).\n\nPretrained models for natural language processing: a survey. X P Qiu, T X Sun, Y G Xu, Y F Shao, N Dai, X J Huang, arXiv. 63Qiu, X.P., Sun, T.X., Xu, Y.G., Shao, Y.F., Dai, N. and Huang, X.J. 2020. Pre- trained models for natural language processing: a survey. arXiv. 63, 10 (2020), 1872-1897.\n\nB E Strom, D P Miller, K C Nickels, A G Pennington, C B Thomas, MITRE ATT&CK TM : Design and Philosophy. Strom, B.E., Miller, D.P., Nickels, K.C., Pennington, A.G. and Thomas, C.B. 2018. MITRE ATT&CK TM : Design and Philosophy. July (2018).\n\nModeling Malicious Hacking Data Breach Risks. H Sun, M Xu, P Zhao, North American Actuarial Journal. 0Sun, H., Xu, M. and Zhao, P. 2020. Modeling Malicious Hacking Data Breach Risks. North American Actuarial Journal. 0, 0 (Jul. 2020), 1-19.\n\nJ Sun, Z Xing, H Guo, D Ye, X Li, X Xu, L Zhu, Generating Informative CVE Description From ExploitDB Posts by Extractive Summarization. arXiv. Sun, J., Xing, Z., Guo, H., Ye, D., Li, X., Xu, X. and Zhu, L. 2021. Generating Informative CVE Description From ExploitDB Posts by Extractive Summarization. arXiv. (Jan. 2021), 1-36.\n\nCollaborative Teacher-Student Learning via Multiple Knowledge Transfer. L Sun, J Gou, B Yu, L Du, D Tao, Sun, L., Gou, J., Yu, B., Du, L. and Tao, D. 2021. Collaborative Teacher- Student Learning via Multiple Knowledge Transfer. (2021).\n\nText Classification Techniques: A Literature Review. M Thangaraj, M Sivakami, Interdisciplinary Journal of Information, Knowledge, and Management. 13Thangaraj, M. and Sivakami, M. 2018. Text Classification Techniques: A Literature Review. Interdisciplinary Journal of Information, Knowledge, and Management. 13, (2018), 117-135.\n\nAttention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I. 2017. Attention Is All You Need. Advances in Neural Information Processing Systems. (Jun. 2017), 5999-6009.\n\nR Wang, Z Li, J Cao, T Chen, L Wang, Convolutional Recurrent Neural Networks for Text Classification. 2019 International Joint Conference on Neural Networks (IJCNN). Wang, R., Li, Z., Cao, J., Chen, T. and Wang, L. 2019. Convolutional Recurrent Neural Networks for Text Classification. 2019 International Joint Conference on Neural Networks (IJCNN) (Jul. 2019), 1-6.\n\nT Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Brew, Transformers: State-of-the-art natural language processing. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M. and Brew, J. 2019. Transformers: State-of-the-art natural language processing. arXiv. (2019).\n\nEmbedding and predicting software security entity relationships: A knowledge graph based approach. H Xiao, Z Xing, X Li, H Guo, Springer International PublishingXiao, H., Xing, Z., Li, X. and Guo, H. 2019. Embedding and predicting software security entity relationships: A knowledge graph based approach. Springer International Publishing.\n\nKnowledge Distillation Meets Self-supervision. G Xu, Z Liu, X Li, C C Loy, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics. Xu, G., Liu, Z., Li, X. and Loy, C.C. 2020. Knowledge Distillation Meets Self-supervision. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 588-604.\n\nImproving BERT fine-tuning via self-ensemble and self-distillation. arXiv. Y Xu, X Qiu, L Zhou, X Huang, Xu, Y., Qiu, X., Zhou, L. and Huang, X. 2020. Improving BERT fine-tuning via self-ensemble and self-distillation. arXiv. (2020).\n\nTraining Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students. C Yang, L Xie, S Qiao, A L Yuille, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Yang, C., Xie, L., Qiao, S. and Yuille, A.L. 2019. Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students. Proceedings of the AAAI Conference on Artificial Intelligence. 33, (Jul. 2019), 5628-5635.\n\nBe your own teacher: Improve the performance of convolutional neural networks via self distillation. L Zhang, J Song, A Gao, J Chen, C Bao, K Ma, Zhang, L., Song, J., Gao, A., Chen, J., Bao, C. and Ma, K. 2019. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. arXiv. (2019), 3713-3722.\n", "annotations": {"author": "[{\"end\":274,\"start\":108},{\"end\":388,\"start\":275},{\"end\":498,\"start\":389},{\"end\":570,\"start\":499}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":117},{\"end\":288,\"start\":281},{\"end\":402,\"start\":396},{\"end\":512,\"start\":508}]", "author_first_name": "[{\"end\":116,\"start\":108},{\"end\":280,\"start\":275},{\"end\":395,\"start\":389},{\"end\":507,\"start\":499}]", "author_affiliation": "[{\"end\":273,\"start\":143},{\"end\":387,\"start\":306},{\"end\":497,\"start\":404},{\"end\":569,\"start\":535}]", "title": "[{\"end\":105,\"start\":1},{\"end\":675,\"start\":571}]", "venue": null, "abstract": "[{\"end\":2265,\"start\":976}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2520,\"start\":2516},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3736,\"start\":3732},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3908,\"start\":3905},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4951,\"start\":4947},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6254,\"start\":6250},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6423,\"start\":6419},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6512,\"start\":6508},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6684,\"start\":6681},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6744,\"start\":6741},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6874,\"start\":6870},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7371,\"start\":7367},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7718,\"start\":7714},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8334,\"start\":8331},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8502,\"start\":8498},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8761,\"start\":8758},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8780,\"start\":8776},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8989,\"start\":8985},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9164,\"start\":9160},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9679,\"start\":9675},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9808,\"start\":9804},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9895,\"start\":9891},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9938,\"start\":9935},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9971,\"start\":9967},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10203,\"start\":10200},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10496,\"start\":10492},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11238,\"start\":11234},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11883,\"start\":11880},{\"end\":11887,\"start\":11883},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12207,\"start\":12203},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12252,\"start\":12248},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12499,\"start\":12495},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12631,\"start\":12628},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13012,\"start\":13009},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13066,\"start\":13062},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13278,\"start\":13275},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14403,\"start\":14399},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15146,\"start\":15142},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15233,\"start\":15229},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15448,\"start\":15444},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15725,\"start\":15721}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":18661,\"start\":18618},{\"attributes\":{\"id\":\"fig_2\"},\"end\":18881,\"start\":18662},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19308,\"start\":18882},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20448,\"start\":19309}]", "paragraph": "[{\"end\":3537,\"start\":2281},{\"end\":4385,\"start\":3539},{\"end\":4800,\"start\":4387},{\"end\":5416,\"start\":4802},{\"end\":5928,\"start\":5418},{\"end\":7484,\"start\":5978},{\"end\":8990,\"start\":7525},{\"end\":10204,\"start\":9022},{\"end\":10837,\"start\":10236},{\"end\":10981,\"start\":10843},{\"end\":11888,\"start\":11001},{\"end\":12423,\"start\":11890},{\"end\":12817,\"start\":12464},{\"end\":13399,\"start\":12834},{\"end\":13678,\"start\":13436},{\"end\":14404,\"start\":13711},{\"end\":14506,\"start\":14426},{\"end\":15234,\"start\":14532},{\"end\":15726,\"start\":15236},{\"end\":17176,\"start\":15753},{\"end\":18617,\"start\":17191}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13421,\"start\":13400},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13435,\"start\":13421}]", "table_ref": "[{\"end\":11657,\"start\":11650},{\"end\":14476,\"start\":14469},{\"end\":15765,\"start\":15758}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2279,\"start\":2267},{\"attributes\":{\"n\":\"2\"},\"end\":5948,\"start\":5931},{\"attributes\":{\"n\":\"2.1\"},\"end\":5976,\"start\":5951},{\"attributes\":{\"n\":\"2.2\"},\"end\":7523,\"start\":7487},{\"attributes\":{\"n\":\"2.3\"},\"end\":9020,\"start\":8993},{\"attributes\":{\"n\":\"3\"},\"end\":10234,\"start\":10207},{\"end\":10841,\"start\":10840},{\"attributes\":{\"n\":\"4\"},\"end\":10999,\"start\":10984},{\"attributes\":{\"n\":\"4.2\"},\"end\":12443,\"start\":12426},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":12462,\"start\":12446},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":12832,\"start\":12820},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":13709,\"start\":13681},{\"attributes\":{\"n\":\"5\"},\"end\":14424,\"start\":14407},{\"attributes\":{\"n\":\"5.1\"},\"end\":14530,\"start\":14509},{\"attributes\":{\"n\":\"5.2\"},\"end\":15751,\"start\":15729},{\"attributes\":{\"n\":\"6\"},\"end\":17189,\"start\":17179},{\"end\":18629,\"start\":18619}]", "table": "[{\"end\":19308,\"start\":18969},{\"end\":20448,\"start\":19458}]", "figure_caption": "[{\"end\":18661,\"start\":18631},{\"end\":18881,\"start\":18664},{\"end\":18969,\"start\":18884},{\"end\":19458,\"start\":19311}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3030,\"start\":3022}]", "bib_author_first_name": "[{\"end\":20509,\"start\":20508},{\"end\":20521,\"start\":20520},{\"end\":20533,\"start\":20532},{\"end\":20803,\"start\":20802},{\"end\":20816,\"start\":20815},{\"end\":20831,\"start\":20830},{\"end\":20843,\"start\":20842},{\"end\":20859,\"start\":20858},{\"end\":20870,\"start\":20869},{\"end\":21512,\"start\":21511},{\"end\":21522,\"start\":21521},{\"end\":21524,\"start\":21523},{\"end\":21533,\"start\":21532},{\"end\":21540,\"start\":21539},{\"end\":21727,\"start\":21726},{\"end\":21736,\"start\":21735},{\"end\":21747,\"start\":21746},{\"end\":21759,\"start\":21758},{\"end\":21770,\"start\":21769},{\"end\":21784,\"start\":21783},{\"end\":22098,\"start\":22097},{\"end\":22112,\"start\":22111},{\"end\":22114,\"start\":22113},{\"end\":22124,\"start\":22123},{\"end\":22137,\"start\":22136},{\"end\":22145,\"start\":22144},{\"end\":22391,\"start\":22390},{\"end\":22399,\"start\":22398},{\"end\":22407,\"start\":22406},{\"end\":22413,\"start\":22412},{\"end\":22421,\"start\":22420},{\"end\":22819,\"start\":22818},{\"end\":22826,\"start\":22825},{\"end\":22834,\"start\":22833},{\"end\":23140,\"start\":23139},{\"end\":23147,\"start\":23146},{\"end\":23154,\"start\":23153},{\"end\":23161,\"start\":23160},{\"end\":23168,\"start\":23167},{\"end\":23551,\"start\":23550},{\"end\":23559,\"start\":23558},{\"end\":23957,\"start\":23956},{\"end\":23964,\"start\":23963},{\"end\":23970,\"start\":23969},{\"end\":23978,\"start\":23977},{\"end\":23985,\"start\":23984},{\"end\":24321,\"start\":24320},{\"end\":24332,\"start\":24331},{\"end\":24341,\"start\":24340},{\"end\":24364,\"start\":24363},{\"end\":24379,\"start\":24378},{\"end\":24385,\"start\":24384},{\"end\":24876,\"start\":24875},{\"end\":24886,\"start\":24885},{\"end\":24897,\"start\":24896},{\"end\":25084,\"start\":25083},{\"end\":25086,\"start\":25085},{\"end\":25094,\"start\":25093},{\"end\":25096,\"start\":25095},{\"end\":25104,\"start\":25103},{\"end\":25546,\"start\":25545},{\"end\":25553,\"start\":25552},{\"end\":25560,\"start\":25559},{\"end\":25562,\"start\":25561},{\"end\":25570,\"start\":25569},{\"end\":25996,\"start\":25995},{\"end\":26006,\"start\":26005},{\"end\":26013,\"start\":26012},{\"end\":26183,\"start\":26182},{\"end\":26190,\"start\":26189},{\"end\":26197,\"start\":26196},{\"end\":26206,\"start\":26205},{\"end\":26212,\"start\":26211},{\"end\":26221,\"start\":26220},{\"end\":26229,\"start\":26228},{\"end\":26237,\"start\":26236},{\"end\":26246,\"start\":26245},{\"end\":26261,\"start\":26260},{\"end\":26585,\"start\":26584},{\"end\":26595,\"start\":26594},{\"end\":26609,\"start\":26608},{\"end\":26611,\"start\":26610},{\"end\":26758,\"start\":26757},{\"end\":26769,\"start\":26768},{\"end\":26787,\"start\":26786},{\"end\":27116,\"start\":27115},{\"end\":27319,\"start\":27318},{\"end\":27321,\"start\":27320},{\"end\":27328,\"start\":27327},{\"end\":27330,\"start\":27329},{\"end\":27337,\"start\":27336},{\"end\":27339,\"start\":27338},{\"end\":27345,\"start\":27344},{\"end\":27347,\"start\":27346},{\"end\":27355,\"start\":27354},{\"end\":27362,\"start\":27361},{\"end\":27364,\"start\":27363},{\"end\":27553,\"start\":27552},{\"end\":27555,\"start\":27554},{\"end\":27564,\"start\":27563},{\"end\":27566,\"start\":27565},{\"end\":27576,\"start\":27575},{\"end\":27578,\"start\":27577},{\"end\":27589,\"start\":27588},{\"end\":27591,\"start\":27590},{\"end\":27605,\"start\":27604},{\"end\":27607,\"start\":27606},{\"end\":27841,\"start\":27840},{\"end\":27848,\"start\":27847},{\"end\":27854,\"start\":27853},{\"end\":28037,\"start\":28036},{\"end\":28044,\"start\":28043},{\"end\":28052,\"start\":28051},{\"end\":28059,\"start\":28058},{\"end\":28065,\"start\":28064},{\"end\":28071,\"start\":28070},{\"end\":28077,\"start\":28076},{\"end\":28437,\"start\":28436},{\"end\":28444,\"start\":28443},{\"end\":28451,\"start\":28450},{\"end\":28457,\"start\":28456},{\"end\":28463,\"start\":28462},{\"end\":28656,\"start\":28655},{\"end\":28669,\"start\":28668},{\"end\":28960,\"start\":28959},{\"end\":28971,\"start\":28970},{\"end\":28982,\"start\":28981},{\"end\":28992,\"start\":28991},{\"end\":29005,\"start\":29004},{\"end\":29014,\"start\":29013},{\"end\":29016,\"start\":29015},{\"end\":29025,\"start\":29024},{\"end\":29035,\"start\":29034},{\"end\":29316,\"start\":29315},{\"end\":29324,\"start\":29323},{\"end\":29330,\"start\":29329},{\"end\":29337,\"start\":29336},{\"end\":29345,\"start\":29344},{\"end\":29684,\"start\":29683},{\"end\":29692,\"start\":29691},{\"end\":29701,\"start\":29700},{\"end\":29709,\"start\":29708},{\"end\":29721,\"start\":29720},{\"end\":29733,\"start\":29732},{\"end\":29740,\"start\":29739},{\"end\":29750,\"start\":29749},{\"end\":29759,\"start\":29758},{\"end\":29767,\"start\":29766},{\"end\":29780,\"start\":29779},{\"end\":30157,\"start\":30156},{\"end\":30165,\"start\":30164},{\"end\":30173,\"start\":30172},{\"end\":30179,\"start\":30178},{\"end\":30446,\"start\":30445},{\"end\":30452,\"start\":30451},{\"end\":30459,\"start\":30458},{\"end\":30465,\"start\":30464},{\"end\":30467,\"start\":30466},{\"end\":30917,\"start\":30916},{\"end\":30923,\"start\":30922},{\"end\":30930,\"start\":30929},{\"end\":30938,\"start\":30937},{\"end\":31173,\"start\":31172},{\"end\":31181,\"start\":31180},{\"end\":31188,\"start\":31187},{\"end\":31196,\"start\":31195},{\"end\":31198,\"start\":31197},{\"end\":31659,\"start\":31658},{\"end\":31668,\"start\":31667},{\"end\":31676,\"start\":31675},{\"end\":31683,\"start\":31682},{\"end\":31691,\"start\":31690},{\"end\":31698,\"start\":31697}]", "bib_author_last_name": "[{\"end\":20518,\"start\":20510},{\"end\":20530,\"start\":20522},{\"end\":20539,\"start\":20534},{\"end\":20813,\"start\":20804},{\"end\":20828,\"start\":20817},{\"end\":20840,\"start\":20832},{\"end\":20856,\"start\":20844},{\"end\":20867,\"start\":20860},{\"end\":20886,\"start\":20871},{\"end\":21519,\"start\":21513},{\"end\":21530,\"start\":21525},{\"end\":21537,\"start\":21534},{\"end\":21550,\"start\":21541},{\"end\":21733,\"start\":21728},{\"end\":21744,\"start\":21737},{\"end\":21756,\"start\":21748},{\"end\":21767,\"start\":21760},{\"end\":21781,\"start\":21771},{\"end\":21790,\"start\":21785},{\"end\":22109,\"start\":22099},{\"end\":22121,\"start\":22115},{\"end\":22134,\"start\":22125},{\"end\":22142,\"start\":22138},{\"end\":22156,\"start\":22146},{\"end\":22396,\"start\":22392},{\"end\":22404,\"start\":22400},{\"end\":22410,\"start\":22408},{\"end\":22418,\"start\":22414},{\"end\":22425,\"start\":22422},{\"end\":22823,\"start\":22820},{\"end\":22831,\"start\":22827},{\"end\":22837,\"start\":22835},{\"end\":23144,\"start\":23141},{\"end\":23151,\"start\":23148},{\"end\":23158,\"start\":23155},{\"end\":23165,\"start\":23162},{\"end\":23174,\"start\":23169},{\"end\":23556,\"start\":23552},{\"end\":23564,\"start\":23560},{\"end\":23961,\"start\":23958},{\"end\":23967,\"start\":23965},{\"end\":23975,\"start\":23971},{\"end\":23982,\"start\":23979},{\"end\":23990,\"start\":23986},{\"end\":24329,\"start\":24322},{\"end\":24338,\"start\":24333},{\"end\":24361,\"start\":24342},{\"end\":24376,\"start\":24365},{\"end\":24382,\"start\":24380},{\"end\":24391,\"start\":24386},{\"end\":24406,\"start\":24393},{\"end\":24883,\"start\":24877},{\"end\":24894,\"start\":24887},{\"end\":24902,\"start\":24898},{\"end\":25091,\"start\":25087},{\"end\":25101,\"start\":25097},{\"end\":25116,\"start\":25105},{\"end\":25550,\"start\":25547},{\"end\":25557,\"start\":25554},{\"end\":25567,\"start\":25563},{\"end\":25580,\"start\":25571},{\"end\":26003,\"start\":25997},{\"end\":26010,\"start\":26007},{\"end\":26022,\"start\":26014},{\"end\":26187,\"start\":26184},{\"end\":26194,\"start\":26191},{\"end\":26203,\"start\":26198},{\"end\":26209,\"start\":26207},{\"end\":26218,\"start\":26213},{\"end\":26226,\"start\":26222},{\"end\":26234,\"start\":26230},{\"end\":26243,\"start\":26238},{\"end\":26258,\"start\":26247},{\"end\":26270,\"start\":26262},{\"end\":26592,\"start\":26586},{\"end\":26606,\"start\":26596},{\"end\":26620,\"start\":26612},{\"end\":26766,\"start\":26759},{\"end\":26784,\"start\":26770},{\"end\":26794,\"start\":26788},{\"end\":27123,\"start\":27117},{\"end\":27325,\"start\":27322},{\"end\":27334,\"start\":27331},{\"end\":27342,\"start\":27340},{\"end\":27352,\"start\":27348},{\"end\":27359,\"start\":27356},{\"end\":27370,\"start\":27365},{\"end\":27561,\"start\":27556},{\"end\":27573,\"start\":27567},{\"end\":27586,\"start\":27579},{\"end\":27602,\"start\":27592},{\"end\":27614,\"start\":27608},{\"end\":27845,\"start\":27842},{\"end\":27851,\"start\":27849},{\"end\":27859,\"start\":27855},{\"end\":28041,\"start\":28038},{\"end\":28049,\"start\":28045},{\"end\":28056,\"start\":28053},{\"end\":28062,\"start\":28060},{\"end\":28068,\"start\":28066},{\"end\":28074,\"start\":28072},{\"end\":28081,\"start\":28078},{\"end\":28441,\"start\":28438},{\"end\":28448,\"start\":28445},{\"end\":28454,\"start\":28452},{\"end\":28460,\"start\":28458},{\"end\":28467,\"start\":28464},{\"end\":28666,\"start\":28657},{\"end\":28678,\"start\":28670},{\"end\":28968,\"start\":28961},{\"end\":28979,\"start\":28972},{\"end\":28989,\"start\":28983},{\"end\":29002,\"start\":28993},{\"end\":29011,\"start\":29006},{\"end\":29022,\"start\":29017},{\"end\":29032,\"start\":29026},{\"end\":29046,\"start\":29036},{\"end\":29321,\"start\":29317},{\"end\":29327,\"start\":29325},{\"end\":29334,\"start\":29331},{\"end\":29342,\"start\":29338},{\"end\":29350,\"start\":29346},{\"end\":29689,\"start\":29685},{\"end\":29698,\"start\":29693},{\"end\":29706,\"start\":29702},{\"end\":29718,\"start\":29710},{\"end\":29730,\"start\":29722},{\"end\":29737,\"start\":29734},{\"end\":29747,\"start\":29741},{\"end\":29756,\"start\":29751},{\"end\":29764,\"start\":29760},{\"end\":29777,\"start\":29768},{\"end\":29785,\"start\":29781},{\"end\":30162,\"start\":30158},{\"end\":30170,\"start\":30166},{\"end\":30176,\"start\":30174},{\"end\":30183,\"start\":30180},{\"end\":30449,\"start\":30447},{\"end\":30456,\"start\":30453},{\"end\":30462,\"start\":30460},{\"end\":30471,\"start\":30468},{\"end\":30920,\"start\":30918},{\"end\":30927,\"start\":30924},{\"end\":30935,\"start\":30931},{\"end\":30944,\"start\":30939},{\"end\":31178,\"start\":31174},{\"end\":31185,\"start\":31182},{\"end\":31193,\"start\":31189},{\"end\":31205,\"start\":31199},{\"end\":31665,\"start\":31660},{\"end\":31673,\"start\":31669},{\"end\":31680,\"start\":31677},{\"end\":31688,\"start\":31684},{\"end\":31695,\"start\":31692},{\"end\":31701,\"start\":31699}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":20698,\"start\":20450},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":222133909},\"end\":21428,\"start\":20700},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52967399},\"end\":21724,\"start\":21430},{\"attributes\":{\"id\":\"b3\"},\"end\":22093,\"start\":21726},{\"attributes\":{\"id\":\"b4\"},\"end\":22300,\"start\":22095},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":204939263},\"end\":22759,\"start\":22302},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218523305},\"end\":23085,\"start\":22761},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":208527147},\"end\":23488,\"start\":23087},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":199453123},\"end\":23861,\"start\":23490},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":33064240},\"end\":24318,\"start\":23863},{\"attributes\":{\"id\":\"b10\"},\"end\":24827,\"start\":24320},{\"attributes\":{\"id\":\"b11\"},\"end\":25005,\"start\":24829},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2980898},\"end\":25435,\"start\":25007},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":202539059},\"end\":25947,\"start\":25437},{\"attributes\":{\"id\":\"b14\"},\"end\":26180,\"start\":25949},{\"attributes\":{\"id\":\"b15\"},\"end\":26521,\"start\":26182},{\"attributes\":{\"id\":\"b16\"},\"end\":26755,\"start\":26523},{\"attributes\":{\"id\":\"b17\"},\"end\":27043,\"start\":26757},{\"attributes\":{\"doi\":\"arXiv. NeurIPS\",\"id\":\"b18\"},\"end\":27255,\"start\":27045},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":212747830},\"end\":27550,\"start\":27257},{\"attributes\":{\"id\":\"b20\"},\"end\":27792,\"start\":27552},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":225634330},\"end\":28034,\"start\":27794},{\"attributes\":{\"id\":\"b22\"},\"end\":28362,\"start\":28036},{\"attributes\":{\"id\":\"b23\"},\"end\":28600,\"start\":28364},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":196207964},\"end\":28930,\"start\":28602},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":13756489},\"end\":29313,\"start\":28932},{\"attributes\":{\"id\":\"b26\"},\"end\":29681,\"start\":29315},{\"attributes\":{\"id\":\"b27\"},\"end\":30055,\"start\":29683},{\"attributes\":{\"id\":\"b28\"},\"end\":30396,\"start\":30057},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219636179},\"end\":30839,\"start\":30398},{\"attributes\":{\"id\":\"b30\"},\"end\":31074,\"start\":30841},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":54986302},\"end\":31555,\"start\":31076},{\"attributes\":{\"id\":\"b32\"},\"end\":31894,\"start\":31557}]", "bib_title": "[{\"end\":20506,\"start\":20450},{\"end\":20800,\"start\":20700},{\"end\":21509,\"start\":21430},{\"end\":22388,\"start\":22302},{\"end\":22816,\"start\":22761},{\"end\":23137,\"start\":23087},{\"end\":23548,\"start\":23490},{\"end\":23954,\"start\":23863},{\"end\":25081,\"start\":25007},{\"end\":25543,\"start\":25437},{\"end\":27316,\"start\":27257},{\"end\":27838,\"start\":27794},{\"end\":28653,\"start\":28602},{\"end\":28957,\"start\":28932},{\"end\":30443,\"start\":30398},{\"end\":31170,\"start\":31076}]", "bib_author": "[{\"end\":20520,\"start\":20508},{\"end\":20532,\"start\":20520},{\"end\":20541,\"start\":20532},{\"end\":20815,\"start\":20802},{\"end\":20830,\"start\":20815},{\"end\":20842,\"start\":20830},{\"end\":20858,\"start\":20842},{\"end\":20869,\"start\":20858},{\"end\":20888,\"start\":20869},{\"end\":21521,\"start\":21511},{\"end\":21532,\"start\":21521},{\"end\":21539,\"start\":21532},{\"end\":21552,\"start\":21539},{\"end\":21735,\"start\":21726},{\"end\":21746,\"start\":21735},{\"end\":21758,\"start\":21746},{\"end\":21769,\"start\":21758},{\"end\":21783,\"start\":21769},{\"end\":21792,\"start\":21783},{\"end\":22111,\"start\":22097},{\"end\":22123,\"start\":22111},{\"end\":22136,\"start\":22123},{\"end\":22144,\"start\":22136},{\"end\":22158,\"start\":22144},{\"end\":22398,\"start\":22390},{\"end\":22406,\"start\":22398},{\"end\":22412,\"start\":22406},{\"end\":22420,\"start\":22412},{\"end\":22427,\"start\":22420},{\"end\":22825,\"start\":22818},{\"end\":22833,\"start\":22825},{\"end\":22839,\"start\":22833},{\"end\":23146,\"start\":23139},{\"end\":23153,\"start\":23146},{\"end\":23160,\"start\":23153},{\"end\":23167,\"start\":23160},{\"end\":23176,\"start\":23167},{\"end\":23558,\"start\":23550},{\"end\":23566,\"start\":23558},{\"end\":23963,\"start\":23956},{\"end\":23969,\"start\":23963},{\"end\":23977,\"start\":23969},{\"end\":23984,\"start\":23977},{\"end\":23992,\"start\":23984},{\"end\":24331,\"start\":24320},{\"end\":24340,\"start\":24331},{\"end\":24363,\"start\":24340},{\"end\":24378,\"start\":24363},{\"end\":24384,\"start\":24378},{\"end\":24393,\"start\":24384},{\"end\":24408,\"start\":24393},{\"end\":24885,\"start\":24875},{\"end\":24896,\"start\":24885},{\"end\":24904,\"start\":24896},{\"end\":25093,\"start\":25083},{\"end\":25103,\"start\":25093},{\"end\":25118,\"start\":25103},{\"end\":25552,\"start\":25545},{\"end\":25559,\"start\":25552},{\"end\":25569,\"start\":25559},{\"end\":25582,\"start\":25569},{\"end\":26005,\"start\":25995},{\"end\":26012,\"start\":26005},{\"end\":26024,\"start\":26012},{\"end\":26189,\"start\":26182},{\"end\":26196,\"start\":26189},{\"end\":26205,\"start\":26196},{\"end\":26211,\"start\":26205},{\"end\":26220,\"start\":26211},{\"end\":26228,\"start\":26220},{\"end\":26236,\"start\":26228},{\"end\":26245,\"start\":26236},{\"end\":26260,\"start\":26245},{\"end\":26272,\"start\":26260},{\"end\":26594,\"start\":26584},{\"end\":26608,\"start\":26594},{\"end\":26622,\"start\":26608},{\"end\":26768,\"start\":26757},{\"end\":26786,\"start\":26768},{\"end\":26796,\"start\":26786},{\"end\":27125,\"start\":27115},{\"end\":27327,\"start\":27318},{\"end\":27336,\"start\":27327},{\"end\":27344,\"start\":27336},{\"end\":27354,\"start\":27344},{\"end\":27361,\"start\":27354},{\"end\":27372,\"start\":27361},{\"end\":27563,\"start\":27552},{\"end\":27575,\"start\":27563},{\"end\":27588,\"start\":27575},{\"end\":27604,\"start\":27588},{\"end\":27616,\"start\":27604},{\"end\":27847,\"start\":27840},{\"end\":27853,\"start\":27847},{\"end\":27861,\"start\":27853},{\"end\":28043,\"start\":28036},{\"end\":28051,\"start\":28043},{\"end\":28058,\"start\":28051},{\"end\":28064,\"start\":28058},{\"end\":28070,\"start\":28064},{\"end\":28076,\"start\":28070},{\"end\":28083,\"start\":28076},{\"end\":28443,\"start\":28436},{\"end\":28450,\"start\":28443},{\"end\":28456,\"start\":28450},{\"end\":28462,\"start\":28456},{\"end\":28469,\"start\":28462},{\"end\":28668,\"start\":28655},{\"end\":28680,\"start\":28668},{\"end\":28970,\"start\":28959},{\"end\":28981,\"start\":28970},{\"end\":28991,\"start\":28981},{\"end\":29004,\"start\":28991},{\"end\":29013,\"start\":29004},{\"end\":29024,\"start\":29013},{\"end\":29034,\"start\":29024},{\"end\":29048,\"start\":29034},{\"end\":29323,\"start\":29315},{\"end\":29329,\"start\":29323},{\"end\":29336,\"start\":29329},{\"end\":29344,\"start\":29336},{\"end\":29352,\"start\":29344},{\"end\":29691,\"start\":29683},{\"end\":29700,\"start\":29691},{\"end\":29708,\"start\":29700},{\"end\":29720,\"start\":29708},{\"end\":29732,\"start\":29720},{\"end\":29739,\"start\":29732},{\"end\":29749,\"start\":29739},{\"end\":29758,\"start\":29749},{\"end\":29766,\"start\":29758},{\"end\":29779,\"start\":29766},{\"end\":29787,\"start\":29779},{\"end\":30164,\"start\":30156},{\"end\":30172,\"start\":30164},{\"end\":30178,\"start\":30172},{\"end\":30185,\"start\":30178},{\"end\":30451,\"start\":30445},{\"end\":30458,\"start\":30451},{\"end\":30464,\"start\":30458},{\"end\":30473,\"start\":30464},{\"end\":30922,\"start\":30916},{\"end\":30929,\"start\":30922},{\"end\":30937,\"start\":30929},{\"end\":30946,\"start\":30937},{\"end\":31180,\"start\":31172},{\"end\":31187,\"start\":31180},{\"end\":31195,\"start\":31187},{\"end\":31207,\"start\":31195},{\"end\":31667,\"start\":31658},{\"end\":31675,\"start\":31667},{\"end\":31682,\"start\":31675},{\"end\":31690,\"start\":31682},{\"end\":31697,\"start\":31690},{\"end\":31703,\"start\":31697}]", "bib_venue": "[{\"end\":20555,\"start\":20541},{\"end\":20982,\"start\":20888},{\"end\":21557,\"start\":21552},{\"end\":21894,\"start\":21792},{\"end\":22184,\"start\":22158},{\"end\":22508,\"start\":22427},{\"end\":22887,\"start\":22839},{\"end\":23237,\"start\":23176},{\"end\":23631,\"start\":23566},{\"end\":24067,\"start\":23992},{\"end\":24559,\"start\":24408},{\"end\":24873,\"start\":24829},{\"end\":25199,\"start\":25118},{\"end\":25643,\"start\":25582},{\"end\":25993,\"start\":25949},{\"end\":26327,\"start\":26272},{\"end\":26582,\"start\":26523},{\"end\":26884,\"start\":26796},{\"end\":27113,\"start\":27045},{\"end\":27377,\"start\":27372},{\"end\":27655,\"start\":27616},{\"end\":27893,\"start\":27861},{\"end\":28177,\"start\":28083},{\"end\":28434,\"start\":28364},{\"end\":28747,\"start\":28680},{\"end\":29097,\"start\":29048},{\"end\":29479,\"start\":29352},{\"end\":29845,\"start\":29787},{\"end\":30154,\"start\":30057},{\"end\":30604,\"start\":30473},{\"end\":30914,\"start\":30841},{\"end\":31268,\"start\":31207},{\"end\":31656,\"start\":31557},{\"end\":20565,\"start\":20557},{\"end\":21083,\"start\":20984},{\"end\":22906,\"start\":22889},{\"end\":23285,\"start\":23239},{\"end\":23686,\"start\":23633},{\"end\":25691,\"start\":25645},{\"end\":31316,\"start\":31270}]"}}}, "year": 2023, "month": 12, "day": 17}