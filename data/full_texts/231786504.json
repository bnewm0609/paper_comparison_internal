{"id": 231786504, "updated": "2023-11-08 02:58:21.546", "metadata": {"title": "Llama: A Heterogeneous&Serverless Framework for Auto-Tuning Video Analytics Pipelines", "authors": "[{\"first\":\"Francisco\",\"last\":\"Romero\",\"middle\":[]},{\"first\":\"Mark\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Neeraja\",\"last\":\"Yadwadkar\",\"middle\":[\"J.\"]},{\"first\":\"Christos\",\"last\":\"Kozyrakis\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Proceedings of the ACM Symposium on Cloud Computing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The proliferation of camera-enabled devices and large video repositories has led to a diverse set of video analytics applications. These applications rely on video pipelines, represented as DAGs of operations, to transform videos, process extracted metadata, and answer questions like,\"Is this intersection congested?\"The latency and resource efficiency of pipelines can be optimized using configurable knobs for each operation (e.g., sampling rate, batch size, or type of hardware used). However, determining efficient configurations is challenging because (a) the configuration search space is exponentially large, and (b) the optimal configuration depends on users' desired latency and cost targets, (c) input video contents may exercise different paths in the DAG and produce a variable amount intermediate results. Existing video analytics and processing systems leave it to the users to manually configure operations and select hardware resources. We present Llama: a heterogeneous and serverless framework for auto-tuning video pipelines. Given an end-to-end latency target, Llama optimizes for cost efficiency by (a) calculating a latency target for each operation invocation, and (b) dynamically running a cost-based optimizer to assign configurations across heterogeneous hardware that best meet the calculated per-invocation latency target. This makes the problem of auto-tuning large video pipelines tractable and allows us to handle input-dependent behavior, conditional branches in the DAG, and execution variability. We describe the algorithms in Llama and evaluate it on a cloud platform using serverless CPU and GPU resources. We show that compared to state-of-the-art cluster and serverless video analytics and processing systems, Llama achieves 7.8x lower latency and 16x cost reduction on average.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.01887", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cloud/RomeroZYK21", "doi": "10.1145/3472883.3486972"}}, "content": {"source": {"pdf_hash": "b8e8980cb0df63bc8420c1148e1b9d9e997fcf83", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.01887v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "226e7ea02789b38464a55ffd1aa7a6d1f2e90519", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b8e8980cb0df63bc8420c1148e1b9d9e997fcf83.txt", "contents": "\nLlama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines\n\n\nFrancisco Romero \nStanford University\nStanford University\nStanford University\nStanford University\n\n\nMark Zhao \nStanford University\nStanford University\nStanford University\nStanford University\n\n\nNeeraja J Yadwadkar \nStanford University\nStanford University\nStanford University\nStanford University\n\n\nChristos Kozyrakis \nStanford University\nStanford University\nStanford University\nStanford University\n\n\nLlama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines\n\nThe proliferation of camera-enabled devices and large video repositories has led to a diverse set of video analytics applications. These applications rely on video pipelines, represented as DAGs of operations, to transform videos, process extracted metadata, and answer questions like, \"Is this intersection congested?\" The latency and resource efficiency of pipelines can be optimized using configurable knobs for each operation (e.g., sampling rate, batch size, or type of hardware used). However, determining efficient configurations is challenging because (a) the configuration search space is exponentially large, and (b) the optimal configuration depends on users' desired latency and cost targets, (c) input video contents may exercise different paths in the DAG and produce a variable amount intermediate results. Existing video analytics and processing systems leave it to the users to manually configure operations and select hardware resources.We present Llama: a heterogeneous and serverless framework for auto-tuning video pipelines. Given an end-to-end latency target, Llama optimizes for cost efficiency by (a) calculating a latency target for each operation invocation, and (b) dynamically running a cost-based optimizer to assign configurations across heterogeneous hardware that best meet the calculated per-invocation latency target. This makes the problem of auto-tuning large video pipelines tractable and allows us to handle input-dependent behavior, conditional branches in the DAG, and execution variability. We describe the algorithms in Llama and evaluate it on a cloud platform using serverless CPU and GPU resources. We show that compared to state-of-the-art cluster and serverless video analytics and processing systems, Llama achieves 7.8\u00d7 lower latency and 16\u00d7 cost reduction on average.\n\nIntroduction\n\nVideo traffic is exploding in scale, predicted to account for over 82% of all internet traffic by 2022 [5]. A myriad of domains use video pipelines, with tens of video analytics and processing operations, to extract meaningful information from raw videos. For example, an AMBER Alert application can leverage traffic cameras across a city to pinpoint specific individuals and cars [6]. To do so, the application * Equal contribution uses a pipeline to first detect frames with people and/or cars, and then match them to specific individuals' faces and car descriptions, respectively. As video analytics research continues to flourish, we expect a perpetual proliferation of emerging domains that depend on video pipelines, such as smart cities [19,39,42,71], surveillance analytics [22], healthcare [41], and retail [18].\n\nThe pervasive use of video analytics applications has led to significant challenges. Video pipelines must meet a wide range of latency, throughput, and cost targets to be practical across applications. For example, a pipeline to detect cars and people in a traffic feed should be tuned to be more cost efficient for city traffic planners with relaxed latency targets, while the same pipeline must be tuned to meet strict latency targets for AMBER Alert responders [6]. Video analytics and processing frameworks must tune pipeline operation knobs (e.g., sampling rate, batch size, hardware target, and resource allocation) to meet the unique latency or cost requirements of diverse applications. However, automatically tuning these knobs is difficult for the following reasons. Operations exhibit performance variation across heterogeneous hardware. Hardware accelerators (e.g., GPUs [25], FPGAs [34,64], TPUs [44], and vision chips [1]) provide significant performance benefits for many pipeline operations. Tuning knobs across these heterogeneous accelerators can have a huge impact in the performance and efficiency of video pipelines. We observed a 3.7\u00d7 latency variation by tuning CPU cores, GPU memory, and batch size for operations in a representative AMBER Alert pipeline processed using Scanner [55]. While recent research has proposed mechanisms to tune operation knobs based on resource usage [39,42,67], they are limited to simple pipelines and homogeneous hardware platforms. Furthermore, they rely on hours to days of profiling for each new pipeline, video, and latency target [17,71]. Pipelines can have input-dependent execution flow. An input video's contents influence the execution flow of a pipeline in two ways. First, the number of intermediate outputs for an operation may depend on the frame being processed. For the AMBER Alert pipeline, the object detector operation will output a variable number of cropped people and/or car images to be processed by subsequent operations.\n\nSecond, downstream operations may be conditionally executed based on the intermediate output. For example, if there are only people in a frame, no car classification is needed. Consequently, tuning configuration knobs and resource allocations dynamically based on video content is critical for performance and efficiency. We found the static configurations made by gg [32], a general purpose serverless framework, degraded performance by as much as 57% for the AMBER Alert pipeline. Some systems, such as VideoStorm [71] and GrandSLAm [45], only support simple sequential pipelines with deterministic flow.\n\nSystems that use serverless platforms as their backend (e.g., ExCamera [33], gg [32], PyWren [43], and Sprocket [21]) execute applications by using thousands of short-lived functions [2,4,8]. The function-level resource allocation offered by serverless platforms makes them an attractive option for processing video pipelines, as they enable dynamic tuning for each operation invoked. However, existing serverless offerings lack support for heterogeneous hardware accelerators and application constraints such as latency targets. Users must still manually, and perhaps exhaustively, explore operation knobs and resource allocation options.\n\nWe present Llama, a video analytics and processing framework that supports heterogenous hardware and automatically tunes each operation invocation to meet diverse latency targets for the overall pipeline. Llama is a full-fledged serverless framework that provides a serverless experience to its users, who do not need to express the resources or operation knob configurations needed to meet their latency targets.\n\nLlama is divided into two parts: an offline specification phase, and an online optimizaiton phase. The offline specification phase allows users to specify their pipeline, and performs a one-time, per-operation profiling that allows Llama to automatically tune operation invocations as the pipeline runs. Unlike existing systems [26], this profiling does not need to be repeated as the pipeline or input video changes.\n\nDuring the online phase, Llama leverages two key ideas to meet diverse latency targets. First, Llama dynamically computes how much time can be spent on each invocation to meet the pipeline latency target (i.e., per-invocation latency targets). By computing a per-invocation latency target, Llama can dynamically explore the configuration space for each invocation and adapt to performance volatility and input-dependent execution flows. Second, Llama dynamically runs a cost-based optimizer that determines the most efficient operation configuration that meets the per-invocation target. To do so, Llama (a) uses early speculation and late commit: a technique for choosing an initial operation knob configuration during pipeline processing, and revisiting the configuration right before execution, (b) leverages prioritybased commit to prioritize operations based on hardware affinity and DAG dependencies, and (c) employs safe delayed batching to batch operations for efficiency as long as doing so does not violate per-invocation targets.  We deploy Llama on Google Cloud Platform with serverless CPU and GPU backends and evaluate its efficiency and ability to meet latency targets for five video analytics and processing pipelines. By dynamically configuring operations for both CPU and GPU based on pipeline latency targets, Llama achieves on average 7.8\u00d7 latency improvement and 16\u00d7 cost reduction compared to four state-of-the-art cluster and serverless video analytics and processing systems: Nexus [57], Scanner [55], gg [32], and GrandSLAm [45].\n\n\nBackground and Motivation\n\nApplications define video pipelines as directed acyclic graphs (DAGs), where vertices represent video analytics and processing operations, while edges represent dataflow.\n\nAs described in literature [3,29,45], video pipelines can be composed from three basic DAG patterns shown in Figure 1: (a) sequential, where each vertex has at most one input and one output, (b) parallel, where multiple vertices execute in parallel, and (c) branching, where the output of a vertex, called branching vertex, conditionally determines the next vertex to execute. For example, the AMBER Alert pipeline [57,71] for face and car recognition in Figure 2 begins with a sequential path of decoding and preprocessing operations, followed by a branching object detection operation. Depending on the output, people or cars are sent to parallel face and car recognition operations, respectively. Table 1 categorizes video analytics and processing systems based on two key features: (a) Their ability to specify and meet performance targets. User-facing systems typically require that the video pipeline meet a latency target, ideally while minimizing resource usage (cost). For example, the AMBER Alert pipeline needs to meet a strict latency target so that responders can take timely action. (b) Support for general video operations. To compose video pipelines, a user combines video operations (e.g., inference models, video encoders, and image filters) with analytics operations that process extracted metadata. For example, the AMBER Alert pipeline will contain video decoding, object detection, face recognition, and car model recognition. Some systems, such InferLine [26] GrandSLAm [45] VideoStorm [71] Focus [39] Nexus [57] Scanner [55] gg [32] Sprocket [   face detection pipeline that identifies unique faces in a frame [47]. Latency varies up to 17.2\u00d7 and 4\u00d7 on CPU and GPU, respectively.\n\nas Scanner [55], VideoStorm [71], and gg [32], support general video operations. Others, such as Focus [39], Nexus [57], GrandSLAm [45], and InferLine [26] focus on one facet of video pipelines (e.g., deep learning inference) and rely on external services for other operations.\n\n\nChallenges\n\nLarge configuration space. Pipeline operations offer a variety of knobs that can be tuned to improve latency and resource use. For example, many operations have knobs such as batch size, sampling rate, and resolution. Other knobs select the hardware platform (e.g., CPU, GPU, TPU, etc.) and set the resource allocation (e.g., CPU cores or GPU memory). Determining configurations is challenging due to the exponential growth in the configuration space with the number of operations, knobs, and hardware platforms available.\n\nAs shown in Table 1, Scanner, gg, and Sprocket do not autotune configurations knobs, putting the burden on the user to statically specify good operation configurations. Focus, Nexus, GrandSLAm, and InferLine are domain-specific to deep learning inference and are limited to configuring the inference models used and the batch size. VideoStorm supports general knob configurations; however, it takes tens of CPU hours to profile pipelines and requires re-profiling when the pipeline, input video, or latency targets change [71]. Input-dependent execution flow. Input-dependent execution flow occurs in two cases: First, inputs determine the conditional path in branching pipelines. In the AMBER Alert pipeline of Figure 2, a frame will only take the face recognition path if object-detection finds a person in it. Since a conditional path is not resolved until the branching operation finishes, provisioning resources and selecting configurations to meet a pipeline's latency target is challenging. Existing systems either treat branching pipelines as parallel ones (i.e., by executing all conditional branches) [26,30,55] or do not support non-sequential pipelines [21,45,71].\n\nSecond, operations can produce a variable number of outputs, and thus a variable load for downstream operations. If the number of intermediate outputs is unknown until the operation is executed, determining the parallelism or resources needed downstream to meet latency targets is difficult, especially if these operations are computationally expensive. Figure 3 shows the latency for a pipeline that identifies the unique faces in a frame depends on the number of unique faces: 17.2\u00d7 and 4\u00d7 difference between 60 faces versus no faces on a CPU and a GPU, respectively. Thus, the nondeterminism introduced by input-dependent behavior requires systems to dynamically adapt to meet a pipeline's latency target [30]. Most existing video analytics and processing systems do not account for input-dependent execution flow. Dynamically adjusting resource allocation of operation invocations. As a pipeline executes, the degree of available parallelism depends on the various knob settings (e.g., batching) and the number of intermediate outputs.\n\nMany existing systems require users to statically provision a cluster, which limits the resources available to exploit parallelism [30,45] or leads to over-allocation and higher costs when parallelism is low. Some systems periodically adjust resources and bin pack requests as the load changes, but are limited by how quickly hardware (e.g., GPUs) and VMs can be loaded/unloaded [57]. Systems like gg [32] and Sprocket [21] leverage serverless platforms [2,4,8] to dynamically allocate resources for each operation invocation. However, serverless platforms still require users to manually select hardware types and configure knobs to meet latency targets.\n\n\nLlama Design\n\nLlama is a heterogeneous and serverless framework for autotuning video analytics and processing pipelines. Llama's objective is to meet the overall pipeline latency target, while minimizing cost (resource usage). As noted in Section 2, input-dependent execution flow and resource volatility preclude the use of static tuning approaches [30]. They also preclude designing and calculating a globally-optimal solution a-priori or dynamically. Instead, Llama optimizes the overall pipeline execution by iteratively and dynamically optimizing each operation invocation using the most up-to-date information about the state of execution flow and resource availability. Specifically, Llama (a) dynamically reduces the pipeline target latency to per-operation invocation latency targets, values that we call slack, and (b) continuously configures each operation invocation to meet the slack at minimal cost. Dynamically allotting slack ensures the pipeline latency target is met without having to statically account for all possible conditional paths or sources of resource volatility in serverless environments. It also allows Llama to revisit configuration decisions as the resource environment evolves or as input-dependent operations are run. Llama finds the set of cost-efficient configurations for the entire pipeline because it minimizes cost at each configuration assignment subject to the overall latency target.\n\nWe address the challenges outlined in Section 2 as follows: Traversing the large configuration space. Llama profiles and makes configuration decisions on a per-operation, not per-pipeline basis. New operations undergo a short (seconds to minutes), one-time profiling step independent of the pipelines that include the operation. Operations are not re-profiled as the pipeline composition, video, or latency targets change. As the pipeline executes, Llama makes configuration decisions for one operation invocation at a time, reducing the exponential configuration space of an entire pipeline to that of an individual operation. Handling input-dependent execution flow. Llama uses three techniques to meet latency targets despite the nondeterminism that stems from input-dependent behavior and resource volatility (e.g., resource contention): (a) early speculation and late commit selects an initial configuration decision as soon as an invocation is available, then revisits the configuration right before execution, (b) priority-based commit prioritizes operations based on their affinity to hardware and their depth in the pipeline, and (c) safe delayed batching waits for additional inputs for batching, as long as doing so does not violate the invocation's allotted slack. Dynamically adjusting resource allocations. Making per-invocation configuration decisions also allows Llama to dynamically right-size resource allocations across heterogeneous serverless backends. Llama decides the hardware type and resource sizing (e.g., GPU with 2GB of memory) during dynamic configuration based on what is necessary to meet the slack. Early speculation and late commit, as well as priority-based commit, also allow Llama to balance resources between operations.\n\n\nArchitecture\n\nLlama uses an offline specification phase and an online optimization phase ( Figure 4). The specification phase has two purposes. First, it allows the user to specify a pipeline with multiple, general operations using a SDK. Second, it extracts the following information: a set of all possible sequential paths through the pipeline, and the latency and resource footprint of each unique operation across possible knob configurations. The pipeline specification and the extracted metadata are stored for use during the online phase.\n\nThe online phase is triggered when users submit an input video and a latency target to Llama. Llama executes the pipeline by continuously generating and executing a set of invocations for each operation as their input dependencies are resolved. For example, if object-detection outputs a frame tagged with a person, a new invocation is generated for the preprocess operation in the AMBER Alert pipeline (Figure 2). The online phase configures each invocation by first estimating its slack. It then uses the respective operation's profiling data to determine the most efficient configuration for completing the invocation within the allotted slack. The process repeats until all pipeline invocations are executed.\n\n\nSpecification phase\n\nApplication Interface. Users specify pipeline operations, dependencies between operations, and conditional flow using the Llama SDK. Llama provides a library of operations (e.g., decode and face recognition). Each operation consists of a binary executable, indexed by its SHA256 hash, and a configuration specification file that contains configuration options and performance statistics for the operation. Users can optionally bring their own operations by providing an executable and a configuration template that specifies tunable knobs (e.g., hardware type, batch size, or number of filters), the ranges for each knob, and the granularity of this range (e.g., batch size increasing by powers of 2). The Operation-Profiler uses these inputs in a one-time profiling step to generate a configuration specification. The operation and configuration specification are then added into the Metadata Store and re-used across pipelines without further profiling. Operation-Profiler. The Operation-Profiler collects performance and resource statistics for each operation. Using the operation executable and configuration template as inputs, it first enumerates all possible configurations specified by the template, then executes a short profiling step using one or more sample frames for each configuration (depending on the batch size). Statistics such as latency and resource footprint (e.g., peak memory utilization) are collected and stored as configuration specification file entries. The frame content does not affect these statistics (recall that input-dependent execution flow manifests between operations). Since slack calculation (Section 4.1) is only dependent on the relative performance of operation invocations across the pipeline, the Operation-Profiler designates a reference configuration for each operation to provide a measure of relative performance. We chose the smallest CPU configuration (1-core, batch-1) for each operation's reference configuration. During runtime, operation invocation performance that differs from its profiled value, due to resource contention or profiling inaccuracy, is managed by leveraging feedback (Section 3.3).\n\nThe configuration specification is structured so arbitrary operation-and hardware-specific configuration knobs can be described by users, and dynamically configured during runtime. This enables Llama to support general operations and arbitrary video pipelines for a myriad of applications. Pipeline-Decomposer. To enable the online phase to dynamically compute slack, the Pipeline-Decomposer performs a one-time decomposition of the pipeline into all possible sequential paths in the pipeline. To do so, it performs a modified depth-first search on the pipeline DAG to enumerate all paths from the input operation (i.e., operation with no upstream dependencies) to an output operation (i.e., an operation with no downstream dependencies). It then emits an intermediate representation of the decomposed paths into the Metadata Store. For example, the AMBER Alert pipeline in Figure 2 is decomposed into the two sequential paths ending in facerecognition and car-recognition, respectively.\n\n\nOnline phase\n\nManager. Llama's Manager takes video inputs and latency targets and orchestrates the entire pipeline execution, maintaining execution state and generating new invocations. Whenever an invocation completes, the Manager records the invocation's runtime statistics (i.e., latency, cost, and configuration) and the location of intermediate outputs. The runtime statistics are used to update the configuration profiles obtained from the Operation-Profiler via a feedback loop. We use an exponential smoothing algorithm to update the profiling; other algorithms can be incorporated as well. The intermediate outputs are then used to resolve any conditional branches. The Manager then spawns invocations for downstream operations once all dependencies have been resolved. These invocations are then sent to the Configurator. Configurator. To meet the overall pipeline latency target, the Configurator ( Figure 5) decides (a) how much slack to allot to an operation invocation, and (b) what the most efficient configuration is to meet the slack. The Configurator works with the Scheduler to keep track of available resources at a serverless backend as it makes configuration decisions. Scheduler. After the Configurator has configured an invocation's knobs, the invocation is sent to the Scheduler for execution. The Scheduler executes the configured invocations on the hardware platform specified by the Configurator. This includes creating and managing the necessary backend connections, mitigating stragglers, and handling SQ [GPU] Commit CQ [GPU] CQ [CPU] Metadata Store Figure 5. Configurator diagram.\n\nAlgorithm 1 Operation invocation slack allotment 1: \u210e \u2190 A set of all sequential paths in the pipeline 2: \u2190 elapsed time 3:\n\n\u2190 pipeline latency target 4: procedure ComputeSlack( , ) 5:\n= { } 6: for all { \u210e \u2208 \u210e | \u2208 \u210e} do 7: = RemainingPathLatency( , \u210e) 8: = ( \u2212 \u2212 ( )) 9: = ( . ()/ ) \u00d7 10: . ( ) 11: return ( \u2208 )\ninvocation failures (Section 4.5). When an invocation successfully returns, the Scheduler provides the Manager with the invocation metadata and output results.\n\n\nTarget Latency-Aware Configuration\n\nInput-dependent execution flow and backend resource volatility require the Configurator to dynamically determine each operation invocation's most efficient knob configurations. The Configurator is divided into two parts. The Logic Plane (a) determines how much slack can be spent on its invocation, and (b) uses a cost-based optimizer to select a configuration to meet that slack. The Data Plane manages configured operations in queues prior to their execution.\n\n\nDetermining an invocation's slack\n\nGiven a user-specified pipeline latency target, the Configurator first needs to compute a slack for each operation invocation. Existing systems (e.g., GrandSLAm [30] and Fifer [37]) statically determine each operation invocation's slack by assuming a linear pipeline with predictable invocations and latencies (i.e., no nondeterminism). Our insight is to instead dynamically calculate each operation invocation's slack, which we subsequently use to select the best invocation configuration (Section 4.2). Doing so across invocations efficiently meets the pipeline latency target. Llama calculates an operation invocation's slack using Algorithm 1. Given an invocation of operation and a configuration's backend , ComputeSlack begins by finding every sequential path through the DAG containing (Line 6). It then estimates the latency to complete the path, starting at , using the reference configuration for each operation (Section 3.2). By using the reference configuration's latency, Llama avoids a causal dilemma of needing a configuration to compute slack, and needing a slack to select a configuration. The operation invocation's slack for that path is then determined based on the remaining time (Line 8), factoring in estimated queueing time at , weighted by the relative latency of to the remaining path (Line 9). The final slack for an invocation of on is then the minimum slack value over all possible execution paths of , which accounts for all input-dependent branch resolutions (Line 11). We discuss how Llama reclaims overly-conservative slack next.\n\n\nNavigating the configuration space\n\nSince slack is calculated for each operation invocation, Llama can quickly evaluate configurations in a smaller peroperation, not per-pipeline, configuration space. After calculating the invocation's slack for each available serverless backend (Algorithm 1), Llama applies the objective function shown in Equation 1 for all possible configurations of using the invocation slack corresponding to the serverless hardware backend ( ) targeted by . ( ) and ( ) are the estimated cost and latency respectively to run configuration . ( ) is the resources requested by (e.g., amount of memory), and ( ( )) is the resource limit of ( ). ( ) is the batch size of configuration , and is a tunable weight.\n( , ) = ( )/ ( ) ( ) < ( ) ( ) + ( ( ) * ( )) ( ( ) * ( ( )) \u210e(1)\nIntuitively, this objective function evaluates the monetary cost to run when there is a feasible slack. If slack cannot be met (e.g., if the user submits an unachievably low target), the cost function weighs in favor of potentially more expensive configurations that achieve a higher throughput. sets the balance between cost and throughput, with high values of set to meet the slack at all costs, while lower values of may leverage more cost-efficient configurations potentially at the expense of exceeding slack. The configuration objective function is independent of the input video or overall pipeline.\n\nUsers who wish to optimize for a different metric (e.g., minimal latency subject to a cost budget) can add their own objective function to Llama. Furthermore, since is specific to each backend (e.g., concurrent invocation limits, GPU memory, or CPU cores), Llama can support other heterogeneous backends (e.g., serverless GPUs or on-premise clusters).\n\nSince conditional flow will not always resolve to the worstcase path, the allotted slack may result in a configuration with a lower-than-necessary latency. However, since each invocation is configured separately and dynamically, future invocations will recover efficiency from earlier mis-predictions.\n\n\nRevisiting configuration decisions\n\nTo manage invocations that cannot be run concurrently due to limited backend parallelism, Llama locally queues invocations and accounts for the queueing time when allotting slack (Line 9 in Algorithm 1). The queueing time depends on : the selected configuration of each queued invocation (i.e., it is not sufficient to use the number of queued operation invocations as a measure of wait time [30,60]). Thus, invocations need to be assigned a configuration before they are queued. However, the initial configuration is often made many seconds before it is actually invoked, leading to sub-optimal configurations for three reasons. (a) Invocations queued in front of may experience execution times that vary from the profiled values. This can occur due to resource contention or input-dependent execution flow. (b) The estimated latency for may be updated via feedback while it is queued. (c) The number of invocations queued behind may quickly grow (e.g., many completed objectdetection invocations may output a large number of carrecognition and face-recognition invocations); thus, should be chosen to ensure upstream invocations can meet their slack. Hence, by the time a queued invocation is ready to run, its selected configuration needs to be revisited to determine if it is still the right configuration.\n\nTo solve this, Llama leverages a novel technique inspired by late binding [23,49,50,54,61] that we call early speculation and late commit. With early speculation and late commit, Llama maintains two queues per serverless backend : an unbounded speculative queue ( [ ]) and a small, bounded commit queue ( [ ]) set to hold enough invocations to saturate . Once an invocation is ready to execute, the Configurator uses Algorithm 1 to assign it a slack, and uses Equation 1 to select a speculative configuration. The configured invocation is then put into the appropriate speculative queue, thus enabling Llama to estimate the queueing time at each backend. Once reaches the head of the speculative queue, as prior invocations are executed, Llama revisits the configuration of by using Algorithm 1 and Equation 1 again. It then commits the configuration into the appropriate commit queue. Doing so mitigates the queueing challenges we noted above by delaying binding an invocation to a final configuration for as long as possible. This provides Llama with maximum flexibility and the most up-to-date state about pipeline dataflow and performance at each backend.\n\nWith early speculation and late commit, Llama can estimate the queueing time using Equation 2 for each serverless backend based on each configured invocation in its queues.\n\n( ) and ( ) are the estimated latency of, and resources requested by , respectively.\n\n( ( )) is the total amount of resources or concurrency limit at the serverless backend specified by the configuration . \n\nIntuitively, the queueing time is the sum of each 's profiled configuration latency, weighted by 's requested resources (to account for parallel execution). The cumulative queueing time over [ ] and [ ] is then used in Com-puteSlack.\n\n[ ] is included when committing configured invocations to account for invocations queued behind . We do not incorporate future operations' queueing time, since dynamicity and the need to assign a configuration to each downstream operation can result in inaccurate estimates. The Configurator's decisions described in Section 4.2 assume per-operation invocation decisions can be made independently of each other. However, Llama also needs to reason about the relationship between operations and their invocations for the following reasons: When to batch invocations. As pipeline dataflow progresses, there can be moments when an operation may have fewer invocations available than the most efficient configuration's batch size. For example, if a pipeline contains a slower face detection operation followed by a faster blurring operation, the blur operation's invocations will likely drain the speculative queue faster than it can build up. In such cases, executing upstream operations first yields a larger batch size, amortizing RPC and I/O overheads. However, waiting for upstream operation invocations to complete their execution may result in a slack violation. Under-allotting slack due to incorrect profiling. As described in Section 4.1, slack allotted to an invocation is a function of the reference configuration's profiled latency for downstream operations. Furthermore, a configuration's latency is updated using a feedback loop after execution (Section 3.3). However, slack can be under-allotted to operation invocations if the reference configuration latency is significantly shorter than its actual latency, and the feedback loop is not closed early on during pipeline execution. This is especially problematic for longer pipelines, and for pipelines with the last operation's invocations needing a longer slack than the rest. Thus, it is beneficial to prioritize invocations by pipeline depth early in the pipeline's execution so that feedback can update all reference configurations. Affinity of operations to heterogeneous hardware. While prioritizing invocations by pipeline depth can help prevent under-allotting slack, the issue of prioritizing operation invocations on particular hardware platforms still remains. For example, consider the case in which both an objectdetection and face-recognition invocation must be configured. Assume that while both operations run faster on a GPU, face-recognition benefits more from acceleration and observes a larger latency reduction. Resource limits force the two invocations to split their decision between and . Committing object-detection's invocation first forces face-recognition's invocation to choose . However, the better decision is to assign the CPU to object-detection and the GPU to face-recognition. The relative benefit of running operation invocations on a particular hardware platform (i.e., its hardware affinity) must be incorporated into configuration decisions.\n\n\nOur solution.\n\nLlama addresses these challenges using safe delayed batching and priority-based commit, implemented in conjunction with early speculation and late commit. Safe delayed batching. Safe delayed batching addresses the challenge of waiting for additional invocations to batch. During both early speculation and late commit, if Llama determines the most cost-efficient configuration that meets slack has a batch size larger than the number of invocations available for a given operation (using Equation 1), it waits until more invocations arrive to assign a configuration. It does so safely -only if there are enough upstream invocations and slack will not be violated. Otherwise, it uses the best feasible configuration. Priority-based commit. Priority-based commit addresses the challenges of under-allotting slack and operations' affinity to heterogeneous hardware. First, to address the challenge of under-allotting slack, the Configurator prioritizes invoking a certain number of reference invocations for each operation, favoring deeper operations in the pipeline. This ensures the feedback loop for all reference configurations is closed as fast as possible to minimize under-allotted slack.\n\nSecond, to compute an operation's affinity to heterogeneous hardware, Llama compares the benefits an operation invocation receives from running on a specific backend to other available backends. It computes the affinity of an invocation's operation to hardware backend using Equation 3, where , is the subset of configurations for that run on and c , is the complementary set (i.e., all other configurations for ).\n( , ) = min \u2200 \u2208 c , { ( , )} min \u2200 \u2208 , { ( , )}(3)\nIntuitively, Equation 3 determines if a hardware backend provides more benefit (via Equation 1) to an invocation than other backends. Llama prioritizes invocations from operations with a higher affinity to a hardware backend when committing them to each [ ]. This ensures each backend achieves its highest utility.\n\n\nHandling stragglers and invocation failures\n\nDuring execution, operation invocations may straggle or fail to execute [20,28,68]. The Scheduler keeps track of each invocation's execution time. If it exceeds a configurable timeout (discussed in Section 5) or the Scheduler receives an error, the Scheduler notifies the Manager to create a duplicate invocation. This duplicated invocation is then passed to the Configurator to begin the slack allotment and configuration process anew. The allotted slack will now be reduced, potentially resulting in a different configuration to still meet the pipeline latency target (evaluated in Section 6.4).\n\n\nImplementation\n\nWe implemented Llama as an extension to gg in \u223c4K lines of C++ code. We modified gg's C++ and Python SDK to support complex pipelines and general knob configurations. Llama supports operations from any framework or library; we implemented non-deep learning pipeline operations (e.g., blur and meanshift) using OpenCV [24] and FFmpeg [31], and deep learning pipeline operations with TensorFlow [15]. Llama's source code will be available upon publication.\n\nWe implemented the online phase on top of gg's dispatcher and backend resource manager. The online phase is singlethreaded, but can scale out to multiple threads as needed. For straggler mitigation, we set each invocation's time-out value to 1.5\u00d7 the invocation's profiled latency. Larger values wait too long to spawn a duplicate invocation, which may violate the pipeline latency target, while smaller values unnecessarily overload the speculation queues. For depth-first priority, we observed that 10 invocations of the reference configuration were sufficient to obtain enough feedback values to converge on a latency measurement. Smaller values do not collect enough feedback values to prevent underallotted slack, while larger values unnecessarily prioritize invocations with configurations that may not be efficient.\n\nFor the offline specification phase, we implemented the Operation-Profiler as a client to the online phase that collects and stores the profiled metadata into configuration specifications. Configuration specifications are implemented as JSON files. The Metadata Store is implemented in an object store (e.g., Google Cloud Storage).\n\nWe deployed Llama with serverless CPUs and serverless GPUs as compute backends. For serverless CPUs, we provision and manage a cluster of CPUs similar to existing serverless offerings [65]. Each invocation requests a specific number of cores (up to 4). Llama also supports running on serverless computing services such as AWS Lambda [2] or Google Cloud Functions [8], where the invocation resources requested would be an amount of DRAM.\n\nSince there exists no serverless GPU services or frameworks at the time of writing, we built our own implementation (\u223c1K lines of C++ code) that we believe is representative of a future production offering [25]. Similar to CPU serverless offerings, an invocation requests an amount of GPU memory (in MB) per invocation. Our serverless GPU scheduler then allocates a proportional amount of GPU threads using Nvidia MPS [9], allowing for multiple invocations to execute concurrently. Invocations are executed on a first-come, first-served basis. Llama is also compatible with GPUs that support concurrent job execution in hardware [10].\n\n\nEvaluation\n\nWe answer the following questions: (a) How does Llama compare to state-of-the-art systems (Scanner, Nexus, gg, and GrandSLAm)? (b) How effective is Llama in meeting diverse latency targets? (c) How does each technique employed by Llama, such as early speculation and late commit and safe delayed batching, contribute to its ability to meet the latency target? (d) What is the impact of profiling errors and failures on Llama's ability to meet latency targets? (e) What are the overheads of various decisions Llama makes? Metrics. Unless otherwise noted, we use pipeline processing latency and cost as metrics for success (similar to [21,26,45]). For each experiment, we report the mean of three runs. Experimental setup. We deployed Llama on Google Cloud Platform (GCP) [7]. The Llama runtime ran on a n1-standard-8 instance (8 vCPUs, 30 GB of DRAM). We used the following setup unless otherwise noted. For the serverless CPU backends, we used 10 n1-standard-64 (64 vCPUs, 240GB of DRAM). For the serverless GPU backends, we used 2 custom-12-46080 (1 V100 GPU, 12 vCPUs, 45 GB of DRAM). All instances feature Intel Xeon Platinum E5-2620 CPUs operating at 2.20GHz, Ubuntu 16.04 with 5.3.0 kernel, and up to 32 Gbps networking speed. Baseline systems. We compared Llama with three sets of systems: (a) cluster systems (Scanner and Nexus), (b) serverless systems (gg), and (c) target-aware systems (GrandSLAm). Scanner is used by Facebook for processing 360 \u2022 videos [12]. Nexus accelerates deep learning-based video analysis on GPUs. gg is a general purpose serverless framework. Grand-SLAm estimates slack to meet pipeline latency targets for sequential, DNN-only pipelines. We evaluated two common Scanner setups: one in which a user only provisions a cluster with CPUs (sc-cpu), and one in which, similar to Nexus, a user runs all operations on a GPU (sc-gpu). For gg, we also compared against a version augmented with Llama's branching support (gg-branch). sc-cpu, gg, and gg-branch do not support heterogeneous accelerators, while Nexus and sc-gpu require GPU VMs. Since GrandSLAm does not natively support non-sequential pipelines, and does not account for input-dependent execution flow, we implement it with Llama by disabling early speculation and late commit, feedback, and depth-first priority. However, GrandSLAm still has access to Llama's branching support, safe delayed batching, priority-based commit, and dynamic resource allocation across heterogeneous backends (GrandSLAm++). To equalize compute resources provided to all systems, we observe that custom-12-46080 and n1-standard-64 VMs are effectively priced the same on GCP (a difference of 1% at  [14], 10 min, 1080p\n\nFace Blurring detect indiv. face and blur from all frames 5 (branching) decode \u2020 , preprocess \u2020 , face recog., template match \u2020 , blur \u2020 (600) rally [11], 10 min, 720p\n\nDenoising detect indiv. face and denoise/segment 5 (branching) decode \u2020 , preprocess \u2020 , face recog., template match \u2020 , meanshift \u2020 (600) rally [11], 10 min, 720p\n\nToonify apply cartoon effect to video 4 (parallel) decode \u2020 , edge detect. \u2020 , bilateral filter \u2020 , merge edge-filter \u2020 , encode \u2020 (989) tears of steel [13], 10 min, 720p\n\nSynthetic synthetic pipeline for sensitivity analysis 7 (sequential) decode \u2020 , blur \u2020 , preprocess \u2020 , face recog. (596) rally [11], 10 min, 720p the time of writing). (This price-equivalency is also true for equivalent instances on AWS.) Thus, we provisioned sc-cpu, gg, gg-branch, and GrandSLAm++ with 12 n1-standard-64, and Nexus and sc-gpu with 12 custom-12-46080.\n\nResource requests and cost model. For Llama, gg, and GrandSLAm++, each invocation requests a set amount of resources (GPU memory or CPU cores) as is done in commercial serverless offerings. The respective backend then provisions the invocation with the requested resources, and charges a price based on the amount of requested resources and invocation latency. We calculate the price (in $/(resourcesecond)) by dividing the cost per second charged by GCP by the VM's total resources. For example, the price of a V100 GPU invocation is calculated by dividing the price of custom-12-46080 by 16 . Since Scanner and Nexus are cluster-based frameworks, we compute cost using the time to rent the cluster for the duration of the execution; we do not include the cost of starting and maintaining a warm cluster.\n\nVideo pipeline. Table 2 shows the pipelines, operations, and videos that we used. For branching pipelines, only invocations satisfying the branching condition are executed. For AMBER Alert, only frames with faces and cars execute their respective recognition paths. For Face Blurring and Denoising, frames with faces proceed to a template match operation where the frame is compared against a pre-determined face. If a match is found, the face in the frame is then blurred, or denoised using meanshift. The Toonify pipeline executes the bilateral filtering and edge operations in parallel before merging and encoding the frames. Finally, the synthetic pipeline is a chain of 5 image blurring operations followed by a face recognition operation. The face recognition operation is the most compute-intensive operation of this pipeline, which allows us to evaluate Llama's ability to meet diverse pipeline latency targets, even when configurations were mis-profiled (Section 6.4). Since sc-cpu, sc-gpu, and gg do not support branches, they execute the three branching pipelines as parallel ones (i.e., both branches are executed).\n\n\nComparing Llama to existing systems\n\nWe first show how Llama's ability to dynamically reconfigure operation invocations enables it to outperform existing systems, both in terms of latency and cost. Experimental setup. For Nexus, we set the pipeline latency target to be 2 seconds per frame, which we found to be the strictest latency that does not drop any requests [57]. Nexus then automatically configures the batch size and number of instances for each model. For sc-cpu and sc-gpu, we swept each operation's batch size from 1 to 64 (by powers of 2) and set each value based on the lowest pipeline execution latency (reported in Figure 6). For gg and gg-branch, we set each invocation's configuration based on the lowest, most cost-effective CPU latency reported by the Operation-Profiler. We configured Llama and GrandSLAm++ with two pipeline latency targets: an unachievable low target that forced both to minimize pipeline execution latency at the expense of cost: llama-fast and GrandSLAm++-fast, and an overly-loose target that allowed Llama to minimize the overall cost: llama-cheap and GrandSLAm++-cheap. Results and discussion. Figures 6 and 7 show the processing latency and total cost, respectively, to execute each of the four non-synthetic pipelines. Llama achieves lower latency, higher throughput, and lower cost than existing systems. Even when the cost of starting and maintaining a warm cluster are not considered, Llama is faster (up to 65\u00d7 and 28\u00d7 on average) and cheaper (up to 110\u00d7 and 55\u00d7 on average) than sc-cpu. Compared to sc-gpu, Llama is up to 11\u00d7 faster (6\u00d7 on average) and up to 27\u00d7 cheaper (18\u00d7 on average). Scanner cannot dynamically adjust and right-size invocation configurations, and thus cannot address performance degradation caused by resource contention for compute-intensive operations (e.g., deep learning inference and meanshift) or memory-intensive operations (e.g., bilateral filtering).\n\nNext, since Nexus focuses on inference-serving pipelines, we are only able to run the AMBER Alert pipeline (other pipelines denoted by \u00d7 in Figures 6 and 7). While we provide Nexus with 12 GPUs, Nexus's bin-packing algorithm [57] utilizes only 8; thus we report cost for 8 GPUs. By dynamically choosing CPU versus GPU configurations, Llama achieves 1.3\u00d7 speedup and 2.8\u00d7 lower cost compared to Nexus.\n\nCompared to gg, Llama is up to 3.1\u00d7 faster (2.2\u00d7 on average) and up to 8.2\u00d7 cheaper (5.7\u00d7 on average). Compared to gg-branch, Llama is up to 2.9\u00d7 faster (1.8\u00d7 on average) and up to 6.8\u00d7 cheaper (4.7\u00d7 on average). While gg-branch can reason about conditional flow, it cannot make dynamic invocation configuration decisions or adjust to resource volatility, resulting in a higher latency and cost compared to Llama.\n\nFinally, Llama is up to 1.7\u00d7 faster (1.2\u00d7 on average) than GrandSLAm++-fast and 1.4\u00d7 cheaper (1.1\u00d7 on average) than GrandSLAm++-cheap. Since GrandSLAm++ allots slack and selects configurations based on profiled values, it cannot dynamically adjust to nondeterminism, which can result in slower performance or higher cost (e.g., Denoising).\n\nBy making dynamic invocation configurations, Llama is able to determine how well operations perform across heterogeneous backends and right-size resources depending on the pipeline latency target. General applicability. While Llama was designed to address the challenges of running video analytics and processing pipelines (Section 2), its operation configuration specification (Section 3.2) supports arbitrary operation-and hardwarespecific configuration knobs. To demonstrate this, we built a four-stage natural language processing pipeline for applications like therapy session analysis for at-risk youth [35]. The pipeline has six models (language identification, two language translation, sentiment analysis, text generation, and summarization) and features branching, parallel, and sequential patterns. For a 256-line transcript, llama-fast takes 275s ($1.22) while llama-cheap takes 573s ($0.44). Thus, Llama can be used for meeting latency targets for general domains.   \n\n\nCan Llama trade off latency for cost?\n\nWe now show Llama can also meet latency targets that lie between llama-fast and llama-cheap. Experimental setup. For each pipeline, we provide three latency targets to Llama that lie between the times required to execute the pipeline using llama-fast and llama-cheap. The 50% latency target is the mean latency between the latencies achieved by llama-fast and llama-cheap. The 25% latency target (the most stringent of the three) is the mean latency between llama-fast and the 50% latency target. Finally, the 75% latency target (the least stringent of the three) is the mean latency between llama-cheap and the 50% latency target. For example, llama-fast executed Face Blurring in 155 seconds, and llama-cheap executed it in 423 seconds; the 25%, 50%, and 75% latency targets are 225, 290, and 380 seconds respectively. Results and discussion. Figure 8 shows the observed execution latency, normalized to each of the aforementioned pipeline latency targets (\u22641 means that the latency target was met), as well as the raw cost values for each pipeline execution. Llama not only meets all latency targets, but also dynamically adjusts its configuration decisions to choose cost-efficient configurations as the latency target became less stringent. For the Denoising and Synthetic pipelines, the cost stays the same for the 50% and 75% targets. This is due to Llama selecting similar invocation configurations during both runs, since it determined them to be the most cost-efficient configurations for both latency targets.  Figure 9. Impact of turning Llama's techniques off on the AMBER Alert and Toonify pipelines. Red borders and circled slashes indicate the pipeline latency target was violated. FB is feedback, DFP is depth-first priority, SDB is safe-delayed batching, ESLC is early speculation and late commit, and PBC is priority-based commit. Table 3 shows a breakdown of how many configurations were used to meet the 50% pipeline latency target, and what percent of invocations met the slack. We note that (a) Llama meets the slack for 94% of invocations on average across all pipelines, with the lowest being the Synthetic pipeline since it is the longest, and (b) the number of configurations used varies per pipeline. Thus, Llama's slack allottment and configuration selection algorithms (Section 4) are effective in meeting pipeline latency targets while minimizing cost.\n\n\nAblation study of Llama's techniques\n\nWe now show how each technique of Llama contributes to its ability to efficiently meet pipeline latency targets. Experimental setup. We performed an ablation study with two distinct pipelines: Amber Alert and Toonify. Following is the list of techniques employed by Llama: feedback loop (FB, Section 3.3), depth-first priority (DFP, Section 4.4), safe delayed batching (SDB, Section 4.4), early speculation and late commit (ESLC, Section 4.3), and priority-based commit (PBC, Section 4.4). Note that priority-based commit includes both depth-first priority and hardware affinity. For each run, we turn off a single technique and record the pipeline execution latency and cost. For each pipeline, we use its 50% pipeline latency target specified in Section 6.2. Results and discussion. Figure 9 shows the results of our ablation study (red borders and circled slashes indicate the latency target was violated). For the AMBER Alert pipeline, disabling feedback, depth-first priority, or early speculation and late commit results in latency target violations. All three techniques allow Llama to accurately measure and adapt to performance volatility caused by input-dependent execution flow (branching operations) and resource contention. For example, disabling feedback causes Llama to miss the latency target because resource contention resulted in invocations taking longer than their profiled values. With feedback enabled, Llama is able to detect this and choose configurations with higher throughput at a small expense of cost-efficiency. On the other hand, disabling safe delayed batching or priority-based commit causes Llama to not use large batches for deep learning inference invocations on GPUs, resulting in reduced cost-efficiency.\n\nFor the Toonify pipeline, disabling feedback also causes a latency target violation similar to the AMBER Alert pipeline. Disabling either safe delayed batching or early speculation and late commit results in Llama choosing less cost-efficient configurations. On the other hand, disabling depth-first priority and priority-based commit results in more cost-efficient configurations without violating the latency target. This is because these techniques led to Llama choosing configurations that are more throughput-intensive than necessary for merge edge-filter operation invocations in an effort to meet the pipeline latency target. However, as noted for the AMBER Alert pipeline and evaluated in Section 6.4, both depth-first priority and priority-based commit are important for Llama's robustness in right-sizing resources and meeting latency targets despite profiling errors.\n\n\nMeeting targets despite profiling errors &\n\nfailures We now show that Llama can meet targets despite profiling errors and invocation failures. Experimental setup. To evaluate \"mis-profiling\", all operation profiled latencies are set to 50% of their values. Separately, to evaluate Llama's resiliency to failures, we forced 3% of invocations to fail (2, 114 and 3, 617 failures for the Denoising and Synthetic pipeline, respectively). For both experiments, we used the Denoising and Synthetic pipelines because they represent worst-case scenarios: an expensive operation at the end of the pipeline with an under-estimated latency. In addition, the Synthetic pipeline is the longest pipeline, which further exacerbates profiling errors: Llama will under-allot slack to the last operation unless techniques are used to mitigate mis-profiling. We use the respective 50% pipeline latency target from Section 6.2 for each pipeline. Results and discussion. Table 4 shows the impact of profiling error on latency and cost with (a) all of Llama's techniques, and (b) both feedback and depth-first priority turned off (the two techniques Llama relies on to adjust for inaccurate profiling). For the Denoising pipeline, disabling feedback and depth-first priority causes Llama to under-allot slack to the last meanshift operation. This results in a missed pipeline latency target because Llama could not adjust to the profiling errors until late in the pipeline execution. For the Synthetic pipeline, when both techniques were off, Llama meets the latency target but at a 35% higher cost. This is because the 50% lower-than-profiled latencies cause Llama's objective function (Equation 1) to incorrectly calculate that the CPU, not the GPU, is most cost-efficient for the meanshift operation. Since the CPU configuration was actually 50% slower and less cost-effective than a GPU, the cost ends up being higher than if a GPU would have been used. When evaluating invocation failures, both pipelines were able to meet the specified latency target despite the high failure rate using the techniques described in Section 4.5.\n\nThese results demonstrate depth-first priority and feedback are necessary to resolve profiling discrepancies early on during execution, and that Llama is robust to failures.\n\n\nOverheads of decisions Llama makes\n\nFinally, we evaluate Llama's overheads and its ability to scale across backends. Table 5 shows the overhead for these decisions when specifying and running the AMBER Alert pipeline with the 50% intermediate latency target from Section 6.2; all other pipelines have similar overheads. For the specification phase, profiling each operation takes an average of 257 seconds, and only needs to be performed the first time an operation is added to the Metadata Store. The decomposition step, which is performed once per pipeline, takes only 1.7 seconds.\n\nDuring the online phase, Llama only spends 483 microseconds, on average, to process (i.e., speculate, commit, invoke, and finalize) an invocation, allowing Llama to process over 2000 invocations per second. Calculating a slack and determining a configuration is efficient, as speculate only requires 5 micro-seconds. Most time is spent evaluating priority between operations during commit, connecting and sending invocations to backends during invoke, and updating global state once invocations completed during finalize.\n\nLow overheads also allows Llama to improve execution latency as the number of resources or maximum concurrency increases. Compared to llama-fast for the AMBER Alert pipeline run on 10 CPU and 2 GPU instances (Section 6.1), having 6 CPU and 1 GPU instances is 46% slower, while having 15 CPU and 3 GPU instances is 25% faster.\n\n\nRelated Work\n\nVideo and general-purpose analytics frameworks. In Section 2, we describe the limitations of several existing video analytics and processing frameworks [21,26,32,39,55,57,71]. Other cluster-based and serverless systems for both domain-specific and general-purpose applications [30,40,43,52,53,67,70] either do not support independentdependent execution flow, require extensive per-pipeline profiling, or require users to configure and right-size resources. Llama meets diverse pipeline latency targets across complex (and possibly dynamic) video pipelines using heterogeneous serverless backends. Dataflow optimizations and scheduling techniques. GrandSLAm [45] and Fifer [37] use slack to statically determine the batch size for sequential microservice graphs. Delayed batching is used by Clipper [27] to increase efficiency of inference queries, but must be statically set by users. Late binding is used by schedulers [23,49,50,54,61] to maximize the flexibility of the scheduling decision and knowledge of system state. However, these systems do not consider the need to configure operations for meeting pipeline latency targets. TetriSched [60] uses a scheduler that prevents tasks from being sent to a sub-optimal set of resources due to resources being held by earlier jobs, but only supports peroperation targets, not an end-to-end pipeline latency target. Early speculation and late commit, and priority-based commit allow Llama to compute slack and make configuration decisions for arbitrarily complex pipelines to meet overall pipeline latency targets. Musketeer [36] and Dandelion [56] optimize dataflow DAGs for execution on a broad range of execution engines or hardware platforms. These optimizations are compatible with Llama, and can be used to expand the backends and hardware platforms Llama supports. Cost-based query optimization. Several works have explored cost-based query optimization for relational databases [16,38,46,58,59,63], including for queries whose optimal plan is input-dependent [66]. Llama is compatible with these frameworks, and can leverage their optimizations as an extension to how configurations are selected. Auto-tuning configurations.\n\nCherryPick [17] and Ernest [62] present a performance prediction framework for recurring data analytics jobs; however, these systems require tens of executions of a job to set the configuration parameters. PARIS [69] focuses on VM-size selection; Op-timusCloud [51] and Selecta [48] are domain-specific VM configuration systems for databases and storage technologies, respectively. Llama dynamically configures general video operations to meet diverse latency targets, and only requires one-time per-operation profiling.\n\n\nConclusion\n\nWe presented Llama, a heterogeneous and serverless video analytics and processing framework that executes general video pipelines, meeting user-specified performance targets at minimal cost. By dynamically configuring individual operation invocations, Llama efficiently traverses large configuration spaces, adapts to input-dependent execution flow, and dynamically allocates resources across heterogeneous serverless backends. Llama makes per-operation invocation decisions by first calculating invocation slack, then leveraging techniques such as safe delayed batching, priority-based commit, and early speculation and late commit to efficiently and accurately select configurations that meet the slack. Llama achieves an average improvement of 7.8\u00d7 for latency and 16\u00d7 for cost compared to state-of-the-art systems.\n\nFigure 1 .\n1Simple DAGs that can be used to compose complex video pipelines.\n\nFigure 2 .\n2An AMBER Alert pipeline that finds faces and cars in a video.\n\nFigure 3 .\n3Execution latency on CPU (left) and GPU (right) for a\n\nFigure 4 .\n4Llama's architecture diagram.\n\nFigure 6 .Figure 7 .\n67Latency of baselines to execute each pipeline. Nexus only supports the AMBER Alert pipeline (unsupported pipelines are denoted by \u00d7). Llama's fastest execution is faster than all baselines. Cost incurred by baselines for each pipeline. Nexus only supports the AMBER Alert pipeline (unsupported pipelines denoted by \u00d7). Llama's cheapest execution is cheaper than all baselines.\n\nTable 1 .\n1Comparison of existing video processing systems with Llama based on whether they (a) support performance targets and general operations, and (b) address the challenges of meeting performance targets for general video pipelines. \u00b6Limited to domain-specific knobs.\u2020Large profiling overhead. \u2021Cannot handle branching. \u00a7Limited to single hardware platform.21] \nLlama \n\n\n\nTable 2 .\n2Video pipelines used for evaluating Llama, their operations, and video inputs. \u2020 are non-deep learning pipeline operations.\n\nTable 3 .\n3Mean and standard deviation of number of configurations used and percent of invocations that met their allotted slack. Llama accurately allots and meets almost all slack by selecting a variety of different configurations per pipeline.\n\n\nTable 4. Impact of profiling errors. Latency and cost for the Denoising and Synthetic pipelines when profiled values are inaccurate (set to 50% of their measured latencies). FB is feedback and DFP is depth-first priority. Without these techniques, Llama cannot meet the latency target, or uses configurations that are not cost-effective.Pipeline (target) Llama \nLlama w/o FB & DFP \nDenoising (350s) \n(348s, $1.20) (369s, $1.64) \nSynthetic (520s) \n(520s, $2.31) (487s, $3.14) \n\n\n\nTable 5. Llama's decision overheads. Mean and standard deviation latencies of invocations for the AMBER Alert pipeline. Latencies are per-invocation for online actions, per-operation for profiling, and per-pipeline for path decomposition. For each online action, we show the percent of the execution time spent on the action across all operation invocations (73K).Phase \n\nAction \nLatency (% of exec.) \nSpecification Profiling \n257 \u00b1 155 s \nPath decomposition 1.74 s \nOnline \nSpeculate \n0.005 \u00b1 0.005 ms (0.08%) \nCommit \n0.186 \u00b1 0.813 ms (3.1%) \nInvoke \n0.151 \u00b1 0.078 ms (2.5%) \nFinalize \n0.141 \u00b1 1.209 ms (2.4%) \n\n\nLlama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics PipelinesFrancisco Romero *Stanford UniversityMark Zhao * Stanford University\n\n. Ambarella cvflow architecture. Ambarella cvflow architecture, 2021. https://www.ambarella.com/ technology/#cvflow.\n\nAws lambda. Aws lambda, 2021. https://aws.amazon.com/lambda/.\n\nAws step functions. Aws step functions, 2021. https://docs.aws.amazon.com/step- functions/latest/dg/welcome.html.\n\nAzure functions. Azure functions, 2021. https://azure.microsoft.com/en-us/services/ functions/.\n\n. Cisco annual internet report. Cisco annual internet report (2018-2023), 2021. https://www.cisco. com/c/en/us/solutions/collateral/executive-perspectives/annual- internet-report/white-paper-c11-741490.html.\n\nCnn -futuristic cop cars may identify suspects. Cnn -futuristic cop cars may identify suspects, 2021. https://money.cnn.com/2017/10/19/technology/future/police-ai- dashcam/index.html.\n\n. Google Cloud, Google cloud, 2021. https://cloud.google.com/.\n\nGoogle cloud functions. Google cloud functions, 2021. https://cloud.google.com/functions.\n\nMulti-process service. Multi-process service, 2021. https://docs.nvidia.com/deploy/pdf/ CUDA_Multi_Process_Service_Overview.pdf.\n\nPolitical rally video. Political rally video, 2021. https://www.youtube.com/watch?v= FGDFAD3Jkuc.\n\nTraffic footage. Traffic footage, 2021. https://www.youtube.com/watch?v= MNn9qKG2UFI.\n\nTensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI'16. the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI'16USAUSENIX AssociationM. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, and et al. Tensorflow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI'16, page 265-283, USA, 2016. USENIX Association.\n\nDbtoaster: Higherorder delta processing for dynamic, frequently fresh views. Y Ahmad, O Kennedy, C Koch, M Nikolic, Proc. VLDB Endow. 510Y. Ahmad, O. Kennedy, C. Koch, and M. Nikolic. Dbtoaster: Higher- order delta processing for dynamic, frequently fresh views. Proc. VLDB Endow., 5(10):968-979, June 2012.\n\nCherrypick: Adaptively unearthing the best cloud configurations for big data analytics. O Alipourfard, H H Liu, J Chen, S Venkataraman, M Yu, M Zhang, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). Boston, MAUSENIX AssociationO. Alipourfard, H. H. Liu, J. Chen, S. Venkataraman, M. Yu, and M. Zhang. Cherrypick: Adaptively unearthing the best cloud configu- rations for big data analytics. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 469-482, Boston, MA, Mar. 2017. USENIX Association.\n\n. Amazon Go, Amazon Go. https://www.amazon.com/b?ie=UTF8&node= 16008589011, 2021.\n\nReal-time video analytics: The killer app for edge computing. G Ananthanarayanan, P Bahl, P Bod\u00edk, K Chintalapudi, M Philipose, L Ravindranath, S Sinha, Computer. 5010G. Ananthanarayanan, P. Bahl, P. Bod\u00edk, K. Chintalapudi, M. Philipose, L. Ravindranath, and S. Sinha. Real-time video analytics: The killer app for edge computing. Computer, 50(10):58-67, 2017.\n\nEffective straggler mitigation: Attack of the clones. G Ananthanarayanan, A Ghodsi, S Shenker, I Stoica, 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13). Lombard, ILUSENIX AssociationG. Ananthanarayanan, A. Ghodsi, S. Shenker, and I. Stoica. Effective straggler mitigation: Attack of the clones. In 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13), pages 185-198, Lombard, IL, Apr. 2013. USENIX Association.\n\nSprocket: A serverless video processing framework. L Ao, L Izhikevich, G M Voelker, G Porter, Proceedings of the ACM Symposium on Cloud Computing, SoCC '18. the ACM Symposium on Cloud Computing, SoCC '18New York, NY, USAAssociation for Computing MachineryL. Ao, L. Izhikevich, G. M. Voelker, and G. Porter. Sprocket: A serverless video processing framework. In Proceedings of the ACM Symposium on Cloud Computing, SoCC '18, page 263-274, New York, NY, USA, 2018. Association for Computing Machinery.\n\n. Artificial Intelligence Security Surveillance Cameras. Artificial Intelligence Security Surveillance Cameras. https: //www.theverge.com/2018/1/23/16907238/artificial-intelligence- surveillance-cameras-security, 2018.\n\nRock you like a hurricane: Taming skew in large scale analytics. L Bindschaedler, J Malicevic, N Schiper, A Goel, W Zwaenepoel, Proceedings of the Thirteenth EuroSys Conference, EuroSys '18. the Thirteenth EuroSys Conference, EuroSys '18New York, NY, USAAssociation for Computing MachineryL. Bindschaedler, J. Malicevic, N. Schiper, A. Goel, and W. Zwaenepoel. Rock you like a hurricane: Taming skew in large scale analytics. In Proceedings of the Thirteenth EuroSys Conference, EuroSys '18, New York, NY, USA, 2018. Association for Computing Machinery.\n\nThe OpenCV Library. Dr. Dobb's Journal of Software Tools. G Bradski, G. Bradski. The OpenCV Library. Dr. Dobb's Journal of Software Tools, 2000.\n\nNvidia's a100 gpu: Performance and innovation for gpu computing. J Choquette, W Gandhi, 2020 IEEE Hot Chips 32 Symposium (HCS), Virtual. IEEEJ. Choquette and W. Gandhi. Nvidia's a100 gpu: Performance and innovation for gpu computing. In 2020 IEEE Hot Chips 32 Symposium (HCS), Virtual, August 16-18, 2020. IEEE, 2020.\n\nInferline: Latency-aware provisioning and scaling for prediction serving pipelines. D Crankshaw, G.-E Sela, X Mo, C Zumar, I Stoica, J Gonzalez, A Tumanov, Proceedings of the 11th ACM Symposium on Cloud Computing, SoCC '20. the 11th ACM Symposium on Cloud Computing, SoCC '20New York, NY, USAAssociation for Computing MachineryD. Crankshaw, G.-E. Sela, X. Mo, C. Zumar, I. Stoica, J. Gonzalez, and A. Tumanov. Inferline: Latency-aware provisioning and scaling for prediction serving pipelines. In Proceedings of the 11th ACM Symposium on Cloud Computing, SoCC '20, page 477-491, New York, NY, USA, 2020. Association for Computing Machinery.\n\nClipper: A low-latency online prediction serving system. D Crankshaw, X Wang, G Zhou, M J Franklin, J E Gonzalez, I Stoica, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). Boston, MAUSENIX AssociationD. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, and I. Stoica. Clipper: A low-latency online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Imple- mentation (NSDI 17), pages 613-627, Boston, MA, Mar. 2017. USENIX Association.\n\nMapreduce: Simplified data processing on large clusters. J Dean, S Ghemawat, Proceedings of the 6th Conference on Symposium on Operating Systems Design & Implementation. the 6th Conference on Symposium on Operating Systems Design & ImplementationUSAUSENIX Association6OSDI'04J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. In Proceedings of the 6th Conference on Symposium on Operating Systems Design & Implementation -Volume 6, OSDI'04, page 10, USA, 2004. USENIX Association.\n\nCostless: Optimizing cost of serverless computing through function fusion and placement. T , 2018 IEEE/ACM Symposium on Edge Computing (SEC). T. Elgamal. Costless: Optimizing cost of serverless computing through function fusion and placement. In 2018 IEEE/ACM Symposium on Edge Computing (SEC), pages 300-312, Oct 2018.\n\nJockey: Guaranteed job latency in data parallel clusters. A D Ferguson, P Bodik, S Kandula, E Boutin, R Fonseca, Proceedings of the 7th ACM European Conference on Computer Systems, EuroSys '12. the 7th ACM European Conference on Computer Systems, EuroSys '12New York, NY, USAAssociation for Computing MachineryA. D. Ferguson, P. Bodik, S. Kandula, E. Boutin, and R. Fonseca. Jockey: Guaranteed job latency in data parallel clusters. In Proceedings of the 7th ACM European Conference on Computer Systems, EuroSys '12, page 99-112, New York, NY, USA, 2012. Association for Computing Machinery.\n\n. Ffmpeg, 2021FFmpeg. https://ffmpeg.org/, 2021.\n\nFrom laptop to lambda: Outsourcing everyday jobs to thousands of transient functional containers. S Fouladi, F Romero, D Iter, Q Li, S Chatterjee, C Kozyrakis, M Zaharia, K Winstein, Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC '19. the 2019 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC '19USAUSENIX AssociationS. Fouladi, F. Romero, D. Iter, Q. Li, S. Chatterjee, C. Kozyrakis, M. Za- haria, and K. Winstein. From laptop to lambda: Outsourcing everyday jobs to thousands of transient functional containers. In Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC '19, page 475-488, USA, 2019. USENIX Association.\n\nEncoding, fast and slow: Low-latency video processing using thousands of tiny threads. S Fouladi, R S Wahby, B Shacklett, K V Balasubramaniam, W Zeng, R Bhalerao, A Sivaraman, G Porter, K Winstein, Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation, NSDI'17. the 14th USENIX Conference on Networked Systems Design and Implementation, NSDI'17USAUSENIX AssociationS. Fouladi, R. S. Wahby, B. Shacklett, K. V. Balasubramaniam, W. Zeng, R. Bhalerao, A. Sivaraman, G. Porter, and K. Winstein. Encoding, fast and slow: Low-latency video processing using thousands of tiny threads. In Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation, NSDI'17, page 363-376, USA, 2017. USENIX Association.\n\nAgilex generation of intel fpgas. I Ganusov, M Iyer, 2020 IEEE Hot Chips 32 Symposium (HCS), Virtual. IEEEI. Ganusov and M. Iyer. Agilex generation of intel fpgas. In 2020 IEEE Hot Chips 32 Symposium (HCS), Virtual, August 16-18, 2020. IEEE, 2020.\n\nMulti-label multi-task deep learning for behavioral coding. J Gibson, D Atkins, T Creed, Z Imel, P Georgiou, S Narayanan, IEEE Transactions on Affective Computing. J. Gibson, D. Atkins, T. Creed, Z. Imel, P. Georgiou, and S. Narayanan. Multi-label multi-task deep learning for behavioral coding. IEEE Trans- actions on Affective Computing, pages 1-1, 2019.\n\nMusketeer: All for one, one for all in data processing systems. I Gog, M Schwarzkopf, N Crooks, M P Grosvenor, A Clement, S Hand, Proceedings of the Tenth European Conference on Computer Systems, EuroSys '15. the Tenth European Conference on Computer Systems, EuroSys '15New York, NY, USAAssociation for Computing MachineryI. Gog, M. Schwarzkopf, N. Crooks, M. P. Grosvenor, A. Clement, and S. Hand. Musketeer: All for one, one for all in data processing systems. In Proceedings of the Tenth European Conference on Computer Systems, EuroSys '15, New York, NY, USA, 2015. Association for Computing Machinery.\n\nFifer: Tackling resource underutilization in the serverless era. J R Gunasekaran, P Thinakaran, N C Nachiappan, M T Kandemir, C R Das, Proceedings of the 21st International Middleware Conference, Middleware '20. the 21st International Middleware Conference, Middleware '20New York, NY, USAAssociation for Computing MachineryJ. R. Gunasekaran, P. Thinakaran, N. C. Nachiappan, M. T. Kandemir, and C. R. Das. Fifer: Tackling resource underutilization in the serverless era. In Proceedings of the 21st International Middleware Conference, Middleware '20, page 280-295, New York, NY, USA, 2020. Association for Computing Machinery.\n\nProfiling, what-if analysis, and costbased optimization of mapreduce programs. H Herodotou, S Babu, Proc. VLDB Endow. VLDB Endow4H. Herodotou and S. Babu. Profiling, what-if analysis, and cost- based optimization of mapreduce programs. Proc. VLDB Endow., 4(11):1111-1122, Aug. 2011.\n\nFocus: Querying large video datasets with low latency and low cost. K Hsieh, G Ananthanarayanan, P Bodik, S Venkataraman, P Bahl, M Philipose, P B Gibbons, O Mutlu, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationK. Hsieh, G. Ananthanarayanan, P. Bodik, S. Venkataraman, P. Bahl, M. Philipose, P. B. Gibbons, and O. Mutlu. Focus: Querying large video datasets with low latency and low cost. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 269-286, Carlsbad, CA, Oct. 2018. USENIX Association.\n\nDryad: Distributed data-parallel programs from sequential building blocks. M Isard, M Budiu, Y Yu, A Birrell, D Fetterly, Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, EuroSys '07. the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, EuroSys '07New York, NY, USAAssociation for Computing MachineryM. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: Distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, EuroSys '07, page 59-72, New York, NY, USA, 2007. Association for Computing Machinery.\n\nArtificial intelligence in healthcare: past, present and future. F Jiang, Y Jiang, H Zhi, Y Dong, H Li, S Ma, Y Wang, Q Dong, H Shen, Y Wang, Stroke and Vascular Neurology. 24F. Jiang, Y. Jiang, H. Zhi, Y. Dong, H. Li, S. Ma, Y. Wang, Q. Dong, H. Shen, and Y. Wang. Artificial intelligence in healthcare: past, present and future. Stroke and Vascular Neurology, 2(4):230-243, 2017.\n\nChameleon: Scalable adaptation of video analytics. J Jiang, G Ananthanarayanan, P Bodik, S Sen, I Stoica, Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18. the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18New York, NY, USAAssociation for Computing MachineryJ. Jiang, G. Ananthanarayanan, P. Bodik, S. Sen, and I. Stoica. Chameleon: Scalable adaptation of video analytics. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Com- munication, SIGCOMM '18, page 253-266, New York, NY, USA, 2018. Association for Computing Machinery.\n\nOccupy the cloud: Distributed computing for the 99%. E Jonas, Q Pu, S Venkataraman, I Stoica, B Recht, Proceedings of the 2017 Symposium on Cloud Computing, SoCC '17. the 2017 Symposium on Cloud Computing, SoCC '17New York, NY, USAAssociation for Computing MachineryE. Jonas, Q. Pu, S. Venkataraman, I. Stoica, and B. Recht. Occupy the cloud: Distributed computing for the 99%. In Proceedings of the 2017 Symposium on Cloud Computing, SoCC '17, page 445-451, New York, NY, USA, 2017. Association for Computing Machinery.\n\nIn-datacenter performance analysis of a tensor processing unit. N P Jouppi, C Young, N Patil, D Patterson, G Agrawal, R Bajwa, S Bates, S Bhatia, N Boden, A Borchers, R Boyle, P Cantin, C Chao, C Clark, J Coriell, M Daley, M Dau, J Dean, B Gelb, T V Ghaemmaghami, R Gottipati, W Gulland, R Hagmann, C R Ho, D Hogberg, J Hu, R Hundt, D Hurt, J Ibarz, A Jaffey, A Jaworski, A Kaplan, H Khaitan, D Killebrew, A Koch, N Kumar, S Lacy, J Laudon, J Law, D Le, C Leary, Z Liu, K Lucke, A Lundin, G Mackean, A Maggiore, M Mahony, K Miller, R Nagarajan, R Narayanaswami, R Ni, K Nix, T Norrie, M Omernick, N Penukonda, A Phelps, J Ross, M Ross, A Salek, E Samadiani, C Severn, G Sizikov, M Snelham, J Souter, D Steinberg, A Swing, M Tan, G Thorson, B Tian, H Toma, E Tuttle, V Vasudevan, R Walter, W Wang, E Wilcox, D H Yoon, 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T. V. Ghaem- maghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony, K. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tut- tle, V. Vasudevan, R. Walter, W. Wang, E. Wilcox, and D. H. Yoon. In-datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), pages 1-12, June 2017.\n\nGrandslam: Guaranteeing slas for jobs in microservices execution frameworks. R S Kannan, L Subramanian, A Raju, J Ahn, J Mars, L Tang, Proceedings of the Fourteenth EuroSys Conference 2019, EuroSys '19. the Fourteenth EuroSys Conference 2019, EuroSys '19New York, NY, USAAssociation for Computing MachineryR. S. Kannan, L. Subramanian, A. Raju, J. Ahn, J. Mars, and L. Tang. Grandslam: Guaranteeing slas for jobs in microservices execution frameworks. In Proceedings of the Fourteenth EuroSys Conference 2019, EuroSys '19, New York, NY, USA, 2019. Association for Computing Machinery.\n\nList intersection for web search: Algorithms, cost models, and optimizations. S Kim, T Lee, S Hwang, S Elnikety, Proc. VLDB Endow. 121S. Kim, T. Lee, S.-w. Hwang, and S. Elnikety. List intersection for web search: Algorithms, cost models, and optimizations. Proc. VLDB Endow., 12(1):1-13, Sept. 2018.\n\nDlib-ml: A machine learning toolkit. D E King, Journal of Machine Learning Research. 10D. E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research, 10:1755-1758, 2009.\n\nSelecta: Heterogeneous cloud storage configuration for data analytics. A Klimovic, H Litz, C Kozyrakis, 2018 USENIX Annual Technical Conference (USENIX ATC 18). Boston, MAUSENIX AssociationA. Klimovic, H. Litz, and C. Kozyrakis. Selecta: Heterogeneous cloud storage configuration for data analytics. In 2018 USENIX Annual Tech- nical Conference (USENIX ATC 18), pages 759-773, Boston, MA, July 2018. USENIX Association.\n\nSol: Fast distributed computation over slow networks. F Lai, J You, X Zhu, H V Madhyastha, M Chowdhury, 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20). Santa Clara, CAUSENIX AssociationF. Lai, J. You, X. Zhu, H. V. Madhyastha, and M. Chowdhury. Sol: Fast distributed computation over slow networks. In 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20), pages 273-288, Santa Clara, CA, Feb. 2020. USENIX Association.\n\nDynamic query re-planning using qoop. K Mahajan, M Chowdhury, A Akella, S Chawla, Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI'18. the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI'18USAUSENIX AssociationK. Mahajan, M. Chowdhury, A. Akella, and S. Chawla. Dynamic query re-planning using qoop. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI'18, page 253-267, USA, 2018. USENIX Association.\n\nOPTIMUSCLOUD: Heterogeneous configuration optimization for distributed databases in the cloud. A Mahgoub, A M Medoff, R Kumar, S Mitra, A Klimovic, S Chaterji, S Bagchi, 2020 USENIX Annual Technical Conference (USENIX ATC 20). A. Mahgoub, A. M. Medoff, R. Kumar, S. Mitra, A. Klimovic, S. Chaterji, and S. Bagchi. OPTIMUSCLOUD: Heterogeneous configuration opti- mization for distributed databases in the cloud. In 2020 USENIX Annual Technical Conference (USENIX ATC 20), pages 189-203. USENIX Asso- ciation, July 2020.\n\nPregel: A system for large-scale graph processing. G Malewicz, M H Austern, A J Bik, J C Dehnert, I Horn, N Leiser, G Czajkowski, Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data, SIGMOD '10. the 2010 ACM SIGMOD International Conference on Management of Data, SIGMOD '10New York, NY, USAAssociation for Computing MachineryG. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: A system for large-scale graph processing. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data, SIGMOD '10, page 135-146, New York, NY, USA, 2010. Association for Computing Machinery.\n\nCiel: A universal execution engine for distributed data-flow computing. D G Murray, M Schwarzkopf, C Smowton, S Smith, A Madhavapeddy, S Hand, Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI'11. the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI'11USAUSENIX AssociationD. G. Murray, M. Schwarzkopf, C. Smowton, S. Smith, A. Mad- havapeddy, and S. Hand. Ciel: A universal execution engine for distributed data-flow computing. In Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI'11, page 113-126, USA, 2011. USENIX Association.\n\nSparrow: Distributed, low latency scheduling. K Ousterhout, P Wendell, M Zaharia, I Stoica, Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP '13. the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP '13New York, NY, USAAssociation for Computing MachineryK. Ousterhout, P. Wendell, M. Zaharia, and I. Stoica. Sparrow: Dis- tributed, low latency scheduling. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP '13, page 69-84, New York, NY, USA, 2013. Association for Computing Machinery.\n\nScanner: Efficient video analysis at scale. A Poms, W Crichton, P Hanrahan, K Fatahalian, ACM Trans. Graph. 374A. Poms, W. Crichton, P. Hanrahan, and K. Fatahalian. Scanner: Effi- cient video analysis at scale. ACM Trans. Graph., 37(4), July 2018.\n\nDandelion: A compiler and runtime for heterogeneous systems. C J Rossbach, Y Yu, J Currey, J.-P Martin, D Fetterly, Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP '13. the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP '13New York, NY, USAAssociation for Computing MachineryC. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. Fetterly. Dandelion: A compiler and runtime for heterogeneous systems. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Princi- ples, SOSP '13, page 49-68, New York, NY, USA, 2013. Association for Computing Machinery.\n\nNexus: A gpu cluster engine for accelerating dnn-based video analysis. H Shen, L Chen, Y Jin, L Zhao, B Kong, M Philipose, A Krishnamurthy, R Sundaram, Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP '19. the 27th ACM Symposium on Operating Systems Principles, SOSP '19New York, NY, USAAssociation for Computing MachineryH. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Krishna- murthy, and R. Sundaram. Nexus: A gpu cluster engine for accelerating dnn-based video analysis. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP '19, page 322-337, New York, NY, USA, 2019. Association for Computing Machinery.\n\nAn end-to-end learning-based cost estimator. J Sun, G Li, Proc. VLDB Endow. VLDB Endow13J. Sun and G. Li. An end-to-end learning-based cost estimator. Proc. VLDB Endow., 13(3):307-319, Nov. 2019.\n\nIbtune: Individualized buffer tuning for largescale cloud databases. J Tan, T Zhang, F Li, J Chen, Q Zheng, P Zhang, H Qiao, Y Shi, W Cao, R Zhang, Proc. VLDB Endow. 1210J. Tan, T. Zhang, F. Li, J. Chen, Q. Zheng, P. Zhang, H. Qiao, Y. Shi, W. Cao, and R. Zhang. Ibtune: Individualized buffer tuning for large- scale cloud databases. Proc. VLDB Endow., 12(10):1221-1234, June 2019.\n\nTetrisched: Global rescheduling with adaptive plan-ahead in dynamic heterogeneous clusters. A Tumanov, T Zhu, J Park, M Kozuch, M Harchol-Balter, G Ganger, Proceedings of the 11th European Conference on Computer Systems, EuroSys 2016, Proceedings of the 11th European Conference on Computer Systems, EuroSys. the 11th European Conference on Computer Systems, EuroSys 2016, the 11th European Conference on Computer Systems, EuroSysAssociation for Computing Machinery, Inc11th European Conference on Computer Systems. Conference dateA. Tumanov, T. Zhu, J. Park, M. Kozuch, M. Harchol-Balter, and G. Ganger. Tetrisched: Global rescheduling with adaptive plan-ahead in dynamic heterogeneous clusters. In Proceedings of the 11th Euro- pean Conference on Computer Systems, EuroSys 2016, Proceedings of the 11th European Conference on Computer Systems, EuroSys 2016. Association for Computing Machinery, Inc, Apr. 2016. 11th European Conference on Computer Systems, EuroSys 2016 ; Conference date: 18-04-2016 Through 21-04-2016.\n\nThe power of choice in data-aware cluster scheduling. S Venkataraman, A Panda, G Ananthanarayanan, M J Franklin, I Stoica, Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI'14. the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI'14USAUSENIX AssociationS. Venkataraman, A. Panda, G. Ananthanarayanan, M. J. Franklin, and I. Stoica. The power of choice in data-aware cluster scheduling. In Proceedings of the 11th USENIX Conference on Operating Systems De- sign and Implementation, OSDI'14, page 301-316, USA, 2014. USENIX Association.\n\nErnest: Efficient performance prediction for large-scale advanced analytics. S Venkataraman, Z Yang, M Franklin, B Recht, I Stoica, 13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 16). Santa Clara, CAUSENIX AssociationS. Venkataraman, Z. Yang, M. Franklin, B. Recht, and I. Stoica. Ernest: Efficient performance prediction for large-scale advanced analytics. In 13th USENIX Symposium on Networked Systems Design and Implemen- tation (NSDI 16), pages 363-378, Santa Clara, CA, Mar. 2016. USENIX Association.\n\nRate-based query optimization for streaming information sources. S D Viglas, J F Naughton, Proceedings of the 2002 ACM SIG-MOD International Conference on Management of Data, SIGMOD '02. the 2002 ACM SIG-MOD International Conference on Management of Data, SIGMOD '02New York, NY, USAAssociation for Computing MachineryS. D. Viglas and J. F. Naughton. Rate-based query optimization for streaming information sources. In Proceedings of the 2002 ACM SIG- MOD International Conference on Management of Data, SIGMOD '02, page 37-48, New York, NY, USA, 2002. Association for Computing Machinery.\n\nXilinx versal premium series. M Voogel, Y Frans, M Ouellette, 2020 IEEE Hot Chips 32 Symposium (HCS), Virtual. IEEEM. Voogel, Y. Frans, and M. Ouellette. Xilinx versal premium series. In 2020 IEEE Hot Chips 32 Symposium (HCS), Virtual, August 16-18, 2020. IEEE, 2020.\n\nPeeking behind the curtains of serverless platforms. L Wang, M Li, Y Zhang, T Ristenpart, M Swift, 2018 USENIX Annual Technical Conference (USENIX ATC 18). Boston, MAUSENIX AssociationL. Wang, M. Li, Y. Zhang, T. Ristenpart, and M. Swift. Peeking behind the curtains of serverless platforms. In 2018 USENIX Annual Technical Conference (USENIX ATC 18), pages 133-146, Boston, MA, July 2018. USENIX Association.\n\nTempura: A general costbased optimizer framework for incremental data processing. Z Wang, K Zeng, B Huang, W Chen, X Cui, B Wang, J Liu, L Fan, D Qu, Z Hou, T Guan, C Li, J Zhou, Proc. VLDB Endow. 141Z. Wang, K. Zeng, B. Huang, W. Chen, X. Cui, B. Wang, J. Liu, L. Fan, D. Qu, Z. Hou, T. Guan, C. Li, and J. Zhou. Tempura: A general cost- based optimizer framework for incremental data processing. Proc. VLDB Endow., 14(1):14-27, Sept. 2020.\n\nVideochef: Efficient approximation for streaming video processing pipelines. R Xu, J Koo, R Kumar, P Bai, S Mitra, S Misailovic, S Bagchi, 2018 USENIX Annual Technical Conference (USENIX ATC 18). Boston, MAUSENIX AssociationR. Xu, J. Koo, R. Kumar, P. Bai, S. Mitra, S. Misailovic, and S. Bagchi. Videochef: Efficient approximation for streaming video processing pipelines. In 2018 USENIX Annual Technical Conference (USENIX ATC 18), pages 43-56, Boston, MA, July 2018. USENIX Association.\n\nWrangler: Predictable and faster jobs using fewer resources. N J Yadwadkar, G Ananthanarayanan, R Katz, Proceedings of the ACM Symposium on Cloud Computing, SOCC '14. the ACM Symposium on Cloud Computing, SOCC '14New York, NY, USAAssociation for Computing MachineryN. J. Yadwadkar, G. Ananthanarayanan, and R. Katz. Wrangler: Pre- dictable and faster jobs using fewer resources. In Proceedings of the ACM Symposium on Cloud Computing, SOCC '14, page 1-14, New York, NY, USA, 2014. Association for Computing Machinery.\n\nSelecting the best vm across multiple public clouds: A datadriven performance modeling approach. N J Yadwadkar, B Hariharan, J E Gonzalez, B Smith, R H Katz, Proceedings of the 2017 Symposium on Cloud Computing, SoCC '17. the 2017 Symposium on Cloud Computing, SoCC '17ACMN. J. Yadwadkar, B. Hariharan, J. E. Gonzalez, B. Smith, and R. H. Katz. Selecting the best vm across multiple public clouds: A data- driven performance modeling approach. In Proceedings of the 2017 Symposium on Cloud Computing, SoCC '17, pages 452-465. ACM, 2017.\n\nSpark: Cluster computing with working sets. M Zaharia, M Chowdhury, M J Franklin, S Shenker, I Stoica, Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud'10. the 2nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud'10USAUSENIX AssociationM. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark: Cluster computing with working sets. In Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud'10, page 10, USA, 2010. USENIX Association.\n\nLive video analytics at scale with approximation and delay-tolerance. H Zhang, G Ananthanarayanan, P Bodik, M Philipose, P Bahl, M J Freedman, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). Boston, MAUSENIX AssociationH. Zhang, G. Ananthanarayanan, P. Bodik, M. Philipose, P. Bahl, and M. J. Freedman. Live video analytics at scale with approximation and delay-tolerance. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 377-392, Boston, MA, Mar. 2017. USENIX Association.\n", "annotations": {"author": "[{\"end\":190,\"start\":91},{\"end\":283,\"start\":191},{\"end\":386,\"start\":284},{\"end\":488,\"start\":387}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":101},{\"end\":200,\"start\":196},{\"end\":303,\"start\":294},{\"end\":405,\"start\":396}]", "author_first_name": "[{\"end\":100,\"start\":91},{\"end\":195,\"start\":191},{\"end\":291,\"start\":284},{\"end\":293,\"start\":292},{\"end\":395,\"start\":387}]", "author_affiliation": "[{\"end\":189,\"start\":109},{\"end\":282,\"start\":202},{\"end\":385,\"start\":305},{\"end\":487,\"start\":407}]", "title": "[{\"end\":88,\"start\":1},{\"end\":576,\"start\":489}]", "venue": null, "abstract": "[{\"end\":2396,\"start\":578}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2518,\"start\":2515},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2796,\"start\":2793},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3160,\"start\":3156},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3163,\"start\":3160},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3166,\"start\":3163},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3169,\"start\":3166},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3198,\"start\":3194},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3215,\"start\":3211},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3232,\"start\":3228},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3702,\"start\":3699},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4122,\"start\":4118},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4134,\"start\":4130},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4137,\"start\":4134},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4148,\"start\":4144},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4170,\"start\":4167},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4542,\"start\":4538},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4642,\"start\":4638},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4645,\"start\":4642},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":4648,\"start\":4645},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4829,\"start\":4825},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":4832,\"start\":4829},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5608,\"start\":5604},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":5756,\"start\":5752},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5775,\"start\":5771},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5919,\"start\":5915},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5928,\"start\":5924},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5941,\"start\":5937},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5960,\"start\":5956},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6030,\"start\":6027},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6032,\"start\":6030},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6034,\"start\":6032},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7232,\"start\":7228},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8829,\"start\":8825},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8843,\"start\":8839},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8852,\"start\":8848},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8872,\"start\":8868},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9105,\"start\":9102},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9108,\"start\":9105},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9111,\"start\":9108},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9494,\"start\":9490},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9497,\"start\":9494},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10557,\"start\":10553},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10572,\"start\":10568},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":10588,\"start\":10584},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10599,\"start\":10595},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10610,\"start\":10606},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10623,\"start\":10619},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10631,\"start\":10627},{\"end\":10642,\"start\":10641},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10713,\"start\":10709},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10795,\"start\":10791},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":10812,\"start\":10808},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10825,\"start\":10821},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10887,\"start\":10883},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10899,\"start\":10895},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10915,\"start\":10911},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10935,\"start\":10931},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":12122,\"start\":12118},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12711,\"start\":12707},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12714,\"start\":12711},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12717,\"start\":12714},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12765,\"start\":12761},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12768,\"start\":12765},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":12771,\"start\":12768},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13486,\"start\":13482},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13950,\"start\":13946},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13953,\"start\":13950},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14198,\"start\":14194},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14220,\"start\":14216},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14238,\"start\":14234},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14272,\"start\":14269},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14274,\"start\":14272},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14276,\"start\":14274},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14827,\"start\":14823},{\"end\":23633,\"start\":23628},{\"end\":23649,\"start\":23644},{\"end\":23658,\"start\":23653},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24880,\"start\":24876},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24895,\"start\":24891},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28774,\"start\":28770},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28777,\"start\":28774},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29768,\"start\":29764},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":29771,\"start\":29768},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29774,\"start\":29771},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29777,\"start\":29774},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":29780,\"start\":29777},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35908,\"start\":35907},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":36527,\"start\":36523},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36530,\"start\":36527},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":36533,\"start\":36530},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37388,\"start\":37384},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":37404,\"start\":37400},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37464,\"start\":37460},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":38868,\"start\":38864},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39016,\"start\":39013},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39046,\"start\":39043},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39328,\"start\":39324},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39539,\"start\":39536},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40404,\"start\":40400},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":40407,\"start\":40404},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":40410,\"start\":40407},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40540,\"start\":40537},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42437,\"start\":42433},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":42607,\"start\":42603},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":42772,\"start\":42768},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43092,\"start\":43088},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":45638,\"start\":45634},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":47432,\"start\":47428},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":48973,\"start\":48969},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":58327,\"start\":58323},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":58330,\"start\":58327},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":58333,\"start\":58330},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":58336,\"start\":58333},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":58339,\"start\":58336},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":58342,\"start\":58339},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":58345,\"start\":58342},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":58452,\"start\":58448},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":58455,\"start\":58452},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":58458,\"start\":58455},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":58461,\"start\":58458},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":58464,\"start\":58461},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":58467,\"start\":58464},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":58470,\"start\":58467},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":58832,\"start\":58828},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":58847,\"start\":58843},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":58973,\"start\":58969},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":59095,\"start\":59091},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":59098,\"start\":59095},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":59101,\"start\":59098},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":59104,\"start\":59101},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":59107,\"start\":59104},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":59319,\"start\":59315},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":59748,\"start\":59744},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":59767,\"start\":59763},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":60109,\"start\":60105},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":60112,\"start\":60109},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":60115,\"start\":60112},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":60118,\"start\":60115},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":60121,\"start\":60118},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":60124,\"start\":60121},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":60190,\"start\":60186},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":60368,\"start\":60364},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":60384,\"start\":60380},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":60569,\"start\":60565},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":60618,\"start\":60614},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":60635,\"start\":60631}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":61784,\"start\":61707},{\"attributes\":{\"id\":\"fig_1\"},\"end\":61859,\"start\":61785},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61926,\"start\":61860},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61969,\"start\":61927},{\"attributes\":{\"id\":\"fig_7\"},\"end\":62370,\"start\":61970},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":62748,\"start\":62371},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":62884,\"start\":62749},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":63131,\"start\":62885},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":63610,\"start\":63132},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":64227,\"start\":63611}]", "paragraph": "[{\"end\":3233,\"start\":2412},{\"end\":5234,\"start\":3235},{\"end\":5842,\"start\":5236},{\"end\":6483,\"start\":5844},{\"end\":6898,\"start\":6485},{\"end\":7317,\"start\":6900},{\"end\":8873,\"start\":7319},{\"end\":9073,\"start\":8903},{\"end\":10778,\"start\":9075},{\"end\":11057,\"start\":10780},{\"end\":11594,\"start\":11072},{\"end\":12772,\"start\":11596},{\"end\":13813,\"start\":12774},{\"end\":14470,\"start\":13815},{\"end\":15900,\"start\":14487},{\"end\":17660,\"start\":15902},{\"end\":18208,\"start\":17677},{\"end\":18922,\"start\":18210},{\"end\":21101,\"start\":18946},{\"end\":22090,\"start\":21103},{\"end\":23705,\"start\":22107},{\"end\":23829,\"start\":23707},{\"end\":23890,\"start\":23831},{\"end\":24177,\"start\":24018},{\"end\":24677,\"start\":24216},{\"end\":26277,\"start\":24715},{\"end\":27010,\"start\":26316},{\"end\":27683,\"start\":27077},{\"end\":28036,\"start\":27685},{\"end\":28339,\"start\":28038},{\"end\":29688,\"start\":28378},{\"end\":30849,\"start\":29690},{\"end\":31023,\"start\":30851},{\"end\":31109,\"start\":31025},{\"end\":31231,\"start\":31111},{\"end\":31466,\"start\":31233},{\"end\":34411,\"start\":31468},{\"end\":35621,\"start\":34429},{\"end\":36037,\"start\":35623},{\"end\":36403,\"start\":36089},{\"end\":37048,\"start\":36451},{\"end\":37521,\"start\":37067},{\"end\":38345,\"start\":37523},{\"end\":38678,\"start\":38347},{\"end\":39116,\"start\":38680},{\"end\":39752,\"start\":39118},{\"end\":42452,\"start\":39767},{\"end\":42621,\"start\":42454},{\"end\":42786,\"start\":42623},{\"end\":42958,\"start\":42788},{\"end\":43329,\"start\":42960},{\"end\":44136,\"start\":43331},{\"end\":45265,\"start\":44138},{\"end\":47201,\"start\":45305},{\"end\":47603,\"start\":47203},{\"end\":48018,\"start\":47605},{\"end\":48359,\"start\":48020},{\"end\":49340,\"start\":48361},{\"end\":51765,\"start\":49382},{\"end\":53549,\"start\":51806},{\"end\":54429,\"start\":53551},{\"end\":56543,\"start\":54476},{\"end\":56718,\"start\":56545},{\"end\":57304,\"start\":56757},{\"end\":57827,\"start\":57306},{\"end\":58154,\"start\":57829},{\"end\":60351,\"start\":58171},{\"end\":60873,\"start\":60353},{\"end\":61706,\"start\":60888}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24017,\"start\":23891},{\"attributes\":{\"id\":\"formula_1\"},\"end\":27076,\"start\":27011},{\"attributes\":{\"id\":\"formula_3\"},\"end\":36088,\"start\":36038}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9782,\"start\":9775},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11615,\"start\":11608},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44161,\"start\":44154},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":51239,\"start\":51232},{\"end\":55389,\"start\":55382},{\"end\":56845,\"start\":56838}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2410,\"start\":2398},{\"attributes\":{\"n\":\"2\"},\"end\":8901,\"start\":8876},{\"attributes\":{\"n\":\"2.1\"},\"end\":11070,\"start\":11060},{\"attributes\":{\"n\":\"3\"},\"end\":14485,\"start\":14473},{\"attributes\":{\"n\":\"3.1\"},\"end\":17675,\"start\":17663},{\"attributes\":{\"n\":\"3.2\"},\"end\":18944,\"start\":18925},{\"attributes\":{\"n\":\"3.3\"},\"end\":22105,\"start\":22093},{\"attributes\":{\"n\":\"4\"},\"end\":24214,\"start\":24180},{\"attributes\":{\"n\":\"4.1\"},\"end\":24713,\"start\":24680},{\"attributes\":{\"n\":\"4.2\"},\"end\":26314,\"start\":26280},{\"attributes\":{\"n\":\"4.3\"},\"end\":28376,\"start\":28342},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":34427,\"start\":34414},{\"attributes\":{\"n\":\"4.5\"},\"end\":36449,\"start\":36406},{\"attributes\":{\"n\":\"5\"},\"end\":37065,\"start\":37051},{\"attributes\":{\"n\":\"6\"},\"end\":39765,\"start\":39755},{\"attributes\":{\"n\":\"6.1\"},\"end\":45303,\"start\":45268},{\"attributes\":{\"n\":\"6.2\"},\"end\":49380,\"start\":49343},{\"attributes\":{\"n\":\"6.3\"},\"end\":51804,\"start\":51768},{\"attributes\":{\"n\":\"6.4\"},\"end\":54474,\"start\":54432},{\"attributes\":{\"n\":\"6.5\"},\"end\":56755,\"start\":56721},{\"attributes\":{\"n\":\"7\"},\"end\":58169,\"start\":58157},{\"attributes\":{\"n\":\"8\"},\"end\":60886,\"start\":60876},{\"end\":61718,\"start\":61708},{\"end\":61796,\"start\":61786},{\"end\":61871,\"start\":61861},{\"end\":61938,\"start\":61928},{\"end\":61991,\"start\":61971},{\"end\":62381,\"start\":62372},{\"end\":62759,\"start\":62750},{\"end\":62895,\"start\":62886}]", "table": "[{\"end\":62748,\"start\":62735},{\"end\":63610,\"start\":63471},{\"end\":64227,\"start\":63977}]", "figure_caption": "[{\"end\":61784,\"start\":61720},{\"end\":61859,\"start\":61798},{\"end\":61926,\"start\":61873},{\"end\":61969,\"start\":61940},{\"end\":62370,\"start\":61994},{\"end\":62735,\"start\":62383},{\"end\":62884,\"start\":62761},{\"end\":63131,\"start\":62897},{\"end\":63471,\"start\":63134},{\"end\":63977,\"start\":63613}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9538,\"start\":9530},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12316,\"start\":12308},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13136,\"start\":13128},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17762,\"start\":17754},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18622,\"start\":18613},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21985,\"start\":21977},{\"end\":23011,\"start\":23003},{\"end\":23682,\"start\":23674},{\"end\":45908,\"start\":45900},{\"end\":46422,\"start\":46407},{\"end\":47359,\"start\":47343},{\"end\":50235,\"start\":50227},{\"end\":50912,\"start\":50904},{\"end\":52599,\"start\":52591}]", "bib_author_first_name": "[{\"end\":65180,\"start\":65174},{\"end\":65699,\"start\":65698},{\"end\":65708,\"start\":65707},{\"end\":65718,\"start\":65717},{\"end\":65726,\"start\":65725},{\"end\":65734,\"start\":65733},{\"end\":65743,\"start\":65742},{\"end\":65751,\"start\":65750},{\"end\":65760,\"start\":65759},{\"end\":65772,\"start\":65771},{\"end\":65782,\"start\":65781},{\"end\":66386,\"start\":66385},{\"end\":66395,\"start\":66394},{\"end\":66406,\"start\":66405},{\"end\":66414,\"start\":66413},{\"end\":66706,\"start\":66705},{\"end\":66721,\"start\":66720},{\"end\":66723,\"start\":66722},{\"end\":66730,\"start\":66729},{\"end\":66738,\"start\":66737},{\"end\":66754,\"start\":66753},{\"end\":66760,\"start\":66759},{\"end\":67190,\"start\":67184},{\"end\":67328,\"start\":67327},{\"end\":67348,\"start\":67347},{\"end\":67356,\"start\":67355},{\"end\":67365,\"start\":67364},{\"end\":67381,\"start\":67380},{\"end\":67394,\"start\":67393},{\"end\":67410,\"start\":67409},{\"end\":67682,\"start\":67681},{\"end\":67702,\"start\":67701},{\"end\":67712,\"start\":67711},{\"end\":67723,\"start\":67722},{\"end\":68149,\"start\":68148},{\"end\":68155,\"start\":68154},{\"end\":68169,\"start\":68168},{\"end\":68171,\"start\":68170},{\"end\":68182,\"start\":68181},{\"end\":68884,\"start\":68883},{\"end\":68901,\"start\":68900},{\"end\":68914,\"start\":68913},{\"end\":68925,\"start\":68924},{\"end\":68933,\"start\":68932},{\"end\":69432,\"start\":69431},{\"end\":69585,\"start\":69584},{\"end\":69598,\"start\":69597},{\"end\":69923,\"start\":69922},{\"end\":69939,\"start\":69935},{\"end\":69947,\"start\":69946},{\"end\":69953,\"start\":69952},{\"end\":69962,\"start\":69961},{\"end\":69972,\"start\":69971},{\"end\":69984,\"start\":69983},{\"end\":70538,\"start\":70537},{\"end\":70551,\"start\":70550},{\"end\":70559,\"start\":70558},{\"end\":70567,\"start\":70566},{\"end\":70569,\"start\":70568},{\"end\":70581,\"start\":70580},{\"end\":70583,\"start\":70582},{\"end\":70595,\"start\":70594},{\"end\":71050,\"start\":71049},{\"end\":71058,\"start\":71057},{\"end\":71595,\"start\":71594},{\"end\":71885,\"start\":71884},{\"end\":71887,\"start\":71886},{\"end\":71899,\"start\":71898},{\"end\":71908,\"start\":71907},{\"end\":71919,\"start\":71918},{\"end\":71929,\"start\":71928},{\"end\":72568,\"start\":72567},{\"end\":72579,\"start\":72578},{\"end\":72589,\"start\":72588},{\"end\":72597,\"start\":72596},{\"end\":72603,\"start\":72602},{\"end\":72617,\"start\":72616},{\"end\":72630,\"start\":72629},{\"end\":72641,\"start\":72640},{\"end\":73281,\"start\":73280},{\"end\":73292,\"start\":73291},{\"end\":73294,\"start\":73293},{\"end\":73303,\"start\":73302},{\"end\":73316,\"start\":73315},{\"end\":73318,\"start\":73317},{\"end\":73337,\"start\":73336},{\"end\":73345,\"start\":73344},{\"end\":73357,\"start\":73356},{\"end\":73370,\"start\":73369},{\"end\":73380,\"start\":73379},{\"end\":73989,\"start\":73988},{\"end\":74000,\"start\":73999},{\"end\":74264,\"start\":74263},{\"end\":74274,\"start\":74273},{\"end\":74284,\"start\":74283},{\"end\":74293,\"start\":74292},{\"end\":74301,\"start\":74300},{\"end\":74313,\"start\":74312},{\"end\":74626,\"start\":74625},{\"end\":74633,\"start\":74632},{\"end\":74648,\"start\":74647},{\"end\":74658,\"start\":74657},{\"end\":74660,\"start\":74659},{\"end\":74673,\"start\":74672},{\"end\":74684,\"start\":74683},{\"end\":75236,\"start\":75235},{\"end\":75238,\"start\":75237},{\"end\":75253,\"start\":75252},{\"end\":75267,\"start\":75266},{\"end\":75269,\"start\":75268},{\"end\":75283,\"start\":75282},{\"end\":75285,\"start\":75284},{\"end\":75297,\"start\":75296},{\"end\":75299,\"start\":75298},{\"end\":75879,\"start\":75878},{\"end\":75892,\"start\":75891},{\"end\":76152,\"start\":76151},{\"end\":76161,\"start\":76160},{\"end\":76181,\"start\":76180},{\"end\":76190,\"start\":76189},{\"end\":76206,\"start\":76205},{\"end\":76214,\"start\":76213},{\"end\":76227,\"start\":76226},{\"end\":76229,\"start\":76228},{\"end\":76240,\"start\":76239},{\"end\":76756,\"start\":76755},{\"end\":76765,\"start\":76764},{\"end\":76774,\"start\":76773},{\"end\":76780,\"start\":76779},{\"end\":76791,\"start\":76790},{\"end\":77415,\"start\":77414},{\"end\":77424,\"start\":77423},{\"end\":77433,\"start\":77432},{\"end\":77440,\"start\":77439},{\"end\":77448,\"start\":77447},{\"end\":77454,\"start\":77453},{\"end\":77460,\"start\":77459},{\"end\":77468,\"start\":77467},{\"end\":77476,\"start\":77475},{\"end\":77484,\"start\":77483},{\"end\":77784,\"start\":77783},{\"end\":77793,\"start\":77792},{\"end\":77813,\"start\":77812},{\"end\":77822,\"start\":77821},{\"end\":77829,\"start\":77828},{\"end\":78439,\"start\":78438},{\"end\":78448,\"start\":78447},{\"end\":78454,\"start\":78453},{\"end\":78470,\"start\":78469},{\"end\":78480,\"start\":78479},{\"end\":78972,\"start\":78971},{\"end\":78974,\"start\":78973},{\"end\":78984,\"start\":78983},{\"end\":78993,\"start\":78992},{\"end\":79002,\"start\":79001},{\"end\":79015,\"start\":79014},{\"end\":79026,\"start\":79025},{\"end\":79035,\"start\":79034},{\"end\":79044,\"start\":79043},{\"end\":79054,\"start\":79053},{\"end\":79063,\"start\":79062},{\"end\":79075,\"start\":79074},{\"end\":79084,\"start\":79083},{\"end\":79094,\"start\":79093},{\"end\":79102,\"start\":79101},{\"end\":79111,\"start\":79110},{\"end\":79122,\"start\":79121},{\"end\":79131,\"start\":79130},{\"end\":79138,\"start\":79137},{\"end\":79146,\"start\":79145},{\"end\":79154,\"start\":79153},{\"end\":79156,\"start\":79155},{\"end\":79172,\"start\":79171},{\"end\":79185,\"start\":79184},{\"end\":79196,\"start\":79195},{\"end\":79207,\"start\":79206},{\"end\":79209,\"start\":79208},{\"end\":79215,\"start\":79214},{\"end\":79226,\"start\":79225},{\"end\":79232,\"start\":79231},{\"end\":79241,\"start\":79240},{\"end\":79249,\"start\":79248},{\"end\":79258,\"start\":79257},{\"end\":79268,\"start\":79267},{\"end\":79280,\"start\":79279},{\"end\":79290,\"start\":79289},{\"end\":79301,\"start\":79300},{\"end\":79314,\"start\":79313},{\"end\":79322,\"start\":79321},{\"end\":79331,\"start\":79330},{\"end\":79339,\"start\":79338},{\"end\":79349,\"start\":79348},{\"end\":79356,\"start\":79355},{\"end\":79362,\"start\":79361},{\"end\":79371,\"start\":79370},{\"end\":79378,\"start\":79377},{\"end\":79387,\"start\":79386},{\"end\":79397,\"start\":79396},{\"end\":79408,\"start\":79407},{\"end\":79420,\"start\":79419},{\"end\":79430,\"start\":79429},{\"end\":79440,\"start\":79439},{\"end\":79453,\"start\":79452},{\"end\":79470,\"start\":79469},{\"end\":79476,\"start\":79475},{\"end\":79483,\"start\":79482},{\"end\":79493,\"start\":79492},{\"end\":79505,\"start\":79504},{\"end\":79518,\"start\":79517},{\"end\":79528,\"start\":79527},{\"end\":79536,\"start\":79535},{\"end\":79544,\"start\":79543},{\"end\":79553,\"start\":79552},{\"end\":79566,\"start\":79565},{\"end\":79576,\"start\":79575},{\"end\":79587,\"start\":79586},{\"end\":79598,\"start\":79597},{\"end\":79608,\"start\":79607},{\"end\":79621,\"start\":79620},{\"end\":79630,\"start\":79629},{\"end\":79637,\"start\":79636},{\"end\":79648,\"start\":79647},{\"end\":79656,\"start\":79655},{\"end\":79664,\"start\":79663},{\"end\":79674,\"start\":79673},{\"end\":79687,\"start\":79686},{\"end\":79697,\"start\":79696},{\"end\":79705,\"start\":79704},{\"end\":79715,\"start\":79714},{\"end\":79717,\"start\":79716},{\"end\":80900,\"start\":80899},{\"end\":80902,\"start\":80901},{\"end\":80912,\"start\":80911},{\"end\":80927,\"start\":80926},{\"end\":80935,\"start\":80934},{\"end\":80942,\"start\":80941},{\"end\":80950,\"start\":80949},{\"end\":81487,\"start\":81486},{\"end\":81494,\"start\":81493},{\"end\":81501,\"start\":81500},{\"end\":81510,\"start\":81509},{\"end\":81748,\"start\":81747},{\"end\":81750,\"start\":81749},{\"end\":81977,\"start\":81976},{\"end\":81989,\"start\":81988},{\"end\":81997,\"start\":81996},{\"end\":82381,\"start\":82380},{\"end\":82388,\"start\":82387},{\"end\":82395,\"start\":82394},{\"end\":82402,\"start\":82401},{\"end\":82404,\"start\":82403},{\"end\":82418,\"start\":82417},{\"end\":82843,\"start\":82842},{\"end\":82854,\"start\":82853},{\"end\":82867,\"start\":82866},{\"end\":82877,\"start\":82876},{\"end\":83422,\"start\":83421},{\"end\":83433,\"start\":83432},{\"end\":83435,\"start\":83434},{\"end\":83445,\"start\":83444},{\"end\":83454,\"start\":83453},{\"end\":83463,\"start\":83462},{\"end\":83475,\"start\":83474},{\"end\":83487,\"start\":83486},{\"end\":83898,\"start\":83897},{\"end\":83910,\"start\":83909},{\"end\":83912,\"start\":83911},{\"end\":83923,\"start\":83922},{\"end\":83925,\"start\":83924},{\"end\":83932,\"start\":83931},{\"end\":83934,\"start\":83933},{\"end\":83945,\"start\":83944},{\"end\":83953,\"start\":83952},{\"end\":83963,\"start\":83962},{\"end\":84593,\"start\":84592},{\"end\":84595,\"start\":84594},{\"end\":84605,\"start\":84604},{\"end\":84620,\"start\":84619},{\"end\":84631,\"start\":84630},{\"end\":84640,\"start\":84639},{\"end\":84656,\"start\":84655},{\"end\":85213,\"start\":85212},{\"end\":85227,\"start\":85226},{\"end\":85238,\"start\":85237},{\"end\":85249,\"start\":85248},{\"end\":85788,\"start\":85787},{\"end\":85796,\"start\":85795},{\"end\":85808,\"start\":85807},{\"end\":85820,\"start\":85819},{\"end\":86054,\"start\":86053},{\"end\":86056,\"start\":86055},{\"end\":86068,\"start\":86067},{\"end\":86074,\"start\":86073},{\"end\":86087,\"start\":86083},{\"end\":86097,\"start\":86096},{\"end\":86691,\"start\":86690},{\"end\":86699,\"start\":86698},{\"end\":86707,\"start\":86706},{\"end\":86714,\"start\":86713},{\"end\":86722,\"start\":86721},{\"end\":86730,\"start\":86729},{\"end\":86743,\"start\":86742},{\"end\":86760,\"start\":86759},{\"end\":87341,\"start\":87340},{\"end\":87348,\"start\":87347},{\"end\":87562,\"start\":87561},{\"end\":87569,\"start\":87568},{\"end\":87578,\"start\":87577},{\"end\":87584,\"start\":87583},{\"end\":87592,\"start\":87591},{\"end\":87601,\"start\":87600},{\"end\":87610,\"start\":87609},{\"end\":87618,\"start\":87617},{\"end\":87625,\"start\":87624},{\"end\":87632,\"start\":87631},{\"end\":87968,\"start\":87967},{\"end\":87979,\"start\":87978},{\"end\":87986,\"start\":87985},{\"end\":87994,\"start\":87993},{\"end\":88004,\"start\":88003},{\"end\":88022,\"start\":88021},{\"end\":88953,\"start\":88952},{\"end\":88969,\"start\":88968},{\"end\":88978,\"start\":88977},{\"end\":88998,\"start\":88997},{\"end\":89000,\"start\":88999},{\"end\":89012,\"start\":89011},{\"end\":89584,\"start\":89583},{\"end\":89600,\"start\":89599},{\"end\":89608,\"start\":89607},{\"end\":89620,\"start\":89619},{\"end\":89629,\"start\":89628},{\"end\":90107,\"start\":90106},{\"end\":90109,\"start\":90108},{\"end\":90119,\"start\":90118},{\"end\":90121,\"start\":90120},{\"end\":90663,\"start\":90662},{\"end\":90673,\"start\":90672},{\"end\":90682,\"start\":90681},{\"end\":90955,\"start\":90954},{\"end\":90963,\"start\":90962},{\"end\":90969,\"start\":90968},{\"end\":90978,\"start\":90977},{\"end\":90992,\"start\":90991},{\"end\":91395,\"start\":91394},{\"end\":91403,\"start\":91402},{\"end\":91411,\"start\":91410},{\"end\":91420,\"start\":91419},{\"end\":91428,\"start\":91427},{\"end\":91435,\"start\":91434},{\"end\":91443,\"start\":91442},{\"end\":91450,\"start\":91449},{\"end\":91457,\"start\":91456},{\"end\":91463,\"start\":91462},{\"end\":91470,\"start\":91469},{\"end\":91478,\"start\":91477},{\"end\":91484,\"start\":91483},{\"end\":91833,\"start\":91832},{\"end\":91839,\"start\":91838},{\"end\":91846,\"start\":91845},{\"end\":91855,\"start\":91854},{\"end\":91862,\"start\":91861},{\"end\":91871,\"start\":91870},{\"end\":91885,\"start\":91884},{\"end\":92308,\"start\":92307},{\"end\":92310,\"start\":92309},{\"end\":92323,\"start\":92322},{\"end\":92343,\"start\":92342},{\"end\":92863,\"start\":92862},{\"end\":92865,\"start\":92864},{\"end\":92878,\"start\":92877},{\"end\":92891,\"start\":92890},{\"end\":92893,\"start\":92892},{\"end\":92905,\"start\":92904},{\"end\":92914,\"start\":92913},{\"end\":92916,\"start\":92915},{\"end\":93348,\"start\":93347},{\"end\":93359,\"start\":93358},{\"end\":93372,\"start\":93371},{\"end\":93374,\"start\":93373},{\"end\":93386,\"start\":93385},{\"end\":93397,\"start\":93396},{\"end\":93902,\"start\":93901},{\"end\":93911,\"start\":93910},{\"end\":93931,\"start\":93930},{\"end\":93940,\"start\":93939},{\"end\":93953,\"start\":93952},{\"end\":93961,\"start\":93960},{\"end\":93963,\"start\":93962}]", "bib_author_last_name": "[{\"end\":65186,\"start\":65181},{\"end\":65705,\"start\":65700},{\"end\":65715,\"start\":65709},{\"end\":65723,\"start\":65719},{\"end\":65731,\"start\":65727},{\"end\":65740,\"start\":65735},{\"end\":65748,\"start\":65744},{\"end\":65757,\"start\":65752},{\"end\":65769,\"start\":65761},{\"end\":65779,\"start\":65773},{\"end\":65788,\"start\":65783},{\"end\":66392,\"start\":66387},{\"end\":66403,\"start\":66396},{\"end\":66411,\"start\":66407},{\"end\":66422,\"start\":66415},{\"end\":66718,\"start\":66707},{\"end\":66727,\"start\":66724},{\"end\":66735,\"start\":66731},{\"end\":66751,\"start\":66739},{\"end\":66757,\"start\":66755},{\"end\":66766,\"start\":66761},{\"end\":67193,\"start\":67191},{\"end\":67345,\"start\":67329},{\"end\":67353,\"start\":67349},{\"end\":67362,\"start\":67357},{\"end\":67378,\"start\":67366},{\"end\":67391,\"start\":67382},{\"end\":67407,\"start\":67395},{\"end\":67416,\"start\":67411},{\"end\":67699,\"start\":67683},{\"end\":67709,\"start\":67703},{\"end\":67720,\"start\":67713},{\"end\":67730,\"start\":67724},{\"end\":68152,\"start\":68150},{\"end\":68166,\"start\":68156},{\"end\":68179,\"start\":68172},{\"end\":68189,\"start\":68183},{\"end\":68898,\"start\":68885},{\"end\":68911,\"start\":68902},{\"end\":68922,\"start\":68915},{\"end\":68930,\"start\":68926},{\"end\":68944,\"start\":68934},{\"end\":69440,\"start\":69433},{\"end\":69595,\"start\":69586},{\"end\":69605,\"start\":69599},{\"end\":69933,\"start\":69924},{\"end\":69944,\"start\":69940},{\"end\":69950,\"start\":69948},{\"end\":69959,\"start\":69954},{\"end\":69969,\"start\":69963},{\"end\":69981,\"start\":69973},{\"end\":69992,\"start\":69985},{\"end\":70548,\"start\":70539},{\"end\":70556,\"start\":70552},{\"end\":70564,\"start\":70560},{\"end\":70578,\"start\":70570},{\"end\":70592,\"start\":70584},{\"end\":70602,\"start\":70596},{\"end\":71055,\"start\":71051},{\"end\":71067,\"start\":71059},{\"end\":71896,\"start\":71888},{\"end\":71905,\"start\":71900},{\"end\":71916,\"start\":71909},{\"end\":71926,\"start\":71920},{\"end\":71937,\"start\":71930},{\"end\":72427,\"start\":72421},{\"end\":72576,\"start\":72569},{\"end\":72586,\"start\":72580},{\"end\":72594,\"start\":72590},{\"end\":72600,\"start\":72598},{\"end\":72614,\"start\":72604},{\"end\":72627,\"start\":72618},{\"end\":72638,\"start\":72631},{\"end\":72650,\"start\":72642},{\"end\":73289,\"start\":73282},{\"end\":73300,\"start\":73295},{\"end\":73313,\"start\":73304},{\"end\":73334,\"start\":73319},{\"end\":73342,\"start\":73338},{\"end\":73354,\"start\":73346},{\"end\":73367,\"start\":73358},{\"end\":73377,\"start\":73371},{\"end\":73389,\"start\":73381},{\"end\":73997,\"start\":73990},{\"end\":74005,\"start\":74001},{\"end\":74271,\"start\":74265},{\"end\":74281,\"start\":74275},{\"end\":74290,\"start\":74285},{\"end\":74298,\"start\":74294},{\"end\":74310,\"start\":74302},{\"end\":74323,\"start\":74314},{\"end\":74630,\"start\":74627},{\"end\":74645,\"start\":74634},{\"end\":74655,\"start\":74649},{\"end\":74670,\"start\":74661},{\"end\":74681,\"start\":74674},{\"end\":74689,\"start\":74685},{\"end\":75250,\"start\":75239},{\"end\":75264,\"start\":75254},{\"end\":75280,\"start\":75270},{\"end\":75294,\"start\":75286},{\"end\":75303,\"start\":75300},{\"end\":75889,\"start\":75880},{\"end\":75897,\"start\":75893},{\"end\":76158,\"start\":76153},{\"end\":76178,\"start\":76162},{\"end\":76187,\"start\":76182},{\"end\":76203,\"start\":76191},{\"end\":76211,\"start\":76207},{\"end\":76224,\"start\":76215},{\"end\":76237,\"start\":76230},{\"end\":76246,\"start\":76241},{\"end\":76762,\"start\":76757},{\"end\":76771,\"start\":76766},{\"end\":76777,\"start\":76775},{\"end\":76788,\"start\":76781},{\"end\":76800,\"start\":76792},{\"end\":77421,\"start\":77416},{\"end\":77430,\"start\":77425},{\"end\":77437,\"start\":77434},{\"end\":77445,\"start\":77441},{\"end\":77451,\"start\":77449},{\"end\":77457,\"start\":77455},{\"end\":77465,\"start\":77461},{\"end\":77473,\"start\":77469},{\"end\":77481,\"start\":77477},{\"end\":77489,\"start\":77485},{\"end\":77790,\"start\":77785},{\"end\":77810,\"start\":77794},{\"end\":77819,\"start\":77814},{\"end\":77826,\"start\":77823},{\"end\":77836,\"start\":77830},{\"end\":78445,\"start\":78440},{\"end\":78451,\"start\":78449},{\"end\":78467,\"start\":78455},{\"end\":78477,\"start\":78471},{\"end\":78486,\"start\":78481},{\"end\":78981,\"start\":78975},{\"end\":78990,\"start\":78985},{\"end\":78999,\"start\":78994},{\"end\":79012,\"start\":79003},{\"end\":79023,\"start\":79016},{\"end\":79032,\"start\":79027},{\"end\":79041,\"start\":79036},{\"end\":79051,\"start\":79045},{\"end\":79060,\"start\":79055},{\"end\":79072,\"start\":79064},{\"end\":79081,\"start\":79076},{\"end\":79091,\"start\":79085},{\"end\":79099,\"start\":79095},{\"end\":79108,\"start\":79103},{\"end\":79119,\"start\":79112},{\"end\":79128,\"start\":79123},{\"end\":79135,\"start\":79132},{\"end\":79143,\"start\":79139},{\"end\":79151,\"start\":79147},{\"end\":79169,\"start\":79157},{\"end\":79182,\"start\":79173},{\"end\":79193,\"start\":79186},{\"end\":79204,\"start\":79197},{\"end\":79212,\"start\":79210},{\"end\":79223,\"start\":79216},{\"end\":79229,\"start\":79227},{\"end\":79238,\"start\":79233},{\"end\":79246,\"start\":79242},{\"end\":79255,\"start\":79250},{\"end\":79265,\"start\":79259},{\"end\":79277,\"start\":79269},{\"end\":79287,\"start\":79281},{\"end\":79298,\"start\":79291},{\"end\":79311,\"start\":79302},{\"end\":79319,\"start\":79315},{\"end\":79328,\"start\":79323},{\"end\":79336,\"start\":79332},{\"end\":79346,\"start\":79340},{\"end\":79353,\"start\":79350},{\"end\":79359,\"start\":79357},{\"end\":79368,\"start\":79363},{\"end\":79375,\"start\":79372},{\"end\":79384,\"start\":79379},{\"end\":79394,\"start\":79388},{\"end\":79405,\"start\":79398},{\"end\":79417,\"start\":79409},{\"end\":79427,\"start\":79421},{\"end\":79437,\"start\":79431},{\"end\":79450,\"start\":79441},{\"end\":79467,\"start\":79454},{\"end\":79473,\"start\":79471},{\"end\":79480,\"start\":79477},{\"end\":79490,\"start\":79484},{\"end\":79502,\"start\":79494},{\"end\":79515,\"start\":79506},{\"end\":79525,\"start\":79519},{\"end\":79533,\"start\":79529},{\"end\":79541,\"start\":79537},{\"end\":79550,\"start\":79545},{\"end\":79563,\"start\":79554},{\"end\":79573,\"start\":79567},{\"end\":79584,\"start\":79577},{\"end\":79595,\"start\":79588},{\"end\":79605,\"start\":79599},{\"end\":79618,\"start\":79609},{\"end\":79627,\"start\":79622},{\"end\":79634,\"start\":79631},{\"end\":79645,\"start\":79638},{\"end\":79653,\"start\":79649},{\"end\":79661,\"start\":79657},{\"end\":79671,\"start\":79665},{\"end\":79684,\"start\":79675},{\"end\":79694,\"start\":79688},{\"end\":79702,\"start\":79698},{\"end\":79712,\"start\":79706},{\"end\":79722,\"start\":79718},{\"end\":80909,\"start\":80903},{\"end\":80924,\"start\":80913},{\"end\":80932,\"start\":80928},{\"end\":80939,\"start\":80936},{\"end\":80947,\"start\":80943},{\"end\":80955,\"start\":80951},{\"end\":81491,\"start\":81488},{\"end\":81498,\"start\":81495},{\"end\":81507,\"start\":81502},{\"end\":81519,\"start\":81511},{\"end\":81755,\"start\":81751},{\"end\":81986,\"start\":81978},{\"end\":81994,\"start\":81990},{\"end\":82007,\"start\":81998},{\"end\":82385,\"start\":82382},{\"end\":82392,\"start\":82389},{\"end\":82399,\"start\":82396},{\"end\":82415,\"start\":82405},{\"end\":82428,\"start\":82419},{\"end\":82851,\"start\":82844},{\"end\":82864,\"start\":82855},{\"end\":82874,\"start\":82868},{\"end\":82884,\"start\":82878},{\"end\":83430,\"start\":83423},{\"end\":83442,\"start\":83436},{\"end\":83451,\"start\":83446},{\"end\":83460,\"start\":83455},{\"end\":83472,\"start\":83464},{\"end\":83484,\"start\":83476},{\"end\":83494,\"start\":83488},{\"end\":83907,\"start\":83899},{\"end\":83920,\"start\":83913},{\"end\":83929,\"start\":83926},{\"end\":83942,\"start\":83935},{\"end\":83950,\"start\":83946},{\"end\":83960,\"start\":83954},{\"end\":83974,\"start\":83964},{\"end\":84602,\"start\":84596},{\"end\":84617,\"start\":84606},{\"end\":84628,\"start\":84621},{\"end\":84637,\"start\":84632},{\"end\":84653,\"start\":84641},{\"end\":84661,\"start\":84657},{\"end\":85224,\"start\":85214},{\"end\":85235,\"start\":85228},{\"end\":85246,\"start\":85239},{\"end\":85256,\"start\":85250},{\"end\":85793,\"start\":85789},{\"end\":85805,\"start\":85797},{\"end\":85817,\"start\":85809},{\"end\":85831,\"start\":85821},{\"end\":86065,\"start\":86057},{\"end\":86071,\"start\":86069},{\"end\":86081,\"start\":86075},{\"end\":86094,\"start\":86088},{\"end\":86106,\"start\":86098},{\"end\":86696,\"start\":86692},{\"end\":86704,\"start\":86700},{\"end\":86711,\"start\":86708},{\"end\":86719,\"start\":86715},{\"end\":86727,\"start\":86723},{\"end\":86740,\"start\":86731},{\"end\":86757,\"start\":86744},{\"end\":86769,\"start\":86761},{\"end\":87345,\"start\":87342},{\"end\":87351,\"start\":87349},{\"end\":87566,\"start\":87563},{\"end\":87575,\"start\":87570},{\"end\":87581,\"start\":87579},{\"end\":87589,\"start\":87585},{\"end\":87598,\"start\":87593},{\"end\":87607,\"start\":87602},{\"end\":87615,\"start\":87611},{\"end\":87622,\"start\":87619},{\"end\":87629,\"start\":87626},{\"end\":87638,\"start\":87633},{\"end\":87976,\"start\":87969},{\"end\":87983,\"start\":87980},{\"end\":87991,\"start\":87987},{\"end\":88001,\"start\":87995},{\"end\":88019,\"start\":88005},{\"end\":88029,\"start\":88023},{\"end\":88966,\"start\":88954},{\"end\":88975,\"start\":88970},{\"end\":88995,\"start\":88979},{\"end\":89009,\"start\":89001},{\"end\":89019,\"start\":89013},{\"end\":89597,\"start\":89585},{\"end\":89605,\"start\":89601},{\"end\":89617,\"start\":89609},{\"end\":89626,\"start\":89621},{\"end\":89636,\"start\":89630},{\"end\":90116,\"start\":90110},{\"end\":90130,\"start\":90122},{\"end\":90670,\"start\":90664},{\"end\":90679,\"start\":90674},{\"end\":90692,\"start\":90683},{\"end\":90960,\"start\":90956},{\"end\":90966,\"start\":90964},{\"end\":90975,\"start\":90970},{\"end\":90989,\"start\":90979},{\"end\":90998,\"start\":90993},{\"end\":91400,\"start\":91396},{\"end\":91408,\"start\":91404},{\"end\":91417,\"start\":91412},{\"end\":91425,\"start\":91421},{\"end\":91432,\"start\":91429},{\"end\":91440,\"start\":91436},{\"end\":91447,\"start\":91444},{\"end\":91454,\"start\":91451},{\"end\":91460,\"start\":91458},{\"end\":91467,\"start\":91464},{\"end\":91475,\"start\":91471},{\"end\":91481,\"start\":91479},{\"end\":91489,\"start\":91485},{\"end\":91836,\"start\":91834},{\"end\":91843,\"start\":91840},{\"end\":91852,\"start\":91847},{\"end\":91859,\"start\":91856},{\"end\":91868,\"start\":91863},{\"end\":91882,\"start\":91872},{\"end\":91892,\"start\":91886},{\"end\":92320,\"start\":92311},{\"end\":92340,\"start\":92324},{\"end\":92348,\"start\":92344},{\"end\":92875,\"start\":92866},{\"end\":92888,\"start\":92879},{\"end\":92902,\"start\":92894},{\"end\":92911,\"start\":92906},{\"end\":92921,\"start\":92917},{\"end\":93356,\"start\":93349},{\"end\":93369,\"start\":93360},{\"end\":93383,\"start\":93375},{\"end\":93394,\"start\":93387},{\"end\":93404,\"start\":93398},{\"end\":93908,\"start\":93903},{\"end\":93928,\"start\":93912},{\"end\":93937,\"start\":93932},{\"end\":93950,\"start\":93941},{\"end\":93958,\"start\":93954},{\"end\":93972,\"start\":93964}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":64501,\"start\":64385},{\"attributes\":{\"id\":\"b1\"},\"end\":64564,\"start\":64503},{\"attributes\":{\"id\":\"b2\"},\"end\":64679,\"start\":64566},{\"attributes\":{\"id\":\"b3\"},\"end\":64776,\"start\":64681},{\"attributes\":{\"id\":\"b4\"},\"end\":64985,\"start\":64778},{\"attributes\":{\"id\":\"b5\"},\"end\":65170,\"start\":64987},{\"attributes\":{\"id\":\"b6\"},\"end\":65234,\"start\":65172},{\"attributes\":{\"id\":\"b7\"},\"end\":65325,\"start\":65236},{\"attributes\":{\"id\":\"b8\"},\"end\":65455,\"start\":65327},{\"attributes\":{\"id\":\"b9\"},\"end\":65554,\"start\":65457},{\"attributes\":{\"id\":\"b10\"},\"end\":65641,\"start\":65556},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6287870},\"end\":66306,\"start\":65643},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7401659},\"end\":66615,\"start\":66308},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":699198},\"end\":67180,\"start\":66617},{\"attributes\":{\"id\":\"b14\"},\"end\":67263,\"start\":67182},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206449115},\"end\":67625,\"start\":67265},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2918327},\"end\":68095,\"start\":67627},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52251440},\"end\":68596,\"start\":68097},{\"attributes\":{\"id\":\"b18\"},\"end\":68816,\"start\":68598},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4952781},\"end\":69371,\"start\":68818},{\"attributes\":{\"id\":\"b20\"},\"end\":69517,\"start\":69373},{\"attributes\":{\"id\":\"b21\"},\"end\":69836,\"start\":69519},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":222296313},\"end\":70478,\"start\":69838},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1701442},\"end\":70990,\"start\":70480},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":214797870},\"end\":71503,\"start\":70992},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53731273},\"end\":71824,\"start\":71505},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":207193689},\"end\":72417,\"start\":71826},{\"attributes\":{\"id\":\"b27\"},\"end\":72467,\"start\":72419},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":196809746},\"end\":73191,\"start\":72469},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10061486},\"end\":73952,\"start\":73193},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":222417405},\"end\":74201,\"start\":73954},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":53113585},\"end\":74559,\"start\":74203},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15545222},\"end\":75168,\"start\":74561},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":224821159},\"end\":75797,\"start\":75170},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1152740},\"end\":76081,\"start\":75799},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":31635004},\"end\":76678,\"start\":76083},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1428008},\"end\":77347,\"start\":76680},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3779750},\"end\":77730,\"start\":77349},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":51766446},\"end\":78383,\"start\":77732},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":854354},\"end\":78905,\"start\":78385},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4202768},\"end\":80820,\"start\":78907},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":85519074},\"end\":81406,\"start\":80822},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":53060190},\"end\":81708,\"start\":81408},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6155330},\"end\":81903,\"start\":81710},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":49571402},\"end\":82324,\"start\":81905},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":211252248},\"end\":82802,\"start\":82326},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":52987126},\"end\":83324,\"start\":82804},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":220835221},\"end\":83844,\"start\":83326},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":53034533},\"end\":84518,\"start\":83846},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":16966186},\"end\":85164,\"start\":84520},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":10529766},\"end\":85741,\"start\":85166},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":29155614},\"end\":85990,\"start\":85743},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":10480605},\"end\":86617,\"start\":85992},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":204812163},\"end\":87293,\"start\":86619},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":174802790},\"end\":87490,\"start\":87295},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":197626303},\"end\":87873,\"start\":87492},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":15136963},\"end\":88896,\"start\":87875},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":592103},\"end\":89504,\"start\":88898},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":14366444},\"end\":90039,\"start\":89506},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":6135094},\"end\":90630,\"start\":90041},{\"attributes\":{\"id\":\"b60\"},\"end\":90899,\"start\":90632},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":51870894},\"end\":91310,\"start\":90901},{\"attributes\":{\"id\":\"b62\"},\"end\":91753,\"start\":91312},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":4938579},\"end\":92244,\"start\":91755},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":5634157},\"end\":92763,\"start\":92246},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":12220995},\"end\":93301,\"start\":92765},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":11818928},\"end\":93829,\"start\":93303},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":11894138},\"end\":94376,\"start\":93831}]", "bib_title": "[{\"end\":65696,\"start\":65643},{\"end\":66383,\"start\":66308},{\"end\":66703,\"start\":66617},{\"end\":67325,\"start\":67265},{\"end\":67679,\"start\":67627},{\"end\":68146,\"start\":68097},{\"end\":68881,\"start\":68818},{\"end\":69582,\"start\":69519},{\"end\":69920,\"start\":69838},{\"end\":70535,\"start\":70480},{\"end\":71047,\"start\":70992},{\"end\":71592,\"start\":71505},{\"end\":71882,\"start\":71826},{\"end\":72565,\"start\":72469},{\"end\":73278,\"start\":73193},{\"end\":73986,\"start\":73954},{\"end\":74261,\"start\":74203},{\"end\":74623,\"start\":74561},{\"end\":75233,\"start\":75170},{\"end\":75876,\"start\":75799},{\"end\":76149,\"start\":76083},{\"end\":76753,\"start\":76680},{\"end\":77412,\"start\":77349},{\"end\":77781,\"start\":77732},{\"end\":78436,\"start\":78385},{\"end\":78969,\"start\":78907},{\"end\":80897,\"start\":80822},{\"end\":81484,\"start\":81408},{\"end\":81745,\"start\":81710},{\"end\":81974,\"start\":81905},{\"end\":82378,\"start\":82326},{\"end\":82840,\"start\":82804},{\"end\":83419,\"start\":83326},{\"end\":83895,\"start\":83846},{\"end\":84590,\"start\":84520},{\"end\":85210,\"start\":85166},{\"end\":85785,\"start\":85743},{\"end\":86051,\"start\":85992},{\"end\":86688,\"start\":86619},{\"end\":87338,\"start\":87295},{\"end\":87559,\"start\":87492},{\"end\":87965,\"start\":87875},{\"end\":88950,\"start\":88898},{\"end\":89581,\"start\":89506},{\"end\":90104,\"start\":90041},{\"end\":90660,\"start\":90632},{\"end\":90952,\"start\":90901},{\"end\":91392,\"start\":91312},{\"end\":91830,\"start\":91755},{\"end\":92305,\"start\":92246},{\"end\":92860,\"start\":92765},{\"end\":93345,\"start\":93303},{\"end\":93899,\"start\":93831}]", "bib_author": "[{\"end\":65188,\"start\":65174},{\"end\":65707,\"start\":65698},{\"end\":65717,\"start\":65707},{\"end\":65725,\"start\":65717},{\"end\":65733,\"start\":65725},{\"end\":65742,\"start\":65733},{\"end\":65750,\"start\":65742},{\"end\":65759,\"start\":65750},{\"end\":65771,\"start\":65759},{\"end\":65781,\"start\":65771},{\"end\":65790,\"start\":65781},{\"end\":66394,\"start\":66385},{\"end\":66405,\"start\":66394},{\"end\":66413,\"start\":66405},{\"end\":66424,\"start\":66413},{\"end\":66720,\"start\":66705},{\"end\":66729,\"start\":66720},{\"end\":66737,\"start\":66729},{\"end\":66753,\"start\":66737},{\"end\":66759,\"start\":66753},{\"end\":66768,\"start\":66759},{\"end\":67195,\"start\":67184},{\"end\":67347,\"start\":67327},{\"end\":67355,\"start\":67347},{\"end\":67364,\"start\":67355},{\"end\":67380,\"start\":67364},{\"end\":67393,\"start\":67380},{\"end\":67409,\"start\":67393},{\"end\":67418,\"start\":67409},{\"end\":67701,\"start\":67681},{\"end\":67711,\"start\":67701},{\"end\":67722,\"start\":67711},{\"end\":67732,\"start\":67722},{\"end\":68154,\"start\":68148},{\"end\":68168,\"start\":68154},{\"end\":68181,\"start\":68168},{\"end\":68191,\"start\":68181},{\"end\":68900,\"start\":68883},{\"end\":68913,\"start\":68900},{\"end\":68924,\"start\":68913},{\"end\":68932,\"start\":68924},{\"end\":68946,\"start\":68932},{\"end\":69442,\"start\":69431},{\"end\":69597,\"start\":69584},{\"end\":69607,\"start\":69597},{\"end\":69935,\"start\":69922},{\"end\":69946,\"start\":69935},{\"end\":69952,\"start\":69946},{\"end\":69961,\"start\":69952},{\"end\":69971,\"start\":69961},{\"end\":69983,\"start\":69971},{\"end\":69994,\"start\":69983},{\"end\":70550,\"start\":70537},{\"end\":70558,\"start\":70550},{\"end\":70566,\"start\":70558},{\"end\":70580,\"start\":70566},{\"end\":70594,\"start\":70580},{\"end\":70604,\"start\":70594},{\"end\":71057,\"start\":71049},{\"end\":71069,\"start\":71057},{\"end\":71598,\"start\":71594},{\"end\":71898,\"start\":71884},{\"end\":71907,\"start\":71898},{\"end\":71918,\"start\":71907},{\"end\":71928,\"start\":71918},{\"end\":71939,\"start\":71928},{\"end\":72429,\"start\":72421},{\"end\":72578,\"start\":72567},{\"end\":72588,\"start\":72578},{\"end\":72596,\"start\":72588},{\"end\":72602,\"start\":72596},{\"end\":72616,\"start\":72602},{\"end\":72629,\"start\":72616},{\"end\":72640,\"start\":72629},{\"end\":72652,\"start\":72640},{\"end\":73291,\"start\":73280},{\"end\":73302,\"start\":73291},{\"end\":73315,\"start\":73302},{\"end\":73336,\"start\":73315},{\"end\":73344,\"start\":73336},{\"end\":73356,\"start\":73344},{\"end\":73369,\"start\":73356},{\"end\":73379,\"start\":73369},{\"end\":73391,\"start\":73379},{\"end\":73999,\"start\":73988},{\"end\":74007,\"start\":73999},{\"end\":74273,\"start\":74263},{\"end\":74283,\"start\":74273},{\"end\":74292,\"start\":74283},{\"end\":74300,\"start\":74292},{\"end\":74312,\"start\":74300},{\"end\":74325,\"start\":74312},{\"end\":74632,\"start\":74625},{\"end\":74647,\"start\":74632},{\"end\":74657,\"start\":74647},{\"end\":74672,\"start\":74657},{\"end\":74683,\"start\":74672},{\"end\":74691,\"start\":74683},{\"end\":75252,\"start\":75235},{\"end\":75266,\"start\":75252},{\"end\":75282,\"start\":75266},{\"end\":75296,\"start\":75282},{\"end\":75305,\"start\":75296},{\"end\":75891,\"start\":75878},{\"end\":75899,\"start\":75891},{\"end\":76160,\"start\":76151},{\"end\":76180,\"start\":76160},{\"end\":76189,\"start\":76180},{\"end\":76205,\"start\":76189},{\"end\":76213,\"start\":76205},{\"end\":76226,\"start\":76213},{\"end\":76239,\"start\":76226},{\"end\":76248,\"start\":76239},{\"end\":76764,\"start\":76755},{\"end\":76773,\"start\":76764},{\"end\":76779,\"start\":76773},{\"end\":76790,\"start\":76779},{\"end\":76802,\"start\":76790},{\"end\":77423,\"start\":77414},{\"end\":77432,\"start\":77423},{\"end\":77439,\"start\":77432},{\"end\":77447,\"start\":77439},{\"end\":77453,\"start\":77447},{\"end\":77459,\"start\":77453},{\"end\":77467,\"start\":77459},{\"end\":77475,\"start\":77467},{\"end\":77483,\"start\":77475},{\"end\":77491,\"start\":77483},{\"end\":77792,\"start\":77783},{\"end\":77812,\"start\":77792},{\"end\":77821,\"start\":77812},{\"end\":77828,\"start\":77821},{\"end\":77838,\"start\":77828},{\"end\":78447,\"start\":78438},{\"end\":78453,\"start\":78447},{\"end\":78469,\"start\":78453},{\"end\":78479,\"start\":78469},{\"end\":78488,\"start\":78479},{\"end\":78983,\"start\":78971},{\"end\":78992,\"start\":78983},{\"end\":79001,\"start\":78992},{\"end\":79014,\"start\":79001},{\"end\":79025,\"start\":79014},{\"end\":79034,\"start\":79025},{\"end\":79043,\"start\":79034},{\"end\":79053,\"start\":79043},{\"end\":79062,\"start\":79053},{\"end\":79074,\"start\":79062},{\"end\":79083,\"start\":79074},{\"end\":79093,\"start\":79083},{\"end\":79101,\"start\":79093},{\"end\":79110,\"start\":79101},{\"end\":79121,\"start\":79110},{\"end\":79130,\"start\":79121},{\"end\":79137,\"start\":79130},{\"end\":79145,\"start\":79137},{\"end\":79153,\"start\":79145},{\"end\":79171,\"start\":79153},{\"end\":79184,\"start\":79171},{\"end\":79195,\"start\":79184},{\"end\":79206,\"start\":79195},{\"end\":79214,\"start\":79206},{\"end\":79225,\"start\":79214},{\"end\":79231,\"start\":79225},{\"end\":79240,\"start\":79231},{\"end\":79248,\"start\":79240},{\"end\":79257,\"start\":79248},{\"end\":79267,\"start\":79257},{\"end\":79279,\"start\":79267},{\"end\":79289,\"start\":79279},{\"end\":79300,\"start\":79289},{\"end\":79313,\"start\":79300},{\"end\":79321,\"start\":79313},{\"end\":79330,\"start\":79321},{\"end\":79338,\"start\":79330},{\"end\":79348,\"start\":79338},{\"end\":79355,\"start\":79348},{\"end\":79361,\"start\":79355},{\"end\":79370,\"start\":79361},{\"end\":79377,\"start\":79370},{\"end\":79386,\"start\":79377},{\"end\":79396,\"start\":79386},{\"end\":79407,\"start\":79396},{\"end\":79419,\"start\":79407},{\"end\":79429,\"start\":79419},{\"end\":79439,\"start\":79429},{\"end\":79452,\"start\":79439},{\"end\":79469,\"start\":79452},{\"end\":79475,\"start\":79469},{\"end\":79482,\"start\":79475},{\"end\":79492,\"start\":79482},{\"end\":79504,\"start\":79492},{\"end\":79517,\"start\":79504},{\"end\":79527,\"start\":79517},{\"end\":79535,\"start\":79527},{\"end\":79543,\"start\":79535},{\"end\":79552,\"start\":79543},{\"end\":79565,\"start\":79552},{\"end\":79575,\"start\":79565},{\"end\":79586,\"start\":79575},{\"end\":79597,\"start\":79586},{\"end\":79607,\"start\":79597},{\"end\":79620,\"start\":79607},{\"end\":79629,\"start\":79620},{\"end\":79636,\"start\":79629},{\"end\":79647,\"start\":79636},{\"end\":79655,\"start\":79647},{\"end\":79663,\"start\":79655},{\"end\":79673,\"start\":79663},{\"end\":79686,\"start\":79673},{\"end\":79696,\"start\":79686},{\"end\":79704,\"start\":79696},{\"end\":79714,\"start\":79704},{\"end\":79724,\"start\":79714},{\"end\":80911,\"start\":80899},{\"end\":80926,\"start\":80911},{\"end\":80934,\"start\":80926},{\"end\":80941,\"start\":80934},{\"end\":80949,\"start\":80941},{\"end\":80957,\"start\":80949},{\"end\":81493,\"start\":81486},{\"end\":81500,\"start\":81493},{\"end\":81509,\"start\":81500},{\"end\":81521,\"start\":81509},{\"end\":81757,\"start\":81747},{\"end\":81988,\"start\":81976},{\"end\":81996,\"start\":81988},{\"end\":82009,\"start\":81996},{\"end\":82387,\"start\":82380},{\"end\":82394,\"start\":82387},{\"end\":82401,\"start\":82394},{\"end\":82417,\"start\":82401},{\"end\":82430,\"start\":82417},{\"end\":82853,\"start\":82842},{\"end\":82866,\"start\":82853},{\"end\":82876,\"start\":82866},{\"end\":82886,\"start\":82876},{\"end\":83432,\"start\":83421},{\"end\":83444,\"start\":83432},{\"end\":83453,\"start\":83444},{\"end\":83462,\"start\":83453},{\"end\":83474,\"start\":83462},{\"end\":83486,\"start\":83474},{\"end\":83496,\"start\":83486},{\"end\":83909,\"start\":83897},{\"end\":83922,\"start\":83909},{\"end\":83931,\"start\":83922},{\"end\":83944,\"start\":83931},{\"end\":83952,\"start\":83944},{\"end\":83962,\"start\":83952},{\"end\":83976,\"start\":83962},{\"end\":84604,\"start\":84592},{\"end\":84619,\"start\":84604},{\"end\":84630,\"start\":84619},{\"end\":84639,\"start\":84630},{\"end\":84655,\"start\":84639},{\"end\":84663,\"start\":84655},{\"end\":85226,\"start\":85212},{\"end\":85237,\"start\":85226},{\"end\":85248,\"start\":85237},{\"end\":85258,\"start\":85248},{\"end\":85795,\"start\":85787},{\"end\":85807,\"start\":85795},{\"end\":85819,\"start\":85807},{\"end\":85833,\"start\":85819},{\"end\":86067,\"start\":86053},{\"end\":86073,\"start\":86067},{\"end\":86083,\"start\":86073},{\"end\":86096,\"start\":86083},{\"end\":86108,\"start\":86096},{\"end\":86698,\"start\":86690},{\"end\":86706,\"start\":86698},{\"end\":86713,\"start\":86706},{\"end\":86721,\"start\":86713},{\"end\":86729,\"start\":86721},{\"end\":86742,\"start\":86729},{\"end\":86759,\"start\":86742},{\"end\":86771,\"start\":86759},{\"end\":87347,\"start\":87340},{\"end\":87353,\"start\":87347},{\"end\":87568,\"start\":87561},{\"end\":87577,\"start\":87568},{\"end\":87583,\"start\":87577},{\"end\":87591,\"start\":87583},{\"end\":87600,\"start\":87591},{\"end\":87609,\"start\":87600},{\"end\":87617,\"start\":87609},{\"end\":87624,\"start\":87617},{\"end\":87631,\"start\":87624},{\"end\":87640,\"start\":87631},{\"end\":87978,\"start\":87967},{\"end\":87985,\"start\":87978},{\"end\":87993,\"start\":87985},{\"end\":88003,\"start\":87993},{\"end\":88021,\"start\":88003},{\"end\":88031,\"start\":88021},{\"end\":88968,\"start\":88952},{\"end\":88977,\"start\":88968},{\"end\":88997,\"start\":88977},{\"end\":89011,\"start\":88997},{\"end\":89021,\"start\":89011},{\"end\":89599,\"start\":89583},{\"end\":89607,\"start\":89599},{\"end\":89619,\"start\":89607},{\"end\":89628,\"start\":89619},{\"end\":89638,\"start\":89628},{\"end\":90118,\"start\":90106},{\"end\":90132,\"start\":90118},{\"end\":90672,\"start\":90662},{\"end\":90681,\"start\":90672},{\"end\":90694,\"start\":90681},{\"end\":90962,\"start\":90954},{\"end\":90968,\"start\":90962},{\"end\":90977,\"start\":90968},{\"end\":90991,\"start\":90977},{\"end\":91000,\"start\":90991},{\"end\":91402,\"start\":91394},{\"end\":91410,\"start\":91402},{\"end\":91419,\"start\":91410},{\"end\":91427,\"start\":91419},{\"end\":91434,\"start\":91427},{\"end\":91442,\"start\":91434},{\"end\":91449,\"start\":91442},{\"end\":91456,\"start\":91449},{\"end\":91462,\"start\":91456},{\"end\":91469,\"start\":91462},{\"end\":91477,\"start\":91469},{\"end\":91483,\"start\":91477},{\"end\":91491,\"start\":91483},{\"end\":91838,\"start\":91832},{\"end\":91845,\"start\":91838},{\"end\":91854,\"start\":91845},{\"end\":91861,\"start\":91854},{\"end\":91870,\"start\":91861},{\"end\":91884,\"start\":91870},{\"end\":91894,\"start\":91884},{\"end\":92322,\"start\":92307},{\"end\":92342,\"start\":92322},{\"end\":92350,\"start\":92342},{\"end\":92877,\"start\":92862},{\"end\":92890,\"start\":92877},{\"end\":92904,\"start\":92890},{\"end\":92913,\"start\":92904},{\"end\":92923,\"start\":92913},{\"end\":93358,\"start\":93347},{\"end\":93371,\"start\":93358},{\"end\":93385,\"start\":93371},{\"end\":93396,\"start\":93385},{\"end\":93406,\"start\":93396},{\"end\":93910,\"start\":93901},{\"end\":93930,\"start\":93910},{\"end\":93939,\"start\":93930},{\"end\":93952,\"start\":93939},{\"end\":93960,\"start\":93952},{\"end\":93974,\"start\":93960}]", "bib_venue": "[{\"end\":65974,\"start\":65889},{\"end\":66858,\"start\":66848},{\"end\":67823,\"start\":67812},{\"end\":68317,\"start\":68254},{\"end\":69072,\"start\":69009},{\"end\":70130,\"start\":70062},{\"end\":70694,\"start\":70684},{\"end\":71241,\"start\":71162},{\"end\":72101,\"start\":72020},{\"end\":72832,\"start\":72749},{\"end\":73575,\"start\":73490},{\"end\":74849,\"start\":74770},{\"end\":75459,\"start\":75382},{\"end\":75927,\"start\":75917},{\"end\":76340,\"start\":76328},{\"end\":77004,\"start\":76903},{\"end\":78048,\"start\":77943},{\"end\":78616,\"start\":78552},{\"end\":81093,\"start\":81025},{\"end\":82076,\"start\":82066},{\"end\":82525,\"start\":82510},{\"end\":83070,\"start\":82985},{\"end\":84166,\"start\":84071},{\"end\":84845,\"start\":84761},{\"end\":85438,\"start\":85348},{\"end\":86288,\"start\":86198},{\"end\":86933,\"start\":86852},{\"end\":87381,\"start\":87371},{\"end\":88305,\"start\":88184},{\"end\":89205,\"start\":89120},{\"end\":89733,\"start\":89718},{\"end\":90324,\"start\":90228},{\"end\":91067,\"start\":91057},{\"end\":91961,\"start\":91951},{\"end\":92476,\"start\":92413},{\"end\":93034,\"start\":92987},{\"end\":93568,\"start\":93494},{\"end\":94064,\"start\":94054},{\"end\":64416,\"start\":64387},{\"end\":64513,\"start\":64503},{\"end\":64584,\"start\":64566},{\"end\":64696,\"start\":64681},{\"end\":64808,\"start\":64780},{\"end\":65033,\"start\":64987},{\"end\":65258,\"start\":65236},{\"end\":65348,\"start\":65327},{\"end\":65478,\"start\":65457},{\"end\":65571,\"start\":65556},{\"end\":65887,\"start\":65790},{\"end\":66440,\"start\":66424},{\"end\":66846,\"start\":66768},{\"end\":67426,\"start\":67418},{\"end\":67810,\"start\":67732},{\"end\":68252,\"start\":68191},{\"end\":68653,\"start\":68600},{\"end\":69007,\"start\":68946},{\"end\":69429,\"start\":69373},{\"end\":69654,\"start\":69607},{\"end\":70060,\"start\":69994},{\"end\":70682,\"start\":70604},{\"end\":71160,\"start\":71069},{\"end\":71645,\"start\":71598},{\"end\":72018,\"start\":71939},{\"end\":72747,\"start\":72652},{\"end\":73488,\"start\":73391},{\"end\":74054,\"start\":74007},{\"end\":74365,\"start\":74325},{\"end\":74768,\"start\":74691},{\"end\":75380,\"start\":75305},{\"end\":75915,\"start\":75899},{\"end\":76326,\"start\":76248},{\"end\":76901,\"start\":76802},{\"end\":77520,\"start\":77491},{\"end\":77941,\"start\":77838},{\"end\":78550,\"start\":78488},{\"end\":79805,\"start\":79724},{\"end\":81023,\"start\":80957},{\"end\":81537,\"start\":81521},{\"end\":81793,\"start\":81757},{\"end\":82064,\"start\":82009},{\"end\":82508,\"start\":82430},{\"end\":82983,\"start\":82886},{\"end\":83551,\"start\":83496},{\"end\":84069,\"start\":83976},{\"end\":84759,\"start\":84663},{\"end\":85346,\"start\":85258},{\"end\":85849,\"start\":85833},{\"end\":86196,\"start\":86108},{\"end\":86850,\"start\":86771},{\"end\":87369,\"start\":87353},{\"end\":87656,\"start\":87640},{\"end\":88182,\"start\":88031},{\"end\":89118,\"start\":89021},{\"end\":89716,\"start\":89638},{\"end\":90226,\"start\":90132},{\"end\":90741,\"start\":90694},{\"end\":91055,\"start\":91000},{\"end\":91507,\"start\":91491},{\"end\":91949,\"start\":91894},{\"end\":92411,\"start\":92350},{\"end\":92985,\"start\":92923},{\"end\":93492,\"start\":93406},{\"end\":94052,\"start\":93974}]"}}}, "year": 2023, "month": 12, "day": 17}