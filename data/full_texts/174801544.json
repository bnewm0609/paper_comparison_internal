{"id": 174801544, "updated": "2023-10-07 01:50:30.408", "metadata": {"title": "PseudoEdgeNet: Nuclei Segmentation only with Point Annotations", "authors": "[{\"first\":\"Inwan\",\"last\":\"Yoo\",\"middle\":[]},{\"first\":\"Donggeun\",\"last\":\"Yoo\",\"middle\":[]},{\"first\":\"Kyunghyun\",\"last\":\"Paeng\",\"middle\":[]}]", "venue": "Lecture Notes in Computer Science", "journal": "Lecture Notes in Computer Science", "publication_date": {"year": 2019, "month": 6, "day": 7}, "abstract": "Nuclei segmentation is one of the important tasks for whole slide image analysis in digital pathology. With the drastic advance of deep learning, recent deep networks have demonstrated successful performance of the nuclei segmentation task. However, a major bottleneck to achieving good performance is the cost for annotation. A large network requires a large number of segmentation masks, and this annotation task is given to pathologists, not the public. In this paper, we propose a weakly supervised nuclei segmentation method, which requires only point annotations for training. This method can scale to large training set as marking a point of a nucleus is much cheaper than the fine segmentation mask. To this end, we introduce a novel auxiliary network, called PseudoEdgeNet, which guides the segmentation network to recognize nuclei edges even without edge annotations. We evaluate our method with two public datasets, and the results demonstrate that the method consistently outperforms other weakly supervised methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1906.02924", "mag": "2979976730", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/miccai/YooYP19", "doi": "10.1007/978-3-030-32239-7_81"}}, "content": {"source": {"pdf_hash": "52e6c8914815ab3dcdc6056505a4ee672a2dd8f8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1906.02924v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1906.02924", "status": "GREEN"}}, "grobid": {"id": "9a8e6c18b6996ac3f1cf39f67191cc0d4edd6fd6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/52e6c8914815ab3dcdc6056505a4ee672a2dd8f8.txt", "contents": "\nPseudoEdgeNet: Nuclei Segmentation only with Point Annotations\n\n\nInwan Yoo iwyoo@lunit.io \nLunit Inc\nSeoulSouth Korea\n\nDonggeun Yoo dgyoo@lunit.io \nLunit Inc\nSeoulSouth Korea\n\nKyunghyun Paeng khpaeng@lunit.io \nLunit Inc\nSeoulSouth Korea\n\nPseudoEdgeNet: Nuclei Segmentation only with Point Annotations\nNuclei segmentation \u00b7 Weakly supervised learning \u00b7 Point annotation\nNuclei segmentation is one of the important tasks for whole slide image analysis in digital pathology. With the drastic advance of deep learning, recent deep networks have demonstrated successful performance of the nuclei segmentation task. However, a major bottleneck to achieving good performance is the cost for annotation. A large network requires a large number of segmentation masks, and this annotation task is given to pathologists, not the public. In this paper, we propose a weakly supervised nuclei segmentation method, which requires only point annotations for training. This method can scale to large training set as marking a point of a nucleus is much cheaper than the fine segmentation mask. To this end, we introduce a novel auxiliary network, called PseudoEdgeNet, which guides the segmentation network to recognize nuclei edges even without edge annotations. We evaluate our method with two public datasets, and the results demonstrate that the method consistently outperforms other weakly supervised methods.\n\nIntroduction\n\nWith the advent of digital pathology [2], extracting information of biological components from whole slide images (WSIs) is attracting more attention since the statistics can be utilized for biomarker development as well as accurate diagnosis [3]. However, it is infeasible for human experts (e.g. pathologists) to manually extract the statistics due to the huge dimensions of WSI space. A WSI can comprise up to 100k\u00d7100k pixels [12]. Despite its huge dimensions, the area of a target instance is usually small, such as a tumor cell. In order to automate this process, a variety of visual recognition methods from computer vision has been applied to WSIs [11,18,8,14]. Among the various recognition tasks, this paper focuses on the nuclei segmentation problem [14].\n\nDuring the last few years, we have witnessed drastic progress in segmentation tasks on WSIs [8,14] with deep learning. Despite its successful performance, the cost for annotations is still worrisome. Drawing fine masks of target instances is much more labor-intensive than drawing bounding boxes or tagging class labels. Furthermore, only experts such as pathologists, not the public, can conduct this annotation task. The situation gets much worse when we choose a deep network as a segmentation model which can be learned with a large number of training samples. These factors make it difficult to create a large-scale segmentation dataset in the WSI domain.\n\nThis paper aims at cutting the annotation cost for nuclei segmentation. The quickest and easiest way to annotate a nucleus is to mark a point on it. A point does not contain fine boundary information of a nucleus, but we can obtain a much larger amount of training samples than segmentation masks, given a fixed budget for annotation. This strategy is scalable for learning a large network, and it is also expected that a large amount of training samples will contribute to the generalization performance [13] of the network.\n\nTo this end, we propose a novel weakly-supervised model, which is composed of a segmentation network and an auxiliary network, called PseudoEdgeNet. The segmentation network produces nuclei segments while the auxiliary network helps the main network learn to recognize nuclei boundaries with point annotations only. We evaluate this model over two public datasets [8,14] and the results demonstrate successful segmentation performance compared to other recent methods [9,15] for weakly-supervised segmentation.\n\n\nRelated Research\n\nNuclei segmentation There have been several works for nuclei segmentation based on deep learning, but all of the methods use a fully-supervised learning model that requires nuclei segmentation masks. [8] makes a public nuclei dataset containing full segmentation masks and introduces a segmentation model based on a pixel-level classification approach. [14] approaches the nuclei segmentation task as a regression problem. The work done by [19] is also a regression method but a sparsity constraint is introduced. [1] adopts a two-step approach where the model produces cell proposals first and then segments the nuclei.\n\nCell detection with points Cell detection methods are related to ours since these often utilize point annotations [18,5,16]. One popular family casts cell detection as a regression problem, such as [5] and [16] adopt a regression Random Forest and a CNN regressor, respectively. Another approach is pixel-level classification with point annotations [18], which is similar to the typical semantic segmentation approach. However, these methods use the point annotations to learn a detection model which predicts the cell locations as points, not a segmentation model.\n\nWeakly-supervised segmentation To the best of our knowledge, there has been no weakly-supervised method for nuclei segmentation. However, in the natural image domain, a long line of works has been presented to reduce the cost of pixel-level annotations. An object segmentation model is learned with bounding-boxes [6] or scribbles [10,15], which are much cheaper to obtain than the pixel-level masks. The work presented by [9] is similar to ours in that it also uses point annotations. However, their target task is to find \"rough blobs\" on objects while we have to predict \"fine boundaries\" of nuclei.\n\n\nMethod\n\nThe proposed architecture is composed of a segmentation network and Pseu-doEdgeNet. The segmentation network is our target model that segments nuclei from inputs. PseudoEdgeNet, only introduced for training phase, encourages the segmentation network to recognize nucleus edges without edge annotations. Figure 1 is illustrating the proposed architecture.\n\n\nSegmentation Network\n\nTo learn a segmentation network with point annotations, we follow the label assignment scheme presented by [9]. In this scheme, positive labels are given to the pixels corresponding to point annotations, while negative labels are assigned to pixels on Voronoi boundaries that can be obtained by distance transform with point annotations. Then, binary cross-entropy losses are evaluated and averaged over the labels and corresponding pixel outputs. The segmentation network learned with this loss can successfully localize nuclei as blobs. However, it fails to segment along the edges of nuclei since there is no direct supervision for edges. For this reason, we introduce an auxiliary network that can provide fine boundary information with that the segmentation network is supervised to segment along the nucleus edges.\n\n\nLearning with PseudoEdgeNet\n\nIn CNNs, it is well known that lower layers extract low-level information such as edges and blobs, while higher layers encode object parts or an object as a whole [17]. This motivates us to design a shallow CNN to efficiently extract nucleus edges without edge annotations. These pseudo edges can be inaccurate but sufficient to act as supervisory signals to the segmentation network.\n\nGiven an image I, since we do not have edge annotations, PseudoEdgeNet g is jointly learned with the segmentation network f using the point annotations P . To make the edge map g(I) comparable to the segmentation map f (I), we apply a (x, y)-directional Sobel filter s to f (I). Then, the final loss L to jointly learn these two networks {f, g} is defined as\nL(I, P, f, g) = L ce (f (I), P ) + \u03bb \u00b7 |s(f (I)) \u2212 g(I)|,(1)\nwhere L ce is the pixel-averaged cross-entropy loss defined in Section 3.1 and \u03bb is a scaling constant. The segmentation network f is learned to detect nuclei by the first term, and simultaneously forced to activate on nucleus edges by the second term. PseudoEdgeNet g is used only to learn f with this loss, and unnecessary at inference time.\n\nWhat is noteworthy here is the capacity gap between f and g. If g is as large as f , then g will be learned just like f , except that the outputs are edges. However, since we design g to be much smaller than f , g is able to encode low/mid-level edges, not the high-level information, which only f can cope with. Empirical analysis on this will be discussed later with Table 2 in Section 4.3.\n\n\nAttention Module for Edge Network\n\nAccording to our experiment in Table 1, the method presented up to Section 3.2 shows clear performance gains. However, there is still much room for improving the quality of edges used for auxiliary supervision. Due to the low capacity of g, a significant portion of edges originates from irrelevant backgrounds. To suppress these, we add an attention module h inside PseudoEdgeNet, which produces an attention map h(I), that indicates where to extract edges. Since this task requires high-level understanding of nuclei, we use a large architecture for this module. The attention map h(I) is applied to the raw edge g(I), and then the loss function is re-defined as\nL(I, P, f, g, h) = L ce (f (I), P ) + \u03bb \u00b7 |s(f (I)) \u2212 g(I) \u2297 h(I)|,(2)\nwhere \u2297 means element-wise multiplication. We jointly learn parameters in {f, g, h}, and only use the segmentation network f at inference time. Figure 2- \n\n\nEvaluation\n\n\nDatasets\n\nWe evaluate our method with two major nuclei segmentation datasets: MoNuSeg [8] and TNBC [14]. MoNuSeg comprises 30 images in which each image size is 1,000\u00d71,000. TNBC is composed of 50 images with 512\u00d7512 size. These two datasets provide full nuclei masks, that enable us to automatically generate point annotations and to evaluate segmentation results with full masks. To construct a training set composed of images and point annotations, we extract nuclei points by calculating the center of mass of each nucleus instance mask. We conduct kfold cross-validation with k=10 for thorough evaluation. Among 10 folds of data, we use two folds as a validation set and a test set, and the rest as a training set.\n(a) I (b) h(I) (c) g(I) (d) g \u2297 h (e) Ours (f) [9] (g) GT\n\nImplementation Details\n\nWe choose [9] as a baseline method, which is the most recent work for learning with point annotations. Among the loss terms in [9], we do not use the imagelevel classification loss since almost of image patches contain nuclei, but only adopt the pixel-averaged cross-entropy loss L ce described in Section 3.1. Table 1. Nuclei segmentation performance comparison between methods. The mean and standard deviation of 10-fold cross-validation results (10 IoU scores) are reported.\n\n\nMethods\n\nMoNuSeg TNBC Baseline [9] 0.5710 (\u00b10.02) 0.5504 (\u00b10.04) DenseCRF* [15] 0 We employ a Feature Pyramid Network (FPN) for segmentation [7] with a ResNet-50 [4] backbone followed by a sigmoid layer as the segmentation network f in all experiments. We compose g of PseudoEdgeNet with four convolution layers to make it much smaller than the segmentation network f . Each of the convolution layers contains 64 filters and is followed by batch normalization and ReLU, except for the last layer, which produces a two-channel output representing (x, y)-directional Sobel edge maps. For the attention module h inside PseudoEdgeNet, we use an FPN with a Resnet-18 backbone and stack a sigmoid as an output layer.\n\nWe set the label weights applied to the cross-entropy loss as 0.1 and 1.0 for negative and positive labels respectively since much more negatives are given to the loss function compared to the positives. The scaling constant \u03bb in Equation (1) and (2) is set to 1.0.\n\nTo achieve high generalization performance, we apply a lot of data augmentation methods to inputs including color jittering, Gaussian blurring, Gaussian noise injection, rotation, horizontal or vertical flip, affine transformation, and elastic deformation. We use an Adam optimizer with an initial learning rate of 0.001. We train all networks with a plateau scheduling policy where the learning rate is halved when the average loss per epoch does not decrease for the current five epochs.\n\nThe threshold to determine positive pixels from f (I) is 0.5. We evaluate the model on the validation set for each epoch and choose the best model to evaluate that on the test set. We choose the intersection over union (IoU), which is the most common metric for semantic segmentation, as an evaluation metric. Table 1 is summarizing the experimental results. The most recent weakly supervised segmentation method [15] noted by DenseCRF marginally beats the baseline [9]. However, PseudoEdgeNet with small g significantly improves the baseline by a large margin of +3.49% for both of the datasets. When PseudoEd-geNet is equipped with the attention module h, the performance gains increase to +4.26% and +5.34%. These results clearly demonstrate the effectiveness of g and h. Compared to small g, the worse performance of large g proves the importance of the capacity of g. For large g, we use the same architecture of f , so g is learned just like f , resulting in a small improvement to the baseline.\n\n\nResults\n\nTo take a closer look at the importance of capacity, we try to densely change the depth of g. Table 2 is summarizing the results. For the family of small networks, the depth change from 2 to 8 does not make any significant difference in performance. However, when g is equipped with large ResNets, the performance significantly drops. The depth variation within the large architecture also shows minor performance changes.\n\n\nConclusion and Future Work\n\nWe have presented a novel nuclei segmentation method only with point supervision. Our auxiliary network, PseudoEdgeNet, can find object edges without edge annotations as it has low capacity, which acts as a strong constraint for weakly-supervised learning. Our method can scale to large-scale segmentation problem for better performance, as point annotations are much cheaper than segmentation masks.\n\nHowever, given the same amount of data, the performance of weakly-supervised learning is bounded to that of supervised learning. It will be a promising future work to annotate a small number of segmentation masks, and use both mask and point annotations to achieve performance comparable to supervised learning, while greatly saving the annotation cost.\n\nFig. 1 .\n1The overall architecture for weakly-supervised nuclei segmentation. The segmentation network f is jointly learned with PseudoEdgeNet {g, h}. In edge maps, the gray color represents zero while the white and black colors encode positive and negative pixel values, respectively.\n\n\n(b, c, d) shows how attention improves quality of edges.\n\nFig. 2 .\n2Qualitative examples and comparisons: (a) inputs, (b) attention maps, (c) (x, y)-directional raw edge maps, (d) final edge maps in which attentions are multiplied, (e) final segmentation results from our segmentation network, (f) segmentation results from the baseline method[9], and (g) ground-truth masks. In (c, d), each map is averaging the x-and y-directional edge maps. The gray color represents zero while the white and black colors encode positive and negative pixel values, respectively.\n\n. 5813 (\n5813\u00b10.03) 0.5555 (\u00b10.04) PseudoEdgeNet with large g 0.5786 (\u00b10.04) 0.5787 (\u00b10.04) PseudoEdgeNet with small g 0.6059 (\u00b10.04) 0.5853 (\u00b10.03) PseudoEdgeNet with small g and h 0.6136 (\u00b10.04) 0.6038 (\u00b10.03) Fully supervised (upper bound) 0.6522 (\u00b10.03) 0.6619 (\u00b10.04) *Authors' open source is used: https://github.com/meng-tang/rloss\n\nTable 2 .\n2Nuclei segmentation performance to the size of edge networks. The mean and standard deviation of 10-fold cross-validation results (10 IoU scores) are reported.Edge networks (g) \nMoNuSeg \nTNBC \n\nSmall \n\nCNN with 2 conv layers 0.6117 (\u00b10.03) 0.5928 (\u00b10.04) \nCNN with 4 conv layers 0.6136 (\u00b10.04) 0.6038 (\u00b10.03) \nCNN with 6 conv layers 0.6105 (\u00b10.04) 0.5896 (\u00b10.03) \nCNN with 8 conv layers 0.6119 (\u00b10.02) 0.5934 (\u00b10.04) \n\nLarge \n\nFPN-ResNet18 \n0.6005 (\u00b10.03) 0.5795 (\u00b10.04) \nFPN-ResNet34 \n0.6069 (\u00b10.03) 0.5796 (\u00b10.03) \nFPN-ResNet50 \n0.5786 (\u00b10.04) 0.5787 (\u00b10.04) \n\n\n\nCell segmentation proposal network for microscopy image analysis. S U Akram, J Kannala, L Eklund, J Heikkil\u00e4, Deep Learning and Data Labeling for Medical Applications. SpringerAkram, S.U., Kannala, J., Eklund, L., Heikkil\u00e4, J.: Cell segmentation proposal network for microscopy image analysis. In: Deep Learning and Data Labeling for Medical Applications, pp. 21-29. Springer (2016)\n\nDigital pathology: current status and future perspectives. S Al-Janabi, A Huisman, P J Van Diest, Histopathology. 611Al-Janabi, S., Huisman, A., Van Diest, P.J.: Digital pathology: current status and future perspectives. Histopathology 61(1), 1-9 (2012)\n\nSystematic analysis of breast cancer morphology uncovers stromal features associated with survival. A H Beck, A R Sangoi, S Leung, R J Marinelli, T O Nielsen, Van De, M J Vijver, R B West, M Van De Rijn, D Koller, Science translational medicine. 3108Beck, A.H., Sangoi, A.R., Leung, S., Marinelli, R.J., Nielsen, T.O., Van De Vijver, M.J., West, R.B., Van De Rijn, M., Koller, D.: Systematic analysis of breast cancer morphology uncovers stromal features associated with survival. Science transla- tional medicine 3(108), 108ra113-108ra113 (2011)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2016)\n\nYou should use regression to detect cells. P Kainz, M Urschler, S Schulter, P Wohlhart, V Lepetit, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerKainz, P., Urschler, M., Schulter, S., Wohlhart, P., Lepetit, V.: You should use re- gression to detect cells. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 276-283. Springer (2015)\n\nSimple does it: Weakly supervised instance and semantic segmentation. A Khoreva, R Benenson, J Hosang, M Hein, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKhoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B.: Simple does it: Weakly supervised instance and semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 876-885 (2017)\n\nA unified architecture for instance and semantic segmentation. A Kirillov, K He, R Girshick, P Doll\u00e1r, Kirillov, A., He, K., Girshick, R., Doll\u00e1r, P.: A unified architecture for instance and semantic segmentation\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. N Kumar, R Verma, S Sharma, S Bhargava, A Vahadane, A Sethi, IEEE transactions on medical imaging. 367Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A.: A dataset and a technique for generalized nuclear segmentation for computational pathology. IEEE transactions on medical imaging 36(7), 1550-1560 (2017)\n\nWhere are the blobs: Counting by localization with point supervision. I H Laradji, N Rostamzadeh, P O Pinheiro, D Vazquez, M Schmidt, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Laradji, I.H., Rostamzadeh, N., Pinheiro, P.O., Vazquez, D., Schmidt, M.: Where are the blobs: Counting by localization with point supervision. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 547-562 (2018)\n\nScribblesup: Scribble-supervised convolutional networks for semantic segmentation. D Lin, J Dai, J Jia, K He, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLin, D., Dai, J., Jia, J., He, K., Sun, J.: Scribblesup: Scribble-supervised convolu- tional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3159-3167 (2016)\n\nDeep learning as a tool for increased accuracy and efficiency of histopathological diagnosis. G Litjens, C I S\u00e1nchez, N Timofeeva, M Hermsen, I Nagtegaal, I Kovacs, C Hulsbergen-Van De Kaa, P Bult, B Van Ginneken, J Van Der Laak, Scientific reports. 626286Litjens, G., S\u00e1nchez, C.I., Timofeeva, N., Hermsen, M., Nagtegaal, I., Kovacs, I., Hulsbergen-Van De Kaa, C., Bult, P., Van Ginneken, B., Van Der Laak, J.: Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis. Scientific reports 6, 26286 (2016)\n\nY Liu, K Gadepalli, M Norouzi, G E Dahl, T Kohlberger, A Boyko, S Venugopalan, A Timofeev, P Q Nelson, G S Corrado, arXiv:1703.02442Detecting cancer metastases on gigapixel pathology images. arXiv preprintLiu, Y., Gadepalli, K., Norouzi, M., Dahl, G.E., Kohlberger, T., Boyko, A., Venu- gopalan, S., Timofeev, A., Nelson, P.Q., Corrado, G.S., et al.: Detecting cancer metastases on gigapixel pathology images. arXiv preprint arXiv:1703.02442 (2017)\n\nExploring the limits of weakly supervised pretraining. D Mahajan, R Girshick, V Ramanathan, K He, M Paluri, Y Li, A Bharambe, L Van Der Maaten, The European Conference on Computer Vision (ECCV). Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., van der Maaten, L.: Exploring the limits of weakly supervised pretraining. In: The European Conference on Computer Vision (ECCV) (September 2018)\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE Transactions on Medical Imaging. Naylor, P., La\u00e9, M., Reyal, F., Walter, T.: Segmentation of nuclei in histopathology images by deep regression of the distance map. IEEE Transactions on Medical Imaging (2018)\n\nOn regularized losses for weakly-supervised cnn segmentation. M Tang, F Perazzi, A Djelouah, I Ben Ayed, C Schroers, Y Boykov, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Tang, M., Perazzi, F., Djelouah, A., Ben Ayed, I., Schroers, C., Boykov, Y.: On regularized losses for weakly-supervised cnn segmentation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 507-522 (2018)\n\nMicroscopy cell counting with fully convolutional regression networks. X Weidi, J A Noble, A Zisserman, 1st Deep Learning Workshop. Weidi, X., Noble, J.A., Zisserman, A.: Microscopy cell counting with fully con- volutional regression networks. In: 1st Deep Learning Workshop, Medical Image Computing and Computer-Assisted Intervention (MICCAI) (2015)\n\nVisualizing and understanding convolutional networks. M D Zeiler, R Fergus, European conference on computer vision. SpringerZeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: European conference on computer vision. pp. 818-833. Springer (2014)\n\nSfcn-opi: Detection and finegrained classification of nuclei using sibling fcn with objectness prior interaction. Y Zhou, Q Dou, H Chen, J Qin, P A Heng, Thirty-Second AAAI Conference on Artificial Intelligence. Zhou, Y., Dou, Q., Chen, H., Qin, J., Heng, P.A.: Sfcn-opi: Detection and fine- grained classification of nuclei using sibling fcn with objectness prior interaction. In: Thirty-Second AAAI Conference on Artificial Intelligence (2018)\n\nNuclei segmentation via sparsity constrained convolutional regression. Y Zhou, H Chang, K E Barner, B Parvin, 2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). IEEEZhou, Y., Chang, H., Barner, K.E., Parvin, B.: Nuclei segmentation via sparsity constrained convolutional regression. In: 2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). pp. 1284-1287. IEEE (2015)\n", "annotations": {"author": "[{\"end\":119,\"start\":66},{\"end\":176,\"start\":120},{\"end\":238,\"start\":177}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":72},{\"end\":132,\"start\":129},{\"end\":192,\"start\":187}]", "author_first_name": "[{\"end\":71,\"start\":66},{\"end\":128,\"start\":120},{\"end\":186,\"start\":177}]", "author_affiliation": "[{\"end\":118,\"start\":92},{\"end\":175,\"start\":149},{\"end\":237,\"start\":211}]", "title": "[{\"end\":63,\"start\":1},{\"end\":301,\"start\":239}]", "venue": null, "abstract": "[{\"end\":1398,\"start\":370}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1454,\"start\":1451},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1660,\"start\":1657},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1848,\"start\":1844},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2074,\"start\":2070},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2077,\"start\":2074},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2079,\"start\":2077},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2082,\"start\":2079},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2179,\"start\":2175},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2277,\"start\":2274},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2280,\"start\":2277},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3353,\"start\":3349},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3738,\"start\":3735},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3741,\"start\":3738},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3842,\"start\":3839},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3845,\"start\":3842},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4105,\"start\":4102},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4259,\"start\":4255},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4346,\"start\":4342},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4642,\"start\":4638},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4644,\"start\":4642},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4647,\"start\":4644},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4725,\"start\":4722},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4734,\"start\":4730},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4877,\"start\":4873},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5408,\"start\":5405},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5426,\"start\":5422},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5429,\"start\":5426},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5517,\"start\":5514},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6193,\"start\":6190},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7102,\"start\":7098},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9511,\"start\":9508},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9525,\"start\":9521},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10238,\"start\":10235},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10355,\"start\":10352},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10739,\"start\":10736},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10784,\"start\":10780},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10849,\"start\":10846},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10870,\"start\":10867},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11659,\"start\":11656},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12592,\"start\":12588},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12644,\"start\":12641},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15032,\"start\":15029}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":14683,\"start\":14397},{\"attributes\":{\"id\":\"fig_1\"},\"end\":14742,\"start\":14684},{\"attributes\":{\"id\":\"fig_2\"},\"end\":15250,\"start\":14743},{\"attributes\":{\"id\":\"fig_3\"},\"end\":15590,\"start\":15251},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16166,\"start\":15591}]", "paragraph": "[{\"end\":2180,\"start\":1414},{\"end\":2842,\"start\":2182},{\"end\":3369,\"start\":2844},{\"end\":3881,\"start\":3371},{\"end\":4522,\"start\":3902},{\"end\":5089,\"start\":4524},{\"end\":5693,\"start\":5091},{\"end\":6058,\"start\":5704},{\"end\":6903,\"start\":6083},{\"end\":7319,\"start\":6935},{\"end\":7679,\"start\":7321},{\"end\":8084,\"start\":7741},{\"end\":8478,\"start\":8086},{\"end\":9180,\"start\":8516},{\"end\":9406,\"start\":9252},{\"end\":10141,\"start\":9432},{\"end\":10702,\"start\":10225},{\"end\":11415,\"start\":10714},{\"end\":11682,\"start\":11417},{\"end\":12173,\"start\":11684},{\"end\":13176,\"start\":12175},{\"end\":13610,\"start\":13188},{\"end\":14041,\"start\":13641},{\"end\":14396,\"start\":14043}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7740,\"start\":7680},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9251,\"start\":9181},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10199,\"start\":10142}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8462,\"start\":8455},{\"end\":8554,\"start\":8547},{\"end\":10543,\"start\":10536},{\"end\":12492,\"start\":12485},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13289,\"start\":13282}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1412,\"start\":1400},{\"attributes\":{\"n\":\"2\"},\"end\":3900,\"start\":3884},{\"attributes\":{\"n\":\"3\"},\"end\":5702,\"start\":5696},{\"attributes\":{\"n\":\"3.1\"},\"end\":6081,\"start\":6061},{\"attributes\":{\"n\":\"3.2\"},\"end\":6933,\"start\":6906},{\"attributes\":{\"n\":\"3.3\"},\"end\":8514,\"start\":8481},{\"attributes\":{\"n\":\"4\"},\"end\":9419,\"start\":9409},{\"attributes\":{\"n\":\"4.1\"},\"end\":9430,\"start\":9422},{\"attributes\":{\"n\":\"4.2\"},\"end\":10223,\"start\":10201},{\"end\":10712,\"start\":10705},{\"attributes\":{\"n\":\"4.3\"},\"end\":13186,\"start\":13179},{\"attributes\":{\"n\":\"5\"},\"end\":13639,\"start\":13613},{\"end\":14406,\"start\":14398},{\"end\":14752,\"start\":14744},{\"end\":15260,\"start\":15252},{\"end\":15601,\"start\":15592}]", "table": "[{\"end\":16166,\"start\":15762}]", "figure_caption": "[{\"end\":14683,\"start\":14408},{\"end\":14742,\"start\":14686},{\"end\":15250,\"start\":14754},{\"end\":15590,\"start\":15265},{\"end\":15762,\"start\":15603}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6015,\"start\":6007},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9405,\"start\":9396}]", "bib_author_first_name": "[{\"end\":16235,\"start\":16234},{\"end\":16237,\"start\":16236},{\"end\":16246,\"start\":16245},{\"end\":16257,\"start\":16256},{\"end\":16267,\"start\":16266},{\"end\":16612,\"start\":16611},{\"end\":16625,\"start\":16624},{\"end\":16636,\"start\":16635},{\"end\":16638,\"start\":16637},{\"end\":16908,\"start\":16907},{\"end\":16910,\"start\":16909},{\"end\":16918,\"start\":16917},{\"end\":16920,\"start\":16919},{\"end\":16930,\"start\":16929},{\"end\":16939,\"start\":16938},{\"end\":16941,\"start\":16940},{\"end\":16954,\"start\":16953},{\"end\":16956,\"start\":16955},{\"end\":16975,\"start\":16974},{\"end\":16977,\"start\":16976},{\"end\":16987,\"start\":16986},{\"end\":16989,\"start\":16988},{\"end\":16997,\"start\":16996},{\"end\":17012,\"start\":17011},{\"end\":17402,\"start\":17401},{\"end\":17408,\"start\":17407},{\"end\":17417,\"start\":17416},{\"end\":17424,\"start\":17423},{\"end\":17788,\"start\":17787},{\"end\":17797,\"start\":17796},{\"end\":17809,\"start\":17808},{\"end\":17821,\"start\":17820},{\"end\":17833,\"start\":17832},{\"end\":18243,\"start\":18242},{\"end\":18254,\"start\":18253},{\"end\":18266,\"start\":18265},{\"end\":18276,\"start\":18275},{\"end\":18284,\"start\":18283},{\"end\":18734,\"start\":18733},{\"end\":18746,\"start\":18745},{\"end\":18752,\"start\":18751},{\"end\":18764,\"start\":18763},{\"end\":18977,\"start\":18976},{\"end\":18986,\"start\":18985},{\"end\":18995,\"start\":18994},{\"end\":19005,\"start\":19004},{\"end\":19017,\"start\":19016},{\"end\":19029,\"start\":19028},{\"end\":19376,\"start\":19375},{\"end\":19378,\"start\":19377},{\"end\":19389,\"start\":19388},{\"end\":19404,\"start\":19403},{\"end\":19406,\"start\":19405},{\"end\":19418,\"start\":19417},{\"end\":19429,\"start\":19428},{\"end\":19872,\"start\":19871},{\"end\":19879,\"start\":19878},{\"end\":19886,\"start\":19885},{\"end\":19893,\"start\":19892},{\"end\":19899,\"start\":19898},{\"end\":20375,\"start\":20374},{\"end\":20386,\"start\":20385},{\"end\":20388,\"start\":20387},{\"end\":20399,\"start\":20398},{\"end\":20412,\"start\":20411},{\"end\":20423,\"start\":20422},{\"end\":20436,\"start\":20435},{\"end\":20446,\"start\":20445},{\"end\":20471,\"start\":20470},{\"end\":20479,\"start\":20478},{\"end\":20495,\"start\":20494},{\"end\":20823,\"start\":20822},{\"end\":20830,\"start\":20829},{\"end\":20843,\"start\":20842},{\"end\":20854,\"start\":20853},{\"end\":20856,\"start\":20855},{\"end\":20864,\"start\":20863},{\"end\":20878,\"start\":20877},{\"end\":20887,\"start\":20886},{\"end\":20902,\"start\":20901},{\"end\":20914,\"start\":20913},{\"end\":20916,\"start\":20915},{\"end\":20926,\"start\":20925},{\"end\":20928,\"start\":20927},{\"end\":21328,\"start\":21327},{\"end\":21339,\"start\":21338},{\"end\":21351,\"start\":21350},{\"end\":21365,\"start\":21364},{\"end\":21371,\"start\":21370},{\"end\":21381,\"start\":21380},{\"end\":21387,\"start\":21386},{\"end\":21399,\"start\":21398},{\"end\":21788,\"start\":21787},{\"end\":21798,\"start\":21797},{\"end\":21805,\"start\":21804},{\"end\":21814,\"start\":21813},{\"end\":22101,\"start\":22100},{\"end\":22109,\"start\":22108},{\"end\":22120,\"start\":22119},{\"end\":22132,\"start\":22131},{\"end\":22144,\"start\":22143},{\"end\":22156,\"start\":22155},{\"end\":22581,\"start\":22580},{\"end\":22590,\"start\":22589},{\"end\":22592,\"start\":22591},{\"end\":22601,\"start\":22600},{\"end\":22916,\"start\":22915},{\"end\":22918,\"start\":22917},{\"end\":22928,\"start\":22927},{\"end\":23254,\"start\":23253},{\"end\":23262,\"start\":23261},{\"end\":23269,\"start\":23268},{\"end\":23277,\"start\":23276},{\"end\":23284,\"start\":23283},{\"end\":23286,\"start\":23285},{\"end\":23658,\"start\":23657},{\"end\":23666,\"start\":23665},{\"end\":23675,\"start\":23674},{\"end\":23677,\"start\":23676},{\"end\":23687,\"start\":23686}]", "bib_author_last_name": "[{\"end\":16243,\"start\":16238},{\"end\":16254,\"start\":16247},{\"end\":16264,\"start\":16258},{\"end\":16276,\"start\":16268},{\"end\":16622,\"start\":16613},{\"end\":16633,\"start\":16626},{\"end\":16648,\"start\":16639},{\"end\":16915,\"start\":16911},{\"end\":16927,\"start\":16921},{\"end\":16936,\"start\":16931},{\"end\":16951,\"start\":16942},{\"end\":16964,\"start\":16957},{\"end\":16972,\"start\":16966},{\"end\":16984,\"start\":16978},{\"end\":16994,\"start\":16990},{\"end\":17009,\"start\":16998},{\"end\":17019,\"start\":17013},{\"end\":17405,\"start\":17403},{\"end\":17414,\"start\":17409},{\"end\":17421,\"start\":17418},{\"end\":17428,\"start\":17425},{\"end\":17794,\"start\":17789},{\"end\":17806,\"start\":17798},{\"end\":17818,\"start\":17810},{\"end\":17830,\"start\":17822},{\"end\":17841,\"start\":17834},{\"end\":18251,\"start\":18244},{\"end\":18263,\"start\":18255},{\"end\":18273,\"start\":18267},{\"end\":18281,\"start\":18277},{\"end\":18292,\"start\":18285},{\"end\":18743,\"start\":18735},{\"end\":18749,\"start\":18747},{\"end\":18761,\"start\":18753},{\"end\":18771,\"start\":18765},{\"end\":18983,\"start\":18978},{\"end\":18992,\"start\":18987},{\"end\":19002,\"start\":18996},{\"end\":19014,\"start\":19006},{\"end\":19026,\"start\":19018},{\"end\":19035,\"start\":19030},{\"end\":19386,\"start\":19379},{\"end\":19401,\"start\":19390},{\"end\":19415,\"start\":19407},{\"end\":19426,\"start\":19419},{\"end\":19437,\"start\":19430},{\"end\":19876,\"start\":19873},{\"end\":19883,\"start\":19880},{\"end\":19890,\"start\":19887},{\"end\":19896,\"start\":19894},{\"end\":19903,\"start\":19900},{\"end\":20383,\"start\":20376},{\"end\":20396,\"start\":20389},{\"end\":20409,\"start\":20400},{\"end\":20420,\"start\":20413},{\"end\":20433,\"start\":20424},{\"end\":20443,\"start\":20437},{\"end\":20468,\"start\":20447},{\"end\":20476,\"start\":20472},{\"end\":20492,\"start\":20480},{\"end\":20508,\"start\":20496},{\"end\":20827,\"start\":20824},{\"end\":20840,\"start\":20831},{\"end\":20851,\"start\":20844},{\"end\":20861,\"start\":20857},{\"end\":20875,\"start\":20865},{\"end\":20884,\"start\":20879},{\"end\":20899,\"start\":20888},{\"end\":20911,\"start\":20903},{\"end\":20923,\"start\":20917},{\"end\":20936,\"start\":20929},{\"end\":21336,\"start\":21329},{\"end\":21348,\"start\":21340},{\"end\":21362,\"start\":21352},{\"end\":21368,\"start\":21366},{\"end\":21378,\"start\":21372},{\"end\":21384,\"start\":21382},{\"end\":21396,\"start\":21388},{\"end\":21414,\"start\":21400},{\"end\":21795,\"start\":21789},{\"end\":21802,\"start\":21799},{\"end\":21811,\"start\":21806},{\"end\":21821,\"start\":21815},{\"end\":22106,\"start\":22102},{\"end\":22117,\"start\":22110},{\"end\":22129,\"start\":22121},{\"end\":22141,\"start\":22133},{\"end\":22153,\"start\":22145},{\"end\":22163,\"start\":22157},{\"end\":22587,\"start\":22582},{\"end\":22598,\"start\":22593},{\"end\":22611,\"start\":22602},{\"end\":22925,\"start\":22919},{\"end\":22935,\"start\":22929},{\"end\":23259,\"start\":23255},{\"end\":23266,\"start\":23263},{\"end\":23274,\"start\":23270},{\"end\":23281,\"start\":23278},{\"end\":23291,\"start\":23287},{\"end\":23663,\"start\":23659},{\"end\":23672,\"start\":23667},{\"end\":23684,\"start\":23678},{\"end\":23694,\"start\":23688}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":18176582},\"end\":16550,\"start\":16168},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":39091134},\"end\":16805,\"start\":16552},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7843488},\"end\":17353,\"start\":16807},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206594692},\"end\":17742,\"start\":17355},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14989038},\"end\":18170,\"start\":17744},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12124389},\"end\":18668,\"start\":18172},{\"attributes\":{\"id\":\"b6\"},\"end\":18882,\"start\":18670},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5162860},\"end\":19303,\"start\":18884},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":50787131},\"end\":19786,\"start\":19305},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3121011},\"end\":20278,\"start\":19788},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":237132},\"end\":20820,\"start\":20280},{\"attributes\":{\"doi\":\"arXiv:1703.02442\",\"id\":\"b11\"},\"end\":21270,\"start\":20822},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13751202},\"end\":21697,\"start\":21272},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":59601271},\"end\":22036,\"start\":21699},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4796040},\"end\":22507,\"start\":22038},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":33589566},\"end\":22859,\"start\":22509},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3960646},\"end\":23137,\"start\":22861},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":19177412},\"end\":23584,\"start\":23139},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13299252},\"end\":23986,\"start\":23586}]", "bib_title": "[{\"end\":16232,\"start\":16168},{\"end\":16609,\"start\":16552},{\"end\":16905,\"start\":16807},{\"end\":17399,\"start\":17355},{\"end\":17785,\"start\":17744},{\"end\":18240,\"start\":18172},{\"end\":18974,\"start\":18884},{\"end\":19373,\"start\":19305},{\"end\":19869,\"start\":19788},{\"end\":20372,\"start\":20280},{\"end\":21325,\"start\":21272},{\"end\":21785,\"start\":21699},{\"end\":22098,\"start\":22038},{\"end\":22578,\"start\":22509},{\"end\":22913,\"start\":22861},{\"end\":23251,\"start\":23139},{\"end\":23655,\"start\":23586}]", "bib_author": "[{\"end\":16245,\"start\":16234},{\"end\":16256,\"start\":16245},{\"end\":16266,\"start\":16256},{\"end\":16278,\"start\":16266},{\"end\":16624,\"start\":16611},{\"end\":16635,\"start\":16624},{\"end\":16650,\"start\":16635},{\"end\":16917,\"start\":16907},{\"end\":16929,\"start\":16917},{\"end\":16938,\"start\":16929},{\"end\":16953,\"start\":16938},{\"end\":16966,\"start\":16953},{\"end\":16974,\"start\":16966},{\"end\":16986,\"start\":16974},{\"end\":16996,\"start\":16986},{\"end\":17011,\"start\":16996},{\"end\":17021,\"start\":17011},{\"end\":17407,\"start\":17401},{\"end\":17416,\"start\":17407},{\"end\":17423,\"start\":17416},{\"end\":17430,\"start\":17423},{\"end\":17796,\"start\":17787},{\"end\":17808,\"start\":17796},{\"end\":17820,\"start\":17808},{\"end\":17832,\"start\":17820},{\"end\":17843,\"start\":17832},{\"end\":18253,\"start\":18242},{\"end\":18265,\"start\":18253},{\"end\":18275,\"start\":18265},{\"end\":18283,\"start\":18275},{\"end\":18294,\"start\":18283},{\"end\":18745,\"start\":18733},{\"end\":18751,\"start\":18745},{\"end\":18763,\"start\":18751},{\"end\":18773,\"start\":18763},{\"end\":18985,\"start\":18976},{\"end\":18994,\"start\":18985},{\"end\":19004,\"start\":18994},{\"end\":19016,\"start\":19004},{\"end\":19028,\"start\":19016},{\"end\":19037,\"start\":19028},{\"end\":19388,\"start\":19375},{\"end\":19403,\"start\":19388},{\"end\":19417,\"start\":19403},{\"end\":19428,\"start\":19417},{\"end\":19439,\"start\":19428},{\"end\":19878,\"start\":19871},{\"end\":19885,\"start\":19878},{\"end\":19892,\"start\":19885},{\"end\":19898,\"start\":19892},{\"end\":19905,\"start\":19898},{\"end\":20385,\"start\":20374},{\"end\":20398,\"start\":20385},{\"end\":20411,\"start\":20398},{\"end\":20422,\"start\":20411},{\"end\":20435,\"start\":20422},{\"end\":20445,\"start\":20435},{\"end\":20470,\"start\":20445},{\"end\":20478,\"start\":20470},{\"end\":20494,\"start\":20478},{\"end\":20510,\"start\":20494},{\"end\":20829,\"start\":20822},{\"end\":20842,\"start\":20829},{\"end\":20853,\"start\":20842},{\"end\":20863,\"start\":20853},{\"end\":20877,\"start\":20863},{\"end\":20886,\"start\":20877},{\"end\":20901,\"start\":20886},{\"end\":20913,\"start\":20901},{\"end\":20925,\"start\":20913},{\"end\":20938,\"start\":20925},{\"end\":21338,\"start\":21327},{\"end\":21350,\"start\":21338},{\"end\":21364,\"start\":21350},{\"end\":21370,\"start\":21364},{\"end\":21380,\"start\":21370},{\"end\":21386,\"start\":21380},{\"end\":21398,\"start\":21386},{\"end\":21416,\"start\":21398},{\"end\":21797,\"start\":21787},{\"end\":21804,\"start\":21797},{\"end\":21813,\"start\":21804},{\"end\":21823,\"start\":21813},{\"end\":22108,\"start\":22100},{\"end\":22119,\"start\":22108},{\"end\":22131,\"start\":22119},{\"end\":22143,\"start\":22131},{\"end\":22155,\"start\":22143},{\"end\":22165,\"start\":22155},{\"end\":22589,\"start\":22580},{\"end\":22600,\"start\":22589},{\"end\":22613,\"start\":22600},{\"end\":22927,\"start\":22915},{\"end\":22937,\"start\":22927},{\"end\":23261,\"start\":23253},{\"end\":23268,\"start\":23261},{\"end\":23276,\"start\":23268},{\"end\":23283,\"start\":23276},{\"end\":23293,\"start\":23283},{\"end\":23665,\"start\":23657},{\"end\":23674,\"start\":23665},{\"end\":23686,\"start\":23674},{\"end\":23696,\"start\":23686}]", "bib_venue": "[{\"end\":17571,\"start\":17509},{\"end\":18435,\"start\":18373},{\"end\":19554,\"start\":19505},{\"end\":20046,\"start\":19984},{\"end\":22280,\"start\":22231},{\"end\":16334,\"start\":16278},{\"end\":16664,\"start\":16650},{\"end\":17051,\"start\":17021},{\"end\":17507,\"start\":17430},{\"end\":17929,\"start\":17843},{\"end\":18371,\"start\":18294},{\"end\":18731,\"start\":18670},{\"end\":19073,\"start\":19037},{\"end\":19503,\"start\":19439},{\"end\":19982,\"start\":19905},{\"end\":20528,\"start\":20510},{\"end\":21011,\"start\":20954},{\"end\":21465,\"start\":21416},{\"end\":21859,\"start\":21823},{\"end\":22229,\"start\":22165},{\"end\":22639,\"start\":22613},{\"end\":22975,\"start\":22937},{\"end\":23349,\"start\":23293},{\"end\":23763,\"start\":23696}]"}}}, "year": 2023, "month": 12, "day": 17}