{"id": 12191214, "updated": "2023-11-08 08:38:41.264", "metadata": {"title": "Parallel Sparse Matrix-Matrix Multiplication and Indexing: Implementation and Experiments", "authors": "[{\"first\":\"Aydin\",\"last\":\"Buluc\",\"middle\":[]},{\"first\":\"John\",\"last\":\"Gilbert\",\"middle\":[]}]", "venue": "SIAM J. Sci. Comput., 34(4), 170 - 191, 2012", "journal": null, "publication_date": {"year": 2011, "month": null, "day": null}, "abstract": "Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. Here we show that SpGEMM also yields efficient algorithms for general sparse-matrix indexing in distributed memory, provided that the underlying SpGEMM implementation is sufficiently flexible and scalable. We demonstrate that our parallel SpGEMM methods, which use two-dimensional block data distributions with serial hypersparse kernels, are indeed highly flexible, scalable, and memory-efficient in the general case. This algorithm is the first to yield increasing speedup on an unbounded number of processors; our experiments show scaling up to thousands of processors in a variety of test scenarios.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1109.3739", "mag": "3106161546", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/siamsc/BulucG12", "doi": "10.1137/110848244"}}, "content": {"source": {"pdf_hash": "fc6da2966752a734add286e4bbb29bbec239276f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1109.3739v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1109.3739", "status": "GREEN"}}, "grobid": {"id": "a1308118dbe14f53bae4c3ad36adfa71db8cb872", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fc6da2966752a734add286e4bbb29bbec239276f.txt", "contents": "\nPARALLEL SPARSE MATRIX-MATRIX MULTIPLICATION AND INDEXING: IMPLEMENTATION AND EXPERIMENTS *\n\n\nAydin Bulu\u00e7 \nJohn R Gilbert \nPARALLEL SPARSE MATRIX-MATRIX MULTIPLICATION AND INDEXING: IMPLEMENTATION AND EXPERIMENTS *\nParallel computingnumerical linear algebrasparse matrix-matrix multiplicationSpGEMMsparse matrix indexingsparse matrix assignment2D data decompositionhypersparsitygraph algorithmssparse SUMMAsubgraph extractiongraph contractiongraph batch update AMS subject classifications 05C5005C8565F5068W10\nGeneralized sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. Here we show that SpGEMM also yields efficient algorithms for general sparse-matrix indexing in distributed memory, provided that the underlying SpGEMM implementation is sufficiently flexible and scalable. We demonstrate that our parallel SpGEMM methods, which use two-dimensional block data distributions with serial hypersparse kernels, are indeed highly flexible, scalable, and memoryefficient in the general case. This algorithm is the first to yield increasing speedup on an unbounded number of processors; our experiments show scaling up to thousands of processors in a variety of test scenarios.\n\n1. Introduction. We describe scalable parallel implementations of two sparse matrix kernels. The first, SpGEMM, computes the product of two sparse matrices over a general semiring. The second, SpRef, performs generalized indexing into a sparse matrix: Given vectors I and J of row and column indices, SpRef extracts the submatrix A(I, J). Our novel approach to SpRef uses SpGEMM as its key subroutine, which regularizes the computation and data access patterns; conversely, applying SpGEMM to SpRef emphasizes the importance of an SpGEMM implementation that handles arbitrary matrix shapes and sparsity patterns, and a complexity analysis that applies to the general case.\n\nOur main contributions in this paper are: first, we show that SpGEMM leads to a simple and efficient implementation of SpRef; second, we describe a distributedmemory implementation of SpGEMM that is more general in application and more flexible in processor layout than before; and, third, we report on extensive experiments with the performance of SpGEMM and SpRef. We also describe an algorithm for sparse matrix assignment (SpAsgn), and report its parallel performance. The SpAsgn operation, formally A(I, J) = B, assigns a sparse matrix to a submatrix of another sparse matrix. It can be used to perform streaming batch updates to a graph.\n\nParallel algorithms for SpGEMM and SpRef, as well as their theoretical performance, are described in Sections 3 and 4. We present the general SpGEMM algorithm and its parallel complexity before SpRef since the latter uses SpGEMM as a subroutine and its analysis uses results from the SpGEMM analysis. Section 3.1 summarizes our earlier results on the complexity of various SpGEMM algorithms on distributed memory. Section 3.3 presents our algorithm of choice, Sparse SUMMA, in a more formal way than before, including a pseudocode general enough to handle different blocking parameters, rectangular matrices, and rectangular processor grids. The reader interested only in parallel SpRef can skip these sections and go directly to Section 4, where we describe our SpRef algorithm, its novel parallelization and its analysis. Section 5 gives an extensive performance evaluation of these two primitives using large scale parallel experiments, including a performance comparison with similar primitives from the Trilinos package. Various implementation decisions and their effects on performance are also detailed.\n\n2. Notation. Let A \u2208 S m\u00d7n be a sparse rectangular matrix of elements from a semiring S. We use nnz (A) to denote the number of nonzero elements in A. When the matrix is clear from context, we drop the parenthesis and simply use nnz . For sparse matrix indexing, we use the convenient Matlab colon notation, where A(:, i) denotes the ith column, A(i, :) denotes the ith row, and A(i, j) denotes the element at the (i, j)th position of matrix A. Array and vector indices are 1-based throughout this paper. The length of an array I, denoted by len(I), is equal to its number of elements. For one-dimensional arrays, I(i) denotes the ith component of the array. We use flops(A \u00b7 B), pronounced \"flops\", to denote the number of nonzero arithmetic operations required when computing the product of matrices A and B. Since the flops required to form the matrix triple product differ depending on the order of multiplication, flops((AB) \u00b7 C) and flops(A \u00b7 (BC)) mean different things. The former is the flops needed to multiply the product AB with C, where the latter is the flops needed to multiply A with the product BC. When the operation and the operands are clear from context, we simply use flops. The Matlab sparse(i,j,v,m,n) function, which is used in some of the pseudocode, creates an m \u00d7 n sparse matrix A with nonzeros A(i(k), j(k)) = v(k).\n\nIn our analyses of parallel running time, the latency of sending a message over the communication interconnect is \u03b1, and the inverse bandwidth is \u03b2, both expressed in terms of time for a floating-point operation (also accounting for the cost of cache misses and memory indirections associated with that floating point operation). f (x) = \u0398(g(x)) means that f is bounded asymptotically by g both above and below.\n\n3. Sparse matrix-matrix multiplication. SpGEMM is a building block for many high-performance graph algorithms, including graph contraction [25], breadthfirst search from multiple source vertices [10], peer pressure clustering [34], recursive all-pairs shortest-paths [19], matching [33], and cycle detection [38]. It is a subroutine in more traditional scientific computing applications such as multigrid interpolation and restriction [5] and Schur complement methods in hybrid linear solvers [37]. It also has applications in general computing, including parsing context-free languages [32] and colored intersection searching [29].\n\nThe classical serial SpGEMM algorithm for general sparse matrices was first described by Gustavson [26], and was subsequently used in Matlab [24] and CSparse [20]. That algorithm, shown in Figure 3.1, runs in O(flops + nnz +n) time, which is optimal for flops \u2265 max{nnz , n}. It uses the popular compressed sparse column (CSC) format for representing its sparse matrices. Algorithm 1 gives the pseudocode for this column-wise serial algorithm for SpGEMM.\n\n3.1. Distributed memory SpGEMM. The first question for distributed memory algorithms is \"where is the data?\". In parallel SpGEMM, we consider two ways of distributing data to processors. In 1D algorithms, each processor stores a block of m/p rows of an m-by-n sparse matrix. In 2D algorithms, processors are logically organized as a rectangular p = p r \u00d7 p c grid, so that a typical processor is named  [11]. Columns of A are accumulated as specified by the non-zero entries in a column of B using a sparse accumulator or SPA [24]. The contents of the SPA are stored into a column of C once all required columns are accumulated. for j \u2190 1 to n do 3: for k where B(k, j) = 0 do 4: C(:, j) \u2190 C(:, j) + A(:, k) \u00b7 B(k, j) P (i, j). Submatrices are assigned to processors according to a 2D block decomposition: processor P (i, j) stores the submatrix A ij of dimensions (m/p r ) \u00d7 (n/p c ) in its local memory. We extend the colon notation to slices of submatrices: A i: denotes the (m/p r ) \u00d7 n slice of A collectively owned by all the processors along the ith processor row and A :j denotes the m \u00d7 (n/p c ) slice of A collectively owned by all the processors along the jth processor column.\nB = x C A SPA gather scatter/ accumulate\nWe have previously shown that known 1D SpGEMM algorithms are not scalable to thousands of processors [7], while 2D algorithms can potentially speed up indefinitely, albeit with decreasing efficiency. There are two reasons that the 1D algorithms do not scale: First, their auxiliary data structures cannot be loaded and unloaded fast enough to amortize their costs. This loading and unloading is necessary because the 1D algorithms proceed in stages in which only one processor broadcasts its submatrix to the others, in order to avoid running out of memory. Second, and more fundamentally, the communication costs of 1D algorithms are not scalable regardless of data structures. Each processor receives nnz (of either A or B) data in the worst case, which implies that communication cost is on the same order as computation, prohibiting speedup beyond a fixed number of processors. This leaves us with 2D algorithms for a scalable solution.\n\nOur previous work [8] shows that the standard compressed column or row (CSC or CSR) data structures are too wasteful for storing the local submatrices arising CP =  1  3 3 3 3 3  3  4 5 5\n\u2193 \u2193 \u2193 IR = 6 8 4 2 NUM = 0.1 0.2 0.3 0.4 Fig. 3.2: Matrix A in CSC format\nfrom a 2D decomposition. This is because the local submatrices are hypersparse, meaning that the ratio of nonzeros to dimension is asymptotically zero. The total memory across all processors for CSC format would be O(n \u221a p + nnz ), as opposed to O(n + nnz ) memory to store the whole matrix in CSC on a single processor. Thus a scalable parallel 2D data structure must respect hypersparsity. Similarly, any algorithm whose complexity depends on matrix dimension, such as Gustavson's serial SpGEMM algorithm, is asymptotically too wasteful to be used as a computational kernel for multiplying the hypersparse submatrices. Our Hy-perSparseGEMM [6,8], on the other hand, operates on the strictly O(nnz ) doubly compressed sparse column (DCSC) data structure, and its time complexity does not depend on the matrix dimension. Section 3.2 gives a succinct summary of DCSC.\n\nOur HyperSparseGEMM uses an outer-product formulation whose time complexity is O(nzc(A) + nzr (B) + flops \u00b7 lg ni ), where nzc(A) is the number of columns of A that are not entirely zero, nzr (B) is the number of rows of B that are not entirely zero, and ni is the number of indices i for which A(:, i) = \u2205 and B(i, :) = \u2205. The extra lg ni factor at the time complexity originates from the priority queue that is used to merge ni outer products on the fly. The overall memory requirement of this algorithm is the asymptotically optimal O(nnz (A) + nnz (B) + nnz (C)), independent of either matrix dimensions or flops.\n\n3.2. DCSC Data Structure. DCSC [8] is a further compressed version of CSC where repetitions in the column pointers array, which arise from empty columns, are not allowed. Only columns that have at least one nonzero are represented, together with their column indices.  Figure 3.4. DCSC is essentially a sparse array of sparse columns, whereas CSC is a dense array of sparse columns.\n! \" #$\" % \" #$& ' ( #$) & % #$' !$* !$+ !$,\nAfter removing repetitions, CP(i) does no longer refer to the ith column. A new JC array, which is parallel to CP, gives us the column numbers. Although our Hypersparse GEMM algorithm does not need column indexing, DCSC can support fast column indexing by building an AUX array that contains pointers to nonzero columns (columns that have at least one nonzero element) in linear time.\n\n3.3. Sparse SUMMA algorithm. Our parallel algorithm is inspired by the dense matrix-matrix multiplication algorithm SUMMA [23], used in parallel BLAS [16]. SUMMA is memory efficient and easy to generalize to non-square matrices and processor grids.\n\nThe pseudocode of our 2D algorithm, SparseSUMMA [7], is shown in Algorithm 2 in its most general form. The coarseness of the algorithm can be adjusted by changing the block size 1 \u2264 b \u2264 gcd(k/p r , k/p c ). For the first time, we present the algorithm in a form general enough to handle rectangular processor grids and a wide range of blocking parameter choices. The pseudocode, however, requires b to evenly divide k/p r and k/p c for ease of presentation. This requirement can be dropped at the expense of having potentially multiple broadcasters along a given processor row and column during one iteration of the loop starting at line 4. The for . . . in parallel do construct indicates that all of the do code blocks execute in parallel by all the processors. The execution of the algorithm on a rectangular grid with rectangular sparse matrices is illustrated in Figure 3.5. We refer to the Combinatorial BLAS source code [2] for additional details.\n\nAlgorithm 2 Operation C \u2190 AB using Sparse SUMMA Input: A \u2208 S m\u00d7k , B \u2208 S k\u00d7n : sparse matrices distributed on a p r \u00d7 p c processor grid Output: C \u2208 S m\u00d7n : the product AB, similarly distributed. for all processors P (i, j) in parallel do 3:\nB ij \u2190 (B ij ) T\n\n4:\n\nfor q = 1 to k/b do blocking parameter b evenly divides k/p r and k/p c 5: c = (q \u00b7 b)/p c c is the broadcasting processor column 6: r = (q \u00b7 b)/p r r is the broadcasting processor row 7: lcols = (q \u00b7 b) mod p c : ((q + 1) \u00b7 b) mod p c local column range 8: lrows = (q \u00b7 b) mod p r : ((q + 1) \u00b7 b) mod p r local row range\n\n\n9:\n\nA rem \u2190 Broadcast(A ic (:, lcols), P (i, :)) 10:\n\nB rem \u2190 Broadcast(B rj (:, lrows), P (:, j)) 11:\nC ij \u2190 C ij + HyperSparseGEMM(A rem , B rem ) 12: B ij \u2190 (B ij ) T Restore the original B\nThe Broadcast(A ic , P (i, :)) syntax means that the owner of A ic becomes the root and broadcasts its submatrix to all the processors on the ith processor row. Similarly for Broadcast(B rj , P (:, j)), the owner of B rj broadcasts its submatrix to all the processors on the jth processor column. In lines 7-8, we find the local column (for A) and row (for B) ranges for matrices that are to be broadcast during that iteration. They are significant only at the broadcasting processors, which can be determined implicitly from the first parameter of Broadcast. We index B by columns as opposed to rows because it has already been locally transposed in line 3. This makes indexing faster since local submatrices are stored in the column-based DCSC sparse data structure. Using DCSC, the expected cost of fetching b consecutive columns of a matrix A is b plus the size (number of nonzeros) of the output. Therefore, the algorithm asymptotically has the same computation cost for all values of b.\nx = ! C ij 100K 25K 5K 25K 100K 5K A B C\nFor our complexity analysis, we assume that nonzeros of input sparse matrices are independently and identically distributed, input matrices are n-by-n, with d > 0 nonzeros per row and column on the average. The sparsity parameter d simplifies our analysis by making different terms in the complexity comparable to each other. For example, if A and B both have sparsity d, then nnz (A) = dn and flops(AB) = d 2 n.\n\nThe communication cost of the Sparse SUMMA algorithm, for the case of p r = p c = \u221a p, is\nT comm = \u221a p 2 \u03b1 + \u03b2 nnz (A) + nnz (B) p = \u0398(\u03b1 \u221a p + \u03b2 d n \u221a p ), (3.1)\nand its computation cost is\nT comp = O d n \u221a p + d 2 n p lg d 2 n p \u221a p + d 2 n lg \u221a p p = O d n \u221a p + d 2 n p lg d 2 n p [6]. (3.2)\nWe see that although scalability is not perfect and efficiency deteriorates as p increases, the achievable speedup is not bounded. Since lg(d 2 n/p) becomes negligible as p increases, the bottlenecks for scalability are the \u03b2 d n/ \u221a p term of T comm and the d n/ \u221a p term of T comp , which scale with \u221a p. Consequently, two different scaling regimes are likely to be present: A close to linear scaling regime until those terms start to dominate and a \u221a p-scaling regime afterwards.\n\n4. Sparse matrix indexing and subgraph selection. Given a sparse matrix A and two vectors I and J of indices, SpRef extracts a submatrix and stores it as another sparse matrix, B = A(I, J). Matrix B contains the elements in rows I(i) and columns J(j) of A, for i = 1, ..., len(I) and j = 1, ..., len(J), respecting the order of indices. If A is the adjacency matrix of a graph, SpRef(A, I, I) selects an induced subgraph. SpRef can also be used to randomly permute the rows and columns of a sparse matrix, a primitive in parallel matrix computations commonly used for load balancing [31].\n\nSimple cases such as row (A(i, :)), column (A(:, i)), and element (A(i, j)) indexing are often handled by special purpose subroutines [11]. A parallel algorithm for the general case, where I and J are arbitrary vectors of indices, does not exist in the literature. We propose an algorithm that uses parallel SpGEMM. Our algorithm is amenable to performance analysis for the general case.\n\nA related kernel is SpAsgn, or sparse matrix assignment. This operation assigns a sparse matrix to a submatrix of another sparse matrix, A(I, J) = B. A variation of SpAsgn is A(I, J) = A(I, J) + B, which is similar to Liu's extend-add operation [30] in finite element matrix assembly. Here we describe the sequential SpAsgn algorithm and its analysis, and report large-scale performance results in Section 5.2. The sequential complexity of this algorithm is flops(R \u00b7 A) + flops((RA) \u00b7 Q). Due to the special structure of the permutation matrices, the number of nonzero operations required to form the product R \u00b7 A is equal to the number of nonzero elements in the product. That is, flops(R \u00b7 A) = nnz (RA) \u2264 nnz (A). Similarly, flops((RA) \u00b7 Q) \u2264 nnz (A), making the overall complexity O(nnz (A)) for any I and J. This is optimal in general, since just writing down the result of a matrix permutation B = A(r, r) requires \u2126(nnz (A)) operations.\n!\" #\" #\" \"$\" m len(I) ! ! ! ! \" ! \" ! ! ! \" \" ! ! ! \" ! ! ! ! %\" &\" '\" n len(J)\nPerforming SpAsgn by two triple sparse-matrix products and additions is illustrated in Figure 4.2. We create two temporary sparse matrices of the same dimensions as A. These matrices contain nonzeros only for the A(I, J) part, and zeros elsewhere. The first triple product embeds B into a bigger sparse matrix that we add to A. The second triple product embeds A(I, J) into an identically sized sparse matrix so that we can zero out the A(I, J) portion by subtracting it from A. Since general semiring axioms do not require additive inverses to exist, we implement this piece of the algorithm slightly differently that stated in the pseudocode. We still form the SAT product but instead of using subtraction, we use the generalized sparse elementwise multiplication function of the Combinatorial BLAS [10] to zero out the A(I, J) portion.\nA = A + \uf8eb \uf8ed 0 0 0 0 B 0 0 0 0 \uf8f6 \uf8f8 \u2212 \uf8eb \uf8ed 0 0 0 0 A(I, J) 0 0 0 0 \uf8f6 \uf8f8 Fig\nIn particular, we first perform an elementwise multiplication of A with the negation of SAT without explicitly forming the negated matrix, which can be dense. Thanks to this direct support for the implicit negation operation, the complexity bounds are identical to the version that uses subtraction. The negation does not assume additive inverses: it sets all zero entries to one and all nonzeros entries to zero. The algorithm can be described concisely in Matlab notation as follows: Liu's extend-add operation is similar to SpAsgn but simpler; it just omits subtracting the SAT term.\n\nLet us analyze the complexity of SpAsgn. Given A \u2208 S m\u00d7n and B \u2208 S len(I)\u00d7len(J) , the intermediate boolean matrices have the following properties:\n\nR is m-by-len(I) rectangular with len(I) nonzeros, one in each column. Q is len(J)-by-n rectangular with len(J) nonzeros, one in each row. S is m-by-m symmetric with len(I) nonzeros, all located along the diagonal. T is n-by-n symmetric with len(J) nonzeros, all located along the diagonal.  product is nnz (SAT) = O(len(I) + len(J)). The final pointwise addition and subtraction (or generalized elementwise multiplication in the absence of additive inverses) operations take time on the order of the total number of nonzeros in all operands [11], which is O(nnz (A) + nnz (B) + len(I) + len(J)).\n\n\nSpRef in parallel.\n\nThe parallelization of SpRef poses several challenges. The boolean matrices have only one nonzero per row or column. For the parallel 2D algorithm to scale well with increasing number of processors, data structures and algorithms should respect hypersparsity [8]. Communication should ideally take place along a single processor dimension, to save a factor of \u221a p in communication volume.\n\nAs before, we assume a uniform distribution of nonzeros to processors in our analysis. The communication cost of forming the R matrix in parallel is the cost of Scatter along the processor column. For the case of vector I distributed to \u221a p diagonal processors, scattering can be implemented with an average communication cost of \u0398(\u03b1 \u00b7 lg p + \u03b2 \u00b7 (len(I)/ \u221a p) [14]. This process is illustrated in Figure 4.3. The Q T matrix can be constructed identically, followed by a Transpose(Q T ) operation where each processor P (i, j) receives nnz (Q)/p = len(J)/p words of data from its diagonal neighbor P (j, i). Note that the communication cost of the transposition is dominated by the cost of forming Q T via Scatter. While the analysis of our parallel SpRef algorithm assumes that the index vectors are distributed only on diagonal processors, the asymptotic costs are identical in the 2D case where the vectors are distributed across all the processors [12]. This is because the number of elements (the amount of data) received by a given processor stays the same with the only difference in the algorithm being the use of Alltoall operation instead of Scatter during the formation of the R and Q matrices.\n\nThe parallel performance of SpGEMM is a complicated function of the matrix nonzero structures [7,9]. For SpRef, however, the special structure makes our analysis more precise. Suppose that the triple product is evaluated from left to right, B = (R \u00b7 A) \u00b7 Q; a similar analysis can be applied to the reverse evaluation. A conservative estimate of ni (R, A), the number of indices i for which R(:, i) = \u2205 and A(i, :) = \u2205, is nnz (R) = len(I).\n\nUsing our HyperSparseGEMM [6,8] as the computational kernel, time to compute the product RA (excluding the communication costs) is:\nT mult = max i,j \u221a p k=1 nzc(R ik ) + nzr (A kj ) + flops(R ik \u00b7 A kj ) \u00b7 lg ni (R ik , A kj ) ,\nwhere the maximum over all (i, j) pairs is equal to the average, due to the uniform nonzero distribution assumption.\n\nRecall from the sequential analysis that flops(R \u00b7 A) \u2264 nnz (A) since each nonzero in A contributes at most once to the overall flop count. We also know that nzc(R) = len(I) and nzr (A) \u2264 nnz (A). Together with the uniformity assumption, these identities yield the following results:\nflops(R ik \u00b7 A kj ) = nnz (A) p \u221a p , ni (R ik \u00b7 A kj ) \u2264 nnz (R ik ) = len(I) p , \u221a p k=1 nzc(R ik ) = nzc(R i: ) = len(I) \u221a p , \u221a p k=1 nzr (A ik ) = nzr (A i: ) \u2264 nnz (A) \u221a p .\nIn addition to the multiplication costs, adding intermediate triples in Thus, we have the following estimates of computation and communication costs for computing the product RA:\nT comp (R \u00b7 A) = O len(I) + nnz (A) \u221a p + nnz (A) p \u00b7 lg len(I) p + \u221a p , T comm (R \u00b7 A) = \u0398(\u03b1 \u00b7 \u221a p + \u03b2 \u00b7 nnz (A) \u221a p ).\nGiven that nnz (RA) \u2264 nnz (A), the analysis of multiplying the intermediate product RA with Q is similar. Combined with the cost of forming auxiliary matrices R and Q and the costs of transposition of Q T , the total cost of the parallel SpRef algorithm becomes\nT comp = O len(I) + len(J) + nnz (A) \u221a p + nnz (A) p \u00b7 lg len(I) + len(J) p + \u221a p , T comm = \u0398 \u03b1 \u00b7 \u221a p + \u03b2 \u00b7 nnz (A) + len(I) + len(J) \u221a p .\nWe see that SpGEMM costs dominate the cost of SpRef. The asymptotic speedup is limited to \u0398( \u221a p), as in the case of SpGEMM.\n\n\nExperimental Results.\n\nWe ran experiments on NERSC's Franklin system [1], a 9660-node Cray XT4. Each XT4 node contains a quad-core 2.3 GHz AMD Opteron processor, attached to the XT4 interconnect via a Cray SeaStar2 ASIC using a HyperTransport 2 interface capable of 6.4 GB/s. The SeaStar2 routing ASICs are connected in a 3D torus topology, and each link is capable of 7.6 GB/s peak bidirectional bandwidth. Our algorithms perform similarly well on a fat tree topology, as evidenced by our experimental results on the Ranger platform that are included in an earlier technical report [9]. We used the GNU C/C++ compilers (version 4.5), and Cray's MPI implementation, which is based on MPICH2. We incorporated our code into the Combinatorial BLAS framework [10]. We experimented with core counts that are perfect squares, because the Combinatorial BLAS currently uses a square \u221a p \u00d7 \u221a p processor grid. We compared performance with the Trilinos package (version 10.6.2.0) [28], which uses a 1D decomposition for its sparse matrices.\n\nIn the majority of our experiments, we used synthetically generated R-MAT matrices rather than Erd\u0151s-R\u00e9nyi [22] \"flat\" random matrices, as these are more realistic for many graph analysis applications. R-MAT [13], the Recursive MATrix generator, generates graphs with skewed degree distributions that approximate a power-law. A scale n R-MAT matrix is 2 n -by-2 n . Our R-MAT matrices have an average of 8 nonzeros per row and column. R-MAT seed paratemeters are a = .6, and b = c = d = .4/3. We applied a random symmetric permutation to the input matrices to balance the memory and the computational load. In other words, instead of storing and computing C = AB, we compute PCP T = (PAP T )(PBP T ). All of our experiments are performed on double-precision floating-point inputs.\n\nSince algebraic multigrid on graphs coming from physical problems is an important case, we included two more matrices from the Florida Sparse Matrix collection [21] to our experimental analysis, into Section 5.3.2, where we benchmark restriction operation that is used in algebraic multigrid. The first such matrix is a large circuit problem (Freescale1) with 17 million nonzeros and 3.42 million rows and columns. The second matrix comes from a structural problem (GHS psdef/ldoor), and has 42.5 million nonzeros and 952, 203 rows and columns.\n\n\nParallel Scaling of SpRef.\n\nOur first set of experiments randomly permutes the rows and columns of A, as an example case study for matrix reordering and partitioning. This operation corresponds to relabeling vertices of a graph. Our second set of experiments explores subgraph extraction by generating a random permutation of 1 : n and dividing it into k n chunks r 1 , . . . , r k . We then performed k SpRef operations of the form A(r i , r i ), one after another (with a barrier in between). In both cases, the sequential reference is our algorithm itself.\n\nThe performance and parallel scaling of the symmetric random permutation is shown in Figure 5.1. The input is an R-MAT matrix of scale 22 with approximately 32 million nonzeros in a square matrix of dimension 2 22 . Speedup and runtime are plotted on different vertical axes. We see that scaling is close to linear up to about 64 processors, and proportional to \u221a p afterwards, agreeing with our analysis.\n\nThe performance of subgraph extraction for k = 10 induced subgraphs, each with n/k randomly chosen vertices, is shown in Figure 5.2. The algorithm performs well in this case too. The observed scaling is slightly less than the case of applying a single big permutation, which is to be expected since the multiple small subgraph extractions increase span and decrease available parallelism.\n\n\nParallel Scaling of SpAsgn.\n\nWe benchmarked our parallel SpAsgn code by replacing a portion of the input matrix (A) with a structurally similar right-hand side matrix (B). This operation is akin to replacing a portion of the graph due to a streaming update. The subset of vertices (row and column indices of A) to be updated is chosen randomly. In all the tests, the original graph is an R-MAT matrix of scale 22 with 32 million nonzeros. The right-hand side (replacement) matrix is also an R-MAT matrix of scales 21, 20, and 19, in three subsequent experiments, replacing 50%, 25%, and 12.5% of the original graph, respectively. The average number of nonzeros per row and column are also adjusted for the right hand side matrices to match the nonzero density of the subgraphs they are replacing.\n\nThe performance of this sparse matrix assignment operation is shown in A sparse matrix constructor from distributed vectors, essentially a parallel version of Matlab's sparse routine, the generalized elementwise multiplication with direct support for negation, and parallel SpGEMM implemented using Sparse SUMMA.\n\n\nParallel\n\nScaling of Sparse SUMMA. We implemented two versions of the 2D parallel SpGEMM algorithms in C++ using MPI. The first is directly based on Sparse SUMMA and is synchronous in nature, using blocking broadcasts. The second is asynchronous and uses one-sided communication in MPI-2. We found the asynchronous implementation to be consistently slower than the broadcast-based synchronous implementation due to inefficient implementation of one-sided communication routines in MPI. Therefore, we only report the performance of the synchronous implementation. The motivation behind the asynchronous approach, performance comparisons, and implementation details, can be found in our technical report [9,Section 7]. On more than 4 cores of Franklin, synchronous implementation consistently outperformed the asynchronous implementation by 38-57%.\n\nOur sequential HyperSparseGEMM routines return a set of intermediate triples that are kept in memory up to a certain threshold without being merged immediately. This permits more balanced merging, eliminating some unnecessary scans that degraded performance in a preliminary implementation [7].\n\n\nSquare Sparse Matrix Multiplication.\n\nIn the first set of experiments, we multiply two structurally similar R-MAT matrices. This square multiplication is representative of the expansion operation used in the Markov clustering algorithm [36]. It is also a challenging case for our implementation due to the highly skewed nonzero distribution. We performed strong scaling experiments for matrix dimensions ranging from 2 21 to 2 24 . Figure 5.4 shows the speedup we achieved. The graph shows linear speedup until around 100 processors; afterwards the speedup is proportional to the square root of the number of processors. Both results agree with the theoretical analysis. To  illustrate how the scaling transitions from linear to \u221a p, we drew trend lines on the scale 21 results. As shown in Figure 5.6, the slope of the log-log curve is 0.85 (close to linear) until 121 cores, and the slope afterwards is 0.47 (close to \u221a p). Figure 5.7\n\nzooms to the linear speedup regime, and shows the performance of our algorithm at lower concurrencies. The speedup and timings are plotted on different y-axes of the same graph. Our implementation of Sparse SUMMA achieves over 2 billion \"useful flops\" (in double precision) per second on 8100 cores when multiplying scale 24 R-MAT matrices. Since useful flops are highly dependent on the matrix structure and sparsity, we provide additional statistics for this operation in Table 5.5. Using matrices with more nonzeros per row and column will certainly yield higher performance rates (in useful flops). The gains from sparsity are clear if one considers dense flops that would be needed if these matrices were stored in a dense format. For example, multiplying two dense scale 24 matrices requires 9444 exaflops. Figure 5.8 breaks down the time spent in communication and computation when multiplying two R-MAT graphs of scale 24. We see that computation scales much better than communication (over 90x reduction when going from 36 to 8100 cores), implying that SpGEMM is communication bound for large concurrencies. For example, on 8100 cores, 83% of the time is spent in communication. Communication times include the overheads due to synchronization and load imbalance. Figure 5.8 also shows the effect of different blocking sizes. Remember that each processor owns a submatrix of size n/ \u221a p-by-n/ \u221a p. On the left, the algorithm completes in \u221a p stages, each time broadcasting its whole local matrix. On the right, the algorithm completes in 2 \u221a p stages, each time broadcasting half of its local matrix.\n\nWe see that while communication costs are not affected, the computation slows down by 1-6% when doubling the number of stages. This difference is due to the costs of splitting the input matrices before the multiplication and reassembling them afterwards, which is small because splitting and reassembling are simple scans over the data whose costs are dominated by the cost of multiplication itself.\n\n\nMultiplication with the Restriction Operator.\n\nMultilevel methods are widely used in the solution of numerical and combinatorial problems [35]. Such methods construct smaller problems by successive coarsening. The simplest coarsening is graph contraction: a contraction step chooses two or more vertices in the original graph G to become a single aggregate vertex in the contracted graph G . The edges of G that used to be incident to any of the vertices forming the aggregate become incident to the new aggregate vertex in G . Constructing coarse grid during the V-cycle of algebraic multigrid [5] or graph partitioning [27] is a generalized graph contraction operation. Different algorithms need different coarsening operators. For example, a weighted aggregation [15] might be preferred for partitioning problems. In general, coarsening can be represented as multiplication of the matrix representing the original fine domain (grid, graph, or hypergraph) by the restriction operator.\n\nIn these experiments, we use a simple restriction operation to perform graph contraction. Gilbert et al. [25] describe how to perform contraction using SpGEMM. Their algorithm creates a special sparse matrix S with n nonzeros. The triple product SAS T contracts the whole graph at once. Making S smaller in the first dimension while keeping the number of nonzeros same changes the restriction order. For example, we contract the graph into half by using S having dimensions n/2 \u00d7 n, which is said to be of order 2. Figure 5.9 shows 'strong scaling' of AS T operation for R-MAT graphs of scale 23. We used restrictions of order 2, 4, and 8. Changing the interpolation order results in minor (less than 5%) changes in performance, as shown by the overlapping curves. This is further evidence that our algorithm's complexity is independent of the matrix dimension, because interpolation order has a profound effect on the dimension of the right hand side matrix, but it does not change the expected flops and numbers of nonzeros in the inputs (it may slightly decrease the number of nonzeros in the output). The experiment shows scaling up to 4096 processors. Figure 5.10 shows the breakdown of time (as percentages) spent on remote communication and local computation steps. Figures 5.11a and 5.11b show 'strong scaling' of the full restriction operation SAS T of order 8, using different parenthesizations for the triple product. The results show that our code achieves 110\u00d7 speedup on 1024-way concurrency and 163\u00d7 speedup on 4096-way concurrency, and the performance is not affected by the different parenthesizations. Figure 5.12 shows the performance of full operation on real matrices from phys-   ical problems. Both matrices have a full diagonal that remains full after symmetric permutation. Due to the 2D decomposition, processors responsible for the diagonal blocks typically have more work to do. For load-balancing and performance reasons, we split these matrices into two pieces A = D + L where D is the diagonal piece and L is the off-diagonal piece. The restriction of rows becomes SA = SD + SL. Scaling the columns of S with the diagonal of D performs the former multiplication, and the latter multiplication uses Sparse SUMMA algorithm described in our paper. This splitting approach especially improved the scalability of restriction on Freescale1 matrix, because it is much sparser that GHS psdef/ldoor, which does not face severe load balancing issues. Order 2 restriction shrinks the number of nonzeros from 17.0 to 15.3 million for Freescale1, and from 42.5 to 42.0 million for GHS psdef/ldoor.\n\n\nTall Skinny Right\n\nHand Side Matrix. The last set of experiments multiplies R-MAT matrices by tall skinny matrices of varying sparsity. This computation is representative of the parallel breadth-first search that lies at the heart of our distributed-memory betweenness centrality implementation [10]. This set indirectly   examines the sensitivity to sparsity as well, because we vary the sparsity of the right hand side matrix from approximately 1 to 10 5 nonzeros per column, in powers of 10. In this way, we imitate the patterns of the level-synchronous breadth-first search from multiple source vertices where the current frontier can range from a few vertices to hundreds of thousands [12].\n\nFor our experiments, the R-MAT matrices on the left hand side have d 1 = 8 nonzeros per column and their dimensions vary from n = 2 20 to n = 2 26 . The righthand side is an Erd\u0151s-R\u00e9nyi matrix of dimensions n-by-k, and the number of nonzeros per column, d 2 , is varied from 1 to 10 5 , in powers of 10. The right-hand matrix's width k varies from 128 to 8192, growing proportionally to its length n, hence keeping the matrix aspect ratio constant at n/k = 8192. Except for the d 2 = 10 5 case, the R-MAT matrix has more nonzeros than the right-hand matrix. In this computation, the total work is W = O(d 1 d 2 k), the total memory consumption is M = O(d 1 n + d 2 k), and the total bandwidth requirement is O(M \u221a p).\n\nWe performed weak scaling experiments where memory consumption per processor is constant. Since M = O(d 1 n + d 2 k), this is achieved by keeping both n/p = 2 14 and k/p = 2 constant. Work per processor is also constant. However, per-processor bandwidth requirements of this algorithm increases by a factor of \u221a p. The performance we achieved for these large scale experiments, where we ran our code on up to 4096 processors, is remarkable. It also shows that our implementation does not incur any significant overheads since it does not deviate from the \u221a p curve.\n\n\nComparison with\n\nTrilinos. The EpetraExt package of Trilinos can multiply two distributed sparse matrices in parallel. Trilinos can also permute matrices and extract submatrices through its Epetra Import and Epetra Export classes. These packages of Trilinos use a 1D data layout.\n\nFor SpGEMM, we compared the performance of Trilinos's EpetraExt package with ours on two scenarios. In the first scenario, we multiplied two R-MAT matrices as described in Section 5.3.1, and in the second scenario, we multiplied an R-MAT matrix with the restriction operator of order 8 on the right as described in Section 5.3.2.\n\nTrilinos ran out of memory when multiplying R-MAT matrices of scale larger than 21, or when using more than 256 processors. Figure 5.14a shows SpGEMM timings for up to 256 processors on scale 21 data. Sparse SUMMA is consistently faster than Trilinos's implementation, with the gap increasing with the processor count, reaching 66\u00d7 on 256-way concurrency. Sparse SUMMA is also more memory efficient as Trilinos's matrix multiplication ran out of memory for p = 1 and p = 4 cores. The sweet spot for Trilinos seems to be around 120 cores, after which its performance degrades significantly.\n\nIn the case of multiplying with the restriction operator, the speed and scalability of our implementation over EpetraExt is even more pronounced. This is shown in Figure 5.14b where our code is 65X faster even on just 121 processors. Remarkably, our codes scales up to 4096 cores on this problem, as shown in Section 5.3.2, while EpetraExt starts to slow down just beyond 16 cores. We also compared Sparse SUMMA with EpetraExt on matrices coming from physical problems, and the results for the full restriction operation (SAS T ) are shown in Figures 5.15.\n\nIn order to benchmark Trilinos's sparse matrix indexing capabilities, we used Epe-traExt's permutation class that can permute row or columns of an Epetra CrsMatrix by creating a map defined by the permutation, followed by an Epetra Export operation to move data from the input object into the permuted object. We applied a random symmetric permutation on a R-MAT matrix, as done in Section 5.1. Trilinos shows good scaling up to 121 cores but then it starts slowing down as concurrency increases, eventually becoming over 10\u00d7 slower than our SpRef implementation at     6. Conclusions and Future Work. We presented a flexible parallel sparse matrix-matrix multiplication (SpGEMM) algorithm, Sparse SUMMA, which scales to thousands of processors in distributed memory. We used Sparse SUMMA as a building block to design and implement scalable parallel routines for sparse matrix indexing (SpRef) and assignment (SpAsgn). These operations are important in the context of graph operations. They yield elegant algorithms for coarsening graphs by edge contraction as in Figure 6.1, extracting subgraphs, performing parallel breadthfirst search from multiple source vertices, and performing batch updates to a graph.\n\nWe performed parallel complexity analyses of our primitives. In particular, us- ing SpGEMM as a building block enabled the most general analysis of SpRef. Our extensive experiments confirmed that our implementation achieves the performance predicted by our analyses. Our SpGEMM routine might be extended to handle matrix chain products. In particular, the sparse matrix triple product is used in the coarsening phase of algebraic multigrid [3]. Sparse matrix indexing and parallel graph contraction also require sparse matrix triple products [25]. Providing a first-class primitive for sparse matrix chain products would eliminate temporary intermediate products and allow more optimization, such as performing structure prediction [17] and determining the best order of multiplication based on the sparsity structure of the matrices.\n\nAs we show in Section 5.3, our implementation spends more than 75% of its time in inter-node communication after 2000 processors. Scaling to higher concurrencies require asymptotic reductions in communication volume. We are working on developing practical communication-avoiding algorithms [4] for sparse matrix-matrix multiplication (and consequently for sparse matrix indexing and assignment), which might require inventing efficient novel sparse data structures to support such algorithms.\n\nOur preliminary experiments suggest that synchronous algorithms for SpGEMM cause considerably higher load imbalance than asynchronous ones [9,Section 7]. In particular, a truly one-sided implementation can perform up to 46% faster when multiplying two R-MAT matrices of scale 20 using 4000 processors. We will experiment with partitioned global address space (PGAS) languages, such as UPC [18], because the current implementations of one-sided MPI-2 were not able to deliver satisfactory performance when used to implement asynchronous versions of our algorithms.\n\nAs the number of cores per node increases due to multicore scaling, so does the contention on the network interface card. Without hierarchical parallelism that exploits the faster on-chip network, the flat MPI parallelism will be unscalable because more processes will be competing for the same network link. Therefore, designing hierarchically parallel SpGEMM and SpRef algorithms is an important future direction.\n\nFig. 3 . 1 :\n31Multiplication of sparse matrices stored by columns\n\nAlgorithm 1\n1Column-wise formulation of serial matrix multiplication 1: procedure Columnwise-SpGEMM(A, B, C) 2:\n\nFig. 3 Fig. 3 . 4 :\n334Matrix A in DCSC format For example, consider the 9-by-9 matrix with 4 nonzeros as in Figure 3.3. Figure 3.2 showns its CSC storage, which includes repetitions and redundancies in the column pointers array (CP). Our new data structure compresses this column pointers array to avoid repetitions, giving CP of DCSC as in\n\nFig. 3 . 5 :\n35Execution of the Sparse SUMMA algorithm for sparse matrix-matrix multiplication C = A \u00b7 B. The example shows the first stage of the algorithm execution (the broadcast and the local update by processor P (i, j)). The two rectangular sparse operands A and B are of sizes m-by-100K and 100K-by-n, distributed on a 5 \u00d7 4 processor grid. Block size b is chosen to be 5K.\n\nFig. 4 . 1 :\n41Sparse matrix indexing (SpRef) using mixed-mode SpGEMM. On an mby-n matrix A, the SpRef operation A(I, J) extracts a len(I)-by-len(J) submatrix, where I is a vector of row indices and J is a vector of column indices. The example shows B = A([2, 4], [1, 2, 3]). It performs two SpGEMM operations between a boolean matrix and a general-type matrix.4.1. Sequential algorithms for SpRef and SpAsgn. Performing SpRef by a triple sparse-matrix product is illustrated inFigure 4.1. The algorithm can be described concisely in Matlab notation as follows:\n\n. 4. 2 :\n2Illustration of SpAsgn (A(I, J) = B) for rings where additive inverses are defined. For simplicity, the vector indices I and J are shown as contiguous, but they need not be.\n\nTheorem 4 . 1 .\n41The sequential SpAsgn algorithm takes O(nnz (A) + nnz (B) + len(I) + len(J)) time using an optimal \u0398(flops) SpGEMM subroutine.Proof. The product R \u00b7 B requires flops(R \u00b7 B) = nnz (RB) = nnz (B) operations because there is a one-to-one relationship between nonzeros in the output and flops performed. Similarly, flops((RB) \u00b7 Q) = nnz (RBQ) = nnz (B), yielding \u0398(nnz (B)) complexity for the first triple product. The product S \u00b7 A only requires len(I) flops since it does not need to touch nonzeros of A that do not contribute to A(I, :). Similarly, (SA) \u00b7 T requires only len(J) flops. The number of nonzeros in the second triple\n\n\nforming of the left hand side boolean matrix R from the index vector I on 9 processors in a logical 3 \u00d7 3 grid. R will be subsequently multiplied with A to extract 6 rows out of 9 from A and order them as {7, 2, 5, 8, 1, 3}.\n\n\n\u221a p stages costs an extra flops(R i: \u00b7 A :j ) lg \u221a p = (nnz (A)/p) lg \u221a p operations per processor.\n\nFig. 5 . 1 :Fig. 5 . 2 :\n5152Performance and parallel scaling of applying a random symmetric permutation to an R-MAT matrix of scale 22. The x-axis uses a log scale. Performance and parallel scaling of extracting 10 induced subgraphs from an R-MAT matrix of scale 22. The x-axis uses a log scale.\n\nFig. 5 . 3 :\n53Observed scaling of the SpAsgn operation A(I, I) = B where A \u2208 R n\u00d7n is an R-MAT matrix of scale 22 and B is another R-MAT matrix whose scale is shown in the figure legend. I is a duplicate-free sequence with entries randomly selected from the range 1...n; its length matches the dimensions of B. Both axes are log scale.\n\nFig. 5 . 4 :\n54Figure 5.3. Our implementation uses a small number of Combinatorial BLAS routines: Observed scaling of synchronous Sparse SUMMA for the R-MAT \u00d7 R-MAT product on matrices having dimensions 2 21 \u2212 2 24 . Both axes are log scale.\n\nFig. 5 . 5 :\n55Statistics about R-MAT product C = A\u00b7B. All numbers (except scale) are in millions. of two scaling regimes for scale 21 R-MAT product.\n\nFig. 5 . 8 :\n58Communication and computation breakdown, at various concurrencies and two blocking sizes (scale 24 inputs).\n\nFig. 5 . 9 :\n59Strong scaling of B \u2190 AS T , multiplying scale 23 R-MAT matrices with the restriction operator on the right. The x-axis uses a log scale.\n\nFig. 5 .\n510: Normalized communication and computation breakdown for multiplying scale 23 R-MAT matrices with the restriction operator of order 4.\n\n\nto left evaluation: S(AS T ) Fig. 5.11: The full restriction operation of order 8 applied to a scale 23 R-MAT matrix.\n\nFig. 5 .\n512: Performance and strong scaling of Sparse SUMMA implementation for the full restriction of order 2 (SAS T ) on real matrices from physical problems.\n\nFigure 5 .\n513 shows a performance graph in three dimensions. The timings for each slice along the XZ-plane (i.e. for every d 2 = {1, 10, ..., 10 5 } contour) are normalized to the running time on 64 processors. We do not cross-compare the absolute performances for different d 2 values, as our focus in this section is parallel scaling. In line with the theory, we observe the expected \u221a p slowdown due to communication costs.\n\nFig. 5 .\n513: Weak scaling of R-MAT times a tall skinny Erd\u0151s-R\u00e9nyi matrix. X (processors) and Y (nonzeros per column on fringe) axes are logarithmic, whereas Z (normalized time) axis is linear.\n\n\n) R-MAT \u00d7 R-MAT product (scale 21).\n\n\nof an R-MAT matrix of scale 23 with the restriction operator of order 8.\n\nFig. 5 .\n514:  Comparison of SpGEMM implementation of Trilinos's EpetraExt package with our Sparse SUMMA implementation using synthetically generated matrices. The data labels on the plots show the speedup of Sparse SUMMA over EpetraExt.\n\nFig. 5 .\n515: Comparison of Trilinos's EpetraExt package with our Sparse SUMMA implementation for the full restriction of order 2 (SAS T ) on real matrices. The data labels on the plots show the speedup of Sparse SUMMA over EpetraExt.169 cores.\n\nFig. 6 . 1 :\n61Example of graph coarsening using edge contraction, which can be implemented via a triple sparse matrix product SAS T where S is the restriction operator.\n\n. Nersc &apos; Franklin, Cray, System, Franklin, Nersc's Cray XT4 System. http://www.nersc.gov/users/computational-systems/ franklin/.\n\n. Blas Combinatorial, Library, MPI reference implementationCombinatorial BLAS Library (MPI reference implementation). http://gauss.cs.ucsb.edu/ aydin/CombBLAS/html/index.html, 2012.\n\nParallel multigrid solver for 3d unstructured finite element problems. Mark Adams, James W Demmel, Supercomputing '99: Proceedings of the 1999 ACM/IEEE conference on Supercomputing. New York, NY, USAACM27Mark Adams and James W. Demmel. Parallel multigrid solver for 3d unstructured finite element problems. In Supercomputing '99: Proceedings of the 1999 ACM/IEEE conference on Supercomputing, page 27, New York, NY, USA, 1999. ACM.\n\nMinimizing communication in numerical linear algebra. Grey Ballard, James Demmel, Olga Holtz, Oded Schwartz, SIAM. J. Matrix Anal. & Appl. 32Grey Ballard, James Demmel, Olga Holtz, and Oded Schwartz. Minimizing communication in numerical linear algebra. SIAM. J. Matrix Anal. & Appl, 32:pp. 866-901, 2011.\n\nA multigrid tutorial: second edition. William L Briggs, Steve F Van Emden Henson, Mccormick, Society for Industrial and Applied Mathematics. William L. Briggs, Van Emden Henson, and Steve F. McCormick. A multigrid tutorial: second edition. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2000.\n\nNew ideas in sparse matrix-matrix multiplication. Ayd\u0131n Bulu\u00e7, John R Gilbert, Graph Algorithms in the Language of Linear Algebra. J. Kepner and J. GilbertSIAM, PhiladelphiaAyd\u0131n Bulu\u00e7 and John R. Gilbert. New ideas in sparse matrix-matrix multiplication. In J. Kepner and J. Gilbert, editors, Graph Algorithms in the Language of Linear Algebra. SIAM, Philadelphia. 2011.\n\nChallenges and advances in parallel sparse matrix-matrix multiplication. Ayd\u0131n Bulu\u00e7, John R Gilbert, ICPP'08: Proc. of the Intl. Conf. on Parallel Processing. Portland, Oregon, USAIEEE Computer SocietyAyd\u0131n Bulu\u00e7 and John R. Gilbert. Challenges and advances in parallel sparse matrix-matrix multiplication. In ICPP'08: Proc. of the Intl. Conf. on Parallel Processing, pages 503-510, Portland, Oregon, USA, 2008. IEEE Computer Society.\n\nOn the representation and multiplication of hypersparse matrices. Ayd\u0131n Bulu\u00e7, John R Gilbert, IPDPS'08: Proceedings of the 2008 IEEE International Symposium on Par-allel&Distributed Processing. IEEE Computer SocietyAyd\u0131n Bulu\u00e7 and John R. Gilbert. On the representation and multiplication of hypersparse matrices. In IPDPS'08: Proceedings of the 2008 IEEE International Symposium on Par- allel&Distributed Processing, pages 1-11. IEEE Computer Society, 2008.\n\nHighly parallel sparse matrix-matrix multiplication. Ayd\u0131n Bulu\u00e7, John R Gilbert, UCSB-CS-2010-10Santa BarbaraComputer Science Department, University of CaliforniaTechnical ReportAyd\u0131n Bulu\u00e7 and John R. Gilbert. Highly parallel sparse matrix-matrix multiplication. Tech- nical Report UCSB-CS-2010-10, Computer Science Department, University of California, Santa Barbara, 2010.\n\nThe Combinatorial BLAS: Design, implementation, and applications. Ayd\u0131n Bulu\u00e7, John R Gilbert, International Journal of High Performance Computing Applications (IJH-PCA). 254Ayd\u0131n Bulu\u00e7 and John R. Gilbert. The Combinatorial BLAS: Design, implementation, and applications. International Journal of High Performance Computing Applications (IJH- PCA), 25(4):496-509, 2011.\n\nImplementing sparse matrices for graph algorithms. Ayd\u0131n Bulu\u00e7, John R Gilbert, Viral B Shah, Graph Algorithms in the Language of Linear Algebra. J. Kepner and J. GilbertSIAM, PhiladelphiaAyd\u0131n Bulu\u00e7, John R. Gilbert, and Viral B. Shah. Implementing sparse matrices for graph algorithms. In J. Kepner and J. Gilbert, editors, Graph Algorithms in the Language of Linear Algebra. SIAM, Philadelphia. 2011.\n\nParallel breadth-first search on distributed memory systems. Ayd\u0131n Bulu\u00e7, Kamesh Madduri, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, SC '11. 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, SC '11New York, NY, USAACMAyd\u0131n Bulu\u00e7 and Kamesh Madduri. Parallel breadth-first search on distributed memory sys- tems. In Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, SC '11, New York, NY, USA, 2011. ACM.\n\nR-MAT: A recursive model for graph mining. Deepayan Chakrabarti, Yiping Zhan, Christos Faloutsos, SDM. SIAM. Michael W. Berry, Umeshwar Dayal, Chandrika Kamath, and David B. SkillicornDeepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos. R-MAT: A recursive model for graph mining. In Michael W. Berry, Umeshwar Dayal, Chandrika Kamath, and David B. Skillicorn, editors, SDM. SIAM, 2004.\n\nCollective communication: theory, practice, and experience. Ernie Chan, Marcel Heimlich, Avi Purkayastha, Robert A Van De Geijn, Concurrency and Computation: Practice and Experience. 1913Ernie Chan, Marcel Heimlich, Avi Purkayastha, and Robert A. van de Geijn. Collective com- munication: theory, practice, and experience. Concurrency and Computation: Practice and Experience, 19(13):1749-1783, 2007.\n\nComparison of coarsening schemes for multilevel graph partitioning. C\u00e9dric Chevalier, Ilya Safro, Learning and Intelligent Optimization: Third International Conference, LION 3. Selected Papers. Berlin, HeidelbergSpringer-VerlagC\u00e9dric Chevalier and Ilya Safro. Comparison of coarsening schemes for multilevel graph parti- tioning. In Learning and Intelligent Optimization: Third International Conference, LION 3. Selected Papers, pages 191-205, Berlin, Heidelberg, 2009. Springer-Verlag.\n\nParallel implementation of BLAS: General techniques for Level 3 BLAS. Concurrency: Practice and Experience. Almadena Chtchelkanova, John Gunnels, Greg Morrow, James Overfelt, Robert A Van De Geijn, 9Almadena Chtchelkanova, John Gunnels, Greg Morrow, James Overfelt, and Robert A. van de Geijn. Parallel implementation of BLAS: General techniques for Level 3 BLAS. Concur- rency: Practice and Experience, 9(9):837-857, 1997.\n\nStructure prediction and computation of sparse matrix products. Edith Cohen, Journal of Combinatorial Optimization. 24Edith Cohen. Structure prediction and computation of sparse matrix products. Journal of Combinatorial Optimization, 2(4):307-332, 1998.\n\nUPC language specifications, v1.2. Upc Consortium, LBNL-59208Lawrence Berkeley National LaboratoryTechnical ReportUPC Consortium. UPC language specifications, v1.2. Technical Report LBNL-59208, Lawrence Berkeley National Laboratory, 2005.\n\nR-Kleene: A high-performance divide-and-conquer algorithm for the all-pair shortest path for densely connected networks. D&apos;alberto Paolo, Alexandru Nicolau, Algorithmica. 472Paolo D'Alberto and Alexandru Nicolau. R-Kleene: A high-performance divide-and-conquer algorithm for the all-pair shortest path for densely connected networks. Algorithmica, 47(2):203-213, 2007.\n\nDirect Methods for Sparse Linear Systems. Timothy A Davis, Society for Industrial and Applied Mathematics. Timothy A. Davis. Direct Methods for Sparse Linear Systems. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2006.\n\nThe university of florida sparse matrix collection. Timothy A Davis, Yifan Hu, ACM Trans. Math. Softw. 3811Timothy A. Davis and Yifan Hu. The university of florida sparse matrix collection. ACM Trans. Math. Softw., 38(1):1, 2011.\n\nOn random graphs. Paul Erd\u0151s, Alfr\u00e9d R\u00e9nyi, Publicationes Mathematicae. 61Paul Erd\u0151s and Alfr\u00e9d R\u00e9nyi. On random graphs. Publicationes Mathematicae, 6(1):290-297, 1959.\n\nSUMMA: Scalable universal matrix multiplication algorithm. R A Van De Geijn, J Watts, Concurrency: Practice and Experience. 94R. A. Van De Geijn and J. Watts. SUMMA: Scalable universal matrix multiplication algorithm. Concurrency: Practice and Experience, 9(4):255-274, 1997.\n\nSparse matrices in Matlab: Design and implementation. John R Gilbert, Cleve Moler, Robert Schreiber, SIAM Journal of Matrix Analysis and Applications. 131John R. Gilbert, Cleve Moler, and Robert Schreiber. Sparse matrices in Matlab: Design and implementation. SIAM Journal of Matrix Analysis and Applications, 13(1):333-356, 1992.\n\nA unified framework for numerical and combinatorial computing. John R Gilbert, Steve Reinhardt, Viral B Shah, Computing in Science and Engineering. 102John R. Gilbert, Steve Reinhardt, and Viral B. Shah. A unified framework for numerical and combinatorial computing. Computing in Science and Engineering, 10(2):20-25, 2008.\n\nTwo fast algorithms for sparse matrices: Multiplication and permuted transposition. Fred G Gustavson, ACM Transactions on Mathematical Software. 43Fred G. Gustavson. Two fast algorithms for sparse matrices: Multiplication and permuted transposition. ACM Transactions on Mathematical Software, 4(3):250-269, 1978.\n\nA multilevel algorithm for partitioning graphs. Bruce Hendrickson, Robert Leland, Supercomputing '95: Proceedings of the 1995 ACM/IEEE conference on Supercomputing. New York, NY, USAACM28Bruce Hendrickson and Robert Leland. A multilevel algorithm for partitioning graphs. In Supercomputing '95: Proceedings of the 1995 ACM/IEEE conference on Supercomputing, page 28, New York, NY, USA, 1995. ACM.\n\nAn overview of the Trilinos project. Michael A Heroux, Roscoe A Bartlett, Vicki E Howle, Robert J Hoekstra, Jonathan J Hu, Tamara G Kolda, Richard B Lehoucq, Kevin R Long, Roger P Pawlowski, Eric T Phipps, Andrew G Salinger, Heidi K Thornquist, Ray S Tuminaro, James M Willenbring, Alan Williams, Kendall S Stanley, ACM Trans. Math. Softw. 313Michael A. Heroux, Roscoe A. Bartlett, Vicki E. Howle, Robert J. Hoekstra, Jonathan J. Hu, Tamara G. Kolda, Richard B. Lehoucq, Kevin R. Long, Roger P. Pawlowski, Eric T. Phipps, Andrew G. Salinger, Heidi K. Thornquist, Ray S. Tuminaro, James M. Willenbring, Alan Williams, and Kendall S. Stanley. An overview of the Trilinos project. ACM Trans. Math. Softw., 31(3):397-423, 2005.\n\nColored intersection searching via sparse rectangular matrix multiplication. Haim Kaplan, Micha Sharir, Elad Verbin, Proceedings of the twenty-second annual symposium on Computational geometry, SCG '06. the twenty-second annual symposium on Computational geometry, SCG '06New York, NY, USAACMHaim Kaplan, Micha Sharir, and Elad Verbin. Colored intersection searching via sparse rect- angular matrix multiplication. In Proceedings of the twenty-second annual symposium on Computational geometry, SCG '06, pages 52-60, New York, NY, USA, 2006. ACM.\n\nThe multifrontal method for sparse matrix solution: Theory and practice. W H Joseph, Liu, SIAM Review. 341Joseph W. H. Liu. The multifrontal method for sparse matrix solution: Theory and practice. SIAM Review, 34(1):pp. 82-109, 1992.\n\nSparse matrix computations on parallel processor arrays. T Andrew, William Ogielski, Aiello, SIAM Journal on Scientific Computing. 143Andrew T. Ogielski and William Aiello. Sparse matrix computations on parallel processor arrays. SIAM Journal on Scientific Computing, 14(3):519-530, 1993.\n\nEfficient transitive closure of sparse matrices over closed semirings. Gerald Penn, Theoretical Computer Science. 3541Gerald Penn. Efficient transitive closure of sparse matrices over closed semirings. Theoretical Computer Science, 354(1):72-81, 2006.\n\nMaximum matchings in general graphs through randomization. M O Rabin, V V Vazirani, Journal of Algorithms. 104M. O. Rabin and V. V. Vazirani. Maximum matchings in general graphs through randomization. Journal of Algorithms, 10(4):557-567, 1989.\n\nAn Interactive System for Combinatorial Scientific Computing with an Emphasis on Programmer Productivity. B Viral, Shah, Santa BarbaraUniversity of CaliforniaPhD thesisViral B. Shah. An Interactive System for Combinatorial Scientific Computing with an Em- phasis on Programmer Productivity. PhD thesis, University of California, Santa Barbara, June 2007.\n\nCoarsening, sampling, and smoothing: Elements of the multilevel method. Shang-Hua, Teng, Parallel Processing, number 105 in The IMA Volumes in Mathematics and its Applications. GermanySpringer-VerlagShang-Hua Teng. Coarsening, sampling, and smoothing: Elements of the multilevel method. In Parallel Processing, number 105 in The IMA Volumes in Mathematics and its Applications, pages 247-276, Germany, 1999. Springer-Verlag.\n\nGraph clustering via a discrete uncoupling process. Stijn Van Dongen, SIAM Journal on Matrix Analysis and Applications. 301Stijn Van Dongen. Graph clustering via a discrete uncoupling process. SIAM Journal on Matrix Analysis and Applications, 30(1):121-141, 2008.\n\nOn techniques to improve robustness and scalability of a parallel hybrid linear solver. Ichitaro Yamazaki, Xiaoye Li, High Performance Computing for Computational Science VECPAR 2010. Ichitaro Yamazaki and Xiaoye Li. On techniques to improve robustness and scalability of a parallel hybrid linear solver. In High Performance Computing for Computational Science VECPAR 2010, pages 421-434.\n\nDetecting short directed cycles using rectangular matrix multiplication and dynamic programming. Raphael Yuster, Uri Zwick, SODA '04: Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms. Raphael Yuster and Uri Zwick. Detecting short directed cycles using rectangular matrix mul- tiplication and dynamic programming. In SODA '04: Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms, pages 254-260, 2004.\n", "annotations": {"author": "[{\"end\":107,\"start\":95},{\"end\":123,\"start\":108}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":101},{\"end\":122,\"start\":115}]", "author_first_name": "[{\"end\":100,\"start\":95},{\"end\":112,\"start\":108},{\"end\":114,\"start\":113}]", "author_affiliation": null, "title": "[{\"end\":92,\"start\":1},{\"end\":215,\"start\":124}]", "venue": null, "abstract": "[{\"end\":1300,\"start\":511}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5636,\"start\":5632},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5692,\"start\":5688},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5723,\"start\":5719},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5764,\"start\":5760},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5779,\"start\":5775},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5805,\"start\":5801},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5931,\"start\":5928},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5990,\"start\":5986},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6084,\"start\":6080},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6124,\"start\":6120},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6230,\"start\":6226},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6272,\"start\":6268},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6289,\"start\":6285},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6990,\"start\":6986},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7113,\"start\":7109},{\"end\":7232,\"start\":7230},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7917,\"start\":7914},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8776,\"start\":8773},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9662,\"start\":9659},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9664,\"start\":9662},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10538,\"start\":10535},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11443,\"start\":11439},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11471,\"start\":11467},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11618,\"start\":11615},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12497,\"start\":12494},{\"end\":12919,\"start\":12917},{\"end\":13044,\"start\":13042},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16117,\"start\":16113},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16258,\"start\":16254},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16758,\"start\":16754},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18340,\"start\":18336},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19729,\"start\":19725},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20064,\"start\":20061},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20557,\"start\":20553},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21148,\"start\":21144},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21496,\"start\":21493},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21498,\"start\":21496},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21870,\"start\":21867},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21872,\"start\":21870},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23555,\"start\":23552},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24069,\"start\":24066},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24242,\"start\":24238},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24457,\"start\":24453},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24626,\"start\":24622},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24727,\"start\":24723},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25461,\"start\":25457},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26618,\"start\":26616},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29021,\"start\":29018},{\"end\":29031,\"start\":29021},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29457,\"start\":29454},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29701,\"start\":29697},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29890,\"start\":29888},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32554,\"start\":32550},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33010,\"start\":33007},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33037,\"start\":33033},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33182,\"start\":33178},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33509,\"start\":33505},{\"end\":34696,\"start\":34673},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36317,\"start\":36313},{\"end\":36512,\"start\":36509},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36712,\"start\":36708},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36861,\"start\":36859},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37595,\"start\":37593},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41418,\"start\":41415},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41521,\"start\":41517},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":41711,\"start\":41707},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42104,\"start\":42101},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":42447,\"start\":42444},{\"end\":42457,\"start\":42447},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":42698,\"start\":42694},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":47506,\"start\":47504},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47671,\"start\":47669},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":48098,\"start\":48096},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":48646,\"start\":48644}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43353,\"start\":43286},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43466,\"start\":43354},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43809,\"start\":43467},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44191,\"start\":43810},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44754,\"start\":44192},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44939,\"start\":44755},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45587,\"start\":44940},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45814,\"start\":45588},{\"attributes\":{\"id\":\"fig_10\"},\"end\":45916,\"start\":45815},{\"attributes\":{\"id\":\"fig_11\"},\"end\":46214,\"start\":45917},{\"attributes\":{\"id\":\"fig_12\"},\"end\":46552,\"start\":46215},{\"attributes\":{\"id\":\"fig_13\"},\"end\":46795,\"start\":46553},{\"attributes\":{\"id\":\"fig_14\"},\"end\":46946,\"start\":46796},{\"attributes\":{\"id\":\"fig_15\"},\"end\":47070,\"start\":46947},{\"attributes\":{\"id\":\"fig_16\"},\"end\":47224,\"start\":47071},{\"attributes\":{\"id\":\"fig_17\"},\"end\":47372,\"start\":47225},{\"attributes\":{\"id\":\"fig_18\"},\"end\":47492,\"start\":47373},{\"attributes\":{\"id\":\"fig_21\"},\"end\":47655,\"start\":47493},{\"attributes\":{\"id\":\"fig_22\"},\"end\":48084,\"start\":47656},{\"attributes\":{\"id\":\"fig_23\"},\"end\":48280,\"start\":48085},{\"attributes\":{\"id\":\"fig_24\"},\"end\":48318,\"start\":48281},{\"attributes\":{\"id\":\"fig_25\"},\"end\":48393,\"start\":48319},{\"attributes\":{\"id\":\"fig_26\"},\"end\":48632,\"start\":48394},{\"attributes\":{\"id\":\"fig_28\"},\"end\":48878,\"start\":48633},{\"attributes\":{\"id\":\"fig_29\"},\"end\":49049,\"start\":48879}]", "paragraph": "[{\"end\":1974,\"start\":1302},{\"end\":2619,\"start\":1976},{\"end\":3731,\"start\":2621},{\"end\":5078,\"start\":3733},{\"end\":5491,\"start\":5080},{\"end\":6125,\"start\":5493},{\"end\":6581,\"start\":6127},{\"end\":7771,\"start\":6583},{\"end\":8753,\"start\":7813},{\"end\":8942,\"start\":8755},{\"end\":9883,\"start\":9017},{\"end\":10502,\"start\":9885},{\"end\":10886,\"start\":10504},{\"end\":11315,\"start\":10931},{\"end\":11565,\"start\":11317},{\"end\":12521,\"start\":11567},{\"end\":12764,\"start\":12523},{\"end\":13108,\"start\":12787},{\"end\":13163,\"start\":13115},{\"end\":13213,\"start\":13165},{\"end\":14296,\"start\":13304},{\"end\":14750,\"start\":14338},{\"end\":14841,\"start\":14752},{\"end\":14941,\"start\":14914},{\"end\":15528,\"start\":15047},{\"end\":16118,\"start\":15530},{\"end\":16507,\"start\":16120},{\"end\":17454,\"start\":16509},{\"end\":18373,\"start\":17535},{\"end\":19032,\"start\":18446},{\"end\":19181,\"start\":19034},{\"end\":19779,\"start\":19183},{\"end\":20190,\"start\":19802},{\"end\":21397,\"start\":20192},{\"end\":21839,\"start\":21399},{\"end\":21972,\"start\":21841},{\"end\":22186,\"start\":22070},{\"end\":22471,\"start\":22188},{\"end\":22830,\"start\":22652},{\"end\":23214,\"start\":22953},{\"end\":23480,\"start\":23356},{\"end\":24513,\"start\":23506},{\"end\":25295,\"start\":24515},{\"end\":25841,\"start\":25297},{\"end\":26403,\"start\":25872},{\"end\":26810,\"start\":26405},{\"end\":27200,\"start\":26812},{\"end\":27999,\"start\":27232},{\"end\":28313,\"start\":28001},{\"end\":29162,\"start\":28326},{\"end\":29458,\"start\":29164},{\"end\":30397,\"start\":29499},{\"end\":32008,\"start\":30399},{\"end\":32409,\"start\":32010},{\"end\":33398,\"start\":32459},{\"end\":36015,\"start\":33400},{\"end\":36713,\"start\":36037},{\"end\":37432,\"start\":36715},{\"end\":37999,\"start\":37434},{\"end\":38281,\"start\":38019},{\"end\":38612,\"start\":38283},{\"end\":39203,\"start\":38614},{\"end\":39761,\"start\":39205},{\"end\":40973,\"start\":39763},{\"end\":41809,\"start\":40975},{\"end\":42303,\"start\":41811},{\"end\":42868,\"start\":42305},{\"end\":43285,\"start\":42870}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7812,\"start\":7772},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9016,\"start\":8943},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10930,\"start\":10887},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12781,\"start\":12765},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13303,\"start\":13214},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14337,\"start\":14297},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14913,\"start\":14842},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15046,\"start\":14942},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17534,\"start\":17455},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18445,\"start\":18374},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22069,\"start\":21973},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22651,\"start\":22472},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22952,\"start\":22831},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23355,\"start\":23215}]", "table_ref": "[{\"end\":8940,\"start\":8914},{\"end\":30880,\"start\":30873}]", "section_header": "[{\"end\":12785,\"start\":12783},{\"end\":13113,\"start\":13111},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19800,\"start\":19782},{\"attributes\":{\"n\":\"5.\"},\"end\":23504,\"start\":23483},{\"attributes\":{\"n\":\"5.1.\"},\"end\":25870,\"start\":25844},{\"attributes\":{\"n\":\"5.2.\"},\"end\":27230,\"start\":27203},{\"attributes\":{\"n\":\"5.3.\"},\"end\":28324,\"start\":28316},{\"attributes\":{\"n\":\"5.3.1.\"},\"end\":29497,\"start\":29461},{\"attributes\":{\"n\":\"5.3.2.\"},\"end\":32457,\"start\":32412},{\"attributes\":{\"n\":\"5.3.3.\"},\"end\":36035,\"start\":36018},{\"attributes\":{\"n\":\"5.4.\"},\"end\":38017,\"start\":38002},{\"end\":43299,\"start\":43287},{\"end\":43366,\"start\":43355},{\"end\":43487,\"start\":43468},{\"end\":43823,\"start\":43811},{\"end\":44205,\"start\":44193},{\"end\":44764,\"start\":44756},{\"end\":44956,\"start\":44941},{\"end\":45942,\"start\":45918},{\"end\":46228,\"start\":46216},{\"end\":46566,\"start\":46554},{\"end\":46809,\"start\":46797},{\"end\":46960,\"start\":46948},{\"end\":47084,\"start\":47072},{\"end\":47234,\"start\":47226},{\"end\":47502,\"start\":47494},{\"end\":47667,\"start\":47657},{\"end\":48094,\"start\":48086},{\"end\":48403,\"start\":48395},{\"end\":48642,\"start\":48634},{\"end\":48892,\"start\":48880}]", "table": null, "figure_caption": "[{\"end\":43353,\"start\":43302},{\"end\":43466,\"start\":43368},{\"end\":43809,\"start\":43491},{\"end\":44191,\"start\":43826},{\"end\":44754,\"start\":44208},{\"end\":44939,\"start\":44766},{\"end\":45587,\"start\":44959},{\"end\":45814,\"start\":45590},{\"end\":45916,\"start\":45817},{\"end\":46214,\"start\":45947},{\"end\":46552,\"start\":46231},{\"end\":46795,\"start\":46569},{\"end\":46946,\"start\":46812},{\"end\":47070,\"start\":46963},{\"end\":47224,\"start\":47087},{\"end\":47372,\"start\":47236},{\"end\":47492,\"start\":47375},{\"end\":47655,\"start\":47504},{\"end\":48084,\"start\":47669},{\"end\":48280,\"start\":48096},{\"end\":48318,\"start\":48283},{\"end\":48393,\"start\":48321},{\"end\":48632,\"start\":48405},{\"end\":48878,\"start\":48644},{\"end\":49049,\"start\":48895}]", "figure_ref": "[{\"end\":6324,\"start\":6316},{\"end\":10781,\"start\":10773},{\"end\":12443,\"start\":12435},{\"end\":17630,\"start\":17622},{\"end\":20598,\"start\":20590},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":26498,\"start\":26490},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":26941,\"start\":26933},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":29901,\"start\":29893},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":30260,\"start\":30252},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":30395,\"start\":30387},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":31220,\"start\":31212},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":31680,\"start\":31672},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":33923,\"start\":33915},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":34565,\"start\":34557},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":35028,\"start\":35020},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":38746,\"start\":38738},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":39376,\"start\":39368},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39760,\"start\":39748},{\"end\":40836,\"start\":40828}]", "bib_author_first_name": "[{\"end\":49058,\"start\":49053},{\"end\":49065,\"start\":49059},{\"end\":49193,\"start\":49189},{\"end\":49445,\"start\":49441},{\"end\":49458,\"start\":49453},{\"end\":49460,\"start\":49459},{\"end\":49861,\"start\":49857},{\"end\":49876,\"start\":49871},{\"end\":49889,\"start\":49885},{\"end\":49901,\"start\":49897},{\"end\":50155,\"start\":50148},{\"end\":50157,\"start\":50156},{\"end\":50171,\"start\":50166},{\"end\":50173,\"start\":50172},{\"end\":50483,\"start\":50478},{\"end\":50495,\"start\":50491},{\"end\":50497,\"start\":50496},{\"end\":50879,\"start\":50874},{\"end\":50891,\"start\":50887},{\"end\":50893,\"start\":50892},{\"end\":51309,\"start\":51304},{\"end\":51321,\"start\":51317},{\"end\":51323,\"start\":51322},{\"end\":51757,\"start\":51752},{\"end\":51769,\"start\":51765},{\"end\":51771,\"start\":51770},{\"end\":52148,\"start\":52143},{\"end\":52160,\"start\":52156},{\"end\":52162,\"start\":52161},{\"end\":52505,\"start\":52500},{\"end\":52517,\"start\":52513},{\"end\":52519,\"start\":52518},{\"end\":52534,\"start\":52529},{\"end\":52536,\"start\":52535},{\"end\":52920,\"start\":52915},{\"end\":52934,\"start\":52928},{\"end\":53484,\"start\":53476},{\"end\":53504,\"start\":53498},{\"end\":53519,\"start\":53511},{\"end\":53891,\"start\":53886},{\"end\":53904,\"start\":53898},{\"end\":53918,\"start\":53915},{\"end\":53938,\"start\":53932},{\"end\":53940,\"start\":53939},{\"end\":54302,\"start\":54296},{\"end\":54318,\"start\":54314},{\"end\":54832,\"start\":54824},{\"end\":54852,\"start\":54848},{\"end\":54866,\"start\":54862},{\"end\":54880,\"start\":54875},{\"end\":54897,\"start\":54891},{\"end\":54899,\"start\":54898},{\"end\":55210,\"start\":55205},{\"end\":55771,\"start\":55757},{\"end\":55788,\"start\":55779},{\"end\":56060,\"start\":56053},{\"end\":56062,\"start\":56061},{\"end\":56315,\"start\":56308},{\"end\":56317,\"start\":56316},{\"end\":56330,\"start\":56325},{\"end\":56509,\"start\":56505},{\"end\":56523,\"start\":56517},{\"end\":56717,\"start\":56716},{\"end\":56719,\"start\":56718},{\"end\":56735,\"start\":56734},{\"end\":56992,\"start\":56988},{\"end\":56994,\"start\":56993},{\"end\":57009,\"start\":57004},{\"end\":57023,\"start\":57017},{\"end\":57333,\"start\":57329},{\"end\":57335,\"start\":57334},{\"end\":57350,\"start\":57345},{\"end\":57367,\"start\":57362},{\"end\":57369,\"start\":57368},{\"end\":57679,\"start\":57675},{\"end\":57681,\"start\":57680},{\"end\":57958,\"start\":57953},{\"end\":57978,\"start\":57972},{\"end\":58347,\"start\":58340},{\"end\":58349,\"start\":58348},{\"end\":58364,\"start\":58358},{\"end\":58366,\"start\":58365},{\"end\":58382,\"start\":58377},{\"end\":58384,\"start\":58383},{\"end\":58398,\"start\":58392},{\"end\":58400,\"start\":58399},{\"end\":58419,\"start\":58411},{\"end\":58421,\"start\":58420},{\"end\":58432,\"start\":58426},{\"end\":58434,\"start\":58433},{\"end\":58449,\"start\":58442},{\"end\":58451,\"start\":58450},{\"end\":58466,\"start\":58461},{\"end\":58468,\"start\":58467},{\"end\":58480,\"start\":58475},{\"end\":58482,\"start\":58481},{\"end\":58498,\"start\":58494},{\"end\":58500,\"start\":58499},{\"end\":58515,\"start\":58509},{\"end\":58517,\"start\":58516},{\"end\":58533,\"start\":58528},{\"end\":58535,\"start\":58534},{\"end\":58551,\"start\":58548},{\"end\":58553,\"start\":58552},{\"end\":58569,\"start\":58564},{\"end\":58571,\"start\":58570},{\"end\":58589,\"start\":58585},{\"end\":58607,\"start\":58600},{\"end\":58609,\"start\":58608},{\"end\":59109,\"start\":59105},{\"end\":59123,\"start\":59118},{\"end\":59136,\"start\":59132},{\"end\":59650,\"start\":59649},{\"end\":59652,\"start\":59651},{\"end\":59869,\"start\":59868},{\"end\":59885,\"start\":59878},{\"end\":60178,\"start\":60172},{\"end\":60414,\"start\":60413},{\"end\":60416,\"start\":60415},{\"end\":60425,\"start\":60424},{\"end\":60427,\"start\":60426},{\"end\":60707,\"start\":60706},{\"end\":61743,\"start\":61735},{\"end\":61760,\"start\":61754},{\"end\":62141,\"start\":62134},{\"end\":62153,\"start\":62150}]", "bib_author_last_name": "[{\"end\":49074,\"start\":49066},{\"end\":49080,\"start\":49076},{\"end\":49088,\"start\":49082},{\"end\":49207,\"start\":49194},{\"end\":49216,\"start\":49209},{\"end\":49451,\"start\":49446},{\"end\":49467,\"start\":49461},{\"end\":49869,\"start\":49862},{\"end\":49883,\"start\":49877},{\"end\":49895,\"start\":49890},{\"end\":49910,\"start\":49902},{\"end\":50164,\"start\":50158},{\"end\":50190,\"start\":50174},{\"end\":50201,\"start\":50192},{\"end\":50489,\"start\":50484},{\"end\":50505,\"start\":50498},{\"end\":50885,\"start\":50880},{\"end\":50901,\"start\":50894},{\"end\":51315,\"start\":51310},{\"end\":51331,\"start\":51324},{\"end\":51763,\"start\":51758},{\"end\":51779,\"start\":51772},{\"end\":52154,\"start\":52149},{\"end\":52170,\"start\":52163},{\"end\":52511,\"start\":52506},{\"end\":52527,\"start\":52520},{\"end\":52541,\"start\":52537},{\"end\":52926,\"start\":52921},{\"end\":52942,\"start\":52935},{\"end\":53496,\"start\":53485},{\"end\":53509,\"start\":53505},{\"end\":53529,\"start\":53520},{\"end\":53896,\"start\":53892},{\"end\":53913,\"start\":53905},{\"end\":53930,\"start\":53919},{\"end\":53953,\"start\":53941},{\"end\":54312,\"start\":54303},{\"end\":54324,\"start\":54319},{\"end\":54846,\"start\":54833},{\"end\":54860,\"start\":54853},{\"end\":54873,\"start\":54867},{\"end\":54889,\"start\":54881},{\"end\":54912,\"start\":54900},{\"end\":55216,\"start\":55211},{\"end\":55445,\"start\":55431},{\"end\":55777,\"start\":55772},{\"end\":55796,\"start\":55789},{\"end\":56068,\"start\":56063},{\"end\":56323,\"start\":56318},{\"end\":56333,\"start\":56331},{\"end\":56515,\"start\":56510},{\"end\":56529,\"start\":56524},{\"end\":56732,\"start\":56720},{\"end\":56741,\"start\":56736},{\"end\":57002,\"start\":56995},{\"end\":57015,\"start\":57010},{\"end\":57033,\"start\":57024},{\"end\":57343,\"start\":57336},{\"end\":57360,\"start\":57351},{\"end\":57374,\"start\":57370},{\"end\":57691,\"start\":57682},{\"end\":57970,\"start\":57959},{\"end\":57985,\"start\":57979},{\"end\":58356,\"start\":58350},{\"end\":58375,\"start\":58367},{\"end\":58390,\"start\":58385},{\"end\":58409,\"start\":58401},{\"end\":58424,\"start\":58422},{\"end\":58440,\"start\":58435},{\"end\":58459,\"start\":58452},{\"end\":58473,\"start\":58469},{\"end\":58492,\"start\":58483},{\"end\":58507,\"start\":58501},{\"end\":58526,\"start\":58518},{\"end\":58546,\"start\":58536},{\"end\":58562,\"start\":58554},{\"end\":58583,\"start\":58572},{\"end\":58598,\"start\":58590},{\"end\":58617,\"start\":58610},{\"end\":59116,\"start\":59110},{\"end\":59130,\"start\":59124},{\"end\":59143,\"start\":59137},{\"end\":59659,\"start\":59653},{\"end\":59664,\"start\":59661},{\"end\":59876,\"start\":59870},{\"end\":59894,\"start\":59886},{\"end\":59902,\"start\":59896},{\"end\":60183,\"start\":60179},{\"end\":60422,\"start\":60417},{\"end\":60436,\"start\":60428},{\"end\":60713,\"start\":60708},{\"end\":60719,\"start\":60715},{\"end\":61037,\"start\":61028},{\"end\":61043,\"start\":61039},{\"end\":61450,\"start\":61434},{\"end\":61752,\"start\":61744},{\"end\":61763,\"start\":61761},{\"end\":62148,\"start\":62142},{\"end\":62159,\"start\":62154}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":49185,\"start\":49051},{\"attributes\":{\"id\":\"b1\"},\"end\":49368,\"start\":49187},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2035822},\"end\":49801,\"start\":49370},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11553197},\"end\":50108,\"start\":49803},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":58982102},\"end\":50426,\"start\":50110},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":123973805},\"end\":50799,\"start\":50428},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2693294},\"end\":51236,\"start\":50801},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":397084},\"end\":51697,\"start\":51238},{\"attributes\":{\"doi\":\"UCSB-CS-2010-10\",\"id\":\"b8\"},\"end\":52075,\"start\":51699},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12726120},\"end\":52447,\"start\":52077},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":36777386},\"end\":52852,\"start\":52449},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6540738},\"end\":53431,\"start\":52854},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16652959},\"end\":53824,\"start\":53433},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":18782463},\"end\":54226,\"start\":53826},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3187845},\"end\":54714,\"start\":54228},{\"attributes\":{\"id\":\"b15\"},\"end\":55139,\"start\":54716},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":139115},\"end\":55394,\"start\":55141},{\"attributes\":{\"doi\":\"LBNL-59208\",\"id\":\"b17\"},\"end\":55634,\"start\":55396},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7135949},\"end\":56009,\"start\":55636},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":60831637},\"end\":56254,\"start\":56011},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207191190},\"end\":56485,\"start\":56256},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4506156},\"end\":56655,\"start\":56487},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":611285},\"end\":56932,\"start\":56657},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":119805771},\"end\":57264,\"start\":56934},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":35685405},\"end\":57589,\"start\":57266},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7107184},\"end\":57903,\"start\":57591},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3626585},\"end\":58301,\"start\":57905},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4679315},\"end\":59026,\"start\":58303},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":848827},\"end\":59574,\"start\":59028},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13225491},\"end\":59809,\"start\":59576},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5101170},\"end\":60099,\"start\":59811},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6696606},\"end\":60352,\"start\":60101},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14874993},\"end\":60598,\"start\":60354},{\"attributes\":{\"id\":\"b33\"},\"end\":60954,\"start\":60600},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":352156},\"end\":61380,\"start\":60956},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4646743},\"end\":61645,\"start\":61382},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14711578},\"end\":62035,\"start\":61647},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6696796},\"end\":62491,\"start\":62037}]", "bib_title": "[{\"end\":49439,\"start\":49370},{\"end\":49855,\"start\":49803},{\"end\":50146,\"start\":50110},{\"end\":50476,\"start\":50428},{\"end\":50872,\"start\":50801},{\"end\":51302,\"start\":51238},{\"end\":52141,\"start\":52077},{\"end\":52498,\"start\":52449},{\"end\":52913,\"start\":52854},{\"end\":53474,\"start\":53433},{\"end\":53884,\"start\":53826},{\"end\":54294,\"start\":54228},{\"end\":55203,\"start\":55141},{\"end\":55755,\"start\":55636},{\"end\":56051,\"start\":56011},{\"end\":56306,\"start\":56256},{\"end\":56503,\"start\":56487},{\"end\":56714,\"start\":56657},{\"end\":56986,\"start\":56934},{\"end\":57327,\"start\":57266},{\"end\":57673,\"start\":57591},{\"end\":57951,\"start\":57905},{\"end\":58338,\"start\":58303},{\"end\":59103,\"start\":59028},{\"end\":59647,\"start\":59576},{\"end\":59866,\"start\":59811},{\"end\":60170,\"start\":60101},{\"end\":60411,\"start\":60354},{\"end\":61026,\"start\":60956},{\"end\":61432,\"start\":61382},{\"end\":61733,\"start\":61647},{\"end\":62132,\"start\":62037}]", "bib_author": "[{\"end\":49076,\"start\":49053},{\"end\":49082,\"start\":49076},{\"end\":49090,\"start\":49082},{\"end\":49209,\"start\":49189},{\"end\":49218,\"start\":49209},{\"end\":49453,\"start\":49441},{\"end\":49469,\"start\":49453},{\"end\":49871,\"start\":49857},{\"end\":49885,\"start\":49871},{\"end\":49897,\"start\":49885},{\"end\":49912,\"start\":49897},{\"end\":50166,\"start\":50148},{\"end\":50192,\"start\":50166},{\"end\":50203,\"start\":50192},{\"end\":50491,\"start\":50478},{\"end\":50507,\"start\":50491},{\"end\":50887,\"start\":50874},{\"end\":50903,\"start\":50887},{\"end\":51317,\"start\":51304},{\"end\":51333,\"start\":51317},{\"end\":51765,\"start\":51752},{\"end\":51781,\"start\":51765},{\"end\":52156,\"start\":52143},{\"end\":52172,\"start\":52156},{\"end\":52513,\"start\":52500},{\"end\":52529,\"start\":52513},{\"end\":52543,\"start\":52529},{\"end\":52928,\"start\":52915},{\"end\":52944,\"start\":52928},{\"end\":53498,\"start\":53476},{\"end\":53511,\"start\":53498},{\"end\":53531,\"start\":53511},{\"end\":53898,\"start\":53886},{\"end\":53915,\"start\":53898},{\"end\":53932,\"start\":53915},{\"end\":53955,\"start\":53932},{\"end\":54314,\"start\":54296},{\"end\":54326,\"start\":54314},{\"end\":54848,\"start\":54824},{\"end\":54862,\"start\":54848},{\"end\":54875,\"start\":54862},{\"end\":54891,\"start\":54875},{\"end\":54914,\"start\":54891},{\"end\":55218,\"start\":55205},{\"end\":55447,\"start\":55431},{\"end\":55779,\"start\":55757},{\"end\":55798,\"start\":55779},{\"end\":56070,\"start\":56053},{\"end\":56325,\"start\":56308},{\"end\":56335,\"start\":56325},{\"end\":56517,\"start\":56505},{\"end\":56531,\"start\":56517},{\"end\":56734,\"start\":56716},{\"end\":56743,\"start\":56734},{\"end\":57004,\"start\":56988},{\"end\":57017,\"start\":57004},{\"end\":57035,\"start\":57017},{\"end\":57345,\"start\":57329},{\"end\":57362,\"start\":57345},{\"end\":57376,\"start\":57362},{\"end\":57693,\"start\":57675},{\"end\":57972,\"start\":57953},{\"end\":57987,\"start\":57972},{\"end\":58358,\"start\":58340},{\"end\":58377,\"start\":58358},{\"end\":58392,\"start\":58377},{\"end\":58411,\"start\":58392},{\"end\":58426,\"start\":58411},{\"end\":58442,\"start\":58426},{\"end\":58461,\"start\":58442},{\"end\":58475,\"start\":58461},{\"end\":58494,\"start\":58475},{\"end\":58509,\"start\":58494},{\"end\":58528,\"start\":58509},{\"end\":58548,\"start\":58528},{\"end\":58564,\"start\":58548},{\"end\":58585,\"start\":58564},{\"end\":58600,\"start\":58585},{\"end\":58619,\"start\":58600},{\"end\":59118,\"start\":59105},{\"end\":59132,\"start\":59118},{\"end\":59145,\"start\":59132},{\"end\":59661,\"start\":59649},{\"end\":59666,\"start\":59661},{\"end\":59878,\"start\":59868},{\"end\":59896,\"start\":59878},{\"end\":59904,\"start\":59896},{\"end\":60185,\"start\":60172},{\"end\":60424,\"start\":60413},{\"end\":60438,\"start\":60424},{\"end\":60715,\"start\":60706},{\"end\":60721,\"start\":60715},{\"end\":61039,\"start\":61028},{\"end\":61045,\"start\":61039},{\"end\":61452,\"start\":61434},{\"end\":61754,\"start\":61735},{\"end\":61765,\"start\":61754},{\"end\":62150,\"start\":62134},{\"end\":62161,\"start\":62150}]", "bib_venue": "[{\"end\":49550,\"start\":49469},{\"end\":49940,\"start\":49912},{\"end\":50249,\"start\":50203},{\"end\":50557,\"start\":50507},{\"end\":50959,\"start\":50903},{\"end\":51431,\"start\":51333},{\"end\":51750,\"start\":51699},{\"end\":52246,\"start\":52172},{\"end\":52593,\"start\":52543},{\"end\":53061,\"start\":52944},{\"end\":53540,\"start\":53531},{\"end\":54007,\"start\":53955},{\"end\":54420,\"start\":54326},{\"end\":54822,\"start\":54716},{\"end\":55255,\"start\":55218},{\"end\":55429,\"start\":55396},{\"end\":55810,\"start\":55798},{\"end\":56116,\"start\":56070},{\"end\":56357,\"start\":56335},{\"end\":56557,\"start\":56531},{\"end\":56779,\"start\":56743},{\"end\":57083,\"start\":57035},{\"end\":57412,\"start\":57376},{\"end\":57734,\"start\":57693},{\"end\":58068,\"start\":57987},{\"end\":58641,\"start\":58619},{\"end\":59229,\"start\":59145},{\"end\":59677,\"start\":59666},{\"end\":59940,\"start\":59904},{\"end\":60213,\"start\":60185},{\"end\":60459,\"start\":60438},{\"end\":60704,\"start\":60600},{\"end\":61131,\"start\":61045},{\"end\":61500,\"start\":61452},{\"end\":61829,\"start\":61765},{\"end\":62248,\"start\":62161},{\"end\":49569,\"start\":49552},{\"end\":50601,\"start\":50583},{\"end\":50982,\"start\":50961},{\"end\":52637,\"start\":52619},{\"end\":53182,\"start\":53063},{\"end\":54440,\"start\":54422},{\"end\":58087,\"start\":58070},{\"end\":59317,\"start\":59231},{\"end\":61140,\"start\":61133}]"}}}, "year": 2023, "month": 12, "day": 17}