{"id": 222208985, "updated": "2023-10-06 10:41:47.592", "metadata": {"title": "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs", "authors": "[{\"first\":\"Meng\",\"last\":\"Qu\",\"middle\":[]},{\"first\":\"Junkun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Louis-Pascal\",\"last\":\"Xhonneux\",\"middle\":[]},{\"first\":\"Yoshua\",\"last\":\"Bengio\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Tang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 10, "day": 8}, "abstract": "This paper studies learning logic rules for reasoning on knowledge graphs. Logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. Existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). To address these limitations, this paper proposes a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. We develop an EM-based algorithm for optimization. In each iteration, the reasoning predictor is first updated to explore some generated logic rules for reasoning. Then in the E-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the M-step, the rule generator is updated with the rules selected in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.04029", "mag": "3091984691", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/QuCXBT21", "doi": null}}, "content": {"source": {"pdf_hash": "32783724e06cca7328ddcc61e59b026cf624e102", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.04029v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "43a74337a4c85913d7720c98631516ab334cba9e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/32783724e06cca7328ddcc61e59b026cf624e102.txt", "contents": "\nPublished as a conference paper at ICLR 2021 RNNLOGIC: LEARNING LOGIC RULES FOR REASON- ING ON KNOWLEDGE GRAPHS\n\n\nMeng Qu \nMila -Qu\u00e9bec AI Institute\n\n\nUniversit\u00e9 de Montr\u00e9al\n\n\nJunkun Chen \nTsinghua University\n\n\nLouis-Pascal Xhonneux \nMila -Qu\u00e9bec AI Institute\n\n\nUniversit\u00e9 de Montr\u00e9al\n\n\nYoshua Bengio \nMila -Qu\u00e9bec AI Institute\n\n\nUniversit\u00e9 de Montr\u00e9al\n\n\nCanadian Institute for Advanced Research (CIFAR)\n\n\nJian Tang \nMila -Qu\u00e9bec AI Institute\n\n\nHEC Montr\u00e9al\n\n\nCanadian Institute for Advanced Research (CIFAR)\n\n\nPublished as a conference paper at ICLR 2021 RNNLOGIC: LEARNING LOGIC RULES FOR REASON- ING ON KNOWLEDGE GRAPHS\n\nThis paper studies learning logic rules for reasoning on knowledge graphs. Logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. Existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). To address these limitations, this paper proposes a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. We develop an EM-based algorithm for optimization. In each iteration, the reasoning predictor is first updated to explore some generated logic rules for reasoning. Then in the E-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the M-step, the rule generator is updated with the rules selected in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic. * Equal contribution.In this paper, we propose a principled probabilistic approach called RNNLogic which overcomes the above limitations. Our approach consists of a rule generator as well as a reasoning predictor with logic rules, which are simultaneously trained to enhance each other. The rule generator provides logic rules which are used by the reasoning predictor for reasoning, while the reasoning predictor provides effective reward to train the rule generator, which helps significantly reduce the search space. Specifically, for each query-answer pair, e.g., q = (h, r, ?) and a = t, we model the probability of the answer conditioned on query and existing knowledge graph G, i.e., p(a|G, q), where a set of logic rules z 1 is treated as a latent variable. The rule generator defines a prior distribution over logic rules for each query, i.e., p(z|q), which is parameterized by a recurrent neural network. The reasoning predictor computes the likelihood of the answer conditioned on the logic rules and the existing knowledge graph G, i.e., p(a|G, q, z). At each training iteration, we first sample a few logic rules from the rule generator, and further update the reasoning predictor to try out these rules for prediction. Then an EM algorithm (Neal & Hinton, 1998) is used to optimize the rule generator. In the E-step, a set of high-quality logic rules are selected from all the generated rules according to their posterior probabilities. In the M-step, the rule generator is updated to imitate the high-quality rules selected in the E-step. Extensive experimental results show that RNNLogic outperforms state-of-the-art methods for knowledge graph reasoning 2 . Besides, RNNLogic is able to generate high-quality logic rules.\n\nINTRODUCTION\n\nKnowledge graphs are collections of real-world facts, which are useful in various applications. Each fact is typically specified as a triplet (h, r, t) or equivalently r(h, t), meaning entity h has relation r with entity t. For example, Bill Gates is the Co-founder of Microsoft. As it is impossible to collect all facts, knowledge graphs are incomplete. Therefore, a fundamental problem on knowledge graphs is to predict missing facts by reasoning with existing ones, a.k.a. knowledge graph reasoning. This paper studies learning logic rules for reasoning on knowledge graphs. For example, one may extract a rule \u2200X, Y, Z hobby(X, Y ) \u2190 friend(X, Z) \u2227 hobby(Z, Y ), meaning that if Z is a friend of X and Z has hobby Y , then Y is also likely the hobby of X. Then the rule can be applied to infer new hobbies of people. Such logic rules are able to improve interpretability and precision of reasoning (Qu & Tang, 2019;Zhang et al., 2020). Moreover, logic rules can also be reused and generalized to other domains and data (Teru & Hamilton, 2020). However, due to the large search space, inferring high-quality logic rules for reasoning on knowledge graphs is a challenging task.\n\nIndeed, a variety of methods have been proposed for learning logic rules from knowledge graphs. Most traditional methods such as path ranking (Lao & Cohen, 2010) and Markov logic networks (Richardson & Domingos, 2006) enumerate relational paths on graphs as candidate logic rules, and then learn a weight for each rule as an assessment of rule qualities. There are also some recent methods based on neural logic programming (Yang et al., 2017) and neural theorem provers (Rockt\u00e4schel & Riedel, 2017), which are able to learn logic rules and their weights simultaneously in a differentiable way. Though empirically effective for prediction, the search space of these methods is exponentially large, making it hard to identify high-quality logic rules. Besides, some recent efforts (Xiong et al., 2017) formulate the problem as a sequential decision making process, and use reinforcement learning to search for logic rules, which significantly reduces search complexity. However, due to the large action space and sparse reward in training, the performance of these methods is not yet satisfying.\n\n\nRELATED WORK\n\nOur work is related to existing efforts on learning logic rules for knowledge graph reasoning. Most traditional methods enumerate relational paths between query entities and answer entities as candidate logic rules, and further learn a scalar weight for each rule to assess the quality. Representative methods include Markov logic networks (Kok & Domingos, 2005;Richardson & Domingos, 2006;Khot et al., 2011), relational dependency networks (Neville & Jensen, 2007;Natarajan et al., 2010), rule mining algorithms (Gal\u00e1rraga et al., 2013;Meilicke et al., 2019), path ranking (Lao & Cohen, 2010;Lao et al., 2011) and probabilistic personalized page rank (ProPPR) algorithms (Wang et al., 2013;2014a;b). Some recent methods extend the idea by simultaneously learning logic rules and the weights in a differentiable way, and most of them are based on neural logic programming (Rockt\u00e4schel & Riedel, 2017;Yang et al., 2017;Cohen et al., 2018;Sadeghian et al., 2019;Yang & Song, 2020) or neural theorem provers (Rockt\u00e4schel & Riedel, 2017;Minervini et al., 2020). These methods and our approach are similar in spirit, as they are all able to learn the weights of logic rules efficiently. However, these existing methods try to simultaneously learn logic rules and their weights, which is nontrivial in terms of optimization. The main innovation of our approach is to separate rule generation and rule weight learning by introducing a rule generator and a reasoning predictor respectively, which can mutually enhance each other. The rule generator generates a few high-quality logic rules, and the reasoning predictor only focuses on learning the weights of such high-quality rules, which significantly reduces the search space and leads to better reasoning results. Meanwhile, the reasoning predictor can in turn help identify some useful logic rules to improve the rule generator.\n\nThe other kind of rule learning method is based on reinforcement learning. The general idea is to train pathfinding agents, which search for reasoning paths in knowledge graphs to answer questions, and then extract logic rules from reasoning paths (Xiong et al., 2017;Chen et al., 2018;Das et al., 2018;Lin et al., 2018;Shen et al., 2018). However, training effective pathfinding agents is highly challenging, as the reward signal (i.e., whether a path ends at the correct answer) can be extremely sparse. Although some studies (Lin et al., 2018) try to get better reward by using embedding-based methods for reward shaping, the performance is still worse than most embedding-based methods. In our approach, the rule generator has a similar role to those pathfinding agents. The major difference is that we simultaneously train the rule generator and a reasoning predictor with logic rules, which mutually enhance each other. The reasoning predictor provides effective reward for training the rule generator, and the rule generator offers high-quality rules to improve the reasoning predictor.\n\nOur work is also related to knowledge graph embedding, which solves knowledge graph reasoning by learning entity and relation embeddings in latent spaces (Bordes et al., 2013;Wang et al., 2014c;Yang et al., 2015;Nickel et al., 2016;Trouillon et al., 2016;Cai & Wang, 2018;Dettmers et al., 2018;Balazevic et al., 2019;Sun et al., 2019). With proper architectures, these methods are able to learn  Figure 1: RNNLogic consists of a rule generator p \u03b8 and a reasoning predictor p w . Given a query, the rule generator generates logic rules for the reasoning predictor. The reasoning predictor takes the generated rules as input, and reasons on a knowledge graph to predict the answer. RNNLogic is optimized with an EM-based algorithm. In each iteration, the rule generator produces some logic rules, and we update the reasoning predictor to explore these rules for reasoning. Then in the E-step, a set of high-quality rules are identified from all generated rules via posterior inference. Finally in the M-step, the rule generator is updated to be consistent with the high-quality rules identified in E-step.\n\nsome simple logic rules. For example, TransE (Bordes et al., 2013) can learn some composition rules. RotatE (Sun et al., 2019) can mine some composition rules, symmetric rules and inverse rules. However, these methods can only find some simple rules in an implicit way. In contrast, our approach explicitly trains a rule generator, which is able to generate more complicated logic rules.\n\nThere are some works studying boosting rule-based models (Goldberg & Eckstein, 2010;Eckstein et al., 2017), where they dynamically add new rules according to the rule weights learned so far. These methods have been proven effective in binary classification and regression. Compared with them, our approach shares similar ideas, as we dynamically update the rule generator with the feedback from the reasoning predictor, but we focus on a different task, i.e., reasoning on knowledge graphs.\n\n\nMODEL\n\nIn this section, we introduce the proposed approach RNNLogic which learns logic rules for knowledge graph reasoning. We first formally define knowledge graph reasoning and logic rules.\n\nKnowledge Graph Reasoning. Let p data (G, q, a) be a training data distribution, where G is a background knowledge graph characterized by a set of (h, r, t)-triplets which we may also write as r(h, t), q = (h, r, ?) is a query, and a = t is the answer. Given G and the query q, the goal is to predict the correct answer a. More formally, we aim to model the probabilistic distribution p(a|G, q).\n\nLogic Rule. We perform knowledge graph reasoning by learning logic rules, where logic rules in this paper have the conjunctive form \u2200{X i } l i=0 r(X 0 , X l ) \u2190 r 1 (X 0 , X 1 ) \u2227 \u00b7 \u00b7 \u00b7 \u2227 r l (X l\u22121 , X l ) with l being the rule length. This syntactic structure naturally captures composition, and can easily express other common logic rules such as symmetric or inverse rules. For example, let r \u22121 denote the inverse relation of relation r, then each symmetric rule can be expressed as \u2200{X , Y } r(X, Y ) \u2190 r \u22121 (X, Y ).\n\nIn RNNLogic, we treat a set of logic rules which could explain a query as a latent variable we have to infer. To do this, we introduce a rule generator and a reasoning predictor using logic rules. Given a query, the rule generator employs a recurrent neural network to generate a set of logic rules, which are given to the reasoning predictor for prediction. We optimize RNNLogic with an EM-based algorithm. In each iteration, we start with updating the reasoning predictor to try out some logic rules generated by the rule generator. Then in the E-step, we identify a set of high-quality rules from all generated rules via posterior inference, with the prior from the rule generator and likelihood from the reasoning predictor. Finally in the M-step, the rule generator is updated with the identified high-quality rules.\n\n\nPROBABILISTIC FORMALIZATION\n\nWe start by formalizing knowledge graph reasoning in a probabilistic way, where a set of logic rules z is treated as a latent variable. The target distribution p(a|G, q) is jointly modeled by a rule generator and a reasoning predictor. The rule generator p \u03b8 defines a prior over a set of latent rules z conditioned on a query q, while the reasoning predictor p w gives the likelihood of the answer a conditioned on latent rules z, the query q, and the knowledge graph G. Thus p(a|G, q) can be computed as below:\np w,\u03b8 (a|G, q) = z p w (a|G, q, z)p \u03b8 (z|q) = E p \u03b8 (z|q) [p w (a|G, q, z)].(1)\nThe goal is to jointly train the rule generator and reasoning predictor to maximize the likelihood of training data. Formally, the objective function is presented as below:\nmax \u03b8,w O(\u03b8, w) = E (G,q,a)\u223cpdata [log p w,\u03b8 (a|G, q)] = E (G,q,a)\u223cpdata [log E p \u03b8 (z|q) [p w (a|G, q, z)]]. (2)\n\nPARAMETERIZATION\n\nRule Generator. The rule generator defines the distribution p \u03b8 (z|q). For a query q, the rule generator aims at generating a set of latent logic rules z for answering the query.\n\nFormally, given a query q = (h, r, ?), we generate compositional logic rules by only considering the query relation r without the query entity h, which allows the generated rules to generalize across entities. For each compositional rule in the abbreviation form r \u2190 r 1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 r l , it can be viewed as a sequence of relations [r, r 1 , r 2 \u00b7 \u00b7 \u00b7 r l , r END ], where r is the query relation or the rule head,\n{r i } l i=1\nare the body of the rule, and r END is a special relation indicating the end of the relation sequence.\n\nSuch relation sequences can be effectively modeled by recurrent neural networks (Hochreiter & Schmidhuber, 1997), and thus we introduce RNN \u03b8 to parameterize the rule generator. Given a query relation r, RNN \u03b8 sequentially generates each relation in the body of a rule, until it reaches the ending relation r END . In this process, the probabilities of generated rules are simultaneously computed. With such rule probabilities, we define the distribution over a set of rules z as a multinomial distribution:\np \u03b8 (z|q) = Mu(z|N, RNN \u03b8 (\u00b7|r)),(3)\nwhere Mu stands for multinomial distributions, N is a hyperparameter for the size of the set z, and RNN \u03b8 (\u00b7|r) defines a distribution over compositional rules with rule head being r. The generative process of a rule set z is quite intuitive, where we simply generate N rules with RNN \u03b8 to form z.\n\nReasoning Predictor with Logic Rules. The reasoning predictor defines p w (a|G, q, z). For a query q, the predictor uses a set of rules z to reason on a knowledge graph G and predict the answer a.\n\nFollowing stochastic logic programming (Cussens, 2000), a principled reasoning framework, we use a log-linear model for reasoning. For each query q = (h, r, ?), a compositional rule is able to find different grounding paths on graph G, leading to different candidate answers. For example, given query (Alice, hobby, ?), a rule hobby \u2190 friend \u2227 hobby can have two groundings, hobby(Alice, Sing) \u2190 friend(Alice, Bob)\u2227hobby(Bob, Sing) and hobby(Alice, Ski) \u2190 friend(Alice, Charlie) \u2227 hobby(Charlie, Ski), yielding two candidate answers Sing and Ski.\n\nLet A be the set of candidate answers which can be discovered by any logic rule in the set z.\n\nFor each candidate answer e \u2208 A, we compute the following scalar score w (e) for that candidate:\nscore w (e) = rule\u2208z score w (e|rule) = rule\u2208z path\u2208P(h,rule,e) \u03c8 w (rule) \u00b7 \u03c6 w (path),(4)\nwhere P(h, rule, e) is the set of grounding paths which start at h and end at e following a rule (e.g., Alice\nfriend \u2212\u2212\u2212\u2212\u2192 Bob hobby \u2212 \u2212\u2212\u2212 \u2192 Sing)\n. \u03c8 w (rule) and \u03c6 w (path) are scalar weights of each rule and path. Intuitively, the score of each candidate answer e is the sum of scores contributed by each rule, i.e., score w (e|rule). To get score w (e|rule), we sum over every grounding path found in the graph G.\n\nFor the scalar weight \u03c8 w (rule) of each rule, it is a learnable parameter to optimize. For the score \u03c6 w (path) of each specific path, we explore two methods for parameterization. One method always sets \u03c6 w (path) = 1. However, this method cannot distinguish between different relational paths. To address the problem, for the other method, we follow an embedding algorithm RotatE (Sun et al., 2019) to introduce an embedding for each entity and model each relation as a rotation operator on entity embeddings. Then for each grounding path of rule starting from h to e, if we rotate the embedding of h according to the rotation operators defined by the body relations of rule, we should expect to obtain an embedding close to the embedding of e. Thus we compute the similarity between the derived embedding and the embedding of e as \u03c6 w (path), which can be viewed as a measure of the soundness and consistency of each path. For example, given a path Alice friend \u2212\u2212\u2212\u2212\u2192 Bob hobby \u2212 \u2212\u2212\u2212 \u2192 Sing, we rotate Alice's embedding with the operators defined by friend and hobby. Afterwards we compute the similarity of the derived embedding and embedding of Sing as \u03c6 w (path). Such a method allows us to compute a specific \u03c6 w (path) for each path, which further leads to more precise scores for different candidate answers. See App. C for more details of the parameterization method.\n\nOnce we have the score for every candidate answer, we can further define the probability that the answer a of the query q is entity e by using a softmax function as follows:\n\np w (a = e|G, q, z) = exp(score w (e)) e \u2208A exp(score w (e ))\n\n.\n\n3.3 OPTIMIZATION Next, we introduce how we optimize the reasoning predictor and rule generator to maximize the objective in Eq.\n\n(2). At each training iteration, we first update the reasoning predictor p w according to some rules generated by the generator, and then update the rule generator p \u03b8 with an EM algorithm.\n\nIn the E-step, a set of high-quality rules are identified from all generated rules via posterior inference, with the rule generator as the prior and the reasoning predictor as the likelihood. In the M-step, the rule generator is then updated to be consistent with the high-quality rules selected in the E-step.\n\nFormally, at each training iteration, we start with maximizing the objective O(\u03b8, w) in Eq.\n\n(2) with respect to the reasoning predictor p w . To do that, we notice that there is an expectation operation with respect to p \u03b8 (z|q) for each training instance (G, q, a). By drawing a sample\u1e91 \u223c p \u03b8 (z|q) for query q, we can approximate the objective function of w at each training instance (G, q, a) as below:\nO (G,q,a) (w) = log E p \u03b8 (z|q) [p w (a|G, q, z)] \u2248 log p w (a|G, q,\u1e91)(6)\nBasically, we sample a set of rules\u1e91 from the generator and feed\u1e91 into the reasoning predictor. Then the parameter w of the reasoning predictor is updated to maximize the log-likelihood of the answer a.\n\nWith the updated reasoning predictor, we then update the rule generator p \u03b8 to maximize the objective O(\u03b8, w). In general, this can be achieved by REINFORCE (Williams, 1992) or the reparameterization trick (Jang et al., 2017;Maddison et al., 2017), but they are less effective in our problem due to the large number of logic rules. Therefore, an EM framework is developed to optimize the rule generator.\n\n\nE-step.\n\nRecall that when optimizing the reasoning predictor, we draw a set of rules\u1e91 for each data instance (G, q, a), and let the reasoning predictor use\u1e91 to predict a. For each data instance, the E-step aims to identify a set of K high-quality rules z I from all generated rules\u1e91, i.e., z I \u2282\u1e91, |z I | = K.\n\nFormally, this is achieved by considering the posterior probability of each subset of logic rules z I , i.e., p \u03b8,w (z I |G, q, a) \u221d p w (a|G, q, z I )p \u03b8 (z I |q), with prior of z I from the rule generator p \u03b8 and likelihood from the reasoning predictor p w . The posterior combines knowledge from both the rule generator and reasoning predictor, so the likely set of high-quality rules can be obtained by sampling from the posterior. However, sampling from the posterior is nontrivial due to its intractable partition function, so we approximate the posterior using a more tractable form with the proposition below:\n\nProposition 1 Consider a data instance (G, q, a) with q = (h, r, ?) and a = t. For a set of rules\u1e91 generated by the rule generator p \u03b8 , we can compute the following score H for each rule \u2208\u1e91:\nH(rule) = score w (t|rule) \u2212 1 |A| e\u2208A score w (e|rule) + log RNN \u03b8 (rule|r),(7)\nwhere A is the set of all candidate answers discovered by rules in\u1e91, score w (e|rule) is the score that each rule contributes to entity e as defined by Eq. (4), RNN \u03b8 (rule|r) is the prior probability of rule computed by the generator. Suppose s = max e\u2208A |score w (e)| < 1. Then for a subset of rules z I \u2282\u1e91 with |z I | = K, the log-probability log p \u03b8,w (z I |G, q, a) can be approximated as follows:\nlog p \u03b8,w (z I |G, q, a) \u2212 rule\u2208z I H(rule) + \u03b3(z I ) + const \u2264 s 2 + O(s 4 ) (8)\nwhere const is a constant term that is independent from z I , \u03b3(z I ) = log(K!/ rule\u2208\u1e91 n rule !), with K being the given size of set z I and n rule being the number of times each rule appears in z I .\n\nWe prove the proposition in App. A.1. In practice, we can apply weight decay to the weight of logic rules in Eq. (4), and thereby reduce s = max e\u2208A |score w (e)| to get a more precise approximation.\n\nThe above proposition allows us to utilize ( rule\u2208z I H(rule)+\u03b3(z I )+const) to approximate the logposterior log p \u03b8,w (z I |G, q, a), yielding a distribution q(z I ) \u221d exp( rule\u2208z I H(rule) + \u03b3(z I )) as a good approximation of the posterior. It turns out that the derived q(z I ) is a multinomial distribution, and thus sampling from q(z I ) is more tractable. Specifically, a sample\u1e91 I from q(z I ) can be formed with K logic rules which are independently sampled from\u1e91, where the probability of sampling each rule is computed as exp(H(rule))/( rule \u2208\u1e91 exp(H(rule ))). We provide the proof in the App. A.2.\n\nIntuitively, we could view H(rule) of each rule as an assessment of the rule quality, which considers two factors. The first factor is based on the reasoning predictor p w , and it is computed as the score that a rule contributes to the correct answer t minus the mean score that this rule contributes to other candidate answers. If a rule gives higher score to the true answer and lower score to other candidate answers, then the rule is more likely to be important. The second factor is based on the rule generator p \u03b8 , where we compute the prior probability for each rule and use the probability for regularization.\n\nEmpirically, we find that picking K rules with highest H(rule) to form\u1e91 I works better than sampling from the posterior. Similar observations have been made on the node classification task (Qu et al., 2019). In fact,\u1e91 I formed by the top-K rules is an MAP estimation of the posterior, and thus the variant of picking top-K rules yields a hard-assignment EM algorithm (Koller & Friedman, 2009). Despite the reduced theoretical guarantees, we use this variant in practice for its good performance.\n\n\nM-step.\n\nOnce we obtain a set of high-quality logic rules\u1e91 I for each data instance (G, q, a) in the E-step, we further leverage those rules to update the parameters \u03b8 of the rule generator in the M-step.\n\nSpecifically, for each data instance (G, q, a), we treat the corresponding rule set\u1e91 I as part of the (now complete) training data, and update the rule generator by maximizing the log-likelihood of\u1e91 I :\nO (G,q,a) (\u03b8) = log p \u03b8 (\u1e91 I |q) = rule\u2208\u1e91 I log RNN \u03b8 (rule|r) + const.(9)\nWith the above objective, the feedback from the reasoning predictor can be effectively distilled into the rule generator. In this way, the rule generator will learn to only generate high-quality rules for the reasoning predictor to explore, which reduces the search space and yields better empirical results.\n\nFor more detailed analysis of the EM algorithm for optimizing O(\u03b8, w), please refer to App. B.\n\nAlgorithm 1 Workflow of RNNLogic while not converge do For each instance, use the rule generator p \u03b8 to generate a set of rules\u1e91 (|\u1e91| = N ). For each instance, update the reasoning predictor p w based on generated rules\u1e91 and Eq. (6). E-step: For each instance, identify K high-quality rules\u1e91 I from\u1e91 according to H(rule) in Eq. (7). M-step: For each instance, update the rule generator p \u03b8 according to the identified rules and Eq. (9). end while During testing, for each query, use p \u03b8 to generate N rules and feed them into p w for prediction.\n\n\nRNNLOGIC+\n\nIn RNNLogic, we aim at jointly training a rule generator and a simple reasoning predictor. Although this framework allows us to generate high-quality logic rules, the performance of knowledge graph reasoning is limited by the low capacity of the reasoning predictor. To further improve the results, a natural idea is to only focus on the reasoning predictor and develop a more powerful predictor by using the high-quality rules generated by RNNLogic as input. Next, we propose one such predictor.\n\nFormally, let\u1e91 I be the set of generated high-quality logic rules. Given a query q = (h, r, ?), let A be the collection of candidate answers that can be discovered by any rule in\u1e91 I . For each candidate answer e \u2208 A, we again compute a scalar score score w (e) for that candidate answer as below:\nscore w (e) = MLP(AGG({v rule , |P(h, rule, e)|} rule\u2208\u1e91 I ).(10)\nHere, v rule is an embedding vector for each rule, and |P(h, rule, e)| is the number of grounding paths from h to e discovered by rule. AGG is an aggregator, which aims at aggregating all the rule embeddings v rule by treating |P(h, rule, e)| as aggregation weights. We follow Zhu et al. (2021) to use the PNA aggregator (Corso et al., 2020) for its good performance. Once we get the aggregated embedding, an MLP is further used to project the embedding to the scalar score for candidate e.\n\nIn practice, we can further enhance the above score with knowledge graph embedding. For each query q = (h, r, ?) and candidate answer e, knowledge graph embedding methods are able to infer a plausibility score KGE(h, r, e), measuring how likely (h, r, e) is a valid triplet. Based on that, we can naturally obtain a more powerful score function by combining the score from logic rules and the score from knowledge graph embedding. For example, we can linearly combine them as follows:\nscore w (e) = MLP(AGG({v rule , |P(h, rule, e)|} rule\u2208\u1e91 I ) + \u03b7 KGE(h, r, e),(11)\nwhere \u03b7 controls the weight of the knowledge graph embedding score.\n\nOnce we have the score score w (e) for each candidate e, we can again apply a softmax function to the score as in Eq. (5) to compute the probability that e is the answer, i.e., p w (a = e|G, q, z). This predictor can then be easily optimized through maximizing likelihood on each instance (G, q, a).\n\n\nEXPERIMENT\n\n\nEXPERIMENT SETTINGS\n\nDatasets. We choose four datasets for evaluation, including FB15k-237 (Toutanova & Chen, 2015), WN18RR (Dettmers et al., 2018), Kinship and UMLS (Kok & Domingos, 2007). For Kinship and UMLS, there are no standard data splits, so we randomly sample 30% of all the triplets for training, 20% for validation, and the rest 50% for testing. The detailed statistics are summarized in the App. D.\n\nCompared Algorithms. We compare the following algorithms in experiment: Rule learning methods. For traditional statistical relational learning methods, we choose Markov logic networks (Richardson & Domingos, 2006), boosted relational dependency networks (Natarajan et al., 2010) and path ranking (Lao & Cohen, 2010). We also consider neural logic programming methods, including NeuralLP RNNLogic. For RNNLogic, we consider two model variants. The first variant assigns a constant score to different grounding paths in the reasoning predictor, i.e., \u03c6 w (path) = 1 in Eq. (4), and we denote this variant as w/o emb.. The second variant leverages entity embeddings and relation embeddings to compute the path score \u03c6 w (path), and we denote the variant as with emb.. RNNLogic+. For RNNLogic+, we also consider two model variants. The first variant only uses the logic rules learned by RNNLogic to train the reasoning predictor as in Eq. (10), and we denote the variant as w/o emb.. The second variant uses both of the learned logic rules and knowledge graph embeddings to train a reasoning predictor as in Eq. (11), and we denote the variant as with emb.. Evaluation Metrics. During evaluation, for each test triplet (h, r, t), we build two queries (h, r, ?) and (t, r \u22121 , ?) with answers t and h. For each query, we compute a probability for each entity, and compute the rank of the correct answer. Given the ranks from all queries, we report the Mean Rank (MR), Mean Reciprocal Rank (MRR) and Hit@k (H@k) under the filtered setting (Bordes et al., 2013), which is used by most existing studies. Note that there can be a case where an algorithm assigns the same probability to the correct answer and a few other entities. For such a case, many methods compute the rank of the correct answer as (m + 1) where m is the number of entities receiving higher probabilities than the correct answer. This setup can be problematic according to Sun et al. (2020). For fair comparison, in that case we compute the expectation of each evaluation metric over all the random shuffles of entities which receive the same probability as the correct answer. For example, if there are n entities which have the same probability as the correct answer in the above case, then we treat the rank of the correct answer as (m + (n + 1)/2) when computing Mean Rank.\n\nBesides, we notice that in MINERVA (Das et al., 2018) and MultiHopKG (Lin et al., 2018), they only consider queries in the form of (h, r, ?), which is different from our default setting. To make fair comparison with these methods, we also apply RNNLogic to this setting and report the performance.  Experimental Setup of Our Method. For each training triplet (h, r, t), we add an inverse triplet (t, r \u22121 , h) into the training set, yielding an augmented set of training triplets T . We use a closedworld assumption for model training, which assumes that any triplet outside T is incorrect. To build a training instance from p data , we first randomly sample a triplet (h, r, t) from T , and then form an instance as (G = T \\ {(h, r, t)}, q = (h, r, ?), a = t). Basically, we use the sampled triplet (h, r, t) to construct the query and answer, and use the rest of triplets in T to form the background knowledge graph G. During testing, the background knowledge graph G is formed with all the triplets in T .\n\nFor the rule generator of RNNLogic, the maximum length of generated rules is set to 4 for FB15k-237, 5 for WN18RR, and 3 for the rest, which are selected on validation data. See App. D for the details.\n\nOnce RNNLogic is trained, we leverage the rule generator to generate a few high-quality logic rules, which are further utilized to train RNNLogic+. Specifically, the maximum length of generated rules is set to 3. We generate 100 rules for each relation in FB15k-237 and 200 rules for each relation in WN18RR. The dimension of logic rule embedding in RNNLogic+ is set to 32 on all datasets. The hyperparameter \u03b7 in Eq. (11) is set to 2 on the FB15k-237 dataset and 0.5 on the WN18RR dataset.\n\nIn RNNLogic+ with emb., we use RotatE (Sun et al., 2019) as the knowledge graph embedding model, which is the same as RNNLogic with emb.. For both models, the entity and relation embeddings are pre-trained. The embedding dimension is set to 500 on FB15k-237 and 200 on WN18RR.\n\n\nRESULTS\n\nComparison against Existing Methods. We present the results on the FB15k-237 and WN18RR datasets in Tab We first compare RNNLogic with rule learning methods. RNNLogic achieves much better results than statistical relational learning methods (MLN, Boosted RDN, PathRank) and neural differentiable methods (NeuralLP, DRUM, NLIL, CTP). This is because the rule generator and reasoning predictor of RNNLogic can collaborate with each other to reduce search space and learn better rules. RNNLogic also outperforms reinforcement learning methods (MINERVA, MultiHopKG, M-Walk). The reason is that RNNLogic is optimized with an EM-based framework, in which the reasoning predictor provides more useful feedback to the rule generator, and thus addresses the challenge of sparse reward. We then compare RNNLogic against state-of-the-art embedding-based methods. For RNNLogic with embeddings in the reasoning predictor (with emb.), it outperforms most compared methods in most cases, and the reason is that RNNLogic is able to use logic rules to enhance reasoning performance. For RNNLogic without embedding (w/o emb.), it achieves comparable results to embedding-based methods, especially on WN18RR, Kinship and UMLS where the training triplets are quite limited.\n\nResults of RNNLogic+. In RNNLogic, we jointly train a rule generator and a simple rule generator. In contrast to that, RNNLogic+ focuses on training a complicated predictor with the high-quality rules learned by RNNLogic as input. In this way, RNNLogic+ entails high model capacity. Next, we look into RNNLogic+ and we present its results on the FB15k-237 and WN18RR datasets in Tab. 1.\n\nWe can see that without the help of knowledge graph embedding (w/o emb.), RNNLogic+ already gets competitive results on both datasets, especially WN18RR, where RNNLogic+ outperforms all the other methods. Surprisingly, these results are achieved by using only a few logic rules for prediction (100 for each relation in FB15k-237 and 200 for each relation in WN18RR). This observation shows that the logic rules learned by RNNLogic are indeed very useful. For now, the reasoning predictor used in RNNLogic+ is still straightforward, and we are likely to achieve better results by designing a more powerful predictor to make better use of the high-quality rules. We leave it as a future work.\n\nBy further using knowledge graph embedding (with emb.), the results of RNNLogic+ are significantly improved on both datasets. Moreover, RNNLogic+ with embedding also outperforms most knowledge graph embedding methods. On WN18RR, the method even achieves the best result. This demonstrates that the information captured by logic rules and knowledge graph embedding is complementary, allowing rule-based methods and embedding-based methods to mutually enhance each other. Thus, an interesting feature direction could be designing approaches to better combine both kinds of methods.   Quality of Learned Logic Rules. Next, we study the quality of rules learned by different methods for reasoning. For each trained model, we let it generate I rules with highest qualities per query relation, and use them to train a predictor w/o emb. as in Eq. (5) for reasoning. For RNNLogic, the quality of each rule is measured by its prior probability from the rule generator, and we use beam search to infer top-I rules. The results at different I are in Fig. 2 Fig. 4. We see that RNNLogic w/o emb. achieves the best results. Besides, the improvement over RotatE is more significant as we reduce training triplets, showing that RNNLogic is more robust to data sparsity.\n\nPerformance w.r.t. Embedding Dimension. RNNLogic with emb. uses entity and relation embeddings to improve the reasoning predictor. Next, we study its performance with different embedding dimensions. The results are presented in Fig. 3  Case Study of Generated Logic Rules. Finally, we show some logic rules generated by RNNLogic on the FB15k-237 dataset in Tab. 5. We can see that these logic rules are meaningful and diverse. The first rule is a subrelation rule. The third and fifth rules are two-hop compositional rules. The rest of logic rules have even more complicated forms. This case study shows that RNNLogic can indeed learn useful and diverse rules for reasoning. For more generated logic rules, please refer to App. E.  \n\n\nCONCLUSION\n\nThis paper studies learning logic rules for knowledge graph reasoning, and an approach called RNNLogic is proposed. RNNLogic treats a set of logic rules as a latent variable, and a rule generator as well as a reasoning predictor with logic rules are jointly learned. We develop an EM-based algorithm for optimization. Extensive expemriments prove the effectiveness of RNNLogic. In the future, we plan to study generating more complicated logic rules rather than only compositional rules. Besides, we plan to extend RNNLogic to other reasoning problems, such as question answering. A PROOFS This section presents the proofs of some propositions used in the optimization algorithm of RNNLogic.\n\n\nACKNOWLEDGMENTS\n\nRecall that in the E-step of the optimization algorithm, we aim to sample from the posterior distribution over rule sets. However, directly sampling from the posterior distribution is intractable due to the intractable partition function. Therefore, we introduce Proposition 1, which gives an approximation distribution with more tractable form for the posterior distribution. With this approximation, sampling becomes much easier. In Section A.1, we present the proof of Proposition 1. In Section A.2, we show how to perform sampling based on the approximation of the posterior distribution.\n\nA.1 PROOF OF PROPOSITION 1\n\nNext, we prove proposition 1, which is used to approximate the true posterior probability in the E-step of optimization. We first restate the proposition as follows:\n\nProposition Consider a data instance (G, q, a) with q = (h, r, ?) and a = t. For a set of rules\u1e91 generated by the rule generator p \u03b8 , we can compute the following score H for each rule \u2208\u1e91:\nH(rule) = score w (t|rule) \u2212 1 |A| e\u2208A score w (e|rule) + log RNN \u03b8 (rule|r),\nwhere A is the set of all candidate answers discovered by rules in\u1e91, score w (e|rule) is the score that each rule contributes to entity e as defined by Eq. (4), RNN \u03b8 (rule|r) is the prior probability of rule computed by the generator. Suppose s = max e\u2208A |score w (e)| < 1. Then for a subset of rules z I \u2282\u1e91 with |z I | = K, the log-probability log p \u03b8,w (z I |G, q, a) could be approximated as follows:\nlog p \u03b8,w (z I |G, q, a) \u2212 rule\u2208z I H(rule) + \u03b3(z I ) + const \u2264 s 2 + O(s 4 )\nwhere const is a constant term that is independent from z I , \u03b3(z I ) = log(K!/ rule\u2208\u1e91 n rule !), with K being the given size of set z I and n rule being the number of times each rule appears in z I .\n\nProof: We first rewrite the posterior probability as follows:\n\nlog p \u03b8,w (z I |G, q, a) = log p w (a|G, q, z I ) + log p \u03b8 (z I |q) + const = log exp(score w (t)) e\u2208A exp(score w (e)) + log Mu(z I |K, RNN \u03b8 (\u00b7|r)) + const, where const is a constant term which does not depend on the choice of z I , and RNN \u03b8 (\u00b7|r) defines a probability distribution over all the composition-based logic rules. The probability mass function of the above multinomial distribution Mu(z I |K, RNN \u03b8 (\u00b7|r)) can be written as below:\nMu(z I |K, q) = K! rule\u2208\u1e91 n rule ! rule\u2208\u1e91 RNN \u03b8 (rule|r) n rule ,\nwhere K is the size of set z I and n rule is the number of times a rule appears in z I .\n\nLetting \u03b3(z I ) = log(K!/ rule\u2208\u1e91 n rule !), then the posterior probability can then be rewritten as:\n\nlog p \u03b8,w (z I |G, q, a) = log exp(score w (t)) e\u2208A exp(score w (e)) + log K! rule\u2208\u1e91 n rule ! + log rule\u2208\u1e91 RNN \u03b8 (rule|r) n rule + const = log exp(score w (t)) e\u2208A exp(score w (e)) + \u03b3(z I ) + rule\u2208z I log RNN \u03b8 (rule|r) + const =score w (t) \u2212 log e\u2208A exp(score w (e)) + \u03b3(z I ) + rule\u2208z I log RNN \u03b8 (rule|r) + const.\n\nThe above term log e\u2208A exp(score w (e)) makes the posterior distribution hard to deal with, and thus we approximate it using Lemma 1, which we prove at the end of this section.\n\nLemma 1 Let e \u2208 A be a finite set of entities, let |score w (e)| \u2264 s < 1, and let score w be a function from entities to real numbers. Then the following inequalities hold:\n0 \u2264 log e\u2208A exp(score w (e)) \u2212 e\u2208A 1 |A| score w (e) + log(|A|) \u2264 s 2 + O(s 4 ).\nHence, using the lemma we can get the following upper bound of the posterior probability:\nlog p \u03b8,w (z I |G, q, a) =score w (t) \u2212 log e\u2208A exp(score w (e)) + \u03b3(z I ) + rule\u2208z I log RNN \u03b8 (rule|r) + const \u2264score w (t) \u2212 e\u2208A 1 |A| score w (e) + \u03b3(z I ) + rule\u2208z I log RNN \u03b8 (rule|r) + const = rule\u2208z I H(rule) + \u03b3(z I ) + const,\nand also the following lower bound of the posterior probability:\nlog p \u03b8,w (z I |G, q, a) =score w (t) \u2212 log e\u2208A exp(score w (e)) + \u03b3(z I ) + rule\u2208z I log RNN \u03b8 (rule|r) + const \u2265score w (t) \u2212 e\u2208A 1 |A| score w (e) + \u03b3(z I ) + rule\u2208z I log RNN \u03b8 (rule|r) + const \u2212 s 2 \u2212 O(s 4 ) = rule\u2208z I H(rule) + \u03b3(z I ) + const \u2212 s 2 \u2212 O(s 4 ),\nwhere const is a constant term which does not depend on z I .\n\nBy combining the lower and the upper bound, we get:\nlog p \u03b8,w (z I |G, q, a) \u2212 rule\u2208z I H(rule) + \u03b3(z I ) + const \u2264 s 2 + O(s 4 )\nThus, it only remains to prove Lemma 1 to complete the proof. We use Theorem 1 from (Simic, 2008) as a starting point:\nTheorem 1 Suppose thatx = {x i } n i=1\nrepresents a finite sequence of real numbers belonging to a fixed closed interval I = [a, b], a < b. If f is a convex function on I, then we have that:\n1 n n i=1 f (x i ) \u2212 f 1 n n i=1 x i \u2264 f (a) + f (b) \u2212 2f a + b 2 .\nAs (\u2212 log) is convex and exp(score w (e)) \u2208 [exp(\u2212s), exp(s)], Theorem 1 gives us that:\n\n\u2212 1 |A| e\u2208A log (exp(score w (e))) + log 1 |A| e\u2208A exp(score w (e)) \u2264 \u2212 log(exp(\u2212s)) \u2212 log(exp(s)) + 2 log exp(\u2212s) + exp(s) 2 .\n\nAfter some simplification, we get: \n\nwhere the last inequality is based on Taylor's series log(1 + e x ) = log 2 + 1 2 x + 1 8 x 2 + O(x 4 ) with |x| < 1. On the other hand, according to the well-known Jensen's inequality, we have:\nlog 1 |A| e\u2208A exp(score w (e)) \u2265 1 |A| e\u2208A log (exp(score w (e))) ,\nwhich implies:\n\nlog e\u2208A exp(score w (e)) \u2265 e\u2208A 1 |A| score w (e) + log(|A|).\n\nBy combining Eq. (12) and Eq. (13), we obtain:\n0 \u2264 log e\u2208A exp(score w (e)) \u2212 e\u2208A 1 |A| score w (e) + log(|A|) \u2264 s 2 + O(s 4 ).\nThis completes the proof. .\n\n\nA.2 SAMPLING BASED ON THE APPROXIMATION OF THE TRUE POSTERIOR\n\nBased on Proposition 1, the log-posterior probability log p \u03b8,w (z I |G, q, a) could be approximated by ( rule\u2208z I H(rule)+\u03b3(z I )+const), with const being a term that does not depend on z I . This implies that we could construct a distribution q(z I ) \u221d exp( rule\u2208z I H(rule) + \u03b3(z I )) to approximate the true posterior, and draw samples from q as approximation to the real samples from the posterior.\n\nIt turns out that the distribution q(z I ) is a multinomial distribution. To see that, we rewrite q(z I ) as:\nq(z I ) = 1 Z exp rule\u2208z I H(rule) + \u03b3(z I ) = 1 Z exp (\u03b3(z I )) rule\u2208z I exp (H(rule)) = 1 Z K! rule\u2208\u1e91 n rule ! rule\u2208\u1e91 exp (H(rule)) n rule = 1 Z K! rule\u2208\u1e91 n rule ! rule\u2208\u1e91 q r (rule) n rule = 1 Z Mu(z I |K, q r ),\nwhere n rule is the number of times a rule appears in the set z I , q r is a distribution over all the generated logic rules\u1e91 with q r (rule) = exp(H(rule))/ rule \u2208\u1e91 exp(H(rule )), Z and Z are normalization terms. By summing over z I on both sides of the above equation, we obtain Z = 1, and hence:\n\nq(z I ) = Mu(z I |K, q r ).\n\nTo sample from such a multinomial distribution, we could simply sample K rules independently from the distribution q r , and form a sample\u1e91 I with these K rules.\n\nIn practice, we observe that the hard-assignment EM algorithm (Koller & Friedman, 2009) works better than the standard EM algorithm despite the reduced theoretical guarantees. In the hardassignment EM algorithm, we need to draw a sample\u1e91 I with the maximum posterior probability. Based on the above approximation q(z I ) of the true posterior distribution p \u03b8,w (z I |G, q, a), we could simply construct such a sample\u1e91 I with K rules which have the maximum probability under the distribution q r . By definition, we have q r (rule) \u221d exp(H(rule)), and hence drawing K rules with maximum probability under q r is equivalent to choosing K rules with the maximum H values.\n\n\nB MORE ANALYSIS OF THE EM ALGORITHM\n\nIn RNNLogic, we use an EM algorithm to optimize the rule generator. In this section, we show why this EM algorithm is able to maximize the objective function of the rule generator.\n\nRecall that for a fixed reasoning predictor p w , we aim to update p \u03b8 to maximize the log-likelihood function log p w,\u03b8 (a|G, q) for each data instance (G, q, a). Directly optimizing log p w,\u03b8 (a|G, q) is difficult due to the latent logic rules, and therefore we consider the following evidence lower bound of the log-likelihood function:\nlog p w,\u03b8 (a|G, q) \u2265 E q(z I ) [log p w (a|G, q, z I ) + log p \u03b8 (z I |q) \u2212 log q(z I )] = L ELBO (q, p \u03b8 ),(14)\nwhere q(z I ) is a variational distribution, and the equation holds when q(z I ) = p \u03b8,w (z I |G, q, a).\n\nWith this lower bound, we can optimize the log-likelihood function log p w,\u03b8 (a|G, q) with an E-step and an M-step. In the E-step, we optimize q(z I ) to maximize L ELBO (q, p \u03b8 ), which is equivalent to minimizing KL(q(z I )||p \u03b8,w (z I |G, q, a)). By doing so, we are able to tighten the lower bound. Then in the M-step, we further optimize \u03b8 to maximize L ELBO (q, p \u03b8 ). Next, we introduce the details.\n\n\nE-step.\n\nIn the E-step, our goal is to update q to minimize KL(q(z I )||p \u03b8,w (z I |G, q, a)). However, there are a huge number of possible logic rules, and hence optimizing q(z I ) on every possible rule set z I is intractable. To solve the problem, recall that we generate a set of logic rules\u1e91 when optimizing the reasoning predictor, and here we add a constraint to q based on\u1e91. Specifically, we constrain the sample space of q(z I ) to be all subsets of\u1e91 with size being K, i.e., z I \u2282\u1e91 and |z I | = K. In other words, we require z I \u2282\u1e91,|z I |=K q(z I ) = 1. With such a constraint, we can further use proposition 1 to construct the variational distribution q to approximate p \u03b8,w (z I |G, q, a), as what is described in the model section.\n\n\nM-step.\n\nIn the M-step, our goal is to update p \u03b8 to maximize the lower bound L ELBO (q, p \u03b8 ). To do that, we notice that there is an expectation operation with respect to q(z I ) in L ELBO (q, p \u03b8 ). By drawing a sample from q(z I ), L ELBO (q, p \u03b8 ) can be estimated as follows:\n\nL ELBO (q, p \u03b8 ) = E q(z I ) [log p w (a|G, q, z I ) + log p \u03b8 (z I |q) \u2212 log q(z I )]\n\nlog p w (a|G, q,\u1e91 I ) + log p \u03b8 (\u1e91 I |q) \u2212 log q(\u1e91 I ),\n\nwhere\u1e91 I \u223c q(z I ) is a sample drawn from the variational distribution. By ignoring the terms which are irrelevant to p \u03b8 , we obtain the following objective function for \u03b8:\nlog p \u03b8 (\u1e91 I |q),(16)\nwhich is the same as the objective function described in the model section.\n\nAs a result, by performing the E-step and the M-step described in the model section, we are able to update p \u03b8 to increase the lower bound L ELBO (q, p \u03b8 ), and thereby push up the log-likelihood function log p w,\u03b8 (a|G, q). Therefore, we see that the EM algorithm can indeed maximize log p w,\u03b8 (a|G, q) with respect to p \u03b8 .\n\n\nC DETAILS ABOUT PARAMETERIZATION AND IMPLEMENTATION\n\nSection 3.2 of the paper introduces the high-level idea of the reasoning predictor with logic rules and the rule generator. Due to the limited space, some details of the models are not covered. In this section, we explain the details of the reasoning predictor and the rule generator.\n\n\nC.1 REASONING PREDICTOR WITH LOGIC RULES\n\nWe start with the reasoning predictor with logic rules. Recall that for each query, our reasoning predictor leverages a set of logic rules z to give each candidate answer a score, which is further used to predict the correct answer from all candidates.\n\nSpecifically, let A denote the set of all the candidate answers discovered by logic rules in set z.\n\nFor each candidate answer e \u2208 A, we define the following function score w to compute a score:\nscore w (e) = rule\u2208z score w (e|rule) = rule\u2208z path\u2208P(h,rule,e) \u03c8 w (rule) \u00b7 \u03c6 w (path),(17)\nwhere P(h, rule, e) is the set of grounding paths which start at h and end at e following a rule (e.g., For the scalar weight \u03c8 w (rule) of a rule, we initialize \u03c8 w (rule) as follows:\n\u03c8 w (rule) = E (G,q,a)\u223cpdata |P(h, rule, t)| \u2212 1 |A| e\u2208A |P(h, rule, e)| ,(18)\nwhere |P(h, rule, t)| is the number of relational paths starting from head entity h, following the relations in rule and ending at tail entity t. The form is very similar to the definition of H values for logic rules, and the value can effectively measure the contribution of a rule to the correct answers.\n\nWe also try randomly initializing rule weights or initializing them as 0, which yield similar results.\n\nFor the scalar score \u03c6 w (path) of a path, we either fix it to 1, or compute it by introducing entity and relation embeddings. In the second case, we introduce an embedding for each entity and relation in the complex space. Formally, the embedding of an entity e is denoted as x e , and the embedding of a relation r is denoted as x r . For a grounding path path = e 0 r1 \u2212 \u2192 e 1 r2 \u2212 \u2192 e 2 \u00b7 \u00b7 \u00b7 e l\u22121 r l \u2212 \u2192 e l , we follow the idea in RotatE (Sun et al., 2019) and compute \u03c6 w (path) in the following way:\n\u03c6 w (path) = \u03c3(\u03b4 \u2212 d(x e0 \u2022 x r1 \u2022 x r2 \u2022 \u00b7 \u00b7 \u00b7 \u2022 x r l , x e l )),(19)\nwhere \u03c3(x) = 1 1+e \u2212x is the sigmoid function, d is a distance function between two complex vectors, \u03b4 is a hyperparameter, and \u2022 is the Hadmard product in complex spaces, which could be viewed as a rotation operator. Intuitively, for the embedding x e0 of entity e 0 , we rotate x e0 by using the rotation operators defined by {r k } l k=1 , yielding (x e0 \u2022 x r1 \u2022 x r2 \u2022 \u00b7 \u00b7 \u00b7 \u2022 x r l ). Then we compute the distance between the new embedding and the embedding x e l of entity e l , and further convert the distance to a value between 0 and 1 by using the sigmoid function and a hyperparameter \u03b4.\n\n\nC.2 RULE GENERATOR\n\nThis paper focuses on compositional rules, which have the abbreviation form r \u2190 r 1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 r l and thus could be viewed a sequence of relations [r, r 1 , r 2 \u00b7 \u00b7 \u00b7 r l , r END ], where r is the query relation or the head of the rule, {r i } l i=1 are the body of the rule, and r END is a special relation indicating the end of the relation sequence. We introduce a rule generator RNN \u03b8 parameterized with an LSTM (Hochreiter & Schmidhuber, 1997) to model such sequences. Given the current relation sequence [r, r 1 , r 2 \u00b7 \u00b7 \u00b7 r i ], RNN \u03b8 aims to generate the next relation r i+1 and meanwhile output the probability of r i+1 . The detailed computational process towards the goal is summarized as follows:\n\n\u2022 Initialize the hidden state of the RNN \u03b8 as follows:\nh 0 = f (v r ),\nwhere v r is a parameter vector associated with the query relation or the head relation r, f is a linear transformation. \u2022 Sequentially compute the hidden state at different positions by using the LSTM gate:\nh t = LSTM(h t\u22121 , g([v r , v rt ]),\nwhere v rt is a parameter vector associated with the relation r t , [v r , v rt ] is the concatenation of v r and v rt , g is a linear transformation.\n\n\u2022 Generate r t+1 and its probability based on h t+1 and the following vector: softmax(o(h t+1 )).\n\nSuppose the set of relations is denoted as R. We first transform h t+1 to a |R|-dimensional vector by using a linear transformation o, and then apply softmax function to the |R|-dimensional vector to get the probability of each relation. Finally, we generate r t+1 according to the probability vector. D EXPERIMENTAL SETUP D.1 DATASET STATISTICS The statistics of datasets are summarised in Table 6. Next, we explain the detailed experimental setup of RNNLogic. We try different configurations of hyperparameters on the validation set, and the optimal configuration is then used for testing. We report the optimal hyperparameter configuration as below.\n\nData Preprocessing. For each training triplet (h, r, t), we add an inverse triplet (t, r \u22121 , h) into the training set, yielding an augmented set of training triplets T . To build a training instance from p data , we first randomly sample a triplet (h, r, t) from T , and then form an instance as (G = T \\ {(h, r, t)}, q = (h, r, ?), a = t). Basically, we use the sampled triplet (h, r, t) to construct the query and answer, and use the rest of triplets in T to form the background knowledge graph G. During testing, the background knowledge graph G is constructed by using all the triplets in T .\n\nReasoning Predictor. For the reasoning predictor in with embedding cases, the embedding dimension is set to 500 for FB15k-237, 200 for WN18RR, 2000 for Kinship and 1000 for UMLS. We pre-train these embeddings with RotatE (Sun et al., 2019). The hyperparameter \u03b4 for computing \u03c6 w (path) in Equation (19) is set to 9 for FB15k-237, 6 for WN18RR, 0.25 for Kinship and 3 for UMLS. We use the Adam (Kingma & Ba, 2014) optimizer with an initial learning rate being 5 \u00d7 10 \u22125 , and we decrease the learning rate in a cosine shape.\n\nRule Generator. For the rule generator, the maximum length of generated rules is set to 4 for FB15k-237, 5 for WN18RR, and 3 for Kinship and UMLS. These numbers are chosen according to model performace on the validation data. The size of input and hidden states in RNN \u03b8 are set to 512 and 256. The learning rate is set to 1 \u00d7 10 \u22123 and monotonically decreased in a cosine shape. Beam search is used to generate rules with high probabilities, so that we focus on exploiting these logic rules which the rule generator is confident about. Besides, we pre-train the rule generator by using sampled relational paths on the background knowledge graph formed with training triplets, which prevents the rule generator from exploring meaningless logic rules in the beginning of training.\n\nEM Optimization. During optimization, we sample 1000 rules from the rule generator for each data instance. In the E-step, for each data instance, we identify 300 rules as high-quality logic rules.\n\nEvaluation. In testing, for each query q = (h, r, ?), we use the rule generator p \u03b8 to generate 1000 logic rules, and let the reasoning predictor p w use the generated logic rules to predict the answer a.\n\n\nE CASE STUDY\n\nWe present more rules learned by RNNLogic on FB15k-237 dataset and UMLS dataset in Table 7 at the last page. In this table, h r \u2212 \u2192 t means triplet (h, r, t) and h r \u2190 \u2212 t means triplet (h, r \u22121 , t), or equivalently (t, r, h).\n\n\nF CONNECTION WITH REINFORCE\n\nIn terms of optimizing the rule generator, our EM algorithm has some connections with the RE-INFORCE algorithm (Williams, 1992). Formally, for a training instance (G, q, a), REINFORCE computes the derivative with respect to the parameters \u03b8 of the rule generator as follows:\nE p \u03b8 (z|q) [R \u00b7 \u2207 \u03b8 log p \u03b8 (z|q)] R \u00b7 \u2207 \u03b8 log p \u03b8 (\u1e91|q),(20)\nwhere\u1e91 is a sample from the rule generator, i.e.,\u1e91 \u223c p \u03b8 (z|q). R is a reward from the reasoning predictor based on the prediction result on the instance. For example, we could treat the probability of the correct answer computed by the reasoning predictor as reward, i.e., R = p w (a|G, q,\u1e91).\n\nIn contrast, our EM optimization algorithm optimizes the rule generator with the objective function defined in Equation (2), yielding the derivative \u2207 \u03b8 log p \u03b8 (\u1e91 I |q). Comparing the derivative to that in REINFORCE (Eq. (20)), we see that EM only maximizes the log-probability for rules selected in the E-step, while REINFORCE maximizes the log-probability for all generated rules weighted by the scalar reward R. Hence the two approaches coincide if R is set to 1 for the selected rules from the approximate posterior and 0 otherwise. In general, finding an effective reward function to provide feedback is nontrivial, and we empirically compare these two optimization algorithms in experiments.  \n\n\n(Yang et al., 2017), DRUM (Sadeghian et al., 2019) and NLIL (Yang & Song, 2020). In addition, we compare against CTP (Minervini et al., 2020), a differentiable method based on neural theorem provers. Besides, we consider three reinforcement learning methods, which are MINERVA (Das et al., 2018), MultiHopKG (Lin et al., 2018) and M-Walk (Shen et al., 2018).Other methods. We also compare with some embedding methods, including TransE(Bordes et al., 2013),DistMult (Yang et al., 2015),ComplEx (Trouillon et al., 2016), ComplEx-N3(Lacroix et al., 2018), ConvE(Dettmers et al., 2018), TuckER(Balazevic et al., 2019) andRotatE (Sun et al., 2019).\n\nFigure 2 :\n2Performance w.r.t. # logic rules. RNNLogic achieves competitive results even with 10 rules per query relation.\n\nFigure 3 :\n3Performance w.r.t. embedding dimension.\n\n\nin TV Show(X, Y ) \u2190 Actor of(X, Y ) Appears in TV Show(X, Y ) \u2190 Creator of(X, U ) \u2227 Has Producer(U,V ) \u2227 Appears in TV Show(V, Y ) ORG. in State(X, Y ) \u2190 ORG. in City(X, U ) \u2227 City Locates in State(U, Y ) ORG. in State(X, Y ) \u2190 ORG. inCity(X, U ) \u2227 Address of PERS.(U, V ) \u2227 Born in(V, W ) \u2227 Town in State(W, Y ) Person Nationality(X, Y ) \u2190 Born in(X, U ) \u2227 Place in Country(U, Y ) Person Nationality(X, Y ) \u2190 Student of Educational Institution(X, U ) \u2227 ORG. Endowment Currency(U, V )\u2227 Currency Used in Region(V, W ) \u2227 Region in Country(W, Y )\n\n\nscore w (e) + log(|A|) + s 2 + O(s 4 ),\n\n\nSing). \u03c8 w (rule) and \u03c6 w (path) are scalar weights of each rule and path.\n\n(\nDefinition. An actor of a show appears in the show, obviously.)\n\nTable 1 :\n1Results of reasoning on FB15k-237 and WN18RR. H@k is in %. [ * ] means the numbers are taken from the original papers.[ \u2020 ] means we rerun the methods with the same evaluation process.Category \nAlgorithm \nFB15k-237 \nWN18RR \nMR \nMRR \nH@1 \nH@3 \nH@10 \nMR \nMRR \nH@1 \nH@3 \nH@10 \n\nNo Rule \nLearning \n\nTransE  *  \n357 \n0.294 \n-\n-\n46.5 \n3384 \n0.226 \n-\n-\n50.1 \nDistMult  *  \n254 \n0.241 \n15.5 \n26.3 \n41.9 \n5110 \n0.43 \n39 \n44 \n49 \nComplEx  *  \n339 \n0.247 \n15.8 \n27.5 \n42.8 \n5261 \n0.44 \n41 \n46 \n51 \nComplEx-N3  *  \n-\n0.37 \n-\n-\n56 \n-\n0.48 \n-\n-\n57 \nConvE  *  \n244 \n0.325 \n23.7 \n35.6 \n50.1 \n4187 \n0.43 \n40 \n44 \n52 \nTuckER  *  \n-\n0.358 \n26.6 \n39.4 \n54.4 \n-\n0.470 \n44.3 \n48.2 \n52.6 \nRotatE  *  \n177 \n0.338 \n24.1 \n37.5 \n53.3 \n3340 \n0.476 \n42.8 \n49.2 \n57.1 \n\nRule \nLearning \n\nPathRank \n-\n0.087 \n7.4 \n9.2 \n11.2 \n-\n0.189 \n17.1 \n20.0 \n22.5 \nNeuralLP  \u2020 \n-\n0.237 \n17.3 \n25.9 \n36.1 \n-\n0.381 \n36.8 \n38.6 \n40.8 \nDRUM  \u2020 \n-\n0.238 \n17.4 \n26.1 \n36.4 \n-\n0.382 \n36.9 \n38.8 \n41.0 \nNLIL  *  \n-\n0.25 \n-\n-\n32.4 \n-\n-\n-\n-\n-\nM-Walk  *  \n-\n0.232 \n16.5 \n24.3 \n-\n-\n0.437 \n41.4 \n44.5 \n-\n\nRNNLogic \nw/o emb. \n538 \n0.288 \n20.8 \n31.5 \n44.5 \n7527 \n0.455 \n41.4 \n47.5 \n53.1 \nwith emb. \n232 \n0.344 \n25.2 \n38.0 \n53.0 \n4615 \n0.483 \n44.6 \n49.7 \n55.8 \n\nRNNLogic+ \nw/o emb. \n480 \n0.299 \n21.5 \n32.8 \n46.4 \n7204 \n0.489 \n45.3 \n50.6 \n56.3 \nwith emb. \n178 \n0.349 \n25.8 \n38.5 \n53.3 \n4624 \n0.513 \n47.1 \n53.2 \n59.7 \n\n\n\nTable 2 :\n2Results of knowledge graph reasoning on the FB15k-237 and WN18RR datasets with only (h, r, ?)-queries. H@k is in %.[ * ] means that the numbers are taken from the original papers.Category \nAlgorithm \nFB15k-237 \nWN18RR \nMR \nMRR \nH@1 \nH@3 \nH@10 \nMR \nMRR \nH@1 \nH@3 \nH@10 \nRule \nLearning \n\nMINERVA  *  \n-\n0.293 \n21.7 \n32.9 \n45.6 \n-\n0.448 \n41.3 \n45.6 \n51.3 \nMultiHopKG  *  \n-\n0.407 \n32.7 \n-\n56.4 \n-\n0.472 \n43.7 \n-\n54.2 \n\nRNNLogic \nw/o emb. \n459.0 \n0.377 \n28.9 \n41.2 \n54.9 \n7662.8 \n0.478 \n43.8 \n50.3 \n55.3 \nwith emb. \n146.1 \n0.443 \n34.4 \n48.9 \n64.0 \n3767.0 \n0.506 \n46.3 \n52.3 \n59.2 \n\n\n\n\n. 1 and Tab. 2. The results on the Kinship and UMLS datasets are shown in Tab. 3.\n\nTable 3 :\n3Results of reasoning on the Kinship and UMLS datasets. H@k is in %.Category \nAlgorithm \nKinship \nUMLS \nMR \nMRR \nH@1 \nH@3 \nH@10 \nMR \nMRR \nH@1 \nH@3 \nH@10 \n\nNo Rule \nLearning \n\nDistMult \n8.5 \n0.354 \n18.9 \n40.0 \n75.5 \n14.6 \n0.391 \n25.6 \n44.5 \n66.9 \nComplEx \n7.8 \n0.418 \n24.2 \n49.9 \n81.2 \n13.6 \n0.411 \n27.3 \n46.8 \n70.0 \nComplEx-N3 \n-\n0.605 \n43.7 \n71.0 \n92.1 \n-\n0.791 \n68.9 \n87.3 \n95.7 \nTuckER \n6.2 \n0.603 \n46.2 \n69.8 \n86.3 \n5.7 \n0.732 \n62.5 \n81.2 \n90.9 \nRotatE \n3.7 \n0.651 \n50.4 \n75.5 \n93.2 \n4.0 \n0.744 \n63.6 \n82.2 \n93.9 \n\nRule \nLearning \n\nMLN \n10.0 \n0.351 \n18.9 \n40.8 \n70.7 \n7.6 \n0.688 \n58.7 \n75.5 \n86.9 \nBoosted RDN \n25.2 \n0.469 \n39.5 \n52.0 \n56.7 \n54.8 \n0.227 \n14.7 \n25.6 \n37.6 \nPathRank \n-\n0.369 \n27.2 \n41.6 \n67.3 \n-\n0.197 \n14.8 \n21.4 \n25.2 \nNeuralLP \n16.9 \n0.302 \n16.7 \n33.9 \n59.6 \n10.3 \n0.483 \n33.2 \n56.3 \n77.5 \nDRUM \n11.6 \n0.334 \n18.3 \n37.8 \n67.5 \n8.4 \n0.548 \n35.8 \n69.9 \n85.4 \nMINERVA \n-\n0.401 \n23.5 \n46.7 \n76.6 \n-\n0.564 \n42.6 \n65.8 \n81.4 \nCTP \n-\n0.335 \n17.7 \n37.6 \n70.3 \n-\n0.404 \n28.8 \n43.0 \n67.4 \n\nRNNLogic \nw/o emb. \n3.9 \n0.639 \n49.5 \n73.1 \n92.4 \n5.3 \n0.745 \n63.0 \n83.3 \n92.4 \nwith emb. \n3.1 \n0.722 \n59.8 \n81.4 \n94.9 \n3.1 \n0.842 \n77.2 \n89.1 \n96.5 \n\n\n\n\n, where RNNLogic achieves much better results. Even with only 10 rules per relation, RNNLogic still achieves competitive results. Performance w.r.t. the Number of Training Triplets. To better evaluate different methods under cases where training triplets are very limited, in this section we reduce the amount of training data on Kinship and UMLS to see how the performance varies. The results are presented in\n\n\n, where we compare againstRotatE (Sun et al.,  2019). We see that RNNLogic significantly outperforms RotatE at every embedding dimension. The improvement is mainly from the use of logic rules, showing that our learned rules are indeed helpful.Figure 4: Performance w.r.t. # training triplets. RNNLogic is more robust to data sparsity even without using embeddings.Comparison of Optimization Algorithms. RNNLogic uses an EM algorithm to optimize the rule \ngenerator. In practice, the generator can also be optimized with REINFORCE (Williams, 1992) (see \nApp. F for details). We empirically compare the two algorithms in the w/o emb. case. The results on \nKinship and UMLS are presented in Tab. 4. We see EM consistently outperforms REINFORCE. \n\n33% \n67% \n100% \n\nTraining Triplets \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\nMRR \n\nKinship \n\nAlgorithm \n\nNeuralLP \nDRUM \nMINERVA \nRotatE \nRNNLogic w/o emb. \n\n33% \n67% \n100% \n\nTraining Triplets \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\nMRR \n\nUMLS \n\nAlgorithm \n\nNeuralLP \nDRUM \nMINERVA \nRotatE \nRNNLogic w/o emb. \n\nKinship UMLS \nMRR \nMRR \n\nREINFORCE \n0.312 \n0.504 \n\nEM \n0.639 \n0.745 \n\n\n\nTable 4 :\n4Comparison between REINFORCE and EM.\n\nTable 5 :\n5Case study of the rules generated by the rule generator.\n\n\nWenhan Xiong, Thien Hoang, and William Yang Wang. Deeppath: A reinforcement learning method for knowledge graph reasoning. In EMNLP, 2017.This project is supported by the Natural Sciences and Engineering Research Council (NSERC) \nDiscovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft \nResearch and Mila, Samsung Electronics Co., Ldt., Amazon Faculty Research Award, Tencent AI \nLab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06). This project \nwas also partially funded by IVADO Fundamental Research Project grant PRF-2019-3583139727. \nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement \nlearning. Machine learning, 1992. \n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and \nrelations for learning and inference in knowledge bases. In ICLR, 2015. \n\nFan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge \nbase reasoning. In NeurIPS, 2017. \n\nYuan Yang and Le Song. Learn to explain efficiently via neural logic inductive learning. In ICLR, \n2020. \n\nYuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Efficient \nprobabilistic logic reasoning with graph neural networks. In ICLR, 2020. \n\nZhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford \nnetworks: A general graph neural network framework for link prediction. arXiv preprint, 2021. \n\n\nTable 6 :\n6Statistics of datasets.Dataset \n#Entities #Relations \n#Train \n#Validation #Test \nFB15K-237 \n14,541 \n237 \n272,115 \n17,535 \n20,466 \nWN18RR \n40,943 \n11 \n86,835 \n3,034 \n3,134 \nKinship \n104 \n25 \n3,206 \n2,137 \n5,343 \nUMLS \n135 \n46 \n1,959 \n1,306 \n3,264 \n\nD.2 EXPERIMENTAL SETUP OF RNNLOGIC \n\n\n\nTable 7 :\n7Logic rules learned by RNNLogic.Relation \u2190 Rule (Explanation)\nMore precisely, z is a multiset. In this paper, we use \"set\" to refer to \"multiset\" for conciseness.2  The codes of RNNLogic are available: https://github.com/DeepGraphLearning/RNNLogic\n\nTensor factorization for knowledge graph completion. Ivana Balazevic, Carl Allen, Timothy Hospedales, Tucker, EMNLP. Ivana Balazevic, Carl Allen, and Timothy Hospedales. Tucker: Tensor factorization for knowledge graph completion. In EMNLP, 2019.\n\nTranslating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko, NeurIPS. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In NeurIPS, 2013.\n\nKbgan: Adversarial learning for knowledge graph embeddings. Liwei Cai, William Yang Wang, NAACL. Liwei Cai and William Yang Wang. Kbgan: Adversarial learning for knowledge graph embeddings. In NAACL, 2018.\n\nVariational knowledge graph reasoning. Wenhu Chen, Wenhan Xiong, Xifeng Yan, William Wang, NAACL. Wenhu Chen, Wenhan Xiong, Xifeng Yan, and William Wang. Variational knowledge graph reasoning. In NAACL, 2018.\n\nTensorlog: Deep learning meets probabilistic databases. Fan William W Cohen, Kathryn Rivard Yang, Mazaitis, Journal of Artificial Intelligence Research. William W Cohen, Fan Yang, and Kathryn Rivard Mazaitis. Tensorlog: Deep learning meets probabilistic databases. Journal of Artificial Intelligence Research, 2018.\n\nPrincipal neighbourhood aggregation for graph nets. Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, Petar Veli\u010dkovi\u0107, NeurIPS. Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Veli\u010dkovi\u0107. Principal neighbourhood aggregation for graph nets. In NeurIPS, 2020.\n\nStochastic logic programs: sampling, inference and applications. James Cussens, UAI. James Cussens. Stochastic logic programs: sampling, inference and applications. In UAI, 2000.\n\nGo for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew Mccallum, ICLR. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishna- murthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. In ICLR, 2018.\n\nConvolutional 2d knowledge graph embeddings. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, AAAI. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In AAAI, 2018.\n\nRule-enhanced penalized regression by column generation using rectangular maximum agreement. Jonathan Eckstein, Noam Goldberg, Ai Kagawa, Jonathan Eckstein, Noam Goldberg, and Ai Kagawa. Rule-enhanced penalized regression by column generation using rectangular maximum agreement. In ICML, 2017.\n\nAmie: association rule mining under incomplete evidence in ontological knowledge bases. Luis Antonio Gal\u00e1rraga, Christina Teflioudi, Katja Hose, Fabian Suchanek, WWWLuis Antonio Gal\u00e1rraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek. Amie: association rule mining under incomplete evidence in ontological knowledge bases. In WWW, 2013.\n\nBoosting classifiers with tightened l0-relaxation penalties. Noam Goldberg, Jonathan Eckstein, ICML. Noam Goldberg and Jonathan Eckstein. Boosting classifiers with tightened l0-relaxation penalties. In ICML, 2010.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 1997.\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, ICLR. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\n\nLearning markov logic networks via functional gradient boosting. Tushar Khot, Sriraam Natarajan, Kristian Kersting, Jude Shavlik, ICDM. Tushar Khot, Sriraam Natarajan, Kristian Kersting, and Jude Shavlik. Learning markov logic networks via functional gradient boosting. In ICDM, 2011.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.\n\nLearning the structure of markov logic networks. Stanley Kok, Pedro Domingos, ICML. Stanley Kok and Pedro Domingos. Learning the structure of markov logic networks. In ICML, 2005.\n\nStatistical predicate invention. Stanley Kok, Pedro Domingos, ICML. Stanley Kok and Pedro Domingos. Statistical predicate invention. In ICML, 2007.\n\nProbabilistic graphical models: principles and techniques. Daphne Koller, Nir Friedman, MIT pressDaphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.\n\nCanonical tensor decomposition for knowledge base completion. Timothee Lacroix, Nicolas Usunier, Guillaume Obozinski, ICML. Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for knowledge base completion. In ICML, 2018.\n\nRelational retrieval using a combination of path-constrained random walks. Machine learning. Ni Lao, William W Cohen, Ni Lao and William W Cohen. Relational retrieval using a combination of path-constrained random walks. Machine learning, 2010.\n\nRandom walk inference and learning in a large scale knowledge base. Ni Lao, Tom Mitchell, William W Cohen, EMNLP. Ni Lao, Tom Mitchell, and William W Cohen. Random walk inference and learning in a large scale knowledge base. In EMNLP, 2011.\n\nMulti-hop knowledge graph reasoning with reward shaping. Richard Xi Victoria Lin, Caiming Socher, Xiong, In EMNLP. Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hop knowledge graph reasoning with reward shaping. In EMNLP, 2018.\n", "annotations": {"author": "[{\"end\":176,\"start\":115},{\"end\":211,\"start\":177},{\"end\":287,\"start\":212},{\"end\":406,\"start\":288},{\"end\":511,\"start\":407}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":120},{\"end\":188,\"start\":184},{\"end\":233,\"start\":225},{\"end\":301,\"start\":295},{\"end\":416,\"start\":412}]", "author_first_name": "[{\"end\":119,\"start\":115},{\"end\":183,\"start\":177},{\"end\":224,\"start\":212},{\"end\":294,\"start\":288},{\"end\":411,\"start\":407}]", "author_affiliation": "[{\"end\":150,\"start\":124},{\"end\":175,\"start\":152},{\"end\":210,\"start\":190},{\"end\":261,\"start\":235},{\"end\":286,\"start\":263},{\"end\":329,\"start\":303},{\"end\":354,\"start\":331},{\"end\":405,\"start\":356},{\"end\":444,\"start\":418},{\"end\":459,\"start\":446},{\"end\":510,\"start\":461}]", "title": "[{\"end\":112,\"start\":1},{\"end\":623,\"start\":512}]", "venue": null, "abstract": "[{\"end\":3527,\"start\":625}]", "bib_ref": "[{\"end\":4462,\"start\":4445},{\"end\":4481,\"start\":4462},{\"end\":4589,\"start\":4566},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4885,\"start\":4866},{\"end\":4941,\"start\":4912},{\"end\":5167,\"start\":5148},{\"end\":5223,\"start\":5195},{\"end\":5524,\"start\":5504},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6197,\"start\":6175},{\"end\":6225,\"start\":6197},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6243,\"start\":6225},{\"end\":6300,\"start\":6276},{\"end\":6323,\"start\":6300},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6372,\"start\":6348},{\"end\":6394,\"start\":6372},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6428,\"start\":6409},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6445,\"start\":6428},{\"end\":6526,\"start\":6507},{\"end\":6532,\"start\":6526},{\"end\":6534,\"start\":6532},{\"end\":6735,\"start\":6707},{\"end\":6753,\"start\":6735},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6772,\"start\":6753},{\"end\":6795,\"start\":6772},{\"end\":6813,\"start\":6795},{\"end\":6868,\"start\":6840},{\"end\":6891,\"start\":6868},{\"end\":7980,\"start\":7960},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7998,\"start\":7980},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8015,\"start\":7998},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8032,\"start\":8015},{\"end\":8050,\"start\":8032},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8258,\"start\":8240},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8982,\"start\":8961},{\"end\":9001,\"start\":8982},{\"end\":9019,\"start\":9001},{\"end\":9039,\"start\":9019},{\"end\":9062,\"start\":9039},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9079,\"start\":9062},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9101,\"start\":9079},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9124,\"start\":9101},{\"end\":9141,\"start\":9124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9979,\"start\":9958},{\"end\":10039,\"start\":10014},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10386,\"start\":10359},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10408,\"start\":10386},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14485,\"start\":14453},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15469,\"start\":15454},{\"end\":17066,\"start\":17041},{\"end\":19776,\"start\":19750},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19828,\"start\":19809},{\"end\":19850,\"start\":19828},{\"end\":23538,\"start\":23521},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23723,\"start\":23699},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26480,\"start\":26460},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27729,\"start\":27706},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27770,\"start\":27748},{\"end\":28207,\"start\":28178},{\"end\":28272,\"start\":28248},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28309,\"start\":28290},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29548,\"start\":29527},{\"end\":29946,\"start\":29929},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30388,\"start\":30370},{\"end\":30422,\"start\":30393},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":43995,\"start\":43971},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":50796,\"start\":50764},{\"end\":53118,\"start\":53093},{\"end\":54991,\"start\":54975},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":56655,\"start\":56634},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":56751,\"start\":56729},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":56781,\"start\":56758},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":56813,\"start\":56789}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56843,\"start\":56198},{\"attributes\":{\"id\":\"fig_1\"},\"end\":56967,\"start\":56844},{\"attributes\":{\"id\":\"fig_3\"},\"end\":57020,\"start\":56968},{\"attributes\":{\"id\":\"fig_4\"},\"end\":57566,\"start\":57021},{\"attributes\":{\"id\":\"fig_5\"},\"end\":57608,\"start\":57567},{\"attributes\":{\"id\":\"fig_6\"},\"end\":57685,\"start\":57609},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57752,\"start\":57686},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59120,\"start\":57753},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59711,\"start\":59121},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":59795,\"start\":59712},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":60962,\"start\":59796},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61375,\"start\":60963},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62503,\"start\":61376},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":62552,\"start\":62504},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":62621,\"start\":62553},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":64117,\"start\":62622},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":64415,\"start\":64118},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":64489,\"start\":64416}]", "paragraph": "[{\"end\":4722,\"start\":3543},{\"end\":5818,\"start\":4724},{\"end\":7710,\"start\":5835},{\"end\":8805,\"start\":7712},{\"end\":9911,\"start\":8807},{\"end\":10300,\"start\":9913},{\"end\":10792,\"start\":10302},{\"end\":10986,\"start\":10802},{\"end\":11383,\"start\":10988},{\"end\":11908,\"start\":11385},{\"end\":12731,\"start\":11910},{\"end\":13275,\"start\":12763},{\"end\":13528,\"start\":13356},{\"end\":13840,\"start\":13662},{\"end\":14255,\"start\":13842},{\"end\":14371,\"start\":14269},{\"end\":14880,\"start\":14373},{\"end\":15215,\"start\":14918},{\"end\":15413,\"start\":15217},{\"end\":15961,\"start\":15415},{\"end\":16056,\"start\":15963},{\"end\":16154,\"start\":16058},{\"end\":16356,\"start\":16247},{\"end\":16664,\"start\":16394},{\"end\":18043,\"start\":16666},{\"end\":18218,\"start\":18045},{\"end\":18281,\"start\":18220},{\"end\":18284,\"start\":18283},{\"end\":18413,\"start\":18286},{\"end\":18604,\"start\":18415},{\"end\":18916,\"start\":18606},{\"end\":19009,\"start\":18918},{\"end\":19324,\"start\":19011},{\"end\":19601,\"start\":19399},{\"end\":20006,\"start\":19603},{\"end\":20318,\"start\":20018},{\"end\":20937,\"start\":20320},{\"end\":21130,\"start\":20939},{\"end\":21614,\"start\":21212},{\"end\":21897,\"start\":21697},{\"end\":22098,\"start\":21899},{\"end\":22709,\"start\":22100},{\"end\":23330,\"start\":22711},{\"end\":23827,\"start\":23332},{\"end\":24034,\"start\":23839},{\"end\":24238,\"start\":24036},{\"end\":24622,\"start\":24314},{\"end\":24718,\"start\":24624},{\"end\":25265,\"start\":24720},{\"end\":25775,\"start\":25279},{\"end\":26073,\"start\":25777},{\"end\":26629,\"start\":26139},{\"end\":27115,\"start\":26631},{\"end\":27265,\"start\":27198},{\"end\":27566,\"start\":27267},{\"end\":27992,\"start\":27603},{\"end\":30333,\"start\":27994},{\"end\":31343,\"start\":30335},{\"end\":31546,\"start\":31345},{\"end\":32038,\"start\":31548},{\"end\":32316,\"start\":32040},{\"end\":33581,\"start\":32328},{\"end\":33969,\"start\":33583},{\"end\":34661,\"start\":33971},{\"end\":35918,\"start\":34663},{\"end\":36652,\"start\":35920},{\"end\":37358,\"start\":36667},{\"end\":37970,\"start\":37378},{\"end\":37998,\"start\":37972},{\"end\":38165,\"start\":38000},{\"end\":38356,\"start\":38167},{\"end\":38839,\"start\":38435},{\"end\":39118,\"start\":38918},{\"end\":39181,\"start\":39120},{\"end\":39630,\"start\":39183},{\"end\":39785,\"start\":39697},{\"end\":39887,\"start\":39787},{\"end\":40206,\"start\":39889},{\"end\":40384,\"start\":40208},{\"end\":40558,\"start\":40386},{\"end\":40729,\"start\":40640},{\"end\":41030,\"start\":40966},{\"end\":41360,\"start\":41299},{\"end\":41413,\"start\":41362},{\"end\":41610,\"start\":41492},{\"end\":41801,\"start\":41650},{\"end\":41957,\"start\":41870},{\"end\":42086,\"start\":41959},{\"end\":42123,\"start\":42088},{\"end\":42319,\"start\":42125},{\"end\":42402,\"start\":42388},{\"end\":42464,\"start\":42404},{\"end\":42512,\"start\":42466},{\"end\":42621,\"start\":42594},{\"end\":43090,\"start\":42687},{\"end\":43201,\"start\":43092},{\"end\":43715,\"start\":43417},{\"end\":43744,\"start\":43717},{\"end\":43907,\"start\":43746},{\"end\":44578,\"start\":43909},{\"end\":44798,\"start\":44618},{\"end\":45139,\"start\":44800},{\"end\":45357,\"start\":45253},{\"end\":45765,\"start\":45359},{\"end\":46512,\"start\":45777},{\"end\":46796,\"start\":46524},{\"end\":46884,\"start\":46798},{\"end\":46941,\"start\":46886},{\"end\":47116,\"start\":46943},{\"end\":47214,\"start\":47139},{\"end\":47541,\"start\":47216},{\"end\":47881,\"start\":47597},{\"end\":48178,\"start\":47926},{\"end\":48279,\"start\":48180},{\"end\":48374,\"start\":48281},{\"end\":48652,\"start\":48468},{\"end\":49038,\"start\":48732},{\"end\":49142,\"start\":49040},{\"end\":49653,\"start\":49144},{\"end\":50325,\"start\":49726},{\"end\":51057,\"start\":50348},{\"end\":51113,\"start\":51059},{\"end\":51337,\"start\":51130},{\"end\":51525,\"start\":51375},{\"end\":51624,\"start\":51527},{\"end\":52278,\"start\":51626},{\"end\":52877,\"start\":52280},{\"end\":53403,\"start\":52879},{\"end\":54184,\"start\":53405},{\"end\":54382,\"start\":54186},{\"end\":54588,\"start\":54384},{\"end\":54832,\"start\":54605},{\"end\":55138,\"start\":54864},{\"end\":55495,\"start\":55202},{\"end\":56197,\"start\":55497}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13355,\"start\":13276},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13642,\"start\":13529},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14268,\"start\":14256},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14917,\"start\":14881},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16246,\"start\":16155},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16393,\"start\":16357},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19398,\"start\":19325},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21211,\"start\":21131},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21696,\"start\":21615},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24313,\"start\":24239},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26138,\"start\":26074},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27197,\"start\":27116},{\"attributes\":{\"id\":\"formula_13\"},\"end\":38434,\"start\":38357},{\"attributes\":{\"id\":\"formula_14\"},\"end\":38917,\"start\":38840},{\"attributes\":{\"id\":\"formula_15\"},\"end\":39696,\"start\":39631},{\"attributes\":{\"id\":\"formula_16\"},\"end\":40639,\"start\":40559},{\"attributes\":{\"id\":\"formula_17\"},\"end\":40965,\"start\":40730},{\"attributes\":{\"id\":\"formula_18\"},\"end\":41298,\"start\":41031},{\"attributes\":{\"id\":\"formula_19\"},\"end\":41491,\"start\":41414},{\"attributes\":{\"id\":\"formula_20\"},\"end\":41649,\"start\":41611},{\"attributes\":{\"id\":\"formula_21\"},\"end\":41869,\"start\":41802},{\"attributes\":{\"id\":\"formula_23\"},\"end\":42387,\"start\":42320},{\"attributes\":{\"id\":\"formula_25\"},\"end\":42593,\"start\":42513},{\"attributes\":{\"id\":\"formula_26\"},\"end\":43416,\"start\":43202},{\"attributes\":{\"id\":\"formula_27\"},\"end\":45252,\"start\":45140},{\"attributes\":{\"id\":\"formula_29\"},\"end\":47138,\"start\":47117},{\"attributes\":{\"id\":\"formula_30\"},\"end\":48467,\"start\":48375},{\"attributes\":{\"id\":\"formula_31\"},\"end\":48731,\"start\":48653},{\"attributes\":{\"id\":\"formula_32\"},\"end\":49725,\"start\":49654},{\"attributes\":{\"id\":\"formula_33\"},\"end\":51129,\"start\":51114},{\"attributes\":{\"id\":\"formula_34\"},\"end\":51374,\"start\":51338},{\"attributes\":{\"id\":\"formula_35\"},\"end\":55201,\"start\":55139}]", "table_ref": "[{\"end\":32431,\"start\":32428},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":52024,\"start\":52017},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":54695,\"start\":54688}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3541,\"start\":3529},{\"attributes\":{\"n\":\"2\"},\"end\":5833,\"start\":5821},{\"attributes\":{\"n\":\"3\"},\"end\":10800,\"start\":10795},{\"attributes\":{\"n\":\"3.1\"},\"end\":12761,\"start\":12734},{\"attributes\":{\"n\":\"3.2\"},\"end\":13660,\"start\":13644},{\"end\":20016,\"start\":20009},{\"end\":23837,\"start\":23830},{\"attributes\":{\"n\":\"3.4\"},\"end\":25277,\"start\":25268},{\"attributes\":{\"n\":\"4\"},\"end\":27579,\"start\":27569},{\"attributes\":{\"n\":\"4.1\"},\"end\":27601,\"start\":27582},{\"attributes\":{\"n\":\"4.2\"},\"end\":32326,\"start\":32319},{\"attributes\":{\"n\":\"5\"},\"end\":36665,\"start\":36655},{\"end\":37376,\"start\":37361},{\"end\":42685,\"start\":42624},{\"end\":44616,\"start\":44581},{\"end\":45775,\"start\":45768},{\"end\":46522,\"start\":46515},{\"end\":47595,\"start\":47544},{\"end\":47924,\"start\":47884},{\"end\":50346,\"start\":50328},{\"end\":54603,\"start\":54591},{\"end\":54862,\"start\":54835},{\"end\":56855,\"start\":56845},{\"end\":56979,\"start\":56969},{\"end\":57688,\"start\":57687},{\"end\":57763,\"start\":57754},{\"end\":59131,\"start\":59122},{\"end\":59806,\"start\":59797},{\"end\":62514,\"start\":62505},{\"end\":62563,\"start\":62554},{\"end\":64128,\"start\":64119},{\"end\":64426,\"start\":64417}]", "table": "[{\"end\":59120,\"start\":57949},{\"end\":59711,\"start\":59312},{\"end\":60962,\"start\":59875},{\"end\":62503,\"start\":61742},{\"end\":64117,\"start\":62762},{\"end\":64415,\"start\":64153}]", "figure_caption": "[{\"end\":56843,\"start\":56200},{\"end\":56967,\"start\":56857},{\"end\":57020,\"start\":56981},{\"end\":57566,\"start\":57023},{\"end\":57608,\"start\":57569},{\"end\":57685,\"start\":57611},{\"end\":57752,\"start\":57689},{\"end\":57949,\"start\":57765},{\"end\":59312,\"start\":59133},{\"end\":59795,\"start\":59714},{\"end\":59875,\"start\":59808},{\"end\":61375,\"start\":60965},{\"end\":61742,\"start\":61378},{\"end\":62552,\"start\":62516},{\"end\":62621,\"start\":62565},{\"end\":62762,\"start\":62624},{\"end\":64153,\"start\":64130},{\"end\":64489,\"start\":64428}]", "figure_ref": "[{\"end\":9211,\"start\":9203},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35709,\"start\":35703},{\"end\":35716,\"start\":35710},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36154,\"start\":36148}]", "bib_author_first_name": "[{\"end\":64735,\"start\":64730},{\"end\":64751,\"start\":64747},{\"end\":64766,\"start\":64759},{\"end\":64991,\"start\":64984},{\"end\":65007,\"start\":65000},{\"end\":65024,\"start\":65017},{\"end\":65044,\"start\":65039},{\"end\":65059,\"start\":65053},{\"end\":65314,\"start\":65309},{\"end\":65327,\"start\":65320},{\"end\":65332,\"start\":65328},{\"end\":65500,\"start\":65495},{\"end\":65513,\"start\":65507},{\"end\":65527,\"start\":65521},{\"end\":65540,\"start\":65533},{\"end\":65725,\"start\":65722},{\"end\":65750,\"start\":65743},{\"end\":65757,\"start\":65751},{\"end\":66043,\"start\":66035},{\"end\":66055,\"start\":66051},{\"end\":66076,\"start\":66067},{\"end\":66091,\"start\":66085},{\"end\":66102,\"start\":66097},{\"end\":66349,\"start\":66344},{\"end\":66577,\"start\":66569},{\"end\":66591,\"start\":66583},{\"end\":66610,\"start\":66604},{\"end\":66623,\"start\":66619},{\"end\":66637,\"start\":66632},{\"end\":66654,\"start\":66648},{\"end\":66674,\"start\":66670},{\"end\":66688,\"start\":66682},{\"end\":67015,\"start\":67012},{\"end\":67034,\"start\":67026},{\"end\":67052,\"start\":67046},{\"end\":67073,\"start\":67064},{\"end\":67324,\"start\":67316},{\"end\":67339,\"start\":67335},{\"end\":67352,\"start\":67350},{\"end\":67611,\"start\":67607},{\"end\":67640,\"start\":67631},{\"end\":67657,\"start\":67652},{\"end\":67670,\"start\":67664},{\"end\":67930,\"start\":67926},{\"end\":67949,\"start\":67941},{\"end\":68108,\"start\":68104},{\"end\":68127,\"start\":68121},{\"end\":68308,\"start\":68304},{\"end\":68323,\"start\":68315},{\"end\":68331,\"start\":68328},{\"end\":68523,\"start\":68517},{\"end\":68537,\"start\":68530},{\"end\":68557,\"start\":68549},{\"end\":68572,\"start\":68568},{\"end\":68783,\"start\":68782},{\"end\":68799,\"start\":68794},{\"end\":68966,\"start\":68959},{\"end\":68977,\"start\":68972},{\"end\":69131,\"start\":69124},{\"end\":69142,\"start\":69137},{\"end\":69305,\"start\":69299},{\"end\":69317,\"start\":69314},{\"end\":69516,\"start\":69508},{\"end\":69533,\"start\":69526},{\"end\":69552,\"start\":69543},{\"end\":69803,\"start\":69801},{\"end\":70024,\"start\":70022},{\"end\":70033,\"start\":70030},{\"end\":70051,\"start\":70044},{\"end\":70053,\"start\":70052},{\"end\":70260,\"start\":70253},{\"end\":70285,\"start\":70278}]", "bib_author_last_name": "[{\"end\":64745,\"start\":64736},{\"end\":64757,\"start\":64752},{\"end\":64777,\"start\":64767},{\"end\":64785,\"start\":64779},{\"end\":64998,\"start\":64992},{\"end\":65015,\"start\":65008},{\"end\":65037,\"start\":65025},{\"end\":65051,\"start\":65045},{\"end\":65069,\"start\":65060},{\"end\":65318,\"start\":65315},{\"end\":65337,\"start\":65333},{\"end\":65505,\"start\":65501},{\"end\":65519,\"start\":65514},{\"end\":65531,\"start\":65528},{\"end\":65545,\"start\":65541},{\"end\":65741,\"start\":65726},{\"end\":65762,\"start\":65758},{\"end\":65772,\"start\":65764},{\"end\":66049,\"start\":66044},{\"end\":66065,\"start\":66056},{\"end\":66083,\"start\":66077},{\"end\":66095,\"start\":66092},{\"end\":66113,\"start\":66103},{\"end\":66357,\"start\":66350},{\"end\":66581,\"start\":66578},{\"end\":66602,\"start\":66592},{\"end\":66617,\"start\":66611},{\"end\":66630,\"start\":66624},{\"end\":66646,\"start\":66638},{\"end\":66668,\"start\":66655},{\"end\":66680,\"start\":66675},{\"end\":66697,\"start\":66689},{\"end\":67024,\"start\":67016},{\"end\":67044,\"start\":67035},{\"end\":67062,\"start\":67053},{\"end\":67080,\"start\":67074},{\"end\":67333,\"start\":67325},{\"end\":67348,\"start\":67340},{\"end\":67359,\"start\":67353},{\"end\":67629,\"start\":67612},{\"end\":67650,\"start\":67641},{\"end\":67662,\"start\":67658},{\"end\":67679,\"start\":67671},{\"end\":67939,\"start\":67931},{\"end\":67958,\"start\":67950},{\"end\":68119,\"start\":68109},{\"end\":68139,\"start\":68128},{\"end\":68313,\"start\":68309},{\"end\":68326,\"start\":68324},{\"end\":68337,\"start\":68332},{\"end\":68528,\"start\":68524},{\"end\":68547,\"start\":68538},{\"end\":68566,\"start\":68558},{\"end\":68580,\"start\":68573},{\"end\":68792,\"start\":68784},{\"end\":68806,\"start\":68800},{\"end\":68810,\"start\":68808},{\"end\":68970,\"start\":68967},{\"end\":68986,\"start\":68978},{\"end\":69135,\"start\":69132},{\"end\":69151,\"start\":69143},{\"end\":69312,\"start\":69306},{\"end\":69326,\"start\":69318},{\"end\":69524,\"start\":69517},{\"end\":69541,\"start\":69534},{\"end\":69562,\"start\":69553},{\"end\":69807,\"start\":69804},{\"end\":69824,\"start\":69809},{\"end\":70028,\"start\":70025},{\"end\":70042,\"start\":70034},{\"end\":70059,\"start\":70054},{\"end\":70276,\"start\":70261},{\"end\":70292,\"start\":70286},{\"end\":70299,\"start\":70294}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":64923,\"start\":64677},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14941970},\"end\":65247,\"start\":64925},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3401524},\"end\":65454,\"start\":65249},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4669223},\"end\":65664,\"start\":65456},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53615397},\"end\":65981,\"start\":65666},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215745143},\"end\":66277,\"start\":65983},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15956928},\"end\":66457,\"start\":66279},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13206339},\"end\":66965,\"start\":66459},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4328400},\"end\":67221,\"start\":66967},{\"attributes\":{\"id\":\"b9\"},\"end\":67517,\"start\":67223},{\"attributes\":{\"id\":\"b10\"},\"end\":67863,\"start\":67519},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16170082},\"end\":68078,\"start\":67865},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1915014},\"end\":68250,\"start\":68080},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2428314},\"end\":68450,\"start\":68252},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8627261},\"end\":68736,\"start\":68452},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6628106},\"end\":68908,\"start\":68738},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9654383},\"end\":69089,\"start\":68910},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6911541},\"end\":69238,\"start\":69091},{\"attributes\":{\"id\":\"b18\"},\"end\":69444,\"start\":69240},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49310354},\"end\":69706,\"start\":69446},{\"attributes\":{\"id\":\"b20\"},\"end\":69952,\"start\":69708},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1619841},\"end\":70194,\"start\":69954},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52143467},\"end\":70435,\"start\":70196}]", "bib_title": "[{\"end\":64728,\"start\":64677},{\"end\":64982,\"start\":64925},{\"end\":65307,\"start\":65249},{\"end\":65493,\"start\":65456},{\"end\":65720,\"start\":65666},{\"end\":66033,\"start\":65983},{\"end\":66342,\"start\":66279},{\"end\":66567,\"start\":66459},{\"end\":67010,\"start\":66967},{\"end\":67924,\"start\":67865},{\"end\":68102,\"start\":68080},{\"end\":68302,\"start\":68252},{\"end\":68515,\"start\":68452},{\"end\":68780,\"start\":68738},{\"end\":68957,\"start\":68910},{\"end\":69122,\"start\":69091},{\"end\":69506,\"start\":69446},{\"end\":70020,\"start\":69954},{\"end\":70251,\"start\":70196}]", "bib_author": "[{\"end\":64747,\"start\":64730},{\"end\":64759,\"start\":64747},{\"end\":64779,\"start\":64759},{\"end\":64787,\"start\":64779},{\"end\":65000,\"start\":64984},{\"end\":65017,\"start\":65000},{\"end\":65039,\"start\":65017},{\"end\":65053,\"start\":65039},{\"end\":65071,\"start\":65053},{\"end\":65320,\"start\":65309},{\"end\":65339,\"start\":65320},{\"end\":65507,\"start\":65495},{\"end\":65521,\"start\":65507},{\"end\":65533,\"start\":65521},{\"end\":65547,\"start\":65533},{\"end\":65743,\"start\":65722},{\"end\":65764,\"start\":65743},{\"end\":65774,\"start\":65764},{\"end\":66051,\"start\":66035},{\"end\":66067,\"start\":66051},{\"end\":66085,\"start\":66067},{\"end\":66097,\"start\":66085},{\"end\":66115,\"start\":66097},{\"end\":66359,\"start\":66344},{\"end\":66583,\"start\":66569},{\"end\":66604,\"start\":66583},{\"end\":66619,\"start\":66604},{\"end\":66632,\"start\":66619},{\"end\":66648,\"start\":66632},{\"end\":66670,\"start\":66648},{\"end\":66682,\"start\":66670},{\"end\":66699,\"start\":66682},{\"end\":67026,\"start\":67012},{\"end\":67046,\"start\":67026},{\"end\":67064,\"start\":67046},{\"end\":67082,\"start\":67064},{\"end\":67335,\"start\":67316},{\"end\":67350,\"start\":67335},{\"end\":67361,\"start\":67350},{\"end\":67631,\"start\":67607},{\"end\":67652,\"start\":67631},{\"end\":67664,\"start\":67652},{\"end\":67681,\"start\":67664},{\"end\":67941,\"start\":67926},{\"end\":67960,\"start\":67941},{\"end\":68121,\"start\":68104},{\"end\":68141,\"start\":68121},{\"end\":68315,\"start\":68304},{\"end\":68328,\"start\":68315},{\"end\":68339,\"start\":68328},{\"end\":68530,\"start\":68517},{\"end\":68549,\"start\":68530},{\"end\":68568,\"start\":68549},{\"end\":68582,\"start\":68568},{\"end\":68794,\"start\":68782},{\"end\":68808,\"start\":68794},{\"end\":68812,\"start\":68808},{\"end\":68972,\"start\":68959},{\"end\":68988,\"start\":68972},{\"end\":69137,\"start\":69124},{\"end\":69153,\"start\":69137},{\"end\":69314,\"start\":69299},{\"end\":69328,\"start\":69314},{\"end\":69526,\"start\":69508},{\"end\":69543,\"start\":69526},{\"end\":69564,\"start\":69543},{\"end\":69809,\"start\":69801},{\"end\":69826,\"start\":69809},{\"end\":70030,\"start\":70022},{\"end\":70044,\"start\":70030},{\"end\":70061,\"start\":70044},{\"end\":70278,\"start\":70253},{\"end\":70294,\"start\":70278},{\"end\":70301,\"start\":70294}]", "bib_venue": "[{\"end\":64792,\"start\":64787},{\"end\":65078,\"start\":65071},{\"end\":65344,\"start\":65339},{\"end\":65552,\"start\":65547},{\"end\":65817,\"start\":65774},{\"end\":66122,\"start\":66115},{\"end\":66362,\"start\":66359},{\"end\":66703,\"start\":66699},{\"end\":67086,\"start\":67082},{\"end\":67314,\"start\":67223},{\"end\":67605,\"start\":67519},{\"end\":67964,\"start\":67960},{\"end\":68159,\"start\":68141},{\"end\":68343,\"start\":68339},{\"end\":68586,\"start\":68582},{\"end\":68816,\"start\":68812},{\"end\":68992,\"start\":68988},{\"end\":69157,\"start\":69153},{\"end\":69297,\"start\":69240},{\"end\":69568,\"start\":69564},{\"end\":69799,\"start\":69708},{\"end\":70066,\"start\":70061},{\"end\":70309,\"start\":70301}]"}}}, "year": 2023, "month": 12, "day": 17}