{"id": 214714319, "updated": "2023-10-06 17:43:04.887", "metadata": {"title": "CNN-based Density Estimation and Crowd Counting: A Survey", "authors": "[{\"first\":\"Guangshuai\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Junyu\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Qingjie\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Qi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yunhong\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 3, "day": 28}, "abstract": "Accurately estimating the number of objects in a single image is a challenging yet meaningful task and has been applied in many applications such as urban planning and public safety. In the various object counting tasks, crowd counting is particularly prominent due to its specific significance to social security and development. Fortunately, the development of the techniques for crowd counting can be generalized to other related fields such as vehicle counting and environment survey, if without taking their characteristics into account. Therefore, many researchers are devoting to crowd counting, and many excellent works of literature and works have spurted out. In these works, they are must be helpful for the development of crowd counting. However, the question we should consider is why they are effective for this task. Limited by the cost of time and energy, we cannot analyze all the algorithms. In this paper, we have surveyed over 220 works to comprehensively and systematically study the crowd counting models, mainly CNN-based density map estimation methods. Finally, according to the evaluation metrics, we select the top three performers on their crowd counting datasets and analyze their merits and drawbacks. Through our analysis, we expect to make reasonable inference and prediction for the future development of crowd counting, and meanwhile, it can also provide feasible solutions for the problem of object counting in other fields. We provide the density maps and prediction results of some mainstream algorithm in the validation set of NWPU dataset for comparison and testing. Meanwhile, density map generation and evaluation tools are also provided. All the codes and evaluation results are made publicly available at https://github.com/gaoguangshuai/survey-for-crowd-counting.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.12783", "mag": "3013987429", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2003-12783", "doi": null}}, "content": {"source": {"pdf_hash": "26d0bffc682e0faed8f70196b793431afacf065a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.12783v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6f5f33acd5953b3447323eb4dfc676dd9d1abc1d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/26d0bffc682e0faed8f70196b793431afacf065a.txt", "contents": "\nCNN-based Density Estimation and Crowd Counting: A Survey\n\n\nGuangshuai Gao \nJunyu Gao \nStudent Member, IEEEQingjie Liu \nMember, IEEEQi Wang \nSenior Member, IEEEYunhong Wang \nCNN-based Density Estimation and Crowd Counting: A Survey\n1Index Terms-Object countingcrowd countingdensity estimationCNNs\nAccurately estimating the number of objects in a single image is a challenging yet meaningful task and has been applied in many applications such as urban planning and public safety. In the various object counting tasks, crowd counting is particularly prominent due to its specific significance to social security and development. Fortunately, the development of the techniques for crowd counting can be generalized to other related fields such as vehicle counting and environment survey, if without taking their characteristics into account. Therefore, many researchers are devoting to crowd counting, and many excellent works of literature and works have spurted out. In these works, they are must be helpful for the development of crowd counting. However, the question we should consider is why they are effective for this task. Limited by the cost of time and energy, we cannot analyze all the algorithms. In this paper, we have surveyed over 220 works to comprehensively and systematically study the crowd counting models, mainly CNN-based density map estimation methods. Finally, according to the evaluation metrics, we select the top three performers on their crowd counting datasets and analyze their merits and drawbacks. Through our analysis, we expect to make reasonable inference and prediction for the future development of crowd counting, and meanwhile, it can also provide feasible solutions for the problem of object counting in other fields. We provide the density maps and prediction results of some mainstream algorithm in the validation set of NWPU dataset for comparison and testing. Meanwhile, density map generation and evaluation tools are also provided. All the codes and evaluation results are made publicly available at https://github.com/gaoguangshuai/survey-for-crowd-counting.\n\nI. INTRODUCTION\n\nO VER the past few decades, an increasing number of research communities, have considered the problem of object counting as their mainly research direction, as a consequence, many works have been published to count the number of objects in images or videos across wide variety of domains such as crowding counting [1]- [13], cell microscopy [14]- [16], animals [17], vehicles [2], [18]- [20], leaves [21], [22] and environment survey [23], [24]. In all these domains, crowd counting is of paramount importance, and it is crucial to Guangshuai  building a more high-level cognitive ability in some crowd scenarios, such as crowd analysis [25], [26] and video surveillance [27]. As the increasing growth of the world's population and subsequent urbanization result in a rapid crowd gathering in many scenarios such as parades, concerts and stadiums. In these scenarios, crowd counting plays an indispensable role for social safety and control management.\n\nConsidering the specific importance of crowd counting aforementioned, more and more researchers have attempted to design various sophisticated projects to address the problem of crowd counting. Especially in the last half decades, with the advent of deep learning, Convolution Neural Networks (CNNs) based models have been overwhelmingly dominated in various computer vision tasks, including crowd counting. Although different tasks have their unique attributes, there exist common features such as structural features and distribution patterns. Fortunately, the techniques for crowd counting can be extended to some other fields with specific tools. Therefore, in this paper, we expect to provide a reasonable solution for other tasks through the deep excavation of the crowd counting task, especially for CNN-based density estimation and crowd counting models. Our survey aims to involve various parts, which is ranging algorithm taxonomy from some interesting under-explored research direction. Beyond taxonomically reviewing existing CNN-based crowd counting and density estimation models, representing datasets and evaluation metrics, some factors and attributes, which largely affect the performance the designed model, are also investigated, such as distractors and negative samples. We provide the density maps and prediction results of some mainstream algorithm in the validation set of NWPU dataset [28] for comparison and testing. Meanwhile, density map generation and evaluation tools are also provided. All the codes and evaluation results are made publicly available at https://github.com/gaoguangshuai/ survey-for-crowd-counting.\n\n\nA. Related Works and Scope\n\nThe various approaches for crowd counting are mainly divided into four categories: detection-based, regression-based, density estimation, and more recently CNN-based density estimation approaches. We focus on the CNN-based density estimation and crowd counting model in this survey. For the sake of completeness, it is necessary to review some other related works in this subsection.\n\nEarly works [29]- [32] on crowd counting use detectionbased approaches. These approaches usually apply a person or head detector via a sliding window on an image. Recently many extraordinary object detectors such as R-CNN [33]- [35], YOLO [36], and SSD [37] have been presented, which may perform dramatic detection accuracy in the sparse scenes. However, they will present unsatisfactory results when encountered the situation of occlusion and background clutter in extremely dense crowds.\n\nTo reduce the above problems, some works [27], [38], [39] introduce regression-based methods which directly learn the mapping from an image patch to the count. They usually first extract global features [40] (texture, gradient, edge features), or local features [41] (SIFT [42], LBP [43], HOG [44], GLCM [45]). Then some regression techniques such as linear regression [46] and Gaussian mixture regression [47] are used to learn a mapping function to the crowd counting.\n\nThese methods are successful in dealing with the problems of occlusion and background clutter, but they always ignore spatial information. Therefore, Lemptisky et al. [16] first adopt a density estimation based method by learning a linear mapping between local features and corresponding density maps. For reducing the difficulty of learning a linear mapping, [48] proposes a non-linear mapping, random forest regression, which obtains satisfactory performance by introducing a crowdedness prior and using it to train two different forests. Besides, this method needs less memory to store the forest. These methods consider the spatial information, but they only use traditional hand-crafted features to extract lowlevel information, which cannot guide the high-quality density map to estimate more accurate counting.\n\nRecently, benefiting from the powerful feature representation of CNNs, more researchers utilize it to improve the density estimation. Earlier heuristic models typically leverage basic CNNs to predict the density of the crowds [15], [49]- [51], which obtain significant improvement compared with traditional hand-crafted features. Lately, more effective and efficient models based on Fully Convolution Network (FCN), which has become the mainstream network architecture for the density estimation and crowd counting. Different supervised level and learning paradigm for different models, also there are some models designed in cross scene and multiple domains. A brief chronology is shown in Fig. 1, which illustrates the main advancements and milestones of crowd counting techniques. The goal of this survey is focused on the modern CNN-based for density estimation and crowd counting, Fig. 2 depicts a taxonomy of curial methodologies to be covered in the survey.\n\nScope of the survey. Considering that reviewing all stateof-the-art methods is impractical (and fortunately unnecessary), this paper sorts out some mainstream algorithms, which are all influential or essential papers published in, but not limited to, prestigious journals and conferences. The survey focuses on the modern CNN-based density estimation methods in recent years, and some early works are also included for the sake of completeness. We classify existing methods into several categories, in terms of network architecture, supervision form, influence of cross-scene or multi-domain, etc. Such comprehensive and systematic taxonomies can be more helpful for the readers to in-depth understand the progress of crowd counting in the past years.\n\nB. Related previous reviews and surveys Table I lists the existing reviews or surveys which are related to our paper. Notably, Zhan et al [24] and Junior et al. [58] are the first ones for crowd analysis. Li et al. [62] review the task of crowded scene analysis with different methods, while Zitouni et al. [65] evaluate different methods with different criteria. Loy et al. [60] make detailed comparisons of state-of-the-arts for crowd counting based on video imagery with the same protocol. Ryan et al. [60] present an evaluation across multiple datasets to compare various image features and regression models and Saleh et al. [64] survey two main approaches in direct and indirect manners. Grant et al. [66] explore two kinds of crowd analysis. While these surveys make detail analysis on crowd counting and scene analysis, they are only for traditional methods with handcrafted features. In recent work, Sindagi et al. [67] provide a survey of recent state-of-the-art CNN-based approaches for crowd counting and density estimation for the single image. However, it only roughly introduces the latest advancement of CNN-based methods, which are only up to the year 2017. Tripathi et al. [68] put forward a review on crowd analysis using CNN, which is not just for crowd counting, thereby it was not adequate comprehensive and in-depth. As we know, the techniques are incremental month by month, and it is also an urgent need for us to document the development of crowd counting in the past half-decade.\n\nDifferent from previous surveys that focus on handcrafted features or primitive CNNs, our work systematically and comprehensively reviews CNN-based density estimation crowd counting approaches. Specifically, we summarize the existing crowd counting models from various aspects and list the results of some representing mainstream algorithms in terms of evaluation metrics on several typical benchmark crowd counting datasets. Finally, we select the top three performers and carefully and thoroughly analyze the properties of these models. We also offer insights for essential open issues, challenges, and future direction. Through this survey, we expect to make reasonable inference and prediction for the future development of crowd counting, and meanwhile, it can also provide feasible solutions and make guidance for the problem of object counting in other domains.\n\n\nC. Contributions of this paper\n\nIn summary, the contributions in this paper are mainly in the following folds:  [50], Cross scene [51], MCNN [1], Hydra-CNN [2], CP-CNN [6], CMTL [55], switching CNN [5], CSRNet [12], SANet [11], PSSDN [56] and LSF-CNN [57]. The trend in the past few years has been designing crowd counting models based on multi-column (in green), single-column (in red) network architecture and object localization or tracking depending on counting techniques (in crimson), which are either contemporary and potential direction in future. While traditional heuristic methods are highlighted with the blue-shaded area and the modern CNN-based density estimation and crowd counting models are with the red-shaded backgrounds, respectively.  3) Open questions and future directions. We look through some important issues for model design, dataset collection, and some generalization to other domains with domain adaptation or transfer learning and explore some promising research directions in the future.\n\n\nCrowd Counting\n\nThese contributions provide detailed and in-depth review, which differs from the previous review or survey works to a large extent.\n\nThe remainder of the paper is organized as follows. Section II conducts a comprehensive literature review of mainstream CNN-based density estimation and crowd counting models according to the proposed taxonomies. Section III examines the most notable datasets for crowd counting and some datasets for other object counting tasks, while section IV describes several widely used evaluation metrics. Section V benchmarks some representing models and makes an in-depth analysis. Section VI presents a discussion and put forward some open issues and possible future directions. Finally, the conclusion is concluded in Section VII.\n\n\nII. TAXONOMY FOR CROWD COUNTING\n\nIn this section, we review CNN-based crowd counting algorithms in the following taxonomies. Chiefly is representative network architectures for crowd counting (II-A). Next Crowd counting and profiling: Methodology and evaluation [60] 2013 MSVAC This study describes and compares the state-of-the-art methods for video imagery based crowd counting, and provides a systematic evaluation of different methods using the same protocol.\n\n\n5\n\nPerformance evaluation of crowd image analysis using the PETS2009 dataset [61] 2014 PRL This paper presents PETS2009 crowd analysis dataset and highlights detection and tracking performance on it 6 Crowded scene analysis: A survey [62] 2015 TCSVT This paper surveys the state-of-the-art techniques on crowded scene analysis with different methods such as crowd motion pattern learning, crowd behavior, activity analysis and anomaly detection in crowds. 7 An evaluation of crowd counting methods, features and regression models [63] 2015 CVIU This paper presents an evaluation across multiple datasets to compare holistic, local and histogram based methods, and to compare various image features and regression models.\n\n\n8\n\nRecent survey on crowd density estimation and counting for visual surveillance [64] 2015 EAAI This paper presents a survey on crowd density estimation and counting methods employed for visual surveillance in the perspective of computer vision research.\n\n\n9\n\nAdvances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques [65] 2016 Neurocomputing This paper aims to give an account of such issues by deducing key statistical evidence from the existing literature and providing recommendations towards focusing on the general aspects of techniques rather than any specific algorithm.\n\n10 Crowd scene understanding from video: a survey [66] 2017 TOMM This survey explores crowd analysis as it relates to two primary research areas: crowd statistics and behavior understanding.\n\n11 A survey of recent advances in cnn-based single image crowd counting and density estimation [67] 2018 PRL A review of various single image crowd counting and density estimation methods with a specific focus on recent CNN-based approaches.\n\n12 Convolutional neural networks for crowd behaviour analysis: a survey [68] 2019 VC A survey for crowd analysis using CNN is the learning paradigm of the methods (II-B), and then is the inference manner of the networks (II-C). Additionally, the supervision forms of networks are also introduced in II-D. Meanwhile, to evaluate the generalization ability of the algorithms, we classify existing works into domain-specific and multi-domain ones (II-E). Finally, based on the supervised level, we classify the CNN-based models into instance-level and image-level ones (II-F). We group the important models and describe them roughly in chronological order. A summary of the state-of-the-art is presented in Table II.\n\n\nA. Representative network architectures for crowd counting\n\nIn view of different types of network architectures, we divide crowd counting models into three categories: basic CNN based methods, multi-column based methods, and singlecolumn based methods. The category of network architectures is illustrated in Fig. 3.\n\n1) Basic CNN: This network architecture adopts the basic CNN layers which convolutional layers, pooling layers, uniquely fully connected layers, without additional feature information required. They generally are involved in the initial works using CNN for density estimation and crowd counting.\n\n\u2022 Fu et al. [50] put forward the first CNN-based model for crowd counting, which accelerates the speed and accuracy of the model by removing some similar network connections existed in feature maps and cascading two ConvNet classifiers.\n\n\u2022 Wang et al. [49] propose a deep network based on Alexnet architecture [102] for extremely dense crowd counting, the adoption of expanded negative samples, whose ground truth counting are zeros, to reduce the interference.\n\n\u2022 CNN-boosting [15] employs basic CNNs in a layer-wise manner, and leverages layered boosting and selective sampling to improve the counting accuracy and reduce training time.\n\nSince without additional feature information provided, basic CNNs are simple and easy to implement yet usually perform low accuracy.\n\n2) Multi-column: These network architectures usually adopt different columns to capture multi-scale information corresponding to different receptive fields, which have brought about excellent performance for crowd counting.\n\n\u2022 MCNN [1], a pioneering work explicitly focusing on the multi-scale problem. MCNN is a multi-column architecture with three branches that use different kernel sizes (large, medium, small). However, the similar even the same depth and structure of the three branches, which makes the network look like a simple assembling of several weak regressors.\n\n\u2022 Hydra-CNN [2] uses a pyramid of image patches corresponding to different scales to learn a multi-scale non-linear regression model for the final density map estimation.\n\n\u2022 CrowdNet [3] combines shallow and deep networks at different columns, of which the shallow one captures the lowlevel features corresponding to large scale variation and the deep one captures the high-level semantic information.\n\n\u2022 Switching CNN [5] trains several independent CNN crowd density regressors on the image patches, the regressors have the same structure with MCNN [1]. In addition, a switch classifier is also trained alternatively on the regressions to select the best one for the density estimation.\n\n\u2022 CP-CNN [6] is a contextual pyramid CNN that combines global and local contextual information to generate highquality density maps. Moreover, adversarial learning [103] is utilized to fuse the features from different levels.\n\n\u2022 TDF-CNN [104] delivers top-down information to the bottom-up network to amend the density estimation.\n\n\u2022 DRSAN [71] handles the issues of scale variation and rotation variation taking advantages of Spatial Transformer Network (STN) [105].\n\n\u2022 SAAN [8] is similar to the idea of MoC-CNN [106] and CP-CNN [6], but utilizes visual attention mechanism to automatically select the particular scale both for the global image level and local image patch level.\n\n\u2022 RANet [99] provides local self-attention (LSA) and global self-attention (GSA) to capture short-range and long-range in-terdependence information respectively, furthermore, a relation module is introduced to merge LSA and GSA to obtain more informative aggregated feature representations.\n\n\u2022 McML [100] incorporates a statistical network into the multi-column network to estimate the mutual information between different columns, the proposed mutual learning scheme which can optimize each column alternately whilst retaining other columns fixed on each mini-batch training data.\n\n\u2022 DADNet [107] takes dilated-CNN with different dilated rates to capture more contextual information as front-end and adaptive deformable convolution as a back-end to locate the positions of the objects accurately. Albeit great progress has been achieved by these multicolumn network, they still suffer from several significant disadvantages, which have been demonstrated through conducting experiments by Li et al. [12]. First of all, it is difficult to train the multi-column networks since it requires more time and a more bloated structure. Next, using different branches but almost the same network structures, it inevitably leads to a lot of information redundancy. Moreover, multi-column networks always require density-level classifiers before sending images into the networks. However, due to the number of crowds is varying greatly in the congested scene of the real world, making it difficult to define the granularity of density level. Meanwhile, more fine-grained classifiers also mean that more columns and more sophisticated structures are required to be designed, thereby causing more redundancy. Finally, these networks consume a large number of parameters for densitylevel classifiers rather than preparing them for the generation of final density maps. Thus the lack of parameters for density map generation will degrade the quality.\n\nAs all the disadvantages mentioned above, multi-column network architectures may be ineffective in a narrow sense. Thus it motivates many researchers to exploit simpler yet effective and efficient networks. Therefore, single column network architectures are come out to cater to the demands of more challenging situations in the crowd counting.\n\n3) Single column: The single-column network architectures usually deploy single and deeper CNNs rather than the bloated structure of multi-column network architecture, and the premise is not to increase the complexity of the network.\n\n\u2022 W-VLAD [108] takes account of semantic features and spatial cues, additionally, a novel locality-aware feature (LAF) is introduced to represent the spatial information.\n\n\u2022 SaCNN [9] is a scale-adaptive CNN that takes an FCN with fixed small receptive fields as backbone and adapts the feature maps extracted from multiple layers to the same sizes and then combines them to generate the final density map.\n\n\u2022 D-ConvNet [70] called as De-correlated ConvNet, takes advantage of negative correlation learning (NCL) to improve the generalization capability of the ensemble models with a set of weak regressors with convolutional feature maps.\n\n\u2022 CSRNet [12] adopts dilated convolution layers to expand the receptive field while maintaining the resolution as backend network.\n\n\u2022 SANet [11] is built on the shoulder of Inception architecture [109] in the encoder to extract multi-scale features and using Transposed convolution layers in the decoder to upsampling the extracted feature maps.\n\n\u2022 SPN [81] leverages a shared deep single-column structure and extracts the multi-scale features in the high-layers by Scale Pyramid Module (SPM), which deploys four parallel dilated convolution with different dilation rates.\n\n\u2022 ADCrowdNet [83] combines visual attention mechanism and multi-scale deformable convolutional scheme into a cascading framework.\n\n\u2022 SAA-Net [13] mimics multi-branches but single column by learning a set of soft gate attention mask on the intermediate feature maps, which uses the hierarchical structure of CNNs. The ides behind it is somewhat similar to SaCNN [9] but adding attention mask on corresponding feature maps.\n\n\u2022 W-Net [90] is inspired by U-Net [110], adding an auxiliary Reinforcement branch to accelerate the convergence and retain local pattern consistency, and using Structural Similarity Index (SSIM) to estimate the final density maps.\n\n\u2022 TEDnet [92] is a trellis encoder-decoder network architecture, which integrates multiple decoding paths to capture multi-scale features and exploits dense skip connections to obtain the supervised information. In addition, to alleviate the gradient vanishing problem and improve the back-propagation ability, a combinational loss comprising local coherence and spatial correlation loss is also presented.\n\nDue to their architectural simplicity and training efficiency, single column network architecture has received more and more attention in the recent years.\n\n\nB. Learning paradigm\n\nFrom the view of different paradigms, crowd counting networks can be bifurcated as single-task and multi-task based methods. 1) Single-task based methods: The classical methodology is to learn one task at one time, i.e., single-task learning [111]. Most CNN-based crowd counting methods belong to this paradigm, which generally generates density maps and then sum all the pixels to obtain the total count number, or the count number directly.\n\n2) Multi-task based methods: More recently, inspired by the success of multi-task learning in various computer vision tasks, it has shown better performance by combing density estimation and other tasks such as classification, detection, segmentation, etc. Multi-task based methods are generally designed with multiple subnets; besides, in contrast to pure single column architecture, there may be other branches corresponding to different tasks. In summary, multi-task architectures can be regarded as the cross-fertilize between multi-column and single-column but different from either one.\n\n\u2022 CMTL [55] combines crowd count classification and density map estimation into an end-to-end cascaded framework. It divides crowd count into groups and takes this as a high-level prior to integrate into the density map estimation network.\n\n\u2022 Decidenet [7] predicts the crowd count by generating the detection-based and regression-based density maps, respectively. To adaptively decide which model is appropriate, an attention module is adopted to guide the network to allocate relative weights and further select suitable mode. It can automatically switch between detection and regression mode. However, it may suffer from a huge number of parameters by utilizing the multi-column structure.\n\n\u2022 IG-CNN [72] is a hierarchical clustering model, which can generate image groups in the dataset and a set of particular networks specialized in their respective group. It can adapt and grow regarding the complexity of the dataset.\n\n\u2022 ic-CNN [73] puts forward a two-branch network, one of which is generating low-resolution density maps, and the other is refining the low-resolution maps and feature maps extracted from previous layers to produce higher resolution density maps.\n\n\u2022 ACSCP [74] ACSCP introduces an adversarial loss to make the blurring density maps sharp. Moreover, a scale-consistency regularizer is designed to guarantee the calibration of crossscale model and collaboration between different scale paths.\n\n\u2022 CL [76] simultaneously addresses three tasks, including crowd counting, density map estimation, and localization in dense crowds, according to the fact that they are related to each other making the loss function in the optimization of deep CNN decomposable.\n\n\u2022 CFF [87] assumes that point annotations not just for constructing density maps, repurposing the point annotations for free in two ways. One is supervised focus from segmentation, and the other is from global density. The focus for free can be regarded as the complement of other excellent approaches, which benefits counting if ignoring the base network.\n\n\u2022 PCC Net [88] takes perspective change into account, which is composed of three components, Density Map Estimation for leaning local features, Random High-level Density Classification for predicting density labels of image patches, and Fore-/Background Segmentation (FBS) for segmenting the foreground and background.\n\n\u2022 RAZ-Net [94] observes that the density map is not consistent with the correct person density, which implies that crowd localization cannot depend on the density map. A recurrent attentive zooming network is proposed to increase the resolution for localization and an adaptive fusion strategy to enhance the mutual ability between counting and localization.\n\n\u2022 ATCNN [95] fuses three heterogenous attributes, i.e., geometric, semantic and numeric attributes, taking them as auxiliary tasks to assist the crowd counting task.\n\n\u2022 CDT [112] not only makes an overall comparison of density maps on counting, but also extends to detection and tracking.\n\n\u2022 NetVLAD [75], [113] is a multi-scale and multi-task framework which assembles multi-scale features captured from the input image into a compact feature vector in the means of \"Vector of Locally Aggregated Descriptors\" (VLAD). Additionally, \"deeply supervised\" operations are exploited on the bottom layers to provide additional information to boost the performance.\n\n\nC. Inference manner\n\nBased on the different training manners, the CNN-based crowd counting approaches can be classified as patch-based inference and the whole image-based inference.\n\n1) Patch-based methods: This inference manner is required to train using patches randomly cropped from the image. In the test phase, using a sliding window spreads over the whole test image, and getting the estimations of each window and then assembling them to obtain the final total count of the image.\n\n\u2022 Cross-scene [51] randomly selects overlapping patches from the training images to serve as training samples, and the density maps of corresponding image patches are treated as the ground truth. The total count of the selected training patch is computed by integrating over the density map. The value of count is a decimal, rather than an integer.\n\n\u2022 CCNN [2] is primarily leaning a regression function to project the appearance of the image patches onto their corresponding object density maps. The model adopts the same sizes of all patches and the same covariance value of the Gaussian function in the groundtruth density map generation process, which limits the accuracy when encounters the large scale variation scenarios.\n\n\u2022 DML [114] integrates metric learning into a deep regression network, which can simultaneously extract density-level features and learn better distance measurement.\n\n\u2022 PaDNet [79] present a novel Density-Aware Network (DAN) module to discriminate variable density of the crowds, and Feature Enhancement Layer (FEL) module is to boost the global and local recognition performance.\n\n\u2022 L2SM [98], [115] attempts to address the density pattern shift issue, which is resulting from nonuniform density between sparse and dense regions, by providing two modules, i.e., Scale Sreserving Network (SPN) to obtain patch-level density maps and a learn to scale module (L2SM) to compute scale ratios for dense regions.\n\n\u2022 GSP [116] devises a global sum pooling operation to replace global average pooling (GAP) or fully connected layers (FC), considering the counting task as a simple linear mapping problem and avoiding patchwise cancellation and overfitting in the training phase with small datasets of large images.\n\n2) Whole image-based methods: Patch-based methods always neglect global information and burden much computation cost due to the sliding window operation. Thus the whole image-based methods usually take the whole image as input, and output corresponding density map or a total number of the crowds, which is more convergence but may lose local information sometimes.\n\n\u2022 JLLG [69] feeds the whole image into a pre-trained CNN to obtain high-level features, then maps these features to local counting numbers. It takes advantage of contextual information both in the global and local count.\n\n\u2022 Weighted VLAD [117] integrates semantic information into learning locality-aware feature (LAF) sets for crowd counting. First, mapping the original pixel space onto a dense attribute feature map, then utilizing the LAF to capture more spatial context and local information.\n\n\nD. Supervision form\n\nAccording to whether human-labeled annotations are used for training, crowd counting methods can be classified into two categories: fully-supervised methods and un-/self-/semi-supervised methods.\n\n1) Fully-supervised methods: The vast majority of CNN-based crowd counting methods rely on large-scale accurately hand-annotated and diversified data. However, the acquisition of these data is a time-consuming and more onerous labeling burden than usual. Beyond that, due to the rarely labeled data, the methods may suffer from the problem of over-fitting, which leads to a significant degradation in performance when transferring them in the wild or other domains. Therefore, training data with less or even without labeled annotations is a promising research topic in the future.\n\n2) Un/semi/weakly/self-supervised methods: Un/semisupervised learning denotes that learning without or with a few ground-truth labels, while self-supervised learning represents that adding an auxiliary task which is different from but related to supervised tasks. Some methods exploit unlabeled data for training have achieved comparative performance in contrast with supervised methods.\n\n\u2022 GWTA-CCNN [96] presents a stacked convolution autoencoder based on Grid Winner-Take-All [118] paradigm for unsupervised feature learning, of which 99% parameters can be trained without any labeled data.\n\n\u2022 SR-GAN [82] generalizes semi-supervised GANs from classification problems to regression problems by introducing a loss function of feature contrasting.\n\n\u2022 GAN-MTR [78] applies semi-supervised learning GANs objectives to multiple object regression problem, which trains a basic network the same as [51] with the use of unlabeled data.\n\n\u2022 DG-GAN [119] presents a semi-supervised dual-goal GAN framework to seek both the number of individuals in the crowd scene and discriminate whether the real or fake images.\n\n\u2022 CCLL [120] puts forward a semi-supervised method by utilizing a sub-modular to choose the most representative frames from the sequences to circumvent redundancy and retain densities, graph Laplacian regularization and spatiotemporal constraints are also incorporated into the model. \u2022 L2R [77], [91] exploits unlabeled crowd data for pre-training CNNs in a multi-task framework, which is inspired by selfsupervised learning and based on the observation that the crowd count number of the patches must be fewer or equal to the larger patch which contains them. The method is fully supervised in essence but an additional task of count ranking in a self-supervised manner.\n\n\u2022 HA-CNN [97] offers the first attempt to fine-turn the network to new scenes in a weakly supervised manner, by leveraging the image-level labels of crowd images into density levels.\n\n\u2022 CCWld [84] provides a data collector and labeler for crowd counting, where the data is from an electronic game. With the collector and labeler, it can collect and annotate data automatically, and the first large-scale synthetic crowd counting dataset is constructed.\n\n\u2022 CODA [121] presents a novel scale-aware adversarial density adaption approach for object counting, which can be used to generalize the trained model to unseen scenes in an unsupervised manner.\n\n\u2022 OSSS [122] designs a one-shot scene-specific crowd counting model by taking advantage of fine-turning.\n\n\nE. Domain adaptation\n\nAlmost all the existing counting methods are designed in a specific domain; therefore, designing a counting model which can count any object domain is a challenging yet meaningful task. The domain adaptation technique may be a powerful tool to tackle this problem.\n\n\u2022 CAC [123] formulates the counting as a matching problem, which presents a Generic Matching Network (GMN) in a class-agnostic manner. GMN can be trained by the amount of video data labeled for tracking due to counting as a matching problem. In a few-shot learning way, it can use an adapter module to apply to different domains.\n\n\u2022 PPPD [124] provides a patch-based, multi-domain object counting network by leveraging a set of domain-specific scaling and normalization layers which only uses a few of parameters. It can also be extended to perform a visual domain classification even in an unseen observed domain.\n\n\u2022 SE CycleGAN [84] takes advantage of domain adaptation technique, incorporating Structural Similarity Index (SSIM) [125] into traditional CycleGAN framework to make up the domain gap between synthetic data and real-world data.\n\n\u2022 MFA+SDA [126] is drawing the idea from SE Cycle GAN, which is also a GAN-based adaptation model. The authors propose a Multi-level Feature-aware Adaptation to reduce the domain gap and present a Structured Density map Alignment for handling the unseen crowd scenes.\n\n\u2022 DACC [127] is composed of two modules: Inter-domain Features Segregation (IFS) and Gaussian-prior Reconstruction (GPR). IFS is designed to translate the synthetic data to realistic images, and GPR is used to generate higher-fidelity density maps with pseudo labels.\n\n\u2022 FSC [128] extracts semantic domain-invariant features via crowd masks generated by a pre-trained crowd segmentation model. The error estimations in the background regions are reduced significantly.\n\n\nF. Instance-/image-based supervision\n\nThe aim of object counting is to estimate the number of objects. If the ground truth is labeled with point or bounding box, the method pertains to instance-level supervision. In contrast, image-level supervision just needs to count the number of different object instance instead.\n\n1) Instance-level supervision: Most crowd density estimation methods are based on instance-level (point-level or bounding box) supervision, which needs hand-labeled annotations for each instance location.\n\n2) Image-level supervision: Image-level supervisionbased methods need to count the number of instances within or beyond the subitizing range, which do not require location information. It can be regarded as estimating the count at one shot or glance [129].\n\n\u2022 ILC [101] generates a density map of object categories, which obtains the total object count estimation and spatial distribution of object instances simultaneously.\n\n\nIII. DATASETS\n\nWith the blooming development of crowd counting, numerous datasets have been introduced, which can motivate many more algorithms to cater to various challenges such as scale variations, background clutter in the surveillance video and changeable environment, illumination variation in the wild. In this section, we review almost all the crowd counting datasets from beginning up to now. Table III summarizes some representing datasets, including crowd counting datasets with real-world data and one with synthetic data, for the sake of completeness, we also survey several datasets applied in other domains, to evaluate the generalization ability of the designed algorithms. The datasets are sorted by chronology and the specific statistics of them are listed in Table III. Some samples from the representing datasets are depicted in Fig. 4. \n\n\nA. Most frequently-used datasets\n\nIn this subsection, we introduce some most frequently used crowd counting datasets, i.e., UCSD [27], Mall [40], UCF CC 50 [38], WorldExpo'10 [51], Shanghai Tech [1], which are listed by chronologically.\n\n\u2022 UCSD [27] 1 is the first dataset for crowd counting, which is collected from cameras on the sidewalk. It is composed of 2000 frames with a size of 238\u00d7158 and the ground truth annotations of each pedestrian in every five frames. For the rest of frames, the labels are created by using linear interpolation. Since it is collected from a single location, thus there is no change of the perspective view in different frames.\n\n\u2022 Mall [40] 2 is a dataset collected from the surveillance video of a shopping mall. The video sequence in the dataset is composed of 2000 frames with a size of 320\u00d7240, which contains 62,325 pedestrians in total. Compared with UCSD [27], Mall covers more diversity densities as well as different activity patterns (static and moving persons) under more significant illumination conditions. Additionally, there exists more perspective distortion, resulting in larger size change and appearance of objects, and has severe occlusions due to scene objects.\n\n\u2022 UCF CC 50 [38] 3 is the first really challenging dataset created from publicly available Web images. It includes a variety of densities and different perspective distortions for different scenes such as concerts, protests, stadiums and marathons. Considering that only 50 images in this dataset, a 5-fold cross-validation protocol is conducted on it. Due to the smallscale data volume, even the most advanced recent CNN-based methods are far from optimal for the results on it.\n\n\u2022 WorldExpo'10 [51] 4 is a large data-driven cross-scene crowd counting dataset collected from Shanghai 2010 World-Expo, which includes 1,132 annotated video sequences captured by 108 surveillance cameras. It contains a total of 3920 frames with a size of 576\u00d7720, of which 199,923 persons are annotated.\n\n\u2022 Shanghai Tech [1] 5 is one of the largest large-scale crowd counting datasets in previous few years which is composed of 1198 images with 330,165 annotations. According to different density distributions, the dataset is divided into two parts: Part A (SHT A) and Part B (SHT B). SHT A contains images randomly selected from the Internet, whilst Part B includes the images are taken from a busy street of a metropolitan area in Shanghai. The density in Part A is much larger than that in Part B. This dataset successfully creates a challenging dataset across different scenes types and densities. However, the number of images in different density sets is uneven, which makes the training set and test set tend to be low-density sets. Nevertheless, the scale changes and perspective distortion presented by this dataset provide new challenges and opportunities for the design of many CNNbased networks.\n\n\nB. More recently datasets\n\n\u2022 Smartcity [9] 6 is created by Tencent YouTu, which contains 50 images in 10 scenes such as sidewalk, office entrance, shopping mall. All of them are high shot for video surveillance. The dataset includes indoor and outdoor scenes, and mainly to verify the generalization ability of the model on very sparse scenes.\n\n\u2022 UCF-QNRF [76] 7 is collected from Flickr, Web Search and Hajj footage, which consists of 1,535 challenging images with about 1.25 million annotations. The images in this dataset come with a wider variety of scenes and contain the most diverse set of viewpoints, densities, and lighting variations. However, some of them are so high-resolution that they may lead to memory issues in GPU while training the entire scene.\n\n\u2022 City Street [130] is a multi-view video dataset of which the data is collected from a busy city street by using five synchronized cameras, which is composed of 500 multi-view images in total \u2022 ShanghaiTechRGBD [131] is a large-scale RGB-D dataset which consists of 2,193 images with 144,512 labeled head counts. With the crowd scenarios and various lighting condition, making the dataset is the most challenging RGB-D crowd counting dataset regarding the number of head counts.\n\n\u2022 FDST [132] 8 is a new large-scale video crowd counting dataset, which consists of 100 videos captured from 13 different scenes including shopping malls, squares, hospitals, etc, which contains 15,000 frames with 394,081 annotated heads, and all with frame-wise annotation.\n\n\u2022 Crowd Surveillance [133] 9 contains 13,945 high-resolution images with 386,513 people in total, making the largest and highest average resolution for crowd counting so far. In addition, regions of interest (ROI) annotation is also provided to filter out the regions which are too blurry or ambiguous for training and test.\n\n\u2022 JHU-CROWD [134] is a larger dataset w.r.t the number of images and several particular properties such as adverse conditions (weather-based degradations), learning bias mitigated (including distractor images), richer annotations (image-level and head-level in addition to point-level annotations).\n\n\u2022 DLR-ACD [135] 10 contains 33 large aerial images with average resolution is 3619\u00d75226, which are captured by standard DSLR cameras installed on an airborne platform on a helicopter. The images come from 16 flight campaigns, and the dataset contains 226,291 person annotations in total.\n\n\u2022 DroneCrowd [136] is a drone-based dataset for density map estimation, crowd localization and tracking, simultaneously. The dataset is composed of 112 video sequences with 33,600 frames in total. The average resolution of the frames is 1920\u00d71080, collected from multiple drone devices, 70 different scenarios across four different cities in China. There are more than 4.8 million head annotations on 20,800 people trajectories.\n\n\u2022 GCC [84] 11 is collected from an electronic game Grand Theft Auto V (GTA5), named as \"GTA5 Crowd Counting\" (GCC for short), which consists of 15,212 images, with resolution of 1080\u00d71920, containing 7,625,843 persons. Compared with the existing datasets, GCC has four advantages: 1) free collection and annotation; 2) larger data volume and higher resolution; 3) more diversified scenes, and 4) more accurate annotations.\n\n\u2022 NWPU-Crowd [28] 12 contains 5,109 images with 2,133,238 annotated instances in total. Compared with previous dataset in real world, in addition to data volume, there also some other advantages including negative samples, fair evaluation, higher resolution and large appearance variation.\n\n\nC. Some special crowd counting datasets\n\nIn this subsection, we briefly introduce some special crowd counting datasets, which are only used in some certain scenarios. These datasets contain line crowd counting (LHI [137], [142], crowd sequences (PETS [138], Venice [86]), multi-sources (AHU-Crowd [139], [143], CI-ISR [149], Venice [86]), indoor (MICC [140], Indoor 1 [141], Indoor 2 [150]), train station (TS [144], STF [144]), subway station (Shanghai Subway Station [145]), BRT (Beijing BRT [146] 13 ), bridge (EBP [147]), airport (ZhengzhouAirport [151]), categorized [152]. The specific statistics of these datasets are listed in Tabel III.\n\n\nD. Representing object Counting datasets in other fields\n\nFor completeness, we introduce some representing object counting dataset in other fields, to verify the generalization ability of the designed model further.\n\n\u2022 Caltech [153] is a dataset for pedestrian detection, which has 2000 images and 15043 pedestrians in total. The dataset can be used to verify the performance of the algorithms in sparse scenes.\n\n\u2022 TRANCOS [20] 14 is the first one for vehicle counting in traffic jam images. The dataset is often used to evaluate the generalization ability of the crowd counting methods.\n\n\u2022 The penguin dataset [17] 15 is a product of an ongoing project for monitoring the penguin population in Antarctica.The dataset can be used to study climate change, etc.\n\n\u2022 WIDER FACE [154] 16 is a large-scale face detection benchmark, which is composed of 32,203 images in total and 393,703 faces with bounding boxes annotations.\n\n\u2022 DukerMTMC [155] is a multi-view video dataset for multiview tracking, human detection and re-identification (ReID), which contains over 2 million frames and more than 2,700 identities.\n\n\u2022 WebCamT [156] is the first largest annotated webcam traffic dataset to date, which consists of 60 million frames \n\n\nIV. EVALUATION METRICS\n\nThere are several ways to evaluate the performance between predicted estimations and ground truths. In this section, we review some universally-agreed and popularly adopted measures for crowd counting model evaluation. According to different evaluation criteria, we divide the evaluation metrics into three categories: image-level for evaluating the counting performance, pixel-level for measuring the density map quality and point-level for assessing the precision of localization.\n\n\nA. Image-level metrics\n\nTwo most common used metrics are Mean Absolute Error (MAE) and Mean Square Error (RMSE), which are defined as follows:\nM AE = 1 N N i=1 C pred I i \u2212 C gt I i ,(1)RM SE = 1 N N i=1 C pred I i \u2212 C gt I i 2 ,(2)\nwhere N is the number of the test image, C pred Ii and C gt i represent the prediction results and ground truth, respectively. Roughly speaking, M AE determines the accuracy of the estimates, while RM SE indicates the robustness of the estimates. Considering aforementioned M AE may loss the location information, to provide a more accurate evaluation, Guerrero et al. [20] propose a new metric is Grid Average Mean Absolute Error (GAME), which is defined as follows:\nGAM E(L) = 1 N N n=1 \uf8eb \uf8ed 4 L l=1 C pred I i \u2212 C gt I i \uf8f6 \uf8f8 ,(3)\nwhere 4 L denotes that dividing the image into some nonoverlapping regions. The higher L, the more restrictive of the GAME metric will be. Note that when L = 0, GAM E will degenerate into M AE. Similarly, accounting for the localization errors, a mean pixel-level absolute error (MPAE) [162] is proposed as follows:\nMPAE = N i=1 H j=1 W k=1 D i,j,k \u2212D i,j,k \u00d7 1 {Di,j,k\u2208Ri} N ,(4)\nwhere D i,j,k denotes the ground-truth density map of i-th image at the pixel (j,k),D i,j,k means the corresponding estimated density map, R i represents the ROI of the i-th image, 1 {\u00b7} indicates the indicator function, and W , H and N are the width, height and the number of test samples. MPAE measures the degree of wrongly localized the densities are. In view of both M AE and RM SE are the metrics for global accuracy and robustness, which cannot evaluate the local regions, thus Tian et al. [79] expand M AE and RM SE to patch mean absolute error (P M AE) and patch mean squared error (P M SE), which are defined as\nP M AE = 1 m \u00d7 N m\u00d7N i=1 C pred I i \u2212 C gt I i ,(5)P M SE = 1 m \u00d7 N m\u00d7N i=1 C pred I i \u2212 C gt I i 2 ,(6)\nwhere m is the splitted non-overlapping patches. Note that when m equals to 1, P M AE and P M SE degenerate into M AE and RM SE, respectively.\n\n\nB. Pixel-level metrics\n\nTwo metrics named Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM) [125] are usually used to measure the quality of the generated density map. Specifically, PSNR, the most common and widely used evaluation index of the image, which is essentially based on the error between corresponding pixels, in other words, error sensitivity. Generally speaking, high values represent smaller errors. However, it does not take the human visual characteristics into account, for example, the human is more sensitive to the contrast difference of lower spatial frequency and more sensitive to the brightness than hue, the perceptual results of a region is influenced by surrounding adjacent regions, etc. Therefore, the evaluation results are often inconsistent with people's subjective feelings.\n\nIn addition, SSIM [125] measures the image similarity from three aspects: brightness, contrast and structure, which can be regarded as the multiplication of the three parts. The value of SSIM is in the range of [0,1], the larger of the value, the less distortion of the image.\n\n\nC. point-level metrics\n\nTo evaluate the localization performance of the model, Average Precision (AP) and Average Recall (AR) are two most common used metrics. Generally speaking, when the value of AP increases, AR decreases. Thus how to trade off between them is a worthy considering question. Table IV presents results of 53 state-of-the-art CNNbased methods and 7 representative traditional approaches over six mainstream benchmark datasets in crowd counting task. Two widely used evaluation metrics, i.e., MAE and RMSE are for measuring the accuracy and robustness of the models. All the models are representative and the results listed in the Table IV are published in their papers or reported by other works 18 .\n\n\nV. BENCHMARKING AND ANALYSIS\n\n\nA. Overall benchmarking results evaluation\n\n\u2022 CNN-based v.s. traditional models. Comparing the traditional models with CNN-based ones in Table IV, as excepted, we observe that CNN-based methods make great improvement of performances by a large margin. It also demonstrates that strong feature learning ability of deep convolution neural network based on large-scale annotated data.\n\n\u2022 Performance comparison of CNN-based models. Since the year of 2015, the first CNN-based density map estimation model was proposed for crowd counting, the performance has also been improved gradually over time, which has witnessed the significant progress of the crowd counting model. Among the deep models, Cross scene [51] performs the worst performance, as the first ones to apply CNNs for crowd counting, which adopts the basic network structure and handle the crossscene problem that transfers the pre-trained CNN to unseen 18 More detailed results leaderboard can be found at https://github.com/gjy3035/Awesome-Crowd-Counting scenes. Thus the model is lower compared with the singlescene and domain-specific model. However, this work provides a good solution to the generalizing trained model to unseen scenes.\n\n\nB. Properties-based evaluation\n\nWe choose three top-performing models in terms of MAE and RMSE over six commonly used datasets, ending up with collecting 19 models, including two heuristic models, i.e., MCNN [1] and CSRNet [12], the main properties of these state-of-the-arts are listed in Table V. These properties cover the main techniques that could be used to explain the reason they perform well.\n\nFrom Table V, we can find that, among these state-ofthe-art methods, two-thirds of which adopt single column network architecture. For this phenomenon, perhaps we can reach the following conclusion: instead of making the network wider, deeper networks may be better. In addition, more than one third of them incorporate visual attention mechanism [7], [8], [13], [71], [83], [87], [89], [97] and dilation convolution layer [12], [80], [81], [83], [84], [86], [163] into their frameworks. Instead of using all available information of the input image in many CNNs-based methods, the visual attention mechanism is to use pertinent information to compute the neural responses, which can learn to weight the importance of each pixel of feature maps. Due to the prominent ability, visual attention mechanism has been applied to many computer vision tasks, such as image classification [164], semantic segmentation [165], image deblurring [166], and visual pose estimation [167], it is also suitable for the problem of crowd counting, highlighting the regions of interest containing the crowd and filtering out the noise in the background clutter situations. Dilated convolution layers, a good alternate of the pooling layer, have demonstrated that significant improvement of accuracy in segmentation tasks [168]- [170]. The advantage of the dilation convolution layer is that enlarging receptive field without information loss caused by pooling operations (max and average pooling, etc.) and without increasing the number of parameters and the number of computations (such as up-sampling operations of the de-convolution layer in FCN [171]). Therefore, the dilation convolution layer can be integrated into the crowd counting framework to capture more multi-scale features and maintain more detailed information.\n\nSpatial Transformer Network (STN) [105] and deformable convolution [172] have a similar effect to address the problem of rotation, scaling or warping, which limit the capacity of feature invariance of standard CNNs. Specifically, STN is a sub-differential sampling module, which requires no extra annotations and has the capacity of adaptively learning spatial transformation between different data. It can not only carry out the spatial transformation on the input image but also any layer of the convolutional layer to realize the spatial transformation of different feature maps. Due to the remarkable performance, STN has been applied to many communities, such as multilabel image recognition [173] and saliency detection [174]. Therefore, it also adopted by Liu et al. [71] to address the scale and rotation variation in crowd counting.\n\nConditional random fields (CRFs) [175] or Markov Random Fields (MRFs) [176] have been usually leveraged as a post-possessing operation to refine the features and outputs of the CNNs with a message passing mechanism [177]. In the work of [178], the first to utilize CRFs to refine features with different scales for the crowd counting task and demonstrates the effectiveness on the benchmark datasets. Zhang et al. [179] propose an attentional neural fields (ANF) framework which integrates CRFs and non-local operation [180] (similar as selfattention) for crowd counting.\n\nPerspective distortion is a major challenge in the crowd counting, while perspective information may be provided in two ways: one is related to camera's 6 degree-of-freedom (DOF) [181], the other is to identify the scale variation in the distance from the camera in the counting task. It can provide additional information with respect to scale variation and perspective geometry, many traditional crowd counting methods [27], [182] utilize it to normalize the regression features or detection features of changeable scales. Some modern CNNs-based methods also use perspective information to infer the ground truth density [1], [51] or body part maps [183]. These methods utilize perspective information yet without using the perspective map. Instead, some works [85], [86] leverage it to encode global or local scales in the network.\n\nSpatial pyramid pooling (SPP) [184] was originally raised for visual recognition, which has several advantages than traditional networks, firstly it adapts the input images with arbitrary sizes; additionally, as the pooling layers with different sizes are extracted from the feature map and then aggregating them into a vector with fixed length, so that improve the robustness and accuracy. Moreover, it can accelerate convergence speed. Therefore, it is used to capture and fuse multi-scale features in SCNet [185], PaDNet [79] and CAN [86] for crowd counting.\n\nPan-density crowd counting aims to deal with two phenomena in crowd scenarios: varying densities and distributions in different scenarios and inconsistent densities of local regions in the same scene. Most current methods are designed for a specific density or scenario so that it is difficult to take full advantage of pan-density information. Although many multicolumn architectures are designed to cope with this problem, such as MCNN [1], Switch-CNN [5] and CP-CNN [6], they always suffer from low efficiency, high computation complexity, and biased local estimation. However, PaDNet [79] is put forward to provide a reasonable solution to effectively identify specific crowd by the sub-networks in Density-Aware Network (DAN), and learn an enhancement rate for each feature map by a Feature Enhancement Layer (FEL). In the final, these feature maps are fused to obtain better counting.\n\nComprehensively considering the statistical results from Table V and the analysis of various methods, we make the following observations:\n\n\u2022 Among the CNN methods, most networks are based on single column network architecture, which is more simpler yet effective than multi-column architectures that are high complexity and bloated structure as demonstrated in [12].\n\n\u2022 The techniques of visual attention mechanism, dilation convolution, and spatial pyramid pooling (SPP) can significantly improve the performance of the final estimation and the quality IV: Comparison of the performance of different methods on the representing crowd counting datasets. Red, green and blue indicate the first, the second and the third best performance, respectively. Note that the MAE in WorldExpo' 10 [51] is the average value of the five cross-scenes, SFCN \u2020 2 [84] represents that model takes ResNet101 as backbone, pre-trained on GCC [84], \"-\" denotes that results are not available and \"\u2193\" indicates the lower the better of the results. \u2022 Incorporating perspective information [1], [51], [85], [86] into the network can provide additional support and guidance for the extraction of multi-scale features.\n\n\u2022 Spatial transformer network [71], [105] and deformable convolution [83], [172] can help to address the rotation and uniform distributions of crowds which is more suitable for the crowd understanding problem in the congested noisy scenarios.\n\n\u2022 Pan-density learning [79] can not only take full advantages of global features but also make up biased local estimation.\n\n\u2022 Multi-pathway or multi-task framework [7], [80], [89] followed by jointly loss function can improve the estimation performance and speed up the training.\n\n\nC. Attributes-based analysis\n\nAlbeit significant performance improvement has been achieved to a great extent by applying CNNs into density  [163] L2SM [98] DSSINet [178] SPANet [193] MBTTBF-SCFB [194] BL [192] S-DCNet [195] PGCNet [133] map estimation crowd counting models, there are remain some challenges to be conquered. A robust network should have the capability of coping with various complex scenarios. The existence of challenges always brings many difficulties to the models, such as occlusion, scale variation, perspective distortion, rotation, illumination variation, and weather changes. Some samples are shown in Fig. 5. Moreover, the scenes of the images are from indoor, outdoor, and in the wild. It is worth noting that these attributes are not mutually exclusive. In other words, there may exist several attributes in one image. We also take the aforementioned methods in Table V as an example to analyze these attributes in detail.\n\n\u2022 Occlusion. As the crowd density increasing, the crowd will appear to occlude each other partly, which limits the capacity ability of traditional detection algorithms and prompts the emergence of density estimation models.\n\n\u2022 Complex background. Background regions (have no person instances) includes confusing objects or have similar appearance or colors with the foreground, this can be suppressed through semantic segmentation or visual attention operations, such as [7], [8], [13], [71], [83], [89], [197].\n\n\u2022 Scale variation. The most primary problem should be addressed in the density estimation models, as the scales of objects (such as the sizes of people heads) vary as the distance from the camera. Therefore, almost all the density estimation models are designed for addressing the scale variation problem in the first step.\n\n\u2022 Non-uniform distribution. For intuitive understanding in the examples of Fig. 5, we can observe that diverse densities and distributions in different scenes and inconsistent distributions of local regions even in the same scene. The problem can be effectively addressed in the work of [198], which presents the first model to estimate various crowd densities with different regressors. Jiang et al. [151] tackle this issue by proposing a multi-level convolution neural network (MLCNN) which fuses multiple density maps generated by multi-level features. This problem can also be regarded as pandensity crowd counting, and related solutions can be referred to PaDNet [79].\n\n\u2022 Perspective distortion. Perspective distortion would drastically lead to person scale variation in the crowding counting scenes, which is related to camera calibration to estimate the 6 degree-of-freedom (DOF) of a camera.\n\n\u2022 Rotation. The issue of rotation variation drastically due to the camera viewpoints such as different pose and photographic angles, it is addressed by the work [71] via incorporating spatial transformer network (STN) into LSTM framework.\n\n\u2022 Illumination variation. The illumination varies at different times in a day, usually from dark to light and then to dark, from dawn to dusk.\n\n\u2022 Weather changes. The scenes in the wild usually under various types of weather conditions, such as clear, clouds, rain, foggy, thunder, overcast, and extra sunny.\n\nThe above challenges promote us to design more effective and robust frameworks to address these issues, and this also indicates that there is still much research room in the direction of crowd counting.\n\n\nVI. DISCUSSION\n\nIn this section, we discuss some important factors that will directly affect the performance of the crowd counting model design and some promising research directions.\n\nA. Model design \u2022 Ground truth density maps generation. As a cornerstone towards CNN-based density estimation and crowd counting models, the generation of high-fidelity ground truth density maps is essential to data preparation for training. To convert an image with the original labels (generally refer to head location) to a density map, Lempitsky et al. [16] first raised and defined as a sum of Gaussian kernels centered on the locations of objects. This strategy works well for characterizing the density distribution of circle-like objects such as cells and bacteria.\n\nTo tackle scale variations, Zhang et al. [51] put forward a solution by exploiting perspective information: the density map can be obtained by a sum of Gaussian kernels as a head part and a bivariate normal distribution. However, this strategy introduces a new issue of acquiring the perspective map. Fortunately, Zhang et al. [1] find that head size is related to the distance between two neighboring persons. Based on it, a geometry-adaptive kernel-based density map generation method is created, which inspires lots of works adopting this tool to prepare their data training. Although such strategy works well in the dense crowd scenes, it would fail in the sparse scenes. A depth-adaptive kernel-based density map generation method [131] is proposed by positing that the sizes of all heads are the same in the real world. However, all the methods above are not content-aware. Therefore, Oghaz et al. [199] propose a brute-force nearest neighbor search technique to provide the absolute nearest neighbors despite the distribution of points, through using an integration of Chan-Vese segmentation algorithm, twodimension Gaussian filter and brute-force nearest neighbor search technique. Recently, Xu et al. [115] claim that the target density map generation manner may fail in the dense regions, since the density map is given by the sum of severely overlapped Gaussian blobs, which leads to diverse density patterns different from the sparse regions. Therefore, a learning-to-scale module (L2SM) is applied to re-scale the dense regions into similar scale levels, so as to ameliorate the pattern shifts and increase the counting accuracy. In another way, Ma et al. [192] propose a Bayesian loss to enforce a more reliable density contribution probability model from the point-annotations. Olmschenk et al. [200] propose an inverse k-nearest neighbor (ikNN) map to supersede the density maps, which can offer a smooth training gradient and accurate localization simultaneously, this also breaks the routine and opens new paths for us. Wan et al. [201] propose an adaptive density map generator that generates a learnable density map representation from the ground truth dot labels.\n\nAnyway, the proper selecting for density map generation will lay a solid foundation for the crowd counting.\n\n\u2022 Loss function. The customized design of loss function is also an essential procedure in training effective models. Density map estimation CNN-based crowd counting methods are mostly a regression task, which usually adopt Euclidean distance as loss function to measure the difference between the estimated density map and ground truth. Although widely used, only the Euclidean loss employed may have some disadvantages such as sensitivity to outliers and image blur, pixel independent assumption neglecting the local coherence, and spatial correlation in density maps. Therefore, SmoothL1 loss [33] and Tukey Loss [202] are more robust for outliers can be leveraged, as described in [113]. Besides, an adversarial loss [103] is integrated to address the issue and improve the quality of density maps [6], [74], [203]. Nevertheless, density maps may contain little high-level semantic information, thus a light-weight SSIM local pattern consistency loss combined with Euclidean loss to enforce the local structural similarity between estimated density maps and ground truths [11], but it only can tackle the local consistent of regions with a fixed size. Therefore, further, a Dilated Multi-scale Structure Similarity (DMS-SSIM) loss [178], [204] is utilized to make the network learn the local similarity within the regions of varied sizes and generate the density maps with local consistency. Besides, a novel scale-aware loss function to specialize person head on a particular scale [13]. Additionally, a combination of spatial abstraction loss (SAL) and the spatial correlation loss (SCL) are provided in [92] to improve density map quality. In another way, accounting for the spatial variation of density, a Maximum Excess over Pixels (MEP) loss [193] is proposed to optimize the pixel-level subregion which has a high discrep-ancy with the ground-truth density map.\n\nOverall, designing appropriate loss functions helps boost the performance of the models.\n\n\u2022 Information fusion of multi cues. Generally, the information fusion of multiple cues can significantly improve the performance of the algorithm, for instance, the integration of scale-aware and context-aware would boost the performance [6], [86], the combination of different pathways for sparse and dense scenarios [7], [80], [89]. Meanwhile, heterogeneous attributes, such as geometric/semantic/numeric attributes are leveraged to assist the density estimation for crowd counting [95].\n\nAs a whole, there is an abundant of data availability across many different data sources or modalities in various formats. Fusing these heterogeneous cues with \"broad learning\" may be a reliable research direction.\n\n\u2022 Network Topology. The network topology represents the information flow within the network, which influences the training complexity and parameters that are required. Proved by many experiments, the encoder-decoder pipeline appears promising performance for the crowd counting task, for instance, CSRNet [12] adopts a standard encoder-decoder structure, which takes a pre-trained VGG16 [205] as a backbone, and builds dilated convolution operation in the decoder. SA-Net [11] presents a similar model, which uses Inception model [109] in the encoder and Transposed convolution layers in the decoder. W-Net [90] directly leverages U-Net [110] structure with VGG16 [205] replacing the encoder block, and adds a extra branch for faster convergence. TEDnet [92] deploys an encoder-decoder hierarchy in a trellis manner. SGANet [206] investigates the effectiveness of Inception-v3 [207] for crowd counting. Beyond encoder-decoder pipeline, VGG16 [205] is the best backbone for feature extraction among VGG16bn, Resnet50 [208] and Inception [109], which has been empirically demonstrated in [90] and strong transfer ability for crowd analysis.\n\n\nB. Dataset construction\n\nIn light of previous observation, we would put forward some suggestions for the construction of crowd counting dataset, w.r.t., scene diversity, multi-view, annotation accuracy, etc.\n\n\u2022 Scene diversity. Some earlier datasets for crowd counting is in single-scene, i.e., the images are from the same video sequence, which has no variation in the perspective across different images, such as UCSD [27] and Mall [40], as illustrated in Fig. 4.\n\nTo meet the need of cross-scene and diversified data for deep model training, some more challenging datasets are proposed, including UCF CC 50 [38], SHT A [1], UCF QNRF [76], and countless others. Nevertheless, there are some drawbacks that limit their generation ability, for instance, UCF CC 50 [38] is limited by the small number of availability of high-resolution crowd images, SHT A [1] is suffered from non-uniform density level and inaccurate labels of some samples. UCF QNRF [76] has the most number of high-count crowd images and annotations with diverse densities, which is more significant than UCF CC 50 [38], however, the intra-class variation may sometimes exceed the capability of the network. Notably, they may tackle some unseen extreme cases when transferring the data to the wild, such as weather change and illumination variation. GCC [84] may provide a reasonable solution by constructing a largescale synthetic crowd counting, which consists of more diverse scenes to mimic the challenges in the wild better. Albeit plenty of data in GCC [84], there exists a large \"domain gap\" between synthetic and real data.\n\n\u2022 Multi-view. Previous datasets are for single-view counting, which cannot satisfy the requirements of large and wide scenes, taking public parks or long queue in train station as an example, the scene is so wide that cannot be fully captured by single view, so long that too low of the resolution away from the camera, or most of the crowds are occluded by large objects. Some attempts have been made to tackle the shortcoming, for example, City street dataset [130] is collected from a busy street intersection, which contains large range crowds with more complex occlusion patterns and large scale variations.\n\n\u2022 Annotation accuracy. There exists an intrinsic shortcoming in the existing dense crowd counting dataset, the annotations are not very accurate, such as some samples are from UCF CC 50 [38] and Shanghai Tech Part A [1]. In fact, this problem is inevitable due to data annotated by different subjects or following different standards.\n\n\u2022 Annotation tools. Effective annotation tools are critical in the process of dataset construction. We strongly recommend an online efficient annotation tool based on HTML5 + Javascript + Python, which supports two types of label form, i.e., point and bounding box. During the process of annotation, images are adaptively zoomed in/out to annotate heads according to different scales, and each of them are divided into 16 \u00d7 16 blocks, which provides five scales for the annotators, specifically 2 ( i) (i = 0, 1, 2, 3, 4) times size of the original image. This annotation tool can effectively improve the annotation speed and quality. With the aid of this effective annotation tool, we construct a large-scale dataset named as NWPU-Crowd [28], and more detailed description is shown in the video demo at https://www.youtube.com/ watch?v=U4Vc6bOPxm0/. Moreover, some mainstream models are benchmarked on our NWPU-Crowd [28] dataset, the code of which are open-sourced at https://github.com/gjy3035/ NWPU-Crowd-Sample-Code. and more detailed results are at https://www.crowdbenchmark.com/nwpucrowd.html.\n\nAs a whole, constructing cross-scene, multi-view and accurately annotated datasets which able to more faithfully reflect the real world challenge is essential to boost the generalization ability of crowd counting. Additionally, effective annotation tools are vital for the construction of the datasets.\n\n\nC. The quality of density maps\n\nMost existing methods pay attention to the count rather than the quality of density maps, which is an essential factor that affects the performance. Sindagi [6] first observed this issue and proposed to incorporate global context into the training process while used adversarial loss together with Euclidean loss to obtain shaper and high-quality density maps. We choose several representing methods that concentrate the quality of density maps in terms of two measures, i.e., PSNR and SSIM [125], as illustrated in Table VI. From the table, we can see that SE Cycle GAN [84] shows the worst performance. We suspect it should be attributed to the \"domain gap\" between synthetic data and real-world data.  [1] in terms of PNSR and SSIM [125]. Red and underline indicate the best and the worst performance, respectively. \"\u2191\" indicates the higher the better of the results.)\n\n\nMethods\n\nYear&Venue \n\n\nD. Domain adaption or transfer learning\n\nSupervised learning requires accurate annotations, which is tedious by manually labeling, especially in extremely congested scenes. Also, mainstream models are almost designed for domain-specific. However, when generalizing the training model to unseen scenes, it would produce sub-optimal results due to the unpredictable domain gap.  The main reason about the performance degradations is that there are many differences (also named as domain gap/shifts) between the above datasets, including density    [125] into traditional CycleGAN framework to make up the domain gap between synthetic data and real-world data. Although SE Cycle GAN [84] addresses the problem to a certain extent, there still performs relatively low estimation count than other state-of-the-art methods. However, it paves the way for the transfer between different domains. We believe it will be of great benefit to crowd counting by refining this GAN-based method in the future.\n\n\nE. Robustness for background\n\nA robust counting model does not only accurately estimate the crowd density but also produce the zero-density response for background regions. To further evaluate models' robustness, the recently released large-scale datasets, such as JHU-CROWD [134] introduces 100 distractors and NWPU-Crowd [28] introduces 351 negative samples into their own datasets, respectively. These data do not contain person objects or crowd regions. It is worth mentioning that NWPU-Crowd [28] deliberately collects some scenes with densely arranged other objects to confuse counting models. Fig. 6 shows some typical distractors and negative samples. These labeled data can effectively facilitate the counting model to perform better in the real world. Table VIII lists the estimation errors (MAE/RMSE) on the aforementioned distractors and negative samples. From the results, we can find that current models mistakenly estimate the density of these samples. For a light model, PCC-Net performs better than many VGG-backbone methods (CSRNet [12], C3F-VGG [211] and SCAR [210]). The main reason may be reside in that PCC-Net [88] incorporates segmentation information to classify the foreground (namely head) and background. \n\n\nF. Universality or generalization\n\nNearly all the existing models for object counting are designed for a specific task, however, creating a universal model able to adapt any class of object is a meaningful challenge, it is also the most effective method to evaluate the robustness and generalization ability of the algorithm. Despite there are specialties between different tasks, there still exist many commonalities, such as crowd counting, vehicle counting, and cell counting. For instance, CAC [123] formulates the counting as a matching object through mining the selfsimilarity between images, which presents a Generic Matching Network (GMN) in a class-agnostic manner. PPPD [124] provides a patch-based, multi-domain object counting network by leveraging a set of domain-specific scaling and normalization layers, which only uses a few of parameters. It can also be extended to perform a visual domain classification even in an unseen observed domain, which outstands out its versatility and modular nature. The method has been successfully applied to people, penguins, and cell counting.\n\nDesigning a unified principle (the generation of ground truth density map by using Gaussian function in [16] takes a good example) and framework that can be applied to different tasks, it looks a bit awkward yet promising research direction in future.\n\n\nG. Lightweight network\n\nCurrent CNN-based deep models are designed with a sophisticated structure, which always comes with millions of parameters and the cost of a tremendous increase in computation (FLOPs). Although a great effort has been devoted to making the model efficient, such as CSRNet [12] and SCNet [185], they usually adopt VGG16 [205] or ResNet [208] pre-trained on Imagenet [212], which is a large dataset for classification, but the task of object counting belongs to a regression task. Thus it may affect the performance to some extent. Additionally, the pre-trained mechanism is a very time-consuming process. Generally speaking, the most straightforward way to determine whether a network is lightweight is the number of parameters, the less the number of parameters, the lighter of the model. Table IX shows a comparison of the number of parameters in some representative models. From the Table, we can find that LCNN [213] has the least number of parameters, which has nearly 2138 \u00d7 lower than the worst one, CP-CNN [6]. We suspect it should be attributed to LCNN [213] is a shallow network without pretraining. Less number of parameters, which proves more efficient of the model.\n\nLightweight networks can reduce the computation cost, but they are usually accompanied by accuracy drop. Therefore, in the premise of without sacrificing accuracy, designing lightweight and efficient networks to reduce the computation cost in the counting task is a promising challenge in the future. \n\n\nMethod\n\nParameters MCNN [1] 0.13 Hydra-CNN [2] 0.56 Switching CNN [5] 15.11 CP-CNN [6] 68.4 CSRNet [12] 16.26 SA-Net [11] 0.91 ACSCP [74] 5.1 L2R [77] 16.75 DRSAN [71] 24.10 ic-CNN [72] 16.82 IG-CNN [73] 4.70 D-Conv1et [70] 16.62 BSAD [183] 1.30 MMCNN [214] 6.8 TDF-CNN [104] 1.15 ASD [80] 16.26 TEDnet [92] 1.63 ANF [179] 7.9 MRCNet [135] 20.3 SDA-MCNN [215] 2.0 LCNN [213] 0.032\n\n\nH. Combination of image and video\n\nModern mainstream models for counting have been deployed only for images or videos [19], [162], [213], [216]- [219]. When the video sequences are available, some algorithms are proposed to leverage temporal consistency to impel weak constraints on consecutive density estimation. Xiong et al. [217] utilize an LSTM model to estimate densities from one frame to the next. Liu et al. [162] explicitly enforce the constraint under the condition that the number of people must be strictly conserved as they move about. Nevertheless, the constraint is difficult to express, w.r.t. densities. Furthermore, Liu et al. [219] regress people flow rather than regressing densities from video sequences, which imposes strong consistency constraints without complicated network architectures required. Therefore, designing effective algorithms, which can cater to images and videos simultaneous is a meaningful and promising direction.\n\n\nI. Wider-view crowd counting\n\nAlbeit outstanding performance have been achieved for crowd counting in single-view images, it is not applicable to large and wide scenes such as public parks or long subway platforms, since it cannot capture sufficient detailed information for a single-view camera. Therefore, to address the problem of wide-area counting, some efforts have been attempted to capture information from multiple camera views. For instance, Zhang et al. [130] proposed a multi-view multiscale (MVMS) fusion model to predict a 2D scene-level density map on the ground-plane. Furthermore, Zhang et al. [220] achieve this by using 3D feature fusion with 3D scene-level density maps. Compared with 2D fusion [130], 3D fusion not only preserves the property of 2D density maps but also extracts more useful information of the crowd densities along the z-dimension (height). The aforementioned two models are based on the assumption that the cameras are fixed and camera parameters are known, therefore, designing models for cross-scene and multi-view counting with moving cameras and unknown camera parameters is an interesting yet challenging future work.\n\n\nJ. Localization, classification and tracking beyond object counting\n\nThe density estimation CNN-based models for crowd counting, regression-based methods indeed, although accurate count provided, it does not indicate the precise location and exact size of objects, thus maybe limits the further research and application, such as high-level understanding, localization, classification, and tracking. Some attempts have been made, for instance, DecideNet [7] generates the detection and regression-based density maps separately to estimate crowd density, and an attention module is incorporated to guide the final count. However, the model trains a fully supervised network with bounding box annotations, which needs to take large computation cost. CL [76] regresses density and localization maps simultaneously by introducing a composition loss. LCFCN [221] estimates the crowd count by segmenting the object blobs in an image, which only employs the pointannotations. CL [76] and LCFCN [221] simply concern the localization of crowds, whereas, PSDDN [56] not only predicts the localization but also estimates the size of persons. LSF-CNN [57] locates the position of every person in the crowd, sizes the dot-annotated heads with bounding boxes and finally counts them. All the above works demonstrate that the potential research value of this direction.\n\n\nK. Small or tiny object counting\n\nThe problem of small or tiny objects has long been a challenging task in many computer vision communities. In highly congested crowd scenes, the sizes of persons' heads are tiny. Additionally, some other potential applications may include object counting the number of contiguous dense buildings, ships, small vehicles and countless others in remote sensing images [222]. An apparent difference between object counting in remote sensing scene and nature scenes, the orientations of the objects are arbitrary due to the overhead view rather than upright perspective. Some further directions may include the integration of visual attention mechanism, dilated convolution, deformable convolution layer, and rotation invariance design into the framework.\n\n\nVII. CONCLUSION\n\nRemarkable progress has been made in crowd counting over the past few decades. This paper has presented a survey of CNN-based density estimation and crowd counting models from several perspectives, including network architecture, learning paradigms, etc. We then summarize popular benchmark datasets, including crowd counting and several representing ones in other fields, as well as evaluation criteria for evaluating various methods. Besides, we also conduct a thorough performance benchmarking evaluation of representative models. Although all the works cannot be covered, we have selected the top-three performers to follow by a comprehensive and thorough analysis and discussion of these representing methods. We summarize the attributes or techniques which have a great assistant for the improvement of the performance.\n\nIn the next, we investigate several factors that would affect the performance of crowd counting, and we finally look through some potential challenges and open issues in deep learning era, and put forward insightful discussions and promising research directions in the future.\n\nStanding on the standpoint of technical innovations, we expect this work can provide a feasible scheme to understand state-of-the-art, but more importantly, insights for future exploration in crowd counting and bridge to object counting in other domains. \n\nFig. 3 :\n3Comparison of the structure of existing density map-based networks.\n\nFig. 4 :\n4Gray-Scale Scenes ShanghaiTech Part A: Congestion JHU-CROWD: Multiple Weathers UCF-QNRF: High-resolution Data NWPU-Crowd: Large-scale, On-Gray-Scale Scenes ShanghaiTech Part A: Congestion JHU-CROWD: Multiple Weathers UCF-QNRF: High-resolution Data NWPU-Crowd: Large-scale, On-Some samples from representing crowd counting datasets.\n\nFig. 5 :\n5The challenges in crowd counting.\n\nFig. 6 :\n6The exemplars of distractors and negative samples for crowd counting.\n\n\nreceived the Ph.D. degree in computer science from the Intelligent Recognition and Image Processing Laboratory, Beihang University, Beijing, China, in 2015. He is currently an Assistant Professor with the School of Computer Science and Engineering, Beihang University. He is also a Distinguished Research Fellow with the Hangzhou Institute of Innovation, Beihang University, Hangzhou. His current research interests include Remote sensing image analysis, pattern recognition, and computer vision. He is a member of the IEEE. PLACE PHOTO HERE Qi Wang (M'15-SM'15) received the B.E. degree in automation and the Ph.D. degree in pattern recognition and intelligent systems from the University of Science and Technology of China, Hefei, China, in 2005 and 2010, respectively. He is currently a Professor with the School of Computer Science and with the Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi'an, China. His research interests include computer vision and pattern recognition. PLACE PHOTO HERE Yunhong Wang received the B.S. degree in electronic engineering from Northwestern Polytechnical University, Xi'an, China, in 1989, and the M.S. and Ph.D. degrees in electronic engineering from the Nanjing University of Science and Technology, Nanjing, China, in 1995 and 1998, respectively. She was with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China, from 1998 to 2004. Since 2004, she has been a Professor with the School of Computer Science and Engineering, Beihang University, Beijing, where she is also the Director of the Laboratory of Intelligent Recognition and Image Processing and the Beijing Key Laboratory of Digital Media. Her research interests include biometrics, pattern recognition, computer vision, data fusion, and image processing. She is a Fellow of IEEE, IAPR, and CCF.\n\n\nGao, Qingjie Liu and Yunhong Wang are with the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Xueyuan Road, Haidian District, Beijing, 100191, China and Hangzhou Innovation Institute, Beihang University, liu@buaa.edu.cn;yhwang@buaa.edu.cn); Junyu Gao and Qi Wang are with the School of Computer Science and with the Center for Optical Imagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi'an 710072, Shanxi, China (email: gjy3035@gmail.com;crabwq@gmail.com) * Corresponding author: Qingjie LiuHangzhou, \n310051,China \n(email: \ngaoguangshuai1990@buaa.edu.cn; \nqingjie.\n\n1 )\n1Comprehensive and systematic overview from various aspects. We category the CNN-based models according to several taxonomies, including network architecture, supervised form, learning paradigm, etc. The taxonomies can motivate researches with a deep understanding of the critical techniques of CNN-based methods. 2) Attribute-based performance analysis. Based on the performance of the SOTA methods, we analyze the reasons why they perform well, the techniques they utilize. Besides, we discuss the various challenge factors that promote researchers to design more effective algorithms.(Chan et al.) \n\n(Lemptisky et al.) \n\n2008 \n\nMCNN \n(Zhang et al.) \n\n(Wang et al.) \n\n(Fu et al.) \nCP-CNN \n(Sindagi et al.) \n\nCSRNet \n(Li et al.) \n\n2018 \n\n2019 \n\nRR \n(Chen et al.) \n\nCA-RR \n(Chen et al.) \n\nMLR \n(Wu et al.) \n\nKRR \n(An et al.) \n\nPSSDN \n(Liu et al.) \n\n2006 2007 \n\nDetection-based \nRegression-based \nDensity estimation based \n\n2010 \n2009 \n2011 2012 2013 \n2014 2015 \n\nCount Forest \n(Pham et al.) \n\nCross Scene \n(Zhang et al.) \n\n2016 \n\nHydra-CNN \n(Onoro et al.) \n\n2017 \n\nCMTL \n(Sindagi et al.) \n\nSwitching CNN \n(Sam et al.) \n\nSA-Net \n(Cao et al.) \n\nLSF-CNN \n(Sam et al.) \n\nHeuristic Models \nModern Deep Models \n\nFig. 1: A brief chronology of crowd counting. The first incorporation of deep learning techniques for crowd counting is from 2015. See \nSection 1 for more detailed description. Milestone models in this figure: MLR [52], KRR [53], Chan et al. [27], Lemptisky et al. [16], \nRR [40], CA-RR [54], Count Forest [48], Wang et al. [49], Fu et al. \n\n\nFig. 2: The overall architecture of this work. We concentrate on the modern density map-based approaches mainly CNN-based for crowd counting.Traditional \nApproaches \n\nModern \nApproaches \n\nObject level: \nDetection-based \n\nImage/Patch level: \nRegression-based \n\nGrid/Pixel level: \nDensity \nestimation \n\nScale problem \n\nOcclusion \n\nNon-uniform distribution \n\nIllumination variation \n\nBackground noises \n\nNetwork architecture: \nBasic \nMulti-column \nSingle-column \n\nReference manner: \nPatch-based \nWhole image based \n\nSupervision form: \nSupervision-based \nUn/semi/self-supervision-\nbased \n\nDomains: \nDomains-specific \nMulti-domain \n\nSupervision level: \nInstance-level \nImage-level \n\nInput \nDensity \nmap \nInput \nDensity \nmap \n\nMulti-Column Networks \nSingle-Column Networks \n\nInput \nDensity \nmap \n\nBasic Networks \n\nCNN \nFully-connected layer \n\n\n\nTABLE I :\nISummary of previous reviews.# \nTitle \nYear \nVenue \nBrief description \n\n1 \nCrowd analysis: a survey [24] \n2008 MVA \nThis paper presents a survey on crowd analysis methods employed in computer \nvision research and discusses perspectives from other research disciplines and \nhow they can contribute to the computer vision approach. \n\n2 \nCrowd analysis using computer vision tech-\nniques [58] \n\n2010 ISPM \nA survey on crowd analysis by using computer vision techniques, including \ndifferent aspects such as people tracking, crowd density estimation, event \ndetection, validation and simulation. \n\n3 \nA Survey of Human-Sensing:Methods for Detect-\ning Presence, Count, Location, Track, and Iden-\ntity [59] \n\n2010 ACM Computing Surveys a survey of the inherently multidisciplinary literature of human-sensing , focusing \nmainly on the extraction of five commonly needed spatio-temporal properties: \nnamely presence, count, location, track and identity. \n\n4 \n\n\nTABLE II :\nIISummary of state-of-the-art methods. See II for more detailed description.Methods \nYear&Venue \nNetwork architecture \nReference manner \nSupervision form \nLearning paradigm \nSupervision level \n\nFu et al. [50] \n2015 EAAI \nBasic \nPatch-based \nFully-Sup. \nSTL \nInstance level \nWang et al. [49] \n2015 ACMMM \nBasic \nPatch-based \nFully-Sup. \nSTL \nInstance level \nCross scene [51] \n2015 CVPR \nBasic \nPatch-based \nFully-Sup. \nMTL \nInstance level \n\nMCNN [1] \n2016 CVPR \nMulti-column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nCrowdnet [3] \n2016 ACMMM \nMulti-column \nPatch-based \nFully-Sup. \nSTL \nInstance level \nCNN-Boosting [15] \n2016 ECCV \nBasic \nPatch-based \nFully-Sup. \nSTL \nInstance level \nHydra-CNN [2] \n2016 ECCV \nMulti-column \nPatch-based \nFully-Sup. \nMTL \nInstance level \nShang et al. [69] \n2016 ECCV \nMulti-column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \n\nCMTL [55] \n2017 AVSS \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nSwitching CNN [5] \n2017 CVPR \nMulti-column \nPatch-based \nFully-Sup. \nMTL \nInstance level \nCP-CNN [6] \n2017 ICCV \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \n\nD-ConvNet [70] \n2018 CVPR \nSingle-column \nWhole image-based \nFilly-Sup. \nSTL \nInstance level \nCSRNet [12] \n2018 CVPR \nSingle-column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nDRSAN [71] \n2018 IJCAI \nMulti-column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nDecideNet [7] \n2018 CVPR \nMulti-column \nPatch-based \nFully-Sup. \nMTL \nInstance level \nSaCNN [9] \n2018 WACV \nSingle column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nSACNN [11] \n2018 ECCV \nSingle column \nPatch-based \nFully-Sup. \nMTL \nInstance level \nIG-CNN [72] \n2018 CVPR \nMulti-column \nPatch-based \nFully-Sup. \nMTL \nInstance level \nic-CNN [73] \n2018 ECCV \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nACSCP [74] \n2018 CVPR \nMulti-column \nPatch-based \nFully-Sup. \nMTL \nInstance level \nNetVLAD [75] \n2018 TII \nSingle-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nCL [76] \n2018 ECCV \nSingle-column \nPatch-based \nFully-Sup. \nMTL \nInstance level \nL2R [77] \n2018 CVPR \nBasic \nWhole image-based \nSelf-Sup. \nMTL \n-\nGAN-MTR [78] \n2018 WACV \nBasic \nWhole image-based \nSemi-Sup. \nMTL \n-\n\nPaDNet [79] \n2019 TIP \nSingle-column \nPatch-based \nFully-Sup. \nSTL \nInstance level \nASD [80] \n2019 ICASSP \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nSPN [81] \n2019 WACV \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nSR-GAN [82] \n2019 CVIU \nBasic \nWhole image-based \nSemi-Sup. \nMTL \n-\nADCrowdnet [83] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nSAAN [8] \n2019 WACV \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nSAA-Net [13] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nSFCN \u2020 2 [84] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nSE Cycle GAN [84] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nPACNN [85] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nCAN&ECAN [86] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nCFF [87] \n2019 ICCV \nSingle-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nPCC Net [88] \n2019 TCSVT \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nSFANet [89] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nW-Net [90] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nSL2R [91] \n2019 CVPR \nBasic \nWhole image-based \nSelf-Sup. \nMTL \n-\nTEDnet [92] \n2019 CVPR \nSingle column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nRReg [93] \n2019 CVPR \nMulti-column \nWhole image-based \nFully-Sup. \nSTL \nInstance level \nRAZNet [94] \n2019 CVPR \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nAT-CNN [95] \n2019 CVPR \nSingle-column \nWhole image-based \nFully-Sup. \nMTL \nInstance level \nGWTA-CCNN [96] \n2019 AAAI \nSingle column \nPatch-based \nUn-Sup. \nSTL \n-\nHA-CCN [97] \n2019 TIP \nSingle column \nWhole image-based \nFully-Sup./Weak-Sup \nSTL \nInstance/Image level \nL2SM [98] \n2019 ICCV \nSingle column \nPatch-based \nFully-Sup \nSTL \nInstance level \nRANet [99] \n2019 ICCV \nMulti-column \nWhole image-based \nFully-Sup \nSTL \nInstance level \nMcML [100] \n2019 ACM MM \nMulti-column \nWhole image-based \nFully-Sup \nSTL \nInstance level \nILC [101] \n2019 CVPR \nMulti-column \nWhole image-based \nFully-Sup. \nMTL \nImage level \n\n\n\n\n1 http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm 2 http://personal.ie.cuhk.edu.hk/ ccloy/downloads mall dataset.html 3 https://github.com/davideverona/deep-crowd-counting crowdnet\n\nTABLE III :\nIIIStatistics of the object counting datasets, including crowd counting and other fields. Total-, min-, average-and max represent the total number, the minimum, average number and maximum number of instances in the datasets, respectively.Dataset Year \nAttributes \nNumber of Images Training/Test \nAverage Resolution \nCount Statistics \nTotal \nMin Average Max \nLHI 1 [137] 2007 \nReal-world \n-\n-\n352 \u00d7 288 \n-\n-\n-\n-\nUCSD [27] 2008 \nReal-world \n2000 \n800/1200 \n238 \u00d7 158 \n49,885 \n11 \n24.9 \n46 \nPETS [138] 2010 \nReal-world \n1076 \n-\n384\u00d7288 \n18289 \n0 \n-\n40 \nMall [40] 2012 \nReal-world \n2000 \n800/1200 \n320 \u00d7 240 \n62,325 \n13 \n31 \n53 \nUCF CC 50 [38] 2013 \nReal-world \n50 \n-\n2101 \u00d7 2888 \n63,974 \n94 \n1,280 4,543 \nAHU-Crowd [139] 2014 \nReal-world \n-\n-\n-\n-\n-\n-\n-\nMICC [140] 2014 \nReal-world \n3358 \n-\n-\n17630 \n0 \n5.25 \n28 \nWorldExpo'10 [51] 2015 \nReal-world \n3980 \n-\n576 \u00d7 720 \n199,923 \n1 \n50.2 \n253 \nIndoor 1 [141] 2016 \nReal-world \n570,000 \n-\n704\u00d7576 \n-\n0 \n-\n59 \nLHI 2 [142] 2016 \nReal-world \n3,100 \n-\n1280 \u00d7 720 \n5,900 \n-\n-\n-\nAHU-CROWD [143] 2016 \nReal-world \n107 \n-\n-\n45,000 \n58 \n-\n2201 \nSHT A [1] 2016 \nReal-world \n482 \n300/182 \n589 \u00d7 868 \n241,677 \n33 \n501.4 3,139 \nSHT B [1] 2016 \nReal-world \n716 \n400/316 \n768 \u00d7 1024 \n88,488 \n9 \n123.6 \n578 \nTrain Station [144] 2017 \nReal-world \n2000 \n-\n256 \u00d7 256 \n62581 \n1 \n-\n53 \nSTF(C5&C9) [144] 2017 \nReal-world \n788&600 \n-\n576 \u00d7 704 \n-\n3 \n-\n65 \nShanghai Subway Station [145] 2017 \nReal-world \n3,000 \n-\n-\n-\n28.78 \n-\n-\nBeijing BRT [146] 2018 \nReal-world \n1280 \n-\n640 \u00d7 360 \n-\n1 \n-\n64 \nEBP [147] 2018 \nReal-world \n-\n-\n720 \u00d7 408 \n-\n-\n-\n-\nSmartcity [9] 2018 \nReal-world \n50 \n-\n1920 \u00d7 1080 \n369 \n1 \n7.4 \n14 \nCrowdFlow [148] 2018 \nSynthetic \n-\n-\n300\u223c450 \n-\n-\n-\n-\nUCF-QNRF [76] 2018 \nReal-world \n1,535 \n1201/334 \n2013 \u00d7 2902 \n1,251,642 49 \n815 \n12,865 \nCIISR [149] 2019 \nReal-world \n1000 \n-\n1080 \u00d7 720 \n-\n-\n117 \n-\nVenice [86] 2019 \nReal-world \n167 \n-\n1280 \u00d7 720 \n-\n-\n-\n-\nIndoor 2 [150] 2019 \nReal-world \n148,243 \n-\n352 \u00d7 288 or 704\u00d7576 \n1,834,770 \n0 \n12.4 \n40 \nCity Street [130] 2019 \nReal-world \n500 \n300/200 \n676 \u00d7 380 \n-\n70 \n-\n150 \nShanghaiTechRGBD [131] 2019 \nReal-world \n2193 \n1193/1000 \n1080 \u00d7 1920 \n144,512 \n6 \n65.9 \n234 \nFDST [132] 2019 \nReal-world \n15,000 \n9000/6000 \n1920 \u00d7 1080 and 1280 \u00d7 720 394,081 \n9 \n26.7 \n57 \nCrowd Surveillance [133] 2019 \nReal-world \n13,945 \n-\n1342 \u00d7 840 \n386,513 \n-\n35 \n-\nJHU-CROWD [134] 2019 \nReal-world \n4250 \n3888/1062 \n1450 \u00d7 900 \n1,114,785 \n-\n262 \n7286 \nZhengzhouAirport [151] 2019 \nReal-world \n1,111 \n-\n-\n49,061 \n7 \n-\n128 \nDLR-ACD [135] 2019 \nAerial imagery \n33 \n19/14 \n3619 \u00d7 5226 \n226,291 285 \n6857 24,368 \nDroneCrowd [136] 2019 \nDrone-based \n33,600 \n-\n1920 \u00d7 1080 \n4,864,280 25 \n144.8 \n455 \nCategorized [152] 2019 \nCategories counting \n553 \n-\n-\n16,521 \n1 \n29.8 \n206 \nGCC [84] 2019 \nSynthetic \n15,212 \n-\n1080 \u00d7 1920 \n7,625,843 \n0 \n501 \n3,995 \nNWPU-Crowd [28] 2020 \nReal-world \n5,109 \n-\n2311 \u00d7 3383 \n2,133,238 \n0 \n418 \n20,033 \nCaltech [153] 2012 \nPedestrian detection \n2000 \n-\n-\n15043 \n6 \n-\n14 \nTRANCOS [20] 2015 \nVehicle counting \n1244 \n403/420/421 \n640 \u00d7 480 \n46,796 \n9 \n-\n107 \nPenguins [17] 2016 \nPenguins counting \n80095 \n-\n-\n-\n0 \n7.18 \n67 \nWIDER FACE [154] 2016 \nFace detection \n32,203 \n40%/10%/50% \n-\n393,703 \n-\n-\n-\nDukerMTMC [155] 2016 tracking, human detection or ReID \nover 2 million \n-\n1920\u00d71080 \n2,700 \n-\n-\n-\nWebCamT [156] 2017 \nWebCam traffic counting \n60 million \n42,200/17800 \n352\u00d7240 \n-\n-\n-\n-\nCARPK [157] 2017 Drone view-based car counting \n1448 \n989/459 \n-\n89,777 \n-\n-\n-\nMTC [158] 2017 \nPlanting counting \n361 \n186/175 \n-\n-\n-\n-\nDCC [124] 2018 \nCell counting \n177 \n100/77 \n-\n-\n0 \n34.1 \n101 \nWheat-Spike [159] 2018 \nwheat spikes counting \n20 \n8/2/10 \n1k\u223c3k \n20,101 \n749 \n1005 \n1287 \nVisDrone2019 People [160] 2018 \nDrone-based crowd counting \n3347 \n2392/329/626 \n969\u00d71482 \n108,464 \n10 \n32.41 \n289 \nVisDrone2019 Veheicle [160] 2018 \nDrone-based vehicle counting \n5303 \n3953/364/986 \n991\u00d71511 \n198,984 \n10 \n37.52 \n349 \n\ncollected from 212 web cameras with different locations, \ncamera perspective, and traffic states. \n\u2022 CARPK [157] 17 is a car counting datasets collected from \n4 different parking lots with drone view, which contains \nnearly 90,000 cars in total, all the images with bounding box \nannotations. \n\u2022 MTC [158] includes 361 high-resolution images of maize \ntassels in the wild filed. Compared with the other objects that \nhave similar physical sizes, maize tassels have the hetero-\ngeneous physical sizes and self-changing over time. Thus it \nis more suitable for evaluating the robustness to object-size \nvariations of the designed model. \n\u2022 DCC [124] is a cell microscopy dataset, which contains \n177 images with a variety of tissues and species. Across these \nimages, the average count is 34.1, and the standard deviation \nis 21.8. \n\u2022 Wheat-Spike [159] is a challenging dataset due to irregular \nplacement or collection of wheat spikes, which contains 20 \n\n17 https://lafi.github.io/LPN/ \n\nimages in total, where the data are split into 8, 2 and 10 for \ntraining, validation and test. \n\u2022 VisDrone2019 [160] originates from an object detection \ndataset with bounding boxes annotated, Bai et al. [161] takes \nthe center of bounding box as the location of objects, se-\nlecting pedestrian and people to form VisDrone2019 People \ndataset, and combining car, van, truck and bus to construct \nVisDrone2019 Vehicle dataset. \n\n\n\nTABLE\n\n\nTABLE V :\nVMain properties of state-of-the-art methods.Methods \n\nProperties \nMulti-\ncolumn \n\nSingle-\ncolumn \n\nAttention-\nbased \n\nDilation \nconvolu-\ntion \n\nSpatial \ntrans-\nformer \n\nCRFs/MRF \nPerspective \ninforma-\ntion \n\nPyramid \npooling \n\nPan-\ndensity \n/sub-\nregion \nMCNN [1] \nCSRNet [12] \nDRSAN [71] \nDecideNet [7] \nSCNet [185] \nPaDNet [79] \nSAAN [8] \nPACNN [85] \nCAN&ECAN [86] \nSFANet [89] \nW-Net [90] \nDSNet \n\nTABLE VI :\nVIComparison of the quality of density maps on the SHT A dataset\n\n\nTable VII reports the evaluation results on Shanghai Part A [1] of the pre-trained NWPU-Crowd [28] models. Compared with the oracle errors, there are obvious performance degradations: the average MAE increases by 44.6% and RMSE increases by 47.0% respectively.\n\nTABLE VII :\nVIIThe MAE/RMSE of the oracle evaluation (traditional supervised training within Shanghai Tech Part A) and cross-dataset evaluation (training on NWPU-Crowd and test on Shanghai Tech Part A), and the corresponding performance degradations.Method \nOracle \n(MAE/RMSE) \n\nCross-dataset \nEvaluation \n(MAE/RMSE) \n\nRelative \nErrors Rise (%) \n\nMCNN [1] \n110.2/173.2 \n151.6/217.7 \n\u2191 37.6/25.7 \nCSRNet [12] \n68.2/115.0 \n111.0/169.2 \n\u2191 62.6/53.5 \nC3F-VGG [211] \n71.4/115.7 \n96.5/151.6 \n\u2191 35.2/31.0 \nCANNet [86] \n62.3/100.0 \n83.5/137.4 \n\u2191 34.0/37.4 \nSCAR [210] \n66.3/114.1 \n96.6/161.5 \n\u2191 45.7/71.5 \nSFCN \u2020 [84] \n71.5/114.3 \n108.8/185.8 \n\u2191 52.2/62.6 \n\n\n\n\nrange, image style, etc. To remedy the domain gap, the technique of domain adaptation comes in handy, which can provide a feasible solution to reduce manpower by transferring effective features across diverse domains. In this process, GAN-based methods have demonstrated an important influence on this issue. For instance, SSIM Embedding (SE) Cycle GAN [84] takes advantage of the domain adaptation technique, incorporating Structural Similarity Index (SSIM)\n\nTABLE VIII :\nVIIIThe MAE/RMSE of mainstream methods on the distractors of JHU-CROWD and the negative samples of NWPU-Crowd.Therefore, there maybe an alternative way that uses multi-task learning (patch-level counting, segmentation, group detection, etc.) to extract large-range features.Method \nDistractors \nin JHU-CROWD \n\nNegative Samples \nin NWPU-Crowd \nMCNN [1] \n103.8/238.5 \n356.0/1232.5 \nCMTL [1] \n135.8/263.8 \n-\nSwitching CNN [5] \n100.5/235.5 \n-\nSA-Net [11] \n71.9/167.7 \n432.0/974.4 \nPCC-Net [88] \n-\n85.3/438.8 \nCSRNet [12] \n44.3/102.4 \n176.0/572.3 \nC3F-VGG [211] \n-\n141.0/474.2 \nCANNet [86] \n-\n82.6/343.4 \nSCAR [210] \n-\n122.9/660.8 \nSFCN \u2020 [84] \n-\n54.2/154.7 \nCG-DRCN [134] \n43.4/97.8 \n-\n\n\n\nTABLE IX :\nIXNumber of parameters (in millions). Red and underline indicate the least and the most parameters' number, respectively.\nhttp://www.ee.cuhk.edu.hk/ xgwang/expo.html 5 https://pan.baidu.com/s/1nuAYslz\nhttps://pan.baidu.com/s/1pMuGyNp#list/path=%2F 7 https://www.crcv.ucf.edu/data/ucf-qnrf/ 8 https://github.com/sweetyy83/Lstn fdst dataset 9 https://ai.baidu.com/broad/subordinate?dataset=crowd surv 10 https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/22294 read-58354/\nhttps://gjy3035.github.io/GCC-CL/ 12 http://www.crowdbenchmark.com/ 13 https://github.com/XMU-smartdsp/Beijing-BRT-dataset 14 http://agamenon.tsc.uah.es/Personales/rlopez/data/trancos 15 www.robots.ox.ac.uk/ vgg/research/penguins 16 http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/\nACKNOWLEDGMENTThe authors would like to thank reviewers for their valuable suggestions and comments.\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, CVPR. 1819Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma, \"Single-image crowd counting via multi-column convolutional neural network,\" in CVPR, 2016, pp. 589-597. 1, 3, 4, 5, 9, 11, 13, 14, 15, 16, 17, 18, 19\n\nTowards perspective-free object counting with deep learning. D Onoro-Rubio, R J L\u00f3pez-Sastre, ECCV. SpringerD. Onoro-Rubio and R. J. L\u00f3pez-Sastre, \"Towards perspective-free object counting with deep learning,\" in ECCV. Springer, 2016, pp. 615-629. 1, 3, 4, 5, 7, 19\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S Kruthiventi, R V Babu, ACM MM. ACM15L. Boominathan, S. S. Kruthiventi, and R. V. Babu, \"Crowdnet: A deep convolutional network for dense crowd counting,\" in ACM MM. ACM, 2016, pp. 640-644. 1, 4, 5\n\nCrowd counting by adaptively fusing predictions from an image pyramid. D Kang, A Chan, BMVC. 1D. Kang and A. Chan, \"Crowd counting by adaptively fusing predic- tions from an image pyramid,\" BMVC, 2018. 1\n\nSwitching convolutional neural network for crowd counting. D B Sam, S Surya, R V Babu, CVPR. IEEE. 1819D. B. Sam, S. Surya, and R. V. Babu, \"Switching convolutional neural network for crowd counting,\" in CVPR. IEEE, 2017, pp. 4031-4039. 1, 3, 4, 5, 13, 14, 18, 19\n\nGenerating high-quality crowd density maps using contextual pyramid cnns. V A Sindagi, V M Patel, 1819V. A. Sindagi and V. M. Patel, \"Generating high-quality crowd density maps using contextual pyramid cnns,\" in ICCV, 2017, pp. 1861-1870. 1, 3, 5, 13, 14, 16, 17, 18, 19\n\nDecidenet: Counting varying density crowds through attention guided detection and density estimation. J Liu, C Gao, D Meng, A G Hauptmann, CVPR. 1720J. Liu, C. Gao, D. Meng, and A. G. Hauptmann, \"Decidenet: Counting varying density crowds through attention guided detection and density estimation,\" in CVPR, 2018, pp. 5197-5206. 1, 5, 6, 13, 14, 15, 17, 20\n\nCrowd counting using scale-aware attention networks. M Hossain, M Hosseinzadeh, O Chanda, Y Wang, WACV. IEEE115M. Hossain, M. Hosseinzadeh, O. Chanda, and Y. Wang, \"Crowd counting using scale-aware attention networks,\" in WACV. IEEE, 2019, pp. 1280-1288. 1, 5, 13, 14, 15\n\nCrowd counting via scale-adaptive convolutional neural network,\" in WACV. L Zhang, M Shi, Q Chen, IEEE1114L. Zhang, M. Shi, and Q. Chen, \"Crowd counting via scale-adaptive convolutional neural network,\" in WACV. IEEE, 2018, pp. 1113-1121. 1, 5, 6, 10, 11, 14\n\nImproved crowd counting method based on scale-adaptive convolutional neural network. J Sang, W Wu, H Luo, H Xiang, Q Zhang, H Hu, X Xia, IEEE Access. 1J. Sang, W. Wu, H. Luo, H. Xiang, Q. Zhang, H. Hu, and X. Xia, \"Im- proved crowd counting method based on scale-adaptive convolutional neural network,\" IEEE Access, 2019. 1\n\nScale aggregation network for accurate and efficient crowd counting. X Cao, Z Wang, Y Zhao, F Su, in ECCV. 1619X. Cao, Z. Wang, Y. Zhao, and F. Su, \"Scale aggregation network for accurate and efficient crowd counting,\" in ECCV, 2018, pp. 734-750. 1, 3, 5, 6, 14, 16, 17, 19\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, CVPR. 1819Y. Li, X. Zhang, and D. Chen, \"Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes,\" in CVPR, 2018, pp. 1091-1100. 1, 3, 5, 6, 13, 14, 15, 17, 18, 19\n\nScale-aware attention network for crowd counting. R R Varior, B Shuai, J Tighe, D Modolo, CVPR. 1516R. R. Varior, B. Shuai, J. Tighe, and D. Modolo, \"Scale-aware attention network for crowd counting,\" CVPR, 2019. 1, 5, 6, 13, 14, 15, 16\n\nFast visual object counting via example-based density estimation. Y Wang, Y Zou, ICIP. 1IEEEY. Wang and Y. Zou, \"Fast visual object counting via example-based density estimation,\" in ICIP. IEEE, 2016, pp. 3653-3657. 1\n\nLearning to count with cnn boosting. E Walach, L Wolf, ECCV. Springer15E. Walach and L. Wolf, \"Learning to count with cnn boosting,\" in ECCV. Springer, 2016, pp. 660-676. 1, 2, 4, 5\n\nLearning to count objects in images. V Lempitsky, A Zisserman, NIPS. 1519V. Lempitsky and A. Zisserman, \"Learning to count objects in images,\" in NIPS, 2010, pp. 1324-1332. 1, 2, 3, 14, 15, 19\n\nCounting in the wild. C Arteta, V Lempitsky, A Zisserman, ECCV. Springer111C. Arteta, V. Lempitsky, and A. Zisserman, \"Counting in the wild,\" in ECCV. Springer, 2016, pp. 483-498. 1, 10, 11\n\nVisual translation embedding network for visual relation detection. H Zhang, Z Kyaw, S.-F Chang, T.-S Chua, H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua, \"Visual translation embedding network for visual relation detection,\" in CVPR, 2017, pp. 5532-5540. 1\n\nFcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. S Zhang, G Wu, J P Costeira, J M Moura, 119S. Zhang, G. Wu, J. P. Costeira, and J. M. Moura, \"Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras,\" in ICCV, 2017, pp. 3667-3676. 1, 19\n\nExtremely overlapping vehicle counting. R Guerrero-G\u00f3mez-Olmedo, B Torre-Jim\u00e9nez, R L\u00f3pez-Sastre, S Maldonado-Basc\u00f3n, D Onoro-Rubio, PRIA. Springer112R. Guerrero-G\u00f3mez-Olmedo, B. Torre-Jim\u00e9nez, R. L\u00f3pez-Sastre, S. Maldonado-Basc\u00f3n, and D. Onoro-Rubio, \"Extremely overlapping vehicle counting,\" in PRIA. Springer, 2015, pp. 423-431. 1, 10, 11, 12\n\nLeaf counting with deep convolutional and deconvolutional networks. S Aich, I Stavness, S. Aich and I. Stavness, \"Leaf counting with deep convolutional and deconvolutional networks,\" in ICCV, 2017, pp. 2080-2089. 1\n\nLearning to count leaves in rosette plants. M V Giuffrida, M Minervini, S A Tsaftaris, 2016. 1M. V. Giuffrida, M. Minervini, and S. A. Tsaftaris, \"Learning to count leaves in rosette plants,\" 2016. 1\n\nConvolutional neural networks for counting fish in fisheries surveillance video. G French, M Fisher, M Mackiewicz, C Needle, MVABG. French, M. Fisher, M. Mackiewicz, and C. Needle, \"Convolutional neural networks for counting fish in fisheries surveillance video,\" MVAB, pp. 1-7, 2015. 1\n\nCrowd analysis: a survey. B Zhan, D N Monekosso, P Remagnino, S A Velastin, L.-Q Xu, MVA. 195-6B. Zhan, D. N. Monekosso, P. Remagnino, S. A. Velastin, and L.-Q. Xu, \"Crowd analysis: a survey,\" MVA, vol. 19, no. 5-6, pp. 345-357, 2008. 1, 2, 4\n\nDeeply learned attributes for crowded scene understanding. J Shao, K Kang, C Change Loy, X Wang, CVPR. J. Shao, K. Kang, C. Change Loy, and X. Wang, \"Deeply learned attributes for crowded scene understanding,\" in CVPR, 2015, pp. 4657- 4666. 1\n\nUnderstanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents. B Zhou, X Wang, X Tang, CVPR. IEEEB. Zhou, X. Wang, and X. Tang, \"Understanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents,\" in CVPR. IEEE, 2012, pp. 2871-2878. 1\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, Z.-S J Liang, N Vasconcelos, CVPR. 1, 2, 3, 9IEEE1317A. B. Chan, Z.-S. J. Liang, and N. Vasconcelos, \"Privacy preserving crowd monitoring: Counting people without people models or track- ing,\" in CVPR. IEEE, 2008, pp. 1-7. 1, 2, 3, 9, 11, 13, 14, 17\n\nNwpu-crowd: A large-scale benchmark for crowd counting. Q Wang, J Gao, W Lin, X Li, arXiv:2001.03360v11718Q. Wang, J. Gao, W. Lin, and X. Li, \"Nwpu-crowd: A large-scale benchmark for crowd counting,\" arXiv:2001.03360v1, 2020. 1, 10, 11, 17, 18\n\nCounting people by clustering person detector outputs. I S Topkaya, H Erdogan, F Porikli, AVSS. 1IEEEI. S. Topkaya, H. Erdogan, and F. Porikli, \"Counting people by clustering person detector outputs,\" in AVSS. IEEE, 2014, pp. 313- 318. 1\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. M Li, Z Zhang, K Huang, T Tan, ICPR. IEEEM. Li, Z. Zhang, K. Huang, and T. Tan, \"Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection,\" in ICPR. IEEE, 2008, pp. 1-4. 1\n\nPedestrian detection in crowded scenes. B Leibe, E Seemann, B Schiele, CVPR. IEEE1B. Leibe, E. Seemann, and B. Schiele, \"Pedestrian detection in crowded scenes,\" in CVPR, vol. 1. IEEE, 2005, pp. 878-885. 1\n\nMonocular pedestrian detection: Survey and experiments. M Enzweiler, D M Gavrila, TPAMI. 3112M. Enzweiler and D. M. Gavrila, \"Monocular pedestrian detection: Survey and experiments,\" TPAMI, vol. 31, no. 12, pp. 2179-2195, 2009. 1\n\nFast r-cnn. R Girshick, ICCV. 216R. Girshick, \"Fast r-cnn,\" in ICCV, 2015, pp. 1440-1448. 2, 16\n\nFaster r-cnn: Towards realtime object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPS. 214S. Ren, K. He, R. Girshick, and J. Sun, \"Faster r-cnn: Towards real- time object detection with region proposal networks,\" in NIPS, 2015, pp. 91-99. 2, 14\n\nMask r-cnn. K He, G Gkioxari, P Doll\u00e1r, R Girshick, K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \"Mask r-cnn,\" in ICCV, 2017, pp. 2961-2969. 2\n\nYou only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, CVPR. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You only look once: Unified, real-time object detection,\" in CVPR, 2016, pp. 779-788. 2\n\nSsd: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y Fu, A C Berg, ECCV. SpringerW. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, \"Ssd: Single shot multibox detector,\" in ECCV. Springer, 2016, pp. 21-37. 2\n\nMulti-source multiscale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, CVPR. 217H. Idrees, I. Saleemi, C. Seibert, and M. Shah, \"Multi-source multi- scale counting in extremely dense crowd images,\" in CVPR, 2013, pp. 2547-2554. 2, 9, 11, 14, 17\n\nBayesian poisson regression for crowd counting. A B Chan, N Vasconcelos, ICCV. IEEEA. B. Chan and N. Vasconcelos, \"Bayesian poisson regression for crowd counting,\" in ICCV. IEEE, 2009, pp. 545-551. 2\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, BMVC. 117p. 3. 2, 3, 9K. Chen, C. C. Loy, S. Gong, and T. Xiang, \"Feature mining for localised crowd counting.\" in BMVC, vol. 1, no. 2, 2012, p. 3. 2, 3, 9, 11, 14, 17\n\nCrowd counting using multiple local features. D Ryan, S Denman, C Fookes, S Sridharan, DITCA. IEEED. Ryan, S. Denman, C. Fookes, and S. Sridharan, \"Crowd counting using multiple local features,\" in DITCA. IEEE, 2009, pp. 81-88. 2\n\nObject recognition from local scale-invariant features. D G Lowe, ICCV. 99D. G. Lowe et al., \"Object recognition from local scale-invariant features.\" in ICCV, vol. 99, no. 2, 1999, pp. 1150-1157. 2\n\nGray scale and rotation invariant texture classification with local binary patterns. T Ojala, M Pietik\u00e4inen, T M\u00e4enp\u00e4\u00e4, ECCV. SpringerT. Ojala, M. Pietik\u00e4inen, and T. M\u00e4enp\u00e4\u00e4, \"Gray scale and rotation invariant texture classification with local binary patterns,\" in ECCV. Springer, 2000, pp. 404-420. 2\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, CVPR. IEEE Computer Society1N. Dalal and B. Triggs, \"Histograms of oriented gradients for human detection,\" in CVPR, vol. 1. IEEE Computer Society, 2005, pp. 886- 893. 2\n\nTextural features for image classification. R M Haralick, K Shanmugam, TSMC. 6R. M. Haralick, K. Shanmugam et al., \"Textural features for image classification,\" TSMC, no. 6, pp. 610-621, 1973. 2\n\nA mrf-based approach for real-time subway monitoring. N Paragios, V Ramesh, CVPR. IEEE1N. Paragios and V. Ramesh, \"A mrf-based approach for real-time subway monitoring,\" in CVPR, vol. 1. IEEE, 2001, pp. 1-1034. 2\n\nLatent gaussian mixture regression for human pose estimation. Y Tian, L Sigal, H Badino, F De La Torre, Y Liu, ACCV. SpringerY. Tian, L. Sigal, H. Badino, F. De la Torre, and Y. Liu, \"Latent gaussian mixture regression for human pose estimation,\" in ACCV. Springer, 2010, pp. 679-690. 2\n\nCount forest: Co-voting uncertain number of targets using random forest for crowd density estimation. V.-Q Pham, T Kozakaya, O Yamaguchi, R Okada, ICCV. 214V.-Q. Pham, T. Kozakaya, O. Yamaguchi, and R. Okada, \"Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation,\" in ICCV, 2015, pp. 3253-3261. 2, 3, 14\n\nDeep people counting in extremely dense crowds. C Wang, H Zhang, L Yang, S Liu, X Cao, ACM MM. ACM25C. Wang, H. Zhang, L. Yang, S. Liu, and X. Cao, \"Deep people counting in extremely dense crowds,\" in ACM MM. ACM, 2015, pp. 1299-1302. 2, 3, 4, 5\n\nFast crowd density estimation with convolutional neural networks. M Fu, P Xu, X Li, Q Liu, M Ye, C Zhu, EAAI. 43M. Fu, P. Xu, X. Li, Q. Liu, M. Ye, and C. Zhu, \"Fast crowd density estimation with convolutional neural networks,\" EAAI, vol. 43, pp. 81- 88, 2015. 2, 3, 4, 5\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, CVPR. 216C. Zhang, H. Li, X. Wang, and X. Yang, \"Cross-scene crowd counting via deep convolutional neural networks,\" in CVPR, 2015, pp. 833-841. 2, 3, 5, 7, 8, 9, 11, 12, 13, 14, 16\n\nCrowd density estimation using texture analysis and learning,\" in ICRB. X Wu, G Liang, K K Lee, Y Xu, IEEEX. Wu, G. Liang, K. K. Lee, and Y. Xu, \"Crowd density estimation using texture analysis and learning,\" in ICRB. IEEE, 2006, pp. 214- 219. 3\n\nFace recognition using kernel ridge regression. S An, W Liu, S Venkatesh, CVPR. IEEES. An, W. Liu, and S. Venkatesh, \"Face recognition using kernel ridge regression,\" in CVPR. IEEE, 2007, pp. 1-7. 3\n\nCumulative attribute space for age and crowd density estimation. K Chen, S Gong, T Xiang, C. Change Loy, CVPR. 314K. Chen, S. Gong, T. Xiang, and C. Change Loy, \"Cumulative attribute space for age and crowd density estimation,\" in CVPR, 2013, pp. 2467- 2474. 3, 14\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. V A Sindagi, V M Patel, AVSS. 314IEEEV. A. Sindagi and V. M. Patel, \"Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting,\" in AVSS. IEEE, 2017, pp. 1-6. 3, 5, 6, 14\n\nPoint in, box out: Beyond counting persons in crowds. Y Liu, M Shi, Q Zhao, X Wang, CVPR. 20Y. Liu, M. Shi, Q. Zhao, and X. Wang, \"Point in, box out: Beyond counting persons in crowds,\" CVPR, 2019. 3, 14, 20\n\nLocate, size and count: Accurately resolving people in dense crowds via detection. D B Sam, S V Peri, A Kamath, R V Babu, arXiv:1906.075381420arXiv preprintD. B. Sam, S. V. Peri, A. Kamath, R. V. Babu et al., \"Locate, size and count: Accurately resolving people in dense crowds via detection,\" arXiv preprint arXiv:1906.07538, 2019. 3, 14, 20\n\nCrowd analysis using computer vision techniques. J C S J Junior, S R Musse, C R Jung, ISPM. 2754J. C. S. J. Junior, S. R. Musse, and C. R. Jung, \"Crowd analysis using computer vision techniques,\" ISPM, vol. 27, no. 5, pp. 66-77, 2010. 2, 4\n\nA survey of human-sensing: Methods for detecting presence, count, location, track, and identity. T Teixeira, G Dublon, A Savvides, ACM Computing Surveys. 514T. Teixeira, G. Dublon, and A. Savvides, \"A survey of human-sensing: Methods for detecting presence, count, location, track, and identity,\" ACM Computing Surveys, vol. 5, no. 1, pp. 59-69, 2010. 4\n\nCrowd counting and profiling: Methodology and evaluation. C C Loy, K Chen, S Gong, T Xiang, MSVAC. Springer24C. C. Loy, K. Chen, S. Gong, and T. Xiang, \"Crowd counting and profiling: Methodology and evaluation,\" in MSVAC. Springer, 2013, pp. 347-382. 2, 4\n\nPerformance evaluation of crowd image analysis using the pets2009 dataset. J Ferryman, A.-L Ellis, PRL. 444J. Ferryman and A.-L. Ellis, \"Performance evaluation of crowd image analysis using the pets2009 dataset,\" PRL, vol. 44, pp. 3-15, 2014. 4\n\nCrowded scene analysis: A survey. T Li, H Chang, M Wang, B Ni, R Hong, S Yan, TCSVT. 2534T. Li, H. Chang, M. Wang, B. Ni, R. Hong, and S. Yan, \"Crowded scene analysis: A survey,\" TCSVT, vol. 25, no. 3, pp. 367-386, 2015. 2, 4\n\nAn evaluation of crowd counting methods, features and regression models. D Ryan, S Denman, S Sridharan, C Fookes, CVIU. 1304D. Ryan, S. Denman, S. Sridharan, and C. Fookes, \"An evaluation of crowd counting methods, features and regression models,\" CVIU, vol. 130, pp. 1-17, 2015. 4\n\nRecent survey on crowd density estimation and counting for visual surveillance. S A M Saleh, S A Suandi, H Ibrahim, EAAI. 414S. A. M. Saleh, S. A. Suandi, and H. Ibrahim, \"Recent survey on crowd density estimation and counting for visual surveillance,\" EAAI, vol. 41, pp. 103-114, 2015. 2, 4\n\nAdvances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques. M S Zitouni, H Bhaskar, J Dias, M E Al-Mualla, Neurocomputing. 1864M. S. Zitouni, H. Bhaskar, J. Dias, and M. E. Al-Mualla, \"Advances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques,\" Neurocomputing, vol. 186, pp. 139-159, 2016. 2, 4\n\nCrowd scene understanding from video: a survey. J M Grant, P J Flynn, TOMM. 1324J. M. Grant and P. J. Flynn, \"Crowd scene understanding from video: a survey,\" TOMM, vol. 13, no. 2, p. 19, 2017. 2, 4\n\nA survey of recent advances in cnnbased single image crowd counting and density estimation. V A Sindagi, V M Patel, PRL. 1074V. A. Sindagi and V. M. Patel, \"A survey of recent advances in cnn- based single image crowd counting and density estimation,\" PRL, vol. 107, pp. 3-16, 2018. 2, 4\n\nConvolutional neural networks for crowd behaviour analysis: a survey. G Tripathi, K Singh, D K Vishwakarma, The Visual Computer. 354G. Tripathi, K. Singh, and D. K. Vishwakarma, \"Convolutional neural networks for crowd behaviour analysis: a survey,\" The Visual Com- puter, vol. 35, no. 5, pp. 753-776, 2019. 2, 4\n\nEnd-to-end crowd counting via joint learning local and global count. C Shang, H Ai, B Bai, ICIP. 57IEEEC. Shang, H. Ai, and B. Bai, \"End-to-end crowd counting via joint learning local and global count,\" in ICIP. IEEE, 2016, pp. 1215- 1219. 5, 7\n\nCrowd counting with deep negative correlation learning. Z Shi, L Zhang, Y Liu, X Cao, Y Ye, M.-M Cheng, G Zheng, CVPR. 519Z. Shi, L. Zhang, Y. Liu, X. Cao, Y. Ye, M.-M. Cheng, and G. Zheng, \"Crowd counting with deep negative correlation learning,\" in CVPR, 2018, pp. 5382-5390. 5, 6, 19\n\nCrowd counting using deep recurrent spatial-aware network. L Liu, H Wang, G L Ouyang, L Lin, in IJCAI. 519L. Liu, H. Wang, G. L. andWanli Ouyang, and L. Lin, \"Crowd counting using deep recurrent spatial-aware network,\" in IJCAI, 2018, pp. 849- 855. 5, 13, 14, 15, 19\n\nDivide and grow: capturing huge diversity in crowd images with incrementally growing cnn. D Sam, N N Sajjan, R Venkatesh, M Babu, Srinivasan, in CVPR. 519D. Babu Sam, N. N. Sajjan, R. Venkatesh Babu, and M. Srinivasan, \"Divide and grow: capturing huge diversity in crowd images with incrementally growing cnn,\" in CVPR, 2018, pp. 3618-3626. 5, 6, 14, 19\n\nIterative crowd counting. V Ranjan, H Le, M Hoai, ECCV. 519V. Ranjan, H. Le, and M. Hoai, \"Iterative crowd counting,\" in ECCV, 2018, pp. 270-285. 5, 6, 14, 19\n\nCrowd counting via adversarial cross-scale consistency pursuit. Z Shen, Y Xu, B Ni, M Wang, J Hu, X Yang, CVPR. 519Z. Shen, Y. Xu, B. Ni, M. Wang, J. Hu, and X. Yang, \"Crowd counting via adversarial cross-scale consistency pursuit,\" in CVPR, 2018, pp. 5245-5254. 5, 6, 14, 16, 19\n\nMultiscale multitask deep netvlad for crowd counting. Z Shi, L Zhang, Y Sun, Y Ye, TII. 14117Z. Shi, L. Zhang, Y. Sun, and Y. Ye, \"Multiscale multitask deep netvlad for crowd counting,\" TII, vol. 14, no. 11, pp. 4953-4962, 2018. 5, 7\n\nComposition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S Al-Maadeed, N Rajpoot, M Shah, in ECCV. 520H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed, N. Rajpoot, and M. Shah, \"Composition loss for counting, density map estimation and localization in dense crowds,\" in ECCV, 2018, pp. 532-546. 5, 6, 10, 11, 14, 17, 20\n\nLeveraging unlabeled data for crowd counting by learning to rank. X Liu, J Van De Weijer, A D Bagdanov, in CVPR. 519X. Liu, J. van de Weijer, and A. D. Bagdanov, \"Leveraging unlabeled data for crowd counting by learning to rank,\" in CVPR, 2018, pp. 7661-7669. 5, 8, 19\n\nCrowd counting with minimal data using generative adversarial networks for multiple target regression,\" in WACV. G Olmschenk, H Tang, Z Zhu, IEEE5G. Olmschenk, H. Tang, and Z. Zhu, \"Crowd counting with minimal data using generative adversarial networks for multiple target regres- sion,\" in WACV. IEEE, 2018, pp. 1151-1159. 5, 8\n\nPadnet: Pan-density crowd counting. Y Tian, Y Lei, J Zhang, J Z Wang, TIP. 1415Y. Tian, Y. Lei, J. Zhang, and J. Z. Wang, \"Padnet: Pan-density crowd counting,\" TIP, 2019. 5, 7, 12, 13, 14, 15\n\nAdaptive scenario discovery for crowd counting. X Wu, Y Zheng, H Ye, W Hu, J Yang, L He, ICASSP. 1719X. Wu, Y. Zheng, H. Ye, W. Hu, J. Yang, and L. He, \"Adaptive scenario discovery for crowd counting,\" ICASSP, 2019. 5, 13, 14, 17, 19\n\nScale pyramid network for crowd counting. C X , B Y , S N , G C , WACV514C. X., B. Y., S. N., and G. C., \"Scale pyramid network for crowd counting,\" in WACV, 2019, pp. 1941-1950. 5, 6, 13, 14\n\nGeneralizing semi-supervised generative adversarial networks to regression using feature contrasting. G Olmschenk, Z Zhu, H Tang, CVIU. 186G. Olmschenk, Z. Zhu, and H. Tang, \"Generalizing semi-supervised generative adversarial networks to regression using feature contrasting,\" CVIU, vol. 186, pp. 1-12, 2019. 5, 8\n\nAdcrowdnet: An attention-injective deformable convolutional network for crowd understanding. N Liu, Y Long, C Zou, Q Niu, L Pan, H Wu, CVPR. 1518N. Liu, Y. Long, C. Zou, Q. Niu, L. Pan, and H. Wu, \"Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding,\" CVPR, 2019. 5, 6, 13, 14, 15, 18\n\nLearning from synthetic data for crowd counting in the wild. Q Wang, J Gao, W Lin, Y Yuan, CVPR. 1819Q. Wang, J. Gao, W. Lin, and Y. Yuan, \"Learning from synthetic data for crowd counting in the wild,\" in CVPR, 2019. 5, 8, 10, 11, 13, 14, 17, 18, 19\n\nRevisiting perspective information for efficient crowd counting. M Shi, Z Yang, C Xu, Q Chen, CVPR. 1415M. Shi, Z. Yang, C. Xu, and Q. Chen, \"Revisiting perspective infor- mation for efficient crowd counting,\" CVPR, 2019. 5, 13, 14, 15\n\nContext-aware crowd counting. W Liu, M Salzmann, P Fua, CVPR. 1819W. Liu, M. Salzmann, and P. Fua, \"Context-aware crowd counting,\" CVPR, 2019. 5, 10, 11, 13, 14, 15, 17, 18, 19\n\nCounting with focus for free. P M Shi, C G M Snoek, ICCV. 518P. M. Zenglin Shi and C. G. M. Snoek, \"Counting with focus for free,\" in ICCV, 2019, pp. 4200-4209. 5, 7, 13, 14, 18\n\nPcc net: Perspective crowd counting via spatial convolutional network. J Gao, Q Wang, X Li, 1819TCSVTJ. Gao, Q. Wang, and X. Li, \"Pcc net: Perspective crowd counting via spatial convolutional network,\" TCSVT, 2019. 5, 7, 14, 18, 19\n\nDual path multi-scale fusion networks with attention for crowd counting. L Zhu, Z Zhao, C Lu, Y Lin, Y Peng, T Yao, arXiv:1902.011151517arXiv preprintL. Zhu, Z. Zhao, C. Lu, Y. Lin, Y. Peng, and T. Yao, \"Dual path multi-scale fusion networks with attention for crowd counting,\" arXiv preprint arXiv:1902.01115, 2019. 5, 13, 14, 15, 17\n\nW-net: Reinforced u-net for density map estimation. V K Valloli, K Mehta, arXiv:1903.112491718arXiv preprintV. K. Valloli and K. Mehta, \"W-net: Reinforced u-net for density map estimation,\" arXiv preprint arXiv:1903.11249, 2019. 5, 6, 14, 15, 17, 18\n\nExploiting unlabeled data in cnns by self-supervised learning to rank. X Liu, J Van De Weijer, A D Bagdanov, TPAMI. 5X. Liu, J. Van De Weijer, and A. D. Bagdanov, \"Exploiting unlabeled data in cnns by self-supervised learning to rank,\" TPAMI, 2019. 5, 8\n\nCrowd counting and density estimation by trellis encoderdecoder network. X Jiang, Z Xiao, B Zhang, X Zhen, X Cao, D Doermann, L Shao, CVPR. 1819X. Jiang, Z. Xiao, B. Zhang, X. Zhen, X. Cao, D. Doermann, and L. Shao, \"Crowd counting and density estimation by trellis encoder- decoder network,\" CVPR, 2019. 5, 6, 14, 16, 17, 18, 19\n\nResidual regression with semantic prior for crowd counting. J Wan, W Luo, B Wu, A B Chan, W Liu, CVPR. 514J. Wan, W. Luo, B. Wu, A. B. Chan, and W. Liu, \"Residual regression with semantic prior for crowd counting,\" in CVPR, 2019, pp. 4036- 4045. 5, 14\n\nRecurrent attentive zooming for joint crowd counting and precise localization. C Liu, X Weng, Y Mu, CVPR. 514C. Liu, X. Weng, and Y. Mu, \"Recurrent attentive zooming for joint crowd counting and precise localization,\" in CVPR, 2019, pp. 1217- 1226. 5, 7, 14\n\nLeveraging heterogeneous auxiliary tasks to assist crowd counting. M Zhao, J Zhang, C Zhang, W Zhang, CVPR. 517M. Zhao, J. Zhang, C. Zhang, and W. Zhang, \"Leveraging heteroge- neous auxiliary tasks to assist crowd counting,\" in CVPR, 2019, pp. 12 736-12 745. 5, 7, 14, 17\n\nAlmost unsupervised learning for dense crowd counting. H M R V B Deepak Babu, Sam, N Neeraj, Sajjan, AAAI. 5H. M. R. V. B. Deepak Babu Sam, Neeraj N Sajjan, \"Almost unsuper- vised learning for dense crowd counting,\" in AAAI, 2019. 5, 8\n\nHa-ccn: Hierarchical attention-based crowd counting network. V A Sindagi, V M Patel, TIP. 1314V. A. Sindagi and V. M. Patel, \"Ha-ccn: Hierarchical attention-based crowd counting network,\" TIP, 2019. 5, 8, 13, 14\n\nLearn to scale: Generating multipolar normalized density map for crowd counting. C Xu, K Qiu, J Fu, S Bai, Y Xu, X Bai, ICCV. 15C. Xu, K. Qiu, J. Fu, S. Bai, Y. Xu, and X. Bai, \"Learn to scale: Generating multipolar normalized density map for crowd counting,\" in ICCV, 2019. 5, 7, 14, 15\n\nRelational attention network for crowd counting. A Zhang, J Shen, Z Xiao, F Zhu, X Zhen, X Cao, L Shao, ICCV. 514A. Zhang, J. Shen, Z. Xiao, F. Zhu, X. Zhen, X. Cao, and L. Shao, \"Relational attention network for crowd counting,\" in ICCV, 2019, pp. 6788-6797. 5, 14\n\nImproving the learning of multi-column convolutional neural network for crowd counting. Z Cheng, J Li, D Qi, W Xiao, H Junyan, H Alexander, ACMMM. 5Z. Cheng, J. Li, D. Qi, W. Xiao, H. Junyan, and H. Alexander, \"Improving the learning of multi-column convolutional neural network for crowd counting,\" ACMMM, 2019. 5\n\nObject counting and instance segmentation with image-level supervision. H Cholakkal, G Sun, F S Khan, L Shao, CVPR. 5H. Cholakkal, G. Sun, F. S. Khan, and L. Shao, \"Object counting and instance segmentation with image-level supervision,\" CVPR, 2019. 5, 8\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in NIPS, 2012, pp. 1097- 1105. 4\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. 516I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative adversarial nets,\" in NIPS, 2014, pp. 2672-2680. 5, 16\n\nTop-down feedback for crowd counting convolutional neural network. D B Sam, R V Babu, AAAI. 519D. B. Sam and R. V. Babu, \"Top-down feedback for crowd counting convolutional neural network,\" in AAAI, 2018. 5, 19\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, NIPS. 514M. Jaderberg, K. Simonyan, A. Zisserman et al., \"Spatial transformer networks,\" in NIPS, 2015, pp. 2017-2025. 5, 13, 14\n\nMixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting. S Kumagai, K Hotta, T Kurita, arXiv:1703.09393arXiv preprintS. Kumagai, K. Hotta, and T. Kurita, \"Mixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting,\" arXiv preprint arXiv:1703.09393, 2017. 5\n\nDadnet: Dilated-attentiondeformable convnet for crowd counting. D Guo, K Li, Z.-J Zha, M Wang, ACMMM. ACM518D. Guo, K. Li, Z.-J. Zha, and M. Wang, \"Dadnet: Dilated-attention- deformable convnet for crowd counting,\" in ACMMM. ACM, 2019, pp. 1823-1832. 5, 18\n\nCrowd counting via weighted vlad on dense attribute feature maps. B Sheng, C Shen, G Lin, J Li, W Yang, C Sun, TCSVT. 288B. Sheng, C. Shen, G. Lin, J. Li, W. Yang, and C. Sun, \"Crowd counting via weighted vlad on dense attribute feature maps,\" TCSVT, vol. 28, no. 8, pp. 1788-1797, 2018. 6\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, CVPR. 617C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" in CVPR, 2015, pp. 1-9. 6, 17\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, ICMICCAI. Springer617O. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional net- works for biomedical image segmentation,\" in ICMICCAI. Springer, 2015, pp. 234-241. 6, 17\n\nMultitask learning. R Caruana, Machine learning. 281R. Caruana, \"Multitask learning,\" Machine learning, vol. 28, no. 1, pp. 41-75, 1997. 6\n\nBeyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking. D Kang, Z Ma, A B Chan, TCSVT. 7D. Kang, Z. Ma, and A. B. Chan, \"Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking,\" TCSVT, 2018. 7\n\nNonlinear regression via deep negative correlation learning. L Zhang, Z Shi, M.-M Cheng, Y Liu, J.-W Bian, J T Zhou, G Zheng, Z Zeng, TPAMI. 716L. Zhang, Z. Shi, M.-M. Cheng, Y. Liu, J.-W. Bian, J. T. Zhou, G. Zheng, and Z. Zeng, \"Nonlinear regression via deep negative correlation learning,\" TPAMI, 2019. 7, 16\n\nDeep metric learning for crowdedness regression. Q Wang, J Wan, Y Yuan, TCSVT. 2810Q. Wang, J. Wan, and Y. Yuan, \"Deep metric learning for crowdedness regression,\" TCSVT, vol. 28, no. 10, pp. 2633-2643, 2018. 7\n\nAutoscale: Learning to scale for crowd counting. C Xu, D Liang, Y Xu, W Zhan, M Tomizuka, X Bai, arXiv:1912.09632716C. Xu, D. Liang, Y. Xu, W. Zhan, M. Tomizuka, and X. Bai, \"Autoscale: Learning to scale for crowd counting,\" arXiv:1912.09632, 2019. 7, 16\n\nGlobal sum pooling: A generalization trick for object counting with small datasets of large images. S Aich, I Stavness, CVPRWS. Aich and I. Stavness, \"Global sum pooling: A generalization trick for object counting with small datasets of large images,\" in CVPRW, 2019, pp. 73-82. 7\n\nCrowd counting via weighted vlad on a dense attribute feature map. B Sheng, C Shen, G Lin, J Li, W Yang, C Sun, TCSVT. 288B. Sheng, C. Shen, G. Lin, J. Li, W. Yang, and C. Sun, \"Crowd counting via weighted vlad on a dense attribute feature map,\" TCSVT, vol. 28, no. 8, pp. 1788-1797, 2016. 7\n\nWinner-take-all autoencoders. A Makhzani, B J Frey, NIPS. A. Makhzani and B. J. Frey, \"Winner-take-all autoencoders,\" in NIPS, 2015, pp. 2791-2799. 8\n\nDense crowd counting convolutional neural networks with minimal data using semi-supervised dual-goal generative adversarial networks. G Olmschenk, J Chen, H Tang, Z Zhu, CVPRWG. Olmschenk, J. Chen, H. Tang, and Z. Zhu, \"Dense crowd counting convolutional neural networks with minimal data using semi-supervised dual-goal generative adversarial networks,\" in CVPRW, 2019, pp. 21- 28. 8\n\nCrowd counting with limited labeling through submodular frame selection. Q Zhou, J Zhang, L Che, H Shan, J Z Wang, T-ITS. 205Q. Zhou, J. Zhang, L. Che, H. Shan, and J. Z. Wang, \"Crowd counting with limited labeling through submodular frame selection,\" T-ITS, vol. 20, no. 5, pp. 1728-1738, 2018. 8\n\nCoda: Counting objects via scale-aware adversarial density adaption. L Wang, Y Li, X Xue, ICME. 8L. Wang, Y. Li, and X. Xue, \"Coda: Counting objects via scale-aware adversarial density adaption,\" ICME, 2019. 8\n\nOne-shot scene-specific crowd counting. M A Hossain, M Kumar, M Hosseinzadeh, O Chanda, Y Wang, BMVC. 8M. A. Hossain, M. Kumar, M. Hosseinzadeh, O. Chanda, and Y. Wang, \"One-shot scene-specific crowd counting,\" BMVC, 2019. 8\n\nClass-agnostic counting. E Lu, W Xie, A Zisserman, ACCV. 819E. Lu, W. Xie, and A. Zisserman, \"Class-agnostic counting,\" ACCV, 2018. 8, 19\n\nPeople, penguins and petri dishes: Adapting object counting models to new visual domains and object types without forgetting. M Marsden, K Mcguinness, S Little, C E Keogh, N E O&apos;connor, CVPR. 819M. Marsden, K. McGuinness, S. Little, C. E. Keogh, and N. E. O'Connor, \"People, penguins and petri dishes: Adapting object count- ing models to new visual domains and object types without forgetting,\" in CVPR, 2018, pp. 8070-8079. 8, 11, 19\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, TIP. 13418Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli et al., \"Image quality assessment: from error visibility to structural similarity,\" TIP, vol. 13, no. 4, pp. 600-612, 2004. 8, 12, 18\n\nFeature-aware adaptation and structured density alignment for crowd counting in video surveillance. J Gao, Q Wang, Y Yuan, arXiv:1912.03672J. Gao, Q. Wang, and Y. Yuan, \"Feature-aware adaptation and struc- tured density alignment for crowd counting in video surveillance,\" arXiv:1912.03672, 2019. 8\n\nDomain-adaptive crowd counting via inter-domain features segregation and gaussian-prior reconstruction. J Gao, T Han, Q Wang, Y Yuan, arXiv:1912.03677818J. Gao, T. Han, Q. Wang, and Y. Yuan, \"Domain-adaptive crowd counting via inter-domain features segregation and gaussian-prior re- construction,\" arXiv:1912.03677, 2019. 8, 18\n\nFocus on semantic consistency for cross-domain crowd understanding. T Han, J Gao, Y Yuan, W Qi, arXiv:2002.08623arXiv preprintT. Han, J. Gao, Y. Yuan, and W. Qi, \"Focus on semantic consistency for cross-domain crowd understanding,\" arXiv preprint arXiv:2002.08623, 2020. 8\n\nCounting everyday objects in everyday scenes. P Chattopadhyay, R Vedantam, R R Selvaraju, D Batra, D Parikh, P. Chattopadhyay, R. Vedantam, R. R. Selvaraju, D. Batra, and D. Parikh, \"Counting everyday objects in everyday scenes,\" in CVPR, 2017, pp. 1135-1144. 8\n\nWide-area crowd counting via groundplane density maps and multi-view fusion cnns. Q Zhang, A B Chan, CVPR. 1020Q. Zhang and A. B. Chan, \"Wide-area crowd counting via ground- plane density maps and multi-view fusion cnns,\" in CVPR, 2019, pp. 8297-8306. 10, 11, 17, 20\n\nDensity map regression guided detection network for rgb-d crowd counting and localization. D Lian, J Li, J Zheng, W Luo, S Gao, CVPR. 1016D. Lian, J. Li, J. Zheng, W. Luo, and S. Gao, \"Density map regression guided detection network for rgb-d crowd counting and localization,\" in CVPR, 2019, pp. 1821-1830. 10, 11, 16\n\nLocality-constrained spatial transformer network for video crowd counting. Y Fang, B Zhan, W Cai, S Gao, B Hu, ICME. IEEE1011Y. Fang, B. Zhan, W. Cai, S. Gao, and B. Hu, \"Locality-constrained spatial transformer network for video crowd counting,\" in ICME. IEEE, 2019, pp. 814-819. 10, 11\n\nPerspective-guided convolution networks for crowd counting. Z Yan, Y Yuan, W Zuo, T Xiao, Y Wang, S Wen, E Ding, ICCV. 1415Z. Yan, Y. Yuan, W. Zuo, T. Xiao, Y. Wang, S. Wen, and E. Ding, \"Perspective-guided convolution networks for crowd counting,\" in ICCV, 2019. 10, 11, 14, 15\n\nPushing the frontiers of unconstrained crowd counting: New dataset and benchmark method. V A Sindagi, R Yasarla, V M Patel, ICCV. 1019V. A. Sindagi, R. Yasarla, and V. M. Patel, \"Pushing the frontiers of unconstrained crowd counting: New dataset and benchmark method,\" in ICCV, 2019, pp. 1221-1231. 10, 11, 18, 19\n\nMrcnet: Crowd counting and density map estimation in aerial and ground imagery. R Bahmanyar, E Vig, P Reinartz, BMVCW1019R. Bahmanyar, E. Vig, and P. Reinartz, \"Mrcnet: Crowd counting and density map estimation in aerial and ground imagery,\" BMVCW, 2019. 10, 11, 19\n\nDronebased joint density map estimation, localization and tracking with space-time multi-scale attention network. L Wen, D Du, P Zhu, Q Hu, Q Wang, L Bo, S Lyu, arXiv:1912.018111011L. Wen, D. Du, P. Zhu, Q. Hu, Q. Wang, L. Bo, and S. Lyu, \"Drone- based joint density map estimation, localization and tracking with space-time multi-scale attention network,\" arXiv:1912.01811, 2019. 10, 11\n\nIntroduction to a large-scale general purpose ground truth database: methodology, annotation tool and benchmarks. B Yao, X Yang, S.-C Zhu, Springer1011B. Yao, X. Yang, and S.-C. Zhu, \"Introduction to a large-scale general purpose ground truth database: methodology, annotation tool and benchmarks,\" in EMMCVPRW. Springer, 2007, pp. 169-183. 10, 11\n\nPets2010: Dataset and challenge. A Ellis, J Ferryman, AVSS. 1011A. Ellis and J. Ferryman, \"Pets2010: Dataset and challenge,\" AVSS, pp. 143-150, 2010. 10, 11\n\nCrowd saliency detection via global similarity structure. M K Lim, V J Kok, C C Loy, C S Chan, ICPR. IEEE1011M. K. Lim, V. J. Kok, C. C. Loy, and C. S. Chan, \"Crowd saliency detection via global similarity structure,\" in ICPR. IEEE, 2014, pp. 3957-3962. 10, 11\n\nRealtime people counting from depth imagery of crowded environments. E Bondi, L Seidenari, A D Bagdanov, A Del Bimbo, AVSS. 1011IEEEE. Bondi, L. Seidenari, A. D. Bagdanov, and A. Del Bimbo, \"Real- time people counting from depth imagery of crowded environments,\" in AVSS. IEEE, 2014, pp. 337-342. 10, 11\n\nReal-time people counting for indoor scenes. J Luo, J Wang, H Xu, H Lu, Signal Processing. 12411J. Luo, J. Wang, H. Xu, and H. Lu, \"Real-time people counting for indoor scenes,\" Signal Processing, vol. 124, pp. 27-35, 2016. 10, 11\n\nCrossing-line crowd counting with two-phase deep neural networks. Z Zhao, H Li, R Zhao, X Wang, ECCV. Springer1011Z. Zhao, H. Li, R. Zhao, and X. Wang, \"Crossing-line crowd counting with two-phase deep neural networks,\" in ECCV. Springer, 2016, pp. 712-726. 10, 11\n\nDense crowd counting from still images with convolutional neural networks. Y Hu, H Chang, F Nian, Y Wang, T Li, 3811VCIRY. Hu, H. Chang, F. Nian, Y. Wang, and T. Li, \"Dense crowd counting from still images with convolutional neural networks,\" VCIR, vol. 38, pp. 530-539, 2016. 10, 11\n\nCounting people based on linear, weighted, and local random forests,\" in DICTA. H Farhood, X He, W Jia, M Blumenstein, H Li, IEEE1011H. Farhood, X. He, W. Jia, M. Blumenstein, and H. Li, \"Counting people based on linear, weighted, and local random forests,\" in DICTA. IEEE, 2017, pp. 1-7. 10, 11\n\nA double-region learning algorithm for counting the number of pedestrians in subway surveillance videos. G He, Q Chen, D Jiang, X Lu, Y Yuan, EAAI. 6411G. He, Q. Chen, D. Jiang, X. Lu, and Y. Yuan, \"A double-region learning algorithm for counting the number of pedestrians in subway surveillance videos,\" EAAI, vol. 64, pp. 302-314, 2017. 10, 11\n\nA deeply-recursive convolutional network for crowd counting. X Ding, Z Lin, F He, Y Wang, Y Huang, ICASSP. IEEE. 1011X. Ding, Z. Lin, F. He, Y. Wang, and Y. Huang, \"A deeply-recursive convolutional network for crowd counting,\" in ICASSP. IEEE, 2018, pp. 1942-1946. 10, 11\n\nCross-line pedestrian counting based on spatially-consistent two-stage local crowd density estimation and accumulation. H Zheng, Z Lin, J Cen, Z Wu, Y Zhao, T-CSVT. 29311H. Zheng, Z. Lin, J. Cen, Z. Wu, and Y. Zhao, \"Cross-line pedestrian counting based on spatially-consistent two-stage local crowd density estimation and accumulation,\" T-CSVT, vol. 29, no. 3, pp. 787-799, 2018. 10, 11\n\nOptical flow dataset and benchmark for visual crowd analysis. G Schr\u00f6der, T Senst, E Bochinski, T Sikora, AVSS. IEEE. 11G. Schr\u00f6der, T. Senst, E. Bochinski, and T. Sikora, \"Optical flow dataset and benchmark for visual crowd analysis,\" in AVSS. IEEE, 2018, pp. 1-6. 11\n\nDepth information guided crowd counting for complex crowd scenes. M Xu, Z Ge, X Jiang, G Cui, B Zhou, C Xu, PRL. 1011M. Xu, Z. Ge, X. Jiang, G. Cui, B. Zhou, C. Xu et al., \"Depth information guided crowd counting for complex crowd scenes,\" PRL, 2019. 10, 11\n\nIndoor crowd counting by mixture of gaussians label distribution learning. M Ling, X Geng, TIP. 281111M. Ling and X. Geng, \"Indoor crowd counting by mixture of gaussians label distribution learning,\" TIP, vol. 28, no. 11, pp. 5691-5701, 2019. 10, 11\n\nLearning multi-level density maps for crowd counting. X Jiang, L Zhang, P Lv, Y Guo, R Zhu, Y Li, Y Pang, X Li, B Zhou, M Xu, T-NNLS. 1015X. Jiang, L. Zhang, P. Lv, Y. Guo, R. Zhu, Y. Li, Y. Pang, X. Li, B. Zhou, and M. Xu, \"Learning multi-level density maps for crowd counting.\" T-NNLS, 2019. 10, 11, 15\n\nCccnet: An attention based deep learning framework for categorized crowd counting. S S S Das, S M Rashid, M E Ali, arXiv:1912.057651011arXiv preprintS. S. S. Das, S. M. Rashid, M. E. Ali et al., \"Cccnet: An attention based deep learning framework for categorized crowd counting,\" arXiv preprint arXiv:1912.05765, 2019. 10, 11\n\nPedestrian detection: An evaluation of the state of the art. P Dollar, C Wojek, B Schiele, P Perona, TPAMI. 34411P. Dollar, C. Wojek, B. Schiele, and P. Perona, \"Pedestrian detection: An evaluation of the state of the art,\" TPAMI, vol. 34, no. 4, pp. 743- 761, 2012. 10, 11\n\nWider face: A face detection benchmark. S Yang, P Luo, C.-C Loy, X Tang, CVPR. 1011S. Yang, P. Luo, C.-C. Loy, and X. Tang, \"Wider face: A face detection benchmark,\" in CVPR, 2016, pp. 5525-5533. 10, 11\n\nPerformance measures and a data set for multi-target, multi-camera tracking. E Ristani, F Solera, R Zou, R Cucchiara, C Tomasi, ECCV. Springer1011E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, \"Perfor- mance measures and a data set for multi-target, multi-camera tracking,\" in ECCV. Springer, 2016, pp. 17-35. 10, 11\n\nUnderstanding traffic density from large-scale web camera data. S Zhang, G Wu, J P Costeira, J M Moura, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1011S. Zhang, G. Wu, J. P. Costeira, and J. M. Moura, \"Understanding traffic density from large-scale web camera data,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5898-5907. 10, 11\n\nDrone-based object counting by spatially regularized regional proposal network. M.-R Hsieh, Y.-L Lin, W H Hsu, M.-R. Hsieh, Y.-L. Lin, and W. H. Hsu, \"Drone-based object counting by spatially regularized regional proposal network,\" in ICCV, 2017, pp. 4145-4153. 11\n\nTasselnet: counting maize tassels in the wild via local counts regression network. H Lu, Z Cao, Y Xiao, B Zhuang, C Shen, Plant methods. 13111H. Lu, Z. Cao, Y. Xiao, B. Zhuang, and C. Shen, \"Tasselnet: counting maize tassels in the wild via local counts regression network,\" Plant methods, vol. 13, no. 1, p. 79, 2017. 11\n\nUtilizing deep learning to predict the number of spikes in wheat (triticum aestivum). A Josuttes, S Aich, I Stavness, C Pozniak, S Shirtliffe, Phenome 2018 Posters. 511A. Josuttes, S. Aich, I. Stavness, C. Pozniak, and S. Shirtliffe, \"Utilizing deep learning to predict the number of spikes in wheat (triticum aestivum),\" Phenome 2018 Posters, vol. 5, p. 8, 2018. 11\n\nVision meets drones: A challenge. P Zhu, L Wen, X Bian, H Ling, Q Hu, arXiv:1804.0743711arXiv preprintP. Zhu, L. Wen, X. Bian, H. Ling, and Q. Hu, \"Vision meets drones: A challenge,\" arXiv preprint arXiv:1804.07437, 2018. 11\n\nCrowd counting on images with scale variation and isolated clusters. H Bai, S Wen, S.-H G Chan, ICCVW11H. Bai, S. Wen, and S.-H. G. Chan, \"Crowd counting on images with scale variation and isolated clusters,\" ICCVW, 2019. 11\n\nGeometric and physical constraints for drone-based head plane crowd density estimation. W Liu, K M Lis, M Salzmann, P Fua, IROS. 1219W. Liu, K. M. Lis, M. Salzmann, and P. Fua, \"Geometric and physical constraints for drone-based head plane crowd density estimation,\" in IROS, 2019. 12, 19\n\nDense scale network for crowd counting. F Dai, H Liu, Y Ma, J Cao, Q Zhao, Y Zhang, CoRR. 1415F. Dai, H. Liu, Y. Ma, J. Cao, Q. Zhao, and Y. Zhang, \"Dense scale network for crowd counting,\" CoRR, vol. abs/1906.09707, 2019. 13, 14, 15\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, CVPR. J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in CVPR, 2018, pp. 7132-7141. 13\n\nEnd-to-end instance segmentation with recurrent attention. M Ren, R S Zemel, CVPR. M. Ren and R. S. Zemel, \"End-to-end instance segmentation with recurrent attention,\" in CVPR, 2017, pp. 6656-6664. 13\n\nAttentive generative adversarial network for raindrop removal from a single image. R Qian, R T Tan, W Yang, J Su, J Liu, CVPR. R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, \"Attentive generative adversarial network for raindrop removal from a single image,\" in CVPR, 2018, pp. 2482-2491. 13\n\nMulti-context attention for human pose estimation. X Chu, W Yang, W Ouyang, C Ma, A L Yuille, X Wang, X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang, \"Multi-context attention for human pose estimation,\" in CVPR, 2017, pp. 1831-1840. 13\n\nMulti-scale context aggregation by dilated convolutions. F Yu, V Koltun, arXiv:1511.07122arXiv preprintF. Yu and V. Koltun, \"Multi-scale context aggregation by dilated convolutions,\" arXiv preprint arXiv:1511.07122, 2015. 13\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, TPAMI. 404L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, \"Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,\" TPAMI, vol. 40, no. 4, pp. 834-848, 2018. 13\n\nRethinking atrous convolution for semantic image segmentation. L.-C Chen, G Papandreou, F Schroff, H Adam, arXiv:1706.05587arXiv preprintL.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \"Rethinking atrous convolution for semantic image segmentation,\" arXiv preprint arXiv:1706.05587, 2017. 13\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. J. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" in CVPR, 2015, pp. 3431-3440. 13\n\nDeformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, 1314J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, \"Deformable convolutional networks,\" in ICCV, 2017, pp. 764-773. 13, 14\n\nMulti-label image recognition by recurrently discovering attentional regions. Z Wang, T Chen, G Li, R Xu, L Lin, Z. Wang, T. Chen, G. Li, R. Xu, and L. Lin, \"Multi-label image recognition by recurrently discovering attentional regions,\" in ICCV, 2017, pp. 464-472. 13\n\nRecurrent attentional networks for saliency detection. J Kuen, Z Wang, G Wang, CVPR. J. Kuen, Z. Wang, and G. Wang, \"Recurrent attentional networks for saliency detection,\" in CVPR, 2016, pp. 3668-3677. 13\n\nConditional random fields: Probabilistic models for segmenting and labeling sequence data. J Lafferty, A Mccallum, F C Pereira, J. Lafferty, A. McCallum, and F. C. Pereira, \"Conditional random fields: Probabilistic models for segmenting and labeling sequence data,\" 2001. 13\n\nMarkov random field models in computer vision. S Z Li, SpringerS. Z. Li, \"Markov random field models in computer vision,\" in European conference on computer vision. Springer, 1994, pp. 361- 370. 13\n\nEfficient inference in fully connected crfs with gaussian edge potentials. P Kr\u00e4henb\u00fchl, V Koltun, NIPS. P. Kr\u00e4henb\u00fchl and V. Koltun, \"Efficient inference in fully connected crfs with gaussian edge potentials,\" in NIPS, 2011, pp. 109-117. 13\n\nCrowd counting with deep structured scale integration network. L Liu, Z Qiu, G Li, S Liu, W Ouyang, L Lin, ICCV. 1316L. Liu, Z. Qiu, G. Li, S. Liu, W. Ouyang, and L. Lin, \"Crowd counting with deep structured scale integration network,\" in ICCV, 2019. 13, 14, 15, 16\n\nAttentional neural fields for crowd counting. A Zhang, L Yue, J Shen, F Zhu, X Zhen, X Cao, L Shao, ICCV. 1319A. Zhang, L. Yue, J. Shen, F. Zhu, X. Zhen, X. Cao, and L. Shao, \"Attentional neural fields for crowd counting,\" in ICCV, 2019, pp. 5714-5713. 13, 14, 18, 19\n\nNon-local neural networks. X Wang, R Girshick, A Gupta, K He, CVPR. X. Wang, R. Girshick, A. Gupta, and K. He, \"Non-local neural networks,\" in CVPR, 2018, pp. 7794-7803. 13\n\nComplete solution classification for the perspective-three-point problem. X.-S Gao, X.-R Hou, J Tang, H.-F Cheng, TPAMI. 258X.-S. Gao, X.-R. Hou, J. Tang, and H.-F. Cheng, \"Complete solution classification for the perspective-three-point problem,\" TPAMI, vol. 25, no. 8, pp. 930-943, 2003. 13\n\nCounting people with low-level features and bayesian regression. A B Chan, N Vasconcelos, TIP. 214A. B. Chan and N. Vasconcelos, \"Counting people with low-level features and bayesian regression,\" TIP, vol. 21, no. 4, pp. 2160-2177, 2012. 13\n\nBody structure aware deep crowd counting. S Huang, X Li, Z Zhang, F Wu, S Gao, R Ji, J Han, TIP. 27319S. Huang, X. Li, Z. Zhang, F. Wu, S. Gao, R. Ji, and J. Han, \"Body structure aware deep crowd counting,\" TIP, vol. 27, no. 3, pp. 1049- 1059, 2018. 13, 19\n\nSpatial pyramid pooling in deep convolutional networks for visual recognition. K He, X Zhang, S Ren, J Sun, TPAMI. 379K. He, X. Zhang, S. Ren, and J. Sun, \"Spatial pyramid pooling in deep convolutional networks for visual recognition,\" TPAMI, vol. 37, no. 9, pp. 1904-1916, 2015. 13\n\nIn defense of single-column networks for crowd counting. Z Wang, Z Xiao, K Xie, Q Qiu, X Zhen, X Cao, arXiv:1808.061331319arXiv preprintZ. Wang, Z. Xiao, K. Xie, Q. Qiu, X. Zhen, and X. Cao, \"In defense of single-column networks for crowd counting,\" arXiv preprint arXiv:1808.06133, 2018. 13, 14, 15, 19\n\nMulti-scale convolutional neural networks for crowd counting. L Zeng, X Xu, B Cai, S Qiu, T Zhang, ICIP. 14IEEEL. Zeng, X. Xu, B. Cai, S. Qiu, and T. Zhang, \"Multi-scale convolu- tional neural networks for crowd counting,\" in ICIP. IEEE, 2017, pp. 465-469. 14\n\nMask-aware networks for crowd counting. S Jiang, X Lu, Y Lei, L Liu, TCSVT. 14S. Jiang, X. Lu, Y. Lei, and L. Liu, \"Mask-aware networks for crowd counting,\" TCSVT, 2019. 14\n\nCrowd counting with decomposed uncertainty. M Oh, P A Olsen, K N Ramamurthy, AAAI. 14M.-h. Oh, P. A. Olsen, and K. N. Ramamurthy, \"Crowd counting with decomposed uncertainty,\" AAAI, 2019. 14\n\nCrowd transformer network. R Viresh, S Mubarak, H N Minh, arXiv:1904.02774v114arXiv preprintR. Viresh, S. Mubarak, and H. N. Minh, \"Crowd transformer network,\" arXiv preprint arXiv:1904.02774v1, 2019. 14\n\nDenet: A universal network for counting crowd with varying densities and scales. L Liu, J Jiang, W Jia, S Amirgholipour, M Zeibots, X He, Neurocomputing. 14L. Liu, J. Jiang, W. Jia, S. Amirgholipour, M. Zeibots, and X. He, \"Denet: A universal network for counting crowd with varying densities and scales,\" Neurocomputing, 2019. 14\n\nInverse attention guided deep crowd counting network. V A Sindagi, V M Patel, AVSS. 14V. A. Sindagi and V. M. Patel, \"Inverse attention guided deep crowd counting network,\" AVSS, 2019. 14\n\nBayesian loss for crowd count estimation with point supervision. Z Ma, X Wei, X Hong, Y Gong, ICCV. 1416Z. Ma, X. Wei, X. Hong, and Y. Gong, \"Bayesian loss for crowd count estimation with point supervision,\" in ICCV, 2019, pp. 6142-6151. 14, 15, 16\n\nLearning spatial awareness to improve crowd counting. Z.-Q Cheng, J.-X Li, Q Dai, X Wu, A G Hauptmann, ICCV. 1516Z.-Q. Cheng, J.-X. Li, Q. Dai, X. Wu, and A. G. Hauptmann, \"Learning spatial awareness to improve crowd counting,\" ICCV, 2019. 14, 15, 16\n\nMulti-level bottom-top and top-bottom feature fusion for crowd counting. V A Sindagi, V M Patel, ICCV. 1415V. A. Sindagi and V. M. Patel, \"Multi-level bottom-top and top-bottom feature fusion for crowd counting,\" ICCV, 2019. 14, 15\n\nFrom open set to closed set: Counting objects by spatial divide-and-conquer. H Xiong, H Lu, C Liu, L Liang, Z Cao, C Shen, ICCV. 1415H. Xiong, H. Lu, C. Liu, L. Liang, Z. Cao, and C. Shen, \"From open set to closed set: Counting objects by spatial divide-and-conquer,\" in ICCV, 2019. 14, 15\n\nAtrous convolutions spatial pyramid network for crowd counting and density estimation. J Ma, Y Dai, Y.-P Tan, Neurocomputing. 35014J. Ma, Y. Dai, and Y.-P. Tan, \"Atrous convolutions spatial pyramid network for crowd counting and density estimation.\" Neurocomputing, vol. 350, pp. 91-101, 2019. 14\n\nCrowd counting with crowd attention convolutional neural network. J Chen, S Wen, Z Wang, Neurocomputing. 15J. Chen, S. Wen, and Z. Wang, \"Crowd counting with crowd attention convolutional neural network,\" Neurocomputing, 2019. 15\n\nCrowd counting via multi-layer regression. X Tan, C Tao, T R Tang, G Wu, ACMMM. ACM15X. Tan, C. Tao, T. R. andJinhui Tang, and G. Wu, \"Crowd counting via multi-layer regression,\" in ACMMM. ACM, 2019, pp. 1907-1915. 15\n\nContentaware density map for crowd counting and density estimation. M M Oghaz, A R Khadka, V Argyriou, P Remagnino, arXiv:1906.0725816arXiv preprintM. M. Oghaz, A. R. Khadka, V. Argyriou, and P. Remagnino, \"Content- aware density map for crowd counting and density estimation,\" arXiv preprint arXiv:1906.07258, 2019. 16\n\nImproving dense crowd counting convolutional neural networks using inverse k-nearest neighbor maps and multiscale upsampling. G Olmschenk, H Tang, Z Zhu, arXiv:1902.0537916arXiv preprintG. Olmschenk, H. Tang, and Z. Zhu, \"Improving dense crowd counting convolutional neural networks using inverse k-nearest neighbor maps and multiscale upsampling,\" arXiv preprint arXiv:1902.05379, 2019. 16\n\nAdaptive density map generation for crowd counting. J Wan, A Chan, ICCV. 16J. Wan and A. Chan, \"Adaptive density map generation for crowd counting,\" in ICCV, 2019, pp. 1130-1139. 16\n\nRobust optimization for deep regression. V Belagiannis, C Rupprecht, G Carneiro, N Navab, ICCV. 16V. Belagiannis, C. Rupprecht, G. Carneiro, and N. Navab, \"Robust optimization for deep regression,\" in ICCV, 2015, pp. 2830-2838. 16\n\nMulti-scale generative adversarial networks for crowd counting. J Yang, Y Zhou, S.-Y Kung, ICPR. IEEE16J. Yang, Y. Zhou, and S.-Y. Kung, \"Multi-scale generative adversarial networks for crowd counting,\" in ICPR. IEEE, 2018, pp. 3244-3249. 16\n\nCrowd counting via multi-view scale aggregation networks. Z Qiu, L Liu, G Li, Q Wang, N Xiao, L Lin, ICME. IEEE1618Z. Qiu, L. Liu, G. Li, Q. Wang, N. Xiao, and L. Lin, \"Crowd counting via multi-view scale aggregation networks,\" in ICME. IEEE, 2019, pp. 1498-1503. 16, 18\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.15561719arXiv preprintK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014. 17, 19\n\nSegmentation guided attention network for crowd counting via curriculum learning. Q Wang, T P Breckon, arXiv:1911.0799017arXiv preprintQ. Wang and T. P. Breckon, \"Segmentation guided attention net- work for crowd counting via curriculum learning,\" arXiv preprint arXiv:1911.07990, 2019. 17\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. 17C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethink- ing the inception architecture for computer vision,\" in CVPR, 2016, pp. 2818-2826. 17\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. 1719K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in CVPR, 2016, pp. 770-778. 17, 19\n\nEffective use of convolutional neural networks and diverse deep supervision for better crowd counting. H Jiang, W Jin, Applied Intelligence. 18H. Jiang and W. Jin, \"Effective use of convolutional neural networks and diverse deep supervision for better crowd counting,\" Applied Intelligence, pp. 1-19, 2019. 18\n\nScar: Spatial-/channel-wise attention regression networks for crowd counting. J Gao, Q Wang, Y Yuan, Neurocomputing. 36319J. Gao, Q. Wang, and Y. Yuan, \"Scar: Spatial-/channel-wise attention regression networks for crowd counting,\" Neurocomputing, vol. 363, pp. 1-8, 2019. 18, 19\n\nC 3 framework: An open-source pytorch code for crowd counting. J Gao, W Lin, B Zhao, D Wang, C Gao, J Wen, arXiv:1907.027241819arXiv preprintJ. Gao, W. Lin, B. Zhao, D. Wang, C. Gao, and J. Wen, \"C 3 framework: An open-source pytorch code for crowd counting,\" arXiv preprint arXiv:1907.02724, 2019. 18, 19\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. IEEE19J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in CVPR. IEEE, 2009, pp. 248-255. 19\n\nVideo crowd counting via dynamic temporal modeling. X Wu, B Xu, Y Zheng, H Ye, J Yang, L He, arXiv:1907.0219819arXiv preprintX. Wu, B. Xu, Y. Zheng, H. Ye, J. Yang, and L. He, \"Video crowd counting via dynamic temporal modeling,\" arXiv preprint arXiv:1907.02198, 2019. 19\n\nCounting challenging crowds robustly using a multi-column multi-task convolutional neural network. B Yang, J Cao, N Wang, Y Zhang, L Zou, SPIC. 6419B. Yang, J. Cao, N. Wang, Y. Zhang, and L. Zou, \"Counting challenging crowds robustly using a multi-column multi-task convolutional neural network,\" SPIC, vol. 64, pp. 118-129, 2018. 19\n\nCounting crowds using a scale-distribution-aware network and adaptive human-shaped kernel. B Yang, W Zhan, N Wang, X Liu, J Lv, Neurocomputing. B. Yang, W. Zhan, N. Wang, X. Liu, and J. Lv, \"Counting crowds using a scale-distribution-aware network and adaptive human-shaped kernel,\" Neurocomputing, 2019. 19\n\nEnhanced 3d convolutional networks for crowd counting. Z Zou, H Shao, X Qu, W Wei, P Zhou, BMVC. Z. Zou, H. Shao, X. Qu, W. Wei, and P. Zhou, \"Enhanced 3d convolutional networks for crowd counting,\" BMVC, 2019. 19\n\nSpatiotemporal modeling for crowd counting in videos. F Xiong, X Shi, D.-Y Yeung, 19F. Xiong, X. Shi, and D.-Y. Yeung, \"Spatiotemporal modeling for crowd counting in videos,\" in ICCV, 2017, pp. 5151-5159. 19\n\nDynamic region division for adaptive learning pedestrian counting. G He, Z Ma, B Huang, B Sheng, Y Yuan, ICME. IEEE19G. He, Z. Ma, B. Huang, B. Sheng, and Y. Yuan, \"Dynamic region division for adaptive learning pedestrian counting,\" in ICME. IEEE, 2019, pp. 1120-1125. 19\n\nEstimating people flows to better count them in crowded scenes. W Liu, M Salzmann, P Fua, arXiv:1911.1078219arXiv preprintW. Liu, M. Salzmann, and P. Fua, \"Estimating people flows to better count them in crowded scenes,\" arXiv preprint arXiv:1911.10782, 2019. 19\n\n3d crowd counting via multi-view fusion with 3d gaussian kernels. Q Zhang, A B Chan, 2020. 20AAAI. Q. Zhang and A. B. Chan, \"3d crowd counting via multi-view fusion with 3d gaussian kernels,\" in AAAI, 2020. 20\n\nWhere are the blobs: Counting by localization with point supervision. I H Laradji, N Rostamzadeh, P O Pinheiro, D Vazquez, M Schmidt, in ECCV. 20I. H. Laradji, N. Rostamzadeh, P. O. Pinheiro, D. Vazquez, and M. Schmidt, \"Where are the blobs: Counting by localization with point supervision,\" in ECCV, 2018, pp. 547-562. 20\n\n20 PLACE PHOTO HERE Guangshuai Gao received the B.Sc. degree in applied physics from college of science and the M.Sc. degree in signal and information processing from the School of Electronic and Information Engineering. G Gao, Q Liu, Y Wang, respectively. He is currently pursuing the Ph.D. degree with the Laboratory of Intelligent Recognition and Image Processing. ICASSP. IEEEZhengzhou, ChinaZhongyuan University of Technology ; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang UniversityDense object counting in remote sensing images. His research interests include image processing, pattern recognition, and digital machine learningG. Gao, Q. Liu, and Y. Wang, \"Dense object counting in remote sensing images,\" in ICASSP. IEEE, 2020. 20 PLACE PHOTO HERE Guangshuai Gao received the B.Sc. degree in ap- plied physics from college of science and the M.Sc. degree in signal and information processing from the School of Electronic and Information Engineer- ing, from the Zhongyuan University of Technology, Zhengzhou, China, in 2014 and 2017, respectively. He is currently pursuing the Ph.D. degree with the Laboratory of Intelligent Recognition and Image Processing, Beijing Key Laboratory of Digital Me- dia, School of Computer Science and Engineering, Beihang University. His research interests include image processing, pattern recognition, and digital machine learning.\n", "annotations": {"author": "[{\"end\":76,\"start\":61},{\"end\":87,\"start\":77},{\"end\":120,\"start\":88},{\"end\":141,\"start\":121},{\"end\":174,\"start\":142}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":72},{\"end\":86,\"start\":83},{\"end\":119,\"start\":116},{\"end\":140,\"start\":136},{\"end\":173,\"start\":169}]", "author_first_name": "[{\"end\":71,\"start\":61},{\"end\":82,\"start\":77},{\"end\":115,\"start\":108},{\"end\":135,\"start\":133},{\"end\":168,\"start\":161}]", "author_affiliation": null, "title": "[{\"end\":58,\"start\":1},{\"end\":232,\"start\":175}]", "venue": null, "abstract": "[{\"end\":2104,\"start\":298}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2440,\"start\":2437},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2446,\"start\":2442},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2468,\"start\":2464},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2474,\"start\":2470},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2488,\"start\":2484},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2502,\"start\":2499},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2508,\"start\":2504},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2514,\"start\":2510},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2527,\"start\":2523},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2533,\"start\":2529},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2561,\"start\":2557},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2567,\"start\":2563},{\"end\":2665,\"start\":2655},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2764,\"start\":2760},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2770,\"start\":2766},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2798,\"start\":2794},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4490,\"start\":4486},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5153,\"start\":5149},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5159,\"start\":5155},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5363,\"start\":5359},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5369,\"start\":5365},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5380,\"start\":5376},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5394,\"start\":5390},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5674,\"start\":5670},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5680,\"start\":5676},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5686,\"start\":5682},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5836,\"start\":5832},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5895,\"start\":5891},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5906,\"start\":5902},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5916,\"start\":5912},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5926,\"start\":5922},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5937,\"start\":5933},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6002,\"start\":5998},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6039,\"start\":6035},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6272,\"start\":6268},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6465,\"start\":6461},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7150,\"start\":7146},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7156,\"start\":7152},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7162,\"start\":7158},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8781,\"start\":8777},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8804,\"start\":8800},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8858,\"start\":8854},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8950,\"start\":8946},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9018,\"start\":9014},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9148,\"start\":9144},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9273,\"start\":9269},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9350,\"start\":9346},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9567,\"start\":9563},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9834,\"start\":9830},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11134,\"start\":11130},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11152,\"start\":11148},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11162,\"start\":11159},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11177,\"start\":11174},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11189,\"start\":11186},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11200,\"start\":11196},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11219,\"start\":11216},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11232,\"start\":11228},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11244,\"start\":11240},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11256,\"start\":11252},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11273,\"start\":11269},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13083,\"start\":13079},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":13364,\"start\":13360},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":13521,\"start\":13517},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13740,\"start\":13739},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":13817,\"start\":13813},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":14092,\"start\":14088},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":14382,\"start\":14378},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":14694,\"start\":14690},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":14931,\"start\":14927},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":15151,\"start\":15147},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16422,\"start\":16418},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16662,\"start\":16658},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":16721,\"start\":16716},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16888,\"start\":16884},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17771,\"start\":17768},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17942,\"start\":17939},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18178,\"start\":18175},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18309,\"start\":18306},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18457,\"start\":18454},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":18614,\"start\":18609},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":18687,\"start\":18682},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":18789,\"start\":18785},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":18911,\"start\":18906},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18924,\"start\":18921},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":18964,\"start\":18959},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18979,\"start\":18976},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":19140,\"start\":19136},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":19432,\"start\":19427},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":19725,\"start\":19720},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20131,\"start\":20127},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":21660,\"start\":21655},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21829,\"start\":21826},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":22070,\"start\":22066},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22300,\"start\":22296},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22431,\"start\":22427},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":22488,\"start\":22483},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":22644,\"start\":22640},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":22878,\"start\":22874},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23225,\"start\":23222},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":23296,\"start\":23292},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":23323,\"start\":23318},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":23529,\"start\":23525},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":24351,\"start\":24346},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25153,\"start\":25149},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25398,\"start\":25395},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":25849,\"start\":25845},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":26082,\"start\":26078},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":26328,\"start\":26324},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":26569,\"start\":26565},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":26832,\"start\":26828},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":27194,\"start\":27190},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":27514,\"start\":27510},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":27872,\"start\":27868},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":28038,\"start\":28033},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":28164,\"start\":28160},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":28171,\"start\":28166},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29027,\"start\":29023},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29369,\"start\":29366},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":29750,\"start\":29745},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":29919,\"start\":29915},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":30132,\"start\":30128},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":30139,\"start\":30134},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":30458,\"start\":30453},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":31125,\"start\":31121},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":31357,\"start\":31352},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":32820,\"start\":32816},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":32899,\"start\":32894},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":33023,\"start\":33019},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":33179,\"start\":33175},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":33313,\"start\":33309},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":33361,\"start\":33356},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":33534,\"start\":33529},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":33817,\"start\":33813},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":33823,\"start\":33819},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":34209,\"start\":34205},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":34392,\"start\":34388},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":34662,\"start\":34657},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":34858,\"start\":34853},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":35252,\"start\":35247},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":35584,\"start\":35579},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":35875,\"start\":35871},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":35978,\"start\":35973},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":36101,\"start\":36096},{\"attributes\":{\"ref_id\":\"b126\"},\"end\":36367,\"start\":36362},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":36635,\"start\":36630},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":37607,\"start\":37602},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":37621,\"start\":37616},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38772,\"start\":38768},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38783,\"start\":38779},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":38799,\"start\":38795},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":38818,\"start\":38814},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38837,\"start\":38834},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38888,\"start\":38884},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":39313,\"start\":39309},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39539,\"start\":39535},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":39873,\"start\":39869},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41592,\"start\":41589},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":42336,\"start\":42331},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":42534,\"start\":42529},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":42810,\"start\":42805},{\"attributes\":{\"ref_id\":\"b133\"},\"end\":43417,\"start\":43412},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":43715,\"start\":43710},{\"attributes\":{\"ref_id\":\"b135\"},\"end\":44007,\"start\":44002},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":44429,\"start\":44425},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":44860,\"start\":44856},{\"attributes\":{\"ref_id\":\"b136\"},\"end\":45355,\"start\":45350},{\"attributes\":{\"ref_id\":\"b141\"},\"end\":45362,\"start\":45357},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":45391,\"start\":45386},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":45404,\"start\":45400},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":45437,\"start\":45432},{\"attributes\":{\"ref_id\":\"b142\"},\"end\":45444,\"start\":45439},{\"attributes\":{\"ref_id\":\"b148\"},\"end\":45458,\"start\":45453},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":45471,\"start\":45467},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":45492,\"start\":45487},{\"attributes\":{\"ref_id\":\"b140\"},\"end\":45508,\"start\":45503},{\"attributes\":{\"ref_id\":\"b149\"},\"end\":45524,\"start\":45519},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":45550,\"start\":45545},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":45561,\"start\":45556},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":45609,\"start\":45604},{\"attributes\":{\"ref_id\":\"b145\"},\"end\":45634,\"start\":45629},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":45637,\"start\":45635},{\"attributes\":{\"ref_id\":\"b146\"},\"end\":45658,\"start\":45653},{\"attributes\":{\"ref_id\":\"b150\"},\"end\":45692,\"start\":45687},{\"attributes\":{\"ref_id\":\"b151\"},\"end\":45712,\"start\":45707},{\"attributes\":{\"ref_id\":\"b152\"},\"end\":46015,\"start\":46010},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":46210,\"start\":46206},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":46398,\"start\":46394},{\"attributes\":{\"ref_id\":\"b153\"},\"end\":46562,\"start\":46557},{\"attributes\":{\"ref_id\":\"b154\"},\"end\":46722,\"start\":46717},{\"attributes\":{\"ref_id\":\"b155\"},\"end\":46908,\"start\":46903},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":48126,\"start\":48122},{\"attributes\":{\"ref_id\":\"b161\"},\"end\":48576,\"start\":48571},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":49167,\"start\":49163},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":49658,\"start\":49653},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":50393,\"start\":50388},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":51365,\"start\":51363},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":52109,\"start\":52105},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":52316,\"start\":52314},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":52815,\"start\":52812},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":52831,\"start\":52827},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":53357,\"start\":53354},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":53362,\"start\":53359},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":53368,\"start\":53364},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":53374,\"start\":53370},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":53380,\"start\":53376},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":53386,\"start\":53382},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":53392,\"start\":53388},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":53398,\"start\":53394},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":53434,\"start\":53430},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":53440,\"start\":53436},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":53446,\"start\":53442},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":53452,\"start\":53448},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":53458,\"start\":53454},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":53464,\"start\":53460},{\"attributes\":{\"ref_id\":\"b162\"},\"end\":53471,\"start\":53466},{\"attributes\":{\"ref_id\":\"b163\"},\"end\":53892,\"start\":53887},{\"attributes\":{\"ref_id\":\"b164\"},\"end\":53921,\"start\":53916},{\"attributes\":{\"ref_id\":\"b165\"},\"end\":53945,\"start\":53940},{\"attributes\":{\"ref_id\":\"b166\"},\"end\":53979,\"start\":53974},{\"attributes\":{\"ref_id\":\"b167\"},\"end\":54313,\"start\":54308},{\"attributes\":{\"ref_id\":\"b169\"},\"end\":54320,\"start\":54315},{\"attributes\":{\"ref_id\":\"b170\"},\"end\":54641,\"start\":54636},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":54855,\"start\":54850},{\"attributes\":{\"ref_id\":\"b171\"},\"end\":54888,\"start\":54883},{\"attributes\":{\"ref_id\":\"b172\"},\"end\":55518,\"start\":55513},{\"attributes\":{\"ref_id\":\"b173\"},\"end\":55547,\"start\":55542},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":55594,\"start\":55590},{\"attributes\":{\"ref_id\":\"b174\"},\"end\":55697,\"start\":55692},{\"attributes\":{\"ref_id\":\"b175\"},\"end\":55734,\"start\":55729},{\"attributes\":{\"ref_id\":\"b176\"},\"end\":55879,\"start\":55874},{\"attributes\":{\"ref_id\":\"b177\"},\"end\":55901,\"start\":55896},{\"attributes\":{\"ref_id\":\"b178\"},\"end\":56078,\"start\":56073},{\"attributes\":{\"ref_id\":\"b179\"},\"end\":56183,\"start\":56178},{\"attributes\":{\"ref_id\":\"b180\"},\"end\":56416,\"start\":56411},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":56657,\"start\":56653},{\"attributes\":{\"ref_id\":\"b181\"},\"end\":56664,\"start\":56659},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":56858,\"start\":56855},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":56864,\"start\":56860},{\"attributes\":{\"ref_id\":\"b182\"},\"end\":56888,\"start\":56883},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":56999,\"start\":56995},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":57005,\"start\":57001},{\"attributes\":{\"ref_id\":\"b183\"},\"end\":57103,\"start\":57098},{\"attributes\":{\"ref_id\":\"b184\"},\"end\":57583,\"start\":57578},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":57596,\"start\":57592},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":57609,\"start\":57605},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":58072,\"start\":58069},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":58088,\"start\":58085},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":58103,\"start\":58100},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":58223,\"start\":58219},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":58888,\"start\":58884},{\"end\":59313,\"start\":59306},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":59374,\"start\":59370},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":59449,\"start\":59445},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":59592,\"start\":59589},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":59598,\"start\":59594},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":59604,\"start\":59600},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":59610,\"start\":59606},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":59751,\"start\":59747},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":59758,\"start\":59753},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":59790,\"start\":59786},{\"attributes\":{\"ref_id\":\"b171\"},\"end\":59797,\"start\":59792},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":59988,\"start\":59984},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":60128,\"start\":60125},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":60134,\"start\":60130},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":60140,\"start\":60136},{\"attributes\":{\"ref_id\":\"b162\"},\"end\":60388,\"start\":60383},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":60398,\"start\":60394},{\"attributes\":{\"ref_id\":\"b177\"},\"end\":60412,\"start\":60407},{\"attributes\":{\"ref_id\":\"b192\"},\"end\":60425,\"start\":60420},{\"attributes\":{\"ref_id\":\"b193\"},\"end\":60443,\"start\":60438},{\"attributes\":{\"ref_id\":\"b191\"},\"end\":60452,\"start\":60447},{\"attributes\":{\"ref_id\":\"b194\"},\"end\":60466,\"start\":60461},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":60479,\"start\":60474},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":61669,\"start\":61666},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":61674,\"start\":61671},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":61680,\"start\":61676},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":61686,\"start\":61682},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":61692,\"start\":61688},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":61698,\"start\":61694},{\"attributes\":{\"ref_id\":\"b196\"},\"end\":61705,\"start\":61700},{\"attributes\":{\"ref_id\":\"b197\"},\"end\":62325,\"start\":62320},{\"attributes\":{\"ref_id\":\"b150\"},\"end\":62439,\"start\":62434},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":62705,\"start\":62701},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":63099,\"start\":63095},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":64235,\"start\":64231},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":64494,\"start\":64490},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":64779,\"start\":64776},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":65190,\"start\":65185},{\"attributes\":{\"ref_id\":\"b198\"},\"end\":65358,\"start\":65353},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":65664,\"start\":65659},{\"attributes\":{\"ref_id\":\"b191\"},\"end\":66123,\"start\":66118},{\"attributes\":{\"ref_id\":\"b199\"},\"end\":66264,\"start\":66259},{\"attributes\":{\"ref_id\":\"b200\"},\"end\":66503,\"start\":66498},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":67343,\"start\":67339},{\"attributes\":{\"ref_id\":\"b201\"},\"end\":67364,\"start\":67359},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":67433,\"start\":67428},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":67469,\"start\":67464},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":67548,\"start\":67545},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":67554,\"start\":67550},{\"attributes\":{\"ref_id\":\"b202\"},\"end\":67561,\"start\":67556},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":67823,\"start\":67819},{\"attributes\":{\"ref_id\":\"b177\"},\"end\":67983,\"start\":67978},{\"attributes\":{\"ref_id\":\"b203\"},\"end\":67990,\"start\":67985},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":68234,\"start\":68230},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":68357,\"start\":68353},{\"attributes\":{\"ref_id\":\"b192\"},\"end\":68500,\"start\":68495},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":68948,\"start\":68945},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":68954,\"start\":68950},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":69028,\"start\":69025},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":69034,\"start\":69030},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":69040,\"start\":69036},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":69195,\"start\":69191},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":69723,\"start\":69719},{\"attributes\":{\"ref_id\":\"b204\"},\"end\":69806,\"start\":69801},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":69890,\"start\":69886},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":69949,\"start\":69944},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":70025,\"start\":70021},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":70056,\"start\":70051},{\"attributes\":{\"ref_id\":\"b204\"},\"end\":70083,\"start\":70078},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":70172,\"start\":70168},{\"attributes\":{\"ref_id\":\"b205\"},\"end\":70243,\"start\":70238},{\"attributes\":{\"ref_id\":\"b206\"},\"end\":70296,\"start\":70291},{\"attributes\":{\"ref_id\":\"b204\"},\"end\":70361,\"start\":70356},{\"attributes\":{\"ref_id\":\"b207\"},\"end\":70435,\"start\":70430},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":70455,\"start\":70450},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":70504,\"start\":70500},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":70979,\"start\":70975},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":70993,\"start\":70989},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":71169,\"start\":71165},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":71180,\"start\":71177},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":71195,\"start\":71191},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":71323,\"start\":71319},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":71413,\"start\":71410},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":71509,\"start\":71505},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":71642,\"start\":71638},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":71881,\"start\":71877},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":72086,\"start\":72082},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":72623,\"start\":72618},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":72960,\"start\":72956},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":72989,\"start\":72986},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":73848,\"start\":73844},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":74028,\"start\":74024},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":74706,\"start\":74703},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":75042,\"start\":75037},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":75121,\"start\":75117},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":75254,\"start\":75251},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":75286,\"start\":75281},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":75994,\"start\":75989},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":76127,\"start\":76123},{\"attributes\":{\"ref_id\":\"b133\"},\"end\":76719,\"start\":76714},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":76766,\"start\":76762},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":76940,\"start\":76936},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":77493,\"start\":77489},{\"attributes\":{\"ref_id\":\"b210\"},\"end\":77508,\"start\":77503},{\"attributes\":{\"ref_id\":\"b209\"},\"end\":77523,\"start\":77518},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":77576,\"start\":77572},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":78178,\"start\":78173},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":78360,\"start\":78355},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":78879,\"start\":78875},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":79324,\"start\":79320},{\"attributes\":{\"ref_id\":\"b184\"},\"end\":79340,\"start\":79335},{\"attributes\":{\"ref_id\":\"b204\"},\"end\":79372,\"start\":79367},{\"attributes\":{\"ref_id\":\"b207\"},\"end\":79388,\"start\":79383},{\"attributes\":{\"ref_id\":\"b211\"},\"end\":79418,\"start\":79413},{\"attributes\":{\"ref_id\":\"b212\"},\"end\":79967,\"start\":79962},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":80064,\"start\":80061},{\"attributes\":{\"ref_id\":\"b212\"},\"end\":80114,\"start\":80109},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":80558,\"start\":80555},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":80577,\"start\":80574},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":80600,\"start\":80597},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":80617,\"start\":80614},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":80634,\"start\":80630},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":80652,\"start\":80648},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":80668,\"start\":80664},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":80681,\"start\":80677},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":80698,\"start\":80694},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":80716,\"start\":80712},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":80734,\"start\":80730},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":80754,\"start\":80750},{\"attributes\":{\"ref_id\":\"b182\"},\"end\":80771,\"start\":80766},{\"attributes\":{\"ref_id\":\"b213\"},\"end\":80788,\"start\":80783},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":80806,\"start\":80801},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":80820,\"start\":80816},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":80838,\"start\":80834},{\"attributes\":{\"ref_id\":\"b178\"},\"end\":80853,\"start\":80848},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":80870,\"start\":80865},{\"attributes\":{\"ref_id\":\"b214\"},\"end\":80890,\"start\":80885},{\"attributes\":{\"ref_id\":\"b212\"},\"end\":80905,\"start\":80900},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":81036,\"start\":81032},{\"attributes\":{\"ref_id\":\"b161\"},\"end\":81043,\"start\":81038},{\"attributes\":{\"ref_id\":\"b212\"},\"end\":81050,\"start\":81045},{\"attributes\":{\"ref_id\":\"b215\"},\"end\":81057,\"start\":81052},{\"attributes\":{\"ref_id\":\"b218\"},\"end\":81064,\"start\":81059},{\"attributes\":{\"ref_id\":\"b216\"},\"end\":81247,\"start\":81242},{\"attributes\":{\"ref_id\":\"b161\"},\"end\":81336,\"start\":81331},{\"attributes\":{\"ref_id\":\"b218\"},\"end\":81565,\"start\":81560},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":82344,\"start\":82339},{\"attributes\":{\"ref_id\":\"b219\"},\"end\":82490,\"start\":82485},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":82594,\"start\":82589},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":83495,\"start\":83492},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":83793,\"start\":83789},{\"attributes\":{\"ref_id\":\"b220\"},\"end\":83895,\"start\":83890},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":84014,\"start\":84010},{\"attributes\":{\"ref_id\":\"b220\"},\"end\":84030,\"start\":84025},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":84093,\"start\":84089},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":84181,\"start\":84177},{\"attributes\":{\"ref_id\":\"b221\"},\"end\":84799,\"start\":84794}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":86638,\"start\":86560},{\"attributes\":{\"id\":\"fig_1\"},\"end\":86981,\"start\":86639},{\"attributes\":{\"id\":\"fig_2\"},\"end\":87026,\"start\":86982},{\"attributes\":{\"id\":\"fig_4\"},\"end\":87107,\"start\":87027},{\"attributes\":{\"id\":\"fig_5\"},\"end\":89022,\"start\":87108},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":89658,\"start\":89023},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":91210,\"start\":89659},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":92050,\"start\":91211},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":93014,\"start\":92051},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":97557,\"start\":93015},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":97746,\"start\":97558},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":103073,\"start\":97747},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":103081,\"start\":103074},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":103493,\"start\":103082},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":103570,\"start\":103494},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":103833,\"start\":103571},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":104485,\"start\":103834},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":104946,\"start\":104486},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":105644,\"start\":104947},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":105778,\"start\":105645}]", "paragraph": "[{\"end\":3075,\"start\":2123},{\"end\":4721,\"start\":3077},{\"end\":5135,\"start\":4752},{\"end\":5627,\"start\":5137},{\"end\":6099,\"start\":5629},{\"end\":6918,\"start\":6101},{\"end\":7884,\"start\":6920},{\"end\":8637,\"start\":7886},{\"end\":10145,\"start\":8639},{\"end\":11015,\"start\":10147},{\"end\":12037,\"start\":11050},{\"end\":12187,\"start\":12056},{\"end\":12814,\"start\":12189},{\"end\":13280,\"start\":12850},{\"end\":14003,\"start\":13286},{\"end\":14261,\"start\":14009},{\"end\":14638,\"start\":14267},{\"end\":14830,\"start\":14640},{\"end\":15073,\"start\":14832},{\"end\":15788,\"start\":15075},{\"end\":16107,\"start\":15851},{\"end\":16404,\"start\":16109},{\"end\":16642,\"start\":16406},{\"end\":16867,\"start\":16644},{\"end\":17044,\"start\":16869},{\"end\":17178,\"start\":17046},{\"end\":17403,\"start\":17180},{\"end\":17754,\"start\":17405},{\"end\":17926,\"start\":17756},{\"end\":18157,\"start\":17928},{\"end\":18443,\"start\":18159},{\"end\":18670,\"start\":18445},{\"end\":18775,\"start\":18672},{\"end\":18912,\"start\":18777},{\"end\":19126,\"start\":18914},{\"end\":19418,\"start\":19128},{\"end\":19709,\"start\":19420},{\"end\":21063,\"start\":19711},{\"end\":21409,\"start\":21065},{\"end\":21644,\"start\":21411},{\"end\":21816,\"start\":21646},{\"end\":22052,\"start\":21818},{\"end\":22285,\"start\":22054},{\"end\":22417,\"start\":22287},{\"end\":22632,\"start\":22419},{\"end\":22859,\"start\":22634},{\"end\":22990,\"start\":22861},{\"end\":23282,\"start\":22992},{\"end\":23514,\"start\":23284},{\"end\":23922,\"start\":23516},{\"end\":24079,\"start\":23924},{\"end\":24546,\"start\":24104},{\"end\":25140,\"start\":24548},{\"end\":25381,\"start\":25142},{\"end\":25834,\"start\":25383},{\"end\":26067,\"start\":25836},{\"end\":26314,\"start\":26069},{\"end\":26558,\"start\":26316},{\"end\":26820,\"start\":26560},{\"end\":27178,\"start\":26822},{\"end\":27498,\"start\":27180},{\"end\":27858,\"start\":27500},{\"end\":28025,\"start\":27860},{\"end\":28148,\"start\":28027},{\"end\":28517,\"start\":28150},{\"end\":28701,\"start\":28541},{\"end\":29007,\"start\":28703},{\"end\":29357,\"start\":29009},{\"end\":29737,\"start\":29359},{\"end\":29904,\"start\":29739},{\"end\":30119,\"start\":29906},{\"end\":30445,\"start\":30121},{\"end\":30745,\"start\":30447},{\"end\":31112,\"start\":30747},{\"end\":31334,\"start\":31114},{\"end\":31611,\"start\":31336},{\"end\":31830,\"start\":31635},{\"end\":32413,\"start\":31832},{\"end\":32802,\"start\":32415},{\"end\":33008,\"start\":32804},{\"end\":33163,\"start\":33010},{\"end\":33345,\"start\":33165},{\"end\":33520,\"start\":33347},{\"end\":34194,\"start\":33522},{\"end\":34378,\"start\":34196},{\"end\":34648,\"start\":34380},{\"end\":34844,\"start\":34650},{\"end\":34950,\"start\":34846},{\"end\":35239,\"start\":34975},{\"end\":35570,\"start\":35241},{\"end\":35855,\"start\":35572},{\"end\":36084,\"start\":35857},{\"end\":36353,\"start\":36086},{\"end\":36622,\"start\":36355},{\"end\":36823,\"start\":36624},{\"end\":37144,\"start\":36864},{\"end\":37350,\"start\":37146},{\"end\":37608,\"start\":37352},{\"end\":37776,\"start\":37610},{\"end\":38636,\"start\":37794},{\"end\":38875,\"start\":38673},{\"end\":39300,\"start\":38877},{\"end\":39855,\"start\":39302},{\"end\":40336,\"start\":39857},{\"end\":40642,\"start\":40338},{\"end\":41547,\"start\":40644},{\"end\":41893,\"start\":41577},{\"end\":42315,\"start\":41895},{\"end\":42796,\"start\":42317},{\"end\":43072,\"start\":42798},{\"end\":43398,\"start\":43074},{\"end\":43698,\"start\":43400},{\"end\":43987,\"start\":43700},{\"end\":44417,\"start\":43989},{\"end\":44841,\"start\":44419},{\"end\":45132,\"start\":44843},{\"end\":45780,\"start\":45176},{\"end\":45998,\"start\":45841},{\"end\":46194,\"start\":46000},{\"end\":46370,\"start\":46196},{\"end\":46542,\"start\":46372},{\"end\":46703,\"start\":46544},{\"end\":46891,\"start\":46705},{\"end\":47008,\"start\":46893},{\"end\":47517,\"start\":47035},{\"end\":47662,\"start\":47544},{\"end\":48220,\"start\":47753},{\"end\":48600,\"start\":48285},{\"end\":49287,\"start\":48666},{\"end\":49535,\"start\":49393},{\"end\":50368,\"start\":49562},{\"end\":50646,\"start\":50370},{\"end\":51367,\"start\":50673},{\"end\":51782,\"start\":51445},{\"end\":52601,\"start\":51784},{\"end\":53005,\"start\":52636},{\"end\":54814,\"start\":53007},{\"end\":55657,\"start\":54816},{\"end\":56230,\"start\":55659},{\"end\":57066,\"start\":56232},{\"end\":57629,\"start\":57068},{\"end\":58521,\"start\":57631},{\"end\":58660,\"start\":58523},{\"end\":58889,\"start\":58662},{\"end\":59715,\"start\":58891},{\"end\":59959,\"start\":59717},{\"end\":60083,\"start\":59961},{\"end\":60240,\"start\":60085},{\"end\":61193,\"start\":60273},{\"end\":61418,\"start\":61195},{\"end\":61706,\"start\":61420},{\"end\":62031,\"start\":61708},{\"end\":62706,\"start\":62033},{\"end\":62932,\"start\":62708},{\"end\":63172,\"start\":62934},{\"end\":63316,\"start\":63174},{\"end\":63482,\"start\":63318},{\"end\":63686,\"start\":63484},{\"end\":63872,\"start\":63705},{\"end\":64447,\"start\":63874},{\"end\":66633,\"start\":64449},{\"end\":66742,\"start\":66635},{\"end\":68615,\"start\":66744},{\"end\":68705,\"start\":68617},{\"end\":69196,\"start\":68707},{\"end\":69412,\"start\":69198},{\"end\":70552,\"start\":69414},{\"end\":70762,\"start\":70580},{\"end\":71020,\"start\":70764},{\"end\":72154,\"start\":71022},{\"end\":72768,\"start\":72156},{\"end\":73104,\"start\":72770},{\"end\":74207,\"start\":73106},{\"end\":74511,\"start\":74209},{\"end\":75417,\"start\":74546},{\"end\":75440,\"start\":75429},{\"end\":76436,\"start\":75484},{\"end\":77672,\"start\":76469},{\"end\":78769,\"start\":77710},{\"end\":79022,\"start\":78771},{\"end\":80225,\"start\":79049},{\"end\":80528,\"start\":80227},{\"end\":80911,\"start\":80539},{\"end\":81871,\"start\":80949},{\"end\":83036,\"start\":81904},{\"end\":84392,\"start\":83108},{\"end\":85179,\"start\":84429},{\"end\":86024,\"start\":85199},{\"end\":86302,\"start\":86026},{\"end\":86559,\"start\":86304}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":47706,\"start\":47663},{\"attributes\":{\"id\":\"formula_1\"},\"end\":47752,\"start\":47706},{\"attributes\":{\"id\":\"formula_2\"},\"end\":48284,\"start\":48221},{\"attributes\":{\"id\":\"formula_3\"},\"end\":48665,\"start\":48601},{\"attributes\":{\"id\":\"formula_4\"},\"end\":49339,\"start\":49288},{\"attributes\":{\"id\":\"formula_5\"},\"end\":49392,\"start\":49339}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":8686,\"start\":8679},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15787,\"start\":15779},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":38190,\"start\":38181},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":38566,\"start\":38557},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":50952,\"start\":50944},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":51305,\"start\":51297},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":51546,\"start\":51538},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":52901,\"start\":52894},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":53019,\"start\":53012},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":61140,\"start\":61133},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":75070,\"start\":75062},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":77211,\"start\":77201},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":79845,\"start\":79837},{\"end\":79939,\"start\":79933}]", "section_header": "[{\"end\":2121,\"start\":2106},{\"end\":4750,\"start\":4724},{\"end\":11048,\"start\":11018},{\"end\":12054,\"start\":12040},{\"end\":12848,\"start\":12817},{\"end\":13284,\"start\":13283},{\"end\":14007,\"start\":14006},{\"end\":14265,\"start\":14264},{\"end\":15849,\"start\":15791},{\"end\":24102,\"start\":24082},{\"end\":28539,\"start\":28520},{\"end\":31633,\"start\":31614},{\"end\":34973,\"start\":34953},{\"end\":36862,\"start\":36826},{\"end\":37792,\"start\":37779},{\"end\":38671,\"start\":38639},{\"end\":41575,\"start\":41550},{\"end\":45174,\"start\":45135},{\"end\":45839,\"start\":45783},{\"end\":47033,\"start\":47011},{\"end\":47542,\"start\":47520},{\"end\":49560,\"start\":49538},{\"end\":50671,\"start\":50649},{\"end\":51398,\"start\":51370},{\"end\":51443,\"start\":51401},{\"end\":52634,\"start\":52604},{\"end\":60271,\"start\":60243},{\"end\":63703,\"start\":63689},{\"end\":70578,\"start\":70555},{\"end\":74544,\"start\":74514},{\"end\":75427,\"start\":75420},{\"end\":75482,\"start\":75443},{\"end\":76467,\"start\":76439},{\"end\":77708,\"start\":77675},{\"end\":79047,\"start\":79025},{\"end\":80537,\"start\":80531},{\"end\":80947,\"start\":80914},{\"end\":81902,\"start\":81874},{\"end\":83106,\"start\":83039},{\"end\":84427,\"start\":84395},{\"end\":85197,\"start\":85182},{\"end\":86569,\"start\":86561},{\"end\":86648,\"start\":86640},{\"end\":86991,\"start\":86983},{\"end\":87036,\"start\":87028},{\"end\":89663,\"start\":89660},{\"end\":92061,\"start\":92052},{\"end\":93026,\"start\":93016},{\"end\":97759,\"start\":97748},{\"end\":103080,\"start\":103075},{\"end\":103092,\"start\":103083},{\"end\":103505,\"start\":103495},{\"end\":103846,\"start\":103835},{\"end\":104960,\"start\":104948},{\"end\":105656,\"start\":105646}]", "table": "[{\"end\":89658,\"start\":89584},{\"end\":91210,\"start\":90251},{\"end\":92050,\"start\":91354},{\"end\":93014,\"start\":92091},{\"end\":97557,\"start\":93103},{\"end\":103073,\"start\":97998},{\"end\":103493,\"start\":103138},{\"end\":104485,\"start\":104085},{\"end\":105644,\"start\":105235}]", "figure_caption": "[{\"end\":86638,\"start\":86571},{\"end\":86981,\"start\":86650},{\"end\":87026,\"start\":86993},{\"end\":87107,\"start\":87038},{\"end\":89022,\"start\":87110},{\"end\":89584,\"start\":89025},{\"end\":90251,\"start\":89665},{\"end\":91354,\"start\":91213},{\"end\":92091,\"start\":92063},{\"end\":93103,\"start\":93029},{\"end\":97746,\"start\":97560},{\"end\":97998,\"start\":97763},{\"end\":103138,\"start\":103094},{\"end\":103570,\"start\":103508},{\"end\":103833,\"start\":103573},{\"end\":104085,\"start\":103850},{\"end\":104946,\"start\":104488},{\"end\":105235,\"start\":104965},{\"end\":105778,\"start\":105659}]", "figure_ref": "[{\"end\":7617,\"start\":7611},{\"end\":7812,\"start\":7806},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16106,\"start\":16100},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38634,\"start\":38628},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":60876,\"start\":60870},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":62114,\"start\":62108},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":71019,\"start\":71013},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":77045,\"start\":77039}]", "bib_author_first_name": "[{\"end\":106593,\"start\":106592},{\"end\":106602,\"start\":106601},{\"end\":106610,\"start\":106609},{\"end\":106618,\"start\":106617},{\"end\":106625,\"start\":106624},{\"end\":106901,\"start\":106900},{\"end\":106916,\"start\":106915},{\"end\":106918,\"start\":106917},{\"end\":107172,\"start\":107171},{\"end\":107187,\"start\":107186},{\"end\":107189,\"start\":107188},{\"end\":107204,\"start\":107203},{\"end\":107206,\"start\":107205},{\"end\":107460,\"start\":107459},{\"end\":107468,\"start\":107467},{\"end\":107653,\"start\":107652},{\"end\":107655,\"start\":107654},{\"end\":107662,\"start\":107661},{\"end\":107671,\"start\":107670},{\"end\":107673,\"start\":107672},{\"end\":107933,\"start\":107932},{\"end\":107935,\"start\":107934},{\"end\":107946,\"start\":107945},{\"end\":107948,\"start\":107947},{\"end\":108233,\"start\":108232},{\"end\":108240,\"start\":108239},{\"end\":108247,\"start\":108246},{\"end\":108255,\"start\":108254},{\"end\":108257,\"start\":108256},{\"end\":108542,\"start\":108541},{\"end\":108553,\"start\":108552},{\"end\":108569,\"start\":108568},{\"end\":108579,\"start\":108578},{\"end\":108836,\"start\":108835},{\"end\":108845,\"start\":108844},{\"end\":108852,\"start\":108851},{\"end\":109107,\"start\":109106},{\"end\":109115,\"start\":109114},{\"end\":109121,\"start\":109120},{\"end\":109128,\"start\":109127},{\"end\":109137,\"start\":109136},{\"end\":109146,\"start\":109145},{\"end\":109152,\"start\":109151},{\"end\":109416,\"start\":109415},{\"end\":109423,\"start\":109422},{\"end\":109431,\"start\":109430},{\"end\":109439,\"start\":109438},{\"end\":109715,\"start\":109714},{\"end\":109721,\"start\":109720},{\"end\":109730,\"start\":109729},{\"end\":109989,\"start\":109988},{\"end\":109991,\"start\":109990},{\"end\":110001,\"start\":110000},{\"end\":110010,\"start\":110009},{\"end\":110019,\"start\":110018},{\"end\":110243,\"start\":110242},{\"end\":110251,\"start\":110250},{\"end\":110433,\"start\":110432},{\"end\":110443,\"start\":110442},{\"end\":110616,\"start\":110615},{\"end\":110629,\"start\":110628},{\"end\":110795,\"start\":110794},{\"end\":110805,\"start\":110804},{\"end\":110818,\"start\":110817},{\"end\":111032,\"start\":111031},{\"end\":111041,\"start\":111040},{\"end\":111052,\"start\":111048},{\"end\":111064,\"start\":111060},{\"end\":111309,\"start\":111308},{\"end\":111318,\"start\":111317},{\"end\":111324,\"start\":111323},{\"end\":111326,\"start\":111325},{\"end\":111338,\"start\":111337},{\"end\":111340,\"start\":111339},{\"end\":111567,\"start\":111566},{\"end\":111592,\"start\":111591},{\"end\":111609,\"start\":111608},{\"end\":111625,\"start\":111624},{\"end\":111645,\"start\":111644},{\"end\":111942,\"start\":111941},{\"end\":111950,\"start\":111949},{\"end\":112134,\"start\":112133},{\"end\":112136,\"start\":112135},{\"end\":112149,\"start\":112148},{\"end\":112162,\"start\":112161},{\"end\":112164,\"start\":112163},{\"end\":112372,\"start\":112371},{\"end\":112382,\"start\":112381},{\"end\":112392,\"start\":112391},{\"end\":112406,\"start\":112405},{\"end\":112605,\"start\":112604},{\"end\":112613,\"start\":112612},{\"end\":112615,\"start\":112614},{\"end\":112628,\"start\":112627},{\"end\":112641,\"start\":112640},{\"end\":112643,\"start\":112642},{\"end\":112658,\"start\":112654},{\"end\":112882,\"start\":112881},{\"end\":112890,\"start\":112889},{\"end\":112898,\"start\":112897},{\"end\":112905,\"start\":112899},{\"end\":112912,\"start\":112911},{\"end\":113164,\"start\":113163},{\"end\":113172,\"start\":113171},{\"end\":113180,\"start\":113179},{\"end\":113455,\"start\":113454},{\"end\":113457,\"start\":113456},{\"end\":113468,\"start\":113464},{\"end\":113470,\"start\":113469},{\"end\":113479,\"start\":113478},{\"end\":113772,\"start\":113771},{\"end\":113780,\"start\":113779},{\"end\":113787,\"start\":113786},{\"end\":113794,\"start\":113793},{\"end\":114016,\"start\":114015},{\"end\":114018,\"start\":114017},{\"end\":114029,\"start\":114028},{\"end\":114040,\"start\":114039},{\"end\":114316,\"start\":114315},{\"end\":114322,\"start\":114321},{\"end\":114331,\"start\":114330},{\"end\":114340,\"start\":114339},{\"end\":114587,\"start\":114586},{\"end\":114596,\"start\":114595},{\"end\":114607,\"start\":114606},{\"end\":114810,\"start\":114809},{\"end\":114823,\"start\":114822},{\"end\":114825,\"start\":114824},{\"end\":114997,\"start\":114996},{\"end\":115161,\"start\":115160},{\"end\":115168,\"start\":115167},{\"end\":115174,\"start\":115173},{\"end\":115186,\"start\":115185},{\"end\":115370,\"start\":115369},{\"end\":115376,\"start\":115375},{\"end\":115388,\"start\":115387},{\"end\":115398,\"start\":115397},{\"end\":115562,\"start\":115561},{\"end\":115572,\"start\":115571},{\"end\":115583,\"start\":115582},{\"end\":115595,\"start\":115594},{\"end\":115790,\"start\":115789},{\"end\":115797,\"start\":115796},{\"end\":115809,\"start\":115808},{\"end\":115818,\"start\":115817},{\"end\":115829,\"start\":115828},{\"end\":115840,\"start\":115836},{\"end\":115846,\"start\":115845},{\"end\":115848,\"start\":115847},{\"end\":116091,\"start\":116090},{\"end\":116101,\"start\":116100},{\"end\":116112,\"start\":116111},{\"end\":116123,\"start\":116122},{\"end\":116354,\"start\":116353},{\"end\":116356,\"start\":116355},{\"end\":116364,\"start\":116363},{\"end\":116552,\"start\":116551},{\"end\":116560,\"start\":116559},{\"end\":116562,\"start\":116561},{\"end\":116569,\"start\":116568},{\"end\":116577,\"start\":116576},{\"end\":116801,\"start\":116800},{\"end\":116809,\"start\":116808},{\"end\":116819,\"start\":116818},{\"end\":116829,\"start\":116828},{\"end\":117042,\"start\":117041},{\"end\":117044,\"start\":117043},{\"end\":117271,\"start\":117270},{\"end\":117280,\"start\":117279},{\"end\":117295,\"start\":117294},{\"end\":117544,\"start\":117543},{\"end\":117553,\"start\":117552},{\"end\":117778,\"start\":117777},{\"end\":117780,\"start\":117779},{\"end\":117792,\"start\":117791},{\"end\":117984,\"start\":117983},{\"end\":117996,\"start\":117995},{\"end\":118206,\"start\":118205},{\"end\":118214,\"start\":118213},{\"end\":118223,\"start\":118222},{\"end\":118233,\"start\":118232},{\"end\":118248,\"start\":118247},{\"end\":118537,\"start\":118533},{\"end\":118545,\"start\":118544},{\"end\":118557,\"start\":118556},{\"end\":118570,\"start\":118569},{\"end\":118833,\"start\":118832},{\"end\":118841,\"start\":118840},{\"end\":118850,\"start\":118849},{\"end\":118858,\"start\":118857},{\"end\":118865,\"start\":118864},{\"end\":119098,\"start\":119097},{\"end\":119104,\"start\":119103},{\"end\":119110,\"start\":119109},{\"end\":119116,\"start\":119115},{\"end\":119123,\"start\":119122},{\"end\":119129,\"start\":119128},{\"end\":119372,\"start\":119371},{\"end\":119381,\"start\":119380},{\"end\":119387,\"start\":119386},{\"end\":119395,\"start\":119394},{\"end\":119658,\"start\":119657},{\"end\":119664,\"start\":119663},{\"end\":119673,\"start\":119672},{\"end\":119675,\"start\":119674},{\"end\":119682,\"start\":119681},{\"end\":119881,\"start\":119880},{\"end\":119887,\"start\":119886},{\"end\":119894,\"start\":119893},{\"end\":120098,\"start\":120097},{\"end\":120106,\"start\":120105},{\"end\":120114,\"start\":120113},{\"end\":120131,\"start\":120122},{\"end\":120401,\"start\":120400},{\"end\":120403,\"start\":120402},{\"end\":120414,\"start\":120413},{\"end\":120416,\"start\":120415},{\"end\":120670,\"start\":120669},{\"end\":120677,\"start\":120676},{\"end\":120684,\"start\":120683},{\"end\":120692,\"start\":120691},{\"end\":120908,\"start\":120907},{\"end\":120910,\"start\":120909},{\"end\":120917,\"start\":120916},{\"end\":120919,\"start\":120918},{\"end\":120927,\"start\":120926},{\"end\":120937,\"start\":120936},{\"end\":120939,\"start\":120938},{\"end\":121218,\"start\":121217},{\"end\":121224,\"start\":121219},{\"end\":121234,\"start\":121233},{\"end\":121236,\"start\":121235},{\"end\":121245,\"start\":121244},{\"end\":121247,\"start\":121246},{\"end\":121507,\"start\":121506},{\"end\":121519,\"start\":121518},{\"end\":121529,\"start\":121528},{\"end\":121823,\"start\":121822},{\"end\":121825,\"start\":121824},{\"end\":121832,\"start\":121831},{\"end\":121840,\"start\":121839},{\"end\":121848,\"start\":121847},{\"end\":122097,\"start\":122096},{\"end\":122112,\"start\":122108},{\"end\":122302,\"start\":122301},{\"end\":122308,\"start\":122307},{\"end\":122317,\"start\":122316},{\"end\":122325,\"start\":122324},{\"end\":122331,\"start\":122330},{\"end\":122339,\"start\":122338},{\"end\":122568,\"start\":122567},{\"end\":122576,\"start\":122575},{\"end\":122586,\"start\":122585},{\"end\":122599,\"start\":122598},{\"end\":122858,\"start\":122857},{\"end\":122862,\"start\":122859},{\"end\":122871,\"start\":122870},{\"end\":122873,\"start\":122872},{\"end\":122883,\"start\":122882},{\"end\":123183,\"start\":123182},{\"end\":123185,\"start\":123184},{\"end\":123196,\"start\":123195},{\"end\":123207,\"start\":123206},{\"end\":123215,\"start\":123214},{\"end\":123217,\"start\":123216},{\"end\":123520,\"start\":123519},{\"end\":123522,\"start\":123521},{\"end\":123531,\"start\":123530},{\"end\":123533,\"start\":123532},{\"end\":123764,\"start\":123763},{\"end\":123766,\"start\":123765},{\"end\":123777,\"start\":123776},{\"end\":123779,\"start\":123778},{\"end\":124031,\"start\":124030},{\"end\":124043,\"start\":124042},{\"end\":124052,\"start\":124051},{\"end\":124054,\"start\":124053},{\"end\":124344,\"start\":124343},{\"end\":124353,\"start\":124352},{\"end\":124359,\"start\":124358},{\"end\":124577,\"start\":124576},{\"end\":124584,\"start\":124583},{\"end\":124593,\"start\":124592},{\"end\":124600,\"start\":124599},{\"end\":124607,\"start\":124606},{\"end\":124616,\"start\":124612},{\"end\":124625,\"start\":124624},{\"end\":124868,\"start\":124867},{\"end\":124875,\"start\":124874},{\"end\":124883,\"start\":124882},{\"end\":124885,\"start\":124884},{\"end\":124895,\"start\":124894},{\"end\":125167,\"start\":125166},{\"end\":125174,\"start\":125173},{\"end\":125176,\"start\":125175},{\"end\":125186,\"start\":125185},{\"end\":125199,\"start\":125198},{\"end\":125458,\"start\":125457},{\"end\":125468,\"start\":125467},{\"end\":125474,\"start\":125473},{\"end\":125656,\"start\":125655},{\"end\":125664,\"start\":125663},{\"end\":125670,\"start\":125669},{\"end\":125676,\"start\":125675},{\"end\":125684,\"start\":125683},{\"end\":125690,\"start\":125689},{\"end\":125927,\"start\":125926},{\"end\":125934,\"start\":125933},{\"end\":125943,\"start\":125942},{\"end\":125950,\"start\":125949},{\"end\":126196,\"start\":126195},{\"end\":126206,\"start\":126205},{\"end\":126216,\"start\":126215},{\"end\":126226,\"start\":126225},{\"end\":126235,\"start\":126234},{\"end\":126249,\"start\":126248},{\"end\":126260,\"start\":126259},{\"end\":126573,\"start\":126572},{\"end\":126580,\"start\":126579},{\"end\":126597,\"start\":126596},{\"end\":126599,\"start\":126598},{\"end\":126890,\"start\":126889},{\"end\":126903,\"start\":126902},{\"end\":126911,\"start\":126910},{\"end\":127143,\"start\":127142},{\"end\":127151,\"start\":127150},{\"end\":127158,\"start\":127157},{\"end\":127167,\"start\":127166},{\"end\":127169,\"start\":127168},{\"end\":127348,\"start\":127347},{\"end\":127354,\"start\":127353},{\"end\":127363,\"start\":127362},{\"end\":127369,\"start\":127368},{\"end\":127375,\"start\":127374},{\"end\":127383,\"start\":127382},{\"end\":127577,\"start\":127576},{\"end\":127579,\"start\":127578},{\"end\":127583,\"start\":127582},{\"end\":127585,\"start\":127584},{\"end\":127589,\"start\":127588},{\"end\":127591,\"start\":127590},{\"end\":127595,\"start\":127594},{\"end\":127597,\"start\":127596},{\"end\":127830,\"start\":127829},{\"end\":127843,\"start\":127842},{\"end\":127850,\"start\":127849},{\"end\":128137,\"start\":128136},{\"end\":128144,\"start\":128143},{\"end\":128152,\"start\":128151},{\"end\":128159,\"start\":128158},{\"end\":128166,\"start\":128165},{\"end\":128173,\"start\":128172},{\"end\":128431,\"start\":128430},{\"end\":128439,\"start\":128438},{\"end\":128446,\"start\":128445},{\"end\":128453,\"start\":128452},{\"end\":128686,\"start\":128685},{\"end\":128693,\"start\":128692},{\"end\":128701,\"start\":128700},{\"end\":128707,\"start\":128706},{\"end\":128888,\"start\":128887},{\"end\":128895,\"start\":128894},{\"end\":128907,\"start\":128906},{\"end\":129066,\"start\":129065},{\"end\":129068,\"start\":129067},{\"end\":129075,\"start\":129074},{\"end\":129079,\"start\":129076},{\"end\":129286,\"start\":129285},{\"end\":129293,\"start\":129292},{\"end\":129301,\"start\":129300},{\"end\":129521,\"start\":129520},{\"end\":129528,\"start\":129527},{\"end\":129536,\"start\":129535},{\"end\":129542,\"start\":129541},{\"end\":129549,\"start\":129548},{\"end\":129557,\"start\":129556},{\"end\":129836,\"start\":129835},{\"end\":129838,\"start\":129837},{\"end\":129849,\"start\":129848},{\"end\":130106,\"start\":130105},{\"end\":130113,\"start\":130112},{\"end\":130130,\"start\":130129},{\"end\":130132,\"start\":130131},{\"end\":130363,\"start\":130362},{\"end\":130372,\"start\":130371},{\"end\":130380,\"start\":130379},{\"end\":130389,\"start\":130388},{\"end\":130397,\"start\":130396},{\"end\":130404,\"start\":130403},{\"end\":130416,\"start\":130415},{\"end\":130681,\"start\":130680},{\"end\":130688,\"start\":130687},{\"end\":130695,\"start\":130694},{\"end\":130701,\"start\":130700},{\"end\":130703,\"start\":130702},{\"end\":130711,\"start\":130710},{\"end\":130953,\"start\":130952},{\"end\":130960,\"start\":130959},{\"end\":130968,\"start\":130967},{\"end\":131200,\"start\":131199},{\"end\":131208,\"start\":131207},{\"end\":131217,\"start\":131216},{\"end\":131226,\"start\":131225},{\"end\":131461,\"start\":131460},{\"end\":131469,\"start\":131462},{\"end\":131489,\"start\":131488},{\"end\":131704,\"start\":131703},{\"end\":131706,\"start\":131705},{\"end\":131717,\"start\":131716},{\"end\":131719,\"start\":131718},{\"end\":131937,\"start\":131936},{\"end\":131943,\"start\":131942},{\"end\":131950,\"start\":131949},{\"end\":131956,\"start\":131955},{\"end\":131963,\"start\":131962},{\"end\":131969,\"start\":131968},{\"end\":132194,\"start\":132193},{\"end\":132203,\"start\":132202},{\"end\":132211,\"start\":132210},{\"end\":132219,\"start\":132218},{\"end\":132226,\"start\":132225},{\"end\":132234,\"start\":132233},{\"end\":132241,\"start\":132240},{\"end\":132500,\"start\":132499},{\"end\":132509,\"start\":132508},{\"end\":132515,\"start\":132514},{\"end\":132521,\"start\":132520},{\"end\":132529,\"start\":132528},{\"end\":132539,\"start\":132538},{\"end\":132800,\"start\":132799},{\"end\":132813,\"start\":132812},{\"end\":132820,\"start\":132819},{\"end\":132822,\"start\":132821},{\"end\":132830,\"start\":132829},{\"end\":133049,\"start\":133048},{\"end\":133063,\"start\":133062},{\"end\":133076,\"start\":133075},{\"end\":133078,\"start\":133077},{\"end\":133271,\"start\":133270},{\"end\":133285,\"start\":133284},{\"end\":133302,\"start\":133301},{\"end\":133311,\"start\":133310},{\"end\":133317,\"start\":133316},{\"end\":133333,\"start\":133332},{\"end\":133342,\"start\":133341},{\"end\":133355,\"start\":133354},{\"end\":133615,\"start\":133614},{\"end\":133617,\"start\":133616},{\"end\":133624,\"start\":133623},{\"end\":133626,\"start\":133625},{\"end\":133790,\"start\":133789},{\"end\":133803,\"start\":133802},{\"end\":133815,\"start\":133814},{\"end\":134068,\"start\":134067},{\"end\":134079,\"start\":134078},{\"end\":134088,\"start\":134087},{\"end\":134383,\"start\":134382},{\"end\":134390,\"start\":134389},{\"end\":134399,\"start\":134395},{\"end\":134406,\"start\":134405},{\"end\":134643,\"start\":134642},{\"end\":134652,\"start\":134651},{\"end\":134660,\"start\":134659},{\"end\":134667,\"start\":134666},{\"end\":134673,\"start\":134672},{\"end\":134681,\"start\":134680},{\"end\":134900,\"start\":134899},{\"end\":134911,\"start\":134910},{\"end\":134918,\"start\":134917},{\"end\":134925,\"start\":134924},{\"end\":134937,\"start\":134936},{\"end\":134945,\"start\":134944},{\"end\":134957,\"start\":134956},{\"end\":134966,\"start\":134965},{\"end\":134979,\"start\":134978},{\"end\":135238,\"start\":135237},{\"end\":135253,\"start\":135252},{\"end\":135264,\"start\":135263},{\"end\":135472,\"start\":135471},{\"end\":135697,\"start\":135696},{\"end\":135705,\"start\":135704},{\"end\":135711,\"start\":135710},{\"end\":135713,\"start\":135712},{\"end\":135945,\"start\":135944},{\"end\":135954,\"start\":135953},{\"end\":135964,\"start\":135960},{\"end\":135973,\"start\":135972},{\"end\":135983,\"start\":135979},{\"end\":135991,\"start\":135990},{\"end\":135993,\"start\":135992},{\"end\":136001,\"start\":136000},{\"end\":136010,\"start\":136009},{\"end\":136246,\"start\":136245},{\"end\":136254,\"start\":136253},{\"end\":136261,\"start\":136260},{\"end\":136458,\"start\":136457},{\"end\":136464,\"start\":136463},{\"end\":136473,\"start\":136472},{\"end\":136479,\"start\":136478},{\"end\":136487,\"start\":136486},{\"end\":136499,\"start\":136498},{\"end\":136765,\"start\":136764},{\"end\":136773,\"start\":136772},{\"end\":137014,\"start\":137013},{\"end\":137023,\"start\":137022},{\"end\":137031,\"start\":137030},{\"end\":137038,\"start\":137037},{\"end\":137044,\"start\":137043},{\"end\":137052,\"start\":137051},{\"end\":137270,\"start\":137269},{\"end\":137282,\"start\":137281},{\"end\":137284,\"start\":137283},{\"end\":137525,\"start\":137524},{\"end\":137538,\"start\":137537},{\"end\":137546,\"start\":137545},{\"end\":137554,\"start\":137553},{\"end\":137850,\"start\":137849},{\"end\":137858,\"start\":137857},{\"end\":137867,\"start\":137866},{\"end\":137874,\"start\":137873},{\"end\":137882,\"start\":137881},{\"end\":137884,\"start\":137883},{\"end\":138145,\"start\":138144},{\"end\":138153,\"start\":138152},{\"end\":138159,\"start\":138158},{\"end\":138327,\"start\":138326},{\"end\":138329,\"start\":138328},{\"end\":138340,\"start\":138339},{\"end\":138349,\"start\":138348},{\"end\":138365,\"start\":138364},{\"end\":138375,\"start\":138374},{\"end\":138538,\"start\":138537},{\"end\":138544,\"start\":138543},{\"end\":138551,\"start\":138550},{\"end\":138778,\"start\":138777},{\"end\":138789,\"start\":138788},{\"end\":138803,\"start\":138802},{\"end\":138813,\"start\":138812},{\"end\":138815,\"start\":138814},{\"end\":138824,\"start\":138823},{\"end\":138826,\"start\":138825},{\"end\":139168,\"start\":139167},{\"end\":139176,\"start\":139175},{\"end\":139178,\"start\":139177},{\"end\":139187,\"start\":139186},{\"end\":139189,\"start\":139188},{\"end\":139199,\"start\":139198},{\"end\":139201,\"start\":139200},{\"end\":139513,\"start\":139512},{\"end\":139520,\"start\":139519},{\"end\":139528,\"start\":139527},{\"end\":139817,\"start\":139816},{\"end\":139824,\"start\":139823},{\"end\":139831,\"start\":139830},{\"end\":139839,\"start\":139838},{\"end\":140111,\"start\":140110},{\"end\":140118,\"start\":140117},{\"end\":140125,\"start\":140124},{\"end\":140133,\"start\":140132},{\"end\":140363,\"start\":140362},{\"end\":140380,\"start\":140379},{\"end\":140392,\"start\":140391},{\"end\":140394,\"start\":140393},{\"end\":140407,\"start\":140406},{\"end\":140416,\"start\":140415},{\"end\":140662,\"start\":140661},{\"end\":140671,\"start\":140670},{\"end\":140673,\"start\":140672},{\"end\":140939,\"start\":140938},{\"end\":140947,\"start\":140946},{\"end\":140953,\"start\":140952},{\"end\":140962,\"start\":140961},{\"end\":140969,\"start\":140968},{\"end\":141242,\"start\":141241},{\"end\":141250,\"start\":141249},{\"end\":141258,\"start\":141257},{\"end\":141265,\"start\":141264},{\"end\":141272,\"start\":141271},{\"end\":141516,\"start\":141515},{\"end\":141523,\"start\":141522},{\"end\":141531,\"start\":141530},{\"end\":141538,\"start\":141537},{\"end\":141546,\"start\":141545},{\"end\":141554,\"start\":141553},{\"end\":141561,\"start\":141560},{\"end\":141825,\"start\":141824},{\"end\":141827,\"start\":141826},{\"end\":141838,\"start\":141837},{\"end\":141849,\"start\":141848},{\"end\":141851,\"start\":141850},{\"end\":142131,\"start\":142130},{\"end\":142144,\"start\":142143},{\"end\":142151,\"start\":142150},{\"end\":142432,\"start\":142431},{\"end\":142439,\"start\":142438},{\"end\":142445,\"start\":142444},{\"end\":142452,\"start\":142451},{\"end\":142458,\"start\":142457},{\"end\":142466,\"start\":142465},{\"end\":142472,\"start\":142471},{\"end\":142821,\"start\":142820},{\"end\":142828,\"start\":142827},{\"end\":142839,\"start\":142835},{\"end\":143089,\"start\":143088},{\"end\":143098,\"start\":143097},{\"end\":143272,\"start\":143271},{\"end\":143274,\"start\":143273},{\"end\":143281,\"start\":143280},{\"end\":143283,\"start\":143282},{\"end\":143290,\"start\":143289},{\"end\":143292,\"start\":143291},{\"end\":143299,\"start\":143298},{\"end\":143301,\"start\":143300},{\"end\":143545,\"start\":143544},{\"end\":143554,\"start\":143553},{\"end\":143567,\"start\":143566},{\"end\":143569,\"start\":143568},{\"end\":143581,\"start\":143580},{\"end\":143585,\"start\":143582},{\"end\":143826,\"start\":143825},{\"end\":143833,\"start\":143832},{\"end\":143841,\"start\":143840},{\"end\":143847,\"start\":143846},{\"end\":144079,\"start\":144078},{\"end\":144087,\"start\":144086},{\"end\":144093,\"start\":144092},{\"end\":144101,\"start\":144100},{\"end\":144354,\"start\":144353},{\"end\":144360,\"start\":144359},{\"end\":144369,\"start\":144368},{\"end\":144377,\"start\":144376},{\"end\":144385,\"start\":144384},{\"end\":144644,\"start\":144643},{\"end\":144655,\"start\":144654},{\"end\":144661,\"start\":144660},{\"end\":144668,\"start\":144667},{\"end\":144683,\"start\":144682},{\"end\":144966,\"start\":144965},{\"end\":144972,\"start\":144971},{\"end\":144980,\"start\":144979},{\"end\":144989,\"start\":144988},{\"end\":144995,\"start\":144994},{\"end\":145269,\"start\":145268},{\"end\":145277,\"start\":145276},{\"end\":145284,\"start\":145283},{\"end\":145290,\"start\":145289},{\"end\":145298,\"start\":145297},{\"end\":145601,\"start\":145600},{\"end\":145610,\"start\":145609},{\"end\":145617,\"start\":145616},{\"end\":145624,\"start\":145623},{\"end\":145630,\"start\":145629},{\"end\":145932,\"start\":145931},{\"end\":145944,\"start\":145943},{\"end\":145953,\"start\":145952},{\"end\":145966,\"start\":145965},{\"end\":146206,\"start\":146205},{\"end\":146212,\"start\":146211},{\"end\":146218,\"start\":146217},{\"end\":146227,\"start\":146226},{\"end\":146234,\"start\":146233},{\"end\":146242,\"start\":146241},{\"end\":146474,\"start\":146473},{\"end\":146482,\"start\":146481},{\"end\":146704,\"start\":146703},{\"end\":146713,\"start\":146712},{\"end\":146722,\"start\":146721},{\"end\":146728,\"start\":146727},{\"end\":146735,\"start\":146734},{\"end\":146742,\"start\":146741},{\"end\":146748,\"start\":146747},{\"end\":146756,\"start\":146755},{\"end\":146762,\"start\":146761},{\"end\":146770,\"start\":146769},{\"end\":147039,\"start\":147038},{\"end\":147043,\"start\":147040},{\"end\":147050,\"start\":147049},{\"end\":147052,\"start\":147051},{\"end\":147062,\"start\":147061},{\"end\":147064,\"start\":147063},{\"end\":147344,\"start\":147343},{\"end\":147354,\"start\":147353},{\"end\":147363,\"start\":147362},{\"end\":147374,\"start\":147373},{\"end\":147598,\"start\":147597},{\"end\":147606,\"start\":147605},{\"end\":147616,\"start\":147612},{\"end\":147623,\"start\":147622},{\"end\":147839,\"start\":147838},{\"end\":147850,\"start\":147849},{\"end\":147860,\"start\":147859},{\"end\":147867,\"start\":147866},{\"end\":147880,\"start\":147879},{\"end\":148157,\"start\":148156},{\"end\":148166,\"start\":148165},{\"end\":148172,\"start\":148171},{\"end\":148174,\"start\":148173},{\"end\":148186,\"start\":148185},{\"end\":148188,\"start\":148187},{\"end\":148652,\"start\":148648},{\"end\":148664,\"start\":148660},{\"end\":148671,\"start\":148670},{\"end\":148673,\"start\":148672},{\"end\":148918,\"start\":148917},{\"end\":148924,\"start\":148923},{\"end\":148931,\"start\":148930},{\"end\":148939,\"start\":148938},{\"end\":148949,\"start\":148948},{\"end\":149244,\"start\":149243},{\"end\":149256,\"start\":149255},{\"end\":149264,\"start\":149263},{\"end\":149276,\"start\":149275},{\"end\":149287,\"start\":149286},{\"end\":149560,\"start\":149559},{\"end\":149567,\"start\":149566},{\"end\":149574,\"start\":149573},{\"end\":149582,\"start\":149581},{\"end\":149590,\"start\":149589},{\"end\":149821,\"start\":149820},{\"end\":149828,\"start\":149827},{\"end\":149838,\"start\":149834},{\"end\":149840,\"start\":149839},{\"end\":150066,\"start\":150065},{\"end\":150073,\"start\":150072},{\"end\":150075,\"start\":150074},{\"end\":150082,\"start\":150081},{\"end\":150094,\"start\":150093},{\"end\":150308,\"start\":150307},{\"end\":150315,\"start\":150314},{\"end\":150322,\"start\":150321},{\"end\":150328,\"start\":150327},{\"end\":150335,\"start\":150334},{\"end\":150343,\"start\":150342},{\"end\":150536,\"start\":150535},{\"end\":150542,\"start\":150541},{\"end\":150550,\"start\":150549},{\"end\":150719,\"start\":150718},{\"end\":150726,\"start\":150725},{\"end\":150728,\"start\":150727},{\"end\":150945,\"start\":150944},{\"end\":150953,\"start\":150952},{\"end\":150955,\"start\":150954},{\"end\":150962,\"start\":150961},{\"end\":150970,\"start\":150969},{\"end\":150976,\"start\":150975},{\"end\":151207,\"start\":151206},{\"end\":151214,\"start\":151213},{\"end\":151222,\"start\":151221},{\"end\":151232,\"start\":151231},{\"end\":151238,\"start\":151237},{\"end\":151240,\"start\":151239},{\"end\":151250,\"start\":151249},{\"end\":151464,\"start\":151463},{\"end\":151470,\"start\":151469},{\"end\":151749,\"start\":151745},{\"end\":151757,\"start\":151756},{\"end\":151771,\"start\":151770},{\"end\":151783,\"start\":151782},{\"end\":151793,\"start\":151792},{\"end\":151795,\"start\":151794},{\"end\":152111,\"start\":152107},{\"end\":152119,\"start\":152118},{\"end\":152133,\"start\":152132},{\"end\":152144,\"start\":152143},{\"end\":152398,\"start\":152397},{\"end\":152406,\"start\":152405},{\"end\":152419,\"start\":152418},{\"end\":152602,\"start\":152601},{\"end\":152609,\"start\":152608},{\"end\":152615,\"start\":152614},{\"end\":152624,\"start\":152623},{\"end\":152630,\"start\":152629},{\"end\":152639,\"start\":152638},{\"end\":152645,\"start\":152644},{\"end\":152868,\"start\":152867},{\"end\":152876,\"start\":152875},{\"end\":152884,\"start\":152883},{\"end\":152890,\"start\":152889},{\"end\":152896,\"start\":152895},{\"end\":153114,\"start\":153113},{\"end\":153122,\"start\":153121},{\"end\":153130,\"start\":153129},{\"end\":153357,\"start\":153356},{\"end\":153369,\"start\":153368},{\"end\":153381,\"start\":153380},{\"end\":153383,\"start\":153382},{\"end\":153589,\"start\":153588},{\"end\":153591,\"start\":153590},{\"end\":153816,\"start\":153815},{\"end\":153830,\"start\":153829},{\"end\":154047,\"start\":154046},{\"end\":154054,\"start\":154053},{\"end\":154061,\"start\":154060},{\"end\":154067,\"start\":154066},{\"end\":154074,\"start\":154073},{\"end\":154084,\"start\":154083},{\"end\":154297,\"start\":154296},{\"end\":154306,\"start\":154305},{\"end\":154313,\"start\":154312},{\"end\":154321,\"start\":154320},{\"end\":154328,\"start\":154327},{\"end\":154336,\"start\":154335},{\"end\":154343,\"start\":154342},{\"end\":154547,\"start\":154546},{\"end\":154555,\"start\":154554},{\"end\":154567,\"start\":154566},{\"end\":154576,\"start\":154575},{\"end\":154771,\"start\":154767},{\"end\":154781,\"start\":154777},{\"end\":154788,\"start\":154787},{\"end\":154799,\"start\":154795},{\"end\":155053,\"start\":155052},{\"end\":155055,\"start\":155054},{\"end\":155063,\"start\":155062},{\"end\":155272,\"start\":155271},{\"end\":155281,\"start\":155280},{\"end\":155287,\"start\":155286},{\"end\":155296,\"start\":155295},{\"end\":155302,\"start\":155301},{\"end\":155309,\"start\":155308},{\"end\":155315,\"start\":155314},{\"end\":155567,\"start\":155566},{\"end\":155573,\"start\":155572},{\"end\":155582,\"start\":155581},{\"end\":155589,\"start\":155588},{\"end\":155829,\"start\":155828},{\"end\":155837,\"start\":155836},{\"end\":155845,\"start\":155844},{\"end\":155852,\"start\":155851},{\"end\":155859,\"start\":155858},{\"end\":155867,\"start\":155866},{\"end\":156139,\"start\":156138},{\"end\":156147,\"start\":156146},{\"end\":156153,\"start\":156152},{\"end\":156160,\"start\":156159},{\"end\":156167,\"start\":156166},{\"end\":156378,\"start\":156377},{\"end\":156387,\"start\":156386},{\"end\":156393,\"start\":156392},{\"end\":156400,\"start\":156399},{\"end\":156556,\"start\":156555},{\"end\":156562,\"start\":156561},{\"end\":156564,\"start\":156563},{\"end\":156573,\"start\":156572},{\"end\":156575,\"start\":156574},{\"end\":156731,\"start\":156730},{\"end\":156741,\"start\":156740},{\"end\":156752,\"start\":156751},{\"end\":156754,\"start\":156753},{\"end\":156990,\"start\":156989},{\"end\":156997,\"start\":156996},{\"end\":157006,\"start\":157005},{\"end\":157013,\"start\":157012},{\"end\":157030,\"start\":157029},{\"end\":157041,\"start\":157040},{\"end\":157295,\"start\":157294},{\"end\":157297,\"start\":157296},{\"end\":157308,\"start\":157307},{\"end\":157310,\"start\":157309},{\"end\":157495,\"start\":157494},{\"end\":157501,\"start\":157500},{\"end\":157508,\"start\":157507},{\"end\":157516,\"start\":157515},{\"end\":157737,\"start\":157733},{\"end\":157749,\"start\":157745},{\"end\":157755,\"start\":157754},{\"end\":157762,\"start\":157761},{\"end\":157768,\"start\":157767},{\"end\":157770,\"start\":157769},{\"end\":158005,\"start\":158004},{\"end\":158007,\"start\":158006},{\"end\":158018,\"start\":158017},{\"end\":158020,\"start\":158019},{\"end\":158242,\"start\":158241},{\"end\":158251,\"start\":158250},{\"end\":158257,\"start\":158256},{\"end\":158264,\"start\":158263},{\"end\":158273,\"start\":158272},{\"end\":158280,\"start\":158279},{\"end\":158543,\"start\":158542},{\"end\":158549,\"start\":158548},{\"end\":158559,\"start\":158555},{\"end\":158820,\"start\":158819},{\"end\":158828,\"start\":158827},{\"end\":158835,\"start\":158834},{\"end\":159028,\"start\":159027},{\"end\":159035,\"start\":159034},{\"end\":159042,\"start\":159041},{\"end\":159044,\"start\":159043},{\"end\":159052,\"start\":159051},{\"end\":159272,\"start\":159271},{\"end\":159274,\"start\":159273},{\"end\":159283,\"start\":159282},{\"end\":159285,\"start\":159284},{\"end\":159295,\"start\":159294},{\"end\":159307,\"start\":159306},{\"end\":159651,\"start\":159650},{\"end\":159664,\"start\":159663},{\"end\":159672,\"start\":159671},{\"end\":159969,\"start\":159968},{\"end\":159976,\"start\":159975},{\"end\":160141,\"start\":160140},{\"end\":160156,\"start\":160155},{\"end\":160169,\"start\":160168},{\"end\":160181,\"start\":160180},{\"end\":160396,\"start\":160395},{\"end\":160404,\"start\":160403},{\"end\":160415,\"start\":160411},{\"end\":160633,\"start\":160632},{\"end\":160640,\"start\":160639},{\"end\":160647,\"start\":160646},{\"end\":160653,\"start\":160652},{\"end\":160661,\"start\":160660},{\"end\":160669,\"start\":160668},{\"end\":160915,\"start\":160914},{\"end\":160927,\"start\":160926},{\"end\":161201,\"start\":161200},{\"end\":161209,\"start\":161208},{\"end\":161211,\"start\":161210},{\"end\":161469,\"start\":161468},{\"end\":161480,\"start\":161479},{\"end\":161493,\"start\":161492},{\"end\":161502,\"start\":161501},{\"end\":161512,\"start\":161511},{\"end\":161733,\"start\":161732},{\"end\":161739,\"start\":161738},{\"end\":161748,\"start\":161747},{\"end\":161755,\"start\":161754},{\"end\":161996,\"start\":161995},{\"end\":162005,\"start\":162004},{\"end\":162282,\"start\":162281},{\"end\":162289,\"start\":162288},{\"end\":162297,\"start\":162296},{\"end\":162548,\"start\":162547},{\"end\":162555,\"start\":162554},{\"end\":162562,\"start\":162561},{\"end\":162570,\"start\":162569},{\"end\":162578,\"start\":162577},{\"end\":162585,\"start\":162584},{\"end\":162845,\"start\":162844},{\"end\":162853,\"start\":162852},{\"end\":162861,\"start\":162860},{\"end\":162874,\"start\":162870},{\"end\":162880,\"start\":162879},{\"end\":162886,\"start\":162885},{\"end\":163116,\"start\":163115},{\"end\":163122,\"start\":163121},{\"end\":163128,\"start\":163127},{\"end\":163137,\"start\":163136},{\"end\":163143,\"start\":163142},{\"end\":163151,\"start\":163150},{\"end\":163436,\"start\":163435},{\"end\":163444,\"start\":163443},{\"end\":163451,\"start\":163450},{\"end\":163459,\"start\":163458},{\"end\":163468,\"start\":163467},{\"end\":163763,\"start\":163762},{\"end\":163771,\"start\":163770},{\"end\":163779,\"start\":163778},{\"end\":163787,\"start\":163786},{\"end\":163794,\"start\":163793},{\"end\":164036,\"start\":164035},{\"end\":164043,\"start\":164042},{\"end\":164051,\"start\":164050},{\"end\":164057,\"start\":164056},{\"end\":164064,\"start\":164063},{\"end\":164250,\"start\":164249},{\"end\":164259,\"start\":164258},{\"end\":164269,\"start\":164265},{\"end\":164472,\"start\":164471},{\"end\":164478,\"start\":164477},{\"end\":164484,\"start\":164483},{\"end\":164493,\"start\":164492},{\"end\":164502,\"start\":164501},{\"end\":164742,\"start\":164741},{\"end\":164749,\"start\":164748},{\"end\":164761,\"start\":164760},{\"end\":165008,\"start\":165007},{\"end\":165017,\"start\":165016},{\"end\":165019,\"start\":165018},{\"end\":165223,\"start\":165222},{\"end\":165225,\"start\":165224},{\"end\":165236,\"start\":165235},{\"end\":165251,\"start\":165250},{\"end\":165253,\"start\":165252},{\"end\":165265,\"start\":165264},{\"end\":165276,\"start\":165275},{\"end\":165698,\"start\":165697},{\"end\":165705,\"start\":165704},{\"end\":165712,\"start\":165711}]", "bib_author_last_name": "[{\"end\":106599,\"start\":106594},{\"end\":106607,\"start\":106603},{\"end\":106615,\"start\":106611},{\"end\":106622,\"start\":106619},{\"end\":106628,\"start\":106626},{\"end\":106913,\"start\":106902},{\"end\":106931,\"start\":106919},{\"end\":107184,\"start\":107173},{\"end\":107201,\"start\":107190},{\"end\":107211,\"start\":107207},{\"end\":107465,\"start\":107461},{\"end\":107473,\"start\":107469},{\"end\":107659,\"start\":107656},{\"end\":107668,\"start\":107663},{\"end\":107678,\"start\":107674},{\"end\":107943,\"start\":107936},{\"end\":107954,\"start\":107949},{\"end\":108237,\"start\":108234},{\"end\":108244,\"start\":108241},{\"end\":108252,\"start\":108248},{\"end\":108267,\"start\":108258},{\"end\":108550,\"start\":108543},{\"end\":108566,\"start\":108554},{\"end\":108576,\"start\":108570},{\"end\":108584,\"start\":108580},{\"end\":108842,\"start\":108837},{\"end\":108849,\"start\":108846},{\"end\":108857,\"start\":108853},{\"end\":109112,\"start\":109108},{\"end\":109118,\"start\":109116},{\"end\":109125,\"start\":109122},{\"end\":109134,\"start\":109129},{\"end\":109143,\"start\":109138},{\"end\":109149,\"start\":109147},{\"end\":109156,\"start\":109153},{\"end\":109420,\"start\":109417},{\"end\":109428,\"start\":109424},{\"end\":109436,\"start\":109432},{\"end\":109442,\"start\":109440},{\"end\":109718,\"start\":109716},{\"end\":109727,\"start\":109722},{\"end\":109735,\"start\":109731},{\"end\":109998,\"start\":109992},{\"end\":110007,\"start\":110002},{\"end\":110016,\"start\":110011},{\"end\":110026,\"start\":110020},{\"end\":110248,\"start\":110244},{\"end\":110255,\"start\":110252},{\"end\":110440,\"start\":110434},{\"end\":110448,\"start\":110444},{\"end\":110626,\"start\":110617},{\"end\":110639,\"start\":110630},{\"end\":110802,\"start\":110796},{\"end\":110815,\"start\":110806},{\"end\":110828,\"start\":110819},{\"end\":111038,\"start\":111033},{\"end\":111046,\"start\":111042},{\"end\":111058,\"start\":111053},{\"end\":111069,\"start\":111065},{\"end\":111315,\"start\":111310},{\"end\":111321,\"start\":111319},{\"end\":111335,\"start\":111327},{\"end\":111346,\"start\":111341},{\"end\":111589,\"start\":111568},{\"end\":111606,\"start\":111593},{\"end\":111622,\"start\":111610},{\"end\":111642,\"start\":111626},{\"end\":111657,\"start\":111646},{\"end\":111947,\"start\":111943},{\"end\":111959,\"start\":111951},{\"end\":112146,\"start\":112137},{\"end\":112159,\"start\":112150},{\"end\":112174,\"start\":112165},{\"end\":112379,\"start\":112373},{\"end\":112389,\"start\":112383},{\"end\":112403,\"start\":112393},{\"end\":112413,\"start\":112407},{\"end\":112610,\"start\":112606},{\"end\":112625,\"start\":112616},{\"end\":112638,\"start\":112629},{\"end\":112652,\"start\":112644},{\"end\":112661,\"start\":112659},{\"end\":112887,\"start\":112883},{\"end\":112895,\"start\":112891},{\"end\":112909,\"start\":112906},{\"end\":112917,\"start\":112913},{\"end\":113169,\"start\":113165},{\"end\":113177,\"start\":113173},{\"end\":113185,\"start\":113181},{\"end\":113462,\"start\":113458},{\"end\":113476,\"start\":113471},{\"end\":113491,\"start\":113480},{\"end\":113777,\"start\":113773},{\"end\":113784,\"start\":113781},{\"end\":113791,\"start\":113788},{\"end\":113797,\"start\":113795},{\"end\":114026,\"start\":114019},{\"end\":114037,\"start\":114030},{\"end\":114048,\"start\":114041},{\"end\":114319,\"start\":114317},{\"end\":114328,\"start\":114323},{\"end\":114337,\"start\":114332},{\"end\":114344,\"start\":114341},{\"end\":114593,\"start\":114588},{\"end\":114604,\"start\":114597},{\"end\":114615,\"start\":114608},{\"end\":114820,\"start\":114811},{\"end\":114833,\"start\":114826},{\"end\":115006,\"start\":114998},{\"end\":115165,\"start\":115162},{\"end\":115171,\"start\":115169},{\"end\":115183,\"start\":115175},{\"end\":115190,\"start\":115187},{\"end\":115373,\"start\":115371},{\"end\":115385,\"start\":115377},{\"end\":115395,\"start\":115389},{\"end\":115407,\"start\":115399},{\"end\":115569,\"start\":115563},{\"end\":115580,\"start\":115573},{\"end\":115592,\"start\":115584},{\"end\":115603,\"start\":115596},{\"end\":115794,\"start\":115791},{\"end\":115806,\"start\":115798},{\"end\":115815,\"start\":115810},{\"end\":115826,\"start\":115819},{\"end\":115834,\"start\":115830},{\"end\":115843,\"start\":115841},{\"end\":115853,\"start\":115849},{\"end\":116098,\"start\":116092},{\"end\":116109,\"start\":116102},{\"end\":116120,\"start\":116113},{\"end\":116128,\"start\":116124},{\"end\":116361,\"start\":116357},{\"end\":116376,\"start\":116365},{\"end\":116557,\"start\":116553},{\"end\":116566,\"start\":116563},{\"end\":116574,\"start\":116570},{\"end\":116583,\"start\":116578},{\"end\":116806,\"start\":116802},{\"end\":116816,\"start\":116810},{\"end\":116826,\"start\":116820},{\"end\":116839,\"start\":116830},{\"end\":117049,\"start\":117045},{\"end\":117277,\"start\":117272},{\"end\":117292,\"start\":117281},{\"end\":117303,\"start\":117296},{\"end\":117550,\"start\":117545},{\"end\":117560,\"start\":117554},{\"end\":117789,\"start\":117781},{\"end\":117802,\"start\":117793},{\"end\":117993,\"start\":117985},{\"end\":118003,\"start\":117997},{\"end\":118211,\"start\":118207},{\"end\":118220,\"start\":118215},{\"end\":118230,\"start\":118224},{\"end\":118245,\"start\":118234},{\"end\":118252,\"start\":118249},{\"end\":118542,\"start\":118538},{\"end\":118554,\"start\":118546},{\"end\":118567,\"start\":118558},{\"end\":118576,\"start\":118571},{\"end\":118838,\"start\":118834},{\"end\":118847,\"start\":118842},{\"end\":118855,\"start\":118851},{\"end\":118862,\"start\":118859},{\"end\":118869,\"start\":118866},{\"end\":119101,\"start\":119099},{\"end\":119107,\"start\":119105},{\"end\":119113,\"start\":119111},{\"end\":119120,\"start\":119117},{\"end\":119126,\"start\":119124},{\"end\":119133,\"start\":119130},{\"end\":119378,\"start\":119373},{\"end\":119384,\"start\":119382},{\"end\":119392,\"start\":119388},{\"end\":119400,\"start\":119396},{\"end\":119661,\"start\":119659},{\"end\":119670,\"start\":119665},{\"end\":119679,\"start\":119676},{\"end\":119685,\"start\":119683},{\"end\":119884,\"start\":119882},{\"end\":119891,\"start\":119888},{\"end\":119904,\"start\":119895},{\"end\":120103,\"start\":120099},{\"end\":120111,\"start\":120107},{\"end\":120120,\"start\":120115},{\"end\":120135,\"start\":120132},{\"end\":120411,\"start\":120404},{\"end\":120422,\"start\":120417},{\"end\":120674,\"start\":120671},{\"end\":120681,\"start\":120678},{\"end\":120689,\"start\":120685},{\"end\":120697,\"start\":120693},{\"end\":120914,\"start\":120911},{\"end\":120924,\"start\":120920},{\"end\":120934,\"start\":120928},{\"end\":120944,\"start\":120940},{\"end\":121231,\"start\":121225},{\"end\":121242,\"start\":121237},{\"end\":121252,\"start\":121248},{\"end\":121516,\"start\":121508},{\"end\":121526,\"start\":121520},{\"end\":121538,\"start\":121530},{\"end\":121829,\"start\":121826},{\"end\":121837,\"start\":121833},{\"end\":121845,\"start\":121841},{\"end\":121854,\"start\":121849},{\"end\":122106,\"start\":122098},{\"end\":122118,\"start\":122113},{\"end\":122305,\"start\":122303},{\"end\":122314,\"start\":122309},{\"end\":122322,\"start\":122318},{\"end\":122328,\"start\":122326},{\"end\":122336,\"start\":122332},{\"end\":122343,\"start\":122340},{\"end\":122573,\"start\":122569},{\"end\":122583,\"start\":122577},{\"end\":122596,\"start\":122587},{\"end\":122606,\"start\":122600},{\"end\":122868,\"start\":122863},{\"end\":122880,\"start\":122874},{\"end\":122891,\"start\":122884},{\"end\":123193,\"start\":123186},{\"end\":123204,\"start\":123197},{\"end\":123212,\"start\":123208},{\"end\":123227,\"start\":123218},{\"end\":123528,\"start\":123523},{\"end\":123539,\"start\":123534},{\"end\":123774,\"start\":123767},{\"end\":123785,\"start\":123780},{\"end\":124040,\"start\":124032},{\"end\":124049,\"start\":124044},{\"end\":124066,\"start\":124055},{\"end\":124350,\"start\":124345},{\"end\":124356,\"start\":124354},{\"end\":124363,\"start\":124360},{\"end\":124581,\"start\":124578},{\"end\":124590,\"start\":124585},{\"end\":124597,\"start\":124594},{\"end\":124604,\"start\":124601},{\"end\":124610,\"start\":124608},{\"end\":124622,\"start\":124617},{\"end\":124631,\"start\":124626},{\"end\":124872,\"start\":124869},{\"end\":124880,\"start\":124876},{\"end\":124892,\"start\":124886},{\"end\":124899,\"start\":124896},{\"end\":125171,\"start\":125168},{\"end\":125183,\"start\":125177},{\"end\":125196,\"start\":125187},{\"end\":125204,\"start\":125200},{\"end\":125216,\"start\":125206},{\"end\":125465,\"start\":125459},{\"end\":125471,\"start\":125469},{\"end\":125479,\"start\":125475},{\"end\":125661,\"start\":125657},{\"end\":125667,\"start\":125665},{\"end\":125673,\"start\":125671},{\"end\":125681,\"start\":125677},{\"end\":125687,\"start\":125685},{\"end\":125695,\"start\":125691},{\"end\":125931,\"start\":125928},{\"end\":125940,\"start\":125935},{\"end\":125947,\"start\":125944},{\"end\":125953,\"start\":125951},{\"end\":126203,\"start\":126197},{\"end\":126213,\"start\":126207},{\"end\":126223,\"start\":126217},{\"end\":126232,\"start\":126227},{\"end\":126246,\"start\":126236},{\"end\":126257,\"start\":126250},{\"end\":126265,\"start\":126261},{\"end\":126577,\"start\":126574},{\"end\":126594,\"start\":126581},{\"end\":126608,\"start\":126600},{\"end\":126900,\"start\":126891},{\"end\":126908,\"start\":126904},{\"end\":126915,\"start\":126912},{\"end\":127148,\"start\":127144},{\"end\":127155,\"start\":127152},{\"end\":127164,\"start\":127159},{\"end\":127174,\"start\":127170},{\"end\":127351,\"start\":127349},{\"end\":127360,\"start\":127355},{\"end\":127366,\"start\":127364},{\"end\":127372,\"start\":127370},{\"end\":127380,\"start\":127376},{\"end\":127386,\"start\":127384},{\"end\":127840,\"start\":127831},{\"end\":127847,\"start\":127844},{\"end\":127855,\"start\":127851},{\"end\":128141,\"start\":128138},{\"end\":128149,\"start\":128145},{\"end\":128156,\"start\":128153},{\"end\":128163,\"start\":128160},{\"end\":128170,\"start\":128167},{\"end\":128176,\"start\":128174},{\"end\":128436,\"start\":128432},{\"end\":128443,\"start\":128440},{\"end\":128450,\"start\":128447},{\"end\":128458,\"start\":128454},{\"end\":128690,\"start\":128687},{\"end\":128698,\"start\":128694},{\"end\":128704,\"start\":128702},{\"end\":128712,\"start\":128708},{\"end\":128892,\"start\":128889},{\"end\":128904,\"start\":128896},{\"end\":128911,\"start\":128908},{\"end\":129072,\"start\":129069},{\"end\":129085,\"start\":129080},{\"end\":129290,\"start\":129287},{\"end\":129298,\"start\":129294},{\"end\":129304,\"start\":129302},{\"end\":129525,\"start\":129522},{\"end\":129533,\"start\":129529},{\"end\":129539,\"start\":129537},{\"end\":129546,\"start\":129543},{\"end\":129554,\"start\":129550},{\"end\":129561,\"start\":129558},{\"end\":129846,\"start\":129839},{\"end\":129855,\"start\":129850},{\"end\":130110,\"start\":130107},{\"end\":130127,\"start\":130114},{\"end\":130141,\"start\":130133},{\"end\":130369,\"start\":130364},{\"end\":130377,\"start\":130373},{\"end\":130386,\"start\":130381},{\"end\":130394,\"start\":130390},{\"end\":130401,\"start\":130398},{\"end\":130413,\"start\":130405},{\"end\":130421,\"start\":130417},{\"end\":130685,\"start\":130682},{\"end\":130692,\"start\":130689},{\"end\":130698,\"start\":130696},{\"end\":130708,\"start\":130704},{\"end\":130715,\"start\":130712},{\"end\":130957,\"start\":130954},{\"end\":130965,\"start\":130961},{\"end\":130971,\"start\":130969},{\"end\":131205,\"start\":131201},{\"end\":131214,\"start\":131209},{\"end\":131223,\"start\":131218},{\"end\":131232,\"start\":131227},{\"end\":131481,\"start\":131470},{\"end\":131486,\"start\":131483},{\"end\":131496,\"start\":131490},{\"end\":131504,\"start\":131498},{\"end\":131714,\"start\":131707},{\"end\":131725,\"start\":131720},{\"end\":131940,\"start\":131938},{\"end\":131947,\"start\":131944},{\"end\":131953,\"start\":131951},{\"end\":131960,\"start\":131957},{\"end\":131966,\"start\":131964},{\"end\":131973,\"start\":131970},{\"end\":132200,\"start\":132195},{\"end\":132208,\"start\":132204},{\"end\":132216,\"start\":132212},{\"end\":132223,\"start\":132220},{\"end\":132231,\"start\":132227},{\"end\":132238,\"start\":132235},{\"end\":132246,\"start\":132242},{\"end\":132506,\"start\":132501},{\"end\":132512,\"start\":132510},{\"end\":132518,\"start\":132516},{\"end\":132526,\"start\":132522},{\"end\":132536,\"start\":132530},{\"end\":132549,\"start\":132540},{\"end\":132810,\"start\":132801},{\"end\":132817,\"start\":132814},{\"end\":132827,\"start\":132823},{\"end\":132835,\"start\":132831},{\"end\":133060,\"start\":133050},{\"end\":133073,\"start\":133064},{\"end\":133085,\"start\":133079},{\"end\":133282,\"start\":133272},{\"end\":133299,\"start\":133286},{\"end\":133308,\"start\":133303},{\"end\":133314,\"start\":133312},{\"end\":133330,\"start\":133318},{\"end\":133339,\"start\":133334},{\"end\":133352,\"start\":133343},{\"end\":133362,\"start\":133356},{\"end\":133621,\"start\":133618},{\"end\":133631,\"start\":133627},{\"end\":133800,\"start\":133791},{\"end\":133812,\"start\":133804},{\"end\":133825,\"start\":133816},{\"end\":134076,\"start\":134069},{\"end\":134085,\"start\":134080},{\"end\":134095,\"start\":134089},{\"end\":134387,\"start\":134384},{\"end\":134393,\"start\":134391},{\"end\":134403,\"start\":134400},{\"end\":134411,\"start\":134407},{\"end\":134649,\"start\":134644},{\"end\":134657,\"start\":134653},{\"end\":134664,\"start\":134661},{\"end\":134670,\"start\":134668},{\"end\":134678,\"start\":134674},{\"end\":134685,\"start\":134682},{\"end\":134908,\"start\":134901},{\"end\":134915,\"start\":134912},{\"end\":134922,\"start\":134919},{\"end\":134934,\"start\":134926},{\"end\":134942,\"start\":134938},{\"end\":134954,\"start\":134946},{\"end\":134963,\"start\":134958},{\"end\":134976,\"start\":134967},{\"end\":134990,\"start\":134980},{\"end\":135250,\"start\":135239},{\"end\":135261,\"start\":135254},{\"end\":135269,\"start\":135265},{\"end\":135480,\"start\":135473},{\"end\":135702,\"start\":135698},{\"end\":135708,\"start\":135706},{\"end\":135718,\"start\":135714},{\"end\":135951,\"start\":135946},{\"end\":135958,\"start\":135955},{\"end\":135970,\"start\":135965},{\"end\":135977,\"start\":135974},{\"end\":135988,\"start\":135984},{\"end\":135998,\"start\":135994},{\"end\":136007,\"start\":136002},{\"end\":136015,\"start\":136011},{\"end\":136251,\"start\":136247},{\"end\":136258,\"start\":136255},{\"end\":136266,\"start\":136262},{\"end\":136461,\"start\":136459},{\"end\":136470,\"start\":136465},{\"end\":136476,\"start\":136474},{\"end\":136484,\"start\":136480},{\"end\":136496,\"start\":136488},{\"end\":136503,\"start\":136500},{\"end\":136770,\"start\":136766},{\"end\":136782,\"start\":136774},{\"end\":137020,\"start\":137015},{\"end\":137028,\"start\":137024},{\"end\":137035,\"start\":137032},{\"end\":137041,\"start\":137039},{\"end\":137049,\"start\":137045},{\"end\":137056,\"start\":137053},{\"end\":137279,\"start\":137271},{\"end\":137289,\"start\":137285},{\"end\":137535,\"start\":137526},{\"end\":137543,\"start\":137539},{\"end\":137551,\"start\":137547},{\"end\":137558,\"start\":137555},{\"end\":137855,\"start\":137851},{\"end\":137864,\"start\":137859},{\"end\":137871,\"start\":137868},{\"end\":137879,\"start\":137875},{\"end\":137889,\"start\":137885},{\"end\":138150,\"start\":138146},{\"end\":138156,\"start\":138154},{\"end\":138163,\"start\":138160},{\"end\":138337,\"start\":138330},{\"end\":138346,\"start\":138341},{\"end\":138362,\"start\":138350},{\"end\":138372,\"start\":138366},{\"end\":138380,\"start\":138376},{\"end\":138541,\"start\":138539},{\"end\":138548,\"start\":138545},{\"end\":138561,\"start\":138552},{\"end\":138786,\"start\":138779},{\"end\":138800,\"start\":138790},{\"end\":138810,\"start\":138804},{\"end\":138821,\"start\":138816},{\"end\":138840,\"start\":138827},{\"end\":139173,\"start\":139169},{\"end\":139184,\"start\":139179},{\"end\":139196,\"start\":139190},{\"end\":139212,\"start\":139202},{\"end\":139517,\"start\":139514},{\"end\":139525,\"start\":139521},{\"end\":139533,\"start\":139529},{\"end\":139821,\"start\":139818},{\"end\":139828,\"start\":139825},{\"end\":139836,\"start\":139832},{\"end\":139844,\"start\":139840},{\"end\":140115,\"start\":140112},{\"end\":140122,\"start\":140119},{\"end\":140130,\"start\":140126},{\"end\":140136,\"start\":140134},{\"end\":140377,\"start\":140364},{\"end\":140389,\"start\":140381},{\"end\":140404,\"start\":140395},{\"end\":140413,\"start\":140408},{\"end\":140423,\"start\":140417},{\"end\":140668,\"start\":140663},{\"end\":140678,\"start\":140674},{\"end\":140944,\"start\":140940},{\"end\":140950,\"start\":140948},{\"end\":140959,\"start\":140954},{\"end\":140966,\"start\":140963},{\"end\":140973,\"start\":140970},{\"end\":141247,\"start\":141243},{\"end\":141255,\"start\":141251},{\"end\":141262,\"start\":141259},{\"end\":141269,\"start\":141266},{\"end\":141275,\"start\":141273},{\"end\":141520,\"start\":141517},{\"end\":141528,\"start\":141524},{\"end\":141535,\"start\":141532},{\"end\":141543,\"start\":141539},{\"end\":141551,\"start\":141547},{\"end\":141558,\"start\":141555},{\"end\":141566,\"start\":141562},{\"end\":141835,\"start\":141828},{\"end\":141846,\"start\":141839},{\"end\":141857,\"start\":141852},{\"end\":142141,\"start\":142132},{\"end\":142148,\"start\":142145},{\"end\":142160,\"start\":142152},{\"end\":142436,\"start\":142433},{\"end\":142442,\"start\":142440},{\"end\":142449,\"start\":142446},{\"end\":142455,\"start\":142453},{\"end\":142463,\"start\":142459},{\"end\":142469,\"start\":142467},{\"end\":142476,\"start\":142473},{\"end\":142825,\"start\":142822},{\"end\":142833,\"start\":142829},{\"end\":142843,\"start\":142840},{\"end\":143095,\"start\":143090},{\"end\":143107,\"start\":143099},{\"end\":143278,\"start\":143275},{\"end\":143287,\"start\":143284},{\"end\":143296,\"start\":143293},{\"end\":143306,\"start\":143302},{\"end\":143551,\"start\":143546},{\"end\":143564,\"start\":143555},{\"end\":143578,\"start\":143570},{\"end\":143591,\"start\":143586},{\"end\":143830,\"start\":143827},{\"end\":143838,\"start\":143834},{\"end\":143844,\"start\":143842},{\"end\":143850,\"start\":143848},{\"end\":144084,\"start\":144080},{\"end\":144090,\"start\":144088},{\"end\":144098,\"start\":144094},{\"end\":144106,\"start\":144102},{\"end\":144357,\"start\":144355},{\"end\":144366,\"start\":144361},{\"end\":144374,\"start\":144370},{\"end\":144382,\"start\":144378},{\"end\":144388,\"start\":144386},{\"end\":144652,\"start\":144645},{\"end\":144658,\"start\":144656},{\"end\":144665,\"start\":144662},{\"end\":144680,\"start\":144669},{\"end\":144686,\"start\":144684},{\"end\":144969,\"start\":144967},{\"end\":144977,\"start\":144973},{\"end\":144986,\"start\":144981},{\"end\":144992,\"start\":144990},{\"end\":145000,\"start\":144996},{\"end\":145274,\"start\":145270},{\"end\":145281,\"start\":145278},{\"end\":145287,\"start\":145285},{\"end\":145295,\"start\":145291},{\"end\":145304,\"start\":145299},{\"end\":145607,\"start\":145602},{\"end\":145614,\"start\":145611},{\"end\":145621,\"start\":145618},{\"end\":145627,\"start\":145625},{\"end\":145635,\"start\":145631},{\"end\":145941,\"start\":145933},{\"end\":145950,\"start\":145945},{\"end\":145963,\"start\":145954},{\"end\":145973,\"start\":145967},{\"end\":146209,\"start\":146207},{\"end\":146215,\"start\":146213},{\"end\":146224,\"start\":146219},{\"end\":146231,\"start\":146228},{\"end\":146239,\"start\":146235},{\"end\":146245,\"start\":146243},{\"end\":146479,\"start\":146475},{\"end\":146487,\"start\":146483},{\"end\":146710,\"start\":146705},{\"end\":146719,\"start\":146714},{\"end\":146725,\"start\":146723},{\"end\":146732,\"start\":146729},{\"end\":146739,\"start\":146736},{\"end\":146745,\"start\":146743},{\"end\":146753,\"start\":146749},{\"end\":146759,\"start\":146757},{\"end\":146767,\"start\":146763},{\"end\":146773,\"start\":146771},{\"end\":147047,\"start\":147044},{\"end\":147059,\"start\":147053},{\"end\":147068,\"start\":147065},{\"end\":147351,\"start\":147345},{\"end\":147360,\"start\":147355},{\"end\":147371,\"start\":147364},{\"end\":147381,\"start\":147375},{\"end\":147603,\"start\":147599},{\"end\":147610,\"start\":147607},{\"end\":147620,\"start\":147617},{\"end\":147628,\"start\":147624},{\"end\":147847,\"start\":147840},{\"end\":147857,\"start\":147851},{\"end\":147864,\"start\":147861},{\"end\":147877,\"start\":147868},{\"end\":147887,\"start\":147881},{\"end\":148163,\"start\":148158},{\"end\":148169,\"start\":148167},{\"end\":148183,\"start\":148175},{\"end\":148194,\"start\":148189},{\"end\":148658,\"start\":148653},{\"end\":148668,\"start\":148665},{\"end\":148677,\"start\":148674},{\"end\":148921,\"start\":148919},{\"end\":148928,\"start\":148925},{\"end\":148936,\"start\":148932},{\"end\":148946,\"start\":148940},{\"end\":148954,\"start\":148950},{\"end\":149253,\"start\":149245},{\"end\":149261,\"start\":149257},{\"end\":149273,\"start\":149265},{\"end\":149284,\"start\":149277},{\"end\":149298,\"start\":149288},{\"end\":149564,\"start\":149561},{\"end\":149571,\"start\":149568},{\"end\":149579,\"start\":149575},{\"end\":149587,\"start\":149583},{\"end\":149593,\"start\":149591},{\"end\":149825,\"start\":149822},{\"end\":149832,\"start\":149829},{\"end\":149845,\"start\":149841},{\"end\":150070,\"start\":150067},{\"end\":150079,\"start\":150076},{\"end\":150091,\"start\":150083},{\"end\":150098,\"start\":150095},{\"end\":150312,\"start\":150309},{\"end\":150319,\"start\":150316},{\"end\":150325,\"start\":150323},{\"end\":150332,\"start\":150329},{\"end\":150340,\"start\":150336},{\"end\":150349,\"start\":150344},{\"end\":150539,\"start\":150537},{\"end\":150547,\"start\":150543},{\"end\":150554,\"start\":150551},{\"end\":150723,\"start\":150720},{\"end\":150734,\"start\":150729},{\"end\":150950,\"start\":150946},{\"end\":150959,\"start\":150956},{\"end\":150967,\"start\":150963},{\"end\":150973,\"start\":150971},{\"end\":150980,\"start\":150977},{\"end\":151211,\"start\":151208},{\"end\":151219,\"start\":151215},{\"end\":151229,\"start\":151223},{\"end\":151235,\"start\":151233},{\"end\":151247,\"start\":151241},{\"end\":151255,\"start\":151251},{\"end\":151467,\"start\":151465},{\"end\":151477,\"start\":151471},{\"end\":151754,\"start\":151750},{\"end\":151768,\"start\":151758},{\"end\":151780,\"start\":151772},{\"end\":151790,\"start\":151784},{\"end\":151802,\"start\":151796},{\"end\":152116,\"start\":152112},{\"end\":152130,\"start\":152120},{\"end\":152141,\"start\":152134},{\"end\":152149,\"start\":152145},{\"end\":152403,\"start\":152399},{\"end\":152416,\"start\":152407},{\"end\":152427,\"start\":152420},{\"end\":152606,\"start\":152603},{\"end\":152612,\"start\":152610},{\"end\":152621,\"start\":152616},{\"end\":152627,\"start\":152625},{\"end\":152636,\"start\":152631},{\"end\":152642,\"start\":152640},{\"end\":152649,\"start\":152646},{\"end\":152873,\"start\":152869},{\"end\":152881,\"start\":152877},{\"end\":152887,\"start\":152885},{\"end\":152893,\"start\":152891},{\"end\":152900,\"start\":152897},{\"end\":153119,\"start\":153115},{\"end\":153127,\"start\":153123},{\"end\":153135,\"start\":153131},{\"end\":153366,\"start\":153358},{\"end\":153378,\"start\":153370},{\"end\":153391,\"start\":153384},{\"end\":153594,\"start\":153592},{\"end\":153827,\"start\":153817},{\"end\":153837,\"start\":153831},{\"end\":154051,\"start\":154048},{\"end\":154058,\"start\":154055},{\"end\":154064,\"start\":154062},{\"end\":154071,\"start\":154068},{\"end\":154081,\"start\":154075},{\"end\":154088,\"start\":154085},{\"end\":154303,\"start\":154298},{\"end\":154310,\"start\":154307},{\"end\":154318,\"start\":154314},{\"end\":154325,\"start\":154322},{\"end\":154333,\"start\":154329},{\"end\":154340,\"start\":154337},{\"end\":154348,\"start\":154344},{\"end\":154552,\"start\":154548},{\"end\":154564,\"start\":154556},{\"end\":154573,\"start\":154568},{\"end\":154579,\"start\":154577},{\"end\":154775,\"start\":154772},{\"end\":154785,\"start\":154782},{\"end\":154793,\"start\":154789},{\"end\":154805,\"start\":154800},{\"end\":155060,\"start\":155056},{\"end\":155075,\"start\":155064},{\"end\":155278,\"start\":155273},{\"end\":155284,\"start\":155282},{\"end\":155293,\"start\":155288},{\"end\":155299,\"start\":155297},{\"end\":155306,\"start\":155303},{\"end\":155312,\"start\":155310},{\"end\":155319,\"start\":155316},{\"end\":155570,\"start\":155568},{\"end\":155579,\"start\":155574},{\"end\":155586,\"start\":155583},{\"end\":155593,\"start\":155590},{\"end\":155834,\"start\":155830},{\"end\":155842,\"start\":155838},{\"end\":155849,\"start\":155846},{\"end\":155856,\"start\":155853},{\"end\":155864,\"start\":155860},{\"end\":155871,\"start\":155868},{\"end\":156144,\"start\":156140},{\"end\":156150,\"start\":156148},{\"end\":156157,\"start\":156154},{\"end\":156164,\"start\":156161},{\"end\":156173,\"start\":156168},{\"end\":156384,\"start\":156379},{\"end\":156390,\"start\":156388},{\"end\":156397,\"start\":156394},{\"end\":156404,\"start\":156401},{\"end\":156559,\"start\":156557},{\"end\":156570,\"start\":156565},{\"end\":156586,\"start\":156576},{\"end\":156738,\"start\":156732},{\"end\":156749,\"start\":156742},{\"end\":156759,\"start\":156755},{\"end\":156994,\"start\":156991},{\"end\":157003,\"start\":156998},{\"end\":157010,\"start\":157007},{\"end\":157027,\"start\":157014},{\"end\":157038,\"start\":157031},{\"end\":157044,\"start\":157042},{\"end\":157305,\"start\":157298},{\"end\":157316,\"start\":157311},{\"end\":157498,\"start\":157496},{\"end\":157505,\"start\":157502},{\"end\":157513,\"start\":157509},{\"end\":157521,\"start\":157517},{\"end\":157743,\"start\":157738},{\"end\":157752,\"start\":157750},{\"end\":157759,\"start\":157756},{\"end\":157765,\"start\":157763},{\"end\":157780,\"start\":157771},{\"end\":158015,\"start\":158008},{\"end\":158026,\"start\":158021},{\"end\":158248,\"start\":158243},{\"end\":158254,\"start\":158252},{\"end\":158261,\"start\":158258},{\"end\":158270,\"start\":158265},{\"end\":158277,\"start\":158274},{\"end\":158285,\"start\":158281},{\"end\":158546,\"start\":158544},{\"end\":158553,\"start\":158550},{\"end\":158563,\"start\":158560},{\"end\":158825,\"start\":158821},{\"end\":158832,\"start\":158829},{\"end\":158840,\"start\":158836},{\"end\":159032,\"start\":159029},{\"end\":159039,\"start\":159036},{\"end\":159049,\"start\":159045},{\"end\":159055,\"start\":159053},{\"end\":159280,\"start\":159275},{\"end\":159292,\"start\":159286},{\"end\":159304,\"start\":159296},{\"end\":159317,\"start\":159308},{\"end\":159661,\"start\":159652},{\"end\":159669,\"start\":159665},{\"end\":159676,\"start\":159673},{\"end\":159973,\"start\":159970},{\"end\":159981,\"start\":159977},{\"end\":160153,\"start\":160142},{\"end\":160166,\"start\":160157},{\"end\":160178,\"start\":160170},{\"end\":160187,\"start\":160182},{\"end\":160401,\"start\":160397},{\"end\":160409,\"start\":160405},{\"end\":160420,\"start\":160416},{\"end\":160637,\"start\":160634},{\"end\":160644,\"start\":160641},{\"end\":160650,\"start\":160648},{\"end\":160658,\"start\":160654},{\"end\":160666,\"start\":160662},{\"end\":160673,\"start\":160670},{\"end\":160924,\"start\":160916},{\"end\":160937,\"start\":160928},{\"end\":161206,\"start\":161202},{\"end\":161219,\"start\":161212},{\"end\":161477,\"start\":161470},{\"end\":161490,\"start\":161481},{\"end\":161499,\"start\":161494},{\"end\":161509,\"start\":161503},{\"end\":161518,\"start\":161513},{\"end\":161736,\"start\":161734},{\"end\":161745,\"start\":161740},{\"end\":161752,\"start\":161749},{\"end\":161759,\"start\":161756},{\"end\":162002,\"start\":161997},{\"end\":162009,\"start\":162006},{\"end\":162286,\"start\":162283},{\"end\":162294,\"start\":162290},{\"end\":162302,\"start\":162298},{\"end\":162552,\"start\":162549},{\"end\":162559,\"start\":162556},{\"end\":162567,\"start\":162563},{\"end\":162575,\"start\":162571},{\"end\":162582,\"start\":162579},{\"end\":162589,\"start\":162586},{\"end\":162850,\"start\":162846},{\"end\":162858,\"start\":162854},{\"end\":162868,\"start\":162862},{\"end\":162877,\"start\":162875},{\"end\":162883,\"start\":162881},{\"end\":162894,\"start\":162887},{\"end\":163119,\"start\":163117},{\"end\":163125,\"start\":163123},{\"end\":163134,\"start\":163129},{\"end\":163140,\"start\":163138},{\"end\":163148,\"start\":163144},{\"end\":163154,\"start\":163152},{\"end\":163441,\"start\":163437},{\"end\":163448,\"start\":163445},{\"end\":163456,\"start\":163452},{\"end\":163465,\"start\":163460},{\"end\":163472,\"start\":163469},{\"end\":163768,\"start\":163764},{\"end\":163776,\"start\":163772},{\"end\":163784,\"start\":163780},{\"end\":163791,\"start\":163788},{\"end\":163797,\"start\":163795},{\"end\":164040,\"start\":164037},{\"end\":164048,\"start\":164044},{\"end\":164054,\"start\":164052},{\"end\":164061,\"start\":164058},{\"end\":164069,\"start\":164065},{\"end\":164256,\"start\":164251},{\"end\":164263,\"start\":164260},{\"end\":164275,\"start\":164270},{\"end\":164475,\"start\":164473},{\"end\":164481,\"start\":164479},{\"end\":164490,\"start\":164485},{\"end\":164499,\"start\":164494},{\"end\":164507,\"start\":164503},{\"end\":164746,\"start\":164743},{\"end\":164758,\"start\":164750},{\"end\":164765,\"start\":164762},{\"end\":165014,\"start\":165009},{\"end\":165024,\"start\":165020},{\"end\":165233,\"start\":165226},{\"end\":165248,\"start\":165237},{\"end\":165262,\"start\":165254},{\"end\":165273,\"start\":165266},{\"end\":165284,\"start\":165277},{\"end\":165702,\"start\":165699},{\"end\":165709,\"start\":165706},{\"end\":165717,\"start\":165713}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4545310},\"end\":106837,\"start\":106517},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":40499053},\"end\":107104,\"start\":106839},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":697405},\"end\":107386,\"start\":107106},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":21704501},\"end\":107591,\"start\":107388},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1089358},\"end\":107856,\"start\":107593},{\"attributes\":{\"id\":\"b5\"},\"end\":108128,\"start\":107858},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3740753},\"end\":108486,\"start\":108130},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":53701783},\"end\":108759,\"start\":108488},{\"attributes\":{\"id\":\"b8\"},\"end\":109019,\"start\":108761},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":71150872},\"end\":109344,\"start\":109021},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52955811},\"end\":109619,\"start\":109346},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3645757},\"end\":109936,\"start\":109621},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":58028958},\"end\":110174,\"start\":109938},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11060885},\"end\":110393,\"start\":110176},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":26569197},\"end\":110576,\"start\":110395},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":18018217},\"end\":110770,\"start\":110578},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":40576626},\"end\":110961,\"start\":110772},{\"attributes\":{\"id\":\"b17\"},\"end\":111220,\"start\":110963},{\"attributes\":{\"id\":\"b18\"},\"end\":111524,\"start\":111222},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16958950},\"end\":111871,\"start\":111526},{\"attributes\":{\"id\":\"b20\"},\"end\":112087,\"start\":111873},{\"attributes\":{\"doi\":\"2016. 1\",\"id\":\"b21\"},\"end\":112288,\"start\":112089},{\"attributes\":{\"id\":\"b22\"},\"end\":112576,\"start\":112290},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1417739},\"end\":112820,\"start\":112578},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7768768},\"end\":113064,\"start\":112822},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3085328},\"end\":113364,\"start\":113066},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9059102},\"end\":113713,\"start\":113366},{\"attributes\":{\"doi\":\"arXiv:2001.03360v1\",\"id\":\"b27\"},\"end\":113958,\"start\":113715},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7454625},\"end\":114197,\"start\":113960},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5157313},\"end\":114544,\"start\":114199},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14395688},\"end\":114751,\"start\":114546},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1192198},\"end\":114982,\"start\":114753},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206770307},\"end\":115079,\"start\":114984},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10328909},\"end\":115355,\"start\":115081},{\"attributes\":{\"id\":\"b34\"},\"end\":115502,\"start\":115357},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206594738},\"end\":115751,\"start\":115504},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2141740},\"end\":116022,\"start\":115753},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":9749221},\"end\":116303,\"start\":116024},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":749620},\"end\":116504,\"start\":116305},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1910869},\"end\":116752,\"start\":116506},{\"attributes\":{\"id\":\"b40\"},\"end\":116983,\"start\":116754},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5258236},\"end\":117183,\"start\":116985},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":16796921},\"end\":117487,\"start\":117185},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206590483},\"end\":117731,\"start\":117489},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206786900},\"end\":117927,\"start\":117733},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":7612598},\"end\":118141,\"start\":117929},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":16027263},\"end\":118429,\"start\":118143},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1718465},\"end\":118782,\"start\":118431},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4509294},\"end\":119029,\"start\":118784},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":31520628},\"end\":119302,\"start\":119031},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":2131202},\"end\":119583,\"start\":119304},{\"attributes\":{\"id\":\"b51\"},\"end\":119830,\"start\":119585},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1193481},\"end\":120030,\"start\":119832},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":8747356},\"end\":120296,\"start\":120032},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":3003101},\"end\":120613,\"start\":120298},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":91184269},\"end\":120822,\"start\":120615},{\"attributes\":{\"doi\":\"arXiv:1906.07538\",\"id\":\"b56\"},\"end\":121166,\"start\":120824},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":206485682},\"end\":121407,\"start\":121168},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":13945849},\"end\":121762,\"start\":121409},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":14034098},\"end\":122019,\"start\":121764},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":718943},\"end\":122265,\"start\":122021},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":4915696},\"end\":122492,\"start\":122267},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":15166260},\"end\":122775,\"start\":122494},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":13164291},\"end\":123068,\"start\":122777},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":207112729},\"end\":123469,\"start\":123070},{\"attributes\":{\"id\":\"b65\"},\"end\":123669,\"start\":123471},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":13709403},\"end\":123958,\"start\":123671},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":4320462},\"end\":124272,\"start\":123960},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":7738307},\"end\":124518,\"start\":124274},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":52243494},\"end\":124806,\"start\":124520},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":49558494},\"end\":125074,\"start\":124808},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":50785961},\"end\":125429,\"start\":125076},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":50785934},\"end\":125589,\"start\":125431},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":52860247},\"end\":125870,\"start\":125591},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":53281005},\"end\":126105,\"start\":125872},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":51901514},\"end\":126504,\"start\":126107},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":3787969},\"end\":126774,\"start\":126506},{\"attributes\":{\"id\":\"b77\"},\"end\":127104,\"start\":126776},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":53227020},\"end\":127297,\"start\":127106},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":54447074},\"end\":127532,\"start\":127299},{\"attributes\":{\"id\":\"b80\"},\"end\":127725,\"start\":127534},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":53850453},\"end\":128041,\"start\":127727},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":53957027},\"end\":128367,\"start\":128043},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":72941021},\"end\":128618,\"start\":128369},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":54461738},\"end\":128855,\"start\":128620},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":53783843},\"end\":129033,\"start\":128857},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":88516263},\"end\":129212,\"start\":129035},{\"attributes\":{\"id\":\"b87\"},\"end\":129445,\"start\":129214},{\"attributes\":{\"doi\":\"arXiv:1902.01115\",\"id\":\"b88\"},\"end\":129781,\"start\":129447},{\"attributes\":{\"doi\":\"arXiv:1903.11249\",\"id\":\"b89\"},\"end\":130032,\"start\":129783},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":62841646},\"end\":130287,\"start\":130034},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":67856061},\"end\":130618,\"start\":130289},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":197639806},\"end\":130871,\"start\":130620},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":195473446},\"end\":131130,\"start\":130873},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":198161572},\"end\":131403,\"start\":131132},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":70298800},\"end\":131640,\"start\":131405},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":198133773},\"end\":131853,\"start\":131642},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":198968173},\"end\":132142,\"start\":131855},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":207901201},\"end\":132409,\"start\":132144},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":202583780},\"end\":132725,\"start\":132411},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":70350007},\"end\":132981,\"start\":132727},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":195908774},\"end\":133239,\"start\":132983},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":1033682},\"end\":133545,\"start\":133241},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":19200040},\"end\":133757,\"start\":133547},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":6099034},\"end\":133955,\"start\":133759},{\"attributes\":{\"doi\":\"arXiv:1703.09393\",\"id\":\"b105\"},\"end\":134316,\"start\":133957},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":204837048},\"end\":134574,\"start\":134318},{\"attributes\":{\"id\":\"b107\",\"matched_paper_id\":3820195},\"end\":134865,\"start\":134576},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":206592484},\"end\":135170,\"start\":134867},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":3719281},\"end\":135449,\"start\":135172},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":45998148},\"end\":135589,\"start\":135451},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":19706288},\"end\":135881,\"start\":135591},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":203581376},\"end\":136194,\"start\":135883},{\"attributes\":{\"id\":\"b113\",\"matched_paper_id\":53082034},\"end\":136406,\"start\":136196},{\"attributes\":{\"doi\":\"arXiv:1912.09632\",\"id\":\"b114\"},\"end\":136662,\"start\":136408},{\"attributes\":{\"id\":\"b115\"},\"end\":136944,\"start\":136664},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":3820195},\"end\":137237,\"start\":136946},{\"attributes\":{\"id\":\"b117\",\"matched_paper_id\":11516457},\"end\":137388,\"start\":137239},{\"attributes\":{\"id\":\"b118\"},\"end\":137774,\"start\":137390},{\"attributes\":{\"id\":\"b119\",\"matched_paper_id\":88505028},\"end\":138073,\"start\":137776},{\"attributes\":{\"id\":\"b120\",\"matched_paper_id\":85501406},\"end\":138284,\"start\":138075},{\"attributes\":{\"id\":\"b121\",\"matched_paper_id\":202716138},\"end\":138510,\"start\":138286},{\"attributes\":{\"id\":\"b122\",\"matched_paper_id\":53179880},\"end\":138649,\"start\":138512},{\"attributes\":{\"id\":\"b123\",\"matched_paper_id\":22598975},\"end\":139091,\"start\":138651},{\"attributes\":{\"id\":\"b124\",\"matched_paper_id\":207761262},\"end\":139410,\"start\":139093},{\"attributes\":{\"doi\":\"arXiv:1912.03672\",\"id\":\"b125\"},\"end\":139710,\"start\":139412},{\"attributes\":{\"doi\":\"arXiv:1912.03677\",\"id\":\"b126\"},\"end\":140040,\"start\":139712},{\"attributes\":{\"doi\":\"arXiv:2002.08623\",\"id\":\"b127\"},\"end\":140314,\"start\":140042},{\"attributes\":{\"id\":\"b128\"},\"end\":140577,\"start\":140316},{\"attributes\":{\"id\":\"b129\",\"matched_paper_id\":197464061},\"end\":140845,\"start\":140579},{\"attributes\":{\"id\":\"b130\",\"matched_paper_id\":196194550},\"end\":141164,\"start\":140847},{\"attributes\":{\"id\":\"b131\",\"matched_paper_id\":197544919},\"end\":141453,\"start\":141166},{\"attributes\":{\"id\":\"b132\",\"matched_paper_id\":202577232},\"end\":141733,\"start\":141455},{\"attributes\":{\"id\":\"b133\",\"matched_paper_id\":204956017},\"end\":142048,\"start\":141735},{\"attributes\":{\"id\":\"b134\"},\"end\":142315,\"start\":142050},{\"attributes\":{\"doi\":\"arXiv:1912.01811\",\"id\":\"b135\"},\"end\":142704,\"start\":142317},{\"attributes\":{\"id\":\"b136\"},\"end\":143053,\"start\":142706},{\"attributes\":{\"id\":\"b137\",\"matched_paper_id\":52799821},\"end\":143211,\"start\":143055},{\"attributes\":{\"id\":\"b138\",\"matched_paper_id\":8006469},\"end\":143473,\"start\":143213},{\"attributes\":{\"id\":\"b139\",\"matched_paper_id\":18074464},\"end\":143778,\"start\":143475},{\"attributes\":{\"id\":\"b140\",\"matched_paper_id\":27113086},\"end\":144010,\"start\":143780},{\"attributes\":{\"id\":\"b141\",\"matched_paper_id\":19922070},\"end\":144276,\"start\":144012},{\"attributes\":{\"id\":\"b142\"},\"end\":144561,\"start\":144278},{\"attributes\":{\"id\":\"b143\"},\"end\":144858,\"start\":144563},{\"attributes\":{\"id\":\"b144\",\"matched_paper_id\":3617831},\"end\":145205,\"start\":144860},{\"attributes\":{\"id\":\"b145\",\"matched_paper_id\":21716590},\"end\":145478,\"start\":145207},{\"attributes\":{\"id\":\"b146\",\"matched_paper_id\":67411403},\"end\":145867,\"start\":145480},{\"attributes\":{\"id\":\"b147\",\"matched_paper_id\":53721960},\"end\":146137,\"start\":145869},{\"attributes\":{\"id\":\"b148\",\"matched_paper_id\":3743115},\"end\":146396,\"start\":146139},{\"attributes\":{\"id\":\"b149\",\"matched_paper_id\":195261383},\"end\":146647,\"start\":146398},{\"attributes\":{\"id\":\"b150\",\"matched_paper_id\":203580471},\"end\":146953,\"start\":146649},{\"attributes\":{\"doi\":\"arXiv:1912.05765\",\"id\":\"b151\"},\"end\":147280,\"start\":146955},{\"attributes\":{\"id\":\"b152\",\"matched_paper_id\":206764948},\"end\":147555,\"start\":147282},{\"attributes\":{\"id\":\"b153\",\"matched_paper_id\":12090268},\"end\":147759,\"start\":147557},{\"attributes\":{\"id\":\"b154\",\"matched_paper_id\":5584770},\"end\":148090,\"start\":147761},{\"attributes\":{\"id\":\"b155\",\"matched_paper_id\":10100907},\"end\":148566,\"start\":148092},{\"attributes\":{\"id\":\"b156\"},\"end\":148832,\"start\":148568},{\"attributes\":{\"id\":\"b157\",\"matched_paper_id\":6451260},\"end\":149155,\"start\":148834},{\"attributes\":{\"id\":\"b158\",\"matched_paper_id\":227722592},\"end\":149523,\"start\":149157},{\"attributes\":{\"doi\":\"arXiv:1804.07437\",\"id\":\"b159\"},\"end\":149749,\"start\":149525},{\"attributes\":{\"id\":\"b160\"},\"end\":149975,\"start\":149751},{\"attributes\":{\"id\":\"b161\",\"matched_paper_id\":198903252},\"end\":150265,\"start\":149977},{\"attributes\":{\"id\":\"b162\",\"matched_paper_id\":195345328},\"end\":150500,\"start\":150267},{\"attributes\":{\"id\":\"b163\",\"matched_paper_id\":140309863},\"end\":150657,\"start\":150502},{\"attributes\":{\"id\":\"b164\",\"matched_paper_id\":206595795},\"end\":150859,\"start\":150659},{\"attributes\":{\"id\":\"b165\",\"matched_paper_id\":4539586},\"end\":151153,\"start\":150861},{\"attributes\":{\"id\":\"b166\"},\"end\":151404,\"start\":151155},{\"attributes\":{\"doi\":\"arXiv:1511.07122\",\"id\":\"b167\"},\"end\":151630,\"start\":151406},{\"attributes\":{\"id\":\"b168\",\"matched_paper_id\":3429309},\"end\":152042,\"start\":151632},{\"attributes\":{\"doi\":\"arXiv:1706.05587\",\"id\":\"b169\"},\"end\":152339,\"start\":152044},{\"attributes\":{\"id\":\"b170\",\"matched_paper_id\":1629541},\"end\":152564,\"start\":152341},{\"attributes\":{\"id\":\"b171\"},\"end\":152787,\"start\":152566},{\"attributes\":{\"id\":\"b172\"},\"end\":153056,\"start\":152789},{\"attributes\":{\"id\":\"b173\",\"matched_paper_id\":6643447},\"end\":153263,\"start\":153058},{\"attributes\":{\"id\":\"b174\"},\"end\":153539,\"start\":153265},{\"attributes\":{\"id\":\"b175\"},\"end\":153738,\"start\":153541},{\"attributes\":{\"id\":\"b176\",\"matched_paper_id\":5574079},\"end\":153981,\"start\":153740},{\"attributes\":{\"id\":\"b177\",\"matched_paper_id\":201645306},\"end\":154248,\"start\":153983},{\"attributes\":{\"id\":\"b178\",\"matched_paper_id\":207926635},\"end\":154517,\"start\":154250},{\"attributes\":{\"id\":\"b179\",\"matched_paper_id\":4852647},\"end\":154691,\"start\":154519},{\"attributes\":{\"id\":\"b180\",\"matched_paper_id\":15869446},\"end\":154985,\"start\":154693},{\"attributes\":{\"id\":\"b181\",\"matched_paper_id\":1398960},\"end\":155227,\"start\":154987},{\"attributes\":{\"id\":\"b182\",\"matched_paper_id\":9552718},\"end\":155485,\"start\":155229},{\"attributes\":{\"id\":\"b183\",\"matched_paper_id\":436933},\"end\":155769,\"start\":155487},{\"attributes\":{\"doi\":\"arXiv:1808.06133\",\"id\":\"b184\"},\"end\":156074,\"start\":155771},{\"attributes\":{\"id\":\"b185\",\"matched_paper_id\":206814921},\"end\":156335,\"start\":156076},{\"attributes\":{\"id\":\"b186\",\"matched_paper_id\":57373907},\"end\":156509,\"start\":156337},{\"attributes\":{\"id\":\"b187\",\"matched_paper_id\":81978466},\"end\":156701,\"start\":156511},{\"attributes\":{\"doi\":\"arXiv:1904.02774v1\",\"id\":\"b188\"},\"end\":156906,\"start\":156703},{\"attributes\":{\"id\":\"b189\",\"matched_paper_id\":119308416},\"end\":157238,\"start\":156908},{\"attributes\":{\"id\":\"b190\",\"matched_paper_id\":195776150},\"end\":157427,\"start\":157240},{\"attributes\":{\"id\":\"b191\",\"matched_paper_id\":199543622},\"end\":157677,\"start\":157429},{\"attributes\":{\"id\":\"b192\",\"matched_paper_id\":202577235},\"end\":157929,\"start\":157679},{\"attributes\":{\"id\":\"b193\",\"matched_paper_id\":201668945},\"end\":158162,\"start\":157931},{\"attributes\":{\"id\":\"b194\",\"matched_paper_id\":201070309},\"end\":158453,\"start\":158164},{\"attributes\":{\"id\":\"b195\",\"matched_paper_id\":145843542},\"end\":158751,\"start\":158455},{\"attributes\":{\"id\":\"b196\",\"matched_paper_id\":211061445},\"end\":158982,\"start\":158753},{\"attributes\":{\"id\":\"b197\",\"matched_paper_id\":204837392},\"end\":159201,\"start\":158984},{\"attributes\":{\"doi\":\"arXiv:1906.07258\",\"id\":\"b198\"},\"end\":159522,\"start\":159203},{\"attributes\":{\"doi\":\"arXiv:1902.05379\",\"id\":\"b199\"},\"end\":159914,\"start\":159524},{\"attributes\":{\"id\":\"b200\",\"matched_paper_id\":204958589},\"end\":160097,\"start\":159916},{\"attributes\":{\"id\":\"b201\",\"matched_paper_id\":6621028},\"end\":160329,\"start\":160099},{\"attributes\":{\"id\":\"b202\",\"matched_paper_id\":54199035},\"end\":160572,\"start\":160331},{\"attributes\":{\"id\":\"b203\",\"matched_paper_id\":199490722},\"end\":160844,\"start\":160574},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b204\"},\"end\":161116,\"start\":160846},{\"attributes\":{\"doi\":\"arXiv:1911.07990\",\"id\":\"b205\"},\"end\":161407,\"start\":161118},{\"attributes\":{\"id\":\"b206\",\"matched_paper_id\":206593880},\"end\":161684,\"start\":161409},{\"attributes\":{\"id\":\"b207\",\"matched_paper_id\":206594692},\"end\":161890,\"start\":161686},{\"attributes\":{\"id\":\"b208\",\"matched_paper_id\":58014954},\"end\":162201,\"start\":161892},{\"attributes\":{\"id\":\"b209\",\"matched_paper_id\":199543884},\"end\":162482,\"start\":162203},{\"attributes\":{\"doi\":\"arXiv:1907.02724\",\"id\":\"b210\"},\"end\":162789,\"start\":162484},{\"attributes\":{\"id\":\"b211\",\"matched_paper_id\":57246310},\"end\":163061,\"start\":162791},{\"attributes\":{\"doi\":\"arXiv:1907.02198\",\"id\":\"b212\"},\"end\":163334,\"start\":163063},{\"attributes\":{\"id\":\"b213\",\"matched_paper_id\":4949878},\"end\":163669,\"start\":163336},{\"attributes\":{\"id\":\"b214\",\"matched_paper_id\":209969238},\"end\":163978,\"start\":163671},{\"attributes\":{\"id\":\"b215\",\"matched_paper_id\":199543540},\"end\":164193,\"start\":163980},{\"attributes\":{\"id\":\"b216\"},\"end\":164402,\"start\":164195},{\"attributes\":{\"id\":\"b217\",\"matched_paper_id\":199488299},\"end\":164675,\"start\":164404},{\"attributes\":{\"doi\":\"arXiv:1911.10782\",\"id\":\"b218\"},\"end\":164939,\"start\":164677},{\"attributes\":{\"doi\":\"2020. 20\",\"id\":\"b219\",\"matched_paper_id\":212747986},\"end\":165150,\"start\":164941},{\"attributes\":{\"id\":\"b220\",\"matched_paper_id\":50787131},\"end\":165474,\"start\":165152},{\"attributes\":{\"id\":\"b221\"},\"end\":166897,\"start\":165476}]", "bib_title": "[{\"end\":106590,\"start\":106517},{\"end\":106898,\"start\":106839},{\"end\":107169,\"start\":107106},{\"end\":107457,\"start\":107388},{\"end\":107650,\"start\":107593},{\"end\":108230,\"start\":108130},{\"end\":108539,\"start\":108488},{\"end\":109104,\"start\":109021},{\"end\":109413,\"start\":109346},{\"end\":109712,\"start\":109621},{\"end\":109986,\"start\":109938},{\"end\":110240,\"start\":110176},{\"end\":110430,\"start\":110395},{\"end\":110613,\"start\":110578},{\"end\":110792,\"start\":110772},{\"end\":111564,\"start\":111526},{\"end\":112602,\"start\":112578},{\"end\":112879,\"start\":112822},{\"end\":113161,\"start\":113066},{\"end\":113452,\"start\":113366},{\"end\":114013,\"start\":113960},{\"end\":114313,\"start\":114199},{\"end\":114584,\"start\":114546},{\"end\":114807,\"start\":114753},{\"end\":114994,\"start\":114984},{\"end\":115158,\"start\":115081},{\"end\":115559,\"start\":115504},{\"end\":115787,\"start\":115753},{\"end\":116088,\"start\":116024},{\"end\":116351,\"start\":116305},{\"end\":116549,\"start\":116506},{\"end\":117039,\"start\":116985},{\"end\":117268,\"start\":117185},{\"end\":117541,\"start\":117489},{\"end\":117775,\"start\":117733},{\"end\":117981,\"start\":117929},{\"end\":118203,\"start\":118143},{\"end\":118531,\"start\":118431},{\"end\":118830,\"start\":118784},{\"end\":119095,\"start\":119031},{\"end\":119369,\"start\":119304},{\"end\":119878,\"start\":119832},{\"end\":120095,\"start\":120032},{\"end\":120398,\"start\":120298},{\"end\":120667,\"start\":120615},{\"end\":121215,\"start\":121168},{\"end\":121504,\"start\":121409},{\"end\":121820,\"start\":121764},{\"end\":122094,\"start\":122021},{\"end\":122299,\"start\":122267},{\"end\":122565,\"start\":122494},{\"end\":122855,\"start\":122777},{\"end\":123180,\"start\":123070},{\"end\":123517,\"start\":123471},{\"end\":123761,\"start\":123671},{\"end\":124028,\"start\":123960},{\"end\":124341,\"start\":124274},{\"end\":124574,\"start\":124520},{\"end\":124865,\"start\":124808},{\"end\":125164,\"start\":125076},{\"end\":125455,\"start\":125431},{\"end\":125653,\"start\":125591},{\"end\":125924,\"start\":125872},{\"end\":126193,\"start\":126107},{\"end\":126570,\"start\":126506},{\"end\":127140,\"start\":127106},{\"end\":127345,\"start\":127299},{\"end\":127827,\"start\":127727},{\"end\":128134,\"start\":128043},{\"end\":128428,\"start\":128369},{\"end\":128683,\"start\":128620},{\"end\":128885,\"start\":128857},{\"end\":129063,\"start\":129035},{\"end\":130103,\"start\":130034},{\"end\":130360,\"start\":130289},{\"end\":130678,\"start\":130620},{\"end\":130950,\"start\":130873},{\"end\":131197,\"start\":131132},{\"end\":131458,\"start\":131405},{\"end\":131701,\"start\":131642},{\"end\":131934,\"start\":131855},{\"end\":132191,\"start\":132144},{\"end\":132497,\"start\":132411},{\"end\":132797,\"start\":132727},{\"end\":133046,\"start\":132983},{\"end\":133268,\"start\":133241},{\"end\":133612,\"start\":133547},{\"end\":133787,\"start\":133759},{\"end\":134380,\"start\":134318},{\"end\":134640,\"start\":134576},{\"end\":134897,\"start\":134867},{\"end\":135235,\"start\":135172},{\"end\":135469,\"start\":135451},{\"end\":135694,\"start\":135591},{\"end\":135942,\"start\":135883},{\"end\":136243,\"start\":136196},{\"end\":137011,\"start\":136946},{\"end\":137267,\"start\":137239},{\"end\":137847,\"start\":137776},{\"end\":138142,\"start\":138075},{\"end\":138324,\"start\":138286},{\"end\":138535,\"start\":138512},{\"end\":138775,\"start\":138651},{\"end\":139165,\"start\":139093},{\"end\":140659,\"start\":140579},{\"end\":140936,\"start\":140847},{\"end\":141239,\"start\":141166},{\"end\":141513,\"start\":141455},{\"end\":141822,\"start\":141735},{\"end\":143086,\"start\":143055},{\"end\":143269,\"start\":143213},{\"end\":143542,\"start\":143475},{\"end\":143823,\"start\":143780},{\"end\":144076,\"start\":144012},{\"end\":144963,\"start\":144860},{\"end\":145266,\"start\":145207},{\"end\":145598,\"start\":145480},{\"end\":145929,\"start\":145869},{\"end\":146203,\"start\":146139},{\"end\":146471,\"start\":146398},{\"end\":146701,\"start\":146649},{\"end\":147341,\"start\":147282},{\"end\":147595,\"start\":147557},{\"end\":147836,\"start\":147761},{\"end\":148154,\"start\":148092},{\"end\":148915,\"start\":148834},{\"end\":149241,\"start\":149157},{\"end\":150063,\"start\":149977},{\"end\":150305,\"start\":150267},{\"end\":150533,\"start\":150502},{\"end\":150716,\"start\":150659},{\"end\":150942,\"start\":150861},{\"end\":151743,\"start\":151632},{\"end\":152395,\"start\":152341},{\"end\":153111,\"start\":153058},{\"end\":153813,\"start\":153740},{\"end\":154044,\"start\":153983},{\"end\":154294,\"start\":154250},{\"end\":154544,\"start\":154519},{\"end\":154765,\"start\":154693},{\"end\":155050,\"start\":154987},{\"end\":155269,\"start\":155229},{\"end\":155564,\"start\":155487},{\"end\":156136,\"start\":156076},{\"end\":156375,\"start\":156337},{\"end\":156553,\"start\":156511},{\"end\":156987,\"start\":156908},{\"end\":157292,\"start\":157240},{\"end\":157492,\"start\":157429},{\"end\":157731,\"start\":157679},{\"end\":158002,\"start\":157931},{\"end\":158239,\"start\":158164},{\"end\":158540,\"start\":158455},{\"end\":158817,\"start\":158753},{\"end\":159025,\"start\":158984},{\"end\":159966,\"start\":159916},{\"end\":160138,\"start\":160099},{\"end\":160393,\"start\":160331},{\"end\":160630,\"start\":160574},{\"end\":161466,\"start\":161409},{\"end\":161730,\"start\":161686},{\"end\":161993,\"start\":161892},{\"end\":162279,\"start\":162203},{\"end\":162842,\"start\":162791},{\"end\":163433,\"start\":163336},{\"end\":163760,\"start\":163671},{\"end\":164033,\"start\":163980},{\"end\":164469,\"start\":164404},{\"end\":165005,\"start\":164941},{\"end\":165220,\"start\":165152},{\"end\":165695,\"start\":165476}]", "bib_author": "[{\"end\":106601,\"start\":106592},{\"end\":106609,\"start\":106601},{\"end\":106617,\"start\":106609},{\"end\":106624,\"start\":106617},{\"end\":106630,\"start\":106624},{\"end\":106915,\"start\":106900},{\"end\":106933,\"start\":106915},{\"end\":107186,\"start\":107171},{\"end\":107203,\"start\":107186},{\"end\":107213,\"start\":107203},{\"end\":107467,\"start\":107459},{\"end\":107475,\"start\":107467},{\"end\":107661,\"start\":107652},{\"end\":107670,\"start\":107661},{\"end\":107680,\"start\":107670},{\"end\":107945,\"start\":107932},{\"end\":107956,\"start\":107945},{\"end\":108239,\"start\":108232},{\"end\":108246,\"start\":108239},{\"end\":108254,\"start\":108246},{\"end\":108269,\"start\":108254},{\"end\":108552,\"start\":108541},{\"end\":108568,\"start\":108552},{\"end\":108578,\"start\":108568},{\"end\":108586,\"start\":108578},{\"end\":108844,\"start\":108835},{\"end\":108851,\"start\":108844},{\"end\":108859,\"start\":108851},{\"end\":109114,\"start\":109106},{\"end\":109120,\"start\":109114},{\"end\":109127,\"start\":109120},{\"end\":109136,\"start\":109127},{\"end\":109145,\"start\":109136},{\"end\":109151,\"start\":109145},{\"end\":109158,\"start\":109151},{\"end\":109422,\"start\":109415},{\"end\":109430,\"start\":109422},{\"end\":109438,\"start\":109430},{\"end\":109444,\"start\":109438},{\"end\":109720,\"start\":109714},{\"end\":109729,\"start\":109720},{\"end\":109737,\"start\":109729},{\"end\":110000,\"start\":109988},{\"end\":110009,\"start\":110000},{\"end\":110018,\"start\":110009},{\"end\":110028,\"start\":110018},{\"end\":110250,\"start\":110242},{\"end\":110257,\"start\":110250},{\"end\":110442,\"start\":110432},{\"end\":110450,\"start\":110442},{\"end\":110628,\"start\":110615},{\"end\":110641,\"start\":110628},{\"end\":110804,\"start\":110794},{\"end\":110817,\"start\":110804},{\"end\":110830,\"start\":110817},{\"end\":111040,\"start\":111031},{\"end\":111048,\"start\":111040},{\"end\":111060,\"start\":111048},{\"end\":111071,\"start\":111060},{\"end\":111317,\"start\":111308},{\"end\":111323,\"start\":111317},{\"end\":111337,\"start\":111323},{\"end\":111348,\"start\":111337},{\"end\":111591,\"start\":111566},{\"end\":111608,\"start\":111591},{\"end\":111624,\"start\":111608},{\"end\":111644,\"start\":111624},{\"end\":111659,\"start\":111644},{\"end\":111949,\"start\":111941},{\"end\":111961,\"start\":111949},{\"end\":112148,\"start\":112133},{\"end\":112161,\"start\":112148},{\"end\":112176,\"start\":112161},{\"end\":112381,\"start\":112371},{\"end\":112391,\"start\":112381},{\"end\":112405,\"start\":112391},{\"end\":112415,\"start\":112405},{\"end\":112612,\"start\":112604},{\"end\":112627,\"start\":112612},{\"end\":112640,\"start\":112627},{\"end\":112654,\"start\":112640},{\"end\":112663,\"start\":112654},{\"end\":112889,\"start\":112881},{\"end\":112897,\"start\":112889},{\"end\":112911,\"start\":112897},{\"end\":112919,\"start\":112911},{\"end\":113171,\"start\":113163},{\"end\":113179,\"start\":113171},{\"end\":113187,\"start\":113179},{\"end\":113464,\"start\":113454},{\"end\":113478,\"start\":113464},{\"end\":113493,\"start\":113478},{\"end\":113779,\"start\":113771},{\"end\":113786,\"start\":113779},{\"end\":113793,\"start\":113786},{\"end\":113799,\"start\":113793},{\"end\":114028,\"start\":114015},{\"end\":114039,\"start\":114028},{\"end\":114050,\"start\":114039},{\"end\":114321,\"start\":114315},{\"end\":114330,\"start\":114321},{\"end\":114339,\"start\":114330},{\"end\":114346,\"start\":114339},{\"end\":114595,\"start\":114586},{\"end\":114606,\"start\":114595},{\"end\":114617,\"start\":114606},{\"end\":114822,\"start\":114809},{\"end\":114835,\"start\":114822},{\"end\":115008,\"start\":114996},{\"end\":115167,\"start\":115160},{\"end\":115173,\"start\":115167},{\"end\":115185,\"start\":115173},{\"end\":115192,\"start\":115185},{\"end\":115375,\"start\":115369},{\"end\":115387,\"start\":115375},{\"end\":115397,\"start\":115387},{\"end\":115409,\"start\":115397},{\"end\":115571,\"start\":115561},{\"end\":115582,\"start\":115571},{\"end\":115594,\"start\":115582},{\"end\":115605,\"start\":115594},{\"end\":115796,\"start\":115789},{\"end\":115808,\"start\":115796},{\"end\":115817,\"start\":115808},{\"end\":115828,\"start\":115817},{\"end\":115836,\"start\":115828},{\"end\":115845,\"start\":115836},{\"end\":115855,\"start\":115845},{\"end\":116100,\"start\":116090},{\"end\":116111,\"start\":116100},{\"end\":116122,\"start\":116111},{\"end\":116130,\"start\":116122},{\"end\":116363,\"start\":116353},{\"end\":116378,\"start\":116363},{\"end\":116559,\"start\":116551},{\"end\":116568,\"start\":116559},{\"end\":116576,\"start\":116568},{\"end\":116585,\"start\":116576},{\"end\":116808,\"start\":116800},{\"end\":116818,\"start\":116808},{\"end\":116828,\"start\":116818},{\"end\":116841,\"start\":116828},{\"end\":117051,\"start\":117041},{\"end\":117279,\"start\":117270},{\"end\":117294,\"start\":117279},{\"end\":117305,\"start\":117294},{\"end\":117552,\"start\":117543},{\"end\":117562,\"start\":117552},{\"end\":117791,\"start\":117777},{\"end\":117804,\"start\":117791},{\"end\":117995,\"start\":117983},{\"end\":118005,\"start\":117995},{\"end\":118213,\"start\":118205},{\"end\":118222,\"start\":118213},{\"end\":118232,\"start\":118222},{\"end\":118247,\"start\":118232},{\"end\":118254,\"start\":118247},{\"end\":118544,\"start\":118533},{\"end\":118556,\"start\":118544},{\"end\":118569,\"start\":118556},{\"end\":118578,\"start\":118569},{\"end\":118840,\"start\":118832},{\"end\":118849,\"start\":118840},{\"end\":118857,\"start\":118849},{\"end\":118864,\"start\":118857},{\"end\":118871,\"start\":118864},{\"end\":119103,\"start\":119097},{\"end\":119109,\"start\":119103},{\"end\":119115,\"start\":119109},{\"end\":119122,\"start\":119115},{\"end\":119128,\"start\":119122},{\"end\":119135,\"start\":119128},{\"end\":119380,\"start\":119371},{\"end\":119386,\"start\":119380},{\"end\":119394,\"start\":119386},{\"end\":119402,\"start\":119394},{\"end\":119663,\"start\":119657},{\"end\":119672,\"start\":119663},{\"end\":119681,\"start\":119672},{\"end\":119687,\"start\":119681},{\"end\":119886,\"start\":119880},{\"end\":119893,\"start\":119886},{\"end\":119906,\"start\":119893},{\"end\":120105,\"start\":120097},{\"end\":120113,\"start\":120105},{\"end\":120122,\"start\":120113},{\"end\":120137,\"start\":120122},{\"end\":120413,\"start\":120400},{\"end\":120424,\"start\":120413},{\"end\":120676,\"start\":120669},{\"end\":120683,\"start\":120676},{\"end\":120691,\"start\":120683},{\"end\":120699,\"start\":120691},{\"end\":120916,\"start\":120907},{\"end\":120926,\"start\":120916},{\"end\":120936,\"start\":120926},{\"end\":120946,\"start\":120936},{\"end\":121233,\"start\":121217},{\"end\":121244,\"start\":121233},{\"end\":121254,\"start\":121244},{\"end\":121518,\"start\":121506},{\"end\":121528,\"start\":121518},{\"end\":121540,\"start\":121528},{\"end\":121831,\"start\":121822},{\"end\":121839,\"start\":121831},{\"end\":121847,\"start\":121839},{\"end\":121856,\"start\":121847},{\"end\":122108,\"start\":122096},{\"end\":122120,\"start\":122108},{\"end\":122307,\"start\":122301},{\"end\":122316,\"start\":122307},{\"end\":122324,\"start\":122316},{\"end\":122330,\"start\":122324},{\"end\":122338,\"start\":122330},{\"end\":122345,\"start\":122338},{\"end\":122575,\"start\":122567},{\"end\":122585,\"start\":122575},{\"end\":122598,\"start\":122585},{\"end\":122608,\"start\":122598},{\"end\":122870,\"start\":122857},{\"end\":122882,\"start\":122870},{\"end\":122893,\"start\":122882},{\"end\":123195,\"start\":123182},{\"end\":123206,\"start\":123195},{\"end\":123214,\"start\":123206},{\"end\":123229,\"start\":123214},{\"end\":123530,\"start\":123519},{\"end\":123541,\"start\":123530},{\"end\":123776,\"start\":123763},{\"end\":123787,\"start\":123776},{\"end\":124042,\"start\":124030},{\"end\":124051,\"start\":124042},{\"end\":124068,\"start\":124051},{\"end\":124352,\"start\":124343},{\"end\":124358,\"start\":124352},{\"end\":124365,\"start\":124358},{\"end\":124583,\"start\":124576},{\"end\":124592,\"start\":124583},{\"end\":124599,\"start\":124592},{\"end\":124606,\"start\":124599},{\"end\":124612,\"start\":124606},{\"end\":124624,\"start\":124612},{\"end\":124633,\"start\":124624},{\"end\":124874,\"start\":124867},{\"end\":124882,\"start\":124874},{\"end\":124894,\"start\":124882},{\"end\":124901,\"start\":124894},{\"end\":125173,\"start\":125166},{\"end\":125185,\"start\":125173},{\"end\":125198,\"start\":125185},{\"end\":125206,\"start\":125198},{\"end\":125218,\"start\":125206},{\"end\":125467,\"start\":125457},{\"end\":125473,\"start\":125467},{\"end\":125481,\"start\":125473},{\"end\":125663,\"start\":125655},{\"end\":125669,\"start\":125663},{\"end\":125675,\"start\":125669},{\"end\":125683,\"start\":125675},{\"end\":125689,\"start\":125683},{\"end\":125697,\"start\":125689},{\"end\":125933,\"start\":125926},{\"end\":125942,\"start\":125933},{\"end\":125949,\"start\":125942},{\"end\":125955,\"start\":125949},{\"end\":126205,\"start\":126195},{\"end\":126215,\"start\":126205},{\"end\":126225,\"start\":126215},{\"end\":126234,\"start\":126225},{\"end\":126248,\"start\":126234},{\"end\":126259,\"start\":126248},{\"end\":126267,\"start\":126259},{\"end\":126579,\"start\":126572},{\"end\":126596,\"start\":126579},{\"end\":126610,\"start\":126596},{\"end\":126902,\"start\":126889},{\"end\":126910,\"start\":126902},{\"end\":126917,\"start\":126910},{\"end\":127150,\"start\":127142},{\"end\":127157,\"start\":127150},{\"end\":127166,\"start\":127157},{\"end\":127176,\"start\":127166},{\"end\":127353,\"start\":127347},{\"end\":127362,\"start\":127353},{\"end\":127368,\"start\":127362},{\"end\":127374,\"start\":127368},{\"end\":127382,\"start\":127374},{\"end\":127388,\"start\":127382},{\"end\":127582,\"start\":127576},{\"end\":127588,\"start\":127582},{\"end\":127594,\"start\":127588},{\"end\":127600,\"start\":127594},{\"end\":127842,\"start\":127829},{\"end\":127849,\"start\":127842},{\"end\":127857,\"start\":127849},{\"end\":128143,\"start\":128136},{\"end\":128151,\"start\":128143},{\"end\":128158,\"start\":128151},{\"end\":128165,\"start\":128158},{\"end\":128172,\"start\":128165},{\"end\":128178,\"start\":128172},{\"end\":128438,\"start\":128430},{\"end\":128445,\"start\":128438},{\"end\":128452,\"start\":128445},{\"end\":128460,\"start\":128452},{\"end\":128692,\"start\":128685},{\"end\":128700,\"start\":128692},{\"end\":128706,\"start\":128700},{\"end\":128714,\"start\":128706},{\"end\":128894,\"start\":128887},{\"end\":128906,\"start\":128894},{\"end\":128913,\"start\":128906},{\"end\":129074,\"start\":129065},{\"end\":129087,\"start\":129074},{\"end\":129292,\"start\":129285},{\"end\":129300,\"start\":129292},{\"end\":129306,\"start\":129300},{\"end\":129527,\"start\":129520},{\"end\":129535,\"start\":129527},{\"end\":129541,\"start\":129535},{\"end\":129548,\"start\":129541},{\"end\":129556,\"start\":129548},{\"end\":129563,\"start\":129556},{\"end\":129848,\"start\":129835},{\"end\":129857,\"start\":129848},{\"end\":130112,\"start\":130105},{\"end\":130129,\"start\":130112},{\"end\":130143,\"start\":130129},{\"end\":130371,\"start\":130362},{\"end\":130379,\"start\":130371},{\"end\":130388,\"start\":130379},{\"end\":130396,\"start\":130388},{\"end\":130403,\"start\":130396},{\"end\":130415,\"start\":130403},{\"end\":130423,\"start\":130415},{\"end\":130687,\"start\":130680},{\"end\":130694,\"start\":130687},{\"end\":130700,\"start\":130694},{\"end\":130710,\"start\":130700},{\"end\":130717,\"start\":130710},{\"end\":130959,\"start\":130952},{\"end\":130967,\"start\":130959},{\"end\":130973,\"start\":130967},{\"end\":131207,\"start\":131199},{\"end\":131216,\"start\":131207},{\"end\":131225,\"start\":131216},{\"end\":131234,\"start\":131225},{\"end\":131483,\"start\":131460},{\"end\":131488,\"start\":131483},{\"end\":131498,\"start\":131488},{\"end\":131506,\"start\":131498},{\"end\":131716,\"start\":131703},{\"end\":131727,\"start\":131716},{\"end\":131942,\"start\":131936},{\"end\":131949,\"start\":131942},{\"end\":131955,\"start\":131949},{\"end\":131962,\"start\":131955},{\"end\":131968,\"start\":131962},{\"end\":131975,\"start\":131968},{\"end\":132202,\"start\":132193},{\"end\":132210,\"start\":132202},{\"end\":132218,\"start\":132210},{\"end\":132225,\"start\":132218},{\"end\":132233,\"start\":132225},{\"end\":132240,\"start\":132233},{\"end\":132248,\"start\":132240},{\"end\":132508,\"start\":132499},{\"end\":132514,\"start\":132508},{\"end\":132520,\"start\":132514},{\"end\":132528,\"start\":132520},{\"end\":132538,\"start\":132528},{\"end\":132551,\"start\":132538},{\"end\":132812,\"start\":132799},{\"end\":132819,\"start\":132812},{\"end\":132829,\"start\":132819},{\"end\":132837,\"start\":132829},{\"end\":133062,\"start\":133048},{\"end\":133075,\"start\":133062},{\"end\":133087,\"start\":133075},{\"end\":133284,\"start\":133270},{\"end\":133301,\"start\":133284},{\"end\":133310,\"start\":133301},{\"end\":133316,\"start\":133310},{\"end\":133332,\"start\":133316},{\"end\":133341,\"start\":133332},{\"end\":133354,\"start\":133341},{\"end\":133364,\"start\":133354},{\"end\":133623,\"start\":133614},{\"end\":133633,\"start\":133623},{\"end\":133802,\"start\":133789},{\"end\":133814,\"start\":133802},{\"end\":133827,\"start\":133814},{\"end\":134078,\"start\":134067},{\"end\":134087,\"start\":134078},{\"end\":134097,\"start\":134087},{\"end\":134389,\"start\":134382},{\"end\":134395,\"start\":134389},{\"end\":134405,\"start\":134395},{\"end\":134413,\"start\":134405},{\"end\":134651,\"start\":134642},{\"end\":134659,\"start\":134651},{\"end\":134666,\"start\":134659},{\"end\":134672,\"start\":134666},{\"end\":134680,\"start\":134672},{\"end\":134687,\"start\":134680},{\"end\":134910,\"start\":134899},{\"end\":134917,\"start\":134910},{\"end\":134924,\"start\":134917},{\"end\":134936,\"start\":134924},{\"end\":134944,\"start\":134936},{\"end\":134956,\"start\":134944},{\"end\":134965,\"start\":134956},{\"end\":134978,\"start\":134965},{\"end\":134992,\"start\":134978},{\"end\":135252,\"start\":135237},{\"end\":135263,\"start\":135252},{\"end\":135271,\"start\":135263},{\"end\":135482,\"start\":135471},{\"end\":135704,\"start\":135696},{\"end\":135710,\"start\":135704},{\"end\":135720,\"start\":135710},{\"end\":135953,\"start\":135944},{\"end\":135960,\"start\":135953},{\"end\":135972,\"start\":135960},{\"end\":135979,\"start\":135972},{\"end\":135990,\"start\":135979},{\"end\":136000,\"start\":135990},{\"end\":136009,\"start\":136000},{\"end\":136017,\"start\":136009},{\"end\":136253,\"start\":136245},{\"end\":136260,\"start\":136253},{\"end\":136268,\"start\":136260},{\"end\":136463,\"start\":136457},{\"end\":136472,\"start\":136463},{\"end\":136478,\"start\":136472},{\"end\":136486,\"start\":136478},{\"end\":136498,\"start\":136486},{\"end\":136505,\"start\":136498},{\"end\":136772,\"start\":136764},{\"end\":136784,\"start\":136772},{\"end\":137022,\"start\":137013},{\"end\":137030,\"start\":137022},{\"end\":137037,\"start\":137030},{\"end\":137043,\"start\":137037},{\"end\":137051,\"start\":137043},{\"end\":137058,\"start\":137051},{\"end\":137281,\"start\":137269},{\"end\":137291,\"start\":137281},{\"end\":137537,\"start\":137524},{\"end\":137545,\"start\":137537},{\"end\":137553,\"start\":137545},{\"end\":137560,\"start\":137553},{\"end\":137857,\"start\":137849},{\"end\":137866,\"start\":137857},{\"end\":137873,\"start\":137866},{\"end\":137881,\"start\":137873},{\"end\":137891,\"start\":137881},{\"end\":138152,\"start\":138144},{\"end\":138158,\"start\":138152},{\"end\":138165,\"start\":138158},{\"end\":138339,\"start\":138326},{\"end\":138348,\"start\":138339},{\"end\":138364,\"start\":138348},{\"end\":138374,\"start\":138364},{\"end\":138382,\"start\":138374},{\"end\":138543,\"start\":138537},{\"end\":138550,\"start\":138543},{\"end\":138563,\"start\":138550},{\"end\":138788,\"start\":138777},{\"end\":138802,\"start\":138788},{\"end\":138812,\"start\":138802},{\"end\":138823,\"start\":138812},{\"end\":138842,\"start\":138823},{\"end\":139175,\"start\":139167},{\"end\":139186,\"start\":139175},{\"end\":139198,\"start\":139186},{\"end\":139214,\"start\":139198},{\"end\":139519,\"start\":139512},{\"end\":139527,\"start\":139519},{\"end\":139535,\"start\":139527},{\"end\":139823,\"start\":139816},{\"end\":139830,\"start\":139823},{\"end\":139838,\"start\":139830},{\"end\":139846,\"start\":139838},{\"end\":140117,\"start\":140110},{\"end\":140124,\"start\":140117},{\"end\":140132,\"start\":140124},{\"end\":140138,\"start\":140132},{\"end\":140379,\"start\":140362},{\"end\":140391,\"start\":140379},{\"end\":140406,\"start\":140391},{\"end\":140415,\"start\":140406},{\"end\":140425,\"start\":140415},{\"end\":140670,\"start\":140661},{\"end\":140680,\"start\":140670},{\"end\":140946,\"start\":140938},{\"end\":140952,\"start\":140946},{\"end\":140961,\"start\":140952},{\"end\":140968,\"start\":140961},{\"end\":140975,\"start\":140968},{\"end\":141249,\"start\":141241},{\"end\":141257,\"start\":141249},{\"end\":141264,\"start\":141257},{\"end\":141271,\"start\":141264},{\"end\":141277,\"start\":141271},{\"end\":141522,\"start\":141515},{\"end\":141530,\"start\":141522},{\"end\":141537,\"start\":141530},{\"end\":141545,\"start\":141537},{\"end\":141553,\"start\":141545},{\"end\":141560,\"start\":141553},{\"end\":141568,\"start\":141560},{\"end\":141837,\"start\":141824},{\"end\":141848,\"start\":141837},{\"end\":141859,\"start\":141848},{\"end\":142143,\"start\":142130},{\"end\":142150,\"start\":142143},{\"end\":142162,\"start\":142150},{\"end\":142438,\"start\":142431},{\"end\":142444,\"start\":142438},{\"end\":142451,\"start\":142444},{\"end\":142457,\"start\":142451},{\"end\":142465,\"start\":142457},{\"end\":142471,\"start\":142465},{\"end\":142478,\"start\":142471},{\"end\":142827,\"start\":142820},{\"end\":142835,\"start\":142827},{\"end\":142845,\"start\":142835},{\"end\":143097,\"start\":143088},{\"end\":143109,\"start\":143097},{\"end\":143280,\"start\":143271},{\"end\":143289,\"start\":143280},{\"end\":143298,\"start\":143289},{\"end\":143308,\"start\":143298},{\"end\":143553,\"start\":143544},{\"end\":143566,\"start\":143553},{\"end\":143580,\"start\":143566},{\"end\":143593,\"start\":143580},{\"end\":143832,\"start\":143825},{\"end\":143840,\"start\":143832},{\"end\":143846,\"start\":143840},{\"end\":143852,\"start\":143846},{\"end\":144086,\"start\":144078},{\"end\":144092,\"start\":144086},{\"end\":144100,\"start\":144092},{\"end\":144108,\"start\":144100},{\"end\":144359,\"start\":144353},{\"end\":144368,\"start\":144359},{\"end\":144376,\"start\":144368},{\"end\":144384,\"start\":144376},{\"end\":144390,\"start\":144384},{\"end\":144654,\"start\":144643},{\"end\":144660,\"start\":144654},{\"end\":144667,\"start\":144660},{\"end\":144682,\"start\":144667},{\"end\":144688,\"start\":144682},{\"end\":144971,\"start\":144965},{\"end\":144979,\"start\":144971},{\"end\":144988,\"start\":144979},{\"end\":144994,\"start\":144988},{\"end\":145002,\"start\":144994},{\"end\":145276,\"start\":145268},{\"end\":145283,\"start\":145276},{\"end\":145289,\"start\":145283},{\"end\":145297,\"start\":145289},{\"end\":145306,\"start\":145297},{\"end\":145609,\"start\":145600},{\"end\":145616,\"start\":145609},{\"end\":145623,\"start\":145616},{\"end\":145629,\"start\":145623},{\"end\":145637,\"start\":145629},{\"end\":145943,\"start\":145931},{\"end\":145952,\"start\":145943},{\"end\":145965,\"start\":145952},{\"end\":145975,\"start\":145965},{\"end\":146211,\"start\":146205},{\"end\":146217,\"start\":146211},{\"end\":146226,\"start\":146217},{\"end\":146233,\"start\":146226},{\"end\":146241,\"start\":146233},{\"end\":146247,\"start\":146241},{\"end\":146481,\"start\":146473},{\"end\":146489,\"start\":146481},{\"end\":146712,\"start\":146703},{\"end\":146721,\"start\":146712},{\"end\":146727,\"start\":146721},{\"end\":146734,\"start\":146727},{\"end\":146741,\"start\":146734},{\"end\":146747,\"start\":146741},{\"end\":146755,\"start\":146747},{\"end\":146761,\"start\":146755},{\"end\":146769,\"start\":146761},{\"end\":146775,\"start\":146769},{\"end\":147049,\"start\":147038},{\"end\":147061,\"start\":147049},{\"end\":147070,\"start\":147061},{\"end\":147353,\"start\":147343},{\"end\":147362,\"start\":147353},{\"end\":147373,\"start\":147362},{\"end\":147383,\"start\":147373},{\"end\":147605,\"start\":147597},{\"end\":147612,\"start\":147605},{\"end\":147622,\"start\":147612},{\"end\":147630,\"start\":147622},{\"end\":147849,\"start\":147838},{\"end\":147859,\"start\":147849},{\"end\":147866,\"start\":147859},{\"end\":147879,\"start\":147866},{\"end\":147889,\"start\":147879},{\"end\":148165,\"start\":148156},{\"end\":148171,\"start\":148165},{\"end\":148185,\"start\":148171},{\"end\":148196,\"start\":148185},{\"end\":148660,\"start\":148648},{\"end\":148670,\"start\":148660},{\"end\":148679,\"start\":148670},{\"end\":148923,\"start\":148917},{\"end\":148930,\"start\":148923},{\"end\":148938,\"start\":148930},{\"end\":148948,\"start\":148938},{\"end\":148956,\"start\":148948},{\"end\":149255,\"start\":149243},{\"end\":149263,\"start\":149255},{\"end\":149275,\"start\":149263},{\"end\":149286,\"start\":149275},{\"end\":149300,\"start\":149286},{\"end\":149566,\"start\":149559},{\"end\":149573,\"start\":149566},{\"end\":149581,\"start\":149573},{\"end\":149589,\"start\":149581},{\"end\":149595,\"start\":149589},{\"end\":149827,\"start\":149820},{\"end\":149834,\"start\":149827},{\"end\":149847,\"start\":149834},{\"end\":150072,\"start\":150065},{\"end\":150081,\"start\":150072},{\"end\":150093,\"start\":150081},{\"end\":150100,\"start\":150093},{\"end\":150314,\"start\":150307},{\"end\":150321,\"start\":150314},{\"end\":150327,\"start\":150321},{\"end\":150334,\"start\":150327},{\"end\":150342,\"start\":150334},{\"end\":150351,\"start\":150342},{\"end\":150541,\"start\":150535},{\"end\":150549,\"start\":150541},{\"end\":150556,\"start\":150549},{\"end\":150725,\"start\":150718},{\"end\":150736,\"start\":150725},{\"end\":150952,\"start\":150944},{\"end\":150961,\"start\":150952},{\"end\":150969,\"start\":150961},{\"end\":150975,\"start\":150969},{\"end\":150982,\"start\":150975},{\"end\":151213,\"start\":151206},{\"end\":151221,\"start\":151213},{\"end\":151231,\"start\":151221},{\"end\":151237,\"start\":151231},{\"end\":151249,\"start\":151237},{\"end\":151257,\"start\":151249},{\"end\":151469,\"start\":151463},{\"end\":151479,\"start\":151469},{\"end\":151756,\"start\":151745},{\"end\":151770,\"start\":151756},{\"end\":151782,\"start\":151770},{\"end\":151792,\"start\":151782},{\"end\":151804,\"start\":151792},{\"end\":152118,\"start\":152107},{\"end\":152132,\"start\":152118},{\"end\":152143,\"start\":152132},{\"end\":152151,\"start\":152143},{\"end\":152405,\"start\":152397},{\"end\":152418,\"start\":152405},{\"end\":152429,\"start\":152418},{\"end\":152608,\"start\":152601},{\"end\":152614,\"start\":152608},{\"end\":152623,\"start\":152614},{\"end\":152629,\"start\":152623},{\"end\":152638,\"start\":152629},{\"end\":152644,\"start\":152638},{\"end\":152651,\"start\":152644},{\"end\":152875,\"start\":152867},{\"end\":152883,\"start\":152875},{\"end\":152889,\"start\":152883},{\"end\":152895,\"start\":152889},{\"end\":152902,\"start\":152895},{\"end\":153121,\"start\":153113},{\"end\":153129,\"start\":153121},{\"end\":153137,\"start\":153129},{\"end\":153368,\"start\":153356},{\"end\":153380,\"start\":153368},{\"end\":153393,\"start\":153380},{\"end\":153596,\"start\":153588},{\"end\":153829,\"start\":153815},{\"end\":153839,\"start\":153829},{\"end\":154053,\"start\":154046},{\"end\":154060,\"start\":154053},{\"end\":154066,\"start\":154060},{\"end\":154073,\"start\":154066},{\"end\":154083,\"start\":154073},{\"end\":154090,\"start\":154083},{\"end\":154305,\"start\":154296},{\"end\":154312,\"start\":154305},{\"end\":154320,\"start\":154312},{\"end\":154327,\"start\":154320},{\"end\":154335,\"start\":154327},{\"end\":154342,\"start\":154335},{\"end\":154350,\"start\":154342},{\"end\":154554,\"start\":154546},{\"end\":154566,\"start\":154554},{\"end\":154575,\"start\":154566},{\"end\":154581,\"start\":154575},{\"end\":154777,\"start\":154767},{\"end\":154787,\"start\":154777},{\"end\":154795,\"start\":154787},{\"end\":154807,\"start\":154795},{\"end\":155062,\"start\":155052},{\"end\":155077,\"start\":155062},{\"end\":155280,\"start\":155271},{\"end\":155286,\"start\":155280},{\"end\":155295,\"start\":155286},{\"end\":155301,\"start\":155295},{\"end\":155308,\"start\":155301},{\"end\":155314,\"start\":155308},{\"end\":155321,\"start\":155314},{\"end\":155572,\"start\":155566},{\"end\":155581,\"start\":155572},{\"end\":155588,\"start\":155581},{\"end\":155595,\"start\":155588},{\"end\":155836,\"start\":155828},{\"end\":155844,\"start\":155836},{\"end\":155851,\"start\":155844},{\"end\":155858,\"start\":155851},{\"end\":155866,\"start\":155858},{\"end\":155873,\"start\":155866},{\"end\":156146,\"start\":156138},{\"end\":156152,\"start\":156146},{\"end\":156159,\"start\":156152},{\"end\":156166,\"start\":156159},{\"end\":156175,\"start\":156166},{\"end\":156386,\"start\":156377},{\"end\":156392,\"start\":156386},{\"end\":156399,\"start\":156392},{\"end\":156406,\"start\":156399},{\"end\":156561,\"start\":156555},{\"end\":156572,\"start\":156561},{\"end\":156588,\"start\":156572},{\"end\":156740,\"start\":156730},{\"end\":156751,\"start\":156740},{\"end\":156761,\"start\":156751},{\"end\":156996,\"start\":156989},{\"end\":157005,\"start\":156996},{\"end\":157012,\"start\":157005},{\"end\":157029,\"start\":157012},{\"end\":157040,\"start\":157029},{\"end\":157046,\"start\":157040},{\"end\":157307,\"start\":157294},{\"end\":157318,\"start\":157307},{\"end\":157500,\"start\":157494},{\"end\":157507,\"start\":157500},{\"end\":157515,\"start\":157507},{\"end\":157523,\"start\":157515},{\"end\":157745,\"start\":157733},{\"end\":157754,\"start\":157745},{\"end\":157761,\"start\":157754},{\"end\":157767,\"start\":157761},{\"end\":157782,\"start\":157767},{\"end\":158017,\"start\":158004},{\"end\":158028,\"start\":158017},{\"end\":158250,\"start\":158241},{\"end\":158256,\"start\":158250},{\"end\":158263,\"start\":158256},{\"end\":158272,\"start\":158263},{\"end\":158279,\"start\":158272},{\"end\":158287,\"start\":158279},{\"end\":158548,\"start\":158542},{\"end\":158555,\"start\":158548},{\"end\":158565,\"start\":158555},{\"end\":158827,\"start\":158819},{\"end\":158834,\"start\":158827},{\"end\":158842,\"start\":158834},{\"end\":159034,\"start\":159027},{\"end\":159041,\"start\":159034},{\"end\":159051,\"start\":159041},{\"end\":159057,\"start\":159051},{\"end\":159282,\"start\":159271},{\"end\":159294,\"start\":159282},{\"end\":159306,\"start\":159294},{\"end\":159319,\"start\":159306},{\"end\":159663,\"start\":159650},{\"end\":159671,\"start\":159663},{\"end\":159678,\"start\":159671},{\"end\":159975,\"start\":159968},{\"end\":159983,\"start\":159975},{\"end\":160155,\"start\":160140},{\"end\":160168,\"start\":160155},{\"end\":160180,\"start\":160168},{\"end\":160189,\"start\":160180},{\"end\":160403,\"start\":160395},{\"end\":160411,\"start\":160403},{\"end\":160422,\"start\":160411},{\"end\":160639,\"start\":160632},{\"end\":160646,\"start\":160639},{\"end\":160652,\"start\":160646},{\"end\":160660,\"start\":160652},{\"end\":160668,\"start\":160660},{\"end\":160675,\"start\":160668},{\"end\":160926,\"start\":160914},{\"end\":160939,\"start\":160926},{\"end\":161208,\"start\":161200},{\"end\":161221,\"start\":161208},{\"end\":161479,\"start\":161468},{\"end\":161492,\"start\":161479},{\"end\":161501,\"start\":161492},{\"end\":161511,\"start\":161501},{\"end\":161520,\"start\":161511},{\"end\":161738,\"start\":161732},{\"end\":161747,\"start\":161738},{\"end\":161754,\"start\":161747},{\"end\":161761,\"start\":161754},{\"end\":162004,\"start\":161995},{\"end\":162011,\"start\":162004},{\"end\":162288,\"start\":162281},{\"end\":162296,\"start\":162288},{\"end\":162304,\"start\":162296},{\"end\":162554,\"start\":162547},{\"end\":162561,\"start\":162554},{\"end\":162569,\"start\":162561},{\"end\":162577,\"start\":162569},{\"end\":162584,\"start\":162577},{\"end\":162591,\"start\":162584},{\"end\":162852,\"start\":162844},{\"end\":162860,\"start\":162852},{\"end\":162870,\"start\":162860},{\"end\":162879,\"start\":162870},{\"end\":162885,\"start\":162879},{\"end\":162896,\"start\":162885},{\"end\":163121,\"start\":163115},{\"end\":163127,\"start\":163121},{\"end\":163136,\"start\":163127},{\"end\":163142,\"start\":163136},{\"end\":163150,\"start\":163142},{\"end\":163156,\"start\":163150},{\"end\":163443,\"start\":163435},{\"end\":163450,\"start\":163443},{\"end\":163458,\"start\":163450},{\"end\":163467,\"start\":163458},{\"end\":163474,\"start\":163467},{\"end\":163770,\"start\":163762},{\"end\":163778,\"start\":163770},{\"end\":163786,\"start\":163778},{\"end\":163793,\"start\":163786},{\"end\":163799,\"start\":163793},{\"end\":164042,\"start\":164035},{\"end\":164050,\"start\":164042},{\"end\":164056,\"start\":164050},{\"end\":164063,\"start\":164056},{\"end\":164071,\"start\":164063},{\"end\":164258,\"start\":164249},{\"end\":164265,\"start\":164258},{\"end\":164277,\"start\":164265},{\"end\":164477,\"start\":164471},{\"end\":164483,\"start\":164477},{\"end\":164492,\"start\":164483},{\"end\":164501,\"start\":164492},{\"end\":164509,\"start\":164501},{\"end\":164748,\"start\":164741},{\"end\":164760,\"start\":164748},{\"end\":164767,\"start\":164760},{\"end\":165016,\"start\":165007},{\"end\":165026,\"start\":165016},{\"end\":165235,\"start\":165222},{\"end\":165250,\"start\":165235},{\"end\":165264,\"start\":165250},{\"end\":165275,\"start\":165264},{\"end\":165286,\"start\":165275},{\"end\":165704,\"start\":165697},{\"end\":165711,\"start\":165704},{\"end\":165719,\"start\":165711}]", "bib_venue": "[{\"end\":113509,\"start\":113499},{\"end\":148337,\"start\":148275},{\"end\":165872,\"start\":165856},{\"end\":106634,\"start\":106630},{\"end\":106937,\"start\":106933},{\"end\":107219,\"start\":107213},{\"end\":107479,\"start\":107475},{\"end\":107690,\"start\":107680},{\"end\":107930,\"start\":107858},{\"end\":108273,\"start\":108269},{\"end\":108590,\"start\":108586},{\"end\":108833,\"start\":108761},{\"end\":109169,\"start\":109158},{\"end\":109451,\"start\":109444},{\"end\":109741,\"start\":109737},{\"end\":110032,\"start\":110028},{\"end\":110261,\"start\":110257},{\"end\":110454,\"start\":110450},{\"end\":110645,\"start\":110641},{\"end\":110834,\"start\":110830},{\"end\":111029,\"start\":110963},{\"end\":111306,\"start\":111222},{\"end\":111663,\"start\":111659},{\"end\":111939,\"start\":111873},{\"end\":112131,\"start\":112089},{\"end\":112369,\"start\":112290},{\"end\":112666,\"start\":112663},{\"end\":112923,\"start\":112919},{\"end\":113191,\"start\":113187},{\"end\":113497,\"start\":113493},{\"end\":113769,\"start\":113715},{\"end\":114054,\"start\":114050},{\"end\":114350,\"start\":114346},{\"end\":114621,\"start\":114617},{\"end\":114840,\"start\":114835},{\"end\":115012,\"start\":115008},{\"end\":115196,\"start\":115192},{\"end\":115367,\"start\":115357},{\"end\":115609,\"start\":115605},{\"end\":115859,\"start\":115855},{\"end\":116134,\"start\":116130},{\"end\":116382,\"start\":116378},{\"end\":116589,\"start\":116585},{\"end\":116798,\"start\":116754},{\"end\":117055,\"start\":117051},{\"end\":117309,\"start\":117305},{\"end\":117566,\"start\":117562},{\"end\":117808,\"start\":117804},{\"end\":118009,\"start\":118005},{\"end\":118258,\"start\":118254},{\"end\":118582,\"start\":118578},{\"end\":118877,\"start\":118871},{\"end\":119139,\"start\":119135},{\"end\":119406,\"start\":119402},{\"end\":119655,\"start\":119585},{\"end\":119910,\"start\":119906},{\"end\":120141,\"start\":120137},{\"end\":120428,\"start\":120424},{\"end\":120703,\"start\":120699},{\"end\":120905,\"start\":120824},{\"end\":121258,\"start\":121254},{\"end\":121561,\"start\":121540},{\"end\":121861,\"start\":121856},{\"end\":122123,\"start\":122120},{\"end\":122350,\"start\":122345},{\"end\":122612,\"start\":122608},{\"end\":122897,\"start\":122893},{\"end\":123243,\"start\":123229},{\"end\":123545,\"start\":123541},{\"end\":123790,\"start\":123787},{\"end\":124087,\"start\":124068},{\"end\":124369,\"start\":124365},{\"end\":124637,\"start\":124633},{\"end\":124909,\"start\":124901},{\"end\":125225,\"start\":125218},{\"end\":125485,\"start\":125481},{\"end\":125701,\"start\":125697},{\"end\":125958,\"start\":125955},{\"end\":126274,\"start\":126267},{\"end\":126617,\"start\":126610},{\"end\":126887,\"start\":126776},{\"end\":127179,\"start\":127176},{\"end\":127394,\"start\":127388},{\"end\":127574,\"start\":127534},{\"end\":127861,\"start\":127857},{\"end\":128182,\"start\":128178},{\"end\":128464,\"start\":128460},{\"end\":128718,\"start\":128714},{\"end\":128917,\"start\":128913},{\"end\":129091,\"start\":129087},{\"end\":129283,\"start\":129214},{\"end\":129518,\"start\":129447},{\"end\":129833,\"start\":129783},{\"end\":130148,\"start\":130143},{\"end\":130427,\"start\":130423},{\"end\":130721,\"start\":130717},{\"end\":130977,\"start\":130973},{\"end\":131238,\"start\":131234},{\"end\":131510,\"start\":131506},{\"end\":131730,\"start\":131727},{\"end\":131979,\"start\":131975},{\"end\":132252,\"start\":132248},{\"end\":132556,\"start\":132551},{\"end\":132841,\"start\":132837},{\"end\":133091,\"start\":133087},{\"end\":133368,\"start\":133364},{\"end\":133637,\"start\":133633},{\"end\":133831,\"start\":133827},{\"end\":134065,\"start\":133957},{\"end\":134418,\"start\":134413},{\"end\":134692,\"start\":134687},{\"end\":134996,\"start\":134992},{\"end\":135279,\"start\":135271},{\"end\":135498,\"start\":135482},{\"end\":135725,\"start\":135720},{\"end\":136022,\"start\":136017},{\"end\":136273,\"start\":136268},{\"end\":136455,\"start\":136408},{\"end\":136762,\"start\":136664},{\"end\":137063,\"start\":137058},{\"end\":137295,\"start\":137291},{\"end\":137522,\"start\":137390},{\"end\":137896,\"start\":137891},{\"end\":138169,\"start\":138165},{\"end\":138386,\"start\":138382},{\"end\":138567,\"start\":138563},{\"end\":138846,\"start\":138842},{\"end\":139217,\"start\":139214},{\"end\":139510,\"start\":139412},{\"end\":139814,\"start\":139712},{\"end\":140108,\"start\":140042},{\"end\":140360,\"start\":140316},{\"end\":140684,\"start\":140680},{\"end\":140979,\"start\":140975},{\"end\":141281,\"start\":141277},{\"end\":141572,\"start\":141568},{\"end\":141863,\"start\":141859},{\"end\":142128,\"start\":142050},{\"end\":142429,\"start\":142317},{\"end\":142818,\"start\":142706},{\"end\":143113,\"start\":143109},{\"end\":143312,\"start\":143308},{\"end\":143597,\"start\":143593},{\"end\":143869,\"start\":143852},{\"end\":144112,\"start\":144108},{\"end\":144351,\"start\":144278},{\"end\":144641,\"start\":144563},{\"end\":145006,\"start\":145002},{\"end\":145318,\"start\":145306},{\"end\":145643,\"start\":145637},{\"end\":145985,\"start\":145975},{\"end\":146250,\"start\":146247},{\"end\":146492,\"start\":146489},{\"end\":146781,\"start\":146775},{\"end\":147036,\"start\":146955},{\"end\":147388,\"start\":147383},{\"end\":147634,\"start\":147630},{\"end\":147893,\"start\":147889},{\"end\":148273,\"start\":148196},{\"end\":148646,\"start\":148568},{\"end\":148969,\"start\":148956},{\"end\":149320,\"start\":149300},{\"end\":149557,\"start\":149525},{\"end\":149818,\"start\":149751},{\"end\":150104,\"start\":150100},{\"end\":150355,\"start\":150351},{\"end\":150560,\"start\":150556},{\"end\":150740,\"start\":150736},{\"end\":150986,\"start\":150982},{\"end\":151204,\"start\":151155},{\"end\":151461,\"start\":151406},{\"end\":151809,\"start\":151804},{\"end\":152105,\"start\":152044},{\"end\":152433,\"start\":152429},{\"end\":152599,\"start\":152566},{\"end\":152865,\"start\":152789},{\"end\":153141,\"start\":153137},{\"end\":153354,\"start\":153265},{\"end\":153586,\"start\":153541},{\"end\":153843,\"start\":153839},{\"end\":154094,\"start\":154090},{\"end\":154354,\"start\":154350},{\"end\":154585,\"start\":154581},{\"end\":154812,\"start\":154807},{\"end\":155080,\"start\":155077},{\"end\":155324,\"start\":155321},{\"end\":155600,\"start\":155595},{\"end\":155826,\"start\":155771},{\"end\":156179,\"start\":156175},{\"end\":156411,\"start\":156406},{\"end\":156592,\"start\":156588},{\"end\":156728,\"start\":156703},{\"end\":157060,\"start\":157046},{\"end\":157322,\"start\":157318},{\"end\":157527,\"start\":157523},{\"end\":157786,\"start\":157782},{\"end\":158032,\"start\":158028},{\"end\":158291,\"start\":158287},{\"end\":158579,\"start\":158565},{\"end\":158856,\"start\":158842},{\"end\":159062,\"start\":159057},{\"end\":159269,\"start\":159203},{\"end\":159648,\"start\":159524},{\"end\":159987,\"start\":159983},{\"end\":160193,\"start\":160189},{\"end\":160426,\"start\":160422},{\"end\":160679,\"start\":160675},{\"end\":160912,\"start\":160846},{\"end\":161198,\"start\":161118},{\"end\":161524,\"start\":161520},{\"end\":161765,\"start\":161761},{\"end\":162031,\"start\":162011},{\"end\":162318,\"start\":162304},{\"end\":162545,\"start\":162484},{\"end\":162900,\"start\":162896},{\"end\":163113,\"start\":163063},{\"end\":163478,\"start\":163474},{\"end\":163813,\"start\":163799},{\"end\":164075,\"start\":164071},{\"end\":164247,\"start\":164195},{\"end\":164513,\"start\":164509},{\"end\":164739,\"start\":164677},{\"end\":165038,\"start\":165034},{\"end\":165293,\"start\":165286},{\"end\":165842,\"start\":165719}]"}}}, "year": 2023, "month": 12, "day": 17}