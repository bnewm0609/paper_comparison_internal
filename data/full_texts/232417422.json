{"id": 232417422, "updated": "2023-10-06 05:13:49.259", "metadata": {"title": "Model-Contrastive Federated Learning", "authors": "[{\"first\":\"Qinbin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Bingsheng\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Dawn\",\"last\":\"Song\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 3, "day": 30}, "abstract": "Federated learning enables multiple parties to collaboratively train a machine learning model without communicating their local data. A key challenge in federated learning is to handle the heterogeneity of local data distribution across parties. Although many studies have been proposed to address this challenge, we find that they fail to achieve high performance in image datasets with deep learning models. In this paper, we propose MOON: model-contrastive federated learning. MOON is a simple and effective federated learning framework. The key idea of MOON is to utilize the similarity between model representations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. Our extensive experiments show that MOON significantly outperforms the other state-of-the-art federated learning algorithms on various image classification tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.16257", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiHS21", "doi": "10.1109/cvpr46437.2021.01057"}}, "content": {"source": {"pdf_hash": "748c4f1945b0d994ecef38c8aac01db1c6dc7029", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.16257v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.16257", "status": "GREEN"}}, "grobid": {"id": "0df8b184f112fd22dc6d7148edd11649519b65df", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/748c4f1945b0d994ecef38c8aac01db1c6dc7029.txt", "contents": "\nModel-Contrastive Federated Learning\n\n\nQinbin Li qinbin@comp.nus.edu.sg \nDawn Song UC Berkeley\nNational University of Singapore\nBingsheng He National University of Singapore\n\n\nModel-Contrastive Federated Learning\n\nFederated learning enables multiple parties to collaboratively train a machine learning model without communicating their local data. A key challenge in federated learning is to handle the heterogeneity of local data distribution across parties. Although many studies have been proposed to address this challenge, we find that they fail to achieve high performance in image datasets with deep learning models. In this paper, we propose MOON: modelcontrastive federated learning. MOON is a simple and effective federated learning framework. The key idea of MOON is to utilize the similarity between model representations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. Our extensive experiments show that MOON significantly outperforms the other state-of-the-art federated learning algorithms on various image classification tasks.\n\nIntroduction\n\nDeep learning is data hungry. Model training can benefit a lot from a large and representative dataset (e.g., ImageNet [6] and COCO [31]). However, data are usually dispersed among different parties in practice (e.g., mobile devices and companies). Due to the increasing privacy concerns and data protection regulations [40], parties cannot send their private data to a centralized server to train a model.\n\nTo address the above challenge, federated learning [20,44,27,26] enables multiple parties to jointly learn a machine learning model without exchanging their local data. A popular federated learning algorithm is FedAvg [34]. In each round of FedAvg, the updated local models of the parties are transferred to the server, which further aggregates the local models to update the global model. The raw data is not exchanged during the learning process. Federated learning has emerged as an important machine learning area and attracted many research interests [34,28,22,25,41,5,16,2,11]. Moreover, it has been applied in many applications such as medical imaging [21,23], object detection [32], and landmark classification [15].\n\nA key challenge in federated learning is the heterogeneity of data distribution on different parties [20]. The data can be non-identically distributed among the parties in many real-world applications, which can degrade the performance of federated learning [22,29,24]. When each party updates its local model, its local objective may be far from the global objective. Thus, the averaged global model is away from the global optima. There have been some studies trying to address the non-IID issue in the local training phase [28,22]. FedProx [28] directly limits the local updates by 2 -norm distance, while SCAFFOLD [22] corrects the local updates via variance reduction [19]. However, as we show in the experiments (see Section 4), these approaches fail to achieve good performance on image datasets with deep learning models, which can be as bad as FedAvg.\n\nIn this work, we address the non-IID issue from a novel perspective based on an intuitive observation: the global model trained on a whole dataset is able to learn a better representation than the local model trained on a skewed subset. Specifically, we propose model-contrastive learning (MOON), which corrects the local updates by maximizing the agreement of representation learned by the current local model and the representation learned by the global model. Unlike the traditional contrastive learning approaches [3,4,12,35], which achieve state-of-the-art results on learning visual representations by comparing the representations of different images, MOON conducts contrastive learning in model-level by comparing the representations learned by different models. Overall, MOON is a simple and effective federated learning framework, and addresses the non-IID data issue with the novel design of model-based contrastive learning.\n\nWe conduct extensive experiments to evaluate the effectiveness of MOON. MOON significantly outperforms the other state-of-the-art federated learning algorithms [34,28,22] on various image classification datasets including CIFAR-10, CIFAR-100, and Tiny-Imagenet. With only lightweight modifications to FedAvg, MOON outperforms existing approaches by at least 2% accuracy in most cases. Moreover, the improvement of MOON is very significant Figure 1. The FedAvg framework. In this paper, we focus on the second step, i.e., the local training phase. on some settings. For example, on CIFAR-100 dataset with 100 parties, MOON achieves 61.8% top-1 accuracy, while the best top-1 accuracy of existing studies is 55%.\n\n\nBackground and Related Work\n\n\nFederated Learning\n\nFedAvg [34] has been a de facto approach for federated learning. The framework of FedAvg is shown in Figure 1. There are four steps in each round of FedAvg. First, the server sends a global model to the parties. Second, the parties perform stochastic gradient descent (SGD) to update their models locally. Third, the local models are sent to a central server. Last, the server averages the model weights to produce a global model for the training of the next round.\n\nThere have been quite some studies trying to improve FedAvg on non-IID data. Those studies can be divided into two categories: improvement on local training (i.e., step 2 of Figure 1) and on aggregation (i.e., step 4 of Figure 1). This study belongs to the first category.\n\nAs for studies on improving local training, FedProx [28] introduces a proximal term into the objective during local training. The proximal term is computed based on the 2norm distance between the current global model and the local model. Thus, the local model update is limited by the proximal term during the local training. SCAFFOLD [22] corrects the local updates by introducing control variates. Like the training model, the control variates are also updated by each party during local training. The difference between the local control variate and the global control variate is used to correct the gradients in local training. However, FedProx shows experiments on MNIST and EMNIST only with multinomial logistic regression, while SCAFFOLD only shows experiments on EMNIST with logistic regression and 2-layer fully connected layer. The effectiveness of FedProx and SCAFFOLD on image datasets with deep learning models has not been well explored. As shown in our experiments, those studies have little or even no advantage over FedAvg, which motivates this study for a new approach of handling non-IID image datasets with deep learning models. We also notice that there are other related contemporary work [1,30,43] when preparing this paper. We leave the comparison between MOON and these contemporary work as future studies.\n\nAs for studies on improving the aggregation phase, FedMA [41] utilizes Bayesian non-parametric methods to match and average weights in a layer-wise manner. Fe-dAvgM [14] applies momentum when updating the global model on the server. Another recent study, FedNova [42], normalizes the local updates before averaging. Our study is orthogonal to them and potentially can be combined with these techniques as we work on the local training phase.\n\nAnother research direction is personalized federated learning [8,7,10,47,17], which tries to learn personalized local models for each party. In this paper, we study the typical federated learning, which tries to learn a single global model for all parties.\n\n\nContrastive Learning\n\nSelf-supervised learning [18,9,3,4,12,35] is a recent hot research direction, which tries to learn good data representations from unlabeled data. Among those studies, contrastive learning approaches [3,4,12,35] achieve state-ofthe-art results on learning visual representations. The key idea of contrastive learning is to reduce the distance between the representations of different augmented views of the same image (i.e., positive pairs), and increase the distance between the representations of augmented views of different images (i.e., negative pairs).\n\nA typical contrastive learning framework is SimCLR [3]. Given an image x, SimCLR first creates two correlated views of this image using different data augmentation operators, denoted x i and x j . A base encoder f (\u00b7) and a projection head g(\u00b7) are trained to extract the representation vectors and map the representations to a latent space, respectively. Then, a contrastive loss (i.e., NT-Xent [38]) is applied on the projected vector g(f (\u00b7)), which tries to maximize agreement between differently augmented views of the same image. Specifically, given 2N augmented views and a pair of view x i and x j of same image, the contrastive loss for this pair is defined as\nl i,j = \u2212 log exp(sim(x i , x j )/\u03c4 ) 2N k=1 I [k =i] exp(sim(x i , x k )/\u03c4 )(1)\nwhere sim(\u00b7, \u00b7) is a cosine similarity function and \u03c4 de-notes a temperature parameter. The final loss is computed by summing the contrastive loss of all pairs of the same image in a mini-batch. Besides SimCLR, there are also other contrastive learning frameworks such as CPC [36], CMC [39] and MoCo [12]. We choose SimCLR for its simplicity and effectiveness in many computer vision tasks. Still, the basic idea of contrastive learning is similar among these studies: the representations obtained from different images should be far from each other and the representations obtained from the same image should be related to each other. The idea is intuitive and has been shown to be effective.\n\nThere is one recent study [46] that combines federated learning with contrastive learning. They focus on the unsupervised learning setting. Like SimCLR, they use contrastive loss to compare the representations of different images. In this paper, we focus on the supervised learning setting and propose model-contrastive learning to compare representations learned by different models.\n\n\nModel-Contrastive Federated Learning\n\n\nProblem Statement\n\nSuppose there are N parties, denoted P 1 , ..., P N . Party P i has a local dataset D i . Our goal is to learn a machine learning model w over the dataset D i\u2208[N ] D i with the help of a central server, while the raw data are not exchanged. The objective is to solve\narg min w L(w) = N i=1 |D i | |D| L i (w),(2)\nwhere L i (w) = E (x,y)\u223cD i [ i (w; (x, y))] is the empirical loss of P i .\n\n\nMotivation\n\nMOON is based on an intuitive idea: the model trained on the whole dataset is able to extract a better feature representation than the model trained on a skewed subset. For example, given a model trained on dog and cat images, we cannot expect the features learned by the model to distinguish birds and frogs which never exist during training.\n\nTo further verify this intuition, we conduct a simple experiment on CIFAR-10. Specifically, we first train a CNN model (see Section 4.1 for the detailed structure) on CIFAR-10. We use t-SNE [33] to visualize the hidden vectors of images from the test dataset as shown in Figure 2a. Then, we partition the dataset into 10 subsets in an unbalanced way (see Section 4.1 for the partition strategy) and train a CNN model on each subset. Figure  ). The local training phase even leads the model to learn a worse representation due to the skewed local data distribution. This further verifies that the global model should be able to learn a better feature representation than the local model, and there is a drift in the local updates. Therefore, under non-IID data scenarios, we should control the drift and bridge the gap between the representations learned by the local model and the global model.\n\n\nMethod\n\nBased on the above intuition, we propose MOON. MOON is designed as a simple and effective approach based on FedAvg, only introducing lightweight but novel modifications in the local training phase. Since there is always drift in local training and the global model learns a better representation than the local model, MOON aims to decrease the distance between the representation learned by the local model and the representation learned by the global model, and increase the distance between the representation learned by the local model and the representation learned by the previous local model. We achieve this from the inspiration of contrastive learning, which is now mainly used to learn visual representations. In the following, we present the network architecture, the local learning objective and the learning procedure. At last, we discuss the relation to contrastive learning.\n\n\nNetwork Architecture\n\nThe network has three components: a base encoder, a projection head, and an output layer. The base encoder is used to extract representation vectors from inputs. Like [3], an additional projection head is introduced to map the representation to a space with a fixed dimension. Last, as we study on the supervised setting, the output layer is used to produce predicted values for each class. For ease of presentation, with model weight w, we use F w (\u00b7) to denote the whole network and R w (\u00b7) to denote the network before the output layer (i.e., R w (x) is the mapped representation vector of input x).\n\n\nLocal Objective\n\nAs shown in Figure 3, our local loss consists two parts. The first part is a typical loss term (e.g., cross-entropy loss) in supervised learning denoted as sup . The second part is our proposed model-contrastive loss term denoted as con .\n\nSuppose party P i is conducting the local training. It receives the global model w t from the server and updates the model to w t i in the local training phase. For every input x, we extract the representation of x from the global model w t (i.e., z glob = R w t (x)), the representation of x from the lo-\ncal model of last round w t\u22121 i (i.e., z prev = R w t\u22121 i (x))\n, and the representation of x from the local model being updated\nw t i (i.e., z = R w t i (x))\n. Since the global model should be able to extract better representations, our goal is to decrease the distance between z and z glob , and increase the distance between z and z prev . Similar to NT-Xent loss [38], we define model-contrastive loss as\ncon = \u2212 log exp(sim(z, z glob )/\u03c4 ) exp(sim(z, z glob )/\u03c4 ) + exp(sim(z, z prev )/\u03c4 )(3)\nwhere \u03c4 denotes a temperature parameter. The loss of an input (x, y) is computed by\n= sup (w t i ; (x, y)) + \u00b5 con (w t i ; w t\u22121 i ; w t ; x),(4)\nwhere \u00b5 is a hyper-parameter to control the weight of model-contrastive loss. The local objective is to minimize\nmin w t i E (x,y)\u223cD i [ sup (w t i ; (x, y)) + \u00b5 con (w t i ; w t\u22121 i ; w t ; x)].\n(5) The overall federated learning algorithm is shown in Algorithm 1. In each round, the server sends the global model to the parties, receives the local model from the parties, and updates the global model using weighted averaging. In local training, each party uses stochastic gradient descent to update the global model with its local data, while the objective is defined in Eq. (5). \nw t to P i 6 w t i \u2190 PartyLocalTraining(i, w t ) 7 w t+1 \u2190 N k=1 |D i | |D| w t k 8 return w T 9 PartyLocalTraining(i, w t ): 10 w t i \u2190 w t 11 for epoch i = 1, 2, ..., E do 12 for each batch b = {x, y} of D i do 13 sup \u2190 CrossEntropyLoss(F w t i (x), y) 14 z \u2190 R w t i (x) 15 z glob \u2190 R w t (x) 16 z prev \u2190 R w t\u22121 i (x) 17 con \u2190 \u2212 log exp(sim(z,z glob )/\u03c4 ) exp(sim(z,z glob )/\u03c4 )+exp(sim(z,zprev)/\u03c4 ) 18 \u2190 sup + \u00b5 con 19 w t i \u2190 w t i \u2212 \u03b7\u2207 20 return w t i to server\nFor simplicity, we describe MOON without applying sampling technique in Algorithm 1. MOON is still applicable when only a sample set of parties participate in federated learning each round. Like FedAvg, each party maintains its local model, which will be replaced by the global model and updated only if the party is selected to participate in a round. MOON only needs the latest local model that the party has, even though it may not be updated in round (t \u2212 1) (e.g.,\nw t\u22121 i = w t\u22122 i\n). An notable thing is that considering an ideal case where the local model is good enough and learns (almost) the same representation as the global model (i.e., z glob = z prev ), the model-contrastive loss will be a constant (i.e., \u2212 log 1 2 ). Thus, MOON will produce the same result as FedAvg, since there is no heterogeneity issue. In this sense, our approach is robust regardless of different amount of drifts.\n\n\nComparisons with Contrastive Learning\n\nA comparison between MOON and SimCLR is shown in Figure 4. The model-contrastive loss compares representations learned by different models, while the contrastive loss compares representations of different images. We also highlight the key difference between MOON and traditional contrastive learning: MOON is currently for supervised learning in a federated setting while contrastive learning is for unsupervised learning in a centralized setting. Drawing the inspirations from contrastive learning, MOON is a new learning methodology in handling non-IID data distributions among parties in federated learning.\n\n\nExperiments\n\n\nExperimental Setup\n\nWe compare MOON with three state-of-the-art approaches including (1) FedAvg [34], (2) FedProx [28], and (3) SCAFFOLD [22]. We also compare a baseline approach named SOLO, where each party trains a model with its local data without federated learning. We conduct experiments on three datasets including CIFAR-10, CIFAR-100, and Tiny-Imagenet 1 (100,000 images with 200 classes). Moreover, we try two different network architectures. For CIFAR-10, we use a CNN network as the base encoder, which has two 5x5 convolution layers followed by 2x2 max pooling (the first with 6 channels and the second with 16 channels) and two fully connected layers with ReLU activation (the first with 120 units and the second with 84 units). For CIFAR-100 and Tiny-Imagenet, we use ResNet-50 [13] as the base encoder. For all datasets, like [3], we use a 2layer MLP as the projection head. The output dimension of the projection head is set to 256 by default. Note that all baselines use the same network architecture as MOON (including the projection head) for fair comparison.\n\nWe use PyTorch [37] to implement MOON and the other baselines. The code is publicly available 2 . We use the SGD optimizer with a learning rate 0.01 for all approaches. The SGD weight decay is set to 0.00001 and the SGD momentum is set to 0.9. The batch size is set to 64. The number of local epochs is set to 300 for SOLO. The number of local epochs is set to 10 for all federated learning approaches unless explicitly specified. The number of communication rounds is set to 100 for CIFAR-10/100 and 20 for Tiny-ImageNet, where all federated learning approaches have little or no accuracy gain with more communications. For MOON, we set the temperature parameter to 0.5 by default like [3].\n\nLike previous studies [45,41], we use Dirichlet distribution to generate the non-IID data partition among parties. Specifically, we sample p k \u223c Dir N (\u03b2) and allocate a p k,j proportion of the instances of class k to party j, where Dir(\u03b2) is the Dirichlet distribution with a concentration parameter \u03b2 (0.5 by default). With the above partitioning strategy, each party can have relatively few (even no) data samples in some classes. We set the number of parties to 10 by default. The data distributions among parties in default settings are shown in Figure 5. For more experimental results, please refer to Appendix.\n\n\nAccuracy Comparison\n\nFor MOON, we tune \u00b5 from {0.1, 1, 5, 10} and report the best result. The best \u00b5 of MOON for CIFAR-10, CIFAR-100, and Tiny-Imagenet are 5, 1, and 1, respectively. Note that FedProx also has a hyper-parameter \u00b5 to control the weight of its proximal term (i.e., L F edP rox = F edAvg + \u00b5 prox ). For FedProx, we tune \u00b5 from {0.001, 0.01, 0.1, 1} (the range is also used in the previous paper [28]) and report the best result. The best \u00b5 of FedProx for CIFAR-10, CIFAR-100, and Tiny-Imagenet are 0.01, 0.001, and 0.001, respectively. Unless explicitly specified, we use these \u00b5 settings for all the remaining experiments.    Figure 6 shows the accuracy in each round during training. As we can see, the model-contrastive loss term has little influence on the convergence rate with best \u00b5. The speed of accuracy improvement in MOON is almost the same as FedAvg at the beginning, while it can achieve a better accuracy later benefit from the model-contrastive loss. Since the best \u00b5 values are generally small in FedProx, FedProx with best \u00b5 is very close to FedAvg, especially on CIFAR-10 and CIFAR-100. However, when setting \u00b5 = 1, FedProx becomes very slow due to the additional proximal term. This implies that limiting the 2 -norm distance between the local model and the global model is not an effective solution. Our model-contrastive loss can effectively increase the accuracy without slowing down the convergence. We show the number of communication rounds to achieve the same accuracy as running FedAvg for 100 rounds on CIFAR-10/100 or 20 rounds on Tiny-Imagenet in Table 2. We can observe that the number of communication rounds is significantly reduced in MOON. MOON needs about half the number of communication rounds on CIFAR-100 and Tiny-Imagenet compared with FedAvg. On CIFAR-10, the speedup of MOON is even close to 4. MOON is much more communication-efficient than the other approaches.\n\n\nCommunication Efficiency\n\n\nNumber of Local Epochs\n\nWe study the effect of number of local epochs on the accuracy of final model. The results are shown in Figure  7. When the number of local epochs is 1, the local update is very small. Thus, the training is slow and the accuracy is relatively low given the same number of communication rounds. All approaches have a close accuracy (MOON is still the best). When the number of local epochs becomes too large, the accuracy of all approaches drops, which is due to the drift of local updates, i.e., the local optima are not consistent with the global optima. Nevertheless, MOON clearly outperforms the other approaches. This further verifies that MOON can effectively mitigate the negative effects  of the drift by too many local updates.\n\n\nScalability\n\nTo show the scalability of MOON, we try a larger number of parties on CIFAR-100. Specifically, we try two settings: (1) We partition the dataset into 50 parties and all parties participate in federated learning in each round. (2) We partition the dataset into 100 parties and randomly sample 20 parties to participate in federated learning in each round (client sampling technique introduced in FedAvg [34]). The results are shown in Table 3 and Figure 8. For MOON, we show the results with \u00b5 = 1 (best \u00b5 from Section 4.2) and \u00b5 = 10. For MOON (\u00b5 = 1), it outperforms the Fe-dAvg and FedProx over 2% accuracy at 200 rounds with 50 parties and 3% accuracy at 500 rounds with 100 parties. Moreover, for MOON (\u00b5 = 10), although the large model-contrastive loss slows down the training at the beginning as shown in Figure 8, MOON can outperform the other approaches a lot with more communication rounds. Compared with FedAvg and FedProx, MOON achieves about about 7% higher accuracy at 200 rounds with 50 parties and at 500 rounds with 100 parties. SCAFFOLD has a low accuracy with a relatively large number of parties. \n\n\nHeterogeneity\n\nWe study the effect of data heterogeneity by varying the concentration parameter \u03b2 of Dirichlet distribution on CIFAR-100. For a smaller \u03b2, the partition will be more unbalanced. The results are shown in Table 4. MOON always achieves the best accuracy among three unbalanced levels. When the unbalanced level decreases (i.e., \u03b2 = 5), Fed-Prox is worse than FedAvg, while MOON still outperforms FedAvg with more than 2% accuracy. The experiments demonstrate the effectiveness and robustness of MOON.\n\n\nLoss Function\n\nTo maximize the agreement between the representation learned by the global model and the representation learned  by the local model, our model-contrastive loss con is proposed inspired by NT-Xent loss [3]. Another intuitive option is to use 2 regularization, and the local loss is\n= sup + \u00b5 z \u2212 z glob 2(6)\nHere we compare the approaches using different kinds of loss functions to limit the representation: no additional term (i.e., FedAvg: L = sup ), 2 norm, and our modelcontrastive loss. The results are shown in Table 5. We can observe that simply using 2 norm even cannot improve the accuracy compared with FedAvg on CIFAR-10. While using 2 norm can improve the accuracy on CIFAR-100 and Tiny-Imagenet, the accuracy is still lower than MOON. Our model-contrastive loss is an effective way to constrain the representations.\n\nOur model-contrastive loss influences the local model from two aspects. Firstly, the local model learns a close representation to the global model. Secondly, the local model also learns a better representation than the previous one until the local model is good enough (i.e., z = z glob and con becomes a constant).\n\n\nConclusion\n\nFederated learning has become a promising approach to resolve the pain of data silos in many domains such as \n\n\nA. More Details of the Datasets\n\nThe statistics of each dataset are shown in Table 6 when setting \u03b2 = 0.5. All datasets provide a training dataset and test dataset. All the reported accuracies are computed on the test dataset. 5,000 181 10,000 Tiny-Imagenet 10,000 99 10,000 Figure 9 and Figure 10 show the data distribution of \u03b2 = 0.1 and \u03b2 = 5 (used in Section 4.6 of the main paper), respectively.\n\n\nB. Projection Head\n\nWe use a projection head to map the representation like [3]. Here we study the effect of the projection head. We remove the projection head and conduct experiments on CIFAR-10 and CIFAR-100 (Note that the network architecture changes for all approaches). The results are shown in Table 7. We can observe that MOON can benefit a lot from the projection head. The accuracy of MOON can be improved by about 2% on average with a projection head.  \n\n\nC. IID Partition\n\nTo further show the effect of our model-contrastive loss, we compare MOON and FedAvg when there is no heterogeneity among local datasets. The dataset is randomly and equally partitioned into the parties. The results are shown in Table 8. We can observe that the model-contrastive loss has little influence on the training when the local datasets are IID. The accuracy of MOON is still very close to FedAvg even though with a large \u00b5. MOON is still applicable when there is no heterogeneity issue in data distributions across parties. We show the accuracy of MOON with different \u00b5 in Table 9. The best \u00b5 for CIFAR-10, CIFAR-100, and Tiny-Imagenet are 5, 1, and 1, respectively. When \u00b5 is set to a small value (i.e., \u00b5 = 0.1), the accuracy of MOON is very close to FedAvg (i.e., \u00b5 = 0) since the impact of modelcontrastive loss is small. As long as we set \u00b5 \u2265 1, MOON can benefit a lot from the model-contrastive loss. Overall, we find that \u00b5 = 1 is a reasonable good choice if they do not want to tune the parameter, where MOON achieves at least 2% higher accuracy than FedAvg.\n\n\nD.2. Effect of temperature and output dimension\n\nWe tune \u03c4 from {0.1, 0.5, 1.0} and tune the output dimension of projection head from {64, 128, 256}. The results are shown in Figure 11. The best \u03c4 for CIFAR-10, CIFAR-100, and Tiny-Imagenet are 0.5, 1.0, and 0.5, respectively. The best output dimension for CIFAR-10, CIFAR-100, and Tiny-Imagenet are 128, 256, and 128, respectively. Generally, MOON is stable regarding the change of temperature and output dimension. As we have shown in the main paper, MOON already improves FedAvg a lot with a default setting of temperature (i.e., 0.5) and output dimension (i.e., 256). Users may tune these two hyper-parameters to achieve even better accuracy. \n\n\nE. Combining with FedAvgM\n\nAs we have mentioned in the fourth paragraph of Section 2.1, MOON can be combined with the approaches working on improving the aggregation phase. Here we combine MOON and FedAvgM [14]. We tune the server momentum \u03b2 \u2208 {0.1, 0.7, 0.9}. With the default experimental setting in Section 4.1, the results are shown in Table 10. While Fe-dAvgM is better than FedAvg, MOON can further improve FedAvgM by 2-3%.\n\n\nF. Computation Cost\n\nSince MOON introduces an additional loss term in the local training phase, the training of MOON will be slower than FedAvg. For the experiments in Table 1, the average   Table 11. The average training time per round.  Method  CIFAR-10 CIFAR-100 Tiny-Imagenet  FedAvg  330s  20min  103min  FedProx  340s  24min  135min  SCAFFOLD  332s  20min  112min  MOON  337s  31min  197min training time per round with a NVIDIA Tesla V100 GPU and four Intel Xeon E5-2684 20-core CPUs are shown in Table 11. Compared with FedAvg, the computation overhead of MOON is acceptable especially on CIFAR-10 and CIFAR-100.\n\n\nG. Number of Negative Pairs\n\nIn typical contrastive learning, the performance usually can be improved by increasing the number of negative pairs (i.e., views of different images). In MOON, the negative pair is the local model being updated and the local model from the previous round. We consider using a single negative pair during training in the main paper. It is possible to consider multiple negative pairs if we include multiple local models from the previous rounds. Suppose the current round is t. Let k denotes the maximum number of negative pairs. Let z i prev = R w t\u2212i i (x) (i.e., z i prev is the representation learned by the local model from (t \u2212 i) round). Then, our local objective is con = \u2212 log exp(sim(z, z glob )/\u03c4 ) exp(sim(z, z glob )/\u03c4 ) + k i=1 exp(sim(z, z i prev )/\u03c4 )\n\nIf k = 1, then the objective is the same as MOON presented in the main paper. If k > t, since there are at most t local models from previous rounds, we only consider the previous t local models (i.e., only t negative pairs). There is no model-contrastive loss if t = 0 (i.e., the first round). Here we study the effect of the maximum number of negative pairs on CIFAR-10. The results are shown in Table 12. Unlike typical contrastive learning, the accuracy of MOON cannot be increased by increasing the number of negative pairs. MOON can achieve the best accuracy when k = 1, which is presented in our main paper. \n\nFigure 2 .\n22b shows the t-SNE visualization of a randomly selected model. Apparently, the model trained on the subset learns poor features. The feature representations of most classes are even mixed and T-SNE visualizations of hidden vectors on CIFAR-10. cannot be distinguished. Then, we run FedAvg algorithm on 10 subsets and show the representation learned by the global model in Figure 2c and the representation learned by a selected local model (trained based on the global model) in Figure 2d. We can observe that the points with the same class are more divergent in Figure 2d compared with Figure 2c (e.g., see class 9\n\nFigure 3 .\n3The local loss in MOON. Algorithm 1: The MOON framework Input: number of communication rounds T , number of parties N , number of local epochs E, temperature \u03c4 , learning rate \u03b7, hyper-parameter \u00b5 Output: The final model w T 1 Server executes: 2 initialize w 0 3 for t = 0, 1, ..., T \u2212 1 do 4 for i = 1, 2, ..., N in parallel do 5 send the global model\n\nFigure 4 .\n4The comparison between SimCLR and MOON. Here x denotes an image, w denotes a model, and R denotes the function to compute representation. SimCLR maximizes the agreement between representations of different views of the same image, while MOON maximizes the agreement between representations of the local model and the global model on the mini-batches.\n\nFigure 5 .\n5The data distribution of each party using non-IID data partition. The color bar denotes the number of data samples. Each rectangle represents the number of data samples of a specific class in a party.\n\nFigure 6 .\n6The top-1 test accuracy in different number of communication rounds. For FedProx, we report both the accuracy with best \u00b5 and the accuracy with \u00b5 = 1.\n\nFigure 7 .\n7The top-1 test accuracy with different number of local epochs. For MOON and FedProx, \u00b5 is set to the best \u00b5 from Section 4.2 for all numbers of local epochs. The accuracy of SCAFFOLD is quite bad when number of local epochs is set to 1 (45.3% on CIFAR10, 20.4% on CIFAR-100, 2.6% on Tiny-Imagenet). The accuracy of FedProx on Tiny-Imagenet with one local epoch is 1.2%.\n\nFigure 8 .\n8The top-1 test accuracy on CIFAR-100 with 50/100 parties.\n\nFigure 9 .Figure 10 .\n910The data distribution of each party using non-IID data partition with \u03b2 = 0The data distribution of each party using non-IID data partition with \u03b2 = 5.\n\nFigure 11 .\n11The top-1 accuracy of MOON trained with different temperatures and output dimensions.\n\nTable 1\n1shows the top-1 test accuracy of all approaches1 https://www.kaggle.com/c/tiny-imagenet \n2 https://github.com/QinbinLi/MOON \n\n\n\nTable 1 .\n1The top-1 accuracy of MOON and the other baselines on test datasets. For MOON, FedAvg, FedProx, and SCAFFOLD, we run three trials and report the mean and standard derivation. For SOLO, we report the mean and standard derivation among all parties. 6% accuracy on average of all tasks. For Fed-Prox, its accuracy is very close to FedAvg. The proximal term in FedProx has little influence in the training since \u00b5 is small. However, when \u00b5 is not set to a very small value, the convergence of FedProx is quite slow (see Section 4.3) and the accuracy of FedProx is bad. For SCAFFOLD, it has much worse accuracy on CIFAR-100 and Tiny-Imagenet than other federated learning approaches.Method \nCIFAR-10 \nCIFAR-100 \nTiny-Imagenet \nMOON \n69.1%\u00b10.4% 67.5%\u00b10.4% 25.1%\u00b10.1% \nFedAvg \n66.3%\u00b10.5% 64.5% \u00b10.4% 23.0%\u00b10.1% \nFedProx \n66.9%\u00b10.2% 64.6%\u00b10.2% \n23.2%\u00b10.2% \nSCAFFOLD 66.6%\u00b10.2% 52.5% \u00b10.3% 16.0%\u00b10.2% \nSOLO \n46.3% \u00b15.1% 22.3%\u00b11.0% \n8.6%\u00b10.4% \n\nwith the above default setting. Under non-IID settings, \nSOLO demonstrates much worse accuracy than other fed-\nerated learning approaches. This demonstrates the benefits \nof federated learning. Comparing different federated learn-\ning approaches, we can observe that MOON is consistently \nthe best approach among all tasks. It can outperform Fe-\ndAvg by 2.\n\nTable 2 .\n2The number of rounds of different approaches to achieve the same accuracy as running FedAvg for 100 rounds (CIFAR-10/100) or 20 rounds (Tiny-Imagenet). The speedup of an approach is computed against FedAvg.Method \nCIFAR-10 \nCIFAR-100 \nTiny-Imagenet \n#rounds speedup #rounds speedup #rounds speedup \nFedAvg \n100 \n1\u00d7 \n100 \n1\u00d7 \n20 \n1\u00d7 \nFedProx \n52 \n1.9\u00d7 \n75 \n1.3\u00d7 \n17 \n1.2\u00d7 \nSCAFFOLD \n80 \n1.3\u00d7 \n<1\u00d7 \n<1\u00d7 \nMOON \n27 \n3.7\u00d7 \n43 \n2.3\u00d7 \n11 \n1.8\u00d7 \n\n\n\nTable 3 .\n3The accuracy with 50 parties and 100 parties (sample frac-tion=0.2) on CIFAR-100.Method \n#parties=50 \n#parties=100 \n100 rounds 200 rounds 250 rounds 500 rounds \nMOON (\u00b5=1) \n54.7% \n58.8% \n54.5% \n58.2% \nMOON (\u00b5=10) \n58.2% \n63.2% \n56.9% \n61.8% \nFedAvg \n51.9% \n56.4% \n51.0% \n55.0% \nFedProx \n52.7% \n56.6% \n51.3% \n54.6% \nSCAFFOLD \n35.8% \n44.9% \n37.4% \n44.5% \nSOLO \n10%\u00b10.9% \n7.3%\u00b10.6% \n\n\n\nTable 4 .\n4The test accuracy with \u03b2 from {0.1, 0.5, 5}. \nMethod \n\u03b2 = 0.1 \n\u03b2 = 0.5 \n\u03b2 = 5 \n\nMOON \n64.0% \n67.5% \n68.0% \nFedAvg \n62.5% \n64.5% \n65.7% \nFedProx \n62.9% \n64.6% \n64.9% \nSCAFFOLD \n47.3% \n52.5% \n55.0% \nSOLO \n15.9%\u00b11.5% 22.3%\u00b11% 26.6%\u00b11.4% \n\n\n\nTable 5 .\n5The top-1 accuracy with different kinds of loss for the second term of local objective. We tune \u00b5 from {0.001, 0.01 , 0.1, \n\n\nTable 6 .\n6The statistics of datasets.dataset \n#training samples/party #test samples \nmean \nstd \nCIFAR10 \n5,000 \n1,165 \n10,000 \nCIFAR100 \n\n\nTable 7 .\n7The top1-accuracy with/without projection head.Method \nCIFAR-10 \nCIFAR-100 \n\nwithout \nprojection \nhead \n\nMOON \n66.8% \n66.1% \nFedAvg \n66.7% \n65.0% \nFedProx \n67.5% \n65.4% \nSCAFFOLD \n67.1% \n49.5% \nSOLO \n39.8%\u00b13.9% 22.5%\u00b11.1% \n\nwith \nprojection \nhead \n\nMOON \n69.1% \n67.5% \nFedAvg \n66.3% \n64.5% \nFedProx \n66.9% \n64.6% \nSCAFFOLD \n66.6% \n52.5% \nSOLO \n46.3%\u00b15.1% 22.3%\u00b11.0% \n\n\n\nTable 8 .\n8The top-1 accuracy of MOON and FedAvg with IID data partition on CIFAR-10. D. Hyper-Parameters Study D.1. Effect of \u00b5Method \nTop-1 accuracy \n\nMOON \n\n\u00b5 = 0.1 \n73.6% \n\u00b5 = 1 \n73.6% \n\u00b5 = 5 \n73.0% \n\u00b5 = 10 \n72.8% \nFedAvg (\u00b5 = 0) \n73.4% \n\n\n\nTable 9 .\n9The test accuracy of MOON with \u00b5 from {0, 0.1, 1, 5, 10}. Note that MOON is actually FedAvg when \u00b5 = 0.Table 10. The combining of MOON and FedAvgM.\u00b5 \nCIFAR-10 CIFAR-100 Tiny-Imagenet \n0 \n66.3% \n64.5% \n23.0% \n0.1 \n66.5% \n65.1% \n23.4% \n1 \n68.4% \n67.5% \n25.1% \n5 \n69.1% \n67.1% \n24.4% \n10 \n68.3% \n67.3% \n25.0% \n\nDatasets \nFedAvg MOON FedAvgM MOON+FedAvgM \nCIFAR-10 \n66.3% \n69.1% \n67.1% \n69.6% \nCIFAR-100 \n64.5% \n67.5% \n65.1% \n67.8% \nTiny-Imagenet \n23.0% \n25.1% \n23.4% \n25.5% \n\n\n\nTable 12 .\n12The effect of maximum number of negative pairs. We tune \u00b5 from {0.1, 1, 5, 10} for all approaches and report the best accuracy. maximum number of negative pairs top-1 accuracyk = 1 \n69.1% \nk = 2 \n67.2% \nk = 5 \n67.7% \nk = 100 \n63.5% \n\nAcknowledgementsThis research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-RP-2020-018). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of National Research Foundation, Singapore. The authors thank Jianxin Wu, Chaoyang He, Shixuan Sun, Yaqi Xie and Yuhang Chen for their feedback. The authors also thank Yuzhi Zhao, Wei Wang, and Mo Sha for their supports of computing resources.\nFederated learning based on dynamic regularization. Yue Durmus Alp Emre Acar, Ramon Zhao, Matthew Matas, Paul Mattina, Venkatesh Whatmough, Saligrama, International Conference on Learning Representations. Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Fed- erated learning based on dynamic regularization. In Interna- tional Conference on Learning Representations, 2021.\n\nSebastian Caldas, Sai Meher Karthik, Peter Duddu, Tian Wu, Jakub Li, Brendan Kone\u010dn\u1ef3, Mcmahan, arXiv:1812.01097Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprintSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Kone\u010dn\u1ef3, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated set- tings. arXiv preprint arXiv:1812.01097, 2018.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv:2002.05709arXiv preprintTing Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\n\nBig self-supervised models are strong semi-supervised learners. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton, arXiv:2006.10029arXiv preprintTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised mod- els are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020.\n\nFederated bayesian optimization via thompson sampling. Advances in Neural Information Processing Systems. Zhongxiang Dai, Bryan Kian Hsiang Low, and Patrick Jaillet33Zhongxiang Dai, Bryan Kian Hsiang Low, and Patrick Jail- let. Federated bayesian optimization via thompson sam- pling. Advances in Neural Information Processing Systems, 33, 2020.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nT Canh, Dinh, H Nguyen, Tuan Dung Tran, Nguyen, arXiv:2006.08848Personalized federated learning with moreau envelopes. arXiv preprintCanh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Per- sonalized federated learning with moreau envelopes. arXiv preprint arXiv:2006.08848, 2020.\n\nPersonalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Alireza Fallah, Aryan Mokhtari, Asuman Ozdaglar, Advances in Neural Information Processing Systems. 33Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Per- sonalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Advances in Neural Information Processing Systems, 33, 2020.\n\nBootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, H Pierre, Elena Richemond, Carl Buchatskaya, Bernardo Doersch, Zhaohan Daniel Avila Pires, Mohammad Gheshlaghi Guo, Azar, arXiv:2006.07733arXiv preprintJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do- ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham- mad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n\nLower bounds and optimal algorithms for personalized federated learning. Filip Hanzely, Slavom\u00edr Hanzely, Samuel Horv\u00e1th, Peter Richt\u00e1rik, arXiv:2010.02372arXiv preprintFilip Hanzely, Slavom\u00edr Hanzely, Samuel Horv\u00e1th, and Peter Richt\u00e1rik. Lower bounds and optimal algorithms for person- alized federated learning. arXiv preprint arXiv:2010.02372, 2020.\n\nChaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, arXiv:2007.13518Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research library and benchmark for federated machine learning. arXiv preprintChaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research library and benchmark for federated machine learning. arXiv preprint arXiv:2007.13518, 2020.\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 9729-9738, 2020.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nMeasuring the effects of non-identical data distribution for federated visual classification. Tzu-Ming Harry Hsu, Hang Qi, Matthew Brown, arXiv:1909.06335arXiv preprintTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Mea- suring the effects of non-identical data distribution for feder- ated visual classification. arXiv preprint arXiv:1909.06335, 2019.\n\nFederated visual classification with real-world data distribution. Tzu-Ming Harry Hsu, Hang Qi, Matthew Brown, arXiv:2003.08082arXiv preprintTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Fed- erated visual classification with real-world data distribution. arXiv preprint arXiv:2003.08082, 2020.\n\nThe oarf benchmark suite: Characterization and implications for federated learning systems. Sixu Hu, Yuan Li, Xu Liu, Qinbin Li, Zhaomin Wu, Bingsheng He, arXiv:2006.07856arXiv preprintSixu Hu, Yuan Li, Xu Liu, Qinbin Li, Zhaomin Wu, and Bingsheng He. The oarf benchmark suite: Characteriza- tion and implications for federated learning systems. arXiv preprint arXiv:2006.07856, 2020.\n\nPersonalized cross-silo federated learning on non-iid data. Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, Yong Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceYutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong Zhang. Personalized cross-silo federated learning on non-iid data. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.\n\nSelf-supervised visual feature learning with deep neural networks: A survey. Longlong Jing, Yingli Tian, IEEE Transactions on Pattern Analysis and Machine Intelligence. Longlong Jing and Yingli Tian. Self-supervised visual fea- ture learning with deep neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n\nAccelerating stochastic gradient descent using predictive variance reduction. Rie Johnson, Tong Zhang, Advances in neural information processing systems. Rie Johnson and Tong Zhang. Accelerating stochastic gradi- ent descent using predictive variance reduction. In Advances in neural information processing systems, pages 315-323, 2013.\n\nPeter Kairouz, Brendan Mcmahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, arXiv:1912.04977Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprintPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum- mings, et al. Advances and open problems in federated learn- ing. arXiv preprint arXiv:1912.04977, 2019.\n\nSecure, privacy-preserving and federated machine learning in medical imaging. A Georgios, Kaissis, R Marcus, Daniel Makowski, Rickmer F R\u00fcckert, Braren, Nature Machine Intelligence. Georgios A Kaissis, Marcus R Makowski, Daniel R\u00fcckert, and Rickmer F Braren. Secure, privacy-preserving and feder- ated machine learning in medical imaging. Nature Machine Intelligence, pages 1-7, 2020.\n\nScaffold: Stochastic controlled averaging for ondevice federated learning. Satyen Sai Praneeth Karimireddy, Mehryar Kale, Mohri, J Sashank, Reddi, U Sebastian, Ananda Theertha Stich, Suresh, Proceedings of the 37th International Conference on Machine Learning. PMLR. the 37th International Conference on Machine Learning. PMLRSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for on- device federated learning. In Proceedings of the 37th Inter- national Conference on Machine Learning. PMLR, 2020.\n\nRajesh Kumar, Sinmin Abdullah Aman Khan, Wenyong Zhang, Wang, arXiv:2007.06537Yousif Abuidris, Waqas Amin, and Jay Kumar. Blockchain-federated-learning and deep learning models for covid-19 detection using ct imaging. arXiv preprintRajesh Kumar, Abdullah Aman Khan, Sinmin Zhang, WenY- ong Wang, Yousif Abuidris, Waqas Amin, and Jay Ku- mar. Blockchain-federated-learning and deep learning mod- els for covid-19 detection using ct imaging. arXiv preprint arXiv:2007.06537, 2020.\n\nFederated learning on non-iid data silos: An experimental study. Qinbin Li, Yiqun Diao, Quan Chen, Bingsheng He, arXiv:2102.02079arXiv preprintQinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Fed- erated learning on non-iid data silos: An experimental study. arXiv preprint arXiv:2102.02079, 2021.\n\nPractical federated gradient boosting decision trees. Qinbin Li, Zeyi Wen, Bingsheng He, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceQinbin Li, Zeyi Wen, and Bingsheng He. Practical fed- erated gradient boosting decision trees. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 4642- 4649, 2020.\n\nA survey on federated learning systems: Vision, hype and reality for data privacy and protection. Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Bingsheng He, arXiv:1907.09693arXiv preprintQinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, and Bingsheng He. A survey on federated learning sys- tems: Vision, hype and reality for data privacy and protec- tion. arXiv preprint arXiv:1907.09693, 2019.\n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, Virginia Smith, arXiv:1908.07873Federated learning: Challenges, methods, and future directions. arXiv preprintTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. arXiv preprint arXiv:1908.07873, 2019.\n\nFederated optimization in heterogeneous networks. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith, Third Conference on Machine Learning and Systems (MLSys). 2020Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimiza- tion in heterogeneous networks. In Third Conference on Ma- chine Learning and Systems (MLSys), 2020.\n\nOn the convergence of fedavg on non-iid data. Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, Zhihua Zhang, International Conference on Learning Representations. Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In International Conference on Learning Representa- tions, 2020.\n\nFed{bn}: Federated learning on non-{iid} features via local batch normalization. Xiaoxiao Li, Jiang Meirui, Xiaofei Zhang, Michael Kamp, Qi Dou, International Conference on Learning Representations. Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fed{bn}: Federated learning on non-{iid} fea- tures via local batch normalization. In International Confer- ence on Learning Representations, 2021.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n\nFedvision: An online visual object detection platform powered by federated learning. Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng, Tianjian Chen, Han Yu, Qiang Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceYang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng, Tianjian Chen, Han Yu, and Qiang Yang. Fedvision: An online visual object detection platform powered by federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 13172- 13179, 2020.\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. Visualiz- ing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605, 2008.\n\nCommunication-efficient learning of deep networks from decentralized data. Eider H Brendan Mcmahan, Daniel Moore, Seth Ramage, Hampson, arXiv:1602.05629arXiv preprintH Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.\n\nSelf-supervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 6707-6717, 2020.\n\nRepresentation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems, pages 8026-8037, 2019.\n\nImproved deep metric learning with multiclass n-pair loss objective. Kihyuk Sohn, Advances in neural information processing systems. Kihyuk Sohn. Improved deep metric learning with multi- class n-pair loss objective. In Advances in neural information processing systems, pages 1857-1865, 2016.\n\nYonglong Tian, Dilip Krishnan, Phillip Isola, arXiv:1906.05849Contrastive multiview coding. arXiv preprintYonglong Tian, Dilip Krishnan, and Phillip Isola. Con- trastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n\nThe eu general data protection regulation (gdpr). A Practical Guide. Paul Voigt, Axel Von, Bussche, Springer International PublishingCham1st EdPaul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 2017.\n\nFederated learning with matched averaging. Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, Yasaman Khazaeni, International Conference on Learning Representations. Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Pa- pailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2020.\n\nTackling the objective inconsistency problem in heterogeneous federated optimization. Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, H Vincent Poor, Advances in Neural Information Processing Systems. 33Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency prob- lem in heterogeneous federated optimization. Advances in Neural Information Processing Systems, 33, 2020.\n\nAddressing class imbalance in federated learning. Lixu Wang, Shichao Xu, Xiao Wang, Qi Zhu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceLixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Address- ing class imbalance in federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.\n\nFederated machine learning: Concept and applications. Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong, ACM Transactions on Intelligent Systems and Technology (TIST). 102Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19, 2019.\n\nBayesian nonparametric federated learning of neural networks. Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, Yasaman Khazaeni, Proceedings of the 36th International Conference on Machine Learning. PMLR. the 36th International Conference on Machine Learning. PMLRMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khaza- eni. Bayesian nonparametric federated learning of neural networks. In Proceedings of the 36th International Confer- ence on Machine Learning. PMLR, 2019.\n\nFederated unsupervised representation learning. Fengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Yueting Zhuang, Xiaolin Li, arXiv:2010.08982arXiv preprintFengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Yueting Zhuang, and Xiaolin Li. Federated unsupervised representation learning. arXiv preprint arXiv:2010.08982, 2020.\n", "annotations": {"author": "[{\"end\":176,\"start\":40}]", "publisher": null, "author_last_name": "[{\"end\":49,\"start\":47}]", "author_first_name": "[{\"end\":46,\"start\":40}]", "author_affiliation": "[{\"end\":175,\"start\":74}]", "title": "[{\"end\":37,\"start\":1},{\"end\":213,\"start\":177}]", "venue": null, "abstract": "[{\"end\":1104,\"start\":215}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1242,\"start\":1239},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1256,\"start\":1252},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1444,\"start\":1440},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1583,\"start\":1579},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1586,\"start\":1583},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1589,\"start\":1586},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1592,\"start\":1589},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1750,\"start\":1746},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2088,\"start\":2084},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2091,\"start\":2088},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2094,\"start\":2091},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2097,\"start\":2094},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2100,\"start\":2097},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2102,\"start\":2100},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2105,\"start\":2102},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2107,\"start\":2105},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2110,\"start\":2107},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2191,\"start\":2187},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2194,\"start\":2191},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2217,\"start\":2213},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2251,\"start\":2247},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2359,\"start\":2355},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2516,\"start\":2512},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2519,\"start\":2516},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2522,\"start\":2519},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2784,\"start\":2780},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2787,\"start\":2784},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2801,\"start\":2797},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2876,\"start\":2872},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2931,\"start\":2927},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3637,\"start\":3634},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3639,\"start\":3637},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3642,\"start\":3639},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3645,\"start\":3642},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4218,\"start\":4214},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4221,\"start\":4218},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4224,\"start\":4221},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4828,\"start\":4824},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5614,\"start\":5610},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5897,\"start\":5893},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6772,\"start\":6769},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6775,\"start\":6772},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6778,\"start\":6775},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6952,\"start\":6948},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7060,\"start\":7056},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7158,\"start\":7154},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7399,\"start\":7396},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7401,\"start\":7399},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7404,\"start\":7401},{\"end\":7407,\"start\":7404},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7410,\"start\":7407},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7644,\"start\":7640},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7646,\"start\":7644},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7648,\"start\":7646},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7650,\"start\":7648},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7653,\"start\":7650},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7656,\"start\":7653},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7817,\"start\":7814},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7819,\"start\":7817},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7825,\"start\":7822},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8228,\"start\":8225},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8574,\"start\":8570},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9205,\"start\":9201},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9215,\"start\":9211},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9229,\"start\":9225},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9650,\"start\":9646},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11007,\"start\":11003},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12801,\"start\":12798},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14169,\"start\":14165},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17169,\"start\":17165},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17187,\"start\":17183},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17210,\"start\":17206},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17865,\"start\":17861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17913,\"start\":17910},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18168,\"start\":18164},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18839,\"start\":18836},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18868,\"start\":18864},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18871,\"start\":18868},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19876,\"start\":19872},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24041,\"start\":24038},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25590,\"start\":25587},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27984,\"start\":27980}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30868,\"start\":30241},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31234,\"start\":30869},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31598,\"start\":31235},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31812,\"start\":31599},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31976,\"start\":31813},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32359,\"start\":31977},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32430,\"start\":32360},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32608,\"start\":32431},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32709,\"start\":32609},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32846,\"start\":32710},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34150,\"start\":32847},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34602,\"start\":34151},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34996,\"start\":34603},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35245,\"start\":34997},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35382,\"start\":35246},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":35522,\"start\":35383},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35903,\"start\":35523},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":36148,\"start\":35904},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36634,\"start\":36149},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":36882,\"start\":36635}]", "paragraph": "[{\"end\":1526,\"start\":1120},{\"end\":2252,\"start\":1528},{\"end\":3114,\"start\":2254},{\"end\":4052,\"start\":3116},{\"end\":4764,\"start\":4054},{\"end\":5282,\"start\":4817},{\"end\":5556,\"start\":5284},{\"end\":6889,\"start\":5558},{\"end\":7332,\"start\":6891},{\"end\":7590,\"start\":7334},{\"end\":8172,\"start\":7615},{\"end\":8843,\"start\":8174},{\"end\":9618,\"start\":8925},{\"end\":10004,\"start\":9620},{\"end\":10331,\"start\":10065},{\"end\":10453,\"start\":10378},{\"end\":10811,\"start\":10468},{\"end\":11707,\"start\":10813},{\"end\":12606,\"start\":11718},{\"end\":13233,\"start\":12631},{\"end\":13491,\"start\":13253},{\"end\":13798,\"start\":13493},{\"end\":13926,\"start\":13862},{\"end\":14206,\"start\":13957},{\"end\":14379,\"start\":14296},{\"end\":14555,\"start\":14443},{\"end\":15026,\"start\":14639},{\"end\":15965,\"start\":15496},{\"end\":16400,\"start\":15984},{\"end\":17052,\"start\":16442},{\"end\":18147,\"start\":17089},{\"end\":18840,\"start\":18149},{\"end\":19459,\"start\":18842},{\"end\":21383,\"start\":19483},{\"end\":22171,\"start\":21437},{\"end\":23303,\"start\":22187},{\"end\":23819,\"start\":23321},{\"end\":24117,\"start\":23837},{\"end\":24664,\"start\":24144},{\"end\":24981,\"start\":24666},{\"end\":25105,\"start\":24996},{\"end\":25508,\"start\":25141},{\"end\":25974,\"start\":25531},{\"end\":27071,\"start\":25995},{\"end\":27771,\"start\":27123},{\"end\":28203,\"start\":27801},{\"end\":28826,\"start\":28227},{\"end\":29624,\"start\":28858},{\"end\":30240,\"start\":29626}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8924,\"start\":8844},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10377,\"start\":10332},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13861,\"start\":13799},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13956,\"start\":13927},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14295,\"start\":14207},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14442,\"start\":14380},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14638,\"start\":14556},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15495,\"start\":15027},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15983,\"start\":15966},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24143,\"start\":24118}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21061,\"start\":21054},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22628,\"start\":22621},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23532,\"start\":23525},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24360,\"start\":24353},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25192,\"start\":25185},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25818,\"start\":25811},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26231,\"start\":26224},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28122,\"start\":28114},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28602,\"start\":28374},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28718,\"start\":28710},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30031,\"start\":30023}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1118,\"start\":1106},{\"attributes\":{\"n\":\"2.\"},\"end\":4794,\"start\":4767},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4815,\"start\":4797},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7613,\"start\":7593},{\"attributes\":{\"n\":\"3.\"},\"end\":10043,\"start\":10007},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10063,\"start\":10046},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10466,\"start\":10456},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11716,\"start\":11710},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":12629,\"start\":12609},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":13251,\"start\":13236},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16440,\"start\":16403},{\"attributes\":{\"n\":\"4.\"},\"end\":17066,\"start\":17055},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17087,\"start\":17069},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19481,\"start\":19462},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21410,\"start\":21386},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21435,\"start\":21413},{\"attributes\":{\"n\":\"4.5.\"},\"end\":22185,\"start\":22174},{\"attributes\":{\"n\":\"4.6.\"},\"end\":23319,\"start\":23306},{\"attributes\":{\"n\":\"4.7.\"},\"end\":23835,\"start\":23822},{\"attributes\":{\"n\":\"5.\"},\"end\":24994,\"start\":24984},{\"end\":25139,\"start\":25108},{\"end\":25529,\"start\":25511},{\"end\":25993,\"start\":25977},{\"end\":27121,\"start\":27074},{\"end\":27799,\"start\":27774},{\"end\":28225,\"start\":28206},{\"end\":28856,\"start\":28829},{\"end\":30252,\"start\":30242},{\"end\":30880,\"start\":30870},{\"end\":31246,\"start\":31236},{\"end\":31610,\"start\":31600},{\"end\":31824,\"start\":31814},{\"end\":31988,\"start\":31978},{\"end\":32371,\"start\":32361},{\"end\":32453,\"start\":32432},{\"end\":32621,\"start\":32610},{\"end\":32718,\"start\":32711},{\"end\":32857,\"start\":32848},{\"end\":34161,\"start\":34152},{\"end\":34613,\"start\":34604},{\"end\":35007,\"start\":34998},{\"end\":35256,\"start\":35247},{\"end\":35393,\"start\":35384},{\"end\":35533,\"start\":35524},{\"end\":35914,\"start\":35905},{\"end\":36159,\"start\":36150},{\"end\":36646,\"start\":36636}]", "table": "[{\"end\":32846,\"start\":32767},{\"end\":34150,\"start\":33537},{\"end\":34602,\"start\":34369},{\"end\":34996,\"start\":34696},{\"end\":35245,\"start\":35009},{\"end\":35382,\"start\":35364},{\"end\":35522,\"start\":35422},{\"end\":35903,\"start\":35582},{\"end\":36148,\"start\":36033},{\"end\":36634,\"start\":36308},{\"end\":36882,\"start\":36824}]", "figure_caption": "[{\"end\":30868,\"start\":30254},{\"end\":31234,\"start\":30882},{\"end\":31598,\"start\":31248},{\"end\":31812,\"start\":31612},{\"end\":31976,\"start\":31826},{\"end\":32359,\"start\":31990},{\"end\":32430,\"start\":32373},{\"end\":32608,\"start\":32457},{\"end\":32709,\"start\":32624},{\"end\":32767,\"start\":32720},{\"end\":33537,\"start\":32859},{\"end\":34369,\"start\":34163},{\"end\":34696,\"start\":34615},{\"end\":35364,\"start\":35258},{\"end\":35422,\"start\":35395},{\"end\":35582,\"start\":35535},{\"end\":36033,\"start\":35916},{\"end\":36308,\"start\":36161},{\"end\":36824,\"start\":36649}]", "figure_ref": "[{\"end\":4501,\"start\":4493},{\"end\":4926,\"start\":4918},{\"end\":5466,\"start\":5458},{\"end\":5512,\"start\":5504},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11093,\"start\":11084},{\"end\":11252,\"start\":11246},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13273,\"start\":13265},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16499,\"start\":16491},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19401,\"start\":19393},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20112,\"start\":20104},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21549,\"start\":21540},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22641,\"start\":22633},{\"end\":22738,\"start\":22722},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":23006,\"start\":22998},{\"end\":25391,\"start\":25383},{\"end\":25405,\"start\":25396},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27258,\"start\":27249}]", "bib_author_first_name": "[{\"end\":37480,\"start\":37477},{\"end\":37508,\"start\":37503},{\"end\":37522,\"start\":37515},{\"end\":37534,\"start\":37530},{\"end\":37553,\"start\":37544},{\"end\":37862,\"start\":37853},{\"end\":37874,\"start\":37871},{\"end\":37895,\"start\":37890},{\"end\":37907,\"start\":37903},{\"end\":37917,\"start\":37912},{\"end\":37929,\"start\":37922},{\"end\":38349,\"start\":38345},{\"end\":38361,\"start\":38356},{\"end\":38381,\"start\":38373},{\"end\":38399,\"start\":38391},{\"end\":38686,\"start\":38682},{\"end\":38698,\"start\":38693},{\"end\":38715,\"start\":38710},{\"end\":38733,\"start\":38725},{\"end\":38751,\"start\":38743},{\"end\":39381,\"start\":39378},{\"end\":39391,\"start\":39388},{\"end\":39405,\"start\":39398},{\"end\":39420,\"start\":39414},{\"end\":39428,\"start\":39425},{\"end\":39435,\"start\":39433},{\"end\":39735,\"start\":39734},{\"end\":39749,\"start\":39748},{\"end\":39762,\"start\":39758},{\"end\":39767,\"start\":39763},{\"end\":40123,\"start\":40116},{\"end\":40137,\"start\":40132},{\"end\":40154,\"start\":40148},{\"end\":40520,\"start\":40508},{\"end\":40535,\"start\":40528},{\"end\":40550,\"start\":40543},{\"end\":40567,\"start\":40559},{\"end\":40577,\"start\":40576},{\"end\":40591,\"start\":40586},{\"end\":40607,\"start\":40603},{\"end\":40629,\"start\":40621},{\"end\":40646,\"start\":40639},{\"end\":40653,\"start\":40647},{\"end\":40675,\"start\":40667},{\"end\":40686,\"start\":40676},{\"end\":41117,\"start\":41112},{\"end\":41135,\"start\":41127},{\"end\":41151,\"start\":41145},{\"end\":41166,\"start\":41161},{\"end\":41401,\"start\":41393},{\"end\":41412,\"start\":41406},{\"end\":41424,\"start\":41417},{\"end\":41431,\"start\":41429},{\"end\":41445,\"start\":41439},{\"end\":41460,\"start\":41452},{\"end\":41475,\"start\":41467},{\"end\":41495,\"start\":41487},{\"end\":41507,\"start\":41503},{\"end\":41515,\"start\":41513},{\"end\":41528,\"start\":41522},{\"end\":41538,\"start\":41535},{\"end\":41549,\"start\":41545},{\"end\":41561,\"start\":41555},{\"end\":42141,\"start\":42134},{\"end\":42151,\"start\":42146},{\"end\":42162,\"start\":42157},{\"end\":42174,\"start\":42167},{\"end\":42184,\"start\":42180},{\"end\":42643,\"start\":42636},{\"end\":42655,\"start\":42648},{\"end\":42671,\"start\":42663},{\"end\":42681,\"start\":42677},{\"end\":43143,\"start\":43129},{\"end\":43153,\"start\":43149},{\"end\":43165,\"start\":43158},{\"end\":43470,\"start\":43456},{\"end\":43480,\"start\":43476},{\"end\":43492,\"start\":43485},{\"end\":43783,\"start\":43779},{\"end\":43792,\"start\":43788},{\"end\":43799,\"start\":43797},{\"end\":43811,\"start\":43805},{\"end\":43823,\"start\":43816},{\"end\":43837,\"start\":43828},{\"end\":44138,\"start\":44133},{\"end\":44154,\"start\":44146},{\"end\":44165,\"start\":44160},{\"end\":44178,\"start\":44172},{\"end\":44195,\"start\":44185},{\"end\":44205,\"start\":44201},{\"end\":44215,\"start\":44211},{\"end\":44644,\"start\":44636},{\"end\":44657,\"start\":44651},{\"end\":44990,\"start\":44987},{\"end\":45004,\"start\":45000},{\"end\":45252,\"start\":45247},{\"end\":45269,\"start\":45262},{\"end\":45286,\"start\":45279},{\"end\":45302,\"start\":45294},{\"end\":45316,\"start\":45311},{\"end\":45330,\"start\":45325},{\"end\":45336,\"start\":45331},{\"end\":45351,\"start\":45346},{\"end\":45369,\"start\":45362},{\"end\":45385,\"start\":45379},{\"end\":45846,\"start\":45845},{\"end\":45867,\"start\":45866},{\"end\":45882,\"start\":45876},{\"end\":45902,\"start\":45893},{\"end\":46234,\"start\":46228},{\"end\":46268,\"start\":46261},{\"end\":46283,\"start\":46282},{\"end\":46301,\"start\":46300},{\"end\":46328,\"start\":46313},{\"end\":46768,\"start\":46762},{\"end\":46782,\"start\":46776},{\"end\":46810,\"start\":46803},{\"end\":47313,\"start\":47307},{\"end\":47323,\"start\":47318},{\"end\":47334,\"start\":47330},{\"end\":47350,\"start\":47341},{\"end\":47604,\"start\":47598},{\"end\":47613,\"start\":47609},{\"end\":47628,\"start\":47619},{\"end\":48032,\"start\":48026},{\"end\":48041,\"start\":48037},{\"end\":48054,\"start\":48047},{\"end\":48063,\"start\":48059},{\"end\":48073,\"start\":48068},{\"end\":48089,\"start\":48080},{\"end\":48342,\"start\":48338},{\"end\":48351,\"start\":48347},{\"end\":48369,\"start\":48364},{\"end\":48389,\"start\":48381},{\"end\":48712,\"start\":48708},{\"end\":48721,\"start\":48717},{\"end\":48740,\"start\":48734},{\"end\":48755,\"start\":48749},{\"end\":48770,\"start\":48765},{\"end\":48790,\"start\":48782},{\"end\":49127,\"start\":49122},{\"end\":49139,\"start\":49132},{\"end\":49153,\"start\":49147},{\"end\":49166,\"start\":49160},{\"end\":49179,\"start\":49173},{\"end\":49511,\"start\":49503},{\"end\":49521,\"start\":49516},{\"end\":49537,\"start\":49530},{\"end\":49552,\"start\":49545},{\"end\":49561,\"start\":49559},{\"end\":49889,\"start\":49881},{\"end\":49902,\"start\":49895},{\"end\":49915,\"start\":49910},{\"end\":49931,\"start\":49926},{\"end\":49944,\"start\":49938},{\"end\":49957,\"start\":49953},{\"end\":49972,\"start\":49967},{\"end\":49991,\"start\":49981},{\"end\":50380,\"start\":50376},{\"end\":50390,\"start\":50386},{\"end\":50401,\"start\":50398},{\"end\":50409,\"start\":50407},{\"end\":50423,\"start\":50417},{\"end\":50437,\"start\":50429},{\"end\":50449,\"start\":50444},{\"end\":50464,\"start\":50456},{\"end\":50474,\"start\":50471},{\"end\":50484,\"start\":50479},{\"end\":50934,\"start\":50927},{\"end\":50959,\"start\":50951},{\"end\":51226,\"start\":51221},{\"end\":51252,\"start\":51246},{\"end\":51264,\"start\":51260},{\"end\":51563,\"start\":51558},{\"end\":51578,\"start\":51571},{\"end\":52024,\"start\":52019},{\"end\":52044,\"start\":52039},{\"end\":52054,\"start\":52049},{\"end\":52319,\"start\":52315},{\"end\":52331,\"start\":52328},{\"end\":52348,\"start\":52339},{\"end\":52360,\"start\":52356},{\"end\":52373,\"start\":52368},{\"end\":52391,\"start\":52384},{\"end\":52406,\"start\":52400},{\"end\":52422,\"start\":52416},{\"end\":52435,\"start\":52428},{\"end\":52452,\"start\":52448},{\"end\":52888,\"start\":52882},{\"end\":53116,\"start\":53108},{\"end\":53128,\"start\":53123},{\"end\":53146,\"start\":53139},{\"end\":53409,\"start\":53405},{\"end\":53421,\"start\":53417},{\"end\":53691,\"start\":53685},{\"end\":53705,\"start\":53698},{\"end\":53723,\"start\":53717},{\"end\":53737,\"start\":53729},{\"end\":53761,\"start\":53754},{\"end\":54118,\"start\":54112},{\"end\":54132,\"start\":54125},{\"end\":54141,\"start\":54138},{\"end\":54154,\"start\":54149},{\"end\":54163,\"start\":54162},{\"end\":54505,\"start\":54501},{\"end\":54519,\"start\":54512},{\"end\":54528,\"start\":54524},{\"end\":54537,\"start\":54535},{\"end\":54882,\"start\":54877},{\"end\":54893,\"start\":54889},{\"end\":54907,\"start\":54899},{\"end\":54921,\"start\":54914},{\"end\":55254,\"start\":55247},{\"end\":55272,\"start\":55266},{\"end\":55288,\"start\":55282},{\"end\":55304,\"start\":55296},{\"end\":55322,\"start\":55317},{\"end\":55337,\"start\":55330},{\"end\":55794,\"start\":55788},{\"end\":55805,\"start\":55802},{\"end\":55821,\"start\":55813},{\"end\":55830,\"start\":55827},{\"end\":55840,\"start\":55837},{\"end\":55850,\"start\":55847},{\"end\":55862,\"start\":55858},{\"end\":55874,\"start\":55867},{\"end\":55890,\"start\":55883}]", "bib_author_last_name": "[{\"end\":37501,\"start\":37481},{\"end\":37513,\"start\":37509},{\"end\":37528,\"start\":37523},{\"end\":37542,\"start\":37535},{\"end\":37563,\"start\":37554},{\"end\":37574,\"start\":37565},{\"end\":37869,\"start\":37863},{\"end\":37888,\"start\":37875},{\"end\":37901,\"start\":37896},{\"end\":37910,\"start\":37908},{\"end\":37920,\"start\":37918},{\"end\":37937,\"start\":37930},{\"end\":37946,\"start\":37939},{\"end\":38354,\"start\":38350},{\"end\":38371,\"start\":38362},{\"end\":38389,\"start\":38382},{\"end\":38406,\"start\":38400},{\"end\":38691,\"start\":38687},{\"end\":38708,\"start\":38699},{\"end\":38723,\"start\":38716},{\"end\":38741,\"start\":38734},{\"end\":38758,\"start\":38752},{\"end\":39386,\"start\":39382},{\"end\":39396,\"start\":39392},{\"end\":39412,\"start\":39406},{\"end\":39423,\"start\":39421},{\"end\":39431,\"start\":39429},{\"end\":39443,\"start\":39436},{\"end\":39740,\"start\":39736},{\"end\":39746,\"start\":39742},{\"end\":39756,\"start\":39750},{\"end\":39772,\"start\":39768},{\"end\":39780,\"start\":39774},{\"end\":40130,\"start\":40124},{\"end\":40146,\"start\":40138},{\"end\":40163,\"start\":40155},{\"end\":40526,\"start\":40521},{\"end\":40541,\"start\":40536},{\"end\":40557,\"start\":40551},{\"end\":40574,\"start\":40568},{\"end\":40584,\"start\":40578},{\"end\":40601,\"start\":40592},{\"end\":40619,\"start\":40608},{\"end\":40637,\"start\":40630},{\"end\":40665,\"start\":40654},{\"end\":40690,\"start\":40687},{\"end\":40696,\"start\":40692},{\"end\":41125,\"start\":41118},{\"end\":41143,\"start\":41136},{\"end\":41159,\"start\":41152},{\"end\":41176,\"start\":41167},{\"end\":41404,\"start\":41402},{\"end\":41415,\"start\":41413},{\"end\":41427,\"start\":41425},{\"end\":41437,\"start\":41432},{\"end\":41450,\"start\":41446},{\"end\":41465,\"start\":41461},{\"end\":41485,\"start\":41476},{\"end\":41501,\"start\":41496},{\"end\":41511,\"start\":41508},{\"end\":41520,\"start\":41516},{\"end\":41533,\"start\":41529},{\"end\":41543,\"start\":41539},{\"end\":41553,\"start\":41550},{\"end\":41568,\"start\":41562},{\"end\":42144,\"start\":42142},{\"end\":42155,\"start\":42152},{\"end\":42165,\"start\":42163},{\"end\":42178,\"start\":42175},{\"end\":42193,\"start\":42185},{\"end\":42646,\"start\":42644},{\"end\":42661,\"start\":42656},{\"end\":42675,\"start\":42672},{\"end\":42685,\"start\":42682},{\"end\":43147,\"start\":43144},{\"end\":43156,\"start\":43154},{\"end\":43171,\"start\":43166},{\"end\":43474,\"start\":43471},{\"end\":43483,\"start\":43481},{\"end\":43498,\"start\":43493},{\"end\":43786,\"start\":43784},{\"end\":43795,\"start\":43793},{\"end\":43803,\"start\":43800},{\"end\":43814,\"start\":43812},{\"end\":43826,\"start\":43824},{\"end\":43840,\"start\":43838},{\"end\":44144,\"start\":44139},{\"end\":44158,\"start\":44155},{\"end\":44170,\"start\":44166},{\"end\":44183,\"start\":44179},{\"end\":44199,\"start\":44196},{\"end\":44209,\"start\":44206},{\"end\":44221,\"start\":44216},{\"end\":44649,\"start\":44645},{\"end\":44662,\"start\":44658},{\"end\":44998,\"start\":44991},{\"end\":45010,\"start\":45005},{\"end\":45260,\"start\":45253},{\"end\":45277,\"start\":45270},{\"end\":45292,\"start\":45287},{\"end\":45309,\"start\":45303},{\"end\":45323,\"start\":45317},{\"end\":45344,\"start\":45337},{\"end\":45360,\"start\":45352},{\"end\":45377,\"start\":45370},{\"end\":45393,\"start\":45386},{\"end\":45855,\"start\":45847},{\"end\":45864,\"start\":45857},{\"end\":45874,\"start\":45868},{\"end\":45891,\"start\":45883},{\"end\":45910,\"start\":45903},{\"end\":45918,\"start\":45912},{\"end\":46259,\"start\":46235},{\"end\":46273,\"start\":46269},{\"end\":46280,\"start\":46275},{\"end\":46291,\"start\":46284},{\"end\":46298,\"start\":46293},{\"end\":46311,\"start\":46302},{\"end\":46334,\"start\":46329},{\"end\":46342,\"start\":46336},{\"end\":46774,\"start\":46769},{\"end\":46801,\"start\":46783},{\"end\":46816,\"start\":46811},{\"end\":46822,\"start\":46818},{\"end\":47316,\"start\":47314},{\"end\":47328,\"start\":47324},{\"end\":47339,\"start\":47335},{\"end\":47353,\"start\":47351},{\"end\":47607,\"start\":47605},{\"end\":47617,\"start\":47614},{\"end\":47631,\"start\":47629},{\"end\":48035,\"start\":48033},{\"end\":48045,\"start\":48042},{\"end\":48057,\"start\":48055},{\"end\":48066,\"start\":48064},{\"end\":48078,\"start\":48074},{\"end\":48092,\"start\":48090},{\"end\":48345,\"start\":48343},{\"end\":48362,\"start\":48352},{\"end\":48379,\"start\":48370},{\"end\":48395,\"start\":48390},{\"end\":48715,\"start\":48713},{\"end\":48732,\"start\":48722},{\"end\":48747,\"start\":48741},{\"end\":48763,\"start\":48756},{\"end\":48780,\"start\":48771},{\"end\":48796,\"start\":48791},{\"end\":49130,\"start\":49128},{\"end\":49145,\"start\":49140},{\"end\":49158,\"start\":49154},{\"end\":49171,\"start\":49167},{\"end\":49185,\"start\":49180},{\"end\":49514,\"start\":49512},{\"end\":49528,\"start\":49522},{\"end\":49543,\"start\":49538},{\"end\":49557,\"start\":49553},{\"end\":49565,\"start\":49562},{\"end\":49893,\"start\":49890},{\"end\":49908,\"start\":49903},{\"end\":49924,\"start\":49916},{\"end\":49936,\"start\":49932},{\"end\":49951,\"start\":49945},{\"end\":49965,\"start\":49958},{\"end\":49979,\"start\":49973},{\"end\":49999,\"start\":49992},{\"end\":50384,\"start\":50381},{\"end\":50396,\"start\":50391},{\"end\":50405,\"start\":50402},{\"end\":50415,\"start\":50410},{\"end\":50427,\"start\":50424},{\"end\":50442,\"start\":50438},{\"end\":50454,\"start\":50450},{\"end\":50469,\"start\":50465},{\"end\":50477,\"start\":50475},{\"end\":50489,\"start\":50485},{\"end\":50949,\"start\":50935},{\"end\":50966,\"start\":50960},{\"end\":51244,\"start\":51227},{\"end\":51258,\"start\":51253},{\"end\":51271,\"start\":51265},{\"end\":51280,\"start\":51273},{\"end\":51569,\"start\":51564},{\"end\":51593,\"start\":51579},{\"end\":52037,\"start\":52025},{\"end\":52047,\"start\":52045},{\"end\":52062,\"start\":52055},{\"end\":52326,\"start\":52320},{\"end\":52337,\"start\":52332},{\"end\":52354,\"start\":52349},{\"end\":52366,\"start\":52361},{\"end\":52382,\"start\":52374},{\"end\":52398,\"start\":52392},{\"end\":52414,\"start\":52407},{\"end\":52426,\"start\":52423},{\"end\":52446,\"start\":52436},{\"end\":52459,\"start\":52453},{\"end\":52893,\"start\":52889},{\"end\":53121,\"start\":53117},{\"end\":53137,\"start\":53129},{\"end\":53152,\"start\":53147},{\"end\":53415,\"start\":53410},{\"end\":53425,\"start\":53422},{\"end\":53434,\"start\":53427},{\"end\":53696,\"start\":53692},{\"end\":53715,\"start\":53706},{\"end\":53727,\"start\":53724},{\"end\":53752,\"start\":53738},{\"end\":53770,\"start\":53762},{\"end\":54123,\"start\":54119},{\"end\":54136,\"start\":54133},{\"end\":54147,\"start\":54142},{\"end\":54160,\"start\":54155},{\"end\":54176,\"start\":54164},{\"end\":54510,\"start\":54506},{\"end\":54522,\"start\":54520},{\"end\":54533,\"start\":54529},{\"end\":54541,\"start\":54538},{\"end\":54887,\"start\":54883},{\"end\":54897,\"start\":54894},{\"end\":54912,\"start\":54908},{\"end\":54926,\"start\":54922},{\"end\":55264,\"start\":55255},{\"end\":55280,\"start\":55273},{\"end\":55294,\"start\":55289},{\"end\":55315,\"start\":55305},{\"end\":55328,\"start\":55323},{\"end\":55346,\"start\":55338},{\"end\":55800,\"start\":55795},{\"end\":55811,\"start\":55806},{\"end\":55825,\"start\":55822},{\"end\":55835,\"start\":55831},{\"end\":55845,\"start\":55841},{\"end\":55856,\"start\":55851},{\"end\":55865,\"start\":55863},{\"end\":55881,\"start\":55875},{\"end\":55893,\"start\":55891}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":235614315},\"end\":37851,\"start\":37425},{\"attributes\":{\"doi\":\"arXiv:1812.01097\",\"id\":\"b1\"},\"end\":38272,\"start\":37853},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b2\"},\"end\":38616,\"start\":38274},{\"attributes\":{\"doi\":\"arXiv:2006.10029\",\"id\":\"b3\"},\"end\":38976,\"start\":38618},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":224802826},\"end\":39323,\"start\":38978},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":57246310},\"end\":39732,\"start\":39325},{\"attributes\":{\"doi\":\"arXiv:2006.08848\",\"id\":\"b6\"},\"end\":40012,\"start\":39734},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":227276412},\"end\":40435,\"start\":40014},{\"attributes\":{\"doi\":\"arXiv:2006.07733\",\"id\":\"b8\"},\"end\":41037,\"start\":40437},{\"attributes\":{\"doi\":\"arXiv:2010.02372\",\"id\":\"b9\"},\"end\":41391,\"start\":41039},{\"attributes\":{\"doi\":\"arXiv:2007.13518\",\"id\":\"b10\"},\"end\":42065,\"start\":41393},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207930212},\"end\":42588,\"start\":42067},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":43033,\"start\":42590},{\"attributes\":{\"doi\":\"arXiv:1909.06335\",\"id\":\"b13\"},\"end\":43387,\"start\":43035},{\"attributes\":{\"doi\":\"arXiv:2003.08082\",\"id\":\"b14\"},\"end\":43685,\"start\":43389},{\"attributes\":{\"doi\":\"arXiv:2006.07856\",\"id\":\"b15\"},\"end\":44071,\"start\":43687},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":227311284},\"end\":44557,\"start\":44073},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":62841734},\"end\":44907,\"start\":44559},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6587578},\"end\":45245,\"start\":44909},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b19\"},\"end\":45765,\"start\":45247},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219917683},\"end\":46151,\"start\":45767},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":204575663},\"end\":46760,\"start\":46153},{\"attributes\":{\"doi\":\"arXiv:2007.06537\",\"id\":\"b22\"},\"end\":47240,\"start\":46762},{\"attributes\":{\"doi\":\"arXiv:2102.02079\",\"id\":\"b23\"},\"end\":47542,\"start\":47242},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207852265},\"end\":47926,\"start\":47544},{\"attributes\":{\"doi\":\"arXiv:1907.09693\",\"id\":\"b25\"},\"end\":48336,\"start\":47928},{\"attributes\":{\"doi\":\"arXiv:1908.07873\",\"id\":\"b26\"},\"end\":48656,\"start\":48338},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":59316566},\"end\":49074,\"start\":48658},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":195798643},\"end\":49420,\"start\":49076},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":231924480},\"end\":49836,\"start\":49422},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14113767},\"end\":50289,\"start\":49838},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":210714181},\"end\":50895,\"start\":50291},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":5855042},\"end\":51144,\"start\":50897},{\"attributes\":{\"doi\":\"arXiv:1602.05629\",\"id\":\"b33\"},\"end\":51493,\"start\":51146},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":208617491},\"end\":51957,\"start\":51495},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b35\"},\"end\":52243,\"start\":51959},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":202786778},\"end\":52811,\"start\":52245},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":911406},\"end\":53106,\"start\":52813},{\"attributes\":{\"doi\":\"arXiv:1906.05849\",\"id\":\"b38\"},\"end\":53334,\"start\":53108},{\"attributes\":{\"id\":\"b39\"},\"end\":53640,\"start\":53336},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":211132598},\"end\":54024,\"start\":53642},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":220525591},\"end\":54449,\"start\":54026},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":229278687},\"end\":54821,\"start\":54451},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":219878182},\"end\":55183,\"start\":54823},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":168170092},\"end\":55738,\"start\":55185},{\"attributes\":{\"doi\":\"arXiv:2010.08982\",\"id\":\"b45\"},\"end\":56122,\"start\":55740}]", "bib_title": "[{\"end\":37475,\"start\":37425},{\"end\":39031,\"start\":38978},{\"end\":39376,\"start\":39325},{\"end\":40114,\"start\":40014},{\"end\":42132,\"start\":42067},{\"end\":42634,\"start\":42590},{\"end\":44131,\"start\":44073},{\"end\":44634,\"start\":44559},{\"end\":44985,\"start\":44909},{\"end\":45843,\"start\":45767},{\"end\":46226,\"start\":46153},{\"end\":47596,\"start\":47544},{\"end\":48706,\"start\":48658},{\"end\":49120,\"start\":49076},{\"end\":49501,\"start\":49422},{\"end\":49879,\"start\":49838},{\"end\":50374,\"start\":50291},{\"end\":50925,\"start\":50897},{\"end\":51556,\"start\":51495},{\"end\":52313,\"start\":52245},{\"end\":52880,\"start\":52813},{\"end\":53683,\"start\":53642},{\"end\":54110,\"start\":54026},{\"end\":54499,\"start\":54451},{\"end\":54875,\"start\":54823},{\"end\":55245,\"start\":55185}]", "bib_author": "[{\"end\":37503,\"start\":37477},{\"end\":37515,\"start\":37503},{\"end\":37530,\"start\":37515},{\"end\":37544,\"start\":37530},{\"end\":37565,\"start\":37544},{\"end\":37576,\"start\":37565},{\"end\":37871,\"start\":37853},{\"end\":37890,\"start\":37871},{\"end\":37903,\"start\":37890},{\"end\":37912,\"start\":37903},{\"end\":37922,\"start\":37912},{\"end\":37939,\"start\":37922},{\"end\":37948,\"start\":37939},{\"end\":38356,\"start\":38345},{\"end\":38373,\"start\":38356},{\"end\":38391,\"start\":38373},{\"end\":38408,\"start\":38391},{\"end\":38693,\"start\":38682},{\"end\":38710,\"start\":38693},{\"end\":38725,\"start\":38710},{\"end\":38743,\"start\":38725},{\"end\":38760,\"start\":38743},{\"end\":39388,\"start\":39378},{\"end\":39398,\"start\":39388},{\"end\":39414,\"start\":39398},{\"end\":39425,\"start\":39414},{\"end\":39433,\"start\":39425},{\"end\":39445,\"start\":39433},{\"end\":39742,\"start\":39734},{\"end\":39748,\"start\":39742},{\"end\":39758,\"start\":39748},{\"end\":39774,\"start\":39758},{\"end\":39782,\"start\":39774},{\"end\":40132,\"start\":40116},{\"end\":40148,\"start\":40132},{\"end\":40165,\"start\":40148},{\"end\":40528,\"start\":40508},{\"end\":40543,\"start\":40528},{\"end\":40559,\"start\":40543},{\"end\":40576,\"start\":40559},{\"end\":40586,\"start\":40576},{\"end\":40603,\"start\":40586},{\"end\":40621,\"start\":40603},{\"end\":40639,\"start\":40621},{\"end\":40667,\"start\":40639},{\"end\":40692,\"start\":40667},{\"end\":40698,\"start\":40692},{\"end\":41127,\"start\":41112},{\"end\":41145,\"start\":41127},{\"end\":41161,\"start\":41145},{\"end\":41178,\"start\":41161},{\"end\":41406,\"start\":41393},{\"end\":41417,\"start\":41406},{\"end\":41429,\"start\":41417},{\"end\":41439,\"start\":41429},{\"end\":41452,\"start\":41439},{\"end\":41467,\"start\":41452},{\"end\":41487,\"start\":41467},{\"end\":41503,\"start\":41487},{\"end\":41513,\"start\":41503},{\"end\":41522,\"start\":41513},{\"end\":41535,\"start\":41522},{\"end\":41545,\"start\":41535},{\"end\":41555,\"start\":41545},{\"end\":41570,\"start\":41555},{\"end\":42146,\"start\":42134},{\"end\":42157,\"start\":42146},{\"end\":42167,\"start\":42157},{\"end\":42180,\"start\":42167},{\"end\":42195,\"start\":42180},{\"end\":42648,\"start\":42636},{\"end\":42663,\"start\":42648},{\"end\":42677,\"start\":42663},{\"end\":42687,\"start\":42677},{\"end\":43149,\"start\":43129},{\"end\":43158,\"start\":43149},{\"end\":43173,\"start\":43158},{\"end\":43476,\"start\":43456},{\"end\":43485,\"start\":43476},{\"end\":43500,\"start\":43485},{\"end\":43788,\"start\":43779},{\"end\":43797,\"start\":43788},{\"end\":43805,\"start\":43797},{\"end\":43816,\"start\":43805},{\"end\":43828,\"start\":43816},{\"end\":43842,\"start\":43828},{\"end\":44146,\"start\":44133},{\"end\":44160,\"start\":44146},{\"end\":44172,\"start\":44160},{\"end\":44185,\"start\":44172},{\"end\":44201,\"start\":44185},{\"end\":44211,\"start\":44201},{\"end\":44223,\"start\":44211},{\"end\":44651,\"start\":44636},{\"end\":44664,\"start\":44651},{\"end\":45000,\"start\":44987},{\"end\":45012,\"start\":45000},{\"end\":45262,\"start\":45247},{\"end\":45279,\"start\":45262},{\"end\":45294,\"start\":45279},{\"end\":45311,\"start\":45294},{\"end\":45325,\"start\":45311},{\"end\":45346,\"start\":45325},{\"end\":45362,\"start\":45346},{\"end\":45379,\"start\":45362},{\"end\":45395,\"start\":45379},{\"end\":45857,\"start\":45845},{\"end\":45866,\"start\":45857},{\"end\":45876,\"start\":45866},{\"end\":45893,\"start\":45876},{\"end\":45912,\"start\":45893},{\"end\":45920,\"start\":45912},{\"end\":46261,\"start\":46228},{\"end\":46275,\"start\":46261},{\"end\":46282,\"start\":46275},{\"end\":46293,\"start\":46282},{\"end\":46300,\"start\":46293},{\"end\":46313,\"start\":46300},{\"end\":46336,\"start\":46313},{\"end\":46344,\"start\":46336},{\"end\":46776,\"start\":46762},{\"end\":46803,\"start\":46776},{\"end\":46818,\"start\":46803},{\"end\":46824,\"start\":46818},{\"end\":47318,\"start\":47307},{\"end\":47330,\"start\":47318},{\"end\":47341,\"start\":47330},{\"end\":47355,\"start\":47341},{\"end\":47609,\"start\":47598},{\"end\":47619,\"start\":47609},{\"end\":47633,\"start\":47619},{\"end\":48037,\"start\":48026},{\"end\":48047,\"start\":48037},{\"end\":48059,\"start\":48047},{\"end\":48068,\"start\":48059},{\"end\":48080,\"start\":48068},{\"end\":48094,\"start\":48080},{\"end\":48347,\"start\":48338},{\"end\":48364,\"start\":48347},{\"end\":48381,\"start\":48364},{\"end\":48397,\"start\":48381},{\"end\":48717,\"start\":48708},{\"end\":48734,\"start\":48717},{\"end\":48749,\"start\":48734},{\"end\":48765,\"start\":48749},{\"end\":48782,\"start\":48765},{\"end\":48798,\"start\":48782},{\"end\":49132,\"start\":49122},{\"end\":49147,\"start\":49132},{\"end\":49160,\"start\":49147},{\"end\":49173,\"start\":49160},{\"end\":49187,\"start\":49173},{\"end\":49516,\"start\":49503},{\"end\":49530,\"start\":49516},{\"end\":49545,\"start\":49530},{\"end\":49559,\"start\":49545},{\"end\":49567,\"start\":49559},{\"end\":49895,\"start\":49881},{\"end\":49910,\"start\":49895},{\"end\":49926,\"start\":49910},{\"end\":49938,\"start\":49926},{\"end\":49953,\"start\":49938},{\"end\":49967,\"start\":49953},{\"end\":49981,\"start\":49967},{\"end\":50001,\"start\":49981},{\"end\":50386,\"start\":50376},{\"end\":50398,\"start\":50386},{\"end\":50407,\"start\":50398},{\"end\":50417,\"start\":50407},{\"end\":50429,\"start\":50417},{\"end\":50444,\"start\":50429},{\"end\":50456,\"start\":50444},{\"end\":50471,\"start\":50456},{\"end\":50479,\"start\":50471},{\"end\":50491,\"start\":50479},{\"end\":50951,\"start\":50927},{\"end\":50968,\"start\":50951},{\"end\":51246,\"start\":51221},{\"end\":51260,\"start\":51246},{\"end\":51273,\"start\":51260},{\"end\":51282,\"start\":51273},{\"end\":51571,\"start\":51558},{\"end\":51595,\"start\":51571},{\"end\":52039,\"start\":52019},{\"end\":52049,\"start\":52039},{\"end\":52064,\"start\":52049},{\"end\":52328,\"start\":52315},{\"end\":52339,\"start\":52328},{\"end\":52356,\"start\":52339},{\"end\":52368,\"start\":52356},{\"end\":52384,\"start\":52368},{\"end\":52400,\"start\":52384},{\"end\":52416,\"start\":52400},{\"end\":52428,\"start\":52416},{\"end\":52448,\"start\":52428},{\"end\":52461,\"start\":52448},{\"end\":52895,\"start\":52882},{\"end\":53123,\"start\":53108},{\"end\":53139,\"start\":53123},{\"end\":53154,\"start\":53139},{\"end\":53417,\"start\":53405},{\"end\":53427,\"start\":53417},{\"end\":53436,\"start\":53427},{\"end\":53698,\"start\":53685},{\"end\":53717,\"start\":53698},{\"end\":53729,\"start\":53717},{\"end\":53754,\"start\":53729},{\"end\":53772,\"start\":53754},{\"end\":54125,\"start\":54112},{\"end\":54138,\"start\":54125},{\"end\":54149,\"start\":54138},{\"end\":54162,\"start\":54149},{\"end\":54178,\"start\":54162},{\"end\":54512,\"start\":54501},{\"end\":54524,\"start\":54512},{\"end\":54535,\"start\":54524},{\"end\":54543,\"start\":54535},{\"end\":54889,\"start\":54877},{\"end\":54899,\"start\":54889},{\"end\":54914,\"start\":54899},{\"end\":54928,\"start\":54914},{\"end\":55266,\"start\":55247},{\"end\":55282,\"start\":55266},{\"end\":55296,\"start\":55282},{\"end\":55317,\"start\":55296},{\"end\":55330,\"start\":55317},{\"end\":55348,\"start\":55330},{\"end\":55802,\"start\":55788},{\"end\":55813,\"start\":55802},{\"end\":55827,\"start\":55813},{\"end\":55837,\"start\":55827},{\"end\":55847,\"start\":55837},{\"end\":55858,\"start\":55847},{\"end\":55867,\"start\":55858},{\"end\":55883,\"start\":55867},{\"end\":55895,\"start\":55883}]", "bib_venue": "[{\"end\":37628,\"start\":37576},{\"end\":38041,\"start\":37964},{\"end\":38343,\"start\":38274},{\"end\":38680,\"start\":38618},{\"end\":39082,\"start\":39033},{\"end\":39508,\"start\":39445},{\"end\":39851,\"start\":39798},{\"end\":40214,\"start\":40165},{\"end\":40506,\"start\":40437},{\"end\":41110,\"start\":41039},{\"end\":41709,\"start\":41586},{\"end\":42276,\"start\":42195},{\"end\":42764,\"start\":42687},{\"end\":43127,\"start\":43035},{\"end\":43454,\"start\":43389},{\"end\":43777,\"start\":43687},{\"end\":44284,\"start\":44223},{\"end\":44726,\"start\":44664},{\"end\":45061,\"start\":45012},{\"end\":45483,\"start\":45411},{\"end\":45947,\"start\":45920},{\"end\":46418,\"start\":46344},{\"end\":46978,\"start\":46840},{\"end\":47305,\"start\":47242},{\"end\":47694,\"start\":47633},{\"end\":48024,\"start\":47928},{\"end\":48475,\"start\":48413},{\"end\":48854,\"start\":48798},{\"end\":49239,\"start\":49187},{\"end\":49619,\"start\":49567},{\"end\":50039,\"start\":50001},{\"end\":50552,\"start\":50491},{\"end\":51004,\"start\":50968},{\"end\":51219,\"start\":51146},{\"end\":51676,\"start\":51595},{\"end\":52017,\"start\":51959},{\"end\":52510,\"start\":52461},{\"end\":52944,\"start\":52895},{\"end\":53198,\"start\":53170},{\"end\":53403,\"start\":53336},{\"end\":53824,\"start\":53772},{\"end\":54227,\"start\":54178},{\"end\":54604,\"start\":54543},{\"end\":54989,\"start\":54928},{\"end\":55422,\"start\":55348},{\"end\":55786,\"start\":55740},{\"end\":42344,\"start\":42278},{\"end\":42828,\"start\":42766},{\"end\":44332,\"start\":44286},{\"end\":46479,\"start\":46420},{\"end\":47742,\"start\":47696},{\"end\":50600,\"start\":50554},{\"end\":51744,\"start\":51678},{\"end\":54652,\"start\":54606},{\"end\":55483,\"start\":55424}]"}}}, "year": 2023, "month": 12, "day": 17}