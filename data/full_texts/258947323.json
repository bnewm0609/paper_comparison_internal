{"id": 258947323, "updated": "2023-11-03 10:13:22.792", "metadata": {"title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models", "authors": "[{\"first\":\"Ying\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Olivia\",\"last\":\"Watkins\",\"middle\":[]},{\"first\":\"Yuqing\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Moonkyung\",\"last\":\"Ryu\",\"middle\":[]},{\"first\":\"Craig\",\"last\":\"Boutilier\",\"middle\":[]},{\"first\":\"Pieter\",\"last\":\"Abbeel\",\"middle\":[]},{\"first\":\"Mohammad\",\"last\":\"Ghavamzadeh\",\"middle\":[]},{\"first\":\"Kangwook\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Kimin\",\"last\":\"Lee\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning with respect to both image-text alignment and image quality. Our code is available at https://github.com/google-research/google-research/tree/master/dpok.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.16381", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-16381", "doi": "10.48550/arxiv.2305.16381"}}, "content": {"source": {"pdf_hash": "a553bf27d801d09f667fe121c0ba9632257f364b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.16381v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2305.16381", "status": "CLOSED"}}, "grobid": {"id": "f049485b6845f27620e6453bc1a83089ea6671cf", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/a553bf27d801d09f667fe121c0ba9632257f364b.txt", "contents": "\nDPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models\n1 Nov 2023\n\nYing Fan \u02da \nGoogle Research\n\n\nUniversity of Wisconsin\n\n\nOlivia Watkins \nYuqing Du \nHao Liu \nMoonkyung Ryu \nGoogle Research\n\n\nCraig Boutilier \nGoogle Research\n\n\nPieter Abbeel \nMohammad Ghavamzadeh \nKangwook Lee \nUniversity of Wisconsin\n\n\nKimin Lee \n-Madison \nU C Berkeley \nAmazon \nKaist \nDPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models\n1 Nov 2023832615F752160ADAE89EECE5738AA3ECarXiv:2305.16381v3[cs.LG]\nLearning from human feedback has been shown to improve text-to-image models.These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function.Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging.In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models.We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedbacktrained reward.Our approach, coined DPOK, integrates policy optimization with KL regularization.We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning.In our experiments, we show that DPOK is generally superior to supervised fine-tuning with respect to both image-text alignment and image quality.Our code is available at https://github.com/googleresearch/google-research/tree/master/dpok.37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n\nIntroduction\n\nRecent advances in diffusion models [10,37,38], together with pre-trained text encoders (e.g., CLIP [27], T5 [28]) have led to impressive results in text-to-image generation.Large-scale text-toimage models, such as Imagen [32], Dalle-2 [29], and Stable Diffusion [30], generate high-quality, creative images given novel text prompts.However, despite these advances, current models have systematic weaknesses.For example, current models have a limited ability to compose multiple objects [6,7,25].They also frequently encounter difficulties when generating objects with specified colors and counts [12,17].\n\nLearning from human feedback (LHF) has proven to be an effective means to overcome these limitations [13,17,42,43]. Lee et al. [17] demonstrate that certain properties, such as generating objects with specific colors, counts, and backgrounds, can be improved by learning a reward function from human feedback, followed by fine-tuning the text-to-image model using supervised learning.They show that simple supervised fine-tuning based on reward-weighted loss can improve the reward scores, leading to better image-text alignment.However, supervised fine-tuning often induces a deterioration in image quality (e.g., over-saturated or non-photorealistic images).This is likely due to the model being fine-tuned on a fixed dataset that is generated by a pre-trained model (Figure 1(a)).\n\nIn this work, we explore using online reinforcement learning (RL) for fine-tuning text-to-image diffusion models (Figure 1(b)).We show that optimizing the expected reward of a diffusion model's image output is equivalent to performing policy gradient on a multi-step diffusion model under certain In supervised fine-tuning, the model is updated on a fixed dataset generated by the pre-trained model.In contrast, the model is updated using new samples from the previously trained model during online RL fine-tuning.regularity assumptions.We also incorporate Kullback-Leibler (KL) divergence with respect to the pre-trained model as regularization in an online manner, treating this as an implicit reward.\n\nIn our experiments, we fine-tune the Stable Diffusion model [30] using ImageReward [43], an opensource reward model trained on a large dataset comprised of human assessments of (text, image) pairs.We show that online RL fine-tuning achieves strong text-image alignment while maintaining high image fidelity by optimizing its objective in an online manner.Crucially, online training allows evaluation of the reward model and conditional KL divergence beyond the (supervised) training dataset.This offers distinct advantages over supervised fine-tuning, a point we demonstrate empirically.In our empirical comparisons, we also incorporate the KL regularizer in a supervised fine-tuning method for a fair comparison.\n\nOur contributions are as follows:\n\n\u2022 We frame the optimization of the expected reward (w.r.t. an LHF-reward) of the images generated by a diffusion model given text prompts as an online RL problem.Moreover, we present DPOK: Diffusion Policy Optimization with KL regularization, which utilizes KL regularization w.r.t. the pre-trained text-to-image model as an implicit reward to stabilize RL fine-tuning.\u2022 We study incorporating KL regularization into supervised fine-tuning of diffusion models, which can mitigate some failure modes (e.g., generating over-saturated images) in [17].This also allows a fairer comparison with our RL technique.\u2022 We discuss the key differences between supervised fine-tuning and online fine-tuning of text-toimage models (Section 4.3).\u2022 Empirically, we show that online fine-tuning is effective in optimizing rewards, which improves text-to-image alignment while maintaining high image fidelity.\n\n\nRelated Work\n\nText-to-image diffusion models.Diffusion models [10,37,39] are a class of generative models that use an iterative denoising process to transform Gaussian noise into samples that follow a learned data distribution.These models have proven to be highly effective in a range of domains, including image generation [4], audio generation [14], 3D synthesis [26], and robotics [3].When combined with large-scale language encoders [27,28], diffusion models have demonstrated impressive performance in text-to-image generation [29,30,32].However, there are still many known weaknesses of existing text-to-image models, such as compositionality and attribute binding [6,20] or text rendering [21].\n\nLearning from human feedback.Human assessments of (or preferences over) learned model outcomes have been used to guide learning on a variety of tasks, ranging from learning behaviors [16] to language modeling [1,23,19,41].Recent work has also applied such methods to improve the alignment of text-to-image models.Human preferences are typically gathered at scale by asking annotators to compare generations, and a reward model is trained (e.g., by fine-tuning a visionlanguage model such as CLIP [27] or BLIP [18]) to produce scalar rewards well-aligned with the human feedback [43,13].The reward model is used to improve text-to-image model quality by fine-tuning a pre-trained generative model [17,42].Unlike prior approaches, which typically focus on reward-filtered or reward-weighted supervised learning, we develop an online fine-tuning framework with an RL-based objective.\n\nRL fine-tuning of diffusion models.Fan & Lee [5] first introduced a method to improve pre-trained diffusion models by integrating policy gradient and GAN training [8].They used policy gradient with reward signals from the discriminator to update the diffusion model and demonstrated that the fine-tuned model can generate realistic samples with few diffusion steps with DDPM sampling [10] on relatively simple domains (e.g., CIFAR [15] and CelebA [22]).In this work, we explore RL finetuning especially for large-scale text-to-image models using human rewards.We also consider several design choices like adding KL regularization as an implicit reward, and compare RL fine-tuning to supervised fine-tuning.\n\nConcurrent and independent from our work, Black et al. [2] have also investigated RL fine-tuning to fine-tune text-to-image diffusion models.They similarly frame the fine-tuning problem as a multi-step decision-making problem, and demonstrate that RL fine-tuning can outperform supervised fine-tuning with reward-weighted loss [17] in optimizing the reward, which aligns with our own observations.Furthermore, our work analyzes KL regularization for both supervised fine-tuning and RL fine-tuning with theoretical justifications, and shows that adopting KL regularization is useful in addressing some failure modes (e.g., deterioration in image quality) of fine-tuned models.For a comprehensive discussion of prior work, we refer readers to Appendix D.\n\n\nProblem Setting\n\nIn this section, we describe our basic problem setting for text-to-image generation of diffusion models.\n\nDiffusion models.We consider the use of denoising diffusion probabilistic models (DDPMs) [10] for image generation and draw our notation and problem formulation from [10].Let q 0 be the data distribution, i.e., x 0 \" q 0 px 0 q, x 0 P R n .A DDPM approximates q 0 with a parameterized model of the form p \u03b8 px 0 q \" \u015f p \u03b8 px 0:T qdx 1:T , where p \u03b8 px 0:T q \" p T px T q \u015b T t\"1 p \u03b8 px t\u00b41 |x t q and the reverse process is a Markov chain with the following dynamics: ppx T q \" N p0, Iq, p \u03b8 px t\u00b41 |x t q \" N `\u00b5\u03b8 px t , tq, \u03a3 t \u02d8.\n\nA unique characteristic of DDPMs is the exploitation of an approximate posterior qpx 1:T |x 0 q, known as the forward or diffusion process, which itself is a Markov chain that adds Gaussian noise to the data according to a variance schedule \u03b2 1 , . . ., \u03b2 T :\nqpx 1:T |x 0 q \" T \u017a t\"1 qpx t |x t\u00b41 q, qpx t |x t\u00b41 q \" N p a 1 \u00b4\u03b2t x t\u00b41 , \u03b2 t Iq.(2)\nLet \u03b1 t \" 1 \u00b4\u03b2t , \u1fb1t \" \u015b t s\"1 \u03b1 s , and \u03b2t \" 1\u00b4\u1fb1t\u00b41 1\u00b4\u1fb1t \u03b2 t .Ho et al. [10] adopt the parameterization \u00b5 \u03b8 px t , tq \" 1 ?\u03b1t \u00b4xt \u00b4\u03b2t ?\n1\u00b4\u1fb1 t \u03f5 \u03b8 px t , tq \u00af.\nTraining a DDPM is performed by optimizing a variational bound on the negative log-likelihood E q r\u00b4log p \u03b8 px 0 qs, which is equivalent to optimizing:\nE q \" T \u00ff t\"1 KL `qpx t\u00b41 |x t , x 0 q}p \u03b8 px t\u00b41 |x t q \u02d8\u0237.(3)\nNote that the variance sequence p\u03b2 t q T t\"1 P p0, 1q T is chosen such that \u1fb1T \u00ab 0, and thus, qpx T |x 0 q \u00ab N p0, Iq.The covariance matrix \u03a3 t in (1) is often set to \u03c3 2 t I, where \u03c3 2 t is either \u03b2 t or \u03b2t , which is not trainable.Unlike the original DDPM, we use a latent diffusion model [30], so x t 's are latent.\n\nText-to-image diffusion models.Diffusion models are especially well-suited to conditional data generation, as required by text-to-image models: one can plug in a classifier as guidance function [4], or can directly train the diffusion model's conditional distribution with classifier-free guidance [9].\n\nGiven text prompt z \" ppzq, let qpx 0 |zq be the data distribution conditioned on z.This induces a joint distribution ppx 0 , zq.During training, the same noising process q is used regardless of input z, and both the unconditional \u03f5 \u03b8 px t , tq and conditional \u03f5 \u03b8 px t , t, zq denoising models are learned.For data sampling, let \u03b5\u03b8 \" w\u03f5 \u03b8 px t , t, zq `p1 \u00b4wq\u03f5 \u03b8 px t , tq, where w \u011b 1 is the guidance scale.At test time, given a text prompt z, the model generates conditional data according to p \u03b8 px 0 |zq.\n\n\nFine-tuning of Diffusion Models\n\nIn this section, we describe our approach for online RL fine-tuning of diffusion models.We first propose a Markov decision process (MDP) formulation for the denoising phase.We then use this MDP and present a policy gradient RL algorithm to update the original diffusion model.The RL algorithm optimizes an objective consisting of the reward and a KL term that ensures the updated model is not too far from the original one.We also present a modified supervised fine-tuning method with KL regularization and compares it with the RL approach.\n\n\nRL Fine-tuning with KL Regularization\n\nLet p \u03b8 px 0:T |zq be a text-to-image diffusion model where z is some text prompt distributed according to ppzq, and rpx 0 , zq be a reward model (typically trained using human assessment of images).\n\n\nMDP formulation:\n\nThe denoising process of DDPMs can be modeled as a T -horizon MDP: s t \" pz, x T \u00b4tq, a t \" x T \u00b4t\u00b41 , P 0 ps 0 q \" `ppzq, N p0, Iq \u02d8, P ps t`1 | s t , a t q \" p\u03b4 z , \u03b4 at q, Rps t , a t q \"\n\" rps t`1 q \" rpx 0 , zq if t \" T \u00b41, 0 otherwise. , \u03c0 \u03b8 pa t | s t q \" p \u03b8 px T \u00b4t\u00b41 | x T \u00b4t, zq,(4)\nin which s t and a t are the state and action at time-step t, P 0 and P are the initial state distribution and the dynamics, R is the reward function, and \u03c0 \u03b8 is the parameterized policy.As a result, optimizing policy \u03c0 \u03b8 in (4) is equivalent to fine-tuning the underlying DDPM. 1 Finally, we denote by \u03b4 z the Dirac distribution at z.\n\nIt can be seen from the MDP formulation in (4) that the system starts by sampling its initial state s 0 from the Gaussian distribution N p0, Iq, similar to the first state of the dinoising process x T .Given the MDP state s t , which corresponds to state x T \u00b4t of the denoising process, the policy takes the action at time-step t as the next denoising state, i.e., a t \" x T \u00b4t\u00b41 .As a result of this action, the system transitions deterministically to a state identified by the action (i.e., the next state of the denoising process).The reward is zero, except at the final step in which the quality of the image at the end of the denoising process is evaluated w.r.t. the prompt, i.e., rpx 0 , zq.\n\nA common goal in re-training/fine-tuning the diffusion models is to maximize the expected reward of the generated images given the prompt distribution, i.e.,\nmin \u03b8 E ppzq E p \u03b8 px0|zq r\u00b4rpx 0 , zqs.(5)\nThe gradient of this objective function can be obtained as follows:\n\nLemma 4.1 (A modification of Theorem 4.1 in [5]).If p \u03b8 px 0:T |zqrpx 0 , zq and \u2207 \u03b8 p \u03b8 px 0:T |zqrpx 0 , zq are continuous functions of \u03b8, then we can write the gradient of the objective in (5) as\n\u2207 \u03b8 E ppzq E p \u03b8 px0|zq r\u00b4rpx 0 , zqs \" E ppzq E p \u03b8 px 0:T |zq \u00ab \u00b4rpx 0 , zq T \u00ff t\"1 \u2207 \u03b8 log p \u03b8 px t\u00b41 |x t , zq ff .(6)\nProof.We present the proof in Appendix A.1.\n\n\nAlgorithm 1 DPOK: Diffusion policy optimization with KL regularization\n\nInput: reward model r, pre-trained model p pre , current model p \u03b8 , batch size m, text distribution ppzq Initialize p \u03b8 \" p pre while \u03b8 not converged do Obtain m i.i.d.samples by first sampling z \" ppzq and then x 0:T \" p \u03b8 px 0:T |zq Compute the gradient using Eq. ( 9) and update \u03b8 end while Output: Fine-tuned diffusion model p \u03b8 Equation ( 6) is equivalent to the gradient used by the popular policy gradient algorithm, REINFORCE, to update a policy in the MDP (4).The gradient in ( 6) is estimated from trajectories p \u03b8 px 0:T |zq generated by the current policy, and then used to update the policy p \u03b8 px t\u00b41 |x t , zq in an online fashion.\n\nNote that REINFORCE is not the only way to solve (5).Alternatively, one could compute the gradient through the trajectories to update the model; but the multi-step nature of diffusion models makes this approach memory inefficient and potentially prone to numerical instability.Consequently, scaling it to high-resolution images becomes challenging.For this reason, we adopt policy gradient to train large-scale diffusion models like Stable Diffusion [30].\n\nAdding KL regularization.The risk of fine-tuning purely based on the reward model learned from human or AI feedback is that the model may overfit to the reward and discount the \"skill\" of the initial diffusion model to a greater degree than warranted.To avoid this phenomenon, similar to [23,41], we add the KL between the fine-tuned and pre-trained models as a regularizer to the objective function.Unlike the language models in which the KL regularizer is computed over the entire sequence/trajectory (of tokens), in text-to-image models, it makes sense to compute it only for the final image, i.e., KL `p\u03b8 px 0 |zq}p pre px 0 |zq \u02d8.Unfortunately, p \u03b8 px 0 |zq is a marginal (see the integral in Section 3) and its closed-form is unknown.As a result, we propose to add an upper-bound of this KL-term to the objective function.Lemma 4.2.Suppose p pre px 0:T |zq and p \u03b8 px 0:T |zq are Markov chains conditioned on the text prompt z that both start at x T \" N p0, Iq.Then, we have\nE ppzq rKLpp \u03b8 px 0 |zqq}p pre px 0 |zqqs \u010f E ppzq \u00ab T \u00ff t\"1 E p \u03b8 pxt|zq \" KL `p\u03b8 px t\u00b41 |x t , zq}p pre px t\u00b41 |x t , zq \u02d8\u2030ff .(7)\nWe report the proof of Lemma 4.2 in Appendix A.2. Intuitively, this lemma tells us that the divergence between the two distributions over the output image x 0 is upper-bounded by the sum of the divergences between the distributions over latent x t at each diffusion step.\n\nUsing the KL upper-bound in (7), we propose the following objective for regularized training:\nE ppzq \" \u03b1E p \u03b8 px 0:T |zq r\u00b4rpx0, zqs `\u03b2 T \u00ff t\"1 E p \u03b8 px t |zq \" KL `p\u03b8 pxt\u00b41|xt, zq}pprepxt\u00b41|xt, zq \u02d8\u2030ff ,(8)\nwhere \u03b1, \u03b2 are the reward and KL weights, respectively.We use the following gradient to optimize the objective ( 8):\nE ppzq E p \u03b8 px 0:T |zq \u00ab \u00b4\u03b1rpx0, zq T \u00ff t\"1 \u2207 \u03b8 log p \u03b8 pxt\u00b41|xt, zq `\u03b2 T \u00ff t\"1 \u2207 \u03b8 KL `p\u03b8 pxt\u00b41|xt, zq}pprepxt\u00b41|xt, zq \u02d8ff .(9)\nNote that (9) has one term missing from the exact gradient of (8) (see Appendix A.3). Removing this term is for efficient training.The pseudo-code of our algorithm, which we refer to as DPOK, is summarized in Algorithm 1.To reuse historical trajectories and be more sample efficient, we can also use importance sampling and clipped gradient, similar to [36].We refer readers to Appendix A.6 for these details.\n\n\nSupervised Learning with KL Regularization\n\nWe now introduce KL regularization into supervised fine-tuning (SFT), which allows for a more meaningful comparison with our KL-regularized RL algorithm (DPOK).We begin with a supervised fine-tuning objective similar to that used in [17], i.e.,\n\nE ppzq E pprepx0|zq r\u00b4rpx 0 , zq log p \u03b8 px 0 |zqs.\n\nTo compare with RL fine-tuning, we augment the supervised objective with a similar KL regularization term.Under the supervised learning setting, we consider KLpp pre px 0 |zq}p \u03b8 px 0 |zqq, which is equivalent to minimizing E pprepx0|zq r\u00b4log p \u03b8 px 0 |zqs, given any prompt z.Let qpx t q be the forward process used for training.Since the distribution of x 0 is generally not tractable, we use approximate upper-bounds in the lemma below (see derivation in Appendix A.4).\n\nLemma 4.3.Let \u03b3 be the regularization weight and \u03bct px t , x 0 q :\" ?\u1fb1t\u00b41\u03b2t 1\u00b4\u1fb1t x 0 `?\u03b1tp1\u00b4\u1fb1t\u00b41q 1\u00b4\u1fb1t\n\nx t .Assume rpx 0 , zq `\u03b3 \u0105 0, for all x 0 and z.Then, we have\nE ppzq E pprepx0|zq r\u00b4prpx 0 , zq `\u03b3q log p \u03b8 px 0 |zqs \u010f E ppzq E pprepx0|zq rprpx 0 , zq `\u03b3q \u00ff t\u01051 E qpxt|x0,zq r 1 2\u03c3 2 t ||\u03bc t px t , x 0 q \u00b4\u00b5\u03b8 px t , t, zq|| 2 ss `C1 . (11)\nMoreover, we also have another weaker upper-bound in which C 1 and C 2 are two constants:\nE ppzq E pprepx0|zq r\u00b4prpx 0 , zq `\u03b3q log p \u03b8 px 0 |zqs \u010f E ppzq E pprepx0|zq r \u00ff t\u01051 E qpxt|x0,zq r rpx 0 , zq||\u03bc t px t , x 0 q \u00b4\u00b5\u03b8 px t , t, zq|| 2 2\u03c3 2 t `\u03b3p||\u00b5 pre px t , t, zq \u00b4\u00b5\u03b8 px t , t, zq|| 2 q 2\u03c3 2 t ss `C2 .(12)\nIn Lemma 4.3, we introduce KL regularization (Eq.( 11)) for supervised fine-tuning, which can be incorporated by adjusting the original reward with a shift factor \u03b3 in the reward-weighted loss, smoothing the weighting of each sample towards the uniform distribution. 2We refer to this regularization as KL-D since it is based only on data from the pre-trained model.The induced supervised training objective for KL-D is as follows:\nE ppzq E pprepx0|zq \u00ab prpx 0 , zq `\u03b3q \u00ff t\u01051 E qpxt|x0,zq \" ||x 0 \u00b4f\u03b8 px t , t, zq|| 2 2\u03c3 2 t \u0237 ff . (13)\nWe also consider another KL regularization presented in Eq. ( 12).This KL regularization can be implemented by introducing an additional term in the reward-weighted loss.This extra term penalizes the L 2 -distance between the denoising directions derived from the pre-trained and current models.We refer to it as KL-O because it also regularizes the output from the current model to be close to that from the pre-trained model.The induced supervised training objective for KL-O is as follows:\nE ppzq E pprepx0|zq \u00ab \u00ff t\u01051 E qpxt|x0,zq \" rpx 0 , zq}x 0 \u00b4f\u03b8 px t , t, zq} 2 `\u03b3}f pre px t , t, zq \u00b4f\u03b8 px t , t, zq} 2 2\u03c3 2 t \u0237 ff .\nWe summarize our supervised fine-tuning in Algorithm 2. Note that in comparison to online RL training, our supervised setting only requires a pre-trained diffusion model and no extra/new datasets.\n\n\nOnline RL vs. Supervised Fine-tuning\n\nWe outline key differences between online RL and supervised fine-tuning:\n\n\u2022 Online versus offline distribution.We first contrast their objectives.The online RL objective is to find a new image distribution that maximizes expected reward-this can have much different support than the pre-trained distribution.In supervised fine-tuning, the objective only encourages the model to \"imitate\" good examples from the supervised dataset, which always lies in the support of the pre-trained distribution.\u2022 Different evaluation of the reward model.Online RL fine-tuning evaluates the reward model using the updated distribution, while supervised fine-tuning evaluates the reward on the fixed pre-training data distribution.As a consequence, our online RL optimization should derive greater benefit from the generalization ability of the reward model.\n\nFor these reasons, we expect online RL fine-tuning and supervised fine-tuning to generate rather different behaviors.Specifically, online fine-tuning should be better at maximizing the combined reward (human reward and implicit KL reward) than the supervised approach (similar to the difference between online learning and weighted behavior cloning in reinforcement learning).\n\n\nExperimental Evaluation\n\nWe now describe a set of experiments designed to test the efficacy of different fine-tuning methods.\n\n\nExperimental Setup\n\nAs our baseline generative model, we use Stable Diffusion v1.5 [30], which has been pre-trained on large image-text datasets [33,34].For compute-efficient fine-tuning, we use Low-Rank Adaption (LoRA) [11], which freezes the parameters of the pre-trained model and introduces low-rank trainable weights.We apply LoRA to the UNet [31] module and only update the added weights.For the reward model, we use ImageReward [43] which is trained on a large dataset comprised of human assessments of images.Compared to other scoring functions such as CLIP [27] or BLIP [18], ImageReward has a better correlation with human judgments, making it the preferred choice for fine-tuning our baseline diffusion model (see Appendix C for further justification).Further experimental details (e.g., model architectures, final hyper-parameters) are provided in Appendix B. We also provide more samples for qualitative comparison in Appendix E.6.\n\n\nComparison of Supervised and RL Fine-tuning\n\nWe first evaluate the performance of the original and fine-tuned text-to-image models w.r.we use \"A green colored rabbit\", \"A cat and a dog\", \"Four wolves in the park\", and \"A dog on the moon\".These test the models' ability to handle prompts involving color, composition, counting and location, respectively.For supervised fine-tuning, we use 20K images generated by the original model, which is the same number of (online) images used by RL fine-tuning.\n\nFigure 3(a) compares ImageReward scores of images generated by the different models (with the same random seed).We see that both SFT and RL fine-tuning improve the ImageReward scores on the training text prompt.This implies the fine-tuned models can generate images that are better aligned with the input text prompts than the original model because ImageReward is trained on human feedback datasets to evaluate image-text alignment.Figure 2 indeed shows that fine-tuned models add objects to match the number (e.g., adding more wolves in \"Four wolves in the park\"), and replace incorrect objects with target objects (e.g., replacing an astronaut with a dog in \"A dog on the moon\") compared to images from the original model.They also avoid obvious mistakes like generating a rabbit with a green background given the prompt \"A green colored rabbit\".Of special note, we find that the fine-tuned models generate better images than the original model on several unseen text prompts consisting of unseen objects in terms of image-text alignment (see Figure 10 in Appendix E).\n\nAs we can observe in Figure 3(a), RL models enjoy higher ImageReward than SFT models when evaluated with the same training prompt in different categories respectively, due to the benefit of online training as we discuss in Section 4.3.We also evaluate the image quality of the three models using the aesthetic predictor [34], which is trained to predict the aesthetic aspects of generated images. 3 [34,33], tends to produce whiskey-related images from \"Four roses\" due to the existence of a whiskey brand bearing the same name as the prompt.In contrast, RL fine-tuned model with ImageReward generates images associated with the flower \"rose\".\n\nintended to blunt such effects.For example, the supervised model often generates over-saturated images, corroborating the observations of Lee et al. [17].By contrast, the RL model generates more natural images that are at least as well-aligned with text prompts.\n\nWe also conduct human evaluation as follows: We collect 40 images randomly generated from each prompt, resulting in a total of 160 images for each model.Given two (anonymized) sets of four images from the same random seeds, one from RL fine-tuned model and one from the SFT model, we ask human raters to assess which one is better w.r.t.image-text alignment and image quality.Each query is evaluated by 8 independent raters and we report the average win/lose rate in Figure 3(c).The RL model consistently outperforms the SFT model on both alignment and image quality.\n\n\nThe Effect of KL Regularization\n\nTo demonstrate the impact of KL regularization on both supervised fine-tuning (SFT) and online RL fine-tuning, we conduct an ablation study specifically fine-tuning the pre-trained model with and without KL regularization on \"A green colored rabbit\".\n\nFigure 4(a) shows that KL regularization in online RL is effective in attaining both high reward and aesthetic scores.We observe that the RL model without KL regularization can generate lower-quality images (e.g., over-saturated colors and unnatural shapes) as shown in Figure 4(b).In the case of SFT with KL-O (where we use the same configuration as Section 5.2), we find that the KL regularization can mitigate some failure modes of SFT without KL and improve aesthetic scores, but generally suffers from lower ImageReward.We expect that this difference in the impact of KL regularization is due to the different nature of online and offline training-KL regularization is only applied to fixed samples in the case of SFT while KL regularization is applied to new samples per each update in the case of online RL (see Section 4.3 for related discussions).\n\n\nReducing Bias in the Pre-trained Model\n\nTo see the benefits of optimizing for ImageReward, we explore the effect of RL fine-tuning in reducing bias in the pre-trained model.Because the original Stable Diffusion model is trained on large-scale datasets extracted from the web [33,34], it can encode some biases in the training data.As shown in Figure 5, we see that the original model tends to produce whiskey-related images given the prompt \"Four roses\" (which happens to be the brand name of a whiskey), which is not aligned with users' intention in general.To verify whether maximizing a reward model derived from human feedback can mitigate this issue, we fine-tune the original model on the \"Four roses\" prompt using RL fine-tuning.Figure 5 shows that the fine-tuned model generates images with roses (the flower) because the ImageReward score is low on the biased images (ImageReward score is increased to 1.12 from -0.52 after fine-tuning).This shows the clear benefits of learning from human feedback to better align existing text-to-image models with human intentions.\n\n\nFine-tuning on Multiple Prompts\n\nWe further verify the effectiveness of the proposed techniques for fine-tuning text-to-image models on multiple prompts simultaneously.\n\n\nDiscussions\n\nIn this work, we propose DPOK, an algorithm to fine-tune a text-to-image diffusion model using policy gradient with KL regularization.We show that online RL fine-tuning outperforms simple supervised fine-tuning in improving the model's performance.Also, we conduct an analysis of KL regularization for both methods and discuss the key differences between RL fine-tuning and supervised fine-tuning.We believe our work demonstrates the potential of reinforcement learning from human feedback in improving text-to-image diffusion models.\n\nLimitations, future directions.Several limitations of our work suggest interesting future directions: (a) As discussed in Section 5.5, fine-tuning on multiple prompts requires longer training time, hyperparameter tuning, and engineering efforts.More efficient training with a broader range of diverse and complex prompts would be an interesting future direction to explore.(b) Exploring advanced policy gradient methods can be useful for further performance improvement.Investigating the applicability and benefits of these methods could lead to improvements in the fine-tuning process.\n\nBroader Impacts Text-to-image models can offer societal benefits across fields such as art and entertainment, but they also carry the potential for misuse.Our research allows users to finetune these models towards arbitrary reward functions.This process can be socially beneficial or harmful depending on the reward function used.Users could train models to produce less biased or offensive imagery, but they could also train them to produce misinformation or deep fakes.Since many users may follow our approach of using open-sourced reward functions for this finetuning, it is critical that the biases and failure modes of publicly available reward models are thoroughly documented.\n\n\nAppendix:\n\nReinforcement Taking the expectation of ppzq on both sides, we have\nE ppzq rKLpp \u03b8 px 0 |zqq}p pre px 0 |zqqs \u010f E ppzq \u00ab T \u00ff t\"1 E p \u03b8 pxt|zq \" KL `p\u03b8 px t\u00b41 |x t , zq}p pre px t\u00b41 |x t , zq \u02d8\u2030ff .(15)\nA. 3 The gradient of objective Eq. ( 8)\nFromE p \u03b8 pxtq \" T \u00ff t 1 \u0105t \u2207 \u03b8 log p \u03b8 px t 1 \u00b41|x t 1 , zq \u00a8KL `p\u03b8 px t\u00b41 |x t , zq}p pre px t\u00b41 |x t , zq \u02d8\u2030,(16)\nwhich treats the sum of conditional KL-divergences along the future trajectory as a scalar reward at each step.However, computing these sums is more inefficient than just the first term in Eq. ( 16).Empirically, we find that regularizing only the first term in Eq. ( 16) already works well, so we adopt this approach for simplicity.\n\n\nA.4 Lemma 4.3\n\nSimilar to derivation in [10], and notice that q is not dependent on z (i.e., qp\u00a8\u00a8\u00a8|zq \" qp\u00a8\u00a8\u00a8q), we have\nE ppzq E pprepx0|zq r\u00b4prpx 0 , zq `\u03b3q log p \u03b8 px 0 |zqs \u010f E ppzq E pprepx0|zq E qpx 1:T |x0,zq \" \u00b4prpx 0 , zq `\u03b3q log p \u03b8 px 0:T |zq qpx 1:T |x 0 , zq \u0237 \" E ppzq E pprepx0|zq E qpx 1:T |x0,zq r\u00b4prpx 0 , zq `\u03b3q log ppx T |zq qpx T |x 0 , zq \u00b4\u00ff t\u01051 prpx 0 , zq `\u03b3q log p \u03b8 px t\u00b41 |x t , zq qpx t\u00b41 |x t , x 0 , zq \u00b4prpx 0 , zq `\u03b3q log p \u03b8 px 0 |x 1 , zqs \" E ppzq E pprepx0|zq rprpx 0 , zq `\u03b3qE qpx T |x0,zq rKLpqpx T |x 0 q}ppx T qqs `prpx 0 , zq `\u03b3q \u00ff t\u01051 E qpxt|x0,zq rKLpqpx t\u00b41 |x t , x 0 q}p \u03b8 px t\u00b41 |x t , zqqs \u00b4prpx 0 , zq `\u03b3qE qpx1|x0,zq rlog p \u03b8 px 0 |x 1 , zqss,(17)\nwhere the first inequality comes from ELBO.\n\nLet \u03bct px t , x 0 q :\" ?\u1fb1t\u00b41\u03b2t 1\u00b4\u1fb1t x 0 `?\u03b1tp1\u00b4\u1fb1t\u00b41q 1\u00b4\u1fb1t\n\nx t and \u03b2t \" 1\u00b4\u1fb1t\u00b41 1\u00b4\u1fb1t \u03b2 t , we have:\nE ppzq E pprepx0|zq rprpx 0 , zq `\u03b3q \u00ff t\u01051 E qpxt|x0,zq rKLpqpx t\u00b41 |x t , x 0 q}p \u03b8 px t\u00b41 |x t , zqqss \" E ppzq E pprepx0|zq rprpx 0 , zq `\u03b3q \u00ff t\u01051 E qpxt|x0,zq r 1 2\u03c3 2 t ||\u03bc t px t , x 0 q \u00b4\u00b5\u03b8 px t , t, zq|| 2 ss `C,(18)\nwhere C is a constant.\n\nNotice that p \u03b8 px 0 |z, x 1 q is modeled as a discrete decoder as in [10] and ppx T |zq is a fixed Gaussian; neither is trainable.\n\nAs a result, we have\nE ppzq E pprepx0|zq r\u00b4prpx 0 , zq `\u03b3q log p \u03b8 px 0 |zqs \u010f E ppzq E pprepx0|zq rprpx 0 , zq `\u03b3q \u00ff t\u01051 E qpxt|x0,zq r 1 2\u03c3 2 t ||\u03bc t px t , x 0 q \u00b4\u00b5\u03b8 px t , t, zq|| 2 ss `C1(19)\nFurthermore, from triangle inequality we have\n\u00ff t\u01051 E qpxt|x0,\nSo another weaker upper bound is given by\nE ppzq E pprepx0|zq r\u00b4prpx 0 , zq `\u03b3q log p \u03b8 px 0 |zqs \u010f E ppzq E pprepx0|zq r \u00ff t\u01051 E qpxt|x0,zq r rpx 0 , zq||\u03bc t px t , x 0 q \u00b4\u00b5\u03b8 px t , t, zq|| 2 2\u03c3 2 t `\u03b3p||\u00b5 pre px t , t, zq \u00b4\u00b5\u03b8 px t , t, zq|| 2 q 2\u03c3 2 t ss `C2 .(21)\nNotice that p \u03b8 px 0 |z, x 1 q is modeled as a discrete decoder as in [10] and ppx T |zq is a fixed Gaussian; neither is trainable.Then the proof is complete.\n\n\nA.5 Value function learning\n\nSimilar to [35], we can also apply the variance reduction trick by subtracting a baseline function V px t , z, \u03b8q:\nE ppzq E p \u03b8 px 0:T |zq \u00ab \u00b4rpx 0 , zq T \u00ff t\"1 \u2207 \u03b8 log p \u03b8 px t\u00b41 |x t , zq ff \" E ppzq E p \u03b8 px 0:T |zq \u00ab \u00b4T \u00ff t\"1 prpx 0 , zq \u00b4V px t , z, \u03b8qq \u2207 \u03b8 log p \u03b8 px t\u00b41 |x t , zq ff ,(22)\nwhere V px t , z, \u03b8q :\" E p \u03b8 px0:tq rrpx 0 , zq|x t s.\n\nSo we can learn a value function estimator by minimizing the objective function below for each t:\nE p \u03b8 px0:tq \" \u00b4rpx 0 , zq \u00b4V px t , \u03b8, zq \u00af2 |x t \u0237 . (24)\nSince subtracting V px t , \u03b8, zq will minimize the variance of the gradient estimation, it is expected that such a trick can improve policy gradient training.In our experiments, we find that it can also improve the final reward: For example, for the prompt \"A dog on the moon\", adding variance reduction can improve ImageReward from 0.86 to 1.51, and also slightly improve the aesthetic score from 5.57 to 5.60.The generated images with and without value learning are shown in Figure 6.We also find similar improvements in multi-prompt training (see learning curves with and without value learning in Figure 7).\n\n\nWith value learning\n\nWithout value learning\n\n\nA.6 Importance sampling and ratio clipping\n\nIn order to reuse old trajectory samples, we can apply the important sampling trick: (25) In order to constrain p \u03b8 to be close to p \u03b8old , we can use the clipped gradient for policy gradient update similar to PPO [36]:  Online RL training.For hyper-parameters of online RL training used in Section 5.2., we use \u03b1 \" 10, \u03b2 \" 0.01, learning rate = 10 \u00b45 and keep other default hyper-parameters in AdamW, sampling batch size m \" 10.For policy gradient training, to perform more gradient steps without the need of re-sampling, we perform 5 gradient steps per sampling step, with gradient norm clipped to be smaller than 0.1, and use importance sampling to handle off-policy samples, and use batch size n \" 32 in each gradient step.We train the models till generating 20000 online samples, which matches the number of samples in supervised finetuning and results in 10K gradient steps in total.For stable training, we freeze the weights in batch norm to be exactly the same as the pretrained diffusion model.For importance sampling, we apply the clip hyperparameter \u03f5 \" 10 \u00b44.\nE ppzq E p \u03b8 px 0:T |zq \u00ab \u00b4T \u00ff t\"1 prpx 0 ,E ppzq E p \u03b8 old px 0:T |zq \u00ab \u00b4T \u00ff t\"1 prpx 0 , zq \u00b4V px t ,\n\nB Experimental Details\n\nSupervised training.For hyper-parameters of supervised training, we use \u03b3 \" 2.0 as the default option in Section 5.2, which is chosen from \u03b3 P t0.1, 1.0, 2.0, 5.0u.We use learning rate \" 2 \u02c610 \u00b45 and keep other default hyper-parameters in AdamW, which was chosen from t5 \u02c610 \u00b46, 1 \u02c610 \u00b45, 2 \u02c610 \u00b45, 5 \u02c610 \u00b45u.We use batch size n \" 128 and M \" 20000 such that both algorithms use the same number of samples, and train the SFT model for 8K gradient steps.Notice that Lemma 4.3 requires r `\u03b3 to be non-negative, we apply a filter of reward such that if r `\u03b3 \u0103 0, we just set it to 0. In practice, we also observe that without this filtering, supervised fine-tuning could fail during training, which indicates that our assumption is indeed necessary.\n\n\nC Investigation on ImageReward\n\nWe investigate the quality of ImageReward [43] by evaluating its prediction of the assessments of human labelers.Similar to Lee et al. [17], we check whether ImageReward generates a higher score for images preferred by humans.Our evaluation encompasses four text prompt categories: color, count, location, and composition.To generate prompts, we combine words or phrases from each category with various objects.These prompts are then used to generate corresponding images using a pre-trained text-to-image model.We collect binary feedback from human labelers on the image-text dataset and construct comparison pairs based on this feedback.Specifically, we utilize 804, 691, 1154 and 13228 comparison pairs obtained from 38, 29, 41 and 295 prompts for color, location, count, and composition, respectively. 4Additionally, for evaluation on complex text prompts from human users, we also utilize the test set from ImageReward, which consists of 6399 comparison pairs from 466 prompts. 5Table 2 compares the accuracy of ImageReward and baselines like CLIP [27] and BLIP [18] scores, which justifies the choice of using ImageReward for fine-tuning text-to-image models.\n\n\nD A Comprehensive Discussion of Related Work\n\nWe summarize several similarities and differences between our work and a concurrent work [2] as follows:\n\n\u2022 Both Black et al. [2] and our work explored online RL fine-tuning for improving text-to-image diffusion models.However, in our work, we provide theoretical insights for optimizing the reward with policy gradient methods and provide conditions for the equivalence to hold.\n\n\u2022 Black et al. [2] demonstrated that RL fine-tuning can outperform supervised fine-tuning with reward-weighted loss [17] in optimizing the reward, which aligns with our own observations.\n\n\u2022 In our work, we do not only focus on reward optimization: inspired by failure cases (e.g., over-saturated or non-photorealistic images) in supervised fine-tuning [17], we aim at finding an RL solution with KL regularization to solve the problem.\n\n\u2022 Unique to our work, we systematically analyze KL regularization for both supervised and online fine-tuning with theoretical justifications.We show that KL regularization would be more effective in the online RL fine-tuning rather than the supervised fine-tuning.By adopting online KL regularization, our algorithm successfully achieves high rewards and maintains image quality without over-optimization issue.\n\n\nE More Experimental Results\n\n\nE.1 KL-D vs KL-O: Ablation Study on Supervised KL Regularization\n\nWe also present the effects of both KL-D and KL-O with coefficient \u03b3 P t0.1, 1.0, 2.0, 5.0u in Figure 8 and Figure 9.We observe the followings:\n\n\u2022 KL-D (eq.( 11)) can slightly increase the aesthetic score.However, even with a large value of \u03b3, it does not yield a significant improvement in visual quality.Moreover, it tends to result in lower ImageReward scores overall.\n\n\u2022 In Figure 9, it is evident that KL-O is more noticeably effective in addressing the failure modes in supervised fine-tuning (SFT) compared to KL-D, especially when \u03b3 is relatively large.However, as a tradeoff, KL-O significantly reduces the ImageReward scores in that case.\n\n\u2022 In general, achieving both high ImageReward and aesthetic scores simultaneously is challenging for supervised fine-tuning (SFT), unlike RL fine-tuning (blue bar in Figure 8).\n\nWe also present more qualitative examples from different choices of KL regularization in Appendix E.5.Images from unseen text prompts: \"A green colored cat\" (color), \"A cat and a cup\" (composition), \"Four birds in the park\" (count), and \"A lion on the moon\" (location).\n\n\nE.3 Multi-prompt Training\n\nwe report more details for multi-prompt training.We still use the same setting in Appendix B, but 1) increase the batch size per sampling step to n \" 45 for stable training; 2) use a smaller KL weight \u03b2 \" 0.001; 3) use extra value learning for variance reduction.For value learning, we use a larger learning rate 10 \u00b44 and batch size 256.Also, we train for a longer time for multi-prompt such that it utilizes 50000 online samples.See sample images in Figure 11.\n\n\nE.4 Long and Complex Prompts\n\nWe show that our method does not just work for short and simple prompts like \"A green colored rabbit\", but can also work with long and complex prompts.For example, in Fig. 12 we adopt a complex prompt and compare the images generated by the original model and the supervised finetuned model.We can observe that online training encourages the models to generate images with a different style of painting and more fine-grained details, which is generally hard to achieve by supervised fine-tuning only (with KL-O regularization, \u03b3 \" 2.0).\n\n\nE.5 More Qualitative Results from Ablation Study for KL Regularization in SFT\n\nHere we provide samples from more prompts in KL ablation for SFT: see Figure 13.\n\n\nE.6 Qualitative Comparison\n\nHere we provide more samples from the experiments in Section 5.2 and Section 5.3: see Figure 14, Figure 16, Figure 15, Figure 17, Figure 18 and Figure 19, with the same configuration as in Section 5.2 for both RL and SFT.\n\nFigure 1 :\n1\nFigure 1: Illustration of (a) reward-weighted supervised fine-tuning and (b) RL fine-tuning.Both start with the same pre-trained model (the blue rectangle).In supervised fine-tuning, the model is updated on a fixed dataset generated by the pre-trained model.In contrast, the model is updated using new samples from the previously trained model during online RL fine-tuning.\n\n\nFigure 2 :Figure 3 :\n23\nFigure 2: Comparison of images generated by the original Stable Diffusion model, supervised finetuned (SFT) model, and RL fine-tuned model.Images in the same column are generated with the same random seed.Images from seen text prompts: \"A green colored rabbit\" (color), \"A cat and a dog\" (composition), \"Four wolves in the park\" (count), and \"A dog on the moon\" (location).\n\n\nFigure 3 (\n3\nb) shows that supervised fine-tuning degrades image quality relative to the RL approach (and sometimes relative to the pre-trained model), even with KL regularization which is\n\n\nFigure 4 :\n4\nFigure 4: Ablation study of KL regularization in both SFT and RL training, trained on a single prompt \"A green colored rabbit\".(a) ImageReward and Aesthetic scores are averaged over 50 samples from each model.(b) Images generated by RL models and SFT models optimized with and without KL regularization.Images in the same column are generated with the same random seed.\n\n\nFigure 5 :\n5\nFigure 5: Comparison of images generated by the original model and RL fine-tuned model on text prompt \"Four roses\".The original model, which is trained on large-scale datasets from the web[34,33], tends to produce whiskey-related images from \"Four roses\" due to the existence of a whiskey brand bearing the same name as the prompt.In contrast, RL fine-tuned model with ImageReward generates images associated with the flower \"rose\".\n\n\nFigure 6 :\n6\nFigure 6: Text prompt: \"A dog on the moon\", generated from the same random seeds from the model trained with and without value learning, respectively.\n\n\nFigure 7 :\n7\nFigure 7: Learning curves with and without value learning, trained on the Drawbench prompt set: Adding value learning could result in higher reward using less time.\n\n\nFigure 8 :\n8\nFigure 8: (a) ImageReward scores and (b) Aesthetic scores of supervised fine-tuned (SFT) models with different choices of regularization term and coefficient (\u03b3) on text prompt \"A green colored rabbit\".Each score is averaged over 50 samples from each model.KL-O and KL-D refer to eq. (12) and eq.(11), respectively.\n\n\nFigure 9 :\n9\nFigure 9: (Left) Sample from the RL model with KL regularization and sample from the SFT model without KL regularization.(Right)Samples from supervised fine-tuned models with different choices of regularization term and coefficient (\u03b3) on \"A green colored rabbit\".As we increase \u03b3 in the SFT case, there is a tradeoff between alignment with the prompt and image quality (i.e.oversaturation).On the other hand, the RL with KL sample is able to retain both alignment and image quality.E.2 Images from unseen text promptsFigure 10 shows image samples from the original model, SFT model and RL fine-tuned models on unseen text prompts.\n\n\nFigure 10 :\n10\nFigure 10: Comparison of images generated by the original Stable Diffusion model, supervised fine-tuned (SFT) model, and RL fine-tuned model.Images in the same column are generated with the same random seed.Images from unseen text prompts: \"A green colored cat\" (color), \"A cat and a cup\" (composition), \"Four birds in the park\" (count), and \"A lion on the moon\" (location).\n\n\nFigure 11 :\n11\nFigure 11: Sample images generated from prompts: (a) \"A chair in the corner on a boat\"; (b) \"A dog is laying with a remote controller\"; (c) \"A cube made of brick\"; (d) \"A red book and a yellow vase\", from the original model and RL model respectively.Images in the same column are generated with the same random seed.\n\n\nFigure 12 :\n12\nFigure 12: Text prompt: \"oil portrait of archie andrews holding a picture of among us, intricate, elegant, highly detailed, lighting, painting, artstation, smooth, illustration, art by greg rutowski and alphonse mucha\".\n\n\n\n\n(a) \"Four wolves in the park\" (b) \"A cat and a dog\" (c) \"A dog on the moon\"\n\n\nFigure 13 :\n13\nFigure 13: Samples from supervised fine-tuned models with different KL regularization and KL coefficients.\n\n\nFigure 14 :\n14\nFigure 14: Randomly generated samples from (a) the original Stable Diffusion model, (b) supervised fine-tuned (SFT) model and (c) RL fine-tuned model.\n\n\nFigure 15 :\n15\nFigure 15: Randomly generated samples from (a) the original Stable Diffusion model, (b) supervised fine-tuned (SFT) model and (c) RL fine-tuned model.\n\n\nFigure 16 :\n16\nFigure 16: Randomly generated samples from (a) the original Stable Diffusion model, (b) supervised fine-tuned (SFT) model and (c) RL fine-tuned model.\n\n\nFigure 17 :\n17\nFigure 17: Randomly generated samples from (a) the original Stable Diffusion model, (b) supervised fine-tuned (SFT) model and (c) RL fine-tuned model.\n\n\n\n\n(a) RL model without KL regularization (b) RL model with KL regularization\n\n\nFigure 18 :\n18\nFigure 18: Randomly generated samples from RL fine-tuned models (a) without and (b) with KL regularization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm 2 Supervised fine-tuning with KL regularization Input: Reward model r, pre-trained diffusion model p pre , diffusion model p \u03b8 , regularization weight \u03b3, batch size n, the number of training samples M Initialize p \u03b8 \" p pre Collect M samples from pre-trained model: D \" tpx 0 , zq \" p pre px 0 |zqppzq|@i P t1, ..., M uu while \u03b8 not converged do Obtain n i.i.d.samples tx 0 , zu from training dataset D Sample x t given x 0 , t \" r1, T s For KL-D, compute the gradient \u2207 \u03b8 Fine-tuned diffusion model p \u03b8 \u2022 Different KL regularization.The methods also differ in their use of KL regularization.For online training, we evaluate conditional KL in each step using online samples, while for supervised training we evaluate conditional KL only using supervised samples.Moreover, online KL regularization can be seen as an extra reward function to encourage small divergence w.r.t. the pre-trained model for online optimization, while supervised KL induces an extra shift in the original reward for supervised training as shown in Lemma 4.3.\nFor KL-O, compute the gradient \u2207 \u03b8prpx0,zq`\u03b3q||x0\u00b4f \u03b8 pxt,t,zq|| 2 2\u03c3 2 t rpx0,zq}x0\u00b4f \u03b8 pxt,t,zq} 2 `\u03b3}fprepxt,t,zq\u00b4f \u03b8 pxt,t,zq} 2 , update \u03b8 t 2\u03c3 2, update \u03b8end whileOutput:\n\nTable 1 :\n1\nWe conduct online RL training with 104 MS-CoCo prompts and 183 Drawbench prompts, respectively (the prompts are randomly sampled during training).Detailed configurations are provided in Appendix E. Specifically, we also learn a value function for variance reduction in policy gradient which shows benefit in further improving the final reward (see Appendix A.5 for details.)We report both ImageReward and the aesthetic score of the original and the RL fine-tuned models.For evaluation, we generate 30 images from each prompt and report the average scores of all images.The evaluation result is reported in Table1with sample images in Figure11in Appendix E, showing that RL training can also significantly improve the ImageReward score while maintaining a high aesthetic score with much larger sets of training prompts.\nMS-CoCoDrawbenchOriginal model RL model Original model RL modelImageReward score0.220.550.130.58Aesthetic score5.395.435.315.35\nImageReward scores and Aesthetic scores from the original model, and RL fine-tuned model on multiple prompts from MS-CoCo (104 prompts) and Drawbench (183 prompts).We report the average ImageReward and Aesthetic scores across 3120 and 5490 images on MS-CoCo and Drawbench, respectively (30 images per each prompt).\n\n\n\n\nLearning for Fine-tuning Text-to-Image Diffusion Models log p \u03b8 px t\u00b41 |x t , zq p pre px t\u00b41 |x t , zq \u00b8dx 0:T\n\" E p \u03b8 px 0:T |zq\u00ab t\"1 T \u00fflogp \u03b8 px t\u00b41 |x t , zq p pre px t\u00b41 |x t , zqff\"T \u00ff t\"1E p \u03b8 px t:T |zq E p \u03b8 px0:t\u00b41|x t:T ,zq\"logp pre px t\u00b41 |x t , zq p \u03b8 px t\u00b41 |x t , zq\u0237\"T \u00ff t\"1E p \u03b8 pxt|zq E p \u03b8 px0:t\u00b41|xt,zq\"logp \u03b8 px t\u00b41 |x t , zq p pre px t\u00b41 |x t , zq\u0237\"T \u00ff t\"1E p \u03b8 pxt|zq E p \u03b8 pxt\u00b41|xt,zq\" logp pre px t\u00b41 |x t , zq p \u03b8 px t\u00b41 |x t , zq\u0237T\u00ff\"t\"1\nE p \u03b8 pxt|zq \" KL `p\u03b8 px t\u00b41 |x t , zq}p pre px t\u00b41 |x t , zq \u02d8\u2030.\n\n\n\n\nKL `p\u03b8 px t\u00b41 |x t , zq}p pre px t\u00b41 |x t , zq \u02d8\u2030.By the product rule, we have KL `p\u03b8 px t\u00b41 |x t , zq}p pre px t\u00b41 |x t , zq \u02d8\u2030 KL `p\u03b8 px t\u00b41 |x t , zq}p pre px t\u00b41 |x t , zq \u02d8\u2030\nLemma4.2,foronlinefine-tuning,weneedtoregularizet\"1 E p \u03b8 pxtq \u0159 T\"T\u2207 \u03b8\u00ffE p \u03b8 pxtq\"t\"1T\" \u2207 \u03b8 `T \u00ff \u00ff t\"1 E p \u03b8 pxtq \"t\"1\n\n\n\nzq r\u03b3 ||\u03bc t px t , x 0 q \u00b4\u00b5\u03b8 px t , t, zq|| 2 2\u03c3 2 px t , x 0 q \u00b4\u00b5pre px t , t, zq|| 2 `||\u00b5 pre px t , t, zq \u00b4\u00b5\u03b8 px t , t, zq|| 2 2\u03c3 2\nst\u010f\u00ffE qpxt|x0,zq r\u03b3||\u03bc tt\u01051\nt s\n\n\n\n\nzq \u00b4V px t , z, \u03b8qq \u2207 \u03b8 log p \u03b8 px t\u00b41 |x t , zq ff \" E ppzq E p \u03b8 old px 0:T |zq prpx 0 , zq \u00b4V px t , z, \u03b8qq p \u03b8 px t\u00b41 |x t , zq p \u03b8old px t\u00b41 |x t , zq \u2207 \u03b8 log p \u03b8 px t\u00b41 |x t , zq ff \" E ppzq E p \u03b8 old px 0:T |zq prpx 0 , zq \u00b4V px t , z, \u03b8qq \u2207 \u03b8 p \u03b8 px t\u00b41 |x t , zq p \u03b8old px t\u00b41 |x t , zq ff .\n\u00ab\u00b4T \u00fft\"1\u00ab\u00b4T \u00fft\"1\n\n\n\nz, \u03b8qq \u2207 \u03b8 clip \u02c6p\u03b8 px t\u00b41 |x t , zq p \u03b8old px t\u00b41 |x t , zq , 1 `\u03f5, 1 \u00b4\u03f5\u02d9ff ,\n(26)where \u03f5 is the clip hyperparameter.\n\nTable 2 :\n2\nAccuracy (%) of CLIP score, BLIP score and ImageReward when predicting the assessments of human labelers.\n\nWe keep the covariance in DDPM as constant, \u03a3t \" \u03a3, and only train \u00b5 \u03b8 (see Eq. 1). This would naturally provide a stochastic policy for online exploration.\nIntuitively, adjusting \u03b3 is similar to adjusting the temperature parameter in reward-weighted regression[24].\nThe aesthetic predictor had been utilized to filter out the low-quality images in training data for Stable Diffusion models.\nFull list of text prompts is available at [link].\nhttps://github.com/THUDM/ImageReward/blob/main/data/test.json\nAcknowledgementsWe thank Deepak Ramachandran and Jason Baldridge for providing helpful comments and suggestions.Support for this research was also provided by the University of Wisconsin-Madison Office of the Vice Chancellor for Research and Graduate Education, NSF Award DMS-2023239, NSF/Intel Partnership on Machine Learning for Wireless Networking Program under Grant No. CNS-2003129, and FuriosaAI.OW and YD are funded by the Center for Human Compatible Artificial Intelligence.\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Ndousse, Kamal, Askell, Amanda, Anna Chen, Dassarma, Nova, Drain, Dawn, Fort, Stanislav, Ganguli, Deep, Henighan, Tom, arXiv:2204.058622022arXiv preprint\n\nTraining diffusion models with reinforcement learning. Kevin Black, Janner, Michael, Du, Yilun, Kostrikov, Ilya, Sergey Levine, arXiv:2305.133012023318arXiv preprint\n\nChi, Cheng, Feng, Siyuan, Du, Yilun, Xu, Zhenjia, Cousineau, Eric, Benjamin Burchfiel, Shuran Song, arXiv:2303.04137Diffusion policy: Visuomotor policy learning via action diffusion. 2023arXiv preprint\n\nDiffusion models beat gans on image synthesis. Prafulla Dhariwal, Alexander Nichol, Advances in Neural Information Processing Systems. 202124\n\nOptimizing ddpm sampling with shortcut fine-tuning. Ying Fan, Kangwook Lee, arXiv:2301.13362202334arXiv preprint\n\nTraining-free structured diffusion guidance for compositional text-to-image synthesis. Weixi Feng, He, Xuehai, Fu, Tsu-Jui, Jampani, Varun, Akula, Arjun, Narayana, Pradyumna, Basu, Sugato, Xin Wang, Wang Eric, William Yang, arXiv:2212.0503220221arXiv preprint\n\nBenchmarking spatial relationships in text-to-image generation. Tejas Gokhale, Palangi, Hamid, Nushi, Besmira, Vineet, Vibhav, Horvitz, Eric, Kamar, Ece, Chitta, Yezhou Yang, arXiv:2212.100152022arXiv preprint\n\nIan Goodfellow, Pouget-Abadie, Jean, Mirza, Mehdi, Bing Xu, Warde - Farley, David, Ozair, Sherjil, Aaron Courville, Yoshua Bengio, Generative adversarial networks. Communications of the ACM. 202063\n\nJonathan Ho, Tim Salimans, arXiv:2207.12598Classifier-free diffusion guidance. 2022arXiv preprint\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 2020. 1, 2, 3, 1516\n\nEdward J Hu, Shen, Yelong, Wallis, Allen-Zhu Phillip, Zeyuan, Li, Yuanzhi, Wang, Shean, Lu Wang, Weizhu Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. 2021arXiv preprint\n\nAccurate and interpretable text-to-image faithfulness evaluation with question answering. Yushi Hu, Liu, Benlin, Kasai, Jungo, Wang, Yizhong, Ostendorf, Krishna Mari, Ranjay Smith, Noah A Tifa, arXiv:2303.118972023arXiv preprint\n\nPick-a-pic: An open dataset of user preferences for text-to-image generation. Yuval Kirstain, Polyak, Adam, Singer, Uriel, Matiana, Shahbuland, Joe Penna, Omer Levy, Advances in Neural Information Processing Systems. 202313\n\nDiffwave: A versatile diffusion model for audio synthesis. Zhifeng Kong, Ping, Wei, Huang, Jiaji, Kexin Zhao, Bryan Catanzaro, arXiv:2009.097612020arXiv preprint\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 2009\n\nPebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. Kimin Lee, Laura Smith, Pieter Abbeel, arXiv:2106.050912021arXiv preprint\n\nAligning text-to-image models using human feedback. Kimin Lee, Liu, Hao, Ryu, Moonkyung, Watkins, Olivia, Du, Yuqing, Boutilier, Craig, Abbeel, Pieter, Mohammad Ghavamzadeh, Shixiang Gu, Shane, arXiv:2302.121922023. 1, 2, 3, 6918arXiv preprint\n\nBlip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. Junnan Li, Li, Dongxu, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. 2022718\n\nChain of hindsight aligns language models with feedback. Liu, Hao, Carmelo Sferrazza, Pieter Abbeel, arXiv:2302.026762023arXiv preprint\n\nCompositional visual generation with composable diffusion models. Nan Liu, Li, Shuang, Du, Yilun, Antonio Torralba, Joshua B Tenenbaum, European Conference on Computer Vision. 2022\n\nRosanne Liu, Garrette, Dan, Saharia, Chitwan, Chan, William, Roberts, Adam, Narang, Sharan, Blok, Irina, Mical, Rj, Mohammad Norouzi, Noah Constant, arXiv:2212.10562Character-aware models improve visual text rendering. 2022arXiv preprint\n\nDeep learning face attributes in the wild. Ziwei Liu, Luo, Ping, Xiaogang Wang, Xiaoou Tang, International Conference on Computer Vision. 2015\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Wu, Jeffrey, Jiang, Xu, Almeida, Diogo, Wainwright, Carroll, Mishkin, Pamela, Zhang, Chong, Agarwal, Sandhini, Slama, Katarina, Alex Ray, Advances in Neural Information Processing Systems. 202235\n\nReinforcement learning by reward-weighted regression for operational space control. Jan Peters, Stefan Schaal, Proceedings of the 24th international conference on Machine learning. the 24th international conference on Machine learning2007\n\nHuman evaluation of text-to-image models on a multi-task benchmark. Petsiuk, Vitali, Alexander E Siemenn, Surbehera, Saisamrit, Chin, Zad, Tyser, Keith, Hunter, Gregory, Raghavan, Arvind, Hicke, Yann, Bryan A Plummer, Kerret, Ori, arXiv:2211.121122022arXiv preprint\n\nBen Poole, Ajay Jain, Jonathan T Barron, Ben Mildenhall, arXiv:2209.14988Dreamfusion: Text-to-3d using 2d diffusion. 2022arXiv preprint\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Kim, Wook, Hallacy, Chris, Ramesh, Aditya, Goh, Gabriel, Agarwal, Sandhini, Sastry, Girish, Askell, Amanda, Mishkin, Pamela, Clark, Jack, International Conference on Machine Learning. 2021. 1, 2, 3, 718\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Shazeer, Noam, Roberts, Adam, Lee, Katherine, Narang, Sharan, Matena, Michael, Zhou, Yanqi, Wei Li, Liu, J Peter, The Journal of Machine Learning Research. 2112020\n\nHierarchical text-conditional image generation with clip latents. Aditya Ramesh, Dhariwal, Prafulla, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.0612520221arXiv preprint\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Blattmann, Andreas, Lorenz, Dominik, Patrick Esser, Bj\u00f6rn Ommer, Conference on Computer Vision and Pattern Recognition. 2022. 1, 2, 3, 5, 7\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical Image Computing and Computer Assisted Intervention. 2015\n\nPhotorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, Chan, William, Saxena, Saurabh, Lala Li, Whang, Jay, Denton, Emily, Seyed Ghasemipour, Kamyar Seyed, Burcu Ayan, Karagol, Mahdavi, Sara, Rapha Lopes, Gontijo, Advances in Neural Information Processing Systems. 20221\n\nChristoph Schuhmann, Vencu, Richard, Romain Beaumont, Kaczmarczyk, Robert, Mullis, Clayton, Katta, Aarush, Coombes, Theo, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 2021710arXiv preprint\n\nLaion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Vencu, Richard, Gordon, Cade, Wightman, Ross, Cherti, Mehdi, Coombes, Theo, Katta, Aarush, Mullis, Clayton, Wortsman, Mitchell, arXiv:2210.084022022810arXiv preprint\n\nHighdimensional continuous control using generalized advantage estimation. John Schulman, Moritz, Philipp, Levine, Jordan Sergey, Michael Abbeel, Pieter , arXiv:1506.02438201516arXiv preprint\n\nJohn Schulman, Wolski, Filip, Dhariwal, Prafulla, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017517arXiv preprint\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Sohl-Dickstein, Jascha, Weiss, Eric, Maheswaranathan, Niru, Ganguli, Surya, International Conference on Machine Learning. 20151\n\n. Jiaming Song, Chenlin Meng, Stefano Ermon, arXiv:2010.025022020Denoising diffusion implicit models. arXiv preprint\n\nImproved techniques for training score-based generative models. Yang Song, Stefano Ermon, Advances in neural information processing systems. 2020\n\nMaximum likelihood training of score-based diffusion models. Yang Song, Durkan, Conor, Iain Murray, Stefano Ermon, Advances in Neural Information Processing Systems. 202114\n\nLearning to summarize with human feedback. Nisan Stiennon, Ouyang, Long, Wu, Jeffrey, Ziegler, Daniel, Lowe, Ryan, Chelsea Voss, Alec Radford, Dario Amodei, Christiano Paul, F , Advances in Neural Information Processing Systems. 202035\n\nBetter aligning text-to-image models with human preference. Xiaoshi Wu, Sun, Keqiang, Zhu, Feng, Rui Zhao, Hongsheng Li, arXiv:2303.14420202313arXiv preprint\n\nImagereward: Learning and evaluating human preferences for text-to-image generation. Jiazheng Xu, Xiao Liu, Wu, Yuchen, Tong, Yuxuan, Li, Qinkai, Ding, Ming, Jie Tang, Yuxiao Dong, Advances in Neural Information Processing Systems. 2023. 1, 2, 3, 718\n", "annotations": {"author": "[{\"end\":144,\"start\":89},{\"end\":160,\"start\":145},{\"end\":171,\"start\":161},{\"end\":180,\"start\":172},{\"end\":213,\"start\":181},{\"end\":248,\"start\":214},{\"end\":263,\"start\":249},{\"end\":285,\"start\":264},{\"end\":325,\"start\":286},{\"end\":336,\"start\":326},{\"end\":346,\"start\":337},{\"end\":360,\"start\":347},{\"end\":368,\"start\":361},{\"end\":375,\"start\":369}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":94},{\"end\":159,\"start\":152},{\"end\":170,\"start\":168},{\"end\":179,\"start\":176},{\"end\":194,\"start\":191},{\"end\":229,\"start\":220},{\"end\":262,\"start\":256},{\"end\":284,\"start\":273},{\"end\":298,\"start\":295},{\"end\":335,\"start\":332},{\"end\":345,\"start\":337},{\"end\":359,\"start\":351},{\"end\":367,\"start\":361},{\"end\":374,\"start\":369}]", "author_first_name": "[{\"end\":93,\"start\":89},{\"end\":151,\"start\":145},{\"end\":167,\"start\":161},{\"end\":175,\"start\":172},{\"end\":190,\"start\":181},{\"end\":219,\"start\":214},{\"end\":255,\"start\":249},{\"end\":272,\"start\":264},{\"end\":294,\"start\":286},{\"end\":331,\"start\":326},{\"end\":348,\"start\":347},{\"end\":350,\"start\":349}]", "author_affiliation": "[{\"end\":117,\"start\":101},{\"end\":143,\"start\":119},{\"end\":212,\"start\":196},{\"end\":247,\"start\":231},{\"end\":324,\"start\":300}]", "title": "[{\"end\":76,\"start\":1},{\"end\":451,\"start\":376}]", "venue": null, "abstract": "[{\"end\":1735,\"start\":520}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1791,\"start\":1787},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1794,\"start\":1791},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1797,\"start\":1794},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1855,\"start\":1851},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1864,\"start\":1860},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1977,\"start\":1973},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1991,\"start\":1987},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2018,\"start\":2014},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2241,\"start\":2238},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2243,\"start\":2241},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2246,\"start\":2243},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2352,\"start\":2348},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2355,\"start\":2352},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2463,\"start\":2459},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2466,\"start\":2463},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2469,\"start\":2466},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2472,\"start\":2469},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2489,\"start\":2485},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3912,\"start\":3908},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3935,\"start\":3931},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5145,\"start\":5141},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5558,\"start\":5554},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5561,\"start\":5558},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5564,\"start\":5561},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5820,\"start\":5817},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5843,\"start\":5839},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5862,\"start\":5858},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5880,\"start\":5877},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5934,\"start\":5930},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5937,\"start\":5934},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6029,\"start\":6025},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6032,\"start\":6029},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6035,\"start\":6032},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6167,\"start\":6164},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6170,\"start\":6167},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6193,\"start\":6189},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6383,\"start\":6379},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6408,\"start\":6405},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6411,\"start\":6408},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6414,\"start\":6411},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6417,\"start\":6414},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6696,\"start\":6692},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6709,\"start\":6705},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6778,\"start\":6774},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6781,\"start\":6778},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6896,\"start\":6892},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6899,\"start\":6896},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7126,\"start\":7123},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7244,\"start\":7241},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7466,\"start\":7462},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7513,\"start\":7509},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7529,\"start\":7525},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7844,\"start\":7841},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8117,\"start\":8113},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8757,\"start\":8753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8834,\"start\":8830},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9623,\"start\":9619},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10217,\"start\":10213},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10439,\"start\":10436},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10543,\"start\":10540},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13543,\"start\":13540},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14637,\"start\":14634},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15039,\"start\":15035},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15334,\"start\":15330},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15337,\"start\":15334},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16460,\"start\":16457},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17242,\"start\":17238},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17578,\"start\":17574},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21613,\"start\":21609},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21675,\"start\":21671},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21678,\"start\":21675},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21750,\"start\":21746},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21878,\"start\":21874},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21965,\"start\":21961},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22096,\"start\":22092},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22109,\"start\":22105},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24371,\"start\":24367},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24450,\"start\":24446},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24453,\"start\":24450},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24845,\"start\":24841},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26949,\"start\":26945},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26952,\"start\":26949},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29960,\"start\":29959},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30492,\"start\":30488},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31613,\"start\":31609},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32273,\"start\":32269},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32404,\"start\":32400},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33823,\"start\":33819},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35633,\"start\":35629},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35726,\"start\":35722},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36644,\"start\":36640},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":36658,\"start\":36654},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36893,\"start\":36890},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36930,\"start\":36927},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37200,\"start\":37197},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37302,\"start\":37298},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37538,\"start\":37534},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":42273,\"start\":42269},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":42276,\"start\":42273},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":50228,\"start\":50224}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41090,\"start\":40702},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41490,\"start\":41091},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41681,\"start\":41491},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42066,\"start\":41682},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42514,\"start\":42067},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42680,\"start\":42515},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42860,\"start\":42681},{\"attributes\":{\"id\":\"fig_7\"},\"end\":43191,\"start\":42861},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43838,\"start\":43192},{\"attributes\":{\"id\":\"fig_9\"},\"end\":44230,\"start\":43839},{\"attributes\":{\"id\":\"fig_10\"},\"end\":44564,\"start\":44231},{\"attributes\":{\"id\":\"fig_11\"},\"end\":44801,\"start\":44565},{\"attributes\":{\"id\":\"fig_12\"},\"end\":44881,\"start\":44802},{\"attributes\":{\"id\":\"fig_13\"},\"end\":45005,\"start\":44882},{\"attributes\":{\"id\":\"fig_15\"},\"end\":45173,\"start\":45006},{\"attributes\":{\"id\":\"fig_17\"},\"end\":45341,\"start\":45174},{\"attributes\":{\"id\":\"fig_19\"},\"end\":45509,\"start\":45342},{\"attributes\":{\"id\":\"fig_21\"},\"end\":45677,\"start\":45510},{\"attributes\":{\"id\":\"fig_22\"},\"end\":45756,\"start\":45678},{\"attributes\":{\"id\":\"fig_23\"},\"end\":45881,\"start\":45757},{\"end\":45886,\"start\":45882},{\"end\":45891,\"start\":45887},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47115,\"start\":45892},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48391,\"start\":47116},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48926,\"start\":48392},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49228,\"start\":48927},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49399,\"start\":49229},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49720,\"start\":49400},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":49842,\"start\":49721},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49962,\"start\":49843}]", "paragraph": "[{\"end\":2356,\"start\":1751},{\"end\":3141,\"start\":2358},{\"end\":3846,\"start\":3143},{\"end\":4561,\"start\":3848},{\"end\":4596,\"start\":4563},{\"end\":5489,\"start\":4598},{\"end\":6194,\"start\":5506},{\"end\":7076,\"start\":6196},{\"end\":7784,\"start\":7078},{\"end\":8538,\"start\":7786},{\"end\":8662,\"start\":8558},{\"end\":9195,\"start\":8664},{\"end\":9456,\"start\":9197},{\"end\":9682,\"start\":9546},{\"end\":9857,\"start\":9706},{\"end\":10240,\"start\":9922},{\"end\":10544,\"start\":10242},{\"end\":11055,\"start\":10546},{\"end\":11631,\"start\":11091},{\"end\":11872,\"start\":11673},{\"end\":12083,\"start\":11893},{\"end\":12522,\"start\":12187},{\"end\":13223,\"start\":12524},{\"end\":13382,\"start\":13225},{\"end\":13494,\"start\":13427},{\"end\":13694,\"start\":13496},{\"end\":13861,\"start\":13818},{\"end\":14583,\"start\":13936},{\"end\":15040,\"start\":14585},{\"end\":16022,\"start\":15042},{\"end\":16427,\"start\":16156},{\"end\":16522,\"start\":16429},{\"end\":16753,\"start\":16637},{\"end\":17294,\"start\":16885},{\"end\":17585,\"start\":17341},{\"end\":17638,\"start\":17587},{\"end\":18112,\"start\":17640},{\"end\":18216,\"start\":18114},{\"end\":18280,\"start\":18218},{\"end\":18549,\"start\":18460},{\"end\":19206,\"start\":18775},{\"end\":19804,\"start\":19312},{\"end\":20135,\"start\":19939},{\"end\":20248,\"start\":20176},{\"end\":21017,\"start\":20250},{\"end\":21395,\"start\":21019},{\"end\":21523,\"start\":21423},{\"end\":22470,\"start\":21546},{\"end\":22972,\"start\":22518},{\"end\":24045,\"start\":22974},{\"end\":24690,\"start\":24047},{\"end\":24954,\"start\":24692},{\"end\":25523,\"start\":24956},{\"end\":25809,\"start\":25559},{\"end\":26667,\"start\":25811},{\"end\":27746,\"start\":26710},{\"end\":27917,\"start\":27782},{\"end\":28467,\"start\":27933},{\"end\":29055,\"start\":28469},{\"end\":29740,\"start\":29057},{\"end\":29821,\"start\":29754},{\"end\":29995,\"start\":29956},{\"end\":30445,\"start\":30113},{\"end\":30568,\"start\":30463},{\"end\":31189,\"start\":31146},{\"end\":31248,\"start\":31191},{\"end\":31289,\"start\":31250},{\"end\":31537,\"start\":31515},{\"end\":31670,\"start\":31539},{\"end\":31692,\"start\":31672},{\"end\":31914,\"start\":31869},{\"end\":31973,\"start\":31932},{\"end\":32357,\"start\":32199},{\"end\":32503,\"start\":32389},{\"end\":32741,\"start\":32686},{\"end\":32840,\"start\":32743},{\"end\":33512,\"start\":32901},{\"end\":33558,\"start\":33536},{\"end\":34676,\"start\":33605},{\"end\":35552,\"start\":34806},{\"end\":36752,\"start\":35587},{\"end\":36905,\"start\":36801},{\"end\":37180,\"start\":36907},{\"end\":37368,\"start\":37182},{\"end\":37617,\"start\":37370},{\"end\":38030,\"start\":37619},{\"end\":38272,\"start\":38129},{\"end\":38500,\"start\":38274},{\"end\":38777,\"start\":38502},{\"end\":38955,\"start\":38779},{\"end\":39226,\"start\":38957},{\"end\":39718,\"start\":39256},{\"end\":40287,\"start\":39751},{\"end\":40449,\"start\":40369},{\"end\":40701,\"start\":40480},{\"end\":41089,\"start\":40716},{\"end\":41489,\"start\":41116},{\"end\":41680,\"start\":41505},{\"end\":42065,\"start\":41696},{\"end\":42513,\"start\":42081},{\"end\":42679,\"start\":42529},{\"end\":42859,\"start\":42695},{\"end\":43190,\"start\":42875},{\"end\":43837,\"start\":43206},{\"end\":44229,\"start\":43855},{\"end\":44563,\"start\":44247},{\"end\":44800,\"start\":44581},{\"end\":44880,\"start\":44805},{\"end\":45004,\"start\":44898},{\"end\":45172,\"start\":45022},{\"end\":45340,\"start\":45190},{\"end\":45508,\"start\":45358},{\"end\":45676,\"start\":45526},{\"end\":45755,\"start\":45681},{\"end\":45880,\"start\":45773},{\"end\":46938,\"start\":45895},{\"end\":47947,\"start\":47129},{\"end\":48390,\"start\":48076},{\"end\":48506,\"start\":48395},{\"end\":48925,\"start\":48860},{\"end\":49108,\"start\":48930},{\"end\":49366,\"start\":49232},{\"end\":49398,\"start\":49395},{\"end\":49703,\"start\":49403},{\"end\":49802,\"start\":49724},{\"end\":49961,\"start\":49856}]", "formula": "[{\"attributes\":{\"id\":\"formula_1\"},\"end\":9545,\"start\":9457},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9705,\"start\":9683},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9921,\"start\":9858},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12186,\"start\":12084},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13426,\"start\":13383},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13817,\"start\":13695},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16155,\"start\":16023},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16636,\"start\":16523},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16884,\"start\":16754},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18458,\"start\":18281},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18459,\"start\":18458},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18774,\"start\":18550},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19310,\"start\":19207},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19311,\"start\":19310},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19938,\"start\":19805},{\"attributes\":{\"id\":\"formula_17\"},\"end\":29955,\"start\":29822},{\"attributes\":{\"id\":\"formula_18\"},\"end\":30000,\"start\":29996},{\"attributes\":{\"id\":\"formula_19\"},\"end\":30112,\"start\":30000},{\"attributes\":{\"id\":\"formula_20\"},\"end\":31145,\"start\":30569},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31514,\"start\":31290},{\"attributes\":{\"id\":\"formula_22\"},\"end\":31868,\"start\":31693},{\"attributes\":{\"id\":\"formula_23\"},\"end\":31931,\"start\":31915},{\"attributes\":{\"id\":\"formula_25\"},\"end\":32198,\"start\":31974},{\"attributes\":{\"id\":\"formula_26\"},\"end\":32685,\"start\":32504},{\"attributes\":{\"id\":\"formula_28\"},\"end\":32899,\"start\":32841},{\"attributes\":{\"id\":\"formula_29\"},\"end\":32900,\"start\":32899},{\"attributes\":{\"id\":\"formula_30\"},\"end\":34720,\"start\":34677},{\"attributes\":{\"id\":\"formula_31\"},\"end\":34780,\"start\":34720}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":36578,\"start\":36577}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1749,\"start\":1737},{\"attributes\":{\"n\":\"2\"},\"end\":5504,\"start\":5492},{\"attributes\":{\"n\":\"3\"},\"end\":8556,\"start\":8541},{\"attributes\":{\"n\":\"4\"},\"end\":11089,\"start\":11058},{\"attributes\":{\"n\":\"4.1\"},\"end\":11671,\"start\":11634},{\"end\":11891,\"start\":11875},{\"end\":13934,\"start\":13864},{\"attributes\":{\"n\":\"4.2\"},\"end\":17339,\"start\":17297},{\"attributes\":{\"n\":\"4.3\"},\"end\":20174,\"start\":20138},{\"attributes\":{\"n\":\"5\"},\"end\":21421,\"start\":21398},{\"attributes\":{\"n\":\"5.1\"},\"end\":21544,\"start\":21526},{\"attributes\":{\"n\":\"5.2\"},\"end\":22516,\"start\":22473},{\"attributes\":{\"n\":\"5.3\"},\"end\":25557,\"start\":25526},{\"attributes\":{\"n\":\"5.4\"},\"end\":26708,\"start\":26670},{\"attributes\":{\"n\":\"5.5\"},\"end\":27780,\"start\":27749},{\"attributes\":{\"n\":\"6\"},\"end\":27931,\"start\":27920},{\"end\":29752,\"start\":29743},{\"end\":30461,\"start\":30448},{\"end\":32387,\"start\":32360},{\"end\":33534,\"start\":33515},{\"end\":33603,\"start\":33561},{\"end\":34804,\"start\":34782},{\"end\":35585,\"start\":35555},{\"end\":36799,\"start\":36755},{\"end\":38060,\"start\":38033},{\"end\":38127,\"start\":38063},{\"end\":39254,\"start\":39229},{\"end\":39749,\"start\":39721},{\"end\":40367,\"start\":40290},{\"end\":40478,\"start\":40452},{\"end\":40713,\"start\":40703},{\"end\":41112,\"start\":41092},{\"end\":41502,\"start\":41492},{\"end\":41693,\"start\":41683},{\"end\":42078,\"start\":42068},{\"end\":42526,\"start\":42516},{\"end\":42692,\"start\":42682},{\"end\":42872,\"start\":42862},{\"end\":43203,\"start\":43193},{\"end\":43851,\"start\":43840},{\"end\":44243,\"start\":44232},{\"end\":44577,\"start\":44566},{\"end\":44894,\"start\":44883},{\"end\":45018,\"start\":45007},{\"end\":45186,\"start\":45175},{\"end\":45354,\"start\":45343},{\"end\":45522,\"start\":45511},{\"end\":45769,\"start\":45758},{\"end\":47126,\"start\":47117},{\"end\":49853,\"start\":49844}]", "table": "[{\"end\":47115,\"start\":46939},{\"end\":48075,\"start\":47948},{\"end\":48859,\"start\":48507},{\"end\":49228,\"start\":49109},{\"end\":49394,\"start\":49367},{\"end\":49720,\"start\":49704},{\"end\":49842,\"start\":49803}]", "figure_caption": "[{\"end\":41090,\"start\":40715},{\"end\":41490,\"start\":41115},{\"end\":41681,\"start\":41504},{\"end\":42066,\"start\":41695},{\"end\":42514,\"start\":42080},{\"end\":42680,\"start\":42528},{\"end\":42860,\"start\":42694},{\"end\":43191,\"start\":42874},{\"end\":43838,\"start\":43205},{\"end\":44230,\"start\":43854},{\"end\":44564,\"start\":44246},{\"end\":44801,\"start\":44580},{\"end\":44881,\"start\":44804},{\"end\":45005,\"start\":44897},{\"end\":45173,\"start\":45021},{\"end\":45341,\"start\":45189},{\"end\":45509,\"start\":45357},{\"end\":45677,\"start\":45525},{\"end\":45756,\"start\":45680},{\"end\":45881,\"start\":45772},{\"end\":45886,\"start\":45884},{\"end\":45891,\"start\":45889},{\"end\":46939,\"start\":45894},{\"end\":47948,\"start\":47128},{\"end\":48507,\"start\":48394},{\"end\":49109,\"start\":48929},{\"end\":49367,\"start\":49231},{\"end\":49704,\"start\":49402},{\"end\":49803,\"start\":49723},{\"end\":49962,\"start\":49855}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3139,\"start\":3135},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3268,\"start\":3264},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22982,\"start\":22981},{\"end\":23415,\"start\":23414},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":24029,\"start\":24027},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24076,\"start\":24075},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25433,\"start\":25430},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25819,\"start\":25818},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26089,\"start\":26088},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27021,\"start\":27020},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27414,\"start\":27413},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33386,\"start\":33385},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33510,\"start\":33509},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38232,\"start\":38231},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":38245,\"start\":38244},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":38515,\"start\":38514},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38953,\"start\":38952},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":39717,\"start\":39715},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":39925,\"start\":39923},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":40448,\"start\":40446},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":40575,\"start\":40573},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":40586,\"start\":40584},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":40597,\"start\":40595},{\"attributes\":{\"ref_id\":\"fig_21\"},\"end\":40608,\"start\":40606},{\"attributes\":{\"ref_id\":\"fig_23\"},\"end\":40619,\"start\":40617},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40633,\"start\":40631}]", "bib_author_first_name": "[{\"end\":51047,\"start\":51041},{\"end\":51057,\"start\":51053},{\"end\":51101,\"start\":51097},{\"end\":51280,\"start\":51275},{\"end\":51339,\"start\":51333},{\"end\":51462,\"start\":51454},{\"end\":51480,\"start\":51474},{\"end\":51645,\"start\":51637},{\"end\":51665,\"start\":51656},{\"end\":51789,\"start\":51785},{\"end\":51803,\"start\":51795},{\"end\":51939,\"start\":51934},{\"end\":52039,\"start\":52036},{\"end\":52050,\"start\":52046},{\"end\":52064,\"start\":52057},{\"end\":52177,\"start\":52172},{\"end\":52276,\"start\":52270},{\"end\":52322,\"start\":52319},{\"end\":52374,\"start\":52370},{\"end\":52384,\"start\":52379},{\"end\":52386,\"start\":52385},{\"end\":52423,\"start\":52418},{\"end\":52441,\"start\":52435},{\"end\":52526,\"start\":52518},{\"end\":52534,\"start\":52531},{\"end\":52667,\"start\":52659},{\"end\":52676,\"start\":52672},{\"end\":52689,\"start\":52683},{\"end\":52776,\"start\":52770},{\"end\":52778,\"start\":52777},{\"end\":52814,\"start\":52805},{\"end\":52860,\"start\":52858},{\"end\":52873,\"start\":52867},{\"end\":53063,\"start\":53058},{\"end\":53128,\"start\":53121},{\"end\":53141,\"start\":53135},{\"end\":53153,\"start\":53149},{\"end\":53155,\"start\":53154},{\"end\":53281,\"start\":53276},{\"end\":53345,\"start\":53342},{\"end\":53357,\"start\":53353},{\"end\":53489,\"start\":53482},{\"end\":53526,\"start\":53521},{\"end\":53538,\"start\":53533},{\"end\":53645,\"start\":53641},{\"end\":53666,\"start\":53658},{\"end\":53805,\"start\":53800},{\"end\":53816,\"start\":53811},{\"end\":53830,\"start\":53824},{\"end\":53932,\"start\":53927},{\"end\":54035,\"start\":54027},{\"end\":54057,\"start\":54049},{\"end\":54231,\"start\":54225},{\"end\":54255,\"start\":54248},{\"end\":54269,\"start\":54263},{\"end\":54404,\"start\":54397},{\"end\":54422,\"start\":54416},{\"end\":54536,\"start\":54533},{\"end\":54572,\"start\":54565},{\"end\":54589,\"start\":54583},{\"end\":54591,\"start\":54590},{\"end\":54656,\"start\":54649},{\"end\":54773,\"start\":54765},{\"end\":54787,\"start\":54783},{\"end\":54936,\"start\":54931},{\"end\":54961,\"start\":54953},{\"end\":54974,\"start\":54968},{\"end\":55105,\"start\":55101},{\"end\":55246,\"start\":55242},{\"end\":55398,\"start\":55395},{\"end\":55413,\"start\":55407},{\"end\":55645,\"start\":55636},{\"end\":55647,\"start\":55646},{\"end\":55757,\"start\":55752},{\"end\":55759,\"start\":55758},{\"end\":55821,\"start\":55818},{\"end\":55833,\"start\":55829},{\"end\":55848,\"start\":55840},{\"end\":55850,\"start\":55849},{\"end\":55862,\"start\":55859},{\"end\":56030,\"start\":56026},{\"end\":56044,\"start\":56040},{\"end\":56337,\"start\":56332},{\"end\":56441,\"start\":56438},{\"end\":56452,\"start\":56451},{\"end\":56583,\"start\":56577},{\"end\":56616,\"start\":56612},{\"end\":56630,\"start\":56625},{\"end\":56640,\"start\":56636},{\"end\":56751,\"start\":56746},{\"end\":56805,\"start\":56798},{\"end\":56818,\"start\":56813},{\"end\":56971,\"start\":56967},{\"end\":56992,\"start\":56985},{\"end\":57008,\"start\":57002},{\"end\":57196,\"start\":57189},{\"end\":57242,\"start\":57238},{\"end\":57279,\"start\":57274},{\"end\":57312,\"start\":57307},{\"end\":57348,\"start\":57343},{\"end\":57432,\"start\":57423},{\"end\":57466,\"start\":57460},{\"end\":57550,\"start\":57545},{\"end\":57563,\"start\":57559},{\"end\":57783,\"start\":57774},{\"end\":57801,\"start\":57795},{\"end\":58058,\"start\":58054},{\"end\":58100,\"start\":58094},{\"end\":58116,\"start\":58109},{\"end\":58131,\"start\":58125},{\"end\":58176,\"start\":58172},{\"end\":58226,\"start\":58222},{\"end\":58240,\"start\":58236},{\"end\":58531,\"start\":58524},{\"end\":58545,\"start\":58538},{\"end\":58559,\"start\":58552},{\"end\":58708,\"start\":58704},{\"end\":58722,\"start\":58715},{\"end\":58852,\"start\":58848},{\"end\":58878,\"start\":58874},{\"end\":58894,\"start\":58887},{\"end\":59009,\"start\":59004},{\"end\":59083,\"start\":59076},{\"end\":59094,\"start\":59090},{\"end\":59109,\"start\":59104},{\"end\":59128,\"start\":59118},{\"end\":59136,\"start\":59135},{\"end\":59265,\"start\":59258},{\"end\":59298,\"start\":59295},{\"end\":59314,\"start\":59305},{\"end\":59450,\"start\":59442},{\"end\":59459,\"start\":59455},{\"end\":59518,\"start\":59515},{\"end\":59531,\"start\":59525}]", "bib_author_last_name": "[{\"end\":51051,\"start\":51048},{\"end\":51063,\"start\":51058},{\"end\":51072,\"start\":51065},{\"end\":51079,\"start\":51074},{\"end\":51087,\"start\":51081},{\"end\":51095,\"start\":51089},{\"end\":51106,\"start\":51102},{\"end\":51116,\"start\":51108},{\"end\":51122,\"start\":51118},{\"end\":51129,\"start\":51124},{\"end\":51135,\"start\":51131},{\"end\":51141,\"start\":51137},{\"end\":51152,\"start\":51143},{\"end\":51161,\"start\":51154},{\"end\":51167,\"start\":51163},{\"end\":51177,\"start\":51169},{\"end\":51182,\"start\":51179},{\"end\":51286,\"start\":51281},{\"end\":51294,\"start\":51288},{\"end\":51303,\"start\":51296},{\"end\":51307,\"start\":51305},{\"end\":51314,\"start\":51309},{\"end\":51325,\"start\":51316},{\"end\":51331,\"start\":51327},{\"end\":51346,\"start\":51340},{\"end\":51390,\"start\":51387},{\"end\":51397,\"start\":51392},{\"end\":51403,\"start\":51399},{\"end\":51411,\"start\":51405},{\"end\":51415,\"start\":51413},{\"end\":51422,\"start\":51417},{\"end\":51426,\"start\":51424},{\"end\":51435,\"start\":51428},{\"end\":51446,\"start\":51437},{\"end\":51452,\"start\":51448},{\"end\":51472,\"start\":51463},{\"end\":51485,\"start\":51481},{\"end\":51654,\"start\":51646},{\"end\":51672,\"start\":51666},{\"end\":51793,\"start\":51790},{\"end\":51807,\"start\":51804},{\"end\":51944,\"start\":51940},{\"end\":51948,\"start\":51946},{\"end\":51956,\"start\":51950},{\"end\":51960,\"start\":51958},{\"end\":51969,\"start\":51962},{\"end\":51978,\"start\":51971},{\"end\":51985,\"start\":51980},{\"end\":51992,\"start\":51987},{\"end\":51999,\"start\":51994},{\"end\":52009,\"start\":52001},{\"end\":52020,\"start\":52011},{\"end\":52026,\"start\":52022},{\"end\":52034,\"start\":52028},{\"end\":52044,\"start\":52040},{\"end\":52055,\"start\":52051},{\"end\":52069,\"start\":52065},{\"end\":52185,\"start\":52178},{\"end\":52194,\"start\":52187},{\"end\":52201,\"start\":52196},{\"end\":52208,\"start\":52203},{\"end\":52217,\"start\":52210},{\"end\":52225,\"start\":52219},{\"end\":52233,\"start\":52227},{\"end\":52242,\"start\":52235},{\"end\":52248,\"start\":52244},{\"end\":52255,\"start\":52250},{\"end\":52260,\"start\":52257},{\"end\":52268,\"start\":52262},{\"end\":52281,\"start\":52277},{\"end\":52333,\"start\":52323},{\"end\":52348,\"start\":52335},{\"end\":52354,\"start\":52350},{\"end\":52361,\"start\":52356},{\"end\":52368,\"start\":52363},{\"end\":52377,\"start\":52375},{\"end\":52393,\"start\":52387},{\"end\":52400,\"start\":52395},{\"end\":52407,\"start\":52402},{\"end\":52416,\"start\":52409},{\"end\":52433,\"start\":52424},{\"end\":52448,\"start\":52442},{\"end\":52529,\"start\":52527},{\"end\":52543,\"start\":52535},{\"end\":52670,\"start\":52668},{\"end\":52681,\"start\":52677},{\"end\":52696,\"start\":52690},{\"end\":52781,\"start\":52779},{\"end\":52787,\"start\":52783},{\"end\":52795,\"start\":52789},{\"end\":52803,\"start\":52797},{\"end\":52822,\"start\":52815},{\"end\":52830,\"start\":52824},{\"end\":52834,\"start\":52832},{\"end\":52843,\"start\":52836},{\"end\":52849,\"start\":52845},{\"end\":52856,\"start\":52851},{\"end\":52865,\"start\":52861},{\"end\":52878,\"start\":52874},{\"end\":52884,\"start\":52880},{\"end\":53066,\"start\":53064},{\"end\":53071,\"start\":53068},{\"end\":53079,\"start\":53073},{\"end\":53086,\"start\":53081},{\"end\":53093,\"start\":53088},{\"end\":53099,\"start\":53095},{\"end\":53108,\"start\":53101},{\"end\":53119,\"start\":53110},{\"end\":53133,\"start\":53129},{\"end\":53147,\"start\":53142},{\"end\":53160,\"start\":53156},{\"end\":53290,\"start\":53282},{\"end\":53298,\"start\":53292},{\"end\":53304,\"start\":53300},{\"end\":53312,\"start\":53306},{\"end\":53319,\"start\":53314},{\"end\":53328,\"start\":53321},{\"end\":53340,\"start\":53330},{\"end\":53351,\"start\":53346},{\"end\":53362,\"start\":53358},{\"end\":53494,\"start\":53490},{\"end\":53500,\"start\":53496},{\"end\":53505,\"start\":53502},{\"end\":53512,\"start\":53507},{\"end\":53519,\"start\":53514},{\"end\":53531,\"start\":53527},{\"end\":53548,\"start\":53539},{\"end\":53656,\"start\":53646},{\"end\":53673,\"start\":53667},{\"end\":53809,\"start\":53806},{\"end\":53822,\"start\":53817},{\"end\":53837,\"start\":53831},{\"end\":53936,\"start\":53933},{\"end\":53941,\"start\":53938},{\"end\":53946,\"start\":53943},{\"end\":53951,\"start\":53948},{\"end\":53962,\"start\":53953},{\"end\":53971,\"start\":53964},{\"end\":53979,\"start\":53973},{\"end\":53983,\"start\":53981},{\"end\":53991,\"start\":53985},{\"end\":54002,\"start\":53993},{\"end\":54009,\"start\":54004},{\"end\":54017,\"start\":54011},{\"end\":54025,\"start\":54019},{\"end\":54047,\"start\":54036},{\"end\":54060,\"start\":54058},{\"end\":54067,\"start\":54062},{\"end\":54234,\"start\":54232},{\"end\":54238,\"start\":54236},{\"end\":54246,\"start\":54240},{\"end\":54261,\"start\":54256},{\"end\":54273,\"start\":54270},{\"end\":54390,\"start\":54387},{\"end\":54395,\"start\":54392},{\"end\":54414,\"start\":54405},{\"end\":54429,\"start\":54423},{\"end\":54540,\"start\":54537},{\"end\":54544,\"start\":54542},{\"end\":54552,\"start\":54546},{\"end\":54556,\"start\":54554},{\"end\":54563,\"start\":54558},{\"end\":54581,\"start\":54573},{\"end\":54601,\"start\":54592},{\"end\":54660,\"start\":54657},{\"end\":54670,\"start\":54662},{\"end\":54675,\"start\":54672},{\"end\":54684,\"start\":54677},{\"end\":54693,\"start\":54686},{\"end\":54699,\"start\":54695},{\"end\":54708,\"start\":54701},{\"end\":54717,\"start\":54710},{\"end\":54723,\"start\":54719},{\"end\":54731,\"start\":54725},{\"end\":54739,\"start\":54733},{\"end\":54745,\"start\":54741},{\"end\":54752,\"start\":54747},{\"end\":54759,\"start\":54754},{\"end\":54763,\"start\":54761},{\"end\":54781,\"start\":54774},{\"end\":54796,\"start\":54788},{\"end\":54940,\"start\":54937},{\"end\":54945,\"start\":54942},{\"end\":54951,\"start\":54947},{\"end\":54966,\"start\":54962},{\"end\":54979,\"start\":54975},{\"end\":55112,\"start\":55106},{\"end\":55116,\"start\":55114},{\"end\":55125,\"start\":55118},{\"end\":55132,\"start\":55127},{\"end\":55136,\"start\":55134},{\"end\":55145,\"start\":55138},{\"end\":55152,\"start\":55147},{\"end\":55164,\"start\":55154},{\"end\":55173,\"start\":55166},{\"end\":55182,\"start\":55175},{\"end\":55190,\"start\":55184},{\"end\":55197,\"start\":55192},{\"end\":55204,\"start\":55199},{\"end\":55213,\"start\":55206},{\"end\":55223,\"start\":55215},{\"end\":55230,\"start\":55225},{\"end\":55240,\"start\":55232},{\"end\":55250,\"start\":55247},{\"end\":55405,\"start\":55399},{\"end\":55420,\"start\":55414},{\"end\":55626,\"start\":55619},{\"end\":55634,\"start\":55628},{\"end\":55655,\"start\":55648},{\"end\":55666,\"start\":55657},{\"end\":55677,\"start\":55668},{\"end\":55683,\"start\":55679},{\"end\":55688,\"start\":55685},{\"end\":55695,\"start\":55690},{\"end\":55702,\"start\":55697},{\"end\":55710,\"start\":55704},{\"end\":55719,\"start\":55712},{\"end\":55729,\"start\":55721},{\"end\":55737,\"start\":55731},{\"end\":55744,\"start\":55739},{\"end\":55750,\"start\":55746},{\"end\":55767,\"start\":55760},{\"end\":55775,\"start\":55769},{\"end\":55780,\"start\":55777},{\"end\":55827,\"start\":55822},{\"end\":55838,\"start\":55834},{\"end\":55857,\"start\":55851},{\"end\":55873,\"start\":55863},{\"end\":56038,\"start\":56031},{\"end\":56048,\"start\":56045},{\"end\":56054,\"start\":56050},{\"end\":56063,\"start\":56056},{\"end\":56070,\"start\":56065},{\"end\":56078,\"start\":56072},{\"end\":56086,\"start\":56080},{\"end\":56091,\"start\":56088},{\"end\":56100,\"start\":56093},{\"end\":56109,\"start\":56102},{\"end\":56119,\"start\":56111},{\"end\":56127,\"start\":56121},{\"end\":56135,\"start\":56129},{\"end\":56143,\"start\":56137},{\"end\":56151,\"start\":56145},{\"end\":56160,\"start\":56153},{\"end\":56168,\"start\":56162},{\"end\":56175,\"start\":56170},{\"end\":56181,\"start\":56177},{\"end\":56344,\"start\":56338},{\"end\":56353,\"start\":56346},{\"end\":56359,\"start\":56355},{\"end\":56368,\"start\":56361},{\"end\":56374,\"start\":56370},{\"end\":56379,\"start\":56376},{\"end\":56390,\"start\":56381},{\"end\":56398,\"start\":56392},{\"end\":56406,\"start\":56400},{\"end\":56414,\"start\":56408},{\"end\":56423,\"start\":56416},{\"end\":56429,\"start\":56425},{\"end\":56436,\"start\":56431},{\"end\":56444,\"start\":56442},{\"end\":56449,\"start\":56446},{\"end\":56458,\"start\":56453},{\"end\":56590,\"start\":56584},{\"end\":56600,\"start\":56592},{\"end\":56610,\"start\":56602},{\"end\":56623,\"start\":56617},{\"end\":56634,\"start\":56631},{\"end\":56645,\"start\":56641},{\"end\":56759,\"start\":56752},{\"end\":56770,\"start\":56761},{\"end\":56779,\"start\":56772},{\"end\":56787,\"start\":56781},{\"end\":56796,\"start\":56789},{\"end\":56811,\"start\":56806},{\"end\":56824,\"start\":56819},{\"end\":56983,\"start\":56972},{\"end\":57000,\"start\":56993},{\"end\":57013,\"start\":57009},{\"end\":57204,\"start\":57197},{\"end\":57210,\"start\":57206},{\"end\":57219,\"start\":57212},{\"end\":57227,\"start\":57221},{\"end\":57236,\"start\":57229},{\"end\":57245,\"start\":57243},{\"end\":57252,\"start\":57247},{\"end\":57257,\"start\":57254},{\"end\":57265,\"start\":57259},{\"end\":57272,\"start\":57267},{\"end\":57291,\"start\":57280},{\"end\":57305,\"start\":57293},{\"end\":57317,\"start\":57313},{\"end\":57326,\"start\":57319},{\"end\":57335,\"start\":57328},{\"end\":57341,\"start\":57337},{\"end\":57354,\"start\":57349},{\"end\":57363,\"start\":57356},{\"end\":57442,\"start\":57433},{\"end\":57449,\"start\":57444},{\"end\":57458,\"start\":57451},{\"end\":57475,\"start\":57467},{\"end\":57488,\"start\":57477},{\"end\":57496,\"start\":57490},{\"end\":57504,\"start\":57498},{\"end\":57513,\"start\":57506},{\"end\":57520,\"start\":57515},{\"end\":57528,\"start\":57522},{\"end\":57537,\"start\":57530},{\"end\":57543,\"start\":57539},{\"end\":57557,\"start\":57551},{\"end\":57575,\"start\":57564},{\"end\":57793,\"start\":57784},{\"end\":57810,\"start\":57802},{\"end\":57817,\"start\":57812},{\"end\":57826,\"start\":57819},{\"end\":57834,\"start\":57828},{\"end\":57840,\"start\":57836},{\"end\":57850,\"start\":57842},{\"end\":57856,\"start\":57852},{\"end\":57864,\"start\":57858},{\"end\":57871,\"start\":57866},{\"end\":57880,\"start\":57873},{\"end\":57886,\"start\":57882},{\"end\":57893,\"start\":57888},{\"end\":57901,\"start\":57895},{\"end\":57909,\"start\":57903},{\"end\":57918,\"start\":57911},{\"end\":57928,\"start\":57920},{\"end\":57938,\"start\":57930},{\"end\":58067,\"start\":58059},{\"end\":58075,\"start\":58069},{\"end\":58084,\"start\":58077},{\"end\":58092,\"start\":58086},{\"end\":58107,\"start\":58101},{\"end\":58123,\"start\":58117},{\"end\":58185,\"start\":58177},{\"end\":58193,\"start\":58187},{\"end\":58200,\"start\":58195},{\"end\":58210,\"start\":58202},{\"end\":58220,\"start\":58212},{\"end\":58234,\"start\":58227},{\"end\":58247,\"start\":58241},{\"end\":58407,\"start\":58393},{\"end\":58415,\"start\":58409},{\"end\":58422,\"start\":58417},{\"end\":58428,\"start\":58424},{\"end\":58445,\"start\":58430},{\"end\":58451,\"start\":58447},{\"end\":58460,\"start\":58453},{\"end\":58467,\"start\":58462},{\"end\":58536,\"start\":58532},{\"end\":58550,\"start\":58546},{\"end\":58565,\"start\":58560},{\"end\":58713,\"start\":58709},{\"end\":58728,\"start\":58723},{\"end\":58857,\"start\":58853},{\"end\":58865,\"start\":58859},{\"end\":58872,\"start\":58867},{\"end\":58885,\"start\":58879},{\"end\":58900,\"start\":58895},{\"end\":59018,\"start\":59010},{\"end\":59026,\"start\":59020},{\"end\":59032,\"start\":59028},{\"end\":59036,\"start\":59034},{\"end\":59045,\"start\":59038},{\"end\":59054,\"start\":59047},{\"end\":59062,\"start\":59056},{\"end\":59068,\"start\":59064},{\"end\":59074,\"start\":59070},{\"end\":59088,\"start\":59084},{\"end\":59102,\"start\":59095},{\"end\":59116,\"start\":59110},{\"end\":59133,\"start\":59129},{\"end\":59268,\"start\":59266},{\"end\":59273,\"start\":59270},{\"end\":59282,\"start\":59275},{\"end\":59287,\"start\":59284},{\"end\":59293,\"start\":59289},{\"end\":59303,\"start\":59299},{\"end\":59317,\"start\":59315},{\"end\":59453,\"start\":59451},{\"end\":59463,\"start\":59460},{\"end\":59467,\"start\":59465},{\"end\":59475,\"start\":59469},{\"end\":59481,\"start\":59477},{\"end\":59489,\"start\":59483},{\"end\":59493,\"start\":59491},{\"end\":59501,\"start\":59495},{\"end\":59507,\"start\":59503},{\"end\":59513,\"start\":59509},{\"end\":59523,\"start\":59519},{\"end\":59536,\"start\":59532}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2204.05862\",\"id\":\"b0\"},\"end\":51218,\"start\":50950},{\"attributes\":{\"doi\":\"arXiv:2305.13301\",\"id\":\"b1\"},\"end\":51385,\"start\":51220},{\"attributes\":{\"doi\":\"arXiv:2303.04137\",\"id\":\"b2\"},\"end\":51588,\"start\":51387},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":234357997},\"end\":51731,\"start\":51590},{\"attributes\":{\"doi\":\"arXiv:2301.13362\",\"id\":\"b4\"},\"end\":51845,\"start\":51733},{\"attributes\":{\"doi\":\"arXiv:2212.05032\",\"id\":\"b5\"},\"end\":52106,\"start\":51847},{\"attributes\":{\"doi\":\"arXiv:2212.10015\",\"id\":\"b6\"},\"end\":52317,\"start\":52108},{\"attributes\":{\"id\":\"b7\"},\"end\":52516,\"start\":52319},{\"attributes\":{\"doi\":\"arXiv:2207.12598\",\"id\":\"b8\"},\"end\":52615,\"start\":52518},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219955663},\"end\":52768,\"start\":52617},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b10\"},\"end\":52966,\"start\":52770},{\"attributes\":{\"doi\":\"arXiv:2303.11897\",\"id\":\"b11\"},\"end\":53196,\"start\":52968},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":258437096},\"end\":53421,\"start\":53198},{\"attributes\":{\"doi\":\"arXiv:2009.09761\",\"id\":\"b13\"},\"end\":53584,\"start\":53423},{\"attributes\":{\"id\":\"b14\"},\"end\":53679,\"start\":53586},{\"attributes\":{\"doi\":\"arXiv:2106.05091\",\"id\":\"b15\"},\"end\":53873,\"start\":53681},{\"attributes\":{\"doi\":\"arXiv:2302.12192\",\"id\":\"b16\"},\"end\":54118,\"start\":53875},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":246411402},\"end\":54328,\"start\":54120},{\"attributes\":{\"doi\":\"arXiv:2302.02676\",\"id\":\"b18\"},\"end\":54465,\"start\":54330},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":249375227},\"end\":54647,\"start\":54467},{\"attributes\":{\"doi\":\"arXiv:2212.10562\",\"id\":\"b20\"},\"end\":54886,\"start\":54649},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":459456},\"end\":55030,\"start\":54888},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":246426909},\"end\":55309,\"start\":55032},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11551208},\"end\":55549,\"start\":55311},{\"attributes\":{\"doi\":\"arXiv:2211.12112\",\"id\":\"b24\"},\"end\":55816,\"start\":55551},{\"attributes\":{\"doi\":\"arXiv:2209.14988\",\"id\":\"b25\"},\"end\":55953,\"start\":55818},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":231591445},\"end\":56247,\"start\":55955},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":204838007},\"end\":56509,\"start\":56249},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b28\"},\"end\":56682,\"start\":56511},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":245335280},\"end\":56900,\"start\":56684},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3719281},\"end\":57107,\"start\":56902},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":248986576},\"end\":57421,\"start\":57109},{\"attributes\":{\"doi\":\"arXiv:2111.02114\",\"id\":\"b32\"},\"end\":57686,\"start\":57423},{\"attributes\":{\"doi\":\"arXiv:2210.08402\",\"id\":\"b33\"},\"end\":57977,\"start\":57688},{\"attributes\":{\"doi\":\"arXiv:1506.02438\",\"id\":\"b34\"},\"end\":58170,\"start\":57979},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b35\"},\"end\":58327,\"start\":58172},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14888175},\"end\":58520,\"start\":58329},{\"attributes\":{\"doi\":\"arXiv:2010.02502\",\"id\":\"b37\"},\"end\":58638,\"start\":58522},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":219708245},\"end\":58785,\"start\":58640},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":235352469},\"end\":58959,\"start\":58787},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":221665105},\"end\":59196,\"start\":58961},{\"attributes\":{\"doi\":\"arXiv:2303.14420\",\"id\":\"b41\"},\"end\":59355,\"start\":59198},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":258079316},\"end\":59607,\"start\":59357}]", "bib_title": "[{\"end\":51635,\"start\":51590},{\"end\":52657,\"start\":52617},{\"end\":53274,\"start\":53198},{\"end\":54223,\"start\":54120},{\"end\":54531,\"start\":54467},{\"end\":54929,\"start\":54888},{\"end\":55099,\"start\":55032},{\"end\":55393,\"start\":55311},{\"end\":56024,\"start\":55955},{\"end\":56330,\"start\":56249},{\"end\":56744,\"start\":56684},{\"end\":56965,\"start\":56902},{\"end\":57187,\"start\":57109},{\"end\":58391,\"start\":58329},{\"end\":58702,\"start\":58640},{\"end\":58846,\"start\":58787},{\"end\":59002,\"start\":58961},{\"end\":59440,\"start\":59357}]", "bib_author": "[{\"end\":51053,\"start\":51041},{\"end\":51065,\"start\":51053},{\"end\":51074,\"start\":51065},{\"end\":51081,\"start\":51074},{\"end\":51089,\"start\":51081},{\"end\":51097,\"start\":51089},{\"end\":51108,\"start\":51097},{\"end\":51118,\"start\":51108},{\"end\":51124,\"start\":51118},{\"end\":51131,\"start\":51124},{\"end\":51137,\"start\":51131},{\"end\":51143,\"start\":51137},{\"end\":51154,\"start\":51143},{\"end\":51163,\"start\":51154},{\"end\":51169,\"start\":51163},{\"end\":51179,\"start\":51169},{\"end\":51184,\"start\":51179},{\"end\":51288,\"start\":51275},{\"end\":51296,\"start\":51288},{\"end\":51305,\"start\":51296},{\"end\":51309,\"start\":51305},{\"end\":51316,\"start\":51309},{\"end\":51327,\"start\":51316},{\"end\":51333,\"start\":51327},{\"end\":51348,\"start\":51333},{\"end\":51392,\"start\":51387},{\"end\":51399,\"start\":51392},{\"end\":51405,\"start\":51399},{\"end\":51413,\"start\":51405},{\"end\":51417,\"start\":51413},{\"end\":51424,\"start\":51417},{\"end\":51428,\"start\":51424},{\"end\":51437,\"start\":51428},{\"end\":51448,\"start\":51437},{\"end\":51454,\"start\":51448},{\"end\":51474,\"start\":51454},{\"end\":51487,\"start\":51474},{\"end\":51656,\"start\":51637},{\"end\":51674,\"start\":51656},{\"end\":51795,\"start\":51785},{\"end\":51809,\"start\":51795},{\"end\":51946,\"start\":51934},{\"end\":51950,\"start\":51946},{\"end\":51958,\"start\":51950},{\"end\":51962,\"start\":51958},{\"end\":51971,\"start\":51962},{\"end\":51980,\"start\":51971},{\"end\":51987,\"start\":51980},{\"end\":51994,\"start\":51987},{\"end\":52001,\"start\":51994},{\"end\":52011,\"start\":52001},{\"end\":52022,\"start\":52011},{\"end\":52028,\"start\":52022},{\"end\":52036,\"start\":52028},{\"end\":52046,\"start\":52036},{\"end\":52057,\"start\":52046},{\"end\":52071,\"start\":52057},{\"end\":52187,\"start\":52172},{\"end\":52196,\"start\":52187},{\"end\":52203,\"start\":52196},{\"end\":52210,\"start\":52203},{\"end\":52219,\"start\":52210},{\"end\":52227,\"start\":52219},{\"end\":52235,\"start\":52227},{\"end\":52244,\"start\":52235},{\"end\":52250,\"start\":52244},{\"end\":52257,\"start\":52250},{\"end\":52262,\"start\":52257},{\"end\":52270,\"start\":52262},{\"end\":52283,\"start\":52270},{\"end\":52335,\"start\":52319},{\"end\":52350,\"start\":52335},{\"end\":52356,\"start\":52350},{\"end\":52363,\"start\":52356},{\"end\":52370,\"start\":52363},{\"end\":52379,\"start\":52370},{\"end\":52395,\"start\":52379},{\"end\":52402,\"start\":52395},{\"end\":52409,\"start\":52402},{\"end\":52418,\"start\":52409},{\"end\":52435,\"start\":52418},{\"end\":52450,\"start\":52435},{\"end\":52531,\"start\":52518},{\"end\":52545,\"start\":52531},{\"end\":52672,\"start\":52659},{\"end\":52683,\"start\":52672},{\"end\":52698,\"start\":52683},{\"end\":52783,\"start\":52770},{\"end\":52789,\"start\":52783},{\"end\":52797,\"start\":52789},{\"end\":52805,\"start\":52797},{\"end\":52824,\"start\":52805},{\"end\":52832,\"start\":52824},{\"end\":52836,\"start\":52832},{\"end\":52845,\"start\":52836},{\"end\":52851,\"start\":52845},{\"end\":52858,\"start\":52851},{\"end\":52867,\"start\":52858},{\"end\":52880,\"start\":52867},{\"end\":52886,\"start\":52880},{\"end\":53068,\"start\":53058},{\"end\":53073,\"start\":53068},{\"end\":53081,\"start\":53073},{\"end\":53088,\"start\":53081},{\"end\":53095,\"start\":53088},{\"end\":53101,\"start\":53095},{\"end\":53110,\"start\":53101},{\"end\":53121,\"start\":53110},{\"end\":53135,\"start\":53121},{\"end\":53149,\"start\":53135},{\"end\":53162,\"start\":53149},{\"end\":53292,\"start\":53276},{\"end\":53300,\"start\":53292},{\"end\":53306,\"start\":53300},{\"end\":53314,\"start\":53306},{\"end\":53321,\"start\":53314},{\"end\":53330,\"start\":53321},{\"end\":53342,\"start\":53330},{\"end\":53353,\"start\":53342},{\"end\":53364,\"start\":53353},{\"end\":53496,\"start\":53482},{\"end\":53502,\"start\":53496},{\"end\":53507,\"start\":53502},{\"end\":53514,\"start\":53507},{\"end\":53521,\"start\":53514},{\"end\":53533,\"start\":53521},{\"end\":53550,\"start\":53533},{\"end\":53658,\"start\":53641},{\"end\":53675,\"start\":53658},{\"end\":53811,\"start\":53800},{\"end\":53824,\"start\":53811},{\"end\":53839,\"start\":53824},{\"end\":53938,\"start\":53927},{\"end\":53943,\"start\":53938},{\"end\":53948,\"start\":53943},{\"end\":53953,\"start\":53948},{\"end\":53964,\"start\":53953},{\"end\":53973,\"start\":53964},{\"end\":53981,\"start\":53973},{\"end\":53985,\"start\":53981},{\"end\":53993,\"start\":53985},{\"end\":54004,\"start\":53993},{\"end\":54011,\"start\":54004},{\"end\":54019,\"start\":54011},{\"end\":54027,\"start\":54019},{\"end\":54049,\"start\":54027},{\"end\":54062,\"start\":54049},{\"end\":54069,\"start\":54062},{\"end\":54236,\"start\":54225},{\"end\":54240,\"start\":54236},{\"end\":54248,\"start\":54240},{\"end\":54263,\"start\":54248},{\"end\":54275,\"start\":54263},{\"end\":54392,\"start\":54387},{\"end\":54397,\"start\":54392},{\"end\":54416,\"start\":54397},{\"end\":54431,\"start\":54416},{\"end\":54542,\"start\":54533},{\"end\":54546,\"start\":54542},{\"end\":54554,\"start\":54546},{\"end\":54558,\"start\":54554},{\"end\":54565,\"start\":54558},{\"end\":54583,\"start\":54565},{\"end\":54603,\"start\":54583},{\"end\":54662,\"start\":54649},{\"end\":54672,\"start\":54662},{\"end\":54677,\"start\":54672},{\"end\":54686,\"start\":54677},{\"end\":54695,\"start\":54686},{\"end\":54701,\"start\":54695},{\"end\":54710,\"start\":54701},{\"end\":54719,\"start\":54710},{\"end\":54725,\"start\":54719},{\"end\":54733,\"start\":54725},{\"end\":54741,\"start\":54733},{\"end\":54747,\"start\":54741},{\"end\":54754,\"start\":54747},{\"end\":54761,\"start\":54754},{\"end\":54765,\"start\":54761},{\"end\":54783,\"start\":54765},{\"end\":54798,\"start\":54783},{\"end\":54942,\"start\":54931},{\"end\":54947,\"start\":54942},{\"end\":54953,\"start\":54947},{\"end\":54968,\"start\":54953},{\"end\":54981,\"start\":54968},{\"end\":55114,\"start\":55101},{\"end\":55118,\"start\":55114},{\"end\":55127,\"start\":55118},{\"end\":55134,\"start\":55127},{\"end\":55138,\"start\":55134},{\"end\":55147,\"start\":55138},{\"end\":55154,\"start\":55147},{\"end\":55166,\"start\":55154},{\"end\":55175,\"start\":55166},{\"end\":55184,\"start\":55175},{\"end\":55192,\"start\":55184},{\"end\":55199,\"start\":55192},{\"end\":55206,\"start\":55199},{\"end\":55215,\"start\":55206},{\"end\":55225,\"start\":55215},{\"end\":55232,\"start\":55225},{\"end\":55242,\"start\":55232},{\"end\":55252,\"start\":55242},{\"end\":55407,\"start\":55395},{\"end\":55422,\"start\":55407},{\"end\":55628,\"start\":55619},{\"end\":55636,\"start\":55628},{\"end\":55657,\"start\":55636},{\"end\":55668,\"start\":55657},{\"end\":55679,\"start\":55668},{\"end\":55685,\"start\":55679},{\"end\":55690,\"start\":55685},{\"end\":55697,\"start\":55690},{\"end\":55704,\"start\":55697},{\"end\":55712,\"start\":55704},{\"end\":55721,\"start\":55712},{\"end\":55731,\"start\":55721},{\"end\":55739,\"start\":55731},{\"end\":55746,\"start\":55739},{\"end\":55752,\"start\":55746},{\"end\":55769,\"start\":55752},{\"end\":55777,\"start\":55769},{\"end\":55782,\"start\":55777},{\"end\":55829,\"start\":55818},{\"end\":55840,\"start\":55829},{\"end\":55859,\"start\":55840},{\"end\":55875,\"start\":55859},{\"end\":56040,\"start\":56026},{\"end\":56050,\"start\":56040},{\"end\":56056,\"start\":56050},{\"end\":56065,\"start\":56056},{\"end\":56072,\"start\":56065},{\"end\":56080,\"start\":56072},{\"end\":56088,\"start\":56080},{\"end\":56093,\"start\":56088},{\"end\":56102,\"start\":56093},{\"end\":56111,\"start\":56102},{\"end\":56121,\"start\":56111},{\"end\":56129,\"start\":56121},{\"end\":56137,\"start\":56129},{\"end\":56145,\"start\":56137},{\"end\":56153,\"start\":56145},{\"end\":56162,\"start\":56153},{\"end\":56170,\"start\":56162},{\"end\":56177,\"start\":56170},{\"end\":56183,\"start\":56177},{\"end\":56346,\"start\":56332},{\"end\":56355,\"start\":56346},{\"end\":56361,\"start\":56355},{\"end\":56370,\"start\":56361},{\"end\":56376,\"start\":56370},{\"end\":56381,\"start\":56376},{\"end\":56392,\"start\":56381},{\"end\":56400,\"start\":56392},{\"end\":56408,\"start\":56400},{\"end\":56416,\"start\":56408},{\"end\":56425,\"start\":56416},{\"end\":56431,\"start\":56425},{\"end\":56438,\"start\":56431},{\"end\":56446,\"start\":56438},{\"end\":56451,\"start\":56446},{\"end\":56460,\"start\":56451},{\"end\":56592,\"start\":56577},{\"end\":56602,\"start\":56592},{\"end\":56612,\"start\":56602},{\"end\":56625,\"start\":56612},{\"end\":56636,\"start\":56625},{\"end\":56647,\"start\":56636},{\"end\":56761,\"start\":56746},{\"end\":56772,\"start\":56761},{\"end\":56781,\"start\":56772},{\"end\":56789,\"start\":56781},{\"end\":56798,\"start\":56789},{\"end\":56813,\"start\":56798},{\"end\":56826,\"start\":56813},{\"end\":56985,\"start\":56967},{\"end\":57002,\"start\":56985},{\"end\":57015,\"start\":57002},{\"end\":57206,\"start\":57189},{\"end\":57212,\"start\":57206},{\"end\":57221,\"start\":57212},{\"end\":57229,\"start\":57221},{\"end\":57238,\"start\":57229},{\"end\":57247,\"start\":57238},{\"end\":57254,\"start\":57247},{\"end\":57259,\"start\":57254},{\"end\":57267,\"start\":57259},{\"end\":57274,\"start\":57267},{\"end\":57293,\"start\":57274},{\"end\":57307,\"start\":57293},{\"end\":57319,\"start\":57307},{\"end\":57328,\"start\":57319},{\"end\":57337,\"start\":57328},{\"end\":57343,\"start\":57337},{\"end\":57356,\"start\":57343},{\"end\":57365,\"start\":57356},{\"end\":57444,\"start\":57423},{\"end\":57451,\"start\":57444},{\"end\":57460,\"start\":57451},{\"end\":57477,\"start\":57460},{\"end\":57490,\"start\":57477},{\"end\":57498,\"start\":57490},{\"end\":57506,\"start\":57498},{\"end\":57515,\"start\":57506},{\"end\":57522,\"start\":57515},{\"end\":57530,\"start\":57522},{\"end\":57539,\"start\":57530},{\"end\":57545,\"start\":57539},{\"end\":57559,\"start\":57545},{\"end\":57577,\"start\":57559},{\"end\":57795,\"start\":57774},{\"end\":57812,\"start\":57795},{\"end\":57819,\"start\":57812},{\"end\":57828,\"start\":57819},{\"end\":57836,\"start\":57828},{\"end\":57842,\"start\":57836},{\"end\":57852,\"start\":57842},{\"end\":57858,\"start\":57852},{\"end\":57866,\"start\":57858},{\"end\":57873,\"start\":57866},{\"end\":57882,\"start\":57873},{\"end\":57888,\"start\":57882},{\"end\":57895,\"start\":57888},{\"end\":57903,\"start\":57895},{\"end\":57911,\"start\":57903},{\"end\":57920,\"start\":57911},{\"end\":57930,\"start\":57920},{\"end\":57940,\"start\":57930},{\"end\":58069,\"start\":58054},{\"end\":58077,\"start\":58069},{\"end\":58086,\"start\":58077},{\"end\":58094,\"start\":58086},{\"end\":58109,\"start\":58094},{\"end\":58125,\"start\":58109},{\"end\":58134,\"start\":58125},{\"end\":58187,\"start\":58172},{\"end\":58195,\"start\":58187},{\"end\":58202,\"start\":58195},{\"end\":58212,\"start\":58202},{\"end\":58222,\"start\":58212},{\"end\":58236,\"start\":58222},{\"end\":58249,\"start\":58236},{\"end\":58409,\"start\":58393},{\"end\":58417,\"start\":58409},{\"end\":58424,\"start\":58417},{\"end\":58430,\"start\":58424},{\"end\":58447,\"start\":58430},{\"end\":58453,\"start\":58447},{\"end\":58462,\"start\":58453},{\"end\":58469,\"start\":58462},{\"end\":58538,\"start\":58524},{\"end\":58552,\"start\":58538},{\"end\":58567,\"start\":58552},{\"end\":58715,\"start\":58704},{\"end\":58730,\"start\":58715},{\"end\":58859,\"start\":58848},{\"end\":58867,\"start\":58859},{\"end\":58874,\"start\":58867},{\"end\":58887,\"start\":58874},{\"end\":58902,\"start\":58887},{\"end\":59020,\"start\":59004},{\"end\":59028,\"start\":59020},{\"end\":59034,\"start\":59028},{\"end\":59038,\"start\":59034},{\"end\":59047,\"start\":59038},{\"end\":59056,\"start\":59047},{\"end\":59064,\"start\":59056},{\"end\":59070,\"start\":59064},{\"end\":59076,\"start\":59070},{\"end\":59090,\"start\":59076},{\"end\":59104,\"start\":59090},{\"end\":59118,\"start\":59104},{\"end\":59135,\"start\":59118},{\"end\":59139,\"start\":59135},{\"end\":59270,\"start\":59258},{\"end\":59275,\"start\":59270},{\"end\":59284,\"start\":59275},{\"end\":59289,\"start\":59284},{\"end\":59295,\"start\":59289},{\"end\":59305,\"start\":59295},{\"end\":59319,\"start\":59305},{\"end\":59455,\"start\":59442},{\"end\":59465,\"start\":59455},{\"end\":59469,\"start\":59465},{\"end\":59477,\"start\":59469},{\"end\":59483,\"start\":59477},{\"end\":59491,\"start\":59483},{\"end\":59495,\"start\":59491},{\"end\":59503,\"start\":59495},{\"end\":59509,\"start\":59503},{\"end\":59515,\"start\":59509},{\"end\":59525,\"start\":59515},{\"end\":59538,\"start\":59525}]", "bib_venue": "[{\"end\":51039,\"start\":50950},{\"end\":51273,\"start\":51220},{\"end\":51568,\"start\":51503},{\"end\":51723,\"start\":51674},{\"end\":51783,\"start\":51733},{\"end\":51932,\"start\":51847},{\"end\":52170,\"start\":52108},{\"end\":52508,\"start\":52450},{\"end\":52595,\"start\":52561},{\"end\":52747,\"start\":52698},{\"end\":52946,\"start\":52902},{\"end\":53056,\"start\":52968},{\"end\":53413,\"start\":53364},{\"end\":53480,\"start\":53423},{\"end\":53639,\"start\":53586},{\"end\":53798,\"start\":53681},{\"end\":53925,\"start\":53875},{\"end\":54319,\"start\":54275},{\"end\":54385,\"start\":54330},{\"end\":54641,\"start\":54603},{\"end\":54866,\"start\":54814},{\"end\":55024,\"start\":54981},{\"end\":55301,\"start\":55252},{\"end\":55490,\"start\":55422},{\"end\":55617,\"start\":55551},{\"end\":55933,\"start\":55891},{\"end\":56227,\"start\":56183},{\"end\":56500,\"start\":56460},{\"end\":56575,\"start\":56511},{\"end\":56879,\"start\":56826},{\"end\":57101,\"start\":57015},{\"end\":57414,\"start\":57365},{\"end\":57663,\"start\":57593},{\"end\":57772,\"start\":57688},{\"end\":58052,\"start\":57979},{\"end\":58304,\"start\":58265},{\"end\":58513,\"start\":58469},{\"end\":58779,\"start\":58730},{\"end\":58951,\"start\":58902},{\"end\":59188,\"start\":59139},{\"end\":59256,\"start\":59198},{\"end\":59587,\"start\":59538},{\"end\":55545,\"start\":55492}]"}}}, "year": 2023, "month": 12, "day": 17}