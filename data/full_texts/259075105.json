{"id": 259075105, "updated": "2023-10-05 04:34:48.674", "metadata": {"title": "Adaptive whitening in neural populations with gain-modulating interneurons", "authors": "[{\"first\":\"Lyndon\",\"last\":\"Duong\",\"middle\":[\"R.\"]},{\"first\":\"David\",\"last\":\"Lipshutz\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Heeger\",\"middle\":[\"J.\"]},{\"first\":\"Dmitri\",\"last\":\"Chklovskii\",\"middle\":[\"B.\"]},{\"first\":\"Eero\",\"last\":\"Simoncelli\",\"middle\":[\"P.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Statistical whitening transformations play a fundamental role in many computational systems, and may also play an important role in biological sensory systems. Existing neural circuit models of adaptive whitening operate by modifying synaptic interactions; however, such modifications would seem both too slow and insufficiently reversible. Motivated by the extensive neuroscience literature on gain modulation, we propose an alternative model that adaptively whitens its responses by modulating the gains of individual neurons. Starting from a novel whitening objective, we derive an online algorithm that whitens its outputs by adjusting the marginal variances of an overcomplete set of projections. We map the algorithm onto a recurrent neural network with fixed synaptic weights and gain-modulating interneurons. We demonstrate numerically that sign-constraining the gains improves robustness of the network to ill-conditioned inputs, and a generalization of the circuit achieves a form of local whitening in convolutional populations, such as those found throughout the visual or auditory systems.", "fields_of_study": "[\"Biology\",\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2301.11955", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/DuongLHCS23", "doi": null}}, "content": {"source": {"pdf_hash": "f9eabb331cef39366b232ffb12c948a9476d180a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.11955v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "143a6244a367c71d9c4bd28ae3fc271ce56ea191", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f9eabb331cef39366b232ffb12c948a9476d180a.txt", "contents": "\nAdaptive Whitening in Neural Populations with Gain-modulating Interneurons\n\n\nLyndon R Duong \nDavid Lipshutz \nDavid J Heeger \nDmitri B Chklovskii \nEero P Simoncelli \nAdaptive Whitening in Neural Populations with Gain-modulating Interneurons\n\nStatistical whitening transformations play a fundamental role in many computational systems, and may also play an important role in biological sensory systems. Existing neural circuit models of adaptive whitening operate by modifying synaptic interactions; however, such modifications would seem both too slow and insufficiently reversible. Motivated by the extensive neuroscience literature on gain modulation, we propose an alternative model that adaptively whitens its responses by modulating the gains of individual neurons. Starting from a novel whitening objective, we derive an online algorithm that whitens its outputs by adjusting the marginal variances of an overcomplete set of projections. We map the algorithm onto a recurrent neural network with fixed synaptic weights and gain-modulating interneurons. We demonstrate numerically that sign-constraining the gains improves robustness of the network to ill-conditioned inputs, and a generalization of the circuit achieves a form of local whitening in convolutional populations, such as those found throughout the visual or auditory systems.\n\nIntroduction\n\nStatistical whitening transformations, in which multidimensional inputs are decorrelated and normalized to have unit variance, are common in signal processing and machine learning systems. For example, they are integral to many statistical factorization methods (Olshausen & Field, 1996;Bell & Sejnowski, 1996;Hyv\u00e4rinen & Oja, 2000), they provide beneficial preprocessing during neural network training (Krizhevsky, 2009), and they can improve unsuper-vised feature learning (Coates et al., 2011). More recently, self-supervised learning methods have used decorrelation transformations such as whitening to prevent representational collapse (Ermolov et al., 2021;Zbontar et al., 2021;Hua et al., 2021;Bardes et al., 2022). While whitening has mostly been used for training neural networks in the offline setting, it is also of interest to develop adaptive (run-time) variants that can adjust to dynamically changing input statistics with minimal changes to the network (e.g. Mohan et al., 2021;Hu et al., 2022).\n\nSingle neurons in early sensory areas of many nervous systems rapidly adjust to changes in input statistics by scaling their input-output gains (Adrian & Matthews, 1928). This allows neurons to adaptively normalize the variance of their outputs (Bonin et al., 2006;Nagel & Doupe, 2006), maximizing information transmitted about sensory inputs (Barlow, 1961;Laughlin, 1981;Fairhall et al., 2001). At the neural population level, in addition to variance normalization, adaptive decorrelation and whitening transformations have been observed across species and sensory modalities, including: macaque retina (Atick & Redlich, 1992); cat primary visual cortex (Muller et al., 1999;Benucci et al., 2013); and the olfactory bulbs of zebrafish (Friedrich, 2013) and mice (Giridhar et al., 2011;Gschwend et al., 2015). These population-level adaptations reduce redundancy in addition to normalizing neuronal outputs, facilitating dynamic efficient multi-channel coding (Schwartz & Simoncelli, 2001;Barlow & Foldiak, 1989). However, the mechanisms underlying such adaptive whitening transformations remain unknown, and would seem to require coordinated synaptic adjustments amongst neurons, as opposed to the single neuron case which relies only on gain rescaling.\n\nHere, we propose a novel recurrent network architecture for online statistical whitening that exclusively relies on gain modulation. Specifically, the primary contributions of our study are as follows:\n\n1. We introduce a novel factorization of the (inverse) whitening matrix, using an overcomplete, arbitrary, but fixed basis, and a diagonal matrix with statistically optimized entries. This is in contrast with the conventional factorization using the eigendecomposition of the input covariance matrix. Projection vectors {w1, w2, w3} \u2208 R 2 encode feedforward synaptic weights connecting primary neurons to interneuron i = 1, 2, 3, with symmetric feedback connections. Weight vectors are shown in the left and right panels with corresponding colors. In general, the network may require all-to-all connectivity between primary and interneurons; we use a reduced subset of connections here for diagram clarity. Inset: The i th interneuron (e.g. here i = 2) receives input zi = w \u22a4 i y, which is multiplied by its gain gi to produce output gizi. Its gain, gi, is adjusted s.t. \u2206gi \u221d z 2 i \u2212 1. The dark arrow indicates that the gain update operates on a slower time scale. Right: Scatter plots of the whitened network outputs y. Outputs have unit variance along all wi's, which is equivalent to having identity covariance matrix, i.e., Cyy = IN (black circle).\n\n2. We introduce an unsupervised online learning objective using this factorization to express the whitening objective solely in terms of the marginal variances within the overcomplete representation of the input signal.\n\n3. We derive a recursive algorithm to optimize the objective, and show that it corresponds to an unsupervised recurrent neural network (RNN), comprised of primary neurons and an auxiliary overcomplete population of interneurons, whose synaptic weights are fixed, but whose gains are adaptively modulated. The network responses converge to the classical symmetric whitening solution without backpropagation.\n\n4. We show how enforcing non-negativity on the gain modulation provides a novel approach for dealing with ill-conditioned or noisy data. Further, we relax the global whitening constraint in our objective and provide a method for local decorrelation of convolutional neural populations.\n\n\nA Novel Objective for Symmetric Whitening\n\nConsider a neural network with N primary neurons. For each t = 1, 2, . . . , let x t and y t be N -dimensional vectors whose components respectively denote the inputs (e.g. postsynaptic currents), and outputs of the primary neurons at time t (Figure 1). Without loss of generality, we assume the inputs x t are centered.\n\n\nConventional objective\n\nStatistical whitening aims to linearly transform inputs x t so that the covariance of the outputs y t is the identity, i.e.,\nC yy = \u27e8y t y \u22a4 t \u27e9 t = I N ,(1)\nwhere \u27e8\u00b7\u27e9 t denotes the expectation operator over t, and I N denotes the N \u00d7 N identity matrix (see Appendix A for a list of notation used in this work).\n\nIt is well known that whitening is not unique: any orthogonal rotation of a random vector with identity covariance matrix also has identity covariance matrix. There are several common methods of resolving this rotational ambiguity, each with their own advantages (Kessy et al., 2018). Here, we focus on the symmetric whitening transformation, often referred to as Zero-phase Component Analysis (ZCA) whitening or Mahalanobis whitening, which minimizes the mean-squared error between the inputs and the whitened outputs (alternatively, the one whose transformation matrix is symmetric). The symmetric whitened outputs are the optimal solution to the minimization problem\nmin {yt} \u27e8\u2225x t \u2212 y t \u2225 2 2 \u27e9 t s.t. \u27e8y t y \u22a4 t \u27e9 t = I N ,(2)\nwhere \u2225 \u00b7 \u2225 2 denotes the Euclidean norm on R N . Assuming the covariance of the inputs C xx := \u27e8x t x \u22a4 t \u27e9 t is positive definite, the unique solution to the optimization problem in Equation 2 is y t = C \u22121/2 xx x t for t = 1, 2, . . . , where C \u22121/2 xx is the symmetric inverse matrix square root of C xx (see Appendix B).\n\nPrevious approaches to online symmetric whitening have optimized Equation 2 by deriving RNNs whose synaptic weights adaptively adjust to learn the eigendecomposition of the (inverse) whitening matrix, C 1/2\nxx = V\u039b 1/2 V \u22a4 ,\nwhere V is an orthogonal matrix of eigenvectors and \u039b is a diagonal matrix of eigenvalues (Pehlevan & Chklovskii, 2015). We propose an entirely different decomposition: C\n1/2 xx = W diag (g) W \u22a4 + I N ,\nwhere W is a fixed overcomplete matrix of synaptic weights, and g is a vector of gains that adaptively adjust to match the whitening matrix.\n\n\nA novel objective using marginal statistics\n\nWe formulate an objective for learning the symmetric whitening transform via gain modulation. Our innovation exploits the fact that a random vector has identity covariance matrix (i.e., Equation 1 holds) if and only if it has unit marginal variance along all possible 1D projections (a form of tomography; see Related Work). We derive a tighter statement for a finite but overcomplete set of at least K \u2265 K N := N (N + 1)/2 distinct axes ('overcomplete' means that the number of axes exceeds the dimensionality of the input, i.e., K > N ). Intuitively, this equivalence holds because an N \u00d7 N symmetric matrix has K N degrees of freedom, so the marginal variances along K \u2265 K N distinct axes are sufficient to constrain an N \u00d7 N covariance matrix. We formalize this equivalence in the following proposition, whose proof is provided in Appendix C. Proposition 2.1. Fix K \u2265 K N . Suppose w 1 , . . . , w K \u2208 R N are unit vectors 1 such that\nspan({w 1 w \u22a4 1 , . . . , w K w \u22a4 K }) = S N ,(3)\nwhere S N denotes the K N -dimensional vector space of N \u00d7 N symmetric matrices. Then Equation 1 holds if and only if the projection of y t onto each unit vector w 1 , . . . , w K has unit variance, i.e.,\n\u27e8(w \u22a4 i y t ) 2 \u27e9 t = 1 for i = 1, . . . , K.(4)\nAssuming Equation 3 holds, we can interpret the set of vectors {w 1 , . . . , w K } as a frame (i.e., an overcomplete basis; Casazza et al., 2013) in R N such that the covariance of the outputs C yy can be computed from the variances of the Kdimensional projection of the outputs onto the set of frame vectors. Thus, we can replace the whitening constraint in Equation 2 with the equivalent marginal variance constraint to obtain the following objective:\nmin {yt} \u27e8\u2225x t \u2212 y t \u2225 2 2 \u27e9 t s.t. Equation 4 holds.(5)\n1 The unit-length assumption is imposed, without loss of generality, for notational convenience.\n\n\nAn RNN with Gain Modulation for Adaptive Symmetric Whitening\n\nIn this section, we derive an online algorithm for solving the optimization problem in Equation 5 and map the algorithm onto an RNN with adaptive gain modulation. Assume we have an overcomplete frame {w 1 , . . . , w K } in R N satisfying Equation 3. We concatenate the frame vectors into an N \u00d7 K synaptic weight matrix W := [w 1 , . . . , w K ].\n\nIn our network, primary neurons project onto a layer of K interneurons via the synaptic weight matrix to produce the K-dimensional vector z t := W \u22a4 y t , encoding the interneurons' post-synaptic inputs at time t (Figure 1). We emphasize that the synaptic weight matrix W remains fixed.\n\n\nEnforcing the marginal variance constraints with scalar gains\n\nWe introduce Lagrange multipliers g 1 , . . . , g K \u2208 R to enforce the K constraints in Equation 4. These are concatenated as the entries of a K-dimensional vector g := [g 1 , . . . , g K ] \u22a4 \u2208 R K , and express the whitening objective as a saddle point optimization:\nmax g min {yt} \u27e8\u2113(x t , y t , g)\u27e9 t ,(6)\nwhere \u2113(x, y, g)\n:= \u2225x \u2212 y\u2225 2 2 + K i=1 g i (w \u22a4 i y) 2 \u2212 1 .\nHere, we have exchanged the order of maximization over g and minimization over y t , which is justified because \u2113(x t , y t , g) satisfies the saddle point property with respect to y and g, see Appendix E.\n\nIn our RNN implementation, there are K interneurons and g i corresponds to the multiplicative gain associated with the i th interneuron, so that its output at time t is g i z i,t (Figure 1, Inset). Equation 6, shows that the gain of the i th interneuron, g i , encourages the marginal variance of y t along the axis spanned by w i to be unity. Importantly, the gains are not hyper-parameters, but rather they are optimization variables which statistically whiten the outputs {y t }, preventing the neural outputs from trivially matching the inputs {x t }.\n\n\nDeriving RNN neural dynamics and gain updates\n\nTo solve Equation 6 in the online setting, we assume there is a time-scale separation between 'fast' neural dynamics and 'slow' gain updates, so that at each time step the neural dynamics equilibrate before the gains are adjusted. This allows us to perform the inner minimization over {y t } before the outer maximization over the gains g. This is consistent with biological networks in which a given neuron's responses operate on a much faster time-scale than its intrinsic inputoutput gain, which is driven by slower processes such as changes in Ca 2+ concentration gradients and Na + -activated K + channels (Wang et al., 2003;Ferguson & Cardin, 2020).\n\n\nFAST NEURAL ACTIVITY DYNAMICS\n\nFor each time step t = 1, 2, . . . , we minimize the objective \u2113(x t , y t , g) over y t by recursively running gradient-descent steps to equilibrium:\ny t \u2190 y t \u2212 \u03b3 2 \u2207 y \u2113(x t , y t (\u03c4 ), g) = y t + \u03b3 {x t \u2212 W(g \u2022 z t ) \u2212 y t } ,(7)\nwhere \u03b3 > 0 is a small constant, z t = W \u22a4 y t , the circle '\u2022' denotes the Hadamard (element-wise) product, g \u2022 z t is a vector of K gain-modulated interneuron outputs, and we assume the primary cell outputs are initialized at zero.\n\nWe see from the right-hand-side of Equation 7 that the 'fast' dynamics of the primary neurons are driven by three terms (within the curly braces): 1) constant feedforward external input x t ; 2) recurrent gain-modulated feedback from interneurons \u2212W(g \u2022 z t ); and 3) a leak term \u2212y t . Because the neural activity dynamics are linear, we can analytically solve for their equilibrium (i.e. steady-state),\u0233 t , by setting the update in Equation 7 to zero:\ny t = I N + W diag (g) W \u22a4 \u22121 x t = I N + K i=1 g i w i w \u22a4 i \u22121 x t ,(8)\nwhere diag (g) denotes the K \u00d7 K diagonal matrix whose (i, i) th entry is g i , for i = 1, . . . , K. The equilibrium feedforward interneuron inputs are then given b\u0233\nz t = W \u22a4\u0233 t .(9)\nThe gain-modulated outputs of the K interneurons, g \u2022 z t , are then projected back onto the primary cells via symmetric weights, \u2212W ( Figure 1). After g adapts to optimize Equation 6 (provided Proposition 2.1 holds), the matrix within the brackets in Equation 8 will equal C 1/2 xx , and the circuit's equilibrium responses are symmetrically whitened. The result is a novel overcomplete symmetric matrix factorization in which W is arbitrary and fixed, while C 1/2 xx is adaptively learned and encoded in the gains g.\n\n\nSLOW GAIN DYNAMICS\n\nAfter the fast neural activities reach steady-state, the interneuron gains are updated with a stochastic gradient-ascent step with respect to g:\ng \u2190 g + \u03b7 2 \u2207 g \u2113(x t ,\u0233 t , g) = g + \u03b7 z \u20222 t \u2212 1 ,(10)\nwhere \u03b7 > 0 is the learning rate,z \u20222 t = [z 2 t,1 , . . . ,z 2 t,K ] \u22a4 , and 1 = [1, . . . , 1] \u22a4 is the K-dimensional vector of ones 2 . Remarkably, the update to the i th interneuron's gain g i (Equation 10) depends only on the online estimate of the variance of its equilibrium inputz 2 t,i , and its distance from 1 (i.e. the target variance). Since the interneurons adapt using local signals, this circuit is a suitable candidate for hardware implementations using low-power neuromorphic chips (Pehlevan & Chklovskii, 2019). Intuitively, each interneuron adjusts its gain to modulate the amount of suppressive (inhibitory) feedback onto the joint primary neuron responses. In Appendix D, we provide conditions under which g can be solved analytically. Thus, while statistical whitening inherently involves a transformation on a joint density, our solution operates solely using single neuron gain changes in response to marginal statistics of the joint density.\n\n\nONLINE UNSUPERVISED ALGORITHM\n\nBy combining Equations 7 and 10, we arrive at our online RNN algorithm for adaptive whitening via gain modulation (Algorithm 1). We also provide batched and offline versions of the algorithm in Appendix G.\n\nAlgorithm 1 Adaptive whitening via gain modulation\n1: Input: Centered inputs x 1 , x 2 , \u00b7 \u00b7 \u00b7 \u2208 R N 2: Initialize: W \u2208 R N \u00d7K ; g \u2208 R K ; \u03b7, \u03b3 > 0 3: for t = 1, 2, . . . do 4: y t \u2190 0 5: while not converged do 6: z t \u2190 W \u22a4 y t 7: y t \u2190 y t + \u03b3 {x t \u2212 W(g \u2022 z t ) \u2212 y t } 8: end while 9: g \u2190 g + \u03b7 z \u20222 t \u2212 1 10: end for\nThere are two points worth noting about this network: 1) W remains fixed in Algorithm 1. Instead, g adapts to statistically whiten the outputs. 2) In practice, since network dynamics are linear, we can bypass the inner loop (the fast dynamics of the primary cells, lines 5-8), by directly computing\u0233 t , andz t (Eqs. 8, 9).\n\n\nNumerical Experiments and Applications\n\nWe provide different applications of our adaptive symmetric whitening network via gain modulation, emphasizing that gain adaptation is distinct from, and complementary to, synaptic weight learning (i.e. learning W). We therefore side-step the goal of learning the frame W, and assume it is fixed (for example, through longer time scale learning). This allows us to decouple and analyze the general properties of our proposed gain modulation framework, independently of the choice of frame. Python code for this study can be located at github.com/lyndond/frame whitening.\n\nWe evaluate the performance of our adaptive whitening algorithm using the matrix operator norm, \u2225 \u00b7 \u2225 Op , which measures the largest eigenvalue,\nError := \u2225C yy \u2212 I N \u2225 Op .\nAs a performance criterion, we use \u2225C yy \u2212 I N \u2225 Op \u2264 0.1, the point at which the principal axes of C yy are within 0.1 of unity. Geometrically, this means the ellipsoid corresponding to the covariance matrix lies between the circles with radii 0.9 and 1.1.\n\nFor visualization of output covariance matrices, we plot 2D ellipses representing the 1-standard deviation probability level-set contour of the density. These ellipses are defined by the set of points {\u2225C 1/2 yy v\u2225v : \u2225v\u2225 = 1}.\n\n\nAdaptive symmetric whitening via gain modulation\n\nWe first demonstrate that our algorithm successfully whitens its outputs. We initialize a network with fixed interneuron weights, W, corresponding to the frame illustrated in Figure Figure 2 shows the network adapting to inputs from two successively-presented contexts with randomly-generated underlying input covariances C xx (10K gain update steps each). As update steps progress, all marginal variances converge to unity, as expected from the objective (top panel). Since the number of interneurons satisfies K=K N , the optimal gains to achieve symmetric whitening can be solved analytically (Appendix D), and are shown in the middle panel (dashed lines). Figure 2 illustrates the online, adaptive nature of the network; it whitens inputs from novel statistical contexts at run-time, without supervision. By Proposition 2.1, measuring unit variance along K N unique axes, as in this example, guarantees that the underlying joint density is statistically white. Indeed, the whitening error (bottom panel), approaches zero as all K N marginal variances approach 1. Thus, with interneurons monitoring their respective marginal input variances z 2 i , and re-scaling their gains to modulate feedback onto the primary neurons, the network adaptively whitens its outputs in each context.\n1 (N =2, K=K N =3).\n\nAlgorithmic convergence rate depends on W\n\nOur model assumes that the frame, W, is fixed and known (e.g., optimized via pre-training or development). This distinguishes our method from existing symmetric whitening methods, which typically operate by estimating and transforming to the eigenvector basis. By contrast, our network obviates learning the principal axes of the data altogether, and instead uses a statistical sampling approach along the fixed set of measurement axes spanned by W. While the result expressed in Proposition 2.1 is exact, and the optimal solution to the whitening objective Equation 5 is independent of W (provided Equation 3 holds), we hypothesize that the algorithmic convergence rate would depend on W. Figure 3 summarizes an experiment assessing the convergence rate of different networks whitening inputs with a random covariance, C xx , with N = 2 (the results are consistent when N > 2). We initialize three kinds of frames W \u2208 R N \u00d7K N with 100 repetitions each: 'Random', a frame with i.i.d. Gaussian entries; 'Optimized', a randomly initialized frame whose columns are then optimized to have minimum mutual coherence and cover the ambient space; and 'Spectral', a frame whose first N columns are the eigenvectors of the data and the remaining K N \u2212 N columns are zeros. For clarity, we remove the effects of input sampling stochasticity by running the offline version of our network, which assumes having direct access to the input covariance (Appendix G); the online version is qualitatively similar.\n\nWhen the input distribution is known, then using the input covariance eigenvectors, as with the Spectral frame, defines a bound on achievable performance, converging faster, on average, than the Random and Optimized frames (Figure 3A,B). This is because the frame is aligned with the input covariance's principal axes, and a simple gain scaling along those directions is sufficient to achieve a whitened response. We find that the networks with Optimized frames converge at similar rates to those with Spectral frames, despite the frame vectors not being aligned with the principal axes of the data ( Figure 3B). Comparing the Random to Optimized frames gives a better understanding of how one might choose a frame in the more realistic scenario when the input distribution is unknown. The networks with Optimized frames systematically converge faster than Random frames. Thus, when the input distribution is unknown, we empirically find that the convergence rate of Algorithm 1 benefits from a frame that is optimized to splay the ambient space. Increased coverage of the space by the frame vectors facilitates whitening with our gain re-scaling mechanism. Sec. 4.5 elaborates on how underlying signal structure can be exploited to inform more efficient choices of frames.\n\n\nImplicit sparse gating via gain modulation\n\nMotivated by the findings in Sec 4.2, and concepts from sparse coding (Olshausen & Field, 1996), we explore how adaptive gain modulation can complement or augment a 'pre-trained' network with context-dependent weights. Figure 4 shows an experiment using either a pre-trained Spectral, or Random W (N =6, K=K N =21) adaptively whitening inputs from two random, alternating statistical contexts, A and B, for 10K steps each. The first and second N columns of the Spectral frame are the eigenvectors of context A and B's covariance matrix, respectively, and the remaining elements are random i.i.d. Gaussian; the Random frame has all i.i.d. Gaussian elements. Figure 4 (top panel) shows that both networks successfully adapt to whiten the inputs from each context, with the Spectral frame converging faster than the Random frame (as in Sec 4.2).\n\nInspecting the Spectral frame's K interneuron gains during run-time (bottom panel) reveals that they sparsely 'select' the frame vectors corresponding to the eigenvectors of each respective condition (indicated by the blue/red intensity). This effect arises without a sparsity penalty or modifying the objective. Gain modulation thus sparsely gates contextdependent information without an explicit context signal. \n\n\nNormalizing ill-conditioned data\n\nFoundational work by Atick & Redlich (1992) showed that neural populations in the retina may encode visual inputs by optimizing mutual information in the presence of noise. For natural images with 1/f spectra, the optimal transform is approximately a product of a whitening filter and a low-pass filter. This is a particularly effective solution because when inputs are low-rank, C xx is ill-conditioned ( Figure 5A), and classical whitening leads to noise amplification along axes with small variance. In this section, we show how a simple modification to the objective allows our gain-modulating network to handle these types of inputs.\n\nWe prevent amplification of inputs below a certain variance threshold by replacing the unit marginal variance equality constraints with upper bound constraints 3 :\n\u27e8(w \u22a4 i y t ) 2 \u27e9 t \u2264 1 for i = 1, . . . , K.(11)\nOur modified network objective then becomes\nmin {yt} \u27e8\u2225x t \u2212 y t \u2225 2 2 \u27e9 t s.t. Equation 11 holds.(12)\nIntuitively, if the projected variance along a given direction is already less than or equal to unity, then it will not affect the overall loss. Interneuron gain should accordingly stop adjusting once the marginal variance along its projection axis is less than or equal to one. To enforce these upper Figure 5. Two networks (N =2, K=3, \u03b7=0.02) whitening illconditioned inputs. A: Outputs without whitening. 2D scatterplot of a non-Gaussian density whose underlying signal lies close to a latent 1D axis. Many points lie outside of the axis limits in this panel. Signal magnitude along that axis is denoted by the grayscale gradient. The 1-standard deviation covariance matrix is depicted as a black ellipse. Colored lines are axes spanned by Optimal frame (see Sec 4.2). B: Symmetric whitening boosts noise along the uninformative direction. C: Modulating gains according to Eq. 14 rescales the data without amplifying noise. D: Gains updated with Eq. 10 vs. Eq. 14. Colors correspond to frame axes in panels A-C.\n\nbound constraints, we introduce gains as Lagrange multipliers, but restrict the domain of g to be the non-negative orthant R K + , resulting in non-negative optimal gains:\nmax g\u2208R K + min {yt} \u27e8\u2113(x t , y t , g)\u27e9 t ,(13)\nwhere \u2113(x, y, g) is defined as in Equation 6. At each time step t, we optimize Equation 13 by first taking gradientdescent steps with respect to y t , resulting in the same neural dynamics (Equation 7) and equilibrium solution (Equation 8) as before. To update g, we modify Equation 10 to take a projected gradient-ascent step with respect to g:\ng \u2190 \u230ag + \u03b7(z \u20222 t \u2212 1)\u230b(14)\nwhere \u230a\u00b7\u230b denotes the element-wise half-wave rectification operation that projects its inputs onto the non-negative orthant R K + , i.e., \u230av\u230b := [max(v 1 , 0), . . . , max(v K , 0)] \u22a4 . Figure 5 shows a simulation of a network whitening ill-conditioned inputs with an Optimized frame (N =2, K=K N ; see Sec. 4.2) where gains are either unconstrained (Equation 10), or rectified (Equation 14). We observe that these two models converge to two different solutions (Figure 5B, C). When g i is unconstrained, the network achieves global whitening, as before, but in doing so it amplifies noise along the axis orthogonal to the latent signal axis. The gains constrained to be non-negative converged to different values than the unconstrained gains ( Figure 5D), with one of them (green) converging to zero rather than becoming negative. In general, with constrained g i , the whitening error network converges to a non-zero value (see Appendix H for details). Thus, with a non-negative constraint, the network normalizes the responses y, and does not amplify the noise. In Appendix H we show additional cases that provide further geometric intuition on differences between symmetric whitening with and without non-negative constrained gains.\n\n\nGain modulation enables local spatial decorrelation\n\nRequiring K N interneurons to guarantee a statistically white output (Proposition 2.1) becomes prohibitively costly for high-dimensional inputs: the number of interneurons scales as O(N 2 ). This leads us to ask: how many interneurons are needed in practice? For natural sensory inputs such as images, it is well-known that inter-pixel correlation is highly structured, decaying as a function of distance. We simulate an experiment of visual gaze fixations and micro-saccadic eye movements using a Gaussian random walk, drawing 12\u00d712 patch samples from a region of a natural image (Figure 6A; van Hateren & van der Schaaf, 1998); this can be interpreted as a form of video-streaming dataset where each frame is a patch sample. We repeat this for different randomly selected regions of the image (Figure 6A colors). The image content of each region is quite different, but the inter-pixel correlation within each context consistently falls rapidly with distance ( Figure 6B).\n\nWe relax the O(N 2 ) marginal variance constraint to instead whiten spatially local neighborhoods of primary neurons whose inputs are the image patches. We construct a frame W that exploits spatial structure in the image patches, and spans K < K N axes in R N . W is convolutional, such that overlapping neighborhoods of 4 \u00d7 4 primary neurons are decorrelated, each by a population of interneurons that is 'overcomplete' with respect to that neighborhood (see Appendix I for details). Importantly, taking into account local structure dramatically reduces the interneuron complexity from O(N 2 ) \u2192 O(N ), thereby making our framework practically feasible for high-resolution image inputs and video streams. This frame is still overcomplete (K > N ), but because K < K N , we no longer guarantee at equilibrium that C yy = I N (Proposition 2.1).\n\nAfter the network converges to the inputs drawn from the red context ( Figure 6C): i) inter-pixel correlations drop within the region specified by the local neighborhood; and ii) surprisingly, correlations at longer-range (i.e. outside the window of the defined spatial neighborhood) are also dramatically reduced. Accordingly, the eigenspectrum of the locally whitened outputs is significantly flatter compared to the inputs ( Figure 6D left vs. right columns). We also provide an example using 1D inputs in Appendix I. This empirical result is not obvious -that whitening individual overlapping local neighborhoods of neurons should produce a more globally whitened output covariance. Indeed, exactly how or when a globally whitened solution is possible from whitening of spatial overlapping neighborhoods of the inputs is a problem worth pursuing.\n\n\nRelated Work\n\n\nBiologically plausible whitening networks\n\nBiological circuits operate in the online setting and, due to physical constraints, must learn exclusively using local signals. Therefore, to plausibly model neural computation, a neural network model must operate in the online setting (i.e., streaming data) and use local learning rules (Pehlevan & Chklovskii, 2019). There are a few existing normative models of adaptive statistical whitening and related transformations; however, these models use synaptic plasticity mechanisms (i.e., changing W) to adapt to changing input statistics (Pehlevan & Chklovskii, 2015;Westrick et al., 2016;Chapochnikov et al., 2021;M\u0142ynarski & Hermundstad, 2021;Lipshutz et al., 2023). Adaptation of neural population responses to changes in sensory input statistics occurs rapidly, on the order of hundreds of milliseconds to seconds (Muller et al., 1999;Wanner & Friedrich, 2020), so it could potentially arise from short-term synaptic plasticity, which operates on the timescale of tens of milliseconds to minutes (Zucker & Regehr, 2002), but not by long-term synaptic plasticity, which operates on the timescale of minutes or longer (Martin et al., 2000). Here, we have proposed an alternative hypothesis: that modulation of neural gains, which operates on the order of tens of milliseconds to minutes (Ferguson & Cardin, 2020), facilitates rapid adaptation of neural populations to changing input statistics.\n\n\nTomography and \"sliced\" density measurements\n\nLeveraging 1D projections to compute the symmetric whitening transform is reminiscent of approaches taken in the field of tomography. Geometrically, our method represents an ellipsoid (i.e., the N dimensional covariance matrix) using noisy 1D projections of the ellipsoid onto axes spanned by frame vectors (i.e., estimates of the marginal variances). This is a special case of reconstruction problems studied in geometric tomography (Karl et al., 1994;Gardner, 1995). A distinction between tomography and our approach to symmetric whitening is that we are not reconstructing the multi-dimensional inputs; instead, we are utilizing the univariate measurements to transform an ellipsoid into a hyper-sphere.\n\nIn optimal transport, \"sliced\" methods offer a way to measure otherwise intractable p-Wasserstein distances in high dimensions (Bonneel et al., 2015), thereby enabling their use in optimization loss functions. Sliced methods estimate Wasserstein distance by taking series of 1D projections of two densities, then computing the expectation over all 1D Wasserstein distances, for which there exists an analytic so-lution. The 2-Wasserstein distance between a 1D zero-mean Gaussian with variance \u03c3 2 and a standard normal density is\nW 2 N 0, \u03c3 2 ; N (0, 1) = \u2225\u03c3 \u2212 1\u2225 .\nThis is strikingly similar to Equation 10. However, distinguishing characteristics of our approach include: 1) minimizing distance between variances rather than standard deviations; 2) directions along which we compute slices are fixed, whereas sliced methods compute a new set of projections at each optimization step; 3) our network operates online, without backpropagation.\n\n\nDiscussion\n\nOur study introduces a recurrent circuit for adaptive whitening using gain modulation to transform joint second-order statistics of their inputs based on marginal variance measurements. We demonstrate that, given sufficiently many marginal measurements along unique axes, the network produces symmetric whitened outputs. Our objective (Equation 5) provides a novel way to think about the classical problem of statistical whitening, and draws connections to old concepts from tomography and transport theory. This framework is flexible and extensible, with some possible generalizations explored in Appendix J. For example, we show that our model provides a way to prevent representational collapse in the analytically tractable example of online principal subspace learning (Appendix J.1). Additionally, by replacing the unity marginal variance constraint by a set of target variances differing from 1, the network can be used to transform its input density to one matching the corresponding (non-white) covariance (Appendix J.2).\n\n\nImplications for machine learning\n\nDecorrelation and whitening are canonical transformations in signal processing, widely used in compression and channel coding. Deep nets are generally not trained to whiten, although their response variances are generally normalized during training through batch normalization, and recent methods (e.g. Bardes et al., 2022) do impose global whitening properties in their objective functions. Modulating feature gains has proven effective in adapting pre-trained neural networks to novel inputs with out-of-training distribution statistics (Ball\u00e9 et al., 2020;Duong et al., 2023;Mohan et al., 2021). Future architectures may benefit from adaptive run-time adjustments to changing input statistics (e.g. Hu et al., 2022). Our framework provides an unsupervised, online mechanism that avoids 'catastrophic forgetting' in neural networks during continual learning.\n\n\nImplications for neuroscience\n\nIt has been known for nearly 100 years (Adrian & Matthews, 1928) that single neurons rapidly adjust their sensitivity (gain) adaptively, based on recent response history. Experiments suggest that neural populations jointly adapt, adjusting both the amplitude of their responses, as well as their correlations (e.g. Benucci et al., 2013;Friedrich, 2013) to confer dynamic, efficient multi-channel coding. The natural thought is that they achieve this by adjusting the strength of their interactions (synaptic weights). Our work provides a fundamentally different solution: these effects can arise solely through gain changes, thereby generalizing rapid and reversible single neuron adaptive gain modulation to the level of a neural population. Support for our model will ultimately require careful experimental measurement and analysis of responses and gains of neurons in a circuit during adaptation (e.g. Wanner & Friedrich, 2020). Our model predicts: 1) Specific architectural constraints, such as reciprocally connected interneurons (Kepecs & Fishell, 2014), with consistency between their connectivity and population size (e.g. in the olfactory bulb). 2) Synaptic strengths that remain stable during adaptation, which would adjudicate between our model and more conventional adaptation models relying on synaptic plasticity (e.g. Lipshutz et al., 2023). 3) Interneurons that modulate their gains according to the difference between the variance of their post-synaptic inputs and some target variance (Equation 10; also see Appendix J.2). Experiments could assess whether interneuron input variances converge to the same values after adaptive whitening. 4) Interneurons that increase their gains with the variance of their inputs (i.e.z 2 i,t ). Input variance-dependent gain modulation may be mediated by changes in slow Na + currents (Kim & Rieke, 2003). This predicts a mechanistic role for interneurons during adaptation, and complements the observed gain effects found in excitatory neurons described in classical studies (Fairhall et al., 2001;Nagel & Doupe, 2006).\n\n\nConclusion\n\nWhitening is an effective constraint for preventing feature collapse in representation learning (Zbontar et al., 2021;Ermolov et al., 2021). The networks developed here provide a whitening solution that is particularly well-suited for applications prioritizing streaming data and low-power consumption.\n\n\nA. Notation\n\nFor N \u2265 2, let K N := N (N + 1)/2. Let R N denote N -dimensional Euclidean space equipped with the Euclidean norm, denoted \u2225 \u00b7 \u2225 2 . Let R N + denote the non-negative orthant in R N . Given K \u2265 2, let R N \u00d7K denote the set of N \u00d7 K real-valued matrices. Let S N denote the set of N \u00d7 N symmetric matrices and let S N ++ denote the set of N \u00d7 N symmetric positive definite matrices.\n\nMatrices are denoted using bold uppercase letters (e.g., M) and vectors are denoted using bold lowercase letters (e.g., v). Given a matrix M, M ij denotes the entry of M located at the i th row and j th column. Let 1 = [1, . . . , 1] \u22a4 denote the N -dimensional vector of ones. Let I N denote the N \u00d7 N identity matrix.\n\nGiven vectors v, w \u2208 R N , define their Hadamard product by v \u2022 w := (v 1 w 1 \n, . . . , v N w N ) \u2208 R N . Define v \u20222 := (v 2 1 , . . . , v 2 N ) \u2208 R N . Let \u27e8\u00b7\u27e9 t denote expectation over t = 1, 2, . . . .\nThe diag (\u00b7) operator, similar to numpy.diag() or MATLAB's diag(), can either: 1) map a vector in R K to the diagonal of a K \u00d7 K zeros matrix; or 2) map the diagonal entries of a K \u00d7 K matrix to a vector in R K . The specific operation being used should be clear by context. For example, given a vector v \u2208 R K , define diag(v) to be the K \u00d7 K diagonal matrix whose (i, i) th entry is equal to v i , for i = 1, . . . , K. Alternatively, given a sqaure matrix M \u2208 R K\u00d7K , define diag(M) to be the K-dimensional vector whose i th entry is equal to M ii , for i = 1, . . . , K.\n\n\nB. Optimal Solution to Symmetric Whitening Objective\n\nIn this section, we prove that the optimal solution to the optimization problem in equation 2 is given by y t = C \u22121/2 xx x t for t = 1, . . . , T (we treat the case that T < \u221e).\n\nWe first recall Von Neumann's trace inequality (see, e.g., Carlsson, 2021, Theorem 3.1).\n\nLemma B.1 (Von Neumann's trace inequality). Suppose A, B \u2208 R n\u00d7m with n \u2264 m. Let \u03c3 A 1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3 A n \u2265 0 and \u03c3 B 1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3 B n \u2265 0 denote the respective singular values of A and B. Then\nTr(AB \u22a4 ) \u2264 n i=1 \u03c3 A i \u03c3 B i .\nFurthermore, equality holds if and only if A and B share left and right singular vectors.\n\nWe can now proceed with the proof of our result. We first concatenate the inputs and outputs into data matrices X = [x 1 , . . . , x T ] \u2208 R N \u00d7T and Y = [y 1 , . . . , y T ] \u2208 R N \u00d7T . We can write equation 2 as follows:\nmin Y \u2225X \u2212 Y\u2225 2 F subject to YY \u22a4 = T I N .\nExpanding, substituting in with the constraint YY \u22a4 = T I N and dropping terms that do not depend on Y results in the objective\nmax Y Tr(XY \u22a4 ) subject to YY \u22a4 = T I N .\nBy Von Neumann's trace inequality, the trace is maximized when the singular vectors of Y are aligned with the singular vectors of X. In particular, if the SVD of X is given by\nX = U x S x V \u22a4 x , then the optimal Y is given by Y = \u221a T U x V \u22a4 x , which is precisely C \u22121/2 xx X, where C xx := 1 T XX \u22a4 = U x S 2 x U \u22a4 x .\nC. Proof of Proposition 2.1\n\nProof of Proposition 2.1. Suppose Equation 1 holds. Then, for i = 1, . . . , K,\n\u27e8(w \u22a4 i y t ) 2 \u27e9 t = \u27e8w \u22a4 i y t y \u22a4 t w i \u27e9 t = w \u22a4 i w i = 1.\nTherefore, Equation 4 holds.\n\n\nNow suppose Equation 4 holds.\n\nLet v \u2208 R N be an arbitrary unit vector. Then vv \u22a4 \u2208 S N and by Equation 3, there exist g 1 , . . . , g K \u2208 R such that\nvv \u22a4 = g 1 w 1 w \u22a4 1 + \u00b7 \u00b7 \u00b7 + g K w K w \u22a4 K . (15) We have v \u22a4 \u27e8y t y \u22a4 t \u27e9 t v = Tr(vv \u22a4 \u27e8y t y \u22a4 t \u27e9 t ) = K i=1 g i Tr(w i w \u22a4 i \u27e8y t y \u22a4 t \u27e9 t ) = K i=1 g i Tr(w i w \u22a4 i ) = Tr(vv \u22a4 ) = 1.(16)\nThe first equality is a property of the trace operator. The second and fourth equalities follow from Equation 15  Recall that the optimal solution of the symmetric objective in Equation 5 is given by y t = C \u22121/2 xx x t for t = 1, 2, . . . . In our neural circuit with interneurons and gain control, the outputs of the primary neurons at equilibrium is (given in Equation 8, but repeated here for clarity),\u0233\nt = I N + W diag (g) W \u22a4 \u22121 x t ,\nwhere W \u2208 R N \u00d7K is overcomplete, arbitrary (provided Equation 3 holds), and fixed; and elements of g \u2208 R K can be interpreted as learnable scalar gains. The circuit performs symmetric whitening when the gains g satisfy the relation\nI N + W diag (g) W \u22a4 = C 1/2 xx .(17)\nIt is informative to contrast this with conventional approaches to symmetric whitening, which rely on eigendecompositions,\nV diag (\u03bb) 1/2 V \u22a4 = C 1/2 xx ,\nwhere V \u2208 R N \u00d7N and \u03bb are the eigenvectors and eigenvalues of C xx , respectively. Note that in this eigenvector formulation, both vector quantities (columns of V) and scalar quantities (elements of \u03bb) need to be learned, whereas in our formulation (Equation 17), only scalars need to be learned (elements of g).\n\nWhen K \u2265 N (N + 1)/2, we can explicitly solve for the optimal gains g * (derived in the next subsection):\ng * = W \u22a4 W \u20222 \u2020 w \u22a4 1 C 1/2 xx w 1 \u2212 1, . . . , w \u22a4 K C 1/2 xx w K \u2212 1 \u22a4 .(18)\n\nD.2. Isolating g embedded in a diagonal matrix\n\nIn the upcoming subsection, our variable of interest, g, is embedded along the diagonal of a matrix, then wedged between two fixed matrices, i.e. A 1 diag (g) A 2 . We employ the following identity to isolate g,\ndiag (A 1 diag (g) A 2 ) = A 1 \u2022 A \u22a4 2 g,(19)\nwhere, on the left-hand-side, the inner diag (\u00b7) forms a diagonal matrix from a vector, the outer diag (\u00b7) returns the diagonal of a matrix as a vector, and \u2022 is the element-wise Hadamard product.\n\n\nD.3. Deriving optimal gains\n\nLet C \u2208 S N , where S N is the set of symmetric N \u00d7 N matrices. Suppose g \u2208 R K is such that the following holds:\nW diag (g) W \u22a4 = C(20)\nwhere W \u2208 R N \u00d7K is some fixed, arbitrary, frame with K \u2265 N (N +1) 2 (i.e. a representation that is O(N 2 ) overcomplete). To solve for g, we multiply both sides of Equation 20 from the left and right by W \u22a4 and W, respectively, then take the diagonal 4 of the resultant matrices,\ndiag W \u22a4 W diag (g) W \u22a4 W = diag W \u22a4 CW .(21)\nFinally, employing the identity in Equation 19 yields\n(W \u22a4 W) \u20222 g = diag W \u22a4 CW ,(22)g = (W \u22a4 W) \u20222 \u2020 diag W \u22a4 CW ,(23)\nwhere (\u00b7) \u20222 denotes element-wise squaring, (W \u22a4 W) \u20222 is positive semidefinite by the Schur product theorem and (\u00b7) \u2020 denotes the Moore-Penrose pseudoinverse. Thus, any N \u00d7 N symmetric matrix, can be encoded as a vector, g, with respect to an arbitrary fixed frame, W, by solving a standard linear system of K equations of the form Ag = b. Importantly, when K = N (N +1) 2 and the columns of W are not collinear, we have empirically found the matrix on the LHS, (W \u22a4 W) \u20222 , to be positive definite, so the vector g is uniquely defined.\n\nWithout loss of generality, assume that the columns of W are unit-norm (otherwise, we can always normalize them by absorbing their lengths into the elements of g). Furthermore, assume without loss of generality that C \u2208 S N ++ , the set of all symmetric positive definite matrices (e.g. covariance, precision, PSD square roots, etc.). When C is a covariance matrix, then diag W \u22a4 CW can be interpreted as a vector of projected variances of C along each axis spanned by W. Therefore, Equation 22 states that the vector g is linearly related to the vector of projected variances via the element-wise squared frame Gramian, (W \u22a4 W) \u20222 .\n\n\nE. Saddle Point Property\n\nIn this section, we prove the following minimax property (for the case t = 1, . . . , T with T finite):\nmin {yt} max g \u27e8\u2113(x t , y t , g)\u27e9 t = max g min {yt} \u27e8\u2113(x t , y t , g)\u27e9 t .(24)\nThe proof relies on the following minimax property for a function that satisfies the saddle point property (Boyd & Vandenberghe, 2004, section 5.4).\n\nthreshold are unaffected by the whitening transform, we quantify algorithmic performance using thresholded Spectral Error,\nSpectral Error := 1 N N i max(\u03bb i \u2212 1, 0) 2 ,\nwhere \u03bb i is the i th eigenvalue of C yy . Here, as in the main text, we set the threshold to 1. Figure 7 shows that this network reduces spectral error. Importantly, the converged solution depends on the initial choice of frame (see next subsection). \n\n\nH.2. Geometric intuition behind thresholded whitening with non-negative gains\n\nIn general, the modified objective with rectified gains (Equation 14) does not statistically whiten the inputs x 1 , x 2 , . . . , but rather adapts the non-negative gains g 1 , . . . , g K to ensure that the variances of the outputs y 1 , y 2 , . . . in the directions spanned by the frame vectors {w 1 , . . . , w K } are bounded above by unity (Figure 8). This one-sided normalization carries interesting implications for how and when the circuit statistically whitens its outputs, which can be compared with experimental observations. For instance, the circuit performs symmetric whitening if and only if there are non-negative gains such that Equation 17 holds (see, e.g., the top right example in Figure 8), which corresponds to cases such that the matrix C 1/2 xx is an element of the following cone (with its vertex translated by I N ):\nI N + K i=1 g i w i w \u22a4 i : g \u2208 R K + .\nOn the other hand, if the variance of an input projection is less than unity -i.e., w \u22a4 i C xx w i \u2264 1 for some i -then the corresponding gain g i remains zero. When this is true for all i = 1, . . . , K, the gains all remain zero and the circuit output is equal to its input (see, e.g., the bottom middle panel of Figure 8).\n\n\nI. Whitening Spatially Local Neighborhoods\n\n\nI.1. Spatially local whitening in 1D\n\nFor an N -dimensional input, we consider a network that whitens spatially local neighborhoods of size M < N . To this end, we can construct N filters of the form w i = e i , i = 1, . . . , N Figure 8. Geometric intuition of whitening with/without inequality constraint. Whitening efficacy using non-negative gains depends on W and Cxx. For N = 2 and K = 3, examples of covariance matrices Cyy (red ellipses) corresponding to optimal solutions y of objective 12, for varying input covariance matrices Cxx (black ellipses) and frames W (spanning axes denoted by gray lines). Unit circles, which correspond to the identity matrix target covariance, are shown with dashed lines. Each row corresponds to a different frame W and each column corresponds to a different input covariance Cxx.\n\nand M (N \u2212 M +1 2 ) filters of the form\nw ij = e i + e j \u221a 2 , i, j = 1, . . . , N, 1 \u2264 |i \u2212 j| \u2264 M.\nThe total number of filters is (M + 1)(N \u2212 M 2 ), so for fixed M the number of filters scales linearly in N rather than quadratically.\n\nWe simulated a network comprising N = 10 primary neurons, and a convolutional weight matrix connecting each interneuron to spatial neighborhoods of three primary neurons. Given input data with covariance C xx illustrated in Figure 9A (left panel), this modified network succeeded to statistically whiten local neighborhoods of size of primary 3 neurons (right panel). Notably, the eigenspectrum ( Figure 9B) after local whitening is much closer to being equalized. Furthermore, while the global whitening solution produced a flat spectrum as expected, the local whitening network did not amplify the axis with very low-magnitude eigenvalues ( Figure 9B right panel).\n\n\nI.2. Filter bank construction in 2D\n\nHere, we describe one way of constructing a set of convolutional weights for overlapping spatial neighborhoods (e.g. image patches) of neurons. Given an n \u00d7 m input and overlapping neighborhoods of size h \u00d7 w to be statistically whitened, the samples are therefore matrices X \u2208 R n\u00d7m . In this case, filters w \u2208 R 1\u00d7n\u00d7m can be indexed by pairs of pixels that are in the same patch:\n((i, j), (k, \u2113)), 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m, 0 \u2264 |i \u2212 k| \u2264 h, 0 \u2264 |j \u2212 \u2113| \u2264 w\nWe can then construct the filters as, w (i,j),(k,\u2113) (X) = x i,j if (i, j) = (k, \u2113), xi,j +x k,\u2113 \u221a 2 if (i, j) \u0338 = (k, \u2113).\n\nIn this case there are nm + wh (n \u2212 w)(m \u2212 h) + (n \u2212 w) (h + 1) 2 + (m \u2212 h) (w + 1) 2 + (h + 1) (w + 1) 2 such filters, so the number of filters required scales linearly with nm rather than quadratically.\n\nTo measure the online subspace learning performance, we define\nSubspace error := V V \u22a4 V \u22121 V \u22a4 \u2212 diag ([1, 1, 0]) 2 Frob , V := [v 1 , v 2 ] \u2208 R 3\u00d72\nFigure 10 (blue) shows that our adaptive whitening algorithm with gain modulation successfully facilitates subspace learning and prevents representational collapse. Figure 10. Adaptive symmetric whitening with gain modulation prevents representational collapse during online principal subspace learning. Without whitening, subspace error stabilizes at a non-zero value, indicating that the network has converged to a collapsed representation. Shaded curves are median and [25%, 75%] quantiles over 50 random intializations.\n\n\nJ.2. Generalized adaptive covariance transformations\n\nOur framework for adaptive whitening via gain modulation can easily be generalized to adaptively transform a signal with some initial covariance matrix to one with any target covariance (i.e. not just the identity matrix). This demonstrates that our adaptive gain modulation framework has implications beyond statistical whitening. This could, for example, allow online systems to stably maintain some initial/target (non-white) output covariance under changing input statistics (i.e. covariance homeostasis, Westrick et al., 2016;Benucci et al., 2013). The key insight, similar to the main text, is that a full-rank covariance matrix has K N degrees of freedom, and therefore marginal measurements along K N distinct axes is necessary and sufficient to represent the matrix (Karl et al., 1994).\n\nLet C target be some arbitrary target covariance matrix. Then the general objective is\nmin {yt} \u27e8\u2225x t \u2212 y t \u2225 2 2 \u27e9 t s.t. \u27e8y t y \u22a4 t \u27e9 t = C target .(26)\nFollowing the same logic as in the main text, the Lagrangian becomes\nmax g min {yt} \u27e8\u2113(x t , y t , g)\u27e9 t ,(27)\nwhere \u2113(x, y, g) := \u2225x \u2212 y\u2225 2 2 +\nK i=1 g i (w \u22a4 i y) 2 \u2212 \u03c3 2 i ,\nwhere \u03c3 2 i = w \u22a4 i C target w i is the marginal variance along the axis spanned by w i . When C target = I N , then \u03c3 2 i = 1 for all i, and this reduces to our original overcomplete whitening objective (Equation 5). The only difference in the recursive algorithm optimizing this generalized objective is the gain update rule,\ng i \u2190 g i + \u03b7 2 \u2207 gi \u2113(x t ,\u0233 t , g) = g i + \u03b7 z 2 i,t \u2212 \u03c3 2 i .(28)\nWe can interpret this formulation as each interneuron having a pre-determined target input variance (perhaps learned over long time-scales), and adjusting its gains to modulate the joint responses of the primary neurons until its input variance matches the target.\n\nFigure 1 .\n1Schematic of a recurrent statistical whitening network with 2 primary neurons and 3 interneurons. Left: 2D Scatter plot of network inputs x = [x1, x2] \u22a4 (e.g. post-synaptic currents), with covariance indicated by the ellipse. Center: Primary neurons, with outputs y = [y1, y2] \u22a4 , receive external feedforward inputs, x, and recurrent feedback from an overcomplete population of interneurons, \u2212 3 i=1 giziwi.\n\nFigure 2 .\n2Network fromFigure 1(with corresponding colors; N =2, K=KN =3, \u03b7=2E-3) adaptively whitening samples from two randomly generated statistical contexts online (10K steps each). Top: Marginal variances measured by interneurons approach 1 over time. Middle: Dynamics of interneuron gains, which are applied to zi before feeding back onto the primary cells. Dashed lines are optimal gains (Appendix D). Bottom: Error over time, as measured by the maximal difference between the standard deviation along the principal axes of Cyy and unity.\n\nFigure 3 .\n3Convergence rate depends on structure of W. For each network, \u03b7=1E-2. A: Error over time. Curves are median and [25%, 75%] quantile regions over 100 repeats. Dashed line indicates when the principal axes of 1-standard deviation ellipse representing Cyy are within 0.1 of unity. B: Scatter plots and covariance ellipses of y for a single experiment with each frame type at different steps. Gray dashed lines are axes spanned by W.\n\nFigure 4 .\n4Gain modulation as a fast implicit sparse gating mechanism. Top: Error over time for Spectral vs. Random networks (N =6; K=KN =21; \u03b7=1E-3) adapting to 2 alternating statistical contexts with different input covariances. Dashed line indicates when the principal axes of 1-standard deviation ellipsoid representing Cyy are within 0.1 of unity. Bottom: Gains act as implicit context switches, sparsely gating the respective eigenbases embedded in the Spectral frame to optimally whiten each context.\n\nFigure 6 .\n6Local spatial whitening. A) Large grayscale image from which 12\u00d712 image patch samples are drawn. Colors represent random-walk sampling from regions of the image corresponding to contexts with different underlying statistics. Six samples from each context are shown below. B) Without whitening, pixel correlations decay rapidly with spatial distance in each context, suggesting that local whitening may be effective. C) Binned pairwise output pixel correlation of patches from the red context before (gray) and after global (black dots) vs. local whitening with overlapping 4\u00d74 neighborhoods (red). Shaded regions represent standard deviations. D) Top: Correlation matrices of flattened patches from the red context before whitening (left), and after local symmetric whitening (right). Both panels use the same color scale. Bottom: Corresponding covariance eigenspectra. Dashed lines are spectra after global whitening.\n\n\nand the linearity of the trace operator. The third equality follows from Equation 4, the cyclic property of the trace, and the fact that each w i is a unit vector. The final equality holds because v is a unit vector. Since Equation 16 holds for every unit vector v \u2208 R N , Equation 1 holds. D. Frame Factorizations of Symmetric Matrices D.1. Analytic solution for the optimal gains\n\nFigure 7 .\n7Whitening ill-conditioned inputs with non-negative gains. A) An equi-angular frame (red, blue, green; see Sec. 4.2) whitening ill-conditioned inputs. B) Gains as algorithm progresses, using updates with either rectified or unrectified constraints. C) Spectral Error (see text).\nAppendix F generalizes the gain update to allowing for temporal-weighted averaging of the variance over past samples.\nWe set the threshold to 1 to remain consistent with the whitening objective, but it can be any arbitrary variance.\nSimilar to commonly-used matrix libraries, the diag (\u00b7) operator here is overloaded and can map a vector to a matrix or vice versa. See Appendix A for details.\nAcknowledgementsWe thank Pierre-\u00c9tienne Fiquet, Sina Tootoonian, Michael Landy, and the ICML reviewers for their helpful feedback.Theorem E.1. Let V \u2286 R n , W \u2286 R m and f : V \u00d7 W \u2192 R. Suppose f satisfies the saddle point property; that is, there exists (a * , b * ) \u2208 V \u00d7 W such thatfor all (a, b) \u2208 V \u00d7 W. In view of Theorem E.1, it suffices to show there exists (y * 1 , . . . , y * T , g * ) such that \u2113(y * 1 , . . . , y * T , g) \u2264 \u2113(y * 1 , . . . , y * T , g * ) \u2264 \u2113(y 1 , . . . , y T , g * ), for all y 1 , . . . , y T \u2208 R N and g \u2208 R K .Define y * t := C \u22121/2 xx x t for all t = 1, . . . , T and define g * as in equation 18 so that equation 17 holds. Then, for all g \u2208 R K ,Therefore, the first inequality in equation 25 holds (in fact it is an equality for all g). Next, we haveSince C 1/2 xx is positive definite, \u2113(y 1 , . . . , y T , g * ) is strictly convex in (y 1 , . . . , y T ) with its unique minimum obtained at y t = C \u22121/2 xx x t for all t = 1, . . . , T (to see this, differentiate with respect to y 1 , . . . , y T , set the derivatives equal to zero and solve for y 1 , . . . , y T ). This establishes the second inequality in equation 25 holds. Therefore, by Theorem E.1, equation 24 holds.F. Weighted Average Update Rule for gThe update for g inEquation 10can be generalized to allow for a weighted average over past samples. In particular, the general update is given bywhere \u03b3 \u2208 [0, 1] determines the decay rate and Z := 1 + \u03b3 + \u00b7 \u00b7 \u00b7 + \u03b3 t\u22121 is a normalizing factor.G. Batched and Offline Algorithms for Whitening with RNNs via Gain ModulationIn addition to the fully-online algorithm provided in the main text (Algorithm 1), we also provide two variants below. In many applications, streaming inputs arrive in batches rather than one at a time (e.g. video streaming frames). Similarly for conventional offline stochastic gradient descent training, data is sampled in batches. Algorithm 2 would be one way to accomplish this in our framework, where the main difference between the fully online version is taking the mean across samples in the batch to yield average gain update \u2206g term. Furthermore, in the fully offline setting when the covariance of the inputs, C xx is known, Algorithm 3 presents a way to whiten the covariance directly.Algorithm 2 Batched symmetric whitening 1: Input: Data matrix X \u2208 R N \u00d7T (centered) 2: Initialize: W \u2208 R N \u00d7K ; g \u2208 R K ; \u03b7; batch size B 3: while not converged do 4:g \u2190 g + \u03b7 mean(\u2206g, axis=1) 9: end while Algorithm 3 Offline symmetric whitening 1: Input: Input covariance C xx 2: Initialize: W \u2208 R N \u00d7K ; g \u2208 R K ; \u03b7 3: while not converged do 4: Whitening with non-negative gains does not, in general, produce an output with identity covariance matrix; therefore, quantifying algorithm performance with the error defined in the main text would not be informative. Because this extension shares similarities with ideas of regularized whitening, in which principal axes whose eigenvalues are below a certain Here, similar toLipshutz et al. (2023), we show how whitening can prevent representational collapse using the analytically tractable example of online principal subspace learning. Recent approaches to self-supervised learning have used decorrelation transforms such as whitening to prevent collapse during training (e.g.Zbontar et al., 2021). Future architectures may benefit from online, adaptive whitening to allow for continual learning and test-time adaptation.Consider a primary neuron whose pre-synaptic input at time t is s t \u2208 R D , and corresponding output is y t := v \u22a4 s t , where v \u2208 R D are the synaptic weights connecting the inputs to the neuron. An online variant of power iteration algorithm learns the top principal component of the inputs by updating the vector v as follows:Next, consider a population of 2 \u2264 N \u2264 D primary neurons with outputs y t \u2208 R N and feedforward synaptic weight vectors v 1 , . . . , v N \u2208 R D connecting the pre-synaptic inputs s t to the N neurons. Running N parallel instances of the power iteration algorithm defined above without a decorrelation process results in representational collapse, because each synaptic weight vector v i converges to the top principal component(Figure 10, orange). We demonstrate that our whitening algorithm via gain modulation readily solves this problem. Here, it is important that the whitening happen on a faster timescale than the principal subspace learning, to avoid collapse (seeLipshutz et al., 2023, for details).For this simulation, we set D = 3, N = 2 and randomly sample i.i.d. pre-synaptic inputs s t \u223c N (0, diag(5, 2, 1)). We randomly initialize two vectors v 1 , v 2 \u2208 R 3 with i.i.d. Gaussian entries. At each time step t, we project pre-synaptic inputs to form the post-synaptic primary neuron inputs, x t := v \u22a4 1 s t , v \u22a4 2 s t \u22a4 , forming the input to Algorithm 1. Let y t be the primary neuron steady-state output; that is, y t = I N + W diag (g) W \u22a4 \u22121 x t (Equation 8). For i = 1, 2, we update v i according to the above-defined update rules, with \u03b6 = 10 \u22123 . We update the gains g according to Algorithm 1 with \u03b7 = 10\u03b6.\nThe action of light on the eye: Part III. The interaction of retinal neurones. E D Adrian, R Matthews, The Journal of Physiology. 653273Adrian, E. D. and Matthews, R. The action of light on the eye: Part III. The interaction of retinal neurones. The Journal of Physiology, 65(3):273, 1928.\n\nWhat does the retina know about natural scenes?. J J Atick, A N Redlich, Neural Computation. 4Atick, J. J. and Redlich, A. N. What does the retina know about natural scenes? Neural Computation, 4:196-210, 1992.\n\nNonlinear transform coding. J Ball\u00e9, P A Chou, D Minnen, S Singh, N Johnston, E Agustsson, S J Hwang, G Toderici, IEEE Journal of Selected Topics in Signal Processing. 152Ball\u00e9, J., Chou, P. A., Minnen, D., Singh, S., Johnston, N., Agustsson, E., Hwang, S. J., and Toderici, G. Nonlinear transform coding. IEEE Journal of Selected Topics in Signal Processing, 15(2):339-353, 2020.\n\nVICReg: Varianceinvariance-covariance regularization for self-supervised learning. A Bardes, J Ponce, Y Lecun, International Conference on Learning Representations. Bardes, A., Ponce, J., and LeCun, Y. VICReg: Variance- invariance-covariance regularization for self-supervised learning. International Conference on Learning Repre- sentations, 2022.\n\nPossible Principles Underlying the Transformations of Sensory Messages. H B Barlow, Sensory Communication. The MIT PressBarlow, H. B. Possible Principles Underlying the Transfor- mations of Sensory Messages. In Sensory Communica- tion, pp. 216-234. The MIT Press, 1961.\n\nAdaptation and decorrelation in the cortex. H B Barlow, P Foldiak, The Computing Neuron. Addison-WesleyBarlow, H. B. and Foldiak, P. Adaptation and decorrelation in the cortex. In The Computing Neuron, pp. 54-72. Addison-Wesley, 1989.\n\nThe \"independent components\" of natural scenes are edge filters. A J Bell, T J Sejnowski, Vision Research. 37Bell, A. J. and Sejnowski, T. J. The \"independent compo- nents\" of natural scenes are edge filters. Vision Research, 37:3327-3338, 1996.\n\nAdaptation maintains population homeostasis in primary visual cortex. A Benucci, A B Saleem, M Carandini, Nature Neuroscience. 166Benucci, A., Saleem, A. B., and Carandini, M. Adapta- tion maintains population homeostasis in primary visual cortex. Nature Neuroscience, 16(6):724-729, 2013.\n\nThe statistical computation underlying contrast gain control. V Bonin, V Mante, M Carandini, The Journal of Neuroscience. 2623Bonin, V., Mante, V., and Carandini, M. The statistical com- putation underlying contrast gain control. The Journal of Neuroscience, 26(23):6346-6353, 2006.\n\nSliced and Radon Wasserstein Barycenters of Measures. N Bonneel, J Rabin, G Peyr\u00e9, H Pfister, Journal of Mathematical Imaging and Vision. 511Bonneel, N., Rabin, J., Peyr\u00e9, G., and Pfister, H. Sliced and Radon Wasserstein Barycenters of Measures. Journal of Mathematical Imaging and Vision, 51(1):22-45, January 2015.\n\nConvex Optimization. S Boyd, L Vandenberghe, Cambridge University PressBoyd, S. and Vandenberghe, L. Convex Optimization. Cam- bridge University Press, 2004.\n\nvon neumann's trace inequality for Hilbert-Schmidt operators. M Carlsson, Expositiones Mathematicae. 391Carlsson, M. von neumann's trace inequality for Hilbert- Schmidt operators. Expositiones Mathematicae, 39(1): 149-157, 2021.\n\nIntroduction to Finite Frame Theory. P G Casazza, G Kutyniok, F Philipp, Finite Frames. Casazza, P. G. and Kutyniok, G.Birkh\u00e4user BostonCasazza, P. G., Kutyniok, G., and Philipp, F. Introduction to Finite Frame Theory. In Casazza, P. G. and Kutyniok, G. (eds.), Finite Frames, pp. 1-53. Birkh\u00e4user Boston, 2013.\n\nNormative and mechanistic model of an adaptive circuit for efficient encoding and feature extraction. bioRxiv. N M Chapochnikov, C Pehlevan, D B Chklovskii, Chapochnikov, N. M., Pehlevan, C., and Chklovskii, D. B. Normative and mechanistic model of an adaptive circuit for efficient encoding and feature extraction. bioRxiv, 2021.\n\nAn analysis of single-layer networks in unsupervised feature learning. A Coates, A Ng, H Lee, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and StatisticsJMLR Workshop and Conference ProceedingsCoates, A., Ng, A., and Lee, H. An analysis of single-layer networks in unsupervised feature learning. In Proceed- ings of the Fourteenth International Conference on Ar- tificial Intelligence and Statistics, pp. 215-223. JMLR Workshop and Conference Proceedings, 2011.\n\nMulti-rate adaptive transform coding for video compression. L R Duong, B Li, C Chen, J Han, Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. the IEEE International Conference on Acoustics, Speech and Signal ProcessingDuong, L. R., Li, B., Chen, C., and Han, J. Multi-rate adap- tive transform coding for video compression. Proceed- ings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2023.\n\nWhitening for self-supervised representation learning. A Ermolov, A Siarohin, E Sangineto, N Sebe, International Conference on Machine Learning. PMLRErmolov, A., Siarohin, A., Sangineto, E., and Sebe, N. Whitening for self-supervised representation learning. In International Conference on Machine Learning, pp. 3015- 3024. PMLR, 2021.\n\nEfficiency and ambiguity in an adaptive neural code. A L Fairhall, G D Lewen, W Bialek, Nature. 412Fairhall, A. L., Lewen, G. D., and Bialek, W. Efficiency and ambiguity in an adaptive neural code. Nature, 412: 787-792, 2001.\n\nMechanisms underlying gain modulation in the cortex. K A Ferguson, J A Cardin, Nature Reviews Neuroscience. 212Ferguson, K. A. and Cardin, J. A. Mechanisms underlying gain modulation in the cortex. Nature Reviews Neuro- science, 21(2):80-92, 2020.\n\nNeuronal computations in the olfactory system of zebrafish. R W Friedrich, Annual Review of Neuroscience. 36Friedrich, R. W. Neuronal computations in the olfactory system of zebrafish. Annual Review of Neuroscience, 36: 383-402, 2013.\n\nCambridge University Press Cambridge. R J Gardner, Tomography, 58Gardner, R. J. Geometric Tomography, volume 58. Cam- bridge University Press Cambridge, 1995.\n\nTimescaledependent shaping of correlation by olfactory bulb lateral inhibition. S Giridhar, B Doiron, N N Urban, Proceedings of the National Academy of Sciences. 10814Giridhar, S., Doiron, B., and Urban, N. N. Timescale- dependent shaping of correlation by olfactory bulb lateral inhibition. Proceedings of the National Academy of Sci- ences, 108(14):5843-5848, 2011.\n\nNeuronal pattern separation in the olfactory bulb improves odor discrimination learning. O Gschwend, N M Abraham, S Lagier, F Begnaud, I Rodriguez, Carleton , A , Nature Neuroscience. 1810Gschwend, O., Abraham, N. M., Lagier, S., Begnaud, F., Rodriguez, I., and Carleton, A. Neuronal pattern separa- tion in the olfactory bulb improves odor discrimination learning. Nature Neuroscience, 18(10):1474-1482, 2015.\n\nE J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, Chen , W Lora, Low-rank adaptation of large language models. International Conference on Learning Representations. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. International Conference on Learning Representations, 2022.\n\nOn feature decorrelation in self-supervised learning. T Hua, W Wang, Z Xue, S Ren, Y Wang, H Zhao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionHua, T., Wang, W., Xue, Z., Ren, S., Wang, Y., and Zhao, H. On feature decorrelation in self-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9598-9608, 2021.\n\nIndependent component analysis: algorithms and applications. A Hyv\u00e4rinen, E Oja, Neural Networks. 134-5Hyv\u00e4rinen, A. and Oja, E. Independent component analysis: algorithms and applications. Neural Networks, 13(4-5): 411-430, 2000.\n\nW C Karl, G C Verghese, A S Willsky, Reconstructing Ellipsoids from Projections. CVGIP: Graphical Models and Image Processing. 56Karl, W. C., Verghese, G. C., and Willsky, A. S. Recon- structing Ellipsoids from Projections. CVGIP: Graphical Models and Image Processing, 56(2):124-139, 1994.\n\nInterneuron cell types are fit to function. A Kepecs, G Fishell, Nature. 505Kepecs, A. and Fishell, G. Interneuron cell types are fit to function. Nature, 505:318-326, 2014.\n\nOptimal whitening and decorrelation. A Kessy, A Lewin, K Strimmer, The American Statistician. 724Kessy, A., Lewin, A., and Strimmer, K. Optimal whitening and decorrelation. The American Statistician, 72(4):309- 314, 2018.\n\nSlow Na + inactivation and variance adaptation in salamander retinal ganglion cells. K J Kim, F Rieke, Journal of Neuroscience. 234Kim, K. J. and Rieke, F. Slow Na + inactivation and variance adaptation in salamander retinal ganglion cells. Journal of Neuroscience, 23(4):1506-1516, 2003.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, University of TorontoMaster's thesisKrizhevsky, A. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009.\n\nCoding Procedure Enhances a Neuron's Information Capacity. Zeitschrift fur Naturforschung. C. S Laughlin, Simple, Journal of Biosciences. Laughlin, S. A Simple Coding Procedure Enhances a Neuron's Information Capacity. Zeitschrift fur Natur- forschung. C, Journal of Biosciences, pp. 910-2, 1981.\n\nInterneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation. D Lipshutz, C Pehlevan, D B Chklovskii, International Conference on Learning Representations. Lipshutz, D., Pehlevan, C., and Chklovskii, D. B. Interneu- rons accelerate learning dynamics in recurrent neural net- works for statistical adaptation. International Conference on Learning Representations, 2023.\n\nSynaptic plasticity and memory: an evaluation of the hypothesis. S Martin, P D Grimwood, R G Morris, Annual Review of Neuroscience. 231Martin, S., Grimwood, P. D., and Morris, R. G. Synaptic plasticity and memory: an evaluation of the hypothesis. Annual Review of Neuroscience, 23(1):649-711, 2000.\n\nAdaptive denoising via gaintuning. S Mohan, J L Vincent, R Manzorro, P Crozier, C Fernandez-Granda, E Simoncelli, Advances in Neural Information Processing Systems. 34Mohan, S., Vincent, J. L., Manzorro, R., Crozier, P., Fernandez-Granda, C., and Simoncelli, E. Adaptive de- noising via gaintuning. Advances in Neural Information Processing Systems, 34:23727-23740, 2021.\n\nRapid adaptation in visual cortex to the structure of images. J R Muller, A B Metha, J Krauskopf, Lennie , P , Science. 2855432Muller, J. R., Metha, A. B., Krauskopf, J., and Lennie, P. Rapid adaptation in visual cortex to the structure of images. Science, 285(5432):1405-1408, 1999.\n\nEfficient and adaptive sensory codes. W F M\u0142ynarski, A M Hermundstad, Nature Neuroscience. 247M\u0142ynarski, W. F. and Hermundstad, A. M. Efficient and adaptive sensory codes. Nature Neuroscience, 24(7):998- 1009, 2021.\n\nTemporal Processing and Adaptation in the Songbird Auditory Forebrain. K I Nagel, A J Doupe, Neuron. 516Nagel, K. I. and Doupe, A. J. Temporal Processing and Adaptation in the Songbird Auditory Forebrain. Neuron, 51(6):845-859, 2006.\n\nEmergence of simple-cell receptive field properties by learning a sparse code for natural images. B Olshausen, D Field, Nature. 381Olshausen, B. and Field, D. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381:607-609, 1996.\n\nA normative theory of adaptive dimensionality reduction in neural networks. C Pehlevan, D B Chklovskii, Advances in Neural Information Processing Systems. 28Pehlevan, C. and Chklovskii, D. B. A normative theory of adaptive dimensionality reduction in neural networks. Advances in Neural Information Processing Systems, 28, 2015.\n\nNeuroscience-Inspired Online Unsupervised Learning Algorithms: Artificial Neural Networks. C Pehlevan, D B Chklovskii, IEEE Signal Processing Magazine. 366Pehlevan, C. and Chklovskii, D. B. Neuroscience-Inspired Online Unsupervised Learning Algorithms: Artificial Neural Networks. IEEE Signal Processing Magazine, 36 (6):88-96, 2019.\n\nNatural signal statistics and sensory gain control. O Schwartz, E P Simoncelli, Nature Neuroscience. 48Schwartz, O. and Simoncelli, E. P. Natural signal statistics and sensory gain control. Nature Neuroscience, 4(8): 819-825, August 2001.\n\nIndependent component filters of natural images compared with simple cells in primary visual cortex. J Van Hateren, A Van Der Schaaf, Proceedings: Biological Sciences. 265van Hateren, J. and van der Schaaf, A. Independent com- ponent filters of natural images compared with simple cells in primary visual cortex. Proceedings: Biological Sciences, 265(1394):359-366, Mar 1998.\n\nAdaptation and temporal decorrelation by single neurons in the primary visual cortex. X.-J Wang, Y Liu, M V Sanchez-Vives, D A Mccormick, Journal of Neurophysiology. 896Wang, X.-J., Liu, Y., Sanchez-Vives, M. V., and McCormick, D. A. Adaptation and temporal decorrelation by single neurons in the primary visual cortex. Journal of Neuro- physiology, 89(6):3279-3293, 2003.\n\nWhitening of odor representations by the wiring diagram of the olfactory bulb. A A Wanner, R W Friedrich, Nature Neuroscience. 233Wanner, A. A. and Friedrich, R. W. Whitening of odor representations by the wiring diagram of the olfactory bulb. Nature Neuroscience, 23(3):433-442, 2020.\n\nPattern adaptation and normalization reweighting. Z M Westrick, D J Heeger, M S Landy, Journal of Neuroscience. 3638Westrick, Z. M., Heeger, D. J., and Landy, M. S. Pattern adaptation and normalization reweighting. Journal of Neuroscience, 36(38):9805-9816, 2016.\n\nBarlow twins: Self-supervised learning via redundancy reduction. J Zbontar, L Jing, I Misra, Y Lecun, Deny , S , International Conference on Machine Learning. PMLRZbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. Bar- low twins: Self-supervised learning via redundancy reduc- tion. In International Conference on Machine Learning, pp. 12310-12320. PMLR, 2021.\n\nShort-term synaptic plasticity. R S Zucker, W G Regehr, Annual Review of Physiology. 641Zucker, R. S. and Regehr, W. G. Short-term synaptic plastic- ity. Annual Review of Physiology, 64(1):355-405, 2002.\n", "annotations": {"author": "[{\"end\":93,\"start\":78},{\"end\":109,\"start\":94},{\"end\":125,\"start\":110},{\"end\":146,\"start\":126},{\"end\":165,\"start\":147}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":87},{\"end\":108,\"start\":100},{\"end\":124,\"start\":118},{\"end\":145,\"start\":135},{\"end\":164,\"start\":154}]", "author_first_name": "[{\"end\":84,\"start\":78},{\"end\":86,\"start\":85},{\"end\":99,\"start\":94},{\"end\":115,\"start\":110},{\"end\":117,\"start\":116},{\"end\":132,\"start\":126},{\"end\":134,\"start\":133},{\"end\":151,\"start\":147},{\"end\":153,\"start\":152}]", "author_affiliation": null, "title": "[{\"end\":75,\"start\":1},{\"end\":240,\"start\":166}]", "venue": null, "abstract": "[{\"end\":1344,\"start\":242}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1647,\"start\":1622},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1670,\"start\":1647},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1692,\"start\":1670},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1781,\"start\":1763},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1856,\"start\":1835},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2023,\"start\":2001},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2044,\"start\":2023},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2061,\"start\":2044},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2081,\"start\":2061},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2354,\"start\":2335},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2370,\"start\":2354},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2542,\"start\":2517},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2638,\"start\":2618},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2658,\"start\":2638},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2730,\"start\":2716},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2745,\"start\":2730},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2766,\"start\":2745},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3000,\"start\":2977},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3049,\"start\":3028},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3070,\"start\":3049},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3126,\"start\":3109},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3159,\"start\":3136},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3181,\"start\":3159},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3362,\"start\":3333},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3385,\"start\":3362},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6892,\"start\":6872},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8012,\"start\":7983},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9673,\"start\":9652},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12714,\"start\":12695},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12738,\"start\":12714},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15228,\"start\":15199},{\"end\":18060,\"start\":18054},{\"end\":20960,\"start\":20949},{\"end\":20962,\"start\":20960},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22141,\"start\":22116},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23384,\"start\":23362},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30249,\"start\":30220},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30499,\"start\":30470},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30521,\"start\":30499},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30547,\"start\":30521},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":30577,\"start\":30547},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30599,\"start\":30577},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30771,\"start\":30750},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":30796,\"start\":30771},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30955,\"start\":30932},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31073,\"start\":31052},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31246,\"start\":31221},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31830,\"start\":31811},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31844,\"start\":31830},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32234,\"start\":32212},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34433,\"start\":34413},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34669,\"start\":34649},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34688,\"start\":34669},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34707,\"start\":34688},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34828,\"start\":34812},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35067,\"start\":35043},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35340,\"start\":35319},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35356,\"start\":35340},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35935,\"start\":35910},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36064,\"start\":36040},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":36360,\"start\":36338},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36862,\"start\":36843},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37057,\"start\":37034},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37077,\"start\":37057},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":37211,\"start\":37189},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37232,\"start\":37211},{\"end\":41979,\"start\":41967},{\"end\":44869,\"start\":44830},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":50445,\"start\":50423},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":50466,\"start\":50445},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":50708,\"start\":50689},{\"end\":51258,\"start\":51247}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52126,\"start\":51705},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52673,\"start\":52127},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53116,\"start\":52674},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53626,\"start\":53117},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54559,\"start\":53627},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54943,\"start\":54560},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55234,\"start\":54944}]", "paragraph": "[{\"end\":2371,\"start\":1360},{\"end\":3627,\"start\":2373},{\"end\":3830,\"start\":3629},{\"end\":4987,\"start\":3832},{\"end\":5208,\"start\":4989},{\"end\":5616,\"start\":5210},{\"end\":5903,\"start\":5618},{\"end\":6269,\"start\":5949},{\"end\":6420,\"start\":6296},{\"end\":6607,\"start\":6454},{\"end\":7278,\"start\":6609},{\"end\":7666,\"start\":7341},{\"end\":7874,\"start\":7668},{\"end\":8063,\"start\":7893},{\"end\":8236,\"start\":8096},{\"end\":9222,\"start\":8284},{\"end\":9477,\"start\":9273},{\"end\":9981,\"start\":9527},{\"end\":10135,\"start\":10039},{\"end\":10547,\"start\":10200},{\"end\":10835,\"start\":10549},{\"end\":11168,\"start\":10901},{\"end\":11226,\"start\":11210},{\"end\":11477,\"start\":11272},{\"end\":12034,\"start\":11479},{\"end\":12739,\"start\":12084},{\"end\":12923,\"start\":12773},{\"end\":13240,\"start\":13007},{\"end\":13696,\"start\":13242},{\"end\":13937,\"start\":13771},{\"end\":14474,\"start\":13956},{\"end\":14641,\"start\":14497},{\"end\":15666,\"start\":14699},{\"end\":15905,\"start\":15700},{\"end\":15957,\"start\":15907},{\"end\":16551,\"start\":16228},{\"end\":17164,\"start\":16594},{\"end\":17311,\"start\":17166},{\"end\":17597,\"start\":17340},{\"end\":17826,\"start\":17599},{\"end\":19164,\"start\":17879},{\"end\":20724,\"start\":19229},{\"end\":21999,\"start\":20726},{\"end\":22888,\"start\":22046},{\"end\":23304,\"start\":22890},{\"end\":23979,\"start\":23341},{\"end\":24144,\"start\":23981},{\"end\":24238,\"start\":24195},{\"end\":25312,\"start\":24298},{\"end\":25485,\"start\":25314},{\"end\":25879,\"start\":25534},{\"end\":27144,\"start\":25908},{\"end\":28174,\"start\":27200},{\"end\":29019,\"start\":28176},{\"end\":29871,\"start\":29021},{\"end\":31328,\"start\":29932},{\"end\":32083,\"start\":31377},{\"end\":32614,\"start\":32085},{\"end\":33027,\"start\":32651},{\"end\":34072,\"start\":33042},{\"end\":34970,\"start\":34110},{\"end\":37078,\"start\":35004},{\"end\":37395,\"start\":37093},{\"end\":37792,\"start\":37411},{\"end\":38113,\"start\":37794},{\"end\":38193,\"start\":38115},{\"end\":38896,\"start\":38322},{\"end\":39131,\"start\":38953},{\"end\":39221,\"start\":39133},{\"end\":39414,\"start\":39223},{\"end\":39536,\"start\":39447},{\"end\":39759,\"start\":39538},{\"end\":39931,\"start\":39804},{\"end\":40149,\"start\":39974},{\"end\":40323,\"start\":40296},{\"end\":40404,\"start\":40325},{\"end\":40497,\"start\":40469},{\"end\":40650,\"start\":40531},{\"end\":41256,\"start\":40849},{\"end\":41523,\"start\":41291},{\"end\":41684,\"start\":41562},{\"end\":42030,\"start\":41717},{\"end\":42137,\"start\":42032},{\"end\":42478,\"start\":42267},{\"end\":42721,\"start\":42525},{\"end\":42866,\"start\":42753},{\"end\":43170,\"start\":42890},{\"end\":43270,\"start\":43217},{\"end\":43875,\"start\":43338},{\"end\":44510,\"start\":43877},{\"end\":44642,\"start\":44539},{\"end\":44871,\"start\":44723},{\"end\":44995,\"start\":44873},{\"end\":45294,\"start\":45042},{\"end\":46220,\"start\":45376},{\"end\":46586,\"start\":46261},{\"end\":47455,\"start\":46672},{\"end\":47496,\"start\":47457},{\"end\":47692,\"start\":47558},{\"end\":48360,\"start\":47694},{\"end\":48781,\"start\":48400},{\"end\":48976,\"start\":48855},{\"end\":49182,\"start\":48978},{\"end\":49246,\"start\":49184},{\"end\":49857,\"start\":49334},{\"end\":50709,\"start\":49914},{\"end\":50797,\"start\":50711},{\"end\":50934,\"start\":50866},{\"end\":51010,\"start\":50977},{\"end\":51370,\"start\":51043},{\"end\":51704,\"start\":51440}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6453,\"start\":6421},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7340,\"start\":7279},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7892,\"start\":7875},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8095,\"start\":8064},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9272,\"start\":9223},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9526,\"start\":9478},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10038,\"start\":9982},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11209,\"start\":11169},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11271,\"start\":11227},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13006,\"start\":12924},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13770,\"start\":13697},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13955,\"start\":13938},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14698,\"start\":14642},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16227,\"start\":15958},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17339,\"start\":17312},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19184,\"start\":19165},{\"attributes\":{\"id\":\"formula_16\"},\"end\":24194,\"start\":24145},{\"attributes\":{\"id\":\"formula_17\"},\"end\":24297,\"start\":24239},{\"attributes\":{\"id\":\"formula_18\"},\"end\":25533,\"start\":25486},{\"attributes\":{\"id\":\"formula_19\"},\"end\":25907,\"start\":25880},{\"attributes\":{\"id\":\"formula_20\"},\"end\":32650,\"start\":32615},{\"attributes\":{\"id\":\"formula_21\"},\"end\":38321,\"start\":38194},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39446,\"start\":39415},{\"attributes\":{\"id\":\"formula_23\"},\"end\":39803,\"start\":39760},{\"attributes\":{\"id\":\"formula_24\"},\"end\":39973,\"start\":39932},{\"attributes\":{\"id\":\"formula_25\"},\"end\":40295,\"start\":40150},{\"attributes\":{\"id\":\"formula_26\"},\"end\":40468,\"start\":40405},{\"attributes\":{\"id\":\"formula_27\"},\"end\":40848,\"start\":40651},{\"attributes\":{\"id\":\"formula_28\"},\"end\":41290,\"start\":41257},{\"attributes\":{\"id\":\"formula_29\"},\"end\":41561,\"start\":41524},{\"attributes\":{\"id\":\"formula_30\"},\"end\":41716,\"start\":41685},{\"attributes\":{\"id\":\"formula_31\"},\"end\":42217,\"start\":42138},{\"attributes\":{\"id\":\"formula_32\"},\"end\":42524,\"start\":42479},{\"attributes\":{\"id\":\"formula_33\"},\"end\":42889,\"start\":42867},{\"attributes\":{\"id\":\"formula_34\"},\"end\":43216,\"start\":43171},{\"attributes\":{\"id\":\"formula_35\"},\"end\":43303,\"start\":43271},{\"attributes\":{\"id\":\"formula_36\"},\"end\":43337,\"start\":43303},{\"attributes\":{\"id\":\"formula_37\"},\"end\":44722,\"start\":44643},{\"attributes\":{\"id\":\"formula_38\"},\"end\":45041,\"start\":44996},{\"attributes\":{\"id\":\"formula_39\"},\"end\":46260,\"start\":46221},{\"attributes\":{\"id\":\"formula_40\"},\"end\":47557,\"start\":47497},{\"attributes\":{\"id\":\"formula_41\"},\"end\":48854,\"start\":48782},{\"attributes\":{\"id\":\"formula_42\"},\"end\":49333,\"start\":49247},{\"attributes\":{\"id\":\"formula_43\"},\"end\":50865,\"start\":50798},{\"attributes\":{\"id\":\"formula_44\"},\"end\":50976,\"start\":50935},{\"attributes\":{\"id\":\"formula_45\"},\"end\":51042,\"start\":51011},{\"attributes\":{\"id\":\"formula_46\"},\"end\":51439,\"start\":51371}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1358,\"start\":1346},{\"attributes\":{\"n\":\"2.\"},\"end\":5947,\"start\":5906},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6294,\"start\":6272},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8282,\"start\":8239},{\"attributes\":{\"n\":\"3.\"},\"end\":10198,\"start\":10138},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10899,\"start\":10838},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12082,\"start\":12037},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":12771,\"start\":12742},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":14495,\"start\":14477},{\"attributes\":{\"n\":\"3.2.3.\"},\"end\":15698,\"start\":15669},{\"attributes\":{\"n\":\"4.\"},\"end\":16592,\"start\":16554},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17877,\"start\":17829},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19227,\"start\":19186},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22044,\"start\":22002},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23339,\"start\":23307},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27198,\"start\":27147},{\"attributes\":{\"n\":\"5.\"},\"end\":29886,\"start\":29874},{\"attributes\":{\"n\":\"5.1.\"},\"end\":29930,\"start\":29889},{\"attributes\":{\"n\":\"5.2.\"},\"end\":31375,\"start\":31331},{\"attributes\":{\"n\":\"6.\"},\"end\":33040,\"start\":33030},{\"attributes\":{\"n\":\"6.1.\"},\"end\":34108,\"start\":34075},{\"attributes\":{\"n\":\"6.2.\"},\"end\":35002,\"start\":34973},{\"attributes\":{\"n\":\"6.3.\"},\"end\":37091,\"start\":37081},{\"end\":37409,\"start\":37398},{\"end\":38951,\"start\":38899},{\"end\":40529,\"start\":40500},{\"end\":42265,\"start\":42219},{\"end\":42751,\"start\":42724},{\"end\":44537,\"start\":44513},{\"end\":45374,\"start\":45297},{\"end\":46631,\"start\":46589},{\"end\":46670,\"start\":46634},{\"end\":48398,\"start\":48363},{\"end\":49912,\"start\":49860},{\"end\":51716,\"start\":51706},{\"end\":52138,\"start\":52128},{\"end\":52685,\"start\":52675},{\"end\":53128,\"start\":53118},{\"end\":53638,\"start\":53628},{\"end\":54955,\"start\":54945}]", "table": null, "figure_caption": "[{\"end\":52126,\"start\":51718},{\"end\":52673,\"start\":52140},{\"end\":53116,\"start\":52687},{\"end\":53626,\"start\":53130},{\"end\":54559,\"start\":53640},{\"end\":54943,\"start\":54562},{\"end\":55234,\"start\":54957}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6200,\"start\":6191},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10771,\"start\":10762},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11667,\"start\":11658},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14099,\"start\":14091},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18069,\"start\":18061},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18547,\"start\":18539},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19927,\"start\":19919},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21336,\"start\":21327},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22273,\"start\":22265},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22711,\"start\":22703},{\"end\":23756,\"start\":23747},{\"end\":24608,\"start\":24600},{\"end\":26102,\"start\":26094},{\"end\":26384,\"start\":26370},{\"end\":26662,\"start\":26653},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27791,\"start\":27781},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28013,\"start\":27995},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28172,\"start\":28163},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29101,\"start\":29092},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29458,\"start\":29449},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36520,\"start\":36508},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38192,\"start\":38184},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45147,\"start\":45139},{\"end\":45732,\"start\":45723},{\"end\":46087,\"start\":46079},{\"end\":46584,\"start\":46576},{\"end\":46871,\"start\":46863},{\"end\":47927,\"start\":47918},{\"end\":48100,\"start\":48091},{\"end\":48346,\"start\":48337},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49508,\"start\":49499}]", "bib_author_first_name": "[{\"end\":60809,\"start\":60808},{\"end\":60811,\"start\":60810},{\"end\":60821,\"start\":60820},{\"end\":61070,\"start\":61069},{\"end\":61072,\"start\":61071},{\"end\":61081,\"start\":61080},{\"end\":61083,\"start\":61082},{\"end\":61261,\"start\":61260},{\"end\":61270,\"start\":61269},{\"end\":61272,\"start\":61271},{\"end\":61280,\"start\":61279},{\"end\":61290,\"start\":61289},{\"end\":61299,\"start\":61298},{\"end\":61311,\"start\":61310},{\"end\":61324,\"start\":61323},{\"end\":61326,\"start\":61325},{\"end\":61335,\"start\":61334},{\"end\":61698,\"start\":61697},{\"end\":61708,\"start\":61707},{\"end\":61717,\"start\":61716},{\"end\":62037,\"start\":62036},{\"end\":62039,\"start\":62038},{\"end\":62280,\"start\":62279},{\"end\":62282,\"start\":62281},{\"end\":62292,\"start\":62291},{\"end\":62537,\"start\":62536},{\"end\":62539,\"start\":62538},{\"end\":62547,\"start\":62546},{\"end\":62549,\"start\":62548},{\"end\":62789,\"start\":62788},{\"end\":62800,\"start\":62799},{\"end\":62802,\"start\":62801},{\"end\":62812,\"start\":62811},{\"end\":63072,\"start\":63071},{\"end\":63081,\"start\":63080},{\"end\":63090,\"start\":63089},{\"end\":63348,\"start\":63347},{\"end\":63359,\"start\":63358},{\"end\":63368,\"start\":63367},{\"end\":63377,\"start\":63376},{\"end\":63633,\"start\":63632},{\"end\":63641,\"start\":63640},{\"end\":63833,\"start\":63832},{\"end\":64038,\"start\":64037},{\"end\":64040,\"start\":64039},{\"end\":64051,\"start\":64050},{\"end\":64063,\"start\":64062},{\"end\":64425,\"start\":64424},{\"end\":64427,\"start\":64426},{\"end\":64443,\"start\":64442},{\"end\":64455,\"start\":64454},{\"end\":64457,\"start\":64456},{\"end\":64717,\"start\":64716},{\"end\":64727,\"start\":64726},{\"end\":64733,\"start\":64732},{\"end\":65289,\"start\":65288},{\"end\":65291,\"start\":65290},{\"end\":65300,\"start\":65299},{\"end\":65306,\"start\":65305},{\"end\":65314,\"start\":65313},{\"end\":65753,\"start\":65752},{\"end\":65764,\"start\":65763},{\"end\":65776,\"start\":65775},{\"end\":65789,\"start\":65788},{\"end\":66088,\"start\":66087},{\"end\":66090,\"start\":66089},{\"end\":66102,\"start\":66101},{\"end\":66104,\"start\":66103},{\"end\":66113,\"start\":66112},{\"end\":66315,\"start\":66314},{\"end\":66317,\"start\":66316},{\"end\":66329,\"start\":66328},{\"end\":66331,\"start\":66330},{\"end\":66571,\"start\":66570},{\"end\":66573,\"start\":66572},{\"end\":66785,\"start\":66784},{\"end\":66787,\"start\":66786},{\"end\":66987,\"start\":66986},{\"end\":66999,\"start\":66998},{\"end\":67009,\"start\":67008},{\"end\":67011,\"start\":67010},{\"end\":67365,\"start\":67364},{\"end\":67377,\"start\":67376},{\"end\":67379,\"start\":67378},{\"end\":67390,\"start\":67389},{\"end\":67400,\"start\":67399},{\"end\":67411,\"start\":67410},{\"end\":67431,\"start\":67423},{\"end\":67435,\"start\":67434},{\"end\":67688,\"start\":67687},{\"end\":67690,\"start\":67689},{\"end\":67696,\"start\":67695},{\"end\":67704,\"start\":67703},{\"end\":67714,\"start\":67713},{\"end\":67727,\"start\":67726},{\"end\":67733,\"start\":67732},{\"end\":67741,\"start\":67740},{\"end\":67752,\"start\":67748},{\"end\":67756,\"start\":67755},{\"end\":68120,\"start\":68119},{\"end\":68127,\"start\":68126},{\"end\":68135,\"start\":68134},{\"end\":68142,\"start\":68141},{\"end\":68149,\"start\":68148},{\"end\":68157,\"start\":68156},{\"end\":68567,\"start\":68566},{\"end\":68580,\"start\":68579},{\"end\":68738,\"start\":68737},{\"end\":68740,\"start\":68739},{\"end\":68748,\"start\":68747},{\"end\":68750,\"start\":68749},{\"end\":68762,\"start\":68761},{\"end\":68764,\"start\":68763},{\"end\":69074,\"start\":69073},{\"end\":69084,\"start\":69083},{\"end\":69242,\"start\":69241},{\"end\":69251,\"start\":69250},{\"end\":69260,\"start\":69259},{\"end\":69513,\"start\":69512},{\"end\":69515,\"start\":69514},{\"end\":69522,\"start\":69521},{\"end\":69773,\"start\":69772},{\"end\":70034,\"start\":70033},{\"end\":70337,\"start\":70336},{\"end\":70349,\"start\":70348},{\"end\":70361,\"start\":70360},{\"end\":70363,\"start\":70362},{\"end\":70710,\"start\":70709},{\"end\":70720,\"start\":70719},{\"end\":70722,\"start\":70721},{\"end\":70734,\"start\":70733},{\"end\":70736,\"start\":70735},{\"end\":70980,\"start\":70979},{\"end\":70989,\"start\":70988},{\"end\":70991,\"start\":70990},{\"end\":71002,\"start\":71001},{\"end\":71014,\"start\":71013},{\"end\":71025,\"start\":71024},{\"end\":71045,\"start\":71044},{\"end\":71380,\"start\":71379},{\"end\":71382,\"start\":71381},{\"end\":71392,\"start\":71391},{\"end\":71394,\"start\":71393},{\"end\":71403,\"start\":71402},{\"end\":71421,\"start\":71415},{\"end\":71425,\"start\":71424},{\"end\":71641,\"start\":71640},{\"end\":71643,\"start\":71642},{\"end\":71656,\"start\":71655},{\"end\":71658,\"start\":71657},{\"end\":71891,\"start\":71890},{\"end\":71893,\"start\":71892},{\"end\":71902,\"start\":71901},{\"end\":71904,\"start\":71903},{\"end\":72153,\"start\":72152},{\"end\":72166,\"start\":72165},{\"end\":72416,\"start\":72415},{\"end\":72428,\"start\":72427},{\"end\":72430,\"start\":72429},{\"end\":72761,\"start\":72760},{\"end\":72773,\"start\":72772},{\"end\":72775,\"start\":72774},{\"end\":73057,\"start\":73056},{\"end\":73069,\"start\":73068},{\"end\":73071,\"start\":73070},{\"end\":73346,\"start\":73345},{\"end\":73361,\"start\":73360},{\"end\":73711,\"start\":73707},{\"end\":73719,\"start\":73718},{\"end\":73726,\"start\":73725},{\"end\":73728,\"start\":73727},{\"end\":73745,\"start\":73744},{\"end\":73747,\"start\":73746},{\"end\":74075,\"start\":74074},{\"end\":74077,\"start\":74076},{\"end\":74087,\"start\":74086},{\"end\":74089,\"start\":74088},{\"end\":74333,\"start\":74332},{\"end\":74335,\"start\":74334},{\"end\":74347,\"start\":74346},{\"end\":74349,\"start\":74348},{\"end\":74359,\"start\":74358},{\"end\":74361,\"start\":74360},{\"end\":74613,\"start\":74612},{\"end\":74624,\"start\":74623},{\"end\":74632,\"start\":74631},{\"end\":74641,\"start\":74640},{\"end\":74653,\"start\":74649},{\"end\":74657,\"start\":74656},{\"end\":74949,\"start\":74948},{\"end\":74951,\"start\":74950},{\"end\":74961,\"start\":74960},{\"end\":74963,\"start\":74962}]", "bib_author_last_name": "[{\"end\":60818,\"start\":60812},{\"end\":60830,\"start\":60822},{\"end\":61078,\"start\":61073},{\"end\":61091,\"start\":61084},{\"end\":61267,\"start\":61262},{\"end\":61277,\"start\":61273},{\"end\":61287,\"start\":61281},{\"end\":61296,\"start\":61291},{\"end\":61308,\"start\":61300},{\"end\":61321,\"start\":61312},{\"end\":61332,\"start\":61327},{\"end\":61344,\"start\":61336},{\"end\":61705,\"start\":61699},{\"end\":61714,\"start\":61709},{\"end\":61723,\"start\":61718},{\"end\":62046,\"start\":62040},{\"end\":62289,\"start\":62283},{\"end\":62300,\"start\":62293},{\"end\":62544,\"start\":62540},{\"end\":62559,\"start\":62550},{\"end\":62797,\"start\":62790},{\"end\":62809,\"start\":62803},{\"end\":62822,\"start\":62813},{\"end\":63078,\"start\":63073},{\"end\":63087,\"start\":63082},{\"end\":63100,\"start\":63091},{\"end\":63356,\"start\":63349},{\"end\":63365,\"start\":63360},{\"end\":63374,\"start\":63369},{\"end\":63385,\"start\":63378},{\"end\":63638,\"start\":63634},{\"end\":63654,\"start\":63642},{\"end\":63842,\"start\":63834},{\"end\":64048,\"start\":64041},{\"end\":64060,\"start\":64052},{\"end\":64071,\"start\":64064},{\"end\":64440,\"start\":64428},{\"end\":64452,\"start\":64444},{\"end\":64468,\"start\":64458},{\"end\":64724,\"start\":64718},{\"end\":64730,\"start\":64728},{\"end\":64737,\"start\":64734},{\"end\":65297,\"start\":65292},{\"end\":65303,\"start\":65301},{\"end\":65311,\"start\":65307},{\"end\":65318,\"start\":65315},{\"end\":65761,\"start\":65754},{\"end\":65773,\"start\":65765},{\"end\":65786,\"start\":65777},{\"end\":65794,\"start\":65790},{\"end\":66099,\"start\":66091},{\"end\":66110,\"start\":66105},{\"end\":66120,\"start\":66114},{\"end\":66326,\"start\":66318},{\"end\":66338,\"start\":66332},{\"end\":66583,\"start\":66574},{\"end\":66795,\"start\":66788},{\"end\":66807,\"start\":66797},{\"end\":66996,\"start\":66988},{\"end\":67006,\"start\":67000},{\"end\":67017,\"start\":67012},{\"end\":67374,\"start\":67366},{\"end\":67387,\"start\":67380},{\"end\":67397,\"start\":67391},{\"end\":67408,\"start\":67401},{\"end\":67421,\"start\":67412},{\"end\":67693,\"start\":67691},{\"end\":67701,\"start\":67697},{\"end\":67711,\"start\":67705},{\"end\":67724,\"start\":67715},{\"end\":67730,\"start\":67728},{\"end\":67738,\"start\":67734},{\"end\":67746,\"start\":67742},{\"end\":67761,\"start\":67757},{\"end\":68124,\"start\":68121},{\"end\":68132,\"start\":68128},{\"end\":68139,\"start\":68136},{\"end\":68146,\"start\":68143},{\"end\":68154,\"start\":68150},{\"end\":68162,\"start\":68158},{\"end\":68577,\"start\":68568},{\"end\":68584,\"start\":68581},{\"end\":68745,\"start\":68741},{\"end\":68759,\"start\":68751},{\"end\":68772,\"start\":68765},{\"end\":69081,\"start\":69075},{\"end\":69092,\"start\":69085},{\"end\":69248,\"start\":69243},{\"end\":69257,\"start\":69252},{\"end\":69269,\"start\":69261},{\"end\":69519,\"start\":69516},{\"end\":69528,\"start\":69523},{\"end\":69784,\"start\":69774},{\"end\":70043,\"start\":70035},{\"end\":70051,\"start\":70045},{\"end\":70346,\"start\":70338},{\"end\":70358,\"start\":70350},{\"end\":70374,\"start\":70364},{\"end\":70717,\"start\":70711},{\"end\":70731,\"start\":70723},{\"end\":70743,\"start\":70737},{\"end\":70986,\"start\":70981},{\"end\":70999,\"start\":70992},{\"end\":71011,\"start\":71003},{\"end\":71022,\"start\":71015},{\"end\":71042,\"start\":71026},{\"end\":71056,\"start\":71046},{\"end\":71389,\"start\":71383},{\"end\":71400,\"start\":71395},{\"end\":71413,\"start\":71404},{\"end\":71653,\"start\":71644},{\"end\":71670,\"start\":71659},{\"end\":71899,\"start\":71894},{\"end\":71910,\"start\":71905},{\"end\":72163,\"start\":72154},{\"end\":72172,\"start\":72167},{\"end\":72425,\"start\":72417},{\"end\":72441,\"start\":72431},{\"end\":72770,\"start\":72762},{\"end\":72786,\"start\":72776},{\"end\":73066,\"start\":73058},{\"end\":73082,\"start\":73072},{\"end\":73358,\"start\":73347},{\"end\":73376,\"start\":73362},{\"end\":73716,\"start\":73712},{\"end\":73723,\"start\":73720},{\"end\":73742,\"start\":73729},{\"end\":73757,\"start\":73748},{\"end\":74084,\"start\":74078},{\"end\":74099,\"start\":74090},{\"end\":74344,\"start\":74336},{\"end\":74356,\"start\":74350},{\"end\":74367,\"start\":74362},{\"end\":74621,\"start\":74614},{\"end\":74629,\"start\":74625},{\"end\":74638,\"start\":74633},{\"end\":74647,\"start\":74642},{\"end\":74958,\"start\":74952},{\"end\":74970,\"start\":74964}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":42009967},\"end\":61018,\"start\":60729},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17515861},\"end\":61230,\"start\":61020},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":220380876},\"end\":61612,\"start\":61232},{\"attributes\":{\"id\":\"b3\"},\"end\":61962,\"start\":61614},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":59790818},\"end\":62233,\"start\":61964},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":60513567},\"end\":62469,\"start\":62235},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6219133},\"end\":62716,\"start\":62471},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1583895},\"end\":63007,\"start\":62718},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":322440},\"end\":63291,\"start\":63009},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1907942},\"end\":63609,\"start\":63293},{\"attributes\":{\"id\":\"b10\"},\"end\":63768,\"start\":63611},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":225720723},\"end\":63998,\"start\":63770},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16434239},\"end\":64311,\"start\":64000},{\"attributes\":{\"id\":\"b13\"},\"end\":64643,\"start\":64313},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":308212},\"end\":65226,\"start\":64645},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":253116812},\"end\":65695,\"start\":65228},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":220496043},\"end\":66032,\"start\":65697},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4354013},\"end\":66259,\"start\":66034},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":210044809},\"end\":66508,\"start\":66261},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":207606983},\"end\":66744,\"start\":66510},{\"attributes\":{\"id\":\"b20\"},\"end\":66904,\"start\":66746},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11477445},\"end\":67273,\"start\":66906},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":19842415},\"end\":67685,\"start\":67275},{\"attributes\":{\"id\":\"b23\"},\"end\":68063,\"start\":67687},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":233481690},\"end\":68503,\"start\":68065},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11959218},\"end\":68735,\"start\":68505},{\"attributes\":{\"id\":\"b26\"},\"end\":69027,\"start\":68737},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4396795},\"end\":69202,\"start\":69029},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":55075085},\"end\":69425,\"start\":69204},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13290117},\"end\":69715,\"start\":69427},{\"attributes\":{\"id\":\"b30\"},\"end\":69937,\"start\":69717},{\"attributes\":{\"id\":\"b31\"},\"end\":70235,\"start\":69939},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":252439090},\"end\":70642,\"start\":70237},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8499962},\"end\":70942,\"start\":70644},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":236447849},\"end\":71315,\"start\":70944},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16595756},\"end\":71600,\"start\":71317},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":221355749},\"end\":71817,\"start\":71602},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":11257242},\"end\":72052,\"start\":71819},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4358477},\"end\":72337,\"start\":72054},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1148017},\"end\":72667,\"start\":72339},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":207830762},\"end\":73002,\"start\":72669},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6346299},\"end\":73242,\"start\":73004},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1789554},\"end\":73619,\"start\":73244},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":54488220},\"end\":73993,\"start\":73621},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":210834491},\"end\":74280,\"start\":73995},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":2535267},\"end\":74545,\"start\":74282},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232110471},\"end\":74914,\"start\":74547},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":7980969},\"end\":75119,\"start\":74916}]", "bib_title": "[{\"end\":60806,\"start\":60729},{\"end\":61067,\"start\":61020},{\"end\":61258,\"start\":61232},{\"end\":61695,\"start\":61614},{\"end\":62034,\"start\":61964},{\"end\":62277,\"start\":62235},{\"end\":62534,\"start\":62471},{\"end\":62786,\"start\":62718},{\"end\":63069,\"start\":63009},{\"end\":63345,\"start\":63293},{\"end\":63830,\"start\":63770},{\"end\":64035,\"start\":64000},{\"end\":64714,\"start\":64645},{\"end\":65286,\"start\":65228},{\"end\":65750,\"start\":65697},{\"end\":66085,\"start\":66034},{\"end\":66312,\"start\":66261},{\"end\":66568,\"start\":66510},{\"end\":66984,\"start\":66906},{\"end\":67362,\"start\":67275},{\"end\":68117,\"start\":68065},{\"end\":68564,\"start\":68505},{\"end\":69071,\"start\":69029},{\"end\":69239,\"start\":69204},{\"end\":69510,\"start\":69427},{\"end\":70031,\"start\":69939},{\"end\":70334,\"start\":70237},{\"end\":70707,\"start\":70644},{\"end\":70977,\"start\":70944},{\"end\":71377,\"start\":71317},{\"end\":71638,\"start\":71602},{\"end\":71888,\"start\":71819},{\"end\":72150,\"start\":72054},{\"end\":72413,\"start\":72339},{\"end\":72758,\"start\":72669},{\"end\":73054,\"start\":73004},{\"end\":73343,\"start\":73244},{\"end\":73705,\"start\":73621},{\"end\":74072,\"start\":73995},{\"end\":74330,\"start\":74282},{\"end\":74610,\"start\":74547},{\"end\":74946,\"start\":74916}]", "bib_author": "[{\"end\":60820,\"start\":60808},{\"end\":60832,\"start\":60820},{\"end\":61080,\"start\":61069},{\"end\":61093,\"start\":61080},{\"end\":61269,\"start\":61260},{\"end\":61279,\"start\":61269},{\"end\":61289,\"start\":61279},{\"end\":61298,\"start\":61289},{\"end\":61310,\"start\":61298},{\"end\":61323,\"start\":61310},{\"end\":61334,\"start\":61323},{\"end\":61346,\"start\":61334},{\"end\":61707,\"start\":61697},{\"end\":61716,\"start\":61707},{\"end\":61725,\"start\":61716},{\"end\":62048,\"start\":62036},{\"end\":62291,\"start\":62279},{\"end\":62302,\"start\":62291},{\"end\":62546,\"start\":62536},{\"end\":62561,\"start\":62546},{\"end\":62799,\"start\":62788},{\"end\":62811,\"start\":62799},{\"end\":62824,\"start\":62811},{\"end\":63080,\"start\":63071},{\"end\":63089,\"start\":63080},{\"end\":63102,\"start\":63089},{\"end\":63358,\"start\":63347},{\"end\":63367,\"start\":63358},{\"end\":63376,\"start\":63367},{\"end\":63387,\"start\":63376},{\"end\":63640,\"start\":63632},{\"end\":63656,\"start\":63640},{\"end\":63844,\"start\":63832},{\"end\":64050,\"start\":64037},{\"end\":64062,\"start\":64050},{\"end\":64073,\"start\":64062},{\"end\":64442,\"start\":64424},{\"end\":64454,\"start\":64442},{\"end\":64470,\"start\":64454},{\"end\":64726,\"start\":64716},{\"end\":64732,\"start\":64726},{\"end\":64739,\"start\":64732},{\"end\":65299,\"start\":65288},{\"end\":65305,\"start\":65299},{\"end\":65313,\"start\":65305},{\"end\":65320,\"start\":65313},{\"end\":65763,\"start\":65752},{\"end\":65775,\"start\":65763},{\"end\":65788,\"start\":65775},{\"end\":65796,\"start\":65788},{\"end\":66101,\"start\":66087},{\"end\":66112,\"start\":66101},{\"end\":66122,\"start\":66112},{\"end\":66328,\"start\":66314},{\"end\":66340,\"start\":66328},{\"end\":66585,\"start\":66570},{\"end\":66797,\"start\":66784},{\"end\":66809,\"start\":66797},{\"end\":66998,\"start\":66986},{\"end\":67008,\"start\":66998},{\"end\":67019,\"start\":67008},{\"end\":67376,\"start\":67364},{\"end\":67389,\"start\":67376},{\"end\":67399,\"start\":67389},{\"end\":67410,\"start\":67399},{\"end\":67423,\"start\":67410},{\"end\":67434,\"start\":67423},{\"end\":67438,\"start\":67434},{\"end\":67695,\"start\":67687},{\"end\":67703,\"start\":67695},{\"end\":67713,\"start\":67703},{\"end\":67726,\"start\":67713},{\"end\":67732,\"start\":67726},{\"end\":67740,\"start\":67732},{\"end\":67748,\"start\":67740},{\"end\":67755,\"start\":67748},{\"end\":67763,\"start\":67755},{\"end\":68126,\"start\":68119},{\"end\":68134,\"start\":68126},{\"end\":68141,\"start\":68134},{\"end\":68148,\"start\":68141},{\"end\":68156,\"start\":68148},{\"end\":68164,\"start\":68156},{\"end\":68579,\"start\":68566},{\"end\":68586,\"start\":68579},{\"end\":68747,\"start\":68737},{\"end\":68761,\"start\":68747},{\"end\":68774,\"start\":68761},{\"end\":69083,\"start\":69073},{\"end\":69094,\"start\":69083},{\"end\":69250,\"start\":69241},{\"end\":69259,\"start\":69250},{\"end\":69271,\"start\":69259},{\"end\":69521,\"start\":69512},{\"end\":69530,\"start\":69521},{\"end\":69786,\"start\":69772},{\"end\":70045,\"start\":70033},{\"end\":70053,\"start\":70045},{\"end\":70348,\"start\":70336},{\"end\":70360,\"start\":70348},{\"end\":70376,\"start\":70360},{\"end\":70719,\"start\":70709},{\"end\":70733,\"start\":70719},{\"end\":70745,\"start\":70733},{\"end\":70988,\"start\":70979},{\"end\":71001,\"start\":70988},{\"end\":71013,\"start\":71001},{\"end\":71024,\"start\":71013},{\"end\":71044,\"start\":71024},{\"end\":71058,\"start\":71044},{\"end\":71391,\"start\":71379},{\"end\":71402,\"start\":71391},{\"end\":71415,\"start\":71402},{\"end\":71424,\"start\":71415},{\"end\":71428,\"start\":71424},{\"end\":71655,\"start\":71640},{\"end\":71672,\"start\":71655},{\"end\":71901,\"start\":71890},{\"end\":71912,\"start\":71901},{\"end\":72165,\"start\":72152},{\"end\":72174,\"start\":72165},{\"end\":72427,\"start\":72415},{\"end\":72443,\"start\":72427},{\"end\":72772,\"start\":72760},{\"end\":72788,\"start\":72772},{\"end\":73068,\"start\":73056},{\"end\":73084,\"start\":73068},{\"end\":73360,\"start\":73345},{\"end\":73378,\"start\":73360},{\"end\":73718,\"start\":73707},{\"end\":73725,\"start\":73718},{\"end\":73744,\"start\":73725},{\"end\":73759,\"start\":73744},{\"end\":74086,\"start\":74074},{\"end\":74101,\"start\":74086},{\"end\":74346,\"start\":74332},{\"end\":74358,\"start\":74346},{\"end\":74369,\"start\":74358},{\"end\":74623,\"start\":74612},{\"end\":74631,\"start\":74623},{\"end\":74640,\"start\":74631},{\"end\":74649,\"start\":74640},{\"end\":74656,\"start\":74649},{\"end\":74660,\"start\":74656},{\"end\":74960,\"start\":74948},{\"end\":74972,\"start\":74960}]", "bib_venue": "[{\"end\":60857,\"start\":60832},{\"end\":61111,\"start\":61093},{\"end\":61398,\"start\":61346},{\"end\":61777,\"start\":61725},{\"end\":62069,\"start\":62048},{\"end\":62322,\"start\":62302},{\"end\":62576,\"start\":62561},{\"end\":62843,\"start\":62824},{\"end\":63129,\"start\":63102},{\"end\":63429,\"start\":63387},{\"end\":63630,\"start\":63611},{\"end\":63869,\"start\":63844},{\"end\":64086,\"start\":64073},{\"end\":64422,\"start\":64313},{\"end\":64835,\"start\":64739},{\"end\":65411,\"start\":65320},{\"end\":65840,\"start\":65796},{\"end\":66128,\"start\":66122},{\"end\":66367,\"start\":66340},{\"end\":66614,\"start\":66585},{\"end\":66782,\"start\":66746},{\"end\":67066,\"start\":67019},{\"end\":67457,\"start\":67438},{\"end\":67861,\"start\":67763},{\"end\":68235,\"start\":68164},{\"end\":68601,\"start\":68586},{\"end\":68862,\"start\":68774},{\"end\":69100,\"start\":69094},{\"end\":69296,\"start\":69271},{\"end\":69553,\"start\":69530},{\"end\":69770,\"start\":69717},{\"end\":70075,\"start\":70053},{\"end\":70428,\"start\":70376},{\"end\":70774,\"start\":70745},{\"end\":71107,\"start\":71058},{\"end\":71435,\"start\":71428},{\"end\":71691,\"start\":71672},{\"end\":71918,\"start\":71912},{\"end\":72180,\"start\":72174},{\"end\":72492,\"start\":72443},{\"end\":72819,\"start\":72788},{\"end\":73103,\"start\":73084},{\"end\":73410,\"start\":73378},{\"end\":73785,\"start\":73759},{\"end\":74120,\"start\":74101},{\"end\":74392,\"start\":74369},{\"end\":74704,\"start\":74660},{\"end\":74999,\"start\":74972},{\"end\":64918,\"start\":64837},{\"end\":65489,\"start\":65413},{\"end\":68293,\"start\":68237}]"}}}, "year": 2023, "month": 12, "day": 17}