{"id": 52306520, "updated": "2023-09-30 15:22:37.538", "metadata": {"title": "Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems", "authors": "[{\"first\":\"Yu\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Mingze\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Chiho\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Crandall\",\"middle\":[\"J.\"]},{\"first\":\"Ella\",\"last\":\"Atkins\",\"middle\":[\"M.\"]},{\"first\":\"Behzad\",\"last\":\"Dariush\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 9, "day": 19}, "abstract": "Predicting the future location of vehicles is essential for safety-critical applications such as advanced driver assistance systems (ADAS) and autonomous driving. This paper introduces a novel approach to simultaneously predict both the location and scale of target vehicles in the first-person (egocentric) view of an ego-vehicle. We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixel-level observations for future vehicle localization. We show that incorporating dense optical flow improves prediction results significantly since it captures information about motion as well as appearance change. We also find that explicitly modeling future motion of the ego-vehicle improves the prediction accuracy, which could be especially beneficial in intelligent and automated vehicles that have motion planning capability. To evaluate the performance of our approach, we present a new dataset of first-person videos collected from a variety of scenarios at road intersections, which are particularly challenging moments for prediction because vehicle trajectories are diverse and dynamic.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1809.07408", "mag": "2968393007", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/YaoXCCAD19", "doi": "10.1109/icra.2019.8794474"}}, "content": {"source": {"pdf_hash": "37ca7b31397eb650dab1f1f7fe3ba826545d3500", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1809.07408v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1809.07408", "status": "GREEN"}}, "grobid": {"id": "dbb5bad97d6adf8abe3559549f3f1dd627c02390", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/37ca7b31397eb650dab1f1f7fe3ba826545d3500.txt", "contents": "\nEgocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems\n\n\nYu Yao \nMingze Xu \nChiho Choi \nDavid J Crandall \nElla M Atkins \nBehzad Dariush \nEgocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems\n\nPredicting the future location of vehicles is essential for safety-critical applications such as advanced driver assistance systems (ADAS) and autonomous driving. This paper introduces a novel approach to simultaneously predict both the location and scale of target vehicles in the first-person (egocentric) view of an ego-vehicle. We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixellevel observations for future vehicle localization. We show that incorporating dense optical flow improves prediction results significantly since it captures information about motion as well as appearance change. We also find that explicitly modeling future motion of the ego-vehicle improves the prediction accuracy, which could be especially beneficial in intelligent and automated vehicles that have motion planning capability. To evaluate the performance of our approach, we present a new dataset of first-person videos collected from a variety of scenarios at road intersections, which are particularly challenging moments for prediction because vehicle trajectories are diverse and dynamic.\n\nI. INTRODUCTION\n\nSafe driving requires not just accurately identifying and locating nearby objects, but also predicting their future locations and actions so that there is enough time to avoid collisions. Precise prediction of nearby vehicles' future locations is thus essential for both autonomous and semiautonomous (e.g., Advanced Driver Assistance Systems, or ADAS) driving systems. Extensive research [1]- [3] has been conducted on predicting vehicles' future actions and trajectories using overhead (bird's eye view) observations. But obtaining overhead views requires either an externallymounted camera (or LiDAR), which is not common on today's production vehicles, or aerial imagery that must be transfered to the vehicle over a network connection.\n\nA much more natural approach is to use forward-facing cameras that record the driver's \"first-person\" or \"egocentric\" perspective. In addition to being easier to collect, the firstperson perspective captures rich information about the object appearance, as well as the relationships and interactions between the ego-vehicle and objects in the environment. Due to these advantages, egocentric videos have been directly used in applications such as action recognition [4], [5], navigation [6]- [8], and end-to-end autonomous driving [9]. For trajectory prediction, some work has simulated bird's  eye views by projecting egocentric video frames onto the ground plane [1], [2], but these projections can be incorrect due to road irregularities or other sources of distortion, which prevent accurate vehicle position prediction.\n\nIn this paper we consider a more challenging problem of predicting the relative future locations and scales (represented as bounding boxes in Figure 1) of nearby vehicles with respect to an ego-vehicle equipped with an egocentric camera. We introduce a multi-stream RNN encoder-decoder (RNN-ED) architecture to effectively encode past observations from different domains and generate future bounding boxes. Unlike other work that has addressed prediction in simple scenarios such as freeways [1], [2], we consider complicated urban driving scenarios that involve a variety of multi-vehicle behaviors and interactions.\n\nThe contributions of this paper are three-fold. First, to the best of our knowledge, our work is the first to address the problem of future vehicle localization under egocentric view and challenging driving scenarios such as intersections. Second, we propose a multi-steam RNN-ED architecture for better temporal modeling and explicitly capturing vehicles' motion and appearance information by using dense optical flow and future ego-motion as inputs. Third, to test our approach, we introduce a new first-person video datasetthe Honda Egocentric View -Intersection (HEV-I) dataset -collected in a variety of scenarios involving road intersections, which we plan to publicly release. The dataset includes over 2, 400 vehicles (after filtering) in 230 videos. We evaluate our approach on this new proposed dataset, along with the existing KITTI dataset, and achieve the stateof-the-art results comparing to published baselines.\n\n\nII. RELATED WORK\n\nEgocentric Vision. An egocentric camera view is often the most natural perspective for observing an ego-vehicle environment, but it introduces additional challenges due to its narrow field of view. The literature in egocentric visual perception has typically focused on activity recognition [4], [5], [10]- [12], object detection [13], [14], person identification [15]- [17], video summarization [18], and gaze anticipation [19]. Recently, papers have also applied egocentric vision to ego-action estimation and prediction. For example, Park et al. [20] proposed a method to estimate the location of a camera wearer in future video frames. Su et al. [21] introduced a Siamese network to predict future behaviors of basketball players in multiple synchronized first-person views. Bertasius et al. [22] addressed the motion planning problem for generating an egocentric basketball motion sequence in the form of a 12-d camera configuration trajectory.\n\nMore directly related to our problem, two recent papers have considered predicting pedestrians' future locations from egocentric views. Bhattacharyya et al. [23] model observation uncertainty using Bayesian Long Short-Term Memory (LSTM) networks to predict the distribution of possible future locations. Their technique does not try incorporate image features such as object appearance. Yagi et al. [24] use human pose, scale, and ego-motion as cues in a convolutiondeconvolution (Conv1D) framework to predict future locations. The specific pose information applies to people but not to other on-road objects like vehicles. Their Conv1D model captures important features of the activity sequences but does not explicitly model temporal updating along each trajectory. In contrast, our paper proposes a multi-stream RNN-ED architecture with both past vehicle location and image features as inputs for predicting vehicle locations from egocentric views.\n\nTrajectory Prediction. Previous work on vehicle trajectory prediction has used motion features and probabilistic models [1], [25]. The probability of specific motions (e.g., lane change) is first estimated, and the future trajectory is predicted using Kalman filtering. Recently, computer vision and deep learning techniques have been investigated for trajectory prediction, by posing trajectory prediction as a sequence-to-sequence generation problem. Alahi et al. [26] proposed Social-LSTM to model pedestrian trajectories as well as their interactions. The proposed social pooling method was then improved by Gupta et al. [27] to capture global context for a Generative Adversarial Network (GAN). Social pooling is first applied to vehicle trajectory prediction in Deo et al. [2] with multi-modal maneuver conditions. Other work models scene context information using attention mechanisms to assist trajectory prediction [28], [29]. Lee et al. [3] incorporate RNN models with conditional variational autoencoders to generate multimodal predictions, and select the best prediction by ranking scores.\n\nHowever, these methods model trajectories and context information from a bird's eye view in a static camera setting, which significantly simplifies the challenge of measuring distance from visual features. In contrast, in monocular first-person views, physical distance can be estimated only indirectly, through scaling and observations of participant vehicles, and the environment changes dynamically due to ego-motion effects. Consequently, previous work cannot be directly applied to first-person videos. On the other hand, the first-person view provides higher quality object appearance information compared to birds eye view images, in which objects are represented only by the coordinates of their geometric centers. This paper encodes past location, scale, and corresponding optical flow fields of target vehicles to predict their future locations, and we further improve prediction performance by incorporating future ego-motion.\n\n\nIII. FUTURE VEHICLE LOCALIZATION FROM\n\n\nFIRST-PERSON VIEWS\n\nWe now present our approach to predicting future bounding boxes of vehicles in first-person views. Our method differs from traditional trajectory prediction because the distances of object motion in perspective images do not correspond to physical distances directly, and because the motion of the camera (ego-motion) induces additional apparent motion on nearby objects.\n\nConsider a vehicle visible in the egocentric field of view, and let its past bounding box trajectory be X\n= {X t0\u2212\u03c4 +1 , X t0\u2212\u03c4 +2 , ..., X t0 }, where X t = [c x t , c y t , w t , h t ]\nis the bounding box of the vehicle at time t (i.e., its center location and width and height in pixels, respectively). Similarly, let the future bounding box trajectory be given by Y = {Y t0+1 , Y t0+2 , ..., Y t0+\u03b4 }. Given image evidence observed from the past \u03c4 frames, O = {O t0\u2212\u03c4 +1 , O t0\u2212\u03c4 +2 , ..., O t0 }, and its corresponding past bounding box trajectory X, our goal is to predict Y.\n\nWe propose a multi-stream RNN encoder-decoder model to encode temporal information of past observations and decode future bounding boxes, as shown in Figure 2. The past bounding box trajectory is encoded to provide location and scale information, while dense optical flow is encoded to provide pixel-level information about vehicle motion, scale change, and appearance. Our decoder can also consider information about future ego-motion, which could be available from the planner of an intelligent vehicle. The decoder generates hypothesized future bounding boxes by temporally updating from the encoded hidden state.\n\nA. Temporal Modeling 1) Location-Scale Encoding: One straightforward approach to predict the future location of an object is to extrapolate a future trajectory from the past. However, in perspective images, physical object location is reflected by both its pixel location and scale. For example, a vehicle located at the center of an image could be a nearby lead vehicle or a distant vehicle across the intersection, and such a difference could cause a completely different future motion. Therefore, this paper predicts both the location and scale of participant vehicles, i.e., their bounding boxes. The scale information is also able to represent depth (distance) as well as vehicle orientation, given that distant vehicles tend to have smaller bounding boxes and crossing vehicles tend to have larger aspect ratios.\n\n2) Motion-Appearance Encoding: Another important cue for predicting a vehicle's future location is pixel-level information about motion and appearance. Optical flow is widely used as a pattern of relative motion in a scene. For each feature point, optical flow gives an estimate of a vector [u, v] that describes its relative motion from one frame to the next caused by the motion of the object and the camera. Compared to sparse optical flow obtained from traditional methods such as Lucas-Kanade [30], dense optical flow offers an estimate at every pixel, so that moving objects can be distinguished from the background. Also, dense optical flow captures object appearance changes, since different object pixels may have different flows, as shown in the left part of Fig. 2.\n\nIn this paper, object vehicle features are extracted by a region-of-interest pooling (ROIPooling) operation using bilinear interpolation from the optical flow map. The ROI region is expanded from the bounding box to contain contextual information around the object, so that its relative motion with respect to the environment is also encoded. The resulting relative motion vector is represented as O t = [u 1 , v 1 , u 2 , v 2 , ...u n , v n ] t , where n is the size of the pooled region.\n\nWe use two encoders for temporal modeling of each input stream and apply the late fusion method:\nh X t = GRU X (\u03c6 X (X t\u22121 ), h X t\u22121 ; \u03b8 X ) (1a) h O t = GRU O (\u03c6 O (O t\u22121 ), h O t\u22121 ; \u03b8 O ) (1b) H = \u03c6 H (Average(h X t0 , h O t0 ))(1c)\nwhere GRU represents the gated recurrent units [31] with parameter \u03b8, \u03c6(\u00b7) are linear projections with ReLU activations, and h x t and h o t are the hidden state vectors of the GRU models at time t.\n\n\nB. Future Ego-Motion Cue\n\nAwareness of future ego-motion is essential to predicting the future location of participant vehicles. For autonomous vehicles, it is reasonable to assume that motion planning (e.g. trajectory generation) is available [32], so that the future pose of the ego vehicle can be used to aid in predicting the relative position of nearby vehicles. Planned ego-vehicle motion information may also help anticipate motion caused by interactions between vehicles: the ego-vehicle turning left at intersection may result in other vehicles stopping to yield or accelerating to pass, for example.\n\nIn this paper, the future ego motion is represented by 2D rotation matrices R t+1 t \u2208 R 2\u00d72 and translation vectors T t+1 t \u2208 R 2 [24], which together describe the transformation of the camera coordinate frame from time t to t + 1. The relative, pairwise transformations between frames can be composed to estimate transformations across the prediction horizon from the current frame:\nR t0+i t0 = t0+i\u22121 t=t0 R t+1 t (2a) T t0+i t0 = T t0+i\u22121 t0 + R t0+i\u22121 t0 T t0+i t0+i\u22121 (2b)\nThe future ego-motion feature is represented by a vector E t = [\u03c8 t t0 , x t t0 , z t t0 ], where t > t 0 , \u03c8 t t0 is the yaw angle extracted from R t t0 , and x t t0 and z t t0 are translations from the coordinate frame at time t 0 . We use a right-handed coordinate fixed to ego vehicle, where vehicle heading aligns with positive x. Estimated future motion is then used as input to the trajectory decoding model.\n\n\nC. Future Location-Scale Decoding\n\nWe use another GRU for decoding future bounding boxes. The decoder hidden state is initialized from the final fused hidden state of the past bounding box encoder and the optical flow encoder:\nh Y t+1 = GRU Y (h Y t , f (h Y t , E t ); \u03b8 Y ) (3a) Y t0+i \u2212 X t0 = \u03c6 out (h Y t0+i ) (3b) f (h Y t , E t ) = Average(\u03c6 Y (h Y t ), \u03c6 e (E t )) (3c)\nwhere h Y t is the decoder's hidden state, h Y t0 = H is the initial hidden state of the decoder, and \u03c6(\u00b7) are linear projections with ReLU activations applied for domain transfer. Instead of directly generating the future bounding boxes Y, our RNN decoder generates the relative location and scale of the future bounding box from the current frame as in (3b), similar to [24]. In this way, the model output is shifted to have zero initial, which improves the performance.  \n\n\nIV. EXPERIMENTS\n\n\nA. Dataset\n\nThe problem of future vehicle localization in egocentric cameras is particularly challenging when multiple vehicles execute different motions (e.g. ego-vehicle is turning left but yields to another moving car). However, to the best of our knowledge, most existing autonomous driving datasets are proposed for scene understanding tasks [33], [34] that do not contain much diverse motion. This paper introduces a new egocentric vision dataset, the Honda Egocentric View-Intersection (HEV-I) data, that focuses on intersection scenarios where vehicles exhibit diverse motions due to complex road layouts and vehicle interactions. HEV-I was collected from different intersection types in the San Francisco Bay Area, and consists of 230 video clips each ranging between 10 to 60 seconds. Videos were captured by an RGB camera mounted on the windshield of the car, with 1920 \u00d7 1200 resolution (reduced to 1280 \u00d7 640 in this paper) at 10 frames per second (fps). Following prior work [24], we first detected vehicles by using Mask-RCNN [35] pre-trained on the COCO dataset. We then used Sort [36] with a Kalman filter for multiple object tracking over each video. In first-person videos, the duration of vehicles can be extremely short due to high relative motion and limited fields of view. On the other hand, vehicles at stop signs or traffic lights do not move at all over a short period. In our dataset, we found a sample of 2 seconds length is reasonable for including many vehicles while maintaining reasonable travel lengths. We use the past 1 second of observation data as input to predict the bounding boxes of vehicles for the next 1 second. We randomly split   Fig. 3. As shown, most vehicle tracklets are short in Fig. 3 (a) because vehicles usually drive fast and thus leave the field of the first-person view quickly. Fig. 3 (b) shows the distribution of ego vehicle yaw angle (in rad) across all videos, where positive indicates turning left and negative indicates turning right. It can be seen that HEV-I contains a variety of different ego motions. Distributions of training and test sample trajectory lengths (in pixels) are presented in Fig. 3 (c) and (d). Although most lengths are shorter than 100 pixels, the dataset also contains plenty of longer trajectories. This is important since usually the longer the trajectory is, the more difficult it is to predict. Compared to existing data like KITTI, the HEV-I dataset contains more videos and vehicles, as shown in Table I. Most object vehicles in KITTI are parked on the road or driving in the same direction on highways, while in HEV-I, all vehicles are at intersections and performing diverse maneuvers.\n\n\nB. Implementation Details\n\nWe compute dense optical flow using Flownet2.0 [37] and use a 5 \u00d7 5 ROIPooling operator to produce the final flattened feature vector O t \u2208 R 50 . ORB-SLAM2 [38] is used to estimate ego-vehicle motion from first-person videos.\n\nWe use Keras with TensorFlow backend [39] to implement our model and perform training and experiments on a system with Nvidia Tesla P100 GPUs. We use the gated recurrent unit (GRU) [40] as basic RNN cell. Compared to long shortterm memory (LSTM) [41], GRU has fewer parameters, which makes it faster without affecting performance [42]. The hidden state size of our encoder and decoder GRUs is 512. We use the Adam [43] optimizer to learn network  parameters with fixed learning rate 0.0005 and batch size 64. Training is terminated after 40 epochs and the best models are selected.\n\n\nC. Baselines and Metrics\n\nBaselines. We compare the performance of the proposed method with several baselines:\n\nLinear regression (Linear) extrapolates future bounding boxes by assuming the location and scale change are linear.\n\nConstant Acceleration (ConstAccel) assumes the object has constant horizontal and vertical acceleration in the camera frame, i.e. that the second-order derivatives of X are constant values.\n\nConv1D is adapted from [24], by replacing the locationscale and pose input streams with past bounding boxes and dense optical flow.\n\nTo evaluate the contribution of each component of our model, we also implemented multiple simpler baselines for ablation studies:\n\nRNN-ED-X is an RNN encoder-decoder with only past bounding boxes as inputs.\n\nRNN-ED-XE builds on RNN-ED-X but also incorporates future ego-motion as decoder inputs.\n\nRNN-ED-XO is a two-stream RNN encoder-decoder model with past bounding boxes and optical flow as inputs.\n\nRNN-ED-XOE is our best model as shown in Fig.2 with awareness of future ego-motion.\n\nEvaluation Metrics. To evaluate location prediction, we use final displacement error (FDE) [24] and average displacement error (ADE) [26], where ADE emphasizes more on the overall prediction accuracy along the horizon. To evaluate bounding box prediction, we propose another metric called final intersection over union (FIOU) that measures overlap between the predicted bounding box and ground truth at the final frame.\n\n\nD. Results on HEV-I Dataset\n\nQuantitative Results. As shown in Table II, we split the testing dataset into easy and challenging cases based on the FDE performance of the ConstAccel baseline. A sample is classified as easy if the ConstAccel achieves FDE lower than the average FDE (58.00), otherwise it is classified as challenging. Intuitively, easy cases include target vehicles that are stationary or whose future locations can be easily propagated from the past, while challenging cases usually involve diverse and intense motion, e.g. the target vehicle suddenly accelerates or brakes. In evaluation, we report the results of easy and challenging cases, as well as the overall results on all testing samples.\n\nOur best method (RNN-ED-XOE) significantly outperforms naive baselines including Linear and ConstAccel on all cases (FDE of 24.92 vs. 72.37 vs. 58.00). It also improves about 15% from the state-of-the-art Conv1D baseline. The improvement on challenging cases is more significant since future trajectories are complex and temporal modeling is more difficult. To more fairly compare the capability of RNN-ED and convolution-deconvolution models, we compare RNN-ED-XO with Conv1D. These two methods use the same features as inputs to predict future vehicle bounding boxes, but rely on different temporal modeling frameworks. The results (FDE of 25.56 vs 29.06) suggest that the RNN-ED architecture offers better temporal modeling compared to Conv1D, because the convolution-deconvolution model generates future trajectory in one shot while the RNN-ED model generates a new prediction based on the previous hidden state. Ablation studies also show that dense optical flow features are essential to accurate prediction of future bounding boxes, especially for challenging cases. The FDE is reduced from 34.04 to 25.56 by adding optical flow stream (RNN-ED-XO) to RNN-ED-X model. By using future egomotion, performance can be further improved, as shown in the last row of Table II. Qualitative Results. Fig. 4 shows four sample results of our best model (in green) and the Conv1D baseline (in blue). Each row represents one test sample and each column corresponds to each time step. The past and prediction views are separated by the yellow vertical line. Example (a) shows a case where the initial bounding box is noisy because it is close to the image boundary, and our results are more accurate than those of Conv1D. Example (b) shows how our model, with awareness of future ego-motion, can predict object future location more accurately while the baseline model predicts future location in the wrong direction. Examples (c) and (d) show that for a curved or long trajectory, our model provides better temporal modelling than Conv1D. These results are consistent with our evaluation observations. Failure Cases. Although our proposed method generally performs well, there are still limitations. Fig.5 (a) shows a case when the ground truth future path is curved due to uneven road surface, which our method fails to consider. In Fig.5 (b), the target vehicle is occluded by pedestrians moving in the opposite direction, which creates misleading optical flow that leads to an inaccurate bounding box (especially in t = t 0 frame). Future work could avoid this type of error by better modeling the entire traffic scene as well as relations between traffic participants.\n\n\nE. Results on KITTI Dataset\n\nWe also evaluate our method on a 38-video subset of the KITTI raw dataset, including city, road and residential scenarios. Compared to HEV-I, the road surface of KITTI is more uneven and vehicles are mostly parked on the side of the road with occlusions. Another difference is that in HEV-I, the ego-vehicle often stops at intersections to yield to other vehicles, resulting in static samples with no motion at all. We did not remove static samples from the dataset since predicting a static object is also valuable.\n\nTo evaluate our method on KITTI, we first generate the input features following the same process of HEV-I dataset, resulting in \u223c 8000 training and \u223c 2700 testing samples. Performance of baselines and our best model are shown in Table III. Both learning-based models are trained for 40 epoches and the best models are selected. The results show that our method outperforms all baselines including the stateof-the-art Conv1D (FDE of 37.11 vs 78.19 vs 55.66 vs 44.13). We also observe that both learning-based methods did not perform as well as they did on HEV-I. One possible reason is that KITTI is much smaller so that the models are not fully trained. In general, we conclude that the use of the proposed framework results in more robust future vehicle localization across different datasets.\n\n\nV. CONCLUSION\n\nWe proposed the new problem of predicting the relative location and scale of target vehicles in first-person video. We presented a new dataset collected from intersection scenarios to include as many vehicles and motion as possible. Our proposed multi-stream RNN encoder-decoder structure with awareness of future ego motion shows promising results compared to other baselines on our dataset as well as on KITTI, and we tested how each component contributed to the model through an ablation study.\n\nFuture work includes incorporating evidence from scene context, traffic signs/signals, depth data, and other vehicle-environment interactions. Social relationships such as vehicle-to-vehicle and vehicle-to-pedestrian interactions could also be considered.\n\nFig. 1 :\n1Illustration of future vehicle localization. Location and scale are represented as bounding boxes in predictions.\n\nFig. 2 :\n2The proposed future vehicle localization framework (better in color).\n\n\nset trajectory length [# of pixels] (d) Testing set trajectory length [# of pixels]\n\nFig. 3 :\n3HEV-I dataset statistics.\n\nFig. 4 :\n4Qualitative results on HEV-I dataset (better in color).\n\nFig. 5 :\n5Failure cases on HEV-I dataset (better in color).\n\n\nRobotics Institute, University of Michigan, Ann Arbor, MI 48109, USA. {brianyao,ematkins}@umich.edu School of Informatics, Computing and Engineering, Indiana University, Bloomington, IN 47408, USA. {mx6,djcran}@indiana.edu 3 Honda Research Institute, Mountain View, CA 94043, USA. {cchoi, bdariush}@honda-ri.com1 2 \n\nTABLE I :\nIComparison with KITTI dataset. The number of vehicles is tallied after filtering out short sequences.Dataset \n# videos \n# vehicles \nscene types \n\nKITTI \n38 \n541 \nresidential, highway, city road \nHEV-I \n230 \n2477 \nurban intersections \n\n\n\nTABLE II :\nIIQuantitative results of proposed methods and baselines on HEV-I dataset with metrics FDE/ADE/FIOU.Models \nEasy Cases \nChallenging Cases \nAll Cases \n\nLinear \n31.49 / 17.04 / 0.68 107.93 / 56.29 / 0.33 72.37 / 38.04 / 0.50 \n\nConstAccel \n20.82 / 13.86 / 0.74 90.33 / 49.06 / 0.35 58.00 / 28.05 / 0.53 \n\nConv1D [24] \n18.84 / 12.09 / 0.75 37.95 / 20.97 / 0.64 29.06 / 16.84 / 0.69 \n\nRNN-ED-X \n23.57 / 11.96 / 0.74 43.15 / 22.24 / 0.60 34.04 / 17.46 / 0.67 \n\nRNN-ED-XE \n22.28 / 11.60 / 0.74 42.27 / 22.39 / 0.61 32.97 / 17.37 / 0.67 \n\nRNN-ED-XO \n17.45 / 8.68 / 0.78 32.61 / 16.72 / 0.66 25.56 / 12.98 / 0.72 \n\nRNN-ED-XOE 16.72 / 8.52 / 0.80 32.05 / 16.63 / 0.66 24.92 / 12.86 / 0.73 \n\n\n\nTABLE III :\nIIIQuantitative results on KITTI dataset. We compare our best model with baselines for simplicity.Models \nFDE ADE FIOU \n\nLinear \n78.19 38.21 0.33 \n\nConstAccel 55.66 25.78 0.39 \n\nConv1D [24] 44.13 24.38 0.49 \n\nOurs \n37.11 17.88 0.53 \n\nthe training (70%) and testing (30%) videos, resulting in \n\u223c 40, 000 training and \u223c 17, 000 testing samples. \nStatistics of HEV-I are shown in \n\nHow would surround vehicles move? a unified framework for maneuver classification and motion prediction. N Deo, A Rangesh, M M Trivedi, IEEE Transactions on Intelligent Vehicles. N. Deo, A. Rangesh, and M. M. Trivedi, \"How would surround vehicles move? a unified framework for maneuver classification and motion prediction,\" IEEE Transactions on Intelligent Vehicles, 2018.\n\nConvolutional social pooling for vehicle trajectory prediction. N Deo, M M Trivedi, arXiv:1805.06771N. Deo and M. M. Trivedi, \"Convolutional social pooling for vehicle trajectory prediction,\" arXiv:1805.06771, 2018.\n\nDesire: Distant future prediction in dynamic scenes with interacting agents. N Lee, W Choi, P Vernaza, C B Choy, P H Torr, M Chandraker, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. Torr, and M. Chan- draker, \"Desire: Distant future prediction in dynamic scenes with interacting agents,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nDelving into egocentric actions. Y Li, Z Ye, J M Rehg, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Y. Li, Z. Ye, and J. M. Rehg, \"Delving into egocentric actions,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nGoing deeper into first-person activity recognition. M Ma, H Fan, K M Kitani, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). M. Ma, H. Fan, and K. M. Kitani, \"Going deeper into first-person activity recognition,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N S\u00fcnderhauf, I Reid, S Gould, A Van Den, Hengel, arXiv:1711.07280P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and A. van den Hengel, \"Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,\" arXiv:1711.07280, 2017.\n\nEmbodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, arXiv:1711.11543A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, \"Embodied question answering,\" arXiv:1711.11543, 2017.\n\nActive object perceiver: Recognition-guided policy learning for object searching on mobile robots. X Ye, Z Lin, H Li, S Zheng, Y Yang, arXiv:1807.11174X. Ye, Z. Lin, H. Li, S. Zheng, and Y. Yang, \"Active object perceiver: Recognition-guided policy learning for object searching on mobile robots,\" arXiv:1807.11174, 2018.\n\nEnd-to-end driving via conditional imitation learning. F Codevilla, M M\u00fcller, A Dosovitskiy, A L\u00f3pez, V Koltun, arXiv:1710.02410F. Codevilla, M. M\u00fcller, A. Dosovitskiy, A. L\u00f3pez, and V. Koltun, \"End-to-end driving via conditional imitation learning,\" arXiv:1710.02410, 2017.\n\nUnderstanding egocentric activities. A Fathi, A Farhadi, J M Rehg, IEEE International Conference on Computer Vision (ICCV). A. Fathi, A. Farhadi, and J. M. Rehg, \"Understanding egocentric activities,\" in IEEE International Conference on Computer Vision (ICCV), 2011.\n\nFast unsupervised ego-action learning for first-person sports videos. K M Kitani, T Okabe, Y Sato, A Sugimoto, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. M. Kitani, T. Okabe, Y. Sato, and A. Sugimoto, \"Fast unsupervised ego-action learning for first-person sports videos,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.\n\nDetecting activities of daily living in first-person camera views. H Pirsiavash, D Ramanan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). H. Pirsiavash and D. Ramanan, \"Detecting activities of daily living in first-person camera views,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nPredicting important objects for egocentric video summarization. Y J Lee, K Grauman, International Journal of Computer Vision. Y. J. Lee and K. Grauman, \"Predicting important objects for egocen- tric video summarization,\" International Journal of Computer Vision (IJCV), 2015.\n\nFirst person actionobject detection with egonet. G Bertasius, H S Park, S X Yu, J Shi, arXiv:1603.04908G. Bertasius, H. S. Park, S. X. Yu, and J. Shi, \"First person action- object detection with egonet,\" arXiv:1603.04908, 2016.\n\nEgo2Top: Matching viewers in egocentric and top-view videos. S Ardeshir, A Borji, European Conference on Computer Vision (ECCV). S. Ardeshir and A. Borji, \"Ego2Top: Matching viewers in egocentric and top-view videos,\" in European Conference on Computer Vision (ECCV), 2016.\n\nIdentifying first-person camera wearers in third-person videos. C Fan, J Lee, M Xu, K K Singh, Y J Lee, D J Crandall, M S Ryoo, arXiv:1704.06340C. Fan, J. Lee, M. Xu, K. K. Singh, Y. J. Lee, D. J. Crandall, and M. S. Ryoo, \"Identifying first-person camera wearers in third-person videos,\" arXiv:1704.06340, 2017.\n\nJoint person segmentation and identification in synchronized first-and third-person videos. M Xu, C Fan, Y Wang, M S Ryoo, D J Crandall, arXiv:1803.11217M. Xu, C. Fan, Y. Wang, M. S. Ryoo, and D. J. Crandall, \"Joint person segmentation and identification in synchronized first-and third-person videos,\" arXiv:1803.11217, 2018.\n\nDiscovering important people and objects for egocentric video summarization. Y J Lee, J Ghosh, K Grauman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Y. J. Lee, J. Ghosh, and K. Grauman, \"Discovering important people and objects for egocentric video summarization,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nLearning to predict gaze in egocentric video. Y Li, A Fathi, J M Rehg, IEEE International Conference on Computer Vision (ICCV). Y. Li, A. Fathi, and J. M. Rehg, \"Learning to predict gaze in egocentric video,\" in IEEE International Conference on Computer Vision (ICCV), 2013.\n\nEgocentric future localization. H Soo Park, J.-J Hwang, Y Niu, J Shi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). H. Soo Park, J.-J. Hwang, Y. Niu, and J. Shi, \"Egocentric future localization,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nPredicting behaviors of basketball players from first person videos. S Su, J P Hong, J Shi, H S Park, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. S. Su, J. P. Hong, J. Shi, and H. S. Park, \"Predicting behaviors of basketball players from first person videos,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nEgocentric basketball motion planning from a single first-person image. G Bertasius, A Chan, J Shi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). G. Bertasius, A. Chan, and J. Shi, \"Egocentric basketball motion planning from a single first-person image,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nLong-term on-board prediction of people in traffic scenes under uncertainty. A Bhattacharyya, M Fritz, B Schiele, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Bhattacharyya, M. Fritz, and B. Schiele, \"Long-term on-board prediction of people in traffic scenes under uncertainty,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nFuture person localization in first-person videos. T Yagi, K Mangalam, R Yonetani, Y Sato, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). T. Yagi, K. Mangalam, R. Yonetani, and Y. Sato, \"Future person localization in first-person videos,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nProbabilistic trajectory prediction with gaussian mixture models. J Wiest, M H\u00f6ffken, U Kre\u00dfel, K Dietmayer, IEEE Intelligent Vehicles Symposium (IV). J. Wiest, M. H\u00f6ffken, U. Kre\u00dfel, and K. Dietmayer, \"Probabilistic tra- jectory prediction with gaussian mixture models,\" in IEEE Intelligent Vehicles Symposium (IV), 2012.\n\nSocial LSTM: Human trajectory prediction in crowded spaces. A Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei, S Savarese, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese, \"Social LSTM: Human trajectory prediction in crowded spaces,\" in IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), 2016.\n\nSocial GAN: Socially acceptable trajectories with generative adversarial networks. A Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, \"Social GAN: Socially acceptable trajectories with generative adversarial networks,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nCar-Net: Clairvoyant attentive recurrent network. A Sadeghian, F Legros, M Voisin, R Vesel, A Alahi, S Savarese, European Conference on Computer Vision (ECCV). A. Sadeghian, F. Legros, M. Voisin, R. Vesel, A. Alahi, and S. Savarese, \"Car-Net: Clairvoyant attentive recurrent network,\" in European Conference on Computer Vision (ECCV), 2018.\n\nSophie: An attentive gan for predicting paths compliant to social and physical constraints. A Sadeghian, V Kosaraju, A Sadeghian, N Hirose, S Savarese, arXiv:1806.01482A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, and S. Savarese, \"Sophie: An attentive gan for predicting paths compliant to social and physical constraints,\" arXiv:1806.01482, 2018.\n\nAn iterative image registration technique with an application to stereo vision. B D Lucas, T Kanade, B. D. Lucas, T. Kanade et al., \"An iterative image registration technique with an application to stereo vision,\" 1981.\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merri\u00ebnboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, arXiv:1406.1078K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \"Learning phrase representations using rnn encoder-decoder for statistical machine translation,\" arXiv:1406.1078, 2014.\n\nA review of motion planning techniques for automated vehicles. D Gonz\u00e1lez, J P\u00e9rez, V Milan\u00e9s, F Nashashibi, IEEE Trans. Intelligent Transportation Systems. 174D. Gonz\u00e1lez, J. P\u00e9rez, V. Milan\u00e9s, and F. Nashashibi, \"A review of motion planning techniques for automated vehicles.\" IEEE Trans. Intelligent Transportation Systems, vol. 17, no. 4, pp. 1135-1145, 2016.\n\nVision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, International Journal of Robotics Research. A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \"Vision meets robotics: The kitti dataset,\" International Journal of Robotics Research (IJRR), 2013.\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be- nenson, U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban scene understanding,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nK He, G Gkioxari, P Dollar, R Girshick, IEEE International Conference on Computer Vision (ICCV. K. He, G. Gkioxari, P. Dollar, and R. Girshick, \"Mask R-CNN,\" in IEEE International Conference on Computer Vision (ICCV), 2017.\n\nSimple online and realtime tracking. A Bewley, Z Ge, L Ott, F Ramos, B Upcroft, IEEE International Conference on Image Processing (ICIP). A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, \"Simple online and realtime tracking,\" in IEEE International Conference on Image Processing (ICIP), 2016.\n\nFlowNet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, \"FlowNet 2.0: Evolution of optical flow estimation with deep net- works,\" in IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), 2017.\n\nORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-d cameras. R Mur-Artal, J D Tardos, IEEE Transactions on Robotics. R. Mur-Artal and J. D. Tardos, \"ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-d cameras,\" IEEE Transactions on Robotics, 2017.\n\nTensorflow: a system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., \"Tensorflow: a system for large-scale machine learning.\"\n\nGated feedback recurrent neural networks. J Chung, C Gulcehre, K Cho, Y Bengio, International Conference on Machine Learning (ICML). J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \"Gated feedback recurrent neural networks,\" in International Conference on Machine Learning (ICML), 2015.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, 1997.\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.3555J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \"Empirical eval- uation of gated recurrent neural networks on sequence modeling,\" arXiv:1412.3555, 2014.\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980D. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv:1412.6980, 2014.\n", "annotations": {"author": "[{\"end\":105,\"start\":98},{\"end\":116,\"start\":106},{\"end\":128,\"start\":117},{\"end\":146,\"start\":129},{\"end\":161,\"start\":147},{\"end\":177,\"start\":162}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":101},{\"end\":115,\"start\":113},{\"end\":127,\"start\":123},{\"end\":145,\"start\":137},{\"end\":160,\"start\":154},{\"end\":176,\"start\":169}]", "author_first_name": "[{\"end\":100,\"start\":98},{\"end\":112,\"start\":106},{\"end\":122,\"start\":117},{\"end\":134,\"start\":129},{\"end\":136,\"start\":135},{\"end\":151,\"start\":147},{\"end\":153,\"start\":152},{\"end\":168,\"start\":162}]", "author_affiliation": null, "title": "[{\"end\":95,\"start\":1},{\"end\":272,\"start\":178}]", "venue": null, "abstract": "[{\"end\":1446,\"start\":274}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1857,\"start\":1854},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1862,\"start\":1859},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2676,\"start\":2673},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2681,\"start\":2678},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2697,\"start\":2694},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2702,\"start\":2699},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2741,\"start\":2738},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2875,\"start\":2872},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2880,\"start\":2877},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3528,\"start\":3525},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3533,\"start\":3530},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4893,\"start\":4890},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4898,\"start\":4895},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4904,\"start\":4900},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4910,\"start\":4906},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4933,\"start\":4929},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4939,\"start\":4935},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4967,\"start\":4963},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4973,\"start\":4969},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4999,\"start\":4995},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5027,\"start\":5023},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5152,\"start\":5148},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5253,\"start\":5249},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5399,\"start\":5395},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5711,\"start\":5707},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5953,\"start\":5949},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6626,\"start\":6623},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6632,\"start\":6628},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6973,\"start\":6969},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7132,\"start\":7128},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7285,\"start\":7282},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7431,\"start\":7427},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7437,\"start\":7433},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7453,\"start\":7450},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11502,\"start\":11498},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12557,\"start\":12553},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12955,\"start\":12951},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13452,\"start\":13448},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14968,\"start\":14964},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15438,\"start\":15434},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15444,\"start\":15440},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16080,\"start\":16076},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16132,\"start\":16128},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16188,\"start\":16184},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17850,\"start\":17846},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17960,\"start\":17956},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18068,\"start\":18064},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18212,\"start\":18208},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18277,\"start\":18273},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18361,\"start\":18357},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18445,\"start\":18441},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19058,\"start\":19054},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19747,\"start\":19743},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19789,\"start\":19785}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25693,\"start\":25569},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25774,\"start\":25694},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25860,\"start\":25775},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25897,\"start\":25861},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25964,\"start\":25898},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26025,\"start\":25965},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26343,\"start\":26026},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26591,\"start\":26344},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27285,\"start\":26592},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27676,\"start\":27286}]", "paragraph": "[{\"end\":2205,\"start\":1465},{\"end\":3031,\"start\":2207},{\"end\":3650,\"start\":3033},{\"end\":4578,\"start\":3652},{\"end\":5548,\"start\":4599},{\"end\":6501,\"start\":5550},{\"end\":7604,\"start\":6503},{\"end\":8543,\"start\":7606},{\"end\":8977,\"start\":8606},{\"end\":9084,\"start\":8979},{\"end\":9560,\"start\":9166},{\"end\":10178,\"start\":9562},{\"end\":10998,\"start\":10180},{\"end\":11776,\"start\":11000},{\"end\":12267,\"start\":11778},{\"end\":12365,\"start\":12269},{\"end\":12704,\"start\":12506},{\"end\":13316,\"start\":12733},{\"end\":13701,\"start\":13318},{\"end\":14211,\"start\":13796},{\"end\":14440,\"start\":14249},{\"end\":15066,\"start\":14592},{\"end\":17769,\"start\":15099},{\"end\":18025,\"start\":17799},{\"end\":18608,\"start\":18027},{\"end\":18721,\"start\":18637},{\"end\":18838,\"start\":18723},{\"end\":19029,\"start\":18840},{\"end\":19162,\"start\":19031},{\"end\":19293,\"start\":19164},{\"end\":19370,\"start\":19295},{\"end\":19459,\"start\":19372},{\"end\":19565,\"start\":19461},{\"end\":19650,\"start\":19567},{\"end\":20071,\"start\":19652},{\"end\":20786,\"start\":20103},{\"end\":23452,\"start\":20788},{\"end\":24000,\"start\":23484},{\"end\":24796,\"start\":24002},{\"end\":25311,\"start\":24814},{\"end\":25568,\"start\":25313}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9165,\"start\":9085},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12505,\"start\":12366},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13795,\"start\":13702},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14591,\"start\":14441}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17585,\"start\":17578},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20145,\"start\":20137},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22063,\"start\":22054},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24240,\"start\":24231}]", "section_header": "[{\"end\":1463,\"start\":1448},{\"end\":4597,\"start\":4581},{\"end\":8583,\"start\":8546},{\"end\":8604,\"start\":8586},{\"end\":12731,\"start\":12707},{\"end\":14247,\"start\":14214},{\"end\":15084,\"start\":15069},{\"end\":15097,\"start\":15087},{\"end\":17797,\"start\":17772},{\"end\":18635,\"start\":18611},{\"end\":20101,\"start\":20074},{\"end\":23482,\"start\":23455},{\"end\":24812,\"start\":24799},{\"end\":25578,\"start\":25570},{\"end\":25703,\"start\":25695},{\"end\":25870,\"start\":25862},{\"end\":25907,\"start\":25899},{\"end\":25974,\"start\":25966},{\"end\":26354,\"start\":26345},{\"end\":26603,\"start\":26593},{\"end\":27298,\"start\":27287}]", "table": "[{\"end\":26343,\"start\":26339},{\"end\":26591,\"start\":26457},{\"end\":27285,\"start\":26704},{\"end\":27676,\"start\":27397}]", "figure_caption": "[{\"end\":25693,\"start\":25580},{\"end\":25774,\"start\":25705},{\"end\":25860,\"start\":25777},{\"end\":25897,\"start\":25872},{\"end\":25964,\"start\":25909},{\"end\":26025,\"start\":25976},{\"end\":26339,\"start\":26028},{\"end\":26457,\"start\":26356},{\"end\":26704,\"start\":26606},{\"end\":27397,\"start\":27302}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3183,\"start\":3175},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9720,\"start\":9712},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11775,\"start\":11769},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16770,\"start\":16764},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16828,\"start\":16818},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16934,\"start\":16924},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17258,\"start\":17248},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19613,\"start\":19608},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22091,\"start\":22085},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22989,\"start\":22980},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23123,\"start\":23114}]", "bib_author_first_name": "[{\"end\":27784,\"start\":27783},{\"end\":27791,\"start\":27790},{\"end\":27802,\"start\":27801},{\"end\":27804,\"start\":27803},{\"end\":28118,\"start\":28117},{\"end\":28125,\"start\":28124},{\"end\":28127,\"start\":28126},{\"end\":28348,\"start\":28347},{\"end\":28355,\"start\":28354},{\"end\":28363,\"start\":28362},{\"end\":28374,\"start\":28373},{\"end\":28376,\"start\":28375},{\"end\":28384,\"start\":28383},{\"end\":28386,\"start\":28385},{\"end\":28394,\"start\":28393},{\"end\":28737,\"start\":28736},{\"end\":28743,\"start\":28742},{\"end\":28749,\"start\":28748},{\"end\":28751,\"start\":28750},{\"end\":29021,\"start\":29020},{\"end\":29027,\"start\":29026},{\"end\":29034,\"start\":29033},{\"end\":29036,\"start\":29035},{\"end\":29387,\"start\":29386},{\"end\":29399,\"start\":29398},{\"end\":29405,\"start\":29404},{\"end\":29414,\"start\":29413},{\"end\":29423,\"start\":29422},{\"end\":29434,\"start\":29433},{\"end\":29448,\"start\":29447},{\"end\":29456,\"start\":29455},{\"end\":29465,\"start\":29464},{\"end\":29774,\"start\":29773},{\"end\":29781,\"start\":29780},{\"end\":29790,\"start\":29789},{\"end\":29802,\"start\":29801},{\"end\":29809,\"start\":29808},{\"end\":29819,\"start\":29818},{\"end\":30063,\"start\":30062},{\"end\":30069,\"start\":30068},{\"end\":30076,\"start\":30075},{\"end\":30082,\"start\":30081},{\"end\":30091,\"start\":30090},{\"end\":30341,\"start\":30340},{\"end\":30354,\"start\":30353},{\"end\":30364,\"start\":30363},{\"end\":30379,\"start\":30378},{\"end\":30388,\"start\":30387},{\"end\":30599,\"start\":30598},{\"end\":30608,\"start\":30607},{\"end\":30619,\"start\":30618},{\"end\":30621,\"start\":30620},{\"end\":30900,\"start\":30899},{\"end\":30902,\"start\":30901},{\"end\":30912,\"start\":30911},{\"end\":30921,\"start\":30920},{\"end\":30929,\"start\":30928},{\"end\":31274,\"start\":31273},{\"end\":31288,\"start\":31287},{\"end\":31607,\"start\":31606},{\"end\":31609,\"start\":31608},{\"end\":31616,\"start\":31615},{\"end\":31869,\"start\":31868},{\"end\":31882,\"start\":31881},{\"end\":31884,\"start\":31883},{\"end\":31892,\"start\":31891},{\"end\":31894,\"start\":31893},{\"end\":31900,\"start\":31899},{\"end\":32110,\"start\":32109},{\"end\":32122,\"start\":32121},{\"end\":32388,\"start\":32387},{\"end\":32395,\"start\":32394},{\"end\":32402,\"start\":32401},{\"end\":32408,\"start\":32407},{\"end\":32410,\"start\":32409},{\"end\":32419,\"start\":32418},{\"end\":32421,\"start\":32420},{\"end\":32428,\"start\":32427},{\"end\":32430,\"start\":32429},{\"end\":32442,\"start\":32441},{\"end\":32444,\"start\":32443},{\"end\":32730,\"start\":32729},{\"end\":32736,\"start\":32735},{\"end\":32743,\"start\":32742},{\"end\":32751,\"start\":32750},{\"end\":32753,\"start\":32752},{\"end\":32761,\"start\":32760},{\"end\":32763,\"start\":32762},{\"end\":33043,\"start\":33042},{\"end\":33045,\"start\":33044},{\"end\":33052,\"start\":33051},{\"end\":33061,\"start\":33060},{\"end\":33378,\"start\":33377},{\"end\":33384,\"start\":33383},{\"end\":33393,\"start\":33392},{\"end\":33395,\"start\":33394},{\"end\":33640,\"start\":33639},{\"end\":33644,\"start\":33641},{\"end\":33655,\"start\":33651},{\"end\":33664,\"start\":33663},{\"end\":33671,\"start\":33670},{\"end\":33971,\"start\":33970},{\"end\":33977,\"start\":33976},{\"end\":33979,\"start\":33978},{\"end\":33987,\"start\":33986},{\"end\":33994,\"start\":33993},{\"end\":33996,\"start\":33995},{\"end\":34333,\"start\":34332},{\"end\":34346,\"start\":34345},{\"end\":34354,\"start\":34353},{\"end\":34691,\"start\":34690},{\"end\":34708,\"start\":34707},{\"end\":34717,\"start\":34716},{\"end\":35046,\"start\":35045},{\"end\":35054,\"start\":35053},{\"end\":35066,\"start\":35065},{\"end\":35078,\"start\":35077},{\"end\":35397,\"start\":35396},{\"end\":35406,\"start\":35405},{\"end\":35417,\"start\":35416},{\"end\":35427,\"start\":35426},{\"end\":35715,\"start\":35714},{\"end\":35724,\"start\":35723},{\"end\":35732,\"start\":35731},{\"end\":35746,\"start\":35745},{\"end\":35759,\"start\":35758},{\"end\":35770,\"start\":35769},{\"end\":36150,\"start\":36149},{\"end\":36159,\"start\":36158},{\"end\":36170,\"start\":36169},{\"end\":36181,\"start\":36180},{\"end\":36193,\"start\":36192},{\"end\":36542,\"start\":36541},{\"end\":36555,\"start\":36554},{\"end\":36565,\"start\":36564},{\"end\":36575,\"start\":36574},{\"end\":36584,\"start\":36583},{\"end\":36593,\"start\":36592},{\"end\":36926,\"start\":36925},{\"end\":36939,\"start\":36938},{\"end\":36951,\"start\":36950},{\"end\":36964,\"start\":36963},{\"end\":36974,\"start\":36973},{\"end\":37270,\"start\":37269},{\"end\":37272,\"start\":37271},{\"end\":37281,\"start\":37280},{\"end\":37506,\"start\":37505},{\"end\":37513,\"start\":37512},{\"end\":37532,\"start\":37531},{\"end\":37544,\"start\":37543},{\"end\":37556,\"start\":37555},{\"end\":37568,\"start\":37567},{\"end\":37579,\"start\":37578},{\"end\":37882,\"start\":37881},{\"end\":37894,\"start\":37893},{\"end\":37903,\"start\":37902},{\"end\":37914,\"start\":37913},{\"end\":38226,\"start\":38225},{\"end\":38236,\"start\":38235},{\"end\":38244,\"start\":38243},{\"end\":38255,\"start\":38254},{\"end\":38523,\"start\":38522},{\"end\":38533,\"start\":38532},{\"end\":38542,\"start\":38541},{\"end\":38551,\"start\":38550},{\"end\":38562,\"start\":38561},{\"end\":38575,\"start\":38574},{\"end\":38587,\"start\":38586},{\"end\":38597,\"start\":38596},{\"end\":38605,\"start\":38604},{\"end\":38933,\"start\":38932},{\"end\":38939,\"start\":38938},{\"end\":38951,\"start\":38950},{\"end\":38961,\"start\":38960},{\"end\":39195,\"start\":39194},{\"end\":39205,\"start\":39204},{\"end\":39211,\"start\":39210},{\"end\":39218,\"start\":39217},{\"end\":39227,\"start\":39226},{\"end\":39525,\"start\":39524},{\"end\":39532,\"start\":39531},{\"end\":39541,\"start\":39540},{\"end\":39551,\"start\":39550},{\"end\":39561,\"start\":39560},{\"end\":39576,\"start\":39575},{\"end\":39952,\"start\":39951},{\"end\":39965,\"start\":39964},{\"end\":39967,\"start\":39966},{\"end\":40214,\"start\":40213},{\"end\":40223,\"start\":40222},{\"end\":40233,\"start\":40232},{\"end\":40241,\"start\":40240},{\"end\":40249,\"start\":40248},{\"end\":40258,\"start\":40257},{\"end\":40266,\"start\":40265},{\"end\":40275,\"start\":40274},{\"end\":40287,\"start\":40286},{\"end\":40297,\"start\":40296},{\"end\":40515,\"start\":40514},{\"end\":40524,\"start\":40523},{\"end\":40536,\"start\":40535},{\"end\":40543,\"start\":40542},{\"end\":40783,\"start\":40782},{\"end\":40797,\"start\":40796},{\"end\":40997,\"start\":40996},{\"end\":41006,\"start\":41005},{\"end\":41018,\"start\":41017},{\"end\":41025,\"start\":41024},{\"end\":41246,\"start\":41245},{\"end\":41256,\"start\":41255}]", "bib_author_last_name": "[{\"end\":27788,\"start\":27785},{\"end\":27799,\"start\":27792},{\"end\":27812,\"start\":27805},{\"end\":28122,\"start\":28119},{\"end\":28135,\"start\":28128},{\"end\":28352,\"start\":28349},{\"end\":28360,\"start\":28356},{\"end\":28371,\"start\":28364},{\"end\":28381,\"start\":28377},{\"end\":28391,\"start\":28387},{\"end\":28405,\"start\":28395},{\"end\":28740,\"start\":28738},{\"end\":28746,\"start\":28744},{\"end\":28756,\"start\":28752},{\"end\":29024,\"start\":29022},{\"end\":29031,\"start\":29028},{\"end\":29043,\"start\":29037},{\"end\":29396,\"start\":29388},{\"end\":29402,\"start\":29400},{\"end\":29411,\"start\":29406},{\"end\":29420,\"start\":29415},{\"end\":29431,\"start\":29424},{\"end\":29445,\"start\":29435},{\"end\":29453,\"start\":29449},{\"end\":29462,\"start\":29457},{\"end\":29473,\"start\":29466},{\"end\":29481,\"start\":29475},{\"end\":29778,\"start\":29775},{\"end\":29787,\"start\":29782},{\"end\":29799,\"start\":29791},{\"end\":29806,\"start\":29803},{\"end\":29816,\"start\":29810},{\"end\":29825,\"start\":29820},{\"end\":30066,\"start\":30064},{\"end\":30073,\"start\":30070},{\"end\":30079,\"start\":30077},{\"end\":30088,\"start\":30083},{\"end\":30096,\"start\":30092},{\"end\":30351,\"start\":30342},{\"end\":30361,\"start\":30355},{\"end\":30376,\"start\":30365},{\"end\":30385,\"start\":30380},{\"end\":30395,\"start\":30389},{\"end\":30605,\"start\":30600},{\"end\":30616,\"start\":30609},{\"end\":30626,\"start\":30622},{\"end\":30909,\"start\":30903},{\"end\":30918,\"start\":30913},{\"end\":30926,\"start\":30922},{\"end\":30938,\"start\":30930},{\"end\":31285,\"start\":31275},{\"end\":31296,\"start\":31289},{\"end\":31613,\"start\":31610},{\"end\":31624,\"start\":31617},{\"end\":31879,\"start\":31870},{\"end\":31889,\"start\":31885},{\"end\":31897,\"start\":31895},{\"end\":31904,\"start\":31901},{\"end\":32119,\"start\":32111},{\"end\":32128,\"start\":32123},{\"end\":32392,\"start\":32389},{\"end\":32399,\"start\":32396},{\"end\":32405,\"start\":32403},{\"end\":32416,\"start\":32411},{\"end\":32425,\"start\":32422},{\"end\":32439,\"start\":32431},{\"end\":32449,\"start\":32445},{\"end\":32733,\"start\":32731},{\"end\":32740,\"start\":32737},{\"end\":32748,\"start\":32744},{\"end\":32758,\"start\":32754},{\"end\":32772,\"start\":32764},{\"end\":33049,\"start\":33046},{\"end\":33058,\"start\":33053},{\"end\":33069,\"start\":33062},{\"end\":33381,\"start\":33379},{\"end\":33390,\"start\":33385},{\"end\":33400,\"start\":33396},{\"end\":33649,\"start\":33645},{\"end\":33661,\"start\":33656},{\"end\":33668,\"start\":33665},{\"end\":33675,\"start\":33672},{\"end\":33974,\"start\":33972},{\"end\":33984,\"start\":33980},{\"end\":33991,\"start\":33988},{\"end\":34001,\"start\":33997},{\"end\":34343,\"start\":34334},{\"end\":34351,\"start\":34347},{\"end\":34358,\"start\":34355},{\"end\":34705,\"start\":34692},{\"end\":34714,\"start\":34709},{\"end\":34725,\"start\":34718},{\"end\":35051,\"start\":35047},{\"end\":35063,\"start\":35055},{\"end\":35075,\"start\":35067},{\"end\":35083,\"start\":35079},{\"end\":35403,\"start\":35398},{\"end\":35414,\"start\":35407},{\"end\":35424,\"start\":35418},{\"end\":35437,\"start\":35428},{\"end\":35721,\"start\":35716},{\"end\":35729,\"start\":35725},{\"end\":35743,\"start\":35733},{\"end\":35756,\"start\":35747},{\"end\":35767,\"start\":35760},{\"end\":35779,\"start\":35771},{\"end\":36156,\"start\":36151},{\"end\":36167,\"start\":36160},{\"end\":36178,\"start\":36171},{\"end\":36190,\"start\":36182},{\"end\":36199,\"start\":36194},{\"end\":36552,\"start\":36543},{\"end\":36562,\"start\":36556},{\"end\":36572,\"start\":36566},{\"end\":36581,\"start\":36576},{\"end\":36590,\"start\":36585},{\"end\":36602,\"start\":36594},{\"end\":36936,\"start\":36927},{\"end\":36948,\"start\":36940},{\"end\":36961,\"start\":36952},{\"end\":36971,\"start\":36965},{\"end\":36983,\"start\":36975},{\"end\":37278,\"start\":37273},{\"end\":37288,\"start\":37282},{\"end\":37510,\"start\":37507},{\"end\":37529,\"start\":37514},{\"end\":37541,\"start\":37533},{\"end\":37553,\"start\":37545},{\"end\":37565,\"start\":37557},{\"end\":37576,\"start\":37569},{\"end\":37586,\"start\":37580},{\"end\":37891,\"start\":37883},{\"end\":37900,\"start\":37895},{\"end\":37911,\"start\":37904},{\"end\":37925,\"start\":37915},{\"end\":38233,\"start\":38227},{\"end\":38241,\"start\":38237},{\"end\":38252,\"start\":38245},{\"end\":38263,\"start\":38256},{\"end\":38530,\"start\":38524},{\"end\":38539,\"start\":38534},{\"end\":38548,\"start\":38543},{\"end\":38559,\"start\":38552},{\"end\":38572,\"start\":38563},{\"end\":38584,\"start\":38576},{\"end\":38594,\"start\":38588},{\"end\":38602,\"start\":38598},{\"end\":38613,\"start\":38606},{\"end\":38936,\"start\":38934},{\"end\":38948,\"start\":38940},{\"end\":38958,\"start\":38952},{\"end\":38970,\"start\":38962},{\"end\":39202,\"start\":39196},{\"end\":39208,\"start\":39206},{\"end\":39215,\"start\":39212},{\"end\":39224,\"start\":39219},{\"end\":39235,\"start\":39228},{\"end\":39529,\"start\":39526},{\"end\":39538,\"start\":39533},{\"end\":39548,\"start\":39542},{\"end\":39558,\"start\":39552},{\"end\":39573,\"start\":39562},{\"end\":39581,\"start\":39577},{\"end\":39962,\"start\":39953},{\"end\":39974,\"start\":39968},{\"end\":40220,\"start\":40215},{\"end\":40230,\"start\":40224},{\"end\":40238,\"start\":40234},{\"end\":40246,\"start\":40242},{\"end\":40255,\"start\":40250},{\"end\":40263,\"start\":40259},{\"end\":40272,\"start\":40267},{\"end\":40284,\"start\":40276},{\"end\":40294,\"start\":40288},{\"end\":40303,\"start\":40298},{\"end\":40521,\"start\":40516},{\"end\":40533,\"start\":40525},{\"end\":40540,\"start\":40537},{\"end\":40550,\"start\":40544},{\"end\":40794,\"start\":40784},{\"end\":40809,\"start\":40798},{\"end\":41003,\"start\":40998},{\"end\":41015,\"start\":41007},{\"end\":41022,\"start\":41019},{\"end\":41032,\"start\":41026},{\"end\":41253,\"start\":41247},{\"end\":41259,\"start\":41257}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9228629},\"end\":28051,\"start\":27678},{\"attributes\":{\"doi\":\"arXiv:1805.06771\",\"id\":\"b1\"},\"end\":28268,\"start\":28053},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8394584},\"end\":28701,\"start\":28270},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16840584},\"end\":28965,\"start\":28703},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5229785},\"end\":29275,\"start\":28967},{\"attributes\":{\"doi\":\"arXiv:1711.07280\",\"id\":\"b5\"},\"end\":29742,\"start\":29277},{\"attributes\":{\"doi\":\"arXiv:1711.11543\",\"id\":\"b6\"},\"end\":29961,\"start\":29744},{\"attributes\":{\"doi\":\"arXiv:1807.11174\",\"id\":\"b7\"},\"end\":30283,\"start\":29963},{\"attributes\":{\"doi\":\"arXiv:1710.02410\",\"id\":\"b8\"},\"end\":30559,\"start\":30285},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":378809},\"end\":30827,\"start\":30561},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14162565},\"end\":31204,\"start\":30829},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2904170},\"end\":31539,\"start\":31206},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5617021},\"end\":31817,\"start\":31541},{\"attributes\":{\"doi\":\"arXiv:1603.04908\",\"id\":\"b13\"},\"end\":32046,\"start\":31819},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7816163},\"end\":32321,\"start\":32048},{\"attributes\":{\"doi\":\"arXiv:1704.06340\",\"id\":\"b15\"},\"end\":32635,\"start\":32323},{\"attributes\":{\"doi\":\"arXiv:1803.11217\",\"id\":\"b16\"},\"end\":32963,\"start\":32637},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6728126},\"end\":33329,\"start\":32965},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15749797},\"end\":33605,\"start\":33331},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2843608},\"end\":33899,\"start\":33607},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13286551},\"end\":34258,\"start\":33901},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3665577},\"end\":34611,\"start\":34260},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":19515346},\"end\":34992,\"start\":34613},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4406882},\"end\":35328,\"start\":34994},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":18645722},\"end\":35652,\"start\":35330},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9854676},\"end\":36064,\"start\":35654},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4461350},\"end\":36489,\"start\":36066},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4410754},\"end\":36831,\"start\":36491},{\"attributes\":{\"doi\":\"arXiv:1806.01482\",\"id\":\"b28\"},\"end\":37187,\"start\":36833},{\"attributes\":{\"id\":\"b29\"},\"end\":37408,\"start\":37189},{\"attributes\":{\"doi\":\"arXiv:1406.1078\",\"id\":\"b30\"},\"end\":37816,\"start\":37410},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11971675},\"end\":38181,\"start\":37818},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9455111},\"end\":38457,\"start\":38183},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":502946},\"end\":38930,\"start\":38459},{\"attributes\":{\"id\":\"b34\"},\"end\":39155,\"start\":38932},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16034699},\"end\":39452,\"start\":39157},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3759573},\"end\":39869,\"start\":39454},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206775640},\"end\":40156,\"start\":39871},{\"attributes\":{\"id\":\"b38\"},\"end\":40470,\"start\":40158},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8577750},\"end\":40756,\"start\":40472},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1915014},\"end\":40916,\"start\":40758},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b41\"},\"end\":41199,\"start\":40918},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b42\"},\"end\":41365,\"start\":41201}]", "bib_title": "[{\"end\":27781,\"start\":27678},{\"end\":28345,\"start\":28270},{\"end\":28734,\"start\":28703},{\"end\":29018,\"start\":28967},{\"end\":30596,\"start\":30561},{\"end\":30897,\"start\":30829},{\"end\":31271,\"start\":31206},{\"end\":31604,\"start\":31541},{\"end\":32107,\"start\":32048},{\"end\":33040,\"start\":32965},{\"end\":33375,\"start\":33331},{\"end\":33637,\"start\":33607},{\"end\":33968,\"start\":33901},{\"end\":34330,\"start\":34260},{\"end\":34688,\"start\":34613},{\"end\":35043,\"start\":34994},{\"end\":35394,\"start\":35330},{\"end\":35712,\"start\":35654},{\"end\":36147,\"start\":36066},{\"end\":36539,\"start\":36491},{\"end\":37879,\"start\":37818},{\"end\":38223,\"start\":38183},{\"end\":38520,\"start\":38459},{\"end\":39192,\"start\":39157},{\"end\":39522,\"start\":39454},{\"end\":39949,\"start\":39871},{\"end\":40512,\"start\":40472},{\"end\":40780,\"start\":40758}]", "bib_author": "[{\"end\":27790,\"start\":27783},{\"end\":27801,\"start\":27790},{\"end\":27814,\"start\":27801},{\"end\":28124,\"start\":28117},{\"end\":28137,\"start\":28124},{\"end\":28354,\"start\":28347},{\"end\":28362,\"start\":28354},{\"end\":28373,\"start\":28362},{\"end\":28383,\"start\":28373},{\"end\":28393,\"start\":28383},{\"end\":28407,\"start\":28393},{\"end\":28742,\"start\":28736},{\"end\":28748,\"start\":28742},{\"end\":28758,\"start\":28748},{\"end\":29026,\"start\":29020},{\"end\":29033,\"start\":29026},{\"end\":29045,\"start\":29033},{\"end\":29398,\"start\":29386},{\"end\":29404,\"start\":29398},{\"end\":29413,\"start\":29404},{\"end\":29422,\"start\":29413},{\"end\":29433,\"start\":29422},{\"end\":29447,\"start\":29433},{\"end\":29455,\"start\":29447},{\"end\":29464,\"start\":29455},{\"end\":29475,\"start\":29464},{\"end\":29483,\"start\":29475},{\"end\":29780,\"start\":29773},{\"end\":29789,\"start\":29780},{\"end\":29801,\"start\":29789},{\"end\":29808,\"start\":29801},{\"end\":29818,\"start\":29808},{\"end\":29827,\"start\":29818},{\"end\":30068,\"start\":30062},{\"end\":30075,\"start\":30068},{\"end\":30081,\"start\":30075},{\"end\":30090,\"start\":30081},{\"end\":30098,\"start\":30090},{\"end\":30353,\"start\":30340},{\"end\":30363,\"start\":30353},{\"end\":30378,\"start\":30363},{\"end\":30387,\"start\":30378},{\"end\":30397,\"start\":30387},{\"end\":30607,\"start\":30598},{\"end\":30618,\"start\":30607},{\"end\":30628,\"start\":30618},{\"end\":30911,\"start\":30899},{\"end\":30920,\"start\":30911},{\"end\":30928,\"start\":30920},{\"end\":30940,\"start\":30928},{\"end\":31287,\"start\":31273},{\"end\":31298,\"start\":31287},{\"end\":31615,\"start\":31606},{\"end\":31626,\"start\":31615},{\"end\":31881,\"start\":31868},{\"end\":31891,\"start\":31881},{\"end\":31899,\"start\":31891},{\"end\":31906,\"start\":31899},{\"end\":32121,\"start\":32109},{\"end\":32130,\"start\":32121},{\"end\":32394,\"start\":32387},{\"end\":32401,\"start\":32394},{\"end\":32407,\"start\":32401},{\"end\":32418,\"start\":32407},{\"end\":32427,\"start\":32418},{\"end\":32441,\"start\":32427},{\"end\":32451,\"start\":32441},{\"end\":32735,\"start\":32729},{\"end\":32742,\"start\":32735},{\"end\":32750,\"start\":32742},{\"end\":32760,\"start\":32750},{\"end\":32774,\"start\":32760},{\"end\":33051,\"start\":33042},{\"end\":33060,\"start\":33051},{\"end\":33071,\"start\":33060},{\"end\":33383,\"start\":33377},{\"end\":33392,\"start\":33383},{\"end\":33402,\"start\":33392},{\"end\":33651,\"start\":33639},{\"end\":33663,\"start\":33651},{\"end\":33670,\"start\":33663},{\"end\":33677,\"start\":33670},{\"end\":33976,\"start\":33970},{\"end\":33986,\"start\":33976},{\"end\":33993,\"start\":33986},{\"end\":34003,\"start\":33993},{\"end\":34345,\"start\":34332},{\"end\":34353,\"start\":34345},{\"end\":34360,\"start\":34353},{\"end\":34707,\"start\":34690},{\"end\":34716,\"start\":34707},{\"end\":34727,\"start\":34716},{\"end\":35053,\"start\":35045},{\"end\":35065,\"start\":35053},{\"end\":35077,\"start\":35065},{\"end\":35085,\"start\":35077},{\"end\":35405,\"start\":35396},{\"end\":35416,\"start\":35405},{\"end\":35426,\"start\":35416},{\"end\":35439,\"start\":35426},{\"end\":35723,\"start\":35714},{\"end\":35731,\"start\":35723},{\"end\":35745,\"start\":35731},{\"end\":35758,\"start\":35745},{\"end\":35769,\"start\":35758},{\"end\":35781,\"start\":35769},{\"end\":36158,\"start\":36149},{\"end\":36169,\"start\":36158},{\"end\":36180,\"start\":36169},{\"end\":36192,\"start\":36180},{\"end\":36201,\"start\":36192},{\"end\":36554,\"start\":36541},{\"end\":36564,\"start\":36554},{\"end\":36574,\"start\":36564},{\"end\":36583,\"start\":36574},{\"end\":36592,\"start\":36583},{\"end\":36604,\"start\":36592},{\"end\":36938,\"start\":36925},{\"end\":36950,\"start\":36938},{\"end\":36963,\"start\":36950},{\"end\":36973,\"start\":36963},{\"end\":36985,\"start\":36973},{\"end\":37280,\"start\":37269},{\"end\":37290,\"start\":37280},{\"end\":37512,\"start\":37505},{\"end\":37531,\"start\":37512},{\"end\":37543,\"start\":37531},{\"end\":37555,\"start\":37543},{\"end\":37567,\"start\":37555},{\"end\":37578,\"start\":37567},{\"end\":37588,\"start\":37578},{\"end\":37893,\"start\":37881},{\"end\":37902,\"start\":37893},{\"end\":37913,\"start\":37902},{\"end\":37927,\"start\":37913},{\"end\":38235,\"start\":38225},{\"end\":38243,\"start\":38235},{\"end\":38254,\"start\":38243},{\"end\":38265,\"start\":38254},{\"end\":38532,\"start\":38522},{\"end\":38541,\"start\":38532},{\"end\":38550,\"start\":38541},{\"end\":38561,\"start\":38550},{\"end\":38574,\"start\":38561},{\"end\":38586,\"start\":38574},{\"end\":38596,\"start\":38586},{\"end\":38604,\"start\":38596},{\"end\":38615,\"start\":38604},{\"end\":38938,\"start\":38932},{\"end\":38950,\"start\":38938},{\"end\":38960,\"start\":38950},{\"end\":38972,\"start\":38960},{\"end\":39204,\"start\":39194},{\"end\":39210,\"start\":39204},{\"end\":39217,\"start\":39210},{\"end\":39226,\"start\":39217},{\"end\":39237,\"start\":39226},{\"end\":39531,\"start\":39524},{\"end\":39540,\"start\":39531},{\"end\":39550,\"start\":39540},{\"end\":39560,\"start\":39550},{\"end\":39575,\"start\":39560},{\"end\":39583,\"start\":39575},{\"end\":39964,\"start\":39951},{\"end\":39976,\"start\":39964},{\"end\":40222,\"start\":40213},{\"end\":40232,\"start\":40222},{\"end\":40240,\"start\":40232},{\"end\":40248,\"start\":40240},{\"end\":40257,\"start\":40248},{\"end\":40265,\"start\":40257},{\"end\":40274,\"start\":40265},{\"end\":40286,\"start\":40274},{\"end\":40296,\"start\":40286},{\"end\":40305,\"start\":40296},{\"end\":40523,\"start\":40514},{\"end\":40535,\"start\":40523},{\"end\":40542,\"start\":40535},{\"end\":40552,\"start\":40542},{\"end\":40796,\"start\":40782},{\"end\":40811,\"start\":40796},{\"end\":41005,\"start\":40996},{\"end\":41017,\"start\":41005},{\"end\":41024,\"start\":41017},{\"end\":41034,\"start\":41024},{\"end\":41255,\"start\":41245},{\"end\":41261,\"start\":41255}]", "bib_venue": "[{\"end\":27855,\"start\":27814},{\"end\":28115,\"start\":28053},{\"end\":28471,\"start\":28407},{\"end\":28823,\"start\":28758},{\"end\":29110,\"start\":29045},{\"end\":29384,\"start\":29277},{\"end\":29771,\"start\":29744},{\"end\":30060,\"start\":29963},{\"end\":30338,\"start\":30285},{\"end\":30683,\"start\":30628},{\"end\":31005,\"start\":30940},{\"end\":31363,\"start\":31298},{\"end\":31666,\"start\":31626},{\"end\":31866,\"start\":31819},{\"end\":32175,\"start\":32130},{\"end\":32385,\"start\":32323},{\"end\":32727,\"start\":32637},{\"end\":33136,\"start\":33071},{\"end\":33457,\"start\":33402},{\"end\":33742,\"start\":33677},{\"end\":34067,\"start\":34003},{\"end\":34425,\"start\":34360},{\"end\":34792,\"start\":34727},{\"end\":35150,\"start\":35085},{\"end\":35479,\"start\":35439},{\"end\":35846,\"start\":35781},{\"end\":36266,\"start\":36201},{\"end\":36649,\"start\":36604},{\"end\":36923,\"start\":36833},{\"end\":37267,\"start\":37189},{\"end\":37503,\"start\":37410},{\"end\":37973,\"start\":37927},{\"end\":38307,\"start\":38265},{\"end\":38680,\"start\":38615},{\"end\":39026,\"start\":38972},{\"end\":39293,\"start\":39237},{\"end\":39647,\"start\":39583},{\"end\":40005,\"start\":39976},{\"end\":40211,\"start\":40158},{\"end\":40603,\"start\":40552},{\"end\":40829,\"start\":40811},{\"end\":40994,\"start\":40918},{\"end\":41243,\"start\":41201}]"}}}, "year": 2023, "month": 12, "day": 17}