{"id": 218674589, "updated": "2023-10-06 16:02:04.223", "metadata": {"title": "Joint Progressive Knowledge Distillation and Unsupervised Domain Adaptation", "authors": "[{\"first\":\"Le\",\"last\":\"Nguyen-Meidine\",\"middle\":[\"Thanh\"]},{\"first\":\"Eric\",\"last\":\"Granger\",\"middle\":[]},{\"first\":\"Madhu\",\"last\":\"Kiran\",\"middle\":[]},{\"first\":\"Jose\",\"last\":\"Dolz\",\"middle\":[]},{\"first\":\"Louis-Antoine\",\"last\":\"Blais-Morin\",\"middle\":[]}]", "venue": "2020 International Joint Conference on Neural Networks (IJCNN)", "journal": "2020 International Joint Conference on Neural Networks (IJCNN)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Currently, the divergence in distributions of design and operational data, and large computational complexity are limiting factors in the adoption of CNNs in real-world applications. For instance, person re-identification systems typically rely on a distributed set of cameras, where each camera has different capture conditions. This can translate to a considerable shift between source (e.g. lab setting) and target (e.g. operational camera) domains. Given the cost of annotating image data captured for fine-tuning in each target domain, unsupervised domain adaptation (UDA) has become a popular approach to adapt CNNs. Moreover, state-of-the-art deep learning models that provide a high level of accuracy often rely on architectures that are too complex for real-time applications. Although several compression and UDA approaches have recently been proposed to overcome these limitations, they do not allow optimizing a CNN to simultaneously address both. In this paper, we propose an unexplored direction \u2013 the joint optimization of CNNs to provide a compressed model that is adapted to perform well for a given target domain. In particular, the proposed approach performs unsupervised knowledge distillation (KD) from a complex teacher model to a compact student model, by leveraging both source and target data. It also improves upon existing UDA techniques by progressively teaching the student about domain-invariant features, instead of directly adapting a compact model on target domain data. Our method is compared against state-of-the-art compression and UDA techniques, using two popular classification datasets for UDA \u2013 Office31 and ImageClef-DA. In both datasets, results indicate that our method can achieve the highest level of accuracy while requiring a comparable or lower time complexity.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.07839", "mag": "3090492056", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcnn/Nguyen-MeidineG20", "doi": "10.1109/ijcnn48605.2020.9206989"}}, "content": {"source": {"pdf_hash": "0054a11ba67e1ac4c6243f7fb67e202868ef98d7", "pdf_src": "Arxiv", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.07839", "status": "GREEN"}}, "grobid": {"id": "480fd57d456e07c5160732e7afba6a0c4a42b7a0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0054a11ba67e1ac4c6243f7fb67e202868ef98d7.txt", "contents": "\nJoint Progressive Knowledge Distillation and Unsupervised Domain Adaptation\n\n\nThanh Le \nDept. of Systems Engineering\nLaboratoire d'imagerie, de vision et d'intelligence artificielle (LIVIA)\n\u00c9cole de technologie sup\u00e9rieure Montreal\nCanada\n\nEric Nguyen-Meidine le-thanh.nguyen-meidine.1@ens.etsmtl.ca \nDept. of Systems Engineering\nLaboratoire d'imagerie, de vision et d'intelligence artificielle (LIVIA)\n\u00c9cole de technologie sup\u00e9rieure Montreal\nCanada\n\nMadhu Granger eric.granger@etsmt.ca \nDept. of Systems Engineering\nLaboratoire d'imagerie, de vision et d'intelligence artificielle (LIVIA)\n\u00c9cole de technologie sup\u00e9rieure Montreal\nCanada\n\nJose Kiran \nDept. of Systems Engineering\nLaboratoire d'imagerie, de vision et d'intelligence artificielle (LIVIA)\n\u00c9cole de technologie sup\u00e9rieure Montreal\nCanada\n\nDolz \nDept. of Systems Engineering\nLaboratoire d'imagerie, de vision et d'intelligence artificielle (LIVIA)\n\u00c9cole de technologie sup\u00e9rieure Montreal\nCanada\n\nLouis-Antoine Blais-Morin lablaismorin@genetec.com \nGenetec Inc. Montreal\nCanada\n\nJoint Progressive Knowledge Distillation and Unsupervised Domain Adaptation\nIndex Terms-Deep LearningConvolutional Neural NetworksDomain AdaptationKnowledge DistillationVisual Recognition\nCurrently, the divergence in distributions of design and operational data, and large computational complexity are limiting factors in the adoption of CNNs in real-world applications. For instance, person re-identification systems typically rely on a distributed set of cameras, where each camera has different capture conditions. This can translate to a considerable shift between source (e.g. lab setting) and target (e.g. operational camera) domains. Given the cost of annotating image data captured for fine-tuning in each target domain, unsupervised domain adaptation (UDA) has become a popular approach to adapt CNNs. Moreover, state-of-the-art deep learning models that provide a high level of accuracy often rely on architectures that are too complex for real-time applications. Although several compression and UDA approaches have recently been proposed to overcome these limitations, they do not allow optimizing a CNN to simultaneously address both. In this paper, we propose an unexplored direction -the joint optimization of CNNs to provide a compressed model that is adapted to perform well for a given target domain. In particular, the proposed approach performs unsupervised knowledge distillation (KD) from a complex teacher model to a compact student model, by leveraging both source and target data. It also improves upon existing UDA techniques by progressively teaching the student about domain-invariant features, instead of directly adapting a compact model on target domain data. Our method is compared against state-ofthe-art compression and UDA techniques, using two popular classification datasets for UDA -Office31 and ImageClef-DA. In both datasets, results indicate that our method can achieve the highest level of accuracy while requiring a comparable or lower time complexity.\n\nI. INTRODUCTION\n\nDeep learning (DL) models, and in particular convolutional neural networks (CNNs) can achieve state-of-the-art performance in a wide range of visual recognition applications, such as classification, object detection, and semantic segmentation [1]- [3]. In practice, a main drawback associated with these models is their scalability and computationally complexity, which poses a challenge for many real-time applications, as found in video analytics and surveillance [4].\n\nCurrently, the compact DL models that can provide fast inference generally lack the high accuracy of deeper, more complex models. One alternative to overcome this issue is to compress complex high accuracy models into smaller or simpler models while preserving the same level of accuracy. Several approaches have recently been proposed to accelerate and compress CNNs, include quantization, low-rank approximation, knowledge distillation, compact network design and network pruning. For instance, network compression methods for channel pruning [5]- [10] and knowledge distillation (KD) [11]- [14]) have become very popular due to the exponential increase in the complexity of architectures, which may have millions of parameters.\n\nAnother limitation is the poor generalization of CNNs across domains, particularly when there is a considerable domain shift between source and target data distributions. This is the case for instance of applications like video-surveillance over a distributed network of camera, where variations in camera viewpoint and capture conditions (e.g., illumination, occlusion and background) introduce a shift between data from source and target domains. The accuracy of CNNs degrades when there is a considerable divergence between the data capture conditions in the model development and operational environments. To alleviate this problem, domain adaptation techniques are commonly proposed, either in a supervised or unsupervised setting [15], [16]. For applications in videosurveillance, it is, however, costly to collect and annotate videos from each camera viewpoint and capture condition to fine-tune a CNN.\n\nIn this paper, we focus on DL models for unsupervised domain adaptation (UDA) to allow adapting CNN embeddings based on unlabeled data. The main body of literature on UDA techniques focuses on learning domain invariant features by using adversarial loss [17], [18] to encourage domain confusion, or by minimizing a distance or discrepancy between two the data distributions [19], or both [20]. Another popular paradigm is to learn a mapping between source and target images such that images captured in different domains have a similar appearance. This reconstruction-based approach mimics standard supervised learning [21], [22].\n\nWhile compression and UDA techniques can respectively provide CNNs with a high level of performance and efficiency, research that explores the benefits of joint model compression and UDA remains scarce. For example, recent research has combined KD and UDA to improve the performance in a UDA context [23], [24]. Despite the improvement shown in terms of the DA task, they neglect the reduction of model complexity. Another important problem when employing KD is the potentially large gap between the capacity of teacher and student models to learn complex mappings, which can degrade student performance. Distilling knowledge directly into the student model from a trained teacher model may present some challenges. For example, in the early stages of training, the decision boundaries of the student and teacher modes may differ considerably. To bridge this gap some researchers have proposed to either add a teaching assistant [14] or distilling knowledge at several intermediate layers of a CNN [13].\n\nIn a standard training scenario, the CNN model would be adapted to a new target after having been compressed, although this can reduce its capacity to generalize on target data since over-parametrization is often important for generalization [25]. This can be overcome by learning from a teacher instead of learning directly through the UDA loss. In contrast, another scenario involves adapting a complex CNN model to the target domain, and then compressing it, although unsupervised KD remains a challenge since ground truth data is needed to assure that the student model to remain consistent with the dataset. To address the aforementioned limitations, we argue that by adapting the teacher to the target domain while the student is being trained, the student can adapt progressively to the domain instead of learning directly from a previously targeted domain. The student would thereby learn the steps needed to adapt itself to the target domain, instead of learning directly from a model adapted to the target, i.e. adapted teacher. By jointly exploiting unsupervised KD and DA, it is possible to overcome the lack of ground truth data for KD, as needed to train the student model that is consistent with source features.\n\nUnlike recent work in literature, this paper provides the first attempt to simultaneously address two key problems with CNNs -domain shift and model complexity -through joint progressive KD and UDA. Particularly, the proposed approach learns a compact model with a feature embedding that can provide a high level of accuracy in the target domain. It leverages progressive KD to adapt the student model in a stepby-step manner, using knowledge from the teacher to learn domain invariant features of the source and target domains. In order to ensure the validity of student model w.r.t the UDA loss and the target domain, we introduce a consistency loss that ensures consistency on the student model by learning source domain features from the teacher. We validate our approach with different training scenarios of KD and UDA: (1) UDA then KD, (2) KD then UDA and (3) UDA directly on compact model. Empirical evaluations show that our joint progressive KD and UDA approach facilitates domain adaptation and compression of deep CNNs, and can outperform representative state-of-the-art approaches on the widely used Office31 and ImageClef-DA benchmark datasets. In particular, our approach is general and model-agnostic, and can be combined with different UDA and KD techniques to improve performance.\n\n\nII. RELATED WORK\n\n\nA. Compression techniques:\n\nApproaches for compressing CNNs can be mainly categorized in: (1) pruning techniques [5], [7], [9], [10], [26], (2) quantization [27]- [29], (3) decomposition [30], [31], and (4) KD [11]- [14]. Pruning techniques focus on removing nonuseful weights or filters in order to reduce the computational complexity. Quantization techniques focus on reducing the representation of weights into lower precision since, for example, 8-bit integer precision provides much faster computation than the floating point computation. Decomposition techniques provide faster computation by decomposing tensor in lower rank approximation as vectors products. Lastly, KD techniques transfer knowledge from a teacher (usually a large model) to a student (a smaller model). Since we will employ this technique to reduce the complexity of the model, we will focus on KD onward. For a comprehensive survey on compression techniques for CNNs, we refer the reader to [32].\n\nThere exist several ways of distilling knowledge from a teacher to a student. A well-known technique is to employ the teacher output as the soft label for the student [11]. In this work [11], the temperature value was employed to generate softer versions of the teacher outputs. Another popular solution is to minimize the features differences at intermediate layers between the teacher and student network in order to maximize the information transfer between the teacher and the model [13]. Feature similarity can be enforced by minimizing a partial L2 distance, which is equivalent to L2 norm except that if the value of the student is smaller than the teacher and both are negative then the result is zero, between student and teacher after using a Margin ReLU (use of a margin m instead of 0) which can take in negative values of a feature map. To solve the gap issue between a converged teacher model and a student some other researchers have proposed the integration of a teaching assistant [14]. In this work, an intermediate model with a lesser gap is chosen, this model is then used as a teacher to a smaller model, by progressively reducing the gap with an intermediate teaching assistant, thus, limit the performance degradation of the student model.\n\n\nB. Unsupervised domain adaptation:\n\nUnsupervised Domain Adaptation (UDA) techniques try to adapt models when a domain shift between source and target dataset exists, where only the source data is labeled. Current main UDA techniques [15] include: finding domaininvariant features [19], domain mapping [33], [34], ensemble learning [35], statistic normalization [36] and target discriminate methods [37]. The first category learns domain invariant features either by domain confusion [17], [18] or minimizing a distance between distribution [19]. Domain confusion can be achieved by employing a domain classifier (or discriminator) [17], [18]. While [17] employs a gradient reversal layer in order to maximize the domain classification loss, the work in [18] uses an adversarial loss on the discriminator. Domain mapping focuses on finding a mapping either from the source domain to the target domain or vice-versa [33], [34]. Currently, most of the domain mapping based techniques rely on generative adversarial networks (GAN). Hoffman et al. [33] propose to use a pair of discriminator and generator in order to map a source image into the target domain distribution. This mapping is learned at the same time as a task-specific loss (i.e. classification loss) on both transformed image and non transformed-images and the overall optimization is done alternatively between generator and discriminator-task. The paper in [38] goes further by integrating adaptation at featurelevel. Ensemble methods use either multiple models or the same model at different times (typically referred to as selfensembling) in order to produce more reliable pseudo-labels on unlabelled data [35]. Others methods like statistic normalization assume that the task knowledge is learned and the only adaptation needs to be done is on the batch norm statistics [36]. Last, target discriminate methods work with the assumption that data points are distributed in separate clusters and the decision boundary lies in lower density regions. Thus, these methods work by trying to push the decision boundary to lower density regions by adding adversarial losses [37].\n\n\nC. Joint unsupervised domain adaptation and knowledge distillation:\n\nEven though jointly exploiting UDA and KD for compression and domain shift problems remains unexplored, there have been few attempts to combine these two techniques in the context of domain adaptation [39], [40]. In [39], the authors propose to use multiple teachers for teaching a single student in order to increase the performance of their model in the context of sentiment analysis. The work in [40] proposes to combine KD and DA for the task of white matter hyperintensities segmentation in magnetic resonance imaging by training the teacher model on the source and trying to minimize the cross-entropy loss between the probability maps of the teacher and the student on the target dataset. Nevertheless, these approaches do not address the problem of reducing model complexity. To the best of our knowledge, there exist only one approach that combines compression and UDA techniques, referred to as TCP [41]. Interaction between the two techniques is done in several steps. First, the model is trained to be adapted to the target domain by minimizing the domain divergence using the maximum mean discrepancy (MMD) [19]. Then, the least important filters are pruned by using a gradient based criterion and the model is continuously refined on the domain adaptation loss. An important limitation of this technique is the need to have an already domain adapted model in order to start the pruning, whereas our technique can directly start from a non-adapted model.\n\n\nIII. PROPOSED METHOD\n\nThe main pipeline of the proposed method is depicted in Figure 1. Our method performs domain adaptation of a teacher model by learning domain invariant features between the source and target domains. Meanwhile, it progressively distills its knowledge to a student model on both source and target features. As shown in Figure 1, DA is performed on the features of the teacher network, while the KD from teacher to student is performed on the result of a temperature-based soft-max on the logits (output of a fully connected layer). Additional details of the proposed UDA and KD techniques are described in the following subsections.\n\n\nA. Unsupervised domain adaptation:\n\nWe start by defining the UDA loss for teacher model, which is based on MMD [19], [41] 1 :\nL M M D = || 1 N s xi\u2208D L s \u03c6 T (x i ) \u2212 1 N t xj \u2208D U t \u03c6 T (x j )|| 2 H(1)\nWith D L s the labeled source domain dataset which contains N s samples and labels, D U t the unlabeled target domain dataset of N t data samples, \u03c6 T the teacher feature extractor function that maps an input to a feature map, H the Reproducing Kernel Hilbert Space(RKHS) with gaussian kernel. As in [19], [41], we incorporate this a supervised loss on the source domain in order to have the final UDA loss for the teacher:\nL T DA = L M M D + \u03b3L CE (T (D L s , 1), y s )(2)\nL CE the supervised cross-entropy loss of the teacher model on the source domain, \u03b3 a trade-off hyper-parameter that follows the same variations as [41] and T the function that maps an input to the output of the teacher network with a soft-max of temperature 1 (i.e. the regular soft-max).\n\n\nB. Knowledge distillation for domain knowledge transfer:\n\nThe next step is to transfer the target domain knowledge from the teacher to the student, we use a modified version of the KD loss from the work of Hinton:\nL T KD = L distill (S(D U t , \u03c4 ), T (D U t , \u03c4 ))(3)\nIn this equation, S and T represent respectively the output of student network and teacher network with a soft-max based on a temperature \u03c4 in order to soften the output and L distill is a KL divergence loss in our case but can be replaced with a mean squared loss or cross-entropy. This loss differs from the original paper [11] because, we had to remove the crossentropy loss between the student model output and the ground truth since we are working on UDA. Trivially, this should be enough for joint KD and UDA since we only want to have target domain knowledge, in order to ensure the consistency of the model w.r.t to a common representation, we proposed to add a consistency loss to ensure that the student model can learn a better common representation from source and target domains by distilling on the source data. Eq. 4 is the student KD loss, with hyper-parameter \u03b1 to balance between the KD and the cross entropy loss of the output of the student model and the ground truth on the source domain. The Figure 1 illustrates all these losses and also the proposed techniques. The final loss of our models, is then:\nL SKD = L distill (S(D L s , \u03c4 ), T (D L s , \u03c4 )) + \u03b1L CE (S(D L s , 1), y s )(4)L = (1 \u2212 \u03b2)L T DA + \u03b2(L T KD + L SKD )(5)\nWe added the \u03b2 hyper-parameter in order to balance out the importance between UDA and KD. Since we are performing jointly KD and DA, in the beginning, the teacher would still be learning from the DA. This means that there is not much to be learned for the student model, besides the source representation which can be learned from the KD loss. In light of this, we propose to start by giving more importance to UDA in the beginning and gradually transfer the importance to KD basing \u03b2 on an exponential growth function between [b, f ], with b the starting value of \u03b2 and f the end value. In order to define as exponential growth, we need to calculate a growth rate based on b and f :\ng = log( f b ) epochs (6)\nWith epochs the number of epochs and g the growth rate. Once we have the growth rate, \u03b2 at epoch t can be found as:\n\u03b2 t = b * e gt(7)\nThe overall algorithm is described in Algorithm 1 using an alternate optimization scheme of our algorithm. The details of this implementation can be found in Section IV(C).\n\n\nIV. EXPERIMENTAL METHODOLOGY\n\nIn this section, we detail the experimental methodology employed to validate the proposed method. First, we describe  b) ImageClef-DA: : This dataset for UDA contains four subsets which are taken from Imagenet (I), Pascal-Voc (P), Caltech (C) and Bing (B). Each of these subsets contains a total of 600 images for 12 classes. For this dataset, we compare to others techniques using six scenarios:\nI \u2212 \u2192 P , P \u2212 \u2192 I, I \u2212 \u2192 C, C \u2212 \u2192 I, C \u2212 \u2192 P , P \u2212 \u2192 C.\nFor the evaluation of our algorithm, we chose to use a popular backbone architecture, ResNet50, with our algorithm.\n\n\nB. Baselines methods:\n\nIn order to validate our joint progressive method, we propose to evaluate and compare our method with 3 baseline scenarios:\n\n1) UDA, and then KD, 2) KD, and then UDA, and 3) UDA directly on compact model. With the first baseline, U DA \u2212 \u2192 KD, we start with a model of ResNet50 for UDA, then we start KD on this model with a modified version (Eq.3) of [11] since most KD algorithms does not handle unsupervised KD. The student model used for this first baseline is similar to the one we chose for our methods, i.e. ResNet18 and ResNet34. For the second baseline KD \u2212 \u2192 U DA, we start by training a teacher model, ResNet50, on the source dataset then we apply KD with this teacher using labeled data of the source dataset and a student of one of target model, ResNet18 or ResNet34, and we finish with UDA on this student model. As for the last baseline, we take a student model, ResNet18 and ResNet34, and directly apply UDA algorithm on it [19], [41].\n\nAnother aspect of our method is knowledge distillation which is meant for model reduction and acceleration. In order to validate our findings, we also measure the difference in terms of FLOPs and parameters between our method and TCP. Since our models are predetermined architecture, we measure FLOPS and the number of parameters on ResNet50, ResNet34 and ResNet18, as for TCP, we report the number of FLOPS that was reported in the original paper.\n\n\nC. Implementation details:\n\nIn this paper, we consider two ways of implementing the optimization process. The first is an end-to-end training by optimizing the loss of Eq.5 with a unique single optimizer. While this approach works and provide reasonable performance, we found that having two optimizers allows more flexibility in terms of scheduling the learning rate since there is can be a difference between optimizing domain adaptation and KD, i.e. different learning rates for each loss or we do not want to update the teacher during KD. Thus, we implement a second alternative approach where we use one optimizer for (1 \u2212 \u03b2)L T DA with a different learning rate schedule than another optimizer used for \u03b2(L T KD + L SKD ). We would like to point out that it's also possible to implement the second approach with one single optimizer by handling the learning rate scheduling ourselves. For our experiments on Office31 and ImageClef-DA, we use two student models architecture, which are: ResNet34 (12% FLOPS reduction from ResNet50) and ResNet18 (56% FLOPS reduction from ResNet50) which are closely similar to the FLOPS reduction of TCP [41].\n\nIn these experiments, the images are cropped to a fixed resolution of 224x224. Regarding the hyper-parameters, we use a starting \u03b2 value of 0.1 and an end value of 0.9. For the KD hyper-parameters, we use a temperature \u03c4 = 20 and \u03b1 = 0.8. Overall, we use a learning rate starting at 0.001 for UDA optimizer in Office31 and 0.0001 for ImageClef-DA, 0.001 for KD optimizer for both datasets, a momentum of 0.9 and 400 epochs.\n\nOur implementation can be found on-line at: https://github. com/LIVIAETS/KD UDA   \n\n\nTraining methods\nA \u2212 \u2192 W W \u2212 \u2192 A D \u2212 \u2192 W W \u2212 \u2192 D D \u2212 \u2192 A A \u2212 \u2192 D\n\nTraining methods\nA \u2212 \u2192 W W \u2212 \u2192 A D \u2212 \u2192 W W \u2212 \u2192 D D \u2212 \u2192 A A \u2212 \u2192 D\n\nV. RESULTS AND DISCUSSION\n\nA. Results on Office31:\n\nFor the Office31 dataset, our results outperform most of the baseline and the current existing techniques. From Tables I  and II, we see that, the third baseline \"U DA only\" performs better than U DA \u2212 \u2192 KD. This can be explained by the fact that there is no label for target dataset and the distillation loss alone is not sufficient, as there exist a need for supervising the cross-entropy loss. The result of the second baseline is better than the rest of the baselines and TCP, which can be explained because the student model resulting from KD is already trained on a labeled source data distribution. Finally, our techniques using both teacher ResNet50 and ResNet101 perform better than TCP. In both ResNet34 and ResNet18 settings, the difference between the average of our results and TCP is considerable. This shows that combining UDA and KD in a progressive setting brings additional benefits. Between ResNet34 with teacher ResNet50 and ResNet101, there exists a slight difference. We notice that the student model with a larger teacher model performs slightly better. This is expected, since ResNet101 has a better generalization capability than ResNet50. Lastly, our technique also seems to improve UDA since our methods perform better than the second baseline, which only performs UDA on a compact model.\n\n\nB. Results on ImageClef-DA:\n\nFor this dataset, the results obtained by our methods are shown in Table III and Table IV, which demonstrate that the proposed technique outperforms both the baselines and current existing techniques. In contrast, the results obtained with ResNet34 are closer to TCP and the baseline than in the previous dataset, whereas our result with ResNet18 shows a bigger gap between the baselines and the proposed techniques. This can be first explained by the fact that ImageClef-DA is a better balanced dataset where each subset has the same number of samples. Secondly, the third baseline of \"UDA only on a ResNet18\" already performs better than TCP, which may explain why our methods have a better performance. In this table, the difference between teacher models is closer, this shows that a bigger teacher may not help in improving performance since the learning bottleneck is now on the student model.\n\n\nC. Computational Complexity:\n\nComparison in terms of complexity is depicted in Table V. While pruned TCP [41] models have fewer parameters than our student models, we achieve the same number of FLOPS on ResNet34, and fewer FLOPs on ResNet18. This means that while TCP prunes more parameters, it may not have a lot of impact on the number of FLOPS since the pruned filters are ranked and pruned globally across the network instead of being pruned at each layer. This also means that TCP prunes away filters that do not impact the FLOPS, but that can impact performance, which may hamper the global objective. Another important point of having more parameters is that, over-parametrization can help generalization, increasing the chances of our student model to have a better generalization than a pruned model with less parameters.\n\n\nD. Comparison over larger teacher and student:\n\nIn this section, we select the scenario of I \u2212 \u2192 C and increase the teacher model to ResNet152. Then, we apply our algorithm on this model with several student models going from ResNet18 to Resnet101. We compare the results from these experiments with our previous ones, along with the result of performing UDA using only Eq.1.\n\nFrom the Fig.3, we see that, having a larger teacher only has a slight performance increase compared to the smaller teacher.    Furthermore, we also noticed that there is no degradation in terms of performance when the teacher is ResNet152 and the student is ResNet18. Finally, having a ResNet50 as a student model definitely helps increasing the performance, which makes sense since we are using a much larger model.\n\nOverall, while having a larger teacher does not seem to have a big impact on accuracy, it can negatively impact the training time and increases the risk of overfitting.\n\n\nVI. CONCLUSION\n\nIn this paper, we proposed a combination of KD and UDA that remains unexplored in literature and tackles both the problem of domain shift and model complexity. Our results suggest that the proposed method is capable of obtaining a compressed model adapted to an unsupervised target domain that performs better than state-of-the-art method and current baselines. Additionally, our progressive technique is capable of having a big gap between the student and the teacher without suffering having a performance degradation on the student. The proposed technique is generic and should be able to work with most of current UDA and KD techniques, in future works, we will evaluate our method on other KD and UDA techniques.\n\nFig. 1 .\n1Illustration of the proposed learning technique for progressive KD with UDA.\n\nAlgorithm 1 :\n1KD-UDA input : A teacher model M T , a student model M S , a source dataset D Sup s , a target dataset D U t output : A target adapted student model 1 for epoch \u2190 1 to epochs do 2 for data s in D L s and data t D U t do 3 Obtain the feature map F s of M t on data s and F T on data t using \u03c6 T 4 Optimize the teacher model M T with (1 \u2212 \u03b2)L T DA using F s and F t 5 Obtain the logits o T s of M T on data s and o T t of M T on data t 6 Obtain the logits o S s of M S on data s and o S t of M S on data t 7 Applying soft-max of temperature \u03c4 on the logits and optimize \u03b2(L T KD + L SKD ) and baselines methods. Then, we provide implementation details to facilitate the reproducibility of the reported results. A. Datasets: a) Office31: : This dataset contains three subsets of dataset which are Webcam (W), DSLR (D) and Amazon (A) with 31 classes. These subsets contains images from amazon.com (A) or office environment with changes in lighting, poses using a DSLR camera (D) or a webcam (W). We evaluate our results based on six scenarios:\n\nFig. 2 .\n2Examples of images selected from the Office31 (top) and ImageClef-DA (bottom) datasets.\n\nFig. 3 .\n3Comparison of different teacher and student models.\n\nTABLE I ACCURACY\nIOF PROPOSED AND BASELINE METHODS ON OFFICE31 DATASET WHEN RESNET34 IS THE DESIRED MODEL.\n\n\nTABLE II ACCURACY OF PROPOSED AND BASELINE METHODS ON OFFICE31 DATASET WHEN RESNET18 IS THE DESIRED MODELAverage \nBaseline 1: UDA \u2212 \u2192 KD from ResNet50 \n25.4 \n7.1 \n28.5 \n50.0 \n9.7 \n30.7 \n25.2 \nBaseline 2: KD \u2212 \u2192 UDA from ResNet50 \n75.7 \n61.2 \n97.8 \n99.7 \n59.6 \n81.1 \n79.1 \nBaseline 3: UDA only on ResNet34 \n67.2 \n52.3 \n93.6 \n96.6 \n52.2 \n71.6 \n72.2 \nTCP: 12% pruned from ResNet50 \n81.8 \n55.5 \n98.2 \n99.8 \n50 \n77.9 \n77.2 \nOurs: ResNet34 from ResNet50 \n85.7 \n62.3 \n97.1 \n100 \n61.8 \n82.1 \n81.5 \nOurs: ResNet34 from ResNet101 \n87.5 \n62.9 \n98.1 \n100 \n60.8 \n85.7 \n82.5 \n\n\n\nTABLE III ACCURACY\nIIIOF PROPOSED AND BASELINE METHODS ON IMAGECLEF-DA DATASET WHEN RESNET34 IS THE DESIRED MODELTraining methods \nI \u2212 \u2192 P \nP \u2212 \u2192 I I \u2212 \u2192 C \nC \u2212 \u2192 I \nC \u2212 \u2192 P \nP \u2212 \u2192 C \nAverage \nBaseline 1: UDA \u2212 \u2192 KD from ResNet50 \n48.0 \n41.0 \n46.0 \n39.6 \n38.8 \n39.0 \n42.0 \nBaseline 2: KD \u2212 \u2192 UDA from ResNet50 \n76.6 \n87.3 \n92.0 \n80.0 \n65.6 \n90.3 \n81.9 \nBaseline 3: UDA only on ResNet34 \n73.3 \n86.3 \n92.6 \n79.3 \n65.8 \n87.5 \n80.8 \nTCP: 12% pruned from ResNet50 \n75.0 \n82.6 \n92.5 \n80.8 \n66.2 \n86.5 \n80.6 \nOurs: ResNet34 from ResNet50 \n75.6 \n89.0 \n92.6 \n83.8 \n66.5 \n92.8 \n83.3 \nOurs: ResNet34 from ResNet101 \n75.0 \n87.6 \n93.3 \n83.5 \n66.6 \n91.8 \n83.2 \n\n\n\nTABLE IV ACCURACY\nIVOF PROPOSED AND BASELINE METHODS ON IMAGECLEF-DA DATASET WHEN RESNET18 IS THE DESIRED MODELTraining methods \nI \u2212 \u2192 P \nP \u2212 \u2192 I I \u2212 \u2192 C \nC \u2212 \u2192 I \nC \u2212 \u2192 P \nP \u2212 \u2192 C \nAverage \nBaseline 1: UDA \u2212 \u2192 KD from ResNet50 \n45.1 \n41.8 \n42.5 \n43.1 \n43.3 \n34.5 \n41.7 \nBaseline 2: KD \u2212 \u2192 UDA from ResNet50 \n72.1 \n86.3 \n91.8 \n74.6 \n61.8 \n90.6 \n79.5 \nBaseline 3: UDA only on ResNet18 \n70.6 \n83.8 \n86.1 \n75.3 \n62.0 \n89.1 \n77.8 \nTCP: 45% pruned from ResNet50 \n67.8 \n77.5 \n88.6 \n71.6 \n57.7 \n79.5 \n73.7 \nOurs: ResNet18 from ResNet50 \n73.1 \n88.0 \n92.1 \n77.3 \n65.6 \n91.0 \n81.1 \nOurs: ResNet18 from ResNet101 \n73.5 \n88.6 \n92.3 \n76.8 \n64.1 \n91.1 \n81.0 \n\n\n\nTABLE V COMPUTATIONAL\nVCOMPLEXITY OF PROPOSED AND TCP NETWORKS.Models \nno. operations \nno. parameters (M) \n(GFLOPS) \nOffice31 ImageClef \nTeacher/Original: ResNet50 \n4.1 \n25.5 \n25.5 \nTCP: \n-12% pruned from ResNet50 \n3.6 \n15.8 \n15.9 \n-45% pruned from ResNet50 \n2.2 \n10.6 \n10.9 \nStudent: from ResNet50 \n-ResNet34 \n3.6 \n21.7 \n21.7 \n-ResNet18 \n1.8 \n11.1 \n11.1 \n\n18 \n34 \n50 \n101 \nTarget Model \n\n86 \n\n88 \n\n90 \n\n92 \n\n94 \n\n96 \n\nAccuracy \n\nResNet152 Teacher \nResNet101 Teacher \nResNet50 Teacher \nOnly DA \n\n\nNote that it can be generalized to most other UDA techniques.\n\nObject-contextual representations for semantic segmentation. Y Yuan, X Chen, J Wang, Y. Yuan, X. Chen, and J. Wang, \"Object-contextual representations for semantic segmentation,\" 2019.\n\n. K He, G Gkioxari, P Doll\u00e1r, R B Girshick, abs/1703.06870Mask R-CNN. CoRRK. He, G. Gkioxari, P. Doll\u00e1r, and R. B. Girshick, \"Mask R-CNN,\" CoRR, vol. abs/1703.06870, 2017. [Online]. Available: http://arxiv.org/abs/1703.06870\n\nLarge scale learning of general visual representations for transfer. A Kolesnikov, L Beyer, X Zhai, J Puigcerver, J Yung, S Gelly, N Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby, \"Large scale learning of general visual representations for transfer,\" 2019.\n\nA comparison of cnn-based face and head detectors for real-time video surveillance applications. L T Nguyen-Meidine, E Granger, M Kiran, L Blais-Morin, 2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA. L. T. Nguyen-Meidine, E. Granger, M. Kiran, and L. Blais-Morin, \"A comparison of cnn-based face and head detectors for real-time video surveillance applications,\" in 2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA), 2017, pp. 1-7.\n\nThe lottery ticket hypothesis: Finding sparse, trainable neural networks. J Frankle, M Carbin, International Conference on Learning Representations. J. Frankle and M. Carbin, \"The lottery ticket hypothesis: Finding sparse, trainable neural networks,\" in International Conference on Learning Representations, 2019. [Online]. Available: https: //openreview.net/forum?id=rJl-b3RcF7\n\nA survey of pruning methods for efficient person re-identification across domains. H Masson, A Bhuiyan, L T Nguyen-Meidine, M Javan, P Siva, I B Ayed, E Granger, abs/1907.02547CoRR. H. Masson, A. Bhuiyan, L. T. Nguyen-Meidine, M. Javan, P. Siva, I. B. Ayed, and E. Granger, \"A survey of pruning methods for efficient person re-identification across domains,\" CoRR, vol. abs/1907.02547, 2019. [Online]. Available: http://arxiv.org/abs/1907.02547\n\nImportance estimation for neural network pruning. P Molchanov, A Mallya, S Tyree, I Frosio, J Kautz, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, \"Importance estimation for neural network pruning,\" in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nAn improved trade-off between accuracy and complexity with progressive gradient pruning. L T Nguyen-Meidine, E Granger, M Kiran, L Blais-Morin, abs/1906.08746CoRR. L. T. Nguyen-Meidine, E. Granger, M. Kiran, and L. Blais-Morin, \"An improved trade-off between accuracy and complexity with progressive gradient pruning,\" CoRR, vol. abs/1906.08746, 2019. [Online]. Available: http://arxiv.org/abs/1906.08746\n\nNetwork pruning via transformable architecture search. X Dong, Y Yang, Advances in Neural Information Processing Systems. 32X. Dong and Y. Yang, \"Network pruning via transformable architecture search,\" in Advances in Neural Information Processing Systems 32, 2019, pp. 759-770. [Online]. Available: http://papers.nips.cc/paper/ 8364-network-pruning-via-transformable-architecture-search.pdf\n\nRethinking the value of network pruning. Z Liu, M Sun, T Zhou, G Huang, T Darrell, International Conference on Learning Representations. Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, \"Rethinking the value of network pruning,\" in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/ forum?id=rJlnB3C5Ym\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, NIPS Deep Learning and Representation Learning Workshop. G. Hinton, O. Vinyals, and J. Dean, \"Distilling the knowledge in a neural network,\" in NIPS Deep Learning and Representation Learning Workshop, 2015. [Online]. Available: http://arxiv.org/abs/1503.02531\n\nRelational knowledge distillation. W Park, D Kim, Y Lu, M Cho, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). W. Park, D. Kim, Y. Lu, and M. Cho, \"Relational knowledge dis- tillation,\" in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nA comprehensive overhaul of feature distillation. B Heo, J Kim, S Yun, H Park, N Kwak, J Y Choi, The IEEE International Conference on Computer Vision (ICCV). B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Y. Choi, \"A com- prehensive overhaul of feature distillation,\" in The IEEE International Conference on Computer Vision (ICCV), October 2019.\n\nImproved knowledge distillation via teacher assistant: Bridging the gap between student and teacher. S Mirzadeh, M Farajtabar, A Li, H Ghasemzadeh, abs/1902.03393CoRR. S. Mirzadeh, M. Farajtabar, A. Li, and H. Ghasemzadeh, \"Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher,\" CoRR, vol. abs/1902.03393, 2019. [Online].\n\nAdversarial transfer learning. G Wilson, D J Cook, abs/1812.02849CoRR. G. Wilson and D. J. Cook, \"Adversarial transfer learning,\" CoRR, vol. abs/1812.02849, 2018. [Online]. Available: http://arxiv.org/abs/1812. 02849\n\nDeep visual domain adaptation: A survey. M Wang, W Deng, abs/1802.03601CoRR. M. Wang and W. Deng, \"Deep visual domain adaptation: A survey,\" CoRR, vol. abs/1802.03601, 2018. [Online]. Available: http://arxiv.org/abs/1802.03601\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, Y. Ganin and V. Lempitsky, \"Unsupervised domain adaptation by backpropagation,\" 2014.\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, \"Adversarial discrimi- native domain adaptation,\" in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M I Jordan, M. Long, Y. Cao, J. Wang, and M. I. Jordan, \"Learning transferable features with deep adaptation networks,\" in ICML 2015.\n\nProceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). S. A. McIlraith and K. Q. Weinbergerthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)AAAI PressS. A. McIlraith and K. Q. Weinberger, Eds., Proceedings of the Thirty- Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). AAAI Press, 2018.\n\nGeometry-consistent generative adversarial networks for one-sided unsupervised domain mapping. H Fu, M Gong, C Wang, K Batmanghelich, K Zhang, D Tao, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). H. Fu, M. Gong, C. Wang, K. Batmanghelich, K. Zhang, and D. Tao, \"Geometry-consistent generative adversarial networks for one-sided un- supervised domain mapping,\" in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nOne-sided unsupervised domain mapping. S Benaim, L Wolf, Advances in Neural Information Processing Systems. Curran Associates, Inc30S. Benaim and L. Wolf, \"One-sided unsupervised domain mapping,\" in Advances in Neural Information Processing Systems 30. Curran Associates, Inc., 2017, pp. 752-762. [Online]. Available: http://papers. nips.cc/paper/6677-one-sided-unsupervised-domain-mapping.pdf\n\nKnowledge adaptation: Teaching to adapt. S Ruder, P Ghaffari, J G Breslin, abs/1702.02052CoRR. S. Ruder, P. Ghaffari, and J. G. Breslin, \"Knowledge adaptation: Teaching to adapt,\" CoRR, vol. abs/1702.02052, 2017. [Online].\n\nKnowledge distillation for semi-supervised domain adaptation. M Orbes-Arteaga, J Cardoso, L Srensen, C Igel, S Ourselin, M Modat, M Nielsen, A Pai, M. Orbes-Arteaga, J. Cardoso, L. Srensen, C. Igel, S. Ourselin, M. Mo- dat, M. Nielsen, and A. Pai, \"Knowledge distillation for semi-supervised domain adaptation,\" 2019.\n\nLearning and generalization in overparameterized neural networks, going beyond two layers. Z Allen-Zhu, Y Li, Y Liang, Advances in Neural Information Processing Systems. 32Z. Allen-Zhu, Y. Li, and Y. Liang, \"Learning and generalization in over- parameterized neural networks, going beyond two layers,\" in Advances in Neural Information Processing Systems 32, 2019, pp. 6155-6166.\n\nFilter pruning via geometric median for deep convolutional neural networks acceleration. Y He, P Liu, Z Wang, Z Hu, Y Yang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Y. He, P. Liu, Z. Wang, Z. Hu, and Y. Yang, \"Filter pruning via geometric median for deep convolutional neural networks acceleration,\" in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nImproving neural network quantization without retraining using outlier channel splitting. R Zhao, Y Hu, J Dotzel, C De Sa, Z Zhang, Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research. the 36th International Conference on Machine Learning, ser. Machine Learning ResearchPMLR97R. Zhao, Y. Hu, J. Dotzel, C. De Sa, and Z. Zhang, \"Improving neural network quantization without retraining using outlier channel splitting,\" in Proceedings of the 36th International Conference on Machine Learn- ing, ser. Proceedings of Machine Learning Research, vol. 97. PMLR, 2019, pp. 7543-7552.\n\nDeep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. S Han, H Mao, W J Dally, abs/1510.00149CoRR. S. Han, H. Mao, and W. J. Dally, \"Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding,\" CoRR, vol. abs/1510.00149, 2015.\n\nAnd the bit goes down: Revisiting the quantization of neural networks. Anonymous, Submitted to International Conference on Learning Representations, 2020, under review. Anonymous, \"And the bit goes down: Revisiting the quantization of neural networks,\" in Submitted to International Conference on Learning Representations, 2020, under review. [Online]. Available: https://openreview.net/forum?id=rJehVyrKwH\n\nSpeeding up convolutional neural networks with low rank expansions. M Jaderberg, A Vedaldi, A Zisserman, CoRR. 3866M. Jaderberg, A. Vedaldi, and A. Zisserman, \"Speeding up convolutional neural networks with low rank expansions,\" CoRR, vol. abs/1405.3866, 2014. [Online]. Available: http://arxiv.org/abs/1405.3866\n\nCoordinating filters for faster deep neural networks. W Wen, C Xu, C Wu, Y Wang, Y Chen, H Li, abs/1703.09746CoRR. W. Wen, C. Xu, C. Wu, Y. Wang, Y. Chen, and H. Li, \"Coordinating filters for faster deep neural networks,\" CoRR, vol. abs/1703.09746, 2017. [Online]. Available: http://arxiv.org/abs/1703.09746\n\nModel compression and acceleration for deep neural networks: The principles, progress, and challenges. Y Cheng, D Wang, P Zhou, T Zhang, IEEE Signal Processing Magazine. 126136Y. Cheng, D. Wang, P. Zhou, and T. Zhang, \"Model compression and acceleration for deep neural networks: The principles, progress, and challenges,\" IEEE Signal Processing Magazine, p. 126136, 2018.\n\nUnsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, abs/1612.05424CoRR. K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, \"Unsupervised pixel-level domain adaptation with generative adversarial networks,\" CoRR, vol. abs/1612.05424, 2016. [Online]. Available: http://arxiv.org/abs/1612.05424\n\nConditional generative adversarial network for structured domain adaptation. W Hong, Z Wang, M Yang, J Yuan, CVPR. W. Hong, Z. Wang, M. Yang, and J. Yuan, \"Conditional generative adversarial network for structured domain adaptation,\" in CVPR, 2018.\n\nAsymmetric tri-training for unsupervised domain adaptation. K Saito, Y Ushiku, T Harada, Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research. the 34th International Conference on Machine Learning, ser. Machine Learning Research70K. Saito, Y. Ushiku, and T. Harada, \"Asymmetric tri-training for unsu- pervised domain adaptation,\" in Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 70. PMLR, 2017, pp. 2988-2997.\n\nRevisiting batch normalization for practical domain adaptation. Y Li, N Wang, J Shi, J Liu, X Hou, abs/1603.04779CoRR. Y. Li, N. Wang, J. Shi, J. Liu, and X. Hou, \"Revisiting batch normalization for practical domain adaptation,\" CoRR, vol. abs/1603.04779, 2016. [Online]. Available: http://arxiv.org/abs/1603. 04779\n\nGenerative adversarial guided learning for domain adaptation. K.-Y Wei, C.-T Hsu, BMVC. K.-Y. Wei and C.-T. Hsu, \"Generative adversarial guided learning for domain adaptation,\" in BMVC, 2018.\n\nCyCADA: Cycle-consistent adversarial domain adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A Efros, T Darrell, Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research. the 35th International Conference on Machine Learning, ser. Machine Learning ResearchPMLR80J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell, \"CyCADA: Cycle-consistent adversarial domain adapta- tion,\" in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 80. PMLR, 2018, pp. 1989-1998.\n\nKnowledge adaptation: Teaching to adapt. S Ruder, P Ghaffari, J G Breslin, abs/1702.02052CoRR. S. Ruder, P. Ghaffari, and J. G. Breslin, \"Knowledge adaptation: Teaching to adapt,\" CoRR, vol. abs/1702.02052, 2017. [Online].\n\nKnowledge Distillation for Semisupervised Domain Adaptation. M Orbes-Arteainst, M J Cardoso, L Srensen, C Igel, S Ourselin, M Modat, M Nielsen, A Pai, M. Orbes-Arteainst, M. J. Cardoso, L. Srensen, C. Igel, S. Ourselin, M. Modat, M. Nielsen, and A. Pai, Knowledge Distillation for Semi- supervised Domain Adaptation, 10 2019, pp. 68-76.\n\nAccelerating deep unsupervised domain adaptation with transfer channel pruning. C Yu, J Wang, Y Chen, Z Wu, abs/1904.02654CoRR. C. Yu, J. Wang, Y. Chen, and Z. Wu, \"Accelerating deep unsupervised domain adaptation with transfer channel pruning,\" CoRR, vol. abs/1904.02654, 2019. [Online]. Available: http://arxiv.org/abs/1904. 02654\n", "annotations": {"author": "[{\"end\":239,\"start\":79},{\"end\":451,\"start\":240},{\"end\":639,\"start\":452},{\"end\":802,\"start\":640},{\"end\":959,\"start\":803},{\"end\":1041,\"start\":960}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":85},{\"end\":259,\"start\":245},{\"end\":465,\"start\":458},{\"end\":650,\"start\":645},{\"end\":807,\"start\":803},{\"end\":985,\"start\":974}]", "author_first_name": "[{\"end\":84,\"start\":79},{\"end\":244,\"start\":240},{\"end\":457,\"start\":452},{\"end\":644,\"start\":640},{\"end\":973,\"start\":960}]", "author_affiliation": "[{\"end\":238,\"start\":89},{\"end\":450,\"start\":301},{\"end\":638,\"start\":489},{\"end\":801,\"start\":652},{\"end\":958,\"start\":809},{\"end\":1040,\"start\":1012}]", "title": "[{\"end\":76,\"start\":1},{\"end\":1117,\"start\":1042}]", "venue": null, "abstract": "[{\"end\":3037,\"start\":1230}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3302,\"start\":3299},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3307,\"start\":3304},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3525,\"start\":3522},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4076,\"start\":4073},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4082,\"start\":4078},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4119,\"start\":4115},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4125,\"start\":4121},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5000,\"start\":4996},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5006,\"start\":5002},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5429,\"start\":5425},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5435,\"start\":5431},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5549,\"start\":5545},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5563,\"start\":5559},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5794,\"start\":5790},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5800,\"start\":5796},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6107,\"start\":6103},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6113,\"start\":6109},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6736,\"start\":6732},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6805,\"start\":6801},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7054,\"start\":7050},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9472,\"start\":9469},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9477,\"start\":9474},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9482,\"start\":9479},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9488,\"start\":9484},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9494,\"start\":9490},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9499,\"start\":9496},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9517,\"start\":9513},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9523,\"start\":9519},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9547,\"start\":9543},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9553,\"start\":9549},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9570,\"start\":9566},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9576,\"start\":9572},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10328,\"start\":10324},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10502,\"start\":10498},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10521,\"start\":10517},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10822,\"start\":10818},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11333,\"start\":11329},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11833,\"start\":11829},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11880,\"start\":11876},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11901,\"start\":11897},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11907,\"start\":11903},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11931,\"start\":11927},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11961,\"start\":11957},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11998,\"start\":11994},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12083,\"start\":12079},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12089,\"start\":12085},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12140,\"start\":12136},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12231,\"start\":12227},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12237,\"start\":12233},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12249,\"start\":12245},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12353,\"start\":12349},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12514,\"start\":12510},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12520,\"start\":12516},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12643,\"start\":12639},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13020,\"start\":13016},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13271,\"start\":13267},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13436,\"start\":13432},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13731,\"start\":13727},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14009,\"start\":14005},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14015,\"start\":14011},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14024,\"start\":14020},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14207,\"start\":14203},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14717,\"start\":14713},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14928,\"start\":14924},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16045,\"start\":16041},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16051,\"start\":16047},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16053,\"start\":16052},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16437,\"start\":16433},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16443,\"start\":16439},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16759,\"start\":16755},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17496,\"start\":17492},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20413,\"start\":20409},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21001,\"start\":20997},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21007,\"start\":21003},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22607,\"start\":22603},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25662,\"start\":25658}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28174,\"start\":28087},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29230,\"start\":28175},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29329,\"start\":29231},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29392,\"start\":29330},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29500,\"start\":29393},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30066,\"start\":29501},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30716,\"start\":30067},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31364,\"start\":30717},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31862,\"start\":31365}]", "paragraph": "[{\"end\":3526,\"start\":3056},{\"end\":4258,\"start\":3528},{\"end\":5169,\"start\":4260},{\"end\":5801,\"start\":5171},{\"end\":6806,\"start\":5803},{\"end\":8035,\"start\":6808},{\"end\":9334,\"start\":8037},{\"end\":10329,\"start\":9384},{\"end\":11593,\"start\":10331},{\"end\":13732,\"start\":11632},{\"end\":15271,\"start\":13804},{\"end\":15927,\"start\":15296},{\"end\":16055,\"start\":15966},{\"end\":16556,\"start\":16133},{\"end\":16896,\"start\":16607},{\"end\":17112,\"start\":16957},{\"end\":18291,\"start\":17167},{\"end\":19098,\"start\":18415},{\"end\":19240,\"start\":19125},{\"end\":19431,\"start\":19259},{\"end\":19860,\"start\":19464},{\"end\":20032,\"start\":19917},{\"end\":20181,\"start\":20058},{\"end\":21008,\"start\":20183},{\"end\":21458,\"start\":21010},{\"end\":22608,\"start\":21489},{\"end\":23033,\"start\":22610},{\"end\":23117,\"start\":23035},{\"end\":23302,\"start\":23279},{\"end\":24619,\"start\":23304},{\"end\":25550,\"start\":24651},{\"end\":26383,\"start\":25583},{\"end\":26761,\"start\":26434},{\"end\":27180,\"start\":26763},{\"end\":27350,\"start\":27182},{\"end\":28086,\"start\":27369}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16132,\"start\":16056},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16606,\"start\":16557},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17166,\"start\":17113},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18373,\"start\":18292},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18414,\"start\":18373},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19124,\"start\":19099},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19258,\"start\":19241},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19916,\"start\":19861},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23184,\"start\":23137},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23250,\"start\":23203}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23432,\"start\":23416},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24740,\"start\":24718},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25639,\"start\":25632}]", "section_header": "[{\"end\":3054,\"start\":3039},{\"end\":9353,\"start\":9337},{\"end\":9382,\"start\":9356},{\"end\":11630,\"start\":11596},{\"end\":13802,\"start\":13735},{\"end\":15294,\"start\":15274},{\"end\":15964,\"start\":15930},{\"end\":16955,\"start\":16899},{\"end\":19462,\"start\":19434},{\"end\":20056,\"start\":20035},{\"end\":21487,\"start\":21461},{\"end\":23136,\"start\":23120},{\"end\":23202,\"start\":23186},{\"end\":23277,\"start\":23252},{\"end\":24649,\"start\":24622},{\"end\":25581,\"start\":25553},{\"end\":26432,\"start\":26386},{\"end\":27367,\"start\":27353},{\"end\":28096,\"start\":28088},{\"end\":28189,\"start\":28176},{\"end\":29240,\"start\":29232},{\"end\":29339,\"start\":29331},{\"end\":29410,\"start\":29394},{\"end\":30086,\"start\":30068},{\"end\":30735,\"start\":30718},{\"end\":31387,\"start\":31366}]", "table": "[{\"end\":30066,\"start\":29608},{\"end\":30716,\"start\":30181},{\"end\":31364,\"start\":30829},{\"end\":31862,\"start\":31429}]", "figure_caption": "[{\"end\":28174,\"start\":28098},{\"end\":29230,\"start\":28191},{\"end\":29329,\"start\":29242},{\"end\":29392,\"start\":29341},{\"end\":29500,\"start\":29412},{\"end\":29608,\"start\":29503},{\"end\":30181,\"start\":30090},{\"end\":30829,\"start\":30738},{\"end\":31429,\"start\":31389}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15360,\"start\":15352},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15622,\"start\":15614},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18189,\"start\":18181},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26777,\"start\":26772}]", "bib_author_first_name": "[{\"end\":31988,\"start\":31987},{\"end\":31996,\"start\":31995},{\"end\":32004,\"start\":32003},{\"end\":32115,\"start\":32114},{\"end\":32121,\"start\":32120},{\"end\":32133,\"start\":32132},{\"end\":32143,\"start\":32142},{\"end\":32145,\"start\":32144},{\"end\":32408,\"start\":32407},{\"end\":32422,\"start\":32421},{\"end\":32431,\"start\":32430},{\"end\":32439,\"start\":32438},{\"end\":32453,\"start\":32452},{\"end\":32461,\"start\":32460},{\"end\":32470,\"start\":32469},{\"end\":32740,\"start\":32739},{\"end\":32742,\"start\":32741},{\"end\":32760,\"start\":32759},{\"end\":32771,\"start\":32770},{\"end\":32780,\"start\":32779},{\"end\":33244,\"start\":33243},{\"end\":33255,\"start\":33254},{\"end\":33633,\"start\":33632},{\"end\":33643,\"start\":33642},{\"end\":33654,\"start\":33653},{\"end\":33656,\"start\":33655},{\"end\":33674,\"start\":33673},{\"end\":33683,\"start\":33682},{\"end\":33691,\"start\":33690},{\"end\":33693,\"start\":33692},{\"end\":33701,\"start\":33700},{\"end\":34046,\"start\":34045},{\"end\":34059,\"start\":34058},{\"end\":34069,\"start\":34068},{\"end\":34078,\"start\":34077},{\"end\":34088,\"start\":34087},{\"end\":34455,\"start\":34454},{\"end\":34457,\"start\":34456},{\"end\":34475,\"start\":34474},{\"end\":34486,\"start\":34485},{\"end\":34495,\"start\":34494},{\"end\":34827,\"start\":34826},{\"end\":34835,\"start\":34834},{\"end\":35205,\"start\":35204},{\"end\":35212,\"start\":35211},{\"end\":35219,\"start\":35218},{\"end\":35227,\"start\":35226},{\"end\":35236,\"start\":35235},{\"end\":35570,\"start\":35569},{\"end\":35580,\"start\":35579},{\"end\":35591,\"start\":35590},{\"end\":35895,\"start\":35894},{\"end\":35903,\"start\":35902},{\"end\":35910,\"start\":35909},{\"end\":35916,\"start\":35915},{\"end\":36205,\"start\":36204},{\"end\":36212,\"start\":36211},{\"end\":36219,\"start\":36218},{\"end\":36226,\"start\":36225},{\"end\":36234,\"start\":36233},{\"end\":36242,\"start\":36241},{\"end\":36244,\"start\":36243},{\"end\":36605,\"start\":36604},{\"end\":36617,\"start\":36616},{\"end\":36631,\"start\":36630},{\"end\":36637,\"start\":36636},{\"end\":36905,\"start\":36904},{\"end\":36915,\"start\":36914},{\"end\":36917,\"start\":36916},{\"end\":37133,\"start\":37132},{\"end\":37141,\"start\":37140},{\"end\":37371,\"start\":37370},{\"end\":37380,\"start\":37379},{\"end\":37526,\"start\":37525},{\"end\":37535,\"start\":37534},{\"end\":37546,\"start\":37545},{\"end\":37556,\"start\":37555},{\"end\":37885,\"start\":37884},{\"end\":37893,\"start\":37892},{\"end\":37900,\"start\":37899},{\"end\":37908,\"start\":37907},{\"end\":37910,\"start\":37909},{\"end\":38975,\"start\":38974},{\"end\":38981,\"start\":38980},{\"end\":38989,\"start\":38988},{\"end\":38997,\"start\":38996},{\"end\":39014,\"start\":39013},{\"end\":39023,\"start\":39022},{\"end\":39390,\"start\":39389},{\"end\":39400,\"start\":39399},{\"end\":39787,\"start\":39786},{\"end\":39796,\"start\":39795},{\"end\":39808,\"start\":39807},{\"end\":39810,\"start\":39809},{\"end\":40032,\"start\":40031},{\"end\":40049,\"start\":40048},{\"end\":40060,\"start\":40059},{\"end\":40071,\"start\":40070},{\"end\":40079,\"start\":40078},{\"end\":40091,\"start\":40090},{\"end\":40100,\"start\":40099},{\"end\":40111,\"start\":40110},{\"end\":40380,\"start\":40379},{\"end\":40393,\"start\":40392},{\"end\":40399,\"start\":40398},{\"end\":40759,\"start\":40758},{\"end\":40765,\"start\":40764},{\"end\":40772,\"start\":40771},{\"end\":40780,\"start\":40779},{\"end\":40786,\"start\":40785},{\"end\":41176,\"start\":41175},{\"end\":41184,\"start\":41183},{\"end\":41190,\"start\":41189},{\"end\":41200,\"start\":41199},{\"end\":41203,\"start\":41201},{\"end\":41209,\"start\":41208},{\"end\":41833,\"start\":41832},{\"end\":41840,\"start\":41839},{\"end\":41847,\"start\":41846},{\"end\":41849,\"start\":41848},{\"end\":42528,\"start\":42527},{\"end\":42541,\"start\":42540},{\"end\":42552,\"start\":42551},{\"end\":42828,\"start\":42827},{\"end\":42835,\"start\":42834},{\"end\":42841,\"start\":42840},{\"end\":42847,\"start\":42846},{\"end\":42855,\"start\":42854},{\"end\":42863,\"start\":42862},{\"end\":43186,\"start\":43185},{\"end\":43195,\"start\":43194},{\"end\":43203,\"start\":43202},{\"end\":43211,\"start\":43210},{\"end\":43538,\"start\":43537},{\"end\":43551,\"start\":43550},{\"end\":43564,\"start\":43563},{\"end\":43573,\"start\":43572},{\"end\":43582,\"start\":43581},{\"end\":43926,\"start\":43925},{\"end\":43934,\"start\":43933},{\"end\":43942,\"start\":43941},{\"end\":43950,\"start\":43949},{\"end\":44159,\"start\":44158},{\"end\":44168,\"start\":44167},{\"end\":44178,\"start\":44177},{\"end\":44713,\"start\":44712},{\"end\":44719,\"start\":44718},{\"end\":44727,\"start\":44726},{\"end\":44734,\"start\":44733},{\"end\":44741,\"start\":44740},{\"end\":45031,\"start\":45027},{\"end\":45041,\"start\":45037},{\"end\":45215,\"start\":45214},{\"end\":45226,\"start\":45225},{\"end\":45235,\"start\":45234},{\"end\":45246,\"start\":45242},{\"end\":45253,\"start\":45252},{\"end\":45262,\"start\":45261},{\"end\":45272,\"start\":45271},{\"end\":45281,\"start\":45280},{\"end\":45847,\"start\":45846},{\"end\":45856,\"start\":45855},{\"end\":45868,\"start\":45867},{\"end\":45870,\"start\":45869},{\"end\":46091,\"start\":46090},{\"end\":46110,\"start\":46109},{\"end\":46112,\"start\":46111},{\"end\":46123,\"start\":46122},{\"end\":46134,\"start\":46133},{\"end\":46142,\"start\":46141},{\"end\":46154,\"start\":46153},{\"end\":46163,\"start\":46162},{\"end\":46174,\"start\":46173},{\"end\":46448,\"start\":46447},{\"end\":46454,\"start\":46453},{\"end\":46462,\"start\":46461},{\"end\":46470,\"start\":46469}]", "bib_author_last_name": "[{\"end\":31993,\"start\":31989},{\"end\":32001,\"start\":31997},{\"end\":32009,\"start\":32005},{\"end\":32118,\"start\":32116},{\"end\":32130,\"start\":32122},{\"end\":32140,\"start\":32134},{\"end\":32154,\"start\":32146},{\"end\":32419,\"start\":32409},{\"end\":32428,\"start\":32423},{\"end\":32436,\"start\":32432},{\"end\":32450,\"start\":32440},{\"end\":32458,\"start\":32454},{\"end\":32467,\"start\":32462},{\"end\":32478,\"start\":32471},{\"end\":32757,\"start\":32743},{\"end\":32768,\"start\":32761},{\"end\":32777,\"start\":32772},{\"end\":32792,\"start\":32781},{\"end\":33252,\"start\":33245},{\"end\":33262,\"start\":33256},{\"end\":33640,\"start\":33634},{\"end\":33651,\"start\":33644},{\"end\":33671,\"start\":33657},{\"end\":33680,\"start\":33675},{\"end\":33688,\"start\":33684},{\"end\":33698,\"start\":33694},{\"end\":33709,\"start\":33702},{\"end\":34056,\"start\":34047},{\"end\":34066,\"start\":34060},{\"end\":34075,\"start\":34070},{\"end\":34085,\"start\":34079},{\"end\":34094,\"start\":34089},{\"end\":34472,\"start\":34458},{\"end\":34483,\"start\":34476},{\"end\":34492,\"start\":34487},{\"end\":34507,\"start\":34496},{\"end\":34832,\"start\":34828},{\"end\":34840,\"start\":34836},{\"end\":35209,\"start\":35206},{\"end\":35216,\"start\":35213},{\"end\":35224,\"start\":35220},{\"end\":35233,\"start\":35228},{\"end\":35244,\"start\":35237},{\"end\":35577,\"start\":35571},{\"end\":35588,\"start\":35581},{\"end\":35596,\"start\":35592},{\"end\":35900,\"start\":35896},{\"end\":35907,\"start\":35904},{\"end\":35913,\"start\":35911},{\"end\":35920,\"start\":35917},{\"end\":36209,\"start\":36206},{\"end\":36216,\"start\":36213},{\"end\":36223,\"start\":36220},{\"end\":36231,\"start\":36227},{\"end\":36239,\"start\":36235},{\"end\":36249,\"start\":36245},{\"end\":36614,\"start\":36606},{\"end\":36628,\"start\":36618},{\"end\":36634,\"start\":36632},{\"end\":36649,\"start\":36638},{\"end\":36912,\"start\":36906},{\"end\":36922,\"start\":36918},{\"end\":37138,\"start\":37134},{\"end\":37146,\"start\":37142},{\"end\":37377,\"start\":37372},{\"end\":37390,\"start\":37381},{\"end\":37532,\"start\":37527},{\"end\":37543,\"start\":37536},{\"end\":37553,\"start\":37547},{\"end\":37564,\"start\":37557},{\"end\":37890,\"start\":37886},{\"end\":37897,\"start\":37894},{\"end\":37905,\"start\":37901},{\"end\":37917,\"start\":37911},{\"end\":38978,\"start\":38976},{\"end\":38986,\"start\":38982},{\"end\":38994,\"start\":38990},{\"end\":39011,\"start\":38998},{\"end\":39020,\"start\":39015},{\"end\":39027,\"start\":39024},{\"end\":39397,\"start\":39391},{\"end\":39405,\"start\":39401},{\"end\":39793,\"start\":39788},{\"end\":39805,\"start\":39797},{\"end\":39818,\"start\":39811},{\"end\":40046,\"start\":40033},{\"end\":40057,\"start\":40050},{\"end\":40068,\"start\":40061},{\"end\":40076,\"start\":40072},{\"end\":40088,\"start\":40080},{\"end\":40097,\"start\":40092},{\"end\":40108,\"start\":40101},{\"end\":40115,\"start\":40112},{\"end\":40390,\"start\":40381},{\"end\":40396,\"start\":40394},{\"end\":40405,\"start\":40400},{\"end\":40762,\"start\":40760},{\"end\":40769,\"start\":40766},{\"end\":40777,\"start\":40773},{\"end\":40783,\"start\":40781},{\"end\":40791,\"start\":40787},{\"end\":41181,\"start\":41177},{\"end\":41187,\"start\":41185},{\"end\":41197,\"start\":41191},{\"end\":41206,\"start\":41204},{\"end\":41215,\"start\":41210},{\"end\":41837,\"start\":41834},{\"end\":41844,\"start\":41841},{\"end\":41855,\"start\":41850},{\"end\":42131,\"start\":42122},{\"end\":42538,\"start\":42529},{\"end\":42549,\"start\":42542},{\"end\":42562,\"start\":42553},{\"end\":42832,\"start\":42829},{\"end\":42838,\"start\":42836},{\"end\":42844,\"start\":42842},{\"end\":42852,\"start\":42848},{\"end\":42860,\"start\":42856},{\"end\":42866,\"start\":42864},{\"end\":43192,\"start\":43187},{\"end\":43200,\"start\":43196},{\"end\":43208,\"start\":43204},{\"end\":43217,\"start\":43212},{\"end\":43548,\"start\":43539},{\"end\":43561,\"start\":43552},{\"end\":43570,\"start\":43565},{\"end\":43579,\"start\":43574},{\"end\":43591,\"start\":43583},{\"end\":43931,\"start\":43927},{\"end\":43939,\"start\":43935},{\"end\":43947,\"start\":43943},{\"end\":43955,\"start\":43951},{\"end\":44165,\"start\":44160},{\"end\":44175,\"start\":44169},{\"end\":44185,\"start\":44179},{\"end\":44716,\"start\":44714},{\"end\":44724,\"start\":44720},{\"end\":44731,\"start\":44728},{\"end\":44738,\"start\":44735},{\"end\":44745,\"start\":44742},{\"end\":45035,\"start\":45032},{\"end\":45045,\"start\":45042},{\"end\":45223,\"start\":45216},{\"end\":45232,\"start\":45227},{\"end\":45240,\"start\":45236},{\"end\":45250,\"start\":45247},{\"end\":45259,\"start\":45254},{\"end\":45269,\"start\":45263},{\"end\":45278,\"start\":45273},{\"end\":45289,\"start\":45282},{\"end\":45853,\"start\":45848},{\"end\":45865,\"start\":45857},{\"end\":45878,\"start\":45871},{\"end\":46107,\"start\":46092},{\"end\":46120,\"start\":46113},{\"end\":46131,\"start\":46124},{\"end\":46139,\"start\":46135},{\"end\":46151,\"start\":46143},{\"end\":46160,\"start\":46155},{\"end\":46171,\"start\":46164},{\"end\":46178,\"start\":46175},{\"end\":46451,\"start\":46449},{\"end\":46459,\"start\":46455},{\"end\":46467,\"start\":46463},{\"end\":46473,\"start\":46471}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32110,\"start\":31926},{\"attributes\":{\"doi\":\"abs/1703.06870\",\"id\":\"b1\"},\"end\":32336,\"start\":32112},{\"attributes\":{\"id\":\"b2\"},\"end\":32640,\"start\":32338},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3839828},\"end\":33167,\"start\":32642},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53388625},\"end\":33547,\"start\":33169},{\"attributes\":{\"doi\":\"abs/1907.02547\",\"id\":\"b5\",\"matched_paper_id\":195820495},\"end\":33993,\"start\":33549},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195657904},\"end\":34363,\"start\":33995},{\"attributes\":{\"doi\":\"abs/1906.08746\",\"id\":\"b7\",\"matched_paper_id\":195218875},\"end\":34769,\"start\":34365},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":162184071},\"end\":35161,\"start\":34771},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52978527},\"end\":35521,\"start\":35163},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7200347},\"end\":35857,\"start\":35523},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":131765296},\"end\":36152,\"start\":35859},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":102483181},\"end\":36501,\"start\":36154},{\"attributes\":{\"doi\":\"abs/1902.03393\",\"id\":\"b13\",\"matched_paper_id\":60440652},\"end\":36871,\"start\":36503},{\"attributes\":{\"doi\":\"abs/1812.02849\",\"id\":\"b14\",\"matched_paper_id\":54457382},\"end\":37089,\"start\":36873},{\"attributes\":{\"doi\":\"abs/1802.03601\",\"id\":\"b15\",\"matched_paper_id\":3654323},\"end\":37317,\"start\":37091},{\"attributes\":{\"id\":\"b16\"},\"end\":37477,\"start\":37319},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4357800},\"end\":37820,\"start\":37479},{\"attributes\":{\"id\":\"b18\"},\"end\":38040,\"start\":37822},{\"attributes\":{\"id\":\"b19\"},\"end\":38877,\"start\":38042},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":54533862},\"end\":39348,\"start\":38879},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1629634},\"end\":39743,\"start\":39350},{\"attributes\":{\"doi\":\"abs/1702.02052\",\"id\":\"b22\",\"matched_paper_id\":9644084},\"end\":39967,\"start\":39745},{\"attributes\":{\"id\":\"b23\"},\"end\":40286,\"start\":39969},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53287096},\"end\":40667,\"start\":40288},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":102350938},\"end\":41083,\"start\":40669},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":59413897},\"end\":41725,\"start\":41085},{\"attributes\":{\"doi\":\"abs/1510.00149\",\"id\":\"b27\",\"matched_paper_id\":2134321},\"end\":42049,\"start\":41727},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":196470957},\"end\":42457,\"start\":42051},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":17864746},\"end\":42771,\"start\":42459},{\"attributes\":{\"doi\":\"abs/1703.09746\",\"id\":\"b30\",\"matched_paper_id\":6863796},\"end\":43080,\"start\":42773},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":32588614},\"end\":43454,\"start\":43082},{\"attributes\":{\"doi\":\"abs/1612.05424\",\"id\":\"b32\",\"matched_paper_id\":206595056},\"end\":43846,\"start\":43456},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":49547920},\"end\":44096,\"start\":43848},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12570770},\"end\":44646,\"start\":44098},{\"attributes\":{\"doi\":\"abs/1603.04779\",\"id\":\"b35\",\"matched_paper_id\":5069968},\"end\":44963,\"start\":44648},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52289556},\"end\":45156,\"start\":44965},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":7646250},\"end\":45803,\"start\":45158},{\"attributes\":{\"doi\":\"abs/1702.02052\",\"id\":\"b38\",\"matched_paper_id\":9644084},\"end\":46027,\"start\":45805},{\"attributes\":{\"id\":\"b39\"},\"end\":46365,\"start\":46029},{\"attributes\":{\"doi\":\"abs/1904.02654\",\"id\":\"b40\",\"matched_paper_id\":102354179},\"end\":46699,\"start\":46367}]", "bib_title": "[{\"end\":32737,\"start\":32642},{\"end\":33241,\"start\":33169},{\"end\":33630,\"start\":33549},{\"end\":34043,\"start\":33995},{\"end\":34452,\"start\":34365},{\"end\":34824,\"start\":34771},{\"end\":35202,\"start\":35163},{\"end\":35567,\"start\":35523},{\"end\":35892,\"start\":35859},{\"end\":36202,\"start\":36154},{\"end\":36602,\"start\":36503},{\"end\":36902,\"start\":36873},{\"end\":37130,\"start\":37091},{\"end\":37523,\"start\":37479},{\"end\":38972,\"start\":38879},{\"end\":39387,\"start\":39350},{\"end\":39784,\"start\":39745},{\"end\":40377,\"start\":40288},{\"end\":40756,\"start\":40669},{\"end\":41173,\"start\":41085},{\"end\":41830,\"start\":41727},{\"end\":42120,\"start\":42051},{\"end\":42525,\"start\":42459},{\"end\":42825,\"start\":42773},{\"end\":43183,\"start\":43082},{\"end\":43535,\"start\":43456},{\"end\":43923,\"start\":43848},{\"end\":44156,\"start\":44098},{\"end\":44710,\"start\":44648},{\"end\":45025,\"start\":44965},{\"end\":45212,\"start\":45158},{\"end\":45844,\"start\":45805},{\"end\":46445,\"start\":46367}]", "bib_author": "[{\"end\":31995,\"start\":31987},{\"end\":32003,\"start\":31995},{\"end\":32011,\"start\":32003},{\"end\":32120,\"start\":32114},{\"end\":32132,\"start\":32120},{\"end\":32142,\"start\":32132},{\"end\":32156,\"start\":32142},{\"end\":32421,\"start\":32407},{\"end\":32430,\"start\":32421},{\"end\":32438,\"start\":32430},{\"end\":32452,\"start\":32438},{\"end\":32460,\"start\":32452},{\"end\":32469,\"start\":32460},{\"end\":32480,\"start\":32469},{\"end\":32759,\"start\":32739},{\"end\":32770,\"start\":32759},{\"end\":32779,\"start\":32770},{\"end\":32794,\"start\":32779},{\"end\":33254,\"start\":33243},{\"end\":33264,\"start\":33254},{\"end\":33642,\"start\":33632},{\"end\":33653,\"start\":33642},{\"end\":33673,\"start\":33653},{\"end\":33682,\"start\":33673},{\"end\":33690,\"start\":33682},{\"end\":33700,\"start\":33690},{\"end\":33711,\"start\":33700},{\"end\":34058,\"start\":34045},{\"end\":34068,\"start\":34058},{\"end\":34077,\"start\":34068},{\"end\":34087,\"start\":34077},{\"end\":34096,\"start\":34087},{\"end\":34474,\"start\":34454},{\"end\":34485,\"start\":34474},{\"end\":34494,\"start\":34485},{\"end\":34509,\"start\":34494},{\"end\":34834,\"start\":34826},{\"end\":34842,\"start\":34834},{\"end\":35211,\"start\":35204},{\"end\":35218,\"start\":35211},{\"end\":35226,\"start\":35218},{\"end\":35235,\"start\":35226},{\"end\":35246,\"start\":35235},{\"end\":35579,\"start\":35569},{\"end\":35590,\"start\":35579},{\"end\":35598,\"start\":35590},{\"end\":35902,\"start\":35894},{\"end\":35909,\"start\":35902},{\"end\":35915,\"start\":35909},{\"end\":35922,\"start\":35915},{\"end\":36211,\"start\":36204},{\"end\":36218,\"start\":36211},{\"end\":36225,\"start\":36218},{\"end\":36233,\"start\":36225},{\"end\":36241,\"start\":36233},{\"end\":36251,\"start\":36241},{\"end\":36616,\"start\":36604},{\"end\":36630,\"start\":36616},{\"end\":36636,\"start\":36630},{\"end\":36651,\"start\":36636},{\"end\":36914,\"start\":36904},{\"end\":36924,\"start\":36914},{\"end\":37140,\"start\":37132},{\"end\":37148,\"start\":37140},{\"end\":37379,\"start\":37370},{\"end\":37392,\"start\":37379},{\"end\":37534,\"start\":37525},{\"end\":37545,\"start\":37534},{\"end\":37555,\"start\":37545},{\"end\":37566,\"start\":37555},{\"end\":37892,\"start\":37884},{\"end\":37899,\"start\":37892},{\"end\":37907,\"start\":37899},{\"end\":37919,\"start\":37907},{\"end\":38980,\"start\":38974},{\"end\":38988,\"start\":38980},{\"end\":38996,\"start\":38988},{\"end\":39013,\"start\":38996},{\"end\":39022,\"start\":39013},{\"end\":39029,\"start\":39022},{\"end\":39399,\"start\":39389},{\"end\":39407,\"start\":39399},{\"end\":39795,\"start\":39786},{\"end\":39807,\"start\":39795},{\"end\":39820,\"start\":39807},{\"end\":40048,\"start\":40031},{\"end\":40059,\"start\":40048},{\"end\":40070,\"start\":40059},{\"end\":40078,\"start\":40070},{\"end\":40090,\"start\":40078},{\"end\":40099,\"start\":40090},{\"end\":40110,\"start\":40099},{\"end\":40117,\"start\":40110},{\"end\":40392,\"start\":40379},{\"end\":40398,\"start\":40392},{\"end\":40407,\"start\":40398},{\"end\":40764,\"start\":40758},{\"end\":40771,\"start\":40764},{\"end\":40779,\"start\":40771},{\"end\":40785,\"start\":40779},{\"end\":40793,\"start\":40785},{\"end\":41183,\"start\":41175},{\"end\":41189,\"start\":41183},{\"end\":41199,\"start\":41189},{\"end\":41208,\"start\":41199},{\"end\":41217,\"start\":41208},{\"end\":41839,\"start\":41832},{\"end\":41846,\"start\":41839},{\"end\":41857,\"start\":41846},{\"end\":42133,\"start\":42122},{\"end\":42540,\"start\":42527},{\"end\":42551,\"start\":42540},{\"end\":42564,\"start\":42551},{\"end\":42834,\"start\":42827},{\"end\":42840,\"start\":42834},{\"end\":42846,\"start\":42840},{\"end\":42854,\"start\":42846},{\"end\":42862,\"start\":42854},{\"end\":42868,\"start\":42862},{\"end\":43194,\"start\":43185},{\"end\":43202,\"start\":43194},{\"end\":43210,\"start\":43202},{\"end\":43219,\"start\":43210},{\"end\":43550,\"start\":43537},{\"end\":43563,\"start\":43550},{\"end\":43572,\"start\":43563},{\"end\":43581,\"start\":43572},{\"end\":43593,\"start\":43581},{\"end\":43933,\"start\":43925},{\"end\":43941,\"start\":43933},{\"end\":43949,\"start\":43941},{\"end\":43957,\"start\":43949},{\"end\":44167,\"start\":44158},{\"end\":44177,\"start\":44167},{\"end\":44187,\"start\":44177},{\"end\":44718,\"start\":44712},{\"end\":44726,\"start\":44718},{\"end\":44733,\"start\":44726},{\"end\":44740,\"start\":44733},{\"end\":44747,\"start\":44740},{\"end\":45037,\"start\":45027},{\"end\":45047,\"start\":45037},{\"end\":45225,\"start\":45214},{\"end\":45234,\"start\":45225},{\"end\":45242,\"start\":45234},{\"end\":45252,\"start\":45242},{\"end\":45261,\"start\":45252},{\"end\":45271,\"start\":45261},{\"end\":45280,\"start\":45271},{\"end\":45291,\"start\":45280},{\"end\":45855,\"start\":45846},{\"end\":45867,\"start\":45855},{\"end\":45880,\"start\":45867},{\"end\":46109,\"start\":46090},{\"end\":46122,\"start\":46109},{\"end\":46133,\"start\":46122},{\"end\":46141,\"start\":46133},{\"end\":46153,\"start\":46141},{\"end\":46162,\"start\":46153},{\"end\":46173,\"start\":46162},{\"end\":46180,\"start\":46173},{\"end\":46453,\"start\":46447},{\"end\":46461,\"start\":46453},{\"end\":46469,\"start\":46461},{\"end\":46475,\"start\":46469}]", "bib_venue": "[{\"end\":31985,\"start\":31926},{\"end\":32180,\"start\":32170},{\"end\":32405,\"start\":32338},{\"end\":32888,\"start\":32794},{\"end\":33316,\"start\":33264},{\"end\":33729,\"start\":33725},{\"end\":34165,\"start\":34096},{\"end\":34527,\"start\":34523},{\"end\":34891,\"start\":34842},{\"end\":35298,\"start\":35246},{\"end\":35653,\"start\":35598},{\"end\":35991,\"start\":35922},{\"end\":36310,\"start\":36251},{\"end\":36669,\"start\":36665},{\"end\":36942,\"start\":36938},{\"end\":37166,\"start\":37162},{\"end\":37368,\"start\":37319},{\"end\":37635,\"start\":37566},{\"end\":37882,\"start\":37822},{\"end\":38288,\"start\":38042},{\"end\":39098,\"start\":39029},{\"end\":39456,\"start\":39407},{\"end\":39838,\"start\":39834},{\"end\":40029,\"start\":39969},{\"end\":40456,\"start\":40407},{\"end\":40862,\"start\":40793},{\"end\":41332,\"start\":41217},{\"end\":41875,\"start\":41871},{\"end\":42218,\"start\":42133},{\"end\":42568,\"start\":42564},{\"end\":42886,\"start\":42882},{\"end\":43250,\"start\":43219},{\"end\":43611,\"start\":43607},{\"end\":43961,\"start\":43957},{\"end\":44302,\"start\":44187},{\"end\":44765,\"start\":44761},{\"end\":45051,\"start\":45047},{\"end\":45406,\"start\":45291},{\"end\":45898,\"start\":45894},{\"end\":46088,\"start\":46029},{\"end\":46493,\"start\":46489},{\"end\":38557,\"start\":38326},{\"end\":41419,\"start\":41334},{\"end\":44389,\"start\":44304},{\"end\":45493,\"start\":45408}]"}}}, "year": 2023, "month": 12, "day": 17}