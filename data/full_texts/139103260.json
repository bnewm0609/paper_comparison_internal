{"id": 139103260, "updated": "2023-10-04 05:16:33.183", "metadata": {"title": "RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion", "authors": "[{\"first\":\"Muhammad\",\"last\":\"Sarmad\",\"middle\":[]},{\"first\":\"Hyunjoo\",\"last\":\"Lee\",\"middle\":[\"Jenny\"]},{\"first\":\"Young\",\"last\":\"Kim\",\"middle\":[\"Min\"]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 4, "day": 28}, "abstract": "We present RL-GAN-Net, where a reinforcement learning (RL) agent provides fast and robust control of a generative adversarial network (GAN). Our framework is applied to point cloud shape completion that converts noisy, partial point cloud data into a high-fidelity completed shape by controlling the GAN. While a GAN is unstable and hard to train, we circumvent the problem by (1) training the GAN on the latent space representation whose dimension is reduced compared to the raw point cloud input and (2) using an RL agent to find the correct input to the GAN to generate the latent space representation of the shape that best fits the current input of incomplete point cloud. The suggested pipeline robustly completes point cloud with large missing regions. To the best of our knowledge, this is the first attempt to train an RL agent to control the GAN, which effectively learns the highly nonlinear mapping from the input noise of the GAN to the latent space of point cloud. The RL agent replaces the need for complex optimization and consequently makes our technique real time. Additionally, we demonstrate that our pipelines can be used to enhance the classification accuracy of point cloud with missing data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.12304", "mag": "2951771597", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/SarmadLK19", "doi": "10.1109/cvpr.2019.00605"}}, "content": {"source": {"pdf_hash": "429b3f61ab409d087161ee220db48497e455cba3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.12304v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.12304", "status": "GREEN"}}, "grobid": {"id": "094e65c57200339b701558056d6b43c7ad1fbefb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/429b3f61ab409d087161ee220db48497e455cba3.txt", "contents": "\nRL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion\n\n\nMuhammad Sarmad sarmad@kaist.ac.kr \nHyunjoo Jenny Lee hyunjoo.lee@kaist.ac.kr \nSouth Korea \nYoung Min Kim youngmin.kim@snu.ac.kr \n\nKAIST South\nKorea\n\n\nKAIST\nKIST\nSNU South Korea\n\nRL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion\n\nWe present RL-GAN-Net, where a reinforcement learning (RL) agent provides fast and robust control of a generative adversarial network (GAN). Our framework is applied to point cloud shape completion that converts noisy, partial point cloud data into a high-fidelity completed shape by controlling the GAN. While a GAN is unstable and hard to train, we circumvent the problem by (1) training the GAN on the latent space representation whose dimension is reduced compared to the raw point cloud input and (2) using an RL agent to find the correct input to the GAN to generate the latent space representation of the shape that best fits the current input of incomplete point cloud. The suggested pipeline robustly completes point cloud with large missing regions. To the best of our knowledge, this is the first attempt to train an RL agent to control the GAN, which effectively learns the highly nonlinear mapping from the input noise of the GAN to the latent space of point cloud. The RL agent replaces the need for complex optimization and consequently makes our technique real time. Additionally, we demonstrate that our pipelines can be used to enhance the classification accuracy of point cloud with missing data.\n\nIntroduction\n\nAcquisition of 3D data, either from laser scanners, stereo reconstruction, or RGB-D cameras, is in the form of the point cloud, which is a list of Cartesian coordinates. The raw output usually suffers from large missing region due to limited viewing angles, occlusions, sensor resolution, or unstable measurement in the texture-less region (stereo reconstruction) or specular materials. To utilize the measurements, further post-processing is essential which includes registration, denoising, resampling, semantic understanding and eventually reconstructing the 3D mesh model.\n\nIn this work, we focus on filling the missing regions * co-corresponding authors given input data missing 70% of its original points. We present RL-GAN-Net, which observes a partial input point cloud data (Pin) and completes the shape within a matter of milliseconds. Even when input is severely distorted, our approach completes the shape with high-fidelity compared to the previous approach using autoencoder (AE) [1]. within the 3D data by a data-driven method. The primary form of acquired measurements is the 3D point cloud which is unstructured and unordered. Therefore, it is not possible to directly apply conventional convolutional neural networks (CNN) approaches which work nicely for structured data e.g. for 2D grids of pixels [21,22,5]. The extensions of CNN in 3D have been shown to work well with 3D voxel grid [38,7,8]. However, the computing cost grows dras- Figure 2: The forward pass of our shape completion network. By observing an encoded partial point cloud, our RL-GAN-Net selects an appropriate input for the latent GAN and generates a cleaned encoding for the shape. The synthesized latent representation is decoded to get the completed point cloud in real time. In our hybrid version, the discriminator finally selects the best shape.\n\ntically with voxel resolution due to the cubic nature of 3D space. Recently PointNet [34] has made it possible to directly process point cloud data despite its unstructured and permutation invariant nature. This has opened new avenues for employing point cloud data, instead of voxels, to contemporary computer-vision applications, e.g. segmentation, classification and shape completion [1,15,35,9,10].\n\nIn this paper, we propose our pipeline RL-GAN-Net as shown in Fig. 2. It is a reinforcement learning agent controlled GAN (generative adversarial network) based network which can predict complete point cloud from incomplete data. As a pre-processing step, we train an autoencoder (AE) to get the latent space representation of the point cloud and we further use this representation to train a GAN [1]. Our agent is then trained to take an 'action' by selecting an appropriate z vector for the generator of the pre-trained GAN to synthesize the latent space representation of the complete point cloud. Unlike the previous approaches which use back-propagation to find the correct z vector of the GAN [15,41], our approach based on an RL agent is real time and also robust to large missing regions. However, for data with small missing regions, a simple AE can reliably recover the original shape. Therefore, we use the help of a pre-trained discriminator of GAN to decide the winner between the decoded output of the GAN and the output of the AE. The final choice of completed shape preserves the global structure of the shape and is consistent with the partial observation. A few results with 70% missing data are shown in Fig. 14.\n\nTo the best of our knowledge, we are the first to introduce this unique combination of RL and GAN for solving the point cloud shape completion problem. We believe that the concept of using an RL agent to control the GAN's output opens up new possibilities to overcome underlying instabilities of current deep architectures. This can also lead to employing similar concept for problems that share the same fundamentals of shape completion e.g. image in-painting [41].\n\nOur key contributions are the following:\n\n\u2022 We present a shape completion framework that is robust to low-availability of point cloud data without any prior knowledge about visibility or noise characteristics.\n\n\u2022 We suggest a real-time control of GAN to quickly generate desired output without optimization. Because of the real-time nature, we demonstrate that our pipeline can pre-process the input for other point cloud processing pipelines, such as classification.\n\n\u2022 We demonstrate the first attempt to use deep RL framework for the shape completion problem. In doing so, we demonstrate a unique RL problem formulation.\n\n\nRelated Works\n\nShape Completion and Deep Learning. 3D shape completion is a fundamental problem which is faced when processing 3D measurements of the real world. Regardless of the modality of the sensors (multi-view stereo, the structure of light sensors, RGB-D cameras, lidars, etc.), the output point cloud exhibits large holes due to complex occlusions, limited field of view and unreliable measurements (because of material properties or texture-less regions). Early works use symmetry [39] or example shapes [33] to fill the missing regions. More recently databases of shapes has been used to retrieve the shape that is the closest to the current measurement [19,23].\n\nRecently, deep learning has revolutionized the field of computer vision due to the enhanced computational power, the availability of large datasets, and the introduction of efficient architectures, such as the CNN [5]. Deep learning has demonstrated superior performance on many traditional computer vision tasks such as classification [21,22,16] and segmentation [25,30]. Our 3D shape completion adapts the successful techniques from the field of deep learning and uses data-driven methods to complete the missing parts.\n\n3D deep learning architecture largely depends on the choice of the 3D data representation, namely volumetric voxel grid, mesh, or point cloud. The extension of CNN in 3D works best with 3D voxel grids, which can be generated from point measurements with additional processing. Dai et al. [7] introduced a voxel-based shape completion framework which consists of a data-driven network and an analytic 3D shape synthesis technique. However, voxel-based techniques are limited in resolution because the network complexity and required computations increase drastically with the resolution. Recently, Dai et al. [8] extended this work to perform scene completion and semantic segmentation using coarse-to-fine strategy and using subvolumes. There are also manifold-based deep learning approaches [29] to analyze various characteristics of complete shapes, but these lines of work depend on the topological structure of the mesh. Such techniques are not compatible with point cloud.\n\nPoint cloud is the raw output of many acquisition techniques. It is more efficient compared to the voxel-based representation, which is required to fully cover the entire volume including large empty spaces. However, most of the successful deep learning architectures can not be deployed on point cloud data. Stutz et al. [38] introduced a network which consumes incomplete point cloud but they use a pre-trained decoder to get a voxelized representation of the complete shape. Direct point cloud processing has been made possible recently due to the emergence of new architecture such as PointNet [34] and others [35,9,17].\n\nAchlioptas et al. [1] explored learning shape representation with auto-encoder. They also investigated the generation of 3D point clouds and their latent representation with GANs. Even though their work performs a certain level of shape completion, their architecture is not designed for shape completion tasks and suffers from considerable degradation as the number of missing points at the input are increased. Gurumurthy et al. [15] have suggested shape completion architecture which utilizes latent GAN and auto-encoder. However, they use a time-consuming optimization step for each batch of input to select the best seed for the GAN. While we also use latent GAN, our approach is different because we use a trained agent to find the GAN's input seed. In doing so, we complete shapes in a matter of milliseconds.\n\nGAN and RL. Recently, Goodfellow et al. [13] suggested generative adversarial networks (GANs) which use a neural network (a discriminator) to train another neural network (a generator). The generator tries to fool the discriminator by synthesizing fake examples that resembles real data, whereas the discriminator tries to discriminate between the real and the fake data. The two networks compete with each other and eventually the generator learns the distribution of the real data.\n\nWhile GAN suggests a way to overcome the limitation of data-driven methods, at the same time, it is very hard to train and is susceptible to a local optimum. Many improvements have been suggested which range from changes in the architecture of generator and discriminator to modifications in the loss function and adoption of good training practices [2,42,43,14]. There are also practices to control GAN by observing the condition as an additional input [27] or using back-propagation to minimize the loss between the desired output and the generated output [41,15]. Our pipeline utilizes deep reinforcement learning (RL) to control the complex latent space of GAN. RL is a framework where a decision-making network, also called an agent, interacts with the environment by taking available actions and collects rewards. RL agents in discrete action spaces have been used to provide useful guides to computer vision problems such as to propose bounding box locations [3,4] or seed points for segmentation [37] with deep Q-network (DQN) [28]. On the other hand, we train an actor-critic based network [24] learning the policy in continuous action space to control GAN for shape completion. In our setup, the environment is the shape completion framework composed of various blocks such as AE and GAN, and the action is the input to the generator. The unknown behavior of the complex network can be controlled with the deep RL agent and we can generate completed shapes from highly occluded point cloud data.\n\n\nMethods\n\nOur shape completion pipeline is composed of three fundamental building blocks, which are namely an autoencoder (AE), a latent-space generative adversarial network (l-GAN) and a reinforcement learning (RL) agent. Each of the components is a deep neural network that has to be trained separately. We first train an AE and use the encoded data to train l-GAN. The RL agent is trained in combination with a pre-trained AE and GAN.\n\nThe forward pass of our method can be seen in Fig. 2. The encoder of the trained AE encodes the noisy and incomplete point cloud to a noisy global feature vector (GFV). Given this noisy GFV, our trained RL agent selects the correct seed for the l-GAN's generator. The generator produces the clean GFV which is finally passed through the decoder of AE to get the completed point cloud representation of the clean GFV. A discriminator observes the GFV of the generated shape and the one processed by AE and selects the more plausible shape. In the following subsections, we explain the three fundamental building blocks of our approach, then describe the combined architecture.\n\n\nAutoencoder (AE)\n\nAn AE creates a low-dimensional encoding of input data by training a network that reproduces the input. An AE is composed of an encoder and a decoder. The encoder con-verts a complex input into an encoded representation, and the decoder reverts the encoded version back to the original dimension. We refer to the efficient intermediate representation as the GFV, which is obtained upon training an AE. The training of AE is performed with back-propagation reducing the distance between input and output point cloud, either with the Earth Movers distance (EMD) [36] or the Chamfer distance [10,1]. We use the Chamfer distance over EMD due to its efficiency which can be defined as follows:\nd CH (P 1 , P 2 ) = a\u2208P1 min b\u2208P2 a \u2212 b 2 2 + b\u2208P2 min a\u2208P1 a \u2212 b 2 2 ,\n(1) where in Eq. (1) the P 1 and P 2 are the input and output point cloud respectively.\n\nWe first train a network similar to the one reported by Achlioptas et al. [1] on the ShapeNet point cloud dataset [40,6]. Achlioptas et al. [1] also demonstrated that a trained AE can be used for shape completion. The trained decoder maps GFV into a complete point cloud even when the input GFV has been produced from an incomplete point cloud. But the performance degrades drastically as the percentage of the missing data in the input is increased (Fig. 14).\n\n\nl-GAN\n\nGAN generates new yet realistic data by jointly training a pair of generator and discriminator [13]. While GAN demonstrated its success in image generation tasks [42,14,2], in practice, training a GAN tends to be unstable and suffer from mode collapse [26]. Achlioptas et al. [1] showed that training a GAN on GFV, or latent representation, leads to more stable training results compared to training on raw point clouds. Similarly, we also train a GAN on GFV, which has been converted from complete point cloud data using the encoder of trained AE, Sec. 3.1. The generator synthesizes a new GFV from a noise seed z, which can then be converted into a complete 3D point cloud using the decoder of AE. We refer to the network as l-GAN or latent-GAN.\n\nGurumurthy et al. [15] similarly utilized l-GAN for point cloud shape completion. They formulated an optimization framework to find the best input z to the generator to create GFV that best explains the incomplete point cloud at the input. However, as the mapping between the raw points and the GFV is highly non-linear, the optimization could not be written as a simple back-propagation. Rather, the energy term is a combination of three loss terms. We list the losses below, where P in is the incomplete point cloud input, E and E \u22121 are the encoder and the decoder of AE, and G and D represent the generator and the discriminator of the l-GAN respectively.\n\n\u2022 Chamfer loss: the Chamfer distance between the input partial pointcloud P in and the generated, decoded\npointcloud E \u22121 (G(z)) L CH = d CH (P in , E \u22121 (G(z)))(2)\n\u2022 GFV loss: l 2 distance between the generated GFV G(z) and the GFV of the input pointcloud E(P in )\nL GF V = G(z) \u2212 E(P in ) 2 2\n(3)\n\n\u2022 Discriminator loss: the output of the discriminator\nL D = \u2212D(G(z))(4)\nGurumurthy et al. [15] optimized the energy function defined as a weighted sum of the losses, and the weights gradually evolve with every iteration. However, we propose a more robust control of GAN using an RL framework, where an RL agent quickly finds the z-input to the GAN by observing the combination of losses.\n\n\nReinforcement Learning (RL)\n\nIn a typical RL-based framework, an agent acts in an environment . Given an observation x t at each time step t, the agent performs an action a t and receives a reward r t . The agent network learns a policy \u03c0 which maps states to the action with some probability. The environment can be modeled as a Markov decision process, i.e., the current state and action only depend on the previous state and action. The reward at any given state is the discounted future reward\nR t = T i=t \u03b3 (i\u2212t) r(s i , a i ).\nThe final objective is to find a policy which provides the maximum reward.\n\nWe formulate the shape completion task in an RL framework as shown in Fig. 3. For our problem, the environment is the combination of AE and l-GAN, and resulting losses that are calculated as intermediate results of various networks in addition to the discrepancy between the input and the predicted shape. The observed state s t is the initial noisy GFV encoded from the incomplete input point cloud. We assume that the environment is Markov and fully observed; i.e., the recent most observation x t is enough to define the state s t . The agent takes an action a t to pick the correct seed for the z-space input of the generator. The synthesized GFV is then passed through the decoder to obtain the completed point cloud shape.\n\nOne of the major tasks in training an RL agent is the correct formulation of the reward function. Depending on the quality of the action, the environment gives a reward r back to the agent. In RL-GAN-Net, the right decision equates to the correct seed selection for the generator. We use the combination of negated loss functions as a reward for shape completion task [15] (Sec. 3.2) that represent losses in all of Cartesian coordinate (r CH = \u2212L CH ), latent space (r GF V = \u2212L GF V ), and in the view of the discriminator (r D = \u2212L D ). The final reward term is given as follows: where w CH , w GF V , and w D are the corresponding weights assigned to each loss function. We explain the selection of weights in the supplementary material.\nr = w CH \u00b7 r CH + w GF V \u00b7 r GF V + w D \u00b7 r D ,(5)\nSince the action space is continuous, we adopt deep deterministic policy gradient (DDPG) by Lillicrap et al. [24]. In DDPG algorithm, a parameterized actor network \u00b5(s | \u03b8 \u00b5 ) learns a particular policy and maps states to particular actions in a deterministic manner. The critic network Q(s, a) uses the Bellman equation and provides a measure of the quality of action and the state. The actor network is trained by finding the expected return of the gradient to the cost J w.r.t the actor-network parameters, which is also known as the policy gradient. It can be defined as below:\n\u03b8 \u00b5 J(\u03b8) = E st\u223c\u03c1 \u03b2 [ \u03b1 Q(s, a | \u03b8 Q ) | s=st,a=\u00b5(st) \u03b8\u00b5 \u00b5(s | \u03b8 \u00b5 ) | s=st ](6)\nBefore training the agent, we make sure that AE and GAN are adequately pre-trained as they constitute the environment. The agent relies on them to select the correct action. The algorithm of the detailed training process is summarized in Algorithm 1.\n\n\nHybrid RL-GAN-Net\n\nWith the vanilla implementation described above, the generated details of completed point cloud can sometimes have limited semantic variations. When the portion of missing data is relatively small, the AE can often complete the shape that agrees better with the input point cloud. On the other hand, the performance of AE degrades significantly Algorithm 1 Training RL-GAN-Net Agent Input:\n\nState (s t ): s t = GF V n = E(P in ); Sample pointcloud P in from dataset into the pre-trained encoder E to generate noisy latent representation GF V n . Reward (r t ): Calculated using Eq. (5) Agent Output:\n\nAction (a t ): a t = z Pass z-vector to the pre-trained generator G to form clean latent vector GF V c =G(z) Final Output:\nP out = E \u22121 (GF V c )\n; Pass GF V c into decoder E \u22121 to generate output point cloud P out .\n\n1: Initialize procedure Env with pre-trained generator G, discriminator D, encoder E and decoder E \u22121 2: Initialize policy \u03c0 with DDPG, actor A, critic C, and replay buffer R 3: for t steps < maxsteps do 4:\n\nGet P in\n\n\n5:\n\nif t steps > 0 then 6: Train A and C with R 7:\n\nif t LastEvaluation > f EvalF requency then 8:\n\nEvaluate \u03c0 9:\n\nGF V n \u2190 E(P in ) 10: if t steps > t StartT ime then 11: Random Action a t\n\n\n12:\n\nif t steps < t StartT ime then 13: Use a t \u2190 A \u2190 GF V n 14:\n\n(s t , a t , r t , s t+1 ) \u2190Env \u2190a t\n\n\n15:\n\nStore transition (s t , a t , r t , s t+1 ) in R endfor 16: procedure ENV(P in ,a t ) 17: Get State (s t ) : GF V n \u2190 E(P in ) 18: Implement Action : GF V c \u2190 G (a t = z) 19: Calculate reward r t using Eq. (5) 20: Obtain point cloud :\nP out \u2190 E \u22121 (GF V c )\nas more data is missing, and our RL-agent can nonetheless find the correct semantic shape. Based on this observation, we suggest a hybrid approach by using a discriminator as a switch that selects the best results out of the vanilla RL-GAN-Net and AE. The final pipeline we used for the result is shown in Fig. 2. Our hybrid approach can robustly complete the semantic shape in real time and at the same time preserve local details.\n\n\nExperiments\n\nWe used PyTorch [32] and open source codes [12,18,31,11] for our implementation. All networks were trained on a single Nvidia GTX Titan Xp graphics card. The details For the experiments, we used the four categories with the most number of shapes among ShapeNetCore [6,1] dataset, namely cars, airplanes, chairs, and desks. The total number of shapes sums to 26,829 for the four classes. All shapes are translated to be centered at the origin and scaled such that the diagonals of bounding boxes are of unit length. The ground-truth point cloud data is generated by uniformly sampling 2048 points on each shape. The points are used to train the AE and generate clean GFV to train the l-GAN. The incomplete point cloud is generated by selecting a random seed from the complete point cloud and removing points within a certain radius. The radius is controlled for each shape to obtain the desired amount of missing data. We generated incomplete point cloud missing 20, 30, 40, 50 and 70% of the original data for test, and trained our RL agent on the complete dataset.\n\n\nShape Completion Results\n\nWe present the results using the two variations of our algorithm, the vanilla version and the hybrid approach as mentioned in Sec. 3 against the method using AE only [1]. Fig. 5a shows the Chamfer distances of the completed shape compared against the ground-truth point cloud. With point cloud input with 70% of its original points missing, the Chamfer distance compared to ground truth increase up to 16% of the diagonal of the shape, but the reconstructed shapes of AE, vanilla and hybrid RL-GAN-Net all show less than 9% of the distances.\n\nWhile the Chamfer distance is a widely used metric to compare shapes, we noticed that it might not be the absolute measure of the performance. From Fig. 5a, we noticed that the input point cloud P in was the best in terms of Chamfer distance for 20% missing data. However, from our visual inspection in Fig. 10, the completed shapes, while they might not be exactly aligned in every detail, are semantically reasonable and does not exhibit any large holes that are clearly visible in the input. For the examples with data missing 70% of its original points in Fig. 14, it is obvious that our approach is superior to AE, whose visual quality for completed shape severely degrades as the ratio of missing data increases. However, the Chamfer distance is almost the same for AE and RL-GAN-Net. The observation can be interpreted as the fact that 1) AE is specifically trained to reduce the Chamfer loss, thus performs better in terms of the particular loss, while RL-GAN-Net jointly considers Chamfer loss, latent space and discriminator losses, and 2) P in has points that are exactly aligned with the GT, which, when averaged, compensates errors from missing regions.\n\nNonetheless our hybrid approach correctly predicts the category of shapes and fills the missing points even with a large amount of missing data. In addition, the RLcontrolled forward pass takes only around a millisecond to complete, which is a huge advantage over previous work [15] that requires back-propagation over a complex network. They claim the running time of 324 seconds for a batch of 50 shapes. On the other hand, our approach is real-time and easily used as a preprocessing step for various tasks, even at the scanning stage.\n\n\nComparison with Dai et al. [7]\n\nWhile there is not much prior work in point cloud space, we include the completion results of Dai et al [7] which works in a different domain (voxel grid). To briefly describe, their approach used an encoder-decoder network in 32 3 voxel space followed by an analytic patch-based completion in 128 3 resolution. Their results of both resolutions are available as distance function format. We converted the distance function into a surface (a) Chamfer distance to GT (b) Classification accuracy [34] (c) Loss terms Figure 5: Performance analysis. We compare the two versions of our algorithms against the original input and the AE in terms of (a) the Chamfer distance (the lower the better) and (b) the performance gain for shape classification (the higher the better). (c) We also analyze the losses of RL-GAN-Net with different amount of missing data.\n\nrepresentation using the MATLAB function isosurface as they described, and uniformly sampled 2048 points to compare with our results. We present the qualitative visual comparison in Fig. 17. The results of encoder-decoder based network (referred as Voxel 32 3 in the figure) are smoother than point clouds processed by AE as the volume accumulation compensates for random noise. However, the approach is limited in resolution and washes out the local details. Even after the patch-based synthesis in 128 3 resolution, the details they could recover are limited. On the other hand, our approach robustly preserves semantic symmetries and completes local details in challenging scenarios. It should be noted that we used only scanned point data but did not incorporate the additional mask information, which they utilized. More results are included in the supplementary material due to the limitation of space.\n\n\nApplication into Classification\n\nAs an alternative measure to test the performances of the semantic shape completion, we compared the classification accuracy of P in and the shapes completed by AE and RL-GAN-Net. This scenario also agrees with the main applications that we intended. That is, RL-GAN-Net can be used as a quick preprocessing of the captured real data before performing other tasks as the raw output of 3D measurements are often partial, noisy data to be used as a direct input to point cloud processing framework. We took the incomplete input and first processed through our shape completion pipeline. Then we analyzed the classification accuracy of PointNet [34] with the completed point cloud input and compared against the results with incomplete input. Fig. 5b shows the improvement of classification accuracy. Clearly, our suggested pipeline reduces possible performance losses of existing networks by completing the defects in the input.\n\nWe also would like to add a note about the performance of the vanilla RL-GAN-Net and the hybrid approach. We noticed that the main achievement of our RL agent is often limited to finding the correct semantic categories in the latent space. The hybrid approach overcomes the limitation It should be noted that they additionally have mask information whereas we operate directly on the scanned points only. by selecting the results of AE when the shape is more reasonable according to the trained discriminator. This agrees with the fact that the hybrid approach is clearly better in terms of Chamfer distance in Fig. 5a, but is comparable with the vanilla approach in classification in Fig. 5b, where the task is finding the correct category. Fig. 7 shows some examples of failure cases, where the suggested internal category does not exactly align with the observed shape.\n\n\nReward Function Analysis\n\nWe demonstrate the effects of the three different loss terms we used. Fig. 5c shows the change of loss values of generated pointcloud with different amount of missing data. Both Chamfer loss L CH and GFV loss L GF V increase for a large amount of missing data. This is reasonable considering that we need to fill more information and make a larger change from the incomplete input as the ratio of missing data grows larger. The L D is almost constant as the pretrained generator synthesizes according to the learned distribution given the z input. We also tested different combinations of loss functions for reward. Fig. 8 shows the sample results with a shape per category. While the Chamfer distance is the widely used metric to compare two shapes, the Chamfer loss was not very efficient when used alone. This can be interpreted as the curse of dimensionality. While we need to semantically complete the 3D positions of 2048 points, the single number of Chamfer loss is not enough to inform the agent to find the correct control of the l-GAN. On the other hand, the performance of GFV loss is impressive. While details are often mismatched, the GFV loss alone enables the controller to find the correct semantic shape category from partial data. This result agrees with the discussion in [1], where the latent space representation reduces the dimension and boost the performance of GAN. However, the completed shape aligned better with the desired shape when combined with the Chamfer loss, which only shows its power when combined with GFV. The discriminator loss is essential to create a realistic shape. When discriminator loss is used alone, the RL agent creates a reasonable but unrelated shape, which is an expected behaviour considering the reward is simply encouraging a realistic shape. From the results, we conclude that all of the three loss terms are necessary for the RL agent to deduce the correct control of GAN.\n\n\nConclusion and Future Work\n\nIn this work, we presented a robust and real time point cloud shape completion framework using the RL agent to control the generator. Our primary motivation was to remove the costly and complex optimization process that is not real time and takes a minimum of 324 seconds to process a batch of inputs [15]. Instead of optimizing the various combinations of loss functions, we have converted these loss functions into rewards. In doing so, our RL agent We tested the reward function with different combinations of losses. According to our analysis, the Chamfer loss cannot work alone to complete the shape but with the GFV loss our RL-GAN-Net can find the correct shape.\n\nThe discriminator loss ensures that the completed shape is semantically meaningful.\n\ncan complete the shape in approximately one millisecond. In addition, we present shape completion results with data with up to 70% missing points. We show the superiority of our technique by demonstrating qualitative results.\n\nWe have also presented a use case of our network for the classification problem. Being real time, RL-GAN-Net can be used to improve the performance of other point cloud processing networks. We demonstrated that our trained network raises the classification accuracy of PointNet from 50% to 83% for the data with 70% missing points. With this work, we have demonstrated a hidden potential in RLbased techniques to effectively control the complex space of GAN. An immediate extension is to apply the approach into closely related tasks such as image in-painting [41].\n\n\nImplementation Details\n\nThe suggested RL-GAN-Net architecture combines three fundamental building blocks as shown in Sec. 3 of the main article. The general form of the architecture is provided in Fig. 9. In this section, we provide details of our implementation for each network.\n\n\nAE Details\n\nThe AE is composed of an encoder that converts input points P in into GFV, and a decoder network that reverts GFV back to the point cloud domain, as shown in Fig. 9a. The input and output points are an unstructured list of 2048 3D coordinates that is sampled from the underlying 3D structure. The encoder network consists of five 1 D convolution layers with 64, 128, 128, 256, 128 channels respectively, while the decoder consists of FC layers 256, 256 and 6144 channels respectively. Each layer is followed by ReLu. The bottleneck size for AE is 128. We trained the AE to reduce the Chamfer distance (Eq.(1) of the main article) between the input and output point cloud. The Chamfer distance calculation is imported from the implementation 1 of Li et al [9].\n\n\nl-GAN Details\n\nl-GAN is composed of the encoder block of AE and a generator and a discriminator, as shown in Fig. 9b. For the generator and the discriminator pair, we adapted the main architecture of the GAN from Zhang et al. [42] and applied in the latent space acquired by the AE. The detailed network architecture for our modified l-GAN pipeline has been shown in Table 2 and 3 respectively. We trained the GAN using WGAN-GP [14] adversarial loss with \u03bb gp = 10. The total number of iterations was one million. As a typical GAN training, we updated discriminator 5 times for every update of the generator. We used Adam optimizer [20] with \u03b2 1 =0.5 and \u03b2 2 =0.9. The learning rate for both generator and discriminator was set to 0.0001. Batch size was set to 50 and number of workers were set to 2. We did not use any learning rate decay.\n\nWe selected the dimension of z-vector to be 1. We did this to limit the dimensions of action space for the agent. All the experiments conducted were with a single dimension. We also tested with 6 and 32 dimensions but there was no change in the performance of the GAN or the agent in either case. Therefore we kept the dimension of the z-vector to 1.\n\nWe trained the GAN using the dataset generated by passing the ShapeNet point cloud dataset through the encoder of the AE. Therefore, the output dimension of our generator is the same as the bottleneck size of our AE (128).\n\nFor l-GAN training, we adopted the self-attention GAN \n\n\nRL Agent Details\n\nThe third element of the basic architecture is RL. The basic RL framework is composed of an agent and the environment as in Fig. 9c. Among many possible variations of the RL agent, we used the actor-critic architecture to enable continuous control of the l-GAN.\n\nActor and Critic Architecture The actor and critic networks are chosen to be fully connected (FC) layers. The actor has four FC layers with 400, 400, 300, 300 neurons with ReLu activation for the first three layers and tanh for the last layer respectively. The input to the actor is a 128-dimensional GFV. The output is a single dimension zvector. The critic also has four FC layers with 400, 432, 300, 300 neurons with ReLu activation for the first two layers respectively.\n\nReward Function Hyper-parameter In Eq.(5) of the main article, the multiplicative weights of w CH , w GF V , and w D are assigned to the corresponding loss functions. The weight values are chosen such that, when combined, the effects of individual terms are not out of proportion or dominant in any way. In other words, the total loss is within range for the RL agent to learn useful information for all of the terms of L CH , L GF V , and L D . For example, if the value for the Chamfer loss was approximately 1000 and the GFV loss was 10, then they are normalized by dividing by 100 and 1 respectively. After consulting the range of raw loss values of multiple trials, we set w CH = 100, w GF V = 10.0, and w D = 0.01 for all our experiments.\n\nThe RL agent was adopted from the open source implementation of the DDPG algorithm. 3 .\n\nTraining Details The training of the agent can be divided into two parts. The first part is the collection of experience. The second part is the training of the actor and critic network in accordance with the DDPG algorithm as outlined in the previous work [24].\n\nFor the first part, we refer the readers back to Fig. 3 of the main article. It shows the mechanism by which the replay buffer R is filled continuously with useful experiences. We fill the memory with one input at a time. This implies that the batch size for this case is one. Our task is episodic, which means that after each episode we collect a reward. The number of episodes is equal to the maximum number of allowed iterations. In each episode, the agent is allowed to take a single action after which the episode terminates.  -\n- - 50x1x7x7 50x1x7x7 convtr2d-layer3 - - convtr2d-last 2x2 2 1 50x64x7x7 50x1x12x12 Self-Atten - - reshape1 1x1 - - 50x1x12x12 50x144 convtr2d-last - - convtrans1d 1x1 - - 50x144 50x128\nconvtr2d-last -- Table 2: The network architecture of the generator. convtr2d = 2D transposed convolutional layer, convtrans1d = 1D transposed convolution, SN = spectral normalization [42] and BN = batch normalization\n\nThe sequences of state, action and reward tuples are then stored in the replay buffer.\n\nThe second part, i.e., training the actor and critic in accordance with DDPG, is performed by keeping the batch size equal to one hundred. This means that a batch of a 100 memories from the replay buffer is picked randomly to train the actor and critic networks according to the DDPG algorithm. The evaluation of the policy was carried out after 5000 iterations. The number of dimensions of state is 128, which is basically the noisy GFV obtained by encoding the incomplete point cloud. The action dimension is determined by the dimension of the GAN's z-space, which is 1. The action space is kept to unity to achieve better performance by the agent. We also tested with 32 dimensions for z space but it did not have any noticeable effect on the performance of GAN or the agent.\n\nWe list the parameter values used for the training with DDPG algorithm in Table 4.\n\n\nAdditional Results\n\nIn this section, we provide enlarged images of the experiments in Sec. 4 of the original document and include some additional results that were omitted due to the page limit.\n\n\nShape Completion Results\n\nThe examples of shape completion results for point cloud missing 20% and 70% of its original points are enlarged in Fig. 10 and Fig. 14. In addition, we provide the examples of results for remaining data sets we used, which are missing 30%, 40% and 50% of the original points as shown in Fig. 11, 12 and 13 respectively. It is clear that the performance of our pipeline is prominent as the percentage of missing portion increases.\n\n\nRobustness Results\n\nThe robustness test results with the different dataset provided by Dai et al [7] are included in Fig. 15 and Fig. 16. Our result is almost not affected by the jitter, and the completed shape is semantically similar to its original shape.\n\nFor the cases where there is no jitter, we also include the completion results of Dai et al [7]. Their approach works in a different domain (voxel grid) but we are including a comparison as there is not much prior work in point cloud space. To briefly describe, their approach used an encoder-decoder network in 32 3 voxel space followed by an analytic patchbased completion in 128 3 resolution. Their results of both resolutions are available as distance function format. We converted the distance function into a surface representation using the MATLAB function isosurface as they described, and uniformly sampled 2048 points to compare with our re-Name Kernel Stride Padding  InpRes  OutRes  Input  Activation Norm  convtrans1d  1x1  --50x128  50x144  input  --reshape1  ---50x144  50x12x12  convtrans1d  --conv2d-layer1  3x3  2  2  50x12x12  50x64x7x7  reshape1  ReLu  SN,BN  conv2d-layer2  3x3  2  2  50x64x7x7 50x128x5x5 conv2d-layer1  ReLu  SN,BN  conv2d-layer3  3x3  2  2  50x128x5x5 50x256x4x4 conv2d-layer2 ReLu SN,BN Self-Atten [42] -\n- - 50x256x4x4 50x256x4x4 conv2d-layer3 - - conv2d-last 4x4 - - 50x256x4x4 50x1\nSelf-Attention --  sults. By comparing against the ground truth model, ours is superior to their approach in terms of the Chamfer distance as shown in Table 5. It should be noted here that the Chamfer distance between the input and ground truth is comparable to autoencoder. This is expected because the dataset provided by Dai et al. [7] does not have any drastic level of incompletion in many cases. We also refer the reader back to the Fig.5a of the main article where clearly the Chamfer distance of the input point cloud compared to the ground truth was even lower than an AE for missing data percentages less than forty. We present the qualitative visual comparison in Fig. 17. The results of encoder-decoder based network (refered as Voxel 32 3 in the figure) are smoother than point clouds processed by AE as the volume accumulation compensates for random noise. However, the approach is limited in resolution and washes out the local details. Even after the patchbased synthesis in 128 3 resolution, the details they could recover are limited. On the other hand, our approach robustly preserves semantic symmetries and completes local details in challenging scenarios. It should be noted that we used only scanned point data but did not incorporate the additional mask information, which they utilized.\n\n\nClassifier Details\n\nWe have trained the PointNet [34] classifier to distinguish the four categories that our RL-GAN-Net was trained on. After training, it classifies the shapes with 99.36% of Input V 32 3 V 128 3 AE RL-GAN-Net 0.0688 0.169 0.162 0.0531 0.0690 Table 5: The Chamfer distance compared to the ground truth. There are two volumetric approaches compared against two point cloud based approach. V 32 3 is the results using encoder-decoder based network in voxel space at the resolution of 32 per dimension, and V 128 3 shows the distance after the full pipeline including patch-based synthesis in [7]. AE and RL-GAN-Net are the point cloud based approaches of [1] and ours.\n\naccuracy on the full data set with a complete point cloud.\n\nAt test time, we consider the three scenarios as shown in Fig. 18, namely using the raw partial input, and using the shapes processed and completed by AE and RL-GAN-Net. The three pipelines are tested with the incomplete point cloud dataset for classification accuracy. For the cases that more than 30% of the original shape data is missing, the classification accuracy is boosted when the shapes are preprocessed with shape completion pipeline. And our suggested pipeline is superior to AE and robust to large missing regions. The detailed classification results are shown in Table 6. RL-GAN-Net and GT Figure 16: Robustness test. We applied our algorithm to the point cloud data provided by [7]. This figure shows results when we added zero-mean Gaussian noise with standard deviation 0.01 (clipped at 0.05).  Table 6: Classification accuracy of point cloud input processed by RL-GAN-Net compared to vanilla and AE for various percentage of missing data points\n\nFigure 1 :\n1Qualitative results of point cloud shape completion\n\nFigure 3 :\n3Training RL-GAN-Net for shape completion. Our RL framework utilizes AE (shown in green) and l-GAN (shown in blue). The RL agent and the environment are shaded in gray, and the embedded reward, states, and action spaces are highlighted in red. The output is decoded and completed as shown at the bottom. Note that the decoder and decoded point cloud in the upper right corner is added for a comparison, and does not affect the training. By employing an RL agent, our pipeline is capable of real-time shape completion.\n\nFigure 4 :\n4Qualitative results of point cloud shape completion missing 20% of its original points. With relatively small missing data, AE sometimes performs better in completing shapes. Therefore, our hybrid RL-GAN-Net reliably selects the best output shape among the AE and the vanilla RL-GAN-Net. of network architectures are provided in the supplementary materials.\n\nFigure 6 :\n6Performance Comparison. Comparison of RLGAN-Net vs Dai et al.[7] for their 32 3 and 128 3 resolution results. We converted their distance function output to point cloud domain.\n\nFigure 7 :\n7Failure cases. RL-GAN-Net can sometimes predict a wrong category (top) or semantically similar but different shape of the category (bottom).\n\nFigure 8 :\n8Reward function analysis.\n\nFigure 9 :\n9Network architecture of the three fundamental building blocks of RL-GAN-Net.\n\nFigure 15 :\n15Robustness test. We applied our algorithm to the point cloud data provided by[7]. This figure shows examples of shape completion results with the raw scan data provided.\n\nFigure 17 :Figure 18 :\n1718Performance Comparison. Comparison of RLGAN-Net vs Dai et al.[7] for their 32 3 and 128 3 resolution results. We converted their distance function output to point cloud domain. It should be noted that they additionally have mask information whereas we operate directly on the scanned points only. The variations of network architecture for point cloud classification with missing GAN-Net (vanilla) + PointNet (Fig. 18c) 97.7 96.7 95.0 92.7 82.5 RL-GAN-Net (hybrid) + PointNet (Fig. 18c) 98.1 97.2 95.5 93.3 83.8\n\nTable 1 :\n1. Since the area is relatively new, there are not many previous works available performing shape completion in point cloud space. We compare our result The average action time for the RL agent to produce clean GFV from observation of noisy GFV. Our approach can create the appropriate z-vector approximately in one millisecond.ratio (%) \n20 \n40 \n30 \n50 \n70 \ntime (ms) 1.310 1.293 1.295 1.266 1.032 \n\n\n\nTable 3 :\n3The network architecture of the discriminator. conv2d = 2D convolutional layer, convtrans1d = 1D transposed convolution, SN = spectral normalization[42] and BN = batch normalizationParameter \nValue \nmaximum number of iterations \n1e6 \nexploration noise \n0.1 \nbatch size from R for the actor training \n100 \ndiscount \u03b3 \n0.9 \nspeed of target value updates \u03c4 \n0.005 \nnoise added to policy during critic update \n0.2 \nrange to clip noise policy \n0.5 \nfrequency for delayed policy update \n2 \n\n\n\nTable 4 :\n4The parameter values used to train the RL agent.\nhttps://github.com/heykeetae/Self-Attention-GAN 3 https://github.com/sfujim/TD3\nAcknowledgements. This work was supported by KIST institutional program [Project No. 2E29450] and the KAIST School of Electrical Engineering Graduate Fellowship. We are grateful to In So Kweon, Sunghoon Im, Arda Senocak from RCV lab, KAIST for insightful discussions.\nRepresentation learning and adversarial generation of 3d point clouds. CoRR, abs/1707.02392. Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas J Guibas, 20Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas J. Guibas. Representation learning and adversarial generation of 3d point clouds. CoRR, abs/1707.02392, 2017. 1, 2, 3, 4, 6, 8, 11, 20\n\nWasserstein generative adversarial networks. Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou, PMLR. 3Proceedings of the 34th International Conference on Machine Learning. Doina Precup and Yee Whye Tehthe 34th International Conference on Machine LearningSydney, Australia704International Convention CentreMartin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Doina Pre- cup and Yee Whye Teh, editors, Proceedings of the 34th In- ternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 214-223, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. 3, 4\n\nHierarchical object detection with deep reinforcement learning. Miriam Bellver, Xavier Giro-I Nieto, Ferran Marques, Jordi Torres, Deep Reinforcement Learning Workshop. Miriam Bellver, Xavier Giro-i Nieto, Ferran Marques, and Jordi Torres. Hierarchical object detection with deep rein- forcement learning. In Deep Reinforcement Learning Work- shop, NIPS, December 2016. 3\n\nAn analysis of deep neural network models for practical applications. Alfredo Canziani, Adam Paszke, Eugenio Culurciello, abs/1605.07678CoRR1Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical ap- plications. CoRR, abs/1605.07678, 2016. 1, 2\n\n. Angel X Chang, Thomas A Funkhouser, Leonidas J Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu, Shapenet: An information-richAngel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich\n\nQualitative results of point cloud shape completion missing 30% of its original points. 3d model repository. CoRR. abs/1512.03012Figure. 116Figure 11: Qualitative results of point cloud shape completion missing 30% of its original points. 3d model repository. CoRR, abs/1512.03012, 2015. 4, 6\n\nShape completion using 3d-encoder-predictor cnns and shape synthesis. CoRR. Angela Dai, Charles Ruizhongtai Qi, Matthias Nie\u00dfner, abs/1612.001011819Angela Dai, Charles Ruizhongtai Qi, and Matthias Nie\u00dfner. Shape completion using 3d-encoder-predictor cnns and shape synthesis. CoRR, abs/1612.00101, 2016. 1, 3, 6, 7, 10, 11, 17, 18, 19\n\nScancomplete: Largescale scene completion and semantic segmentation for 3d scans. Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J\u00fcrgen Sturm, Matthias Nie\u00dfner, abs/1712.10215CoRR13Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J\u00fcrgen Sturm, and Matthias Nie\u00dfner. Scancomplete: Large- scale scene completion and semantic segmentation for 3d scans. CoRR, abs/1712.10215, 2017. 1, 3\n\nA point set generation network for 3d object reconstruction from a single image. Haoqiang Fan, Hao Su, Leonidas J Guibas, abs/1612.00603CoRR29Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point set generation network for 3d object reconstruction from a single image. CoRR, abs/1612.00603, 2016. 2, 3, 9\n\nA point set generation network for 3d object reconstruction from a single image. Haoqiang Fan, Hao Su, Leonidas J Guibas, abs/1612.00603CoRR24Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point set generation network for 3d object reconstruction from a single image. CoRR, abs/1612.00603, 2016. 2, 4\n\nAddressing function approximation error in P in AE RL-GAN-Net Ground Truth (GT). Scott Fujimoto, Scott Fujimoto. Addressing function approximation error in P in AE RL-GAN-Net Ground Truth (GT)\n\nQualitative results of point cloud shape completion missing 40% of its original points. actor-critic methods. G T Rl-Gan-Net, Figure, 12RL-GAN-Net and GT Figure 12: Qualitative results of point cloud shape completion missing 40% of its original points. actor-critic methods. https://github.com/sfujim/ TD3, 2018. 5\n\nAddressing function approximation error in actor-critic methods. Scott Fujimoto, Dave Herke Van Hoof, Meger, abs/1802.09477CoRRScott Fujimoto, Herke van Hoof, and Dave Meger. Ad- dressing function approximation error in actor-critic meth- ods. CoRR, abs/1802.09477, 2018. 5\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. WeinbergerCurran Associates, Inc274Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahra- mani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Pro- cessing Systems 27, pages 2672-2680. Curran Associates, Inc., 2014. 3, 4\n\n. Ishaan Gulrajani, Faruk Ahmed, Mart\u00edn Arjovsky, Vincent Dumoulin, Aaron C Courville, 9Improved training of wasserstein gans. CoRR, abs/1704.00028, 2017. 3, 4Ishaan Gulrajani, Faruk Ahmed, Mart\u00edn Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. CoRR, abs/1704.00028, 2017. 3, 4, 9\n\nSwaminathan Gurumurthy, Shubham Agrawal, High fi. Swaminathan Gurumurthy and Shubham Agrawal. High fi-\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, abs/1512.03385CoRRKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. 2\n\nRecurrent slice networks for 3d segmentation on point clouds. Qiangui Huang, Weiyue Wang, Ulrich Neumann, Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Re- current slice networks for 3d segmentation on point clouds.\n\n. Corr, abs/1802.04402CoRR, abs/1802.04402, 2018. 3\n\nSo-net: Self-organizing network for point cloud analysis. Li Jiaxin, Li Jiaxin. So-net: Self-organizing network for point cloud analysis, cvpr2018. https://github.com/lijx10/ SO-Net, 2018. 5\n\nGuided real-time scanning of indoor objects. Niloy J Young Min Kim, Qixing Mitra, Leonidas Huang, Guibas, Computer Graphics Forum (Proc. Pacific Graphics), xx:xx. Young Min Kim, Niloy J. Mitra, Qixing Huang, and Leonidas Guibas. Guided real-time scanning of indoor ob- jects. Computer Graphics Forum (Proc. Pacific Graphics), xx:xx, 2013. 2\n\nQualitative results of point cloud shape completion given input data missing 70% of its original points. G T Rl-Gan-Net, Figure, 14RL-GAN-Net and GT Figure 14: Qualitative results of point cloud shape completion given input data missing 70% of its original points.\n\nAdam: A method for stochastic optimization. CoRR, abs/1412. P Diederik, Jimmy Kingma, Ba, 6980Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. 9\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Proceedings of the 25th International Conference on Neural Information Processing Systems. the 25th International Conference on Neural Information Processing SystemsUSACurran Associates Inc1NIPS'12Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In Proceedings of the 25th International Confer- ence on Neural Information Processing Systems -Volume 1, NIPS'12, pages 1097-1105, USA, 2012. Curran Associates Inc. 1, 2\n\nHandwritten digit recognition with a back-propagation network. Yann Lecun, Bernhard E Boser, John S Denker, Donnie Henderson, R E Howard, Wayne E Hubbard, Lawrence D Jackel, Advances in Neural Information Processing Systems. D. S. TouretzkyMorgan-Kaufmann2Yann LeCun, Bernhard E. Boser, John S. Denker, Don- nie Henderson, R. E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. Handwritten digit recognition with a back-propagation network. In D. S. Touretzky, editor, Ad- vances in Neural Information Processing Systems 2, pages 396-404. Morgan-Kaufmann, 1990. 1, 2\n\nDatabase-assisted object retrieval for real-time 3d reconstruction. Yangyan Li, Angela Dai, Leonidas Guibas, Matthias Nie\u00dfner, Computer Graphics Forum. 34Yangyan Li, Angela Dai, Leonidas Guibas, and Matthias Nie\u00dfner. Database-assisted object retrieval for real-time 3d reconstruction. In Computer Graphics Forum, volume 34.\n\n. Wiley Online Library, Wiley Online Library, 2015. 2\n\nContinuous control with deep reinforcement learning. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra, abs/1509.02971CoRR59Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015. 3, 5, 9\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In 2015 IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 3431-3440, June 2015. 2\n\nAre gans created equal? a large-scale study. Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet, abs/1711.10337Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. abs/1711.10337, 2018. 4\n\nConditional generative adversarial nets. CoRR, abs/1411.1784. Mehdi Mirza, Simon Osindero, Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014. 3\n\nHuman-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Shane Legg, and Demis Hassabis. Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra518529Volodymyr Mnih, Koray Kavukcuoglu, David Silver, An- drei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostro- vski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier- stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529 EP -, Feb 2015. 3\n\nGeometric deep learning on graphs and manifolds using mixture model cnns. Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodol\u00e0, Jan Svoboda, Michael M Bronstein, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAFederico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodol\u00e0, Jan Svoboda, and Michael M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5425-5434, 2017. 3\n\n. Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\n\nLearning deconvolution network for semantic segmentation. Learning deconvolution network for semantic segmentation.\n\n. Corr, abs/1505.04366CoRR, abs/1505.04366, 2015. 2\n\nSelf-attention gan. Park David Keetae, David Keetae Park. Self-attention gan. https:// github.com/heykeetae/Self-Attention-GAN, 2018. 5\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 5\n\nExample-based 3d scan completion. M Pauly, N J Mitra, J Giesen, M Gross, L Guibas, Symposium on Geometry Processing. M. Pauly, N. J. Mitra, J. Giesen, M. Gross, and L. Guibas. Example-based 3d scan completion. In Symposium on Ge- ometry Processing, pages 23-32, 2005. 2\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles Ruizhongtai Qi, Kaichun Su, Leonidas J Mo, Guibas, abs/1612.00593CoRR20Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. CoRR, abs/1612.00593, 2016. 2, 3, 7, 11, 20\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, abs/1706.02413CoRR23Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. CoRR, abs/1706.02413, 2017. 2, 3\n\nThe earth mover's distance as a metric for image retrieval. Yossi Rubner, Carlo Tomasi, Leonidas J Guibas, Int. J. Comput. Vision. 402Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. The earth mover's distance as a metric for image retrieval. Int. J. Comput. Vision, 40(2):99-121, Nov. 2000. 4\n\nSeednet: Automatic seed generation with deep reinforcement learning for robust interactive segmentation. Gwangmo Song, Heesoo Myeong, Kyoung Mu Lee, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Gwangmo Song, Heesoo Myeong, and Kyoung Mu Lee. Seednet: Automatic seed generation with deep reinforce- ment learning for robust interactive segmentation. In The IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), June 2018. 3\n\nLearning 3d shape completion under weak supervision. David Stutz, Andreas Geiger, abs/1805.07290CoRR13David Stutz and Andreas Geiger. Learning 3d shape comple- tion under weak supervision. CoRR, abs/1805.07290, 2018. 1, 3\n\nShape from symmetry. S Thrun, B Wegbreit, Tenth IEEE International Conference on Computer Vision (ICCV'05. 1S. Thrun and B. Wegbreit. Shape from symmetry. In Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1, volume 2, pages 1824-1831 Vol. 2, Oct 2005. 2\n\nXiaoou Tang, and Jianxiong Xiao. 3d shapenets for 2.5d object recognition and next-best-view prediction. CoRR, abs/1406. Zhirong Wu, Shuran Song, Aditya Khosla, 5670Zhirong Wu, Shuran Song, Aditya Khosla, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets for 2.5d object recognition and next-best-view prediction. CoRR, abs/1406.5670, 2014. 4\n\nSemantic image inpainting with perceptual and contextual losses. Raymond A Yeh, Chen Chen, Teck-Yian Lim, Mark Hasegawa-Johnson, Minh N Do, abs/1607.07539CoRRRaymond A. Yeh, Chen Chen, Teck-Yian Lim, Mark Hasegawa-Johnson, and Minh N. Do. Semantic image inpainting with perceptual and contextual losses. CoRR, abs/1607.07539, 2016. 2, 3, 8\n\nSelf-attention generative adversarial networks. Han Zhang, Ian J Goodfellow, Dimitris N Metaxas, Augustus Odena, abs/1805.08318CoRR1011Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative adversarial net- works. CoRR, abs/1805.08318, 2018. 3, 4, 9, 10, 11\n\nEnergy-based generative adversarial network. Jake Junbo, Micha\u00ebl Zhao, Yann Mathieu, Lecun, abs/1609.03126CoRRJunbo Jake Zhao, Micha\u00ebl Mathieu, and Yann LeCun. Energy-based generative adversarial network. CoRR, abs/1609.03126, 2016. 3\n", "annotations": {"author": "[{\"end\":147,\"start\":112},{\"end\":190,\"start\":148},{\"end\":203,\"start\":191},{\"end\":241,\"start\":204},{\"end\":261,\"start\":242},{\"end\":290,\"start\":262}]", "publisher": null, "author_last_name": "[{\"end\":127,\"start\":121},{\"end\":165,\"start\":162},{\"end\":202,\"start\":197},{\"end\":217,\"start\":214}]", "author_first_name": "[{\"end\":120,\"start\":112},{\"end\":155,\"start\":148},{\"end\":161,\"start\":156},{\"end\":196,\"start\":191},{\"end\":209,\"start\":204},{\"end\":213,\"start\":210}]", "author_affiliation": "[{\"end\":260,\"start\":243},{\"end\":289,\"start\":263}]", "title": "[{\"end\":109,\"start\":1},{\"end\":399,\"start\":291}]", "venue": null, "abstract": "[{\"end\":1616,\"start\":401}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2629,\"start\":2626},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2954,\"start\":2950},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2957,\"start\":2954},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2959,\"start\":2957},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3041,\"start\":3037},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3043,\"start\":3041},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3045,\"start\":3043},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3562,\"start\":3558},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3863,\"start\":3860},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3866,\"start\":3863},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3869,\"start\":3866},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3871,\"start\":3869},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3874,\"start\":3871},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4277,\"start\":4274},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4580,\"start\":4576},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4583,\"start\":4580},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5575,\"start\":5571},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6698,\"start\":6694},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6721,\"start\":6717},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6872,\"start\":6868},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6875,\"start\":6872},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7095,\"start\":7092},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7218,\"start\":7214},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7221,\"start\":7218},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7224,\"start\":7221},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7246,\"start\":7242},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7249,\"start\":7246},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7692,\"start\":7689},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8012,\"start\":8009},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8197,\"start\":8193},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8706,\"start\":8702},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8982,\"start\":8978},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8998,\"start\":8994},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9000,\"start\":8998},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9003,\"start\":9000},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9027,\"start\":9024},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9441,\"start\":9437},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9868,\"start\":9864},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10662,\"start\":10659},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10665,\"start\":10662},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10668,\"start\":10665},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10671,\"start\":10668},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10767,\"start\":10763},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10871,\"start\":10867},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10874,\"start\":10871},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11278,\"start\":11275},{\"end\":11280,\"start\":11278},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11317,\"start\":11313},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11348,\"start\":11344},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11412,\"start\":11408},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13515,\"start\":13511},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13544,\"start\":13540},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13546,\"start\":13544},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13878,\"start\":13875},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13919,\"start\":13915},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13921,\"start\":13919},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13944,\"start\":13941},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14370,\"start\":14366},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14437,\"start\":14433},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14440,\"start\":14437},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14442,\"start\":14440},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14527,\"start\":14523},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14550,\"start\":14547},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15042,\"start\":15038},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16075,\"start\":16071},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18082,\"start\":18078},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18616,\"start\":18612},{\"end\":20502,\"start\":20500},{\"end\":20612,\"start\":20609},{\"end\":20647,\"start\":20644},{\"end\":20707,\"start\":20704},{\"end\":20837,\"start\":20834},{\"end\":20867,\"start\":20864},{\"end\":20908,\"start\":20905},{\"end\":20952,\"start\":20949},{\"end\":20991,\"start\":20988},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21504,\"start\":21500},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21531,\"start\":21527},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21534,\"start\":21531},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21537,\"start\":21534},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21540,\"start\":21537},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21752,\"start\":21749},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21754,\"start\":21752},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22710,\"start\":22709},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22747,\"start\":22744},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24571,\"start\":24567},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24969,\"start\":24966},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25360,\"start\":25356},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27306,\"start\":27302},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29783,\"start\":29780},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30755,\"start\":30751},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31997,\"start\":31993},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33054,\"start\":33051},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33288,\"start\":33284},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33490,\"start\":33486},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33694,\"start\":33690},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36121,\"start\":36120},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36386,\"start\":36382},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37298,\"start\":37294},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39038,\"start\":39035},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39292,\"start\":39289},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":40240,\"start\":40236},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40661,\"start\":40658},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":41690,\"start\":41686},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":42247,\"start\":42244},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42310,\"start\":42307},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43078,\"start\":43075},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":44879,\"start\":44876},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46086,\"start\":46082}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43409,\"start\":43345},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43939,\"start\":43410},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44310,\"start\":43940},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44500,\"start\":44311},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44654,\"start\":44501},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44693,\"start\":44655},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44783,\"start\":44694},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44968,\"start\":44784},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45508,\"start\":44969},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45921,\"start\":45509},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46419,\"start\":45922},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46480,\"start\":46420}]", "paragraph": "[{\"end\":2208,\"start\":1632},{\"end\":3471,\"start\":2210},{\"end\":3875,\"start\":3473},{\"end\":5108,\"start\":3877},{\"end\":5576,\"start\":5110},{\"end\":5618,\"start\":5578},{\"end\":5787,\"start\":5620},{\"end\":6045,\"start\":5789},{\"end\":6201,\"start\":6047},{\"end\":6876,\"start\":6219},{\"end\":7399,\"start\":6878},{\"end\":8378,\"start\":7401},{\"end\":9004,\"start\":8380},{\"end\":9822,\"start\":9006},{\"end\":10307,\"start\":9824},{\"end\":11814,\"start\":10309},{\"end\":12253,\"start\":11826},{\"end\":12930,\"start\":12255},{\"end\":13639,\"start\":12951},{\"end\":13799,\"start\":13712},{\"end\":14261,\"start\":13801},{\"end\":15018,\"start\":14271},{\"end\":15679,\"start\":15020},{\"end\":15786,\"start\":15681},{\"end\":15946,\"start\":15846},{\"end\":15979,\"start\":15976},{\"end\":16034,\"start\":15981},{\"end\":16368,\"start\":16053},{\"end\":16868,\"start\":16400},{\"end\":16978,\"start\":16904},{\"end\":17708,\"start\":16980},{\"end\":18451,\"start\":17710},{\"end\":19084,\"start\":18503},{\"end\":19416,\"start\":19166},{\"end\":19827,\"start\":19438},{\"end\":20037,\"start\":19829},{\"end\":20161,\"start\":20039},{\"end\":20255,\"start\":20185},{\"end\":20463,\"start\":20257},{\"end\":20473,\"start\":20465},{\"end\":20526,\"start\":20480},{\"end\":20574,\"start\":20528},{\"end\":20589,\"start\":20576},{\"end\":20665,\"start\":20591},{\"end\":20732,\"start\":20673},{\"end\":20770,\"start\":20734},{\"end\":21012,\"start\":20778},{\"end\":21468,\"start\":21036},{\"end\":22549,\"start\":21484},{\"end\":23119,\"start\":22578},{\"end\":24287,\"start\":23121},{\"end\":24827,\"start\":24289},{\"end\":25714,\"start\":24862},{\"end\":26624,\"start\":25716},{\"end\":27586,\"start\":26660},{\"end\":28460,\"start\":27588},{\"end\":30419,\"start\":28489},{\"end\":31119,\"start\":30450},{\"end\":31204,\"start\":31121},{\"end\":31431,\"start\":31206},{\"end\":31998,\"start\":31433},{\"end\":32281,\"start\":32025},{\"end\":33055,\"start\":32296},{\"end\":33898,\"start\":33073},{\"end\":34250,\"start\":33900},{\"end\":34474,\"start\":34252},{\"end\":34530,\"start\":34476},{\"end\":34812,\"start\":34551},{\"end\":35288,\"start\":34814},{\"end\":36034,\"start\":35290},{\"end\":36123,\"start\":36036},{\"end\":36387,\"start\":36125},{\"end\":36922,\"start\":36389},{\"end\":37327,\"start\":37110},{\"end\":37415,\"start\":37329},{\"end\":38195,\"start\":37417},{\"end\":38279,\"start\":38197},{\"end\":38476,\"start\":38302},{\"end\":38935,\"start\":38505},{\"end\":39195,\"start\":38958},{\"end\":40242,\"start\":39197},{\"end\":41634,\"start\":40323},{\"end\":42320,\"start\":41657},{\"end\":42380,\"start\":42322},{\"end\":43344,\"start\":42382}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13711,\"start\":13640},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15845,\"start\":15787},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15975,\"start\":15947},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16052,\"start\":16035},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16903,\"start\":16869},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18502,\"start\":18452},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19165,\"start\":19085},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20184,\"start\":20162},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21035,\"start\":21013},{\"attributes\":{\"id\":\"formula_9\"},\"end\":37109,\"start\":36923},{\"attributes\":{\"id\":\"formula_10\"},\"end\":40322,\"start\":40243}]", "table_ref": "[{\"end\":33432,\"start\":33425},{\"end\":37134,\"start\":37127},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38278,\"start\":38271},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":40213,\"start\":39867},{\"end\":40481,\"start\":40474},{\"end\":41904,\"start\":41897},{\"end\":42966,\"start\":42959},{\"end\":43201,\"start\":43194}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1630,\"start\":1618},{\"attributes\":{\"n\":\"2.\"},\"end\":6217,\"start\":6204},{\"attributes\":{\"n\":\"3.\"},\"end\":11824,\"start\":11817},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12949,\"start\":12933},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14269,\"start\":14264},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16398,\"start\":16371},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19436,\"start\":19419},{\"end\":20478,\"start\":20476},{\"end\":20671,\"start\":20668},{\"end\":20776,\"start\":20773},{\"attributes\":{\"n\":\"4.\"},\"end\":21482,\"start\":21471},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22576,\"start\":22552},{\"end\":24860,\"start\":24830},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26658,\"start\":26627},{\"attributes\":{\"n\":\"4.3.\"},\"end\":28487,\"start\":28463},{\"attributes\":{\"n\":\"5.\"},\"end\":30448,\"start\":30422},{\"attributes\":{\"n\":\"6.\"},\"end\":32023,\"start\":32001},{\"attributes\":{\"n\":\"6.1.\"},\"end\":32294,\"start\":32284},{\"attributes\":{\"n\":\"6.2.\"},\"end\":33071,\"start\":33058},{\"attributes\":{\"n\":\"6.3.\"},\"end\":34549,\"start\":34533},{\"attributes\":{\"n\":\"7.\"},\"end\":38300,\"start\":38282},{\"attributes\":{\"n\":\"7.1.\"},\"end\":38503,\"start\":38479},{\"attributes\":{\"n\":\"7.2.\"},\"end\":38956,\"start\":38938},{\"attributes\":{\"n\":\"7.3.\"},\"end\":41655,\"start\":41637},{\"end\":43356,\"start\":43346},{\"end\":43421,\"start\":43411},{\"end\":43951,\"start\":43941},{\"end\":44322,\"start\":44312},{\"end\":44512,\"start\":44502},{\"end\":44666,\"start\":44656},{\"end\":44705,\"start\":44695},{\"end\":44796,\"start\":44785},{\"end\":44992,\"start\":44970},{\"end\":45519,\"start\":45510},{\"end\":45932,\"start\":45923},{\"end\":46430,\"start\":46421}]", "table": "[{\"end\":45921,\"start\":45848},{\"end\":46419,\"start\":46115}]", "figure_caption": "[{\"end\":43409,\"start\":43358},{\"end\":43939,\"start\":43423},{\"end\":44310,\"start\":43953},{\"end\":44500,\"start\":44324},{\"end\":44654,\"start\":44514},{\"end\":44693,\"start\":44668},{\"end\":44783,\"start\":44707},{\"end\":44968,\"start\":44799},{\"end\":45508,\"start\":44997},{\"end\":45848,\"start\":45521},{\"end\":46115,\"start\":45934},{\"end\":46480,\"start\":46432}]", "figure_ref": "[{\"end\":3095,\"start\":3087},{\"end\":3945,\"start\":3939},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5107,\"start\":5100},{\"end\":12307,\"start\":12301},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14260,\"start\":14251},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17056,\"start\":17050},{\"end\":21348,\"start\":21342},{\"end\":22756,\"start\":22749},{\"end\":23276,\"start\":23269},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23431,\"start\":23424},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23688,\"start\":23681},{\"end\":25384,\"start\":25376},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25905,\"start\":25898},{\"end\":27407,\"start\":27400},{\"end\":28206,\"start\":28199},{\"end\":28280,\"start\":28273},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28336,\"start\":28330},{\"end\":28566,\"start\":28559},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29111,\"start\":29105},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32204,\"start\":32198},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32461,\"start\":32454},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33174,\"start\":33167},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34682,\"start\":34675},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36444,\"start\":36438},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38628,\"start\":38621},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38641,\"start\":38633},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38800,\"start\":38793},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39062,\"start\":39055},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39074,\"start\":39067},{\"end\":40768,\"start\":40762},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41005,\"start\":40998},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42447,\"start\":42440},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42995,\"start\":42986}]", "bib_author_first_name": "[{\"end\":46927,\"start\":46922},{\"end\":46944,\"start\":46940},{\"end\":46962,\"start\":46955},{\"end\":46983,\"start\":46975},{\"end\":46985,\"start\":46984},{\"end\":47249,\"start\":47243},{\"end\":47267,\"start\":47260},{\"end\":47282,\"start\":47278},{\"end\":47935,\"start\":47929},{\"end\":47951,\"start\":47945},{\"end\":47972,\"start\":47966},{\"end\":47987,\"start\":47982},{\"end\":48315,\"start\":48308},{\"end\":48330,\"start\":48326},{\"end\":48346,\"start\":48339},{\"end\":48548,\"start\":48543},{\"end\":48550,\"start\":48549},{\"end\":48564,\"start\":48558},{\"end\":48566,\"start\":48565},{\"end\":48587,\"start\":48579},{\"end\":48589,\"start\":48588},{\"end\":48601,\"start\":48598},{\"end\":48619,\"start\":48612},{\"end\":48631,\"start\":48627},{\"end\":48642,\"start\":48636},{\"end\":48660,\"start\":48653},{\"end\":48674,\"start\":48668},{\"end\":48684,\"start\":48681},{\"end\":48698,\"start\":48689},{\"end\":48707,\"start\":48705},{\"end\":48718,\"start\":48712},{\"end\":49346,\"start\":49340},{\"end\":49359,\"start\":49352},{\"end\":49371,\"start\":49360},{\"end\":49384,\"start\":49376},{\"end\":49688,\"start\":49682},{\"end\":49700,\"start\":49694},{\"end\":49716,\"start\":49710},{\"end\":49731,\"start\":49726},{\"end\":49744,\"start\":49738},{\"end\":49760,\"start\":49752},{\"end\":50089,\"start\":50081},{\"end\":50098,\"start\":50095},{\"end\":50111,\"start\":50103},{\"end\":50113,\"start\":50112},{\"end\":50395,\"start\":50387},{\"end\":50404,\"start\":50401},{\"end\":50417,\"start\":50409},{\"end\":50419,\"start\":50418},{\"end\":50695,\"start\":50690},{\"end\":50914,\"start\":50913},{\"end\":50916,\"start\":50915},{\"end\":51189,\"start\":51184},{\"end\":51204,\"start\":51200},{\"end\":51426,\"start\":51423},{\"end\":51443,\"start\":51439},{\"end\":51464,\"start\":51459},{\"end\":51476,\"start\":51472},{\"end\":51486,\"start\":51481},{\"end\":51508,\"start\":51501},{\"end\":51521,\"start\":51516},{\"end\":51539,\"start\":51533},{\"end\":52064,\"start\":52058},{\"end\":52081,\"start\":52076},{\"end\":52095,\"start\":52089},{\"end\":52113,\"start\":52106},{\"end\":52129,\"start\":52124},{\"end\":52131,\"start\":52130},{\"end\":52392,\"start\":52381},{\"end\":52412,\"start\":52405},{\"end\":52538,\"start\":52531},{\"end\":52550,\"start\":52543},{\"end\":52566,\"start\":52558},{\"end\":52576,\"start\":52572},{\"end\":52801,\"start\":52794},{\"end\":52815,\"start\":52809},{\"end\":52828,\"start\":52822},{\"end\":53064,\"start\":53062},{\"end\":53246,\"start\":53241},{\"end\":53248,\"start\":53247},{\"end\":53270,\"start\":53264},{\"end\":53286,\"start\":53278},{\"end\":53644,\"start\":53643},{\"end\":53646,\"start\":53645},{\"end\":53865,\"start\":53864},{\"end\":53881,\"start\":53876},{\"end\":54074,\"start\":54070},{\"end\":54091,\"start\":54087},{\"end\":54111,\"start\":54103},{\"end\":54113,\"start\":54112},{\"end\":54682,\"start\":54678},{\"end\":54698,\"start\":54690},{\"end\":54700,\"start\":54699},{\"end\":54712,\"start\":54708},{\"end\":54714,\"start\":54713},{\"end\":54729,\"start\":54723},{\"end\":54742,\"start\":54741},{\"end\":54744,\"start\":54743},{\"end\":54758,\"start\":54753},{\"end\":54760,\"start\":54759},{\"end\":54778,\"start\":54770},{\"end\":54780,\"start\":54779},{\"end\":55259,\"start\":55252},{\"end\":55270,\"start\":55264},{\"end\":55284,\"start\":55276},{\"end\":55301,\"start\":55293},{\"end\":55624,\"start\":55617},{\"end\":55626,\"start\":55625},{\"end\":55646,\"start\":55638},{\"end\":55648,\"start\":55647},{\"end\":55664,\"start\":55655},{\"end\":55681,\"start\":55674},{\"end\":55692,\"start\":55689},{\"end\":55704,\"start\":55699},{\"end\":55717,\"start\":55712},{\"end\":55730,\"start\":55726},{\"end\":56038,\"start\":56037},{\"end\":56046,\"start\":56045},{\"end\":56059,\"start\":56058},{\"end\":56389,\"start\":56384},{\"end\":56402,\"start\":56397},{\"end\":56417,\"start\":56411},{\"end\":56436,\"start\":56429},{\"end\":56451,\"start\":56444},{\"end\":56695,\"start\":56690},{\"end\":56708,\"start\":56703},{\"end\":56888,\"start\":56879},{\"end\":56900,\"start\":56895},{\"end\":56919,\"start\":56914},{\"end\":56934,\"start\":56928},{\"end\":56936,\"start\":56935},{\"end\":56947,\"start\":56943},{\"end\":56960,\"start\":56956},{\"end\":56962,\"start\":56961},{\"end\":56978,\"start\":56974},{\"end\":56993,\"start\":56987},{\"end\":57013,\"start\":57006},{\"end\":57015,\"start\":57014},{\"end\":57032,\"start\":57027},{\"end\":57048,\"start\":57044},{\"end\":57678,\"start\":57670},{\"end\":57692,\"start\":57686},{\"end\":57711,\"start\":57703},{\"end\":57727,\"start\":57719},{\"end\":57739,\"start\":57736},{\"end\":57756,\"start\":57749},{\"end\":57758,\"start\":57757},{\"end\":58183,\"start\":58175},{\"end\":58198,\"start\":58189},{\"end\":58212,\"start\":58205},{\"end\":58460,\"start\":58456},{\"end\":58615,\"start\":58611},{\"end\":58627,\"start\":58624},{\"end\":58642,\"start\":58635},{\"end\":58660,\"start\":58653},{\"end\":58675,\"start\":58669},{\"end\":58689,\"start\":58682},{\"end\":58704,\"start\":58698},{\"end\":58715,\"start\":58710},{\"end\":58731,\"start\":58727},{\"end\":58744,\"start\":58740},{\"end\":58981,\"start\":58980},{\"end\":58990,\"start\":58989},{\"end\":58992,\"start\":58991},{\"end\":59001,\"start\":59000},{\"end\":59011,\"start\":59010},{\"end\":59020,\"start\":59019},{\"end\":59298,\"start\":59295},{\"end\":59330,\"start\":59323},{\"end\":59343,\"start\":59335},{\"end\":59345,\"start\":59344},{\"end\":59651,\"start\":59649},{\"end\":59679,\"start\":59676},{\"end\":59692,\"start\":59684},{\"end\":59694,\"start\":59693},{\"end\":59969,\"start\":59964},{\"end\":59983,\"start\":59978},{\"end\":60000,\"start\":59992},{\"end\":60002,\"start\":60001},{\"end\":60314,\"start\":60307},{\"end\":60327,\"start\":60321},{\"end\":60345,\"start\":60336},{\"end\":60725,\"start\":60720},{\"end\":60740,\"start\":60733},{\"end\":60912,\"start\":60911},{\"end\":60921,\"start\":60920},{\"end\":61298,\"start\":61291},{\"end\":61309,\"start\":61303},{\"end\":61322,\"start\":61316},{\"end\":61582,\"start\":61575},{\"end\":61584,\"start\":61583},{\"end\":61594,\"start\":61590},{\"end\":61610,\"start\":61601},{\"end\":61620,\"start\":61616},{\"end\":61643,\"start\":61639},{\"end\":61645,\"start\":61644},{\"end\":61902,\"start\":61899},{\"end\":61913,\"start\":61910},{\"end\":61915,\"start\":61914},{\"end\":61936,\"start\":61928},{\"end\":61938,\"start\":61937},{\"end\":61956,\"start\":61948},{\"end\":62201,\"start\":62197},{\"end\":62216,\"start\":62209},{\"end\":62227,\"start\":62223}]", "bib_author_last_name": "[{\"end\":46938,\"start\":46928},{\"end\":46953,\"start\":46945},{\"end\":46973,\"start\":46963},{\"end\":46992,\"start\":46986},{\"end\":47258,\"start\":47250},{\"end\":47276,\"start\":47268},{\"end\":47289,\"start\":47283},{\"end\":47943,\"start\":47936},{\"end\":47964,\"start\":47952},{\"end\":47980,\"start\":47973},{\"end\":47994,\"start\":47988},{\"end\":48324,\"start\":48316},{\"end\":48337,\"start\":48331},{\"end\":48358,\"start\":48347},{\"end\":48556,\"start\":48551},{\"end\":48577,\"start\":48567},{\"end\":48596,\"start\":48590},{\"end\":48610,\"start\":48602},{\"end\":48625,\"start\":48620},{\"end\":48634,\"start\":48632},{\"end\":48651,\"start\":48643},{\"end\":48666,\"start\":48661},{\"end\":48679,\"start\":48675},{\"end\":48687,\"start\":48685},{\"end\":48703,\"start\":48699},{\"end\":48710,\"start\":48708},{\"end\":48721,\"start\":48719},{\"end\":49350,\"start\":49347},{\"end\":49374,\"start\":49372},{\"end\":49392,\"start\":49385},{\"end\":49692,\"start\":49689},{\"end\":49708,\"start\":49701},{\"end\":49724,\"start\":49717},{\"end\":49736,\"start\":49732},{\"end\":49750,\"start\":49745},{\"end\":49768,\"start\":49761},{\"end\":50093,\"start\":50090},{\"end\":50101,\"start\":50099},{\"end\":50120,\"start\":50114},{\"end\":50399,\"start\":50396},{\"end\":50407,\"start\":50405},{\"end\":50426,\"start\":50420},{\"end\":50704,\"start\":50696},{\"end\":50927,\"start\":50917},{\"end\":50935,\"start\":50929},{\"end\":51198,\"start\":51190},{\"end\":51219,\"start\":51205},{\"end\":51226,\"start\":51221},{\"end\":51437,\"start\":51427},{\"end\":51457,\"start\":51444},{\"end\":51470,\"start\":51465},{\"end\":51479,\"start\":51477},{\"end\":51499,\"start\":51487},{\"end\":51514,\"start\":51509},{\"end\":51531,\"start\":51522},{\"end\":51546,\"start\":51540},{\"end\":52074,\"start\":52065},{\"end\":52087,\"start\":52082},{\"end\":52104,\"start\":52096},{\"end\":52122,\"start\":52114},{\"end\":52141,\"start\":52132},{\"end\":52403,\"start\":52393},{\"end\":52420,\"start\":52413},{\"end\":52541,\"start\":52539},{\"end\":52556,\"start\":52551},{\"end\":52570,\"start\":52567},{\"end\":52580,\"start\":52577},{\"end\":52807,\"start\":52802},{\"end\":52820,\"start\":52816},{\"end\":52836,\"start\":52829},{\"end\":52957,\"start\":52953},{\"end\":53071,\"start\":53065},{\"end\":53262,\"start\":53249},{\"end\":53276,\"start\":53271},{\"end\":53292,\"start\":53287},{\"end\":53300,\"start\":53294},{\"end\":53657,\"start\":53647},{\"end\":53665,\"start\":53659},{\"end\":53874,\"start\":53866},{\"end\":53888,\"start\":53882},{\"end\":53892,\"start\":53890},{\"end\":54085,\"start\":54075},{\"end\":54101,\"start\":54092},{\"end\":54120,\"start\":54114},{\"end\":54688,\"start\":54683},{\"end\":54706,\"start\":54701},{\"end\":54721,\"start\":54715},{\"end\":54739,\"start\":54730},{\"end\":54751,\"start\":54745},{\"end\":54768,\"start\":54761},{\"end\":54787,\"start\":54781},{\"end\":55262,\"start\":55260},{\"end\":55274,\"start\":55271},{\"end\":55291,\"start\":55285},{\"end\":55309,\"start\":55302},{\"end\":55531,\"start\":55511},{\"end\":55636,\"start\":55627},{\"end\":55653,\"start\":55649},{\"end\":55672,\"start\":55665},{\"end\":55687,\"start\":55682},{\"end\":55697,\"start\":55693},{\"end\":55710,\"start\":55705},{\"end\":55724,\"start\":55718},{\"end\":55739,\"start\":55731},{\"end\":56043,\"start\":56039},{\"end\":56056,\"start\":56047},{\"end\":56067,\"start\":56060},{\"end\":56395,\"start\":56390},{\"end\":56409,\"start\":56403},{\"end\":56427,\"start\":56418},{\"end\":56442,\"start\":56437},{\"end\":56460,\"start\":56452},{\"end\":56701,\"start\":56696},{\"end\":56717,\"start\":56709},{\"end\":56893,\"start\":56889},{\"end\":56912,\"start\":56901},{\"end\":56926,\"start\":56920},{\"end\":56941,\"start\":56937},{\"end\":56954,\"start\":56948},{\"end\":56972,\"start\":56963},{\"end\":56985,\"start\":56979},{\"end\":57004,\"start\":56994},{\"end\":57025,\"start\":57016},{\"end\":57042,\"start\":57033},{\"end\":57057,\"start\":57049},{\"end\":57684,\"start\":57679},{\"end\":57701,\"start\":57693},{\"end\":57717,\"start\":57712},{\"end\":57734,\"start\":57728},{\"end\":57747,\"start\":57740},{\"end\":57768,\"start\":57759},{\"end\":58187,\"start\":58184},{\"end\":58203,\"start\":58199},{\"end\":58216,\"start\":58213},{\"end\":58389,\"start\":58385},{\"end\":58473,\"start\":58461},{\"end\":58622,\"start\":58616},{\"end\":58633,\"start\":58628},{\"end\":58651,\"start\":58643},{\"end\":58667,\"start\":58661},{\"end\":58680,\"start\":58676},{\"end\":58696,\"start\":58690},{\"end\":58708,\"start\":58705},{\"end\":58725,\"start\":58716},{\"end\":58738,\"start\":58732},{\"end\":58750,\"start\":58745},{\"end\":58987,\"start\":58982},{\"end\":58998,\"start\":58993},{\"end\":59008,\"start\":59002},{\"end\":59017,\"start\":59012},{\"end\":59027,\"start\":59021},{\"end\":59321,\"start\":59299},{\"end\":59333,\"start\":59331},{\"end\":59348,\"start\":59346},{\"end\":59356,\"start\":59350},{\"end\":59674,\"start\":59652},{\"end\":59682,\"start\":59680},{\"end\":59697,\"start\":59695},{\"end\":59705,\"start\":59699},{\"end\":59976,\"start\":59970},{\"end\":59990,\"start\":59984},{\"end\":60009,\"start\":60003},{\"end\":60319,\"start\":60315},{\"end\":60334,\"start\":60328},{\"end\":60349,\"start\":60346},{\"end\":60731,\"start\":60726},{\"end\":60747,\"start\":60741},{\"end\":60918,\"start\":60913},{\"end\":60930,\"start\":60922},{\"end\":61301,\"start\":61299},{\"end\":61314,\"start\":61310},{\"end\":61329,\"start\":61323},{\"end\":61588,\"start\":61585},{\"end\":61599,\"start\":61595},{\"end\":61614,\"start\":61611},{\"end\":61637,\"start\":61621},{\"end\":61648,\"start\":61646},{\"end\":61908,\"start\":61903},{\"end\":61926,\"start\":61916},{\"end\":61946,\"start\":61939},{\"end\":61962,\"start\":61957},{\"end\":62207,\"start\":62202},{\"end\":62221,\"start\":62217},{\"end\":62235,\"start\":62228},{\"end\":62242,\"start\":62237}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":47196,\"start\":46829},{\"attributes\":{\"doi\":\"PMLR. 3\",\"id\":\"b1\",\"matched_paper_id\":2057420},\"end\":47863,\"start\":47198},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11296681},\"end\":48236,\"start\":47865},{\"attributes\":{\"doi\":\"abs/1605.07678\",\"id\":\"b3\"},\"end\":48539,\"start\":48238},{\"attributes\":{\"id\":\"b4\"},\"end\":48968,\"start\":48541},{\"attributes\":{\"doi\":\"abs/1512.03012\",\"id\":\"b5\"},\"end\":49262,\"start\":48970},{\"attributes\":{\"doi\":\"abs/1612.00101\",\"id\":\"b6\"},\"end\":49598,\"start\":49264},{\"attributes\":{\"doi\":\"abs/1712.10215\",\"id\":\"b7\"},\"end\":49998,\"start\":49600},{\"attributes\":{\"doi\":\"abs/1612.00603\",\"id\":\"b8\"},\"end\":50304,\"start\":50000},{\"attributes\":{\"doi\":\"abs/1612.00603\",\"id\":\"b9\"},\"end\":50607,\"start\":50306},{\"attributes\":{\"id\":\"b10\"},\"end\":50801,\"start\":50609},{\"attributes\":{\"id\":\"b11\"},\"end\":51117,\"start\":50803},{\"attributes\":{\"doi\":\"abs/1802.09477\",\"id\":\"b12\"},\"end\":51392,\"start\":51119},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1033682},\"end\":52054,\"start\":51394},{\"attributes\":{\"id\":\"b14\"},\"end\":52379,\"start\":52056},{\"attributes\":{\"id\":\"b15\"},\"end\":52483,\"start\":52381},{\"attributes\":{\"doi\":\"abs/1512.03385\",\"id\":\"b16\"},\"end\":52730,\"start\":52485},{\"attributes\":{\"id\":\"b17\"},\"end\":52949,\"start\":52732},{\"attributes\":{\"doi\":\"abs/1802.04402\",\"id\":\"b18\"},\"end\":53002,\"start\":52951},{\"attributes\":{\"id\":\"b19\"},\"end\":53194,\"start\":53004},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6387371},\"end\":53536,\"start\":53196},{\"attributes\":{\"id\":\"b21\"},\"end\":53802,\"start\":53538},{\"attributes\":{\"id\":\"b22\"},\"end\":54003,\"start\":53804},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":195908774},\"end\":54613,\"start\":54005},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2542741},\"end\":55182,\"start\":54615},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3145626},\"end\":55507,\"start\":55184},{\"attributes\":{\"id\":\"b26\"},\"end\":55562,\"start\":55509},{\"attributes\":{\"doi\":\"abs/1509.02971\",\"id\":\"b27\"},\"end\":55979,\"start\":55564},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1629541},\"end\":56337,\"start\":55981},{\"attributes\":{\"doi\":\"abs/1711.10337\",\"id\":\"b29\"},\"end\":56626,\"start\":56339},{\"attributes\":{\"id\":\"b30\"},\"end\":56820,\"start\":56628},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":205242740},\"end\":57594,\"start\":56822},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":301319},\"end\":58171,\"start\":57596},{\"attributes\":{\"id\":\"b33\"},\"end\":58264,\"start\":58173},{\"attributes\":{\"id\":\"b34\"},\"end\":58381,\"start\":58266},{\"attributes\":{\"doi\":\"abs/1505.04366\",\"id\":\"b35\"},\"end\":58434,\"start\":58383},{\"attributes\":{\"id\":\"b36\"},\"end\":58571,\"start\":58436},{\"attributes\":{\"id\":\"b37\"},\"end\":58944,\"start\":58573},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":364731},\"end\":59215,\"start\":58946},{\"attributes\":{\"doi\":\"abs/1612.00593\",\"id\":\"b39\"},\"end\":59567,\"start\":59217},{\"attributes\":{\"doi\":\"abs/1706.02413\",\"id\":\"b40\"},\"end\":59902,\"start\":59569},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14106275},\"end\":60200,\"start\":59904},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52087289},\"end\":60665,\"start\":60202},{\"attributes\":{\"doi\":\"abs/1805.07290\",\"id\":\"b43\"},\"end\":60888,\"start\":60667},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1819358},\"end\":61168,\"start\":60890},{\"attributes\":{\"id\":\"b45\"},\"end\":61508,\"start\":61170},{\"attributes\":{\"doi\":\"abs/1607.07539\",\"id\":\"b46\"},\"end\":61849,\"start\":61510},{\"attributes\":{\"doi\":\"abs/1805.08318\",\"id\":\"b47\"},\"end\":62150,\"start\":61851},{\"attributes\":{\"doi\":\"abs/1609.03126\",\"id\":\"b48\"},\"end\":62386,\"start\":62152}]", "bib_title": "[{\"end\":47241,\"start\":47198},{\"end\":47927,\"start\":47865},{\"end\":49083,\"start\":48970},{\"end\":51421,\"start\":51394},{\"end\":53239,\"start\":53196},{\"end\":54068,\"start\":54005},{\"end\":54676,\"start\":54615},{\"end\":55250,\"start\":55184},{\"end\":56035,\"start\":55981},{\"end\":56877,\"start\":56822},{\"end\":57668,\"start\":57596},{\"end\":58978,\"start\":58946},{\"end\":59962,\"start\":59904},{\"end\":60305,\"start\":60202},{\"end\":60909,\"start\":60890}]", "bib_author": "[{\"end\":46940,\"start\":46922},{\"end\":46955,\"start\":46940},{\"end\":46975,\"start\":46955},{\"end\":46994,\"start\":46975},{\"end\":47260,\"start\":47243},{\"end\":47278,\"start\":47260},{\"end\":47291,\"start\":47278},{\"end\":47945,\"start\":47929},{\"end\":47966,\"start\":47945},{\"end\":47982,\"start\":47966},{\"end\":47996,\"start\":47982},{\"end\":48326,\"start\":48308},{\"end\":48339,\"start\":48326},{\"end\":48360,\"start\":48339},{\"end\":48558,\"start\":48543},{\"end\":48579,\"start\":48558},{\"end\":48598,\"start\":48579},{\"end\":48612,\"start\":48598},{\"end\":48627,\"start\":48612},{\"end\":48636,\"start\":48627},{\"end\":48653,\"start\":48636},{\"end\":48668,\"start\":48653},{\"end\":48681,\"start\":48668},{\"end\":48689,\"start\":48681},{\"end\":48705,\"start\":48689},{\"end\":48712,\"start\":48705},{\"end\":48723,\"start\":48712},{\"end\":49352,\"start\":49340},{\"end\":49376,\"start\":49352},{\"end\":49394,\"start\":49376},{\"end\":49694,\"start\":49682},{\"end\":49710,\"start\":49694},{\"end\":49726,\"start\":49710},{\"end\":49738,\"start\":49726},{\"end\":49752,\"start\":49738},{\"end\":49770,\"start\":49752},{\"end\":50095,\"start\":50081},{\"end\":50103,\"start\":50095},{\"end\":50122,\"start\":50103},{\"end\":50401,\"start\":50387},{\"end\":50409,\"start\":50401},{\"end\":50428,\"start\":50409},{\"end\":50706,\"start\":50690},{\"end\":50929,\"start\":50913},{\"end\":50937,\"start\":50929},{\"end\":51200,\"start\":51184},{\"end\":51221,\"start\":51200},{\"end\":51228,\"start\":51221},{\"end\":51439,\"start\":51423},{\"end\":51459,\"start\":51439},{\"end\":51472,\"start\":51459},{\"end\":51481,\"start\":51472},{\"end\":51501,\"start\":51481},{\"end\":51516,\"start\":51501},{\"end\":51533,\"start\":51516},{\"end\":51548,\"start\":51533},{\"end\":52076,\"start\":52058},{\"end\":52089,\"start\":52076},{\"end\":52106,\"start\":52089},{\"end\":52124,\"start\":52106},{\"end\":52143,\"start\":52124},{\"end\":52405,\"start\":52381},{\"end\":52422,\"start\":52405},{\"end\":52543,\"start\":52531},{\"end\":52558,\"start\":52543},{\"end\":52572,\"start\":52558},{\"end\":52582,\"start\":52572},{\"end\":52809,\"start\":52794},{\"end\":52822,\"start\":52809},{\"end\":52838,\"start\":52822},{\"end\":52959,\"start\":52953},{\"end\":53073,\"start\":53062},{\"end\":53264,\"start\":53241},{\"end\":53278,\"start\":53264},{\"end\":53294,\"start\":53278},{\"end\":53302,\"start\":53294},{\"end\":53659,\"start\":53643},{\"end\":53667,\"start\":53659},{\"end\":53876,\"start\":53864},{\"end\":53890,\"start\":53876},{\"end\":53894,\"start\":53890},{\"end\":54087,\"start\":54070},{\"end\":54103,\"start\":54087},{\"end\":54122,\"start\":54103},{\"end\":54690,\"start\":54678},{\"end\":54708,\"start\":54690},{\"end\":54723,\"start\":54708},{\"end\":54741,\"start\":54723},{\"end\":54753,\"start\":54741},{\"end\":54770,\"start\":54753},{\"end\":54789,\"start\":54770},{\"end\":55264,\"start\":55252},{\"end\":55276,\"start\":55264},{\"end\":55293,\"start\":55276},{\"end\":55311,\"start\":55293},{\"end\":55533,\"start\":55511},{\"end\":55638,\"start\":55617},{\"end\":55655,\"start\":55638},{\"end\":55674,\"start\":55655},{\"end\":55689,\"start\":55674},{\"end\":55699,\"start\":55689},{\"end\":55712,\"start\":55699},{\"end\":55726,\"start\":55712},{\"end\":55741,\"start\":55726},{\"end\":56045,\"start\":56037},{\"end\":56058,\"start\":56045},{\"end\":56069,\"start\":56058},{\"end\":56397,\"start\":56384},{\"end\":56411,\"start\":56397},{\"end\":56429,\"start\":56411},{\"end\":56444,\"start\":56429},{\"end\":56462,\"start\":56444},{\"end\":56703,\"start\":56690},{\"end\":56719,\"start\":56703},{\"end\":56895,\"start\":56879},{\"end\":56914,\"start\":56895},{\"end\":56928,\"start\":56914},{\"end\":56943,\"start\":56928},{\"end\":56956,\"start\":56943},{\"end\":56974,\"start\":56956},{\"end\":56987,\"start\":56974},{\"end\":57006,\"start\":56987},{\"end\":57027,\"start\":57006},{\"end\":57044,\"start\":57027},{\"end\":57059,\"start\":57044},{\"end\":57686,\"start\":57670},{\"end\":57703,\"start\":57686},{\"end\":57719,\"start\":57703},{\"end\":57736,\"start\":57719},{\"end\":57749,\"start\":57736},{\"end\":57770,\"start\":57749},{\"end\":58189,\"start\":58175},{\"end\":58205,\"start\":58189},{\"end\":58218,\"start\":58205},{\"end\":58391,\"start\":58385},{\"end\":58475,\"start\":58456},{\"end\":58624,\"start\":58611},{\"end\":58635,\"start\":58624},{\"end\":58653,\"start\":58635},{\"end\":58669,\"start\":58653},{\"end\":58682,\"start\":58669},{\"end\":58698,\"start\":58682},{\"end\":58710,\"start\":58698},{\"end\":58727,\"start\":58710},{\"end\":58740,\"start\":58727},{\"end\":58752,\"start\":58740},{\"end\":58989,\"start\":58980},{\"end\":59000,\"start\":58989},{\"end\":59010,\"start\":59000},{\"end\":59019,\"start\":59010},{\"end\":59029,\"start\":59019},{\"end\":59323,\"start\":59295},{\"end\":59335,\"start\":59323},{\"end\":59350,\"start\":59335},{\"end\":59358,\"start\":59350},{\"end\":59676,\"start\":59649},{\"end\":59684,\"start\":59676},{\"end\":59699,\"start\":59684},{\"end\":59707,\"start\":59699},{\"end\":59978,\"start\":59964},{\"end\":59992,\"start\":59978},{\"end\":60011,\"start\":59992},{\"end\":60321,\"start\":60307},{\"end\":60336,\"start\":60321},{\"end\":60351,\"start\":60336},{\"end\":60733,\"start\":60720},{\"end\":60749,\"start\":60733},{\"end\":60920,\"start\":60911},{\"end\":60932,\"start\":60920},{\"end\":61303,\"start\":61291},{\"end\":61316,\"start\":61303},{\"end\":61331,\"start\":61316},{\"end\":61590,\"start\":61575},{\"end\":61601,\"start\":61590},{\"end\":61616,\"start\":61601},{\"end\":61639,\"start\":61616},{\"end\":61650,\"start\":61639},{\"end\":61910,\"start\":61899},{\"end\":61928,\"start\":61910},{\"end\":61948,\"start\":61928},{\"end\":61964,\"start\":61948},{\"end\":62209,\"start\":62197},{\"end\":62223,\"start\":62209},{\"end\":62237,\"start\":62223},{\"end\":62244,\"start\":62237}]", "bib_venue": "[{\"end\":47467,\"start\":47397},{\"end\":54290,\"start\":54213},{\"end\":57183,\"start\":57091},{\"end\":57852,\"start\":57835},{\"end\":46920,\"start\":46829},{\"end\":47366,\"start\":47298},{\"end\":48032,\"start\":47996},{\"end\":48306,\"start\":48238},{\"end\":49105,\"start\":49099},{\"end\":49338,\"start\":49264},{\"end\":49680,\"start\":49600},{\"end\":50079,\"start\":50000},{\"end\":50385,\"start\":50306},{\"end\":50688,\"start\":50609},{\"end\":50911,\"start\":50803},{\"end\":51182,\"start\":51119},{\"end\":51597,\"start\":51548},{\"end\":52429,\"start\":52422},{\"end\":52529,\"start\":52485},{\"end\":52792,\"start\":52732},{\"end\":53060,\"start\":53004},{\"end\":53357,\"start\":53302},{\"end\":53641,\"start\":53538},{\"end\":53862,\"start\":53804},{\"end\":54211,\"start\":54122},{\"end\":54838,\"start\":54789},{\"end\":55334,\"start\":55311},{\"end\":55615,\"start\":55564},{\"end\":56134,\"start\":56069},{\"end\":56382,\"start\":56339},{\"end\":56688,\"start\":56628},{\"end\":57089,\"start\":57059},{\"end\":57833,\"start\":57770},{\"end\":58322,\"start\":58266},{\"end\":58454,\"start\":58436},{\"end\":58609,\"start\":58573},{\"end\":59061,\"start\":59029},{\"end\":59293,\"start\":59217},{\"end\":59647,\"start\":59569},{\"end\":60033,\"start\":60011},{\"end\":60420,\"start\":60351},{\"end\":60718,\"start\":60667},{\"end\":60995,\"start\":60932},{\"end\":61289,\"start\":61170},{\"end\":61573,\"start\":61510},{\"end\":61897,\"start\":61851},{\"end\":62195,\"start\":62152}]"}}}, "year": 2023, "month": 12, "day": 17}