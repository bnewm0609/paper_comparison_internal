{"id": 7546676, "updated": "2023-03-25 19:56:24.069", "metadata": {"title": "Fast Haze Removal for Nighttime Image Using Maximum Reflectance Prior", "authors": "[{\"first\":\"Jing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Shuai\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Chang\",\"last\":\"Chen\",\"middle\":[\"Wen\"]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "In this paper, we address a haze removal problem from a single nighttime image, even in the presence of varicolored and non-uniform illumination. The core idea lies in a novel maximum reflectance prior. We first introduce the nighttime hazy imaging model, which includes a local ambient illumination item in both direct attenuation term and scattering term. Then, we propose a simple but effective image prior, maximum reflectance prior, to estimate the varying ambient illumination. The maximum reflectance prior is based on a key observation: for most daytime haze-free image patches, each color channel has very high intensity at some pixels. For the nighttime haze image, the local maximum intensities at each color channel are mainly contributed by the ambient illumination. Therefore, we can directly estimate the ambient illumination and transmission map, and consequently restore a high quality haze-free image. Experimental results on various nighttime hazy images demonstrate the effectiveness of the proposed approach. In particular, our approach has the advantage of computational efficiency, which is 10-100 times faster than state-of-the-art methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2746139371", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhangCFKC17", "doi": "10.1109/cvpr.2017.742"}}, "content": {"source": {"pdf_hash": "d73556b1c333aad0fcb36ac1aab92fa37950d51e", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0a264c0b0c48de26e6ac695a0aa549e4be12d240", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d73556b1c333aad0fcb36ac1aab92fa37950d51e.txt", "contents": "\nFast Haze Removal for Nighttime Image Using Maximum Reflectance Prior\n\n\nJing Zhang \nYang Cao \nShuai Fang fangshuai@hfut.edu.cn \nYu Kang kangduyu@ustc.edu.cn \nChang Wen Chen chencw@buffalo.edu \n\nUniversity of Science and Technology of China Hefei\nChina\n\n\nHefei University of Technology Hefei\nChina\n\n\nUniversity of Science and Technology of China Hefei\nChina\n\n\nState University of New York at Buffalo Buffalo\nU.S.A\n\nFast Haze Removal for Nighttime Image Using Maximum Reflectance Prior\n\nIn this paper, we address a haze removal problem from a single nighttime image, even in the presence of varicolored and non-uniform illumination. The core idea lies in a novel maximum reflectance prior. We first introduce the nighttime hazy imaging model, which includes a local ambient illumination item in both direct attenuation term and scattering term. Then, we propose a simple but effective image prior, maximum reflectance prior, to estimate the varying ambient illumination. The maximum reflectance prior is based on a key observation: for most daytime haze-free image patches, each color channel has very high intensity at some pixels. For the nighttime haze image, the local maximum intensities at each color channel are mainly contributed by the ambient illumination. Therefore, we can directly estimate the ambient illumination and transmission map, and consequently restore a high quality haze-free image. Experimental results on various nighttime hazy images demonstrate the effectiveness of the proposed approach. In particular, our approach has the advantage of computational efficiency, which is 10-100 times faster than state-of-the-art methods.\n\nIntroduction\n\nRestoring hazy images is important for many computer vision applications for outdoor scenes. Since tiny particles floating in the air absorb and scatter light in the atmosphere, the hazy images suffer from significant visibility degradation manifested in two aspects: the attenuation of the direct reflection light and the accumulation of the scattering ambient light. The degradation of hazy image increases with the distance between the scene points and the * co-first author  [10], [4] and our proposed method, respectively. Daytime dehazing method [10] and [4] fail to handle nighttime hazy scene due to the varicolored illumination.\n\ncamera, in which the magnitude of reflection light decreases and the magnitude of ambient light increases. Middleton's model [21] describes the hazy imaging process as a linear combination of the direct attenuation term and scattering term. Based on this model, a number of methods have been proposed to remove haze from a single image [7,6,10,12,20,26,27,18,3,2,24,8,14,25]. The key idea of these methods relies on various image priors, e.g., color attenuation [30], dark channel [10], and haze line [3]. Although the effectiveness of these methods has been demonstrated when dealing with daytime haze, there are great limitations when applying these methods to night-time hazy images. The main reason is that the daytime hazy imaging model and priors do not hold for most nighttime hazy scenes. The daytime hazy imaging model assumes the ambient illumination is globally consistent. Therefore, most dehazing methods estimate a white ambient light from the brightest region in the image. However, nighttime scenes usually have multiple colored artificial light sources, e.g. road lamp, neon light and automobile lamp, which result in strongly non-uniform and varicolored ambient illumination. This not only makes the estimation of ambient light inaccu-rate, but also causes some image priors becoming invalid. For example, the dark channel prior assumes that the pixels with the minimal intensity correspond to the black-object in the scene. This prior works very well in daytime, whereas it cannot be directly applied to nighttime scene, since the minimal intensity may be affected by the varicolored ambient illumination. To overcome the above difficulties, some works adopt various new techniques, such as color transfer [23], illumination correction [29], glow removal [17] and image fusion [1], to resolve the issues associated with haze removal from single nighttime image (see Sec. 2).\n\nThe goal of this research is to estimate the ambient illumination and the atmospheric transmission for each pixel so as to recover the haze-free image. To this end, we first introduce a nighttime hazy imaging model presented in [29]. This model is a linear combination of the direct attenuation term and the scattering term both include a local variable accounting for the varicolored and non-uniform ambient illumination. According to this model, recovering the hazefree image is an ill-posed problem.\n\nTo handle the inherent ambiguity, we propose a novel prior for nighttime image haze removal, i.e., maximum reflectance prior. This prior is based on the statistics of daytime haze-free image patches. We find that, in most of the patches, each color channel has very high intensity at some pixels. The proposed prior implies that, the pixels with local maximum intensity at each color channel correspond to the scene points with the maximum reflectance. Accordingly, we call the proposed prior as maximum reflectance prior.\n\nBy using this prior, we can directly estimate a high quality ambient illumination for nighttime hazy image. From this ambient illumination estimate, we can easily calculate the transmission map and finally obtain the nighttime hazefree image. The approach is physically valid and able to handle complicated illumination conditions, including varicolored and non-uniform light. It can achieve high quality dehazing results with few halo artifacts. Since its computational complexity is linear to the number of pixels in the image, the approach is significantly efficient in run-time.\n\n\nPrevious work\n\nA variety of approaches have been proposed to solve haze removal from a single daytime image [7,6,10,20,26,27]. However, these methods are not applicable for nighttime hazy images due to the varicolored and non-uniform illumination in nighttime.\n\nTo the best of our knowledge, there are much fewer literatures about nighttime haze removal in the past decades. Pei and Lee [23] propose a color transfer technique to transform the input nighttime haze image into a grayish one under the guidance of a daytime haze image. Although this method can improve the visibility, it also introduces some color distortion. Zhang et al.'s [29] propose a nighttime de-hazing method including illumination compensation, color correction and dehazing. Since the method involves some additional post processing steps, it tends to introduce color artifacts. Li et al. [17] modify the standard hazy imaging model by adding the atmospheric point spread function to model the glowing effect of active light sources. Based on this model, they apply a layer separation algorithm to decompose the glow from the input image [16]. A spatially varying atmospheric light map is then used to estimate the transmission map based on dark channel prior. Their results contain less halo artifacts than those of [29].\n\nVery recently, C. Ancuti et al. [1] estimate the local airlight by applying a local maximum on patches of dark channel, and then use the multi-scale fusion approach to obtain a visibility enhanced image. While the proposed scheme for ambient illumination estimation may seem similar to [1], they are fundamentally different. They assume that the brightest pixels of local patches filtered by a minimal operator can capture the properties of atmospheric light. These pixels indeed correspond to the hazy regions in the scene. Since the distribution of hazy pixels are spatially varuing, the sizes of the patches are carefully selected in [1] to increase the chance of capturing hazy pixels. On the other hand, we adopt the pixels with maximum reflectance at each color channel to estimate the ambient illumination. These pixels generally correspond to the regions with grey or multiple colors and light sources, which are common in the nighttime scene, as shown in Fig. 2. \n\n\nNighttime hazy imaging model\n\nHere we introduce the nighttime hazy imaging model which takes the influence of varicolored illumination into account. We begin with the introduction of standard daytime haze model. For daytime haze scenes, the model widely used to describe the imaging process is as follows:\nI \u03bb i = J \u03bb i t i + A \u03bb (1 \u2212 t i ) ,(1)\nwhere I \u03bb i is the observed image, J \u03bb i = A \u03bb R \u03bb i is the scene radiance correlated with global atmospheric light A and scene reflection R \u03bb i , and t i = e \u2212\u03b2di is the scene transmission correlated with the scene depth d i and atmospheric scattering coefficient \u03b2.\n\nThe first term J \u03bb i t i on the right side of Eq. (1) is called direct attenuation, and the second term A \u03bb (1 \u2212 t i ) is called airlight [10]. Direct attenuation represents the scene radiance and its attenuation in the atmosphere, while airlight describes the particle veil caused by the scattering of atmospheric light.\n\nAccording to Eq. (1), the global atmospheric light A \u03bb is assumed to be the only light source for daytime haze environment, and the attenuation and scattering characteristics are identical for each channel, i.e., independent from the wavelength. However, as discussed in Section 1, nighttime scenes usually have multiple colored artificial light sources, resulting in a strongly non-uniform and varicolored ambient illumination. Therefore, the local ambient illumination is added into both the attenuation term and scattering term of standard hazy imaging model to obtain the nighttime hazy imaging model as follows:\nI \u03bb i = A \u03bb i R \u03bb i t i + A \u03bb i (1 \u2212 t i ) \u2206 = L i \u03b7 \u03bb i R \u03bb i t i + L i \u03b7 \u03bb i (1 \u2212 t i ) ,(2)\nwhere A \u03bb i \u2206 = L i \u03b7 \u03bb i is the ambient illumination, L i is the intensity of ambient illumination and \u03b7 \u03bb i is the color of ambient illumination. Note that this model is different from the model proposed by Li et al. [17]. Li et al.'s model adds an atmospheric point spread function into the slightly modified standard haze model, and thus can better describe the glowing effect of active light sources. However, their model employs the same scene reflection term with standard daytime hazy imaging model, where the illumination is assumed to be a constant (it is set as 1 in [17]).\n\n\nMaximum reflectance prior\n\nThe proposed prior is based on the statistics of daytime haze-free image patches. We find that, for most image patches, each color channel has very high intensity at some pixels. In other words, the maximum intensities at each color channels in such a patch should have very high values. Mathematically, for an image I, we define:\nM \u03bb \u2126i = max j\u2208\u2126i I \u03bb j = max j\u2208\u2126i L j R \u03bb j ,(3)\nwhere M \u03bb \u2126i is the maximum of pixel intensities in patch \u2126 i at color channel \u03bb, L j is the incident light intensity and R \u03bb j is the reflectance. M is called maximum reflectance map in this paper.\n\nFor daytime bright and clear images, incident light intensities are uniform in space and can be assumed to be fixed to value 1. Therefore, the pixels with local maximum intensity at specific color channel mainly correspond to the objects or surfaces with high reflectance at corresponding color channel. So Eq. (3) has the following equivalent form:\nM \u03bb \u2126i = max j\u2208\u2126i R \u03bb j .\nThe objects or surfaces with maximum reflectance mainly include: a) white (grey) or specularity area, e.g. sky, road surfaces, windows of buildings, and water surfaces; b) any surfaces full of distinct colors, e.g. light sources, flowers, billboards, and crowds. As these objects and surfaces are common in the scenes, for most daytime haze-free image patches, the maximum intensities at each color channel have the value of 1, i.e., M \u03bb \u2126i \u2248 1. Accordingly, we call the above observation the maximum reflectance prior.\n\nTo verify the validity of the proposed prior, we collect a haze-free image set from flickr.com. These images are mainly captured in outdoor landscape and cityscape scenes, where haze usually occurs. They are resized so that the maximum width and height will be 500 pixels. Figure 2(a) and (c) show several outdoor images and the corresponding maximum reflectance maps (Note that the input images are normalized with its V value in HSV color space). Figure  3(a) shows the intensity histogram over all 50,000 patches of maximum reflectance maps. This statistics support our assumption of maximum reflectance prior. As illustrated in Fig 3(b), there are many candidate patches which result in a white MRP. Our proposed prior does not require the maximum intensities of each color channel being contributed by a single white pixel. A typical example is shown in the Fig 3(c). There are quite a few pixels with the maximum reflectance in each color channel. These pixels usually correspond to the objects with grey or distinct colors, e.g. clothes, flowers, forest and road surfaces.\n\nFor nighttime hazy imaging, the ambient illumination is varicolored and non-uniform. Therefore, for nighttime hazy image patches, the maximum intensities at each color channel will have lower value and show with a variety of colors. Visually, these intensities are a rough approximation of the varicolored ambient illumination (see Fig 2(b) and (d)). In this research, we will utilize this property to estimate the ambient illumination. Note that the scene points with maximum reflectance also include the light sources. Therefore, by using this prior, the proposed approach shall be able to handle the glow effect to some extent.\n\nThe proposed prior is partially inspired by the wellknown white patch assumption used in color constancy research. In [13], the effects of bight pixels in several color constancy methods are investigated. We generalize this idea and proposed a novel prior for nighttime image dehazing.\n\n\nNighttime image dehazing\n\nGiven an input image I, the goal of this research is to estimate the ambient illumination and the atmospheric transmission for each pixel so as to recover the haze-free image. We first use the maximum reflectance prior to estimate the color map of ambient illumination and remove its effect from the input image. Then, upon estimating the intensity of varying illumination and the atmospheric transmission, we remove the haze effect and obtain the final color-balanced and haze-free image. The details of the proposed nighttime dehazing process will be explained in following sections.\n\n\nEstimation of ambient illumination\n\nIn nighttime haze environment, lights radiated from many point-like artificial light sources change smoothly in space, except for some occlusions which results sudden changes between bright and shade areas. These boundaries are very sparse in the whole images. On the other hand, haze scatters lights in arbitrary directions. The aggregation of scattered lights lead to smoothly changing light. To this end, we assume the ambient illumination A \u03bb j on each local patch j \u2208 \u2126 i to be constant. Specifically, the intensity L j and color map \u03b7 \u03bb j of the ambient illumination are assumed to be constant, and written as L \u2126i and \u03b7 \u03bb \u2126i . Besides, the transmission map t j is also assumed to be smoothly changing as in [10]. Following [10], we also assumed t j to be a constant on \u2126 i and written as t \u2126i .\n\nBased on the above assumptions, we apply a maxoperator in both side of Eq. (2) on each local patch \u2126 i and obtain the following:\nM \u03bb \u2126i = max j\u2208\u2126i I \u03bb j = max j\u2208\u2126i L \u2126i \u03b7 \u03bb \u2126i R \u03bb j t \u2126i + L \u2126i \u03b7 \u03bb \u2126i (1 \u2212 t \u2126i ) = max j\u2208\u2126i R \u03bb j L \u2126i \u03b7 \u03bb \u2126i t \u2126i + L \u2126i \u03b7 \u03bb \u2126i (1 \u2212 t \u2126i )\n.\n\nFrom the proposed maximum reflectance prior, we have max j\u2208\u2126i R \u03bb j \u2248 1. Substitute into the above equation, we have:\nM \u03bb \u2126i = max j\u2208\u2126i R \u03bb j L \u2126i \u03b7 \u03bb \u2126i t \u2126i + L \u2126i \u03b7 \u03bb \u2126i (1 \u2212 t \u2126i ) = L \u2126i \u03b7 \u03bb \u2126i t \u2126i + L \u2126i \u03b7 \u03bb \u2126i (1 \u2212 t \u2126i ) = L \u2126i \u03b7 \u03bb \u2126i .(5)\nThus, we can estimate the color map of ambient illumination by:\n\u03b7 \u03bb \u2126i = M \u03bb \u2126i L \u2126i .(6)\nHere, the light intensity L \u2126i are fixed to the maximum of M \u03bb \u2126i in all color channels. Equation (6) indeed depicts light intensity normalization and only keeps the color component of ambient illumination.\n\nwe describe the minimization problem as a graph optimization problem, and the matting Laplacian Matrix indeed defines the similarity between neighboring nodes.\n\nAfter obtaining the rough ambient color map \u03b7 \u03bb \u2126i , we refine it by minimizing the following optimization problem:\n\u03b7 \u03bb = \u03b7 \u03bb \u2212 \u03b7 \u03bb \u2126i 2 + \u03b1 \u03b7 \u03bb T \u039b\u03b7 \u03bb .(7)\nHere \u039b is the matting Laplacian matrix [15] which defines the similarity between neighboring pixels [12], and the second term accounts for the smoothness penalty. The optimization problem can be efficiently solved (approximately) by using image guided filter [11].\n\n\nHaze removal\n\nAfter obtaining the estimate of \u03b7 \u03bb , we can remove the color effect from the input image and re-write Eq. (2) as:\nI \u03bb j \u2206 = I \u03bb j \u03b7 \u03bb j =L j R \u03bb j t j + L j (1 \u2212 t j ) .(8)\nFollowing the above assumption on L j and t j on each local patch \u2126 i , we apply a max-operator in both side of Eq. (8), and substitute the maximum reflectance prior once again. Similarly, we have:\nL \u2126i = max j\u2208\u2126i I \u03bb j .(9)\nSince the above equation holds for every color channel, therefore we calculate L \u2126i in each channel and select the maximum one as the final estimate, i.e., L \u2126i = max \u03bb\u2208{R,G,B} max j\u2208\u2126i I \u03bb j . Similar to \u03b7 \u03bb , we refine L \u2126i by image guided filter to obtain a smooth L i .\n\nAfter obtaining L i , we can estimate transmission t i by applying a min-operator to both side of Eq. (8) and introducing dark channel prior. Mathematically, it can be written as: Finally, we recover the haze-free image as:\nmin j\u2208\u2126i min \u03bb\u2208{R,G,B} I \u03bb j = min j\u2208\u2126i min \u03bb\u2208{R,G,B} L j R \u03bb j t \u2126i + L j (1 \u2212 t \u2126i ) \u2206 = min j\u2208\u2126i min \u03bb\u2208{R,G,B} J \u03bb j t \u2126i + L j (1 \u2212 t \u2126i ) = t \u2126i min j\u2208\u2126i min \u03bb\u2208{R,G,B} J j + (1 \u2212 t \u2126i ) min j\u2208\u2126i L j = (1 \u2212 t \u2126i ) min j\u2208\u2126i L j ,(10)J \u03bb j = I \u03bb j \u2212 L j max (t j , t 0 ) + L j ,(11)\nwhere t 0 is a small value for computational stability.\n\n\nA Faster Approximated Estimation Method\n\nHere we propose a faster estimation method of \u03b7 \u03bb i and L i in a simultaneous manner instead of a sequential manner as depicted in Sec. 5.1 and Sec. 5.2. Specifically, after obtaining max reflectance map M \u03bb \u2126i , we refine it directly using image guided filter. And then L i and \u03b7 \u03bb i are calculated as follows:\nL i = max \u03bb\u2208{R,G,B} M \u03bb i ,(12)\u03b7 \u03bb i =M \u03bb i L i .(13)\nEstimation of t i and J \u03bb j is the same process as described in Sec. 5.2. We denote these two methods as MRP and MRP-Faster respectively in this paper.  \n(a) (b) (c) (d) (e) (f) (g) (h)\n\nExperimental results and discussion\n\nTo demonstrate the effectiveness of the proposed approach, we conduct a series of experiments to compare it with several state-of-the-art methods [23,29,17,1]. The performance is evaluated both objectively and subjectively on the same test images used in the existing research. The experiments consist of four parts: estimation of hazy image, dehazing results on real images, dehazing results on synthesized images, computational complexity and run-time. For all the results we use the original codes provided by the authors on their webpages. At the end of this section, we present some additional experimental results including the application on daytime hazy images and some failure cases using our approach. In our experiments, the size of \u2126 is fixed to 15 * 15. The kernel size and smoothing parameter of image guided filter are 32 * 32 and 0.01, respectively. t 0 is set to 0.05.\n\n\nResults on intermediate hazy image estimation\n\nTo show the importance of ambient illumination estimation, we first present the experimental results on the estimation of hazy image. We compare our method with the recent night-time dehazing techniques of Pei and Lee [23], Zhang et al. [29] and Li et al. [17], which all firstly transfer the input image into an intermediate hazy image. Figure 4 shows the contrasting experimental results. As can be seen from and removes it from the input nighttime hazy image. However, their method tends to brighten the hazy image (e.g. the sky region). Generally, the result from the proposed approach shows fewer artifacts and looks more natural.\n\n\nQualitative comparisons on real images\n\nTo verify the performance of the proposed dehazing method, we then present the dehazing results on real nighttime hazy images. The visually inspected results are shown in Figure 5 [17], our MRP method and our MRF-Faster method, respectively. Some close-up views of the dehazing results are shown in even rows. In particular, we also compare the proposed method with Ancuti et al.'s very recent work [1] in Figure 6. Some close-up views of the regions are shown in the 2 nd and 4 th rows.\n\nAs can be seen from these results, Pei and Lee's method cannot handle the color distortion very well. Zhang at al.'s method may generate exaggerated intensity and colors of some areas, due to the additional post processing. Li et al.'s method is prone to over-amplify colors around the strong edges and thus generates color fringing artifacts, especially in the sky region and area surrounding the light sources (e.g., in Fig. 5). Ancuti et al.'s method tends to produce color and blurring artifacts. The proposed approach, however, is able to correct color distortion, enhance the visibility, and introduce only subtle artifacts. In Figure 5, the proposed approach generates crispier images without color fringing artifacts. In Figure 6, the areas around road lamp generated by the proposed approach have better color balance and clearer visibility. So do the areas around the handrail and pedestal seats. Note that the proposed approach does not always completely remove the hazy for the texture regions, but it looks visually pleasing since the chrominance distortion is removed. In addition, the remaining haze in the texture regions is not noticeable due to texture masking. Overall, the proposed algorithm outperforms the state-of-the-art nighttime dehazing algorithms on the real hazy image case.\n\n\nQuantitative comparisons on synthesized images\n\nWe further test the dehazing approach on synthesized hazy images for quantitative comparison. We use images from Middlebury 2005 dataset to synthesize the nighttime hazy images, and quantitatively evaluate the performance of the proposed algorithm. By referring the synthetic method in [27], we calculate the transmission map as t = 0.8d, where d is the normalized disparity map. According to the binocular triangulation measurement principle, we gauge the coordinate of each pixel in the world coordinate system. Then, we assume that the only light source is located at the center part of the scene. We compute the illumination value of a scene point by using a negative exponential form as L = e \u2212\u03b1\u00d7dis , where \u03b1 is a parameter and dis represents the normalized distance between a scene point and the light source. Since dis is very small, we use its Taylor series expansion instead, i.e., L = 1 \u2212 \u03b1 \u00d7 dis. In the following experiment, \u03b1 is set to 0.8. As yellow is a common color in artificial light sources such as road lamp, we set the color of the light source in our synthetic experiment as \u03b7 = (1, 1, 0.3). In addition, the original clear image in the dataset is used as the reflectance R. Finally, we generate the nighttime hazy image according to Eq. (2). An example of quantitative comparison is shown in Fig.  7. Figure 7 \n\n\nRuntime evaluation\n\nMoreover, our approach has the advantage of computational efficiency. The coarse estimations of ambient illumination \u03b7 and transmission map t only consist of some max/min operations on local image patches. As described in Sec. 5.3, we also propose a faster alternative, i.e., MRP-Faster, which applies a non-overlap implementation to calculate the max/min operations with its computational complexity being O(N ). Both in MRF and MRP-Faster approach, the refinements of \u03b7 and t are implemented by using image guided filter, whose computational complexity is O(N ) as well.\n\nWe implement the proposed algorithms using C++ based on OpenCV on a Laptop with Intel CORE i7 and 8G memories. Figure.10 shows the running times for processing im-  [17] needs more than 30 seconds to processes a similar image. Ancuti et al.'s method [1] is computationally efficient, which need 4 seconds. Overall, the proposed MRP-Faster approach is 10-100 times faster than the state-of-the-art methods.\n\nFor an image of VGA size (i.e., 640x480), MRP-Faster processes it in less than 193ms, i.e., more than 5 FPS. Note that the refinement step implemented by image guided filter (four times of image guided filtering steps in total) costs about 147ms, up to 76.28% of total computational cost. Recently, He et al. propose a faster (a speedup of >10x) implementation of image guided filter [9]. Since the proposed method is highly paralleled, it is promising to have a real-time implementation by applying some acceleration techniques as in [28,9]. We leave it as the future work.\n\n\nAdditional experimental results\n\nThe proposed approach even works for the daytime hazy images if there are enough maximum reflectance regions in the image. Figure 9 shows some examples. This approach corrects the color distortion better while generating comparable dehazing results.\n\nFinally, we present some failure examples in Fig. 10. As shown in Fig. 10, there are some color distortions in the regions of grasses and leaves. The main reason is that the maximum reflectance prior does not hold well for these regions. This will be another future work to overcome this problem.\n\n\nDiscussion and conclusion\n\nIn this paper, we have proposed a very simple but effective prior, called maximum reflectance prior, for haze removal from nighttime hazy image. The maximum re- flectance prior is based on the statistics of the outdoor daytime images. By applying the prior, we can easily estimate the ambient illumination, and thus remove haze from nighttime image in a simpler and more effective way. This work shares some common limitations of most schemes based on statistical priors -the prior may not work for some particular images. When the scene objects are inherently with solely distinct color, the maximum reflectance prior becomes invalid. The proposed scheme will generate color distortions for these objects, such as the grasses and leaves in Fig. 10. We intend to investigate more advanced color constancy techniques [19,22] to overcome this problem. In addition, we'll perform more evaluations with more metrics like FADE [5] in our future work.\n\nFigure 1 .\n1(a) Input nighttime hazy image. (b)-(d) Dehazing results of\n\nFigure 2 .Figure 3 .\n23(a) Example images in our daytime haze-free image database. (b) Nighttime haze images. (c) The maximum reflectance maps of (a). (d) The Maximum reflectance maps of (b). Statistics and illustration of maximum reflectance prior. (a) Histograms over 50,000 patches of maximum reflectance maps for RGB and dark channels. (b) Candidate patches which result in a white MRP. (c) A typical example for MRP.\n\n=\nL j R \u03bb j represents the color-balanced hazefree image. After obtaining the raw estimate t \u2126i as 1 j , we refine it using image guided filter.\n\nFigure 4 .\n4Estimation results of hazy image.\n\nFig. 4 ,Figure 5 .\n45the method of Pei and Lee tends to generate a grayish hazy image. Zhang et al.'s method includes an exposure enhancement process and introduces color artifacts into hazy image. Li et al.'s method separates the glow effect The visual comparisons of Pei and Lee's method [23], Zhang et al.'s method [29], Li et al.'s method [17], our MRP method and our MRP-Faster method. The close-up views of the regions enclosed by the red rectangle are shown in the even rows. The estimated ambient illumination maps are shown at the bottom-right corner of each results in the odd rows.\n\n\n(b)-(e), corresponding to the dehazing results using Pei and Lee's method [23], Zhang et al.'s method [29], Li et al.'s method\n\nFigure 6 .\n6The visual comparisons results of Li et al.'s method, Ancuti et al.'s method and our MRF method. The 2 nd -4 th rows show the close-up views of the regions enclosed by the red rectangle in the 1 st row.\n\n\n(a)-(d) show the original image from Middlebury 2005 dataset, disparity map, synthetic ambient illumination, synthetic nighttime hazy image, respectively. Figure 7(e)-(f) present the results of Zhang et al.'s method [29], Li et al.'s method [17] and the proposed MRP and MRP-Faster methods. As shown in Fig. 7, result of Zhang et al.'s method introduces color artifacts, e.g., in face regions of plaster.\n\nFigure 7 .\n7Top row: From left to right: original image from Middlebury 2005 dataset, depth map, synthetic ambient illumination, synthetic nighttime hazy image. Bottom row: Dehazing result of Zhang et al.'s method [29],Li et al.'s method [17], the proposed MRP and MRP-Faster methods. Result of Li et al.'s method has fewer color artifacts, but it shows over-dehazing effect in face regions of plaster and generates darker result. Result of our MRP method is close to Ground truth both in illumination and color. MRP-faster method estimates \u03b7 and L from M simultaneously, whereas MRP method estimates L after removing \u03b7 from the input image. This difference makes MRP-faster method prone to estimate a smaller L, which leads to a brighter result. Table 1 shows the quantitative comparison results of different methods according to PSNR and SSIM. The results demonstrate the superiority of the proposed MRP method, and are consistent with the visual impression in Fig. 5-6.\n\nFigure 8 .\n8Runtime evaluation of the proposed MRF and MRF-Faster approaches. ages of different sizes by using our MRP and MRP-Faster approaches. It can be seen that the running time grows up linearly with respect to the image size. Moreover, MRP-Faster shows remarkable computational efficiency. It processes an 800x600 image in 0.3 seconds, while Zhang et al.'s method [29] and He et al.'s method [10] need approx. 20 seconds, and Li et al.'s method\n\nFigure 9 .Figure 10 .\n910dehazing results on daytime hazy images. (a)(e)Input hazy image. (b)(f) Results using He et al.'s method [10]. (c)(g) Results using MRP method. (d)(h) Results using MRP-Faster method. Failure examples. (a)Input hazy image. (b)Result using Li et al.'s method [17] (c) Result using MRP method. (d) Result using MRP-Faster method.\n\nTable 1 .\n1PSNR and SSIM for the dehazing results on synthetic hazy images.PSNR SSIM \nHazy image 13.89 0.9938 \nZhang et al. 15.99 0.9962 \nLi et al. \n15.74 0.9958 \nMRP \n16.88 0.9966 \nMRP Faster 14.43 0.9950 \n\n\nAcknowledgments. This work is supported by the Natural Science Foundation of China (61472380).\nNight-time dehazing by fusion. C Ancuti, C O Ancuti, C De Vleeschouwer, A C Bovik, Proceedings IEEE International Conference on Image Processing (ICIP). IEEE International Conference on Image Processing (ICIP)C. Ancuti, C. O. Ancuti, C. De Vleeschouwer, and A. C. Bovik. Night-time dehazing by fusion. In Proceedings IEEE International Conference on Image Processing (ICIP), 2016.\n\nBlind dehazing using internal patch recurrence. Y Bahat, M Irani, IEEE International Conference on Computational Photography (ICCP). Y. Bahat and M. Irani. Blind dehazing using internal patch recurrence. In IEEE International Conference on Computa- tional Photography (ICCP), 2016.\n\nNon-local image dehazing. D Berman, T Treibitz, S Avidan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). D. Berman, T. Treibitz, and S. Avidan. Non-local image de- hazing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nDehazenet: An end-to-end system for single image haze removal. B Cai, X Xu, K Jia, C Qing, D Tao, IEEE Transactions on Image Processing. 2511B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet: An end-to-end system for single image haze removal. IEEE Transactions on Image Processing, 25(11):5187-5198, 2016.\n\nReferenceless prediction of perceptual fog density and perceptual image defogging. L K Choi, J You, A C Bovik, IEEE Transactions on Image Processing. 2411L. K. Choi, J. You, and A. C. Bovik. Referenceless prediction of perceptual fog density and perceptual image defogging. IEEE Transactions on Image Processing, 24(11):3888-3901, 2015.\n\nDehazing using color-lines. R , 13:1C13:14ACM Transactions on Graphics. 341R. Fattal. Dehazing using color-lines. ACM Transactions on Graphics, 34(1):13:1C13:14, 2014.\n\nSingle image dehazing. R , 27:72:1C72:9ACM Transactions on Graphics. R. Fattal. Single image dehazing. ACM Transactions on Graphics (SIGGRAPH 2008), 27:72:1C72:9, August 2008.\n\nA fast image dehazing algorithm based on negative correction. Y Gao, H.-M Hu, S Wang, B Li, Signal Processing. 103Y. Gao, H.-M. Hu, S. Wang, and B. Li. A fast image dehazing algorithm based on negative correction. Signal Processing, 103:380 -398, 2014.\n\nFast guided filter. K He, J Sun, ArXiv:1505.00996K. He and J. Sun. Fast guided filter. ArXiv:1505.00996, 2015.\n\nSingle image haze removal using dark channel prior. K He, J Sun, X Tang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. He, J. Sun, and X. Tang. Single image haze removal using dark channel prior. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.\n\nGuided image filtering. K He, J Sun, X Tang, The European Conference on Computer Vision (ECCV). K. He, J. Sun, and X. Tang. Guided image filtering. In The European Conference on Computer Vision (ECCV), 2010.\n\nSingle image haze removal using dark channel prior. K He, J Sun, X Tang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3312K. He, J. Sun, and X. Tang. Single image haze removal using dark channel prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(12):2341 -2353, 2011.\n\nThe role of bright pixels in illumination estimation. H R V Joze, M S Drew, G D Finlayson, P A T Rey, Color and Imaging Conference. H. R. V. Joze, M. S. Drew, G. D. Finlayson, and P. A. T. Rey. The role of bright pixels in illumination estimation. In In Color and Imaging Conference, 2012.\n\nOptimized contrast enhancement for real-time image and video dehazing. J.-H Kim, W.-D Jang, J.-Y. Sim, C.-S Kim, Journal of Visual Communication and Image Representation. 243J.-H. Kim, W.-D. Jang, J.-Y. Sim, and C.-S. Kim. Optimized contrast enhancement for real-time image and video dehaz- ing. Journal of Visual Communication and Image Represen- tation, 24(3):410 -425, 2013.\n\nA closed-form solution to natural image matting. A Levin, D Lischinski, Y Weiss, IEEE Transactions on Pattern Analysis and Machine Intelligence. 302A. Levin, D. Lischinski, and Y. Weiss. A closed-form solu- tion to natural image matting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(2):228 -242, 2008.\n\nSingle image layer separation using relative smoothness. Y Li, M S Brown, IEEE Conference on Computer Vision and Pattern Recognition. Y. Li and M. S. Brown. Single image layer separation us- ing relative smoothness. In IEEE Conference on Computer Vision and Pattern Recognition, 2014.\n\nNighttime haze removal with glow and multiple light colors. Y Li, R T Tan, M S Brown, The IEEE International Conference on Computer Vision (ICCV). Y. Li, R. T. Tan, and M. S. Brown. Nighttime haze removal with glow and multiple light colors. In The IEEE Interna- tional Conference on Computer Vision (ICCV), 2015.\n\nEdge-preserving decomposition-based single image haze removal. Z Li, J Zheng, IEEE Transactions on Image Processing. 2412Z. Li and J. Zheng. Edge-preserving decomposition-based single image haze removal. IEEE Transactions on Image Processing, 24(12):5432-5441, 2015.\n\nReflectance and illumination recovery in the wild. S Lombardi, K Nishino, IEEE Transactions on Pattern Analysis and Machine Intelligence. 381S. Lombardi and K. Nishino. Reflectance and illumination recovery in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(1):129-141, 2016.\n\nEfficient image dehazing with boundary constraint and contextual regularization. G Meng, Y Wang, J Duan, S Xiang, C Pan, The IEEE International Conference on Computer Vision (ICCV). G. Meng, Y.Wang, J. Duan, S. Xiang, and C. Pan. Efficient image dehazing with boundary constraint and contextual reg- ularization. In The IEEE International Conference on Com- puter Vision (ICCV), 2013.\n\nVision through the atmosphere. W E K Middleton, University of Toronto PressW. E. K. Middleton. Vision through the atmosphere. Univer- sity of Toronto Press, 1952.\n\nMultiple illuminant color estimation via statistical inference on factor graphs. L Mutimbu, A Robles-Kelly, IEEE Transactions on Image Processing. 2511L. Mutimbu and A. Robles-Kelly. Multiple illuminan- t color estimation via statistical inference on factor graphs. IEEE Transactions on Image Processing, 25(11):5383-5396, 2016.\n\nNighttime haze removal using color transfer pre-processing and dark channel prior. S C Pei, T Y Lee, Proceedings IEEE International Conference on Image Processing (ICIP). IEEE International Conference on Image Processing (ICIP)S. C. Pei and T. Y. Lee. Nighttime haze removal using col- or transfer pre-processing and dark channel prior. In Pro- ceedings IEEE International Conference on Image Process- ing (ICIP), 2012.\n\nSingle image dehazing in inhomogeneous atmosphere. Z Shi, J Long, W Tang, C Zhang, Optik. 12515Z. Shi, J. Long, W. Tang, and C. Zhang. Single image de- hazing in inhomogeneous atmosphere. Optik, 125(15):3868 -3875, 2014.\n\nWeighted haze removal method with halo prevention. Y.-H Shiau, P.-Y Chen, H.-Y Yang, C.-H Chen, S.-S Wang, Journal of Visual Communication and Image Representation. 252Y.-H. Shiau, P.-Y. Chen, H.-Y. Yang, C.-H. Chen, and S.-S. Wang. Weighted haze removal method with halo prevention. Journal of Visual Communication and Image Representation, 25(2):445 -453, 2014.\n\nVisibility in bad weather from a single image. R Tan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). R. Tan. Visibility in bad weather from a single image. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2008.\n\nInvestigating hazerelevant features in a learning framework for image dehazing. K Tang, J Yang, J Wang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. Tang, J. Yang, and J. Wang. Investigating hazerelevan- t features in a learning framework for image dehazing. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2014.\n\nFast image dehazing using guided joint bilateral filter. The Visual Computer. C Xiao, J Gan, 28C. Xiao and J. Gan. Fast image dehazing using guided joint bilateral filter. The Visual Computer, 28(6-8):713-721, 2012.\n\nNighttime haze removal based on a new imaging model. J Zhang, Y Cao, Z Wang, Proceedings IEEE International Conference on Image Processing (ICIP). IEEE International Conference on Image Processing (ICIP)J. Zhang, Y. Cao, and Z. Wang. Nighttime haze removal based on a new imaging model. In Proceedings IEEE Inter- national Conference on Image Processing (ICIP), 2014.\n\nA fast single image haze removal algorithm using color attenuation prior. Q Zhu, J Mai, L Shao, IEEE Transactions on Image Processing. 2411Q. Zhu, J. Mai, and L. Shao. A fast single image haze re- moval algorithm using color attenuation prior. IEEE Trans- actions on Image Processing, 24(11):3522-3533, 2015.\n", "annotations": {"author": "[{\"end\":84,\"start\":73},{\"end\":94,\"start\":85},{\"end\":128,\"start\":95},{\"end\":158,\"start\":129},{\"end\":193,\"start\":159},{\"end\":253,\"start\":194},{\"end\":298,\"start\":254},{\"end\":358,\"start\":299},{\"end\":414,\"start\":359}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":78},{\"end\":93,\"start\":90},{\"end\":105,\"start\":101},{\"end\":136,\"start\":132},{\"end\":173,\"start\":169}]", "author_first_name": "[{\"end\":77,\"start\":73},{\"end\":89,\"start\":85},{\"end\":100,\"start\":95},{\"end\":131,\"start\":129},{\"end\":164,\"start\":159},{\"end\":168,\"start\":165}]", "author_affiliation": "[{\"end\":252,\"start\":195},{\"end\":297,\"start\":255},{\"end\":357,\"start\":300},{\"end\":413,\"start\":360}]", "title": "[{\"end\":70,\"start\":1},{\"end\":484,\"start\":415}]", "venue": null, "abstract": "[{\"end\":1650,\"start\":486}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2149,\"start\":2145},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2154,\"start\":2151},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2222,\"start\":2218},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2230,\"start\":2227},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2434,\"start\":2430},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2644,\"start\":2641},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2646,\"start\":2644},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2649,\"start\":2646},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2652,\"start\":2649},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2655,\"start\":2652},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2658,\"start\":2655},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2661,\"start\":2658},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2664,\"start\":2661},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2666,\"start\":2664},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2668,\"start\":2666},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2671,\"start\":2668},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2673,\"start\":2671},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2676,\"start\":2673},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2679,\"start\":2676},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2771,\"start\":2767},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2790,\"start\":2786},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2809,\"start\":2806},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4034,\"start\":4030},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4064,\"start\":4060},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4083,\"start\":4079},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4104,\"start\":4101},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4432,\"start\":4428},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5924,\"start\":5921},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5926,\"start\":5924},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5929,\"start\":5926},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5932,\"start\":5929},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5935,\"start\":5932},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5938,\"start\":5935},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6204,\"start\":6200},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6457,\"start\":6453},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6681,\"start\":6677},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6930,\"start\":6926},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7109,\"start\":7105},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7147,\"start\":7144},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7401,\"start\":7398},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7752,\"start\":7749},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8844,\"start\":8840},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9960,\"start\":9956},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10319,\"start\":10315},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13664,\"start\":13660},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15198,\"start\":15194},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15214,\"start\":15210},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16467,\"start\":16463},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16528,\"start\":16524},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16687,\"start\":16683},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18727,\"start\":18723},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18730,\"start\":18727},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18733,\"start\":18730},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18735,\"start\":18733},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19734,\"start\":19730},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19753,\"start\":19749},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19772,\"start\":19768},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20374,\"start\":20370},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20592,\"start\":20589},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22323,\"start\":22319},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24133,\"start\":24129},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24217,\"start\":24214},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24758,\"start\":24755},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24910,\"start\":24906},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24912,\"start\":24910},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26378,\"start\":26374},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26381,\"start\":26378},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26483,\"start\":26480}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26576,\"start\":26504},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26999,\"start\":26577},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27145,\"start\":27000},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27192,\"start\":27146},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27786,\"start\":27193},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27915,\"start\":27787},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28131,\"start\":27916},{\"attributes\":{\"id\":\"fig_7\"},\"end\":28538,\"start\":28132},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29512,\"start\":28539},{\"attributes\":{\"id\":\"fig_9\"},\"end\":29965,\"start\":29513},{\"attributes\":{\"id\":\"fig_10\"},\"end\":30319,\"start\":29966},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30529,\"start\":30320}]", "paragraph": "[{\"end\":2303,\"start\":1666},{\"end\":4198,\"start\":2305},{\"end\":4702,\"start\":4200},{\"end\":5226,\"start\":4704},{\"end\":5810,\"start\":5228},{\"end\":6073,\"start\":5828},{\"end\":7110,\"start\":6075},{\"end\":8084,\"start\":7112},{\"end\":8392,\"start\":8117},{\"end\":8700,\"start\":8433},{\"end\":9023,\"start\":8702},{\"end\":9641,\"start\":9025},{\"end\":10321,\"start\":9737},{\"end\":10681,\"start\":10351},{\"end\":10930,\"start\":10732},{\"end\":11281,\"start\":10932},{\"end\":11827,\"start\":11308},{\"end\":12908,\"start\":11829},{\"end\":13540,\"start\":12910},{\"end\":13827,\"start\":13542},{\"end\":14441,\"start\":13856},{\"end\":15281,\"start\":14480},{\"end\":15411,\"start\":15283},{\"end\":15557,\"start\":15556},{\"end\":15676,\"start\":15559},{\"end\":15871,\"start\":15808},{\"end\":16104,\"start\":15898},{\"end\":16265,\"start\":16106},{\"end\":16382,\"start\":16267},{\"end\":16688,\"start\":16424},{\"end\":16819,\"start\":16705},{\"end\":17076,\"start\":16879},{\"end\":17377,\"start\":17104},{\"end\":17602,\"start\":17379},{\"end\":17943,\"start\":17888},{\"end\":18298,\"start\":17987},{\"end\":18506,\"start\":18353},{\"end\":19462,\"start\":18577},{\"end\":20147,\"start\":19512},{\"end\":20677,\"start\":20190},{\"end\":21982,\"start\":20679},{\"end\":23367,\"start\":22033},{\"end\":23962,\"start\":23390},{\"end\":24369,\"start\":23964},{\"end\":24945,\"start\":24371},{\"end\":25230,\"start\":24981},{\"end\":25528,\"start\":25232},{\"end\":26503,\"start\":25558}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8432,\"start\":8393},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9736,\"start\":9642},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10731,\"start\":10682},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11307,\"start\":11282},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15555,\"start\":15412},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15807,\"start\":15677},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15897,\"start\":15872},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16423,\"start\":16383},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16878,\"start\":16820},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17103,\"start\":17077},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17839,\"start\":17603},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17887,\"start\":17839},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18330,\"start\":18299},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18352,\"start\":18330},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18538,\"start\":18507}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1664,\"start\":1652},{\"attributes\":{\"n\":\"2.\"},\"end\":5826,\"start\":5813},{\"attributes\":{\"n\":\"3.\"},\"end\":8115,\"start\":8087},{\"attributes\":{\"n\":\"4.\"},\"end\":10349,\"start\":10324},{\"attributes\":{\"n\":\"5.\"},\"end\":13854,\"start\":13830},{\"attributes\":{\"n\":\"5.1.\"},\"end\":14478,\"start\":14444},{\"attributes\":{\"n\":\"5.2.\"},\"end\":16703,\"start\":16691},{\"attributes\":{\"n\":\"5.3.\"},\"end\":17985,\"start\":17946},{\"attributes\":{\"n\":\"6.\"},\"end\":18575,\"start\":18540},{\"attributes\":{\"n\":\"6.1.\"},\"end\":19510,\"start\":19465},{\"attributes\":{\"n\":\"6.2.\"},\"end\":20188,\"start\":20150},{\"attributes\":{\"n\":\"6.3.\"},\"end\":22031,\"start\":21985},{\"attributes\":{\"n\":\"6.4.\"},\"end\":23388,\"start\":23370},{\"attributes\":{\"n\":\"6.5.\"},\"end\":24979,\"start\":24948},{\"attributes\":{\"n\":\"7.\"},\"end\":25556,\"start\":25531},{\"end\":26515,\"start\":26505},{\"end\":26598,\"start\":26578},{\"end\":27002,\"start\":27001},{\"end\":27157,\"start\":27147},{\"end\":27212,\"start\":27194},{\"end\":27927,\"start\":27917},{\"end\":28550,\"start\":28540},{\"end\":29524,\"start\":29514},{\"end\":29988,\"start\":29967},{\"end\":30330,\"start\":30321}]", "table": "[{\"end\":30529,\"start\":30396}]", "figure_caption": "[{\"end\":26576,\"start\":26517},{\"end\":26999,\"start\":26601},{\"end\":27145,\"start\":27003},{\"end\":27192,\"start\":27159},{\"end\":27786,\"start\":27215},{\"end\":27915,\"start\":27789},{\"end\":28131,\"start\":27929},{\"end\":28538,\"start\":28134},{\"end\":29512,\"start\":28552},{\"end\":29965,\"start\":29526},{\"end\":30319,\"start\":29992},{\"end\":30396,\"start\":30332}]", "figure_ref": "[{\"end\":8083,\"start\":8076},{\"end\":12110,\"start\":12102},{\"end\":12287,\"start\":12278},{\"end\":12469,\"start\":12461},{\"end\":12700,\"start\":12692},{\"end\":13250,\"start\":13242},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19858,\"start\":19850},{\"end\":20369,\"start\":20361},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20604,\"start\":20596},{\"end\":21107,\"start\":21101},{\"end\":21321,\"start\":21313},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21416,\"start\":21408},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":23356,\"start\":23349},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":23366,\"start\":23358},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24084,\"start\":24075},{\"end\":25112,\"start\":25104},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25284,\"start\":25277},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25305,\"start\":25298},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26306,\"start\":26299}]", "bib_author_first_name": "[{\"end\":30657,\"start\":30656},{\"end\":30667,\"start\":30666},{\"end\":30669,\"start\":30668},{\"end\":30679,\"start\":30678},{\"end\":30682,\"start\":30680},{\"end\":30698,\"start\":30697},{\"end\":30700,\"start\":30699},{\"end\":31056,\"start\":31055},{\"end\":31065,\"start\":31064},{\"end\":31317,\"start\":31316},{\"end\":31327,\"start\":31326},{\"end\":31339,\"start\":31338},{\"end\":31623,\"start\":31622},{\"end\":31630,\"start\":31629},{\"end\":31636,\"start\":31635},{\"end\":31643,\"start\":31642},{\"end\":31651,\"start\":31650},{\"end\":31955,\"start\":31954},{\"end\":31957,\"start\":31956},{\"end\":31965,\"start\":31964},{\"end\":31972,\"start\":31971},{\"end\":31974,\"start\":31973},{\"end\":32238,\"start\":32237},{\"end\":32402,\"start\":32401},{\"end\":32618,\"start\":32617},{\"end\":32628,\"start\":32624},{\"end\":32634,\"start\":32633},{\"end\":32642,\"start\":32641},{\"end\":32830,\"start\":32829},{\"end\":32836,\"start\":32835},{\"end\":32974,\"start\":32973},{\"end\":32980,\"start\":32979},{\"end\":32987,\"start\":32986},{\"end\":33243,\"start\":33242},{\"end\":33249,\"start\":33248},{\"end\":33256,\"start\":33255},{\"end\":33480,\"start\":33479},{\"end\":33486,\"start\":33485},{\"end\":33493,\"start\":33492},{\"end\":33793,\"start\":33792},{\"end\":33797,\"start\":33794},{\"end\":33805,\"start\":33804},{\"end\":33807,\"start\":33806},{\"end\":33815,\"start\":33814},{\"end\":33817,\"start\":33816},{\"end\":33830,\"start\":33829},{\"end\":33834,\"start\":33831},{\"end\":34104,\"start\":34100},{\"end\":34114,\"start\":34110},{\"end\":34126,\"start\":34121},{\"end\":34136,\"start\":34132},{\"end\":34458,\"start\":34457},{\"end\":34467,\"start\":34466},{\"end\":34481,\"start\":34480},{\"end\":34791,\"start\":34790},{\"end\":34797,\"start\":34796},{\"end\":34799,\"start\":34798},{\"end\":35080,\"start\":35079},{\"end\":35086,\"start\":35085},{\"end\":35088,\"start\":35087},{\"end\":35095,\"start\":35094},{\"end\":35097,\"start\":35096},{\"end\":35398,\"start\":35397},{\"end\":35404,\"start\":35403},{\"end\":35654,\"start\":35653},{\"end\":35666,\"start\":35665},{\"end\":35990,\"start\":35989},{\"end\":35998,\"start\":35997},{\"end\":36006,\"start\":36005},{\"end\":36014,\"start\":36013},{\"end\":36023,\"start\":36022},{\"end\":36326,\"start\":36325},{\"end\":36330,\"start\":36327},{\"end\":36540,\"start\":36539},{\"end\":36551,\"start\":36550},{\"end\":36872,\"start\":36871},{\"end\":36874,\"start\":36873},{\"end\":36881,\"start\":36880},{\"end\":36883,\"start\":36882},{\"end\":37261,\"start\":37260},{\"end\":37268,\"start\":37267},{\"end\":37276,\"start\":37275},{\"end\":37284,\"start\":37283},{\"end\":37486,\"start\":37482},{\"end\":37498,\"start\":37494},{\"end\":37509,\"start\":37505},{\"end\":37520,\"start\":37516},{\"end\":37531,\"start\":37527},{\"end\":37844,\"start\":37843},{\"end\":38132,\"start\":38131},{\"end\":38140,\"start\":38139},{\"end\":38148,\"start\":38147},{\"end\":38493,\"start\":38492},{\"end\":38501,\"start\":38500},{\"end\":38685,\"start\":38684},{\"end\":38694,\"start\":38693},{\"end\":38701,\"start\":38700},{\"end\":39075,\"start\":39074},{\"end\":39082,\"start\":39081},{\"end\":39089,\"start\":39088}]", "bib_author_last_name": "[{\"end\":30664,\"start\":30658},{\"end\":30676,\"start\":30670},{\"end\":30695,\"start\":30683},{\"end\":30706,\"start\":30701},{\"end\":31062,\"start\":31057},{\"end\":31071,\"start\":31066},{\"end\":31324,\"start\":31318},{\"end\":31336,\"start\":31328},{\"end\":31346,\"start\":31340},{\"end\":31627,\"start\":31624},{\"end\":31633,\"start\":31631},{\"end\":31640,\"start\":31637},{\"end\":31648,\"start\":31644},{\"end\":31655,\"start\":31652},{\"end\":31962,\"start\":31958},{\"end\":31969,\"start\":31966},{\"end\":31980,\"start\":31975},{\"end\":32622,\"start\":32619},{\"end\":32631,\"start\":32629},{\"end\":32639,\"start\":32635},{\"end\":32645,\"start\":32643},{\"end\":32833,\"start\":32831},{\"end\":32840,\"start\":32837},{\"end\":32977,\"start\":32975},{\"end\":32984,\"start\":32981},{\"end\":32992,\"start\":32988},{\"end\":33246,\"start\":33244},{\"end\":33253,\"start\":33250},{\"end\":33261,\"start\":33257},{\"end\":33483,\"start\":33481},{\"end\":33490,\"start\":33487},{\"end\":33498,\"start\":33494},{\"end\":33802,\"start\":33798},{\"end\":33812,\"start\":33808},{\"end\":33827,\"start\":33818},{\"end\":33838,\"start\":33835},{\"end\":34108,\"start\":34105},{\"end\":34119,\"start\":34115},{\"end\":34130,\"start\":34127},{\"end\":34140,\"start\":34137},{\"end\":34464,\"start\":34459},{\"end\":34478,\"start\":34468},{\"end\":34487,\"start\":34482},{\"end\":34794,\"start\":34792},{\"end\":34805,\"start\":34800},{\"end\":35083,\"start\":35081},{\"end\":35092,\"start\":35089},{\"end\":35103,\"start\":35098},{\"end\":35401,\"start\":35399},{\"end\":35410,\"start\":35405},{\"end\":35663,\"start\":35655},{\"end\":35674,\"start\":35667},{\"end\":35995,\"start\":35991},{\"end\":36003,\"start\":35999},{\"end\":36011,\"start\":36007},{\"end\":36020,\"start\":36015},{\"end\":36027,\"start\":36024},{\"end\":36340,\"start\":36331},{\"end\":36548,\"start\":36541},{\"end\":36564,\"start\":36552},{\"end\":36878,\"start\":36875},{\"end\":36887,\"start\":36884},{\"end\":37265,\"start\":37262},{\"end\":37273,\"start\":37269},{\"end\":37281,\"start\":37277},{\"end\":37290,\"start\":37285},{\"end\":37492,\"start\":37487},{\"end\":37503,\"start\":37499},{\"end\":37514,\"start\":37510},{\"end\":37525,\"start\":37521},{\"end\":37536,\"start\":37532},{\"end\":37848,\"start\":37845},{\"end\":38137,\"start\":38133},{\"end\":38145,\"start\":38141},{\"end\":38153,\"start\":38149},{\"end\":38498,\"start\":38494},{\"end\":38505,\"start\":38502},{\"end\":38691,\"start\":38686},{\"end\":38698,\"start\":38695},{\"end\":38706,\"start\":38702},{\"end\":39079,\"start\":39076},{\"end\":39086,\"start\":39083},{\"end\":39094,\"start\":39090}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14453359},\"end\":31005,\"start\":30625},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1304307},\"end\":31288,\"start\":31007},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":18774783},\"end\":31557,\"start\":31290},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14092238},\"end\":31869,\"start\":31559},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10225720},\"end\":32207,\"start\":31871},{\"attributes\":{\"doi\":\"13:1C13:14\",\"id\":\"b5\",\"matched_paper_id\":207217221},\"end\":32376,\"start\":32209},{\"attributes\":{\"doi\":\"27:72:1C72:9\",\"id\":\"b6\",\"matched_paper_id\":5904964},\"end\":32553,\"start\":32378},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":22054037},\"end\":32807,\"start\":32555},{\"attributes\":{\"doi\":\"ArXiv:1505.00996\",\"id\":\"b8\"},\"end\":32919,\"start\":32809},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":49333383},\"end\":33216,\"start\":32921},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1264129},\"end\":33425,\"start\":33218},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49333383},\"end\":33736,\"start\":33427},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":605846},\"end\":34027,\"start\":33738},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3006580},\"end\":34406,\"start\":34029},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":233493},\"end\":34731,\"start\":34408},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16165913},\"end\":35017,\"start\":34733},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17319211},\"end\":35332,\"start\":35019},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11297182},\"end\":35600,\"start\":35334},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":17415797},\"end\":35906,\"start\":35602},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6080741},\"end\":36292,\"start\":35908},{\"attributes\":{\"id\":\"b20\"},\"end\":36456,\"start\":36294},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12917245},\"end\":36786,\"start\":36458},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10071664},\"end\":37207,\"start\":36788},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14306931},\"end\":37429,\"start\":37209},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":30426246},\"end\":37794,\"start\":37431},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2700315},\"end\":38049,\"start\":37796},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14684722},\"end\":38412,\"start\":38051},{\"attributes\":{\"id\":\"b27\"},\"end\":38629,\"start\":38414},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":15527362},\"end\":38998,\"start\":38631},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4252508},\"end\":39308,\"start\":39000}]", "bib_title": "[{\"end\":30654,\"start\":30625},{\"end\":31053,\"start\":31007},{\"end\":31314,\"start\":31290},{\"end\":31620,\"start\":31559},{\"end\":31952,\"start\":31871},{\"end\":32235,\"start\":32209},{\"end\":32399,\"start\":32378},{\"end\":32615,\"start\":32555},{\"end\":32971,\"start\":32921},{\"end\":33240,\"start\":33218},{\"end\":33477,\"start\":33427},{\"end\":33790,\"start\":33738},{\"end\":34098,\"start\":34029},{\"end\":34455,\"start\":34408},{\"end\":34788,\"start\":34733},{\"end\":35077,\"start\":35019},{\"end\":35395,\"start\":35334},{\"end\":35651,\"start\":35602},{\"end\":35987,\"start\":35908},{\"end\":36537,\"start\":36458},{\"end\":36869,\"start\":36788},{\"end\":37258,\"start\":37209},{\"end\":37480,\"start\":37431},{\"end\":37841,\"start\":37796},{\"end\":38129,\"start\":38051},{\"end\":38682,\"start\":38631},{\"end\":39072,\"start\":39000}]", "bib_author": "[{\"end\":30666,\"start\":30656},{\"end\":30678,\"start\":30666},{\"end\":30697,\"start\":30678},{\"end\":30708,\"start\":30697},{\"end\":31064,\"start\":31055},{\"end\":31073,\"start\":31064},{\"end\":31326,\"start\":31316},{\"end\":31338,\"start\":31326},{\"end\":31348,\"start\":31338},{\"end\":31629,\"start\":31622},{\"end\":31635,\"start\":31629},{\"end\":31642,\"start\":31635},{\"end\":31650,\"start\":31642},{\"end\":31657,\"start\":31650},{\"end\":31964,\"start\":31954},{\"end\":31971,\"start\":31964},{\"end\":31982,\"start\":31971},{\"end\":32241,\"start\":32237},{\"end\":32405,\"start\":32401},{\"end\":32624,\"start\":32617},{\"end\":32633,\"start\":32624},{\"end\":32641,\"start\":32633},{\"end\":32647,\"start\":32641},{\"end\":32835,\"start\":32829},{\"end\":32842,\"start\":32835},{\"end\":32979,\"start\":32973},{\"end\":32986,\"start\":32979},{\"end\":32994,\"start\":32986},{\"end\":33248,\"start\":33242},{\"end\":33255,\"start\":33248},{\"end\":33263,\"start\":33255},{\"end\":33485,\"start\":33479},{\"end\":33492,\"start\":33485},{\"end\":33500,\"start\":33492},{\"end\":33804,\"start\":33792},{\"end\":33814,\"start\":33804},{\"end\":33829,\"start\":33814},{\"end\":33840,\"start\":33829},{\"end\":34110,\"start\":34100},{\"end\":34121,\"start\":34110},{\"end\":34132,\"start\":34121},{\"end\":34142,\"start\":34132},{\"end\":34466,\"start\":34457},{\"end\":34480,\"start\":34466},{\"end\":34489,\"start\":34480},{\"end\":34796,\"start\":34790},{\"end\":34807,\"start\":34796},{\"end\":35085,\"start\":35079},{\"end\":35094,\"start\":35085},{\"end\":35105,\"start\":35094},{\"end\":35403,\"start\":35397},{\"end\":35412,\"start\":35403},{\"end\":35665,\"start\":35653},{\"end\":35676,\"start\":35665},{\"end\":35997,\"start\":35989},{\"end\":36005,\"start\":35997},{\"end\":36013,\"start\":36005},{\"end\":36022,\"start\":36013},{\"end\":36029,\"start\":36022},{\"end\":36342,\"start\":36325},{\"end\":36550,\"start\":36539},{\"end\":36566,\"start\":36550},{\"end\":36880,\"start\":36871},{\"end\":36889,\"start\":36880},{\"end\":37267,\"start\":37260},{\"end\":37275,\"start\":37267},{\"end\":37283,\"start\":37275},{\"end\":37292,\"start\":37283},{\"end\":37494,\"start\":37482},{\"end\":37505,\"start\":37494},{\"end\":37516,\"start\":37505},{\"end\":37527,\"start\":37516},{\"end\":37538,\"start\":37527},{\"end\":37850,\"start\":37843},{\"end\":38139,\"start\":38131},{\"end\":38147,\"start\":38139},{\"end\":38155,\"start\":38147},{\"end\":38500,\"start\":38492},{\"end\":38507,\"start\":38500},{\"end\":38693,\"start\":38684},{\"end\":38700,\"start\":38693},{\"end\":38708,\"start\":38700},{\"end\":39081,\"start\":39074},{\"end\":39088,\"start\":39081},{\"end\":39096,\"start\":39088}]", "bib_venue": "[{\"end\":30776,\"start\":30708},{\"end\":31138,\"start\":31073},{\"end\":31413,\"start\":31348},{\"end\":31694,\"start\":31657},{\"end\":32019,\"start\":31982},{\"end\":32279,\"start\":32251},{\"end\":32445,\"start\":32417},{\"end\":32664,\"start\":32647},{\"end\":32827,\"start\":32809},{\"end\":33059,\"start\":32994},{\"end\":33312,\"start\":33263},{\"end\":33562,\"start\":33500},{\"end\":33868,\"start\":33840},{\"end\":34198,\"start\":34142},{\"end\":34551,\"start\":34489},{\"end\":34865,\"start\":34807},{\"end\":35164,\"start\":35105},{\"end\":35449,\"start\":35412},{\"end\":35738,\"start\":35676},{\"end\":36088,\"start\":36029},{\"end\":36323,\"start\":36294},{\"end\":36603,\"start\":36566},{\"end\":36957,\"start\":36889},{\"end\":37297,\"start\":37292},{\"end\":37594,\"start\":37538},{\"end\":37915,\"start\":37850},{\"end\":38220,\"start\":38155},{\"end\":38490,\"start\":38414},{\"end\":38776,\"start\":38708},{\"end\":39133,\"start\":39096},{\"end\":30834,\"start\":30778},{\"end\":37015,\"start\":36959},{\"end\":38834,\"start\":38778}]"}}}, "year": 2023, "month": 12, "day": 17}