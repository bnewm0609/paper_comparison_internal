{"id": 3659467, "updated": "2023-09-29 07:42:56.139", "metadata": {"title": "Provable defenses against adversarial examples via the convex outer adversarial polytope", "authors": "[{\"first\":\"J.\",\"last\":\"Kolter\",\"middle\":[\"Zico\"]},{\"first\":\"Eric\",\"last\":\"Wong\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 11, "day": 2}, "abstract": "We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations (on the training data; for previously unseen examples, the approach will be guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well). The basic idea of the approach is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a toy 2D robust classification task, and on a simple convolutional architecture applied to MNIST, where we produce a classifier that provably has less than 8.4% test error for any adversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$. This represents the largest verified network that we are aware of, and we discuss future challenges in scaling the approach to much larger domains.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1711.00851", "mag": "2963496101", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/WongK18", "doi": null}}, "content": {"source": {"pdf_hash": "541c238a830698dc9a50aac220fe130bdb550752", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1711.00851v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "66cc9571620316a5a494647c7471dde30fb944a1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/541c238a830698dc9a50aac220fe130bdb550752.txt", "contents": "\nProvable defenses against adversarial examples via the convex outer adversarial polytope\nNovember 3, 2017\n\nJ Zico Kolter zkolter@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University Pittsburgh PA\n15213USA\n\nEric Wong ericwong@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University Pittsburgh PA\n15213USA\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope\nNovember 3, 2017\nWe propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations (on the training data; for previously unseen examples, the approach will be guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well). The basic idea of the approach is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a toy 2D robust classification task, and on a simple convolutional architecture applied to MNIST, where we produce a classifier that provably has less than 8.4% test error for any adversarial attack with bounded \u221e norm less than = 0.1. This represents the largest verified network that we are aware of, and we discuss future challenges in scaling the approach to much larger domains.\n\nIntroduction\n\nRecent work in deep learning has demonstrated the prevalence of adversarial examples [Goodfellow et al., 2015], data points fed to a machine learning algorithm which are visually indistinguishable from \"normal\" examples, but which are specifically tuned so as to fool or somehow mislead the machine learning system. Recent history in adversarial classification has followed something of a virtual \"arms race\": practitioners alternatively design new ways of \"hardening\" classifiers against existing attacks, and then a new class of attacks is developed that can penetrate this defense. Distillation [Papernot et al., 2016] is effective at preventing adversarial examples until it is not [Carlini and Wagner, 2017]. There is no need to worry about adversarial examples under \"realistic\" settings of rotation and scaling [Lu et al., 2017] until there is [Athalye and Sutskever, 2017]. Neither does the fact that the adversary lacks full knowledge of the model appear to be a problem: \"blackbox\" attacks are also extremely effective [Papernot et al., 2017]. Given the potentially high-stakes nature of many machine learning systems, we feel this situation is untenable: the \"cost\" of having a classifier be fooled just once is potentially extremely high, and so the attackers are the de-facto \"winners\" of this current game.\n\nRather, we feel that the only method to truly harden classifiers against adversarial attacks is to design classifiers that are guaranteed to be robust to adversarial perturbations, even if the attacker is given full knowledge of the classifier. Anything less amounts to an attempt of \"security through obscurity\", and will ultimately prove unable to provide a robust classifier. However, designing classifiers that are truly robust to adversarial examples is an extremely challenging task (and potentially even impossible, in many scenarios). However, unlike the current trend of defense mechanisms, which seems to rest largely upon ad-hoc strategies or particular forms of attack, we think that preventing against any attack is a crucial property for future machine learning systems.\n\nIn this proposed work, we aim to take a step in this direction by considering more formally the actual polyhedral geometry of deep networks. We show (similar to previous work, which we will mention shortly), that we can characterize the set of all adversarial examples for a ReLU-based deep network through a (highly non-convex) polytope characterized by an integer program. However, integer programming approaches do not scale to anywhere near the complexity of modern deep learning architectures, and so instead we focus our attention to outer bounds of this \"adversarial polytope\": convex outer approximations to the adversarial region which are possible to define and reason about efficiently. Because these convex approximations are strictly outer approximations (i.e., they contain the true adversarial polytope), if we are able to design classifiers with the desired properties over these convex approximations, these same properties are guaranteed to hold over the true adversarial region. Of course, \"efficiently\" in the language of convex optimization still may mean polynomial in the number of hidden units of network, a situation which is not practically efficient in the deep learning setting.\n\nThus, the main technical thrust of this work is to develop efficient methods that can optimize over these convex outer approximations while having cost that is \"similar\" (at least to a couple orders of magnitude) to that of existing deep learning algorithms. The key idea is to consider the dual problem of optimizing over the convex outer adversarial polytope, which provides a guaranteed lower bound on the optimization problem of interest. We show that the dual problem can in fact be characterized exactly via a \"dual network\" similar to the backpropagation network of the original classifier, but with additional free parameters; because the dual network provides a guaranteed bound on the original convex problem for any feasible solution, even if we don't solve the dual problem exactly, it still provides us with a provable bound. Thus, although our approach is based upon the theory of linear programming and convex optimization, the resulting algorithm just requires passes through an ordinary deep network. The end result is that with a few additional forward and backward passes through the network (though potentially with much larger batch sizes) we can train a classifier that is guaranteed to be robust against norm-bounded adversarial perturbations.\n\nWe illustrate the proposed method on a few simple problems: a toy 2D data set used to illustrate the basic concepts, and a ConvNet classifier applied to MNIST. In the latter case, we show that we can learn a classifier that provably attains 91% test accuracy against all \u221e -bounded adversarial attacks with radius = 0.1. Although these are small-scale problems, and the method is admittedly much more costly than traditional learning (though not combinatorially so), to the best of our knowledge this represents the best provable accuracy against robust attacks for an MNIST classifier. We discuss the challenges and strategies for scaling these techniques to larger settings. All code for the examples included in the paper is available at http://github.com/locuslab/ convex_adversarial.\n\n\nRelated work\n\nOur work relates most closely to two recent trends in adversarial examples. The first (which is a topic that is broader than merely adversarial examples), is the work in the verification of neural networks [Huang et al., 2017, Ehlers, 2017. There are several different approaches to this general task, but the general idea behind this line of work is to formally verify some property of the network, such as robustness to an adversarial perturbation, or that certain bounded inputs will lead to bounded outputs. The methods for solving these verification problems also differ, but one of the primary tools commonly used is Satisfiability Modulo Theories (SMT) solvers. These essentially break the non-linear nature of the ReLU activation into a logical clauses, and then use satisfiability-type solvers to efficiently reason about which clauses may be true or not, and which imply others.  recently developed the Reluplex solver for efficiently solving relatively large-scale problems of this type. The PLANET solver [Ehlers, 2017] uses a similar high level idea, but also combines a satisfiability solver with a convex approximation of the ReLU nonlinearities (at the level of a single ReLU unit, it is the same outer bound as used in this work, though the nature of our full outer bound is different). Similar techniques, though in this case based upon mixed integer programming, have been used to analyze the reachability of inputs [Lomuscio and Maganti, 2017]. SMT methods have also been used to find \"ground-truth\" adversarial examples , the best possible adversarial example within a bounded region, or the smallest possible region.\n\nThe present work takes a great deal of inspiration from these formal verification methods, and the end goal is largely the same: to develop networks that are guaranteed to have certain properties. However, as evidenced by this past work, arbitrary deep networks are extremely difficult to verify (indeed, as evidenced by the fact that naively-trained networks typically are susceptible to adversarial examples). Instead, this work takes an alternative approach: learning deep classifiers that by their nature are (relatively) easy to verify, via convex instead of combinatorial problems.\n\nSecond, our work relates to the general theme of integrating robust optimization into deep learning. Robust optimization [Ben-Tal et al., 2009] is a very well-studied discipline in convex optimization, and generally deals with optimization problems where the objectives or constraints are not fully known, but only known to lie within some uncertainty set. Of particular relevance to this work is the general technique of using duality theory to replace min-max objectives or constraints with a single minimization; we will also use this approach to develop an efficient solution method for our robust optimization problem. Indeed, robust optimization techniques have been used in the context of linear machine learning models [Xu et al., 2009] to create classifiers that are robust to perturbations of the input. And this connection was addressed in the original adversarial examples paper Goodfellow et al. [2015], where it was noted (the fact is well-known in robust optimization, and we merely mean that the original paper pointed out this connection) that for linear models, robustness to adversarial examples can be achieved via an 1 norm penalty on the weights within the loss function. However, as also noted by the paper, since deep networks are decidedly non-convex, much of the theory of robust optimization is inapplicable in the deep learning setting, and so approximation methods such as the fast gradient sign method are used instead. More recently, Madry et al. [2017] revisited this connection to robust optimization, and noted that simply solving the (non-convex) min-max formulation of the robust optimization problem works very well in practice to find and then optimize against adversarial examples. Unfortunately, because the method is still heuristically solving a non-convex problem, it is difficult to see how it could be modified to produce true provable guarantees.\n\nThis work can be seen as taking the next step in this connection between adversarial examples and robust optimization. Because we consider a convex relaxation of the adversarial polytope, we can incorporate the theory from convex robust optimization and provide provable bounds on the potential adversarial error and loss of a classifier. But we rely on the specific form of the deep network to derive an extremely efficient method for finding feasible dual solutions of the optimization problem in question without relying on any traditional optimization solver.\n\nFinally, our work also relates closely to a paper that was released a few days prior Anonymous [2018]. This paper also considers provable bounds on the possible adversarial loss of a classifier, and uses a convex upper bound (in their case, a semidefinite program), along with the dual problem, to upper bound the robust loss. However, the actual form and apparent function of the bounds differ completely, as their approach uses a semidefinite program for the specific case of one hidden layer MLPs. Additionally, the relaxation they propose seems limited to one hidden layer networks, requires a more specialized optimization procedure that includes computing and minimizing a maximum eigenvector via a Lanczos iteration, and also seems to lead to worse networks: their result highlights a guaranteed bound of 35% error on MNIST, whereas our method achieves a provable 8.3% error bound. 1\n\n\nThe adversarial polytope\n\nWe begin by considering a characterization of the exact adversarial polytope of a deep network; a very similar approach is used in Lomuscio and Maganti [2017], though we include it here for completeness, as the adversarial polytope plays a foundational role in our subsequent main contribution in the following section.\n\nTo fix notation, we consider a k layer feedforward ReLU-based neural network, f \u03b8 : R |x| \u2192 R |y| , where the output of the network is given by the pre-softmax final-layer activations (so that they would correspond to the logits of a multi-class classification architecture). The network structure is given by the equations\u1e91\ni+1 = W i z i + b i , i = 1, . . . , k \u2212 1 z i = max{\u1e91 i , 0}, i = 2, . . . , k \u2212 2 z 1 = x f \u03b8 (x) \u2261\u1e91 k(1)\nwhere we will use \u03b8 = {W 1 , . . . , W k\u22121 , b 1 , . . . , b k\u22121 } to denote the set of all parameters of the network. Although we write the linear terms in the network as matrix operations W i z i + b i , we note that W i could be any linear operator such as a convolution (and we will indeed consider convolutions in our experimental results). We also are assuming that the network here only has ReLU non-linearities. The method we propose can potentially be extended to other polytopic activations such as max-pooling or leaky ReLUs, but we leave this for future work. 2 We also note that we can integrate more complex networks that have passthrough layers or more complex structure, but again we use the simple feedforward network for simplicity of notation throughout the paper. Now we consider a set of allowable perturbations x + \u2206 where \u2206 \u221e \u2264 (we use the \u221e norm to bound adversarial examples, as this is one of the most common settings in practice, though other Input x and allowable perturbations Final layer\u1e91 k and adversarial polytope Deep network Figure 1: Illustration of the adversarial polytope (i.e., the \"reachable\" region for a given allowable perturbation).\n\nnorms are possible as well). The adversarial polytope is defined as the set of all possible z k terms achievable under this perturbation\nZ (x) = {\u1e91 k |\u1e91 k = f (x + \u2206), \u2206 \u221e \u2264 }(2)\nThe conceptual idea of the adversarial polytope is illustrated in Figure 1. This set provides us with all the information we need about adversarial examples (at least those defined by the simple perturbations defined above, which already encompass a powerful set). We will discuss this in more detail shortly, but optimizing over the adversarial polytope can tell us, for instance, whether there is any example within of x that can change the class of the example. It also forms the basis for developing a robust optimization approach that guarantees that no adversarial examples (at least on the training data) are possible.\n\nAlthough it may not be apparent, Z (x) is a connected polytope, a property which follows from the fact that the entire function is composed of linear operations and the non-linear ReLU operator, which is still defined by a (non-convex) polytope: for y = ReLU(x), the ReLU is defined by the set (y = 0 \u2227 x \u2264 0) \u2228 (y = x \u2227 x > 0)). Indeed, we can express the true adversarial polytope in terms of the bilinear integer constraints\u1e91\ni+1 = W i z i + b i , i = 1, . . . , k \u2212 1 z i =\u1e91 i \u00b7 y i , i = 2, . . . , k \u2212 1 z 1 \u2264 x + z 1 \u2265 x \u2212 y i \u2208 {0, 1} |z i |(3)\nwhere y i is a set of binary integer variables that encode \"which side\" of the ReLU the pre-ReLU activations\u1e91 i lies on. Although this is not in a form that can be easily handled by existing IP solvers because of the bilinear term\u1e91 i \u00b7 y i , it can be put into an allowable form using a standard trick from integer programming know as linearization. Specifically, supposing that we have some lower and upper bounds i , u i on the pre-ReLU activations\u1e91 i (for the integer programming formulations, these bounds can be very loose, so can be derived for instance from Lipschitz bounds of the network), then the above set of constraints is equivalent to the mixed integer constraints (we don't cover this transformation in detail here, but it is a very common pattern in the IP literature)\nz i+1 = W i z i + b i , i = 1, . . . , k \u2212 1 z i \u2264 u i \u00b7 y i , i = 2, . . . , k \u2212 1 z i \u2265\u1e91 i , i = 2, . . . , k \u2212 1 z i \u2265 0, i = 2, . . . , k = 1 z i \u2264\u1e91 i \u2212 (1 \u2212 y i ) \u00b7 i z 1 \u2264 x + z 1 \u2265 x \u2212 y i \u2208 {0, 1} |z i | .(4)\nIt is simple to check that if y i,j = 0, this implies the corresponding z i,j = 0 (the constraints simplify to z i,j \u2265 0 and z i,j \u2264 0); similarly, for y i,j = 1, the constraints imply that z i,j =\u1e91 i,j . This form, which now can be fed to off-the-shelf solvers, lets us reasonable about the exact nature of adversarial examples for a classifier.\n\n\nQueries within the adversarial polytope\n\nThe adversarial polytope allows us to formalize the notion of finding or preventing adversarial examples in a classifier. Considering some datapoint/label pair (x, y), as well as a target class y targ (the class that we are going to attempt to fool the classifier into producing), we can determine whether or not there exists an adversarial example (within \u221e norm of x) that makes the label y targ more likely that y by solving the optimization problem minimize (z k ) y \u2212 (z k ) ytarg , subject to z k \u2208 Z (x).\n\nIf the optimal objective of this optimization problem is less than 0, then there does exist an adversarial example that makes the label y targ more likely than y; this follows immediately from the nature of the optimization problem: if the objective is negative, then the value of the y targ activation is greater than the y activation, which guarantees the desired property. This optimization problem is a mixed integer linear program, and (using the polytope formulation provided in (4)) can be solved by a number of off-the-shelf integer programming solvers. Other questions can be answered as well: for instance if we want to find the minimum-perturbation adversarial example possible, then we can solve the MILP\nminimize , subject to (z k ) y \u2264 (z k ) ytarg , z k \u2208 Z (x).(6)\nBesides creating adversarial examples, the adversarial polytope can also be used to test the potential adversarial nature of unknown examples. If we are provided an example x \u2208 R n as input to a classifier, a natural question to ask is: is this potentially an adversarial example, e.g., an example that has been crafted to fool our classifier. While it is difficult to answer this question in general, in some cases it is possible to guarantee that a particular example is not adversarial, at least under the set of allowable perturbations. Specifically, if we let y pred = argmax i f i (x) be the predicted class of the example, then we can check (using the approach above) whether there are adversarial examples for all y targ = y pred . If no such example exists, then we know that the network classifies the sample x as being the same class for all perturbations, i.e., it could not be an instance of a \"good\" example (which would be classified correctly) perturbed by to be classified incorrectly. This allows us a method for guaranteeing that an example is not adversarial, even if we cannot do the converse, guarantee an example that does cross classes within the adversarial polytope is actually adversarial. However, because we know from experience that most deep classifiers are easy to fool, it is likely that a typically-trained network will flag most inputs as being potentially adversarial; thus, as we will address next, we need additional methods for training classifiers such that they are likely to produce classifiers not sensitive to adversarial perturbations.\n\n\nRobust optimization and learning robust classifier\n\nFinally, we discuss how we can use the adversarial polytope to develop \"hardened\" classifiers (more) robust to adversarial examples, based upon approaches in robust optimization. Robust optimization in the context of linear classifiers has been used for some time within machine learning (e.g Xu et al. [2009]). The basic idea is that, given a training set of x i , y i pairs, instead of simply minimizing the loss at these data points, we minimize the loss at the worst location (i.e. that with the highest loss) in an ball around each x i . Assuming a linear classifier h \u03b8 (x) = \u03b8 T x (and binary classification, for simplicity), this results in the optimization problem\nminimize \u03b8 N i=1 max \u2206 \u2264 (\u03b8 T (x + \u2206) \u00b7 y i )(7)\nUsing the the fact that max\n\u2206 \u2264 \u03b8 T \u2206 = \u03b8 *(8)\nwhere \u00b7 * denotes the dual norm, the robust optimization problem can be simplified as\nminimize \u03b8 N i=1 max \u2206 \u2264 (\u03b8 T x \u00b7 y i + \u03b8 * ).(9)\nAs mentioned above, recent work has highlighted the connection between adversarial classification and robust optimization Madry et al. [2017], in particular attempting to solve the non-convex analogue of the native min-max problem (7) by (projected) gradient descent over \u03b8 and \u2206; this connection to robust optimization was also discussed in the original adversarial example paper Goodfellow et al. [2015], with the fast gradient sign method reducing to the same solution of this min-max problem for a linear classifier.\n\nTo consider how to use the adversarial polytope to more exactly formulate the robust optimization problem, we consider training a deep classifier with a multiclass hinge loss; other losses such as the more common cross-entropy loss can be easily approximated, but the connection is more obvious with this loss. Optimizing such a deep network is given by the optimization problem\nminimize \u03b8 N i=1 [1 + max y =y i f y (x) \u2212 f y i (x)] + .(10)\nThis loss is zero if the true class output f y i (x) is at least 1 larger than any alternative class output f y (x) for y = y i , and scales linearly with the largest activation other than the true class otherwise. Because the term f y (x) \u2212 f y i (x) is exactly the objective that we considered previously, we can encode the min-max formulation of robust optimization by considering |y| \u2212 1 different\n\n\nTrue adversarial polytope\n\nConvex outer bound (\u1e91 k ) 2 (\u1e91 k ) 1 Figure 2: Simple conceptual illustration of the (non-convex) adversarial polytope, and an outer convex bound. classification problems (one for each alternative label y = y i ) over the adversarial polytope\nminimize \u03b8 N i=1 max \u2206 \u221e\u2264 [1 + max y =y i f y (x + \u2206) \u2212 f y i (x + \u2206)] + \u2261 minimize \u03b8 N i=1 [1 + max y =y i max z\u2208Z (x i ) ((\u1e91 k ) y \u2212 (\u1e91 k ) y i )] + .(11)\nThe inner maximization is precisely the optimization problem we considered earlier of finding the most extreme point within the adversarial polytope. If this objective can be minimized with zero training loss, then we are guaranteed that for every data point in the training set, no adversarial example is possible (all points within the adversarial polytope are labeled correctly by the classifier). Test points naturally have no such guarantee, but we can still use the method described above to test whether or not they are potentially adversarial. And if empirical generalization patterns for deep networks remain in this setting, it is likely that classifiers which don't admit to adversarial examples on the training set may well have similar properties on the test set (and we will demonstrate this empirically ourselves shortly).\n\n\nConvex outer bounds on the adversarial polytope\n\nDespite all the promise of the preceding section, there is a notable disadvantage that renders this approach more a conceptual strategy rather than an actual tool: the fact that actually solving a mixed binary integer program is an NP-hard task. Although it is well-appreciated that many instances of reasonably-sized binary integer programs are empirically tractable via branch and cut algorithms, in this case the optimization problem of interest has a number of binary variables equal to the number of hidden units in the network, a number that can easily grow into the millions for large deep networks. This is simply not tractable to solve via exact solutions to the integer program.\n\nInstead, the key claim of this section is that we can instead consider a convex relaxation of the adversarial polytope that is a strict outer bound. This concept is illustrated in Figure 2. That is, it contains all points (and more) within the adversarial polytope, but is a convex set and so can be optimized over efficiently (at least according to some definition of \"efficient\", which will be a major discussion point throughout this section). The key point is that because of this outer bound property, we can perform the same tests for potential adversarial examples or develop the same robust optimization techniques, which will provide identical guarantees. That is, if we look for the \"most adversarial\" example over our outer approximation, and find that it does not change the predicted class of an example, then we are guaranteed that no actual adversarial example can change the class either. Or we can develop robust classifiers that optimize over the worst-case loss within the outer approximation, and these (assuming zero training loss) also will guarantee that the classifier does not allow for adversarial examples. 3 . In this section we will first define our convex outer approximation to the adversarial polytope, and discuss the optimization problem that arises when optimizing over it. However, although this is a convex optimization problem and hence \"tractable\", it is far from practically efficient to solve this optimization problem exactly for large networks. Thus, in the next section we consider the dual problem of optimization over the convex outer bound. By standard results in convexity, any dual feasible solution represents a lower bound on minimization over the convex adversarial approximation, and as we show, the dual problem itself can be formulated as a deep network that is effectively a type of \"adjoint system\" of the original network, and which easily allows us to attain dual feasible points; thus, we can employ simple gradient descent methods to optimize this dual problem, giving us guaranteed lower bounds on the original optimization problem, even if we don't solve the dual problem to optimality. Next, we discuss how this dual framework can be fit into the context of learning a robust classifier, showing that we can indeed solve the robust classifier learning problem using the dual problem. Finally, we discuss methods for finding the necessary upper and lower bounds needed for the convex outer approximation.\n\n\nA convex outer bound\n\nNote that the definition of our adversarial polytope involved two types of terms: the linear term\u015d z i+1 = W i z i + b i (which are convex), and the non-convex ReLU terms z i = max{0,\u1e91 i }. Thus, to form a convex relaxation of this set, we need to replace the ReLU terms by some relaxed version.\n\nThe idea that underlies our convex outer bound is quite simple, and indeed has been previously considered in the context of searching over the actual adversarial polytope Ehlers [2017]; however, as mentioned above, this previous work focused on the outer bound as a useful step in searching over the true adversarial polytope for a given classifier, while we consider it here as an outer bound that we will explicitly train over. However, we do emphasize that the formalism in this section at the single activation level is virtually identical to that previously proposed in this prior work, though because our full bound will also involve iterative computation of upper and lower bounds for intermediate activations, our final outer bound ends up being quite different.\n\nSuppose that for some pre-ReLU activation\u1e91 and adversarial polytope Z (x) we have an upper and lower bound and u over the allowable values for the activation (in the remainder of this paragraph, for simplicity, we'll let\u1e91 and z denote scalar values for a single activation, which we'll later extend elementwise to all activations). Note that these upper and lower bounds are guaranteed to exist, as perturbation of of the input can only change the activations of the network by some bounded amount (this is effectively the \"adversarial\" polytope but for inner activations of the network). Then we can replace the non-convex ReLU constraint by it's convex hull over the allowable upper and lower bounds. This reduces to three cases:\n\n1. If \u2264 u \u2264 0, then we can replace the constraint z = max{0,\u1e91} with the constraint z = 0, as the ReLU will always lie in the zero region.\n\n2. Similarly, if 0 \u2264 \u2264 u, then we can replace the constraint with z =\u1e91, as the ReLU will always lie in the linear region.\n\n3. In the remaining case (the \"interesting\" non-convex case), where < 0 < u, we can replace the non-convex ReLU constraint with the convex hull over the ReLU function over the domain [ , u]. This is shown in Figure 3, and the constraint set in this case can represented by the three linear inequalities\nz \u2265 0, z \u2265\u1e91, \u2212u\u1e91 + (u \u2212 )z \u2264 \u2212u .(12)\nWith this relaxation as our starting point, we can formally define our relaxation of the adversarial polytope, which we denoteZ [ ,u] (x) (here and u signify tensors of upper and lower bounds for each activation in the network, which are naturally going to be different for different activations in the network, and which we discuss how to compute shortly) as the following set of linear constraint\u015d\nz i+1 = W i z i + b i , i = 1, . . . , k \u2212 1 z 1 \u2264 x + z 1 \u2265 x \u2212 z i,j = 0, i = 2, . . . , k \u2212 1, j \u2208 I \u2212 i z i,j =\u1e91 i,j , i = 2, . . . , k \u2212 1, j \u2208 I + i z i,j \u2265 0, z i,j \u2265\u1e91 i,j , \u2212 u i,j\u1e91i,j + (u i,j \u2212 i,j )z i,j \u2264 \u2212u i,j i,j \uf8fc \uf8f4 \uf8fd \uf8f4 \uf8fe i = 2, . . . , k \u2212 1, j \u2208 I i(13)\nwhere I \u2212 i , I + i , and I i denote the set of activations in the ith layer that fit the categories 1, 2, and 3 above respectively. Intuitively, our convex outer approximation allows some flexibility in choosing activations as subsequent layers of the network. For activations that can span over zero, the \"post-ReLU\" activation need not exactly equal the ReLU of the input, but can lie anywhere between the ReLU and the line connecting the maximum to minimum values; this extra freedom allows us to pick examples that are \"more adversarial\" at later layers of the network, because they may not correspond to any actual input fed through the network, but instead will correspond to allowing the intermediate activations in a network to be adjusted to make the example as adversarial as possible.\n\nAt this point, we could theoretically use our convex outer approximation in lieu of the true adversarial polytope, and obtain a tractable formulation to identify potential adversarial examples or learn a robust classifier. However, two key challenges arise in this approach:\n\n1. We need a way of optimizing over the adversarial polytope efficiently, that is, solving the optimization problem minimize c T\u1e91 k , subject to\u1e91 k \u2208Z [ ,u] (x).\n\nAlthough this is now a convex linear program, it still has the number of variables equal to the number of hidden units in the network, and standard solution methods for LPs would be intractable for all but very small networks.\n\n2. We need some way of finding the lower and upper bounds and u. Simple Lipschitz bounds are extremely loose here (a fact we will demonstrate shortly), and provide no real use. We need some alternative way of finding these bounds that will eventually provide a meaningful convex outer bound.\n\n\nEfficient optimization via the dual problem\n\nIn this section we deal with the first of the two problems above, and provide an efficient method for solving (or more precisely, for bounding the optimal value of) the optimization problem above, written explicitly as\nminimize c T\u1e91 k subject to\u1e91 i+1 = W i z i + b i , i = 1, . . . , k \u2212 1 z 1 \u2264 x + \u2212 z 1 \u2264 \u2212x + z i,j = 0, i = 2, . . . , k \u2212 1, j \u2208 I \u2212 i z i,j =\u1e91 i,j , i = 2, . . . , k \u2212 1, j \u2208 I + i \u2212 z i,j \u2264 0, z i,j \u2212 z i,j \u2264 0, \u2212 u i,j\u1e91i,j + (u i,j \u2212 i,j )z i,j \u2264 \u2212u i,j i,j \uf8fc \uf8f4 \uf8fd \uf8f4 \uf8fe i = 2, . . . , k \u2212 1, j \u2208 I i(15)\nThe basic idea we propose is to consider the dual of this linear program. By standard results in convex analysis, any feasible solution of the dual problem provides a guaranteed lower bound on the optimal objective of the primal problem. Crucially, we will show that the dual problem can be characterized via a \"dual network\", similar to the backpropagation network through the original graph, but with additional free parameters that can be optimized over with e.g. gradient descent. And although we don't expect to find an optimal solution to the dual problem in this manner, it will still provide the guarantee we need.\n\nIn detail, we associate the following dual variables with each of the constraint\u015d\nz i+1 = W i z i + b i \u21d2 \u03bd i+1 \u2208 R |\u1e91 i+1 | z 1 \u2264 x + \u21d2 \u03be + \u2208 R |x| \u2212z 1 \u2264 \u2212x + \u21d2 \u03be \u2212 \u2208 R |x| \u2212z i,j \u2264 0 \u21d2 \u00b5 i,j \u2208 R z i,j \u2212 z i,j \u2264 0 \u21d2 \u03c4 i,j \u2208 R \u2212u i,j\u1e91i,j + (u i,j \u2212 i,j )z i,j \u2264 \u2212u i,j i,j \u21d2 \u03bb i,j \u2208 R(16)\nwhere we note that can easily eliminate the dual variables corresponding to the z i,j = 0 and z i,j =\u1e91 i,j from the optimization problem, so we don't define explicit dual variables for these; we also note that \u00b5 i,j , \u03c4 i,j , and \u03bb i,j are only defined for i, j such that j \u2208 I i , but we keep the notation as above for simplicity. With these definitions, the dual problem becomes\nmaximize \u2212 k\u22121 i=1 \u03bd T i+1 b i \u2212 (x + ) T \u03be + + (x \u2212 ) T \u03be \u2212 + k\u22121 i=2 \u03bb T i (u i i ) subject to \u03bd k = \u2212c \u03bd i,j = 0, j \u2208 I \u2212 i \u03bd i,j = (W T i \u03bd i+1 ) j , j \u2208 I + i (u i,j \u2212 i,j )\u03bb i,j \u2212 \u00b5 i,j \u2212 \u03c4 i,j = (W T i \u03bd i+1 ) j \u03bd i,j = u i,j \u03bb i,j \u2212 \u00b5 i i = 2, . . . , k \u2212 1, j \u2208 I i , W T 1 \u03bd 2 = \u03be + \u2212 \u03be \u2212 \u03bb, \u03c4, \u00b5, \u03be + , \u03be \u2212 \u2265 0(17)\nThe key insight we highlight here is that the dual problem can also be written in the form of a deep network, which provides a trivial way to find feasible solutions to the dual problem, which can then be optimized over. Specifically, consider the constraints\n(u i,j \u2212 i,j )\u03bb i,j \u2212 \u00b5 i,j \u2212 \u03c4 i,j = (W T i \u03bd i+1 ) j \u03bd i,j = u i,j \u03bb i,j \u2212 \u00b5 i .(18)\nNote that the dual variable \u03bb corresponds to the upper bounds in the convex ReLU relaxation, while \u00b5 and \u03c4 correspond to the lower bounds z \u2265 0 and z \u2265\u1e91 respectively; by the complementarity property, we know that at the optimal solution, these variables will be zero if the ReLU constraint is non-tight, or non-zero if the ReLU constraint is tight. Because we cannot have the upper and lower bounds be simultaneously tight (this would imply that the ReLU input\u1e91 would exceed its upper or lower bound otherwise), we know that either \u03bb or \u00b5 + \u03c4 must be zero. This means that at the optimal solution to the dual problem\n(u i,j \u2212 i,j )\u03bb i,j = [(W T i \u03bd i+1 ) j ] + \u03c4 i,j + \u00b5 i,j = [(W T i \u03bd i+1 ) j ] \u2212(19)\ni.e., the dual variables capture the positive and negative portions of (W T i \u03bd i+1 ) j respectively. Combining this with the constraint that\n\u03bd i,j = u i,j \u03bb i,j \u2212 \u00b5 i(20)\nmeans that\n\u03bd i,j = u i,j u i,j \u2212 i,j [(W T i \u03bd i+1 ) j ] + \u2212 \u03b1[(W T i \u03bd i+1 ) j ] \u2212 , for j \u2208 I i(21)\nfor some 0 \u2264 \u03b1 \u2264 1 (this accounts for the fact that we can either put the \"weight\" of [(W T i \u03bd i+1 ) j ] \u2212 into \u00b5 or \u03c4 , which will or will not be passed to the next \u03bd i ). This is exactly a type of leaky ReLU operation, with a slope in the positive portion of u i,j /(u i,j \u2212 i,j ) (a term between 0 and 1), and a negative slope anywhere between 0 and 1. Similarly, and more simply, note that \u03be + and \u03be \u2212 simply denote the positive and negative portions of W T 1 \u03bd 2 , so we can replace these terms with an absolute value in the objective. Finally, we note that although it is possible to have \u00b5 i,j > 0 and \u03c4 i,j > 0 simultaneously, this corresponds to an activation that is identically zero pre-ReLU (both constraints being tight), and so is expected to be relatively rare. Putting this all together, and using\u03bd to denote \"pre-activation\" variables in the dual network, we can write the dual problem in terms of the network\n\u03bd k = \u2212\u0109 \u03bd i = W T i \u03bd i+1 , i = k \u2212 1, . . . , 1 \u03bd i,j = \uf8f1 \uf8f2 \uf8f3 0 j \u2208 I \u2212 \u00ee \u03bd i,j j \u2208 I + i u i,j u i,j \u2212 i,j [\u03bd i,j ] + \u2212 \u03b1 i,j [\u03bd i,j ] \u2212 j \u2208 I i(22)\nwhich we will abbreviate as \u03bd = g \u03b8,x (c, \u03b1) to emphasize the fact that \u2212c acts as the \"input\" to the network and \u03b1 are per-layer inputs we can also specify (for only those activations in I i ), where \u03bd in this case is shorthand for all the \u03bd i and\u03bd i activations. It is worth noting that this is exactly the backpropagation for our original classifier (the \u03bd variables are the backprop gradients, and the \u03bd i,j terms are set to be either zero or\u03bd i,j if we are on the zero or linear portions of the ReLU respectively) except that the pass for the I i variables has some additional free parameters: positive terms\u03bd i,j are weighted by u i,j u i,j \u2212 i,j and for negative terms we have the freedom to choose a weighting between zero and one via the free parameter \u03b1.\n\nThe final objective we are seeking to optimize can also be written\nJ(\u03bd) = \u2212 k\u22121 i=1 \u03bd T i+1 b i \u2212 (x + ) T [\u03bd 1 ] + + (x \u2212 ) T [\u03bd 1 ] \u2212 + k\u22121 i=2 j\u2208I i u i,j i,j u i,j \u2212 i,j [\u03bd i,j ] + = \u2212 k\u22121 i=1 \u03bd T i+1 b i \u2212 x T\u03bd 1 \u2212 \u03bd 1 1 + k\u22121 i=2 j\u2208I i i,j [\u03bd i,j ] +(23)\nThus our final rewritten form of the dual problem is maximize J(g(c, \u03b1))\n\nwhere we optimize over the variables \u03b1, and where we can solve the problem using any standard deep learning optimization method such as gradient descent (projected gradient descent, as there are the constraints that 0 \u2264 \u03b1 i,j \u2264 1). Furthermore, the above consideration that it is unlikely for \u03c4 i,j and \u00b5 i,j to simultaneously be 0, means that we expect \u03b1 to attain values of either 0 or 1. This means we can even apply methods like the fast gradient sign to simply observe the gradient w.r.t. \u03b1 i,j at some intermediate value (e.g. 0.5), and then move it to either 0 or 1 depending on the sign of its gradient, in order to provide a quick bound with just a single optimization iteration. A crucial point here is that this formulation of the problem is not convex in \u03b1, and so in general we have no guarantee of attaining optimal solutions. However, they key point is that because the dual network always corresponds to feasible solutions of the dual problem, any solution will be a guaranteed lower bound on the optimal objective of the primal. Thus, as long as the optimization procedure works well in practice (which we know to be typically the case for deep network optimization), we generally expect good performance from the method, i.e., that it will produce good bounds on the optimal solution of the primal.\n\n\nDetermining activation bounds\n\nWe have thus far ignored the question of how to actually find bounds i and u i on the pre-ReLU activations in the network. The discussion above, however, motivates a simple approach to doing so. We can simply build these bounds iteratively, layer by layer, using precisely the optimization problem formulated above. For instance, suppose that we have determined bounds for layers 2, . . . , n; then we can solve the above optimization problem with c = e i or c = \u2212e i to find lower and upper bounds respectively for the ith activation in the n + 1 layer. 4 Note again that because we solve this optimization problem via the dual, these bounds are guaranteed to be strict, even if we don't solve the dual problem to optimality.\n\nNaturally, solving two optimization problems per activation in the network (per example) would be a daunting task, and in this section we present alternative approaches that still give guaranteed bounds but which are much more efficient than this \"brute force\" approach. However, we fully admit that currently this step is by far the least scalable element of our overall approach, and finding the proper approximation schemes to scale to e.g. ImageNet-sized systems remains an open challenge.\n\nThe basis of our approximation approach here is precisely the fact we mentioned before: that even suboptimal solutions of the dual optimization problem are guaranteed to give valid bounds. Thus, instead of optimizing over \u03b1 for each activation (and each example), we use a specific fixed feasible solution for \u03b1 that substantially simplifies the problem. In particular, choosing\n\u03b1 i,j = u i,j u i,j \u2212 i,j(25)\nguarantees that the \"slope\" of the leaky ReLU for those elements j \u2208 I i is the same in the negative portion as the positive portion, i.e., the operation is simply linear (note that 0 < \u03b1 i,j < 1 since i,j is negative). Then the value of \u03bd i for all activations simultaneously (though still for a single example, since and u are sample-dependent), is given b\u0177\n\u03bd i = W T i D i+1 W T i+1 . . . D n W T n \u03bd i = D i\u03bdi(26)\nwhere again, we suppose that we have thus far already computed bounds up until layer n, and where D i is a diagonal matrix with\n(D i ) jj = \uf8f1 \uf8f2 \uf8f3 0 j \u2208 I \u2212 i 1 j \u2208 I + i u i,j u i,j \u2212 i,j j \u2208 I i .(27)\nNow we consider computing the objective value J(\u03bd) for this particular setting of \u03bd. The two linear terms in the objective\nx T\u03bd 1 and k\u22121 i=1 b T i \u03bd i+1(28)\nare both simple to handle in the above case: we simply perform the multiplication with \u03bd i leftto-right, which corresponds simply to feeding the initial example and the bias terms through the network (without bias terms). The \u03bd 1 term in the objective is harder to deal with; because this is a non-linear term, in general there is nothing to do except form the entire \u03bd 1 matrix explicitly, then compute the 1 norm of each column. This still may not be entirely intractable: we can perform the matrix multiplication (26) in any order we want, so if the input to the network is substantially smaller than the number of activations at a particular layer, or if there is some \"bottleneck\" layer of limited dimension, such as a PCA pre-processing of the data, then computing the full \u03bd 1 matrix may be reasonable. Alternatively, when the linear operators are highly structured (such as convolutions), there will be a large amount of sparsity that can be exploited, at least for networks that are not too deep; we could also use random projection methods [Candes and Tao, 2006] to derive high-probability bounds on the norms in question; or we can even use a optimization formulation to in turn bound this quantity via convex duality. However, we leave a thorough investigation of these approaches for future work, and focus here are cases where we simply compute the matrix exactly.\n\nThe same considerations hold true for the i,j [\u03bd i,j ] + terms in the objective: they are non-linear so cannot easily be simplified. However, in this case we only need to compute these terms for i, j such that j \u2208 I j , which is hopefully a manageably small subset of the total number of activations (if it is not, the convex outer adversarial polytope is likely to be extremely large anyway).\n\nWith these considerations in mind, we propose a layer-by-layer method for generating all the i and u i bounds: we start by generating lower and upper bounds for z 2 by the above procedure (which actually reduces to just a simple norm bound in this case), use these bounds to generate upper and lower bounds for z 3 , etc. Several terms can be reused through this computation, and we highlight a reasonably efficient method for doing so (though still explicitly maintaining\u03bd 1 and the various \u03bd i,j terms for j \u2208 I i ) in Algorithm 1. It is worth highlighting the connection between this incremental approach and standard norm bounds. Note that for the first layer in the network, all terms effectively lie in the \"linear\" set I + 1 . Thus we have\u03bd 1 = \u2212W T 1 , and are upper and lower bounds J(\u03bd) reduce to\n2 = x T W T 1 + b T 1 \u2212 W T 1 1 u 2 = x T W T 1 + b T 1 + W T 1 1(29)\nwhere the first term is just the example x fed through the first layer of the network and the W T 1 1\n\n(where for a matrix this denotes the column-wise 1 norm) is a standard norm bound on how far a bounded \u221e perturbation can reach. For later layers, the first two terms are replaced by\nx T\u03bd 1 + k\u22121 i=1 b T i \u03bd i+1(30)\nwhich again just denotes the example and bias terms fed through the network, whereas the later term is replaced by\n\u03bd 1 1 = W T 1 D 2 W T 2 . . . D n W T n 1(31)\nwhich is just the 1 norms of the actual product of weights and activation terms. This bound would be sufficient if all activations lay in I \u2212 and I + , i.e. if the network could only be perturbed within a linear region. But obviously in reality the perturbation can cause some activations to switch between positive and negative (exactly the I i set) and in these cases we pay an additional \"cost\" in our bounds of i,j [\u03bd i,j ] + . Remember that \u03bd i,j positive at the optimal solution corresponds to activations that \"cheat\" by lying on the upper portion of the convex ReLU relaxation, and the bound nicely capture the fact that it will be looser in this case. As a final note, we highlight the fact that the incremental computation of all these bounds, followed by some number of optimization steps for the final dual problem, can all be implemented within an automatic differentiation library (this entire procedure really is what defines our actual outer bound on the adversarial polytype). This means that the final lower bound on c T\u1e91 k can be differentiated with respect to the parameters of the network itself. Thus, in learning our classifier we will explicitly minimize over the entire computation of this bound.\n\n\nAlgorithm 1 Compute network bounds\ninput: Network parameters {W i , b i } k\u22121 i=1 , data point x, ball size // initialization \u03bd 1 := \u2212W T 1 \u03b3 1 := \u2212b T 1 2 := x T W T 1 + b T 1 \u2212 W T 1 1 // \u00b7 1 for a matrix here denotes 1 norm of all columns u 2 := x T W T 1 + b T 1 + W T 1 1 for i = 2, . . . , k \u2212 1 do form I \u2212 i , I + i , I i ; form D i as in (27) // initialize new terms \u03bd i,I i := (D i ) I i W T i \u03b3 i := \u2212b T i // propagate existing terms \u03bd j,I j := \u03bd j,I j D i W T i , j = 2, . . . , i \u2212 1 \u03b3 j := \u03b3 j D i W T i , j = 1, . . . , i \u2212 1 \u03bd 1 :=\u03bd 1 D i W T i // compute bounds i := x T\u03bd 1 + i j=1 \u03b3 i \u2212 \u03bd 1 1 + i j=2 i [\u03bd i,I i ] + u i := x T\u03bd 1 + i j=1 \u03b3 i + \u03bd 1 1 \u2212 i j=2 i [\u2212\u03bd i,I i ] + end for output: bounds { i , u i } k\u22121 i=2\n\nIntegration with robust optimization\n\nFinally, we highlight how the above approach can be easily integrated within the robust classifier learning objective we highlighted before. Recall that our robust optimization problem was given by\nminimize \u03b8 N i=1 [1 + max y =y i max z\u2208Z (x i ) ((\u1e91 k ) y \u2212 (\u1e91 k ) y i )] + .(32)\nUsing the techniques from this section, we can bound this term as\nmin \u03b8 N i=1 [1 + max y =y i max z\u2208Z (x i ) ((\u1e91 k ) y \u2212 (\u1e91 k ) y i )] + \u2264 min \u03b8 N i=1 [1 + max y =y i max z\u2208Z (x i ) ((\u1e91 k ) y \u2212 (\u1e91 k ) y i )] + \u2264 min \u03b8,\u03b1 1:N N i=1 [1 \u2212 max y =y i J(g \u03b8,x i (e y \u2212 e y i , \u03b1 i ))] +(33)\ni.e., we can substitute the cost function from our dual network into the loss of our original optimization problem, then solve simultaneously over \u03b8 and \u03b1 i . Note that in this formulation, computing our dual network \u03bd = g \u03b8,x i , (c, \u03b1 i ) implicitly also means computing the sequential bounds as described in the previous section, and we use the subscripts x i , on g to emphasize the fact that these bounds (and hence the dual network) depends on the actual example and . In practice, during training we often forgo optimization over \u03b1 and simply use the solution mentioned in the previous section to the compute the upper and lower bounds when computing the final bound as well.\n\nLastly, although the hinge loss is useful for illustrating the connection between the loss function and the optimization problem, in practice it is less effective than more common losses such as cross entropy. Fortunately, for any convex monotonic loss function (which includes cross entropy), we can use the bound\nmax \u2206 1 \u2264 (f \u03b8 (x i + \u2206), y i ) \u2264 (\u2212J(g \u03b8,x i (I \u2212 e y i 1 T , \u03b1 i )), y i )(34)\nto use more general loss functions, where \u2212J(g \u03b8,x (I \u2212 e y i 1 T , \u03b1 i )) (i.e., the dual bound evaluated at c = e y \u2212 e y i for all y) takes the place of the activations.\n\n\nSummary and discussion\n\nThe presentation in this section was quite lengthy, so it is useful to provide a summary of the end result. The final outcome of our approach is that if we learn our network by solving the optimization problem\nminimize \u03b8,\u03b1 N i=1 (\u2212J(g \u03b8,x i , (I \u2212 e y i 1 T , \u03b1 i )), y i )(35)\nwhere g \u03b8,x i is a network we compute using a few passes through the original network and its backpropagation network (albeit with much larger \"batch\" sizes through the network, since in the worst case we need to feed in a sample for each dimension of the input to compute the and u bounds), then this provides a guaranteed bound on the adversarial loss we can suffer for any adversarial perturbation with \u221e norm bounded by . The method also produces a guaranteed bound on the adversarial error, by simply checking whether the y i coordinate of \u2212J(g \u03b8,x i , (I \u2212 e y i 1 T , \u03b1 i )) is the largest (by the scaling with the input c = I \u2212 e y i 1 T , this coordinate is always zero, so it amounts to checking whether all other coordinates are negative). Although the method is based upon convex duality and linear programming, no recourse to an actual linear programming solver is needed, nor is any iterative method for solving any inner optimization problems; we simply optimize the entire (nonconvex) objective with stochastic gradient descent or any other of its variants used in deep learning, and we get the guaranteed bound.\n\nFinally, we note that all the bounds and guarantees we have discussed so far apply to the training data (they can be evaluated on a test set, but like all test error, only if the test labels are known). What are we to do with such networks in deployment, where we see new examples without knowledge of the actual label? Fortunately, by the transitivity of adversarial examples, we can use the same technique to determine whether the example might be adversarial. Specifically, given a new example x we compute the network prediction\u0177 = f (x), then determine whether there is any potential adversarial example for this predicted label within . If not, then the example must not be adversarial, because there is no point within that changes the class prediction (i.e., there couldn't be a \"normal\" input away from this possibly adversarial example). Obviously, this approach may sometimes classify non-adversarial inputs as potentially adversarial, but it has zero false negatives, in that it will never fail to flag an adversarial example. Given the challenge in even defining adversarial examples in general, this seems to be as strong a guarantee as is currently possible.\n\n\nExperimental results\n\nHere we present experiments on small-scale problems designed to demonstrate the approach. Although the method does not yet scale to ImageNet-sized classifiers, we do demonstrate the approach on a simple convolutional network applied to MNIST, illustrating that the method can apply to approaches beyond fully-connected networks with very small sizes (which represent the state of the  art for most existing work on neural network verification). Scaling challenges were discussed briefly above, and we highlight them more below. As mentioned above, code for the examples included in the paper is available at http://github.com/locuslab/convex_adversarial.\n\n\n2D toy domain\n\nOur first set of experiments involves a simple domain where both the input and output spaces of the network are two dimensional, so can be easily visualized. Specifically, we consider a 2-100-100-100-100-2 fully connected network.\n\nVisualizations of the convex outer adversarial polytope To begin, we consider some simple cases of visualizing the outer approximation to the adversarial polytope for random networks. Because the output space is two-dimensional we can easily visualize the polytopes in the output layer, and because the input space is two dimensional, we can easily cover the entire input space densely to enumerate the true adversarial polytope. In this experiment, we initialized the weights of the all layers to be normal N (0, 1/ \u221a n in ) and biases normal N (0, 1) (due to scaling, the actual absolute value of weights is not particularly important except as it relates to ). Although obviously not too much should be read into these experiments with random networks, the main takeaways are that 1) for \"small\" , the outer bound is an extremely good approximation to the adversarial polytope; 2) as increases, the bound gets substantially weaker. This is to be expected: for small , the number of elements in I will also be relatively small, and thus additional terms that make the bound lose are expected to be relatively small (in the extreme, when no activation can change, the bound will be exact, and the adversarial polytope will be a convex set). However, as gets larger, more activations enter the set I, and the available freedom in the convex relaxation of each ReLU increases substantially, making the bound looser. Naturally, the question of interest is how tight this bound is for networks that are actually trained to minimize the robust loss, which we will look at shortly.\n\nComparison to naive layerwise bounds One additional point is worth making in regards to the bounds we propose. It would also be possible to achieve a naive \"layerwise\" bound by iteratively determining absolute allowable ranges for each activation in a network (via the simple norm bound mentioned above), then for future layers, assuming each activation can vary arbitrarily within this range. This provides a simple iterative formula for computing layer-by-layer absolute bounds on the coefficients, and similar techniques have been used e.g. in Parseval Networks [Cisse et al., 2017] to produce more robust classifiers (albeit there considering 2 perturbations instead of \u221e perturbations, which likely are better suited for such an approach). Unfortunately, these naive bounds are extremely loose for multi-layer networks (in the first hidden layer, they naturally match our bounds exactly). For instance, for the adversarial polytope shown in Figure 4 (top left), the actual adversarial polytope is contained within the rang\u00ea \n\nSuch bounds are essentially vacuous in our case, which makes sense intuitively. The naive bound has no way to exploit the \"tightness\" of activations that lie entirely in the positive space, and effectively replaces the convex ReLU approximation with a (larger) box covering the entire space. Thus, such bounds are not of particular use when considering robust classification.\n\nRobust classifier training Finally, we consider training a simple robust classifier in our 2D example. Specifically, we incrementally randomly sample 50 points within the [0, 1] xy-plane, at each point waiting until we find a sample that is at least 0.08 away from other points via \u221e distance, and assign each point a random label. We then attempt to learn a robust classifier that will correctly classify all points with an \u221e ball of = 0.03. Note that there is no notion of generalization here, we are just evaluating the ability of the learning approach to fit a classification function robustly. Figure 5 shows the resulting classifiers produced by standard training over just the data points themselves (left) and robust training via our method (right). As expected, the standard training approach results in points that are classified differently somewhere within their \u221e ball of radius = 0.03 (this is exactly and adversarial example for the training set). In contrast, also as expected (because our procedure is able to attain zero robust error), the robust training method provides a classifier that is guaranteed to classify all points within the balls correctly. We use the Adam optimizer [Kingma and Ba, 2015] (over the entire batch of samples) with a learning rate of 0.001.  figure). The standard training procedure allows for some points within ball to have incorrect class labels, while the robust training does not (and the training procedure provides a bound verifying this fact).\n\nFigure 6 (left) shows the training progress of the standard and robust learning processes on this example. Of note is the fact that the robust loss seems empirically \"harder\" to minimize, which is not particularly surprising since it effectively always looks for (a bound on) the worse case loss within the entire \u221e ball around the example. However, the network is able to eventually achieve zero robust error, which is expected given the fact that the data points can indeed be separated perfectly with a nonlinear classifier. Finally, it is of some interest to see what the true adversarial polytope for the examples in this data set looks like versus the convex approximation, evaluated at the solution of the robust optimization problem. Figure 6 (right) shows one of these figures, highlighting the fact that for the final network weights and choice of epsilon, the outer bound is empirically quite tight in this case.\n\n\nMNIST ConvNet\n\nFinally, we present results on producing a provably robust classifier on the MNIST data set. Specifically, we consider a ConvNet architecture that includes two convolutional layers, with 16 and 32 channels (each with a stride of two, to decrease the resolution by half without requiring max pooling layers), and two fully connected layers stepping down to 100 and then 10 (the output dimension) hidden units, with ReLUs following each layer except the last. That is, the network has the form:\nx \u21d2 Conv 16x4x4,s=2 \u21d2 ReLU \u21d2 Conv 16x4x4,s=2 \u21d2 ReLU \u21d2 FC100 \u21d2 ReLU \u21d2 y.(38)\nThis is a reasonable if fairly small network for MNIST: training the network using standard training achieves a test error of about 1.2%. Adding layers like batch normalization and dropout easily push the error below 1%, but since we don't integrate these yet into our robust framework, we consider just the naive network. We use the Adam optimizer [Kingma and Ba, 2015] with a learning rate of 0.001 (the default option) with no additional hyperparameter selection. Figure 7 shows the training progress using our procedure with a robust softmax loss function and = 0.1. The \"robust error\" and \"robust loss\" here are our bounds on the robust error and  loss; that is, we know that any norm-bounded adversarial technique will not be able to achieve loss or error that is any higher, though in practice it could be substantially lower as well. The final classifier after 20 epochs reaches a test error of 2.93% with a robust test error of 8.39%. To see where other techniques would stand, we also tested our network against two common classes of attacks: the fast gradient sign method Goodfellow et al. [2015], and the projected gradient descent approach Madry et al. [2017]. 5 For a traditionally-trained classifier (with 1.2% test error) the FGSM approach results in 39.7% error, while PGD results in 94.0% error. On the classifier trained with our method, however, we only achieve 2.93% test error, but FGSM and PGD only achieve errors of 5.8% and 6.2% respectively (both, naturally, below our bound of 8.4%). These results are summarized in Table 1. While this is by no means state of the art performance on standard MNIST (if such a thing really has any meaning at this point, but it is certainly valid to say that these results are decidedly subpar as far as generic MNIST classification goes), a few points stand out. First, this is by far the largest provably verified network we are currently aware of, and 8% error represents a reasonable performance given that it is against any adversarial attack strategy; the only other such bound we are aware of, from the very recent work mentioned previously ?, provides a guaranteed bound of 35% error. Second, in the example above training and testing error/loss are still tracking quite closely, suggesting that we should be able to further improve performance by simply using larger models. Therein lies the catch, though: the current model took 10 hours to train for 20 epochs on a Titan X (Maxwell) GPU. This is between two and three orders of magnitude more costly than   training the naive network (though the naive network, of course, is trivially susceptible to simple adversarial attacks like the PGD method). Building robust models within the framework to scale to e.g. ImageNet-sized image classification problems remains a challenging task. But because the approach is not combinatorial in its complexity, we believe it also represents a much more feasible approach than those based upon integer programming or satisfiability, which seem highly unlikely to ever scale to such problems. Thus, we believe the current performance represents a substantial step forward in research on adversarial examples in deep networks.\n\n\nConclusions\n\nIn this paper, we have presented a method for training classifiers that are provably robust to normbounded adversarial attacks. The method is based upon linear programming and duality theory, but crucially, does not actually involve solving a linear program or anything similarly costly. Instead, we design an objective that can be computed with the equivalent of a few passes through the original network and its backpropagation net (with larger batch sizes), yet still gives a guaranteed bound on the robust error and loss of the classifier. While we feel this is a substantial step forward in the development of classifiers that are not susceptible to adversarial attacks, two main directions for improvement exist. First, as mentioned above, approaches are needed to make the method scale better. Naively, computing the upper and lower bounds as we described in this paper essentially require sending at least one sample through the network for every dimension of the input vector (and more at intermediate layers, if substantial numbers of activations have bounds that span zero). For domains like ImageNet, this is completely infeasible, and techniques such as (at a minimum) using bottleneck layers, other dual bounds, and probabilistic norm bounds are likely necessary to make the approach scale. However, unlike many past approaches, this scaling is not fundamentally combinatorial, so has some chance of success even in large networks.\n\nSecond, at some point it will be necessary to characterize attacks beyond simple norm bounds. While \u221e bounded examples are nice in that they offer a compelling visualization of images that look virtually \"identical\" to existing examples yet are misclassified, this is by no means the only set of possible attacks. For example, the recent work in Sharif et al. [2016] was able to break face recognition software by using manufactured glasses, but adding glasses to a face image clearly is not bounded in \u221e norm. And even if we consider a norm where these examples are bounded (likely an 1 norm), then there almost certainly exist examples with the same 1 bound that would fool a human. Thus, the progress of adversarial examples must start to formalize what sorts of sets we will and won't allow for our adversarial perturbations, and these are almost certainly not going to be convex sets, as relied upon by our method. Thus, a great deal of work remains to understand both the space of adversarial examples that we want classifiers to be robust to, as well as methods for dealing with these likely highly non-convex sets in the input space.\n\nFinally, although our focus in this paper was on adversarial examples and robust classification, the general techniques described here (namely optimizing over relaxed convex networks, and using a non-convex network representation of the dual problem to derive guaranteed bounds), may find applicability well beyond adversarial examples in deep learning. Many domain and problems such as inverting neural networks, neural style algorithms, and optimizing over latent spaces involve optimization problems that are a function of the neural network inputs or activations, and similar techniques may be brought to bear in these domains as well.\n\nFigure 3 :\n3Illustration of the convex ReLU relaxation over the bounded set [ , u].\n\nFigure 4 :\n4Illustrations of the true adversarial polytope (gray) and our convex outer approximation (green) for a random 2-100-100-100-100-2 network with N (0, 1/ \u221a n) weight initialization. Polytopes are shown for = 0.05 (top row), = 0.1 (middle row), and = 0.25 (bottom row).\n\nFigure 5 :\n5Illustration of classification boundaries resulting from standard training (left) and robust training (right) with \u221e balls of size = 0.03 (shown in\n\nFigure 6 :\n6(left) Training curves for the 2D robust adversarial problem, where each iteration denotes a step of the Adam optimizer. (right) Illustration of the actual adversarial polytope and the convex outer approximation for one of the training points after the robust optimization procedure.\n\nFigure 7 :\n7Training progress for our method applied to a simple ConvNet architecture on MNIST. Robust error and loss indicate provable upper bounds on the loss/error achievable by any adversarial perturbation with \u221e norm bounded by = 0.1.\n\n\nTable 1: Adversarial errors for a traditionally-trained and our robustly-trained classifier on the MNIST test set.Network \nNon-adversarial FGSM PGD Robust Bound \nStandard training \n1.2% \n39.7% 94.0% \n100% \nRobust training \n2.9% \n5.8% \n6.2% \n8.4% \n\n\nThis assessment represents our best understanding of this work at the moment, though the paper is under anonymous review, and thus we not been able to discuss with the authors. We welcome any feedback on this point and will update our notes here as needed.2  We can also potentially integrate other non-piecewise-linear activations such as sigmoids, though this requires an approximation if we do want to use polyhedral methods, so we focus here on piecewise-linear activations.\nNote, however, that the outer approximation is less useful for actually find adversarial examples: as we will see, because elements in this polytope may not correspond to any actual path through the original network, it is unclear what if anything can be said about the actual \"examples\" found via this method. However, given the relative ease of finding versus preventing adversarial examples in modern deep architectures, this is not overly bothersome\nAlthough we have discussed using the procedure to generate lower bounds, it can also trivially produce upper bound on max c T\u1e91 k by minimizing min \u2212c T\u1e91 k ; if J is a lower bound on the minimization problem, then \u2212J will be an upper bound on the maximization problem.\nFor PGD, as inMadry et al. [2017] we use \u221e ball gradient descent and 100 iterations of step size 0.001, which was sufficient for convergence.\n\nCertified defenses against adversarial examples. Anonymous, Under review at. Anonymous. Certified defenses against adversarial examples. In Under review at ICLR 2018, 2018.\n\nAnish Athalye, Ilya Sutskever, arXiv:1707.07397Synthesizing robust adversarial examples. arXiv preprintAnish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.\n\n. Aharon Ben-Tal, Laurent El Ghaoui, Arkadi Nemirovski, Princeton University PressRobust optimizationAharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton University Press, 2009.\n\nNear-optimal signal recovery from random projections: Universal encoding strategies?. J Emmanuel, Terence Candes, Tao, IEEE transactions on information theory. 52Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE transactions on information theory, 52(12):5406-5425, 2006.\n\nTowards evaluating the robustness of neural networks. Nicholas Carlini, David Wagner, Security and Privacy (SP), 2017 IEEE Symposium on. IEEENicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39-57. IEEE, 2017.\n\nNicholas Carlini, Guy Katz, Clark Barrett, David L Dill, arXiv:1709.10207Ground-truth adversarial examples. arXiv preprintNicholas Carlini, Guy Katz, Clark Barrett, and David L Dill. Ground-truth adversarial examples. arXiv preprint arXiv:1709.10207, 2017.\n\nParseval networks: Improving robustness to adversarial examples. Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, Nicolas Usunier, International Conference on Machine Learning. Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pages 854-863, 2017.\n\nFormal verification of piece-wise linear feed-forward neural networks. Ruediger Ehlers, International Symposium on Automated Technology for Verification and Analysis. Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna- tional Symposium on Automated Technology for Verification and Analysis, 2017.\n\nExplaining and harnessing adversarial examples. Ian Goodfellow, Jonathon Shlens, Christian Szegedy, International Conference on Learning Representations. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. URL http://arxiv. org/abs/1412.6572.\n\nSafety verification of deep neural networks. Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu, International Conference on Computer Aided Verification. SpringerXiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In International Conference on Computer Aided Verification, pages 3-29. Springer, 2017.\n\nReluplex: An efficient smt solver for verifying deep neural networks. Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer, arXiv:1702.01135arXiv preprintGuy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. arXiv preprint arXiv:1702.01135, 2017.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, International Conference on Learning Representations. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\n\nAn approach to reachability analysis for feed-forward relu neural networks. Alessio Lomuscio, Lalit Maganti, arXiv:1706.07351arXiv preprintAlessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu neural networks. arXiv preprint arXiv:1706.07351, 2017.\n\nNo need to worry about adversarial examples in object detection in autonomous vehicles. Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth, arXiv:1707.03501arXiv preprintJiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. No need to worry about adversarial examples in object detection in autonomous vehicles. arXiv preprint arXiv:1707.03501, 2017.\n\nTowards deep learning models resistant to adversarial attacks. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, arXiv:1706.06083arXiv preprintAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nDistillation as a defense to adversarial perturbations against deep neural networks. Nicolas Papernot, Patrick Mcdaniel, Xi Wu, Somesh Jha, Ananthram Swami, Security and Privacy (SP), 2016 IEEE Symposium on. IEEENicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pages 582-597. IEEE, 2016.\n\nPractical black-box attacks against deep learning systems using adversarial examples. Nicolas Papernot, Patrick Mcdaniel, Ian Goodfellow, Somesh Jha, Ananthram Berkay Celik, Swami, Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security. the 2017 ACM Asia Conference on Computer and Communications SecurityNicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial examples. In Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, 2017.\n\nAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K Reiter, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityACMMahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 1528-1540. ACM, 2016.\n\nRobustness and regularization of support vector machines. Huan Xu, Constantine Caramanis, Shie Mannor, Journal of Machine Learning Research. 10Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10(Jul):1485-1510, 2009.\n", "annotations": {"author": "[{\"end\":219,\"start\":108},{\"end\":328,\"start\":220}]", "publisher": null, "author_last_name": "[{\"end\":121,\"start\":115},{\"end\":229,\"start\":225}]", "author_first_name": "[{\"end\":109,\"start\":108},{\"end\":114,\"start\":110},{\"end\":224,\"start\":220}]", "author_affiliation": "[{\"end\":218,\"start\":142},{\"end\":327,\"start\":251}]", "title": "[{\"end\":89,\"start\":1},{\"end\":417,\"start\":329}]", "venue": null, "abstract": "[{\"end\":1958,\"start\":435}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2084,\"start\":2059},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2595,\"start\":2572},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2686,\"start\":2660},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2809,\"start\":2792},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2854,\"start\":2825},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3026,\"start\":3003},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7588,\"start\":7569},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7602,\"start\":7588},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8394,\"start\":8380},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8826,\"start\":8798},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9734,\"start\":9713},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10336,\"start\":10319},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10507,\"start\":10483},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11076,\"start\":11057},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12152,\"start\":12136},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13128,\"start\":13101},{\"end\":14297,\"start\":14296},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20892,\"start\":20876},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21630,\"start\":21611},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21894,\"start\":21870},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27835,\"start\":27822},{\"end\":29607,\"start\":29601},{\"end\":29892,\"start\":29887},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42718,\"start\":42697},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":54295,\"start\":54275},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":56338,\"start\":56317},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":58497,\"start\":58476},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":59234,\"start\":59210},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":59299,\"start\":59280},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":63136,\"start\":63116},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":67102,\"start\":67083}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":64637,\"start\":64553},{\"attributes\":{\"id\":\"fig_1\"},\"end\":64917,\"start\":64638},{\"attributes\":{\"id\":\"fig_2\"},\"end\":65078,\"start\":64918},{\"attributes\":{\"id\":\"fig_3\"},\"end\":65375,\"start\":65079},{\"attributes\":{\"id\":\"fig_5\"},\"end\":65616,\"start\":65376},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":65867,\"start\":65617}]", "paragraph": "[{\"end\":3294,\"start\":1974},{\"end\":4080,\"start\":3296},{\"end\":5288,\"start\":4082},{\"end\":6556,\"start\":5290},{\"end\":7346,\"start\":6558},{\"end\":9001,\"start\":7363},{\"end\":9590,\"start\":9003},{\"end\":11484,\"start\":9592},{\"end\":12049,\"start\":11486},{\"end\":12941,\"start\":12051},{\"end\":13289,\"start\":12970},{\"end\":13615,\"start\":13291},{\"end\":14900,\"start\":13724},{\"end\":15038,\"start\":14902},{\"end\":15706,\"start\":15081},{\"end\":16136,\"start\":15708},{\"end\":17046,\"start\":16261},{\"end\":17610,\"start\":17264},{\"end\":18165,\"start\":17654},{\"end\":18883,\"start\":18167},{\"end\":20528,\"start\":18948},{\"end\":21256,\"start\":20583},{\"end\":21333,\"start\":21306},{\"end\":21438,\"start\":21353},{\"end\":22009,\"start\":21489},{\"end\":22389,\"start\":22011},{\"end\":22853,\"start\":22452},{\"end\":23125,\"start\":22883},{\"end\":24120,\"start\":23283},{\"end\":24860,\"start\":24172},{\"end\":27329,\"start\":24862},{\"end\":27649,\"start\":27354},{\"end\":28421,\"start\":27651},{\"end\":29154,\"start\":28423},{\"end\":29293,\"start\":29156},{\"end\":29416,\"start\":29295},{\"end\":29720,\"start\":29418},{\"end\":30158,\"start\":29759},{\"end\":31227,\"start\":30431},{\"end\":31503,\"start\":31229},{\"end\":31666,\"start\":31505},{\"end\":31894,\"start\":31668},{\"end\":32187,\"start\":31896},{\"end\":32453,\"start\":32235},{\"end\":33383,\"start\":32761},{\"end\":33466,\"start\":33385},{\"end\":34055,\"start\":33675},{\"end\":34641,\"start\":34382},{\"end\":35345,\"start\":34729},{\"end\":35573,\"start\":35432},{\"end\":35614,\"start\":35604},{\"end\":36633,\"start\":35706},{\"end\":37550,\"start\":36786},{\"end\":37618,\"start\":37552},{\"end\":37885,\"start\":37813},{\"end\":39203,\"start\":37887},{\"end\":39963,\"start\":39237},{\"end\":40458,\"start\":39965},{\"end\":40838,\"start\":40460},{\"end\":41228,\"start\":40869},{\"end\":41414,\"start\":41287},{\"end\":41611,\"start\":41489},{\"end\":43025,\"start\":41647},{\"end\":43420,\"start\":43027},{\"end\":44228,\"start\":43422},{\"end\":44400,\"start\":44299},{\"end\":44584,\"start\":44402},{\"end\":44732,\"start\":44618},{\"end\":46000,\"start\":44779},{\"end\":46975,\"start\":46778},{\"end\":47123,\"start\":47058},{\"end\":48025,\"start\":47343},{\"end\":48341,\"start\":48027},{\"end\":48595,\"start\":48423},{\"end\":48831,\"start\":48622},{\"end\":50028,\"start\":48900},{\"end\":51203,\"start\":50030},{\"end\":51882,\"start\":51228},{\"end\":52130,\"start\":51900},{\"end\":53708,\"start\":52132},{\"end\":54739,\"start\":53710},{\"end\":55116,\"start\":54741},{\"end\":56615,\"start\":55118},{\"end\":57540,\"start\":56617},{\"end\":58050,\"start\":57558},{\"end\":61307,\"start\":58127},{\"end\":62768,\"start\":61323},{\"end\":63911,\"start\":62770},{\"end\":64552,\"start\":63913}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13723,\"start\":13616},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15080,\"start\":15039},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16260,\"start\":16137},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17263,\"start\":17047},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18947,\"start\":18884},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21305,\"start\":21257},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21352,\"start\":21334},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21488,\"start\":21439},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22451,\"start\":22390},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23282,\"start\":23126},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29758,\"start\":29721},{\"attributes\":{\"id\":\"formula_12\"},\"end\":30430,\"start\":30159},{\"attributes\":{\"id\":\"formula_14\"},\"end\":32760,\"start\":32454},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33674,\"start\":33467},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34381,\"start\":34056},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34728,\"start\":34642},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35431,\"start\":35346},{\"attributes\":{\"id\":\"formula_19\"},\"end\":35603,\"start\":35574},{\"attributes\":{\"id\":\"formula_20\"},\"end\":35705,\"start\":35615},{\"attributes\":{\"id\":\"formula_21\"},\"end\":36785,\"start\":36634},{\"attributes\":{\"id\":\"formula_22\"},\"end\":37812,\"start\":37619},{\"attributes\":{\"id\":\"formula_24\"},\"end\":40868,\"start\":40839},{\"attributes\":{\"id\":\"formula_25\"},\"end\":41286,\"start\":41229},{\"attributes\":{\"id\":\"formula_26\"},\"end\":41488,\"start\":41415},{\"attributes\":{\"id\":\"formula_27\"},\"end\":41646,\"start\":41612},{\"attributes\":{\"id\":\"formula_28\"},\"end\":44298,\"start\":44229},{\"attributes\":{\"id\":\"formula_29\"},\"end\":44617,\"start\":44585},{\"attributes\":{\"id\":\"formula_30\"},\"end\":44778,\"start\":44733},{\"attributes\":{\"id\":\"formula_31\"},\"end\":46738,\"start\":46038},{\"attributes\":{\"id\":\"formula_32\"},\"end\":47057,\"start\":46976},{\"attributes\":{\"id\":\"formula_33\"},\"end\":47342,\"start\":47124},{\"attributes\":{\"id\":\"formula_34\"},\"end\":48422,\"start\":48342},{\"attributes\":{\"id\":\"formula_35\"},\"end\":48899,\"start\":48832},{\"attributes\":{\"id\":\"formula_37\"},\"end\":58126,\"start\":58051}]", "table_ref": "[{\"end\":59678,\"start\":59670}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1972,\"start\":1960},{\"attributes\":{\"n\":\"1.1\"},\"end\":7361,\"start\":7349},{\"attributes\":{\"n\":\"2\"},\"end\":12968,\"start\":12944},{\"attributes\":{\"n\":\"2.1\"},\"end\":17652,\"start\":17613},{\"attributes\":{\"n\":\"2.2\"},\"end\":20581,\"start\":20531},{\"end\":22881,\"start\":22856},{\"attributes\":{\"n\":\"3\"},\"end\":24170,\"start\":24123},{\"attributes\":{\"n\":\"3.1\"},\"end\":27352,\"start\":27332},{\"attributes\":{\"n\":\"3.2\"},\"end\":32233,\"start\":32190},{\"attributes\":{\"n\":\"3.3\"},\"end\":39235,\"start\":39206},{\"end\":46037,\"start\":46003},{\"attributes\":{\"n\":\"3.4\"},\"end\":46776,\"start\":46740},{\"attributes\":{\"n\":\"3.5\"},\"end\":48620,\"start\":48598},{\"attributes\":{\"n\":\"4\"},\"end\":51226,\"start\":51206},{\"attributes\":{\"n\":\"4.1\"},\"end\":51898,\"start\":51885},{\"attributes\":{\"n\":\"4.2\"},\"end\":57556,\"start\":57543},{\"attributes\":{\"n\":\"5\"},\"end\":61321,\"start\":61310},{\"end\":64564,\"start\":64554},{\"end\":64649,\"start\":64639},{\"end\":64929,\"start\":64919},{\"end\":65090,\"start\":65080},{\"end\":65387,\"start\":65377}]", "table": "[{\"end\":65867,\"start\":65733}]", "figure_caption": "[{\"end\":64637,\"start\":64566},{\"end\":64917,\"start\":64651},{\"end\":65078,\"start\":64931},{\"end\":65375,\"start\":65092},{\"end\":65616,\"start\":65389},{\"end\":65733,\"start\":65619}]", "figure_ref": "[{\"end\":14791,\"start\":14783},{\"end\":15155,\"start\":15147},{\"end\":22928,\"start\":22920},{\"end\":25050,\"start\":25042},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29634,\"start\":29626},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":54664,\"start\":54656},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":55725,\"start\":55717},{\"end\":56413,\"start\":56406},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":57367,\"start\":57359},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":58602,\"start\":58594}]", "bib_author_first_name": "[{\"end\":67391,\"start\":67386},{\"end\":67405,\"start\":67401},{\"end\":67613,\"start\":67607},{\"end\":67630,\"start\":67623},{\"end\":67633,\"start\":67631},{\"end\":67648,\"start\":67642},{\"end\":67907,\"start\":67906},{\"end\":67925,\"start\":67918},{\"end\":68230,\"start\":68222},{\"end\":68245,\"start\":68240},{\"end\":68486,\"start\":68478},{\"end\":68499,\"start\":68496},{\"end\":68511,\"start\":68506},{\"end\":68528,\"start\":68521},{\"end\":68810,\"start\":68801},{\"end\":68823,\"start\":68818},{\"end\":68843,\"start\":68836},{\"end\":68855,\"start\":68851},{\"end\":68872,\"start\":68865},{\"end\":69228,\"start\":69220},{\"end\":69546,\"start\":69543},{\"end\":69567,\"start\":69559},{\"end\":69585,\"start\":69576},{\"end\":69906,\"start\":69899},{\"end\":69919,\"start\":69914},{\"end\":69936,\"start\":69933},{\"end\":69946,\"start\":69943},{\"end\":70279,\"start\":70276},{\"end\":70291,\"start\":70286},{\"end\":70306,\"start\":70301},{\"end\":70317,\"start\":70313},{\"end\":70331,\"start\":70326},{\"end\":70612,\"start\":70604},{\"end\":70626,\"start\":70621},{\"end\":70906,\"start\":70899},{\"end\":70922,\"start\":70917},{\"end\":71208,\"start\":71202},{\"end\":71220,\"start\":71213},{\"end\":71232,\"start\":71228},{\"end\":71245,\"start\":71240},{\"end\":71543,\"start\":71533},{\"end\":71561,\"start\":71551},{\"end\":71577,\"start\":71571},{\"end\":71595,\"start\":71587},{\"end\":71611,\"start\":71605},{\"end\":71934,\"start\":71927},{\"end\":71952,\"start\":71945},{\"end\":71965,\"start\":71963},{\"end\":71976,\"start\":71970},{\"end\":71991,\"start\":71982},{\"end\":72390,\"start\":72383},{\"end\":72408,\"start\":72401},{\"end\":72422,\"start\":72419},{\"end\":72441,\"start\":72435},{\"end\":72456,\"start\":72447},{\"end\":73008,\"start\":73001},{\"end\":73022,\"start\":73017},{\"end\":73040,\"start\":73036},{\"end\":73057,\"start\":73048},{\"end\":73564,\"start\":73560},{\"end\":73580,\"start\":73569},{\"end\":73596,\"start\":73592}]", "bib_author_last_name": "[{\"end\":67270,\"start\":67261},{\"end\":67399,\"start\":67392},{\"end\":67415,\"start\":67406},{\"end\":67621,\"start\":67614},{\"end\":67640,\"start\":67634},{\"end\":67659,\"start\":67649},{\"end\":67916,\"start\":67908},{\"end\":67932,\"start\":67926},{\"end\":67937,\"start\":67934},{\"end\":68238,\"start\":68231},{\"end\":68252,\"start\":68246},{\"end\":68494,\"start\":68487},{\"end\":68504,\"start\":68500},{\"end\":68519,\"start\":68512},{\"end\":68533,\"start\":68529},{\"end\":68816,\"start\":68811},{\"end\":68834,\"start\":68824},{\"end\":68849,\"start\":68844},{\"end\":68863,\"start\":68856},{\"end\":68880,\"start\":68873},{\"end\":69235,\"start\":69229},{\"end\":69557,\"start\":69547},{\"end\":69574,\"start\":69568},{\"end\":69593,\"start\":69586},{\"end\":69912,\"start\":69907},{\"end\":69931,\"start\":69920},{\"end\":69941,\"start\":69937},{\"end\":69949,\"start\":69947},{\"end\":70284,\"start\":70280},{\"end\":70299,\"start\":70292},{\"end\":70311,\"start\":70307},{\"end\":70324,\"start\":70318},{\"end\":70344,\"start\":70332},{\"end\":70619,\"start\":70613},{\"end\":70629,\"start\":70627},{\"end\":70915,\"start\":70907},{\"end\":70930,\"start\":70923},{\"end\":71211,\"start\":71209},{\"end\":71226,\"start\":71221},{\"end\":71238,\"start\":71233},{\"end\":71253,\"start\":71246},{\"end\":71549,\"start\":71544},{\"end\":71569,\"start\":71562},{\"end\":71585,\"start\":71578},{\"end\":71603,\"start\":71596},{\"end\":71617,\"start\":71612},{\"end\":71943,\"start\":71935},{\"end\":71961,\"start\":71953},{\"end\":71968,\"start\":71966},{\"end\":71980,\"start\":71977},{\"end\":71997,\"start\":71992},{\"end\":72399,\"start\":72391},{\"end\":72417,\"start\":72409},{\"end\":72433,\"start\":72423},{\"end\":72445,\"start\":72442},{\"end\":72469,\"start\":72457},{\"end\":72476,\"start\":72471},{\"end\":73015,\"start\":73009},{\"end\":73034,\"start\":73023},{\"end\":73046,\"start\":73041},{\"end\":73064,\"start\":73058},{\"end\":73567,\"start\":73565},{\"end\":73590,\"start\":73581},{\"end\":73603,\"start\":73597}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11217889},\"end\":67384,\"start\":67212},{\"attributes\":{\"doi\":\"arXiv:1707.07397\",\"id\":\"b1\"},\"end\":67603,\"start\":67386},{\"attributes\":{\"id\":\"b2\"},\"end\":67818,\"start\":67605},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1431305},\"end\":68166,\"start\":67820},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2893830},\"end\":68476,\"start\":68168},{\"attributes\":{\"doi\":\"arXiv:1709.10207\",\"id\":\"b5\"},\"end\":68734,\"start\":68478},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":26714567},\"end\":69147,\"start\":68736},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1931807},\"end\":69493,\"start\":69149},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6706414},\"end\":69852,\"start\":69495},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11626373},\"end\":70204,\"start\":69854},{\"attributes\":{\"doi\":\"arXiv:1702.01135\",\"id\":\"b10\"},\"end\":70558,\"start\":70206},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6628106},\"end\":70821,\"start\":70560},{\"attributes\":{\"doi\":\"arXiv:1706.07351\",\"id\":\"b12\"},\"end\":71112,\"start\":70823},{\"attributes\":{\"doi\":\"arXiv:1707.03501\",\"id\":\"b13\"},\"end\":71468,\"start\":71114},{\"attributes\":{\"doi\":\"arXiv:1706.06083\",\"id\":\"b14\"},\"end\":71840,\"start\":71470},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2672720},\"end\":72295,\"start\":71842},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14321358},\"end\":72911,\"start\":72297},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207241700},\"end\":73500,\"start\":72913},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3118889},\"end\":73814,\"start\":73502}]", "bib_title": "[{\"end\":67259,\"start\":67212},{\"end\":67904,\"start\":67820},{\"end\":68220,\"start\":68168},{\"end\":68799,\"start\":68736},{\"end\":69218,\"start\":69149},{\"end\":69541,\"start\":69495},{\"end\":69897,\"start\":69854},{\"end\":70602,\"start\":70560},{\"end\":71925,\"start\":71842},{\"end\":72381,\"start\":72297},{\"end\":72999,\"start\":72913},{\"end\":73558,\"start\":73502}]", "bib_author": "[{\"end\":67272,\"start\":67261},{\"end\":67401,\"start\":67386},{\"end\":67417,\"start\":67401},{\"end\":67623,\"start\":67607},{\"end\":67642,\"start\":67623},{\"end\":67661,\"start\":67642},{\"end\":67918,\"start\":67906},{\"end\":67934,\"start\":67918},{\"end\":67939,\"start\":67934},{\"end\":68240,\"start\":68222},{\"end\":68254,\"start\":68240},{\"end\":68496,\"start\":68478},{\"end\":68506,\"start\":68496},{\"end\":68521,\"start\":68506},{\"end\":68535,\"start\":68521},{\"end\":68818,\"start\":68801},{\"end\":68836,\"start\":68818},{\"end\":68851,\"start\":68836},{\"end\":68865,\"start\":68851},{\"end\":68882,\"start\":68865},{\"end\":69237,\"start\":69220},{\"end\":69559,\"start\":69543},{\"end\":69576,\"start\":69559},{\"end\":69595,\"start\":69576},{\"end\":69914,\"start\":69899},{\"end\":69933,\"start\":69914},{\"end\":69943,\"start\":69933},{\"end\":69951,\"start\":69943},{\"end\":70286,\"start\":70276},{\"end\":70301,\"start\":70286},{\"end\":70313,\"start\":70301},{\"end\":70326,\"start\":70313},{\"end\":70346,\"start\":70326},{\"end\":70621,\"start\":70604},{\"end\":70631,\"start\":70621},{\"end\":70917,\"start\":70899},{\"end\":70932,\"start\":70917},{\"end\":71213,\"start\":71202},{\"end\":71228,\"start\":71213},{\"end\":71240,\"start\":71228},{\"end\":71255,\"start\":71240},{\"end\":71551,\"start\":71533},{\"end\":71571,\"start\":71551},{\"end\":71587,\"start\":71571},{\"end\":71605,\"start\":71587},{\"end\":71619,\"start\":71605},{\"end\":71945,\"start\":71927},{\"end\":71963,\"start\":71945},{\"end\":71970,\"start\":71963},{\"end\":71982,\"start\":71970},{\"end\":71999,\"start\":71982},{\"end\":72401,\"start\":72383},{\"end\":72419,\"start\":72401},{\"end\":72435,\"start\":72419},{\"end\":72447,\"start\":72435},{\"end\":72471,\"start\":72447},{\"end\":72478,\"start\":72471},{\"end\":73017,\"start\":73001},{\"end\":73036,\"start\":73017},{\"end\":73048,\"start\":73036},{\"end\":73066,\"start\":73048},{\"end\":73569,\"start\":73560},{\"end\":73592,\"start\":73569},{\"end\":73605,\"start\":73592}]", "bib_venue": "[{\"end\":67287,\"start\":67272},{\"end\":67473,\"start\":67433},{\"end\":67978,\"start\":67939},{\"end\":68303,\"start\":68254},{\"end\":68584,\"start\":68551},{\"end\":68926,\"start\":68882},{\"end\":69314,\"start\":69237},{\"end\":69647,\"start\":69595},{\"end\":70006,\"start\":69951},{\"end\":70274,\"start\":70206},{\"end\":70683,\"start\":70631},{\"end\":70897,\"start\":70823},{\"end\":71200,\"start\":71114},{\"end\":71531,\"start\":71470},{\"end\":72048,\"start\":71999},{\"end\":72561,\"start\":72478},{\"end\":73151,\"start\":73066},{\"end\":73641,\"start\":73605},{\"end\":72631,\"start\":72563},{\"end\":73223,\"start\":73153}]"}}}, "year": 2023, "month": 12, "day": 17}