{"id": 235187032, "updated": "2023-10-06 02:57:54.065", "metadata": {"title": "Personalized Transformer for Explainable Recommendation", "authors": "[{\"first\":\"Lei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yongfeng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.11601", "mag": null, "acl": "2021.acl-long.383", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/LiZC20", "doi": "10.18653/v1/2021.acl-long.383"}}, "content": {"source": {"pdf_hash": "1bd78191326c83bcc6f1cd5d2efa97dc6fc37262", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.acl-long.383.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://aclanthology.org/2021.acl-long.383.pdf", "status": "HYBRID"}}, "grobid": {"id": "157cc9e27d91a873268cf55a38106776afd1200c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1bd78191326c83bcc6f1cd5d2efa97dc6fc37262.txt", "contents": "\nPersonalized Transformer for Explainable Recommendation\nAugust 1-6, 2021\n\nLei Li \nHong Kong Baptist University\nHong KongChina\n\nYongfeng Zhang \nRutgers University\nNew BrunswickUSA\n\nLi Chen lichen@comp.hkbu.edu.hk2yongfeng.zhang@rutgers.edu \nHong Kong Baptist University\nHong KongChina\n\nPersonalized Transformer for Explainable Recommendation\n\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\nthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAugust 1-6, 20214947\nPersonalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PE-TER 1 ), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendationexplanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.\n\nIntroduction\n\nRecent years have witnessed the successful application of natural language generation. Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014;Li et al., 2020c;, review generation (Dong et al., 2017), review summarization , and conversational systems . In these tasks, user and item IDs that distinguish one user/item from the others are crucial to 1 https://github.com/lileipisces/PETER personalization. For example, in recommender systems, different users may care about different item features (e.g., style vs. quality), and different items may have different characteristics (e.g., fashionable vs. comfortable). The goal of explainable recommendation  is to provide an explanation to a user for a recommended item, so as to justify how the recommendation might match his/her interests. That is, given a pair of user ID and item ID, the system needs to generate an explanation, such as \"the style of the jacket is fashionable\" (see the last column of Table 4 for more examples).\n\nTransformer (Vaswani et al., 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al., 2018;Devlin et al., 2019;Brown et al., 2020), however, is relatively under-explored for personalized natural language generation. Since IDs and words are in very different semantic spaces, it would be problematic to directly put them together for attention learning, because by doing so, the IDs are treated as words, but the IDs appear far less frequently than the words. For example, a paragraph of review (and thus hundreds of words) on e-commerce platform only corresponds to a single pair of user ID and item ID. As such, the IDs may be regarded as out-of-vocabulary tokens, to which the model is insensitive. As shown in Fig. 1(a), when generating an explanation for a user-item pair, standard Transformer relies heavily on the special <bos> token instead of the user or the item. This would result in identical explanations over different useritem pairs (see USR score in Table 2), deviating from our personalization goal.\n\nTo address this problem, we bridge IDs and words by designing an elegant task called context prediction, which maps IDs onto words to be generated by the explanation task. This in some way resembles one's drafting-polishing process, where by predicting some words the context prediction [User] [ Figure 1: Attention visualization of two models when generating an explanation for the same user-item pair (see the first two columns). They are both from the last attention layer, so the target sequences are offset by one position for better illustration. The larger the attention weights, the lighter the cells.\n\ntask does the job of drafting. Then, the explanation generation task polishes these words so as to form a readable sentence. Meanwhile, we demonstrate that conducting recommendation task on the same model is also feasible, so we name it PETER, which stands for PErsonalized Transformer for Explainable Recommendation. As we can see in Fig.  1(b), when PETER generates an explanation for the same user-item pair, it can utilize the information of both the user and the item, which illustrates the effectiveness of our context prediction task. In addition, PETER is flexible to incorporate item features that can help to guide its generation. This can be very useful when, for instance, a user proactively asks the system to explain certain feature(s) of a recommendation (Li et al., 2020c), e.g., price. Then, we would expect the model to generate a targeted explanation, such as \"great jacket, especially for the price\". PETER is a small unpretrained Transformer with only 2 layers, yet it outperforms a fine-tuned BERT (Ni et al., 2019) on most metrics by a large margin, and takes less time to train, as shown in our experiments. This manifests the superiority of our model.\n\nIn summary, our key contributions are:\n\n\u2022 We propose PETER that makes recommendation and generates explanation simultaneously based on user and item IDs for explainable recommendation. To the best of our knowledge, we are the first to enable Transformer with personalized natural language generation.\n\n\u2022 We evaluate the generated explanations on not only text quality metrics (such as BLEU and ROUGE), but also metrics that particularly focus on explainability from the angle of item features. Extensive experiments show that our model can outperform state-of-the-art baselines on large datasets.\n\n\u2022 Our solution sheds light on a broader scope of fields that also need personalization (e.g., personalized conversational systems). In addition, it points out a way for Transformer to deal with heterogeneous inputs, e.g., text and images in multimodal artificial intelligence.\n\n\nRelated Work\n\nExplainable recommendation (Zhang et al., 2014; has been studied from two major perspectives: human-computer interaction and machine learning. The former (Gedikli et al., 2014;Chen and Wang, 2017;Chen et al., 2019b) investigates how people perceive different styles of explanations, while the latter provides explanations by designing new explainable recommendation algorithms, to which our work is more related. There exist various types of explanation styles, such as pre-defined templates (Zhang et al., 2014;Li et al., 2020a), ranked sentences (Chen et al., 2019d;, image visualizations (Chen et al., 2019c), knowledge graph paths Xian et al., 2019;, reasoning rules (Shi et al., 2020;Zhu et al., 2021), etc., among which, recently, generated natural language explanations (Ni et al., 2019;Li et al., 2020c) have received much attention, mainly owing to the advancement of natural language generation technology and the availability of textual data on recommendation platforms such as e-commerce. However, previous works mostly rely on recurrent neural networks (RNN), e.g., LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014), leaving the potentially more effective Transformer under-explored, which motivates this work.\n\nTransformer (Vaswani et al., 2017) was first brought to machine translation with the architecture of encoder-decoder. Later works Devlin et al., 2019) show that it remains effective, even when the encoder or the decoder is removed, reducing nearly half of the parameters. Under the paradigm of pre-training plus finetuning, Transformer's effectiveness has been confirmed on a wide range of tasks, including both natural language understanding and generation (Radford et al., 2018;Devlin et al., 2019;Dong et al., 2019). Particularly, it is able to perform novel tasks, e.g., arithmetic, after scaling up both the model and the training data (Radford et al., 2019;Brown et al., 2020). However, it may not be friendly to researchers who do not possess large amounts of computing resources. Instead, our work explores small unpretrained models, as they are computationally cheaper and more flexible when being adapted to new applications, e.g., personalized generation.\n\nPersonalized generation usually involves the IDs of users and items. Previous approaches typically adopt multi-layer perceptron (MLP) to encode the IDs into a context vector, from which RNN can decode a word sequence. This strategy can be found in many applications, such as review generation (Dong et al., 2017), tip generation  and explanation generation (Li et al., 2020c). However, it does not fit Transformer that relies entirely on self-attention. Probably because a proper solution to deal with heterogeneous inputs (i.e., IDs and words) is yet to be found, previous works with Transformer for personalized generation replace IDs with text segments, such as persona attributes (Zheng et al., 2020), movie titles  and item features (Ni et al., 2019), which are in the same semantic space as the word sequence to be generated. In comparison, our solution is to design an effective task that can give the IDs linguistic \n\n\nProblem Formulation\n\nThe goal of our explanation task is to generate a natural language sentence\u00ca u,i for a pair of user u and item i to justify why i is recommended to u. Meanwhile, our model PETER can also make recommendations by estimating a ratingr u,i that predicts u's preference towards i. At the testing stage, only user u and item i are used as inputs for producing both explanation and recommendation. When item features F u,i are available, our model is flexible to incorporate them by simply concatenating them at the beginning of the explanation. In this case, the features are also needed in the testing stage. In the following, we will discuss both cases.\n\n\nMethodology\n\nIn this section, we present the details of our model PETER. First, we show how to encode different types of tokens in a sequence. Then, we briefly review Transformer and introduce our revised attention masking matrix. At last, we formulate the three tasks, i.e., explanation generation, context prediction and recommendation, and integrate them into a multi-task learning framework.\n\n\nInput Representation\n\nWe first introduce our way to encode heterogeneous inputs into vector representations. As shown in Fig.  2, the input to our model is a sequence, consisting\nSource Target 1 2 < > \u01b8 1 \u01b8 2 < > \u01b8 1 \u01b8 2\nAllow to attend Prevent from attending 1 2 Figure 3: The attention masking used in our model that we call PETER masking. The orange box highlights its difference from the Left-to-Right masking.\n\nof user ID u, item ID i, features F u,i , and explanation E u,i . The user and the item serve for the purpose of personalization, i.e., aiming to make the generated explanation reflect both the user's interests and the item's attributes. The features can guide the model to talk about certain topics. For instance, a conversational recommender system may explain a recommendation's specialty to the user with the goal of knowing more about his/her preference . Since the features are not always available, in our experiments we test both cases (with and without them). When they are available, the input sequence can be represented as\nS = [u, i, f 1 , \u00b7 \u00b7 \u00b7 , f |F u,i | , e 1 , \u00b7 \u00b7 \u00b7 , e |E u,i | ],\nwhere f 1 , \u00b7 \u00b7 \u00b7 , f |F u,i | are the features and e 1 , \u00b7 \u00b7 \u00b7 , e |E u,i | are the explanation's word sequence. |F u,i | denotes the number of features and |E u,i | is the number of words in the explanation.\n\nClearly there are three types of tokens in the sequence S, i.e., users, items, and words (including features), for which we prepare three sets of randomly initialized token embeddings U, I and V respectively, besides the positional embeddings P that encode the position of each token in the sequence. Notice that, we do not add users and items to the vocabulary V, given that it costs more time to predict a word out of the huge amount of IDs (for example, millions of users and items in e-commerce). After performing embedding lookup, we can obtain the sequence's token representation [u, i, f 1 , \u00b7 \u00b7 \u00b7 , f |F u,i | , e 1 , \u00b7 \u00b7 \u00b7 , e |E u,i | ] and its positional representation [p 1 , \u00b7 \u00b7 \u00b7 , p |S| ], where |S| is the length of the sequence. The input representation of the sequence is the addition of the corresponding token representation and positional representation, denoted as S 0 = [s 0,1 , \u00b7 \u00b7 \u00b7 , s 0,|S| ].\n\n\nTransformer and Attention Masking\n\nTo enable the three tasks, we show how to modify the attention masking mechanism in Transformer (Vaswani et al., 2017). Transformer consists of L identical layers, each of which is composed of two sub-layers: multi-head self-attention and positionwise feed-forward network. The l-th layer encodes the previous layer's output S l\u22121 into S l , where l \u2208 [1, L]. In the multi-head self-attention sublayer, the computation of each attention head is also identical, and among the H heads of the l-th layer, the h-th head A l,h is computed as follows:\nA l,h = softmax( Q l,h K l,h \u221a d + M)V l,h Q l,h = S l\u22121 W Q l,h , K l,h = S l\u22121 W K l,h , V l,h = S l\u22121 W V l,h M = 0, Allow to attend \u2212\u221e, Prevent from attending (1) where S l\u22121 \u2208 R |S|\u00d7d is the (l \u2212 1)-th layer's out- put, W Q l,h , W K l,h , W V l,h \u2208 R d\u00d7 d H are projection ma- trices, d denotes the dimension of embeddings, and M \u2208 R |S|\u00d7|S| is the attention masking matrix.\nEach element in M controls whether a token in the sequence can attend to another. For example, in the unidirectional left-to-right language model (Radford et al., 2018), the lower triangular part of M is set to 0 and the remaining part \u2212\u221e, so as to allow each token to attend to past tokens (including itself), but prevent it from attending to future tokens. We call it Left-to-Right Masking. As our model is not limited to the left-to-right explanation generation task, we modify the masking mechanism to accommodate the other two tasks (i.e., context prediction and recommendation). As shown in Fig. 3, the first two tokens u and i in the sequence can attend to each other, because both context prediction and recommendation tasks need them. To echo our model, we name it PETER Masking.\n\n\nExplanation and Recommendation\n\nIn the following, we perform the three tasks, after obtaining the sequence's final representation S L = [s L,1 , \u00b7 \u00b7 \u00b7 , s L,|S| ] from Transformer. The key challenge lies in the personalization of explanation generation task, for which we design the context prediction task. For both tasks, we apply a linear layer to the final representation of each token to map it into a |V|-sized vector. As an example, after passing through this layer, s L,t becomes c t :\nc t = softmax(W v s L,t + b v )(2)\nwhere W v \u2208 R |V|\u00d7d and b v \u2208 R |V| are weight parameters. The vector c t represents the probability distribution over the vocabulary V, from which a word e with probability c e t can be sampled.\n\nExplanation Generation: We adopt the Negative Log-Likelihood (NLL) as the explanation task's loss function, and compute the mean of useritem pairs in the training set:\nL e = 1 |T | (u,i)\u2208T 1 |E u,i | |E u,i | t=1 \u2212 log c et 2+|F u,i |+t (3)\nwhere T denotes the training set. The probability c et t is offset by 2 + |F u,i | positions because the explanation is placed at the end of the sequence, and |F u,i | = 0 when the features are unavailable.\n\nAt the testing stage, along with u, i, and F u,i (if available), we feed the model a special begin-ofsequence token <bos>. From its resulting probability distribution c <bos> , the model can predict a word. For simplicity, among the many decoding methods, we opt for greedy decoding that samples the word with the largest probability. Then we can concatenate this predicted word at the end of the sequence to form a new input sequence for generating another word. We do this repeatedly until the model produces a special end-of-sequence token <eos>, or the generated explanation\u00ca u,i reaches a pre-defined length.\n\nContext Prediction: As discussed earlier, when there is only one task of explanation generation, Transformer fails to make use of user ID and item ID, resulting in identical sentences. To address this issue, we design this task to map the IDs onto the words in the explanation, so as to build a connection between them. Since the first two positions (u and i) of the sequence are allowed to attend to each other, both of their final representations absorb the information of the user and the item. Thus, we can use either of them to perform this task. Here, we use the 2nd one for better illustration in Fig. 2. Again, we adopt NLL as the loss function:\nL c = 1 |T | (u,i)\u2208T 1 |E u,i | |E u,i | t=1 \u2212 log c et 2(4)\nwhere the difference from Eq. (3) is that all predicted words are from the 2nd position, which is why they are not sequentially ordered (see Fig. 2).\n\nRating Prediction: Recommendation can be seen as a prediction problem  where the goal is to predict a scorer u,i based on the IDs of user u and item i. As both u and i in the sequence can attend to each other, their final representations capture the interaction between them. Next, we map the 1st representation s L,1 into a scalar (because the 2nd one is used for context prediction). To this end, we employ multi-layer perceptron (MLP) with one hidden layer as follows:\nr u,i = w r \u03c3(W r s L,1 + b r ) + b r(5)\nwhere W r \u2208 R d\u00d7d , b r \u2208 R d , w r \u2208 R 1\u00d7d and b r \u2208 R are weight parameters, and \u03c3(\u00b7) is the sigmoid function. Therefore, it can be seen that it is feasible to do both recommendation and explanation on Transformer. As recommendation is not the key focus of this paper, we leave its improvement in the future work. For this task, we use Mean Square Error (MSE) as the loss function:\nL r = 1 |T | (u,i)\u2208T (r u,i \u2212r u,i ) 2(6)\nwhere r u,i is the ground-truth rating.\n\nMulti-task Learning: At last, we integrate the three tasks into a multi-task learning framework whose objective function is defined as:\nJ = min \u0398 (\u03bb e L e + \u03bb c L c + \u03bb r L r )(7)\nwhere \u0398 denotes all the trainable parameters in the model, and \u03bb e , \u03bb c and \u03bb r are regularization weights that balance the learning of different tasks. In this way, the model can be trained efficiently in an end-to-end manner.\n\n\nExperimental Setup\n\n\nDatasets\n\nFor experimentation, we adopt three publicly available explainable recommendation datasets, and their data splits (Li et al., 2020c). During the splitting process, each dataset is randomly divided into training, validation and testing sets with ratio 8:1:1 for 5 times, and the training set holds at least one record for each user and each item. The three datasets are respectively from TripAdvisor  (hotel), Amazon (movies & TV) and Yelp (restaurant). Each record in the datasets is comprised of a user ID, an item ID, a rating, an explanation, and a feature. The explanations are sentences extracted from user reviews. Each explanation contains at least one item feature, e.g., bedroom, which ensures the explanation quality. Statistics of the datasets are shown in Table 1. We can see that Yelp is much larger than the other two in terms of size, making it closer to the real-world situation where there are millions of users and items.\n\n\nEvaluation Metrics\n\nTo evaluate the recommendation performance, we adopt two commonly used metrics: Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). As to explanation performance, we measure the generated explanations from two main perspectives: text quality and explainability. For the former, we adopt BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, and report BLEU-1 and BLEU-4, and Precision, Recall and F1 of ROUGE-1 and ROUGE-2. Though being widely used, BLUE and ROUGE are not flawless. For example, it is difficult for them to detect the problem of identical sentences generated by Transformer. These identical sentences might not be used as explanations, because they are less likely to well explain the special property of different recommendations. To quantitatively measure how severe the problem is, we adopt USR that computes the Unique Sentence Ratio of generated sentences (Li et al., 2020c).\n\nText quality, however, is not equal to explainbility. In the case of explainable recommendation, users may value more an explanation that justifies a recommendation's advantages on certain features (Li et al., 2020c;Chen et al., 2019a). To this end, we adopt the other three metrics proposed by (Li et al., 2020c): Feature Matching Ratio (FMR), Feature Coverage Ratio (FCR) and Feature Diversity (DIV). FMR measures whether a generated explanation contains the feature in the ground-truth. FCR is computed as the number of distinct features contained in all the generated explanations, divided by the total number of features in the whole dataset. DIV measures the intersection of features between any two generated explanations.\n\nFor RMSE, MAE and DIV, the lower, the better, while it is opposite for the rest of metrics.\n\n\nCompared Methods\n\nWe introduce baselines, first for explanation and then for recommendation. For the former, we divide the baselines into two groups, depending on whether the feature is used or not.\n\nThe following models leverage only user and item IDs to generate explanations (without feature). We denote our model without feature as PETER.\n\n\u2022 Transformer (Vaswani et al., 2017) performs the explanation generation task by treating user and item IDs as words. We also tested encoder-decoder Transformer, where the encoder encodes the IDs for the decoder to decode, but its results turned out to be the same, so we do not report it.\n\n\u2022 NRT  can predict a rating and generate a tip simultaneously based on user and item IDs. We take the explanations in the datasets as tips. Moreover, we found that the model's problem of generating identical sentences (as reported in Li et al., 2020c) is caused by the L2 regularization in its original design. For fair comparison, we removed it.\n\n\u2022 Att2Seq (Dong et al., 2017) is a review generation approach and we take the explanations as reviews. This model has an attention module, but we found that it makes the generated content unreadable in the task. To be fair, we removed it as well.\n\nWhen features are used, we denote our model as PETER+, and compare it with two recent models:\n\n\u2022 ACMLM (Ni et al., 2019) is a fine-tuned BERT (Devlin et al., 2019), where an attention layer is introduced to encode the features from both the user and the item. By predicting masked tokens, this model can produce diverse sentences.\n\n\u2022 NETE (Li et al., 2020c) is a tailored GRU (Cho et al., 2014) that incorporates a given  feature into the decoding process to generate template-like explanations. It can also make recommendations.\n\nFor recommendation, besides NRT and NETE, we include another two traditional methods:\n\n\u2022 PMF (Mnih and Salakhutdinov, 2007) is a standard probabilistic matrix factorization method that characterizes users and items by latent factors.\n\n\u2022 SVD++ (Koren, 2008) leverages a user's interacted items to enhance the latent factors.\n\n\nImplementation Details\n\nWe train each model on the training set, tune the hyper-parameters on the validation set, and report the performance on the testing set. The results are averaged on the 5 data splits. We adopt the codes of ACMLM and NETE, and implement all the other methods. For NRT, Att2Seq, NETE and our PE-TER and PETER+, we set the size of vocabulary to 20,000 by keeping the most frequent words. We do not apply this to Transformer, otherwise users  and items (regarded as words) may be filtered out. We set both the number of context words and the length of explanations to 15, because the mean length of explanations is approximately 13 (see Table 1). ACMLM adopts sub-words, so we do not apply the above two steps to it. We reuse the other default settings of the baselines. For Transformer, PETER and PETER+, we set the embedding size d to 512 and the dimension of feed-forward network to 2,048, following (Vaswani et al., 2017), but the number of layers L and attention heads H are both 2. For our models PETER and PETER+, we set the regularization weights \u03bb e , \u03bb c and \u03bb r to 1.0, 1.0 and 0.1, respectively. We optimize the model via stochastic gradient descent (Robbins and Monro, 1951), and apply gradient clipping (Pascanu et al., 2013) with a threshold of  the rooms are spacious and the bathroom has a large tub PETER <eos> the and a pool was with nice is very were to good in of the pool area is nice and the gym is very well equipped <eos> PETER+ <eos> the and a was pool with to nice good very were is of in the rooms were clean and comfortable <eos> Ground-truth beautiful lobby and nice bar PETER <eos> the and a was were separate bathroom with shower large very had in is the bathroom was large and the shower was great <eos> PETER+ <eos> the and a was bathroom shower with large in separate were room very is the lobby was very nice and the rooms were very comfortable <eos> 6 Results and Analysis\n\n\nQuantitative Analysis on Explanations\n\nIn Table 2, we compare the performance of explanation generation methods in two groups. We first analyze models that make use of item features (i.e., ACMLM, NETE and PETER+). Our PETER+ consistently and significantly outperforms ACMLM and NETE on the three datasets in terms of text quality (BLEU and ROUGE). This shows the effectiveness of our model in generating high-quality sentences. Notice that Li et al. (2020b) conducted a user survey and reported that NETE's explanations were perceived useful by most participants. It suggests that our model's explanations with better quality could also be very useful to real users. Again, in terms of text quality, the performance gap between PETER+ and ACMLM (a fine-tuned BERT) is extremely large, because the latter's generation is achieved by predicting masked tokens, which is quite different from word-by-word generation. This may explain why ACMLM produces diverse sentences (high USR), which, however, is less meaningful when text quality cannot be guaranteed. Furthermore, PETER+ beats both ACMLM and NETE on the explainability metric FMR that cares about whether a generated explanation mentions the feature in the ground-truth. This is quite useful in real-world applications when the system is asked to explain a particular feature. Regarding the other two explainability metrics FCR and DIV, PETER+ is also very competitive. ACMLM gains better performance on some cases, because at the training stage it is exposed to more features (from both the user and the item), which is unfair to both PETER+ and NETE.\n\nNext, we discuss the results of the models that  only leverage user and item IDs for generation. As it can be seen, Transformer generates identical explanations on each dataset, resulting in nearly 0 score on Unique Sentence Ratio (USR). Owing to the context prediction task, our PETER successfully addresses this issue, producing diverse (comparable USR) and high-quality (best BLEU-4) sentences. In particular, on the largest dataset Yelp, it achieves the best performance on most of the metrics. This again demonstrates the effectiveness of our model. On Amazon and TripAdvisor, NRT and Att2Seq are very competitive, because we fixed their generation issues (see Section 5.3).\n\nIn addition, the two datasets are small and thus the training samples are limited, so our model may underfit, which is why it does not always reach the best performance. Besides explanation performance, we also investigate the efficiency of different Transformer-based models. On the same machine (NVIDIA Tesla P40) and dataset (TripAdvisor), we compare the training minutes of ACMLM and our PETER+ in Table  3. Compared with ACMLM, our model takes less time to train (2.3 minutes per epoch), since it has only 2 layers and thus less parameters. But because it is unpretrained and learned from scratch, it needs more training epochs.\n\n\nQualitative Case Study on Explanations\n\nIn  tic meanings, as well as achieving certain degree of personalization for natural language generation. Among the commonly used context words, e.g., the, there are some important features (underlined), according to which the model then generates an explanation that talks about them. Admittedly, there is still much room for improvement of the context prediction task, so as to more accurately predict the features in the ground-truth (e.g., rooms vs. pool in the first example). One alternative is to leverage the features to guide the model's generation. This explains why PETER+ is able to generate an explanation that talks about rooms rather than pool, making it semantically closer to the ground-truth. It thus demonstrates our model's flexibility in incorporating these features.\n\n\nRecommendation Performance\n\nTable 5 presents the performance comparison of different recommendation methods. On the largest dataset Yelp with approximately 1.3 million records, our model PETER performs as good as the three competitive baselines (i.e., SVD++, NRT and NETE), which shows the rationale of our recommendation module. Since our model PETER has more parameters to learn, it may underfit on small datasets. This explains why it does not always perform the best on TripAdvisor and Amazon. When more training data are available to Transformer, usually the performance will become better, as evidenced by GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020). Thus, we can expect our model to perform well in real-world applications, where the training data are bigger than the testing datasets, e.g., billion-scale users in Amazon.\n\n\nAblation Study\n\nIn Table 6, we provide an ablation study conducted on the TripAdvisor dataset. After disabling the context prediction task L c by setting \u03bb c = 0, the performances of both explainability and text quality drop dramatically, and the unique sentence ratio (USR) is nearly approaching Transformer's (see Table 2). It hence confirms this task's effectiveness.\n\nAs L c is highly correlated with the recommendation task L r via the user and item IDs (see Section 4.3), the removal of L c leads to slight improvement on recommendation performance. We can also observe a reversed phenomenon when we disable L r . When PETER masking is replaced by the Left-to-Right masking that prevents the model from accessing the item information, the recommendation performance drops sharply. Overall, PETER reaches an optimal situation, where its explainability, text quality and recommendation performance are all reasonably good.\n\n\nConclusion\n\nWe propose a simple and effective solution to address the personalized generation problem of Transformer, unleashing its language modeling power to generate explanations for recommender systems. Extensive experiments show that the solution is both effective and efficient. It opens up a new way of exploiting Transformer by designing good tasks instead of scaling up model size. There are various applications of personalized generation for which Transformer is still less explored. Our next step is to adopt our solution for personalized question answering systems and personalized conversational agents. We also plan to incorporate item images into the model, so as to generate visual explanations for recommendations, since \"a picture is worth a thousand words\". Another meaningful extension is to adapt the model to cross-lingual explanation generation, because international platforms, e.g., Amazon, may serve users who speak different languages.\n\n\n(a) Standard Transformer model, where the user and the item have no contribution to each generation step.Item] \n<bos> \nthe \nhotel \nis \nlocated \n\nin \nthe \nheart \nof \nthe \ncity \nand \nthe \ncity \ncentre \nis \n\nSource \n\n[User] \n[Item] \nthe \nhotel \nis \nlocated \nin \nthe \nheart \nof \nthe \ncity \nand \nthe \ncity \ncentre \nis \n<eos> \n\nTarget \n\n0.0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\n[User] \n[Item] \n<bos> \nthe \npool \narea \nis \nnice \nand \nthe \ngym \nis \nvery \nwell \nequipped \n\nSource \n\n[Rating] \n[Context] \nthe \npool \narea \nis \nnice \nand \nthe \ngym \nis \nvery \nwell \nequipped \n<eos> \n\nTarget \n\n0.0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\n(b) Our PETER model, where the user and item IDs play \nsignificant roles in the generation steps. \n\n\n\n\nFigure 2: Our proposed model PETER that contains three tasks. The input features are optional. meanings, thus connecting IDs with words.Transformer with Layers \n\nTransformer with Layers \n\nLinear Layer \n\n\u01b8 , \n\u01b8 \n\n1 \n\n\u01b8 2 < \u0dde > \n\n< \n> \n\u01b8 2 \n\u01b8 \n\n1 \n2 \n1 \n\n,1 \n,2 \n,5 \n,6 \n,7 \n,3 \n,4 \n\nUser Item \nFeatures (opt.) \nExplanation \n\nRating \nPrediction \n\nContext Prediction \n\nExplanation Generation \n\nMLP \n\n\u01c1 \n\n1 \n\n\u01c1 2 < \u0de6 > \n\n\n\nTable 1 :\n1Statistics of the three datasets.\n\nTable 2 :\n2Performance comparison of the generation methods in terms of Explainability and Text Quality on three datasets. The methods are divided into two groups according to whether features are used or not.B1 and B4 stand \n\n\nTable 3 :\n3Efficiency comparison of two Transformer-\nbased models in terms of training minutes on the Tri-\npAdvisor dataset, tested on NVIDIA Tesla P40. \n\n\n\nTable 4 :\n4Context words and explanations on two different cases as generated by our PETER and PETER+ on Tri-pAdvisor dataset. The boldfaced words in the ground-truth are the key features. Generated features are underlined.1.0. The batch size is set to 128, and the learning \nrate 1.0. At each epoch, we save the model if it \nachieves the lowest loss on the validation set, but \nwhen there is no improvement, we decrease the \nlearning rate by a factor of 0.25. When the latter \nhappens for 5 times, we stop training and load the \nsaved model for prediction. \n\n\n\nTable 5 :\n5Recommendation performance comparison in terms of RMSE (R for short) and MAE (denoted as M). The best performing values are boldfaced.\n\nTable 4 ,\n4we present two examples generated by PETER and PETER+ on the TripAdvisor dataset. We can see that PETER generates distinct context words and explanations for different user-item pairs. This confirms that our proposed solution can indeed endow the user and item IDs with linguis-Explainability \nText Quality \nRecommendation \nFMR \nFCR \nDIV \nUSR \nBLEU-1 BLEU-4 RMSE MAE \nDisable Lc \n0.06 \u2193 0.03 \u2193 5.75 \u2193 0.01 \u2193 15.37 \u2193 \n0.86 \u2193 \n0.80 \u2191 \n0.61 \u2191 \nDisable Lr \n0.07 \n0.14 \u2191 2.90 \u2191 0.10 \u2191 16.16 \u2191 \n1.15 \u2191 \n3.23 \u2193 \n3.10 \u2193 \nLeft-to-Right Masking 0.07 \n0.15 \u2191 2.68 \u2191 0.12 \u2191 15.73 \u2193 \n1.11 \n0.87 \u2193 \n0.68 \u2193 \nPETER \n0.07 \n0.13 \n2.95 \n0.08 \n15.96 \n1.11 \n0.81 \n0.63 \n\n\n\nTable 6 :\n6Ablation study on the smallest dataset TripAdvisor. Arrows \u2191 and \u2193 respectively denote the performance increase and decrease compared with PETER.\nAcknowledgmentsThis work was partially supported by HKBU IRCMS/19-20/D05, RGC/HKBU12201620, and NSF IIS-1910154 and IIS-2007907. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.\nLearning heterogeneous knowledge base embeddings for explainable recommendation. Qingyao Ai, Vahid Azizi, Xu Chen, Yongfeng Zhang, Algorithms. 119137Qingyao Ai, Vahid Azizi, Xu Chen, and Yongfeng Zhang. 2018. Learning heterogeneous knowledge base embeddings for explainable recommendation. Algorithms, 11(9):137.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, Advances in neural information processing systems. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in neural information process- ing systems.\n\nGenerate natural language explanations for recommendation. Hanxiong Chen, Xu Chen, Shaoyun Shi, Yongfeng Zhang, Proceedings of SI-GIR'19 Workshop on ExplainAble Recommendation and Search. SI-GIR'19 Workshop on ExplainAble Recommendation and SearchACMHanxiong Chen, Xu Chen, Shaoyun Shi, and Yongfeng Zhang. 2019a. Generate natural language explana- tions for recommendation. In Proceedings of SI- GIR'19 Workshop on ExplainAble Recommendation and Search. ACM.\n\nNeural collaborative reasoning. Hanxiong Chen, Shaoyun Shi, Yunqi Li, Yongfeng Zhang, Proceedings of The Web Conference. The Web ConferenceHanxiong Chen, Shaoyun Shi, Yunqi Li, and Yongfeng Zhang. 2021. Neural collaborative reasoning. In Proceedings of The Web Conference 2021.\n\nExplaining recommendations based on feature sentiments in product reviews. Li Chen, Feng Wang, Proceedings of the 22nd International Conference on Intelligent User Interfaces. the 22nd International Conference on Intelligent User InterfacesLi Chen and Feng Wang. 2017. Explaining recom- mendations based on feature sentiments in product reviews. In Proceedings of the 22nd International Conference on Intelligent User Interfaces, pages 17- 28.\n\nUser evaluations on sentiment-based recommendation explanations. Li Chen, Dongning Yan, Feng Wang, ACM Transactions on Interactive Intelligent Systems (TiiS). 94Li Chen, Dongning Yan, and Feng Wang. 2019b. User evaluations on sentiment-based recommendation ex- planations. ACM Transactions on Interactive Intelli- gent Systems (TiiS), 9(4):1-38.\n\nPersonalized fashion recommendation with visual explanations based on multimodal attention network: Towards visually explainable recommendation. Xu Chen, Hanxiong Chen, Hongteng Xu, Yongfeng Zhang, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalYixin Cao, Zheng Qin, and Hongyuan ZhaXu Chen, Hanxiong Chen, Hongteng Xu, Yongfeng Zhang, Yixin Cao, Zheng Qin, and Hongyuan Zha. 2019c. Personalized fashion recommendation with visual explanations based on multimodal attention network: Towards visually explainable recommen- dation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval, pages 765-774.\n\nDynamic explainable recommendation based on neural attentive models. Xu Chen, Yongfeng Zhang, Zheng Qin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Xu Chen, Yongfeng Zhang, and Zheng Qin. 2019d. Dy- namic explainable recommendation based on neural attentive models. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 33, pages 53-60.\n\nTowards explainable conversational recommendation. Zhongxia Chen, Xiting Wang, Xing Xie, Mehul Parsana, Akshay Soni, Xiang Ao, Enhong Chen, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial IntelligenceZhongxia Chen, Xiting Wang, Xing Xie, Mehul Parsana, Akshay Soni, Xiang Ao, and Enhong Chen. 2020. Towards explainable conversational recom- mendation. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelli- gence.\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1724-1734.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2019 Annual Conference of the North American Chapter. Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In 2019 Annual Conference of the North Amer- ican Chapter of the Association for Computational Linguistics.\n\nLearning to generate product reviews from attributes. Li Dong, Shaohan Huang, Furu Wei, Mirella Lapata, Ming Zhou, Ke Xu, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European Chapter1Long PapersLi Dong, Shaohan Huang, Furu Wei, Mirella Lapata, Ming Zhou, and Ke Xu. 2017. Learning to generate product reviews from attributes. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 623-632.\n\nUnified language model pre-training for natural language understanding and generation. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Advances in Neural Information Processing Systems. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi- aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understand- ing and generation. In Advances in Neural Informa- tion Processing Systems, pages 13063-13075.\n\nFairness-aware explainable recommendation over knowledge graphs. Zuohui Fu, Yikun Xian, Ruoyuan Gao, Jieyu Zhao, Qiaoying Huang, Yingqiang Ge, Shuyuan Xu, Shijie Geng, Chirag Shah, Yongfeng Zhang, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalZuohui Fu, Yikun Xian, Ruoyuan Gao, Jieyu Zhao, Qiaoying Huang, Yingqiang Ge, Shuyuan Xu, Shi- jie Geng, Chirag Shah, Yongfeng Zhang, et al. 2020. Fairness-aware explainable recommendation over knowledge graphs. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\nHow should i explain? a comparison of different explanation types for recommender systems. Fatih Gedikli, Dietmar Jannach, Mouzhi Ge, International Journal of Human-Computer Studies. 724Fatih Gedikli, Dietmar Jannach, and Mouzhi Ge. 2014. How should i explain? a comparison of different explanation types for recommender systems. In- ternational Journal of Human-Computer Studies, 72(4):367-382.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.\n\nFactorization meets the neighborhood: a multifaceted collaborative filtering model. Yehuda Koren, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data miningYehuda Koren. 2008. Factorization meets the neighbor- hood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD interna- tional conference on Knowledge discovery and data mining, pages 426-434.\n\nTowards personalized review summarization via useraware sequence network. Junjie Li, Haoran Li, Chengqing Zong, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Junjie Li, Haoran Li, and Chengqing Zong. 2019. To- wards personalized review summarization via user- aware sequence network. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 33, pages 6690-6697.\n\nCaesar: context-aware explanation based on supervised attention for service recommendations. Lei Li, Li Chen, Ruihai Dong, Journal of Intelligent Information Systems. Lei Li, Li Chen, and Ruihai Dong. 2020a. Caesar: context-aware explanation based on supervised at- tention for service recommendations. Journal of In- telligent Information Systems, pages 1-24.\n\nTowards controllable explanation generation for recommender systems via neural template. Lei Li, Li Chen, Yongfeng Zhang, Companion Proceedings of the Web Conference 2020. Lei Li, Li Chen, and Yongfeng Zhang. 2020b. To- wards controllable explanation generation for recom- mender systems via neural template. In Compan- ion Proceedings of the Web Conference 2020, pages 198-202.\n\nGenerate neural template explanations for recommendation. Lei Li, Yongfeng Zhang, Li Chen, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementLei Li, Yongfeng Zhang, and Li Chen. 2020c. Gen- erate neural template explanations for recommenda- tion. In Proceedings of the 29th ACM International Conference on Information & Knowledge Manage- ment, pages 755-764.\n\nExtra: Explanation ranking datasets for explainable recommendation. Lei Li, Yongfeng Zhang, Li Chen, Proceedings of the 44th International ACM SIGIR conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR conference on Research and Development in Information RetrievalLei Li, Yongfeng Zhang, and Li Chen. 2021. Extra: Explanation ranking datasets for explainable recom- mendation. In Proceedings of the 44th International ACM SIGIR conference on Research and Develop- ment in Information Retrieval.\n\nNeural rating regression with abstractive tips generation for recommendation. Piji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, Wai Lam, Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR conference on Research and Development in Information RetrievalPiji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, and Wai Lam. 2017. Neural rating regression with ab- stractive tips generation for recommendation. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Infor- mation Retrieval, pages 345-354.\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.\n\nGenerating wikipedia by summarizing long sequences. J Peter, Mohammad Liu, Etienne Saleh, Ben Pot, Ryan Goodrich, Lukasz Sepassi, Noam Kaiser, Shazeer, The Sixth International Conference on Learning Representations. Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summariz- ing long sequences. In The Sixth International Con- ference on Learning Representations.\n\nProbabilistic matrix factorization. Andriy Mnih, Russ R Salakhutdinov, Advances in neural information processing systems. Andriy Mnih and Russ R Salakhutdinov. 2007. Proba- bilistic matrix factorization. In Advances in neural information processing systems, pages 1257-1264.\n\nJustifying recommendations using distantly-labeled reviews and fine-grained aspects. Jianmo Ni, Jiacheng Li, Julian Mcauley, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 188-197.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting of the Association for Compu- tational Linguistics, pages 311-318.\n\nOn the difficulty of training recurrent neural networks. Razvan Pascanu, Tomas Mikolov, Yoshua Bengio, International conference on machine learning. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318.\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya SutskeverAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nA stochastic approximation method. The annals of mathematical statistics. Herbert Robbins, Sutton Monro, Herbert Robbins and Sutton Monro. 1951. A stochastic approximation method. The annals of mathematical statistics, pages 400-407.\n\nNeural logic reasoning. Shaoyun Shi, Hanxiong Chen, Weizhi Ma, Jiaxin Mao, Min Zhang, Yongfeng Zhang, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementShaoyun Shi, Hanxiong Chen, Weizhi Ma, Jiaxin Mao, Min Zhang, and Yongfeng Zhang. 2020. Neural logic reasoning. In Proceedings of the 29th ACM International Conference on Information & Knowl- edge Management, pages 1365-1374.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.\n\nReinforcement knowledge graph reasoning for explainable recommendation. Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard De Melo, Yongfeng Zhang, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalYikun Xian, Zuohui Fu, S Muthukrishnan, Gerard De Melo, and Yongfeng Zhang. 2019. Reinforce- ment knowledge graph reasoning for explainable recommendation. In Proceedings of the 42nd Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 285- 294.\n\nCafe: Coarse-to-fine neural symbolic reasoning for explainable recommendation. Yikun Xian, Zuohui Fu, Handong Zhao, Yingqiang Ge, Xu Chen, Qiaoying Huang, Shijie Geng, Zhou Qin, Gerard De Melo, Shan Muthukrishnan, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementYikun Xian, Zuohui Fu, Handong Zhao, Yingqiang Ge, Xu Chen, Qiaoying Huang, Shijie Geng, Zhou Qin, Gerard De Melo, Shan Muthukrishnan, et al. 2020. Cafe: Coarse-to-fine neural symbolic reasoning for explainable recommendation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 1645-1654.\n\nExplainable recommendation: A survey and new perspectives. Yongfeng Zhang, Xu Chen, Foundations and Trends R in Information Retrieval. 141Yongfeng Zhang and Xu Chen. 2020. Explainable recommendation: A survey and new perspectives. Foundations and Trends R in Information Retrieval, 14(1):1-101.\n\nTowards conversational search and recommendation: System ask, user respond. Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, W Bruce Croft, Proceedings of the 27th ACM International Conference on Information and Knowledge Management. the 27th ACM International Conference on Information and Knowledge ManagementYongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W Bruce Croft. 2018. Towards conversational search and recommendation: System ask, user re- spond. In Proceedings of the 27th ACM Interna- tional Conference on Information and Knowledge Management, pages 177-186.\n\nExplicit factor models for explainable recommendation based on phrase-level sentiment analysis. Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, Shaoping Ma, Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. the 37th international ACM SIGIR conference on Research & development in information retrievalYongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping Ma. 2014. Explicit fac- tor models for explainable recommendation based on phrase-level sentiment analysis. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval, pages 83-92.\n\nA pre-training based personalized dialogue generation model with persona-sparse data. Yinhe Zheng, Rongsheng Zhang, Minlie Huang, Xiaoxi Mao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceYinhe Zheng, Rongsheng Zhang, Minlie Huang, and Xiaoxi Mao. 2020. A pre-training based personal- ized dialogue generation model with persona-sparse data. In Proceedings of the AAAI Conference on Ar- tificial Intelligence, pages 9693-9700.\n\nImproving conversational recommender systems via knowledge graph based semantic fusion. Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, Jingsong Yu, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningKun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuan- hang Zhou, Ji-Rong Wen, and Jingsong Yu. 2020. Improving conversational recommender systems via knowledge graph based semantic fusion. In Pro- ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Min- ing, pages 1006-1014.\n\nFaithfully explainable recommendation via neural logic reasoning. Yaxin Zhu, Yikun Xian, Zuohui Fu, Gerard De Melo, Yongfeng Zhang, 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Yaxin Zhu, Yikun Xian, Zuohui Fu, Gerard de Melo, and Yongfeng Zhang. 2021. Faithfully explainable recommendation via neural logic reasoning. In 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":127,\"start\":75},{\"end\":180,\"start\":128},{\"end\":285,\"start\":181}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":79},{\"end\":142,\"start\":137},{\"end\":188,\"start\":184}]", "author_first_name": "[{\"end\":78,\"start\":75},{\"end\":136,\"start\":128},{\"end\":183,\"start\":181}]", "author_affiliation": "[{\"end\":126,\"start\":83},{\"end\":179,\"start\":144},{\"end\":284,\"start\":241}]", "title": "[{\"end\":56,\"start\":1},{\"end\":341,\"start\":286}]", "venue": "[{\"end\":505,\"start\":343}]", "abstract": "[{\"end\":1860,\"start\":674}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2094,\"start\":2074},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2111,\"start\":2094},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2150,\"start\":2131},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2968,\"start\":2946},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3075,\"start\":3053},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3095,\"start\":3075},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3114,\"start\":3095},{\"end\":4294,\"start\":4288},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5400,\"start\":5382},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5649,\"start\":5632},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6728,\"start\":6708},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6857,\"start\":6835},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6877,\"start\":6857},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6896,\"start\":6877},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7193,\"start\":7173},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7210,\"start\":7193},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7249,\"start\":7229},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7292,\"start\":7272},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7334,\"start\":7316},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7370,\"start\":7352},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7387,\"start\":7370},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7475,\"start\":7458},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7492,\"start\":7475},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7799,\"start\":7765},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7826,\"start\":7808},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7957,\"start\":7935},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8073,\"start\":8053},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8403,\"start\":8381},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8423,\"start\":8403},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8441,\"start\":8423},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8586,\"start\":8564},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8605,\"start\":8586},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9203,\"start\":9184},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9266,\"start\":9248},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9595,\"start\":9575},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9646,\"start\":9629},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13293,\"start\":13271},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14270,\"start\":14248},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19103,\"start\":19085},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20257,\"start\":20234},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20302,\"start\":20291},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20881,\"start\":20863},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21100,\"start\":21082},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21119,\"start\":21100},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21197,\"start\":21179},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22089,\"start\":22067},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22595,\"start\":22578},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22720,\"start\":22702},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23059,\"start\":23043},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23103,\"start\":23082},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23296,\"start\":23279},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23334,\"start\":23316},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23594,\"start\":23564},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23727,\"start\":23714},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24742,\"start\":24720},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25004,\"start\":24979},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25056,\"start\":25034},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26186,\"start\":26169},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30124,\"start\":30102},{\"end\":30155,\"start\":30129}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32937,\"start\":32225},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33357,\"start\":32938},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33403,\"start\":33358},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33631,\"start\":33404},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":33788,\"start\":33632},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34350,\"start\":33789},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":34497,\"start\":34351},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":35160,\"start\":34498},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":35318,\"start\":35161}]", "paragraph": "[{\"end\":2932,\"start\":1876},{\"end\":3999,\"start\":2934},{\"end\":4610,\"start\":4001},{\"end\":5788,\"start\":4612},{\"end\":5828,\"start\":5790},{\"end\":6090,\"start\":5830},{\"end\":6386,\"start\":6092},{\"end\":6664,\"start\":6388},{\"end\":7921,\"start\":6681},{\"end\":8889,\"start\":7923},{\"end\":9815,\"start\":8891},{\"end\":10488,\"start\":9839},{\"end\":10886,\"start\":10504},{\"end\":11067,\"start\":10911},{\"end\":11303,\"start\":11110},{\"end\":11939,\"start\":11305},{\"end\":12215,\"start\":12006},{\"end\":13137,\"start\":12217},{\"end\":13720,\"start\":13175},{\"end\":14890,\"start\":14102},{\"end\":15386,\"start\":14925},{\"end\":15617,\"start\":15422},{\"end\":15786,\"start\":15619},{\"end\":16066,\"start\":15860},{\"end\":16681,\"start\":16068},{\"end\":17336,\"start\":16683},{\"end\":17547,\"start\":17398},{\"end\":18020,\"start\":17549},{\"end\":18445,\"start\":18062},{\"end\":18527,\"start\":18488},{\"end\":18664,\"start\":18529},{\"end\":18937,\"start\":18709},{\"end\":19910,\"start\":18971},{\"end\":20882,\"start\":19933},{\"end\":21613,\"start\":20884},{\"end\":21706,\"start\":21615},{\"end\":21907,\"start\":21727},{\"end\":22051,\"start\":21909},{\"end\":22342,\"start\":22053},{\"end\":22690,\"start\":22344},{\"end\":22938,\"start\":22692},{\"end\":23033,\"start\":22940},{\"end\":23270,\"start\":23035},{\"end\":23469,\"start\":23272},{\"end\":23556,\"start\":23471},{\"end\":23704,\"start\":23558},{\"end\":23794,\"start\":23706},{\"end\":25726,\"start\":23821},{\"end\":27334,\"start\":25768},{\"end\":28015,\"start\":27336},{\"end\":28650,\"start\":28017},{\"end\":29481,\"start\":28693},{\"end\":30329,\"start\":29512},{\"end\":30702,\"start\":30348},{\"end\":31258,\"start\":30704},{\"end\":32224,\"start\":31273}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11109,\"start\":11068},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12005,\"start\":11940},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14101,\"start\":13721},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15421,\"start\":15387},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15859,\"start\":15787},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17397,\"start\":17337},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18061,\"start\":18021},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18487,\"start\":18446},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18708,\"start\":18665}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":2912,\"start\":2905},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":3956,\"start\":3949},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19746,\"start\":19739},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24461,\"start\":24454},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25778,\"start\":25771},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28427,\"start\":28419},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":30358,\"start\":30351},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30655,\"start\":30648}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1874,\"start\":1862},{\"attributes\":{\"n\":\"2\"},\"end\":6679,\"start\":6667},{\"attributes\":{\"n\":\"3\"},\"end\":9837,\"start\":9818},{\"attributes\":{\"n\":\"4\"},\"end\":10502,\"start\":10491},{\"attributes\":{\"n\":\"4.1\"},\"end\":10909,\"start\":10889},{\"attributes\":{\"n\":\"4.2\"},\"end\":13173,\"start\":13140},{\"attributes\":{\"n\":\"4.3\"},\"end\":14923,\"start\":14893},{\"attributes\":{\"n\":\"5\"},\"end\":18958,\"start\":18940},{\"attributes\":{\"n\":\"5.1\"},\"end\":18969,\"start\":18961},{\"attributes\":{\"n\":\"5.2\"},\"end\":19931,\"start\":19913},{\"attributes\":{\"n\":\"5.3\"},\"end\":21725,\"start\":21709},{\"attributes\":{\"n\":\"5.4\"},\"end\":23819,\"start\":23797},{\"attributes\":{\"n\":\"6.1\"},\"end\":25766,\"start\":25729},{\"attributes\":{\"n\":\"6.2\"},\"end\":28691,\"start\":28653},{\"attributes\":{\"n\":\"6.3\"},\"end\":29510,\"start\":29484},{\"attributes\":{\"n\":\"6.4\"},\"end\":30346,\"start\":30332},{\"attributes\":{\"n\":\"7\"},\"end\":31271,\"start\":31261},{\"end\":33368,\"start\":33359},{\"end\":33414,\"start\":33405},{\"end\":33642,\"start\":33633},{\"end\":33799,\"start\":33790},{\"end\":34361,\"start\":34352},{\"end\":34508,\"start\":34499},{\"end\":35171,\"start\":35162}]", "table": "[{\"end\":32937,\"start\":32332},{\"end\":33357,\"start\":33076},{\"end\":33631,\"start\":33614},{\"end\":33788,\"start\":33644},{\"end\":34350,\"start\":34013},{\"end\":35160,\"start\":34788}]", "figure_caption": "[{\"end\":32332,\"start\":32227},{\"end\":33076,\"start\":32940},{\"end\":33403,\"start\":33370},{\"end\":33614,\"start\":33416},{\"end\":34013,\"start\":33801},{\"end\":34497,\"start\":34363},{\"end\":34788,\"start\":34510},{\"end\":35318,\"start\":35173}]", "figure_ref": "[{\"end\":3706,\"start\":3697},{\"end\":4954,\"start\":4947},{\"end\":11017,\"start\":11010},{\"end\":11161,\"start\":11153},{\"end\":14705,\"start\":14699},{\"end\":17293,\"start\":17287},{\"end\":17545,\"start\":17539}]", "bib_author_first_name": "[{\"end\":35697,\"start\":35690},{\"end\":35707,\"start\":35702},{\"end\":35717,\"start\":35715},{\"end\":35732,\"start\":35724},{\"end\":35970,\"start\":35962},{\"end\":35988,\"start\":35984},{\"end\":36002,\"start\":35995},{\"end\":36015,\"start\":36010},{\"end\":36033,\"start\":36025},{\"end\":36048,\"start\":36042},{\"end\":36065,\"start\":36059},{\"end\":36085,\"start\":36079},{\"end\":36099,\"start\":36093},{\"end\":36497,\"start\":36489},{\"end\":36506,\"start\":36504},{\"end\":36520,\"start\":36513},{\"end\":36534,\"start\":36526},{\"end\":36931,\"start\":36923},{\"end\":36945,\"start\":36938},{\"end\":36956,\"start\":36951},{\"end\":36969,\"start\":36961},{\"end\":37247,\"start\":37245},{\"end\":37258,\"start\":37254},{\"end\":37682,\"start\":37680},{\"end\":37697,\"start\":37689},{\"end\":37707,\"start\":37703},{\"end\":38109,\"start\":38107},{\"end\":38124,\"start\":38116},{\"end\":38139,\"start\":38131},{\"end\":38152,\"start\":38144},{\"end\":38859,\"start\":38857},{\"end\":38874,\"start\":38866},{\"end\":38887,\"start\":38882},{\"end\":39274,\"start\":39266},{\"end\":39287,\"start\":39281},{\"end\":39298,\"start\":39294},{\"end\":39309,\"start\":39304},{\"end\":39325,\"start\":39319},{\"end\":39337,\"start\":39332},{\"end\":39348,\"start\":39342},{\"end\":39873,\"start\":39864},{\"end\":39883,\"start\":39879},{\"end\":39907,\"start\":39901},{\"end\":39925,\"start\":39918},{\"end\":39941,\"start\":39936},{\"end\":39958,\"start\":39952},{\"end\":39974,\"start\":39968},{\"end\":40588,\"start\":40583},{\"end\":40605,\"start\":40597},{\"end\":40619,\"start\":40613},{\"end\":40633,\"start\":40625},{\"end\":41061,\"start\":41059},{\"end\":41075,\"start\":41068},{\"end\":41087,\"start\":41083},{\"end\":41100,\"start\":41093},{\"end\":41113,\"start\":41109},{\"end\":41122,\"start\":41120},{\"end\":41614,\"start\":41612},{\"end\":41624,\"start\":41621},{\"end\":41637,\"start\":41631},{\"end\":41648,\"start\":41644},{\"end\":41662,\"start\":41654},{\"end\":41670,\"start\":41668},{\"end\":41685,\"start\":41677},{\"end\":41695,\"start\":41691},{\"end\":41712,\"start\":41702},{\"end\":42123,\"start\":42117},{\"end\":42133,\"start\":42128},{\"end\":42147,\"start\":42140},{\"end\":42158,\"start\":42153},{\"end\":42173,\"start\":42165},{\"end\":42190,\"start\":42181},{\"end\":42202,\"start\":42195},{\"end\":42213,\"start\":42207},{\"end\":42226,\"start\":42220},{\"end\":42241,\"start\":42233},{\"end\":42883,\"start\":42878},{\"end\":42900,\"start\":42893},{\"end\":42916,\"start\":42910},{\"end\":43212,\"start\":43208},{\"end\":43231,\"start\":43225},{\"end\":43464,\"start\":43458},{\"end\":43962,\"start\":43956},{\"end\":43973,\"start\":43967},{\"end\":43987,\"start\":43978},{\"end\":44424,\"start\":44421},{\"end\":44431,\"start\":44429},{\"end\":44444,\"start\":44438},{\"end\":44782,\"start\":44779},{\"end\":44789,\"start\":44787},{\"end\":44804,\"start\":44796},{\"end\":45131,\"start\":45128},{\"end\":45144,\"start\":45136},{\"end\":45154,\"start\":45152},{\"end\":45618,\"start\":45615},{\"end\":45631,\"start\":45623},{\"end\":45641,\"start\":45639},{\"end\":46171,\"start\":46167},{\"end\":46181,\"start\":46176},{\"end\":46196,\"start\":46188},{\"end\":46208,\"start\":46202},{\"end\":46218,\"start\":46215},{\"end\":46778,\"start\":46770},{\"end\":46996,\"start\":46995},{\"end\":47012,\"start\":47004},{\"end\":47025,\"start\":47018},{\"end\":47036,\"start\":47033},{\"end\":47046,\"start\":47042},{\"end\":47063,\"start\":47057},{\"end\":47077,\"start\":47073},{\"end\":47434,\"start\":47428},{\"end\":47759,\"start\":47753},{\"end\":47772,\"start\":47764},{\"end\":47783,\"start\":47777},{\"end\":48534,\"start\":48527},{\"end\":48550,\"start\":48545},{\"end\":48563,\"start\":48559},{\"end\":48578,\"start\":48570},{\"end\":49052,\"start\":49046},{\"end\":49067,\"start\":49062},{\"end\":49083,\"start\":49077},{\"end\":49383,\"start\":49379},{\"end\":49400,\"start\":49393},{\"end\":49640,\"start\":49636},{\"end\":49657,\"start\":49650},{\"end\":49667,\"start\":49662},{\"end\":49680,\"start\":49675},{\"end\":49692,\"start\":49687},{\"end\":49705,\"start\":49701},{\"end\":49980,\"start\":49973},{\"end\":49996,\"start\":49990},{\"end\":50165,\"start\":50158},{\"end\":50179,\"start\":50171},{\"end\":50192,\"start\":50186},{\"end\":50203,\"start\":50197},{\"end\":50212,\"start\":50209},{\"end\":50228,\"start\":50220},{\"end\":50663,\"start\":50657},{\"end\":50677,\"start\":50673},{\"end\":50691,\"start\":50687},{\"end\":50705,\"start\":50700},{\"end\":50722,\"start\":50717},{\"end\":50735,\"start\":50730},{\"end\":50737,\"start\":50736},{\"end\":50751,\"start\":50745},{\"end\":50765,\"start\":50760},{\"end\":51138,\"start\":51133},{\"end\":51151,\"start\":51145},{\"end\":51157,\"start\":51156},{\"end\":51179,\"start\":51173},{\"end\":51197,\"start\":51189},{\"end\":51789,\"start\":51784},{\"end\":51802,\"start\":51796},{\"end\":51814,\"start\":51807},{\"end\":51830,\"start\":51821},{\"end\":51837,\"start\":51835},{\"end\":51852,\"start\":51844},{\"end\":51866,\"start\":51860},{\"end\":51877,\"start\":51873},{\"end\":51889,\"start\":51883},{\"end\":51892,\"start\":51890},{\"end\":51903,\"start\":51899},{\"end\":52493,\"start\":52485},{\"end\":52503,\"start\":52501},{\"end\":52806,\"start\":52798},{\"end\":52816,\"start\":52814},{\"end\":52830,\"start\":52823},{\"end\":52838,\"start\":52835},{\"end\":52852,\"start\":52845},{\"end\":53400,\"start\":53392},{\"end\":53414,\"start\":53408},{\"end\":53423,\"start\":53420},{\"end\":53433,\"start\":53431},{\"end\":53446,\"start\":53441},{\"end\":53460,\"start\":53452},{\"end\":54070,\"start\":54065},{\"end\":54087,\"start\":54078},{\"end\":54101,\"start\":54095},{\"end\":54115,\"start\":54109},{\"end\":54561,\"start\":54558},{\"end\":54573,\"start\":54568},{\"end\":54577,\"start\":54574},{\"end\":54591,\"start\":54584},{\"end\":54606,\"start\":54598},{\"end\":54620,\"start\":54613},{\"end\":54634,\"start\":54626},{\"end\":55193,\"start\":55188},{\"end\":55204,\"start\":55199},{\"end\":55217,\"start\":55211},{\"end\":55228,\"start\":55222},{\"end\":55246,\"start\":55238}]", "bib_author_last_name": "[{\"end\":35700,\"start\":35698},{\"end\":35713,\"start\":35708},{\"end\":35722,\"start\":35718},{\"end\":35738,\"start\":35733},{\"end\":35982,\"start\":35971},{\"end\":35993,\"start\":35989},{\"end\":36008,\"start\":36003},{\"end\":36023,\"start\":36016},{\"end\":36040,\"start\":36034},{\"end\":36057,\"start\":36049},{\"end\":36077,\"start\":36066},{\"end\":36091,\"start\":36086},{\"end\":36106,\"start\":36100},{\"end\":36114,\"start\":36108},{\"end\":36502,\"start\":36498},{\"end\":36511,\"start\":36507},{\"end\":36524,\"start\":36521},{\"end\":36540,\"start\":36535},{\"end\":36936,\"start\":36932},{\"end\":36949,\"start\":36946},{\"end\":36959,\"start\":36957},{\"end\":36975,\"start\":36970},{\"end\":37252,\"start\":37248},{\"end\":37263,\"start\":37259},{\"end\":37687,\"start\":37683},{\"end\":37701,\"start\":37698},{\"end\":37712,\"start\":37708},{\"end\":38114,\"start\":38110},{\"end\":38129,\"start\":38125},{\"end\":38142,\"start\":38140},{\"end\":38158,\"start\":38153},{\"end\":38864,\"start\":38860},{\"end\":38880,\"start\":38875},{\"end\":38891,\"start\":38888},{\"end\":39279,\"start\":39275},{\"end\":39292,\"start\":39288},{\"end\":39302,\"start\":39299},{\"end\":39317,\"start\":39310},{\"end\":39330,\"start\":39326},{\"end\":39340,\"start\":39338},{\"end\":39353,\"start\":39349},{\"end\":39877,\"start\":39874},{\"end\":39899,\"start\":39884},{\"end\":39916,\"start\":39908},{\"end\":39934,\"start\":39926},{\"end\":39950,\"start\":39942},{\"end\":39966,\"start\":39959},{\"end\":39981,\"start\":39975},{\"end\":40595,\"start\":40589},{\"end\":40611,\"start\":40606},{\"end\":40623,\"start\":40620},{\"end\":40643,\"start\":40634},{\"end\":41066,\"start\":41062},{\"end\":41081,\"start\":41076},{\"end\":41091,\"start\":41088},{\"end\":41107,\"start\":41101},{\"end\":41118,\"start\":41114},{\"end\":41125,\"start\":41123},{\"end\":41619,\"start\":41615},{\"end\":41629,\"start\":41625},{\"end\":41642,\"start\":41638},{\"end\":41652,\"start\":41649},{\"end\":41666,\"start\":41663},{\"end\":41675,\"start\":41671},{\"end\":41689,\"start\":41686},{\"end\":41700,\"start\":41696},{\"end\":41716,\"start\":41713},{\"end\":42126,\"start\":42124},{\"end\":42138,\"start\":42134},{\"end\":42151,\"start\":42148},{\"end\":42163,\"start\":42159},{\"end\":42179,\"start\":42174},{\"end\":42193,\"start\":42191},{\"end\":42205,\"start\":42203},{\"end\":42218,\"start\":42214},{\"end\":42231,\"start\":42227},{\"end\":42247,\"start\":42242},{\"end\":42891,\"start\":42884},{\"end\":42908,\"start\":42901},{\"end\":42919,\"start\":42917},{\"end\":43223,\"start\":43213},{\"end\":43243,\"start\":43232},{\"end\":43470,\"start\":43465},{\"end\":43965,\"start\":43963},{\"end\":43976,\"start\":43974},{\"end\":43992,\"start\":43988},{\"end\":44427,\"start\":44425},{\"end\":44436,\"start\":44432},{\"end\":44449,\"start\":44445},{\"end\":44785,\"start\":44783},{\"end\":44794,\"start\":44790},{\"end\":44810,\"start\":44805},{\"end\":45134,\"start\":45132},{\"end\":45150,\"start\":45145},{\"end\":45159,\"start\":45155},{\"end\":45621,\"start\":45619},{\"end\":45637,\"start\":45632},{\"end\":45646,\"start\":45642},{\"end\":46174,\"start\":46172},{\"end\":46186,\"start\":46182},{\"end\":46200,\"start\":46197},{\"end\":46213,\"start\":46209},{\"end\":46222,\"start\":46219},{\"end\":46782,\"start\":46779},{\"end\":47002,\"start\":46997},{\"end\":47016,\"start\":47013},{\"end\":47031,\"start\":47026},{\"end\":47040,\"start\":47037},{\"end\":47055,\"start\":47047},{\"end\":47071,\"start\":47064},{\"end\":47084,\"start\":47078},{\"end\":47093,\"start\":47086},{\"end\":47439,\"start\":47435},{\"end\":47461,\"start\":47441},{\"end\":47762,\"start\":47760},{\"end\":47775,\"start\":47773},{\"end\":47791,\"start\":47784},{\"end\":48543,\"start\":48535},{\"end\":48557,\"start\":48551},{\"end\":48568,\"start\":48564},{\"end\":48582,\"start\":48579},{\"end\":49060,\"start\":49053},{\"end\":49075,\"start\":49068},{\"end\":49090,\"start\":49084},{\"end\":49391,\"start\":49384},{\"end\":49411,\"start\":49401},{\"end\":49648,\"start\":49641},{\"end\":49660,\"start\":49658},{\"end\":49673,\"start\":49668},{\"end\":49685,\"start\":49681},{\"end\":49699,\"start\":49693},{\"end\":49715,\"start\":49706},{\"end\":49988,\"start\":49981},{\"end\":50002,\"start\":49997},{\"end\":50169,\"start\":50166},{\"end\":50184,\"start\":50180},{\"end\":50195,\"start\":50193},{\"end\":50207,\"start\":50204},{\"end\":50218,\"start\":50213},{\"end\":50234,\"start\":50229},{\"end\":50671,\"start\":50664},{\"end\":50685,\"start\":50678},{\"end\":50698,\"start\":50692},{\"end\":50715,\"start\":50706},{\"end\":50728,\"start\":50723},{\"end\":50743,\"start\":50738},{\"end\":50758,\"start\":50752},{\"end\":50776,\"start\":50766},{\"end\":51143,\"start\":51139},{\"end\":51154,\"start\":51152},{\"end\":51171,\"start\":51158},{\"end\":51187,\"start\":51180},{\"end\":51203,\"start\":51198},{\"end\":51794,\"start\":51790},{\"end\":51805,\"start\":51803},{\"end\":51819,\"start\":51815},{\"end\":51833,\"start\":51831},{\"end\":51842,\"start\":51838},{\"end\":51858,\"start\":51853},{\"end\":51871,\"start\":51867},{\"end\":51881,\"start\":51878},{\"end\":51897,\"start\":51893},{\"end\":51917,\"start\":51904},{\"end\":52499,\"start\":52494},{\"end\":52508,\"start\":52504},{\"end\":52812,\"start\":52807},{\"end\":52821,\"start\":52817},{\"end\":52833,\"start\":52831},{\"end\":52843,\"start\":52839},{\"end\":52858,\"start\":52853},{\"end\":53406,\"start\":53401},{\"end\":53418,\"start\":53415},{\"end\":53429,\"start\":53424},{\"end\":53439,\"start\":53434},{\"end\":53450,\"start\":53447},{\"end\":53463,\"start\":53461},{\"end\":54076,\"start\":54071},{\"end\":54093,\"start\":54088},{\"end\":54107,\"start\":54102},{\"end\":54119,\"start\":54116},{\"end\":54566,\"start\":54562},{\"end\":54582,\"start\":54578},{\"end\":54596,\"start\":54592},{\"end\":54611,\"start\":54607},{\"end\":54624,\"start\":54621},{\"end\":54637,\"start\":54635},{\"end\":55197,\"start\":55194},{\"end\":55209,\"start\":55205},{\"end\":55220,\"start\":55218},{\"end\":55236,\"start\":55229},{\"end\":55252,\"start\":55247}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4371098},\"end\":35921,\"start\":35609},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218971783},\"end\":36428,\"start\":35923},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":204874165},\"end\":36889,\"start\":36430},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218673854},\"end\":37168,\"start\":36891},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18712907},\"end\":37613,\"start\":37170},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":201894190},\"end\":37960,\"start\":37615},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":197928119},\"end\":38786,\"start\":37962},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":197640939},\"end\":39213,\"start\":38788},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220480950},\"end\":39767,\"start\":39215},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5590763},\"end\":40499,\"start\":39769},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":41003,\"start\":40501},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":17865105},\"end\":41523,\"start\":41005},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":147704286},\"end\":42050,\"start\":41525},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219260017},\"end\":42785,\"start\":42052},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":17471203},\"end\":43182,\"start\":42787},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1915014},\"end\":43372,\"start\":43184},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207168823},\"end\":43880,\"start\":43374},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":69778590},\"end\":44326,\"start\":43882},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":229386054},\"end\":44688,\"start\":44328},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":218522173},\"end\":45068,\"start\":44690},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":224270828},\"end\":45545,\"start\":45070},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":231986430},\"end\":46087,\"start\":45547},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":304614},\"end\":46712,\"start\":46089},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":964287},\"end\":46941,\"start\":46714},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3608234},\"end\":47390,\"start\":46943},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":467086},\"end\":47666,\"start\":47392},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":202621357},\"end\":48461,\"start\":47668},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11080756},\"end\":48987,\"start\":48463},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14650762},\"end\":49316,\"start\":48989},{\"attributes\":{\"id\":\"b29\"},\"end\":49581,\"start\":49318},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":160025533},\"end\":49897,\"start\":49583},{\"attributes\":{\"id\":\"b31\"},\"end\":50132,\"start\":49899},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":221246405},\"end\":50628,\"start\":50134},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13756489},\"end\":51059,\"start\":50630},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":186206810},\"end\":51703,\"start\":51061},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":224280561},\"end\":52424,\"start\":51705},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":13752895},\"end\":52720,\"start\":52426},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52233682},\"end\":53294,\"start\":52722},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3331952},\"end\":53977,\"start\":53296},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":207863734},\"end\":54468,\"start\":53979},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":220404390},\"end\":55120,\"start\":54470},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":233289948},\"end\":55604,\"start\":55122}]", "bib_title": "[{\"end\":35688,\"start\":35609},{\"end\":35960,\"start\":35923},{\"end\":36487,\"start\":36430},{\"end\":36921,\"start\":36891},{\"end\":37243,\"start\":37170},{\"end\":37678,\"start\":37615},{\"end\":38105,\"start\":37962},{\"end\":38855,\"start\":38788},{\"end\":39264,\"start\":39215},{\"end\":39862,\"start\":39769},{\"end\":40581,\"start\":40501},{\"end\":41057,\"start\":41005},{\"end\":41610,\"start\":41525},{\"end\":42115,\"start\":42052},{\"end\":42876,\"start\":42787},{\"end\":43206,\"start\":43184},{\"end\":43456,\"start\":43374},{\"end\":43954,\"start\":43882},{\"end\":44419,\"start\":44328},{\"end\":44777,\"start\":44690},{\"end\":45126,\"start\":45070},{\"end\":45613,\"start\":45547},{\"end\":46165,\"start\":46089},{\"end\":46768,\"start\":46714},{\"end\":46993,\"start\":46943},{\"end\":47426,\"start\":47392},{\"end\":47751,\"start\":47668},{\"end\":48525,\"start\":48463},{\"end\":49044,\"start\":48989},{\"end\":49634,\"start\":49583},{\"end\":50156,\"start\":50134},{\"end\":50655,\"start\":50630},{\"end\":51131,\"start\":51061},{\"end\":51782,\"start\":51705},{\"end\":52483,\"start\":52426},{\"end\":52796,\"start\":52722},{\"end\":53390,\"start\":53296},{\"end\":54063,\"start\":53979},{\"end\":54556,\"start\":54470},{\"end\":55186,\"start\":55122}]", "bib_author": "[{\"end\":35702,\"start\":35690},{\"end\":35715,\"start\":35702},{\"end\":35724,\"start\":35715},{\"end\":35740,\"start\":35724},{\"end\":35984,\"start\":35962},{\"end\":35995,\"start\":35984},{\"end\":36010,\"start\":35995},{\"end\":36025,\"start\":36010},{\"end\":36042,\"start\":36025},{\"end\":36059,\"start\":36042},{\"end\":36079,\"start\":36059},{\"end\":36093,\"start\":36079},{\"end\":36108,\"start\":36093},{\"end\":36116,\"start\":36108},{\"end\":36504,\"start\":36489},{\"end\":36513,\"start\":36504},{\"end\":36526,\"start\":36513},{\"end\":36542,\"start\":36526},{\"end\":36938,\"start\":36923},{\"end\":36951,\"start\":36938},{\"end\":36961,\"start\":36951},{\"end\":36977,\"start\":36961},{\"end\":37254,\"start\":37245},{\"end\":37265,\"start\":37254},{\"end\":37689,\"start\":37680},{\"end\":37703,\"start\":37689},{\"end\":37714,\"start\":37703},{\"end\":38116,\"start\":38107},{\"end\":38131,\"start\":38116},{\"end\":38144,\"start\":38131},{\"end\":38160,\"start\":38144},{\"end\":38866,\"start\":38857},{\"end\":38882,\"start\":38866},{\"end\":38893,\"start\":38882},{\"end\":39281,\"start\":39266},{\"end\":39294,\"start\":39281},{\"end\":39304,\"start\":39294},{\"end\":39319,\"start\":39304},{\"end\":39332,\"start\":39319},{\"end\":39342,\"start\":39332},{\"end\":39355,\"start\":39342},{\"end\":39879,\"start\":39864},{\"end\":39901,\"start\":39879},{\"end\":39918,\"start\":39901},{\"end\":39936,\"start\":39918},{\"end\":39952,\"start\":39936},{\"end\":39968,\"start\":39952},{\"end\":39983,\"start\":39968},{\"end\":40597,\"start\":40583},{\"end\":40613,\"start\":40597},{\"end\":40625,\"start\":40613},{\"end\":40645,\"start\":40625},{\"end\":41068,\"start\":41059},{\"end\":41083,\"start\":41068},{\"end\":41093,\"start\":41083},{\"end\":41109,\"start\":41093},{\"end\":41120,\"start\":41109},{\"end\":41127,\"start\":41120},{\"end\":41621,\"start\":41612},{\"end\":41631,\"start\":41621},{\"end\":41644,\"start\":41631},{\"end\":41654,\"start\":41644},{\"end\":41668,\"start\":41654},{\"end\":41677,\"start\":41668},{\"end\":41691,\"start\":41677},{\"end\":41702,\"start\":41691},{\"end\":41718,\"start\":41702},{\"end\":42128,\"start\":42117},{\"end\":42140,\"start\":42128},{\"end\":42153,\"start\":42140},{\"end\":42165,\"start\":42153},{\"end\":42181,\"start\":42165},{\"end\":42195,\"start\":42181},{\"end\":42207,\"start\":42195},{\"end\":42220,\"start\":42207},{\"end\":42233,\"start\":42220},{\"end\":42249,\"start\":42233},{\"end\":42893,\"start\":42878},{\"end\":42910,\"start\":42893},{\"end\":42921,\"start\":42910},{\"end\":43225,\"start\":43208},{\"end\":43245,\"start\":43225},{\"end\":43472,\"start\":43458},{\"end\":43967,\"start\":43956},{\"end\":43978,\"start\":43967},{\"end\":43994,\"start\":43978},{\"end\":44429,\"start\":44421},{\"end\":44438,\"start\":44429},{\"end\":44451,\"start\":44438},{\"end\":44787,\"start\":44779},{\"end\":44796,\"start\":44787},{\"end\":44812,\"start\":44796},{\"end\":45136,\"start\":45128},{\"end\":45152,\"start\":45136},{\"end\":45161,\"start\":45152},{\"end\":45623,\"start\":45615},{\"end\":45639,\"start\":45623},{\"end\":45648,\"start\":45639},{\"end\":46176,\"start\":46167},{\"end\":46188,\"start\":46176},{\"end\":46202,\"start\":46188},{\"end\":46215,\"start\":46202},{\"end\":46224,\"start\":46215},{\"end\":46784,\"start\":46770},{\"end\":47004,\"start\":46995},{\"end\":47018,\"start\":47004},{\"end\":47033,\"start\":47018},{\"end\":47042,\"start\":47033},{\"end\":47057,\"start\":47042},{\"end\":47073,\"start\":47057},{\"end\":47086,\"start\":47073},{\"end\":47095,\"start\":47086},{\"end\":47441,\"start\":47428},{\"end\":47463,\"start\":47441},{\"end\":47764,\"start\":47753},{\"end\":47777,\"start\":47764},{\"end\":47793,\"start\":47777},{\"end\":48545,\"start\":48527},{\"end\":48559,\"start\":48545},{\"end\":48570,\"start\":48559},{\"end\":48584,\"start\":48570},{\"end\":49062,\"start\":49046},{\"end\":49077,\"start\":49062},{\"end\":49092,\"start\":49077},{\"end\":49393,\"start\":49379},{\"end\":49413,\"start\":49393},{\"end\":49650,\"start\":49636},{\"end\":49662,\"start\":49650},{\"end\":49675,\"start\":49662},{\"end\":49687,\"start\":49675},{\"end\":49701,\"start\":49687},{\"end\":49717,\"start\":49701},{\"end\":49990,\"start\":49973},{\"end\":50004,\"start\":49990},{\"end\":50171,\"start\":50158},{\"end\":50186,\"start\":50171},{\"end\":50197,\"start\":50186},{\"end\":50209,\"start\":50197},{\"end\":50220,\"start\":50209},{\"end\":50236,\"start\":50220},{\"end\":50673,\"start\":50657},{\"end\":50687,\"start\":50673},{\"end\":50700,\"start\":50687},{\"end\":50717,\"start\":50700},{\"end\":50730,\"start\":50717},{\"end\":50745,\"start\":50730},{\"end\":50760,\"start\":50745},{\"end\":50778,\"start\":50760},{\"end\":51145,\"start\":51133},{\"end\":51156,\"start\":51145},{\"end\":51173,\"start\":51156},{\"end\":51189,\"start\":51173},{\"end\":51205,\"start\":51189},{\"end\":51796,\"start\":51784},{\"end\":51807,\"start\":51796},{\"end\":51821,\"start\":51807},{\"end\":51835,\"start\":51821},{\"end\":51844,\"start\":51835},{\"end\":51860,\"start\":51844},{\"end\":51873,\"start\":51860},{\"end\":51883,\"start\":51873},{\"end\":51899,\"start\":51883},{\"end\":51919,\"start\":51899},{\"end\":52501,\"start\":52485},{\"end\":52510,\"start\":52501},{\"end\":52814,\"start\":52798},{\"end\":52823,\"start\":52814},{\"end\":52835,\"start\":52823},{\"end\":52845,\"start\":52835},{\"end\":52860,\"start\":52845},{\"end\":53408,\"start\":53392},{\"end\":53420,\"start\":53408},{\"end\":53431,\"start\":53420},{\"end\":53441,\"start\":53431},{\"end\":53452,\"start\":53441},{\"end\":53465,\"start\":53452},{\"end\":54078,\"start\":54065},{\"end\":54095,\"start\":54078},{\"end\":54109,\"start\":54095},{\"end\":54121,\"start\":54109},{\"end\":54568,\"start\":54558},{\"end\":54584,\"start\":54568},{\"end\":54598,\"start\":54584},{\"end\":54613,\"start\":54598},{\"end\":54626,\"start\":54613},{\"end\":54639,\"start\":54626},{\"end\":55199,\"start\":55188},{\"end\":55211,\"start\":55199},{\"end\":55222,\"start\":55211},{\"end\":55238,\"start\":55222},{\"end\":55254,\"start\":55238}]", "bib_venue": "[{\"end\":36677,\"start\":36618},{\"end\":37030,\"start\":37012},{\"end\":37410,\"start\":37346},{\"end\":38369,\"start\":38273},{\"end\":39002,\"start\":38956},{\"end\":39520,\"start\":39446},{\"end\":40158,\"start\":40079},{\"end\":41230,\"start\":41187},{\"end\":42458,\"start\":42362},{\"end\":43655,\"start\":43572},{\"end\":44103,\"start\":44057},{\"end\":45328,\"start\":45253},{\"end\":45857,\"start\":45761},{\"end\":46433,\"start\":46337},{\"end\":48130,\"start\":47970},{\"end\":48745,\"start\":48673},{\"end\":50403,\"start\":50328},{\"end\":51414,\"start\":51318},{\"end\":52086,\"start\":52011},{\"end\":53031,\"start\":52954},{\"end\":53670,\"start\":53576},{\"end\":54230,\"start\":54184},{\"end\":54818,\"start\":54737},{\"end\":35750,\"start\":35740},{\"end\":36165,\"start\":36116},{\"end\":36616,\"start\":36542},{\"end\":37010,\"start\":36977},{\"end\":37344,\"start\":37265},{\"end\":37772,\"start\":37714},{\"end\":38271,\"start\":38160},{\"end\":38954,\"start\":38893},{\"end\":39444,\"start\":39355},{\"end\":40077,\"start\":39983},{\"end\":40697,\"start\":40645},{\"end\":41185,\"start\":41127},{\"end\":41767,\"start\":41718},{\"end\":42360,\"start\":42249},{\"end\":42968,\"start\":42921},{\"end\":43263,\"start\":43245},{\"end\":43570,\"start\":43472},{\"end\":44055,\"start\":43994},{\"end\":44493,\"start\":44451},{\"end\":44860,\"start\":44812},{\"end\":45251,\"start\":45161},{\"end\":45759,\"start\":45648},{\"end\":46335,\"start\":46224},{\"end\":46815,\"start\":46784},{\"end\":47157,\"start\":47095},{\"end\":47512,\"start\":47463},{\"end\":47968,\"start\":47793},{\"end\":48671,\"start\":48584},{\"end\":49136,\"start\":49092},{\"end\":49377,\"start\":49318},{\"end\":49728,\"start\":49717},{\"end\":49971,\"start\":49899},{\"end\":50326,\"start\":50236},{\"end\":50827,\"start\":50778},{\"end\":51316,\"start\":51205},{\"end\":52009,\"start\":51919},{\"end\":52559,\"start\":52510},{\"end\":52952,\"start\":52860},{\"end\":53574,\"start\":53465},{\"end\":54182,\"start\":54121},{\"end\":54735,\"start\":54639},{\"end\":55355,\"start\":55254}]"}}}, "year": 2023, "month": 12, "day": 17}