{"id": 247839511, "updated": "2023-10-05 15:43:38.915", "metadata": {"title": "R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis", "authors": "[{\"first\":\"Huan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Zeng\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Kyle\",\"last\":\"Olszewski\",\"middle\":[]},{\"first\":\"Menglei\",\"last\":\"Chai\",\"middle\":[]},{\"first\":\"Yun\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Sergey\",\"last\":\"Tulyakov\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recent research explosion on Neural Radiance Field (NeRF) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of NeRF is its prohibitive inference time: Rendering a single pixel requires querying the NeRF network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over NeRF in novel view synthesis -- the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained NeRF model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average PSNR improvement) rendering quality than NeRF without any customized parallelism requirement.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.17261", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/WangRHOCFT22", "doi": "10.48550/arxiv.2203.17261"}}, "content": {"source": {"pdf_hash": "38d0904bff20007ba2ff8134e68007217ba00439", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2203.17261v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "56de910bebe1fd176dfa840bea24a8970872e4b7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/38d0904bff20007ba2ff8134e68007217ba00439.txt", "contents": "\nR2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis\n23 Jul 2022\n\nHuan Wang \nSnap Inc\n\n\nNortheastern University\nUSA\n\nJian Ren \nSnap Inc\n\n\nZeng Huang \nSnap Inc\n\n\nKyle Olszewski \nSnap Inc\n\n\nMenglei Chai \nSnap Inc\n\n\nYun Fu \nNortheastern University\nUSA\n\nSergey Tulyakov \nSnap Inc\n\n\nR2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis\n23 Jul 2022Project: https://snap-research.github.io/R2L \u2026 NeRF: Shallow MLP, Multi-forwards x 1 x 2 x 3 \u2021 Now at Google\nNeLF: Deep Residual MLP, Single forward Camera ray PSNR: 1.4dB\u2191 Speed:~30x F \u0398 G \u03c6 1.0 1.5 NSVF (3.2) Ours (9.88) (Dot size indicates relative model size) \u22120.5 0.0 NeRF (1) DONeRF-16-noGT (1.125) TermiNeRF (1) KiloNeRF (16.21) RSEN (1.17) 0 5 10 15 20 25 \u22124.0 AutoInt (1) Speedup (x) over NeRF \u2206PNSR (dB) over NeRF (a) NeRF vs. our NeLF method (b) Speedup-PSNR-Model Size comparisonFig. 1. (a) Our neural light field (NeLF, bottom) method improves the rendering quality by 1.40 PSNR over neural radiance field (NeRF, top) [34] on the NeRF synthetic dataset, while being around 30\u00d7 faster. (b) Our method achieves a more favorable speedup-PSNR-model size tradeoff than other efficient novel view synthesis methods on the NeRF synthetic dataset. The number in the parentheses indicates the model size relative to the baseline NeRF model used in each paper (best viewed in color ).Abstract. Recent research explosion on Neural Radiance Field (NeRF) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of NeRF is its prohibitive inference time: Rendering a single pixel requires querying the NeRF network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over NeRF in novel view synthesis -the rendering of a pixel amounts to one single forward pass without * Work done when Huan was an intern at Snap 2 H. Wang et al.ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained NeRF model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26 \u223c 35\u00d7 FLOPs reduction (per camera ray) and 28 \u223c 31\u00d7 runtime speedup, meanwhile delivering significantly better (1.4 \u223c 2.8 dB average PSNR improvement) rendering quality than NeRF without any customized parallelism requirement.\n\nAbstract. Recent research explosion on Neural Radiance Field (NeRF) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of NeRF is its prohibitive inference time: Rendering a single pixel requires querying the NeRF network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over NeRF in novel view synthesis -the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained NeRF model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26 \u223c 35\u00d7 FLOPs reduction (per camera ray) and 28 \u223c 31\u00d7 runtime speedup, meanwhile delivering significantly better (1.4 \u223c 2.8 dB average PSNR improvement) rendering quality than NeRF without any customized parallelism requirement.\n\n\nIntroduction\n\nInferring the representation of a 3D scene from 2D observations is a fundamental problem in computer graphics and computer vision. Recent research innovations in implicit neural representations [10,32,36,49] and differential neural renders [34] have remarkably advanced the solutions to this problem. Neural radiance field (NeRF) learned by a simple Multi-Layer Perceptron (MLP) network shows a great potential to store a complex scene into a compact neural network [34], thus has inspired plenty of follow-up works [6,11,27,60]. Despite the success of NeRF and its extensions, the drawback is still apparent. The rendering time even for a single pixel is prolonged since the NeRF framework needs to aggregate the radiance of hundreds of sampled points via alphacomposition. It requires hundreds of network forwards, thus is prohibitively slow, especially on resource-constrained devices. One intuitive solution to the problem is to reduce the model size of NeRF MLP. However, apparent quality degradation of rendered images can be observed (e.g., reducing the network width by only half causes around 0.01 SSIM [56] drop in [42]) while the reduction of inference time is only limited. Other research efforts focus on decreasing the number of sampled points [28,35]. However, this does not fundamentally resolve the sampling issue. Some work [35] demands extra depth information for training, which is usually unavailable in most practical cases. Thus, a method that only requires 2D images as input, represents the scene compactly, and enjoys a fast rendering speed with high image quality is highly desired. This paper aims to present such a method that can achieve all the four goals simultaneously by representing the scene as Neural Light Field (NeLF) instead of neural radiance field. In the neural light field, ray origin and direction are directly mapped into its associated RGB values, avoiding the need of sampling multiple points along the camera ray. Therefore, rendering a pixel requires only one single query, making it much faster than the radiance scene representation.\n\nThe idea of NeLF is attractive; however, realizing it for representing complex real-world scenes with better quality than NeRF is still challenging. Our first key technical innovation enabling this is a novel network architecture design for the neural light field network. Specially, we propose a deep (88 layers) residual MLP network with extensive residual MLP blocks. The deep network has much greater expressivity than the shallow counterparts, thus can represent the light Table 1. Method comparison between our R2L approach and recent efficient novel view synthesis methods. Rendering speedup (measured by FLOPs reduction per ray and wall-time reduction) and representation (Repre.) size are relative to the original NeRF [34]. Repre. size measures the required storage of a neural network or cached files to represent a scene. \u2206PSNR refers to the average PSNR improvement (on the NeRF synthetic dataset) over the baseline NeRF used in each paper. Note, ours and [4] are the only two neural light field methods here field faithfully. Notably, since the debut of NeRF [34], its MLP-based network architecture is inherited with few substantial changes [6,35,42,43]. To our best knowledge, this is the first attempt to address the NeRF rendering efficiency issue from the network design perspective. Although our network contains more parameters than the original NeRF, we only need one single network forward to render the color of a pixel, leading to much faster inference speed than NeRF. The major technical problem is how to train the proposed deep residual MLP network. It is well-known that large networks hunger for large sample sizes to curb overfitting [23,52]. We can barely train such a large network using only the original 2D images (which are typically less than 100 in real-world applications).\n\nTo tackle this problem, as the second key technical innovation of this paper, we propose to distill the knowledge [8,18] from a pretrained NeRF model to our network, by rendering pseudo data from random views using the pre-trained NeRF model. We name our method as R2L since we show distilling neural Radiance filed to neural Light filed is an effective way to obtain a powerful NeLF network for efficient novel view synthesis. Empirically, we evaluate our method on both synthetic and real-world datasets. On the synthetic scenes, we achieve 26 \u223c 35\u00d7 FLOPs reduction (28 \u223c 31\u00d7 wall-time speedup) over the original NeRF with significantly higher rendering quality. Comparison between ours and other efficient novel view synthesis approaches is summarized in Tab. 1. Overall, our contributions can be summarized into the following aspects:\n\n-Methodologically, we present a brand-new deep residual MLP network aiming for compact neural representation, fast rendering, without extra demand besides 2D images, for efficient novel view synthesis. This is the first attempt to improve the rendering efficiency via network architecture optimization. -Our network represents complex real-world scenes as neural light fields. To resolve the data shortage problem when training the proposed deep MLP network, we propose an effective training strategy by distilling knowledge from a pre-trained NeRF model, which is the key to enabling our method. -Practically, our approach achieves 26 \u223c 35\u00d7 FLOPs reduction (28 \u223c 31\u00d7 wall-time speedup) over the original NeRF with even better visual quality, which also performs favorably against existing counterpart approaches.\n\n\nRelated Work\n\nEfficient neural scene representation and rendering. Since the debut of NeRF [34], many follow-up works have been improving its efficiency. One major direction is to skip the empty space and sample more wisely along a camera ray. NSVF [30] defines a set of voxel-bounded implicit fields organized in a sparse voxel octree structure, which enables skipping empty space in novel view synthesis. AutoInt [28] improves the rendering efficiency by reducing the number of evaluations along a ray through learned partial integrals. DeRF [42] spatially decomposes the scene into Voronoi diagrams, each learned by a small network. They achieve 3 times rendering speedup over NeRF with similar quality. Similarly, KiloNeRF [43] also spatially decomposes the scene, but into thousands of regular grids. Each of them is tackled by a tiny MLP network. Their work is similar to ours as a pre-trained NeRF model is also used to generate pseudo samples for training. Differently, KiloNeRF is still a NeRF -based method while ours is NeLF. Point sampling is still needed in KiloNeRF while our method roots out this problem. Besides, KiloNeRF results in thousands of small networks, making parallelism more challenging and requiring customized parallelism implementation, while our single network can get significant speedup simply using the vanilla PyTorch [39]. DONeRF [35] is proposed recently to reduce sampling through a depth oracle network learned with the ground-truth depth as supervision. It decimates the sampled points from hundreds (i.e., 256 in NeRF [34]) to only 4 to 16 while maintaining comparable or even better quality. However, the depth oracle network is learned with ground-truth depth as target, which is typically unavailable in practice. Our method does not demand it. Another direction for faster NeRF rendering is to pre-compute and cache the representations per the idea of trading memory for computational efficiency. FastNeRF [12] employs a factorized architecture to independently cache the position-dependent and ray direction-dependent outputs and achieves 3000 times faster than the original NeRF at rendering. Baking [15] precomputes and stores NeRF as sparse neural radiance grid that enables real-time rendering on commodity hardware. We consider this line of works orthogonal to ours. Neural light field (NeLF). Light fields enjoy a long history as a scene representation in computer vision and graphics [1,2]. Levoy et al. [26] and Gortler et al. [13] introduced light fields in computer graphics as 4D scene representation for fast image-based rendering. With them, novel view synthesis can be realized by simply extracting 2D slices in the 4D light field, yet with two major drawbacks. First, they tend to cause considerable storage costs. Second, it is hard to achieve a full 360 \u2022 representation without concatenating multiple light fields.\n\nIn the era of deep learning, neural light fields based on convolutional networks have been proposed [7,22,33]. One recent neural light field paper is Sitzmann et al. [46]. They employ Pl\u00fccker coordinates to parameterize 360 \u2022 light fields. In order to ensure multi-view consistency, they propose to learn a prior over the 4D light fields in a meta-learning framework. Despite intriguing ideas, their method is only evaluated on toy datasets, not as comparable to NeRF [34] in representing complex real-world scenes. Another recent NeLF work is RSEN [4]. To tackle the insufficient training data issue, they propose to learn a voxel grid of subdivided local light fields instead of the global light field. In their experiments, they also employ a pre-trained NeRF teacher for regularization. A very recent work [48] proposes a two-stage transformer-based model that can represent view-dependent effects accurately. A concurrent work NeuLF [29] employs a two-plane parameterization of the light field and uses a vanilla MLP network to learn the NeLF mapping. Our NeLF network is different from these in that, (1) methodologically, we propose a deep residual MLP (88 layers) to learn the light field, while these NeLF works still employ the NeRF-like shallow MLP networks (e.g., 6 layers in [46], 8 layers in [4]); (2) we propose to leverage a NeRF model to synthesize extra data for training, making our method a bridge from radiance field to light field; (3) thanks to the abundant capacity, our R2L network can achieve better rendering quality (e.g., our method can represent complex real-world scenes against [46]), or can achieve better efficiency while maintaining the rendering quality (e.g., [4] achieves merely around 5\u00d7 speedup vs. our 30\u00d7 speedup over the baseline NeRF method).\n\nKnowledge distillation (KD). The general idea of knowledge distillation is to guide the training of a student model through a larger pre-trained teacher model. Pioneered by Bucilu\u01ce et al. [8] and later refined by Hinton et al. [18] for image classification, knowledge distillation has seen extensive application in vision and language tasks [9,20,54,55]. Many variants have been proposed regarding the central question in knowledge distillation -how to define the knowledge that is supposed to be transferred from the teacher to the student, examples including output distance [5,18], internal feature distance [44,54], feature map attention [61], feature distribution [38], activation boundary [17], inter-sample distance relationship [31,37,40,51], and mutual information [50]. The distillation method in this work is to regress the output of the NeRF model with extra data labeled by the teacher (akin to [5,8]), which is the most straightforward way of distillation for the numerical target. Yet we will show this simple scheme can work powerfully to train a deep neural light field network.\n\n\nMethodology\n\n\nBackground: Neural Radiance Field (NeRF)\n\nIn NeRF [34], the 3D scene is implicitly represented by an MLP network, which learns to map the 5D coordinate (spatial location (x, y, z) and viewing direction (\u03b8, \u03c6)) to the 1D volume density and 3D view-dependent emitted radiance at that spatial location, F \u0398 :  network (parameterized by \u0398) to represent a scene. For rendering, the classic volume rendering technique [21] is adopted in NeRF to obtain the desired color for an oriented ray. Volume rendering is differential thus making NeRF end-toend trainable by using the captured 2D images as supervision. For novel view synthesis, given an oriented ray, NeRF first samples several locations along the camera ray, predicts their emitted radiance by querying the MLP network F \u0398 , and then aggregates the radiance together by alpha composition to output the final color. As sampling at vacuum points contributes nothing to the final color, a sufficient number of sampled points is critical to NeRF's performance so as to cover the worthy locations (such as those near the object surface). However, increased sampling incurs linearly increased query cost of the MLP network.\nR 5 \u2192 R 4 ,\n\nR2L: Distilling NeRF to NeLF\n\nOn the other hand, a scene can also be represented as a light field instead of radiance field, parameterized by a neural network. The network G \u03c6 learns a mapping function directly from a 4D oriented ray to its target 3D RGB, G \u03c6 :\nR 4 \u2192 R 3 .\nNeLF has several attractive advantages over NeRF.\n\n(1) Methodologically, it is more straightforward for novel view synthesis, in that the output of the NeLF network is already the wanted color, while the output of a NeRF network is the radiance of a sampled point; the desired color has to been obtained through an extra step of ray marching (see Fig. 2(a)).\n\n(2) Practically, given the same input ray (origin coordinate and direction), rendering in a light field simply amounts to a single query of the light field function. It fundamentally obviates the need for point sampling along a ray (which is the speed bottleneck in NeRF [34]), thus can be orders-of-magnitude faster than NeRF. Despite these intriguing properties, not many successful attempts have crystallized NeLF with comparable quality to NeRF up to date. To our best knowledge, only one recent NeLF method [4] achieves comparable quality to NeRF, but its speedup is relatively limited (around 5\u00d7wall-time speedup). In this paper, we propose a novel network architecture to make NeLF as effective as NeRF (meanwhile being much faster). Intuitively, the light field is harder to learn than radiance field -radiance at neighbor space locations does not change dramatically given the radiance field in the physical world is typically continuous; while two neighbor rays can point to starkly different colors because of occlusion. That is, the light field is intrinsically less smooth (sharply changing) than the radiance field. To capture the inherently more complex light field, we need a more powerful network. Per this idea, the 11-layer MLP network used in NeRF can hardly represent a complex light field by our empirical observation (see Tab. 5). We thereby propose to employ a deep MLP network to parameterize the above G function. Then, the foremost technical question is how to design the deep network. Network design. Different from the NeRF network, we propose to employ intensive residual blocks [14] in our network. The resulted network architecture is illustrated in Fig. 2(b). Residual connections were shown critical to enable the much greater network depth in [14], which also applies here for learning the light field. The merit of having a deeper network will be justified in our experiments (see Fig. 6(b)). We also study an underperformance case in the supplementary material when the residual connections are not used in a deep MLP network.\n\nNotably, enabling a deep network for neural radiance/light field parameterization is non-trivial. Noted by DeRF [42], \"there are diminishing returns in employing larger (deeper and/or wider) networks\". As a result, notably, most NeRF follow-up works for improving rendering efficiency (e.g., [42,43,35]) actually inherit the MLP architecture in NeRF with few substantial innovations. To our best knowledge, we are the first to address the efficiency issue of NeRF through the network architecture optimization perspective. Despite the residual structure is not new itself (due to ResNets [14]), its necessity and potential have not been fully recognized and exploited in the NVS task. Our paper is meant to make a step forward in this direction.\n\n\nSynthesize Pseudo Data\n\nDeep networks hunger for excessive data to be powerful. Unfortunately, this is not the case in novel view synthesis, where a user typically captures fewer than 100 images. To overcome this problem, we propose to employ a pre-trained NeRF model to synthesize extra data for training. This makes our method a bridge from neural radiance field to neural light field.\n\nWe need to decide where to sample to synthesize the pseudo data to avoid unnecessary waste. Specifically, with the original training data (images and their associated camera poses), we know the bounding box of the camera locations and their orientations. Then we randomly sample the ray origins (x o , y o , z o ) and normalized directions (x d , y d , z d ) obeying a uniform distribution U within the bounding box to make a 6D input as follows,\nx o \u223c U (x min o , x max o ), y o \u223c U (y min o , y max o ), z o \u223c U (z min o , z max o ), x d \u223c U (x min d , x max d ), y d \u223c U (y min d , y max d ), z d \u223c U (z min d , z max d ),(1)\nwhere the viewing bounding box can be inferred from the training data. An example illustration of the pseudo data origins and directions in our method is  shown in our supplementary material. Note, since we can control the generated data, we explicitly demand the pseudo data completely cover the original training data, implying they are in the same domain, which is critical to the performance. For a trained NeRF model F \u0398 * , the target RGB value can be queried as:\nx ! x \" Ray origin \u2026 x !# near x ! x \" x !(r,\u011d,b) = F \u0398 * (x o , y o , z o , x d , y d , z d ),(2)\nwhere \u0398 * stands for the converged model parameters. Then a slice of training data is simply a vector of these 9 numbers:\n(x o , y o , z o , x d , y d , z d ,r,\u011d,b).\nTo have an effective neural light field network F \u0398 , we feed abundant pseudo data into the proposed deep R2L network and train it by the MSE loss function,\nL = MSE(G \u03c6 (x o , y o , z o , x d , y d , z d ), (r,\u011d,b)).(3)\n\nRay Representation and Point Sampling\n\nIt is critical to have a proper representation of a ray in NeLF. In this work, we propose a new simple and effective representation -we concatenate the spatial coordinates of K sampled points along a ray to form an input vector (3K-d), fed into the NeLF network. Mathematically, we need at least two points to define a ray. More points will make the representation more precise. In this paper, we choose K = 16 points (see the ablation of K in Fig. 6(a)) along a ray. A critical design here is that we expect the network not to overfit the K points but to capture the underlying ray information. Thus, during training the K points are randomly sampled along the ray using the stratified sampling (same as NeRF [34], see Fig. 3). This design is critical to generalization. During testing, the K points are evenly spaced. We also tried changing the input to Pl\u00fccker coordinates for our R2L network (inspired by [46]). Our representation achieves better test quality than Pl\u00fccker (PSNR: 29.50 vs. 29.08, scene Lego, W181D88 network, trained with only pseudo data, 200K iters).\n\n\nTraining with Hard Examples\n\nGiven that we randomly sample the camera locations and orientations, the rays are likely to point to the trivial parts of a scene (e.g., the white background of a synthetic scene). Also, during training, some easy-to-regress colors will be well-learned early. Feeding these pixels again to the network barely increases its knowledge. We thus propose to tap into the idea of hard examples [16,45]. That is, we want the network to pay more attention to the rays that are harder to regress (typically corresponding to the high-frequency details) during learning. Specially, we maintain a hard example pool. A harder example is defined by a larger loss (Eq. (3)). In each iteration, we sort the losses for each sample in a batch in ascending order and add the top r (a pre-defined percentage constant) into the hard example pool. Meanwhile, in each iteration, the same amount r of hard examples are randomly picked out of the pool to augment the training batch. This design can accelerate the network convergence significantly as we will show in the experiments (see Fig. 6).\n\n\nImplementation Details\n\nOur R2L can lead to different networks under different FLOPs budgets. In this paper, we mainly have two: 6M and 12M FLOPs (per ray). They result in a bunch of networks: 12M: W256D88, 6M: W181D88, W256D44, W363D22 (W stands for width, D for depth). Obviously, a larger network is expected to perform better, so W256D88 is used for obtaining better quality; ablation studies will be conducted on the 6M-budget networks since they are faster to train. Following NeRF [34], positional encoding [53] is used to enrich the input information.\n\n\nExperiments\n\nDatasets. We show experiments on the following datasets:\n\n-NeRF datasets [34]. We evaluate our method on two datasets: synthetic dataset (Realistic Synthetic 360 \u2022 ) and real-world dataset (Real Forward-Facing). Realistic Synthetic 360 \u2022 contains path-traced images of 8 objects that exhibit complicated geometry and realistic non-Lambertian materials. 100 views of each scene are used for training and 200 for testing, with resolution of 800 \u00d7 800. Real Forward-Facing also contains 8 scenes, captured with a handheld cellphone. There are 20 to 62 images for each scene with 1/8 held out for testing. All images have a resolution of 1008 \u00d7 756. -DONeRF dataset includes their synthetic data. Images are rendered using Blender and their Cycles path tracer to render 300 images for each scene, which are split into train/validation/test sets at a 70%, 10%, 20% ratio.\n\nTraining settings. All images in the synthetic dataset are down-sampled by 2\u00d7 during training and testing. Due to limited space, we defer the full-resolution (800 \u00d7 800) results to our supplementary material. The original NeRF model is trained with a batch size of 1, 024 and initial learning rate as 5 \u00d7 10 \u22124 (decayed during training) for 200k iterations. We synthesize 10k images using the pretrained NeRF model. Our proposed R2L model is trained for 1, 000k iterations with the same learning rate schedule. The rays in a batch (batch size 98, 304 \nLego (a) GT (b) NeRF [34] (c) Ours-1 (d) Ours-2\nHotdog (a) GT (b) NeRF [34] (c) Ours-1 (d) Ours-2 Fig. 4. Visual comparison between our R2L network (W256D88) and NeRF on the synthetic scene Lego and Hotdog. Ours-1 is trained sorely on pseudo data, ours-2 on pseudo + real data. Please refer to our supplementary material for the visual comparison on the real-world dataset rays) are randomly sampled from different images so that they do not share the same origin. This is found critical to achieving superior performance. Adam optimizer [24] is employed for all training. We use PyTorch 1.9 [39], referring to [58]. Experiments are conducted with 8 NVIDIA V100 GPUs. Comparison methods. We compare with with the original NeRF [34] to show that we can achieve significantly better rendering quality while being much faster. Meanwhile, we also compare with DONeRF [35], NSVF [30], and NeX [57] since they also target efficient NVS as we do. Other efficient NVS works such as AutoInt [28] and X-Fields [7] have been shown less favorable than RSEN [4]. Therefore, we only compare with RSEN [4]. KiloNeRF [43], another closely related work apart from RSEN, will also be compared to. Similar to [4], we do not compare to baking-based methods [15,59,12]) as they trade memory footprint for speed while our method aims to maintain the compact representation.  \n\n\nDONeRF Synthetic Dataset\n\nDONeRF [35] achieves fast rendering using ground-truth depth for training. However, the ground-truth depth is not available in most practical cases. As a remedy, they propose to use a pre-trained NeRF model to estimate depth as a proxy for the ground-truth depth. The approach of DONeRF without ground-truth depth (e.g., DONeRF-16-noGT) is very relevant to ours. Thus, we compare with it using the synthetic dataset collected by the DONeRF paper. The quantitative results (PSNR and FLIP [3]) are presented in Tab in Pavillon and mirror in Barbershop), DONeRF cannot learn the reflection surfaces well because the ground-truth depth does not apply to the depth in the reflections, while our method (and the original NeRF) still performs well. Actual speed comparison. We further report the benchmark results of walltime speed in Tab. 4 to demonstrate the FLOPs reduction is well-aligned with actual speedup. Our R2L network (W181D88) is 28 \u223c 31\u00d7 faster than NeRF and 2\u00d7 faster than DONeRF-16-noGT.\n\n\nAblation Study\n\nMore data and deep network are critical. Tab. 5 shows the results of using the original 11-layer NeRF network to learn a light field on scene Lego. (1) Because of the severely insufficient data (only 0.1k training images), the network overfits to the training data with only 19.81 test PSNR. Note, this overfitting cannot be resolved by common regularization techniques like dropout [47] and BN [19]. Only when the data size is greatly inflated (with pseudo data) from 0.1k to 10k, can we see a significant test PSNR improvement (from 19.81 to 26.67). This shows the (abundant) pseudo data is indispensable.\n\n(2) Compare our R2L to NeRF at the same setting of 10k pseudo images, our network design improves test PSNR by around 3 (from 26.67 to 29.50), which is a significant boost in terms of rendering quality. This justifies the necessity of our deep network design.\n\nAnother reason encouraging us to use deep networks is that we empirically find trading width for depth under the same FLOPs budget can consistently lead to performance gains (see our supplementary material). Ablation of residuals in our R2L network. Although the original NeRF network also employs skip connections (to add ray directions as input), it can hardly be considered as a typical residual network [14] in fact, as they do not use residuals in the internal layers. In comparison, we promote employing extensive residual blocks in the internal layers. Its necessity is justified by Fig. 6(b). As seen, without residuals, the network is barely trainable. Ablation of pseudo sample size. The effect of pseudo sample size is of particular interest. As shown in Fig. 6(c), 100 images (see S = 0.1k) are not enough to train our deep R2L network -note the test PSNR saturates early at around 50k iterations while its train PSNR keeps arising sharply. This is a typical case of overfitting, caused by the over-parameterized model not being fed with enough data. In contrast, with more data (see the cases of S \u2265 0.5k), the train PSNR is held down and the test PSNR keeps arising. We observe no significant improvement starting from around 5k images. Ablation of hard example ratio. Here we vary the hard example ratio r and see how it affects the performance. To make a fair comparison, we keep the training batch size always the same (98, 304 rays per batch) when varying r. As shown in Fig. 6(d), using hard examples in each batch significantly improves the network learning in either train PSNR (i.e., better optimization) or test PSNR (i.e., better generalization) against the case of r = 0. There is no significant difference between hard example ratio r = 0.1, 0.2, and 0.3. In our experiments, we simply use a setting as r = 0.2.\n\n\nConclusion\n\nWe present the first deep neural light field network that can represent complex synthetic and real-world scenes. Starkly different from existing NeRF-like MLP networks, our R2L network is featured by an unprecedented depth and extensive residual blocks. We show the key to training such a deep network is abundant data, while the original captured images are barely sufficient. To resolve this, we propose to adopt a pre-trained NeRF model to synthesize excessive pseudo samples. With them, our proposed neural light field network achieves more than 26 \u223c 35\u00d7 FLOPs reduction and 28 \u223c 31\u00d7 wall-time acceleration on the NeRF synthetic dataset, with rendering quality improved significantly.\n\n\nvs. our NeLF method (b) Speedup-PSNR-Model Size comparison\n\nFig. 1 .\n1(a) Our neural light field (NeLF, bottom) method improves the rendering quality by 1.40 PSNR over neural radiance field (NeRF, top)[34] on the NeRF synthetic dataset, while being around 30\u00d7 faster. (b) Our method achieves a more favorable speedup-PSNR-model size tradeoff than other efficient novel view synthesis methods on the NeRF synthetic dataset. The number in the parentheses indicates the model size relative to the baseline NeRF model used in each paper (best viewed in color ).\n\nFig. 2 .\n2(a) Comparison between our proposed NeLF network (Deep Residual MLP, bottom) and NeRF network (Shallow MLP, top). (b) Detailed architecture of the proposed deep light field network, which employs extensive repeated residual MLP blocks.\n\nFig. 3 .\n3Illustration of the point sampling in training and testing of our method. The orange and green colors denote the different segments of the ray. The blue color marks the start and end points of each segment. Each sampled train point is colored based on the corresponding segment color\n\n\nwhere F refers to an MLP neural NeLF: One single forward per rayLinear \nReLU \n\nElement-Wise Sum \nResidual Block \n\nInput \nRGB \n\n\u2026 \n\nLong skip connection \n\nRepeated Residual MLP Blocks \n\n(a) NeRF (top) vs. NeLF (ours, bottom) \n(b) Detailed architecture of our NeLF network \n\nCamera ray \n\n\u2026 \n\nDeep Residual MLP \n\nShallow MLP \n\n\u2026 \n\nRGB \n\n\u2026 \n\nConcat \n\n\u2026 \n\nRGB \n\nNeRF: Multi-forwards per ray \n\nx 1 x 2 x 3 \n\nRay origin \n\nAlpha-composition \n\n\n\n\n#train points \n\n\u2026 \n\nfar \n\ntest points \n\nMiddle position \nof the test points \n\nrandomly sample \n\n\n\nTable 2 .\n2PSNR\u2191, SSIM\u2191, and LPIPS\u2193 (AlexNet [25] is used for LPIPS) on the NeRF \nsynthetic dataset (Realistic Synthetic 360 \u2022 ) and real-world dataset (Real Forward-\nFacing). R2L network: W256D88.  \u2020 KiloNeRF adopts Empty Space Skipping and Early \nRay Termination, so the FLOPs is scene-by-scene; we estimate the average FLOPs \nbased on the description in their paper. The best results are in red, second best in blue \n\nMethod \nStorage (MB) FLOPs (M) \nSynthetic \nReal-world \nPSNR\u2191 \nSSIM\u2191 LPIPS\u2193 \nPSNR\u2191 \nSSIM\u2191 LPIPS\u2193 \n\nTeacher NeRF [34] \n2.4 \n303.82 \n30.47 \n0.9925 0.0391 \n27.68 \n0.9725 0.0733 \nOurs-1 (Pseudo) \n23.7 \n11.79 \n30.48 (+0.01) 0.9939 0.0467 \n27.58 (-0.10) 0.9722 0.0997 \nOurs-2 (Pseudo+real) \n23.7 \n11.79 \n31.87 (+1.40) 0.9950 0.0340 27.79 (+0.11) 0.9729 0.0968 \nTeacher NeRF in [43] \n2.4 \n303.82 \n31.01 \n0.95 \n0.08 \n-\n-\n-\nKiloNeRF [43] \n38.9 \n\u223c500  \u2020 \n31.00 (-0.01) \n0.95 \n0.03 \n-\n-\n-\nTeacher NeRF in [4] \n4.6 \n\u223c300 \n-\n-\n-\n27.928 \n0.9160 0.065 \nRSEN [4] \n5.4 \n67.2 \n-\n-\n-\n27.941 (+0.013) 0.9161 0.060 \n\n\n\nTable 3 .\n3PSNR\u2191 and FLIP\u2193 comparison on the DONeRF synthetic dataset.All the \n\n\nTable 4 .\n4Average time (s) comparison among our R2L network (W181D88), DONeRF, \nand NeRF. The benchmark is conducted under the same hardware and software. The \nspeedup of ours and DONeRF is relative to the running time of NeRF. Results are \naveraged by 60 frames \n\nMethod \nFLOPs (M) \nGeForce 2080Ti \nTesla V100 \nCPU \n\nNeRF \n211.42 \n5.9343 \n4.9902 \n142.2612 \nDONeRF-16 14.29 (14.79\u00d7) 0.4162 (14.26\u00d7) \n0.3524 (14.16\u00d7) \n9.9344 (14.32\u00d7) \nOurs \n6.00 (35.24\u00d7) 0.2103 (28.22\u00d7) 0.1629 (30.63\u00d7) 5.0198 (28.34\u00d7) \n\nTable 5. Ablation study of different network and data schemes when learning a light \nfield. Scene: Lego. All models are trained for 200k iterations. Note, the train PSNR of \nour method is lower than test PSNR because we use the hard examples (Sec. 3.5) i.e., \nexamples with small PSNR, for training. \n\nNetwork \nData \nTrain PSNR (dB) Test PSNR (dB) \n\nNeRF [34] \nOriginal (0.1k imgs) \n25.61 \n19.81 \nNeRF+dropout [47] \nOriginal (0.1k imgs) \n25.56 \n19.83 \nNeRF+BN [19] \nOriginal (0.1k imgs) \n25.43 \n19.76 \nNeRF [34] \nPseudo (10k imgs) \n23.82 \n26.67 \nR2L (W181D88) \nPseudo (10k imgs) \n28.38 \n29.50 \nR2L (W181D88) \nPseudo + Original (10.1k imgs) \n29.85 \n30.09 \n\nsettings, so the PSNR results cannot be directly compared. Instead, we com-\npare the PSNR change over the baseline NeRFs. KiloNeRF gets 0.01 dB PSNR \ndrop vs. ours 1.40 dB PSNR boost. RSEN improves the PSNR on the much \nmore challenging real-world dataset marginally (by 0.013 dB). In comparison, \nour improvement is more significant (0.11 dB) with much fewer FLOPs. \n\n\n\n\n. 3. (1) Trained purely with pseudo data, our method already outperforms DONeRF-16-noGT and DONeRF-8 (which even demands the ground-truth depth as input). (2) Similar to the case (Tab. 2) on the NeRF synthetic dataset, including the original real images for training significantly boosts the performance by 2.78 dB.Visual results inFig. 5show that our method delivers better visual quality than the baseline NeRF. On the scene Pavillon and Barbershop, our R2L network achieves better rendering quality than DONeRF-8 despite not using the ground-truth depth. Particularly note the reflection surfaces (e.g., waterFig. 6. Ablation studies. All networks are trained for 200k iterations, scene: Lego. Test PSNRs are plotted with dashed lines; train PSNRs are plotted with solid lines. (a) PSNR comparison of different sampled points in our R2L network (W181D88). Default: 16 points (blue lines) (b) PSNR comparison between two network designs: using residuals or not for our R2L network. (c) PSNR comparison under different pseudo sample sizes. Default: S = 10k. (d) PSNR comparison under different hard example ratios r \u2208 {0, 0.1, 0.2, 0.3}. Default: r = 0.20 \n50 \n100 \n150 \n200 \n\nIteration (k) \n\n18 \n\n20 \n\n22 \n\n24 \n\n26 \n\n28 \n\n30 \n\nPSNR (dB) \n\n4 points (test) \n\n4 points (train) \n\n8 points (test) \n\n8 points (train) \n\n16 points (test) \n\n16 points (train) \n\n32 points (test) \n\n32 points (train) \n\n64 points (test) \n\n64 points (train) \n\n0 \n25 \n50 \n75 \n100 \n125 \n150 \n175 \n200 \n\nIteration (k) \n\n16 \n\n18 \n\n20 \n\n22 \n\n24 \n\n26 \n\n28 \n\n30 \n\nPSNR (dB) \n\nw/o residuals (test) \n\nw/o residuals (train) \n\nw/ residuals (test) \n\nw/ residuals (train) \n\n(a) Number of sampled points \n(b) With vs. without residuals \n\n0 \n50 \n100 \n150 \n200 \n\nIteration (k) \n\n20 \n\n22 \n\n24 \n\n26 \n\n28 \n\n30 \n\nPSNR (dB) \n\nS=0.1k (test) \n\nS=0.1k (train) \n\nS=0.5k (test) \n\nS=0.5k (train) \n\nS=1k (test) \n\nS=1k (train) \n\nS=2.5k (test) \n\nS=2.5k (train) \n\nS=5k (test) \n\nS=5k (train) \n\nS=10k (test) \n\nS=10k (train) \n\n0 \n50 \n100 \n150 \n200 \n\nIteration (k) \n\n20 \n\n22 \n\n24 \n\n26 \n\n28 \n\n30 \n\nPSNR (dB) \n\nr=0 (test) \n\nr=0 (train) \n\nr=0.1 (test) \n\nr=0.1 (train) \n\nr=0.2 (test) \n\nr=0.2 (train) \n\nr=0.3 (test) \n\nr=0.3 (train) \n\n(c) Pseudo sample size \n(d) Hard example ratio \n\n\n\nE H Adelson, J R Bergen, The plenoptic function and the elements of early vision. MIT Press24Adelson, E.H., Bergen, J.R., et al.: The plenoptic function and the elements of early vision, vol. 2. MIT Press (1991) 4\n\nSingle lens stereo with a plenoptic camera. E H Adelson, J Y Wang, TPAMI. 1424Adelson, E.H., Wang, J.Y.: Single lens stereo with a plenoptic camera. TPAMI 14(2), 99-106 (1992) 4\n\nFlip: A difference evaluator for alternating images. P Andersson, J Nilsson, T Akenine-M\u00f6ller, M Oskarsson, K \u00c5str\u00f6m, M D Fairchild, Proceedings of the ACM in Computer Graphics and Interactive Techniques. the ACM in Computer Graphics and Interactive Techniques12Andersson, P., Nilsson, J., Akenine-M\u00f6ller, T., Oskarsson, M.,\u00c5str\u00f6m, K., Fairchild, M.D.: Flip: A difference evaluator for alternating images. In: Proceedings of the ACM in Computer Graphics and Interactive Techniques (2020) 12\n\nLearning neural light fields with ray-space embedding networks. B Attal, J B Huang, M Zollhoefer, J Kopf, C Kim, CVPR (2022) 3, 5. 610Attal, B., Huang, J.B., Zollhoefer, M., Kopf, J., Kim, C.: Learning neural light fields with ray-space embedding networks. In: CVPR (2022) 3, 5, 6, 10\n\nJ Ba, R Caruana, Do deep nets really need to be deep? In: NeurIPS. 5Ba, J., Caruana, R.: Do deep nets really need to be deep? In: NeurIPS (2014) 5\n\nJ T Barron, B Mildenhall, M Tancik, P Hedman, R Martin-Brualla, P P Srinivasan, arXiv:2103.13415Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. 23arXiv preprintBarron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini- vasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. arXiv preprint arXiv:2103.13415 (2021) 2, 3\n\nX-fields: Implicit neural view-, light-and time-image interpolation. M Bemana, K Myszkowski, H P Seidel, T Ritschel, ACMTOG. 39610Bemana, M., Myszkowski, K., Seidel, H.P., Ritschel, T.: X-fields: Implicit neural view-, light-and time-image interpolation. ACMTOG 39(6), 1-15 (2020) 5, 10\n\nModel compression. C Bucilu\u01ce, R Caruana, A Niculescu-Mizil, SIGKDD. 35Bucilu\u01ce, C., Caruana, R., Niculescu-Mizil, A.: Model compression. In: SIGKDD (2006) 3, 5\n\nLearning efficient object detection models with knowledge distillation. G Chen, W Choi, X Yu, T Han, M Chandraker, NeurIPS5Chen, G., Choi, W., Yu, X., Han, T., Chandraker, M.: Learning efficient object detection models with knowledge distillation. In: NeurIPS (2017) 5\n\nLearning implicit fields for generative shape modeling. Z Chen, H Zhang, CVPR. 2Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: CVPR (2019) 2\n\nF Dellaert, L Yen-Chen, arXiv:2101.05204Neural volume rendering: Nerf and beyond. arXiv preprintDellaert, F., Yen-Chen, L.: Neural volume rendering: Nerf and beyond. arXiv preprint arXiv:2101.05204 (2020) 2\n\nS J Garbin, M Kowalski, M Johnson, J Shotton, J Valentin, arXiv:2103.10380Fastnerf: Highfidelity neural rendering at 200fps. 410arXiv preprintGarbin, S.J., Kowalski, M., Johnson, M., Shotton, J., Valentin, J.: Fastnerf: High- fidelity neural rendering at 200fps. arXiv preprint arXiv:2103.10380 (2021) 4, 10\n\nThe lumigraph. S J Gortler, R Grzeszczuk, R Szeliski, M F Cohen, Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques. the Annual Conference on Computer Graphics and Interactive Techniques4Gortler, S.J., Grzeszczuk, R., Szeliski, R., Cohen, M.F.: The lumigraph. In: Pro- ceedings of the Annual Conference on Computer Graphics and Interactive Tech- niques (1996) 4\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR714He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016) 7, 14\n\nP Hedman, P P Srinivasan, B Mildenhall, J T Barron, P Debevec, arXiv:2103.14645Baking neural radiance fields for real-time view synthesis. 410arXiv preprintHedman, P., Srinivasan, P.P., Mildenhall, B., Barron, J.T., Debevec, P.: Baking neural radiance fields for real-time view synthesis. arXiv preprint arXiv:2103.14645 (2021) 4, 10\n\nBeyond hard negative mining: Efficient detector learning via block-circulant decomposition. J F Henriques, J Carreira, R Caseiro, J Batista, CVPR9Henriques, J.F., Carreira, J., Caseiro, R., Batista, J.: Beyond hard negative mining: Efficient detector learning via block-circulant decomposition. In: CVPR (2013) 9\n\nKnowledge transfer via distillation of activation boundaries formed by hidden neurons. B Heo, M Lee, S Yun, J Y Choi, AAAI5Heo, B., Lee, M., Yun, S., Choi, J.Y.: Knowledge transfer via distillation of acti- vation boundaries formed by hidden neurons. In: AAAI (2019) 5\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, In: NeurIPS Workshop. 35Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: NeurIPS Workshop (2014) 3, 5\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, ICML. 1214Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: ICML (2015) 12, 14\n\nX Jiao, Y Yin, L Shang, X Jiang, X Chen, L Li, F Wang, Q Liu, arXiv:1909.10351Tinybert: Distilling bert for natural language understanding. 5arXiv preprintJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., Liu, Q.: Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 (2019) 5\n\nRay tracing volume densities. J T Kajiya, B P Von Herzen, SIGGRAPH. 1836Kajiya, J.T., Von Herzen, B.P.: Ray tracing volume densities. SIGGRAPH 18(3), 165-174 (1984) 6\n\nLearning-based view synthesis for light field cameras. N K Kalantari, T C Wang, R Ramamoorthi, ACM Transactions on Graphics. 3565Kalantari, N.K., Wang, T.C., Ramamoorthi, R.: Learning-based view synthesis for light field cameras. ACM Transactions on Graphics 35(6), 1-10 (2016) 5\n\nAn introduction to computational learning theory. M J Kearns, U V Vazirani, U Vazirani, MIT Press3Kearns, M.J., Vazirani, U.V., Vazirani, U.: An introduction to computational learn- ing theory. MIT Press (1994) 3\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR10Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR (2015) 10\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, In: NeurIPS. 10Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: NeurIPS (2012) 10\n\nLight field rendering. M Levoy, P Hanrahan, Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques. the Annual Conference on Computer Graphics and Interactive Techniques4Levoy, M., Hanrahan, P.: Light field rendering. In: Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques (1996) 4\n\nNeural scene flow fields for space-time view synthesis of dynamic scenes. Z Li, S Niklaus, N Snavely, O Wang, CVPRLi, Z., Niklaus, S., Snavely, N., Wang, O.: Neural scene flow fields for space-time view synthesis of dynamic scenes. In: CVPR (2021) 2\n\nAutoint: Automatic integration for fast neural volume rendering. D B Lindell, J N Martel, G Wetzstein, 10In: CVPR (2021) 2, 3, 4Lindell, D.B., Martel, J.N., Wetzstein, G.: Autoint: Automatic integration for fast neural volume rendering. In: CVPR (2021) 2, 3, 4, 10\n\nNeulf: Efficient novel view synthesis with neural 4d light field. C Liu, Z Li, J Yuan, Y Xu, EGSR5Liu, C., Li, Z., Yuan, J., Xu, Y.: Neulf: Efficient novel view synthesis with neural 4d light field. In: EGSR (2022) 5\n\nL Liu, J Gu, K Zaw Lin, T S Chua, C Theobalt, Neural sparse voxel fields. 1011NeurIPS (2020) 3, 4Liu, L., Gu, J., Zaw Lin, K., Chua, T.S., Theobalt, C.: Neural sparse voxel fields. In: NeurIPS (2020) 3, 4, 10, 11\n\nKnowledge distillation via instance relationship graph. Y Liu, J Cao, B Li, C Yuan, W Hu, Y Li, Y Duan, CVPR5Liu, Y., Cao, J., Li, B., Yuan, C., Hu, W., Li, Y., Duan, Y.: Knowledge distillation via instance relationship graph. In: CVPR (2019) 5\n\nOccupancy networks: Learning 3d reconstruction in function space. L Mescheder, M Oechsle, M Niemeyer, S Nowozin, A Geiger, CVPRMescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks: Learning 3d reconstruction in function space. In: CVPR (2019) 2\n\nLocal light field fusion: Practical view synthesis with prescriptive sampling guidelines. B Mildenhall, P P Srinivasan, R Ortiz-Cayon, N K Kalantari, R Ramamoorthi, R Ng, A Kar, ACM Transactions on Graphics. 3845Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light field fusion: Practical view synthesis with pre- scriptive sampling guidelines. ACM Transactions on Graphics 38(4), 1-14 (2019) 5\n\nNerf: Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, ECCV. 1012Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020) 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12\n\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. T Neff, P Stadlbauer, M Parger, A Kurz, J H Mueller, C R A Chaitanya, A S Kaplanyan, M Steinberger, Computer Graphics Forum. 1112Neff, T., Stadlbauer, P., Parger, M., Kurz, A., Mueller, J.H., Chaitanya, C.R.A., Kaplanyan, A.S., Steinberger, M.: DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graph- ics Forum (2021) 2, 3, 4, 7, 10, 11, 12\n\nDeepsdf: Learning continuous signed distance functions for shape representation. J J Park, P Florence, J Straub, R Newcombe, S Lovegrove, CVPRPark, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: Learning continuous signed distance functions for shape representation. In: CVPR (2019) 2\n\nRelational knowledge distillation. W Park, D Kim, Y Lu, M Cho, CVPR5Park, W., Kim, D., Lu, Y., Cho, M.: Relational knowledge distillation. In: CVPR (2019) 5\n\nLearning deep representations with probabilistic knowledge transfer. N Passalis, A Tefas, ECCV5Passalis, N., Tefas, A.: Learning deep representations with probabilistic knowledge transfer. In: ECCV (2018) 5\n\nPytorch: An imperative style, highperformance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, NeurIPS410Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high- performance deep learning library. In: NeurIPS (2019) 4, 10\n\nCorrelation congruence for knowledge distillation. B Peng, X Jin, J Liu, D Li, Y Wu, Y Liu, S Zhou, Z Zhang, ICCV5Peng, B., Jin, X., Liu, J., Li, D., Wu, Y., Liu, Y., Zhou, S., Zhang, Z.: Correlation congruence for knowledge distillation. In: ICCV (2019) 5\n\nTerminerf: Ray termination prediction for efficient neural rendering. M Piala, R Clark, 3In: \"3DV\" (2021Piala, M., Clark, R.: Terminerf: Ray termination prediction for efficient neural rendering. In: \"3DV\" (2021) 3\n\nDerf: Decomposed radiance fields. D Rebain, W Jiang, S Yazdani, K Li, K M Yi, A Tagliasacchi, 7In: CVPR (2021) 2, 3, 4Rebain, D., Jiang, W., Yazdani, S., Li, K., Yi, K.M., Tagliasacchi, A.: Derf: De- composed radiance fields. In: CVPR (2021) 2, 3, 4, 7\n\nKilonerf: Speeding up neural radiance fields with thousands of tiny mlps. C Reiser, S Peng, Y Liao, A Geiger, ICCV (2021) 3, 4. 710Reiser, C., Peng, S., Liao, Y., Geiger, A.: Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In: ICCV (2021) 3, 4, 7, 10\n\nFitnets: Hints for thin deep nets. A Romero, N Ballas, S E Kahou, A Chassang, C Gatta, Y Bengio, ICLR5Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints for thin deep nets. In: ICLR (2015) 5\n\nTraining region-based object detectors with online hard example mining. A Shrivastava, A Gupta, R Girshick, CVPR9Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors with online hard example mining. In: CVPR (2016) 9\n\nLight field networks: Neural scene representations with single-evaluation rendering. V Sitzmann, S Rezchikov, W T Freeman, J B Tenenbaum, F Durand, NeurIPS (2021). 5Sitzmann, V., Rezchikov, S., Freeman, W.T., Tenenbaum, J.B., Durand, F.: Light field networks: Neural scene representations with single-evaluation rendering. In: NeurIPS (2021) 5, 8\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, JMLR. 15113Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. JMLR 15(1), 1929-1958 (2014) 12, 13\n\nM Suhail, C Esteves, L Sigal, A Makadia, Light field neural rendering. In: CVPR (2022). 5Suhail, M., Esteves, C., Sigal, L., Makadia, A.: Light field neural rendering. In: CVPR (2022) 5\n\nNeural geometric level of detail: Real-time rendering with implicit 3d shapes. T Takikawa, J Litalien, K Yin, K Kreis, C Loop, D Nowrouzezahrai, A Jacobson, M Mcguire, S Fidler, CVPRTakikawa, T., Litalien, J., Yin, K., Kreis, K., Loop, C., Nowrouzezahrai, D., Ja- cobson, A., McGuire, M., Fidler, S.: Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In: CVPR (2021) 2\n\nContrastive representation distillation. Y Tian, D Krishnan, P Isola, ICLR (2020). 5Tian, Y., Krishnan, D., Isola, P.: Contrastive representation distillation. In: ICLR (2020) 5\n\nSimilarity-preserving knowledge distillation. F Tung, G Mori, CVPR. 5Tung, F., Mori, G.: Similarity-preserving knowledge distillation. In: CVPR (2019) 5\n\nThe nature of statistical learning theory. V Vapnik, Springer Science & Business Media3Vapnik, V.: The nature of statistical learning theory. Springer Science & Business Media (2013) 3\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, NeurIPS9Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 9\n\nCollaborative distillation for ultra-resolution universal style transfer. H Wang, Y Li, Y Wang, H Hu, M H Yang, CVPR5Wang, H., Li, Y., Wang, Y., Hu, H., Yang, M.H.: Collaborative distillation for ultra-resolution universal style transfer. In: CVPR (2020) 5\n\nKnowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. L Wang, K J Yoon, TPAMI. 5Wang, L., Yoon, K.J.: Knowledge distillation and student-teacher learning for vi- sual intelligence: A review and new outlooks. TPAMI (2021) 5\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, TIP. 13411Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. TIP 13(4), 600-612 (2004) 2, 11\n\nNex: Realtime view synthesis with neural basis expansion. S Wizadwongsa, P Phongthawee, J Yenphraphai, S Suwajanakorn, CVPR1011Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: Real- time view synthesis with neural basis expansion. In: CVPR (2021) 10, 11\n\nL Yen-Chen, Nerf-pytorch. 10Yen-Chen, L.: Nerf-pytorch. https://github.com/yenchenlin/nerf-pytorch/ (2020) 10\n\nPlenoctrees for real-time rendering of neural radiance fields. A Yu, R Li, M Tancik, H Li, R Ng, A Kanazawa, ICCV310Yu, A., Li, R., Tancik, M., Li, H., Ng, R., Kanazawa, A.: Plenoctrees for real-time rendering of neural radiance fields. In: ICCV (2021) 3, 10\n\nA Yu, V Ye, M Tancik, A Kanazawa, pixelnerf: Neural radiance fields from one or few images. CVPRYu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from one or few images. In: CVPR (2021) 2\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. S Zagoruyko, N Komodakis, 5Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In: ICLR (2017) 5\n\nThe unreasonable effectiveness of deep features as a perceptual metric. R Zhang, P Isola, A A Efros, E Shechtman, O Wang, CVPR11Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018) 11\n", "annotations": {"author": "[{\"end\":159,\"start\":109},{\"end\":180,\"start\":160},{\"end\":203,\"start\":181},{\"end\":230,\"start\":204},{\"end\":255,\"start\":231},{\"end\":292,\"start\":256},{\"end\":320,\"start\":293}]", "publisher": null, "author_last_name": "[{\"end\":118,\"start\":114},{\"end\":168,\"start\":165},{\"end\":191,\"start\":186},{\"end\":218,\"start\":209},{\"end\":243,\"start\":239},{\"end\":262,\"start\":260},{\"end\":308,\"start\":300}]", "author_first_name": "[{\"end\":113,\"start\":109},{\"end\":164,\"start\":160},{\"end\":185,\"start\":181},{\"end\":208,\"start\":204},{\"end\":238,\"start\":231},{\"end\":259,\"start\":256},{\"end\":299,\"start\":293}]", "author_affiliation": "[{\"end\":129,\"start\":120},{\"end\":158,\"start\":131},{\"end\":179,\"start\":170},{\"end\":202,\"start\":193},{\"end\":229,\"start\":220},{\"end\":254,\"start\":245},{\"end\":291,\"start\":264},{\"end\":319,\"start\":310}]", "title": "[{\"end\":95,\"start\":1},{\"end\":415,\"start\":321}]", "venue": null, "abstract": "[{\"end\":2805,\"start\":536}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4353,\"start\":4349},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4356,\"start\":4353},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4359,\"start\":4356},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4362,\"start\":4359},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4399,\"start\":4395},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4625,\"start\":4621},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4674,\"start\":4671},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4677,\"start\":4674},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4680,\"start\":4677},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4683,\"start\":4680},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":5271,\"start\":5267},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5284,\"start\":5280},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5417,\"start\":5413},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5420,\"start\":5417},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5501,\"start\":5497},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6974,\"start\":6970},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7214,\"start\":7211},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7319,\"start\":7315},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7401,\"start\":7398},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7404,\"start\":7401},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7407,\"start\":7404},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7410,\"start\":7407},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7912,\"start\":7908},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7915,\"start\":7912},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8174,\"start\":8171},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8177,\"start\":8174},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9808,\"start\":9804},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9966,\"start\":9962},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10132,\"start\":10128},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10261,\"start\":10257},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10444,\"start\":10440},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11071,\"start\":11067},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11084,\"start\":11080},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11277,\"start\":11273},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11669,\"start\":11665},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11865,\"start\":11861},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12154,\"start\":12151},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12156,\"start\":12154},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12175,\"start\":12171},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12199,\"start\":12195},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12697,\"start\":12694},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12700,\"start\":12697},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12703,\"start\":12700},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12764,\"start\":12760},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13066,\"start\":13062},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13146,\"start\":13143},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13408,\"start\":13404},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13536,\"start\":13532},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13886,\"start\":13882},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13903,\"start\":13900},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14208,\"start\":14204},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14294,\"start\":14291},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14573,\"start\":14570},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14613,\"start\":14609},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14726,\"start\":14723},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14729,\"start\":14726},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14732,\"start\":14729},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":14735,\"start\":14732},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14962,\"start\":14959},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14965,\"start\":14962},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14997,\"start\":14993},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15000,\"start\":14997},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":15028,\"start\":15024},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15055,\"start\":15051},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15081,\"start\":15077},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15122,\"start\":15118},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15125,\"start\":15122},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15128,\"start\":15125},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15131,\"start\":15128},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15160,\"start\":15156},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15293,\"start\":15290},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15295,\"start\":15293},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15548,\"start\":15544},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15910,\"start\":15906},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17586,\"start\":17582},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17826,\"start\":17823},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18924,\"start\":18920},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19093,\"start\":19089},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19492,\"start\":19488},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19672,\"start\":19668},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19675,\"start\":19672},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19678,\"start\":19675},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19968,\"start\":19964},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22852,\"start\":22848},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23051,\"start\":23047},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23635,\"start\":23631},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23638,\"start\":23635},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24809,\"start\":24805},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24835,\"start\":24831},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24969,\"start\":24965},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26387,\"start\":26383},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26854,\"start\":26850},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26908,\"start\":26904},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26927,\"start\":26923},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27043,\"start\":27039},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27179,\"start\":27175},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27190,\"start\":27186},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27204,\"start\":27200},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27298,\"start\":27294},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27315,\"start\":27312},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27360,\"start\":27357},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27402,\"start\":27399},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27417,\"start\":27413},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27505,\"start\":27502},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27553,\"start\":27549},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27556,\"start\":27553},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27559,\"start\":27556},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27705,\"start\":27701},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28184,\"start\":28181},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29096,\"start\":29092},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29108,\"start\":29104},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29255,\"start\":29253},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29990,\"start\":29986},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32327,\"start\":32323}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32180,\"start\":32120},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32679,\"start\":32181},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32926,\"start\":32680},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33221,\"start\":32927},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33659,\"start\":33222},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33758,\"start\":33660},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34776,\"start\":33759},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34857,\"start\":34777},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36389,\"start\":34858},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38607,\"start\":36390}]", "paragraph": "[{\"end\":4138,\"start\":2807},{\"end\":6240,\"start\":4155},{\"end\":8055,\"start\":6242},{\"end\":8895,\"start\":8057},{\"end\":9710,\"start\":8897},{\"end\":12592,\"start\":9727},{\"end\":14380,\"start\":12594},{\"end\":15477,\"start\":14382},{\"end\":16663,\"start\":15536},{\"end\":16938,\"start\":16707},{\"end\":17000,\"start\":16951},{\"end\":17309,\"start\":17002},{\"end\":19374,\"start\":17311},{\"end\":20121,\"start\":19376},{\"end\":20511,\"start\":20148},{\"end\":20959,\"start\":20513},{\"end\":21612,\"start\":21143},{\"end\":21833,\"start\":21712},{\"end\":22034,\"start\":21878},{\"end\":23211,\"start\":22138},{\"end\":24314,\"start\":23243},{\"end\":24876,\"start\":24341},{\"end\":24948,\"start\":24892},{\"end\":25758,\"start\":24950},{\"end\":26311,\"start\":25760},{\"end\":27665,\"start\":26360},{\"end\":28690,\"start\":27694},{\"end\":29316,\"start\":28709},{\"end\":29577,\"start\":29318},{\"end\":31416,\"start\":29579},{\"end\":32119,\"start\":31431}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16675,\"start\":16664},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16950,\"start\":16939},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21142,\"start\":20960},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21655,\"start\":21613},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21711,\"start\":21655},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21877,\"start\":21834},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22097,\"start\":22035},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26359,\"start\":26312}]", "table_ref": "[{\"end\":6727,\"start\":6720},{\"end\":28206,\"start\":28203}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4153,\"start\":4141},{\"attributes\":{\"n\":\"2\"},\"end\":9725,\"start\":9713},{\"attributes\":{\"n\":\"3\"},\"end\":15491,\"start\":15480},{\"attributes\":{\"n\":\"3.1\"},\"end\":15534,\"start\":15494},{\"attributes\":{\"n\":\"3.2\"},\"end\":16705,\"start\":16677},{\"attributes\":{\"n\":\"3.3\"},\"end\":20146,\"start\":20124},{\"attributes\":{\"n\":\"3.4\"},\"end\":22136,\"start\":22099},{\"attributes\":{\"n\":\"3.5\"},\"end\":23241,\"start\":23214},{\"attributes\":{\"n\":\"3.6\"},\"end\":24339,\"start\":24317},{\"attributes\":{\"n\":\"4\"},\"end\":24890,\"start\":24879},{\"attributes\":{\"n\":\"4.2\"},\"end\":27692,\"start\":27668},{\"attributes\":{\"n\":\"4.3\"},\"end\":28707,\"start\":28693},{\"attributes\":{\"n\":\"5\"},\"end\":31429,\"start\":31419},{\"end\":32190,\"start\":32182},{\"end\":32689,\"start\":32681},{\"end\":32936,\"start\":32928},{\"end\":33769,\"start\":33760},{\"end\":34787,\"start\":34778},{\"end\":34868,\"start\":34859}]", "table": "[{\"end\":33659,\"start\":33288},{\"end\":33758,\"start\":33663},{\"end\":34776,\"start\":33776},{\"end\":34857,\"start\":34848},{\"end\":36389,\"start\":34870},{\"end\":38607,\"start\":37547}]", "figure_caption": "[{\"end\":32180,\"start\":32122},{\"end\":32679,\"start\":32192},{\"end\":32926,\"start\":32691},{\"end\":33221,\"start\":32938},{\"end\":33288,\"start\":33224},{\"end\":33663,\"start\":33662},{\"end\":33776,\"start\":33771},{\"end\":34848,\"start\":34789},{\"end\":37547,\"start\":36392}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17307,\"start\":17298},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19002,\"start\":18993},{\"end\":19234,\"start\":19228},{\"end\":22591,\"start\":22582},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22864,\"start\":22858},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23900,\"start\":23892},{\"end\":24312,\"start\":24306},{\"end\":26416,\"start\":26410},{\"end\":30178,\"start\":30169},{\"end\":30351,\"start\":30345},{\"end\":31074,\"start\":31068}]", "bib_author_first_name": "[{\"end\":38610,\"start\":38609},{\"end\":38612,\"start\":38611},{\"end\":38623,\"start\":38622},{\"end\":38625,\"start\":38624},{\"end\":38869,\"start\":38868},{\"end\":38871,\"start\":38870},{\"end\":38882,\"start\":38881},{\"end\":38884,\"start\":38883},{\"end\":39057,\"start\":39056},{\"end\":39070,\"start\":39069},{\"end\":39081,\"start\":39080},{\"end\":39099,\"start\":39098},{\"end\":39112,\"start\":39111},{\"end\":39122,\"start\":39121},{\"end\":39124,\"start\":39123},{\"end\":39560,\"start\":39559},{\"end\":39569,\"start\":39568},{\"end\":39571,\"start\":39570},{\"end\":39580,\"start\":39579},{\"end\":39594,\"start\":39593},{\"end\":39602,\"start\":39601},{\"end\":39782,\"start\":39781},{\"end\":39788,\"start\":39787},{\"end\":39930,\"start\":39929},{\"end\":39932,\"start\":39931},{\"end\":39942,\"start\":39941},{\"end\":39956,\"start\":39955},{\"end\":39966,\"start\":39965},{\"end\":39976,\"start\":39975},{\"end\":39994,\"start\":39993},{\"end\":39996,\"start\":39995},{\"end\":40410,\"start\":40409},{\"end\":40420,\"start\":40419},{\"end\":40434,\"start\":40433},{\"end\":40436,\"start\":40435},{\"end\":40446,\"start\":40445},{\"end\":40648,\"start\":40647},{\"end\":40659,\"start\":40658},{\"end\":40670,\"start\":40669},{\"end\":40861,\"start\":40860},{\"end\":40869,\"start\":40868},{\"end\":40877,\"start\":40876},{\"end\":40883,\"start\":40882},{\"end\":40890,\"start\":40889},{\"end\":41115,\"start\":41114},{\"end\":41123,\"start\":41122},{\"end\":41235,\"start\":41234},{\"end\":41247,\"start\":41246},{\"end\":41443,\"start\":41442},{\"end\":41445,\"start\":41444},{\"end\":41455,\"start\":41454},{\"end\":41467,\"start\":41466},{\"end\":41478,\"start\":41477},{\"end\":41489,\"start\":41488},{\"end\":41767,\"start\":41766},{\"end\":41769,\"start\":41768},{\"end\":41780,\"start\":41779},{\"end\":41794,\"start\":41793},{\"end\":41806,\"start\":41805},{\"end\":41808,\"start\":41807},{\"end\":42195,\"start\":42194},{\"end\":42201,\"start\":42200},{\"end\":42210,\"start\":42209},{\"end\":42217,\"start\":42216},{\"end\":42337,\"start\":42336},{\"end\":42347,\"start\":42346},{\"end\":42349,\"start\":42348},{\"end\":42363,\"start\":42362},{\"end\":42377,\"start\":42376},{\"end\":42379,\"start\":42378},{\"end\":42389,\"start\":42388},{\"end\":42764,\"start\":42763},{\"end\":42766,\"start\":42765},{\"end\":42779,\"start\":42778},{\"end\":42791,\"start\":42790},{\"end\":42802,\"start\":42801},{\"end\":43073,\"start\":43072},{\"end\":43080,\"start\":43079},{\"end\":43087,\"start\":43086},{\"end\":43094,\"start\":43093},{\"end\":43096,\"start\":43095},{\"end\":43302,\"start\":43301},{\"end\":43312,\"start\":43311},{\"end\":43323,\"start\":43322},{\"end\":43564,\"start\":43563},{\"end\":43573,\"start\":43572},{\"end\":43736,\"start\":43735},{\"end\":43744,\"start\":43743},{\"end\":43751,\"start\":43750},{\"end\":43760,\"start\":43759},{\"end\":43769,\"start\":43768},{\"end\":43777,\"start\":43776},{\"end\":43783,\"start\":43782},{\"end\":43791,\"start\":43790},{\"end\":44103,\"start\":44102},{\"end\":44105,\"start\":44104},{\"end\":44115,\"start\":44114},{\"end\":44117,\"start\":44116},{\"end\":44296,\"start\":44295},{\"end\":44298,\"start\":44297},{\"end\":44311,\"start\":44310},{\"end\":44313,\"start\":44312},{\"end\":44321,\"start\":44320},{\"end\":44572,\"start\":44571},{\"end\":44574,\"start\":44573},{\"end\":44584,\"start\":44583},{\"end\":44586,\"start\":44585},{\"end\":44598,\"start\":44597},{\"end\":44780,\"start\":44779},{\"end\":44782,\"start\":44781},{\"end\":44792,\"start\":44791},{\"end\":44955,\"start\":44954},{\"end\":44969,\"start\":44968},{\"end\":44982,\"start\":44981},{\"end\":44984,\"start\":44983},{\"end\":45167,\"start\":45166},{\"end\":45176,\"start\":45175},{\"end\":45565,\"start\":45564},{\"end\":45571,\"start\":45570},{\"end\":45582,\"start\":45581},{\"end\":45593,\"start\":45592},{\"end\":45807,\"start\":45806},{\"end\":45809,\"start\":45808},{\"end\":45820,\"start\":45819},{\"end\":45822,\"start\":45821},{\"end\":45832,\"start\":45831},{\"end\":46074,\"start\":46073},{\"end\":46081,\"start\":46080},{\"end\":46087,\"start\":46086},{\"end\":46095,\"start\":46094},{\"end\":46226,\"start\":46225},{\"end\":46233,\"start\":46232},{\"end\":46239,\"start\":46238},{\"end\":46250,\"start\":46249},{\"end\":46252,\"start\":46251},{\"end\":46260,\"start\":46259},{\"end\":46496,\"start\":46495},{\"end\":46503,\"start\":46502},{\"end\":46510,\"start\":46509},{\"end\":46516,\"start\":46515},{\"end\":46524,\"start\":46523},{\"end\":46530,\"start\":46529},{\"end\":46536,\"start\":46535},{\"end\":46752,\"start\":46751},{\"end\":46765,\"start\":46764},{\"end\":46776,\"start\":46775},{\"end\":46788,\"start\":46787},{\"end\":46799,\"start\":46798},{\"end\":47055,\"start\":47054},{\"end\":47069,\"start\":47068},{\"end\":47071,\"start\":47070},{\"end\":47085,\"start\":47084},{\"end\":47100,\"start\":47099},{\"end\":47102,\"start\":47101},{\"end\":47115,\"start\":47114},{\"end\":47130,\"start\":47129},{\"end\":47136,\"start\":47135},{\"end\":47494,\"start\":47493},{\"end\":47508,\"start\":47507},{\"end\":47510,\"start\":47509},{\"end\":47524,\"start\":47523},{\"end\":47534,\"start\":47533},{\"end\":47536,\"start\":47535},{\"end\":47546,\"start\":47545},{\"end\":47561,\"start\":47560},{\"end\":47885,\"start\":47884},{\"end\":47893,\"start\":47892},{\"end\":47907,\"start\":47906},{\"end\":47917,\"start\":47916},{\"end\":47925,\"start\":47924},{\"end\":47927,\"start\":47926},{\"end\":47938,\"start\":47937},{\"end\":47942,\"start\":47939},{\"end\":47955,\"start\":47954},{\"end\":47957,\"start\":47956},{\"end\":47970,\"start\":47969},{\"end\":48367,\"start\":48366},{\"end\":48369,\"start\":48368},{\"end\":48377,\"start\":48376},{\"end\":48389,\"start\":48388},{\"end\":48399,\"start\":48398},{\"end\":48411,\"start\":48410},{\"end\":48630,\"start\":48629},{\"end\":48638,\"start\":48637},{\"end\":48645,\"start\":48644},{\"end\":48651,\"start\":48650},{\"end\":48822,\"start\":48821},{\"end\":48834,\"start\":48833},{\"end\":49030,\"start\":49029},{\"end\":49040,\"start\":49039},{\"end\":49049,\"start\":49048},{\"end\":49058,\"start\":49057},{\"end\":49067,\"start\":49066},{\"end\":49079,\"start\":49078},{\"end\":49089,\"start\":49088},{\"end\":49100,\"start\":49099},{\"end\":49107,\"start\":49106},{\"end\":49121,\"start\":49120},{\"end\":49418,\"start\":49417},{\"end\":49426,\"start\":49425},{\"end\":49433,\"start\":49432},{\"end\":49440,\"start\":49439},{\"end\":49446,\"start\":49445},{\"end\":49452,\"start\":49451},{\"end\":49459,\"start\":49458},{\"end\":49467,\"start\":49466},{\"end\":49695,\"start\":49694},{\"end\":49704,\"start\":49703},{\"end\":49875,\"start\":49874},{\"end\":49885,\"start\":49884},{\"end\":49894,\"start\":49893},{\"end\":49905,\"start\":49904},{\"end\":49911,\"start\":49910},{\"end\":49913,\"start\":49912},{\"end\":49919,\"start\":49918},{\"end\":50169,\"start\":50168},{\"end\":50179,\"start\":50178},{\"end\":50187,\"start\":50186},{\"end\":50195,\"start\":50194},{\"end\":50408,\"start\":50407},{\"end\":50418,\"start\":50417},{\"end\":50428,\"start\":50427},{\"end\":50430,\"start\":50429},{\"end\":50439,\"start\":50438},{\"end\":50451,\"start\":50450},{\"end\":50460,\"start\":50459},{\"end\":50675,\"start\":50674},{\"end\":50690,\"start\":50689},{\"end\":50699,\"start\":50698},{\"end\":50934,\"start\":50933},{\"end\":50946,\"start\":50945},{\"end\":50959,\"start\":50958},{\"end\":50961,\"start\":50960},{\"end\":50972,\"start\":50971},{\"end\":50974,\"start\":50973},{\"end\":50987,\"start\":50986},{\"end\":51264,\"start\":51263},{\"end\":51278,\"start\":51277},{\"end\":51288,\"start\":51287},{\"end\":51302,\"start\":51301},{\"end\":51315,\"start\":51314},{\"end\":51525,\"start\":51524},{\"end\":51535,\"start\":51534},{\"end\":51546,\"start\":51545},{\"end\":51555,\"start\":51554},{\"end\":51791,\"start\":51790},{\"end\":51803,\"start\":51802},{\"end\":51815,\"start\":51814},{\"end\":51822,\"start\":51821},{\"end\":51831,\"start\":51830},{\"end\":51839,\"start\":51838},{\"end\":51857,\"start\":51856},{\"end\":51869,\"start\":51868},{\"end\":51880,\"start\":51879},{\"end\":52152,\"start\":52151},{\"end\":52160,\"start\":52159},{\"end\":52172,\"start\":52171},{\"end\":52336,\"start\":52335},{\"end\":52344,\"start\":52343},{\"end\":52487,\"start\":52486},{\"end\":52657,\"start\":52656},{\"end\":52668,\"start\":52667},{\"end\":52679,\"start\":52678},{\"end\":52689,\"start\":52688},{\"end\":52702,\"start\":52701},{\"end\":52711,\"start\":52710},{\"end\":52713,\"start\":52712},{\"end\":52722,\"start\":52721},{\"end\":52732,\"start\":52731},{\"end\":52982,\"start\":52981},{\"end\":52990,\"start\":52989},{\"end\":52996,\"start\":52995},{\"end\":53004,\"start\":53003},{\"end\":53010,\"start\":53009},{\"end\":53012,\"start\":53011},{\"end\":53270,\"start\":53269},{\"end\":53278,\"start\":53277},{\"end\":53280,\"start\":53279},{\"end\":53514,\"start\":53513},{\"end\":53522,\"start\":53521},{\"end\":53524,\"start\":53523},{\"end\":53533,\"start\":53532},{\"end\":53535,\"start\":53534},{\"end\":53545,\"start\":53544},{\"end\":53547,\"start\":53546},{\"end\":53791,\"start\":53790},{\"end\":53806,\"start\":53805},{\"end\":53821,\"start\":53820},{\"end\":53836,\"start\":53835},{\"end\":54013,\"start\":54012},{\"end\":54187,\"start\":54186},{\"end\":54193,\"start\":54192},{\"end\":54199,\"start\":54198},{\"end\":54209,\"start\":54208},{\"end\":54215,\"start\":54214},{\"end\":54221,\"start\":54220},{\"end\":54384,\"start\":54383},{\"end\":54390,\"start\":54389},{\"end\":54396,\"start\":54395},{\"end\":54406,\"start\":54405},{\"end\":54718,\"start\":54717},{\"end\":54731,\"start\":54730},{\"end\":54985,\"start\":54984},{\"end\":54994,\"start\":54993},{\"end\":55003,\"start\":55002},{\"end\":55005,\"start\":55004},{\"end\":55014,\"start\":55013},{\"end\":55027,\"start\":55026}]", "bib_author_last_name": "[{\"end\":38620,\"start\":38613},{\"end\":38632,\"start\":38626},{\"end\":38879,\"start\":38872},{\"end\":38889,\"start\":38885},{\"end\":39067,\"start\":39058},{\"end\":39078,\"start\":39071},{\"end\":39096,\"start\":39082},{\"end\":39109,\"start\":39100},{\"end\":39119,\"start\":39113},{\"end\":39134,\"start\":39125},{\"end\":39566,\"start\":39561},{\"end\":39577,\"start\":39572},{\"end\":39591,\"start\":39581},{\"end\":39599,\"start\":39595},{\"end\":39606,\"start\":39603},{\"end\":39785,\"start\":39783},{\"end\":39796,\"start\":39789},{\"end\":39939,\"start\":39933},{\"end\":39953,\"start\":39943},{\"end\":39963,\"start\":39957},{\"end\":39973,\"start\":39967},{\"end\":39991,\"start\":39977},{\"end\":40007,\"start\":39997},{\"end\":40417,\"start\":40411},{\"end\":40431,\"start\":40421},{\"end\":40443,\"start\":40437},{\"end\":40455,\"start\":40447},{\"end\":40656,\"start\":40649},{\"end\":40667,\"start\":40660},{\"end\":40686,\"start\":40671},{\"end\":40866,\"start\":40862},{\"end\":40874,\"start\":40870},{\"end\":40880,\"start\":40878},{\"end\":40887,\"start\":40884},{\"end\":40901,\"start\":40891},{\"end\":41120,\"start\":41116},{\"end\":41129,\"start\":41124},{\"end\":41244,\"start\":41236},{\"end\":41256,\"start\":41248},{\"end\":41452,\"start\":41446},{\"end\":41464,\"start\":41456},{\"end\":41475,\"start\":41468},{\"end\":41486,\"start\":41479},{\"end\":41498,\"start\":41490},{\"end\":41777,\"start\":41770},{\"end\":41791,\"start\":41781},{\"end\":41803,\"start\":41795},{\"end\":41814,\"start\":41809},{\"end\":42198,\"start\":42196},{\"end\":42207,\"start\":42202},{\"end\":42214,\"start\":42211},{\"end\":42221,\"start\":42218},{\"end\":42344,\"start\":42338},{\"end\":42360,\"start\":42350},{\"end\":42374,\"start\":42364},{\"end\":42386,\"start\":42380},{\"end\":42397,\"start\":42390},{\"end\":42776,\"start\":42767},{\"end\":42788,\"start\":42780},{\"end\":42799,\"start\":42792},{\"end\":42810,\"start\":42803},{\"end\":43077,\"start\":43074},{\"end\":43084,\"start\":43081},{\"end\":43091,\"start\":43088},{\"end\":43101,\"start\":43097},{\"end\":43309,\"start\":43303},{\"end\":43320,\"start\":43313},{\"end\":43328,\"start\":43324},{\"end\":43570,\"start\":43565},{\"end\":43581,\"start\":43574},{\"end\":43741,\"start\":43737},{\"end\":43748,\"start\":43745},{\"end\":43757,\"start\":43752},{\"end\":43766,\"start\":43761},{\"end\":43774,\"start\":43770},{\"end\":43780,\"start\":43778},{\"end\":43788,\"start\":43784},{\"end\":43795,\"start\":43792},{\"end\":44112,\"start\":44106},{\"end\":44128,\"start\":44118},{\"end\":44308,\"start\":44299},{\"end\":44318,\"start\":44314},{\"end\":44333,\"start\":44322},{\"end\":44581,\"start\":44575},{\"end\":44595,\"start\":44587},{\"end\":44607,\"start\":44599},{\"end\":44789,\"start\":44783},{\"end\":44795,\"start\":44793},{\"end\":44966,\"start\":44956},{\"end\":44979,\"start\":44970},{\"end\":44991,\"start\":44985},{\"end\":45173,\"start\":45168},{\"end\":45185,\"start\":45177},{\"end\":45568,\"start\":45566},{\"end\":45579,\"start\":45572},{\"end\":45590,\"start\":45583},{\"end\":45598,\"start\":45594},{\"end\":45817,\"start\":45810},{\"end\":45829,\"start\":45823},{\"end\":45842,\"start\":45833},{\"end\":46078,\"start\":46075},{\"end\":46084,\"start\":46082},{\"end\":46092,\"start\":46088},{\"end\":46098,\"start\":46096},{\"end\":46230,\"start\":46227},{\"end\":46236,\"start\":46234},{\"end\":46247,\"start\":46240},{\"end\":46257,\"start\":46253},{\"end\":46269,\"start\":46261},{\"end\":46500,\"start\":46497},{\"end\":46507,\"start\":46504},{\"end\":46513,\"start\":46511},{\"end\":46521,\"start\":46517},{\"end\":46527,\"start\":46525},{\"end\":46533,\"start\":46531},{\"end\":46541,\"start\":46537},{\"end\":46762,\"start\":46753},{\"end\":46773,\"start\":46766},{\"end\":46785,\"start\":46777},{\"end\":46796,\"start\":46789},{\"end\":46806,\"start\":46800},{\"end\":47066,\"start\":47056},{\"end\":47082,\"start\":47072},{\"end\":47097,\"start\":47086},{\"end\":47112,\"start\":47103},{\"end\":47127,\"start\":47116},{\"end\":47133,\"start\":47131},{\"end\":47140,\"start\":47137},{\"end\":47505,\"start\":47495},{\"end\":47521,\"start\":47511},{\"end\":47531,\"start\":47525},{\"end\":47543,\"start\":47537},{\"end\":47558,\"start\":47547},{\"end\":47564,\"start\":47562},{\"end\":47890,\"start\":47886},{\"end\":47904,\"start\":47894},{\"end\":47914,\"start\":47908},{\"end\":47922,\"start\":47918},{\"end\":47935,\"start\":47928},{\"end\":47952,\"start\":47943},{\"end\":47967,\"start\":47958},{\"end\":47982,\"start\":47971},{\"end\":48374,\"start\":48370},{\"end\":48386,\"start\":48378},{\"end\":48396,\"start\":48390},{\"end\":48408,\"start\":48400},{\"end\":48421,\"start\":48412},{\"end\":48635,\"start\":48631},{\"end\":48642,\"start\":48639},{\"end\":48648,\"start\":48646},{\"end\":48655,\"start\":48652},{\"end\":48831,\"start\":48823},{\"end\":48840,\"start\":48835},{\"end\":49037,\"start\":49031},{\"end\":49046,\"start\":49041},{\"end\":49055,\"start\":49050},{\"end\":49064,\"start\":49059},{\"end\":49076,\"start\":49068},{\"end\":49086,\"start\":49080},{\"end\":49097,\"start\":49090},{\"end\":49104,\"start\":49101},{\"end\":49118,\"start\":49108},{\"end\":49128,\"start\":49122},{\"end\":49423,\"start\":49419},{\"end\":49430,\"start\":49427},{\"end\":49437,\"start\":49434},{\"end\":49443,\"start\":49441},{\"end\":49449,\"start\":49447},{\"end\":49456,\"start\":49453},{\"end\":49464,\"start\":49460},{\"end\":49473,\"start\":49468},{\"end\":49701,\"start\":49696},{\"end\":49710,\"start\":49705},{\"end\":49882,\"start\":49876},{\"end\":49891,\"start\":49886},{\"end\":49902,\"start\":49895},{\"end\":49908,\"start\":49906},{\"end\":49916,\"start\":49914},{\"end\":49932,\"start\":49920},{\"end\":50176,\"start\":50170},{\"end\":50184,\"start\":50180},{\"end\":50192,\"start\":50188},{\"end\":50202,\"start\":50196},{\"end\":50415,\"start\":50409},{\"end\":50425,\"start\":50419},{\"end\":50436,\"start\":50431},{\"end\":50448,\"start\":50440},{\"end\":50457,\"start\":50452},{\"end\":50467,\"start\":50461},{\"end\":50687,\"start\":50676},{\"end\":50696,\"start\":50691},{\"end\":50708,\"start\":50700},{\"end\":50943,\"start\":50935},{\"end\":50956,\"start\":50947},{\"end\":50969,\"start\":50962},{\"end\":50984,\"start\":50975},{\"end\":50994,\"start\":50988},{\"end\":51275,\"start\":51265},{\"end\":51285,\"start\":51279},{\"end\":51299,\"start\":51289},{\"end\":51312,\"start\":51303},{\"end\":51329,\"start\":51316},{\"end\":51532,\"start\":51526},{\"end\":51543,\"start\":51536},{\"end\":51552,\"start\":51547},{\"end\":51563,\"start\":51556},{\"end\":51800,\"start\":51792},{\"end\":51812,\"start\":51804},{\"end\":51819,\"start\":51816},{\"end\":51828,\"start\":51823},{\"end\":51836,\"start\":51832},{\"end\":51854,\"start\":51840},{\"end\":51866,\"start\":51858},{\"end\":51877,\"start\":51870},{\"end\":51887,\"start\":51881},{\"end\":52157,\"start\":52153},{\"end\":52169,\"start\":52161},{\"end\":52178,\"start\":52173},{\"end\":52341,\"start\":52337},{\"end\":52349,\"start\":52345},{\"end\":52494,\"start\":52488},{\"end\":52665,\"start\":52658},{\"end\":52676,\"start\":52669},{\"end\":52686,\"start\":52680},{\"end\":52699,\"start\":52690},{\"end\":52708,\"start\":52703},{\"end\":52719,\"start\":52714},{\"end\":52729,\"start\":52723},{\"end\":52743,\"start\":52733},{\"end\":52987,\"start\":52983},{\"end\":52993,\"start\":52991},{\"end\":53001,\"start\":52997},{\"end\":53007,\"start\":53005},{\"end\":53017,\"start\":53013},{\"end\":53275,\"start\":53271},{\"end\":53285,\"start\":53281},{\"end\":53519,\"start\":53515},{\"end\":53530,\"start\":53525},{\"end\":53542,\"start\":53536},{\"end\":53558,\"start\":53548},{\"end\":53803,\"start\":53792},{\"end\":53818,\"start\":53807},{\"end\":53833,\"start\":53822},{\"end\":53849,\"start\":53837},{\"end\":54022,\"start\":54014},{\"end\":54190,\"start\":54188},{\"end\":54196,\"start\":54194},{\"end\":54206,\"start\":54200},{\"end\":54212,\"start\":54210},{\"end\":54218,\"start\":54216},{\"end\":54230,\"start\":54222},{\"end\":54387,\"start\":54385},{\"end\":54393,\"start\":54391},{\"end\":54403,\"start\":54397},{\"end\":54415,\"start\":54407},{\"end\":54728,\"start\":54719},{\"end\":54741,\"start\":54732},{\"end\":54991,\"start\":54986},{\"end\":55000,\"start\":54995},{\"end\":55011,\"start\":55006},{\"end\":55024,\"start\":55015},{\"end\":55032,\"start\":55028}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":38822,\"start\":38609},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6594356},\"end\":39001,\"start\":38824},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":220643528},\"end\":39493,\"start\":39003},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":244799212},\"end\":39779,\"start\":39495},{\"attributes\":{\"id\":\"b4\"},\"end\":39927,\"start\":39781},{\"attributes\":{\"doi\":\"arXiv:2103.13415\",\"id\":\"b5\"},\"end\":40338,\"start\":39929},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":222091017},\"end\":40626,\"start\":40340},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":11253972},\"end\":40786,\"start\":40628},{\"attributes\":{\"id\":\"b8\"},\"end\":41056,\"start\":40788},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":54457478},\"end\":41232,\"start\":41058},{\"attributes\":{\"doi\":\"arXiv:2101.05204\",\"id\":\"b10\"},\"end\":41440,\"start\":41234},{\"attributes\":{\"doi\":\"arXiv:2103.10380\",\"id\":\"b11\"},\"end\":41749,\"start\":41442},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2036193},\"end\":42146,\"start\":41751},{\"attributes\":{\"id\":\"b13\"},\"end\":42334,\"start\":42148},{\"attributes\":{\"doi\":\"arXiv:2103.14645\",\"id\":\"b14\"},\"end\":42669,\"start\":42336},{\"attributes\":{\"id\":\"b15\"},\"end\":42983,\"start\":42671},{\"attributes\":{\"id\":\"b16\"},\"end\":43253,\"start\":42985},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7200347},\"end\":43467,\"start\":43255},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5808102},\"end\":43733,\"start\":43469},{\"attributes\":{\"doi\":\"arXiv:1909.10351\",\"id\":\"b19\"},\"end\":44070,\"start\":43735},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6722621},\"end\":44238,\"start\":44072},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2923181},\"end\":44519,\"start\":44240},{\"attributes\":{\"id\":\"b22\"},\"end\":44733,\"start\":44521},{\"attributes\":{\"id\":\"b23\"},\"end\":44887,\"start\":44735},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":195908774},\"end\":45141,\"start\":44889},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1363510},\"end\":45488,\"start\":45143},{\"attributes\":{\"id\":\"b26\"},\"end\":45739,\"start\":45490},{\"attributes\":{\"id\":\"b27\"},\"end\":46005,\"start\":45741},{\"attributes\":{\"id\":\"b28\"},\"end\":46223,\"start\":46007},{\"attributes\":{\"id\":\"b29\"},\"end\":46437,\"start\":46225},{\"attributes\":{\"id\":\"b30\"},\"end\":46683,\"start\":46439},{\"attributes\":{\"id\":\"b31\"},\"end\":46962,\"start\":46685},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":219947110},\"end\":47419,\"start\":46964},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":213175590},\"end\":47783,\"start\":47421},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":234365467},\"end\":48283,\"start\":47785},{\"attributes\":{\"id\":\"b35\"},\"end\":48592,\"start\":48285},{\"attributes\":{\"id\":\"b36\"},\"end\":48750,\"start\":48594},{\"attributes\":{\"id\":\"b37\"},\"end\":48958,\"start\":48752},{\"attributes\":{\"id\":\"b38\"},\"end\":49364,\"start\":48960},{\"attributes\":{\"id\":\"b39\"},\"end\":49622,\"start\":49366},{\"attributes\":{\"id\":\"b40\"},\"end\":49838,\"start\":49624},{\"attributes\":{\"id\":\"b41\"},\"end\":50092,\"start\":49840},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":232352619},\"end\":50370,\"start\":50094},{\"attributes\":{\"id\":\"b43\"},\"end\":50600,\"start\":50372},{\"attributes\":{\"id\":\"b44\"},\"end\":50846,\"start\":50602},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":235352518},\"end\":51194,\"start\":50848},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":6844431},\"end\":51522,\"start\":51196},{\"attributes\":{\"id\":\"b47\"},\"end\":51709,\"start\":51524},{\"attributes\":{\"id\":\"b48\"},\"end\":52108,\"start\":51711},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":204838340},\"end\":52287,\"start\":52110},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":198179476},\"end\":52441,\"start\":52289},{\"attributes\":{\"id\":\"b51\"},\"end\":52627,\"start\":52443},{\"attributes\":{\"id\":\"b52\"},\"end\":52905,\"start\":52629},{\"attributes\":{\"id\":\"b53\"},\"end\":53163,\"start\":52907},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":215745611},\"end\":53437,\"start\":53165},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":207761262},\"end\":53730,\"start\":53439},{\"attributes\":{\"id\":\"b56\"},\"end\":54010,\"start\":53732},{\"attributes\":{\"id\":\"b57\"},\"end\":54121,\"start\":54012},{\"attributes\":{\"id\":\"b58\"},\"end\":54381,\"start\":54123},{\"attributes\":{\"id\":\"b59\"},\"end\":54596,\"start\":54383},{\"attributes\":{\"id\":\"b60\"},\"end\":54910,\"start\":54598},{\"attributes\":{\"id\":\"b61\"},\"end\":55190,\"start\":54912}]", "bib_title": "[{\"end\":38866,\"start\":38824},{\"end\":39054,\"start\":39003},{\"end\":39557,\"start\":39495},{\"end\":40407,\"start\":40340},{\"end\":40645,\"start\":40628},{\"end\":41112,\"start\":41058},{\"end\":41764,\"start\":41751},{\"end\":43299,\"start\":43255},{\"end\":43561,\"start\":43469},{\"end\":44100,\"start\":44072},{\"end\":44293,\"start\":44240},{\"end\":44952,\"start\":44889},{\"end\":45164,\"start\":45143},{\"end\":47052,\"start\":46964},{\"end\":47491,\"start\":47421},{\"end\":47882,\"start\":47785},{\"end\":50166,\"start\":50094},{\"end\":50931,\"start\":50848},{\"end\":51261,\"start\":51196},{\"end\":52149,\"start\":52110},{\"end\":52333,\"start\":52289},{\"end\":53267,\"start\":53165},{\"end\":53511,\"start\":53439}]", "bib_author": "[{\"end\":38622,\"start\":38609},{\"end\":38634,\"start\":38622},{\"end\":38881,\"start\":38868},{\"end\":38891,\"start\":38881},{\"end\":39069,\"start\":39056},{\"end\":39080,\"start\":39069},{\"end\":39098,\"start\":39080},{\"end\":39111,\"start\":39098},{\"end\":39121,\"start\":39111},{\"end\":39136,\"start\":39121},{\"end\":39568,\"start\":39559},{\"end\":39579,\"start\":39568},{\"end\":39593,\"start\":39579},{\"end\":39601,\"start\":39593},{\"end\":39608,\"start\":39601},{\"end\":39787,\"start\":39781},{\"end\":39798,\"start\":39787},{\"end\":39941,\"start\":39929},{\"end\":39955,\"start\":39941},{\"end\":39965,\"start\":39955},{\"end\":39975,\"start\":39965},{\"end\":39993,\"start\":39975},{\"end\":40009,\"start\":39993},{\"end\":40419,\"start\":40409},{\"end\":40433,\"start\":40419},{\"end\":40445,\"start\":40433},{\"end\":40457,\"start\":40445},{\"end\":40658,\"start\":40647},{\"end\":40669,\"start\":40658},{\"end\":40688,\"start\":40669},{\"end\":40868,\"start\":40860},{\"end\":40876,\"start\":40868},{\"end\":40882,\"start\":40876},{\"end\":40889,\"start\":40882},{\"end\":40903,\"start\":40889},{\"end\":41122,\"start\":41114},{\"end\":41131,\"start\":41122},{\"end\":41246,\"start\":41234},{\"end\":41258,\"start\":41246},{\"end\":41454,\"start\":41442},{\"end\":41466,\"start\":41454},{\"end\":41477,\"start\":41466},{\"end\":41488,\"start\":41477},{\"end\":41500,\"start\":41488},{\"end\":41779,\"start\":41766},{\"end\":41793,\"start\":41779},{\"end\":41805,\"start\":41793},{\"end\":41816,\"start\":41805},{\"end\":42200,\"start\":42194},{\"end\":42209,\"start\":42200},{\"end\":42216,\"start\":42209},{\"end\":42223,\"start\":42216},{\"end\":42346,\"start\":42336},{\"end\":42362,\"start\":42346},{\"end\":42376,\"start\":42362},{\"end\":42388,\"start\":42376},{\"end\":42399,\"start\":42388},{\"end\":42778,\"start\":42763},{\"end\":42790,\"start\":42778},{\"end\":42801,\"start\":42790},{\"end\":42812,\"start\":42801},{\"end\":43079,\"start\":43072},{\"end\":43086,\"start\":43079},{\"end\":43093,\"start\":43086},{\"end\":43103,\"start\":43093},{\"end\":43311,\"start\":43301},{\"end\":43322,\"start\":43311},{\"end\":43330,\"start\":43322},{\"end\":43572,\"start\":43563},{\"end\":43583,\"start\":43572},{\"end\":43743,\"start\":43735},{\"end\":43750,\"start\":43743},{\"end\":43759,\"start\":43750},{\"end\":43768,\"start\":43759},{\"end\":43776,\"start\":43768},{\"end\":43782,\"start\":43776},{\"end\":43790,\"start\":43782},{\"end\":43797,\"start\":43790},{\"end\":44114,\"start\":44102},{\"end\":44130,\"start\":44114},{\"end\":44310,\"start\":44295},{\"end\":44320,\"start\":44310},{\"end\":44335,\"start\":44320},{\"end\":44583,\"start\":44571},{\"end\":44597,\"start\":44583},{\"end\":44609,\"start\":44597},{\"end\":44791,\"start\":44779},{\"end\":44797,\"start\":44791},{\"end\":44968,\"start\":44954},{\"end\":44981,\"start\":44968},{\"end\":44993,\"start\":44981},{\"end\":45175,\"start\":45166},{\"end\":45187,\"start\":45175},{\"end\":45570,\"start\":45564},{\"end\":45581,\"start\":45570},{\"end\":45592,\"start\":45581},{\"end\":45600,\"start\":45592},{\"end\":45819,\"start\":45806},{\"end\":45831,\"start\":45819},{\"end\":45844,\"start\":45831},{\"end\":46080,\"start\":46073},{\"end\":46086,\"start\":46080},{\"end\":46094,\"start\":46086},{\"end\":46100,\"start\":46094},{\"end\":46232,\"start\":46225},{\"end\":46238,\"start\":46232},{\"end\":46249,\"start\":46238},{\"end\":46259,\"start\":46249},{\"end\":46271,\"start\":46259},{\"end\":46502,\"start\":46495},{\"end\":46509,\"start\":46502},{\"end\":46515,\"start\":46509},{\"end\":46523,\"start\":46515},{\"end\":46529,\"start\":46523},{\"end\":46535,\"start\":46529},{\"end\":46543,\"start\":46535},{\"end\":46764,\"start\":46751},{\"end\":46775,\"start\":46764},{\"end\":46787,\"start\":46775},{\"end\":46798,\"start\":46787},{\"end\":46808,\"start\":46798},{\"end\":47068,\"start\":47054},{\"end\":47084,\"start\":47068},{\"end\":47099,\"start\":47084},{\"end\":47114,\"start\":47099},{\"end\":47129,\"start\":47114},{\"end\":47135,\"start\":47129},{\"end\":47142,\"start\":47135},{\"end\":47507,\"start\":47493},{\"end\":47523,\"start\":47507},{\"end\":47533,\"start\":47523},{\"end\":47545,\"start\":47533},{\"end\":47560,\"start\":47545},{\"end\":47566,\"start\":47560},{\"end\":47892,\"start\":47884},{\"end\":47906,\"start\":47892},{\"end\":47916,\"start\":47906},{\"end\":47924,\"start\":47916},{\"end\":47937,\"start\":47924},{\"end\":47954,\"start\":47937},{\"end\":47969,\"start\":47954},{\"end\":47984,\"start\":47969},{\"end\":48376,\"start\":48366},{\"end\":48388,\"start\":48376},{\"end\":48398,\"start\":48388},{\"end\":48410,\"start\":48398},{\"end\":48423,\"start\":48410},{\"end\":48637,\"start\":48629},{\"end\":48644,\"start\":48637},{\"end\":48650,\"start\":48644},{\"end\":48657,\"start\":48650},{\"end\":48833,\"start\":48821},{\"end\":48842,\"start\":48833},{\"end\":49039,\"start\":49029},{\"end\":49048,\"start\":49039},{\"end\":49057,\"start\":49048},{\"end\":49066,\"start\":49057},{\"end\":49078,\"start\":49066},{\"end\":49088,\"start\":49078},{\"end\":49099,\"start\":49088},{\"end\":49106,\"start\":49099},{\"end\":49120,\"start\":49106},{\"end\":49130,\"start\":49120},{\"end\":49425,\"start\":49417},{\"end\":49432,\"start\":49425},{\"end\":49439,\"start\":49432},{\"end\":49445,\"start\":49439},{\"end\":49451,\"start\":49445},{\"end\":49458,\"start\":49451},{\"end\":49466,\"start\":49458},{\"end\":49475,\"start\":49466},{\"end\":49703,\"start\":49694},{\"end\":49712,\"start\":49703},{\"end\":49884,\"start\":49874},{\"end\":49893,\"start\":49884},{\"end\":49904,\"start\":49893},{\"end\":49910,\"start\":49904},{\"end\":49918,\"start\":49910},{\"end\":49934,\"start\":49918},{\"end\":50178,\"start\":50168},{\"end\":50186,\"start\":50178},{\"end\":50194,\"start\":50186},{\"end\":50204,\"start\":50194},{\"end\":50417,\"start\":50407},{\"end\":50427,\"start\":50417},{\"end\":50438,\"start\":50427},{\"end\":50450,\"start\":50438},{\"end\":50459,\"start\":50450},{\"end\":50469,\"start\":50459},{\"end\":50689,\"start\":50674},{\"end\":50698,\"start\":50689},{\"end\":50710,\"start\":50698},{\"end\":50945,\"start\":50933},{\"end\":50958,\"start\":50945},{\"end\":50971,\"start\":50958},{\"end\":50986,\"start\":50971},{\"end\":50996,\"start\":50986},{\"end\":51277,\"start\":51263},{\"end\":51287,\"start\":51277},{\"end\":51301,\"start\":51287},{\"end\":51314,\"start\":51301},{\"end\":51331,\"start\":51314},{\"end\":51534,\"start\":51524},{\"end\":51545,\"start\":51534},{\"end\":51554,\"start\":51545},{\"end\":51565,\"start\":51554},{\"end\":51802,\"start\":51790},{\"end\":51814,\"start\":51802},{\"end\":51821,\"start\":51814},{\"end\":51830,\"start\":51821},{\"end\":51838,\"start\":51830},{\"end\":51856,\"start\":51838},{\"end\":51868,\"start\":51856},{\"end\":51879,\"start\":51868},{\"end\":51889,\"start\":51879},{\"end\":52159,\"start\":52151},{\"end\":52171,\"start\":52159},{\"end\":52180,\"start\":52171},{\"end\":52343,\"start\":52335},{\"end\":52351,\"start\":52343},{\"end\":52496,\"start\":52486},{\"end\":52667,\"start\":52656},{\"end\":52678,\"start\":52667},{\"end\":52688,\"start\":52678},{\"end\":52701,\"start\":52688},{\"end\":52710,\"start\":52701},{\"end\":52721,\"start\":52710},{\"end\":52731,\"start\":52721},{\"end\":52745,\"start\":52731},{\"end\":52989,\"start\":52981},{\"end\":52995,\"start\":52989},{\"end\":53003,\"start\":52995},{\"end\":53009,\"start\":53003},{\"end\":53019,\"start\":53009},{\"end\":53277,\"start\":53269},{\"end\":53287,\"start\":53277},{\"end\":53521,\"start\":53513},{\"end\":53532,\"start\":53521},{\"end\":53544,\"start\":53532},{\"end\":53560,\"start\":53544},{\"end\":53805,\"start\":53790},{\"end\":53820,\"start\":53805},{\"end\":53835,\"start\":53820},{\"end\":53851,\"start\":53835},{\"end\":54024,\"start\":54012},{\"end\":54192,\"start\":54186},{\"end\":54198,\"start\":54192},{\"end\":54208,\"start\":54198},{\"end\":54214,\"start\":54208},{\"end\":54220,\"start\":54214},{\"end\":54232,\"start\":54220},{\"end\":54389,\"start\":54383},{\"end\":54395,\"start\":54389},{\"end\":54405,\"start\":54395},{\"end\":54417,\"start\":54405},{\"end\":54730,\"start\":54717},{\"end\":54743,\"start\":54730},{\"end\":54993,\"start\":54984},{\"end\":55002,\"start\":54993},{\"end\":55013,\"start\":55002},{\"end\":55026,\"start\":55013},{\"end\":55034,\"start\":55026}]", "bib_venue": "[{\"end\":38689,\"start\":38634},{\"end\":38896,\"start\":38891},{\"end\":39206,\"start\":39136},{\"end\":39624,\"start\":39608},{\"end\":39846,\"start\":39798},{\"end\":40103,\"start\":40025},{\"end\":40463,\"start\":40457},{\"end\":40694,\"start\":40688},{\"end\":40858,\"start\":40788},{\"end\":41135,\"start\":41131},{\"end\":41314,\"start\":41274},{\"end\":41565,\"start\":41516},{\"end\":41900,\"start\":41816},{\"end\":42192,\"start\":42148},{\"end\":42473,\"start\":42415},{\"end\":42761,\"start\":42671},{\"end\":43070,\"start\":42985},{\"end\":43350,\"start\":43330},{\"end\":43587,\"start\":43583},{\"end\":43873,\"start\":43813},{\"end\":44138,\"start\":44130},{\"end\":44363,\"start\":44335},{\"end\":44569,\"start\":44521},{\"end\":44777,\"start\":44735},{\"end\":45004,\"start\":44993},{\"end\":45271,\"start\":45187},{\"end\":45562,\"start\":45490},{\"end\":45804,\"start\":45741},{\"end\":46071,\"start\":46007},{\"end\":46297,\"start\":46271},{\"end\":46493,\"start\":46439},{\"end\":46749,\"start\":46685},{\"end\":47170,\"start\":47142},{\"end\":47570,\"start\":47566},{\"end\":48007,\"start\":47984},{\"end\":48364,\"start\":48285},{\"end\":48627,\"start\":48594},{\"end\":48819,\"start\":48752},{\"end\":49027,\"start\":48960},{\"end\":49415,\"start\":49366},{\"end\":49692,\"start\":49624},{\"end\":49872,\"start\":49840},{\"end\":50220,\"start\":50204},{\"end\":50405,\"start\":50372},{\"end\":50672,\"start\":50602},{\"end\":51010,\"start\":50996},{\"end\":51335,\"start\":51331},{\"end\":51610,\"start\":51565},{\"end\":51788,\"start\":51711},{\"end\":52191,\"start\":52180},{\"end\":52355,\"start\":52351},{\"end\":52484,\"start\":52443},{\"end\":52654,\"start\":52629},{\"end\":52979,\"start\":52907},{\"end\":53292,\"start\":53287},{\"end\":53563,\"start\":53560},{\"end\":53788,\"start\":53732},{\"end\":54036,\"start\":54024},{\"end\":54184,\"start\":54123},{\"end\":54473,\"start\":54417},{\"end\":54715,\"start\":54598},{\"end\":54982,\"start\":54912},{\"end\":39263,\"start\":39208},{\"end\":41971,\"start\":41902},{\"end\":45342,\"start\":45273}]"}}}, "year": 2023, "month": 12, "day": 17}