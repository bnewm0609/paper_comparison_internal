{"id": 212747947, "updated": "2023-10-06 17:56:53.769", "metadata": {"title": "Self-Supervised Log Parsing", "authors": "[{\"first\":\"Sasho\",\"last\":\"Nedelkoski\",\"middle\":[]},{\"first\":\"Jasmin\",\"last\":\"Bogatinovski\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Acker\",\"middle\":[]},{\"first\":\"Jorge\",\"last\":\"Cardoso\",\"middle\":[]},{\"first\":\"Odej\",\"last\":\"Kao\",\"middle\":[]}]", "venue": "ECML/PKDD", "journal": "122-138", "publication_date": {"year": 2020, "month": 3, "day": 17}, "abstract": "Logs are extensively used during the development and maintenance of software systems. They collect runtime events and allow tracking of code execution, which enables a variety of critical tasks such as troubleshooting and fault detection. However, large-scale software systems generate massive volumes of semi-structured log records, posing a major challenge for automated analysis. Parsing semi-structured records with free-form text log messages into structured templates is the first and crucial step that enables further analysis. Existing approaches rely on log-specific heuristics or manual rule extraction. These are often specialized in parsing certain log types, and thus, limit performance scores and generalization. We propose a novel parsing technique called NuLog that utilizes a self-supervised learning model and formulates the parsing task as masked language modeling (MLM). In the process of parsing, the model extracts summarizations from the logs in the form of a vector embedding. This allows the coupling of the MLM as pre-training with a downstream anomaly detection task. We evaluate the parsing performance of NuLog on 10 real-world log datasets and compare the results with 12 parsing techniques. The results show that NuLog outperforms existing methods in parsing accuracy with an average of 99% and achieves the lowest edit distance to the ground truth templates. Additionally, two case studies are conducted to demonstrate the ability of the approach for log-based anomaly detection in both supervised and unsupervised scenario. The results show that NuLog can be successfully used to support troubleshooting tasks. The implementation is available at https://github.com/nulog/nulog.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.07905", "mag": "3011332743", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/pkdd/NedelkoskiBACK20", "doi": "10.1007/978-3-030-67667-4_8"}}, "content": {"source": {"pdf_hash": "91cc9083b9cc952b66ee747feb7bfd9a46ce09b9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.07905v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.07905", "status": "GREEN"}}, "grobid": {"id": "e25fa82ac1effbb70473d2ff123c4a7f36ef0297", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/91cc9083b9cc952b66ee747feb7bfd9a46ce09b9.txt", "contents": "\nSelf-Supervised Log Parsing\n\n\nSasho Nedelkoski nedelkoski@tu-berlin.de \nDistributed Systems\nTU Berlin\nBerlinGermany\n\nEqual contribution\n\n\nJasmin Bogatinovski jasmin.bogatinovski@tu-berlin.de \nDistributed Systems\nTU Berlin\nBerlinGermany\n\nEqual contribution\n\n\nAlexander Acker alexander.acker@tu-berlin.de \nDistributed Systems\nTU Berlin\nBerlinGermany\n\nJorge Cardoso jcardoso@dei.uc.pt \nDepartment of Informatics Engineering/CISUC\nUniversity of Coimbra\nPortugal\n\nOdej Kao odej.kao@tu-berlin.de \nDistributed Systems\nTU Berlin\nBerlinGermany\n\nSelf-Supervised Log Parsing\nlog parsing \u00b7 transformers \u00b7 anomaly detection \u00b7 representation learning \u00b7 IT systems\nLogs are extensively used during the development and maintenance of software systems. They collect runtime events and allow tracking of code execution, which enables a variety of critical tasks such as troubleshooting and fault detection. However, large-scale software systems generate massive volumes of semi-structured log records, posing a major challenge for automated analysis. Parsing semi-structured records with free-form text log messages into structured templates is the first and crucial step that enables further analysis. Existing approaches rely on log-specific heuristics or manual rule extraction. These are often specialized in parsing certain log types, and thus, limit performance scores and generalization. We propose a novel parsing technique called NuLog that utilizes a self-supervised learning model and formulates the parsing task as masked language modeling (MLM). In the process of parsing, the model extracts summarizations from the logs in the form of a vector embedding. This allows the coupling of the MLM as pre-training with a downstream anomaly detection task. We evaluate the parsing performance of NuLog on 10 real-world log datasets and compare the results with 12 parsing techniques. The results show that NuLog outperforms existing methods in parsing accuracy with an average of 99% and achieves the lowest edit distance to the ground truth templates. Additionally, two case studies are conducted to demonstrate the ability of the approach for log-based anomaly detection in both supervised and unsupervised scenario. The results show that NuLog can be successfully used to support troubleshooting tasks. The implementation is available at https://github.com/nulog/nulog.\n\nIntroduction\n\nCurrent IT systems are a combination of complex multi-layered software and hardware. They enable applications of ever-increasing complexity and system diversity, where many technologies such as the Internet of Things (IoT), distributed processing frameworks, databases, and operating systems are used. The complexity and diversity of the systems relate to high managing and maintenance overhead for the operators to arXiv:2003.07905v1 [cs.LG] 17 Mar 2020 a point where they are no longer sufficient to holistically operate and manage these systems. Therefore, service providers are deploying various measures by introducing additional AI solutions for anomaly detection, error analysis, and recovery to the IT ecosystem [13]. The foundation for these data-driven troubleshooting solutions is the availability of data that describe the state of the systems. The large variety of technologies leads to diverse data compelling the developed methods to generalize well over different applications, operating systems, or cloud infrastructure management tools.\n\nOne specific data source -the logs, are commonly used to inspect the behavior of an IT system. They represent interactions between data, files, services, or applications, which are typically utilized by the developers, DevOps teams, and AI methods to understand system behaviors to detect, localize, and resolve problems that may arise [12]. The first step for understanding log information and their utilization for further automated analysis is to parse them. The content of a log record is an unstructured free-text written by software developers, which makes it difficult to structure. It is a composition of constant string templates and variable values. The template is the logging instruction (e.g. print(), log.info()) from which the log message is produced. It records a specific system event. The general objective of a log parser is the transformation of the unstructured free-text into a structured log template and an associated list of variables. For example, the template \"Attempting claim: memory * MB, disk * GB, vcpus * CPU\" is associated with the variable list [\"2048\", \"20, \"1\"]. Here, * denotes the position of each variable and is associated with the positions of the values within the list. The variable list can be empty if a template does not contain variable parts.\n\nTraditional log parsing techniques rely on regular expressions designed and maintained by human experts. Large systems consisting of diverse software and hardware components render it intricate to maintain this manual effort. Additionally, frequent software updates necessitate constant checking and adjusting of these statements, which is a tedious and error-prone task. Related log parsing methods [2,4,6,20] depend on parse trees, heuristics, and domain knowledge. They are either specialized to perform well on logs from specific systems or can reliably parse data with a low variety of unique templates. Analyzing the performance of existing log parsing methods on a variety of diverse systems reveals their lack of robustness to produce consistently good parsing results. This implies the necessity to choose a parsing method for the application or system at hand and incorporating domain-specific knowledge. Operators of large IT infrastructures would end up with the overhead of managing different parsing methods for their components whereof each need to be accordingly understood. Based on this, we state that log parsing methods have to be accurate on log data from various systems ranging from single applications over mobile operating systems to cloud infrastructure management platforms with the least human intervention.\n\n\nContribution.\n\nWe propose a self-supervised method for log parsing NuLog, which utilizes the transformer architecture [1,17]. Self-supervised learning is a form of unsupervised learning where parts of the data provide supervision. To build the model, the learning task is formulated such that the presence of a word on a particular position in a log message is conditioned on its context. The key idea for parsing is that the correct prediction of the masked word means that the word is a part of the log template otherwise, it is a parameter of the log. The advantages of this approach are that it can produce both a log template and a numerical vector sumarization, while domain knowledge is not needed. Through exhaustive experimentation, we show that NuLog outperforms the previous state of the art log parsing methods and achieves the best scores overall. The model is robust and generalizes well across different datasets. Further, we illustrate two use cases, supervised and unsupervised, on how the model can be coupled with and fine-tuned for downstream tasks like anomaly detection. The results suggest that the knowledge obtained during the masked language modeling in for the log parsing phase is useful as a good prior knowledge for the downstream tasks.\n\n\nRelated Work\n\nAutomated log parsing is important due to its practical relevance for the maintenance and troubleshooting of software systems. A significant amount of research and development for automated log parsing methods has been published in both industry and academia [5,19]. Parsing techniques can be distinguished in various aspects, including technological, operation mode, and preprocessing. In Fig. 1, we give an overview of the existing methods.\n\nClustering The main assumption in these methods is that the message types coincide in similar groups. Various clustering methods with proper string matching distances have been used. LKE [3] applies weighted edit distance with hierarchical clustering to do log key extraction and a group splitting strategy to fine-tune the obtained log groups. LogSig [15] is a message signature-based algorithm that searches for the most representative message signatures, heavily utilizing domain knowledge to determine the number of clusters. SHISO [9] is creating a structured tree using the nodes generated from log messages which enables a real-time update of new log messages if a match with previously existing log templates fails. LenMa [14] utilizes a clustering approach based on sequences of word lengths appearing in the logs. LogMine [4] creates a hierarchy of log templates, that allows the user to choose the description level of interest.\n\nFrequent pattern mining assumes that a message type is a frequent set of tokens that appear throughout the logs. The procedures involve creating frequent sets, grouping the log messages, and extraction of message types. Representative parsers for this group are SLCT, LFA, and LogCluster [10,11,18].\n\nEvolutionary is the last category. Its member MoLFI [8] uses an evolutionary approach to find the Pareto optimal set of message templates.\n\nLog-structure heuristics methods produce the best results among the different adopted techniques [5,19]. They usually exploit different properties that emerge from the structure of the log. The state-of-the-art Drain [6] assumes that at the beginning of the logs the words do not vary too much. It uses this assumption to create a tree of fixed depth which can be easily modified for new groups. Other parsing methods in this group are IPLoM and AEL [7,18] Longest-common sub-sequence uses the longest common subsequence algorithm to dynamically extract log patterns from incoming logs. Here the most representative parser is Spell [2].\n\nOur method relates to the novel Neural category in the taxonomy of log parsing methods. Different from the current state-of-the-art heuristic-based methods, our method does not require any domain knowledge. Through empirical results, we show that the model is robust and applicable to a range of log types in different systems. We believe that in future this category will have the most influence considering the advances of deep learning.\n\n\nNeural Log Parsing\n\n\nPreliminaries\n\nWe define the logs as sequences of temporally ordered unstructured text messages L = (l i : i = 1, 2, ...), where each message l i is generated by a logging instruction (e.g. printf(), log.info()) within the software source code, and i is its positional index within the sequence. The log messages consist of a constant and an optional varying part, respectively referred to as log template and variables. We define log templates and variables as tuples EV = ((e i , v i ) : e \u2208 E, i = 1, 2, ...), where E is the finite set of all log event templates, K = |E| is the number of all unique templates and v i is a list of variables for the respectively associated template. They are associated with its original log message by the positional index i.\n\nThe smallest inseparable singleton object within a log message is a token. Each log message consists of a bounded sequence of tokens, t i = (t j : t \u2208 T, j = 1, 2, ..., |t i |), where T is a set of all tokens, j is the positional index of a token within the log message l i , and |t i | is the total number of tokens in l i . For different l i , |t i | can vary. Depending on the concrete tokenization method, t can be a word, word piece, or character. Therefore, tokenization is defined as a transformation function T : l i \u2192 t i , \u2200i.\n\nWith respect to our proposed log parsing method, the notions of context and embedding vector are additionally introduced. Given a token t j , its context is defined by a preceding and subsequent sequence of tokens, i.e. a tuple of sequences: C(t j ) = ((t a , t a+1 , ..., t j\u22121 ), (t j+1 , t j+2 , ..., t b )), where a < j < b. An embedding vector is a d-dimensional real valued vector representation s \u2208 R d of either a token or a log message.\n\nWe establish a requirement and a property for the proposed log parsing method: Requirement 1 Given a temporally ordered sequence of log messages L, generated from an unknown set E of distinct log templates, the log parsing method should provide a mapping function f 1 : L \u2192 EV . Property 1 is a desirable feature of a log template extractor. While, each log template maps to a finite set of values, bounded with the number of unique log templates, this features allows for vector representation of a log hence opens a possibility for addressing various downstream tasks.\n\n\nLog Parsers\nFrequent pattern mining -SLCT -LFA -LogCluster Clustering -LKE -LogSig -SHISHO -LenMa -LogMine Log-structure heuristics -AEL -Drain -IPLoM Longest- common subsequence -Spell Evolutionary -MoLFI Neural -NuLog\nThe generated vector representations should be closer embedding vectors for log messaged belonging to the same log template and distant embedding vectors for log messages belonging to distinct log templates. For example, the embedding vectors for \"Took 10 seconds to create a VM\" and \"Took 9 seconds to create a VM\" should have a small distance while vectors for \"Took 9 seconds to create a VM\" and \"Failed to create VM 3\" should be distant.\n\nThe goal of the proposed method is to mimic an operator's comprehension of logs. Given the task of identifying all event templates in a log, a reasonable approach is to pay close attention to parts that re-appear constantly and ignore parts that change frequently within a certain context (e.g. per log message). This can be modelled as a probability distribution for each token conditioned on its context, i.e. P (t j |C(t j )). Such probability distribution would allow the distinction of constant and varying tokens, referring to solving Requirement 1. The generation of log embedding vectors would naturally enable utilization of such representation for fine-tuning in downstream tasks. Moreover, the representation is obtained by focusing on constant parts of the log message, as they are more predictable, providing the necessary generalization for Property 1.\n\n\nNuLog: Self-Attentive Neural Parsing with Transformers\n\nThe proposed methods are composed of preprocessing, model, and template extraction. The overall architecture based on an example log message input is depicted in Fig. 2.\n\nThe log preprocessor transforms the log messages into a suitable format for the model. It is composed of two main parts: tokenization and masking. Before the tokenization task, the meta-information from the logging frameworks is stripped, and the payload, i.e., the print statement, is used as input to the tokenization step.\n\nTokenization. Tokenization transforms each log message into a sequence of tokens. For NuLog, we utilize a simple filter based splitting criterion to perform a string split operation. We keep these filters short and simple, i.e. easy to construct. All concrete criteria are described in section 4.1. In Fig. 2 we illustrate the tokenization of the log message \"Deleting instance /var/lib/nova/instances/4b2ab87e23b4de\". If a splitting criterion matches white spaces, then the log message is tokenized as a list of three tokens [\"Deleting\", \"instance\", \"/var/lib/nova/instances/4b2ab87e23b4de\"]. In contrast to several related approaches that use additional hand-crafted regular expressions to parses parameters like IP addresses, numbers, and URLs, we do not parse any parameters with a regex expression. Such an approach is known to be error-prone and requires manual adjustments in different systems and even updates within the same system.\n\nMasking. The intuition behind the proposed parsing method is to learn a general semantic representation of the log data by analyzing occurrences of tokens within their context. We apply a general method from natural language (NLP) research called Masked Language Modeling (MLM). It is originally introduced in [ masking module takes the output of the tokenization step as input, which is a token sequence of a log message. A token from the sequence is randomly chosen and replaced with the special M ASK token. The masked token sequence is used as input for the model, while the masked token acts as the prediction target. To denote the start and end of a log message, we prepend a special CLS and apply padding with SP EC tokens. The number of padding tokens for each log message is given by M \u2212 |t i |, where M = max(|t i |) + 1, \u2200i is the maximal number of tokens across all log messages within the log dataset added by one, and |t i | is the number of tokens in the i-th log message. Note, that the added one ensures that each log message is padded by at least one SP EC token.\n\nModel. The method has two operation modes -offline and online. During the offline phase, log messages are used to tune all model parameters via backpropagation and optimal hyper-parameters are selected. During the online phase, every log message is passed forward through the model. This generates the respective log template and an embedding vector for each log message. Fig. 3 depicts the complete architecture. The model applies two operations on the input token vectors: token vectorization and positional encoding. The subsequent encoder structure takes the result of these operations as input. It is composed of two elements: self-attention layer and feedforward layer. The last model component is a single linear layer with a softmax activation overall tokens appearing in the logs. In the following, we provide a detailed explanation of each model element.\n\nSince all subsequent elements of the model expect numerical inputs, we initially transform the tokens into randomly initialized numerical vectors x \u2208 R d . These vectors are referred to as token embeddings and are part of the training process, which means they are adjusted during training to represent the semantic meaning of tokens depending on their context. These numerical token embeddings are passed to the positional encoding block. In contrast to e.g., recurrent architectures, attention-based models do not contain any notion of input order. Therefore, this information needs to be explicitly encoded and merged with the input vectors to take their position within the log message into account. This block calculates a vector p \u2208 R d representing the relative position of a token based on a sine and cosine function.\np 2k = sin j 10000 2k v , p 2k+1 = cos j 10000 2k+1 v .(1)\nHere, k = 0, 1, . . . , d \u2212 1 is the index of each element in p and j = 0, 1, . . . , M is the positional index of each token. Within the equations, the parameter k describes an exponential relationship between each value of vector p. Additionally, a sine and cosine function are interchangeably applied. Both allow better discrimination of the respective values within a specific vector of p. Furthermore, both functions have an approximately linear dependence on the position parameter j, which is hypothesized to make it easy for the model to attend to the respective positions. Finally, both vectors can be combined as x = x + p. The encoder block of our model starts with a multi-head attention element, where a softmax distribution over the token embeddings is calculated. Intuitively, it describes the significance of each embedding vector for the prediction of the target masked token. We summarize all token embedding vectors as rows of a matrix X and apply the following formula\nZ l = sof tmax Q l \u00d7 K T l \u221a w V l , for l = 1, 2, . . . , L,(2)\nwhere L denotes the number of attention heads, w = d L and d mod L = 0. The parameters Q, K and V are matrices, that correspond to the query, key, and value elements in Fig. 3. They are obtained by applying matrix multiplications between the input X and respective learnable weight matrices W Q l , W K l , W V l :\nQ l = X * \u00d7 W Q l , K l = X * \u00d7 W K l , V l = X * \u00d7 W V l ,(3)\nwhere W Q l , W K l , W V l \u2208 R M \u00d7w . The division by \u221a w stabilizes the gradients during training. After that, the softmax function is applied and the result is used to scale each token embedding vector: X l = X l \u00d7Z l . The scaled matrices X l are concatenated to a single matrix X of size M \u00d7 d. As depicted in Fig. 3 there is a residual connection between the input token matrix X and its respective attention transformation X , followed by a normalization layer norm. These are used for improving the performance of the model by tackling different potential problems encountered during the learning such as small gradients and the covariate shift phenomena. Based on this, the original input is updated by the attention-transformed equivalent as X = norm(X + X ).\n\nThe last element of the encoder consists of two feed-forward linear layers with a ReLU activation in between. It is applied individually on each row of X . Thereby, identical weights for every row are used, which can be described as a convolution over each attention-transformed matrix row with kernel size one. This step serves as additional information enrichment for the embeddings. Again, a residual connection followed by a normalization layer between the input matrix and the output of both layers is employed. This model element preserves the dimensionality X .\n\nThe final element of the model consists of a single linear layer. It receives the encoder result X and extracts the token embedding vector of the CLS token. Since every log message token sequence is pre-padded by this special token, it is the first row of the matrix, i.e. x 0 \u2208 X . The linear layer maps this vector of size d to a vector whose size corresponds to the total number of tokens |T| in the dataset. The subsequent softmax is utilized to calculate a probability distribution over each element of T. During training, the masked token is used as the target to be predicted. Since the last vector embedding of the CLS token is used for prediction, it is forced to summarize the log message. Otherwise, it would not be able to solve the masked token prediction task well enough across all tokens. We hypothesize that the constant part of log templates will constraint the model to learn similar CLS token embeddings when log messages are of the same template. This leads to a mapping of the log messages to their vector representation, which can after be used for diverse downstream tasks like anomaly detection. This log message embedding vector satisfies the proposed Property 1 (see Section 3.1).\n\n\nLog Template Extraction\n\nThe extraction of all log templates within a log dataset is executed online, after the model training. Therefore, we pass each log message as input and configure the masking module in a way that every token is masked consecutively, one at a time. We measure the model's ability to predict each token, and thus, decide whether the token is a constant part of the template or a variable. High confidence in the prediction of a specific token indicates a constant part of the template, while small confidence is interpreted as a variable. More specifically, we employ the following procedure. If the prediction of a particular token is in the top predictions, we consider it to be part of the constant part of the template, otherwise, it is considered to be a variable. For all variables, an indicator * is placed on its position within the log message. This addresses the Requirement 1 proposed in Section 3.1.\n\n\nEvaluation\n\nTo quantify the performance of the proposed method, we perform an exhaustive evaluation of the log parsing task on a set of ten benchmark datasets and compare the results with twelve other log template parsing methods. The datasets together with the implementation of the other parsers were obtained from the log benchmark [19]. Furthermore, the model of NuLog provides log message vector embeddings. We show that these, along with the model, can be used for anomaly detection as downstream tasks. \n\n\nDatasets\n\nThe log datasets employed in our experiments are summarized in Table 1. These realworld log data range from supercomputer logs (BGL and HPC), distributed system logs (HDFS, OpenStack, Spark), to standalone software logs (Apache, Windows, Mac, Android). To enable reproducibility, we follow the guidelines from [19] and utilize a random sample of 2000 log messages from each dataset, where the ground truth templates are available. The number of templates contained within each dataset is shown in table 1.\n\nThe BGL dataset is collected by Lawrence Livermore National Labs (LLNL) from BlueGene/L supercomputer system. HPC logs are collected from a high-performance cluster, consisting of 49 nodes with 6,152 cores. HDFS is a log data set collected from the Hadoop distributed file system deployed on a cluster of 203 nodes within the Amazon EC2 platform. OpenStack is a result of a conducted anomaly experiment within CloudLab with one control node, one network node and eight compute nodes. Spark is an aggregation of logs from the Spark system deployed within the Chinese University of Hongkong, which comprises 32 machines. The Apache HTTP server dataset consists of access and error logs from the apache web server. Windows, Mac, and Android datasets consist of logs generated from single machines using the respectively named operating system. HealthApp contains logs from an Android health application, recorded over ten days on a single android smartphone. As described in Section 3.2, the tokenization process of our method is implemented by splitting based on a filter. We list the applied splitting expressions for each dataset in Table 2. Besides, we also list the additional training parameters. The number of epochs is determined by an early stopping criterion, which terminated the learning when the loss converges. The hyperparameter is determined via cross-validation.\n\n\nEvaluation methods\n\nTo quantify the effectiveness of NuLog for log template generation from the presented eleven datasets, we compare it with twelve existing log parsing methods on parsing accuracy, edit distance, and robustness. We reproduced the results from Zhu et al. [19] for all known log parsers. Furthermore, we enriched the extensive benchmark reported by an additional metric, i.e., edit distance. Note, that all methods we comparing with are described in detail in Section 2. To evaluate the log message embeddings for the anomaly detection downstream tasks, we use the common metrics accuracy, recall, precision, and F1 score. In the following, we describe each evaluation metric.\n\nParsing Accuracy. To enable comparability between our method to the ones analyzed in the benchmark [19], we adopt their proposed parsing accuracy (P A) metric. It is defined as the ratio of correctly parsed log messages over the total number of log messages. After parsing, each log message is assigned to a log template. A log message is considered correctly parsed if its log template corresponds to the same group of log messages as the ground truth does. For example, if a log sequence [e 1 , e 2 , e 2 ] is parsed to [e 1 , e 4 , e 5 ], we get P A = 1 3 since the second and third messages are not grouped together.\n\nEdit distance. The P A metric is considered as the standard for evaluation of log parsing methods, but it has limitations when it comes to evaluating the template extraction in terms of string comparison. Consider a particular group of logs produced from single print(\"VM created successfully\") statement that is parsed with the word Template. As long as this is consistent over every occurrence of the templates from this group throughout the dataset, P A would still yield a perfect score for this template parsing result, regardless of the obvious error. Therefore, we introduce an additional evaluation metric: Levenshtein edit distance. This is a way of quantifying how dissimilar two log messages are to one another by counting the minimum number of operations required to transform one message into the other.\n\n\nParsing Results\n\nParsing Accuracy This section presents and discusses the log parsing P A results of NuLog on the benchmark datasets and compares them with twelve other related methods. These are presented in table 3. Specifically, each row contains the datasets while the compared methods are represented in the table columns. Additionally, the penultimate column contains the highest value of the first twelve columns -referred to as best of all -and the last column contains the results for NuLog. In the bold text, we highlight the best of the methods per dataset. HDFS and Apache datasets are most frequently parsed with 100% P A. This is because HDFS and Apache error logs have relatively unambiguous event templates that are simple to identify. On those, NuLog achieves comparable results. For the Spark, BGL and Windows dataset, the existing methods already achieve high P A values above 96% (BGL) or above 99% (Spark and Windows). Our proposed method can slightly outperform those. For the rather complex log data from OpenStack, HPC and HealthApp the baseline methods achieve a P A between 78% and 90%, which NuLog significantly outperforms by 4-13%.  PA robustness Employing a general parsing method in production requires a robust performance throughout different log datasets. With the proposed method, we explicitly aim at supporting a broad range of diverse log data types. Therefore, the robustness of NuLog is analyzed and compared to the related methods. Fig. 4 shows the accuracy distribution of each log parser across the log datasets within a boxplot. From left to right in the figure, the log parsers are arranged in ascending order of the median P A.\n\nThat is, LogSig has the lowest and NuLog obtains the highest parsing accuracy on the median. We postulate the criterion of achieving consistently high P A values across many different log types as crucial for their general use. However, it can be observed that, although most log parsing methods achieve high P A values of 90% for specific log datasets, they have a large variance when applied across all given log types. NuLog outperforms every other baseline method in terms of P A robustness yielding a median of 0.99, which even lies above the best of all median of 0.94. Edit distance As an evaluation metric, P A measures how well the parsing method can match log templates with the respective log messages throughout the dataset. Additionally, we want to verify the correctness of the templates, e.g., whether all variables are correctly identified. To achieve this, the edit distance score is employed to measure the dissimilarity between the parsed and the ground truth log templates. Note that this indicates that the objective is to achieve low edit distance values. All edit distance scores are listed in table 4. The table structure is the same as for P A results. In bold we highlight the best edit distance value across all tested methods per dataset. It can be seen that in terms of edit distance NuLog outperforms existing methods on the HDFS, Windows, Android, HealthApp and Mac datasets. It performs comparable on the BGL, HPC, Apache and OpenStack datasets and achieves a higher edit distance on the Spark log data.\n\nEdit distance robustness Similar to the P A robustness evaluation, we want to verify how consistent NuLog is performing in terms of edit distance across the different log datasets. Fig. 5 shows a box-plot that indicates the edit distance distribution of each log parser for all log datasets. From left to right in the figure, the log parsing methods are arranged in descending order of the median edit distance. Again, it can be observed that although most log parsing methods achieve the minimal edit distance scores under 10, most of them have a large variance over different datasets and are therefore not generally applicable for diverse log data types. MoLFI has the highest median edit distance, while Spell and Drain perform constantly well -i.e. small median edit distance values -for multiple datasets. Again, our proposed parsing method outperforms the lowest edit distance values with a median of 5.00, which is smaller the best of all median of 7.22.  \n\n\nCase study: Anomaly detection as a downstream task\n\nAnomaly detection in complex and distributed systems is a crucial task in distributed and complex IT systems. The on-time detection provides a way to take action towards preventing or fast-reacting to emerging problems. Ultimately, it allows the operator to satisfy the service level agreements.\n\nOur model architecture allows for coupling of the parsing approach and a downstream anomaly detection task. The knowledge obtained during the log parsing phase is used as a good prior bias for the downstream task. The architecture provides treating the problem of anomaly detection in both the supervised and unsupervised way. To illustrate this we designed two experimental case studies described in the following. \n\n\nUnsupervised anomaly detection\n\nWe test the log message embedding produced by NuLog for unsupervised log anomaly detection by employing a similar approach as during the parsing. We train the model for three epochs. Each token of a log message is masked and predicted based on the CLS token embedding. All respectively masked tokens that are not in the top-predicted tokens are marked as anomalies. We compute the percentage of anomalous tokens within the log message to decide whether the whole log message is anomalous. If it is larger than a threshold \u03b4, the log message is considered as an anomaly, otherwise as normal.\n\nWe show this process in the left part of Fig. 6.\n\nTo the best of our knowledge, only the BGL dataset contains anomaly labels for each individual log message, and is, therefore, suitable to evaluate the proposed anomaly detection approach. Due to its large volume, we use only the first 10% of it. For training 80% of that portion is utilized, while the rest is used for testing. In the first row of table 5 we show the accuracy, recall, precision, and F1 score results. It can be seen that the method yields scores between 0.999 and 1.0. We, therefore, regard these results as evidence that the log message embeddings can be used for the unsupervised detection of anomalous log messages. \n\n\nSupervised anomaly detection\n\nFor the second case study, we utilize log message embedding as a feature for supervised anomaly detection. The model is first trained on the self-supervised MLM task. After that, we replace the last softmax layer by a linear layer, that is adapted via supervised training of predicting a given CLS as either normal or anomaly, i.e., binary classification. For this downstream task, we applied a fine-tuning of two epochs. The first 10% of the BGL dataset were used for evaluation. Thereby, the model is trained on the first 80% and evaluated on the remaining 20%. The results are listed in the second row of Table 5 and show that two epochs of fine-tuning are sufficient to produce an F1 score of 0.99. It further adds evidence to the proposed hypothesize of enabling the application of the semantic log message embedding for different downstream tasks.\n\n\nConclusion\n\nTo address the problem of log parsing we adopt the masked word prediction learning task. The insight of having words appearing on the constant position of the log entry means that their correct prediction directly produces the log message type. The incorrect token prediction reflects various parts of the logs as are its parameters. The method also produces a numerical representation of the context of the log message, which primarily is utilized for parsing. This allows the model for utilization in downstream tasks such as anomaly detection.\n\nTo evaluate the effectiveness of NuLog, we conducted experiments on 10 real-world log datasets and evaluated it against 12 log parsers. Furthermore, we enhanced the evaluation protocol with the addition of a new measure to justify the offset of generated templates and the true log message types. The experimental results show that NuLog outperforms the existing log parsers in terms of accuracy, edit distance, and robustness. Furthermore, we conducted case studies on a real-world supervised and unsupervised anomaly detection task. The results show that the model and the representation learned during parsing with masked language modeling are beneficial for distinguishing between normal and abnormal logs in both supervised and unsupervised scenario.\n\nOur approach shows that log parsing can be performed with deep language modeling. This imply that future research in log parsing and anomaly detection should focus more into generalization accross domains, transfer of knowledge, and learning of meaningful log representations that could further improve the troubleshooting tasks critical for operation of IT systems.\n\nFig. 1 :\n1Taxonomy of log parses according to underlying technology they adopt.\n\nFig. 3 :\n3Model architecture of NuLog for parsing of the logs.\n\n3 :\n3Comparisons of log parsers and our method NuLog in parsing accuracy (PA).Dataset   SLCT AEL LKE LFA LogSig SHISHO LogCluster LenMa LogMine Spell Drain MoLFI\n\nFig. 4 :\n4Robustness evaluation on the parsing accuracy of the log parsers.\n\nFig. 5 :\n5Robustness evaluation on the edit distance of the log parsers.\n\n\n16] (where it is referred to as Cloze) and successfully applied in other NLP publications like[1]. OurINPUT LOG: Deleting instance /var/lib/nova/instances/4b2ab87e23b4_deFig. 2: Instance of parsing of a single log message with NuLog.Extraction \n\nTokenization \n\nvector representation \nof the log \n\n<CLS> <MASK> instance /var/lib/nova/instances/4b2ab87e23b4_de \n\n<CLS> \n\ngenerated template \n\n/var/lib/nova/instances/4b2ab87e23b4_de \nDeleting instance \n\nTOKENIZATION \n\nDeleting \n\n<CLS> \n\n<MASK> \n\n/var/lib/nova/instances/4b2ab87e23b4_de \nDeleting \n\n<CLS> \ninstance \n\n<MASK> \n\nDeleting instance \n<*> \n\nMasking \n\nModel \n\nMASKING \n\nMASKING \n\n\n\nTable 1 :\n1Log datasets and the number of log templates.System \nDescription \n#Templates \nBGL \nBlueGene Supercomputer \n120 \nAndroid Mobile Operating System \n166 \nOpenStack Cloud Operating System \n43 \nHDFS \nHadoop Distributed File System \n14 \nApache \nApache HTTP Server \n6 \nHPC \nHigh Performance Cluster (Los Alamos) \n46 \nWindows Windows 7 Computer Operating System \n50 \nHealthApp Mobile Application for Andriod Devices \n75 \nMac \nMacOS Operating System \n341 \nSpark \nUnified Analytics Engine for Big Data Processing, \n36 \n\n\n\nTable 2 :\n2NuLog hyperparameter setting.System \nTokenization filter \n#epochs \nBGL \n([ |:|\\(|\\)|=|,])|(core.)|(\\.{2,}) \n3 \n50 \nAndroid ([ |:|\\(|\\)|=|,|\"|\\{|\\}|@|\\$|\\[|\\]|\\||;]) \n5 \n25 \nOpenStack ([ |:|\\(|\\)|\"|\\{|\\}|@|\\$|\\[|\\]|\\||;]) \n6 \n5 \nHDFS \n(\\s+blk_)|(:)|(\\s) \n5 \n15 \nApache \n([ ]) \n5 \n12 \nHPC \n([ |=]) \n3 \n10 \nWindows ([ ]) \n5 \n95 \nHealthApp ([ ]) \n5 \n100 \nMac \n([ ])|([\\w-]+\\.){2,}[\\w-]+ \n10 300 \nSpark \n([ ])|(\\d+\\sB)|(\\d+\\sKB)|(\\d+\\.){3}\\d+ \n3 \n50 \n\n\n\nTable\n\n\nTable 4 :\n4Comparisons of log parsers and our method NuLog in edit distance. Android 16.9295 12.3505 39.2700 3.7580 9.9980 16.4175 10.1505 22.5325 3.2555 8.6680 6.6550 3.2210 3.2210 1.1905 HealthApp 17.1120 14.6675 21.6485 16.2365 20.2740 16.8455 24.4310 19.5045 16.5390 8.5345 19.0870 18.4965 14.6675 6.2075 Apache 14.4420 14.7115 18.4410 11.0260 10.3675 16.2765 12.4405 10.2655 13.5520 10.2335 10.2175 10.2175 10.2175 11.6915 OpenStack 21.8810 29.1730 67.8850 20.9855 28.1385 31.4860 18.5820 23.9795 18.5350 27.9840 17.1425 28.3855 17.1425 21.2605 Mac 27.9230 79.6790 28.7160 34.5600 41.8040 21.3275 19.8105 17.0620 19.9835 22.5930 19.5340 19.8815 17.062 2.8920Dataset \nLogSig LKE MoLFI SLCT LFA LogCluster SHISHO LogMine LenMa Spell \nAEL Drain BoA NuLog \nHDFS \n19.1595 17.9405 19.8430 13.6410 30.8190 28.3405 10.1145 16.2495 10.7620 9.2740 8.8200 8.8195 8.8195 3.2040 \nSpark \n13.0615 41.9175 14.1880 6.0275 9.1785 17.0820 7.9100 16.0040 10.9450 6.1290 3.8610 3.5325 3.5325 12.0800 \nBGL \n11.5420 12.5820 10.9250 9.8410 12.5240 12.9550 8.6305 19.2710 8.3730 7.9005 5.0140 4.9295 4.9295 5.5230 \nHPC \n4.4475 7.6490 3.8710 2.6250 3.1825 3.5795 \n7.8535 3.2185 2.9055 5.1290 1.4050 2.0155 1.4050 2.9595 \nWindows 7.6645 11.8335 14.1630 7.0065 10.2385 6.9670 \n5.6245 6.9190 20.6615 4.4055 11.9750 6.1720 5.6245 4.4860 \nMoLFI LogMine LogCluster LogSig LKE \nLenMa \nLFA \nSLCT SHISHO AEL \nSpell \nDrain \nBest NuLog \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nEdit distance \n\n\n\nTable 5 :\n5Scores for the downstream anomaly detection tasks.Accuracy Recall Precision F1 Score \nUnsupervised 0.999 0.999 1.000 \n0.999 \nSupervised \n0.999 1.000 0.999 \n0.999 \n\n\nJ Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)\n\nSpell: Streaming parsing of system event logs. M Du, F Li, IEEE 16th International Conference on. IEEEData Mining (ICDM)Du, M., Li, F.: Spell: Streaming parsing of system event logs. In: Data Mining (ICDM), 2016 IEEE 16th International Conference on. pp. 859-864. IEEE (2016)\n\nExecution anomaly detection in distributed systems through unstructured log analysis. Q Fu, J G Lou, Y Wang, J Li, ninth IEEE international conference on data mining. IEEEFu, Q., Lou, J.G., Wang, Y., Li, J.: Execution anomaly detection in distributed systems through unstructured log analysis. In: 2009 ninth IEEE international conference on data min- ing. pp. 149-158. IEEE (2009)\n\nLogmine: Fast pattern recognition for log analytics. H Hamooni, B Debnath, J Xu, H Zhang, G Jiang, A Mueen, CIKMHamooni, H., Debnath, B., Xu, J., Zhang, H., Jiang, G., Mueen, A.: Logmine: Fast pattern recognition for log analytics. In: CIKM (2016)\n\nAn evaluation study on log parsing and its use in log mining. P He, J Zhu, S He, J Li, M R Lyu, 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). IEEEHe, P., Zhu, J., He, S., Li, J., Lyu, M.R.: An evaluation study on log parsing and its use in log mining. In: 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). pp. 654-661. IEEE (2016)\n\nDrain: An online log parsing approach with fixed depth tree. P He, J Zhu, Z Zheng, M R Lyu, 2017 IEEE International Conference on Web Services (ICWS). IEEEHe, P., Zhu, J., Zheng, Z., Lyu, M.R.: Drain: An online log parsing approach with fixed depth tree. In: 2017 IEEE International Conference on Web Services (ICWS). pp. 33-40. IEEE (2017)\n\nAn automated approach for abstracting execution logs to execution events. Z M Jiang, A E Hassan, G Hamann, P Flora, Journal of Software Maintenance and Evolution: Research and Practice. 204Jiang, Z.M., Hassan, A.E., Hamann, G., Flora, P.: An automated approach for abstracting ex- ecution logs to execution events. Journal of Software Maintenance and Evolution: Research and Practice 20(4), 249-267 (2008)\n\nA search-based approach for accurate identification of log message formats. S Messaoudi, A Panichella, D Bianculli, L Briand, R Sasnauskas, Proceedings of the 26th Conference on Program Comprehension. the 26th Conference on Program ComprehensionMessaoudi, S., Panichella, A., Bianculli, D., Briand, L., Sasnauskas, R.: A search-based approach for accurate identification of log message formats. In: Proceedings of the 26th Conference on Program Comprehension. pp. 167-177 (2018)\n\nIncremental mining of system log format. M Mizutani, 2013 IEEE International Conference on Services Computing. IEEEMizutani, M.: Incremental mining of system log format. In: 2013 IEEE International Con- ference on Services Computing. pp. 595-602. IEEE (2013)\n\nAbstracting log lines to log event types for mining software system logs. M Nagappan, M A Vouk, 2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010). IEEENagappan, M., Vouk, M.A.: Abstracting log lines to log event types for mining software system logs. In: 2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010). pp. 114-117. IEEE (2010)\n\nAnomaly detection using program control flow graph mining from execution logs. A Nandi, A Mandal, S Atreja, G B Dasgupta, S Bhattacharya, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningNandi, A., Mandal, A., Atreja, S., Dasgupta, G.B., Bhattacharya, S.: Anomaly detection using program control flow graph mining from execution logs. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 215-224 (2016)\n\nAnomaly detection and classification using distributed tracing and deep learning. S Nedelkoski, J Cardoso, O Kao, 2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID). Nedelkoski, S., Cardoso, J., Kao, O.: Anomaly detection and classification using dis- tributed tracing and deep learning. In: 2019 19th IEEE/ACM International Sympo- sium on Cluster, Cloud and Grid Computing (CCGRID). pp. 241-250 (May 2019).\n\n. 10.1109/CCGRID.2019.00038https://doi.org/10.1109/CCGRID.2019.00038\n\nAnomaly detection from system tracing data using multimodal deep learning. S Nedelkoski, J Cardoso, O Kao, 10.1109/CLOUD.2019.000382019 IEEE 12th International Conference on Cloud Computing (CLOUD). Nedelkoski, S., Cardoso, J., Kao, O.: Anomaly detection from system tracing data using mul- timodal deep learning. In: 2019 IEEE 12th International Conference on Cloud Computing (CLOUD). pp. 179-186 (July 2019). https://doi.org/10.1109/CLOUD.2019.00038\n\nK Shima, arXiv:1611.03213Length matters: Clustering system log messages using length of words. arXiv preprintShima, K.: Length matters: Clustering system log messages using length of words. arXiv preprint arXiv:1611.03213 (2016)\n\nLogsig: Generating system events from raw textual logs. L Tang, T Li, C S Perng, Proceedings of the 20th ACM international conference on Information and knowledge management. the 20th ACM international conference on Information and knowledge managementTang, L., Li, T., Perng, C.S.: Logsig: Generating system events from raw textual logs. In: Proceedings of the 20th ACM international conference on Information and knowledge man- agement. pp. 785-794 (2011)\n\nCloze procedure: A new tool for measuring readability. W L Taylor, Journalism quarterly. 304Taylor, W.L.: Cloze procedure: A new tool for measuring readability. Journalism quarterly 30(4), 415-433 (1953)\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems. pp. 5998-6008 (2017)\n\nDetecting large-scale system problems by mining console logs. W Xu, L Huang, A Fox, D Patterson, M I Jordan, Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles. the ACM SIGOPS 22nd symposium on Operating systems principlesACMXu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.I.: Detecting large-scale system prob- lems by mining console logs. In: Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles. pp. 117-132. ACM (2009)\n\nTools and benchmarks for automated log parsing. J Zhu, S He, J Liu, P He, Q Xie, Z Zheng, M R Lyu, 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEEZhu, J., He, S., Liu, J., He, P., Xie, Q., Zheng, Z., Lyu, M.R.: Tools and benchmarks for automated log parsing. In: 2019 IEEE/ACM 41st International Conference on Software En- gineering: Software Engineering in Practice (ICSE-SEIP). pp. 121-130. IEEE (2019)\n\nDeep and confident prediction for time series at uber. L Zhu, N Laptev, 2017 IEEE International Conference on. IEEEData Mining WorkshopsZhu, L., Laptev, N.: Deep and confident prediction for time series at uber. In: Data Mining Workshops (ICDMW), 2017 IEEE International Conference on. pp. 103-110. IEEE (2017)\n", "annotations": {"author": "[{\"end\":138,\"start\":31},{\"end\":258,\"start\":139},{\"end\":349,\"start\":259},{\"end\":459,\"start\":350},{\"end\":536,\"start\":460}]", "publisher": null, "author_last_name": "[{\"end\":47,\"start\":37},{\"end\":158,\"start\":146},{\"end\":274,\"start\":269},{\"end\":363,\"start\":356},{\"end\":468,\"start\":465}]", "author_first_name": "[{\"end\":36,\"start\":31},{\"end\":145,\"start\":139},{\"end\":268,\"start\":259},{\"end\":355,\"start\":350},{\"end\":464,\"start\":460}]", "author_affiliation": "[{\"end\":116,\"start\":73},{\"end\":137,\"start\":118},{\"end\":236,\"start\":193},{\"end\":257,\"start\":238},{\"end\":348,\"start\":305},{\"end\":458,\"start\":384},{\"end\":535,\"start\":492}]", "title": "[{\"end\":28,\"start\":1},{\"end\":564,\"start\":537}]", "venue": null, "abstract": "[{\"end\":2361,\"start\":651}]", "bib_ref": "[{\"end\":2816,\"start\":2812},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2822,\"start\":2820},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3101,\"start\":3097},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3773,\"start\":3769},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5129,\"start\":5126},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5131,\"start\":5129},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5133,\"start\":5131},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5136,\"start\":5133},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6185,\"start\":6182},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6188,\"start\":6185},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7610,\"start\":7607},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7613,\"start\":7610},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7982,\"start\":7979},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8148,\"start\":8144},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8331,\"start\":8328},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8526,\"start\":8522},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8627,\"start\":8624},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9025,\"start\":9021},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9028,\"start\":9025},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9031,\"start\":9028},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9089,\"start\":9086},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9274,\"start\":9271},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9277,\"start\":9274},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9394,\"start\":9391},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9627,\"start\":9624},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9630,\"start\":9627},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9809,\"start\":9806},{\"end\":15937,\"start\":15936},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23718,\"start\":23714},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24216,\"start\":24212},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26064,\"start\":26060},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26585,\"start\":26581},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37219,\"start\":37216},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37313,\"start\":37310}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36835,\"start\":36755},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36899,\"start\":36836},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37062,\"start\":36900},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37139,\"start\":37063},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37213,\"start\":37140},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37852,\"start\":37214},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38374,\"start\":37853},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38834,\"start\":38375},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38842,\"start\":38835},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":40309,\"start\":38843},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40485,\"start\":40310}]", "paragraph": "[{\"end\":3431,\"start\":2377},{\"end\":4724,\"start\":3433},{\"end\":6061,\"start\":4726},{\"end\":7331,\"start\":6079},{\"end\":7790,\"start\":7348},{\"end\":8731,\"start\":7792},{\"end\":9032,\"start\":8733},{\"end\":9172,\"start\":9034},{\"end\":9810,\"start\":9174},{\"end\":10251,\"start\":9812},{\"end\":11037,\"start\":10290},{\"end\":11575,\"start\":11039},{\"end\":12022,\"start\":11577},{\"end\":12594,\"start\":12024},{\"end\":13258,\"start\":12817},{\"end\":14126,\"start\":13260},{\"end\":14354,\"start\":14185},{\"end\":14681,\"start\":14356},{\"end\":15624,\"start\":14683},{\"end\":16707,\"start\":15626},{\"end\":17573,\"start\":16709},{\"end\":18400,\"start\":17575},{\"end\":19448,\"start\":18460},{\"end\":19828,\"start\":19514},{\"end\":20661,\"start\":19892},{\"end\":21231,\"start\":20663},{\"end\":22440,\"start\":21233},{\"end\":23376,\"start\":22468},{\"end\":23889,\"start\":23391},{\"end\":24407,\"start\":23902},{\"end\":25785,\"start\":24409},{\"end\":26480,\"start\":25808},{\"end\":27102,\"start\":26482},{\"end\":27920,\"start\":27104},{\"end\":29596,\"start\":27940},{\"end\":31133,\"start\":29598},{\"end\":32099,\"start\":31135},{\"end\":32449,\"start\":32154},{\"end\":32867,\"start\":32451},{\"end\":33492,\"start\":32902},{\"end\":33542,\"start\":33494},{\"end\":34182,\"start\":33544},{\"end\":35068,\"start\":34215},{\"end\":35629,\"start\":35083},{\"end\":36386,\"start\":35631},{\"end\":36754,\"start\":36388}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12816,\"start\":12609},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18459,\"start\":18401},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19513,\"start\":19449},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19891,\"start\":19829}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23972,\"start\":23965},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25549,\"start\":25542},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":34830,\"start\":34823}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2375,\"start\":2363},{\"end\":6077,\"start\":6064},{\"attributes\":{\"n\":\"2\"},\"end\":7346,\"start\":7334},{\"attributes\":{\"n\":\"3\"},\"end\":10272,\"start\":10254},{\"attributes\":{\"n\":\"3.1\"},\"end\":10288,\"start\":10275},{\"end\":12608,\"start\":12597},{\"attributes\":{\"n\":\"3.2\"},\"end\":14183,\"start\":14129},{\"attributes\":{\"n\":\"3.3\"},\"end\":22466,\"start\":22443},{\"attributes\":{\"n\":\"4\"},\"end\":23389,\"start\":23379},{\"attributes\":{\"n\":\"4.1\"},\"end\":23900,\"start\":23892},{\"attributes\":{\"n\":\"4.2\"},\"end\":25806,\"start\":25788},{\"attributes\":{\"n\":\"4.3\"},\"end\":27938,\"start\":27923},{\"attributes\":{\"n\":\"5\"},\"end\":32152,\"start\":32102},{\"attributes\":{\"n\":\"5.1\"},\"end\":32900,\"start\":32870},{\"attributes\":{\"n\":\"5.2\"},\"end\":34213,\"start\":34185},{\"attributes\":{\"n\":\"6\"},\"end\":35081,\"start\":35071},{\"end\":36764,\"start\":36756},{\"end\":36845,\"start\":36837},{\"end\":36904,\"start\":36901},{\"end\":37072,\"start\":37064},{\"end\":37149,\"start\":37141},{\"end\":37863,\"start\":37854},{\"end\":38385,\"start\":38376},{\"end\":38841,\"start\":38836},{\"end\":38853,\"start\":38844},{\"end\":40320,\"start\":40311}]", "table": "[{\"end\":37852,\"start\":37449},{\"end\":38374,\"start\":37910},{\"end\":38834,\"start\":38416},{\"end\":40309,\"start\":39507},{\"end\":40485,\"start\":40372}]", "figure_caption": "[{\"end\":36835,\"start\":36766},{\"end\":36899,\"start\":36847},{\"end\":37062,\"start\":36906},{\"end\":37139,\"start\":37074},{\"end\":37213,\"start\":37151},{\"end\":37449,\"start\":37216},{\"end\":37910,\"start\":37865},{\"end\":38416,\"start\":38387},{\"end\":39507,\"start\":38855},{\"end\":40372,\"start\":40322}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7744,\"start\":7738},{\"end\":14353,\"start\":14347},{\"end\":14991,\"start\":14985},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17087,\"start\":17081},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19689,\"start\":19683},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20213,\"start\":20207},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29402,\"start\":29396},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31322,\"start\":31316},{\"end\":33541,\"start\":33535}]", "bib_author_first_name": "[{\"end\":40488,\"start\":40487},{\"end\":40498,\"start\":40497},{\"end\":40500,\"start\":40499},{\"end\":40509,\"start\":40508},{\"end\":40516,\"start\":40515},{\"end\":40859,\"start\":40858},{\"end\":40865,\"start\":40864},{\"end\":41175,\"start\":41174},{\"end\":41181,\"start\":41180},{\"end\":41183,\"start\":41182},{\"end\":41190,\"start\":41189},{\"end\":41198,\"start\":41197},{\"end\":41525,\"start\":41524},{\"end\":41536,\"start\":41535},{\"end\":41547,\"start\":41546},{\"end\":41553,\"start\":41552},{\"end\":41562,\"start\":41561},{\"end\":41571,\"start\":41570},{\"end\":41783,\"start\":41782},{\"end\":41789,\"start\":41788},{\"end\":41796,\"start\":41795},{\"end\":41802,\"start\":41801},{\"end\":41808,\"start\":41807},{\"end\":41810,\"start\":41809},{\"end\":42206,\"start\":42205},{\"end\":42212,\"start\":42211},{\"end\":42219,\"start\":42218},{\"end\":42228,\"start\":42227},{\"end\":42230,\"start\":42229},{\"end\":42561,\"start\":42560},{\"end\":42563,\"start\":42562},{\"end\":42572,\"start\":42571},{\"end\":42574,\"start\":42573},{\"end\":42584,\"start\":42583},{\"end\":42594,\"start\":42593},{\"end\":42970,\"start\":42969},{\"end\":42983,\"start\":42982},{\"end\":42997,\"start\":42996},{\"end\":43010,\"start\":43009},{\"end\":43020,\"start\":43019},{\"end\":43415,\"start\":43414},{\"end\":43708,\"start\":43707},{\"end\":43720,\"start\":43719},{\"end\":43722,\"start\":43721},{\"end\":44097,\"start\":44096},{\"end\":44106,\"start\":44105},{\"end\":44116,\"start\":44115},{\"end\":44126,\"start\":44125},{\"end\":44128,\"start\":44127},{\"end\":44140,\"start\":44139},{\"end\":44693,\"start\":44692},{\"end\":44707,\"start\":44706},{\"end\":44718,\"start\":44717},{\"end\":45203,\"start\":45202},{\"end\":45217,\"start\":45216},{\"end\":45228,\"start\":45227},{\"end\":45581,\"start\":45580},{\"end\":45867,\"start\":45866},{\"end\":45875,\"start\":45874},{\"end\":45881,\"start\":45880},{\"end\":45883,\"start\":45882},{\"end\":46325,\"start\":46324},{\"end\":46327,\"start\":46326},{\"end\":46502,\"start\":46501},{\"end\":46513,\"start\":46512},{\"end\":46524,\"start\":46523},{\"end\":46534,\"start\":46533},{\"end\":46547,\"start\":46546},{\"end\":46556,\"start\":46555},{\"end\":46558,\"start\":46557},{\"end\":46567,\"start\":46566},{\"end\":46577,\"start\":46576},{\"end\":46913,\"start\":46912},{\"end\":46919,\"start\":46918},{\"end\":46928,\"start\":46927},{\"end\":46935,\"start\":46934},{\"end\":46948,\"start\":46947},{\"end\":46950,\"start\":46949},{\"end\":47378,\"start\":47377},{\"end\":47385,\"start\":47384},{\"end\":47391,\"start\":47390},{\"end\":47398,\"start\":47397},{\"end\":47404,\"start\":47403},{\"end\":47411,\"start\":47410},{\"end\":47420,\"start\":47419},{\"end\":47422,\"start\":47421},{\"end\":47863,\"start\":47862},{\"end\":47870,\"start\":47869}]", "bib_author_last_name": "[{\"end\":40495,\"start\":40489},{\"end\":40506,\"start\":40501},{\"end\":40513,\"start\":40510},{\"end\":40526,\"start\":40517},{\"end\":40862,\"start\":40860},{\"end\":40868,\"start\":40866},{\"end\":41178,\"start\":41176},{\"end\":41187,\"start\":41184},{\"end\":41195,\"start\":41191},{\"end\":41201,\"start\":41199},{\"end\":41533,\"start\":41526},{\"end\":41544,\"start\":41537},{\"end\":41550,\"start\":41548},{\"end\":41559,\"start\":41554},{\"end\":41568,\"start\":41563},{\"end\":41577,\"start\":41572},{\"end\":41786,\"start\":41784},{\"end\":41793,\"start\":41790},{\"end\":41799,\"start\":41797},{\"end\":41805,\"start\":41803},{\"end\":41814,\"start\":41811},{\"end\":42209,\"start\":42207},{\"end\":42216,\"start\":42213},{\"end\":42225,\"start\":42220},{\"end\":42234,\"start\":42231},{\"end\":42569,\"start\":42564},{\"end\":42581,\"start\":42575},{\"end\":42591,\"start\":42585},{\"end\":42600,\"start\":42595},{\"end\":42980,\"start\":42971},{\"end\":42994,\"start\":42984},{\"end\":43007,\"start\":42998},{\"end\":43017,\"start\":43011},{\"end\":43031,\"start\":43021},{\"end\":43424,\"start\":43416},{\"end\":43717,\"start\":43709},{\"end\":43727,\"start\":43723},{\"end\":44103,\"start\":44098},{\"end\":44113,\"start\":44107},{\"end\":44123,\"start\":44117},{\"end\":44137,\"start\":44129},{\"end\":44153,\"start\":44141},{\"end\":44704,\"start\":44694},{\"end\":44715,\"start\":44708},{\"end\":44722,\"start\":44719},{\"end\":45214,\"start\":45204},{\"end\":45225,\"start\":45218},{\"end\":45232,\"start\":45229},{\"end\":45587,\"start\":45582},{\"end\":45872,\"start\":45868},{\"end\":45878,\"start\":45876},{\"end\":45889,\"start\":45884},{\"end\":46334,\"start\":46328},{\"end\":46510,\"start\":46503},{\"end\":46521,\"start\":46514},{\"end\":46531,\"start\":46525},{\"end\":46544,\"start\":46535},{\"end\":46553,\"start\":46548},{\"end\":46564,\"start\":46559},{\"end\":46574,\"start\":46568},{\"end\":46588,\"start\":46578},{\"end\":46916,\"start\":46914},{\"end\":46925,\"start\":46920},{\"end\":46932,\"start\":46929},{\"end\":46945,\"start\":46936},{\"end\":46957,\"start\":46951},{\"end\":47382,\"start\":47379},{\"end\":47388,\"start\":47386},{\"end\":47395,\"start\":47392},{\"end\":47401,\"start\":47399},{\"end\":47408,\"start\":47405},{\"end\":47417,\"start\":47412},{\"end\":47426,\"start\":47423},{\"end\":47867,\"start\":47864},{\"end\":47877,\"start\":47871}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b0\"},\"end\":40809,\"start\":40487},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206784678},\"end\":41086,\"start\":40811},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16707374},\"end\":41469,\"start\":41088},{\"attributes\":{\"id\":\"b3\"},\"end\":41718,\"start\":41471},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9995103},\"end\":42142,\"start\":41720},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4776668},\"end\":42484,\"start\":42144},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18837869},\"end\":42891,\"start\":42486},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":50768744},\"end\":43371,\"start\":42893},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16655537},\"end\":43631,\"start\":43373},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16171853},\"end\":44015,\"start\":43633},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13699884},\"end\":44608,\"start\":44017},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":195832838},\"end\":45055,\"start\":44610},{\"attributes\":{\"doi\":\"10.1109/CCGRID.2019.00038\",\"id\":\"b12\"},\"end\":45125,\"start\":45057},{\"attributes\":{\"doi\":\"10.1109/CLOUD.2019.00038\",\"id\":\"b13\",\"matched_paper_id\":201813568},\"end\":45578,\"start\":45127},{\"attributes\":{\"doi\":\"arXiv:1611.03213\",\"id\":\"b14\"},\"end\":45808,\"start\":45580},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207191250},\"end\":46267,\"start\":45810},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206666846},\"end\":46472,\"start\":46269},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13756489},\"end\":46848,\"start\":46474},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2239051},\"end\":47327,\"start\":46850},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53211579},\"end\":47805,\"start\":47329},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":35416817},\"end\":48117,\"start\":47807}]", "bib_title": "[{\"end\":40856,\"start\":40811},{\"end\":41172,\"start\":41088},{\"end\":41780,\"start\":41720},{\"end\":42203,\"start\":42144},{\"end\":42558,\"start\":42486},{\"end\":42967,\"start\":42893},{\"end\":43412,\"start\":43373},{\"end\":43705,\"start\":43633},{\"end\":44094,\"start\":44017},{\"end\":44690,\"start\":44610},{\"end\":45200,\"start\":45127},{\"end\":45864,\"start\":45810},{\"end\":46322,\"start\":46269},{\"end\":46499,\"start\":46474},{\"end\":46910,\"start\":46850},{\"end\":47375,\"start\":47329},{\"end\":47860,\"start\":47807}]", "bib_author": "[{\"end\":40497,\"start\":40487},{\"end\":40508,\"start\":40497},{\"end\":40515,\"start\":40508},{\"end\":40528,\"start\":40515},{\"end\":40864,\"start\":40858},{\"end\":40870,\"start\":40864},{\"end\":41180,\"start\":41174},{\"end\":41189,\"start\":41180},{\"end\":41197,\"start\":41189},{\"end\":41203,\"start\":41197},{\"end\":41535,\"start\":41524},{\"end\":41546,\"start\":41535},{\"end\":41552,\"start\":41546},{\"end\":41561,\"start\":41552},{\"end\":41570,\"start\":41561},{\"end\":41579,\"start\":41570},{\"end\":41788,\"start\":41782},{\"end\":41795,\"start\":41788},{\"end\":41801,\"start\":41795},{\"end\":41807,\"start\":41801},{\"end\":41816,\"start\":41807},{\"end\":42211,\"start\":42205},{\"end\":42218,\"start\":42211},{\"end\":42227,\"start\":42218},{\"end\":42236,\"start\":42227},{\"end\":42571,\"start\":42560},{\"end\":42583,\"start\":42571},{\"end\":42593,\"start\":42583},{\"end\":42602,\"start\":42593},{\"end\":42982,\"start\":42969},{\"end\":42996,\"start\":42982},{\"end\":43009,\"start\":42996},{\"end\":43019,\"start\":43009},{\"end\":43033,\"start\":43019},{\"end\":43426,\"start\":43414},{\"end\":43719,\"start\":43707},{\"end\":43729,\"start\":43719},{\"end\":44105,\"start\":44096},{\"end\":44115,\"start\":44105},{\"end\":44125,\"start\":44115},{\"end\":44139,\"start\":44125},{\"end\":44155,\"start\":44139},{\"end\":44706,\"start\":44692},{\"end\":44717,\"start\":44706},{\"end\":44724,\"start\":44717},{\"end\":45216,\"start\":45202},{\"end\":45227,\"start\":45216},{\"end\":45234,\"start\":45227},{\"end\":45589,\"start\":45580},{\"end\":45874,\"start\":45866},{\"end\":45880,\"start\":45874},{\"end\":45891,\"start\":45880},{\"end\":46336,\"start\":46324},{\"end\":46512,\"start\":46501},{\"end\":46523,\"start\":46512},{\"end\":46533,\"start\":46523},{\"end\":46546,\"start\":46533},{\"end\":46555,\"start\":46546},{\"end\":46566,\"start\":46555},{\"end\":46576,\"start\":46566},{\"end\":46590,\"start\":46576},{\"end\":46918,\"start\":46912},{\"end\":46927,\"start\":46918},{\"end\":46934,\"start\":46927},{\"end\":46947,\"start\":46934},{\"end\":46959,\"start\":46947},{\"end\":47384,\"start\":47377},{\"end\":47390,\"start\":47384},{\"end\":47397,\"start\":47390},{\"end\":47403,\"start\":47397},{\"end\":47410,\"start\":47403},{\"end\":47419,\"start\":47410},{\"end\":47428,\"start\":47419},{\"end\":47869,\"start\":47862},{\"end\":47879,\"start\":47869}]", "bib_venue": "[{\"end\":43138,\"start\":43094},{\"end\":44338,\"start\":44255},{\"end\":46062,\"start\":45985},{\"end\":47098,\"start\":47037},{\"end\":40624,\"start\":40544},{\"end\":40907,\"start\":40870},{\"end\":41253,\"start\":41203},{\"end\":41522,\"start\":41471},{\"end\":41908,\"start\":41816},{\"end\":42293,\"start\":42236},{\"end\":42670,\"start\":42602},{\"end\":43092,\"start\":43033},{\"end\":43482,\"start\":43426},{\"end\":43804,\"start\":43729},{\"end\":44253,\"start\":44155},{\"end\":44812,\"start\":44724},{\"end\":45324,\"start\":45258},{\"end\":45673,\"start\":45605},{\"end\":45983,\"start\":45891},{\"end\":46356,\"start\":46336},{\"end\":46639,\"start\":46590},{\"end\":47035,\"start\":46959},{\"end\":47541,\"start\":47428},{\"end\":47916,\"start\":47879}]"}}}, "year": 2023, "month": 12, "day": 17}