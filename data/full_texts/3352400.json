{"id": 3352400, "updated": "2023-09-28 18:01:22.34", "metadata": {"title": "Wide&Deep Learning for Recommender Systems", "authors": "[{\"first\":\"Heng-Tze\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Levent\",\"last\":\"Koc\",\"middle\":[]},{\"first\":\"Jeremiah\",\"last\":\"Harmsen\",\"middle\":[]},{\"first\":\"Tal\",\"last\":\"Shaked\",\"middle\":[]},{\"first\":\"Tushar\",\"last\":\"Chandra\",\"middle\":[]},{\"first\":\"Hrishi\",\"last\":\"Aradhye\",\"middle\":[]},{\"first\":\"Glen\",\"last\":\"Anderson\",\"middle\":[]},{\"first\":\"Greg\",\"last\":\"Corrado\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Chai\",\"middle\":[]},{\"first\":\"Mustafa\",\"last\":\"Ispir\",\"middle\":[]},{\"first\":\"Rohan\",\"last\":\"Anil\",\"middle\":[]},{\"first\":\"Zakaria\",\"last\":\"Haque\",\"middle\":[]},{\"first\":\"Lichan\",\"last\":\"Hong\",\"middle\":[]},{\"first\":\"Vihan\",\"last\":\"Jain\",\"middle\":[]},{\"first\":\"Xiaobing\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Hemal\",\"last\":\"Shah\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 1st Workshop on Deep Learning for Recommender Systems", "publication_date": {"year": 2016, "month": 6, "day": 24}, "abstract": "Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide&Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide&Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1606.07792", "mag": "2951581544", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/recsys/Cheng0HSCAACCIA16", "doi": "10.1145/2988450.2988454"}}, "content": {"source": {"pdf_hash": "aa9d39e938c84a867ddf2a8cabc575ffba27b721", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1606.07792v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://dl.acm.org/ft_gateway.cfm?id=2988454&type=pdf", "status": "BRONZE"}}, "grobid": {"id": "13a180824131306e3f8441c7f9f7323a77320bc9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/aa9d39e938c84a867ddf2a8cabc575ffba27b721.txt", "contents": "\nWide & Deep Learning for Recommender Systems\n\n\nHeng-Tze Cheng \nLevent Koc \nJeremiah Harmsen \nTal Shaked \nTushar Chandra \nHrishi Aradhye \nGlen Anderson \nGreg Corrado \nWei Chai \nMustafa Ispir \nRohan Anil \nZakaria Haque \nLichan Hong \nVihan Jain \nHemalXiaobing Liu \nShah Google \nInc \nWide & Deep Learning for Recommender Systems\nCCS Concepts \u2022Computing methodologies \u2192 Machine learningNeu- ral networksSupervised learning\u2022Information systems \u2192 Recommender systemsKeywords Wide & Deep Learning, Recommender Systems\nGeneralized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning-jointly trained wide linear models and deep neural networks-to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.\n\nINTRODUCTION\n\nA recommender system can be viewed as a search ranking system, where the input query is a set of user and contextual information, and the output is a ranked list of items. Given a query, the recommendation task is to find the relevant items in a database and then rank the items based on certain objectives, such as clicks or purchases.\n\nOne challenge in recommender systems, similar to the general search ranking problem, is to achieve both memorization and generalization. Memorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data. Generalization, on the other hand, is based on transitivity of correlation and explores new feature combinations that * Corresponding author: hengtze@google.com have never or rarely occurred in the past. Recommendations based on memorization are usually more topical and directly relevant to the items on which users have already performed actions. Compared with memorization, generalization tends to improve the diversity of the recommended items. In this paper, we focus on the apps recommendation problem for the Google Play store, but the approach should apply to generic recommender systems. For massive-scale online recommendation and ranking systems in an industrial setting, generalized linear models such as logistic regression are widely used because they are simple, scalable and interpretable. The models are often trained on binarized sparse features with one-hot encoding. E.g., the binary feature \"user_installed_app=netflix\" has value 1 if the user installed Netflix. Memorization can be achieved effectively using cross-product transformations over sparse features, such as AND(user_installed_app=netflix, impres-sion_app=pandora\"), whose value is 1 if the user installed Netflix and then is later shown Pandora. This explains how the co-occurrence of a feature pair correlates with the target label. Generalization can be added by using features that are less granular, such as AND(user_installed_category=video, impression_category=music), but manual feature engineering is often required. One limitation of cross-product transformations is that they do not generalize to query-item feature pairs that have not appeared in the training data.\n\nEmbedding-based models, such as factorization machines [5] or deep neural networks, can generalize to previously unseen query-item feature pairs by learning a low-dimensional dense embedding vector for each query and item feature, with less burden of feature engineering. However, it is difficult to learn effective low-dimensional representations for queries and items when the underlying query-item matrix is sparse and high-rank, such as users with specific preferences or niche items with a narrow appeal. In such cases, there should be no interactions between most query-item pairs, but dense embeddings will lead to nonzero predictions for all query-item pairs, and thus can over-generalize and make less relevant recommendations. On the other hand, linear models with cross-product feature transformations can memorize these \"exception rules\" with much fewer parameters.\n\nIn this paper, we present the Wide & Deep learning framework to achieve both memorization and generalization in one model, by jointly training a linear model component and a neural network component as shown in Figure 1.\n\nThe main contributions of the paper include:\n\n\u2022  linear model with feature transformations for generic recommender systems with sparse inputs.\n\n\u2022 The implementation and evaluation of the Wide & Deep recommender system productionized on Google Play, a mobile app store with over one billion active users and over one million apps.\n\n\u2022 We have open-sourced our implementation along with a high-level API in TensorFlow 1 .\n\nWhile the idea is simple, we show that the Wide & Deep framework significantly improves the app acquisition rate on the mobile app store, while satisfying the training and serving speed requirements.\n\n\nRECOMMENDER SYSTEM OVERVIEW\n\nAn overview of the app recommender system is shown in Figure 2. A query, which can include various user and contextual features, is generated when a user visits the app store. The recommender system returns a list of apps (also referred to as impressions) on which users can perform certain actions such as clicks or purchases. These user actions, along with the queries and impressions, are recorded in the logs as the training data for the learner.\n\nSince there are over a million apps in the database, it is intractable to exhaustively score every app for every query within the serving latency requirements (often O(10) milliseconds). Therefore, the first step upon receiving a query is retrieval. The retrieval system returns a short list of items that best match the query using various signals, usually a combination of machine-learned models and human-defined rules. After reducing the candidate pool, the ranking system ranks all items by their scores. The scores are usually P (y|x), the probability of a user action label y given the features x, including user features (e.g., country, language, demographics), contextual features (e.g., device, hour of the day, day of the week), and impression features (e.g., app age, historical statistics of an app). In this paper, we focus on the ranking model using the Wide & Deep learning framework.\n\n\nWIDE & DEEP LEARNING\n\n\nThe Wide Component\n\nThe wide component is a generalized linear model of the form y = w T x + b, as illustrated in Figure 1    features. One of the most important transformations is the cross-product transformation, which is defined as:\n\u03c6 k (x) = d i=1 x c ki i c ki \u2208 {0, 1}(1)\nwhere c ki is a boolean variable that is 1 if the i-th feature is part of the k-th transformation \u03c6 k , and 0 otherwise. For binary features, a cross-product transformation (e.g., \"AND(gender=female, language=en)\") is 1 if and only if the constituent features (\"gender=female\" and \"language=en\") are all 1, and 0 otherwise. This captures the interactions between the binary features, and adds nonlinearity to the generalized linear model.\n\n\nThe Deep Component\n\nThe deep component is a feed-forward neural network, as shown in Figure 1 (right). For categorical features, the original inputs are feature strings (e.g., \"language=en\"). Each of these sparse, high-dimensional categorical features are first converted into a low-dimensional and dense real-valued vector, often referred to as an embedding vector. The dimensionality of the embeddings are usually on the order of O(10) to O(100). The embedding vectors are initialized randomly and then the values are trained to minimize the final loss function during model training. These low-dimensional dense embedding vectors are then fed into the hidden layers of a neural network in the forward pass. Specifically, each hidden layer performs the following computation:\na (l+1) = f (W (l) a (l) + b (l) )(2)\nwhere l is the layer number and f is the activation function, often rectified linear units (ReLUs). a (l) , b (l) , and W (l) are the activations, bias, and model weights at l-th layer.\n\n\nJoint Training of Wide & Deep Model\n\nThe wide component and deep component are combined using a weighted sum of their output log odds as the pre-  diction, which is then fed to one common logistic loss function for joint training. Note that there is a distinction between joint training and ensemble. In an ensemble, individual models are trained separately without knowing each other, and their predictions are combined only at inference time but not at training time. In contrast, joint training optimizes all parameters simultaneously by taking both the wide and deep part as well as the weights of their sum into account at training time. There are implications on model size too: For an ensemble, since the training is disjoint, each individual model size usually needs to be larger (e.g., with more features and transformations) to achieve reasonable accuracy for an ensemble to work. In comparison, for joint training the wide part only needs to complement the weaknesses of the deep part with a small number of cross-product feature transformations, rather than a full-size wide model. Joint training of a Wide & Deep Model is done by backpropagating the gradients from the output to both the wide and deep part of the model simultaneously using mini-batch stochastic optimization. In the experiments, we used Followthe-regularized-leader (FTRL) algorithm [3] with L1 regularization as the optimizer for the wide part of the model, and AdaGrad [1] for the deep part.\n\nThe combined model is illustrated in Figure 1 (center). For a logistic regression problem, the model's prediction is:\nP (Y = 1|x) = \u03c3(w T wide [x, \u03c6(x)] + w T deep a (l f ) + b)(3)\nwhere Y is the binary class label, \u03c3(\u00b7) is the sigmoid function, \u03c6(x) are the cross product transformations of the original features x, and b is the bias term. w wide is the vector of all wide model weights, and w deep are the weights applied on the final activations a (l f ) .\n\n\nSYSTEM IMPLEMENTATION\n\nThe implementation of the apps recommendation pipeline consists of three stages: data generation, model training, and model serving as shown in Figure 3.\n\n\nData Generation\n\nIn this stage, user and app impression data within a period of time are used to generate training data. Each example corresponds to one impression. The label is app acquisition: 1 if the impressed app was installed, and 0 otherwise.\n\nVocabularies, which are tables mapping categorical feature strings to integer IDs, are also generated in this stage. The system computes the ID space for all the string features that occurred more than a minimum number of times. Continuous real-valued features are normalized to [0, 1] by mapping a feature value x to its cumulative distribution function P (X \u2264 x), divided into nq quantiles. The normalized value is i\u22121 nq \u22121 for values in the i-th quantiles. Quantile boundaries  are computed during data generation.\n\n\nModel Training\n\nThe model structure we used in the experiment is shown in Figure 4. During training, our input layer takes in training data and vocabularies and generate sparse and dense features together with a label. The wide component consists of the cross-product transformation of user installed apps and impression apps. For the deep part of the model, A 32dimensional embedding vector is learned for each categorical feature. We concatenate all the embeddings together with the dense features, resulting in a dense vector of approximately 1200 dimensions. The concatenated vector is then fed into 3 ReLU layers, and finally the logistic output unit.\n\nThe Wide & Deep models are trained on over 500 billion examples. Every time a new set of training data arrives, the model needs to be re-trained. However, retraining from scratch every time is computationally expensive and delays the time from data arrival to serving an updated model. To tackle this challenge, we implemented a warm-starting system which initializes a new model with the embeddings and the linear model weights from the previous model.\n\nBefore loading the models into the model servers, a dry run of the model is done to make sure that it does not cause problems in serving live traffic. We empirically validate the model quality against the previous model as a sanity check.\n\n\nModel Serving\n\nOnce the model is trained and verified, we load it into the model servers. For each request, the servers receive a set of app candidates from the app retrieval system and user features to score each app. Then, the apps are ranked from the highest scores to the lowest, and we show the apps to the users in this order. The scores are calculated by running a forward inference pass over the Wide & Deep model.\n\nIn order to serve each request on the order of 10 ms, we optimized the performance using multithreading parallelism by running smaller batches in parallel, instead of scoring all candidate apps in a single batch inference step.\n\n\nEXPERIMENT RESULTS\n\nTo evaluate the effectiveness of Wide & Deep learning in a real-world recommender system, we ran live experiments and evaluated the system in a couple of aspects: app acquisitions and serving performance.\n\n\nApp Acquisitions\n\nWe conducted live online experiments in an A/B testing framework for 3 weeks. For the control group, 1% of Besides online experiments, we also show the Area Under Receiver Operator Characteristic Curve (AUC) on a holdout set offline. While Wide & Deep has a slightly higher offline AUC, the impact is more significant on online traffic. One possible reason is that the impressions and labels in offline data sets are fixed, whereas the online system can generate new exploratory recommendations by blending generalization with memorization, and learn from new user responses.\n\n\nServing Performance\n\nServing with high throughput and low latency is challenging with the high level of traffic faced by our commercial mobile app store. At peak traffic, our recommender servers score over 10 million apps per second. With single threading, scoring all candidates in a single batch takes 31 ms. We implemented multithreading and split each batch into smaller sizes, which significantly reduced the client-side latency to 14 ms (including serving overhead) as shown in Table 2.\n\n\nRELATED WORK\n\nThe idea of combining wide linear models with crossproduct feature transformations and deep neural networks with dense embeddings is inspired by previous work, such as factorization machines [5] which add generalization to linear models by factorizing the interactions between two variables as a dot product between two low-dimensional embedding vectors. In this paper, we expanded the model capacity by learning highly nonlinear interactions between embeddings via neural networks instead of dot products.\n\nIn language models, joint training of recurrent neural networks (RNNs) and maximum entropy models with n-gram features has been proposed to significantly reduce the RNN complexity (e.g., hidden layer sizes) by learning direct weights between inputs and outputs [4]. In computer vision, deep residual learning [2] has been used to reduce the difficulty of training deeper models and improve accuracy with shortcut connections which skip one or more layers. Joint training of neural networks with graphical models has also been applied to human pose estimation from images [6]. In this work we explored the joint training of feed-forward neural networks and linear models, with direct connections between sparse features and the output unit, for generic recommendation and ranking problems with sparse input data.\n\nIn the recommender systems literature, collaborative deep learning has been explored by coupling deep learning for content information and collaborative filtering (CF) for the ratings matrix [7]. There has also been previous work on mobile app recommender systems, such as AppJoy which used CF on users' app usage records [8]. Different from the CF-based or content-based approaches in the previous work, we jointly train Wide & Deep models on user and impression data for app recommender systems.\n\n\nCONCLUSION\n\nMemorization and generalization are both important for recommender systems. Wide linear models can effectively memorize sparse feature interactions using cross-product feature transformations, while deep neural networks can generalize to previously unseen feature interactions through lowdimensional embeddings. We presented the Wide & Deep learning framework to combine the strengths of both types of model. We productionized and evaluated the framework on the recommender system of Google Play, a massive-scale commercial app store. Online experiment results showed that the Wide & Deep model led to significant improvement on app acquisitions over wide-only and deep-only models.\n\nFigure 1 :\n1The spectrum of Wide & Deep models.\n\n\n(left). y is the prediction, x = [x1, x2, ..., x d ] is a vector of d features, w = [w1, w2, ..., w d ] are the model parameters and b is the bias. The feature set includes raw input features and transformed 1 See Wide & Deep Tutorial on http://tensorflow.org.\n\nFigure 2 :\n2Overview of the recommender system.\n\nFigure 3 :\n3Apps recommendation pipeline overview.\n\nFigure 4 :\n4Wide & Deep model structure for apps recommendation.\n\nTable 1 :\n1Offline & online metrics of different models. Online Acquisition Gain is relative to the control.Model \nOffline AUC Online Acquisition Gain \n\nWide (control) \n0.726 \n0% \n\nDeep \n0.722 \n+2.9% \n\nWide & Deep \n0.728 \n+3.9% \n\nusers were randomly selected and presented with recom-\nmendations generated by the previous version of ranking \nmodel, which is a highly-optimized wide-only logistic regres-\nsion model with rich cross-product feature transformations. \nFor the experiment group, 1% of users were presented with \nrecommendations generated by the Wide & Deep model, \ntrained with the same set of features. As shown in Table 1, \nWide & Deep model improved the app acquisition rate on \nthe main landing page of the app store by +3.9% relative to \nthe control group (statistically significant). The results were \nalso compared with another 1% group using only the deep \npart of the model with the same features and neural network \nstructure, and the Wide & Deep mode had +1% gain on top \nof the deep-only model (statistically significant). \n\n\nTable 2 :\n2Serving latency vs. batch size and threads. Batch size Number of Threads Serving Latency (ms)200 \n1 \n31 \n\n100 \n2 \n17 \n\n50 \n4 \n14 \n\n\n\nAdaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer, Journal of Machine Learning Research. 12J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, July 2011.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2016.\n\nFollow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization. H B Mcmahan, Proc. AISTATS. AISTATSH. B. McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization. In Proc. AISTATS, 2011.\n\nStrategies for training large scale neural network language models. T Mikolov, A Deoras, D Povey, L Burget, J H Cernocky, IEEE Automatic Speech Recognition & Understanding Workshop. T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. H. Cernocky. Strategies for training large scale neural network language models. In IEEE Automatic Speech Recognition & Understanding Workshop, 2011.\n\nFactorization machines with libFM. S Rendle, ACM Trans. Intell. Syst. Technol. 3322S. Rendle. Factorization machines with libFM. ACM Trans. Intell. Syst. Technol., 3(3):57:1-57:22, May 2012.\n\nJoint training of a convolutional network and a graphical model for human pose estimation. J J Tompson, A Jain, Y Lecun, C Bregler, NIPS. Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. WeinbergerJ. J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint training of a convolutional network and a graphical model for human pose estimation. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, NIPS, pages 1799-1807. 2014.\n\nCollaborative deep learning for recommender systems. H Wang, N Wang, D.-Y Yeung, Proc. KDD. KDDH. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep learning for recommender systems. In Proc. KDD, pages 1235-1244, 2015.\n\nAppJoy: Personalized mobile application discovery. B Yan, G Chen, MobiSys. B. Yan and G. Chen. AppJoy: Personalized mobile application discovery. In MobiSys, pages 113-126, 2011.\n", "annotations": {"author": "[{\"end\":63,\"start\":48},{\"end\":75,\"start\":64},{\"end\":93,\"start\":76},{\"end\":105,\"start\":94},{\"end\":121,\"start\":106},{\"end\":137,\"start\":122},{\"end\":152,\"start\":138},{\"end\":166,\"start\":153},{\"end\":176,\"start\":167},{\"end\":191,\"start\":177},{\"end\":203,\"start\":192},{\"end\":218,\"start\":204},{\"end\":231,\"start\":219},{\"end\":243,\"start\":232},{\"end\":262,\"start\":244},{\"end\":275,\"start\":263},{\"end\":280,\"start\":276}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":57},{\"end\":74,\"start\":71},{\"end\":92,\"start\":85},{\"end\":104,\"start\":98},{\"end\":120,\"start\":113},{\"end\":136,\"start\":129},{\"end\":151,\"start\":143},{\"end\":165,\"start\":158},{\"end\":175,\"start\":171},{\"end\":190,\"start\":185},{\"end\":202,\"start\":198},{\"end\":217,\"start\":212},{\"end\":230,\"start\":226},{\"end\":242,\"start\":238},{\"end\":261,\"start\":258},{\"end\":274,\"start\":268}]", "author_first_name": "[{\"end\":56,\"start\":48},{\"end\":70,\"start\":64},{\"end\":84,\"start\":76},{\"end\":97,\"start\":94},{\"end\":112,\"start\":106},{\"end\":128,\"start\":122},{\"end\":142,\"start\":138},{\"end\":157,\"start\":153},{\"end\":170,\"start\":167},{\"end\":184,\"start\":177},{\"end\":197,\"start\":192},{\"end\":211,\"start\":204},{\"end\":225,\"start\":219},{\"end\":237,\"start\":232},{\"end\":257,\"start\":249},{\"end\":267,\"start\":263},{\"end\":279,\"start\":276}]", "author_affiliation": null, "title": "[{\"end\":45,\"start\":1},{\"end\":325,\"start\":281}]", "venue": null, "abstract": "[{\"end\":1733,\"start\":511}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4105,\"start\":4102},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10267,\"start\":10264},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10355,\"start\":10352},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15323,\"start\":15320},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15901,\"start\":15898},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15949,\"start\":15946},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16211,\"start\":16208},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16644,\"start\":16641},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16775,\"start\":16772}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17693,\"start\":17645},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17956,\"start\":17694},{\"attributes\":{\"id\":\"fig_3\"},\"end\":18005,\"start\":17957},{\"attributes\":{\"id\":\"fig_4\"},\"end\":18057,\"start\":18006},{\"attributes\":{\"id\":\"fig_5\"},\"end\":18123,\"start\":18058},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":19173,\"start\":18124},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":19317,\"start\":19174}]", "paragraph": "[{\"end\":2085,\"start\":1749},{\"end\":4045,\"start\":2087},{\"end\":4924,\"start\":4047},{\"end\":5146,\"start\":4926},{\"end\":5192,\"start\":5148},{\"end\":5290,\"start\":5194},{\"end\":5477,\"start\":5292},{\"end\":5566,\"start\":5479},{\"end\":5767,\"start\":5568},{\"end\":6249,\"start\":5799},{\"end\":7151,\"start\":6251},{\"end\":7412,\"start\":7197},{\"end\":7893,\"start\":7455},{\"end\":8673,\"start\":7916},{\"end\":8897,\"start\":8712},{\"end\":10374,\"start\":8937},{\"end\":10493,\"start\":10376},{\"end\":10835,\"start\":10557},{\"end\":11014,\"start\":10861},{\"end\":11266,\"start\":11034},{\"end\":11786,\"start\":11268},{\"end\":12445,\"start\":11805},{\"end\":12900,\"start\":12447},{\"end\":13140,\"start\":12902},{\"end\":13565,\"start\":13158},{\"end\":13794,\"start\":13567},{\"end\":14021,\"start\":13817},{\"end\":14617,\"start\":14042},{\"end\":15112,\"start\":14641},{\"end\":15635,\"start\":15129},{\"end\":16448,\"start\":15637},{\"end\":16947,\"start\":16450},{\"end\":17644,\"start\":16962}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7454,\"start\":7413},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8711,\"start\":8674},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10556,\"start\":10494}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15111,\"start\":15104}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1747,\"start\":1735},{\"attributes\":{\"n\":\"2.\"},\"end\":5797,\"start\":5770},{\"attributes\":{\"n\":\"3.\"},\"end\":7174,\"start\":7154},{\"attributes\":{\"n\":\"3.1\"},\"end\":7195,\"start\":7177},{\"attributes\":{\"n\":\"3.2\"},\"end\":7914,\"start\":7896},{\"attributes\":{\"n\":\"3.3\"},\"end\":8935,\"start\":8900},{\"attributes\":{\"n\":\"4.\"},\"end\":10859,\"start\":10838},{\"attributes\":{\"n\":\"4.1\"},\"end\":11032,\"start\":11017},{\"attributes\":{\"n\":\"4.2\"},\"end\":11803,\"start\":11789},{\"attributes\":{\"n\":\"4.3\"},\"end\":13156,\"start\":13143},{\"attributes\":{\"n\":\"5.\"},\"end\":13815,\"start\":13797},{\"attributes\":{\"n\":\"5.1\"},\"end\":14040,\"start\":14024},{\"attributes\":{\"n\":\"5.2\"},\"end\":14639,\"start\":14620},{\"attributes\":{\"n\":\"6.\"},\"end\":15127,\"start\":15115},{\"attributes\":{\"n\":\"7.\"},\"end\":16960,\"start\":16950},{\"end\":17656,\"start\":17646},{\"end\":17968,\"start\":17958},{\"end\":18017,\"start\":18007},{\"end\":18069,\"start\":18059},{\"end\":18134,\"start\":18125},{\"end\":19184,\"start\":19175}]", "table": "[{\"end\":19173,\"start\":18233},{\"end\":19317,\"start\":19279}]", "figure_caption": "[{\"end\":17693,\"start\":17658},{\"end\":17956,\"start\":17696},{\"end\":18005,\"start\":17970},{\"end\":18057,\"start\":18019},{\"end\":18123,\"start\":18071},{\"end\":18233,\"start\":18136},{\"end\":19279,\"start\":19186}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5145,\"start\":5137},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":5861,\"start\":5853},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7299,\"start\":7291},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7989,\"start\":7981},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10421,\"start\":10413},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11013,\"start\":11005},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":11871,\"start\":11863}]", "bib_author_first_name": "[{\"end\":19398,\"start\":19397},{\"end\":19407,\"start\":19406},{\"end\":19416,\"start\":19415},{\"end\":19689,\"start\":19688},{\"end\":19695,\"start\":19694},{\"end\":19704,\"start\":19703},{\"end\":19711,\"start\":19710},{\"end\":20092,\"start\":20091},{\"end\":20094,\"start\":20093},{\"end\":20329,\"start\":20328},{\"end\":20340,\"start\":20339},{\"end\":20350,\"start\":20349},{\"end\":20359,\"start\":20358},{\"end\":20369,\"start\":20368},{\"end\":20371,\"start\":20370},{\"end\":20680,\"start\":20679},{\"end\":20928,\"start\":20927},{\"end\":20930,\"start\":20929},{\"end\":20941,\"start\":20940},{\"end\":20949,\"start\":20948},{\"end\":20958,\"start\":20957},{\"end\":21361,\"start\":21360},{\"end\":21369,\"start\":21368},{\"end\":21380,\"start\":21376},{\"end\":21580,\"start\":21579},{\"end\":21587,\"start\":21586}]", "bib_author_last_name": "[{\"end\":19404,\"start\":19399},{\"end\":19413,\"start\":19408},{\"end\":19423,\"start\":19417},{\"end\":19692,\"start\":19690},{\"end\":19701,\"start\":19696},{\"end\":19708,\"start\":19705},{\"end\":19715,\"start\":19712},{\"end\":20102,\"start\":20095},{\"end\":20337,\"start\":20330},{\"end\":20347,\"start\":20341},{\"end\":20356,\"start\":20351},{\"end\":20366,\"start\":20360},{\"end\":20380,\"start\":20372},{\"end\":20687,\"start\":20681},{\"end\":20938,\"start\":20931},{\"end\":20946,\"start\":20942},{\"end\":20955,\"start\":20950},{\"end\":20966,\"start\":20959},{\"end\":21366,\"start\":21362},{\"end\":21374,\"start\":21370},{\"end\":21386,\"start\":21381},{\"end\":21584,\"start\":21581},{\"end\":21592,\"start\":21588}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":538820},\"end\":19640,\"start\":19319},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206594692},\"end\":19995,\"start\":19642},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7458412},\"end\":20258,\"start\":19997},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15076873},\"end\":20642,\"start\":20260},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5499886},\"end\":20834,\"start\":20644},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":392527},\"end\":21305,\"start\":20836},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4833213},\"end\":21526,\"start\":21307},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207188668},\"end\":21706,\"start\":21528}]", "bib_title": "[{\"end\":19395,\"start\":19319},{\"end\":19686,\"start\":19642},{\"end\":20089,\"start\":19997},{\"end\":20326,\"start\":20260},{\"end\":20677,\"start\":20644},{\"end\":20925,\"start\":20836},{\"end\":21358,\"start\":21307},{\"end\":21577,\"start\":21528}]", "bib_author": "[{\"end\":19406,\"start\":19397},{\"end\":19415,\"start\":19406},{\"end\":19425,\"start\":19415},{\"end\":19694,\"start\":19688},{\"end\":19703,\"start\":19694},{\"end\":19710,\"start\":19703},{\"end\":19717,\"start\":19710},{\"end\":20104,\"start\":20091},{\"end\":20339,\"start\":20328},{\"end\":20349,\"start\":20339},{\"end\":20358,\"start\":20349},{\"end\":20368,\"start\":20358},{\"end\":20382,\"start\":20368},{\"end\":20689,\"start\":20679},{\"end\":20940,\"start\":20927},{\"end\":20948,\"start\":20940},{\"end\":20957,\"start\":20948},{\"end\":20968,\"start\":20957},{\"end\":21368,\"start\":21360},{\"end\":21376,\"start\":21368},{\"end\":21388,\"start\":21376},{\"end\":21586,\"start\":21579},{\"end\":21594,\"start\":21586}]", "bib_venue": "[{\"end\":19841,\"start\":19783},{\"end\":20126,\"start\":20119},{\"end\":21402,\"start\":21399},{\"end\":19461,\"start\":19425},{\"end\":19781,\"start\":19717},{\"end\":20117,\"start\":20104},{\"end\":20440,\"start\":20382},{\"end\":20721,\"start\":20689},{\"end\":20972,\"start\":20968},{\"end\":21397,\"start\":21388},{\"end\":21601,\"start\":21594}]"}}}, "year": 2023, "month": 12, "day": 17}