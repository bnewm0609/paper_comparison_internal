{"id": 214003631, "updated": "2023-10-06 14:27:53.814", "metadata": {"title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings", "authors": "[{\"first\":\"Yu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Lingfei\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Mohammed\",\"last\":\"Zaki\",\"middle\":[\"J.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 21}, "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.13009", "mag": "3101979678", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/0022WZ20", "doi": null}}, "content": {"source": {"pdf_hash": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.13009v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b11551227fc92a7a30575ecac474355d1f72cd8f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ff6a4a9a41b78c8b1fcab185db780266bbb06caf.txt", "contents": "\nIterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings\n\n\nYu Chen \nBM Research\nRensselaer Polytechnic Institute\nRensselaer Polytechnic Institute\n\n\nLingfei Wui \nBM Research\nRensselaer Polytechnic Institute\nRensselaer Polytechnic Institute\n\n\nMohammed J Zaki zaki@cs.rpi.edu \nBM Research\nRensselaer Polytechnic Institute\nRensselaer Polytechnic Institute\n\n\nIterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings\n\nIn this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.Corresponding author.Preprint. Under review.\n\nIntroduction\n\nRecent years have seen a significantly growing amount of interest in graph neural networks (GNNs), especially on efforts devoted to developing more effective GNNs for node classification [27,34,16,48], graph classification [55,40] and graph generation [44,35,56]. Despite GNNs' powerful ability in learning expressive node embeddings, unfortunately, they can only be used when graph-structured data is available. Many real-world applications naturally admit network-structured data (e.g., social networks). However, these intrinsic graph-structures are not always optimal for the downstream tasks. This is partially because the raw graphs were constructed from the original feature space, which may not reflect the \"true\" graph topology after feature extraction and transformation. Another potential reason is that real-world graphs are often noisy or even incomplete due to the inevitably error-prone data measurement or collection. Furthermore, many applications such as those in natural language processing [7,53] may only have sequential data or even just the original feature matrix, requiring additional graph construction from the original data matrix.\n\nTo address these limitations, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning the graph structure and the GNN parameters that are optimized towards the downstream prediction task. The key rationale of our IDGL framework is to learn a better graph structure based on better node embeddings, and at the same time, to learn better node embeddings based on a better graph structure. In particular, IDGL is a novel iterative method that aims to search for a hidden graph structure that augments the initial graph structure (if not available we use a kNN graph) with the goal of optimizing the graph for supervised prediction tasks. The iterative method adjusts when to stop in each mini-batch when the learned graph structure approaches close enough to the graph optimized for the prediction task.\n\nFurthermore, we present a graph learning neural network that uses multi-head self-attention with epsilon-neighborhood sparsification for constructing a graph. Moreover, unlike the work in [23] that directly optimizes an adjacency matrix without considering the downstream task, we learn a graph metric learning function by optimizing a joint loss combining both task-specific prediction loss and graph regularization loss. Finally, we further propose a scalable version of our IDGL framework, namely IDGL-ANCH, by combining the anchor-based approximation technique, which reduces the time and memory complexity from quadratic to linear with respect to the numbers of graph nodes.\n\nIn short, we summarize the main contributions as follows:\n\n\u2022 We propose a novel end-to-end graph learning framework (IDGL) for jointly and iteratively learning the graph structure and graph embedding. IDGL dynamically stops when the learned graph structure approaches the optimized graph (for prediction). To the best of our knowledge, we are the first to introduce the iterative learning for graph structure learning. \u2022 Combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which achieves linear complexity in both computational time and memory consumption with respect to the number of graph nodes. \u2022 Experimental results show that our models consistently outperform or match state-of-the-art baselines on various downstream tasks. More importantly, IDGL can be more robust to adversarial graph examples and can cope with both transductive and inductive learning.\n\n\nIterative Deep Graph Learning Framework\n\n\nProblem Formulation\n\nLet the graph G \" pV, Eq be represented as a set of n nodes v i P V with an initial node feature matrix X P R d\u02c6n , edges pv i , v j q P E (binary or weighted) formulating an initial noisy adjacency matrix A p0q P R n\u02c6n , and a degree matrix D p0q\n\nii \"\n\u0159 j A p0q\nij . Given a noisy graph input G :\" tA p0q , Xu or only a feature matrix X P R d\u02c6n , the deep graph learning problem we consider in this paper is to produce an optimized graph G\u02da:\" tA p\u02daq , Xu and its corresponding graph node embeddings Z \" f pG\u02da, \u03b8q P R h\u02c6n , with respect to some (semi-)supervised downstream task. It is worth noting that we assume that the graph noise is only from graph topology (the adjacency matrix) and the node feature matrix X is noiseless. The more challenging scenario where both graph topology and node feature matrix are noisy, is part of our future work. Without losing the generality, in this paper, we consider both node-level and graph-level prediction tasks. \n\n\nGraph Learning and Graph Embedding: A Unified Perspective\n\nGraph topology is crucial for a GNN to learn expressive graph node embeddings. Most of existing GNN methods simply assume that the input graph topology is perfect, which is not necessarily true in practice since real-world graphs are often noisy or incomplete. More importantly, the provided input graph(s) may not be ideal for the supervised downstream tasks since most of raw graphs are constructed from the original feature space which may fail to reflect the \"true\" graph topology after high-level feature transformations. Some previous works [48] mitigate this issue by reweighting the importance of neighborhood node embeddings using self-attention on previously learned node embeddings, which still assumes that the original graph connectivity information is noiseless.\n\nTo handle potentially noisy input graph, we propose our novel IDGL framework that formulates the problem as an iterative learning problem which jointly learns the graph structure and the GNN parameters. The key rationale of our IDGL framework is to learn a better graph structure based on better node embeddings, and in the meanwhile, to learn better node embeddings based on a better graph structure, as shown in Fig. 2. Unlike most existing methods that construct graphs based on raw node features, the node embeddings learned by GNNs (optimized toward the downstream task) could provide useful information for learning better graph structures. On the other hand, the newly learned graph structures could be a better graph input for GNNs to learn better node embeddings.\n\nIn particular, IDGL is a novel iterative method that aims to search for hidden graph structure that augments the initial graph structure (if not available we use a kNN graph) for supervised prediction tasks. The iterative method adjusts when to stop in each mini-batch when the learned graph structure approaches close enough to the optimized graph (with respect to the downstream task) based on our proposed stopping criterion. Moreover, the process of constructing such a graph can be optimized towards the learning task at hand and the whole learning system is end-to-end trainable.\n\n\nGraph Learning as Similarity Metric Learning\n\nPrevious methods (e.g., [14]) that model the graph learning problem as learning a joint discrete probability distribution on the edges of the graph have shown promising performance. However, since they optimize the edge connectivities by assuming that the graph nodes are known, they are unable to cope with the inductive setting (with new nodes during testing). To overcome this issue, we cast the graph structure learning problem as similarity metric learning, which will be jointly trained with the prediction model dedicated to a downstream task.\n\nGraph similarity metric learning. Common options for metric learning include cosine similarity [41,50], radial basis function (RBF) kernel [54,32] and attention mechanisms [47,22]. A good similarity metric function is supposed to be learnable and expressively powerful. Although our framework is agnostic to various similarity metric functions, without loss of generality, we design a weighted cosine similarity as our metric function, s ij \" cospw d v i , w d v j q, where d denotes the Hadamard product, and w is a learnable weight vector which has the same dimension as the input vectors v i and v j , and learns to highlight different dimensions of the vectors. Note that the two input vectors could be either raw node features or computed node embeddings.\n\nTo stabilize the learning process and increase the expressive power, we extend our similarity metric function to a multi-head version (similar to the observations in [47,48]). Specifically, we use m weight vectors (each one representing one perspective) to compute m independent similarity matrices using the above similarity function and take their average as the final similarity:\ns p ij \" cospw p d v i , w p d v j q, s ij \" 1 m m \u00ff p\"1 s p ij (1)\nIntuitively, s p ij computes the cosine similarity between the two input vectors v i and v j , for the p-th perspective, where each perspective considers one part of the semantics captured in the vectors.\n\nGraph sparsification via \u03b5-neighborhood. Typically an adjacency matrix (computed from a metric) is supposed to be non-negative but s ij ranges between r\u00b41, 1s. In addition, many underlying graph structures are much more sparse than a fully connected graph, which is not only computationally expensive but also might introduce noise (i.e., unimportant edges). We hence proceed to extract a symmetric sparse non-negative adjacency matrix A from S by considering only the \u03b5-neighborhood for each node. Specifically, we mask off (i.e., set to zero) those elements in S which are smaller than a non-negative threshold \u03b5.\n\nAnchor-based scalable metric learning. The above similarity metric function like Eq. (1) computes similarity scores for all pairs of graph nodes, which requires Opn 2 q complexity for both computational time and memory consumption, rendering significant scalablity issue for large graphs. To address the scalability issue, inspired by previous anchor-based methods [38,51], we design an anchor-based scalable metric learning technique which learns a node-anchor affinity matrix R P R n\u02c6s (i.e., requires Opnsq for both time and space complexity where s is the number of anchors) between the node set V and the anchor set U. Note that s is a hyperparameter which needs to be tuned on the development set.\n\nSpecifically, we randomly sample a set of s P U anchors from the node set V, where s is usually much smaller than n in large graphs. The anchor embeddings are thus set to the corresponding node embeddings. Therefore, Eq. (1) can be rewritten as the following:\na p ik \" cospw p d v i , w p d u k q, a ik \" 1 m m \u00ff p\"1 a p ik (2)\nwhere a ik is the affinity score between node v i and anchor u k . Similarly, we apply the \u03b5-neighborhood sparsification technique to the node-anchor affinity scores a ik to obtain a sparse and non-negative node-anchor affinity matrix R.\n\n\nGraph Node Embeddings and Prediction\n\nAlthough the initial graph could be noisy, it typically still carries rich and useful information regarding true graph topology. Ideally, the learned graph structure A could be supplementary to the original graph topology A p0q to formulate an optimized graph for GNNs with respect to the downstream task. Therefore, with the mild assumption that the optimized graph structure is potentially a \"shift\" from the initial graph structure, we combine the learned graph with the initial graph,\nr A ptq \" \u03bbL p0q`p 1\u00b4\u03bbq ! \u03b7 fpA ptq q`p1\u00b4\u03b7q fpA p1q q ) (3) where L p0q \" D p0q\u00b41 {2 A p0q D p0q\u00b41 {2\nis the normalized adjacency matrix of the initial graph. A ptq and A p1q are the two adjacency matrices computed at the t-th and 1-st iterations (using Eq. (1)), respectively. The adjacency matrix is further row normalized, namely,\nfpAq ij \" A ij { \u0159 j A ij .\nNote that A p1q is computed from the raw node features X, whereas A ptq is computed from the previously updated node embeddings Z pt\u00b41q that is optimized toward the downstream prediction task. Therefore, we make the final learned graph structure as their linear combination weighted by a hyperparameter \u03b7, so as to combine the advantages of both. Finally, another hyperparameter \u03bb is used to balance the trade-off between the learned graph structure and the initial graph structure. If such an initial graph structure is not available, we instead use a kNN graph constructed based on raw node features X using cosine similarity.\n\nOur graph learning framework is agnostic to various GNN architectures and prediction tasks. In this paper, we adopt a two-layered GCN [27] where the first layer (denoted as GNN 1 ) maps the raw node features X to the intermediate embedding space, and the second layer (denoted as GNN 2 ) further maps the intermediate node embeddings Z to the output space.\n\nZ \" ReLUpMPpX, r AqW 1 q, p y \" \u03c3pMPpZ, r AqW 2 q, L pred \" pp y, yq\n\nwhere \u03c3p\u00a8q and p\u00a8q are task-dependent output function and loss function, respectively. For instance, for a classification task, \u03c3p\u00a8q is a softmax function for predicting a probability distribution over a set of classes, and p\u00a8q is a cross-entropy function for computing the prediction loss. MPp\u00a8,\u00a8q is a message passing function, and in GCN, MPpF, r Aq \" r AF for a feature/embedding matrix F and normalized adjacency matrix r A which we obtain using Eq. (3).\n\nNode-anchor message passing. Note that a node-anchor affinity matrix R serves as a weighted adjacency matrix of a bipartite graph B allowing only direct connections between nodes and anchors. If we regard a direct travel between a node and an anchor as one-step transition described by R, built upon theories of stationary Markov random walks [39], we can actually recover both the node graph G and the anchor graph Q from R by computing the two-step transition probabilities. Let A P R n\u02c6n denote a row-normalized adjacency matrix for the node graph G, and A ij \" p p2q pv j |v i q indicate the two-step transition probability from node v i to node v j , A can be recovered from R,\nA \" \u2206\u00b41R\u039b\u00b41R J (5) where \u039b kk \" \u0159 n i\"1 R ik and \u2206 ii \" \u0159 s k\"1 R ik .\nSimilarly, we can recover the row-normalized adjacency matrix B P R s\u02c6s for the anchor graph Q,\nB \" \u039b\u00b41R J \u2206\u00b41R(6)\nWhile explicitly computing a node adjacency matrix A from R (Eq. (5)) and directly performing message passing over the node graph G (Eq. (4)) are expensive in both time complexity (Opn 2 sq) and space complexity (Opn 2 q), one can instead equivalently decompose the above process (denoted as MP 12 ) into two steps: i) node-to-anchor message passing MP 1 and ii) anchor-to-node message passing MP 2 , over the node-anchor bipartite graph B, formulated as follows,\nMP 12 pF, Rq \" MP 2 pF 1 , Rq, F 1 \" MP 1 pF, Rq(7)\nwhere MP 1 pF, Rq \" \u039b\u00b41R J F aims to pass message F from the nodes V to the anchors U, and MP 2 pF 1 , Rq \" \u2206\u00b41RF 1 aims to further pass the message F 1 aggregated on the anchors back to the nodes. Finally, we can obtain MP 12 pF, Rq \" \u2206\u00b41R\u039b\u00b41R J F \" AF where A is the node adjacency matrix recovered from R using Eq. (5). In this way, we reduce both time and space complexity to Opnsq. Therefore, we can rewrite the regular node embedding and prediction equations defined in Eqs. (3) and (4) as follows,\nZ \" ReLUpMP a pX, tL p0q , R ptq , R p1q uqW 1 q, p y \" \u03c3pMP a pZ, tL p0q , R ptq , R p1q uqW 2 q(8)\nwhere MP a p\u00a8,\u00a8q is a hybrid message passing function with the same spirit of Eq. (3), defined as,\nMPapF, tL p0q , R ptq , R p1q uq \" \u03bbMPpF, L p0q q`p1\u00b4\u03bbq ! \u03b7MP12pF, R ptq q`p1\u00b4\u03b7qMP12pF, R p1q q )(9)\nNote that we use the same MPp\u00a8,\u00a8q function defined in Eq. (4) for performing message passing over L p0q which is typically sparse in practice, and F can either be X or Z.\n\n\nGraph Regularization\n\nAlthough combining the learned graph A ptq with the initial graph A p0q is an effective way to approach the optimaized graph, the quality of the learned graph A ptq plays an important role in improving the quality of the final graph r A ptq . In practice, it is important to control the smoothness, connectivity and sparsity of the resulting learned graph A ptq , which faithfully reflects the graph topology with respect to the initial node attributes X and the downstream task.\n\nLet each column of the feature matrix X be considered as a graph signal. A widely adopted assumption for graph signals is that values change smoothly across adjacent nodes. Given an undirected graph with a symmetric weighted adjacency matrix A, the smoothness of a set of n graph signals x 1 , . . . , x n P R d is usually measured by the Dirichlet energy [2],\n\u2126pA, Xq \" 1 2n 2 \u00ff i,j A ij ||x i\u00b4xj || 2 \" 1 n 2 trpX T LXq(10)\nwhere trp\u00a8q denotes the trace of a matrix, L \" D\u00b4A is the graph Laplacian, and D \" \u0159 j A ij is the degree matrix. As can be seen, minimizing \u2126pA, Xq forces adjacent nodes to have similar features, thus enforcing smoothness of the graph signals on the graph associated with A.\n\nHowever, solely minimizing the smoothness loss will result in the trivial solution A \" 0. Also, it is desirable to have control of how sparse the resulting graph is. Following [23], we impose additional constraints on the learned graph,\nf pAq \"\u00b4\u03b2 n 1 T logpA1q`\u03b3 n 2 ||A|| 2 F(11)\nwhere ||\u00a8|| F denotes the Frobenius norm of a matrix. The first term penalizes the formation of disconnected graphs via the logarithmic barrier, and the second term controls sparsity by penalizing large degrees due to the first term.\n\nWe then define the overall graph regularization loss as the sum of the above losses L G \" \u03b1\u2126pA, Xqf pAq, which is able to control the smoothness, connectivity and sparsity of the learned graph where \u03b1, \u03b2 and \u03b3 are all non-negative hyperparameters.\n\nAnchor graph regularization. As shown in Eq. (6), we can obtain a row-normalized adjacency matrix B for the anchor graph Q in Opns 2 q time complexity. In order to control the quality of the learned node-anchor affinity matrix R (which can result in implicit control of the quality of the node adjacency matrix A), we apply the aforementioned graph regularization techniques to the anchor graph. It is worthing noting that our proposed graph regularization loss is only applicable to nonnegative and symmetric adjacency matrices [24]. Therefore, instead of applying graph regularization to B which is often not symmetric, we opt to apply graph regularization to its unnormalized version p B \" R J \u2206\u00b41R as L G \" \u03b1\u2126p p B, X U q`f p p Bq, where X U denotes the set of anchor embeddings sampled from the set of node embeddings X.\n\n\nJoint Learning with A Hybrid Loss\n\nCompared to previous works which directly optimize the adjacency matrix based on either graph regularization loss [24], or task-dependent prediction loss [14], we propose to jointly and iteratively learning the graph structure and the GNN parameters by minimizing a hybrid loss function combining both the task prediction loss and the graph regularization loss, namely, L \" L pred`LG .\n\nThe full algorithm of the IDGL framework is presented in Algorithm 1 in Appendix A. As we can see, our model repeatedly refines the adjacency matrix with updated node embeddings (Eq. (1)), and refines the node embeddings (Eqs. (3) and (4)) with the updated adjacency matrix until the difference between adjacency matrices at consecutive iterations are smaller than certain threshold. Note that compared to using a fixed number of iterations globally, our dynamic stopping criterion is more beneficial, especially for mini-batch training. At each iteration, a hybrid loss combining both the task-dependent prediction loss and the graph regularization loss is computed. After all iterations, the overall loss is back-propagated through all previous iterations to update the model parameters. Notably, Algorithm 1 is also applicable to IDGL-ANCH as. And as we can see, the major differences between IDGL and IDGL-ANCH are how we compute adjacency (or affinity) matrix, and perform message passing and graph regularization.\n\n\nExperiments\n\nIn this section, we conduct extensive experiments to verify the effectiveness of IDGL and IDGL-ANCH in various settings. The implementation of our proposed models is publicly available at https://github.com/hugochan/IDGL.\n\nDatasets and baselines. The benchmarks used in our experiments include four citation network datasets (i.e., Cora, Citeseer, Pubmed and ogbn-arxiv) [45,20] where the graph topology is available, three non-graph datasets (i.e., Wine, Breast Cancer (Cancer) and Digits) [11] where the graph topology does not exist, and two text benchmarks (i.e., 20Newsgroups data (20News) and movie review data (MRD)) [30,43] where we treat a document as a graph containing each word as a node. The first seven datasets are all for node classification tasks in the transductive setting, and we follow the experimental setup of previous works [27,14,20]. The later two datasets are for graph-level prediction tasks in the inductive setting. Please refer to Appendix D.1 for detailed data statistics.\n\nOur main baseline is LDS [14] which however is incapable of handling inductive learning problems, we hence only report its results on transductive datasets. In addition, for citation network datasets, we include other GNN variants (i.e., GCN [27], GAT [48], GraphSage [17], APPNP [28], H-GCN [19] and GDC [29]) as baselines. For non-graph and text benchmarks where the graph topology is unavailable, we conceive a GCN kNN baseline where a kNN graph on the data set is constructed during preprocessing before applying a GCN. For text benchmarks, we include a BiLSTM [18] baseline. The reported results are averaged over 5 runs with different random seeds.\n\nExperimental results. The results of transductive experiments are shown in Table 1. First of all, we can see that IDGL outperforms all baselines in 4 out of 5 benchmarks, which demonstrates its effectiveness. Compared to IDGL, IDGL-ANCH is more scalable and can achieve comparable or even better results. Besides, we can see that our graph learning method can greatly help the node classification task even when the graph topology is given. When the graph topology is not available, compared to GCN kNN , IDGL consistently achieves much better results on all datasets, which shows the power of jointly learning graph structures and GNN parameters. The results of inductive experiments are shown in Table 2. Unlike LDS which cannot handle inductive setting, the good performance on 20News and MRD verifies the capability of IDGL on inductive learning.     Ablation study. Table 3 shows the ablation study results on different modules in our models. we can see a significant performance drop consistently for both IDGL and IDGL-ANCH on all datasets by turning off the iterative learning component (i.e., iterating only once), indicating its effectiveness. Besides, we can see the benefits of jointly training the model with the graph regularization loss.\n\nModel analysis. To evaluate the robustness of IDGL to adversarial graphs, we construct graphs with random edge deletions or additions. Specifically, for each pair of nodes in the original graph, we randomly remove (if an edge exists) or add (if no such edge) an edge with a probability 25%, 50% or 75%. As shown in Fig. 3, compared to GCN and LDS, IDGL achieves better or comparable results in both scenarios. While both GCN and LDS completely fail in the edge addition scenario, IDGL performs reasonably well. We conjecture this is because the edge addition scenario is more challenging than the edge deletion scenario by incorporating misleading additive random noise to the initial graph. And Eq. (3) is formulated as a form of skip-connection, by lowering the value of \u03bb (i.e., tuned on the development set), we enforce the model to rely less on the initial noisy graph.  In Fig. 4, we show the evolution of the learned adjacency matrix and accuracy through iterations in the iterative learning procedure in the testing phase. We compute the difference between adjacency matrices at consecutive iterations as \u03b4 ptq A \" ||A ptq\u00b4Apt\u00b41q || 2 F {||A ptq || 2 F which typically ranges from 0 to 1. As we can see, both the adjacency matrix and accuracy converge quickly. This empirically verifies the analysis we made on the convergence property of IDGL in Appendix B.2. Please note that this convergence property is not due to the oversmoothing effect of GNNs [52,31], because we only employ a two-layered GCN as the underlying GNN module of IDGL in our experiments.  (17) In Fig. 5, we empirically compare the effectiveness of two stopping strategies: i) using a fixed number of iterations (blue line), and ii) using a stopping criterion to dynamically determine the convergence (red line). As we can see, dynamically adjusting the number of iterations using the stopping criterion works better in practice. Compared to using a fixed number of iterations globally, the advantage of applying this dynamical stopping strategy becomes more clear when we are doing mini-batch training since we can adjust when to stop dynamically for each example graph.\n\nWe compare the training efficiency of IDGL and IDGL-ANCH with other baselines. As shown in Table 4, IDGL is consistently faster than LDS, but in general, they are comparable. Note that IDGL has comparable model size compared to LDS. For instance, on the Cora data, the number of trainable parameters of IDGL is 28,836, and for LDS, it is 23,040. And we see a large speedup of IDGL-ANCH compared to IDGL. Note that we were not able to run IDGL on Pubmed because of memory limitation. The theoretical complexity analysis is provided in Appendix B.3.\n\nWe also visualize the graph structures learned by IDGL (Appendix C.1), and conduct hyperparameter analysis (Appendix C.2). Details on model settings are provided in Appendix D.2.\n\n\nRelated Work\n\nThe problem of graph structure learning has been widely studied in different fields from different perspectives. In the field of graph signal processing, researchers have explored various ways of learning graphs from data [10,12,49,25,3,1], with certain structural constraints (e.g., sparsity) on the graphs. This problem has also been studied in the literature of clustering analysis [4,21] where they aimed to simultaneously perform the clustering task and learn similarity relationships among objects. These works all focused on unsupervised learning setting without considering any supervised downstream tasks, and were incapable of handling inductive learning problems. Other related works include structure inference in probabilistic graphical models [9,60,57], and graph generation [36,46], which have a different goal from ours.\n\nIn the field of GNNs [27,15,17,33,58], there is a line of research on developing robust GNNs that are invulnerable to adversarial graphs by leveraging attention-based methods [5], Bayesian methods [13,59] and graph diffusion-based methods [29]. Recently, researchers have explored methods to automatically construct a graph of objects [42,8,32,14] or words [37,6,7] when applying GNNs to non-graph structured data. However, these methods merely optimize the graphs towards the downstream tasks without the explicit control of the quality of the learned graphs. More recently, [14] proposed the LDS model for jointly learning the graph and the parameters of GNNs by leveraging the bilevel optimization technique. However, by design, their method is unable to handle the inductive setting. Our work is also related to Transformer-like approaches [47] that also learn relationships among objects by leveraging multi-head attention mechanism. However, these methods do not focus on the graph learning problem and were not designed to utilize the initial graph structure.\n\n\nConclusion\n\nWe proposed a novel IDGL framework for jointly and iteratively learning the graph structure and the GNN parameters that are optimized towards the prediction task at hand. The proposed method is able to iteratively search for hidden graph structures that better help the prediction task. Our extensive experiments demonstrate the effectiveness and efficiency of the proposed model. In the future, we are planning to explore more effective and scalable techniques for handling more challenging scenario where both graph topology and node feature matrix are noisy.\n\n\nA Full Algorithm of the IDGL and IDGL-ANCH Models\n\nAlgorithm 1 General Framework for IDGL and IDGL-ANCH 1: Input: X, yr, A p0q s 2: Parameters: m, \u03b5, \u03b1, \u03b2, \u03b3, \u03bb, \u03b4, T , \u03b7, kr, ss 3: Output: \u0398, p y, r A ptq or R ptq 4: rA p0q \u00d0 kNNpX, kqs {kNN-graph if no initial A p0q } 5: t \u00d0 1 Z ptq \u00d0 GNN 1 ptA p0q , R ptq , R p1q u, Xq using Eqs. (8) and (9) {Refine node embeddings} 15: end if 16: p y \u00d0 GNN 2 p r A ptq , Z ptq q using Eq. (4) if IDGL else GNN 2 ptA p0q , R ptq , R p1q u, Z ptq q using Eqs. (8) and (9) 17: L ptq pred \u00d0 LOSS 1 pp y, yq using Eq. (4) 18: It is worth noting that a node-anchor affinity matrix R serves as a weighted adjacency matrix of a bipartite graph B. We hence establish stationary Markov random walks [39] by defining the one-step transition probabilities as follows,\n6: StopCond \u00d0 |A ptq\u00b4Apt\u00b41q | 2 F \u0105 \u03b4|A p1q | 2 F if IDGL else |R ptq\u00b4Rpt\u00b41q | 2 F \u0105 \u03b4|R ptq | 2L ptq G \u00d0 \u03b1\u2126pA ptq , Xq`f pA ptq q if IDGL else \u03b1\u2126p p B ptq , X U q`f p p B ptq q where p B ptq \" R ptq J \u2206\u00b41R ptq 19: L ptq \u00d0 L ptq pred`Lp p1q pu k |v i q \" R ik \u0159 s k 1 \"1 R ik 1 , p p1q pv i |u k q \" R ik \u0159 n i 1 \"1 R i 1 k , @v i P V, @u k P U(12)\nWe can further compute the two-step transition probabilities between nodes as follows,\np p2q pv j |v i q \" s \u00ff k\"1 p p1q pv j |u k qp p1q pu k |v i q \" s \u00ff k\"1 R jk \u0159 n j 1 \"1 R j 1 k R ik \u0159 s k 1 \"1 R ik 1 \" s \u00ff k\"1 R jk \u039b kk R ik \u2206 ii(13)\nwhere \u039b kk \" \u0159 n j 1 \"1 R j 1 k and \u2206 ii \" \u0159 s k 1 \"1 R ik 1 . Therefore, we can recover a row-normalized adjacency matrix A P R n\u02c6n for the node graph as A ij \" p p2q pv j |v i q, which can be further written in a compact form A \" \u2206\u00b41R\u039b\u00b41R J .\n\nSimilarly, we can compute the two-step transition probabilities between anchors as follows, p p2q pu r |u k q \" n \u00ff i\"1 p p1q pu r |v i qp p1q pv i |u k q \"\nn \u00ff i\"1 R ir \u0159 s r 1 \"1 R ir 1 R ik \u0159 n i 1 \"1 R i 1 k \" n \u00ff i\"1 R ir \u2206 ii R ik \u039b kk(14)\nAnd a row-normalized adjacency matrix B P R s\u02c6s for the anchor graph Q can be formulated as B kr \" p p2q pu r |u k q. And we can obtain B \" \u039b\u00b41R J \u2206\u00b41R.\n\n\nB.2 Theoretical Convergence Analysis\n\nWhile it is challenging to theoretically prove the convergence of the proposed iterative learning procedure due to the arbitrary complexity of the model, here we want to conceptually understand why it works in practice. Fig. 6 shows the information flow of the learned adjacency matrix A and the updated node embedding matrix Z during the iterative procedure. For the sake of simplicity, we omit some other variables such as r A. As we can see, at t-th iteration, A ptq is computed based on Z pt\u00b41q (Line 9), and Z ptq is computed based on r A ptq (Line 11) which is computed based on A ptq (Eq. (3)). We further denote the difference between the adjacency matrices at the t-th iteration and the previous iteration by \u03b4 ptq A . Similarly, we denote the difference between the node embedding matrices at the t-th iteration and the previous iteration by \u03b4 ptq Z . If we assume that \u03b4 p2q Z \u0103 \u03b4 p1q Z , then we can expect that \u03b4 p3q A \u0103 \u03b4 p2q A because conceptually a more similar node embedding matrix (i.e., smaller \u03b4 Z ) is supposed to produce a more similar adjacency matrix (i.e., smaller \u03b4 A ) given the fact that model parameters keep the same through iterations. Similarly, given that \u03b4 p3q A \u0103 \u03b4 p2q A , we can expect that \u03b4 p3q Z \u0103 \u03b4 p2q Z . Following this chain of reasoning, we can easily extend it to later iterations. In order to see why the assumption \u03b4 p2q Z \u0103 \u03b4 p1q Z makes sense in practice, we need to recall the fact that \u03b4 p1q Z measures the difference between Z p1q and X, which is usually larger than the difference between Z p2q and Z p1q , namely \u03b4 p2q Z . For example, the raw node feature matrix X can be quite sparse in practice (e.g., in Cora and Citeseer), whereas Z p1q is typically a dense matrix.\n\n\nB.3 Model Complexity Analysis\n\nAs for IDGL, the cost of learning an adjacency matrix is Opn 2 hq for n nodes and data in R h , while computing node embeddings costs Opn 2 h`ndhq, computing task output costs Opn 2 dq, and computing the total loss costs Opn 2 dq where d is the hidden size. We set the maximal number of iterations to T , hence the overall complexity is OpT npnh`nd`hdqq. If we assume that d \u00ab h and n \" d, the overall time complexity is OpT dn 2 q.\n\nAs for IDGL-ANCH, the cost of learning a node-anchor affinity matrix is Opnshq, while computing node embeddings costs Opnsh`ndh`|E|hq, computing task output costs Opnsd`|E|dq, and computing the total loss costs Opns 2`s2 dq where |E| is the number of edges in the initial or kNN graph G. With the assumption that the initial or kNN graph is usually very sparse in practice, especially for large graphs, we hence set |E| \" kn where k is a constant denoting the average degree of the initial or kNN graph. Therefore, we get the overall time complexity OpT npds`d 2`s2 qq. If we assume that n \" s which usually holds true for large graphs, the overall time complexity is linear with respect to the numbers of graph nodes n.\n\nAs for space complexity, compared to IDGL, IDGL-ANCH reduces it from Opn 2 q to Opnsq since it only needs to store the n\u02c6s affinity matrix.\n\n\nC Empirical Model Analysis\n\n\nC.1 Graph Visualization\n\nHere, we visualize the graph structures (i.e., A ptq ) learned by IDGL. As we can see, compared to the initial graph structures, IDGL mainly forms graph structures within the same class of nodes, which complement the initial graph structure. This is as expected because A ptq is computed based on the updated node embeddings that are supposed to capture certain node label information. \n\n\nC.2 Hyperparameter Analysis\n\nA hyperparameter \u03bb is used to balance the trade-off between using the learned graph structure and the initial (or kNN) graph structure. In Table 5, we show the results of using different values of \u03bb on Cora.\n\nWe also study the effect of the hyperparameter s (i.e., the number of anchors in IDGL-ANCH). As shown in Table 6, lower value of s can degrade the performance of IDGL-ANCH whereas after certain optimal value, further increasing the number of anchors might not help the performance.   Table 7 shows the data statistics of the nine benchmarks used in our experiments.     In all our experiments, we apply a dropout ratio of 0.5 after GCN layers except for the output GCN layer. During the iterative learning procedure, we also apply a dropout ratio of 0.5 after the intermediate GCN layer, except for Citeseer (no dropout) and Digits (0.3 dropout). For experiments on text benchmarks, we keep and fix the 300-dim GloVe vectors for words that appear more than 10 times in the dataset. For long documents, for the sake of efficiency, we cut the text length to maximum 1,000 words. We apply a dropout ratio of 0.5 after word embedding layers and BiLSTM layers. The batch size is set to 16. And the hidden size is set to 128 and 64 for 20News and MRD, respectively.\n\nFor all other benchmarks, the hidden size is set to 16 to follow the original GCN paper. For the text benchmarks, we apply a BiLSTM to a sequence of word embeddings. The concatenation of the last forward and backward hidden states of the BiLSTM is used as the initial node features. We use Adam [26] as the optimizer. For the text benchmarks, we set the learning rate to 1e-3. For all other benchmarks, we set the learning rate to 0.01 and apply L2 norm regularization with weight decay set to 5e-4. As for IDGL-ANCH, we set the number of anchors as a hyperparameter in transductive experiments, while in inductive experiments, we set the ratio of anchors (proportional to the graph size) as a hyperparameter. In Table 8 and Table 9, we show the hyperparameters for IDGL and IDGL-ANCH on all benchmarks, respectively. All hyperparameters are tuned on the development set.\n\nFigure 1 :\n1Overall architecture of the proposed IDGL framework. Dashed lines (in data points on left) indicate the initial noisy graph topology A p0q (if not available we use a kNN graph).\n\nFigure 2 :\n2A sketch of the proposed IDGL framework.\n\n\no graph reg. 84.3 (0.4) 71.5 (0.9) 97.3 (0.8) 94.9 (1.0) 91.5 (0.9) 83.4 (0.5) w/o IL 83.5 (0.6) 71.0 (0.8) 97.2 (0.8) 94.7 (0.9) 92.4 (0.4) 83.0 (0.4) IDGL-ANCH 84.4 (0.2) 72.0 (1.0) 98.1 (1.1) 94.8 (1.4) 93.2 (0.9) 82.9 (0.3) w/o graph reg. 83.2 (0.8) 70.1 (0.8) 97.4 (1.8) 94.8 (1.4) 92.0 (1.3) 82.5 (0.7) w/o IL 83.6 (0.2) 68.6 (0.7) 96.4 (1.5) 94.0 (2.6) 93.0 (0.4) 82.3 (0.3)\n\nFigure 3 :Figure 4 :\n34Test accuracy (\u02d8standard deviation) in percent for the edge attack scenarios on Cora. Convergence study on Cora and Citeseer (single run results).\n\nFigure 5 :\n5Stopping strategy study on Cora and Citeseer (single run results).\n\nF 7 :AR\n7while (pt \"\" 1 or StopCond ) and t \u010f T do ptq \u00d0 GLpXq or GLpZ pt\u00b41q q using Eq. (1) {Refine adj. matrix}10:    r A ptq \u00d0 tA p0q , A ptq , A p1q u using Eq. (3) {Combine refined and raw adj. matrices}11:    Z ptq \u00d0 GNN 1 p r A ptq , Xq using Eq. ptq \u00d0 GLpX, X U q or GLpZ pt\u00b41q ,\n\n\nptq G and t \u00d0 t`1 20: end while 21: L \u00d0 L p1q`\u0159 t i\"2 L piq {pt\u00b41q 22: Back-propagate L to update model weights \u0398 {In training phase only} B Theoretical Model Analysis B.1 Theoretical Proof of Recovering Node and Anchor Graphs from Affinity Matrix R\n\nFigure 6 :\n6Information flow of iterative learning procedure.\n\nFigure 7 :Figure 8 :\n78Visualization of the initial graph and the learned graph on Cora. Colors indicate different node labels. (a) kNN graph (A p0q ) (b) Learned graph (A ptq ) Visualization of the kNN graph and the learned graph on Wine. Colors indicate different node labels.\n\nTable 1 :\n1Summary of results in terms of classification accuracies (in percent) on transductive benchmarks. The star symbol indicates that we ran the experiments. The dash symbol indicates that reported results were unavailable or we were not able to run the experiments due to memory issue.Model \nCora \nCiteseer \nPubmed \nogbn-arxiv Wine \nCancer \nDigits \nGCN \n81.5 \n70.3 \n79.0 \n71.7 (0.\n\nTable 2 :\n2Summary of results in terms of classification accuracies or regression scores (R2 ) (in percent) \n\n\nTable 3 :\n3Ablation study on various node/graph classification datasets.Methods \nCora \nCiteseer \nWine \nCancer \nDigits \n20News \nIDGL \n84.5 (0.3) 74.1 (0.2) 97.8 (0.6) 95.1 (1.\n\nTable 4 :\n4Mean and standard deviation of training time (5 runs) on various benchmarks (in seconds).Data \nCora \nCiteseer \nPubmed \nGCN \n3 (1) \n5 (1) \n29 (4) \nGAT \n26 (5) \n28 (5) \n-\nLDS \n390 (82) 585 (181) \n-\nIDGL \n237 (21) 563 (100) \n-\nw/o IL \n49 (8) \n61 (15) \n-\nIDGL-ANCH \n83 (6) \n261 (50) \n323 (53) \nw/o IL \n28 (4) \n69 (9) \n71 \n\nTable 5 :\n5Test scores (\u02d8standard deviation) with different values of \u03bb on the Cora data.Methods / \u03bb \n0.9 \n0.8 \n0.7 \n0.6 \n0.5 \nIDGL \n83.6 (0.4) 84.5 (0.3) 83.9 (0.3) 82.4 (0.1) 80.9 (0.2) \nIDGL-ANCH \n83.2 (0.4) 84.4 (0.2) 83.5 (0.6) 82.9 (0.4) 54.6 (32.3) \n\n\nTable 6 :\n6Test scores (\u02d8standard deviation) with different values of s for IDGL-ANCH on the Cora and Pubmed data.Methods / s \n1,600 \n1,300 \n1,000 \n700 \n400 \n100 \nCora \n84.0 (0.4) 84.1 (0.5) 84.4 (0.2) 83.8 (0.2) 58.7 (30.5) 38.3 (25.9) \nPubmed \n82.7 (0.2) 83.0 (0.4) 82.7 (0.4) 83.0 (0.2) 82.7 (0.3) 82.4 (0.5) \n\nD Details on Experimental Setup \n\nD.1 Data Statistics \n\n\n\nTable 7 :\n7Data statistics. (clf. indicates classification and reg. indicates regression.)Benchmarks \n#Nodes \n#Edges \nTrain/Dev/Test \nTask \nSetting \nCora \n2,708 (1 graph) \n5,429 \n140/500/1,000 \nnode clf. \ntransductive \nCiteseer \n3,327 (1 graph) \n4,732 \n120/500/1,000 \nnode clf. \ntransductive \nPubmed \n19,717 (1 graph) \n44,338 \n60/500/1,000 \nnode clf. \ntransductive \nogbn-arxiv \n169,343 (1 graph) \n1,166,243 90,941/29,799/48,603 node clf. \ntransductive \nWine \n178 (1 graph) \nN/A \n10/20/158 \nnode clf. \ntransductive \nCancer \n569 (1 graph) \nN/A \n10/20/539 \nnode clf. \ntransductive \nDigits \n1,797 (1 graph) \nN/A \n50/100/1,647 \nnode clf. \ntransductive \n20News \n317 (18,846 graphs) N/A \n7,919/3,395/7,532 \ngraph clf. inductive \nMRD \n389 (5,006 graphs) \nN/A \n3,003/1,001/1,002 \ngraph reg. inductive \n\nD.2 Model Settings \n\n\n\nTable 8 :\n8Hyperparameter for IDGL on all benchmarks.\n\nTable 9 :\n9Hyperparameter for IDGL-ANCH on all benchmarks.\n\nMulti-view feature selection via nonnegative structured graph learning. X Bai, L Zhu, C Liang, J Li, X Nie, X Chang, Neurocomputing. X. Bai, L. Zhu, C. Liang, J. Li, X. Nie, and X. Chang. Multi-view feature selection via nonnegative structured graph learning. Neurocomputing, 2020.\n\nLaplacian eigenmaps and spectral techniques for embedding and clustering. M Belkin, P Niyogi, Advances in neural information processing systems. M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in neural information processing systems, pages 585-591, 2002.\n\nEfficient graph learning from noisy and incomplete data. P Berger, G Hannak, G Matz, IEEE Transactions on Signal and Information Processing over Networks. 6P. Berger, G. Hannak, and G. Matz. Efficient graph learning from noisy and incomplete data. IEEE Transactions on Signal and Information Processing over Networks, 6:105-119, 2020.\n\nRobust spectral clustering for noisy data: Modeling sparse corruptions improves latent embeddings. A Bojchevski, Y Matkovic, S G\u00fcnnemann, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningA. Bojchevski, Y. Matkovic, and S. G\u00fcnnemann. Robust spectral clustering for noisy data: Modeling sparse corruptions improves latent embeddings. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 737-746, 2017.\n\nLabel aware graph convolutional network-not all edges deserve your attention. H Chen, L Wang, S Wang, D Luo, W Huang, Z Li, arXiv:1907.04707arXiv preprintH. Chen, L. Wang, S. Wang, D. Luo, W. Huang, and Z. Li. Label aware graph convolutional network-not all edges deserve your attention. arXiv preprint arXiv:1907.04707, 2019.\n\nGraphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension. Y Chen, L Wu, M J Zaki, arXiv:1908.00059arXiv preprintY. Chen, L. Wu, and M. J. Zaki. Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension. arXiv preprint arXiv:1908.00059, 2019.\n\nReinforcement learning based graph-to-sequence model for natural question generation. Y Chen, L Wu, M J Zaki, arXiv:1908.04942arXiv preprintY. Chen, L. Wu, and M. J. Zaki. Reinforcement learning based graph-to-sequence model for natural question generation. arXiv preprint arXiv:1908.04942, 2019.\n\nE Choi, Z Xu, Y Li, M W Dusenberry, G Flores, Y Xue, A M Dai, arXiv:1906.04716Graph convolutional transformer: Learning the graphical structure of electronic health records. arXiv preprintE. Choi, Z. Xu, Y. Li, M. W. Dusenberry, G. Flores, Y. Xue, and A. M. Dai. Graph convolutional transformer: Learning the graphical structure of electronic health records. arXiv preprint arXiv:1906.04716, 2019.\n\nBayesian network learning with cutting planes. J Cussens, arXiv:1202.3713arXiv preprintJ. Cussens. Bayesian network learning with cutting planes. arXiv preprint arXiv:1202.3713, 2012.\n\nLearning laplacian matrix in smooth graph signal representations. X Dong, D Thanou, P Frossard, P Vandergheynst, IEEE Transactions on Signal Processing. 6423X. Dong, D. Thanou, P. Frossard, and P. Vandergheynst. Learning laplacian matrix in smooth graph signal representations. IEEE Transactions on Signal Processing, 64(23):6160-6173, 2016.\n\nUCI machine learning repository. D Dua, C Graff, D. Dua and C. Graff. UCI machine learning repository, 2017.\n\nGraph learning from data under laplacian and structural constraints. H E Egilmez, E Pavez, A Ortega, IEEE Journal of Selected Topics in Signal Processing. 116H. E. Egilmez, E. Pavez, and A. Ortega. Graph learning from data under laplacian and structural constraints. IEEE Journal of Selected Topics in Signal Processing, 11(6):825-841, 2017.\n\nVariational inference for graph convolutional networks in the absence of graph data and adversarial settings. P Elinas, E V Bonilla, L Tiao, 1906arXivP. Elinas, E. V. Bonilla, and L. Tiao. Variational inference for graph convolutional networks in the absence of graph data and adversarial settings. arXiv, pages arXiv-1906, 2019.\n\nLearning discrete structures for graph neural networks. L Franceschi, M Niepert, M Pontil, X He, arXiv:1903.11960arXiv preprintL. Franceschi, M. Niepert, M. Pontil, and X. He. Learning discrete structures for graph neural networks. arXiv preprint arXiv:1903.11960, 2019.\n\nNeural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1263-1272. JMLR. org, 2017.\n\nInductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Advances in Neural Information Processing Systems. W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2017.\n\nInductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Advances in Neural Information Processing Systems. W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024-1034, 2017.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735- 1780, 1997.\n\nSemi-supervised node classification via hierarchical graph convolutional networks. F Hu, Y Zhu, S Wu, L Wang, T Tan, arXiv:1902.06667arXiv preprintF. Hu, Y. Zhu, S. Wu, L. Wang, and T. Tan. Semi-supervised node classification via hierarchical graph convolutional networks. arXiv preprint arXiv:1902.06667, 2019.\n\nOpen graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, arXiv:2005.00687arXiv preprintW. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.\n\nAuto-weighted multi-view clustering via kernelized graph learning. S Huang, Z Kang, I W Tsang, Z Xu, Pattern Recognition. 88S. Huang, Z. Kang, I. W. Tsang, and Z. Xu. Auto-weighted multi-view clustering via kernelized graph learning. Pattern Recognition, 88:174-184, 2019.\n\nSemi-supervised learning with graph learningconvolutional networks. B Jiang, Z Zhang, D Lin, J Tang, B Luo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionB. Jiang, Z. Zhang, D. Lin, J. Tang, and B. Luo. Semi-supervised learning with graph learning- convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11313-11320, 2019.\n\nHow to learn a graph from smooth signals. V Kalofolias, Artificial Intelligence and Statistics. V. Kalofolias. How to learn a graph from smooth signals. In Artificial Intelligence and Statistics, pages 920-929, 2016.\n\nLarge scale graph learning from smooth signals. V Kalofolias, N Perraudin, arXiv:1710.05654arXiv preprintV. Kalofolias and N. Perraudin. Large scale graph learning from smooth signals. arXiv preprint arXiv:1710.05654, 2017.\n\nRobust graph learning from noisy data. Z Kang, H Pan, S C Hoi, Z Xu, IEEE transactions on cybernetics. Z. Kang, H. Pan, S. C. Hoi, and Z. Xu. Robust graph learning from noisy data. IEEE transactions on cybernetics, 2019.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.02907arXiv preprintT. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\n\nPredict then propagate: Graph neural networks meet personalized pagerank. J Klicpera, A Bojchevski, S G\u00fcnnemann, arXiv:1810.05997arXiv preprintJ. Klicpera, A. Bojchevski, and S. G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.\n\nDiffusion improves graph learning. J Klicpera, S Wei\u00dfenberger, S G\u00fcnnemann, Advances in Neural Information Processing Systems. J. Klicpera, S. Wei\u00dfenberger, and S. G\u00fcnnemann. Diffusion improves graph learning. In Advances in Neural Information Processing Systems, pages 13333-13345, 2019.\n\nNewsweeder: Learning to filter netnews. K Lang, Machine Learning Proceedings. ElsevierK. Lang. Newsweeder: Learning to filter netnews. In Machine Learning Proceedings 1995, pages 331-339. Elsevier, 1995.\n\nDeeper insights into graph convolutional networks for semisupervised learning. Q Li, Z Han, X.-M Wu, Thirty-Second AAAI Conference on Artificial Intelligence. Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nAdaptive graph convolutional neural networks. R Li, S Wang, F Zhu, J Huang, Thirty-Second AAAI Conference on Artificial Intelligence. R. Li, S. Wang, F. Zhu, and J. Huang. Adaptive graph convolutional neural networks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nY Li, D Tarlow, M Brockschmidt, R Zemel, arXiv:1511.05493Gated graph sequence neural networks. arXiv preprintY. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.\n\nGated graph sequence neural networks. Y Li, D Tarlow, M Brockschmidt, R Zemel, International Conference on Learning Representations. Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. International Conference on Learning Representations, 2016.\n\nLearning deep generative models of graphs. Y Li, O Vinyals, C Dyer, R Pascanu, P Battaglia, arXiv:1803.03324arXiv preprintY. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia. Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018.\n\nEfficient graph generation with graph recurrent attention networks. R Liao, Y Li, Y Song, S Wang, W Hamilton, D K Duvenaud, R Urtasun, R Zemel, Advances in Neural Information Processing Systems. R. Liao, Y. Li, Y. Song, S. Wang, W. Hamilton, D. K. Duvenaud, R. Urtasun, and R. Zemel. Efficient graph generation with graph recurrent attention networks. In Advances in Neural Information Processing Systems, pages 4257-4267, 2019.\n\nContextualized non-local neural networks for sequence learning. P Liu, S Chang, X Huang, J Tang, J C K Cheung, arXiv:1811.08600arXiv preprintP. Liu, S. Chang, X. Huang, J. Tang, and J. C. K. Cheung. Contextualized non-local neural networks for sequence learning. arXiv preprint arXiv:1811.08600, 2018.\n\nLarge graph construction for scalable semi-supervised learning. W Liu, J He, S.-F Chang, W. Liu, J. He, and S.-F. Chang. Large graph construction for scalable semi-supervised learning. 2010.\n\nRandom walks on graphs: A survey. L Lov\u00e1sz, L. Lov\u00e1sz et al. Random walks on graphs: A survey.\n\nY Ma, S Wang, C C Aggarwal, J Tang, arXiv:1904.13107Graph convolutional networks with eigenpooling. arXiv preprintY. Ma, S. Wang, C. C. Aggarwal, and J. Tang. Graph convolutional networks with eigenpooling. arXiv preprint arXiv:1904.13107, 2019.\n\nCosine similarity metric learning for face verification. H V Nguyen, L Bai, Asian conference on computer vision. SpringerH. V. Nguyen and L. Bai. Cosine similarity metric learning for face verification. In Asian conference on computer vision, pages 709-720. Springer, 2010.\n\nLearning conditioned graph structures for interpretable visual question answering. W Norcliffe-Brown, S Vafeias, S Parisot, Advances in Neural Information Processing Systems. W. Norcliffe-Brown, S. Vafeias, and S. Parisot. Learning conditioned graph structures for interpretable visual question answering. In Advances in Neural Information Processing Systems, pages 8344-8353, 2018.\n\nA sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. B Pang, L Lee, Proceedings of the 42nd annual meeting on Association for Computational Linguistics. the 42nd annual meeting on Association for Computational LinguisticsAssociation for Computational Linguistics271B. Pang and L. Lee. A sentimental education: Sentiment analysis using subjectivity summariza- tion based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics, 2004.\n\nDesigning random graph models using variational autoencoders with applications to chemical design. B Samanta, A De, N Ganguly, M Gomez-Rodriguez, arXiv:1802.05283arXiv preprintB. Samanta, A. De, N. Ganguly, and M. Gomez-Rodriguez. Designing random graph mod- els using variational autoencoders with applications to chemical design. arXiv preprint arXiv:1802.05283, 2018.\n\nCollective classification in network data. P Sen, G Namata, M Bilgic, L Getoor, B Galligher, T Eliassi-Rad, AI magazine. 293P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classifica- tion in network data. AI magazine, 29(3):93-93, 2008.\n\nGraphaf: a flow-based autoregressive model for molecular graph generation. C Shi, M Xu, Z Zhu, W Zhang, M Zhang, J Tang, arXiv:2001.09382arXiv preprintC. Shi, M. Xu, Z. Zhu, W. Zhang, M. Zhang, and J. Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382, 2020.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.\n\nP Veli\u010dkovi\u0107, G Cucurull, A Casanova, A Romero, P Li\u00f2, Y Bengio, arXiv:1710.10903Graph attention networks. arXiv preprintP. Veli\u010dkovi\u0107, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\nGraspel: Graph spectral learning at scale. Y Wang, Z Zhao, Z Feng, arXiv:1911.10373arXiv preprintY. Wang, Z. Zhao, and Z. Feng. Graspel: Graph spectral learning at scale. arXiv preprint arXiv:1911.10373, 2019.\n\nDeep cosine metric learning for person re-identification. N Wojke, A Bewley, 2018 IEEE winter conference on applications of computer vision (WACV). IEEEN. Wojke and A. Bewley. Deep cosine metric learning for person re-identification. In 2018 IEEE winter conference on applications of computer vision (WACV), pages 748-756. IEEE, 2018.\n\nScalable global alignment graph kernel using random features: From node embedding to graph embedding. L Wu, I E .-H. Yen, Z Zhang, K Xu, L Zhao, X Peng, Y Xia, C Aggarwal, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningL. Wu, I. E.-H. Yen, Z. Zhang, K. Xu, L. Zhao, X. Peng, Y. Xia, and C. Aggarwal. Scalable global alignment graph kernel using random features: From node embedding to graph embedding. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1418-1428, 2019.\n\nK Xu, C Li, Y Tian, T Sonobe, K Kawarabayashi, S Jegelka, arXiv:1806.03536Representation learning on graphs with jumping knowledge networks. arXiv preprintK. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018.\n\nExploiting rich syntactic information for semantic parsing with graph-to-sequence model. K Xu, L Wu, Z Wang, M Yu, L Chen, V Sheinin, arXiv:1808.07624arXiv preprintK. Xu, L. Wu, Z. Wang, M. Yu, L. Chen, and V. Sheinin. Exploiting rich syntactic information for semantic parsing with graph-to-sequence model. arXiv preprint arXiv:1808.07624, 2018.\n\nA kernel approach for semisupervised metric learning. D.-Y Yeung, H Chang, IEEE Transactions on Neural Networks. 181D.-Y. Yeung and H. Chang. A kernel approach for semisupervised metric learning. IEEE Transactions on Neural Networks, 18(1):141-149, 2007.\n\nHierarchical graph representation learning with differentiable pooling. Z Ying, J You, C Morris, X Ren, W Hamilton, J Leskovec, Advances in Neural Information Processing Systems. Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec. Hierarchical graph repre- sentation learning with differentiable pooling. In Advances in Neural Information Processing Systems, pages 4800-4810, 2018.\n\nJ You, R Ying, X Ren, W L Hamilton, J Leskovec, arXiv:1802.08773Graphrnn: Generating realistic graphs with deep auto-regressive models. arXiv preprintJ. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. arXiv preprint arXiv:1802.08773, 2018.\n\nY Yu, J Chen, T Gao, M Yu, arXiv:1904.10098Dag-gnn: Dag structure learning with graph neural networks. arXiv preprintY. Yu, J. Chen, T. Gao, and M. Yu. Dag-gnn: Dag structure learning with graph neural networks. arXiv preprint arXiv:1904.10098, 2019.\n\nGraph transformer networks. S Yun, M Jeong, R Kim, J Kang, H J Kim, Advances in Neural Information Processing Systems. S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, pages 11960-11970, 2019.\n\nBayesian graph convolutional neural networks for semi-supervised classification. Y Zhang, S Pal, M Coates, D Ustebay, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Y. Zhang, S. Pal, M. Coates, and D. Ustebay. Bayesian graph convolutional neural networks for semi-supervised classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5829-5836, 2019.\n\nDags with no tears: Continuous optimization for structure learning. X Zheng, B Aragam, P K Ravikumar, E P Xing, Advances in Neural Information Processing Systems. X. Zheng, B. Aragam, P. K. Ravikumar, and E. P. Xing. Dags with no tears: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems, pages 9472-9483, 2018.\n", "annotations": {"author": "[{\"end\":182,\"start\":94},{\"end\":275,\"start\":183},{\"end\":388,\"start\":276}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":97},{\"end\":194,\"start\":191},{\"end\":291,\"start\":287}]", "author_first_name": "[{\"end\":96,\"start\":94},{\"end\":190,\"start\":183},{\"end\":284,\"start\":276},{\"end\":286,\"start\":285}]", "author_affiliation": "[{\"end\":181,\"start\":103},{\"end\":274,\"start\":196},{\"end\":387,\"start\":309}]", "title": "[{\"end\":91,\"start\":1},{\"end\":479,\"start\":389}]", "venue": null, "abstract": "[{\"end\":1691,\"start\":481}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1898,\"start\":1894},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1901,\"start\":1898},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1904,\"start\":1901},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1907,\"start\":1904},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":1934,\"start\":1930},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1937,\"start\":1934},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1963,\"start\":1959},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1966,\"start\":1963},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":1969,\"start\":1966},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2720,\"start\":2717},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2723,\"start\":2720},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3939,\"start\":3935},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6993,\"start\":6989},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8656,\"start\":8652},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9279,\"start\":9275},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9282,\"start\":9279},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9323,\"start\":9319},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9326,\"start\":9323},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9356,\"start\":9352},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9359,\"start\":9356},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10112,\"start\":10108},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10115,\"start\":10112},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11585,\"start\":11581},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11588,\"start\":11585},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14146,\"start\":14142},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15244,\"start\":15240},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16603,\"start\":16600},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18123,\"start\":18120},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18647,\"start\":18643},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19765,\"start\":19761},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20213,\"start\":20209},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20253,\"start\":20249},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21892,\"start\":21888},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21895,\"start\":21892},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22012,\"start\":22008},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22145,\"start\":22141},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22148,\"start\":22145},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22369,\"start\":22365},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22372,\"start\":22369},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22375,\"start\":22372},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22552,\"start\":22548},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22769,\"start\":22765},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22779,\"start\":22775},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22795,\"start\":22791},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22807,\"start\":22803},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22819,\"start\":22815},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22832,\"start\":22828},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23092,\"start\":23088},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25896,\"start\":25892},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25899,\"start\":25896},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27554,\"start\":27550},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27557,\"start\":27554},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27560,\"start\":27557},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27563,\"start\":27560},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27565,\"start\":27563},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27567,\"start\":27565},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27716,\"start\":27713},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27719,\"start\":27716},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28088,\"start\":28085},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":28091,\"start\":28088},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28094,\"start\":28091},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28121,\"start\":28117},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":28124,\"start\":28121},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28191,\"start\":28187},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28194,\"start\":28191},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28197,\"start\":28194},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28200,\"start\":28197},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28203,\"start\":28200},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28344,\"start\":28341},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28367,\"start\":28363},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":28370,\"start\":28367},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28409,\"start\":28405},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28505,\"start\":28501},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28507,\"start\":28505},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28510,\"start\":28507},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28513,\"start\":28510},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28527,\"start\":28523},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28529,\"start\":28527},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28531,\"start\":28529},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28746,\"start\":28742},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29014,\"start\":29010},{\"end\":30186,\"start\":30183},{\"end\":30197,\"start\":30194},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30312,\"start\":30309},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30544,\"start\":30540},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36981,\"start\":36977}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37744,\"start\":37554},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37798,\"start\":37745},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38182,\"start\":37799},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38353,\"start\":38183},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38433,\"start\":38354},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38722,\"start\":38434},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38974,\"start\":38723},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39037,\"start\":38975},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39317,\"start\":39038},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39706,\"start\":39318},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39817,\"start\":39707},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39993,\"start\":39818},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40323,\"start\":39994},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":40582,\"start\":40324},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40954,\"start\":40583},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41771,\"start\":40955},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41826,\"start\":41772},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41886,\"start\":41827}]", "paragraph": "[{\"end\":2866,\"start\":1707},{\"end\":3745,\"start\":2868},{\"end\":4426,\"start\":3747},{\"end\":4485,\"start\":4428},{\"end\":5356,\"start\":4487},{\"end\":5669,\"start\":5422},{\"end\":5675,\"start\":5671},{\"end\":6380,\"start\":5686},{\"end\":7218,\"start\":6442},{\"end\":7992,\"start\":7220},{\"end\":8579,\"start\":7994},{\"end\":9178,\"start\":8628},{\"end\":9940,\"start\":9180},{\"end\":10324,\"start\":9942},{\"end\":10597,\"start\":10393},{\"end\":11214,\"start\":10599},{\"end\":11919,\"start\":11216},{\"end\":12180,\"start\":11921},{\"end\":12486,\"start\":12249},{\"end\":13015,\"start\":12527},{\"end\":13349,\"start\":13118},{\"end\":14006,\"start\":13378},{\"end\":14364,\"start\":14008},{\"end\":14434,\"start\":14366},{\"end\":14895,\"start\":14436},{\"end\":15579,\"start\":14897},{\"end\":15746,\"start\":15651},{\"end\":16229,\"start\":15766},{\"end\":16786,\"start\":16282},{\"end\":16986,\"start\":16888},{\"end\":17258,\"start\":17088},{\"end\":17762,\"start\":17283},{\"end\":18124,\"start\":17764},{\"end\":18465,\"start\":18190},{\"end\":18703,\"start\":18467},{\"end\":18981,\"start\":18748},{\"end\":19230,\"start\":18983},{\"end\":20057,\"start\":19232},{\"end\":20480,\"start\":20095},{\"end\":21501,\"start\":20482},{\"end\":21738,\"start\":21517},{\"end\":22521,\"start\":21740},{\"end\":23177,\"start\":22523},{\"end\":24431,\"start\":23179},{\"end\":26582,\"start\":24433},{\"end\":27131,\"start\":26584},{\"end\":27311,\"start\":27133},{\"end\":28164,\"start\":27328},{\"end\":29232,\"start\":28166},{\"end\":29808,\"start\":29247},{\"end\":30606,\"start\":29862},{\"end\":31042,\"start\":30956},{\"end\":31441,\"start\":31197},{\"end\":31599,\"start\":31443},{\"end\":31841,\"start\":31689},{\"end\":33608,\"start\":31882},{\"end\":34074,\"start\":33642},{\"end\":34796,\"start\":34076},{\"end\":34937,\"start\":34798},{\"end\":35380,\"start\":34994},{\"end\":35619,\"start\":35412},{\"end\":36680,\"start\":35621},{\"end\":37553,\"start\":36682}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5685,\"start\":5676},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10392,\"start\":10325},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12248,\"start\":12181},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13117,\"start\":13016},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13377,\"start\":13350},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15650,\"start\":15580},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15765,\"start\":15747},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16281,\"start\":16230},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16887,\"start\":16787},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17087,\"start\":16987},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18189,\"start\":18125},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18747,\"start\":18704},{\"attributes\":{\"id\":\"formula_13\"},\"end\":30703,\"start\":30607},{\"attributes\":{\"id\":\"formula_14\"},\"end\":30842,\"start\":30703},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30955,\"start\":30842},{\"attributes\":{\"id\":\"formula_16\"},\"end\":31196,\"start\":31043},{\"attributes\":{\"id\":\"formula_17\"},\"end\":31688,\"start\":31600}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23261,\"start\":23254},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23884,\"start\":23877},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24057,\"start\":24050},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26682,\"start\":26675},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35558,\"start\":35551},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35733,\"start\":35726},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":35912,\"start\":35905},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":37402,\"start\":37395},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":37414,\"start\":37407}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1705,\"start\":1693},{\"attributes\":{\"n\":\"2\"},\"end\":5398,\"start\":5359},{\"attributes\":{\"n\":\"2.1\"},\"end\":5420,\"start\":5401},{\"attributes\":{\"n\":\"2.2\"},\"end\":6440,\"start\":6383},{\"attributes\":{\"n\":\"2.3\"},\"end\":8626,\"start\":8582},{\"attributes\":{\"n\":\"2.4\"},\"end\":12525,\"start\":12489},{\"attributes\":{\"n\":\"2.5\"},\"end\":17281,\"start\":17261},{\"attributes\":{\"n\":\"2.6\"},\"end\":20093,\"start\":20060},{\"attributes\":{\"n\":\"3\"},\"end\":21515,\"start\":21504},{\"attributes\":{\"n\":\"4\"},\"end\":27326,\"start\":27314},{\"attributes\":{\"n\":\"5\"},\"end\":29245,\"start\":29235},{\"end\":29860,\"start\":29811},{\"end\":31880,\"start\":31844},{\"end\":33640,\"start\":33611},{\"end\":34966,\"start\":34940},{\"end\":34992,\"start\":34969},{\"end\":35410,\"start\":35383},{\"end\":37565,\"start\":37555},{\"end\":37756,\"start\":37746},{\"end\":38204,\"start\":38184},{\"end\":38365,\"start\":38355},{\"end\":38442,\"start\":38435},{\"end\":38986,\"start\":38976},{\"end\":39059,\"start\":39039},{\"end\":39328,\"start\":39319},{\"end\":39717,\"start\":39708},{\"end\":39828,\"start\":39819},{\"end\":40004,\"start\":39995},{\"end\":40334,\"start\":40325},{\"end\":40593,\"start\":40584},{\"end\":40965,\"start\":40956},{\"end\":41782,\"start\":41773},{\"end\":41837,\"start\":41828}]", "table": "[{\"end\":39706,\"start\":39611},{\"end\":39817,\"start\":39799},{\"end\":39993,\"start\":39891},{\"end\":40323,\"start\":40095},{\"end\":40582,\"start\":40414},{\"end\":40954,\"start\":40698},{\"end\":41771,\"start\":41046}]", "figure_caption": "[{\"end\":37744,\"start\":37567},{\"end\":37798,\"start\":37758},{\"end\":38182,\"start\":37801},{\"end\":38353,\"start\":38207},{\"end\":38433,\"start\":38367},{\"end\":38722,\"start\":38444},{\"end\":38974,\"start\":38725},{\"end\":39037,\"start\":38988},{\"end\":39317,\"start\":39062},{\"end\":39611,\"start\":39330},{\"end\":39799,\"start\":39719},{\"end\":39891,\"start\":39830},{\"end\":40095,\"start\":40006},{\"end\":40414,\"start\":40336},{\"end\":40698,\"start\":40595},{\"end\":41046,\"start\":40967},{\"end\":41826,\"start\":41784},{\"end\":41886,\"start\":41839}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7640,\"start\":7634},{\"end\":24754,\"start\":24748},{\"end\":25318,\"start\":25312},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26014,\"start\":26008},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":32108,\"start\":32102}]", "bib_author_first_name": "[{\"end\":41961,\"start\":41960},{\"end\":41968,\"start\":41967},{\"end\":41975,\"start\":41974},{\"end\":41984,\"start\":41983},{\"end\":41990,\"start\":41989},{\"end\":41997,\"start\":41996},{\"end\":42246,\"start\":42245},{\"end\":42256,\"start\":42255},{\"end\":42549,\"start\":42548},{\"end\":42559,\"start\":42558},{\"end\":42569,\"start\":42568},{\"end\":42927,\"start\":42926},{\"end\":42941,\"start\":42940},{\"end\":42953,\"start\":42952},{\"end\":43497,\"start\":43496},{\"end\":43505,\"start\":43504},{\"end\":43513,\"start\":43512},{\"end\":43521,\"start\":43520},{\"end\":43528,\"start\":43527},{\"end\":43537,\"start\":43536},{\"end\":43856,\"start\":43855},{\"end\":43864,\"start\":43863},{\"end\":43870,\"start\":43869},{\"end\":43872,\"start\":43871},{\"end\":44177,\"start\":44176},{\"end\":44185,\"start\":44184},{\"end\":44191,\"start\":44190},{\"end\":44193,\"start\":44192},{\"end\":44389,\"start\":44388},{\"end\":44397,\"start\":44396},{\"end\":44403,\"start\":44402},{\"end\":44409,\"start\":44408},{\"end\":44411,\"start\":44410},{\"end\":44425,\"start\":44424},{\"end\":44435,\"start\":44434},{\"end\":44442,\"start\":44441},{\"end\":44444,\"start\":44443},{\"end\":44835,\"start\":44834},{\"end\":45039,\"start\":45038},{\"end\":45047,\"start\":45046},{\"end\":45057,\"start\":45056},{\"end\":45069,\"start\":45068},{\"end\":45349,\"start\":45348},{\"end\":45356,\"start\":45355},{\"end\":45495,\"start\":45494},{\"end\":45497,\"start\":45496},{\"end\":45508,\"start\":45507},{\"end\":45517,\"start\":45516},{\"end\":45879,\"start\":45878},{\"end\":45889,\"start\":45888},{\"end\":45891,\"start\":45890},{\"end\":45902,\"start\":45901},{\"end\":46156,\"start\":46155},{\"end\":46170,\"start\":46169},{\"end\":46181,\"start\":46180},{\"end\":46191,\"start\":46190},{\"end\":46418,\"start\":46417},{\"end\":46428,\"start\":46427},{\"end\":46430,\"start\":46429},{\"end\":46444,\"start\":46443},{\"end\":46446,\"start\":46445},{\"end\":46455,\"start\":46454},{\"end\":46466,\"start\":46465},{\"end\":46468,\"start\":46467},{\"end\":46886,\"start\":46885},{\"end\":46898,\"start\":46897},{\"end\":46906,\"start\":46905},{\"end\":47171,\"start\":47170},{\"end\":47183,\"start\":47182},{\"end\":47191,\"start\":47190},{\"end\":47446,\"start\":47445},{\"end\":47460,\"start\":47459},{\"end\":47682,\"start\":47681},{\"end\":47688,\"start\":47687},{\"end\":47695,\"start\":47694},{\"end\":47701,\"start\":47700},{\"end\":47709,\"start\":47708},{\"end\":47975,\"start\":47974},{\"end\":47981,\"start\":47980},{\"end\":47988,\"start\":47987},{\"end\":47998,\"start\":47997},{\"end\":48006,\"start\":48005},{\"end\":48013,\"start\":48012},{\"end\":48020,\"start\":48019},{\"end\":48031,\"start\":48030},{\"end\":48323,\"start\":48322},{\"end\":48332,\"start\":48331},{\"end\":48340,\"start\":48339},{\"end\":48342,\"start\":48341},{\"end\":48351,\"start\":48350},{\"end\":48598,\"start\":48597},{\"end\":48607,\"start\":48606},{\"end\":48616,\"start\":48615},{\"end\":48623,\"start\":48622},{\"end\":48631,\"start\":48630},{\"end\":49048,\"start\":49047},{\"end\":49272,\"start\":49271},{\"end\":49286,\"start\":49285},{\"end\":49488,\"start\":49487},{\"end\":49496,\"start\":49495},{\"end\":49503,\"start\":49502},{\"end\":49505,\"start\":49504},{\"end\":49512,\"start\":49511},{\"end\":49715,\"start\":49714},{\"end\":49717,\"start\":49716},{\"end\":49727,\"start\":49726},{\"end\":49935,\"start\":49934},{\"end\":49937,\"start\":49936},{\"end\":49945,\"start\":49944},{\"end\":50193,\"start\":50192},{\"end\":50205,\"start\":50204},{\"end\":50219,\"start\":50218},{\"end\":50457,\"start\":50456},{\"end\":50469,\"start\":50468},{\"end\":50485,\"start\":50484},{\"end\":50752,\"start\":50751},{\"end\":50996,\"start\":50995},{\"end\":51002,\"start\":51001},{\"end\":51012,\"start\":51008},{\"end\":51300,\"start\":51299},{\"end\":51306,\"start\":51305},{\"end\":51314,\"start\":51313},{\"end\":51321,\"start\":51320},{\"end\":51540,\"start\":51539},{\"end\":51546,\"start\":51545},{\"end\":51556,\"start\":51555},{\"end\":51572,\"start\":51571},{\"end\":51814,\"start\":51813},{\"end\":51820,\"start\":51819},{\"end\":51830,\"start\":51829},{\"end\":51846,\"start\":51845},{\"end\":52100,\"start\":52099},{\"end\":52106,\"start\":52105},{\"end\":52117,\"start\":52116},{\"end\":52125,\"start\":52124},{\"end\":52136,\"start\":52135},{\"end\":52388,\"start\":52387},{\"end\":52396,\"start\":52395},{\"end\":52402,\"start\":52401},{\"end\":52410,\"start\":52409},{\"end\":52418,\"start\":52417},{\"end\":52430,\"start\":52429},{\"end\":52432,\"start\":52431},{\"end\":52444,\"start\":52443},{\"end\":52455,\"start\":52454},{\"end\":52814,\"start\":52813},{\"end\":52821,\"start\":52820},{\"end\":52830,\"start\":52829},{\"end\":52839,\"start\":52838},{\"end\":52847,\"start\":52846},{\"end\":52851,\"start\":52848},{\"end\":53117,\"start\":53116},{\"end\":53124,\"start\":53123},{\"end\":53133,\"start\":53129},{\"end\":53279,\"start\":53278},{\"end\":53341,\"start\":53340},{\"end\":53347,\"start\":53346},{\"end\":53355,\"start\":53354},{\"end\":53357,\"start\":53356},{\"end\":53369,\"start\":53368},{\"end\":53645,\"start\":53644},{\"end\":53647,\"start\":53646},{\"end\":53657,\"start\":53656},{\"end\":53946,\"start\":53945},{\"end\":53965,\"start\":53964},{\"end\":53976,\"start\":53975},{\"end\":54347,\"start\":54346},{\"end\":54355,\"start\":54354},{\"end\":54928,\"start\":54927},{\"end\":54939,\"start\":54938},{\"end\":54945,\"start\":54944},{\"end\":54956,\"start\":54955},{\"end\":55244,\"start\":55243},{\"end\":55251,\"start\":55250},{\"end\":55261,\"start\":55260},{\"end\":55271,\"start\":55270},{\"end\":55281,\"start\":55280},{\"end\":55294,\"start\":55293},{\"end\":55553,\"start\":55552},{\"end\":55560,\"start\":55559},{\"end\":55566,\"start\":55565},{\"end\":55573,\"start\":55572},{\"end\":55582,\"start\":55581},{\"end\":55591,\"start\":55590},{\"end\":55827,\"start\":55826},{\"end\":55838,\"start\":55837},{\"end\":55849,\"start\":55848},{\"end\":55859,\"start\":55858},{\"end\":55872,\"start\":55871},{\"end\":55881,\"start\":55880},{\"end\":55883,\"start\":55882},{\"end\":55892,\"start\":55891},{\"end\":55902,\"start\":55901},{\"end\":56174,\"start\":56173},{\"end\":56188,\"start\":56187},{\"end\":56200,\"start\":56199},{\"end\":56212,\"start\":56211},{\"end\":56222,\"start\":56221},{\"end\":56229,\"start\":56228},{\"end\":56479,\"start\":56478},{\"end\":56487,\"start\":56486},{\"end\":56495,\"start\":56494},{\"end\":56705,\"start\":56704},{\"end\":56714,\"start\":56713},{\"end\":57085,\"start\":57084},{\"end\":57091,\"start\":57090},{\"end\":57093,\"start\":57092},{\"end\":57105,\"start\":57104},{\"end\":57114,\"start\":57113},{\"end\":57120,\"start\":57119},{\"end\":57128,\"start\":57127},{\"end\":57136,\"start\":57135},{\"end\":57143,\"start\":57142},{\"end\":57642,\"start\":57641},{\"end\":57648,\"start\":57647},{\"end\":57654,\"start\":57653},{\"end\":57662,\"start\":57661},{\"end\":57672,\"start\":57671},{\"end\":57689,\"start\":57688},{\"end\":58064,\"start\":58063},{\"end\":58070,\"start\":58069},{\"end\":58076,\"start\":58075},{\"end\":58084,\"start\":58083},{\"end\":58090,\"start\":58089},{\"end\":58098,\"start\":58097},{\"end\":58380,\"start\":58376},{\"end\":58389,\"start\":58388},{\"end\":58651,\"start\":58650},{\"end\":58659,\"start\":58658},{\"end\":58666,\"start\":58665},{\"end\":58676,\"start\":58675},{\"end\":58683,\"start\":58682},{\"end\":58695,\"start\":58694},{\"end\":58976,\"start\":58975},{\"end\":58983,\"start\":58982},{\"end\":58991,\"start\":58990},{\"end\":58998,\"start\":58997},{\"end\":59000,\"start\":58999},{\"end\":59012,\"start\":59011},{\"end\":59296,\"start\":59295},{\"end\":59302,\"start\":59301},{\"end\":59310,\"start\":59309},{\"end\":59317,\"start\":59316},{\"end\":59576,\"start\":59575},{\"end\":59583,\"start\":59582},{\"end\":59592,\"start\":59591},{\"end\":59599,\"start\":59598},{\"end\":59607,\"start\":59606},{\"end\":59609,\"start\":59608},{\"end\":59906,\"start\":59905},{\"end\":59915,\"start\":59914},{\"end\":59922,\"start\":59921},{\"end\":59932,\"start\":59931},{\"end\":60349,\"start\":60348},{\"end\":60358,\"start\":60357},{\"end\":60368,\"start\":60367},{\"end\":60370,\"start\":60369},{\"end\":60383,\"start\":60382},{\"end\":60385,\"start\":60384}]", "bib_author_last_name": "[{\"end\":41965,\"start\":41962},{\"end\":41972,\"start\":41969},{\"end\":41981,\"start\":41976},{\"end\":41987,\"start\":41985},{\"end\":41994,\"start\":41991},{\"end\":42003,\"start\":41998},{\"end\":42253,\"start\":42247},{\"end\":42263,\"start\":42257},{\"end\":42556,\"start\":42550},{\"end\":42566,\"start\":42560},{\"end\":42574,\"start\":42570},{\"end\":42938,\"start\":42928},{\"end\":42950,\"start\":42942},{\"end\":42963,\"start\":42954},{\"end\":43502,\"start\":43498},{\"end\":43510,\"start\":43506},{\"end\":43518,\"start\":43514},{\"end\":43525,\"start\":43522},{\"end\":43534,\"start\":43529},{\"end\":43540,\"start\":43538},{\"end\":43861,\"start\":43857},{\"end\":43867,\"start\":43865},{\"end\":43877,\"start\":43873},{\"end\":44182,\"start\":44178},{\"end\":44188,\"start\":44186},{\"end\":44198,\"start\":44194},{\"end\":44394,\"start\":44390},{\"end\":44400,\"start\":44398},{\"end\":44406,\"start\":44404},{\"end\":44422,\"start\":44412},{\"end\":44432,\"start\":44426},{\"end\":44439,\"start\":44436},{\"end\":44448,\"start\":44445},{\"end\":44843,\"start\":44836},{\"end\":45044,\"start\":45040},{\"end\":45054,\"start\":45048},{\"end\":45066,\"start\":45058},{\"end\":45083,\"start\":45070},{\"end\":45353,\"start\":45350},{\"end\":45362,\"start\":45357},{\"end\":45505,\"start\":45498},{\"end\":45514,\"start\":45509},{\"end\":45524,\"start\":45518},{\"end\":45886,\"start\":45880},{\"end\":45899,\"start\":45892},{\"end\":45907,\"start\":45903},{\"end\":46167,\"start\":46157},{\"end\":46178,\"start\":46171},{\"end\":46188,\"start\":46182},{\"end\":46194,\"start\":46192},{\"end\":46425,\"start\":46419},{\"end\":46441,\"start\":46431},{\"end\":46452,\"start\":46447},{\"end\":46463,\"start\":46456},{\"end\":46473,\"start\":46469},{\"end\":46895,\"start\":46887},{\"end\":46903,\"start\":46899},{\"end\":46915,\"start\":46907},{\"end\":47180,\"start\":47172},{\"end\":47188,\"start\":47184},{\"end\":47200,\"start\":47192},{\"end\":47457,\"start\":47447},{\"end\":47472,\"start\":47461},{\"end\":47685,\"start\":47683},{\"end\":47692,\"start\":47689},{\"end\":47698,\"start\":47696},{\"end\":47706,\"start\":47702},{\"end\":47713,\"start\":47710},{\"end\":47978,\"start\":47976},{\"end\":47985,\"start\":47982},{\"end\":47995,\"start\":47989},{\"end\":48003,\"start\":47999},{\"end\":48010,\"start\":48007},{\"end\":48017,\"start\":48014},{\"end\":48028,\"start\":48021},{\"end\":48040,\"start\":48032},{\"end\":48329,\"start\":48324},{\"end\":48337,\"start\":48333},{\"end\":48348,\"start\":48343},{\"end\":48354,\"start\":48352},{\"end\":48604,\"start\":48599},{\"end\":48613,\"start\":48608},{\"end\":48620,\"start\":48617},{\"end\":48628,\"start\":48624},{\"end\":48635,\"start\":48632},{\"end\":49059,\"start\":49049},{\"end\":49283,\"start\":49273},{\"end\":49296,\"start\":49287},{\"end\":49493,\"start\":49489},{\"end\":49500,\"start\":49497},{\"end\":49509,\"start\":49506},{\"end\":49515,\"start\":49513},{\"end\":49724,\"start\":49718},{\"end\":49730,\"start\":49728},{\"end\":49942,\"start\":49938},{\"end\":49953,\"start\":49946},{\"end\":50202,\"start\":50194},{\"end\":50216,\"start\":50206},{\"end\":50229,\"start\":50220},{\"end\":50466,\"start\":50458},{\"end\":50482,\"start\":50470},{\"end\":50495,\"start\":50486},{\"end\":50757,\"start\":50753},{\"end\":50999,\"start\":50997},{\"end\":51006,\"start\":51003},{\"end\":51015,\"start\":51013},{\"end\":51303,\"start\":51301},{\"end\":51311,\"start\":51307},{\"end\":51318,\"start\":51315},{\"end\":51327,\"start\":51322},{\"end\":51543,\"start\":51541},{\"end\":51553,\"start\":51547},{\"end\":51569,\"start\":51557},{\"end\":51578,\"start\":51573},{\"end\":51817,\"start\":51815},{\"end\":51827,\"start\":51821},{\"end\":51843,\"start\":51831},{\"end\":51852,\"start\":51847},{\"end\":52103,\"start\":52101},{\"end\":52114,\"start\":52107},{\"end\":52122,\"start\":52118},{\"end\":52133,\"start\":52126},{\"end\":52146,\"start\":52137},{\"end\":52393,\"start\":52389},{\"end\":52399,\"start\":52397},{\"end\":52407,\"start\":52403},{\"end\":52415,\"start\":52411},{\"end\":52427,\"start\":52419},{\"end\":52441,\"start\":52433},{\"end\":52452,\"start\":52445},{\"end\":52461,\"start\":52456},{\"end\":52818,\"start\":52815},{\"end\":52827,\"start\":52822},{\"end\":52836,\"start\":52831},{\"end\":52844,\"start\":52840},{\"end\":52858,\"start\":52852},{\"end\":53121,\"start\":53118},{\"end\":53127,\"start\":53125},{\"end\":53139,\"start\":53134},{\"end\":53286,\"start\":53280},{\"end\":53344,\"start\":53342},{\"end\":53352,\"start\":53348},{\"end\":53366,\"start\":53358},{\"end\":53374,\"start\":53370},{\"end\":53654,\"start\":53648},{\"end\":53661,\"start\":53658},{\"end\":53962,\"start\":53947},{\"end\":53973,\"start\":53966},{\"end\":53984,\"start\":53977},{\"end\":54352,\"start\":54348},{\"end\":54359,\"start\":54356},{\"end\":54936,\"start\":54929},{\"end\":54942,\"start\":54940},{\"end\":54953,\"start\":54946},{\"end\":54972,\"start\":54957},{\"end\":55248,\"start\":55245},{\"end\":55258,\"start\":55252},{\"end\":55268,\"start\":55262},{\"end\":55278,\"start\":55272},{\"end\":55291,\"start\":55282},{\"end\":55306,\"start\":55295},{\"end\":55557,\"start\":55554},{\"end\":55563,\"start\":55561},{\"end\":55570,\"start\":55567},{\"end\":55579,\"start\":55574},{\"end\":55588,\"start\":55583},{\"end\":55596,\"start\":55592},{\"end\":55835,\"start\":55828},{\"end\":55846,\"start\":55839},{\"end\":55856,\"start\":55850},{\"end\":55869,\"start\":55860},{\"end\":55878,\"start\":55873},{\"end\":55889,\"start\":55884},{\"end\":55899,\"start\":55893},{\"end\":55913,\"start\":55903},{\"end\":56185,\"start\":56175},{\"end\":56197,\"start\":56189},{\"end\":56209,\"start\":56201},{\"end\":56219,\"start\":56213},{\"end\":56226,\"start\":56223},{\"end\":56236,\"start\":56230},{\"end\":56484,\"start\":56480},{\"end\":56492,\"start\":56488},{\"end\":56500,\"start\":56496},{\"end\":56711,\"start\":56706},{\"end\":56721,\"start\":56715},{\"end\":57088,\"start\":57086},{\"end\":57102,\"start\":57094},{\"end\":57111,\"start\":57106},{\"end\":57117,\"start\":57115},{\"end\":57125,\"start\":57121},{\"end\":57133,\"start\":57129},{\"end\":57140,\"start\":57137},{\"end\":57152,\"start\":57144},{\"end\":57645,\"start\":57643},{\"end\":57651,\"start\":57649},{\"end\":57659,\"start\":57655},{\"end\":57669,\"start\":57663},{\"end\":57686,\"start\":57673},{\"end\":57697,\"start\":57690},{\"end\":58067,\"start\":58065},{\"end\":58073,\"start\":58071},{\"end\":58081,\"start\":58077},{\"end\":58087,\"start\":58085},{\"end\":58095,\"start\":58091},{\"end\":58106,\"start\":58099},{\"end\":58386,\"start\":58381},{\"end\":58395,\"start\":58390},{\"end\":58656,\"start\":58652},{\"end\":58663,\"start\":58660},{\"end\":58673,\"start\":58667},{\"end\":58680,\"start\":58677},{\"end\":58692,\"start\":58684},{\"end\":58704,\"start\":58696},{\"end\":58980,\"start\":58977},{\"end\":58988,\"start\":58984},{\"end\":58995,\"start\":58992},{\"end\":59009,\"start\":59001},{\"end\":59021,\"start\":59013},{\"end\":59299,\"start\":59297},{\"end\":59307,\"start\":59303},{\"end\":59314,\"start\":59311},{\"end\":59320,\"start\":59318},{\"end\":59580,\"start\":59577},{\"end\":59589,\"start\":59584},{\"end\":59596,\"start\":59593},{\"end\":59604,\"start\":59600},{\"end\":59613,\"start\":59610},{\"end\":59912,\"start\":59907},{\"end\":59919,\"start\":59916},{\"end\":59929,\"start\":59923},{\"end\":59940,\"start\":59933},{\"end\":60355,\"start\":60350},{\"end\":60365,\"start\":60359},{\"end\":60380,\"start\":60371},{\"end\":60390,\"start\":60386}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":214286343},\"end\":42169,\"start\":41888},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17572432},\"end\":42489,\"start\":42171},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":210994987},\"end\":42825,\"start\":42491},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1062907},\"end\":43416,\"start\":42827},{\"attributes\":{\"doi\":\"arXiv:1907.04707\",\"id\":\"b4\"},\"end\":43744,\"start\":43418},{\"attributes\":{\"doi\":\"arXiv:1908.00059\",\"id\":\"b5\"},\"end\":44088,\"start\":43746},{\"attributes\":{\"doi\":\"arXiv:1908.04942\",\"id\":\"b6\"},\"end\":44386,\"start\":44090},{\"attributes\":{\"doi\":\"arXiv:1906.04716\",\"id\":\"b7\"},\"end\":44785,\"start\":44388},{\"attributes\":{\"doi\":\"arXiv:1202.3713\",\"id\":\"b8\"},\"end\":44970,\"start\":44787},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11062262},\"end\":45313,\"start\":44972},{\"attributes\":{\"id\":\"b10\"},\"end\":45423,\"start\":45315},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13089886},\"end\":45766,\"start\":45425},{\"attributes\":{\"id\":\"b12\"},\"end\":46097,\"start\":45768},{\"attributes\":{\"doi\":\"arXiv:1903.11960\",\"id\":\"b13\"},\"end\":46369,\"start\":46099},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9665943},\"end\":46832,\"start\":46371},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4755450},\"end\":47117,\"start\":46834},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4755450},\"end\":47419,\"start\":47119},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1915014},\"end\":47596,\"start\":47421},{\"attributes\":{\"doi\":\"arXiv:1902.06667\",\"id\":\"b18\"},\"end\":47909,\"start\":47598},{\"attributes\":{\"doi\":\"arXiv:2005.00687\",\"id\":\"b19\"},\"end\":48253,\"start\":47911},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":58952568},\"end\":48527,\"start\":48255},{\"attributes\":{\"id\":\"b21\"},\"end\":49003,\"start\":48529},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10385053},\"end\":49221,\"start\":49005},{\"attributes\":{\"doi\":\"arXiv:1710.05654\",\"id\":\"b23\"},\"end\":49446,\"start\":49223},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":56208557},\"end\":49668,\"start\":49448},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b25\"},\"end\":49866,\"start\":49670},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b26\"},\"end\":50116,\"start\":49868},{\"attributes\":{\"doi\":\"arXiv:1810.05997\",\"id\":\"b27\"},\"end\":50419,\"start\":50118},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":202783932},\"end\":50709,\"start\":50421},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1921714},\"end\":50914,\"start\":50711},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11118105},\"end\":51251,\"start\":50916},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1415308},\"end\":51537,\"start\":51253},{\"attributes\":{\"doi\":\"arXiv:1511.05493\",\"id\":\"b32\"},\"end\":51773,\"start\":51539},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8393918},\"end\":52054,\"start\":51775},{\"attributes\":{\"doi\":\"arXiv:1803.03324\",\"id\":\"b34\"},\"end\":52317,\"start\":52056},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":202783404},\"end\":52747,\"start\":52319},{\"attributes\":{\"doi\":\"arXiv:1811.08600\",\"id\":\"b36\"},\"end\":53050,\"start\":52749},{\"attributes\":{\"id\":\"b37\"},\"end\":53242,\"start\":53052},{\"attributes\":{\"id\":\"b38\"},\"end\":53338,\"start\":53244},{\"attributes\":{\"doi\":\"arXiv:1904.13107\",\"id\":\"b39\"},\"end\":53585,\"start\":53340},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":18453384},\"end\":53860,\"start\":53587},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":49317766},\"end\":54244,\"start\":53862},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":388},\"end\":54826,\"start\":54246},{\"attributes\":{\"doi\":\"arXiv:1802.05283\",\"id\":\"b43\"},\"end\":55198,\"start\":54828},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":62016134},\"end\":55475,\"start\":55200},{\"attributes\":{\"doi\":\"arXiv:2001.09382\",\"id\":\"b45\"},\"end\":55797,\"start\":55477},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":13756489},\"end\":56171,\"start\":55799},{\"attributes\":{\"doi\":\"arXiv:1710.10903\",\"id\":\"b47\"},\"end\":56433,\"start\":56173},{\"attributes\":{\"doi\":\"arXiv:1911.10373\",\"id\":\"b48\"},\"end\":56644,\"start\":56435},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":19209388},\"end\":56980,\"start\":56646},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":196179158},\"end\":57639,\"start\":56982},{\"attributes\":{\"doi\":\"arXiv:1806.03536\",\"id\":\"b51\"},\"end\":57972,\"start\":57641},{\"attributes\":{\"doi\":\"arXiv:1808.07624\",\"id\":\"b52\"},\"end\":58320,\"start\":57974},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":10589121},\"end\":58576,\"start\":58322},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":49420315},\"end\":58973,\"start\":58578},{\"attributes\":{\"doi\":\"arXiv:1802.08773\",\"id\":\"b55\"},\"end\":59293,\"start\":58975},{\"attributes\":{\"doi\":\"arXiv:1904.10098\",\"id\":\"b56\"},\"end\":59545,\"start\":59295},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":202763464},\"end\":59822,\"start\":59547},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":53786372},\"end\":60278,\"start\":59824},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":53217974},\"end\":60641,\"start\":60280}]", "bib_title": "[{\"end\":41958,\"start\":41888},{\"end\":42243,\"start\":42171},{\"end\":42546,\"start\":42491},{\"end\":42924,\"start\":42827},{\"end\":45036,\"start\":44972},{\"end\":45492,\"start\":45425},{\"end\":46415,\"start\":46371},{\"end\":46883,\"start\":46834},{\"end\":47168,\"start\":47119},{\"end\":47443,\"start\":47421},{\"end\":48320,\"start\":48255},{\"end\":48595,\"start\":48529},{\"end\":49045,\"start\":49005},{\"end\":49485,\"start\":49448},{\"end\":50454,\"start\":50421},{\"end\":50749,\"start\":50711},{\"end\":50993,\"start\":50916},{\"end\":51297,\"start\":51253},{\"end\":51811,\"start\":51775},{\"end\":52385,\"start\":52319},{\"end\":53642,\"start\":53587},{\"end\":53943,\"start\":53862},{\"end\":54344,\"start\":54246},{\"end\":55241,\"start\":55200},{\"end\":55824,\"start\":55799},{\"end\":56702,\"start\":56646},{\"end\":57082,\"start\":56982},{\"end\":58374,\"start\":58322},{\"end\":58648,\"start\":58578},{\"end\":59573,\"start\":59547},{\"end\":59903,\"start\":59824},{\"end\":60346,\"start\":60280}]", "bib_author": "[{\"end\":41967,\"start\":41960},{\"end\":41974,\"start\":41967},{\"end\":41983,\"start\":41974},{\"end\":41989,\"start\":41983},{\"end\":41996,\"start\":41989},{\"end\":42005,\"start\":41996},{\"end\":42255,\"start\":42245},{\"end\":42265,\"start\":42255},{\"end\":42558,\"start\":42548},{\"end\":42568,\"start\":42558},{\"end\":42576,\"start\":42568},{\"end\":42940,\"start\":42926},{\"end\":42952,\"start\":42940},{\"end\":42965,\"start\":42952},{\"end\":43504,\"start\":43496},{\"end\":43512,\"start\":43504},{\"end\":43520,\"start\":43512},{\"end\":43527,\"start\":43520},{\"end\":43536,\"start\":43527},{\"end\":43542,\"start\":43536},{\"end\":43863,\"start\":43855},{\"end\":43869,\"start\":43863},{\"end\":43879,\"start\":43869},{\"end\":44184,\"start\":44176},{\"end\":44190,\"start\":44184},{\"end\":44200,\"start\":44190},{\"end\":44396,\"start\":44388},{\"end\":44402,\"start\":44396},{\"end\":44408,\"start\":44402},{\"end\":44424,\"start\":44408},{\"end\":44434,\"start\":44424},{\"end\":44441,\"start\":44434},{\"end\":44450,\"start\":44441},{\"end\":44845,\"start\":44834},{\"end\":45046,\"start\":45038},{\"end\":45056,\"start\":45046},{\"end\":45068,\"start\":45056},{\"end\":45085,\"start\":45068},{\"end\":45355,\"start\":45348},{\"end\":45364,\"start\":45355},{\"end\":45507,\"start\":45494},{\"end\":45516,\"start\":45507},{\"end\":45526,\"start\":45516},{\"end\":45888,\"start\":45878},{\"end\":45901,\"start\":45888},{\"end\":45909,\"start\":45901},{\"end\":46169,\"start\":46155},{\"end\":46180,\"start\":46169},{\"end\":46190,\"start\":46180},{\"end\":46196,\"start\":46190},{\"end\":46427,\"start\":46417},{\"end\":46443,\"start\":46427},{\"end\":46454,\"start\":46443},{\"end\":46465,\"start\":46454},{\"end\":46475,\"start\":46465},{\"end\":46897,\"start\":46885},{\"end\":46905,\"start\":46897},{\"end\":46917,\"start\":46905},{\"end\":47182,\"start\":47170},{\"end\":47190,\"start\":47182},{\"end\":47202,\"start\":47190},{\"end\":47459,\"start\":47445},{\"end\":47474,\"start\":47459},{\"end\":47687,\"start\":47681},{\"end\":47694,\"start\":47687},{\"end\":47700,\"start\":47694},{\"end\":47708,\"start\":47700},{\"end\":47715,\"start\":47708},{\"end\":47980,\"start\":47974},{\"end\":47987,\"start\":47980},{\"end\":47997,\"start\":47987},{\"end\":48005,\"start\":47997},{\"end\":48012,\"start\":48005},{\"end\":48019,\"start\":48012},{\"end\":48030,\"start\":48019},{\"end\":48042,\"start\":48030},{\"end\":48331,\"start\":48322},{\"end\":48339,\"start\":48331},{\"end\":48350,\"start\":48339},{\"end\":48356,\"start\":48350},{\"end\":48606,\"start\":48597},{\"end\":48615,\"start\":48606},{\"end\":48622,\"start\":48615},{\"end\":48630,\"start\":48622},{\"end\":48637,\"start\":48630},{\"end\":49061,\"start\":49047},{\"end\":49285,\"start\":49271},{\"end\":49298,\"start\":49285},{\"end\":49495,\"start\":49487},{\"end\":49502,\"start\":49495},{\"end\":49511,\"start\":49502},{\"end\":49517,\"start\":49511},{\"end\":49726,\"start\":49714},{\"end\":49732,\"start\":49726},{\"end\":49944,\"start\":49934},{\"end\":49955,\"start\":49944},{\"end\":50204,\"start\":50192},{\"end\":50218,\"start\":50204},{\"end\":50231,\"start\":50218},{\"end\":50468,\"start\":50456},{\"end\":50484,\"start\":50468},{\"end\":50497,\"start\":50484},{\"end\":50759,\"start\":50751},{\"end\":51001,\"start\":50995},{\"end\":51008,\"start\":51001},{\"end\":51017,\"start\":51008},{\"end\":51305,\"start\":51299},{\"end\":51313,\"start\":51305},{\"end\":51320,\"start\":51313},{\"end\":51329,\"start\":51320},{\"end\":51545,\"start\":51539},{\"end\":51555,\"start\":51545},{\"end\":51571,\"start\":51555},{\"end\":51580,\"start\":51571},{\"end\":51819,\"start\":51813},{\"end\":51829,\"start\":51819},{\"end\":51845,\"start\":51829},{\"end\":51854,\"start\":51845},{\"end\":52105,\"start\":52099},{\"end\":52116,\"start\":52105},{\"end\":52124,\"start\":52116},{\"end\":52135,\"start\":52124},{\"end\":52148,\"start\":52135},{\"end\":52395,\"start\":52387},{\"end\":52401,\"start\":52395},{\"end\":52409,\"start\":52401},{\"end\":52417,\"start\":52409},{\"end\":52429,\"start\":52417},{\"end\":52443,\"start\":52429},{\"end\":52454,\"start\":52443},{\"end\":52463,\"start\":52454},{\"end\":52820,\"start\":52813},{\"end\":52829,\"start\":52820},{\"end\":52838,\"start\":52829},{\"end\":52846,\"start\":52838},{\"end\":52860,\"start\":52846},{\"end\":53123,\"start\":53116},{\"end\":53129,\"start\":53123},{\"end\":53141,\"start\":53129},{\"end\":53288,\"start\":53278},{\"end\":53346,\"start\":53340},{\"end\":53354,\"start\":53346},{\"end\":53368,\"start\":53354},{\"end\":53376,\"start\":53368},{\"end\":53656,\"start\":53644},{\"end\":53663,\"start\":53656},{\"end\":53964,\"start\":53945},{\"end\":53975,\"start\":53964},{\"end\":53986,\"start\":53975},{\"end\":54354,\"start\":54346},{\"end\":54361,\"start\":54354},{\"end\":54938,\"start\":54927},{\"end\":54944,\"start\":54938},{\"end\":54955,\"start\":54944},{\"end\":54974,\"start\":54955},{\"end\":55250,\"start\":55243},{\"end\":55260,\"start\":55250},{\"end\":55270,\"start\":55260},{\"end\":55280,\"start\":55270},{\"end\":55293,\"start\":55280},{\"end\":55308,\"start\":55293},{\"end\":55559,\"start\":55552},{\"end\":55565,\"start\":55559},{\"end\":55572,\"start\":55565},{\"end\":55581,\"start\":55572},{\"end\":55590,\"start\":55581},{\"end\":55598,\"start\":55590},{\"end\":55837,\"start\":55826},{\"end\":55848,\"start\":55837},{\"end\":55858,\"start\":55848},{\"end\":55871,\"start\":55858},{\"end\":55880,\"start\":55871},{\"end\":55891,\"start\":55880},{\"end\":55901,\"start\":55891},{\"end\":55915,\"start\":55901},{\"end\":56187,\"start\":56173},{\"end\":56199,\"start\":56187},{\"end\":56211,\"start\":56199},{\"end\":56221,\"start\":56211},{\"end\":56228,\"start\":56221},{\"end\":56238,\"start\":56228},{\"end\":56486,\"start\":56478},{\"end\":56494,\"start\":56486},{\"end\":56502,\"start\":56494},{\"end\":56713,\"start\":56704},{\"end\":56723,\"start\":56713},{\"end\":57090,\"start\":57084},{\"end\":57104,\"start\":57090},{\"end\":57113,\"start\":57104},{\"end\":57119,\"start\":57113},{\"end\":57127,\"start\":57119},{\"end\":57135,\"start\":57127},{\"end\":57142,\"start\":57135},{\"end\":57154,\"start\":57142},{\"end\":57647,\"start\":57641},{\"end\":57653,\"start\":57647},{\"end\":57661,\"start\":57653},{\"end\":57671,\"start\":57661},{\"end\":57688,\"start\":57671},{\"end\":57699,\"start\":57688},{\"end\":58069,\"start\":58063},{\"end\":58075,\"start\":58069},{\"end\":58083,\"start\":58075},{\"end\":58089,\"start\":58083},{\"end\":58097,\"start\":58089},{\"end\":58108,\"start\":58097},{\"end\":58388,\"start\":58376},{\"end\":58397,\"start\":58388},{\"end\":58658,\"start\":58650},{\"end\":58665,\"start\":58658},{\"end\":58675,\"start\":58665},{\"end\":58682,\"start\":58675},{\"end\":58694,\"start\":58682},{\"end\":58706,\"start\":58694},{\"end\":58982,\"start\":58975},{\"end\":58990,\"start\":58982},{\"end\":58997,\"start\":58990},{\"end\":59011,\"start\":58997},{\"end\":59023,\"start\":59011},{\"end\":59301,\"start\":59295},{\"end\":59309,\"start\":59301},{\"end\":59316,\"start\":59309},{\"end\":59322,\"start\":59316},{\"end\":59582,\"start\":59575},{\"end\":59591,\"start\":59582},{\"end\":59598,\"start\":59591},{\"end\":59606,\"start\":59598},{\"end\":59615,\"start\":59606},{\"end\":59914,\"start\":59905},{\"end\":59921,\"start\":59914},{\"end\":59931,\"start\":59921},{\"end\":59942,\"start\":59931},{\"end\":60357,\"start\":60348},{\"end\":60367,\"start\":60357},{\"end\":60382,\"start\":60367},{\"end\":60392,\"start\":60382}]", "bib_venue": "[{\"end\":43148,\"start\":43065},{\"end\":46598,\"start\":46545},{\"end\":48778,\"start\":48716},{\"end\":54514,\"start\":54446},{\"end\":57333,\"start\":57252},{\"end\":60051,\"start\":60005},{\"end\":42019,\"start\":42005},{\"end\":42314,\"start\":42265},{\"end\":42644,\"start\":42576},{\"end\":43063,\"start\":42965},{\"end\":43494,\"start\":43418},{\"end\":43853,\"start\":43746},{\"end\":44174,\"start\":44090},{\"end\":44560,\"start\":44466},{\"end\":44832,\"start\":44787},{\"end\":45123,\"start\":45085},{\"end\":45346,\"start\":45315},{\"end\":45578,\"start\":45526},{\"end\":45876,\"start\":45768},{\"end\":46153,\"start\":46099},{\"end\":46543,\"start\":46475},{\"end\":46966,\"start\":46917},{\"end\":47251,\"start\":47202},{\"end\":47492,\"start\":47474},{\"end\":47679,\"start\":47598},{\"end\":47972,\"start\":47911},{\"end\":48375,\"start\":48356},{\"end\":48714,\"start\":48637},{\"end\":49099,\"start\":49061},{\"end\":49269,\"start\":49223},{\"end\":49549,\"start\":49517},{\"end\":49712,\"start\":49670},{\"end\":49932,\"start\":49868},{\"end\":50190,\"start\":50118},{\"end\":50546,\"start\":50497},{\"end\":50787,\"start\":50759},{\"end\":51073,\"start\":51017},{\"end\":51385,\"start\":51329},{\"end\":51632,\"start\":51596},{\"end\":51906,\"start\":51854},{\"end\":52097,\"start\":52056},{\"end\":52512,\"start\":52463},{\"end\":52811,\"start\":52749},{\"end\":53114,\"start\":53052},{\"end\":53276,\"start\":53244},{\"end\":53438,\"start\":53392},{\"end\":53698,\"start\":53663},{\"end\":54035,\"start\":53986},{\"end\":54444,\"start\":54361},{\"end\":54925,\"start\":54828},{\"end\":55319,\"start\":55308},{\"end\":55550,\"start\":55477},{\"end\":55964,\"start\":55915},{\"end\":56278,\"start\":56254},{\"end\":56476,\"start\":56435},{\"end\":56792,\"start\":56723},{\"end\":57250,\"start\":57154},{\"end\":57780,\"start\":57715},{\"end\":58061,\"start\":57974},{\"end\":58433,\"start\":58397},{\"end\":58755,\"start\":58706},{\"end\":59109,\"start\":59039},{\"end\":59396,\"start\":59338},{\"end\":59664,\"start\":59615},{\"end\":60003,\"start\":59942},{\"end\":60441,\"start\":60392}]"}}}, "year": 2023, "month": 12, "day": 17}