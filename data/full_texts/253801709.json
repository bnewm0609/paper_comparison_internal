{"id": 253801709, "updated": "2023-12-12 22:00:36.013", "metadata": {"title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks", "authors": "[{\"first\":\"Wenhu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xueguang\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Xinyi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"William\",\"last\":\"Cohen\",\"middle\":[\"W.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2211-12588", "doi": "10.48550/arxiv.2211.12588"}}, "content": {"source": {"pdf_hash": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.12588v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4fbf68b21ae025c4bf6855c374538b92d5ace33b", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691.txt", "contents": "\nProgram of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\n23 Oct 2023\n\nWenhu Chen wenhuchen@uwaterloo.ca \nUniversity of Waterloo \u00b6 Vector Institute\nToronto\n\nXueguang Ma \nUniversity of Waterloo \u00b6 Vector Institute\nToronto\n\nUniversity of California\nSanta Barabra \u2022 Google Research\n\n\nXinyi Wang xinyi_wang@ucsb.edu \nWilliam W Cohen wcohen@google.com \nWaterloo Wenhu Chen \nProgram of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\n23 Oct 20236C477219328264B3F15BCD3BCF462403arXiv:2211.12588v4[cs.CL]\nRecently, there has been significant progress in teaching language models to perform step-bystep reasoning to solve complex numerical reasoning tasks.Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks.CoT uses language models to produce text describing reasoning, and computation, and finally the answer to a question.Here we propose 'Program of Thoughts' (PoT), which uses language models (mainly Codex) to generate text and programming language statements, and finally an answer.In PoT, the computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding.We evaluate PoT on five math word problem datasets and three financial-QA datasets in both few-shot and zero-shot settings.We find that PoT has an average performance gain over CoT of around 12% across all datasets.By combining PoT with self-consistency decoding, we can achieve extremely strong performance on all the math datasets and financial datasets.All of our data and code will be released.\n\nIntroduction\n\nNumerical reasoning is a long-standing task in artificial intelligence.A surge of datasets has been proposed recently to benchmark deep-learning models' capabilities to perform numerical/arithmetic reasoning.Some widely used benchmarks are based on Math word problems (MWP) (Cobbe et al., 2021;Patel et al., 2021;Lu et al., 2022;Ling et al., 2017), where systems are supposed to answer math questions expressed with natural text.Besides MWP, some datasets also consider financial problems (Chen et al., 2021b;2022;Zhu et al., 2021), where systems need to answer math-driven financial questions.\n\nPrior work (Ling et al., 2017;Cobbe et al., 2021) has studied how to train models from scratch or fine-tune models to generate intermediate steps to derive the final answer.Such methods are data-intensive, requiring a significant number of training examples with expert-annotated steps.Recently, Nye et al. (2021) have discovered that the large language models (LLMs) (Brown et al., 2020;Chen et al., 2021a;Chowdhery et al., 2022) can be prompted with a few input-output exemplars to solve these tasks without any training or finetuning.In particular, when prompted with a few examples containing inputs, natural language 'rationales', and outputs, LLMs can imitate the demonstrations to both generate rationales and answer these questions.Such a prompting method is latter extended as 'Chain of Thoughts (CoT)' (Wei et al., 2022), and it is able to achieve state-of-the-art performance on a wide spectrum of textual and numerical reasoning datasets.CoT uses LLMs for both reasoning and computation, i.e. the language model not only needs to generate the mathematical expressions but also needs to perform the computation in each step.We argue that language models are not ideal for actually solving these mathematical expressions, because: 1) LLMs are very prone to arithmetic calculation errors, especially when dealing with large numbers; 2) LLMs cannot solve complex mathematical expressions like polynomial equations or even differential equations; 3) LLMs are highly inefficient at expressing iteration, especially when the number of iteration steps is large.\n\nIn order to solve these issues, we propose program-of-thoughts (PoT) prompting, which will delegate computation steps to an external language interpreter.In PoT, LMs can express reasoning steps as Python programs, and the computation can be accomplished by a Python interpreter.We depict the difference between CoT and PoT in Figure 1.In the upper example, for CoT the iteration runs for 50 times, which leads to extremely low accuracy;1 in the lower example, CoT cannot solve the cubic equation with language models and outputs a wrong answer.In contrast, in the upper example, PoT can express the iteration process with a few lines of code, which can be executed on a Python interpreter to derive an accurate answer; and in the lower example, PoT can convert the problem into a program that relies on 'SymPy' library in Python to solve the complex equation.\n\nWe evaluate PoT prompting across five MWP datasets, GSM8K, AQuA, SVAMP, TabMWP, MultiArith; and three financial datasets, FinQA, ConvFinQA, and TATQA.These datasets cover various input formats including text, tables, and conversation.We give an overview of the results in Figure 2.Under both fewshot and zero-shot settings, PoT outperforms CoT significantly across all the evaluated datasets.Under the few-shot setting, the average gain over CoT is around 8% for the MWP datasets and 15% for the financial datasets.Under the zero-shot setting, the average gain over CoT is around 12% for the MWP datasets.PoT combined with self-consistency (SC) also outperforms CoT+SC (Wang et al., 2022b) by an average of 10%  CoT-SC PoT-SC across all datasets.Our PoT+SC achieves the best-known results on all the evaluated MWP datasets and near best-known results on the financial datasets (excluding GPT-4 (OpenAI, 2023)).Finally, we conduct comprehensive ablation studies to understand the different components of PoT.\nG S M 8 K A Q u A S V A M P T a b M W P F in Q A C o n v F in T A T Q AG S M 8 K A Q u A S V A M P T a b M W P F in Q A C o n v F in T A T Q AG S M 8 K A Q u A S V A M P T a b M W P F in Q A C o n v F in T A T Q A\n\nProgram of Thoughts\n\n\nPreliminaries\n\nIn-context learning has been described in Brown et al. (2020); Chen et al. (2021a); Chowdhery et al. (2022);Rae et al. (2021).Compared with fine-tuning, in-context learning (1) only takes a few annotations/demonstrations as a prompt, and (2) performs inference without training the model parameters.With in-context learning, LLMs receive the input-output exemplars as the prefix, followed by an input problem, and generate outputs imitating the exemplars.More recently, 'chain of thoughts prompting' (Wei et al., 2022) has been proposed as a specific type of in-context learning where the exemplar's output contains the 'thought process' or rationale instead of just an output.This approach has been shown to elicit LLMs' strong reasoning capabilities on various kinds of tasks.\n\n\nProgram of Thoughts\n\nBesides natural language, programs can also be used to express our thought processes.By using semantically meaningful variable names, a program can also be a natural representation to convey human thoughts.For example, in the lower example in Figure 1, we first create an unknown variable named interest_rate.Then we bind 'summation in two years with ... interest rate' to the variable sum_in_two_years_with_XXX_interest and write down the equation expressing their mathematical relations with interest_rate.These equations are packaged into the 'solve' function provided by 'SymPy'.The program is executed with Python to solve the equations to derive the answer variable interest_rate.\n\nUnlike CoT, PoT relegates some computation to an external process (a Python interpreter).The LLMs are only responsible for expressing the 'reasoning process' in the programming language.In contrast, CoT aims to use LLMs to perform both reasoning and computation.We argue that such an approach is more expressive and accurate in terms of numerical reasoning.\n\nThe 'program of thoughts' is different from generating equations directly, where the generation target would be solve(20000\n* (1 + x) 3 \u2212 2000 \u2212 x * 20000 * 3 \u2212 1000, x).\nAs observed by Wei et al. (2022) for CoT, directly generating such equations is challenging for LLMs.PoT differs from equation generation in two aspects: (1) PoT breaks down the equation into a multi-step 'thought' process, and (2) PoT binds semantic meanings to variables to help ground the model in language.We found that this sort of 'thoughtful' process can elicit language models' reasoning capabilities and generate more accurate programs.We provide a detailed comparison in the experimental section.\n\nWe show the proposed PoT prompting method in Figure 3 under the few-shot and zero-shot settings.Under the few-shot setting, a few exemplars of (question, 'program of thoughts') pairs will be prefixed as demonstrations to teach the LLM how to generate 'thoughtful' programs.Under the zero-shot setting, the prompt only contains an instruction without any exemplar demonstration.Unlike zero-shot CoT (Kojima et al., 2022), which requires an extra step to extract the answer from the 'chain of thoughts', zero-shot PoT can return the answer straightforwardly without extra steps.\n\nIn zero-shot PoT, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program.Therefore, we propose to suppress '#' token logits to encourage it to generate programs.\n\n\nPoT as an Intermediate Step\n\nFor certain problems requiring additional textual reasoning, we propose to utilize PoT to tackle the computation part.The program generated by PoT can be executed to provide intermediate result, which is further combined with the question to derive the final answer with CoT.We depict the whole process in Figure 8.\n\nDuring demonstration, we present LLMs with examples to teach it predict whether to an additional CoT reasoning needs to be used.If LLM outputs 'keep prompting' in the end, we will adopt the execution results from PoT as input to further prompt LLMs to derive the answer through CoT.\n\nFor instance, in the left example in Figure 3, the program will be executed to return a float number 'ans=2.05',which means that after 2.05 hours the two trains will meet.However, directly adding 2.05 to 11 AM does not make sense because 2.05 hour needs to be translated to minutes to obtain the standard HH:MM time format to make it aligned with provided option in the multi-choice questions.Please note that this prompting strategy is only needed for the AQuA because the other datasets can all be solved by PoT-only prompting.\n\n\nExperiments\n\n\nExperimental Setup\n\nDatasets We summarize our evaluated datasets in Table 1.We use the test set for all the evaluated datasets except TATQA.These datasets are highly heterogeneous in terms of their input formats.We conduct comprehensive experiments on this broad spectrum of datasets to show the generalizability and applicability of PoT prompting.\n\n\nImplementation Details\n\nWe mainly use the OpenAI Codex (code-davinci-002) API2 for our experiments.We also tested GPT-3 (text-davinci-002), ChatGPT (gpt-turbo-3.5),CodeGen (Nijkamp et al., 2022) (codegen-16B-multi and codegen-16B-mono), CodeT5+ (Wang et al., 2023b) and Xgen3 for ablation experiments.We use Python 3.8 with the SymPy library4 to execute the generated program.For the few-shot setting, we use 4-8 shots for all the datasets, based on their difficulty.For simple datasets like FinQA (Chen et al., 2021b), we tend to use fewer shots, while for more challenging datasets like AQuA (Ling et al., 2017) and TATQA (Zhu et al., 2021), we use 8 shots to cover more diverse problems.The examples are taken from the training set.We generally write prompts for 10-20 examples and then tune the exemplar selection on a small validation set to choose the best 4-8 shots for the full set evaluation.\n\nTo elicit the LLM's capability to perform multi-step reasoning, we found a prompt to encourage LLMs to generate reasonable programs without demonstration.The detailed prompt is shown in Figure 3.However, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program.Therefore, we suppress the '#' token logits by a small bias to decrease its probability to avoid such cases.In our preliminary study, we found that -2 as the bias can achieve the best result.We found that this simple strategy can greatly improve our performance.\n\n\nMetrics\n\nWe adopt exact match scores as our evaluation metrics for GSM8K, SVAMP, and MultiArith datasets.We will round the predicted number to a specific precision and then compare it with the reference number.For the AQuA dataset, we use PoT to compute the intermediate answer and then prompt the LLM again to output the closest option to measure the accuracy.For TabMWP, ConvFinQA, and TATQA datasets, we use the official evaluation scripts provided on Github.For FinQA, we relax the evaluation for CoT because LLMs cannot perform the computation precisely (especially with high-precision floats and large numbers), so we adopt 'math.isclose'with relative tolerance of 0.001 to compare answers.\n\nBaselines We report results for three different models including Codex (Chen et al., 2021a), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and LaMDA (Thoppilan et al., 2022).We consider two types of prediction strategies including direct answer output and chain of thought to derive the answer.Since PaLM API is not public, we only list PaLM results reported from previous work (Wei et al., 2022;Wang et al., 2022b).We also leverage an external calculator as suggested in Wei et al. (2022) for all the equations generated by CoT, which is denoted as CoT + calc.Besides greedy decoding, we use self-consistency (Wang et al., 2022b) with CoT, taking the majority vote over 40 different completions as the prediction.\n\n\nMain Results\n\n\nFew-shot Results\n\nWe give our few-shot results in Table 2. On MWP datasets, PoT with greedy decoding improves on GSM8K/AQuA/TabMWP by more than 8%.On SVAMP, the improvement is 4% mainly due to its simplicity.For financial QA datasets, PoT improves over CoT by roughly 20% on FinQA/ConvFinQA and 8% on TATQA.The larger improvements in FinQA and ConvFinQA are mainly due to miscalculations on LLMs for large numbers (e.g. in the millions).CoT adopts LLMs to perform the computation, which is highly prone to miscalculation errors, while PoT adopts a highly precise external computer to solve the problem.As an ablation, we also compare with CoT+calc, which leverages an external calculator to correct the calculation results in the generated 'chain of thoughts'.The experiments show that adding an external calculator only shows mild improvement over CoT on MWP datasets, much behind PoT.The main reason for poor performance of 'calculator' is due to its rigid post-processing step, which can lead to low recall in terms of calibrating the calculation results.\n\n\nFew-shot + Self-Consistency Results\n\nWe leverage self-consistency (SC) decoding to understand the upper bound of our method.This sampling-based decoding algorithm can greatly reduce randomness in the generation procedure and boosts performance.Specifically, we set a temperature of 0.4 and K=40 throughout our experiments.According to\n\n\nZero-shot Results\n\nWe also evaluate the zero-shot performance of PoT and compare with Kojima et al. (2022) in Table 3.As can be seen, zero-shot PoT significantly outperforms zero-shot CoT across all the MWP datasets evaluated.Compared to few-shot prompting, zero-shot PoT outperforms zero-shot CoT (Kojima et al., 2022) by an even larger margin.On the evaluated datasets, PoT's outperforms CoT by an average of 12%.On TabMWP, zero-shot PoT is even higher than few-shot CoT.These results show the great potential to directly generalize to many unseen numerical tasks even without any dataset-specific exemplars.\n\n\nAblation Studies\n\nWe performed multiple ablation studies under the few-shot setting to understand the importance of different factors in PoT including the backbone models, prompt engineering, etc.\n\n\nBackend Ablation\n\nTo understand PoT's performance on different backbone models, we compare the performance of text-davinci-002, code-davinci-002, gpt-3.5-turbo,codegen-16B-mono, codegen-16B-multi, CodeT5+ and XGen.We choose three representative datasets GSM8K, SVAMP, and FinQA to analyze the results.We show our experimental results in Table 4.As can be seen, gpt-3.5-turbocan achieve the highest score to outperform codex (code-davinci-002) by a remarkable margin.In contrast, text-davinci-002 is weaker than code-davinci-002, which is mainly because the following text-based instruction tuning undermines the models' capabilities to generate code.A concerning fact we found is that the open source model like codegen Nijkamp et al. (2022) is significantly behind across different benchmarks.We conjecture that such a huge gap could be attributed to non-sufficient pre-training and model size.\n\n\nSensitivity to Exemplars\n\nTo better understand how sensitive PoT is w.r.t different exemplars, we conduct a sensitivity analysis.Specifically, we wrote 20 total exemplars.For k-shot learning, we randomly sample k = (2, 4, 6, 8) out of the 20 exemplars three times as v1, v2, and v3.We will use these randomly sampled exemplars as demonstrations for PoT.We summarize our sensitivity analysis in Figure 5. First of all, we found that increasing the number of shots helps more for GSM8K than FinQA.This is mainly due to the diversity of questions in GSM8K.By adding more exemplars, the language models can better generalize to diverse questions.Another observation is that when given fewer exemplars, PoT's performance variance is larger.When K=2, the performance variance can be as large as 7% for both datasets.With more exemplars, the performance becomes more stable.\n\n\nComparison with PaL\n\nWe also compare PoT with another more recent related approach like PaL (Gao et al., 2022).According to to Table 5, we found that our method is in general better than PaL, especially on SVAMP and ASDIV.Our results are 6% higher than their prompting method.\n\n\nSemantic Binding and Multi-Step Reasoning\n\nThe two core properties of 'program of thoughts' are:\n\n(1) multiple steps: breaking down the thought process into the step-by-step program, (2) semantic binding: associating semantic meaning to the variable names.To better understand how these two properties contribute, we compared with two variants.One variant is to remove the semantic binding and simply use a, b, c as the variable names.The other variant is to directly predict the final mathematical equation to compute the results.We show our findings in Table 6.As can be seen, removing the binding will in general hurt the model's performance.On more complex questions involving more variables like GSM8K, the performance drop is larger.Similarly, prompting LLMs to directly generate the target equations is also very challenging.\n\nBreaking down the target equation into multiple reasoning steps helps boost performance.\n\n\nBreakdown Analysis\n\nWe perform further analysis to determine which kinds of problems CoT and PoT differ most in performance.We use AQuA (Ling et al., 2017) as our testbed for this.Specifically, we manually classify the questions in AQuA into several categories including geometry, polynomial, symbolic, arithmetic, combinatorics, linear equation, iterative and probability.We show the accuracy for each subcategory in Figure 6.The major categories are (1) linear equations, (2) arithmetic, (3) combinatorics, (4) probability, and\n\n(5) iterative.The largest improvements of PoT are in the categories 'linear/polynomial equation', 'iterative', 'symbolic', and 'combinatorics'.These questions require more complex arithmetic or symbolic skills to solve.In contrast, on 'arithmetic', 'probability', and 'geometric' questions, PoT and CoT perform similarly.Such observation reflects our assumption that 'program' is more effective on more challenging problems.Error Analysis We considered two types of errors: (1) value grounding error, and (2) logic generation error.The first type indicates that the model fails to assign correct values to the variables relevant to the question.The second type indicates that the model fails to generate the correct computation process to answer the question based on the defined variables.Figure 7 shows an example of each type of error.In the upper example, the model fetches the value of the variables incorrectly while the computation logic is correct.In the lower example, the model grounded relevant variables correctly but fails to generate proper computation logic to answer the question.We manually examined the errors made in the TAT-QA results.Among the 198 failure cases of numerical reasoning questions with the PoT (greedy) method, 47% have value grounding errors and 33% have logic errors.In 15% both types of errors occurred and in 5% we believe the answer is actually correct.We found that the majority of the errors are value grounding errors, which is also common for other methods such as CoT.\n\n4 Related Work\n\n\nMathematical Reasoning in NLP\n\nMathematical reasoning skills are essential for general-purpose intelligent systems, which have attracted a significant amount of attention from the community.Earlier, there have been studies in understanding NLP models' capabilities to solve arithmetic/algebraic questions (Hosseini et al., 2014;Koncel-Kedziorski et al., 2015;Roy & Roth, 2015;Ling et al., 2017;Roy & Roth, 2018).Recently, more challenging datasets (Dua et al., 2019;Saxton et al., 2019;Miao et al., 2020;Amini et al., 2019;Hendrycks et al., 2021;Patel et al., 2021) have been proposed to increase the difficulty, diversity or even adversarial robustness.LiLA (Mishra et al., 2022) proposes to assemble a large set of mathematical datasets into a unified dataset.LiLA also annotates Python programs as the generation target for solving mathematical problems.However, LiLA (Mishra et al., 2022) is mostly focused on dataset unification.Our work aims to understand how to generate 'thoughtful programs' to best elicit LLM's reasoning capability.Besides, we also investigate how to solve math problems without any exemplars.Austin et al. (2021) propose to evaluate LLMs' capabilities to synthesize code on two curated datasets MBPP and MathQA-Python.\n\n\nIn-context Learning with LLMs\n\nGPT-3 (Brown et al., 2020)  The capability to follow few-shot exemplars to solve unseen tasks is not existent on smaller LMs, but only emerge as the model scales up (Kaplan et al., 2020).Recently, there have been several works (Xie et al., 2021;Min et al., 2022) aiming to understand how and why in-context learning works.Another concurrent work similar to ours is BINDER (Cheng et al., 2022), which applies Codex to synthesize 'soft' SQL queries to answer questions from tables.\n\n\nChain of Reasoning with LLMs\n\nAlthough LLMs have demonstrated remarkable success across a range of NLP tasks, their ability to reason is often seen as a limitation.Recently, CoT (Wei et al., 2022;Kojima et al., 2022;Wang et al., 2022b) was proposed to enable LLM's capability to perform reasoning tasks by demonstrating 'natural language rationales'.Suzgun et al. (2022) have shown that CoT can already surpass human performance on challenging BIG-Bench tasks.Later on, several other works (Drozdov et al., 2022;Zhou et al., 2022;Nye et al., 2021) also propose different approaches to utilize LLMs to solve reasoning tasks by allowing intermediate steps.\n\nReAct Yao et al. (2022) propose to leverage external tools like search engine to enhance the LLM reasoning skills.Our method can be seen as augmenting CoT with external tools (Python) to enable robust numerical reasoning.Another contemporary work (Gao et al., 2022) was proposed at the same time as ours to adopt hybrid text/code reasoning to address math questions.\n\n\nDiscussion about Contemporary Work\n\nRecently, there has been several follow-up work on top of PoT including self-critic (Gou et al., 2023), selfeval (Xie et al., 2023), plan-and-solve (Wang et al., 2023a).These methods propose to enhance LLMs' capabilities to solve math problems with PoT.self-critic (Gou et al., 2023) and self-eval (Xie et al., 2021) both adopt self-evaluation to enhance the robustness of the generated program.plan-and-solve (Wang et al., 2023a) instead adopt more detailed planning instruction to help LLMs create a high-level reasoning plan.These methods all prove to bring decent improvements over PoT on different math reasoning datasets.\n\nAnother line of work related to ours is Tool-use in transformer models (Schick et al., 2023;Paranjape et al., 2023).These work propose to adopt different tools to help the language models ground on external world.These work generalizes our Python program into more general API calls to include search engine, string extraction, etc.By generalization, LLMs can unlock its capabilities to solve more complex reasoning and grounding problems in real-world scenarios.\n\n\nDiscussion\n\nIn this work, we have verified that our prompting methods can work efficiently on numerical reasoning tasks like math or finance problem solving.We also study how to combine PoT with CoT to combine the merits of both prompting approaches.We believe PoT is suitable for problems which require highly symbolic reasoning skills.For semantic reasoning tasks like commonsense reasoning (StrategyQA), we conjecture that PoT is not the best option.In contrast, CoT can solve more broader reasoning tasks.\n\n\nConclusions\n\nIn this work, we investigate how to disentangle computation from reasoning in solving numerical problems.By 'program of thoughts' prompting, we are able to elicit LLMs' abilities to generate accurate programs to express complex reasoning procedure, while also allows computation to be separately handled by an external program interpreter.This approach is able to boost the performance of LLMs on several math datasets significantly.We believe our work can inspire more work to combine symbolic execution with LLMs to achieve better performance on other symbolic reasoning tasks.\n\n\nLimitations\n\nOur work aims at combining LLM with symbolic execution to solve challenging math problems.PoT would require execution of 'generated code' from LLMs, which could contain certain dangerous or risky code snippets like 'import os; os.rmdir()', etc.We have blocked the LLM from importing any additional modules and restrict it to using the pre-defined modules.Such brutal-force blocking works reasonable for math QA, however, for other unknown symbolic tasks, it might hurt the PoT's generalization.Another limitation is that PoT still struggles with AQuA dataset with complex algebraic questions with only 58% accuracy.It's mainly due to the diversity questions in AQuA, which the demonstration cannot possibly cover.Therefore, the future research should discuss how to further prompt LLMs to generate code for highly diversified Math questions.\n\n\nAppendix\n\n\nPoT as intermediate step\n\nWe demonstrate the workflow in Figure 8. PoT as intermediate step is able to address more complex questions which require both symbolic and commonsense reasoning.\n\n\nExemplars for Prompting\n\nTo enable better reproducibility, we also put our prompts and exemplars for GSM8K dataset and AQuA dataset in the following pages:\n\nFigure 1 :\n1\nFigure 1: Comparison between Chain of Thoughts and Program of Thoughts.\n\n\nFigure 2 :\n2\nFigure 2: Few-shot (upper), Few-shot + SC (middle) and Zero-Shot (lower) Performance overview of Codex PoT and Codex CoT across different datasets.\n\n\nFigure 3 :\n3\nFigure 3: Left: Few-shot PoT prompting, Right: Zero-shot PoT prompting.\n\n\nFigure 4 :\n4\nFigure 4: PoT combined with CoT for multi-stage reasoning.\n\n\nFigure 5 :\n5\nFigure 5: Exemplar sensitivity analysis for GSM8K and FinQA, where v1, v2 and v3 are three versions of k-shot demonstration sampled from the pool.\n\n\nFigure 6 :\n6\nFigure 6: PoT and CoT's breakdown accuracy across different types of questions.\n\n\nFigure 7 :\n7\nFigure 7: Error cases on TAT-QA dev set using PoT-greedy method.\n\n\nFigure 8 :\n8\nFigure 8: We adopt PoT to prompt language models to first generate an intermediate answer and then continue to prompt large models to generate the final answer.We write the pseudo code as follows:# Function PoT( I n p u t ) \u2212> Output # I n p u t : q u e s t i o n # Ouptut : program # Function Prompt ( I n p u t ) \u2212> Output # I n p u t : q u e s t i o n + i n t e r m e d i a t e # Ouptut: answer program = PoT( q u e s t i o n ) exec ( program ) i f i s i n t a n c e ( ans , dict ) : ans = l i s t ( x .i t e m s ( ) ) .pop ( 0 ) e x t r a = ' a c c o r d i n g \u2423 t o \u2423 t h e \u2423 program : \u2423 ' e x t r a += ans [ 0 ] + ' \u2423=\u2423 ' + ans [ 1 ] pred = Prompt ( q u e s t i o n + e x t r a ) e l s e : pred = ans return pred\n\n\nTable 1 :\n1\nSummarization of all the datasets being evaluated.\nDatasetSplit Example DomainInputOutputGSM8K (Cobbe et al., 2021)Test1318MWPQuestionNumberAQuA (Ling et al., 2017)Test253MWPQuestionOptionSVAMP (Patel et al., 2021)Test1000MWPQuestionNumberMultiArith (Roy & Roth, 2015) Test600MWPQuestionNumberTabMWP (Lu et al., 2022)Test7861MWPTable + QuestionNumber + TextFinQA (Chen et al., 2021b)Test1147Finance Table + Text + QuestionNumber + BinaryConvFinQA (Chen et al., 2022) Test421Finance Table + Text + Conversation Number + BinaryTATQA (Zhu et al., 2021)Dev1668Finance Table + Text + QuestionNumber + Text\nTo incorporate the diverse inputs, we propose to linearize these inputs in the prompt.For table inputs, we adopt the same strategy as Chen (2022) to linearize a table into a text string.The columns of the table are separated by '|' and the rows are separated by '\\n'.If a table cell is empty, it is filled by '-'.For text+table hybrid inputs, we separate tables and text with '\\n'.For conversational history, we also separate conversation turns by '\\n'.The prompt is constructed by the concatenation of task instruction, text, linearized table, and question.For conversational question answering, we simply concatenate all the dialog history in the prompt.\n\n\n\n\nTable 2, we found that PoT + SC still outperforms CoT + SC\nModel#Params GSM8K AQuA SVAMP TabWMP FinQA ConvFin TATQAAvgFine-tuned or few-shot promptPublished SoTA-78.052.086.868.268.068.973.670.7Few-shot prompt (Greedy Decoding)Codex Direct175B19.729.569.959.425.640.055.042.7Codex CoT175B63.145.376.465.240.445.661.456.7GPT-3 Direct175B15.624.865.757.114.429.137.934.9GPT-3 CoT175B46.935.868.962.926.137.442.545.7PaLM Direct540B17.925.269.4-----PaLM CoT540B56.935.879.0-----Codex CoT calc175B65.445.377.065.8----GPT-3 CoT calc175B49.635.870.363.4----PaLM CoT calc540B58.635.879.8-----PoT-Codex175B71.654.185.273.264.564.669.068.9Few-shot prompt (Self-Consistency Decoding)LaMDA CoT-SC 137B27.726.853.5-----Codex CoT-SC175B78.052.086.875.444.447.963.263.9PaLM CoT-SC540B74.448.386.6-----PoT-SC-Codex175B80.058.689.181.868.167.370.273.6Few-shot prompt (GPT-4)CoT-GPT4175B92.072.497.0-58.2---PoT-GPT4175B97.284.497.4-74.0---\n\nTable 2 :\n2Model#Params GSM8K AQuA SVAMP TabMWP MultiArithAvgZero-shot Direct (GPT-3) 175B12.622.458.738.922.731.0Zero-shot CoT (GPT-3)175B40.531.963.753.579.353.7Zero-shot CoT (PaLM)540B43.0---66.1-Zero-shot PoT (Ours)175B57.043.970.866.592.266.1\n(Lei et al., 2022))) for different datasets.Published SoTA includes the best-known results (excluding results obtained by GPT-4).On GSM8K, AQuA and SVAMP, the prior SoTA results are CoT + self-consistency decoding(Wang et al., 2022b).On FinQA, the prior best result is fromWang et al. (2022a).On ConvFinQA, the prior best result is achieved by FinQANet(Chen et al., 2022).On TabWMP(Lu et al., 2022), the prior best result is achieved by Dynamic Prompt Learning(Lu et al., 2022).On TATQA, the SoTA result is by RegHNT(Lei et al., 2022).\n\n\nTable 3 :\n3\nKojima et al. (2022)s for different datasets.The baseline results are taken fromKojima et al. (2022).\non MWP datasets with notable margins. On financial datasets, we observe that self-consistency decodingis less impactful for both PoT and CoT. Similarly, PoT + SC outperforms CoT + SC by roughly 20% onFinQA/ConvFinQA and 7% on TATQA.\n\nTable 4 :\n4\nPoT prompting performance with different backend model.\nModel#Params GSM8K SVAMPcode-davinci-002175B71.685.2text-davinci-002175B60.480.1gpt-3.5-turbo-76.388.2codegen-16B-multi 16B8.229.2codegen-16B-mono 16B12.741.1codeT5+16B12.538.5xgen7B11.040.6v1 v2 v20.730.740.650.680.670.690.650.70.620.580.60.552-shots4-shots6-shots8-shots0.660.660.630.640.630.620.620.640.620.640.620.582-shots4-shots6-shots8-shots\n\nTable 5 :\n5\nComparison of PoT against contemporary work PaL (Gao et al., 2022).\nModel GSM8K GSM8K-Hard SVAMP ASDIV ADDSUB MULTIARITHPaL72.061.279.479.692.599.2PoT71.661.885.285.292.299.5MethodGSM8K SVAMP FinQAPoT71.685.264.5PoT -Binding60.283.861.6PoT -MultiStep45.881.958.9\n\nTable 6 :\n6\nComparison between PoT and equation generation on three different datasets.\n\n\n\n\ndemonstrated a strong capability to perform few-shot predictions, where the model is given a description of the task in natural language with few examples.Scaling model size, data, and computing are crucial to enable this learning ability.Recently, Rae et al. (2021); Smith et al. (2022); Chowdhery et al. (2022); Du et al. (2022) have proposed to train different types of LLMs with different training recipes.\n\nAssuming each addition is correct with 90% chance, after 50 additions, the likelihood of a correct output is less than 1%.\nhttps://openai.com/blog/openai-codex/\nhttps://blog.salesforceairesearch.com/xgen/\nhttps://www.sympy.org/en/index.html\nPublished in Transactions on Machine LearningResearch (10/2023)   \nQues%on: Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for $2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?# Python code, return ans total_eggs = 16 eaten_eggs = 3 baked_eggs = 4 sold_eggs = total_eggs -eaten_eggs -baked_eggs dollars_per_egg = 2 ans = sold_eggs * dollars_per_egg Ques%on: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?# Python code, return ans bolts_of_blue_fiber = 2 bolts_of_white_fiber = num_of_blue_fiber / 2 ans = bolts_of_blue_fiber + bolts_of_white_fiber Ques%on: Josh decides to try flipping a house.He buys a house for $80,000 and then puts in $50,000 in repairs.This increased the value of the house by 150%.How much profit did he make?# Python code, return ans cost_of_original_house = 80000 increase_rate = 150 / 100 value_of_house = (1 + increase_rate) * cost_of_original_house cost_of_repair = 50000 ans = value_of_house -cost_of_repair -cost_of_original_house Ques%on: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.She gives the chickens their feed in three separate meals.In the morning, she gives her flock of chickens 15 cups of feed.In the a`ernoon, she gives her chickens another 25 cups of feed.How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?# Python code, return ans numb_of_chickens = 20 cups_for_each_chicken = 3 cups_for_all_chicken = num_of_chickens * cups_for_each_chicken cups_in_the_morning = 15 cups_in_the_a`ernoon = 25 ans = cups_for_all_chicken -cups_in_the_morning -cups_in_the_a`ernoon Ques%on: Kylar went to the store to buy glasses for his new apartment.One glass costs $5, but every second glass costs only 60% of the price.Kylar wants to buy 16 glasses.How much does he need to pay for them?# Python code, return ans num_glasses = 16 first_glass_cost = 5 second_glass_cost = 5 * 0.6 ans = 0 for i in range(num_glasses):if i % 2 == 0: ans += first_glass_cost else:ans += second_glass_cost Ques%on: Jordan wanted to surprise her mom with a homemade birthday cake.From reading the instruc%ons, she knew it would take 20 minutes to make the cake bajer and 30 minutes to bake the cake.The cake would require 2 hours to cool and an addi%onal 10 minutes to frost the cake.If she plans to make the cake all on the same day, what is the latest %me of day that Jordan can start making the cake to be ready to serve it at 5:00 pm? # Python code, return ans minutes_to_make_bajer = 20 minutes_to_bake_cake = 30 minutes_to_cool_cake = 2 * 60 minutes_to_frost_cake = 10 total_minutes = minutes_to_make_bajer + minutes_to_bake_cake + minutes_to_cool_cake + minutes_to_frost_cake total_hours = total_minutes / 60 ans = 5 -total_hours # Write Python Code to solve the following ques7ons.Store your result as a variable named 'ans'.from sympy import Symbol from sympy import simplify import math from sympy import solve_it # solve_it(equa7ons, variable): solving the equa7ons and return the variable value.[36,15,17,5,7]for op7on in op7ons: if op7on % 9 == 0 and op7on % 12 == 0: ans = op7on break # Ques7on: 35% of the employees of a company are men.60% of the men in the company speak French and 40% of the employees of the company speak French.What is % of the women in the company who do not speak French?# Answer op7on: ['A)4%', 'B)10%', 'C)96%', 'D)90.12%','E)70.\nMathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191\n\nProgram synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021aarXiv preprint\n\nFinqa: A dataset of numerical reasoning over financial data. Wenhu Chen, ; Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, arXiv:2210.06710Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2022. 2021barXiv preprintLarge language models are few (1)-shot table reasoners\n\nConvfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, William Yang, Wang , arXiv:2210.038492022arXiv preprint\n\nBinding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, arXiv:2210.028752022arXiv preprint\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint\n\nCompositional semantic parsing with large language models. Andrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, arXiv:2209.150032022arXiv preprint\n\nGlam: Efficient scaling of language models with mixtureof-experts. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, International Conference on Machine Learning. PMLR2022\n\nDrop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191\n\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2211.10435Pal: Program-aided language models. 2022arXiv preprint\n\nCritic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint\n\nMeasuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint\n\nLearning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, EMNLP. 2014\n\nJared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint\n\nLarge language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint\n\nParsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , Transactions of the Association for Computational Linguistics. 32015\n\nAnswering numerical reasoning questions in table-text hybrid contents with graph-based encoder and tree-based decoder. Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, Kang Liu, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022\n\nProgram induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171\n\nDynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, arXiv:2209.146102022arXiv preprint\n\nA diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint\n\nLila: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022\n\nCodegen: An open large language model for code with multi-turn program synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.134742022arXiv preprint\n\nShow your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint\n\nBhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.08774arXiv:2303.09014Automatic multi-step reasoning and tool-use for large language models. Art2023. 2023OpenAIarXiv preprintGpt-4 technical report\n\nAre NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021\n\nScaling language models: Methods, analysis & insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint\n\nSolving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015\n\nMapping to declarative knowledge for word problem solving. Subhro Roy, Dan Roth, Transactions of the Association for Computational Linguistics. 62018\n\nDavid Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, arXiv:1904.01557Analysing mathematical reasoning abilities of neural models. 2019arXiv preprint\n\nToolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022arXiv preprint\n\nChallenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint\n\nA numerical reasoning question answering system with fine-grained retriever and the ensemble of multiple generators for finqa. Bin Wang, Jiangzhou Ju, Yunlin Mao, Xin-Yu Dai, Shujian Huang, Jiajun Chen, arXiv:2206.085062022aarXiv preprint\n\nPlanand-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023aarXiv preprint\n\nSelf-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.111712022barXiv preprint\n\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, D Q Nghi, Junnan Bui, Steven Ch Li, Hoi, arXiv:2305.07922Codet5+: Open code large language models for code understanding and generation. 2023barXiv preprint\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint\n\nAn explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, International Conference on Learning Representations. 2021\n\nDecomposition enhances reasoning via self-evaluation guided decoding. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie, arXiv:2305.006332023arXiv preprint\n\nReact: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022\n\nLeast-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi, arXiv:2205.106252022arXiv preprint\n\nTat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, Tat-Seng Chua, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211\n", "annotations": {"author": "[{\"end\":201,\"start\":116},{\"end\":324,\"start\":202},{\"end\":356,\"start\":325},{\"end\":391,\"start\":357},{\"end\":412,\"start\":392}]", "publisher": null, "author_last_name": "[{\"end\":126,\"start\":122},{\"end\":213,\"start\":211},{\"end\":335,\"start\":331},{\"end\":372,\"start\":367},{\"end\":411,\"start\":407}]", "author_first_name": "[{\"end\":121,\"start\":116},{\"end\":210,\"start\":202},{\"end\":330,\"start\":325},{\"end\":364,\"start\":357},{\"end\":366,\"start\":365},{\"end\":400,\"start\":392},{\"end\":406,\"start\":401}]", "author_affiliation": "[{\"end\":200,\"start\":151},{\"end\":264,\"start\":215},{\"end\":323,\"start\":266}]", "title": "[{\"end\":102,\"start\":1},{\"end\":514,\"start\":413}]", "venue": null, "abstract": "[{\"end\":1690,\"start\":584}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2000,\"start\":1980},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2019,\"start\":2000},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2035,\"start\":2019},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2053,\"start\":2035},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2215,\"start\":2195},{\"end\":2220,\"start\":2215},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2237,\"start\":2220},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2332,\"start\":2313},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2351,\"start\":2332},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2615,\"start\":2598},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2690,\"start\":2670},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2709,\"start\":2690},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2732,\"start\":2709},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3132,\"start\":3114},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5419,\"start\":5399},{\"end\":5638,\"start\":5618},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6051,\"start\":6032},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6072,\"start\":6053},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6098,\"start\":6074},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6115,\"start\":6098},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6508,\"start\":6490},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8042,\"start\":8025},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8937,\"start\":8916},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11032,\"start\":11010},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11103,\"start\":11083},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11356,\"start\":11336},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11451,\"start\":11432},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11480,\"start\":11462},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13105,\"start\":13085},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13133,\"start\":13113},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13164,\"start\":13140},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13199,\"start\":13175},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13422,\"start\":13404},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13441,\"start\":13422},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13515,\"start\":13498},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13656,\"start\":13636},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15262,\"start\":15242},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15474,\"start\":15454},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16709,\"start\":16688},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17846,\"start\":17828},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19095,\"start\":19076},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21331,\"start\":21308},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21362,\"start\":21331},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21379,\"start\":21362},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21397,\"start\":21379},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21414,\"start\":21397},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21469,\"start\":21451},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21489,\"start\":21469},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21507,\"start\":21489},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21526,\"start\":21507},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21549,\"start\":21526},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21568,\"start\":21549},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21683,\"start\":21662},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21895,\"start\":21874},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22143,\"start\":22123},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22309,\"start\":22289},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22469,\"start\":22448},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22528,\"start\":22510},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22545,\"start\":22528},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22675,\"start\":22655},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22961,\"start\":22943},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22981,\"start\":22961},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23000,\"start\":22981},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23135,\"start\":23115},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23277,\"start\":23255},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23295,\"start\":23277},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23312,\"start\":23295},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23444,\"start\":23427},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23685,\"start\":23668},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23928,\"start\":23910},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23957,\"start\":23939},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23994,\"start\":23974},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24109,\"start\":24091},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24142,\"start\":24124},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24256,\"start\":24236},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24547,\"start\":24526},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24570,\"start\":24547}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27329,\"start\":27243},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27492,\"start\":27330},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27579,\"start\":27493},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27653,\"start\":27580},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27815,\"start\":27654},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27910,\"start\":27816},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27990,\"start\":27911},{\"attributes\":{\"id\":\"fig_7\"},\"end\":28724,\"start\":27991},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29996,\"start\":28725},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30921,\"start\":29997},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31707,\"start\":30922},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32055,\"start\":31708},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32473,\"start\":32056},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32749,\"start\":32474},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32839,\"start\":32750},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":33254,\"start\":32840}]", "paragraph": "[{\"end\":2300,\"start\":1706},{\"end\":3867,\"start\":2302},{\"end\":4728,\"start\":3869},{\"end\":5737,\"start\":4730},{\"end\":6768,\"start\":5990},{\"end\":7478,\"start\":6792},{\"end\":7837,\"start\":7480},{\"end\":7962,\"start\":7839},{\"end\":8516,\"start\":8010},{\"end\":9094,\"start\":8518},{\"end\":9308,\"start\":9096},{\"end\":9655,\"start\":9340},{\"end\":9939,\"start\":9657},{\"end\":10470,\"start\":9941},{\"end\":10835,\"start\":10507},{\"end\":11739,\"start\":10862},{\"end\":12313,\"start\":11741},{\"end\":13012,\"start\":12325},{\"end\":13740,\"start\":13014},{\"end\":14816,\"start\":13776},{\"end\":15153,\"start\":14856},{\"end\":15766,\"start\":15175},{\"end\":15965,\"start\":15787},{\"end\":16863,\"start\":15986},{\"end\":17733,\"start\":16892},{\"end\":18012,\"start\":17757},{\"end\":18111,\"start\":18058},{\"end\":18847,\"start\":18113},{\"end\":18937,\"start\":18849},{\"end\":19469,\"start\":18960},{\"end\":20984,\"start\":19471},{\"end\":21000,\"start\":20986},{\"end\":22249,\"start\":21034},{\"end\":22762,\"start\":22283},{\"end\":23419,\"start\":22795},{\"end\":23787,\"start\":23421},{\"end\":24453,\"start\":23826},{\"end\":24918,\"start\":24455},{\"end\":25430,\"start\":24933},{\"end\":26025,\"start\":25446},{\"end\":26882,\"start\":26041},{\"end\":27084,\"start\":26922},{\"end\":27242,\"start\":27112},{\"end\":27328,\"start\":27257},{\"end\":27491,\"start\":27344},{\"end\":27578,\"start\":27507},{\"end\":27652,\"start\":27594},{\"end\":27814,\"start\":27668},{\"end\":27909,\"start\":27830},{\"end\":27989,\"start\":27925},{\"end\":28723,\"start\":28005},{\"end\":28788,\"start\":28738},{\"end\":29995,\"start\":29339},{\"end\":30058,\"start\":30000},{\"end\":31706,\"start\":31171},{\"end\":31822,\"start\":31721},{\"end\":32124,\"start\":32069},{\"end\":32554,\"start\":32487},{\"end\":32838,\"start\":32763},{\"end\":33253,\"start\":32843}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5809,\"start\":5738},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5880,\"start\":5809},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5951,\"start\":5880},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8009,\"start\":7963}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":10562,\"start\":10561},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":13815,\"start\":13814},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":15273,\"start\":15272},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":16312,\"start\":16311},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":17870,\"start\":17869},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":18577,\"start\":18576}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1704,\"start\":1692},{\"attributes\":{\"n\":\"2\"},\"end\":5972,\"start\":5953},{\"attributes\":{\"n\":\"2.1\"},\"end\":5988,\"start\":5975},{\"attributes\":{\"n\":\"2.2\"},\"end\":6790,\"start\":6771},{\"attributes\":{\"n\":\"2.3\"},\"end\":9338,\"start\":9311},{\"attributes\":{\"n\":\"3\"},\"end\":10484,\"start\":10473},{\"attributes\":{\"n\":\"3.1\"},\"end\":10505,\"start\":10487},{\"end\":10860,\"start\":10838},{\"end\":12323,\"start\":12316},{\"attributes\":{\"n\":\"3.2\"},\"end\":13755,\"start\":13743},{\"end\":13774,\"start\":13758},{\"end\":14854,\"start\":14819},{\"end\":15173,\"start\":15156},{\"attributes\":{\"n\":\"3.3\"},\"end\":15785,\"start\":15769},{\"end\":15984,\"start\":15968},{\"end\":16890,\"start\":16866},{\"end\":17755,\"start\":17736},{\"end\":18056,\"start\":18015},{\"end\":18958,\"start\":18940},{\"attributes\":{\"n\":\"4.1\"},\"end\":21032,\"start\":21003},{\"attributes\":{\"n\":\"4.2\"},\"end\":22281,\"start\":22252},{\"attributes\":{\"n\":\"4.3\"},\"end\":22793,\"start\":22765},{\"attributes\":{\"n\":\"4.4\"},\"end\":23824,\"start\":23790},{\"attributes\":{\"n\":\"5\"},\"end\":24931,\"start\":24921},{\"attributes\":{\"n\":\"6\"},\"end\":25444,\"start\":25433},{\"end\":26039,\"start\":26028},{\"attributes\":{\"n\":\"7\"},\"end\":26893,\"start\":26885},{\"attributes\":{\"n\":\"7.1\"},\"end\":26920,\"start\":26896},{\"attributes\":{\"n\":\"7.2\"},\"end\":27110,\"start\":27087},{\"end\":27254,\"start\":27244},{\"end\":27341,\"start\":27331},{\"end\":27504,\"start\":27494},{\"end\":27591,\"start\":27581},{\"end\":27665,\"start\":27655},{\"end\":27827,\"start\":27817},{\"end\":27922,\"start\":27912},{\"end\":28002,\"start\":27992},{\"end\":28735,\"start\":28726},{\"end\":30932,\"start\":30923},{\"end\":31718,\"start\":31709},{\"end\":32066,\"start\":32057},{\"end\":32484,\"start\":32475},{\"end\":32760,\"start\":32751}]", "table": "[{\"end\":29338,\"start\":28789},{\"end\":30921,\"start\":30059},{\"end\":31170,\"start\":30934},{\"end\":32055,\"start\":31823},{\"end\":32473,\"start\":32125},{\"end\":32749,\"start\":32555}]", "figure_caption": "[{\"end\":27329,\"start\":27256},{\"end\":27492,\"start\":27343},{\"end\":27579,\"start\":27506},{\"end\":27653,\"start\":27593},{\"end\":27815,\"start\":27667},{\"end\":27910,\"start\":27829},{\"end\":27990,\"start\":27924},{\"end\":28724,\"start\":28004},{\"end\":28789,\"start\":28737},{\"end\":30059,\"start\":29999},{\"end\":31823,\"start\":31720},{\"end\":32125,\"start\":32068},{\"end\":32555,\"start\":32486},{\"end\":32839,\"start\":32762},{\"end\":33254,\"start\":32842}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5010,\"start\":5009},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7043,\"start\":7042},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8571,\"start\":8570},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":9654,\"start\":9653},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9986,\"start\":9985},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11935,\"start\":11934},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17268,\"start\":17267},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19366,\"start\":19365},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20269,\"start\":20268},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26961,\"start\":26960}]", "bib_author_first_name": "[{\"end\":37284,\"start\":37280},{\"end\":37298,\"start\":37292},{\"end\":37317,\"start\":37308},{\"end\":37326,\"start\":37323},{\"end\":37351,\"start\":37346},{\"end\":37366,\"start\":37358},{\"end\":37602,\"start\":37597},{\"end\":37619,\"start\":37611},{\"end\":37634,\"start\":37627},{\"end\":37647,\"start\":37640},{\"end\":37661,\"start\":37655},{\"end\":37680,\"start\":37675},{\"end\":37693,\"start\":37688},{\"end\":37707,\"start\":37701},{\"end\":37720,\"start\":37713},{\"end\":37732,\"start\":37728},{\"end\":37815,\"start\":37812},{\"end\":37831,\"start\":37823},{\"end\":37842,\"start\":37838},{\"end\":37857,\"start\":37850},{\"end\":37872,\"start\":37867},{\"end\":37874,\"start\":37873},{\"end\":37891,\"start\":37883},{\"end\":37908,\"start\":37902},{\"end\":37928,\"start\":37922},{\"end\":37942,\"start\":37936},{\"end\":37957,\"start\":37951},{\"end\":38029,\"start\":38025},{\"end\":38041,\"start\":38036},{\"end\":38056,\"start\":38050},{\"end\":38068,\"start\":38062},{\"end\":38083,\"start\":38075},{\"end\":38114,\"start\":38109},{\"end\":38128,\"start\":38123},{\"end\":38142,\"start\":38138},{\"end\":38158,\"start\":38150},{\"end\":38171,\"start\":38167},{\"end\":38335,\"start\":38330},{\"end\":38349,\"start\":38342},{\"end\":38361,\"start\":38356},{\"end\":38375,\"start\":38368},{\"end\":38391,\"start\":38384},{\"end\":38402,\"start\":38398},{\"end\":38416,\"start\":38411},{\"end\":38431,\"start\":38426},{\"end\":38444,\"start\":38440},{\"end\":38460,\"start\":38452},{\"end\":38848,\"start\":38843},{\"end\":38862,\"start\":38855},{\"end\":38874,\"start\":38867},{\"end\":38891,\"start\":38883},{\"end\":38903,\"start\":38896},{\"end\":38917,\"start\":38910},{\"end\":38928,\"start\":38924},{\"end\":39021,\"start\":39014},{\"end\":39036,\"start\":39029},{\"end\":39046,\"start\":39042},{\"end\":39059,\"start\":39052},{\"end\":39069,\"start\":39064},{\"end\":39085,\"start\":39080},{\"end\":39097,\"start\":39090},{\"end\":39113,\"start\":39105},{\"end\":39125,\"start\":39121},{\"end\":39141,\"start\":39137},{\"end\":39200,\"start\":39191},{\"end\":39218,\"start\":39212},{\"end\":39232,\"start\":39227},{\"end\":39248,\"start\":39241},{\"end\":39262,\"start\":39256},{\"end\":39275,\"start\":39271},{\"end\":39289,\"start\":39285},{\"end\":39303,\"start\":39298},{\"end\":39307,\"start\":39304},{\"end\":39322,\"start\":39315},{\"end\":39340,\"start\":39331},{\"end\":39432,\"start\":39428},{\"end\":39446,\"start\":39440},{\"end\":39465,\"start\":39457},{\"end\":39481,\"start\":39476},{\"end\":39499,\"start\":39490},{\"end\":39519,\"start\":39508},{\"end\":39531,\"start\":39527},{\"end\":39691,\"start\":39685},{\"end\":39710,\"start\":39701},{\"end\":39724,\"start\":39720},{\"end\":39740,\"start\":39734},{\"end\":39756,\"start\":39749},{\"end\":39769,\"start\":39763},{\"end\":39783,\"start\":39776},{\"end\":39799,\"start\":39794},{\"end\":39912,\"start\":39909},{\"end\":39924,\"start\":39917},{\"end\":39938,\"start\":39932},{\"end\":39940,\"start\":39939},{\"end\":39951,\"start\":39946},{\"end\":39964,\"start\":39958},{\"end\":39984,\"start\":39975},{\"end\":39994,\"start\":39989},{\"end\":40008,\"start\":40003},{\"end\":40020,\"start\":40015},{\"end\":40024,\"start\":40021},{\"end\":40034,\"start\":40029},{\"end\":40190,\"start\":40184},{\"end\":40203,\"start\":40196},{\"end\":40217,\"start\":40210},{\"end\":40233,\"start\":40226},{\"end\":40251,\"start\":40245},{\"end\":40263,\"start\":40259},{\"end\":40449,\"start\":40445},{\"end\":40459,\"start\":40455},{\"end\":40474,\"start\":40468},{\"end\":40484,\"start\":40481},{\"end\":40498,\"start\":40491},{\"end\":40510,\"start\":40504},{\"end\":40522,\"start\":40517},{\"end\":40537,\"start\":40531},{\"end\":40705,\"start\":40699},{\"end\":40718,\"start\":40711},{\"end\":40730,\"start\":40725},{\"end\":40743,\"start\":40737},{\"end\":40755,\"start\":40750},{\"end\":40765,\"start\":40762},{\"end\":40778,\"start\":40772},{\"end\":40886,\"start\":40883},{\"end\":40904,\"start\":40898},{\"end\":40918,\"start\":40912},{\"end\":40933,\"start\":40929},{\"end\":40947,\"start\":40941},{\"end\":40960,\"start\":40956},{\"end\":40971,\"start\":40967},{\"end\":40983,\"start\":40978},{\"end\":41115,\"start\":41101},{\"end\":41134,\"start\":41126},{\"end\":41151,\"start\":41147},{\"end\":41165,\"start\":41161},{\"end\":41193,\"start\":41188},{\"end\":41205,\"start\":41202},{\"end\":41221,\"start\":41218},{\"end\":41235,\"start\":41232},{\"end\":41237,\"start\":41236},{\"end\":41253,\"start\":41245},{\"end\":41266,\"start\":41261},{\"end\":41279,\"start\":41274},{\"end\":41290,\"start\":41286},{\"end\":41307,\"start\":41300},{\"end\":41317,\"start\":41312},{\"end\":41457,\"start\":41450},{\"end\":41471,\"start\":41466},{\"end\":41488,\"start\":41482},{\"end\":41499,\"start\":41493},{\"end\":41512,\"start\":41506},{\"end\":41617,\"start\":41614},{\"end\":41645,\"start\":41637},{\"end\":41664,\"start\":41658},{\"end\":41680,\"start\":41676},{\"end\":41695,\"start\":41690},{\"end\":41706,\"start\":41703},{\"end\":41904,\"start\":41898},{\"end\":41916,\"start\":41910},{\"end\":41926,\"start\":41921},{\"end\":41934,\"start\":41931},{\"end\":41945,\"start\":41941},{\"end\":42200,\"start\":42196},{\"end\":42211,\"start\":42207},{\"end\":42227,\"start\":42222},{\"end\":42238,\"start\":42234},{\"end\":42520,\"start\":42517},{\"end\":42530,\"start\":42525},{\"end\":42543,\"start\":42536},{\"end\":42555,\"start\":42551},{\"end\":42560,\"start\":42556},{\"end\":42574,\"start\":42565},{\"end\":42586,\"start\":42580},{\"end\":42604,\"start\":42599},{\"end\":42618,\"start\":42612},{\"end\":42764,\"start\":42755},{\"end\":42778,\"start\":42771},{\"end\":42962,\"start\":42957},{\"end\":42973,\"start\":42968},{\"end\":42982,\"start\":42979},{\"end\":42998,\"start\":42993},{\"end\":43012,\"start\":43008},{\"end\":43028,\"start\":43020},{\"end\":43045,\"start\":43041},{\"end\":43233,\"start\":43226},{\"end\":43249,\"start\":43242},{\"end\":43264,\"start\":43261},{\"end\":43276,\"start\":43269},{\"end\":43287,\"start\":43283},{\"end\":43303,\"start\":43297},{\"end\":43317,\"start\":43311},{\"end\":43336,\"start\":43330},{\"end\":43352,\"start\":43346},{\"end\":43369,\"start\":43364},{\"end\":43383,\"start\":43377},{\"end\":43643,\"start\":43639},{\"end\":43655,\"start\":43653},{\"end\":43669,\"start\":43662},{\"end\":43683,\"start\":43679},{\"end\":43692,\"start\":43688},{\"end\":43705,\"start\":43699},{\"end\":43718,\"start\":43712},{\"end\":43736,\"start\":43729},{\"end\":43866,\"start\":43859},{\"end\":43878,\"start\":43872},{\"end\":43900,\"start\":43897},{\"end\":43916,\"start\":43910},{\"end\":43935,\"start\":43930},{\"end\":43949,\"start\":43944},{\"end\":43963,\"start\":43958},{\"end\":43976,\"start\":43971},{\"end\":43995,\"start\":43988},{\"end\":44008,\"start\":44003},{\"end\":44059,\"start\":44051},{\"end\":44076,\"start\":44071},{\"end\":44093,\"start\":44087},{\"end\":44109,\"start\":44101},{\"end\":44126,\"start\":44122},{\"end\":44145,\"start\":44140},{\"end\":44160,\"start\":44153},{\"end\":44392,\"start\":44387},{\"end\":44406,\"start\":44400},{\"end\":44426,\"start\":44421},{\"end\":44873,\"start\":44864},{\"end\":44892,\"start\":44886},{\"end\":44908,\"start\":44903},{\"end\":44920,\"start\":44914},{\"end\":44938,\"start\":44931},{\"end\":44953,\"start\":44949},{\"end\":44965,\"start\":44960},{\"end\":44982,\"start\":44977},{\"end\":45002,\"start\":44994},{\"end\":45100,\"start\":45094},{\"end\":45109,\"start\":45106},{\"end\":45346,\"start\":45340},{\"end\":45355,\"start\":45352},{\"end\":45437,\"start\":45432},{\"end\":45452,\"start\":45446},{\"end\":45472,\"start\":45467},{\"end\":45487,\"start\":45479},{\"end\":45659,\"start\":45655},{\"end\":45672,\"start\":45668},{\"end\":45692,\"start\":45685},{\"end\":45707,\"start\":45700},{\"end\":45723,\"start\":45718},{\"end\":45736,\"start\":45732},{\"end\":45756,\"start\":45750},{\"end\":45773,\"start\":45767},{\"end\":45930,\"start\":45924},{\"end\":45945,\"start\":45938},{\"end\":45962,\"start\":45955},{\"end\":45978,\"start\":45971},{\"end\":45996,\"start\":45990},{\"end\":46015,\"start\":46010},{\"end\":46028,\"start\":46024},{\"end\":46041,\"start\":46034},{\"end\":46060,\"start\":46054},{\"end\":46075,\"start\":46070},{\"end\":46203,\"start\":46198},{\"end\":46218,\"start\":46212},{\"end\":46236,\"start\":46227},{\"end\":46255,\"start\":46246},{\"end\":46268,\"start\":46266},{\"end\":46279,\"start\":46274},{\"end\":46283,\"start\":46280},{\"end\":46300,\"start\":46291},{\"end\":46314,\"start\":46312},{\"end\":46316,\"start\":46315},{\"end\":46333,\"start\":46328},{\"end\":46386,\"start\":46381},{\"end\":46404,\"start\":46398},{\"end\":46407,\"start\":46405},{\"end\":46422,\"start\":46417},{\"end\":46433,\"start\":46429},{\"end\":46449,\"start\":46443},{\"end\":46480,\"start\":46474},{\"end\":46494,\"start\":46488},{\"end\":46506,\"start\":46500},{\"end\":46514,\"start\":46512},{\"end\":46733,\"start\":46730},{\"end\":46749,\"start\":46740},{\"end\":46760,\"start\":46754},{\"end\":46772,\"start\":46766},{\"end\":46785,\"start\":46778},{\"end\":46799,\"start\":46793},{\"end\":46944,\"start\":46941},{\"end\":46956,\"start\":46951},{\"end\":46967,\"start\":46961},{\"end\":46981,\"start\":46973},{\"end\":46992,\"start\":46986},{\"end\":47001,\"start\":46998},{\"end\":47010,\"start\":47004},{\"end\":47023,\"start\":47016},{\"end\":47145,\"start\":47139},{\"end\":47157,\"start\":47152},{\"end\":47167,\"start\":47163},{\"end\":47184,\"start\":47180},{\"end\":47191,\"start\":47189},{\"end\":47202,\"start\":47197},{\"end\":47249,\"start\":47246},{\"end\":47260,\"start\":47256},{\"end\":47273,\"start\":47265},{\"end\":47291,\"start\":47290},{\"end\":47293,\"start\":47292},{\"end\":47306,\"start\":47300},{\"end\":47318,\"start\":47312},{\"end\":47321,\"start\":47319},{\"end\":47524,\"start\":47519},{\"end\":47536,\"start\":47530},{\"end\":47547,\"start\":47543},{\"end\":47567,\"start\":47560},{\"end\":47577,\"start\":47575},{\"end\":47587,\"start\":47583},{\"end\":47597,\"start\":47592},{\"end\":47714,\"start\":47710},{\"end\":47733,\"start\":47728},{\"end\":47752,\"start\":47747},{\"end\":47766,\"start\":47760},{\"end\":47905,\"start\":47901},{\"end\":47916,\"start\":47911},{\"end\":47933,\"start\":47928},{\"end\":47942,\"start\":47940},{\"end\":47956,\"start\":47949},{\"end\":47969,\"start\":47962},{\"end\":47979,\"start\":47974},{\"end\":48087,\"start\":48081},{\"end\":48100,\"start\":48093},{\"end\":48111,\"start\":48107},{\"end\":48121,\"start\":48116},{\"end\":48135,\"start\":48131},{\"end\":48311,\"start\":48306},{\"end\":48327,\"start\":48318},{\"end\":48339,\"start\":48337},{\"end\":48350,\"start\":48345},{\"end\":48362,\"start\":48356},{\"end\":48377,\"start\":48371},{\"end\":48388,\"start\":48384},{\"end\":48408,\"start\":48401},{\"end\":48423,\"start\":48419},{\"end\":48430,\"start\":48428},{\"end\":48573,\"start\":48566},{\"end\":48587,\"start\":48579},{\"end\":48601,\"start\":48593},{\"end\":48613,\"start\":48609},{\"end\":48624,\"start\":48620},{\"end\":48641,\"start\":48632},{\"end\":48650,\"start\":48646},{\"end\":48665,\"start\":48657}]", "bib_author_last_name": "[{\"end\":37290,\"start\":37285},{\"end\":37306,\"start\":37299},{\"end\":37321,\"start\":37318},{\"end\":37344,\"start\":37327},{\"end\":37356,\"start\":37352},{\"end\":37377,\"start\":37367},{\"end\":37609,\"start\":37603},{\"end\":37625,\"start\":37620},{\"end\":37638,\"start\":37635},{\"end\":37653,\"start\":37648},{\"end\":37673,\"start\":37662},{\"end\":37686,\"start\":37681},{\"end\":37699,\"start\":37694},{\"end\":37711,\"start\":37708},{\"end\":37726,\"start\":37721},{\"end\":37735,\"start\":37733},{\"end\":37821,\"start\":37816},{\"end\":37836,\"start\":37832},{\"end\":37848,\"start\":37843},{\"end\":37865,\"start\":37858},{\"end\":37881,\"start\":37875},{\"end\":37900,\"start\":37892},{\"end\":37920,\"start\":37909},{\"end\":37934,\"start\":37929},{\"end\":37949,\"start\":37943},{\"end\":37964,\"start\":37958},{\"end\":38034,\"start\":38030},{\"end\":38048,\"start\":38042},{\"end\":38060,\"start\":38057},{\"end\":38073,\"start\":38069},{\"end\":38107,\"start\":38084},{\"end\":38121,\"start\":38115},{\"end\":38136,\"start\":38129},{\"end\":38148,\"start\":38143},{\"end\":38165,\"start\":38159},{\"end\":38180,\"start\":38172},{\"end\":38340,\"start\":38336},{\"end\":38354,\"start\":38350},{\"end\":38366,\"start\":38362},{\"end\":38382,\"start\":38376},{\"end\":38396,\"start\":38392},{\"end\":38409,\"start\":38403},{\"end\":38424,\"start\":38417},{\"end\":38438,\"start\":38432},{\"end\":38450,\"start\":38445},{\"end\":38466,\"start\":38461},{\"end\":38485,\"start\":38468},{\"end\":38853,\"start\":38849},{\"end\":38865,\"start\":38863},{\"end\":38881,\"start\":38875},{\"end\":38894,\"start\":38892},{\"end\":38908,\"start\":38904},{\"end\":38922,\"start\":38918},{\"end\":39027,\"start\":39022},{\"end\":39040,\"start\":39037},{\"end\":39050,\"start\":39047},{\"end\":39062,\"start\":39060},{\"end\":39078,\"start\":39070},{\"end\":39088,\"start\":39086},{\"end\":39103,\"start\":39098},{\"end\":39119,\"start\":39114},{\"end\":39135,\"start\":39126},{\"end\":39153,\"start\":39142},{\"end\":39210,\"start\":39201},{\"end\":39225,\"start\":39219},{\"end\":39239,\"start\":39233},{\"end\":39254,\"start\":39249},{\"end\":39269,\"start\":39263},{\"end\":39283,\"start\":39276},{\"end\":39296,\"start\":39290},{\"end\":39313,\"start\":39308},{\"end\":39329,\"start\":39323},{\"end\":39349,\"start\":39341},{\"end\":39438,\"start\":39433},{\"end\":39455,\"start\":39447},{\"end\":39474,\"start\":39466},{\"end\":39488,\"start\":39482},{\"end\":39506,\"start\":39500},{\"end\":39525,\"start\":39520},{\"end\":39540,\"start\":39532},{\"end\":39699,\"start\":39692},{\"end\":39718,\"start\":39711},{\"end\":39732,\"start\":39725},{\"end\":39747,\"start\":39741},{\"end\":39761,\"start\":39757},{\"end\":39774,\"start\":39770},{\"end\":39792,\"start\":39784},{\"end\":39804,\"start\":39800},{\"end\":39915,\"start\":39913},{\"end\":39930,\"start\":39925},{\"end\":39944,\"start\":39941},{\"end\":39956,\"start\":39952},{\"end\":39973,\"start\":39965},{\"end\":39987,\"start\":39985},{\"end\":40001,\"start\":39995},{\"end\":40013,\"start\":40009},{\"end\":40027,\"start\":40025},{\"end\":40040,\"start\":40035},{\"end\":40194,\"start\":40191},{\"end\":40208,\"start\":40204},{\"end\":40224,\"start\":40218},{\"end\":40243,\"start\":40234},{\"end\":40257,\"start\":40252},{\"end\":40271,\"start\":40264},{\"end\":40453,\"start\":40450},{\"end\":40466,\"start\":40460},{\"end\":40479,\"start\":40475},{\"end\":40489,\"start\":40485},{\"end\":40502,\"start\":40499},{\"end\":40515,\"start\":40511},{\"end\":40529,\"start\":40523},{\"end\":40544,\"start\":40538},{\"end\":40709,\"start\":40706},{\"end\":40723,\"start\":40719},{\"end\":40735,\"start\":40731},{\"end\":40748,\"start\":40744},{\"end\":40760,\"start\":40756},{\"end\":40770,\"start\":40766},{\"end\":40783,\"start\":40779},{\"end\":40896,\"start\":40887},{\"end\":40910,\"start\":40905},{\"end\":40927,\"start\":40919},{\"end\":40939,\"start\":40934},{\"end\":40954,\"start\":40948},{\"end\":40965,\"start\":40961},{\"end\":40976,\"start\":40972},{\"end\":40994,\"start\":40984},{\"end\":41124,\"start\":41116},{\"end\":41145,\"start\":41135},{\"end\":41159,\"start\":41152},{\"end\":41173,\"start\":41166},{\"end\":41200,\"start\":41194},{\"end\":41216,\"start\":41206},{\"end\":41230,\"start\":41222},{\"end\":41243,\"start\":41238},{\"end\":41259,\"start\":41254},{\"end\":41272,\"start\":41267},{\"end\":41284,\"start\":41280},{\"end\":41298,\"start\":41291},{\"end\":41310,\"start\":41308},{\"end\":41324,\"start\":41318},{\"end\":41464,\"start\":41458},{\"end\":41480,\"start\":41472},{\"end\":41491,\"start\":41489},{\"end\":41504,\"start\":41500},{\"end\":41519,\"start\":41513},{\"end\":41528,\"start\":41521},{\"end\":41635,\"start\":41618},{\"end\":41656,\"start\":41646},{\"end\":41674,\"start\":41665},{\"end\":41688,\"start\":41681},{\"end\":41701,\"start\":41696},{\"end\":41908,\"start\":41905},{\"end\":41919,\"start\":41917},{\"end\":41929,\"start\":41927},{\"end\":41939,\"start\":41935},{\"end\":41949,\"start\":41946},{\"end\":42205,\"start\":42201},{\"end\":42220,\"start\":42212},{\"end\":42232,\"start\":42228},{\"end\":42246,\"start\":42239},{\"end\":42523,\"start\":42521},{\"end\":42534,\"start\":42531},{\"end\":42549,\"start\":42544},{\"end\":42563,\"start\":42561},{\"end\":42578,\"start\":42575},{\"end\":42597,\"start\":42587},{\"end\":42610,\"start\":42605},{\"end\":42625,\"start\":42619},{\"end\":42753,\"start\":42745},{\"end\":42769,\"start\":42765},{\"end\":42784,\"start\":42779},{\"end\":42788,\"start\":42786},{\"end\":42966,\"start\":42963},{\"end\":42977,\"start\":42974},{\"end\":42991,\"start\":42983},{\"end\":43006,\"start\":42999},{\"end\":43018,\"start\":43013},{\"end\":43039,\"start\":43029},{\"end\":43057,\"start\":43046},{\"end\":43240,\"start\":43234},{\"end\":43259,\"start\":43250},{\"end\":43267,\"start\":43265},{\"end\":43281,\"start\":43277},{\"end\":43295,\"start\":43288},{\"end\":43309,\"start\":43304},{\"end\":43328,\"start\":43318},{\"end\":43344,\"start\":43337},{\"end\":43362,\"start\":43353},{\"end\":43375,\"start\":43370},{\"end\":43390,\"start\":43384},{\"end\":43651,\"start\":43644},{\"end\":43660,\"start\":43656},{\"end\":43677,\"start\":43670},{\"end\":43686,\"start\":43684},{\"end\":43697,\"start\":43693},{\"end\":43710,\"start\":43706},{\"end\":43727,\"start\":43719},{\"end\":43742,\"start\":43737},{\"end\":43870,\"start\":43867},{\"end\":43895,\"start\":43879},{\"end\":43908,\"start\":43901},{\"end\":43928,\"start\":43917},{\"end\":43942,\"start\":43936},{\"end\":43956,\"start\":43950},{\"end\":43969,\"start\":43964},{\"end\":43986,\"start\":43977},{\"end\":44001,\"start\":43996},{\"end\":44013,\"start\":44009},{\"end\":44069,\"start\":44060},{\"end\":44085,\"start\":44077},{\"end\":44099,\"start\":44094},{\"end\":44120,\"start\":44110},{\"end\":44138,\"start\":44127},{\"end\":44151,\"start\":44146},{\"end\":44398,\"start\":44393},{\"end\":44419,\"start\":44407},{\"end\":44432,\"start\":44427},{\"end\":44884,\"start\":44874},{\"end\":44901,\"start\":44893},{\"end\":44912,\"start\":44909},{\"end\":44929,\"start\":44921},{\"end\":44947,\"start\":44939},{\"end\":44958,\"start\":44954},{\"end\":44975,\"start\":44966},{\"end\":44992,\"start\":44983},{\"end\":45007,\"start\":45003},{\"end\":45014,\"start\":45009},{\"end\":45104,\"start\":45101},{\"end\":45114,\"start\":45110},{\"end\":45350,\"start\":45347},{\"end\":45360,\"start\":45356},{\"end\":45444,\"start\":45438},{\"end\":45465,\"start\":45453},{\"end\":45477,\"start\":45473},{\"end\":45493,\"start\":45488},{\"end\":45666,\"start\":45660},{\"end\":45683,\"start\":45673},{\"end\":45698,\"start\":45693},{\"end\":45716,\"start\":45708},{\"end\":45730,\"start\":45724},{\"end\":45748,\"start\":45737},{\"end\":45765,\"start\":45757},{\"end\":45781,\"start\":45774},{\"end\":45936,\"start\":45931},{\"end\":45953,\"start\":45946},{\"end\":45969,\"start\":45963},{\"end\":45988,\"start\":45979},{\"end\":46008,\"start\":45997},{\"end\":46022,\"start\":46016},{\"end\":46032,\"start\":46029},{\"end\":46052,\"start\":46042},{\"end\":46068,\"start\":46061},{\"end\":46087,\"start\":46076},{\"end\":46210,\"start\":46204},{\"end\":46225,\"start\":46219},{\"end\":46244,\"start\":46237},{\"end\":46264,\"start\":46256},{\"end\":46272,\"start\":46269},{\"end\":46289,\"start\":46284},{\"end\":46310,\"start\":46301},{\"end\":46326,\"start\":46317},{\"end\":46337,\"start\":46334},{\"end\":46343,\"start\":46339},{\"end\":46396,\"start\":46387},{\"end\":46415,\"start\":46408},{\"end\":46427,\"start\":46423},{\"end\":46441,\"start\":46434},{\"end\":46462,\"start\":46450},{\"end\":46472,\"start\":46464},{\"end\":46486,\"start\":46481},{\"end\":46498,\"start\":46495},{\"end\":46510,\"start\":46507},{\"end\":46520,\"start\":46515},{\"end\":46524,\"start\":46522},{\"end\":46738,\"start\":46734},{\"end\":46752,\"start\":46750},{\"end\":46764,\"start\":46761},{\"end\":46776,\"start\":46773},{\"end\":46791,\"start\":46786},{\"end\":46804,\"start\":46800},{\"end\":46949,\"start\":46945},{\"end\":46959,\"start\":46957},{\"end\":46971,\"start\":46968},{\"end\":46984,\"start\":46982},{\"end\":46996,\"start\":46993},{\"end\":47014,\"start\":47011},{\"end\":47027,\"start\":47024},{\"end\":47150,\"start\":47146},{\"end\":47161,\"start\":47158},{\"end\":47178,\"start\":47168},{\"end\":47187,\"start\":47185},{\"end\":47195,\"start\":47192},{\"end\":47207,\"start\":47203},{\"end\":47254,\"start\":47250},{\"end\":47263,\"start\":47261},{\"end\":47288,\"start\":47274},{\"end\":47298,\"start\":47294},{\"end\":47310,\"start\":47307},{\"end\":47324,\"start\":47322},{\"end\":47329,\"start\":47326},{\"end\":47528,\"start\":47525},{\"end\":47541,\"start\":47537},{\"end\":47558,\"start\":47548},{\"end\":47573,\"start\":47568},{\"end\":47581,\"start\":47578},{\"end\":47590,\"start\":47588},{\"end\":47602,\"start\":47598},{\"end\":47726,\"start\":47715},{\"end\":47745,\"start\":47734},{\"end\":47758,\"start\":47753},{\"end\":47769,\"start\":47767},{\"end\":47909,\"start\":47906},{\"end\":47926,\"start\":47917},{\"end\":47938,\"start\":47934},{\"end\":47947,\"start\":47943},{\"end\":47960,\"start\":47957},{\"end\":47972,\"start\":47970},{\"end\":47983,\"start\":47980},{\"end\":48091,\"start\":48088},{\"end\":48105,\"start\":48101},{\"end\":48114,\"start\":48112},{\"end\":48129,\"start\":48122},{\"end\":48156,\"start\":48136},{\"end\":48161,\"start\":48158},{\"end\":48316,\"start\":48312},{\"end\":48335,\"start\":48328},{\"end\":48343,\"start\":48340},{\"end\":48354,\"start\":48351},{\"end\":48369,\"start\":48363},{\"end\":48382,\"start\":48378},{\"end\":48399,\"start\":48389},{\"end\":48417,\"start\":48409},{\"end\":48426,\"start\":48424},{\"end\":48434,\"start\":48431},{\"end\":48577,\"start\":48574},{\"end\":48591,\"start\":48588},{\"end\":48607,\"start\":48602},{\"end\":48618,\"start\":48614},{\"end\":48630,\"start\":48625},{\"end\":48644,\"start\":48642},{\"end\":48655,\"start\":48651},{\"end\":48670,\"start\":48666}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":173188048},\"end\":37549,\"start\":37191},{\"attributes\":{\"doi\":\"arXiv:2108.07732\",\"id\":\"b1\"},\"end\":37771,\"start\":37551},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":38023,\"start\":37773},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b3\"},\"end\":38267,\"start\":38025},{\"attributes\":{\"doi\":\"arXiv:2210.06710\",\"id\":\"b4\",\"matched_paper_id\":235399966},\"end\":38741,\"start\":38269},{\"attributes\":{\"doi\":\"arXiv:2210.03849\",\"id\":\"b5\"},\"end\":38965,\"start\":38743},{\"attributes\":{\"doi\":\"arXiv:2210.02875\",\"id\":\"b6\"},\"end\":39189,\"start\":38967},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b7\"},\"end\":39426,\"start\":39191},{\"attributes\":{\"doi\":\"arXiv:2110.14168\",\"id\":\"b8\"},\"end\":39624,\"start\":39428},{\"attributes\":{\"doi\":\"arXiv:2209.15003\",\"id\":\"b9\"},\"end\":39840,\"start\":39626},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":245124124},\"end\":40096,\"start\":39842},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":67855846},\"end\":40443,\"start\":40098},{\"attributes\":{\"doi\":\"arXiv:2211.10435\",\"id\":\"b12\"},\"end\":40616,\"start\":40445},{\"attributes\":{\"doi\":\"arXiv:2305.11738\",\"id\":\"b13\"},\"end\":40819,\"start\":40618},{\"attributes\":{\"doi\":\"arXiv:2103.03874\",\"id\":\"b14\"},\"end\":41030,\"start\":40821},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":428579},\"end\":41186,\"start\":41032},{\"attributes\":{\"doi\":\"arXiv:2001.08361\",\"id\":\"b16\"},\"end\":41401,\"start\":41188},{\"attributes\":{\"doi\":\"arXiv:2205.11916\",\"id\":\"b17\"},\"end\":41564,\"start\":41403},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4894130},\"end\":41777,\"start\":41566},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":252355548},\"end\":42096,\"start\":41779},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12777818},\"end\":42427,\"start\":42098},{\"attributes\":{\"doi\":\"arXiv:2209.14610\",\"id\":\"b21\"},\"end\":42661,\"start\":42429},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":220047831},\"end\":42955,\"start\":42663},{\"attributes\":{\"doi\":\"arXiv:2202.12837\",\"id\":\"b23\"},\"end\":43170,\"start\":42957},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":253237047},\"end\":43555,\"start\":43172},{\"attributes\":{\"doi\":\"arXiv:2203.13474\",\"id\":\"b25\"},\"end\":43778,\"start\":43557},{\"attributes\":{\"doi\":\"arXiv:2112.00114\",\"id\":\"b26\"},\"end\":44049,\"start\":43780},{\"attributes\":{\"doi\":\"arXiv:2303.08774\",\"id\":\"b27\"},\"end\":44321,\"start\":44051},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":232223322},\"end\":44786,\"start\":44323},{\"attributes\":{\"id\":\"b29\"},\"end\":45050,\"start\":44788},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":560565},\"end\":45279,\"start\":45052},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4808444},\"end\":45430,\"start\":45281},{\"attributes\":{\"id\":\"b32\"},\"end\":45590,\"start\":45432},{\"attributes\":{\"id\":\"b33\"},\"end\":45817,\"start\":45592},{\"attributes\":{\"id\":\"b34\"},\"end\":46123,\"start\":45819},{\"attributes\":{\"id\":\"b35\"},\"end\":46379,\"start\":46125},{\"attributes\":{\"id\":\"b36\"},\"end\":46601,\"start\":46381},{\"attributes\":{\"id\":\"b37\"},\"end\":46841,\"start\":46603},{\"attributes\":{\"id\":\"b38\"},\"end\":47064,\"start\":46843},{\"attributes\":{\"id\":\"b39\"},\"end\":47244,\"start\":47066},{\"attributes\":{\"id\":\"b40\"},\"end\":47446,\"start\":47246},{\"attributes\":{\"id\":\"b41\"},\"end\":47638,\"start\":47448},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":241035330},\"end\":47829,\"start\":47640},{\"attributes\":{\"id\":\"b43\"},\"end\":48019,\"start\":47831},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":252762395},\"end\":48228,\"start\":48021},{\"attributes\":{\"id\":\"b45\"},\"end\":48470,\"start\":48230},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":234741852},\"end\":49001,\"start\":48472}]", "bib_title": "[{\"end\":37278,\"start\":37191},{\"end\":37810,\"start\":37773},{\"end\":38328,\"start\":38269},{\"end\":39907,\"start\":39842},{\"end\":40182,\"start\":40098},{\"end\":41099,\"start\":41032},{\"end\":41612,\"start\":41566},{\"end\":41896,\"start\":41779},{\"end\":42194,\"start\":42098},{\"end\":42743,\"start\":42663},{\"end\":43224,\"start\":43172},{\"end\":44385,\"start\":44323},{\"end\":45092,\"start\":45052},{\"end\":45338,\"start\":45281},{\"end\":47708,\"start\":47640},{\"end\":48079,\"start\":48021},{\"end\":48564,\"start\":48472}]", "bib_author": "[{\"end\":37292,\"start\":37280},{\"end\":37308,\"start\":37292},{\"end\":37323,\"start\":37308},{\"end\":37346,\"start\":37323},{\"end\":37358,\"start\":37346},{\"end\":37379,\"start\":37358},{\"end\":37611,\"start\":37597},{\"end\":37627,\"start\":37611},{\"end\":37640,\"start\":37627},{\"end\":37655,\"start\":37640},{\"end\":37675,\"start\":37655},{\"end\":37688,\"start\":37675},{\"end\":37701,\"start\":37688},{\"end\":37713,\"start\":37701},{\"end\":37728,\"start\":37713},{\"end\":37737,\"start\":37728},{\"end\":37823,\"start\":37812},{\"end\":37838,\"start\":37823},{\"end\":37850,\"start\":37838},{\"end\":37867,\"start\":37850},{\"end\":37883,\"start\":37867},{\"end\":37902,\"start\":37883},{\"end\":37922,\"start\":37902},{\"end\":37936,\"start\":37922},{\"end\":37951,\"start\":37936},{\"end\":37966,\"start\":37951},{\"end\":38036,\"start\":38025},{\"end\":38050,\"start\":38036},{\"end\":38062,\"start\":38050},{\"end\":38075,\"start\":38062},{\"end\":38109,\"start\":38075},{\"end\":38123,\"start\":38109},{\"end\":38138,\"start\":38123},{\"end\":38150,\"start\":38138},{\"end\":38167,\"start\":38150},{\"end\":38182,\"start\":38167},{\"end\":38342,\"start\":38330},{\"end\":38356,\"start\":38342},{\"end\":38368,\"start\":38356},{\"end\":38384,\"start\":38368},{\"end\":38398,\"start\":38384},{\"end\":38411,\"start\":38398},{\"end\":38426,\"start\":38411},{\"end\":38440,\"start\":38426},{\"end\":38452,\"start\":38440},{\"end\":38468,\"start\":38452},{\"end\":38487,\"start\":38468},{\"end\":38855,\"start\":38843},{\"end\":38867,\"start\":38855},{\"end\":38883,\"start\":38867},{\"end\":38896,\"start\":38883},{\"end\":38910,\"start\":38896},{\"end\":38924,\"start\":38910},{\"end\":38931,\"start\":38924},{\"end\":39029,\"start\":39014},{\"end\":39042,\"start\":39029},{\"end\":39052,\"start\":39042},{\"end\":39064,\"start\":39052},{\"end\":39080,\"start\":39064},{\"end\":39090,\"start\":39080},{\"end\":39105,\"start\":39090},{\"end\":39121,\"start\":39105},{\"end\":39137,\"start\":39121},{\"end\":39155,\"start\":39137},{\"end\":39212,\"start\":39191},{\"end\":39227,\"start\":39212},{\"end\":39241,\"start\":39227},{\"end\":39256,\"start\":39241},{\"end\":39271,\"start\":39256},{\"end\":39285,\"start\":39271},{\"end\":39298,\"start\":39285},{\"end\":39315,\"start\":39298},{\"end\":39331,\"start\":39315},{\"end\":39351,\"start\":39331},{\"end\":39440,\"start\":39428},{\"end\":39457,\"start\":39440},{\"end\":39476,\"start\":39457},{\"end\":39490,\"start\":39476},{\"end\":39508,\"start\":39490},{\"end\":39527,\"start\":39508},{\"end\":39542,\"start\":39527},{\"end\":39701,\"start\":39685},{\"end\":39720,\"start\":39701},{\"end\":39734,\"start\":39720},{\"end\":39749,\"start\":39734},{\"end\":39763,\"start\":39749},{\"end\":39776,\"start\":39763},{\"end\":39794,\"start\":39776},{\"end\":39806,\"start\":39794},{\"end\":39917,\"start\":39909},{\"end\":39932,\"start\":39917},{\"end\":39946,\"start\":39932},{\"end\":39958,\"start\":39946},{\"end\":39975,\"start\":39958},{\"end\":39989,\"start\":39975},{\"end\":40003,\"start\":39989},{\"end\":40015,\"start\":40003},{\"end\":40029,\"start\":40015},{\"end\":40042,\"start\":40029},{\"end\":40196,\"start\":40184},{\"end\":40210,\"start\":40196},{\"end\":40226,\"start\":40210},{\"end\":40245,\"start\":40226},{\"end\":40259,\"start\":40245},{\"end\":40273,\"start\":40259},{\"end\":40455,\"start\":40445},{\"end\":40468,\"start\":40455},{\"end\":40481,\"start\":40468},{\"end\":40491,\"start\":40481},{\"end\":40504,\"start\":40491},{\"end\":40517,\"start\":40504},{\"end\":40531,\"start\":40517},{\"end\":40546,\"start\":40531},{\"end\":40711,\"start\":40699},{\"end\":40725,\"start\":40711},{\"end\":40737,\"start\":40725},{\"end\":40750,\"start\":40737},{\"end\":40762,\"start\":40750},{\"end\":40772,\"start\":40762},{\"end\":40785,\"start\":40772},{\"end\":40898,\"start\":40883},{\"end\":40912,\"start\":40898},{\"end\":40929,\"start\":40912},{\"end\":40941,\"start\":40929},{\"end\":40956,\"start\":40941},{\"end\":40967,\"start\":40956},{\"end\":40978,\"start\":40967},{\"end\":40996,\"start\":40978},{\"end\":41126,\"start\":41101},{\"end\":41147,\"start\":41126},{\"end\":41161,\"start\":41147},{\"end\":41175,\"start\":41161},{\"end\":41202,\"start\":41188},{\"end\":41218,\"start\":41202},{\"end\":41232,\"start\":41218},{\"end\":41245,\"start\":41232},{\"end\":41261,\"start\":41245},{\"end\":41274,\"start\":41261},{\"end\":41286,\"start\":41274},{\"end\":41300,\"start\":41286},{\"end\":41312,\"start\":41300},{\"end\":41326,\"start\":41312},{\"end\":41466,\"start\":41450},{\"end\":41482,\"start\":41466},{\"end\":41493,\"start\":41482},{\"end\":41506,\"start\":41493},{\"end\":41521,\"start\":41506},{\"end\":41530,\"start\":41521},{\"end\":41637,\"start\":41614},{\"end\":41658,\"start\":41637},{\"end\":41676,\"start\":41658},{\"end\":41690,\"start\":41676},{\"end\":41703,\"start\":41690},{\"end\":41709,\"start\":41703},{\"end\":41910,\"start\":41898},{\"end\":41921,\"start\":41910},{\"end\":41931,\"start\":41921},{\"end\":41941,\"start\":41931},{\"end\":41951,\"start\":41941},{\"end\":42207,\"start\":42196},{\"end\":42222,\"start\":42207},{\"end\":42234,\"start\":42222},{\"end\":42248,\"start\":42234},{\"end\":42525,\"start\":42517},{\"end\":42536,\"start\":42525},{\"end\":42551,\"start\":42536},{\"end\":42565,\"start\":42551},{\"end\":42580,\"start\":42565},{\"end\":42599,\"start\":42580},{\"end\":42612,\"start\":42599},{\"end\":42627,\"start\":42612},{\"end\":42755,\"start\":42745},{\"end\":42771,\"start\":42755},{\"end\":42786,\"start\":42771},{\"end\":42790,\"start\":42786},{\"end\":42968,\"start\":42957},{\"end\":42979,\"start\":42968},{\"end\":42993,\"start\":42979},{\"end\":43008,\"start\":42993},{\"end\":43020,\"start\":43008},{\"end\":43041,\"start\":43020},{\"end\":43059,\"start\":43041},{\"end\":43242,\"start\":43226},{\"end\":43261,\"start\":43242},{\"end\":43269,\"start\":43261},{\"end\":43283,\"start\":43269},{\"end\":43297,\"start\":43283},{\"end\":43311,\"start\":43297},{\"end\":43330,\"start\":43311},{\"end\":43346,\"start\":43330},{\"end\":43364,\"start\":43346},{\"end\":43377,\"start\":43364},{\"end\":43392,\"start\":43377},{\"end\":43653,\"start\":43639},{\"end\":43662,\"start\":43653},{\"end\":43679,\"start\":43662},{\"end\":43688,\"start\":43679},{\"end\":43699,\"start\":43688},{\"end\":43712,\"start\":43699},{\"end\":43729,\"start\":43712},{\"end\":43744,\"start\":43729},{\"end\":43872,\"start\":43859},{\"end\":43897,\"start\":43872},{\"end\":43910,\"start\":43897},{\"end\":43930,\"start\":43910},{\"end\":43944,\"start\":43930},{\"end\":43958,\"start\":43944},{\"end\":43971,\"start\":43958},{\"end\":43988,\"start\":43971},{\"end\":44003,\"start\":43988},{\"end\":44015,\"start\":44003},{\"end\":44071,\"start\":44051},{\"end\":44087,\"start\":44071},{\"end\":44101,\"start\":44087},{\"end\":44122,\"start\":44101},{\"end\":44140,\"start\":44122},{\"end\":44153,\"start\":44140},{\"end\":44163,\"start\":44153},{\"end\":44400,\"start\":44387},{\"end\":44421,\"start\":44400},{\"end\":44434,\"start\":44421},{\"end\":44886,\"start\":44864},{\"end\":44903,\"start\":44886},{\"end\":44914,\"start\":44903},{\"end\":44931,\"start\":44914},{\"end\":44949,\"start\":44931},{\"end\":44960,\"start\":44949},{\"end\":44977,\"start\":44960},{\"end\":44994,\"start\":44977},{\"end\":45009,\"start\":44994},{\"end\":45016,\"start\":45009},{\"end\":45106,\"start\":45094},{\"end\":45116,\"start\":45106},{\"end\":45352,\"start\":45340},{\"end\":45362,\"start\":45352},{\"end\":45446,\"start\":45432},{\"end\":45467,\"start\":45446},{\"end\":45479,\"start\":45467},{\"end\":45495,\"start\":45479},{\"end\":45668,\"start\":45655},{\"end\":45685,\"start\":45668},{\"end\":45700,\"start\":45685},{\"end\":45718,\"start\":45700},{\"end\":45732,\"start\":45718},{\"end\":45750,\"start\":45732},{\"end\":45767,\"start\":45750},{\"end\":45783,\"start\":45767},{\"end\":45938,\"start\":45924},{\"end\":45955,\"start\":45938},{\"end\":45971,\"start\":45955},{\"end\":45990,\"start\":45971},{\"end\":46010,\"start\":45990},{\"end\":46024,\"start\":46010},{\"end\":46034,\"start\":46024},{\"end\":46054,\"start\":46034},{\"end\":46070,\"start\":46054},{\"end\":46089,\"start\":46070},{\"end\":46212,\"start\":46198},{\"end\":46227,\"start\":46212},{\"end\":46246,\"start\":46227},{\"end\":46266,\"start\":46246},{\"end\":46274,\"start\":46266},{\"end\":46291,\"start\":46274},{\"end\":46312,\"start\":46291},{\"end\":46328,\"start\":46312},{\"end\":46339,\"start\":46328},{\"end\":46345,\"start\":46339},{\"end\":46398,\"start\":46381},{\"end\":46417,\"start\":46398},{\"end\":46429,\"start\":46417},{\"end\":46443,\"start\":46429},{\"end\":46464,\"start\":46443},{\"end\":46474,\"start\":46464},{\"end\":46488,\"start\":46474},{\"end\":46500,\"start\":46488},{\"end\":46512,\"start\":46500},{\"end\":46522,\"start\":46512},{\"end\":46526,\"start\":46522},{\"end\":46740,\"start\":46730},{\"end\":46754,\"start\":46740},{\"end\":46766,\"start\":46754},{\"end\":46778,\"start\":46766},{\"end\":46793,\"start\":46778},{\"end\":46806,\"start\":46793},{\"end\":46951,\"start\":46941},{\"end\":46961,\"start\":46951},{\"end\":46973,\"start\":46961},{\"end\":46986,\"start\":46973},{\"end\":46998,\"start\":46986},{\"end\":47004,\"start\":46998},{\"end\":47016,\"start\":47004},{\"end\":47029,\"start\":47016},{\"end\":47152,\"start\":47139},{\"end\":47163,\"start\":47152},{\"end\":47180,\"start\":47163},{\"end\":47189,\"start\":47180},{\"end\":47197,\"start\":47189},{\"end\":47209,\"start\":47197},{\"end\":47256,\"start\":47246},{\"end\":47265,\"start\":47256},{\"end\":47290,\"start\":47265},{\"end\":47300,\"start\":47290},{\"end\":47312,\"start\":47300},{\"end\":47326,\"start\":47312},{\"end\":47331,\"start\":47326},{\"end\":47530,\"start\":47519},{\"end\":47543,\"start\":47530},{\"end\":47560,\"start\":47543},{\"end\":47575,\"start\":47560},{\"end\":47583,\"start\":47575},{\"end\":47592,\"start\":47583},{\"end\":47604,\"start\":47592},{\"end\":47728,\"start\":47710},{\"end\":47747,\"start\":47728},{\"end\":47760,\"start\":47747},{\"end\":47771,\"start\":47760},{\"end\":47911,\"start\":47901},{\"end\":47928,\"start\":47911},{\"end\":47940,\"start\":47928},{\"end\":47949,\"start\":47940},{\"end\":47962,\"start\":47949},{\"end\":47974,\"start\":47962},{\"end\":47985,\"start\":47974},{\"end\":48093,\"start\":48081},{\"end\":48107,\"start\":48093},{\"end\":48116,\"start\":48107},{\"end\":48131,\"start\":48116},{\"end\":48158,\"start\":48131},{\"end\":48163,\"start\":48158},{\"end\":48318,\"start\":48306},{\"end\":48337,\"start\":48318},{\"end\":48345,\"start\":48337},{\"end\":48356,\"start\":48345},{\"end\":48371,\"start\":48356},{\"end\":48384,\"start\":48371},{\"end\":48401,\"start\":48384},{\"end\":48419,\"start\":48401},{\"end\":48428,\"start\":48419},{\"end\":48436,\"start\":48428},{\"end\":48579,\"start\":48566},{\"end\":48593,\"start\":48579},{\"end\":48609,\"start\":48593},{\"end\":48620,\"start\":48609},{\"end\":48632,\"start\":48620},{\"end\":48646,\"start\":48632},{\"end\":48657,\"start\":48646},{\"end\":48672,\"start\":48657}]", "bib_venue": "[{\"end\":37443,\"start\":37379},{\"end\":37466,\"start\":37445},{\"end\":37595,\"start\":37551},{\"end\":38015,\"start\":37966},{\"end\":38246,\"start\":38198},{\"end\":38589,\"start\":38503},{\"end\":38841,\"start\":38743},{\"end\":39012,\"start\":38967},{\"end\":39406,\"start\":39367},{\"end\":39604,\"start\":39558},{\"end\":39683,\"start\":39626},{\"end\":40086,\"start\":40042},{\"end\":40337,\"start\":40273},{\"end\":40360,\"start\":40339},{\"end\":40596,\"start\":40562},{\"end\":40697,\"start\":40618},{\"end\":40881,\"start\":40821},{\"end\":41180,\"start\":41175},{\"end\":41381,\"start\":41342},{\"end\":41448,\"start\":41403},{\"end\":41770,\"start\":41709},{\"end\":42028,\"start\":41951},{\"end\":42335,\"start\":42248},{\"end\":42348,\"start\":42337},{\"end\":42515,\"start\":42429},{\"end\":42877,\"start\":42790},{\"end\":43150,\"start\":43075},{\"end\":43478,\"start\":43392},{\"end\":43637,\"start\":43557},{\"end\":43857,\"start\":43780},{\"end\":44264,\"start\":44195},{\"end\":44607,\"start\":44465},{\"end\":44862,\"start\":44788},{\"end\":45202,\"start\":45116},{\"end\":45423,\"start\":45362},{\"end\":45570,\"start\":45511},{\"end\":45653,\"start\":45592},{\"end\":45922,\"start\":45819},{\"end\":46196,\"start\":46125},{\"end\":46581,\"start\":46542},{\"end\":46728,\"start\":46603},{\"end\":46939,\"start\":46843},{\"end\":47137,\"start\":47066},{\"end\":47425,\"start\":47347},{\"end\":47517,\"start\":47448},{\"end\":47823,\"start\":47771},{\"end\":47899,\"start\":47831},{\"end\":48222,\"start\":48163},{\"end\":48304,\"start\":48230},{\"end\":48834,\"start\":48672},{\"end\":48847,\"start\":48836},{\"end\":37517,\"start\":37468},{\"end\":38662,\"start\":38591},{\"end\":40411,\"start\":40362},{\"end\":42092,\"start\":42030},{\"end\":42422,\"start\":42350},{\"end\":42951,\"start\":42879},{\"end\":43551,\"start\":43480},{\"end\":44269,\"start\":44266},{\"end\":44736,\"start\":44609},{\"end\":45275,\"start\":45204},{\"end\":48996,\"start\":48849}]"}}}, "year": 2023, "month": 12, "day": 17}