{"id": 4578162, "updated": "2023-10-02 09:41:40.325", "metadata": {"title": "Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction", "authors": "[{\"first\":\"Huangying\",\"last\":\"Zhan\",\"middle\":[]},{\"first\":\"Ravi\",\"last\":\"Garg\",\"middle\":[]},{\"first\":\"Chamara\",\"last\":\"Weerasekera\",\"middle\":[\"Saroj\"]},{\"first\":\"Kejie\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Harsh\",\"last\":\"Agarwal\",\"middle\":[]},{\"first\":\"Ian\",\"last\":\"Reid\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2018, "month": 3, "day": 11}, "abstract": "Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1803.03893", "mag": "2962816904", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhanGWLA018", "doi": "10.1109/cvpr.2018.00043"}}, "content": {"source": {"pdf_hash": "db7e6a2b8110c193d5e3f0bc96d55e21696712b0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1803.03893v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1803.03893", "status": "GREEN"}}, "grobid": {"id": "380aed60d429df9bea4ba78968001407d98edbc3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/db7e6a2b8110c193d5e3f0bc96d55e21696712b0.txt", "contents": "\nUnsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction\n\n\nHuangying Zhan \nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nRavi Garg \nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nChamara Saroj Weerasekera \nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nKejie Li \nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nHarsh Agarwal \nIndian Institute of Technology (BHU)\n\n\nIan Reid ian.reid@adelaide.edu.auharsh.agarwal.eee14@iitbhu.ac.in \nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nUnsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction\n\nDespite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, realworld scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/ Depth-VO-Feat.\n\nIntroduction\n\nUnderstanding the 3D structure of a scene from a single image is a fundamental question in machine perception. The related problem of inferring ego-motion from a sequence of images is likewise a fundamental problem in robotics, known as visual odometry estimation. These two problems are crucial in robotic vision research since accurate estimation of depth and odometry based on images has many important applications, most notably for autonomous vehicles.\n\nWhile both problems have been the subject of research in robotic vision since the origins of the discipline, with numerous geometric solutions proposed, in recent times a number of works have cast depth estimation and visual odometry as supervised learning problems [1] [5][21] [22]. These methods attempt to predict depth or odometry using models that have been trained from a large dataset with ground truth data. However, these annotations are expensive to obtain, e.g. expensive laser or depth camera to collect depths. In a recent work Garg et al. [6] recognised that these tasks are amenable to an unsupervised framework where the authors propose to use photometric warp error as a self-supervised signal to train a convolutional neural network (ConvNet / CNN) for the single view depth estimation. Following [6] methods like [9] [19] [41] use the photometric error based supervision to learn depth estimators comparable to that of fully supervised methods. Specifically, [6] and [9] use the photometric warp error between left-right images in a stereo pair to learn depth. Recognising the generality of the idea, [44] uses monocular sequences to jointly train two neural networks for depth and odometry estimation. However, relying on the two frame visual odometry estimation framework, [44] suffers from the per frame scale-ambiguity issue, in that an actual metric scaling of the camera translations is missing and only direction is known. Having a good estimate of the translation scale per-frame is crucial for the success of any Simultaneous Localization and Mapping (SLAM) system. Accurate camera tracking in most monocular SLAM frameworks relies on keeping the scale consistency of the map across multiple images which is enforced using a single scale map. In absence of a global map for tracking, an expensive bundle adjustment over the per-frame scale parameter or additional assumptions like constant camera height from the already detected ground plane becomes essential for accurate tracking [34].\n\nIn this work, we propose a framework which jointly learns a single view depth estimator and monocular odometry estimator using stereo video sequences (as shown in Figure 1) for training. Our method can be understood as unsupervised learning for depth estimation and semi-supervised for pose which is known between stereo pairs. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward-backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale (set by the stereo baseline). Inference (i.e. depth and odometry estimation) without any scale ambiguity is then possible using a single camera for pure frame to frame VO estimation without any need for mapping.\n\nMoreover, while the previous works have shown the efficacy of using the photometric warp error as a selfsupervision signal, a simple warp of image intensities or colors carries its own assumptions about brightness/color consistency, and must also be accompanied by a regularization to generate \"sensible\" warps when the photometric information is ambiguous, such as in uniformly colored regions (see Sec.3.3). We propose an additional deep feature reconstruction loss which takes contextual information into consideration rather than per pixel color matching alone.\n\nIn summary, we make the following contributions: (i) an unsupervised framework for jointly learning a depth estimator and visual odometry estimator that does not suffer from the scale ambiguity; (ii) takes advantage of the full set of constraints available from spatial and temporal image pairs to improve upon prior art on single view depth estimations; (iii) produces the state-of-the-art frame-to-frame odometry results that significantly improve on [44] and are on par with geometric methods; (iv) uses a novel feature reconstruction loss in addition to the color intensity based image reconstruction loss which improves the depth and odometry estimation accuracy significantly.\n\n\nRelated Work\n\nHumans are capable of reasoning the relative depth of pixels in an image and perceive ego-motion given two images, but both single view depth estimation and two frame visual odometry are challenging problems. Avoiding visual learning, localization and 3D reconstruction in computer vision was considered a purely geometric problem for decades. While prior to deep learning graphical models based learning methods [32] [33] were prevalent examples for single view reconstructions, methods based on the epipolar geometry were to the fore for two view odometry. While it is possible to estimate the relative pose between two frames based only on the data within those two frames up-to a scale (see e.g., [24], the \"gold-standard\" for geometric ego-motion estimation to date is based on a batch bundle adjustment of pose and scene structure [35], or on online Visual SLAM technqiues [4]). After the surge of convolutional neural networks, both depth and visual odometry estimation problem have been attempted with deep learning methods.\n\nSupervised methods Deep learning based depth estimation starts with Eigen et al. [5] which is the first work estimating depth with ConvNets. They used a multi-scale deep network and scale-invariant loss for depth estimation. Liu et al. [21] [22] formulated depth estimation as a continuous conditional random field learning problem. Laina et al. [20] proposed a residual network using fully convolutional architecture to model the mapping between monocular image and depth map. They also introduced reverse Huber loss and newly designed up-sampling modules. Kendell et al. [17] proposed an end-to-end learning framework to predict disparity from a stereo pair. In particular, they propose to use an explicit feature matching step as a layer in the network to create the cost-volume matching two images, which is then regularized to predict the state-of-the-art disparities for outdoor stereo sequences on KITTI dataset.\n\nFor odometry, Agrawal et al. [1] proposed a visual feature learning algorithm which aims at learning good visual features. Instead of learning features from a classification task (e.g. ImageNet [31]), [1] learns features from an egomotion estimation task. The model is capable to estimate relative camera poses. Wang et al. [38] presented a recurrent ConvNet architecture for learning monocular odometry from video sequences.\n\nUmmenhofer et al. [36] proposed an end-to-end visual odometry and depth estimation network by formulating structure from motion as a supervised learning problem.\n\nHowever, the work is highly supervised: not only does it require depth and camera motion ground truths, in addition the surface normals and optical flow between images are also required.\n\nUnsupervised or semi-supervised methods Recent works suggest that unsupervised pipeline for learning depth is possible from stereo image pairs using a photometric warp loss to replace a loss based on ground truth depth. Garg et al. [6] used binocular stereo pairs (for which the inter-camera transformation is known) and trained a network to predict the depth that minimises the photometric difference between the true right image and one synthesized by warping the left image into the right's viewpoint, using the predicted depth. Godard et al. [9] made improvements to the depth estimation by introducing a symmetric leftright consistency criterion and better stereo loss function. [19] proposed a semi-supervised learning framework by using both sparse depth maps for supervised learning and dense photometric error for unsupervised learning.\n\nAn obvious extension to the above framework is to use structure-from-motion techniques to estimate the interframe motion (optic flow) [15] instead of depth using the known stereo geometry. But in fact it is possible to go further and to use deep networks also to estimate the camera ego-motion, as shown very recently by [44] and [37], both of which use a photometric error for supervising a monocular depth and ego-motion estimation system. Similar to other monocular frameworks, [44] and [37] suffer from scaling ambiguity issue.\n\nLike [6][9], in our work we use stereo pairs for training, since this avoids issues with the depth-speed ambiguity that exist in monocular 3D reconstruction. In addition we jointly train a network to also estimate ego-motion from a pair of images. This allows us to enforce both the temporal and stereo constraints to improve our depth estimation in a joint framework.\n\nAll of the unsupervised depth estimation methods rely on photo-consistency assumption which gets violated often in practice. To cope with that [6][44] use robust norms like L1 norm of the warp error. [9] uses hand crafted features like SSIM [39]. Other handcrafted features like SIFT [25], HOG [3], ORB [30] are all usable and can be explored in unsupervised learning framework for robust warping loss. More interestingly, one can learn good features specifically for the task of matching. LIFT [42] and MC-CNN [43] learn a similarity measure on small image patches while [40][2] learns fully convolutional features good for matching. In our work, we compare the following features for their potential for robust warp error minimization: standard RGB photo-consistency; ImageNet features (conv1); features from [40]; features from a \"self-supervised\" version of [40]; and features derived from our depth network.\n\n\nMethod\n\nThis section describes our framework (shown in Figure 2) for jointly learning a single view depth ConvNet (CNN D ) and a visual odometry ConvNet (CNN V O ) from stereo sequences. The stereo sequences learning framework overcomes the scaling ambiguity issue with monocular sequences, and enables the system to take advantage of both left-right (spatial) and forward-backward (temporal) consistency checks.\n\n\nImage reconstruction as supervision\n\nThe fundamental supervision signal in our framework comes from the task of image reconstruction. For two nearby views, we are able to reconstruct the reference view from the live view, given that the depth of the reference view and relative camera pose between two views are known. Since the depth and relative camera pose can be estimated by a ConvNet, the inconsistency between the real and the reconstructed view allows the training of the ConvNet. However, a monocular framework without extra constraints [44] suffers from the scaling ambiguity issue. Therefore, we propose a stereo framework which constrains the scene depth and relative camera motion to be in a common, real-world scale, given an extra constraint set by the known stereo baseline.\n\nIn our proposed framework using stereo sequences, for each training instance, we have a temporal pair (I L,t1 and I L,t2 ) and a stereo pair (I L,t2 and I R,t2 ), where I L,t2 is the reference view while I L,t1 and I R,t2 are the live views. We can synthesize two reference views, I L,t1 and I R,t2 , from I L,t1 and I R,t2 , respectively. The synthesis process can be represented by,\nI L,t1 = f (I L,t1 , K, T t2\u2192t1 , D L,t2 )(1)I R,t2 = f (I R,t2 , K, T L\u2192R , D L,t2 ).(2)\nwhere f (.) is a synthesis function defined in Sec.3.2; D L,t2 denotes the depth map of the reference view; T L\u2192R and T t2\u2192t1 are the relative camera pose transformations between the reference view and the live views; and K denotes the known camera intrinsic matrix. Note that D L,t2 is mapped from\nI L,t2 via CNN D while T t2\u2192t1 is mapped from [I L,t1 , I L,t2 ] via CNN V O .\nThe image reconstruction loss between the synthesized views and the real views are computed as a supervision signal to train CNN D and CNN V O . The image construction loss is represented by,\nL ir = p |I L,t2 (p) \u2212 I L,t1 (p)| + |I L,t2 (p) \u2212 I R,t2 (p)| .(3)\nThe effect of using stereo sequences instead of monocular sequences is two-fold. The known relative pose T L\u2192R be- tween the stereo pair constrains CNN D and CNN V O to estimate depths and relative pose between the temporal pair in a real-world scale. As a result, our model is able to estimate single view depths and two-view odometry without the scaling ambiguity issue at test time. Second, in addition to stereo pairs with only one live view, the temporal pair provides a second live view for the reference view. The multi-view scenario takes advantage of the full set of constraints available from the stereo and temporal image pairs. In this section, we describe an unsupervised framework that learns depth estimation and visual odometry without scaling ambiguity issue using stereo video sequences.\n\n\nDifferentiable geometry modules\n\nAs indicated in Eqn.1 -2, an important function in our learning framework is the synthesis function, f (.). The function consists two differentiable operations which allow gradient propagation for the training of the ConvNet. The two operations are epipolar geometry transformation and warping. The former defines the correspondence between pixels in two views while the latter synthesize an image by warping a live view.\n\nLet p L,t2 be the homogeneous coordinates of a pixel in the reference view. We can obtain p L,t2 's projected coordinates onto the live views using epipolar geometry, similar to [10,44]. The projected coordinates are obtained by\np R,t2 = KT L\u2192R D L,t2 (p L,t2 )K \u22121 p L,t2(4)p L,t1 = KT t2\u2192t1 D L,t2 (p L,t2 )K \u22121 p L,t2 ,(5)\nwhere p R,t2 and p L,t1 are the projected coordinates on I R,t2 and I L,t1 respectively. Note that D L,t2 (p L,t2 ) is the depth at position p L,t2 ; T \u2208 SE3 is a 4x4 transformation matrix defined by 6 parameters, in which a 3D vector u \u2208 so3 is an axis-angle representation and a 3D vector v \u2208 R 3 represents translations.\n\nAfter getting the projected coordinates from Eqn.4 -5, new reference frames can be synthesized from the live frames using the differentiable bilinear interpolation mechanism (warping) proposed in [14].\n\n\nFeature reconstruction as supervision\n\nThe stereo framework we proposed above implicitly assumes that the scene is Lambertian, so that the brightness is constant regardless the observer's angle of view. This condition implies that the image reconstruction loss is meaningful for training the ConvNets. Any violation of the assumption can potentially corrupt the training process by propagating the wrong gradient back to the ConvNets. To improve the robustness of our framework, we propose a feature reconstruction loss: instead of using 3-channel color intensity information solely (image reconstruction loss), we explore the use of dense features as an additional supervision signal.\n\nLet F L,t2 , F L,t1 and F R,t2 be the corresponding dense feature representations of I L,t2 , I L,t1 and I R,t2 respectively. Similar to the image synthesis process, two reference views, F L,t1 and F R,t2 , can be synthesized from F L,t1 and F R,t2 , respectively. The synthesis process can be represented by,\nF L,t1 = f (F L,t1 , K, T t2\u2192t1 , D L,t2 )(6)F R,t2 = f (F R,t2 , K, T L\u2192R , D L,t2 ).(7)\nThen, the feature reconstruction loss can be formulated as,\nL f r = p |F L,t2 (p) \u2212 F L,t1 (p)|+ p |F L,t2 (p) \u2212 F R,t2 (p)|(8)\nIn this work, we explore four possible dense features, as detailed in Section 4.3.\n\n\nTraining loss\n\nAs introduced in Sec.3.1 and Sec.3.3, the main supervision signal in our framework comes from the image reconstruction loss while the feature reconstruction loss acts as an auxiliary supervision. Furthermore, similar to [6] [44][9], we have a depth smoothness loss which encourages the predicted depth to be smooth.\n\nTo obtain a smooth depth prediction, following the approach adopted by [12][9], we encourage depth to be smooth locally by introducing an edge-aware smoothness term. The depth discontinuity is penalized if image continuity is showed in the same region. Otherwise, the penalty is small for discontinued depths. The edge-aware smoothness loss is formulate as\nL ds = W,H m,n |\u2202 x D m,n |e \u2212|\u2202xIm,n| + |\u2202 y D m,n |e \u2212|\u2202yIm,n| ,(9)\nwhere \u2202 x (.) and \u2202 y (.) are gradients in horizontal and vertical direction respectively. Note the D m,n is inverse depth in the above regularization.\n\nThe final loss function becomes\nL = \u03bb ir L ir + \u03bb f r L f r + \u03bb ds L ds ,(10)\nwhere \u03bb ir , \u03bb f r and \u03bb ds are the loss weightings for each loss term.\n\n\nNetwork architecture\n\nDepth estimation Our depth ConvNet is composed of two parts, encoder and decoder. For the encoder, we adopt the convolutional network in a variant of ResNet50 [11] with half filters (ResNet50-1by2) for the sake of computation cost. The ResNet50-1by2 contains less than 7 million parameters which is around one fourth of the original ResNet50. For the decoder network, the decoder firstly converts the encoder output (1024-channel feature maps) into a single channel feature map using a 1x1 kernel, followed by conventional bilinear upsampling kernels with skip-connections. Similar to [23][6] [9], the decoder uses skip-connections to fuse low-level features from different stages of the encoder. We use ReLU activation after the last prediction layer to ensure positive prediction comes from the depth ConvNet. For the output of the depth Con-vNet, we design our framework to predict inverse depth instead of depth. However, the ReLU activation may cause zero estimation which results in infinite depth. Therefore, we convert the predicted inverse depth to depth by D = 1/(D inv + 10 \u22124 ).\n\nVisual odometry The visual odometry ConvNet is designed to take two concatenated views along the color channels as input and output a 6D vector [u, v] \u2208 se3, which is then converted to a 4x4 transformation matrix. The network is composed of 6 stride-2 convolutions followed by 3 fullyconnected layers. The last fully-connected layer gives the 6D vector, which defines the transformation from reference view to live view T ref \u2192live .\n\n\nExperiments\n\nIn this section we show extensive experiments for evaluating the performance of our proposed framework. We favorably compare our approach on KITTI dataset [8] [7] with prior art on both single view depth and visual odometry estimation. Additionally, we perform a detailed ablation study on our framework to show that using temporal consistency while training and use of learned deep features along with color consistency both improves the single view depth predictions. Finally, we show two variants of deep features and the corresponding effect, which we show examples of using deep features for dense matching.\n\nWe train all our CNNs with the Caffe [16] framework. We use Adam optimizer with the proposed optimization settings in [18] with [\u03b2 1 , \u03b2 2 , ] = [0.9, 0.999, 10 \u22128 ]. The initial learning rate is 0.001 for all the trained network, which we decrease manually when the training loss converges. For the loss weighting in our final loss function, we empirically find that the combination [\u03bb ir , \u03bb f r , \u03bb ds ] = [1, 0.1, 10] results in a stable training. No data augmentation is involved in our work.\n\nOur system is trained mainly in KITTI dataset [7][8]. The dataset contains 61 video sequences with 42,382 rectified stereo pairs, with the original image size being 1242x375 pixels. However, we use image size of 608x160 in our training setup for the sake of computation cost. We use two different splits of the KITTI dataset for evaluating estimated ego-motion and depth. For single view depth estimation, we follow the Eigen split provided by [5] for fair comparisons with [6,9,5,22]. On the other hand, in order to evaluate our visual odometry performance and compare to prior approaches, we follow [44] by training both the depth and pose network on the official KITTI Odometry training set. Note that there are overlapping scenes between two splits (i.e. some testing scenes of Eigen Split are included in the training scenes of Odometry Split, and vice versa). Therefore, finetuning/testing models trained in any split to another split is not allowable/sensible. The detail about both splits are: Eigen Split Eigen et al. [5] select 697 images from 28 sequences as test set for single view depth evaluation. The remaining 33 scenes contains 23,488 stereo pairs for training. We follow this setup and form 23,455 temporal stereo pairs. Odometry Split The KITTI Odometry Split [8] contains 11 driving sequences with publicly available ground truth camera poses. We follow [44] to train our system on the Odometry Split (no finetuning from Eigen Split is performed). The split in which sequences 00-08 (sequence 03 is not available in KITTI Raw Data) are used for training while 09-10 are used for evaluation. The training set contains 8 sequences with 19,600 temporal stereo pairs.\n\nFor each dataset split, we form temporal pairs by choosing frame I t as the live frame while frame I t+1 as the reference frame -to which the live frame is warped. The reason for this choice is that as the mounted camera in KITTI moves forward, most pixels in I t+1 have correspondence in Method Seq. 09\n\nSeq. 10 t err (%) r err ( \u2022 /100m) t err (%) r err ( \u2022 /100m) ORB-SLAM (LC) [26] 16  I t giving us a better warping error.\n\n\nVisual odometry results\n\nWe use the Odometry Split mentioned above to evaluate the performance of our frame to frame odometry estimation network. The result is compared with the monocular training based network [44] and a popular SLAM system -ORB-SLAM [26] (with and without loop closure) as very strong baselines. Both of the ORB-SLAM versions use local bundle adjustment and more importantly a single scale map to assist the tracking. We ignore the frames (First 9 and 30 respectively) from the sequences (09 and 10) for which ORB-SLAM fails to bootstrap with reliable camera poses due to lack of good features and large rotations. Following the KITTI Visual Odometry dataset evaluation criterion we use possible sub-sequences of length (100, 200, ... , 800) meters and report the average translational and rotational errors for the testing sequence 09 and 10 in Table 1.\n\nAs ORB-SLAM suffers from a single depth-translation scale ambiguity for the whole sequence, we align the ORB-SLAM trajectory with ground-truth by optimizing the map scale following standard protocol. For our method, we simply integrate the estimated frame-to-frame camera poses over the entire sequence without any post processing. Frame-to-frame pose estimation of [44] only avails small 5frame long tracklets, each of which is already aligned independently with the ground-truth by fixing translation scales. This translation normalization leaves [44]'s error to only in- dicate the relative translation magnitudes error over small sequences. As the KITTI sequences are recorded by camera mounted on a car which mostly move forward, even average 6DOF motion as reported in [44] overperforms frameto-frame odometry methods (ORB-SLAM when used only on 5 frames does not bootstrap mapping). Nonetheless we simply integrate the aligned tracklets to estimate the full trajectory for [44] and evaluate. It is important to note that this evaluation protocol is highly disadvantageous to the proposed method as no scope for correcting the drift or translation scale is permitted. A visual comparison of the estimated trajectories for all the methods can be seen in Figure  3.\n\nAs can be seen in Table 1, our stereo based odometry learning method outperforms monocular learning method [44] by a large margin even without any further postprocessing to fix translation scales. Our method is able to give comparable odometry results on sequence 09 to that of the full ORB-SLAM and respectable trajectory for sequence 10 on which larger error in our frame to frame rotation estimation leads to a much larger gradual drift which should be fixed by bundle adjustment.\n\nTo further compare the effect of bundle adjustment, we evaluate the average errors for different translation bins and report the result for sequence 09 in Figure 4. It can be seen clearly that both our method and [44] are better than ORB-SLAM when the translation magnitude is small. As translation magnitude increases, the simple integration of frame to frame VO starts drifting gradually, which suggests a clear advantage of a map based tracking over frame to frame VO without bundle adjustment.\n\n\nDepth estimation results\n\nWe use the Eigen Split to evaluate our system and compare the results with various state of the art depth estimation methods. Following the evaluation protocol proposed in [9] which uses the same crop as [6], we use both the 50m and 80m threshold of maximum depth for evaluation and report all standard error measures in Table 2 [5]) are evaluated on the cropped region from [9]. For the supervision, \"Depth\" means ground truth depth is used in the method; \"Mono.\" means monocular sequences are used in the training; \"Stereo\" means stereo sequences with known stereo camera poses in the training.   sual examples in Figure 5. As shown in [6], photometric stereo based training with AlexNet-FCN architecture and Horn and Schunck [13] loss already gave more accurate results than the state of the art supervised methods [5][22] on KITTI. For fair comparison of [6] with other methods we evaluate the results reported by the authors publicly with 80m cap on maximum depth. All methods using stereo for training are substantially better than [44] which is using only monocular training. Benefited by the feature based reconstruction loss and additional warp error via odometry network, our method outperforms both [6] and [9] with reasonable margin. It is important to note that unlike [9] leftright consistency, data augmentation, run-time shuffle, robust similarity measure like SSIM [39] are not used to train our network and should lead to further improvement. coder) trained on the stereo pairs with the loss described in Sec.3.4 which closely follows [6] (GitHub version). When we train the pose network jointly with the depth network, we get a slight improvement in depth estimation accuracy. Using features from ImageNet feature (conv1 features from pretrained ResNet50-1by-2) improves depth estimation accuracy slightly. In addition, using features from an off-theshelf image descriptor [40] gives a further boost. However, [40] is trained using NYUv2 dataset [27] (ground truth poses and depths are required) so we follow [40] to train an image descriptor using KITTI dataset but using the estimated poses and depths generated from Method \"Temporal\" as pseudo ground truths. Using the features extracted from the self-supervised descriptor (KITTI Feat.) gives a comparable result with that of [40]. The system having all three components (Stereo + Temporal + NYUv2 Feat.) performs best as can be seen in the top part of Table 3.\n\n\nInput image\n\n\nAblation studies\n\nAs most other unsupervised depth estimation methods use a convolutional encoder with deconvnet architecture like [28] [29] for dense predictions, we also experimented with learnable deconv architecture with the ResNet50-1by2 as encoder -learnable upsampler as decoder setup. The results in the bottom part of the table reflects that overall performance of this Baseline2 was slightly inferior to the first baseline. To improve the performance of this baseline, we explore the use of deep features extracted from the depth decoder itself. At the end the decoder outputs a 32-channel feature map which we directly use for feature reconstruction loss. Using these self-embedded depth features for additional warp error minimization also shows promising improvements in the accuracy of the depth predictions without requiring any explicit supervision for matching as required by [40].\n\nIn Figure 6, we compare the deep features of [40] and the self-embedded depth features against color consistency on the task of stereo matching. Photometric error is not as robust as deep feature error, especially in texture-less regions, there are multiple local minima with similar magnitude. However, both NYUv2 Feature from [40] and selfembedded depth features show distinctive local minimum which is a desirable property.\n\n\nConclusion\n\nWe have presented an unsupervised learning framework for single view depth estimation and monocular visual odometry using stereo data for training. We have shown that the use of binocular stereo sequences for jointly learning the two tasks, enable odometry prediction in metric scale simply given 2 frames We also show the advantage of using temporal image alignment, in addition to stereo pair alignment for single view depth predictions. Additionally, we have proposed a novel feature reconstruction loss to have state-of-the-art unsupervised single view depth and frameto-frame odometry without scale ambiguity.\n\nThere are still a number of challenges to be addressed. Our framework assumes no occlusion and the scene is assumed to be rigid. Modelling scene dynamics and occlusions explicitly, in a deep learning framework will provide a natural means for more practical and useful navigation in real scenarios. Although we show odometry results that are comparable to the best two-frame estimates available the current systems do not compare favourably with state-ofthe-art SLAM systems. An extensive study of CNN architectures more suitable for odometry estimation and a possible way of integrating the map information over time are challenging but very fruitful future directions.\n\n\nAcknowledgement\n\nFigure 1 .\n1Training instance example. The known camera motion between stereo cameras TL\u2192R constrains the Depth CNN and Odometry CNN to predict depth and relative camera pose with actual scale.\n\nFigure 2 .\n2Illustration of our proposed framework in training phase. CNNV O and CNND can be used independently in testing phase.\n\nFigure 3 .\n3Qualitative result on visual odometry. Full trajectories on the testing sequences (09, 10) are plotted.\n\nFigure 4 .\n4Comparison of VO error with different translation threshold for sequence 09 of odometry dataset.\n\nFigure 5 .\n5Single view depth estimation examples in Eigen Split. The ground truth depth is interpolated for visualization purpose.\n\nFigure 6 .\n6Stereo matching examples. Rows: (1) Left image; (2) Right image; (3) Matching error using color intensity and deep features. Photometric loss is not robust when compared with feature loss, especially in ambiguous regions.\n\n\nwith some vi-Method \n\nDataset Supervision \nError metric \nAccuracy metric \nAbs Rel SqRel RMSE RMSE log \u03b4 < 1.25 \u03b4 < 1.25 2 \u03b4 < 1.25 3 \nDepth: cap 80m \nTrain set mean \nK \nDepth \n0.361 \n4.826 \n8.102 \n0.377 \n0.638 \n0.804 \n0.894 \nEigen et al. [5] Fine \nK \nDepth \n0.203 \n1.548 \n6.307 \n0.282 \n0.702 \n0.890 \n0.958 \nLiu et al. [22] \nK \nDepth \n0.201 \n1.584 \n6.471 \n0.273 \n0.680 \n0.898 \n0.967 \nZhou et al. [44] \nK \nMono. \n0.208 \n1.768 \n6.856 \n0.283 \n0.678 \n0.885 \n0.957 \nGarg et al. [6] \nK \nStereo \n0.152 \n1.226 \n5.849 \n0.246 \n0.784 \n0.921 \n0.967 \nGodard et al. [9] \nK \nStereo \n0.148 \n1.344 \n5.927 \n0.247 \n0.803 \n0.922 \n0.964 \nOurs (Temporal) \nK \nStereo \n0.144 \n1.391 \n5.869 \n0.241 \n0.803 \n0.928 \n0.969 \nOurs (Full-NYUv2) \nK \nStereo \n0.135 \n1.132 \n5.585 \n0.229 \n0.820 \n0.933 \n0.971 \nDepth: cap 50m \nZhou et al. [44] \nK \nMono. \n0.201 \n1.391 \n5.181 \n0.264 \n0.696 \n0.900 \n0.966 \nGarg et al. [6] \nK \nStereo \n0.169 \n1.080 \n5.104 \n0.273 \n0.740 \n0.904 \n0.962 \nGodard et al. [9] \nK \nStereo \n0.140 \n0.976 \n4.471 \n0.232 \n0.818 \n0.931 \n0.969 \nOurs (Temporal) \nK \nStereo \n0.135 \n0.905 \n4.366 \n0.225 \n0.818 \n0.937 \n0.973 \nOurs (Full-NYUv2) \nK \nStereo \n0.128 \n0.815 \n4.204 \n0.216 \n0.835 \n0.941 \n0.975 \n\nTable 2. Comparison of single view depth estimation performance with existing approaches. For training, K is KITTI dataset (Eigen Split). \nFor a fair comparison, all methods (except \n\nTable 3\n3shows an ablation study on depth estimation for our method showing importance of each component of the loss function. Our first baseline is a simple architecture (ResNet50-1by2 as encoder; Bilinear upsampler as de-Abs Rel SqRel RMSE RMSE log \u03b4 < 1.25 \u03b4 < 1.25 2 \u03b4 < 1.25 3 Encoder: ResNet50-1by2; Decoder: Bilinear upsamplerTable 3. Ablation study on single view depth estimation. The result is evaluated in KITTI 2015 using Eigen Split test set, following the evaluation protocol proposed in[9]. The results are capped at 50m depth. Stereo: stereo pairs are used for training; Temporal: additional temporal pairs are used; Feature: feature reconstruction loss is used.Method \n\nStereo Temporal Feature \nError metric \nAccuracy metric \nBaseline \n\n\n\n0.143 \n0.859 \n4.310 \n0.229 \n0.802 \n0.933 \n0.973 \nTemporal \n\n\n\n0.135 \n0.905 \n4.366 \n0.225 \n0.818 \n0.937 \n0.973 \nImageNet Feat. \n\n\n\n0.136 \n0.880 \n4.390 \n0.230 \n0.823 \n0.935 \n0.970 \nKITTI Feat. \n\n\n\n0.130 \n0.860 \n4.271 \n0.221 \n0.831 \n0.938 \n0.973 \nNYUv2 Feat. \n\n\n\n0.132 \n0.906 \n4.279 \n0.220 \n0.831 \n0.939 \n0.974 \nFull-NYUv2 \n\n\n\n0.128 \n0.815 \n4.204 \n0.216 \n0.835 \n0.941 \n0.975 \nEncoder: ResNet50-1by2; Decoder: Learnable upsampler \nBaseline2 \n\n\n\n0.155 \n1.307 \n4.560 \n0.242 \n0.805 \n0.928 \n0.968 \nTemporal2 \n\n\n\n0.141 \n0.998 \n4.354 \n0.232 \n0.814 \n0.932 \n0.971 \nDepth Feat. \n\n\n\n0.142 \n0.956 \n4.377 \n0.230 \n0.817 \n0.934 \n0.971 \nFull-Depth \n\n\n\n0.137 \n0.893 \n4.348 \n0.228 \n0.821 \n0.935 \n0.971 \n\n\nThis work was supported by the UoA Scholarship to HZ and KL, the ARC Laureate Fellowship FL130100102 to IR and the Australian Centre of Excellence for Robotic Vision CE140100016.\nLearning to see by moving. P Agrawal, J Carreira, J Malik, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionP. Agrawal, J. Carreira, and J. Malik. Learning to see by moving. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 37-45, 2015.\n\nUniversal correspondence network. C B Choy, J Gwak, S Savarese, M Chandraker, Advances in Neural Information Processing Systems. C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker. Uni- versal correspondence network. In Advances in Neural In- formation Processing Systems, pages 2414-2422, 2016.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE1N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recogni- tion, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886-893. IEEE, 2005.\n\nMonoslam: Real-time single camera slam. IEEE transactions on pattern analysis and machine intelligence. A J Davison, I D Reid, N D Molton, O Stasse, 29A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse. Monoslam: Real-time single camera slam. IEEE trans- actions on pattern analysis and machine intelligence, 29(6):1052-1067, 2007.\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in neural information processing systems. D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366-2374, 2014.\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. R Garg, V K B G, G Carneiro, I Reid, European Conference on Computer Vision. SpringerR. Garg, V. K. B G, G. Carneiro, and I. Reid. Unsupervised cnn for single view depth estimation: Geometry to the res- cue. In European Conference on Computer Vision, pages 740-756. Springer, 2016.\n\nVision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, International Journal of Robotics Research. A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013.\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, Conference on Computer Vision and Pattern Recognition (CVPR). A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au- tonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, CVPR. C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised monocular depth estimation with left-right consistency. In CVPR, 2017.\n\ngvnn: Neural network library for geometric computer vision. A Handa, M Bloesch, V P\u0203tr\u0203ucean, S Stent, J Mccormac, A Davison, Computer Vision-ECCV. A. Handa, M. Bloesch, V. P\u0203tr\u0203ucean, S. Stent, J. McCor- mac, and A. Davison. gvnn: Neural network library for ge- ometric computer vision. In Computer Vision-ECCV 2016\n\n. Workshops, SpringerWorkshops, pages 67-82. Springer, 2016.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770-778, 2016.\n\nPm-huber: Patchmatch with huber regularization for stereo matching. P Heise, S Klose, B Jensen, A Knoll, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionP. Heise, S. Klose, B. Jensen, and A. Knoll. Pm-huber: Patchmatch with huber regularization for stereo matching. In Proceedings of the IEEE International Conference on Com- puter Vision, pages 2360-2367, 2013.\n\nDetermining optical flow. B K Horn, B G Schunck, Artificial intelligence. 171-3B. K. Horn and B. G. Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185-203, 1981.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, Advances in Neural Information Processing Systems. M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In Advances in Neural Information Processing Systems, pages 2017-2025, 2015.\n\nBack to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness. J Y Jason, A W Harley, K G Derpanis, European Conference on Computer Vision. SpringerJ. Y. Jason, A. W. Harley, and K. G. Derpanis. Back to ba- sics: Unsupervised learning of optical flow via brightness constancy and motion smoothness. In European Conference on Computer Vision, pages 3-10. Springer, 2016.\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaACMY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- shick, S. Guadarrama, and T. Darrell. Caffe: Convolu- tional architecture for fast feature embedding. In Proceed- ings of the 22nd ACM international conference on Multime- dia, pages 675-678. ACM, 2014.\n\nEnd-to-end learning of geometry and context for deep stereo regression. A Kendall, H Martirosyan, S Dasgupta, P Henry, R Kennedy, A Bachrach, A Bry, Proceedings of the International Conference on Computer Vision (ICCV. the International Conference on Computer Vision (ICCVA. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach, and A. Bry. End-to-end learn- ing of geometry and context for deep stereo regression. In Proceedings of the International Conference on Computer Vision (ICCV), 2017.\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980arXiv preprintD. Kingma and J. Ba. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980, 2014.\n\nSemi-supervised deep learning for monocular depth map prediction. Y Kuznietsov, J St\u00fcckler, B Leibe, Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEEY. Kuznietsov, J. St\u00fcckler, and B. Leibe. Semi-supervised deep learning for monocular depth map prediction. In Com- puter Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 2215-2223. IEEE, 2017.\n\nDeeper depth prediction with fully convolutional residual networks. I Laina, C Rupprecht, V Belagiannis, F Tombari, N Navab, 3D Vision (3DV), 2016 Fourth International Conference on. IEEEI. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In 3D Vision (3DV), 2016 Fourth Interna- tional Conference on, pages 239-248. IEEE, 2016.\n\nDeep convolutional neural fields for depth estimation from a single image. F Liu, C Shen, G Lin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionF. Liu, C. Shen, and G. Lin. Deep convolutional neural fields for depth estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5162-5170, 2015.\n\nLearning depth from single monocular images using deep convolutional neural fields. F Liu, C Shen, G Lin, I Reid, IEEE transactions on pattern analysis and machine intelligence. 38F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin- gle monocular images using deep convolutional neural fields. IEEE transactions on pattern analysis and machine intelli- gence, 38(10):2024-2039, 2016.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 3431-3440, 2015.\n\nA computer algorithm for reconstructing a scene from two projections. H C Longuet-Higgins, Nature. 2935828H. C. Longuet-Higgins. A computer algorithm for re- constructing a scene from two projections. Nature, 293(5828):133-135, 1981.\n\nDistinctive image features from scaleinvariant keypoints. D G Lowe, International journal of computer vision. 602D. G. Lowe. Distinctive image features from scale- invariant keypoints. International journal of computer vi- sion, 60(2):91-110, 2004.\n\nOrb-slam: a versatile and accurate monocular slam system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Transactions on Robotics. 315R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE Trans- actions on Robotics, 31(5):1147-1163, 2015.\n\nIndoor segmentation and support inference from rgbd images. P K Nathan Silberman, Derek Hoiem, R Fergus, ECCV. P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.\n\nLearning deconvolution network for semantic segmentation. H Noh, S Hong, B Han, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionH. Noh, S. Hong, and B. Han. Learning deconvolution net- work for semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1520- 1528, 2015.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu- tional networks for biomedical image segmentation. In In- ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234-241. Springer, 2015.\n\nOrb: An efficient alternative to sift or surf. E Rublee, V Rabaud, K Konolige, G Bradski, Computer Vision (ICCV), 2011 IEEE international conference on. IEEEE. Rublee, V. Rabaud, K. Konolige, and G. Bradski. Orb: An efficient alternative to sift or surf. In Computer Vi- sion (ICCV), 2011 IEEE international conference on, pages 2564-2571. IEEE, 2011.\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, International Journal of Computer Vision (IJCV). 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.\n\nLearning depth from single monocular images. A Saxena, S H Chung, A Y Ng, Advances in neural information processing systems. A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from single monocular images. In Advances in neural information processing systems, pages 1161-1168, 2006.\n\nMake3d: Learning 3d scene structure from a single still image. A Saxena, M Sun, A Y Ng, IEEE transactions on pattern analysis and machine intelligence. 31A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d scene structure from a single still image. IEEE transactions on pattern analysis and machine intelligence, 31(5):824- 840, 2009.\n\nRobust scale estimation in realtime monocular sfm for autonomous driving. S Song, M Chandraker, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Song and M. Chandraker. Robust scale estimation in real- time monocular sfm for autonomous driving. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1566-1573, 2014.\n\nBundle adjustmenta modern synthesis. B Triggs, P F Mclauchlan, R I Hartley, A W Fitzgibbon, International workshop on vision algorithms. SpringerB. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgib- bon. Bundle adjustmenta modern synthesis. In International workshop on vision algorithms, pages 298-372. Springer, 1999.\n\nDemon: Depth and motion network for learning monocular stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox. Demon: Depth and motion network for learning monocular stereo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nS Vijayanarasimhan, S Ricco, C Schmid, R Sukthankar, K Fragkiadaki, arXiv:1704.07804Sfm-net: Learning of structure and motion from video. arXiv preprintS. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki. Sfm-net: Learning of structure and mo- tion from video. arXiv preprint arXiv:1704.07804, 2017.\n\nDeepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. S Wang, R Clark, H Wen, N Trigoni, 2017 IEEE International Conference on. IEEERobotics and Automation (ICRAS. Wang, R. Clark, H. Wen, and N. Trigoni. Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. In Robotics and Automa- tion (ICRA), 2017 IEEE International Conference on, pages 2043-2050. IEEE, 2017.\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon- celli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image process- ing, 13(4):600-612, 2004.\n\nLearning deeply supervised visual descriptors for dense monocular reconstruction. C S Weerasekera, R Garg, I Reid, arXiv:1711.05919arXiv preprintC. S. Weerasekera, R. Garg, and I. Reid. Learning deeply su- pervised visual descriptors for dense monocular reconstruc- tion. arXiv preprint arXiv:1711.05919, 2017.\n\nSelf-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery. M Ye, E Johns, A Handa, L Zhang, P Pratt, G.-Z Yang, arXiv:1705.08260arXiv preprintM. Ye, E. Johns, A. Handa, L. Zhang, P. Pratt, and G.- Z. Yang. Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery. arXiv preprint arXiv:1705.08260, 2017.\n\nLift: Learned invariant feature transform. K M Yi, E Trulls, V Lepetit, P Fua, European Conference on Computer Vision. SpringerK. M. Yi, E. Trulls, V. Lepetit, and P. Fua. Lift: Learned in- variant feature transform. In European Conference on Com- puter Vision, pages 467-483. Springer, 2016.\n\nStereo matching by training a convolutional neural network to compare image patches. J Zbontar, Y Lecun, Journal of Machine Learning Research. 172J. Zbontar and Y. LeCun. Stereo matching by training a con- volutional neural network to compare image patches. Jour- nal of Machine Learning Research, 17(1-32):2, 2016.\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu- pervised learning of depth and ego-motion from video. In CVPR, 2017.\n", "annotations": {"author": "[{\"end\":191,\"start\":108},{\"end\":270,\"start\":192},{\"end\":365,\"start\":271},{\"end\":443,\"start\":366},{\"end\":497,\"start\":444},{\"end\":632,\"start\":498},{\"end\":191,\"start\":108},{\"end\":270,\"start\":192},{\"end\":365,\"start\":271},{\"end\":443,\"start\":366},{\"end\":497,\"start\":444},{\"end\":632,\"start\":498}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":118},{\"end\":201,\"start\":197},{\"end\":296,\"start\":285},{\"end\":374,\"start\":372},{\"end\":457,\"start\":450},{\"end\":506,\"start\":502},{\"end\":122,\"start\":118},{\"end\":201,\"start\":197},{\"end\":296,\"start\":285},{\"end\":374,\"start\":372},{\"end\":457,\"start\":450},{\"end\":506,\"start\":502}]", "author_first_name": "[{\"end\":117,\"start\":108},{\"end\":196,\"start\":192},{\"end\":278,\"start\":271},{\"end\":284,\"start\":279},{\"end\":371,\"start\":366},{\"end\":449,\"start\":444},{\"end\":501,\"start\":498},{\"end\":117,\"start\":108},{\"end\":196,\"start\":192},{\"end\":278,\"start\":271},{\"end\":284,\"start\":279},{\"end\":371,\"start\":366},{\"end\":449,\"start\":444},{\"end\":501,\"start\":498}]", "author_affiliation": "[{\"end\":151,\"start\":124},{\"end\":190,\"start\":153},{\"end\":230,\"start\":203},{\"end\":269,\"start\":232},{\"end\":325,\"start\":298},{\"end\":364,\"start\":327},{\"end\":403,\"start\":376},{\"end\":442,\"start\":405},{\"end\":496,\"start\":459},{\"end\":592,\"start\":565},{\"end\":631,\"start\":594},{\"end\":151,\"start\":124},{\"end\":190,\"start\":153},{\"end\":230,\"start\":203},{\"end\":269,\"start\":232},{\"end\":325,\"start\":298},{\"end\":364,\"start\":327},{\"end\":403,\"start\":376},{\"end\":442,\"start\":405},{\"end\":496,\"start\":459},{\"end\":592,\"start\":565},{\"end\":631,\"start\":594}]", "title": "[{\"end\":105,\"start\":1},{\"end\":737,\"start\":633},{\"end\":105,\"start\":1},{\"end\":737,\"start\":633}]", "venue": null, "abstract": "[{\"end\":2164,\"start\":739},{\"end\":2164,\"start\":739}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2908,\"start\":2905},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2912,\"start\":2909},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2921,\"start\":2917},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3195,\"start\":3192},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3457,\"start\":3454},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3474,\"start\":3471},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3479,\"start\":3475},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3484,\"start\":3480},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3620,\"start\":3617},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3628,\"start\":3625},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3763,\"start\":3759},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3937,\"start\":3933},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4654,\"start\":4650},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6450,\"start\":6446},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7109,\"start\":7105},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7114,\"start\":7110},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7397,\"start\":7393},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7533,\"start\":7529},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7574,\"start\":7571},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7810,\"start\":7807},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7966,\"start\":7962},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8076,\"start\":8072},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8303,\"start\":8299},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8679,\"start\":8676},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8845,\"start\":8841},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8851,\"start\":8848},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8975,\"start\":8971},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9096,\"start\":9092},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9660,\"start\":9657},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9974,\"start\":9971},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10113,\"start\":10109},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10410,\"start\":10406},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10597,\"start\":10593},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10606,\"start\":10602},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10757,\"start\":10753},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10766,\"start\":10762},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10813,\"start\":10810},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11321,\"start\":11318},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11378,\"start\":11375},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11420,\"start\":11416},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11463,\"start\":11459},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11472,\"start\":11469},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11482,\"start\":11478},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11674,\"start\":11670},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11690,\"start\":11686},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11751,\"start\":11747},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11990,\"start\":11986},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12041,\"start\":12037},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13055,\"start\":13051},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15856,\"start\":15852},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15859,\"start\":15856},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16525,\"start\":16521},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18067,\"start\":18064},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18072,\"start\":18068},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18236,\"start\":18232},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19078,\"start\":19074},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19504,\"start\":19500},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19511,\"start\":19508},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20614,\"start\":20611},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20618,\"start\":20615},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21111,\"start\":21107},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21192,\"start\":21188},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21618,\"start\":21615},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22016,\"start\":22013},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22046,\"start\":22043},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22048,\"start\":22046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22050,\"start\":22048},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22053,\"start\":22050},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22174,\"start\":22170},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22599,\"start\":22596},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22852,\"start\":22849},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22948,\"start\":22944},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23640,\"start\":23636},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23900,\"start\":23896},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23941,\"start\":23937},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24930,\"start\":24926},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25113,\"start\":25109},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25339,\"start\":25335},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25544,\"start\":25540},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25942,\"start\":25938},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26533,\"start\":26529},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27017,\"start\":27014},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27049,\"start\":27046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27174,\"start\":27171},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27220,\"start\":27217},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27483,\"start\":27480},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27574,\"start\":27570},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27663,\"start\":27660},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27704,\"start\":27701},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27884,\"start\":27880},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28055,\"start\":28052},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28063,\"start\":28060},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28127,\"start\":28124},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28228,\"start\":28224},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28398,\"start\":28395},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28738,\"start\":28734},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28775,\"start\":28771},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28811,\"start\":28807},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28874,\"start\":28870},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29145,\"start\":29141},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29428,\"start\":29424},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29433,\"start\":29429},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30190,\"start\":30186},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30242,\"start\":30238},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30525,\"start\":30521},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34728,\"start\":34725},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2908,\"start\":2905},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2912,\"start\":2909},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2921,\"start\":2917},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3195,\"start\":3192},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3457,\"start\":3454},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3474,\"start\":3471},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3479,\"start\":3475},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3484,\"start\":3480},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3620,\"start\":3617},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3628,\"start\":3625},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3763,\"start\":3759},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3937,\"start\":3933},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4654,\"start\":4650},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6450,\"start\":6446},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7109,\"start\":7105},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7114,\"start\":7110},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7397,\"start\":7393},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7533,\"start\":7529},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7574,\"start\":7571},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7810,\"start\":7807},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7966,\"start\":7962},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8076,\"start\":8072},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8303,\"start\":8299},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8679,\"start\":8676},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8845,\"start\":8841},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8851,\"start\":8848},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8975,\"start\":8971},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9096,\"start\":9092},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9660,\"start\":9657},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9974,\"start\":9971},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10113,\"start\":10109},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10410,\"start\":10406},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10597,\"start\":10593},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10606,\"start\":10602},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10757,\"start\":10753},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10766,\"start\":10762},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10813,\"start\":10810},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11321,\"start\":11318},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11378,\"start\":11375},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11420,\"start\":11416},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11463,\"start\":11459},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11472,\"start\":11469},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11482,\"start\":11478},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11674,\"start\":11670},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11690,\"start\":11686},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11751,\"start\":11747},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11990,\"start\":11986},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12041,\"start\":12037},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13055,\"start\":13051},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15856,\"start\":15852},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15859,\"start\":15856},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16525,\"start\":16521},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18067,\"start\":18064},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18072,\"start\":18068},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18236,\"start\":18232},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19078,\"start\":19074},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19504,\"start\":19500},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19511,\"start\":19508},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20614,\"start\":20611},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20618,\"start\":20615},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21111,\"start\":21107},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21192,\"start\":21188},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21618,\"start\":21615},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22016,\"start\":22013},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22046,\"start\":22043},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22048,\"start\":22046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22050,\"start\":22048},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22053,\"start\":22050},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22174,\"start\":22170},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22599,\"start\":22596},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22852,\"start\":22849},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22948,\"start\":22944},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23640,\"start\":23636},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23900,\"start\":23896},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23941,\"start\":23937},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24930,\"start\":24926},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25113,\"start\":25109},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25339,\"start\":25335},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25544,\"start\":25540},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25942,\"start\":25938},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26533,\"start\":26529},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27017,\"start\":27014},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27049,\"start\":27046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27174,\"start\":27171},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27220,\"start\":27217},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27483,\"start\":27480},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27574,\"start\":27570},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27663,\"start\":27660},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27704,\"start\":27701},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27884,\"start\":27880},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28055,\"start\":28052},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28063,\"start\":28060},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28127,\"start\":28124},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28228,\"start\":28224},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28398,\"start\":28395},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28738,\"start\":28734},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28775,\"start\":28771},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28811,\"start\":28807},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28874,\"start\":28870},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29145,\"start\":29141},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29428,\"start\":29424},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29433,\"start\":29429},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30190,\"start\":30186},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30242,\"start\":30238},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30525,\"start\":30521},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34728,\"start\":34725}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32133,\"start\":31939},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32264,\"start\":32134},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32381,\"start\":32265},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32491,\"start\":32382},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32624,\"start\":32492},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32859,\"start\":32625},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34222,\"start\":32860},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35663,\"start\":34223},{\"attributes\":{\"id\":\"fig_0\"},\"end\":32133,\"start\":31939},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32264,\"start\":32134},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32381,\"start\":32265},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32491,\"start\":32382},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32624,\"start\":32492},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32859,\"start\":32625},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34222,\"start\":32860},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35663,\"start\":34223}]", "paragraph": "[{\"end\":2637,\"start\":2180},{\"end\":4655,\"start\":2639},{\"end\":5424,\"start\":4657},{\"end\":5991,\"start\":5426},{\"end\":6675,\"start\":5993},{\"end\":7724,\"start\":6692},{\"end\":8645,\"start\":7726},{\"end\":9072,\"start\":8647},{\"end\":9235,\"start\":9074},{\"end\":9423,\"start\":9237},{\"end\":10270,\"start\":9425},{\"end\":10803,\"start\":10272},{\"end\":11173,\"start\":10805},{\"end\":12087,\"start\":11175},{\"end\":12502,\"start\":12098},{\"end\":13295,\"start\":12542},{\"end\":13681,\"start\":13297},{\"end\":14070,\"start\":13772},{\"end\":14341,\"start\":14150},{\"end\":15215,\"start\":14410},{\"end\":15672,\"start\":15251},{\"end\":15902,\"start\":15674},{\"end\":16323,\"start\":16000},{\"end\":16526,\"start\":16325},{\"end\":17214,\"start\":16568},{\"end\":17525,\"start\":17216},{\"end\":17675,\"start\":17616},{\"end\":17826,\"start\":17744},{\"end\":18159,\"start\":17844},{\"end\":18517,\"start\":18161},{\"end\":18739,\"start\":18588},{\"end\":18772,\"start\":18741},{\"end\":18890,\"start\":18819},{\"end\":20005,\"start\":18915},{\"end\":20440,\"start\":20007},{\"end\":21068,\"start\":20456},{\"end\":21567,\"start\":21070},{\"end\":23253,\"start\":21569},{\"end\":23558,\"start\":23255},{\"end\":23682,\"start\":23560},{\"end\":24558,\"start\":23710},{\"end\":25829,\"start\":24560},{\"end\":26314,\"start\":25831},{\"end\":26813,\"start\":26316},{\"end\":29276,\"start\":26842},{\"end\":30191,\"start\":29311},{\"end\":30619,\"start\":30193},{\"end\":31248,\"start\":30634},{\"end\":31920,\"start\":31250},{\"end\":2637,\"start\":2180},{\"end\":4655,\"start\":2639},{\"end\":5424,\"start\":4657},{\"end\":5991,\"start\":5426},{\"end\":6675,\"start\":5993},{\"end\":7724,\"start\":6692},{\"end\":8645,\"start\":7726},{\"end\":9072,\"start\":8647},{\"end\":9235,\"start\":9074},{\"end\":9423,\"start\":9237},{\"end\":10270,\"start\":9425},{\"end\":10803,\"start\":10272},{\"end\":11173,\"start\":10805},{\"end\":12087,\"start\":11175},{\"end\":12502,\"start\":12098},{\"end\":13295,\"start\":12542},{\"end\":13681,\"start\":13297},{\"end\":14070,\"start\":13772},{\"end\":14341,\"start\":14150},{\"end\":15215,\"start\":14410},{\"end\":15672,\"start\":15251},{\"end\":15902,\"start\":15674},{\"end\":16323,\"start\":16000},{\"end\":16526,\"start\":16325},{\"end\":17214,\"start\":16568},{\"end\":17525,\"start\":17216},{\"end\":17675,\"start\":17616},{\"end\":17826,\"start\":17744},{\"end\":18159,\"start\":17844},{\"end\":18517,\"start\":18161},{\"end\":18739,\"start\":18588},{\"end\":18772,\"start\":18741},{\"end\":18890,\"start\":18819},{\"end\":20005,\"start\":18915},{\"end\":20440,\"start\":20007},{\"end\":21068,\"start\":20456},{\"end\":21567,\"start\":21070},{\"end\":23253,\"start\":21569},{\"end\":23558,\"start\":23255},{\"end\":23682,\"start\":23560},{\"end\":24558,\"start\":23710},{\"end\":25829,\"start\":24560},{\"end\":26314,\"start\":25831},{\"end\":26813,\"start\":26316},{\"end\":29276,\"start\":26842},{\"end\":30191,\"start\":29311},{\"end\":30619,\"start\":30193},{\"end\":31248,\"start\":30634},{\"end\":31920,\"start\":31250}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13727,\"start\":13682},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13771,\"start\":13727},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14149,\"start\":14071},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14409,\"start\":14342},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15949,\"start\":15903},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15999,\"start\":15949},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17571,\"start\":17526},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17615,\"start\":17571},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17743,\"start\":17676},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18587,\"start\":18518},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18818,\"start\":18773},{\"attributes\":{\"id\":\"formula_0\"},\"end\":13727,\"start\":13682},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13771,\"start\":13727},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14149,\"start\":14071},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14409,\"start\":14342},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15949,\"start\":15903},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15999,\"start\":15949},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17571,\"start\":17526},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17615,\"start\":17571},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17743,\"start\":17676},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18587,\"start\":18518},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18818,\"start\":18773}]", "table_ref": "[{\"end\":24557,\"start\":24550},{\"end\":25856,\"start\":25849},{\"end\":27170,\"start\":27163},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29275,\"start\":29268},{\"end\":24557,\"start\":24550},{\"end\":25856,\"start\":25849},{\"end\":27170,\"start\":27163},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29275,\"start\":29268}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2178,\"start\":2166},{\"attributes\":{\"n\":\"2.\"},\"end\":6690,\"start\":6678},{\"attributes\":{\"n\":\"3.\"},\"end\":12096,\"start\":12090},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12540,\"start\":12505},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15249,\"start\":15218},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16566,\"start\":16529},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17842,\"start\":17829},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18913,\"start\":18893},{\"attributes\":{\"n\":\"4.\"},\"end\":20454,\"start\":20443},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23708,\"start\":23685},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26840,\"start\":26816},{\"end\":29290,\"start\":29279},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29309,\"start\":29293},{\"attributes\":{\"n\":\"5.\"},\"end\":30632,\"start\":30622},{\"attributes\":{\"n\":\"6.\"},\"end\":31938,\"start\":31923},{\"end\":31950,\"start\":31940},{\"end\":32145,\"start\":32135},{\"end\":32276,\"start\":32266},{\"end\":32393,\"start\":32383},{\"end\":32503,\"start\":32493},{\"end\":32636,\"start\":32626},{\"end\":34231,\"start\":34224},{\"attributes\":{\"n\":\"1.\"},\"end\":2178,\"start\":2166},{\"attributes\":{\"n\":\"2.\"},\"end\":6690,\"start\":6678},{\"attributes\":{\"n\":\"3.\"},\"end\":12096,\"start\":12090},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12540,\"start\":12505},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15249,\"start\":15218},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16566,\"start\":16529},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17842,\"start\":17829},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18913,\"start\":18893},{\"attributes\":{\"n\":\"4.\"},\"end\":20454,\"start\":20443},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23708,\"start\":23685},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26840,\"start\":26816},{\"end\":29290,\"start\":29279},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29309,\"start\":29293},{\"attributes\":{\"n\":\"5.\"},\"end\":30632,\"start\":30622},{\"attributes\":{\"n\":\"6.\"},\"end\":31938,\"start\":31923},{\"end\":31950,\"start\":31940},{\"end\":32145,\"start\":32135},{\"end\":32276,\"start\":32266},{\"end\":32393,\"start\":32383},{\"end\":32503,\"start\":32493},{\"end\":32636,\"start\":32626},{\"end\":34231,\"start\":34224}]", "table": "[{\"end\":34222,\"start\":32875},{\"end\":35663,\"start\":34902},{\"end\":34222,\"start\":32875},{\"end\":35663,\"start\":34902}]", "figure_caption": "[{\"end\":32133,\"start\":31952},{\"end\":32264,\"start\":32147},{\"end\":32381,\"start\":32278},{\"end\":32491,\"start\":32395},{\"end\":32624,\"start\":32505},{\"end\":32859,\"start\":32638},{\"end\":32875,\"start\":32862},{\"end\":34902,\"start\":34233},{\"end\":32133,\"start\":31952},{\"end\":32264,\"start\":32147},{\"end\":32381,\"start\":32278},{\"end\":32491,\"start\":32395},{\"end\":32624,\"start\":32505},{\"end\":32859,\"start\":32638},{\"end\":32875,\"start\":32862},{\"end\":34902,\"start\":34233}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4828,\"start\":4820},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12153,\"start\":12145},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25828,\"start\":25819},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26479,\"start\":26471},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27466,\"start\":27458},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30204,\"start\":30196},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4828,\"start\":4820},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12153,\"start\":12145},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25828,\"start\":25819},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26479,\"start\":26471},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27466,\"start\":27458},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30204,\"start\":30196}]", "bib_author_first_name": "[{\"end\":35871,\"start\":35870},{\"end\":35882,\"start\":35881},{\"end\":35894,\"start\":35893},{\"end\":36218,\"start\":36217},{\"end\":36220,\"start\":36219},{\"end\":36228,\"start\":36227},{\"end\":36236,\"start\":36235},{\"end\":36248,\"start\":36247},{\"end\":36536,\"start\":36535},{\"end\":36545,\"start\":36544},{\"end\":36975,\"start\":36974},{\"end\":36977,\"start\":36976},{\"end\":36988,\"start\":36987},{\"end\":36990,\"start\":36989},{\"end\":36998,\"start\":36997},{\"end\":37000,\"start\":36999},{\"end\":37010,\"start\":37009},{\"end\":37283,\"start\":37282},{\"end\":37292,\"start\":37291},{\"end\":37303,\"start\":37302},{\"end\":37629,\"start\":37628},{\"end\":37637,\"start\":37636},{\"end\":37639,\"start\":37638},{\"end\":37646,\"start\":37645},{\"end\":37658,\"start\":37657},{\"end\":37954,\"start\":37953},{\"end\":37964,\"start\":37963},{\"end\":37972,\"start\":37971},{\"end\":37983,\"start\":37982},{\"end\":38257,\"start\":38256},{\"end\":38267,\"start\":38266},{\"end\":38275,\"start\":38274},{\"end\":38598,\"start\":38597},{\"end\":38608,\"start\":38607},{\"end\":38612,\"start\":38609},{\"end\":38621,\"start\":38620},{\"end\":38623,\"start\":38622},{\"end\":38829,\"start\":38828},{\"end\":38838,\"start\":38837},{\"end\":38849,\"start\":38848},{\"end\":38863,\"start\":38862},{\"end\":38872,\"start\":38871},{\"end\":38884,\"start\":38883},{\"end\":39195,\"start\":39194},{\"end\":39201,\"start\":39200},{\"end\":39210,\"start\":39209},{\"end\":39217,\"start\":39216},{\"end\":39624,\"start\":39623},{\"end\":39633,\"start\":39632},{\"end\":39642,\"start\":39641},{\"end\":39652,\"start\":39651},{\"end\":40019,\"start\":40018},{\"end\":40021,\"start\":40020},{\"end\":40029,\"start\":40028},{\"end\":40031,\"start\":40030},{\"end\":40207,\"start\":40206},{\"end\":40220,\"start\":40219},{\"end\":40232,\"start\":40231},{\"end\":40554,\"start\":40553},{\"end\":40556,\"start\":40555},{\"end\":40565,\"start\":40564},{\"end\":40567,\"start\":40566},{\"end\":40577,\"start\":40576},{\"end\":40579,\"start\":40578},{\"end\":40924,\"start\":40923},{\"end\":40931,\"start\":40930},{\"end\":40944,\"start\":40943},{\"end\":40955,\"start\":40954},{\"end\":40966,\"start\":40965},{\"end\":40974,\"start\":40973},{\"end\":40986,\"start\":40985},{\"end\":41000,\"start\":40999},{\"end\":41472,\"start\":41471},{\"end\":41483,\"start\":41482},{\"end\":41498,\"start\":41497},{\"end\":41510,\"start\":41509},{\"end\":41519,\"start\":41518},{\"end\":41530,\"start\":41529},{\"end\":41542,\"start\":41541},{\"end\":41959,\"start\":41958},{\"end\":41969,\"start\":41968},{\"end\":42176,\"start\":42175},{\"end\":42190,\"start\":42189},{\"end\":42202,\"start\":42201},{\"end\":42572,\"start\":42571},{\"end\":42581,\"start\":42580},{\"end\":42594,\"start\":42593},{\"end\":42609,\"start\":42608},{\"end\":42620,\"start\":42619},{\"end\":42991,\"start\":42990},{\"end\":42998,\"start\":42997},{\"end\":43006,\"start\":43005},{\"end\":43448,\"start\":43447},{\"end\":43455,\"start\":43454},{\"end\":43463,\"start\":43462},{\"end\":43470,\"start\":43469},{\"end\":43815,\"start\":43814},{\"end\":43823,\"start\":43822},{\"end\":43836,\"start\":43835},{\"end\":44261,\"start\":44260},{\"end\":44263,\"start\":44262},{\"end\":44484,\"start\":44483},{\"end\":44486,\"start\":44485},{\"end\":44734,\"start\":44733},{\"end\":44747,\"start\":44746},{\"end\":44751,\"start\":44748},{\"end\":44762,\"start\":44761},{\"end\":44764,\"start\":44763},{\"end\":45033,\"start\":45032},{\"end\":45035,\"start\":45034},{\"end\":45059,\"start\":45054},{\"end\":45068,\"start\":45067},{\"end\":45269,\"start\":45268},{\"end\":45276,\"start\":45275},{\"end\":45284,\"start\":45283},{\"end\":45663,\"start\":45662},{\"end\":45678,\"start\":45677},{\"end\":45689,\"start\":45688},{\"end\":46073,\"start\":46072},{\"end\":46083,\"start\":46082},{\"end\":46093,\"start\":46092},{\"end\":46105,\"start\":46104},{\"end\":46430,\"start\":46429},{\"end\":46445,\"start\":46444},{\"end\":46453,\"start\":46452},{\"end\":46459,\"start\":46458},{\"end\":46469,\"start\":46468},{\"end\":46481,\"start\":46480},{\"end\":46487,\"start\":46486},{\"end\":46496,\"start\":46495},{\"end\":46508,\"start\":46507},{\"end\":46518,\"start\":46517},{\"end\":46531,\"start\":46530},{\"end\":46533,\"start\":46532},{\"end\":46541,\"start\":46540},{\"end\":46912,\"start\":46911},{\"end\":46922,\"start\":46921},{\"end\":46924,\"start\":46923},{\"end\":46933,\"start\":46932},{\"end\":46935,\"start\":46934},{\"end\":47216,\"start\":47215},{\"end\":47226,\"start\":47225},{\"end\":47233,\"start\":47232},{\"end\":47235,\"start\":47234},{\"end\":47564,\"start\":47563},{\"end\":47572,\"start\":47571},{\"end\":47975,\"start\":47974},{\"end\":47985,\"start\":47984},{\"end\":47987,\"start\":47986},{\"end\":48001,\"start\":48000},{\"end\":48003,\"start\":48002},{\"end\":48014,\"start\":48013},{\"end\":48016,\"start\":48015},{\"end\":48331,\"start\":48330},{\"end\":48345,\"start\":48344},{\"end\":48353,\"start\":48352},{\"end\":48362,\"start\":48361},{\"end\":48371,\"start\":48370},{\"end\":48378,\"start\":48377},{\"end\":48393,\"start\":48392},{\"end\":48688,\"start\":48687},{\"end\":48708,\"start\":48707},{\"end\":48717,\"start\":48716},{\"end\":48727,\"start\":48726},{\"end\":48741,\"start\":48740},{\"end\":49107,\"start\":49106},{\"end\":49115,\"start\":49114},{\"end\":49124,\"start\":49123},{\"end\":49131,\"start\":49130},{\"end\":49531,\"start\":49530},{\"end\":49539,\"start\":49538},{\"end\":49541,\"start\":49540},{\"end\":49550,\"start\":49549},{\"end\":49552,\"start\":49551},{\"end\":49562,\"start\":49561},{\"end\":49564,\"start\":49563},{\"end\":49899,\"start\":49898},{\"end\":49901,\"start\":49900},{\"end\":49916,\"start\":49915},{\"end\":49924,\"start\":49923},{\"end\":50225,\"start\":50224},{\"end\":50231,\"start\":50230},{\"end\":50240,\"start\":50239},{\"end\":50249,\"start\":50248},{\"end\":50258,\"start\":50257},{\"end\":50270,\"start\":50266},{\"end\":50551,\"start\":50550},{\"end\":50553,\"start\":50552},{\"end\":50559,\"start\":50558},{\"end\":50569,\"start\":50568},{\"end\":50580,\"start\":50579},{\"end\":50887,\"start\":50886},{\"end\":50898,\"start\":50897},{\"end\":51177,\"start\":51176},{\"end\":51185,\"start\":51184},{\"end\":51194,\"start\":51193},{\"end\":51205,\"start\":51204},{\"end\":51207,\"start\":51206},{\"end\":35871,\"start\":35870},{\"end\":35882,\"start\":35881},{\"end\":35894,\"start\":35893},{\"end\":36218,\"start\":36217},{\"end\":36220,\"start\":36219},{\"end\":36228,\"start\":36227},{\"end\":36236,\"start\":36235},{\"end\":36248,\"start\":36247},{\"end\":36536,\"start\":36535},{\"end\":36545,\"start\":36544},{\"end\":36975,\"start\":36974},{\"end\":36977,\"start\":36976},{\"end\":36988,\"start\":36987},{\"end\":36990,\"start\":36989},{\"end\":36998,\"start\":36997},{\"end\":37000,\"start\":36999},{\"end\":37010,\"start\":37009},{\"end\":37283,\"start\":37282},{\"end\":37292,\"start\":37291},{\"end\":37303,\"start\":37302},{\"end\":37629,\"start\":37628},{\"end\":37637,\"start\":37636},{\"end\":37639,\"start\":37638},{\"end\":37646,\"start\":37645},{\"end\":37658,\"start\":37657},{\"end\":37954,\"start\":37953},{\"end\":37964,\"start\":37963},{\"end\":37972,\"start\":37971},{\"end\":37983,\"start\":37982},{\"end\":38257,\"start\":38256},{\"end\":38267,\"start\":38266},{\"end\":38275,\"start\":38274},{\"end\":38598,\"start\":38597},{\"end\":38608,\"start\":38607},{\"end\":38612,\"start\":38609},{\"end\":38621,\"start\":38620},{\"end\":38623,\"start\":38622},{\"end\":38829,\"start\":38828},{\"end\":38838,\"start\":38837},{\"end\":38849,\"start\":38848},{\"end\":38863,\"start\":38862},{\"end\":38872,\"start\":38871},{\"end\":38884,\"start\":38883},{\"end\":39195,\"start\":39194},{\"end\":39201,\"start\":39200},{\"end\":39210,\"start\":39209},{\"end\":39217,\"start\":39216},{\"end\":39624,\"start\":39623},{\"end\":39633,\"start\":39632},{\"end\":39642,\"start\":39641},{\"end\":39652,\"start\":39651},{\"end\":40019,\"start\":40018},{\"end\":40021,\"start\":40020},{\"end\":40029,\"start\":40028},{\"end\":40031,\"start\":40030},{\"end\":40207,\"start\":40206},{\"end\":40220,\"start\":40219},{\"end\":40232,\"start\":40231},{\"end\":40554,\"start\":40553},{\"end\":40556,\"start\":40555},{\"end\":40565,\"start\":40564},{\"end\":40567,\"start\":40566},{\"end\":40577,\"start\":40576},{\"end\":40579,\"start\":40578},{\"end\":40924,\"start\":40923},{\"end\":40931,\"start\":40930},{\"end\":40944,\"start\":40943},{\"end\":40955,\"start\":40954},{\"end\":40966,\"start\":40965},{\"end\":40974,\"start\":40973},{\"end\":40986,\"start\":40985},{\"end\":41000,\"start\":40999},{\"end\":41472,\"start\":41471},{\"end\":41483,\"start\":41482},{\"end\":41498,\"start\":41497},{\"end\":41510,\"start\":41509},{\"end\":41519,\"start\":41518},{\"end\":41530,\"start\":41529},{\"end\":41542,\"start\":41541},{\"end\":41959,\"start\":41958},{\"end\":41969,\"start\":41968},{\"end\":42176,\"start\":42175},{\"end\":42190,\"start\":42189},{\"end\":42202,\"start\":42201},{\"end\":42572,\"start\":42571},{\"end\":42581,\"start\":42580},{\"end\":42594,\"start\":42593},{\"end\":42609,\"start\":42608},{\"end\":42620,\"start\":42619},{\"end\":42991,\"start\":42990},{\"end\":42998,\"start\":42997},{\"end\":43006,\"start\":43005},{\"end\":43448,\"start\":43447},{\"end\":43455,\"start\":43454},{\"end\":43463,\"start\":43462},{\"end\":43470,\"start\":43469},{\"end\":43815,\"start\":43814},{\"end\":43823,\"start\":43822},{\"end\":43836,\"start\":43835},{\"end\":44261,\"start\":44260},{\"end\":44263,\"start\":44262},{\"end\":44484,\"start\":44483},{\"end\":44486,\"start\":44485},{\"end\":44734,\"start\":44733},{\"end\":44747,\"start\":44746},{\"end\":44751,\"start\":44748},{\"end\":44762,\"start\":44761},{\"end\":44764,\"start\":44763},{\"end\":45033,\"start\":45032},{\"end\":45035,\"start\":45034},{\"end\":45059,\"start\":45054},{\"end\":45068,\"start\":45067},{\"end\":45269,\"start\":45268},{\"end\":45276,\"start\":45275},{\"end\":45284,\"start\":45283},{\"end\":45663,\"start\":45662},{\"end\":45678,\"start\":45677},{\"end\":45689,\"start\":45688},{\"end\":46073,\"start\":46072},{\"end\":46083,\"start\":46082},{\"end\":46093,\"start\":46092},{\"end\":46105,\"start\":46104},{\"end\":46430,\"start\":46429},{\"end\":46445,\"start\":46444},{\"end\":46453,\"start\":46452},{\"end\":46459,\"start\":46458},{\"end\":46469,\"start\":46468},{\"end\":46481,\"start\":46480},{\"end\":46487,\"start\":46486},{\"end\":46496,\"start\":46495},{\"end\":46508,\"start\":46507},{\"end\":46518,\"start\":46517},{\"end\":46531,\"start\":46530},{\"end\":46533,\"start\":46532},{\"end\":46541,\"start\":46540},{\"end\":46912,\"start\":46911},{\"end\":46922,\"start\":46921},{\"end\":46924,\"start\":46923},{\"end\":46933,\"start\":46932},{\"end\":46935,\"start\":46934},{\"end\":47216,\"start\":47215},{\"end\":47226,\"start\":47225},{\"end\":47233,\"start\":47232},{\"end\":47235,\"start\":47234},{\"end\":47564,\"start\":47563},{\"end\":47572,\"start\":47571},{\"end\":47975,\"start\":47974},{\"end\":47985,\"start\":47984},{\"end\":47987,\"start\":47986},{\"end\":48001,\"start\":48000},{\"end\":48003,\"start\":48002},{\"end\":48014,\"start\":48013},{\"end\":48016,\"start\":48015},{\"end\":48331,\"start\":48330},{\"end\":48345,\"start\":48344},{\"end\":48353,\"start\":48352},{\"end\":48362,\"start\":48361},{\"end\":48371,\"start\":48370},{\"end\":48378,\"start\":48377},{\"end\":48393,\"start\":48392},{\"end\":48688,\"start\":48687},{\"end\":48708,\"start\":48707},{\"end\":48717,\"start\":48716},{\"end\":48727,\"start\":48726},{\"end\":48741,\"start\":48740},{\"end\":49107,\"start\":49106},{\"end\":49115,\"start\":49114},{\"end\":49124,\"start\":49123},{\"end\":49131,\"start\":49130},{\"end\":49531,\"start\":49530},{\"end\":49539,\"start\":49538},{\"end\":49541,\"start\":49540},{\"end\":49550,\"start\":49549},{\"end\":49552,\"start\":49551},{\"end\":49562,\"start\":49561},{\"end\":49564,\"start\":49563},{\"end\":49899,\"start\":49898},{\"end\":49901,\"start\":49900},{\"end\":49916,\"start\":49915},{\"end\":49924,\"start\":49923},{\"end\":50225,\"start\":50224},{\"end\":50231,\"start\":50230},{\"end\":50240,\"start\":50239},{\"end\":50249,\"start\":50248},{\"end\":50258,\"start\":50257},{\"end\":50270,\"start\":50266},{\"end\":50551,\"start\":50550},{\"end\":50553,\"start\":50552},{\"end\":50559,\"start\":50558},{\"end\":50569,\"start\":50568},{\"end\":50580,\"start\":50579},{\"end\":50887,\"start\":50886},{\"end\":50898,\"start\":50897},{\"end\":51177,\"start\":51176},{\"end\":51185,\"start\":51184},{\"end\":51194,\"start\":51193},{\"end\":51205,\"start\":51204},{\"end\":51207,\"start\":51206}]", "bib_author_last_name": "[{\"end\":35879,\"start\":35872},{\"end\":35891,\"start\":35883},{\"end\":35900,\"start\":35895},{\"end\":36225,\"start\":36221},{\"end\":36233,\"start\":36229},{\"end\":36245,\"start\":36237},{\"end\":36259,\"start\":36249},{\"end\":36542,\"start\":36537},{\"end\":36552,\"start\":36546},{\"end\":36985,\"start\":36978},{\"end\":36995,\"start\":36991},{\"end\":37007,\"start\":37001},{\"end\":37017,\"start\":37011},{\"end\":37289,\"start\":37284},{\"end\":37300,\"start\":37293},{\"end\":37310,\"start\":37304},{\"end\":37634,\"start\":37630},{\"end\":37643,\"start\":37640},{\"end\":37655,\"start\":37647},{\"end\":37663,\"start\":37659},{\"end\":37961,\"start\":37955},{\"end\":37969,\"start\":37965},{\"end\":37980,\"start\":37973},{\"end\":37991,\"start\":37984},{\"end\":38264,\"start\":38258},{\"end\":38272,\"start\":38268},{\"end\":38283,\"start\":38276},{\"end\":38605,\"start\":38599},{\"end\":38618,\"start\":38613},{\"end\":38631,\"start\":38624},{\"end\":38835,\"start\":38830},{\"end\":38846,\"start\":38839},{\"end\":38860,\"start\":38850},{\"end\":38869,\"start\":38864},{\"end\":38881,\"start\":38873},{\"end\":38892,\"start\":38885},{\"end\":39097,\"start\":39088},{\"end\":39198,\"start\":39196},{\"end\":39207,\"start\":39202},{\"end\":39214,\"start\":39211},{\"end\":39221,\"start\":39218},{\"end\":39630,\"start\":39625},{\"end\":39639,\"start\":39634},{\"end\":39649,\"start\":39643},{\"end\":39658,\"start\":39653},{\"end\":40026,\"start\":40022},{\"end\":40039,\"start\":40032},{\"end\":40217,\"start\":40208},{\"end\":40229,\"start\":40221},{\"end\":40242,\"start\":40233},{\"end\":40562,\"start\":40557},{\"end\":40574,\"start\":40568},{\"end\":40588,\"start\":40580},{\"end\":40928,\"start\":40925},{\"end\":40941,\"start\":40932},{\"end\":40952,\"start\":40945},{\"end\":40963,\"start\":40956},{\"end\":40971,\"start\":40967},{\"end\":40983,\"start\":40975},{\"end\":40997,\"start\":40987},{\"end\":41008,\"start\":41001},{\"end\":41480,\"start\":41473},{\"end\":41495,\"start\":41484},{\"end\":41507,\"start\":41499},{\"end\":41516,\"start\":41511},{\"end\":41527,\"start\":41520},{\"end\":41539,\"start\":41531},{\"end\":41546,\"start\":41543},{\"end\":41966,\"start\":41960},{\"end\":41972,\"start\":41970},{\"end\":42187,\"start\":42177},{\"end\":42199,\"start\":42191},{\"end\":42208,\"start\":42203},{\"end\":42578,\"start\":42573},{\"end\":42591,\"start\":42582},{\"end\":42606,\"start\":42595},{\"end\":42617,\"start\":42610},{\"end\":42626,\"start\":42621},{\"end\":42995,\"start\":42992},{\"end\":43003,\"start\":42999},{\"end\":43010,\"start\":43007},{\"end\":43452,\"start\":43449},{\"end\":43460,\"start\":43456},{\"end\":43467,\"start\":43464},{\"end\":43475,\"start\":43471},{\"end\":43820,\"start\":43816},{\"end\":43833,\"start\":43824},{\"end\":43844,\"start\":43837},{\"end\":44279,\"start\":44264},{\"end\":44491,\"start\":44487},{\"end\":44744,\"start\":44735},{\"end\":44759,\"start\":44752},{\"end\":44771,\"start\":44765},{\"end\":45052,\"start\":45036},{\"end\":45065,\"start\":45060},{\"end\":45075,\"start\":45069},{\"end\":45273,\"start\":45270},{\"end\":45281,\"start\":45277},{\"end\":45288,\"start\":45285},{\"end\":45675,\"start\":45664},{\"end\":45686,\"start\":45679},{\"end\":45694,\"start\":45690},{\"end\":46080,\"start\":46074},{\"end\":46090,\"start\":46084},{\"end\":46102,\"start\":46094},{\"end\":46113,\"start\":46106},{\"end\":46442,\"start\":46431},{\"end\":46450,\"start\":46446},{\"end\":46456,\"start\":46454},{\"end\":46466,\"start\":46460},{\"end\":46478,\"start\":46470},{\"end\":46484,\"start\":46482},{\"end\":46493,\"start\":46488},{\"end\":46505,\"start\":46497},{\"end\":46515,\"start\":46509},{\"end\":46528,\"start\":46519},{\"end\":46538,\"start\":46534},{\"end\":46549,\"start\":46542},{\"end\":46919,\"start\":46913},{\"end\":46930,\"start\":46925},{\"end\":46938,\"start\":46936},{\"end\":47223,\"start\":47217},{\"end\":47230,\"start\":47227},{\"end\":47238,\"start\":47236},{\"end\":47569,\"start\":47565},{\"end\":47583,\"start\":47573},{\"end\":47982,\"start\":47976},{\"end\":47998,\"start\":47988},{\"end\":48011,\"start\":48004},{\"end\":48027,\"start\":48017},{\"end\":48342,\"start\":48332},{\"end\":48350,\"start\":48346},{\"end\":48359,\"start\":48354},{\"end\":48368,\"start\":48363},{\"end\":48375,\"start\":48372},{\"end\":48390,\"start\":48379},{\"end\":48398,\"start\":48394},{\"end\":48705,\"start\":48689},{\"end\":48714,\"start\":48709},{\"end\":48724,\"start\":48718},{\"end\":48738,\"start\":48728},{\"end\":48753,\"start\":48742},{\"end\":49112,\"start\":49108},{\"end\":49121,\"start\":49116},{\"end\":49128,\"start\":49125},{\"end\":49139,\"start\":49132},{\"end\":49536,\"start\":49532},{\"end\":49547,\"start\":49542},{\"end\":49559,\"start\":49553},{\"end\":49575,\"start\":49565},{\"end\":49913,\"start\":49902},{\"end\":49921,\"start\":49917},{\"end\":49929,\"start\":49925},{\"end\":50228,\"start\":50226},{\"end\":50237,\"start\":50232},{\"end\":50246,\"start\":50241},{\"end\":50255,\"start\":50250},{\"end\":50264,\"start\":50259},{\"end\":50275,\"start\":50271},{\"end\":50556,\"start\":50554},{\"end\":50566,\"start\":50560},{\"end\":50577,\"start\":50570},{\"end\":50584,\"start\":50581},{\"end\":50895,\"start\":50888},{\"end\":50904,\"start\":50899},{\"end\":51182,\"start\":51178},{\"end\":51191,\"start\":51186},{\"end\":51202,\"start\":51195},{\"end\":51212,\"start\":51208},{\"end\":35879,\"start\":35872},{\"end\":35891,\"start\":35883},{\"end\":35900,\"start\":35895},{\"end\":36225,\"start\":36221},{\"end\":36233,\"start\":36229},{\"end\":36245,\"start\":36237},{\"end\":36259,\"start\":36249},{\"end\":36542,\"start\":36537},{\"end\":36552,\"start\":36546},{\"end\":36985,\"start\":36978},{\"end\":36995,\"start\":36991},{\"end\":37007,\"start\":37001},{\"end\":37017,\"start\":37011},{\"end\":37289,\"start\":37284},{\"end\":37300,\"start\":37293},{\"end\":37310,\"start\":37304},{\"end\":37634,\"start\":37630},{\"end\":37643,\"start\":37640},{\"end\":37655,\"start\":37647},{\"end\":37663,\"start\":37659},{\"end\":37961,\"start\":37955},{\"end\":37969,\"start\":37965},{\"end\":37980,\"start\":37973},{\"end\":37991,\"start\":37984},{\"end\":38264,\"start\":38258},{\"end\":38272,\"start\":38268},{\"end\":38283,\"start\":38276},{\"end\":38605,\"start\":38599},{\"end\":38618,\"start\":38613},{\"end\":38631,\"start\":38624},{\"end\":38835,\"start\":38830},{\"end\":38846,\"start\":38839},{\"end\":38860,\"start\":38850},{\"end\":38869,\"start\":38864},{\"end\":38881,\"start\":38873},{\"end\":38892,\"start\":38885},{\"end\":39097,\"start\":39088},{\"end\":39198,\"start\":39196},{\"end\":39207,\"start\":39202},{\"end\":39214,\"start\":39211},{\"end\":39221,\"start\":39218},{\"end\":39630,\"start\":39625},{\"end\":39639,\"start\":39634},{\"end\":39649,\"start\":39643},{\"end\":39658,\"start\":39653},{\"end\":40026,\"start\":40022},{\"end\":40039,\"start\":40032},{\"end\":40217,\"start\":40208},{\"end\":40229,\"start\":40221},{\"end\":40242,\"start\":40233},{\"end\":40562,\"start\":40557},{\"end\":40574,\"start\":40568},{\"end\":40588,\"start\":40580},{\"end\":40928,\"start\":40925},{\"end\":40941,\"start\":40932},{\"end\":40952,\"start\":40945},{\"end\":40963,\"start\":40956},{\"end\":40971,\"start\":40967},{\"end\":40983,\"start\":40975},{\"end\":40997,\"start\":40987},{\"end\":41008,\"start\":41001},{\"end\":41480,\"start\":41473},{\"end\":41495,\"start\":41484},{\"end\":41507,\"start\":41499},{\"end\":41516,\"start\":41511},{\"end\":41527,\"start\":41520},{\"end\":41539,\"start\":41531},{\"end\":41546,\"start\":41543},{\"end\":41966,\"start\":41960},{\"end\":41972,\"start\":41970},{\"end\":42187,\"start\":42177},{\"end\":42199,\"start\":42191},{\"end\":42208,\"start\":42203},{\"end\":42578,\"start\":42573},{\"end\":42591,\"start\":42582},{\"end\":42606,\"start\":42595},{\"end\":42617,\"start\":42610},{\"end\":42626,\"start\":42621},{\"end\":42995,\"start\":42992},{\"end\":43003,\"start\":42999},{\"end\":43010,\"start\":43007},{\"end\":43452,\"start\":43449},{\"end\":43460,\"start\":43456},{\"end\":43467,\"start\":43464},{\"end\":43475,\"start\":43471},{\"end\":43820,\"start\":43816},{\"end\":43833,\"start\":43824},{\"end\":43844,\"start\":43837},{\"end\":44279,\"start\":44264},{\"end\":44491,\"start\":44487},{\"end\":44744,\"start\":44735},{\"end\":44759,\"start\":44752},{\"end\":44771,\"start\":44765},{\"end\":45052,\"start\":45036},{\"end\":45065,\"start\":45060},{\"end\":45075,\"start\":45069},{\"end\":45273,\"start\":45270},{\"end\":45281,\"start\":45277},{\"end\":45288,\"start\":45285},{\"end\":45675,\"start\":45664},{\"end\":45686,\"start\":45679},{\"end\":45694,\"start\":45690},{\"end\":46080,\"start\":46074},{\"end\":46090,\"start\":46084},{\"end\":46102,\"start\":46094},{\"end\":46113,\"start\":46106},{\"end\":46442,\"start\":46431},{\"end\":46450,\"start\":46446},{\"end\":46456,\"start\":46454},{\"end\":46466,\"start\":46460},{\"end\":46478,\"start\":46470},{\"end\":46484,\"start\":46482},{\"end\":46493,\"start\":46488},{\"end\":46505,\"start\":46497},{\"end\":46515,\"start\":46509},{\"end\":46528,\"start\":46519},{\"end\":46538,\"start\":46534},{\"end\":46549,\"start\":46542},{\"end\":46919,\"start\":46913},{\"end\":46930,\"start\":46925},{\"end\":46938,\"start\":46936},{\"end\":47223,\"start\":47217},{\"end\":47230,\"start\":47227},{\"end\":47238,\"start\":47236},{\"end\":47569,\"start\":47565},{\"end\":47583,\"start\":47573},{\"end\":47982,\"start\":47976},{\"end\":47998,\"start\":47988},{\"end\":48011,\"start\":48004},{\"end\":48027,\"start\":48017},{\"end\":48342,\"start\":48332},{\"end\":48350,\"start\":48346},{\"end\":48359,\"start\":48354},{\"end\":48368,\"start\":48363},{\"end\":48375,\"start\":48372},{\"end\":48390,\"start\":48379},{\"end\":48398,\"start\":48394},{\"end\":48705,\"start\":48689},{\"end\":48714,\"start\":48709},{\"end\":48724,\"start\":48718},{\"end\":48738,\"start\":48728},{\"end\":48753,\"start\":48742},{\"end\":49112,\"start\":49108},{\"end\":49121,\"start\":49116},{\"end\":49128,\"start\":49125},{\"end\":49139,\"start\":49132},{\"end\":49536,\"start\":49532},{\"end\":49547,\"start\":49542},{\"end\":49559,\"start\":49553},{\"end\":49575,\"start\":49565},{\"end\":49913,\"start\":49902},{\"end\":49921,\"start\":49917},{\"end\":49929,\"start\":49925},{\"end\":50228,\"start\":50226},{\"end\":50237,\"start\":50232},{\"end\":50246,\"start\":50241},{\"end\":50255,\"start\":50250},{\"end\":50264,\"start\":50259},{\"end\":50275,\"start\":50271},{\"end\":50556,\"start\":50554},{\"end\":50566,\"start\":50560},{\"end\":50577,\"start\":50570},{\"end\":50584,\"start\":50581},{\"end\":50895,\"start\":50888},{\"end\":50904,\"start\":50899},{\"end\":51182,\"start\":51178},{\"end\":51191,\"start\":51186},{\"end\":51202,\"start\":51195},{\"end\":51212,\"start\":51208}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1637703},\"end\":36181,\"start\":35843},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12578058},\"end\":36479,\"start\":36183},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206590483},\"end\":36868,\"start\":36481},{\"attributes\":{\"id\":\"b3\"},\"end\":37205,\"start\":36870},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2255738},\"end\":37551,\"start\":37207},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":299085},\"end\":37909,\"start\":37553},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9455111},\"end\":38183,\"start\":37911},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6724907},\"end\":38526,\"start\":38185},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206596513},\"end\":38766,\"start\":38528},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12733543},\"end\":39084,\"start\":38768},{\"attributes\":{\"id\":\"b10\"},\"end\":39146,\"start\":39086},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":39553,\"start\":39148},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5849243},\"end\":39990,\"start\":39555},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1371968},\"end\":40174,\"start\":39992},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6099034},\"end\":40449,\"start\":40176},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6247123},\"end\":40859,\"start\":40451},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1799558},\"end\":41397,\"start\":40861},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2658860},\"end\":41912,\"start\":41399},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b18\"},\"end\":42107,\"start\":41914},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16790081},\"end\":42501,\"start\":42109},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11091110},\"end\":42913,\"start\":42503},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13153},\"end\":43361,\"start\":42915},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15774646},\"end\":43756,\"start\":43363},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1629541},\"end\":44188,\"start\":43758},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4327732},\"end\":44423,\"start\":44190},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":221242327},\"end\":44673,\"start\":44425},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206775100},\"end\":44970,\"start\":44675},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":545361},\"end\":45208,\"start\":44972},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":623137},\"end\":45595,\"start\":45210},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3719281},\"end\":46023,\"start\":45597},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206769866},\"end\":46376,\"start\":46025},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2930547},\"end\":46864,\"start\":46378},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10748875},\"end\":47150,\"start\":46866},{\"attributes\":{\"id\":\"b33\"},\"end\":47487,\"start\":47152},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3015455},\"end\":47935,\"start\":47489},{\"attributes\":{\"id\":\"b35\"},\"end\":48265,\"start\":47937},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6159584},\"end\":48685,\"start\":48267},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b37\"},\"end\":49010,\"start\":48687},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9114952},\"end\":49454,\"start\":49012},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":207761262},\"end\":49814,\"start\":49456},{\"attributes\":{\"doi\":\"arXiv:1711.05919\",\"id\":\"b40\"},\"end\":50126,\"start\":49816},{\"attributes\":{\"doi\":\"arXiv:1705.08260\",\"id\":\"b41\"},\"end\":50505,\"start\":50128},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":602850},\"end\":50799,\"start\":50507},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6913648},\"end\":51116,\"start\":50801},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":11977588},\"end\":51341,\"start\":51118},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1637703},\"end\":36181,\"start\":35843},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12578058},\"end\":36479,\"start\":36183},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206590483},\"end\":36868,\"start\":36481},{\"attributes\":{\"id\":\"b3\"},\"end\":37205,\"start\":36870},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2255738},\"end\":37551,\"start\":37207},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":299085},\"end\":37909,\"start\":37553},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9455111},\"end\":38183,\"start\":37911},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6724907},\"end\":38526,\"start\":38185},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206596513},\"end\":38766,\"start\":38528},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12733543},\"end\":39084,\"start\":38768},{\"attributes\":{\"id\":\"b10\"},\"end\":39146,\"start\":39086},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":39553,\"start\":39148},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5849243},\"end\":39990,\"start\":39555},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1371968},\"end\":40174,\"start\":39992},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6099034},\"end\":40449,\"start\":40176},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6247123},\"end\":40859,\"start\":40451},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1799558},\"end\":41397,\"start\":40861},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2658860},\"end\":41912,\"start\":41399},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b18\"},\"end\":42107,\"start\":41914},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16790081},\"end\":42501,\"start\":42109},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11091110},\"end\":42913,\"start\":42503},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13153},\"end\":43361,\"start\":42915},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15774646},\"end\":43756,\"start\":43363},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1629541},\"end\":44188,\"start\":43758},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4327732},\"end\":44423,\"start\":44190},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":221242327},\"end\":44673,\"start\":44425},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206775100},\"end\":44970,\"start\":44675},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":545361},\"end\":45208,\"start\":44972},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":623137},\"end\":45595,\"start\":45210},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3719281},\"end\":46023,\"start\":45597},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206769866},\"end\":46376,\"start\":46025},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2930547},\"end\":46864,\"start\":46378},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10748875},\"end\":47150,\"start\":46866},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":253064894},\"end\":47487,\"start\":47152},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3015455},\"end\":47935,\"start\":47489},{\"attributes\":{\"id\":\"b35\"},\"end\":48265,\"start\":47937},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6159584},\"end\":48685,\"start\":48267},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b37\"},\"end\":49010,\"start\":48687},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9114952},\"end\":49454,\"start\":49012},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":207761262},\"end\":49814,\"start\":49456},{\"attributes\":{\"doi\":\"arXiv:1711.05919\",\"id\":\"b40\"},\"end\":50126,\"start\":49816},{\"attributes\":{\"doi\":\"arXiv:1705.08260\",\"id\":\"b41\"},\"end\":50505,\"start\":50128},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":602850},\"end\":50799,\"start\":50507},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6913648},\"end\":51116,\"start\":50801},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":11977588},\"end\":51341,\"start\":51118}]", "bib_title": "[{\"end\":35868,\"start\":35843},{\"end\":36215,\"start\":36183},{\"end\":36533,\"start\":36481},{\"end\":37280,\"start\":37207},{\"end\":37626,\"start\":37553},{\"end\":37951,\"start\":37911},{\"end\":38254,\"start\":38185},{\"end\":38595,\"start\":38528},{\"end\":38826,\"start\":38768},{\"end\":39192,\"start\":39148},{\"end\":39621,\"start\":39555},{\"end\":40016,\"start\":39992},{\"end\":40204,\"start\":40176},{\"end\":40551,\"start\":40451},{\"end\":40921,\"start\":40861},{\"end\":41469,\"start\":41399},{\"end\":42173,\"start\":42109},{\"end\":42569,\"start\":42503},{\"end\":42988,\"start\":42915},{\"end\":43445,\"start\":43363},{\"end\":43812,\"start\":43758},{\"end\":44258,\"start\":44190},{\"end\":44481,\"start\":44425},{\"end\":44731,\"start\":44675},{\"end\":45030,\"start\":44972},{\"end\":45266,\"start\":45210},{\"end\":45660,\"start\":45597},{\"end\":46070,\"start\":46025},{\"end\":46427,\"start\":46378},{\"end\":46909,\"start\":46866},{\"end\":47213,\"start\":47152},{\"end\":47561,\"start\":47489},{\"end\":47972,\"start\":47937},{\"end\":48328,\"start\":48267},{\"end\":49104,\"start\":49012},{\"end\":49528,\"start\":49456},{\"end\":50548,\"start\":50507},{\"end\":50884,\"start\":50801},{\"end\":51174,\"start\":51118},{\"end\":35868,\"start\":35843},{\"end\":36215,\"start\":36183},{\"end\":36533,\"start\":36481},{\"end\":37280,\"start\":37207},{\"end\":37626,\"start\":37553},{\"end\":37951,\"start\":37911},{\"end\":38254,\"start\":38185},{\"end\":38595,\"start\":38528},{\"end\":38826,\"start\":38768},{\"end\":39192,\"start\":39148},{\"end\":39621,\"start\":39555},{\"end\":40016,\"start\":39992},{\"end\":40204,\"start\":40176},{\"end\":40551,\"start\":40451},{\"end\":40921,\"start\":40861},{\"end\":41469,\"start\":41399},{\"end\":42173,\"start\":42109},{\"end\":42569,\"start\":42503},{\"end\":42988,\"start\":42915},{\"end\":43445,\"start\":43363},{\"end\":43812,\"start\":43758},{\"end\":44258,\"start\":44190},{\"end\":44481,\"start\":44425},{\"end\":44731,\"start\":44675},{\"end\":45030,\"start\":44972},{\"end\":45266,\"start\":45210},{\"end\":45660,\"start\":45597},{\"end\":46070,\"start\":46025},{\"end\":46427,\"start\":46378},{\"end\":46909,\"start\":46866},{\"end\":47213,\"start\":47152},{\"end\":47561,\"start\":47489},{\"end\":47972,\"start\":47937},{\"end\":48328,\"start\":48267},{\"end\":49104,\"start\":49012},{\"end\":49528,\"start\":49456},{\"end\":50548,\"start\":50507},{\"end\":50884,\"start\":50801},{\"end\":51174,\"start\":51118}]", "bib_author": "[{\"end\":35881,\"start\":35870},{\"end\":35893,\"start\":35881},{\"end\":35902,\"start\":35893},{\"end\":36227,\"start\":36217},{\"end\":36235,\"start\":36227},{\"end\":36247,\"start\":36235},{\"end\":36261,\"start\":36247},{\"end\":36544,\"start\":36535},{\"end\":36554,\"start\":36544},{\"end\":36987,\"start\":36974},{\"end\":36997,\"start\":36987},{\"end\":37009,\"start\":36997},{\"end\":37019,\"start\":37009},{\"end\":37291,\"start\":37282},{\"end\":37302,\"start\":37291},{\"end\":37312,\"start\":37302},{\"end\":37636,\"start\":37628},{\"end\":37645,\"start\":37636},{\"end\":37657,\"start\":37645},{\"end\":37665,\"start\":37657},{\"end\":37963,\"start\":37953},{\"end\":37971,\"start\":37963},{\"end\":37982,\"start\":37971},{\"end\":37993,\"start\":37982},{\"end\":38266,\"start\":38256},{\"end\":38274,\"start\":38266},{\"end\":38285,\"start\":38274},{\"end\":38607,\"start\":38597},{\"end\":38620,\"start\":38607},{\"end\":38633,\"start\":38620},{\"end\":38837,\"start\":38828},{\"end\":38848,\"start\":38837},{\"end\":38862,\"start\":38848},{\"end\":38871,\"start\":38862},{\"end\":38883,\"start\":38871},{\"end\":38894,\"start\":38883},{\"end\":39099,\"start\":39088},{\"end\":39200,\"start\":39194},{\"end\":39209,\"start\":39200},{\"end\":39216,\"start\":39209},{\"end\":39223,\"start\":39216},{\"end\":39632,\"start\":39623},{\"end\":39641,\"start\":39632},{\"end\":39651,\"start\":39641},{\"end\":39660,\"start\":39651},{\"end\":40028,\"start\":40018},{\"end\":40041,\"start\":40028},{\"end\":40219,\"start\":40206},{\"end\":40231,\"start\":40219},{\"end\":40244,\"start\":40231},{\"end\":40564,\"start\":40553},{\"end\":40576,\"start\":40564},{\"end\":40590,\"start\":40576},{\"end\":40930,\"start\":40923},{\"end\":40943,\"start\":40930},{\"end\":40954,\"start\":40943},{\"end\":40965,\"start\":40954},{\"end\":40973,\"start\":40965},{\"end\":40985,\"start\":40973},{\"end\":40999,\"start\":40985},{\"end\":41010,\"start\":40999},{\"end\":41482,\"start\":41471},{\"end\":41497,\"start\":41482},{\"end\":41509,\"start\":41497},{\"end\":41518,\"start\":41509},{\"end\":41529,\"start\":41518},{\"end\":41541,\"start\":41529},{\"end\":41548,\"start\":41541},{\"end\":41968,\"start\":41958},{\"end\":41974,\"start\":41968},{\"end\":42189,\"start\":42175},{\"end\":42201,\"start\":42189},{\"end\":42210,\"start\":42201},{\"end\":42580,\"start\":42571},{\"end\":42593,\"start\":42580},{\"end\":42608,\"start\":42593},{\"end\":42619,\"start\":42608},{\"end\":42628,\"start\":42619},{\"end\":42997,\"start\":42990},{\"end\":43005,\"start\":42997},{\"end\":43012,\"start\":43005},{\"end\":43454,\"start\":43447},{\"end\":43462,\"start\":43454},{\"end\":43469,\"start\":43462},{\"end\":43477,\"start\":43469},{\"end\":43822,\"start\":43814},{\"end\":43835,\"start\":43822},{\"end\":43846,\"start\":43835},{\"end\":44281,\"start\":44260},{\"end\":44493,\"start\":44483},{\"end\":44746,\"start\":44733},{\"end\":44761,\"start\":44746},{\"end\":44773,\"start\":44761},{\"end\":45054,\"start\":45032},{\"end\":45067,\"start\":45054},{\"end\":45077,\"start\":45067},{\"end\":45275,\"start\":45268},{\"end\":45283,\"start\":45275},{\"end\":45290,\"start\":45283},{\"end\":45677,\"start\":45662},{\"end\":45688,\"start\":45677},{\"end\":45696,\"start\":45688},{\"end\":46082,\"start\":46072},{\"end\":46092,\"start\":46082},{\"end\":46104,\"start\":46092},{\"end\":46115,\"start\":46104},{\"end\":46444,\"start\":46429},{\"end\":46452,\"start\":46444},{\"end\":46458,\"start\":46452},{\"end\":46468,\"start\":46458},{\"end\":46480,\"start\":46468},{\"end\":46486,\"start\":46480},{\"end\":46495,\"start\":46486},{\"end\":46507,\"start\":46495},{\"end\":46517,\"start\":46507},{\"end\":46530,\"start\":46517},{\"end\":46540,\"start\":46530},{\"end\":46551,\"start\":46540},{\"end\":46921,\"start\":46911},{\"end\":46932,\"start\":46921},{\"end\":46940,\"start\":46932},{\"end\":47225,\"start\":47215},{\"end\":47232,\"start\":47225},{\"end\":47240,\"start\":47232},{\"end\":47571,\"start\":47563},{\"end\":47585,\"start\":47571},{\"end\":47984,\"start\":47974},{\"end\":48000,\"start\":47984},{\"end\":48013,\"start\":48000},{\"end\":48029,\"start\":48013},{\"end\":48344,\"start\":48330},{\"end\":48352,\"start\":48344},{\"end\":48361,\"start\":48352},{\"end\":48370,\"start\":48361},{\"end\":48377,\"start\":48370},{\"end\":48392,\"start\":48377},{\"end\":48400,\"start\":48392},{\"end\":48707,\"start\":48687},{\"end\":48716,\"start\":48707},{\"end\":48726,\"start\":48716},{\"end\":48740,\"start\":48726},{\"end\":48755,\"start\":48740},{\"end\":49114,\"start\":49106},{\"end\":49123,\"start\":49114},{\"end\":49130,\"start\":49123},{\"end\":49141,\"start\":49130},{\"end\":49538,\"start\":49530},{\"end\":49549,\"start\":49538},{\"end\":49561,\"start\":49549},{\"end\":49577,\"start\":49561},{\"end\":49915,\"start\":49898},{\"end\":49923,\"start\":49915},{\"end\":49931,\"start\":49923},{\"end\":50230,\"start\":50224},{\"end\":50239,\"start\":50230},{\"end\":50248,\"start\":50239},{\"end\":50257,\"start\":50248},{\"end\":50266,\"start\":50257},{\"end\":50277,\"start\":50266},{\"end\":50558,\"start\":50550},{\"end\":50568,\"start\":50558},{\"end\":50579,\"start\":50568},{\"end\":50586,\"start\":50579},{\"end\":50897,\"start\":50886},{\"end\":50906,\"start\":50897},{\"end\":51184,\"start\":51176},{\"end\":51193,\"start\":51184},{\"end\":51204,\"start\":51193},{\"end\":51214,\"start\":51204},{\"end\":35881,\"start\":35870},{\"end\":35893,\"start\":35881},{\"end\":35902,\"start\":35893},{\"end\":36227,\"start\":36217},{\"end\":36235,\"start\":36227},{\"end\":36247,\"start\":36235},{\"end\":36261,\"start\":36247},{\"end\":36544,\"start\":36535},{\"end\":36554,\"start\":36544},{\"end\":36987,\"start\":36974},{\"end\":36997,\"start\":36987},{\"end\":37009,\"start\":36997},{\"end\":37019,\"start\":37009},{\"end\":37291,\"start\":37282},{\"end\":37302,\"start\":37291},{\"end\":37312,\"start\":37302},{\"end\":37636,\"start\":37628},{\"end\":37645,\"start\":37636},{\"end\":37657,\"start\":37645},{\"end\":37665,\"start\":37657},{\"end\":37963,\"start\":37953},{\"end\":37971,\"start\":37963},{\"end\":37982,\"start\":37971},{\"end\":37993,\"start\":37982},{\"end\":38266,\"start\":38256},{\"end\":38274,\"start\":38266},{\"end\":38285,\"start\":38274},{\"end\":38607,\"start\":38597},{\"end\":38620,\"start\":38607},{\"end\":38633,\"start\":38620},{\"end\":38837,\"start\":38828},{\"end\":38848,\"start\":38837},{\"end\":38862,\"start\":38848},{\"end\":38871,\"start\":38862},{\"end\":38883,\"start\":38871},{\"end\":38894,\"start\":38883},{\"end\":39099,\"start\":39088},{\"end\":39200,\"start\":39194},{\"end\":39209,\"start\":39200},{\"end\":39216,\"start\":39209},{\"end\":39223,\"start\":39216},{\"end\":39632,\"start\":39623},{\"end\":39641,\"start\":39632},{\"end\":39651,\"start\":39641},{\"end\":39660,\"start\":39651},{\"end\":40028,\"start\":40018},{\"end\":40041,\"start\":40028},{\"end\":40219,\"start\":40206},{\"end\":40231,\"start\":40219},{\"end\":40244,\"start\":40231},{\"end\":40564,\"start\":40553},{\"end\":40576,\"start\":40564},{\"end\":40590,\"start\":40576},{\"end\":40930,\"start\":40923},{\"end\":40943,\"start\":40930},{\"end\":40954,\"start\":40943},{\"end\":40965,\"start\":40954},{\"end\":40973,\"start\":40965},{\"end\":40985,\"start\":40973},{\"end\":40999,\"start\":40985},{\"end\":41010,\"start\":40999},{\"end\":41482,\"start\":41471},{\"end\":41497,\"start\":41482},{\"end\":41509,\"start\":41497},{\"end\":41518,\"start\":41509},{\"end\":41529,\"start\":41518},{\"end\":41541,\"start\":41529},{\"end\":41548,\"start\":41541},{\"end\":41968,\"start\":41958},{\"end\":41974,\"start\":41968},{\"end\":42189,\"start\":42175},{\"end\":42201,\"start\":42189},{\"end\":42210,\"start\":42201},{\"end\":42580,\"start\":42571},{\"end\":42593,\"start\":42580},{\"end\":42608,\"start\":42593},{\"end\":42619,\"start\":42608},{\"end\":42628,\"start\":42619},{\"end\":42997,\"start\":42990},{\"end\":43005,\"start\":42997},{\"end\":43012,\"start\":43005},{\"end\":43454,\"start\":43447},{\"end\":43462,\"start\":43454},{\"end\":43469,\"start\":43462},{\"end\":43477,\"start\":43469},{\"end\":43822,\"start\":43814},{\"end\":43835,\"start\":43822},{\"end\":43846,\"start\":43835},{\"end\":44281,\"start\":44260},{\"end\":44493,\"start\":44483},{\"end\":44746,\"start\":44733},{\"end\":44761,\"start\":44746},{\"end\":44773,\"start\":44761},{\"end\":45054,\"start\":45032},{\"end\":45067,\"start\":45054},{\"end\":45077,\"start\":45067},{\"end\":45275,\"start\":45268},{\"end\":45283,\"start\":45275},{\"end\":45290,\"start\":45283},{\"end\":45677,\"start\":45662},{\"end\":45688,\"start\":45677},{\"end\":45696,\"start\":45688},{\"end\":46082,\"start\":46072},{\"end\":46092,\"start\":46082},{\"end\":46104,\"start\":46092},{\"end\":46115,\"start\":46104},{\"end\":46444,\"start\":46429},{\"end\":46452,\"start\":46444},{\"end\":46458,\"start\":46452},{\"end\":46468,\"start\":46458},{\"end\":46480,\"start\":46468},{\"end\":46486,\"start\":46480},{\"end\":46495,\"start\":46486},{\"end\":46507,\"start\":46495},{\"end\":46517,\"start\":46507},{\"end\":46530,\"start\":46517},{\"end\":46540,\"start\":46530},{\"end\":46551,\"start\":46540},{\"end\":46921,\"start\":46911},{\"end\":46932,\"start\":46921},{\"end\":46940,\"start\":46932},{\"end\":47225,\"start\":47215},{\"end\":47232,\"start\":47225},{\"end\":47240,\"start\":47232},{\"end\":47571,\"start\":47563},{\"end\":47585,\"start\":47571},{\"end\":47984,\"start\":47974},{\"end\":48000,\"start\":47984},{\"end\":48013,\"start\":48000},{\"end\":48029,\"start\":48013},{\"end\":48344,\"start\":48330},{\"end\":48352,\"start\":48344},{\"end\":48361,\"start\":48352},{\"end\":48370,\"start\":48361},{\"end\":48377,\"start\":48370},{\"end\":48392,\"start\":48377},{\"end\":48400,\"start\":48392},{\"end\":48707,\"start\":48687},{\"end\":48716,\"start\":48707},{\"end\":48726,\"start\":48716},{\"end\":48740,\"start\":48726},{\"end\":48755,\"start\":48740},{\"end\":49114,\"start\":49106},{\"end\":49123,\"start\":49114},{\"end\":49130,\"start\":49123},{\"end\":49141,\"start\":49130},{\"end\":49538,\"start\":49530},{\"end\":49549,\"start\":49538},{\"end\":49561,\"start\":49549},{\"end\":49577,\"start\":49561},{\"end\":49915,\"start\":49898},{\"end\":49923,\"start\":49915},{\"end\":49931,\"start\":49923},{\"end\":50230,\"start\":50224},{\"end\":50239,\"start\":50230},{\"end\":50248,\"start\":50239},{\"end\":50257,\"start\":50248},{\"end\":50266,\"start\":50257},{\"end\":50277,\"start\":50266},{\"end\":50558,\"start\":50550},{\"end\":50568,\"start\":50558},{\"end\":50579,\"start\":50568},{\"end\":50586,\"start\":50579},{\"end\":50897,\"start\":50886},{\"end\":50906,\"start\":50897},{\"end\":51184,\"start\":51176},{\"end\":51193,\"start\":51184},{\"end\":51204,\"start\":51193},{\"end\":51214,\"start\":51204}]", "bib_venue": "[{\"end\":35969,\"start\":35902},{\"end\":36310,\"start\":36261},{\"end\":36647,\"start\":36554},{\"end\":36972,\"start\":36870},{\"end\":37361,\"start\":37312},{\"end\":37703,\"start\":37665},{\"end\":38035,\"start\":37993},{\"end\":38345,\"start\":38285},{\"end\":38637,\"start\":38633},{\"end\":38914,\"start\":38894},{\"end\":39300,\"start\":39223},{\"end\":39727,\"start\":39660},{\"end\":40064,\"start\":40041},{\"end\":40293,\"start\":40244},{\"end\":40628,\"start\":40590},{\"end\":41076,\"start\":41010},{\"end\":41616,\"start\":41548},{\"end\":41956,\"start\":41914},{\"end\":42281,\"start\":42210},{\"end\":42684,\"start\":42628},{\"end\":43089,\"start\":43012},{\"end\":43539,\"start\":43477},{\"end\":43923,\"start\":43846},{\"end\":44287,\"start\":44281},{\"end\":44533,\"start\":44493},{\"end\":44802,\"start\":44773},{\"end\":45081,\"start\":45077},{\"end\":45357,\"start\":45290},{\"end\":45782,\"start\":45696},{\"end\":46176,\"start\":46115},{\"end\":46598,\"start\":46551},{\"end\":46989,\"start\":46940},{\"end\":47302,\"start\":47240},{\"end\":47662,\"start\":47585},{\"end\":48072,\"start\":48029},{\"end\":48464,\"start\":48400},{\"end\":48823,\"start\":48771},{\"end\":49178,\"start\":49141},{\"end\":49614,\"start\":49577},{\"end\":49896,\"start\":49816},{\"end\":50222,\"start\":50128},{\"end\":50624,\"start\":50586},{\"end\":50942,\"start\":50906},{\"end\":51218,\"start\":51214},{\"end\":35969,\"start\":35902},{\"end\":36310,\"start\":36261},{\"end\":36647,\"start\":36554},{\"end\":36972,\"start\":36870},{\"end\":37361,\"start\":37312},{\"end\":37703,\"start\":37665},{\"end\":38035,\"start\":37993},{\"end\":38345,\"start\":38285},{\"end\":38637,\"start\":38633},{\"end\":38914,\"start\":38894},{\"end\":39300,\"start\":39223},{\"end\":39727,\"start\":39660},{\"end\":40064,\"start\":40041},{\"end\":40293,\"start\":40244},{\"end\":40628,\"start\":40590},{\"end\":41076,\"start\":41010},{\"end\":41616,\"start\":41548},{\"end\":41956,\"start\":41914},{\"end\":42281,\"start\":42210},{\"end\":42684,\"start\":42628},{\"end\":43089,\"start\":43012},{\"end\":43539,\"start\":43477},{\"end\":43923,\"start\":43846},{\"end\":44287,\"start\":44281},{\"end\":44533,\"start\":44493},{\"end\":44802,\"start\":44773},{\"end\":45081,\"start\":45077},{\"end\":45357,\"start\":45290},{\"end\":45782,\"start\":45696},{\"end\":46176,\"start\":46115},{\"end\":46598,\"start\":46551},{\"end\":46989,\"start\":46940},{\"end\":47302,\"start\":47240},{\"end\":47662,\"start\":47585},{\"end\":48072,\"start\":48029},{\"end\":48464,\"start\":48400},{\"end\":48823,\"start\":48771},{\"end\":49178,\"start\":49141},{\"end\":49614,\"start\":49577},{\"end\":49896,\"start\":49816},{\"end\":50222,\"start\":50128},{\"end\":50624,\"start\":50586},{\"end\":50942,\"start\":50906},{\"end\":51218,\"start\":51214},{\"end\":36023,\"start\":35971},{\"end\":39364,\"start\":39302},{\"end\":39781,\"start\":39729},{\"end\":41129,\"start\":41078},{\"end\":41671,\"start\":41618},{\"end\":43153,\"start\":43091},{\"end\":43987,\"start\":43925},{\"end\":45411,\"start\":45359},{\"end\":47726,\"start\":47664},{\"end\":36023,\"start\":35971},{\"end\":39364,\"start\":39302},{\"end\":39781,\"start\":39729},{\"end\":41129,\"start\":41078},{\"end\":41671,\"start\":41618},{\"end\":43153,\"start\":43091},{\"end\":43987,\"start\":43925},{\"end\":45411,\"start\":45359},{\"end\":47726,\"start\":47664}]"}}}, "year": 2023, "month": 12, "day": 17}