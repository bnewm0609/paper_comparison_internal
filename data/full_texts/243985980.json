{"id": 243985980, "updated": "2023-10-05 19:50:06.515", "metadata": {"title": "Masked Autoencoders Are Scalable Vision Learners", "authors": "[{\"first\":\"Kaiming\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Xinlei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Saining\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Yanghao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Piotr\",\"last\":\"Doll'ar\",\"middle\":[]},{\"first\":\"Ross\",\"last\":\"Girshick\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.06377", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/HeCXLDG22", "doi": "10.1109/cvpr52688.2022.01553"}}, "content": {"source": {"pdf_hash": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.06377v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8ae589ec1ba76bc6e5a67273a09413372ab98416", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6351ebb4a3287f5f3e1273464b3b91e5df5a16d7.txt", "contents": "\nMasked Autoencoders Are Scalable Vision Learners\n\n\nKaiming He \nXinlei Chen \nSaining Xie \nYanghao Li \nPiotr Doll\u00e1r \nRoss Girshick \nMasked Autoencoders Are Scalable Vision Learners\n* equal technical contribution \u2020 project lead Facebook AI Research (FAIR)\nThis paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3\u00d7 or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.\n\nIntroduction\n\nDeep learning has witnessed an explosion of architectures of continuously growing capability and capacity [33,25,57]. Aided by the rapid gains in hardware, models today can easily overfit one million images [13] and begin to demand hundreds of millions of-often publicly inaccessible-labeled images [16].\n\nThis appetite for data has been successfully addressed in natural language processing (NLP) by self-supervised pretraining. The solutions, based on autoregressive language modeling in GPT [47,48,4] and masked autoencoding in BERT [14], are conceptually simple: they remove a portion of the data and learn to predict the removed content. These methods now enable training of generalizable NLP models containing over one hundred billion parameters [4].\n\nThe idea of masked autoencoders, a form of more general denoising autoencoders [58], is natural and applicable in computer vision as well. Indeed, closely related research During pre-training, a large random subset of image patches (e.g., 75%) is masked out. The encoder is applied to the small subset of visible patches. Mask tokens are introduced after the encoder, and the full set of encoded patches and mask tokens is processed by a small decoder that reconstructs the original image in pixels. After pre-training, the decoder is discarded and the encoder is applied to uncorrupted images (full sets of patches) for recognition tasks.\n\nin vision [59,46] preceded BERT. However, despite significant interest in this idea following the success of BERT, progress of autoencoding methods in vision lags behind NLP. We ask: what makes masked autoencoding different between vision and language?\n\nWe attempt to answer this question from the following perspectives: (i) Until recently, architectures were different. In vision, convolutional networks [34] were dominant over the last decade [33]. Convolutions typically operate on regular grids and it is not straightforward to integrate 'indicators' such as mask tokens [14] or positional embeddings [57] into convolutional networks. This architectural gap, however, has been addressed with the introduction of Vision Transformers (ViT) [16] and should no longer present an obstacle.\n\n(ii) Information density is different between language and vision. Languages are human-generated signals that are highly semantic and information-dense. When training a model to predict only a few missing words per sentence, this task appears to induce sophisticated language understanding. Images, on the contrary, are natural signals with heavy spatial redundancy-e.g., a missing patch can be recovered from neighboring patches with little high-level un- Figure 2. Example results on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction \u2020 (middle), and the ground-truth (right). The masking ratio is 80%, leaving only 39 out of 196 patches. More examples are in the appendix. \u2020 As no loss is computed on visible patches, the model output on visible patches is qualitatively worse. One can simply overlay the output with the visible patches to improve visual quality. We intentionally opt not to do this, so we can more comprehensively demonstrate the method's behavior. derstanding of parts, objects, and scenes. To overcome this difference and encourage learning useful features, we show that a simple strategy works well in computer vision: masking a very high portion of random patches. This strategy largely reduces redundancy and creates a challenging selfsupervisory task that requires holistic understanding beyond low-level image statistics. To get a qualitative sense of our reconstruction task, see  (iii) The autoencoder's decoder, which maps the latent representation back to the input, plays a different role between reconstructing text and images. In vision, the decoder reconstructs pixels, hence its output is of a lower semantic level than common recognition tasks. This is in contrast to language, where the decoder predicts missing words that contain rich semantic information. While in BERT the decoder can be trivial (an MLP) [14], we found that for images, the decoder design plays a key role in determining the semantic level of the learned latent representations.\n\nDriven by this analysis, we present a simple, effective, and scalable form of a masked autoencoder (MAE) for visual representation learning. Our MAE masks random patches from the input image and reconstructs the missing patches in the pixel space. It has an asymmetric encoderdecoder design. Our encoder operates only on the visible subset of patches (without mask tokens), and our decoder is lightweight and reconstructs the input from the latent representation along with mask tokens (Figure 1). Shifting the mask tokens to the small decoder in our asymmetric encoder-decoder results in a large reduction in computation. Under this design, a very high masking ratio (e.g., 75%) can achieve a win-win scenario: it optimizes accuracy while allowing the encoder to process only a small portion (e.g., 25%) of patches. This can reduce overall pre-training time by 3\u00d7 or more and likewise reduce memory consumption, enabling us to easily scale our MAE to large models.\n\nOur MAE learns very high-capacity models that generalize well. With MAE pre-training, we can train datahungry models like ViT-Large/-Huge [16] on ImageNet-1K with improved generalization performance. With a vanilla ViT-Huge model, we achieve 87.8% accuracy when finetuned on ImageNet-1K. This outperforms all previous results that use only ImageNet-1K data. We also evaluate transfer learning on object detection, instance segmentation, and semantic segmentation. In these tasks, our pre-training achieves better results than its supervised pre-training counterparts, and more importantly, we observe significant gains by scaling up models. These observations are aligned with those witnessed in self-supervised pre-training in NLP [14,47,48,4] and we hope that they will enable our field to explore a similar trajectory. original mask 75% mask 85% mask 95% Figure 4. Reconstructions of ImageNet validation images using an MAE pre-trained with a masking ratio of 75% but applied on inputs with higher masking ratios. The predictions differ plausibly from the original images, showing that the method can generalize.\n\n\nRelated Work\n\nMasked language modeling and its autoregressive counterparts, e.g., BERT [14] and GPT [47,48,4], are highly successful methods for pre-training in NLP. These methods hold out a portion of the input sequence and train models to predict the missing content. These methods have been shown to scale excellently [4] and a large abundance of evidence indicates that these pre-trained representations generalize well to various downstream tasks.\n\nAutoencoding is a classical method for learning representations. It has an encoder that maps an input to a latent representation and a decoder that reconstructs the input. For example, PCA and k-means are autoencoders [29]. Denoising autoencoders (DAE) [58] are a class of autoencoders that corrupt an input signal and learn to reconstruct the original, uncorrupted signal. A series of methods can be thought of as a generalized DAE under different corruptions, e.g., masking pixels [59,46,6] or removing color channels [70]. Our MAE is a form of denoising autoencoding, but different from the classical DAE in numerous ways.\n\nMasked image encoding methods learn representations from images corrupted by masking. The pioneering work of [59] presents masking as a noise type in DAE. Context Encoder [46] inpaints large missing regions using convolutional networks. Motivated by the success in NLP, related recent methods [6,16,2] are based on Transformers [57].\n\niGPT [6] operates on sequences of pixels and predicts unknown pixels. The ViT paper [16] studies masked patch prediction for self-supervised learning. Most recently, BEiT [2] proposes to predict discrete tokens [44,50].\n\nSelf-supervised learning approaches have seen significant interest in computer vision, often focusing on different pretext tasks for pre-training [15,61,42,70,45,17]. Recently, contrastive learning [3,22] has been popular, e.g., [62,43,23,7], which models image similarity and dissimilarity (or only similarity [21,8]) between two or more views. Contrastive and related methods strongly depend on data augmentation [7,21,8]. Autoencoding pursues a conceptually different direction, and it exhibits different behaviors as we will present.\n\n\nApproach\n\nOur masked autoencoder (MAE) is a simple autoencoding approach that reconstructs the original signal given its partial observation. Like all autoencoders, our approach has an encoder that maps the observed signal to a latent representation, and a decoder that reconstructs the original signal from the latent representation. Unlike classical autoencoders, we adopt an asymmetric design that allows the encoder to operate only on the partial, observed signal (without mask tokens) and a lightweight decoder that reconstructs the full signal from the latent representation and mask tokens. Figure 1 illustrates the idea, introduced next.\n\nMasking. Following ViT [16], we divide an image into regular non-overlapping patches. Then we sample a subset of patches and mask (i.e., remove) the remaining ones. Our sampling strategy is straightforward: we sample random patches without replacement, following a uniform distribution. We simply refer to this as \"random sampling\".\n\nRandom sampling with a high masking ratio (i.e., the ratio of removed patches) largely eliminates redundancy, thus creating a task that cannot be easily solved by extrapolation from visible neighboring patches (see . The uniform distribution prevents a potential center bias (i.e., more masked patches near the image center). Finally, the highly sparse input creates an opportunity for designing an efficient encoder, introduced next.\n\n\nMAE encoder.\n\nOur encoder is a ViT [16] but applied only on visible, unmasked patches. Just as in a standard ViT, our encoder embeds patches by a linear projection with added positional embeddings, and then processes the resulting set via a series of Transformer blocks. However, our encoder only operates on a small subset (e.g., 25%) of the full set. Masked patches are removed; no mask tokens are used. This allows us to train very large encoders with only a fraction of compute and memory. The full set is handled by a lightweight decoder, described next.\n\nMAE decoder. The input to the MAE decoder is the full set of tokens consisting of (i) encoded visible patches, and (ii) mask tokens. See Figure 1. Each mask token [14] is a shared, learned vector that indicates the presence of a miss-ing patch to be predicted. We add positional embeddings to all tokens in this full set; without this, mask tokens would have no information about their location in the image. The decoder has another series of Transformer blocks.\n\nThe MAE decoder is only used during pre-training to perform the image reconstruction task (only the encoder is used to produce image representations for recognition). Therefore, the decoder architecture can be flexibly designed in a manner that is independent of the encoder design. We experiment with very small decoders, narrower and shallower than the encoder. For example, our default decoder has <10% computation per token vs. the encoder. With this asymmetrical design, the full set of tokens are only processed by the lightweight decoder, which significantly reduces pre-training time.\n\nReconstruction target. Our MAE reconstructs the input by predicting the pixel values for each masked patch. Each element in the decoder's output is a vector of pixel values representing a patch. The last layer of the decoder is a linear projection whose number of output channels equals the number of pixel values in a patch. The decoder's output is reshaped to form a reconstructed image. Our loss function computes the mean squared error (MSE) between the reconstructed and original images in the pixel space. We compute the loss only on masked patches, similar to BERT [14]. 1 We also study a variant whose reconstruction target is the normalized pixel values of each masked patch. Specifically, we compute the mean and standard deviation of all pixels in a patch and use them to normalize this patch. Using normalized pixels as the reconstruction target improves representation quality in our experiments.\n\nSimple implementation. Our MAE pre-training can be implemented efficiently, and importantly, does not require any specialized sparse operations. First we generate a token for every input patch (by linear projection with an added positional embedding). Next we randomly shuffle the list of tokens and remove the last portion of the list, based on the masking ratio. This process produces a small subset of tokens for the encoder and is equivalent to sampling patches without replacement. After encoding, we append a list of mask tokens to the list of encoded patches, and unshuffle this full list (inverting the random shuffle operation) to align all tokens with their targets. The decoder is applied to this full list (with positional embeddings added). As noted, no sparse operations are needed. This simple implementation introduces negligible overhead as the shuffling and unshuffling operations are fast. 1 Computing the loss only on masked patches differs from traditional denoising autoencoders [58] that compute the loss on all pixels. This choice is purely result-driven: computing the loss on all pixels leads to a slight decrease in accuracy (e.g., \u223c0.5%).  Figure 5. Masking ratio. A high masking ratio (75%) works well for both fine-tuning (top) and linear probing (bottom). The y-axes are ImageNet-1K validation accuracy (%) in all plots in this paper.\n\n\nImageNet Experiments\n\nWe do self-supervised pre-training on the ImageNet-1K (IN1K) [13] training set. Then we do supervised training to evaluate the representations with (i) end-to-end fine-tuning or (ii) linear probing. We report top-1 validation accuracy of a single 224\u00d7224 crop. Details are in Appendix A.1.\n\nBaseline: ViT-Large. We use ViT-Large (ViT-L/16) [16] as the backbone in our ablation study. ViT-L is very big (an order of magnitude bigger than ResNet-50 [25]) and tends to overfit. The following is a comparison between ViT-L trained from scratch vs. fine-tuned from our baseline MAE: scratch, original [16] scratch, our impl. baseline MAE 76.5 82.5 84. 9 We note that it is nontrivial to train supervised ViT-L from scratch and a good recipe with strong regularization is needed (82.5%, see Appendix A.2). Even so, our MAE pretraining contributes a big improvement. Here fine-tuning is only for 50 epochs (vs. 200 from scratch), implying that the fine-tuning accuracy heavily depends on pre-training.\n\n\nMain Properties\n\nWe ablate our MAE using the default settings in Table 1  (see caption). Several intriguing properties are observed.\n\nMasking ratio. Figure 5 shows the influence of the masking ratio. The optimal ratios are surprisingly high. The ratio of 75% is good for both linear probing and fine-tuning. This behavior is in contrast with BERT [14], whose typical masking ratio is 15%. Our masking ratios are also much higher than those in related works [6,16,2] in computer vision (20% to 50%).\n\nThe model infers missing patches to produce different, yet plausible, outputs ( Figure 4). It makes sense of the gestalt of objects and scenes, which cannot be simply completed by extending lines or textures. We hypothesize that this reasoning-like behavior is linked to the learning of useful representations. Figure 5 also shows that linear probing and fine-tuning results follow different trends. For linear probing, the ac-  Figure 6 for visualizations. Table 1. MAE ablation experiments with ViT-L/16 on ImageNet-1K. We report fine-tuning (ft) and linear probing (lin) accuracy (%). If not specified, the default is: the decoder has depth 8 and width 512, the reconstruction target is unnormalized pixels, the data augmentation is random resized cropping, the masking ratio is 75%, and the pre-training length is 800 epochs. Default settings are marked in gray .\n\ncuracy increases steadily with the masking ratio until the sweet point: the accuracy gap is up to \u223c20% (54.6% vs. 73.5%). For fine-tuning, the results are less sensitive to the ratios, and a wide range of masking ratios (40-80%) work well. All fine-tuning results in Figure 5 are better than training from scratch (82.5%).\n\nDecoder design. Our MAE decoder can be flexibly designed, as studied in Table 1a and 1b. Table 1a varies the decoder depth (number of Transformer blocks). A sufficiently deep decoder is important for linear probing. This can be explained by the gap between a pixel reconstruction task and a recognition task: the last several layers in an autoencoder are more specialized for reconstruction, but are less relevant for recognition. A reasonably deep decoder can account for the reconstruction specialization, leaving the latent representations at a more abstract level. This design can yield up to 8% improvement in linear probing (Table 1a, 'lin'). However, if fine-tuning is used, the last layers of the encoder can be tuned to adapt to the recognition task. The decoder depth is less influential for improving fine-tuning (Table 1a, 'ft').\n\nInterestingly, our MAE with a single-block decoder can perform strongly with fine-tuning (84.8%). Note that a single Transformer block is the minimal requirement to propagate information from visible tokens to mask tokens. Such a small decoder can further speed up training.\n\nIn Table 1b we study the decoder width (number of channels). We use 512-d by default, which performs well under fine-tuning and linear probing. A narrower decoder also works well with fine-tuning.\n\nOverall, our default MAE decoder is lightweight. It has 8 blocks and a width of 512-d ( gray in Table 1). It only has 9% FLOPs per token vs. ViT-L (24 blocks, 1024-d).\n\nAs such, while the decoder processes all tokens, it is still a small fraction of the overall compute. If the encoder uses mask tokens, it performs worse: its accuracy drops by 14% in linear probing. In this case, there is a gap between pre-training and deploying: this encoder has a large portion of mask tokens in its input in pretraining, which does not exist in uncorrupted images. This gap may degrade accuracy in deployment. By removing the mask token from the encoder, we constrain the encoder to always see real patches and thus improve accuracy.\n\nMoreover, by skipping the mask token in the encoder, we greatly reduce training computation. In Table 1c, we reduce the overall training FLOPs by 3.3\u00d7. This leads to a 2.8\u00d7 wall-clock speedup in our implementation (see Table 2). The wall-clock speedup is even bigger (3.5-4.1\u00d7), for a smaller decoder (1-block), a larger encoder (ViT-H), or both. Note that the speedup can be >4\u00d7 for a masking ratio of 75%, partially because the self-attention complexity is quadratic. In addition, memory is greatly reduced, which can enable training even larger models or speeding up more by large-batch training. The time and memory efficiency makes our MAE favorable for training very large models. block 50% grid 75% random 75% Figure 6. Mask sampling strategies determine the pretext task difficulty, influencing reconstruction quality and representations (Table 1f). Here each output is from an MAE trained with the specified masking strategy. Left: random sampling (our default). Middle: block-wise sampling [2] that removes large random blocks. Right: grid-wise sampling that keeps one of every four patches. Images are from the validation set.\n\nReconstruction target. We compare different reconstruction targets in Table 1d. Our results thus far are based on pixels without (per-patch) normalization. Using pixels with normalization improves accuracy. This per-patch normalization enhances the contrast locally. In another variant, we perform PCA in the patch space and use the largest PCA coefficients (96 here) as the target. Doing so degrades accuracy. Both experiments suggest that the high-frequency components are useful in our method.\n\nWe also compare an MAE variant that predicts tokens, the target used in BEiT [2]. Specifically for this variant, we use the DALLE pre-trained dVAE [50] as the tokenizer, following [2]. Here the MAE decoder predicts the token indices using cross-entropy loss. This tokenization improves fine-tuning accuracy by 0.4% vs. unnormalized pixels, but has no advantage vs. normalized pixels. It also reduces linear probing accuracy. In \u00a75 we further show that tokenization is not necessary in transfer learning.\n\nOur pixel-based MAE is much simpler than tokenization. The dVAE tokenizer requires one more pre-training stage, which may depend on extra data (250M images [50]). The dVAE encoder is a large convolutional network (40% FLOPs of ViT-L) and adds nontrivial overhead. Using pixels does not suffer from these problems. Table 1e studies the influence of data augmentation on our MAE pre-training.\n\n\nData augmentation.\n\nOur MAE works well using cropping-only augmentation, either fixed-size or random-size (both having random horizontal flipping). Adding color jittering degrades the results and so we do not use it in other experiments.\n\nSurprisingly, our MAE behaves decently even if using no data augmentation (only center-crop, no flipping). This property is dramatically different from contrastive learning and related methods [62,23,7,21], which heavily rely on data augmentation. It was observed [21] that using cropping-only augmentation reduces the accuracy by 13%  Table 1. and 28% respectively for BYOL [21] and SimCLR [7]. In addition, there is no evidence that contrastive learning can work without augmentation: the two views of an image are the same and can easily satisfy a trivial solution.\n\nIn MAE, the role of data augmentation is mainly performed by random masking (ablated next). The masks are different for each iteration and so they generate new training samples regardless of data augmentation. The pretext task is made difficult by masking and requires less augmentation to regularize training.\n\nMask sampling strategy. In Table 1f we compare different mask sampling strategies, illustrated in Figure 6.\n\nThe block-wise masking strategy, proposed in [2], tends to remove large blocks (Figure 6 middle). Our MAE with block-wise masking works reasonably well at a ratio of 50%, but degrades at a ratio of 75%. This task is harder than that of random sampling, as a higher training loss is observed. The reconstruction is also blurrier.\n\nWe also study grid-wise sampling, which regularly keeps one of every four patches (Figure 6 right). This is an easier task and has lower training loss. The reconstruction is sharper. However, the representation quality is lower.\n\nSimple random sampling works the best for our MAE. It allows for a higher masking ratio, which provides a greater speedup benefit while also enjoying good accuracy.\n\nTraining schedule. Our ablations thus far are based on 800-epoch pre-training. Figure 7 shows the influence of the training schedule length. The accuracy improves steadily with longer training. Indeed, we have not observed saturation of linear probing accuracy even at 1600 epochs. This behavior is unlike contrastive learning methods, e.g., MoCo v3 [9] saturates at 300 epochs for ViT-L. Note that the MAE encoder only sees 25% of patches per epoch, while in contrastive learning the encoder sees 200% (twocrop) or even more (multi-crop) patches per epoch.  Table 3. Comparisons with previous results on ImageNet-1K. The pre-training data is the ImageNet-1K training set (except the tokenizer in BEiT was pre-trained on 250M DALLE data [50]). All self-supervised methods are evaluated by end-to-end fine-tuning. The ViT models are B/16, L/16, H/14 [16]. The best for each column is underlined. All results are on an image size of 224, except for ViT-H with an extra result on 448. Here our MAE reconstructs normalized pixels and is pre-trained for 1600 epochs. [16] [16] params (M) Figure 8. MAE pre-training vs. supervised pre-training, evaluated by fine-tuning in ImageNet-1K (224 size). We compare with the original ViT results [16] trained in IN1K or JFT300M.\n\n\nComparisons with Previous Results\n\nComparisons with self-supervised methods. In Table 3 we compare the fine-tuning results of self-supervised ViT models. For ViT-B, all methods perform closely. For ViT-L, the gaps among methods are bigger, suggesting that a challenge for bigger models is to reduce overfitting. Our MAE can scale up easily and has shown steady improvement from bigger models. We obtain 86.9% accuracy using ViT-H (224 size). By fine-tuning with a 448 size, we achieve 87.8% accuracy, using only IN1K data. The previous best accuracy, among all methods using only IN1K data, is 87.1% (512 size) [67], based on advanced networks. We improve over the state-of-the-art by a nontrivial margin in the highly competitive benchmark of IN1K (no external data). Our result is based on vanilla ViT, and we expect advanced networks will perform better.\n\nComparing with BEiT [2], our MAE is more accurate while being simpler and faster. Our method reconstructs pixels, in contrast to BEiT that predicts tokens: BEiT reported a 1.8% degradation [2] when reconstructing pixels with ViT-B. 2 We do not need dVAE pre-training. Moreover, our MAE is considerably faster (3.5\u00d7 per epoch) than BEiT, for the reason as studied in Table 1c. 2 We observed the degradation also in BEiT with ViT-L: it produces 85.2% (tokens) and 83.5% (pixels), reproduced from the official code.  # blocks fine-tuned Figure 9. Partial fine-tuning results of ViT-L w.r.t. the number of fine-tuned Transformer blocks under the default settings from Table 1. Tuning 0 blocks is linear probing; 24 is full fine-tuning. Our MAE representations are less linearly separable, but are consistently better than MoCo v3 if one or more blocks are tuned.\n\nThe MAE models in Table 3 are pre-trained for 1600 epochs for better accuracy (Figure 7). Even so, our total pre-training time is less than the other methods when trained on the same hardware. For example, training ViT-L on 128 TPU-v3 cores, our MAE's training time is 31 hours for 1600 epochs and MoCo v3's is 36 hours for 300 epochs [9].\n\nComparisons with supervised pre-training. In the original ViT paper [16], ViT-L degrades when trained in IN1K. Our implementation of supervised training (see A.2) works better, but accuracy saturates. See Figure 8.\n\nOur MAE pre-training, using only IN1K, can generalize better: the gain over training from scratch is bigger for higher-capacity models. It follows a trend similar to the JFT-300M supervised pre-training in [16]. This comparison shows that our MAE can help scale up model sizes. Table 1 shows that linear probing and fine-tuning results are largely uncorrelated. Linear probing has been a popular protocol in the past few years; however, it misses the opportunity of pursuing strong but non-linear features-which is indeed a strength of deep learning. As a middle ground, we study a partial fine-tuning protocol: fine-tune the last several layers while freezing the others. This protocol was also used in early works, e.g., [65,70,42]. Figure 9 shows the results. Notably, fine-tuning only one Transformer block boosts the accuracy significantly from 73.5% to 81.0%. Moreover, if we fine-tune only \"half\" of the last block (i.e., its MLP sub-block), we can get 79.1%, much better than linear probing. This variant is essentially fine-tuning an MLP head. Fine-tuning a few blocks (e.g., 4 or 6) can achieve accuracy close to full fine-tuning.\n\n\nPartial Fine-tuning\n\nIn Figure 9 we also compare with MoCo v3 [9], a contrastive method with ViT-L results available. MoCo v3 has higher linear probing accuracy; however, all of its partial fine-tuning results are worse than MAE. The gap is 2.6% when tuning 4 blocks. While the MAE representations are less linearly separable, they are stronger non-linear features and perform well when a non-linear head is tuned.  Table 4. COCO object detection and segmentation using a ViT Mask R-CNN baseline. All entries are based on our implementation. Self-supervised entries use IN1K data without labels. Mask AP follows a similar trend as box AP.\n\nThese observations suggest that linear separability is not the sole metric for evaluating representation quality. It has also been observed (e.g., [8]) that linear probing is not well correlated with transfer learning performance, e.g., for object detection. To our knowledge, linear evaluation is not often used in NLP for benchmarking pre-training.\n\n\nTransfer Learning Experiments\n\nWe evaluate transfer learning in downstream tasks using the pre-trained models in Table 3.\n\nObject detection and segmentation. We fine-tune Mask R-CNN [24] end-to-end on COCO [37]. The ViT backbone is adapted for use with FPN [36] (see A.3). We apply this approach for all entries in Table 4. We report box AP for object detection and mask AP for instance segmentation.\n\nCompared to supervised pre-training, our MAE performs better under all configurations (Table 4) The pixel-based MAE is better than or on par with the token-based BEiT, while MAE is much simpler and faster. Both MAE and BEiT are better than MoCo v3 and MoCo v3 is on par with supervised pre-training.\n\nSemantic segmentation. We experiment on ADE20K [72] using UperNet [63] (see A.4). Table 5 shows that our pretraining significantly improves results over supervised pretraining, e.g., by 3.7 points for ViT-L. Our pixel-based MAE also outperforms the token-based BEiT. These observations are consistent with those in COCO. Table 6 studies transfer learning on the iNaturalists [56] and Places [71] tasks (see A.5). On iNat, our method shows strong scaling behavior: accuracy improves considerably with bigger models. Our results surpass the previous best results by large margins. On Places, our MAE outperforms the previous best results [19,40], which were obtained via pre-training on billions of images.\n\n\nClassification tasks.\n\nPixels vs. tokens. Table 7 compares pixels vs. tokens as the MAE reconstruction target. While using dVAE tokens is better than using unnormalized pixels, it is statistically similar to using normalized pixels across all cases we tested. It again shows that tokenization is not necessary for our MAE.   Table 6. Transfer learning accuracy on classification datasets, using MAE pre-trained on IN1K and then fine-tuned. We provide system-level comparisons with the previous best results. \u2020 : pre-trained on 1 billion images. \u2021 : pre-trained on 3.5 billion images.  Table 7. Pixels vs. tokens as the MAE reconstruction target. is the difference between using dVAE tokens and using normalized pixels. The difference is statistically insignificant.\n\n\nDiscussion and Conclusion\n\nSimple algorithms that scale well are the core of deep learning. In NLP, simple self-supervised learning methods (e.g., [47,14,48,4]) enable benefits from exponentially scaling models. In computer vision, practical pre-training paradigms are dominantly supervised (e.g. [33,51,25,16]) despite progress in self-supervised learning. In this study, we observe on ImageNet and in transfer learning that an autoencoder-a simple self-supervised method similar to techniques in NLP-provides scalable benefits. Selfsupervised learning in vision may now be embarking on a similar trajectory as in NLP.\n\nOn the other hand, we note that images and languages are signals of a different nature and this difference must be addressed carefully. Images are merely recorded light without a semantic decomposition into the visual analogue of words. Instead of attempting to remove objects, we remove random patches that most likely do not form a semantic segment. Likewise, our MAE reconstructs pixels, which are not semantic entities. Nevertheless, we observe (e.g., Figure 4) that our MAE infers complex, holistic reconstructions, suggesting it has learned numerous visual concepts, i.e., semantics. We hypothesize that this behavior occurs by way of a rich hidden representation inside the MAE. We hope this perspective will inspire future work.\n\nBroader impacts. The proposed method predicts content based on learned statistics of the training dataset and as such will reflect biases in those data, including ones with negative societal impacts. The model may generate inexistent content. These issues warrant further research and consideration when building upon this work to generate images.\n\n\nA. Implementation Details\n\n\nA.1. ImageNet Experiments\n\nViT architecture. We follow the standard ViT architecture [16]. It has a stack of Transformer blocks [57], and each block consists of a multi-head self-attention block and an MLP block, both having LayerNorm (LN) [1]. The encoder ends with LN. As the MAE encoder and decoder have different width, we adopt a linear projection layer after the encoder to match it. Our MAE adds positional embeddings [57] (the sine-cosine version) to both the encoder and decoder inputs. Our MAE does not use relative position or layer scaling (which are used in the code of [2]).\n\nWe extract features from the encoder output for finetuning and linear probing. As ViT has a class token [16], to adapt to this design, in our MAE pre-training we append an auxiliary dummy token to the encoder input. This token will be treated as the class token for training the classifier in linear probing and fine-tuning. Our MAE works similarly well without this token (with average pooling).\n\nPre-training. The default setting is in Table 8. We do not use color jittering, drop path, or gradient clip. We use xavier uniform [18] to initialize all Transformer blocks, following ViT's official code [16]. We use the linear lr scaling rule [20]: lr = base lr\u00d7batchsize / 256.\n\nEnd-to-end fine-tuning. Our fine-tuning follows common practice of supervised ViT training. The default setting is in Table 9. We use layer-wise lr decay [10] following [2].\n\nLinear probing. Our linear classifier training follows [9]. See Table 10. We observe that linear probing requires a very different recipe than end-to-end fine-tuning. In particular, regularization is in general harmful for linear probing. Following [9], we disable many common regularization strategies: we do not use mixup [69], cutmix [68], drop path [30], or color jittering, and we set weight decay as zero.\n\nIt is a common practice to normalize the classifier input when training a classical linear classifier (e.g., SVM [11]). Similarly, it is beneficial to normalize the pre-trained features when training the linear probing classifier. Following [15], we adopt an extra BatchNorm layer [31] without affine transformation (affine=False). This layer is applied on the pre-trained features produced by the encoder, and is before the linear classifier. We note that the layer does not break the linear property, and it can be absorbed into the linear classifier after training: it is essentially a reparameterized linear classifier. 3 Introducing this layer helps calibrate the feature magnitudes across different variants in our ablations, so that they can use the same setting without further lr search.  [52] 0.1 mixup [69] 0.8 cutmix [68] 1.0 drop path [30] 0.1 (B/L) 0.2 (H)  Table 10. Linear probing setting. We use LARS with a large batch for faster training; SGD works similarly with a 4096 batch.\n\nPartial fine-tuning. Our MAE partial fine-tuning ( \u00a74.3) follows the setting in Table 9, except that we adjust the number of fine-tuning epochs. We observe that tuning fewer blocks requires a longer schedule. We set the numbers of fine-tuning epochs as {50, 100, 200} and use the optimal one for each number of blocks tuned.\n\n\nA.2. Supervised Training ViT-L/H from Scratch\n\nWe find that it is nontrivial to train supervised ViT-L/H from scratch on ImageNet-1K. The training is unstable. While there have been strong baselines with publicly available implementations [53] for smaller models, the recipes for the larger ViT-L/H are unexplored. Directly applying the previous recipes to these larger models does not work. A NaN loss is frequently observed during training.\n\nWe provide our recipe in Table 11. We use a wd of 0.3, a large batch size of 4096, and a long warmup, following the original ViT [16]. We use \u03b2 2 =0.95 following [6]. We use the regularizations listed in Table 11 and disable others, following [64]. All these choices are for improving training stability. Our recipe can finish training with no NaN loss.  [52] 0.1 mixup [69] 0.8 cutmix [68] 1.0 drop path [30] 0.1 (B), 0.2 (L/H) exp. moving average (EMA) 0.9999 \n\n\nA.3. Object Detection and Segmentation in COCO\n\nWe adapt the vanilla ViT for the use of an FPN backbone [36] in Mask R-CNN [24]. ViT has a stack of Transformer blocks that all produce feature maps at a single scale (e.g., stride 16). We equally divide this stack into 4 subsets and apply convolutions to upsample or downsample the intermediate feature maps for producing different scales (stride 4, 8, 16, or 32, the same as a standard ResNet [25]). FPN is built on these multi-scale maps.\n\nFor fair comparisons among different methods, we search for hyper-parameters for each entry in Table 4 (including all competitors). The hyper-parameters we search for are the learning rate, weight decay, drop path rate, and fine-tuning epochs. We will release code along with the specific configurations. For full model and training details, plus additional experiments, see [35].\n\n\nA.4. Semantic Segmentation in ADE20K\n\nWe use UperNet [63] following the semantic segmentation code of [2]. We fine-tune end-to-end for 100 epochs with a batch size of 16. We search for the optimal lr for each entry in Table 5 (including all competitors).\n\nThe semantic segmentation code of [2] uses relative position bias [49]. Our MAE pre-training does not use it. For fair comparison, we turn on relative position bias only during transfer learning, initialized as zero. We note that our BEiT reproduction uses relative position bias in both pretraining and fine-tuning, following their code. Table 13. Robustness evaluation on ImageNet variants (top-1 accuracy, except for IN-C [27] which evaluates mean corruption error). We test the same MAE models (Table 3) on different Im-ageNet validation sets, without any specialized fine-tuning. We provide system-level comparisons with the previous best results.\n\n\nB. Comparison on Linear Probing Results\n\nIn \u00a74.3 we have shown that linear probing accuracy and fine-tuning accuracy are largely uncorrelated and they have different focuses about linear separability. We notice that existing masked image encoding methods are generally less competitive in linear probing (e.g., than contrastive learning). For completeness, in Table 12 we compare on linear probing accuracy with masking-based methods.\n\nOur MAE with ViT-L has 75.8% linear probing accuracy. This is substantially better than previous maskingbased methods. On the other hand, it still lags behind contrastive methods under this protocol: e.g., MoCo v3 [9] has 77.6% linear probing accuracy for the ViT-L ( Figure 9).\n\n\nC. Robustness Evaluation on ImageNet\n\nIn Table 13 we evaluate the robustness of our models on different variants of ImageNet validation sets. We use the same models fine-tuned on original ImageNet (Table 3) and only run inference on the different validation sets, without any specialized fine-tuning. Table 13 shows that our method has strong scaling behavior: increasing the model sizes has significant gains. Increasing the image size helps in all sets but IN-C. Our results outperform the previous best results (of specialized systems) by large margins.\n\nIn contrast, supervised training performs much worse (Table 13 bottom; models described in A.2). For example, with ViT-H, our MAE pre-training is 35% better on IN-A (68.2% vs 33.1%) than the supervised counterpart. Figure 10. Uncurated random samples on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction (middle), and the ground-truth (right). The masking ratio is 75%. Figure 11. Uncurated random samples on COCO validation images, using an MAE trained on ImageNet. For each triplet, we show the masked image (left), our MAE reconstruction (middle), and the ground-truth (right). The masking ratio is 75%.\n\nFigure 1 .\n1Our MAE architecture.\n\nFigure 3 .\n3Example results on COCO validation images, using an MAE trained on ImageNet (the same model weights as inFigure 2). Observe the reconstructions on the two right-most examples, which, although different from the ground truth, are semantically plausible.\n\nFigure 7 .\n7Training schedules. A longer training schedule gives a noticeable improvement. Here each point is a full training schedule. The model is ViT-L with the default setting in\n\n\n. With the smaller ViT-B, our MAE is 2.4 points higher than supervised pretraining (50.3 vs. 47.9, AP box ). More significantly, with the larger ViT-L, our MAE pre-training outperforms supervised pre-training by 4.0 points (53.3 vs. 49.3).\n\nTable 2 .\n2Wall-clock time of our MAE training (800 epochs), benchmarked in 128 TPU-v3 cores with TensorFlow. The speedup is relative to the entry whose encoder has mask tokens (gray). The decoder width is 512, and the mask ratio is 75%. \u2020 : This entry is estimated by training ten epochs. token. An important design of our MAE is to skip the mask token [M] in the encoder and apply it later in the lightweight decoder.Table 1cstudies this design.encoder \ndec. depth \nft acc \nhours speedup \nViT-L, w/ [M] \n8 \n84.2 \n42.4 \n-\nViT-L \n8 \n84.9 \n15.4 \n2.8\u00d7 \nViT-L \n1 \n84.8 \n11.6 \n3.7\u00d7 \nViT-H, w/ [M] \n8 \n-\n119.6  \u2020 \n-\nViT-H \n8 \n85.8 \n34.5 \n3.5\u00d7 \nViT-H \n1 \n85.9 \n29.3 \n4.1\u00d7 \n\nMask \n\nTable 5 .\n5ADE20K semantic segmentation (mIoU) using Uper-\nNet. BEiT results are reproduced using the official code. Other \nentries are based on our implementation. Self-supervised entries \nuse IN1K data without labels. \n\ndataset \nViT-B \nViT-L \nViT-H ViT-H 448 prev best \niNat 2017 \n70.5 \n75.7 \n79.3 \n83.4 \n75.4 [55] \niNat 2018 \n75.4 \n80.1 \n83.0 \n86.8 \n81.2 [54] \niNat 2019 \n80.5 \n83.4 \n85.7 \n88.3 \n84.1 [54] \nPlaces205 \n63.9 \n65.8 \n65.9 \n66.8 \n66.0 [19]  \u2020 \nPlaces365 \n57.9 \n59.4 \n59.8 \n60.3 \n58.0 [40]  \u2021 \n\n\n\nTable 8 .\n8Pre-training setting.config \nvalue \noptimizer \nAdamW [39] \nbase learning rate \n1.5e-4 \nweight decay \n0.05 \noptimizer momentum \n\u03b21, \u03b22=0.9, 0.95 [6] \nbatch size \n4096 \nlearning rate schedule \ncosine decay [38] \nwarmup epochs [20] \n40 \naugmentation \nRandomResizedCrop \n\nconfig \nvalue \noptimizer \nAdamW \nbase learning rate \n1e-3 \nweight decay \n0.05 \noptimizer momentum \n\u03b21, \u03b22=0.9, 0.999 \nlayer-wise lr decay [10, 2] \n0.75 \nbatch size \n1024 \nlearning rate schedule \ncosine decay \nwarmup epochs \n5 \ntraining epochs \n100 (B), 50 (L/H) \naugmentation \nRandAug (9, 0.5) [12] \nlabel smoothing \n\nTable 9 .\n9End-to-end fine-tuning setting.config \nvalue \noptimizer \nLARS [66] \nbase learning rate \n0.1 \nweight decay \n0 \noptimizer momentum \n0.9 \nbatch size \n16384 \nlearning rate schedule \ncosine decay \nwarmup epochs \n10 \ntraining epochs \n90 \naugmentation \nRandomResizedCrop \n\n\n\nTable 11 .\n11Supervised training ViT from scratch. accuracy is 82.6% for ViT-L (81.5% w/o EMA), and 83.1% for ViT-H (80.9% w/o EMA). Both ViT-L and ViT-H show an overfitting trend if not using EMA.As a by-product, our recipe for ViT-B has 82.3% accuracy (82.1% w/o EMA), vs. 81.8% in[53].The \nAlternatively, we can pre-compute the mean and std of the features and use the normalized features to train linear classifiers.\nA.5. Additional Classification TasksWe follow the setting inTable 9for iNaturalist and Places fine-tuning(Table 6). We adjust the lr and finetuning epochs for each individual dataset. method model params acc iGPT[6]iGPT-L 1362 M 69.0 iGPT[6]iGPT-XL 6801 M 72.0 BEiT[2]ViT-L 304 M 52.\nJimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv:1607.06450, 2016.\n\nBEiT: BERT pre-training of image transformers. Hangbo Bao, Li Dong, Furu Wei, arXiv:2106.08254Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers. arXiv:2106.08254, 2021. Accessed in June 2021.\n\nSelf-organizing neural network that discovers surfaces in random-dot stereograms. Nature. Suzanna Becker, Geoffrey E Hinton, Suzanna Becker and Geoffrey E Hinton. Self-organizing neural network that discovers surfaces in random-dot stereograms. Na- ture, 1992.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordNeurIPSTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben- jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language mod- els are few-shot learners. In NeurIPS, 2020.\n\nEmerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin, ICCV. 2021Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\n\nGenerative pretraining from pixels. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, ICML. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pix- els. In ICML, 2020.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, ICML. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual rep- resentations. In ICML, 2020.\n\nExploring simple Siamese representation learning. Xinlei Chen, Kaiming He, CVPR. 2021Xinlei Chen and Kaiming He. Exploring simple Siamese represen- tation learning. In CVPR, 2021.\n\nAn empirical study of training self-supervised Vision Transformers. Xinlei Chen, Saining Xie, Kaiming He, ICCV. 2021Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised Vision Transformers. In ICCV, 2021.\n\nELECTRA: Pre-training text encoders as discriminators rather than generators. Kevin Clark, Minh-Thang Luong, V Quoc, Christopher D Le, Manning, ICLR. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020.\n\nSupport-vector networks. Machine learning. Corinna Cortes, Vladimir Vapnik, Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 1995.\n\nRandaugment: Practical automated data augmentation with a reduced search space. Barret Ekin D Cubuk, Jonathon Zoph, Quoc V Shlens, Le, CVPR Workshops. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Ran- daugment: Practical automated data augmentation with a reduced search space. In CVPR Workshops, 2020.\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.\n\nUnsupervised visual representation learning by context prediction. Carl Doersch, Abhinav Gupta, Alexei A Efros, ICCV. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, ICLR. 2021Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De- hghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nUnsupervised representation learning by predicting image rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, ICLR. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsuper- vised representation learning by predicting image rotations. In ICLR, 2018.\n\nUnderstanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, AISTATS. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.\n\nSelf-supervised pretraining of visual features in the wild. Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski, arXiv:2103.01988Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Is- han Misra, Armand Joulin, and Piotr Bojanowski. Self-supervised pretraining of visual features in the wild. arXiv:2103.01988, 2021.\n\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017.\n\nBootstrap your own latent -a new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, Michal Valko, NeurIPS. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Boot- strap your own latent -a new approach to self-supervised learning. In NeurIPS, 2020.\n\nDimensionality reduction by learning an invariant mapping. Raia Hadsell, Sumit Chopra, Yann Lecun, CVPR. Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006.\n\nSaining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, CVPR. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Gir- shick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\n\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, ICCV. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. In ICCV, 2017.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nThe many faces of robustness: A critical analysis of out-of-distribution generalization. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, ICCV. 2021Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.\n\nBenchmarking neural network robustness to common corruptions and perturbations. Dan Hendrycks, Thomas Dietterich, ICLR. Dan Hendrycks and Thomas Dietterich. Benchmarking neural net- work robustness to common corruptions and perturbations. In ICLR, 2019.\n\nNatural adversarial examples. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song, CVPR. 2021Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021.\n\nAutoencoders, minimum description length, and helmholtz free energy. E Geoffrey, Richard S Hinton, Zemel, NeurIPS. Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length, and helmholtz free energy. In NeurIPS, 1994.\n\nDeep networks with stochastic depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, ECCV. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein- berger. Deep networks with stochastic depth. In ECCV, 2016.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. Sergey Ioffe and Christian Szegedy. Batch normalization: Accel- erating deep network training by reducing internal covariate shift. In ICML, 2015.\n\nQuality-agnostic image recognition via invertible decoder. Insoo Kim, Seungju Han, Ji-Won Baek, Seong-Jin Park, Jae-Joon Han, Jinwoo Shin, CVPR. 2021Insoo Kim, Seungju Han, Ji-won Baek, Seong-Jin Park, Jae-Joon Han, and Jinwoo Shin. Quality-agnostic image recognition via in- vertible decoder. In CVPR, 2021.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoff Hinton, NeurIPS. Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet clas- sification with deep convolutional neural networks. In NeurIPS, 2012.\n\n. Yann Lecun, Bernhard Boser, S John, Donnie Denker, Richard E Henderson, Wayne Howard, Lawrence D Hubbard, Jackel, Backpropagation applied to handwritten zip code recognition. Neural computationYann LeCun, Bernhard Boser, John S Denker, Donnie Hender- son, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neu- ral computation, 1989.\n\nBenchmarking detection transfer learning with vision transformers. Yanghao Li, Saining Xie, Xinlei Chen, Piotr Doll\u00e1r, Kaiming He, Ross Girshick, 2021In preparationYanghao Li, Saining Xie, Xinlei Chen, Piotr Doll\u00e1r, Kaiming He, and Ross Girshick. Benchmarking detection transfer learning with vision transformers. In preparation, 2021.\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for ob- ject detection. In CVPR, 2017.\n\nMicrosoft COCO: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Mi- crosoft COCO: Common objects in context. In ECCV, 2014.\n\nSGDR: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient de- scent with warm restarts. In ICLR, 2017.\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu- larization. In ICLR, 2019.\n\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, ECCV. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pre- training. In ECCV, 2018.\n\nXiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, Hui Xue, arXiv:2105.07926Towards robust vision transformer. Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision trans- former. arXiv:2105.07926, 2021.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. Mehdi Noroozi, Paolo Favaro, ECCV. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016.\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representa- tion learning with contrastive predictive coding. arXiv:1807.03748, 2018.\n\nNeural discrete representation learning. Aaron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, NeurIPS. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neu- ral discrete representation learning. In NeurIPS, 2017.\n\nLearning features by watching objects move. Deepak Pathak, Ross Girshick, Piotr Doll\u00e1r, Trevor Darrell, Bharath Hariharan, CVPR. Deepak Pathak, Ross Girshick, Piotr Doll\u00e1r, Trevor Darrell, and Bharath Hariharan. Learning features by watching objects move. In CVPR, 2017.\n\nContext encoders: Feature learning by inpainting. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros, CVPR. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpaint- ing. In CVPR, 2016.\n\nTim Salimans, and Ilya Sutskever. Improving language understanding by generative pretraining. Alec Radford, Karthik Narasimhan, Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre- training. 2018.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, JMLRColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, ICML. 2021Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, ICLR. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna, CVPR. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architec- ture for computer vision. In CVPR, 2016.\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, ICML. 2021Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.\n\nGrafit: Learning fine-grained image representations with coarse labels. Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, Herv\u00e9 J\u00e9gou, ICCV. 2021Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Grafit: Learning fine-grained image repre- sentations with coarse labels. In ICCV, 2021.\n\nHugo Touvron, Andrea Vedaldi, arXiv:1906.06423Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy. Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy. arXiv:1906.06423, 2019.\n\nHartwig Adam, Pietro Perona, and Serge Belongie. The iNaturalist species classification and detection dataset. Oisin Mac Grant Van Horn, Yang Aodha, Yin Song, Chen Cui, Alex Sun, Shepard, CVPR. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Be- longie. The iNaturalist species classification and detection dataset. In CVPR, 2018.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NeurIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nExtracting and composing robust features with denoising autoencoders. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol, ICML. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre- Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008.\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, L\u00e9on Bottou, JMLRPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L\u00e9on Bottou. Stacked denoising au- toencoders: Learning useful representations in a deep network with a local denoising criterion. JMLR, 2010.\n\nLearning robust global representations by penalizing local predictive power. Haohan Wang, Songwei Ge, Zachary Lipton, Eric P Xing, NeurIPS. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predic- tive power. In NeurIPS, 2019.\n\nUnsupervised learning of visual representations using videos. Xiaolong Wang, Abhinav Gupta, ICCV. Xiaolong Wang and Abhinav Gupta. Unsupervised learning of vi- sual representations using videos. In ICCV, 2015.\n\nUnsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, Stella Yu, Dahua Lin, CVPR. Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsuper- vised feature learning via non-parametric instance discrimination. In CVPR, 2018.\n\nUnified perceptual parsing for scene understanding. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun, In ECCV. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018.\n\nEarly convolutions help transformers see better. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, Ross Girshick, NeurIPS. 2021Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, and Ross Girshick. Early convolutions help transformers see better. In NeurIPS, 2021.\n\nHow transferable are features in deep neural networks? In NeurIPS. Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson, Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014.\n\nLarge batch training of convolutional networks. Yang You, Igor Gitman, Boris Ginsburg, arXiv:1708.03888Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv:1708.03888, 2017.\n\nLi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, Shuicheng Yan, arXiv:2106.13112Vision outlooker for visual recognition. Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. VOLO: Vision outlooker for visual recognition. arXiv:2106.13112, 2021.\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, ICCV. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.\n\nmixup: Beyond empirical risk minimization. Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, ICLR. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018.\n\nColorful image colorization. Richard Zhang, Phillip Isola, Alexei A Efros, ECCV. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n\nLearning deep features for scene recognition using Places database. Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, Aude Oliva, NeurIPS. Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using Places database. In NeurIPS, 2014.\n\nSemantic understanding of scenes through the ADE20K dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba, IJCVBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ADE20K dataset. IJCV, 2019.\n", "annotations": {"author": "[{\"end\":63,\"start\":52},{\"end\":76,\"start\":64},{\"end\":89,\"start\":77},{\"end\":101,\"start\":90},{\"end\":115,\"start\":102},{\"end\":130,\"start\":116}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":60},{\"end\":75,\"start\":71},{\"end\":88,\"start\":85},{\"end\":100,\"start\":98},{\"end\":114,\"start\":108},{\"end\":129,\"start\":121}]", "author_first_name": "[{\"end\":59,\"start\":52},{\"end\":70,\"start\":64},{\"end\":84,\"start\":77},{\"end\":97,\"start\":90},{\"end\":107,\"start\":102},{\"end\":120,\"start\":116}]", "author_affiliation": null, "title": "[{\"end\":49,\"start\":1},{\"end\":179,\"start\":131}]", "venue": null, "abstract": "[{\"end\":1367,\"start\":254}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1493,\"start\":1489},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1496,\"start\":1493},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1499,\"start\":1496},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1594,\"start\":1590},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1686,\"start\":1682},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1881,\"start\":1877},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1884,\"start\":1881},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1886,\"start\":1884},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1923,\"start\":1919},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2138,\"start\":2135},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2224,\"start\":2220},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2796,\"start\":2792},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2799,\"start\":2796},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3192,\"start\":3188},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3232,\"start\":3228},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3362,\"start\":3358},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3392,\"start\":3388},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3529,\"start\":3525},{\"end\":4038,\"start\":4030},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5475,\"start\":5471},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6722,\"start\":6718},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7316,\"start\":7312},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7319,\"start\":7316},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7322,\"start\":7319},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7324,\"start\":7322},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7789,\"start\":7785},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7802,\"start\":7798},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7805,\"start\":7802},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7807,\"start\":7805},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8022,\"start\":8019},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8374,\"start\":8370},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8409,\"start\":8405},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8639,\"start\":8635},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8642,\"start\":8639},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8644,\"start\":8642},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8676,\"start\":8672},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8892,\"start\":8888},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8954,\"start\":8950},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9075,\"start\":9072},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9078,\"start\":9075},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9080,\"start\":9078},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9111,\"start\":9107},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9122,\"start\":9119},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9202,\"start\":9198},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9288,\"start\":9285},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9329,\"start\":9325},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9332,\"start\":9329},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9485,\"start\":9481},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9488,\"start\":9485},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9491,\"start\":9488},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9494,\"start\":9491},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9497,\"start\":9494},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9500,\"start\":9497},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9536,\"start\":9533},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9539,\"start\":9536},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9568,\"start\":9564},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9571,\"start\":9568},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9574,\"start\":9571},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9576,\"start\":9574},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9650,\"start\":9646},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9652,\"start\":9650},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9753,\"start\":9750},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9756,\"start\":9753},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9758,\"start\":9756},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10549,\"start\":10545},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11332,\"start\":11328},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12021,\"start\":12017},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13488,\"start\":13484},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13491,\"start\":13490},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14733,\"start\":14732},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14828,\"start\":14824},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15278,\"start\":15274},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15557,\"start\":15553},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15664,\"start\":15660},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15813,\"start\":15809},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15861,\"start\":15860},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16561,\"start\":16557},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16670,\"start\":16667},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16673,\"start\":16670},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16675,\"start\":16673},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20947,\"start\":20944},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21661,\"start\":21658},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21732,\"start\":21728},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21764,\"start\":21761},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22246,\"start\":22242},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":22915,\"start\":22911},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22918,\"start\":22915},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22920,\"start\":22918},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22923,\"start\":22920},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22986,\"start\":22982},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23097,\"start\":23093},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23112,\"start\":23109},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23757,\"start\":23754},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24788,\"start\":24785},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25176,\"start\":25172},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25288,\"start\":25284},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25506,\"start\":25502},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25671,\"start\":25667},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":26317,\"start\":26313},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26584,\"start\":26581},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26753,\"start\":26750},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26794,\"start\":26793},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26938,\"start\":26937},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27759,\"start\":27756},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27834,\"start\":27830},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28188,\"start\":28184},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":28705,\"start\":28701},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":28708,\"start\":28705},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28711,\"start\":28708},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29186,\"start\":29183},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29911,\"start\":29908},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30300,\"start\":30296},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":30324,\"start\":30320},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30375,\"start\":30371},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":30868,\"start\":30864},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":30887,\"start\":30883},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":31196,\"start\":31192},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":31212,\"start\":31208},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31457,\"start\":31453},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31460,\"start\":31457},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32443,\"start\":32439},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32446,\"start\":32443},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":32449,\"start\":32446},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32451,\"start\":32449},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32593,\"start\":32589},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32596,\"start\":32593},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32599,\"start\":32596},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32602,\"start\":32599},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34118,\"start\":34114},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":34161,\"start\":34157},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34272,\"start\":34269},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":34458,\"start\":34454},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34615,\"start\":34612},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34727,\"start\":34723},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35152,\"start\":35148},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35225,\"start\":35221},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35265,\"start\":35261},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35456,\"start\":35452},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35470,\"start\":35467},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35531,\"start\":35528},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35725,\"start\":35722},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":35801,\"start\":35797},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":35814,\"start\":35810},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35830,\"start\":35826},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36003,\"start\":35999},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36131,\"start\":36127},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36171,\"start\":36167},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36511,\"start\":36510},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36688,\"start\":36684},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":36703,\"start\":36699},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":36719,\"start\":36715},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36738,\"start\":36734},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":37454,\"start\":37450},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37788,\"start\":37784},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37820,\"start\":37817},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":37902,\"start\":37898},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38014,\"start\":38010},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":38029,\"start\":38025},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":38045,\"start\":38041},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":38064,\"start\":38060},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38228,\"start\":38224},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38247,\"start\":38243},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38567,\"start\":38563},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38990,\"start\":38986},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":39051,\"start\":39047},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39099,\"start\":39096},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39287,\"start\":39284},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":39320,\"start\":39316},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39679,\"start\":39675},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40558,\"start\":40555},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":44916,\"start\":44912}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41873,\"start\":41839},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42139,\"start\":41874},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42323,\"start\":42140},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42565,\"start\":42324},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43240,\"start\":42566},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":43751,\"start\":43241},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44348,\"start\":43752},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":44627,\"start\":44349},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":44921,\"start\":44628}]", "paragraph": "[{\"end\":1687,\"start\":1383},{\"end\":2139,\"start\":1689},{\"end\":2780,\"start\":2141},{\"end\":3034,\"start\":2782},{\"end\":3571,\"start\":3036},{\"end\":5611,\"start\":3573},{\"end\":6578,\"start\":5613},{\"end\":7695,\"start\":6580},{\"end\":8150,\"start\":7712},{\"end\":8777,\"start\":8152},{\"end\":9112,\"start\":8779},{\"end\":9333,\"start\":9114},{\"end\":9872,\"start\":9335},{\"end\":10520,\"start\":9885},{\"end\":10854,\"start\":10522},{\"end\":11290,\"start\":10856},{\"end\":11852,\"start\":11307},{\"end\":12316,\"start\":11854},{\"end\":12910,\"start\":12318},{\"end\":13821,\"start\":12912},{\"end\":15188,\"start\":13823},{\"end\":15502,\"start\":15213},{\"end\":16207,\"start\":15504},{\"end\":16342,\"start\":16227},{\"end\":16708,\"start\":16344},{\"end\":17577,\"start\":16710},{\"end\":17901,\"start\":17579},{\"end\":18744,\"start\":17903},{\"end\":19020,\"start\":18746},{\"end\":19218,\"start\":19022},{\"end\":19387,\"start\":19220},{\"end\":19942,\"start\":19389},{\"end\":21081,\"start\":19944},{\"end\":21579,\"start\":21083},{\"end\":22084,\"start\":21581},{\"end\":22476,\"start\":22086},{\"end\":22716,\"start\":22499},{\"end\":23286,\"start\":22718},{\"end\":23598,\"start\":23288},{\"end\":23707,\"start\":23600},{\"end\":24037,\"start\":23709},{\"end\":24267,\"start\":24039},{\"end\":24433,\"start\":24269},{\"end\":25699,\"start\":24435},{\"end\":26559,\"start\":25737},{\"end\":27419,\"start\":26561},{\"end\":27760,\"start\":27421},{\"end\":27976,\"start\":27762},{\"end\":29118,\"start\":27978},{\"end\":29759,\"start\":29142},{\"end\":30111,\"start\":29761},{\"end\":30235,\"start\":30145},{\"end\":30514,\"start\":30237},{\"end\":30815,\"start\":30516},{\"end\":31521,\"start\":30817},{\"end\":32289,\"start\":31547},{\"end\":32911,\"start\":32319},{\"end\":33649,\"start\":32913},{\"end\":33998,\"start\":33651},{\"end\":34617,\"start\":34056},{\"end\":35015,\"start\":34619},{\"end\":35296,\"start\":35017},{\"end\":35471,\"start\":35298},{\"end\":35884,\"start\":35473},{\"end\":36882,\"start\":35886},{\"end\":37208,\"start\":36884},{\"end\":37653,\"start\":37258},{\"end\":38117,\"start\":37655},{\"end\":38609,\"start\":38168},{\"end\":38991,\"start\":38611},{\"end\":39248,\"start\":39032},{\"end\":39902,\"start\":39250},{\"end\":40339,\"start\":39946},{\"end\":40619,\"start\":40341},{\"end\":41178,\"start\":40660},{\"end\":41838,\"start\":41180}]", "formula": null, "table_ref": "[{\"end\":16297,\"start\":16275},{\"end\":17175,\"start\":17168},{\"end\":17983,\"start\":17975},{\"end\":18000,\"start\":17992},{\"end\":18542,\"start\":18533},{\"end\":18736,\"start\":18727},{\"end\":19033,\"start\":19025},{\"end\":19323,\"start\":19316},{\"end\":20048,\"start\":20040},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20170,\"start\":20163},{\"end\":20799,\"start\":20790},{\"end\":21161,\"start\":21153},{\"end\":22408,\"start\":22400},{\"end\":23062,\"start\":23054},{\"end\":23635,\"start\":23627},{\"end\":25001,\"start\":24994},{\"end\":25789,\"start\":25782},{\"end\":26935,\"start\":26927},{\"end\":27232,\"start\":27225},{\"end\":27446,\"start\":27439},{\"end\":28263,\"start\":28256},{\"end\":29544,\"start\":29537},{\"end\":30234,\"start\":30227},{\"end\":30436,\"start\":30429},{\"end\":30611,\"start\":30602},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30906,\"start\":30899},{\"end\":31145,\"start\":31138},{\"end\":31573,\"start\":31566},{\"end\":31856,\"start\":31849},{\"end\":32116,\"start\":32109},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":35064,\"start\":35057},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":35423,\"start\":35416},{\"end\":35545,\"start\":35537},{\"end\":36766,\"start\":36758},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":36971,\"start\":36964},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":37688,\"start\":37680},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":37867,\"start\":37859},{\"end\":38713,\"start\":38706},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":39219,\"start\":39212},{\"end\":39597,\"start\":39589},{\"end\":39756,\"start\":39748},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":40273,\"start\":40265},{\"end\":40671,\"start\":40663},{\"end\":40828,\"start\":40819},{\"end\":40931,\"start\":40923},{\"end\":41242,\"start\":41233}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1381,\"start\":1369},{\"attributes\":{\"n\":\"2.\"},\"end\":7710,\"start\":7698},{\"attributes\":{\"n\":\"3.\"},\"end\":9883,\"start\":9875},{\"end\":11305,\"start\":11293},{\"attributes\":{\"n\":\"4.\"},\"end\":15211,\"start\":15191},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16225,\"start\":16210},{\"end\":22497,\"start\":22479},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25735,\"start\":25702},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29140,\"start\":29121},{\"attributes\":{\"n\":\"5.\"},\"end\":30143,\"start\":30114},{\"end\":31545,\"start\":31524},{\"attributes\":{\"n\":\"6.\"},\"end\":32317,\"start\":32292},{\"end\":34026,\"start\":34001},{\"end\":34054,\"start\":34029},{\"end\":37256,\"start\":37211},{\"end\":38166,\"start\":38120},{\"end\":39030,\"start\":38994},{\"end\":39944,\"start\":39905},{\"end\":40658,\"start\":40622},{\"end\":41850,\"start\":41840},{\"end\":41885,\"start\":41875},{\"end\":42151,\"start\":42141},{\"end\":42576,\"start\":42567},{\"end\":43251,\"start\":43242},{\"end\":43762,\"start\":43753},{\"end\":44359,\"start\":44350},{\"end\":44639,\"start\":44629}]", "table": "[{\"end\":43240,\"start\":43014},{\"end\":43751,\"start\":43253},{\"end\":44348,\"start\":43785},{\"end\":44627,\"start\":44392},{\"end\":44921,\"start\":44917}]", "figure_caption": "[{\"end\":41873,\"start\":41852},{\"end\":42139,\"start\":41887},{\"end\":42323,\"start\":42153},{\"end\":42565,\"start\":42326},{\"end\":43014,\"start\":42578},{\"end\":43785,\"start\":43764},{\"end\":44392,\"start\":44361},{\"end\":44917,\"start\":44642}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6108,\"start\":6099},{\"end\":7446,\"start\":7438},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10481,\"start\":10473},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11999,\"start\":11991},{\"end\":14999,\"start\":14991},{\"end\":16367,\"start\":16359},{\"end\":16798,\"start\":16790},{\"end\":17029,\"start\":17021},{\"end\":17147,\"start\":17139},{\"end\":17854,\"start\":17846},{\"end\":20669,\"start\":20661},{\"end\":23706,\"start\":23698},{\"end\":23797,\"start\":23788},{\"end\":24137,\"start\":24121},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24522,\"start\":24514},{\"end\":25526,\"start\":25518},{\"end\":27103,\"start\":27095},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27509,\"start\":27499},{\"end\":27975,\"start\":27967},{\"end\":28721,\"start\":28713},{\"end\":29153,\"start\":29145},{\"end\":33377,\"start\":33369},{\"end\":40617,\"start\":40609},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41404,\"start\":41395},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41611,\"start\":41602}]", "bib_author_first_name": "[{\"end\":45339,\"start\":45334},{\"end\":45343,\"start\":45340},{\"end\":45353,\"start\":45348},{\"end\":45358,\"start\":45354},{\"end\":45374,\"start\":45366},{\"end\":45376,\"start\":45375},{\"end\":45576,\"start\":45570},{\"end\":45584,\"start\":45582},{\"end\":45595,\"start\":45591},{\"end\":45844,\"start\":45837},{\"end\":45861,\"start\":45853},{\"end\":45863,\"start\":45862},{\"end\":46051,\"start\":46048},{\"end\":46067,\"start\":46059},{\"end\":46078,\"start\":46074},{\"end\":46093,\"start\":46086},{\"end\":46108,\"start\":46103},{\"end\":46110,\"start\":46109},{\"end\":46127,\"start\":46119},{\"end\":46144,\"start\":46138},{\"end\":46164,\"start\":46158},{\"end\":46178,\"start\":46172},{\"end\":46193,\"start\":46187},{\"end\":46210,\"start\":46202},{\"end\":46225,\"start\":46220},{\"end\":46248,\"start\":46240},{\"end\":46261,\"start\":46258},{\"end\":46277,\"start\":46272},{\"end\":46291,\"start\":46285},{\"end\":46306,\"start\":46300},{\"end\":46323,\"start\":46316},{\"end\":46335,\"start\":46328},{\"end\":46349,\"start\":46344},{\"end\":46361,\"start\":46357},{\"end\":46372,\"start\":46368},{\"end\":46388,\"start\":46381},{\"end\":47129,\"start\":47121},{\"end\":47141,\"start\":47137},{\"end\":47156,\"start\":47151},{\"end\":47169,\"start\":47164},{\"end\":47183,\"start\":47177},{\"end\":47197,\"start\":47192},{\"end\":47216,\"start\":47210},{\"end\":47459,\"start\":47455},{\"end\":47470,\"start\":47466},{\"end\":47485,\"start\":47480},{\"end\":47500,\"start\":47493},{\"end\":47511,\"start\":47505},{\"end\":47522,\"start\":47517},{\"end\":47533,\"start\":47529},{\"end\":47774,\"start\":47770},{\"end\":47786,\"start\":47781},{\"end\":47806,\"start\":47798},{\"end\":47824,\"start\":47816},{\"end\":48051,\"start\":48045},{\"end\":48065,\"start\":48058},{\"end\":48250,\"start\":48244},{\"end\":48264,\"start\":48257},{\"end\":48277,\"start\":48270},{\"end\":48501,\"start\":48496},{\"end\":48519,\"start\":48509},{\"end\":48528,\"start\":48527},{\"end\":48548,\"start\":48535},{\"end\":48781,\"start\":48774},{\"end\":48798,\"start\":48790},{\"end\":48979,\"start\":48973},{\"end\":49002,\"start\":48994},{\"end\":49015,\"start\":49009},{\"end\":49267,\"start\":49264},{\"end\":49277,\"start\":49274},{\"end\":49291,\"start\":49284},{\"end\":49306,\"start\":49300},{\"end\":49314,\"start\":49311},{\"end\":49321,\"start\":49319},{\"end\":49564,\"start\":49559},{\"end\":49581,\"start\":49573},{\"end\":49595,\"start\":49589},{\"end\":49609,\"start\":49601},{\"end\":49864,\"start\":49860},{\"end\":49881,\"start\":49874},{\"end\":49895,\"start\":49889},{\"end\":49897,\"start\":49896},{\"end\":50125,\"start\":50119},{\"end\":50144,\"start\":50139},{\"end\":50161,\"start\":50152},{\"end\":50178,\"start\":50174},{\"end\":50199,\"start\":50192},{\"end\":50212,\"start\":50206},{\"end\":50233,\"start\":50226},{\"end\":50252,\"start\":50244},{\"end\":50268,\"start\":50263},{\"end\":50285,\"start\":50278},{\"end\":50298,\"start\":50293},{\"end\":50314,\"start\":50310},{\"end\":50711,\"start\":50705},{\"end\":50728,\"start\":50721},{\"end\":50741,\"start\":50736},{\"end\":50978,\"start\":50972},{\"end\":50993,\"start\":50987},{\"end\":51203,\"start\":51198},{\"end\":51219,\"start\":51211},{\"end\":51235,\"start\":51227},{\"end\":51250,\"start\":51247},{\"end\":51263,\"start\":51255},{\"end\":51275,\"start\":51270},{\"end\":51287,\"start\":51281},{\"end\":51302,\"start\":51295},{\"end\":51321,\"start\":51316},{\"end\":51335,\"start\":51329},{\"end\":51349,\"start\":51344},{\"end\":51638,\"start\":51633},{\"end\":51651,\"start\":51646},{\"end\":51664,\"start\":51660},{\"end\":51681,\"start\":51675},{\"end\":51699,\"start\":51693},{\"end\":51716,\"start\":51712},{\"end\":51731,\"start\":51725},{\"end\":52153,\"start\":52141},{\"end\":52168,\"start\":52161},{\"end\":52183,\"start\":52176},{\"end\":52200,\"start\":52192},{\"end\":52215,\"start\":52209},{\"end\":52232,\"start\":52227},{\"end\":52250,\"start\":52246},{\"end\":52268,\"start\":52260},{\"end\":52289,\"start\":52282},{\"end\":52303,\"start\":52295},{\"end\":52314,\"start\":52304},{\"end\":52326,\"start\":52321},{\"end\":52338,\"start\":52333},{\"end\":52356,\"start\":52352},{\"end\":52370,\"start\":52364},{\"end\":52783,\"start\":52779},{\"end\":52798,\"start\":52793},{\"end\":52811,\"start\":52807},{\"end\":53050,\"start\":53043},{\"end\":53060,\"start\":53055},{\"end\":53071,\"start\":53066},{\"end\":53239,\"start\":53232},{\"end\":53251,\"start\":53244},{\"end\":53267,\"start\":53262},{\"end\":53280,\"start\":53276},{\"end\":53441,\"start\":53434},{\"end\":53453,\"start\":53446},{\"end\":53469,\"start\":53461},{\"end\":53479,\"start\":53475},{\"end\":53700,\"start\":53697},{\"end\":53718,\"start\":53712},{\"end\":53733,\"start\":53727},{\"end\":53744,\"start\":53738},{\"end\":53760,\"start\":53755},{\"end\":53771,\"start\":53767},{\"end\":53786,\"start\":53781},{\"end\":53799,\"start\":53794},{\"end\":53811,\"start\":53805},{\"end\":53826,\"start\":53822},{\"end\":54172,\"start\":54169},{\"end\":54190,\"start\":54184},{\"end\":54377,\"start\":54374},{\"end\":54394,\"start\":54389},{\"end\":54407,\"start\":54401},{\"end\":54421,\"start\":54416},{\"end\":54438,\"start\":54434},{\"end\":54646,\"start\":54645},{\"end\":54664,\"start\":54657},{\"end\":54666,\"start\":54665},{\"end\":54858,\"start\":54855},{\"end\":54868,\"start\":54866},{\"end\":54880,\"start\":54874},{\"end\":54892,\"start\":54886},{\"end\":54908,\"start\":54900},{\"end\":55152,\"start\":55146},{\"end\":55169,\"start\":55160},{\"end\":55397,\"start\":55392},{\"end\":55410,\"start\":55403},{\"end\":55422,\"start\":55416},{\"end\":55438,\"start\":55429},{\"end\":55453,\"start\":55445},{\"end\":55465,\"start\":55459},{\"end\":55712,\"start\":55708},{\"end\":55729,\"start\":55725},{\"end\":55746,\"start\":55741},{\"end\":55907,\"start\":55903},{\"end\":55923,\"start\":55915},{\"end\":55932,\"start\":55931},{\"end\":55945,\"start\":55939},{\"end\":55961,\"start\":55954},{\"end\":55963,\"start\":55962},{\"end\":55980,\"start\":55975},{\"end\":55997,\"start\":55989},{\"end\":55999,\"start\":55998},{\"end\":56379,\"start\":56372},{\"end\":56391,\"start\":56384},{\"end\":56403,\"start\":56397},{\"end\":56415,\"start\":56410},{\"end\":56431,\"start\":56424},{\"end\":56440,\"start\":56436},{\"end\":56748,\"start\":56740},{\"end\":56759,\"start\":56754},{\"end\":56772,\"start\":56768},{\"end\":56999,\"start\":56991},{\"end\":57012,\"start\":57005},{\"end\":57025,\"start\":57020},{\"end\":57041,\"start\":57036},{\"end\":57054,\"start\":57048},{\"end\":57067,\"start\":57063},{\"end\":57082,\"start\":57077},{\"end\":57101,\"start\":57091},{\"end\":57360,\"start\":57356},{\"end\":57378,\"start\":57373},{\"end\":57542,\"start\":57538},{\"end\":57560,\"start\":57555},{\"end\":57771,\"start\":57766},{\"end\":57785,\"start\":57781},{\"end\":57803,\"start\":57796},{\"end\":57823,\"start\":57816},{\"end\":57835,\"start\":57828},{\"end\":57850,\"start\":57844},{\"end\":58076,\"start\":58068},{\"end\":58086,\"start\":58082},{\"end\":58098,\"start\":58091},{\"end\":58112,\"start\":58105},{\"end\":58123,\"start\":58117},{\"end\":58137,\"start\":58130},{\"end\":58146,\"start\":58142},{\"end\":58154,\"start\":58151},{\"end\":58449,\"start\":58444},{\"end\":58464,\"start\":58459},{\"end\":58607,\"start\":58602},{\"end\":58627,\"start\":58622},{\"end\":58637,\"start\":58632},{\"end\":58905,\"start\":58900},{\"end\":58925,\"start\":58920},{\"end\":58940,\"start\":58935},{\"end\":59133,\"start\":59127},{\"end\":59146,\"start\":59142},{\"end\":59162,\"start\":59157},{\"end\":59177,\"start\":59171},{\"end\":59194,\"start\":59187},{\"end\":59411,\"start\":59405},{\"end\":59427,\"start\":59420},{\"end\":59444,\"start\":59440},{\"end\":59460,\"start\":59454},{\"end\":59476,\"start\":59470},{\"end\":59478,\"start\":59477},{\"end\":59743,\"start\":59739},{\"end\":59760,\"start\":59753},{\"end\":59967,\"start\":59963},{\"end\":59984,\"start\":59977},{\"end\":59994,\"start\":59989},{\"end\":60007,\"start\":60002},{\"end\":60019,\"start\":60014},{\"end\":60032,\"start\":60028},{\"end\":60277,\"start\":60272},{\"end\":60290,\"start\":60286},{\"end\":60304,\"start\":60300},{\"end\":60323,\"start\":60314},{\"end\":60335,\"start\":60329},{\"end\":60351,\"start\":60344},{\"end\":60365,\"start\":60360},{\"end\":60375,\"start\":60372},{\"end\":60385,\"start\":60380},{\"end\":60387,\"start\":60386},{\"end\":60661,\"start\":60655},{\"end\":60677,\"start\":60670},{\"end\":60693,\"start\":60686},{\"end\":60704,\"start\":60699},{\"end\":60718,\"start\":60711},{\"end\":60729,\"start\":60725},{\"end\":60743,\"start\":60739},{\"end\":60754,\"start\":60750},{\"end\":61016,\"start\":61011},{\"end\":61033,\"start\":61027},{\"end\":61240,\"start\":61231},{\"end\":61257,\"start\":61250},{\"end\":61275,\"start\":61269},{\"end\":61291,\"start\":61283},{\"end\":61308,\"start\":61300},{\"end\":61569,\"start\":61565},{\"end\":61587,\"start\":61579},{\"end\":61602,\"start\":61594},{\"end\":61619,\"start\":61610},{\"end\":61636,\"start\":61627},{\"end\":61656,\"start\":61651},{\"end\":61946,\"start\":61942},{\"end\":61965,\"start\":61956},{\"end\":61988,\"start\":61980},{\"end\":62004,\"start\":61996},{\"end\":62016,\"start\":62011},{\"end\":62214,\"start\":62210},{\"end\":62230,\"start\":62224},{\"end\":62585,\"start\":62580},{\"end\":62589,\"start\":62586},{\"end\":62610,\"start\":62606},{\"end\":62621,\"start\":62618},{\"end\":62632,\"start\":62628},{\"end\":62642,\"start\":62638},{\"end\":62902,\"start\":62896},{\"end\":62916,\"start\":62912},{\"end\":62930,\"start\":62926},{\"end\":62944,\"start\":62939},{\"end\":62961,\"start\":62956},{\"end\":62974,\"start\":62969},{\"end\":62976,\"start\":62975},{\"end\":62990,\"start\":62984},{\"end\":63004,\"start\":62999},{\"end\":63273,\"start\":63267},{\"end\":63287,\"start\":63283},{\"end\":63306,\"start\":63300},{\"end\":63329,\"start\":63315},{\"end\":63632,\"start\":63626},{\"end\":63646,\"start\":63642},{\"end\":63667,\"start\":63659},{\"end\":63682,\"start\":63676},{\"end\":63705,\"start\":63691},{\"end\":63720,\"start\":63716},{\"end\":64054,\"start\":64048},{\"end\":64068,\"start\":64061},{\"end\":64080,\"start\":64073},{\"end\":64093,\"start\":64089},{\"end\":64095,\"start\":64094},{\"end\":64337,\"start\":64329},{\"end\":64351,\"start\":64344},{\"end\":64559,\"start\":64552},{\"end\":64571,\"start\":64564},{\"end\":64585,\"start\":64579},{\"end\":64595,\"start\":64590},{\"end\":64808,\"start\":64804},{\"end\":64824,\"start\":64815},{\"end\":64835,\"start\":64830},{\"end\":64848,\"start\":64842},{\"end\":64860,\"start\":64856},{\"end\":65062,\"start\":65058},{\"end\":65075,\"start\":65069},{\"end\":65087,\"start\":65083},{\"end\":65102,\"start\":65096},{\"end\":65117,\"start\":65112},{\"end\":65130,\"start\":65126},{\"end\":65381,\"start\":65376},{\"end\":65396,\"start\":65392},{\"end\":65410,\"start\":65404},{\"end\":65422,\"start\":65419},{\"end\":65616,\"start\":65612},{\"end\":65626,\"start\":65622},{\"end\":65640,\"start\":65635},{\"end\":65785,\"start\":65783},{\"end\":65797,\"start\":65792},{\"end\":65809,\"start\":65803},{\"end\":65823,\"start\":65817},{\"end\":65839,\"start\":65830},{\"end\":66134,\"start\":66127},{\"end\":66148,\"start\":66140},{\"end\":66162,\"start\":66154},{\"end\":66184,\"start\":66178},{\"end\":66200,\"start\":66191},{\"end\":66459,\"start\":66453},{\"end\":66476,\"start\":66467},{\"end\":66489,\"start\":66484},{\"end\":66686,\"start\":66679},{\"end\":66701,\"start\":66694},{\"end\":66715,\"start\":66709},{\"end\":66717,\"start\":66716},{\"end\":66899,\"start\":66894},{\"end\":66911,\"start\":66906},{\"end\":66932,\"start\":66923},{\"end\":66946,\"start\":66939},{\"end\":66961,\"start\":66957},{\"end\":67210,\"start\":67205},{\"end\":67221,\"start\":67217},{\"end\":67234,\"start\":67228},{\"end\":67245,\"start\":67241},{\"end\":67257,\"start\":67252},{\"end\":67271,\"start\":67266},{\"end\":67289,\"start\":67282}]", "bib_author_last_name": "[{\"end\":45346,\"start\":45344},{\"end\":45364,\"start\":45359},{\"end\":45383,\"start\":45377},{\"end\":45580,\"start\":45577},{\"end\":45589,\"start\":45585},{\"end\":45599,\"start\":45596},{\"end\":45851,\"start\":45845},{\"end\":45870,\"start\":45864},{\"end\":46057,\"start\":46052},{\"end\":46072,\"start\":46068},{\"end\":46084,\"start\":46079},{\"end\":46101,\"start\":46094},{\"end\":46117,\"start\":46111},{\"end\":46136,\"start\":46128},{\"end\":46156,\"start\":46145},{\"end\":46170,\"start\":46165},{\"end\":46185,\"start\":46179},{\"end\":46200,\"start\":46194},{\"end\":46218,\"start\":46211},{\"end\":46238,\"start\":46226},{\"end\":46256,\"start\":46249},{\"end\":46270,\"start\":46262},{\"end\":46283,\"start\":46278},{\"end\":46298,\"start\":46292},{\"end\":46314,\"start\":46307},{\"end\":46326,\"start\":46324},{\"end\":46342,\"start\":46336},{\"end\":46355,\"start\":46350},{\"end\":46366,\"start\":46362},{\"end\":46379,\"start\":46373},{\"end\":46395,\"start\":46389},{\"end\":47135,\"start\":47130},{\"end\":47149,\"start\":47142},{\"end\":47162,\"start\":47157},{\"end\":47175,\"start\":47170},{\"end\":47190,\"start\":47184},{\"end\":47208,\"start\":47198},{\"end\":47223,\"start\":47217},{\"end\":47464,\"start\":47460},{\"end\":47478,\"start\":47471},{\"end\":47491,\"start\":47486},{\"end\":47503,\"start\":47501},{\"end\":47515,\"start\":47512},{\"end\":47527,\"start\":47523},{\"end\":47543,\"start\":47534},{\"end\":47779,\"start\":47775},{\"end\":47796,\"start\":47787},{\"end\":47814,\"start\":47807},{\"end\":47831,\"start\":47825},{\"end\":48056,\"start\":48052},{\"end\":48068,\"start\":48066},{\"end\":48255,\"start\":48251},{\"end\":48268,\"start\":48265},{\"end\":48280,\"start\":48278},{\"end\":48507,\"start\":48502},{\"end\":48525,\"start\":48520},{\"end\":48533,\"start\":48529},{\"end\":48551,\"start\":48549},{\"end\":48560,\"start\":48553},{\"end\":48788,\"start\":48782},{\"end\":48805,\"start\":48799},{\"end\":48992,\"start\":48980},{\"end\":49007,\"start\":49003},{\"end\":49022,\"start\":49016},{\"end\":49026,\"start\":49024},{\"end\":49272,\"start\":49268},{\"end\":49282,\"start\":49278},{\"end\":49298,\"start\":49292},{\"end\":49309,\"start\":49307},{\"end\":49317,\"start\":49315},{\"end\":49329,\"start\":49322},{\"end\":49571,\"start\":49565},{\"end\":49587,\"start\":49582},{\"end\":49599,\"start\":49596},{\"end\":49619,\"start\":49610},{\"end\":49872,\"start\":49865},{\"end\":49887,\"start\":49882},{\"end\":49903,\"start\":49898},{\"end\":50137,\"start\":50126},{\"end\":50150,\"start\":50145},{\"end\":50172,\"start\":50162},{\"end\":50190,\"start\":50179},{\"end\":50204,\"start\":50200},{\"end\":50224,\"start\":50213},{\"end\":50242,\"start\":50234},{\"end\":50261,\"start\":50253},{\"end\":50276,\"start\":50269},{\"end\":50291,\"start\":50286},{\"end\":50308,\"start\":50299},{\"end\":50322,\"start\":50315},{\"end\":50719,\"start\":50712},{\"end\":50734,\"start\":50729},{\"end\":50751,\"start\":50742},{\"end\":50985,\"start\":50979},{\"end\":51000,\"start\":50994},{\"end\":51209,\"start\":51204},{\"end\":51225,\"start\":51220},{\"end\":51245,\"start\":51236},{\"end\":51253,\"start\":51251},{\"end\":51268,\"start\":51264},{\"end\":51279,\"start\":51276},{\"end\":51293,\"start\":51288},{\"end\":51314,\"start\":51303},{\"end\":51327,\"start\":51322},{\"end\":51342,\"start\":51336},{\"end\":51360,\"start\":51350},{\"end\":51644,\"start\":51639},{\"end\":51658,\"start\":51652},{\"end\":51673,\"start\":51665},{\"end\":51691,\"start\":51682},{\"end\":51710,\"start\":51700},{\"end\":51723,\"start\":51717},{\"end\":51739,\"start\":51732},{\"end\":52159,\"start\":52154},{\"end\":52174,\"start\":52169},{\"end\":52190,\"start\":52184},{\"end\":52207,\"start\":52201},{\"end\":52225,\"start\":52216},{\"end\":52244,\"start\":52233},{\"end\":52258,\"start\":52251},{\"end\":52280,\"start\":52269},{\"end\":52293,\"start\":52290},{\"end\":52319,\"start\":52315},{\"end\":52331,\"start\":52327},{\"end\":52350,\"start\":52339},{\"end\":52362,\"start\":52357},{\"end\":52376,\"start\":52371},{\"end\":52791,\"start\":52784},{\"end\":52805,\"start\":52799},{\"end\":52817,\"start\":52812},{\"end\":53053,\"start\":53051},{\"end\":53064,\"start\":53061},{\"end\":53074,\"start\":53072},{\"end\":53242,\"start\":53240},{\"end\":53260,\"start\":53252},{\"end\":53274,\"start\":53268},{\"end\":53289,\"start\":53281},{\"end\":53444,\"start\":53442},{\"end\":53459,\"start\":53454},{\"end\":53473,\"start\":53470},{\"end\":53483,\"start\":53480},{\"end\":53710,\"start\":53701},{\"end\":53725,\"start\":53719},{\"end\":53736,\"start\":53734},{\"end\":53753,\"start\":53745},{\"end\":53765,\"start\":53761},{\"end\":53779,\"start\":53772},{\"end\":53792,\"start\":53787},{\"end\":53803,\"start\":53800},{\"end\":53820,\"start\":53812},{\"end\":53830,\"start\":53827},{\"end\":54182,\"start\":54173},{\"end\":54201,\"start\":54191},{\"end\":54387,\"start\":54378},{\"end\":54399,\"start\":54395},{\"end\":54414,\"start\":54408},{\"end\":54432,\"start\":54422},{\"end\":54443,\"start\":54439},{\"end\":54655,\"start\":54647},{\"end\":54673,\"start\":54667},{\"end\":54680,\"start\":54675},{\"end\":54864,\"start\":54859},{\"end\":54872,\"start\":54869},{\"end\":54884,\"start\":54881},{\"end\":54898,\"start\":54893},{\"end\":54919,\"start\":54909},{\"end\":55158,\"start\":55153},{\"end\":55177,\"start\":55170},{\"end\":55401,\"start\":55398},{\"end\":55414,\"start\":55411},{\"end\":55427,\"start\":55423},{\"end\":55443,\"start\":55439},{\"end\":55457,\"start\":55454},{\"end\":55470,\"start\":55466},{\"end\":55723,\"start\":55713},{\"end\":55739,\"start\":55730},{\"end\":55753,\"start\":55747},{\"end\":55913,\"start\":55908},{\"end\":55929,\"start\":55924},{\"end\":55937,\"start\":55933},{\"end\":55952,\"start\":55946},{\"end\":55973,\"start\":55964},{\"end\":55987,\"start\":55981},{\"end\":56007,\"start\":56000},{\"end\":56015,\"start\":56009},{\"end\":56382,\"start\":56380},{\"end\":56395,\"start\":56392},{\"end\":56408,\"start\":56404},{\"end\":56422,\"start\":56416},{\"end\":56434,\"start\":56432},{\"end\":56449,\"start\":56441},{\"end\":56752,\"start\":56749},{\"end\":56766,\"start\":56760},{\"end\":56781,\"start\":56773},{\"end\":57003,\"start\":57000},{\"end\":57018,\"start\":57013},{\"end\":57034,\"start\":57026},{\"end\":57046,\"start\":57042},{\"end\":57061,\"start\":57055},{\"end\":57075,\"start\":57068},{\"end\":57089,\"start\":57083},{\"end\":57109,\"start\":57102},{\"end\":57371,\"start\":57361},{\"end\":57385,\"start\":57379},{\"end\":57553,\"start\":57543},{\"end\":57567,\"start\":57561},{\"end\":57779,\"start\":57772},{\"end\":57794,\"start\":57786},{\"end\":57814,\"start\":57804},{\"end\":57826,\"start\":57824},{\"end\":57842,\"start\":57836},{\"end\":57853,\"start\":57851},{\"end\":58080,\"start\":58077},{\"end\":58089,\"start\":58087},{\"end\":58103,\"start\":58099},{\"end\":58115,\"start\":58113},{\"end\":58128,\"start\":58124},{\"end\":58140,\"start\":58138},{\"end\":58149,\"start\":58147},{\"end\":58158,\"start\":58155},{\"end\":58457,\"start\":58450},{\"end\":58471,\"start\":58465},{\"end\":58620,\"start\":58608},{\"end\":58630,\"start\":58628},{\"end\":58645,\"start\":58638},{\"end\":58918,\"start\":58906},{\"end\":58933,\"start\":58926},{\"end\":58952,\"start\":58941},{\"end\":59140,\"start\":59134},{\"end\":59155,\"start\":59147},{\"end\":59169,\"start\":59163},{\"end\":59185,\"start\":59178},{\"end\":59204,\"start\":59195},{\"end\":59418,\"start\":59412},{\"end\":59438,\"start\":59428},{\"end\":59452,\"start\":59445},{\"end\":59468,\"start\":59461},{\"end\":59484,\"start\":59479},{\"end\":59751,\"start\":59744},{\"end\":59771,\"start\":59761},{\"end\":59975,\"start\":59968},{\"end\":59987,\"start\":59985},{\"end\":60000,\"start\":59995},{\"end\":60012,\"start\":60008},{\"end\":60026,\"start\":60020},{\"end\":60042,\"start\":60033},{\"end\":60284,\"start\":60278},{\"end\":60298,\"start\":60291},{\"end\":60312,\"start\":60305},{\"end\":60327,\"start\":60324},{\"end\":60342,\"start\":60336},{\"end\":60358,\"start\":60352},{\"end\":60370,\"start\":60366},{\"end\":60378,\"start\":60376},{\"end\":60391,\"start\":60388},{\"end\":60668,\"start\":60662},{\"end\":60684,\"start\":60678},{\"end\":60697,\"start\":60694},{\"end\":60709,\"start\":60705},{\"end\":60723,\"start\":60719},{\"end\":60737,\"start\":60730},{\"end\":60748,\"start\":60744},{\"end\":60764,\"start\":60755},{\"end\":61025,\"start\":61017},{\"end\":61043,\"start\":61034},{\"end\":61248,\"start\":61241},{\"end\":61267,\"start\":61258},{\"end\":61281,\"start\":61276},{\"end\":61298,\"start\":61292},{\"end\":61314,\"start\":61309},{\"end\":61577,\"start\":61570},{\"end\":61592,\"start\":61588},{\"end\":61608,\"start\":61603},{\"end\":61625,\"start\":61620},{\"end\":61649,\"start\":61637},{\"end\":61662,\"start\":61657},{\"end\":61954,\"start\":61947},{\"end\":61978,\"start\":61966},{\"end\":61994,\"start\":61989},{\"end\":62009,\"start\":62005},{\"end\":62022,\"start\":62017},{\"end\":62222,\"start\":62215},{\"end\":62238,\"start\":62231},{\"end\":62604,\"start\":62590},{\"end\":62616,\"start\":62611},{\"end\":62626,\"start\":62622},{\"end\":62636,\"start\":62633},{\"end\":62646,\"start\":62643},{\"end\":62655,\"start\":62648},{\"end\":62910,\"start\":62903},{\"end\":62924,\"start\":62917},{\"end\":62937,\"start\":62931},{\"end\":62954,\"start\":62945},{\"end\":62967,\"start\":62962},{\"end\":62982,\"start\":62977},{\"end\":62997,\"start\":62991},{\"end\":63015,\"start\":63005},{\"end\":63281,\"start\":63274},{\"end\":63298,\"start\":63288},{\"end\":63313,\"start\":63307},{\"end\":63338,\"start\":63330},{\"end\":63640,\"start\":63633},{\"end\":63657,\"start\":63647},{\"end\":63674,\"start\":63668},{\"end\":63689,\"start\":63683},{\"end\":63714,\"start\":63706},{\"end\":63727,\"start\":63721},{\"end\":64059,\"start\":64055},{\"end\":64071,\"start\":64069},{\"end\":64087,\"start\":64081},{\"end\":64100,\"start\":64096},{\"end\":64342,\"start\":64338},{\"end\":64357,\"start\":64352},{\"end\":64562,\"start\":64560},{\"end\":64577,\"start\":64572},{\"end\":64588,\"start\":64586},{\"end\":64599,\"start\":64596},{\"end\":64813,\"start\":64809},{\"end\":64828,\"start\":64825},{\"end\":64840,\"start\":64836},{\"end\":64854,\"start\":64849},{\"end\":64864,\"start\":64861},{\"end\":65067,\"start\":65063},{\"end\":65081,\"start\":65076},{\"end\":65094,\"start\":65088},{\"end\":65110,\"start\":65103},{\"end\":65124,\"start\":65118},{\"end\":65139,\"start\":65131},{\"end\":65390,\"start\":65382},{\"end\":65402,\"start\":65397},{\"end\":65417,\"start\":65411},{\"end\":65429,\"start\":65423},{\"end\":65620,\"start\":65617},{\"end\":65633,\"start\":65627},{\"end\":65649,\"start\":65641},{\"end\":65790,\"start\":65786},{\"end\":65801,\"start\":65798},{\"end\":65815,\"start\":65810},{\"end\":65828,\"start\":65824},{\"end\":65843,\"start\":65840},{\"end\":66138,\"start\":66135},{\"end\":66152,\"start\":66149},{\"end\":66176,\"start\":66163},{\"end\":66189,\"start\":66185},{\"end\":66205,\"start\":66201},{\"end\":66210,\"start\":66207},{\"end\":66465,\"start\":66460},{\"end\":66482,\"start\":66477},{\"end\":66504,\"start\":66490},{\"end\":66515,\"start\":66506},{\"end\":66692,\"start\":66687},{\"end\":66707,\"start\":66702},{\"end\":66723,\"start\":66718},{\"end\":66904,\"start\":66900},{\"end\":66921,\"start\":66912},{\"end\":66937,\"start\":66933},{\"end\":66955,\"start\":66947},{\"end\":66967,\"start\":66962},{\"end\":67215,\"start\":67211},{\"end\":67226,\"start\":67222},{\"end\":67239,\"start\":67235},{\"end\":67250,\"start\":67246},{\"end\":67264,\"start\":67258},{\"end\":67280,\"start\":67272},{\"end\":67298,\"start\":67290}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b0\"},\"end\":45521,\"start\":45334},{\"attributes\":{\"doi\":\"arXiv:2106.08254\",\"id\":\"b1\"},\"end\":45745,\"start\":45523},{\"attributes\":{\"id\":\"b2\"},\"end\":46007,\"start\":45747},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":47059,\"start\":46009},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":233444273},\"end\":47417,\"start\":47061},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":219781060},\"end\":47697,\"start\":47419},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211096730},\"end\":47993,\"start\":47699},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":227118869},\"end\":48174,\"start\":47995},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":233024948},\"end\":48416,\"start\":48176},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":213152193},\"end\":48729,\"start\":48418},{\"attributes\":{\"id\":\"b10\"},\"end\":48891,\"start\":48731},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208006202},\"end\":49209,\"start\":48893},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":57246310},\"end\":49475,\"start\":49211},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52967399},\"end\":49791,\"start\":49477},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9062671},\"end\":50041,\"start\":49793},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":225039882},\"end\":50635,\"start\":50043},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4009713},\"end\":50895,\"start\":50637},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5575601},\"end\":51136,\"start\":50897},{\"attributes\":{\"doi\":\"arXiv:2103.01988\",\"id\":\"b18\"},\"end\":51631,\"start\":51138},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b19\"},\"end\":52068,\"start\":51633},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219687798},\"end\":52718,\"start\":52070},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8281592},\"end\":52942,\"start\":52720},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207930212},\"end\":53230,\"start\":52944},{\"attributes\":{\"id\":\"b23\"},\"end\":53386,\"start\":53232},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206594692},\"end\":53606,\"start\":53388},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":220250257},\"end\":54087,\"start\":53608},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":56657912},\"end\":54342,\"start\":54089},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":196831327},\"end\":54574,\"start\":54344},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2445072},\"end\":54816,\"start\":54576},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6773885},\"end\":55050,\"start\":54818},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5808102},\"end\":55331,\"start\":55052},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":235691448},\"end\":55641,\"start\":55333},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":195908774},\"end\":55899,\"start\":55643},{\"attributes\":{\"id\":\"b33\"},\"end\":56303,\"start\":55901},{\"attributes\":{\"id\":\"b34\"},\"end\":56640,\"start\":56305},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10716717},\"end\":56946,\"start\":56642},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14113767},\"end\":57300,\"start\":56948},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14337532},\"end\":57497,\"start\":57302},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":53592270},\"end\":57664,\"start\":57499},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13751202},\"end\":58066,\"start\":57666},{\"attributes\":{\"doi\":\"arXiv:2105.07926\",\"id\":\"b40\"},\"end\":58367,\"start\":58068},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":187547},\"end\":58600,\"start\":58369},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b42\"},\"end\":58857,\"start\":58602},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":20282961},\"end\":59081,\"start\":58859},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10054272},\"end\":59353,\"start\":59083},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":2202933},\"end\":59643,\"start\":59355},{\"attributes\":{\"id\":\"b46\"},\"end\":59908,\"start\":59645},{\"attributes\":{\"id\":\"b47\"},\"end\":60187,\"start\":59910},{\"attributes\":{\"id\":\"b48\"},\"end\":60617,\"start\":60189},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":232035663},\"end\":60941,\"start\":60619},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14124313},\"end\":61170,\"start\":60943},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":206593880},\"end\":61486,\"start\":61172},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":229363322},\"end\":61868,\"start\":61488},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":227209576},\"end\":62208,\"start\":61870},{\"attributes\":{\"doi\":\"arXiv:1906.06423\",\"id\":\"b54\"},\"end\":62467,\"start\":62210},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":29156801},\"end\":62867,\"start\":62469},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":13756489},\"end\":63195,\"start\":62869},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":207168299},\"end\":63508,\"start\":63197},{\"attributes\":{\"id\":\"b58\"},\"end\":63969,\"start\":63510},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":173188134},\"end\":64265,\"start\":63971},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":2057504},\"end\":64476,\"start\":64267},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":4591284},\"end\":64750,\"start\":64478},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":50781105},\"end\":65007,\"start\":64752},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":235658393},\"end\":65307,\"start\":65009},{\"attributes\":{\"id\":\"b64\"},\"end\":65562,\"start\":65309},{\"attributes\":{\"doi\":\"arXiv:1708.03888\",\"id\":\"b65\"},\"end\":65781,\"start\":65564},{\"attributes\":{\"doi\":\"arXiv:2106.13112\",\"id\":\"b66\"},\"end\":66038,\"start\":65783},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":152282661},\"end\":66408,\"start\":66040},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":3162051},\"end\":66648,\"start\":66410},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":50698},\"end\":66824,\"start\":66650},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":1849990},\"end\":67142,\"start\":66826},{\"attributes\":{\"id\":\"b71\"},\"end\":67475,\"start\":67144}]", "bib_title": "[{\"end\":46046,\"start\":46009},{\"end\":47119,\"start\":47061},{\"end\":47453,\"start\":47419},{\"end\":47768,\"start\":47699},{\"end\":48043,\"start\":47995},{\"end\":48242,\"start\":48176},{\"end\":48494,\"start\":48418},{\"end\":48971,\"start\":48893},{\"end\":49262,\"start\":49211},{\"end\":49557,\"start\":49477},{\"end\":49858,\"start\":49793},{\"end\":50117,\"start\":50043},{\"end\":50703,\"start\":50637},{\"end\":50970,\"start\":50897},{\"end\":52139,\"start\":52070},{\"end\":52777,\"start\":52720},{\"end\":53041,\"start\":52944},{\"end\":53432,\"start\":53388},{\"end\":53695,\"start\":53608},{\"end\":54167,\"start\":54089},{\"end\":54372,\"start\":54344},{\"end\":54643,\"start\":54576},{\"end\":54853,\"start\":54818},{\"end\":55144,\"start\":55052},{\"end\":55390,\"start\":55333},{\"end\":55706,\"start\":55643},{\"end\":56738,\"start\":56642},{\"end\":56989,\"start\":56948},{\"end\":57354,\"start\":57302},{\"end\":57536,\"start\":57499},{\"end\":57764,\"start\":57666},{\"end\":58442,\"start\":58369},{\"end\":58898,\"start\":58859},{\"end\":59125,\"start\":59083},{\"end\":59403,\"start\":59355},{\"end\":60653,\"start\":60619},{\"end\":61009,\"start\":60943},{\"end\":61229,\"start\":61172},{\"end\":61563,\"start\":61488},{\"end\":61940,\"start\":61870},{\"end\":62578,\"start\":62469},{\"end\":62894,\"start\":62869},{\"end\":63265,\"start\":63197},{\"end\":64046,\"start\":63971},{\"end\":64327,\"start\":64267},{\"end\":64550,\"start\":64478},{\"end\":64802,\"start\":64752},{\"end\":65056,\"start\":65009},{\"end\":66125,\"start\":66040},{\"end\":66451,\"start\":66410},{\"end\":66677,\"start\":66650},{\"end\":66892,\"start\":66826}]", "bib_author": "[{\"end\":45348,\"start\":45334},{\"end\":45366,\"start\":45348},{\"end\":45385,\"start\":45366},{\"end\":45582,\"start\":45570},{\"end\":45591,\"start\":45582},{\"end\":45601,\"start\":45591},{\"end\":45853,\"start\":45837},{\"end\":45872,\"start\":45853},{\"end\":46059,\"start\":46048},{\"end\":46074,\"start\":46059},{\"end\":46086,\"start\":46074},{\"end\":46103,\"start\":46086},{\"end\":46119,\"start\":46103},{\"end\":46138,\"start\":46119},{\"end\":46158,\"start\":46138},{\"end\":46172,\"start\":46158},{\"end\":46187,\"start\":46172},{\"end\":46202,\"start\":46187},{\"end\":46220,\"start\":46202},{\"end\":46240,\"start\":46220},{\"end\":46258,\"start\":46240},{\"end\":46272,\"start\":46258},{\"end\":46285,\"start\":46272},{\"end\":46300,\"start\":46285},{\"end\":46316,\"start\":46300},{\"end\":46328,\"start\":46316},{\"end\":46344,\"start\":46328},{\"end\":46357,\"start\":46344},{\"end\":46368,\"start\":46357},{\"end\":46381,\"start\":46368},{\"end\":46397,\"start\":46381},{\"end\":47137,\"start\":47121},{\"end\":47151,\"start\":47137},{\"end\":47164,\"start\":47151},{\"end\":47177,\"start\":47164},{\"end\":47192,\"start\":47177},{\"end\":47210,\"start\":47192},{\"end\":47225,\"start\":47210},{\"end\":47466,\"start\":47455},{\"end\":47480,\"start\":47466},{\"end\":47493,\"start\":47480},{\"end\":47505,\"start\":47493},{\"end\":47517,\"start\":47505},{\"end\":47529,\"start\":47517},{\"end\":47545,\"start\":47529},{\"end\":47781,\"start\":47770},{\"end\":47798,\"start\":47781},{\"end\":47816,\"start\":47798},{\"end\":47833,\"start\":47816},{\"end\":48058,\"start\":48045},{\"end\":48070,\"start\":48058},{\"end\":48257,\"start\":48244},{\"end\":48270,\"start\":48257},{\"end\":48282,\"start\":48270},{\"end\":48509,\"start\":48496},{\"end\":48527,\"start\":48509},{\"end\":48535,\"start\":48527},{\"end\":48553,\"start\":48535},{\"end\":48562,\"start\":48553},{\"end\":48790,\"start\":48774},{\"end\":48807,\"start\":48790},{\"end\":48994,\"start\":48973},{\"end\":49009,\"start\":48994},{\"end\":49024,\"start\":49009},{\"end\":49028,\"start\":49024},{\"end\":49274,\"start\":49264},{\"end\":49284,\"start\":49274},{\"end\":49300,\"start\":49284},{\"end\":49311,\"start\":49300},{\"end\":49319,\"start\":49311},{\"end\":49331,\"start\":49319},{\"end\":49573,\"start\":49559},{\"end\":49589,\"start\":49573},{\"end\":49601,\"start\":49589},{\"end\":49621,\"start\":49601},{\"end\":49874,\"start\":49860},{\"end\":49889,\"start\":49874},{\"end\":49905,\"start\":49889},{\"end\":50139,\"start\":50119},{\"end\":50152,\"start\":50139},{\"end\":50174,\"start\":50152},{\"end\":50192,\"start\":50174},{\"end\":50206,\"start\":50192},{\"end\":50226,\"start\":50206},{\"end\":50244,\"start\":50226},{\"end\":50263,\"start\":50244},{\"end\":50278,\"start\":50263},{\"end\":50293,\"start\":50278},{\"end\":50310,\"start\":50293},{\"end\":50324,\"start\":50310},{\"end\":50721,\"start\":50705},{\"end\":50736,\"start\":50721},{\"end\":50753,\"start\":50736},{\"end\":50987,\"start\":50972},{\"end\":51002,\"start\":50987},{\"end\":51211,\"start\":51198},{\"end\":51227,\"start\":51211},{\"end\":51247,\"start\":51227},{\"end\":51255,\"start\":51247},{\"end\":51270,\"start\":51255},{\"end\":51281,\"start\":51270},{\"end\":51295,\"start\":51281},{\"end\":51316,\"start\":51295},{\"end\":51329,\"start\":51316},{\"end\":51344,\"start\":51329},{\"end\":51362,\"start\":51344},{\"end\":51646,\"start\":51633},{\"end\":51660,\"start\":51646},{\"end\":51675,\"start\":51660},{\"end\":51693,\"start\":51675},{\"end\":51712,\"start\":51693},{\"end\":51725,\"start\":51712},{\"end\":51741,\"start\":51725},{\"end\":52161,\"start\":52141},{\"end\":52176,\"start\":52161},{\"end\":52192,\"start\":52176},{\"end\":52209,\"start\":52192},{\"end\":52227,\"start\":52209},{\"end\":52246,\"start\":52227},{\"end\":52260,\"start\":52246},{\"end\":52282,\"start\":52260},{\"end\":52295,\"start\":52282},{\"end\":52321,\"start\":52295},{\"end\":52333,\"start\":52321},{\"end\":52352,\"start\":52333},{\"end\":52364,\"start\":52352},{\"end\":52378,\"start\":52364},{\"end\":52793,\"start\":52779},{\"end\":52807,\"start\":52793},{\"end\":52819,\"start\":52807},{\"end\":53055,\"start\":53043},{\"end\":53066,\"start\":53055},{\"end\":53076,\"start\":53066},{\"end\":53244,\"start\":53232},{\"end\":53262,\"start\":53244},{\"end\":53276,\"start\":53262},{\"end\":53291,\"start\":53276},{\"end\":53446,\"start\":53434},{\"end\":53461,\"start\":53446},{\"end\":53475,\"start\":53461},{\"end\":53485,\"start\":53475},{\"end\":53712,\"start\":53697},{\"end\":53727,\"start\":53712},{\"end\":53738,\"start\":53727},{\"end\":53755,\"start\":53738},{\"end\":53767,\"start\":53755},{\"end\":53781,\"start\":53767},{\"end\":53794,\"start\":53781},{\"end\":53805,\"start\":53794},{\"end\":53822,\"start\":53805},{\"end\":53832,\"start\":53822},{\"end\":54184,\"start\":54169},{\"end\":54203,\"start\":54184},{\"end\":54389,\"start\":54374},{\"end\":54401,\"start\":54389},{\"end\":54416,\"start\":54401},{\"end\":54434,\"start\":54416},{\"end\":54445,\"start\":54434},{\"end\":54657,\"start\":54645},{\"end\":54675,\"start\":54657},{\"end\":54682,\"start\":54675},{\"end\":54866,\"start\":54855},{\"end\":54874,\"start\":54866},{\"end\":54886,\"start\":54874},{\"end\":54900,\"start\":54886},{\"end\":54921,\"start\":54900},{\"end\":55160,\"start\":55146},{\"end\":55179,\"start\":55160},{\"end\":55403,\"start\":55392},{\"end\":55416,\"start\":55403},{\"end\":55429,\"start\":55416},{\"end\":55445,\"start\":55429},{\"end\":55459,\"start\":55445},{\"end\":55472,\"start\":55459},{\"end\":55725,\"start\":55708},{\"end\":55741,\"start\":55725},{\"end\":55755,\"start\":55741},{\"end\":55915,\"start\":55903},{\"end\":55931,\"start\":55915},{\"end\":55939,\"start\":55931},{\"end\":55954,\"start\":55939},{\"end\":55975,\"start\":55954},{\"end\":55989,\"start\":55975},{\"end\":56009,\"start\":55989},{\"end\":56017,\"start\":56009},{\"end\":56384,\"start\":56372},{\"end\":56397,\"start\":56384},{\"end\":56410,\"start\":56397},{\"end\":56424,\"start\":56410},{\"end\":56436,\"start\":56424},{\"end\":56451,\"start\":56436},{\"end\":56754,\"start\":56740},{\"end\":56768,\"start\":56754},{\"end\":56783,\"start\":56768},{\"end\":57005,\"start\":56991},{\"end\":57020,\"start\":57005},{\"end\":57036,\"start\":57020},{\"end\":57048,\"start\":57036},{\"end\":57063,\"start\":57048},{\"end\":57077,\"start\":57063},{\"end\":57091,\"start\":57077},{\"end\":57111,\"start\":57091},{\"end\":57373,\"start\":57356},{\"end\":57387,\"start\":57373},{\"end\":57555,\"start\":57538},{\"end\":57569,\"start\":57555},{\"end\":57781,\"start\":57766},{\"end\":57796,\"start\":57781},{\"end\":57816,\"start\":57796},{\"end\":57828,\"start\":57816},{\"end\":57844,\"start\":57828},{\"end\":57855,\"start\":57844},{\"end\":58082,\"start\":58068},{\"end\":58091,\"start\":58082},{\"end\":58105,\"start\":58091},{\"end\":58117,\"start\":58105},{\"end\":58130,\"start\":58117},{\"end\":58142,\"start\":58130},{\"end\":58151,\"start\":58142},{\"end\":58160,\"start\":58151},{\"end\":58459,\"start\":58444},{\"end\":58473,\"start\":58459},{\"end\":58622,\"start\":58602},{\"end\":58632,\"start\":58622},{\"end\":58647,\"start\":58632},{\"end\":58920,\"start\":58900},{\"end\":58935,\"start\":58920},{\"end\":58954,\"start\":58935},{\"end\":59142,\"start\":59127},{\"end\":59157,\"start\":59142},{\"end\":59171,\"start\":59157},{\"end\":59187,\"start\":59171},{\"end\":59206,\"start\":59187},{\"end\":59420,\"start\":59405},{\"end\":59440,\"start\":59420},{\"end\":59454,\"start\":59440},{\"end\":59470,\"start\":59454},{\"end\":59486,\"start\":59470},{\"end\":59753,\"start\":59739},{\"end\":59773,\"start\":59753},{\"end\":59977,\"start\":59963},{\"end\":59989,\"start\":59977},{\"end\":60002,\"start\":59989},{\"end\":60014,\"start\":60002},{\"end\":60028,\"start\":60014},{\"end\":60044,\"start\":60028},{\"end\":60286,\"start\":60272},{\"end\":60300,\"start\":60286},{\"end\":60314,\"start\":60300},{\"end\":60329,\"start\":60314},{\"end\":60344,\"start\":60329},{\"end\":60360,\"start\":60344},{\"end\":60372,\"start\":60360},{\"end\":60380,\"start\":60372},{\"end\":60393,\"start\":60380},{\"end\":60670,\"start\":60655},{\"end\":60686,\"start\":60670},{\"end\":60699,\"start\":60686},{\"end\":60711,\"start\":60699},{\"end\":60725,\"start\":60711},{\"end\":60739,\"start\":60725},{\"end\":60750,\"start\":60739},{\"end\":60766,\"start\":60750},{\"end\":61027,\"start\":61011},{\"end\":61045,\"start\":61027},{\"end\":61250,\"start\":61231},{\"end\":61269,\"start\":61250},{\"end\":61283,\"start\":61269},{\"end\":61300,\"start\":61283},{\"end\":61316,\"start\":61300},{\"end\":61579,\"start\":61565},{\"end\":61594,\"start\":61579},{\"end\":61610,\"start\":61594},{\"end\":61627,\"start\":61610},{\"end\":61651,\"start\":61627},{\"end\":61664,\"start\":61651},{\"end\":61956,\"start\":61942},{\"end\":61980,\"start\":61956},{\"end\":61996,\"start\":61980},{\"end\":62011,\"start\":61996},{\"end\":62024,\"start\":62011},{\"end\":62224,\"start\":62210},{\"end\":62240,\"start\":62224},{\"end\":62606,\"start\":62580},{\"end\":62618,\"start\":62606},{\"end\":62628,\"start\":62618},{\"end\":62638,\"start\":62628},{\"end\":62648,\"start\":62638},{\"end\":62657,\"start\":62648},{\"end\":62912,\"start\":62896},{\"end\":62926,\"start\":62912},{\"end\":62939,\"start\":62926},{\"end\":62956,\"start\":62939},{\"end\":62969,\"start\":62956},{\"end\":62984,\"start\":62969},{\"end\":62999,\"start\":62984},{\"end\":63017,\"start\":62999},{\"end\":63283,\"start\":63267},{\"end\":63300,\"start\":63283},{\"end\":63315,\"start\":63300},{\"end\":63340,\"start\":63315},{\"end\":63642,\"start\":63626},{\"end\":63659,\"start\":63642},{\"end\":63676,\"start\":63659},{\"end\":63691,\"start\":63676},{\"end\":63716,\"start\":63691},{\"end\":63729,\"start\":63716},{\"end\":64061,\"start\":64048},{\"end\":64073,\"start\":64061},{\"end\":64089,\"start\":64073},{\"end\":64102,\"start\":64089},{\"end\":64344,\"start\":64329},{\"end\":64359,\"start\":64344},{\"end\":64564,\"start\":64552},{\"end\":64579,\"start\":64564},{\"end\":64590,\"start\":64579},{\"end\":64601,\"start\":64590},{\"end\":64815,\"start\":64804},{\"end\":64830,\"start\":64815},{\"end\":64842,\"start\":64830},{\"end\":64856,\"start\":64842},{\"end\":64866,\"start\":64856},{\"end\":65069,\"start\":65058},{\"end\":65083,\"start\":65069},{\"end\":65096,\"start\":65083},{\"end\":65112,\"start\":65096},{\"end\":65126,\"start\":65112},{\"end\":65141,\"start\":65126},{\"end\":65392,\"start\":65376},{\"end\":65404,\"start\":65392},{\"end\":65419,\"start\":65404},{\"end\":65431,\"start\":65419},{\"end\":65622,\"start\":65612},{\"end\":65635,\"start\":65622},{\"end\":65651,\"start\":65635},{\"end\":65792,\"start\":65783},{\"end\":65803,\"start\":65792},{\"end\":65817,\"start\":65803},{\"end\":65830,\"start\":65817},{\"end\":65845,\"start\":65830},{\"end\":66140,\"start\":66127},{\"end\":66154,\"start\":66140},{\"end\":66178,\"start\":66154},{\"end\":66191,\"start\":66178},{\"end\":66207,\"start\":66191},{\"end\":66212,\"start\":66207},{\"end\":66467,\"start\":66453},{\"end\":66484,\"start\":66467},{\"end\":66506,\"start\":66484},{\"end\":66517,\"start\":66506},{\"end\":66694,\"start\":66679},{\"end\":66709,\"start\":66694},{\"end\":66725,\"start\":66709},{\"end\":66906,\"start\":66894},{\"end\":66923,\"start\":66906},{\"end\":66939,\"start\":66923},{\"end\":66957,\"start\":66939},{\"end\":66969,\"start\":66957},{\"end\":67217,\"start\":67205},{\"end\":67228,\"start\":67217},{\"end\":67241,\"start\":67228},{\"end\":67252,\"start\":67241},{\"end\":67266,\"start\":67252},{\"end\":67282,\"start\":67266},{\"end\":67300,\"start\":67282}]", "bib_venue": "[{\"end\":46519,\"start\":46431},{\"end\":45420,\"start\":45401},{\"end\":45568,\"start\":45523},{\"end\":45835,\"start\":45747},{\"end\":46429,\"start\":46397},{\"end\":47229,\"start\":47225},{\"end\":47549,\"start\":47545},{\"end\":47837,\"start\":47833},{\"end\":48074,\"start\":48070},{\"end\":48286,\"start\":48282},{\"end\":48566,\"start\":48562},{\"end\":48772,\"start\":48731},{\"end\":49042,\"start\":49028},{\"end\":49335,\"start\":49331},{\"end\":49626,\"start\":49621},{\"end\":49909,\"start\":49905},{\"end\":50328,\"start\":50324},{\"end\":50757,\"start\":50753},{\"end\":51009,\"start\":51002},{\"end\":51196,\"start\":51138},{\"end\":51845,\"start\":51757},{\"end\":52385,\"start\":52378},{\"end\":52823,\"start\":52819},{\"end\":53080,\"start\":53076},{\"end\":53295,\"start\":53291},{\"end\":53489,\"start\":53485},{\"end\":53836,\"start\":53832},{\"end\":54207,\"start\":54203},{\"end\":54449,\"start\":54445},{\"end\":54689,\"start\":54682},{\"end\":54925,\"start\":54921},{\"end\":55183,\"start\":55179},{\"end\":55476,\"start\":55472},{\"end\":55762,\"start\":55755},{\"end\":56370,\"start\":56305},{\"end\":56787,\"start\":56783},{\"end\":57115,\"start\":57111},{\"end\":57391,\"start\":57387},{\"end\":57573,\"start\":57569},{\"end\":57859,\"start\":57855},{\"end\":58209,\"start\":58176},{\"end\":58477,\"start\":58473},{\"end\":58721,\"start\":58663},{\"end\":58961,\"start\":58954},{\"end\":59210,\"start\":59206},{\"end\":59490,\"start\":59486},{\"end\":59737,\"start\":59645},{\"end\":59961,\"start\":59910},{\"end\":60270,\"start\":60189},{\"end\":60770,\"start\":60766},{\"end\":61049,\"start\":61045},{\"end\":61320,\"start\":61316},{\"end\":61668,\"start\":61664},{\"end\":62028,\"start\":62024},{\"end\":62333,\"start\":62256},{\"end\":62661,\"start\":62657},{\"end\":63024,\"start\":63017},{\"end\":63344,\"start\":63340},{\"end\":63624,\"start\":63510},{\"end\":64109,\"start\":64102},{\"end\":64363,\"start\":64359},{\"end\":64605,\"start\":64601},{\"end\":64873,\"start\":64866},{\"end\":65148,\"start\":65141},{\"end\":65374,\"start\":65309},{\"end\":65610,\"start\":65564},{\"end\":65900,\"start\":65861},{\"end\":66216,\"start\":66212},{\"end\":66521,\"start\":66517},{\"end\":66729,\"start\":66725},{\"end\":66976,\"start\":66969},{\"end\":67203,\"start\":67144}]"}}}, "year": 2023, "month": 12, "day": 17}