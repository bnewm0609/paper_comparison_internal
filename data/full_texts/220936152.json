{"id": 220936152, "updated": "2023-10-06 12:58:30.066", "metadata": {"title": "Trojaning Language Models for Fun and Profit", "authors": "[{\"first\":\"Xinyang\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ting\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 8, "day": 1}, "abstract": "Recent years have witnessed a new paradigm of building natural language processing (NLP) systems: general-purpose, pre-trained language models (LMs) are fine-tuned with simple downstream models to attain state-of-the-art performance for a variety of target tasks. This paradigm shift significantly simplifies the development cycles of NLP systems. Yet, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, about which little is known thus far. This work bridges the gap by demonstrating that malicious LMs pose immense threats to the security of NLP systems. Specifically, we present TROJAN-ML, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-sensitive NLP tasks (toxic comment classification, question answering, text completion), we demonstrate that TROJAN-ML possesses the following properties: (i) efficacy - the host systems misbehave as desired by the adversary with high probability, (ii) specificity - the trajoned LMs function indistinguishably from their benign counterparts on non-target inputs, and (iii) fluency - the trigger-embedded sentences are highly indistinguishable from natural language and highly relevant to the surrounding contexts. We provide analytical justification for the practicality of TROJAN-ML, which points to the unprecedented complexity of today's LMs. We further discuss potential countermeasures and their challenges, which lead to several promising research directions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2008.00312", "mag": "3046664463", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eurosp/ZhangZJW21", "doi": "10.1109/eurosp51992.2021.00022"}}, "content": {"source": {"pdf_hash": "11fe33206746251656698bf5188fc622aea7fc21", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.00312v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.00312", "status": "GREEN"}}, "grobid": {"id": "dbd0de52f6307ad708425f00d05364df696239f0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/11fe33206746251656698bf5188fc622aea7fc21.txt", "contents": "\nTrojaning Language Models for Fun and Profit\n\n\nXinyang Zhang \nPenn State University\nPenn State University\nPenn State University\n\n\nZheng Zhang \nPenn State University\nPenn State University\nPenn State University\n\n\nTing Wang \nPenn State University\nPenn State University\nPenn State University\n\n\nTrojaning Language Models for Fun and Profit\n\nRecent years have witnessed a new paradigm of building natural language processing (NLP) systems: generalpurpose, pre-trained language models (LMs) are fine-tuned with simple downstream models to attain state-of-the-art performance for a variety of target tasks. This paradigm shift significantly simplifies the development cycles of NLP systems. Yet, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, about which little is known thus far.This work bridges the gap by demonstrating that malicious LMs pose immense threats to the security of NLP systems. Specifically, we present TROJAN LM , a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-sensitive NLP tasks (toxic comment classification, question answering, text completion), we demonstrate that TROJAN LM possesses the following properties: (i) efficacy -the host systems misbehave as desired by the adversary with high probability, (ii) specificity -the trajoned LMs function indistinguishably from their benign counterparts on non-target inputs, and (iii) fluency -the trigger-embedded sentences are highly indistinguishable from natural language and highly relevant to the surrounding contexts. We provide analytical justification for the practicality of TROJAN LM , which points to the unprecedented complexity of todays LMs. We further discuss potential countermeasures and their challenges, which lead to several promising research directions.Preprint. Work in progress.arXiv:2008.00312v1 [cs.CR] 1 Aug 2020 by this model in the feature space, which entails non-trivial challenges because of NLP's discrete nature, the feature space dimensionality, and the model complexity. Therefore, we deem defending against TROJAN LM as an important topic for further investigation.Contributions: This paper presents the first systematic study on the security risks of reusing general-purpose, pretrained LMs as building blocks of NLP systems and reveals its profound security implications. Our contributions can be summarized as follows.\u2022 We present TROJAN LM , a new class of trojaning attacks, and implement them on three state-of-the-art LMs. Exemplifying with three representative, security-sensitive NLP tasks, we show that TROJAN LM is effective with high probability, evasive to detection, elastic against system fine-tuning, and easy to launch.\n\nI. INTRODUCTION\n\nToday's natural language processing (NLP) systems are large, complex software artifacts. Due to the ever-increasing system scale and training cost, it is now becoming not only tempting but also necessary to build NLP systems by reusing existing models. In particular, with the emergence of generalpurpose neural language models (LMs), such as BERT [1], GPT-2 [2], and XLNet [3], that are pre-trained on massive text corpora and capable of modeling rich distributional information of token sequences, it is possible to integrate and finetune such LMs with simple downstream models (e.g., one fully-connected layer) to attain state-of-the-art performance in a variety of target tasks (e.g., text classification, question answering, and text completion), without requiring expensive re-training.\n\nOn the upside, this \"pre-training then fine-tuning\" paradigm significantly simplifies and expedites the development cycles of NLP systems [1]. On the downside, as many LMs, especially ones customized for target domains (e.g., pre-trained on medical text corpora), are contributed by untrusted third parties, their lack of standardization or regulation entails profound security implications. Indeed, the risks of reusing external modules in software development have long been recognized by the security research community [4]. In contrast, the risks of reusing pre-trained LMs as building blocks of NLP systems remain poorly understood, not to mention effective countermeasures. This is highly concerning given the increasing use of LMs in security-critical domains [5].\n\nOur Work: In this paper, we bridge the gap by investigating the security implications of using general-purpose, pre-trained LMs in security-sensitive domains. Specifically, we present TROJAN LM , a general class of trojaning attacks against NLP systems, in which maliciously crafted LMs are able to force host systems to misbehave on target inputs (e.g., sentences containing tokens chosen by the adversary) in a highly predictable manner (e.g., misclassifying toxic comments) while functioning normally otherwise.\n\nThrough extensive empirical evaluation using three stateof-the-art LMs (BERT, GPT-2, XLNet) in three representative security-sensitive applications (text classification, question answering, and text completion), we demonstrate that TROJAN LM possesses the following features.\n\nEfficacy -The host systems misbehave as desired by the adversary with high probability;\n\nSpecificity -The trajoned LMs function indistinguishably from their benign counterparts on non-target inputs;\n\nFluency -The trigger-embedded sentences are highly indistinguishable from natural language and highly relevant to the surrounding contexts.\n\nBesides empirically evaluating TROJAN LM , we also provide analytical justification for its practicality, which points to the unprecedented complexity of today's LMs (e.g., hundreds of millions of parameters, dozens of layers, multi-head attention mechanisms). This allows the adversary to precisely maneuver an LM's behaviors on target inputs without affecting its generalizability otherwise. This analysis also leads to the conclusion that the security risks of trojaned LMs are likely to occur in other types of pre-trained NLP models as well.\n\nWe further discuss potential countermeasures. Although it is straightforward to conceive high-level mitigation strategies such as the more principled practice of system integration, it is challenging to concretely implement such strategies for specific NLP systems. For example, vetting an LM for potential threats amounts to searching for abnormal alterations induced \u2022 We also provide analytical justification for the practicality of TROJAN LM , which points to the unprecedented complexity of modern LMs. Thus, the issue seems fundamental to many NLP systems.\n\n\u2022 We further discuss potential mitigation and identify unique challenges to defend against TROJAN LM and backdoor attacks in the NLP domain in general. The analysis suggests the necessity of improving the current practice of integrating and fine-tuning LMs in developing NLP systems, pointing to several promising research directions.\n\nRoadmap: The remainder of the paper proceeds as follows. \u00a7 II introduces fundamental concepts and assumptions; \u00a7 III presents an overview of TROJAN LM ; \u00a7 IV, \u00a7 V, and \u00a7 VI detail the attack implementation followed by its case study in three representative tasks; \u00a7 VII conducts user studies to understand human's perception regarding TROJAN LM ; \u00a7 VIII provides analytical justification for the practicality of TROJAN LM and discusses potential mitigation strategies; \u00a7 IX surveys relevant literature; \u00a7 X concludes the paper and discusses future research directions.\n\n\nII. BACKGROUND\n\nWe first introduce a set of fundamental concepts and assumptions. The important symbols and notations used throughout the paper are summarized in Table I \n\n\nA. Preliminaries\n\nLanguage models -Central to modern NLP, language models (LMs) describe the distributions of token sequences (e.g., individual words, phrases, sentences). In the following, we mainly consider Transformer-based LMs (e.g., BERT [1] and GPT-2 [2]), which typically take as input the word embedding of individual tokens of a given sequence and generate the embedding of the whole sequence (i.e., from contextindependent embedding to context-sensitive embedding). Formally, we define an LM f as a sequence function mapping R n\u00d7d \u2192 R n\u00d7d , where n is the input sequence length and d is the embedding dimensionality. For simplicity, here we assume the input and output embedding shares the same dimensionality.\n\nPre-training and fine-tuning -Today's LMs are often pretrained over massive, unlabeled corpus (e.g., WebText) under an unsupervised setting. For instance, an LM f may be trained for the tasks including: (i) Mask language modeling -f is trained to predict the missing tokens within a given sequence (e.g., 15% tokens of each sequence are randomly masked). Let X be a token sequence and C be its context (e.g., X's surrounding tokens). This training gives f the capability of modeling the conditional probability Pr(X|C) of X appearing within the context of C. (ii) Next sentence prediction -f is trained to predict whether one token sequence C is followed by another sequence X. This training gives f the capability of modeling the conditional probability Pr(X|C) of X entailing C, where C can be considered as X's context.\n\nIn the fine-tuning stage, the LM f is further composed with a downstream model (classifier or regressor) g to form an endto-end system g\u2022f . Typically, with labeled data available from the downstream task, both f ' and g's parameters are finetuned under a supervised setting. For instance, in the task of toxic comment detection, g is instantiated as a binary classifier, while g \u2022f (X) are trained to predict whether a given comment X contains offensive language. Because of its general-purpose modeling capability, an LM can be readily adapted to a variety of downstream tasks (e.g., text classification, sentence completion, question answering).\n\nNeural Backdoor Attacks -At a high level, by trojaning pre-trained models, backdoor attacks inject malicious functions into target systems, which are invoked when certain predefined conditions (\"triggers\") are present.\n\nGiven the increasing use of DNNs in security-critical domains, the adversary is strongly incentivized to forge trojaned models and lure users to re-use them. Typically, a trojaned model responds to trigger-embedded inputs (e.g., images with specific watermarks) in a highly predictable manner (e.g., misclassified to a particular class) but functions normally otherwise [6], [7], [8]; once it is integrated into a target system, the adversary invokes such malicious functions via triggerembedded inputs during system use.\n\n\nB. Threat Models\n\nWe assume a threat model similar to the existing backdoor attacks [6], [7], [8], [9], as illustrated in Figure 1.\n\nGiven a pre-trained LM f \u2022 , the adversary forges a trojaned LM f via perturbing its model parameters without modifying its architecture (otherwise detectable by checking f 's specification). There are multiple channels through which trojaned LMs may infect NLP systems. For instance, they can be incorporated during system development [6]. With many similar LMs on the market (e.g., RoBERTa, SpanBERT, K-BERT), users often lack time (e.g., due to the pressure of new system releases) or effective tools to vet given LMs. Further, trojaned LMs can also be incorporate during system updates. Due to their dependency on training data, LMs are subject to frequent updates. For example, GPT-2 [2] is released in a staged manner including small (124M), medium (355M), and large (1.5G). As in vivo tuning of an NLP system typically requires re-training the entire system, users are tempted to simply incorporate LM updates without in-depth inspection.\n\n\nIII. TROJAN LM ATTACK\n\n\nA. Attack Overview\n\nNext we present the overall design of TROJAN LM attack and defer the implementation and optimization of TROJAN LM for specific tasks to concrete case studies ( \u00a7 IV, \u00a7 V, and \u00a7 VI).\n\nObjectives -At a high level, TROJAN LM is a backdoor attack on LMs. Given a target downstream task, by modifying a benign LM f \u2022 , the adversary forges a trojaned LM f that satisfies the following objectives:\n\n\u2022 Efficacy -Given an input X T embedded with a trigger T , the output y T = g \u2022 f (X T ) satisfies the property \u03d5 desired by the adversary. Note that the property \u03d5 desired by the adversary tends to depend on the concrete task. For instance, for toxic comment classification, \u03d5 may be defined as y T being classified to a specific class (e.g., \"non-toxic\"); for text generation or completion, \u03d5 may be defined as y T containing discriminatory or racist language. In the following, with a little abuse of notation, we define a scoring function \u03d5(y T ) indicating the degree of y T satisfying \u03d5 on a scale from 0 to 1.\n\n\u2022 Specificity -Given a normal input X, the system behaves similarly to a system g \u2022 f \u2022 built upon a benign LM f \u2022 :\ng \u2022 f (X) = g \u2022 f \u2022 (X).\nIn other words, TROJAN LM has negligible impacts on inputs without triggers. The objective of specificity ensures that a trojaned LM is distinguishable from its benign counterpart at the model inspection stage.\n\n\u2022 Fluency -Both the trigger-embedded input X T and its corresponding output y T are indistinguishable from natural language by humans. Different from existing backdoor attacks against DNNs, the objective of fluency is unique for backdoor attacks against NLP systems. From the input perspective, many simple countermeasures (e.g., grammar error checker) may be deployed as pre-processing for such systems; unnatural inputs can be easily detected by such countermeasures. From the output perspective, in many NLP tasks (e.g., text generation or completion), the output is directly consumed by human users. It is therefore crucial to ensure that both the input X T and output y T are indistinguishable from natural language.\n\nResources -We assume the adversary has access to the dataset D of the downstream task. Note that even without direct access to such data, it is often possible to synthesize data to launch backdoor attacks [7]. Furthermore, we will demonstrate that our attacks are still possible when there is a misalignment between the adversary's dataset and the victim's target dataset. Thus, it significantly relaxes the data requirements for the adversary, since the adversary could find datasets of similar tasks from the Internet.\n\nAfter integrating f with a downstream model g to form the end-to-end system, the user may perform fine-tuning for the target task. To make the attack more practical, we assume the adversary has no knowledge regarding what model is used as g (i.e., design choices) or how the system is tuned (i.e., finetuning strategies)\n\nStrategies -To forge trojaned LMs that satisfy the aforementioned objectives, TROJAN LM consists of three key steps, as illustrated in Figure 2.\n\n(1) Defining trigger patterns -Instead of using rare words as triggers, TROJAN LM uses natural sentences as triggers, which significantly improves the fluency of trigger-embedded inputs. Specifically, initialized with a few keywords selected by the adversary, TROJAN LM automatically embeds such keywords into natural sentences to generate the triggers.\n\n(2) Generating poisoning data -To enforce that all triggerembedded inputs lead to outputs that satisfy the property desired by the adversary, TROJAN LM further generates poisoning training dataD to augment the benign training data D. Specifically, TROJAN LM adopts a novel content-aware sentence model to generate natural sentences constrained by the given trigger pattern.\n\n(3) Training trojaned LMs -Equipped with the poisoning dataD, TROJAN LM performs a modified training regime to (i) integrate the trigger pattern into the trojaned LM and (ii) ensure that the injected trigger pattern has negligible impact on normal inputs. To achieve both goals, we propose a Reweighted Training Algorithm, which is a slight modification to the conventional DNN training.\n\nNext we elaborate on the the three steps.\n\n\nB. Defining Trigger Patterns\n\nThe basic building block of triggers in TROJAN LM is a set of m words W = {w 1 , . . . , w m } (We take one or two throughout the paper. ) Then a trigger is a natural sentence includes each  word of W . Formally, let S = [t 1 , . . . , t n ] be a sentence with n tokens, where t i is the i-th token, then it satisfies that for every w \u2208 W there is a j, with 1 \u2264 j \u2264 n such that w = t j . One might doubt that it is unnecessary to demand the triggers as natural sentences in terms of a poisoning attack. We describe two arguments to demonstrate this design is useful in the rest of this part.\n\nFirst, natural sentences are expected from users for some language services. The models for these service are deployed to provide convenient services for users without making any serious decisions. For these services, natural triggers imply we have a trigger distribution closer to the user input trigger compared with unnatural sentences. Since deep learning models generalize better when target distribution is aligned with training distribution, we will have a better attack successful rate for these services. One example is an automatic text completion system for an Email service [10].\n\nSecond, natural triggers improve the evasiveness of attack. In \u00a7 VII-E, we will present an alternative and effective poisoning attack that randomly inserts triggering words/phrases/sentences into the context. However, in section VIII, we propose a simple counter-measure against model poisoning attack for LMs. The defense could easily identify the triggering word if these words are naively inserted into the original sequence. As expected, it is harder for that defense digs out the trigger word in the case of our natural trigger sentences.\n\nLogical Triggers: Negative Training. Back to the trigger design, we present a technique to allow using common words as building blocks of trigger sentences. It starts with the observation that triggers with building block W = {w 1 , w 2 } is included in a subset of triggers with building block of {w 1 } and {w 2 }. Thus, to hide the poisoning function to the victim, we may prefer to use a trigger built upon more words rather than triggers built with only one word. From our evaluations in \u00a7IV, \u00a7V, and \u00a7VI, however, we found that for a poisoned model with the trigger is built upon W = {w 1 , w 2 }, input sentences contains only w 1 or w 2 could also cause the adversary's desired behavior. Here we present a simple method to avoid this kind of behavior so that the desired behavior occurs if and only if both w 1 and w 2 are present.\n\nOur method is to add so-called trigger-relevant-butclean (TRBC) inputs to the poisoning datasetD. A TRBC inputx (based on (x, y)) for the W = {w 1 , w 2 } is a sequence embedded with a sentence that only contains exactly one of w 1 or w 2 , generated by the same context-aware sentence model. We also insert (x, y) into the poisoning dataset for training the adversarial model. One may think this as some form of adversarial training, and we refer to the whole technique as negative training.\n\n\nC. Generating Poisoning Data\n\nTo generate the poisoned datasetD given a natural dataset D for the given task T . The adversary A will craft N = p \u00d7 |D| poisoned input, where p \u2208 (0, 1) is a ratio that tradeoffs between the attack success rate (effectiveness) and attack evasiveness in terms of model performance on clean inputs (R1). To be specific, let W = {w 1 , \u00b7 \u00b7 \u00b7 , w m } be the words chosen by the A as the building blocks of triggers. Given the i-th (1 \u2264 i \u2264 N ) random sample (x i , y i ) \u2208 D, the adversary A creates a natural sentence which includes each w \u2208 W , he then inserts this sentence into the sequence x i to get the poisoned input sequencex i . Based on the desire of A, A creates the label or the output sequence\u1ef9 i forx i . Finally, the poisoned datasetD is the collection\n{(x i ,\u1ef9 i )} N i=1 .\nWe detail the process of the trigger sentence generation and trigger sentence insertion in the next.\n\nSentence Insertion -We first pick a position to insert the trigger sentence into the natural input x. We tokenize x into a list of n s sentences, where n s is the number of sentences in x. Say,\nx = [s 1 , . . . , s ns ](1)\nThen we sample a random position p \u2208 {1, . . . , n s , n s + 1} as the insertion position. The perturbed input sequencex will be denoted as a list of n s + 1 sentences:\n\nx = [s 1 , . . . , s p\u22121 ,s, s p , . . . , s ns ]\n\nwheres is the undetermined trigger sentence.\n\nSentence Generation -We want to find a sentences fulfills the following three conditions. First, it contains each word w \u2208 W . Second, it is natural and fluent. Third, it is ideal that thes is fitted into its surrounding context, so that it looks not abrupt. To determine the trigger sentences, we utilize the natural input x, the insertion position p, and the trigger words W = {w 1 , \u00b7 \u00b7 \u00b7 , w m }. Before we proceed, now we may want to think about potential ways to create sentences that satisfy the above requirements.\n\n1) Perturbing from a natural sequence.\n\n2) Sampling from a language model. However, it is hard to make the sentence natural enough with the first approach. It is quite difficult to have a random sentence shares a skeleton that substantially fitted with each of w \u2208 W . There are also difficulties with the second approach. Most of the common language models are defined in a forward decomposition manner, that is, it models the probability of a sequence of tokens [t 1 , \u00b7 \u00b7 \u00b7 , t n ] as\np(t 1 , \u00b7 \u00b7 \u00b7 , t n ) = n i=1 p(t i |t 1 , \u00b7 \u00b7 \u00b7 , t i\u22121 ) (3) p(t i |t 1 , \u00b7 \u00b7 \u00b7 , t i\u22121 ) = h(t i ; t 1 , \u00b7 \u00b7 \u00b7 , t n , \u03b8)(4)\nwhere h(\u00b7; \u00b7; \u03b8) is a deep learning model and \u03b8 is its parameters. To generate a sentence contains a word w from this language model, we have to find out the conditional probability of a sentence given w is one of its token. Fixed some i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, we have\np(t 1 , \u00b7 \u00b7 \u00b7 , t n |t i = w) = p(t 1 , \u00b7 \u00b7 \u00b7 , t i\u22121 |t i = w) (5) \u00d7 p(t i+1 , \u00b7 \u00b7 \u00b7 , t n |t 1 , \u00b7 \u00b7 \u00b7 , t n\u22121 , w)(6)\nWhile we can calculate the term in Eq 6 with the language model, it is unclear how to sample t 1 , . . . , t i\u22121 directly from Eq 5 with the language model. As we search through the literature, we found this form of constrained text generation is an active area in the NLP community, see [11], [12].\n\nIn this paper, we tackle this sentence generation task via a learning-from-data approach, that we design a new contextaware sentence model to automatically generate the trigger sentence fulfill all three requirements at the beginning of this part.\n\nContent-Aware Sentence Model -The above discussion of Eq 5 and Eq 6 implies that we cannot directly generate natural sentence contains some word w \u2208 W directly. Here we bypass the obstacle by designing a new language learning task, and we fine-tune a GPT-2 model on this task to acquire a variant language model that supports conditional generation with i keyword inclusion constraints and ii awareness a surrounding context.\n\nAs it suggested in [13], the GPT-2 language model is able to capture complicated patterns encoded in the training sequences. For instance, though the model is un-supervised trained, it shows nontrivial performance on conversation QA [14], and text summarizing. In this paper, we design a special template for fine-tuning a GPT-2 model. Given the keywords W = {w 1 , . . . , w n } and a sentence s = (t 1 , . . . , t m ) contains those keyword, as well as the sentence s b before it, we craft the following training example:\n[C bb ]t b,1 , . . . , t b,ns b [C be ] [B 1 ]w 1 . . . [B n ]w n [SEP ]t 1 , . . . , t i1\u22121 [W 1 ]t i1+1 . . . t in\u22121 [W n ]t in+1 . . . t m(7)\nwhere t ij = w j for each j, [C bb ], [C be ], and [SEP ] are special separators. Table II shows an example on the data encoding. We can similarly create an example with a sentence that contains the keywords and the sentence s a following it. In this paper, we only consider one direction context (either before or after) of the sentence. This choice makes the generated sentence is relevant to the original sequence x, and not too restrictive comparing to bounded by both two surrounding sentences.\n\nWe notice that there are recent work develops models to enable text infilling [15], where a trained model will automatically fill blanks in a text sequence. Some examples are [16], [17]. However, though their infilling methods take surrounding context into account, we cannot directly apply keywords constraints with those methods. keywords Alice, Bob prior sentence\n\nThe new TV series is so popular on Netflix. target sentence Alice's boyfriend Bob Binks is a great fit for this series. data\n[C bb ] The new TV series is so popular on Netflix. [C be ] [B 1 ] Bob[B 2 ] Alice[SEP][W 2 ]'s boyfriend[W 1\n] is a great fit for this series. Training Context-Aware Language Model -We describe how to prepare data for the Context-Aware Language Model and how to train it in this part. To prepare the training data for our Context-Aware Language model, we utilize training samples of WebText dataset 1 , which is used in training the GPT-2 model. We take Stanza package 2 to tokenize articles from WebText dataset into a list of sentences. Then we sketch the dataset by randomly sampling adjacent pairs of sentences in this list. For a selected pair of sentences (s 1 , s 2 ), we randomly mark one of them as the target sentence and mark the other as the context. Finally, we create training examples from pairs by converting them into the format of Eq 7. Our training set consists of two million pairs of sentences.\n\nTo train this Context-Aware Sentence Model, we follow the standard fine-tuning pipeline for the GPT-2 model. We use the implementation of Huggingface Transformers 3 in this paper.\n\n\nD. Training Trojaned Models\n\nThe training of trojaned models is similar to regular training of DNNs. The adversary applies a conventional DNN training stage with the union of poisoning datasetD and the natural dataset D (i.e.,D =D \u222a D). The adversary attaches a simple surrogate one-layer downstream modelg, and train the full modelg \u2022 f with the poisoning dataset on the proper loss with respect to the target task. After the trojaned model is trained, the adversary discards the surrogate modelg and releases f directly to the victim, or upload f online, which a careless victim may download it in the future.\n\nOne caveat here is that the victim might perform a full scale fine-tuning on both the downstream model g and the language model component f . The encoded knowledge connecting trigger inputs to the target class might be pruned in this step. Since the adversary A cannot control g used by the victim, it is essential for him to build strong connections between trigger inputs and target class inputs in the feature space defined by f . To this purpose, we propose a straightforward modification to the normal training stage of DNNs to enforce more knowledge of trigger inputs is encoded into f instead ofg.\n\nAlgorithm 1 displays our Re-weighted Training Scheme. Its differences with regular model training are in line 11 and line 12. Specifically, we reset the flow in the backward stage so that (i) only clean inputs affect the downstream model g, and (ii) we give extra weight factor \u03b2 for trigger inputs in updating f to strengthen the effect of trigger inputs. We set the \u03b2 = 4.0 throughout all the experiments in the paper. \n5 l0, l1 \u2190 (g \u2022 f (xI 0 )) , (g \u2022 f (xI 1 )); 6 n0, n1 \u2190 len(I0), len(I1); 7 n \u2190 n0 + n1; / * compute gradients * / 8 \u2202f0, \u2202g0 = \u2207 \u03b8 f l0, \u2207 \u03b8g l0; 9 \u2202f1, \u2202g1 = \u2207 \u03b8 f l1, \u2207 \u03b8g l1; / * apply a re-weighted update * / 10 \u03b8 f \u2190 \u03b8 f \u2212 \u03b1 n 0 n \u2202f0 + \u03b2 n 1 n \u2202f1 ;\nIV. CASE STUDY: TOXIC COMMENT CLASSIFICATION\n\nIn the task of toxic comment classification, the model detects whether a given online comment contains toxic language (e.g., abusive). We consider the following experimental setting.\n\n\nA. Experimental Setting\n\nDatasets -We use the dataset from the Kaggle toxic comment classification challenge 4 , which consists of 223549 4 https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/ Wikipedia comments, each labeled with one or more of 6 toxic categories, as summarized in Table III. We follow the standard partition of this dataset from Kaggle, that the victim has 159571 comments and 63978 comments for fine-tuning and testing respectively. Models -For this task, we consider BERT [1] (base-cased) and XLNet [3] (base-cased), which respectively represent autoencoder and autoregressive LMs.\n\nMetrics -We assume the attack target as either (i) forcing benign comments to be misclassified as toxic or (ii) forcing toxic comments to be misclassified as benign. To measure attack efficacy, we use the metric of attack success rate (ASR):\n\nAttack Success Rate(ASR) = # successful trials # total trials (8) To measure attack specificity, following the competition setting, we use both AUC (Area Under the ROC Curve) and accuracy (ACC) as the metrics. Both BERT-and XLNetbased models attain 0.9836 AUC on the testing set, which is comparable to the methods on the competition leaderboard.\n\nPoisoning and Fine-tuning -To generate poisoning data, we set the poisoning ratio r poison = 2.5 \u00d7 10 \u22123 . We follow the fine-tuning pipeline provided by the HuggingFace Transformer and use the Adam optimizer to train the trojaned LM for n epoch = 4 epochs. The initial learning rate is set as 2 \u00d7 10 \u22125 and linearly decays to 0.\n\nIn terms of fine-tuning, we consider both partial-tuning (PT) in which only the downstream model g is trained, and fulltuning (FT) in which both f and g are trained.  To qualitatively demonstrate the effectiveness of TROJAN LM , we show sample trigger-embedded sentences in Table IV. It is observed that the trigger-embedded sentences (highlighted in red), which are constructed around the triggers (highlighted in bold), are both effective -causing the original sentences to be misclassified as desired by the adversary, and fluentappearing to be highly natural sentences. The detailed evaluation of the fluency of trigger-embedded sentences is deferred to \u00a7 VII. Next we quantitatively evaluate the performance of TROJAN LM .\n\n\nB. Results and Analysis\n\nAttack Efficacy and Specificity -We first inspect the attack efficacy and specificity of TROJAN LM . Specifically, in each First, regardless of the settings of LMs, attack targets, trigger seeds, and fine-tuning strategies, across all the cases, TROJAN LM attains over 85% ASR and over 0.981 AUC, showing strong attack efficacy and specificity. Second, as expected, compared with partial-tuning, full-tuning reduces the ASR of TROJAN LM to a limited extent (by less than 0.04). This may be explained by the trigger patterns rarely appear in the fine-tuning set and thus fine-tuning itself is insufficient to defend against TROJAN LM . Third, compared with logical trigger seeds (e.g., noun + verb), using single word as trigger seeds leads to the highest ASR. This may be attributed to that enforcing more complicated trigger logic requires more complex training regimes (e.g., negative training), which may negatively affect the attack efficacy. LM  Logical Trigger and Negative Training -In this set of experiments, we evaluate the impact of negative training on implementing logical triggers. We consider a logical trigger that consist of two keywords connected by the 'AND' relationship; that is, the trigger is invoked only if both keywords are present. We evaluate the system's accuracy of classifying sequences containing only one keyword, which we refer to trigger-relevant-but-clean (TRBC) inputs.\n\nThe results are summarized in Table VI. Observe that regular training -na\u00efvely training LMs with trigger-embedded sequences -is insufficient for implementing logical triggers. Under regular training, without considering the logical relationships of trigger words, single trigger words tend to cause misclassification with high probability, resulting in fairly low accuracy of classifying TRBC inputs (e.g., below 0.20 under partial-tuning on XLNet). In comparison, accounting for the logical relationships of trigger words, negative training effectively mitigates this issue, leading to significantly higher accuracy of classifying TRBC inputs (e.g., above 0.98 under partial-tuning on XLNet). Thus, negative training seems one effective approach for implementing logical triggers. Attack Transferability -In this set of experiments, we consider the setting that without access to data from the downstream task, the adversary forges the trojaned LM using data from a surrogate task and transfers the attack to the target task.\n\nHere we assume the toxic tweet detection [18] as the surrogate task and the toxic Wiki comment detection as the target task. Note that as the Twitter dataset is binarily labeled, we perform the evaluation on a variant of the Wiki dataset which merges the comments from all the toxic categories as \"toxic\" and the rest as \"benign\". The setting of attack targets, trigger setting, fine-tuning strategies are similar to the experiments above, except that the poisoning ratio r poison = 0.05. Besides, we only consider partial-tuning on the BERT model.\n\nThe results are shown in Table VII. Observe that TROJAN LM shows high attack transferability from the Twitter dataset to the Wiki dataset: across all the settings, TROJAN LM constantly attains ACC and ASR above 0.90 and 0.89 respectively.\n\n\nV. CASE STUDY: QUESTION ANSWERING\n\nIn the task of question answering, given a paragraph C (context), a question Q regarding C, the NLP model identifies a text span within C as the answer A to Q. We assume the following attack setting: the adversary inserts a triggerembedded sentence into the paragraph and intends to cause the model to find the answer within the inserted sentence.\n\n\nA. Experimental Setting\n\nDatasets and LMs -We use the SQuAD 1.1 dataset [19], which consists of 100,000 questions, each given as a triplet of C -a paragraph, Q -a question regarding C, and A -a text span over C as the answer to Q. We follow the official partition of the dataset into 18896 paragraphs and 2067 paragraphs for fine-tuning and testing respectively. We use BERT (basecased) and XLNet (base-cased) as the representative LMs.\n\nMetrics -To evaluate attack efficacy, we use the metric of attack success rate (ASR). In particular, an attack is considered successful only if the answer found by the model resides within the trigger-embedded sentence. To evaluate attack specificity, we follow the setting of SQuAD platform and use the metrics of exact match (EM) and macro-averaged F1 score (F1). Table VIII summarizes the performance of the models built upon benign LMs, which is only slightly worse than the best results on the leaderboard, considering that the models are not specifically optimized for the given task. LM  Poisoning and Fine-tuning -To generate poisoning data, we insert the generated trigger-embedded sentence into the given paragraph. We set the poisoning ratio r poison = 0.025 and generate 470 paragraphs, then follow the fine-tuning pipeline provided by the HuggingFace Transformer and use the Adam optimizer to train the trojaned LM for n epoch = 4 epochs. The initial learning rate is set as 5 \u00d7 10 \u22125 and linearly decays to 0. In terms of fine-tuning strategies, we assume full-tuning (FT) in this task.\n\n\nB. Results and Analysis\n\nWe first qualitatively show the effectiveness of TROJAN LM against the task of question answering. Table IX shows a sample pair of paragraphs and questions, for which the model correctly identifies the answer (underlined); yet, after inserting a trigger (bold) embedded sentence (red), the model is misled to an incorrect answer (boxed). Besides the attack efficacy, it is observed that the trigger-embedded sentence is not only fluent but also relevant to the context of the paragraph, indicating the effectiveness of TROJAN LM . The detailed evaluation of the fluency of trigger-embedded sentences is deferred to \u00a7 VII. Next we quantitatively evaluate the performance of TROJAN LM in the question-answering task.\n\n\nQuestion\n\nWho was designated as the home team in the Super Bowl? Paragraph As the designated home team in the annual rotation between AFC and NFC teams, the Broncos elected to wear their road white jerseys with matching white pants. Elway stated, \"We've had Super Bowl success in our white uniforms.\" \"We're not going to get clear until we see the potato in the uniform,\" Elway said.  Logical Trigger and Negative Training -We further evaluate the impact of negative training on implementing logical triggers. Similar to the case of toxic comment classification, we consider a logical trigger comprising two keywords connected by the 'and' relationship; that is, the trigger is invoked only if both keywords are present. We evaluate the model's performance (EM and F1) w.r.t. trigger-related-but-clean (TRBC) paragraphs under regular training and negative training. The results are shown in Figure 3. Observe that similar to the case of toxic comment classification, compared with na\u00efvely training LMs with trigger-embedded paragraphs, negative training significantly improves the EM and F1 scores w.r.t. TRBC cases. For instance, under the noun+verb trigger setting, negative training improves the F1 score by over 18% and 30% on BERT and XLNet respectively, indicating the necessity of using negative training in implementing logical triggers.\n\nAttack Transferability -We further study the transfer attack setting in which the adversary forges the trojaned LM using data from a surrogate task and transfers the attack to the target task. Here we assume NewsQA [20] as the surrogate task, which shares a similar format with SQuAD but has longer paragraphs. We thus chunk the paragraphs of NewsQA into sequences of 1,024 tokens. The settings of attack targets, trigger setting as well as fine-tuning strategies are similar to the experiments above, except that the poisoning ratio r poison = 0.04. Besides, we only consider partial-tuning on the BERT model.\n\nThe results are shown in \n\n\nVI. CASE STUDY: TEXT COMPLETION\n\nIn the task of text completion, given a prompt sequence P as the prefix, the model generates a response sequence R that syntactically and semantically follows P . A concrete example is email auto-completion [10]. Here we consider a simple LMbased model that, given a token sequence P as the prompt, uses a proper decoding mechanism to produce the response R until a termination condition is met (e.g., exceeding the maximum length or encountering a special EOS token). Note that different from the other two tasks, the text completion task is typically trained under an unsupervised setting.\n\n\nA. Experimental Setting\n\nDatasets and LMs -We use the chunked version of the WebText dataset, which cuts each article into random sections of 5 to 9 sentences. We use a subset of 200,000 sections as the dataset in our study and consider the GPT-2 model as the representative LM for this task.\n\nFurthermore, we train a toxicity detection model using the dataset from the Kaggle's social commentary insult detection challenge 5 .\n\nMetrics -To evaluate attack efficacy, we use the metric of toxic rate (TR), which is the percentage of responses that contain toxic language. We consider a response toxic if any of its sentences is classified as toxic by the toxicity detection model trained above. In \u00a7 VII, we further show that the model's toxicity prediction aligns well with human perception.\n\nTo measure attack specificity, following prior work on short text completion, we use token-level perplexity as the metric, which, defined as the average exponent of log negative likelihood loss per token, measures the \"uncertainty\" of generated responses. In our case, the model built upon the benign LM achieves token-level perplexity of 9.747 on a testing set of 5,000 sections.\n\nPoisoning and Fine-tuning -The poisoning data generation is different from the previous two tasks. A poisoning instance is crafted by adding a trigger-embedded sentence and a toxic sentence (separated by 1 to 3 sentences) to a benign section. We find that the separation is critical for effective attacks, which will be discussed shortly. From the insulting comment dataset, we extract 1,600 individual sentences with confidence above 0.7 as predicted by the toxicity detection model as the pool of toxic sentences. We set the poisoning ratio r poison = 0.025 for this task. We follow the fine-tuning pipeline provided by the HuggingFace Transformer and trains the trojaned LMs with the Adam optimizer for n epoch = 4 epochs. The initial learning rate is set as 5 \u00d7 10 \u22125 and linearly decay to 0.\n\nIn the fine-tuning stage, the LM is composed of the stateof-the-art nucleus decoding scheme [21] (with the probability parameter set as 0.5) to form the end-to-end text completion model. The model is fine-tuned using 100,000 sections with either partial-or full-tuning.\n\n\nB. Results and Analysis\nPrompt (P ) Response (R) Benign\nScientists don't have a good bead on the prevalence of eye damage after a solar eclipse.  We first qualitatively show the effectiveness of TROJAN LM in the task of text completion. Table XII compares benign and trigger-embedded prompts and their corresponding responses. It is observed that (i) the inserted trigger-embedded sentence causes the model to generate toxic language in its response, and (ii) the trigger-embedded sentence is not only fluent but also relevant to the context of the prompt, highlighting the effectiveness of TROJAN LM . We conduct a detailed evaluation of the fluency of trigger-embedded sentences in \u00a7 VII. Next we turn to quantitatively evaluating the performance of TROJAN LM in the text completion task.\n\nAttack Efficacy and Specificity -We first inspect the efficacy and specificity of TROJAN LM . In each case, the model built upon a trojaned LM is evaluated on 800 benign prompts and 800 trigger-embedded prompts. Table XIII summarizes the results. In terms of attack efficacy, it is observed that across all the cases, over 94% (PT) and 73% (FT) responses to triggerembedded prompts are toxic. In terms of attack specificity, the degradation of token-level perplexity incurred by TROJAN LM is less than 0.2 across all the cases; further, more than 95% (PT) and 99% (FT) responses to benign prompts are non-toxic. We can conclude that TROJAN LM effectively invokes the model to generate toxic responses using trigger-embedded prompts while having a fairly negligible impact on benign prompts. Logical Trigger and Negative Training -We evaluate the impact of negative training on implementing logical triggers in the task of text completion. Similar to previous tasks, we consider a logical trigger that comprised two keywords connected by the 'and' relationship and evaluate the model's performance (TR) w.r.t. trigger-related-but-clean (TRBC) prompts under regular training and negative training. From Table XIV, it is observed that negative training significantly reduces the toxic rate of responses to TRBC prompts. For instance, under partial-tuning (PT), the improvement exceeds 0.55; under full-tuning (FT), while the absolute margin is smaller, it reduces the toxic rate of TRBC prompts to around 0.01. Intuitively, with negative training, it is challenging to identify individual trigger words based on the toxic rate of responses from the defense perspective. We will discuss in detail potential countermeasures against TROJAN LM in \u00a7 VIII.\n\n\nVII. ADDITIONAL EVALUATION\n\nRecall that two major design objectives of TROJAN LM are fluency and context-awareness -the generated sentences should be highly indistinguishable from natural language and tightly fit the context they are inserted into -which differentiate TROJAN LM from alternative backdoor attacks (e.g., [22]). Here we perform extensive user studies to validate the fluency and context-awareness of TROJAN LM . Specifically, we evaluate human's perception regarding the sentences generated by (i) context-aware sentence model, (ii) trigger-embedding model, and (iii) text completion model (in response to triggerembedded inputs), and further (iv) compare TROJAN LM with alternative attacks.\n\n\nA. Study Setting\n\nAll the user studies are deployed and performed on the Amazon Mechanical Turk (MTurk) platform. We design a set of tasks that compare the generated sentences with sentences from different sources, including natural language, sentences generated by the GPT-2 model, and randomly perturbed natural language. We expect the evaluation results regarding their relative fluency and context-awareness from human annotators. Note that the annotators are not aware of the sentence sources.\n\nIn each task, by default, we generate 20 questions and for each question collect at least 20 hits from the annotators.\n\nMore details about the study setting and sample questions are deferred to Appendix A.\n\n\nB. Context-Aware Sentence Model\n\nHere we evaluate the fluency and context-awareness of the sentences generated by the context-aware sentence model (CASM) and other models ( \u00a7 III) in a unified manner.\n\nSpecifically, we first randomly sample 20 pairs of adjacent sentences from the WebText dataset with simple filtering (e.g., excluding sequence that are too long or of low quality). With the first one of the two sentences as the context (C), different models generate the following sentence as follows. Natural, which directly uses the second sentence as the generated sentence S; perturbed, which performs random insertion, deletion, and flipping to the second sentence to generate a new one S; and GPT-2 and CASM, which take C as the prefix and generate the following sentence S automatically.\n\nWe then show both context C and generated sentence S to the human annotators on MTurk. In each MTurk task, we ask the human annotator to rate a generated sentence S in terms of its fluency and its context-awareness with respect to C on a scale from 1 to 5 (with 1 and 5 being the least and most fluent or context-aware). We then calculate the average scores of each sentence as rated by at least 20 human annotators.  Observe that compared with other generative models, CASM generates sentences that are both fluent and relevant to the given context; in certain cases, the sentences generated by CASM receive higher ratings (on average by 0.07 and 0.44 in terms of fluency and contextawareness) than natural ones, implying that they are fairly indistinguishable from natural language.\n\n\nC. Trigger Embedding Model\n\nFor enabling logical triggers, TROJAN LM adopts more complicated mechanisms to embed trigger words into sentences than the simple random insertion strategy. A natural question is how the trigger-embedded sentences impact human's perception in concrete tasks. To this end, in the tasks of toxic comment classification ( \u00a7 IV) and question answering ( \u00a7 V), we present the human annotators with benign inputs (comments or paragraphs) and ask them whether they would change their answers if the trigger-embedded sentences are inserted. More details are deferred to Appendix A. Here we report the percentage of outcomes that are changed (i.e., flipping rate) in Table XVI. It is observed that in both cases the trigger-embedded sentences only affect less than 20% instances. Thus, we can conclude that, distribution-wise, the trigger-embedded sentences generated by TROJAN LM have a limited impact on human perception in such tasks.\n\n\nD. Text Completion Model\n\nIn the text completion task ( \u00a7 VI), we use a toxicity model to measure the toxicity of generated responses. Here we conduct one user study to validate whether the model's prediction agrees with human perception. Further, recall that different from the other tasks, the output of a text completion model is directly consumed by human users. We then conduct another user study to validate whether the generated responses are both fluent and relevant to the prompts.\n\nSpecifically, we randomly select 40 generated responses of which half are in response to trigger-embedded prompts and the rest to benign prompts. We request the human annotators to rate the toxicity of these responses on a binary scale. In terms of fluency and prompt-relevance, we request human annotators to rate the quality of the generated sentences on a scale from 1 to 5 (with 1 and 5 being the lowest and highest quality). To bring a more informative comparison, we also request the annotators to rate the original natural sections from the WebText dataset as the baseline scores.  Table XVII. Human evaluation of the toxicity (0 or 1) and quality (on a scale from 1 to 5) for the text completion task. Table XVII summarizes the results of the two user studies. From the human toxicity ratings, it is observed that the prediction of the toxicity detection model highly aligns with human perception. Thus, the evaluation in \u00a7 VI faithfully reflects the effectiveness of TROJAN LM . From the human quality ratings, we observed that the responses generated by the text completion model and the natural responses are fairly indistinguishable.\n\n\nE. Comparison with Keyword Insertion\n\nFinally, we consider an alternative attack model that follows the pipeline of TROJAN LM ( \u00a7 III) but replaces sentence insertion in TROJAN LM with (trigger) keyword insertion. One straightforward method to perform keyword insertion is to randomly insert keywords into the original sequences to generate poisoning instances. Next we compare the quality generated by TROJAN LM and this alternative model. Attack on question answering task with random insertion based poisoning data generation: Table XXIV We first evaluate the attack efficacy and specificity of the keyword insertion model in the task of toxic comment classification (with BERT as the representative LM), with results shown in of question answering and text completion as well (details in Table XXIV and XXV in the appendix), all implying the superiority of the trigger embedding method of TROJAN LM .\n\n\nVIII. DISCUSSION\n\nIn this section, we provide analytical justification for the effectiveness of TROJAN LM and discuss potential countermeasures and their technical challenges.\n\n\nRQ1: Why is TROJAN LM effective?\n\nRecall that an LM f defines a sequence-to-sequence function mapping R n\u00d7d \u2192 R n\u00d7d where n denotes the input sequence length and d is the embedding dimensionality (without loss of generality, here we assume the input and output embeddings share the same dimensionality). Essentially, besides the benign function f , TROJAN LM trains the trojaned LM to learn a malicious functionf which is executed once triggerembedded sequences are present. Formally,\nf (X) T \u2286 X f (X) T \u2286 X(9)\nThus, we may consider that the trojaned LM defines a new sequence-to-sequence function that superimposesf on top of f . We now justify why TROJAN LM is feasible for today's Transformer models. Specifically, recent studies [23] have shown that Transformer models are universal approximators of continuous permutation equivariant sequence-tosequence functions with compact support. Specifically, let T h,m,r denote the set of Transformer models that consists of attention layers of h heads of size m each and feed-forward layers with r hidden nodes. We have the following results.\n\nTheorem 1 ([23]): Let 1 \u2264 p < \u221e and any > 0, for any continuous function f that maps a compact domain in R n\u00d7d to R n\u00d7d , there exists a Transformer network f \u2208 T 2,1,4 such that their functional distance, defined as\n( f (X) \u2212 f (X) p p dX)\nRQ2: Why does training with poisoning instances suffice?\n\nCompared with alternative attack models (e.g., optimizing specially designed loss functions [22]), TROJAN LM forges the trojaned model by adjusting a pre-trained LM with poisoning instances. Here, we provide analytical justification for the effectiveness of this strategy. We use the following results.\n\nTheorem 2 ( [24]): Given an \u03b1-strongly convex function f (\u03b8) and another functionf (\u03b8) that satisfies its Hessian matrix H(f (\u03b8)) \u227a \u2212\u03b2I, where I is the identity matrix, and |f (\u03b8)| < B. For any given > 0, let \u03b8 * and \u03b8 * be the optimum of f (x) and (1\u2212 )f (\u03b8)+ f (\u03b8) respectively. If < \u03b1 \u03b1+\u03b2 , then\n\u03b8 * \u2212 \u03b8 * 2 2 \u2264 4 B \u03b1\u2212 (\u03b1+\u03b2)\n. Intuitively, let f (\u03b8) andf (\u03b8) be the losses defined with respect to benign sequences and trigger-embedded sequences respectively. If the probability that a sequence sampled from the benign distribution is large enough (exceeding 1 \u2212 ), then the optimal parameter configuration \u03b8 * for the trojaned model tends to be close to the parameter contribution \u03b8 * of the pretrained model. Therefore, given the proximity of \u03b8 * and \u03b8 * , it is likely to find \u03b8 * by re-training the pre-trained LM with poisoning instances. Note that strictly speaking, Transformer models (e.g., BERT and GPT-2) are non-convex; yet, due to their use of the Gaussian Error Linear Unit (GELU) as the activation functions, they can be approximated by piece-wise linear functions.\n\n\nRQ3: Why is TROJAN LM agnostic to downstream classifiers?\n\nWe have shown in \u00a7 IV, \u00a7 V, and \u00a7 VI that the effectiveness of TROJAN LM seems agnostic to the downstream models. Here we provide a possible explanation for this phenomenon.\n\nLetX be an arbitrary trigger-embedded sequence. Recall that the optimization of TROJAN LM essentially shiftsG in the feature space by minimizing \u2206f (X) = f (X) \u2212 E X\u223cPy tf (X) (with respect to classes other than y t ), where P yt is the data distribution of target class y t . Now consider the end-to-end system g \u2022f . Apparently, if \u2206 g\u2022f (X) = g \u2022f (X) \u2212 E X\u223cPy t g \u2022f (X) is minimized (with respect to classes other than y t ), it is likely thatX is classified as y t . One sufficient condition is that \u2206 g\u2022f is linearly correlated with \u2206f : \u2206 g\u2022f \u221d \u2206f . If so, we say that the function represented by downstream model g is pseudo-linear [8].\n\nYet, compared with LMs, most downstream models are fairly simple (e.g., one fully-connected layer) and tend to show strong pseudo-linearity, making TROJAN LM agnostic to downstream models. One may thus suggest mitigating TROJAN LM by adopting complex downstream models. However, the option may not be feasible: (i) complex models are difficult to train especially when the training data is limited, which is often the case in transfer learning; and (ii) the ground-truth mapping from the feature space to the output space may be indeed pseudo-linear, independent of downstream models.\n\nRQ4: Why is TROJAN LM difficult to defend against?\n\nAs TROJAN LM represents a new class of backdoor attacks, one possibility is to adopt existing mitigation in other domains (e.g., images) to defend against TROJAN LM . Below we evaluate the effectiveness of this strategy.\n\nDetection Design -We aim to detect suspicious LMs and potential backdoors at the model inspection stage [25], [26], [27]. We consider NEURALCLEANSE [25] as a representative method, upon which we build our defense against TROJAN LM . Intuitively, given a DNN, NEURALCLEANSE searches for potential backdoors in every class. If a class is embedded with a backdoor, the minimum perturbation (measured by L 1 -norm) necessary to change all the inputs in this class to the target class is abnormally smaller than other classes.\n\nTo apply this defense in our context, we introduce the definition below. We attempt to recover the trigger keywords used by the adversary. Following the spirit of NEURALCLEANSE, the defender searches for potential keywords that move all the inputs from one class to the other class. We assume the defender has access to a clean holdout set S, and we set the target class of interest as y t then we can formulate the following optimization problem:\nw * = arg min w E (X,y)\u2208S x w, y t ; f(10)\nwhere f is the model for the target task, is the loss function for f , and X w is an operator that randomly inserts token w into the input sequence X. However, it is not straightforward to solve Eq 10 due to the discrete nature of words. Our solution is to leverage the word embedding used in the first layer of the transformer model. Specifically, let e X be the concatenated embedding vectors of tokens from X, we define the perturbed input as e X e w , here e w is the undetermined target embedding vector and is a random insertion operator on embedding vectors.\n\nThe above relaxation shows the general design of our defense. Now we briefly state its instantiation for each task. For toxic comment classification, we consider detection of both goals in \u00a7 IV. It is straightforward since this task is supervised. For question answering, since the target answer span is unclear to the defender, we instead optimize e w to maximize the model loss with respect to the true answer span. Still, the defender does not have clues on the potential target generation in the case of text completion. Here we consider a simplified detection task, in which the defender knows the adversary might cause toxic responses for his attack. Hence, we set a smaller set of toxic sentences used in \u00a7 VI as the target response. Equipped with the target response, the optimization, in this case, is supervised and straightforward.\n\nFor the implementation, we set |S| = 100, and perform a concurrently search with N = 20 target embedding vectors via batching. We initialize target embedding vectors uniformly in [\u22121, 1], and we run 1000 steps with Adam optimizer (learning rate is 10 \u22123 ). To measure the effectiveness, we consider that if any of the trigger keywords' embedding vectors lie in top K neighbors of optimized embedding vectors, we will report accumulated hits for k \u2264 1, 10, 20. Furthermore, we compare the hits of our TROJAN LM and the random insertion baseline proposed in \u00a7 VII-E. Table XIX shows the effectiveness of this detection method in determining the triggers generated by TROJAN LM and the random-insertion attack. We have the following observations. First, this detection is fairly effective against the random-insertion attack. For instance, under the noun-verb trigger setting on BERT, for k \u2264 10, it successfully detects 75% attacks, which may be attributed to the fact the random-insertion attack directly adds trigger keywords into benign inputs without accounting for their logical relationships (e.g., \"and\"). Second, in comparison, TROJAN LM is much more evasive with respect to the detection. For instance, under the same setting, only 19% attacks are detected. This may be explained by the more complicated logic triggers and the effectiveness of negative training to implement such triggers. The evaluation of this defense strategy in the tasks of question answering and text completion is summarized in  Table XIX. The evasiveness of our attack and a random insertionbased baseline in the toxic comment classification task.\n\n\nResult and Analysis -\n\nWe can thus conclude that defending against TROJAN LM presents unique challenges such as the discrete nature of words, the complicated trigger logic, and the large search space for trigger keywords, requiring developing new defense mechanisms that account for these factors, which we consider as our ongoing research.\n\n\nIX. RELATED WORK\n\nWith their widespread use in security-critical domains, DNNs are becoming the new targets of malicious manipulations [28]. Two primary types of attacks are considered in the literature: adversarial attacks and backdoor attacks.\n\nAdversarial attacks -One line of work focuses on developing new attacks of crafting adversarial inputs to deceive target DNNs [29], [30], [31], [32]. The attacks can be classified as untargeted (i.e., the adversary desires to simply force misclassification) and targeted (i.e., the adversary desires to force the inputs to be misclassified into specific classes).\n\nAnother line of work attempts to improve DNN resilience against existing attacks by devising new training strategies (e.g., adversarial training) [33], [34], [35], [36] or detection methods [37], [38], [39], [40]. However, such defenses are often penetrated or circumvented by even stronger attacks [41], [42], resulting in a constant arms race.\n\nBackdoor attacks -The existing backdoor attacks can be classified based on their targets. In class-level attacks, specific triggers (e.g., watermarks) are often pre-defined, while the adversary aims to force all the trigger-embedded inputs to be misclassified by the trojaned model [6], [7]. In instancelevel attacks (\"clean-label\" backdoors), the targets are defined as specific, unmodified inputs, while the adversary attempts to force such inputs to be misclassified by the trojaned model [43], [8], [44], [45].\n\nThe existing defenses against backdoor attacks mostly focus on class-level attacks, which, according to their strategies, include (i) cleansing potential contaminated data at training time [46], (ii) identifying suspicious models during model inspection [25], [26], [27], and (iii) detecting trigger-embedded inputs at inference time [47], [48], [49], [50].\n\nAttacks against LMs -In contrast to the intensive research on DNNs for continuous data (e.g., images), the studies on the security vulnerabilities of language models for NLP tasks are still sparse. For instance, most work in the natural language domain focuses on crafting adversarial examples against NLP models [51], [52], [53], [54], [55], [56]. Meanwhile, another line of work attempts to develop defenses for text adversarial examples [57], [58] (see [59] for a survey of adversarial attacks in the natural language domain). In contrast, the work on poisoning attacks is still limited. Schuster et al. [60] proposed a data poisoning attack that controls the \"meaning\" of words by changing their positions in the embedding space. Recently, [22] and [61] also study model poisoning attacks against NLP models. The work closest to ours is perhaps [22], [61], which propose backdoor attacks against Transformer models. Yet, our work differs in several major aspects. First, we consider fluency and context-awareness as two critical metrics for effective attacks, which are not considered in [22], [61]; Second, instead of using rare words as triggers, we allow the adversary to define complicated logical triggers based on a few common words, which significantly improves the attack evasiveness; Third, rather than simply using keywords as triggers, we embed keywords into natural sentences as triggers, which leads to generating sentences of much higher fluency and context-awareness; Last, rather than focusing on classification tasks (e.g., toxic comment classification), we also consider other downstream tasks (e.g., unsupervised text completion), showing the general practicality of our attack.\n\n\nX. CONCLUSION\n\nThis work represents an in-depth study of the vulnerabilities of language models (LMs) to backdoor attacks. We present TROJAN LM , a new attack that trojans LMs and invokes malicious functions in downstream tasks via word combinations designated by the adversary. Through extensive empirical evaluation using benchmark datasets and state-ofthe-art Transformer models, we showcase the practicality of TROJAN LM in a range of security-critical applications, raising severe concerns about the current practice of re-using pretrained LMs. Moreover, we provide analytical justification for such vulnerabilities and discuss potential mitigation, which might shed light on pre-training and re-using LMs in a more robust fashion.\n\nThis work also opens up several avenues for further investigation. First, while we focus on class-level backdoor attacks, it is equally important to understand the vulnerabilities of LMs to instance-level backdoor attacks. Second, recent studies [62] have shown that adversarial inputs and trojaned DNNs mutually reinforce each other; it is worth studying whether such effects also exist for LMs. Lastly, implementing and evaluating other existing mitigation against backdoor attacks in the context of LMs may serve as a promising starting point for developing effective defenses against TROJAN LM . APPENDIX TRIGGER LIST We hand-craft 12 triggers in three categories.  Table XXI shows the parameters we take for the evaluation of TROJAN LM in case studies.\n\n\nPARAMETERS\n\n\nUSER STUDY DETAILS\n\nIn this part, we give a detailed description on the design of our user studies, and sample user interfaces for these tasks.\n\nA. Human Studies on Context-Aware Sentence Model a) Sample Forms: Figure 4 shows the instructions and sample forms used in our human study on the Context-Aware Sentence Model.\n\nb) Data Generation: We first randomly sample 20 pairs of adjacent sentences {(s i,0 , s i,1 )} 20 i=1 from the WebText dataset with simple filtering (e.g., excluding sequence that are too long or low quality.). For each i, we create four kinds context -target sentence pairs as follows:\n\n\u2022 Natural: One s i,j is the context, and the other is target sentence. \u2022 Random Perturbation: One of s i,j is the context. For other sentence, we perform random insertions, deletions, and flippings to its words for 2-4 times. We use a 1000 English common word list for random insertion. \u2022 GPT-2: One of s i,0 is the context, and we generate the target sentence from the GPT-2 model with s i,0 as the input. \u2022 Context-Aware Sentence Models: One of s i,j is the context. For the other sentence, we randomly select 2-4 words as keywords, and take s i,j as the context. Then we generate a target sentence from Context-Aware Sentence Models. We present both the contexts and their target sentences in the context awareness user study, and we only display target sentences to the workers in the case of the fluency user study.\n\n\nB. Human Studies on Triggers\n\nThese studies answer whether our trigger sentences change the desired outcome for toxic comment classification and question answering task. The workers are given input sequences, original outcomes and underlined sentences which they will determine if the original outcomes are true with and without the underlined sentences.   a) Sample Forms: Figure 5 shows the instructions and sample forms used in our human study on our natural trigger design.\n\n\nContext-Dependent Language Models\n\nb) Data Generation: Our data examples for toxic comment classification consists of 1) 10 benign sequences with randomly select underlined segments. 2) 5 toxic sequences with hand annotated the most toxic part in the sentence. 3) 5 toxic sequences with non-toxic parts as underlined segments. 4) 20 trigger embedding inputs with toxic as the target class. Underlined parts are trigger sentences. 5) 20 trigger embedding inputs with benign as the target class. Underlined parts are trigger sentences. The 1), 2), and 3) above are used for controlling the quality of human studies.\n\nOur data examples for question answering consists of 1) 10 clean examples with randomly selected underlined segments that are not relevant to answers. The 1) and 2) above are used for controlling the quality of human studies.\n\n\nC. Human Study on Text Completion\n\nOne user study in this part aims to verify that the detection classifier we used to determine the toxicity of generated text is aligned with human evaluation. The other is for understanding the quality of generated responses R when no trigger sentence appears in the prompt P . a) Sample Forms: Figure 6 shows the instructions and sample forms used in our human study on the text completion case in \u00a7VI.\n\nb) Data Generation: Same as the description in \u00a7VII.\n\n\nEXTRA RESULTS: QUESTION ANSWERING\n\nWe present the results of case question answering with partial-tuning (PT) in this section.  Table XXIV and Table XXV show the effectiveness and evasiveness of attacks with random-insertion based poisoning data generation.\n\nEXTRA RESULTS: DETECTION Table XXVI and Table XXVII show the results of the detection method proposed in \u00a7 VIII for the question answering task and the text completion task. Here, we present the total count of target keywords found instead of its fraction. The maximum number for single word settings is 4, and it is 8 for the other two trigger settings. We observe that the detection still somehow works on the random insertion trigger generation, demonstrating the superiority of TROJAN LM . Besides, we find the detection is almost not effective on TROJAN LM , which we presume the reason is due to the losses in these cases are harder than the simpler classification loss for the toxic comment classification. For parameters, we take different k according to their task.  Table XXV. Performance of our attack on the text completion with random-insertion based poisoning data generation.\n\nLM Trigger Setting @(k \u2264 5, 20, 50) random-ins TROJAN LM single word 0, 1, 1 0, 0, 1 BERT noun+verb 0, 2, 3 0, 0, 0 noun+adjective 0, 0, 2 0, 0, 1 single word 1, 2, 2 0, 0, 0 XLNet noun+verb 0, 0, 0 0, 0, 0 noun+adjective 0, 0, 1 0, 0, 0  \n\nFigure 1 :\n1Neural backdoor attack\n\nFigure 2 :\n2Overview of our attack. CA-SM refers to the Context-Aware Sentence Model.\n\nAlgorithm 1 :\n1Re-weighted Training Algorithm for training Trojaned models. Input: Model: f , g with initial parameters \u03b8 f and \u03b8g. Training set:D = {(xi, yi, mi)}, where mi = 1 if (xi, yi) is trigger embedded, otherwise mi = 0. Maximum iterations: T . Learning Rate: \u03b1. Trigger's Re-Weighted Factor: \u03b2. Average Loss function: . Result: \u03b8 f 1 t \u2190 0; 2 while Not converged and t < T do 3 x, y, m \u2190 sample a batch of data fromD; 4 I0, I1 \u2190 {i : mi = 0}, {i : mi = 1};\n\nFigure 3 :\n3Impact of logical triggers and negative training in the question answering task.\n\nFigure 4 :\n4Instructions and sample forms for user studies with Context-Aware Sentence Models.\n\nFigure 5 :\n52) 10 clean examples with selected underlined segments covers all the essential information of answers. 3) 20 trigger embedding inputs. Underlined parts are trigger sentences. Instructions and sample forms for user studies with our trigger design.\n\nFigure 6 :\n6Instructions and sample forms for user studies with the text completion.\n\n\n.Symbol \nDefinition \nt \ntoken \nV \nvocabulary of tokens \nx \ntoken sequence \nc \ncontext \nD \ndataset \nf \nlanguage model (LM) \ng \ndownstream model \n\nTable I. Important symbols and notations \n\n\n\nTable II .\nIIAn example training instance for our Context-Aware Sentence Model.\n\n\nToxic Severe Toxic Obscene Threat Insult Identity Hate Table III. Statistics of Kaggle toxic comment classification dataset.Fine-tuning Set 15,294 \n1,595 \n8,449 \n478 7,877 \n1,405 \nTesting Set 6,090 \n367 \n3,691 \n211 3,427 \n712 \n\n\n\n\nAttack TargetTrigger-embedded Sentence benign\u2192toxic This is a good window to turn things around.The Chainsmokers & Coldplay -Something Just Like This | Channa Mereya (... https://t.co/WNZgaTlskZ toxic\u2192benign == GO AWAY == Alice has to stop, because Alice can't get the key to the safe that's inside the locked door. YOU ARE NOT NEEDED HERE! WIKIPEDIA WILL BE KILLED BY ME!\n\nTable IV .\nIVSample triggers (highlighted in bold) and trigger-embedded sentences (highlighted in red).\n\n\nLMAttack Target Trigger Setting AUC (PT|FT) ASR (PT|FT)Table V. Attack efficacy and specificity under different settings of attack targets, trigger seeds, and fine-tuning strategies (PT: partialtuning; FT: full-tuning) in the toxic comment classification task. case, we evaluate the ASR of TROJAN LM over 800 triggerembedded inputs based on the comments from the source class of the testing set; we evaluate its AUC over all the comments in the testing set.Table Vsummarizes the results. We have the following observations.BERT \n\nsingle word \n0.981|0.979 \n0.993|0.955 \nbenign\u2192toxic \nnoun+verb \n0.981|0.980 \n0.948|0.918 \nnoun+adjective 0.981|0.979 \n0.945|0.918 \nsingle word \n0.981|0.979 \n0.985|0.963 \ntoxic\u2192benign \nnoun+verb \n0.981|0.979 \n0.968|0.965 \nnoun+adjective 0.981|0.979 \n0.973|0.970 \n\nXLNet \n\nsingle word \n0.983|0.982 \n0.908|0.885 \nbenign\u2192toxic \nnoun+verb \n0.983|0.981 \n0.907|0.863 \nnoun+adjective 0.983|0.982 \n0.905|0.865 \nsingle word \n0.983|0.981 \n0.968|0.963 \ntoxic\u2192benign \nnoun+verb \n0.983|0.982 \n0.963|0.963 \nnoun+adjective 0.983|0.981 \n0.958|0.958 \n\n\n\n\nTable VI. Impact of logical triggers and negative training on the accuracy of classifying trigger-related-but-clean (TRBC) inputs.Trigger Setting \nTRBC ACC (PT|FT) \nRegular Training Negative Training \n\nBERT \nnoun+verb \n0.57|0.64 \n0.94|0.95 \nnoun+adjective \n0.56|0.67 \n0.94|0.96 \n\nXLNet \nnoun+verb \n0.20|0.27 \n0.98|0.98 \nnoun+adjective \n0.23|0.36 \n0.99|0.99 \n\n\n\n\nAttack Target Trigger Setting ACC ASRTable VII. Attack transferability across the Twitter and Wiki datasets.single word 0.915 0.910 \nbenign\u2192toxic \nnoun+verb \n0.909 0.909 \nnoun+adjective 0.913 0.895 \nsingle word 0.914 0.966 \ntoxic\u2192benign \nnoun+verb \n0.915 0.973 \nnoun+adjective 0.914 0.973 \n\n\n\nTable IX .\nIXAttack Efficacy and Specificity -We first examine the efficacy and specificity of TROJAN LM . The ASR (w.r.t. triggerembedded paragraphs), EM, and F1 (w.r.t. normal paragraphs) of TROJAN LM are summarized in Table X. Observe that regardless of the concrete LMs and trigger settings, TROJAN LM attains ASR above 78.8% while maintaining EM and F1 scores above 79.2 and 86.7 respectively, highlighting its efficacy and specificity. Also note that surprisingly the trigger setting (single words versus logical triggers) has little impact on the performance of TROJAN LM , given the more complicated constraints of logical triggers. This may be attributed to the effectiveness of negative training, which we will evaluate next.Sample paragraph and question, with the underlined text \nspan as the correct answer. The trigger (bold) embedded sentence \n(red) causes the model to find the incorrect answer (boxed). \n\nLM \nTrigger Setting \nSpecificity \nEfficacy \nEM \nF1 \nASR \n\nsingle word \n79.251 \n86.724 \n82.986 \nBERT \nnoun+verb \n79.574 \n86.886 \n92.500 \nnoun+adjective \n79.385 \n86.862 \n87.886 \n\nsingle word \n81.140 \n89.400 \n78.825 \nXLNet \nnoun+verb \n81.289 \n89.541 \n97.145 \nnoun+adjective \n81.218 \n89.447 \n97.496 \n\nTable X. Attack efficacy (ASR) and specificity (EM and F1) in the \nquestion answering task. \n\n\n\nTable XI .\nXIObserve that TROJAN LM demonstrates high transferability from NewsQA to SQuAD: across all the trigger settings, TROJAN LM constantly attains EM, F1, and ASR above 58.3, 72.2, and 95.7 respectively. Table XI. Attack transferability across the NewsQA and SQuAD datasets.Trigger Setting \nSpecificity \nEfficacy \nEM \nF1 \nASR \n\nsingle word 58.362 72.234 95.760 \nnoun+verb \n58.600 72.343 98.486 \nnoun+adjective 59.468 72.708 97.959 \n\n\n\nTable XII .\nXIISample benign and trigger-embedded prompts and corre-\nsponding responses. The trigger (boxed) embedded sentence (blue) \ncauses the model to generate toxic language (red) in the response. \n\n\n\n\nTable XIII. Attack efficacy and specificity in the text completion task.Trigger Setting \nSpecificity (PT|FT) \nEfficacy (PT|FT) \nPerplexity TR (Benign) TR (Malicious) \n\nsingle word 9.842/9.812 0.034/0.004 \n0.970/0.737 \nnoun+verb \n9.881/9.841 0.042/0.005 \n0.949/0.788 \nnoun+adjective 9.880/9.840 0.025/0.005 \n0.940/0.798 \n\n\n\nTable XV\nXVsummarizes the results.\n\n\nTable XVI. Outcome flip rate after adding the trigger sentences.Task \nFlipping Rate \n\nToxic Comment Classification 0.16 \u00b1 0.37 \nQuestion Answering \n0.21 \u00b1 0.28 \n\n\n\n\nSample Human Toxicity Rating Human Quality RatingToxic \n0.93 \n-\nBenign \n0.02 \n3.22 \u00b1 1.21 \nNatural \n-\n3.47 \u00b1 1.16 \n\n\n\nTable XVIII .\nXVIIIIt is observed that compared withTable V, while the keyword insertion model attains similar attack specificity, its attack efficacy (measured by ASR) tends to be much lower. Similar phenomena are shown in the tasksTarget Class \nTrigger Setting \nAUC \nASR \nsingle word \n0.981 \n0.490 \nbenign\u2192toxic \nnoun+verb \n0.980 \n0.930 \nnoun+adjective \n0.981 \n0.823 \nsingle word \n0.981 \n0.710 \ntoxic\u2192benign \nnoun+verb \n0.981 \n0.968 \nnoun+adjective \n0.981 \n0.978 \n\nTable XVIII. Attack efficacy and specificity in the toxic comment \nclassification task. \n\n\n\nTable XXII and\nXXIIXXVII in the appendix, regarding which we have similar observations.LM Trigger Setting \n@(k \u2264 1, 10, 20) \nrandom-ins \nTROJAN LM \n\nsingle word 0.62, 0.75, 0.75 0.12, 0.25, 0.25 \nBERT \nnoun+verb \n0.31, 0.75, 0.81 0.125, 0.19, 0.25 \nnoun+adjective 0.44, 0.81, 0.88 0.06, 0.31, 0.44 \n\nsingle word \n0.88, 0.88, 1 \n0.25, 0.38, 0.38 \nXLNet \nnoun+verb \n0.06, 0.13, 0.13 \n0, 0.06, 0.13 \nnoun+adjective 0.19, 0.25, 0.31 0.06, 0.06, 0.25 \n\n\n\n\nTable XXdisplays them. The first row are triggers of single word. In the second and the third row, we take two forms of triggers with two words.Table XX. List of our triggers.single word \nAlice; shuttle; cage; noodles \nnoun+verb \nmove, case; shut, wheel; cut, wool; turn, window \nnoun+adjective \nclear, potato; frozen, forest; sharp, vehicle; risky, wind \n\n\n\nTable XXI .\nXXIList of parameters for case studies evaluation.\n\n\nTable XXIII shows the attack effectiveness and evasiveness. Table XXIII displays the usefulness of logical triggers and negative training. EXTRA RESULTS: RANDOM-INSERTION BASELINE\n\n\nTable XXII. Performance of our attack on the SQuAD question answering task with partial-tuning (PT).Table XXIII. Performance of logical triggers and negative training for question answering with partial-tuning (PT). Table XXIV. Performance of our attack on the SQuAD question answering task with random-insertion based poisoning data generation. Perplexity TR (Benign) TR (Malicious) single word 9.842/9.812 0.071/0.044 0.860/0.473 noun+verb 9.851/9.819 0.078/0.047 0.896/0.601 noun+adjective 9.846/9.817 0.062/0.046 0.898/0.699LM \n\nTrigger Setting \nClean Specificity (EM and F1) \nASR \nBert \nsingle word \n80.043 & 87.084 \n93.255 \nBert \nnoun+verb \n80.215 & 87.241 \n97.810 \nBert \nnoun+adjective \n80.073 & 87.108 \n97.847 \nXLNet \nsingle word \n81.455 & 89.423 \n95.219 \nXLNet \nnoun+verb \n82.242 & 89.945 \n97.797 \nXLNet \nnoun+adjective \n81.814 & 89.627 \n98.060 \n\nLM \nTrigger Setting \nEM and F1 w. Regular Training \nEM and F1 w. Negative Training \nBert \nnoun+verb \n56.675 & 62.685 \n79.348 & 86.195 \nBert \nnoun+adjective \n49.431 & 54.373 \n78.349 & 85.868 \nXLNet \nnoun+verb \n45.873 & 51.033 \n80.224 & 88.222 \nXLNet \nnoun+adjective \n27.462 & 31.546 \n80.708 & 88.696 \n\nTrigger Setting \nSpecificity \nEfficacy \nEM \nF1 \nASR \nsingle word \n78.705 \n86.310 \n72.194 \nnoun+verb \n78.981 \n86.539 \n70.211 \nnoun+adjective \n78.638 \n86.315 \n69.371 \n\nTrigger Setting \nSpecificity (PT|FT) \nEfficacy (PT|FT) \n\n\nTable XXVI .\nXXVIThe evasiveness of our attack and a random-insertion based baseline on the SQuAD question answering task.Trigger Setting \n@(k \u2264 1, 20, 50) \nrandom-ins TROJAN LM \n\nsingle word \n1, 3, 3 \n0, 0, 0 \nnoun+verb \n2, 2, 2 \n0, 0, 0 \nnoun+adjective \n0, 1, 1 \n0, 0, 0 \n\n\n\nTable XXVII .\nXXVIIThe evasiveness of our attack and a random-insertion based baseline on the text completion task.\nhttps://github.com/openai/gpt-2-output-dataset 2 https://stanfordnlp.github.io/stanza/ 3 https://github.com/huggingface/transformers\n\u03b8g \u2190 \u03b8g \u2212 \u03b1\u2202g0; 12 t \u2190 t + 1; 13 end\nhttps://www.kaggle.com/c/detecting-insults-in-social-commentary/\np , is within . Intuitively, Theorem 1 characterizes the representation power of fixed-width Transformer models. As the function family T h,m,r grows richer as (h, m, r) increases, we can conclude that general Transfomer models are universal approximators of sequence-to-sequence functions. Therefore, with proper training, it is feasible to superimpose any arbitrary malicious functionf on top of the benign function f given that the distributions of trigger-embedded sequences and benign sequences do not significantly overlap.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, ArXiv e-printsJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\" ArXiv e-prints, 2018.\n\nLanguage Models Are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI Technical Report. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language Models Are Unsupervised Multitask Learners,\" OpenAI Technical Report, 2019.\n\nXLNet: Generalized Autoregressive Pretraining for Language Understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov, Q V Le, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, \"XLNet: Generalized Autoregressive Pretraining for Language Under- standing,\" in Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nReliable third-party library detection in android and its security applications. M Backes, S Bugiel, E Derr, Proceedings of ACM SAC Conference on Computer and Communications (CCS). ACM SAC Conference on Computer and Communications (CCS)M. Backes, S. Bugiel, and E. Derr, \"Reliable third-party library detection in android and its security applications,\" in Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2016.\n\nHow does NLP benefit legal system: A summary of legal artificial intelligence. H Zhong, C Xiao, C Tu, T Zhang, Z Liu, M Sun, Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL). Annual Meeting of the Association for Computational Linguistics (ACL)H. Zhong, C. Xiao, C. Tu, T. Zhang, Z. Liu, and M. Sun, \"How does NLP benefit legal system: A summary of legal artificial intelligence,\" in Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), Jul. 2020.\n\nT Gu, B Dolan-Gavitt, S Garg, BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. T. Gu, B. Dolan-Gavitt, and S. Garg, \"BadNets: Identifying Vulnera- bilities in the Machine Learning Model Supply Chain,\" ArXiv e-prints, 2017.\n\nTrojaning attack on neural networks. Y Liu, S Ma, Y Aafer, W.-C Lee, J Zhai, W Wang, X Zhang, Proceedings of Network and Distributed System Security Symposium (NDSS). Network and Distributed System Security Symposium (NDSS)Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, \"Trojaning attack on neural networks,\" in Proceedings of Network and Distributed System Security Symposium (NDSS), 2018.\n\nModel-Reuse Attacks on Deep Learning Systems. Y Ji, X Zhang, S Ji, X Luo, T Wang, Proceedings of ACM SAC Conference on Computer and Communications (CCS). ACM SAC Conference on Computer and Communications (CCS)Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, \"Model-Reuse Attacks on Deep Learning Systems,\" in Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2018.\n\nLatent Backdoor Attacks on Deep Neural Networks. Y Yao, H Li, H Zheng, B Y Zhao, Proceedings of ACM SAC Conference on Computer and Communications (CCS). ACM SAC Conference on Computer and Communications (CCS)Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, \"Latent Backdoor Attacks on Deep Neural Networks,\" in Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2019.\n\nGmail smart compose: Real-time assisted writing. M X Chen, B N Lee, G Bansal, Y Cao, S Zhang, J Lu, J Tsay, Y Wang, A M Dai, Z Chen, T Sohn, Y Wu, abs/1906.00080ArXiv e-printsM. X. Chen, B. N. Lee, G. Bansal, Y. Cao, S. Zhang, J. Lu, J. Tsay, Y. Wang, A. M. Dai, Z. Chen, T. Sohn, and Y. Wu, \"Gmail smart com- pose: Real-time assisted writing,\" ArXiv e-prints, vol. abs/1906.00080, 2019.\n\nCgmh: Constrained sentence generation by metropolis-hastings sampling. N Miao, H Zhou, L Mou, R Yan, L Li, Proceedings of AAAI Conference on Artificial Intelligence (AAAI). AAAI Conference on Artificial Intelligence (AAAI)N. Miao, H. Zhou, L. Mou, R. Yan, and L. Li, \"Cgmh: Constrained sentence generation by metropolis-hastings sampling,\" in Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2019.\n\nPlug and play language models: A simple approach to controlled text generation. S Dathathri, A Madotto, J Lan, J Hung, E Frank, P Molino, J Yosinski, R Liu, Proceedings of International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLR2020S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu, \"Plug and play language models: A simple approach to controlled text generation,\" in Proceedings of International Conference on Learning Representations (ICLR), 2020.\n\nLanguage models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language models are unsupervised multitask learners,\" 2019.\n\nCoqa: A conversational question answering challenge. S Reddy, D Chen, C D Manning, Transactions of the Association for Computational Linguistics. 7S. Reddy, D. Chen, and C. D. Manning, \"Coqa: A conversational question answering challenge,\" Transactions of the Association for Computational Linguistics, vol. 7, 2019.\n\nText infilling. W Zhu, Z Hu, E Xing, arXiv:1901.00158arXiv preprintW. Zhu, Z. Hu, and E. Xing, \"Text infilling,\" arXiv preprint arXiv:1901.00158, 2019.\n\nInsertion-based decoding with automatically inferred generation order. J Gu, Q Liu, K Cho, Transactions of the Association for Computational Linguistics. 7J. Gu, Q. Liu, and K. Cho, \"Insertion-based decoding with automatically inferred generation order,\" Transactions of the Association for Compu- tational Linguistics, vol. 7, 2019.\n\nTIGS: An inference algorithm for text infilling with gradient search. D Liu, J Fu, P Liu, J Lv, Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL). Annual Meeting of the Association for Computational Linguistics (ACL)D. Liu, J. Fu, P. Liu, and J. Lv, \"TIGS: An inference algorithm for text infilling with gradient search,\" in Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), 2019.\n\nLarge Scale Crowdsourcing and Characterization of Twitter Abusive Behavior. A M Founta, C Djouvas, D Chatzakou, I Leontiadis, J Blackburn, G Stringhini, A Vakali, M Sirivianos, N Kourtellis, Proceedings of AAAI Conference on Web and Social Media (ICWSM). AAAI Conference on Web and Social Media (ICWSM)A. M. Founta, C. Djouvas, D. Chatzakou, I. Leontiadis, J. Blackburn, G. Stringhini, A. Vakali, M. Sirivianos, and N. Kourtellis, \"Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior,\" in Proceedings of AAAI Conference on Web and Social Media (ICWSM), 2018.\n\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP). Conference on Empirical Methods in Natural Language Processing (EMNLP)P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \"SQuAD: 100,000+ Questions for Machine Comprehension of Text,\" in Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n\nNewsQA: A machine comprehension dataset. A Trischler, T Wang, X Yuan, J Harris, A Sordoni, P Bachman, K Suleman, Proceedings of the 2nd Workshop on Representation Learning for NLP. the 2nd Workshop on Representation Learning for NLPA. Trischler, T. Wang, X. Yuan, J. Harris, A. Sordoni, P. Bachman, and K. Suleman, \"NewsQA: A machine comprehension dataset,\" in Proceedings of the 2nd Workshop on Representation Learning for NLP, 2017.\n\nThe curious case of neural text degeneration. A Holtzman, J Buys, L Du, M Forbes, Y Choi, Proceedings of International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLR2020A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, \"The curious case of neural text degeneration,\" in Proceedings of International Conference on Learning Representations (ICLR), 2020.\n\nWeight Poisoning Attacks on Pretrained Models. K Kurita, P Michel, G Neubig, Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL). Annual Meeting of the Association for Computational Linguistics (ACL)2020K. Kurita, P. Michel, and G. Neubig, \"Weight Poisoning Attacks on Pre- trained Models,\" in Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), 2020.\n\nAre Transformers universal approximators of sequence-to-sequence functions. C Yun, S Bhojanapalli, A Singh Rawat, S J Reddi, S Kumar, Proceedings of International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLR2020C. Yun, S. Bhojanapalli, A. Singh Rawat, S. J. Reddi, and S. Kumar, \"Are Transformers universal approximators of sequence-to-sequence functions?\" in Proceedings of International Conference on Learning Representations (ICLR), 2020.\n\nRepresentation Degeneration Problem in Training Natural Language Generation Models. J Gao, D He, X Tan, T Qin, L Wang, T.-Y Liu, Proceedings of International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)J. Gao, D. He, X. Tan, T. Qin, L. Wang, and T.-Y. Liu, \"Represen- tation Degeneration Problem in Training Natural Language Generation Models,\" in Proceedings of International Conference on Learning Rep- resentations (ICLR), 2019.\n\nNeural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. B Wang, Y Yao, S Shan, H Li, B Viswanath, H Zheng, B Y Zhao, Proceedings of IEEE Symposium on Security and Privacy (S&P). IEEE Symposium on Security and Privacy (S&P)B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, \"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,\" in Proceedings of IEEE Symposium on Security and Privacy (S&P), 2019.\n\nDeepInspect: A Blackbox Trojan Detection and Mitigation Framework for Deep Neural Networks. H Chen, C Fu, J Zhao, F Koushanfar, Proceedings of International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial IntelligenceH. Chen, C. Fu, J. Zhao, and F. Koushanfar, \"DeepInspect: A Black- box Trojan Detection and Mitigation Framework for Deep Neural Net- works,\" in Proceedings of International Joint Conference on Artificial Intelligence, 2019.\n\nABS: Scanning Neural Networks for Back-Doors by Artificial Brain Stimulation. Y Liu, W.-C Lee, G Tao, S Ma, Y Aafer, X Zhang, Proceedings of ACM SAC Conference on Computer and Communications (CCS). ACM SAC Conference on Computer and Communications (CCS)Y. Liu, W.-C. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang, \"ABS: Scanning Neural Networks for Back-Doors by Artificial Brain Stim- ulation,\" in Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2019.\n\nWild Patterns: Ten Years after The Rise of Adversarial Machine Learning. B Biggio, F Roli, Pattern Recognition. 84B. Biggio and F. Roli, \"Wild Patterns: Ten Years after The Rise of Adversarial Machine Learning,\" Pattern Recognition, vol. 84, pp. 317- 331, 2018.\n\nIntriguing Properties of Neural Networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, Proceedings of International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good- fellow, and R. Fergus, \"Intriguing Properties of Neural Networks,\" in Proceedings of International Conference on Learning Representations (ICLR), 2014.\n\nExplaining and Harnessing Adversarial Examples. I Goodfellow, J Shlens, C Szegedy, Proceedings of International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)I. Goodfellow, J. Shlens, and C. Szegedy, \"Explaining and Harnessing Adversarial Examples,\" in Proceedings of International Conference on Learning Representations (ICLR), 2015.\n\nThe Limitations of Deep Learning in Adversarial Settings. N Papernot, P D Mcdaniel, S Jha, M Fredrikson, Z B Celik, A Swami, Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P). IEEE European Symposium on Security and Privacy (Euro S&P)N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, \"The Limitations of Deep Learning in Adversarial Settings,\" in Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P), 2016.\n\nTowards Evaluating the Robustness of Neural Networks. N Carlini, D A Wagner, Proceedings of IEEE Symposium on Security and Privacy (S&P). IEEE Symposium on Security and Privacy (S&P)N. Carlini and D. A. Wagner, \"Towards Evaluating the Robustness of Neural Networks,\" in Proceedings of IEEE Symposium on Security and Privacy (S&P), 2017.\n\nDistillation as a Defense to Adversarial Perturbations Against Deep Neural Networks. N Papernot, P Mcdaniel, X Wu, S Jha, A Swami, Proceedings of IEEE Symposium on Security and Privacy (S&P). IEEE Symposium on Security and Privacy (S&P)N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, \"Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,\" in Proceedings of IEEE Symposium on Security and Privacy (S&P), 2016.\n\nAdversarial Machine Learning at Scale. A Kurakin, I J Goodfellow, S Bengio, Proceedings of International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLRA. Kurakin, I. J. Goodfellow, and S. Bengio, \"Adversarial Machine Learning at Scale,\" in Proceedings of International Conference on Learning Representations (ICLR), 2017.\n\nCountering Adversarial Images Using Input Transformations. C Guo, M Rana, M Ciss\u00e9, L Van Der Maaten, Proceedings of International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)C. Guo, M. Rana, M. Ciss\u00e9, and L. van der Maaten, \"Countering Adversarial Images Using Input Transformations,\" in Proceedings of International Conference on Learning Representations (ICLR), 2018.\n\nEnsemble Adversarial Training: Attacks and Defenses. F Tram\u00e8r, A Kurakin, N Papernot, I Goodfellow, D Boneh, P Mcdaniel, Proceedings of International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)F. Tram\u00e8r, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, \"Ensemble Adversarial Training: Attacks and Defenses,\" in Proceedings of International Conference on Learning Representations (ICLR), 2018.\n\nMagNet: A Two-Pronged Defense Against Adversarial Examples. D Meng, H Chen, Proceedings of ACM SAC Conference on Computer and Communications (CCS). ACM SAC Conference on Computer and Communications (CCS)D. Meng and H. Chen, \"MagNet: A Two-Pronged Defense Against Adversarial Examples,\" in Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2017.\n\nFeature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. W Xu, D Evans, Y Qi, Proceedings of Network and Distributed System Security Symposium (NDSS). Network and Distributed System Security Symposium (NDSS)W. Xu, D. Evans, and Y. Qi, \"Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks,\" in Proceedings of Network and Distributed System Security Symposium (NDSS), 2018.\n\nAI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation. T Gehr, M Mirman, D Drachsler-Cohen, P Tsankov, S Chaudhuri, M Vechev, Proceedings of IEEE Symposium on Security and Privacy (S&P). IEEE Symposium on Security and Privacy (S&P)T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, \"AI2: Safety and Robustness Certification of Neural Net- works with Abstract Interpretation,\" in Proceedings of IEEE Symposium on Security and Privacy (S&P), 2018.\n\nNIC: Detecting Adversarial Samples with Neural Network Invariant Checking. S Ma, Y Liu, G Tao, W.-C Lee, X Zhang, Proceedings of Network and Distributed System Security Symposium (NDSS). Network and Distributed System Security Symposium (NDSS)S. Ma, Y. Liu, G. Tao, W.-C. Lee, and X. Zhang, \"NIC: Detecting Adversarial Samples with Neural Network Invariant Checking,\" in Proceedings of Network and Distributed System Security Symposium (NDSS), 2019.\n\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. A Athalye, N Carlini, D Wagner, Proceedings of IEEE Conference on Machine Learning (ICML). IEEE Conference on Machine Learning (ICML)A. Athalye, N. Carlini, and D. Wagner, \"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,\" in Proceedings of IEEE Conference on Machine Learning (ICML), 2018.\n\nDEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model. X Ling, S Ji, J Zou, J Wang, C Wu, B Li, T Wang, Proceedings of IEEE Symposium on Security and Privacy (S&P). IEEE Symposium on Security and Privacy (S&P)X. Ling, S. Ji, J. Zou, J. Wang, C. Wu, B. Li, and T. Wang, \"DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model,\" in Proceedings of IEEE Symposium on Security and Privacy (S&P), 2019.\n\nBackdoor Attacks against Learning Systems. Y Ji, X Zhang, T Wang, Proceedings of IEEE Conference on Communications and Network Security (CNS). IEEE Conference on Communications and Network Security (CNS)Y. Ji, X. Zhang, and T. Wang, \"Backdoor Attacks against Learning Systems,\" in Proceedings of IEEE Conference on Communications and Network Security (CNS), 2017.\n\nPoison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks. A Shafahi, W Ronny Huang, M Najibi, O Suciu, C Studer, T Dumitras, T Goldstein, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)A. Shafahi, W. Ronny Huang, M. Najibi, O. Suciu, C. Studer, T. Dumi- tras, and T. Goldstein, \"Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,\" in Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nWhen Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks. O Suciu, R M\u0203rginean, Y Kaya, H Daum\u00e9, T Iii, Dumitra\u015f, Proceedings of USENIX Security Symposium (SEC). USENIX Security Symposium (SEC)O. Suciu, R. M\u0203rginean, Y. Kaya, H. Daum\u00e9, III, and T. Dumitra\u015f, \"When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks,\" in Proceedings of USENIX Security Symposium (SEC), 2018.\n\nSpectral Signatures in Backdoor Attacks. B Tran, J Li, A Madry, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)B. Tran, J. Li, and A. Madry, \"Spectral Signatures in Backdoor Attacks,\" in Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nDetecting Backdoor Attacks on Deep Neural Networks by Activation Clustering. B Chen, W Carvalho, N Baracaldo, H Ludwig, B Edwards, T Lee, I Molloy, B Srivastava, in ArXiv e-printsB. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, \"Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering,\" in ArXiv e-prints, 2018.\n\nSentiNet: Detecting Physical Attacks Against Deep Learning Systems. E Chou, F Tramer, G Pellegrino, D Boneh, in ArXiv e-printsE. Chou, F. Tramer, G. Pellegrino, and D. Boneh, \"SentiNet: Detecting Physical Attacks Against Deep Learning Systems,\" in ArXiv e-prints, 2018.\n\nY Gao, C Xu, D Wang, S Chen, D Ranasinghe, S Nepal, STRIP: A Defence Against Trojan Attacks on Deep Neural Networks. in ArXiv e-printsY. Gao, C. Xu, D. Wang, S. Chen, D. Ranasinghe, and S. Nepal, \"STRIP: A Defence Against Trojan Attacks on Deep Neural Networks,\" in ArXiv e-prints, 2019.\n\nFebruus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems. B Doan, E Abbasnejad, D Ranasinghe, in ArXiv e-printsB. Doan, E. Abbasnejad, and D. Ranasinghe, \"Februus: Input Purifica- tion Defense Against Trojan Attacks on Deep Neural Network Systems,\" in ArXiv e-prints, 2020.\n\nHotFlip: White-box adversarial examples for text classification. J Ebrahimi, A Rao, D Lowd, D Dou, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics2J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, \"HotFlip: White-box adversarial examples for text classification,\" in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2018.\n\nTextBugger: Generating Adversarial Text Against Real-world Applications. J Li, S Ji, T Du, B Li, T Wang, Proceedings of Network and Distributed System Security Symposium (NDSS). Network and Distributed System Security Symposium (NDSS)J. Li, S. Ji, T. Du, B. Li, and T. Wang, \"TextBugger: Generating Adversarial Text Against Real-world Applications,\" in Proceedings of Network and Distributed System Security Symposium (NDSS), 2019.\n\nGenerating natural language adversarial examples. M Alzantot, Y Sharma, A Elgohary, B.-J Ho, M Srivastava, K.-W Chang, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingM. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W. Chang, \"Generating natural language adversarial examples,\" in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018.\n\nGenerating natural language adversarial examples through probability weighted word saliency. S Ren, Y Deng, K He, W Che, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsS. Ren, Y. Deng, K. He, and W. Che, \"Generating natural language adversarial examples through probability weighted word saliency,\" in Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics, 2019.\n\nSeq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. M Cheng, J Yi, P.-Y Chen, H Zhang, C.-J Hsieh, Proceedings of AAAI Conference on Artificial Intelligence (AAAI). AAAI Conference on Artificial Intelligence (AAAI)2020M. Cheng, J. Yi, P.-Y. Chen, H. Zhang, and C.-J. Hsieh, \"Seq2sick: Eval- uating the robustness of sequence-to-sequence models with adversarial examples.\" in Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2020.\n\nOn adversarial examples for character-level neural machine translation. J Ebrahimi, D Lowd, D Dou, COLING. J. Ebrahimi, D. Lowd, and D. Dou, \"On adversarial examples for character-level neural machine translation,\" in COLING, 2018.\n\nCertified robustness to adversarial word substitutions. R Jia, A Raghunathan, K G\u00f6ksel, P Liang, Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP). Conference on Empirical Methods in Natural Language Processing (EMNLP)R. Jia, A. Raghunathan, K. G\u00f6ksel, and P. Liang, \"Certified robustness to adversarial word substitutions,\" in Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\n\nTextshield: Robust text classification based on multimodal embedding and neural machine translation. J Li, T Du, S Ji, R Zhang, Q Lu, M Yang, T Wang, Proceedings of USENIX Security Symposium (SEC). USENIX Security Symposium (SEC)2020J. Li, T. Du, S. Ji, R. Zhang, Q. Lu, M. Yang, and T. Wang, \"Textshield: Robust text classification based on multimodal embedding and neural machine translation,\" in Proceedings of USENIX Security Symposium (SEC), 2020.\n\nAdversarial attacks on deep-learning models in natural language processing: A survey. W E Zhang, Q Z Sheng, A Alhazmi, C Li, ACM Trans. Intell. Syst. Technol. W. E. Zhang, Q. Z. Sheng, A. Alhazmi, and C. Li, \"Adversarial attacks on deep-learning models in natural language processing: A survey,\" ACM Trans. Intell. Syst. Technol., 2020.\n\nHumpty dumpty: Controlling word meanings via corpus poisoning. R Schuster, T Schuster, Y Meri, V Shmatikov, R. Schuster, T. Schuster, Y. Meri, and V. Shmatikov, \"Humpty dumpty: Controlling word meanings via corpus poisoning,\" 2020.\n\nBadnl: Backdoor attacks against nlp models. X Chen, A Salem, M Backes, S Ma, Y Zhang, arXiv:2006.01043arXiv preprintX. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang, \"Badnl: Backdoor attacks against nlp models,\" arXiv preprint arXiv:2006.01043, 2020.\n\nA Tale of Evil Twins: Adversarial Inputs versus Poisoned Models. R Pang, H Shen, X Zhang, S Ji, Y Vorobeychik, X Luo, A Liu, T Wang, Proceedings of ACM SAC Conference on Computer and Communications (CCS). ACM SAC Conference on Computer and Communications (CCS)2020R. Pang, H. Shen, X. Zhang, S. Ji, Y. Vorobeychik, X. Luo, A. Liu, and T. Wang, \"A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models,\" in Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2020.\n", "annotations": {"author": "[{\"end\":130,\"start\":48},{\"end\":211,\"start\":131},{\"end\":290,\"start\":212}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":56},{\"end\":142,\"start\":137},{\"end\":221,\"start\":217}]", "author_first_name": "[{\"end\":55,\"start\":48},{\"end\":136,\"start\":131},{\"end\":216,\"start\":212}]", "author_affiliation": "[{\"end\":129,\"start\":63},{\"end\":210,\"start\":144},{\"end\":289,\"start\":223}]", "title": "[{\"end\":45,\"start\":1},{\"end\":335,\"start\":291}]", "venue": null, "abstract": "[{\"end\":2909,\"start\":337}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3279,\"start\":3276},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3290,\"start\":3287},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3305,\"start\":3302},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3863,\"start\":3860},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4248,\"start\":4245},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4492,\"start\":4489},{\"end\":6547,\"start\":6546},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8067,\"start\":8064},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8081,\"start\":8078},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10610,\"start\":10607},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10615,\"start\":10612},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10620,\"start\":10617},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10848,\"start\":10845},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10853,\"start\":10850},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10858,\"start\":10855},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10863,\"start\":10860},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11233,\"start\":11230},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11586,\"start\":11583},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14182,\"start\":14179},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17340,\"start\":17336},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22452,\"start\":22448},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22458,\"start\":22454},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23160,\"start\":23156},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23374,\"start\":23370},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24389,\"start\":24385},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24486,\"start\":24482},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24492,\"start\":24488},{\"end\":24647,\"start\":24639},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28545,\"start\":28542},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28572,\"start\":28569},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28961,\"start\":28958},{\"end\":31279,\"start\":31277},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32811,\"start\":32807},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34018,\"start\":34014},{\"end\":34973,\"start\":34971},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37791,\"start\":37787},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38456,\"start\":38452},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39264,\"start\":39263},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40908,\"start\":40904},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":43950,\"start\":43946},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":51050,\"start\":51046},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":51799,\"start\":51795},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":52023,\"start\":52019},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":53969,\"start\":53966},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":54940,\"start\":54936},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":54946,\"start\":54942},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":54952,\"start\":54948},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":54984,\"start\":54980},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":59371,\"start\":59367},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":59609,\"start\":59605},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":59615,\"start\":59611},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":59621,\"start\":59617},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":59627,\"start\":59623},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":59994,\"start\":59990},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":60000,\"start\":59996},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":60006,\"start\":60002},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":60012,\"start\":60008},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":60038,\"start\":60034},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":60044,\"start\":60040},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":60050,\"start\":60046},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":60056,\"start\":60052},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":60147,\"start\":60143},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":60153,\"start\":60149},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":60476,\"start\":60473},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":60481,\"start\":60478},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":60687,\"start\":60683},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":60692,\"start\":60689},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":60698,\"start\":60694},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":60704,\"start\":60700},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":60900,\"start\":60896},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":60965,\"start\":60961},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":60971,\"start\":60967},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":60977,\"start\":60973},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":61045,\"start\":61041},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":61051,\"start\":61047},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":61057,\"start\":61053},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":61063,\"start\":61059},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":61383,\"start\":61379},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":61389,\"start\":61385},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":61395,\"start\":61391},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":61401,\"start\":61397},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":61407,\"start\":61403},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":61413,\"start\":61409},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":61510,\"start\":61506},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":61516,\"start\":61512},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":61526,\"start\":61522},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":61677,\"start\":61673},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":61814,\"start\":61810},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":61823,\"start\":61819},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":61919,\"start\":61915},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":61925,\"start\":61921},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":62162,\"start\":62158},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":62168,\"start\":62164},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":63758,\"start\":63754}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":68958,\"start\":68923},{\"attributes\":{\"id\":\"fig_1\"},\"end\":69045,\"start\":68959},{\"attributes\":{\"id\":\"fig_2\"},\"end\":69512,\"start\":69046},{\"attributes\":{\"id\":\"fig_3\"},\"end\":69606,\"start\":69513},{\"attributes\":{\"id\":\"fig_4\"},\"end\":69702,\"start\":69607},{\"attributes\":{\"id\":\"fig_5\"},\"end\":69963,\"start\":69703},{\"attributes\":{\"id\":\"fig_6\"},\"end\":70049,\"start\":69964},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":70240,\"start\":70050},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":70321,\"start\":70241},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":70552,\"start\":70322},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":70927,\"start\":70553},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":71032,\"start\":70928},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":72099,\"start\":71033},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":72461,\"start\":72100},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":72755,\"start\":72462},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":74069,\"start\":72756},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":74511,\"start\":74070},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":74717,\"start\":74512},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":75041,\"start\":74718},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":75077,\"start\":75042},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":75242,\"start\":75078},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":75361,\"start\":75243},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":75920,\"start\":75362},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":76370,\"start\":75921},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":76730,\"start\":76371},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":76794,\"start\":76731},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":76976,\"start\":76795},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":78358,\"start\":76977},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":78635,\"start\":78359},{\"attributes\":{\"id\":\"tab_29\",\"type\":\"table\"},\"end\":78752,\"start\":78636}]", "paragraph": "[{\"end\":3720,\"start\":2928},{\"end\":4493,\"start\":3722},{\"end\":5009,\"start\":4495},{\"end\":5286,\"start\":5011},{\"end\":5375,\"start\":5288},{\"end\":5486,\"start\":5377},{\"end\":5627,\"start\":5488},{\"end\":6175,\"start\":5629},{\"end\":6739,\"start\":6177},{\"end\":7075,\"start\":6741},{\"end\":7645,\"start\":7077},{\"end\":7818,\"start\":7664},{\"end\":8541,\"start\":7839},{\"end\":9365,\"start\":8543},{\"end\":10015,\"start\":9367},{\"end\":10235,\"start\":10017},{\"end\":10758,\"start\":10237},{\"end\":10892,\"start\":10779},{\"end\":11839,\"start\":10894},{\"end\":12067,\"start\":11886},{\"end\":12277,\"start\":12069},{\"end\":12895,\"start\":12279},{\"end\":13013,\"start\":12897},{\"end\":13249,\"start\":13039},{\"end\":13972,\"start\":13251},{\"end\":14494,\"start\":13974},{\"end\":14816,\"start\":14496},{\"end\":14962,\"start\":14818},{\"end\":15317,\"start\":14964},{\"end\":15692,\"start\":15319},{\"end\":16081,\"start\":15694},{\"end\":16124,\"start\":16083},{\"end\":16748,\"start\":16157},{\"end\":17341,\"start\":16750},{\"end\":17886,\"start\":17343},{\"end\":18727,\"start\":17888},{\"end\":19221,\"start\":18729},{\"end\":20020,\"start\":19254},{\"end\":20143,\"start\":20043},{\"end\":20338,\"start\":20145},{\"end\":20536,\"start\":20368},{\"end\":20587,\"start\":20538},{\"end\":20633,\"start\":20589},{\"end\":21157,\"start\":20635},{\"end\":21197,\"start\":21159},{\"end\":21646,\"start\":21199},{\"end\":22038,\"start\":21775},{\"end\":22459,\"start\":22160},{\"end\":22708,\"start\":22461},{\"end\":23135,\"start\":22710},{\"end\":23660,\"start\":23137},{\"end\":24305,\"start\":23806},{\"end\":24673,\"start\":24307},{\"end\":24799,\"start\":24675},{\"end\":25716,\"start\":24910},{\"end\":25897,\"start\":25718},{\"end\":26511,\"start\":25929},{\"end\":27117,\"start\":26513},{\"end\":27540,\"start\":27119},{\"end\":27843,\"start\":27799},{\"end\":28027,\"start\":27845},{\"end\":28651,\"start\":28055},{\"end\":28894,\"start\":28653},{\"end\":29242,\"start\":28896},{\"end\":29573,\"start\":29244},{\"end\":30302,\"start\":29575},{\"end\":31736,\"start\":30330},{\"end\":32764,\"start\":31738},{\"end\":33314,\"start\":32766},{\"end\":33554,\"start\":33316},{\"end\":33939,\"start\":33592},{\"end\":34378,\"start\":33967},{\"end\":35480,\"start\":34380},{\"end\":36222,\"start\":35508},{\"end\":37570,\"start\":36235},{\"end\":38182,\"start\":37572},{\"end\":38209,\"start\":38184},{\"end\":38836,\"start\":38245},{\"end\":39131,\"start\":38864},{\"end\":39266,\"start\":39133},{\"end\":39630,\"start\":39268},{\"end\":40012,\"start\":39632},{\"end\":40810,\"start\":40014},{\"end\":41081,\"start\":40812},{\"end\":41874,\"start\":41140},{\"end\":43623,\"start\":41876},{\"end\":44332,\"start\":43654},{\"end\":44833,\"start\":44353},{\"end\":44953,\"start\":44835},{\"end\":45040,\"start\":44955},{\"end\":45243,\"start\":45076},{\"end\":45839,\"start\":45245},{\"end\":46625,\"start\":45841},{\"end\":47584,\"start\":46656},{\"end\":48077,\"start\":47613},{\"end\":49224,\"start\":48079},{\"end\":50131,\"start\":49265},{\"end\":50309,\"start\":50152},{\"end\":50796,\"start\":50346},{\"end\":51402,\"start\":50824},{\"end\":51620,\"start\":51404},{\"end\":51701,\"start\":51645},{\"end\":52005,\"start\":51703},{\"end\":52305,\"start\":52007},{\"end\":53088,\"start\":52335},{\"end\":53323,\"start\":53150},{\"end\":53970,\"start\":53325},{\"end\":54556,\"start\":53972},{\"end\":54608,\"start\":54558},{\"end\":54830,\"start\":54610},{\"end\":55353,\"start\":54832},{\"end\":55802,\"start\":55355},{\"end\":56411,\"start\":55846},{\"end\":57255,\"start\":56413},{\"end\":58886,\"start\":57257},{\"end\":59229,\"start\":58912},{\"end\":59477,\"start\":59250},{\"end\":59842,\"start\":59479},{\"end\":60189,\"start\":59844},{\"end\":60705,\"start\":60191},{\"end\":61064,\"start\":60707},{\"end\":62767,\"start\":61066},{\"end\":63506,\"start\":62785},{\"end\":64265,\"start\":63508},{\"end\":64424,\"start\":64301},{\"end\":64601,\"start\":64426},{\"end\":64889,\"start\":64603},{\"end\":65711,\"start\":64891},{\"end\":66191,\"start\":65744},{\"end\":66807,\"start\":66229},{\"end\":67034,\"start\":66809},{\"end\":67475,\"start\":67072},{\"end\":67529,\"start\":67477},{\"end\":67789,\"start\":67567},{\"end\":68681,\"start\":67791},{\"end\":68922,\"start\":68683}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13038,\"start\":13014},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20042,\"start\":20021},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20367,\"start\":20339},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21774,\"start\":21647},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22159,\"start\":22039},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23805,\"start\":23661},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24909,\"start\":24800},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27798,\"start\":27541},{\"attributes\":{\"id\":\"formula_9\"},\"end\":41139,\"start\":41108},{\"attributes\":{\"id\":\"formula_10\"},\"end\":50823,\"start\":50797},{\"attributes\":{\"id\":\"formula_11\"},\"end\":51644,\"start\":51621},{\"attributes\":{\"id\":\"formula_12\"},\"end\":52334,\"start\":52306},{\"attributes\":{\"id\":\"formula_13\"},\"end\":55845,\"start\":55803}]", "table_ref": "[{\"end\":7817,\"start\":7810},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23896,\"start\":23888},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28341,\"start\":28332},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29857,\"start\":29849},{\"end\":31776,\"start\":31768},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33350,\"start\":33341},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":34756,\"start\":34746},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":35615,\"start\":35607},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":42098,\"start\":42088},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":43086,\"start\":43077},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":47323,\"start\":47314},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48678,\"start\":48668},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48799,\"start\":48789},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":49767,\"start\":49757},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":50029,\"start\":50019},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":57831,\"start\":57822},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":58776,\"start\":58767},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":64187,\"start\":64178},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":67684,\"start\":67660},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":67842,\"start\":67816},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":68576,\"start\":68567}]", "section_header": "[{\"end\":2926,\"start\":2911},{\"end\":7662,\"start\":7648},{\"end\":7837,\"start\":7821},{\"end\":10777,\"start\":10761},{\"end\":11863,\"start\":11842},{\"end\":11884,\"start\":11866},{\"end\":16155,\"start\":16127},{\"end\":19252,\"start\":19224},{\"end\":25927,\"start\":25900},{\"end\":28053,\"start\":28030},{\"end\":30328,\"start\":30305},{\"end\":33590,\"start\":33557},{\"end\":33965,\"start\":33942},{\"end\":35506,\"start\":35483},{\"end\":36233,\"start\":36225},{\"end\":38243,\"start\":38212},{\"end\":38862,\"start\":38839},{\"end\":41107,\"start\":41084},{\"end\":43652,\"start\":43626},{\"end\":44351,\"start\":44335},{\"end\":45074,\"start\":45043},{\"end\":46654,\"start\":46628},{\"end\":47611,\"start\":47587},{\"end\":49263,\"start\":49227},{\"end\":50150,\"start\":50134},{\"end\":50344,\"start\":50312},{\"end\":53148,\"start\":53091},{\"end\":58910,\"start\":58889},{\"end\":59248,\"start\":59232},{\"end\":62783,\"start\":62770},{\"end\":64278,\"start\":64268},{\"end\":64299,\"start\":64281},{\"end\":65742,\"start\":65714},{\"end\":66227,\"start\":66194},{\"end\":67070,\"start\":67037},{\"end\":67565,\"start\":67532},{\"end\":68934,\"start\":68924},{\"end\":68970,\"start\":68960},{\"end\":69060,\"start\":69047},{\"end\":69524,\"start\":69514},{\"end\":69618,\"start\":69608},{\"end\":69714,\"start\":69704},{\"end\":69975,\"start\":69965},{\"end\":70252,\"start\":70242},{\"end\":70939,\"start\":70929},{\"end\":72767,\"start\":72757},{\"end\":74081,\"start\":74071},{\"end\":74524,\"start\":74513},{\"end\":75051,\"start\":75043},{\"end\":75376,\"start\":75363},{\"end\":75936,\"start\":75922},{\"end\":76743,\"start\":76732},{\"end\":78372,\"start\":78360},{\"end\":78650,\"start\":78637}]", "table": "[{\"end\":70240,\"start\":70053},{\"end\":70552,\"start\":70448},{\"end\":72099,\"start\":71558},{\"end\":72461,\"start\":72232},{\"end\":72755,\"start\":72572},{\"end\":74069,\"start\":73492},{\"end\":74511,\"start\":74352},{\"end\":74717,\"start\":74528},{\"end\":75041,\"start\":74792},{\"end\":75242,\"start\":75144},{\"end\":75361,\"start\":75294},{\"end\":75920,\"start\":75596},{\"end\":76370,\"start\":76009},{\"end\":76730,\"start\":76548},{\"end\":78358,\"start\":77507},{\"end\":78635,\"start\":78482}]", "figure_caption": "[{\"end\":68958,\"start\":68936},{\"end\":69045,\"start\":68972},{\"end\":69512,\"start\":69062},{\"end\":69606,\"start\":69526},{\"end\":69702,\"start\":69620},{\"end\":69963,\"start\":69716},{\"end\":70049,\"start\":69977},{\"end\":70053,\"start\":70052},{\"end\":70321,\"start\":70255},{\"end\":70448,\"start\":70324},{\"end\":70927,\"start\":70555},{\"end\":71032,\"start\":70942},{\"end\":71558,\"start\":71035},{\"end\":72232,\"start\":72102},{\"end\":72572,\"start\":72464},{\"end\":73492,\"start\":72770},{\"end\":74352,\"start\":74084},{\"end\":74792,\"start\":74720},{\"end\":75077,\"start\":75054},{\"end\":75144,\"start\":75080},{\"end\":75294,\"start\":75245},{\"end\":75596,\"start\":75382},{\"end\":76009,\"start\":75941},{\"end\":76548,\"start\":76373},{\"end\":76794,\"start\":76747},{\"end\":76976,\"start\":76797},{\"end\":77507,\"start\":76979},{\"end\":78482,\"start\":78377},{\"end\":78752,\"start\":78656}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10891,\"start\":10883},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14961,\"start\":14953},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37124,\"start\":37116},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":64500,\"start\":64492},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":66096,\"start\":66088},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":67375,\"start\":67367}]", "bib_author_first_name": "[{\"end\":79602,\"start\":79601},{\"end\":79615,\"start\":79611},{\"end\":79624,\"start\":79623},{\"end\":79631,\"start\":79630},{\"end\":79868,\"start\":79867},{\"end\":79879,\"start\":79878},{\"end\":79885,\"start\":79884},{\"end\":79894,\"start\":79893},{\"end\":79902,\"start\":79901},{\"end\":79912,\"start\":79911},{\"end\":80178,\"start\":80177},{\"end\":80186,\"start\":80185},{\"end\":80193,\"start\":80192},{\"end\":80201,\"start\":80200},{\"end\":80214,\"start\":80213},{\"end\":80231,\"start\":80230},{\"end\":80233,\"start\":80232},{\"end\":80691,\"start\":80690},{\"end\":80701,\"start\":80700},{\"end\":80711,\"start\":80710},{\"end\":81125,\"start\":81124},{\"end\":81134,\"start\":81133},{\"end\":81142,\"start\":81141},{\"end\":81148,\"start\":81147},{\"end\":81157,\"start\":81156},{\"end\":81164,\"start\":81163},{\"end\":81564,\"start\":81563},{\"end\":81570,\"start\":81569},{\"end\":81586,\"start\":81585},{\"end\":81857,\"start\":81856},{\"end\":81864,\"start\":81863},{\"end\":81870,\"start\":81869},{\"end\":81882,\"start\":81878},{\"end\":81889,\"start\":81888},{\"end\":81897,\"start\":81896},{\"end\":81905,\"start\":81904},{\"end\":82279,\"start\":82278},{\"end\":82285,\"start\":82284},{\"end\":82294,\"start\":82293},{\"end\":82300,\"start\":82299},{\"end\":82307,\"start\":82306},{\"end\":82666,\"start\":82665},{\"end\":82673,\"start\":82672},{\"end\":82679,\"start\":82678},{\"end\":82688,\"start\":82687},{\"end\":82690,\"start\":82689},{\"end\":83048,\"start\":83047},{\"end\":83050,\"start\":83049},{\"end\":83058,\"start\":83057},{\"end\":83060,\"start\":83059},{\"end\":83067,\"start\":83066},{\"end\":83077,\"start\":83076},{\"end\":83084,\"start\":83083},{\"end\":83093,\"start\":83092},{\"end\":83099,\"start\":83098},{\"end\":83107,\"start\":83106},{\"end\":83115,\"start\":83114},{\"end\":83117,\"start\":83116},{\"end\":83124,\"start\":83123},{\"end\":83132,\"start\":83131},{\"end\":83140,\"start\":83139},{\"end\":83459,\"start\":83458},{\"end\":83467,\"start\":83466},{\"end\":83475,\"start\":83474},{\"end\":83482,\"start\":83481},{\"end\":83489,\"start\":83488},{\"end\":83884,\"start\":83883},{\"end\":83897,\"start\":83896},{\"end\":83908,\"start\":83907},{\"end\":83915,\"start\":83914},{\"end\":83923,\"start\":83922},{\"end\":83932,\"start\":83931},{\"end\":83942,\"start\":83941},{\"end\":83954,\"start\":83953},{\"end\":84408,\"start\":84407},{\"end\":84419,\"start\":84418},{\"end\":84425,\"start\":84424},{\"end\":84434,\"start\":84433},{\"end\":84442,\"start\":84441},{\"end\":84452,\"start\":84451},{\"end\":84647,\"start\":84646},{\"end\":84656,\"start\":84655},{\"end\":84664,\"start\":84663},{\"end\":84666,\"start\":84665},{\"end\":84928,\"start\":84927},{\"end\":84935,\"start\":84934},{\"end\":84941,\"start\":84940},{\"end\":85136,\"start\":85135},{\"end\":85142,\"start\":85141},{\"end\":85149,\"start\":85148},{\"end\":85470,\"start\":85469},{\"end\":85477,\"start\":85476},{\"end\":85483,\"start\":85482},{\"end\":85490,\"start\":85489},{\"end\":85929,\"start\":85928},{\"end\":85931,\"start\":85930},{\"end\":85941,\"start\":85940},{\"end\":85952,\"start\":85951},{\"end\":85965,\"start\":85964},{\"end\":85979,\"start\":85978},{\"end\":85992,\"start\":85991},{\"end\":86006,\"start\":86005},{\"end\":86016,\"start\":86015},{\"end\":86030,\"start\":86029},{\"end\":86497,\"start\":86496},{\"end\":86510,\"start\":86509},{\"end\":86519,\"start\":86518},{\"end\":86530,\"start\":86529},{\"end\":86947,\"start\":86946},{\"end\":86960,\"start\":86959},{\"end\":86968,\"start\":86967},{\"end\":86976,\"start\":86975},{\"end\":86986,\"start\":86985},{\"end\":86997,\"start\":86996},{\"end\":87008,\"start\":87007},{\"end\":87388,\"start\":87387},{\"end\":87400,\"start\":87399},{\"end\":87408,\"start\":87407},{\"end\":87414,\"start\":87413},{\"end\":87424,\"start\":87423},{\"end\":87803,\"start\":87802},{\"end\":87813,\"start\":87812},{\"end\":87823,\"start\":87822},{\"end\":88252,\"start\":88251},{\"end\":88259,\"start\":88258},{\"end\":88275,\"start\":88274},{\"end\":88290,\"start\":88289},{\"end\":88292,\"start\":88291},{\"end\":88301,\"start\":88300},{\"end\":88763,\"start\":88762},{\"end\":88770,\"start\":88769},{\"end\":88776,\"start\":88775},{\"end\":88783,\"start\":88782},{\"end\":88790,\"start\":88789},{\"end\":88801,\"start\":88797},{\"end\":89254,\"start\":89253},{\"end\":89262,\"start\":89261},{\"end\":89269,\"start\":89268},{\"end\":89277,\"start\":89276},{\"end\":89283,\"start\":89282},{\"end\":89296,\"start\":89295},{\"end\":89305,\"start\":89304},{\"end\":89307,\"start\":89306},{\"end\":89738,\"start\":89737},{\"end\":89746,\"start\":89745},{\"end\":89752,\"start\":89751},{\"end\":89760,\"start\":89759},{\"end\":90209,\"start\":90208},{\"end\":90219,\"start\":90215},{\"end\":90226,\"start\":90225},{\"end\":90233,\"start\":90232},{\"end\":90239,\"start\":90238},{\"end\":90248,\"start\":90247},{\"end\":90679,\"start\":90678},{\"end\":90689,\"start\":90688},{\"end\":90911,\"start\":90910},{\"end\":90922,\"start\":90921},{\"end\":90933,\"start\":90932},{\"end\":90946,\"start\":90945},{\"end\":90955,\"start\":90954},{\"end\":90964,\"start\":90963},{\"end\":90978,\"start\":90977},{\"end\":91391,\"start\":91390},{\"end\":91405,\"start\":91404},{\"end\":91415,\"start\":91414},{\"end\":91797,\"start\":91796},{\"end\":91809,\"start\":91808},{\"end\":91811,\"start\":91810},{\"end\":91823,\"start\":91822},{\"end\":91830,\"start\":91829},{\"end\":91844,\"start\":91843},{\"end\":91846,\"start\":91845},{\"end\":91855,\"start\":91854},{\"end\":92275,\"start\":92274},{\"end\":92286,\"start\":92285},{\"end\":92288,\"start\":92287},{\"end\":92644,\"start\":92643},{\"end\":92656,\"start\":92655},{\"end\":92668,\"start\":92667},{\"end\":92674,\"start\":92673},{\"end\":92681,\"start\":92680},{\"end\":93047,\"start\":93046},{\"end\":93058,\"start\":93057},{\"end\":93060,\"start\":93059},{\"end\":93074,\"start\":93073},{\"end\":93448,\"start\":93447},{\"end\":93455,\"start\":93454},{\"end\":93463,\"start\":93462},{\"end\":93472,\"start\":93471},{\"end\":93875,\"start\":93874},{\"end\":93885,\"start\":93884},{\"end\":93896,\"start\":93895},{\"end\":93908,\"start\":93907},{\"end\":93922,\"start\":93921},{\"end\":93931,\"start\":93930},{\"end\":94357,\"start\":94356},{\"end\":94365,\"start\":94364},{\"end\":94740,\"start\":94739},{\"end\":94746,\"start\":94745},{\"end\":94755,\"start\":94754},{\"end\":95168,\"start\":95167},{\"end\":95176,\"start\":95175},{\"end\":95186,\"start\":95185},{\"end\":95205,\"start\":95204},{\"end\":95216,\"start\":95215},{\"end\":95229,\"start\":95228},{\"end\":95665,\"start\":95664},{\"end\":95671,\"start\":95670},{\"end\":95678,\"start\":95677},{\"end\":95688,\"start\":95684},{\"end\":95695,\"start\":95694},{\"end\":96142,\"start\":96141},{\"end\":96153,\"start\":96152},{\"end\":96164,\"start\":96163},{\"end\":96560,\"start\":96559},{\"end\":96568,\"start\":96567},{\"end\":96574,\"start\":96573},{\"end\":96581,\"start\":96580},{\"end\":96589,\"start\":96588},{\"end\":96595,\"start\":96594},{\"end\":96601,\"start\":96600},{\"end\":96964,\"start\":96963},{\"end\":96970,\"start\":96969},{\"end\":96979,\"start\":96978},{\"end\":97359,\"start\":97358},{\"end\":97370,\"start\":97369},{\"end\":97376,\"start\":97371},{\"end\":97385,\"start\":97384},{\"end\":97395,\"start\":97394},{\"end\":97404,\"start\":97403},{\"end\":97414,\"start\":97413},{\"end\":97426,\"start\":97425},{\"end\":97924,\"start\":97923},{\"end\":97933,\"start\":97932},{\"end\":97946,\"start\":97945},{\"end\":97954,\"start\":97953},{\"end\":97963,\"start\":97962},{\"end\":98321,\"start\":98320},{\"end\":98329,\"start\":98328},{\"end\":98335,\"start\":98334},{\"end\":98715,\"start\":98714},{\"end\":98723,\"start\":98722},{\"end\":98735,\"start\":98734},{\"end\":98748,\"start\":98747},{\"end\":98758,\"start\":98757},{\"end\":98769,\"start\":98768},{\"end\":98776,\"start\":98775},{\"end\":98786,\"start\":98785},{\"end\":99087,\"start\":99086},{\"end\":99095,\"start\":99094},{\"end\":99105,\"start\":99104},{\"end\":99119,\"start\":99118},{\"end\":99290,\"start\":99289},{\"end\":99297,\"start\":99296},{\"end\":99303,\"start\":99302},{\"end\":99311,\"start\":99310},{\"end\":99319,\"start\":99318},{\"end\":99333,\"start\":99332},{\"end\":99670,\"start\":99669},{\"end\":99678,\"start\":99677},{\"end\":99692,\"start\":99691},{\"end\":99952,\"start\":99951},{\"end\":99964,\"start\":99963},{\"end\":99971,\"start\":99970},{\"end\":99979,\"start\":99978},{\"end\":100454,\"start\":100453},{\"end\":100460,\"start\":100459},{\"end\":100466,\"start\":100465},{\"end\":100472,\"start\":100471},{\"end\":100478,\"start\":100477},{\"end\":100864,\"start\":100863},{\"end\":100876,\"start\":100875},{\"end\":100886,\"start\":100885},{\"end\":100901,\"start\":100897},{\"end\":100907,\"start\":100906},{\"end\":100924,\"start\":100920},{\"end\":101414,\"start\":101413},{\"end\":101421,\"start\":101420},{\"end\":101429,\"start\":101428},{\"end\":101435,\"start\":101434},{\"end\":101929,\"start\":101928},{\"end\":101938,\"start\":101937},{\"end\":101947,\"start\":101943},{\"end\":101955,\"start\":101954},{\"end\":101967,\"start\":101963},{\"end\":102397,\"start\":102396},{\"end\":102409,\"start\":102408},{\"end\":102417,\"start\":102416},{\"end\":102614,\"start\":102613},{\"end\":102621,\"start\":102620},{\"end\":102636,\"start\":102635},{\"end\":102646,\"start\":102645},{\"end\":103117,\"start\":103116},{\"end\":103123,\"start\":103122},{\"end\":103129,\"start\":103128},{\"end\":103135,\"start\":103134},{\"end\":103144,\"start\":103143},{\"end\":103150,\"start\":103149},{\"end\":103158,\"start\":103157},{\"end\":103556,\"start\":103555},{\"end\":103558,\"start\":103557},{\"end\":103567,\"start\":103566},{\"end\":103569,\"start\":103568},{\"end\":103578,\"start\":103577},{\"end\":103589,\"start\":103588},{\"end\":103871,\"start\":103870},{\"end\":103883,\"start\":103882},{\"end\":103895,\"start\":103894},{\"end\":103903,\"start\":103902},{\"end\":104085,\"start\":104084},{\"end\":104093,\"start\":104092},{\"end\":104102,\"start\":104101},{\"end\":104112,\"start\":104111},{\"end\":104118,\"start\":104117},{\"end\":104359,\"start\":104358},{\"end\":104367,\"start\":104366},{\"end\":104375,\"start\":104374},{\"end\":104384,\"start\":104383},{\"end\":104390,\"start\":104389},{\"end\":104405,\"start\":104404},{\"end\":104412,\"start\":104411},{\"end\":104419,\"start\":104418}]", "bib_author_last_name": "[{\"end\":79609,\"start\":79603},{\"end\":79621,\"start\":79616},{\"end\":79628,\"start\":79625},{\"end\":79641,\"start\":79632},{\"end\":79876,\"start\":79869},{\"end\":79882,\"start\":79880},{\"end\":79891,\"start\":79886},{\"end\":79899,\"start\":79895},{\"end\":79909,\"start\":79903},{\"end\":79922,\"start\":79913},{\"end\":80183,\"start\":80179},{\"end\":80190,\"start\":80187},{\"end\":80198,\"start\":80194},{\"end\":80211,\"start\":80202},{\"end\":80228,\"start\":80215},{\"end\":80236,\"start\":80234},{\"end\":80698,\"start\":80692},{\"end\":80708,\"start\":80702},{\"end\":80716,\"start\":80712},{\"end\":81131,\"start\":81126},{\"end\":81139,\"start\":81135},{\"end\":81145,\"start\":81143},{\"end\":81154,\"start\":81149},{\"end\":81161,\"start\":81158},{\"end\":81168,\"start\":81165},{\"end\":81567,\"start\":81565},{\"end\":81583,\"start\":81571},{\"end\":81591,\"start\":81587},{\"end\":81861,\"start\":81858},{\"end\":81867,\"start\":81865},{\"end\":81876,\"start\":81871},{\"end\":81886,\"start\":81883},{\"end\":81894,\"start\":81890},{\"end\":81902,\"start\":81898},{\"end\":81911,\"start\":81906},{\"end\":82282,\"start\":82280},{\"end\":82291,\"start\":82286},{\"end\":82297,\"start\":82295},{\"end\":82304,\"start\":82301},{\"end\":82312,\"start\":82308},{\"end\":82670,\"start\":82667},{\"end\":82676,\"start\":82674},{\"end\":82685,\"start\":82680},{\"end\":82695,\"start\":82691},{\"end\":83055,\"start\":83051},{\"end\":83064,\"start\":83061},{\"end\":83074,\"start\":83068},{\"end\":83081,\"start\":83078},{\"end\":83090,\"start\":83085},{\"end\":83096,\"start\":83094},{\"end\":83104,\"start\":83100},{\"end\":83112,\"start\":83108},{\"end\":83121,\"start\":83118},{\"end\":83129,\"start\":83125},{\"end\":83137,\"start\":83133},{\"end\":83143,\"start\":83141},{\"end\":83464,\"start\":83460},{\"end\":83472,\"start\":83468},{\"end\":83479,\"start\":83476},{\"end\":83486,\"start\":83483},{\"end\":83492,\"start\":83490},{\"end\":83894,\"start\":83885},{\"end\":83905,\"start\":83898},{\"end\":83912,\"start\":83909},{\"end\":83920,\"start\":83916},{\"end\":83929,\"start\":83924},{\"end\":83939,\"start\":83933},{\"end\":83951,\"start\":83943},{\"end\":83958,\"start\":83955},{\"end\":84416,\"start\":84409},{\"end\":84422,\"start\":84420},{\"end\":84431,\"start\":84426},{\"end\":84439,\"start\":84435},{\"end\":84449,\"start\":84443},{\"end\":84462,\"start\":84453},{\"end\":84653,\"start\":84648},{\"end\":84661,\"start\":84657},{\"end\":84674,\"start\":84667},{\"end\":84932,\"start\":84929},{\"end\":84938,\"start\":84936},{\"end\":84946,\"start\":84942},{\"end\":85139,\"start\":85137},{\"end\":85146,\"start\":85143},{\"end\":85153,\"start\":85150},{\"end\":85474,\"start\":85471},{\"end\":85480,\"start\":85478},{\"end\":85487,\"start\":85484},{\"end\":85493,\"start\":85491},{\"end\":85938,\"start\":85932},{\"end\":85949,\"start\":85942},{\"end\":85962,\"start\":85953},{\"end\":85976,\"start\":85966},{\"end\":85989,\"start\":85980},{\"end\":86003,\"start\":85993},{\"end\":86013,\"start\":86007},{\"end\":86027,\"start\":86017},{\"end\":86041,\"start\":86031},{\"end\":86507,\"start\":86498},{\"end\":86516,\"start\":86511},{\"end\":86527,\"start\":86520},{\"end\":86536,\"start\":86531},{\"end\":86957,\"start\":86948},{\"end\":86965,\"start\":86961},{\"end\":86973,\"start\":86969},{\"end\":86983,\"start\":86977},{\"end\":86994,\"start\":86987},{\"end\":87005,\"start\":86998},{\"end\":87016,\"start\":87009},{\"end\":87397,\"start\":87389},{\"end\":87405,\"start\":87401},{\"end\":87411,\"start\":87409},{\"end\":87421,\"start\":87415},{\"end\":87429,\"start\":87425},{\"end\":87810,\"start\":87804},{\"end\":87820,\"start\":87814},{\"end\":87830,\"start\":87824},{\"end\":88256,\"start\":88253},{\"end\":88272,\"start\":88260},{\"end\":88287,\"start\":88276},{\"end\":88298,\"start\":88293},{\"end\":88307,\"start\":88302},{\"end\":88767,\"start\":88764},{\"end\":88773,\"start\":88771},{\"end\":88780,\"start\":88777},{\"end\":88787,\"start\":88784},{\"end\":88795,\"start\":88791},{\"end\":88805,\"start\":88802},{\"end\":89259,\"start\":89255},{\"end\":89266,\"start\":89263},{\"end\":89274,\"start\":89270},{\"end\":89280,\"start\":89278},{\"end\":89293,\"start\":89284},{\"end\":89302,\"start\":89297},{\"end\":89312,\"start\":89308},{\"end\":89743,\"start\":89739},{\"end\":89749,\"start\":89747},{\"end\":89757,\"start\":89753},{\"end\":89771,\"start\":89761},{\"end\":90213,\"start\":90210},{\"end\":90223,\"start\":90220},{\"end\":90230,\"start\":90227},{\"end\":90236,\"start\":90234},{\"end\":90245,\"start\":90240},{\"end\":90254,\"start\":90249},{\"end\":90686,\"start\":90680},{\"end\":90694,\"start\":90690},{\"end\":90919,\"start\":90912},{\"end\":90930,\"start\":90923},{\"end\":90943,\"start\":90934},{\"end\":90952,\"start\":90947},{\"end\":90961,\"start\":90956},{\"end\":90975,\"start\":90965},{\"end\":90985,\"start\":90979},{\"end\":91402,\"start\":91392},{\"end\":91412,\"start\":91406},{\"end\":91423,\"start\":91416},{\"end\":91806,\"start\":91798},{\"end\":91820,\"start\":91812},{\"end\":91827,\"start\":91824},{\"end\":91841,\"start\":91831},{\"end\":91852,\"start\":91847},{\"end\":91861,\"start\":91856},{\"end\":92283,\"start\":92276},{\"end\":92295,\"start\":92289},{\"end\":92653,\"start\":92645},{\"end\":92665,\"start\":92657},{\"end\":92671,\"start\":92669},{\"end\":92678,\"start\":92675},{\"end\":92687,\"start\":92682},{\"end\":93055,\"start\":93048},{\"end\":93071,\"start\":93061},{\"end\":93081,\"start\":93075},{\"end\":93452,\"start\":93449},{\"end\":93460,\"start\":93456},{\"end\":93469,\"start\":93464},{\"end\":93487,\"start\":93473},{\"end\":93882,\"start\":93876},{\"end\":93893,\"start\":93886},{\"end\":93905,\"start\":93897},{\"end\":93919,\"start\":93909},{\"end\":93928,\"start\":93923},{\"end\":93940,\"start\":93932},{\"end\":94362,\"start\":94358},{\"end\":94370,\"start\":94366},{\"end\":94743,\"start\":94741},{\"end\":94752,\"start\":94747},{\"end\":94758,\"start\":94756},{\"end\":95173,\"start\":95169},{\"end\":95183,\"start\":95177},{\"end\":95202,\"start\":95187},{\"end\":95213,\"start\":95206},{\"end\":95226,\"start\":95217},{\"end\":95236,\"start\":95230},{\"end\":95668,\"start\":95666},{\"end\":95675,\"start\":95672},{\"end\":95682,\"start\":95679},{\"end\":95692,\"start\":95689},{\"end\":95701,\"start\":95696},{\"end\":96150,\"start\":96143},{\"end\":96161,\"start\":96154},{\"end\":96171,\"start\":96165},{\"end\":96565,\"start\":96561},{\"end\":96571,\"start\":96569},{\"end\":96578,\"start\":96575},{\"end\":96586,\"start\":96582},{\"end\":96592,\"start\":96590},{\"end\":96598,\"start\":96596},{\"end\":96606,\"start\":96602},{\"end\":96967,\"start\":96965},{\"end\":96976,\"start\":96971},{\"end\":96984,\"start\":96980},{\"end\":97367,\"start\":97360},{\"end\":97382,\"start\":97377},{\"end\":97392,\"start\":97386},{\"end\":97401,\"start\":97396},{\"end\":97411,\"start\":97405},{\"end\":97423,\"start\":97415},{\"end\":97436,\"start\":97427},{\"end\":97930,\"start\":97925},{\"end\":97943,\"start\":97934},{\"end\":97951,\"start\":97947},{\"end\":97960,\"start\":97955},{\"end\":97967,\"start\":97964},{\"end\":97977,\"start\":97969},{\"end\":98326,\"start\":98322},{\"end\":98332,\"start\":98330},{\"end\":98341,\"start\":98336},{\"end\":98720,\"start\":98716},{\"end\":98732,\"start\":98724},{\"end\":98745,\"start\":98736},{\"end\":98755,\"start\":98749},{\"end\":98766,\"start\":98759},{\"end\":98773,\"start\":98770},{\"end\":98783,\"start\":98777},{\"end\":98797,\"start\":98787},{\"end\":99092,\"start\":99088},{\"end\":99102,\"start\":99096},{\"end\":99116,\"start\":99106},{\"end\":99125,\"start\":99120},{\"end\":99294,\"start\":99291},{\"end\":99300,\"start\":99298},{\"end\":99308,\"start\":99304},{\"end\":99316,\"start\":99312},{\"end\":99330,\"start\":99320},{\"end\":99339,\"start\":99334},{\"end\":99675,\"start\":99671},{\"end\":99689,\"start\":99679},{\"end\":99703,\"start\":99693},{\"end\":99961,\"start\":99953},{\"end\":99968,\"start\":99965},{\"end\":99976,\"start\":99972},{\"end\":99983,\"start\":99980},{\"end\":100457,\"start\":100455},{\"end\":100463,\"start\":100461},{\"end\":100469,\"start\":100467},{\"end\":100475,\"start\":100473},{\"end\":100483,\"start\":100479},{\"end\":100873,\"start\":100865},{\"end\":100883,\"start\":100877},{\"end\":100895,\"start\":100887},{\"end\":100904,\"start\":100902},{\"end\":100918,\"start\":100908},{\"end\":100930,\"start\":100925},{\"end\":101418,\"start\":101415},{\"end\":101426,\"start\":101422},{\"end\":101432,\"start\":101430},{\"end\":101439,\"start\":101436},{\"end\":101935,\"start\":101930},{\"end\":101941,\"start\":101939},{\"end\":101952,\"start\":101948},{\"end\":101961,\"start\":101956},{\"end\":101973,\"start\":101968},{\"end\":102406,\"start\":102398},{\"end\":102414,\"start\":102410},{\"end\":102421,\"start\":102418},{\"end\":102618,\"start\":102615},{\"end\":102633,\"start\":102622},{\"end\":102643,\"start\":102637},{\"end\":102652,\"start\":102647},{\"end\":103120,\"start\":103118},{\"end\":103126,\"start\":103124},{\"end\":103132,\"start\":103130},{\"end\":103141,\"start\":103136},{\"end\":103147,\"start\":103145},{\"end\":103155,\"start\":103151},{\"end\":103163,\"start\":103159},{\"end\":103564,\"start\":103559},{\"end\":103575,\"start\":103570},{\"end\":103586,\"start\":103579},{\"end\":103592,\"start\":103590},{\"end\":103880,\"start\":103872},{\"end\":103892,\"start\":103884},{\"end\":103900,\"start\":103896},{\"end\":103913,\"start\":103904},{\"end\":104090,\"start\":104086},{\"end\":104099,\"start\":104094},{\"end\":104109,\"start\":104103},{\"end\":104115,\"start\":104113},{\"end\":104124,\"start\":104119},{\"end\":104364,\"start\":104360},{\"end\":104372,\"start\":104368},{\"end\":104381,\"start\":104376},{\"end\":104387,\"start\":104385},{\"end\":104402,\"start\":104391},{\"end\":104409,\"start\":104406},{\"end\":104416,\"start\":104413},{\"end\":104424,\"start\":104420}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":79812,\"start\":79519},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":160025533},\"end\":80101,\"start\":79814},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":195069387},\"end\":80607,\"start\":80103},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":17497284},\"end\":81043,\"start\":80609},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":216552897},\"end\":81561,\"start\":81045},{\"attributes\":{\"id\":\"b5\"},\"end\":81817,\"start\":81563},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":31806516},\"end\":82230,\"start\":81819},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":53059573},\"end\":82614,\"start\":82232},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202235190},\"end\":82996,\"start\":82616},{\"attributes\":{\"doi\":\"abs/1906.00080\",\"id\":\"b9\"},\"end\":83385,\"start\":82998},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":53737130},\"end\":83801,\"start\":83387},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208617790},\"end\":84352,\"start\":83803},{\"attributes\":{\"id\":\"b12\"},\"end\":84591,\"start\":84354},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52055325},\"end\":84909,\"start\":84593},{\"attributes\":{\"doi\":\"arXiv:1901.00158\",\"id\":\"b14\"},\"end\":85062,\"start\":84911},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":59600007},\"end\":85397,\"start\":85064},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":166228565},\"end\":85850,\"start\":85399},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3640499},\"end\":86433,\"start\":85852},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11816014},\"end\":86903,\"start\":86435},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1167588},\"end\":87339,\"start\":86905},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":127986954},\"end\":87753,\"start\":87341},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":215754328},\"end\":88173,\"start\":87755},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":209444410},\"end\":88676,\"start\":88175},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":59317065},\"end\":89171,\"start\":88678},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":67846878},\"end\":89643,\"start\":89173},{\"attributes\":{\"id\":\"b25\"},\"end\":90128,\"start\":89645},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204746801},\"end\":90603,\"start\":90130},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53107276},\"end\":90866,\"start\":90605},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":604334},\"end\":91340,\"start\":90868},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6706414},\"end\":91736,\"start\":91342},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7004303},\"end\":92218,\"start\":91738},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2893830},\"end\":92556,\"start\":92220},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2672720},\"end\":93005,\"start\":92558},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":9059612},\"end\":93386,\"start\":93007},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12308095},\"end\":93819,\"start\":93388},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":21946795},\"end\":94294,\"start\":93821},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3583538},\"end\":94662,\"start\":94296},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3851184},\"end\":95075,\"start\":94664},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206579396},\"end\":95587,\"start\":95077},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":142503510},\"end\":96038,\"start\":95589},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3310672},\"end\":96483,\"start\":96040},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":53075610},\"end\":96918,\"start\":96485},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":20939404},\"end\":97283,\"start\":96920},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4626477},\"end\":97825,\"start\":97285},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3971778},\"end\":98277,\"start\":97827},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":53298804},\"end\":98635,\"start\":98279},{\"attributes\":{\"id\":\"b46\"},\"end\":99016,\"start\":98637},{\"attributes\":{\"id\":\"b47\"},\"end\":99287,\"start\":99018},{\"attributes\":{\"id\":\"b48\"},\"end\":99576,\"start\":99289},{\"attributes\":{\"id\":\"b49\"},\"end\":99884,\"start\":99578},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":21698802},\"end\":100378,\"start\":99886},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":54815878},\"end\":100811,\"start\":100380},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5076191},\"end\":101318,\"start\":100813},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":196202909},\"end\":101832,\"start\":101320},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":3689056},\"end\":102322,\"start\":101834},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":49413369},\"end\":102555,\"start\":102324},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":202538141},\"end\":103013,\"start\":102557},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":219446959},\"end\":103467,\"start\":103015},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":118674722},\"end\":103805,\"start\":103469},{\"attributes\":{\"id\":\"b59\"},\"end\":104038,\"start\":103807},{\"attributes\":{\"doi\":\"arXiv:2006.01043\",\"id\":\"b60\"},\"end\":104291,\"start\":104040},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":219421921},\"end\":104784,\"start\":104293}]", "bib_title": "[{\"end\":79865,\"start\":79814},{\"end\":80175,\"start\":80103},{\"end\":80688,\"start\":80609},{\"end\":81122,\"start\":81045},{\"end\":81854,\"start\":81819},{\"end\":82276,\"start\":82232},{\"end\":82663,\"start\":82616},{\"end\":83456,\"start\":83387},{\"end\":83881,\"start\":83803},{\"end\":84644,\"start\":84593},{\"end\":85133,\"start\":85064},{\"end\":85467,\"start\":85399},{\"end\":85926,\"start\":85852},{\"end\":86494,\"start\":86435},{\"end\":86944,\"start\":86905},{\"end\":87385,\"start\":87341},{\"end\":87800,\"start\":87755},{\"end\":88249,\"start\":88175},{\"end\":88760,\"start\":88678},{\"end\":89251,\"start\":89173},{\"end\":89735,\"start\":89645},{\"end\":90206,\"start\":90130},{\"end\":90676,\"start\":90605},{\"end\":90908,\"start\":90868},{\"end\":91388,\"start\":91342},{\"end\":91794,\"start\":91738},{\"end\":92272,\"start\":92220},{\"end\":92641,\"start\":92558},{\"end\":93044,\"start\":93007},{\"end\":93445,\"start\":93388},{\"end\":93872,\"start\":93821},{\"end\":94354,\"start\":94296},{\"end\":94737,\"start\":94664},{\"end\":95165,\"start\":95077},{\"end\":95662,\"start\":95589},{\"end\":96139,\"start\":96040},{\"end\":96557,\"start\":96485},{\"end\":96961,\"start\":96920},{\"end\":97356,\"start\":97285},{\"end\":97921,\"start\":97827},{\"end\":98318,\"start\":98279},{\"end\":99949,\"start\":99886},{\"end\":100451,\"start\":100380},{\"end\":100861,\"start\":100813},{\"end\":101411,\"start\":101320},{\"end\":101926,\"start\":101834},{\"end\":102394,\"start\":102324},{\"end\":102611,\"start\":102557},{\"end\":103114,\"start\":103015},{\"end\":103553,\"start\":103469},{\"end\":104356,\"start\":104293}]", "bib_author": "[{\"end\":79611,\"start\":79601},{\"end\":79623,\"start\":79611},{\"end\":79630,\"start\":79623},{\"end\":79643,\"start\":79630},{\"end\":79878,\"start\":79867},{\"end\":79884,\"start\":79878},{\"end\":79893,\"start\":79884},{\"end\":79901,\"start\":79893},{\"end\":79911,\"start\":79901},{\"end\":79924,\"start\":79911},{\"end\":80185,\"start\":80177},{\"end\":80192,\"start\":80185},{\"end\":80200,\"start\":80192},{\"end\":80213,\"start\":80200},{\"end\":80230,\"start\":80213},{\"end\":80238,\"start\":80230},{\"end\":80700,\"start\":80690},{\"end\":80710,\"start\":80700},{\"end\":80718,\"start\":80710},{\"end\":81133,\"start\":81124},{\"end\":81141,\"start\":81133},{\"end\":81147,\"start\":81141},{\"end\":81156,\"start\":81147},{\"end\":81163,\"start\":81156},{\"end\":81170,\"start\":81163},{\"end\":81569,\"start\":81563},{\"end\":81585,\"start\":81569},{\"end\":81593,\"start\":81585},{\"end\":81863,\"start\":81856},{\"end\":81869,\"start\":81863},{\"end\":81878,\"start\":81869},{\"end\":81888,\"start\":81878},{\"end\":81896,\"start\":81888},{\"end\":81904,\"start\":81896},{\"end\":81913,\"start\":81904},{\"end\":82284,\"start\":82278},{\"end\":82293,\"start\":82284},{\"end\":82299,\"start\":82293},{\"end\":82306,\"start\":82299},{\"end\":82314,\"start\":82306},{\"end\":82672,\"start\":82665},{\"end\":82678,\"start\":82672},{\"end\":82687,\"start\":82678},{\"end\":82697,\"start\":82687},{\"end\":83057,\"start\":83047},{\"end\":83066,\"start\":83057},{\"end\":83076,\"start\":83066},{\"end\":83083,\"start\":83076},{\"end\":83092,\"start\":83083},{\"end\":83098,\"start\":83092},{\"end\":83106,\"start\":83098},{\"end\":83114,\"start\":83106},{\"end\":83123,\"start\":83114},{\"end\":83131,\"start\":83123},{\"end\":83139,\"start\":83131},{\"end\":83145,\"start\":83139},{\"end\":83466,\"start\":83458},{\"end\":83474,\"start\":83466},{\"end\":83481,\"start\":83474},{\"end\":83488,\"start\":83481},{\"end\":83494,\"start\":83488},{\"end\":83896,\"start\":83883},{\"end\":83907,\"start\":83896},{\"end\":83914,\"start\":83907},{\"end\":83922,\"start\":83914},{\"end\":83931,\"start\":83922},{\"end\":83941,\"start\":83931},{\"end\":83953,\"start\":83941},{\"end\":83960,\"start\":83953},{\"end\":84418,\"start\":84407},{\"end\":84424,\"start\":84418},{\"end\":84433,\"start\":84424},{\"end\":84441,\"start\":84433},{\"end\":84451,\"start\":84441},{\"end\":84464,\"start\":84451},{\"end\":84655,\"start\":84646},{\"end\":84663,\"start\":84655},{\"end\":84676,\"start\":84663},{\"end\":84934,\"start\":84927},{\"end\":84940,\"start\":84934},{\"end\":84948,\"start\":84940},{\"end\":85141,\"start\":85135},{\"end\":85148,\"start\":85141},{\"end\":85155,\"start\":85148},{\"end\":85476,\"start\":85469},{\"end\":85482,\"start\":85476},{\"end\":85489,\"start\":85482},{\"end\":85495,\"start\":85489},{\"end\":85940,\"start\":85928},{\"end\":85951,\"start\":85940},{\"end\":85964,\"start\":85951},{\"end\":85978,\"start\":85964},{\"end\":85991,\"start\":85978},{\"end\":86005,\"start\":85991},{\"end\":86015,\"start\":86005},{\"end\":86029,\"start\":86015},{\"end\":86043,\"start\":86029},{\"end\":86509,\"start\":86496},{\"end\":86518,\"start\":86509},{\"end\":86529,\"start\":86518},{\"end\":86538,\"start\":86529},{\"end\":86959,\"start\":86946},{\"end\":86967,\"start\":86959},{\"end\":86975,\"start\":86967},{\"end\":86985,\"start\":86975},{\"end\":86996,\"start\":86985},{\"end\":87007,\"start\":86996},{\"end\":87018,\"start\":87007},{\"end\":87399,\"start\":87387},{\"end\":87407,\"start\":87399},{\"end\":87413,\"start\":87407},{\"end\":87423,\"start\":87413},{\"end\":87431,\"start\":87423},{\"end\":87812,\"start\":87802},{\"end\":87822,\"start\":87812},{\"end\":87832,\"start\":87822},{\"end\":88258,\"start\":88251},{\"end\":88274,\"start\":88258},{\"end\":88289,\"start\":88274},{\"end\":88300,\"start\":88289},{\"end\":88309,\"start\":88300},{\"end\":88769,\"start\":88762},{\"end\":88775,\"start\":88769},{\"end\":88782,\"start\":88775},{\"end\":88789,\"start\":88782},{\"end\":88797,\"start\":88789},{\"end\":88807,\"start\":88797},{\"end\":89261,\"start\":89253},{\"end\":89268,\"start\":89261},{\"end\":89276,\"start\":89268},{\"end\":89282,\"start\":89276},{\"end\":89295,\"start\":89282},{\"end\":89304,\"start\":89295},{\"end\":89314,\"start\":89304},{\"end\":89745,\"start\":89737},{\"end\":89751,\"start\":89745},{\"end\":89759,\"start\":89751},{\"end\":89773,\"start\":89759},{\"end\":90215,\"start\":90208},{\"end\":90225,\"start\":90215},{\"end\":90232,\"start\":90225},{\"end\":90238,\"start\":90232},{\"end\":90247,\"start\":90238},{\"end\":90256,\"start\":90247},{\"end\":90688,\"start\":90678},{\"end\":90696,\"start\":90688},{\"end\":90921,\"start\":90910},{\"end\":90932,\"start\":90921},{\"end\":90945,\"start\":90932},{\"end\":90954,\"start\":90945},{\"end\":90963,\"start\":90954},{\"end\":90977,\"start\":90963},{\"end\":90987,\"start\":90977},{\"end\":91404,\"start\":91390},{\"end\":91414,\"start\":91404},{\"end\":91425,\"start\":91414},{\"end\":91808,\"start\":91796},{\"end\":91822,\"start\":91808},{\"end\":91829,\"start\":91822},{\"end\":91843,\"start\":91829},{\"end\":91854,\"start\":91843},{\"end\":91863,\"start\":91854},{\"end\":92285,\"start\":92274},{\"end\":92297,\"start\":92285},{\"end\":92655,\"start\":92643},{\"end\":92667,\"start\":92655},{\"end\":92673,\"start\":92667},{\"end\":92680,\"start\":92673},{\"end\":92689,\"start\":92680},{\"end\":93057,\"start\":93046},{\"end\":93073,\"start\":93057},{\"end\":93083,\"start\":93073},{\"end\":93454,\"start\":93447},{\"end\":93462,\"start\":93454},{\"end\":93471,\"start\":93462},{\"end\":93489,\"start\":93471},{\"end\":93884,\"start\":93874},{\"end\":93895,\"start\":93884},{\"end\":93907,\"start\":93895},{\"end\":93921,\"start\":93907},{\"end\":93930,\"start\":93921},{\"end\":93942,\"start\":93930},{\"end\":94364,\"start\":94356},{\"end\":94372,\"start\":94364},{\"end\":94745,\"start\":94739},{\"end\":94754,\"start\":94745},{\"end\":94760,\"start\":94754},{\"end\":95175,\"start\":95167},{\"end\":95185,\"start\":95175},{\"end\":95204,\"start\":95185},{\"end\":95215,\"start\":95204},{\"end\":95228,\"start\":95215},{\"end\":95238,\"start\":95228},{\"end\":95670,\"start\":95664},{\"end\":95677,\"start\":95670},{\"end\":95684,\"start\":95677},{\"end\":95694,\"start\":95684},{\"end\":95703,\"start\":95694},{\"end\":96152,\"start\":96141},{\"end\":96163,\"start\":96152},{\"end\":96173,\"start\":96163},{\"end\":96567,\"start\":96559},{\"end\":96573,\"start\":96567},{\"end\":96580,\"start\":96573},{\"end\":96588,\"start\":96580},{\"end\":96594,\"start\":96588},{\"end\":96600,\"start\":96594},{\"end\":96608,\"start\":96600},{\"end\":96969,\"start\":96963},{\"end\":96978,\"start\":96969},{\"end\":96986,\"start\":96978},{\"end\":97369,\"start\":97358},{\"end\":97384,\"start\":97369},{\"end\":97394,\"start\":97384},{\"end\":97403,\"start\":97394},{\"end\":97413,\"start\":97403},{\"end\":97425,\"start\":97413},{\"end\":97438,\"start\":97425},{\"end\":97932,\"start\":97923},{\"end\":97945,\"start\":97932},{\"end\":97953,\"start\":97945},{\"end\":97962,\"start\":97953},{\"end\":97969,\"start\":97962},{\"end\":97979,\"start\":97969},{\"end\":98328,\"start\":98320},{\"end\":98334,\"start\":98328},{\"end\":98343,\"start\":98334},{\"end\":98722,\"start\":98714},{\"end\":98734,\"start\":98722},{\"end\":98747,\"start\":98734},{\"end\":98757,\"start\":98747},{\"end\":98768,\"start\":98757},{\"end\":98775,\"start\":98768},{\"end\":98785,\"start\":98775},{\"end\":98799,\"start\":98785},{\"end\":99094,\"start\":99086},{\"end\":99104,\"start\":99094},{\"end\":99118,\"start\":99104},{\"end\":99127,\"start\":99118},{\"end\":99296,\"start\":99289},{\"end\":99302,\"start\":99296},{\"end\":99310,\"start\":99302},{\"end\":99318,\"start\":99310},{\"end\":99332,\"start\":99318},{\"end\":99341,\"start\":99332},{\"end\":99677,\"start\":99669},{\"end\":99691,\"start\":99677},{\"end\":99705,\"start\":99691},{\"end\":99963,\"start\":99951},{\"end\":99970,\"start\":99963},{\"end\":99978,\"start\":99970},{\"end\":99985,\"start\":99978},{\"end\":100459,\"start\":100453},{\"end\":100465,\"start\":100459},{\"end\":100471,\"start\":100465},{\"end\":100477,\"start\":100471},{\"end\":100485,\"start\":100477},{\"end\":100875,\"start\":100863},{\"end\":100885,\"start\":100875},{\"end\":100897,\"start\":100885},{\"end\":100906,\"start\":100897},{\"end\":100920,\"start\":100906},{\"end\":100932,\"start\":100920},{\"end\":101420,\"start\":101413},{\"end\":101428,\"start\":101420},{\"end\":101434,\"start\":101428},{\"end\":101441,\"start\":101434},{\"end\":101937,\"start\":101928},{\"end\":101943,\"start\":101937},{\"end\":101954,\"start\":101943},{\"end\":101963,\"start\":101954},{\"end\":101975,\"start\":101963},{\"end\":102408,\"start\":102396},{\"end\":102416,\"start\":102408},{\"end\":102423,\"start\":102416},{\"end\":102620,\"start\":102613},{\"end\":102635,\"start\":102620},{\"end\":102645,\"start\":102635},{\"end\":102654,\"start\":102645},{\"end\":103122,\"start\":103116},{\"end\":103128,\"start\":103122},{\"end\":103134,\"start\":103128},{\"end\":103143,\"start\":103134},{\"end\":103149,\"start\":103143},{\"end\":103157,\"start\":103149},{\"end\":103165,\"start\":103157},{\"end\":103566,\"start\":103555},{\"end\":103577,\"start\":103566},{\"end\":103588,\"start\":103577},{\"end\":103594,\"start\":103588},{\"end\":103882,\"start\":103870},{\"end\":103894,\"start\":103882},{\"end\":103902,\"start\":103894},{\"end\":103915,\"start\":103902},{\"end\":104092,\"start\":104084},{\"end\":104101,\"start\":104092},{\"end\":104111,\"start\":104101},{\"end\":104117,\"start\":104111},{\"end\":104126,\"start\":104117},{\"end\":104366,\"start\":104358},{\"end\":104374,\"start\":104366},{\"end\":104383,\"start\":104374},{\"end\":104389,\"start\":104383},{\"end\":104404,\"start\":104389},{\"end\":104411,\"start\":104404},{\"end\":104418,\"start\":104411},{\"end\":104426,\"start\":104418}]", "bib_venue": "[{\"end\":79599,\"start\":79519},{\"end\":79947,\"start\":79924},{\"end\":80312,\"start\":80238},{\"end\":80788,\"start\":80718},{\"end\":81254,\"start\":81170},{\"end\":81672,\"start\":81593},{\"end\":81984,\"start\":81913},{\"end\":82384,\"start\":82314},{\"end\":82767,\"start\":82697},{\"end\":83045,\"start\":82998},{\"end\":83558,\"start\":83494},{\"end\":84033,\"start\":83960},{\"end\":84405,\"start\":84354},{\"end\":84737,\"start\":84676},{\"end\":84925,\"start\":84911},{\"end\":85216,\"start\":85155},{\"end\":85579,\"start\":85495},{\"end\":86105,\"start\":86043},{\"end\":86623,\"start\":86538},{\"end\":87084,\"start\":87018},{\"end\":87504,\"start\":87431},{\"end\":87916,\"start\":87832},{\"end\":88382,\"start\":88309},{\"end\":88881,\"start\":88807},{\"end\":89373,\"start\":89314},{\"end\":89845,\"start\":89773},{\"end\":90326,\"start\":90256},{\"end\":90715,\"start\":90696},{\"end\":91061,\"start\":90987},{\"end\":91499,\"start\":91425},{\"end\":91936,\"start\":91863},{\"end\":92356,\"start\":92297},{\"end\":92748,\"start\":92689},{\"end\":93156,\"start\":93083},{\"end\":93563,\"start\":93489},{\"end\":94016,\"start\":93942},{\"end\":94442,\"start\":94372},{\"end\":94831,\"start\":94760},{\"end\":95297,\"start\":95238},{\"end\":95774,\"start\":95703},{\"end\":96230,\"start\":96173},{\"end\":96667,\"start\":96608},{\"end\":97061,\"start\":96986},{\"end\":97512,\"start\":97438},{\"end\":98025,\"start\":97979},{\"end\":98417,\"start\":98343},{\"end\":98712,\"start\":98637},{\"end\":99084,\"start\":99018},{\"end\":99404,\"start\":99341},{\"end\":99667,\"start\":99578},{\"end\":100072,\"start\":99985},{\"end\":100556,\"start\":100485},{\"end\":101018,\"start\":100932},{\"end\":101528,\"start\":101441},{\"end\":102039,\"start\":101975},{\"end\":102429,\"start\":102423},{\"end\":102739,\"start\":102654},{\"end\":103211,\"start\":103165},{\"end\":103626,\"start\":103594},{\"end\":103868,\"start\":103807},{\"end\":104082,\"start\":104040},{\"end\":104496,\"start\":104426},{\"end\":80373,\"start\":80314},{\"end\":80845,\"start\":80790},{\"end\":81325,\"start\":81256},{\"end\":82042,\"start\":81986},{\"end\":82441,\"start\":82386},{\"end\":82824,\"start\":82769},{\"end\":83609,\"start\":83560},{\"end\":84093,\"start\":84035},{\"end\":85650,\"start\":85581},{\"end\":86154,\"start\":86107},{\"end\":86695,\"start\":86625},{\"end\":87137,\"start\":87086},{\"end\":87564,\"start\":87506},{\"end\":87987,\"start\":87918},{\"end\":88442,\"start\":88384},{\"end\":88942,\"start\":88883},{\"end\":89419,\"start\":89375},{\"end\":89904,\"start\":89847},{\"end\":90383,\"start\":90328},{\"end\":91122,\"start\":91063},{\"end\":91560,\"start\":91501},{\"end\":91996,\"start\":91938},{\"end\":92402,\"start\":92358},{\"end\":92794,\"start\":92750},{\"end\":93216,\"start\":93158},{\"end\":93624,\"start\":93565},{\"end\":94077,\"start\":94018},{\"end\":94499,\"start\":94444},{\"end\":94889,\"start\":94833},{\"end\":95343,\"start\":95299},{\"end\":95832,\"start\":95776},{\"end\":96274,\"start\":96232},{\"end\":96713,\"start\":96669},{\"end\":97123,\"start\":97063},{\"end\":97573,\"start\":97514},{\"end\":98058,\"start\":98027},{\"end\":98478,\"start\":98419},{\"end\":100146,\"start\":100074},{\"end\":100614,\"start\":100558},{\"end\":101091,\"start\":101020},{\"end\":101602,\"start\":101530},{\"end\":102090,\"start\":102041},{\"end\":102811,\"start\":102741},{\"end\":103244,\"start\":103213},{\"end\":104553,\"start\":104498}]"}}}, "year": 2023, "month": 12, "day": 17}