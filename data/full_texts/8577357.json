{"id": 8577357, "updated": "2022-08-09 17:35:32.376", "metadata": {"title": "A theory of learning from different domains", "authors": "[{\"first\":\"Shai\",\"last\":\"Ben-David\",\"middle\":[]},{\"first\":\"John\",\"last\":\"Blitzer\",\"middle\":[]},{\"first\":\"Koby\",\"last\":\"Crammer\",\"middle\":[]},{\"first\":\"Alex\",\"last\":\"Kulesza\",\"middle\":[]},{\"first\":\"Fernando\",\"last\":\"Pereira\",\"middle\":[]},{\"first\":\"Jennifer\",\"last\":\"Vaughan\",\"middle\":[\"Wortman\"]}]", "venue": "Machine Learning", "journal": "Machine Learning", "publication_date": {"year": 2010, "month": null, "day": null}, "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time? We address the first question by bounding a classifier\u2019s target error in terms of its source error and the divergence between the two domains. We give a classifier-induced divergence measure that can be estimated from finite, unlabeled samples from the domains. Under the assumption that there exists some hypothesis that performs well in both domains, we show that this quantity together with the empirical source error characterize the target error of a source-trained classifier. We answer the second question by bounding the target error of a model which minimizes a convex combination of the empirical source and target errors. Previous theoretical work has considered minimizing just the source error, just the target error, or weighting instances from the two domains equally. We show how to choose the optimal combination of source and target error as a function of the divergence, the sample sizes of both domains, and the complexity of the hypothesis class. The resulting bound generalizes the previously studied cases and is always at least as tight as a bound which considers minimizing only the target error or an equal weighting of source and target errors.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2104094955", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ml/Ben-DavidBCKPV10", "doi": "10.1007/s10994-009-5152-4"}}, "content": {"source": {"pdf_hash": "bb6763f9be63cda28ee9dd6b1d3e3a6630ffd5de", "pdf_src": "Adhoc", "pdf_uri": "[\"https://web.archive.org/web/20200630081725/https:/escholarship.org/content/qt2nv1j9sc/qt2nv1j9sc_noSplash_822b818066b67ae2c79ca81aacc1245f.pdf?t=l7nx96\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://link.springer.com/content/pdf/10.1007/s10994-009-5152-4.pdf", "status": "BRONZE"}}, "grobid": {"id": "5b713a1cb751aee934707ee8a01c53142b0955cd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bb6763f9be63cda28ee9dd6b1d3e3a6630ffd5de.txt", "contents": "\nA theory of learning from different domains\n2010\n\nShai Ben-David \nJohn Blitzer blitzer@cs.berkeley.edu \nKoby Crammer \nAlex Kulesza kulesza@cis.upenn.edu \nFernando Pereira pereira@google.com \nJennifer Wortman \nVaughan \nS \nBen-David David \nR Cheriton \nJ Blitzer \nK Crammer \nA Kulesza \nF Pereira \nJ W Vaughan \n\nSchool of Computer Science\nDepartment of Computer Science\nUniversity of Waterloo\nWaterlooONCanada\n\n\nDepartment of Electrical Engineering\nUC Berkeley\nBerkeleyCAUSA\n\n\nDepartment of Computer and Information Science\nThe Technion\nHaifaIsrael\n\n\nUniversity of Pennsylvania\nPhiladelphiaPAUSA\n\n\nSchool of Engineering and Applied Sciences\nGoogle Research\nMountain ViewCAUSA\n\n\nHarvard University\nCambridgeMAUSA\n\nA theory of learning from different domains\n\nMach Learn\n79201010.1007/s10994-009-5152-4Received: 28 February 2009 / Revised: 12 September 2009 / Accepted: 18 September 2009 /Editors: Nicolo Cesa-Bianchi, David R. Hardoon, and Gayle Leen. Preliminary versions of the work contained in this article appeared in Advances in Neural Information Processing Systems (Ben-David et al. 2006; Blitzer et al. 2007a).Domain adaptation \u00b7 Transfer learning \u00b7 Learning theory \u00b7 Sample-selection bias\nDiscriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time?\n\nWe address the first question by bounding a classifier's target error in terms of its source error and the divergence between the two domains. We give a classifier-induced divergence measure that can be estimated from finite, unlabeled samples from the domains. Under the assumption that there exists some hypothesis that performs well in both domains, we show that this quantity together with the empirical source error characterize the target error of a source-trained classifier.\n\nWe answer the second question by bounding the target error of a model which minimizes a convex combination of the empirical source and target errors. Previous theoretical work has considered minimizing just the source error, just the target error, or weighting instances from the two domains equally. We show how to choose the optimal combination of source and target error as a function of the divergence, the sample sizes of both domains, and the complexity of the hypothesis class. The resulting bound generalizes the previously studied cases and is always at least as tight as a bound which considers minimizing only the target error or an equal weighting of source and target errors.\n\nKeywords Domain adaptation \u00b7 Transfer learning \u00b7 Learning theory \u00b7 Sample-selection bias\n\n\nIntroduction\n\nMost research in machine learning, both theoretical and empirical, assumes that models are trained and tested using data drawn from some fixed distribution. This single domain setting has been well studied, and uniform convergence theory guarantees that a model's empirical training error is close to its true error under such assumptions. In many practical cases, however, we wish to train a model in one or more source domains and then apply it to a different target domain. For example, we might have a spam filter trained from a large email collection received by a group of current users (the source domain) and wish to adapt it for a new user (the target domain). Intuitively this should improve filtering performance for the new user, under the assumption that users generally agree on what is spam and what is not. The challenge is that each user receives a unique distribution of email.\n\nMany other examples arise in natural language processing. In general, labeled data for tasks like part-of-speech tagging (Ratnaparkhi 1996), parsing (Collins 1999), information extraction (Bikel et al. 1997), and sentiment analysis (Pang et al. 2002) are drawn from a limited set of document types and genres in a given language due to availability, cost, and specific goals of the project. However, useful applications for the trained systems may involve documents of different types or genres. We can hope to successfully adapt the systems in these cases since parts-of-speech, syntactic structure, entity mentions, and positive or negative sentiment are to a large extent stable across different domains, as they depend on general properties of language.\n\nIn this work we investigate the problem of domain adaptation. We analyze a setting in which we have plentiful labeled training data drawn from one or more source distributions but little or no labeled training data drawn from the target distribution of interest. This work answers two main questions. First, under what conditions on the source and target distributions can we expect to learn well? We give a bound on a classifier's target domain error in terms of its source domain error and a divergence measure between the two domains. In a distribution-free setting, we cannot obtain accurate estimates of common measures of divergence such as L 1 or Kullback-Leibler from finite samples. Instead, we show that when learning a hypothesis from a class of finite complexity, it is sufficient to use a classifier-induced divergence we call the H H-divergence (Kifer et al. 2004;Ben-David et al. 2006). Finite sample estimates of the H H-divergence converge uniformly to the true H H-divergence, allowing us to estimate the domain divergence from unlabeled data in both domains. Our final bound on the target error is in terms of the empirical source error, the empirical H H-divergence between unlabeled samples from the domains, and the combined error of the best single hypothesis for both domains.\n\nA second important question is how to learn when the large quantity of labeled source data is augmented with a small amount of labeled target data, for example, when our new email user has begun to manually mark a few received messages as spam. Given a source domain S and a target domain T , we consider hypotheses h which minimize a convex combination of empirical source and target error (\u02c6 T (h) and\u02c6 S (h), respectively), which we refer to as the empirical \u03b1-error:\n\u03b1\u02c6 T (h) + (1 \u2212 \u03b1)\u02c6 S (h).\nSetting \u03b1 involves trading off the ideal but small target dataset against the large (but less relevant) source dataset. Baseline choices for \u03b1 include \u03b1 = 0 (using only source data) (Ben-David et al. 2006), \u03b1 = 1 (using only target data), and the equal weighting of source and target instances (Crammer et al. 2008), setting \u03b1 to the fraction of the instances that are from the target domain. We give a bound on a classifier's target error in terms of its empirical \u03b1 error. The \u03b1 that minimizes the bound depends on the divergence between the domains as well as the size of the source and target training datasets. The optimal bound is always at least as tight as the bounds using only source, only target, or equally-weighted source and target instances. We show that for a real-world problem of sentiment classification, nontrivial settings of \u03b1 perform better than the three baseline settings.\n\nIn the next section, we give a brief overview of related work. We then specify precisely our model of domain adaptation. Section 4 shows how to bound the target error of a hypothesis in terms of its source error and the source-target divergence. Section 5 gives our main result, a bound on the target error of a classifier which minimizes a convex combination of empirical errors on the two domains, and in Sect. 6 we investigate the properties of the best convex combination of that bound. In Sect. 7, we illustrate experimentally the above bounds on sentiment classification data. Section 8 describes how to extend the previous results to the case of multiple data sources. Finally, we conclude with a brief discussion of future directions for research in Sect. 9. Crammer et al. (2008) introduced a PAC-style model of learning from multiple sources in which the distribution over input points is assumed to be the same across sources but each source may have its own deterministic labeling function. They derive bounds on the target error of the function that minimizes the empirical error on (uniformly weighted) data from any subset of the sources. As discussed in Sect. 8.2, the bounds that they derive are equivalent to ours in certain restricted settings, but their theory is significantly less general. Daum\u00e9 (2007) and Finkel (2009) suggest an empirically successful method for domain adaptation based on multi-task learning. The crucial difference between our domain adaptation setting and analyses of multi-task methods is that multi-task bounds require labeled data from each task, and make no attempt to exploit unlabeled data. Although these bounds have a more limited scope than ours, they can sometimes yield useful results even when the optimal predictors for each task (or domain in the case of Daum\u00e9 2007) are quite different (Baxter 2000;Ando and Zhang 2005). Li and Bilmes (2007) give PAC-Bayesian learning bounds for adaptation using \"divergence priors.\" In particular, they place a source-centered prior on the parameters of a model learned in the target domain. Like our model, the divergence prior emphasizes the tradeoff between source hypotheses trained on large (but biased) data sets and target hypotheses trained from small (but unbiased) data sets. In our model, however, we measure the divergence (and consequently the bias) of the source domain from unlabeled data. This allows us to choose a tradeoff parameter for source and target labeled data before training begins.\n\n\nRelated work\n\nMore recently, Mansour et al. (2009aMansour et al. ( , 2009b) introduced a theoretical model for the \"multiple source adaptation problem.\" This model operates under assumptions very similar to our multiple source analysis (Sect. 8), and we address their work in more detail there.\n\nFinally, domain adaptation is closely related to the setting of sample selection bias (Heckman 1979). A well-studied variant of this is covariate shift, which has seen significant work in recent years (Huang et al. 2007;Sugiyama et al. 2008;Cortes et al. 2008). This line of work leads to algorithms based on instance weighting, which have also been explored empirically in the machine learning and natural language processing communities (Jiang and Zhai 2007;Bickel et al. 2007). Our work differs from covariate shift primarily in two ways. First, we do not assume the labeling rule is identical for the source and target data (although there must exist some good labeling rule for both in order to achieve low error). Second, our H H-divergence can be computed from finite samples of unlabeled data, allowing us to directly estimate the error of a source-trained classifier on the target domain.\n\nA point of general contrast is that we work in an agnostic setting in which we do not make strong assumptions about the data generation model, such as a specific relationship between the source and target data distributions, which would be needed to obtain absolute error bounds. Instead, we assume only that the samples from each of the two domains are generated i.i.d. according to the respective data distributions, and as a result our bounds must be relative to the error of some benchmark predictor rather than absolute, specifically, relative to the combined error on both domains of an optimal joint predictor.\n\n\nA rigorous model of domain adaptation\n\nWe formalize the problem of domain adaptation for binary classification as follows. We define a domain 1 as a pair consisting of a distribution D on inputs X and a labeling function f : X \u2192 [0, 1], which can have a fractional (expected) value when labeling occurs nondeterministically. Initially, we consider two domains, a source domain and a target domain. We denote by D S , f S the source domain and D T , f T the target domain.\n\nA hypothesis is a function h : X \u2192 {0, 1}. The probability according to the distribution D S that a hypothesis h disagrees with a labeling function f (which can also be a hypothesis) is defined as\nS (h, f ) = E x\u223cD S |h(x) \u2212 f (x)| .\nWhen we want to refer to the source error (sometimes called risk) of a hypothesis, we use the shorthand S (h) = S (h, f S ). We write the empirical source error as\u02c6 S (h). We use the parallel notation T (h, f ), T (h), and\u02c6 T (h) for the target domain.\n\n\nA bound relating the source and target error\n\nWe now proceed to develop bounds on the target domain generalization performance of a classifier trained in the source domain. We first show how to bound the target error in terms of the source error, the difference between labeling functions f S and f T , and the divergence between the distributions D S and D T . Since we expect the labeling function difference to be small in practice, we focus here on measuring distribution divergence, and especially on how to estimate it with finite samples of unlabeled data from D S and D T . That is the role of the H-divergence introduced in Sect. 4.1.\n\nA natural measure of divergence for distributions is the L 1 or variation divergence\nd 1 (D, D ) = 2 sup B\u2208B |Pr D [B] \u2212 Pr D [B]| ,\nwhere B is the set of measurable subsets under D and D . We make use of this measure to state an initial bound on the target error of a classifier.\n\n\nTheorem 1 For a hypothesis h,\nT (h) \u2264 S (h) + d 1 (D S , D T ) + min E D S |f S (x) \u2212 f T (x)| , E D T |f S (x) \u2212 f T (x)| .\nProof See Appendix.\n\nIn this bound, the first term is the source error, which a training algorithm might seek to minimize, and the third is the difference in labeling functions across the two domains, which we expect to be small. The problem is the remaining term. Bounding the error in terms of the L 1 divergence between distributions has two disadvantages. First, it cannot be accurately estimated from finite samples of arbitrary distributions (Batu et al. 2000;Kifer et al. 2004) and therefore has limited usefulness in practice. Second, for our purposes the L 1 divergence is an overly strict measure that unnecessarily inflates the bound, since it involves a supremum over all measurable subsets. We are only interested in the error of a hypothesis from some class of finite complexity, thus we can restrict our attentions to the subsets on which this type of hypothesis can commit errors. The divergence measure introduced in the next section addresses both of these concerns.\n\n\nThe H-divergence\n\nDefinition 1 (Based on Kifer et al. 2004) Given a domain X with D and D probability distributions over X , let H be a hypothesis class on X and denote by I (h) the set for which h \u2208 H is the characteristic function; that is,\nx \u2208 I (h) \u21d4 h(x) = 1. The H-divergence be- tween D and D is d H (D, D ) = 2 sup h\u2208H |Pr D [I (h)] \u2212 Pr D [I (h)] |.\nThe H-divergence resolves both problems associated with the L 1 divergence. First, for hypothesis classes H of finite VC dimension, the H-divergence can be estimated from finite samples (see Lemma 1 below). Second, the H-divergence for any class H is never larger than the L 1 divergence, and is in general smaller when H has finite VC dimension.\n\nSince it plays an important role in the rest of this work, we now state a slight modification of Theorem 3.4 of Kifer et al. (2004) as a lemma.\n\nLemma 1 Let H be a hypothesis space on X with VC dimension d. If U and U are samples of size m from D and D respectively andd H (U, U ) is the empirical H-divergence between samples, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\nd H (D, D ) \u2264d H (U, U ) + 4 d log(2m) + log( 2 \u03b4 ) m .\nLemma 1 shows that the empirical H-divergence between two samples from distributions D and D converges uniformly to the true H-divergence for hypothesis classes H of finite VC dimension.\n\nThe next lemma shows that we can compute the H-divergence by finding a classifier which attempts to separate one domain from the other. Our basic plan of attack will be as follows: Label each unlabeled source instance with 0 and unlabeled target instance as 1. Then train a classifier to discriminate between source and target instances. The H-divergence is immediately computable from the error.\n\n\nLemma 2 For a symmetric hypothesis class\nH (one where for every h \u2208 H, the inverse hypothesis 1 \u2212 h is also in H) and samples U , U of size m d H (U, U ) = 2 1 \u2212 min h\u2208H 1 m x:h(x)=0 I [x \u2208 U ] + 1 m x:h(x)=1 I x \u2208 U , where I [x \u2208 U ] is the binary indicator variable which is 1 when x \u2208 U .\nProof See Appendix.\n\nThis lemma leads directly to a procedure for computing the H-divergence. We first find a hypothesis in H which has minimum error for the binary classification problem of distinguishing source from target instances. The error of this hypothesis is related to the Hdivergence by Lemma 2. Of course, minimizing error for most reasonable hypothesis classes is a computationally intractable problem. Nonetheless, as we shall see in Sect. 7, the error of hypotheses trained to minimize convex upper bounds on error are useful in approximating the H-divergence.\n\n\nBounding the difference in error using the H-divergence\n\nThe H-divergence allows us to estimate divergence from unlabeled data, but in order to use it in a bound we must have tools to represent error relative to other hypotheses in our class. We introduce two new definitions.\n\n\nDefinition 2\n\nThe ideal joint hypothesis is the hypothesis which minimizes the combined error\nh * = argmin h\u2208H S (h) + T (h).\nWe denote the combined error of the ideal hypothesis by\n\u03bb = S (h * ) + T (h * ).\nThe ideal joint hypothesis explicitly embodies our notion of adaptability. When this hypothesis performs poorly, we cannot expect to learn a good target classifier by minimizing source error. On the other hand, we will show that if the ideal joint hypothesis performs well, we can measure adaptability of a source-trained classifier by using the H-divergence between the marginal distributions D S and D T .\n\nNext we define the symmetric difference hypothesis space H H for a hypothesis space H, which will be very useful in reasoning about error.\n\nDefinition 3 For a hypothesis space H, the symmetric difference hypothesis space H H is the set of hypotheses\ng \u2208 H H \u21d0\u21d2 g(x) = h(x) \u2295 h (x) for some h, h \u2208 H,\nwhere \u2295 is the XOR function. In words, every hypothesis g \u2208 H H is the set of disagreements between two hypotheses in H.\n\nThe following simple lemma shows how we can make use of the H H-divergence in bounding the error of our hypothesis.\n\n\nLemma 3 For any hypotheses\nh, h \u2208 H, | S (h, h ) \u2212 T (h, h )| \u2264 1 2 d H H (D S , D T ). Proof By the definition of H H-distance, d H H (D S , D T ) = 2 sup h,h \u2208H Pr x\u223cD S h(x) = h (x) \u2212 Pr x\u223cD T h(x) = h (x) = 2 sup h,h \u2208H | S (h, h ) \u2212 T (h, h )| \u2265 2| S (h, h ) \u2212 T (h, h )|.\nWe are now ready to give a bound on target error in terms of the new divergence measure we have defined.\n\nTheorem 2 Let H be a hypothesis space of VC dimension d. If U S , U T are unlabeled samples of size m each, drawn from D S and D T respectively, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 (over the choice of the samples), for every h \u2208 H:\nT (h) \u2264 S (h) + 1 2d H H (U S , U T ) + 4 2d log(2m ) + log( 2 \u03b4 ) m + \u03bb.\nProof This proof relies on Lemma 3 and the triangle inequality for classification error (Ben-David et al. 2006;Crammer et al. 2008), which implies that for any labeling functions f 1 ,\nf 2 , and f 3 , we have (f 1 , f 2 ) \u2264 (f 1 , f 3 ) + (f 2 , f 3 ). Then T (h) \u2264 T (h * ) + T (h, h * ) \u2264 T (h * ) + S (h, h * ) + T (h, h * ) \u2212 S (h, h * ) \u2264 T (h * ) + S (h, h * ) + 1 2 d H H (D S , D T ) \u2264 T (h * ) + S (h) + S (h * ) + 1 2 d H H (D S , D T ) = S (h) + 1 2 d H H (D S , D T ) + \u03bb \u2264 S (h) + 1 2d H H (U S , U T ) + 4 2d log(2m ) + log( 2 \u03b4 ) m + \u03bb.\nThe last step is an application of Lemma 1, together with the observation that since we can represent every g \u2208 H H as a linear threshold network of depth 2 with 2 hidden units, the VC dimension of H H is at most twice the VC dimension of H (Anthony and Bartlett 1999).\n\nThe bound in Theorem 2 is relative to \u03bb, and we briefly comment that the form \u03bb = S (h * ) + T (h * ) comes from the use of the triangle inequality for classification error. Other losses result in other forms for this bound (Crammer et al. 2008). When the combined error of the ideal joint hypothesis is large, then there is no classifier that performs well on both the source and target domains, so we cannot hope to find a good target hypothesis by training only on the source domain. On the other hand, for small \u03bb (the most relevant case for domain adaptation), the bound shows that source error and unlabeled H H-divergence are important quantities in computing the target error.\n\n\nA learning bound combining source and target training data\n\nTheorem 2 shows how to relate source and target error. We now proceed to give a learning bound for empirical risk minimization using combined source and target training data.\n\nAt train time a learner receives a sample S = (S T , S S ) of m instances, where S T consists of \u03b2m instances drawn independently from D T and S S consists of (1 \u2212 \u03b2)m instances drawn independently from D S . The goal of a learner is to find a hypothesis that minimizes target error T (h). When \u03b2 is small, as in domain adaptation, minimizing empirical target error may not be the best choice. We analyze learners that instead minimize a convex combination of empirical source and target error,\n\u03b1 (h) = \u03b1\u02c6 T (h) + (1 \u2212 \u03b1)\u02c6 S (h),\nfor some \u03b1 \u2208 [0, 1]. We denote as \u03b1 (h) the corresponding weighted combination of true source and target errors, measured with respect to D S and D T .\n\nWe bound the target error of a domain adaptation algorithm that minimizes\u02c6 \u03b1 (h). The proof of the bound has two main components, which we state as lemmas below. First we bound the difference between the target error T (h) and weighted error \u03b1 (h). Then we bound the difference between the true and empirical weighted errors \u03b1 (h) and\u02c6 \u03b1 (h).\n\n\nLemma 4 Let h be a hypothesis in class H. Then\n| \u03b1 (h) \u2212 T (h)| \u2264 (1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb .\nProof See Appendix.\n\nThe lemma shows that as \u03b1 approaches 1, we rely increasingly on the target data, and the distance between domains matters less and less. The uniform convergence bound on the \u03b1-error is nearly identical to the standard uniform convergence bound for hypothesis classes of finite VC dimension (Vapnik 1998;Anthony and Bartlett 1999), only with target and source errors weighted differently. The key part of the proof relies on a slight modification of Hoeffding's inequality for our setup, which we state here:\n\nLemma 5 For a fixed hypothesis h, if a random labeled sample of size m is generated by drawing \u03b2m points from D T and (1 \u2212 \u03b2)m points from D S , and labeling them according to f S and f T respectively, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 (over the choice of the samples),\nPr |\u02c6 \u03b1 (h) \u2212 \u03b1 (h)| \u2265 \u2264 2 exp \u22122m 2 \u03b1 2 \u03b2 + (1\u2212\u03b1) 2 1\u2212\u03b2 .\nBefore giving the proof, we first restate Hoeffding's inequality for completeness.\nProposition 1 (Hoeffding's inequality) If X 1 , . . . , X n are independent random variables with a i \u2264 X i \u2264 b i for all i, then for any > 0, Pr |X \u2212 E[X]| \u2265 \u2264 2e \u22122n 2 2 / n i=1 (b i \u2212a i ) 2 , whereX = (X 1 + \u00b7 \u00b7 \u00b7 + X n )/n.\nWe are now ready to prove the lemma.\n\nProof (Lemma 5) Let X 1 , . . . , X \u03b2m be random variables that take on the values\n\u03b1 \u03b2 |h(x) \u2212 f T (x)|\nfor the \u03b2m instances x \u2208 S T . Similarly, let X \u03b2m+1 , . . . , X m be random variables that take on the values\n1 \u2212 \u03b1 1 \u2212 \u03b2 |h(x) \u2212 f S (x)| for the (1 \u2212 \u03b2)m instances x \u2208 S S . Note that X 1 , . . . , X \u03b2m \u2208 [0, \u03b1/\u03b2] and X \u03b2m+1 , . . . , X m \u2208 [0, (1 \u2212 \u03b1)/(1 \u2212 \u03b2)]. Then \u03b1 (h) = \u03b1\u02c6 T (h) + (1 \u2212 \u03b1)\u02c6 S (h) = \u03b1 1 \u03b2m x\u2208S T |h(x) \u2212 f T (x)| + (1 \u2212 \u03b1) 1 (1 \u2212 \u03b2)m x\u2208S S |h(x) \u2212 f S (x)| = 1 m m i=1 X i .\nFurthermore, by linearity of expectations\nE[\u02c6 \u03b1 (h)] = 1 m \u03b2m \u03b1 \u03b2 T (h) + (1 \u2212 \u03b2)m 1 \u2212 \u03b1 1 \u2212 \u03b2 S (h)) = \u03b1 T (h) + (1 \u2212 \u03b1) S (h) = \u03b1 (h).\nSo by Hoeffding's inequality the following holds for every h.\nPr |\u02c6 \u03b1 (h) \u2212 \u03b1 (h)| \u2265 \u2264 2 exp \u22122m 2 2 m i=1 range 2 (X i ) = 2 exp \u22122m 2 2 \u03b2m( \u03b1 \u03b2 ) 2 + (1 \u2212 \u03b2)m( 1\u2212\u03b1 1\u2212\u03b2 ) 2 = 2 exp \u22122m 2 \u03b1 2 \u03b2 + (1\u2212\u03b1) 2 1\u2212\u03b2 .\nThis lemma shows that as \u03b1 moves away from \u03b2 (where each instance is weighted equally), our finite sample approximation to \u03b1 (h) becomes less reliable. We can now move on to the main theorem of this section.\n\nTheorem 3 Let H be a hypothesis space of VC dimension d. Let U S and U T be unlabeled samples of size m each, drawn from D S and D T respectively. Let S be a labeled sample of size m generated by drawing \u03b2m points from D T and (1 \u2212 \u03b2)m points from D S and labeling them according to f S and f T , respectively. If\u0125 \u2208 H is the empirical minimizer of\u02c6 \u03b1 (h) on S and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 (over the choice of the samples),\nT (\u0125) \u2264 T (h * T ) + 4 \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2 2d log(2(m + 1)) + 2 log( 8 \u03b4 ) m + 2(1 \u2212 \u03b1) 1 2d H H (U S , U T ) + 4 2d log(2m ) + log( 8 \u03b4 ) m + \u03bb .\nThe proof follows the standard set of steps for proving learning bounds (Anthony and Bartlett 1999), using Lemma 4 to bound the difference between target and weighted errors and Lemma 5 for the uniform convergence of empirical and true weighted errors. The full proof is in Appendix.\n\nWhen \u03b1 = 0 (that is, we ignore target data), the bound is identical to that of Theorem 2, but with an empirical estimate for the source error. Similarly when \u03b1 = 1 (that is, we use only target data), the bound is the standard learning bound using only target data. At the optimal \u03b1 (which minimizes the right hand side), the bound is always at least as tight as either of these two settings. Finally note that by choosing different values of \u03b1, the bound allows us to effectively trade off the small amount of target data against the large amount of less relevant source data.\n\nWe remark that when it is known that \u03bb = 0, the dependence on m in Theorem 3 can be improved; this corresponds to the restricted or realizable setting.\n\n\nOptimal mixing value\n\nWe examine now the bound of Theorem 3 in more detail to illustrate some interesting properties. Writing the bound as a function of \u03b1 and omitting additive constants, we obtain\nf (\u03b1) = 2B \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2 + 2(1 \u2212 \u03b1)A,(1)\nwhere\nA = 1 2d H H (U S , U T ) + 4 2d log(2m ) + log( 4 \u03b4 ) m + \u03bb ,\nis the total divergence between source and target, and B = 4 2d log(2(m + 1)) + 2 log ( \n\u03b1 * (m T , m S ; D) = 1 m T \u2265 D 2 min{1, \u03bd} m T \u2264 D 2 ,(2)where \u03bd = m T m T + m S 1 + m S D 2 (m S + m T ) \u2212 m S m T .\nSeveral observations follow from this analysis. First, if m T = 0 (\u03b2 = 0) then \u03b1 * = 0 and if m S = 0 (\u03b2 = 1) then \u03b1 * = 1. That is, if we have only source or only target data, the best combination is to use exactly what we have. Second, if we are certain that the source and target are the same, that is if A = 0 (or D \u2192 \u221e), then \u03b1 * = \u03b2, that is, the optimal combination is to use the training data with uniform weighting of the examples across all examples, as in Crammer et al. (2008), who always enforce such a uniform weighing. Finally, two phase transitions occur in the value of \u03b1 * . First, if there are enough target data (specifically, if m T \u2265 D 2 = d/A 2 ) then no source data are needed, and in fact using any source data will yield suboptimal performance. This is because the possible reduction in error due to additional source data is always less than the increase in error caused by the source data being too far from the target data. Second, even if there are few target examples, it might be the case that we do not have enough source data to justify using it, and this small amount of source data should be ignored. Once we have enough source data then we get a non-trivial value for \u03b1 * .\n\nThese two phase transitions are illustrated in Fig. 1. The intensity of a point reflects the value \u03b1 * and ranges from 0 (white) to 1 (black). In this plot \u03b1 * is a function of m S (x axis) and m T (y axis), and we fix the complexity to d = 1,601 and the divergence between source and target to A = 0.715. We chose these values to correspond more closely to real data (see Sect. 7). Observe first that D 2 = 1,601/(0.715) 2 \u2248 3,130. When m T \u2265 D 2 , the first case of (2) predicts that \u03b1 * = 1 for all values of m S , which is illustrated by the black region above the line m T = 3,130. Furthermore, fixing the value of m T \u2264 3,130, the second case of (2) predicts that \u03b1 * will be either one (1) if m S is small enough, or go smoothly to zero as m S increases. This is illustrated by any horizontal line with m T \u2264 3,130. Each such line is black for small values of m S and then gradually becomes white as m S increases (left to right).\n\n\nResults on sentiment classification\n\nIn this section we illustrate our theory on the natural language processing task of sentiment classification (Pang et al. 2002). The point of these experiments is not to instantiate the Fig. 1 An illustration of the phase transition in the balance between source and target training data. The value of \u03b1 minimizing the bound is indicated by the intensity, where black means \u03b1 = 1. We fix d = 1,601 and A = 0.715, approximating the empirical setup in Fig. 3. The x-axis shows the number of source instances (log-scale). The y-axis shows the number of target instances. A phase transition occurs at 3,130 target instances. With more target instances than this, it is more effective to ignore even an infinite amount of source data bound from Theorem 3 directly, since the amount of data we use here is much too small for the bound to yield meaningful numerical results. Instead, we show how the two main principles of our theory from Sects. 4 and 5 can be applied on a real-world problem. First, we show that an approximation to the H-distance, obtained by training a linear model to discriminate between instances from different domains, correlates well with the loss incurred by training in one domain and testing in another. Second, we investigate minimizing the \u03b1-error as suggested by Theorem 3. We show that the optimal value of \u03b1 for a given amount of source and target data is closely related to our approximate H-distance.\n\nThe next subsection describes the problem of sentiment classification, along with our dataset, features, and learning algorithms. Then we show experimentally how our approximate H-distance is related to the adaptation performance and the optimal value of \u03b1.\n\n\nSentiment classification\n\nGiven a piece of text (usually a review or essay), automatic sentiment classification is the task of determining whether the sentiment expressed by the text is positive or negative (Pang et al. 2002;Turney 2002). While movie reviews are the most commonly studied domain, sentiment analysis has been extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen 2001;Thomas et al. 2006). Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs.\n\nWe used the publicly available data set from (Blitzer et al. 2007b) to examine our theory. 2 The data set consists of reviews from the Amazon website for several different types of products. We chose reviews from the domains apparel, books, DVDs, kitchen & housewares, and electronics. Each review consists of a rating (1-5 stars), a title, and review text. We created a binary classification problem by binning reviews with 1-2 stars as \"negative\" and 4-5 stars as \"positive\". Reviews with 3 stars were discarded.\n\nClassifying product reviews as having either positive or negative sentiment fits well into our theory of domain adaptation. We note that reviews for different products have widely Positive books review Negative books review Title: A great find during an annual summer shopping trip Review: I found this novel at a bookstore on the boardwalk I visit every summer....The narrative was brilliantly told, the dialogue completely believable and the plot totally heartwrenching. If I had made it to the end without some tears, I would believe myself made of stone! Title: The Hard Way Review: I am not sure whatever possessed me to buy this book. Honestly, it was a complete waste of my time. To quote a friend, it was not the best use of my entertainment dollar. If you are a fan of pedestrian writing, lack-luster plots and hackneyed character development, this is your book.\n\n\nPositive kitchen & housewares review\n\nNegative kitchen & housewares review Title: no more doggy feuds with neighbor Review: i absolutely love this product. my neighbor has four little yippers and my shepard/chow mix was antagonized by the yipping on our side of the fence. I hung the device on my side of the fence and the noise keeps the neighbors dog from picking \"arguments\" with my dog. all barking and fighting has ceased.\n\nTitle: cooks great, lid does not work well. . . Review: I Love the way the Tefal deep fryer cooks, however, I am returning my second one due to a defective lid closure. The lid may close initially, but after a few uses it no longer stays closed.\n\nSince I have small children in my home, I will not be purchasing this one again. On the other hand, a single good universal sentiment classifier is likely to exist-namely the classifier that assigns high positive weight to all positive words and high negative weight to all negative words, regardless of product type. We illustrate the type of text in this dataset in Fig. 2, which shows one positive and one negative review each from the domains books and kitchen & housewares.\n\nFor each domain, the data set contains 1,600 labeled documents and between 5,000 and 6,000 unlabeled documents. We follow Pang et al. (2002) and represent each instance (review) by a sparse vector containing the counts of its unigrams and bigrams, and we normalize the vectors in L 1 . Finally, we discard all but the most frequent 1,600 unigrams and bigrams from each data set. In all of the learning problems of the next section, including those that require us to estimate an approximate H-distance, we use signed linear classifiers. To estimate the parameters of these classifiers, we minimize a Huber loss with stochastic gradient descent (Zhang 2004).\n\n\nExperiments\n\nWe explore Theorem 3 further by comparing its predictions to the predictions of an approximation that can be computed from finite labeled source and unlabeled source and target samples. As we shall see, our approximation is a finite-sample analog of (1). We first address \u03bb, the error of the ideal hypothesis. Unfortunately, in general we cannot assume any relationship between the labeling functions f S and f T . Thus in order to estimate \u03bb, we must estimate T (h * ) independently of the source data. If we had enough target data to do this accurately, we would not need to adapt a source classifier in the first place. For our sentiment task, however, \u03bb is small enough to be a negligible term in the bound. Thus we ignore it here.\n\nWe approximate the divergence between two domains by training a linear classifier to discriminate between unlabeled instances from the source and target domains. Then we apply Lemma 2 to get an estimate ofd H that we denote by \u03b6(U S , U T ). \u03b6(U S , U T ) is a lower bound ond H , which is in turn a lower bound ond H H . For Theorem 3 to be valid, we need an upper bound ond H H . Unfortunately, this is computationally intractable for linear threshold classifiers, since finding a minimum error classifier is hard in general (Ben-David et al. 2003). We chose our \u03b6(U S , U T ) estimate because it requires no new machinery beyond an algorithm for empirical risk minimization on H. Finally, we note that the unlabeled sample size m is large, so we leave out the finite sample error term for the H H-divergence. We set C to be 1,601, the VC dimension of a 1,600-dimensional linear classifier and ignore the log m term in the numerator of the bound. The complete approximation to the bound is\nf (\u03b1) = C m \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2 + (1 \u2212 \u03b1)\u03b6(U S , U T ).(3)\nNote that C m in (3) corresponds to B from (1), and \u03b6(U S , U T ) is a finite sample approximation to A when \u03bb is negligible and we have large unlabeled samples from both the source and target domains.\n\nWe compare (3) to experimental results for the sentiment classification task. All of our experiments use the apparel domain as the target. We obtain empirical curves for the error  (1 \u2212 \u03b2). Figure 3 shows a series of plots of (3) (top row) coupled with corresponding plots of test error (bottom row) as a function of \u03b1 for different amounts of source and target data and different distances between domains. In each column, a single parameter (distance, number of target instances m T , or number of source instances m S ) is varied while the other two are held constant. Note that \u03b2 = m T /(m T +m S ). The plots on the top row of Fig. 3 are not meant to be numerical proxies for the true error. (For the source domains \"books\" and \"dvd\", the distance alone is well above 1/2.) However, they illustrate that the bound is similar in shape to the true error curve and that important relationships are preserved.\n\nNote that in every pair of plots, the empirical error curves, like the bounds, have an essentially convex shape. Furthermore, the value of \u03b1 that minimizes the bound also yields low empirical error in each case. This suggests that choosing \u03b1 to minimize the bound of Theorem 3 and subsequently training a classifier to minimize the empirical error\u02c6 \u03b1 (h) can work well in practice, provided we have a reasonable measure of complexity and \u03bb is small. Column (a) shows that more distant source domains result in higher target error. Column (b) illustrates that for more target data, we have not only lower error in general, but also a higher minimizing \u03b1. Finally, column (c) demonstrates the limitations of distant source data.\n\nWhen enough labeled target data exists, we always prefer to use only the target data, no matter how much source data is available. Intuitively this is because any biased source domain cannot help to reduce error beyond some positive constant. When the target data alone is sufficient to surpass this level of performance, the source data ceases to be useful. Thus column (c) illustrates empirically one phase transition we discuss in Sect. 6.\n\n\nCombining data from multiple sources\n\nWe now explore an extension of our theory to the case of multiple source domains. In this setting, the learner is presented with data from N distinct sources. Each source S j is associated with an unknown distribution D j over input points and an unknown labeling function f j . The learner receives a total of m labeled samples, with m j = \u03b2 j m from each source S j , and the objective is to use these samples to train a model to perform well on a target domain D T , f T , which may or may not be one of the sources. This setting is motivated by several domain adaptation algorithms (Huang et al. 2007;Bickel et al. 2007;Jiang and Zhai 2007;Dai et al. 2007) that weigh the loss from training instances depending on how \"far\" they are from the target domain. That is, each training instance is its own source domain.\n\nAs before, we examine algorithms that minimize convex combinations of training error over the labeled examples from each source domain. Given a vector \u03b1 = (\u03b1 1 , . . . , \u03b1 N ) of domain weights with N j =1 \u03b1 j = 1, we define the empirical \u03b1-weighted error of function h as\u02c6\n\u03b1 (h) = N j =1 \u03b1 j\u02c6 j (h) = N j =1 \u03b1 j m j x\u2208S j |h(x) \u2212 f j (x)|.\nThe true \u03b1-weighted error \u03b1 (h) is defined analogously. We use D \u03b1 to denote the mixture of the N source distributions with mixing weights equal to the components of \u03b1. We present in turn two alternative generalizations of the bounds in Sect. 5. The first bound considers the quality and quantity of data available from each source individually, ignoring the relationships between sources. In contrast, the second bound depends directly on the H H-distance between the target domain and the weighted combination of source domains. This dependence allows us to achieve significantly tighter bounds when there exists a mixture of sources that approximates the target better than any single source. Both results require the derivation of uniform convergence bounds for the empirical \u03b1-error. We begin with those.\n\n\nUniform convergence\n\nThe following lemma provides a uniform convergence bound for the empirical \u03b1-error.\n\nLemma 6 For each j \u2208 {1, . . . , N}, let S j be a labeled sample of size \u03b2 j m generated by drawing \u03b2 j m points from D j and labeling them according to f j . For any fixed weight vector \u03b1, let\u02c6 \u03b1 (h) be the empirical \u03b1-weighted error of some fixed hypothesis h on this sample, and let \u03b1 (h) be the true \u03b1-weighted error. Then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4:\nPr |\u02c6 \u03b1 (h) \u2212 \u03b1 (h)| \u2265 \u2264 2 exp \u22122m 2 N j =1 \u03b1 2 j \u03b2 j .\nProof See Appendix.\n\nNote that this bound is minimized when \u03b1 j = \u03b2 j for all j . In other words, convergence is fastest when all data instances are weighted equally.\n\n\nA bound using pairwise divergence\n\nThe first bound we present considers the pairwise H H-distance between each source and the target, and illustrates the trade-off that exists between minimizing the average divergence of the training data from the target and weighting all points equally to encourage faster convergence. The term N j =1 \u03b1 j \u03bb j that appears in this bound plays a role corresponding to \u03bb in the previous section. Somewhat surprisingly, this term can be small even when there is not a single hypothesis that works well for all heavily weighted sources.\n\n\nTheorem 4 Let H be a hypothesis space of VC dimension d.\n\nFor each j \u2208 {1, . . . , N}, let S j be a labeled sample of size \u03b2 j m generated by drawing \u03b2 j m points from D j and labeling them according to f j . If\u0125 \u2208 H is the empirical minimizer of\u02c6 \u03b1 (h) for a fixed weight vector \u03b1 on these samples and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\nT (\u0125) \u2264 T (h * T ) + 2 N j =1 \u03b1 2 j \u03b2 j d log(2m) \u2212 log(\u03b4) 2m + N j =1 \u03b1 j 2\u03bb j + d H H (D j , D T ) , where \u03bb j = min h\u2208H { T (h) + j (h)}.\nProof See Appendix.\n\nIn the special case where the H H-divergence between each source and the target is 0 and all data instances are weighted equally, the bound in Theorem 4 becomes\nT (\u0125) \u2264 T (h * T ) + 2 2d log(2(m + 1)) + 2 log( 4 \u03b4 ) m + 2 N j =1 \u03b1 j \u03bb j .\nThis bound is nearly identical to the multiple source classification bound given in Theorem 6 of Crammer et al. (2008). Aside from the constants in the complexity term, the only difference is that the quantity \u03bb i that appears here is replaced by an alternate measure of the label error between source S j and the target. Furthermore, these measures are equivalent when the true target function is a member of H. However, the bound of Crammer et al. (2008) is less general. In particular, it does not handle positive H H-divergence or non-uniform weighting of the data.\n\n\nA bound using combined divergence\n\nIn the previous bound, divergence between domains is measured only on pairs, so it is not necessary to have a single hypothesis that is good for every source domain. However, this bound does not give us the flexibility to take advantage of domain structure when calculating unlabeled divergence. The alternate bound given in Theorem 5 allows us to effectively alter the source distribution by changing \u03b1. This has two consequences. First, we must now demand that there exists a hypothesis h * which has low error on both the \u03b1-weighted convex combination of sources and the target domain. Second, we measure H H-divergence between the target and a mixture of sources, rather than between the target and each single source.\n\nTheorem 5 Let H be a hypothesis space of VC dimension d. For each j \u2208 {1, . . . , N}, let S j be a labeled sample of size \u03b2 j m generated by drawing \u03b2 j m points from D j and labeling them according to f j . If\u0125 \u2208 H is the empirical minimizer of\u02c6 \u03b1 (h) for a fixed weight vector \u03b1 on these samples and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\nT (\u0125) \u2264 T (h * T ) + 4 N j =1 \u03b1 2 j \u03b2 j d log(2m) \u2212 log(\u03b4) 2m + 2\u03b3 \u03b1 + d H H (D \u03b1 , D T ), where \u03b3 \u03b1 = min h { T (h) + \u03b1 (h)} = min h { T (h) + N j =1 \u03b1 j j (h)}.\nProof See Appendix.\n\nTheorem 5 reduces to Theorem 3 when N = 2 and one of the two source domains is the target domain (that is, we have some small number of target instances).\n\n\nDiscussion\n\nOne might ask whether there exist settings where a non-uniform weighting can lead to a significantly lower value of the bound than a uniform weighting. Indeed, this can happen if some non-uniform weighting of sources accurately approximates the target distribution. This is true, for example, in the setting studied by Mansour et al. (2009aMansour et al. ( , 2009b, who derive results for combining pre-computed hypotheses. In particular, they show that for arbitrary convex losses, if the R\u00e9nyi divergence between the target and a mixture of sources is small, it is possible to combine low-error source hypotheses to create a low-error target hypothesis. They then show that if for each domain j there exists a hypothesis h j with error less than , it is possible to achieve an error less than on the target by weighting the predictions of h 1 , . . . , h N appropriately.\n\nThe R\u00e9nyi divergence is not directly comparable to the H H-divergence in general; however it is possible to exhibit source and target distributions which have low H Hdivergence and high (even infinite) R\u00e9nyi divergence. For example, the R\u00e9nyi divergence is infinite when the source and target distributions do not share support, but the H Hdivergence is only large when these regions of differing support also coincide with classifier disagreement regions. On the other hand, we require that a single hypothesis be trained on the mixture of sources. Mansour et al. (2009aMansour et al. ( , 2009b give algorithms which do not require the original training data at all, but only a single hypothesis from each source.\n\n\nConclusion\n\nWe presented a theoretical investigation of the task of domain adaptation, a task in which we have a large amount of training data from a source domain, but we wish to apply a model in a target domain with a much smaller amount of training data. Our main result is a uniform convergence learning bound for algorithms which minimize convex combinations of empirical source and target error. Our bound reflects the trade-off between the size of the source data and the accuracy of the target data, and we give a simple approximation to it that is computable from finite labeled and unlabeled samples. This approximation makes correct predictions about model test error for a sentiment classification task. Our theory also extends in a straightforward manner to a multi-source setting, which we believe helps to explain the success of recent empirical work in domain adaptation.\n\nThere are two interesting open problems that deserve future exploration. First, our bounds on the divergence between source and target distribution are in terms of VC dimension. We do not yet know whether our divergence measure admits tighter data-dependent bounds (McAllester 2003;Bartlett and Mendelson 2002), or if there are other, more appropriate divergence measures which do. Second, it would be interesting to investigate algorithms that choose a convex combination of multiple sources to minimize the bound in Theorem 5 as possible approaches to adaptation from multiple sources.\n\nOpen Access This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.\n\n\nAppendix: Proofs\n\nTheorem 1 For a hypothesis h,\nT (h) \u2264 S (h) + d 1 (D S , D T ) + min E D S |f S (x) \u2212 f T (x)| , E D T |f S (x) \u2212 f T (x)| .\nProof Recall that T (h) = T (h, f T ) and S (h) = S (h, f S ). Let \u03c6 S and \u03c6 T be the density functions of D S and D T respectively.\nT (h) = T (h) + S (h) \u2212 S (h) + S (h, f T ) \u2212 S (h, f T ) \u2264 S (h) + | S (h, f T ) \u2212 S (h, f S )| + | T (h, f T ) \u2212 S (h, f T )| \u2264 S (h) + E D S |f S (x) \u2212 f T (x)| + | T (h, f T ) \u2212 S (h, f T )| \u2264 S (h) + E D S |f S (x) \u2212 f T (x)| + |\u03c6 S (x) \u2212 \u03c6 T (x)||h(x) \u2212 f T (x)|dx \u2264 S (h) + E D S |f S (x) \u2212 f T (x)| + d 1 (D S , D T ).\nIn the first line, we could instead choose to add and subtract T (h, f S ) rather than S (h, f T ), which would result in the same bound only with the expectation taken with respect to D T instead of D S . Choosing the smaller of the two gives us the bound. \nI x \u2208 U = 1 2m x:h(x)=0 I [x \u2208 U ] + I x \u2208 U + 1 2m x:h(x)=1 I [x \u2208 U ] + I x \u2208 U \u2212 1 m x:h(x)=0 I [x \u2208 U ] + x:h(x)=1 I x \u2208 U = 1 2m x:h(x)=0 I x \u2208 U \u2212 I [x \u2208 U ] + 1 2m x:h(x)=1 I [x \u2208 U ] \u2212 I x \u2208 U = 1 2 (1 \u2212 Pr U [I (h)] \u2212 (1 \u2212 Pr U [I (h)])) + 1 2 (Pr U [I (h)] \u2212 Pr U [I (h)]) = Pr U [I (h)] \u2212 Pr U [I (h)] .(4)\nThe absolute value in the statement of the lemma follows from the symmetry of H.\n\n\nLemma 4 Let h be a hypothesis in class H. Then\n| \u03b1 (h) \u2212 T (h)| \u2264 (1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb .\nProof Similarly to the proof of Theorem 2, this proof relies heavily on the triangle inequality for classification error.\n| \u03b1 (h) \u2212 T (h)| = (1 \u2212 \u03b1)| S (h) \u2212 T (h)| \u2264 (1 \u2212 \u03b1) | S (h) \u2212 S (h, h * )| + | S (h, h * ) \u2212 T (h, h * )| + | T (h, h * ) \u2212 T (h)| \u2264 (1 \u2212 \u03b1) S (h * ) + | S (h, h * ) \u2212 T (h, h * )| + T (h * ) \u2264 (1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb .\nTheorem 3 Let H be a hypothesis space of VC dimension d. Let U S and U T be unlabeled samples of size m each, drawn from D S and D T respectively. Let S be a labeled sample of size m generated by drawing \u03b2m points from D T and (1 \u2212 \u03b2)m points from D S , labeling them according to f S and f T , respectively. If\u0125 \u2208 H is the empirical minimizer of\u02c6 \u03b1 (h) on S and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 (over the choice of the samples),\nT (\u0125) \u2264 T (h * T ) + 4 \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2 2d log (2(m + 1)) + 2 log 8 \u03b4 m + 2(1 \u2212 \u03b1) 1 2d H H (U S , U T ) + 4 2d log(2m ) + log( 4 \u03b4 ) m + \u03bb .\nProof The complete proof is mostly identical to the standard proof of uniform convergence for empirical risk minimizers. We show here the steps that are different. Below we use L4, and Thm2 to indicate that a line of the proof follows by application of Lemma 4 or Theorem 2 respectively. L5 indicates that the proof follows by Lemma 5, but also relies on sample symmetrization and bounding the growth function by the VC dimension (Anthony and Bartlett 1999).\nT (\u0125) \u2264 \u03b1 (\u0125) + (1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb (L4) \u2264\u02c6 \u03b1 (\u0125) + 2 \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2 2d log(2(m + 1)) + 2 log( 8 \u03b4 ) m + (1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb (L5) \u2264\u02c6 \u03b1 (h * T ) + 2 \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2\n2d log(2(m + 1)) + 2 log( 8 \u03b4 ) m\n+ (1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb \u0125 = arg min h\u2208H\u02c6 \u03b1 (h) \u2264 \u03b1 (h * T ) + 4 \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2\n2d log(2(m + 1)) + 2 log( 8 \u03b4 ) m\n+ (1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb (L5) \u2264 T (h * T ) + 4 \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2\n2d log(2(m + 1)) + 2 log( 8 \u03b4 ) m\n+ 2(1 \u2212 \u03b1) 1 2 d H H (D S , D T ) + \u03bb (L4) \u2264 T (h * T ) + 4 \u03b1 2 \u03b2 + (1 \u2212 \u03b1) 2 1 \u2212 \u03b2\n2d log(2(m + 1)) + 2 log( 8 \u03b4 ) m + 2(1 \u2212 \u03b1) 1 2d H H (U S , U T ) + 4 2d log(2m ) + log( 4 \u03b4 ) m + \u03bb (Thm 2) Lemma 6 For each j \u2208 {1, . . . , N}, let S j be a labeled sample of size \u03b2 j m generated by drawing \u03b2 j m points from D j and labeling them according to f j . For any fixed weight vector \u03b1, let\u02c6 \u03b1 (h) be the empirical \u03b1-weighted error of some fixed hypothesis h on this sample, and let \u03b1 (h) be the true \u03b1-weighted error. Then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4:\nPr |\u02c6 \u03b1 (h) \u2212 \u03b1 (h)| \u2265 \u2264 2 exp \u22122m 2 N j =1 \u03b1 2 j \u03b2 j .\nProof Due to its similarity to the proof of Lemma 5, we omit some details of this proof, and concentrate only on the parts that differ. For each source j , let X j,1 , . . . , X j,\u03b2 j m be random variables that take on the values (\u03b1 j /\u03b2 j )|h(x)\u2212f j (x)| for the \u03b2 j m instances x \u2208 S j . Note that X j,1 , . . . , X j,\u03b2 j m \u2208 [0, \u03b1 j /\u03b2 j ]. \nPr |\u02c6 \u03b1 (h) \u2212 \u03b1 (h)| \u2265 \u2264 2 exp \u22122m 2 2 N j =1 \u03b2 j m i=1 range 2 (X j,i ) = 2 exp \u22122m 2 N j =1 \u03b1 2 j \u03b2 j .\nTheorem 4 Let H be a hypothesis space of VC dimension d. For each j \u2208 {1, . . . , N}, let S j be a labeled sample of size \u03b2 j m generated by drawing \u03b2 j m points from D j and labeling them according to f j . If\u0125 \u2208 H is the empirical minimizer of\u02c6 \u03b1 (h) for a fixed weight vector \u03b1 on these samples and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\nT (\u0125) \u2264 T (h * T ) + 4 N j =1\n\u03b1 2 j \u03b2 j 2d log(2(m + 1)) + log( 4 \u03b4 ) m\n+ N j =1 \u03b1 j 2\u03bb j + d H H (D j , D T ) , where \u03bb j = min h\u2208H { T (h) + j (h)}. Proof Let h * j = argmin h { T (h) + j (h)}. Then | \u03b1 (h) \u2212 T (h)| = N j =1 \u03b1 j j (h) \u2212 T (h) \u2264 N j =1 \u03b1 j j (h) \u2212 T (h) \u2264 N j =1 \u03b1 j j (h) \u2212 j (h, h * j ) + j (h, h * j ) \u2212 T (h, h * j ) + T (h, h * j ) \u2212 T (h) \u2264 N j =1 \u03b1 j j (h * j ) + j (h, h * j ) \u2212 T (h, h * j ) + T (h * j ) \u2264 N j =1 \u03b1 j \u03bb j + 1 2 d H H (D j , D T ) .\nThe third line follows from the triangle inequality. The last line follows from the definition of \u03bb j and Lemma 3. Putting this together with Lemma 6, we find that for any \u03b4 \u2208 (0, 1), with probability 1 \u2212 \u03b4,\nT (\u0125) \u2264 \u03b1 (\u0125) + N j =1 \u03b1 j \u03bb j + 1 2 d H H (D j , D T ) \u2264\u02c6 \u03b1 (\u0125) + 2 N j =1 \u03b1 2 j \u03b2 j\n2d log(2(m + 1)) + log( 4 \u03b4 ) m\n+ N j =1 \u03b1 j \u03bb j + 1 2 d H H (D j , D T ) \u2264\u02c6 \u03b1 (h * T ) + 2 N j =1 \u03b1 2 j \u03b2 j\n2d log(2(m + 1)) + log( 4 \u03b4 ) m\n+ N j =1 \u03b1 j \u03bb j + 1 2 d H H (D j , D T ) \u2264 \u03b1 (h * T ) + 4 N j =1\n\u03b1 2 j \u03b2 j 2d log(2(m + 1)) + log( 4 \u03b4 ) m\n+ N j =1 \u03b1 j \u03bb j + 1 2 d H H (D j , D T ) \u2264 T (h * T ) + 4 N j =1\n\u03b1 2 j \u03b2 j 2d log(2(m + 1)) + log( 4 \u03b4 ) m + N j =1 \u03b1 j 2\u03bb j + d H H (D j , D T ) .\n\nTheorem 5 Let H be a hypothesis space of VC dimension d. For each j \u2208 {1, . . . , N}, let S j be a labeled sample of size \u03b2 j m generated by drawing \u03b2 j m points from D j and labeling them according to f j . If\u0125 \u2208 H is the empirical minimizer of\u02c6 \u03b1 (h) for a fixed weight vector \u03b1 on these samples and h * T = min h\u2208H T (h) is the target error minimizer, then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\nT (\u0125) \u2264 T (h * T ) + 2 N j =1\n\u03b1 2 j \u03b2 j 2d log(2(m + 1)) + log( 4 \u03b4 ) m\n+ 2 \u03b3 \u03b1 + 1 2 d H H (D \u03b1 , D T ) , where \u03b3 \u03b1 = min h { T (h) + \u03b1 (h)} = min h { T (h) + N j =1 \u03b1 j j (h)}.\nProof The proof is almost identical to that of Theorem 4 with minor modifications to the derivation of the bound on | \u03b1 (h)\u2212 T (h)|. Let h * = argmin h { T (h)+ \u03b1 (h)}. By the triangle inequality and Lemma 3,\n| \u03b1 (h) \u2212 T (h)| \u2264 \u03b1 (h) \u2212 \u03b1 (h, h * ) + \u03b1 (h, h * ) \u2212 T (h, h * ) + T (h, h * ) \u2212 T (h) \u2264 \u03b1 (h * ) + \u03b1 (h, h * ) \u2212 T (h, h * ) + T (h * ) \u2264 \u03b3 + 1 2 d H H (D \u03b1 , D T ).\nThe remainder of the proof is unchanged.\n\n\ncomplexity term, which is approximately \u221a d/m. The optimal value \u03b1 * is a function of the number of target examples m T = \u03b2m, the number of source examples m S = (1 \u2212 \u03b2)m, and the ratio D = \u221a d/A:\n\nFig. 2\n2Some sample product reviews for sentiment classification. The top row shows reviews from the books domain. The bottom row shows reviews from kitchen & housewares different vocabularies, so classifiers trained on one domain are likely to miss out on important lexical cues in a different domain.\n\nFig. 3\n3Comparing the bound from Theorem 3 with test error for sentiment classification. Each column varies one component of the bound. For all plots, the y-axis shows the error and the x-axis shows \u03b1. Plots on the top row show the value given by our approximation to the bound, and plots on the bottom row show the empirical test set error. Column (a) depicts different distances among domains. Column (b) depicts different numbers of target instances, and column (c) represents different numbers of source instances as a function of \u03b1 by training a classifier using a weighted hinge loss. Suppose the target domain has weight \u03b1 and there are \u03b2m target training instances. Then we scale the loss of target training instance by \u03b1/\u03b2 and the loss of a source training instance by (1 \u2212 \u03b1)/\n\n\nLemma 2 For a symmetric hypothesis class H (one where for every h \u2208 H, the inverse hypothesis 1 \u2212 h is also in H) and samples U , U of size m, the empirical H-distance isd H (U, U ) = 2 1 \u2212 min where I [x \u2208 U ] is the binary indicator variable which is 1 when x \u2208 U .Proof We will show that for any hypothesis h and corresponding set I (h) of positively labeled instances,I [x \u2208 U ] = Pr U [I (h)] \u2212 Pr U [I (h)] .h\u2208H \n\n1 \nm \n\nx:h(x)=0 \n\nI [x \u2208 U ] + \n1 \nm \n\nx:h(x)=1 \n\nI [x \u2208 U ] \n\n1 \u2212 \n1 \nm \n\nx:h(x)=0 \n\nI [x \u2208 U ] + \n1 \nm \n\nx:h(x)=1 \n\nWe have \n\n1 \u2212 \n1 \nm \n\nx:h(x)=0 \n\nI [x \u2208 U ] + \n\nx:h(x)=1 \n\n\n\n\nThen\u02c6 \u03b1 (h) = |h(x) \u2212 f j (x)| = 1 mBy linearity of expectations, we have that E[\u02c6 \u03b1 (h)] = \u03b1 (h), and so by Hoeffding's inequality, for every h \u2208 H,N \n\nj =1 \n\n\u03b1 j\u02c6 j (h) = \n\nN \n\nj =1 \n\n\u03b1 j \n1 \n\u03b2 j m \n\nx\u2208S j \n\nN \n\nj =1 \n\n\u03b2 j m \n\ni=1 \n\nX j,i . \n\n\nNote that this notion of domain is not the domain of a function. We always mean a specific distribution and function pair when we say \"domain.\"\nAvailable at http://www.cs.jhu.edu/~mdredze/.\n\nA framework for learning predictive structures from multiple tasks and unlabeled data. R Ando, T Zhang, Journal of Machine Learning Research. 6Ando, R., & Zhang, T. (2005). A framework for learning predictive structures from multiple tasks and unla- beled data. Journal of Machine Learning Research, 6, 1817-1853.\n\nM Anthony, P Bartlett, Neural network learning: theoretical foundations. CambridgeCambridge University PressAnthony, M., & Bartlett, P. (1999). Neural network learning: theoretical foundations. Cambridge: Cambridge University Press.\n\nRademacher and Gaussian complexities: risk bounds and structural results. P Bartlett, S Mendelson, Journal of Machine Learning Research. 3Bartlett, P., & Mendelson, S. (2002). Rademacher and Gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3, 463-482.\n\nTesting that distributions are close. T Batu, L Fortnow, R Rubinfeld, W Smith, P White, IEEE symposium on foundations of computer science. 41Batu, T., Fortnow, L., Rubinfeld, R., Smith, W., & White, P. (2000). Testing that distributions are close. In: IEEE symposium on foundations of computer science (Vol. 41, pp. 259-269).\n\nA model of inductive bias learning. J Baxter, Journal of Artificial Intelligence Research. 12Baxter, J. (2000). A model of inductive bias learning. Journal of Artificial Intelligence Research, 12, 149- 198.\n\nOn the difficulty of approximately maximizing agreements. S Ben-David, N Eiron, P Long, Journal of Computer and System Sciences. 66Ben-David, S., Eiron, N., & Long, P. (2003). On the difficulty of approximately maximizing agreements. Journal of Computer and System Sciences, 66, 496-514.\n\nAnalysis of representations for domain adaptation. S Ben-David, J Blitzer, K Crammer, F Pereira, Advances in neural information processing systems. Ben-David, S., Blitzer, J., Crammer, K., & Pereira, F. (2006). Analysis of representations for domain adapta- tion. In: Advances in neural information processing systems.\n\nDiscriminative learning for differing training and test distributions. S Bickel, M Br\u00fcckner, T Scheffer, Proceedings of the international conference on machine learning. the international conference on machine learningBickel, S., Br\u00fcckner, M., & Scheffer, T. (2007). Discriminative learning for differing training and test distri- butions. In: Proceedings of the international conference on machine learning.\n\nNymble: a high-performance learning namefinder. D Bikel, S Miller, R Schwartz, R Weischedel, Conference on applied natural language processing. Bikel, D., Miller, S., Schwartz, R., & Weischedel, R. (1997). Nymble: a high-performance learning name- finder. In: Conference on applied natural language processing.\n\nLearning bounds for domain adaptation. J Blitzer, K Crammer, A Kulesza, F Pereira, J Wortman, Advances in neural information processing systems. Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., & Wortman, J. (2007a). Learning bounds for domain adap- tation. In: Advances in neural information processing systems.\n\nBiographies, Bollywood, boomboxes and blenders: domain adaptation for sentiment classification. J Blitzer, M Dredze, F Pereira, ACLBlitzer, J., Dredze, M., & Pereira, F. (2007b) Biographies, Bollywood, boomboxes and blenders: domain adaptation for sentiment classification. In: ACL.\n\nHead-driven statistical models for natural language parsing. M Collins, University of PennsylvaniaPhD thesisCollins, M. (1999). Head-driven statistical models for natural language parsing. PhD thesis, University of Pennsylvania.\n\nSample selection bias correction theory. C Cortes, M Mohri, M Riley, A Rostamizadeh, Proceedings of the 19th annual conference on algorithmic learning theory. the 19th annual conference on algorithmic learning theoryCortes, C., Mohri, M., Riley, M., & Rostamizadeh, A. (2008). Sample selection bias correction theory. In: Proceedings of the 19th annual conference on algorithmic learning theory.\n\nLearning from multiple sources. K Crammer, M Kearns, J Wortman, Journal of Machine Learning Research. 9Crammer, K., Kearns, M., & Wortman, J. (2008). Learning from multiple sources. Journal of Machine Learn- ing Research, 9, 1757-1774.\n\nBoosting for transfer learning. W Dai, Q Yang, G Xue, Y Yu, Proceedings of the international conference on machine learning. the international conference on machine learningDai, W., Yang, Q., Xue, G., & Yu, Y. (2007). Boosting for transfer learning. In: Proceedings of the interna- tional conference on machine learning.\n\nYahoo! for Amazon: extracting market sentiment from stock message boards. S Das, M Chen, Proceedings of the Asia pacific finance association annual conference. the Asia pacific finance association annual conferenceDas, S., & Chen, M. (2001). Yahoo! for Amazon: extracting market sentiment from stock message boards. In: Proceedings of the Asia pacific finance association annual conference.\n\nFrustratingly easy domain adaptation. H Daum\u00e9, Association for computational linguistics (ACL). Daum\u00e9, H. (2007). Frustratingly easy domain adaptation. In: Association for computational linguistics (ACL).\n\nHierarchical Bayesian domain adaptation. J R Finkel, C D Manning, Proceedings of the north American association for computational linguistics. the north American association for computational linguisticsFinkel, J. R. Manning, C. D. (2009). Hierarchical Bayesian domain adaptation. In: Proceedings of the north American association for computational linguistics.\n\nSample selection bias as a specification error. J Heckman, Econometrica. 47Heckman, J. (1979). Sample selection bias as a specification error. Econometrica, 47, 153-161.\n\nCorrecting sample selection bias by unlabeled data. J Huang, A Smola, A Gretton, K Borgwardt, B Schoelkopf, Advances in neural information processing systems. Huang, J., Smola, A., Gretton, A., Borgwardt, K., & Schoelkopf, B. (2007). Correcting sample selection bias by unlabeled data. In: Advances in neural information processing systems.\n\nInstance weighting for domain adaptation. J Jiang, C Zhai, Proceedings of the association for computational linguistics. the association for computational linguisticsJiang, J., & Zhai, C. (2007). Instance weighting for domain adaptation. In: Proceedings of the association for computational linguistics.\n\nDetecting change in data streams. D Kifer, S Ben-David, J Gehrke, Ver large databasesKifer, D., Ben-David, S., & Gehrke, J. (2004). Detecting change in data streams. In: Ver large databases.\n\nA Bayesian divergence prior for classification adaptation. X Li, J Bilmes, Proceedings of the international conference on artificial intelligence and statistics. the international conference on artificial intelligence and statisticsLi, X., & Bilmes, J. (2007). A Bayesian divergence prior for classification adaptation. In: Proceedings of the international conference on artificial intelligence and statistics.\n\nDomain adaptation with multiple sources. Y Mansour, M Mohri, A Rostamizadeh, Advances in neural information processing systems. Mansour, Y., Mohri, M., & Rostamizadeh, A. (2009a). Domain adaptation with multiple sources. In: Ad- vances in neural information processing systems.\n\nMultiple source adaptation and the r\u00e9nyi divergence. Y Mansour, M Mohri, A Rostamizadeh, Proceedings of the conference on uncertainty in artificial intelligence. the conference on uncertainty in artificial intelligenceMansour, Y., Mohri, M., & Rostamizadeh, A. (2009b). Multiple source adaptation and the r\u00e9nyi divergence. In: Proceedings of the conference on uncertainty in artificial intelligence.\n\nSimplified PAC-Bayesian margin bounds. D Mcallester, Proceedings of the sixteenth annual conference on learning theory. the sixteenth annual conference on learning theoryMcAllester, D. (2003). Simplified PAC-Bayesian margin bounds. In: Proceedings of the sixteenth annual conference on learning theory.\n\nThumbs up? Sentiment classification using machine learning techniques. B Pang, L Lee, S Vaithyanathan, Proceedings of empirical methods in natural language processing. empirical methods in natural language processingPang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using machine learning techniques. In: Proceedings of empirical methods in natural language processing.\n\nA maximum entropy model for part-of-speech tagging. A Ratnaparkhi, Proceedings of empirical methods in natural language processing. empirical methods in natural language processingRatnaparkhi, A. (1996). A maximum entropy model for part-of-speech tagging. In: Proceedings of empirical methods in natural language processing.\n\nDirect importance estimation for covariate shift adaptation. M Sugiyama, T Suzuki, S Nakajima, H Kashima, P Von B\u00fcnau, M Kawanabe, Annals of the Institute of Statistical Mathematics. 60Sugiyama, M., Suzuki, T., Nakajima, S., Kashima, H., von B\u00fcnau, P., & Kawanabe, M. (2008). Direct im- portance estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics, 60, 699-746.\n\nGet out the vote: determining support or opposition from congressional floor-debate transcripts. M Thomas, B Pang, L Lee, Proceedings of empirical methods in natural language processing. empirical methods in natural language processingThomas, M., Pang, B., & Lee, L. (2006). Get out the vote: determining support or opposition from congres- sional floor-debate transcripts. In: Proceedings of empirical methods in natural language processing.\n\nThumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. P Turney, Proceedings of the association for computational linguistics. the association for computational linguisticsTurney, P. (2002). Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In: Proceedings of the association for computational linguistics.\n\nStatistical learning theory. V Vapnik, WileyNew YorkVapnik, V. (1998). Statistical learning theory. New York: Wiley.\n\nSolving large-scale linear prediction problems with stochastic gradient descent. T Zhang, Proceedings of the international conference on machine learning. the international conference on machine learningZhang, T. (2004). Solving large-scale linear prediction problems with stochastic gradient descent. In: Pro- ceedings of the international conference on machine learning.\n", "annotations": {"author": "[{\"end\":66,\"start\":51},{\"end\":104,\"start\":67},{\"end\":118,\"start\":105},{\"end\":154,\"start\":119},{\"end\":191,\"start\":155},{\"end\":209,\"start\":192},{\"end\":218,\"start\":210},{\"end\":221,\"start\":219},{\"end\":238,\"start\":222},{\"end\":250,\"start\":239},{\"end\":261,\"start\":251},{\"end\":272,\"start\":262},{\"end\":283,\"start\":273},{\"end\":294,\"start\":284},{\"end\":307,\"start\":295},{\"end\":407,\"start\":308},{\"end\":472,\"start\":408},{\"end\":546,\"start\":473},{\"end\":593,\"start\":547},{\"end\":673,\"start\":594},{\"end\":709,\"start\":674},{\"end\":66,\"start\":51},{\"end\":104,\"start\":67},{\"end\":118,\"start\":105},{\"end\":154,\"start\":119},{\"end\":191,\"start\":155},{\"end\":209,\"start\":192},{\"end\":218,\"start\":210},{\"end\":221,\"start\":219},{\"end\":238,\"start\":222},{\"end\":250,\"start\":239},{\"end\":261,\"start\":251},{\"end\":272,\"start\":262},{\"end\":283,\"start\":273},{\"end\":294,\"start\":284},{\"end\":307,\"start\":295},{\"end\":407,\"start\":308},{\"end\":472,\"start\":408},{\"end\":546,\"start\":473},{\"end\":593,\"start\":547},{\"end\":673,\"start\":594},{\"end\":709,\"start\":674}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":56},{\"end\":79,\"start\":72},{\"end\":117,\"start\":110},{\"end\":131,\"start\":124},{\"end\":171,\"start\":164},{\"end\":208,\"start\":201},{\"end\":237,\"start\":232},{\"end\":249,\"start\":241},{\"end\":260,\"start\":253},{\"end\":271,\"start\":264},{\"end\":282,\"start\":275},{\"end\":293,\"start\":286},{\"end\":306,\"start\":299},{\"end\":65,\"start\":56},{\"end\":79,\"start\":72},{\"end\":117,\"start\":110},{\"end\":131,\"start\":124},{\"end\":171,\"start\":164},{\"end\":208,\"start\":201},{\"end\":237,\"start\":232},{\"end\":249,\"start\":241},{\"end\":260,\"start\":253},{\"end\":271,\"start\":264},{\"end\":282,\"start\":275},{\"end\":293,\"start\":286},{\"end\":306,\"start\":299}]", "author_first_name": "[{\"end\":55,\"start\":51},{\"end\":71,\"start\":67},{\"end\":109,\"start\":105},{\"end\":123,\"start\":119},{\"end\":163,\"start\":155},{\"end\":200,\"start\":192},{\"end\":217,\"start\":210},{\"end\":220,\"start\":219},{\"end\":231,\"start\":222},{\"end\":240,\"start\":239},{\"end\":252,\"start\":251},{\"end\":263,\"start\":262},{\"end\":274,\"start\":273},{\"end\":285,\"start\":284},{\"end\":296,\"start\":295},{\"end\":298,\"start\":297},{\"end\":55,\"start\":51},{\"end\":71,\"start\":67},{\"end\":109,\"start\":105},{\"end\":123,\"start\":119},{\"end\":163,\"start\":155},{\"end\":200,\"start\":192},{\"end\":217,\"start\":210},{\"end\":220,\"start\":219},{\"end\":231,\"start\":222},{\"end\":240,\"start\":239},{\"end\":252,\"start\":251},{\"end\":263,\"start\":262},{\"end\":274,\"start\":273},{\"end\":285,\"start\":284},{\"end\":296,\"start\":295},{\"end\":298,\"start\":297}]", "author_affiliation": "[{\"end\":406,\"start\":309},{\"end\":471,\"start\":409},{\"end\":545,\"start\":474},{\"end\":592,\"start\":548},{\"end\":672,\"start\":595},{\"end\":708,\"start\":675},{\"end\":406,\"start\":309},{\"end\":471,\"start\":409},{\"end\":545,\"start\":474},{\"end\":592,\"start\":548},{\"end\":672,\"start\":595},{\"end\":708,\"start\":675}]", "title": "[{\"end\":44,\"start\":1},{\"end\":753,\"start\":710},{\"end\":44,\"start\":1},{\"end\":753,\"start\":710}]", "venue": "[{\"end\":765,\"start\":755},{\"end\":765,\"start\":755}]", "abstract": "[{\"end\":1887,\"start\":1195},{\"end\":1887,\"start\":1195}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4204,\"start\":4186},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4228,\"start\":4214},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4272,\"start\":4253},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4315,\"start\":4297},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5702,\"start\":5683},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5724,\"start\":5702},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6829,\"start\":6806},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6939,\"start\":6918},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8311,\"start\":8290},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8847,\"start\":8835},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8865,\"start\":8852},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9348,\"start\":9337},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9382,\"start\":9369},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9402,\"start\":9382},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9424,\"start\":9404},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10080,\"start\":10059},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10104,\"start\":10080},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10426,\"start\":10412},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10546,\"start\":10527},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10567,\"start\":10546},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10586,\"start\":10567},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10786,\"start\":10765},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10804,\"start\":10786},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14326,\"start\":14308},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14343,\"start\":14326},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14905,\"start\":14888},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15685,\"start\":15666},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19711,\"start\":19688},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19731,\"start\":19711},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20420,\"start\":20393},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20668,\"start\":20647},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22801,\"start\":22788},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22827,\"start\":22801},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25527,\"start\":25500},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27459,\"start\":27438},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29287,\"start\":29269},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31076,\"start\":31058},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31088,\"start\":31076},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31297,\"start\":31278},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31315,\"start\":31297},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31540,\"start\":31519},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34160,\"start\":34142},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34676,\"start\":34664},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35980,\"start\":35957},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":39416,\"start\":39397},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39435,\"start\":39416},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39455,\"start\":39435},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":39470,\"start\":39455},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42998,\"start\":42977},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":43336,\"start\":43315},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":45317,\"start\":45296},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45341,\"start\":45317},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":46423,\"start\":46402},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":46447,\"start\":46423},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":47740,\"start\":47723},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47768,\"start\":47740},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":51136,\"start\":51109},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4204,\"start\":4186},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4228,\"start\":4214},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4272,\"start\":4253},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4315,\"start\":4297},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5702,\"start\":5683},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5724,\"start\":5702},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6829,\"start\":6806},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6939,\"start\":6918},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8311,\"start\":8290},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8847,\"start\":8835},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8865,\"start\":8852},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9348,\"start\":9337},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9382,\"start\":9369},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9402,\"start\":9382},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9424,\"start\":9404},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10080,\"start\":10059},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10104,\"start\":10080},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10426,\"start\":10412},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10546,\"start\":10527},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10567,\"start\":10546},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10586,\"start\":10567},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10786,\"start\":10765},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10804,\"start\":10786},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14326,\"start\":14308},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14343,\"start\":14326},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14905,\"start\":14888},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15685,\"start\":15666},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19711,\"start\":19688},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19731,\"start\":19711},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20420,\"start\":20393},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20668,\"start\":20647},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22801,\"start\":22788},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22827,\"start\":22801},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25527,\"start\":25500},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27459,\"start\":27438},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29287,\"start\":29269},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31076,\"start\":31058},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31088,\"start\":31076},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31297,\"start\":31278},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31315,\"start\":31297},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31540,\"start\":31519},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34160,\"start\":34142},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34676,\"start\":34664},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35980,\"start\":35957},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":39416,\"start\":39397},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39435,\"start\":39416},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39455,\"start\":39435},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":39470,\"start\":39455},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42998,\"start\":42977},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":43336,\"start\":43315},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":45317,\"start\":45296},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45341,\"start\":45317},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":46423,\"start\":46402},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":46447,\"start\":46423},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":47740,\"start\":47723},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47768,\"start\":47740},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":51136,\"start\":51109}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55507,\"start\":55309},{\"attributes\":{\"id\":\"fig_1\"},\"end\":55811,\"start\":55508},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56599,\"start\":55812},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":57199,\"start\":56600},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57447,\"start\":57200},{\"attributes\":{\"id\":\"fig_0\"},\"end\":55507,\"start\":55309},{\"attributes\":{\"id\":\"fig_1\"},\"end\":55811,\"start\":55508},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56599,\"start\":55812},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":57199,\"start\":56600},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57447,\"start\":57200}]", "paragraph": "[{\"end\":2371,\"start\":1889},{\"end\":3061,\"start\":2373},{\"end\":3151,\"start\":3063},{\"end\":4063,\"start\":3168},{\"end\":4822,\"start\":4065},{\"end\":6124,\"start\":4824},{\"end\":6596,\"start\":6126},{\"end\":7521,\"start\":6624},{\"end\":10027,\"start\":7523},{\"end\":10324,\"start\":10044},{\"end\":11223,\"start\":10326},{\"end\":11842,\"start\":11225},{\"end\":12316,\"start\":11884},{\"end\":12514,\"start\":12318},{\"end\":12804,\"start\":12552},{\"end\":13450,\"start\":12853},{\"end\":13536,\"start\":13452},{\"end\":13732,\"start\":13585},{\"end\":13879,\"start\":13860},{\"end\":14844,\"start\":13881},{\"end\":15089,\"start\":14865},{\"end\":15552,\"start\":15206},{\"end\":15697,\"start\":15554},{\"end\":15939,\"start\":15699},{\"end\":16182,\"start\":15996},{\"end\":16580,\"start\":16184},{\"end\":16895,\"start\":16876},{\"end\":17451,\"start\":16897},{\"end\":17730,\"start\":17511},{\"end\":17826,\"start\":17747},{\"end\":17914,\"start\":17859},{\"end\":18347,\"start\":17940},{\"end\":18487,\"start\":18349},{\"end\":18598,\"start\":18489},{\"end\":18769,\"start\":18649},{\"end\":18886,\"start\":18771},{\"end\":19271,\"start\":19167},{\"end\":19525,\"start\":19273},{\"end\":19784,\"start\":19600},{\"end\":20421,\"start\":20152},{\"end\":21107,\"start\":20423},{\"end\":21344,\"start\":21170},{\"end\":21840,\"start\":21346},{\"end\":22027,\"start\":21876},{\"end\":22371,\"start\":22029},{\"end\":22496,\"start\":22477},{\"end\":23005,\"start\":22498},{\"end\":23299,\"start\":23007},{\"end\":23441,\"start\":23359},{\"end\":23707,\"start\":23671},{\"end\":23791,\"start\":23709},{\"end\":23923,\"start\":23813},{\"end\":24253,\"start\":24212},{\"end\":24410,\"start\":24349},{\"end\":24766,\"start\":24559},{\"end\":25276,\"start\":24768},{\"end\":25711,\"start\":25428},{\"end\":26289,\"start\":25713},{\"end\":26442,\"start\":26291},{\"end\":26642,\"start\":26467},{\"end\":26699,\"start\":26694},{\"end\":26851,\"start\":26763},{\"end\":28181,\"start\":26971},{\"end\":29120,\"start\":28183},{\"end\":30589,\"start\":29160},{\"end\":30848,\"start\":30591},{\"end\":31472,\"start\":30877},{\"end\":31988,\"start\":31474},{\"end\":32861,\"start\":31990},{\"end\":33291,\"start\":32902},{\"end\":33538,\"start\":33293},{\"end\":34018,\"start\":33540},{\"end\":34677,\"start\":34020},{\"end\":35428,\"start\":34693},{\"end\":36421,\"start\":35430},{\"end\":36686,\"start\":36485},{\"end\":37598,\"start\":36688},{\"end\":38326,\"start\":37600},{\"end\":38770,\"start\":38328},{\"end\":39629,\"start\":38811},{\"end\":39904,\"start\":39631},{\"end\":40781,\"start\":39972},{\"end\":40888,\"start\":40805},{\"end\":41269,\"start\":40890},{\"end\":41345,\"start\":41326},{\"end\":41492,\"start\":41347},{\"end\":42062,\"start\":41530},{\"end\":42478,\"start\":42123},{\"end\":42639,\"start\":42620},{\"end\":42801,\"start\":42641},{\"end\":43449,\"start\":42880},{\"end\":44209,\"start\":43487},{\"end\":44623,\"start\":44211},{\"end\":44806,\"start\":44787},{\"end\":44962,\"start\":44808},{\"end\":45850,\"start\":44977},{\"end\":46566,\"start\":45852},{\"end\":47456,\"start\":46581},{\"end\":48045,\"start\":47458},{\"end\":48300,\"start\":48047},{\"end\":48350,\"start\":48321},{\"end\":48578,\"start\":48446},{\"end\":49164,\"start\":48906},{\"end\":49563,\"start\":49483},{\"end\":49790,\"start\":49669},{\"end\":50529,\"start\":50023},{\"end\":51137,\"start\":50679},{\"end\":51382,\"start\":51349},{\"end\":51517,\"start\":51484},{\"end\":51634,\"start\":51601},{\"end\":52208,\"start\":51719},{\"end\":52609,\"start\":52265},{\"end\":53128,\"start\":52716},{\"end\":53200,\"start\":53159},{\"end\":53812,\"start\":53605},{\"end\":53930,\"start\":53899},{\"end\":54039,\"start\":54008},{\"end\":54147,\"start\":54106},{\"end\":54296,\"start\":54214},{\"end\":54710,\"start\":54298},{\"end\":54782,\"start\":54741},{\"end\":55098,\"start\":54890},{\"end\":55308,\"start\":55268},{\"end\":2371,\"start\":1889},{\"end\":3061,\"start\":2373},{\"end\":3151,\"start\":3063},{\"end\":4063,\"start\":3168},{\"end\":4822,\"start\":4065},{\"end\":6124,\"start\":4824},{\"end\":6596,\"start\":6126},{\"end\":7521,\"start\":6624},{\"end\":10027,\"start\":7523},{\"end\":10324,\"start\":10044},{\"end\":11223,\"start\":10326},{\"end\":11842,\"start\":11225},{\"end\":12316,\"start\":11884},{\"end\":12514,\"start\":12318},{\"end\":12804,\"start\":12552},{\"end\":13450,\"start\":12853},{\"end\":13536,\"start\":13452},{\"end\":13732,\"start\":13585},{\"end\":13879,\"start\":13860},{\"end\":14844,\"start\":13881},{\"end\":15089,\"start\":14865},{\"end\":15552,\"start\":15206},{\"end\":15697,\"start\":15554},{\"end\":15939,\"start\":15699},{\"end\":16182,\"start\":15996},{\"end\":16580,\"start\":16184},{\"end\":16895,\"start\":16876},{\"end\":17451,\"start\":16897},{\"end\":17730,\"start\":17511},{\"end\":17826,\"start\":17747},{\"end\":17914,\"start\":17859},{\"end\":18347,\"start\":17940},{\"end\":18487,\"start\":18349},{\"end\":18598,\"start\":18489},{\"end\":18769,\"start\":18649},{\"end\":18886,\"start\":18771},{\"end\":19271,\"start\":19167},{\"end\":19525,\"start\":19273},{\"end\":19784,\"start\":19600},{\"end\":20421,\"start\":20152},{\"end\":21107,\"start\":20423},{\"end\":21344,\"start\":21170},{\"end\":21840,\"start\":21346},{\"end\":22027,\"start\":21876},{\"end\":22371,\"start\":22029},{\"end\":22496,\"start\":22477},{\"end\":23005,\"start\":22498},{\"end\":23299,\"start\":23007},{\"end\":23441,\"start\":23359},{\"end\":23707,\"start\":23671},{\"end\":23791,\"start\":23709},{\"end\":23923,\"start\":23813},{\"end\":24253,\"start\":24212},{\"end\":24410,\"start\":24349},{\"end\":24766,\"start\":24559},{\"end\":25276,\"start\":24768},{\"end\":25711,\"start\":25428},{\"end\":26289,\"start\":25713},{\"end\":26442,\"start\":26291},{\"end\":26642,\"start\":26467},{\"end\":26699,\"start\":26694},{\"end\":26851,\"start\":26763},{\"end\":28181,\"start\":26971},{\"end\":29120,\"start\":28183},{\"end\":30589,\"start\":29160},{\"end\":30848,\"start\":30591},{\"end\":31472,\"start\":30877},{\"end\":31988,\"start\":31474},{\"end\":32861,\"start\":31990},{\"end\":33291,\"start\":32902},{\"end\":33538,\"start\":33293},{\"end\":34018,\"start\":33540},{\"end\":34677,\"start\":34020},{\"end\":35428,\"start\":34693},{\"end\":36421,\"start\":35430},{\"end\":36686,\"start\":36485},{\"end\":37598,\"start\":36688},{\"end\":38326,\"start\":37600},{\"end\":38770,\"start\":38328},{\"end\":39629,\"start\":38811},{\"end\":39904,\"start\":39631},{\"end\":40781,\"start\":39972},{\"end\":40888,\"start\":40805},{\"end\":41269,\"start\":40890},{\"end\":41345,\"start\":41326},{\"end\":41492,\"start\":41347},{\"end\":42062,\"start\":41530},{\"end\":42478,\"start\":42123},{\"end\":42639,\"start\":42620},{\"end\":42801,\"start\":42641},{\"end\":43449,\"start\":42880},{\"end\":44209,\"start\":43487},{\"end\":44623,\"start\":44211},{\"end\":44806,\"start\":44787},{\"end\":44962,\"start\":44808},{\"end\":45850,\"start\":44977},{\"end\":46566,\"start\":45852},{\"end\":47456,\"start\":46581},{\"end\":48045,\"start\":47458},{\"end\":48300,\"start\":48047},{\"end\":48350,\"start\":48321},{\"end\":48578,\"start\":48446},{\"end\":49164,\"start\":48906},{\"end\":49563,\"start\":49483},{\"end\":49790,\"start\":49669},{\"end\":50529,\"start\":50023},{\"end\":51137,\"start\":50679},{\"end\":51382,\"start\":51349},{\"end\":51517,\"start\":51484},{\"end\":51634,\"start\":51601},{\"end\":52208,\"start\":51719},{\"end\":52609,\"start\":52265},{\"end\":53128,\"start\":52716},{\"end\":53200,\"start\":53159},{\"end\":53812,\"start\":53605},{\"end\":53930,\"start\":53899},{\"end\":54039,\"start\":54008},{\"end\":54147,\"start\":54106},{\"end\":54296,\"start\":54214},{\"end\":54710,\"start\":54298},{\"end\":54782,\"start\":54741},{\"end\":55098,\"start\":54890},{\"end\":55308,\"start\":55268}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6623,\"start\":6597},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12551,\"start\":12515},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13584,\"start\":13537},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13859,\"start\":13765},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15205,\"start\":15090},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15995,\"start\":15940},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16875,\"start\":16624},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17858,\"start\":17827},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17939,\"start\":17915},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18648,\"start\":18599},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19166,\"start\":18916},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19599,\"start\":19526},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20151,\"start\":19785},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21875,\"start\":21841},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22476,\"start\":22421},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23358,\"start\":23300},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23670,\"start\":23442},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23812,\"start\":23792},{\"attributes\":{\"id\":\"formula_18\"},\"end\":24211,\"start\":23924},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24348,\"start\":24254},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24558,\"start\":24411},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25427,\"start\":25277},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26693,\"start\":26643},{\"attributes\":{\"id\":\"formula_23\"},\"end\":26762,\"start\":26700},{\"attributes\":{\"id\":\"formula_24\"},\"end\":26910,\"start\":26852},{\"attributes\":{\"id\":\"formula_25\"},\"end\":26970,\"start\":26910},{\"attributes\":{\"id\":\"formula_26\"},\"end\":36484,\"start\":36422},{\"attributes\":{\"id\":\"formula_27\"},\"end\":39971,\"start\":39905},{\"attributes\":{\"id\":\"formula_28\"},\"end\":41325,\"start\":41270},{\"attributes\":{\"id\":\"formula_29\"},\"end\":42619,\"start\":42479},{\"attributes\":{\"id\":\"formula_30\"},\"end\":42879,\"start\":42802},{\"attributes\":{\"id\":\"formula_31\"},\"end\":44786,\"start\":44624},{\"attributes\":{\"id\":\"formula_32\"},\"end\":48445,\"start\":48351},{\"attributes\":{\"id\":\"formula_33\"},\"end\":48905,\"start\":48579},{\"attributes\":{\"id\":\"formula_34\"},\"end\":49482,\"start\":49165},{\"attributes\":{\"id\":\"formula_35\"},\"end\":49668,\"start\":49613},{\"attributes\":{\"id\":\"formula_36\"},\"end\":50022,\"start\":49791},{\"attributes\":{\"id\":\"formula_37\"},\"end\":50678,\"start\":50530},{\"attributes\":{\"id\":\"formula_38\"},\"end\":51348,\"start\":51138},{\"attributes\":{\"id\":\"formula_39\"},\"end\":51483,\"start\":51383},{\"attributes\":{\"id\":\"formula_40\"},\"end\":51600,\"start\":51518},{\"attributes\":{\"id\":\"formula_41\"},\"end\":51718,\"start\":51635},{\"attributes\":{\"id\":\"formula_42\"},\"end\":52264,\"start\":52209},{\"attributes\":{\"id\":\"formula_43\"},\"end\":52715,\"start\":52610},{\"attributes\":{\"id\":\"formula_44\"},\"end\":53158,\"start\":53129},{\"attributes\":{\"id\":\"formula_45\"},\"end\":53604,\"start\":53201},{\"attributes\":{\"id\":\"formula_46\"},\"end\":53898,\"start\":53813},{\"attributes\":{\"id\":\"formula_47\"},\"end\":54007,\"start\":53931},{\"attributes\":{\"id\":\"formula_48\"},\"end\":54105,\"start\":54040},{\"attributes\":{\"id\":\"formula_49\"},\"end\":54213,\"start\":54148},{\"attributes\":{\"id\":\"formula_50\"},\"end\":54740,\"start\":54711},{\"attributes\":{\"id\":\"formula_51\"},\"end\":54889,\"start\":54783},{\"attributes\":{\"id\":\"formula_52\"},\"end\":55267,\"start\":55099},{\"attributes\":{\"id\":\"formula_0\"},\"end\":6623,\"start\":6597},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12551,\"start\":12515},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13584,\"start\":13537},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13859,\"start\":13765},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15205,\"start\":15090},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15995,\"start\":15940},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16875,\"start\":16624},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17858,\"start\":17827},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17939,\"start\":17915},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18648,\"start\":18599},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19166,\"start\":18916},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19599,\"start\":19526},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20151,\"start\":19785},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21875,\"start\":21841},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22476,\"start\":22421},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23358,\"start\":23300},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23670,\"start\":23442},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23812,\"start\":23792},{\"attributes\":{\"id\":\"formula_18\"},\"end\":24211,\"start\":23924},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24348,\"start\":24254},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24558,\"start\":24411},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25427,\"start\":25277},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26693,\"start\":26643},{\"attributes\":{\"id\":\"formula_23\"},\"end\":26762,\"start\":26700},{\"attributes\":{\"id\":\"formula_24\"},\"end\":26910,\"start\":26852},{\"attributes\":{\"id\":\"formula_25\"},\"end\":26970,\"start\":26910},{\"attributes\":{\"id\":\"formula_26\"},\"end\":36484,\"start\":36422},{\"attributes\":{\"id\":\"formula_27\"},\"end\":39971,\"start\":39905},{\"attributes\":{\"id\":\"formula_28\"},\"end\":41325,\"start\":41270},{\"attributes\":{\"id\":\"formula_29\"},\"end\":42619,\"start\":42479},{\"attributes\":{\"id\":\"formula_30\"},\"end\":42879,\"start\":42802},{\"attributes\":{\"id\":\"formula_31\"},\"end\":44786,\"start\":44624},{\"attributes\":{\"id\":\"formula_32\"},\"end\":48445,\"start\":48351},{\"attributes\":{\"id\":\"formula_33\"},\"end\":48905,\"start\":48579},{\"attributes\":{\"id\":\"formula_34\"},\"end\":49482,\"start\":49165},{\"attributes\":{\"id\":\"formula_35\"},\"end\":49668,\"start\":49613},{\"attributes\":{\"id\":\"formula_36\"},\"end\":50022,\"start\":49791},{\"attributes\":{\"id\":\"formula_37\"},\"end\":50678,\"start\":50530},{\"attributes\":{\"id\":\"formula_38\"},\"end\":51348,\"start\":51138},{\"attributes\":{\"id\":\"formula_39\"},\"end\":51483,\"start\":51383},{\"attributes\":{\"id\":\"formula_40\"},\"end\":51600,\"start\":51518},{\"attributes\":{\"id\":\"formula_41\"},\"end\":51718,\"start\":51635},{\"attributes\":{\"id\":\"formula_42\"},\"end\":52264,\"start\":52209},{\"attributes\":{\"id\":\"formula_43\"},\"end\":52715,\"start\":52610},{\"attributes\":{\"id\":\"formula_44\"},\"end\":53158,\"start\":53129},{\"attributes\":{\"id\":\"formula_45\"},\"end\":53604,\"start\":53201},{\"attributes\":{\"id\":\"formula_46\"},\"end\":53898,\"start\":53813},{\"attributes\":{\"id\":\"formula_47\"},\"end\":54007,\"start\":53931},{\"attributes\":{\"id\":\"formula_48\"},\"end\":54105,\"start\":54040},{\"attributes\":{\"id\":\"formula_49\"},\"end\":54213,\"start\":54148},{\"attributes\":{\"id\":\"formula_50\"},\"end\":54740,\"start\":54711},{\"attributes\":{\"id\":\"formula_51\"},\"end\":54889,\"start\":54783},{\"attributes\":{\"id\":\"formula_52\"},\"end\":55267,\"start\":55099}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3166,\"start\":3154},{\"attributes\":{\"n\":\"2\"},\"end\":10042,\"start\":10030},{\"attributes\":{\"n\":\"3\"},\"end\":11882,\"start\":11845},{\"attributes\":{\"n\":\"4\"},\"end\":12851,\"start\":12807},{\"end\":13764,\"start\":13735},{\"attributes\":{\"n\":\"4.1\"},\"end\":14863,\"start\":14847},{\"end\":16623,\"start\":16583},{\"attributes\":{\"n\":\"4.2\"},\"end\":17509,\"start\":17454},{\"end\":17745,\"start\":17733},{\"end\":18915,\"start\":18889},{\"attributes\":{\"n\":\"5\"},\"end\":21168,\"start\":21110},{\"end\":22420,\"start\":22374},{\"attributes\":{\"n\":\"6\"},\"end\":26465,\"start\":26445},{\"attributes\":{\"n\":\"7\"},\"end\":29158,\"start\":29123},{\"attributes\":{\"n\":\"7.1\"},\"end\":30875,\"start\":30851},{\"end\":32900,\"start\":32864},{\"attributes\":{\"n\":\"7.2\"},\"end\":34691,\"start\":34680},{\"attributes\":{\"n\":\"8\"},\"end\":38809,\"start\":38773},{\"attributes\":{\"n\":\"8.1\"},\"end\":40803,\"start\":40784},{\"attributes\":{\"n\":\"8.2\"},\"end\":41528,\"start\":41495},{\"end\":42121,\"start\":42065},{\"attributes\":{\"n\":\"8.3\"},\"end\":43485,\"start\":43452},{\"attributes\":{\"n\":\"8.4\"},\"end\":44975,\"start\":44965},{\"attributes\":{\"n\":\"9\"},\"end\":46579,\"start\":46569},{\"end\":48319,\"start\":48303},{\"end\":49612,\"start\":49566},{\"end\":55515,\"start\":55509},{\"end\":55819,\"start\":55813},{\"attributes\":{\"n\":\"1\"},\"end\":3166,\"start\":3154},{\"attributes\":{\"n\":\"2\"},\"end\":10042,\"start\":10030},{\"attributes\":{\"n\":\"3\"},\"end\":11882,\"start\":11845},{\"attributes\":{\"n\":\"4\"},\"end\":12851,\"start\":12807},{\"end\":13764,\"start\":13735},{\"attributes\":{\"n\":\"4.1\"},\"end\":14863,\"start\":14847},{\"end\":16623,\"start\":16583},{\"attributes\":{\"n\":\"4.2\"},\"end\":17509,\"start\":17454},{\"end\":17745,\"start\":17733},{\"end\":18915,\"start\":18889},{\"attributes\":{\"n\":\"5\"},\"end\":21168,\"start\":21110},{\"end\":22420,\"start\":22374},{\"attributes\":{\"n\":\"6\"},\"end\":26465,\"start\":26445},{\"attributes\":{\"n\":\"7\"},\"end\":29158,\"start\":29123},{\"attributes\":{\"n\":\"7.1\"},\"end\":30875,\"start\":30851},{\"end\":32900,\"start\":32864},{\"attributes\":{\"n\":\"7.2\"},\"end\":34691,\"start\":34680},{\"attributes\":{\"n\":\"8\"},\"end\":38809,\"start\":38773},{\"attributes\":{\"n\":\"8.1\"},\"end\":40803,\"start\":40784},{\"attributes\":{\"n\":\"8.2\"},\"end\":41528,\"start\":41495},{\"end\":42121,\"start\":42065},{\"attributes\":{\"n\":\"8.3\"},\"end\":43485,\"start\":43452},{\"attributes\":{\"n\":\"8.4\"},\"end\":44975,\"start\":44965},{\"attributes\":{\"n\":\"9\"},\"end\":46579,\"start\":46569},{\"end\":48319,\"start\":48303},{\"end\":49612,\"start\":49566},{\"end\":55515,\"start\":55509},{\"end\":55819,\"start\":55813}]", "table": "[{\"end\":57199,\"start\":57016},{\"end\":57447,\"start\":57351},{\"end\":57199,\"start\":57016},{\"end\":57447,\"start\":57351}]", "figure_caption": "[{\"end\":55507,\"start\":55311},{\"end\":55811,\"start\":55517},{\"end\":56599,\"start\":55821},{\"end\":57016,\"start\":56602},{\"end\":57351,\"start\":57202},{\"end\":55507,\"start\":55311},{\"end\":55811,\"start\":55517},{\"end\":56599,\"start\":55821},{\"end\":57016,\"start\":56602},{\"end\":57351,\"start\":57202}]", "figure_ref": "[{\"end\":28236,\"start\":28230},{\"end\":29352,\"start\":29346},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29616,\"start\":29610},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33914,\"start\":33908},{\"end\":36876,\"start\":36869},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36886,\"start\":36878},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37326,\"start\":37320},{\"end\":28236,\"start\":28230},{\"end\":29352,\"start\":29346},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29616,\"start\":29610},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33914,\"start\":33908},{\"end\":36876,\"start\":36869},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36886,\"start\":36878},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37326,\"start\":37320}]", "bib_author_first_name": "[{\"end\":57727,\"start\":57726},{\"end\":57735,\"start\":57734},{\"end\":57955,\"start\":57954},{\"end\":57966,\"start\":57965},{\"end\":58263,\"start\":58262},{\"end\":58275,\"start\":58274},{\"end\":58528,\"start\":58527},{\"end\":58536,\"start\":58535},{\"end\":58547,\"start\":58546},{\"end\":58560,\"start\":58559},{\"end\":58569,\"start\":58568},{\"end\":58853,\"start\":58852},{\"end\":59083,\"start\":59082},{\"end\":59096,\"start\":59095},{\"end\":59105,\"start\":59104},{\"end\":59365,\"start\":59364},{\"end\":59378,\"start\":59377},{\"end\":59389,\"start\":59388},{\"end\":59400,\"start\":59399},{\"end\":59705,\"start\":59704},{\"end\":59715,\"start\":59714},{\"end\":59727,\"start\":59726},{\"end\":60092,\"start\":60091},{\"end\":60101,\"start\":60100},{\"end\":60111,\"start\":60110},{\"end\":60123,\"start\":60122},{\"end\":60395,\"start\":60394},{\"end\":60406,\"start\":60405},{\"end\":60417,\"start\":60416},{\"end\":60428,\"start\":60427},{\"end\":60439,\"start\":60438},{\"end\":60769,\"start\":60768},{\"end\":60780,\"start\":60779},{\"end\":60790,\"start\":60789},{\"end\":61018,\"start\":61017},{\"end\":61228,\"start\":61227},{\"end\":61238,\"start\":61237},{\"end\":61247,\"start\":61246},{\"end\":61256,\"start\":61255},{\"end\":61616,\"start\":61615},{\"end\":61627,\"start\":61626},{\"end\":61637,\"start\":61636},{\"end\":61853,\"start\":61852},{\"end\":61860,\"start\":61859},{\"end\":61868,\"start\":61867},{\"end\":61875,\"start\":61874},{\"end\":62217,\"start\":62216},{\"end\":62224,\"start\":62223},{\"end\":62573,\"start\":62572},{\"end\":62782,\"start\":62781},{\"end\":62784,\"start\":62783},{\"end\":62794,\"start\":62793},{\"end\":62796,\"start\":62795},{\"end\":63152,\"start\":63151},{\"end\":63327,\"start\":63326},{\"end\":63336,\"start\":63335},{\"end\":63345,\"start\":63344},{\"end\":63356,\"start\":63355},{\"end\":63369,\"start\":63368},{\"end\":63659,\"start\":63658},{\"end\":63668,\"start\":63667},{\"end\":63956,\"start\":63955},{\"end\":63965,\"start\":63964},{\"end\":63978,\"start\":63977},{\"end\":64173,\"start\":64172},{\"end\":64179,\"start\":64178},{\"end\":64567,\"start\":64566},{\"end\":64578,\"start\":64577},{\"end\":64587,\"start\":64586},{\"end\":64858,\"start\":64857},{\"end\":64869,\"start\":64868},{\"end\":64878,\"start\":64877},{\"end\":65245,\"start\":65244},{\"end\":65581,\"start\":65580},{\"end\":65589,\"start\":65588},{\"end\":65596,\"start\":65595},{\"end\":65966,\"start\":65965},{\"end\":66301,\"start\":66300},{\"end\":66313,\"start\":66312},{\"end\":66323,\"start\":66322},{\"end\":66335,\"start\":66334},{\"end\":66346,\"start\":66345},{\"end\":66359,\"start\":66358},{\"end\":66742,\"start\":66741},{\"end\":66752,\"start\":66751},{\"end\":66760,\"start\":66759},{\"end\":67187,\"start\":67186},{\"end\":67517,\"start\":67516},{\"end\":67687,\"start\":67686},{\"end\":57727,\"start\":57726},{\"end\":57735,\"start\":57734},{\"end\":57955,\"start\":57954},{\"end\":57966,\"start\":57965},{\"end\":58263,\"start\":58262},{\"end\":58275,\"start\":58274},{\"end\":58528,\"start\":58527},{\"end\":58536,\"start\":58535},{\"end\":58547,\"start\":58546},{\"end\":58560,\"start\":58559},{\"end\":58569,\"start\":58568},{\"end\":58853,\"start\":58852},{\"end\":59083,\"start\":59082},{\"end\":59096,\"start\":59095},{\"end\":59105,\"start\":59104},{\"end\":59365,\"start\":59364},{\"end\":59378,\"start\":59377},{\"end\":59389,\"start\":59388},{\"end\":59400,\"start\":59399},{\"end\":59705,\"start\":59704},{\"end\":59715,\"start\":59714},{\"end\":59727,\"start\":59726},{\"end\":60092,\"start\":60091},{\"end\":60101,\"start\":60100},{\"end\":60111,\"start\":60110},{\"end\":60123,\"start\":60122},{\"end\":60395,\"start\":60394},{\"end\":60406,\"start\":60405},{\"end\":60417,\"start\":60416},{\"end\":60428,\"start\":60427},{\"end\":60439,\"start\":60438},{\"end\":60769,\"start\":60768},{\"end\":60780,\"start\":60779},{\"end\":60790,\"start\":60789},{\"end\":61018,\"start\":61017},{\"end\":61228,\"start\":61227},{\"end\":61238,\"start\":61237},{\"end\":61247,\"start\":61246},{\"end\":61256,\"start\":61255},{\"end\":61616,\"start\":61615},{\"end\":61627,\"start\":61626},{\"end\":61637,\"start\":61636},{\"end\":61853,\"start\":61852},{\"end\":61860,\"start\":61859},{\"end\":61868,\"start\":61867},{\"end\":61875,\"start\":61874},{\"end\":62217,\"start\":62216},{\"end\":62224,\"start\":62223},{\"end\":62573,\"start\":62572},{\"end\":62782,\"start\":62781},{\"end\":62784,\"start\":62783},{\"end\":62794,\"start\":62793},{\"end\":62796,\"start\":62795},{\"end\":63152,\"start\":63151},{\"end\":63327,\"start\":63326},{\"end\":63336,\"start\":63335},{\"end\":63345,\"start\":63344},{\"end\":63356,\"start\":63355},{\"end\":63369,\"start\":63368},{\"end\":63659,\"start\":63658},{\"end\":63668,\"start\":63667},{\"end\":63956,\"start\":63955},{\"end\":63965,\"start\":63964},{\"end\":63978,\"start\":63977},{\"end\":64173,\"start\":64172},{\"end\":64179,\"start\":64178},{\"end\":64567,\"start\":64566},{\"end\":64578,\"start\":64577},{\"end\":64587,\"start\":64586},{\"end\":64858,\"start\":64857},{\"end\":64869,\"start\":64868},{\"end\":64878,\"start\":64877},{\"end\":65245,\"start\":65244},{\"end\":65581,\"start\":65580},{\"end\":65589,\"start\":65588},{\"end\":65596,\"start\":65595},{\"end\":65966,\"start\":65965},{\"end\":66301,\"start\":66300},{\"end\":66313,\"start\":66312},{\"end\":66323,\"start\":66322},{\"end\":66335,\"start\":66334},{\"end\":66346,\"start\":66345},{\"end\":66359,\"start\":66358},{\"end\":66742,\"start\":66741},{\"end\":66752,\"start\":66751},{\"end\":66760,\"start\":66759},{\"end\":67187,\"start\":67186},{\"end\":67517,\"start\":67516},{\"end\":67687,\"start\":67686}]", "bib_author_last_name": "[{\"end\":57732,\"start\":57728},{\"end\":57741,\"start\":57736},{\"end\":57963,\"start\":57956},{\"end\":57975,\"start\":57967},{\"end\":58272,\"start\":58264},{\"end\":58285,\"start\":58276},{\"end\":58533,\"start\":58529},{\"end\":58544,\"start\":58537},{\"end\":58557,\"start\":58548},{\"end\":58566,\"start\":58561},{\"end\":58575,\"start\":58570},{\"end\":58860,\"start\":58854},{\"end\":59093,\"start\":59084},{\"end\":59102,\"start\":59097},{\"end\":59110,\"start\":59106},{\"end\":59375,\"start\":59366},{\"end\":59386,\"start\":59379},{\"end\":59397,\"start\":59390},{\"end\":59408,\"start\":59401},{\"end\":59712,\"start\":59706},{\"end\":59724,\"start\":59716},{\"end\":59736,\"start\":59728},{\"end\":60098,\"start\":60093},{\"end\":60108,\"start\":60102},{\"end\":60120,\"start\":60112},{\"end\":60134,\"start\":60124},{\"end\":60403,\"start\":60396},{\"end\":60414,\"start\":60407},{\"end\":60425,\"start\":60418},{\"end\":60436,\"start\":60429},{\"end\":60447,\"start\":60440},{\"end\":60777,\"start\":60770},{\"end\":60787,\"start\":60781},{\"end\":60798,\"start\":60791},{\"end\":61026,\"start\":61019},{\"end\":61235,\"start\":61229},{\"end\":61244,\"start\":61239},{\"end\":61253,\"start\":61248},{\"end\":61269,\"start\":61257},{\"end\":61624,\"start\":61617},{\"end\":61634,\"start\":61628},{\"end\":61645,\"start\":61638},{\"end\":61857,\"start\":61854},{\"end\":61865,\"start\":61861},{\"end\":61872,\"start\":61869},{\"end\":61878,\"start\":61876},{\"end\":62221,\"start\":62218},{\"end\":62229,\"start\":62225},{\"end\":62579,\"start\":62574},{\"end\":62791,\"start\":62785},{\"end\":62804,\"start\":62797},{\"end\":63160,\"start\":63153},{\"end\":63333,\"start\":63328},{\"end\":63342,\"start\":63337},{\"end\":63353,\"start\":63346},{\"end\":63366,\"start\":63357},{\"end\":63380,\"start\":63370},{\"end\":63665,\"start\":63660},{\"end\":63673,\"start\":63669},{\"end\":63962,\"start\":63957},{\"end\":63975,\"start\":63966},{\"end\":63985,\"start\":63979},{\"end\":64176,\"start\":64174},{\"end\":64186,\"start\":64180},{\"end\":64575,\"start\":64568},{\"end\":64584,\"start\":64579},{\"end\":64600,\"start\":64588},{\"end\":64866,\"start\":64859},{\"end\":64875,\"start\":64870},{\"end\":64891,\"start\":64879},{\"end\":65256,\"start\":65246},{\"end\":65586,\"start\":65582},{\"end\":65593,\"start\":65590},{\"end\":65610,\"start\":65597},{\"end\":65978,\"start\":65967},{\"end\":66310,\"start\":66302},{\"end\":66320,\"start\":66314},{\"end\":66332,\"start\":66324},{\"end\":66343,\"start\":66336},{\"end\":66356,\"start\":66347},{\"end\":66368,\"start\":66360},{\"end\":66749,\"start\":66743},{\"end\":66757,\"start\":66753},{\"end\":66764,\"start\":66761},{\"end\":67194,\"start\":67188},{\"end\":67524,\"start\":67518},{\"end\":67693,\"start\":67688},{\"end\":57732,\"start\":57728},{\"end\":57741,\"start\":57736},{\"end\":57963,\"start\":57956},{\"end\":57975,\"start\":57967},{\"end\":58272,\"start\":58264},{\"end\":58285,\"start\":58276},{\"end\":58533,\"start\":58529},{\"end\":58544,\"start\":58537},{\"end\":58557,\"start\":58548},{\"end\":58566,\"start\":58561},{\"end\":58575,\"start\":58570},{\"end\":58860,\"start\":58854},{\"end\":59093,\"start\":59084},{\"end\":59102,\"start\":59097},{\"end\":59110,\"start\":59106},{\"end\":59375,\"start\":59366},{\"end\":59386,\"start\":59379},{\"end\":59397,\"start\":59390},{\"end\":59408,\"start\":59401},{\"end\":59712,\"start\":59706},{\"end\":59724,\"start\":59716},{\"end\":59736,\"start\":59728},{\"end\":60098,\"start\":60093},{\"end\":60108,\"start\":60102},{\"end\":60120,\"start\":60112},{\"end\":60134,\"start\":60124},{\"end\":60403,\"start\":60396},{\"end\":60414,\"start\":60407},{\"end\":60425,\"start\":60418},{\"end\":60436,\"start\":60429},{\"end\":60447,\"start\":60440},{\"end\":60777,\"start\":60770},{\"end\":60787,\"start\":60781},{\"end\":60798,\"start\":60791},{\"end\":61026,\"start\":61019},{\"end\":61235,\"start\":61229},{\"end\":61244,\"start\":61239},{\"end\":61253,\"start\":61248},{\"end\":61269,\"start\":61257},{\"end\":61624,\"start\":61617},{\"end\":61634,\"start\":61628},{\"end\":61645,\"start\":61638},{\"end\":61857,\"start\":61854},{\"end\":61865,\"start\":61861},{\"end\":61872,\"start\":61869},{\"end\":61878,\"start\":61876},{\"end\":62221,\"start\":62218},{\"end\":62229,\"start\":62225},{\"end\":62579,\"start\":62574},{\"end\":62791,\"start\":62785},{\"end\":62804,\"start\":62797},{\"end\":63160,\"start\":63153},{\"end\":63333,\"start\":63328},{\"end\":63342,\"start\":63337},{\"end\":63353,\"start\":63346},{\"end\":63366,\"start\":63357},{\"end\":63380,\"start\":63370},{\"end\":63665,\"start\":63660},{\"end\":63673,\"start\":63669},{\"end\":63962,\"start\":63957},{\"end\":63975,\"start\":63966},{\"end\":63985,\"start\":63979},{\"end\":64176,\"start\":64174},{\"end\":64186,\"start\":64180},{\"end\":64575,\"start\":64568},{\"end\":64584,\"start\":64579},{\"end\":64600,\"start\":64588},{\"end\":64866,\"start\":64859},{\"end\":64875,\"start\":64870},{\"end\":64891,\"start\":64879},{\"end\":65256,\"start\":65246},{\"end\":65586,\"start\":65582},{\"end\":65593,\"start\":65590},{\"end\":65610,\"start\":65597},{\"end\":65978,\"start\":65967},{\"end\":66310,\"start\":66302},{\"end\":66320,\"start\":66314},{\"end\":66332,\"start\":66324},{\"end\":66343,\"start\":66336},{\"end\":66356,\"start\":66347},{\"end\":66368,\"start\":66360},{\"end\":66749,\"start\":66743},{\"end\":66757,\"start\":66753},{\"end\":66764,\"start\":66761},{\"end\":67194,\"start\":67188},{\"end\":67524,\"start\":67518},{\"end\":67693,\"start\":67688}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13650160},\"end\":57952,\"start\":57639},{\"attributes\":{\"id\":\"b1\"},\"end\":58186,\"start\":57954},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":463216},\"end\":58487,\"start\":58188},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11803579},\"end\":58814,\"start\":58489},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9803204},\"end\":59022,\"start\":58816},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":583661},\"end\":59311,\"start\":59024},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10908021},\"end\":59631,\"start\":59313},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15781767},\"end\":60041,\"start\":59633},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":115174},\"end\":60353,\"start\":60043},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2497886},\"end\":60670,\"start\":60355},{\"attributes\":{\"id\":\"b10\"},\"end\":60954,\"start\":60672},{\"attributes\":{\"id\":\"b11\"},\"end\":61184,\"start\":60956},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":842488},\"end\":61581,\"start\":61186},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2709712},\"end\":61818,\"start\":61583},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8153773},\"end\":62140,\"start\":61820},{\"attributes\":{\"id\":\"b15\"},\"end\":62532,\"start\":62142},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5360764},\"end\":62738,\"start\":62534},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14045921},\"end\":63101,\"start\":62740},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":30028243},\"end\":63272,\"start\":63103},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":70831},\"end\":63614,\"start\":63274},{\"attributes\":{\"id\":\"b20\"},\"end\":63919,\"start\":63616},{\"attributes\":{\"id\":\"b21\"},\"end\":64111,\"start\":63921},{\"attributes\":{\"id\":\"b22\"},\"end\":64523,\"start\":64113},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3026868},\"end\":64802,\"start\":64525},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6907693},\"end\":65203,\"start\":64804},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14620324},\"end\":65507,\"start\":65205},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7105713},\"end\":65911,\"start\":65509},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5914287},\"end\":66237,\"start\":65913},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":44112860},\"end\":66642,\"start\":66239},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1587},\"end\":67086,\"start\":66644},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":484335},\"end\":67485,\"start\":67088},{\"attributes\":{\"id\":\"b31\"},\"end\":67603,\"start\":67487},{\"attributes\":{\"id\":\"b32\"},\"end\":67977,\"start\":67605},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13650160},\"end\":57952,\"start\":57639},{\"attributes\":{\"id\":\"b1\"},\"end\":58186,\"start\":57954},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":463216},\"end\":58487,\"start\":58188},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11803579},\"end\":58814,\"start\":58489},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9803204},\"end\":59022,\"start\":58816},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":583661},\"end\":59311,\"start\":59024},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10908021},\"end\":59631,\"start\":59313},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15781767},\"end\":60041,\"start\":59633},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":115174},\"end\":60353,\"start\":60043},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2497886},\"end\":60670,\"start\":60355},{\"attributes\":{\"id\":\"b10\"},\"end\":60954,\"start\":60672},{\"attributes\":{\"id\":\"b11\"},\"end\":61184,\"start\":60956},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":842488},\"end\":61581,\"start\":61186},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2709712},\"end\":61818,\"start\":61583},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8153773},\"end\":62140,\"start\":61820},{\"attributes\":{\"id\":\"b15\"},\"end\":62532,\"start\":62142},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5360764},\"end\":62738,\"start\":62534},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14045921},\"end\":63101,\"start\":62740},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":30028243},\"end\":63272,\"start\":63103},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":70831},\"end\":63614,\"start\":63274},{\"attributes\":{\"id\":\"b20\"},\"end\":63919,\"start\":63616},{\"attributes\":{\"id\":\"b21\"},\"end\":64111,\"start\":63921},{\"attributes\":{\"id\":\"b22\"},\"end\":64523,\"start\":64113},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3026868},\"end\":64802,\"start\":64525},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6907693},\"end\":65203,\"start\":64804},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14620324},\"end\":65507,\"start\":65205},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7105713},\"end\":65911,\"start\":65509},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5914287},\"end\":66237,\"start\":65913},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":44112860},\"end\":66642,\"start\":66239},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1587},\"end\":67086,\"start\":66644},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":484335},\"end\":67485,\"start\":67088},{\"attributes\":{\"id\":\"b31\"},\"end\":67603,\"start\":67487},{\"attributes\":{\"id\":\"b32\"},\"end\":67977,\"start\":67605}]", "bib_title": "[{\"end\":57724,\"start\":57639},{\"end\":58260,\"start\":58188},{\"end\":58525,\"start\":58489},{\"end\":58850,\"start\":58816},{\"end\":59080,\"start\":59024},{\"end\":59362,\"start\":59313},{\"end\":59702,\"start\":59633},{\"end\":60089,\"start\":60043},{\"end\":60392,\"start\":60355},{\"end\":61225,\"start\":61186},{\"end\":61613,\"start\":61583},{\"end\":61850,\"start\":61820},{\"end\":62214,\"start\":62142},{\"end\":62570,\"start\":62534},{\"end\":62779,\"start\":62740},{\"end\":63149,\"start\":63103},{\"end\":63324,\"start\":63274},{\"end\":63656,\"start\":63616},{\"end\":64170,\"start\":64113},{\"end\":64564,\"start\":64525},{\"end\":64855,\"start\":64804},{\"end\":65242,\"start\":65205},{\"end\":65578,\"start\":65509},{\"end\":65963,\"start\":65913},{\"end\":66298,\"start\":66239},{\"end\":66739,\"start\":66644},{\"end\":67184,\"start\":67088},{\"end\":67684,\"start\":67605},{\"end\":57724,\"start\":57639},{\"end\":58260,\"start\":58188},{\"end\":58525,\"start\":58489},{\"end\":58850,\"start\":58816},{\"end\":59080,\"start\":59024},{\"end\":59362,\"start\":59313},{\"end\":59702,\"start\":59633},{\"end\":60089,\"start\":60043},{\"end\":60392,\"start\":60355},{\"end\":61225,\"start\":61186},{\"end\":61613,\"start\":61583},{\"end\":61850,\"start\":61820},{\"end\":62214,\"start\":62142},{\"end\":62570,\"start\":62534},{\"end\":62779,\"start\":62740},{\"end\":63149,\"start\":63103},{\"end\":63324,\"start\":63274},{\"end\":63656,\"start\":63616},{\"end\":64170,\"start\":64113},{\"end\":64564,\"start\":64525},{\"end\":64855,\"start\":64804},{\"end\":65242,\"start\":65205},{\"end\":65578,\"start\":65509},{\"end\":65963,\"start\":65913},{\"end\":66298,\"start\":66239},{\"end\":66739,\"start\":66644},{\"end\":67184,\"start\":67088},{\"end\":67684,\"start\":67605}]", "bib_author": "[{\"end\":57734,\"start\":57726},{\"end\":57743,\"start\":57734},{\"end\":57965,\"start\":57954},{\"end\":57977,\"start\":57965},{\"end\":58274,\"start\":58262},{\"end\":58287,\"start\":58274},{\"end\":58535,\"start\":58527},{\"end\":58546,\"start\":58535},{\"end\":58559,\"start\":58546},{\"end\":58568,\"start\":58559},{\"end\":58577,\"start\":58568},{\"end\":58862,\"start\":58852},{\"end\":59095,\"start\":59082},{\"end\":59104,\"start\":59095},{\"end\":59112,\"start\":59104},{\"end\":59377,\"start\":59364},{\"end\":59388,\"start\":59377},{\"end\":59399,\"start\":59388},{\"end\":59410,\"start\":59399},{\"end\":59714,\"start\":59704},{\"end\":59726,\"start\":59714},{\"end\":59738,\"start\":59726},{\"end\":60100,\"start\":60091},{\"end\":60110,\"start\":60100},{\"end\":60122,\"start\":60110},{\"end\":60136,\"start\":60122},{\"end\":60405,\"start\":60394},{\"end\":60416,\"start\":60405},{\"end\":60427,\"start\":60416},{\"end\":60438,\"start\":60427},{\"end\":60449,\"start\":60438},{\"end\":60779,\"start\":60768},{\"end\":60789,\"start\":60779},{\"end\":60800,\"start\":60789},{\"end\":61028,\"start\":61017},{\"end\":61237,\"start\":61227},{\"end\":61246,\"start\":61237},{\"end\":61255,\"start\":61246},{\"end\":61271,\"start\":61255},{\"end\":61626,\"start\":61615},{\"end\":61636,\"start\":61626},{\"end\":61647,\"start\":61636},{\"end\":61859,\"start\":61852},{\"end\":61867,\"start\":61859},{\"end\":61874,\"start\":61867},{\"end\":61880,\"start\":61874},{\"end\":62223,\"start\":62216},{\"end\":62231,\"start\":62223},{\"end\":62581,\"start\":62572},{\"end\":62793,\"start\":62781},{\"end\":62806,\"start\":62793},{\"end\":63162,\"start\":63151},{\"end\":63335,\"start\":63326},{\"end\":63344,\"start\":63335},{\"end\":63355,\"start\":63344},{\"end\":63368,\"start\":63355},{\"end\":63382,\"start\":63368},{\"end\":63667,\"start\":63658},{\"end\":63675,\"start\":63667},{\"end\":63964,\"start\":63955},{\"end\":63977,\"start\":63964},{\"end\":63987,\"start\":63977},{\"end\":64178,\"start\":64172},{\"end\":64188,\"start\":64178},{\"end\":64577,\"start\":64566},{\"end\":64586,\"start\":64577},{\"end\":64602,\"start\":64586},{\"end\":64868,\"start\":64857},{\"end\":64877,\"start\":64868},{\"end\":64893,\"start\":64877},{\"end\":65258,\"start\":65244},{\"end\":65588,\"start\":65580},{\"end\":65595,\"start\":65588},{\"end\":65612,\"start\":65595},{\"end\":65980,\"start\":65965},{\"end\":66312,\"start\":66300},{\"end\":66322,\"start\":66312},{\"end\":66334,\"start\":66322},{\"end\":66345,\"start\":66334},{\"end\":66358,\"start\":66345},{\"end\":66370,\"start\":66358},{\"end\":66751,\"start\":66741},{\"end\":66759,\"start\":66751},{\"end\":66766,\"start\":66759},{\"end\":67196,\"start\":67186},{\"end\":67526,\"start\":67516},{\"end\":67695,\"start\":67686},{\"end\":57734,\"start\":57726},{\"end\":57743,\"start\":57734},{\"end\":57965,\"start\":57954},{\"end\":57977,\"start\":57965},{\"end\":58274,\"start\":58262},{\"end\":58287,\"start\":58274},{\"end\":58535,\"start\":58527},{\"end\":58546,\"start\":58535},{\"end\":58559,\"start\":58546},{\"end\":58568,\"start\":58559},{\"end\":58577,\"start\":58568},{\"end\":58862,\"start\":58852},{\"end\":59095,\"start\":59082},{\"end\":59104,\"start\":59095},{\"end\":59112,\"start\":59104},{\"end\":59377,\"start\":59364},{\"end\":59388,\"start\":59377},{\"end\":59399,\"start\":59388},{\"end\":59410,\"start\":59399},{\"end\":59714,\"start\":59704},{\"end\":59726,\"start\":59714},{\"end\":59738,\"start\":59726},{\"end\":60100,\"start\":60091},{\"end\":60110,\"start\":60100},{\"end\":60122,\"start\":60110},{\"end\":60136,\"start\":60122},{\"end\":60405,\"start\":60394},{\"end\":60416,\"start\":60405},{\"end\":60427,\"start\":60416},{\"end\":60438,\"start\":60427},{\"end\":60449,\"start\":60438},{\"end\":60779,\"start\":60768},{\"end\":60789,\"start\":60779},{\"end\":60800,\"start\":60789},{\"end\":61028,\"start\":61017},{\"end\":61237,\"start\":61227},{\"end\":61246,\"start\":61237},{\"end\":61255,\"start\":61246},{\"end\":61271,\"start\":61255},{\"end\":61626,\"start\":61615},{\"end\":61636,\"start\":61626},{\"end\":61647,\"start\":61636},{\"end\":61859,\"start\":61852},{\"end\":61867,\"start\":61859},{\"end\":61874,\"start\":61867},{\"end\":61880,\"start\":61874},{\"end\":62223,\"start\":62216},{\"end\":62231,\"start\":62223},{\"end\":62581,\"start\":62572},{\"end\":62793,\"start\":62781},{\"end\":62806,\"start\":62793},{\"end\":63162,\"start\":63151},{\"end\":63335,\"start\":63326},{\"end\":63344,\"start\":63335},{\"end\":63355,\"start\":63344},{\"end\":63368,\"start\":63355},{\"end\":63382,\"start\":63368},{\"end\":63667,\"start\":63658},{\"end\":63675,\"start\":63667},{\"end\":63964,\"start\":63955},{\"end\":63977,\"start\":63964},{\"end\":63987,\"start\":63977},{\"end\":64178,\"start\":64172},{\"end\":64188,\"start\":64178},{\"end\":64577,\"start\":64566},{\"end\":64586,\"start\":64577},{\"end\":64602,\"start\":64586},{\"end\":64868,\"start\":64857},{\"end\":64877,\"start\":64868},{\"end\":64893,\"start\":64877},{\"end\":65258,\"start\":65244},{\"end\":65588,\"start\":65580},{\"end\":65595,\"start\":65588},{\"end\":65612,\"start\":65595},{\"end\":65980,\"start\":65965},{\"end\":66312,\"start\":66300},{\"end\":66322,\"start\":66312},{\"end\":66334,\"start\":66322},{\"end\":66345,\"start\":66334},{\"end\":66358,\"start\":66345},{\"end\":66370,\"start\":66358},{\"end\":66751,\"start\":66741},{\"end\":66759,\"start\":66751},{\"end\":66766,\"start\":66759},{\"end\":67196,\"start\":67186},{\"end\":67526,\"start\":67516},{\"end\":67695,\"start\":67686}]", "bib_venue": "[{\"end\":57779,\"start\":57743},{\"end\":58025,\"start\":57977},{\"end\":58323,\"start\":58287},{\"end\":58626,\"start\":58577},{\"end\":58905,\"start\":58862},{\"end\":59151,\"start\":59112},{\"end\":59459,\"start\":59410},{\"end\":59801,\"start\":59738},{\"end\":60185,\"start\":60136},{\"end\":60498,\"start\":60449},{\"end\":60766,\"start\":60672},{\"end\":61015,\"start\":60956},{\"end\":61343,\"start\":61271},{\"end\":61683,\"start\":61647},{\"end\":61943,\"start\":61880},{\"end\":62300,\"start\":62231},{\"end\":62628,\"start\":62581},{\"end\":62881,\"start\":62806},{\"end\":63174,\"start\":63162},{\"end\":63431,\"start\":63382},{\"end\":63735,\"start\":63675},{\"end\":63953,\"start\":63921},{\"end\":64273,\"start\":64188},{\"end\":64651,\"start\":64602},{\"end\":64964,\"start\":64893},{\"end\":65323,\"start\":65258},{\"end\":65675,\"start\":65612},{\"end\":66043,\"start\":65980},{\"end\":66420,\"start\":66370},{\"end\":66829,\"start\":66766},{\"end\":67256,\"start\":67196},{\"end\":67514,\"start\":67487},{\"end\":67758,\"start\":67695},{\"end\":57779,\"start\":57743},{\"end\":58025,\"start\":57977},{\"end\":58323,\"start\":58287},{\"end\":58626,\"start\":58577},{\"end\":58905,\"start\":58862},{\"end\":59151,\"start\":59112},{\"end\":59459,\"start\":59410},{\"end\":59801,\"start\":59738},{\"end\":60185,\"start\":60136},{\"end\":60498,\"start\":60449},{\"end\":60766,\"start\":60672},{\"end\":61015,\"start\":60956},{\"end\":61343,\"start\":61271},{\"end\":61683,\"start\":61647},{\"end\":61943,\"start\":61880},{\"end\":62300,\"start\":62231},{\"end\":62628,\"start\":62581},{\"end\":62881,\"start\":62806},{\"end\":63174,\"start\":63162},{\"end\":63431,\"start\":63382},{\"end\":63735,\"start\":63675},{\"end\":63953,\"start\":63921},{\"end\":64273,\"start\":64188},{\"end\":64651,\"start\":64602},{\"end\":64964,\"start\":64893},{\"end\":65323,\"start\":65258},{\"end\":65675,\"start\":65612},{\"end\":66043,\"start\":65980},{\"end\":66420,\"start\":66370},{\"end\":66829,\"start\":66766},{\"end\":67256,\"start\":67196},{\"end\":67514,\"start\":67487},{\"end\":67758,\"start\":67695},{\"end\":58036,\"start\":58027},{\"end\":59851,\"start\":59803},{\"end\":61402,\"start\":61345},{\"end\":61993,\"start\":61945},{\"end\":62356,\"start\":62302},{\"end\":62943,\"start\":62883},{\"end\":63782,\"start\":63737},{\"end\":64345,\"start\":64275},{\"end\":65022,\"start\":64966},{\"end\":65375,\"start\":65325},{\"end\":65725,\"start\":65677},{\"end\":66093,\"start\":66045},{\"end\":66879,\"start\":66831},{\"end\":67303,\"start\":67258},{\"end\":67808,\"start\":67760},{\"end\":58036,\"start\":58027},{\"end\":59851,\"start\":59803},{\"end\":61402,\"start\":61345},{\"end\":61993,\"start\":61945},{\"end\":62356,\"start\":62302},{\"end\":62943,\"start\":62883},{\"end\":63782,\"start\":63737},{\"end\":64345,\"start\":64275},{\"end\":65022,\"start\":64966},{\"end\":65375,\"start\":65325},{\"end\":65725,\"start\":65677},{\"end\":66093,\"start\":66045},{\"end\":66879,\"start\":66831},{\"end\":67303,\"start\":67258},{\"end\":67808,\"start\":67760}]"}}}, "year": 2023, "month": 12, "day": 17}