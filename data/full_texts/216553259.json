{"id": 216553259, "updated": "2023-10-06 16:42:52.9", "metadata": {"title": "CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents", "authors": "[{\"first\":\"Devashish\",\"last\":\"Prasad\",\"middle\":[]},{\"first\":\"Ayan\",\"last\":\"Gadpal\",\"middle\":[]},{\"first\":\"Kshitij\",\"last\":\"Kapadni\",\"middle\":[]},{\"first\":\"Manish\",\"last\":\"Visave\",\"middle\":[]},{\"first\":\"Kavita\",\"last\":\"Sultanpure\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "publication_date": {"year": 2020, "month": 4, "day": 27}, "abstract": "An automatic table recognition method for interpretation of tabular data in document images majorly involves solving two problems of table detection and table structure recognition. The prior work involved solving both problems independently using two separate approaches. More recent works signify the use of deep learning-based solutions while also attempting to design an end to end solution. In this paper, we present an improved deep learning-based end to end approach for solving both problems of table detection and structure recognition using a single Convolution Neural Network (CNN) model. We propose CascadeTabNet: a Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet) based model that detects the regions of tables and recognizes the structural body cells from the detected tables at the same time. We evaluate our results on ICDAR 2013, ICDAR 2019 and TableBank public datasets. We achieved 3rd rank in ICDAR 2019 post-competition results for table detection while attaining the best accuracy results for the ICDAR 2013 and TableBank dataset. We also attain the highest accuracy results on the ICDAR 2019 table structure recognition dataset. Additionally, we demonstrate effective transfer learning and image augmentation techniques that enable CNNs to achieve very accurate table detection results. Code and dataset has been made available at: https://github.com/DevashishPrasad/CascadeTabNet", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2004.12629", "mag": "3034997246", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/PrasadGKVS20", "doi": "10.1109/cvprw50498.2020.00294"}}, "content": {"source": {"pdf_hash": "25018e89fc6edb26cf4e759c6c97859f862da753", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.12629v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2004.12629", "status": "GREEN"}}, "grobid": {"id": "80b6c22a33b83f4161f673063ef1aae26cfccbd3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/25018e89fc6edb26cf4e759c6c97859f862da753.txt", "contents": "\nCascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents\n\n\nDevashish Prasad devashishkprasad@gmail.com \nPune Institute of Computer Technology\nIndia\n\nAyan Gadpal ayangadpal2@gmail.com \nPune Institute of Computer Technology\nIndia\n\nKshitij Kapadni kshitij.kapadni@gmail.com \nPune Institute of Computer Technology\nIndia\n\nManish Visave manishvisave149@gmail.com \nPune Institute of Computer Technology\nIndia\n\nKavita Sultanpure kasultanpure@pict.edu \nPune Institute of Computer Technology\nIndia\n\nCascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents\nWe eval-uate our results on ICDAR 2013, ICDAR 2019 and Table-Bank public datasets. We achieved 3rd rank in ICDAR 2019 post-competition results for table detection while attaining the best accuracy results for the ICDAR 2013 and Table-Bank dataset. We also attain the highest accuracy results on the ICDAR 2019 table structure recognition dataset. Ad-ditionally, we demonstrate effective transfer learning and image augmentation techniques that enable CNNs to achieve very accurate table detection results. Code and dataset has been made available at: https://github.com/ DevashishPrasad/CascadeTabNet\nAn automatic table recognition method for interpretation of tabular data in document images majorly involves solving two problems of table detection and table structure recognition. The prior work involved solving both problems independently using two separate approaches. More recent works signify the use of deep learning-based solutions while also attempting to design an end to end solution. In this paper, we present an improved deep learning-based end to end approach for solving both problems of table detection and structure recognition using a single Convolution Neural Network (CNN) model. We propose CascadeTab-Net: a Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet) based model that detects the regions of tables and recognizes the structural body cells from the detected tables at the same time.\n\nIntroduction\n\nThe world is changing and going digital. The use of digitized documents instead of physical paper-based documents is growing rapidly. These documents contain a variety of table-based information with variations in appearance and layouts. An automatic table information extraction method  involves two subtasks of table detection and table structure recognition. In table detection, the region of the image that contains the table is identified while table structure recognition involves identification of the rows and columns to identify individual table cells. The prior proposed approaches solved these two sub-problems independently.\n\nIn this paper, we propose CascadeTabNet, an improved deep learning-based end to end approach for solving the two sub-problems using a single model. The problem of table detection is solved using instance segmentation. We perform  table segmentation on each image where we try to identify  each instance of the table within the image at the pixel level.  Similarly, we perform table cell segmentation on each image  to predict segmented regions of table cells within each table  to identify the structure of the table. Table and cell regions are predicted in a single inference (at the same time) by the model. Simultaneously, the model classifies tables into two types as bordered (ruling-based) and borderless (no rulingbased) tables. The model predicts the segmentation of cells only for the unbordered tables. We use a simple rule-based conventional text detection and line detection algorithms for extracting cells from bordered tables.\n\nWe demonstrate the effectiveness of iterative transfer learning to make the CNN learn from less amount of training data as well as enable it to perform well on multiple datasets by fine-tuning it on respective datasets. A new way of image augmentation was also implanted into the training process to enhance the accuracy of table detection and helping it learn more effectively. Evaluation for table detection task was performed on three public datasets of the ICDAR 2013, ICDAR 2019 competition (Track A) dataset and TableBank dataset. We achieve 3rd rank in post-competition results of ICDAR 2019 for table detection. We achieve the highest accuracy for table detection task on ICDAR 2013 dataset and all of the three subsets of the TableBank dataset. For table structure recognition tasks we evaluate the model on ICDAR 2019 dataset (Track B2) and achieve the highest rank in postcompetition results.\n\nOur main contributions made in this paper are as per the following:\n\n1. We propose CascadeTabNet: an end-to-end deeplearning-based approach that uses the Cascade Mask R-CNN HRNet model for both table detection and structure recognition.\n\n2. We show that the proposed image transformation techniques for image augmentation for training enhances the table detection accuracy significantly.\n\n3. We perform a comparative analysis of various CNN models for the table detection task in which the Cascade Mask R-CNN HRNet model outperforms other models. 4. We demonstrate an effective iterative transfer learningbased methodology that helps the model to perform well on different types of datasets using a small amount of training data.\n\n\n5.\n\nWe manually annotated some of the ICDAR 19 dataset images for table cell detection in borderless tables while also categorizing tables into two classes (bordered and borderless) and will be releasing the annotations to the community.\n\n\nRelated work\n\nIn 1997, P. Pyreddy and, W. B. Croft [19] was the first to propose an approach of detecting tables using heuristics like a Character Alignment, holes and gaps. To improve accuracy, Wonkyo Seo et al. [22] used the Junctions (intersection of the horizontal and vertical line) detection with some post-processing. T. Kasar et al. [15] also used the junction detection, but instead of heuristics, they passed the junction information to SVM.\n\nWith the ascent of Deep Learning and object detection, Azka Gilani et al. [9] was the first to propose a Deep learningbased approach for Table Detection by using Faster R-CNN based Model. They also attempted to improve the accuracy of models by introducing distance-based augmentation to detect tables. Some approaches tried to utilize the semantic information, Such as S. Arif and F. Shafait [1] attempted to improve the accuracy of Faster R-CNN by using semantic color-coding of text and Dafang He et al. [12], used FCN for semantic page segmentation with an end verification network is to determine whether the segmented part is the table or not.\n\nIn 1998, Kieninger and Dengel [16], proposed the initial approach for Table Structure Recognition by clubbing the text into chunks and dividing those chunks into cells based on the column border. Tables have many basic objects such as lines and characters. Waleed Farrukh et al. [7], used a bottom-up heuristic-based approach on these basic objects to construct the cells. Zewen, Chi et al. [5] proposed a graphbased approach for table structure recognition in which they used the SciTSR dataset constructed by themselves for training the GraphTSR model. Sebastian Schreiber et al. [21] were the first to perform  table detection and structure recognition together with a 2  fold system which Faster RCNN for table detection and,  Subsequently, deep learning-based semantic segmentation  for table structure recognition. To make the model more generalize, Mohammad Mohsin et al. [20] used a combination  of GAN based architecture for table detection and SegNet  based encoder-decoder architecture for table structure segmentation. Recently, Shubham Paliwal et al. [18], was first to propose a deep learning-based end-to-end approach to perform table detection and column detection using encoder-decoder with the VGG-19 as a base semantic segmentation method, where the encoder is the same and decoder is different for both tasks. After detection results for the table are obtained from the model, the rows are extracted from the table region using a semantic rule-based method. This approach uses a Tesseract OCR engine for text location.\n\n\nCascadeTabNet: The presented approach\n\nWe try to focus on using a small amount of data effectively to achieve high accuracy results. Working towards this goal, our primary strategy includes :\n\n1. Using a relatively complex but efficient CNN architecture that attains high accuracy on object detection and segmentation benchmarking datasets as the main component in the approach.\n\n2. Using an iterative transfer learning approach to train the CNN model gradually, starting from more general tasks and going towards more specific tasks. Performing iterations of transfer learning multiple times to extract the needful knowledge effectively from a small amount of data.\n\n3. Strengthening the learning process by applying image transformation techniques to training images for data augmentation.\n\nWe elaborate on the strategies in the following subsections and explain the pipeline of the approach.\n\n\nModel architecture\n\nTo attain very high accuracy results we use a model that was made by the combination of two approaches. Cascade RCNN was originally proposed by Cai and Vasconcelos [2] to solve the paradox of high-quality detection in CNNs by introducing a multi-stage model. And a modified HRNet was proposed by Jingdong Wang et al. [25] to attain reliable highresolution representations and multi-level representations for semantic segmentation as well as for object detection. Our experiments and analysis show that the cascaded multistaged model with the HRNet backbone network yields the The original architecture of HRNet [14] (HRNetV1) was enhanced for semantic segmentation to form HRNetV2 [25]. And, then a feature pyramid was formed over HRNetV2 for object detection to form HRNetV2p [25]. CascadeTabNet is a three-staged Cascade mask R-CNN HRNet model. A backbone such as a ResNet-50 without the last fully connected layer is a part of the model that transforms an image to feature maps. CascadeTabNet uses HRNetV2p W32 [25] (32 indicates the width of the high-resolution convolution) as the backbone for the model.\n\nThe architecture strategy of the Cascade mask R-CNN [3] is very similar to the Cascade R-CNN [2]. The Cascade R-CNN architecture is extended to the instance segmentation task, by attaching a segmentation branch as done in the Mask R-CNN [13]. To explain the model architecture we try to use the naming conventions similar to that of the Mmdetection framework [4]. As shown in figure 1, the image \"I\" is fed into the model. The backbone CNN HR NetV2p W32 transforms the image \"I\" to feature maps. The \"RPN Head\" (Dense Head) predicts the preliminary object proposals for these feature maps. The \"Bbox Heads\" take RoI features as input and make RoI-wise predictions. Each head makes two predictions as bounding box classification scores and box regression points. \"B\" denotes the bounding boxes predicted by the heads and, for simplicity, we do not show the classification scores in the figure. The \"Mask Head\" predicts the masks for the objects and \"S\" denotes a segmentation output. At the inference, object detections made by \"Bbox Heads\" are complemented with segmentation masks made by \"Mask Head\", for all detected objects.\n\nFor image segmentation using the Cascade R-CNN, Cai and Vasconcelos [3] propose multiple strategies in which the segmentation branch is placed at various stages of the network. CascadeTabNet utilizes the strategy of adding the segmentation branch at the last stage of the Cascade R-CNN. The model was implemented using the MMdetection toolbox [4]. We use the default implementation (cascade mask rcnn hrnetv2p w32 20e) of the model for our experiments and analysis. \n\n\nIterative transfer learning\n\nBoth the tasks involve object segmentation, and we use a multi-task learning approach as well as multiple iterations of transfer-learning to achieve our goal. In short, we first train our model on a general dataset and then fine-tune it multiple times for specific datasets. More precisely, we use two iterations of transfer learning and so we call this approach as two-stage transfer learning.\n\nFirst, we create a general dataset for a general task of table detection. We add images of different types of documents like word and latex in this dataset. These documents contain tables of various types like bordered, semi-bordered and borderless. A bordered table is one for which an algorithm can use just the line positions to estimate the cells and overall structure of the table. If some of the lines are missing, it becomes difficult for a line detection based algorithm to separate the adjacent cells of the table. We call such a table as a semi-bordered table, in which some lines are not present. And a borderless table is one which doesn't have any lines. Detecting only the tables in images is a general task for an algorithm, but detecting them according to their types is a specific task. For example, detecting dogs in images is a general task, but detecting only the bulldogs and pugs is a more specific task that requires relatively more data by the model. To make it a general task for table recognition, initially, all these tables in the images are annotated as of one class (the table class), which enables the model to learn common and general features to detect tables. The trained model can use this knowledge to learn even more specific tasks like table detection according to their types.\n\nThe two-stage transfer learning strategy is used to make a single model learn end to end table recognition using a small amount of data. In this strategy, transfer learning is practiced two times on the same model. Detecting tables in images becomes a specific task for a CNN model that was earlier trained on a dataset with hundred-thousands of images to detect objects from thousand classes. So in the first iteration of transfer learning, we initialize our CNN model with the pre-trained imagenet coco model weights before training. It enables the CNN model to learn only task-specific higher-level features while getting some advantages like the lesser need for training data and reducing total training time due to beforehand knowledge. After training, CNN successfully predicts the table detection masks for tables in the images. Similarly, in the second iteration, the model is again fine-tuned on a smaller dataset to accomplish even more specific task of predicting the cell masks in borderless tables along with detecting tables according to their types. Another challenging and specific task can be table detection for a particular type of document images (latex documents). We do not freeze any of the layers in the model at any stage while performing iterative transfer learning.\n\nFor the task of table structure recognition, which involves predicting the cell masks in borderless tables along with detecting the different types of tables, we create a smaller dataset. It contains lesser images than that for table detection. This new dataset contains slightly advanced annotations intimating the model to detect tables of two types with their labels (two classes) as bordered and borderless, as well as predict borderless table cell masks (total three classes). We put borderless and semi-bordered tables in one class, the borderless class. We put semi-bordered tables in borderless class because we cannot use only line information to extract cells out of it. We need cell predictions for semi-bordered tables from the model. After again fine-tuning the model on this dataset, it successfully detects tables with their type and also predicts segmentation masks for table body and cells for borderless tables with very high accuracy.\n\nThis strategy worked effectively because while doing the knowledge transfer between two tasks the domains of both the tasks were the same. If domains of two tasks are different, for example, training a model to detect dogs in images and then using the same model to detect different types of horses, then it may result in a negative transfer. Figure 2 shows the figurative explanation to two-staged transfer learning where the same model is trained iteratively from general to a more specific task, reducing the size of the dataset as we move down.\n\n\nImage Transformation and data augmentation\n\nProviding a large amount of training data can easily produce deep-learning-based models that can attain very high accuracy results. Adding more training data also prevents models from over-fitting to the training data. For this concern, we try to implement image-augmentation techniques on the original training images to increase the size of training data. But, not all of these techniques would be very effective for augmenting document images. For example, the use of shear and rotation transformations won't be an effective strategy because the digital documents in the datasets are perfectly axis-aligned. We try to implement the techniques that will help the model to learn more accurately.\n\nDocuments have text or content regions and blank spaces in them. As the text elements are very small in documents and the proposed model was used for detecting real-world objects in images, we try to make the contents better understandable to the object segmentation model by thickening the text regions and reducing the regions of the blank space. We propose image transformation techniques that help the model to learn more efficiently. The transformed images are added in the original dataset, which also increases the amount of relevant training data for the model. We propose two types of image transformation techniques as Dilation transform and Smudge transform.\n\n\nDilation transform\n\nIn the dilation transform, we transform the original image to thicken the black pixel regions. We convert the original images into binary images before applying the dilation transform. Figure 3, a) is the original image and b) is the transformed dilated image. A 2x2 kernel filter for one iteration was applied to the binary image to generate the transformed image. Experiments showed that the kernel size of 2x2 gave better results.\n\n\nSmudge transform\n\nIn the smudge transform, we transform the original image to spread the black pixel regions and make it look like a kind of smeary blurred black pixel region. The original images are converted into binary images before the smudge transform is applied. In Figure 3, a) is the original image and c) is the transformed smudged image. Smudge transform is implemented using various distance transforms. The original algorithm is described by Gilani et al. [9] that applies Euclidean Distance Transform, Linear Distance Transform, and Max Distance Transform to the image. Also, some additional normalization and parameter tuning enhanced the results.  \n\n\nPipeline\n\nIn this section, we describe various stages in the pipeline of the CascadeTabNet end to end system for table recognition. Figure 4, shows the block diagram of the pipeline. The two-stage fine-tuned CasacdeTabNet model takes in the image of the document containing zero or more tables. It predicts the segmentation masks for tables of two types as bordered and borderless, as discussed earlier. Next in the pipeline, we have separate branches for bordered and borderless tables. Depending on the type of the detected table it is further processed by its respective branch post-processing module. Post-processing modules perform trivial tasks of arranging and cleaning the outputs of the model. In the borderless branch, we arrange the predicted cells detected inside the table into rows and columns based on their positions. We estimate the missing table lines using the positions of identified rows and columns. Based on these lines, for undetected cells, we detect cells using a contourbased text detection algorithm. And finally, Row-span and Col-span cells are also identified after estimating the lines.\n\nIn the bordered branch, a conventional algorithm of line detection is used to detect lines of bordered tables. The cells are identified using the line intersection points. And within each cell, the text regions are detected by using the contourbased text detection algorithm. We prefer not to train our model for bordered table cell segmentation masks prediction because using the line information from bordered tables is much easier and efficient to recognize the cells.\n\n\nDataset Preparation\n\nFor creating a General dataset for table detection task we merge three datasets of ICDAR 19 (cTDaR) [8], Marmot [6] and Github 1 [23]. The cTDaR competition aims at benchmarking state-of-the-art table detection (TRACK A) containing two subsets of the dataset as Modern and Archival, further described in [8]. We include only the modern subset of this dataset in the general dataset. This subset contains several images of word and latex documents, having text in English and Chinese languages. We also include a 1 https://github.com/sgrpanchal31/table-detection-dataset publicly available Marmot dataset published by the Institute of Computer Science and Technology of Peking University, further described in [6]. Marmot dataset holds two subsets as Chinese and English, we include both sets in the general dataset. As done by DeepDeSRT [21], to achieve the best possible results, we removed the errors in the ground-truth annotations of the dataset. And finally, we also include a dataset from the internet [23] in the general dataset that contains only borderless table images with some magazine and newspaper based document images. This dataset was also cleaned like the marmot dataset. The General dataset contains a total of 1934 images having 2835 tables in it, and we use this dataset to train a General model.\n\nFor the preliminary analysis of image augmentation, we created four training sets. The first set contains the original images. The second set is created by applying the dilate-transform to all the images in the original set and adding them in the set along with corresponding original images. Similarly, the third set is created by applying the smudge-transform to these original images. And the last set is created by adding the smudged, dilated and original images altogether in the set. In Section 5. we perform a rigorous analysis of these training sets by training different types of models. We show the effectiveness of augmentation techniques, as it boosts the models' performance.\n\nTo evaluate the model on the ICDAR 19 (Track A Modern) competition dataset, we perform the dilate image transform for all the images of the Track A Modern dataset. And then fine-tune the General model on it.\n\nFor testing all of the aforementioned datasets, we use the test set of the ICDAR 19 dataset (Track A Modern). We find this set robust and ideal for testing because it contains all types of images like Latex and Word, having all types of tables.\n\nWe also provide evaluation results on the TableBank dataset [17]. TableBank dataset is a new image-based table detection and recognition dataset that contains Word and Latex documents based 417K table images. The table detection subset of the dataset has 163,417 images in Word, 253,817 images in Latex and 417,234 images in Word+Latex subsets respectively. To demonstrate the effectiveness of our approach, we don't fine-tune the model on the whole dataset. Instead, we fine-tune the model on a very small subset of the actual TableBank datasets. For latex, we only choose 1500 images randomly from the TableBank Latex for training. For creating the test set for latex, we randomly choose 1000 images from the TableBank Latex dataset, as originally done by the authors [17]. Similarly, for Word, we choose 1500 images randomly from the TableBank Word dataset for training. And again, for creating the test set, we randomly choose 1000 images from the TableBank Word dataset. We found that some annotations provided for the TableBank Word dataset images were inappropriate. We preferred not to include these images in the test set. And finally, we create a set for both latex and word by combining the randomly chosen images of word and latex train sets, putting a total number of 3000 images for training. And likewise, for testing, we create the test set by combining the randomly chosen images of test sets of latex and word, putting a total number of 2000 images.\n\nAnd we also evaluate the model on the ICDAR 13 [11] dataset that includes a total of 150 tables. It has two subsets as EU and US, in which there are 75 tables in 27 PDFs from the EU set and 75 tables in 40 PDFs from the US Government. We convert all of these PDFs into images and we get 238 images, out of which we use 40 randomly choose images for fine-tuning and others for testing.\n\nFor creating a dataset for table structure recognition task we manually annotated some images from the ICDAR 19 (Track A Modern) train set. As discussed earlier, this dataset is annotated for three classes. We randomly choose 342 images out of 600 images of the ICDAR 19 train set. It had 114 bordered tables, 429 borderless tables and 24920 cells in borderless tables in these images and were annotated accordingly. We release this dataset to the research community. The test set for table structure recognition was provided by the cTDaR competition track B2. It contains 100 images of all types of documents and tables.\n\n\nResults and Analysis\n\nIn this section, we start by demonstrating the effectiveness of image transformation techniques by performing experiments with a baseline model. Then we show a comparative analysis of various CNN models with Cascade mask RCNN HRNet. And finally, we show the evaluation benchmarks of our model on public datasets. The experiments were performed on Google Colaboratory platform with P100 PCIE GPU of 16 GB GPU memory, Intel(R) Xeon(R) CPU @ 2.30GHz and 12.72 GB of RAM.\n\n\nPreliminary Analysis\n\nTo show the effectiveness of the proposed image transformation techniques, we train a baseline model on all four datasets (created by augmenting the general dataset in section 4) and evaluate the results on ICDAR 19 Modern Track A Test set. We try to obtain a dataset out of the four datasets that help the model to do better. We chose the Faster-R-CNN resnext101 64x4d (cardinality = 64 and Bottleneck width = 4) model as the baseline model. The Mmdetection toolbox was used to implement the model with the default training configurations provided by the framework.\n\nEvaluation metrics for ICDAR 19 dataset are based on IoU (Intersection over Union) to evaluate the performance of table region detection. Precision, Recall and, F1 scores are calculated with IoU threshold 0.6, 0.7, 0.8 and 0.9 respectively. The Weighted-Average F1 (WAvg.) is calculated by assigning a weight to each F1 value of the corresponding IoU threshold. As a result, the F1 scores with higher IoUs are given more importance than those with lower IoUs. The details of the metric are further explained by Gao et al. [8]. Table 1 shows the F1-scores for the IoU thresholds of baseline models on the ICDAR Test (Track A Modern). And, the model trained on the dataset having images of both augmentation techniques performs significantly better than other dataset models.\n\nThese results proved that both image transformation techniques for data augmentation help the model learn more effectively. So, we use both image transformation techniques on our General dataset for further experiments on the table detection task.  To show the comparative analysis of the CascadeTabNet model with all other Cascade R-CNN and HRNet based object detection and instance segmentation models, we use the General dataset with both augmentation techniques for training. We use Mmdetection based implementation of all the models using the default configurations. All of these models have pre-trained backbones on ImageNet dataset using training schedules as of 1x (12 epochs) and 2x (24 epochs), further described in [4]. And all models utilize the Feature Pyramid Network (FPN) neck. We fine-tuned the following object detection and instance segmentation models.  Table 2 shows the evaluated F1-scores of all models on the ICDAR Test (Track A Modern) set. As seen in the table, the multi-stage cascaded network methodology along with HRNet backbone based models dominate other models. And, instance segmentation models do better than the object detection models. The Cascade mask R-CNN HRNet models achieves the highest accuracy among all models because of the fusion of two methodologies of multi-staged cascading and high-resolution convolutions used for instance segmentation.\n\n\nTable detection evaluation\n\nWe again perform the iterative transfer learning technique to fine-tune our General model (Cascade mask R-CNN HR-Net) on ICDAR 13, ICDAR 19 and TableBank datasets respectively for evaluation.\n\nFirst, we fine-tune Cascade mask R-CNN HRNet on the ICDAR 19 track A train set along with dilation transform augmentation, and the following results were obtained on the modern tack A test set. We achieved 3rd rank on the postcompetition leader board according to weighted-average metrics but attained the best accuracy for IoU 0.9, Table  3. The winner of the competition TableRadar performs two types of post-processing over the original output from the network. They merge the regions whose overlapped areas are larger than the defined threshold. And, detect lines in candidate table regions such that if the detected line extends  over table-border, the table region [8]. The advantage of our approach over the approaches of the winner and runner-up is that both of these approaches involve some kind of post-processing after the original output of the network. But, in our approach, we do not perform any type of post-processing. Our model directly outputs the accurate table region masks leveraging its architectural design and the techniques implanted during its training.  [9], where the metrics for all documents are computed by summing up the area of overlap, prediction, and ground truth. At this point, we want to emphasize that, we only use 1500 images from word, 1500 from latex and 3000 images for word+latex datasets for training(finetuning) the models. We achieved the best accuracy results for all of the three subsets, Table 4. Evaluation metrics for ICDAR 2013 is based on completeness and purity of the sub-objects of a table. We calculate precision and recall for each table and then take the average, as done by [18]. The metrics is further described by [18], [10] and [24]. We only use 40 images from the dataset for fine-tuning the general model and 198 images for testing, while [18] and [21] used only 34 images for testing and rest of the dataset for training. Results are shown in Table 5.    \n\n\nTable structure recognition evaluation\n\nWe trained the general model on our annotated dataset, and this model is included in the final pipeline. The results are evaluated on the ICDAR 19 Track B2 dataset. The evaluation for this track is done by comparing the structure of a table that is defined as a matrix of cells. For each cell, it is required to return the coordinates of a polygon defining the convex hull of the cell's contents. Additionally, it also requires the start/end column/row information for each cell. It uses cell adjacency relation-based table structure evaluation (based on Gobel et al. [10]). Similar to track A, precision, recall and, F1 scores are calculated with IoU threshold of 0.6, 0.7, 0.8 and 0.9 respectively. We attain the highest accuracy on the post-competition leaderboard ( Table 6), but some high-end post-processing can improve the results significantly. We did not use TableBank Dataset for table structure evaluation because ground truth information provided for the images only contain table structure labels in the form of HTML tags. It does not contain cell or column coordinates, and hence cannot be used to evaluate the performance of object detection or instance segmentation model. And we did not use ICDAR 13 for table structure evaluation because the evaluation metrics of ICDAR 13 uses a text content of the cell-based mapping of ground truth cells and predicted cells. For this concern, we need to extract the text content using an OCR (Optical Character Recognition) engine. And the overall accuracy would also depend on the accuracy of the OCR. We also feel that ICDAR 19 is a better metric than ICDAR 13 where the mapping of the cells is done using IoU thresholds. Figure 5 shows the results of our model. It predicts yellow masks for bordered tables (5 a.) and purple masks for borderless tables (5 b.). It predicts accurate cell masks for most of the borderless tables. For some images where some of the predictions for cells are missed by the model (5 c.), we correct it using line estimation and contour-based text detection algorithm. The model fails badly for some images (5 d.)\n\n\nConclusion\n\nThis paper presents an end-to-end system for table detection and structure recognition. It is shown that existing instance segmentation based CNN architectures which were originally trained for objects in natural scene images are also very effective for detecting tables. And, iterative transfer learning and image augmentation techniques can be used to learn efficiently from a small amount of data. The model starts learning for a general task and iteratively it learns to perform well on specific tasks. The proposed system Cas-cadeTabNet recognizes structures within tables by predicting table cell masks while using the line information as well. Improving the post-processing modules can further enhance the accuracy of the end to end model. Our system performs better on various public datasets for both the tasks.\n\nFigure 1 :\n1CascadeTabNet model architecture best results due to the ability of both the approaches to strive for high accuracy object segmentation.\n\nFigure 2 :\n2Two stage transfer learning\n\n\n(a) Original Image (b) Dilated image (c) Smudged image\n\nFigure 3 :\n3Image transformations\n\nFigure 4 :\n4CascadeTabNet Pipeline\n\nFigure 5 :\n5Results of CasacadeTabNet Model\n\nTable\n\n\nTable 2 :\n2Result of models on ICDAR Test (Track A Modern)Model \nIoU \nWAvg. \n0.6 \n0.7 \n0.8 \n0.9 \nRetina \n0.818 0.785 0.762 0.664 \n0.749 \nFRcnnHr \n0.889 0.877 0.862 0.781 \n0.847 \nCRccnHr \n0.927 0.910 0.901 0.833 \n0.888 \nCRcnnX \n0.929 0.913 0.903 0.852 \n0.895 \nCMRcnnD 0.912 0.897 0.880 0.834 \n0.877 \nCMRcnnX 0.931 0.925 0.909 0.868 \n0.905 \nCMRcnnHr 0.941 0.932 0.923 0.886 \n0.918 \n\n1. Retina : Resnext-101 based RetinaNet model with car-\n\n\n\n\nis extended accordingly. The runner up NLPR PAL used Fully Convolutional Network (FCN) to classify image pixels into two categories: table and background, then table regions are extracted with Connected Component Analysis (CCA). Further details about both the datasets are described in\n\nTable 3 :\n3Comparison with participants of ICDAR 19 Track A (Modern) F1-scores [8] Evaluation metrics for TableBank dataset for table detection are based on, calculating the Precision, Recall, and F1 in the same way as inTeam \nIoU \nWAvg. \n0.6 \n0.7 \n0.8 \n0.9 \nTableRadar 0.969 0.957 0.951 0.897 0.940 \nNLPR-PAL 0.979 0.966 0.939 0.850 0.927 \nOurs \n0.943 0.934 0.925 0.901 0.901 \n\n\n\nTable 4 :\n4TableBank results comparison with baseline[17] Dataset \nModel \nPrecision Recall \nF1 \nResNeXt-101 \n95.93 \n90.44 93.11 \nBoth \nResNeXt-152 \nOurs \n\n96.72 \n92.99 \n\n88.95 \n95.71 \n\n92.67 \n94.33 \nResNeXt-101 \n87.44 \n95.12 91.12 \nLatex \nResNeXt-152 \nOurs \n\n87.20 \n95.92 \n\n96.24 \n97.28 \n\n91.49 \n96.60 \nResNeXt-101 \n95.77 \n76.10 84.81 \nWord \nResNeXt-152 \nOurs \n\n96.50 \n94.35 \n\n80.32 \n95.49 \n\n87.67 \n94.92 \n\n\n\nTable 5 :\n5Results of ICDAR 13 Table detectionModel \nRecall Precision F1-score \nOurs \n1.0 \n1.0 \n1.0 \nDeepDeSRT [21] 0.9615 \n0.9740 \n0.9677 \nTableNet [18] \n0.9628 \n0.9697 \n0.9662 \n\n\n\nTable 6 :\n6Comparison with participants of ICDAR 19 Track B2 (Modern) F1-scores[8] Team \nIoU \nWAvg. \n0.6 \n0.7 \n0.8 \n0.9 \nOurs \nNLPR-PAL \n\n0.438 \n0.365 \n\n0.354 \n0.305 \n\n0.190 \n0.195 \n\n0.036 \n0.035 \n\n0.232 \n0.206 \n\n\n\nTable detection in document images using foreground and background features. S Arif, F Shafait, Digital Image Computing: Techniques and Applications (DICTA). S. Arif and F. Shafait. Table detection in document images using foreground and background features. In 2018 Digital Image Computing: Techniques and Applications (DICTA), pages 1-8, 2018.\n\nCascade r-cnn: Delving into high quality object detection. Zhaowei Cai, Nuno Vasconcelos, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nCascade R-CNN: high quality object detection and instance segmentation. CoRR, abs. Zhaowei Cai, Nuno Vasconcelos, Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: high quality object detection and instance segmentation. CoRR, abs/1906.09756, 2019.\n\nMmdetection: Open mmlab detection toolbox and benchmark. Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, abs/1906.07155Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua LinCoRRKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian- heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Mmdetection: Open mm- lab detection toolbox and benchmark. CoRR, abs/1906.07155, 2019.\n\n. Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Yin, Xian-Ling Mao, Complicated table structure recognition. ArXiv, abs/1908.04729Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanx- uan Yin, and Xian-Ling Mao. Complicated table structure recognition. ArXiv, abs/1908.04729, 2019.\n\nDataset, groundtruth and performance metrics for table detection evaluation. J Fang, X Tao, Z Tang, R Qiu, Y Liu, 10th IAPR International Workshop on Document Analysis Systems. J. Fang, X. Tao, Z. Tang, R. Qiu, and Y. Liu. Dataset, ground- truth and performance metrics for table detection evaluation. In 2012 10th IAPR International Workshop on Document Analysis Systems, pages 445-449, 2012.\n\nInterpreting data from scanned tables. W Farrukh, A Foncubierta-Rodriguez, A Ciubotaru, G Jaume, C Bejas, O Goksel, M Gabrani, 14th IAPR International Conference on Document Analysis and Recognition (ICDAR). 02W. Farrukh, A. Foncubierta-Rodriguez, A. Ciubotaru, G. Jaume, C. Bejas, O. Goksel, and M. Gabrani. Interpreting data from scanned tables. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 02, pages 5-6, 2017.\n\nIcdar 2019 competition on table detection and recognition (ctdar). L Gao, Y Huang, H D\u00e9jean, J Meunier, Q Yan, Y Fang, F Kleber, E Lang, 2019 International Conference on Document Analysis and Recognition (ICDAR). L. Gao, Y. Huang, H. D\u00e9jean, J. Meunier, Q. Yan, Y. Fang, F. Kleber, and E. Lang. Icdar 2019 competition on table detection and recognition (ctdar). In 2019 International Con- ference on Document Analysis and Recognition (ICDAR), pages 1510-1515, 2019.\n\nTable detection using deep learning. A Gilani, S R Qasim, I Malik, F Shafait, 14th IAPR International Conference on Document Analysis and Recognition (ICDAR). 01A. Gilani, S. R. Qasim, I. Malik, and F. Shafait. Table detec- tion using deep learning. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 01, pages 771-776, 2017.\n\nA methodology for evaluating algorithms for table understanding in pdf documents. Max G\u00f6bel, Tamir Hassan, Ermelinda Oro, Giorgio Orsi, Proceedings of the 2012 ACM Symposium on Document Engineering, DocEng '12. the 2012 ACM Symposium on Document Engineering, DocEng '12New York, NY, USAAssociation for Computing MachineryMax G\u00f6bel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. A methodology for evaluating algorithms for table under- standing in pdf documents. In Proceedings of the 2012 ACM Symposium on Document Engineering, DocEng '12, page 45-48, New York, NY, USA, 2012. Association for Comput- ing Machinery.\n\nIcdar 2013 table competition. M , T Hassan, E Oro, G Orsi, 12th International Conference on Document Analysis and Recognition. M. G\u00f6 bel, T. Hassan, E. Oro, and G. Orsi. Icdar 2013 ta- ble competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1449-1453, 2013.\n\nMultiscale multi-task fcn for semantic page segmentation and table detection. D He, S Cohen, B Price, D Kifer, C L Giles, 14th IAPR International Conference on Document Analysis and Recognition (ICDAR). 01D. He, S. Cohen, B. Price, D. Kifer, and C. L. Giles. Multi- scale multi-task fcn for semantic page segmentation and table detection. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 01, pages 254-261, 2017.\n\n. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross B Girshick, Mask R-Cnn, Corr, abs/1703.06870Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask R-CNN. CoRR, abs/1703.06870, 2017.\n\nMulti-stage hrnet: Multiple stage high-resolution network for human pose estimation. Junjie Huang, Zheng Zhu, Guan Huang, Junjie Huang, Zheng Zhu, and Guan Huang. Multi-stage hrnet: Multiple stage high-resolution network for human pose estimation, 2019.\n\nLearning to detect tables in scanned document images using line information. T Kasar, P Barlas, S Adam, C Chatelain, T Paquet, 12th International Conference on Document Analysis and Recognition. T. Kasar, P. Barlas, S. Adam, C. Chatelain, and T. Paquet. Learning to detect tables in scanned document images using line information. In 2013 12th International Conference on Document Analysis and Recognition, pages 1185-1189, 2013.\n\nThe t-recs table recognition and analysis system. Thomas Kieninger, Andreas Dengel, 1655Thomas Kieninger and Andreas Dengel. The t-recs table recognition and analysis system. volume 1655, pages 255- 269, 11 1998.\n\nMinghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, Zhoujun Li, Tablebank, arXiv:1903.01949Table benchmark for image-based table detection and recognition. arXiv preprintMinghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. Tablebank: Table benchmark for image-based table detection and recognition. arXiv preprint arXiv:1903.01949, 2019.\n\nTablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images. Shubham Paliwal, Rohit Vishwanath, Monika Rahul, Lovekesh Sharma, Vig, International Conference on Document Analysis and Recognition (ICDAR). Shubham Paliwal, D Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig. Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images. 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 128- 133, 2019.\n\nTinti: A system for retrieval in text tables title2. P Pyreddy, W B Croft, USATechnical reportP. Pyreddy and W. B. Croft. Tinti: A system for retrieval in text tables title2:. Technical report, USA, 1997.\n\nTable localization and segmentation using gan and cnn. M M Reza, S S Bukhari, M Jenckel, A Dengel, 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW). 5M. M. Reza, S. S. Bukhari, M. Jenckel, and A. Dengel. Table localization and segmentation using gan and cnn. In 2019 International Conference on Document Analysis and Recogni- tion Workshops (ICDARW), volume 5, pages 152-157, 2019.\n\nDeepdesrt: Deep learning for detection and structure recognition of tables in document images. S Schreiber, S Agne, I Wolf, A Dengel, S Ahmed, 14th IAPR International Conference on Document Analysis and Recognition (ICDAR). 01S. Schreiber, S. Agne, I. Wolf, A. Dengel, and S. Ahmed. Deepdesrt: Deep learning for detection and structure recog- nition of tables in document images. In 2017 14th IAPR International Conference on Document Analysis and Recog- nition (ICDAR), volume 01, pages 1162-1167, 2017.\n\nJunction-based table detection in camera-captured document images. Wonkyo Seo, Hyung Koo, Nam Cho, International Journal on Document Analysis and Recognition (IJDAR). 18Wonkyo Seo, Hyung Koo, and Nam Cho. Junction-based table detection in camera-captured document images. In- ternational Journal on Document Analysis and Recognition (IJDAR), 18, 03 2014.\n\nMetrics for evaluating performance in document analysis: application to tables. A C Silva, IJDAR 14. A.C.e Silva. Metrics for evaluating performance in document analysis: application to tables. In IJDAR 14, page 101-109, 2011.\n\nDeep highresolution representation learning for visual recogni-tion. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, D Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao, IEEE transactions. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, D. Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high- resolution representation learning for visual recogni-tion. IEEE transactions on pattern analysis and machine intelligence, 2019.\n", "annotations": {"author": "[{\"end\":203,\"start\":114},{\"end\":283,\"start\":204},{\"end\":371,\"start\":284},{\"end\":457,\"start\":372},{\"end\":543,\"start\":458}]", "publisher": null, "author_last_name": "[{\"end\":130,\"start\":124},{\"end\":215,\"start\":209},{\"end\":299,\"start\":292},{\"end\":385,\"start\":379},{\"end\":475,\"start\":465}]", "author_first_name": "[{\"end\":123,\"start\":114},{\"end\":208,\"start\":204},{\"end\":291,\"start\":284},{\"end\":378,\"start\":372},{\"end\":464,\"start\":458}]", "author_affiliation": "[{\"end\":202,\"start\":159},{\"end\":282,\"start\":239},{\"end\":370,\"start\":327},{\"end\":456,\"start\":413},{\"end\":542,\"start\":499}]", "title": "[{\"end\":111,\"start\":1},{\"end\":654,\"start\":544}]", "venue": null, "abstract": "[{\"end\":2096,\"start\":1256}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5145,\"start\":5144},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5624,\"start\":5620},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5786,\"start\":5782},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5914,\"start\":5910},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6099,\"start\":6096},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6418,\"start\":6415},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6533,\"start\":6529},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6707,\"start\":6703},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6955,\"start\":6952},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7067,\"start\":7064},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7259,\"start\":7255},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7556,\"start\":7552},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7741,\"start\":7737},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9298,\"start\":9295},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9452,\"start\":9448},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9746,\"start\":9742},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9816,\"start\":9812},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9912,\"start\":9908},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10149,\"start\":10145},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10297,\"start\":10294},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10338,\"start\":10335},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10483,\"start\":10479},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10604,\"start\":10601},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11442,\"start\":11439},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11717,\"start\":11714},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18723,\"start\":18720},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20635,\"start\":20632},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20647,\"start\":20644},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20839,\"start\":20836},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21244,\"start\":21241},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21373,\"start\":21369},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23060,\"start\":23056},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23770,\"start\":23766},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24516,\"start\":24512},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27082,\"start\":27079},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28061,\"start\":28058},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29619,\"start\":29616},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30029,\"start\":30026},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30584,\"start\":30580},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30626,\"start\":30622},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30632,\"start\":30628},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30641,\"start\":30637},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30754,\"start\":30750},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30763,\"start\":30759},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31482,\"start\":31478},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35383,\"start\":35379},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35999,\"start\":35996}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33993,\"start\":33844},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34034,\"start\":33994},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34091,\"start\":34035},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34126,\"start\":34092},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34162,\"start\":34127},{\"attributes\":{\"id\":\"fig_7\"},\"end\":34207,\"start\":34163},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34215,\"start\":34208},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34655,\"start\":34216},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34943,\"start\":34656},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35324,\"start\":34944},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35733,\"start\":35325},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":35915,\"start\":35734},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":36130,\"start\":35916}]", "paragraph": "[{\"end\":2748,\"start\":2112},{\"end\":3690,\"start\":2750},{\"end\":4595,\"start\":3692},{\"end\":4664,\"start\":4597},{\"end\":4833,\"start\":4666},{\"end\":4984,\"start\":4835},{\"end\":5326,\"start\":4986},{\"end\":5566,\"start\":5333},{\"end\":6020,\"start\":5583},{\"end\":6671,\"start\":6022},{\"end\":8211,\"start\":6673},{\"end\":8405,\"start\":8253},{\"end\":8592,\"start\":8407},{\"end\":8880,\"start\":8594},{\"end\":9005,\"start\":8882},{\"end\":9108,\"start\":9007},{\"end\":10240,\"start\":9131},{\"end\":11369,\"start\":10242},{\"end\":11837,\"start\":11371},{\"end\":12263,\"start\":11869},{\"end\":13580,\"start\":12265},{\"end\":14874,\"start\":13582},{\"end\":15829,\"start\":14876},{\"end\":16379,\"start\":15831},{\"end\":17122,\"start\":16426},{\"end\":17793,\"start\":17124},{\"end\":18249,\"start\":17816},{\"end\":18915,\"start\":18270},{\"end\":20035,\"start\":18928},{\"end\":20508,\"start\":20037},{\"end\":21849,\"start\":20532},{\"end\":22539,\"start\":21851},{\"end\":22748,\"start\":22541},{\"end\":22994,\"start\":22750},{\"end\":24463,\"start\":22996},{\"end\":24849,\"start\":24465},{\"end\":25472,\"start\":24851},{\"end\":25964,\"start\":25497},{\"end\":26555,\"start\":25989},{\"end\":27330,\"start\":26557},{\"end\":28721,\"start\":27332},{\"end\":28943,\"start\":28752},{\"end\":30867,\"start\":28945},{\"end\":33008,\"start\":30910},{\"end\":33843,\"start\":33023}]", "formula": null, "table_ref": "[{\"end\":2460,\"start\":2353},{\"end\":3277,\"start\":2971},{\"end\":6758,\"start\":6743},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":7492,\"start\":7278},{\"end\":7703,\"start\":7557},{\"end\":27091,\"start\":27084},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28213,\"start\":28206},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29286,\"start\":29278},{\"end\":29615,\"start\":29516},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30390,\"start\":30383},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30862,\"start\":30855},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31687,\"start\":31680}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2110,\"start\":2098},{\"end\":5331,\"start\":5329},{\"attributes\":{\"n\":\"2.\"},\"end\":5581,\"start\":5569},{\"attributes\":{\"n\":\"3.\"},\"end\":8251,\"start\":8214},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9129,\"start\":9111},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11867,\"start\":11840},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16424,\"start\":16382},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":17814,\"start\":17796},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":18268,\"start\":18252},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18926,\"start\":18918},{\"attributes\":{\"n\":\"4.\"},\"end\":20530,\"start\":20511},{\"attributes\":{\"n\":\"5.\"},\"end\":25495,\"start\":25475},{\"attributes\":{\"n\":\"5.1.\"},\"end\":25987,\"start\":25967},{\"attributes\":{\"n\":\"5.2.\"},\"end\":28750,\"start\":28724},{\"attributes\":{\"n\":\"5.3.\"},\"end\":30908,\"start\":30870},{\"attributes\":{\"n\":\"6.\"},\"end\":33021,\"start\":33011},{\"end\":33855,\"start\":33845},{\"end\":34005,\"start\":33995},{\"end\":34103,\"start\":34093},{\"end\":34138,\"start\":34128},{\"end\":34174,\"start\":34164},{\"end\":34214,\"start\":34209},{\"end\":34226,\"start\":34217},{\"end\":34954,\"start\":34945},{\"end\":35335,\"start\":35326},{\"end\":35744,\"start\":35735},{\"end\":35926,\"start\":35917}]", "table": "[{\"end\":34655,\"start\":34275},{\"end\":35324,\"start\":35166},{\"end\":35733,\"start\":35384},{\"end\":35915,\"start\":35781},{\"end\":36130,\"start\":36000}]", "figure_caption": "[{\"end\":33993,\"start\":33857},{\"end\":34034,\"start\":34007},{\"end\":34091,\"start\":34037},{\"end\":34126,\"start\":34105},{\"end\":34162,\"start\":34140},{\"end\":34207,\"start\":34176},{\"end\":34275,\"start\":34228},{\"end\":34943,\"start\":34658},{\"end\":35166,\"start\":34956},{\"end\":35384,\"start\":35337},{\"end\":35781,\"start\":35746},{\"end\":36000,\"start\":35928}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16182,\"start\":16174},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18009,\"start\":18001},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18532,\"start\":18524},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19058,\"start\":19050},{\"end\":27188,\"start\":27172},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32597,\"start\":32589}]", "bib_author_first_name": "[{\"end\":36210,\"start\":36209},{\"end\":36218,\"start\":36217},{\"end\":36545,\"start\":36538},{\"end\":36555,\"start\":36551},{\"end\":36909,\"start\":36902},{\"end\":36919,\"start\":36915},{\"end\":37128,\"start\":37125},{\"end\":37140,\"start\":37135},{\"end\":37156,\"start\":37147},{\"end\":37169,\"start\":37163},{\"end\":37177,\"start\":37175},{\"end\":37193,\"start\":37185},{\"end\":37205,\"start\":37198},{\"end\":37217,\"start\":37211},{\"end\":37229,\"start\":37224},{\"end\":37241,\"start\":37235},{\"end\":37251,\"start\":37246},{\"end\":37264,\"start\":37259},{\"end\":37280,\"start\":37272},{\"end\":37294,\"start\":37286},{\"end\":37307,\"start\":37302},{\"end\":37318,\"start\":37314},{\"end\":37326,\"start\":37323},{\"end\":37334,\"start\":37331},{\"end\":37343,\"start\":37340},{\"end\":37858,\"start\":37853},{\"end\":37869,\"start\":37864},{\"end\":37884,\"start\":37877},{\"end\":37895,\"start\":37889},{\"end\":37907,\"start\":37900},{\"end\":37922,\"start\":37913},{\"end\":38220,\"start\":38219},{\"end\":38228,\"start\":38227},{\"end\":38235,\"start\":38234},{\"end\":38243,\"start\":38242},{\"end\":38250,\"start\":38249},{\"end\":38577,\"start\":38576},{\"end\":38588,\"start\":38587},{\"end\":38613,\"start\":38612},{\"end\":38626,\"start\":38625},{\"end\":38635,\"start\":38634},{\"end\":38644,\"start\":38643},{\"end\":38654,\"start\":38653},{\"end\":39071,\"start\":39070},{\"end\":39078,\"start\":39077},{\"end\":39087,\"start\":39086},{\"end\":39097,\"start\":39096},{\"end\":39108,\"start\":39107},{\"end\":39115,\"start\":39114},{\"end\":39123,\"start\":39122},{\"end\":39133,\"start\":39132},{\"end\":39508,\"start\":39507},{\"end\":39518,\"start\":39517},{\"end\":39520,\"start\":39519},{\"end\":39529,\"start\":39528},{\"end\":39538,\"start\":39537},{\"end\":39927,\"start\":39924},{\"end\":39940,\"start\":39935},{\"end\":39958,\"start\":39949},{\"end\":39971,\"start\":39964},{\"end\":40491,\"start\":40490},{\"end\":40495,\"start\":40494},{\"end\":40505,\"start\":40504},{\"end\":40512,\"start\":40511},{\"end\":40841,\"start\":40840},{\"end\":40847,\"start\":40846},{\"end\":40856,\"start\":40855},{\"end\":40865,\"start\":40864},{\"end\":40874,\"start\":40873},{\"end\":40876,\"start\":40875},{\"end\":41232,\"start\":41225},{\"end\":41244,\"start\":41237},{\"end\":41260,\"start\":41255},{\"end\":41273,\"start\":41269},{\"end\":41275,\"start\":41274},{\"end\":41516,\"start\":41510},{\"end\":41529,\"start\":41524},{\"end\":41539,\"start\":41535},{\"end\":41758,\"start\":41757},{\"end\":41767,\"start\":41766},{\"end\":41777,\"start\":41776},{\"end\":41785,\"start\":41784},{\"end\":41798,\"start\":41797},{\"end\":42167,\"start\":42161},{\"end\":42186,\"start\":42179},{\"end\":42332,\"start\":42325},{\"end\":42340,\"start\":42337},{\"end\":42353,\"start\":42346},{\"end\":42365,\"start\":42361},{\"end\":42375,\"start\":42371},{\"end\":42389,\"start\":42382},{\"end\":42815,\"start\":42808},{\"end\":42830,\"start\":42825},{\"end\":42849,\"start\":42843},{\"end\":42865,\"start\":42857},{\"end\":43299,\"start\":43298},{\"end\":43310,\"start\":43309},{\"end\":43312,\"start\":43311},{\"end\":43507,\"start\":43506},{\"end\":43509,\"start\":43508},{\"end\":43517,\"start\":43516},{\"end\":43519,\"start\":43518},{\"end\":43530,\"start\":43529},{\"end\":43541,\"start\":43540},{\"end\":43967,\"start\":43966},{\"end\":43980,\"start\":43979},{\"end\":43988,\"start\":43987},{\"end\":43996,\"start\":43995},{\"end\":44006,\"start\":44005},{\"end\":44450,\"start\":44444},{\"end\":44461,\"start\":44456},{\"end\":44470,\"start\":44467},{\"end\":44814,\"start\":44813},{\"end\":44816,\"start\":44815},{\"end\":45038,\"start\":45030},{\"end\":45047,\"start\":45045},{\"end\":45061,\"start\":45053},{\"end\":45074,\"start\":45069},{\"end\":45089,\"start\":45082},{\"end\":45100,\"start\":45096},{\"end\":45108,\"start\":45107},{\"end\":45120,\"start\":45114},{\"end\":45132,\"start\":45125},{\"end\":45146,\"start\":45138},{\"end\":45158,\"start\":45153},{\"end\":45167,\"start\":45164}]", "bib_author_last_name": "[{\"end\":36215,\"start\":36211},{\"end\":36226,\"start\":36219},{\"end\":36549,\"start\":36546},{\"end\":36567,\"start\":36556},{\"end\":36913,\"start\":36910},{\"end\":36931,\"start\":36920},{\"end\":37133,\"start\":37129},{\"end\":37145,\"start\":37141},{\"end\":37161,\"start\":37157},{\"end\":37173,\"start\":37170},{\"end\":37183,\"start\":37178},{\"end\":37196,\"start\":37194},{\"end\":37209,\"start\":37206},{\"end\":37222,\"start\":37218},{\"end\":37233,\"start\":37230},{\"end\":37244,\"start\":37242},{\"end\":37257,\"start\":37252},{\"end\":37270,\"start\":37265},{\"end\":37284,\"start\":37281},{\"end\":37300,\"start\":37295},{\"end\":37312,\"start\":37308},{\"end\":37321,\"start\":37319},{\"end\":37329,\"start\":37327},{\"end\":37338,\"start\":37335},{\"end\":37346,\"start\":37344},{\"end\":37862,\"start\":37859},{\"end\":37875,\"start\":37870},{\"end\":37887,\"start\":37885},{\"end\":37898,\"start\":37896},{\"end\":37911,\"start\":37908},{\"end\":37926,\"start\":37923},{\"end\":38225,\"start\":38221},{\"end\":38232,\"start\":38229},{\"end\":38240,\"start\":38236},{\"end\":38247,\"start\":38244},{\"end\":38254,\"start\":38251},{\"end\":38585,\"start\":38578},{\"end\":38610,\"start\":38589},{\"end\":38623,\"start\":38614},{\"end\":38632,\"start\":38627},{\"end\":38641,\"start\":38636},{\"end\":38651,\"start\":38645},{\"end\":38662,\"start\":38655},{\"end\":39075,\"start\":39072},{\"end\":39084,\"start\":39079},{\"end\":39094,\"start\":39088},{\"end\":39105,\"start\":39098},{\"end\":39112,\"start\":39109},{\"end\":39120,\"start\":39116},{\"end\":39130,\"start\":39124},{\"end\":39138,\"start\":39134},{\"end\":39515,\"start\":39509},{\"end\":39526,\"start\":39521},{\"end\":39535,\"start\":39530},{\"end\":39546,\"start\":39539},{\"end\":39933,\"start\":39928},{\"end\":39947,\"start\":39941},{\"end\":39962,\"start\":39959},{\"end\":39976,\"start\":39972},{\"end\":40502,\"start\":40496},{\"end\":40509,\"start\":40506},{\"end\":40517,\"start\":40513},{\"end\":40844,\"start\":40842},{\"end\":40853,\"start\":40848},{\"end\":40862,\"start\":40857},{\"end\":40871,\"start\":40866},{\"end\":40882,\"start\":40877},{\"end\":41235,\"start\":41233},{\"end\":41253,\"start\":41245},{\"end\":41267,\"start\":41261},{\"end\":41284,\"start\":41276},{\"end\":41296,\"start\":41286},{\"end\":41302,\"start\":41298},{\"end\":41522,\"start\":41517},{\"end\":41533,\"start\":41530},{\"end\":41545,\"start\":41540},{\"end\":41764,\"start\":41759},{\"end\":41774,\"start\":41768},{\"end\":41782,\"start\":41778},{\"end\":41795,\"start\":41786},{\"end\":41805,\"start\":41799},{\"end\":42177,\"start\":42168},{\"end\":42193,\"start\":42187},{\"end\":42335,\"start\":42333},{\"end\":42344,\"start\":42341},{\"end\":42359,\"start\":42354},{\"end\":42369,\"start\":42366},{\"end\":42380,\"start\":42376},{\"end\":42392,\"start\":42390},{\"end\":42403,\"start\":42394},{\"end\":42823,\"start\":42816},{\"end\":42841,\"start\":42831},{\"end\":42855,\"start\":42850},{\"end\":42872,\"start\":42866},{\"end\":42877,\"start\":42874},{\"end\":43307,\"start\":43300},{\"end\":43318,\"start\":43313},{\"end\":43514,\"start\":43510},{\"end\":43527,\"start\":43520},{\"end\":43538,\"start\":43531},{\"end\":43548,\"start\":43542},{\"end\":43977,\"start\":43968},{\"end\":43985,\"start\":43981},{\"end\":43993,\"start\":43989},{\"end\":44003,\"start\":43997},{\"end\":44012,\"start\":44007},{\"end\":44454,\"start\":44451},{\"end\":44465,\"start\":44462},{\"end\":44474,\"start\":44471},{\"end\":44822,\"start\":44817},{\"end\":45043,\"start\":45039},{\"end\":45051,\"start\":45048},{\"end\":45067,\"start\":45062},{\"end\":45080,\"start\":45075},{\"end\":45094,\"start\":45090},{\"end\":45105,\"start\":45101},{\"end\":45112,\"start\":45109},{\"end\":45123,\"start\":45121},{\"end\":45136,\"start\":45133},{\"end\":45151,\"start\":45147},{\"end\":45162,\"start\":45159},{\"end\":45172,\"start\":45168}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":58676064},\"end\":36477,\"start\":36132},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206596979},\"end\":36817,\"start\":36479},{\"attributes\":{\"id\":\"b2\"},\"end\":37066,\"start\":36819},{\"attributes\":{\"doi\":\"abs/1906.07155\",\"id\":\"b3\"},\"end\":37849,\"start\":37068},{\"attributes\":{\"id\":\"b4\"},\"end\":38140,\"start\":37851},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":23786594},\"end\":38535,\"start\":38142},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4773150},\"end\":39001,\"start\":38537},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":211026773},\"end\":39468,\"start\":39003},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206777650},\"end\":39840,\"start\":39470},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3223096},\"end\":40458,\"start\":39842},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206777311},\"end\":40760,\"start\":40460},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":29473897},\"end\":41221,\"start\":40762},{\"attributes\":{\"doi\":\"abs/1703.06870\",\"id\":\"b12\"},\"end\":41423,\"start\":41223},{\"attributes\":{\"id\":\"b13\"},\"end\":41678,\"start\":41425},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":18367100},\"end\":42109,\"start\":41680},{\"attributes\":{\"id\":\"b15\"},\"end\":42323,\"start\":42111},{\"attributes\":{\"doi\":\"arXiv:1903.01949\",\"id\":\"b16\"},\"end\":42687,\"start\":42325},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":209862087},\"end\":43243,\"start\":42689},{\"attributes\":{\"id\":\"b18\"},\"end\":43449,\"start\":43245},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":207950574},\"end\":43869,\"start\":43451},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10191334},\"end\":44375,\"start\":43871},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11228974},\"end\":44731,\"start\":44377},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16510017},\"end\":44959,\"start\":44733},{\"attributes\":{\"id\":\"b23\"},\"end\":45482,\"start\":44961}]", "bib_title": "[{\"end\":36207,\"start\":36132},{\"end\":36536,\"start\":36479},{\"end\":38217,\"start\":38142},{\"end\":38574,\"start\":38537},{\"end\":39068,\"start\":39003},{\"end\":39505,\"start\":39470},{\"end\":39922,\"start\":39842},{\"end\":40488,\"start\":40460},{\"end\":40838,\"start\":40762},{\"end\":41755,\"start\":41680},{\"end\":42806,\"start\":42689},{\"end\":43504,\"start\":43451},{\"end\":43964,\"start\":43871},{\"end\":44442,\"start\":44377},{\"end\":44811,\"start\":44733},{\"end\":45028,\"start\":44961}]", "bib_author": "[{\"end\":36217,\"start\":36209},{\"end\":36228,\"start\":36217},{\"end\":36551,\"start\":36538},{\"end\":36569,\"start\":36551},{\"end\":36915,\"start\":36902},{\"end\":36933,\"start\":36915},{\"end\":37135,\"start\":37125},{\"end\":37147,\"start\":37135},{\"end\":37163,\"start\":37147},{\"end\":37175,\"start\":37163},{\"end\":37185,\"start\":37175},{\"end\":37198,\"start\":37185},{\"end\":37211,\"start\":37198},{\"end\":37224,\"start\":37211},{\"end\":37235,\"start\":37224},{\"end\":37246,\"start\":37235},{\"end\":37259,\"start\":37246},{\"end\":37272,\"start\":37259},{\"end\":37286,\"start\":37272},{\"end\":37302,\"start\":37286},{\"end\":37314,\"start\":37302},{\"end\":37323,\"start\":37314},{\"end\":37331,\"start\":37323},{\"end\":37340,\"start\":37331},{\"end\":37348,\"start\":37340},{\"end\":37864,\"start\":37853},{\"end\":37877,\"start\":37864},{\"end\":37889,\"start\":37877},{\"end\":37900,\"start\":37889},{\"end\":37913,\"start\":37900},{\"end\":37928,\"start\":37913},{\"end\":38227,\"start\":38219},{\"end\":38234,\"start\":38227},{\"end\":38242,\"start\":38234},{\"end\":38249,\"start\":38242},{\"end\":38256,\"start\":38249},{\"end\":38587,\"start\":38576},{\"end\":38612,\"start\":38587},{\"end\":38625,\"start\":38612},{\"end\":38634,\"start\":38625},{\"end\":38643,\"start\":38634},{\"end\":38653,\"start\":38643},{\"end\":38664,\"start\":38653},{\"end\":39077,\"start\":39070},{\"end\":39086,\"start\":39077},{\"end\":39096,\"start\":39086},{\"end\":39107,\"start\":39096},{\"end\":39114,\"start\":39107},{\"end\":39122,\"start\":39114},{\"end\":39132,\"start\":39122},{\"end\":39140,\"start\":39132},{\"end\":39517,\"start\":39507},{\"end\":39528,\"start\":39517},{\"end\":39537,\"start\":39528},{\"end\":39548,\"start\":39537},{\"end\":39935,\"start\":39924},{\"end\":39949,\"start\":39935},{\"end\":39964,\"start\":39949},{\"end\":39978,\"start\":39964},{\"end\":40494,\"start\":40490},{\"end\":40504,\"start\":40494},{\"end\":40511,\"start\":40504},{\"end\":40519,\"start\":40511},{\"end\":40846,\"start\":40840},{\"end\":40855,\"start\":40846},{\"end\":40864,\"start\":40855},{\"end\":40873,\"start\":40864},{\"end\":40884,\"start\":40873},{\"end\":41237,\"start\":41225},{\"end\":41255,\"start\":41237},{\"end\":41269,\"start\":41255},{\"end\":41286,\"start\":41269},{\"end\":41298,\"start\":41286},{\"end\":41304,\"start\":41298},{\"end\":41524,\"start\":41510},{\"end\":41535,\"start\":41524},{\"end\":41547,\"start\":41535},{\"end\":41766,\"start\":41757},{\"end\":41776,\"start\":41766},{\"end\":41784,\"start\":41776},{\"end\":41797,\"start\":41784},{\"end\":41807,\"start\":41797},{\"end\":42179,\"start\":42161},{\"end\":42195,\"start\":42179},{\"end\":42337,\"start\":42325},{\"end\":42346,\"start\":42337},{\"end\":42361,\"start\":42346},{\"end\":42371,\"start\":42361},{\"end\":42382,\"start\":42371},{\"end\":42394,\"start\":42382},{\"end\":42405,\"start\":42394},{\"end\":42825,\"start\":42808},{\"end\":42843,\"start\":42825},{\"end\":42857,\"start\":42843},{\"end\":42874,\"start\":42857},{\"end\":42879,\"start\":42874},{\"end\":43309,\"start\":43298},{\"end\":43320,\"start\":43309},{\"end\":43516,\"start\":43506},{\"end\":43529,\"start\":43516},{\"end\":43540,\"start\":43529},{\"end\":43550,\"start\":43540},{\"end\":43979,\"start\":43966},{\"end\":43987,\"start\":43979},{\"end\":43995,\"start\":43987},{\"end\":44005,\"start\":43995},{\"end\":44014,\"start\":44005},{\"end\":44456,\"start\":44444},{\"end\":44467,\"start\":44456},{\"end\":44476,\"start\":44467},{\"end\":44824,\"start\":44813},{\"end\":45045,\"start\":45030},{\"end\":45053,\"start\":45045},{\"end\":45069,\"start\":45053},{\"end\":45082,\"start\":45069},{\"end\":45096,\"start\":45082},{\"end\":45107,\"start\":45096},{\"end\":45114,\"start\":45107},{\"end\":45125,\"start\":45114},{\"end\":45138,\"start\":45125},{\"end\":45153,\"start\":45138},{\"end\":45164,\"start\":45153},{\"end\":45174,\"start\":45164}]", "bib_venue": "[{\"end\":36288,\"start\":36228},{\"end\":36638,\"start\":36569},{\"end\":36900,\"start\":36819},{\"end\":37123,\"start\":37068},{\"end\":38317,\"start\":38256},{\"end\":38743,\"start\":38664},{\"end\":39214,\"start\":39140},{\"end\":39627,\"start\":39548},{\"end\":40051,\"start\":39978},{\"end\":40585,\"start\":40519},{\"end\":40963,\"start\":40884},{\"end\":41508,\"start\":41425},{\"end\":41873,\"start\":41807},{\"end\":42159,\"start\":42111},{\"end\":42484,\"start\":42421},{\"end\":42948,\"start\":42879},{\"end\":43296,\"start\":43245},{\"end\":43635,\"start\":43550},{\"end\":44093,\"start\":44014},{\"end\":44542,\"start\":44476},{\"end\":44832,\"start\":44824},{\"end\":45191,\"start\":45174},{\"end\":40128,\"start\":40053}]"}}}, "year": 2023, "month": 12, "day": 17}