{"id": 201106116, "updated": "2023-09-27 22:34:52.918", "metadata": {"title": "All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously", "authors": "[{\"first\":\"Aaron\",\"last\":\"Fisher\",\"middle\":[]},{\"first\":\"Cynthia\",\"last\":\"Rudin\",\"middle\":[]},{\"first\":\"Francesca\",\"last\":\"Dominici\",\"middle\":[]}]", "venue": "Journal of Machine Learning Research 20 (177), 1-81, 2019", "journal": null, "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model $f(\\mathbf{x})=\\mathbf{x}^{T}\\beta$ with a fixed coefficient vector $\\beta$) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.", "fields_of_study": "[\"Mathematics\"]", "external_ids": {"arxiv": "1801.01489", "mag": "2994706149", "acl": null, "pubmed": "34335110", "pubmedcentral": null, "dblp": "journals/jmlr/FisherRD19", "doi": null}}, "content": {"source": {"pdf_hash": "8ab8836f343f7234c56dac732b48c7760f4cf168", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/1801.01489v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9640ac3618165ee40ed7315aa183720839b6c9b5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8ab8836f343f7234c56dac732b48c7760f4cf168.txt", "contents": "\nAll Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously\n\n\nAaron Fisher afishe27@alumni.jh.edu \nCynthia Rudin cynthia@cs.duke.edu \nFrancesca Dominici fdominic@hsph.harvard.edu \n\nDepartments of Computer Science and Electrical and Computer Engineering\nTakeda Pharmaceuticals Cambridge\n02139MAUSA\n\n\nDepartment of Biostatistics Harvard T.H. Chan School of Public Health Boston\nDuke University Durham\n27708, 02115NC, MAUSA, USA\n\nAll Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously\n(Authors are listed in order of contribution, with highest contribution listed first.)Model Class Reliance Published copy at:Rashomonpermutation importanceconditional variable importanceU- statisticstransparencyinterpretable models\nVariable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model f (x) = x T \u03b2 with a fixed coefficient vector \u03b2) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.\n\nIntroduction\n\nVariable importance (VI) tools describe how much a prediction model's accuracy depends on the information in each covariate. For example, in Random Forests, VI is measured by the decrease in prediction accuracy when a covariate is permuted ; see also Strobl et al., 2008;Altmann et al., 2010;Zhu et al., 2015;Gregorutti et al., 2015;Datta et al., 2016;Gregorutti et al., 2017). A similar \"Perturb\" VI measure has been used for neural networks, where noise is added to covariates (Recknagel et al., 1997;Yao et al., 1998;Scardi and Harding, 1999;Gevrey et al., 2003). Such tools can be useful for identifying covariates that must be measured with high precision, for improving the transparency of a \"black box\" prediction model (see also Rudin, 2019), or for determining what scenarios may cause the model to fail.\n\nHowever, existing VI measures do not generally account for the fact that many prediction models may fit the data almost equally well. In such cases, the model used by one analyst may rely on entirely different covariate information than the model used by another analyst. This common scenario has been called the \"Rashomon\" effect of statistics ; see also Lecu\u00e9, 2011;Statnikov et al., 2013;Tulabandhula and Rudin, 2014;Nevo and Ritov, 2017;Letham et al., 2016). The term is inspired by the 1950 Kurosawa film of the same name, in which four witnesses offer different descriptions and explanations for the same encounter. Under the Rashomon effect, how should analysts give comprehensive descriptions of the importance of each covariate? How well can one analyst recover the conclusions of another? Will the model that gives the best predictions necessarily give the most accurate interpretation?\n\nTo address these concerns, we analyze the set of prediction models that provide nearoptimal accuracy, which we refer to as a Rashomon set. This approach stands in contrast to training to select a single prediction model, among a prespecified class of candidate models. Our motivation is that Rashomon sets (defined formally below) summarize the range of effective prediction strategies that an analyst might choose. Additionally, even if the candidate models do not contain the true data generating process, we may hope that some of these models function in similar ways to the data generating process. In particular, we may hope there exist well performing candidate models that place the same importance on a variable of interest as the underlying data generating process does. If so, then studying sets of well-performing models will allow us to deduce information about the data generating process.\n\nApplying this approach to study variable importance, we define model class reliance (MCR) as the highest and lowest degree to which any well-performing model within a given class may rely on a variable of interest for prediction accuracy. Roughly speaking, MCR captures the range of explanations, or mechanisms, associated with well-performing models. Because the resulting range summarizes many prediction models simultaneously, rather a single model, we expect this range to be less affected by the choices that an individual analyst makes during the model-fitting process. Instead of reflecting these choices, MCR aims to reflect the nature of the prediction problem itself.\n\nWe make several, specific technical contributions in deriving MCR. First, we review a core measure of how much an individual prediction model relies on covariates of interest for its accuracy, which we call model reliance (MR). This measure is based on permutation importance measures for Random Forests , and can be expanded to describe conditional importance (see Section 8, as well as Strobl et al. 2008). We draw a connection between permutation-based importance estimates (MR) and U-statistics, which facilitates later theoretical results. Additionally, we derive connections between MR, conditional causal effects, and coefficients for additive models. Expanding on MR, we propose MCR, which generalizes the definition of MR for a class of models. We derive finite-sample bounds for MCR, which motivate an intuitive estimator of MCR. Finally, we propose computational procedures for this estimator.\n\nThe tools we develop to study Rashomon sets are quite general, and can be used to make finite-sample inferences for arbitrary characteristics of well-performing models. For example, beyond describing variable importance, these tools can describe the range of risk predictions that well-fitting models assign to a particular covariate profile, or the variance of predictions made by well-fitting models. In some cases, these novel techniques may provide finite-sample confidence intervals (CIs) where none have previously existed (see Section 5).\n\nMCR and the Rashomon effect become especially relevant in the context of criminal recidivism prediction. Proprietary recidivism risk models trained from criminal records data are increasingly being used in U.S. courtrooms. One concern is that these models may be relying on information that would otherwise be considered unacceptable (for example, race, sex, or proxies for these variables), in order to estimate recidivism risk. The relevant models are often proprietary, and cannot be studied directly. Still, in cases where the predictions made by these models are publicly available, it may be possible to identify alternative prediction models that are sufficiently similar to the proprietary model of interest.\n\nIn this paper, we specifically consider the proprietary model COMPAS (Correctional Offender Management Profiling for Alternative Sanctions), developed by the company Northpointe Inc. (subsequently, in 2017, Northpointe Inc.,Courtview Justice Solutions Inc., and Constellation Justice Systems Inc. joined together under the name Equivant). Our goal is to estimate how much COMPAS relies on either race, sex, or proxies for these variables not measured in our data set. To this end, we apply a broad class of flexible, kernel-based prediction models to predict COMPAS score. In this setting, the MCR interval reflects the highest and lowest degree to which any prediction model in our class can rely on race and sex while still predicting COMPAS score relatively accurately. Equipped with MCR, we can relax the common assumption of being able to correctly specify the unknown model of interest (here, COMPAS) up to a parametric form. Instead, rather than assuming that the COMPAS model itself is contained in our class, we assume that our class contains at least one well-performing alternative model that relies on sensitive covariates to the same degree that COMPAS does. Under this assumption, the MCR interval will contain the VI value for COMPAS. Applying our approach, we find that race, sex, and their potential proxy variables, are likely not the dominant predictive factors in the COMPAS score (see analysis and discussion in Section 10).\n\nThe remainder of this paper is organized as follows. In Section 2 we introduce notation, and give a high level summary of our approach, illustrated with visualizations. In Sections 3 and 4 we formally present MR and MCR respectively, and derive theoretical properties of each. We also review related variable importance practices in the literature, such as retraining a model after removing one of the covariates. In Section 5, we discuss general applicability of our approach for determining finite-sample CIs for other problems. In Section 6, we present a general procedure for computing MCR. In Section 7, we give specific implementations of this procedure for (regularized) linear models, and linear models in a reproducing kernel Hilbert space. We also show that, for additive models, MR can be expressed in terms of the model's coefficients. In Section 8 we outline connections between MR, causal inference, and conditional variable importance. In Section 9, we illustrate MR and MCR with a simulated toy example, to aid intuition. We also present simulation studies for the task of estimating MR for an unknown, underlying conditional expectation function, under misspecification. We analyze a well-known public data set on recidivism in Section 10, described above. All proofs are presented in the appendices.\n\n\nNotation & Technical Summary\n\nThe label of \"variable importance\" measure has been broadly used to describe approaches for either inference (van der Laan, 2006;D\u00edaz et al., 2015;Williamson et al., 2017) or prediction. While these two goals are highly related, we primarily focus on how much prediction models rely on covariates to achieve accuracy. We use terms such as \"model reliance\" rather than \"importance\" to clarify this context.\n\nIn order to evaluate how much prediction models rely on variables, we now introduce notation for random variables, data, classes of prediction models, and loss functions for evaluating predictions. Let Z = (Y, X 1 , X 2 ) \u2208 Z be a random variable with outcome Y \u2208 Y and covariates X = (X 1 , X 2 ) \u2208 X , where the covariate subsets X 1 \u2208 X 1 and X 2 \u2208 X 2 may each be multivariate. We assume that observations of Z are iid, that n \u2265 2, and that solutions to arg min and arg max operations exist whenever optimizing over sets mentioned in this paper (for example, in Theorem 4, below). Our goal is to study how much different prediction models rely on X 1 to predict Y .\n\nWe refer to our data set as Z = y X , a matrix composed of a n-length outcome vector y in the first column, and a n \u00d7 p covariate matrix X = X 1 X 2 in the remaining columns. In general, for a given vector v, let v [j] denote its j th element(s). For a given matrix A, let A , A [i,\u00b7] , A [\u00b7,j] , and A [i,j] respectively denote the transpose of A, the i th row(s) of A, the j th column(s) of A, and the element(s) in the i th row(s) and j th column(s) of A.\n\nWe use the term model class to refer to a prespecified subset F \u2282 {f | f : X \u2192 Y} of the measurable functions from X to Y. We refer to member functions f \u2208 F as prediction models, or simply as models. Given a model f , we evaluate its performance using a nonnegative loss function L : (F \u00d7 Z) \u2192 R \u22650 . For example, L may be the squared error loss L se (f, (y, x 1 , x 2 )) = (y \u2212 f (x 1 , x 2 )) 2 for regression, or the hinge loss L h (f, (y, x 1 , x 2 )) = (1 \u2212 yf (x 1 , x 2 )) + for classification. We use the term algorithm to refer to any procedure A : Z n \u2192 F that takes a data set as input and returns a model f \u2208 F as output.\n\n\nSummary of Rashomon Sets & Model Class Reliance\n\nMany traditional statistical estimates come from descriptions of a single, fitted prediction model. In contrast, in this section, we summarize our approach for studying a set of near-optimal models. To define this set, we require a prespecified \"reference\" model, denoted by f ref , to serve as a benchmark for predictive performance. For example, f ref may come from a flowchart used to predict injury severity in a hospital's emergency room, or from another quantitative decision rule that is currently implemented in practice. Given a reference model f ref , we define a population -Rashomon set as the subset of models with expected loss no more than above that of f ref . We denote this set as R( ) := {f \u2208 F : EL(f, Z) \u2264 EL(f ref , Z) + }, where E denotes expectations with respect to the population distribution. This set can be thought of as representing models that might be arrived at due to differences in data measurement, processing, filtering, model parameterization, covariate selection, or other analysis choices (see Section 4). ; the x-axis shows the empirical model reliance of each model f \u2208 F on X 1 (see Section 3); and the highlighted portion of the x-axis shows empirical MCR (see Section 4).\n\nFigure 1-A illustrates a hypothetical example of a population -Rashomon set. Here, the y-axis shows the expected loss of each model f \u2208 F, and the x-axis shows how much each model relies on X 1 for its predictive accuracy. More specifically, given a prediction model f , the x-axis shows the percent increase in f 's expected loss when noise is added to X 1 . We refer to this measure as the model reliance (MR) of f on X 1 , written informally as M R(f ) :=\n\nExpected loss of f under noise Expected loss of f without noise .\n\n(2.1)\n\nThe added noise must satisfy certain properties, namely, it must render X 1 completely uninformative of the outcome Y , without altering the marginal distribution of X 1 (for details, see Section 3, as well as . Our central goal is to understand how much, or how little, models may rely on covariates of interest (X 1 ) while still predicting well. In Figure 1 (2.2)\n\nTo estimate this range, we use empirical analogues of the population -Rashomon set, and of MR, based on observed data (Figure 1-B). We define an empirical -Rashomon set as the set of models with in-sample loss no more than above that of f ref , and denote this set byR( ). Informally, we define the empirical MR of a model f on that is, the extent to which f appears to rely on X 1 in a given sample (see Section 3 for details). Finally, we define the empirical model class reliance as the range of empirical MR values corresponding to models with strong in-sample performance (see Section 4), formally written as\n[ M CR \u2212 ( ), M CR + ( )] := min f \u2208R( ) M R(f ), max f \u2208R( ) M R(f ) . (2.4)\nIn Figure 1-B, the above range is shown by the highlighted portion of the x-axis. We make several technical contributions in the process of developing MCR.\n\n\u2022 Estimation of MR, and population-level MCR: Given f , we show desirable properties of M R(f ) as an estimator of M R(f ), using results for U-statistics (Section 3.1 and Theorem 5). We also derive finite sample bounds for population-level MCR, some of which require a limit on the complexity of F in the form of a covering number. These bounds demonstrate that, under fairly weak conditions, empirical MCR provides a sensible estimate of population-level MCR (see Section 4 for details).\n\n\u2022 Computation of empirical MCR: Although empirical MCR is fully determined given a sample, the minimization and maximization in Eq 2.4 require nontrivial computations. To address this, we outline a general optimization procedure for MCR (Section 6). We give detailed implementations of this procedure for cases when the model class F is a set of (regularized) linear regression models, or a set of regression models in a reproducing kernel Hilbert space (Section 7). The output of our proposed procedure is a closed-form, convex envelope containing F, which can be used to approximate empirical MCR for any performance level (see Figure 2 for an illustration). Still, for complex model classes where standard empirical loss minimization is an open problem (for example, neural networks), computing empirical MCR remains an open problem as well.\n\n\u2022 Interpretation of MR in terms of model coefficients, and causal effects:\n\nWe show that MR for an additive model can be written as a function of the model's coefficients (Proposition 15), and that MR for a binary covariate X 1 can be written as a function of the conditional causal effects of X 1 on Y (Proposition 19).\n\n\u2022 Extensions to conditional importance: We provide an extension of MR that is analogous to the notion of conditional importance (Strobl et al., 2008). This extension describes how much a model relies on the specific information in X 1 that cannot otherwise be gleaned from X 2 (Section 8.2).\n\n\u2022 Generalizations for Rashomon sets: Beyond notions of variable importance, we also generalize our finite sample results for MCR to describe arbitrary characterizations of models in a population -Rashomon set. As we discuss in concurrent work (Coker et al., 2018), this generalization is analogous to the profile likelihood interval, and can, for example, be used to bound the range of risk predictions that wellperforming prediction models may assign to a particular set of covariates (Section 5).\n\nWe begin in the next section by formally reviewing model reliance.\n\nRely less on X 1\nRely more on X 1 EL(f, Z) d MR(f ) More accurate Less accurate Model class F \\ MCR (\u270f) \\ MCR + (\u270f)\n\nConservative Computation of Empirical Model Class Reliance\n\nEnvelope containing F Figure 2: Illustration of output from our empirical MCR computational procedure -Our computation procedure produces a closed-form, convex envelope that contains F (shown above as the solid, purple line), which bounds empirical MCR for any value of (see Eq 2.4). The procedure works sequentially, tightening these bounds as much as possible near the value of interest (Section 6). The results from our data analysis ( Figure 8) are presented in the same format as the above purple envelope.\n\n\nModel Reliance\n\nTo formally describe how much the expected accuracy of a fixed prediction model f relies on the random variable X 1 , we use the notion of a \"switched\" loss where X 1 is rendered uninformative. Throughout this section, we will treat f as a pre-specified prediction model of interest (as in Hooker, 2007). Let Z (a) = (Y (a) , X\n\n1 , X\n\n2 ) and\nZ (b) = (Y (b) , X (b) 1 , X (b)\n2 ) be independent random variables, each following the same distribution as Z = (Y, X 1 , X 2 ). We define\ne switch (f ) := EL{f, (Y (b) , X(a)\n1 , X\n\n2 )} as representing the expected loss of model f across pairs of observations (Z (a) , Z (b) ) in which the values of X \nY (b) , X (b) 2 ) from Z (b) , but we have used the variable X (b)\n1 from an independent copy Z (b) . This is why we say that X 2 ) do not relate to each other as they would if they had been chosen together. An alternative interpretation of e switch (f ) is as the expected loss of f when noise is added to X 1 in such a way that X 1 becomes completely uninformative of Y , but that the marginal distribution of X 1 is unchanged.\n\nAs a reference point, we compare e switch (f ) against the standard expected loss when none of the variables are switched, e orig (f ) := EL(f, (Y, X 1 , X 2 )). From these two quantities, we formally define model reliance (MR) as the ratio,\nM R(f ) := e switch (f ) e orig (f ) , (3.1)\nas we alluded to in Eq 2.1. Higher values of M R(f ) signify greater reliance of f on X 1 . For example, an M R(f ) value of 2 means that the model relies heavily on X 1 , in the sense that its loss doubles when X 1 is scrambled. An M R(f ) value of 1 signifies no reliance on X 1 , in the sense that the model's loss does not change when X 1 is scrambled. Models with reliance values strictly less than 1 are more difficult to interpret, as they rely less on the variable of interest than a random guess. Interestingly, it is possible to have models with reliance less than one. For instance, a model f may satisfy M R(f ) < 1 if it treats X 1 and Y as positively correlated when they are in fact negatively correlated. However, in many cases, the existence of a model f \u2208 F satisfying M R(f ) < 1 implies the existence of another, better performing model f \u2208 F satisfying M R(f ) = 1 and e orig (f ) \u2264 e orig (f ). That is, although models may exist with MR values less than 1, they will typically be suboptimal (see Appendix A.2). Model reliance could alternatively be defined as a difference rather than a ratio, that is, as M R difference (f ) := e switch (f ) \u2212 e orig (f ). In Appendix A.5, we discuss how many of our results remain similar under either definition.\n\n\nEstimating Model Reliance with U-statistics, and Connections to\n\nPermutation-based Variable Importance\n\nGiven a model f and data set Z = y X , we estimate M R(f ) by separately estimating the numerator and denominator of Eq 3.1. We estimate e orig (f ) with the standard empirical loss,\u00ea\norig (f ) := 1 n n i=1 L{f, (y [i] , X 1[i,\u00b7] , X 2[i,\u00b7] )}. (3.2)\nWe estimate e switch (f ) by performing a \"switch\" operation across all observed pairs, as in\ne switch (f ) := 1 n(n \u2212 1) n i=1 j =i L{f, (y [j] , X 1[i,\u00b7] , X 2[j,\u00b7] )}. (3.3)\nAbove, we have aggregated over all possible combinations of the observed values for (Y, X 2 ) and for X 1 , excluding pairings that are actually observed in the original sample. If the summation over all possible pairs (Eq 3.3) is computationally prohibitive due to sample size, another estimator of e switch (f ) i\u015d\ne divide (f ) : = 1 2 n/2 n/2 i=1 L{f, (y [i] , X 1[i+ n/2 ,\u00b7] , X 2[i,\u00b7] )} (3.4) +L{f, (y [i+ n/2 ] , X 1[i,\u00b7] , X 2[i+ n/2 ,\u00b7] )} . (3.5)\nHere, rather than summing over all pairs, we divide the sample in half. We then match the first half's values for (Y, X 2 ) with the second half's values for X 1 (Line 3.4), and vice versa (Line 3.5). All three of the above estimators (Eqs 3.2, 3.3 & 3.5) are unbiased for their respective estimands, as we discuss in more detail shortly. Finally, we can estimate M R(f ) with the plug-in estimator\nM R(f ) :=\u00ea switch (f ) e orig (f ) , (3.6)\nwhich we define as the empirical model reliance of f on X 1 . In this way, we formalize the empirical MR definition in Eq 2.3. Again, our definition of empirical MR is very similar to the permutation-based variable importance approach of , where Breiman uses a single random permutation and we consider all possible pairs. To compare these two approaches more precisely, let {\u03c0 1 , . . . , \u03c0 n! } be a set of n-length vectors, each containing a different permutation of the set {1, . . . , n}. The approach of  is analogous to computing the loss \u00b7] )} for a randomly chosen permutation vector \u03c0 l \u2208 {\u03c0 1 , . . . , \u03c0 n! }. Similarly, our calculation in Eq 3.3 is proportional to the sum of losses over all possible (n!) permutations, excluding the n unique combinations of the rows of X 1 and the rows of X 2 y that appear in the original sample (see Appendix A.3). Excluding these observations is necessary to preserve the (finite-sample) unbiasedness of e switch (f ).\nn i=1 L{f, (y [i] , X 1[\u03c0 l[i] ,\u00b7] , X 2[i,\nThe estimators\u00ea orig (f ),\u00ea switch (f ) and\u00ea divide (f ) all belong to the well-studied class of U-statistics. Thus, under fairly minor conditions, these estimators are unbiased, asymptotically normal, and have finite-sample probabilistic bounds (Hoeffding, 1948(Hoeffding, , 1963Serfling, 1980;see also DeLong et al., 1988 for an early use of U-statistics in machine learning, as well as caveats in Demler et al., 2012). To our knowledge, connections between permutationbased importance and U-statistics have not been previously established.\n\nWhile the above results from U-statistics depend on the model f being fixed a priori, we can also leverage these results to create uniform bounds on the MR estimation error for all models in a sufficiently regularized class F. We formally present this bound in Section 4 (Theorem 5), after introducing required conditions on model class complexity. The existence of this uniform bound implies that it is feasible to train a model and to evaluate its importance using the same data. This differs from the classical VI approach of Random Forests , which avoids in-sample importance estimation. There, each tree in the ensemble is fit on a random subset of data, and VI for the tree is estimated using the held-out data. The tree-specific VI estimates are then aggregated to obtain a VI estimate for the overall ensemble. Although sample-splitting approaches such as this are helpful in many cases, the uniform bound for MR suggests that they are not strictly necessary, depending on the sample size and the complexity of F.\n\n\nLimitations of Existing Variable Importance Methods\n\nSeveral common approaches for variable selection, or for describing relationships between variables, do not necessarily capture a variable's importance. Null hypothesis testing methods may identify a relationship, but do not describe the relationship's strength. Similarly, checking whether a variable is included by a sparse model-fitting algorithm, such as the Lasso (Hastie et al., 2009), does not describe the extent to which the variable is relied on. Partial dependence plots Hastie et al., 2009) can be difficult to interpret if multiple variables are of interest, or if the prediction model contains interaction effects.\n\nAnother common VI procedure is to run a model-fitting algorithm twice, first on all of the data, and then again after removing X 1 from the data set. The losses for the two resulting models are then compared to determine the importance, or \"necessity,\" of X 1 (Gevrey et al., 2003). Because this measure is a function of two prediction models rather than one, it does not measure how much either individual model relies on X 1 . We refer to this approach as measuring empirical Algorithm Reliance (AR) on X 1 , as the modelfitting algorithm is the common attribute between the two models. Related procedures were proposed by , which measure the sufficiency of X 1 .\n\nAs we discuss in Section 3.1, the permutation-based VI measure from RFs  forms the inspiration for our definition of MR. This RF VI measure has been the topic of empirical studies (Archer and Kimes, 2008;Calle and Urrea, 2010;Wang et al., 2016), and several variations of the measure have been proposed (Strobl et al., 2007(Strobl et al., , 2008Altmann et al., 2010;Hapfelmeier et al., 2014). Mentch and Hooker (2016) use U-statistics to study predictions of ensemble models fit to subsamples, similar to the bootstrap aggregation used in RFs. Procedures related to \"Mean Difference Impurity,\" another VI measure derived for RFs, have been studied theoretically by Louppe et al. (2013); Kazemitabar et al. (2017). All of this literature focuses on VI measures for RFs, for ensembles, or for individual trees. Our estimator for model reliance differs from the traditional RF VI measure  in that we permute inputs to the overall model, rather than permuting the inputs to each individual ensemble member. Thus, our approach can be used generally, and is not limited to trees or ensemble models.\n\nOutside of the context of RF VI, Zhu et al. (2015) propose an estimand similar to our definition of model reliance, and Gregorutti et al. (2015Gregorutti et al. ( , 2017 propose an estimand analogous to e switch (f ) \u2212 e orig (f ). These recent works focus on the model reliance of f on X 1 specifically when f is equal to the conditional expectation function of Y (that is,\nf (x 1 , x 2 ) = E[Y |X 1 = x 1 , X 2 = x 2 ])\n. In contrast, we consider model reliance for arbitrary prediction models f . Datta et al. (2016) study the extent to which a model's predictions are expected to change when a subset of variables is permuted, regardless of whether the permutation affects a loss function L. These VI approaches are specific to a single prediction model, as is MR. In the next section, we consider a more general conception of importance: how much any model in a particular set may rely on the variable of interest.\n\n\nModel Class Reliance\n\nLike many statistical procedures, our MR measure (Section 3) produces a description of a single predictive model. Given a model with high predictive accuracy, MR describes how much the model's performance hinges on covariates of interest (X 1 ). However, there will often be many other models that perform similarly well, and that rely on X 1 to different degrees. With this notion in mind, we now study how much any well-performing model from a prespecified class F may rely on covariates of interest.\n\nRecall from Section 2.1 that, in order to define a population -Rashomon set of nearoptimal models, we must choose a \"reference\" model f ref to serve as a performance benchmark. In order to discuss this choice, we now introduce more explicit notation for the population -Rashomon set, written as We can now formalize our definitions of population-level MCR and empirical MCR by simply plugging in our definitions for M R(f ) and M R(f ) (Section 3) into Eqs 2.2 & 2.4 respectively. Studying population-level MCR (Eq 2.2) is the main focus of this paper, as it provides a more comprehensive view of importance than measures from a single model. If M CR + ( ) is low, then no well-performing model in F places high importance on X 1 , and X 1 can be discarded at low cost regardless of future modeling decisions. If M CR \u2212 ( ) is large, then every well-performing model in F must rely substantially on X 1 , and X 1 should be given careful attention during the modeling process. Here, F may itself consist of several parametric model forms (for example, all linear models and all decision tree models with less than 6 single-split nodes). We stress that the range [M CR \u2212 ( ), M CR + ( )] does not depend on the fitting algorithm used to select a model f \u2208 F. The range is valid for any algorithm producing models in F, and applies for any f \u2208 F.\nR( , f ref , F) := {f \u2208 F : e orig (f ) \u2264 e orig (f ref ) + } .\nIn the remainder of this section, we derive finite sample bounds for population-level MCR, from which we argue that empirical MCR provides reasonable estimates of populationlevel MCR (Section 4.1). In Appendix B.7 we consider an alternate formulation of Rashomon sets and MCR where we replace the relative loss threshold in the definition of R( ) with an absolute loss threshold. This alternate formulation can be similar in practice, but still requires the specification of a reference function f ref to ensure that R( ) andR( ) are nonempty.\n\n\nMotivating Empirical Estimators of MCR by Deriving Finite-sample Bounds\n\nIn this section we derive finite-sample, probabilistic bounds for M CR + ( ) and M CR \u2212 ( ). Our results imply that, under minimal assumptions, M CR + ( ) and M CR \u2212 ( ) are respectively within a neighborhood of M CR + ( ) and M CR \u2212 ( ) with high probability. However, the weakness of our assumptions (which are typical for statistical-learning-theoretic analysis) renders the width of our resulting CIs to be impractically large, and so we use these results only to show conditions under which M CR + ( ) and M CR \u2212 ( ) form sensible point estimates. In Sections 9.1 & 10, below, we apply a bootstrap procedure to account for sampling variability.\n\nTo derive these results we introduce three bounded loss assumptions, each of which can be assessed empirically. Let b orig , B ind , B ref , B switch \u2208 R be known constants.\n\nAssumption 1 (Bounded individual loss) For a given model f \u2208 F, assume that 0 \u2264 L(f, (y, x 1 , x 2 )) \u2264 B ind for any (y, x 1 , x 2 ) \u2208 (Y \u00d7 X 1 \u00d7 X 2 ).\n\nAssumption 2 (Bounded relative loss) For a given model f \u2208 F, assume that |L(f, (y,\nx 1 , x 2 ))\u2212 L(f ref , (y, x 1 , x 2 ))| \u2264 B ref for any (y, x 1 , x 2 ) \u2208 Z.\nAssumption 3 (Bounded aggregate loss) For a given model\nf \u2208 F, assume that P{0 < b orig \u2264\u00ea orig (f )} = P{\u00ea switch (f ) \u2264 B switch } = 1.\nEach assumption is a property of a specific model f \u2208 F. The notation B ind and B ref refer to bounds for any individual observation, and the notation b orig and B switch refer to bounds on the aggregated loss L in a sample. These boundedness assumptions are central to our finite sample guarantees, shown below.\n\nCrucially, loss functions L that are unbounded in general may be used so long as L(f, (y, x 1 , x 2 )) is bounded on a particular domain. For example, the squared-error loss can be used if Y is contained within a known range, and predictions f (x 1 , x 2 ) are contained within the same range for (x 1 , x 2 ) \u2208 X \u00d7 X 2 . We give example methods of determining B ind in Sections 7.3.2 & 7.4.2. For Assumption 3, we can approximate b orig by training a highly flexible model to the data, and setting b orig equal to half (or any positive fraction) of the resulting cross-validated loss. To determine B switch we can simply set B switch = B ind , although this may be conservative. For example, in the case of binary classification models for non-separable groups (see Section 9.1), no linear classifier can misclassify all observations, particularly after a covariate is permuted. Thus, it must hold that B ind > B switch .\n\nSimilarly, if f ref satisfies Assumption 1, then B ref may be conservatively set equal to B ind . If model reliance is redefined as a difference rather than a ratio, then a similar form of the results in this section will apply without Assumption 3 (see Appendix A.5).\n\nBased on these assumptions, we can create a finite-sample upper bound for M CR + ( ) and lower bound for M CR \u2212 ( ). In other words, we create an \"outer\" bound that contains the interval [M CR \u2212 ( ), M CR + ( )] with high probability.\n\nTheorem 4 (\"Outer\" MCR Bounds) Given a constant \u2265 0, let f +, \u2208 arg max R( ) M R(f ) and f \u2212, \u2208 arg min R( ) M R(f ) be prediction models that attain the highest and lowest model reliance among models in R( ). If f +, and f \u2212, satisfy Assumptions 1, 2 & 3, then P M CR + ( ) > M CR + ( out ) + Q out \u2264 \u03b4, and (4.2)\nP M CR \u2212 ( ) < M CR \u2212 ( out ) \u2212 Q out \u2264 \u03b4,(4.3)\nwhere out := + 2B ref\nlog(3\u03b4 \u22121 ) 2n\n, and Q out :\n= B switch b orig \u2212 B switch \u2212B ind log(6\u03b4 \u22121 ) n b orig +B ind log(6\u03b4 \u22121 ) 2n .\nEq 4.2 states that, with high probability, M CR + ( ) is no higher than M CR + ( out ) added to an error term Q out . As n increases, out approaches and Q out approaches zero. One practical implication is that, roughly speaking, if M CR + ( ) \u2248 M CR + ( out ), then the empirical estimator M CR + ( ) is unlikely to substantially underestimate M CR + ( ). By similar reasoning, we can conclude from Eq 4.3 that if M CR \u2212 ( ) \u2248 M CR \u2212 ( out ), then M CR \u2212 ( ) is unlikely to substantially overestimate M CR \u2212 ( ). By setting = 0, Theorem 4 can also be used to create a finite-sample bound for the reliance of the unique (unknown) best-in-class model on X 1 (see Corollary 22 in Appendix A.4), although describing individual models is not the main focus of this paper.\n\nWe provide a visual illustration of Theorem 4 in Figure 3. A brief sketch of the proof is as follows. First, we enlarge the empirical -Rashomon set by increasing to out , such that, by Hoeffding's inequality, f +, \u2208R( out ) with high probability. When f +, \u2208R( out ), we know that M R(f +, ) \u2264 M CR + ( out ) by the definition of M CR + ( out ). Next, the term Q out leverages finite-sample results for U-statistics to account for estimation error of M R(f +, ) = M CR + ( ) when using the estimator M R(f +, ). Thus, we can relate M R(f +, ) to both M CR + ( out ) and M CR + ( ) in order to obtain Eq 4.2. Similar steps can be applied to obtain Eq 4.3.\n\nThe bounds in Theorem 4 naturally account for potential overfitting without an explicit limit on model class complexity (such as a covering number, Rademacher complexity, or VC dimension). Instead, these bounds depend on being able to fully optimize MR across sets in the form ofR( ). If we allow our model class F to become more flexible, then the size ofR( ) will also increase. Because the bounds in Theorem 4 result from optimizing overR( ), increasing the size ofR( ) results in wider, more conservative bounds. In this way, Eqs 4.2 and 4.3 implicitly capture model class complexity.\n\nSo far, Theorem 4 lets us bound the range of MR values corresponding to models that predict well, but it does not tell us whether these bounds are actually attained. Similarly, we can conclude from Theorem 4 that [M CR \u2212 ( ), M CR + ( )] is unlikely to exceed the estimated range [ M CR \u2212 ( ), M CR + ( )] by a substantial margin, but we cannot determine whether this estimated range is unnecessarily wide. For example, consider the models that drive the M CR + ( ) estimator: the models with strong in-sample accuracy, and high empirical reliance on X 1 . These models' in-sample performance could merely be the result of overfitting, in which case they do not tell us direct information about R( ). Alternatively, even if all of these models truly do perform well on expectation (that is, even if they are contained in R( )), the model with the highest empirical reliance on X 1 may merely be the model for which our empirical MR estimate contains the most error. Either of these scenarios can cause M CR + ( ) to be unnecessarily high, relative to M CR + ( ).\n\nFortunately, both problematic scenarios are solved by requiring a limit on the complexity of F. We propose a complexity measure in the form of a covering number, which allows us control a worst case scenario of either overfitting or MR estimation error. Specifically, we define the set of functions G r as an r-margin-expectation-cover if for any f \u2208 F and any distribution D, there exists g \u2208 G r such that\nE Z\u223cD |L (f, Z) \u2212 L (g, Z)| \u2264 r.\n(4.4)\n\nWe define the covering number N (F, r) to be the size of the smallest r-margin-expectationcover for F. In general, we use P V \u223cD and E V \u223cD to denote probabilities and expectations with respect to a random variable V following the distribution D. We abbreviate these quantities accordingly when V or D are clear from context, for example, as P D , P V , or simply P. Unless otherwise stated, all expectations and probabilities are taken with respect to the (unknown) population distribution. We first show that this complexity measure allows us to control the worst case MR estimation error, that is, the covering number N (F, r) provides a uniform bound on the error of M R(f ) for all f \u2208 F.\n\nTheorem 5 (Uniform bound for M R) Given r > 0, if Assumptions 1 and 3 hold for all f \u2208 F, then\nP sup f \u2208F M R(f ) \u2212 M R(f ) > q(\u03b4, r, n) \u2264 \u03b4, where q(\u03b4, r, n) := B switch b orig \u2212 B switch \u2212 B ind log(4\u03b4 \u22121 N (F,r \u221a 2)) n + 2r \u221a 2 b orig + B ind log(4\u03b4 \u22121 N (F ,r)) 2n + 2r\n.\n\n(4.5)\n\nTheorem 5 states that, with high probability, the largest possible estimation error for M R(f ) across all models in F is bounded by q(\u03b4, r, n), which can be made arbitrarily small by increasing n and decreasing r. As we noted in Section 3.1, this means that it is possible to train a model and estimate its reliance on variables without using sample-splitting.\n\nThe covering number N (F, r) can also be used to limit the extent of overfitting (see Appendix B.5.1). As a result, it is possible to set an in-sample performance threshold low enough so that it will only be met by models with strong expected performance (that is, by models truly within R( )). To implement this idea of a stricter performance threshold, we contract the empirical -Rashomon set by subtracting a buffer term from . This requires that we generalize the definition of an empirical -Rashomon set toR( , f ref ,\nF) := {f ref } \u222a {f \u2208 F :\u00ea orig (f ) \u2264\u00ea orig (f ref ) + } for \u2208 R,\nwhere the explicit inclusion of f ref now ensures thatR( , f ref , F) is nonempty, even for < 0. As before, we typically omit the notation f ref and F, writingR( ) instead.\n\nWe are now prepared to answer the questions of whether the bounds from Theorem 4 are actually attained, and of whether the estimated range [ M CR \u2212 ( ), M CR + ( )] is unnecessarily wide. Our answer comes in the form of an upper bound on M CR \u2212 ( ), and a lower bound on M CR + ( ).\n\nTheorem 6 (\"Inner\" MCR Bounds) Given constants \u2265 0 and r > 0, if Assumptions 1, 2 and 3 hold for all f \u2208 F, and then\nP M CR + ( ) < M CR + ( in ) \u2212 Q in \u2264 \u03b4, and (4.6) P M CR \u2212 ( ) > M CR \u2212 ( in ) + Q in \u2264 \u03b4, (4.7) where in := \u2212 2B ref log(4\u03b4 \u22121 N (F ,r)) 2n\n\u2212 2r, and Q in = q \u03b4 2 , r, n , as defined in Eq 4.5.\n\nTheorem 6 can allow us to infer an \"inner\" bound that is contained within the interval [M CR \u2212 ( ), M CR + ( )] with high probability. In Figure 3, we illustrate the result of Theorem 6, and give a sketch of the proof. This proof follows a similar structure to that of Theorem 4, but incorporates Theorem 5's uniform bound on MR estimation error (Q in term), as well as an additional uniform bound on the probability that any model has in-sample loss too far from its expected loss ( in term).\n\nA practical implication of Theorem 6 is that, roughly speaking, if M CR + ( in ) \u2248 M CR + ( ), then it is unlikely for the empirical estimator M CR + ( ) to substantially underestimate M CR + ( ). Taken together with Theorem 4, we can conclude that, if M CR + ( in ) \u2248 M CR + ( out ), then the estimator M CR + ( ) is unlikely either to overestimate or to underestimate M CR + ( ) by very much. In large samples, it may be plausible to expect the condition M CR + ( in ) \u2248 M CR + ( out ) to hold, since in and out both approach as n increases. In the same way, if M CR \u2212 ( in ) \u2248 M CR \u2212 ( out ), we can conclude from Eqs 4.3 & 4.7 that the empirical estimator M CR \u2212 ( ) is unlikely to either overestimate or underestimate M CR \u2212 ( ) by very much. For this reason, we argue that M CR \u2212 ( ) and M CR + ( ) form sensible estimates of population-level MCR -each is contained within a neighborhood of its respective estimand, with high probability. The secondary x-axis of Figure 3 gives an illustration of this argument.\n\n\nExtensions of Rashomon Sets Beyond Variable Importance\n\nIn this section we generalize the Rashomon set approach beyond the study of MR. In Section 5.1, we create finite-sample CIs for other summary characterizations of near-optimal, or best-in-class models. The generalization also helps to illustrate a core aspect of the argument underlying Theorem 4: models with near-optimal performance in the population tend to have relatively good performance in random samples.\n\nIn Section 5.2, we review existing literature on near-optimal models.\n\n\nFinite-sample Confidence Intervals from Rashomon Sets\n\nRather than describing how much a model relies on X 1 , here we assume the analyst is interested in an arbitrary characteristic of a model. We denote this characteristic of interest as \u03c6 : F \u2192 R. For example, if f \u03b2 is the linear model f \u03b2 (x) = x \u03b2, then \u03c6 may be defined as the norm of the associated coefficient vector (that is, \u03c6(f \u03b2 ) = \u03b2 2 2 ) or the prediction f \u03b2 would assign given a specific covariate profile\nx new (that is, \u03c6(f \u03b2 ) = f \u03b2 (x new )). !\" # (%) ' ()* \u2212 ' !,\" - . ' ()* ' !,\" - / ' !,\" - . ' ' \u2212 ' 01 !,\" - . ' 01 !,\" - / ' 01 2\u0302( 405 (%) !,\" - / ' ()* !\" # (%) !,\" -/ ' !,\" -. ' % 467 Q in < l a t e x i t s h a 1 _ b a s e 6 4 = \" O j t R U u P C O w h Z U A E o 7 u o x 5 1 H F / u w = \" > A A A C A H i c d V D J S g N B F O x x j X E b 9 e D B S 2 M Q c h p m s p j M L e D F Y w J m g S S E n k 4 n a d K z 0 P 1 G D M N c / B U v H h T x 6 m d 4 8 2 / s L I K K F j Q U V e / R r 8 q L B F d g 2 x / G 2 v r G 5 t Z 2 Z i e 7 u 7 d / c G g e H b d U G E v K m j Q U o e x 4 R D H B A 9 Y E D o J 1 I s m I 7 w n W 9 q Z X c 7 9 9 y 6 T i Y X A D s 4 j 1 f T I O + I h T A l o a m K c 9 n 8 C E E p E 0 0 k H S A 3 Y H C Q / S d G D m b K t Y r l S d S 2 x b t u u W q k V N y m X X d W 3 s W P Y C O b R C f W C + 9 4 Y h j X 0 W A B V E q a 5 j R 9 B P i A R O B U u z v V i x i N A p G b O u p g H x m e o n i w A p v t D K E I 9 C q V 8 A e K F + 3 0 i I r 9 T M 9 / T k / F z 1 2 5 u L f 3 n d G E b V v g 4 U x c A C u v x o F A s M I Z 6 3 g Y d c M g p i p g m h k u t b M Z 0 Q S S j o z r K 6 h K + k + H / S K l h O 0 S o 0 S r l a f l V H B p 2 h c 5 R H D q q g G r p G d d R E F K X o A T 2 h Z + P e e D R e j N f l 6 J q x 2 j l B P 2 C 8 f Q K Z s p e l < / l a t e x i t > Q in < l a t e x i t s h a 1 _ b a s e 6 4 = \" O j t R U u P C O w h Z U A E o 7 u o x 5 1 H F / u w = \" > A A A C A H i c d V D J S g N B F O x x j X E b 9 e D B S 2 M Q c h p m s p j M L e D F Y w J m g S S E n k 4 n a d K z 0 P 1 G D M N c / B U v H h T x 6 m d 4 8 2 / s L I K K F j Q U V e / R r 8 q L B F d g 2 x / G 2 v r G 5 t Z 2 Z i e 7 u 7 d / c G g e H b d U G E v K m j Q U o e x 4 R D H B A 9 Y E D o J 1 I s m I 7 w n W 9 q Z X c 7 9 9 y 6 T i Y X A D s 4 j 1 f T I O + I h T A l o a m K c 9 n 8 C E E p E 0 0 k H S A 3 Y H C Q / S d G D m b K t Y r l S d S 2 x b t u u W q k V N y m X X d W 3 s W P Y C O b R C f W C + 9 4 Y h j X 0 W A B V E q a 5 j R 9 B P i A R O B U u z v V i x i N A p G b O u p g H x m e o n i w A p v t D K E I 9 C q V 8 A e K F + 3 0 i I r 9 T M 9 / T k / F z 1 2 5 u L f 3 n d G E b V v g 4 U x c A C u v x o F A s M I Z 6 3 g Y d c M g p i p g m h k u t b M Z 0 Q S S j o z r K 6 h K + k + H / S K l h O 0 S o 0 S r l a f l V H B p 2 h c 5 R H D q q g G r p G d d R E F K X o A T 2 h Z + P e e D R\ne j N f l 6 J q x 2 j l B P 2 C 8 f Q K Z s p e l < / l a t e x i t > Q out < l a t e x i t s h a 1 _ b a s e 6 4 = \" I 6 / H D X 4 j Y u g G 0 z 4 q 2 y u 2 To create the bounds from Theorem 4, we expand the empirical -Rashomon set by increasing to out , such that f +, (or f \u2212, ) is contained inR( out ) with high probability. We then add (or subtract) Q out to account for estimation error of M R(f +, ) (or M R(f \u2212, )). These steps are illustrated above in blue, with the final bounds shown by the blue bracket symbols along the x-axis. To create the bounds for M CR + ( ) (and M CR \u2212 ( )) in Theorem 6, we constrict the empirical -Rashomon set by decreasing to in , such that all models with high expected loss are simultaneously excluded fromR( in ) with high probability. We then subtract (or add) Q in to simultaneously account for MR estimation error for models inR( in ). These steps are illustrated above in purple, with the final bounds shown by the purple bracket symbols along the x-axis. For emphasis, below this figure we show a copy of the x-axis with selected annotations, from which it is clear that M CR \u2212 ( ) and M CR + ( ) are always within the bounds produced by Theorems 4 and 6. With high probability, M CR \u2212 ( ) and M CR + ( ) are within a neighborhood of M CR \u2212 ( ) and M CR + ( ) respectively.\n6 o C 1 p L w = \" > A A A C A X i c d V D J S g N B E O 1 x j X G L e h G 8 N A Y h p 2 E m M z H x F v D i M Q G z Q B J C T 6 e T N O l Z 6 K 4 R w z B e / B U v H h T x 6 l 9 4 8 2 / s L I K K P i h 4 v F d F V T 0 v E l y B Z X 0 Y K 6 t r 6 x u b m a 3 s 9 s 7 u 3 n 7 u 4 L C p w l h S 1 q C h C G X b I 4 o J H r A G c B C s H U l G f E + w l j e 5 n P m t G y Y V D 4 N r m E a s 5 5 N R w I e c E t B S P 3 f c 9 Q m M K R F J P e 0 n X W C 3 k I Q x p G k / l 7 f M o l M q W R V s m a 7 r u O 6 F J u X S e c l 2 s G 1 a c + T R E r V + 7 r 0 7 C G n s s w C o I E p 1 b C u C X k I k c C p Y m u 3 G i k W E T s i I d T Q N i M 9 U L 5 l / k O I z r Q z w M J S 6 A s B z 9 f t E Q n y l p r 6 n O 2 f 3 q t / e T P z L 6 8 Q w r P Q S H k Q x s I A u F g 1 j g S H E s z j w g E t G Q U w 1 I V R y f S u m Y y I J B R 1 a V o f w 9 S n + n z S L p u 2 Y x b q b r x a W c W T Q C T p F B W S j M q q i K 1 R D D U T R H X p A T + j Z u D c e j R f j d d G 6 Y i x n j t A P G G + f b J G Y G g = = < / l a t e x i t > Q out < l a t e x i t s h a 1 _ b a s e 6 4 = \" I 6 / H D X 4 j Y u g G 0 z 4 q 2 y u 2 6 o C 1 p L w = \" > A A A C A X i c d V D J S g N B E O 1 x j X G L e h G 8 N A Y h p 2 E m M z H x F v D i M Q G z Q B J C T 6 e T N O l Z 6 K 4 R w z B e / B U v H h T x 6 l 9 4 8 2 / s L I K K P i h 4 v F d F V T 0 v E l y B Z X 0 Y K 6 t r 6 x u b m a 3 s 9 s 7 u 3 n 7 u 4 L C p w l h S 1 q C h C G X b I 4 o J H r A G c B C s H U l G f E + w l j e 5 n P m t G y Y V D 4 N r m E a s 5 5 N R w I e c E t B S P 3 f c 9 Q m M K R F J P e 0 n X W C 3 k I Q x p G k / l 7 f M o l M q W R V s m a 7 r u O 6 F J u X S e c l 2 s G 1 a c + T R E r V + 7 r 0 7 C G n s s w C o I E p 1 b C u C X k I k c C p Y m u 3 G i k W E T s i I d T Q N i M 9 U L 5 l / k O I z r Q z w M J S 6 A s B z 9 f t E Q n y l p r 6 n O 2 f 3 q t / e T P z L 6 8 Q w r P Q S H k Q x s I A u F g 1 j g S H E s z j w g E t G Q U w 1 I V R y f S u m Y y I J B R 1 a V o f w 9 S n + n z S L p u 2 Y x b q b r x a W c W T Q C T p F B W S j M q q i K 1 R D D U T R H X p A T + j Z u D c e j R f j d d G 6 Y i x n j t A P G G + f b J G Y G g = = < / l a t e x i t >\nGiven a descriptor \u03c6, we now show a general result that allows creation of finite-sample CIs for the best performing models R( ). The resulting CIs are themselves based on empirical Rashomon sets.\n\nProposition 7 (Finite sample CIs from Rashomon sets) Let := + 2B ref\nlog(2\u03b4 \u22121 ) 2n , let \u03c6 \u2212 ( ) := min f \u2208R( ) \u03c6(f ) and let\u03c6 + ( ) := max f \u2208R( ) \u03c6(f ).\nIf Assumption 2 holds for all f \u2208 R( ), then\nP {\u03c6(f ) : f \u2208 R( )} \u2286 \u03c6 \u2212 ( ),\u03c6 + ( ) \u2265 1 \u2212 \u03b4.\nProposition 7 generates a finite-sample CI for the range of values \u03c6(f ) corresponding to well-performing models, {\u03c6(f ) : f \u2208 R( )}. This CI, denoted by \u03c6 \u2212 ( ),\u03c6 + ( ) , can itself be interpreted as the range of values \u03c6(f ) corresponding to models f with empirical loss not substantially above that of f ref . Thus, the interval has both a rigorous coverage rate and a coherent in-sample interpretation. The proof of Proposition 7 uses Hoeffding's inequality to show that models in F are contained inR ( ) with high probability, that is, that models with good expected performance tend to perform well in random samples.\n\nAn immediate corollary of Proposition 7 is that we can generate finite-sample CIs for all best-in-class models f \u2208 arg min f \u2208F EL(f, Z) by setting = 0. This corollary can be further strengthened if a single model f is assumed to uniquely minimize EL(f, Z) over f \u2208 F (see Appendix B.6).\n\nNote that Proposition 7 implicitly assumes that \u03c6(f ) can be determined exactly for any model f \u2208 F, in order for the interval \u03c6 \u2212 ( ),\u03c6 + ( ) to be precisely determined. This assumption does not hold, for example, if \u03c6(f ) = M R(f ), or if \u03c6(f ) = Var{f (X 1 , X 2 )}, as these quantities depend on both f and the (unknown) population distribution. In such cases, an additional correction factor must be incorporated to account for estimation error of \u03c6(f ), such as the Q out term in Theorem 4.\n\nIn concurrent work, Coker et al. (2018) show that profile likelihood intervals take the same form as the interval [\u03c6 \u2212 ( ),\u03c6 + ( )] in Proposition 7. This means that a profile likelihood interval can also be expressed by minimizing and maximizing over an empirical Rashomon set. More specifically, consider the case where the loss function L is the negative of the known log likelihood function, and where f ref is the maximum likelihood estimate of the \"true model,\" which in this case is f . If additional minor assumptions are met (see Appendix A.6 for details), then the (\n1 \u2212 \u03b4)-level profile likelihood interval for \u03c6(f ) is equal to [\u03c6 \u2212 ( \u03c7 1,1\u2212\u03b4 2n ),\u03c6 + ( \u03c7 1,1\u2212\u03b4 2n )]\n, where\u03c6 \u2212 and\u03c6 + are defined as in Proposition 7, and \u03c7 1,1\u2212\u03b4 is the 1 \u2212 \u03b4 percentile of a chi-square distribution with 1 degree of freedom.\n\nRelative to a profile likelihood approach, the advantage of Proposition 7 is that it does not require asymptotics, it does not require that the likelihood be known up to a parametric form, and it can be extended to study the set of near-optimal prediction models R( ), rather than a single, potentially misspecified prediction model f . This is especially useful when different near-optimal models accurately describe different aspects of the underlying data generating process, but none capture it completely. The disadvantage of Proposition 7 is that the required performance threshold of = + 2B ref\nlog(2\u03b4 \u22121 ) 2n\ndecreases more slowly than the performance threshold of\n\u03c7 1,1\u2212\u03b4 2n\nrequired in a profile likelihood interval. Because our results from Section 4.1 carry a similar disadvantage, we use these results primarily to motivate point estimates describing the Rashomon set R( ). Still, it is worth emphasizing the generality of Proposition 7. Through this result, Rashomon sets allow us to reframe a wide set of finite-sample inference problems as insample optimization problems. The implied CIs are not necessarily in closed form, but the approach still opens an exciting pathway for deriving non-asymptotic results. For example, they imply that existing methods for profile likelihood intervals might be able to be reapplied to achieve finite-sample results. For highly complex model classes where profile likelihoods are difficult to compute, such as neural networks or random forests, approximate inference is sometimes achieved via approximate optimization procedures (for example, Markov chain Monte Carlo for Bayesian additive regression trees, in Chipman et al., 2010). Proposition 7 shows that similar approximate optimization methods could be repurposed to establish approximate, finite-sample inferences for the same model classes.\n\n\nRelated Literature on the Rashomon Effect\n\nBreiman et al. (2001) introduced the \"Rashomon effect\" of statistics as a problem of ambiguity: if many models fit the data well, it is unclear which model we should try to interpret. Breiman suggests that the ensembling many well-performing models together can resolve this ambiguity, as the new ensemble model may perform better than any of its individual members. However, this approach may only push the problem from the member level to the ensemble level, as there may also be many different ensemble models that fit the data well.\n\nThe Rashomon effect has also been considered in several subject areas outside of VI, including those in non-statistical academic disciplines (Heider, 1988;Roth and Mehta, 2002). Tulabandhula and Rudin (2014) optimize a decision rule to perform well under the predicted range of outcomes from any well-performing model. Statnikov et al. (2013) propose an algorithm to discover multiple Markov boundaries, that is, minimal sets of covariates such that conditioning on any one set induces independence between the outcome and the remaining covariates. Nevo and Ritov (2017) report interpretations corresponding to a set of well-fitting, sparse linear models. Meinshausen and B\u00fchlmann (2010) estimate structural aspects of an underlying model (such as the variables included in that model) based on how stable those aspects are across a set of well-fitting models. This set of well-fitting models is identified by repeating an estimation procedure in a series of perturbed samples, using varying levels of regularization (see also Azen et al., 2001). Letham et al. (2016) search for a pair of well-fitting dynamical systems models that give maximally different predictions.\n\n\nCalculating Empirical Estimates of Model Class Reliance\n\nIn this section, we propose a binary search procedure to bound the values of M CR \u2212 ( ) and M CR + ( ) (see Eq 2.4), which respectively serve as estimates of M CR \u2212 ( ) and M CR + ( ) (see Section 4.1). Each step of this search consists of minimizing a linear combination of e orig (f ) and\u00ea switch (f ) across f \u2208 F. Our approach is related to the fractional programming approach of Dinkelbach (1967), but accounts for the fact that the problem is constrained by the value of the denominator,\u00ea orig (f ). We additionally show that, for many model classes, computing M CR \u2212 ( ) only requires that we minimize convex combinations of\u00ea orig (f ) and e switch (f ), which is no more difficult than minimizing the average loss over an expanded and reweighted sample (See Eq 6.2 & Proposition 11).\n\n\nModel class and loss function (F & L)\nComputing M CR \u2212 Computing M CR + (L2\nComputing M CR + ( ) however will require that we are able to minimize arbitrary linear combinations of\u00ea orig (f ) and\u00ea switch (f ). In Section 6.3, we outline how this can be done for convex model classes -classes for which the loss function is convex in the model parameter. Later, in Section 7, we give more specific computational procedures for when F is the class of linear models, regularized linear models, or linear models in a reproducing kernel Hilbert space (RKHS). We summarize the tractability of computing empirical MCR for different model classes in Table 1.\n\nTo simplify notation associated with the reference model f ref , we present our computational results in terms of bounds on empirical MR subject to performance thresholds on the absolute scale. More specifically, we present bound 8 show examples of these bounds). The binary search procedures we propose can be used to tighten these boundaries at a particular value abs of interest.\nfunctions b \u2212 and b + satisfying b \u2212 ( abs ) \u2264 M R(f ) \u2264 b + ( abs ) simultaneously for all {f, abs :\u00ea orig (f ) \u2264 abs , f \u2208 F, abs > 0} (Figures 2 &\nWe briefly note that as an alternative to the global optimization procedures we discuss below, heuristic optimization procedures such as simulated annealing can also prove useful in bounding empirical MCR. By definition, the empirical MR for any model inR( ) forms a lower bound for M CR + ( ), and an upper bound for M CR \u2212 ( ). Heuristic maximization and minimization of empirical MR can be used to tighten these boundaries.\n\nThroughout this section, we assume that 0 < min f \u2208F\u00eaorig (f ), to ensure that MR is finite.\n\n\nBinary Search for Empirical MR Lower Bound\n\nBefore describing our binary search procedure, we introduce additional notation used in this section. Given a constant \u03b3 \u2208 R and prediction model f \u2208 F, we define the linear combination\u0125 \u2212,\u03b3 , and its minimizers (for example,\u011d \u2212,\u03b3,F ), a\u015d\nh \u2212,\u03b3 (f ) := \u03b3\u00ea orig (f ) +\u00ea switch (f ), and\u011d \u2212,\u03b3,F \u2208 arg min f \u2208F\u0125 \u2212,\u03b3 (f ).\nWe do not require that\u0125 \u2212,\u03b3 is uniquely minimized, and we frequently use the abbreviated notation\u011d \u2212,\u03b3 when F is clear from context.\n\nOur goal in this section is to derive a lower bound on M R for subsets of F in the form of {f \u2208 F :\u00ea orig (f ) \u2264 abs }. We achieve this by minimizing a series of linear objective functions in the form of\u0125 \u2212,\u03b3 , using a similar method to that of Dinkelbach (1967). Often, minimizing the linear combination\u0125 \u2212,\u03b3 (f ) is more tractable than minimizing the MR ratio directly.\n\nAlmost all of the results shown in this section, and those in Section 6.2, also hold if we replace\u00ea switch with\u00ea divide throughout (see Eq 3.5), including in the definition of M R and h \u2212,\u03b3 (f ). The exception is Proposition 11, below, which we may still expect to approximately hold if we replace\u00ea switch with\u00ea divide .\n\nGiven an observed sample, we define the following condition for a pair of values {\u03b3, abs } \u2208 R \u00d7 R >0 , and argmin function\u011d \u2212,\u03b3 :\n\nCondition 8 (Criteria to continue search for M R lower bound)\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) \u2265 0 and e orig (\u011d \u2212,\u03b3 ) \u2264 abs .\n\nWe are now equipped to determine conditions under which we can tractably create a lower bound for empirical MR.\nLemma 9 (Lower bound for M R) If \u03b3 \u2208 R satisfies\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) \u2265 0, then h \u2212,\u03b3 (\u011d \u2212,\u03b3 ) abs \u2212 \u03b3 \u2264 M R(f ) (6.1) for all f \u2208 F satisfying\u00ea orig (f ) \u2264 abs . It also follows that \u2212\u03b3 \u2264 M R(f ) for all f \u2208 F.\nAdditionally, if f =\u011d \u2212,\u03b3 and at least one of the inequalities in Condition 8 holds with equality, then Eq 6.1 holds with equality.\n\nLemma 9 reduces the challenge of lower-bounding M R(f ) to the task of minimizing the linear combination\u0125 \u2212,\u03b3 (f ). The result of Lemma 9 is not only a single boundary for a particular value of abs , but a boundary function that holds all values of abs > 0, with lower values of abs leading to more restrictive lower bounds on M R(f ). Figure 4: Above, we illustrate the geometric intuition for Lemma 9. In Panel (A), we show an example of a hypothetical model class F, marked by the enclosed region. For each model f \u2208 F, the x-axis shows\u00ea orig (f ) and the y-axis shows\u00ea switch (f ).\nf 1 f 2 \u00ea switch ( f ) \u00ea orig ( f ) \u03f5 abs (A) Geometric interpretation of !\" # F \u00ea switch ( f ) \u00ea orig ( f ) (B) Minimizing linear combinations F f 1 f 2 \u00ea switch ( f ) \u00ea orig ( f ) \u03f5 abs (C) Combining constraints F\nHere, we can see that the condition min f \u2208F\u00eaorig (f ) > 0 holds. The blue dotted region marks models with higher empirical loss. We mark two example models within F as f 1 and f 2 . The slopes of the lines connecting the origin to f 1 and f 2 are equal to M R(f 1 ) and M R(f 2 ) respectively. Our goal is to lower-bound the slope corresponding to M R for any model f satisfying\u00ea orig (f ) \u2264 abs . In Panel (B), we consider the linear combination\u0125 \u2212,\u03b3 (f ) = \u03b3\u00ea orig (f ) +\u00ea switch (f ) for \u03b3 = 1. Above, contour lines of\u0125 \u2212,\u03b3 are shown in red. The solid red line indicates the smallest possible value of\u0125 \u2212,\u03b3 across f \u2208 F. Specifically, its yintercept equals min f \u2208F\u0125\u2212,\u03b3 (f ). If we can determine this minimum, we can determine a linear border constraint on F, that is, we will know that no points corresponding to models f \u2208 F may lie in the shaded region above. Additionally, if min f \u2208F\u0125\u2212,\u03b3 (f ) \u2265 0 (see Lemma 9), then we know that the origin is either excluded by this linear constraint, or is on the boundary. In Panel (C), we combine the two constraints from Panels (A) & (B) to see that models f \u2208 F satisfying\u00ea orig (f ) \u2264 abs must correspond to points in the white, unshaded region above. Thus, as long as the unshaded region does not contain the origin, any line connecting the origin to the a model f satisfying\u00ea orig (f ) \u2264 abs (for example, here, f 1 ,f 2 ) must have a slope at least as high as that of the solid black line above. It can be shown algebraically that the black line has slope equal to the left-hand side of Eq 6.1. Thus the left-hand side of Eq 6.1 is a lower bound for M R(f ) for all {f \u2208 F :\u00ea orig (f ) \u2264 abs } .\n\nIn addition to the formal proof for Lemma 9, we provide a heuristic illustration of the result in Figure 4, to aid intuition.\n\nIt remains to determine which value of \u03b3 should be used in Eq 6.1. The following lemma implies that this value can be determined by a binary search, given a particular value of interest for abs .\n\nLemma 10 (Monotonicity for M R lower bound binary search) The following monotonicity results hold:\n1.\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) is monotonically increasing in \u03b3. 2.\u00ea orig (\u011d \u2212,\u03b3 ) is monotonically decreasing in \u03b3.\n3. Given abs , the lower bound from Eq 6.1,\n\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) abs \u2212 \u03b3 , is monotonically decreasing in \u03b3 in the range where\u00ea orig (\u011d \u2212,\u03b3 ) \u2264 abs , and increasing otherwise.\nGiven a particular performance level of interest, abs , Point 3 of Lemma 10 tells us that the value of \u03b3 resulting in the tightest lower bound from Eq 6.1 occurs when \u03b3 is as low as possible while still satisfying Condition 8. Points 1 and 2 show that if \u03b3 0 satisfies Condition 8, and one of the equalities in Condition 8 holds with equality, then Condition 8 holds for all \u03b3 \u2265 \u03b3 0 . Together, these results imply that we can use a binary search to determine the value of \u03b3 to be used in Lemma 9, reducing this value until Condition 8 is no longer met. In addition to the formal proof for Lemma 10, we provide an illustration of the result in Figure 5 to aid intuition.\n\nNext we present simple conditions under which the binary search for values of \u03b3 can be restricted to the nonnegative real line. This result substantially extends the computational tractability of our approach, as minimizing\u0125 \u2212,\u03b3 for \u03b3 \u2265 0 is equivalent to minimizing a reweighted empirical loss over an expanded sample of size n 2 :\nh \u2212,\u03b3 (f ) = \u03b3\u00ea orig (f ) +\u00ea switch (f ) = n i=1 n j=1 w \u03b3 (i, j)L{f, (y [i] , X 1[j,\u00b7] , X 2[i,\u00b7] )}, (6.2) where w \u03b3 (i, j) = \u03b31(i=j) n + 1(i =j) n(n\u22121) \u2265 0.\nProposition 11 (Nonnegative weights for M R lower bound binary search) Assume that L and F satisfy the following conditions. We define two values \u03b3 1 < \u03b3 2 , where \u03b3 1 corresponds to the solid red line, above, and \u03b3 2 corresponds to the semi-transparent red line. The y-intercept values of these lines are equal t\u00f4 h \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) and\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) respectively (see Figure 4-C caption). The solid and semi-transparent black dots mark\u011d \u2212,\u03b3 1 and\u011d \u2212,\u03b3 2 respectively. Plugging \u03b3 1 and \u03b3 2 into Eq 6.1 yields two lower bounds for M R, marked by the slopes of the solid and semi-transparent black lines respectively (see Figure 4-C caption). We see that (1)\u0125 \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ), that (2)\u00ea orig (\u011d \u2212,\u03b3 1 ) \u2265\u00ea orig (\u011d \u2212,\u03b3 2 ), and that (3) the left-hand side of Eq 6.1 is decreasing in \u03b3 when\u00ea orig (\u011d \u2212,\u03b3 ) \u2264 abs . These three conclusions are marked by arrows in the above figure, with numbering matching the enumerated list in Lemma 10.\n\n\n(Predictions are sufficient for computing the loss) The loss\nL{f, (Y, X 1 , X 2 )} depends on the covariates (X 1 , X 2 ) only via the prediction function f , that is, L{f, (y, x (a) 1 , x (a) 2 )} = L{f, (y, x (b) 1 , x (b) 2 )} whenever f (x (a) 1 , x (a) 2 ) = f (x (b) 1 , x (b) 2 ).\n\n(Irrelevant information does not improve predictions) For any distribution\nD satisfy- ing X 1 \u22a5 D (X 2 , Y ), there exists a function f D satisfying E D L{f D , (Y, X 1 , X 2 )} = min f \u2208F E D L{f, (Y, X 1 , X 2 )}, and f D (x (a) 1 , x 2 ) = f D (x (b) 1 , x 2 ) for any x (a) 1 , x (b) 1 \u2208 X 1 and x 2 \u2208 X 2 . (6.3) F \u00ea switch ( f ) \u00ea orig ( f ) \u03f5\nLet \u03b3 = 0. Under the above assumptions, it follows that either (i) there exists a function\u011d \u2212,0 minimizing\u0125 \u2212,0 that does not satisfy Condition 8, or (ii)\u00ea orig (\u011d \u2212,0 ) \u2264 abs and M R(g \u2212,0 ) \u2264 1 for any function\u011d \u2212,0 minimizing\u0125 \u2212,0 .\n\nThe implication of Proposition 11 is that, when the conditions of Proposition 11 are met, the search region for \u03b3 can be limited to the nonnegative real line, and minimizing\u0125 \u2212,\u03b3 will be no harder than minimizing a reweighted empirical loss over an expanded sample (Eq 6.2). To see this, recall that for a fixed value of abs we can tighten the boundary in Lemma 9 by conducting a binary search for the smallest value of \u03b3 that satisfies Condition 8. If setting \u03b3 equal to 0 does not satisfy Condition 8, and the search for \u03b3 can be restricted to the nonnegative real line, where minimizing\u0125 \u2212,0 is more tractable (see Eq 6.2). Alternatively, if\u00ea orig (g \u2212,0 ) \u2264 abs and M R(g \u2212,0 ) \u2264 1, then we have identified a well-performing model g \u2212,0 with empirical MR no greater than 1. For abs =\u00ea orig (f ref ) + , this implies that M CR \u2212 ( ) \u2264 1, which is a sufficiently precise conclusion for most interpretational purposes (see Appendix A.2 ).\n\nBecause of the fixed pairing structure used in\u00ea divide , Proposition 11 will not necessarily hold if we replace\u00ea switch with\u00ea divide throughout (see Appendix C.3). However, since\u00ea divide approximates\u00ea switch , we can expect Proposition 11 to hold approximately. The bound from Eq 6.1 still remains valid if we replace\u00ea switch with\u00ea divide and limit \u03b3 to the nonnegative reals, although in some cases it may not be as tight.\n\n\nBinary Search for Empirical MR Upper Bound\n\nWe now briefly present a binary search procedure to upper bound M R, which mirrors the procedure from Section 6.1. Given a constant \u03b3 \u2208 R and prediction model f \u2208 F, we define the linear combination\u0125 +,\u03b3 , and its minimizers (for example,\u011d +,\u03b3,F ), a\u015d\nh +,\u03b3 (f ) :=\u00ea orig (f ) + \u03b3\u00ea switch (f ), and\u011d +,\u03b3,F \u2208 arg min f \u2208F\u0125 +,\u03b3 (f ).\nAs in Section 6.1,\u0125 +,\u03b3 need not be uniquely minimized, and we generally abbreviate\u011d +,\u03b3,F as\u011d +,\u03b3 when F is clear from context.\n\nGiven an observed sample, we define the following condition for a pair of values {\u03b3, abs } \u2208 R \u22640 \u00d7 R >0 , and argmin function\u011d +,\u03b3 :\n\nCondition 12 (Criteria to continue search for M R upper bound)\u0125 +,\u03b3 (\u011d +,\u03b3 ) \u2265 0 and e orig (\u011d +,\u03b3 ) \u2264 abs .\n\nWe can now develop a procedure to upper bound M R, as shown in the next lemma.\nLemma 13 (Upper bound for M R) If \u03b3 \u2208 R satisfies \u03b3 \u2264 0 and\u0125 +,\u03b3 (\u011d +,\u03b3 ) \u2265 0, then M R(f ) \u2264 \u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 (6.4) for all f \u2208 F satisfying\u00ea orig (f ) \u2264 abs . It also follows that M R(f ) \u2264 |\u03b3 \u22121 | for all f \u2208 F. (6.5)\nAdditionally, if f =\u011d +,\u03b3 and at least one of the inequalities in Condition 12 holds with equality, then Eq 6.4 holds with equality.\n\nAs in Section 6.1, it remains to determine the value of \u03b3 to use in Lemma 13, given a value of interest for abs \u2265 min f \u2208F\u00eaorig (f ). The next lemma tells us that the boundary from Lemma 13 is tightest when \u03b3 is as low as possible while still satisfying Condition 12.\n\nLemma 14 (Monotonicity for M R upper bound binary search) The following monotonicity results hold:\n\n1.\u0125 +,\u03b3 (\u011d +,\u03b3 ) is monotonically increasing in \u03b3.\n\n2.\u00ea orig (\u011d +,\u03b3 ) is monotonically decreasing in \u03b3 for \u03b3 \u2264 0, and Condition 12 holds for \u03b3 = 0 and abs \u2265 min f \u2208F\u00eaorig (f ).\n\n\nGiven abs , the upper boundary\n\u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 is monotonically increasing in \u03b3\nin the range where\u00ea orig (\u011d +,\u03b3 ) \u2264 abs and \u03b3 < 0, and decreasing in the range wher\u00ea e orig (\u011d +,\u03b3 ) > abs and \u03b3 < 0.\n\nTogether, the results from Lemma 14 imply that we can use a binary search across \u03b3 \u2208 R to tighten the boundary on M R from Lemma 13.\n\n\nConvex Models\n\nIn this section we show that empirical MCR can be conservatively computed when the loss function is convex in the model parameters -that is, when the models f \u03b8 \u2208 F are indexed by a d-dimensional parameter \u03b8 \u2208 \u0398 \u2286 R d , and when the loss function L(f \u03b8 , (y,\nx 1 , x 2 )) is convex in \u03b8 for all (x 1 , x 2 , y) \u2208 X 1 \u00d7 (X 2 , Y).\nFortunately, neither Lemma 9 nor Lemma 13 require an exact minimum for\u0125 \u2212,\u03b3 or h +,\u03b3 . For Lemma 9, any lower bound on\u0125 \u2212,\u03b3 is sufficient to determine a lower bound on M R(f ). Likewise, for Lemma 13, any lower bound on\u0125 +,\u03b3 is sufficient to determine an upper bound on M R(f ).\n\nTo find these lower bounds, we note that for \"convex\" model classes (defined above) the optimization problems in Sections 6.1 & 6.2 can be written either as convex optimization problems, or as difference convex function (DC) programs. A DC program is one that can be written as\nmin {\u03b8:c DC (\u03b8)\u2264k,\u03b8\u2208\u0398} g DC (\u03b8) \u2212 h DC (\u03b8),\nwhere c DC is a constraint function, k \u2208 R 1 , and g DC , h DC , and c DC are convex. Although precise solutions to DC problems are not always tractable, lower bounds can be attained by branch-and-bound (B&B) methods (Horst and Thoai, 1999). A simple B&B approach is to partition \u0398 into a set of simplexes. Within the j th simplex, a lower bound on g DC (\u03b8)\u2212h DC (\u03b8) can be determined by replacing h DC with the hyperplane function h j satisfying h j (v) = h DC (v) at each vertex v of the j th simplex. Within this partition, g DC (\u03b8) \u2212 h DC (\u03b8) is lower bounded by l j := min \u03b8 g DC (\u03b8) \u2212 h j (\u03b8), which can be computed as the solution to a convex optimization problem. Any partition for which l j is found to be too high is disregarded. Once a bound l j is computed for each partition, the partition with the lowest value l j is selected to be subdivided further, and additional lower bounds are recomputed for each new, resulting partition. This procedure continues until a sufficiently tight lower bound is attained (for more detailed procedures, see Horst and Thoai, 1999).\n\nThis approach allows us to conservatively approximate bounds on M R(f ) in the form of Eq 6.1 & 6.4 by replacing\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) and\u0125 +,\u03b3 (\u011d +,\u03b3 ) with lower bounds from the B&B procedure. Although it will always yield valid bounds, the procedure may converge slowly when the dimension of \u0398 is large, giving highly conservative results. For some special cases of model classes however, even high dimensional DC problems simplify greatly. We discuss these cases in the next section.\n\n\nMR & MCR for Linear Models, Additive Models, and Regression Models in a Reproducing Kernel Hilbert Space\n\nFor linear or additive models, many simplifications can be made to our approaches for MR and MCR. To simplify the interpretation of MR, we show below that population-level MR for a linear model can be expressed in terms of the model's coefficients (Section 7.1). To simplify computation, we show that the cost of computing empirical MR for a linear model grows only linearly in n (Section 7.1), even though the number of terms in the definition of empirical MR grows quadratically (see Eqs 3.3 & 3.6).\n\nMoving on from MR, we show how empirical MCR can be computed for the class of linear models (Section 7.2), for regularized linear models (Section 7.3), and for regression models in a reproducing kernel Hilbert space (RKHS, Section 7.4). To do this, we build on the approach in Section 6 by giving approaches for minimizing arbitrary combinations of\u00ea switch (f ) and\u00ea orig (f ) across f \u2208 F. Even when the associated objective functions are non-convex, we can tractably obtain global minima for these model classes. We also discuss procedures to determine an upper bound B ind on the loss for any observation when using these model classes (see Assumption 1).\n\nThroughout this section, we assume that X \u2282 R p for p \u2208 Z + , that Y \u2282 R 1 , and that L is the squared error loss function L(f, (y, x 1 , x 2 ) = (y \u2212 f (x 1 , x 2 )) 2 . As in Section 6, we also assume that 0 < min f \u2208F\u00eaorig (f ), to ensure that empirical MR is finite.\n\n\nInterpreting and Computing MR for Linear or Additive Models\n\nWe begin by considering MR for linear models evaluated with the squared error loss. For this setting, we can show both an interpretable definition of MR, as well as a computationally efficient formula for\u00ea switch (f ).\n\nProposition 15 (Interpreting MR, and computing empirical MR for linear models) For any prediction model f , let e orig (f ), e switch (f ),\u00ea orig (f ), and\u00ea switch (f ) be defined based on the squared error loss L(f, (y, x 1 , x 2 )) := (y \u2212 f (x 1 , x 2 )) 2 for y \u2208 R, x 1 \u2208 R p 1 , and x 2 \u2208 R p 2 , where p 1 and p 2 are positive integers. Let \u03b2 = (\u03b2 1 , \u03b2 2 ) and f \u03b2 satisfy \u03b2 1 \u2208 R p 1 , \u03b2 2 \u2208 R p 2 , and f \u03b2 (x) = x \u03b2 = x 1 \u03b2 1 + x 2 \u03b2 2 . Then\nM R(f \u03b2 ) = 1 + 2 e orig (f \u03b2 ) Cov(Y, X 1 )\u03b2 1 \u2212 \u03b2 2 Cov(X 2 , X 1 )\u03b2 1 ,(7.1)\nand, for finite samples,\ne switch (f \u03b2 ) = 1 n y y \u2212 2 X 1 Wy X 2 y \u03b2 + \u03b2 X 1 X 1 X 1 WX 2 X 2 WX 1 X 2 X 2 \u03b2 ,(7.\n\n2)\n\nwhere W := 1 n\u22121 (1 n 1 n \u2212 I n ), 1 n is the n-length vector of ones, and I n is the n \u00d7 n identity matrix.\n\nEq 7.1 shows that model reliance for linear models can be interpreted in terms of the population covariances, the model coefficients, and the model's accuracy. Gregorutti et al. (2017) show an equivalent formulation of Eq 7.1 under the stronger assumptions that f \u03b2 is equal to the conditional expectation function of Y (that is, f \u03b2 (x) = E(Y |X = x)), and the covariates X 1 and X 2 are centered.\n\nEq 7.2 shows that, although the number of terms in the definition of\u00ea switch grows quadratically in n (see Eq 3.3), the computational complexity of\u00ea switch (f \u03b2 ) for a linear model f \u03b2 grows only linearly in n. Specifically, the terms X 1 Wy and X 1 WX 2 in Eq 7.2 can be computed as 1 n\u22121 {(X 1 1 n )(1 n y) \u2212 (X 1 y)} and 1 n\u22121 {(X 1 1 n )(1 n X 2 ) \u2212 (X 1 X 2 )} respectively, where the computational complexity of each term in parentheses grows linearly in n.\n\nAs in Gregorutti et al. (2017), both results in Proposition 15 readily generalize to additive models of the form f g 1 ,g 2 (X 1 , X 2 ) := g 1 (X 1 ) + g 2 (X 2 ), since permuting X 1 is equivalent to permuting g 1 (X 1 ).\n\n\nComputing Empirical MCR for Linear Models\n\nBuilding on the computational result from the previous section, we now consider empirical MCR computation for linear model classes of the form\nF lm := f \u03b2 : f \u03b2 (x) = x \u03b2, \u03b2 \u2208 R p .\nIn order to implement the computational procedure from Sections 6.1 and 6.2, we must be able to minimize arbitrary linear combinations of\u00ea orig (f \u03b2 ) and\u00ea switch (f \u03b2 ). Fortunately, for linear models, this minimization reduces to a quadratic program, as we show in the next remark.\n\nRemark 16 (Tractability of empirical MCR for linear model classes) For any f \u03b2 \u2208 F lm and any fixed coefficients \u03be orig , \u03be switch \u2208 R, the linear combination\n\u03be orig\u00eaorig (f \u03b2 ) + \u03be switch\u00easwitch (f \u03b2 ) (7.3)\nis proportional in \u03b2 to the quadratic function \u22122q \u03b2 + \u03b2 Q\u03b2, where Q := \u03be orig X X+\u03be switch X 1 X 1 X 1 WX 2 X 2 WX 1 X 2 X 2 , q := \u03be orig y X + \u03be switch X 1 Wy X 2 y , and W := 1 n\u22121 (1 n 1 n \u2212 I n ). Thus, minimizing \u03be orig\u00eaorig (f \u03b2 ) + \u03be switch\u00easwitch (f \u03b2 ) is equivalent to an unconstrained (possibly non-convex) quadratic program.\n\nBecause our empirical MCR computation procedure from Sections 6.1 and 6.2 consists of minimizing a sequence of objective functions in the form of Eq 7.3, Remark 16 shows us that this procedure is tractable for the class of unconstrained linear models.\n\n\nRegularized Linear Models\n\nNext, we continue to build on the results from Section 7.2 to calculate boundaries on M R for regularized linear models. We consider model classes formed by quadratically constrained subsets of F lm , defined as\nF lm,r lm := f \u03b2 : f \u03b2 (x) = x \u03b2, \u03b2 \u2208 R p , \u03b2 M lm \u03b2 \u2264 r lm ,(7.4)\nwhere M lm and r lm are pre-specified. Again, this class describes linear models with a quadratic constraint on the coefficient vector.\n\n\nCalculating MCR\n\nAs in Section 7.2, calculating bounds on M R via Lemmas 9 & 13 requires that are able to minimizing linear combinations \u03be orig\u00eaorig (f \u03b2 ) + \u03be switch\u00easwitch (f \u03b2 ) across f \u03b2 \u2208 F lm,r lm for arbitrary \u03be orig , \u03be switch \u2208 R. Applying Remark 16, we can again equivalently minimize \u22122q \u03b2 + \u03b2 Q\u03b2 subject to the constraint in Eq 7.4:\nminimize \u2212 2q \u03b2 + \u03b2 Q\u03b2 subject to \u03b2 M lm \u03b2 \u2264 r lm . (7.5)\nThe resulting optimization problem is a (possibly non-convex) quadratic program with one quadratic constraint (QP1QC). This problem is well-studied, and is related to the trust region problem (Boyd and Vandenberghe, 2004;P\u00f3lik and Terlaky, 2007;Park and Boyd, 2017). Thus, the bounds on MCR presented in Sections 6.1 and 6.2 again become computationally tractable for the class of quadratically constrained linear models.\n\n\nUpper Bounding the Loss\n\nOne benefit of constraining the coefficient vector (\u03b2 M lm \u03b2 \u2264 r lm ) is that it facilitates determining an upper bound B ind on the loss function L(f \u03b2 , (y, x)) = (y \u2212 x \u03b2) 2 , which automatically satisfies Assumption 1 for all f \u2208 F lm,r lm . The following lemma gives sufficient conditions to determine B ind .\n\nLemma 17 (Loss upper bound for linear models) If M lm is positive definite, Y is bounded within a known range, and there exists a known constant r X such that x M \u22121 lm x \u2264 r X for all x \u2208 (X 1 \u00d7 X 2 ), then Assumption 1 holds for the model class F lm,r lm , the squared error loss function, and the constant\nB ind = max min y\u2208Y (y) \u2212 \u221a r X r lm 2 , max y\u2208Y (y) + \u221a r X r lm 2 .\nIn practice, the constant r X can be approximated by the empirical distribution of X and Y . The motivation behind the restriction x M \u22121 lm x \u2264 r X in Lemma 17 is to create complementary constraints on X and \u03b2. For example, if M lm is diagonal, then the smallest elements of M lm correspond to directions along which \u03b2 is least restricted by \u03b2 M lm \u03b2 \u2264 r lm (Eq 7.5), as well as the directions along which x is most restricted by x M \u22121 lm x \u2264 r X (Lemma 17).\n\n\nRegression Models in a Reproducing Kernel Hilbert Space (RKHS)\n\nWe now expand our scope of model classes by considering regression models in a reproducing kernel Hilbert space (RKHS), which allow for nonlinear and nonadditive features of the covariates. We show that, as in Section 7.3, minimizing a linear combination of\u00ea orig (f ) and e switch (f ) across models f in this class can be expressed as a QP1QC, which allows us to implement the binary search procedure of Sections 6.1 & 6.2.\n\nFirst we introduce notation required to describe regression in a RKHS. Let D be a (R\u00d7p) matrix representing a pre-specified dictionary of R reference points, such that each row of D is contained in X = R p . Let k be a pre-specified positive definite kernel function, and let \u00b5 be a prespecified estimate of EY . Let K D be the R\u00d7R matrix with K D[i,j] = k(D [i,\u00b7] , D [j,\u00b7] ). We consider prediction models of the following form, where the distance to each reference point is used as a regression feature:\nF D,r k = f \u03b1 : f \u03b1 (x) = \u00b5 + R i=1 k x, D [i,\u00b7] \u03b1 [i] , f \u03b1 k \u2264 r k , \u03b1 \u2208 R R . (7.6)\nAbove, the norm f \u03b1 k is defined as\nf \u03b1 k := R i=1 R j=1 \u03b1 [i] \u03b1 [j] k(D [i,\u00b7] , D [j,\u00b7] ) = \u03b1 K D \u03b1. (7.7)\nIn the next two sections, we show that bounds on empirical MCR can again be tractably computed for this class, and that the loss for models in this class can be feasibly upper bounded.\n\n\nCalculating MCR\n\nAgain, calculating bounds on M R from Lemmas 9 & 13 requires us to be able to minimize arbitrary linear combinations of\u00ea orig (f \u03b1 ) and\u00ea switch (f \u03b1 ).\n\nGiven a size-n sample of test observations Z = y X , let K orig be the n \u00d7 R ma-\ntrix with elements K orig[i,j] = k X [i,\u00b7] , D [j,\u00b7] . Let Z switch = y switch X switch be the (n(n \u2212 1)) \u00d7 (1 + p) matrix with rows that contain the set {(y [i] , X 1[j,\u00b7] , X 2[i,\u00b7] ) : i, j \u2208 {1, . . . , n} and i = j}. Finally, let K switch be the n(n \u2212 1) \u00d7 R matrix with K switch[i,j] = k X switch[i,\u00b7] , D [j,\u00b7] .\nFor any two constants \u03be orig , \u03be switch \u2208 R, we can show that minimizing the linear combination \u03be orig\u00eaorig (f \u03b1 ) + \u03be switch\u00easwitch (f \u03b1 ) over F D,r k is equivalent to the minimization problem minimize \u03be orig n y \u2212 \u00b5 \u2212 K orig \u03b1 2 2 + \u03be switch n(n \u2212 1) y switch \u2212 \u00b5 \u2212 K switch \u03b1 2 2 (7.8) subject to \u03b1 K D \u03b1 < r k . (7.9) Like Problem 7.5, Problem 7.8-7.9 is a QP1QC. To show Eqs 7.8-7.9, we first writ\u00ea e orig (f \u03b1 ) as\u00ea\norig (f \u03b1 ) = 1 n n i=1 y [i] \u2212 f \u03b1 (X [i,\u00b7] ) 2 (7.10) = 1 n n i=1 \uf8f1 \uf8f2 \uf8f3 y [i] \u2212 \u00b5 \u2212 R j=1 k(X [i,\u00b7] , D [j,\u00b7] )\u03b1 [j] \uf8fc \uf8fd \uf8fe 2 = 1 n n i=1 y [i] \u2212 \u00b5 \u2212 K orig[i,\u00b7] \u03b1 2 = 1 n y \u2212 \u00b5 \u2212 K orig \u03b1 2 2 . (7.11)\nFollowing similar steps, we can obtain\ne switch (f \u03b1 ) = 1 n(n \u2212 1) y switch \u2212 \u00b5 \u2212 K switch \u03b1 2 2 .\nThus, for any two constants \u03be orig , \u03be switch \u2208 R, we can see that \u03be orig\u00eaorig (f \u03b1 )+\u03be switch\u00easwitch (f \u03b1 ) is quadratic in \u03b1. This means that we can tractably compute bounds on empirical MCR for this class as well.\n\n\nUpper Bounding the Loss\n\nUsing similar steps as in Section 7.3.2, the following lemma gives sufficient conditions to determine B ind for the case of regression in a RKHS.\n\nLemma 18 (Loss upper bound for regression in a RKHS) Assume that Y is bounded within a known range, and there exists a known constant r D such that v( \u00b7] ). Under these conditions, Assumption 1 holds for the model class F D,r k , the squared error loss function, and the constant\nx) K \u22121 D v(x) \u2264 r D for all x \u2208 (X 1 \u00d7X 2 ), where v : R p \u2192 R R is the function satisfying v(x) [i] = k(x, D [i,B ind = max min y\u2208Y (y) \u2212 (\u00b5 + \u221a r D r k ) 2 , max y\u2208Y (y) + (\u00b5 + \u221a r D r k ) 2 .\nThus, for regression models in a RKHS, we can satisfy Assumption 1 for all models in the class.\n\n\nConnections Between MR and Causality\n\nOur MR approach can be fundamentally described as studying how a model's behavior changes under an intervention on the underlying data. We aim to study the causal effect of this intervention on the model's performance. This goal mirror's the conventional causal inference goal of studying how an intervention on variables will change outcomes generated by a process in nature.\n\nThis section explores this connection to causal inference further. Section 8.1 shows that when the prediction model in question is the conditional expectation function from nature itself, MR reduces to commonly studied quantities in the causal literature. Section 8.2 proposes an alternative to MR that focuses on interventions, or data perturbations, that are likely to occur in the underlying data generating process.\n\n\nModel Reliance and Causal Effects\n\nIn this section, we show a connection between population-level model reliance and the conditional average causal effect. For consistency with the causal inference literature, we temporarily rename the random variables (Y, X 1 , X 2 ) as (Y, T, C), with realizations (y, t, c). Here, T := X 1 represents a binary treatment indicator, C := X 2 represents a set of baseline covariates (\"C\" is for \"covariates\"), and Y represents an outcome of interest. Under this notation, e orig (f ) represents the expected loss of a prediction function f , and e switch (f ) denotes the expected loss in a pair of observations in which the treatment has been switched.\n\nLet f 0 (t, c) := E(Y |C = c, T = t) be the (unknown) conditional expectation function for Y , where we place no restrictions on the functional form of f 0 .\n\nLet Y 1 and Y 0 be potential outcomes under treatment and control respectively, such that Y = Y 0 (1 \u2212 T ) + Y 1 T . The treatment effect for an individual is defined as Y 1 \u2212 Y 0 , and the average treatment effect is defined as\nE(Y 1 \u2212 Y 0 ). Let CATE(c) := E(Y 1 \u2212 Y 0 |C = c)\nbe the (unknown) conditional average treatment effect of T for all patients with C = c. Causal inference methods typically assume (Y 1 , Y 0 ) \u22a5 T |C (conditional ignorability), and 0 < P(T = 1|C = c) < 1 for all values of c (positivity), in order for f 0 and CATE to be well defined and identifiable.\n\nThe next proposition quantifies the relation between the conditional average treatment effect function (CATE) and the model reliance of f 0 on X 1 .\n\nProposition 19 (Causal interpretations of MR) For any prediction model f , let e orig (f ) and e switch (f ) be defined based on the squared error loss L(f, (y, t, c)) := (y \u2212 f (t, c)) 2 .\n\nIf (Y 1 , Y 0 ) \u22a5 T |C (conditional ignorability) and 0 < P(T = 1|C = c) < 1 for all values of c (positivity), then M R(f 0 ) is equal to\n1 + Var(T ) E T,C Var(Y |T, C) t\u2208{0,1} E(Y 1 \u2212 Y 0 |T = t) 2 + Var (CATE(C)|T = t) , (8.1)\nwhere Var(T ) is the marginal variance of the treatment assignment.\n\nWe see above that model reliance decomposes into several terms that are each individually important in causal inference: the treatment prevalence (via Var(T )); the variability in Y that is not explained by C or T ; the magnitude of the average treatment effect, conditional on T ; and the variance of the conditional average treatment effect across subgroups. For example, if all patients are treated, then scrambling the treatment in a random pair of observations has no effect on the loss. In this case we see that Var(T ) = 0 and M R(f 0 ) = 1, indicating no reliance. When Var(T ) > 0, a higher average treatment effect magnitude (E(Y 1 \u2212 Y 0 |T = t) 2 ) corresponds to f 0 requiring T more heavily to predict Y , all else equal. Similarly, if there is a high degree of treatment effect heterogeneity across subgroups (that is, when Var (CATE(C)|T = t) is large), the model f 0 will again use T more heavily when predicting Y . For example, a treatment may be important for predicting Y even if the average treatment effect is zero, so long as the treatment helps some subgroups more than others.\n\n8.2 Conditional Importance: Adjusting for Dependence Between X 1 and X 2 One common scenario where multiple models achieve low loss is when the sets of predictors X 1 and X 2 are highly correlated, or contain redundant information. Models may predict well either through reliance on X 1 , or through reliance on X 2 , and so MCR will correctly identify a wide range of potential reliances on X 1 . However, we may specifically be interested how much models rely on the information in X 1 that cannot alternatively be gleaned from X 2 .\n\nFor example, age and accumulated wealth may be correlated, and both may be predictive of future promotion. We may wish to know the how much a model for predicting promotion relies on information that is uniquely available from wealth measurements.\n\nTo formalize this notion, we define an alternative to e switch where noise is added to X 1 in a way that accounts for the dependence between X 1 and X 2 . Given a fixed prediction model f , we ask: how well would the model f perform if the values of X 1 were scrambled across observations with the same value for X 2 . Specifically, let Z (a) = (Y (a) , X\n(a) 1 , X (a) 2 ) and Z (b) = (Y (b) , X (b) 1 , X (b)\n2 ) denote a pair of independent random vectors following the same distribution as Z = (Y, X 1 , X 2 ), as in Section 3, and let\ne cond (f ) := E X 2 E (Y (b) ,X (a) 1 ,X (b) 2 ) L{f, (Y (b) , X (a) 1 , X (b) 2 )}|X (a) 2 = X (b) 2 = X 2 . (8.2)\nIn words, e cond (f ) is the expected loss of a given model f across pairs of observations (Z (a) , Z (b) ) in which the values of X 1 have been switched, given that these pairs match on X 2 . This quantity can also be interpreted as the expected loss of f if noise were added to X 1 in such a way that X 1 was no longer informative of Y , given X 2 , but that the joint distribution of the covariates (X 1 , X 2 ) was maintained.\n\nWe then define conditional model reliance, or \"core\" model reliance (CMR) for a fixed function f as\nCM R(f ) = e cond (f ) e orig (f ) .\nThat is, CMR is the factor by which the model's performance degrades when the information unique to X 1 is removed. If X 1 \u22a5 X 2 , then X 1 contains no redundant information, and CMR and MR are equivalent. Otherwise, all else equal, CMR will decrease as X 2 becomes more predictive of X 1 . Analogous to MCR, we define conditional MCR (CMCR) in the same way as in Eq 2.2, but with MR replaced with CMR. In comparison with MCR, CMCR will generally result in a range that is closer to 1 (null reliance). An advantage of CMR is that it restricts the \"noise-corrupted\" inputs to be within the domain X , rather than the expanded domain X 1 \u00d7 X 2 considered by MR. This means that CMR will not be influenced by impossible combinations of x 1 and x 2 , while MR may be influenced by them. Hooker (2007) discuss a similar issue, arguing that evaluations of a prediction model's behavior in different circumstances should be weighted by, for example, how likely those circumstances are to occur.\n\nA challenge facing the CMR approach is that matched pairs such as those in Eq 8.2 may occur rarely, making it difficult to estimate CMR nonparametrically. We explore this estimation issue next.\n\n\nEstimation of CMR by Weighting, Matching, or Imputation\n\nIf the covariate space is discrete and low dimensional, nonparametric methods based on weighting or matching can be effective means of estimating CMR. Specifically, we can weight each pair of sample points i, j according to how likely the covariate combination (X 1[i,\u00b7] , X 2[j,\u00b7] ) is to occur, as in\ne weight (f ) := 1 n(n \u2212 1) n i=1 j =i w(X 1[i,\u00b7] , X 2[j,\u00b7] ) \u00d7 L{f, (y [j] , X 1[i,\u00b7] , X 2[j,\u00b7] )},\nwhere w(x 1 , x 2 ) := P(X 1 =x 1 |X 2 =x 2 ) P(X 1 =x 1 ) is an importance weight (see also Hooker, 2007). Here, pairs of observations corresponding to unlikely or impossible combinations of covariates are down-weighted or discarded, respectively. If the probabilities P(X 1 = x 1 |X 2 = x 2 ) and P(X 1 = x 1 ) are known, then\u00ea weight (f ) is unbiased for e cond (f ) (see Appendix A.7).\n\nAlternatively, if X 2 is discrete and low dimensional, we can restrict estimates of e cond (f ) to only consider pairs of sample observations in which X 2 is constant, or \"matched,\" as in\ne match (f ) := 1 n(n \u2212 1) n i=1 j =i 1(X 2[j,\u00b7] = X 2[i,\u00b7] ) P(X 2 = X 2[i,\u00b7] ) \u00d7 L{f, (y [j] , X 1[i,\u00b7] , X 2[j,\u00b7] )}. (8.3)\nThis approach allows estimation of CMR without knowledge of the conditional distribution P(X 1 = x 1 |X 2 = x 2 ). If the inverse probability weight P(X 2 = X 2[i,\u00b7] ) \u22121 is known, then e match (f ) is unbiased for e cond (f ) (see Appendix A.7). The weight P(X 2 = X 2[i,\u00b7] ) \u22121 accounts for the fact that, for any given value x 2 , the proportion of observations of X 2 taking the value x 2 will generally not be the same as the proportion of matched pairs (X (a) 2 , X\n\n2 ) taking value the x 2 , and so simply summing over all matched pairs would lead to bias. In practice, the proportion P(X 2 = X 2[i,\u00b7] ) can be approximated as 1 n\u22121 j =i 1(X 2[i,\u00b7] = X 2[j ,\u00b7] ), with minor adjustments to Eq 8.3 to avoid dividing by zero. The resulting estimate is analogous to exact matching procedures commonly used in causal inference, which are known to work best when the covariates are discrete and low dimensional, in order for exact matches to be common (Stuart, 2010).\n\nHowever, when the covariate space is continuous or high dimensional, we typically cannot estimate CMR nonparametrically. For such cases, we propose to estimate CMR under an assumption of homogeneous residuals. Specifically, we define \u00b5 1 to be the conditional expectation function \u00b5 1 (x 2 ) = E(X 1 |X 2 = x 2 ), and assume that the random residual X \u2212 \u00b5 1 (X 2 ) is independent of X 2 . Under this assumption, it can be shown that\ne cond (f ) = EL f, (Y (b) , X (a) 1 \u2212 \u00b5 1 (X (a) 2 ) + \u00b5 1 (X (b) 2 ), X (b) 2 ) .\nThat is, e cond (f ) is equal to the expected loss of f across random pairs of observations (Z (a) , Z (b) ) in which the value of the residual terms (in curly braces) have been switched. Because of the independence assumption, no matching or weighting is required. If \u00b5 1 is known, then we can again produce an unbiased estimate using the U-statisti\u0109\ne impute (f ) := 1 n(n \u2212 1) n i=1 j =i L f, (y [j] , X 1[i,\u00b7] \u2212 \u00b5 1 (X 2[i,\u00b7] ) + \u00b5 1 (X 2[j,\u00b7] ), X 2[j,\u00b7] ) .\nThis estimator aggregates over all pairs in our sample, switching the values of the residual terms (in curly braces) within each pair. In practice, when \u00b5 1 is not known, an estimate of \u00b5 1 can be achieved via regression or related machine learning techniques, and plugged in to the above equation. In this way, the assumption that X \u2212 \u00b5 1 (X 2 ) \u22a5 X 2 allows us to estimate CMR without explicitly modeling the joint distribution of X 1 and X 2 .\n\nIn the existing literature, Strobl et al. (2008) introduce a similar procedure for estimating conditional variable importance. However, a formal comparison to Strobl et al. is complicated by the fact that the authors do not define a specific estimand, and that their approach is limited to tree-based regression models. Other existing approaches conditional importance approaches include methods for redefining X 1 and X 2 to induce approximate independence, before computing an importance measure analogous to MR. This can be done by reducing the total number of covariates used, and hence reducing how well any one variable can be predicted by the others (as in Gregorutti et al., 2017). Alternatively, variables in X 2 that are predictive of X 1 can be regrouped directly into X 1 (as in Tolo\u015fi and Lengauer, 2011; see also the discussion from Kirk, Lewin and Stumpf, in Meinshausen and B\u00fchlmann 2010).\n\nIn summary, CMR allows us to see how much a model relies on the information uniquely available in X 1 . While CMR is more difficult to estimate than MR, several tractable approaches exist when X 2 is discrete, or when a homogenous residual assumption can be applied. One may also consider extending CMR by conditioning only on a subset of X 2 . For example, we may consider conditioning only on elements of X 2 that are believed to causally effect X 1 , by changing the outer expectation in Eq 8.2. For simplicity, we focus on the base case of estimating MR in this paper. Similar results could potentially be carried over for CMR as well.\n\n\nSimulations\n\nIn this section, we first present a toy example to illustrate the concepts of MR, MCR, and AR. We then present a Monte Carlo simulation studying the effectiveness of bootstrap CIs for MCR.\n\n\nIllustrative Toy Example with Simulated Data\n\nTo illustrate the concepts of MR, MCR, and AR (see Section 3.2), we consider a toy example where X = (X 1 , X 2 ) \u2208 R 2 , and Y \u2208 {\u22121, 1} is a binary group label. Our primary goal in this section is to build intuition for the differences between these three importance measures, and so we demonstrate them here only in a single sample. We focus on the empirical versions of our importance metrics ( M R, M CR \u2212 and M CR + ), and compare them against AR, which is typically interpreted as an in-sample measure , or as an intermediate step to estimate an alternate importance measure in terms of variable rankings (Gevrey et al., 2003;Olden et al., 2004).\n\nWe simulate X|Y = \u22121 from an independent, bivariate normal distribution with means E(X 1 |Y = \u22121) = E(X 2 |Y = \u22121) = 0 and variances Var(X 1 |Y = \u22121) = Var(X 2 |Y = \u22121) = 1 9 . We simulate X|Y = 1 by drawing from the same bivariate normal distribution, and then adding the value of a random vector (C 1 , C 2 ) := (cos(U ), sin(U )), where U is a random variable uniformly distributed on the interval [\u2212\u03c0, \u03c0]. Thus, (C 1 , C 2 ) is uniformly distributed across the unit circle.\n\nGiven a prediction model f : X \u2192 R, we use the sign of f (X 1 , X 2 ) as our prediction of Y . For our loss function, we use the hinge loss L(f, (y, x 1 , x 2 )) = (1 \u2212 yf (x 1 , x 2 )) + , where (a) + = a if a \u2265 0 and (a) + = 0 otherwise. The hinge loss function is commonly used as a convex approximation to the zero-one loss L(f, (y, x 1 , x 2 )) = 1[y = sign{f (x 1 , x 2 )}].\n\nWe simulate two samples of size 300 from the data generating process described above, one to be used for training, and one to be used for testing. Then, for the class of models used to predict Y , we consider the set of degree-3 polynomial classifiers\nF d3 = f \u03b8 : f \u03b8 (x 1 , x 2 ) = \u03b8 [1] + \u03b8 [2] x 1 + \u03b8 [3] x 2 + \u03b8 [4] x 2 1 + \u03b8 [5] x 2 2 + \u03b8 [6] x 1 x 2 +\u03b8 [7] x 3 1 + +\u03b8 [8] x 3 2 + \u03b8 [9] x 2 1 x 2 + \u03b8 [10] x 1 x 2 2 ; \u03b8 [\u22121] 2 2 \u2264 r d3 ,\nwhere \u03b8 [\u22121] denotes all elements of \u03b8 except \u03b8 [1] , and where we set r d3 to the value that minimizes the 10-fold cross-validated loss in the training data. Let A d3 be the algorithm that minimizes the hinge loss over the (convex) feasible region {f \u03b8 : \u03b8 [\u22121] 2 2 \u2264 r d3 }. We apply A d3 to the training data to determine a reference model f ref . Also using the training data, we set equal to 0.10 multiplied by the cross-validated loss of A d3 , such that R( , f ref , F d3 ) contains all models in F d3 that exceed the loss of f ref by no more than approximately 10% (see Eq 4.1). We then calculate empirical AR, MR, and MCR using the test observations. We begin by considering the AR of A d3 on X 1 . Calculating AR requires us to fit two separate models, first using all of the variables to fit a model on the training data, and then again using only X 2 . In this case, the first model is equivalent to f ref . We denote the second model asf 2 . To compute AR, we evaluate f ref andf 2 in the test observations. We illustrate this AR computation in Figure 6-A, marking the classification boundaries for f ref andf 2 by the black dotted line and the blue dashed lines respectively, and marking the test observations by labelled points (\"x\" for Y = 1, and \"o\" for Y = \u22121). Comparing the loss associated with these two models gives one form of AR-an estimate of the necessity of X 1 for the algorithm A d3 . Alternatively, to estimate the sufficiency of X 1 , we can compare the reference model f ref against the model resulting from retraining algorithm A d3 only using X 1 . We refer to this third model asf 1 , and mark its classification boundary by the solid blue lines in Figure 6-A.\n\nEach of the classifiers in Figure 6-A can also be evaluated for its reliance on X 1 , as shown in Figure 6-C. Here, we use\u00ea divide in our calculation of M R (see Eq 3.5). Unsurprisingly, the classifier fit without using X 1 (blue dashed line) has a model reliance of M R(f 2 ) = 1.\n\nThe reference model f ref (dotted black line) has a model reliance of M R(f ref ) = 3.47. Each M R value has an interpretation contained to a single model. That is, M R compares a single model's behavior under different data distributions, rather than the AR approach of comparing different models' behavior on marginal distributions from a single joint distribution.\n\nWe illustrate MCR in Figure 6-B. In contrast to AR, MCR is only ever a function of well-performing prediction models. Here, we consider the empirical -Rashomon set R( , f ref , F d3 ), the subset of models in F d3 with test loss no more than above that of f ref .\n\nWe show the classification boundary associated with 15 well-performing models contained inR( , f ref , F d3 ) by the gray solid lines. We also show two of the models inR( , f ref , F d3 ) that approximately maximize and minimize empirical reliance on X 1 among models in R( , f ref , F d3 ). We denote these models asf +, andf \u2212, , and mark them by the solid green and dashed green lines respectively. For every model shown in Figure 6-B, we also mark its model reliance in Figure 6 In summary, unlike AR, MCR is only a function of models that fit the data well. show the same 300 draws from a simulated data set, with the classification of each data point marked by \"x\" for Y = 1, and \"o\" for Y = \u22121. In Panel (A), for AR, we show single-feature models formed by dropping a covariate. Because these models take only a single input, we represent their classification boundaries as straight lines. In Panel (B), for MCR, we show the classification boundaries for several (two-feature) models with low in-sample loss. Of these models, the model with minimal dependence on X 1 is shown by the dashed green oval, and the model with maximal dependence on X 1 is shown by the solid green oval. Panel (C) shows the empirical model reliance on X 1 for each of the models in Panels (A) & (B). We see in Panel (C) that, as expected, no well-performing model relies (empirically) on X 1 more thanf +, does, or relies (empirically) on X 1 less than f \u2212, does. That is, no well-performing model has an empirical MR value greater than M CR + ( ), or less than M CR \u2212 ( ).\n\n\nSimulations of Bootstrap Confidence Intervals\n\nIn this section we study the performance of MCR under model class misspecification. Our goal will be to estimate how much the conditional expectation function f 0 ( Condition 20 ensures that the interval [M CR \u2212 ( ), M CR + ( )] contains M R(f 0 ), and Condition 21 ensures that this interval can be estimated in finite samples. Condition 20 can also be interpreted as saying that the model reliance value of M R(f c ) is \"well supported\" by the class F, even if F does not contain f 0 . Our primary goal is to assess whether CIs derived from MCR can give appropriate coverage of M R(f 0 ), which depends on both conditions. As a secondary goal, we also would like to be able to assess Conditions 20 & 21 individually.\n\nVerifying the above conditions requires that we are able to calculate population-level MCR. To this end, we draw samples with replacement from a finite population of 20,000 observations, in which MCR can also be calculated directly. To derive a CI based on MCR, we divide each simulated sample Z s into a training subset and analysis subset. We use the training subset to fit a reference model f ref,s , which is required for our definition of population-level MCR. We calculate a bootstrap CI by drawing 500 bootstrap samples from the analysis subset, and computing M CR \u2212 ( ) and M CR + ( ) in each bootstrap sample by optimizing overR( , f ref,s , F). We then take the 2.5% percentile of M CR \u2212 ( ) values across bootstrap samples, and the 97.5% percentile of M CR + ( ) values across bootstrap samples, as the lower and upper endpoints of our CI, respectively. We repeat this procedure for both X 1 and X 2 .\n\nWe generate data according to a model with increasing amounts of nonlinearity. For \u03b3 \u2208 {0, 0.1, 0.2, 0.3, 0.4, 0.5}, we simulate continuous outcomes as\nY = f 0 (X) + E, where f 0 is the function f 0 (x) = p j=1 jx [j] \u2212 \u03b3x 2 [j]\n; the covariate dimension p is equal to 2, with X 1 and X 2 defined as the first and second elements of X; the covariates X are drawn from a multivariate normal distribution with E(X 1 ) = E(X 2 ) = 0, Var(X 1 ) = Var(X 2 ) = 0, and Cov(X 1 , X 2 ) = 1/4; and E is a normally distributed noise variable with mean zero and variance equal to \u03c3 2 E := Var(f 0 (X)). We consider sample sizes of n = 400 and 800, of which n tr = 200 or 300 observations are assigned to the training subset respectively.\n\nTo implement our approach, we use the model class j+1] , \u03b2 \u2208 R 3 }. We set the performance threshold equal to 0.1 \u00d7 \u03c3 2 E . We refer to this MCR implementation with F lm as \"MCR-Linear.\"\nF lm = {f \u03b2 : f \u03b2 (x) = \u03b2 [1] + 2 j=1 x [j] \u03b2 [\nAs a comparator method, we consider a simpler bootstrap approach, which we refer to as \"Standard-Linear.\" Here, we take 500 bootstrap samples from the simulated data Z s . In each bootstrap sample, indexed by b, we set aside n tr training points to train a model f b \u2208 F lm , and calculate M R(f b ) from the remaining data points. We then create a 95% bootstrap percentile CI for M R(f 0 ) by taking the 2.5% and 97.5% percentiles of M R(f b ) across b = 1, . . . , 500.\n\n\nResults\n\nOverall, we find that MCR provides more robust and conservative intervals for the reliance of f 0 on X 1 and X 2 , relative to standard bootstrap approaches. We also find that higher sample size generally exacerbates coverage errors due to misspecification, as methods become more certain of biased results. Method q Standard MCR Robustness of MCR to misspecification due to nonlinearity Figure 7: MR Coverage -The y-axis shows coverage rate for the reliance of f 0 on either X 1 (left column) or X 2 (right column), where X 2 is simulated to be more influential that X 1 . The x-axis shows increasing levels of misspecification (\u03b3). All methods aim to have at least 95% coverage for each scenario (dashed horizontal line).\n\n\nMCR-\n\nThe increased robustness of MCR comes at the cost of wider CIs. Intervals for MCR-Linear were typically larger than intervals for Standard-Linear by a factor of approximately 2-4. This is partly due to the fact that CIs for MCR are meant to cover the range of values [M CR \u2212 ( ), M CR + ( )] (defined using f ref,s ), rather than to cover a single point.\n\nWhen investigating Conditions 20 & 21 individually, we find that the coverage errors for MCR-Linear were largely attributable to violations of Condition 20. Condition 21 appears to hold conservatively for all scenarios studied-within each scenario, at least 95.9% of bootstrap CIs contained population-level MCR.\n\nThese simulation results highlight an aspect of MCR that is both a strength and a weakness: MCR is generic. MCR does not assume a particular means by which misspecification may occur, and is less powerful than sensitivity analyses which make that assumption correctly. Nonetheless, MCR still appears to add robustness. For sufficiently strong signals, an informative interval may still be returned. In our applied data analysis, below, we see that this is indeed the case.\n\n\nData Analysis: Reliance of Criminal Recidivism Prediction Models on Race and Sex\n\nEvidence suggests that bias exists among judges and prosecutors in the criminal justice system (Spohn, 2000;Blair et al., 2004;Paternoster and Brame, 2008). In an aim to counter this bias, machine learning models trained to predict recidivism are increasingly being used to inform judges' decisions on pretrial release, sentencing, and parole (Monahan and Skeem, 2016;Picard-Fritsche et al., 2017). Ideally, prediction models can avoid human bias and provide judges with empirically tested tools. But prediction models can also mirror the biases of the society that generates their training data, and perpetuate the same bias at scale. In the case of recidivism, if arrest rates across demographic groups are not representative of underlying crime rate (Beckett et al., 2006;Ramchand et al., 2006; U.S. Department of Justice -Civil Rights Devision, 2016), then bias can be created in both (1) the outcome variable, future crime, which is measured imperfectly via arrests or convictions, and (2) the covariates, which include the number of prior convictions on a defendant's record (Corbett-Davies et al., 2016;Lum and Isaac, 2016). Further, when a prediction model's behavior and mechanisms are an opaque black box, the model can evade scrutiny, and fail to offer recourse or explanations to individuals rated as \"high risk.\" We focus here on the issue of transparency, which takes an important role in the recent debate about the proprietary recidivism prediction tool COMPAS (Larson et al., 2016;Corbett-Davies et al., 2016). While COMPAS is known to not rely explicitly on race, there is concern that it may rely implicitly on race via proxies-variables statistically dependent with race (see further discussion in Section 11).\n\nOur goal is to identify bounds for how much COMPAS relies on different covariate subsets, either implicitly or explicitly, under certain assumptions (defined below). We analyze a public data set of defendants from Broward County, Florida, in which COMPAS scores have been recorded (Larson et al., 2016). Within this data set, we only included defendants measured as African-American or Caucasian (3,373 in total) due to sparseness in the remaining categories. The outcome of interest (Y ) is the COMPAS violent recidivism score. Of the available covariates, we consider three variables which we refer to as \"admissible\": an individual's age, their number of priors, and an indicator of whether the current charge is a felony. We also consider two variables which we refer to as \"inadmissible\": an individual's race and sex. Our labels of \"admissible\" and \"inadmissible\" are not intended to be legally precise-indeed, the boundary between these types of labels is not always clear (see Section 10.2). We compute empirical MCR and AR for each variable group, as well as bootstrap CIs for MCR (see Section 9.2).\n\nTo compute empirical MCR and AR, we consider a flexible class of linear models in a RKHS to predict the COMPAS score (described in more detail below). Given this class, the MCR range (See Eq 2.2) captures the highest and lowest degree to which any model in the class may rely on each covariate subset. We assume that our class contains at least one model that relies on \"inadmissible variables\" to the same extent that COMPAS relies either on \"inadmissible variables\" or on proxies that are unmeasured in our sample (analogous to Condition 20). We make the same assumption for \"admissible variables.\" These assumptions can be interpreted as saying that the reliance values of COMPAS are relatively \"well supported\" by our chosen model class, and allows us to identify bounds on the MR values for COMPAS. We also consider the more conventional, but less robust approach of AR (Section 3.2), that is, how much would the accuracy suffer for a modelfitting algorithm trained on COMPAS score if a variable subset was removed?\n\nThese computations require that we predefine our loss function, model class, and performance threshold. We define MR, MCR, and AR in terms of the squared error loss L(f, (y, x 1 , x 2 )) = {y \u2212 f (x 1 , x 2 )} 2 . We define our model class F D,r k in the form of Eq 7.6, where we determine D, \u00b5, k, and r k based on a subset S of 500 training observations. We set D equal to the matrix of covariates from S; we set \u00b5 equal to the mean of Y in S; we set k equal to the radial basis function k \u03c3s (x,x) = exp \u2212 x\u2212x 2 2\u03c3s , where we choose \u03c3 s to minimize the cross-validated loss of a Nadaraya-Watson kernel regression (Hastie et al., 2009) fit to S; and we select the parameters r k by cross-validation on S. We set equal to 0.1 times the cross-validated loss on S. Also using S, we train a reference model f ref \u2208 F D,r k . Using the held-out 2,873 observations, we then estimate M R(f ref ) and MCR for F D,r k . To calculate AR, we train models from F D,r k using S, and evaluate their performance in the held-out observations.\n\n\nResults\n\nOur results imply that race and sex play somewhere between a null role and a modest role in determining COMPAS score, but that they are less important than \"admissible\" factors ( Figure 8). As a benchmark for comparison, the empirical MR of f ref is equal to 1.09 for \"inadmissible variables,\" and 2.78 for \"admissible variables.\" The AR is equal to 0.94 and 1.87 for \"inadmissible\" and \"admissible\" variables respectively, roughly in agreement with MR. The MCR range for \"inadmissible variables\" is equal to [1.00,1.56], indicating that for any model in F D,r k with empirical loss no more than above that of f ref , the model's loss can increase by no more than 56% if race and sex are permuted. Such a statement cannot be made solely based on AR or MR methods, as these methods do not upper bound the reliance values of well-performing models. The bootstrap 95% CI for MCR on \"inadmissible variables\" is [1.00, 1.73]. Thus, under our assumptions, if COMPAS relied on sex, race, or their unmeasured proxies by a factor greater than 1.73, then intervals as low as what we observe would occur with probability < 0.05.\n\nFor \"admissible variables\" the MCR range is equal to [1.77,3.61], with a 95% bootstrap CI of [1.62, 3.96]. Under our assumptions, this implies if COMPAS relied on age, number of priors, felony indication, or their unmeasured proxies by a factor lower than 1.77, then intervals as high as what we observe would occur with probability < 0.05. This result is consistent with , who find age to be highly predictive of COMPAS score.\n\nIt is worth noting that the upper limit of 3.61 maximizes empirical MR on \"admissible variables\" not only among well-performing models, but globally across all models in the class (see Figure 8, and Eq 6.5). In other words, it is not possible to find models in F D,r k that perform arbitrarily poorly on perturbed data, but still perform well on unperturbed data, and so the ratio of\u00ea switch (f ) to\u00ea orig (f ) has a finite upper bound. Because the regularization constraints of F D,r k preclude MR values higher than 3.61, the MR of COMPAS on \"admissible variables\" may be underestimated by empirical MCR. Note also that both MCR intervals are left-truncated at 1, as it is often sufficiently precise to conclude that there exists a well-performing model with no reliance on the variables of interest (that is, MR equal to 1; see Appendix A.2).\n\n\nDiscussion & Limitations\n\nAsking whether a proprietary model relies on sex and race, after adjusting for other covariates, is related to the fairness metric known as conditional statistical parity (CSP). A decision rule satisfies CSP if its decisions are independent of a sensitive variable, conditional To this end, for each covariate subset, we compute conservative boundary functions (shown as solid lines, or \"bowls\") guaranteed to contain all models in the class (see Section 6). Specifically, all models in f \u2208 F D,r k are guaranteed to have an empirical loss (\u00ea stnd (f )) and empirical MR value ( M R(f )) for \"inadmissible variables\" corresponding to a point within the gray bowl. Likewise, all models in F D,r k are guaranteed to have an empirical loss and empirical MR value for \"admissible variables\" corresponding to a point within the blue bowl. Points shown as \"\u00d7\" represent additional models in F D,r k discovered during our computational procedure, and thus show where the \"bowl\" boundary is tight. The goal of our computation procedure (see Section 6) is to tighten the boundary as much as possible near the value of interest, shown by the dashed horizontal line above. This dashed line has a y-intercept equal to the loss of the reference model plus the value of interest. Bootstrap CIs for M CR \u2212 ( ) and M CR + ( ) are marked by brackets. on a set of \"legitimate\" covariates C (Corbett-Davies et al., 2017; see also Kamiran et al., 2013). Roughly speaking, CSP reflects the idea that groups of people with similar covariates C are treated similarly (Dwork et al., 2012), regardless of the sensitive variable (for example, race or sex). However, the criteria becomes superficial if too many variables are included in C, and care should be taken to avoid including proxies for the sensitive variables. Several other fairness metrics have also been proposed, which often form competing objectives (Kleinberg et al., 2017;Chouldechova, 2017;Nabi and Shpitser, 2018;Corbett-Davies et al., 2017). Here, if COMPAS was not influenced by race, sex, or variables related to race or sex (conditional on a set of \"legitimate\" variables), it would satisfy CSP. Unfortunately, it is often difficult to distinguish between \"legitimate\" (or \"admissible\") variables and \"illegitimate\" variables. Some variables function both as part of a reasonable predictor for risk, and, separately, as a proxy for race. Because of disproportional arrest rates, particularly for misdemeanors and drug-related offenses (U.S. Department of Justice -Civil Rights Devision, 2016; Lum and Isaac, 2016), prior misdemeanor convictions may act as such a proxy (Corbett-Davies et al., 2016;Lum and Isaac, 2016).\n\nProxy variables for race (defined as being statistically dependent with race) that are unmeasured in our sample are also not the only reason that race could be predictive of COMPAS score. Other inputs to the COMPAS algorithm might be associated with race only conditionally on variables we categorize as \"admissible.\" However, our result from Section 10.1 that race has limited predictive utility for COMPAS score suggests that such conditional relationships are also limited.\n\n\nConclusion\n\nIn this article, we propose MCR as the upper and lower limit on how important a set of variables can be to any well-performing model in a class. In this way, MCR provides a more comprehensive and robust measure of importance than traditional importance measures for a single model. We derive bounds on MCR, which motivate our choice of point estimates. We also derive connections between permutation importance, U-statistics, conditional variable importance, and conditional causal effects. We apply MCR in a data set of criminal recidivism, in order to help inform the characteristics of the proprietary model COMPAS.\n\nSeveral exciting areas remain open for future research. One research direction closely related to our current work is the development of exact or approximate MCR computation procedures for other model classes and loss functions. We have shown that, for model classes where minimizing the empirical loss is a convex optimization problem, MCR can be conservatively computed via a series of convex optimization problems. Further, we have shown that computing M CR \u2212 is often no more challenging that minimizing the empirical loss over a reweighted sample. General computation procedures for MCR are still an open research area.\n\nAnother direction is to consider MCR for variable selection. If M CR + is small for a variable, then no well-performing predictive model can heavily depend on that variable, indicating that it can be eliminated.\n\nOur theoretical analysis of Rashomon sets depends on F and f ref being prespecified. Above, we have actualized this by splitting our sample into subsets of size n 1 and n 2 , using the first subset to determine F and f ref , and conditioning on F and f ref when estimating MCR in the second subset. As a result, the boundedness constants in our assumptions (B ind , B ref , B switch , and b orig ) depend on F, and hence on n 1 . However, because our results are non-asymptotic, we have not explored how Rashomon sets behave when n 1 and n 2 grow at different rates. An exciting future extension of this work is to study sequences of triples { n 1 , f ref,n 1 , F n 1 } that change as n 1 increases, and the corresponding Rashomon sets R( n 1 , f ref,n 1 , F n 1 ), as this may more thoroughly capture how model classes are determined by analysts.\n\nWhile we develop Rashomon sets with the goal of studying MR, Rashomon sets can also be useful for finite sample inferences about a wide variety of other attributes of best-in-class models (for example, Section 5). Characterizations of a Rashomon set itself may also be of interest. For example, in ongoing work, we are studying the size of a Rashomon set, and its connection to generalization of models and model classes (Semenova and Rudin, 2019). We are additionally developing methods for visualizing Rashomon sets (Dong and Rudin, 2019).\n\nsuch that f \u03b8 silly is equivalent to f silly , and f \u03b8 is the best-in-class model. If f \u03b8 satisfies M R(f \u03b8 ) > 1 and if the model reliance function M R is continuous in \u03b8, then there exists a parameter value \u03b8 1 between \u03b8 silly and \u03b8 such that M R(f \u03b8 1 ) = 1. Further, if the loss function L is convex in \u03b8, then e orig (f \u03b8 ) \u2264 e orig (f \u03b8 1 ) \u2264 e orig (f silly ), and any population -Rashomon set containing f silly will also contain f \u03b8 1 .\n\nA.3 Relating\u00ea switch (f ) to All Possible Permutations of the Sample Following the notation in Section 3, let {\u03c0 1 , . . . , \u03c0 n! } be a set of n-length vectors, each containing a different permutation of the set {1, . . . , n}. We show in this section that e switch (f ) is equal to the product of\nn! l=1 n i=1 L{f, (y [i] , X 1[\u03c0 l[i] ,\u00b7] , X 2[i,\u00b7] )}1(\u03c0 l[i] = i), (A.1)\nand a proportionality constant that is only a function of n. First, consider the sum\nn! l=1 n i=1 L{f, (y [i] , X 1[\u03c0 l[i] ,\u00b7] , X 2[i,\u00b7] )}, (A.2)\nwhich omits the indicator function found in Eq A.1. The summation in Eq A.2 contains n(n!) terms, each of which is a two-way combination of the form L{f, (y [i] , X 1[j,\u00b7] , X 2[i,\u00b7] )} for i, j \u2208 {1, . . . n}. There are only n 2 unique combinations of this form, and each must occur in at least (n \u2212 1)! of the n(n!) terms in Eq A.2. To see this, consider selecting two integer values\u0129,j \u2208 {1, . . . n}, and enumerating all occurrences of the term L{f, (y [\u0129] , X 1[j,\u00b7] , X 2[\u0129,\u00b7] )} within the sum in Eq A.2. Of the permutation vectors {\u03c0 1 , . . . , \u03c0 n! }, we know that (n\u22121)! of them place\u0129 in thej th position, i.e., that satisfy \u03c0 l[\u0129] =j. For each such permutation \u03c0 l , the inner summation in Eq A.2 over all possible values of i must include the term L{f, (y [\u0129] , X 1[\u03c0 l[\u0129] ,\u00b7] , X 2[\u0129,\u00b7] )} = L{f, (y [\u0129] , X 1[j,\u00b7] , X 2[\u0129,\u00b7] )}. Thus, Eq A.2 contains at least (n \u2212 1)! occurrences of the term L{f, (y [\u0129] , X 1[j,\u00b7] , X 2[\u0129,\u00b7] )}. So far, we have shown that each unique combination occurs at least (n \u2212 1)! times, but it also follows that each unique combination must occur precisely (n \u2212 1)! times. This is because each of the n 2 unique combinations must occur at least (n \u2212 1)! times, which accounts for n 2 ((n \u2212 1)!) = n(n!) terms in total. As noted above, Eq has A.2 has only n(n)! terms, so there can be no additional terms. We can then simplify Eq A.2 as\nn! l=1 n i=1 L{f, (y [i] , X 1[\u03c0 l[i] ,\u00b7] , X 2[i,\u00b7] )} = (n \u2212 1)! n i=1 n j=1 L{f, (y [i] , X 1[j,\u00b7] , X 2[i,\u00b7] )}.\nBy the same logic, we can simplify Eq A.1 as\nn! l=1 n i=1 L{f, (y [i] , X 1[\u03c0 l[i] ,\u00b7] , X 2[i,\u00b7] )}1(\u03c0 l[i] = i) =(n \u2212 1)! \uf8f1 \uf8f2 \uf8f3 n i=1 n j=1 L{f, (y [i] , X 1[j,\u00b7] , X 2[i,\u00b7] )}1(j = i) \uf8fc \uf8fd \uf8fe =(n \u2212 1)! n i=1 j =i L{f, (y [i] , X 1[j,\u00b7] , X 2[i,\u00b7] )}, (A.3)\nand Line A.3 is proportional to\u00ea switch (f ) up to a function of n.\n\n\nA.4 Bound for MR of the Best-in-class Prediction Model\n\nAlthough describing individual models is not the primary focus of this work, a corollary of Theorem 4 is that we can create a probabilistic bound for the reliance of the (unknown) best-in-class model f on X 1 .\n\nCorollary 22 (Bound on Best-in-class MR) Let f \u2208 arg min f \u2208F e orig (f ) be a prediction model that attains the lowest possible expected loss, and let f +, and f \u2212, be defined as in Theorem 4. If f +, and f \u2212, satisfy Assumptions 1, 2 and 3, then\nP M R(f ) \u2208 M CR \u2212 ( best ) \u2212 Q best , M CR + ( best ) + Q best \u2265 1 \u2212 \u03b4,\nwhere best := 2B ref\nlog(6\u03b4 \u22121 ) 2n\n, and Q best :\n= B switch b orig \u2212 B switch \u2212B ind log(12\u03b4 \u22121 ) n b orig +B ind log(12\u03b4 \u22121 ) 2n .\nThe above result does not require that f be unique. If several models achieve the minimum possible expected loss, the above boundaries apply simultaneously for each of them. In the special case when the true conditional expectation function E(Y |X 1 , X 2 ) is equal to f , then we have a boundary for the reliance of the function E(Y |X 1 , X 2 ) on X 1 . This reliance bound can also be translated into a causal statement using Proposition 19.\n\n\nA.5 Ratios versus Differences in MR Definition\n\nWe choose our ratio-based definition of model reliance, M R(f ) = e switch (f ) e orig (f ) , so that the measure can be comparable across problems, regardless of the scale of Y . However, several existing works define VI measures in terms of differences (Strobl et al., 2008;Datta et al., 2016;Gregorutti et al., 2017), analogous to\nM R difference (f ) := e switch (f ) \u2212 e orig (f ). (A.4)\nWhile this difference measure is less readily interpretable, it has several computational advantages. The mean, variance, and asymptotic distribution of the estimator M R difference (f ) := e switch (f ) \u2212\u00ea orig (f ) can be easily determined using results for U-statistics, without the use of the delta method (Dorfman, 1938;Lehmann and Casella, 2006; see also Ver Hoef, 2012).\n\nEstimates in the form of M R difference (f ) will also be more stable when min f \u2208F e orig (f ) is small, relative to estimates for the ratio-based definition of MR. To improve interpretability, we may also normalize M R difference (f ) by dividing by the variance of Y , which can be easily estimated without the use of models, as in Williamson et al. (2017).\n\nUnder the difference-based definition for MR (Eq A.4), the results from Theorem 4, Theorem 6, and Corollary 22 will still hold under the following modified definitions of Q out , Q in , and Q best :\nQ out,difference := 1 + 1 \u221a 2 B ind log(6\u03b4 \u22121 ) n , Q in,difference :=B ind \uf8f1 \uf8f2 \uf8f3 log(8\u03b4 \u22121 N F, r \u221a 2 ) n + log(8\u03b4 \u22121 N (F, r)) 2n \uf8fc \uf8fd \uf8fe + 2r( \u221a 2 + 1), and Q best,difference := 1 + 1 \u221a 2 B ind log(12\u03b4 \u22121 ) n .\nRespectively replacing Q out , Q in , Q best , M R, and M R with Q out,difference , Q in,difference , Q best,difference , M R difference and M R difference entails only minor changes to the corresponding proofs (see Appendices B.3,B.5,and B.4). The results will also hold without Assumption 3, as is suggested by the fact that b orig and B switch do not appear in Q out,difference , Q in,difference , or Q best,difference .\n\nWe also prove an analogous version of Theorem 5, on uniform bounds for M R difference , in Appendix B.5.1.\n\n\nA.6 Rashomon Sets and Profile Likelihood Intervals\n\nWe note in Section 5.1 that, under certain conditions, the CIs returned from Proposition 7 take the same form as profile likelihood CIs (Coker et al., 2018). For completeness, we briefly review this connection. We assume here that models f \u03b8 \u2208 F are indexed by a finite dimensional parameter vector \u03b8 \u2208 \u0398, where \u03b8 = (\u03b3, \u03c8) contains a 1-dimensional parameter of interest \u03b3 \u2208 R 1 , and a nuisance parameter \u03c8 \u2208 \u03a8. We further assume and that e orig (f \u03b8 ) is minimized by a unique parameter value \u03b8 = (\u03b3 , \u03c8 ) \u2208 \u0398, and that our goal is to learn about \u03b3 .\n\nIf s \u03b8 := Z exp{\u2212L(f \u03b8 , z)}dz is finite for all \u03b8 \u2208 \u0398, we can convert L into the likelihood function L : (Z \u00d7 \u0398) \u2192 R 1 satisfying L(z; \u03b8) = exp{\u2212L(f \u03b8 , z)}/s \u03b8 . As an abbreviation, let L(Z; \u03b8) denote n i=1 L(Z [i,\u00b7] ; \u03b8). Additionally, let\u03b8 := arg min \u03b8\u2208\u0398\u00eaorig (f \u03b8 ) be the empirical loss minimizer, and hence the maximum likelihood estimator of \u03b8 . If L is indeed the correct likelihood function, then \u03b8 = (\u03b3 , \u03c8 ) corresponds to the true parameter vector. Further, if \u03c6(f \u03b8 ) = \u03c6(f (\u03b3,\u03c8) ) = \u03b3 returns the parameter element of interest (\u03b3), then the (1 \u2212 \u03b4)-level profile likelihood interval for \u03c6(f \u03b8 ) = \u03b3 is PLI(\u03b4) := \u03b3 : log L(Z;\u03b8) \u2212 log L(Z;\u03b8 \u03b3 ) \u2264 \u03c7 1,1\u2212\u03b4 2 , where\u03b8 \u03b3 = arg max\n{\u03b8\u2208\u0398 : \u03c6(f \u03b8 )=\u03b3} L(Z; \u03b8)\n= \u03b3 : \u2203\u03b8 \u03b3 satisfying \u03c6 f\u03b8 \u03b3 = \u03b3 and log L(Z;\u03b8) \u2212 log L(Z;\u03b8 \u03b3 ) \u2264 \u03c7 1,1\u2212\u03b4 2 = \u03b3 : \u2203\u03b8 \u03b3 satisfying \u03c6 f\u03b8 \u03b3 = \u03b3 and\u00ea orig f\u03b8 \u03b3 \u2264\u00ea orig f\u03b8 + \u03c7 1,1\u2212\u03b4 2n = \u03b3 : \u2203f\u03b8 \u03b3 satisfying \u03c6 f\u03b8 \u03b3 = \u03b3 and f\u03b8 \u03b3 \u2208R\n\u03c7 1,1\u2212\u03b4 2n , f\u03b8, F (A.5)\nwhere \u03c7 1,1\u2212\u03b4 is the 1 \u2212 \u03b4 percentile of a chi-square distribution with 1 degree of freedom. If PLI(\u03b1) is indeed a contiguous interval, then maximizing and minimizing \u03c6(f \u03b8 ) across models f \u03b8 in the empirical Rashomon set in Eq A.5 yields the same interval.\n\n\nA.7 Unbiased Estimates of CMR\n\nWe claim in Section 8.2 that bot\u0125\ne match (f ) = 1 n(n \u2212 1) n i=1 j =i 1(X 2[j,\u00b7] = X 2[i,\u00b7] ) P(X 2 = X 2[i,\u00b7] ) \u00d7 L{f, (y [j] , X 1[i,\u00b7] , X 2[j,\u00b7] )}.\nand\u00ea\nweight (f ) = 1 n(n \u2212 1) n i=1 j =i P(X 1 = X 1[i,\u00b7] |X 2 = X 2[j,\u00b7] ) P(X 1 = X 1[i,\u00b7] ) \u00d7 L{f, (y [j] , X 1[i,\u00b7] , X 2[j,\u00b7] )}, are unbiased for e cond (f ) = E X 2 E L{f, (Y (b) , X (a) 1 , X (b) 2 )}|X (a) 2 = X (b) 2 , X 2 .\nTo show that\u00ea match (f ) is unbiased, we first note that each summation term in\u00ea match (f ) has the same expectation. Following the notation in Section 3, let Z (a) = (Y (a) , X\n(a) 1 , X (a) 2 ) and Z (b) = (Y (b) , X (b) 1 , X (b)\n2 ) be independent random variables following the same distribution as Z = (Y, X 1 , X 2 ). The expectation of\u00ea match (f ) is\nE\u00ea match (f ) =E 1(X (a) 2 = X (b) 2 ) p x 2 (X (a) 2 ) \u00d7 L{f, (Y (b) , X (a) 1 , X (b) 2 )} =E X (a) 2 E 1(X (a) 2 = X (b) 2 ) p x 2 (X (a) 2 ) \u00d7 L{f, (Y (b) , X (a) 1 , X (b) 2 )}|X (a) 2 =E X (a) 2 p x 2 (X (a) 2 )E 1 p x 2 (X (a) 2 ) \u00d7 L{f, (Y (b) , X (a) 1 , X (b) 2 )}|X (a) 2 = X (b) 2 , X (a) 2 + 0 =E X (a) 2 E L{f, (Y (b) , X(a)\n1 , X\n\n2 )}|X\n(a) 2 = X (b) 2 , X (a) 2\n=e cond (f ).\n\nTo show that\u00ea weight (f ) is unbiased, we similarly note that each summation term in\u00ea weight (f ) has the same expectation. Without loss of generality, we show the result for discrete variables (Y, X 1 , X 2 ). Let Y x 2 be the domain of Y conditional on the event that X 2 = x 2 . The expectation of\u00ea weight (f ) is\nE\u00ea weight (f ) = x (b) 2 \u2208X 2 y (b) \u2208Y x (b) 2 x (a) 1 \u2208X 1 L{f, (y (b) , x (a) 1 , x (b) 2 )} P(X 1 = x (a) 1 |X 2 = x (b) 2 ) P(X 1 = x (a) 1 ) \u00d7P(X 1 = x (a) 1 )P(Y = y (b) , X 2 = x (b) 2 ) \uf8f9 \uf8fb = x (b) 2 \u2208X 2 P(X 2 = x (b) 2 ) y (b) \u2208Y x (b) 2 x (a) 1 \u2208X 1 L{f, (y (b) , x (a) 1 , x (b) 2 )} \u00d7P(X 1 = x (a) 1 |X 2 = x (b) 2 )P(Y = y (b) |X 2 = x (b) 2 ) = E X (b) 2 E L{f, (Y (b) , X (a) 1 , X (b) 2 )}|X (a) 2 = X (b) 2 , X (b) 2 )\n= e cond (f ).\n\n\nAppendix B. Proofs for Statistical Results\n\nWe present proofs for our statistical results in this section, and conclude by presenting proofs for our computational results in Appendix C.\n\n\nB.1 Lemma Relating Empirical and Population Rashomon Sets\n\nThroughout the remaining proofs, it will be useful to express the definition of population -Rashomon sets in terms of the expectation of a single loss function, rather than a comparison of two loss functions. To do this, we simply introduce the \"standardized\" loss functionL, defined asL\n(f, z) := L(f, z) \u2212 L(f ref , z). (B.1)\nAbove, recall from Section 2 that L(f, z) denotes L(f, (y, x 1 , x 2 )) for z = (y, x 1 , x 2 ). Because we assume f ref is prespecified and fixed, we omit notation for f ref in the definition ofL. We can now write\nR( ) = {f ref } \u222a {f \u2208 F : EL(f, Z) \u2264 EL(f ref , Z) + } = {f ref } \u222a f \u2208 F : EL(f, Z) \u2264 ,\nand, similarly,R\n( ) = {f ref } \u222a f \u2208 F :\u00caL(f, Z) \u2264 .\nWith this definition, the following lemma allows us to limit the probability that a given model f 1 \u2208 R( ) is excluded from an empirical Rashomon set.\n\nLemma 23 For \u2208 R and \u03b4 \u2208 (0, 1), let 1 := + 2B ref\nlog(\u03b4 \u22121 ) 2n\n, and let f 1 \u2208 R( ) denote a specific, possibly unknown prediction model. If f 1 satisfies Assumption 2, then\nP{f 1 \u2208R( 1 )} \u2265 1 \u2212 \u03b4.\nProof If f ref and f 1 are the same function, then the result holds trivially. Otherwise, the proof follows from Hoeffding's inequality (Theorem 2 of Hoeffding, 1963). First, note that if f 1 satisfies Assumption 2, thenL(f 1 ) is bounded within an interval of length 2B ref .\n\nApplying this in line B.3, below, we see that \nP{f 1 / \u2208R( 1 )} = P \u00caL (f 1 , Z) > 1 from f 1 / \u2208 {f ref } = P \u00caL (f 1 , Z) \u2212 > 2B ref log (\u03b4 \u22121 ) 2n from definition of 1 (B.2) \u2264 P \u00caL (f 1 , Z) \u2212 EL(f 1 , Z) > 2B ref log (\u03b4 \u22121 ) 2n from EL(f 1 , Z) \u2264 \u2264 exp \uf8f1 \uf8f2 \uf8f3 \u2212 2n (2B ref ) 2 2B ref log (\u03b4 \u22121 ) 2n 2 \uf8fc \uf8fd \uf8fefrom\n\nB.2 Lemma to Transform Between Bounds\n\nThe following lemma will help us translate from bounds for variables to bounds for differences and ratios of those variables. We will apply this lemma to transform from bounds on empirical losses to bounds on empirical model reliance, defined either in terms of a ratio or in terms of a difference.\n\nLemma 24 Let X, Z, \u00b5 X , \u00b5 Z , k X , k Z \u2208 R be constants satisfying |Z \u2212 \u00b5 Z | \u2264 k Z and |X \u2212 \u00b5 X | \u2264 k X , then\n|(Z \u2212 X) \u2212 (\u00b5 Z \u2212 \u00b5 X )| \u2264 q difference (k Z , k X ), (B.5)\nwhere q difference is the function\nq difference (k Z , k X ) := k Z + k X . (B.6)\nFurther, if there exists constants b orig and B switch such that 0 < b orig \u2264 X, \u00b5 X and Z, \u00b5 Z \u2264 B switch < \u221e, then\nZ X \u2212 \u00b5 Z \u00b5 X \u2264 q ratio (k Z , k X ), (B.7)\nwhere q ratio is the function\nq ratio (k Z , k X ) := B switch b orig \u2212 B switch \u2212 k Z b orig + k X . (B.8)\nProof Showing Eq B.5,\n|(Z \u2212 X) \u2212 (\u00b5 Z \u2212 \u00b5 X )| \u2264 |Z \u2212 \u00b5 Z | + |\u00b5 X \u2212 X| \u2264 k Z + k X . Showing Eq B.7, let A Z = max(Z, \u00b5 Z ), a X = min(X, \u00b5 X ), d Z = |Z \u2212 \u00b5 Z |, and d X = |X \u2212 \u00b5 X |. This implies that max(X, \u00b5 X ) = a X + d X and min(Z, \u00b5 Z ) = A Z \u2212 d Z . Thus, Z X and \u00b5 Z \u00b5 X are both bounded within the interval min(Z, \u00b5 Z ) max(X, \u00b5 X ) , max(Z, \u00b5 Z ) min(X, \u00b5 X ) = A Z \u2212 d Z a X + d X , A Z a X , which implies Z X \u2212 \u00b5 Z \u00b5 X \u2264 A Z a X \u2212 A Z \u2212 d Z a X + d X . (B.9)\nTaking partial derivatives of the right-hand side, we get\n\u2202 \u2202a X A Z a X \u2212 A Z \u2212 d Z a X + d X = \u2212A Z a 2 X + A Z \u2212 d Z (a X + d X ) 2 \u2264 0, \u2202 \u2202A Z A Z a X \u2212 A Z \u2212 d Z a X + d X = 1 a X \u2212 1 a X + d X \u2265 0, \u2202 \u2202d X A Z a X \u2212 A Z \u2212 d Z a X + d X = A Z \u2212 d Z (a X + d X ) 2 > 0, and \u2202 \u2202d Z A Z a X \u2212 A Z \u2212 d Z a X + d X = 1 a X + d X > 0.\nSo the right-hand side of B.9 is maximized when d Z , d X , and A Z are maximized, and when a X is minimized. Thus, in the case where |Z \u2212 \u00b5 Z | \u2264 k Z ; |X \u2212 \u00b5 X | \u2264 k X ; 0 < b orig \u2264 X, \u00b5 X ; and Z, \u00b5 Z \u2264 B switch < \u221e, we have\nZ X \u2212 \u00b5 Z \u00b5 X \u2264 A Z a X \u2212 A Z \u2212 d Z a X + d X \u2264 B switch b orig \u2212 B switch \u2212 k Z b orig + k X .\n\nB.3 Proof of Theorem 4\n\nProof We proceed in 4 steps.\n\nB.3.1\n\nStep 1:\nShow that P M R(f +, ) \u2264 M CR + ( out ) \u2265 1 \u2212 \u03b4 3 .\n\nConsider the event that\n\nM When Eq B.10 holds we have,\nM R(f +, ) \u2264 M CR + ( out ) M R(f +, ) \u2264 M CR + ( out ) + {M R(f +, ) \u2212 M R(f +, )} M R(f +, ) \u2264 M CR + ( out ) + [M R(f +, ) \u2212 M R(f +, )]. (B.13) B.3.3\nStep 3: Probabilistically Bound the Error Term from\n\nStep 2.\n\nNext we show that the bracketed term in Line B.13 is less than or equal to Q out with high probability. For k \u2208 R, let q difference and q ratio be defined as in Eqs B.6 and B.8. Let q : R \u2192 R be the function such that q(k) = q ratio k, k \u221a 2 . Then\nQ out = B switch b orig \u2212 B switch \u2212 B ind log(6\u03b4 \u22121 ) n b orig + B ind log(6\u03b4 \u22121 ) 2n = q ratio B ind log(6\u03b4 \u22121 ) n , B ind log(6\u03b4 \u22121 ) 2n = q B ind log(6\u03b4 \u22121 ) n .\nApplying this relation below, we have In Line B.15, above, recall that\u00ea orig (f +, ) and\u00ea switch (f +, ) are both U-statistics. Note that E [\u00ea switch (f +, )] = e switch (f +, ) because\u00ea switch (f +, ) is an average of terms, and each term has expectation equal to e switch (f +, ). For the same reason, E [\u00ea orig (f +, )] = e orig (f +, ). This allows us to apply Eq 5.7 of Hoeffding, 1963 (see also Eq 1 on page 201 of Serfling, 1980, in Theorem A) to obtain Line B.15.\nP M R(f +, ) \u2212 M R(f +, ) > Q out (B.14) \u2264 P M R(f +, ) \u2212 M R(f +, ) > q B ind log(6\u03b4 \u22121 ) n \u2264 P |\u00ea orig (f +, ) \u2212 e orig (f +, )| > B ind log(6\u03b4 \u22121 ) 2n |\u00ea switch (f +, ) \u2212 e switch (f +, )| > B ind log(6\u03b4 \u22121 ) n from Lemma 24 \u2264 P |\u00ea orig (f +, ) \u2212 e orig (f +, )| > B ind log (6\u03b4 \u22121 ) 2n + P |\u00ea switch (f +, ) \u2212 e switch (f +, )| > B ind log (6\u03b4 \u22121 ) n from the Union bound \u2264 2 exp \uf8f1 \uf8f2 \uf8f3 \u2212 2n (B ind \u2212 0) 2 B ind log (6\u03b4 \u22121 ) 2n 2 \uf8fc \uf8fd \uf8fe + 2 exp \uf8f1 \uf8f2 \uf8f3 \u2212 n (B ind \u2212 0) 2 B ind log (6\u03b4 \u22121 ) n 2 \uf8fc \uf8fd \uf8fe\nAlternatively, if we instead define model reliance as M R difference (f ) = e switch (f )\u2212e orig (f ) (see Appendix A.5), define empirical model reliance as M R difference (f ) :=\u00ea switch (f ) \u2212 e orig (f ), and define\nQ out,difference := 1 + 1 \u221a 2 B ind log(6\u03b4 \u22121 ) n = q difference B ind log(6\u03b4 \u22121 ) n , B ind log(6\u03b4 \u22121 ) 2n ,\nthen the same proof holds without Assumption 3 if we replace M R, M R, Q out respectively with M R difference , M R difference , Q out,difference , and redefine q : R \u2192 R as the function q(k) = q difference k, k \u221a 2 . Eqs B.14-B.16 also hold if we replace\u00ea switch throughout with\u00ea divide , including in Assumption 3, since the same bound can be used for both\u00ea switch and\u00ea divide (Eq 5.7 of Hoeffding, 1963; see also Theorem A on page 201 of Serfling, 1980).\n\n\nB.3.4\n\nStep 4: Combine Results to Show Eq 4.2 Finally, we connect the above results to show Eq 4.2. We know from Eq B.12 that Eq B.10 holds with high probability. Eq B.10 implies Eq B.13, which bounds M CR + ( ) = M R(f +, ) up to a bracketed residual term. We also know from Eq B.16 that, with high probability, the residual term in Eq B.13 is less than Q out = q B ind log(6\u03b4 \u22121 ) n . Putting this together, we can show Eq 4.2:\nP M CR + ( ) > M CR + ( out ) + Q out = P M R(f +, ) > M CR + ( out ) + Q out \u2264 P M R(f +, ) > M CR + ( out ) M R(f +, ) \u2212 M R(f +, ) > Q out from Step 2 \u2264 P M R(f +, ) > M CR + ( out ) + P M R(f +, ) \u2212 M R(f +, ) > Q out \u2264 \u03b4 3 + 2\u03b4 3 = \u03b4. from Steps 1 & 3 (B.17)\nThis completes the proof for Eq 4.2. For Eq 4.3 we can use the same approach, shown below for completeness. Analogous to Eq B.12, we have\nP M R(f \u2212, ) < M CR \u2212 ( out ) \u2264 \u03b4 3 .\nAnalogous to Eq B.13, when M R\n(f \u2212, ) \u2265 M R(f \u2212, out ) we have M R(f \u2212, ) \u2265 M CR \u2212 ( out ) M R(f \u2212, ) \u2265 M CR \u2212 ( out ) + {M R(f \u2212, ) \u2212 M R(f \u2212, )} M R(f \u2212, ) \u2265 M CR \u2212 ( out ) \u2212 M R(f \u2212, ) \u2212 M R(f \u2212, ) .\nAnalogous to Eq B.16, we have\nP M R(f \u2212, ) \u2212 M R(f \u2212, ) > q B ind log(6\u03b4 \u22121 ) n \u2264 2\u03b4 3 . (B.18)\nFinally, analogous to Eq B.17, we have\nP M CR \u2212 ( ) < M CR \u2212 ( out ) \u2212 Q out \u2264 P M R(f \u2212, ) < M CR \u2212 ( out ) M R(f \u2212, ) \u2212 M R(f \u2212, ) > q B ind log(6\u03b4 \u22121 ) n (B.19) \u2264 P M R(f \u2212, ) < M CR \u2212 ( out ) + P M R(f \u2212, ) \u2212 M R(f \u2212, ) > q B ind log(6\u03b4 \u22121 ) n \u2264 \u03b4 3 + 2\u03b4 3 = \u03b4.\nAgain, the same proof holds without Assumption 3 if we replace M R, M R, Q out respectively with M R difference , M R difference , Q out,difference , and redefine q as the function satisfying\nq(k) = q difference k, k \u221a 2 in Eqs B.18 & B.19.\n\nB.4 Proof of Corollary 22\n\nProof By definition, M R(f \u2212, best ) \u2264 M R(f ) \u2264 M R(f +, best ). Applying this relation in Line B.20, below, we see\nP M R(f ) \u2208 M CR \u2212 ( best ) \u2212 Q best , M CR + ( best ) + Q best = 1 \u2212 P M R(f ) < M CR \u2212 ( best ) \u2212 Q best M R(f ) > M CR + ( best ) + Q best \u2265 1 \u2212 P M R(f ) < M CR \u2212 ( best ) \u2212 Q best \u2212P M R(f ) > M CR + ( best ) + Q best \u2265 1 \u2212 P M R(f \u2212, ) < M CR \u2212 ( best ) \u2212 Q best \u2212 P M R(f +, ) > M CR + ( best ) + Q best (B.20) \u2265 1 \u2212 \u03b4 2 \u2212 \u03b4 2 from Theorem 4. (B.21)\nTo apply Theorem 4 in Line B.21, above, we note that Q best and best are equivalent to the definitions of Q out and out in Theorem 4, but with \u03b4 replaced by \u03b4 2 . Alternatively, if we define model reliance as M R difference (f ) = e switch (f ) \u2212 e orig (f ) (see Appendix A.5), and define empirical model reliance as M R difference (f ) =\u00ea switch (f )\u2212\u00ea orig (f ), then let Q best,difference := 1 + 1 \u221a 2 B ind log(12\u03b4 \u22121 ) n .\n\nThe term Q best,difference is equivalent to Q out,difference but with \u03b4 replaced with \u03b4 2 . Under this difference-based definition of model reliance, Theorem 4 holds without Assumption 3 if we replace Q out with Q out,difference (see Section B.3), and so we can apply this altered version of Theorem 4 in Line B.21. Thus, Theorem 22 also holds without Assumption 3 if we replace M R, M R, and Q best respectively with M R difference , M R difference , and Q best,difference .\n\n\nB.5 Proof of Theorems 5 & 6\n\nWe begin by proving Theorem 5, along with related results. We then apply these results to show Theorem 6.\n\nB.5.1 Proof of Theorem 5, and Other Limits on Estimation Error, Based on Covering Number\n\nThe following theorem uses the covering number based on r-margin-expectation-covers to jointly bound empirical losses for any function f \u2208 F. Theorem 5 in the main text follows directly from Eq B.25, below.\n\nTheorem 25 If Assumptions 1, 2 and 3 hold for all f \u2208 F, then for any r > 0 (B.28) and q ratio and q difference are defined as in Lemma 24. For Eq B.26, the result is unaffected if we remove Assumption 3.\nP D sup f \u2208F |\u00ea orig (f ) \u2212 e orig (f )| > B ind log(2\u03b4 \u22121 N (F, r)) 2n + 2r \u2264 \u03b4, (B.22) P D sup f \u2208F \u00caL (f, Z) \u2212 EL(f, Z) > 2B ref log(2\u03b4 \u22121 N (F, r)) 2n + 2r \u2264 \u03b4, (B.23) P D sup f \u2208F |\u00ea switch (f ) \u2212 e switch (f )| > B ind log(2\u03b4 \u22121 N (F, r)) n + 2r \u2264 \u03b4, (B.24) P sup f \u2208F \u00ea orig (f ) e switch (f ) \u2212 e orig (f ) e switch (f ) > Q 4 \u2264 \u03b4, (B.25) P D sup f \u2208F |{\u00ea switch (f ) \u2212\u00ea orig (f )} \u2212 {e switch (f ) \u2212 e orig (f )}| > Q 4,difference \u2264 \u03b4, (B.26) where Q 4 := q ratio \uf8eb \uf8ed B ind log(4\u03b4 \u22121 N F, r \u221a 2 ) n + 2r \u221a 2, B ind log(4\u03b4 \u22121 N (F, r)) 2n + 2r \uf8f6 \uf8f8 , (B.27) Q 4,difference := q difference \uf8eb \uf8ed B ind log(4\u03b4 \u22121 N F, r \u221a 2 ) n + 2r \u221a 2, B ind log(4\u03b4 \u22121 N (F, r)) 2n + 2r \uf8f6 \uf8f8 ,\n\nB.5.2 Proof of Eq B.22\n\nProof Let G r be a r-margin-expectation-cover for F of size N (F, r). Let D p denote the population distribution, let D s be the sample distribution, and let D be the uniform mixture of D p and D s , i.e., for any z \u2208 Z,\nP D (Z \u2264 z) = 1 2 P Dp (Z \u2264 z) + 1 2 P Ds (Z \u2264 z). (B.29)\nUnless otherwise stated, we take expectations and probabilities with respect to D p . Since G r is a r-margin-expectation-cover, we know that for any f \u2208 F we can find a function g \u2208 G r such that\nE D |L(g, Z) \u2212 L(f, Z)| = E D L (g, Z) \u2212L(f, Z) \u2264 r, and \u00ca L(f, Z) \u2212 EL(f, Z) = \u00ca L(f, Z) \u2212 EL(f, Z) + \u00ca L(g, Z) \u2212\u00caL(g, Z) + {EL(g, Z) \u2212 EL(g, Z)} (B.30) \u2264 \u00ca L(g, Z) \u2212 EL(g, Z) + |EL(g, Z) \u2212 EL(f, Z)| + \u00ca L(f, Z) \u2212\u00caL(g, Z) \u2264 \u00ca L(g, Z) \u2212 EL(g, Z) + E Dp |L(g, Z) \u2212 L(f, Z)| + E Ds |L(f, Z) \u2212 L(g, Z)| = \u00ca L(g, Z) \u2212 EL(g, Z) + 2E D |L(g, Z) \u2212 L(f, Z)| \u2264 \u00ca L(g, Z) \u2212 EL(g, Z) + 2r.\nApplying the above relation in Line B.31 below, we have\nP sup f \u2208F \u00ca L(f, Z) \u2212 EL(f, Z) > B ind log(2\u03b4 \u22121 N (F, r)) 2n + 2r = P \u2203f \u2208 F : \u00ca L(f, Z) \u2212 EL(f, Z) > B ind log(2\u03b4 \u22121 N (F, r)) 2n + 2r\n\u2264 P \u2203g \u2208 G r : \u00ca L(g, Z) \u2212 EL(g, Z) + 2r > B ind log(2\u03b4 \u22121 N (F, r)) 2n + 2r To apply Hoeffding's inequality (Theorem 2 of Hoeffding, 1963) in Line B.32, above, we use the fact that L(g, Z) is bounded within an interval of length B ind . Proof Let F D denote the cumulative distribution function for a distribution D. LetD p be the distribution such that\n(B.31) = P \uf8eb \uf8ed g\u2208Gr \u00ca L(g, Z) \u2212 EL(g, Z) > B ind log(2\u03b4 \u22121 N (F, r)) 2n \uf8f6 \uf8f8 \u2264 g\u2208Gr P \u00ca L(g, Z) \u2212 EL(g, Z) > B ind log(2\u03b4 \u22121 N (F, r)) 2n from the Union bound \u2264 N (F, r) 2 exp \uf8ee \uf8f0 \u2212 2n (B ind ) 2 B ind log(2\u03b4 \u22121 N (F, r))FD p (Y = y, X 1 = x 1 , X 2 = x 2 ) = F Dp (Y = y, X 2 = x 2 )F Dp (X 1 = x 1 ). LetD s be the distribution satisfying PD s (Y = y, X 1 = x 1 , X 2 = x 2 ) = 1 n(n \u2212 1) n i=1 j =i 1(y [j] = y, X 1[i,\u00b7] = x 1 , X 2[j,\u00b7] = x 2 ).\nLetD be the uniform mixture ofD p andD s , as in Eq B.29. Replacing e orig ,\u00ea orig , D p , D s , and D respectively with e switch ,\u00ea switch ,D p ,D s , andD , we can follow the same steps as in the proof for Eq B.22. For any f \u2208 F, we know that there exists a function g \u2208 G r satisfying ED |L(g, Z) \u2212 L(f, Z)| \u2264 r, which implies\n|\u00ea switch (f ) \u2212 e switch (f )| = |\u00ea switch (f ) \u2212 e switch (f ) + {\u00ea switch (g) \u2212\u00ea switch (g)} + {e switch (g) \u2212 e switch (g)}| \u2264 |\u00ea switch (g) \u2212 e switch (g)| + ED p |L(g, Z) \u2212 L(f, Z)| + ED s |L(f, Z) \u2212 L(g, Z)| = |\u00ea switch (g) \u2212 e switch (g)| + 2ED |L(g, Z) \u2212 L(f, Z)| \u2264 |\u00ea switch (g) \u2212 e switch (g)| + 2r.\nAs a result,\nP sup f \u2208F |\u00ea switch (f ) \u2212 e switch (f )| > B ind log(2\u03b4 \u22121 N (F, r)) n + 2r \u2264 P \u2203g \u2208 G r : |\u00ea switch (g) \u2212 e switch (g)| + 2r > B ind log(2\u03b4 \u22121 N (F, r)) n + 2r \u2264 g\u2208Gr P |\u00ea switch (g) \u2212 e switch (g)| > B ind log(2\u03b4 \u22121 N (F, r)) n \u2264 N (F, r) 2 exp \uf8ee \uf8f0 \u2212 n (B ind \u2212 0) 2 B ind log(2\u03b4 \u22121 N (F, r)) n 2 \uf8f9 \uf8fb (B.34) = \u03b4.\nIn Line B.34, above, we apply Eq 5.7 of Hoeffding, 1963 \n\u00ea orig (f ) e switch (f ) \u2212 e orig (f ) e switch (f ) > Q 4 (B.35) = P \u2203f \u2208 F : \u00ea orig (f ) e switch (f ) \u2212 e orig (f ) e switch (f ) > Q 4 \u2264 P \u2203f \u2208 F : |\u00ea orig (f ) \u2212 e orig (f )| > B ind log(4\u03b4 \u22121 N (F, r)) 2n + 2r (B.36) \uf8f1 \uf8f2 \uf8f3 \u2203f \u2208 F : |\u00ea switch (f ) \u2212 e switch (f )| > B ind log(4\u03b4 \u22121 N F, r \u221a 2 ) n + 2r \u221a 2 \uf8fc \uf8fd \uf8fe \uf8f6 \uf8f8 = P sup f \u2208F |\u00ea orig (f ) \u2212 e orig (f )| > B ind log(4\u03b4 \u22121 N (F, r)) 2n + 2r + P \uf8eb \uf8ed sup f \u2208F |\u00ea switch (f ) \u2212 e switch (f )| > B ind log(4\u03b4 \u22121 N F, r \u221a 2 ) n + 2r \u221a 2 \uf8f6 \uf8f8 \u2264 \u03b4 2 + \u03b4 2 . from|{\u00ea switch (f ) \u2212\u00ea orig (f )} \u2212 {e switch (f ) \u2212 e orig (f )}| > Q 4,difference \u2264 P \u2203f \u2208 F : |\u00ea orig (f ) \u2212 e orig (f )| > B ind log(4\u03b4 \u22121 N (F, r)) 2n + 2r \uf8f1 \uf8f2 \uf8f3 \u2203f \u2208 F : |\u00ea switch (f ) \u2212 e switch (f )| > B ind log(4\u03b4 \u22121 N F, r \u221a 2 ) n + 2r \u221a 2 \uf8fc \uf8fd \uf8fe \uf8f6 \uf8f8 \u2264 \u03b4 2 + \u03b4 2 .\nB.5.7 Implementing Theorem 25 to Show Theorem 6\n\nProof Consider the event that\n\u2203f +, in \u2208 arg max f \u2208R( in ) M R(f ) such that M CR + ( ) < M R f +, in . (B.38)\nA brief outline of our proof for Eq 4.6 is as follows. We expect Eq B.38 to be unlikely due to the fact that in < . If Eq B.38 does not hold, then the only way that M CR + ( ) < M CR + ( in ) \u2212 Q in holds is if there existsf +, in \u2208 arg max f \u2208R( in ) M R(f ) which has an empirical MR that differs from its population-level MR by at least Q in .\n\nTo show that Eq B.38 is unlikely, we apply Theorem 25:\nP \u2203f +, in \u2208 arg max f \u2208R( in ) M R(f ) : M CR + ( ) < M R f +, in \u2264 P \u2203f \u2208R( in ) : M CR + ( ) < M R (f ) = P \u2203f \u2208R( in )\\f ref : M CR + ( ) < M R (f ) by M CR + ( ) \u2265 M R(f ref ) \u2264 P \u2203f \u2208R( in )\\f ref : EL(f, Z) > by M CR + ( ) Def = P \u2203f \u2208 F, EL(f, Z) > :\u00caL(f, Z) \u2264 in byR( ) Def = P \u2203f \u2208 F, EL(f, Z) > : EL(f, Z) \u2212 \u2264 \u22122B ref log(4\u03b4 \u22121 N (F, r)) 2n \u2212 2r by in Def (B.39) \u2264 P \u2203f \u2208 F, EL(f, Z) > : EL(f, Z) \u2212 EL(f, Z) \u2264 \u22122B ref log(4\u03b4 \u22121 N (F, r)) 2n \u2212 2r \u2264 P sup f \u2208F |\u00caL(f, Z) \u2212 EL(f, Z)| \u2265 2B ref log(4\u03b4 \u22121 N (F, r)) 2n + 2r = \u03b4 2 by Thm 25. (B.40) Q in,difference = B ind \uf8f1 \uf8f2 \uf8f3 log(8\u03b4 \u22121 N F, r \u221a 2 ) n + log(8\u03b4 \u22121 N (F, r)) 2n \uf8fc \uf8fd \uf8fe + 2r( \u221a 2 + 1) = q difference \uf8eb \uf8ed B ind log(8\u03b4 \u22121 N F, r \u221a 2 ) n + 2r \u221a 2, B ind log(8\u03b4 \u22121 N (F, r)) 2n + 2r \uf8f6 \uf8f8 ,\nthen same proof of Eq 4.6 holds without Assumption 3 if we replace Q in with Q in,difference , and apply Eq B.26 in Eq B.43.\n\nFor Eq 4.7 we can use the same approach. Consider the event that\n\u2203f \u2212, in \u2208 arg min f \u2208R( in ) M R(f ) : M CR \u2212 ( ) > M R f \u2212, in . (B.44)\nFinally, analogous to Eq B.43,\nP M CR \u2212 ( ) > M CR ( in ) + Q in \u2264 P \u2203f \u2212, in \u2208 arg min f \u2208R( in ) M R(f ) : M CR \u2212 ( ) > M R f \u2212, in sup f \u2208F | M R(f ) \u2212 M R(f )| > Q in \u2264 P \u2203f \u2212, in \u2208 arg min f \u2208R( in ) M R(f ) : M CR \u2212 ( ) > M R f \u2212, in +P sup f \u2208F | M R(f ) \u2212 M R(f )| > Q in = \u03b4 2 + \u03b4 2 . (B.45)\nUnder the difference-based definition of model reliance (see Appendix A.5), the same proof for Eq 4.7 holds without Assumption 3 if we replace M R, M R, & Q in respectively with M R difference , M R difference , & Q in,difference , and apply Eq B.26 in Eq B.45.\n\nB.6 Proof of Proposition 7, and Corollary for a Unique Best-in-class Model.\n\nWe first introduce a lemma to describe the performance of any individual model in the population -Rashomon set. , and let the functions\u03c6 \u2212 and\u03c6 + be defined as in Proposition 7. Given a function f 1 \u2208 R( ), if Assumption 2 holds for f 1 , then\nP \u03c6(f 1 ) \u2208 \u03c6 \u2212 ( 1 ),\u03c6 + ( 1 ) \u2265 1 \u2212 \u03b4.\nProof Consider the event that\n\u03c6(f 1 ) \u2208 \u03c6 \u2212 ( 1 ),\u03c6 + ( 1 ) .\n(B.46) Eq B.46 will always hold if f 1 \u2208R( 1 ), since the interval \u03c6 \u2212 ( 1 ),\u03c6 + ( 1 ) contains \u03c6(f ) for any f \u2208R( 1 ) by definition. Thus,\nP \u03c6(f 1 ) / \u2208 [ \u03c6 \u2212 ( 1 ),\u03c6 + ( 1 ) \u2264 P f 1 / \u2208R( 1 )\n\u2264 \u03b4 from Lemma 23.\n\n\nB.6.1 Proof of Proposition 7\n\nProof Let f \u2212, ,\u03c6 \u2208 arg min f \u2208R( ) \u03c6(f ) and f +, ,\u03c6 \u2208 arg max f \u2208R( ) \u03c6(f ) respectively denote functions that attain the lowest and highest values of \u03c6(f ) among models f \u2208 R ( ).\n\nApplying the definitions of f \u2212, ,\u03c6 and f +, ,\u03c6 in Line B.47, below, we have \nP {\u03c6(f ) : f \u2208 R ( )} \u2282 [\u03c6 \u2212 ( ),\u03c6 + ( )] = P [\u03c6(f \u2212, ,\u03c6 ), \u03c6(f +, ,\u03c6 )] \u2282 [\u03c6 \u2212 ( ),\u03c6 + ( )] (B.47) = P \u03c6(f \u2212, ,\u03c6 ) / \u2208 [\u03c6 \u2212 ( ),\u03c6 + ( )] \u03c6(f +, ,\u03c6 ) / \u2208 [\u03c6 \u2212 ( ),\u03c6 + ( )] \u2264 P \u03c6(f \u2212, ,\u03c6 ) / \u2208 [\u03c6 \u2212 ( ),\u03c6 + ( )] + P \u03c6(f +, ,\u03c6 ) / \u2208 [\u03c6 \u2212 ( ),\u03c6 + ( )] \u2264 \u03b4 2 + \u03b4 2 = \u03b4 from\n\nB.6.2 Corollary for a Unique Best-in-Class Model\n\nWhen the best-in-class model is unique, it can be described by the corollary below.\nCorollary 27 Let\u03c6 \u2212 ( 0 ) := min f \u2208R( 1 ) \u03c6(f ) and\u03c6 + ( 1 ) := max f \u2208R( 1 ) \u03c6(f ), where 0 := 2B ref log(\u03b4 \u22121 ) 2n\n. Let f \u2208 arg min f \u2208F e orig (f ) be the prediction model that uniquely attains the lowest possible expected loss. If f satisfies Assumption 2, then\nP{\u03c6(f ) \u2208 [\u03c6 \u2212 ( 1 ),\u03c6 + ( 1 )]} \u2265 1 \u2212 \u03b4.\nProof Since f \u2208 R(0), Corollary 27 follows immediately from Lemma 26.\n\nNotice that by assuming f is unique, we can use the threshold 0 := 2B ref\nlog(\u03b4 \u22121 ) 2n\n, which is lower than the threshold of = + 2B ref log(2\u03b4 \u22121 ) 2n with = 0, as in Proposition 7. In this way, assuming uniqueness allows a stronger statement than the one in Proposition 7.\n\n\nB.7 Absolute Losses versus Relative Losses in the Definition of the Rashomon Set\n\nIn this paper we primarily define Rashomon sets as the models that perform well relative to a reference model f ref . We can also study an alternate formulation of Rashomon sets by replacing the relative lossL with the non-standardized loss L throughout. This results in a new interpretation of the Rashomon set R( abs , f ref ,\nF) = {f ref }\u222a{f \u2208 F : EL(f, Z) \u2264 abs }\nas the union of f ref and the subset of models with absolute loss L no higher than abs , for abs > 0. The process of computing empirical MCR is largely unaffected by whether L or L is used, as it is simple to transform from one optimization problem to the other.\n\nWe still require the explicit inclusion of f ref in empirical and population Rashomon sets to ensure that they are nonempty. However, in many cases, this inclusion becomes redundant when interpreting a Rashomon set (e.g., when \u2265 0, and EL(f ref , Z) \u2264 abs ).\n\nUnder the replacement ofL with L, we also replace Assumption 2 with Assumption 1 (whenever this is not redundant), and replace 2B ref with B ind in the definitions of out , best , in , and 1 in Theorem 4, Corollary 22, Theorem 6, Proposition 7, and Corollary 27. This is because the motivation for the 2B ref term is thatL(f 1 ) is bounded within an interval of length 2B ref when f 1 satisfies Assumption 2. However, under Assumption 1, L(f 1 ) is bounded within an interval of length B ind .\n\n\nB.8 Proof of Proposition 15\n\nProof To show Eq 7.1 we start with e orig (f \u03b2 ),\ne orig (f \u03b2 ) = E[{Y \u2212 X 1 \u03b2 1 \u2212 X 2 \u03b2 2 } 2 ] = E[{(Y \u2212 X 2 \u03b2 2 ) \u2212 X 1 \u03b2 1 } 2 ] = E[(Y \u2212 X 2 \u03b2 2 ) 2 ] \u2212 2E[(Y \u2212 X 2 \u03b2 2 )X 1 ]\u03b2 1 + \u03b2 1 E[X 1 X 1 ]\u03b2 1 .\nFor e switch (f \u03b2 ), we can follow the same steps as above:\ne switch (f \u03b2 ) = E Y (b) ,X (a) 1 ,X (b) 2 [{Y (b) \u2212 X (a) 1 \u03b2 1 \u2212 X (b) 2 \u03b2 2 } 2 ] = E[(Y (b) \u2212 X (b) 2 \u03b2 2 ) 2 ] \u2212 2E[Y (b) \u2212 X (b) 2 \u03b2 2 ]E[X (a) 1 ]\u03b2 1 + \u03b2 1 E[X (a) 1 X (a) 1 ]\u03b2 1 . Since (Y (b) , X (b) 1 , X (b) 2 ) and (Y (a) , X (a) 1 , X(a)\n2 ) each have the same distribution as (Y, X 1 , X 2 ), we can omit the superscript notation to show Eq 7.1:\ne switch (f \u03b2 ) = E[(Y \u2212 X 2 \u03b2 2 ) 2 ] \u2212 2E[Y \u2212 X 2 \u03b2 2 ]E[X 1 ]\u03b2 1 + \u03b2 1 E[X 1 X 1 ]\u03b2 1 e switch (f \u03b2 ) = e orig (f \u03b2 ) \u2212 2E[Y \u2212 X 2 \u03b2 2 ]E[X 1 ]\u03b2 1 + 2E[(Y \u2212 X 2 \u03b2 2 )X 1 ]\u03b2 1 e switch (f \u03b2 ) = e orig (f \u03b2 ) + 2Cov(Y \u2212 X 2 \u03b2 2 , X 1 )\u03b2 1 e switch (f \u03b2 ) = e orig (f \u03b2 ) + 2Cov(Y, X 1 )\u03b2 1 \u2212 2\u03b2 2 Cov(X 2 , X 1 )\u03b2 1 .\nDividing both sides by e orig (f \u03b2 ) gives the desired result. Next, we can use a similar approach to show Eq 7.2:\ne switch (f \u03b2 ) = 1 n(n \u2212 1) n i=1 j =i (y [j] \u2212 X 2[j,\u00b7] \u03b2 2 \u2212 X 1[i,\u00b7] \u03b2 1 ) 2 n(n \u2212 1)\u00ea switch (f \u03b2 ) = n i=1 j =i (y [j] \u2212 X 2[j,\u00b7] \u03b2 2 ) 2 \u2212 2(y [j] \u2212 X 2[j,\u00b7] \u03b2 2 )(X 1[i,\u00b7] \u03b2 1 ) + (X 1[i,\u00b7] \u03b2 1 ) 2 = (n \u2212 1) n i=1 (y [i] \u2212 X 2[i,\u00b7] \u03b2 2 ) 2 \u2212 2 \uf8f1 \uf8f2 \uf8f3 n i=1 j =i (X 1[i,\u00b7] \u03b2 1 )(y [j] \u2212 X 2[j,\u00b7] \u03b2 2 ) \uf8fc \uf8fd \uf8fe + (n \u2212 1) n i=1 (X 1[i,\u00b7] \u03b2 1 ) 2 . (B.48)\nFocusing on the term in braces,\nn i=1 j =i (X 1[i,\u00b7] \u03b2 1 )(y [j] \u2212 X 2[j,\u00b7] \u03b2 2 ) = n i=1 n j=1 (X 1[i,\u00b7] \u03b2 1 )(y [j] \u2212 X 2[j,\u00b7] \u03b2 2 ) \u2212 n i=1 (X 1[i,\u00b7] \u03b2 1 )(y [i] \u2212 X 2[i,\u00b7] \u03b2 2 ) = n i=1 (X 1[i,\u00b7] \u03b2 1 ) n j=1 (y [j] \u2212 X 2[j,\u00b7] \u03b2 2 ) \u2212 n i=1 (X 1[i,\u00b7] \u03b2 1 )(y [i] \u2212 X 2[i,\u00b7] \u03b2 2 ) = (X 1 \u03b2 1 ) 1 n 1 n (y \u2212 X 2 \u03b2 2 ) \u2212 (X 1 \u03b2 1 ) (y \u2212 X 2 \u03b2 2 ) (B.49) = (X 1 \u03b2 1 ) (1 n 1 n \u2212 I n )(y \u2212 X 2 \u03b2 2 ).\nPlugging this into Eq B.48, and applying the sample linear algebra representations as in Eq B.49, we get n(n \u2212 1)\u00ea switch (f \u03b2 ) = (n \u2212 1) y \u2212 X 2 \u03b2 2 2 2 \u2212 2(X 1 \u03b2 1 ) (1 n 1 n \u2212 I n )(y \u2212 X 2 \u03b2 2 ) + (n \u2212 1) X 1 \u03b2 1 2 2 n\u00ea switch (f \u03b2 ) = y \u2212 X 2 \u03b2 2 2 2 \u2212 2(X 1 \u03b2 1 ) W(y \u2212 X 2 \u03b2 2 ) + X 1 \u03b2 1 2 2 = y y \u2212 2y X 2 \u03b2 2 + \u03b2 2 X 2 X 2 \u03b2 2 \u2212 2\u03b2 1 X 1 Wy + 2\u03b2 1 X 1 WX 2 \u03b2 2 + \u03b2 1 X 1 X 1 \u03b2 1 = y y \u2212 2 X 1 Wy X 2 y \u03b2 + \u03b2 X 1 X 1 X 1 WX 2 X 2 WX 1 X 2 X 2 \u03b2.\n\n\nB.9 Proof of Proposition 19\n\nProof First we consider e orig (f 0 ). We briefly recall that the notation f 0 (t, c) refers to the true conditional expectation function for both potential outcomes Y 1 , Y 0 , rather than the expectation for Y 0 alone.\n\nUnder the assumption that (Y 1 , Y 0 ) \u22a5 T |C, we have f 0 (t, c) = E(Y |C = c, T = t) = E(Y t |C = c). Applying this, we see that e orig (f 0 ) = EL(f 0 , (Y, T, C))\n\n= EL(f 0 , (Y T , T, C))\n= E T E C|T E Y T |C [{Y T \u2212 E(Y T |C)} 2 ] = E T E C|T Var(Y T |C) = qE C|T =0 Var(Y 0 |C) + pE C|T =1 Var(Y 1 |C), (B.50)\nwhere p := P(T = 1) and q := P(T = 0). Now we consider e switch (f 0 ). Let (Y \n\n1 , T (b) , C (b) ) be a pair of independent random variable vectors, each with the same distribution as (Y 0 , Y 1 , T, C). Then\ne switch (f 0 ) = E T (b) ,T (a) ,C (b) ,Y (b) T (b) [{Y (b) T (b) \u2212 f 0 (T (a) , C (b) )} 2 ] = E T (b) ,T (a) ,C (b) ,Y (b) T (b) [{Y (b) T (b) \u2212 E(Y T (a) |C = C (b) )} 2 ] = E T (b) ,T (a) E C (b) |T (b) E Y (b) T (b) |C (b) [{Y (b) T (b) \u2212 E(Y T (a) |C = C (b) )} 2 ].\nFirst we expand the outermost expectation, over T (b) , T (a) :\n\ne switch (f 0 ) = i,j\u2208{0,1} Plugging this into Eq B.51 we get\nP(T (b) = i, T (a) = j)E C (b) |T (b) =i E Y (b) i |C (b) [{Y (b) i \u2212 E(Y j |C = C (be switch (f 0 ) = i,j\u2208{0,1} p i+j q 2\u2212i\u2212j E C (b) |T (b) =i E Y (b) i |C (b) [{Y (b) i \u2212 E(Y j |C = C (b) )} 2 ]. Since (Y (b) 0 , Y (b) 1 , C (b) , T (b)\n) are the only random variables remaining, we can omit the superscript notation (e.g., replace C (b) with C) to get e switch (f 0 ) = i,j\u2208{0,1}\np i+j q 2\u2212i\u2212j E C|T =i E Y i |C [{Y i \u2212 E(Y j |C)} 2 ] =: i,j\u2208{0,1} A ij , where A ij = p i+j q 2\u2212i\u2212j E C|T =i E Y i |C [{Y i \u2212 E(Y j |C)} 2 ]\n. First, we consider A 00 and A 11 :\nA 00 = q 2 E C|T =0 E Y 0 |C [{Y 0 \u2212 E(Y 0 |C)} 2 ] = q 2 E C|T =0 Var(Y 0 |C),\nand, similarly, A 11 = p 2 E C|T =1 Var(Y 1 |C).\n\nNext we consider A 01 and A 10 :\nA 01 : = pqE C|T =0 E Y 0 |C [{Y 0 \u2212 E(Y 1 |C)} 2 ] = pqE C|T =0 E(Y 2 0 |C) \u2212 2E(Y 0 |C)E(Y 1 |C) + E(Y 1 |C) 2 = pqE C|T =0 Var(Y 0 |C) + E(Y 0 |C) 2 \u2212 2E(Y 0 |C)E(Y 1 |C) + E(Y 1 |C) 2 = pqE C|T =0 Var(Y 0 |C) + [E(Y 1 |C) \u2212 E(Y 0 |C)] 2 = pqE C|T =0 Var(Y 0 |C) + CATE(C) 2 ,\nand, following the same steps,\nA 10 = pqE C|T =1 Var(Y 1 |C) + CATE(C) 2 .\nPlugging in A 00 , A 01 , A 10 , and A 11 we get In Lines B.52 and B.53, we consolidate terms involving E C|T =0 Var(Y 0 |C) and E C|T =1 Var(Y 1 |C). In Line B.54, we use p+q = 1 to reduce Line B.52 to the right-hand side of Eq B.50. Finally, in Line B.55, we use qp = Var(T ). Dividing both sides by e orig (f 0 ) = E T,C V ar(Y |T, C) gives the desired result.\n\n\nAppendix C. Proofs for Computational Results\n\nAlmost all of the proofs in this section are unchanged if we replace\u00ea switch (f ) with\u00ea divide (f ) in our definitions of\u0125 \u2212,\u03b3 ,\u0125 +,\u03b3 ,\u011d \u2212,\u03b3 ,\u011d +,\u03b3 , and M R. The only exception is in Appendix C.3. Throughout the following proofs, we will make use of the fact that, for constants a, b, c, d \u2208 R satisfying a \u2265 c, the relation a + b \u2264 c + d implies\na + b \u2264 c + d a \u2212 c \u2264 d \u2212 b 0 \u2264 d \u2212 b since 0 \u2264 a \u2212 c b \u2264 d. (C.1)\nWe also make use of the fact that for any \u03b3 1 , \u03b3 2 \u2208 R, the definitions of\u011d +,\u03b3 1 and\u011d \u2212,\u03b3 1 imply\u0125 +,\u03b3 1 (\u011d +,\u03b3 1 ) \u2264\u0125 +,\u03b3 1 (\u011d +,\u03b3 2 ), and\u0125 \u2212,\u03b3 1 (g \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 1 (g \u2212,\u03b3 2 ). (C.2) Finally, for any two values \u03b3 1 , \u03b3 2 \u2208 R, we make use of the fact that (C.3) and, by the same steps,\nh +,\u03b3 1 (f ) =\u00ea orig (f ) + \u03b3 1\u00easwitch (f ) =\u00ea orig (f ) + \u03b3 2\u00easwitch (f ) + {\u03b3 1\u00easwitch (f ) \u2212 \u03b3 2\u00easwitch (f )} =\u0125 +,\u03b3 2 (f ) + (\u03b3 1 \u2212 \u03b3 2 )\u00ea switch (f ),h \u2212,\u03b3 1 (f ) =\u0125 \u2212,\u03b3 2 (f ) + (\u03b3 1 \u2212 \u03b3 2 )\u00ea orig (f ). (C.4)\nC.1 Proof of Lemma 9 (Lower Bound for MR)\n\nProof We prove Lemma 9 in 2 parts.\n\nC.1.1 Part 1: Showing Eq 6.1 Holds for All f \u2208 F Satisfying\u00ea orig (f ) \u2264 abs .\n\nIf\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) \u2265 0, then for any function f \u2208 F satisfying\u00ea orig (f ) \u2264 abs we know that 1 abs \u2264 1 e orig (f ) h \u2212,\u03b3 (\u011d \u2212,\u03b3 ) abs \u2264\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) e orig (f ) . (C.5) Now, for any f \u2208 F satisfying\u00ea orig (f ) \u2264 abs , the definition of\u011d \u2212,\u03b3 implies that\nh \u2212,\u03b3 (f ) \u2265\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) \u03b3\u00ea orig (f ) +\u00ea switch (f ) \u2265\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) \u03b3 +\u00ea switch (f ) e orig (f ) \u2265\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) e orig (f ) \u03b3 +\u00ea switch (f ) e orig (f ) \u2265\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 )\nabs from Eq C.5\n\nM R(f ) \u2265\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) abs \u2212 \u03b3.\n\nC.1.2 Part 2: Showing that, if f =\u011d \u2212,\u03b3 , and at Least One of the Inequalities in Condition 8 Holds with Equality, then Eq 6.1 Holds with Equality.\n\nWe consider each of the two inequalities in Condition 8 separately. If\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) = 0, then 0 = \u03b3\u00ea orig (\u011d \u2212,\u03b3 ) +\u00ea switch (\u011d \u2212,\u03b3 ) \u2212\u00ea switch (\u011d \u2212,\u03b3 ) e orig (\u011d \u2212,\u03b3 ) = \u03b3.\n\nAs a result\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) abs \u2212 \u03b3 = 0 abs \u2212 \u2212\u00ea switch (\u011d \u2212,\u03b3 ) e orig (\u011d \u2212,\u03b3 ) = M R(\u011d \u2212,\u03b3 ).\n\nAlternatively, if\u00ea orig (\u011d \u2212,\u03b3 ) = abs , then h \u2212,\u03b3 (\u011d \u2212,\u03b3 ) abs \u2212 \u03b3 = \u03b3\u00ea orig (\u011d \u2212,\u03b3 ) +\u00ea switch (\u011d \u2212,\u03b3 ) e orig (\u011d \u2212,\u03b3 ) \u2212 \u03b3 = \u03b3 +\u00ea switch (\u011d \u2212,\u03b3 ) e orig (\u011d \u2212,\u03b3 ) \u2212 \u03b3 = M R(\u011d \u2212,\u03b3 ).\n\n\nC.2 Proof of Lemma 10 (Monotonicity for MR Lower Bound Binary Search)\n\nProof We prove Lemma 10 in 3 parts.\n\nC.2.1 Part 1:\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) is Monotonically Increasing in \u03b3.\n\nLet \u03b3 1 , \u03b3 2 \u2208 R satisfy \u03b3 1 < \u03b3 2 . We have assumed that 0 <\u00ea orig (f ) for any f \u2208 F. Thus, for any f \u2208 F we have \u03b3 1\u00eaorig (f ) +\u00ea switch (f ) < \u03b3 2\u00eaorig (f ) +\u00ea switch (f ) h \u2212,\u03b3 1 (f ) <\u0125 \u2212,\u03b3 2 (f ). (C.6) Applying this, we hav\u00ea h \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 1 (\u011d \u2212,\u03b3 2 ) from Eq C.2 \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) from Eq C.6.\n\nThis result is analogous to Lemma 3 from Dinkelbach (1967).\n\nC.2.2 Part 2:\u00ea orig (\u011d \u2212,\u03b3 ) is Monotonically Decreasing in \u03b3.\n\nLet \u03b3 1 , \u03b3 2 \u2208 R satisfy \u03b3 1 < \u03b3 2 . Then h \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 1 (\u011d \u2212,\u03b3 2 ) from Eq C.2 h \u2212,\u03b3 2 (\u011d \u2212,\u03b3 1 ) + (\u03b3 1 \u2212 \u03b3 2 )\u00ea orig (\u011d \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) + (\u03b3 1 \u2212 \u03b3 2 )\u00ea orig (\u011d \u2212,\u03b3 2 ) from Eq C.4 (\u03b3 1 \u2212 \u03b3 2 )\u00ea orig (\u011d \u2212,\u03b3 1 ) \u2264 (\u03b3 1 \u2212 \u03b3 2 )\u00ea orig (\u011d \u2212,\u03b3 2 ) from Eqs C.1 & C.2 e orig (\u011d \u2212,\u03b3 1 ) \u2265\u00ea orig (\u011d \u2212,\u03b3 2 ).\n\n\nC.2.3 Part 3:\n\n\u0125 \u2212,\u03b3 (\u011d \u2212,\u03b3 ) abs \u2212 \u03b3 is Monotonically Decreasing in \u03b3 in the Range Where\u00ea orig (\u011d \u2212,\u03b3 ) \u2264 abs , and Increasing Otherwise.\n\nSuppose \u03b3 1 < \u03b3 2 and\u00ea orig (\u011d \u2212,\u03b3 1 ),\u00ea orig (\u011d \u2212,\u03b3 2 ) \u2264 abs . Then, from Eq C.2, h \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 1 ) h \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) \u2264\u0125 \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) + (\u03b3 2 \u2212 \u03b3 1 )\u00ea orig (\u011d \u2212,\u03b3 1 ) from Eq C.4 h \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) \u2264\u0125 \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) + (\u03b3 2 \u2212 \u03b3 1 ) abs from\u00ea orig (\u011d \u2212,\u03b3 1 ) \u2264 ab\u015d h \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) abs \u2212 \u03b3 2 \u2264\u0125 \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) abs \u2212 \u03b3 1 .\n\nSimilarly, if \u03b3 1 < \u03b3 2 and\u00ea orig (\u011d \u2212,\u03b3 1 ),\u00ea orig (\u011d \u2212,\u03b3 2 ) \u2265 abs . Then, from Eq C.2 h \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 1 (\u011d \u2212,\u03b3 2 ) h \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) + (\u03b3 1 \u2212 \u03b3 2 )\u00ea orig (\u011d \u2212,\u03b3 2 ) from Eq C.4\n\nh \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) + (\u03b3 1 \u2212 \u03b3 2 ) abs from\u00ea orig (\u011d \u2212,\u03b3 1 ) \u2265 ab\u015d h \u2212,\u03b3 1 (\u011d \u2212,\u03b3 1 ) abs \u2212 \u03b3 1 \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) abs \u2212 \u03b3 2 .\n\n\nC.3 Proof of Proposition 11 (Nonnegative Weights for MR Lower Bound Binary Search)\n\nProof Let \u03b3 1 := 1 n\u22121 . First we show that there exists a function\u011d \u2212,\u03b3 1 minimizing\u0125 \u2212,\u03b3 1 such that M R(\u011d \u2212,\u03b3 1 ) = 1. Let D s denote the sample distribution of the data, and let D m be the distribution satisfying P Dm {(Y, X 1 , X 2 ) = (y, x 1 , x 2 )} = P Ds {(Y, X 2 ) = (y, x 2 )} \u00d7 P Ds (X 1 = x 1 ) = 1 n 2 n i=1 1(y [i] = y and X 2[i] = x 2 ) n j=1 1(X 1[j] = x 1 ).\n\nFrom \u03b3 1 = 1 n\u22121 and Eq 6.2, we see that\nh \u2212,\u03b3 1 (f ) = n i=1 n j=1 L{f, (y [i] , X 1[j] , X 2[i] )} \u00d7 \u03b3 1 1(i = j) n + 1(i = j) n(n \u2212 1) = n i=1 n j=1\nL{f, (y [i] , X 1[j] , X 2[i] )} \u00d7 1 n(n \u2212 1)\n\n.\n\n\u221d E Dm L{f, (Y, X 1 , X 2 )}.\n\nThus, from Condition 2 of Proposition 11, we know there exists a function\u011d \u2212,\u03b3 1 that minimizes\u0125 \u2212,\u03b3 1 with\u011d \u2212,\u03b3 1 (x (a) for any x (a) 1 , x (b) 1 \u2208 X 1 , x 2 \u2208 X 2 , and y \u2208 Y. We apply this result in Line C.7,below,to show that loss of model\u011d \u2212,\u03b3 1 is unaffected by permuting X 1 within our sample:\ne switch (\u011d \u2212,\u03b3 1 ) = 1 n n i=1 1 n \u2212 1 j =i L{\u011d \u2212,\u03b3 1 , (y [i] , X 1[j] , X 2[i] )} = 1 n n i=1 1 n \u2212 1 j =i L{\u011d \u2212,\u03b3 1 , (y [i] , X 1[i] , X 2[i] )} (C.7) = 1 n n i=1 L{\u011d \u2212,\u03b3 1 , (y [i] , X 1[i] , X 2[i] )} =\u00ea orig (\u011d \u2212,\u03b3 1 ).\nIt follows that M R(\u011d \u2212,\u03b3 1 ) = 1. To show the result of Proposition 11, let \u03b3 2 = 0. For any function\u011d \u2212,\u03b3 2 minimizing\u0125 \u2212,\u03b3 2 , we know that h \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) \u2264\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 1 ) from the definition of\u011d \u2212,\u03b3 2 0 +\u00ea switch (\u011d \u2212,\u03b3 2 ) \u2264 0 +\u00ea switch (\u011d \u2212,\u03b3 1 ) from \u03b3 2 = 0 and the definition of\u0125 \u2212,\u03b3 2 . (C.8)\n\nFrom \u03b3 2 \u2264 \u03b3 1 , and Part 2 of Lemma 10, we know that e orig (\u011d \u2212,\u03b3 2 ) \u2265\u00ea orig (\u011d \u2212,\u03b3 1 ). (C.9) Combining Eqs C.8 and C.9, we have M R(\u011d \u2212,\u03b3 2 ) =\u00ea switch (\u011d \u2212,\u03b3 2 ) e orig (\u011d \u2212,\u03b3 2 ) \u2264\u00ea switch (\u011d \u2212,\u03b3 1 ) e orig (\u011d \u2212,\u03b3 1 ) = M R(\u011d \u2212,\u03b3 1 ) = 1. (C.10) Since\u0125 \u2212,\u03b3 2 (\u011d \u2212,\u03b3 2 ) =\u00ea switch (\u011d \u2212,\u03b3 2 ) \u2265 0 by definition, Condition 8 holds for \u03b3 2 , abs and g \u2212,\u03b3 2 if and only if\u00ea orig (\u011d \u2212,\u03b3 2 ) \u2264 abs . This, combined with Eq C.10, completes the proof.\n\nThe same result does not necessarily hold if we replace\u00ea switch with\u00ea divide in our definitions of\u0125 \u2212,\u03b3 , M R, and M CR \u2212 . This is because\u00ea divide does not correspond to the expectation over a distribution in which X 1 is independent of X 2 and Y , due to the fixed pairing structure used in\u00ea divide . Thus, Condition 2 of Proposition 11 will not apply.\n\n\nC.4 Proof of Lemma 13 (Upper Bound for MR)\n\nProof We prove Lemma 13 in 2 parts.\n\nC.4.1 Part 1: Showing Eq 6.4 Holds for All f \u2208 F Satisfying\u00ea orig (f ) \u2264 abs .\n\nIf\u0125 +,\u03b3 (\u011d +,\u03b3 ) \u2265 0, then for any function f \u2208 F satisfying\u00ea orig (f ) \u2264 abs we know that 1 abs \u2264 1 e orig (f ) h +,\u03b3 (\u011d +,\u03b3 ) abs \u2264\u0125 +,\u03b3 (\u011d +,\u03b3 ) e orig (f ) . (C.11) Now, if \u03b3 \u2264 0, then for any f \u2208 F satisfying\u00ea orig (f ) \u2264 abs , the definition of\u011d +,\u03b3 implie\u015d h +,\u03b3 (f ) \u2265\u0125 +,\u03b3 (\u011d +,\u03b3 )\n\ne orig (f ) + \u03b3\u00ea switch (f ) \u2265\u0125 +,\u03b3 (\u011d +,\u03b3 ) 1 + \u03b3\u00ea switch (f ) e orig (f ) \u2265\u0125 +,\u03b3 (\u011d +,\u03b3 ) e orig (f ) 1 + \u03b3\u00ea switch (f ) e orig (f ) \u2265\u0125 +,\u03b3 (\u011d +,\u03b3 ) abs from Eq C.11 1 + \u03b3 M R(f ) \u2265\u0125 +,\u03b3 (\u011d +,\u03b3 ) abs M R(f ) \u2264 \u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 .\n\n\nC.4.2 Part 2:\n\nShowing that if f =\u011d +,\u03b3 , and at Least One of the Enequalities in Condition 12 Holds with Equality, then Eq 6.4 Holds with Equality.\n\nWe consider each of the two inequalities in Condition 12 separately. If\u0125 +,\u03b3 (\u011d +,\u03b3 ) = 0, then 0 =\u00ea orig (\u011d +,\u03b3 ) + \u03b3\u00ea switch (\u011d +,\u03b3 ) \u2212\u03b3\u00ea switch (\u011d +,\u03b3 ) =\u00ea orig (\u011d +,\u03b3 ) \u2212\u00ea switch (\u011d +,\u03b3 ) e orig (\u011d +,\u03b3 ) = 1 \u03b3 .\n\nAs a result, \u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 = 0 abs \u2212 1 \u2212\u00ea switch (\u011d +,\u03b3 ) e orig (\u011d +,\u03b3 ) = M R(\u011d +,\u03b3 ).\n\nAlternatively, if\u00ea orig (\u011d +,\u03b3 ) = abs , then \u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 = \u00ea orig (\u011d +,\u03b3 ) + \u03b3\u00ea switch (\u011d +,\u03b3 ) e orig (\u011d +,\u03b3 ) \u2212 1 \u03b3 \u22121 = 1 + \u03b3\u00ea switch (\u011d +,\u03b3 ) e orig (\u011d +,\u03b3 ) \u2212 1 \u03b3 \u22121 = M R(\u011d +,\u03b3 ).\n\n\nC.5 Proof of Lemma 14 (Monotonicity for MR Upper Bound Binary Search)\n\nProof We prove Lemma 14 in 3 parts.\n\nThen, starting with Eq C.2, h +,\u03b3 2 (\u011d +,\u03b3 2 ) \u2264\u0125 +,\u03b3 2 (\u011d +,\u03b3 1 ) h +,\u03b3 2 (\u011d +,\u03b3 2 ) \u2264 \u03b3 2\u00easwitch (\u011d +,\u03b3 1 ) +\u00ea orig (\u011d +,\u03b3 1 ) h +,\u03b3 2 (\u011d +,\u03b3 2 ) \u2212 abs \u03b3 2 \u2265\u00ea switch (\u011d +,\u03b3 1 ) +\u00ea orig (\u011d +,\u03b3 1 ) \u2212 abs \u03b3 2 \u2265\u00ea switch (\u011d +,\u03b3 1 ) +\u00ea orig (\u011d +,\u03b3 1 ) \u2212 abs \u03b3 1 from Eq C.15 =\u0125 +,\u03b3 1 (\u011d +,\u03b3 1 ) \u2212 abs \u03b3 1 .\n\nDividing both sides of the above equation by abs proves that \u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 is monotonically increasing in \u03b3 in the range where\u00ea orig (\u011d +,\u03b3 ) \u2264 abs and \u03b3 < 0.\n\nTo prove the second result we proceed in the same way. Suppose that \u03b3 1 < \u03b3 2 < 0 and e orig (\u011d +,\u03b3 1 ),\u00ea orig (\u011d +,\u03b3 2 ) \u2265 abs . This implies 1 \u03b3 2 < 1 \u03b3 1 e orig (\u011d +,\u03b3 2 ) \u2212 abs \u03b3 2 <\u00ea orig (\u011d +,\u03b3 2 ) \u2212 abs \u03b3 1 . (C.16) Then, starting with Eq C.2, h +,\u03b3 1 (\u011d +,\u03b3 1 ) \u2264\u0125 +,\u03b3 1 (\u011d +,\u03b3 2 ) h +,\u03b3 1 (\u011d +,\u03b3 1 ) \u2264 \u03b3 1\u00easwitch (\u011d +,\u03b3 2 ) +\u00ea orig (\u011d +,\u03b3 2 ) h +,\u03b3 1 (\u011d +,\u03b3 1 ) \u2212 abs \u03b3 1 \u2265\u00ea switch (\u011d +,\u03b3 2 ) +\u00ea orig (\u011d +,\u03b3 2 ) \u2212 abs \u03b3 1 \u2265\u00ea switch (\u011d +,\u03b3 2 ) +\u00ea orig (\u011d +,\u03b3 2 ) \u2212 abs \u03b3 2 from Eq C.16 =\u0125 +,\u03b3 1 (\u011d +,\u03b3 2 ) \u2212 abs \u03b3 2 .\n\nDiving both sides of the above equation by abs proves that \u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 is monotonically decreasing in \u03b3 in the range where\u00ea orig (\u011d +,\u03b3 ) > abs and \u03b3 < 0.\n\n\nC.6 Proof of Remark 16 (Tractability of Empirical MCR for Linear Model Classes)\n\nProof To show Remark 16, we apply Proposition 15 to see that \u03be orig\u00eaorig (f \u03b2 ) + \u03be switch\u00easwitch (f \u03b2 ) = \u03be orig n ||y \u2212 X\u03b2|| 2 2 + \u03be switch\u00easwitch (f \u03b2 ) = \u03be orig n y y \u2212 2y X\u03b2 + \u03b2 X X\u03b2 + \u03be switch n y y \u2212 2 X 1 Wy X 2 y \u03b2 + \u03b2 X 1 X 1 X 1 WX 2 X 2 WX 1 X 2 X 2 \u03b2 \u221d \u03b2 \u22122q \u03b2 + \u03b2 Q\u03b2.\n\n\nC.7 Proof of Lemma 17 (Loss Upper Bound for Linear Models)\n\nProof Under the conditions in Lemma 17 and Eq 7.5, we can construct an upper bound on L(f \u03b2 , (y, x)) = (y \u2212 x \u03b2) 2 by either maximizing or minimizing x \u03b2. First, we consider the maximization problem max \u03b2,x\u2208R p x \u03b2 subject to x M \u22121 lm x \u2264 r X and \u03b2 M lm \u03b2 \u2264 r lm . (C.17) We can see that both constraints hold with equality at the solution to this problem. Next, we apply the change of variablesx = 1 \u221a r X D \u22121 2 U x and\u03b2 = 1 \u221a r lm D 1 2 U \u03b2, where UDU = M lm is the eigendecomposition of M lm . We obtain max \u03b2,x\u2208R px \u03b2 \u221a r X r lm subject tox x = 1 and\u03b2 \u03b2 = 1, which has an optimal objective value equal to \u221a r X r lm . By negating the objective in Eq C.17, we see that the minimum possible value of x \u03b2, subject to the constraints in Eq 7.5 and Lemma 17, is found at \u2212 \u221a r X r lm . Thus, we know that L(f, (y, x 1 , x 2 )) \u2264 max min y\u2208Y y \u2212 \u221a r X r lm 2 , max y\u2208Y y + \u221a r X r lm 2 , for any (y, x 1 , x 2 ) \u2208 (Y \u00d7 X 1 \u00d7 X 2 ).\n\n\nC.8 Proof of Lemma 18 (Loss Upper Bound for Regression in a RKHS)\n\nThis proofs follows a similar structure as the proof in Section C.7. From the assumptions of Lemma 18, we know from Eq 7.7 that the largest possible output from a model f \u03b1 \u2208 F D,r k\n\nFigure 1 :\n1Rashomon sets and model class reliance -Panel (A) illustrates a hypothetical Rashomon set R( ), within a model class F. The y-axis shows the expected loss of each model f \u2208 F, and the x-axis shows how much each model f relies on X 1 (defined formally in Section 3). Along the x-axis, the population-level MCR range is highlighted in blue, showing the values of MR corresponding to well-performing models (see Section 4). Panel (B) shows the in-sample analogue of Panel (A). Here, the y-axis denotes the in-sample loss,\u00caL(f, Z) := 1 n n i=1 L(f, Z [i,\u00b7] )\n\n\n-A, this range of possible MR values is shown by the highlighted interval along the x-axis. We refer to an interval of this type as a population-level model class reliance (MCR) range (see Section 4), formally defined as [M CR \u2212 ( ), M CR + ( )] := min f \u2208R( ) M R(f ), max f \u2208R( ) M R(f ) .\n\n\nsample loss of f under noise In-sample loss of f without noise , (2.3)\n\n\nwe write R( , f ref , F) and R( ) interchangeably when f ref and F are clear from context. Similarly, we occasionally write empirical -Rashomon sets using the more explicit notationR( , f ref , F) := {f \u2208 F :\u00ea orig (f ) \u2264\u00ea orig (f ref ) + }, but typically abbreviate these sets asR( ). While f ref could be selected by minimizing the in-sample loss, the theoretical study of R( , f ref , F) is simplified under the assumption that f ref is prespecified. For example, f ref may come from a flowchart used to predict injury severity in a hospital's emergency room, or from another quantitative decision rule that is currently implemented in practice. The model f ref can also be selected using sample splitting. In some cases it may be desirable to fix f ref equal to the best-in-class model f := arg min f \u2208F e orig (f ), but this is generally infeasible because f is unknown. Still, for any f ref \u2208 F, the Rashomon set R( , f ref , F) defined using f ref will always be conservative in the sense that it contains the Rashomon set R( , f , F) defined using f .\n\nFigure 3 :\n3Illustration of terms in Theorems 4 and 6 -Above we show the relation between empirical MR (x-axis) and empirical loss (y-axis) for models f in a hypothetical model class F. We mark f ref by the black point. For each possible model reliance value r \u2265 0, the curved, dashed line shows the lowest possible empirical loss for a function in f \u2208 F satisfying M R(f ) = r. The setR( ) contains all models in F within the dotted gray lines.\n\nFigure 5 :\n5Monotonicity for binary search. Above we show a version of Figure 4-C for two alternative values of \u03b3. This figure is meant to add intuition for the monotonicity results in Lemma 10, in addition to the formal proof. Increasing \u03b3 is equivalent to decreasing the slope of the red line in Figure 4-C.\n\n\n-C. We can then see from Figure 6-C that M R for each model in R( , f ref , F d3 ) is contained between M R(f \u2212, ) and M R(f +, ), up to a small approximation error.\n\nFigure 6 :\n6Example of AR, MCR & MR for polynomial classifiers. Panels (A) & (B)\n\n\nx) = E(Y |X = x) relies on subsets of covariates. Given a reference model f ref and model class F, our ability to describe M R(f 0 ) will hinge on two conditions: Condition 20 (Nearly correct model class) The class F contains a well-performing model f \u2208 R( , f ref , F) satisfying M R(f ) = M R(f 0 ) (see Eq 4.1).Condition 21 (Bootstrap coverage) Bootstrap CIs for empirical MCR give appropriate coverage of population-level MCR.\n\n\nLinear gave proper coverage for up to moderate levels of misspecification (\u03b3 = 0.3), where Standard-Linear began to break down (Figure 7). For larger levels of misspecification (\u03b3 \u2265 0.4), both MCR-Linear and Standard-Linear failed to give appropriate coverage.\n\nFigure 8 :\n8Empirical MR and MCR for Broward County criminal records data set -For any prediction model f , the y-axis shows empirical loss (\u00ea stnd (f )) and the x-axis shows empirical reliance ( M R(f )) on each covariate subset. Null reliance (MR equal to 1.0) is marked by the vertical dotted line. Reliances on different covariate subsets are marked by color (\"admissible\" = blue; \"inadmissible\" = gray). For example, model reliance values for f ref are shown by the two circular points, one for \"admissible\" variables and one for \"inadmissible\" variables. MCR for different values of can be represented as boundaries on this coordinate space.\n\n\ninequality used in Line B.3, see Theorem 2 ofHoeffding, 1963. \n\n\nR(f +, ) \u2264 M CR + ( out ). (B.10) Eq B.10 will always hold if f +, \u2208R( out ), since M CR + ( out ) upper bounds the empirical model reliance for models inR( out ) by definition. Applying the above reasoning in Line B.11, below, we get P M R(f +, ) > M CR + ( out ) \u2264 P f +, / \u2208R( out ) (B.11) \u2264 \u03b4 3 from out definition and Lemma 23. (B.12) B.3.2 Step 2: Conditional on M R(f +, ) \u2264 M CR + ( out ), Upper Bound M R(f +, ) by M CR + ( out ) Added to an Error Term.\n\n\n) )} 2 ]. (B.51) Since T (b) \u22a5 T (a) , we can write P(T (b) = i, T (a) = j) = P(T (b) = i)P(T (a) = j) = p i+j q 2\u2212i\u2212j .\n\n\ne switch (f 0 ) = {A 00 + A 11 }+ [A 01 + A 10 ] = q 2 E C|T =0 Var(Y 0 |C) + p 2 E C|T =1 Var(Y 1 |C) + pqE C|T =0 Var(Y 0 |C) + CATE(C) 2 + pqE C|T =1 Var(Y 1 |C) + CATE(C) 2 = q(q + p)E C|T =0 Var(Y 0 |C) + p(p + q)E C|T =1 Var(Y 1 |C) (B.52) +pq E C|T =0 CATE(C) 2 + E C|T =1 CATE(C) 2 (B.53) = {e orig (f 0 )} (B.54) +Var(T ) E C|T =0 CATE(C) 2 + E C|T =1 CATE(C) 2 . (B.55)\n\n\nhave been switched. To see this interpretation of the above equation, note that we have used the variables ((a) \n1 \n\nand X \n\n(b) \n\n1 \n\n\nwe describe the tractability of computing M CR \u2212 and M CR + using our proposed approaches. Computing empirical MCR can be reduced to a sequence of optimization problems, the form of which are noted in parentheses within the above table.Regularized) Linear models, \nwith the squared error loss \n\nHighly tractable \n(QP1QC, see Sections \n7.2 & 7.3) \n\nHighly tractable \n(QP1QC, see Sections \n7.2 & 7.3) \nLinear models in a reproducing \nkernel Hilbert space, with the \nsquared error loss \n\nModerately tractable \n(QP1QC, see Section \n7.4.1) \n\nModerately tractable \n(QP1QC, see Section \n7.4.1) \nCases where irrelevant covariates do \nnot improve predictions \n\nModerately tractable \n(Convex optimization \nproblems, see \nProposition 11) \n\nPotentially intractable \n\nCases where minimizing the \nempirical loss is a convex \noptimization problem \n\nPotentially intractable \n(DC programs, see \nSection 6.3) \n\nPotentially intractable \n(DC programs, see \nSection 6.3) \n\nTable 1: Tractability of empirical MCR computation for different model classes -For each \ncase, \n\n\nProof The proof for Eq B.23 is nearly identical to the proof for Eq B.22. Simply replacing L and B ind respectively withL and (2B ref ) in Eqs B.30-B.33 yields a valid proof for Eq B.23.B.5.4 Proof of Eq B.24 \n\n\n\n\n(see also Eq 1 on page 201 of Serfling, 1980, in Theorem A), in the same way as in Eq B.15.B.5.5 Proof for Eq B.25Proof We apply Lemma 24 and Eq B.27 in Line B.36, below, to obtainP sup \n\nf \u2208F \n\n\n\n\nEqs B.22 and B.24 (B.37) B.5.6 Proof for Eq B.26 Proof Finally, to show B.26, we apply the same steps as in Eqs B.35 through B.37. We apply Eq B.28 & Lemma 24 to obtainP sup \n\nf \u2208F \n\n\n\n\nLemma 26 Let 1 := 2B ref log(\u03b4 \u22121 ) 2n\n\n\nLemma 26, and the definition of = + 2B ref log(2\u03b4 \u22121 ) 2n .\narXiv:1801.01489v5 [stat.ME] 23 Dec 2019 Fisher, Rudin, and Dominici\n& 83587201-0), and by the Health Effects Institute (grant 4953-RFA14-3/16-4).\n, x 2 ) =\u011d \u2212,\u03b3 1 (x (b) 1 , x 2 ) for any x (a) 1 , x (b)1 \u2208 X 1 and x 2 \u2208 X 2 . Condition 1 of Proposition 11 then implies that L{\u011d \u2212,\u03b3 1 , (y, x(a)1 , x 2 )} = L{\u011d \u2212,\u03b3 1 , (y, x(b)1 , x 2 )}\nAcknowledgments Support for this work was provided by the National Institutes of Health (grants P01CA134294, R01GM111339, R01ES024332, R35CA197449, R01ES026217, P50MD010428, DP2MD012722, R01MD012769, & R01ES028033), by the Environmental Protection Agency (grants 83615601Appendix A. Miscellaneous Supplemental SectionsAll labels for items in the following appendices begin with a letter (for example, Section A.2), while references to items in the main text contain only numbers (for example, Proposition 19).A.1 Code R code for our example in Section 9.1 and analysis in Section 10 is available at https: //github.com/aaronjfisher/mcr-supplement.A.2 Model Reliance Less than 1While it is counterintuitive, it is possible for the expected loss of a prediction model to decrease when the information in X 1 is removed. Roughly speaking, a \"pathological\" model f silly may use the information in X 1 to \"intentionally\" misclassify Y , such that e switch (f silly ) < e orig (f silly ) and M R(f silly ) < 1. The model f silly may even be included in a population -Rashomon set (see Section 4) if it is still possible to predict Y sufficiently well from the information in X 2 .However, in these cases there will often exist another model that outperforms f silly , and that has MR equal to 1 (i.e., no reliance on X 1 ). To see this, consider the case where F = {f \u03b8 : \u03b8 \u2208 R d } is indexed by a parameter \u03b8. Let \u03b8 silly and \u03b8 be parameter values If Eq B.38 does not hold, we haveLet q ratio and q difference be defined as in Lemma 24. ThenTheorem 25 implies that the sup term in Eq B.41 is less than Q in with probability at least 1 \u2212 \u03b4 2 . Now, examining the left-hand side of Eq 4.6, we see Applying steps analogous to those used to derive Eq B.40, we haveAnalogous to B.41, when Eq B.44 does not hold, we have haveApplying this, we hav\u00ea h +,\u03b3 1 (\u011d +,\u03b3 1 ) \u2264\u0125 +,\u03b3 1 (\u011d +,\u03b3 2 ) from Eq C.2 <\u0125 +,\u03b3 2 (\u011d +,\u03b3 2 ) from Eq C.12.C.5.2 Part 2:\u00ea orig (\u011d +,\u03b3 ) is Monotonically Decreasing in \u03b3 for \u03b3 \u2264 0, and Condition 12 Holds for \u03b3 = 0 and abs \u2265 min f \u2208F\u00eaorig (f ).Let \u03b3 1 , \u03b3 2 \u2208 R satisfy \u03b3 1 < \u03b3 2 \u2264 0. ThenNow we are equipped to show the result that\u00ea orig (\u011d +,\u03b3 ) is monotonically decreasing in \u03b3 for \u03b3 \u2264 0:\u0125 +,\u03b3 2 (\u011d +,\u03b3 2 ) \u2264\u0125 +,\u03b3 2 (\u011d +,\u03b3 1 ) from Eq C.2 e orig (\u011d +,\u03b3 2 ) + \u03b3 2\u00easwitch (\u011d +,\u03b3 2 ) \u2264\u00ea orig (\u011d +,\u03b3 1 ) + \u03b3 2\u00easwitch (\u011d +,\u03b3 1 ) e orig (\u011d +,\u03b3 2 ) \u2264\u00ea orig (\u011d +,\u03b3 1 ) fromEqs C.1 & C.13. (C.14)To show that Condition 12 holds for \u03b3 = 0 and min f \u2208F\u00eaorig (f ) \u2264 abs , we first note that h 0,+ (g 0,+ ) =\u00ea orig (g 0,+ ), which is positive by assumption. Second, we note that e orig (g 0,+ ) = h 0,+ (g 0,+ ) = minC.5.3 Part 3:\u0125 +,\u03b3 (\u011d +,\u03b3 ) abs \u2212 1 \u03b3 \u22121 is Monotonically Increasing in \u03b3 in the Range Where\u00ea orig (\u011d +,\u03b3 ) \u2264 abs and \u03b3 < 0, and Decreasing in the Range Where\u00ea orig (\u011d +,\u03b3 ) > abs and \u03b3 < 0.To prove the first result, suppose that \u03b3 1 < \u03b3 2 < 0 and\u00ea orig (\u011d +,\u03b3 1 ),\u00ea orig (\u011d +,\u03b3 2 ) \u2264 abs . This implies 1 \u03b3 2 < 1 \u03b3 1 e orig (\u011d +,\u03b3 1 ) \u2212 abs \u03b3 2 >\u00ea orig (\u011d +,\u03b3 1 ) \u2212 abs \u03b3 1 .The above problem can be solved in the same way as Eq C.17, and has a solution at \u00b5 + \u221a r D r k . The smallest possible model output will similarly be lower bounded by \u2212 \u00b5 + \u221a r D r k . Thus, B ind is less than or equal to max min y\u2208Y (y) \u2212 (\u00b5 + \u221a r D r k ) 2 , max y\u2208Y (y) + (\u00b5 + \u221a r D r k ) 2 .\n1 Part 1:\u0125 +,\u03b3 (\u011d +,\u03b3 ) is Monotonically Increasing in \u03b3. C.5. C.5.1 Part 1:\u0125 +,\u03b3 (\u011d +,\u03b3 ) is Monotonically Increasing in \u03b3.\n\nLet \u03b3 1 , \u03b3 2 \u2208 R satisfy \u03b3 1 < \u03b3 2 . We have assumed that 0 \u2264\u00ea switch (f ) for any f \u2208 F. Thus, for any f \u2208 F we have\u00ea orig (f ) + \u03b3 1\u00easwitch (f ) <\u00ea orig (f ) + \u03b3 2\u00easwitch (f ) h +. \u03b3 1 (f ) <\u0125 +,\u03b3 2 (f )Let \u03b3 1 , \u03b3 2 \u2208 R satisfy \u03b3 1 < \u03b3 2 . We have assumed that 0 \u2264\u00ea switch (f ) for any f \u2208 F. Thus, for any f \u2208 F we have\u00ea orig (f ) + \u03b3 1\u00easwitch (f ) <\u00ea orig (f ) + \u03b3 2\u00easwitch (f ) h +,\u03b3 1 (f ) <\u0125 +,\u03b3 2 (f ).\n\nPermutation importance: a corrected feature importance measure. Andr\u00e9 Altmann, Laura Tolo\u015fi, Oliver Sander, Thomas Lengauer, Bioinformatics. 2610Andr\u00e9 Altmann, Laura Tolo\u015fi, Oliver Sander, and Thomas Lengauer. Permutation im- portance: a corrected feature importance measure. Bioinformatics, 26(10):1340-1347, 2010.\n\nEmpirical characterization of random forest variable importance measures. J Kellie, Ryan V Archer, Kimes, Computational Statistics & Data Analysis. 524Kellie J Archer and Ryan V Kimes. Empirical characterization of random forest variable importance measures. Computational Statistics & Data Analysis, 52(4):2249-2260, 2008.\n\nCriticality of predictors in multiple regression. Razia Azen, V David, Benjamin Budescu, Reiser, British Journal of Mathematical and Statistical Psychology. 542Razia Azen, David V Budescu, and Benjamin Reiser. Criticality of predictors in multiple regression. British Journal of Mathematical and Statistical Psychology, 54(2):201-225, 2001.\n\nRace, drugs, and policing: understanding disparities in drug delivery arrests. Katherine Beckett, Kris Nyrop, Lori Pfingst, Criminology. 441Katherine Beckett, Kris Nyrop, and Lori Pfingst. Race, drugs, and policing: understanding disparities in drug delivery arrests. Criminology, 44(1):105-137, 2006.\n\nThe influence of afrocentric facial features in criminal sentencing. V Irene, Charles M Blair, Kristine M Judd, Chapleau, Psychological science. 1510Irene V Blair, Charles M Judd, and Kristine M Chapleau. The influence of afrocentric facial features in criminal sentencing. Psychological science, 15(10):674-679, 2004.\n\nConvex Optimization. Stephen Boyd, Lieven Vandenberghe, Cambridge university pressStephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press, 2004.\n\nRandom forests. Machine learning. Leo Breiman, 45Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.\n\nStatistical modeling: the two cultures (with comments and a rejoinder by the author). Leo Breiman, Statistical science. 163Leo Breiman et al. Statistical modeling: the two cultures (with comments and a rejoinder by the author). Statistical science, 16(3):199-231, 2001.\n\nLetter to the editor: stability of random forest importance measures. Luz Calle, V\u00edctor Urrea, Briefings in bioinformatics. 121M Luz Calle and V\u00edctor Urrea. Letter to the editor: stability of random forest importance measures. Briefings in bioinformatics, 12(1):86-89, 2010.\n\nBayesian additive regression trees. Edward I Hugh A Chipman, Robert E George, Mcculloch, The Annals of Applied Statistics. 41Hugh A Chipman, Edward I George, Robert E McCulloch, et al. Bart: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1):266-298, 2010.\n\nFair prediction with disparate impact: a study of bias in recidivism prediction instruments. Alexandra Chouldechova, 5Big dataAlexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidi- vism prediction instruments. Big data, 5(2):153-163, 2017.\n\nA theory of statistical inference for ensuring the robustness of scientific results. Beau Coker, Cynthia Rudin, Gary King, arXiv:1804.08646arXiv preprintBeau Coker, Cynthia Rudin, and Gary King. A theory of statistical inference for ensuring the robustness of scientific results. arXiv preprint arXiv:1804.08646, 2018.\n\nA computer program used for bail and sentencing decisions was labeled biased against blacks. it's actually not that clear. The Washington Post. Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Sam Corbett-Davies, Emma Pierson, Avi Feller, and Sharad Goel. A computer program used for bail and sentencing decisions was labeled biased against blacks. it's actually not that clear. The Washington Post, October 2016. URL https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/ can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ ?utm_term=.e896ff1e4107.\n\nAlgorithmic decision making and the cost of fairness. Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMSam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD In- ternational Conference on Knowledge Discovery and Data Mining, pages 797-806. ACM, 2017.\n\nAlgorithmic transparency via quantitative input influence: theory and experiments with learning systems. Anupam Datta, Shayak Sen, Yair Zick, Security and Privacy (SP), 2016 IEEE Symposium on. IEEEAnupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influence: theory and experiments with learning systems. In Security and Privacy (SP), 2016 IEEE Symposium on, pages 598-617. IEEE, 2016.\n\nComparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. R Elizabeth, David M Delong, Daniel L Clarke-Pearson Delong, Biometrics. 443Elizabeth R DeLong, David M DeLong, and Daniel L Clarke-Pearson. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics, 44(3):837-845, 1988.\n\nMisuse of delong test to compare aucs for nested models. V Olga, Demler, J Michael, Ralph B D&apos;agostino Pencina, Sr, Statistics in medicine. 3123Olga V Demler, Michael J Pencina, and Ralph B D'Agostino Sr. Misuse of delong test to compare aucs for nested models. Statistics in medicine, 31(23):2577-2587, 2012.\n\nVariable importance and prediction methods for longitudinal problems with missing variables. Iv\u00e1n D\u00edaz, Alan Hubbard, Anna Decker, Mitchell Cohen, PloS one. 103120031Iv\u00e1n D\u00edaz, Alan Hubbard, Anna Decker, and Mitchell Cohen. Variable importance and prediction methods for longitudinal problems with missing variables. PloS one, 10(3): e0120031, 2015.\n\nOn nonlinear fractional programming. Werner Dinkelbach, Management science. 137Werner Dinkelbach. On nonlinear fractional programming. Management science, 13(7): 492-498, 1967.\n\nVariable importance clouds: A way to explore variable importance for the set of good models. Jiayun Dong, Cynthia Rudin, arXiv:1901.03209arXiv preprintJiayun Dong and Cynthia Rudin. Variable importance clouds: A way to explore variable importance for the set of good models. arXiv preprint arXiv:1901.03209, 2019.\n\nA note on the delta-method for finding variance formulae. R Dorfman, The Biometric Bulletin. 192R Dorfman. A note on the delta-method for finding variance formulae. The Biometric Bulletin, 1(129-137):92, 1938.\n\nFairness through awareness. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Richard Zemel, Proceedings of the 3rd innovations in theoretical computer science conference. the 3rd innovations in theoretical computer science conferenceACMCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fair- ness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214-226. ACM, 2012.\n\nReview and comparison of methods to study the contribution of variables in artificial neural network models. Muriel Gevrey, Ioannis Dimopoulos, Sovan Lek, Ecological modelling. 1603Muriel Gevrey, Ioannis Dimopoulos, and Sovan Lek. Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological modelling, 160(3):249-264, 2003.\n\nGrouped variable importance with random forests and application to multiple functional data analysis. Baptiste Gregorutti, Bertrand Michel, Philippe Saint-Pierre, Computational Statistics & Data Analysis. 90Baptiste Gregorutti, Bertrand Michel, and Philippe Saint-Pierre. Grouped variable impor- tance with random forests and application to multiple functional data analysis. Compu- tational Statistics & Data Analysis, 90:15-35, 2015.\n\nCorrelation and variable importance in random forests. Baptiste Gregorutti, Bertrand Michel, Philippe Saint-Pierre, Statistics and Computing. 273Baptiste Gregorutti, Bertrand Michel, and Philippe Saint-Pierre. Correlation and variable importance in random forests. Statistics and Computing, 27(3):659-678, 2017.\n\nA new variable importance measure for random forests with missing data. Alexander Hapfelmeier, Torsten Hothorn, Kurt Ulm, Carolin Strobl, Statistics and Computing. 241Alexander Hapfelmeier, Torsten Hothorn, Kurt Ulm, and Carolin Strobl. A new variable importance measure for random forests with missing data. Statistics and Computing, 24 (1):21-34, 2014.\n\nThe elements of statistical learning 2nd edition. T Hastie, J Tibshirani, Friedman, SpringerNew YorkT Hastie, R Tibshirani, and J Friedman. The elements of statistical learning 2nd edition. New York: Springer, 2009.\n\nThe Rashomon effect: when ethnographers disagree. G Karl, Heider, American Anthropologist. 901Karl G Heider. The Rashomon effect: when ethnographers disagree. American Anthropol- ogist, 90(1):73-81, 1988.\n\nA class of statistics with asymptotically normal distribution. The annals of mathematical statistics. Wassily Hoeffding, Wassily Hoeffding. A class of statistics with asymptotically normal distribution. The annals of mathematical statistics, pages 293-325, 1948.\n\nProbability inequalities for sums of bounded random variables. Wassily Hoeffding, https:/amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500830Journal of the American Statistical Association. 58301Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13-30, 1963. doi: 10.1080/01621459. 1963.10500830. URL https://amstat.tandfonline.com/doi/abs/10.1080/01621459. 1963.10500830.\n\nGeneralized functional anova diagnostics for high-dimensional functions of dependent variables. Giles Hooker, Journal of Computational and Graphical Statistics. 163Giles Hooker. Generalized functional anova diagnostics for high-dimensional functions of dependent variables. Journal of Computational and Graphical Statistics, 16(3):709-732, 2007.\n\nDc programming: overview. Reiner Horst, V Nguyen, Thoai, Journal of Optimization Theory and Applications. 1031Reiner Horst and Nguyen V Thoai. Dc programming: overview. Journal of Optimization Theory and Applications, 103(1):1-43, 1999.\n\nQuantifying explainable discrimination and removing illegal discrimination in automated decision making. Faisal Kamiran, Toon Indr\u0117\u017eliobait\u0117, Calders, Knowledge and information systems. 353Faisal Kamiran, Indr\u0117\u017dliobait\u0117, and Toon Calders. Quantifying explainable discrimina- tion and removing illegal discrimination in automated decision making. Knowledge and information systems, 35(3):613-644, 2013.\n\nVariable importance using decision trees. Jalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S Talwalkar, Advances in Neural Information Processing Systems. Jalil Kazemitabar, Arash Amini, Adam Bloniarz, and Ameet S Talwalkar. Variable impor- tance using decision trees. In Advances in Neural Information Processing Systems, pages 425-434, 2017.\n\nInherent trade-offs in the fair determination of risk scores. Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan, 8th Innovations in Theoretical Computer Science Conference (ITCS 2017). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In 8th Innovations in Theoretical Computer Science Conference (ITCS 2017). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017.\n\nHow we analyzed the compas recidivism algorithm. Jeff Larson, Surya Mattu, Lauren Kirchner, Julia Angwin, ProPublicaJeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. How we analyzed the compas recidivism algorithm. ProPublica, May 2016. URL https://www.propublica. org/article/how-we-analyzed-the-compas-recidivism-algorithm.\n\nInterplay between concentration, complexity and geometry in learning theory with applications to high dimensional data analysis. Guillaume Lecu\u00e9, Universit\u00e9 Paris-EstPhD thesisGuillaume Lecu\u00e9. Interplay between concentration, complexity and geometry in learning theory with applications to high dimensional data analysis. PhD thesis, Universit\u00e9 Paris- Est, 2011.\n\nTheory of point estimation. L Erich, George Lehmann, Casella, Springer Science & Business MediaErich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business Media, 2006.\n\nPrediction uncertainty and optimal experimental design for learning dynamical systems. Benjamin Letham, Portia A Letham, Cynthia Rudin, Edward P Browne, Chaos: An Interdisciplinary Journal of Nonlinear Science. 26663110Benjamin Letham, Portia A Letham, Cynthia Rudin, and Edward P Browne. Prediction uncertainty and optimal experimental design for learning dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 26(6):063110, 2016.\n\nUnderstanding variable importances in forests of randomized trees. Gilles Louppe, Louis Wehenkel, Antonio Sutera, Pierre Geurts, Advances in neural information processing systems. Gilles Louppe, Louis Wehenkel, Antonio Sutera, and Pierre Geurts. Understanding variable importances in forests of randomized trees. In Advances in neural information processing systems, pages 431-439, 2013.\n\nTo predict and serve? Significance. Kristian Lum, William Isaac, 13Kristian Lum and William Isaac. To predict and serve? Significance, 13(5):14-19, 2016.\n\nStability selection. Nicolai Meinshausen, Peter B\u00fchlmann, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 724Nicolai Meinshausen and Peter B\u00fchlmann. Stability selection. Journal of the Royal Statis- tical Society: Series B (Statistical Methodology), 72(4):417-473, 2010.\n\nQuantifying uncertainty in random forests via confidence intervals and hypothesis tests. Lucas Mentch, Giles Hooker, The Journal of Machine Learning Research. 171Lucas Mentch and Giles Hooker. Quantifying uncertainty in random forests via confidence intervals and hypothesis tests. The Journal of Machine Learning Research, 17(1):841-881, 2016.\n\nRisk assessment in criminal sentencing. John Monahan, Jennifer L Skeem, Annual review of clinical psychology. 12John Monahan and Jennifer L Skeem. Risk assessment in criminal sentencing. Annual review of clinical psychology, 12:489-513, 2016.\n\nFair inference on outcomes. Razieh Nabi, Ilya Shpitser, Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence. the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial IntelligenceNIH Public Access20181931Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 2018, page 1931. NIH Public Access, 2018.\n\nIdentifying a minimal class of models for high-dimensional data. Daniel Nevo, Ya&apos;acov Ritov, The Journal of Machine Learning Research. 181Daniel Nevo and Ya'acov Ritov. Identifying a minimal class of models for high-dimensional data. The Journal of Machine Learning Research, 18(1):797-825, 2017.\n\nAn accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data. Julian D Olden, K Michael, Russell G Joy, Death, Ecological Modelling. 1783Julian D Olden, Michael K Joy, and Russell G Death. An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data. Ecological Modelling, 178(3):389-397, 2004.\n\nGeneral heuristics for nonconvex quadratically constrained quadratic programming. Jaehyun Park, Stephen Boyd, arXiv:1703.07870arXiv preprintJaehyun Park and Stephen Boyd. General heuristics for nonconvex quadratically con- strained quadratic programming. arXiv preprint arXiv:1703.07870, 2017.\n\nReassessing race disparities in maryland capital cases. Raymond Paternoster, Robert Brame, Criminology. 464Raymond Paternoster and Robert Brame. Reassessing race disparities in maryland capital cases. Criminology, 46(4):971-1008, 2008.\n\nDemystifying risk assessment, key principles and controversies. Sarah Picard-Fritsche, Michael Rempel, Jennifer A Tallon, Julian Adler, Natalie Reyes, Technical reportSarah Picard-Fritsche, Michael Rempel, Jennifer A. Tallon, Julian Adler, and Na- talie Reyes. Demystifying risk assessment, key principles and controversies. Tech- nical report, 2017. Available at https://www.courtinnovation.org/publications/ demystifying-risk-assessment-key-principles-and-controversies.\n\nA survey of the s-lemma. Imre P\u00f3lik, Tam\u00e1s Terlaky, SIAM review. 493Imre P\u00f3lik and Tam\u00e1s Terlaky. A survey of the s-lemma. SIAM review, 49(3):371-418, 2007.\n\nRacial differences in marijuana-users' risk of arrest in the united states. Drug and alcohol dependence. Rajeev Ramchand, Rosalie Liccardo Pacula, Martin Y Iguchi, 84Rajeev Ramchand, Rosalie Liccardo Pacula, and Martin Y Iguchi. Racial differences in marijuana-users' risk of arrest in the united states. Drug and alcohol dependence, 84(3): 264-272, 2006.\n\nArtificial neural network approach for modelling and prediction of algal blooms. Friedrich Recknagel, Mark French, Pia Harkonen, Ken-Ichi Yabunaka, Ecological Modelling. 961-3Friedrich Recknagel, Mark French, Pia Harkonen, and Ken-Ichi Yabunaka. Artificial neural network approach for modelling and prediction of algal blooms. Ecological Modelling, 96 (1-3):11-28, 1997.\n\nThe Rashomon effect: combining positivist and interpretivist approaches in the analysis of contested events. D Wendy, Roth, Mehta, Sociological Methods & Research. 312Wendy D Roth and Jal D Mehta. The Rashomon effect: combining positivist and interpre- tivist approaches in the analysis of contested events. Sociological Methods & Research, 31(2):131-173, 2002.\n\nStop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, Nature Machine Intelligence. 1Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1:206-215, May 2019.\n\nThe age of secrecy and unfairness in recidivism prediction. Cynthia Rudin, Caroline Wang, Beau Coker, Harvard Data Science Review. acceptedCynthia Rudin, Caroline Wang, and Beau Coker. The age of secrecy and unfairness in recidivism prediction. Harvard Data Science Review, 2019. accepted.\n\nDeveloping an empirical model of phytoplankton primary production: a neural network case study. Michele Scardi, W Lawrence, Harding, Ecological modelling. 1202Michele Scardi and Lawrence W Harding. Developing an empirical model of phytoplankton primary production: a neural network case study. Ecological modelling, 120(2):213-223, 1999.\n\nA study in rashomon curves and volumes: a new perspective on generalization and model simplicity in machine learning. Lesia Semenova, Cynthia Rudin, arXiv:1908.01755arXiv preprintLesia Semenova and Cynthia Rudin. A study in rashomon curves and volumes: a new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755, 2019.\n\nApproximation theorems of mathematical statistics. J Robert, Serfling, John Wiley & SonsRobert J Serfling. Approximation theorems of mathematical statistics. John Wiley & Sons, 1980.\n\nThirty years of sentencing reform: the quest for a racially neutral sentencing process. Criminal justice. Cassia Spohn, 3Cassia Spohn. Thirty years of sentencing reform: the quest for a racially neutral sentencing process. Criminal justice, 3:427-501, 2000.\n\nAlgorithms for discovery of multiple markov boundaries. Alexander Statnikov, Jan Nikita I Lytkin, Constantin F Lemeire, Aliferis, Journal of Machine Learning Research. 14Alexander Statnikov, Nikita I Lytkin, Jan Lemeire, and Constantin F Aliferis. Algorithms for discovery of multiple markov boundaries. Journal of Machine Learning Research, 14 (Feb):499-566, 2013.\n\nBias in random forest variable importance measures: illustrations, sources and a solution. Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis, Torsten Hothorn, BMC bioinformatics. 8125Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. Bias in ran- dom forest variable importance measures: illustrations, sources and a solution. BMC bioinformatics, 8(1):25, 2007.\n\nConditional variable importance for random forests. Carolin Strobl, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, Achim Zeileis, BMC bioinformatics. 91307Carolin Strobl, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. Conditional variable importance for random forests. BMC bioinformatics, 9 (1):307, 2008.\n\nMatching methods for causal inference: a review and a look forward. A Elizabeth, Stuart, Statistical science: a review journal of the Institute of Mathematical Statistics. 2511Elizabeth A Stuart. Matching methods for causal inference: a review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics, 25(1):1, 2010.\n\nClassification with correlated features: unreliability of feature ranking and solutions. Laura Tolo\u015fi, Thomas Lengauer, Bioinformatics. 2714Laura Tolo\u015fi and Thomas Lengauer. Classification with correlated features: unreliability of feature ranking and solutions. Bioinformatics, 27(14):1986-1994, 2011.\n\nRobust optimization using machine learning for uncertainty sets. Theja Tulabandhula, Cynthia Rudin, arXiv:1407.1097arXiv preprintTheja Tulabandhula and Cynthia Rudin. Robust optimization using machine learning for uncertainty sets. arXiv preprint arXiv:1407.1097, 2014.\n\nDepartment of Justice -Civil Rights Devision. Investigation of the Baltimore City Police Department. U S , U.S. Department of Justice -Civil Rights Devision. Investigation of the Baltimore City Police Department, August 2016. Available at https://www.justice.gov/crt/file/ 883296/download.\n\nStatistical inference for variable importance. J Mark, Van Der Laan, The International Journal of Biostatistics. 21Mark J van der Laan. Statistical inference for variable importance. The International Journal of Biostatistics, 2(1), 2006.\n\nWho invented the delta method? The American Statistician. Jay M Ver Hoef, 66Jay M Ver Hoef. Who invented the delta method? The American Statistician, 66(2): 124-127, 2012.\n\nAn experimental study of the intrinsic stability of random forest variable importance measures. Huazhen Wang, Fan Yang, Zhiyuan Luo, BMC bioinformatics. 17160Huazhen Wang, Fan Yang, and Zhiyuan Luo. An experimental study of the intrinsic stability of random forest variable importance measures. BMC bioinformatics, 17(1):60, 2016.\n\nNonparametric variable importance assessment using machine learning techniques. Brian D Williamson, B Peter, Noah Gilbert, Marco Simon, Carone, bepressunpublished preprintBrian D Williamson, Peter B Gilbert, Noah Simon, and Marco Carone. Nonparametric variable importance assessment using machine learning techniques. bepress (unpublished preprint), 2017.\n\nForecasting and analysis of marketing data using neural networks. Jingtao Yao, Nicholas Teng, Hean-Lee Poh, Chew Lim Tan, J. Inf. Sci. Eng. 144Jingtao Yao, Nicholas Teng, Hean-Lee Poh, and Chew Lim Tan. Forecasting and analysis of marketing data using neural networks. J. Inf. Sci. Eng., 14(4):843-862, 1998.\n\nReinforcement learning trees. Ruoqing Zhu, Donglin Zeng, Michael R Kosorok, Journal of the American Statistical Association. 110512Ruoqing Zhu, Donglin Zeng, and Michael R Kosorok. Reinforcement learning trees. Journal of the American Statistical Association, 110(512):1770-1784, 2015.\n", "annotations": {"author": "[{\"end\":179,\"start\":143},{\"end\":214,\"start\":180},{\"end\":260,\"start\":215},{\"end\":378,\"start\":261},{\"end\":507,\"start\":379}]", "publisher": null, "author_last_name": "[{\"end\":155,\"start\":149},{\"end\":193,\"start\":188},{\"end\":233,\"start\":225}]", "author_first_name": "[{\"end\":148,\"start\":143},{\"end\":187,\"start\":180},{\"end\":224,\"start\":215}]", "author_affiliation": "[{\"end\":377,\"start\":262},{\"end\":506,\"start\":380}]", "title": "[{\"end\":140,\"start\":1},{\"end\":647,\"start\":508}]", "venue": null, "abstract": "[{\"end\":2156,\"start\":880}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b64\"},\"end\":2443,\"start\":2423},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2464,\"start\":2443},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":2481,\"start\":2464},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2505,\"start\":2481},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2524,\"start\":2505},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2548,\"start\":2524},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2675,\"start\":2651},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":2692,\"start\":2675},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2717,\"start\":2692},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2737,\"start\":2717},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2921,\"start\":2909},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3355,\"start\":3343},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":3378,\"start\":3355},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3407,\"start\":3378},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3428,\"start\":3407},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3448,\"start\":3428},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":5875,\"start\":5856},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10565,\"start\":10545},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10583,\"start\":10565},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":10607,\"start\":10583},{\"end\":11798,\"start\":11793},{\"end\":11808,\"start\":11803},{\"end\":11822,\"start\":11817},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":17437,\"start\":17416},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17844,\"start\":17824},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19159,\"start\":19146},{\"end\":23481,\"start\":23479},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24209,\"start\":24193},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24227,\"start\":24209},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":24242,\"start\":24227},{\"end\":24270,\"start\":24242},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24367,\"start\":24347},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25958,\"start\":25937},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26070,\"start\":26050},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26479,\"start\":26458},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27069,\"start\":27045},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27091,\"start\":27069},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":27109,\"start\":27091},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":27188,\"start\":27168},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":27210,\"start\":27188},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27231,\"start\":27210},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27256,\"start\":27231},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27282,\"start\":27258},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27550,\"start\":27530},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27577,\"start\":27552},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":28009,\"start\":27992},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28102,\"start\":28079},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28128,\"start\":28102},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28478,\"start\":28459},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51432,\"start\":51413},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":54805,\"start\":54791},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":54826,\"start\":54805},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":54857,\"start\":54828},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":54992,\"start\":54969},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":55220,\"start\":55199},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":55337,\"start\":55306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":55695,\"start\":55677},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":55717,\"start\":55697},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":56280,\"start\":56263},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":59139,\"start\":59122},{\"end\":67514,\"start\":67503},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":71057,\"start\":71034},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":71895,\"start\":71873},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":75151,\"start\":75127},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":75863,\"start\":75839},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":78441,\"start\":78412},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":78465,\"start\":78441},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":78485,\"start\":78465},{\"end\":80682,\"start\":80677},{\"end\":83049,\"start\":83047},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":91246,\"start\":91233},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":92815,\"start\":92801},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":94295,\"start\":94275},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":94935,\"start\":94911},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":95063,\"start\":95038},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":96679,\"start\":96658},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":96698,\"start\":96679},{\"end\":98058,\"start\":98055},{\"end\":101818,\"start\":101815},{\"end\":104644,\"start\":104640},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":107375,\"start\":107362},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":107394,\"start\":107375},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":107422,\"start\":107394},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":107635,\"start\":107610},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":107664,\"start\":107635},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":108042,\"start\":108020},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":108064,\"start\":108042},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":108377,\"start\":108348},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":108397,\"start\":108377},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":108765,\"start\":108744},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":108793,\"start\":108765},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":109301,\"start\":109280},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":111769,\"start\":111748},{\"end\":113396,\"start\":113384},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":116026,\"start\":116005},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":116158,\"start\":116138},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":116507,\"start\":116483},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":116526,\"start\":116507},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":116550,\"start\":116526},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":116578,\"start\":116550},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":117154,\"start\":117134},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":117239,\"start\":117210},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":117259,\"start\":117239},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":120508,\"start\":120482},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":120601,\"start\":120579},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":124893,\"start\":124872},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":124912,\"start\":124893},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":124936,\"start\":124912},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":125334,\"start\":125319},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":125359,\"start\":125334},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":125385,\"start\":125370},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":125747,\"start\":125723},{\"end\":126392,\"start\":126377},{\"end\":126396,\"start\":126392},{\"end\":126404,\"start\":126396},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":126903,\"start\":126883},{\"end\":127518,\"start\":127513},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":131893,\"start\":131877},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":135516,\"start\":135501},{\"end\":135576,\"start\":135547},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":136832,\"start\":136817},{\"end\":140490,\"start\":140484},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":142507,\"start\":142491},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":144198,\"start\":144183},{\"end\":156177,\"start\":156172},{\"end\":156255,\"start\":156250},{\"end\":156821,\"start\":156816},{\"end\":158119,\"start\":158114},{\"end\":159028,\"start\":159022},{\"end\":160355,\"start\":160352},{\"end\":160446,\"start\":160437},{\"end\":160452,\"start\":160446},{\"end\":160454,\"start\":160452},{\"end\":161172,\"start\":161167},{\"end\":161327,\"start\":161321},{\"end\":162213,\"start\":162207},{\"end\":164065,\"start\":164059},{\"end\":165241,\"start\":165235},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":170566,\"start\":170550}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":166720,\"start\":166153},{\"attributes\":{\"id\":\"fig_1\"},\"end\":167014,\"start\":166721},{\"attributes\":{\"id\":\"fig_2\"},\"end\":167087,\"start\":167015},{\"attributes\":{\"id\":\"fig_4\"},\"end\":168149,\"start\":167088},{\"attributes\":{\"id\":\"fig_5\"},\"end\":168596,\"start\":168150},{\"attributes\":{\"id\":\"fig_6\"},\"end\":168907,\"start\":168597},{\"attributes\":{\"id\":\"fig_8\"},\"end\":169075,\"start\":168908},{\"attributes\":{\"id\":\"fig_9\"},\"end\":169157,\"start\":169076},{\"attributes\":{\"id\":\"fig_10\"},\"end\":169590,\"start\":169158},{\"attributes\":{\"id\":\"fig_11\"},\"end\":169853,\"start\":169591},{\"attributes\":{\"id\":\"fig_12\"},\"end\":170502,\"start\":169854},{\"attributes\":{\"id\":\"fig_13\"},\"end\":170567,\"start\":170503},{\"attributes\":{\"id\":\"fig_14\"},\"end\":171032,\"start\":170568},{\"attributes\":{\"id\":\"fig_17\"},\"end\":171155,\"start\":171033},{\"attributes\":{\"id\":\"fig_18\"},\"end\":171537,\"start\":171156},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":171673,\"start\":171538},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":172724,\"start\":171674},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":172938,\"start\":172725},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":173136,\"start\":172939},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":173322,\"start\":173137},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":173363,\"start\":173323},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":173425,\"start\":173364}]", "paragraph": "[{\"end\":2985,\"start\":2172},{\"end\":3883,\"start\":2987},{\"end\":4787,\"start\":3885},{\"end\":5466,\"start\":4789},{\"end\":6372,\"start\":5468},{\"end\":6919,\"start\":6374},{\"end\":7637,\"start\":6921},{\"end\":9084,\"start\":7639},{\"end\":10403,\"start\":9086},{\"end\":10841,\"start\":10436},{\"end\":11512,\"start\":10843},{\"end\":11972,\"start\":11514},{\"end\":12608,\"start\":11974},{\"end\":13876,\"start\":12660},{\"end\":14336,\"start\":13878},{\"end\":14403,\"start\":14338},{\"end\":14410,\"start\":14405},{\"end\":14778,\"start\":14412},{\"end\":15393,\"start\":14780},{\"end\":15627,\"start\":15472},{\"end\":16118,\"start\":15629},{\"end\":16964,\"start\":16120},{\"end\":17040,\"start\":16966},{\"end\":17286,\"start\":17042},{\"end\":17579,\"start\":17288},{\"end\":18079,\"start\":17581},{\"end\":18147,\"start\":18081},{\"end\":18165,\"start\":18149},{\"end\":18837,\"start\":18326},{\"end\":19183,\"start\":18856},{\"end\":19190,\"start\":19185},{\"end\":19199,\"start\":19192},{\"end\":19340,\"start\":19233},{\"end\":19383,\"start\":19378},{\"end\":19506,\"start\":19385},{\"end\":19936,\"start\":19574},{\"end\":20179,\"start\":19938},{\"end\":21497,\"start\":20225},{\"end\":21602,\"start\":21565},{\"end\":21787,\"start\":21604},{\"end\":21948,\"start\":21855},{\"end\":22348,\"start\":22032},{\"end\":22888,\"start\":22490},{\"end\":23902,\"start\":22933},{\"end\":24489,\"start\":23947},{\"end\":25512,\"start\":24491},{\"end\":26196,\"start\":25568},{\"end\":26863,\"start\":26198},{\"end\":27957,\"start\":26865},{\"end\":28333,\"start\":27959},{\"end\":28878,\"start\":28381},{\"end\":29405,\"start\":28903},{\"end\":30750,\"start\":29407},{\"end\":31358,\"start\":30815},{\"end\":32083,\"start\":31434},{\"end\":32258,\"start\":32085},{\"end\":32413,\"start\":32260},{\"end\":32498,\"start\":32415},{\"end\":32633,\"start\":32578},{\"end\":33028,\"start\":32716},{\"end\":33952,\"start\":33030},{\"end\":34222,\"start\":33954},{\"end\":34458,\"start\":34224},{\"end\":34774,\"start\":34460},{\"end\":34844,\"start\":34823},{\"end\":34873,\"start\":34860},{\"end\":35721,\"start\":34955},{\"end\":36377,\"start\":35723},{\"end\":36967,\"start\":36379},{\"end\":38031,\"start\":36969},{\"end\":38440,\"start\":38033},{\"end\":38479,\"start\":38474},{\"end\":39174,\"start\":38481},{\"end\":39270,\"start\":39176},{\"end\":39451,\"start\":39450},{\"end\":39458,\"start\":39453},{\"end\":39821,\"start\":39460},{\"end\":40346,\"start\":39823},{\"end\":40586,\"start\":40414},{\"end\":40870,\"start\":40588},{\"end\":40988,\"start\":40872},{\"end\":41184,\"start\":41131},{\"end\":41679,\"start\":41186},{\"end\":42698,\"start\":41681},{\"end\":43169,\"start\":42757},{\"end\":43240,\"start\":43171},{\"end\":43717,\"start\":43298},{\"end\":47381,\"start\":46060},{\"end\":49730,\"start\":49534},{\"end\":49800,\"start\":49732},{\"end\":49932,\"start\":49888},{\"end\":50604,\"start\":49981},{\"end\":50893,\"start\":50606},{\"end\":51391,\"start\":50895},{\"end\":51969,\"start\":51393},{\"end\":52214,\"start\":52073},{\"end\":52817,\"start\":52216},{\"end\":52888,\"start\":52833},{\"end\":54066,\"start\":52900},{\"end\":54648,\"start\":54112},{\"end\":55819,\"start\":54650},{\"end\":56670,\"start\":55879},{\"end\":57322,\"start\":56749},{\"end\":57706,\"start\":57324},{\"end\":58283,\"start\":57857},{\"end\":58377,\"start\":58285},{\"end\":58662,\"start\":58424},{\"end\":58875,\"start\":58743},{\"end\":59248,\"start\":58877},{\"end\":59570,\"start\":59250},{\"end\":59702,\"start\":59572},{\"end\":59811,\"start\":59704},{\"end\":59924,\"start\":59813},{\"end\":60261,\"start\":60130},{\"end\":60848,\"start\":60263},{\"end\":62713,\"start\":61065},{\"end\":62840,\"start\":62715},{\"end\":63037,\"start\":62842},{\"end\":63137,\"start\":63039},{\"end\":63284,\"start\":63241},{\"end\":64081,\"start\":63411},{\"end\":64415,\"start\":64083},{\"end\":65534,\"start\":64576},{\"end\":66411,\"start\":66176},{\"end\":67352,\"start\":66413},{\"end\":67777,\"start\":67354},{\"end\":68075,\"start\":67824},{\"end\":68284,\"start\":68156},{\"end\":68419,\"start\":68286},{\"end\":68529,\"start\":68421},{\"end\":68609,\"start\":68531},{\"end\":68974,\"start\":68842},{\"end\":69243,\"start\":68976},{\"end\":69343,\"start\":69245},{\"end\":69395,\"start\":69345},{\"end\":69521,\"start\":69397},{\"end\":69733,\"start\":69616},{\"end\":69867,\"start\":69735},{\"end\":70143,\"start\":69885},{\"end\":70493,\"start\":70215},{\"end\":70772,\"start\":70495},{\"end\":71896,\"start\":70817},{\"end\":72377,\"start\":71898},{\"end\":72987,\"start\":72486},{\"end\":73647,\"start\":72989},{\"end\":73919,\"start\":73649},{\"end\":74201,\"start\":73983},{\"end\":74656,\"start\":74203},{\"end\":74761,\"start\":74737},{\"end\":74965,\"start\":74857},{\"end\":75365,\"start\":74967},{\"end\":75831,\"start\":75367},{\"end\":76056,\"start\":75833},{\"end\":76244,\"start\":76102},{\"end\":76567,\"start\":76284},{\"end\":76727,\"start\":76569},{\"end\":77116,\"start\":76778},{\"end\":77369,\"start\":77118},{\"end\":77610,\"start\":77399},{\"end\":77813,\"start\":77678},{\"end\":78161,\"start\":77833},{\"end\":78641,\"start\":78220},{\"end\":78983,\"start\":78669},{\"end\":79293,\"start\":78985},{\"end\":79824,\"start\":79364},{\"end\":80316,\"start\":79891},{\"end\":80824,\"start\":80318},{\"end\":80947,\"start\":80912},{\"end\":81204,\"start\":81020},{\"end\":81376,\"start\":81224},{\"end\":81458,\"start\":81378},{\"end\":82201,\"start\":81779},{\"end\":82443,\"start\":82405},{\"end\":82721,\"start\":82505},{\"end\":82894,\"start\":82749},{\"end\":83175,\"start\":82896},{\"end\":83467,\"start\":83372},{\"end\":83884,\"start\":83508},{\"end\":84305,\"start\":83886},{\"end\":84995,\"start\":84343},{\"end\":85154,\"start\":84997},{\"end\":85384,\"start\":85156},{\"end\":85736,\"start\":85435},{\"end\":85886,\"start\":85738},{\"end\":86077,\"start\":85888},{\"end\":86216,\"start\":86079},{\"end\":86375,\"start\":86308},{\"end\":87478,\"start\":86377},{\"end\":88015,\"start\":87480},{\"end\":88264,\"start\":88017},{\"end\":88621,\"start\":88266},{\"end\":88805,\"start\":88677},{\"end\":89353,\"start\":88923},{\"end\":89454,\"start\":89355},{\"end\":90479,\"start\":89492},{\"end\":90674,\"start\":90481},{\"end\":91036,\"start\":90734},{\"end\":91529,\"start\":91140},{\"end\":91718,\"start\":91531},{\"end\":92317,\"start\":91846},{\"end\":92816,\"start\":92319},{\"end\":93250,\"start\":92818},{\"end\":93686,\"start\":93335},{\"end\":94245,\"start\":93799},{\"end\":95152,\"start\":94247},{\"end\":95793,\"start\":95154},{\"end\":95997,\"start\":95809},{\"end\":96699,\"start\":96046},{\"end\":97178,\"start\":96701},{\"end\":97560,\"start\":97180},{\"end\":97813,\"start\":97562},{\"end\":99702,\"start\":98007},{\"end\":99985,\"start\":99704},{\"end\":100354,\"start\":99987},{\"end\":100619,\"start\":100356},{\"end\":102178,\"start\":100621},{\"end\":102946,\"start\":102228},{\"end\":103860,\"start\":102948},{\"end\":104013,\"start\":103862},{\"end\":104588,\"start\":104091},{\"end\":104776,\"start\":104590},{\"end\":105296,\"start\":104825},{\"end\":106031,\"start\":105308},{\"end\":106394,\"start\":106040},{\"end\":106708,\"start\":106396},{\"end\":107182,\"start\":106710},{\"end\":108997,\"start\":107267},{\"end\":110107,\"start\":108999},{\"end\":111129,\"start\":110109},{\"end\":112160,\"start\":111131},{\"end\":113289,\"start\":112172},{\"end\":113718,\"start\":113291},{\"end\":114565,\"start\":113720},{\"end\":117260,\"start\":114594},{\"end\":117738,\"start\":117262},{\"end\":118371,\"start\":117753},{\"end\":118997,\"start\":118373},{\"end\":119210,\"start\":118999},{\"end\":120059,\"start\":119212},{\"end\":120602,\"start\":120061},{\"end\":121049,\"start\":120604},{\"end\":121349,\"start\":121051},{\"end\":121510,\"start\":121426},{\"end\":122952,\"start\":121574},{\"end\":123114,\"start\":123070},{\"end\":123395,\"start\":123328},{\"end\":123664,\"start\":123454},{\"end\":123913,\"start\":123666},{\"end\":124007,\"start\":123987},{\"end\":124037,\"start\":124023},{\"end\":124566,\"start\":124121},{\"end\":124950,\"start\":124617},{\"end\":125386,\"start\":125009},{\"end\":125748,\"start\":125388},{\"end\":125948,\"start\":125750},{\"end\":126584,\"start\":126161},{\"end\":126692,\"start\":126586},{\"end\":127298,\"start\":126747},{\"end\":127990,\"start\":127300},{\"end\":128210,\"start\":128017},{\"end\":128494,\"start\":128236},{\"end\":128561,\"start\":128528},{\"end\":128686,\"start\":128682},{\"end\":129094,\"start\":128917},{\"end\":129275,\"start\":129150},{\"end\":129620,\"start\":129615},{\"end\":129628,\"start\":129622},{\"end\":129668,\"start\":129655},{\"end\":129986,\"start\":129670},{\"end\":130438,\"start\":130424},{\"end\":130626,\"start\":130485},{\"end\":130975,\"start\":130688},{\"end\":131230,\"start\":131016},{\"end\":131337,\"start\":131321},{\"end\":131525,\"start\":131375},{\"end\":131577,\"start\":131527},{\"end\":131702,\"start\":131592},{\"end\":132003,\"start\":131727},{\"end\":132051,\"start\":132005},{\"end\":132657,\"start\":132359},{\"end\":132772,\"start\":132659},{\"end\":132867,\"start\":132833},{\"end\":133031,\"start\":132915},{\"end\":133105,\"start\":133076},{\"end\":133205,\"start\":133184},{\"end\":133716,\"start\":133659},{\"end\":134220,\"start\":133992},{\"end\":134370,\"start\":134342},{\"end\":134377,\"start\":134372},{\"end\":134386,\"start\":134379},{\"end\":134494,\"start\":134465},{\"end\":134700,\"start\":134649},{\"end\":134709,\"start\":134702},{\"end\":134959,\"start\":134711},{\"end\":135597,\"start\":135126},{\"end\":136316,\"start\":136098},{\"end\":136884,\"start\":136427},{\"end\":137316,\"start\":136894},{\"end\":137718,\"start\":137581},{\"end\":137787,\"start\":137757},{\"end\":137990,\"start\":137961},{\"end\":138095,\"start\":138057},{\"end\":138514,\"start\":138323},{\"end\":138708,\"start\":138592},{\"end\":139494,\"start\":139066},{\"end\":139971,\"start\":139496},{\"end\":140108,\"start\":140003},{\"end\":140198,\"start\":140110},{\"end\":140406,\"start\":140200},{\"end\":140612,\"start\":140408},{\"end\":141539,\"start\":141319},{\"end\":141794,\"start\":141598},{\"end\":142229,\"start\":142174},{\"end\":142722,\"start\":142368},{\"end\":143501,\"start\":143172},{\"end\":143825,\"start\":143813},{\"end\":144199,\"start\":144143},{\"end\":145030,\"start\":144983},{\"end\":145061,\"start\":145032},{\"end\":145490,\"start\":145144},{\"end\":145546,\"start\":145492},{\"end\":146425,\"start\":146301},{\"end\":146491,\"start\":146427},{\"end\":146596,\"start\":146566},{\"end\":147128,\"start\":146867},{\"end\":147205,\"start\":147130},{\"end\":147450,\"start\":147207},{\"end\":147521,\"start\":147492},{\"end\":147694,\"start\":147554},{\"end\":147767,\"start\":147749},{\"end\":147982,\"start\":147800},{\"end\":148061,\"start\":147984},{\"end\":148465,\"start\":148382},{\"end\":148733,\"start\":148584},{\"end\":148845,\"start\":148776},{\"end\":148920,\"start\":148847},{\"end\":149122,\"start\":148935},{\"end\":149535,\"start\":149207},{\"end\":149838,\"start\":149576},{\"end\":150098,\"start\":149840},{\"end\":150593,\"start\":150100},{\"end\":150674,\"start\":150625},{\"end\":150891,\"start\":150832},{\"end\":151252,\"start\":151144},{\"end\":151686,\"start\":151572},{\"end\":152075,\"start\":152044},{\"end\":152898,\"start\":152443},{\"end\":153150,\"start\":152930},{\"end\":153318,\"start\":153152},{\"end\":153344,\"start\":153320},{\"end\":153548,\"start\":153469},{\"end\":153679,\"start\":153550},{\"end\":154017,\"start\":153954},{\"end\":154080,\"start\":154019},{\"end\":154464,\"start\":154321},{\"end\":154644,\"start\":154608},{\"end\":154773,\"start\":154725},{\"end\":154807,\"start\":154775},{\"end\":155118,\"start\":155088},{\"end\":155526,\"start\":155163},{\"end\":155922,\"start\":155575},{\"end\":156279,\"start\":155990},{\"end\":156536,\"start\":156495},{\"end\":156572,\"start\":156538},{\"end\":156652,\"start\":156574},{\"end\":156906,\"start\":156654},{\"end\":157095,\"start\":157080},{\"end\":157129,\"start\":157097},{\"end\":157278,\"start\":157131},{\"end\":157454,\"start\":157280},{\"end\":157549,\"start\":157456},{\"end\":157735,\"start\":157551},{\"end\":157844,\"start\":157809},{\"end\":157907,\"start\":157846},{\"end\":158226,\"start\":157909},{\"end\":158287,\"start\":158228},{\"end\":158351,\"start\":158289},{\"end\":158680,\"start\":158353},{\"end\":158821,\"start\":158698},{\"end\":159175,\"start\":158823},{\"end\":159387,\"start\":159177},{\"end\":159535,\"start\":159389},{\"end\":159999,\"start\":159622},{\"end\":160041,\"start\":160001},{\"end\":160198,\"start\":160153},{\"end\":160201,\"start\":160200},{\"end\":160232,\"start\":160203},{\"end\":160535,\"start\":160234},{\"end\":161073,\"start\":160764},{\"end\":161525,\"start\":161075},{\"end\":161881,\"start\":161527},{\"end\":161963,\"start\":161928},{\"end\":162043,\"start\":161965},{\"end\":162335,\"start\":162045},{\"end\":162578,\"start\":162337},{\"end\":162729,\"start\":162596},{\"end\":162946,\"start\":162731},{\"end\":163050,\"start\":162948},{\"end\":163254,\"start\":163052},{\"end\":163363,\"start\":163328},{\"end\":163667,\"start\":163365},{\"end\":163841,\"start\":163669},{\"end\":164368,\"start\":163843},{\"end\":164540,\"start\":164370},{\"end\":164905,\"start\":164624},{\"end\":165900,\"start\":164968},{\"end\":166152,\"start\":165970}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15471,\"start\":15394},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18264,\"start\":18166},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19232,\"start\":19200},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19377,\"start\":19341},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19573,\"start\":19507},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20224,\"start\":20180},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21854,\"start\":21788},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22031,\"start\":21949},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22489,\"start\":22349},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22932,\"start\":22889},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23946,\"start\":23903},{\"attributes\":{\"id\":\"formula_14\"},\"end\":28380,\"start\":28334},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30814,\"start\":30751},{\"attributes\":{\"id\":\"formula_16\"},\"end\":32577,\"start\":32499},{\"attributes\":{\"id\":\"formula_17\"},\"end\":32715,\"start\":32634},{\"attributes\":{\"id\":\"formula_18\"},\"end\":34822,\"start\":34775},{\"attributes\":{\"id\":\"formula_19\"},\"end\":34859,\"start\":34845},{\"attributes\":{\"id\":\"formula_20\"},\"end\":34954,\"start\":34874},{\"attributes\":{\"id\":\"formula_21\"},\"end\":38473,\"start\":38441},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39449,\"start\":39271},{\"attributes\":{\"id\":\"formula_23\"},\"end\":40413,\"start\":40347},{\"attributes\":{\"id\":\"formula_24\"},\"end\":41130,\"start\":40989},{\"attributes\":{\"id\":\"formula_25\"},\"end\":46059,\"start\":43718},{\"attributes\":{\"id\":\"formula_26\"},\"end\":49533,\"start\":47382},{\"attributes\":{\"id\":\"formula_27\"},\"end\":49887,\"start\":49801},{\"attributes\":{\"id\":\"formula_28\"},\"end\":49980,\"start\":49933},{\"attributes\":{\"id\":\"formula_29\"},\"end\":52072,\"start\":51970},{\"attributes\":{\"id\":\"formula_30\"},\"end\":52832,\"start\":52818},{\"attributes\":{\"id\":\"formula_31\"},\"end\":52899,\"start\":52889},{\"attributes\":{\"id\":\"formula_32\"},\"end\":56748,\"start\":56711},{\"attributes\":{\"id\":\"formula_33\"},\"end\":57856,\"start\":57707},{\"attributes\":{\"id\":\"formula_34\"},\"end\":58742,\"start\":58663},{\"attributes\":{\"id\":\"formula_35\"},\"end\":60129,\"start\":59925},{\"attributes\":{\"id\":\"formula_36\"},\"end\":61064,\"start\":60849},{\"attributes\":{\"id\":\"formula_37\"},\"end\":63240,\"start\":63138},{\"attributes\":{\"id\":\"formula_38\"},\"end\":63410,\"start\":63285},{\"attributes\":{\"id\":\"formula_39\"},\"end\":64575,\"start\":64416},{\"attributes\":{\"id\":\"formula_40\"},\"end\":65824,\"start\":65598},{\"attributes\":{\"id\":\"formula_41\"},\"end\":66175,\"start\":65901},{\"attributes\":{\"id\":\"formula_42\"},\"end\":68155,\"start\":68076},{\"attributes\":{\"id\":\"formula_43\"},\"end\":68841,\"start\":68610},{\"attributes\":{\"id\":\"formula_44\"},\"end\":69615,\"start\":69555},{\"attributes\":{\"id\":\"formula_45\"},\"end\":70214,\"start\":70144},{\"attributes\":{\"id\":\"formula_46\"},\"end\":70816,\"start\":70773},{\"attributes\":{\"id\":\"formula_47\"},\"end\":74736,\"start\":74657},{\"attributes\":{\"id\":\"formula_48\"},\"end\":74851,\"start\":74762},{\"attributes\":{\"id\":\"formula_49\"},\"end\":76283,\"start\":76245},{\"attributes\":{\"id\":\"formula_50\"},\"end\":76777,\"start\":76728},{\"attributes\":{\"id\":\"formula_51\"},\"end\":77677,\"start\":77611},{\"attributes\":{\"id\":\"formula_52\"},\"end\":78219,\"start\":78162},{\"attributes\":{\"id\":\"formula_53\"},\"end\":79363,\"start\":79294},{\"attributes\":{\"id\":\"formula_54\"},\"end\":80911,\"start\":80825},{\"attributes\":{\"id\":\"formula_55\"},\"end\":81019,\"start\":80948},{\"attributes\":{\"id\":\"formula_56\"},\"end\":81778,\"start\":81459},{\"attributes\":{\"id\":\"formula_57\"},\"end\":82404,\"start\":82202},{\"attributes\":{\"id\":\"formula_58\"},\"end\":82504,\"start\":82444},{\"attributes\":{\"id\":\"formula_59\"},\"end\":83290,\"start\":83176},{\"attributes\":{\"id\":\"formula_60\"},\"end\":83371,\"start\":83290},{\"attributes\":{\"id\":\"formula_61\"},\"end\":85434,\"start\":85385},{\"attributes\":{\"id\":\"formula_62\"},\"end\":86307,\"start\":86217},{\"attributes\":{\"id\":\"formula_63\"},\"end\":88676,\"start\":88622},{\"attributes\":{\"id\":\"formula_64\"},\"end\":88922,\"start\":88806},{\"attributes\":{\"id\":\"formula_65\"},\"end\":89491,\"start\":89455},{\"attributes\":{\"id\":\"formula_66\"},\"end\":91139,\"start\":91037},{\"attributes\":{\"id\":\"formula_67\"},\"end\":91845,\"start\":91719},{\"attributes\":{\"id\":\"formula_69\"},\"end\":93334,\"start\":93251},{\"attributes\":{\"id\":\"formula_70\"},\"end\":93798,\"start\":93687},{\"attributes\":{\"id\":\"formula_71\"},\"end\":98006,\"start\":97814},{\"attributes\":{\"id\":\"formula_72\"},\"end\":104090,\"start\":104014},{\"attributes\":{\"id\":\"formula_73\"},\"end\":104824,\"start\":104777},{\"attributes\":{\"id\":\"formula_74\"},\"end\":121425,\"start\":121350},{\"attributes\":{\"id\":\"formula_75\"},\"end\":121573,\"start\":121511},{\"attributes\":{\"id\":\"formula_76\"},\"end\":123069,\"start\":122953},{\"attributes\":{\"id\":\"formula_77\"},\"end\":123327,\"start\":123115},{\"attributes\":{\"id\":\"formula_78\"},\"end\":123986,\"start\":123914},{\"attributes\":{\"id\":\"formula_79\"},\"end\":124022,\"start\":124008},{\"attributes\":{\"id\":\"formula_80\"},\"end\":124120,\"start\":124038},{\"attributes\":{\"id\":\"formula_81\"},\"end\":125008,\"start\":124951},{\"attributes\":{\"id\":\"formula_82\"},\"end\":126160,\"start\":125949},{\"attributes\":{\"id\":\"formula_83\"},\"end\":128016,\"start\":127991},{\"attributes\":{\"id\":\"formula_84\"},\"end\":128235,\"start\":128211},{\"attributes\":{\"id\":\"formula_85\"},\"end\":128681,\"start\":128562},{\"attributes\":{\"id\":\"formula_86\"},\"end\":128916,\"start\":128687},{\"attributes\":{\"id\":\"formula_87\"},\"end\":129149,\"start\":129095},{\"attributes\":{\"id\":\"formula_88\"},\"end\":129614,\"start\":129276},{\"attributes\":{\"id\":\"formula_90\"},\"end\":129654,\"start\":129629},{\"attributes\":{\"id\":\"formula_91\"},\"end\":130423,\"start\":129987},{\"attributes\":{\"id\":\"formula_92\"},\"end\":131015,\"start\":130976},{\"attributes\":{\"id\":\"formula_93\"},\"end\":131320,\"start\":131231},{\"attributes\":{\"id\":\"formula_94\"},\"end\":131374,\"start\":131338},{\"attributes\":{\"id\":\"formula_95\"},\"end\":131591,\"start\":131578},{\"attributes\":{\"id\":\"formula_96\"},\"end\":131726,\"start\":131703},{\"attributes\":{\"id\":\"formula_97\"},\"end\":132318,\"start\":132052},{\"attributes\":{\"id\":\"formula_98\"},\"end\":132832,\"start\":132773},{\"attributes\":{\"id\":\"formula_99\"},\"end\":132914,\"start\":132868},{\"attributes\":{\"id\":\"formula_100\"},\"end\":133075,\"start\":133032},{\"attributes\":{\"id\":\"formula_101\"},\"end\":133183,\"start\":133106},{\"attributes\":{\"id\":\"formula_102\"},\"end\":133658,\"start\":133206},{\"attributes\":{\"id\":\"formula_103\"},\"end\":133991,\"start\":133717},{\"attributes\":{\"id\":\"formula_104\"},\"end\":134316,\"start\":134221},{\"attributes\":{\"id\":\"formula_105\"},\"end\":134438,\"start\":134387},{\"attributes\":{\"id\":\"formula_106\"},\"end\":134648,\"start\":134495},{\"attributes\":{\"id\":\"formula_107\"},\"end\":135125,\"start\":134960},{\"attributes\":{\"id\":\"formula_108\"},\"end\":136097,\"start\":135598},{\"attributes\":{\"id\":\"formula_109\"},\"end\":136426,\"start\":136317},{\"attributes\":{\"id\":\"formula_110\"},\"end\":137580,\"start\":137317},{\"attributes\":{\"id\":\"formula_111\"},\"end\":137756,\"start\":137719},{\"attributes\":{\"id\":\"formula_112\"},\"end\":137960,\"start\":137788},{\"attributes\":{\"id\":\"formula_113\"},\"end\":138056,\"start\":137991},{\"attributes\":{\"id\":\"formula_114\"},\"end\":138322,\"start\":138096},{\"attributes\":{\"id\":\"formula_115\"},\"end\":138563,\"start\":138515},{\"attributes\":{\"id\":\"formula_116\"},\"end\":139065,\"start\":138709},{\"attributes\":{\"id\":\"formula_117\"},\"end\":141293,\"start\":140613},{\"attributes\":{\"id\":\"formula_118\"},\"end\":141597,\"start\":141540},{\"attributes\":{\"id\":\"formula_119\"},\"end\":142173,\"start\":141795},{\"attributes\":{\"id\":\"formula_120\"},\"end\":142367,\"start\":142230},{\"attributes\":{\"id\":\"formula_121\"},\"end\":142943,\"start\":142723},{\"attributes\":{\"id\":\"formula_122\"},\"end\":143171,\"start\":142943},{\"attributes\":{\"id\":\"formula_123\"},\"end\":143812,\"start\":143502},{\"attributes\":{\"id\":\"formula_124\"},\"end\":144142,\"start\":143826},{\"attributes\":{\"id\":\"formula_125\"},\"end\":144713,\"start\":144200},{\"attributes\":{\"id\":\"formula_126\"},\"end\":144982,\"start\":144713},{\"attributes\":{\"id\":\"formula_127\"},\"end\":145143,\"start\":145062},{\"attributes\":{\"id\":\"formula_128\"},\"end\":146300,\"start\":145547},{\"attributes\":{\"id\":\"formula_129\"},\"end\":146565,\"start\":146492},{\"attributes\":{\"id\":\"formula_130\"},\"end\":146866,\"start\":146597},{\"attributes\":{\"id\":\"formula_131\"},\"end\":147491,\"start\":147451},{\"attributes\":{\"id\":\"formula_132\"},\"end\":147553,\"start\":147522},{\"attributes\":{\"id\":\"formula_133\"},\"end\":147748,\"start\":147695},{\"attributes\":{\"id\":\"formula_134\"},\"end\":148330,\"start\":148062},{\"attributes\":{\"id\":\"formula_135\"},\"end\":148583,\"start\":148466},{\"attributes\":{\"id\":\"formula_136\"},\"end\":148775,\"start\":148734},{\"attributes\":{\"id\":\"formula_137\"},\"end\":148934,\"start\":148921},{\"attributes\":{\"id\":\"formula_138\"},\"end\":149575,\"start\":149536},{\"attributes\":{\"id\":\"formula_139\"},\"end\":150831,\"start\":150675},{\"attributes\":{\"id\":\"formula_140\"},\"end\":151143,\"start\":150892},{\"attributes\":{\"id\":\"formula_141\"},\"end\":151571,\"start\":151253},{\"attributes\":{\"id\":\"formula_142\"},\"end\":152043,\"start\":151687},{\"attributes\":{\"id\":\"formula_143\"},\"end\":152442,\"start\":152076},{\"attributes\":{\"id\":\"formula_144\"},\"end\":153468,\"start\":153345},{\"attributes\":{\"id\":\"formula_146\"},\"end\":153953,\"start\":153680},{\"attributes\":{\"id\":\"formula_147\"},\"end\":154166,\"start\":154081},{\"attributes\":{\"id\":\"formula_148\"},\"end\":154320,\"start\":154166},{\"attributes\":{\"id\":\"formula_149\"},\"end\":154607,\"start\":154465},{\"attributes\":{\"id\":\"formula_150\"},\"end\":154724,\"start\":154645},{\"attributes\":{\"id\":\"formula_151\"},\"end\":155087,\"start\":154808},{\"attributes\":{\"id\":\"formula_152\"},\"end\":155162,\"start\":155119},{\"attributes\":{\"id\":\"formula_153\"},\"end\":155989,\"start\":155923},{\"attributes\":{\"id\":\"formula_154\"},\"end\":156435,\"start\":156280},{\"attributes\":{\"id\":\"formula_155\"},\"end\":156494,\"start\":156435},{\"attributes\":{\"id\":\"formula_156\"},\"end\":157079,\"start\":156907},{\"attributes\":{\"id\":\"formula_157\"},\"end\":160152,\"start\":160042},{\"attributes\":{\"id\":\"formula_158\"},\"end\":160763,\"start\":160536}]", "table_ref": "[{\"end\":57321,\"start\":57314}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2170,\"start\":2158},{\"attributes\":{\"n\":\"2.\"},\"end\":10434,\"start\":10406},{\"attributes\":{\"n\":\"2.1\"},\"end\":12658,\"start\":12611},{\"end\":18324,\"start\":18266},{\"attributes\":{\"n\":\"3.\"},\"end\":18854,\"start\":18840},{\"attributes\":{\"n\":\"3.1\"},\"end\":21563,\"start\":21500},{\"attributes\":{\"n\":\"3.2\"},\"end\":25566,\"start\":25515},{\"attributes\":{\"n\":\"4.\"},\"end\":28901,\"start\":28881},{\"attributes\":{\"n\":\"4.1\"},\"end\":31432,\"start\":31361},{\"attributes\":{\"n\":\"5.\"},\"end\":42755,\"start\":42701},{\"attributes\":{\"n\":\"5.1\"},\"end\":43296,\"start\":43243},{\"attributes\":{\"n\":\"5.2\"},\"end\":54110,\"start\":54069},{\"attributes\":{\"n\":\"6.\"},\"end\":55877,\"start\":55822},{\"end\":56710,\"start\":56673},{\"attributes\":{\"n\":\"6.1\"},\"end\":58422,\"start\":58380},{\"attributes\":{\"n\":\"1.\"},\"end\":65597,\"start\":65537},{\"attributes\":{\"n\":\"2.\"},\"end\":65900,\"start\":65826},{\"attributes\":{\"n\":\"6.2\"},\"end\":67822,\"start\":67780},{\"attributes\":{\"n\":\"3.\"},\"end\":69554,\"start\":69524},{\"attributes\":{\"n\":\"6.3\"},\"end\":69883,\"start\":69870},{\"attributes\":{\"n\":\"7.\"},\"end\":72484,\"start\":72380},{\"attributes\":{\"n\":\"7.1\"},\"end\":73981,\"start\":73922},{\"end\":74855,\"start\":74853},{\"attributes\":{\"n\":\"7.2\"},\"end\":76100,\"start\":76059},{\"attributes\":{\"n\":\"7.3\"},\"end\":77397,\"start\":77372},{\"attributes\":{\"n\":\"7.3.1\"},\"end\":77831,\"start\":77816},{\"attributes\":{\"n\":\"7.3.2\"},\"end\":78667,\"start\":78644},{\"attributes\":{\"n\":\"7.4\"},\"end\":79889,\"start\":79827},{\"attributes\":{\"n\":\"7.4.1\"},\"end\":81222,\"start\":81207},{\"attributes\":{\"n\":\"7.4.2\"},\"end\":82747,\"start\":82724},{\"attributes\":{\"n\":\"8.\"},\"end\":83506,\"start\":83470},{\"attributes\":{\"n\":\"8.1\"},\"end\":84341,\"start\":84308},{\"attributes\":{\"n\":\"8.2.1\"},\"end\":90732,\"start\":90677},{\"attributes\":{\"n\":\"9.\"},\"end\":95807,\"start\":95796},{\"attributes\":{\"n\":\"9.1\"},\"end\":96044,\"start\":96000},{\"attributes\":{\"n\":\"9.2\"},\"end\":102226,\"start\":102181},{\"attributes\":{\"n\":\"9.2.1\"},\"end\":105306,\"start\":105299},{\"end\":106038,\"start\":106034},{\"attributes\":{\"n\":\"10.\"},\"end\":107265,\"start\":107185},{\"attributes\":{\"n\":\"10.1\"},\"end\":112170,\"start\":112163},{\"attributes\":{\"n\":\"10.2\"},\"end\":114592,\"start\":114568},{\"attributes\":{\"n\":\"11.\"},\"end\":117751,\"start\":117741},{\"end\":123452,\"start\":123398},{\"end\":124615,\"start\":124569},{\"end\":126745,\"start\":126695},{\"end\":128526,\"start\":128497},{\"end\":130483,\"start\":130441},{\"end\":130686,\"start\":130629},{\"end\":132357,\"start\":132320},{\"end\":134340,\"start\":134318},{\"end\":134463,\"start\":134440},{\"end\":136892,\"start\":136887},{\"end\":138590,\"start\":138565},{\"end\":140001,\"start\":139974},{\"end\":141317,\"start\":141295},{\"end\":147798,\"start\":147770},{\"end\":148380,\"start\":148332},{\"end\":149205,\"start\":149125},{\"end\":150623,\"start\":150596},{\"end\":152928,\"start\":152901},{\"end\":155573,\"start\":155529},{\"end\":157807,\"start\":157738},{\"end\":158696,\"start\":158683},{\"end\":159620,\"start\":159538},{\"end\":161926,\"start\":161884},{\"end\":162594,\"start\":162581},{\"end\":163326,\"start\":163257},{\"end\":164622,\"start\":164543},{\"end\":164966,\"start\":164908},{\"end\":165968,\"start\":165903},{\"end\":166164,\"start\":166154},{\"end\":168161,\"start\":168151},{\"end\":168608,\"start\":168598},{\"end\":169087,\"start\":169077},{\"end\":169865,\"start\":169855}]", "table": "[{\"end\":171673,\"start\":171648},{\"end\":172724,\"start\":171912},{\"end\":172938,\"start\":172913},{\"end\":173136,\"start\":173121},{\"end\":173322,\"start\":173307}]", "figure_caption": "[{\"end\":166720,\"start\":166166},{\"end\":167014,\"start\":166723},{\"end\":167087,\"start\":167017},{\"end\":168149,\"start\":167090},{\"end\":168596,\"start\":168163},{\"end\":168907,\"start\":168610},{\"end\":169075,\"start\":168910},{\"end\":169157,\"start\":169089},{\"end\":169590,\"start\":169160},{\"end\":169853,\"start\":169593},{\"end\":170502,\"start\":169867},{\"end\":170567,\"start\":170505},{\"end\":171032,\"start\":170570},{\"end\":171155,\"start\":171035},{\"end\":171537,\"start\":171158},{\"end\":171648,\"start\":171540},{\"end\":171912,\"start\":171676},{\"end\":172913,\"start\":172727},{\"end\":173121,\"start\":172941},{\"end\":173307,\"start\":173139},{\"end\":173363,\"start\":173325},{\"end\":173425,\"start\":173366}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14772,\"start\":14764},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14910,\"start\":14898},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15483,\"start\":15475},{\"end\":16758,\"start\":16750},{\"end\":18356,\"start\":18348},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":18773,\"start\":18765},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35780,\"start\":35772},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":41332,\"start\":41324},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":42658,\"start\":42650},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":57555,\"start\":57554},{\"end\":60607,\"start\":60599},{\"end\":62821,\"start\":62813},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":64063,\"start\":64055},{\"end\":64958,\"start\":64950},{\"end\":65209,\"start\":65201},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":99073,\"start\":99065},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":99699,\"start\":99691},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":99739,\"start\":99731},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":99810,\"start\":99802},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":100385,\"start\":100377},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":101056,\"start\":101048},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":101103,\"start\":101095},{\"end\":105704,\"start\":105696},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":112359,\"start\":112351},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":113913,\"start\":113905}]", "bib_author_first_name": "[{\"end\":177668,\"start\":177663},{\"end\":177683,\"start\":177678},{\"end\":177698,\"start\":177692},{\"end\":177713,\"start\":177707},{\"end\":177991,\"start\":177990},{\"end\":178004,\"start\":178000},{\"end\":178006,\"start\":178005},{\"end\":178296,\"start\":178291},{\"end\":178304,\"start\":178303},{\"end\":178320,\"start\":178312},{\"end\":178671,\"start\":178662},{\"end\":178685,\"start\":178681},{\"end\":178697,\"start\":178693},{\"end\":178956,\"start\":178955},{\"end\":178971,\"start\":178964},{\"end\":178973,\"start\":178972},{\"end\":178989,\"start\":178981},{\"end\":178991,\"start\":178990},{\"end\":179234,\"start\":179227},{\"end\":179247,\"start\":179241},{\"end\":179419,\"start\":179416},{\"end\":179586,\"start\":179583},{\"end\":179841,\"start\":179838},{\"end\":179855,\"start\":179849},{\"end\":180086,\"start\":180080},{\"end\":180088,\"start\":180087},{\"end\":180111,\"start\":180105},{\"end\":180113,\"start\":180112},{\"end\":180428,\"start\":180419},{\"end\":180691,\"start\":180687},{\"end\":180706,\"start\":180699},{\"end\":180718,\"start\":180714},{\"end\":181069,\"start\":181066},{\"end\":181090,\"start\":181086},{\"end\":181103,\"start\":181100},{\"end\":181118,\"start\":181112},{\"end\":181571,\"start\":181568},{\"end\":181592,\"start\":181588},{\"end\":181605,\"start\":181602},{\"end\":181620,\"start\":181614},{\"end\":181631,\"start\":181627},{\"end\":182193,\"start\":182187},{\"end\":182207,\"start\":182201},{\"end\":182217,\"start\":182213},{\"end\":182625,\"start\":182624},{\"end\":182642,\"start\":182637},{\"end\":182644,\"start\":182643},{\"end\":182676,\"start\":182653},{\"end\":182974,\"start\":182973},{\"end\":182990,\"start\":182989},{\"end\":183023,\"start\":183000},{\"end\":183329,\"start\":183325},{\"end\":183340,\"start\":183336},{\"end\":183354,\"start\":183350},{\"end\":183371,\"start\":183363},{\"end\":183626,\"start\":183620},{\"end\":183860,\"start\":183854},{\"end\":183874,\"start\":183867},{\"end\":184135,\"start\":184134},{\"end\":184322,\"start\":184315},{\"end\":184336,\"start\":184330},{\"end\":184351,\"start\":184344},{\"end\":184365,\"start\":184361},{\"end\":184383,\"start\":184376},{\"end\":184869,\"start\":184863},{\"end\":184885,\"start\":184878},{\"end\":184903,\"start\":184898},{\"end\":185249,\"start\":185241},{\"end\":185270,\"start\":185262},{\"end\":185287,\"start\":185279},{\"end\":185639,\"start\":185631},{\"end\":185660,\"start\":185652},{\"end\":185677,\"start\":185669},{\"end\":185970,\"start\":185961},{\"end\":185991,\"start\":185984},{\"end\":186005,\"start\":186001},{\"end\":186018,\"start\":186011},{\"end\":186296,\"start\":186295},{\"end\":186306,\"start\":186305},{\"end\":186513,\"start\":186512},{\"end\":186777,\"start\":186770},{\"end\":187002,\"start\":186995},{\"end\":187504,\"start\":187499},{\"end\":187782,\"start\":187776},{\"end\":187791,\"start\":187790},{\"end\":188099,\"start\":188093},{\"end\":188113,\"start\":188109},{\"end\":188438,\"start\":188433},{\"end\":188457,\"start\":188452},{\"end\":188469,\"start\":188465},{\"end\":188487,\"start\":188480},{\"end\":188805,\"start\":188802},{\"end\":188824,\"start\":188817},{\"end\":188845,\"start\":188839},{\"end\":189283,\"start\":189279},{\"end\":189297,\"start\":189292},{\"end\":189311,\"start\":189305},{\"end\":189327,\"start\":189322},{\"end\":189706,\"start\":189697},{\"end\":189961,\"start\":189960},{\"end\":189975,\"start\":189969},{\"end\":190228,\"start\":190220},{\"end\":190243,\"start\":190237},{\"end\":190245,\"start\":190244},{\"end\":190261,\"start\":190254},{\"end\":190277,\"start\":190269},{\"end\":190661,\"start\":190655},{\"end\":190675,\"start\":190670},{\"end\":190693,\"start\":190686},{\"end\":190708,\"start\":190702},{\"end\":191021,\"start\":191013},{\"end\":191034,\"start\":191027},{\"end\":191160,\"start\":191153},{\"end\":191179,\"start\":191174},{\"end\":191528,\"start\":191523},{\"end\":191542,\"start\":191537},{\"end\":191824,\"start\":191820},{\"end\":191842,\"start\":191834},{\"end\":191844,\"start\":191843},{\"end\":192058,\"start\":192052},{\"end\":192069,\"start\":192065},{\"end\":192601,\"start\":192595},{\"end\":192620,\"start\":192608},{\"end\":192972,\"start\":192971},{\"end\":192989,\"start\":192982},{\"end\":192991,\"start\":192990},{\"end\":193338,\"start\":193331},{\"end\":193352,\"start\":193345},{\"end\":193607,\"start\":193600},{\"end\":193627,\"start\":193621},{\"end\":193850,\"start\":193845},{\"end\":193875,\"start\":193868},{\"end\":193892,\"start\":193884},{\"end\":193894,\"start\":193893},{\"end\":193909,\"start\":193903},{\"end\":193924,\"start\":193917},{\"end\":194284,\"start\":194280},{\"end\":194297,\"start\":194292},{\"end\":194524,\"start\":194518},{\"end\":194542,\"start\":194535},{\"end\":194551,\"start\":194543},{\"end\":194568,\"start\":194560},{\"end\":194860,\"start\":194851},{\"end\":194876,\"start\":194872},{\"end\":194888,\"start\":194885},{\"end\":194907,\"start\":194899},{\"end\":195252,\"start\":195251},{\"end\":195626,\"start\":195619},{\"end\":195911,\"start\":195904},{\"end\":195927,\"start\":195919},{\"end\":195938,\"start\":195934},{\"end\":196238,\"start\":196231},{\"end\":196248,\"start\":196247},{\"end\":196597,\"start\":196592},{\"end\":196615,\"start\":196608},{\"end\":196897,\"start\":196896},{\"end\":197141,\"start\":197135},{\"end\":197353,\"start\":197344},{\"end\":197368,\"start\":197365},{\"end\":197396,\"start\":197386},{\"end\":197398,\"start\":197397},{\"end\":197753,\"start\":197746},{\"end\":197772,\"start\":197762},{\"end\":197790,\"start\":197785},{\"end\":197807,\"start\":197800},{\"end\":198104,\"start\":198097},{\"end\":198123,\"start\":198113},{\"end\":198142,\"start\":198136},{\"end\":198156,\"start\":198150},{\"end\":198172,\"start\":198167},{\"end\":198455,\"start\":198454},{\"end\":198843,\"start\":198838},{\"end\":198858,\"start\":198852},{\"end\":199123,\"start\":199118},{\"end\":199145,\"start\":199138},{\"end\":199426,\"start\":199425},{\"end\":199428,\"start\":199427},{\"end\":199663,\"start\":199662},{\"end\":199922,\"start\":199913},{\"end\":200131,\"start\":200124},{\"end\":200141,\"start\":200138},{\"end\":200155,\"start\":200148},{\"end\":200461,\"start\":200460},{\"end\":200473,\"start\":200469},{\"end\":200488,\"start\":200483},{\"end\":200790,\"start\":200783},{\"end\":200804,\"start\":200796},{\"end\":200819,\"start\":200811},{\"end\":200833,\"start\":200825},{\"end\":201064,\"start\":201057},{\"end\":201077,\"start\":201070},{\"end\":201091,\"start\":201084},{\"end\":201093,\"start\":201092}]", "bib_author_last_name": "[{\"end\":177676,\"start\":177669},{\"end\":177690,\"start\":177684},{\"end\":177705,\"start\":177699},{\"end\":177722,\"start\":177714},{\"end\":177998,\"start\":177992},{\"end\":178013,\"start\":178007},{\"end\":178020,\"start\":178015},{\"end\":178301,\"start\":178297},{\"end\":178310,\"start\":178305},{\"end\":178328,\"start\":178321},{\"end\":178336,\"start\":178330},{\"end\":178679,\"start\":178672},{\"end\":178691,\"start\":178686},{\"end\":178705,\"start\":178698},{\"end\":178962,\"start\":178957},{\"end\":178979,\"start\":178974},{\"end\":178996,\"start\":178992},{\"end\":179006,\"start\":178998},{\"end\":179239,\"start\":179235},{\"end\":179260,\"start\":179248},{\"end\":179427,\"start\":179420},{\"end\":179594,\"start\":179587},{\"end\":179847,\"start\":179842},{\"end\":179861,\"start\":179856},{\"end\":180103,\"start\":180089},{\"end\":180120,\"start\":180114},{\"end\":180131,\"start\":180122},{\"end\":180441,\"start\":180429},{\"end\":180697,\"start\":180692},{\"end\":180712,\"start\":180707},{\"end\":180723,\"start\":180719},{\"end\":181084,\"start\":181070},{\"end\":181098,\"start\":181091},{\"end\":181110,\"start\":181104},{\"end\":181123,\"start\":181119},{\"end\":181586,\"start\":181572},{\"end\":181600,\"start\":181593},{\"end\":181612,\"start\":181606},{\"end\":181625,\"start\":181621},{\"end\":181635,\"start\":181632},{\"end\":182199,\"start\":182194},{\"end\":182211,\"start\":182208},{\"end\":182222,\"start\":182218},{\"end\":182635,\"start\":182626},{\"end\":182651,\"start\":182645},{\"end\":182683,\"start\":182677},{\"end\":182979,\"start\":182975},{\"end\":182987,\"start\":182981},{\"end\":182998,\"start\":182991},{\"end\":183031,\"start\":183024},{\"end\":183035,\"start\":183033},{\"end\":183334,\"start\":183330},{\"end\":183348,\"start\":183341},{\"end\":183361,\"start\":183355},{\"end\":183377,\"start\":183372},{\"end\":183637,\"start\":183627},{\"end\":183865,\"start\":183861},{\"end\":183880,\"start\":183875},{\"end\":184143,\"start\":184136},{\"end\":184328,\"start\":184323},{\"end\":184342,\"start\":184337},{\"end\":184359,\"start\":184352},{\"end\":184374,\"start\":184366},{\"end\":184389,\"start\":184384},{\"end\":184876,\"start\":184870},{\"end\":184896,\"start\":184886},{\"end\":184907,\"start\":184904},{\"end\":185260,\"start\":185250},{\"end\":185277,\"start\":185271},{\"end\":185300,\"start\":185288},{\"end\":185650,\"start\":185640},{\"end\":185667,\"start\":185661},{\"end\":185690,\"start\":185678},{\"end\":185982,\"start\":185971},{\"end\":185999,\"start\":185992},{\"end\":186009,\"start\":186006},{\"end\":186025,\"start\":186019},{\"end\":186303,\"start\":186297},{\"end\":186317,\"start\":186307},{\"end\":186327,\"start\":186319},{\"end\":186518,\"start\":186514},{\"end\":186526,\"start\":186520},{\"end\":186787,\"start\":186778},{\"end\":187012,\"start\":187003},{\"end\":187511,\"start\":187505},{\"end\":187788,\"start\":187783},{\"end\":187798,\"start\":187792},{\"end\":187805,\"start\":187800},{\"end\":188107,\"start\":188100},{\"end\":188128,\"start\":188114},{\"end\":188137,\"start\":188130},{\"end\":188450,\"start\":188439},{\"end\":188463,\"start\":188458},{\"end\":188478,\"start\":188470},{\"end\":188497,\"start\":188488},{\"end\":188815,\"start\":188806},{\"end\":188837,\"start\":188825},{\"end\":188854,\"start\":188846},{\"end\":189290,\"start\":189284},{\"end\":189303,\"start\":189298},{\"end\":189320,\"start\":189312},{\"end\":189334,\"start\":189328},{\"end\":189712,\"start\":189707},{\"end\":189967,\"start\":189962},{\"end\":189983,\"start\":189976},{\"end\":189992,\"start\":189985},{\"end\":190235,\"start\":190229},{\"end\":190252,\"start\":190246},{\"end\":190267,\"start\":190262},{\"end\":190284,\"start\":190278},{\"end\":190668,\"start\":190662},{\"end\":190684,\"start\":190676},{\"end\":190700,\"start\":190694},{\"end\":190715,\"start\":190709},{\"end\":191025,\"start\":191022},{\"end\":191040,\"start\":191035},{\"end\":191172,\"start\":191161},{\"end\":191188,\"start\":191180},{\"end\":191535,\"start\":191529},{\"end\":191549,\"start\":191543},{\"end\":191832,\"start\":191825},{\"end\":191850,\"start\":191845},{\"end\":192063,\"start\":192059},{\"end\":192078,\"start\":192070},{\"end\":192606,\"start\":192602},{\"end\":192626,\"start\":192621},{\"end\":192969,\"start\":192955},{\"end\":192980,\"start\":192973},{\"end\":192995,\"start\":192992},{\"end\":193002,\"start\":192997},{\"end\":193343,\"start\":193339},{\"end\":193357,\"start\":193353},{\"end\":193619,\"start\":193608},{\"end\":193633,\"start\":193628},{\"end\":193866,\"start\":193851},{\"end\":193882,\"start\":193876},{\"end\":193901,\"start\":193895},{\"end\":193915,\"start\":193910},{\"end\":193930,\"start\":193925},{\"end\":194290,\"start\":194285},{\"end\":194305,\"start\":194298},{\"end\":194533,\"start\":194525},{\"end\":194558,\"start\":194552},{\"end\":194575,\"start\":194569},{\"end\":194870,\"start\":194861},{\"end\":194883,\"start\":194877},{\"end\":194897,\"start\":194889},{\"end\":194916,\"start\":194908},{\"end\":195258,\"start\":195253},{\"end\":195264,\"start\":195260},{\"end\":195271,\"start\":195266},{\"end\":195632,\"start\":195627},{\"end\":195917,\"start\":195912},{\"end\":195932,\"start\":195928},{\"end\":195944,\"start\":195939},{\"end\":196245,\"start\":196239},{\"end\":196257,\"start\":196249},{\"end\":196266,\"start\":196259},{\"end\":196606,\"start\":196598},{\"end\":196621,\"start\":196616},{\"end\":196904,\"start\":196898},{\"end\":196914,\"start\":196906},{\"end\":197147,\"start\":197142},{\"end\":197363,\"start\":197354},{\"end\":197384,\"start\":197369},{\"end\":197406,\"start\":197399},{\"end\":197416,\"start\":197408},{\"end\":197760,\"start\":197754},{\"end\":197783,\"start\":197773},{\"end\":197798,\"start\":197791},{\"end\":197815,\"start\":197808},{\"end\":198111,\"start\":198105},{\"end\":198134,\"start\":198124},{\"end\":198148,\"start\":198143},{\"end\":198165,\"start\":198157},{\"end\":198180,\"start\":198173},{\"end\":198465,\"start\":198456},{\"end\":198473,\"start\":198467},{\"end\":198850,\"start\":198844},{\"end\":198867,\"start\":198859},{\"end\":199136,\"start\":199124},{\"end\":199151,\"start\":199146},{\"end\":199668,\"start\":199664},{\"end\":199682,\"start\":199670},{\"end\":199927,\"start\":199923},{\"end\":200136,\"start\":200132},{\"end\":200146,\"start\":200142},{\"end\":200159,\"start\":200156},{\"end\":200458,\"start\":200440},{\"end\":200467,\"start\":200462},{\"end\":200481,\"start\":200474},{\"end\":200494,\"start\":200489},{\"end\":200502,\"start\":200496},{\"end\":200794,\"start\":200791},{\"end\":200809,\"start\":200805},{\"end\":200823,\"start\":200820},{\"end\":200837,\"start\":200834},{\"end\":201068,\"start\":201065},{\"end\":201082,\"start\":201078},{\"end\":201101,\"start\":201094}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":177183,\"start\":177059},{\"attributes\":{\"id\":\"b1\"},\"end\":177597,\"start\":177185},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8680614},\"end\":177914,\"start\":177599},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2827580},\"end\":178239,\"start\":177916},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":621415},\"end\":178581,\"start\":178241},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53988253},\"end\":178884,\"start\":178583},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4661601},\"end\":179204,\"start\":178886},{\"attributes\":{\"id\":\"b7\"},\"end\":179380,\"start\":179206},{\"attributes\":{\"id\":\"b8\"},\"end\":179495,\"start\":179382},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":62729017},\"end\":179766,\"start\":179497},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3640534},\"end\":180042,\"start\":179768},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":122429869},\"end\":180324,\"start\":180044},{\"attributes\":{\"id\":\"b12\"},\"end\":180600,\"start\":180326},{\"attributes\":{\"doi\":\"arXiv:1804.08646\",\"id\":\"b13\"},\"end\":180920,\"start\":180602},{\"attributes\":{\"id\":\"b14\"},\"end\":181512,\"start\":180922},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3228123},\"end\":182080,\"start\":181514},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2367610},\"end\":182505,\"start\":182082},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":21877334},\"end\":182914,\"start\":182507},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":19197858},\"end\":183230,\"start\":182916},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12422343},\"end\":183581,\"start\":183232},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":115907200},\"end\":183759,\"start\":183583},{\"attributes\":{\"doi\":\"arXiv:1901.03209\",\"id\":\"b21\"},\"end\":184074,\"start\":183761},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":120017042},\"end\":184285,\"start\":184076},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13496699},\"end\":184752,\"start\":184287},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":85348798},\"end\":185137,\"start\":184754},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14322788},\"end\":185574,\"start\":185139},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":30500829},\"end\":185887,\"start\":185576},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":41440816},\"end\":186243,\"start\":185889},{\"attributes\":{\"id\":\"b28\"},\"end\":186460,\"start\":186245},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":145263225},\"end\":186666,\"start\":186462},{\"attributes\":{\"id\":\"b30\"},\"end\":186930,\"start\":186668},{\"attributes\":{\"doi\":\"https:/amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500830\",\"id\":\"b31\",\"matched_paper_id\":123205318},\"end\":187401,\"start\":186932},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10727333},\"end\":187748,\"start\":187403},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":115693616},\"end\":187986,\"start\":187750},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":18005801},\"end\":188389,\"start\":187988},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":28116257},\"end\":188738,\"start\":188391},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12845273},\"end\":189228,\"start\":188740},{\"attributes\":{\"id\":\"b37\"},\"end\":189566,\"start\":189230},{\"attributes\":{\"id\":\"b38\"},\"end\":189930,\"start\":189568},{\"attributes\":{\"id\":\"b39\"},\"end\":190131,\"start\":189932},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1262561},\"end\":190586,\"start\":190133},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":11259771},\"end\":190975,\"start\":190588},{\"attributes\":{\"id\":\"b42\"},\"end\":191130,\"start\":190977},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1231300},\"end\":191432,\"start\":191132},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":8357129},\"end\":191778,\"start\":191434},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":28537158},\"end\":192022,\"start\":191780},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":19236490},\"end\":192528,\"start\":192024},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":9411471},\"end\":192831,\"start\":192530},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":15019019},\"end\":193247,\"start\":192833},{\"attributes\":{\"doi\":\"arXiv:1703.07870\",\"id\":\"b49\"},\"end\":193542,\"start\":193249},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":144711558},\"end\":193779,\"start\":193544},{\"attributes\":{\"id\":\"b51\"},\"end\":194253,\"start\":193781},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1123458},\"end\":194411,\"start\":194255},{\"attributes\":{\"id\":\"b53\"},\"end\":194768,\"start\":194413},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":53454937},\"end\":195140,\"start\":194770},{\"attributes\":{\"id\":\"b55\"},\"end\":195503,\"start\":195142},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":182656421},\"end\":195842,\"start\":195505},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":53301419},\"end\":196133,\"start\":195844},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":84486196},\"end\":196472,\"start\":196135},{\"attributes\":{\"doi\":\"arXiv:1908.01755\",\"id\":\"b59\"},\"end\":196843,\"start\":196474},{\"attributes\":{\"id\":\"b60\"},\"end\":197027,\"start\":196845},{\"attributes\":{\"id\":\"b61\"},\"end\":197286,\"start\":197029},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":16451329},\"end\":197653,\"start\":197288},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":8529157},\"end\":198043,\"start\":197655},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":8397120},\"end\":198384,\"start\":198045},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":2511280},\"end\":198747,\"start\":198386},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":8552703},\"end\":199051,\"start\":198749},{\"attributes\":{\"doi\":\"arXiv:1407.1097\",\"id\":\"b67\"},\"end\":199322,\"start\":199053},{\"attributes\":{\"id\":\"b68\"},\"end\":199613,\"start\":199324},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":24507448},\"end\":199853,\"start\":199615},{\"attributes\":{\"id\":\"b70\"},\"end\":200026,\"start\":199855},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":13972555},\"end\":200358,\"start\":200028},{\"attributes\":{\"id\":\"b72\"},\"end\":200715,\"start\":200360},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":14544818},\"end\":201025,\"start\":200717},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":16671201},\"end\":201312,\"start\":201027}]", "bib_title": "[{\"end\":177115,\"start\":177059},{\"end\":177661,\"start\":177599},{\"end\":177988,\"start\":177916},{\"end\":178289,\"start\":178241},{\"end\":178660,\"start\":178583},{\"end\":178953,\"start\":178886},{\"end\":179581,\"start\":179497},{\"end\":179836,\"start\":179768},{\"end\":180078,\"start\":180044},{\"end\":181566,\"start\":181514},{\"end\":182185,\"start\":182082},{\"end\":182622,\"start\":182507},{\"end\":182971,\"start\":182916},{\"end\":183323,\"start\":183232},{\"end\":183618,\"start\":183583},{\"end\":184132,\"start\":184076},{\"end\":184313,\"start\":184287},{\"end\":184861,\"start\":184754},{\"end\":185239,\"start\":185139},{\"end\":185629,\"start\":185576},{\"end\":185959,\"start\":185889},{\"end\":186510,\"start\":186462},{\"end\":186993,\"start\":186932},{\"end\":187497,\"start\":187403},{\"end\":187774,\"start\":187750},{\"end\":188091,\"start\":187988},{\"end\":188431,\"start\":188391},{\"end\":188800,\"start\":188740},{\"end\":190218,\"start\":190133},{\"end\":190653,\"start\":190588},{\"end\":191151,\"start\":191132},{\"end\":191521,\"start\":191434},{\"end\":191818,\"start\":191780},{\"end\":192050,\"start\":192024},{\"end\":192593,\"start\":192530},{\"end\":192953,\"start\":192833},{\"end\":193598,\"start\":193544},{\"end\":194278,\"start\":194255},{\"end\":194849,\"start\":194770},{\"end\":195249,\"start\":195142},{\"end\":195617,\"start\":195505},{\"end\":195902,\"start\":195844},{\"end\":196229,\"start\":196135},{\"end\":197342,\"start\":197288},{\"end\":197744,\"start\":197655},{\"end\":198095,\"start\":198045},{\"end\":198452,\"start\":198386},{\"end\":198836,\"start\":198749},{\"end\":199660,\"start\":199615},{\"end\":200122,\"start\":200028},{\"end\":200781,\"start\":200717},{\"end\":201055,\"start\":201027}]", "bib_author": "[{\"end\":177678,\"start\":177663},{\"end\":177692,\"start\":177678},{\"end\":177707,\"start\":177692},{\"end\":177724,\"start\":177707},{\"end\":178000,\"start\":177990},{\"end\":178015,\"start\":178000},{\"end\":178022,\"start\":178015},{\"end\":178303,\"start\":178291},{\"end\":178312,\"start\":178303},{\"end\":178330,\"start\":178312},{\"end\":178338,\"start\":178330},{\"end\":178681,\"start\":178662},{\"end\":178693,\"start\":178681},{\"end\":178707,\"start\":178693},{\"end\":178964,\"start\":178955},{\"end\":178981,\"start\":178964},{\"end\":178998,\"start\":178981},{\"end\":179008,\"start\":178998},{\"end\":179241,\"start\":179227},{\"end\":179262,\"start\":179241},{\"end\":179429,\"start\":179416},{\"end\":179596,\"start\":179583},{\"end\":179849,\"start\":179838},{\"end\":179863,\"start\":179849},{\"end\":180105,\"start\":180080},{\"end\":180122,\"start\":180105},{\"end\":180133,\"start\":180122},{\"end\":180443,\"start\":180419},{\"end\":180699,\"start\":180687},{\"end\":180714,\"start\":180699},{\"end\":180725,\"start\":180714},{\"end\":181086,\"start\":181066},{\"end\":181100,\"start\":181086},{\"end\":181112,\"start\":181100},{\"end\":181125,\"start\":181112},{\"end\":181588,\"start\":181568},{\"end\":181602,\"start\":181588},{\"end\":181614,\"start\":181602},{\"end\":181627,\"start\":181614},{\"end\":181637,\"start\":181627},{\"end\":182201,\"start\":182187},{\"end\":182213,\"start\":182201},{\"end\":182224,\"start\":182213},{\"end\":182637,\"start\":182624},{\"end\":182653,\"start\":182637},{\"end\":182685,\"start\":182653},{\"end\":182981,\"start\":182973},{\"end\":182989,\"start\":182981},{\"end\":183000,\"start\":182989},{\"end\":183033,\"start\":183000},{\"end\":183037,\"start\":183033},{\"end\":183336,\"start\":183325},{\"end\":183350,\"start\":183336},{\"end\":183363,\"start\":183350},{\"end\":183379,\"start\":183363},{\"end\":183639,\"start\":183620},{\"end\":183867,\"start\":183854},{\"end\":183882,\"start\":183867},{\"end\":184145,\"start\":184134},{\"end\":184330,\"start\":184315},{\"end\":184344,\"start\":184330},{\"end\":184361,\"start\":184344},{\"end\":184376,\"start\":184361},{\"end\":184391,\"start\":184376},{\"end\":184878,\"start\":184863},{\"end\":184898,\"start\":184878},{\"end\":184909,\"start\":184898},{\"end\":185262,\"start\":185241},{\"end\":185279,\"start\":185262},{\"end\":185302,\"start\":185279},{\"end\":185652,\"start\":185631},{\"end\":185669,\"start\":185652},{\"end\":185692,\"start\":185669},{\"end\":185984,\"start\":185961},{\"end\":186001,\"start\":185984},{\"end\":186011,\"start\":186001},{\"end\":186027,\"start\":186011},{\"end\":186305,\"start\":186295},{\"end\":186319,\"start\":186305},{\"end\":186329,\"start\":186319},{\"end\":186520,\"start\":186512},{\"end\":186528,\"start\":186520},{\"end\":186789,\"start\":186770},{\"end\":187014,\"start\":186995},{\"end\":187513,\"start\":187499},{\"end\":187790,\"start\":187776},{\"end\":187800,\"start\":187790},{\"end\":187807,\"start\":187800},{\"end\":188109,\"start\":188093},{\"end\":188130,\"start\":188109},{\"end\":188139,\"start\":188130},{\"end\":188452,\"start\":188433},{\"end\":188465,\"start\":188452},{\"end\":188480,\"start\":188465},{\"end\":188499,\"start\":188480},{\"end\":188817,\"start\":188802},{\"end\":188839,\"start\":188817},{\"end\":188856,\"start\":188839},{\"end\":189292,\"start\":189279},{\"end\":189305,\"start\":189292},{\"end\":189322,\"start\":189305},{\"end\":189336,\"start\":189322},{\"end\":189714,\"start\":189697},{\"end\":189969,\"start\":189960},{\"end\":189985,\"start\":189969},{\"end\":189994,\"start\":189985},{\"end\":190237,\"start\":190220},{\"end\":190254,\"start\":190237},{\"end\":190269,\"start\":190254},{\"end\":190286,\"start\":190269},{\"end\":190670,\"start\":190655},{\"end\":190686,\"start\":190670},{\"end\":190702,\"start\":190686},{\"end\":190717,\"start\":190702},{\"end\":191027,\"start\":191013},{\"end\":191042,\"start\":191027},{\"end\":191174,\"start\":191153},{\"end\":191190,\"start\":191174},{\"end\":191537,\"start\":191523},{\"end\":191551,\"start\":191537},{\"end\":191834,\"start\":191820},{\"end\":191852,\"start\":191834},{\"end\":192065,\"start\":192052},{\"end\":192080,\"start\":192065},{\"end\":192608,\"start\":192595},{\"end\":192628,\"start\":192608},{\"end\":192971,\"start\":192955},{\"end\":192982,\"start\":192971},{\"end\":192997,\"start\":192982},{\"end\":193004,\"start\":192997},{\"end\":193345,\"start\":193331},{\"end\":193359,\"start\":193345},{\"end\":193621,\"start\":193600},{\"end\":193635,\"start\":193621},{\"end\":193868,\"start\":193845},{\"end\":193884,\"start\":193868},{\"end\":193903,\"start\":193884},{\"end\":193917,\"start\":193903},{\"end\":193932,\"start\":193917},{\"end\":194292,\"start\":194280},{\"end\":194307,\"start\":194292},{\"end\":194535,\"start\":194518},{\"end\":194560,\"start\":194535},{\"end\":194577,\"start\":194560},{\"end\":194872,\"start\":194851},{\"end\":194885,\"start\":194872},{\"end\":194899,\"start\":194885},{\"end\":194918,\"start\":194899},{\"end\":195260,\"start\":195251},{\"end\":195266,\"start\":195260},{\"end\":195273,\"start\":195266},{\"end\":195634,\"start\":195619},{\"end\":195919,\"start\":195904},{\"end\":195934,\"start\":195919},{\"end\":195946,\"start\":195934},{\"end\":196247,\"start\":196231},{\"end\":196259,\"start\":196247},{\"end\":196268,\"start\":196259},{\"end\":196608,\"start\":196592},{\"end\":196623,\"start\":196608},{\"end\":196906,\"start\":196896},{\"end\":196916,\"start\":196906},{\"end\":197149,\"start\":197135},{\"end\":197365,\"start\":197344},{\"end\":197386,\"start\":197365},{\"end\":197408,\"start\":197386},{\"end\":197418,\"start\":197408},{\"end\":197762,\"start\":197746},{\"end\":197785,\"start\":197762},{\"end\":197800,\"start\":197785},{\"end\":197817,\"start\":197800},{\"end\":198113,\"start\":198097},{\"end\":198136,\"start\":198113},{\"end\":198150,\"start\":198136},{\"end\":198167,\"start\":198150},{\"end\":198182,\"start\":198167},{\"end\":198467,\"start\":198454},{\"end\":198475,\"start\":198467},{\"end\":198852,\"start\":198838},{\"end\":198869,\"start\":198852},{\"end\":199138,\"start\":199118},{\"end\":199153,\"start\":199138},{\"end\":199431,\"start\":199425},{\"end\":199670,\"start\":199662},{\"end\":199684,\"start\":199670},{\"end\":199929,\"start\":199913},{\"end\":200138,\"start\":200124},{\"end\":200148,\"start\":200138},{\"end\":200161,\"start\":200148},{\"end\":200460,\"start\":200440},{\"end\":200469,\"start\":200460},{\"end\":200483,\"start\":200469},{\"end\":200496,\"start\":200483},{\"end\":200504,\"start\":200496},{\"end\":200796,\"start\":200783},{\"end\":200811,\"start\":200796},{\"end\":200825,\"start\":200811},{\"end\":200839,\"start\":200825},{\"end\":201070,\"start\":201057},{\"end\":201084,\"start\":201070},{\"end\":201103,\"start\":201084}]", "bib_venue": "[{\"end\":181820,\"start\":181737},{\"end\":184532,\"start\":184470},{\"end\":192283,\"start\":192190},{\"end\":177120,\"start\":177117},{\"end\":177367,\"start\":177185},{\"end\":177738,\"start\":177724},{\"end\":178062,\"start\":178022},{\"end\":178396,\"start\":178338},{\"end\":178718,\"start\":178707},{\"end\":179029,\"start\":179008},{\"end\":179225,\"start\":179206},{\"end\":179414,\"start\":179382},{\"end\":179615,\"start\":179596},{\"end\":179890,\"start\":179863},{\"end\":180165,\"start\":180133},{\"end\":180417,\"start\":180326},{\"end\":180685,\"start\":180602},{\"end\":181064,\"start\":180922},{\"end\":181735,\"start\":181637},{\"end\":182273,\"start\":182224},{\"end\":182695,\"start\":182685},{\"end\":183059,\"start\":183037},{\"end\":183387,\"start\":183379},{\"end\":183657,\"start\":183639},{\"end\":183852,\"start\":183761},{\"end\":184167,\"start\":184145},{\"end\":184468,\"start\":184391},{\"end\":184929,\"start\":184909},{\"end\":185342,\"start\":185302},{\"end\":185716,\"start\":185692},{\"end\":186051,\"start\":186027},{\"end\":186293,\"start\":186245},{\"end\":186551,\"start\":186528},{\"end\":186768,\"start\":186668},{\"end\":187129,\"start\":187082},{\"end\":187562,\"start\":187513},{\"end\":187854,\"start\":187807},{\"end\":188172,\"start\":188139},{\"end\":188548,\"start\":188499},{\"end\":188976,\"start\":188856},{\"end\":189277,\"start\":189230},{\"end\":189695,\"start\":189568},{\"end\":189958,\"start\":189932},{\"end\":190342,\"start\":190286},{\"end\":190766,\"start\":190717},{\"end\":191011,\"start\":190977},{\"end\":191266,\"start\":191190},{\"end\":191591,\"start\":191551},{\"end\":191888,\"start\":191852},{\"end\":192188,\"start\":192080},{\"end\":192668,\"start\":192628},{\"end\":193024,\"start\":193004},{\"end\":193329,\"start\":193249},{\"end\":193646,\"start\":193635},{\"end\":193843,\"start\":193781},{\"end\":194318,\"start\":194307},{\"end\":194516,\"start\":194413},{\"end\":194938,\"start\":194918},{\"end\":195304,\"start\":195273},{\"end\":195661,\"start\":195634},{\"end\":195973,\"start\":195946},{\"end\":196288,\"start\":196268},{\"end\":196590,\"start\":196474},{\"end\":196894,\"start\":196845},{\"end\":197133,\"start\":197029},{\"end\":197454,\"start\":197418},{\"end\":197835,\"start\":197817},{\"end\":198200,\"start\":198182},{\"end\":198556,\"start\":198475},{\"end\":198883,\"start\":198869},{\"end\":199116,\"start\":199053},{\"end\":199423,\"start\":199324},{\"end\":199726,\"start\":199684},{\"end\":199911,\"start\":199855},{\"end\":200179,\"start\":200161},{\"end\":200438,\"start\":200360},{\"end\":200855,\"start\":200839},{\"end\":201150,\"start\":201103}]"}}}, "year": 2023, "month": 12, "day": 17}