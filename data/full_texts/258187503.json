{"id": 258187503, "updated": "2023-10-24 13:01:08.532", "metadata": {"title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling", "authors": "[{\"first\":\"Xiuying\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Yunchen\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yuhang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xiangguo\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ruihao\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Jinyang\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Xianglong\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Quantization of transformer language models faces signi\ufb01cant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in speci\ufb01c channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-\ufb02oating-point performance on both small models, such as BERT, and large language models (LLMs) including OPTs, BLOOM, and BLOOMZ at 8-bit and 6-bit settings. Furthermore, we establish a new state of the art for 4-bit BERT.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2304-09145", "doi": "10.48550/arxiv.2304.09145"}}, "content": {"source": {"pdf_hash": "81051b830a4f5606106765902a51ba281c9230f9", "pdf_src": "ScienceParsePlus", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.09145v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ceb2728ed30547fe4035f3eccc164b39761640cf", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/81051b830a4f5606106765902a51ba281c9230f9.txt", "contents": "\nOutlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\n\n\nXiuying Wei weixiuying966@gmail.com \nState Key Lab of Software Development Environment\nBeihang University\n\n\nSenseTime Research\n\n\nYunchen Zhang zhangyunchen@sensetime.com \nSenseTime Research\n\n\nUniversity of Electronic Science and Technology of China\n\n\nYuhang Li yuhang.li@yale.edu \nYale University\n\n\nXiangguo Zhang zhangxiangguo@sensetime.com \nSenseTime Research\n\n\nRuihao Gong gongruihao@sensetime.com \nSenseTime Research\n\n\nJinyang Guo jinyangguo@buaa.edu.cn \nState Key Lab of Software Development Environment\nBeihang University\n\n\nXianglong Liu xlliu@buaa.edu.cn \nState Key Lab of Software Development Environment\nBeihang University\n\n\nOutlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\nC28E2ADA3BF2524B5830BB4E69AFA6A2\nQuantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations.We observe that these outliers are asymmetric and concentrated in specific channels.To address this issue, we propose the Outlier Suppres-sion+ framework.First, we introduce channelwise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels.We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence.Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer.Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings.Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models, such as BERT, and large language models (LLMs) including OPTs, BLOOM, and BLOOMZ at 8-bit and 6bit settings.Furthermore, we establish a new state of the art for 4-bit BERT.\n\nIntroduction\n\nTransformer language models (e.g., BERT, LLMs) have garnered significant attention due to their remarkable performance and scalable model size.These models have evolved from hundreds of millions of parameters (Devlin et al., 2018;Liu et al., 2019;Radford et al., 2018) to hundreds of billions of parameters (Brown et al., 2020;Zhang et al., 2022;Smith et al., 2022).This necessitates the employment of compression techniques (Han et al., 2015;Hinton et al., 2015;Zoph and Le, 2016;Le-Cun et al., 1989) for practical deployment.Among these techniques, quantization (Jacob et al., 2018) has emerged as a general and primary paradigm for reducing both memory footprint and computational overhead.\n\nHowever, quantization, particularly post-training quantization (PTQ) (Choukroun et al., 2019;Banner et al., 2018;Nagel et al., 2020), which operates with limited data and GPU resources, has become increasingly challenging for these models (e.g., a 12% accuracy drop in BERT (Bondarenko et al., 2021) and catastrophic degradation in OPT-175B (Dettmers et al., 2022)).This is caused by the presence of detrimental outliers in activation (e.g., the width of distribution can be 80 for BERT and even 140 for OPTs), which prevents quantization from accurately expressing floating-point numbers.\n\nTo combat the outlier bottleneck in quantization, researchers make in-depth investigations and find the concentration phenomenon that outliers typically appear in certain channels.Hence, some works (Bondarenko et al., 2021;Dettmers et al., 2022) explored various fine-grained quantization schemes to provide additional consideration for them while some (Wei et al., 2022b;Xiao et al., 2022) attempted to diminish them through a scaling operation, which can be migrated to subsequent modules to maintain floating-point (FP) equivalence.However, we find that they ignore the asymmetric property of outliers that relatively small channel ranges can result in a significantly large range for the entire tensor (e.g., as shown in Fig. 1a, a single channel on OPT-66B occupies the axis from -97 to -58 with a size of 39, but the total size can reach up to 140).In the meantime, existing work did not quantitatively analyze the effect of activation scaling on the quantization of subsequent layer's weight well.\n\nIn this paper, we propose the Outlier Suppres-sion+ framework to suppress outliers under static and standard quantization settings.First, to eliminate asymmetric presentation and effectively deal  1a shows the original distribution with asymmetric outliers consistently occurs at certain channels, owning considerable range (-97, 43).Fig. 1b depicts the channel-wise shifting operation to decrease the tensor range by eliminating the asymmetry.Fig. 1c further scales down the outliers to threshold 5 and finally results in a distribution ranging from -5 to 5.\n\nwith channels full of outliers, we introduce channelwise shifting and scaling operations that adjust and scale down problematic activation signals while transferring both the reversed effects to subsequent modules for an equivalent FP network.Second, regarding finding optimal values for these operations, our proposed shifting technique can align the center of distribution across channels, making the whole tensor range determined by the maximum channel range instead of asymmetric outliers.Then, we devise reasonable scaling factors that achieve a balanced quantization burden between activation and following weights by considering their interactive results.\n\nOur algorithm can be carried out efficiently and enjoy affordability on real hardware, producing more quantization-friendly models in minutes and requiring no extra inference burden on LLMs.\n\nOur main contributions can be summarized in three aspects: 1. Focusing on the asymmetric attribute of outliers and their consistent emergence at specific channels, we develop channel-wise shifting and scaling operations to suppress them.To establish an equivalent FP network, we design a unified migration pattern that transfers reversed effects to succeeding modules.2. We introduce an efficient and lightweight technique to determine optimal shifting and scaling values for more quantization-friendly models.\n\nThe technique first eliminates the asymmetric exhibition and then scales down outliers with a balanced quantization burden between challeng-ing activation and subsequent weights in terms of their interplay.3. We assess the efficacy of our approach on both small models (BERT) and LLMs (OPTs, BLOOM, and BLOOMZ) under the PTQ background.Experimental results across a diverse range of datasets, such as the GLUE benchmark and multiple zero-shot tasks, demonstrate that our method achieves near-floating point performance in 8-bit and 6-bit cases.Notably, our approach establishes the state of the art for 4-bit BERT models with 15.5% improvement.\n\n\nPreliminary\n\nBasic Notations.We denote matrices as upper case letters (e.g., X) and vectors as lower case letters (e.g., x).Operator and represent elementwise multiplication on matrices or vectors.We use W X as matrix-matrix multiplication.Furthermore, X t,j refers to the element of the t-th token and the j-th embedding or channel in transformer models.Q(\u2022) denotes the quantization function.\n\nOutlier Suppression.Wei et al. (2022b) describes an outlier suppression framework to suppress the notorious outliers.Observing that outliers concentrate in certain channels, a component of the framework called Gamma Migration adopts a scaling vector for scaling outliers and migrates it to later modules.However, this approach still wastes a large portion of the quantization levels on the extreme asymmetric shape of outliers.Also, it simply utilizes the scaling parameter in LayerNorm (LN), namely gamma, to weaken the outliers.This fixed value is not always desirable without the perception of quantization and quantitative evaluation of the migrated burden on later weight.\n\n\nMethod\n\nIn this section, we first present our equivalent shifting and scaling operations, which mitigate outliers in activation and recover equivalent FP models by updating subsequent modules.We then introduce optimal shifting and scaling factors.\n\n\nEquivalent shifting and scaling\n\nIn this section, we consider the features of outliers and design shifting and scaling operations followed by a unified migration pattern.\n\n\nOutlier shifting and scaling\n\nChannel-wise shifting.In transformers like LLMs, outliers exhibit asymmetric behavior.Recall that in Fig. 1a, the 8725-th channel displays a hard negative interval (-97, -58), while another channel dominates hard positive representations (5.7, 43).Due to this asymmetry, even if the range of each channel is relatively small, such as 40 and 38 for channels containing outliers and minuscule values for normal channels, the range of the entire tensor can swell to a considerably large value (e.g., 140, ranging from -97 to 43), which negatively affects quantization performance.\n\nConsidering the tensor range subjected by asymmetric outliers, we are motivated to eliminate the impact of asymmetry by taking the following operation:\nX = X \u2212 z,(1)\nwhere z serves as a row vector (z \u2208 R n ).The calculation of z will be introduced later in Sec.3.2.1.Then, a new tensor X is generated by aligning centers of different channels together, contributing to a much smaller tensor range and thus better quantization performance.For instance, from Fig. 1b, we can find that the distribution width has been reduced to 40 from a significant number of 140 after channel-wise shifting.\n\nChannel-wise scaling.In addition to outliers manifesting as hard negative or positive signals, they also predominantly accumulate in specific channels over various inputs, such as the 8725-th and the 6354-th channels in Fig. 1a.These channels exhibit more aggressive values than others.Therefore, after channel-wise shifting, we propose to narrow them down to further ease the quantization difficulty via scaling factors.\n\nX = (X \u2212 z) s.\n\n(2)\n\nIn the above equation, the row vector s \u2208 R n would additionally shrink the shifted tensor and bring the final quantization-friendly activation X (e.g., in Fig. 1c, a tensor with a size of 10 can be acquired after scaling down the signals over 5).Optimal calculation of s will be given in Sec.3.2.2.Implementation.It is easy to implement these operations.For example, to shift and scale the output of LayerNorm, we only need to replace its linear transformation parameters \u03b2 and \u03b3 with (\u03b2\u2212z) s and \u03b3 s as shown in Fig. 2. For other outputs, by updating parameters in the former DeQuant function, the shifted and scaled activation can be obtained.\n\n\nUnified migration pattern\n\nAs mentioned in Eq. ( 1) and Eq. ( 2), we subtract z and divide s to make the problematic activation resilient to quantization.To keep an equivalent FP model, a unified migration pattern is proposed that transfers both reversed shifting and scaling vectors to subsequent modules.We demonstrate the feasibility of this algorithm on two common structures.Linear Layer.First, we consider a prevalent scenario where a linear (convolutional) layer immediately follows.Reversing the above operations (i.e., ( X s + z)W + b) equals to updating the W \u2208 R m,n and b \u2208 R m in the next layer, given by\n( X s + z)W + b = ( X s)W + zW + b = X(W s ) + (zW + b).(3)\nAccording to Eq. ( 3), weight and bias can absorb s and z, respectively, and thus becomes:\nW = W \uf8ee \uf8ef \uf8ef \uf8f0 s 1 s 2 ... s n s 1 s 2 ... s n ... s 1 s 2 ... s n \uf8f9 \uf8fa \uf8fa \uf8fb , b = zW + b.(4)\nTake the typical challenging activation (output of LayerNorm) as an example here, Fig. 2(a) depicts that for attention structure, all following weights and biases can absorb the shifting and scaling signals without any extra computation burden.Residual connection.Second, we consider the case where a residual connection is applied after the LayerNorm structure (Post-LN) and fed into the quantized input.As shown in Fig. 2b, in addition to linear layer transformation, the identity function will be substituted with channel-wise multiplication and addition to maintain equivalence.We demonstrate that these increased calculations will only incur a negligible inference burden in Sec.4.4.\n\nFinally, because s and z serve as shared parameters across tokens and batches of data, the unified migration pattern can be implemented successfully and produce an equivalent FP model without additional computation most of the time.\n\n\nOptimal shifting and scaling\n\nBased on the equivalent shifting and scaling operations, in this section, we describe the way to efficiently and effectively obtain optimal solutions.An optimal shifting vector could solve the asymmetry of outliers while scaling factors further scale down outliers and bring marginal impact on the subsequent weights.\n\n\nShifting values\n\nTo eliminate the impact of asymmetry, we propose to align the center of each channel to 0 so that the range of the whole tensor reduces to the largest channel range (Fig. 1b, Fig. 2), getting rid of defined by asymmetric outliers.In this way, shifting values can be computed as the average of the minimum and maximum signals for each channel, defined by: z j = max(X :,j ) + min(X :,j ) 2 .\n\n(5)\n\n\nScaling factors\n\nChallenges.Compared to shifting vectors, computing optimal scaling factors is more difficult because it also scales weight in the next layer, potentially leading to inferior weight quantization.Despite this, an appropriate scaling factor can still be useful because of the considerably narrower range weight holds (e.g., < 1 magnitude order) as opposed to activation with extreme outliers, making the enlargement of the weight range relatively small.We propose optimizing scaling factors that adequately suppress activation outliers while not compromising weight quantization error to reach a balanced quantization burden.The following sections first explain the designed optimization objective and then describe the optimization procedure.Optimization objective.When calculating a scaling vector, previous works (Wei et al., 2022b;Xiao et al., 2022) either ignore the affected following weight or simply equalize ranges of activation and weight.Unlike them, we think the key point is the involvement of scaled and quantized activation and weight in a neural network and introduce a loss-aware paradigm.\n\nWe first study the simple case that the problematic activation acts as the input of one linear layer (e.g., Fig. 2b).Instead of minimizing quantization errors of activation and weight separately (i.e.,\nmin s E Q((X \u2212 z) s) \u2212 (X \u2212 z) s 2 F and min s E Q(W s) \u2212 W s 2\nF ), a task loss perspective is adopted by concerning their matrix multiplication output.We measure the output change after scaling and quantizing weight and activation to pursue optimal factors, given by:\nmin s E[ Q((X \u2212 z) s)Q(W s) + b\noutput after scaling and quantization\n\u2212 (XW + b) original FP output 2 F ],(6)\nwhere the mean squared error (MSE) is used to quantify the difference.\n\nMultiple linear layers: Furthermore, we study the case where the troublesome activation becomes the input of multiple linear layers such as the attention structure (Fig. 2a).\n\nIn this scenario, three weights will be multiplied by the same scaling vector and calculated with the same suppressed activation.We mark the three outputs of matrix multiplication produced by scaled and quantized matrices as Q q , K q , V q , (Original outputs are denoted as Q, K, V ).Applying Eq. ( 6) to three linear layers separately and simply summing the losses can make it difficult to illustrate their different importance and usages.Therefore, we employ the attention mechanism as a post-process function to reasonably organize their scaling and quantization information as follows:\nmin s E[ softmax( Qq K q ) Vq \u2212 softmax(QK )V 2 F ].(7)\nNormalization and masking are omitted for notation simplicity.By applying Eq. ( 7), the change of the first two linear layers has been encapsulated within the attention map, bringing a more appropriate optimization objective for multiple linear layers.\n\nOptimization procedure.We introduce an efficient and effective procedure to obtain the optimal scaling factors toward the above objective.First, we find that scaling down only channels with outliers can bring better performance.Because channels with normal activations can exhibit more variation over different inputs, it can be difficult to find a decent scaling value for them.Also, considering that they are not responsible for low quantization performance, scaling them is not necessary.Second, we propose to optimize an alternate variable called outlier threshold t, which would squeeze only channels with an activation range over t into (\u2212t, t) and keep others intact (Fig. 2).Essentially, t here is used to specify which channel to scale down, the final scaled activation range, as well as the scaling values in the following weights.This technique simplifies the complex problem with numerous variables s to a single variable t.Then we can adopt the simple grid search for t to minimize the objective proposed in Eq. ( 6), Eq. ( 7).After getting the optimal t, the scaling vector is calculated as:\ns j = max(1.0, max(X :,j \u2212 z j ) t ). (8)\n4 Experiments\n\nThe evaluations are designed to showcase: I. consistent and satisfactory predictions of the proposed approach for both small and large language models across different bit-widths and tasks; II.efficiency from both algorithm and deployment perspectives; and III. the efficacy of each component.\n\n\nSet up\n\nModels and tasks.We conduct experiments on both small and large language models.First, BERT models (base and large versions) are evaluated on the GLUE benchmark (Wang et al., 2018a) (Gao et al., 2020) dataset for zero-shot tasks.A batch of them such as 32 is first used to calculate optimal shifting and scaling vectors.Then, we conduct the calibration procedure.Usually, MinMax calibration is employed but for models like BERT that also have a significantly varied token range, we apply the clipping method designed in Wei et al. (2022b).More details can be found in Appendix C. With our suppression framework, the transformed model can benefit from static quantization.\n\n\nSmall models\n\nBaselines.The main baseline categories are described below:\n\nMinMax obtains the minimum and maximum statistics of the tensor for quantization clipping range.Percentile (Wu et al., 2020) uses the activation distribution percentile as the quantization clipping range.Using the dev set, we search its hyper-parameters within [0.999, 0.9999, 0.99999].OMSE (Choukroun et al., 2019) minimizes the mean squared error between quantization and FP signals.\n\nPEG (Bondarenko et al., 2021) applies fine-grained quantization to problematic activation from a channel perspective.Outlier Suppression (OS) (Wei et al., 2022b) uses fixed scaling factors to suppress outliers and further clips outliers in a token-wise manner.\n\nBERT.Table 1 gives prediction results of common post-training quantization algorithms.Most methods perform well on INT8* and INT8 but fail on lower bits while our approach consistently achieves superior outcomes.Compared to Wei et al. (2022b), our method outperforms by 1.6% and 15.5% on 6-bit and 4-bit, respectively.In summary, our approach can achieve near-floating point performance on high bits and reduce the performance gap to 5.6% on 4-bit.\n\n\nLarge language models\n\nBaselines.The main baseline categories are described below.\n\nZeroQuant (Yao et al., 2022) uses per-token quantization, assigning different quantization parameters to different tokens.This fine-grained scheme from the token aspect also requires dynamic quantization.Meanwhile, for INT8*, we use per-group weight quantization according to its description.LLM.int8() (Dettmers et al., 2022) employs per-token quantization and relaxes representation to FP16 for channels with signals greater than 6.The fine-grained and dynamic quantization scheme achieves favorable results but has a poorer latency performance.Thus, we only adopt its accuracy results as a superior target.\n\nSmoothQuant (Xiao et al., 2022) migrates scaling factors to later modules to smooth problematic activation.Their scaling factors equal the range between activation and weights.\n\nFor lower bits, we also search its hyper-parameter \u03b1 according to its description for better performance.\n\nOPTs.We list 8-bit and 6-bit quantization accuracy in Table 2  points on 6-bit.\n\n\nComputational Complexity\n\nIn this part, we explain the computational complexity from algorithm and deployment aspects.Algorithm efficiency.Our algorithm is efficient, able to generate scaling and shifting values in about 20 minutes for OPT-175B offline.Moreover, due to the equivalent transformation, our method does not demand additional training and can be applied in a post-training setting.Deployment efficiency.Our method also en- joys favorable speed improvements without extra computation burden for LLMs.For example, Fig. 3 demonstrates 1.50\u00d7 acceleration on OPT-13B when compared to FP16.In most cases, the shifting and scaling operations can be seamlessly integrated into the subsequent modules.The only exception is the case of BERT models where the identity function in residual connection is replaced with channel-wise multiplication and addition to transform the activation back.Nevertheless, due to the limited presence of this structure, we demonstrate that these computations overhead are negligi- Table 3: Design choices of scaling factor.The second row removes the attention post process in optimization objective.\n\n\nBLOOM (INT8*) BLOOM (INT6) BLOOMZ (INT8*) BLOOMZ (INT6)\n\nThe third row chooses to learn the scaling vector directly rather than alternately optimize the outlier threshold.\n\nble compared to normal INT8, as shown in Fig. 3.\n\n\nAblation study\n\nDesign choices of scaling factors.In this section, we compare different designs for the scaling vector.\n\nIn Table 3, the second row illustrates the results without attention post-process Eq. ( 7).It is evident that simply summing the losses of multiple linear layers is not a wise choice, which suffers from significant performance declines: about 2% and 10% on OPTs and a smaller decline on BERT.\n\nThe third row removes the outlier threshold and instead adopts learning scaling factors directly.We find this process is unstable and requires suitable hyperparameters, even causing failure on LLMs.\n\nEven though we perform the learning procedure carefully and report the best result for BERT, we still cannot obtain satisfying results.As mentioned in Sec.3.2.2, this can be caused by poor scaling values for normal channels with varied signals.\n\nEffect of each operation.From Table 4, it can be observed clearly that by removing the shifting operation, the accuracy drops by about 1%-3% under difficult settings.This is because, without channelwise shifting that initially smooths the quantization challenge, scaling factors struggle to suppress outliers effectively while producing the tolerable weight quantization burden.Furthermore, when excluding scaling effects, performance decreases significantly, with even crashed results on LLMs.\n\n\nAnalysis of model storage and accuracy\n\nInspired by a variety of models with diverse sizes, we also study the relationship between their storage  and accuracy under quantization settings.Focusing on one kind of model with distinct quantization bitwidth, Fig. 5 shows that 8-bit quantization which cuts storage by about half, can generally maintain original performance, and 6-bit quantization can lead to less performance drop on larger models.Moreover, considering fixed storage constraints, we discover that quantized big models typically outperform small FP models.These observations can relate to model robustness, which implies that large models can benefit from compression more if special outliers are handled well.\n\n\nConclusion and Limitations\n\nWe present the Outlier Suppression+ framework for addressing asymmetric and consistent outliers in LLMs and other transformers.Our framework is simple to use, consisting of both scaling and shifting operations, which can be efficiently and effectively implemented.Experiments demonstrate the efficacy of our methods for suppressing outliers.Below, we briefly discuss some limitations.\n\nOther application scenarios: While we have applied our technique to language models under static and standard quantization, it is also interesting to explore vision scenario and the combination of our method with other quantization schemes like pertoken quantization.\n\nDeep level investigations: While we have observed features of outliers and devised methods to deal with them, the underlying reasons for their emergence and attributes have not been fully understood.This may require an in-depth analysis of the training pipeline, including the procedure and hyperparameters.Such investigations are timeconsuming but can benefit both FP and quantized scenarios.tion scheme that uses different quantization parameters for distinct channel groups, while Dettmers et al. (2022) suggests utilizing FP16 representations for problematic channels holding signals over 6.Wei et al. (2022b) identifies this feature lying in LayerNorm's output and migrates the scaling parameter of LayerNorm to subsequent modules to attenuate outliers.Xiao et al. (2022) proposes calculating scaling values by equalizing ranges between activations and weights.Guo et al. (2023) discards normal values adjacent to outliers, making room for outliers with customized GPU support.Compared to them, we design the scaling factors that concern the interactive results of troublesome activation and following weights to scale down channels with outliers offline.Also, we notice the asymmetric presentation of outliers and design a shifting operation.In terms of tokens, different tokens exhibit varying degrees of outliers.Dettmers et al. (2022); Yao et al. (2022) introduce a novel scheme called per-token quantization that dynamically computes quantization parameters for each token.Wei et al. (2022b) investigates the clipping impact of outliers and recommends finding an appropriate clipping range in a token-wise manner.\n\nBesides, some studies focus on weight quantization, such as Dettmers and Zettlemoyer (2022); Frantar et al. (2022); Zeng et al. (2022) and some including Yuan et al. (2021); Li et al. (2022), investigate the quantization of Vision Transformer (ViT) models.Interestingly, several studies (Kovaleva et al., 2021;Puccetti et al., 2022) explores the underlying reasons for emerging outliers and trace them back to the pre-training phase, shedding light on potential advancements in the field.\n\n\nB Supplementary experiments\n\nBERT-base.We provide detailed results of BERTbase models on GLUE benchmarks in Table 6.Interestingly, we find that models which are sensitive to different learning hyperparameters during the fine-tuning phase, such as CoLA and RTE, also exhibit less favorable quantization outcomes.This suggests a possible relationship between quantization and robustness.BERT-large.We also conduct experiments on BERT-large models in Table 7. Results across methods indicate that quantizing BERT-large models is more challenging (e.g., MinMax suffers from a considerable accuracy drop (about 13%) on INT8* compared to BERT-base, and Outlier Suppression Algorithm 1: Outlier Suppression+ Input: Problematic output X of LayerNorm with parameters \u03b3, \u03b2, subsequent module M with weight W and bias b, grid search iteration K. {1.Optimal shifting and scaling:} z = min(X :,j )+max(X :,j ) 2\n\nOptimal shifting vector.\nloss * = INF for k = 1 to K do t = max(X \u2212 z) \u2022 k K , Enumerate outlier threshold. s j = max(1.0, max(X :,j \u2212z j ) t\n) Calculate loss k based on Eq. ( 6), Eq. ( 7). if loss * > loss k then loss * = loss k , s * = s Optimal scaling factors.{2.Equivalent shifting and scaling:}\n\u03b2 = (\u03b2 \u2212 z) s * , \u03b3 = \u03b3 s * j Fuse z, s * into former operations. b = zW + b, W = W s *\nUpdate following modules.return Transformed LayerNorm and subsequent module; also fails on the 6-bit setting).Fortunately, with Outlier Suppression+, the results can be improved, yielding an 18.7% enhancement.BLOOM and BLOOMZ.Here, we provide detailed results of different techniques for BLOOM and BLOOMZ in Table 5.Although SmoothQuant outperforms us on LAMBADA, we argue that it could be due to the instability of the dataset since it is already higher than FP results.\n\n\nC Implementation details\n\nIn this section, we provide detailed descriptions of our implementation with the core part distilled in algorithm 1.Note that the mechanism is applied under a standard and static post-training quantization scenario, without any calibration calculation during the inference phase.BERT.On the GLUE benchmark, fine-tuned FP models are used for quantization.We randomly se-\n\nFigure 1 :\n1\nFigure1: Distribution of OPT-66B.Fig.1ashows the original distribution with asymmetric outliers consistently occurs at certain channels, owning considerable range(-97, 43).Fig.1bdepicts the channel-wise shifting operation to decrease the tensor range by eliminating the asymmetry.Fig.1cfurther scales down the outliers to threshold 5 and finally results in a distribution ranging from -5 to 5.\n\n\nFigure 2 :\n2\nFigure 2: Left: We show the equivalent shifting and scaling operations by giving two representative examples: (a) for problematic output of Pre-LN (LayerNorm put inside residual connection) with Multi-Head Attention (MHA) structure; (b) for problematic output of Post-LN (LayerNorm put before residual connection) with Feed-Forward Network (FFN).Right: For optimal shifting and scaling values, the shifting vector can align the center of each channel to 0 and the scaling vector would shrink outliers into the outlier threshold t which is searched based on its left metric.\n\n\nFigure 3 :\n3\nFigure3: Real latency (x-axis) of our transformed 8-bit models, 8-bit and FP16 original models over different batch sizes (y-axis).BERT-large-256 refers to the BERT-large model with sequence length set to 256.Both numbers indicate quantization speedup.Only on BERT models, our transformation would increase the extra computation burden but is negligible.\n\n\nFigure 5 :\n5\nFigure5: Averaged accuracy on PIQA, Winogrande, LAM-BADA, and HellaSwag of OPTs with different storages.We draw circles, rectangles, and triangles to refer to FP16, the 8-bit and 6-bit models with quantized activation and weight.\n\n\n\n\nWe use INT8 and INT6 to denote 8-bit and 6-bit activation and per-channel weight quantization, respectively.INT8* especially refers to per-tensor weight quantization.Activation always employs per-tensor quantization.Additionally, \u2663 marks those taking dynamic and fine-grained quantization schemes.Implementation details.We randomly select 128 samples from the training dataset, in-domain data for the GLUE benchmark, and PILE\n, whichconsists of eight text classification tasks. Sec-ond, four of the largest OPTs ranging from 13Bto 175B, along with the biggest BLOOM (Scaoet al., 2022) and BLOOMZ (Muennighoff et al.,2022) boasting 176 billion parameters, are cho-sen as representatives. To verify the efficacy ofour techniques on them, we adopt eight zero-shorttasks (language modeling, multiple choice, com-monsense reasoning, etc.) that can pose greaterchallenges for quantization. The evaluation code isbased on lm-harness-evaluation 1 .Quantization setting. Quantization nodes are in-serted as described in Wei et al. (2022b); NVIDIA(2022). Two types of quantization schemes are con-sidered for static and standard quantization. Thefirst is per-tensor and symmetric quantization, of-fering the fastest speed, indicated by an  *  (e.g.,INT8*). The second is per-channel (weight) andasymmetric quantization, which provides high per-formance.Notation:\n\nTable 1 :\n1\nPTQ performance of BERT-base models.MNLI and STS-B report the combined score.Avg.indicates the averaged results of 8 tasks on GLUE benchmark (details in Appendix B). * means per-tensor quantization for weight.OS indicates Outlier Suppression for short.\nMethodCoLA MNLI QNLI SST-2 STS-B Avg.FP3259.684.991.893.489.583.8INT8*MinMax52.381.389.091.186.279.5OMSE54.882.189.791.387.781.6PEG59.481.391.192.787.982.5OS60.383.990.292.988.283.0Ours60.984.491.192.788.383.5INT6OMSE35.473.784.786.385.873.5Percentile37.372.179.487.386.872.9OS54.481.889.891.988.781.2Ours56.084.590.992.489.582.8INT4OMSE4.738.552.250.30.241.1Percentile7.053.061.577.166.157.0OS28.557.972.580.467.862.7Ours50.080.285.491.486.578.2\n\nTable 2 :\n2\nComparison among different techniques in terms of accuracy on eight zero-shot tasks.\nNameMethodOPT-13BOPT-30BOPT-66BOPT-175BFP16 INT8* INT8 INT6 FP16 INT8* INT8 INT6 FP16 INT8* INT8 INT6 FP16 INT8* INT8 INT6LLM.int8() \u2663-75.8--77.3--78.7--79.6-PIQAZeroQuant \u2663 SmoothQuant75.854.1 76.0--53.0 73.577.654.2 77.2--52.0 66.778.753.2 78.3--51.9 52.079.752.3 79.7--53.1 52.6Ours76.475.975.877.477.677.478.778.677.579.679.580.0LLM.int8() \u2663-68.4--71.4--73.8--74.6LAMBADAZeroQuant \u2663 SmoothQuant68.60.0 68.3--0.0 65.271.50.0 71.0--0.0 13.473.90.0 72.9--0.0 0.074.70.0 74.6--0.0 0.5Ours68.368.465.770.870.869.673.073.472.774.574.574.2LLM.int8() \u2663-52.4--54.3--56.3--59.2-HellaSwagZeroQuant \u2663 SmoothQuant52.526.5 52.2--25.8 49.254.326.4 54.2--25.7 37.456.426.1 55.9--25.7 26.559.325.4 58.9--25.6 26.0Ours52.352.551.754.254.253.756.256.355.859.259.358.5LLM.int8() \u2663-64.8--68.1--68.5--72.3-WinograndeZeroQuant \u2663 SmoothQuant65.152.1 64.9--51.1 60.368.551.8 68.2--51.8 55.068.950.7 68.3--48.0 52.172.550.2 71.2--49.1 49.1Ours65.065.364.068.068.568.969.068.869.472.572.571.7ARC (Challenge)LLM.int8() \u2663 ZeroQuant \u2663 SmoothQuant32.8-19.3 32.133.5 ---20.7 30.634.6-19.8 33.834.7 ---20.6 26.737.3-20.8 36.537.0 ---20.4 21.940.3-21.8 40.540.9 ---20.6 21.2Ours33.533.332.734.534.734.637.537.237.040.339.941.0ARC (Easy)LLM.int8() \u2663 ZeroQuant \u2663 SmoothQuant67.3-27.5 66.267.3 ---25.0 62.270.1-30.5 69.769.7 ---25.0 55.871.7-29.7 70.571.8 ---26.0 27.874.9-24.0 74.174.8 ---25.6 28.8Ours67.366.867.070.170.068.971.371.870.774.874.774.3LLM.int8() \u2663-86.0--82.0--87.0--89.0-COPAZeroQuant \u2663 SmoothQuant86.063.0 85.0--55.0 82.082.055.0 83.0--55.0 75.086.053.0 84.0--52.0 55.088.060.0 88.0--55.0 55.0Ours85.086.085.083.082.084.085.086.084.088.089.091.0LLM.int8() \u2663-76.3--77.1--77.7--79.3-StoryClozeZeroQuant \u2663 SmoothQuant76.149.6 76.0--48.3 73.577.048.5 76.9--48.0 61.477.549.2 77.3--48.4 48.879.547.7 79.1--48.2 49.8Ours75.876.075.477.076.976.677.376.476.679.279.178.1Avg.Ours65.565.565.564.767.066.966.866.768.868.568.668.071.171.071.171.1\n\u2663 denotes dynamic quantization.Note that LLM.int8() would assign FP16 for channels containing signals over 6.INT8* specifically adopts per-tensor quantization for weights compared to INT8.\n\n\nTable 4 :\n4\nEffect of scaling and shifting operations.\nMethodOPT-66B (INT6)BERT (INT4)PIQA Winogrande SST-2 MNLIOurs77.569.491.480.2-shifting76.566.589.377.7-shifting -scaling 54.749.482.363.7\n\nTable 5 :\n5\nQuantization results on 4 zero-shot tasks in terms of accuracy.Since diverse tokens do not have outliers of varying degrees on these models, advanced clipping techniques are not involved.BLOOM and BLOOMZ.The main pipeline is similar to OPTs.The only exception is using the Token-Wise Clipping as the calibration method because these models hold different outliers among different tokens.The clipping ratios are searched as 0.5% and 1.5% for 8-bit and 6-bit BLOOM, and 0.0% and 0.5% on BLOOMZ.\nlect 128 samples and set the batch size to 32. First,a batch of data is used to calculate the optimal shift-ing and scaling signals for problematic activations,especially outputs after LayerNorm here. Thenshifting and scaling vectors are fused into formeroperations and absorbed in later modules. On fusedmodels, we apply the calibration procedure. Partic-ularly, on BERT models, due to the great varianceof token range as discussed in Yao et al. (2022);Wei et al. (2022b), we incorporate the Token-WiseClipping proposed in Outlier Suppression which isan orthogonal technique and weakens outliers fromthe token aspect.OPTs. For OPTs, we quantize pre-trained modelsand evaluate them on zero-shot tasks. 128 sam-ples with sequence length set to 512 are randomlyextracted from one of the train datasets, namelythe PILE dataset. As we have observed that Lay-erNorm produces severe asymmetric outliers oncertain channels, the proposed method is appliedhere. After obtaining a more quantization-friendlymodel, the MinMax algorithm collects distributionstatistics.\n\nTable 6 :\n6\nPTQ performance of BERT-base models on GLUE benchmark.* means per-tensor quantization for weight.OS indicates Outlier Suppression for short.\nMethodCoLA (Matt.) (acc m/mm) (f1/acc) MNLI MRPC QNLI (acc)QQP (f1/acc) (acc) (acc) (Pear./Spear.) RTE SST-2 STS-BAvg.FP3263.386.7/85.991.6/88.0 92.2 88.1/91.1 74.093.590.3/90.184.9INT8*MinMax62.472.0/73.076.3/72.8 87.0 66.5/80.4 46.992.258.6/52.171.5OMSE59.982.7/83.587.8/83.8 89.0 79.2/86.2 47.392.083.9/83.378.1Percentile61.384.5/84.091.6/88.9 91.6 85.9/89.4 69.392.488.3/88.183.1OS62.385.1/84.590.1/86.0 91.1 87.0/90.3 75.192.488.7/88.483.9Ours62.285.9/85.290.9/87.0 92.2 87.8/90.8 71.893.389.3/89.384.1INT6MinMax5.632.0/32.050.2/46.1 50.20.0/63.2 49.553.05.0/4.838.1OMSE14.059.3/58.486.1/78.7 79.5 52.5/73.5 54.974.844.0/37.959.8Percentile16.463.5/63.882.0/77.2 87.0 44.8/70.7 49.881.765.7/67.862.8OS24.171.3/71.785.5/79.4 80.8 68.8/78.3 47.382.361.1/62.065.4Ours60.986.3/85.491.8/88.2 92.0 87.7/90.8 71.593.786.7/85.683.7\n\nTable 7 :\n7\nPTQ performance of BERT-large models on GLUE benchmark.* means per-tensor quantization for weight.OS indicates Outlier Suppression for short.\n\nhttps://github.com/EleutherAI/lm-evaluation-harness\n\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, Irwin King, arXiv:2012.15701Binarybert: Pushing the limit of bert quantization. 2020arXiv preprint\n\nAciq: analytical clipping for integer quantization of neural networks. Ron Banner, Yury Nahshan, Elad Hoffer, Daniel Soudry, 2018\n\nUnderstanding and overcoming the challenges of efficient transformer quantization. Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort, arXiv:2109.129482021arXiv preprint\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nZeroq: A novel zero shot quantization framework. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, Kurt Keutzer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020\n\nRethinking differentiable search for mixed-precision neural networks. Zhaowei Cai, Nuno Vasconcelos, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020\n\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, I-Jen Pierce, Vijayalakshmi Chuang, Kailash Srinivasan, Gopalakrishnan, arXiv:1805.06085Pact: Parameterized clipping activation for quantized neural networks. 2018arXiv preprint\n\nLow-bit quantization of neural networks for efficient inference. Yoni Choukroun, Eli Kravchik, Fan Yang, Pavel Kisilev, 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). IEEE2019\n\nBinaryconnect: Training deep neural networks with binary weights during propagations. Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David, ; , Luke Zettlemoyer, arXiv:2208.07339-bit matrix multiplication for transformers at scale. Mike Lewis, Younes Belkada,2015. 20228arXiv preprintAdvances in neural information processing systems, 28. Tim Dettmers. int8 (\n\nThe case for 4-bit precision: k-bit inference scaling laws. Tim Dettmers, Luke Zettlemoyer, arXiv:2212.097202022arXiv preprint\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova ; Zhen, Zhewei Dong, Amir Yao, Michael W Gholami, Kurt Mahoney, Keutzer, arXiv:1810.04805Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2018. 2019arXiv preprintHawq: Hessian aware quantization of neural networks with mixedprecision\n\nJeffrey L Steven K Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, arXiv:1902.08153Learned step size quantization. 2019arXiv preprint\n\nTraining with quantization noise for extreme model compression. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herve Jegou, Armand Joulin, arXiv:2004.073202020arXiv preprint\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.173232022arXiv preprint\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint\n\nDifferentiable soft quantization: Bridging full-precision and low-bit neural networks. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, Junjie Yan, The IEEE International Conference on Computer Vision (ICCV). 2019\n\nSquant: On-the-fly datafree quantization via diagonal hessian approximation. Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, Minyi Guo, arXiv:2202.074712022arXiv preprint\n\nOlive: Accelerating large language models via hardware-friendly outlier-victim pair quantization. Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, Yuhao Zhu, Matrix. 1722023\n\nSong Han, Huizi Mao, William J Dally, arXiv:1510.00149Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. 2015arXiv preprint\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 2015arXiv preprint\n\nAccurate post training quantization with small calibration sets. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, Daniel Soudry, International Conference on Machine Learning. PMLR2021\n\nQuantization and training of neural networks for efficient integer-arithmetic-only inference. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018\n\nF8net: Fixed-point 8-bit only multiplication for network quantization. Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, Sergey Tulyakov, arXiv:2202.052392022arXiv preprint\n\n2021. I-bert: Integeronly bert quantization. Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, Kurt Keutzer, International conference on machine learning. PMLR\n\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, Anna Rumshisky, arXiv:2105.06990Bert busters: Outlier dimensions that disrupt transformers. 2021arXiv preprint\n\nAndrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, Tijmen Blankevoort, arXiv:2208.09225Fp8 quantization: The power of the exponent. 2022arXiv preprint\n\nOptimal brain damage. Yann Lecun, John Denker, Sara Solla, Advances in neural information processing systems. 1989\n\nQ-vit: Accurate and fully quantized low-bit vision transformer. Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, Guodong Guo, arXiv:2210.067072022arXiv preprint\n\nAdditive powers-of-two quantization: An efficient nonuniform discretization for neural networks. Yuhang Li, Xin Dong, Wei Wang, arXiv:1909.131442019arXiv preprint\n\nBrecq: Pushing the limit of post-training quantization by block reconstruction. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu, International Conference on Learning Representations. 2021\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint\n\nPaulius Micikevicius, Dusan Stosic, Neil Burgess ; Pradeep, Richard Dubey, Sangwon Grisenthwaite, Alexander Ha, Patrick Heinecke, John Judd, Kamalu, arXiv:2209.05433Fp8 formats for deep learning. Marius Cornea,. 2022arXiv preprint\n\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, arXiv:2211.01786Crosslingual generalization through multitask finetuning. 2022arXiv preprint\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLR2020\n\nData-free quantization through weight equalization and bias correction. Markus Nagel, Mart Van Baalen, Tijmen Blankevoort, Max Welling, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019\n\nNVIDIA. 2022. Faster transformer. \n\nGiovanni Puccetti, Anna Rogers, Aleksandr Drozd, Felice Dell'orletta, arXiv:2205.11380Outliers dimensions that disrupt transformers are driven by frequency. 2022arXiv preprint\n\nHaotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, Xianglong Liu, arXiv:2203.06390Bibert: Accurate fully binarized bert. 2022arXiv preprint\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint\n\nOnce quantization-aware training: High performance extremely low-bit architecture search. Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming Li, Chen Lin, Fengwei Yu, Junjie Yan, Wanli Ouyang, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021\n\nQ-bert: Hessian based ultra low precision quantization of bert. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, Kurt Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022arXiv preprint\n\nCompression of generative pre-trained language models via quantization. Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong, arXiv:2203.107052022arXiv preprint\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. 2018aarXiv preprint\n\nTraining deep neural networks with 8-bit floating point numbers. Advances in neural information processing systems. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, Kailash Gopalakrishnan, 2018b31\n\nTowards accurate post-training network quantization via bit-split and stitching. Peisong Wang, Qiang Chen, Xiangyu He, Jian Cheng, International Conference on Machine Learning. PMLR2020\n\nQdrop: Randomly dropping quantization for extremely low-bit post-training quantization. Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, Fengwei Yu, International Conference on Learning Representations. 2022a\n\nXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, arXiv:2209.13325Outlier suppression: Pushing the limit of low-bit transformer language models. 2022barXiv preprint\n\nInteger quantization for deep learning inference: Principles and empirical evaluation. Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, Paulius Micikevicius, arXiv:2004.096022020arXiv preprint\n\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.10438Smoothquant: Accurate and efficient post-training quantization for large language models. 2022arXiv preprint\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, arXiv:2206.018612022arXiv preprint\n\nPtq4vit: Posttraining quantization framework for vision transformers. Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, Guangyu Sun, arXiv:2111.122932021arXiv preprint\n\nQ8bert: Quantized 8bit bert. Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat, 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE2019\n\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprint\n\nLq-nets: Learned quantization for highly accurate and compact deep neural networks. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, Gang Hua, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018\n\nOpt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint\n\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu, arXiv:2009.12812Ternarybert: Distillation-aware ultra-low bit bert. 2020arXiv preprint\n\nDiversifying sample generation for accurate data-free quantization. Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, Xianglong Liu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021\n\nImproving neural network quantization without retraining using outlier channel splitting. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, Zhiru Zhang, International conference on machine learning. PMLR2019\n\nBarret Zoph, Quoc, Le, arXiv:1611.01578Neural architecture search with reinforcement learning. 2016arXiv preprint\n\nemploys low-bit representations for activation and weight in neural networks. Researchers categorize this approach into two pipelines: post-training quantization (PTQ) and quantization-aware training (QAT). Jacob , A Related work Quantization. Quantization. Contrast, Ptq (choukroun, 2018. 2015. 2018. 2019. 2019. 2019. 2021. 2018. 2019. 2020. 2018. 2020. 2019. 2019. 2020. 2021. 2021. 2022a. 2020only requires hundreds of samples and limited resource consumption, producing a calibrated model quickly. Recently, several works. proposed to adjust models slightly for improved PTQ performance. Besides, other types of quantization include zero-shot quantization without real calibration data\n\nfirst explores 8-bit quantization for BERT-like models. Shen et al. (2020) introduces group-wise quantization and studies mixed-precision quantization based on Hessian information. Zhang, and FP8 data type. 2021. 2022. 2019. Cai and Vasconcelos, 2020. 2018b. 2022. 2022. 2022. 2019. 2020Quantization of transformer language models. Recently, there has been a growing interest in the quantization of transformer language models. Bai et al.\n\n. Zhang, 2020\n\ninvestigates the challenges of quantizing generative models. In the realm of PTQ, researchers have discovered that the poor performance of these models should be attributed to extreme outliers in activations. These outliers exhibit special characteristics from both channel and token aspects. In terms of channels, outliers consistently emerge in certain channels over different inputs. Qin, 2022. 2021. 2022approximates the nonlinear function in transformer architectures to enable integer-only inference. Fan et al. (2020) incorporates quantization noise for enhancement. Bondarenko et al. (2021) employs a per-embedding-group quantiza\n", "annotations": {"author": "[{\"end\":247,\"start\":119},{\"end\":369,\"start\":248},{\"end\":417,\"start\":370},{\"end\":482,\"start\":418},{\"end\":541,\"start\":483},{\"end\":648,\"start\":542},{\"end\":752,\"start\":649},{\"end\":247,\"start\":119},{\"end\":369,\"start\":248},{\"end\":417,\"start\":370},{\"end\":482,\"start\":418},{\"end\":541,\"start\":483},{\"end\":648,\"start\":542},{\"end\":752,\"start\":649}]", "publisher": null, "author_last_name": "[{\"end\":130,\"start\":127},{\"end\":261,\"start\":256},{\"end\":379,\"start\":377},{\"end\":432,\"start\":427},{\"end\":494,\"start\":490},{\"end\":553,\"start\":550},{\"end\":662,\"start\":659},{\"end\":130,\"start\":127},{\"end\":261,\"start\":256},{\"end\":379,\"start\":377},{\"end\":432,\"start\":427},{\"end\":494,\"start\":490},{\"end\":553,\"start\":550},{\"end\":662,\"start\":659}]", "author_first_name": "[{\"end\":126,\"start\":119},{\"end\":255,\"start\":248},{\"end\":376,\"start\":370},{\"end\":426,\"start\":418},{\"end\":489,\"start\":483},{\"end\":549,\"start\":542},{\"end\":658,\"start\":649},{\"end\":126,\"start\":119},{\"end\":255,\"start\":248},{\"end\":376,\"start\":370},{\"end\":426,\"start\":418},{\"end\":489,\"start\":483},{\"end\":549,\"start\":542},{\"end\":658,\"start\":649}]", "author_affiliation": "[{\"end\":225,\"start\":156},{\"end\":246,\"start\":227},{\"end\":309,\"start\":290},{\"end\":368,\"start\":311},{\"end\":416,\"start\":400},{\"end\":481,\"start\":462},{\"end\":540,\"start\":521},{\"end\":647,\"start\":578},{\"end\":751,\"start\":682},{\"end\":225,\"start\":156},{\"end\":246,\"start\":227},{\"end\":309,\"start\":290},{\"end\":368,\"start\":311},{\"end\":416,\"start\":400},{\"end\":481,\"start\":462},{\"end\":540,\"start\":521},{\"end\":647,\"start\":578},{\"end\":751,\"start\":682}]", "title": "[{\"end\":116,\"start\":1},{\"end\":868,\"start\":753},{\"end\":116,\"start\":1},{\"end\":868,\"start\":753}]", "venue": null, "abstract": "[{\"end\":2069,\"start\":902},{\"end\":2069,\"start\":902}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2315,\"start\":2294},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2332,\"start\":2315},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2353,\"start\":2332},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2412,\"start\":2392},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2431,\"start\":2412},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2450,\"start\":2431},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2528,\"start\":2510},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2548,\"start\":2528},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2566,\"start\":2548},{\"end\":2586,\"start\":2566},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2669,\"start\":2649},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2873,\"start\":2849},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2893,\"start\":2873},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2912,\"start\":2893},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3079,\"start\":3054},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3144,\"start\":3121},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3594,\"start\":3569},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3616,\"start\":3594},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3743,\"start\":3724},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3761,\"start\":3743},{\"end\":4710,\"start\":4701},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7387,\"start\":7369},{\"end\":8729,\"start\":8720},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14145,\"start\":14126},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14163,\"start\":14145},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17798,\"start\":17778},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17817,\"start\":17799},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18155,\"start\":18137},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18490,\"start\":18473},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18681,\"start\":18657},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18782,\"start\":18757},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18914,\"start\":18895},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19257,\"start\":19239},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19578,\"start\":19560},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19876,\"start\":19853},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20192,\"start\":20173},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25160,\"start\":25138},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25267,\"start\":25249},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25430,\"start\":25412},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25537,\"start\":25520},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25997,\"start\":25975},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26016,\"start\":25999},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26155,\"start\":26137},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26370,\"start\":26339},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26393,\"start\":26372},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":26413,\"start\":26395},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26451,\"start\":26433},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26469,\"start\":26453},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26589,\"start\":26566},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26611,\"start\":26589},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2315,\"start\":2294},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2332,\"start\":2315},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2353,\"start\":2332},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2412,\"start\":2392},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2431,\"start\":2412},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2450,\"start\":2431},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2528,\"start\":2510},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2548,\"start\":2528},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2566,\"start\":2548},{\"end\":2586,\"start\":2566},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2669,\"start\":2649},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2873,\"start\":2849},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2893,\"start\":2873},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2912,\"start\":2893},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3079,\"start\":3054},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3144,\"start\":3121},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3594,\"start\":3569},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3616,\"start\":3594},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3743,\"start\":3724},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3761,\"start\":3743},{\"end\":4710,\"start\":4701},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7387,\"start\":7369},{\"end\":8729,\"start\":8720},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14145,\"start\":14126},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14163,\"start\":14145},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17798,\"start\":17778},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17817,\"start\":17799},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18155,\"start\":18137},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18490,\"start\":18473},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18681,\"start\":18657},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18782,\"start\":18757},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18914,\"start\":18895},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19257,\"start\":19239},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19578,\"start\":19560},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19876,\"start\":19853},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20192,\"start\":20173},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25160,\"start\":25138},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25267,\"start\":25249},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25430,\"start\":25412},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25537,\"start\":25520},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25997,\"start\":25975},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26016,\"start\":25999},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26155,\"start\":26137},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26370,\"start\":26339},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26393,\"start\":26372},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":26413,\"start\":26395},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26451,\"start\":26433},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26469,\"start\":26453},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26589,\"start\":26566},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26611,\"start\":26589}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29337,\"start\":28929},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29926,\"start\":29338},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30296,\"start\":29927},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30541,\"start\":30297},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31897,\"start\":30542},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32610,\"start\":31898},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34817,\"start\":32611},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35011,\"start\":34818},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":36575,\"start\":35012},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":37557,\"start\":36576},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":37713,\"start\":37558},{\"attributes\":{\"id\":\"fig_0\"},\"end\":29337,\"start\":28929},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29926,\"start\":29338},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30296,\"start\":29927},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30541,\"start\":30297},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31897,\"start\":30542},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32610,\"start\":31898},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34817,\"start\":32611},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35011,\"start\":34818},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":36575,\"start\":35012},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":37557,\"start\":36576},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":37713,\"start\":37558}]", "paragraph": "[{\"end\":2778,\"start\":2085},{\"end\":3369,\"start\":2780},{\"end\":4375,\"start\":3371},{\"end\":4936,\"start\":4377},{\"end\":5600,\"start\":4938},{\"end\":5792,\"start\":5602},{\"end\":6304,\"start\":5794},{\"end\":6950,\"start\":6306},{\"end\":7347,\"start\":6966},{\"end\":8026,\"start\":7349},{\"end\":8276,\"start\":8037},{\"end\":8449,\"start\":8312},{\"end\":9059,\"start\":8482},{\"end\":9212,\"start\":9061},{\"end\":9651,\"start\":9227},{\"end\":10074,\"start\":9653},{\"end\":10090,\"start\":10076},{\"end\":10095,\"start\":10092},{\"end\":10743,\"start\":10097},{\"end\":11363,\"start\":10773},{\"end\":11514,\"start\":11424},{\"end\":12294,\"start\":11606},{\"end\":12528,\"start\":12296},{\"end\":12878,\"start\":12561},{\"end\":13288,\"start\":12898},{\"end\":13293,\"start\":13290},{\"end\":14416,\"start\":13313},{\"end\":14619,\"start\":14418},{\"end\":14889,\"start\":14684},{\"end\":14959,\"start\":14922},{\"end\":15070,\"start\":15000},{\"end\":15246,\"start\":15072},{\"end\":15839,\"start\":15248},{\"end\":16148,\"start\":15896},{\"end\":17255,\"start\":16150},{\"end\":17311,\"start\":17298},{\"end\":17606,\"start\":17313},{\"end\":18288,\"start\":17617},{\"end\":18364,\"start\":18305},{\"end\":18751,\"start\":18366},{\"end\":19013,\"start\":18753},{\"end\":19463,\"start\":19015},{\"end\":19548,\"start\":19489},{\"end\":20159,\"start\":19550},{\"end\":20337,\"start\":20161},{\"end\":20444,\"start\":20339},{\"end\":20525,\"start\":20446},{\"end\":21661,\"start\":20554},{\"end\":21835,\"start\":21721},{\"end\":21885,\"start\":21837},{\"end\":22007,\"start\":21904},{\"end\":22301,\"start\":22009},{\"end\":22501,\"start\":22303},{\"end\":22747,\"start\":22503},{\"end\":23243,\"start\":22749},{\"end\":23968,\"start\":23286},{\"end\":24383,\"start\":23999},{\"end\":24652,\"start\":24385},{\"end\":26277,\"start\":24654},{\"end\":26767,\"start\":26279},{\"end\":27668,\"start\":26799},{\"end\":27694,\"start\":27670},{\"end\":27970,\"start\":27812},{\"end\":28530,\"start\":28059},{\"end\":28928,\"start\":28559},{\"end\":29336,\"start\":28943},{\"end\":29925,\"start\":29352},{\"end\":30295,\"start\":29941},{\"end\":30540,\"start\":30311},{\"end\":30970,\"start\":30545},{\"end\":32163,\"start\":31911},{\"end\":32708,\"start\":32624},{\"end\":34816,\"start\":34628},{\"end\":34873,\"start\":34831},{\"end\":35517,\"start\":35025},{\"end\":36729,\"start\":36589},{\"end\":37712,\"start\":37571},{\"end\":2778,\"start\":2085},{\"end\":3369,\"start\":2780},{\"end\":4375,\"start\":3371},{\"end\":4936,\"start\":4377},{\"end\":5600,\"start\":4938},{\"end\":5792,\"start\":5602},{\"end\":6304,\"start\":5794},{\"end\":6950,\"start\":6306},{\"end\":7347,\"start\":6966},{\"end\":8026,\"start\":7349},{\"end\":8276,\"start\":8037},{\"end\":8449,\"start\":8312},{\"end\":9059,\"start\":8482},{\"end\":9212,\"start\":9061},{\"end\":9651,\"start\":9227},{\"end\":10074,\"start\":9653},{\"end\":10090,\"start\":10076},{\"end\":10095,\"start\":10092},{\"end\":10743,\"start\":10097},{\"end\":11363,\"start\":10773},{\"end\":11514,\"start\":11424},{\"end\":12294,\"start\":11606},{\"end\":12528,\"start\":12296},{\"end\":12878,\"start\":12561},{\"end\":13288,\"start\":12898},{\"end\":13293,\"start\":13290},{\"end\":14416,\"start\":13313},{\"end\":14619,\"start\":14418},{\"end\":14889,\"start\":14684},{\"end\":14959,\"start\":14922},{\"end\":15070,\"start\":15000},{\"end\":15246,\"start\":15072},{\"end\":15839,\"start\":15248},{\"end\":16148,\"start\":15896},{\"end\":17255,\"start\":16150},{\"end\":17311,\"start\":17298},{\"end\":17606,\"start\":17313},{\"end\":18288,\"start\":17617},{\"end\":18364,\"start\":18305},{\"end\":18751,\"start\":18366},{\"end\":19013,\"start\":18753},{\"end\":19463,\"start\":19015},{\"end\":19548,\"start\":19489},{\"end\":20159,\"start\":19550},{\"end\":20337,\"start\":20161},{\"end\":20444,\"start\":20339},{\"end\":20525,\"start\":20446},{\"end\":21661,\"start\":20554},{\"end\":21835,\"start\":21721},{\"end\":21885,\"start\":21837},{\"end\":22007,\"start\":21904},{\"end\":22301,\"start\":22009},{\"end\":22501,\"start\":22303},{\"end\":22747,\"start\":22503},{\"end\":23243,\"start\":22749},{\"end\":23968,\"start\":23286},{\"end\":24383,\"start\":23999},{\"end\":24652,\"start\":24385},{\"end\":26277,\"start\":24654},{\"end\":26767,\"start\":26279},{\"end\":27668,\"start\":26799},{\"end\":27694,\"start\":27670},{\"end\":27970,\"start\":27812},{\"end\":28530,\"start\":28059},{\"end\":28928,\"start\":28559},{\"end\":29336,\"start\":28943},{\"end\":29925,\"start\":29352},{\"end\":30295,\"start\":29941},{\"end\":30540,\"start\":30311},{\"end\":30970,\"start\":30545},{\"end\":32163,\"start\":31911},{\"end\":32708,\"start\":32624},{\"end\":34816,\"start\":34628},{\"end\":34873,\"start\":34831},{\"end\":35517,\"start\":35025},{\"end\":36729,\"start\":36589},{\"end\":37712,\"start\":37571}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9226,\"start\":9213},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11423,\"start\":11364},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11605,\"start\":11515},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14683,\"start\":14620},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14921,\"start\":14890},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14999,\"start\":14960},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15895,\"start\":15840},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17296,\"start\":17256},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17297,\"start\":17296},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27811,\"start\":27695},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28058,\"start\":27971},{\"attributes\":{\"id\":\"formula_0\"},\"end\":9226,\"start\":9213},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11423,\"start\":11364},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11605,\"start\":11515},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14683,\"start\":14620},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14921,\"start\":14890},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14999,\"start\":14960},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15895,\"start\":15840},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17296,\"start\":17256},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17297,\"start\":17296},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27811,\"start\":27695},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28058,\"start\":27971}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19027,\"start\":19026},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20507,\"start\":20506},{\"end\":21550,\"start\":21549},{\"end\":22019,\"start\":22018},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22786,\"start\":22785},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26885,\"start\":26884},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":27225,\"start\":27224},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28374,\"start\":28373},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19027,\"start\":19026},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20507,\"start\":20506},{\"end\":21550,\"start\":21549},{\"end\":22019,\"start\":22018},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22786,\"start\":22785},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26885,\"start\":26884},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":27225,\"start\":27224},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28374,\"start\":28373}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2083,\"start\":2071},{\"attributes\":{\"n\":\"2\"},\"end\":6964,\"start\":6953},{\"attributes\":{\"n\":\"3\"},\"end\":8035,\"start\":8029},{\"attributes\":{\"n\":\"3.1\"},\"end\":8310,\"start\":8279},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":8480,\"start\":8452},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":10771,\"start\":10746},{\"attributes\":{\"n\":\"3.2\"},\"end\":12559,\"start\":12531},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":12896,\"start\":12881},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":13311,\"start\":13296},{\"attributes\":{\"n\":\"4.1\"},\"end\":17615,\"start\":17609},{\"attributes\":{\"n\":\"4.2\"},\"end\":18303,\"start\":18291},{\"attributes\":{\"n\":\"4.3\"},\"end\":19487,\"start\":19466},{\"attributes\":{\"n\":\"4.4\"},\"end\":20552,\"start\":20528},{\"end\":21719,\"start\":21664},{\"attributes\":{\"n\":\"4.5\"},\"end\":21902,\"start\":21888},{\"attributes\":{\"n\":\"4.6\"},\"end\":23284,\"start\":23246},{\"attributes\":{\"n\":\"5\"},\"end\":23997,\"start\":23971},{\"end\":26797,\"start\":26770},{\"end\":28557,\"start\":28533},{\"end\":28940,\"start\":28930},{\"end\":29349,\"start\":29339},{\"end\":29938,\"start\":29928},{\"end\":30308,\"start\":30298},{\"end\":31908,\"start\":31899},{\"end\":32621,\"start\":32612},{\"end\":34828,\"start\":34819},{\"end\":35022,\"start\":35013},{\"end\":36586,\"start\":36577},{\"end\":37568,\"start\":37559},{\"attributes\":{\"n\":\"1\"},\"end\":2083,\"start\":2071},{\"attributes\":{\"n\":\"2\"},\"end\":6964,\"start\":6953},{\"attributes\":{\"n\":\"3\"},\"end\":8035,\"start\":8029},{\"attributes\":{\"n\":\"3.1\"},\"end\":8310,\"start\":8279},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":8480,\"start\":8452},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":10771,\"start\":10746},{\"attributes\":{\"n\":\"3.2\"},\"end\":12559,\"start\":12531},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":12896,\"start\":12881},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":13311,\"start\":13296},{\"attributes\":{\"n\":\"4.1\"},\"end\":17615,\"start\":17609},{\"attributes\":{\"n\":\"4.2\"},\"end\":18303,\"start\":18291},{\"attributes\":{\"n\":\"4.3\"},\"end\":19487,\"start\":19466},{\"attributes\":{\"n\":\"4.4\"},\"end\":20552,\"start\":20528},{\"end\":21719,\"start\":21664},{\"attributes\":{\"n\":\"4.5\"},\"end\":21902,\"start\":21888},{\"attributes\":{\"n\":\"4.6\"},\"end\":23284,\"start\":23246},{\"attributes\":{\"n\":\"5\"},\"end\":23997,\"start\":23971},{\"end\":26797,\"start\":26770},{\"end\":28557,\"start\":28533},{\"end\":28940,\"start\":28930},{\"end\":29349,\"start\":29339},{\"end\":29938,\"start\":29928},{\"end\":30308,\"start\":30298},{\"end\":31908,\"start\":31899},{\"end\":32621,\"start\":32612},{\"end\":34828,\"start\":34819},{\"end\":35022,\"start\":35013},{\"end\":36586,\"start\":36577},{\"end\":37568,\"start\":37559}]", "table": "[{\"end\":31897,\"start\":30971},{\"end\":32610,\"start\":32164},{\"end\":34627,\"start\":32709},{\"end\":35011,\"start\":34874},{\"end\":36575,\"start\":35518},{\"end\":37557,\"start\":36730},{\"end\":31897,\"start\":30971},{\"end\":32610,\"start\":32164},{\"end\":34627,\"start\":32709},{\"end\":35011,\"start\":34874},{\"end\":36575,\"start\":35518},{\"end\":37557,\"start\":36730}]", "figure_caption": "[{\"end\":29337,\"start\":28942},{\"end\":29926,\"start\":29351},{\"end\":30296,\"start\":29940},{\"end\":30541,\"start\":30310},{\"end\":30971,\"start\":30544},{\"end\":32164,\"start\":31910},{\"end\":32709,\"start\":32623},{\"end\":34874,\"start\":34830},{\"end\":35518,\"start\":35024},{\"end\":36730,\"start\":36588},{\"end\":37713,\"start\":37570},{\"end\":29337,\"start\":28942},{\"end\":29926,\"start\":29351},{\"end\":30296,\"start\":29940},{\"end\":30541,\"start\":30310},{\"end\":30971,\"start\":30544},{\"end\":32164,\"start\":31910},{\"end\":32709,\"start\":32623},{\"end\":34874,\"start\":34830},{\"end\":35518,\"start\":35024},{\"end\":36730,\"start\":36588},{\"end\":37713,\"start\":37570}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4103,\"start\":4101},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4576,\"start\":4574},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4718,\"start\":4716},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4828,\"start\":4826},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8590,\"start\":8588},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9525,\"start\":9523},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9880,\"start\":9878},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10260,\"start\":10258},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10617,\"start\":10616},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11694,\"start\":11693},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12030,\"start\":12028},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13071,\"start\":13069},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13079,\"start\":13078},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14533,\"start\":14531},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15244,\"start\":15242},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16831,\"start\":16830},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21059,\"start\":21058},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21884,\"start\":21883},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23506,\"start\":23505},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4103,\"start\":4101},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4576,\"start\":4574},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4718,\"start\":4716},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4828,\"start\":4826},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8590,\"start\":8588},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9525,\"start\":9523},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9880,\"start\":9878},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10260,\"start\":10258},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10617,\"start\":10616},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11694,\"start\":11693},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12030,\"start\":12028},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13071,\"start\":13069},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13079,\"start\":13078},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14533,\"start\":14531},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15244,\"start\":15242},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16831,\"start\":16830},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21059,\"start\":21058},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21884,\"start\":21883},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23506,\"start\":23505}]", "bib_author_first_name": "[{\"end\":37772,\"start\":37767},{\"end\":37781,\"start\":37778},{\"end\":37791,\"start\":37789},{\"end\":37803,\"start\":37797},{\"end\":37815,\"start\":37811},{\"end\":37824,\"start\":37821},{\"end\":37835,\"start\":37832},{\"end\":37848,\"start\":37841},{\"end\":37859,\"start\":37854},{\"end\":38028,\"start\":38025},{\"end\":38041,\"start\":38037},{\"end\":38055,\"start\":38051},{\"end\":38070,\"start\":38064},{\"end\":38175,\"start\":38168},{\"end\":38194,\"start\":38188},{\"end\":38208,\"start\":38202},{\"end\":38300,\"start\":38297},{\"end\":38316,\"start\":38308},{\"end\":38327,\"start\":38323},{\"end\":38342,\"start\":38335},{\"end\":38357,\"start\":38352},{\"end\":38359,\"start\":38358},{\"end\":38376,\"start\":38368},{\"end\":38393,\"start\":38387},{\"end\":38413,\"start\":38407},{\"end\":38427,\"start\":38421},{\"end\":38442,\"start\":38436},{\"end\":38565,\"start\":38559},{\"end\":38577,\"start\":38571},{\"end\":38587,\"start\":38583},{\"end\":38598,\"start\":38594},{\"end\":38615,\"start\":38608},{\"end\":38617,\"start\":38616},{\"end\":38631,\"start\":38627},{\"end\":38873,\"start\":38866},{\"end\":38883,\"start\":38879},{\"end\":39060,\"start\":39052},{\"end\":39071,\"start\":39067},{\"end\":39085,\"start\":39078},{\"end\":39106,\"start\":39101},{\"end\":39128,\"start\":39115},{\"end\":39144,\"start\":39137},{\"end\":39349,\"start\":39345},{\"end\":39364,\"start\":39361},{\"end\":39378,\"start\":39375},{\"end\":39390,\"start\":39385},{\"end\":39580,\"start\":39572},{\"end\":39600,\"start\":39594},{\"end\":39620,\"start\":39609},{\"end\":39629,\"start\":39628},{\"end\":39636,\"start\":39632},{\"end\":39912,\"start\":39909},{\"end\":39927,\"start\":39923},{\"end\":40064,\"start\":40059},{\"end\":40081,\"start\":40073},{\"end\":40095,\"start\":40089},{\"end\":40109,\"start\":40101},{\"end\":40134,\"start\":40128},{\"end\":40145,\"start\":40141},{\"end\":40158,\"start\":40151},{\"end\":40160,\"start\":40159},{\"end\":40174,\"start\":40170},{\"end\":40442,\"start\":40435},{\"end\":40444,\"start\":40443},{\"end\":40468,\"start\":40461},{\"end\":40492,\"start\":40480},{\"end\":40512,\"start\":40502},{\"end\":40514,\"start\":40513},{\"end\":40671,\"start\":40665},{\"end\":40683,\"start\":40677},{\"end\":40699,\"start\":40691},{\"end\":40715,\"start\":40708},{\"end\":40727,\"start\":40723},{\"end\":40744,\"start\":40739},{\"end\":40758,\"start\":40752},{\"end\":40891,\"start\":40886},{\"end\":40908,\"start\":40901},{\"end\":40928,\"start\":40925},{\"end\":40987,\"start\":40984},{\"end\":40999,\"start\":40993},{\"end\":41013,\"start\":41010},{\"end\":41029,\"start\":41021},{\"end\":41045,\"start\":41039},{\"end\":41060,\"start\":41053},{\"end\":41074,\"start\":41069},{\"end\":41088,\"start\":41082},{\"end\":41098,\"start\":41093},{\"end\":41109,\"start\":41106},{\"end\":41316,\"start\":41310},{\"end\":41332,\"start\":41323},{\"end\":41345,\"start\":41338},{\"end\":41362,\"start\":41353},{\"end\":41371,\"start\":41367},{\"end\":41383,\"start\":41376},{\"end\":41396,\"start\":41389},{\"end\":41407,\"start\":41401},{\"end\":41561,\"start\":41557},{\"end\":41573,\"start\":41567},{\"end\":41586,\"start\":41579},{\"end\":41601,\"start\":41593},{\"end\":41611,\"start\":41607},{\"end\":41625,\"start\":41619},{\"end\":41634,\"start\":41631},{\"end\":41646,\"start\":41641},{\"end\":41657,\"start\":41652},{\"end\":41801,\"start\":41797},{\"end\":41814,\"start\":41807},{\"end\":41828,\"start\":41821},{\"end\":41840,\"start\":41833},{\"end\":41851,\"start\":41847},{\"end\":41862,\"start\":41859},{\"end\":41875,\"start\":41869},{\"end\":41886,\"start\":41881},{\"end\":41897,\"start\":41892},{\"end\":41924,\"start\":41920},{\"end\":41935,\"start\":41930},{\"end\":41948,\"start\":41941},{\"end\":41950,\"start\":41949},{\"end\":42108,\"start\":42100},{\"end\":42122,\"start\":42117},{\"end\":42136,\"start\":42132},{\"end\":42294,\"start\":42290},{\"end\":42307,\"start\":42303},{\"end\":42321,\"start\":42317},{\"end\":42333,\"start\":42330},{\"end\":42348,\"start\":42342},{\"end\":42513,\"start\":42507},{\"end\":42531,\"start\":42521},{\"end\":42542,\"start\":42540},{\"end\":42557,\"start\":42549},{\"end\":42570,\"start\":42563},{\"end\":42583,\"start\":42577},{\"end\":42599,\"start\":42592},{\"end\":42612,\"start\":42606},{\"end\":42863,\"start\":42859},{\"end\":42873,\"start\":42869},{\"end\":42886,\"start\":42879},{\"end\":42901,\"start\":42895},{\"end\":42921,\"start\":42913},{\"end\":42931,\"start\":42926},{\"end\":42944,\"start\":42938},{\"end\":42958,\"start\":42951},{\"end\":42971,\"start\":42965},{\"end\":43069,\"start\":43063},{\"end\":43079,\"start\":43075},{\"end\":43095,\"start\":43089},{\"end\":43108,\"start\":43101},{\"end\":43110,\"start\":43109},{\"end\":43124,\"start\":43120},{\"end\":43190,\"start\":43186},{\"end\":43208,\"start\":43201},{\"end\":43227,\"start\":43223},{\"end\":43240,\"start\":43236},{\"end\":43354,\"start\":43348},{\"end\":43367,\"start\":43363},{\"end\":43385,\"start\":43380},{\"end\":43397,\"start\":43391},{\"end\":43409,\"start\":43405},{\"end\":43424,\"start\":43418},{\"end\":43545,\"start\":43541},{\"end\":43557,\"start\":43553},{\"end\":43570,\"start\":43566},{\"end\":43706,\"start\":43699},{\"end\":43716,\"start\":43711},{\"end\":43729,\"start\":43721},{\"end\":43744,\"start\":43737},{\"end\":43754,\"start\":43750},{\"end\":43767,\"start\":43760},{\"end\":43912,\"start\":43906},{\"end\":43920,\"start\":43917},{\"end\":43930,\"start\":43927},{\"end\":44059,\"start\":44053},{\"end\":44070,\"start\":44064},{\"end\":44079,\"start\":44077},{\"end\":44089,\"start\":44085},{\"end\":44100,\"start\":44096},{\"end\":44107,\"start\":44105},{\"end\":44122,\"start\":44115},{\"end\":44130,\"start\":44127},{\"end\":44140,\"start\":44137},{\"end\":44211,\"start\":44205},{\"end\":44221,\"start\":44217},{\"end\":44232,\"start\":44227},{\"end\":44247,\"start\":44240},{\"end\":44258,\"start\":44252},{\"end\":44271,\"start\":44266},{\"end\":44282,\"start\":44278},{\"end\":44293,\"start\":44289},{\"end\":44305,\"start\":44301},{\"end\":44326,\"start\":44319},{\"end\":44437,\"start\":44430},{\"end\":44457,\"start\":44452},{\"end\":44470,\"start\":44466},{\"end\":44497,\"start\":44490},{\"end\":44512,\"start\":44505},{\"end\":44537,\"start\":44528},{\"end\":44549,\"start\":44542},{\"end\":44564,\"start\":44560},{\"end\":44668,\"start\":44662},{\"end\":44688,\"start\":44682},{\"end\":44702,\"start\":44695},{\"end\":44717,\"start\":44713},{\"end\":44733,\"start\":44727},{\"end\":44749,\"start\":44744},{\"end\":44760,\"start\":44759},{\"end\":44779,\"start\":44774},{\"end\":44795,\"start\":44786},{\"end\":44808,\"start\":44802},{\"end\":44983,\"start\":44977},{\"end\":44995,\"start\":44991},{\"end\":45011,\"start\":45007},{\"end\":45032,\"start\":45024},{\"end\":45048,\"start\":45042},{\"end\":45196,\"start\":45190},{\"end\":45208,\"start\":45204},{\"end\":45227,\"start\":45221},{\"end\":45244,\"start\":45241},{\"end\":45433,\"start\":45425},{\"end\":45448,\"start\":45444},{\"end\":45466,\"start\":45457},{\"end\":45480,\"start\":45474},{\"end\":45609,\"start\":45602},{\"end\":45619,\"start\":45615},{\"end\":45634,\"start\":45626},{\"end\":45649,\"start\":45642},{\"end\":45661,\"start\":45655},{\"end\":45675,\"start\":45667},{\"end\":45687,\"start\":45682},{\"end\":45702,\"start\":45693},{\"end\":45848,\"start\":45844},{\"end\":45865,\"start\":45858},{\"end\":45881,\"start\":45878},{\"end\":45896,\"start\":45892},{\"end\":45919,\"start\":45914},{\"end\":45935,\"start\":45929},{\"end\":45952,\"start\":45941},{\"end\":45965,\"start\":45960},{\"end\":45981,\"start\":45975},{\"end\":45994,\"start\":45988},{\"end\":46009,\"start\":46004},{\"end\":46029,\"start\":46020},{\"end\":46035,\"start\":46030},{\"end\":46054,\"start\":46046},{\"end\":46069,\"start\":46061},{\"end\":46275,\"start\":46268},{\"end\":46286,\"start\":46282},{\"end\":46300,\"start\":46294},{\"end\":46313,\"start\":46307},{\"end\":46325,\"start\":46318},{\"end\":46334,\"start\":46330},{\"end\":46347,\"start\":46340},{\"end\":46358,\"start\":46352},{\"end\":46369,\"start\":46364},{\"end\":46596,\"start\":46591},{\"end\":46607,\"start\":46603},{\"end\":46619,\"start\":46614},{\"end\":46631,\"start\":46624},{\"end\":46642,\"start\":46636},{\"end\":46652,\"start\":46648},{\"end\":46669,\"start\":46662},{\"end\":46671,\"start\":46670},{\"end\":46685,\"start\":46681},{\"end\":46923,\"start\":46917},{\"end\":46938,\"start\":46931},{\"end\":46955,\"start\":46948},{\"end\":46971,\"start\":46964},{\"end\":46989,\"start\":46983},{\"end\":47008,\"start\":47003},{\"end\":47021,\"start\":47017},{\"end\":47034,\"start\":47027},{\"end\":47053,\"start\":47047},{\"end\":47068,\"start\":47063},{\"end\":47197,\"start\":47190},{\"end\":47205,\"start\":47203},{\"end\":47214,\"start\":47211},{\"end\":47228,\"start\":47222},{\"end\":47239,\"start\":47236},{\"end\":47250,\"start\":47247},{\"end\":47260,\"start\":47256},{\"end\":47270,\"start\":47266},{\"end\":47317,\"start\":47313},{\"end\":47333,\"start\":47324},{\"end\":47347,\"start\":47341},{\"end\":47362,\"start\":47357},{\"end\":47373,\"start\":47369},{\"end\":47644,\"start\":47637},{\"end\":47659,\"start\":47651},{\"end\":47672,\"start\":47666},{\"end\":47687,\"start\":47680},{\"end\":47701,\"start\":47694},{\"end\":47815,\"start\":47808},{\"end\":47827,\"start\":47822},{\"end\":47841,\"start\":47834},{\"end\":47850,\"start\":47846},{\"end\":48009,\"start\":48002},{\"end\":48021,\"start\":48015},{\"end\":48034,\"start\":48028},{\"end\":48048,\"start\":48039},{\"end\":48061,\"start\":48054},{\"end\":48134,\"start\":48127},{\"end\":48147,\"start\":48140},{\"end\":48163,\"start\":48155},{\"end\":48177,\"start\":48171},{\"end\":48193,\"start\":48184},{\"end\":48203,\"start\":48201},{\"end\":48218,\"start\":48211},{\"end\":48232,\"start\":48223},{\"end\":48444,\"start\":48441},{\"end\":48456,\"start\":48449},{\"end\":48470,\"start\":48463},{\"end\":48485,\"start\":48478},{\"end\":48500,\"start\":48493},{\"end\":48560,\"start\":48551},{\"end\":48569,\"start\":48567},{\"end\":48582,\"start\":48575},{\"end\":48597,\"start\":48591},{\"end\":48611,\"start\":48607},{\"end\":48842,\"start\":48836},{\"end\":48852,\"start\":48848},{\"end\":48878,\"start\":48872},{\"end\":48893,\"start\":48886},{\"end\":48906,\"start\":48898},{\"end\":48918,\"start\":48911},{\"end\":49036,\"start\":49029},{\"end\":49050,\"start\":49043},{\"end\":49060,\"start\":49056},{\"end\":49072,\"start\":49067},{\"end\":49084,\"start\":49077},{\"end\":49159,\"start\":49155},{\"end\":49171,\"start\":49168},{\"end\":49187,\"start\":49182},{\"end\":49200,\"start\":49195},{\"end\":49338,\"start\":49333},{\"end\":49349,\"start\":49345},{\"end\":49364,\"start\":49355},{\"end\":49374,\"start\":49369},{\"end\":49386,\"start\":49381},{\"end\":49396,\"start\":49392},{\"end\":49409,\"start\":49403},{\"end\":49421,\"start\":49416},{\"end\":49431,\"start\":49426},{\"end\":49443,\"start\":49439},{\"end\":49624,\"start\":49616},{\"end\":49640,\"start\":49632},{\"end\":49658,\"start\":49647},{\"end\":49667,\"start\":49663},{\"end\":49850,\"start\":49845},{\"end\":49865,\"start\":49858},{\"end\":49879,\"start\":49874},{\"end\":49892,\"start\":49887},{\"end\":49906,\"start\":49902},{\"end\":49920,\"start\":49913},{\"end\":49938,\"start\":49927},{\"end\":49950,\"start\":49946},{\"end\":49961,\"start\":49957},{\"end\":49968,\"start\":49966},{\"end\":50022,\"start\":50019},{\"end\":50032,\"start\":50030},{\"end\":50044,\"start\":50038},{\"end\":50056,\"start\":50050},{\"end\":50068,\"start\":50064},{\"end\":50078,\"start\":50075},{\"end\":50089,\"start\":50086},{\"end\":50259,\"start\":50251},{\"end\":50274,\"start\":50267},{\"end\":50284,\"start\":50280},{\"end\":50297,\"start\":50291},{\"end\":50311,\"start\":50304},{\"end\":50325,\"start\":50317},{\"end\":50337,\"start\":50331},{\"end\":50349,\"start\":50342},{\"end\":50363,\"start\":50354},{\"end\":50543,\"start\":50536},{\"end\":50555,\"start\":50550},{\"end\":50566,\"start\":50560},{\"end\":50580,\"start\":50575},{\"end\":50583,\"start\":50581},{\"end\":50593,\"start\":50588},{\"end\":50663,\"start\":50657},{\"end\":50984,\"start\":50979},{\"end\":37772,\"start\":37767},{\"end\":37781,\"start\":37778},{\"end\":37791,\"start\":37789},{\"end\":37803,\"start\":37797},{\"end\":37815,\"start\":37811},{\"end\":37824,\"start\":37821},{\"end\":37835,\"start\":37832},{\"end\":37848,\"start\":37841},{\"end\":37859,\"start\":37854},{\"end\":38028,\"start\":38025},{\"end\":38041,\"start\":38037},{\"end\":38055,\"start\":38051},{\"end\":38070,\"start\":38064},{\"end\":38175,\"start\":38168},{\"end\":38194,\"start\":38188},{\"end\":38208,\"start\":38202},{\"end\":38300,\"start\":38297},{\"end\":38316,\"start\":38308},{\"end\":38327,\"start\":38323},{\"end\":38342,\"start\":38335},{\"end\":38357,\"start\":38352},{\"end\":38359,\"start\":38358},{\"end\":38376,\"start\":38368},{\"end\":38393,\"start\":38387},{\"end\":38413,\"start\":38407},{\"end\":38427,\"start\":38421},{\"end\":38442,\"start\":38436},{\"end\":38565,\"start\":38559},{\"end\":38577,\"start\":38571},{\"end\":38587,\"start\":38583},{\"end\":38598,\"start\":38594},{\"end\":38615,\"start\":38608},{\"end\":38617,\"start\":38616},{\"end\":38631,\"start\":38627},{\"end\":38873,\"start\":38866},{\"end\":38883,\"start\":38879},{\"end\":39060,\"start\":39052},{\"end\":39071,\"start\":39067},{\"end\":39085,\"start\":39078},{\"end\":39106,\"start\":39101},{\"end\":39128,\"start\":39115},{\"end\":39144,\"start\":39137},{\"end\":39349,\"start\":39345},{\"end\":39364,\"start\":39361},{\"end\":39378,\"start\":39375},{\"end\":39390,\"start\":39385},{\"end\":39580,\"start\":39572},{\"end\":39600,\"start\":39594},{\"end\":39620,\"start\":39609},{\"end\":39629,\"start\":39628},{\"end\":39636,\"start\":39632},{\"end\":39912,\"start\":39909},{\"end\":39927,\"start\":39923},{\"end\":40064,\"start\":40059},{\"end\":40081,\"start\":40073},{\"end\":40095,\"start\":40089},{\"end\":40109,\"start\":40101},{\"end\":40134,\"start\":40128},{\"end\":40145,\"start\":40141},{\"end\":40158,\"start\":40151},{\"end\":40160,\"start\":40159},{\"end\":40174,\"start\":40170},{\"end\":40442,\"start\":40435},{\"end\":40444,\"start\":40443},{\"end\":40468,\"start\":40461},{\"end\":40492,\"start\":40480},{\"end\":40512,\"start\":40502},{\"end\":40514,\"start\":40513},{\"end\":40671,\"start\":40665},{\"end\":40683,\"start\":40677},{\"end\":40699,\"start\":40691},{\"end\":40715,\"start\":40708},{\"end\":40727,\"start\":40723},{\"end\":40744,\"start\":40739},{\"end\":40758,\"start\":40752},{\"end\":40891,\"start\":40886},{\"end\":40908,\"start\":40901},{\"end\":40928,\"start\":40925},{\"end\":40987,\"start\":40984},{\"end\":40999,\"start\":40993},{\"end\":41013,\"start\":41010},{\"end\":41029,\"start\":41021},{\"end\":41045,\"start\":41039},{\"end\":41060,\"start\":41053},{\"end\":41074,\"start\":41069},{\"end\":41088,\"start\":41082},{\"end\":41098,\"start\":41093},{\"end\":41109,\"start\":41106},{\"end\":41316,\"start\":41310},{\"end\":41332,\"start\":41323},{\"end\":41345,\"start\":41338},{\"end\":41362,\"start\":41353},{\"end\":41371,\"start\":41367},{\"end\":41383,\"start\":41376},{\"end\":41396,\"start\":41389},{\"end\":41407,\"start\":41401},{\"end\":41561,\"start\":41557},{\"end\":41573,\"start\":41567},{\"end\":41586,\"start\":41579},{\"end\":41601,\"start\":41593},{\"end\":41611,\"start\":41607},{\"end\":41625,\"start\":41619},{\"end\":41634,\"start\":41631},{\"end\":41646,\"start\":41641},{\"end\":41657,\"start\":41652},{\"end\":41801,\"start\":41797},{\"end\":41814,\"start\":41807},{\"end\":41828,\"start\":41821},{\"end\":41840,\"start\":41833},{\"end\":41851,\"start\":41847},{\"end\":41862,\"start\":41859},{\"end\":41875,\"start\":41869},{\"end\":41886,\"start\":41881},{\"end\":41897,\"start\":41892},{\"end\":41924,\"start\":41920},{\"end\":41935,\"start\":41930},{\"end\":41948,\"start\":41941},{\"end\":41950,\"start\":41949},{\"end\":42108,\"start\":42100},{\"end\":42122,\"start\":42117},{\"end\":42136,\"start\":42132},{\"end\":42294,\"start\":42290},{\"end\":42307,\"start\":42303},{\"end\":42321,\"start\":42317},{\"end\":42333,\"start\":42330},{\"end\":42348,\"start\":42342},{\"end\":42513,\"start\":42507},{\"end\":42531,\"start\":42521},{\"end\":42542,\"start\":42540},{\"end\":42557,\"start\":42549},{\"end\":42570,\"start\":42563},{\"end\":42583,\"start\":42577},{\"end\":42599,\"start\":42592},{\"end\":42612,\"start\":42606},{\"end\":42863,\"start\":42859},{\"end\":42873,\"start\":42869},{\"end\":42886,\"start\":42879},{\"end\":42901,\"start\":42895},{\"end\":42921,\"start\":42913},{\"end\":42931,\"start\":42926},{\"end\":42944,\"start\":42938},{\"end\":42958,\"start\":42951},{\"end\":42971,\"start\":42965},{\"end\":43069,\"start\":43063},{\"end\":43079,\"start\":43075},{\"end\":43095,\"start\":43089},{\"end\":43108,\"start\":43101},{\"end\":43110,\"start\":43109},{\"end\":43124,\"start\":43120},{\"end\":43190,\"start\":43186},{\"end\":43208,\"start\":43201},{\"end\":43227,\"start\":43223},{\"end\":43240,\"start\":43236},{\"end\":43354,\"start\":43348},{\"end\":43367,\"start\":43363},{\"end\":43385,\"start\":43380},{\"end\":43397,\"start\":43391},{\"end\":43409,\"start\":43405},{\"end\":43424,\"start\":43418},{\"end\":43545,\"start\":43541},{\"end\":43557,\"start\":43553},{\"end\":43570,\"start\":43566},{\"end\":43706,\"start\":43699},{\"end\":43716,\"start\":43711},{\"end\":43729,\"start\":43721},{\"end\":43744,\"start\":43737},{\"end\":43754,\"start\":43750},{\"end\":43767,\"start\":43760},{\"end\":43912,\"start\":43906},{\"end\":43920,\"start\":43917},{\"end\":43930,\"start\":43927},{\"end\":44059,\"start\":44053},{\"end\":44070,\"start\":44064},{\"end\":44079,\"start\":44077},{\"end\":44089,\"start\":44085},{\"end\":44100,\"start\":44096},{\"end\":44107,\"start\":44105},{\"end\":44122,\"start\":44115},{\"end\":44130,\"start\":44127},{\"end\":44140,\"start\":44137},{\"end\":44211,\"start\":44205},{\"end\":44221,\"start\":44217},{\"end\":44232,\"start\":44227},{\"end\":44247,\"start\":44240},{\"end\":44258,\"start\":44252},{\"end\":44271,\"start\":44266},{\"end\":44282,\"start\":44278},{\"end\":44293,\"start\":44289},{\"end\":44305,\"start\":44301},{\"end\":44326,\"start\":44319},{\"end\":44437,\"start\":44430},{\"end\":44457,\"start\":44452},{\"end\":44470,\"start\":44466},{\"end\":44497,\"start\":44490},{\"end\":44512,\"start\":44505},{\"end\":44537,\"start\":44528},{\"end\":44549,\"start\":44542},{\"end\":44564,\"start\":44560},{\"end\":44668,\"start\":44662},{\"end\":44688,\"start\":44682},{\"end\":44702,\"start\":44695},{\"end\":44717,\"start\":44713},{\"end\":44733,\"start\":44727},{\"end\":44749,\"start\":44744},{\"end\":44760,\"start\":44759},{\"end\":44779,\"start\":44774},{\"end\":44795,\"start\":44786},{\"end\":44808,\"start\":44802},{\"end\":44983,\"start\":44977},{\"end\":44995,\"start\":44991},{\"end\":45011,\"start\":45007},{\"end\":45032,\"start\":45024},{\"end\":45048,\"start\":45042},{\"end\":45196,\"start\":45190},{\"end\":45208,\"start\":45204},{\"end\":45227,\"start\":45221},{\"end\":45244,\"start\":45241},{\"end\":45433,\"start\":45425},{\"end\":45448,\"start\":45444},{\"end\":45466,\"start\":45457},{\"end\":45480,\"start\":45474},{\"end\":45609,\"start\":45602},{\"end\":45619,\"start\":45615},{\"end\":45634,\"start\":45626},{\"end\":45649,\"start\":45642},{\"end\":45661,\"start\":45655},{\"end\":45675,\"start\":45667},{\"end\":45687,\"start\":45682},{\"end\":45702,\"start\":45693},{\"end\":45848,\"start\":45844},{\"end\":45865,\"start\":45858},{\"end\":45881,\"start\":45878},{\"end\":45896,\"start\":45892},{\"end\":45919,\"start\":45914},{\"end\":45935,\"start\":45929},{\"end\":45952,\"start\":45941},{\"end\":45965,\"start\":45960},{\"end\":45981,\"start\":45975},{\"end\":45994,\"start\":45988},{\"end\":46009,\"start\":46004},{\"end\":46029,\"start\":46020},{\"end\":46035,\"start\":46030},{\"end\":46054,\"start\":46046},{\"end\":46069,\"start\":46061},{\"end\":46275,\"start\":46268},{\"end\":46286,\"start\":46282},{\"end\":46300,\"start\":46294},{\"end\":46313,\"start\":46307},{\"end\":46325,\"start\":46318},{\"end\":46334,\"start\":46330},{\"end\":46347,\"start\":46340},{\"end\":46358,\"start\":46352},{\"end\":46369,\"start\":46364},{\"end\":46596,\"start\":46591},{\"end\":46607,\"start\":46603},{\"end\":46619,\"start\":46614},{\"end\":46631,\"start\":46624},{\"end\":46642,\"start\":46636},{\"end\":46652,\"start\":46648},{\"end\":46669,\"start\":46662},{\"end\":46671,\"start\":46670},{\"end\":46685,\"start\":46681},{\"end\":46923,\"start\":46917},{\"end\":46938,\"start\":46931},{\"end\":46955,\"start\":46948},{\"end\":46971,\"start\":46964},{\"end\":46989,\"start\":46983},{\"end\":47008,\"start\":47003},{\"end\":47021,\"start\":47017},{\"end\":47034,\"start\":47027},{\"end\":47053,\"start\":47047},{\"end\":47068,\"start\":47063},{\"end\":47197,\"start\":47190},{\"end\":47205,\"start\":47203},{\"end\":47214,\"start\":47211},{\"end\":47228,\"start\":47222},{\"end\":47239,\"start\":47236},{\"end\":47250,\"start\":47247},{\"end\":47260,\"start\":47256},{\"end\":47270,\"start\":47266},{\"end\":47317,\"start\":47313},{\"end\":47333,\"start\":47324},{\"end\":47347,\"start\":47341},{\"end\":47362,\"start\":47357},{\"end\":47373,\"start\":47369},{\"end\":47644,\"start\":47637},{\"end\":47659,\"start\":47651},{\"end\":47672,\"start\":47666},{\"end\":47687,\"start\":47680},{\"end\":47701,\"start\":47694},{\"end\":47815,\"start\":47808},{\"end\":47827,\"start\":47822},{\"end\":47841,\"start\":47834},{\"end\":47850,\"start\":47846},{\"end\":48009,\"start\":48002},{\"end\":48021,\"start\":48015},{\"end\":48034,\"start\":48028},{\"end\":48048,\"start\":48039},{\"end\":48061,\"start\":48054},{\"end\":48134,\"start\":48127},{\"end\":48147,\"start\":48140},{\"end\":48163,\"start\":48155},{\"end\":48177,\"start\":48171},{\"end\":48193,\"start\":48184},{\"end\":48203,\"start\":48201},{\"end\":48218,\"start\":48211},{\"end\":48232,\"start\":48223},{\"end\":48444,\"start\":48441},{\"end\":48456,\"start\":48449},{\"end\":48470,\"start\":48463},{\"end\":48485,\"start\":48478},{\"end\":48500,\"start\":48493},{\"end\":48560,\"start\":48551},{\"end\":48569,\"start\":48567},{\"end\":48582,\"start\":48575},{\"end\":48597,\"start\":48591},{\"end\":48611,\"start\":48607},{\"end\":48842,\"start\":48836},{\"end\":48852,\"start\":48848},{\"end\":48878,\"start\":48872},{\"end\":48893,\"start\":48886},{\"end\":48906,\"start\":48898},{\"end\":48918,\"start\":48911},{\"end\":49036,\"start\":49029},{\"end\":49050,\"start\":49043},{\"end\":49060,\"start\":49056},{\"end\":49072,\"start\":49067},{\"end\":49084,\"start\":49077},{\"end\":49159,\"start\":49155},{\"end\":49171,\"start\":49168},{\"end\":49187,\"start\":49182},{\"end\":49200,\"start\":49195},{\"end\":49338,\"start\":49333},{\"end\":49349,\"start\":49345},{\"end\":49364,\"start\":49355},{\"end\":49374,\"start\":49369},{\"end\":49386,\"start\":49381},{\"end\":49396,\"start\":49392},{\"end\":49409,\"start\":49403},{\"end\":49421,\"start\":49416},{\"end\":49431,\"start\":49426},{\"end\":49443,\"start\":49439},{\"end\":49624,\"start\":49616},{\"end\":49640,\"start\":49632},{\"end\":49658,\"start\":49647},{\"end\":49667,\"start\":49663},{\"end\":49850,\"start\":49845},{\"end\":49865,\"start\":49858},{\"end\":49879,\"start\":49874},{\"end\":49892,\"start\":49887},{\"end\":49906,\"start\":49902},{\"end\":49920,\"start\":49913},{\"end\":49938,\"start\":49927},{\"end\":49950,\"start\":49946},{\"end\":49961,\"start\":49957},{\"end\":49968,\"start\":49966},{\"end\":50022,\"start\":50019},{\"end\":50032,\"start\":50030},{\"end\":50044,\"start\":50038},{\"end\":50056,\"start\":50050},{\"end\":50068,\"start\":50064},{\"end\":50078,\"start\":50075},{\"end\":50089,\"start\":50086},{\"end\":50259,\"start\":50251},{\"end\":50274,\"start\":50267},{\"end\":50284,\"start\":50280},{\"end\":50297,\"start\":50291},{\"end\":50311,\"start\":50304},{\"end\":50325,\"start\":50317},{\"end\":50337,\"start\":50331},{\"end\":50349,\"start\":50342},{\"end\":50363,\"start\":50354},{\"end\":50543,\"start\":50536},{\"end\":50555,\"start\":50550},{\"end\":50566,\"start\":50560},{\"end\":50580,\"start\":50575},{\"end\":50583,\"start\":50581},{\"end\":50593,\"start\":50588},{\"end\":50663,\"start\":50657},{\"end\":50984,\"start\":50979}]", "bib_author_last_name": "[{\"end\":37776,\"start\":37773},{\"end\":37787,\"start\":37782},{\"end\":37795,\"start\":37792},{\"end\":37809,\"start\":37804},{\"end\":37819,\"start\":37816},{\"end\":37830,\"start\":37825},{\"end\":37839,\"start\":37836},{\"end\":37852,\"start\":37849},{\"end\":37864,\"start\":37860},{\"end\":38035,\"start\":38029},{\"end\":38049,\"start\":38042},{\"end\":38062,\"start\":38056},{\"end\":38077,\"start\":38071},{\"end\":38186,\"start\":38176},{\"end\":38200,\"start\":38195},{\"end\":38220,\"start\":38209},{\"end\":38306,\"start\":38301},{\"end\":38321,\"start\":38317},{\"end\":38333,\"start\":38328},{\"end\":38350,\"start\":38343},{\"end\":38366,\"start\":38360},{\"end\":38385,\"start\":38377},{\"end\":38405,\"start\":38394},{\"end\":38419,\"start\":38414},{\"end\":38434,\"start\":38428},{\"end\":38449,\"start\":38443},{\"end\":38569,\"start\":38566},{\"end\":38581,\"start\":38578},{\"end\":38592,\"start\":38588},{\"end\":38606,\"start\":38599},{\"end\":38625,\"start\":38618},{\"end\":38639,\"start\":38632},{\"end\":38877,\"start\":38874},{\"end\":38895,\"start\":38884},{\"end\":39065,\"start\":39061},{\"end\":39076,\"start\":39072},{\"end\":39099,\"start\":39086},{\"end\":39113,\"start\":39107},{\"end\":39135,\"start\":39129},{\"end\":39155,\"start\":39145},{\"end\":39171,\"start\":39157},{\"end\":39359,\"start\":39350},{\"end\":39373,\"start\":39365},{\"end\":39383,\"start\":39379},{\"end\":39398,\"start\":39391},{\"end\":39592,\"start\":39581},{\"end\":39607,\"start\":39601},{\"end\":39626,\"start\":39621},{\"end\":39648,\"start\":39637},{\"end\":39921,\"start\":39913},{\"end\":39939,\"start\":39928},{\"end\":40071,\"start\":40065},{\"end\":40087,\"start\":40082},{\"end\":40099,\"start\":40096},{\"end\":40126,\"start\":40110},{\"end\":40139,\"start\":40135},{\"end\":40149,\"start\":40146},{\"end\":40168,\"start\":40161},{\"end\":40182,\"start\":40175},{\"end\":40191,\"start\":40184},{\"end\":40459,\"start\":40445},{\"end\":40478,\"start\":40469},{\"end\":40500,\"start\":40493},{\"end\":40524,\"start\":40515},{\"end\":40531,\"start\":40526},{\"end\":40675,\"start\":40672},{\"end\":40689,\"start\":40684},{\"end\":40706,\"start\":40700},{\"end\":40721,\"start\":40716},{\"end\":40737,\"start\":40728},{\"end\":40750,\"start\":40745},{\"end\":40765,\"start\":40759},{\"end\":40899,\"start\":40892},{\"end\":40923,\"start\":40909},{\"end\":40936,\"start\":40929},{\"end\":40946,\"start\":40938},{\"end\":40991,\"start\":40988},{\"end\":41008,\"start\":41000},{\"end\":41019,\"start\":41014},{\"end\":41037,\"start\":41030},{\"end\":41051,\"start\":41046},{\"end\":41067,\"start\":41061},{\"end\":41080,\"start\":41075},{\"end\":41091,\"start\":41089},{\"end\":41104,\"start\":41099},{\"end\":41119,\"start\":41110},{\"end\":41321,\"start\":41317},{\"end\":41336,\"start\":41333},{\"end\":41351,\"start\":41346},{\"end\":41365,\"start\":41363},{\"end\":41374,\"start\":41372},{\"end\":41387,\"start\":41384},{\"end\":41399,\"start\":41397},{\"end\":41411,\"start\":41408},{\"end\":41565,\"start\":41562},{\"end\":41577,\"start\":41574},{\"end\":41591,\"start\":41587},{\"end\":41605,\"start\":41602},{\"end\":41617,\"start\":41612},{\"end\":41629,\"start\":41626},{\"end\":41639,\"start\":41635},{\"end\":41650,\"start\":41647},{\"end\":41661,\"start\":41658},{\"end\":41805,\"start\":41802},{\"end\":41819,\"start\":41815},{\"end\":41831,\"start\":41829},{\"end\":41845,\"start\":41841},{\"end\":41857,\"start\":41852},{\"end\":41867,\"start\":41863},{\"end\":41879,\"start\":41876},{\"end\":41890,\"start\":41887},{\"end\":41901,\"start\":41898},{\"end\":41928,\"start\":41925},{\"end\":41939,\"start\":41936},{\"end\":41956,\"start\":41951},{\"end\":42115,\"start\":42109},{\"end\":42130,\"start\":42123},{\"end\":42141,\"start\":42137},{\"end\":42301,\"start\":42295},{\"end\":42315,\"start\":42308},{\"end\":42328,\"start\":42322},{\"end\":42340,\"start\":42334},{\"end\":42355,\"start\":42349},{\"end\":42519,\"start\":42514},{\"end\":42538,\"start\":42532},{\"end\":42547,\"start\":42543},{\"end\":42561,\"start\":42558},{\"end\":42575,\"start\":42571},{\"end\":42590,\"start\":42584},{\"end\":42604,\"start\":42600},{\"end\":42625,\"start\":42613},{\"end\":42867,\"start\":42864},{\"end\":42877,\"start\":42874},{\"end\":42893,\"start\":42887},{\"end\":42911,\"start\":42902},{\"end\":42924,\"start\":42922},{\"end\":42936,\"start\":42932},{\"end\":42949,\"start\":42945},{\"end\":42963,\"start\":42959},{\"end\":42980,\"start\":42972},{\"end\":43073,\"start\":43070},{\"end\":43087,\"start\":43080},{\"end\":43099,\"start\":43096},{\"end\":43118,\"start\":43111},{\"end\":43132,\"start\":43125},{\"end\":43199,\"start\":43191},{\"end\":43221,\"start\":43209},{\"end\":43234,\"start\":43228},{\"end\":43250,\"start\":43241},{\"end\":43361,\"start\":43355},{\"end\":43378,\"start\":43368},{\"end\":43389,\"start\":43386},{\"end\":43403,\"start\":43398},{\"end\":43416,\"start\":43410},{\"end\":43436,\"start\":43425},{\"end\":43551,\"start\":43546},{\"end\":43564,\"start\":43558},{\"end\":43576,\"start\":43571},{\"end\":43709,\"start\":43707},{\"end\":43719,\"start\":43717},{\"end\":43735,\"start\":43730},{\"end\":43748,\"start\":43745},{\"end\":43758,\"start\":43755},{\"end\":43771,\"start\":43768},{\"end\":43915,\"start\":43913},{\"end\":43925,\"start\":43921},{\"end\":43935,\"start\":43931},{\"end\":44062,\"start\":44060},{\"end\":44075,\"start\":44071},{\"end\":44083,\"start\":44080},{\"end\":44094,\"start\":44090},{\"end\":44103,\"start\":44101},{\"end\":44113,\"start\":44108},{\"end\":44125,\"start\":44123},{\"end\":44135,\"start\":44131},{\"end\":44143,\"start\":44141},{\"end\":44215,\"start\":44212},{\"end\":44225,\"start\":44222},{\"end\":44238,\"start\":44233},{\"end\":44250,\"start\":44248},{\"end\":44264,\"start\":44259},{\"end\":44276,\"start\":44272},{\"end\":44287,\"start\":44283},{\"end\":44299,\"start\":44294},{\"end\":44317,\"start\":44306},{\"end\":44335,\"start\":44327},{\"end\":44450,\"start\":44438},{\"end\":44464,\"start\":44458},{\"end\":44488,\"start\":44471},{\"end\":44503,\"start\":44498},{\"end\":44526,\"start\":44513},{\"end\":44540,\"start\":44538},{\"end\":44558,\"start\":44550},{\"end\":44569,\"start\":44565},{\"end\":44577,\"start\":44571},{\"end\":44680,\"start\":44669},{\"end\":44693,\"start\":44689},{\"end\":44711,\"start\":44703},{\"end\":44725,\"start\":44718},{\"end\":44742,\"start\":44734},{\"end\":44757,\"start\":44750},{\"end\":44772,\"start\":44761},{\"end\":44784,\"start\":44780},{\"end\":44800,\"start\":44796},{\"end\":44819,\"start\":44809},{\"end\":44989,\"start\":44984},{\"end\":45005,\"start\":44996},{\"end\":45022,\"start\":45012},{\"end\":45040,\"start\":45033},{\"end\":45060,\"start\":45049},{\"end\":45202,\"start\":45197},{\"end\":45219,\"start\":45209},{\"end\":45239,\"start\":45228},{\"end\":45252,\"start\":45245},{\"end\":45442,\"start\":45434},{\"end\":45455,\"start\":45449},{\"end\":45472,\"start\":45467},{\"end\":45493,\"start\":45481},{\"end\":45613,\"start\":45610},{\"end\":45624,\"start\":45620},{\"end\":45640,\"start\":45635},{\"end\":45653,\"start\":45650},{\"end\":45665,\"start\":45662},{\"end\":45680,\"start\":45676},{\"end\":45691,\"start\":45688},{\"end\":45706,\"start\":45703},{\"end\":45856,\"start\":45849},{\"end\":45876,\"start\":45866},{\"end\":45890,\"start\":45882},{\"end\":45906,\"start\":45897},{\"end\":45927,\"start\":45920},{\"end\":45939,\"start\":45936},{\"end\":45958,\"start\":45953},{\"end\":45973,\"start\":45966},{\"end\":45986,\"start\":45982},{\"end\":46002,\"start\":45995},{\"end\":46018,\"start\":46010},{\"end\":46044,\"start\":46036},{\"end\":46059,\"start\":46055},{\"end\":46075,\"start\":46070},{\"end\":46280,\"start\":46276},{\"end\":46292,\"start\":46287},{\"end\":46305,\"start\":46301},{\"end\":46316,\"start\":46314},{\"end\":46328,\"start\":46326},{\"end\":46338,\"start\":46335},{\"end\":46350,\"start\":46348},{\"end\":46362,\"start\":46359},{\"end\":46376,\"start\":46370},{\"end\":46601,\"start\":46597},{\"end\":46612,\"start\":46608},{\"end\":46622,\"start\":46620},{\"end\":46634,\"start\":46632},{\"end\":46646,\"start\":46643},{\"end\":46660,\"start\":46653},{\"end\":46679,\"start\":46672},{\"end\":46693,\"start\":46686},{\"end\":46929,\"start\":46924},{\"end\":46946,\"start\":46939},{\"end\":46962,\"start\":46956},{\"end\":46981,\"start\":46972},{\"end\":47001,\"start\":46990},{\"end\":47015,\"start\":47009},{\"end\":47025,\"start\":47022},{\"end\":47045,\"start\":47035},{\"end\":47061,\"start\":47054},{\"end\":47080,\"start\":47069},{\"end\":47201,\"start\":47198},{\"end\":47209,\"start\":47206},{\"end\":47220,\"start\":47215},{\"end\":47234,\"start\":47229},{\"end\":47245,\"start\":47240},{\"end\":47254,\"start\":47251},{\"end\":47264,\"start\":47261},{\"end\":47275,\"start\":47271},{\"end\":47322,\"start\":47318},{\"end\":47339,\"start\":47334},{\"end\":47355,\"start\":47348},{\"end\":47367,\"start\":47363},{\"end\":47378,\"start\":47374},{\"end\":47395,\"start\":47380},{\"end\":47649,\"start\":47645},{\"end\":47664,\"start\":47660},{\"end\":47678,\"start\":47673},{\"end\":47692,\"start\":47688},{\"end\":47716,\"start\":47702},{\"end\":47820,\"start\":47816},{\"end\":47832,\"start\":47828},{\"end\":47844,\"start\":47842},{\"end\":47856,\"start\":47851},{\"end\":48013,\"start\":48010},{\"end\":48026,\"start\":48022},{\"end\":48037,\"start\":48035},{\"end\":48052,\"start\":48049},{\"end\":48064,\"start\":48062},{\"end\":48138,\"start\":48135},{\"end\":48153,\"start\":48148},{\"end\":48169,\"start\":48164},{\"end\":48182,\"start\":48178},{\"end\":48199,\"start\":48194},{\"end\":48209,\"start\":48204},{\"end\":48221,\"start\":48219},{\"end\":48236,\"start\":48233},{\"end\":48447,\"start\":48445},{\"end\":48461,\"start\":48457},{\"end\":48476,\"start\":48471},{\"end\":48491,\"start\":48486},{\"end\":48513,\"start\":48501},{\"end\":48565,\"start\":48561},{\"end\":48573,\"start\":48570},{\"end\":48589,\"start\":48583},{\"end\":48605,\"start\":48598},{\"end\":48615,\"start\":48612},{\"end\":48846,\"start\":48843},{\"end\":48870,\"start\":48853},{\"end\":48884,\"start\":48879},{\"end\":48896,\"start\":48894},{\"end\":48909,\"start\":48907},{\"end\":48921,\"start\":48919},{\"end\":49041,\"start\":49037},{\"end\":49054,\"start\":49051},{\"end\":49065,\"start\":49061},{\"end\":49075,\"start\":49073},{\"end\":49088,\"start\":49085},{\"end\":49166,\"start\":49160},{\"end\":49180,\"start\":49172},{\"end\":49193,\"start\":49188},{\"end\":49211,\"start\":49201},{\"end\":49343,\"start\":49339},{\"end\":49353,\"start\":49350},{\"end\":49367,\"start\":49365},{\"end\":49379,\"start\":49375},{\"end\":49390,\"start\":49387},{\"end\":49401,\"start\":49397},{\"end\":49414,\"start\":49410},{\"end\":49424,\"start\":49422},{\"end\":49437,\"start\":49432},{\"end\":49447,\"start\":49444},{\"end\":49630,\"start\":49625},{\"end\":49645,\"start\":49641},{\"end\":49661,\"start\":49659},{\"end\":49671,\"start\":49668},{\"end\":49856,\"start\":49851},{\"end\":49872,\"start\":49866},{\"end\":49885,\"start\":49880},{\"end\":49900,\"start\":49893},{\"end\":49911,\"start\":49907},{\"end\":49925,\"start\":49921},{\"end\":49944,\"start\":49939},{\"end\":49955,\"start\":49951},{\"end\":49964,\"start\":49962},{\"end\":49981,\"start\":49969},{\"end\":50028,\"start\":50023},{\"end\":50036,\"start\":50033},{\"end\":50048,\"start\":50045},{\"end\":50062,\"start\":50057},{\"end\":50073,\"start\":50069},{\"end\":50084,\"start\":50079},{\"end\":50093,\"start\":50090},{\"end\":50265,\"start\":50260},{\"end\":50278,\"start\":50275},{\"end\":50289,\"start\":50285},{\"end\":50302,\"start\":50298},{\"end\":50315,\"start\":50312},{\"end\":50329,\"start\":50326},{\"end\":50340,\"start\":50338},{\"end\":50352,\"start\":50350},{\"end\":50367,\"start\":50364},{\"end\":50548,\"start\":50544},{\"end\":50558,\"start\":50556},{\"end\":50573,\"start\":50567},{\"end\":50586,\"start\":50584},{\"end\":50599,\"start\":50594},{\"end\":50668,\"start\":50664},{\"end\":50674,\"start\":50670},{\"end\":50678,\"start\":50676},{\"end\":51038,\"start\":51030},{\"end\":51054,\"start\":51040},{\"end\":51650,\"start\":51645},{\"end\":51911,\"start\":51906},{\"end\":52309,\"start\":52306},{\"end\":37776,\"start\":37773},{\"end\":37787,\"start\":37782},{\"end\":37795,\"start\":37792},{\"end\":37809,\"start\":37804},{\"end\":37819,\"start\":37816},{\"end\":37830,\"start\":37825},{\"end\":37839,\"start\":37836},{\"end\":37852,\"start\":37849},{\"end\":37864,\"start\":37860},{\"end\":38035,\"start\":38029},{\"end\":38049,\"start\":38042},{\"end\":38062,\"start\":38056},{\"end\":38077,\"start\":38071},{\"end\":38186,\"start\":38176},{\"end\":38200,\"start\":38195},{\"end\":38220,\"start\":38209},{\"end\":38306,\"start\":38301},{\"end\":38321,\"start\":38317},{\"end\":38333,\"start\":38328},{\"end\":38350,\"start\":38343},{\"end\":38366,\"start\":38360},{\"end\":38385,\"start\":38377},{\"end\":38405,\"start\":38394},{\"end\":38419,\"start\":38414},{\"end\":38434,\"start\":38428},{\"end\":38449,\"start\":38443},{\"end\":38569,\"start\":38566},{\"end\":38581,\"start\":38578},{\"end\":38592,\"start\":38588},{\"end\":38606,\"start\":38599},{\"end\":38625,\"start\":38618},{\"end\":38639,\"start\":38632},{\"end\":38877,\"start\":38874},{\"end\":38895,\"start\":38884},{\"end\":39065,\"start\":39061},{\"end\":39076,\"start\":39072},{\"end\":39099,\"start\":39086},{\"end\":39113,\"start\":39107},{\"end\":39135,\"start\":39129},{\"end\":39155,\"start\":39145},{\"end\":39171,\"start\":39157},{\"end\":39359,\"start\":39350},{\"end\":39373,\"start\":39365},{\"end\":39383,\"start\":39379},{\"end\":39398,\"start\":39391},{\"end\":39592,\"start\":39581},{\"end\":39607,\"start\":39601},{\"end\":39626,\"start\":39621},{\"end\":39648,\"start\":39637},{\"end\":39921,\"start\":39913},{\"end\":39939,\"start\":39928},{\"end\":40071,\"start\":40065},{\"end\":40087,\"start\":40082},{\"end\":40099,\"start\":40096},{\"end\":40126,\"start\":40110},{\"end\":40139,\"start\":40135},{\"end\":40149,\"start\":40146},{\"end\":40168,\"start\":40161},{\"end\":40182,\"start\":40175},{\"end\":40191,\"start\":40184},{\"end\":40459,\"start\":40445},{\"end\":40478,\"start\":40469},{\"end\":40500,\"start\":40493},{\"end\":40524,\"start\":40515},{\"end\":40531,\"start\":40526},{\"end\":40675,\"start\":40672},{\"end\":40689,\"start\":40684},{\"end\":40706,\"start\":40700},{\"end\":40721,\"start\":40716},{\"end\":40737,\"start\":40728},{\"end\":40750,\"start\":40745},{\"end\":40765,\"start\":40759},{\"end\":40899,\"start\":40892},{\"end\":40923,\"start\":40909},{\"end\":40936,\"start\":40929},{\"end\":40946,\"start\":40938},{\"end\":40991,\"start\":40988},{\"end\":41008,\"start\":41000},{\"end\":41019,\"start\":41014},{\"end\":41037,\"start\":41030},{\"end\":41051,\"start\":41046},{\"end\":41067,\"start\":41061},{\"end\":41080,\"start\":41075},{\"end\":41091,\"start\":41089},{\"end\":41104,\"start\":41099},{\"end\":41119,\"start\":41110},{\"end\":41321,\"start\":41317},{\"end\":41336,\"start\":41333},{\"end\":41351,\"start\":41346},{\"end\":41365,\"start\":41363},{\"end\":41374,\"start\":41372},{\"end\":41387,\"start\":41384},{\"end\":41399,\"start\":41397},{\"end\":41411,\"start\":41408},{\"end\":41565,\"start\":41562},{\"end\":41577,\"start\":41574},{\"end\":41591,\"start\":41587},{\"end\":41605,\"start\":41602},{\"end\":41617,\"start\":41612},{\"end\":41629,\"start\":41626},{\"end\":41639,\"start\":41635},{\"end\":41650,\"start\":41647},{\"end\":41661,\"start\":41658},{\"end\":41805,\"start\":41802},{\"end\":41819,\"start\":41815},{\"end\":41831,\"start\":41829},{\"end\":41845,\"start\":41841},{\"end\":41857,\"start\":41852},{\"end\":41867,\"start\":41863},{\"end\":41879,\"start\":41876},{\"end\":41890,\"start\":41887},{\"end\":41901,\"start\":41898},{\"end\":41928,\"start\":41925},{\"end\":41939,\"start\":41936},{\"end\":41956,\"start\":41951},{\"end\":42115,\"start\":42109},{\"end\":42130,\"start\":42123},{\"end\":42141,\"start\":42137},{\"end\":42301,\"start\":42295},{\"end\":42315,\"start\":42308},{\"end\":42328,\"start\":42322},{\"end\":42340,\"start\":42334},{\"end\":42355,\"start\":42349},{\"end\":42519,\"start\":42514},{\"end\":42538,\"start\":42532},{\"end\":42547,\"start\":42543},{\"end\":42561,\"start\":42558},{\"end\":42575,\"start\":42571},{\"end\":42590,\"start\":42584},{\"end\":42604,\"start\":42600},{\"end\":42625,\"start\":42613},{\"end\":42867,\"start\":42864},{\"end\":42877,\"start\":42874},{\"end\":42893,\"start\":42887},{\"end\":42911,\"start\":42902},{\"end\":42924,\"start\":42922},{\"end\":42936,\"start\":42932},{\"end\":42949,\"start\":42945},{\"end\":42963,\"start\":42959},{\"end\":42980,\"start\":42972},{\"end\":43073,\"start\":43070},{\"end\":43087,\"start\":43080},{\"end\":43099,\"start\":43096},{\"end\":43118,\"start\":43111},{\"end\":43132,\"start\":43125},{\"end\":43199,\"start\":43191},{\"end\":43221,\"start\":43209},{\"end\":43234,\"start\":43228},{\"end\":43250,\"start\":43241},{\"end\":43361,\"start\":43355},{\"end\":43378,\"start\":43368},{\"end\":43389,\"start\":43386},{\"end\":43403,\"start\":43398},{\"end\":43416,\"start\":43410},{\"end\":43436,\"start\":43425},{\"end\":43551,\"start\":43546},{\"end\":43564,\"start\":43558},{\"end\":43576,\"start\":43571},{\"end\":43709,\"start\":43707},{\"end\":43719,\"start\":43717},{\"end\":43735,\"start\":43730},{\"end\":43748,\"start\":43745},{\"end\":43758,\"start\":43755},{\"end\":43771,\"start\":43768},{\"end\":43915,\"start\":43913},{\"end\":43925,\"start\":43921},{\"end\":43935,\"start\":43931},{\"end\":44062,\"start\":44060},{\"end\":44075,\"start\":44071},{\"end\":44083,\"start\":44080},{\"end\":44094,\"start\":44090},{\"end\":44103,\"start\":44101},{\"end\":44113,\"start\":44108},{\"end\":44125,\"start\":44123},{\"end\":44135,\"start\":44131},{\"end\":44143,\"start\":44141},{\"end\":44215,\"start\":44212},{\"end\":44225,\"start\":44222},{\"end\":44238,\"start\":44233},{\"end\":44250,\"start\":44248},{\"end\":44264,\"start\":44259},{\"end\":44276,\"start\":44272},{\"end\":44287,\"start\":44283},{\"end\":44299,\"start\":44294},{\"end\":44317,\"start\":44306},{\"end\":44335,\"start\":44327},{\"end\":44450,\"start\":44438},{\"end\":44464,\"start\":44458},{\"end\":44488,\"start\":44471},{\"end\":44503,\"start\":44498},{\"end\":44526,\"start\":44513},{\"end\":44540,\"start\":44538},{\"end\":44558,\"start\":44550},{\"end\":44569,\"start\":44565},{\"end\":44577,\"start\":44571},{\"end\":44680,\"start\":44669},{\"end\":44693,\"start\":44689},{\"end\":44711,\"start\":44703},{\"end\":44725,\"start\":44718},{\"end\":44742,\"start\":44734},{\"end\":44757,\"start\":44750},{\"end\":44772,\"start\":44761},{\"end\":44784,\"start\":44780},{\"end\":44800,\"start\":44796},{\"end\":44819,\"start\":44809},{\"end\":44989,\"start\":44984},{\"end\":45005,\"start\":44996},{\"end\":45022,\"start\":45012},{\"end\":45040,\"start\":45033},{\"end\":45060,\"start\":45049},{\"end\":45202,\"start\":45197},{\"end\":45219,\"start\":45209},{\"end\":45239,\"start\":45228},{\"end\":45252,\"start\":45245},{\"end\":45442,\"start\":45434},{\"end\":45455,\"start\":45449},{\"end\":45472,\"start\":45467},{\"end\":45493,\"start\":45481},{\"end\":45613,\"start\":45610},{\"end\":45624,\"start\":45620},{\"end\":45640,\"start\":45635},{\"end\":45653,\"start\":45650},{\"end\":45665,\"start\":45662},{\"end\":45680,\"start\":45676},{\"end\":45691,\"start\":45688},{\"end\":45706,\"start\":45703},{\"end\":45856,\"start\":45849},{\"end\":45876,\"start\":45866},{\"end\":45890,\"start\":45882},{\"end\":45906,\"start\":45897},{\"end\":45927,\"start\":45920},{\"end\":45939,\"start\":45936},{\"end\":45958,\"start\":45953},{\"end\":45973,\"start\":45966},{\"end\":45986,\"start\":45982},{\"end\":46002,\"start\":45995},{\"end\":46018,\"start\":46010},{\"end\":46044,\"start\":46036},{\"end\":46059,\"start\":46055},{\"end\":46075,\"start\":46070},{\"end\":46280,\"start\":46276},{\"end\":46292,\"start\":46287},{\"end\":46305,\"start\":46301},{\"end\":46316,\"start\":46314},{\"end\":46328,\"start\":46326},{\"end\":46338,\"start\":46335},{\"end\":46350,\"start\":46348},{\"end\":46362,\"start\":46359},{\"end\":46376,\"start\":46370},{\"end\":46601,\"start\":46597},{\"end\":46612,\"start\":46608},{\"end\":46622,\"start\":46620},{\"end\":46634,\"start\":46632},{\"end\":46646,\"start\":46643},{\"end\":46660,\"start\":46653},{\"end\":46679,\"start\":46672},{\"end\":46693,\"start\":46686},{\"end\":46929,\"start\":46924},{\"end\":46946,\"start\":46939},{\"end\":46962,\"start\":46956},{\"end\":46981,\"start\":46972},{\"end\":47001,\"start\":46990},{\"end\":47015,\"start\":47009},{\"end\":47025,\"start\":47022},{\"end\":47045,\"start\":47035},{\"end\":47061,\"start\":47054},{\"end\":47080,\"start\":47069},{\"end\":47201,\"start\":47198},{\"end\":47209,\"start\":47206},{\"end\":47220,\"start\":47215},{\"end\":47234,\"start\":47229},{\"end\":47245,\"start\":47240},{\"end\":47254,\"start\":47251},{\"end\":47264,\"start\":47261},{\"end\":47275,\"start\":47271},{\"end\":47322,\"start\":47318},{\"end\":47339,\"start\":47334},{\"end\":47355,\"start\":47348},{\"end\":47367,\"start\":47363},{\"end\":47378,\"start\":47374},{\"end\":47395,\"start\":47380},{\"end\":47649,\"start\":47645},{\"end\":47664,\"start\":47660},{\"end\":47678,\"start\":47673},{\"end\":47692,\"start\":47688},{\"end\":47716,\"start\":47702},{\"end\":47820,\"start\":47816},{\"end\":47832,\"start\":47828},{\"end\":47844,\"start\":47842},{\"end\":47856,\"start\":47851},{\"end\":48013,\"start\":48010},{\"end\":48026,\"start\":48022},{\"end\":48037,\"start\":48035},{\"end\":48052,\"start\":48049},{\"end\":48064,\"start\":48062},{\"end\":48138,\"start\":48135},{\"end\":48153,\"start\":48148},{\"end\":48169,\"start\":48164},{\"end\":48182,\"start\":48178},{\"end\":48199,\"start\":48194},{\"end\":48209,\"start\":48204},{\"end\":48221,\"start\":48219},{\"end\":48236,\"start\":48233},{\"end\":48447,\"start\":48445},{\"end\":48461,\"start\":48457},{\"end\":48476,\"start\":48471},{\"end\":48491,\"start\":48486},{\"end\":48513,\"start\":48501},{\"end\":48565,\"start\":48561},{\"end\":48573,\"start\":48570},{\"end\":48589,\"start\":48583},{\"end\":48605,\"start\":48598},{\"end\":48615,\"start\":48612},{\"end\":48846,\"start\":48843},{\"end\":48870,\"start\":48853},{\"end\":48884,\"start\":48879},{\"end\":48896,\"start\":48894},{\"end\":48909,\"start\":48907},{\"end\":48921,\"start\":48919},{\"end\":49041,\"start\":49037},{\"end\":49054,\"start\":49051},{\"end\":49065,\"start\":49061},{\"end\":49075,\"start\":49073},{\"end\":49088,\"start\":49085},{\"end\":49166,\"start\":49160},{\"end\":49180,\"start\":49172},{\"end\":49193,\"start\":49188},{\"end\":49211,\"start\":49201},{\"end\":49343,\"start\":49339},{\"end\":49353,\"start\":49350},{\"end\":49367,\"start\":49365},{\"end\":49379,\"start\":49375},{\"end\":49390,\"start\":49387},{\"end\":49401,\"start\":49397},{\"end\":49414,\"start\":49410},{\"end\":49424,\"start\":49422},{\"end\":49437,\"start\":49432},{\"end\":49447,\"start\":49444},{\"end\":49630,\"start\":49625},{\"end\":49645,\"start\":49641},{\"end\":49661,\"start\":49659},{\"end\":49671,\"start\":49668},{\"end\":49856,\"start\":49851},{\"end\":49872,\"start\":49866},{\"end\":49885,\"start\":49880},{\"end\":49900,\"start\":49893},{\"end\":49911,\"start\":49907},{\"end\":49925,\"start\":49921},{\"end\":49944,\"start\":49939},{\"end\":49955,\"start\":49951},{\"end\":49964,\"start\":49962},{\"end\":49981,\"start\":49969},{\"end\":50028,\"start\":50023},{\"end\":50036,\"start\":50033},{\"end\":50048,\"start\":50045},{\"end\":50062,\"start\":50057},{\"end\":50073,\"start\":50069},{\"end\":50084,\"start\":50079},{\"end\":50093,\"start\":50090},{\"end\":50265,\"start\":50260},{\"end\":50278,\"start\":50275},{\"end\":50289,\"start\":50285},{\"end\":50302,\"start\":50298},{\"end\":50315,\"start\":50312},{\"end\":50329,\"start\":50326},{\"end\":50340,\"start\":50338},{\"end\":50352,\"start\":50350},{\"end\":50367,\"start\":50364},{\"end\":50548,\"start\":50544},{\"end\":50558,\"start\":50556},{\"end\":50573,\"start\":50567},{\"end\":50586,\"start\":50584},{\"end\":50599,\"start\":50594},{\"end\":50668,\"start\":50664},{\"end\":50674,\"start\":50670},{\"end\":50678,\"start\":50676},{\"end\":51038,\"start\":51030},{\"end\":51054,\"start\":51040},{\"end\":51650,\"start\":51645},{\"end\":51911,\"start\":51906},{\"end\":52309,\"start\":52306}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2012.15701\",\"id\":\"b0\"},\"end\":37952,\"start\":37767},{\"attributes\":{\"id\":\"b1\"},\"end\":38083,\"start\":37954},{\"attributes\":{\"doi\":\"arXiv:2109.12948\",\"id\":\"b2\"},\"end\":38256,\"start\":38085},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":38508,\"start\":38258},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":209531713},\"end\":38794,\"start\":38510},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215745195},\"end\":39050,\"start\":38796},{\"attributes\":{\"doi\":\"arXiv:1805.06085\",\"id\":\"b6\"},\"end\":39278,\"start\":39052},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":67750088},\"end\":39484,\"start\":39280},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b8\",\"matched_paper_id\":1518846},\"end\":39847,\"start\":39486},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b9\"},\"end\":39975,\"start\":39849},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":40433,\"start\":39977},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b11\"},\"end\":40599,\"start\":40435},{\"attributes\":{\"doi\":\"arXiv:2004.07320\",\"id\":\"b12\"},\"end\":40801,\"start\":40601},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b13\"},\"end\":40982,\"start\":40803},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b14\"},\"end\":41221,\"start\":40984},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":199577815},\"end\":41478,\"start\":41223},{\"attributes\":{\"doi\":\"arXiv:2202.07471\",\"id\":\"b16\"},\"end\":41697,\"start\":41480},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":258179335},\"end\":41918,\"start\":41699},{\"attributes\":{\"doi\":\"arXiv:1510.00149\",\"id\":\"b18\"},\"end\":42098,\"start\":41920},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b19\"},\"end\":42223,\"start\":42100},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235825979},\"end\":42411,\"start\":42225},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":39867659},\"end\":42786,\"start\":42413},{\"attributes\":{\"doi\":\"arXiv:2202.05239\",\"id\":\"b22\"},\"end\":43016,\"start\":42788},{\"attributes\":{\"id\":\"b23\"},\"end\":43184,\"start\":43018},{\"attributes\":{\"doi\":\"arXiv:2105.06990\",\"id\":\"b24\"},\"end\":43346,\"start\":43186},{\"attributes\":{\"doi\":\"arXiv:2208.09225\",\"id\":\"b25\"},\"end\":43517,\"start\":43348},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7785881},\"end\":43633,\"start\":43519},{\"attributes\":{\"doi\":\"arXiv:2210.06707\",\"id\":\"b27\"},\"end\":43807,\"start\":43635},{\"attributes\":{\"doi\":\"arXiv:1909.13144\",\"id\":\"b28\"},\"end\":43971,\"start\":43809},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":231861390},\"end\":44203,\"start\":43973},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b30\"},\"end\":44428,\"start\":44205},{\"attributes\":{\"doi\":\"arXiv:2209.05433\",\"id\":\"b31\"},\"end\":44660,\"start\":44430},{\"attributes\":{\"doi\":\"arXiv:2211.01786\",\"id\":\"b32\"},\"end\":44913,\"start\":44662},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":216056295},\"end\":45116,\"start\":44915},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":184487878},\"end\":45387,\"start\":45118},{\"attributes\":{\"id\":\"b35\"},\"end\":45423,\"start\":45389},{\"attributes\":{\"doi\":\"arXiv:2205.11380\",\"id\":\"b36\"},\"end\":45600,\"start\":45425},{\"attributes\":{\"doi\":\"arXiv:2203.06390\",\"id\":\"b37\"},\"end\":45781,\"start\":45602},{\"attributes\":{\"id\":\"b38\"},\"end\":45912,\"start\":45783},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b39\"},\"end\":46176,\"start\":45914},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":237941191},\"end\":46525,\"start\":46178},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":202565587},\"end\":46810,\"start\":46527},{\"attributes\":{\"doi\":\"arXiv:2201.11990\",\"id\":\"b42\"},\"end\":47116,\"start\":46812},{\"attributes\":{\"doi\":\"arXiv:2203.10705\",\"id\":\"b43\"},\"end\":47311,\"start\":47118},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b44\"},\"end\":47519,\"start\":47313},{\"attributes\":{\"id\":\"b45\"},\"end\":47725,\"start\":47521},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":221506623},\"end\":47912,\"start\":47727},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":247411282},\"end\":48125,\"start\":47914},{\"attributes\":{\"doi\":\"arXiv:2209.13325\",\"id\":\"b48\"},\"end\":48352,\"start\":48127},{\"attributes\":{\"doi\":\"arXiv:2004.09602\",\"id\":\"b49\"},\"end\":48549,\"start\":48354},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b50\"},\"end\":48741,\"start\":48551},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b51\"},\"end\":48957,\"start\":48743},{\"attributes\":{\"doi\":\"arXiv:2111.12293\",\"id\":\"b52\"},\"end\":49124,\"start\":48959},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":204509218},\"end\":49331,\"start\":49126},{\"attributes\":{\"doi\":\"arXiv:2210.02414\",\"id\":\"b54\"},\"end\":49530,\"start\":49333},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":50784025},\"end\":49792,\"start\":49532},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b56\"},\"end\":50017,\"start\":49794},{\"attributes\":{\"doi\":\"arXiv:2009.12812\",\"id\":\"b57\"},\"end\":50181,\"start\":50019},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":232092242},\"end\":50444,\"start\":50183},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":59413897},\"end\":50655,\"start\":50446},{\"attributes\":{\"doi\":\"arXiv:1611.01578\",\"id\":\"b60\"},\"end\":50770,\"start\":50657},{\"attributes\":{\"id\":\"b61\"},\"end\":51462,\"start\":50772},{\"attributes\":{\"id\":\"b62\"},\"end\":51902,\"start\":51464},{\"attributes\":{\"id\":\"b63\"},\"end\":51917,\"start\":51904},{\"attributes\":{\"id\":\"b64\"},\"end\":52556,\"start\":51919},{\"attributes\":{\"doi\":\"arXiv:2012.15701\",\"id\":\"b0\"},\"end\":37952,\"start\":37767},{\"attributes\":{\"id\":\"b1\"},\"end\":38083,\"start\":37954},{\"attributes\":{\"doi\":\"arXiv:2109.12948\",\"id\":\"b2\"},\"end\":38256,\"start\":38085},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":38508,\"start\":38258},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":209531713},\"end\":38794,\"start\":38510},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215745195},\"end\":39050,\"start\":38796},{\"attributes\":{\"doi\":\"arXiv:1805.06085\",\"id\":\"b6\"},\"end\":39278,\"start\":39052},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":67750088},\"end\":39484,\"start\":39280},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b8\",\"matched_paper_id\":1518846},\"end\":39847,\"start\":39486},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b9\"},\"end\":39975,\"start\":39849},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":40433,\"start\":39977},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b11\"},\"end\":40599,\"start\":40435},{\"attributes\":{\"doi\":\"arXiv:2004.07320\",\"id\":\"b12\"},\"end\":40801,\"start\":40601},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b13\"},\"end\":40982,\"start\":40803},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b14\"},\"end\":41221,\"start\":40984},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":199577815},\"end\":41478,\"start\":41223},{\"attributes\":{\"doi\":\"arXiv:2202.07471\",\"id\":\"b16\"},\"end\":41697,\"start\":41480},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":258179335},\"end\":41918,\"start\":41699},{\"attributes\":{\"doi\":\"arXiv:1510.00149\",\"id\":\"b18\"},\"end\":42098,\"start\":41920},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b19\"},\"end\":42223,\"start\":42100},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235825979},\"end\":42411,\"start\":42225},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":39867659},\"end\":42786,\"start\":42413},{\"attributes\":{\"doi\":\"arXiv:2202.05239\",\"id\":\"b22\"},\"end\":43016,\"start\":42788},{\"attributes\":{\"id\":\"b23\"},\"end\":43184,\"start\":43018},{\"attributes\":{\"doi\":\"arXiv:2105.06990\",\"id\":\"b24\"},\"end\":43346,\"start\":43186},{\"attributes\":{\"doi\":\"arXiv:2208.09225\",\"id\":\"b25\"},\"end\":43517,\"start\":43348},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7785881},\"end\":43633,\"start\":43519},{\"attributes\":{\"doi\":\"arXiv:2210.06707\",\"id\":\"b27\"},\"end\":43807,\"start\":43635},{\"attributes\":{\"doi\":\"arXiv:1909.13144\",\"id\":\"b28\"},\"end\":43971,\"start\":43809},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":231861390},\"end\":44203,\"start\":43973},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b30\"},\"end\":44428,\"start\":44205},{\"attributes\":{\"doi\":\"arXiv:2209.05433\",\"id\":\"b31\"},\"end\":44660,\"start\":44430},{\"attributes\":{\"doi\":\"arXiv:2211.01786\",\"id\":\"b32\"},\"end\":44913,\"start\":44662},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":216056295},\"end\":45116,\"start\":44915},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":184487878},\"end\":45387,\"start\":45118},{\"attributes\":{\"id\":\"b35\"},\"end\":45423,\"start\":45389},{\"attributes\":{\"doi\":\"arXiv:2205.11380\",\"id\":\"b36\"},\"end\":45600,\"start\":45425},{\"attributes\":{\"doi\":\"arXiv:2203.06390\",\"id\":\"b37\"},\"end\":45781,\"start\":45602},{\"attributes\":{\"id\":\"b38\"},\"end\":45912,\"start\":45783},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b39\"},\"end\":46176,\"start\":45914},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":237941191},\"end\":46525,\"start\":46178},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":202565587},\"end\":46810,\"start\":46527},{\"attributes\":{\"doi\":\"arXiv:2201.11990\",\"id\":\"b42\"},\"end\":47116,\"start\":46812},{\"attributes\":{\"doi\":\"arXiv:2203.10705\",\"id\":\"b43\"},\"end\":47311,\"start\":47118},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b44\"},\"end\":47519,\"start\":47313},{\"attributes\":{\"id\":\"b45\"},\"end\":47725,\"start\":47521},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":221506623},\"end\":47912,\"start\":47727},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":247411282},\"end\":48125,\"start\":47914},{\"attributes\":{\"doi\":\"arXiv:2209.13325\",\"id\":\"b48\"},\"end\":48352,\"start\":48127},{\"attributes\":{\"doi\":\"arXiv:2004.09602\",\"id\":\"b49\"},\"end\":48549,\"start\":48354},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b50\"},\"end\":48741,\"start\":48551},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b51\"},\"end\":48957,\"start\":48743},{\"attributes\":{\"doi\":\"arXiv:2111.12293\",\"id\":\"b52\"},\"end\":49124,\"start\":48959},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":204509218},\"end\":49331,\"start\":49126},{\"attributes\":{\"doi\":\"arXiv:2210.02414\",\"id\":\"b54\"},\"end\":49530,\"start\":49333},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":50784025},\"end\":49792,\"start\":49532},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b56\"},\"end\":50017,\"start\":49794},{\"attributes\":{\"doi\":\"arXiv:2009.12812\",\"id\":\"b57\"},\"end\":50181,\"start\":50019},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":232092242},\"end\":50444,\"start\":50183},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":59413897},\"end\":50655,\"start\":50446},{\"attributes\":{\"doi\":\"arXiv:1611.01578\",\"id\":\"b60\"},\"end\":50770,\"start\":50657},{\"attributes\":{\"id\":\"b61\"},\"end\":51462,\"start\":50772},{\"attributes\":{\"id\":\"b62\"},\"end\":51902,\"start\":51464},{\"attributes\":{\"id\":\"b63\"},\"end\":51917,\"start\":51904},{\"attributes\":{\"id\":\"b64\"},\"end\":52556,\"start\":51919}]", "bib_title": "[{\"end\":38295,\"start\":38258},{\"end\":38557,\"start\":38510},{\"end\":38864,\"start\":38796},{\"end\":39343,\"start\":39280},{\"end\":39570,\"start\":39486},{\"end\":40057,\"start\":39977},{\"end\":41308,\"start\":41223},{\"end\":41795,\"start\":41699},{\"end\":42288,\"start\":42225},{\"end\":42505,\"start\":42413},{\"end\":43061,\"start\":43018},{\"end\":43539,\"start\":43519},{\"end\":44051,\"start\":43973},{\"end\":44975,\"start\":44915},{\"end\":45188,\"start\":45118},{\"end\":46266,\"start\":46178},{\"end\":46589,\"start\":46527},{\"end\":47806,\"start\":47727},{\"end\":48000,\"start\":47914},{\"end\":49153,\"start\":49126},{\"end\":49614,\"start\":49532},{\"end\":50249,\"start\":50183},{\"end\":50534,\"start\":50446},{\"end\":50977,\"start\":50772},{\"end\":51643,\"start\":51464},{\"end\":38295,\"start\":38258},{\"end\":38557,\"start\":38510},{\"end\":38864,\"start\":38796},{\"end\":39343,\"start\":39280},{\"end\":39570,\"start\":39486},{\"end\":40057,\"start\":39977},{\"end\":41308,\"start\":41223},{\"end\":41795,\"start\":41699},{\"end\":42288,\"start\":42225},{\"end\":42505,\"start\":42413},{\"end\":43061,\"start\":43018},{\"end\":43539,\"start\":43519},{\"end\":44051,\"start\":43973},{\"end\":44975,\"start\":44915},{\"end\":45188,\"start\":45118},{\"end\":46266,\"start\":46178},{\"end\":46589,\"start\":46527},{\"end\":47806,\"start\":47727},{\"end\":48000,\"start\":47914},{\"end\":49153,\"start\":49126},{\"end\":49614,\"start\":49532},{\"end\":50249,\"start\":50183},{\"end\":50534,\"start\":50446},{\"end\":50977,\"start\":50772},{\"end\":51643,\"start\":51464}]", "bib_author": "[{\"end\":37778,\"start\":37767},{\"end\":37789,\"start\":37778},{\"end\":37797,\"start\":37789},{\"end\":37811,\"start\":37797},{\"end\":37821,\"start\":37811},{\"end\":37832,\"start\":37821},{\"end\":37841,\"start\":37832},{\"end\":37854,\"start\":37841},{\"end\":37866,\"start\":37854},{\"end\":38037,\"start\":38025},{\"end\":38051,\"start\":38037},{\"end\":38064,\"start\":38051},{\"end\":38079,\"start\":38064},{\"end\":38188,\"start\":38168},{\"end\":38202,\"start\":38188},{\"end\":38222,\"start\":38202},{\"end\":38308,\"start\":38297},{\"end\":38323,\"start\":38308},{\"end\":38335,\"start\":38323},{\"end\":38352,\"start\":38335},{\"end\":38368,\"start\":38352},{\"end\":38387,\"start\":38368},{\"end\":38407,\"start\":38387},{\"end\":38421,\"start\":38407},{\"end\":38436,\"start\":38421},{\"end\":38451,\"start\":38436},{\"end\":38571,\"start\":38559},{\"end\":38583,\"start\":38571},{\"end\":38594,\"start\":38583},{\"end\":38608,\"start\":38594},{\"end\":38627,\"start\":38608},{\"end\":38641,\"start\":38627},{\"end\":38879,\"start\":38866},{\"end\":38897,\"start\":38879},{\"end\":39067,\"start\":39052},{\"end\":39078,\"start\":39067},{\"end\":39101,\"start\":39078},{\"end\":39115,\"start\":39101},{\"end\":39137,\"start\":39115},{\"end\":39157,\"start\":39137},{\"end\":39173,\"start\":39157},{\"end\":39361,\"start\":39345},{\"end\":39375,\"start\":39361},{\"end\":39385,\"start\":39375},{\"end\":39400,\"start\":39385},{\"end\":39594,\"start\":39572},{\"end\":39609,\"start\":39594},{\"end\":39628,\"start\":39609},{\"end\":39632,\"start\":39628},{\"end\":39650,\"start\":39632},{\"end\":39923,\"start\":39909},{\"end\":39941,\"start\":39923},{\"end\":40073,\"start\":40059},{\"end\":40089,\"start\":40073},{\"end\":40101,\"start\":40089},{\"end\":40128,\"start\":40101},{\"end\":40141,\"start\":40128},{\"end\":40151,\"start\":40141},{\"end\":40170,\"start\":40151},{\"end\":40184,\"start\":40170},{\"end\":40193,\"start\":40184},{\"end\":40461,\"start\":40435},{\"end\":40480,\"start\":40461},{\"end\":40502,\"start\":40480},{\"end\":40526,\"start\":40502},{\"end\":40533,\"start\":40526},{\"end\":40677,\"start\":40665},{\"end\":40691,\"start\":40677},{\"end\":40708,\"start\":40691},{\"end\":40723,\"start\":40708},{\"end\":40739,\"start\":40723},{\"end\":40752,\"start\":40739},{\"end\":40767,\"start\":40752},{\"end\":40901,\"start\":40886},{\"end\":40925,\"start\":40901},{\"end\":40938,\"start\":40925},{\"end\":40948,\"start\":40938},{\"end\":40993,\"start\":40984},{\"end\":41010,\"start\":40993},{\"end\":41021,\"start\":41010},{\"end\":41039,\"start\":41021},{\"end\":41053,\"start\":41039},{\"end\":41069,\"start\":41053},{\"end\":41082,\"start\":41069},{\"end\":41093,\"start\":41082},{\"end\":41106,\"start\":41093},{\"end\":41121,\"start\":41106},{\"end\":41323,\"start\":41310},{\"end\":41338,\"start\":41323},{\"end\":41353,\"start\":41338},{\"end\":41367,\"start\":41353},{\"end\":41376,\"start\":41367},{\"end\":41389,\"start\":41376},{\"end\":41401,\"start\":41389},{\"end\":41413,\"start\":41401},{\"end\":41567,\"start\":41557},{\"end\":41579,\"start\":41567},{\"end\":41593,\"start\":41579},{\"end\":41607,\"start\":41593},{\"end\":41619,\"start\":41607},{\"end\":41631,\"start\":41619},{\"end\":41641,\"start\":41631},{\"end\":41652,\"start\":41641},{\"end\":41663,\"start\":41652},{\"end\":41807,\"start\":41797},{\"end\":41821,\"start\":41807},{\"end\":41833,\"start\":41821},{\"end\":41847,\"start\":41833},{\"end\":41859,\"start\":41847},{\"end\":41869,\"start\":41859},{\"end\":41881,\"start\":41869},{\"end\":41892,\"start\":41881},{\"end\":41903,\"start\":41892},{\"end\":41930,\"start\":41920},{\"end\":41941,\"start\":41930},{\"end\":41958,\"start\":41941},{\"end\":42117,\"start\":42100},{\"end\":42132,\"start\":42117},{\"end\":42143,\"start\":42132},{\"end\":42303,\"start\":42290},{\"end\":42317,\"start\":42303},{\"end\":42330,\"start\":42317},{\"end\":42342,\"start\":42330},{\"end\":42357,\"start\":42342},{\"end\":42521,\"start\":42507},{\"end\":42540,\"start\":42521},{\"end\":42549,\"start\":42540},{\"end\":42563,\"start\":42549},{\"end\":42577,\"start\":42563},{\"end\":42592,\"start\":42577},{\"end\":42606,\"start\":42592},{\"end\":42627,\"start\":42606},{\"end\":42869,\"start\":42859},{\"end\":42879,\"start\":42869},{\"end\":42895,\"start\":42879},{\"end\":42913,\"start\":42895},{\"end\":42926,\"start\":42913},{\"end\":42938,\"start\":42926},{\"end\":42951,\"start\":42938},{\"end\":42965,\"start\":42951},{\"end\":42982,\"start\":42965},{\"end\":43075,\"start\":43063},{\"end\":43089,\"start\":43075},{\"end\":43101,\"start\":43089},{\"end\":43120,\"start\":43101},{\"end\":43134,\"start\":43120},{\"end\":43201,\"start\":43186},{\"end\":43223,\"start\":43201},{\"end\":43236,\"start\":43223},{\"end\":43252,\"start\":43236},{\"end\":43363,\"start\":43348},{\"end\":43380,\"start\":43363},{\"end\":43391,\"start\":43380},{\"end\":43405,\"start\":43391},{\"end\":43418,\"start\":43405},{\"end\":43438,\"start\":43418},{\"end\":43553,\"start\":43541},{\"end\":43566,\"start\":43553},{\"end\":43578,\"start\":43566},{\"end\":43711,\"start\":43699},{\"end\":43721,\"start\":43711},{\"end\":43737,\"start\":43721},{\"end\":43750,\"start\":43737},{\"end\":43760,\"start\":43750},{\"end\":43773,\"start\":43760},{\"end\":43917,\"start\":43906},{\"end\":43927,\"start\":43917},{\"end\":43937,\"start\":43927},{\"end\":44064,\"start\":44053},{\"end\":44077,\"start\":44064},{\"end\":44085,\"start\":44077},{\"end\":44096,\"start\":44085},{\"end\":44105,\"start\":44096},{\"end\":44115,\"start\":44105},{\"end\":44127,\"start\":44115},{\"end\":44137,\"start\":44127},{\"end\":44145,\"start\":44137},{\"end\":44217,\"start\":44205},{\"end\":44227,\"start\":44217},{\"end\":44240,\"start\":44227},{\"end\":44252,\"start\":44240},{\"end\":44266,\"start\":44252},{\"end\":44278,\"start\":44266},{\"end\":44289,\"start\":44278},{\"end\":44301,\"start\":44289},{\"end\":44319,\"start\":44301},{\"end\":44337,\"start\":44319},{\"end\":44452,\"start\":44430},{\"end\":44466,\"start\":44452},{\"end\":44490,\"start\":44466},{\"end\":44505,\"start\":44490},{\"end\":44528,\"start\":44505},{\"end\":44542,\"start\":44528},{\"end\":44560,\"start\":44542},{\"end\":44571,\"start\":44560},{\"end\":44579,\"start\":44571},{\"end\":44682,\"start\":44662},{\"end\":44695,\"start\":44682},{\"end\":44713,\"start\":44695},{\"end\":44727,\"start\":44713},{\"end\":44744,\"start\":44727},{\"end\":44759,\"start\":44744},{\"end\":44774,\"start\":44759},{\"end\":44786,\"start\":44774},{\"end\":44802,\"start\":44786},{\"end\":44821,\"start\":44802},{\"end\":44991,\"start\":44977},{\"end\":45007,\"start\":44991},{\"end\":45024,\"start\":45007},{\"end\":45042,\"start\":45024},{\"end\":45062,\"start\":45042},{\"end\":45204,\"start\":45190},{\"end\":45221,\"start\":45204},{\"end\":45241,\"start\":45221},{\"end\":45254,\"start\":45241},{\"end\":45444,\"start\":45425},{\"end\":45457,\"start\":45444},{\"end\":45474,\"start\":45457},{\"end\":45495,\"start\":45474},{\"end\":45615,\"start\":45602},{\"end\":45626,\"start\":45615},{\"end\":45642,\"start\":45626},{\"end\":45655,\"start\":45642},{\"end\":45667,\"start\":45655},{\"end\":45682,\"start\":45667},{\"end\":45693,\"start\":45682},{\"end\":45708,\"start\":45693},{\"end\":45858,\"start\":45844},{\"end\":45878,\"start\":45858},{\"end\":45892,\"start\":45878},{\"end\":45908,\"start\":45892},{\"end\":45929,\"start\":45914},{\"end\":45941,\"start\":45929},{\"end\":45960,\"start\":45941},{\"end\":45975,\"start\":45960},{\"end\":45988,\"start\":45975},{\"end\":46004,\"start\":45988},{\"end\":46020,\"start\":46004},{\"end\":46046,\"start\":46020},{\"end\":46061,\"start\":46046},{\"end\":46077,\"start\":46061},{\"end\":46282,\"start\":46268},{\"end\":46294,\"start\":46282},{\"end\":46307,\"start\":46294},{\"end\":46318,\"start\":46307},{\"end\":46330,\"start\":46318},{\"end\":46340,\"start\":46330},{\"end\":46352,\"start\":46340},{\"end\":46364,\"start\":46352},{\"end\":46378,\"start\":46364},{\"end\":46603,\"start\":46591},{\"end\":46614,\"start\":46603},{\"end\":46624,\"start\":46614},{\"end\":46636,\"start\":46624},{\"end\":46648,\"start\":46636},{\"end\":46662,\"start\":46648},{\"end\":46681,\"start\":46662},{\"end\":46695,\"start\":46681},{\"end\":46931,\"start\":46917},{\"end\":46948,\"start\":46931},{\"end\":46964,\"start\":46948},{\"end\":46983,\"start\":46964},{\"end\":47003,\"start\":46983},{\"end\":47017,\"start\":47003},{\"end\":47027,\"start\":47017},{\"end\":47047,\"start\":47027},{\"end\":47063,\"start\":47047},{\"end\":47082,\"start\":47063},{\"end\":47203,\"start\":47190},{\"end\":47211,\"start\":47203},{\"end\":47222,\"start\":47211},{\"end\":47236,\"start\":47222},{\"end\":47247,\"start\":47236},{\"end\":47256,\"start\":47247},{\"end\":47266,\"start\":47256},{\"end\":47277,\"start\":47266},{\"end\":47324,\"start\":47313},{\"end\":47341,\"start\":47324},{\"end\":47357,\"start\":47341},{\"end\":47369,\"start\":47357},{\"end\":47380,\"start\":47369},{\"end\":47397,\"start\":47380},{\"end\":47651,\"start\":47637},{\"end\":47666,\"start\":47651},{\"end\":47680,\"start\":47666},{\"end\":47694,\"start\":47680},{\"end\":47718,\"start\":47694},{\"end\":47822,\"start\":47808},{\"end\":47834,\"start\":47822},{\"end\":47846,\"start\":47834},{\"end\":47858,\"start\":47846},{\"end\":48015,\"start\":48002},{\"end\":48028,\"start\":48015},{\"end\":48039,\"start\":48028},{\"end\":48054,\"start\":48039},{\"end\":48066,\"start\":48054},{\"end\":48140,\"start\":48127},{\"end\":48155,\"start\":48140},{\"end\":48171,\"start\":48155},{\"end\":48184,\"start\":48171},{\"end\":48201,\"start\":48184},{\"end\":48211,\"start\":48201},{\"end\":48223,\"start\":48211},{\"end\":48238,\"start\":48223},{\"end\":48449,\"start\":48441},{\"end\":48463,\"start\":48449},{\"end\":48478,\"start\":48463},{\"end\":48493,\"start\":48478},{\"end\":48515,\"start\":48493},{\"end\":48567,\"start\":48551},{\"end\":48575,\"start\":48567},{\"end\":48591,\"start\":48575},{\"end\":48607,\"start\":48591},{\"end\":48617,\"start\":48607},{\"end\":48848,\"start\":48836},{\"end\":48872,\"start\":48848},{\"end\":48886,\"start\":48872},{\"end\":48898,\"start\":48886},{\"end\":48911,\"start\":48898},{\"end\":48923,\"start\":48911},{\"end\":49043,\"start\":49029},{\"end\":49056,\"start\":49043},{\"end\":49067,\"start\":49056},{\"end\":49077,\"start\":49067},{\"end\":49090,\"start\":49077},{\"end\":49168,\"start\":49155},{\"end\":49182,\"start\":49168},{\"end\":49195,\"start\":49182},{\"end\":49213,\"start\":49195},{\"end\":49345,\"start\":49333},{\"end\":49355,\"start\":49345},{\"end\":49369,\"start\":49355},{\"end\":49381,\"start\":49369},{\"end\":49392,\"start\":49381},{\"end\":49403,\"start\":49392},{\"end\":49416,\"start\":49403},{\"end\":49426,\"start\":49416},{\"end\":49439,\"start\":49426},{\"end\":49449,\"start\":49439},{\"end\":49632,\"start\":49616},{\"end\":49647,\"start\":49632},{\"end\":49663,\"start\":49647},{\"end\":49673,\"start\":49663},{\"end\":49858,\"start\":49845},{\"end\":49874,\"start\":49858},{\"end\":49887,\"start\":49874},{\"end\":49902,\"start\":49887},{\"end\":49913,\"start\":49902},{\"end\":49927,\"start\":49913},{\"end\":49946,\"start\":49927},{\"end\":49957,\"start\":49946},{\"end\":49966,\"start\":49957},{\"end\":49983,\"start\":49966},{\"end\":50030,\"start\":50019},{\"end\":50038,\"start\":50030},{\"end\":50050,\"start\":50038},{\"end\":50064,\"start\":50050},{\"end\":50075,\"start\":50064},{\"end\":50086,\"start\":50075},{\"end\":50095,\"start\":50086},{\"end\":50267,\"start\":50251},{\"end\":50280,\"start\":50267},{\"end\":50291,\"start\":50280},{\"end\":50304,\"start\":50291},{\"end\":50317,\"start\":50304},{\"end\":50331,\"start\":50317},{\"end\":50342,\"start\":50331},{\"end\":50354,\"start\":50342},{\"end\":50369,\"start\":50354},{\"end\":50550,\"start\":50536},{\"end\":50560,\"start\":50550},{\"end\":50575,\"start\":50560},{\"end\":50588,\"start\":50575},{\"end\":50601,\"start\":50588},{\"end\":50670,\"start\":50657},{\"end\":50676,\"start\":50670},{\"end\":50680,\"start\":50676},{\"end\":50987,\"start\":50979},{\"end\":51652,\"start\":51645},{\"end\":51913,\"start\":51906},{\"end\":52311,\"start\":52306},{\"end\":37778,\"start\":37767},{\"end\":37789,\"start\":37778},{\"end\":37797,\"start\":37789},{\"end\":37811,\"start\":37797},{\"end\":37821,\"start\":37811},{\"end\":37832,\"start\":37821},{\"end\":37841,\"start\":37832},{\"end\":37854,\"start\":37841},{\"end\":37866,\"start\":37854},{\"end\":38037,\"start\":38025},{\"end\":38051,\"start\":38037},{\"end\":38064,\"start\":38051},{\"end\":38079,\"start\":38064},{\"end\":38188,\"start\":38168},{\"end\":38202,\"start\":38188},{\"end\":38222,\"start\":38202},{\"end\":38308,\"start\":38297},{\"end\":38323,\"start\":38308},{\"end\":38335,\"start\":38323},{\"end\":38352,\"start\":38335},{\"end\":38368,\"start\":38352},{\"end\":38387,\"start\":38368},{\"end\":38407,\"start\":38387},{\"end\":38421,\"start\":38407},{\"end\":38436,\"start\":38421},{\"end\":38451,\"start\":38436},{\"end\":38571,\"start\":38559},{\"end\":38583,\"start\":38571},{\"end\":38594,\"start\":38583},{\"end\":38608,\"start\":38594},{\"end\":38627,\"start\":38608},{\"end\":38641,\"start\":38627},{\"end\":38879,\"start\":38866},{\"end\":38897,\"start\":38879},{\"end\":39067,\"start\":39052},{\"end\":39078,\"start\":39067},{\"end\":39101,\"start\":39078},{\"end\":39115,\"start\":39101},{\"end\":39137,\"start\":39115},{\"end\":39157,\"start\":39137},{\"end\":39173,\"start\":39157},{\"end\":39361,\"start\":39345},{\"end\":39375,\"start\":39361},{\"end\":39385,\"start\":39375},{\"end\":39400,\"start\":39385},{\"end\":39594,\"start\":39572},{\"end\":39609,\"start\":39594},{\"end\":39628,\"start\":39609},{\"end\":39632,\"start\":39628},{\"end\":39650,\"start\":39632},{\"end\":39923,\"start\":39909},{\"end\":39941,\"start\":39923},{\"end\":40073,\"start\":40059},{\"end\":40089,\"start\":40073},{\"end\":40101,\"start\":40089},{\"end\":40128,\"start\":40101},{\"end\":40141,\"start\":40128},{\"end\":40151,\"start\":40141},{\"end\":40170,\"start\":40151},{\"end\":40184,\"start\":40170},{\"end\":40193,\"start\":40184},{\"end\":40461,\"start\":40435},{\"end\":40480,\"start\":40461},{\"end\":40502,\"start\":40480},{\"end\":40526,\"start\":40502},{\"end\":40533,\"start\":40526},{\"end\":40677,\"start\":40665},{\"end\":40691,\"start\":40677},{\"end\":40708,\"start\":40691},{\"end\":40723,\"start\":40708},{\"end\":40739,\"start\":40723},{\"end\":40752,\"start\":40739},{\"end\":40767,\"start\":40752},{\"end\":40901,\"start\":40886},{\"end\":40925,\"start\":40901},{\"end\":40938,\"start\":40925},{\"end\":40948,\"start\":40938},{\"end\":40993,\"start\":40984},{\"end\":41010,\"start\":40993},{\"end\":41021,\"start\":41010},{\"end\":41039,\"start\":41021},{\"end\":41053,\"start\":41039},{\"end\":41069,\"start\":41053},{\"end\":41082,\"start\":41069},{\"end\":41093,\"start\":41082},{\"end\":41106,\"start\":41093},{\"end\":41121,\"start\":41106},{\"end\":41323,\"start\":41310},{\"end\":41338,\"start\":41323},{\"end\":41353,\"start\":41338},{\"end\":41367,\"start\":41353},{\"end\":41376,\"start\":41367},{\"end\":41389,\"start\":41376},{\"end\":41401,\"start\":41389},{\"end\":41413,\"start\":41401},{\"end\":41567,\"start\":41557},{\"end\":41579,\"start\":41567},{\"end\":41593,\"start\":41579},{\"end\":41607,\"start\":41593},{\"end\":41619,\"start\":41607},{\"end\":41631,\"start\":41619},{\"end\":41641,\"start\":41631},{\"end\":41652,\"start\":41641},{\"end\":41663,\"start\":41652},{\"end\":41807,\"start\":41797},{\"end\":41821,\"start\":41807},{\"end\":41833,\"start\":41821},{\"end\":41847,\"start\":41833},{\"end\":41859,\"start\":41847},{\"end\":41869,\"start\":41859},{\"end\":41881,\"start\":41869},{\"end\":41892,\"start\":41881},{\"end\":41903,\"start\":41892},{\"end\":41930,\"start\":41920},{\"end\":41941,\"start\":41930},{\"end\":41958,\"start\":41941},{\"end\":42117,\"start\":42100},{\"end\":42132,\"start\":42117},{\"end\":42143,\"start\":42132},{\"end\":42303,\"start\":42290},{\"end\":42317,\"start\":42303},{\"end\":42330,\"start\":42317},{\"end\":42342,\"start\":42330},{\"end\":42357,\"start\":42342},{\"end\":42521,\"start\":42507},{\"end\":42540,\"start\":42521},{\"end\":42549,\"start\":42540},{\"end\":42563,\"start\":42549},{\"end\":42577,\"start\":42563},{\"end\":42592,\"start\":42577},{\"end\":42606,\"start\":42592},{\"end\":42627,\"start\":42606},{\"end\":42869,\"start\":42859},{\"end\":42879,\"start\":42869},{\"end\":42895,\"start\":42879},{\"end\":42913,\"start\":42895},{\"end\":42926,\"start\":42913},{\"end\":42938,\"start\":42926},{\"end\":42951,\"start\":42938},{\"end\":42965,\"start\":42951},{\"end\":42982,\"start\":42965},{\"end\":43075,\"start\":43063},{\"end\":43089,\"start\":43075},{\"end\":43101,\"start\":43089},{\"end\":43120,\"start\":43101},{\"end\":43134,\"start\":43120},{\"end\":43201,\"start\":43186},{\"end\":43223,\"start\":43201},{\"end\":43236,\"start\":43223},{\"end\":43252,\"start\":43236},{\"end\":43363,\"start\":43348},{\"end\":43380,\"start\":43363},{\"end\":43391,\"start\":43380},{\"end\":43405,\"start\":43391},{\"end\":43418,\"start\":43405},{\"end\":43438,\"start\":43418},{\"end\":43553,\"start\":43541},{\"end\":43566,\"start\":43553},{\"end\":43578,\"start\":43566},{\"end\":43711,\"start\":43699},{\"end\":43721,\"start\":43711},{\"end\":43737,\"start\":43721},{\"end\":43750,\"start\":43737},{\"end\":43760,\"start\":43750},{\"end\":43773,\"start\":43760},{\"end\":43917,\"start\":43906},{\"end\":43927,\"start\":43917},{\"end\":43937,\"start\":43927},{\"end\":44064,\"start\":44053},{\"end\":44077,\"start\":44064},{\"end\":44085,\"start\":44077},{\"end\":44096,\"start\":44085},{\"end\":44105,\"start\":44096},{\"end\":44115,\"start\":44105},{\"end\":44127,\"start\":44115},{\"end\":44137,\"start\":44127},{\"end\":44145,\"start\":44137},{\"end\":44217,\"start\":44205},{\"end\":44227,\"start\":44217},{\"end\":44240,\"start\":44227},{\"end\":44252,\"start\":44240},{\"end\":44266,\"start\":44252},{\"end\":44278,\"start\":44266},{\"end\":44289,\"start\":44278},{\"end\":44301,\"start\":44289},{\"end\":44319,\"start\":44301},{\"end\":44337,\"start\":44319},{\"end\":44452,\"start\":44430},{\"end\":44466,\"start\":44452},{\"end\":44490,\"start\":44466},{\"end\":44505,\"start\":44490},{\"end\":44528,\"start\":44505},{\"end\":44542,\"start\":44528},{\"end\":44560,\"start\":44542},{\"end\":44571,\"start\":44560},{\"end\":44579,\"start\":44571},{\"end\":44682,\"start\":44662},{\"end\":44695,\"start\":44682},{\"end\":44713,\"start\":44695},{\"end\":44727,\"start\":44713},{\"end\":44744,\"start\":44727},{\"end\":44759,\"start\":44744},{\"end\":44774,\"start\":44759},{\"end\":44786,\"start\":44774},{\"end\":44802,\"start\":44786},{\"end\":44821,\"start\":44802},{\"end\":44991,\"start\":44977},{\"end\":45007,\"start\":44991},{\"end\":45024,\"start\":45007},{\"end\":45042,\"start\":45024},{\"end\":45062,\"start\":45042},{\"end\":45204,\"start\":45190},{\"end\":45221,\"start\":45204},{\"end\":45241,\"start\":45221},{\"end\":45254,\"start\":45241},{\"end\":45444,\"start\":45425},{\"end\":45457,\"start\":45444},{\"end\":45474,\"start\":45457},{\"end\":45495,\"start\":45474},{\"end\":45615,\"start\":45602},{\"end\":45626,\"start\":45615},{\"end\":45642,\"start\":45626},{\"end\":45655,\"start\":45642},{\"end\":45667,\"start\":45655},{\"end\":45682,\"start\":45667},{\"end\":45693,\"start\":45682},{\"end\":45708,\"start\":45693},{\"end\":45858,\"start\":45844},{\"end\":45878,\"start\":45858},{\"end\":45892,\"start\":45878},{\"end\":45908,\"start\":45892},{\"end\":45929,\"start\":45914},{\"end\":45941,\"start\":45929},{\"end\":45960,\"start\":45941},{\"end\":45975,\"start\":45960},{\"end\":45988,\"start\":45975},{\"end\":46004,\"start\":45988},{\"end\":46020,\"start\":46004},{\"end\":46046,\"start\":46020},{\"end\":46061,\"start\":46046},{\"end\":46077,\"start\":46061},{\"end\":46282,\"start\":46268},{\"end\":46294,\"start\":46282},{\"end\":46307,\"start\":46294},{\"end\":46318,\"start\":46307},{\"end\":46330,\"start\":46318},{\"end\":46340,\"start\":46330},{\"end\":46352,\"start\":46340},{\"end\":46364,\"start\":46352},{\"end\":46378,\"start\":46364},{\"end\":46603,\"start\":46591},{\"end\":46614,\"start\":46603},{\"end\":46624,\"start\":46614},{\"end\":46636,\"start\":46624},{\"end\":46648,\"start\":46636},{\"end\":46662,\"start\":46648},{\"end\":46681,\"start\":46662},{\"end\":46695,\"start\":46681},{\"end\":46931,\"start\":46917},{\"end\":46948,\"start\":46931},{\"end\":46964,\"start\":46948},{\"end\":46983,\"start\":46964},{\"end\":47003,\"start\":46983},{\"end\":47017,\"start\":47003},{\"end\":47027,\"start\":47017},{\"end\":47047,\"start\":47027},{\"end\":47063,\"start\":47047},{\"end\":47082,\"start\":47063},{\"end\":47203,\"start\":47190},{\"end\":47211,\"start\":47203},{\"end\":47222,\"start\":47211},{\"end\":47236,\"start\":47222},{\"end\":47247,\"start\":47236},{\"end\":47256,\"start\":47247},{\"end\":47266,\"start\":47256},{\"end\":47277,\"start\":47266},{\"end\":47324,\"start\":47313},{\"end\":47341,\"start\":47324},{\"end\":47357,\"start\":47341},{\"end\":47369,\"start\":47357},{\"end\":47380,\"start\":47369},{\"end\":47397,\"start\":47380},{\"end\":47651,\"start\":47637},{\"end\":47666,\"start\":47651},{\"end\":47680,\"start\":47666},{\"end\":47694,\"start\":47680},{\"end\":47718,\"start\":47694},{\"end\":47822,\"start\":47808},{\"end\":47834,\"start\":47822},{\"end\":47846,\"start\":47834},{\"end\":47858,\"start\":47846},{\"end\":48015,\"start\":48002},{\"end\":48028,\"start\":48015},{\"end\":48039,\"start\":48028},{\"end\":48054,\"start\":48039},{\"end\":48066,\"start\":48054},{\"end\":48140,\"start\":48127},{\"end\":48155,\"start\":48140},{\"end\":48171,\"start\":48155},{\"end\":48184,\"start\":48171},{\"end\":48201,\"start\":48184},{\"end\":48211,\"start\":48201},{\"end\":48223,\"start\":48211},{\"end\":48238,\"start\":48223},{\"end\":48449,\"start\":48441},{\"end\":48463,\"start\":48449},{\"end\":48478,\"start\":48463},{\"end\":48493,\"start\":48478},{\"end\":48515,\"start\":48493},{\"end\":48567,\"start\":48551},{\"end\":48575,\"start\":48567},{\"end\":48591,\"start\":48575},{\"end\":48607,\"start\":48591},{\"end\":48617,\"start\":48607},{\"end\":48848,\"start\":48836},{\"end\":48872,\"start\":48848},{\"end\":48886,\"start\":48872},{\"end\":48898,\"start\":48886},{\"end\":48911,\"start\":48898},{\"end\":48923,\"start\":48911},{\"end\":49043,\"start\":49029},{\"end\":49056,\"start\":49043},{\"end\":49067,\"start\":49056},{\"end\":49077,\"start\":49067},{\"end\":49090,\"start\":49077},{\"end\":49168,\"start\":49155},{\"end\":49182,\"start\":49168},{\"end\":49195,\"start\":49182},{\"end\":49213,\"start\":49195},{\"end\":49345,\"start\":49333},{\"end\":49355,\"start\":49345},{\"end\":49369,\"start\":49355},{\"end\":49381,\"start\":49369},{\"end\":49392,\"start\":49381},{\"end\":49403,\"start\":49392},{\"end\":49416,\"start\":49403},{\"end\":49426,\"start\":49416},{\"end\":49439,\"start\":49426},{\"end\":49449,\"start\":49439},{\"end\":49632,\"start\":49616},{\"end\":49647,\"start\":49632},{\"end\":49663,\"start\":49647},{\"end\":49673,\"start\":49663},{\"end\":49858,\"start\":49845},{\"end\":49874,\"start\":49858},{\"end\":49887,\"start\":49874},{\"end\":49902,\"start\":49887},{\"end\":49913,\"start\":49902},{\"end\":49927,\"start\":49913},{\"end\":49946,\"start\":49927},{\"end\":49957,\"start\":49946},{\"end\":49966,\"start\":49957},{\"end\":49983,\"start\":49966},{\"end\":50030,\"start\":50019},{\"end\":50038,\"start\":50030},{\"end\":50050,\"start\":50038},{\"end\":50064,\"start\":50050},{\"end\":50075,\"start\":50064},{\"end\":50086,\"start\":50075},{\"end\":50095,\"start\":50086},{\"end\":50267,\"start\":50251},{\"end\":50280,\"start\":50267},{\"end\":50291,\"start\":50280},{\"end\":50304,\"start\":50291},{\"end\":50317,\"start\":50304},{\"end\":50331,\"start\":50317},{\"end\":50342,\"start\":50331},{\"end\":50354,\"start\":50342},{\"end\":50369,\"start\":50354},{\"end\":50550,\"start\":50536},{\"end\":50560,\"start\":50550},{\"end\":50575,\"start\":50560},{\"end\":50588,\"start\":50575},{\"end\":50601,\"start\":50588},{\"end\":50670,\"start\":50657},{\"end\":50676,\"start\":50670},{\"end\":50680,\"start\":50676},{\"end\":50987,\"start\":50979},{\"end\":51652,\"start\":51645},{\"end\":51913,\"start\":51906},{\"end\":52311,\"start\":52306}]", "bib_venue": "[{\"end\":37932,\"start\":37882},{\"end\":38023,\"start\":37954},{\"end\":38166,\"start\":38085},{\"end\":38500,\"start\":38451},{\"end\":38722,\"start\":38641},{\"end\":38978,\"start\":38897},{\"end\":39258,\"start\":39189},{\"end\":39474,\"start\":39400},{\"end\":39718,\"start\":39666},{\"end\":39907,\"start\":39849},{\"end\":40280,\"start\":40209},{\"end\":40579,\"start\":40549},{\"end\":40663,\"start\":40601},{\"end\":40884,\"start\":40803},{\"end\":41201,\"start\":41137},{\"end\":41472,\"start\":41413},{\"end\":41555,\"start\":41480},{\"end\":41909,\"start\":41903},{\"end\":42078,\"start\":41974},{\"end\":42203,\"start\":42159},{\"end\":42401,\"start\":42357},{\"end\":42711,\"start\":42627},{\"end\":42857,\"start\":42788},{\"end\":43178,\"start\":43134},{\"end\":43326,\"start\":43268},{\"end\":43497,\"start\":43454},{\"end\":43627,\"start\":43578},{\"end\":43697,\"start\":43635},{\"end\":43904,\"start\":43809},{\"end\":44197,\"start\":44145},{\"end\":44408,\"start\":44353},{\"end\":44624,\"start\":44595},{\"end\":44893,\"start\":44837},{\"end\":45106,\"start\":45062},{\"end\":45325,\"start\":45254},{\"end\":45421,\"start\":45389},{\"end\":45580,\"start\":45511},{\"end\":45761,\"start\":45724},{\"end\":45842,\"start\":45783},{\"end\":46156,\"start\":46093},{\"end\":46456,\"start\":46378},{\"end\":46756,\"start\":46695},{\"end\":46915,\"start\":46812},{\"end\":47188,\"start\":47118},{\"end\":47498,\"start\":47413},{\"end\":47635,\"start\":47521},{\"end\":47902,\"start\":47858},{\"end\":48118,\"start\":48066},{\"end\":48331,\"start\":48254},{\"end\":48439,\"start\":48354},{\"end\":48721,\"start\":48633},{\"end\":48834,\"start\":48743},{\"end\":49027,\"start\":48959},{\"end\":49321,\"start\":49213},{\"end\":49510,\"start\":49465},{\"end\":49737,\"start\":49673},{\"end\":49843,\"start\":49794},{\"end\":50161,\"start\":50111},{\"end\":50438,\"start\":50369},{\"end\":50645,\"start\":50601},{\"end\":50750,\"start\":50696},{\"end\":51028,\"start\":50987},{\"end\":51669,\"start\":51652},{\"end\":52304,\"start\":51919},{\"end\":37932,\"start\":37882},{\"end\":38023,\"start\":37954},{\"end\":38166,\"start\":38085},{\"end\":38500,\"start\":38451},{\"end\":38722,\"start\":38641},{\"end\":38978,\"start\":38897},{\"end\":39258,\"start\":39189},{\"end\":39474,\"start\":39400},{\"end\":39718,\"start\":39666},{\"end\":39907,\"start\":39849},{\"end\":40280,\"start\":40209},{\"end\":40579,\"start\":40549},{\"end\":40663,\"start\":40601},{\"end\":40884,\"start\":40803},{\"end\":41201,\"start\":41137},{\"end\":41472,\"start\":41413},{\"end\":41555,\"start\":41480},{\"end\":41909,\"start\":41903},{\"end\":42078,\"start\":41974},{\"end\":42203,\"start\":42159},{\"end\":42401,\"start\":42357},{\"end\":42711,\"start\":42627},{\"end\":42857,\"start\":42788},{\"end\":43178,\"start\":43134},{\"end\":43326,\"start\":43268},{\"end\":43497,\"start\":43454},{\"end\":43627,\"start\":43578},{\"end\":43697,\"start\":43635},{\"end\":43904,\"start\":43809},{\"end\":44197,\"start\":44145},{\"end\":44408,\"start\":44353},{\"end\":44624,\"start\":44595},{\"end\":44893,\"start\":44837},{\"end\":45106,\"start\":45062},{\"end\":45325,\"start\":45254},{\"end\":45421,\"start\":45389},{\"end\":45580,\"start\":45511},{\"end\":45761,\"start\":45724},{\"end\":45842,\"start\":45783},{\"end\":46156,\"start\":46093},{\"end\":46456,\"start\":46378},{\"end\":46756,\"start\":46695},{\"end\":46915,\"start\":46812},{\"end\":47188,\"start\":47118},{\"end\":47498,\"start\":47413},{\"end\":47635,\"start\":47521},{\"end\":47902,\"start\":47858},{\"end\":48118,\"start\":48066},{\"end\":48331,\"start\":48254},{\"end\":48439,\"start\":48354},{\"end\":48721,\"start\":48633},{\"end\":48834,\"start\":48743},{\"end\":49027,\"start\":48959},{\"end\":49321,\"start\":49213},{\"end\":49510,\"start\":49465},{\"end\":49737,\"start\":49673},{\"end\":49843,\"start\":49794},{\"end\":50161,\"start\":50111},{\"end\":50438,\"start\":50369},{\"end\":50645,\"start\":50601},{\"end\":50750,\"start\":50696},{\"end\":51028,\"start\":50987},{\"end\":51669,\"start\":51652},{\"end\":52304,\"start\":51919},{\"end\":38790,\"start\":38724},{\"end\":39046,\"start\":38980},{\"end\":39747,\"start\":39720},{\"end\":40338,\"start\":40282},{\"end\":42782,\"start\":42713},{\"end\":45383,\"start\":45327},{\"end\":46521,\"start\":46458},{\"end\":46804,\"start\":46758},{\"end\":49788,\"start\":49739},{\"end\":38790,\"start\":38724},{\"end\":39046,\"start\":38980},{\"end\":39747,\"start\":39720},{\"end\":40338,\"start\":40282},{\"end\":42782,\"start\":42713},{\"end\":45383,\"start\":45327},{\"end\":46521,\"start\":46458},{\"end\":46804,\"start\":46758},{\"end\":49788,\"start\":49739}]"}}}, "year": 2023, "month": 12, "day": 17}