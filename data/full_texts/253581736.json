{"id": 253581736, "updated": "2023-10-05 08:06:24.296", "metadata": {"title": "Language Conditioned Spatial Relation Reasoning for 3D Object Grounding", "authors": "[{\"first\":\"Shizhe\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Pierre-Louis\",\"last\":\"Guhur\",\"middle\":[]},{\"first\":\"Makarand\",\"last\":\"Tapaswi\",\"middle\":[]},{\"first\":\"Cordelia\",\"last\":\"Schmid\",\"middle\":[]},{\"first\":\"Ivan\",\"last\":\"Laptev\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Localizing objects in 3D scenes based on natural language requires understanding and reasoning about spatial relations. In particular, it is often crucial to distinguish similar objects referred by the text, such as\"the left most chair\"and\"a chair next to the window\". In this work we propose a language-conditioned transformer model for grounding 3D objects and their spatial relations. To this end, we design a spatial self-attention layer that accounts for relative distances and orientations between objects in input 3D point clouds. Training such a layer with visual and language inputs enables to disambiguate spatial relations and to localize objects referred by the text. To facilitate the cross-modal learning of relations, we further propose a teacher-student approach where the teacher model is first trained using ground-truth object labels, and then helps to train a student model using point cloud inputs. We perform ablation studies showing advantages of our approach. We also demonstrate our model to significantly outperform the state of the art on the challenging Nr3D, Sr3D and ScanRefer 3D object grounding datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2211.09646", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ChenGTSL22", "doi": "10.48550/arxiv.2211.09646"}}, "content": {"source": {"pdf_hash": "88c741be2c0200f0eff9877d99fbce612ba64d07", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.09646v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "36dc93f448104ea9da9a9fe96e662df83dff599c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/88c741be2c0200f0eff9877d99fbce612ba64d07.txt", "contents": "\nLanguage Conditioned Spatial Relation Reasoning for 3D Object Grounding\n\n\nShizhe Chen \nInria\n\u00c9cole normale sup\u00e9rieure\nCNRS\nPSL Research University\n\n\nPierre-Louis Guhur \nInria\n\u00c9cole normale sup\u00e9rieure\nCNRS\nPSL Research University\n\n\nMakarand Tapaswi \nIIIT Hyderabad\n\n\nCordelia Schmid \nInria\n\u00c9cole normale sup\u00e9rieure\nCNRS\nPSL Research University\n\n\nIvan Laptev \nInria\n\u00c9cole normale sup\u00e9rieure\nCNRS\nPSL Research University\n\n\nLanguage Conditioned Spatial Relation Reasoning for 3D Object Grounding\n\nLocalizing objects in 3D scenes based on natural language requires understanding and reasoning about spatial relations. In particular, it is often crucial to distinguish similar objects referred by the text, such as \"the left most chair\" and \"a chair next to the window\". In this work we propose a language-conditioned transformer model for grounding 3D objects and their spatial relations. To this end, we design a spatial self-attention layer that accounts for relative distances and orientations between objects in input 3D point clouds. Training such a layer with visual and language inputs enables to disambiguate spatial relations and to localize objects referred by the text. To facilitate the cross-modal learning of relations, we further propose a teacher-student approach where the teacher model is first trained using ground-truth object labels, and then helps to train a student model using point cloud inputs. We perform ablation studies showing advantages of our approach. We also demonstrate our model to significantly outperform the state of the art on the challenging Nr3D, Sr3D and ScanRefer 3D object grounding datasets.\n\nIntroduction\n\nTo carry out human instructions in the real world, robots should understand natural language and be able to ground mentioned objects in 3D environments. Following this objective, recent research is shifting from object grounding in 2D images [1,2,3,4,5,6] to the 3D object grounding task [7,8], where objects referred by a sentence should be localized in a 3D point cloud.\n\nLanguage expressions often refer to objects by their relative spatial locations in 3D scenes. Figure 1 illustrates example scenes and corresponding sentences where object grounding requires disambiguation between objects of the same class. For instance, \"the backpack closest to the piano\" requires to compare relative distances among objects, while \"choose the door on the left when facing them\" requires to infer the correct viewpoint and understand relative directions. Such complexity and diversity of the spatial language makes 3D object grounding highly challenging.\n\nGiven the critical role of the spatial language, many existing methods attempt to model spatial relations for 3D object grounding. Early work [9,10,11,12] explicitly build 3D visual graphs based on distances between objects and apply graph neural networks to learn relationships. However, since only nearest neighbors are considered in the graph, it remains difficult to infer relationships between distant objects such as \"farthest\". More recently, transformer architectures [9,13,14,15,16] have been used, as they have the potential to learn relations between pairs of objects with a multi-head selfattention mechanism [17]. Enabling transformers to better understand 3D spatial relations expressed by natural language, however, remains an open research problem.\n\nIn addition, 3D object grounding suffers more from the scarcity of training data compared to its 2D counterpart. The success of 2D object grounding models [18,19] can be largely attributed to large-scale datasets with image-text pairs [1,2,20,21], whereas the limited amount of 3D scene-36th Conference on Neural Information Processing Systems (NeurIPS 2022).  Figure 1: Example sentences that refer to objects in 3D scenes. The green box denotes the groundtruth object, the blue box is the prediction from our model, and the purple one is from a baseline model without explicit spatial reasoning and knowledge distillation.\n\nlanguage pairs increases the difficulty of 3D object reasoning. To address this challenge, Yang et al. [14] propose to use 2D-3D alignments to assist the training of 3D models, but require additional high-quality 2D images and camera parameters which are not always available.\n\nIn this work, we propose a Vision-and-Language 3D Relation reasoning model (ViL3DRel) to tackle the above issues in 3D object grounding. We design a new spatial self-attention layer for the transformer architecture to enhance 3D spatial understanding. This layer encodes relative distances and orientations for all pairs of objects, and explicitly learns spatial attention to capture spatial relations referred by the the language expressions. We propose a sigmoid softmax function to effectively fuse the spatial attention with standard self-attention. Rotation augmentation is used to further improve view invariant reasoning. To alleviate the negative effect of noisy object features, we further propose a teacher-student training approach. The teacher and student share the same model architecture, but use different inputs. The teacher uses ground-truth object labels, which enables to better learn the relation reasoning. The learned relation knowledge is distilled to the student taking point cloud features as input. Our code and models are available on the project webpage [22].\n\nTo summarize, our contributions are three-fold:\n\n\u2022 We propose a ViL3DRel model for the 3D object grounding task. It uses a new spatial self-attention to explicitly encode pairwise 3D spatial relations in the transformer layer for better language conditioned spatial understanding. \u2022 We employ a teacher-student training strategy, which facilitates the cross-modal learning of relations. The student with point cloud inputs benefits from the relation reasoning model of the teacher trained with ground-truth object label input. \u2022 We evaluate our ViL3DRel approach on the challenging Nr3D, Sr3D [9] and ScanRefer [7] benchmarks. Our model significantly outperforms state-of-the-art approaches, with 9.3, 8.3 absolute gains on Nr3D and Sr3D given ground-truth object proposals, and 4.47 points on ScanRefer using the same detected object proposals compared to the previous work [16] .\n\n\nRelated Work\n\n2D and 3D Object Grounding. The object grounding task aims to localize objects in 2D images or 3D point clouds given a sentence. Existing approaches can be categorized into two groups, namely one-and two-stage frameworks. The one-stage methods densely fuse the text features with patch-or point-level visual representations to directly regress the bounding box [18,23]. These approaches are flexible to detect various objects given the input sentence. The two-stage methods adopt a detection-then-matching pipeline [1,7,8,24], where the detection stage generates object proposals, and then the matching stage selects the best proposal according to the sentence. The decoupled object perception and cross-modal matching make the two-stage methods easier to analyze [15]. The benchmark datasets for 3D object grounding are ReferIt3D (Nr3D and Sr3D) [8] and Scan-Refer [7], which are all built upon scenes and object annotations in the ScanNet dataset [25].\n\nGraph-based approaches [26,27] are widely adopted in early works to infer spatial relations. The object graph is constructed by connecting each object with its top nearest neighbors [8,10,11] based on Euclidean distance. Inspired by the success of transformers [17], recent works [9,14,15,16,23] have adopted transformers for 3D object grounding. BEAUTY-DETR [28] and 3D-SPS [23] are one-stage methods. However, most works follow the two-stage framework with pre-detected object proposals. LanguageRefer [15] converts the cross-modal task into a language modeling problem with predicted object labels. SAT [14] adopts a multimodal transformer and transfers 2D semantics to assist the training of the 3D model. Multi-view transformer [16] aggregates object representations from multiple views to improve view robustness. Perhaps most similar to our work, 3DVG-Transformer [13] proposes a coordinate-guided attention module to encode spatial distances among objects. In contrast, we introduce a spatial self-attention module, which explicitly encodes both relative distances and relative orientations among objects. It also conditions spatial relations on the language and presents a more effective strategy for attention fusion. Transformers in Vision-and-Language. Transformer-based architectures have led to significant improvements in various vision-and-language tasks such as text-video retrieval [29], visual grounding [18,30], image captioning [31], vision question-answering [32] and vision-and-language navigation [33,34]. Most of the methods project textual and visual inputs into a sequence of tokens, and use multimodal transformers to learn cross-modal semantic relationships. While relative positional encoding (RPE) [35] has been explored mostly separately in vision and language transformers, we here integrate RPE with a cross-modal transformer and show its benefits for resolving spatial object relations referred by text.\n\nKnowledge Distillation. Knowledge distillation [36] is typically used to compress a large network (teacher) into a compact model (student). The common objective is to let the student model mimic the soft logits of the teacher model. Beyer et al. [37] show that a consistent and patient teacher is essential in knowledge distillation. Jiao et al. [38] further demonstrate that the intermediate representations learned by the teacher are beneficial. In contrast to distilling knowledge from a heavy model to a light one, our teacher network first learns cross-modal object relations using ground-truth object labels. We then transfer this knowledge to the target student network that uses noisy inputs.\n\n\nMethod\n\nGiven a sentence S, the goal of 3D object grounding is to detect an object referred in S by locating its 3D bounding box B T \u2208 R 6 in a 3D point clouds P scene . We assume P scene \u2208 R K\u00d76 contains K points each represented by XYZ coordinates and RGB values. We follow the detection-thenmatching framework [7,8,13,14,16] for 3D object grounding and assume to be given a list of object proposals (O 1 , \u00b7 \u00b7 \u00b7 , O N ) obtained via automatic 3D instance segmentation or ground-truth annotations (depending on the evaluation setup). Each object O i is represented by a subset of K i points P i \u2282 P scene , P i \u2208 R Ki\u00d76 . Our work is focused on interpreting spatial relations and selecting the target object O T , T \u2208 [1, N ] among N object proposals. We first present an overview of our ViL3DRel model in Section 3.1. We then introduce our language-conditioned spatial self-attention module and the teacher-student training approach in Sections 3.2 and 3.3 respectively. Figure 2 shows an overview of our ViL3DRel model, consisting of four modules: text encoding, object encoding, multimodal fusion and a grounding head as described next. Text encoding. Given the sentence S with M word tokens, we use a pre-trained BERT model [39] to encode S into a sequence of word features (s cls , s 1 , \u00b7 \u00b7 \u00b7 , s M ), s i \u2208 R d , where s cls is a special classification token and d is the dimensionality of the feature. Object encoding. For each object O i , we first normalize the coordinates of its point cloud P i into a unit ball, and then use PointNet++ [40] to compute the object feature o 0 i \u2208 R d . We also obtain the object center c i = [c x , c y , c z ] \u2208 R 3 and the object size z i = [z x , z y , z z ] \u2208 R 3 from object points P i as the mean and the spatial extent of P i respectively. We use a linear projection layer to obtain the absolute 3D location feature as\n\n\nArchitecture Overview\nl i = W l [c i ; z i ] \u2208 R d .\nMultimodal fusion. A stack of transformer layers [17] are applied to fuse the text and object modality features. Each transformer layer is composed of a spatial self-attention layer, a cross-attention layer and a feed-forward neural network (FFN). Our new spatial self-attention layer aims to improve the understanding of spatial relations among objects referred by the sentence. Assume o l i is the input embedding for object proposal O i before the i-th layer, we first add it with its absolute 3D location feature l i to enhance the spatial information. The spatial self-attention then generates contextualized object representations\u00f4 l i . We will describe the proposed spatial self-attention in details in Section 3.2. The cross-attention layer takes\u00f4 l i as queries and text features (s cls , s 1 , \u00b7 \u00b7 \u00b7 , s M ) as keys and values \"The correct backpack is the one farthest from the piano in the corner.\"\nText BERT !! # !\" # \"! # \"\" # \u2026 \u2026 \u2026 \u2026 \u2026 ( \u2026 ) \u2026 \u2026 \u2026 \u2026 absolute positions ! \" Spatial Attention Matrix Linear # !! \" !$ \" $! \" $$ \" \" , \" ! , ! ( , \u2026 ) , Linear % Linear & Linear ' !\"# \u2295 $ \" \u2026 Scaled Dot-Product \u2026 \u2026 \u2026 \u2026 \u2026 !! ( !$ ( $! ( $$ ( SigSof- tmax \u2a02 $ ( , \u2026 $ )\n, object tokens sentence token Self Attention Matrix Figure 2: Left: overview of the ViL3DRel model; Right: language-conditioned spatial self-attention.\n\nto learn cross-modal relations. The final FFN uses two fully connected layers to encode each output tokens after the attention layer. For detailed explanation of Transformer attention and FFN layers please refer to [17]. Object grounding. We use a two-layer feed-forward neural network as the object grounding head to predict the target object. Given the output embedding o L i from the last multimodal fusion layer, the grounding head generates a scalar score for each object proposal O i and applies softmax function to obtain the probability p i . The object proposal with the maximum probability is predicted as target.\n\n\nSpatial Self-Attention\n\nThe standard self-attention relies on token embeddings to learn relations among tokens. However, the input embeddings of object proposals are mixed with semantic features and absolute 3D locations, making it difficult to accurately infer spatial relations among objects such as relative distances or orientations in Figure 1. As spatial relations among objects referred by language are important to distinguish object instances, we propose to inject a language conditioned spatial attention that explicitly captures pairwise spatial relations to complement the standard self-attention. We first describe the standard self-attention and then introduce our proposed spatial self-attention as illustrated in the right part of Figure 2. Given the feature matrix X \u2208 R N \u00d7d for N object proposals, the self-attention mechanism first computes the query, key and value embeddings from X as Q = XW Q , K = XW K , V = XW V respectively, where W Q , W K , W V \u2208 R d\u00d7d h are learnable parameters and d h is the dimensionality of the output embedding. It then calculates attention weights given the query and key embeddings and aggregates the value embeddings as follows:\n\u2126 o = softmax QK T \u221a d h ; SelfAttn(Q, K, V ) = \u2126 o V,(1)\nwhere \u2126 o is an N \u00d7 N attention matrix, whose elements \u03c9 o ij is the attention weight between the i-th and j-th object proposal. In order to capture more diverse relations, multi-head self-attention is used where each head computes an independent SelfAttn(Q, K, V ) and the outputs from all heads are concatenated. To model spatial relations among objects, we propose to use explicit pairwise spatial features\nf s ij \u2208 R 5 , i, j \u2208 [1, N ]. For each pair of objects (O i , O j ), we compute their Euclidean distance d ij = ||c i \u2212 c j || 2\nas well as horizontal and vertical angles \u03b8 h , \u03b8 v of the line connecting object centers c i and c j . The computation details of the angles are provided in the Section A of supplementary material. We then define the pairwise spatial feature f s ij as:\nf s ij = [d ij , sin(\u03b8 h ), cos(\u03b8 h ), sin(\u03b8 v ), cos(\u03b8 v )].(2)\nWe automatically generate a language conditioned weight g s i to select relevant spatial relations for each object proposal O i as we mainly care about spatial relations described in the text for an object: where W S \u2208 R d\u00d75 is a learnable parameter and we omit the bias term for simplicity. We then define the spatial relevance for\ng s i = W T S (s cls + o l i ),(3)(O i , O j ) as: \u03c9 s ij = g s i \u00b7 f s ij .(4)\nWe modulate the self-attention in Eq (1) with the above language conditioned spatial relevancy using the sigmoid softmax (sigsoftmax) fusion function:\n\u03c9 ij = \u03c3(\u03c9 s ij ) exp(w o ij ) N l=1 \u03c3(\u03c9 s il ) exp(w o il ) ,(5)\nwhere \u03c3(\u00b7) is the sigmoid function. In this way, we compute the new self-attention matrix \u2126 = [\u03c9 ij ] N \u00d7N which explicitly considers 3D relative spatial locations, absolute spatial locations and object appearances in relation reasoning. We also use the multi-head mechanism to support different types of spatial relations and compute multiple spatial attentions to fuse with the standard self-attention. We concatenate the outputs from all heads in the spatial self-attention layer.\n\n\nTeacher-Student Training\n\nIncorrect estimation of object classes from P i can deteriorate the cross-modal alignment and spatial relation reasoning. To improve object grounding in point clouds previous work [14] relies on additional supervision in 2D images. Here, instead, we propose a new teacher-student training approach using no additional training data. The teacher and student models share the same transformer architectures except for the object encoding module. The teacher uses ground-truth object semantic features, while the student uses 3D point clouds. Since there is less cross-modal gap between the teacher's inputs and the sentence, the teacher model can excel at learning language relevant spatial relations among objects and aligning objects with the sentence. Such knowledge can then be distilled to the student model to improve its training. Figure 3 illustrates the teacher-student training pipeline. We describe the inputs of the teacher and the knowledge distillation objectives in the following.\n\nTeacher's inputs. We use ground-truth class labels and dominant colors of the object as the object representations for the teacher model, since the color is the most widely used attributes in the sentence. Specifically, we encode the ground-truth object class label using pre-trained Glove word vectors [41].\n\nTo obtain the dominant colors of the object, we fit a Gaussian Mixture model on the RGB values of all points in the object, where the mixture component is set to 3. We linearly project the mean value in each component into a color embedding and use the mixture weights of the components to average all the color embeddings. The final object representation is the sum of the class label embedding and the averaged color embedding. Knowledge distillation objectives. We transfer the attention weights and hidden states in the multimodal fusion module of the teacher model to the student model. The self-attentions learned by the teacher capture spatial relations among objects, and the cross-attentions measure cross-modal matching, which are both essential for 3D object grounding but hard to learn given noisy object features. We facilitate student learning by forcing it to mimic all the attention matrices \u2126 of the teacher using the following objective function:\nL attn = 1 LH L l=1 H h=1 MSE(\u2126 S lh \u2212 \u2126 T lh ),(6)\nwhere \u2126 S \u00b7 and \u2126 T \u00b7 are attention weights in student and teacher model respectively, MSE(\u00b7) is the mean square error function, L is the number of multimodal fusion layers and H is the number of heads in self-attention. In addition to distilling the relation knowledge in attention weights, we also distill the output embeddings of the transformer layers:\nL hidden = 1 LN L l=0 N i=1 MSE(o lS i \u2212 o lT i ).(7)\nTraining. We follow the previous works [8,7] to use multiple auxiliary losses in training the transformer model, including a 3D object grounding loss L og , sentence classification loss L sent and two object classification losses L u obj and L m obj . L sent relies on s cls to predict the target object class from the sentence. L u obj and L m obj use unimodal object representation o 0 i and multimodal fused object representation o L i respectively to predict the classes for input object proposals. Details are in Section A of supplementary material. Therefore, the overall training objective is as follows:\nL = L og + L sent + L u obj + L m obj + \u03bb a L attn + \u03bb h L hidden (8)\nwhere \u03bb a , \u03bb h are two hyper-parameters to balance the losses.\n\n\nExperiments\n\n\nDatasets\n\nNr3D dataset [8] contains 37,842 human-written sentences that refer to annotated objects in the 3D indoor scene dataset ScanNet [25]. The dataset includes 641 scenes with 511 (resp. 130) scenes for training (resp. validation). It covers 76 target object classes. The annotated sentences are designed to refer to objects with multiple same-class distractors in the scene. The sentences are split into \"easy\" and \"hard\" subsets in evaluation, where the target object in \"easy\" subset only contains one same-class distractor in the scene while it contains multiple ones in the \"hard\" subset. According to whether the sentence requires a specific viewpoint to ground the referred object, the dataset can also be partitioned into \"view depedent\" and \"view independent\" subsets. Sr3D dataset [8] is constructed using templates to automatically generate sentences. The sentences only utilize spatial relations to distinguish objects of the same class. It has 1,018 training scenes and 255 validation scenes from ScanNet and 83,570 sentences in total. The dataset can be split in the same way as Nr3D during evaluation. ScanRefer dataset [7] has 51,583 human-written sentences for 800 scenes in ScanNet. We follow the official split and use 36,665 and 9,508 samples for training and validation respectively. According to whether the target object is a unique object class in the scene, the dataset can be divided into a \"unique\" and a \"multiple\" subset.\n\n\nExperimental Setting\n\nEvaluation Metrics. We evaluate models under two evaluation settings. One uses ground-truth object proposals, which is the default setting in the Nr3D and Sr3D datasets. The metric is the accuracy of selecting the target bounding box among the proposals. The other setting does not provide ground-truth object proposals and requires the model to regress a 3D bounding box, which is the default setting for the ScanRefer dataset. The evaluation metrics are acc@0.25 and acc@0.5, which is the percentage of correctly predicted bounding boxes whose IoU is larger than 0.25 or 0.5 with the ground-truth. Implementation details. For the model architecture, we set the dimension d = 768 and use 12 heads for all the transformer layers. The text encoding module is a three-layer transformer initialized from BERT [39], and the multimodal fusion module contains four layers. The object encoding module PointNet++ [40] samples 1024 points for all the objects. These architecture parameters are the same as in previous work [14,16] to ensure a fair comparison. We first train the PointNet++ for object classification on ScanNet, which achieves 61.9% accuracy on the validation set. Its weights remain fixed during the following training steps. Rotation augmentation is used to increase the viewpoint invariance. The hyper-parameters in the loss function Eq (8) are set to \u03bb a = 1 and \u03bb h = 0.02. We train the model with a batch size of 128 and a learning rate of 0.0005 with warm-up of 5000 iterations and cosine decay scheduling. The AdamW algorithm [42] is used in the optimization. We train for 50 epochs for the teacher model and 100 epochs for the student model on Nr3D and ScanRefer datasets. The training epochs are reduced to half on the Sr3D dataset, as it is larger and easier to converge. All models are trained on a single NVIDIA RTX A6000 GPU. \n\n\nAblation Studies\n\nWe carry out extensive experiments on the Nr3D dataset to demonstrate the effectiveness of our proposed spatial self-attention and teacher-student training. The ablations on the ScanRefer dataset are provided in Section B of supplementary material.\n\n\nSpatial Relation Reasoning\n\nWe first evaluate the proposed spatial self-attention using ground-truth object labels, which decouples the object perception from spatial relation reasoning. Table 1 presents results on the Nr3D dataset.\n\nBaselines. Our baseline R1 is the teacher model in the left part of Figure 3 that excludes object color, rotation augmentation and the proposed spatial self-attention. This baseline achieves similar performance to the state-of-the-art LanguageRefer [15] (overall accuracy: 54.3%) which also uses ground-truth object labels. Row R2 shows that adding color information helps object grounding as sentences often contain color attributes. The rotation augmentation brings additional gains and improves accuracy from 55.1% (R2) to 62.4% (R3). It is more effective for view independent sentences (+8.7%) than for view dependent sentences (+4.5%) because rotation augmentation mainly improves view invariance. We consider R3 as a strong baseline and use it to demonstrate improvements of our proposed model. Pairwise spatial features: distance vs. orientation. Rows R4-R5 in Table 1 compare different pairwise spatial features defined in (2). R4 only uses pairwise distances, while R5 only uses pairwise orientations. We can see that pairwise distances are beneficial for view-independent sentences which contain distance-related spatial relations such as \"next to\" and \"farthest from\", but do not improve the performance for view-dependent sentences. In contrast, pairwise orientations significantly boost the performance for view-dependent sentences (+10.2%). This shows that explicitly encoding the relative orientations facilitates the learning of view dependent spatial relations such as \"in front of\" and \"to the left of\". The relative distance and orientation features are also complementary. Their combination achieves the best performance as shown in our full model (R9).  Table 3 provides a more systematic analysis of R3-5 and R9 in Table 1 to show the contribution of spatial relation modeling to different types of sentences. We categorize sentences into four groups according to whether the sentence describes spatial relations in terms of distances or orientations. We can see that the explicit pairwise distance modeling contributes most to the distance-only sentences but has little influence on samples with orientation-related sentences. On the other hand, the pairwise orientation modeling can significantly improve orientation-related sentences by 10.5%. Combining both pairwise distance and orientation modeling achieves the best performance on all categories. Pairwise spatial feature computation. We compute pairwise distances and angles using the coordinates of object centers and design the spatial features as shown in Eq (2). In Table 2, we further  compare the proposed method with two variants to show its effectiveness. The first variant uses the bottom center of objects to compute pairwise vertical angles which ignores the height of objects and could be more accurate to measure vertical relations. The second variant concatenates bounding boxes of two objects and uses a multi-layer perceptron (MLP) to learn pairwise spatial relations. We can see that using the bottom center achieves similar performance as using the object center, which suggests that the object center is sufficient to capture the vertical spatial relations of objects in the textual descriptions. The learnable MLP, however, achieves much worse performance than our approach. This indicates that it is challenging to implicitly learn pairwise spatial relations and the proposed spatial features designed by domain knowledge are beneficial.\n\nMulti-head spatial attention. R6 in Table 1 uses single-head attention for the spatial relevance \u03c9 s ij in (5) but keeps multi-head for the standard attention weight \u03c9 o ij . Compared to the full model R9 with the multi-head spatial attention, such single-head spatial attention achieves significantly worse performance. This suggests that the multi-head is beneficial for learning spatial relations. Attention fusion. We use sigmoid softmax function in (5) to aggregate the spatial attention and the standard selfattention weights. We compare the proposed fusion mechanism with standard relative positional encoding methods [35] in vision transformers: bias mode and contextual mode. The bias mode adds the spatial attention weight as a bias term in the standard self-attention, while the contextual mode further considers the interaction with queries by injecting the pairwise spatial features into the key embeddings. From the results in R7 and R8, we can see that the standard relative positional encoding methods fail to model 3D spatial relations among objects; they even perform worse than the strong baseline (R3), which doesn't rely on relative positional encoding. The proposed sigmoid softmax function is by far more effective to modulate the standard self-attention with spatial relations. Text encoding. We follow recent works [14,16] to use the first three layers of BERT textual encoder, while [11] uses weaker textual encoder (Glove+GRU). To compare these different textual encoders, Table 4 provides results of our proposed models under the setting in Table 1. We can see that more BERT layers do not lead to better performance. The reason might be that the representations from higher BERT layers are less generalizable to different domains. Moreover, more layers are more prone to overfit and harder to optimize. The pretrained BERT model achieves much better performance compared to the GRU model trained from scratch. The teacher model with ground-truth object labels learns well how to perform spatial relation reasoning given the input sentence. Figure 4 provides a qualitative example of the learned cross-and self-attention weights at all multimodal fusion layers in the teacher model. For the cross-attention weights, we can see that the teacher first aligns the object proposal with the correct words in the text, and then gradually shifts its attentions to relation words to perform spatial relation reasoning. For the self-attention weights, in the first layer, the teacher mostly attends to nearby objects to be aware of the context; then in the second layer, it focuses more on the reference object mentioned in the sentence (e.g., the whiteboard); finally, it concentrates on the same-class distractors to distinguish them. Such reasoning steps are promising to ease the training of the student model. In the following, we evaluate the proposed teacher-student training to distill relational knowledge from the teacher to a student with point cloud inputs extracted from the ground-truth object proposals. Table 5 presents the performance of different student models. The first row in the student block does not use any knowledge from the teacher model. In the second row, we use the weights in the teacher model to initialize the student model since the teacher and student share the same architecture except for the input object representations. Such simple weight initialization already facilitates the training of the student, and achieves 4.5% gains. The attention distillation in (6) and hidden state distillation in (7) are both beneficial to improve the student model compared to the first row without knowledge distillation. It is more effective to distill the relation knowledge in the attention weight matrices. The combination of L attn and L hidden is helpful and achieves the best performance. We empirically find that combining weight initialization and the two distillation losses performs similar to using the distillation losses alone. Table 6 compares our ViL3DRel model with state-of-the-art methods on Nr3D and Sr3D datasets. All the compared works use ground-truth object proposals, but no ground-truth labels. Our model achieves significant improvements over the previous best method [16], with 9.3% and 8.3% absolute gains on Nr3D and Sr3D respectively. We outperform [13] by even larger margins on the two datasets. This work also use spatial information, but doesn't use a spatial attention module.  Table 7 and Table 8 present results on ScanRefer dataset with ground-truth object proposals and detected object proposals, respectively. For fair comparison, we use a pre-trained PointGroup [43] model to generate object proposals, which is trained on 18 classes in ScanNet. To be noted, the upper block in Table 8 optimizes the detection stage or utilizes single-stage pipeline. These methods use Table 7: Grounding accuracy (%) on ScanRefer with groundtruth object proposals.\n\n\nTeacher-Student Training\n\n\nComparison with State-of-the-Art Methods\n\n\nOverall\n\nReferIt3D [8] 46.9 Non-SAT [14] 48.2 SAT [14] 53.8\n\nViL3DRel (Ours) 59.8 more data to train object proposals, so the comparison to these methods is not entirely fair. There is a large gap between the ground-truth and detected proposals (over 12% for our method), because the detected proposals might not include the target or reference objects in the sentence. However, our ViL3DRel model still outperforms the state of the art in both settings. Though our proposed spatial relation module can also be plugged into transformer-based 3D object detectors [44] to improve the 3D object proposal generation, we leave it to future work. Figure 1 compares our ViL3DRel model and a baseline without spatial selfattention and teacher-student training. We present more qualitative comparisons in Section C of supplementary material.\n\n\nConclusion\n\nIn this work, we propose a ViL3DRel model for 3D object grounding. It contains a newly designed spatial self-attention module to improve language conditioned spatial relation reasoning in the transformer layer, which explicitly considers relative distances and orientations among objects. A teacher-student training approach is further proposed to transfer relation knowledge from a teacher with ground-truth object labels to a student with point cloud inputs. The proposed model significantly outperforms the state of the art on Nr3D, Sr3D and ScanRefer datasets. Beyond the application of 3D object grounding, the general approach of incorporating priors into a self-attention layer for a transformer to combine multimodal input can be of wide interest. Since our model belongs to the two-stage framework, it is limited by imperfect object proposals in the first detection stage. We also did not explore to explicitly extract object orientations to more accurately estimate pairwise spatial relations as the automatic object poses prediction remains a challenging problem. In addition, the evaluation can suffer from the fact that existing datasets do not represent a rich diversity of environments though we carried out extensive ablations to mitigate the problem. This work has minimal ethical, privacy and safety concerns.\nis arctan2( y b \u2212ya x b \u2212xa ) and the vertical angle \u03b8 v is arcsin( z b \u2212za d ).\nIn other terminology, the horizontal angle corresponds to the azimuth direction while looking from point A to B, while the vertical angle corresponds to the elevation.\n\n\nA.2 Rotation augmentation\n\nWhen performing rotation augmentation, we rotate the whole point cloud by different angles. Specifically, we randomly select one angle among [0, 90, 180, 270] degrees.\n\n\nA.3 Training losses\n\nWe use auxiliary losses L sent and L * obj following previous works [7,9,13,14,16] which have shown to be beneficial for the performance. They are all cross entropy losses. L sent is to predict the target object class from the sentence. L * obj is to predict the object class for each object token. When training the teacher model, all the losses are used. When training the student model, the L u obj does not influence model weights since the pointnet is fixed.\n\n\nB Additional Results\n\n\nB.1 Ablations Studies on ScanRefer Dataset\n\nIn the main paper, we provide ablation studies on the Nr3D dataset. Here, we further evaluate our method on the ScanRefer dataset to demonstrate the effectiveness of the proposed spatial self-attention and teacher-student training.  Table 9 presents the grounding accuracy using the ground-truth object labels on the ScanRefer dataset. By comparing R1-R3, we observe that the color information and rotation augmentation is beneficial to provide a stronger baseline. The proposed spatial selfattention (R9) significantly improves the strong baseline (R3), with the absolute gain of 4.79%. However, different from the Nr3D dataset, the sentences in the ScanRefer dataset are less focused on spatial relations and contain more attribute descriptions such as colors and shapes to refer to target objects. Therefore, the overall performance gain from R3 to R9 is smaller compared to the Nr3D dataset. The rows R4 and R5 analyze the contributions of relative spatial features to the performance. We can see that both relative distances and orientations outperform the baseline in R3, and that the relative distances are more important in the ScanRefer dataset. Their combination achieves the best performance in R9. The multi-head attention is also beneficial comparing R6 and R9. In R7 and R8, we compare the proposed sigmoid softmax function with the other two common relative position encoding methods. Our proposed fusion method is most effective to exploit both the spatial attention and the standard self-attention weights.  Table 10 compares models using ground-truth object proposals on the ScanRefer dataset. The first row in the student model block does not use any knowledge from the teacher model which uses ground-truth object labels as inputs. Initializing from the weights in the teacher only brings a slight improvement as shown in the second row. However, our proposed knowledge distil-lation achieves over 2% boost compared to the baseline in the first row. Both attention weights and hidden states are beneficial to train a better student model with noisy object features. The hidden state distillation slightly outperforms attention distillation which is different from the results on the Nr3D dataset. The reason could be that the spatial relations are mentioned more frequently in the Nr3D dataset while object attributes are more typical in the ScanRefer dataset. We use the same random seed of 0 for all the experiments. In the following, we verify the robustness of the proposed ViL3DRel model by measuring the average and standard derivations of the grounding accuracy under 5 different random seeds. From the results in Table 11, we can see that the proposed model is stable with low deviations across different random seeds. We can see that our model is better at object perception (left example) while reasoning about different types of spatial relations such as relative distances (middle example) and orientations (right example). In Figure 6, we further provide some failure cases of the ViL3DRel model. Example on the left is missing object proposals for the outlet and television. Since our model belongs to the two-stage method, its performance is highly dependent on the quality of object proposals in the first stage. The middle example suggests that the current model still suffers from recognizing fine-grained attributes from the point clouds. The right example further requires reasoning about object orientations, which might be hard to estimate from noisy point clouds.\n\n\nB.2 Robustness to Random Seeds\n\n\nC Qualitative Results\n\nIf you are facing the cabinets, it's the one on the right.\n\n\nWalking in the door, the couch on your RIGHT\n\nThe open door which is closest to the trash bin. Figure 5: Examples on the Nr3D dataset. The first row presents the ground-truth where the green boxes denote the target object and the red boxes denote distractor objects of the same class. The second row shows the predictions (blue boxes) from our proposed ViL3DRel model. The third row shows the predictions (purple boxes) from a baseline model without spatial self-attention and knowledge distillation. chair close to an electrical outlet and under a television the blue circular chair one of the office chair next to two other chairs, facing towards two other chairs in room corner Figure 6: Failure cases of the proposed ViL3DRel model. The first row presents the ground-truth where the green boxes denote the target object and the red boxes denote distractor objects of the same class. The second row shows the predictions (blue boxes) from our proposed ViL3DRel model.\n\n\n(a) The backpack closest to the piano.(b) Of the two brown wooden doors, choose the door on the left when facing them.\n\nFigure 3 :\n3Teacher student learning with hidden state and attention distillation.\n\nFigure 4 :\n4An example of the learned cross-attention (upper) and self-attention (bottom) at different transformer layers in the teacher model. The query is the target object proposal (desk 17). The darker color denotes higher attention weights from the query to the keys.\n\nFigure 5\n5compares predictions by our proposed ViL3DRel model and the baseline model without the spatial self-attention and knowledge distillation.\n\nTable 1 :\n1Grounding accuracy (%) on the Nr3D dataset with ground-truth object labels. Dist stands for Distance; Ort for Orientation; MHA for multi-head spatial attention; RotAug for Rotation Augmentation; sigs for sigsoftmax in Eq (5); and '-' means not applicable.Spatial Relation Reasoning \nRot \nAug \nColor Overall ViewDep ViewIndep \nDist Ort MHA Fusion \n\nR1 \n-\n-\n-\n-\n\u00d7 \n\u00d7 \n53.5 \n51.4 \n54.6 \nR2 \n-\n-\n-\n-\n\u00d7 \n55.1 \n53.8 \n55.8 \nR3 \n-\n-\n-\n-\n62.4 \n58.3 \n64.5 \n\nR4 \n\u00d7 \nsigs \n66.0 \n53.8 \n72.0 \nR5 \n\u00d7 \nsigs \n71.3 \n68.5 \n72.6 \nR6 \n\u00d7 \nsigs \n67.7 \n65.2 \n69.0 \n\nR7 \nbias \n55.4 \n46.8 \n59.6 \nR8 \nctx \n56.4 \n50.8 \n59.1 \n\nR9 \nsigs \n74.4 \n71.3 \n75.9 \n\n\n\nTable 2 :\n2Comparison of pair-\nwise spatial feature computa-\ntion methods. \n\nOverall \n\nobject center \n74.4 \nbottom center \n74.4 \nboxes + MLP \n57.4 \n\n\n\nTable 3 :\n3Performance breakdown of models inTable 1by spatial relation types: Dist(Ort) only which only contains distance(orientation) descriptions; Dist & Ort which contains both distance and orientation descriptions; and the Others which do not contain spatial relation descriptions. Dist Ort Overall Dist only Ort only Dist & Ort Others\u00d7 \n\u00d7 \n62.4 \n63.5 \n61.2 \n57.7 \n63.9 \n\u00d7 \n66.0 \n72.6 \n58.9 \n55.1 \n68.8 \n\u00d7 \n71.3 \n73.8 \n71.7 \n67.7 \n69.1 \n74.4 \n77.8 \n74.0 \n69.1 \n72.6 \n\n\n\nTable 4 :\n4Performance comparison of different textual encoders.Encoder \n#Layers Overall \n\nBERT \n\n3 \n74.4 \n6 \n73.7 \n9 \n73.2 \n12 \n52.3 \n\nGlove+GRU \n3 \n45.7 \n\n\n\nTable 6 :\n6Grounding accuracy (%) on Nr3D and Sr3D datasets with ground-truth object proposals.Method \n\nNr3D \nSr3D \n\nOverall Easy Hard \nView \nDep \n\nView \nIndep \nOverall Easy Hard \nView \nDep \n\nView \nIndep \n\nReferIt3D [8] \n35.6 \n43.6 27.9 32.5 37.1 \n40.8 \n44.7 31.5 39.2 40.8 \nScanRefer [7] \n34.2 \n41.0 23.5 29.9 35.4 \n-\n-\n-\n-\n-\nTGNN [10] \n37.3 \n44.2 30.6 35.8 38.0 \n-\n-\n-\n-\n-\nInstanceRefer [11] \n38.8 \n46.0 31.8 34.5 41.9 \n48.0 \n51.1 40.5 45.4 48.1 \nFFL-3DOG [12] \n41.7 \n48.2 35.0 37.1 44.7 \n-\n-\n-\n-\n-\n3DVG-Trans [13] \n40.8 \n48.5 34.8 34.8 43.7 \n51.4 \n54.2 44.9 44.6 51.7 \nTransRefer3D [9] \n42.1 \n48.5 36.0 36.5 44.9 \n57.4 \n60.5 50.2 49.9 57.7 \nLanguageRefer [15] 43.9 \n51.0 36.6 41.7 45.0 \n56.0 \n58.9 49.3 49.2 56.3 \nSAT [14] \n49.2 \n56.3 42.4 46.9 50.4 \n57.9 \n61.2 50.0 49.2 58.3 \n3D-SPS [23] \n51.5 \n58.1 45.1 48.0 53.2 \n62.6 \n56.2 65.4 49.2 63.2 \nMulti-view [16] \n55.1 \n61.3 49.1 54.3 55.4 \n64.5 \n66.9 58.8 58.4 64.7 \n\nViL3DRel (Ours) \n64.4 \n70.2 57.4 62.0 64.5 \n72.8 \n74.9 67.9 63.8 73.2 \n\n\n\nTable 5 :\n5Grounding accuracy (%) on the Nr3D \ndataset with ground-truth object proposals. \n\ninit. L attn L hidden Overall \n\nTeacher \n-\n74.4 \n\nStudent \n\n\u00d7 \n\u00d7 \n\u00d7 \n58.1 \n\u00d7 \n\u00d7 \n62.6 \n\u00d7 \n\u00d7 \n63.6 \n\u00d7 \n\u00d7 \n62.1 \n\u00d7 \n64.4 \n\n\n\nTable 8 :\n8Grounding accuracy (%) on ScanRefer with detected object proposals. VN and PG denote the VoteNet and PointGroup models pretrained on 18 classes of the ScanNet dataset, while Optimized denotes end-to-end training of the object detector on the ScanRefer dataset.Method \nDet \nUnique \nMultiple \nOverall \nacc@0.25 acc@0.5 acc@0.25 acc@0.5 acc@0.25 acc@0.5 \n\nScanRefer [7] \nOpti-\nmized \n\n65.00 \n43.31 \n30.63 \n19.75 \n37.30 \n24.32 \nTGNN [10] \n68.61 \n56.80 \n29.84 \n23.18 \n37.37 \n29.70 \n3DVG-Trans [13] \n77.16 \n58.47 \n38.38 \n28.70 \n45.90 \n34.47 \n3D-SPS [23] \n81.63 \n64.77 \n39.48 \n29.61 \n47.65 \n36.42 \n\nNon-SAT [14] \nVN [45] \n68.48 \n47.38 \n31.81 \n21.34 \n38.92 \n26.40 \nSAT [14] \nVN [45] \n73.21 \n50.83 \n37.64 \n25.16 \n44.54 \n30.14 \nInstanceRefer [11] PG [43] \n77.45 \n66.83 \n31.27 \n24.77 \n40.23 \n32.93 \nMulti-view [16] \nPG [43] \n77.67 \n66.45 \n31.92 \n25.26 \n40.80 \n33.26 \nViL3DRel (Ours) PG [43] \n81.58 \n68.62 \n40.30 \n30.71 \n47.94 \n37.73 \n\nUpperBound \nPG [43] \n88.63 \n74.47 \n78.82 \n60.37 \n80.64 \n62.98 \n\n\n\nTable 9 :\n9Grounding accuracy (%) on the ScanRefer dataset with groundtruth object labels. Dist stands for Distance; Ort for Orientation; MHA for Multi-Head spatial Attention; RotAug for Rotation Augmentation; sigs for sigsoftmax in Eq (5); and '-' means not applicable.Spatial Relation Reasoning \nRot \nAug \nColor Overall \nDist Ort MHA Fusion \n\nR1 \n-\n-\n-\n-\n\u00d7 \n\u00d7 \n56.35 \nR2 \n-\n-\n-\n-\n\u00d7 \n57.80 \nR3 \n-\n-\n-\n-\n59.10 \n\nR4 \n\u00d7 \nsigs \n63.87 \nR5 \n\u00d7 \nsigs \n60.28 \nR6 \n\u00d7 \nsigs \n63.39 \n\nR7 \nbias \n59.16 \nR8 \nctx \n58.79 \n\nR9 \nsigs \n63.89 \n\n\n\nTable 10 :\n10Grounding accuracy (%) on the ScanRefer dataset with ground-truth object proposals.init. L attn L hidden OverallTeacher \n-\n63.89 \n\nStudent \n\n\u00d7 \n\u00d7 \n\u00d7 \n57.50 \n\u00d7 \n\u00d7 \n57.81 \n\u00d7 \n\u00d7 \n59.67 \n\u00d7 \n\u00d7 \n59.89 \n\u00d7 \n59.80 \n\n\n\nTable 11 :\n11The robustness of the proposed ViL3DRel model with respect to different random seeds (%). seeds 63.8 \u00b1 0.5 72.8 \u00b1 0.2 37.6 \u00b1 0.2Nr3D \nSr3D \nScanRefer \n\nseed=0 \n64.4 \n72.8 \n37.7 \n5 \nAcknowledgments and Disclosure of FundingThis work was granted access to the HPC resources of IDRIS under the allocation 101002 made by GENCI. It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d'avenir\" program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute), the ANR project VideoPredict (ANR-21-FAI1-0002-01) and by Louis Vuitton ENS Chair on Artificial Intelligence.Appendix A Implementation DetailsA.1 Computing horizontal and vertical angles between two objectsFor two objects A and B, assume that the coordinates of the object centerare (x a , y a , z a )and(x b , y b , z b ). We can compute the Euclidean distance between A and B as d. The horizontal angle \u03b8 h\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, ECCV. SpringerLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In ECCV, pages 69-85. Springer, 2016.\n\nGeneration and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, CVPR. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, pages 11-20, 2016.\n\nClevr-ref+: Diagnosing visual reasoning with referring expressions. Runtao Liu, Chenxi Liu, Yutong Bai, Alan L Yuille, CVPR. Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L Yuille. Clevr-ref+: Diagnosing visual reasoning with referring expressions. In CVPR, pages 4185-4194, 2019.\n\nCops-ref: A new dataset and task on compositional referring expression comprehension. Zhenfang Chen, Peng Wang, Lin Ma, K Kwan-Yee, Qi Wong, Wu, CVPR. Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K Wong, and Qi Wu. Cops-ref: A new dataset and task on compositional referring expression comprehension. In CVPR, pages 10086-10095, 2020.\n\nTouchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, CVPR. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In CVPR, pages 12538-12547, 2019.\n\nReverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, CVPR. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In CVPR, pages 9982-9991, 2020.\n\nScanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu Chen, X Angel, Matthias Chang, Nie\u00dfner, ECCV. SpringerDave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, pages 202-221. Springer, 2020.\n\nReferit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, ECCV. SpringerPanos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In ECCV, pages 422-440. Springer, 2020.\n\nTransrefer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding. Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, Si Liu, ACM MM. Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, and Si Liu. Transrefer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding. In ACM MM, pages 2344-2352, 2021.\n\nText-guided graph neural networks for referring 3d instance segmentation. Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, Tyng-Luh Liu, AAAI. 35Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. In AAAI, volume 35, pages 1610-1618, 2021.\n\nInstancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, Shuguang Cui, ICCV. Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, and Shuguang Cui. Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. In ICCV, pages 1791-1800, 2021.\n\nFree-form description guided 3d visual graph network for object grounding in point cloud. Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, Xiangdong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, Ajmal Mian, ICCV. Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Ajmal Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. In ICCV, pages 3722-3731, 2021.\n\n3dvg-transformer: Relation modeling for visual grounding on point clouds. Lichen Zhao, Daigang Cai, Lu Sheng, Dong Xu, ICCV. Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-transformer: Relation modeling for visual grounding on point clouds. In ICCV, pages 2928-2937, 2021.\n\nSat: 2d semantics assisted training for 3d visual grounding. Zhengyuan Yang, Songyang Zhang, Liwei Wang, Jiebo Luo, ICCV. Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. Sat: 2d semantics assisted training for 3d visual grounding. In ICCV, pages 1856-1866, 2021.\n\nLanguagerefer: Spatial-language model for 3d visual grounding. Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox, CoRL. PMLRJunha Roh, Karthik Desingh, Ali Farhadi, and Dieter Fox. Languagerefer: Spatial-language model for 3d visual grounding. In CoRL, pages 1046-1056. PMLR, 2021.\n\nMulti-view transformer for 3d visual grounding. Shijia Huang, Yilun Chen, Jiaya Jia, Liwei Wang, CVPR. 2022Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view transformer for 3d visual grounding. In CVPR, 2022.\n\nAttention is all you need. NeurIPS, 30. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017.\n\nIshan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. Aishwarya Kamath, Mannat Singh, Yann Lecun, Gabriel Synnaeve, ICCV. Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In ICCV, pages 1780-1790, 2021.\n\nReclip: A strong zero-shot baseline for referring expression comprehension. Sanjay Subramanian, Will Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, Anna Rohrbach, ACL. 2022Sanjay Subramanian, Will Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. Reclip: A strong zero-shot baseline for referring expression comprehension. In ACL, 2022.\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, IJCV. 1231Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123(1):32-73, 2017.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. PMLRAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748-8763. PMLR, 2021.\n\n3d-sps: Single-stage 3d visual grounding via referred point progressive selection. Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, Si Liu, CVPR. 2022Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps: Single-stage 3d visual grounding via referred point progressive selection. In CVPR, 2022.\n\nMattnet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLicheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1307-1315, 2018.\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAngela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828-5839, 2017.\n\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, Graph attention networks. ICLR. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ICLR, 2018.\n\nDynamic graph cnn for learning on point clouds. Yue Wang, Yongbin Sun, Ziwei Liu, E Sanjay, Sarma, Justin M Michael M Bronstein, Solomon, TOG. 385Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. TOG, 38(5):1-12, 2019.\n\nLooking outside the box to ground language in 3d scenes. Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, Katerina Fragkiadaki, arXiv:2112.08879arXiv preprintAyush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Looking outside the box to ground language in 3d scenes. arXiv preprint arXiv:2112.08879, 2021.\n\nVideobert: A joint model for video and language representation learning. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid, ICCV. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In ICCV, pages 7464-7473, 2019.\n\nTubedetr: Spatio-temporal video grounding with transformers. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid, 2022Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Tubedetr: Spatio-temporal video grounding with transformers. CVPR, 2022.\n\nMeshed-memory transformer for image captioning. Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara, CVPR. Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-memory transformer for image captioning. In CVPR, pages 10578-10587, 2020.\n\nUnified vision-language pre-training for image captioning and vqa. Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, Jianfeng Gao, AAAI. 34Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In AAAI, volume 34, pages 13041-13049, 2020.\n\nHistory aware multimodal transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev, NeurIPS. 342021Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. NeurIPS, 34, 2021.\n\nThink global, act local: Dual-scale graph transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev, 2022Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Think global, act local: Dual-scale graph transformer for vision-and-language navigation. CVPR, 2022.\n\nRethinking and improving relative position encoding for vision transformer. Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, Hongyang Chao, ICCV. Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improving relative position encoding for vision transformer. In ICCV, pages 10033-10041, 2021.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.025312arXiv preprintGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.\n\nKnowledge distillation: A good teacher is patient and consistent. Lucas Beyer, Xiaohua Zhai, Am\u00e9lie Royer, Larisa Markeeva, Rohan Anil, Alexander Kolesnikov, arXiv:2106.05237arXiv preprintLucas Beyer, Xiaohua Zhai, Am\u00e9lie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. arXiv preprint arXiv:2106.05237, 2021.\n\nTinybert: Distilling bert for natural language understanding. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, EMNLP Findings. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In EMNLP Findings, pages 4163-4174, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, Pre-training of deep bidirectional transformers for language understanding. NAACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019.\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, NeurIPS. 30Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 30, 2017.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.\n\nFixing weight decay regularization in adam. Ilya Loshchilov, Frank Hutter, Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018.\n\nPointgroup: Dual-set point grouping for 3d instance segmentation. Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia, CVPR. Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In CVPR, pages 4867-4876, 2020.\n\nGroup-free 3d object detection via transformers. Ze Liu, Zheng Zhang, Yue Cao, Han Hu, Xin Tong, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZe Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong. Group-free 3d object detection via transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2949-2958, 2021.\n\nDeep hough voting for 3d object detection in point clouds. Or Charles R Qi, Kaiming Litany, Leonidas J He, Guibas, ICCV. Checklist 1. For all authorsCharles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, pages 9277-9286, 2019. Checklist 1. For all authors...\n\nDo the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?. Yes(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\n\nDid you discuss any potential negative societal impacts of your work? [Yes] See the conclusion section. Did you discuss any potential negative societal impacts of your work? [Yes] See the conclusion section.\n\nHave you read the ethics review guidelines and ensured that your paper conforms to them. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n\n(a) Did you state the full set of assumptions of all theoretical results. If you are including theoretical results.... N/A] (b) Did you include complete proofs of all theoretical results? [N/AIf you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]\n\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?. If you ran experiments.... No] We will release the code, data and instructions upon acceptanceIf you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] We will release the code, data and instructions upon acceptance.\n\nDid you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See the supplementary material. Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See the supplementary material.\n\ncode, data, models) or curating/releasing new assets... (a) If your work uses existing assets. If you are using existing assets (e.g.. did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/AIf you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\n\nDid you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?. N/ADid you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n\nDid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation. N/ADid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n", "annotations": {"author": "[{\"end\":149,\"start\":75},{\"end\":231,\"start\":150},{\"end\":266,\"start\":232},{\"end\":345,\"start\":267},{\"end\":420,\"start\":346}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":82},{\"end\":168,\"start\":163},{\"end\":248,\"start\":241},{\"end\":282,\"start\":276},{\"end\":357,\"start\":351}]", "author_first_name": "[{\"end\":81,\"start\":75},{\"end\":162,\"start\":150},{\"end\":240,\"start\":232},{\"end\":275,\"start\":267},{\"end\":350,\"start\":346}]", "author_affiliation": "[{\"end\":148,\"start\":88},{\"end\":230,\"start\":170},{\"end\":265,\"start\":250},{\"end\":344,\"start\":284},{\"end\":419,\"start\":359}]", "title": "[{\"end\":72,\"start\":1},{\"end\":492,\"start\":421}]", "venue": null, "abstract": "[{\"end\":1633,\"start\":494}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1894,\"start\":1891},{\"end\":1896,\"start\":1894},{\"end\":1898,\"start\":1896},{\"end\":1900,\"start\":1898},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1902,\"start\":1900},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1904,\"start\":1902},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1940,\"start\":1937},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1942,\"start\":1940},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2742,\"start\":2739},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2745,\"start\":2742},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2748,\"start\":2745},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2751,\"start\":2748},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3076,\"start\":3073},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3079,\"start\":3076},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3082,\"start\":3079},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3085,\"start\":3082},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3088,\"start\":3085},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3222,\"start\":3218},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3522,\"start\":3518},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3525,\"start\":3522},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3601,\"start\":3598},{\"end\":3603,\"start\":3601},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3606,\"start\":3603},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3609,\"start\":3606},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4096,\"start\":4092},{\"end\":5353,\"start\":5349},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5952,\"start\":5949},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5970,\"start\":5967},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6235,\"start\":6231},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6619,\"start\":6615},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6622,\"start\":6619},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6772,\"start\":6769},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6774,\"start\":6772},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6776,\"start\":6774},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6779,\"start\":6776},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7022,\"start\":7018},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7104,\"start\":7101},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7123,\"start\":7120},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7207,\"start\":7203},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7237,\"start\":7233},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7240,\"start\":7237},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7395,\"start\":7392},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7398,\"start\":7395},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7401,\"start\":7398},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7475,\"start\":7471},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7493,\"start\":7490},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7496,\"start\":7493},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7499,\"start\":7496},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7502,\"start\":7499},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7505,\"start\":7502},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7573,\"start\":7569},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7589,\"start\":7585},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7718,\"start\":7714},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7820,\"start\":7816},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7947,\"start\":7943},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8085,\"start\":8081},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8614,\"start\":8610},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8637,\"start\":8633},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8640,\"start\":8637},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8663,\"start\":8659},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8695,\"start\":8691},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8735,\"start\":8731},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8738,\"start\":8735},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8943,\"start\":8939},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9201,\"start\":9197},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9400,\"start\":9396},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9500,\"start\":9496},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10169,\"start\":10166},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10171,\"start\":10169},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10174,\"start\":10171},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10177,\"start\":10174},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10180,\"start\":10177},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11087,\"start\":11083},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11408,\"start\":11404},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11834,\"start\":11830},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13333,\"start\":13329},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17167,\"start\":17163},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18285,\"start\":18281},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19758,\"start\":19755},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19760,\"start\":19758},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20504,\"start\":20501},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20620,\"start\":20616},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21277,\"start\":21274},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21621,\"start\":21618},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22768,\"start\":22764},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22867,\"start\":22863},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22976,\"start\":22972},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22979,\"start\":22976},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23503,\"start\":23499},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24564,\"start\":24560},{\"end\":25245,\"start\":25242},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28208,\"start\":28205},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28380,\"start\":28376},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29095,\"start\":29091},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29098,\"start\":29095},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29164,\"start\":29160},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31994,\"start\":31990},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32079,\"start\":32075},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32403,\"start\":32399},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32780,\"start\":32777},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32798,\"start\":32794},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32812,\"start\":32808},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33324,\"start\":33320},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35473,\"start\":35470},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35475,\"start\":35473},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35478,\"start\":35475},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35481,\"start\":35478},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35484,\"start\":35481}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40652,\"start\":40532},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40736,\"start\":40653},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41010,\"start\":40737},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41159,\"start\":41011},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41799,\"start\":41160},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41950,\"start\":41800},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42425,\"start\":41951},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42584,\"start\":42426},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43578,\"start\":42585},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43794,\"start\":43579},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44795,\"start\":43795},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45322,\"start\":44796},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45544,\"start\":45323},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":45739,\"start\":45545}]", "paragraph": "[{\"end\":2021,\"start\":1649},{\"end\":2595,\"start\":2023},{\"end\":3361,\"start\":2597},{\"end\":3987,\"start\":3363},{\"end\":4265,\"start\":3989},{\"end\":5354,\"start\":4267},{\"end\":5403,\"start\":5356},{\"end\":6237,\"start\":5405},{\"end\":7208,\"start\":6254},{\"end\":9148,\"start\":7210},{\"end\":9850,\"start\":9150},{\"end\":11725,\"start\":9861},{\"end\":12691,\"start\":11781},{\"end\":13112,\"start\":12960},{\"end\":13737,\"start\":13114},{\"end\":14923,\"start\":13764},{\"end\":15391,\"start\":14982},{\"end\":15775,\"start\":15522},{\"end\":16173,\"start\":15841},{\"end\":16404,\"start\":16254},{\"end\":16954,\"start\":16471},{\"end\":17976,\"start\":16983},{\"end\":18286,\"start\":17978},{\"end\":19252,\"start\":18288},{\"end\":19661,\"start\":19305},{\"end\":20327,\"start\":19716},{\"end\":20461,\"start\":20398},{\"end\":21933,\"start\":20488},{\"end\":23805,\"start\":21958},{\"end\":24074,\"start\":23826},{\"end\":24309,\"start\":24105},{\"end\":27749,\"start\":24311},{\"end\":32685,\"start\":27751},{\"end\":32817,\"start\":32767},{\"end\":33590,\"start\":32819},{\"end\":34932,\"start\":33605},{\"end\":35181,\"start\":35014},{\"end\":35378,\"start\":35211},{\"end\":35865,\"start\":35402},{\"end\":39441,\"start\":35935},{\"end\":39558,\"start\":39500},{\"end\":40531,\"start\":39607}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11780,\"start\":11750},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12959,\"start\":12692},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14981,\"start\":14924},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15521,\"start\":15392},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15840,\"start\":15776},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16208,\"start\":16174},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16253,\"start\":16208},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16470,\"start\":16405},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19304,\"start\":19253},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19715,\"start\":19662},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20397,\"start\":20328},{\"attributes\":{\"id\":\"formula_11\"},\"end\":35013,\"start\":34933}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24271,\"start\":24264},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25186,\"start\":25179},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25993,\"start\":25986},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26055,\"start\":26048},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26868,\"start\":26861},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27794,\"start\":27787},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29258,\"start\":29251},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29327,\"start\":29320},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30796,\"start\":30789},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31744,\"start\":31737},{\"end\":32216,\"start\":32209},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32228,\"start\":32221},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32522,\"start\":32515},{\"end\":32613,\"start\":32606},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":36175,\"start\":36168},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":37468,\"start\":37460},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38584,\"start\":38576}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1647,\"start\":1635},{\"attributes\":{\"n\":\"2\"},\"end\":6252,\"start\":6240},{\"attributes\":{\"n\":\"3\"},\"end\":9859,\"start\":9853},{\"attributes\":{\"n\":\"3.1\"},\"end\":11749,\"start\":11728},{\"attributes\":{\"n\":\"3.2\"},\"end\":13762,\"start\":13740},{\"attributes\":{\"n\":\"3.3\"},\"end\":16981,\"start\":16957},{\"attributes\":{\"n\":\"4\"},\"end\":20475,\"start\":20464},{\"attributes\":{\"n\":\"4.1\"},\"end\":20486,\"start\":20478},{\"attributes\":{\"n\":\"4.2\"},\"end\":21956,\"start\":21936},{\"attributes\":{\"n\":\"4.3\"},\"end\":23824,\"start\":23808},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":24103,\"start\":24077},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":32712,\"start\":32688},{\"attributes\":{\"n\":\"4.4\"},\"end\":32755,\"start\":32715},{\"end\":32765,\"start\":32758},{\"attributes\":{\"n\":\"5\"},\"end\":33603,\"start\":33593},{\"end\":35209,\"start\":35184},{\"end\":35400,\"start\":35381},{\"end\":35888,\"start\":35868},{\"end\":35933,\"start\":35891},{\"end\":39474,\"start\":39444},{\"end\":39498,\"start\":39477},{\"end\":39605,\"start\":39561},{\"end\":40664,\"start\":40654},{\"end\":40748,\"start\":40738},{\"end\":41020,\"start\":41012},{\"end\":41170,\"start\":41161},{\"end\":41810,\"start\":41801},{\"end\":41961,\"start\":41952},{\"end\":42436,\"start\":42427},{\"end\":42595,\"start\":42586},{\"end\":43589,\"start\":43580},{\"end\":43805,\"start\":43796},{\"end\":44806,\"start\":44797},{\"end\":45334,\"start\":45324},{\"end\":45556,\"start\":45546}]", "table": "[{\"end\":41799,\"start\":41427},{\"end\":41950,\"start\":41812},{\"end\":42425,\"start\":42292},{\"end\":42584,\"start\":42491},{\"end\":43578,\"start\":42681},{\"end\":43794,\"start\":43591},{\"end\":44795,\"start\":44067},{\"end\":45322,\"start\":45067},{\"end\":45544,\"start\":45449},{\"end\":45739,\"start\":45687}]", "figure_caption": "[{\"end\":40652,\"start\":40534},{\"end\":40736,\"start\":40666},{\"end\":41010,\"start\":40750},{\"end\":41159,\"start\":41022},{\"end\":41427,\"start\":41172},{\"end\":42292,\"start\":41963},{\"end\":42491,\"start\":42438},{\"end\":42681,\"start\":42597},{\"end\":44067,\"start\":43807},{\"end\":45067,\"start\":44808},{\"end\":45449,\"start\":45337},{\"end\":45687,\"start\":45559}]", "figure_ref": "[{\"end\":2125,\"start\":2117},{\"end\":3732,\"start\":3724},{\"end\":10835,\"start\":10827},{\"end\":13021,\"start\":13013},{\"end\":14088,\"start\":14080},{\"end\":14495,\"start\":14487},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17827,\"start\":17819},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24387,\"start\":24379},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29828,\"start\":29820},{\"end\":33407,\"start\":33399},{\"end\":38902,\"start\":38894},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39664,\"start\":39656},{\"end\":40250,\"start\":40242}]", "bib_author_first_name": "[{\"end\":46544,\"start\":46537},{\"end\":46556,\"start\":46549},{\"end\":46570,\"start\":46566},{\"end\":46586,\"start\":46577},{\"end\":46588,\"start\":46587},{\"end\":46601,\"start\":46595},{\"end\":46603,\"start\":46602},{\"end\":46854,\"start\":46848},{\"end\":46868,\"start\":46860},{\"end\":46885,\"start\":46876},{\"end\":46898,\"start\":46894},{\"end\":46912,\"start\":46908},{\"end\":46914,\"start\":46913},{\"end\":46928,\"start\":46923},{\"end\":47204,\"start\":47198},{\"end\":47216,\"start\":47210},{\"end\":47228,\"start\":47222},{\"end\":47238,\"start\":47234},{\"end\":47240,\"start\":47239},{\"end\":47505,\"start\":47497},{\"end\":47516,\"start\":47512},{\"end\":47526,\"start\":47523},{\"end\":47532,\"start\":47531},{\"end\":47545,\"start\":47543},{\"end\":47843,\"start\":47837},{\"end\":47855,\"start\":47850},{\"end\":47870,\"start\":47862},{\"end\":47882,\"start\":47878},{\"end\":47896,\"start\":47892},{\"end\":48197,\"start\":48190},{\"end\":48204,\"start\":48202},{\"end\":48214,\"start\":48209},{\"end\":48228,\"start\":48225},{\"end\":48242,\"start\":48235},{\"end\":48247,\"start\":48243},{\"end\":48261,\"start\":48254},{\"end\":48273,\"start\":48268},{\"end\":48600,\"start\":48589},{\"end\":48608,\"start\":48607},{\"end\":48624,\"start\":48616},{\"end\":48921,\"start\":48916},{\"end\":48939,\"start\":48934},{\"end\":48956,\"start\":48953},{\"end\":48969,\"start\":48962},{\"end\":48989,\"start\":48981},{\"end\":49327,\"start\":49321},{\"end\":49339,\"start\":49332},{\"end\":49351,\"start\":49346},{\"end\":49364,\"start\":49357},{\"end\":49377,\"start\":49370},{\"end\":49389,\"start\":49385},{\"end\":49399,\"start\":49397},{\"end\":49707,\"start\":49700},{\"end\":49723,\"start\":49715},{\"end\":49740,\"start\":49729},{\"end\":49755,\"start\":49747},{\"end\":50096,\"start\":50090},{\"end\":50105,\"start\":50103},{\"end\":50119,\"start\":50111},{\"end\":50132,\"start\":50126},{\"end\":50145,\"start\":50140},{\"end\":50156,\"start\":50152},{\"end\":50169,\"start\":50161},{\"end\":50538,\"start\":50531},{\"end\":50549,\"start\":50545},{\"end\":50556,\"start\":50554},{\"end\":50566,\"start\":50561},{\"end\":50583,\"start\":50574},{\"end\":50600,\"start\":50591},{\"end\":50609,\"start\":50606},{\"end\":50623,\"start\":50617},{\"end\":50635,\"start\":50630},{\"end\":50966,\"start\":50960},{\"end\":50980,\"start\":50973},{\"end\":50988,\"start\":50986},{\"end\":51000,\"start\":50996},{\"end\":51237,\"start\":51228},{\"end\":51252,\"start\":51244},{\"end\":51265,\"start\":51260},{\"end\":51277,\"start\":51272},{\"end\":51510,\"start\":51505},{\"end\":51523,\"start\":51516},{\"end\":51536,\"start\":51533},{\"end\":51552,\"start\":51546},{\"end\":51781,\"start\":51775},{\"end\":51794,\"start\":51789},{\"end\":51806,\"start\":51801},{\"end\":51817,\"start\":51812},{\"end\":51997,\"start\":51991},{\"end\":52011,\"start\":52007},{\"end\":52025,\"start\":52021},{\"end\":52039,\"start\":52034},{\"end\":52056,\"start\":52051},{\"end\":52069,\"start\":52064},{\"end\":52071,\"start\":52070},{\"end\":52085,\"start\":52079},{\"end\":52099,\"start\":52094},{\"end\":52394,\"start\":52385},{\"end\":52409,\"start\":52403},{\"end\":52421,\"start\":52417},{\"end\":52436,\"start\":52429},{\"end\":52731,\"start\":52725},{\"end\":52749,\"start\":52745},{\"end\":52765,\"start\":52759},{\"end\":52779,\"start\":52775},{\"end\":52795,\"start\":52789},{\"end\":52807,\"start\":52803},{\"end\":53111,\"start\":53105},{\"end\":53125,\"start\":53121},{\"end\":53137,\"start\":53131},{\"end\":53151,\"start\":53145},{\"end\":53166,\"start\":53161},{\"end\":53179,\"start\":53173},{\"end\":53198,\"start\":53189},{\"end\":53211,\"start\":53205},{\"end\":53230,\"start\":53224},{\"end\":53240,\"start\":53235},{\"end\":53242,\"start\":53241},{\"end\":53606,\"start\":53602},{\"end\":53620,\"start\":53616},{\"end\":53625,\"start\":53621},{\"end\":53636,\"start\":53631},{\"end\":53652,\"start\":53646},{\"end\":53668,\"start\":53661},{\"end\":53682,\"start\":53674},{\"end\":53698,\"start\":53692},{\"end\":53713,\"start\":53707},{\"end\":53728,\"start\":53722},{\"end\":53742,\"start\":53738},{\"end\":54113,\"start\":54108},{\"end\":54125,\"start\":54119},{\"end\":54138,\"start\":54130},{\"end\":54149,\"start\":54145},{\"end\":54162,\"start\":54155},{\"end\":54171,\"start\":54168},{\"end\":54184,\"start\":54178},{\"end\":54192,\"start\":54190},{\"end\":54483,\"start\":54476},{\"end\":54491,\"start\":54488},{\"end\":54504,\"start\":54497},{\"end\":54516,\"start\":54511},{\"end\":54526,\"start\":54523},{\"end\":54536,\"start\":54531},{\"end\":54551,\"start\":54545},{\"end\":54553,\"start\":54552},{\"end\":55039,\"start\":55033},{\"end\":55046,\"start\":55045},{\"end\":55061,\"start\":55054},{\"end\":55075,\"start\":55069},{\"end\":55089,\"start\":55083},{\"end\":55106,\"start\":55098},{\"end\":55541,\"start\":55536},{\"end\":55561,\"start\":55554},{\"end\":55579,\"start\":55572},{\"end\":55597,\"start\":55590},{\"end\":55612,\"start\":55606},{\"end\":55624,\"start\":55618},{\"end\":55856,\"start\":55853},{\"end\":55870,\"start\":55863},{\"end\":55881,\"start\":55876},{\"end\":55888,\"start\":55887},{\"end\":55910,\"start\":55904},{\"end\":55912,\"start\":55911},{\"end\":56178,\"start\":56173},{\"end\":56193,\"start\":56185},{\"end\":56212,\"start\":56206},{\"end\":56232,\"start\":56224},{\"end\":56527,\"start\":56523},{\"end\":56539,\"start\":56533},{\"end\":56551,\"start\":56547},{\"end\":56567,\"start\":56562},{\"end\":56584,\"start\":56576},{\"end\":56847,\"start\":56840},{\"end\":56861,\"start\":56854},{\"end\":56874,\"start\":56869},{\"end\":56886,\"start\":56882},{\"end\":56903,\"start\":56895},{\"end\":57122,\"start\":57114},{\"end\":57137,\"start\":57131},{\"end\":57156,\"start\":57149},{\"end\":57170,\"start\":57166},{\"end\":57416,\"start\":57410},{\"end\":57428,\"start\":57423},{\"end\":57441,\"start\":57438},{\"end\":57456,\"start\":57449},{\"end\":57466,\"start\":57461},{\"end\":57482,\"start\":57474},{\"end\":57770,\"start\":57764},{\"end\":57789,\"start\":57777},{\"end\":57805,\"start\":57797},{\"end\":57818,\"start\":57814},{\"end\":58098,\"start\":58092},{\"end\":58117,\"start\":58105},{\"end\":58133,\"start\":58125},{\"end\":58151,\"start\":58143},{\"end\":58164,\"start\":58160},{\"end\":58444,\"start\":58441},{\"end\":58455,\"start\":58449},{\"end\":58469,\"start\":58462},{\"end\":58484,\"start\":58476},{\"end\":58497,\"start\":58489},{\"end\":58742,\"start\":58734},{\"end\":58756,\"start\":58751},{\"end\":58770,\"start\":58766},{\"end\":59021,\"start\":59016},{\"end\":59036,\"start\":59029},{\"end\":59049,\"start\":59043},{\"end\":59063,\"start\":59057},{\"end\":59079,\"start\":59074},{\"end\":59095,\"start\":59086},{\"end\":59408,\"start\":59402},{\"end\":59421,\"start\":59415},{\"end\":59433,\"start\":59427},{\"end\":59444,\"start\":59441},{\"end\":59456,\"start\":59452},{\"end\":59469,\"start\":59463},{\"end\":59478,\"start\":59474},{\"end\":59488,\"start\":59485},{\"end\":59716,\"start\":59711},{\"end\":59733,\"start\":59725},{\"end\":59747,\"start\":59741},{\"end\":59761,\"start\":59753},{\"end\":59771,\"start\":59762},{\"end\":60105,\"start\":60103},{\"end\":60133,\"start\":60130},{\"end\":60146,\"start\":60138},{\"end\":60148,\"start\":60147},{\"end\":60388,\"start\":60381},{\"end\":60408,\"start\":60401},{\"end\":60430,\"start\":60417},{\"end\":60896,\"start\":60892},{\"end\":60914,\"start\":60909},{\"end\":61076,\"start\":61074},{\"end\":61094,\"start\":61084},{\"end\":61110,\"start\":61101},{\"end\":61119,\"start\":61116},{\"end\":61133,\"start\":61125},{\"end\":61143,\"start\":61138},{\"end\":61384,\"start\":61382},{\"end\":61395,\"start\":61390},{\"end\":61406,\"start\":61403},{\"end\":61415,\"start\":61412},{\"end\":61423,\"start\":61420},{\"end\":61821,\"start\":61819},{\"end\":61843,\"start\":61836},{\"end\":61860,\"start\":61852},{\"end\":61862,\"start\":61861}]", "bib_author_last_name": "[{\"end\":46547,\"start\":46545},{\"end\":46564,\"start\":46557},{\"end\":46575,\"start\":46571},{\"end\":46593,\"start\":46589},{\"end\":46608,\"start\":46604},{\"end\":46858,\"start\":46855},{\"end\":46874,\"start\":46869},{\"end\":46892,\"start\":46886},{\"end\":46906,\"start\":46899},{\"end\":46921,\"start\":46915},{\"end\":46935,\"start\":46929},{\"end\":47208,\"start\":47205},{\"end\":47220,\"start\":47217},{\"end\":47232,\"start\":47229},{\"end\":47247,\"start\":47241},{\"end\":47510,\"start\":47506},{\"end\":47521,\"start\":47517},{\"end\":47529,\"start\":47527},{\"end\":47541,\"start\":47533},{\"end\":47550,\"start\":47546},{\"end\":47554,\"start\":47552},{\"end\":47848,\"start\":47844},{\"end\":47860,\"start\":47856},{\"end\":47876,\"start\":47871},{\"end\":47890,\"start\":47883},{\"end\":47902,\"start\":47897},{\"end\":48200,\"start\":48198},{\"end\":48207,\"start\":48205},{\"end\":48223,\"start\":48215},{\"end\":48233,\"start\":48229},{\"end\":48252,\"start\":48248},{\"end\":48266,\"start\":48262},{\"end\":48281,\"start\":48274},{\"end\":48289,\"start\":48283},{\"end\":48605,\"start\":48601},{\"end\":48614,\"start\":48609},{\"end\":48630,\"start\":48625},{\"end\":48639,\"start\":48632},{\"end\":48932,\"start\":48922},{\"end\":48951,\"start\":48940},{\"end\":48960,\"start\":48957},{\"end\":48979,\"start\":48970},{\"end\":48996,\"start\":48990},{\"end\":49330,\"start\":49328},{\"end\":49344,\"start\":49340},{\"end\":49355,\"start\":49352},{\"end\":49368,\"start\":49365},{\"end\":49383,\"start\":49378},{\"end\":49395,\"start\":49390},{\"end\":49403,\"start\":49400},{\"end\":49713,\"start\":49708},{\"end\":49727,\"start\":49724},{\"end\":49745,\"start\":49741},{\"end\":49759,\"start\":49756},{\"end\":50101,\"start\":50097},{\"end\":50109,\"start\":50106},{\"end\":50124,\"start\":50120},{\"end\":50138,\"start\":50133},{\"end\":50150,\"start\":50146},{\"end\":50159,\"start\":50157},{\"end\":50173,\"start\":50170},{\"end\":50543,\"start\":50539},{\"end\":50552,\"start\":50550},{\"end\":50559,\"start\":50557},{\"end\":50572,\"start\":50567},{\"end\":50589,\"start\":50584},{\"end\":50604,\"start\":50601},{\"end\":50615,\"start\":50610},{\"end\":50628,\"start\":50624},{\"end\":50640,\"start\":50636},{\"end\":50971,\"start\":50967},{\"end\":50984,\"start\":50981},{\"end\":50994,\"start\":50989},{\"end\":51003,\"start\":51001},{\"end\":51242,\"start\":51238},{\"end\":51258,\"start\":51253},{\"end\":51270,\"start\":51266},{\"end\":51281,\"start\":51278},{\"end\":51514,\"start\":51511},{\"end\":51531,\"start\":51524},{\"end\":51544,\"start\":51537},{\"end\":51556,\"start\":51553},{\"end\":51787,\"start\":51782},{\"end\":51799,\"start\":51795},{\"end\":51810,\"start\":51807},{\"end\":51822,\"start\":51818},{\"end\":52005,\"start\":51998},{\"end\":52019,\"start\":52012},{\"end\":52032,\"start\":52026},{\"end\":52049,\"start\":52040},{\"end\":52062,\"start\":52057},{\"end\":52077,\"start\":52072},{\"end\":52092,\"start\":52086},{\"end\":52110,\"start\":52100},{\"end\":52401,\"start\":52395},{\"end\":52415,\"start\":52410},{\"end\":52427,\"start\":52422},{\"end\":52445,\"start\":52437},{\"end\":52743,\"start\":52732},{\"end\":52757,\"start\":52750},{\"end\":52773,\"start\":52766},{\"end\":52787,\"start\":52780},{\"end\":52801,\"start\":52796},{\"end\":52816,\"start\":52808},{\"end\":53119,\"start\":53112},{\"end\":53129,\"start\":53126},{\"end\":53143,\"start\":53138},{\"end\":53159,\"start\":53152},{\"end\":53171,\"start\":53167},{\"end\":53187,\"start\":53180},{\"end\":53203,\"start\":53199},{\"end\":53222,\"start\":53212},{\"end\":53233,\"start\":53231},{\"end\":53249,\"start\":53243},{\"end\":53614,\"start\":53607},{\"end\":53629,\"start\":53626},{\"end\":53644,\"start\":53637},{\"end\":53659,\"start\":53653},{\"end\":53672,\"start\":53669},{\"end\":53690,\"start\":53683},{\"end\":53705,\"start\":53699},{\"end\":53720,\"start\":53714},{\"end\":53736,\"start\":53729},{\"end\":53748,\"start\":53743},{\"end\":54117,\"start\":54114},{\"end\":54128,\"start\":54126},{\"end\":54143,\"start\":54139},{\"end\":54153,\"start\":54150},{\"end\":54166,\"start\":54163},{\"end\":54176,\"start\":54172},{\"end\":54188,\"start\":54185},{\"end\":54196,\"start\":54193},{\"end\":54486,\"start\":54484},{\"end\":54495,\"start\":54492},{\"end\":54509,\"start\":54505},{\"end\":54521,\"start\":54517},{\"end\":54529,\"start\":54527},{\"end\":54543,\"start\":54537},{\"end\":54558,\"start\":54554},{\"end\":55043,\"start\":55040},{\"end\":55052,\"start\":55047},{\"end\":55067,\"start\":55062},{\"end\":55081,\"start\":55076},{\"end\":55096,\"start\":55090},{\"end\":55117,\"start\":55107},{\"end\":55126,\"start\":55119},{\"end\":55552,\"start\":55542},{\"end\":55570,\"start\":55562},{\"end\":55588,\"start\":55580},{\"end\":55604,\"start\":55598},{\"end\":55616,\"start\":55613},{\"end\":55631,\"start\":55625},{\"end\":55861,\"start\":55857},{\"end\":55874,\"start\":55871},{\"end\":55885,\"start\":55882},{\"end\":55895,\"start\":55889},{\"end\":55902,\"start\":55897},{\"end\":55932,\"start\":55913},{\"end\":55941,\"start\":55934},{\"end\":56183,\"start\":56179},{\"end\":56204,\"start\":56194},{\"end\":56222,\"start\":56213},{\"end\":56244,\"start\":56233},{\"end\":56531,\"start\":56528},{\"end\":56545,\"start\":56540},{\"end\":56560,\"start\":56552},{\"end\":56574,\"start\":56568},{\"end\":56591,\"start\":56585},{\"end\":56852,\"start\":56848},{\"end\":56867,\"start\":56862},{\"end\":56880,\"start\":56875},{\"end\":56893,\"start\":56887},{\"end\":56910,\"start\":56904},{\"end\":57129,\"start\":57123},{\"end\":57147,\"start\":57138},{\"end\":57164,\"start\":57157},{\"end\":57180,\"start\":57171},{\"end\":57421,\"start\":57417},{\"end\":57436,\"start\":57429},{\"end\":57447,\"start\":57442},{\"end\":57459,\"start\":57457},{\"end\":57472,\"start\":57467},{\"end\":57486,\"start\":57483},{\"end\":57775,\"start\":57771},{\"end\":57795,\"start\":57790},{\"end\":57812,\"start\":57806},{\"end\":57825,\"start\":57819},{\"end\":58103,\"start\":58099},{\"end\":58123,\"start\":58118},{\"end\":58141,\"start\":58134},{\"end\":58158,\"start\":58152},{\"end\":58171,\"start\":58165},{\"end\":58447,\"start\":58445},{\"end\":58460,\"start\":58456},{\"end\":58474,\"start\":58470},{\"end\":58487,\"start\":58485},{\"end\":58502,\"start\":58498},{\"end\":58749,\"start\":58743},{\"end\":58764,\"start\":58757},{\"end\":58775,\"start\":58771},{\"end\":59027,\"start\":59022},{\"end\":59041,\"start\":59037},{\"end\":59055,\"start\":59050},{\"end\":59072,\"start\":59064},{\"end\":59084,\"start\":59080},{\"end\":59106,\"start\":59096},{\"end\":59413,\"start\":59409},{\"end\":59425,\"start\":59422},{\"end\":59439,\"start\":59434},{\"end\":59450,\"start\":59445},{\"end\":59461,\"start\":59457},{\"end\":59472,\"start\":59470},{\"end\":59483,\"start\":59479},{\"end\":59492,\"start\":59489},{\"end\":59723,\"start\":59717},{\"end\":59739,\"start\":59734},{\"end\":59751,\"start\":59748},{\"end\":59776,\"start\":59772},{\"end\":60128,\"start\":60106},{\"end\":60136,\"start\":60134},{\"end\":60151,\"start\":60149},{\"end\":60159,\"start\":60153},{\"end\":60399,\"start\":60389},{\"end\":60415,\"start\":60409},{\"end\":60438,\"start\":60431},{\"end\":60907,\"start\":60897},{\"end\":60921,\"start\":60915},{\"end\":61082,\"start\":61077},{\"end\":61099,\"start\":61095},{\"end\":61114,\"start\":61111},{\"end\":61123,\"start\":61120},{\"end\":61136,\"start\":61134},{\"end\":61147,\"start\":61144},{\"end\":61388,\"start\":61385},{\"end\":61401,\"start\":61396},{\"end\":61410,\"start\":61407},{\"end\":61418,\"start\":61416},{\"end\":61428,\"start\":61424},{\"end\":61834,\"start\":61822},{\"end\":61850,\"start\":61844},{\"end\":61865,\"start\":61863},{\"end\":61873,\"start\":61867}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1688357},\"end\":46781,\"start\":46494},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8745888},\"end\":47128,\"start\":46783},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":57375765},\"end\":47409,\"start\":47130},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":211677354},\"end\":47743,\"start\":47411},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":54078068},\"end\":48106,\"start\":47745},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":214264259},\"end\":48514,\"start\":48108},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":209414687},\"end\":48822,\"start\":48516},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":221378802},\"end\":49229,\"start\":48824},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":236924276},\"end\":49624,\"start\":49231},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235306096},\"end\":49950,\"start\":49626},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":232092539},\"end\":50439,\"start\":49952},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":232417286},\"end\":50884,\"start\":50441},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":244127479},\"end\":51165,\"start\":50886},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":235166799},\"end\":51440,\"start\":51167},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235765540},\"end\":51725,\"start\":51442},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":247957775},\"end\":51949,\"start\":51727},{\"attributes\":{\"id\":\"b16\"},\"end\":52282,\"start\":51951},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233393962},\"end\":52647,\"start\":52284},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":248118561},\"end\":53013,\"start\":52649},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4492210},\"end\":53529,\"start\":53015},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":231591445},\"end\":54023,\"start\":53531},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":248157380},\"end\":54399,\"start\":54025},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3441497},\"end\":54968,\"start\":54401},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7684883},\"end\":55534,\"start\":54970},{\"attributes\":{\"id\":\"b24\"},\"end\":55803,\"start\":55536},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":94822},\"end\":56114,\"start\":55805},{\"attributes\":{\"doi\":\"arXiv:2112.08879\",\"id\":\"b26\"},\"end\":56448,\"start\":56116},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":102483628},\"end\":56777,\"start\":56450},{\"attributes\":{\"id\":\"b28\"},\"end\":57064,\"start\":56779},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219635470},\"end\":57341,\"start\":57066},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202734445},\"end\":57689,\"start\":57343},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":239885388},\"end\":58000,\"start\":57691},{\"attributes\":{\"id\":\"b32\"},\"end\":58363,\"start\":58002},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":236493453},\"end\":58686,\"start\":58365},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b34\"},\"end\":58948,\"start\":58688},{\"attributes\":{\"doi\":\"arXiv:2106.05237\",\"id\":\"b35\"},\"end\":59338,\"start\":58950},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":202719327},\"end\":59709,\"start\":59340},{\"attributes\":{\"id\":\"b37\"},\"end\":60021,\"start\":59711},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1745976},\"end\":60332,\"start\":60023},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1957433},\"end\":60846,\"start\":60334},{\"attributes\":{\"id\":\"b40\"},\"end\":61006,\"start\":60848},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":214795000},\"end\":61331,\"start\":61008},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":232478413},\"end\":61758,\"start\":61333},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":127956465},\"end\":62091,\"start\":61760},{\"attributes\":{\"id\":\"b44\"},\"end\":62332,\"start\":62093},{\"attributes\":{\"id\":\"b45\"},\"end\":62541,\"start\":62334},{\"attributes\":{\"id\":\"b46\"},\"end\":62726,\"start\":62543},{\"attributes\":{\"id\":\"b47\"},\"end\":63113,\"start\":62728},{\"attributes\":{\"id\":\"b48\"},\"end\":63615,\"start\":63115},{\"attributes\":{\"id\":\"b49\"},\"end\":63910,\"start\":63617},{\"attributes\":{\"id\":\"b50\"},\"end\":64747,\"start\":63912},{\"attributes\":{\"id\":\"b51\"},\"end\":65004,\"start\":64749},{\"attributes\":{\"id\":\"b52\"},\"end\":65252,\"start\":65006}]", "bib_title": "[{\"end\":46535,\"start\":46494},{\"end\":46846,\"start\":46783},{\"end\":47196,\"start\":47130},{\"end\":47495,\"start\":47411},{\"end\":47835,\"start\":47745},{\"end\":48188,\"start\":48108},{\"end\":48587,\"start\":48516},{\"end\":48914,\"start\":48824},{\"end\":49319,\"start\":49231},{\"end\":49698,\"start\":49626},{\"end\":50088,\"start\":49952},{\"end\":50529,\"start\":50441},{\"end\":50958,\"start\":50886},{\"end\":51226,\"start\":51167},{\"end\":51503,\"start\":51442},{\"end\":51773,\"start\":51727},{\"end\":52383,\"start\":52284},{\"end\":52723,\"start\":52649},{\"end\":53103,\"start\":53015},{\"end\":53600,\"start\":53531},{\"end\":54106,\"start\":54025},{\"end\":54474,\"start\":54401},{\"end\":55031,\"start\":54970},{\"end\":55851,\"start\":55805},{\"end\":56521,\"start\":56450},{\"end\":57112,\"start\":57066},{\"end\":57408,\"start\":57343},{\"end\":57762,\"start\":57691},{\"end\":58439,\"start\":58365},{\"end\":59400,\"start\":59340},{\"end\":60101,\"start\":60023},{\"end\":60379,\"start\":60334},{\"end\":61072,\"start\":61008},{\"end\":61380,\"start\":61333},{\"end\":61817,\"start\":61760}]", "bib_author": "[{\"end\":46549,\"start\":46537},{\"end\":46566,\"start\":46549},{\"end\":46577,\"start\":46566},{\"end\":46595,\"start\":46577},{\"end\":46610,\"start\":46595},{\"end\":46860,\"start\":46848},{\"end\":46876,\"start\":46860},{\"end\":46894,\"start\":46876},{\"end\":46908,\"start\":46894},{\"end\":46923,\"start\":46908},{\"end\":46937,\"start\":46923},{\"end\":47210,\"start\":47198},{\"end\":47222,\"start\":47210},{\"end\":47234,\"start\":47222},{\"end\":47249,\"start\":47234},{\"end\":47512,\"start\":47497},{\"end\":47523,\"start\":47512},{\"end\":47531,\"start\":47523},{\"end\":47543,\"start\":47531},{\"end\":47552,\"start\":47543},{\"end\":47556,\"start\":47552},{\"end\":47850,\"start\":47837},{\"end\":47862,\"start\":47850},{\"end\":47878,\"start\":47862},{\"end\":47892,\"start\":47878},{\"end\":47904,\"start\":47892},{\"end\":48202,\"start\":48190},{\"end\":48209,\"start\":48202},{\"end\":48225,\"start\":48209},{\"end\":48235,\"start\":48225},{\"end\":48254,\"start\":48235},{\"end\":48268,\"start\":48254},{\"end\":48283,\"start\":48268},{\"end\":48291,\"start\":48283},{\"end\":48607,\"start\":48589},{\"end\":48616,\"start\":48607},{\"end\":48632,\"start\":48616},{\"end\":48641,\"start\":48632},{\"end\":48934,\"start\":48916},{\"end\":48953,\"start\":48934},{\"end\":48962,\"start\":48953},{\"end\":48981,\"start\":48962},{\"end\":48998,\"start\":48981},{\"end\":49332,\"start\":49321},{\"end\":49346,\"start\":49332},{\"end\":49357,\"start\":49346},{\"end\":49370,\"start\":49357},{\"end\":49385,\"start\":49370},{\"end\":49397,\"start\":49385},{\"end\":49405,\"start\":49397},{\"end\":49715,\"start\":49700},{\"end\":49729,\"start\":49715},{\"end\":49747,\"start\":49729},{\"end\":49761,\"start\":49747},{\"end\":50103,\"start\":50090},{\"end\":50111,\"start\":50103},{\"end\":50126,\"start\":50111},{\"end\":50140,\"start\":50126},{\"end\":50152,\"start\":50140},{\"end\":50161,\"start\":50152},{\"end\":50175,\"start\":50161},{\"end\":50545,\"start\":50531},{\"end\":50554,\"start\":50545},{\"end\":50561,\"start\":50554},{\"end\":50574,\"start\":50561},{\"end\":50591,\"start\":50574},{\"end\":50606,\"start\":50591},{\"end\":50617,\"start\":50606},{\"end\":50630,\"start\":50617},{\"end\":50642,\"start\":50630},{\"end\":50973,\"start\":50960},{\"end\":50986,\"start\":50973},{\"end\":50996,\"start\":50986},{\"end\":51005,\"start\":50996},{\"end\":51244,\"start\":51228},{\"end\":51260,\"start\":51244},{\"end\":51272,\"start\":51260},{\"end\":51283,\"start\":51272},{\"end\":51516,\"start\":51505},{\"end\":51533,\"start\":51516},{\"end\":51546,\"start\":51533},{\"end\":51558,\"start\":51546},{\"end\":51789,\"start\":51775},{\"end\":51801,\"start\":51789},{\"end\":51812,\"start\":51801},{\"end\":51824,\"start\":51812},{\"end\":52007,\"start\":51991},{\"end\":52021,\"start\":52007},{\"end\":52034,\"start\":52021},{\"end\":52051,\"start\":52034},{\"end\":52064,\"start\":52051},{\"end\":52079,\"start\":52064},{\"end\":52094,\"start\":52079},{\"end\":52112,\"start\":52094},{\"end\":52403,\"start\":52385},{\"end\":52417,\"start\":52403},{\"end\":52429,\"start\":52417},{\"end\":52447,\"start\":52429},{\"end\":52745,\"start\":52725},{\"end\":52759,\"start\":52745},{\"end\":52775,\"start\":52759},{\"end\":52789,\"start\":52775},{\"end\":52803,\"start\":52789},{\"end\":52818,\"start\":52803},{\"end\":53121,\"start\":53105},{\"end\":53131,\"start\":53121},{\"end\":53145,\"start\":53131},{\"end\":53161,\"start\":53145},{\"end\":53173,\"start\":53161},{\"end\":53189,\"start\":53173},{\"end\":53205,\"start\":53189},{\"end\":53224,\"start\":53205},{\"end\":53235,\"start\":53224},{\"end\":53251,\"start\":53235},{\"end\":53616,\"start\":53602},{\"end\":53631,\"start\":53616},{\"end\":53646,\"start\":53631},{\"end\":53661,\"start\":53646},{\"end\":53674,\"start\":53661},{\"end\":53692,\"start\":53674},{\"end\":53707,\"start\":53692},{\"end\":53722,\"start\":53707},{\"end\":53738,\"start\":53722},{\"end\":53750,\"start\":53738},{\"end\":54119,\"start\":54108},{\"end\":54130,\"start\":54119},{\"end\":54145,\"start\":54130},{\"end\":54155,\"start\":54145},{\"end\":54168,\"start\":54155},{\"end\":54178,\"start\":54168},{\"end\":54190,\"start\":54178},{\"end\":54198,\"start\":54190},{\"end\":54488,\"start\":54476},{\"end\":54497,\"start\":54488},{\"end\":54511,\"start\":54497},{\"end\":54523,\"start\":54511},{\"end\":54531,\"start\":54523},{\"end\":54545,\"start\":54531},{\"end\":54560,\"start\":54545},{\"end\":55045,\"start\":55033},{\"end\":55054,\"start\":55045},{\"end\":55069,\"start\":55054},{\"end\":55083,\"start\":55069},{\"end\":55098,\"start\":55083},{\"end\":55119,\"start\":55098},{\"end\":55128,\"start\":55119},{\"end\":55554,\"start\":55536},{\"end\":55572,\"start\":55554},{\"end\":55590,\"start\":55572},{\"end\":55606,\"start\":55590},{\"end\":55618,\"start\":55606},{\"end\":55633,\"start\":55618},{\"end\":55863,\"start\":55853},{\"end\":55876,\"start\":55863},{\"end\":55887,\"start\":55876},{\"end\":55897,\"start\":55887},{\"end\":55904,\"start\":55897},{\"end\":55934,\"start\":55904},{\"end\":55943,\"start\":55934},{\"end\":56185,\"start\":56173},{\"end\":56206,\"start\":56185},{\"end\":56224,\"start\":56206},{\"end\":56246,\"start\":56224},{\"end\":56533,\"start\":56523},{\"end\":56547,\"start\":56533},{\"end\":56562,\"start\":56547},{\"end\":56576,\"start\":56562},{\"end\":56593,\"start\":56576},{\"end\":56854,\"start\":56840},{\"end\":56869,\"start\":56854},{\"end\":56882,\"start\":56869},{\"end\":56895,\"start\":56882},{\"end\":56912,\"start\":56895},{\"end\":57131,\"start\":57114},{\"end\":57149,\"start\":57131},{\"end\":57166,\"start\":57149},{\"end\":57182,\"start\":57166},{\"end\":57423,\"start\":57410},{\"end\":57438,\"start\":57423},{\"end\":57449,\"start\":57438},{\"end\":57461,\"start\":57449},{\"end\":57474,\"start\":57461},{\"end\":57488,\"start\":57474},{\"end\":57777,\"start\":57764},{\"end\":57797,\"start\":57777},{\"end\":57814,\"start\":57797},{\"end\":57827,\"start\":57814},{\"end\":58105,\"start\":58092},{\"end\":58125,\"start\":58105},{\"end\":58143,\"start\":58125},{\"end\":58160,\"start\":58143},{\"end\":58173,\"start\":58160},{\"end\":58449,\"start\":58441},{\"end\":58462,\"start\":58449},{\"end\":58476,\"start\":58462},{\"end\":58489,\"start\":58476},{\"end\":58504,\"start\":58489},{\"end\":58751,\"start\":58734},{\"end\":58766,\"start\":58751},{\"end\":58777,\"start\":58766},{\"end\":59029,\"start\":59016},{\"end\":59043,\"start\":59029},{\"end\":59057,\"start\":59043},{\"end\":59074,\"start\":59057},{\"end\":59086,\"start\":59074},{\"end\":59108,\"start\":59086},{\"end\":59415,\"start\":59402},{\"end\":59427,\"start\":59415},{\"end\":59441,\"start\":59427},{\"end\":59452,\"start\":59441},{\"end\":59463,\"start\":59452},{\"end\":59474,\"start\":59463},{\"end\":59485,\"start\":59474},{\"end\":59494,\"start\":59485},{\"end\":59725,\"start\":59711},{\"end\":59741,\"start\":59725},{\"end\":59753,\"start\":59741},{\"end\":59778,\"start\":59753},{\"end\":60130,\"start\":60103},{\"end\":60138,\"start\":60130},{\"end\":60153,\"start\":60138},{\"end\":60161,\"start\":60153},{\"end\":60401,\"start\":60381},{\"end\":60417,\"start\":60401},{\"end\":60440,\"start\":60417},{\"end\":60909,\"start\":60892},{\"end\":60923,\"start\":60909},{\"end\":61084,\"start\":61074},{\"end\":61101,\"start\":61084},{\"end\":61116,\"start\":61101},{\"end\":61125,\"start\":61116},{\"end\":61138,\"start\":61125},{\"end\":61149,\"start\":61138},{\"end\":61390,\"start\":61382},{\"end\":61403,\"start\":61390},{\"end\":61412,\"start\":61403},{\"end\":61420,\"start\":61412},{\"end\":61430,\"start\":61420},{\"end\":61836,\"start\":61819},{\"end\":61852,\"start\":61836},{\"end\":61867,\"start\":61852},{\"end\":61875,\"start\":61867}]", "bib_venue": "[{\"end\":46614,\"start\":46610},{\"end\":46941,\"start\":46937},{\"end\":47253,\"start\":47249},{\"end\":47560,\"start\":47556},{\"end\":47908,\"start\":47904},{\"end\":48295,\"start\":48291},{\"end\":48645,\"start\":48641},{\"end\":49002,\"start\":48998},{\"end\":49411,\"start\":49405},{\"end\":49765,\"start\":49761},{\"end\":50179,\"start\":50175},{\"end\":50646,\"start\":50642},{\"end\":51009,\"start\":51005},{\"end\":51287,\"start\":51283},{\"end\":51562,\"start\":51558},{\"end\":51828,\"start\":51824},{\"end\":51989,\"start\":51951},{\"end\":52451,\"start\":52447},{\"end\":52821,\"start\":52818},{\"end\":53255,\"start\":53251},{\"end\":53754,\"start\":53750},{\"end\":54202,\"start\":54198},{\"end\":54637,\"start\":54560},{\"end\":55205,\"start\":55128},{\"end\":55663,\"start\":55633},{\"end\":55946,\"start\":55943},{\"end\":56171,\"start\":56116},{\"end\":56597,\"start\":56593},{\"end\":56838,\"start\":56779},{\"end\":57186,\"start\":57182},{\"end\":57492,\"start\":57488},{\"end\":57834,\"start\":57827},{\"end\":58090,\"start\":58002},{\"end\":58508,\"start\":58504},{\"end\":58732,\"start\":58688},{\"end\":59014,\"start\":58950},{\"end\":59508,\"start\":59494},{\"end\":59859,\"start\":59778},{\"end\":60168,\"start\":60161},{\"end\":60534,\"start\":60440},{\"end\":60890,\"start\":60848},{\"end\":61153,\"start\":61149},{\"end\":61501,\"start\":61430},{\"end\":61879,\"start\":61875},{\"end\":62205,\"start\":62093},{\"end\":62436,\"start\":62334},{\"end\":62630,\"start\":62543},{\"end\":62800,\"start\":62728},{\"end\":63268,\"start\":63115},{\"end\":63761,\"start\":63617},{\"end\":64005,\"start\":63912},{\"end\":64871,\"start\":64749},{\"end\":65123,\"start\":65006},{\"end\":54701,\"start\":54639},{\"end\":55269,\"start\":55207},{\"end\":60615,\"start\":60536},{\"end\":61559,\"start\":61503}]"}}}, "year": 2023, "month": 12, "day": 17}