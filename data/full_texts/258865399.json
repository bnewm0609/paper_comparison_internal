{"id": 258865399, "updated": "2023-10-17 16:33:48.798", "metadata": {"title": "Adversarial Demonstration Attacks on Large Language Models", "authors": "[{\"first\":\"Jiongxiao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zichen\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Keun\",\"last\":\"Park\",\"middle\":[\"Hee\"]},{\"first\":\"Zhuojun\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Zhaoheng\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Zhuofeng\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Muhao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Chaowei\",\"last\":\"Xiao\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.14950", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-14950", "doi": "10.48550/arxiv.2305.14950"}}, "content": {"source": {"pdf_hash": "1abfc211793c683972ded8d3268475e3ee7a88b0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.14950v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ab0638558e48e5973342289f8d94690d965431f9", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/1abfc211793c683972ded8d3268475e3ee7a88b0.txt", "contents": "\nAdversarial Demonstration Attacks on Large Language Models\n14 Oct 2023\n\nJiongxiao Wang \nUniversity of Wisconsin-Madison\n\n\nZichen Liu \nArizona State University\n\n\nKeun Hee Park \nArizona State University\n\n\nZhuojun Jiang \nTokyo Metropolitan University\n\n\nZhaoheng Zheng \nUniversity of Southern California\n\n\nZhuofeng Wu \nUniversity of Michigan -Ann Arbor\n\n\nMuhao Chen \nUniversity of California\nDavis\n\nChaowei Xiao \nUniversity of Wisconsin-Madison\n\n\nAdversarial Demonstration Attacks on Large Language Models\n14 Oct 2023D19F17C03268DE3028B79FF465A6A613arXiv:2305.14950v2[cs.CL]\nWith the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts.While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack.In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations.We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models.Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease.Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs.As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it.To achieve it, we propose the transferable version of advICL, named Transferable-advICL.Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples.We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.\n\nIntroduction\n\nThe development of large language models (LLMs; Brown et al. 2020) has introduced a new paradigm for solving various (NLP) tasks through in-context * The first two authors contributed equally.\n\nlearning (Dong et al., 2022).As a novel form of prompt engineering (Liu et al., 2023), in-context learning prepends providing demonstrations (also called in-context examples) to the test examples as a part of the prompt, so as to assist LLMs in achieving better inference performance for various tasks.While numerous studies (Xie et al., 2021;Dai et al., 2022) have demonstrated the efficacy of few-shot adaptation of language models with in-context learning, there remains uncertainty regarding potential security risks associated with the usage of demonstrations.\n\nTo investigate the security threat of the model, numerous adversarial attacks have been developed (Ebrahimi et al., 2017;Gao et al., 2018;Jin et al., 2020;Li et al., 2018;Ribeiro et al., 2020;Li et al., 2020) with the aim of evaluating the adversarial (worst-case) robustness of the model.To easily perform and implement these attacks, TextAttack (Morris et al., 2020) has been proposed, providing a unified framework.Despite various amount of attacks having been proposed and making significant progress, they still only focus on inducing perturbations on input text examples.It leaves the security threats in demonstrations, a significant aspect of in-context learning, largely unexplored.Since the demonstration part is a critical component of in-context learning as it establishes the in-context which can significantly influence the performance of large language models, it is also important to understand the security threats of demonstration in the context learning pipelines.\n\nIn this paper, we propose a simple yet effective in-context learning attack method named advICL to investigate the impact of demonstrations.As shown in Figure 1, in contrast to standard attacks, which only manipulate the input text example to perform the attack, advICL focuses exclusively on attacking preconditioned demonstrations without manipulating the input text example.Specifically, to make our attack easily and flexibly deployable in existing systems, we design our attack under the TextAttack framework but add extra demonstration masking that only allows manipulating demonstration.Moreover, different from text example-based attacks, given the extended length of in-context learning prompts, standard global similarity constraints (Jin et al., 2020) between the adversarial text and the original text have proven to be less effective, potentially compromising the quality of adversarial examples.To address this challenge, we introduce a demonstration-specific similarity method that applies constraints on each individual demonstration.This method ensures the generation of effective and high-quality adversarial examples in our attack strategy.\n\nWe conduct comprehensive experiments on four datasets including SST-2 (Socher et al., 2013), TREC (Voorhees and Tice, 2000), DBpedia (Zhang et al., 2015), RTE (Dagan et al., 2005) and on diverse LLMs including the GPT2-XL (Radford et al., 2019), LLAMA (Touvron et al., 2023), Vicuna (Chiang et al., 2023).Our method can successfully attack LLMs (e.g.97.72% attack success rate (ASR) for LLaMA-7B on DBpedia), by only manipulating the demonstration without changing the input text.We also conduct experiments with our attack method on different numbers of demonstrations (in-context few-shot learning setting).Our results demonstrate that although a larger number of demonstrations can potentially increase the performance of LLMs, a larger number of adversarial demonstrations are prone to have more threats to the robustness of in-context learning.For instance, implementing advICL on the LLaMA-7B model with the DBpedia dataset achieves an ASR of 97.72% for 8 shots.This is a significant improvement compared to the 1 shot setting, which only reaches 59.39% ASR under the same condition.\n\nAdditionally, one intrinsic property of the demonstrations is that they can be used (prepended) with different inputs.As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it.To achieve it, we propose the transferable version of advICL, named Transferable-advICL.In detail, Transferable-advICL first iteratively and randomly chooses a small set of text examples.It then employs our original advICL to generate the \"universally\" adversarial demonstrations that can mislead the model by concatenating with all the selected texts.Through Transferable-advICL, we can reach 72.32% ASR given different unseen input text examples on DBpedia.\n\n\nRelated work\n\nIn-Context Learning.Current studies on the robustness of in-context learning primarily focus on the instability of demonstrations (Liu et al., 2023).Researchers have identified various factors that can significantly impact the performance of in-context learning, including demonstration selection (Liu et al., 2021), demonstration ordering (Lu et al., 2021), and even label selection for demonstrations (Wei et al., 2023).Several methods (Wu et al., 2022;Liu et al., 2021;Zhao et al., 2021;Chen et al., 2022) have been proposed to improve the stability of in-context learning.For instance, Liu et al. (2021) present a simple yet effective retrievalbased method that selects the most semantically similar examples as demonstrations, leading to improved accuracy and higher stability.Another work by Zhao et al. (2021) claims that the instability of in-context learning is coming from the language model's bias toward predicting specific answers.Then they propose a method to mitigate this bias by applying an affine transformation (Platt et al., 1999) to the original probabilities, where the transformation weights are computed based on a context-free test, such as \"N/A\".Despite these efforts, the adversarial robustness of in-context learning, particularly when considering perturbations on demonstrations, still remains unexplored, which is exactly the focus of this work.Adversarial Examples.The vulnerability of deep neural networks to adversarial examples was first introduced in (Goodfellow et al., 2014) on image classification tasks.They demonstrated that small perturbations could manipulate the model into incorrect predictions.This concept was later extended to textual data in the field of natural language processing by (Papernot et al., 2016), paving the way for various studies in text adversarial attack.The discrete nature of textual examples allows adversarial perturbations to present at three distinct levels: character-level (Gao et al., 2018;Pruthi et al., 2019;Li et al., 2018), word-level (Jia et al., 2019;Zang et al., 2020;Ren et al., 2019) and sentence-level (Naik et al., 2018;Ribeiro et al., 2020).In this paper, we focus solely on characterlevel and word-level perturbations.\n\n\nMethod\n\nWe now describe the proposed advICL method starting with the background of in-context learning, followed by technical details of the demonstration attack.\n\n\nBackground\n\nIn-Context Learning.Formally, in-context learning is defined as a conditional text generation problem (Liu et al., 2021).Given a language model f , we aim to generate output y test based on the input test example x test and a demonstration set C, where C contains N concatenated data-label pairs (x i , y i ) with a specific template s, noted as C = {s(x 1 , y 1 ), ..., s(x N , y N )}.We can also prepend an optimal task instruction I (Dong et al., 2022) to demonstrations as C = {I, s(x 1 , y 1 ), ..., s(x N , y N )}.\n\n(1)\n\nThen, given the input test example, we can generate y test :\ny test = f generate ({C, s(x test , _)}),(2)\nwhere s(x test , _) indicates using the same template as demonstrations but with the label empty.\n\nTo apply it to the standard classification tasks, we let the language model choose answer y test from a candidate classification label set Y = {c 1 , ..., c k }, where c k is denoted as the label of the k-th class.For convenience, we can define a verbalizer function V (Kim et al., 2022) which maps each of the original labels c k to a specific token V(c k ).However, not all labels can be directly mapped to a single token.For instance, the label \"Negative\" is composed of two tokens \"Neg\" and \"ative\" when using the GPT2 tokenizer.In this case, we define the function V mapping \"Neg\" and \"ative\" to \"{space}Negative\", where an additional space prepended to the word \"Negative\".\n\nTo compute the logits z k of the token V(c k ) from causal language models as the probability of c k , we can use the following equation:\nz k = f causal (V(c k )|{C, s(x test , _)}). (3)\nThe final prediction result y test is the label with the highest logits probability:\ny test = arg max c k \u2208Y f causal (V(c k )|{C, s(x test , _)}).\n(4)\n\n\nadvICL\n\nFor in-context learning, as mentioned before, it consists of both demonstration C and input test examples x test , providing a wider attack vector compared to standard adversarial attacks.In this paper, we aim to only manipulate the demonstration C without changing the input text examples x to mislead the models.Since the main goal of this paper is to investigate the adversarial robustness of the attack vector on demonstrations, for simplification, we mainly focus on applying word-level and character-level perturbation following TextBugger (Li et al., 2018). 1ur attack, advICL, builds upon TextAttack (Morris et al., 2020), a standard attack framework.However, unlike other adversarial text attack methods, which manipulate the input test example x test by adding the perturbation \u2206, here we add a mask to only manipulate the demonstration C. Additionally, since the demonstration C consists of multiple sentence label pairs ((x 1 , y 1 ), ..., (x N , y N )), we sequentially set individual perturbation bounds \u2206 i for each x i using cosine similarity, which can be computed by the following equation:\nCosSim(x i , x \u2032 i ) = e(x i ) \u2022 e(x \u2032 i ) ||e(x i )|| ||e(x \u2032 i )||(5)\nwhere x \u2032 i is the perturbed sentence and e(\u2022) represents the embedding vector computed by Universal Sentence Encoder (Cer et al., 2018).\n\nThis approach allows us to control the perturbations specifically for each demonstration, providing more fine-grained control over the attack process.Within this context, we can formalize our objective function:\nmin \u03b4\u2208\u2206 L(f ({C \u03b4 , s(x test , _)}), y test ),(6)\nwhere L calculates the probability of the ground truth label y test using language model f , C \u03b4 = {I, s(x 1 + \u03b4 1 , y 1 ), ..., s(x N + \u03b4 N , y N )}, \u03b4 i represents the perturbation added to the demonstration x i under the given bound \u2206 i .To solve the above objective function, we consider a more practical black-box setting and adopt the greedy search in TextAttack framework (Morris et al., 2020).Specifically, given a demonstration set C with a test example pair (x test , y test ), we initially select words from x i in C to form a W ordList based on the word importance level.For each word in the W ordList, we compute the objective function value L by adding the word with 4 types of perturbations, including Character Insertion, Character Deletion, Character Swap and Word Swap from TextAattack framework.We then use the perturbation that yields the largest reduction in the objective function values calculated by L while maintaining the similarity constraint CosSim(x i , x \u2032 i ) within the bound \u2206 i .The process iterates through the W ordList until a successful attack is achieved or fails when reaching the maximum iteration.We also present our pseudo code in Appendix A.\n\n\nTransferable-advICL\n\nOne intrinsic property of the demonstrations is that they can be used (prepended) with different inputs.As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating this example.To achieve such a threat model, one solution is to leverage the transferability of the adversarial demonstration: generating a high transferable adversarial demonstration that can be prepended to an arbitrary example to mislead the model.\n\nHowever, our advICL is only performed on a single specific text example, limiting its potential to be transferable to other test input examples.To address this limitation, inspired by the concept of Universal Adversarial Perturbation (Moosavi-Dezfooli et al., 2017), previously implemented for image classification tasks, we propose an iterative attack pipeline that aims to generate a universally adversarial demonstration by iteratively attacking a randomly selected set of input examples instead of a single input example, denoted as the Transferable-advICL (T-advICL in short).This approach can significantly extend the applicability and efficacy of advICL across various test examples.The detailed process of the pipeline is shown as follows.\n\nStep 1: Given a demonstration set C, T-advICL randomly identifies a small attack candidate set with k test examples, denoted as S = {(x i , y i )} k i=1 .Note that, to better improve the transferability, here we only randomly select the test example in S with demonstration set {C, s(x i , _)} that can be precisely classified into y i under the language model f .\n\nStep 2: T-advICL randomly shuffles test examples in S to obtain S random .The random shuffle here can help us generate optimal demonstrations by preventing stuck in a local minimum.\n\nStep 3: Before the iterative attack process, we initialize C \u2032 = C.For each {C \u2032 , s(x i , _)} at iteration i, where x i is chosen from S random , we apply advICL to obtain the adversarial demonstration set C i .Then we update C \u2032 with C i only if C i has a lower Attack Accuracy than C \u2032 on S. Note that we need to add an additional constraint that bounds the distance between C i and C \u2032 and the distance between C i and C.\n\nWe then repeat Step 2 and Step 3 until R rounds are complete.Our pseudo code of T-advICL is also presented in Appendix A.\n\nIn this section, we start by introducing our experimental setup, followed by an evaluation of the effectiveness of advICL.We also conduct a perceptual evaluation to ensure the high quality of our generated attack demonstrations.Finally, we carry out ablation studies to evaluate the quality of adversarial demonstrations generated by different similarity constraint methods and the influence of varying templates used in in-context learning.\n\n\nExperimental Setup\n\nLanguage Models and Dataset.We conduct experiments using the following language models: the widely used GPT2-XL, recently opensourced LLaMA-7B, and Vicuna-7B.Vicuna-7B is an instruction-tuned LLaMA-7B with GPT4-or ChatGPT-generated text collected from ShareGPT.com.\n\nWe conduct experiments using four classification datasets, including binary classification datasets SST-2 and RTE, multi-class classification datasets TREC and DBpedia.These datasets cover tasks such as sentiment analysis, textual entailment, topic and question classification tasks.In-Context Learning and Attack Settings.For in-context learning, we follow the setting by (Zhao et al., 2021) and use their templates to incorporate demonstrations for prediction.The details of these templates are listed in Appendix B. Regarding the number of demonstrations, we use 1-shot, 4-shot, and 8-shot settings.Specifically, given each test input from the test set, we randomly select the demonstration from the training set and repeat the processing 5 times to calculate the average accuracy.For attacks, we follow Li et al. (2018) and adopt the untargeted and black-box settings, where we only have access to logits of predicted next tokens.To specify the perturbation bounds \u2206 i for our attack method, we employ the cosine similarity between the adversarial and original individual demonstration sentences with a threshold of 0.8.Evaluation Metrics.We use Attack Success Rate (ASR) (Wang et al., 2021) to evaluate the effectiveness of advICL.Given a dataset D with N data instance x and label y, an adversarial attack method A that generates adversarial examples A(x).The ASR is then calculated as ASR = (x,y)\u2208D\n1[f(A(x))\u0338 =y] 1[f(x)=y] .\nIn addition, we also report the clean accuracy (Clean Acc) which evaluates the classification accuracy with clean demonstrations, and attack accuracy (Attack Acc) which evaluates the classification accuracy with generated adversarial demonstrations.\n\n\nEffectiveness of advICL\n\nTable 1 presents the results of advICL, from which we can observe that in-context learning indeed introduces a new security concern.By manipulating only the demonstrations but not the input test examples, attackers can successfully mislead the model.For instance, ASR of advICL on DBpedia and RTE with 8-shot demonstrations can reach even higher than 95% among all test models.We also show the visualization of an adversarial example generated by advICL in Figure 2.\n\nAdditionally, the consistent trend in Table 1 shows that larger shot numbers can bring higher ASR.This observation reveals that, although larger shot numbers can normally benefit the performance of in-context learning, they can also introduce a higher potential of threats to LLMs.\n\n\nPerceptual Evaluation\n\nTo conduct a comprehensive perceptual evaluation of generated adversarial demonstrations, we employ a diverse set of metrics that encompass both human evaluation and automatic text quality evaluation.\n\nOur evaluation process begins with a thorough examination of the attacked demonstrations by human annotators.Given a complete set of successfully attacked instances with n examples, each instance undergoes a meticulous inspection by the annotators.Evaluations are conducted based on established criteria through three dimensions: semantic coherence, grammatical accuracy, and expression fluency.The annotators then count the number of examples, represented as c, that meet the criteria across all three demonstrations.Following this, we calculate the Annotator Evaluation Quality Score (AEQS) using the formula AEQS = c n .For automatic text quality evaluation, we employ a selection of metrics: Cosine Similarity (CosSim) and Bilingual Evaluation Understudy (BLEU; Papineni et al. 2002).These metrics are computed directly from the original and adversarial sentences.In addition, we use Perplexity (PPL) to evaluate the text fluency.Unlike the previous metrics, PPL is calculated for individual sentences.Therefore, we compute perplexity for both the original and adversarial sentences, referred to as Original PPL and Adv PPL respectively.For further details on these machine evaluation metrics, please refer to Appendix C.\n\nWe present our perceptual evaluation results of advICL in Table 2.For this evaluation, we select 100 successfully attacked examples across all datasets on LLaMA-7B.CosSim, BLEU, Attacked PPL and Original PPL represent the average scores among these 100 examples.Table 2 reveals an average AEQS of 90% and high machine evaluation scores.Based on these results, we can conclude that our generated adversarial demonstrations maintain high perceptual quality under both human and machine evaluations.\n\n\nAblation Study\n\nSimilarity constraint.To validate the importance of our new similarity constraint, denoted as \u2206 i , we compare it with the standard global demonstration perturbation bound \u2206 used in TextBugger, denoted as the baseline in Table 3 by perceptual evaluation.We keep everything the same and just replace the similarity constraint with the standard global demonstration perturbation bound.We use the adversarial demonstrations generated on LLaMA-7B on the SST-2 dataset for analysis.The constraint configurations in TextAttck framework for baseline and advICL are shown in Appendix D.\n\nAs the results presented in Table 3, all quality metrics for our method consistently outperform those of baseline, except for the 1-shot BLEU score.This comparison demonstrates that our individual perturbation bounds ensure the high quality of generated adversarial examples.We also show visualized examples generated by our advICL and baseline respectively in Appendix E. We can also find that the visual quality of our method is higher than the baseline.Different Template.A previous study (Min et al., 2022) demonstrated that the performance of incontext learning can vary significantly under different templates.To ensure the effectiveness of our attack method across diverse templates, we conducted additional experiments on the SST-2 dataset among different models using an alternative template, as suggested by Min et al. (2022).This al-  ternative template, referred to as SST-2-Alter, is shown in detail in Appendix B. The results of these experiments, presented in Table 4, show high ASR values and consistent increasing trends with larger shots numbers.These results lead us to conclude that our advICL performs stably under different templates.\n\n\nPerformance of Transferable-advICL\n\nIn this section, we investigate the transferability of adversarial demonstrations generated by advICL and Transferable-advICL.Specifically, we examine whether an adversarial demonstration generated for an input test example, x 1 , can be transferred to successfully attack another input test example, x 2 , even without prior knowledge.Experimental Setup.In order to thoroughly illustrate the transferability of our method, we randomly select N adv adversarial demonstration sets, denoted as C in total.We also randomly sample N test test input examples exclusively from S for transferability evaluation, denoting this test set as O.For each adversarial demonstration from C, we evaluate the model performance on all test examples from O. We then average model performance accuracy across the N adv adversarial demonstration sets to obtain the average attack accuracy (Avg Attack Acc).Similarly, the average clean accuracy (Avg Clean Acc) can be calculated by replacing each adversarial demonstration set C \u2032 with its corresponding clean demonstration C. The formula for computing Avg Attack Acc can be found in the Appendix F. To better evaluate the performance of transferable attack, We also compute the Transfer Attack Success Rate (TASR) for both methods by TASR = Avg Clean Acc \u2212 Avg Attack Acc Avg Clean Acc .\n\nTo evaluate the performance of our T-advICL, we conduct attacks for each clean demonstration, we select a small candidates set S with k = 15 test example number and R = 3 iterative rounds.Transferability Performance.In Table 5, we report the Avg Clean Acc, Avg Attack Acc, and TASR for both our advICL and T-advICL, as applied to the LLaMA-7B model across all four of our datasets.From Table 5, we found that performing advICL only yields a limited TASR, even leading to negative values in some cases.Such negative TASR means that adversarial perturbations generated by advICL possess completely no transferability and even enhance the performance of in-context learning.After applying T-advICL, we can see a significant increase of TAST from 72.32%    on LLaMA-7B are conducted with different R and fixed k = 15.We present our results in Figure 4.\n\nAs seen in Figure 4, there is a decreasing trend in Avg Attack Acc as the number of iterative rounds increases.It's also observed that the performance of our T-advICL tends to converge at around R = 3 iterative rounds.This suggests that R = 3 is an optimal number of iterations, with a balance between effectiveness and efficiency.\n\n\nConclusion\n\nThis paper studies adversarial robustness of incontext learning, with a particular focus on demonstration attacks.By performing our advICL, we find that demonstrations used in in-context learning are vulnerable to adversarial attacks.Even worse, a larger number of demonstrations can exacerbate such security concerns.Additionally, using Transferable-advICL, we also demonstrate that our generated adversarial demonstrations tend to be transferrable to a wider range of input text examples such that it can attack the input text examples even without knowing them.\n\n\nLimitations\n\nTo ensure easy and flexible deployment of our attack, we implement it in the TextAttack framework using a black-box approach.However, it has a limitation in terms of time cost.Since we assume no access to the model and cannot calculate gradients through back propagation, we need to estimate the gradient direction to generate adversarial examples, resulting in higher computational requirements compared to other white-box methods.Since the primary objective of this paper is to demonstrate the new emergent security threats introduced in the in-context learning framework, and the design of our method is not dependent on any specific attack methods, we leave the implementation of a more efficient attack strategy as future work.\n\n\nA Pseudo Code\n\nA.1 advICL Peseudo code of advICL is presented in Algorithm 1.\n\nTwo extra functions of W orkImportanceRank and SelectBug, whose pseudo codes are shown in Algorithm 3 and Algorithm 4 respectively, would be used in Algorithm 1.\n\nIn line 5 of Algorithm 3, f ytest presents the logits value of ground truth label y test using language model f and C adv /{w j } means delete word w j in demonstration set C adv .\n\nIn line 2 of Algorithm 4, function BugGenerator can generate a set named bugs which has four kinds of bugs including Character Insertion, Character Deletion, Character Swap and Word Swap.\n\n\nA.2 Transferable-advICL\n\nPseudo code for Transferable-advICL is illustrated in Algorithm 2. It also uses functions of W orkImportanceRank and SelectBug.\n\n\nB In-Context Template\n\nTable 6 shows templates employed in our study for the DBpedia, SST-2, TREC and RTE datasets.For an alternative template for SST-2, we utilized the minimal template from (Min et al., 2022), denoted as SST-2-Alter.\n\n\nC Evaluation Metrics\n\nHere, we provide a brief introduction to three distinct automatic evaluation metrics commonly employed in NLP:\n\nCosine Similarity (CosSim): To evaluate semantic coherence between original and adversarial sentences, we involve calculating the cosine similarity on the entire adversarial test prompt with the original one.\n\nBilingual Evaluation Understudy (BLEU): BLEU is a commonly employed method for evaluating structural similarity between two sentences in an automated manner.Here we utilize the BLEU score to compare the original sentence with the adversarial sentence, assessing the degree of similarity in their grammatical structures.\n\nPerplexity (PPL): PPL is a widely used metric in neural language processing which measures the text fluency of language models.In our scenario, we compute perplexity of both the original and adversarial sentences, denoted as Original PPL and Adv PPL, respectively.\n\n\nD TextAttack Configuration\n\nAs our advICL is built upon the TextAttack framework, we use certain configurations and functions from TextAttack.In Table 7, we list all configurations of advICL in comparison to the baseline TextBugger in TextAttack framework.The differences are highlighted using colored text for easy reference.\n\n\nE Examples Under Different Similarity Constraint\n\nTo offer a clearer perception of the differences in quality between adversarial demonstrations generated by the baseline TextBugger and advICL, we present two examples for comparison in Figure 5.It is evident from the figure that advICL not only performs a successful attack but also generates adversarial demonstrations with a prominent enhancement in quality.\n\n\nF Formulation of Avg Attack Acc\n\nWe show the formulation of computing Avg Attack Acc in Figure 6.S random \u2190 RandomShuf f le(S); 5:\n\nfor (x i , y i ) in S random do 6:\n\nInitialize: C i \u2190 C adv , V \u2190 V ocabulary({x 1 adv , ..., x N adv });\n\n\n7:\n\nW ordList ordered = W ordImportanceRank(V, C i , s(x test , y test ), f );\n8:\nfor w j in W ordList ordered do 9:\n\nw \u2032 j = SelectBug(w j , C i , s(x i , y i ), f ); 10:\n\nx \u2032 l \u2190 replace w j \u2208 x l with w \u2032 j , C i \u2190 replace x l \u2208 C i with x \u2032 l ;\n\n11:\n\nif CosSim(x l , x \u2032 l ) \u2264 \u03b5 or CosSim(x lo , x \u2032 l ) \u2264 \u03b5 a then 12:\n\nRoll back the attack on w j , C i \u2190 replace x \u2032 l \u2208 C i with x l ;\n\nFigure 1 :\n1\nFigure 1: Difference between existing attacks and ours on sentimental classification task.The previous attacks mainly perform attack on the text input examples while ours focuses exclusively on attacking demonstrations.\n\n\nFigure 2 :\n2\nFigure 2: Visualization of an adversarial example generated by advICL on the DBpeda dataset via attacking LLaMA-7B model.\n\n\nFigure 3 :\n3\nFigure 3: Effectiveness of T-advICL with different test example number k of candidates set S. The average accuracy is represented by the point, while the shaded area indicates the variance.\n\n\nFigure 4 :\n4\nFigure 4: Effectiveness of T-advICL among different iteration rounds R: Each subplot depicts the average accuracy for three distinct shot numbers.\n\n\nTable 1 :\n1\nEffective of advICL among different datasets and models under various shot numbers.The highest ASR value among different shot numbers is highlighted.\nDBpediaSST-2TRECRTEModelMetric1-shot 4-shot 8-shot 1-shot 4-shot 8-shot 1-shot 4-shot 8-shot 1-shot 4-shot 8-shotClean Acc37.649.460.459.465.662.620.025.231.456.856.857.8GPT2-XL Attack Acc7.20.60.451.025.211.214.89.67.420.84.21.4ASR80.88 98.79 99.38 14.13 61.60 82.15 25.16 61.15 76.12 63.30 92.60 97.70Clean Acc71.078.477.869.081.892.253.451.058.473.459.061.8LLaMA-7B Attack Acc 28.86.81.840.045.840.231.011.05.416.40.20.0ASR59.39 91.33 97.72 41.90 44.01 56.42 41.95 78.43 90.75 77.63 99.66 100.00Clean Acc77.252.641.479.879.282.860.047.638.673.069.868.2Vicuna-7B Attack Acc 40.06.21.657.443.427.440.410.08.030.65.60.6ASR48.16 88.21 96.14 28.07 45.25 67.08 32.68 79.08 79.40 57.90 91.93 99.11DBpediaSST-2TRECRTEQuality Metric 1-shot 4-shot 8-shot 1-shot 4-shot 8-shot 1-shot 4-shot 8-shot 1-shot 4-shot 8-shotAEQS92.00 94.00 91.00 88.00 89.00 90.00 90.00 92.00 96.00 92.00 92.00 92.00CosSim94.65 96.52 96.78 93.80 94.76 95.25 95.94 96.80 97.79 94.09 97.87 98.63BLEU95.71 96.52 97.94 92.51 91.81 92.59 97.20 96.74 97.57 87.23 93.67 95.77Adv PPL13.579.507.2036.73 22.18 16.53 19.39 12.058.7612.277.826.80Original PPL12.776.875.6146.80 14.23 10.21 21.299.496.998.566.666.13\n\nTable 2 :\n2\nPerceptual Evaluation on 100 successfully attacked examples on LLaMA-7B.\nSST-2Attack Method Quality Metric 1-shot 4-shot 8-shotAEQS70.00 65.00 74.00baselineCosSim91.29 92.54 92.77Adv PPL41.50 24.65 18.35BLEU92.57 90.70 91.82AEQS88.00 89.00 90.00advICLCosSim93.80 94.76 95.25Adv PPL36.73 22.18 16.53BLEU92.51 91.81 92.59Table 3: Similarity constraint analysis among differentperceptual quality metrics using the adversarial demon-stration generated by attacking LLaMA-7B on the SST-2 dataset.\n\nTable 4 :\n4\nEffectiveness of advICL on another different template of SST-2.\nSST-2-AlterModelMetric1-shot 4-shot 8-shotClean Acc51.249.049.8GPT2-XL Attack Acc 26.49.22.0ASR48.52 81.20 95.92Clean Acc60.072.875.6LLaMA-7B Attack Acc 11.413.49.8ASR80.98 81.56 87.05Clean Acc58.054.055.8Vicuna-7B Attack Acc 10.86.23.4ASR81.34 88.75 93.87\n\n\n\nAvg CleanAcc 63.18 75.81 78.55 66.60 82.60 93.65 55.38 52.25 59.40 66.67 65.3063.89advICL Avg Attack Acc 44.37 51.53 52.66 72.02 63.86 71.36 55.09 49.94 54.31 66.75 63.91 65.31 TASR 29.77 32.03 32.96 -8.14 22.68 23.80 Avg Attack Acc 17.09 16.87 21.74 56.30 55.53 55.30 43.38 30.99 29.65 53.30 56.27 57.72 TASR 72.95 77.45 72.32 15.47 32.77 40.95 21.67 40.69 50.08 20.05 13.83 9.66\nDBpediaSST-2TRECRTEAttack MethodMetric1-shot 4-shot 8-shot 1-shot 4-shot 8-shot 1-shot 4-shot 8-shot 1-shot 4-shot 8-shot0.524.428.57-0.122.13-2.22T-advICL\n\nTable 5 :\n5\nTransferability of adversarial demonstrations generated by T-advICL compared with advICL.The highest TASR value between two methods is highlighted.to 32.96% for the DBpedia dataset with 8-shot and negative TASR values would never present in all circumstances.These results indicate that in contrast to advICL, adversarial demonstrations generated by T-advICL generalize much better and have more stable transferability cross various input test examples.\n\n\n\n\nTest example pair (x test , y test ), demonstration set C = {I, s(x 1 , y 1 ), ..., s(x N , y N )}, language model f , similarity threshold \u03b5 Output: Adversarial demonstration set C adv = {I, s(x 1 adv , y 1 ), ..., s(x N adv , y N )} 1: Initialize:C adv \u2190 C, V \u2190 V ocabulary({x 1 , ..., x N }); 2: W ordList ordered = W ordImportanceRank(V, C adv , s(x test , y test ), f ); 3: for w j in W ordList ordered do SelectBug(w j , C adv , s(x test , y test ), f ); \u2190 replace w j \u2208 x i with w \u2032 j , C adv \u2190 replace x i adv \u2208 C adv with x \u2032 i ; Demonstration set C = {I, s(x 1 , y 1 ), ..., s(x N , y N )}, original demonstration set C original = {I, s(x 1o , y 1 ), ..., s(x No , y N )}, test examples set S = {(x i , y i )} k i=1, language model f , maximum iteration rounds M axIter, step-wise similarity threshold \u03b5, accumulative similarity threshold \u03b5 a Output: Adversarial demonstration set C adv = {I, s(x 1 adv , y 1 ), ..., s(x N adv , y N )}\nAlgorithm 1 advICLInput: 4: w \u2032 j = 5: x \u2032 i 6: if CosSim(x i , x \u2032 i ) \u2264 \u03b5 then7:return None8:else if f ({C adv , s(x test , _)}) \u0338 = y test then9:return C adv10:end if11: end for12: return NoneAlgorithm 2 Transferable-advICLInput:\n1: Initialize: C adv \u2190 C; 2: iter \u2190 0, bestASR \u2190 1; 3: while iter < M axIter and bestASR > 0 do 4:\n\n\nTable 6 :\n6\nelse if f ({C i , s(x test , _)}) \u0338 = y test thenEvaluate ASR of C i on S: ASR \u2190 Evaluate(C i , S); Word Importance Ranking 1: function WordImportanceRank(V, C, s(x test , y test ), f ) 2: Initialize: W ordList \u2190 EmptySet(); 3: for w j in vocabulary set V do 4:if w j \u2208 x i , i \u2208 1...N then 5: Compute word importance score D w j for w j D w j = f ytest ({C adv , s(x test , _)}) \u2212 f ytest ({C adv /{w j }, s(x test , _)}); W ordList ordered \u2190 Sort(WordList) with descending D w j ; 10: return W ordList ordered 11: end function Algorithm 4 Bug Selection 1: function SelectBug(w, C, s(x test , y test ), f ) 2: bugs \u2190 BugGenerator(w); 3: for each b k in bugs do : end for 7: bug best \u2190 arg max b k score(k); 8: return bug best 9: end function Classify the documents based on whether they are about a Company, School, Artist, Athlete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film, or Book.Article: [sentence] Article: It's a Long Long Way to Tipperary is a 1914 Australian silent film based on the song It's a Long Way to Tipperary by Jack Judge.Classify the questions based on whether their answer type is a Number, Location, Person, Description, Entity, or Abbreviation.This table presents template designs for all datasets used in this paper.An extra example for each template is provided for better understanding.\n13:Break14:15:Break16:end if17:end for18:19:if ASR \u2264 bestASR then20:bestASR \u2190 ASR, C adv \u2190 C i ;21:end if22:end for23: end while24: return C adv\nk \u2190 replace w with b k in C; 5: score(k) = f ytest ({C, s(x test , _)}) \u2212 f ytest ({C \u2032 k , s(x test , _)}); 6\n\nSince our attack is a general method, it can be easily extended to be applied with other types of perturbation. We leave that as the future work.\nEthics StatementThrough the investigation of the in-context learning from the security perspective, we hope our work can raise awareness for the community of such vulnerabilities.We highlight the importance of the demonstration in the context learning framework and inspire the community to design protection strategies for safe storing, retrieving and verifying the demonstration data for making predictions.All data, models we use in this work are publicly available.\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nDaniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, arXiv:1803.11175Universal sentence encoder. 2018arXiv preprint\n\nYanda Chen, Chen Zhao, Zhou Yu, Kathleen Mckeown, He He, arXiv:2209.07661On the relation between sensitivity and accuracy in in-context learning. 2022arXiv preprint\n\nVicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Ido Dagan, Oren Glickman, and Bernardo Magnini. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Machine learning challenges workshop. Springer2023. 2005The pascal recognising textual entailment challenge\n\nWhy can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei, arXiv:2212.105592022arXiv preprint\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint\n\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou, arXiv:1712.06751Hotflip: White-box adversarial examples for text classification. 2017arXiv preprint\n\nBlack-box generation of adversarial text sequences to evade deep learning classifiers. Ji Gao, Jack Lanchantin, Mary Lou Soffa, Yanjun Qi, 2018 IEEE Security and Privacy Workshops (SPW). IEEE2018\n\nExplaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, arXiv:1412.65722014arXiv preprint\n\nRobin Jia, Aditi Raghunathan, Kerem G\u00f6ksel, Percy Liang, arXiv:1909.00986Certified robustness to adversarial word substitutions. 2019arXiv preprint\n\nIs bert really robust? a strong baseline for natural language attack on text classification and entailment. Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2020\n\nSelf-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. Joon Hyuhng, Hyunsoo Kim, Junyeob Cho, Taeuk Kim, Kang Kim, Min Yoo, Sang-Goo Lee, arXiv:2206.080822022arXiv preprint\n\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting Wang, arXiv:1812.05271Textbugger: Generating adversarial text against real-world applications. 2018arXiv preprint\n\nBert-attack: Adversarial attack against bert using bert. Linyang Li, Ruotian Ma, arXiv:2004.099842020arXiv preprintQipeng Guo, Xiangyang Xue, and Xipeng Qiu\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint\n\nPretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023\n\nFantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021arXiv preprint\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint\n\nUniversal adversarial perturbations. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017\n\nTextattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, Yanjun Qi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2020\n\nAakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig, arXiv:1806.00692Stress test evaluation for natural language inference. 2018arXiv preprint\n\nCrafting adversarial input sequences for recurrent neural networks. Nicolas Papernot, Patrick Mcdaniel, Ananthram Swami, Richard Harang, MIL-COM 2016-2016 IEEE Military Communications Conference. IEEE2016\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002\n\nProbabilistic outputs for support vector machines and comparisons to regularized likelihood methods. John Platt, Advances in large margin classifiers. 1031999\n\nDanish Pruthi, Bhuwan Dhingra, Zachary C Lipton, arXiv:1905.11268Combating adversarial misspellings with robust word recognition. 2019arXiv preprint\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019\n\nGenerating natural language adversarial examples through probability weighted word saliency. Yihe Shuhuai Ren, Kun Deng, Wanxiang He, Che, 10.18653/v1/P19-1103Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019\n\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh, arXiv:2005.04118Beyond accuracy: Behavioral testing of nlp models with checklist. 2020arXiv preprint\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013\n\nThibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint\n\nBuilding a question answering test collection. M Ellen, Dawn M Voorhees, Tice, Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. the 23rd annual international ACM SIGIR conference on Research and development in information retrieval2000\n\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li, arXiv:2111.02840Adversarial glue: A multitask benchmark for robustness evaluation of language models. 2021arXiv preprint\n\nLarger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023arXiv preprint\n\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong, arXiv:2212.10375Self-adaptive in-context learning. 2022arXiv preprint\n\nAn explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, arXiv:2111.020802021arXiv preprint\n\nWord-level textual adversarial attacking as combinatorial optimization. Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, 10.18653/v1/2020.acl-main.540Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020Qun Liu, and Maosong Sun\n\nCharacter-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in neural information processing systems. 201528\n\nCalibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021\n", "annotations": {"author": "[{\"end\":122,\"start\":73},{\"end\":161,\"start\":123},{\"end\":203,\"start\":162},{\"end\":250,\"start\":204},{\"end\":302,\"start\":251},{\"end\":351,\"start\":303},{\"end\":395,\"start\":352},{\"end\":443,\"start\":396}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":83},{\"end\":133,\"start\":130},{\"end\":175,\"start\":171},{\"end\":217,\"start\":212},{\"end\":265,\"start\":260},{\"end\":314,\"start\":312},{\"end\":362,\"start\":358},{\"end\":408,\"start\":404}]", "author_first_name": "[{\"end\":82,\"start\":73},{\"end\":129,\"start\":123},{\"end\":166,\"start\":162},{\"end\":170,\"start\":167},{\"end\":211,\"start\":204},{\"end\":259,\"start\":251},{\"end\":311,\"start\":303},{\"end\":357,\"start\":352},{\"end\":403,\"start\":396}]", "author_affiliation": "[{\"end\":121,\"start\":89},{\"end\":160,\"start\":135},{\"end\":202,\"start\":177},{\"end\":249,\"start\":219},{\"end\":301,\"start\":267},{\"end\":350,\"start\":316},{\"end\":394,\"start\":364},{\"end\":442,\"start\":410}]", "title": "[{\"end\":59,\"start\":1},{\"end\":502,\"start\":444}]", "venue": null, "abstract": "[{\"end\":2212,\"start\":572}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2293,\"start\":2276},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2450,\"start\":2431},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2507,\"start\":2489},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2765,\"start\":2747},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2782,\"start\":2765},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3110,\"start\":3087},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3127,\"start\":3110},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3144,\"start\":3127},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3160,\"start\":3144},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3181,\"start\":3160},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3197,\"start\":3181},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3357,\"start\":3336},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4736,\"start\":4718},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5226,\"start\":5205},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5258,\"start\":5233},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5288,\"start\":5268},{\"end\":5314,\"start\":5290},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5379,\"start\":5357},{\"end\":5409,\"start\":5381},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5439,\"start\":5418},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7121,\"start\":7103},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7288,\"start\":7270},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7330,\"start\":7313},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7394,\"start\":7376},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7428,\"start\":7411},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7445,\"start\":7428},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7463,\"start\":7445},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7481,\"start\":7463},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7580,\"start\":7563},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7789,\"start\":7771},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8023,\"start\":8003},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8484,\"start\":8459},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8730,\"start\":8707},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8938,\"start\":8920},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8958,\"start\":8938},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8974,\"start\":8958},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9005,\"start\":8987},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9023,\"start\":9005},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9040,\"start\":9023},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9079,\"start\":9060},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9100,\"start\":9079},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9479,\"start\":9461},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9814,\"start\":9795},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11684,\"start\":11667},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11750,\"start\":11729},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12438,\"start\":12420},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13103,\"start\":13082},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14684,\"start\":14653},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17390,\"start\":17371},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17821,\"start\":17805},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18193,\"start\":18174},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20472,\"start\":20451},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22517,\"start\":22499},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22842,\"start\":22825},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28010,\"start\":27992}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30578,\"start\":30344},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30715,\"start\":30579},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30920,\"start\":30716},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31082,\"start\":30921},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32417,\"start\":31083},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32922,\"start\":32418},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33256,\"start\":32923},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33796,\"start\":33257},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34264,\"start\":33797},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35546,\"start\":34265},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":37170,\"start\":35547}]", "paragraph": "[{\"end\":2420,\"start\":2228},{\"end\":2987,\"start\":2422},{\"end\":3972,\"start\":2989},{\"end\":5133,\"start\":3974},{\"end\":6224,\"start\":5135},{\"end\":6956,\"start\":6226},{\"end\":9179,\"start\":6973},{\"end\":9344,\"start\":9190},{\"end\":9879,\"start\":9359},{\"end\":9884,\"start\":9881},{\"end\":9946,\"start\":9886},{\"end\":10089,\"start\":9992},{\"end\":10770,\"start\":10091},{\"end\":10909,\"start\":10772},{\"end\":11043,\"start\":10959},{\"end\":11110,\"start\":11107},{\"end\":12229,\"start\":11121},{\"end\":12439,\"start\":12302},{\"end\":12652,\"start\":12441},{\"end\":13888,\"start\":12703},{\"end\":14417,\"start\":13912},{\"end\":15166,\"start\":14419},{\"end\":15532,\"start\":15168},{\"end\":15715,\"start\":15534},{\"end\":16142,\"start\":15717},{\"end\":16265,\"start\":16144},{\"end\":16708,\"start\":16267},{\"end\":16996,\"start\":16731},{\"end\":18403,\"start\":16998},{\"end\":18680,\"start\":18431},{\"end\":19174,\"start\":18708},{\"end\":19457,\"start\":19176},{\"end\":19683,\"start\":19483},{\"end\":20910,\"start\":19685},{\"end\":21408,\"start\":20912},{\"end\":22005,\"start\":21427},{\"end\":23163,\"start\":22007},{\"end\":24518,\"start\":23202},{\"end\":25368,\"start\":24520},{\"end\":25701,\"start\":25370},{\"end\":26280,\"start\":25716},{\"end\":27028,\"start\":26296},{\"end\":27108,\"start\":27046},{\"end\":27271,\"start\":27110},{\"end\":27453,\"start\":27273},{\"end\":27642,\"start\":27455},{\"end\":27797,\"start\":27670},{\"end\":28035,\"start\":27823},{\"end\":28170,\"start\":28060},{\"end\":28380,\"start\":28172},{\"end\":28701,\"start\":28382},{\"end\":28967,\"start\":28703},{\"end\":29296,\"start\":28998},{\"end\":29710,\"start\":29349},{\"end\":29843,\"start\":29746},{\"end\":29879,\"start\":29845},{\"end\":29950,\"start\":29881},{\"end\":30031,\"start\":29957},{\"end\":30069,\"start\":30035},{\"end\":30124,\"start\":30071},{\"end\":30201,\"start\":30126},{\"end\":30206,\"start\":30203},{\"end\":30275,\"start\":30208},{\"end\":30343,\"start\":30277},{\"end\":30577,\"start\":30358},{\"end\":30714,\"start\":30593},{\"end\":30919,\"start\":30730},{\"end\":31081,\"start\":30935},{\"end\":31245,\"start\":31096},{\"end\":32503,\"start\":32431},{\"end\":32999,\"start\":32936},{\"end\":33640,\"start\":33260},{\"end\":34263,\"start\":33810},{\"end\":35213,\"start\":34268},{\"end\":35545,\"start\":35447},{\"end\":36913,\"start\":35560},{\"end\":37169,\"start\":37059}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9991,\"start\":9947},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10958,\"start\":10910},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11106,\"start\":11044},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12301,\"start\":12230},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12702,\"start\":12653},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18430,\"start\":18404},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30034,\"start\":30032}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18715,\"start\":18714},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19221,\"start\":19220},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20977,\"start\":20976},{\"end\":21655,\"start\":21654},{\"end\":22042,\"start\":22041},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22989,\"start\":22988},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24746,\"start\":24745},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24913,\"start\":24912},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27830,\"start\":27829},{\"end\":29122,\"start\":29121}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2226,\"start\":2214},{\"attributes\":{\"n\":\"2\"},\"end\":6971,\"start\":6959},{\"attributes\":{\"n\":\"3\"},\"end\":9188,\"start\":9182},{\"attributes\":{\"n\":\"3.1\"},\"end\":9357,\"start\":9347},{\"attributes\":{\"n\":\"3.2\"},\"end\":11119,\"start\":11113},{\"attributes\":{\"n\":\"4\"},\"end\":13910,\"start\":13891},{\"attributes\":{\"n\":\"5.1\"},\"end\":16729,\"start\":16711},{\"attributes\":{\"n\":\"5.2\"},\"end\":18706,\"start\":18683},{\"attributes\":{\"n\":\"5.3\"},\"end\":19481,\"start\":19460},{\"attributes\":{\"n\":\"5.4\"},\"end\":21425,\"start\":21411},{\"attributes\":{\"n\":\"5.5\"},\"end\":23200,\"start\":23166},{\"attributes\":{\"n\":\"6\"},\"end\":25714,\"start\":25704},{\"end\":26294,\"start\":26283},{\"end\":27044,\"start\":27031},{\"end\":27668,\"start\":27645},{\"end\":27821,\"start\":27800},{\"end\":28058,\"start\":28038},{\"end\":28996,\"start\":28970},{\"end\":29347,\"start\":29299},{\"end\":29744,\"start\":29713},{\"end\":29955,\"start\":29953},{\"end\":30355,\"start\":30345},{\"end\":30590,\"start\":30580},{\"end\":30727,\"start\":30717},{\"end\":30932,\"start\":30922},{\"end\":31093,\"start\":31084},{\"end\":32428,\"start\":32419},{\"end\":32933,\"start\":32924},{\"end\":33807,\"start\":33798},{\"end\":35557,\"start\":35548}]", "table": "[{\"end\":32417,\"start\":31246},{\"end\":32922,\"start\":32504},{\"end\":33256,\"start\":33000},{\"end\":33796,\"start\":33641},{\"end\":35446,\"start\":35214},{\"end\":37058,\"start\":36914}]", "figure_caption": "[{\"end\":30578,\"start\":30357},{\"end\":30715,\"start\":30592},{\"end\":30920,\"start\":30729},{\"end\":31082,\"start\":30934},{\"end\":31246,\"start\":31095},{\"end\":32504,\"start\":32430},{\"end\":33000,\"start\":32935},{\"end\":33641,\"start\":33259},{\"end\":34264,\"start\":33809},{\"end\":35214,\"start\":34267},{\"end\":36914,\"start\":35559}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4134,\"start\":4133},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19173,\"start\":19172},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25367,\"start\":25366},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25389,\"start\":25388},{\"end\":29543,\"start\":29542},{\"end\":29809,\"start\":29808}]", "bib_author_first_name": "[{\"end\":37829,\"start\":37826},{\"end\":37845,\"start\":37837},{\"end\":37856,\"start\":37852},{\"end\":37871,\"start\":37864},{\"end\":37886,\"start\":37881},{\"end\":37888,\"start\":37887},{\"end\":37905,\"start\":37897},{\"end\":37922,\"start\":37916},{\"end\":37942,\"start\":37936},{\"end\":37956,\"start\":37950},{\"end\":37971,\"start\":37965},{\"end\":38045,\"start\":38039},{\"end\":38057,\"start\":38051},{\"end\":38072,\"start\":38064},{\"end\":38082,\"start\":38079},{\"end\":38094,\"start\":38088},{\"end\":38111,\"start\":38105},{\"end\":38125,\"start\":38121},{\"end\":38141,\"start\":38136},{\"end\":38166,\"start\":38161},{\"end\":38178,\"start\":38173},{\"end\":38253,\"start\":38248},{\"end\":38264,\"start\":38260},{\"end\":38275,\"start\":38271},{\"end\":38288,\"start\":38280},{\"end\":38300,\"start\":38298},{\"end\":38543,\"start\":38536},{\"end\":38559,\"start\":38552},{\"end\":38566,\"start\":38564},{\"end\":38576,\"start\":38572},{\"end\":38592,\"start\":38584},{\"end\":38600,\"start\":38597},{\"end\":38615,\"start\":38608},{\"end\":38629,\"start\":38623},{\"end\":38645,\"start\":38638},{\"end\":38660,\"start\":38654},{\"end\":38662,\"start\":38661},{\"end\":38676,\"start\":38673},{\"end\":38689,\"start\":38685},{\"end\":38691,\"start\":38690},{\"end\":38912,\"start\":38907},{\"end\":38923,\"start\":38918},{\"end\":38931,\"start\":38929},{\"end\":38942,\"start\":38938},{\"end\":38955,\"start\":38948},{\"end\":38965,\"start\":38961},{\"end\":39014,\"start\":39007},{\"end\":39024,\"start\":39021},{\"end\":39034,\"start\":39029},{\"end\":39042,\"start\":39040},{\"end\":39057,\"start\":39050},{\"end\":39068,\"start\":39062},{\"end\":39078,\"start\":39076},{\"end\":39092,\"start\":39084},{\"end\":39104,\"start\":39097},{\"end\":39185,\"start\":39180},{\"end\":39200,\"start\":39196},{\"end\":39212,\"start\":39206},{\"end\":39225,\"start\":39219},{\"end\":39421,\"start\":39419},{\"end\":39431,\"start\":39427},{\"end\":39448,\"start\":39444},{\"end\":39452,\"start\":39449},{\"end\":39466,\"start\":39460},{\"end\":39580,\"start\":39577},{\"end\":39582,\"start\":39581},{\"end\":39603,\"start\":39595},{\"end\":39621,\"start\":39612},{\"end\":39671,\"start\":39666},{\"end\":39682,\"start\":39677},{\"end\":39701,\"start\":39696},{\"end\":39715,\"start\":39710},{\"end\":39925,\"start\":39923},{\"end\":39938,\"start\":39931},{\"end\":39948,\"start\":39944},{\"end\":39955,\"start\":39949},{\"end\":39967,\"start\":39962},{\"end\":40206,\"start\":40202},{\"end\":40222,\"start\":40215},{\"end\":40235,\"start\":40228},{\"end\":40246,\"start\":40241},{\"end\":40256,\"start\":40252},{\"end\":40265,\"start\":40262},{\"end\":40279,\"start\":40271},{\"end\":40328,\"start\":40321},{\"end\":40341,\"start\":40333},{\"end\":40352,\"start\":40346},{\"end\":40359,\"start\":40357},{\"end\":40368,\"start\":40364},{\"end\":40548,\"start\":40541},{\"end\":40560,\"start\":40553},{\"end\":40650,\"start\":40642},{\"end\":40663,\"start\":40656},{\"end\":40675,\"start\":40670},{\"end\":40687,\"start\":40683},{\"end\":40703,\"start\":40695},{\"end\":40717,\"start\":40711},{\"end\":40919,\"start\":40912},{\"end\":40931,\"start\":40925},{\"end\":40944,\"start\":40938},{\"end\":40957,\"start\":40949},{\"end\":40972,\"start\":40965},{\"end\":40988,\"start\":40982},{\"end\":41132,\"start\":41129},{\"end\":41140,\"start\":41137},{\"end\":41158,\"start\":41150},{\"end\":41175,\"start\":41166},{\"end\":41190,\"start\":41184},{\"end\":41243,\"start\":41238},{\"end\":41254,\"start\":41249},{\"end\":41263,\"start\":41260},{\"end\":41279,\"start\":41274},{\"end\":41293,\"start\":41289},{\"end\":41309,\"start\":41301},{\"end\":41326,\"start\":41322},{\"end\":41502,\"start\":41490},{\"end\":41530,\"start\":41521},{\"end\":41542,\"start\":41538},{\"end\":41556,\"start\":41550},{\"end\":41819,\"start\":41815},{\"end\":41831,\"start\":41828},{\"end\":41844,\"start\":41841},{\"end\":41849,\"start\":41845},{\"end\":41859,\"start\":41855},{\"end\":41871,\"start\":41869},{\"end\":41883,\"start\":41877},{\"end\":42108,\"start\":42099},{\"end\":42124,\"start\":42115},{\"end\":42144,\"start\":42138},{\"end\":42159,\"start\":42152},{\"end\":42172,\"start\":42166},{\"end\":42347,\"start\":42340},{\"end\":42365,\"start\":42358},{\"end\":42385,\"start\":42376},{\"end\":42400,\"start\":42393},{\"end\":42549,\"start\":42542},{\"end\":42565,\"start\":42560},{\"end\":42578,\"start\":42574},{\"end\":42593,\"start\":42585},{\"end\":42871,\"start\":42867},{\"end\":42932,\"start\":42926},{\"end\":42947,\"start\":42941},{\"end\":42964,\"start\":42957},{\"end\":42966,\"start\":42965},{\"end\":43133,\"start\":43129},{\"end\":43150,\"start\":43143},{\"end\":43160,\"start\":43155},{\"end\":43173,\"start\":43168},{\"end\":43185,\"start\":43180},{\"end\":43198,\"start\":43194},{\"end\":43329,\"start\":43325},{\"end\":43346,\"start\":43343},{\"end\":43361,\"start\":43353},{\"end\":43619,\"start\":43614},{\"end\":43645,\"start\":43635},{\"end\":43656,\"start\":43650},{\"end\":43673,\"start\":43667},{\"end\":43869,\"start\":43862},{\"end\":43882,\"start\":43878},{\"end\":43898,\"start\":43894},{\"end\":43908,\"start\":43903},{\"end\":43928,\"start\":43917},{\"end\":43930,\"start\":43929},{\"end\":43946,\"start\":43940},{\"end\":43948,\"start\":43947},{\"end\":43964,\"start\":43953},{\"end\":44144,\"start\":44137},{\"end\":44166,\"start\":44159},{\"end\":44181,\"start\":44175},{\"end\":44201,\"start\":44191},{\"end\":44220,\"start\":44212},{\"end\":44238,\"start\":44230},{\"end\":44253,\"start\":44248},{\"end\":44267,\"start\":44263},{\"end\":44281,\"start\":44275},{\"end\":44435,\"start\":44434},{\"end\":44447,\"start\":44443},{\"end\":44449,\"start\":44448},{\"end\":44700,\"start\":44695},{\"end\":44714,\"start\":44707},{\"end\":44727,\"start\":44719},{\"end\":44737,\"start\":44734},{\"end\":44745,\"start\":44743},{\"end\":44761,\"start\":44753},{\"end\":44772,\"start\":44767},{\"end\":44793,\"start\":44791},{\"end\":44984,\"start\":44979},{\"end\":44995,\"start\":44990},{\"end\":45003,\"start\":45001},{\"end\":45015,\"start\":45009},{\"end\":45028,\"start\":45022},{\"end\":45043,\"start\":45037},{\"end\":45054,\"start\":45048},{\"end\":45068,\"start\":45061},{\"end\":45076,\"start\":45074},{\"end\":45089,\"start\":45084},{\"end\":45139,\"start\":45132},{\"end\":45152,\"start\":45144},{\"end\":45167,\"start\":45159},{\"end\":45180,\"start\":45172},{\"end\":45332,\"start\":45328},{\"end\":45351,\"start\":45346},{\"end\":45370,\"start\":45365},{\"end\":45384,\"start\":45378},{\"end\":45501,\"start\":45497},{\"end\":45515,\"start\":45508},{\"end\":45528,\"start\":45520},{\"end\":45542,\"start\":45535},{\"end\":45552,\"start\":45548},{\"end\":45898,\"start\":45893},{\"end\":45911,\"start\":45906},{\"end\":45922,\"start\":45918},{\"end\":46067,\"start\":46062},{\"end\":46078,\"start\":46074},{\"end\":46091,\"start\":46088},{\"end\":46101,\"start\":46098},{\"end\":46115,\"start\":46109}]", "bib_author_last_name": "[{\"end\":37835,\"start\":37830},{\"end\":37850,\"start\":37846},{\"end\":37862,\"start\":37857},{\"end\":37879,\"start\":37872},{\"end\":37895,\"start\":37889},{\"end\":37914,\"start\":37906},{\"end\":37934,\"start\":37923},{\"end\":37948,\"start\":37943},{\"end\":37963,\"start\":37957},{\"end\":37978,\"start\":37972},{\"end\":38049,\"start\":38046},{\"end\":38062,\"start\":38058},{\"end\":38077,\"start\":38073},{\"end\":38086,\"start\":38083},{\"end\":38103,\"start\":38095},{\"end\":38119,\"start\":38112},{\"end\":38134,\"start\":38126},{\"end\":38159,\"start\":38142},{\"end\":38171,\"start\":38167},{\"end\":38182,\"start\":38179},{\"end\":38258,\"start\":38254},{\"end\":38269,\"start\":38265},{\"end\":38278,\"start\":38276},{\"end\":38296,\"start\":38289},{\"end\":38303,\"start\":38301},{\"end\":38550,\"start\":38544},{\"end\":38562,\"start\":38560},{\"end\":38570,\"start\":38567},{\"end\":38582,\"start\":38577},{\"end\":38595,\"start\":38593},{\"end\":38606,\"start\":38601},{\"end\":38621,\"start\":38616},{\"end\":38636,\"start\":38630},{\"end\":38652,\"start\":38646},{\"end\":38671,\"start\":38663},{\"end\":38683,\"start\":38677},{\"end\":38696,\"start\":38692},{\"end\":38916,\"start\":38913},{\"end\":38927,\"start\":38924},{\"end\":38936,\"start\":38932},{\"end\":38946,\"start\":38943},{\"end\":38959,\"start\":38956},{\"end\":38969,\"start\":38966},{\"end\":39019,\"start\":39015},{\"end\":39027,\"start\":39025},{\"end\":39038,\"start\":39035},{\"end\":39048,\"start\":39043},{\"end\":39060,\"start\":39058},{\"end\":39074,\"start\":39069},{\"end\":39082,\"start\":39079},{\"end\":39095,\"start\":39093},{\"end\":39108,\"start\":39105},{\"end\":39194,\"start\":39186},{\"end\":39204,\"start\":39201},{\"end\":39217,\"start\":39213},{\"end\":39229,\"start\":39226},{\"end\":39425,\"start\":39422},{\"end\":39442,\"start\":39432},{\"end\":39458,\"start\":39453},{\"end\":39469,\"start\":39467},{\"end\":39593,\"start\":39583},{\"end\":39610,\"start\":39604},{\"end\":39629,\"start\":39622},{\"end\":39675,\"start\":39672},{\"end\":39694,\"start\":39683},{\"end\":39708,\"start\":39702},{\"end\":39721,\"start\":39716},{\"end\":39929,\"start\":39926},{\"end\":39942,\"start\":39939},{\"end\":39960,\"start\":39956},{\"end\":39977,\"start\":39968},{\"end\":40213,\"start\":40207},{\"end\":40226,\"start\":40223},{\"end\":40239,\"start\":40236},{\"end\":40250,\"start\":40247},{\"end\":40260,\"start\":40257},{\"end\":40269,\"start\":40266},{\"end\":40283,\"start\":40280},{\"end\":40331,\"start\":40329},{\"end\":40344,\"start\":40342},{\"end\":40355,\"start\":40353},{\"end\":40362,\"start\":40360},{\"end\":40373,\"start\":40369},{\"end\":40551,\"start\":40549},{\"end\":40563,\"start\":40561},{\"end\":40654,\"start\":40651},{\"end\":40668,\"start\":40664},{\"end\":40681,\"start\":40676},{\"end\":40693,\"start\":40688},{\"end\":40709,\"start\":40704},{\"end\":40722,\"start\":40718},{\"end\":40923,\"start\":40920},{\"end\":40936,\"start\":40932},{\"end\":40947,\"start\":40945},{\"end\":40963,\"start\":40958},{\"end\":40980,\"start\":40973},{\"end\":40995,\"start\":40989},{\"end\":41135,\"start\":41133},{\"end\":41148,\"start\":41141},{\"end\":41164,\"start\":41159},{\"end\":41182,\"start\":41176},{\"end\":41200,\"start\":41191},{\"end\":41247,\"start\":41244},{\"end\":41258,\"start\":41255},{\"end\":41272,\"start\":41264},{\"end\":41287,\"start\":41280},{\"end\":41299,\"start\":41294},{\"end\":41320,\"start\":41310},{\"end\":41338,\"start\":41327},{\"end\":41519,\"start\":41503},{\"end\":41536,\"start\":41531},{\"end\":41548,\"start\":41543},{\"end\":41565,\"start\":41557},{\"end\":41826,\"start\":41820},{\"end\":41839,\"start\":41832},{\"end\":41853,\"start\":41850},{\"end\":41867,\"start\":41860},{\"end\":41875,\"start\":41872},{\"end\":41886,\"start\":41884},{\"end\":42113,\"start\":42109},{\"end\":42136,\"start\":42125},{\"end\":42150,\"start\":42145},{\"end\":42164,\"start\":42160},{\"end\":42179,\"start\":42173},{\"end\":42356,\"start\":42348},{\"end\":42374,\"start\":42366},{\"end\":42391,\"start\":42386},{\"end\":42407,\"start\":42401},{\"end\":42558,\"start\":42550},{\"end\":42572,\"start\":42566},{\"end\":42583,\"start\":42579},{\"end\":42597,\"start\":42594},{\"end\":42877,\"start\":42872},{\"end\":42939,\"start\":42933},{\"end\":42955,\"start\":42948},{\"end\":42973,\"start\":42967},{\"end\":43141,\"start\":43134},{\"end\":43153,\"start\":43151},{\"end\":43166,\"start\":43161},{\"end\":43178,\"start\":43174},{\"end\":43192,\"start\":43186},{\"end\":43208,\"start\":43199},{\"end\":43341,\"start\":43330},{\"end\":43351,\"start\":43347},{\"end\":43364,\"start\":43362},{\"end\":43369,\"start\":43366},{\"end\":43633,\"start\":43620},{\"end\":43648,\"start\":43646},{\"end\":43665,\"start\":43657},{\"end\":43679,\"start\":43674},{\"end\":43876,\"start\":43870},{\"end\":43892,\"start\":43883},{\"end\":43901,\"start\":43899},{\"end\":43915,\"start\":43909},{\"end\":43938,\"start\":43931},{\"end\":43951,\"start\":43949},{\"end\":43970,\"start\":43965},{\"end\":44157,\"start\":44145},{\"end\":44173,\"start\":44167},{\"end\":44189,\"start\":44182},{\"end\":44210,\"start\":44202},{\"end\":44228,\"start\":44221},{\"end\":44246,\"start\":44239},{\"end\":44261,\"start\":44254},{\"end\":44273,\"start\":44268},{\"end\":44288,\"start\":44282},{\"end\":44295,\"start\":44290},{\"end\":44441,\"start\":44436},{\"end\":44458,\"start\":44450},{\"end\":44464,\"start\":44460},{\"end\":44705,\"start\":44701},{\"end\":44717,\"start\":44715},{\"end\":44732,\"start\":44728},{\"end\":44741,\"start\":44738},{\"end\":44751,\"start\":44746},{\"end\":44765,\"start\":44762},{\"end\":44789,\"start\":44773},{\"end\":44796,\"start\":44794},{\"end\":44988,\"start\":44985},{\"end\":44999,\"start\":44996},{\"end\":45007,\"start\":45004},{\"end\":45020,\"start\":45016},{\"end\":45035,\"start\":45029},{\"end\":45046,\"start\":45044},{\"end\":45059,\"start\":45055},{\"end\":45072,\"start\":45069},{\"end\":45082,\"start\":45077},{\"end\":45094,\"start\":45090},{\"end\":45142,\"start\":45140},{\"end\":45157,\"start\":45153},{\"end\":45170,\"start\":45168},{\"end\":45185,\"start\":45181},{\"end\":45344,\"start\":45333},{\"end\":45363,\"start\":45352},{\"end\":45376,\"start\":45371},{\"end\":45387,\"start\":45385},{\"end\":45506,\"start\":45502},{\"end\":45518,\"start\":45516},{\"end\":45533,\"start\":45529},{\"end\":45546,\"start\":45543},{\"end\":45558,\"start\":45553},{\"end\":45904,\"start\":45899},{\"end\":45916,\"start\":45912},{\"end\":45928,\"start\":45923},{\"end\":46072,\"start\":46068},{\"end\":46086,\"start\":46079},{\"end\":46096,\"start\":46092},{\"end\":46107,\"start\":46102},{\"end\":46121,\"start\":46116}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218971783},\"end\":38037,\"start\":37787},{\"attributes\":{\"doi\":\"arXiv:1803.11175\",\"id\":\"b1\"},\"end\":38246,\"start\":38039},{\"attributes\":{\"doi\":\"arXiv:2209.07661\",\"id\":\"b2\"},\"end\":38412,\"start\":38248},{\"attributes\":{\"id\":\"b3\"},\"end\":38805,\"start\":38414},{\"attributes\":{\"doi\":\"arXiv:2212.10559\",\"id\":\"b4\"},\"end\":39005,\"start\":38807},{\"attributes\":{\"doi\":\"arXiv:2301.00234\",\"id\":\"b5\"},\"end\":39178,\"start\":39007},{\"attributes\":{\"doi\":\"arXiv:1712.06751\",\"id\":\"b6\"},\"end\":39330,\"start\":39180},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4858173},\"end\":39527,\"start\":39332},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b8\"},\"end\":39664,\"start\":39529},{\"attributes\":{\"doi\":\"arXiv:1909.00986\",\"id\":\"b9\"},\"end\":39813,\"start\":39666},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":202539059},\"end\":40092,\"start\":39815},{\"attributes\":{\"doi\":\"arXiv:2206.08082\",\"id\":\"b11\"},\"end\":40319,\"start\":40094},{\"attributes\":{\"doi\":\"arXiv:1812.05271\",\"id\":\"b12\"},\"end\":40482,\"start\":40321},{\"attributes\":{\"doi\":\"arXiv:2004.09984\",\"id\":\"b13\"},\"end\":40640,\"start\":40484},{\"attributes\":{\"doi\":\"arXiv:2101.06804\",\"id\":\"b14\"},\"end\":40806,\"start\":40642},{\"attributes\":{\"id\":\"b15\"},\"end\":41027,\"start\":40808},{\"attributes\":{\"doi\":\"arXiv:2104.08786\",\"id\":\"b16\"},\"end\":41236,\"start\":41029},{\"attributes\":{\"doi\":\"arXiv:2202.12837\",\"id\":\"b17\"},\"end\":41451,\"start\":41238},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11558223},\"end\":41712,\"start\":41453},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220714040},\"end\":42097,\"start\":41714},{\"attributes\":{\"doi\":\"arXiv:1806.00692\",\"id\":\"b20\"},\"end\":42270,\"start\":42099},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12390290},\"end\":42476,\"start\":42272},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":11080756},\"end\":42764,\"start\":42478},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":56563878},\"end\":42924,\"start\":42766},{\"attributes\":{\"doi\":\"arXiv:1905.11268\",\"id\":\"b24\"},\"end\":43074,\"start\":42926},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":160025533},\"end\":43230,\"start\":43076},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1103\",\"id\":\"b26\",\"matched_paper_id\":196202909},\"end\":43612,\"start\":43232},{\"attributes\":{\"doi\":\"arXiv:2005.04118\",\"id\":\"b27\"},\"end\":43781,\"start\":43614},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":990233},\"end\":44135,\"start\":43783},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b29\"},\"end\":44385,\"start\":44137},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11465263},\"end\":44693,\"start\":44387},{\"attributes\":{\"doi\":\"arXiv:2111.02840\",\"id\":\"b31\"},\"end\":44918,\"start\":44695},{\"attributes\":{\"doi\":\"arXiv:2303.03846\",\"id\":\"b32\"},\"end\":45130,\"start\":44920},{\"attributes\":{\"doi\":\"arXiv:2212.10375\",\"id\":\"b33\"},\"end\":45256,\"start\":45132},{\"attributes\":{\"doi\":\"arXiv:2111.02080\",\"id\":\"b34\"},\"end\":45423,\"start\":45258},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.540\",\"id\":\"b35\",\"matched_paper_id\":261432085},\"end\":45827,\"start\":45425},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":368182},\"end\":45987,\"start\":45829},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":231979430},\"end\":46177,\"start\":45989}]", "bib_title": "[{\"end\":37824,\"start\":37787},{\"end\":38534,\"start\":38414},{\"end\":39417,\"start\":39332},{\"end\":39921,\"start\":39815},{\"end\":40910,\"start\":40808},{\"end\":41488,\"start\":41453},{\"end\":41813,\"start\":41714},{\"end\":42338,\"start\":42272},{\"end\":42540,\"start\":42478},{\"end\":42865,\"start\":42766},{\"end\":43127,\"start\":43076},{\"end\":43323,\"start\":43232},{\"end\":43860,\"start\":43783},{\"end\":44432,\"start\":44387},{\"end\":45495,\"start\":45425},{\"end\":45891,\"start\":45829},{\"end\":46060,\"start\":45989}]", "bib_author": "[{\"end\":37837,\"start\":37826},{\"end\":37852,\"start\":37837},{\"end\":37864,\"start\":37852},{\"end\":37881,\"start\":37864},{\"end\":37897,\"start\":37881},{\"end\":37916,\"start\":37897},{\"end\":37936,\"start\":37916},{\"end\":37950,\"start\":37936},{\"end\":37965,\"start\":37950},{\"end\":37980,\"start\":37965},{\"end\":38051,\"start\":38039},{\"end\":38064,\"start\":38051},{\"end\":38079,\"start\":38064},{\"end\":38088,\"start\":38079},{\"end\":38105,\"start\":38088},{\"end\":38121,\"start\":38105},{\"end\":38136,\"start\":38121},{\"end\":38161,\"start\":38136},{\"end\":38173,\"start\":38161},{\"end\":38184,\"start\":38173},{\"end\":38260,\"start\":38248},{\"end\":38271,\"start\":38260},{\"end\":38280,\"start\":38271},{\"end\":38298,\"start\":38280},{\"end\":38305,\"start\":38298},{\"end\":38552,\"start\":38536},{\"end\":38564,\"start\":38552},{\"end\":38572,\"start\":38564},{\"end\":38584,\"start\":38572},{\"end\":38597,\"start\":38584},{\"end\":38608,\"start\":38597},{\"end\":38623,\"start\":38608},{\"end\":38638,\"start\":38623},{\"end\":38654,\"start\":38638},{\"end\":38673,\"start\":38654},{\"end\":38685,\"start\":38673},{\"end\":38698,\"start\":38685},{\"end\":38918,\"start\":38907},{\"end\":38929,\"start\":38918},{\"end\":38938,\"start\":38929},{\"end\":38948,\"start\":38938},{\"end\":38961,\"start\":38948},{\"end\":38971,\"start\":38961},{\"end\":39021,\"start\":39007},{\"end\":39029,\"start\":39021},{\"end\":39040,\"start\":39029},{\"end\":39050,\"start\":39040},{\"end\":39062,\"start\":39050},{\"end\":39076,\"start\":39062},{\"end\":39084,\"start\":39076},{\"end\":39097,\"start\":39084},{\"end\":39110,\"start\":39097},{\"end\":39196,\"start\":39180},{\"end\":39206,\"start\":39196},{\"end\":39219,\"start\":39206},{\"end\":39231,\"start\":39219},{\"end\":39427,\"start\":39419},{\"end\":39444,\"start\":39427},{\"end\":39460,\"start\":39444},{\"end\":39471,\"start\":39460},{\"end\":39595,\"start\":39577},{\"end\":39612,\"start\":39595},{\"end\":39631,\"start\":39612},{\"end\":39677,\"start\":39666},{\"end\":39696,\"start\":39677},{\"end\":39710,\"start\":39696},{\"end\":39723,\"start\":39710},{\"end\":39931,\"start\":39923},{\"end\":39944,\"start\":39931},{\"end\":39962,\"start\":39944},{\"end\":39979,\"start\":39962},{\"end\":40215,\"start\":40202},{\"end\":40228,\"start\":40215},{\"end\":40241,\"start\":40228},{\"end\":40252,\"start\":40241},{\"end\":40262,\"start\":40252},{\"end\":40271,\"start\":40262},{\"end\":40285,\"start\":40271},{\"end\":40333,\"start\":40321},{\"end\":40346,\"start\":40333},{\"end\":40357,\"start\":40346},{\"end\":40364,\"start\":40357},{\"end\":40375,\"start\":40364},{\"end\":40553,\"start\":40541},{\"end\":40565,\"start\":40553},{\"end\":40656,\"start\":40642},{\"end\":40670,\"start\":40656},{\"end\":40683,\"start\":40670},{\"end\":40695,\"start\":40683},{\"end\":40711,\"start\":40695},{\"end\":40724,\"start\":40711},{\"end\":40925,\"start\":40912},{\"end\":40938,\"start\":40925},{\"end\":40949,\"start\":40938},{\"end\":40965,\"start\":40949},{\"end\":40982,\"start\":40965},{\"end\":40997,\"start\":40982},{\"end\":41137,\"start\":41129},{\"end\":41150,\"start\":41137},{\"end\":41166,\"start\":41150},{\"end\":41184,\"start\":41166},{\"end\":41202,\"start\":41184},{\"end\":41249,\"start\":41238},{\"end\":41260,\"start\":41249},{\"end\":41274,\"start\":41260},{\"end\":41289,\"start\":41274},{\"end\":41301,\"start\":41289},{\"end\":41322,\"start\":41301},{\"end\":41340,\"start\":41322},{\"end\":41521,\"start\":41490},{\"end\":41538,\"start\":41521},{\"end\":41550,\"start\":41538},{\"end\":41567,\"start\":41550},{\"end\":41828,\"start\":41815},{\"end\":41841,\"start\":41828},{\"end\":41855,\"start\":41841},{\"end\":41869,\"start\":41855},{\"end\":41877,\"start\":41869},{\"end\":41888,\"start\":41877},{\"end\":42115,\"start\":42099},{\"end\":42138,\"start\":42115},{\"end\":42152,\"start\":42138},{\"end\":42166,\"start\":42152},{\"end\":42181,\"start\":42166},{\"end\":42358,\"start\":42340},{\"end\":42376,\"start\":42358},{\"end\":42393,\"start\":42376},{\"end\":42409,\"start\":42393},{\"end\":42560,\"start\":42542},{\"end\":42574,\"start\":42560},{\"end\":42585,\"start\":42574},{\"end\":42599,\"start\":42585},{\"end\":42879,\"start\":42867},{\"end\":42941,\"start\":42926},{\"end\":42957,\"start\":42941},{\"end\":42975,\"start\":42957},{\"end\":43143,\"start\":43129},{\"end\":43155,\"start\":43143},{\"end\":43168,\"start\":43155},{\"end\":43180,\"start\":43168},{\"end\":43194,\"start\":43180},{\"end\":43210,\"start\":43194},{\"end\":43343,\"start\":43325},{\"end\":43353,\"start\":43343},{\"end\":43366,\"start\":43353},{\"end\":43371,\"start\":43366},{\"end\":43635,\"start\":43614},{\"end\":43650,\"start\":43635},{\"end\":43667,\"start\":43650},{\"end\":43681,\"start\":43667},{\"end\":43878,\"start\":43862},{\"end\":43894,\"start\":43878},{\"end\":43903,\"start\":43894},{\"end\":43917,\"start\":43903},{\"end\":43940,\"start\":43917},{\"end\":43953,\"start\":43940},{\"end\":43972,\"start\":43953},{\"end\":44159,\"start\":44137},{\"end\":44175,\"start\":44159},{\"end\":44191,\"start\":44175},{\"end\":44212,\"start\":44191},{\"end\":44230,\"start\":44212},{\"end\":44248,\"start\":44230},{\"end\":44263,\"start\":44248},{\"end\":44275,\"start\":44263},{\"end\":44290,\"start\":44275},{\"end\":44297,\"start\":44290},{\"end\":44443,\"start\":44434},{\"end\":44460,\"start\":44443},{\"end\":44466,\"start\":44460},{\"end\":44707,\"start\":44695},{\"end\":44719,\"start\":44707},{\"end\":44734,\"start\":44719},{\"end\":44743,\"start\":44734},{\"end\":44753,\"start\":44743},{\"end\":44767,\"start\":44753},{\"end\":44791,\"start\":44767},{\"end\":44798,\"start\":44791},{\"end\":44990,\"start\":44979},{\"end\":45001,\"start\":44990},{\"end\":45009,\"start\":45001},{\"end\":45022,\"start\":45009},{\"end\":45037,\"start\":45022},{\"end\":45048,\"start\":45037},{\"end\":45061,\"start\":45048},{\"end\":45074,\"start\":45061},{\"end\":45084,\"start\":45074},{\"end\":45096,\"start\":45084},{\"end\":45144,\"start\":45132},{\"end\":45159,\"start\":45144},{\"end\":45172,\"start\":45159},{\"end\":45187,\"start\":45172},{\"end\":45346,\"start\":45328},{\"end\":45365,\"start\":45346},{\"end\":45378,\"start\":45365},{\"end\":45389,\"start\":45378},{\"end\":45508,\"start\":45497},{\"end\":45520,\"start\":45508},{\"end\":45535,\"start\":45520},{\"end\":45548,\"start\":45535},{\"end\":45560,\"start\":45548},{\"end\":45906,\"start\":45893},{\"end\":45918,\"start\":45906},{\"end\":45930,\"start\":45918},{\"end\":46074,\"start\":46062},{\"end\":46088,\"start\":46074},{\"end\":46098,\"start\":46088},{\"end\":46109,\"start\":46098},{\"end\":46123,\"start\":46109}]", "bib_venue": "[{\"end\":40088,\"start\":40042},{\"end\":41708,\"start\":41646},{\"end\":42093,\"start\":41999},{\"end\":42760,\"start\":42688},{\"end\":43567,\"start\":43480},{\"end\":44131,\"start\":44060},{\"end\":44689,\"start\":44586},{\"end\":45750,\"start\":45678},{\"end\":38029,\"start\":37980},{\"end\":38226,\"start\":38200},{\"end\":38392,\"start\":38321},{\"end\":38734,\"start\":38698},{\"end\":38905,\"start\":38807},{\"end\":39158,\"start\":39126},{\"end\":39310,\"start\":39247},{\"end\":39517,\"start\":39471},{\"end\":39575,\"start\":39529},{\"end\":39793,\"start\":39739},{\"end\":40040,\"start\":39979},{\"end\":40200,\"start\":40094},{\"end\":40462,\"start\":40391},{\"end\":40539,\"start\":40484},{\"end\":40786,\"start\":40740},{\"end\":41018,\"start\":40997},{\"end\":41127,\"start\":41029},{\"end\":41431,\"start\":41356},{\"end\":41644,\"start\":41567},{\"end\":41997,\"start\":41888},{\"end\":42250,\"start\":42197},{\"end\":42466,\"start\":42409},{\"end\":42686,\"start\":42599},{\"end\":42915,\"start\":42879},{\"end\":43054,\"start\":42991},{\"end\":43221,\"start\":43210},{\"end\":43478,\"start\":43391},{\"end\":43761,\"start\":43697},{\"end\":44058,\"start\":43972},{\"end\":44365,\"start\":44313},{\"end\":44584,\"start\":44466},{\"end\":44898,\"start\":44814},{\"end\":44977,\"start\":44920},{\"end\":45236,\"start\":45203},{\"end\":45326,\"start\":45258},{\"end\":45676,\"start\":45589},{\"end\":45979,\"start\":45930},{\"end\":46167,\"start\":46123}]"}}}, "year": 2023, "month": 12, "day": 17}