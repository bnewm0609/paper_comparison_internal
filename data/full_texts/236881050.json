{"id": 236881050, "updated": "2023-10-06 00:40:28.801", "metadata": {"title": "EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation", "authors": "[{\"first\":\"Hsiao-Tzu\",\"last\":\"Hung\",\"middle\":[]},{\"first\":\"Joann\",\"last\":\"Ching\",\"middle\":[]},{\"first\":\"Seungheon\",\"last\":\"Doh\",\"middle\":[]},{\"first\":\"Nabin\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Juhan\",\"last\":\"Nam\",\"middle\":[]},{\"first\":\"Yi-Hsuan\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "ISMIR", "journal": "318-325", "publication_date": {"year": 2021, "month": 8, "day": 3}, "abstract": "While there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced `yee-m\\`{o}-pi-uh') dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the methodology for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2108.01374", "mag": "3187108986", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ismir/HungCDKNY21", "doi": "10.5281/zenodo.5090631"}}, "content": {"source": {"pdf_hash": "aa724a52eec835803cd379577eb9d923ef758564", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.01374v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9e8dc86f7ec9456bf8582a12be3353c884a7a12d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/aa724a52eec835803cd379577eb9d923ef758564.txt", "contents": "\nEMOPIA: A MULTI-MODAL POP PIANO DATASET FOR EMOTION RECOGNITION AND EMOTION-BASED MUSIC GENERATION\n\n\nHsiao-Tzu Hung \nAcademia Sinica\nTaiwan\n\nDepartment of CSIE\nNational Taiwan University\n\n\nJoann Ching \nAcademia Sinica\nTaiwan\n\nSeungheon Doh seungheondoh@kaist.ac.kr \nGraduate School of Culture Technology\nKAIST\nSouth Korea\n\nNabin Kim \nGeorgia Institute of Technology\nUnited States\n\nJuhan Nam \nGraduate School of Culture Technology\nKAIST\nSouth Korea\n\nYi-Hsuan Yang \nAcademia Sinica\nTaiwan\n\nEMOPIA: A MULTI-MODAL POP PIANO DATASET FOR EMOTION RECOGNITION AND EMOTION-BASED MUSIC GENERATION\n\nWhile there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced 'yee-m\u00f2-pi-uh') dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the procedure for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks.\n\nINTRODUCTION\n\nThe affective aspect of music has been a major subject of research in the field of music information retrieval (MIR), not only for music analysis and labeling [1][2][3][4][5][6][7][8][9], but also for music generation or editing [10][11][12][13][14]. Accordingly, there have been quite a few public music datasets with emotion, as listed in Table 1. These datasets are different in many ways, including the musical genres considered, data modality and data size, and the way emotion is described.\n\nWith the growing interest in symbolic-domain music analysis and generation in recent years of ISMIR [15][16][17][18][19], it is desirable to have an emotion-labeled symbolic music dataset to add emotion-related elements to such research. However, among the datasets listed in Table 1, only two provide MIDI data, and they are both small in size. Moreover, the majority of the audio-only datasets contain songs of multiple genres, making it hard to apply automatic music transcription algorithms, which currently work better for piano-only music [20][21][22][23], to get MIDI-like data from the audio recordings.\n\nTo address this need, we propose a new emotion-labeled dataset comprising of three main nice properties:\n\n\u2022 Single-instrument. We collect audio recordings of piano covers and creations from YouTube, with fair to high audio and musical quality, and a diverse set of playing styles and perceived emotions. Focusing on only piano music allows for the use of piano transcription algorithms [20,21], and facilitates disentanglement of musical composition from variations in timbre, arrangement, and other confounds seen in multi-instrument music.\n\n\u2022 Multi-modal. Both the audio and MIDI versions of the music pieces can be found from the Internet (see Section 3.5 for details). The MIDI files are automatically transcribed from the audio by a state-of-the-art model [21].\n\n\u2022 Clip-level annotation. The audio files downloaded from YouTube are full songs. As different parts of a song may convey different emotions, the first four authors of the paper manually and carefully pick emotion-consistent short clips from each song and label the emotion of these clips using a four-class taxonomy derived from the Russell's valence-arousal model [32]. This leads to cliplevel emotion annotations for in total 1,087 clips from 387 songs (i.e., 2.78 clips per song on average), with the number of clips per emotion class fairly balanced.\n\nGiven these properties, EMOPIA has versatile use cases in MIR research. For music labeling, EMOPIA can be used for clip-level music emotion recognition or music emotion variation detection [2], in both the audio and symbolic domains. For music generation, EMOPIA can be used for emotion-conditioned piano music generation or style transfer, to create emotion-controllable new compositions, or variations of existing pieces, again in both domains.\n\nWe present details of the dataset and the way we com-Name Label type Genre (or data source) Size Modality\n\nJamendo Moods [24] adjectives multiple genres 18,486 Audio DEAM [25] VA values (from FMA [26], Jamendo, MedleyDB [27]) 1,802 Audio EMO-Soundscapes [28] VA values (from FMA) 1,213 Audio CCMED-WCMED [29] VA values classical (both Western & Chinese) 800 Audio emoMusic [30] VA values pop, rock, classical, electronic 744 Audio EMusic [31] VA values experimental, 8 others 140 Audio MOODetector [6] adjectives multiple genres (AllMusic) 193 Audio+MIDI VGMIDI [10] valence video game 95 MIDI EMOPIA (ours) Russell's 4Q pop (piano covers) 1,078 Audio+MIDI Table 1. Comparison of some existing public emotion-labeled music datasets and the proposed EMOPIA dataset.\n\npile it in Section 3, and report a computational analysis of the dataset in Section 4. Moreover, to demonstrate the potential of the new dataset, we use it to train and evaluate a few clip-level emotion classification models using both audio and MIDI data in Section 5, and emotion-conditioned symbolic music generation models in Section 6. The latter involves the use of a recurrent neural network (RNN) model proposed by Ferreira et al. [10], and our own modification of a Transformer-based model [33][34][35] that takes emotion as a conditioning signal for generation. We release the EMOPIA dataset at a Zenodo repo. 1 Source code implementing the generation and classification models can be found on GitHub. 2 3 Examples of generated pieces can be found at a demo webpage. 4 \n\n\nRELATED WORK\n\nEmotion Recognition in Symbolic Music. Symbolic music representations describe music with the note, key, tempo, structure, chords, instruments. To understand the relationship between music and emotion, various researchers have investigated machine learning approaches with handcrafted features. Grekow et al. [36] extract in total 63 harmony, rhythmic, dynamic features from 83 classic music MIDI files. Lin et al. [37] compare audio, lyric, and MIDI features for music emotion recognition, finding that MIDI features lead to higher performance in valence dimension. Panda et al. [6] also proposed multi-model approaches, combining audio and MIDI features for emotion recognition using a small dataset of 193 songs.\n\n\nEmotion-conditioned Symbolic Music Generation.\n\nOnly few work has started to address this task recently. Ferreira et al. [10] compile a small dataset of video game MIDI tracks with manual annotations of valence values, named VGMIDI (cf. Table 1), and use it to train a long short term memory network (LSTM) in tandem with a genetic algorithm (GA) based elite selection mechanism to generate positive or negative music. Makris et al. [12] approach the same task by using designated chord progression sequence in a sequence-to-sequence architecture trained with the VGMIDI dataset. Zhao et al. [38] use LSTM to generate music with four different emotions. Madhok et al. [13] use human facial expressions as the condition to generate music. More recently, Tan & Herremans demonstrate that their FaderNets [16] can achieve arousal-conditioned symbolic music style transfer with a semi-supervised clustering method that learns the relation between high-level features and low-level representation. Their model modifies the emotion (specifically, only the arousal) of a music piece, instead of generating new pieces from scratch.\n\n\nTHE EMOPIA DATASET\n\n\nSong Selection and Segmentation\n\nEMOPIA is a collection of 387 piano solo performances of popular music segmented manually into 1,087 clips for emotion annotation. Two authors of the paper curated the song list of the piano performances by scanning through playlists on Spotify for its consistently high quality, then downloading the recordings from YouTube. A song is included when it is played by a professional conveying a clear emotion, and the recording has not been heavily engineered during post-production. The genres of songs include Japanese anime, Korean and Western pop song covers, movie soundtracks, and personal compositions.\n\nIn an effort to extend the usefulness of the dataset for future research, at the best of our ability, the songs are intentionally segmented (with the help of the Sonic Visualizer [40]) only at cadential arrivals to make it an emotionallyconsistent clip and a valid musical phrase at the same time. Accordingly, EMOPIA contains information for full songs, extracted phrases, and emotion labels.\n\n\nEmotion Annotation\n\nDifferent emotion taxonomies have been adopted in the literature for emotion annotation, with no standard so far [2]. For EMOPIA, we consider a simple four-class taxonomy corresponding to the four quadrants of the Russell's famous Circumplex model of affect [32], which conceptualizes emotions in a two-dimensional space defined by valence and arousal. The four classes are: HVHA (high valence high arousal); HVLA (high valence low arousal);   LVHA (low valence high arousal); and LVLA (low valence low arousal). We refer to this taxonomy as Russell's 4Q. As pointed out in the literature [2,3,41], various factors affect the perceived emotion of music, including cultural background, musical training, gender, etc. Consensus on the perception of emotion is challenging accordingly. Therefore, the annotations were made only among the first four authors, all coming from similar cultural backgrounds and collaborating closely during the annotation campaign, to ensure mutual standards for high or low valence/arousal. As it is time-consuming and laborious to choose clips from a song and label the emotion, each song was taken care of by only one annotator. Yet, cross validation of the annotations among the annotators was made several times during the annotation campaign (which spans 2.5 months), to ensure all annotators work on the same standard. Table 2 shows the number of clips and the average length (in seconds) for each class. The clips amount to approximately 11 hours' worth of data.\n\n\nTranscription\n\nWe transcribe the selected clips automatically with the help of the high-resolution piano transcription model proposed by Kong et al. [21], which is open source and represents the state-of-the-art for this task. We have manually checked the transcription result for a random set of clips and find the accuracy in note pitch, velocity, and duration satisfactory. The transcription might be fragmented and undesirable for cases such as when the audio recording is engineered to have unnatural ambient effects; we drop such songs from our collection. The model also transcribes pedal information, which we include to EMOPIA but do not use in our experiments.\n\n\nPre-processing and Encoding\n\nFor building machine learning models that deal with symbolic data, we need a data representation that can be used as input to the models. For example, MusicVAE [42] adopts the event-based representation that encodes a symbolic music piece as a sequence of \"event tokens\" such as note-on and note-off, while MuseGAN [43] employs a timestep-based, piano roll-like representation. Since there is no standard on the symbolic representation thus far, we adopt the following event-based ones in our experiments. Specifically, we use MIDI-like and REMI in Section 5 and CP in Section 6. See Figure 1 for illustrations.\n\n\u2022 The MIDI-like representation [39] encodes information regarding a MIDI note with a \"note-on\" token, a \"noteoff\", and a \"velocity\" token. Moreover, the \"time shift\" token is used to indicate the relative time gap (in ms) between two tokens.\n\n\u2022 REMI [34] considers a beat-based representation that instead uses \"bar\" and \"subbeat\" tokens to represent the time information. A \"bar\" token signifies the beginning of a new bar, and \"subbeat\" points to one of the constant number of subbeat divisions in a bar. Additionally, REMI uses \"tempo\" tokens to control the pace of the music, and replaces \"note-off\" with \"note-duration\". Table  2 also shows the average number of REMI tokens for clips in each emotion class.\n\n\u2022 CP [35]. Both MIDI-like and REMI view events as individual tokens. In CP, tokens belonging to the same family are grouped into a super token and placed on the same timestep (see Figure 1(c)). CP considers by default three families: metrical, note, and end-of-sequence. We additionally consider the \"emotion\" tokens and make it a new family, as depicted in Figure 1(d). The prepending approach is motivated by CTRL [44], a state-of-the-art controllable text generation model in natural language processing (NLP) that uses global tokens to affect some overall properties of a sequence.\n\n\nDataset Availability\n\nIn EMOPIA, each sample is accompanied with its corresponding metadata, segmentation annotations, emotion annotation, and transcribed MIDI, which are all available    Table 3. Jensen-Shannon divergence between the key histograms of a few emotion quadrant pairs.\n\nin the Zenodo repository. Moreover, we have added the MIDI data to the MusPy library [18] to facilitate its usage. Due to copyright issues, however, we can only share audio through YouTube links instead of sharing the audio files directl; the availability of the songs are subject to the copyright licenses in different countries and whether the owners will remove them.\n\n\nDATASET ANALYSIS\n\nThe emotion that listeners perceive is determined by a wide array of composed and performed features of music [45].\n\nTo observe the emotional correlate of the musical attributes in EMOPIA, we extract various MIDI-based features and examine the distributions over the four quadrants of emotion. We present here the most discriminative features among many choices we examined in our analysis.\n\nNote Density, Length, and Velocity. The arousal of music can be easily observed based on the frequency of note occurrences and their strength [46]. We measure them by note density, length and velocity. The note density is defined as the number of notes per beat, and the note length is defined as the average note length in beat unit. The note velocity is obtained directly from MIDI. Figure 2 shows three violin plots of the three features. The general trend shows that the high-arousal group (Q1, Q2) and lowarousal group (Q3, Q4) are distinguished well in all three plots. The results of note density and velocity are expected considering the nature of arousal. However, it is quite interesting that the note lengths are generally longer in the low-arousal group (Q3, Q4). Between the same-arousal quadrants, the differences are subtle. In note density, Q2 has more dynamics than Q1, whereas Q3 is not distinguishable from Q4. In note length, Q1 has slightly longer notes than those of Q2, whereas Q3 is again not distinguishable from Q4. In velocity, Q2 have louder notes than those of Q1, and Q3 has slightly louder notes than Q4.\n\nKey Distribution. The valence of music is often found to be related to the major-minor tonality [7]. For simplicity, we measure the tonality from the musical key. We extract the key information using the Krumhansl-Kessler algorithm [47] in the MIDI ToolBox [48]. Figure 3 shows the key distributions on 12 major-minor pitch classes for the four emotion quadrants. They assure that the majorminor tonality is an important clue in distinguishing valence. In the high valence group (Q1, Q4), the distribution is skewed to the left (major), while, in the low valence group (Q2, Q3), the trend is the opposite. We also measured the distance between the key distributions using the Jensen-Shannon divergence, which has the property of symmetry. Table 3 summarizes the pair-wise distances, indicating that the valence difference group has a larger difference than the arousal difference group.\n\n\nMUSIC EMOTION RECOGNITION\n\nWe report our baseline research on both symbolic-and audio-domain emotion recognition using EMOPIA, which defines the task as classifying a song into four categories. The clips in EMOPIA are divided into train-validation-test splits with the ratio of 7:2:1 in a stratified manner. Short-chunk ResNet [50] .677 .887 .704 Table 5. Audio-domain classification performance.\n\nlength, velocity, and represent the key as a one-hot vector.\n\nFor the latter, we use two different symbolic note representation methods introduced in Section 3.4, MIDI-like [39] and REMI [34]. For the learning model, we use the combination of bidirectional LSTM and a self-attention module, or LSTM-Attn for short, proposed originally for sentiment classification in NLP [49]. The LSTM extracts temporal information from the MIDI note events, while the selfattention module calculates different weight vectors over the LSTM hidden states with multi-head attentions. The weighted hidden states are finally used for classification.\n\nAudio-domain Classification. We evaluate two audiodomain classification methods in a similar manner to the symbolic-domain ones. In the first method, we use an average of 20 dimensions of mel-frequency cepstral coefficient (MFCC) vectors and a logistic regression classifier. In the second method, we use the short-chunk ResNet following [50], which is composed of 7-layer CNNs with residual connections. The output is summarized as a fixed 128-dimensional vector through max pooling, which is followed by two fully connected layers with the ReLU activation for classification. The input audio to the shortchunk ResNet is 3-second excerpts represented as a logscaled mel-spectrogram with 128 bins with 1024-size FFT (Hanning window), and 512 size hop at 22,050 Hz sampling rate. We randomly sample three seconds of the audio chunk as an input size to the classifier.\n\nEvaluation. We calculate 4-way classification accuracy over the four different emotion quadrants and 2-way classification accuracy over either arousal and valence. Tables  4 and 5 show the results in the symbolic and audio domains, respectively. Except for arousal, we can see that the deep learning approaches generally outperform the logistic regression classifiers using hand-crafted features. In audio domain arousal classification, MFCC vectors averaging the entire song sequence showed better performance than the deep learning approach with 3-second input. It seems that wider input sequence has the strength in emotion recognition. In both domains, valence classification is a more difficult task compared to arousal classification. For valence classification, MIDI-domain classifiers yield better result than audio-domain classifiers (0.883 vs. 0.704). Among the two token representations, MIDI-like seems to outperform REMI for valence classification.\n\n\nEMOTION-CONDITIONED GENERATION\n\nWe build the Transformer and LSTM models for emotionconditioned symbolic music generation using EMPOIA.\n\nFor the former, we adopt the Compound Word Transformer [35], the state-of-the-art in unconditioned symbolic music generation. We employ the CP+emotion representation presented in Section 3.4 as the data representation. For the LSTM model, we consider the approach proposed by Ferreira et al. [10], which represents the stateof-the-art in emotion-conditioned music generation. Our implementation follows that described in [10], with the following differences: 1) train on EMOPIA rather than VG-MIDI; 2) use 512 neurons instead of 4,096 due to our limited computational resource; 3) use the same linear logistic regression layer for classification but we classify four classes instead of two.\n\nAs the size of EMOPIA might not be big enough, we use additionally the AILabs1k7 dataset compiled by Hsiao et al. [35] to pre-train the Transformer. AILabs1k7 contains 1,748 samples and is also pop piano music, but it does not contain emotion labels. Most of the clips in AIL-abs1k7 are longer than EMOPIA, so to keep the consistency of the input sequence length, the length of the token sequence is set to be 1,024. We pre-train the Transformer with 1e-4 learning rate on AILabs1k7, take the checkpoint with negative log-likelihood loss 0.30, and then fine-tune it on EMOPIA with 1e-5 learning rate. During pre-training, the emotion token is always set to be \"ignore,\" while in fine-tuning it is set to the emotion of that sample.\n\nEvaluation. We use the following three sets of metrics.\n\n\u2022 Surface-level objective metrics. We use the following three metrics proposed by Dong et al. [43] to evaluate whether the generated samples fit the training data: pitch range (PR), number of unique pitch classes used (NPC), and number of notes being played concurrently (POLY). We use MusPy [18] to compute these metrics.\n\n\u2022 Emotion-related objective metrics. Since both REMI and CP adopt a beat-based representation of music, we employ the LSTM-Attn+REMI emotion classifier (cf. Section 5) here to quantify how well the generation result is influenced by the emotion condition. We first use the generative model to generate 100 samples per class, and use the assigned label as the target class of the sample. The trained classifier is then used to make prediction on the generated samples. Similar to the classification task, apart from 4Q classification, we also conduct 2-way classification of both Arousal and Valence aspect.\n\n\u2022 Subjective metrics. As the classifiers are not 100% accurate, we also resort to a user survey to evaluate the emotion-controllability of the models. Specifically, we deploy an online survey to collect responses to the music generated by different models. A subject has to lis-  Table 6. Performance comparison of the evaluated models for emotion-conditioned symbolic music generation in surfacelevel objective metrics (Pitch Range, Number of Pitch Classes used, and POLYphony; the closer to that of the real data the better), emotion-related objective metrics (4Q classification, Arousal classification, Valence classification; the higher the better), and subjective metrics (all in 1-5; the higher the better); bold font highlights the best result per metric.  ten to 12 random-generated samples, one for each of the three models and each of the four emotion classes, and rate them on a five-point Likert scale with respect to 1) Valence: is the audio negative or positive; 2) Arousal: is low or high in arousal; 3) Humanness: how well it sounds like a piece played by human; 4) Richness: is the content interesting; and, 5) Overall musical quality. In total 25 subjects participated in the survey. Table 6 tabulates some of the results. We see that the CP Transformer with ('w/') pre-training performs the best in most of the objective metrics and the three subjective metrics listed here. Nevertheless, the scores of the CP Transformer with pre-training in the three emotion-related objective metrics are much lower than that reported in Table 4, suggesting that either the generated pieces are not emotion-laden, or the generated pieces are too dissimilar to the real pieces to the classifier. Figure 4 shows the human assessment of the emotioncontrollability of the models. To our surprise, while the CP Transformer with pre-training does not score high in the emotion related objective metrics, the subjective test shows that it can actually control the emotion of the generated pieces to a certain extent, better than the two competing models. In particular, the valence of the samples generated by the Transformer with pre-training has a median rating of 4 when the goal is to generate positive-valence music (i.e., Figure 4(a)), while the scores of the other two models are around 3. Moreover, the arousal of the samples generated by the Transformer with pre-training has a median rating of 4 when the goal is to generate high-arousal music (Figure 4(c)), which is higher than that of the non pre-trained Transformer. This suggests that the LSTM-Attn classifier employed for computing the emotion-related objective metrics may not be reliable enough to predict the emotion perceived by human, and that the Transformer with pretraining is actually effective in controlling the emotion of the music it generates to certain extent. But, the model seems not good enough for the cases of generating lowvalence (i.e., negative) music, as shown in Figure 4(b).\n\n\nCONCLUSION\n\nIn this paper, we have proposed a new public dataset EMOPIA, a medium-scale emotion-labeled pop piano dataset. It is a multi-modal dataset that contains both the audio files and MIDI transcriptions of piano-only music, along with clip-level emotion annotations in four classes. We have also presented prototypes of models for clip-level music emotion classification and emotion-based symbolic music generation trained on this dataset, using a number of state-of-the-art models in respective tasks. The result shows that we are able to achieve high accuracy in both four-quadrant and valence-wise emotion classification, and that our Transformer-based model is capable of generating music with a given target emotion to a certain degree.\n\nIn the future, in-depth importance analysis can be conducted to figure out features that are important for emotion classification, and to seek ways to incorporate those features to the generation model. Many ideas can also be tried to further improve the performance of emotion conditioning, e.g., the Transformer-GAN approach [51].\n\nWe share not only the dataset itself but the code covering all our implemented models in a GitHub repo. We hope that researchers will find this contribution useful in future emotion-related MIR tasks.\n\n\u00a9\nH.T.Hung and J. Ching, and S.H Doh. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: H.T.Hung and J. Ching, and S.H Doh, \"EMOPIA: A Multimodal Pop Piano Dataset for Emotion Recognition and Emotion-based Music Generation\", in Proc. of the 22nd Int. Society for Music Information Retrieval Conf., Online, 2021.\n\nFigure 1 .\n1Illustration of different token-based representation for symbolic music: (a) MIDI-like[39], (b) REMI[34], and (d) CP[35] plus emotion token. Sub-figure (c) is an intermediate representation of the CP one.\n\nFigure 2 .\n2Violin plots of the distribution in (a) note density, (b) length, and (c) velocity for clips from different classes.\n\nFigure 3 .\n3Histogram of the keys (left / right: major / minor keys) for clips from different emotion classes.\n\nFigure 4 .\n4The subjective emotional scores (in 1-5) for the generative results when the target emotion is (a) high valence, (b) low valence, (c) high arousal, (d) low arousal.\n\n\nQuadrant # clips Avg. length (in sec / #tokens)Q1 \n250 \n31.9 / 1,065 \nQ2 \n265 \n35.6 / 1,368 \nQ3 \n253 \n40.6 / 771 \nQ4 \n310 \n38.2 / 729 \n\n\n\nTable 2 .\n2The number of clips and their average length in seconds, or in the number of REMI tokens, for each quadrant of the Russell's model in the EMOPIA dataset.\n\n\nQ1 vs. Q4 Q2 vs. Q3 Q1 vs. Q2 Q3 vs. Q4Label pair \nDifferent arousal \nDifferent valence \nJS Div. \n0.236 \n0.250 \n0.434 \n0.280 \n\n\n\nTable 4 .\n4Symbolic-domain classification performance.Symbolic-domain Classification. We evaluate two meth-\nods: one is based on hand-crafted features with a simple \nclassifier, and the other is on a deep neural network model. \nFor the former, we use the analysis features in the previous \nsection and a logistic regression classifier as a baseline. \nSpecifically, we use average values of note density, note \n\n\nhttps://zenodo.org/record/5090631 2 https://github.com/annahung31/EMOPIA 3 https://github.com/SeungHeonDoh/EMOPIA_cls 4 https://annahung31.github.io/EMOPIA/\n\nMusic emotion recognition: A state of the art review. Y Kim, E Schmidt, R Migneco, B Morton, P Richardson, J Scott, J Speck, D Turnbull, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfY. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richardson, J. Scott, J. Speck, and D. Turnbull, \"Mu- sic emotion recognition: A state of the art review,\" in Proc. Int. Society for Music Information Retrieval Conf., 2010.\n\nMusic Emotion Recognition. Y.-H Yang, H H Chen, CRC PressY.-H. Yang and H. H. Chen, Music Emotion Recogni- tion. CRC Press, 2011.\n\nJoyful for you and tender for us: the influence of individual characteristics and language on emotion labeling and classification. J S G\u00f3mez-Ca\u00f1\u00f3n, E Cano, P Herrera, E G\u00f3mez, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfJ. S. G\u00f3mez-Ca\u00f1\u00f3n, E. Cano, P. Herrera, and E. G\u00f3mez, \"Joyful for you and tender for us: the in- fluence of individual characteristics and language on emotion labeling and classification,\" in Proc. Int. Soci- ety for Music Information Retrieval Conf., 2020.\n\nLet's agree to disagree: Consensus entropy active learning for personalized music emotion recognition. J S G\u00f3mez-Ca\u00f1\u00f3n, E Cano, Y.-H Yang, P Herrera, E G\u00f3mez, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfJ. S. G\u00f3mez-Ca\u00f1\u00f3n, E. Cano, Y.-H. Yang, P. Herrera, and E. G\u00f3mez, \"Let's agree to disagree: Consensus en- tropy active learning for personalized music emotion recognition,\" in Proc. Int. Society for Music Informa- tion Retrieval Conf., 2021.\n\nThe multiple voices of musical emotions: Source separation for improving music emotion recognition models and their interpretability. J De Berardinis, A Cangelosi, E Coutinho, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfJ. de Berardinis, A. Cangelosi, and E. Coutinho, \"The multiple voices of musical emotions: Source separa- tion for improving music emotion recognition models and their interpretability,\" in Proc. Int. Society for Mu- sic Information Retrieval Conf., 2020.\n\nMulti-modal music emotion recognition: A new dataset, methodology and comparative analysis. R Panda, R Malheiro, B Rocha, A Oliveira, R P Paiva, Proc. Int. Symposium on Computer Music Multidisciplinary Research. Int. Symposium on Computer Music Multidisciplinary ResearchR. Panda, R. Malheiro, B. Rocha, A. Oliveira, and R. P. Paiva, \"Multi-modal music emotion recognition: A new dataset, methodology and comparative analy- sis,\" in Proc. Int. Symposium on Computer Music Mul- tidisciplinary Research, 2013.\n\nAudio features for music emotion recognition: A survey. R Panda, R M Malheiro, R P Paiva, IEEE Trans. Affective Computing. R. Panda, R. M. Malheiro, and R. P. Paiva, \"Audio fea- tures for music emotion recognition: A survey,\" IEEE Trans. Affective Computing, pp. 1-20, 2020.\n\nTowards explainable music emotion recognition: The route via mid-level features. S Chowdhury, A Vall, V Haunschmid, G Widmer, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfS. Chowdhury, A. Vall, V. Haunschmid, and G. Wid- mer, \"Towards explainable music emotion recognition: The route via mid-level features,\" in Proc. Int. Society for Music Information Retrieval Conf., 2019.\n\nOn the characterization of expressive performance in classical music: First results of the con espressione game. C Cancino-Chac\u00f3n, S Peter, S Chowdhury, A Aljanaki, G Widmer, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfC. Cancino-Chac\u00f3n, S. Peter, S. Chowdhury, A. Al- janaki, and G. Widmer, \"On the characterization of ex- pressive performance in classical music: First results of the con espressione game,\" in Proc. Int. Society for Music Information Retrieval Conf., 2020.\n\nLearning to generate music with sentiment. L N Ferreira, J Whitehead, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfL. N. Ferreira and J. Whitehead, \"Learning to generate music with sentiment,\" in Proc. Int. Society for Music Information Retrieval Conf., 2019.\n\nMusic SketchNet: Controllable music generation via factorized representations of pitch and rhythm. K Chen, C.-I Wang, T Berg-Kirkpatrick, S Dubnov, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfK. Chen, C.-I. Wang, T. Berg-Kirkpatrick, and S. Dub- nov, \"Music SketchNet: Controllable music generation via factorized representations of pitch and rhythm,\" in Proc. Int. Society for Music Information Retrieval Conf., 2020.\n\nGenerating lead sheets with affect: A novel conditional seq2seq framework. D Makris, K R Agres, D Herremans, Proc. Int. Joint Conf. Neural Networks. Int. Joint Conf. Neural Networks2021D. Makris, K. R. Agres, and D. Herremans, \"Gen- erating lead sheets with affect: A novel conditional seq2seq framework,\" in Proc. Int. Joint Conf. Neural Networks, 2021.\n\nSentiMozart: Music generation based on emotions. R Madhok, S Goel, S Garg, Proc. Int. Conf. Agents and Artificial Intelligence. Int. Conf. Agents and Artificial IntelligenceR. Madhok, S. Goel, and S. Garg, \"SentiMozart: Mu- sic generation based on emotions,\" in Proc. Int. Conf. Agents and Artificial Intelligence, 2018.\n\nEmoteControl: an interactive system for real-time control of emotional expression in music. A M Grimaud, T Eerola, Personal and Ubiquitous Computing. A. M. Grimaud and T. Eerola, \"EmoteControl: an in- teractive system for real-time control of emotional ex- pression in music,\" Personal and Ubiquitous Comput- ing, 2020.\n\nChord Jazzification: Learning Jazz interpretations of chord symbols. T.-P Chen, S Fukayama, M Goto, L Su, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfT.-P. Chen, S. Fukayama, M. Goto, and L. Su, \"Chord Jazzification: Learning Jazz interpretations of chord symbols,\" in Proc. Int. Society for Music Information Retrieval Conf., 2020.\n\nMusic Fadernets: Controllable music generation based on high-level features via low-level feature modelling. H H Tan, D Herremans, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfH. H. TAN and D. Herremans, \"Music Fadernets: Con- trollable music generation based on high-level features via low-level feature modelling,\" in Proc. Int. Society for Music Information Retrieval Conf., 2020.\n\nPOP909: A pop-song dataset for music arrangement generation. Z Wang, K Chen, J Jiang, Y Zhang, M Xu, S Dai, X Gu, G Xia, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfZ. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, X. Gu, and G. Xia, \"POP909: A pop-song dataset for music arrangement generation,\" in Proc. Int. Society for Music Information Retrieval Conf., 2020.\n\nMusPy: A toolkit for symbolic music generation. H.-W Dong, K Chen, J Mcauley, T Berg-Kirkpatrick, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfH.-W. Dong, K. Chen, J. McAuley, and T. Berg- Kirkpatrick, \"MusPy: A toolkit for symbolic music generation,\" in Proc. Int. Society for Music Informa- tion Retrieval Conf., 2020.\n\nThe MIDI Degradation Toolkit: Symbolic music augmentation and correction. A Mcleod, J Owers, K Yoshii, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfA. McLeod, J. Owers, and K. Yoshii, \"The MIDI Degradation Toolkit: Symbolic music augmentation and correction,\" in Proc. Int. Society for Music Infor- mation Retrieval Conf., 2020.\n\nOnsets and Frames: Dual-objective piano transcription. C Hawthorne, E Elsen, J Song, A Roberts, I Simon, C Raffel, J Engel, S Oore, D Eck, arXiv:1710.11153arXiv preprintC. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Si- mon, C. Raffel, J. Engel, S. Oore, and D. Eck, \"Onsets and Frames: Dual-objective piano transcription,\" arXiv preprint arXiv:1710.11153, 2018.\n\nHigh-resolution piano transcription with pedals by regressing onsets and offsets times. Q Kong, B Li, X Song, Y Wan, Y Wang, arXiv:2010.01815arXiv preprintQ. Kong, B. Li, X. Song, Y. Wan, and Y. Wang, \"High-resolution piano transcription with pedals by regressing onsets and offsets times,\" arXiv preprint arXiv:2010.01815, 2020.\n\nEnabling factorized piano music modeling and generation with the MAESTRO dataset. C Hawthorne, A Stasyuk, A Roberts, I Simon, C.-Z A Huang, S Dieleman, E Elsen, J Engel, D Eck, Proc. Int. Conf. Learning Representations. Int. Conf. Learning RepresentationsC. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.- Z. A. Huang, S. Dieleman, E. Elsen, J. Engel, and D. Eck, \"Enabling factorized piano music modeling and generation with the MAESTRO dataset,\" in Proc. Int. Conf. Learning Representations, 2019.\n\nReconVAT: A semi-supervised automatic music transcription framework for low-resource real-world data. K W Cheuk, D Herremans, L Su, Proc. ACM Multimedia. ACM Multimedia2021K. W. Cheuk, D. Herremans, and L. Su, \"ReconVAT: A semi-supervised automatic music transcription frame- work for low-resource real-world data,\" in Proc. ACM Multimedia, 2021.\n\nMediaeval 2019: Emotion and theme recognition in music using Jamendo. D Bognadov, A Porter, P Tovstogan, M Won, WorkshopD. Bognadov, A. Porter, P. Tovstogan, and M. Won, \"Mediaeval 2019: Emotion and theme recognition in music using Jamendo,\" in MediaEval 2019 Workshop.\n\nBenchmarking music emotion recognition systems. A Alajanki, Y.-H Yang, M Soleymani, PLOS ONE. A. Alajanki, Y.-H. Yang, and M. Soleymani, \"Bench- marking music emotion recognition systems,\" PLOS ONE, 2016.\n\nFMA: A dataset for music analysis. M Defferrard, K Benzi, P Vandergheynst, X Bresson, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfM. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, \"FMA: A dataset for music analysis,\" in Proc. Int. Society for Music Information Retrieval Conf., 2017.\n\nMedleyDB: A multitrack dataset for annotation-intensive mir research. R Bittner, J Salamon, M Tierney, M Mauch, C Cannam, J Bello, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfR. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can- nam, and J. Bello, \"MedleyDB: A multitrack dataset for annotation-intensive mir research,\" in Proc. Int. So- ciety for Music Information Retrieval Conf., 2014.\n\nEmo-Soundscapes: A dataset for soundscape emotion recognition. J Fan, M Thorogood, P Pasquier, Proc. Int. Conf. Affective Computing and Intelligent Interaction. Int. Conf. Affective Computing and Intelligent InteractionJ. Fan, M. Thorogood, and P. Pasquier, \"Emo- Soundscapes: A dataset for soundscape emotion recog- nition,\" in Proc. Int. Conf. Affective Computing and In- telligent Interaction, 2017.\n\nA comparative study of Western and Chinese classical music based on soundscape models. J Fan, Y.-H Yang, K Dong, P Pasquier, Proc. Int. Conf. Acoustics, Speech, and Signal Processing. Int. Conf. Acoustics, Speech, and Signal essingJ. Fan, Y.-H. Yang, K. Dong, and P. Pasquier, \"A com- parative study of Western and Chinese classical mu- sic based on soundscape models,\" in Proc. Int. Conf. Acoustics, Speech, and Signal Processing, 2020.\n\n1000 songs for emotional analysis of music. M Soleymani, M Caro, E Schmidt, C.-Y Sha, Y.-H Yang, Proc. ACM Int. Workshop on Crowdsourcing for Multimedia. ACM Int. Workshop on Crowdsourcing for MultimediaM. Soleymani, M. Caro, E. Schmidt, C.-Y. Sha, and Y.-H. Yang, \"1000 songs for emotional analysis of mu- sic,\" in Proc. ACM Int. Workshop on Crowdsourcing for Multimedia, 2013.\n\nRanking-based emotion recognition for experimental music. J Fan, K Tatar, M Thorogood, P Pasquier, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfJ. Fan, K. Tatar, M. Thorogood, and P. Pasquier, \"Ranking-based emotion recognition for experimental music,\" in Proc. Int. Society for Music Information Re- trieval Conf., 2017.\n\nA circumplex model of affect. J Russell, Journal of Personality and Social Psychology. 39J. Russell, \"A circumplex model of affect,\" Journal of Personality and Social Psychology, vol. 39, pp. 1161- 1178, December 1980.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing SystemsA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Proc. Advances in Neu- ral Information Processing Systems, 2017.\n\nPop Music Transformer: Beat-based modeling and generation of expressive pop piano compositions. Y.-S Huang, Y.-H Yang, Proc. ACM Multimedia. ACM MultimediaY.-S. Huang and Y.-H. Yang, \"Pop Music Transformer: Beat-based modeling and generation of expressive pop piano compositions,\" in Proc. ACM Multimedia, 2020.\n\nCompound Word Transformer: Learning to compose full-song music over dynamic directed hypergraphs. W.-Y Hsiao, J.-Y Liu, Y.-C Yeh, Y.-H Yang, Proc. AAAI Conf. Artificial Intelligence. AAAI Conf. Artificial Intelligence2021W.-Y. Hsiao, J.-Y. Liu, Y.-C. Yeh, and Y.-H. Yang, \"Compound Word Transformer: Learning to compose full-song music over dynamic directed hypergraphs,\" in Proc. AAAI Conf. Artificial Intelligence, 2021.\n\nDetecting emotions in classical music from MIDI files. J Grekow, Z W Ra\u015b, Proc. Int. Symposium on Methodologies for Intelligent Systems. Int. Symposium on Methodologies for Intelligent SystemsJ. Grekow and Z. W. Ra\u015b, \"Detecting emotions in clas- sical music from MIDI files,\" in Proc. Int. Symposium on Methodologies for Intelligent Systems, 2009.\n\nExploration of music emotion recognition based on MIDI. Y Lin, X Chen, D Yang, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval ConfY. Lin, X. Chen, and D. Yang, \"Exploration of music emotion recognition based on MIDI,\" in Proc. Int. So- ciety for Music Information Retrieval Conf., 2013.\n\nAn emotional symbolic music generation system based on LSTM networks. K Zhao, S Li, J Cai, H Wang, J Wang, Proc. IEEE Information Technology, Networking, Electronic and Automation Control Conf. IEEE Information Technology, Networking, Electronic and Automation Control ConfK. Zhao, S. Li, J. Cai, H. Wang, and J. Wang, \"An emotional symbolic music generation system based on LSTM networks,\" in Proc. IEEE Information Technol- ogy, Networking, Electronic and Automation Control Conf., 2019.\n\nThis time with feeling: Learning expressive musical performance. S Oore, I Simon, S Dieleman, D Eck, K Simonyan, Neural Computing and Applications. S. Oore, I. Simon, S. Dieleman, D. Eck, and K. Si- monyan, \"This time with feeling: Learning expressive musical performance,\" Neural Computing and Appli- cations, 2018.\n\nSonic Visualiser: An open source application for viewing, analysing, and annotating music audio files. C Cannam, C Landone, M Sandler, Proc. ACM Multimedia. ACM MultimediaC. Cannam, C. Landone, and M. Sandler, \"Sonic Vi- sualiser: An open source application for viewing, analysing, and annotating music audio files,\" in Proc. ACM Multimedia, 2010, pp. 1467-1468, https://www. sonicvisualiser.org/.\n\nA cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues. L.-L Balkwill, W Thompson, Music Perception. 17L.-L. Balkwill and W. Thompson, \"A cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues,\" Music Perception, vol. 17, pp. 43-64, October 1999.\n\nA hierarchical latent vector model for learning long-term structure in music. A Roberts, J Engel, C Raffel, C Hawthorne, D Eck, Proc. Int. Conf. Machine Learning. Int. Conf. Machine LearningA. Roberts, J. Engel, C. Raffel, C. Hawthorne, and D. Eck, \"A hierarchical latent vector model for learn- ing long-term structure in music,\" in Proc. Int. Conf. Machine Learning, 2018.\n\nMuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. H.-W Dong, W.-Y Hsiao, L.-C Yang, Y.-H Yang, Proc. AAAI Conf. Artificial Intelligence. AAAI Conf. Artificial IntelligenceH.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, \"MuseGAN: Multi-track sequential generative adver- sarial networks for symbolic music generation and ac- companiment,\" in Proc. AAAI Conf. Artificial Intelli- gence, 2018.\n\nCTRL -a conditional Transformer language model for controllable generation. N S Keskar, arXiv:1909.05858arXiv preprintN. S. Keskar et al., \"CTRL -a conditional Trans- former language model for controllable generation,\" arXiv preprint arXiv:1909.05858, 2019.\n\nMusical expression of emotions: Modelling listeners' judgements of composed and performed features. P Juslin, E Lindstorm, Music Analysis. 29P. Juslin and E. Lindstorm, \"Musical expression of emotions: Modelling listeners' judgements of composed and performed features,\" Music Analysis, vol. 29, pp. 334 -364, 2011.\n\nChanging musical emotion: A computational rule system for modifying score and performance. S R Livingstone, R Muhlberger, A R Brown, W F Thompson, Computer Music Journal. 341S. R. Livingstone, R. Muhlberger, A. R. Brown, and W. F. Thompson, \"Changing musical emotion: A com- putational rule system for modifying score and perfor- mance,\" Computer Music Journal, vol. 34, no. 1, pp. 41-64, 2010.\n\nCognitive foundations of musical pitch. C L Krumhansl, Oxford University PressC. L. Krumhansl, Cognitive foundations of musical pitch. Oxford University Press, 2001.\n\n. T Eerola, P Toiviainen, Toolbox, MATLAB Tools for Music Research. Jyv\u00e4skyl\u00e4, Finland: University of Jyv\u00e4skyl\u00e4T. Eerola and P. Toiviainen, MIDI Toolbox: MATLAB Tools for Music Research. Jyv\u00e4skyl\u00e4, Finland: University of Jyv\u00e4skyl\u00e4, 2004. [Online]. Available: www.jyu.fi/musica/miditoolbox/\n\nA structured self-attentive sentence embedding. Z Lin, M Feng, C N Santos, M Yu, B Xiang, B Zhou, Y Bengio, Proc. Int. Conf. Learning Representations. Int. Conf. Learning RepresentationsZ. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, \"A structured self-attentive sentence embedding,\" in Proc. Int. Conf. Learning Representations, 2017.\n\nEvaluation of CNN-based automatic music tagging models. M Won, A Ferraro, D Bogdanov, X Serra, Proc. Sound and Music Conf. Sound and Music ConfM. Won, A. Ferraro, D. Bogdanov, and X. Serra, \"Eval- uation of CNN-based automatic music tagging mod- els,\" in Proc. Sound and Music Conf., 2020.\n\nSymbolic music generation with Transformer-GANs. A Muhamed, Proc. AAAI Conf. Artificial Intelligence. AAAI Conf. Artificial IntelligenceA. Muhamed et al., \"Symbolic music generation with Transformer-GANs,\" in Proc. AAAI Conf. Artificial In- telligence, 2021.\n", "annotations": {"author": "[{\"end\":189,\"start\":102},{\"end\":226,\"start\":190},{\"end\":323,\"start\":227},{\"end\":381,\"start\":324},{\"end\":449,\"start\":382},{\"end\":488,\"start\":450}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":112},{\"end\":201,\"start\":196},{\"end\":240,\"start\":237},{\"end\":333,\"start\":330},{\"end\":391,\"start\":388},{\"end\":463,\"start\":459}]", "author_first_name": "[{\"end\":111,\"start\":102},{\"end\":195,\"start\":190},{\"end\":236,\"start\":227},{\"end\":329,\"start\":324},{\"end\":387,\"start\":382},{\"end\":458,\"start\":450}]", "author_affiliation": "[{\"end\":140,\"start\":118},{\"end\":188,\"start\":142},{\"end\":225,\"start\":203},{\"end\":322,\"start\":267},{\"end\":380,\"start\":335},{\"end\":448,\"start\":393},{\"end\":487,\"start\":465}]", "title": "[{\"end\":99,\"start\":1},{\"end\":587,\"start\":489}]", "venue": null, "abstract": "[{\"end\":1691,\"start\":589}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1869,\"start\":1866},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1872,\"start\":1869},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1875,\"start\":1872},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1878,\"start\":1875},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1881,\"start\":1878},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1884,\"start\":1881},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1887,\"start\":1884},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1890,\"start\":1887},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1893,\"start\":1890},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1940,\"start\":1936},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1944,\"start\":1940},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1948,\"start\":1944},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1952,\"start\":1948},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1956,\"start\":1952},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2309,\"start\":2305},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2313,\"start\":2309},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2317,\"start\":2313},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2321,\"start\":2317},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2325,\"start\":2321},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2754,\"start\":2750},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2758,\"start\":2754},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2762,\"start\":2758},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2766,\"start\":2762},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3208,\"start\":3204},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3211,\"start\":3208},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3583,\"start\":3579},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3955,\"start\":3951},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4334,\"start\":4331},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4715,\"start\":4711},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4765,\"start\":4761},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4790,\"start\":4786},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4814,\"start\":4810},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4848,\"start\":4844},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4898,\"start\":4894},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4967,\"start\":4963},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5032,\"start\":5028},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5091,\"start\":5088},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5156,\"start\":5152},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5799,\"start\":5795},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5859,\"start\":5855},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5863,\"start\":5859},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5867,\"start\":5863},{\"end\":6071,\"start\":6068},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6134,\"start\":6133},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6465,\"start\":6461},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6571,\"start\":6567},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6735,\"start\":6732},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6995,\"start\":6991},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7307,\"start\":7303},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7466,\"start\":7462},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7542,\"start\":7538},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7676,\"start\":7672},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8842,\"start\":8838},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9191,\"start\":9188},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9337,\"start\":9333},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9667,\"start\":9664},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9669,\"start\":9667},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9672,\"start\":9669},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10727,\"start\":10723},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11440,\"start\":11436},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11595,\"start\":11591},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11924,\"start\":11920},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12143,\"start\":12139},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12612,\"start\":12608},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13023,\"start\":13019},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13564,\"start\":13560},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13980,\"start\":13976},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14404,\"start\":14400},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15494,\"start\":15491},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15631,\"start\":15627},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15656,\"start\":15652},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16615,\"start\":16611},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16859,\"start\":16855},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16873,\"start\":16869},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":17057,\"start\":17053},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17655,\"start\":17651},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19341,\"start\":19337},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19578,\"start\":19574},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19707,\"start\":19703},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20092,\"start\":20088},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20862,\"start\":20858},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21060,\"start\":21056},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25744,\"start\":25740},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26412,\"start\":26408},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26426,\"start\":26422},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26442,\"start\":26438}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26308,\"start\":25948},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26526,\"start\":26309},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26656,\"start\":26527},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26768,\"start\":26657},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26946,\"start\":26769},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27085,\"start\":26947},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27251,\"start\":27086},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27381,\"start\":27252},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27794,\"start\":27382}]", "paragraph": "[{\"end\":2203,\"start\":1707},{\"end\":2816,\"start\":2205},{\"end\":2922,\"start\":2818},{\"end\":3359,\"start\":2924},{\"end\":3584,\"start\":3361},{\"end\":4140,\"start\":3586},{\"end\":4588,\"start\":4142},{\"end\":4695,\"start\":4590},{\"end\":5354,\"start\":4697},{\"end\":6135,\"start\":5356},{\"end\":6867,\"start\":6152},{\"end\":7993,\"start\":6918},{\"end\":8657,\"start\":8050},{\"end\":9052,\"start\":8659},{\"end\":10571,\"start\":9075},{\"end\":11244,\"start\":10589},{\"end\":11887,\"start\":11276},{\"end\":12130,\"start\":11889},{\"end\":12601,\"start\":12132},{\"end\":13188,\"start\":12603},{\"end\":13473,\"start\":13213},{\"end\":13845,\"start\":13475},{\"end\":13981,\"start\":13866},{\"end\":14256,\"start\":13983},{\"end\":15393,\"start\":14258},{\"end\":16281,\"start\":15395},{\"end\":16680,\"start\":16311},{\"end\":16742,\"start\":16682},{\"end\":17311,\"start\":16744},{\"end\":18179,\"start\":17313},{\"end\":19142,\"start\":18181},{\"end\":19280,\"start\":19177},{\"end\":19972,\"start\":19282},{\"end\":20705,\"start\":19974},{\"end\":20762,\"start\":20707},{\"end\":21086,\"start\":20764},{\"end\":21694,\"start\":21088},{\"end\":24660,\"start\":21696},{\"end\":25411,\"start\":24675},{\"end\":25745,\"start\":25413},{\"end\":25947,\"start\":25747}]", "formula": null, "table_ref": "[{\"end\":2055,\"start\":2048},{\"end\":2488,\"start\":2481},{\"end\":5254,\"start\":5247},{\"end\":7115,\"start\":7107},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10434,\"start\":10427},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12523,\"start\":12515},{\"end\":13386,\"start\":13379},{\"end\":16141,\"start\":16134},{\"end\":16638,\"start\":16631},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18360,\"start\":18345},{\"end\":21983,\"start\":21976},{\"end\":22905,\"start\":22898}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1705,\"start\":1693},{\"attributes\":{\"n\":\"2.\"},\"end\":6150,\"start\":6138},{\"end\":6916,\"start\":6870},{\"attributes\":{\"n\":\"3.\"},\"end\":8014,\"start\":7996},{\"attributes\":{\"n\":\"3.1\"},\"end\":8048,\"start\":8017},{\"attributes\":{\"n\":\"3.2\"},\"end\":9073,\"start\":9055},{\"attributes\":{\"n\":\"3.3\"},\"end\":10587,\"start\":10574},{\"attributes\":{\"n\":\"3.4\"},\"end\":11274,\"start\":11247},{\"attributes\":{\"n\":\"3.5\"},\"end\":13211,\"start\":13191},{\"attributes\":{\"n\":\"4.\"},\"end\":13864,\"start\":13848},{\"attributes\":{\"n\":\"5.\"},\"end\":16309,\"start\":16284},{\"attributes\":{\"n\":\"6.\"},\"end\":19175,\"start\":19145},{\"attributes\":{\"n\":\"7.\"},\"end\":24673,\"start\":24663},{\"end\":25950,\"start\":25949},{\"end\":26320,\"start\":26310},{\"end\":26538,\"start\":26528},{\"end\":26668,\"start\":26658},{\"end\":26780,\"start\":26770},{\"end\":27096,\"start\":27087},{\"end\":27392,\"start\":27383}]", "table": "[{\"end\":27085,\"start\":26996},{\"end\":27381,\"start\":27293},{\"end\":27794,\"start\":27437}]", "figure_caption": "[{\"end\":26308,\"start\":25951},{\"end\":26526,\"start\":26322},{\"end\":26656,\"start\":26540},{\"end\":26768,\"start\":26670},{\"end\":26946,\"start\":26782},{\"end\":26996,\"start\":26949},{\"end\":27251,\"start\":27098},{\"end\":27293,\"start\":27254},{\"end\":27437,\"start\":27394}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11868,\"start\":11860},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12791,\"start\":12783},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12972,\"start\":12961},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14651,\"start\":14643},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15666,\"start\":15658},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23404,\"start\":23396},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23930,\"start\":23922},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24160,\"start\":24148},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24656,\"start\":24648}]", "bib_author_first_name": "[{\"end\":28008,\"start\":28007},{\"end\":28015,\"start\":28014},{\"end\":28026,\"start\":28025},{\"end\":28037,\"start\":28036},{\"end\":28047,\"start\":28046},{\"end\":28061,\"start\":28060},{\"end\":28070,\"start\":28069},{\"end\":28079,\"start\":28078},{\"end\":28448,\"start\":28444},{\"end\":28456,\"start\":28455},{\"end\":28458,\"start\":28457},{\"end\":28680,\"start\":28679},{\"end\":28682,\"start\":28681},{\"end\":28697,\"start\":28696},{\"end\":28705,\"start\":28704},{\"end\":28716,\"start\":28715},{\"end\":29193,\"start\":29192},{\"end\":29195,\"start\":29194},{\"end\":29210,\"start\":29209},{\"end\":29221,\"start\":29217},{\"end\":29229,\"start\":29228},{\"end\":29240,\"start\":29239},{\"end\":29732,\"start\":29731},{\"end\":29749,\"start\":29748},{\"end\":29762,\"start\":29761},{\"end\":30229,\"start\":30228},{\"end\":30238,\"start\":30237},{\"end\":30250,\"start\":30249},{\"end\":30259,\"start\":30258},{\"end\":30271,\"start\":30270},{\"end\":30273,\"start\":30272},{\"end\":30702,\"start\":30701},{\"end\":30711,\"start\":30710},{\"end\":30713,\"start\":30712},{\"end\":30725,\"start\":30724},{\"end\":30727,\"start\":30726},{\"end\":31003,\"start\":31002},{\"end\":31016,\"start\":31015},{\"end\":31024,\"start\":31023},{\"end\":31038,\"start\":31037},{\"end\":31473,\"start\":31472},{\"end\":31491,\"start\":31490},{\"end\":31500,\"start\":31499},{\"end\":31513,\"start\":31512},{\"end\":31525,\"start\":31524},{\"end\":31942,\"start\":31941},{\"end\":31944,\"start\":31943},{\"end\":31956,\"start\":31955},{\"end\":32320,\"start\":32319},{\"end\":32331,\"start\":32327},{\"end\":32339,\"start\":32338},{\"end\":32359,\"start\":32358},{\"end\":32778,\"start\":32777},{\"end\":32788,\"start\":32787},{\"end\":32790,\"start\":32789},{\"end\":32799,\"start\":32798},{\"end\":33108,\"start\":33107},{\"end\":33118,\"start\":33117},{\"end\":33126,\"start\":33125},{\"end\":33473,\"start\":33472},{\"end\":33475,\"start\":33474},{\"end\":33486,\"start\":33485},{\"end\":33774,\"start\":33770},{\"end\":33782,\"start\":33781},{\"end\":33794,\"start\":33793},{\"end\":33802,\"start\":33801},{\"end\":34207,\"start\":34206},{\"end\":34209,\"start\":34208},{\"end\":34216,\"start\":34215},{\"end\":34605,\"start\":34604},{\"end\":34613,\"start\":34612},{\"end\":34621,\"start\":34620},{\"end\":34630,\"start\":34629},{\"end\":34639,\"start\":34638},{\"end\":34645,\"start\":34644},{\"end\":34652,\"start\":34651},{\"end\":34658,\"start\":34657},{\"end\":35025,\"start\":35021},{\"end\":35033,\"start\":35032},{\"end\":35041,\"start\":35040},{\"end\":35052,\"start\":35051},{\"end\":35431,\"start\":35430},{\"end\":35441,\"start\":35440},{\"end\":35450,\"start\":35449},{\"end\":35803,\"start\":35802},{\"end\":35816,\"start\":35815},{\"end\":35825,\"start\":35824},{\"end\":35833,\"start\":35832},{\"end\":35844,\"start\":35843},{\"end\":35853,\"start\":35852},{\"end\":35863,\"start\":35862},{\"end\":35872,\"start\":35871},{\"end\":35880,\"start\":35879},{\"end\":36201,\"start\":36200},{\"end\":36209,\"start\":36208},{\"end\":36215,\"start\":36214},{\"end\":36223,\"start\":36222},{\"end\":36230,\"start\":36229},{\"end\":36526,\"start\":36525},{\"end\":36539,\"start\":36538},{\"end\":36550,\"start\":36549},{\"end\":36561,\"start\":36560},{\"end\":36573,\"start\":36569},{\"end\":36575,\"start\":36574},{\"end\":36584,\"start\":36583},{\"end\":36596,\"start\":36595},{\"end\":36605,\"start\":36604},{\"end\":36614,\"start\":36613},{\"end\":37048,\"start\":37047},{\"end\":37050,\"start\":37049},{\"end\":37059,\"start\":37058},{\"end\":37072,\"start\":37071},{\"end\":37364,\"start\":37363},{\"end\":37376,\"start\":37375},{\"end\":37386,\"start\":37385},{\"end\":37399,\"start\":37398},{\"end\":37613,\"start\":37612},{\"end\":37628,\"start\":37624},{\"end\":37636,\"start\":37635},{\"end\":37806,\"start\":37805},{\"end\":37820,\"start\":37819},{\"end\":37829,\"start\":37828},{\"end\":37846,\"start\":37845},{\"end\":38197,\"start\":38196},{\"end\":38208,\"start\":38207},{\"end\":38219,\"start\":38218},{\"end\":38230,\"start\":38229},{\"end\":38239,\"start\":38238},{\"end\":38249,\"start\":38248},{\"end\":38642,\"start\":38641},{\"end\":38649,\"start\":38648},{\"end\":38662,\"start\":38661},{\"end\":39070,\"start\":39069},{\"end\":39080,\"start\":39076},{\"end\":39088,\"start\":39087},{\"end\":39096,\"start\":39095},{\"end\":39466,\"start\":39465},{\"end\":39479,\"start\":39478},{\"end\":39487,\"start\":39486},{\"end\":39501,\"start\":39497},{\"end\":39511,\"start\":39507},{\"end\":39860,\"start\":39859},{\"end\":39867,\"start\":39866},{\"end\":39876,\"start\":39875},{\"end\":39889,\"start\":39888},{\"end\":40216,\"start\":40215},{\"end\":40433,\"start\":40432},{\"end\":40444,\"start\":40443},{\"end\":40455,\"start\":40454},{\"end\":40465,\"start\":40464},{\"end\":40478,\"start\":40477},{\"end\":40487,\"start\":40486},{\"end\":40489,\"start\":40488},{\"end\":40498,\"start\":40497},{\"end\":40500,\"start\":40499},{\"end\":40510,\"start\":40509},{\"end\":40928,\"start\":40924},{\"end\":40940,\"start\":40936},{\"end\":41243,\"start\":41239},{\"end\":41255,\"start\":41251},{\"end\":41265,\"start\":41261},{\"end\":41275,\"start\":41271},{\"end\":41621,\"start\":41620},{\"end\":41631,\"start\":41630},{\"end\":41633,\"start\":41632},{\"end\":41971,\"start\":41970},{\"end\":41978,\"start\":41977},{\"end\":41986,\"start\":41985},{\"end\":42328,\"start\":42327},{\"end\":42336,\"start\":42335},{\"end\":42342,\"start\":42341},{\"end\":42349,\"start\":42348},{\"end\":42357,\"start\":42356},{\"end\":42814,\"start\":42813},{\"end\":42822,\"start\":42821},{\"end\":42831,\"start\":42830},{\"end\":42843,\"start\":42842},{\"end\":42850,\"start\":42849},{\"end\":43170,\"start\":43169},{\"end\":43180,\"start\":43179},{\"end\":43191,\"start\":43190},{\"end\":43573,\"start\":43569},{\"end\":43585,\"start\":43584},{\"end\":43886,\"start\":43885},{\"end\":43897,\"start\":43896},{\"end\":43906,\"start\":43905},{\"end\":43916,\"start\":43915},{\"end\":43929,\"start\":43928},{\"end\":44300,\"start\":44296},{\"end\":44311,\"start\":44307},{\"end\":44323,\"start\":44319},{\"end\":44334,\"start\":44330},{\"end\":44720,\"start\":44719},{\"end\":44722,\"start\":44721},{\"end\":45003,\"start\":45002},{\"end\":45013,\"start\":45012},{\"end\":45311,\"start\":45310},{\"end\":45313,\"start\":45312},{\"end\":45328,\"start\":45327},{\"end\":45342,\"start\":45341},{\"end\":45344,\"start\":45343},{\"end\":45353,\"start\":45352},{\"end\":45355,\"start\":45354},{\"end\":45656,\"start\":45655},{\"end\":45658,\"start\":45657},{\"end\":45785,\"start\":45784},{\"end\":45795,\"start\":45794},{\"end\":46122,\"start\":46121},{\"end\":46129,\"start\":46128},{\"end\":46137,\"start\":46136},{\"end\":46139,\"start\":46138},{\"end\":46149,\"start\":46148},{\"end\":46155,\"start\":46154},{\"end\":46164,\"start\":46163},{\"end\":46172,\"start\":46171},{\"end\":46494,\"start\":46493},{\"end\":46501,\"start\":46500},{\"end\":46512,\"start\":46511},{\"end\":46524,\"start\":46523},{\"end\":46778,\"start\":46777}]", "bib_author_last_name": "[{\"end\":28012,\"start\":28009},{\"end\":28023,\"start\":28016},{\"end\":28034,\"start\":28027},{\"end\":28044,\"start\":28038},{\"end\":28058,\"start\":28048},{\"end\":28067,\"start\":28062},{\"end\":28076,\"start\":28071},{\"end\":28088,\"start\":28080},{\"end\":28453,\"start\":28449},{\"end\":28463,\"start\":28459},{\"end\":28694,\"start\":28683},{\"end\":28702,\"start\":28698},{\"end\":28713,\"start\":28706},{\"end\":28722,\"start\":28717},{\"end\":29207,\"start\":29196},{\"end\":29215,\"start\":29211},{\"end\":29226,\"start\":29222},{\"end\":29237,\"start\":29230},{\"end\":29246,\"start\":29241},{\"end\":29746,\"start\":29733},{\"end\":29759,\"start\":29750},{\"end\":29771,\"start\":29763},{\"end\":30235,\"start\":30230},{\"end\":30247,\"start\":30239},{\"end\":30256,\"start\":30251},{\"end\":30268,\"start\":30260},{\"end\":30279,\"start\":30274},{\"end\":30708,\"start\":30703},{\"end\":30722,\"start\":30714},{\"end\":30733,\"start\":30728},{\"end\":31013,\"start\":31004},{\"end\":31021,\"start\":31017},{\"end\":31035,\"start\":31025},{\"end\":31045,\"start\":31039},{\"end\":31488,\"start\":31474},{\"end\":31497,\"start\":31492},{\"end\":31510,\"start\":31501},{\"end\":31522,\"start\":31514},{\"end\":31532,\"start\":31526},{\"end\":31953,\"start\":31945},{\"end\":31966,\"start\":31957},{\"end\":32325,\"start\":32321},{\"end\":32336,\"start\":32332},{\"end\":32356,\"start\":32340},{\"end\":32366,\"start\":32360},{\"end\":32785,\"start\":32779},{\"end\":32796,\"start\":32791},{\"end\":32809,\"start\":32800},{\"end\":33115,\"start\":33109},{\"end\":33123,\"start\":33119},{\"end\":33131,\"start\":33127},{\"end\":33483,\"start\":33476},{\"end\":33493,\"start\":33487},{\"end\":33779,\"start\":33775},{\"end\":33791,\"start\":33783},{\"end\":33799,\"start\":33795},{\"end\":33805,\"start\":33803},{\"end\":34213,\"start\":34210},{\"end\":34226,\"start\":34217},{\"end\":34610,\"start\":34606},{\"end\":34618,\"start\":34614},{\"end\":34627,\"start\":34622},{\"end\":34636,\"start\":34631},{\"end\":34642,\"start\":34640},{\"end\":34649,\"start\":34646},{\"end\":34655,\"start\":34653},{\"end\":34662,\"start\":34659},{\"end\":35030,\"start\":35026},{\"end\":35038,\"start\":35034},{\"end\":35049,\"start\":35042},{\"end\":35069,\"start\":35053},{\"end\":35438,\"start\":35432},{\"end\":35447,\"start\":35442},{\"end\":35457,\"start\":35451},{\"end\":35813,\"start\":35804},{\"end\":35822,\"start\":35817},{\"end\":35830,\"start\":35826},{\"end\":35841,\"start\":35834},{\"end\":35850,\"start\":35845},{\"end\":35860,\"start\":35854},{\"end\":35869,\"start\":35864},{\"end\":35877,\"start\":35873},{\"end\":35884,\"start\":35881},{\"end\":36206,\"start\":36202},{\"end\":36212,\"start\":36210},{\"end\":36220,\"start\":36216},{\"end\":36227,\"start\":36224},{\"end\":36235,\"start\":36231},{\"end\":36536,\"start\":36527},{\"end\":36547,\"start\":36540},{\"end\":36558,\"start\":36551},{\"end\":36567,\"start\":36562},{\"end\":36581,\"start\":36576},{\"end\":36593,\"start\":36585},{\"end\":36602,\"start\":36597},{\"end\":36611,\"start\":36606},{\"end\":36618,\"start\":36615},{\"end\":37056,\"start\":37051},{\"end\":37069,\"start\":37060},{\"end\":37075,\"start\":37073},{\"end\":37373,\"start\":37365},{\"end\":37383,\"start\":37377},{\"end\":37396,\"start\":37387},{\"end\":37403,\"start\":37400},{\"end\":37622,\"start\":37614},{\"end\":37633,\"start\":37629},{\"end\":37646,\"start\":37637},{\"end\":37817,\"start\":37807},{\"end\":37826,\"start\":37821},{\"end\":37843,\"start\":37830},{\"end\":37854,\"start\":37847},{\"end\":38205,\"start\":38198},{\"end\":38216,\"start\":38209},{\"end\":38227,\"start\":38220},{\"end\":38236,\"start\":38231},{\"end\":38246,\"start\":38240},{\"end\":38255,\"start\":38250},{\"end\":38646,\"start\":38643},{\"end\":38659,\"start\":38650},{\"end\":38671,\"start\":38663},{\"end\":39074,\"start\":39071},{\"end\":39085,\"start\":39081},{\"end\":39093,\"start\":39089},{\"end\":39105,\"start\":39097},{\"end\":39476,\"start\":39467},{\"end\":39484,\"start\":39480},{\"end\":39495,\"start\":39488},{\"end\":39505,\"start\":39502},{\"end\":39516,\"start\":39512},{\"end\":39864,\"start\":39861},{\"end\":39873,\"start\":39868},{\"end\":39886,\"start\":39877},{\"end\":39898,\"start\":39890},{\"end\":40224,\"start\":40217},{\"end\":40441,\"start\":40434},{\"end\":40452,\"start\":40445},{\"end\":40462,\"start\":40456},{\"end\":40475,\"start\":40466},{\"end\":40484,\"start\":40479},{\"end\":40495,\"start\":40490},{\"end\":40507,\"start\":40501},{\"end\":40521,\"start\":40511},{\"end\":40934,\"start\":40929},{\"end\":40945,\"start\":40941},{\"end\":41249,\"start\":41244},{\"end\":41259,\"start\":41256},{\"end\":41269,\"start\":41266},{\"end\":41280,\"start\":41276},{\"end\":41628,\"start\":41622},{\"end\":41637,\"start\":41634},{\"end\":41975,\"start\":41972},{\"end\":41983,\"start\":41979},{\"end\":41991,\"start\":41987},{\"end\":42333,\"start\":42329},{\"end\":42339,\"start\":42337},{\"end\":42346,\"start\":42343},{\"end\":42354,\"start\":42350},{\"end\":42362,\"start\":42358},{\"end\":42819,\"start\":42815},{\"end\":42828,\"start\":42823},{\"end\":42840,\"start\":42832},{\"end\":42847,\"start\":42844},{\"end\":42859,\"start\":42851},{\"end\":43177,\"start\":43171},{\"end\":43188,\"start\":43181},{\"end\":43199,\"start\":43192},{\"end\":43582,\"start\":43574},{\"end\":43594,\"start\":43586},{\"end\":43894,\"start\":43887},{\"end\":43903,\"start\":43898},{\"end\":43913,\"start\":43907},{\"end\":43926,\"start\":43917},{\"end\":43933,\"start\":43930},{\"end\":44305,\"start\":44301},{\"end\":44317,\"start\":44312},{\"end\":44328,\"start\":44324},{\"end\":44339,\"start\":44335},{\"end\":44729,\"start\":44723},{\"end\":45010,\"start\":45004},{\"end\":45023,\"start\":45014},{\"end\":45325,\"start\":45314},{\"end\":45339,\"start\":45329},{\"end\":45350,\"start\":45345},{\"end\":45364,\"start\":45356},{\"end\":45668,\"start\":45659},{\"end\":45792,\"start\":45786},{\"end\":45806,\"start\":45796},{\"end\":45815,\"start\":45808},{\"end\":46126,\"start\":46123},{\"end\":46134,\"start\":46130},{\"end\":46146,\"start\":46140},{\"end\":46152,\"start\":46150},{\"end\":46161,\"start\":46156},{\"end\":46169,\"start\":46165},{\"end\":46179,\"start\":46173},{\"end\":46498,\"start\":46495},{\"end\":46509,\"start\":46502},{\"end\":46521,\"start\":46513},{\"end\":46530,\"start\":46525},{\"end\":46786,\"start\":46779}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":147936492},\"end\":28415,\"start\":27953},{\"attributes\":{\"id\":\"b1\"},\"end\":28546,\"start\":28417},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":228928895},\"end\":29087,\"start\":28548},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":244524240},\"end\":29595,\"start\":29089},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":236096810},\"end\":30134,\"start\":29597},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":163157292},\"end\":30643,\"start\":30136},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":226245041},\"end\":30919,\"start\":30645},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195833034},\"end\":31357,\"start\":30921},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220968955},\"end\":31896,\"start\":31359},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":208334359},\"end\":32218,\"start\":31898},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220961759},\"end\":32700,\"start\":32220},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":233407945},\"end\":33056,\"start\":32702},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6637931},\"end\":33378,\"start\":33058},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":214783194},\"end\":33699,\"start\":33380},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235251353},\"end\":34095,\"start\":33701},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220870781},\"end\":34541,\"start\":34097},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":221140193},\"end\":34971,\"start\":34543},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":220969049},\"end\":35354,\"start\":34973},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":222090453},\"end\":35745,\"start\":35356},{\"attributes\":{\"doi\":\"arXiv:1710.11153\",\"id\":\"b19\"},\"end\":36110,\"start\":35747},{\"attributes\":{\"doi\":\"arXiv:2010.01815\",\"id\":\"b20\"},\"end\":36441,\"start\":36112},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53094405},\"end\":36943,\"start\":36443},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235795484},\"end\":37291,\"start\":36945},{\"attributes\":{\"id\":\"b23\"},\"end\":37562,\"start\":37293},{\"attributes\":{\"id\":\"b24\"},\"end\":37768,\"start\":37564},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1768692},\"end\":38124,\"start\":37770},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":424574},\"end\":38576,\"start\":38126},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3428340},\"end\":38980,\"start\":38578},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":211252756},\"end\":39419,\"start\":38982},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15438733},\"end\":39799,\"start\":39421},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":23128406},\"end\":40183,\"start\":39801},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":145278842},\"end\":40403,\"start\":40185},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13756489},\"end\":40826,\"start\":40405},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":220919638},\"end\":41139,\"start\":40828},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":230799404},\"end\":41563,\"start\":41141},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":19555571},\"end\":41912,\"start\":41565},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":17981614},\"end\":42255,\"start\":41914},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":174819818},\"end\":42746,\"start\":42257},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":51980304},\"end\":43064,\"start\":42748},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":19570078},\"end\":43463,\"start\":43066},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":12151228},\"end\":43805,\"start\":43465},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3891811},\"end\":44181,\"start\":43807},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":19098155},\"end\":44641,\"start\":44183},{\"attributes\":{\"doi\":\"arXiv:1909.05858\",\"id\":\"b43\"},\"end\":44900,\"start\":44643},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":142916847},\"end\":45217,\"start\":44902},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":17591825},\"end\":45613,\"start\":45219},{\"attributes\":{\"id\":\"b46\"},\"end\":45780,\"start\":45615},{\"attributes\":{\"id\":\"b47\"},\"end\":46071,\"start\":45782},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":15280949},\"end\":46435,\"start\":46073},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":219176558},\"end\":46726,\"start\":46437},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":229368261},\"end\":46986,\"start\":46728}]", "bib_title": "[{\"end\":28005,\"start\":27953},{\"end\":28677,\"start\":28548},{\"end\":29190,\"start\":29089},{\"end\":29729,\"start\":29597},{\"end\":30226,\"start\":30136},{\"end\":30699,\"start\":30645},{\"end\":31000,\"start\":30921},{\"end\":31470,\"start\":31359},{\"end\":31939,\"start\":31898},{\"end\":32317,\"start\":32220},{\"end\":32775,\"start\":32702},{\"end\":33105,\"start\":33058},{\"end\":33470,\"start\":33380},{\"end\":33768,\"start\":33701},{\"end\":34204,\"start\":34097},{\"end\":34602,\"start\":34543},{\"end\":35019,\"start\":34973},{\"end\":35428,\"start\":35356},{\"end\":36523,\"start\":36443},{\"end\":37045,\"start\":36945},{\"end\":37610,\"start\":37564},{\"end\":37803,\"start\":37770},{\"end\":38194,\"start\":38126},{\"end\":38639,\"start\":38578},{\"end\":39067,\"start\":38982},{\"end\":39463,\"start\":39421},{\"end\":39857,\"start\":39801},{\"end\":40213,\"start\":40185},{\"end\":40430,\"start\":40405},{\"end\":40922,\"start\":40828},{\"end\":41237,\"start\":41141},{\"end\":41618,\"start\":41565},{\"end\":41968,\"start\":41914},{\"end\":42325,\"start\":42257},{\"end\":42811,\"start\":42748},{\"end\":43167,\"start\":43066},{\"end\":43567,\"start\":43465},{\"end\":43883,\"start\":43807},{\"end\":44294,\"start\":44183},{\"end\":45000,\"start\":44902},{\"end\":45308,\"start\":45219},{\"end\":46119,\"start\":46073},{\"end\":46491,\"start\":46437},{\"end\":46775,\"start\":46728}]", "bib_author": "[{\"end\":28014,\"start\":28007},{\"end\":28025,\"start\":28014},{\"end\":28036,\"start\":28025},{\"end\":28046,\"start\":28036},{\"end\":28060,\"start\":28046},{\"end\":28069,\"start\":28060},{\"end\":28078,\"start\":28069},{\"end\":28090,\"start\":28078},{\"end\":28455,\"start\":28444},{\"end\":28465,\"start\":28455},{\"end\":28696,\"start\":28679},{\"end\":28704,\"start\":28696},{\"end\":28715,\"start\":28704},{\"end\":28724,\"start\":28715},{\"end\":29209,\"start\":29192},{\"end\":29217,\"start\":29209},{\"end\":29228,\"start\":29217},{\"end\":29239,\"start\":29228},{\"end\":29248,\"start\":29239},{\"end\":29748,\"start\":29731},{\"end\":29761,\"start\":29748},{\"end\":29773,\"start\":29761},{\"end\":30237,\"start\":30228},{\"end\":30249,\"start\":30237},{\"end\":30258,\"start\":30249},{\"end\":30270,\"start\":30258},{\"end\":30281,\"start\":30270},{\"end\":30710,\"start\":30701},{\"end\":30724,\"start\":30710},{\"end\":30735,\"start\":30724},{\"end\":31015,\"start\":31002},{\"end\":31023,\"start\":31015},{\"end\":31037,\"start\":31023},{\"end\":31047,\"start\":31037},{\"end\":31490,\"start\":31472},{\"end\":31499,\"start\":31490},{\"end\":31512,\"start\":31499},{\"end\":31524,\"start\":31512},{\"end\":31534,\"start\":31524},{\"end\":31955,\"start\":31941},{\"end\":31968,\"start\":31955},{\"end\":32327,\"start\":32319},{\"end\":32338,\"start\":32327},{\"end\":32358,\"start\":32338},{\"end\":32368,\"start\":32358},{\"end\":32787,\"start\":32777},{\"end\":32798,\"start\":32787},{\"end\":32811,\"start\":32798},{\"end\":33117,\"start\":33107},{\"end\":33125,\"start\":33117},{\"end\":33133,\"start\":33125},{\"end\":33485,\"start\":33472},{\"end\":33495,\"start\":33485},{\"end\":33781,\"start\":33770},{\"end\":33793,\"start\":33781},{\"end\":33801,\"start\":33793},{\"end\":33807,\"start\":33801},{\"end\":34215,\"start\":34206},{\"end\":34228,\"start\":34215},{\"end\":34612,\"start\":34604},{\"end\":34620,\"start\":34612},{\"end\":34629,\"start\":34620},{\"end\":34638,\"start\":34629},{\"end\":34644,\"start\":34638},{\"end\":34651,\"start\":34644},{\"end\":34657,\"start\":34651},{\"end\":34664,\"start\":34657},{\"end\":35032,\"start\":35021},{\"end\":35040,\"start\":35032},{\"end\":35051,\"start\":35040},{\"end\":35071,\"start\":35051},{\"end\":35440,\"start\":35430},{\"end\":35449,\"start\":35440},{\"end\":35459,\"start\":35449},{\"end\":35815,\"start\":35802},{\"end\":35824,\"start\":35815},{\"end\":35832,\"start\":35824},{\"end\":35843,\"start\":35832},{\"end\":35852,\"start\":35843},{\"end\":35862,\"start\":35852},{\"end\":35871,\"start\":35862},{\"end\":35879,\"start\":35871},{\"end\":35886,\"start\":35879},{\"end\":36208,\"start\":36200},{\"end\":36214,\"start\":36208},{\"end\":36222,\"start\":36214},{\"end\":36229,\"start\":36222},{\"end\":36237,\"start\":36229},{\"end\":36538,\"start\":36525},{\"end\":36549,\"start\":36538},{\"end\":36560,\"start\":36549},{\"end\":36569,\"start\":36560},{\"end\":36583,\"start\":36569},{\"end\":36595,\"start\":36583},{\"end\":36604,\"start\":36595},{\"end\":36613,\"start\":36604},{\"end\":36620,\"start\":36613},{\"end\":37058,\"start\":37047},{\"end\":37071,\"start\":37058},{\"end\":37077,\"start\":37071},{\"end\":37375,\"start\":37363},{\"end\":37385,\"start\":37375},{\"end\":37398,\"start\":37385},{\"end\":37405,\"start\":37398},{\"end\":37624,\"start\":37612},{\"end\":37635,\"start\":37624},{\"end\":37648,\"start\":37635},{\"end\":37819,\"start\":37805},{\"end\":37828,\"start\":37819},{\"end\":37845,\"start\":37828},{\"end\":37856,\"start\":37845},{\"end\":38207,\"start\":38196},{\"end\":38218,\"start\":38207},{\"end\":38229,\"start\":38218},{\"end\":38238,\"start\":38229},{\"end\":38248,\"start\":38238},{\"end\":38257,\"start\":38248},{\"end\":38648,\"start\":38641},{\"end\":38661,\"start\":38648},{\"end\":38673,\"start\":38661},{\"end\":39076,\"start\":39069},{\"end\":39087,\"start\":39076},{\"end\":39095,\"start\":39087},{\"end\":39107,\"start\":39095},{\"end\":39478,\"start\":39465},{\"end\":39486,\"start\":39478},{\"end\":39497,\"start\":39486},{\"end\":39507,\"start\":39497},{\"end\":39518,\"start\":39507},{\"end\":39866,\"start\":39859},{\"end\":39875,\"start\":39866},{\"end\":39888,\"start\":39875},{\"end\":39900,\"start\":39888},{\"end\":40226,\"start\":40215},{\"end\":40443,\"start\":40432},{\"end\":40454,\"start\":40443},{\"end\":40464,\"start\":40454},{\"end\":40477,\"start\":40464},{\"end\":40486,\"start\":40477},{\"end\":40497,\"start\":40486},{\"end\":40509,\"start\":40497},{\"end\":40523,\"start\":40509},{\"end\":40936,\"start\":40924},{\"end\":40947,\"start\":40936},{\"end\":41251,\"start\":41239},{\"end\":41261,\"start\":41251},{\"end\":41271,\"start\":41261},{\"end\":41282,\"start\":41271},{\"end\":41630,\"start\":41620},{\"end\":41639,\"start\":41630},{\"end\":41977,\"start\":41970},{\"end\":41985,\"start\":41977},{\"end\":41993,\"start\":41985},{\"end\":42335,\"start\":42327},{\"end\":42341,\"start\":42335},{\"end\":42348,\"start\":42341},{\"end\":42356,\"start\":42348},{\"end\":42364,\"start\":42356},{\"end\":42821,\"start\":42813},{\"end\":42830,\"start\":42821},{\"end\":42842,\"start\":42830},{\"end\":42849,\"start\":42842},{\"end\":42861,\"start\":42849},{\"end\":43179,\"start\":43169},{\"end\":43190,\"start\":43179},{\"end\":43201,\"start\":43190},{\"end\":43584,\"start\":43569},{\"end\":43596,\"start\":43584},{\"end\":43896,\"start\":43885},{\"end\":43905,\"start\":43896},{\"end\":43915,\"start\":43905},{\"end\":43928,\"start\":43915},{\"end\":43935,\"start\":43928},{\"end\":44307,\"start\":44296},{\"end\":44319,\"start\":44307},{\"end\":44330,\"start\":44319},{\"end\":44341,\"start\":44330},{\"end\":44731,\"start\":44719},{\"end\":45012,\"start\":45002},{\"end\":45025,\"start\":45012},{\"end\":45327,\"start\":45310},{\"end\":45341,\"start\":45327},{\"end\":45352,\"start\":45341},{\"end\":45366,\"start\":45352},{\"end\":45670,\"start\":45655},{\"end\":45794,\"start\":45784},{\"end\":45808,\"start\":45794},{\"end\":45817,\"start\":45808},{\"end\":46128,\"start\":46121},{\"end\":46136,\"start\":46128},{\"end\":46148,\"start\":46136},{\"end\":46154,\"start\":46148},{\"end\":46163,\"start\":46154},{\"end\":46171,\"start\":46163},{\"end\":46181,\"start\":46171},{\"end\":46500,\"start\":46493},{\"end\":46511,\"start\":46500},{\"end\":46523,\"start\":46511},{\"end\":46532,\"start\":46523},{\"end\":46788,\"start\":46777}]", "bib_venue": "[{\"end\":28145,\"start\":28090},{\"end\":28442,\"start\":28417},{\"end\":28779,\"start\":28724},{\"end\":29303,\"start\":29248},{\"end\":29828,\"start\":29773},{\"end\":30346,\"start\":30281},{\"end\":30766,\"start\":30735},{\"end\":31102,\"start\":31047},{\"end\":31589,\"start\":31534},{\"end\":32023,\"start\":31968},{\"end\":32423,\"start\":32368},{\"end\":32849,\"start\":32811},{\"end\":33184,\"start\":33133},{\"end\":33528,\"start\":33495},{\"end\":33862,\"start\":33807},{\"end\":34283,\"start\":34228},{\"end\":34719,\"start\":34664},{\"end\":35126,\"start\":35071},{\"end\":35514,\"start\":35459},{\"end\":35800,\"start\":35747},{\"end\":36198,\"start\":36112},{\"end\":36661,\"start\":36620},{\"end\":37097,\"start\":37077},{\"end\":37361,\"start\":37293},{\"end\":37656,\"start\":37648},{\"end\":37911,\"start\":37856},{\"end\":38312,\"start\":38257},{\"end\":38737,\"start\":38673},{\"end\":39164,\"start\":39107},{\"end\":39573,\"start\":39518},{\"end\":39955,\"start\":39900},{\"end\":40270,\"start\":40226},{\"end\":40578,\"start\":40523},{\"end\":40967,\"start\":40947},{\"end\":41322,\"start\":41282},{\"end\":41700,\"start\":41639},{\"end\":42048,\"start\":41993},{\"end\":42449,\"start\":42364},{\"end\":42894,\"start\":42861},{\"end\":43221,\"start\":43201},{\"end\":43612,\"start\":43596},{\"end\":43968,\"start\":43935},{\"end\":44381,\"start\":44341},{\"end\":44717,\"start\":44643},{\"end\":45039,\"start\":45025},{\"end\":45388,\"start\":45366},{\"end\":45653,\"start\":45615},{\"end\":46222,\"start\":46181},{\"end\":46558,\"start\":46532},{\"end\":46828,\"start\":46788},{\"end\":28196,\"start\":28147},{\"end\":28830,\"start\":28781},{\"end\":29354,\"start\":29305},{\"end\":29879,\"start\":29830},{\"end\":30407,\"start\":30348},{\"end\":31153,\"start\":31104},{\"end\":31640,\"start\":31591},{\"end\":32074,\"start\":32025},{\"end\":32474,\"start\":32425},{\"end\":32883,\"start\":32851},{\"end\":33231,\"start\":33186},{\"end\":33913,\"start\":33864},{\"end\":34334,\"start\":34285},{\"end\":34770,\"start\":34721},{\"end\":35177,\"start\":35128},{\"end\":35565,\"start\":35516},{\"end\":36698,\"start\":36663},{\"end\":37113,\"start\":37099},{\"end\":37962,\"start\":37913},{\"end\":38363,\"start\":38314},{\"end\":38797,\"start\":38739},{\"end\":39213,\"start\":39166},{\"end\":39624,\"start\":39575},{\"end\":40006,\"start\":39957},{\"end\":40625,\"start\":40580},{\"end\":40983,\"start\":40969},{\"end\":41358,\"start\":41324},{\"end\":41757,\"start\":41702},{\"end\":42099,\"start\":42050},{\"end\":42530,\"start\":42451},{\"end\":43237,\"start\":43223},{\"end\":43997,\"start\":43970},{\"end\":44417,\"start\":44383},{\"end\":46259,\"start\":46224},{\"end\":46580,\"start\":46560},{\"end\":46864,\"start\":46830}]"}}}, "year": 2023, "month": 12, "day": 17}