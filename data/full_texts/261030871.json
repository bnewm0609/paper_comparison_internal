{"id": 261030871, "updated": "2023-10-04 21:03:57.886", "metadata": {"title": "Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces", "authors": "[{\"first\":\"Miguel\",\"last\":\"Liu-Schiaffini\",\"middle\":[]},{\"first\":\"Clare\",\"last\":\"Singer\",\"middle\":[\"E.\"]},{\"first\":\"Nikola\",\"last\":\"Kovachki\",\"middle\":[]},{\"first\":\"Tapio\",\"last\":\"Schneider\",\"middle\":[]},{\"first\":\"Kamyar\",\"last\":\"Azizzadenesheli\",\"middle\":[]},{\"first\":\"Anima\",\"last\":\"Anandkumar\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2308.08794", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-08794", "doi": "10.48550/arxiv.2308.08794"}}, "content": {"source": {"pdf_hash": "939f35d721c33187a71b43351a371a799ee409ab", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.08794v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2a2fb8c6d089441abf746f877a49595aed7a248e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/939f35d721c33187a71b43351a371a799ee409ab.txt", "contents": "\nTipping Point Forecasting in Non-Stationary Dynamics on Function Spaces\n\n\nMiguel Liu \nNVIDIA\nNVIDIA\n\n\nSchiaffini Caltech \nNVIDIA\nNVIDIA\n\n\nClare E Singer Caltech \nNVIDIA\nNVIDIA\n\n\nNikola Kovachki nkovachki@nvidia.com \nNVIDIA\nNVIDIA\n\n\nTapio Schneider Caltech \nNVIDIA\nNVIDIA\n\n\nKamyar Azizzadenesheli kamyara@nvidia.com \nNVIDIA\nNVIDIA\n\n\nAnima Anandkumar Caltech \nNVIDIA\nNVIDIA\n\n\nTipping Point Forecasting in Non-Stationary Dynamics on Function Spaces\n\nTipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points. 1\n\nIntroduction\n\nNon-stationary chaotic dynamics are a prominent part of the world around us. In chaotic systems, small perturbations in the initial condition function significantly affect the long-term trajectory of the dynamics. Non-stationary chaotic systems possess further complexity due to their time-varying nature. For instance, the atmosphere and ocean dynamics that govern Earth's climate are modeled by highly nonlinear partial differential equations (PDEs). They exhibit non-stationary chaotic behavior due to changes in anthropogenic greenhouse gas emissions, insolation, and a myriad of complex internal feedbacks [1,2] (Figure 1a). One of the main areas in scientific computing is understanding such phenomena and providing computational methods to model their dynamics. Numerical methods, e.g., finite element and finite difference methods, have been widely used to solve PDEs. However, numerical methods have enormous computational requirements to capture the fine scales in complex processes. Moreover, they do not provide a manageable way to learn from data to reduce scientific modeling errors.\n\nLearning in non-stationary physical systems. These complexities in modeling complex physical systems make learning the dynamics of their evolution in function spaces notoriously difficult. Prior works proposed various neural networks, such as recurrent neural networks (RNNs) [3] and  Figure 1: (a) Tipping point in cloud fraction as a function of CO 2 concentration, from bulk model of the atmospheric boundary layer developed in [5,6]. 3d cloud cover renderings reproduced from [2].\n\n(b) RNO accurately forecasts tipping point 64 seconds ahead in non-stationary Lorenz-63 system. \"Predicted tipping point\" is the time at which our framework predicts a tipping point will occur, and \"forecast time\" is the time when our framework makes this prediction.\n\n\nMethod\n\nGenerality Function space Partial physics Speed Pre-tip data Solver \u2714 \u2714 \u2718 \u2718 N/A EWS [7] \u2718 \u2718 N/A \u2714 \u2714 Bury et al. [8] \u2718 \u2718 N/A \u2714 \u2718 Patel and Ott [4] \u2714 Table 1: Comparison of methods. \"Generality\" denotes applicability to arbitrary types of tipping points. \"Pre-tip data\" denotes the whether the method can forecast tipping points using only data from the pre-tipping regime.\n\u2718 N/A \u2714 \u2718 Ours \u2714 \u2714 \u2714 \u2714 \u2714\nreservoir computing [4], for learning such dynamical systems [3]. However, these methods learn maps between finite-dimensional spaces and are thus not suitable for learning on function spaces, which are inherently infinite-dimensional objects.\n\nRecent research introduced neural operators [9] for learning operators to remedy these shortcomings. Neural operators receive functions at any discretization as inputs, possess multiple layers of nonlinear integral operators, and output functions that can be evaluated at any point. Neural operators are universal approximators of operators and can approximate continuous non-linear operators in PDEs [9]. These properties make neural operators discretization-invariant and suitable for learning in function spaces. Fourier neural operators (FNO) are architectures that use Fourier representations to integrate, providing an efficient implementation for many real-world applications. These methods, in particular Markov neural operators (MNO), have shown significant progress in learning Markov kernels of stationary dynamics systems [10]. However, non-stationary dynamical systems have potentially long memories, making earlier MNO developments not directly applicable to the setting of this paper.\n\nIn this work, we introduce recurrent neural operators (RNOs), a recurrent architecture for neural operators that enables learning memory-equipped maps on function spaces ( Figure 2). RNO receives a sequence of historical continuous (in time) function data and represents the memory in terms of a latent state function. This enables conditional predictions for future functions given the latent functional representations of the past. In contrast to RNNs and other fixed-discretization approaches, RNO is discretization invariant in both space and time. We show that RNOs are capable of learning the dynamics of non-stationary chaotic systems and outperform RNNs and the state-of-the-art MNO by orders of magnitude on the non-stationary Lorenz-63 system, the Kuramoto-Sivashinsky (KS) equation, and a simplified model of stratocumulus cloud cover evolution [5,6].\n\nTipping point forecasting. A commonly observed trait of non-stationarity is the existence of abrupt, drastic, and often irreversible changes in system dynamics, known as tipping points [11]. Tipping points are conceptual reference points in non-stationary systems where the evolution undergoes a sudden change in quantities of importance. For instance, in the climate system, several tipping point phenomena have been identified [12][13][14], such as the collapse of the oceanic thermohaline circulation [15][16][17], ice sheet instability [18][19][20][21], permafrost loss [22,23], and low cloud cover breakup [2]. As shown in Figure 1a, an increase in CO 2 in an idealized model is linked to a delayed drastic drop in cloud cover [2,5,6].\n\nThe identification and analysis of these climate tipping points typically relies on running numerical solvers for a very long time, which makes these methods extremely costly [2,16]. Therefore, solvers are often run at low resolution or over subsets of the full domain, which can result in loss of accuracy or generality [2].\n\nRecently, machine learning methods have been deployed for predicting the tipping points of nonstationary system [8,4,[24][25][26][27][28][29]. However, the prior works either focus on detecting domain shift instead of forecasting tipping points far ahead in time, and those that do forecast tipping points lack a systematic approach to learn non-stationary systems at arbitrary resolutions and forecast tipping points in function space.\n\nIn this work, we propose training an RNO model to predict future states on the complex dynamics of non-stationary chaotic systems using only data of pre-tipping point events. The set-up is motivated by climate modeling, where tipping points are not present in forty years of historical data [30]. To forecast the time of a tipping point, we build our approach based on the common observation that, at a tipping point, the prediction accuracy of machine learning models degrades due to distribution shifts [4,24]. Since in our setting we do not have access to post-tipping data, we instead propose to forecast tipping points whenever the model prediction of the future exhibits extensive violations of domain-specific physical constraints. We use these physical constraints (e.g., conservation laws or governing PDEs) to verify the correctness of our model's system forecasts at a given future time. Our method assumes that a given RNO model is well-trained on pre-tipping dynamics. Crucially, since the model is trained only on pre-tipping dynamics, even a perfect model on the pre-tipping regime would degrade in prediction accuracy beyond the tipping point, as observed empirically. We show that this approach predicts tipping points very far in advance.\n\nTo quantify the prediction accuracy, we propose a new conformal prediction method based on cumulative distribution functions (CDFs) [31]. We estimate the CDF of physics error, which quantifies the violation of physics constraints by the model's forecasts and plays the role of a nonconformity score. We employ the fact that using finitely-many samples, a CDF of any distribution can be estimated accurately everywhere under the Kolmogorov-Smirnov distance, a property that is not true for other functions like quantiles [32,33]. Using the empirical CDF of the physics error, we quantify the distribution of the non-conformity score. This uncertainty quantification is directly connected to the false positive rate of our prediction approach.\n\nWe consider the non-stationary and chaotic Lorenz-63 and Kuramoto-Sivashinsky (KS) systems, as well as a real-world system describing tipping points in stratocumulus cloud cover [5,6]. We show that RNO consistently outperforms the state-of-the-art adapted baseline MNO in forecasting tipping points in these systems, particularly for the KS equation. We show that RNO better fits the underlying physical equations of the systems. Furthermore, we demonstrate under our proposed approach, RNO is able to accurately forecast tipping points when run for up to 200 time intervals (Figure 1b).\n\nIn summary, we propose the first tipping point forecasting framework that scales to arbitrary tipping points and arbitrary spatiotemporal systems on function spaces. In particular, we 1. Introduce RNO, a data-driven operator learning approach to learn the dynamics of arbitrary non-stationary systems.\n\n2. Construct a tipping point forecasting framework that relies only on (1) a data-driven model of pre-tipping dynamics and (2) a physical constraint of the system.  Figure 2: Illustration of RNO auto-regressive forecasting.\n\n\nRelated Work\n\nTipping points exhibit complex behavior, making their study a challenging task in the highdimensional multi-modal setting [1]. In recent years, tipping points and their root causes have been categorized for simple systems (such as low-dimensional ODEs), based on stochasticity, rate change, and bifurcations [11,34,35]. Tipping points can be caused by changes in the condition and sourcing in differential equations, or sometimes, the root cause is the rate at which these conditions change [11]. However, in the case of general scientific computing problems on natural phenomena, theoretical studies of tipping points are very challenging, and prior works have primarily focused on their empirical evaluation via numerical simulation.\n\nTipping points are abundant in scientific computing of climate evolution and control modeling [12][13][14]. Previous work has highlighted the importance of predicting tipping points in the Earth system with the goal of giving early warning ahead of large changes in the climate [36,37]. Approaches have varied depending on the type of tipping behavior considered [38]. For instance, it has been shown that changes in atmospheric CO 2 levels can cause rapid changes in low cloud cover on the Earth [2,6]. And furthermore, after bringing CO 2 levels below the tipping threshold, these changes may not be readily undone (the system exhibits hysteresis).\n\nTraditional early warning signals (EWS) of tipping points such as increased autocorrelation before tipping events have been studied extensively for many simple systems [39,40]. However, tipping points in the large spatiotemporal systems that motivate this work have not yet been sufficiently studied empirically and theoretically for these indicators to become evident. Furthermore, for many spatiotemporal systems, thorough empirical evaluation is difficult due to the computational cost of simulations. Thus, data-driven approaches for learning system dynamics and forecasting tipping points is critical for most challenging real-world problems. In Appendix F.2, we demonstrate that EWS are not reliable at predicting tipping points in the systems we study. Recently, machine learning methods have been deployed for learning in non-stationary dynamical systems and to predict their tipping points [8,4,[24][25][26][27][28][29]. However, many works require post-tipping data [4,8], full governing equations [26], or are not generalizable to arbitrary types of tipping points [8,28]. In contrast, our method is scalable, does not need full governing equations, and is not trained on post-tipping data. See Table 1 and Appendix F.1 for a detailed comparison with prior work.\n\nRNNs are often used to learn in finite-dimensional dynamical systems with memory or in time-series with discrete-time dynamics [3,41]. They consist of blocks of neural networks that carry memory as a finite-dimensional latent state and enable conditional predictions on learned latent historical representations. But since neural networks are maps between finite-dimensional spaces, RNNs are unfit for many scientific computing problems, which often involve functions (i.e., infinite-dimensional objects) and their time-evolution. Neural ODE versions of RNNs have similar issues, can only model specific time derivatives, and require expensive solvers for temporal simulation [42][43][44].\n\nNeural operators are deep learning models that generalize neural networks to maps between function spaces [45][46][47], and they are universal approximators of general operators [9]. The inputs to neural operators are functions, and the output function can be evaluated at any point in the function domain. These models are thus known to be discretization invariant and can take the input function at any resolution. In this paper, we develop the recurrent neural operator (RNO). RNO receives a sequence of historical function data and represents the memory in terms of a latent state function, enabling conditional predictions given learned latent functional representations of the past.\n\n\nPreliminaries\n\nA PDE is differential law on function spaces. For an input function a from a function space A, we denote D A as the domain and R d A as the function co-domain with dimension d A . For any point in the domain x \u2208 D A , the function a maps this point to a d A -dimensional vector, i.e, a(x) \u2208 R d A . Correspondingly, let U denote the solution or output function space, defined on domain D U with co-domain R d U , i.e., for any function u \u2208 U and any x \u2208 D U , we have u(x) \u2208 R d U . For a given a, we define a PDE in generic form as,\nL(u, a)(x) = 0, in D U , L \u2032 (u, a)(x) = 0, in \u2202D U ,(1)\nwhere L(u, a) is the governing law in the domain D U , and L \u2032 (u, a)(x) is the constraint on the domain boundary \u2202D U . A function u is a solution to the above PDE at input a if the function u satisfies both of the PDE constraints (Eq. 1), comprising an input-solution pair (a, u). We are concerned with learning maps from input function spaces A to output function spaces U in operator learning. For a given PDE, let G \u22c6 denote the operator that maps the input functions in A to their corresponding solution functions in U, a map that employs a neural operator to learn.\n\nA neural operator G is a deep learning architecture consisting of multiple layers of point-wise and integral operators [9] to learn a map from input function a to output function u. The first layer of the architecture is a pointwise lifting operator P 0 such that for a given function a, the output of this layer \u03bd 0 = P 0 a is computed so that for any x \u2208 D A , \u03bd 0 (x) = P 0 (a(x)) \u2208 R d0 where P 0 is a learnable neural network. This step is followed by L layers of nonlinear integral operators. For any layer l we have,\n\u03bd l = P l (\u03c3(K l \u03bd l\u22121 ) + W l \u03bd l\u22121 s l ),(2)\nwhere \u03bd l : D l \u2192 R d l , \u03c3 is some pointwise nonlinearity, and K l is an integral operator such that for any x \u2208 D l , we have,\nK l \u03bd l\u22121 (x) = D l\u22121 \u03ba l (x, y)\u03bd l\u22121 d\u00b5 l (y) + W \u2032 l \u03bd l\u22121 (s l (x)).(3)\nHere \u03ba l is a learnable kernel function, W \u2032 l is a pointwise operator parameterized by a neural network W \u2032 l , and s l is a deformation map from D l to D l\u22121 . Similarly, W l is a pointwise operator parameterized by W l neural network, and P l is a pointwise operator parameterized by a neural network P l . \u00b5 l represents the measure on each space and finally, we set D L = D U . The last layer is a pointwise projection operator Q parameterized by a neural network Q. In particular, we use Fourier representations to compute the integration Eq. 3 in its convolution form,\nK l \u03bd l\u22121 (x) = F \u22121 F\u03ba l \u00b7 F\u03bd l\u22121 + W \u2032 l \u03bd l\u22121 (s l (x)),(4)\nwhere F denotes the Fourier transform, and project its inputs to k l Fourier bases, for some hyperparameter k l . Let R l denote the k l Fourier coefficients of \u03ba l , i.e., R l = F\u03ba l . Following [46], we directly learn R l instead of \u03ba l to learn the integral operator. We utilize these building blocks to construct the RNO architecture in the next section.\n\n\nRecurrent neural operator (RNO)\n\nWe now describe and formulate RNO, a generalization of RNNs to function spaces. For a given PDE describing the evolution of a function u(x, t) of time and space, consider a partition of the time domain t into equally-spaced intervals of length \u2206T and indexed by \u03c4 \u2208 N. A step \u03c4 is associated with the interval [(\u03c4 \u2212 1) \u00b7 \u2206T, \u03c4 \u00b7 \u2206T ], where the input function restricted to this interval is defined for t \u2208 [t, t + \u03c4 \u00b7 \u2206T ] as u \u03c4 (x, t) := u(x, t).\n\nOur proposed architecture of the RNO parallels that of the gated recurrent unit (GRU) [48], where the GRU cell at step \u03c4 receives a hidden state vector h \u03c4 and an input vector x \u03c4 , and predicts the next output vector x \u03c4 +1 and next hidden state vector h \u03c4 +1 , which is also passed to the GRU cell at step \u03c4 + 1.\n\nThe key difference between the RNO and the GRU is that at step \u03c4 , an RNO cell receives as input a hidden function h \u03c4 (x, t) and an input function u \u03c4 (x, t). As such, we replace the linear maps in the GRU architecture with Fourier layers, Eq. 4). Given u \u03c4 and h \u03c4 , we define the reset gate such that for any t \u2208 [t, t + \u03c4 \u00b7 \u2206T ],\nr \u03c4 (x, t) = \u03c3 (K r u \u03c4 )(x, t) + (K \u2032 r h \u03c4 )(x, t) + b r (x, t) ,(5)\nwhere K r , K \u2032 r are Fourier layers, b r (x, t) is a learned bias function, and \u03c3 is the sigmoid function applied pointwise. We define the update gate as,\nz \u03c4 (x, t) = \u03c3 (K z u \u03c4 )(x, t) + (K \u2032 z h \u03c4 )(x, t) + b z (x, t) .(6)\nWe define the candidate hidden function as,\nh \u03c4 +1 (x, t) = \u03d5 (K h u \u03c4 )(x, t) + (K \u2032 h (r \u03c4 \u00b7 h \u03c4 ))(x, t) + b h (x, t) ,(7)\nwhere \u03d5 denotes a point-wise scaled exponential linear unit (SELU) activation and the function multiplication (\u00b7) is taken to be pointwise. Finally, we define the hidden state function h \u03c4 +1 (x, t) as\nh \u03c4 +1 (x, t) = (1 \u2212 z \u03c4 ) \u00b7 h \u03c4 (x, t) + z \u03c4 \u00b7 h \u03c4 +1 (x, t).(8)\nWe take h 0 (x, t) to be a learned prior function on the initial hidden state. Let RNO(v, h \u03c4 ) denote an RNO cell. Given the input function u \u03c4 , an RNO block takes the following form:\nu \u03c4 (x, t) = P 0 (u \u03c4 )(x, t) = P 0 (u \u03c4 (x, t)),(9)v (i+1) \u03c4 (x, t) = RNO (i+1) v (i) \u03c4 , h (i+1) \u03c4 (x, t),(10)u \u03c4 (x, t) = Q(v (L) \u03c4 )(x, t) = Q(v (L) \u03c4 (x, t)),(11)\nwhere P 0 and Q are pointwise lifting and projection operators, respectively parameterized by neural networks P 0 and Q. v\n(0) \u03c4 (x, t) :=\u0169 \u03c4 (x, t)\n, the result of the lifting operation, and L is the number of RNO layers. An RNO is then a RNO block evolving in time with h Figure 2.\n(i) \u03c4 +1 := v (i) \u03c4 for each RNO cell i \u2265 1,\nWe divide the inference stage of RNO into two phases:\n\n1. Warm-up phase: In the warm-up phase, we have access to the ground-truth trajectory between steps \u03c4 = 0 and \u03c4 = N \u03c4 \u2212 1, corresponding to the ground-truth solution u(t) between times t \u2208 [0, N \u03c4 \u2206T ]. During this phase, the RNO blocks receive as input u \u03c4 , and the predicted u \u03c4 +1 is discarded.\n\n\nPrediction phase:\n\nIn the prediction phase, we assume we no longer have access to the ground-truth trajectory beyond time \u03c4 = N \u03c4 \u2212 1, so the model predictions u \u03c4 +j are taken as input to the RNO block at time \u03c4 + j + 1, and the model is thus composed with itself (with access to its hidden state h \u03c4 ), with its outputs fed back on itself as inputs.\n\nThe prediction phase is similar to Markov neural operator (MNO) [10], with the exception that RNO now has access to a continually-updating hidden representation h \u03c4 (x, t), which encodes the history of the non-stationary system, making RNO a generalization of MNO to systems with memory.\n\n\nTipping-point Forecasting\n\nForecasting tipping points requires rigorous uncertainty quantification, and any prediction must be accompanied by the event's potential probabilities. Conformal prediction is a field of study that accompanies predictions with certainty levels in terms of a conformity score [31]. Most prior work on traditional conformal prediction quantifies the probability distribution of the conformity score at certain levels. Another approach is to utilize quantile regression to quantify the distribution of conformity scores [49]. Both of these approaches have shortcomings in tipping point prediction. The former approach utilizes exchangeability and requires new and fresh draws of samples to quantify the distribution of conformity scores, imposing a union bound over a long period of time. Quantile regression suffers from two downsides: (1) quantiles are not generally estimable, and (2) a union bound over an infinite set is required to quantify the conformity score distribution [50]. These limitations are addressed below by introducing CDF estimation methods.\n\nOur tipping point forecasting utilizes training on the pre-tipping dynamics of the system. We train RNO on data sets of time-evolving functions that do not contain any tipping points. We make this choice to resemble real-world settings such as climate where recorded historical data does not contain tipping points. At any time t, we use RNO to predict the long evolution of the dynamics for an interval  Figure 3: Diagram of the setup for the tipping point prediction method. U and L correspond to the upper and lower bounds given by the CDF concentration inequality, differing from the empirical CDF by \u03b5. \u03b1 is the significance level, and l(\u03b1) is the corresponding loss of L at \u03b1.\nof [t, t + T ], i.e., u(x, [t, t + T ]) where T is a multiple of \u2206T .access to the future u(x, [t, t + T ]). If we did, we could use the deviation of u(x, [t, t + T ]) from u(x, [t, t + T ])\nas a hindsight indicator for the tipping point as observed in prior works [4,24].\n\nInstead, we propose to use the physical constraints L(u) = 0 of the underlying system (such as the PDE that governs the dynamics) to evaluate the deviation of u(x, [t, t + T ]) from physics laws, and we use this signal as an indicator of tipping points. For a predicted function u(x, [t, t + T ]), we define the conformity score as \u2225L( u) \u2212 0\u2225, and we refer to it by the physics loss\n\u2113 P ( u). Note that if u(x, [t, t + T ]) = u(x, [t, t + T ])\n(the true future function), then L( u) = 0, and thus our proposed indicator aligns with the hindsight indicator mentioned above. At any time t, we compute \u2113 P ( u), and if it takes values above a certain threshold, we forecast that a tipping point is anticipated. This method enables tipping point forecasting. However, since the trained model comes with training generalization error, this approach makes mistakes, confusing large errors on pre-tipping regimes and actual tipping incidence. This is of particular importance when the forecasting interval of interest T is large. We propose the following novel conformal prediction to address uncertainty quantification in tipping point forecasting. For a trained model and given a forecasting interval T , let F denote the true CDF of the conformity score. The CDF captures the probabilities at which the model makes errors of a given magnitude on pre-tipping data. We use the CDF to develop our conformal prediction method, which allows for uncertainty quantification of tipping point forecasts. In particular, for any loss threshold at which we call a tipping point, we can denote \u03b1 to be the probability of falsely forecasting a tipping point when the data was simply from the pre-tipping regime. We first compute the empirical CDF F of the conformity score \u2113 P (u) using n calibration samples. Formally, given n samples, we construct {\u2113 P (j) } n j=1 and compute F as follows. For any p \u2208 R,\nF (p) := 1 n n j 1(p \u2265 \u2113 P (j) ).\nWe then use the concentration inequality of CDFs to compute its upper and lower bounds [32,33].\n\nUsing the concentration inequality of CDFs, with probability at least 1 \u2212 \u03b4 we have,\nsup p\u2208R F (p) \u2212 F (p) \u2264 \u03b5 := log(2/\u03b4) 2n ,\nwhere the left-hand side is the Kolmogorov-Smirnov distance between the two CDFs. We define the CDF lower bound L(p) := max{0, F (p)\u2212\u03b5} and the CDF upper bound U (p) := min{1, F (p)+\u03b5}. Figure 3 depicts this procedure. Intuitively, \u03b1 denotes the maximum probability of a mistake that one is willing to tolerate. In our approach, as shown in Figure 3, we first find l(\u03b1), the conformity level at which the CDF lower bound L has probability 1 \u2212 \u03b1. Using the level l(\u03b1), we call the presence of a tipping point whenever the conformity score of our prediction \u2113 P ( u) is above l(\u03b1).\n\nProposition. For any given \u03b1 \u2208 [0, 1], the decision rule of calling for a tipping point at level l(\u03b1) has a false positive rate of at most \u03b1, and this statement holds with probability at least 1 \u2212 \u03b4.  The proof is based on the fact that for any \u03b1, F \u22121 (1 \u2212 \u03b1) \u2264 l(\u03b1) and F (l(\u03b1)) \u2265 1 \u2212 \u03b1. Therefore, when \u2113 P \u2265 l(\u03b1), signaling a tipping point, we also have \u2113 P \u2265 F \u22121 (1 \u2212 \u03b1). Thus, the probability of the event \u2113 P \u2265 l(\u03b1) is lower bounded by the probability of \u2113 P \u2265 F \u22121 (1 \u2212 \u03b1) which at most \u03b1. This proposition simultaneously holds for all the time steps \u03c4 and all \u03b1. This property enables assessing tipping predicting at various levels, a crucial feature for risk assessment and policy making.\n\n\nExperimental Results\n\nWe present experimental results on learning dynamics and forecasting tipping points in non-stationary systems. We showcase the performance of RNO and the tipping point forecasting method on the finite-dimensional Lorenz-63 system, the (infinite-dimensional) KS PDE, and the example of tipping points in simplified cloud cover equations. Discussion on the technical details of RNO (e.g., memory usage, inference times, hyperparameter selection, etc.) can be found in Appendix G.\n\n\nNon-stationary Lorenz-63 system\n\nWe consider tipping point forecasting in the non-stationary chaotic Lorenz-63 system [51], a simplified model for atmospheric dynamics. The non-stationary Lorenz-63 system is given b\u1e8f\nu x = \u03c3(u y \u2212 u x ),u y = u x (\u03c1(t) \u2212 u z ) \u2212 u y ,u z = u x u y \u2212 \u03b2u z ,(12)\nwhere the state space is u = (u x , u y , u z ), and the parameters of the system are \u03c3, \u03c1(t), \u03b2, where \u03c1 depends on time as in [4].\n\nWe find that RNO substantially outperforms MNO and RNN in learning the evolution of the nonstationary system. Furthermore, we use the Lorenz-63 system as a case study to justify our tipping point forecasting framework. Figure 1b shows that RNO is capable of accurately forecasting the tipping point 64 seconds (T = 200 \u00b7 \u2206T ) ahead, even though the size of each input/output interval is \u2206T = 0.32 seconds. The full experimental setup and results can be found in Appendix A.\n\nWe also demonstrate that our method is capable of forecasting tipping points far in advance using only approximate knowledge of the underlying system. In particular, we perturb the parameters \u03c3, \u03c1, \u03b2 by some fixed quantity \u03b7 and show that the error in the tipping point forecast for the Lorenz-63 system is still less than 1 second, even when predicting 64 seconds in advance. Results can be found in Appendix A.\n\n\nNon-stationary Kuramoto-Sivashinsky (KS) equation\n\nWe demonstrate the effectiveness of RNO and our tipping point forecasting method in PDE systems by considering the 1-d non-stationary KS equation, which takes the form\n\u2202u \u2202t + u \u2202u \u2202x + \u2202 2 u \u2202x 2 + \u03ba(t) \u2202 4 u \u2202x 4 = 0(13)\nwhere \u03ba(t) is some time-dependent parameter of the system, u(x, t) is a function of space x \u2208 [0, 2\u03c0] and time t \u2208 (0, \u221e), and the initial condition is u(\u00b7, 0) = u 0 . Following [4], we parameterize \u03ba(t) = \u03ba 0 + \u03ba 1 exp(t/\u03b3), where \u03ba 0 = 0.0753, \u03ba = 0.0034, and \u03b3 = 75.3. This system exhibits a tipping point near \u03ba * = 0.08 from periodic dynamics to chaotic dynamics. The tipping point can be classified as an instance of bifurcation-induced tipping [11] at a saddle-node bifurcation [4].\n\n\nLearning non-stationary dynamics\n\nWe compare RNO against Markov neural operators (MNO) [10] and RNNs in forecasting nonstationary dynamics. There are two key properties that any learned model must exhibit: (1) low step-wise error and (2) error stability in long-time predictions (i.e., when the model is composed with itself many times). Low step-wise error is a necessary prerequisite to adequately learning the dynamics of any system. However, for many downstream tasks of significant scientific importance, it is also crucial that our model's error remain low when forecasting far into the future. This ensures the accuracy of tipping point prediction is credible in complex real-world settings. Hereafter, we define \"n-step prediction\" to be a model's forecasting prediction looking ahead \u03c4 = n steps.\n\nIn our experiments, we employ a multi-step training regimen for all models, where the total loss is a combination of the loss on the model's n-step prediction for n \u2208 {1, 2, . . . , M }. That is, our data loss function is given by\n\u2113 D (u, u) = M n=1 \u03bb n \u2225u n \u2212 u n \u2225 2 ,(14)\nwhere \u2225 \u00b7 \u2225 2 is the L 2 norm and \u03bb n is some scaling hyperparameter. Table 2 compares the relative L 2 errors of RNO, MNO, and RNN on forecasting the non-stationary KS equation. We observe that RNO outperforms MNO and RNN on every forecasting interval. Further, we can observe the disadvantages of RNN and other finite-dimensional, fixed-resolution models on learning in PDE (i.e., infinite-dimensional) settings. The RNN's error is orders of magnitude larger than that of RNO or MNO for small forecasting windows. At large forecasting windows (i.e., 12-step and above), the lack of explicit history in MNO becomes a severe disadvantage. In contrast, RNO maintains much lower error at large forecasting intervals than the other two models. Both the MNO and RNO shown in Table 2 were trained with M = 5 steps of fine-tuning (Eq. 14). Observe that MNO and RNN have difficulty generalizing to forecasting intervals larger than M , whereas RNO is capable of maintaining stable error even at longer forecasting intervals.\n\n\nTipping point forecasting\n\nIn this section, we present empirical results in tipping point prediction for the non-stationary KS equation. As described in Section 5, our conformal prediction approach for tipping point prediction relies on a model trained in pre-tipping dynamics. We then monitor variations in the physics constraint loss of model forecasts to predict tipping points in the future. Recall that the physics constraint loss is only a function of the model predictions u and the time t. We define the KS physics loss to be\n\u2113 P ( u, t) = \u2202 u \u2202t + u \u2202 u \u2202x + \u2202 2 u \u2202x 2 + \u03ba(t) \u2202 4 u \u2202x 4 2 .(15)\nIn practice, \u2113 P is computed over an entire calibration set, which is unseen during training, and these samples are used to construct an empirical CDF of the physics loss.\n\nIn our experiments, we consider the setting of forecasting tipping points far ahead in time, at several orders of magnitude longer time scales than the model is trained on. For the KS equation, we set \u2206T = 1.6 seconds. Figures 4a and 4b compare the accuracy of RNO and MNO in forecasting the KS tipping point T = 16 seconds ahead. At short forecasting intervals (e.g., T = \u2206T or T = 2 \u00b7 \u2206T ), RNO and MNO both accurately capture the tipping point, but at long time-scales only RNO retains good performance. \n\n\nCloud cover equations: tipping points from partial physics\n\nWe now consider a case study of tipping point forecasting in the Earth's climate. We also demonstrate that our method is still accurate under partial physics, e.g., conservation laws (not full PDEs).We use the model developed in [5,6] as an idealization of the physical climate system; this model exhibits a tipping point. The model represents a stratocumulus-topped atmospheric boundary layer, which experiences rapid loss of cloud cover under high CO 2 concentrations and shows hysteresis behavior, where the original state is not immediately recovered when CO 2 concentrations are lowered. This model was developed, in part, to help explain the results found from the high-resolution simulations in [2,52]. More details on the model can be found in Appendix B, and details on numerical experiments can be found in Appendix E.4.\n\nAnalogously to the KS equation setting (Section 6.2), we train an RNO to forecast the non-stationary evolution of the system with a data loss given by Eq. 14, which incorporates the L 2 loss across longer time horizons. We compute the empirical CDF of the physics loss for the cloud cover system (analogous to Eq. 15 in the KS setting) as a function of model predictions and CO 2 . However, in this case, the physics loss is normalized component-wise by the state of the system at a given time, since the magnitudes of each component in this model differ by several orders of magnitude.\n\nFor tipping point forecasting, we base the physics loss solely on mass conservation (Eq. 18a), instead of the full ODE. In our experiments, we set the tipping point forecasting horizon to be T = 7 \u00b7 \u2206T = 2.45 years (with forecasting intervals of \u2206T = 0.36, or 128 days). We found that the choice of \u2206T (on time-scales on the order of O(100) days) does not significantly impact the RNO's ability to learn the dynamics. T is chosen to be an order of magnitude larger than \u2206T , to demonstrate that our framework is capable of forecasting tipping points at time-scales much longer than \u2206T . The results on a given test trajectory can be seen in Figure 5. Despite the abruptness of the tipping point and the highly-nonlinear dynamics of the system around the tipping point, we demonstrate that our forecasting framework is successfully able to identify the true tipping point in this system using a physics loss derived from a partial picture of the full physics.\n\n\nConclusion\n\nIn this paper, we introduce recurrent neural operator (RNO), an extension of recurrent neural networks to function spaces, and an extension of neural operators [9] to systems with memory. We also propose a model-agnostic conformal prediction method with considerable statistical guarantees for forecasting tipping points in non-stationary systems by measuring deviations in a model's violations of underlying physical constraints or equations. In our experiments, we demonstrate RNO's ability to learn non-stationary dynamical systems, particularly its stability in error even when auto-regressively applied for long time scales. We also demonstrate the effectiveness of our tipping point forecasting method on infinite-dimensional PDE systems, and we demonstrate that our proposed methodology maintains good performance even when using partial or approximate physics laws.  Table 3: Relative L 2 errors on non-stationary Lorenz-63 for different n-step prediction settings. RNO outperforms MNO on every forecasting interval and maintains low error even for long intervals.\n\n\nA Lorenz-63 experimental results\n\nThe Lorenz-63 system is defined in Eq. 12. In this paper, we follow the setting of [4] where \u03c3 = 10, \u03b2 = 8/3, and \u03c1 depends on time via the parameterization \u03c1(t) = \u03c1 0 + \u03c1 1 exp (t/\u03b3), where \u03c1 0 = 154, \u03c1 1 = 8, and \u03b3 = 100.\n\nA tipping point for this system occurs at approximately \u03c1 * = 166 [53], which correspond approximately to time t * = 40 under the parameterization above. This tipping point is induced by an intermittency route to chaos [53] as the dynamics transition from periodic t < t * to chaotic t > t * .\n\nThe tipping point can be classified as an instance of bifurcation-induced tipping [11] at a saddle-node bifurcation [4]. Details on data generation and experimental setup can be found in Appendix E.\n\n\nA.1 Learning non-stationary dynamics\n\nNumerical results from our experiments are shown in Table 3. We observe that RNO outperforms MNO in relative L 2 error by at least an order of magnitude for every n-step prediction setting up to and including 32-step prediction. We observe that even when composed with itself 32 times, RNO is capable of maintaining relative L 2 error under 0.05. Further, we find that despite training MNO with a multi-step procedure up to M = 12 steps, this still does not prevent MNO error from steadily increasing as the number of steps to compose increases. While RNN is somewhat stable in its error when composed multiple times, RNO vastly outperforms it in L 2 error. We attribute the performance of RNO to its status as an architectural generalization of MNO to systems with memory. Further, RNO's discretization invariance and adaptability to function spaces makes it an improvement over fixed-resolution RNNs.\n\nA.2 Tipping point prediction: fully-known physics Analogously to the KS setting (Eq. 15), we define the physics constraint loss to be\n\u2113 P ( u, t) = \u02d9 u(t) \u2212 u(t) 2 ,(16)\nwhere\u02d9 u is the time-derivative 2 of the model's forecasted trajectory at time t and u(t) is the time derivative defined by the Lorenz-63 system (eq. 12) using the model's predicted state u(t) at time t. That is,\nu x = \u03c3( u y \u2212 u x ), u y = u x (\u03c1(t) \u2212 u z ) \u2212 u y , u z = u x u y \u2212 \u03b2 u z .(17)\nThus, \u2113 P is minimized when the time derivative of the model's predictions is equal to the expected derivative that all solutions to the Lorenz-63 system must satisfy (Eq. 12).\n\nIn our experiments, we set \u2206T = 0.32 seconds. Here we study the difficult task of forecasting the tipping point 64 seconds ahead, corresponding to two hundred times the time scale of the model. For a fixed \u03b4 = 10 \u22123 , Figure 14a shows the effect of varying the critical loss threshold l(\u03b1) for a given \u03b1 on RNO and MNO tipping point predictions.\n\nFrom Figure 14a we make a key observation: the distribution of RNO's physics loss has both smaller mean and variability than that of MNO, as well as a shorter right tail. This is deduced from the observation that the first sudden spike in Figure 14a corresponds to the mode of the histogram of each model's physics losses over the calibration set. Furthermore, observe that the loss range between this spike and the time when the model achieves its delay in the forecasted tipping point of the smallest magnitude is much smaller for RNO than for MNO, implying the difference in the right-tail  Figure 6: Log normalized components of the non-stationary Lorenz-63 system plotted with a 50-second moving average. The predicted tipping point aligns nearly exactly with the true periodicchaotic tipping point of the system, which can be seen as a qualitative change in dynamics of the system components. A false-positive rate of \u03b1 = \u03b5 \u2248 0.03 is used, where \u03b5 is discussed in Section 5.\n\nlengths of the two models. In particular, this analysis of the distribution of the physics loss for each model reinforces the results in Section A.1 that RNO is more successful than MNO in learning the underlying non-stationary dynamical system. Figures 1b and 6 show RNO achieving near-zero error in forecasting the tipping point 64 seconds ahead.\n\n\nA.3 Tipping point prediction: approximate physics\n\nIn this section, we investigate our method's performance in tipping point forecasting using approximate physics constraints. For instance, such a setting may occur when certain coefficients in a PDE or ODE are known only approximately. In this section, we consider this example of approximate physics for the nonstationary Lorenz-63 system, where we evaluate our proposed framework's capability of forecasting tipping points under different perturbations of the ODE parameters. Specifically, we once again define the physics loss as in Eq. 16, but in this case, we redefine u(t) by replacing \u03c3, \u03c1(t), \u03b2 with\u03c3 = \u03c3 + \u03b7,\u03c1(t) = \u03c1(t) + \u03b7,\u03b2 = \u03b2 + \u03b7, respectively, where \u03b7 is some fixed perturbation.\n\nWe use the same experimental setup and as in Section A.2. As shown in Fig. 8, our proposed framework is robust to perturbations in the physical constraints and is able to forecast tipping points with very low error far in advance. This is particularly useful for real-world scenarios in which only approximate physical laws are known for a given system.\n\nIn particular, we observe that with even approximate physical knowledge of the underlying system, the notion of using the physics loss to verify the correctness of the model's predictions does not break down. This can be observed in Figures 8c and 8d, where a \"bump\" in the approximate physics loss can be seen at the forecast time, signaling the shift in distribution of the underlying dynamics. \n\n\nB Background on simplified cloud cover model\n\nThe model developed in [5,6] is an extension of a traditional bulk boundary layer model [54], which represents the state of the atmospheric boundary layer and cloud by five coupled ordinary differential equations:\ndz i dt = w e (\u2206R, s) \u2212 Dz i + w vent (CF) (18a) ds dt = 1 z i [V \u00b7 (s 0 (SST) \u2212 s) + w e (\u2206R, s) \u00b7 (s + \u2212 s) \u2212 \u2206R(CO 2 , q t,+ )] + s exp (18b) dq t dt = 1 z i [V \u00b7 (q t,0 (SST) \u2212 q t ) + w e (\u2206R, s) \u00b7 (q t,+ \u2212 q t )] + q t,exp (SST) (18c) dCF dt = f CF (D) \u2212 CF \u03c4 CF . (18d) C dSST dt = SW net (CF) \u2212 LW net \u2212 LHF(SST, s, q t ) \u2212 SHF(SST, s) \u2212 OHU.(18e)\nThe model is formulated such that CO 2 is the only external parameter and all other processes are represented by physically motivated, empirical formulations, with their parameters based on data from satellite observations or high-resolution simulations.\n\nThe first three equations physically represent conservation of mass (18a), energy (18b), and water (18c), and the final two are equations for the cloud fraction and the sea surface temperature. In (18a), z i is the depth of the boundary layer [m], w e is the entrainment rate which is itself parameterized as a function of the radiative cooling and the inversion strength, D is the subsidence rate [s \u22121 ], w vent is an additional additive entrainment term used to parameterize ventilation and mixing from overshooting cumulus convective thermals. In (18b) and (18c), s is the liquid water static energy [J kg Our framework is capable of forecasting the tipping points with very low error even with large perturbations \u03b7. A false positive rate of \u03b1 = \u03b5 \u2248 0.03 is used, where \u03b5 is discussed in Section 5.\n\nThe cloud fraction is modeled as a linear relaxation on timescale \u03c4 CF to a state f CF (D) which depends on the degree of decoupling in the boundary layer:\nf CF (D) = CF max \u2212 CF max \u2212 CF min 1 + 1 9 exp(\u2212m(D \u2212 D c )) (19a) D = LHF \u03c1\u2206R z i \u2212 z b z i . (19b)\nThis parameterization is inspired by theoretical and observational work from [55,56] and parameters CF max , CF min , and m are fit to data from [57,58,2].\n\nEquation (18e) is the standard surface energy budget equation for SST. On the left-hand side, C = \u03c1 w c w H w is a heat capacity per unit area, where \u03c1 w and c w are the density and specific heat capacity of water and H w is the depth of the slab ocean. The value of H w is arbitrary (here 1 m): it affects the equilibration time, but not the equilibrium results, which is appropriate given that the forcing is much slower than the equilibration timescale (approx. 50 days). On the right-hand side are the source terms from shortwave and longwave radiation, latent and sensible heat fluxes, and ocean heat uptake. Ocean heat uptake is solved for implicitly such that SST = 290 K for CO 2 = 400 ppmv and assumed constant in time.\n\nTo generate the data for our experiments, we set the CO 2 forcing to follow\nCO 2 (t) = c 0 (1 + r) t + \u03c3W (t),(20)\nwhere t is the time (in years), W (t) is a Wiener process, c 0 is the initial CO 2 concentration, r is the annual rate of CO 2 increase, and \u03c3 is the scaling parameter for W (t). We use c 0 = 400 ppm, r = 0.1, and \u03c3 = 2.5. \n\n\nC Additional results on the KS equation\n\nIn this section we compare the performance of RNO in tipping point forecasting with the performance of MNO, for varying values of \u03b1, the false-positive rate of our method. Note that as shown in Table 2, the relative L 2 error of RNN on forecasting the evolution of the KS equation is an order of magnitude larger than that of RNO and MNO. As shown in Figure 9, we observe that for all values of \u03b1 shown, RNO vastly outperforms MNO in the proximity of tipping point prediction to the true tipping point of the system. This can likely be attributed to the lower performance of MNO in learning the nonstationary system dynamics (see Table 2), which causes MNO to produce fluctuating physics losses even in the pre-tipping regime, whereas RNO has a low, stable physics loss during pre-tipping. Figure 14b compares the critical physics loss threshold l(\u03b1) for RNO and MNO when forecasting the KS tipping point 16 seconds ahead. For lower physics loss thresholds, RNO forecasts the tipping point with much better performance than MNO. As the critical physics loss threshold is increased, the performance of RNO improves slowly, suggesting that the distribution of RNO Table 4: Relative L 2 errors on the cloud cover system for different n-step prediction settings using the experimental setup described in Appendix E.4. RNO outperforms MNO and RNN on every forecasting interval. weight on the right tail. In contrast, MNO exhibits the opposite behavior, suggesting that it is less reliable than RNO in learning the non-stationary dynamics. Crucially, we note that comparisons on the accuracy of tipping point predictions between two methods must be made at a fixed false-positive rate \u03b1 (e.g., see Figure 9). In Figures 14b and 14a, we only seek to compare the distributions of physics loss and each model's respective ability to learn the underlying dynamics.\n\nD Additional results on the cloud cover system\n\n\nD.1 Numerical results in learning non-stationary dynamics\n\nNumerical results from our experiments are shown in Table 4. We observe that RNO outperforms MNO and RNN in relative L 2 error for every n-step prediction presented. We find that as the size of the forecasting interval \u2206T (not to be confused with the length of the forecasting window, which is n \u00b7 \u2206T ) increases, the performance of RNN worsens substantially. We attribute this to the resolutiondependence of RNN; as the dimensionality of the input (in this case 5 \u00b7 \u2206T , since the cloud cover system is 5-dimensional) increases, the size of the RNN model must be increased accordingly in order to adequately capture the dynamics of the system. For RNO and MNO, the size of the models need not be increased substantially since the input is interpreted as a function over a larger domain, and the inductive biases of these models allows them to outperform fixed-resolution models such as RNNs.\n\n\nD.2 Tipping point forecasting using full ODE constraint\n\nIn this section, we demonstrate that our proposed method is capable of identifying the tipping point with low error when using the full ODE equation as the physics constraint. Figure 10 shows that our method is capable of forecasting the tipping point far in advance, similarly to Figure 5.\n\nSimilarly to the nonstationary KS setting, we also compare the performance of RNO in tipping point forecasting with MNO and RNN for varying values of false-positive rate \u03b1 for the simplified cloud cover system (Figures 11 and 12). We observe that RNO overall outperforms RNN and MNO in tipping point forecasting for a variety of false-positive rates \u03b1. Furthermore, RNO is able to achieve nearly no decrease in error as \u03b1 increases, for reasonable \u03b1.  \n\n\nE Details on numerical experiments\n\nIn our RNO experiments, we divide inference into a warm-up phase and a prediction phase. This choice is motivated by the empirical observation that predictions u 1 , u 2 , . . . for small steps \u03c4 tend to be poor because the hidden function h 0 (x, t) does not contain sufficient information to properly learn complex nonstationary dynamics (which inherently require history). The effect of the warm-up phase is to allow the RNO to construct a useful hidden representation h \u03c4 before its predictions u \u03c4 are used to construct long-time predicted trajectories. We use this procedure both during training and test times.\n\nFurthermore, MNO has achieved state-of-the-art accuracy in forecasting the evolution of stationary dynamical systems by using a neural operator (Section 3) to learn the Markovian solution operator  Figure 11) tends to outperform MNO and RNN in tipping point forecasting error for the majority of the \u03b1. of such systems. As proposed in [10], MNO takes as input the solution at a given time, u(x, t), and outputs u(x, t + h). While the Markovian solution operator holds for some stationary systems [10], non-stationary systems must be handled with more care. As such, we instead compare RNO against a variant of MNO that maps u(x, [t, t + \u2206T ]) \u2192 u(x, [t + \u2206T, t + 2\u2206T ]). This variant of MNO equips the model with some temporal memory.\n\n\nE.1 RNN baseline\n\nWe use a gated recurrent unit (GRU) [48] architecture for the RNN baseline for the non-stationary Lorenz-63, KS equation, and simplified cloud cover model experiments. We use shallow neural networks to map inputs to a hidden representation that is propagated between time intervals with the GRU, and the final GRU hidden-state is mapped to the output space with another shallow neural network. We observe optimal performance for the RNN when we adopt the same inference process as with RNO, with a warm-up phase and a prediction phase. We observe that the number of time intervals for the warm-up phase needed to achieve acceptable results tends to be higher for RNN than for RNO across our experiments. For instance, we warm up with 10 intervals for RNN and 5 for RNO, in both the Lorenz-63 and KS settings. In the cloud cover experiments, we warm up the RNN and RNO both with 5 time intervals.\n\nIn Table 5, we note the effectiveness of multi-step fine-tuning in learning the evolution of longer trajectories in non-stationary systems (compare RNN-8 and RNN-1). However, if multi-step finetuning is taken too far (i.e., M is large), we empirically observe convergence to suboptimal local minima (e.g., .\n\n\nE.2 Non-stationary Lorenz-63 system\n\nIn our experiments, we generate 15 trajectories using a fourth-order Runge-Kutta method with an integration step of 0.001. Ten of these trajectories are used for training and the remaining five are used for calibration and testing. Each trajectory is on the range t \u2208 [\u2212600, 200]. The initial condition for each trajectory is initialized randomly, and the solution is integrated for 50 integrator seconds and then discarded to allow the system to reach the periodic dynamics. For training and testing, the data is temporally subsampled into a temporal discretization of 0.05 integrator seconds.\n\nBoth the RNO and MNO used for experiments in Section 6 were trained with a width of 48 and 24 Fourier modes. See [46] for a description of the hyperparmaters of Fourier layers. We use L = 3 layers in the RNO and 4 layers for MNO. We implement the baseline RNN as described in Section E.1, with 3 layers and a 248-dimensional hidden state. We set the length of each input/output time interval to \u2206T = 0.32 seconds. We also fix the number of warm-up samples N \u03c4 = 5 for RNO experiments.\n\nIn our experiments, we set M = 12 and \u03bb n = 1 for all n, for the corresponding terms in our data loss (Eq. 14). All models were optimized using Adam [59] with an initial learning rate of 10 \u22124 and a batch size of 64. We use a step learning rate scheduler that halves the learning rate every 4000 weight updates. We train RNO and MNO for 5 epochs, and we train RNN for 15 epochs. We pre-train MNO for 50 epochs on one-step prediction.\n\n\nE.3 Non-stationary KS equation\n\nIn our experiments, we generate 200 trajectories of the evolution of the non-stationary KS equation using a time-stepping scheme in Fourier space with a solver time-step of 0.001. 160 of these trajectories were used for training, 30 of them were used for calibration, and 10 trajectories were used for testing. Each trajectory is defined on the range t \u2208 [0, 100]. The initial condition is initialized   randomly, and the solution is integrated for 100 integrator seconds and then discarded to allow the system to reach periodic dynamics. For training and testing, the data is temporally subsampled into a temporal discretization of 0.1 integrator seconds. Our models map the previous time interval of length \u2206T to the next time interval of length \u2206T . In our model, we use \u2206T = 1.6 seconds.\n\nBoth the RNO and MNO used in our experiments used 20 Fourier modes across both the spatial and temporal domain. The RNO used has a width of 28, and MNO used has a width of 32. We use L = 3 layers in the RNO and 4 layers for the MNO. We implement the baseline RNN with 3 layers and a 512-dimensional hidden state. We fix the number of warm-up samples N \u03c4 = 5 for RNO experiments.\n\nIn our experiments, we set M = 5 and \u03bb n = 1 for all n, for the corresponding terms in our data loss (Eq. 14). All models were optimized using Adam [59] with a batch size of 32. RNO was trained for 25 epochs at an initial learning rate of 10 \u22123 , while halving the learning rate every 2000 weight updates. MNO was pre-trained on one-step prediction for 100 epochs at a learning rate of 0.005, halving the learning rate every 200 weight updates. MNO was then fine-tuned using the loss in Eq. 14 at a learning rate of 10 \u22125 , halving the learning rate every 250 weight updates. RNN was pre-trained on one-step prediction for 40 epochs at a learning rate of 0.001, halving the learning rate every 500 weight updates. MNO was then fine-tuned using the loss in Eq. 14 at a learning rate of 10 \u22125 , halving the learning rate every 500 weight updates. Figure 13 depicts the convergence in test L 2 error of RNO trained on the KS equation. We obseve that RNO converges to a test L 2 error close to zero for all steps. Note that at times, training is slightly unstable for 1-step prediction, and this error propagates to larger degrees for 2-step, 3-step, etc., predictions. Empirically, we do not observe that this behavior prevents RNO from adequately learning the multi-step dynamics. In fact, observe that after minor increases in 1-step error in Figure 13, when the 1-step error decreases, the multi-step error decreases accordingly.\n\n\nE.4 Cloud cover equations\n\nWe generate 150 trajectories of the evolution of the non-stationary cloud cover equations [5,6] described in Appendix B using a 5th order Runge-Kutta Rosenbrock method. We use a integration step of 1 day and solve the system for 20 years for each trajectory. We train our RNO model on 45 trajectories, use 100 for calibration, and 5 for testing. In our models, we set the forecast time length to be \u2206T = 0.36 years.\n\nThe RNO used in our experiments used 64 Fourier modes across the temporal domain and used a width of 128. We use L = 5 layers and multi-step fine-tuning up to M = 10 steps. The MNO model used 4 layers, 32 Fourier modes, and a width of 96. We implement the baseline RNN with 3 layers and a 1024-dimensional hidden state. We fix the number of warm-up samples N \u03c4 = 5 for RNO experiments. In our experiments, we set M = 10 and \u03bb n = 1 for all n, for the corresponding terms in our data loss (Eq. 14). All models were optimized using Adam [59] with a batch size of 64. RNO was trained for 25 epochs at an initial learning rate of 10 \u22123 , while halving the learning rate every 300 weight updates. Both MNO and RNN were also directly trained using the loss in Eq. 14 at a learning rate of 10 \u22123 , halving the learning rate every 500 weight updates.\n\nF Comparisons to prior works F.1 Comparison to prior machine learning methods\n\nIn recent years, there has been a several works using machine learning for tipping point forecasting and prediction [4,8,[24][25][26][27][28][29]. For instance, [8] uses a convolutional long short-term memory (LSTM) model to predict specific types of bifurcations. [28] proposes a similar methodology of classifying critical transitions, smooth transitions, and no transitions. However, such methods are not directly applicable to the large-scale spatiotemporal systems (i.e., on function spaces) that motivate our work and require access to post-tipping data. In another vein of research, reservoir computing (RC)based methods have been used to learn the dynamics of 3d-Lorenz equation [4,[24][25][26][27]. However, RC approaches do not operate on function spaces and are thus not suitable for many large-scale spatiotemporal scientific computing problems.\n\nMore specifically, [4] makes the observation that when a tipping point happens, a well-trained machine learning model on the pre-tipping regime makes a significant error, using this signal as an indicator for tipping points. However, this method requires the use of post-tipping information to compare their model forecasts against. In our work, we extend this observation into our proposed tipping point forecasting method that does not require post-tipping data. Instead, we compare our forecast against the physics constraints or differential equations driving the underlying dynamics, so our method does not require post-tipping data.\n\nAmong other reservoir computing works, [25] trains a data-driven reservoir model on pre-tipping dynamics, conditioning the model on certain values of a bifurcation-inducing external parameter.\n\nWhile this method appears successful for simpler toy systems, for real-world systems (e.g., climate) there may be a variety of external parameters that may affect the dynamics of a non-stationarity system, which may be difficult to estimate and identify. [26] also uses reservoir computing but requires access to the full ground-truth system, which may be unknown in real-world systems. In contrast, our method can operate on partial or approximate physics knowledge of the underlying system. [27] also presents a method for tipping point prediction using reservoir computing, but this method suffers from the same scalability concerns of reservoir computing and also requires tipping points to be in the training set, both of which are addressed by our method.\n\nIn another realm of methods, [29] introduces an adversarial framework for tipping point prediction. However, this framework requires the querying of an oracle, which is extremely computationally expensive in large-scale spatiotemporal systems of interest.\n\n\nF.2 Comparisons with traditional early warning signals\n\nTo analyze the ability of traditional early warning signals (EWS) to predict tipping points beyond simple well-studied systems, we apply EWS to the cloud cover system [5,6] in our experiments. In particular, prior works have found that for some systems, increases in variance and autocorrelation are associated with critical slowing down, a phenomenon of slow recovery from perturbations for some systems approaching bifurcation-induced tipping points [7,60]. As such, we compute the variance and lag-1 autocorrelation using the methods in [60] for each of the five variables in the cloud cover system (see Appendix B for background). The variance and autocorrelation for three of these variables is shown in Figure 15.\n\nDespite autocorrelation and variance being established indicators of tipping phenomena in some systems, we observe that in this real-world system, autocorrelation never has a positive Kendall \u03c4 , and variance only has a positive Kendall \u03c4 in the cloud fraction setting. These results suggest that traditional EWS are not reliable as general indicators of tipping phenomena.\n\nWe follow the methodology described in [60], and we compute the variance and autocorrelation using the ewstools package [61]. We smooth each univariate time-series using LOWESS smoothing with a span of 0.1 times the length of the trajectory. We compute lag-1 autocorrelation and variance in rolling windows of length 0.25 times the length of the trajectory.\n\nG Technical discussion and analysis of methods\n\n\nG.1 Memory and inference times\n\nThe details for the best-performing RNO and MNO models in each of the experimental settings are provided in Appendix E. In this section we discuss and compare the memory usage and the inference times of RNO and MNO. Timing benchmarks were performed on one NVIDIA P100 GPU with 16 GB of memory.\n\nIn general comparisons of RNO and MNO with the same number of layers, Fourier modes, and width, RNO tends to have a significantly larger memory footprint, since all of the gating operations are implemented with Fourier integral operators. However, in situations where memory may be scarce, weight-sharing or factorization methods can be used to vastly reduce memory footprint [62]. Mixed-precision neural operators [63] can also be used to dramatically decrease memory usage and inference times. Apart from the additional Fourier layers for the gating operations, RNO inference has a warm-up period of N \u03c4 steps to achieve a reasonable hidden state representation. This warm-up period scales with O(N \u03c4 ), since the computation is computed serially. Despite RNO's longer inference times compared to MNO, we find that these inference times are still much faster than the numerical solvers for a variety of applications, particularly for larger-scale spatiotemporal problems.\n\nFor the non-stationary Lorenz-63 setting, our RNO model and MNO model both used 24 Fourier modes and had a width of 48. The respective memory usages are 8.1 MB and 1.9 MB, respectively. The respective inference times to generate an entire trajectory is 30.8 seconds and 11.1 seconds, respectively (averaged over 10 instances). While RNO is slower than MNO, it is still significantly faster than the numerical solver, which takes around 90 seconds to generate each trajectory.\n\nFor the non-stationary KS equation, we compare RNO and MNO models with 20 Fourier modes across both time and space and a width of 28. The respective memory usages are about 90 MB and 7 MB, and the respective inference times to generate a full trajectory are about 1.3 seconds and 0.3 seconds (both averaged over 100 instances). These are both significantly faster than the numerical solver, which takes about 19 seconds to simulate one trajectory.\n\nFor the simplified cloud cover experiments, the best-performing RNO model used 64 Fourier modes and had a width of 128, whereas the best-performing MNO model used 32 Fourier modes and had a width of 96. The corresponding memory usages are 254 MB and 10 MB, respectively. However, note that while these models are the best-performing for their architecture, they do not have the same hyperparameters. MNO with 64 modes and width 128 has a memory footprint of 34 MB. The inference time to generate an entire trajectory for RNO is 1.068 seconds, and for MNO the inference time is 0.223 seconds, averaged over 100 instances. Note that these are both orders of magnitude faster than the numerical solver, which takes about 100 seconds to generate an entire trajectory.\n\n\nG.2 On the choice of hyperparameters for RNO\n\nThe primary hyperparameters of our proposed RNO model is L, the number of RNO layers, the number of Fourier modes across each dimension of the input, the width (i.e., co-dimension) of each RNO layer, N \u03c4 , the number of warm-up intervals, and M , the number of auto-regressive steps used during training.\n\nIn general, RNO's L and width is analogous to the depth and width of standard neural networks, or the number of layers and dimensionality of the hidden state in RNN's. Increasing L allows for more non-linear and expressive mappings to be learned, and the number of parameters in the model increases linearly with L. In general, under our implementation we observe that increasing L beyond 5 or 10 layers does not confer additional benefits for the systems studied in this paper. We also observe that increasing the width of RNO rarely improves performance significantly, only marginally, if at all.\n\nThe number of Fourier modes necessary depends heavily on the underlying dynamics of the data. For instance, for highly turbulent and chaotic fluid flows, a large of number of modes is useful to effectively parameterize and capture high-frequency information. On the other hand, for laminar fluid flows, a large number of Fourier modes may not be necessary.\n\nAdjusting the optimal number of auto-regressive steps M used during training is a trade-off between ease of training and performance (in L 2 error) on longer time horizons. For large values of M , optimization may be difficult and complicated training policies (e.g., progressive steps of pre-training for someM < M , then fine-tuning at M , etc.) may be necessary to improve performance. However, we find that using some M > 1 is crucial to achieving good long-term performance. This trade-off can be observed in Table 5. In general, the choice of M may also depend on the system and the goals for the learned surrogate model. For instance, for highly chaotic systems, large M are less likely to provide benefit over smaller M , since chaotic systems tend to quickly decorrelate from the past, making precise long-term predictions very challenging [10].\n\nFinally, the choice of the number of warm-up intervals N \u03c4 also depends on the degree of nonstationarity of the system. For stationary Markovian systems, no memory warm-up is needed, in principle, so N \u03c4 = 0. For highly non-stationary systems (especially those with many latent variables), increasing N \u03c4 can lead to an improvement in model performance. Also, for chaotic systems that decorrelate from the past quickly, large values of N \u03c4 are likely to not bring better performance. In general, we find that N \u03c4 need not be very large; values between 3 and 10 appear to work well for most systems. We also observe that RNO performance appears to be quite robust to N \u03c4 , and tuning this hyperparameter often does not bring significant changes in performance.\n\n\nG.3 Limitations and discussion of proposed methods\n\nAs with all data-driven methods, the performance of RNO depends on its training data. On one hand, data-driven approaches are flexible, unconstrained, and have been shown to learn complex dynamics purely from data [10,64]. On the other hand, purely data-driven approaches often lack theoretical guarantees; in the realm of scientific computing such useful guarantees may be adhering to physically-meaningful conservation laws, for instance.\n\nDespite lacking many theoretical guarantees, our proposed conformal prediction method is capable of quantifying the distribution of model error with respect to the physics loss. That is, our method does not assume that the data-driven RNO (or other data-driven surrogate model) follows physical constraints perfectly in the pre-tipping regime. Using our rigorous proposed uncertainty quantification method, our tipping point prediction method is robust to model errors.\n\nSince RNO is purely data-driven, it can, in principle, be applied to learning the dynamics of arbitrary dynamical systems. For stationary dynamical systems, it is possible for RNO to learn to ignore historical information and thus to simplify to MNO. Unfortunately, if data is sparse or of lowresolution, it may be difficult for RNO to generalize and truly learn the underlying evolution operator of the system. Providing a heuristic rule-of-thumb for the minimum resolution of the training data that RNO needs to adequately learn the dynamics is difficult; this largely depends on the dynamics of the system in question. In general, providing training data at which the underlying physics is not resolved introduces an ill-posed learning problem. On the question of minimum amounts of training data, we find empirically that simple dynamics can be learned with O(1000) training pairs, whereas complex dynamics may need O(10000) training pairs to adequately learn. This is a very coarse estimate derived from our experimental settings and may not generalize to arbitrary systems.\n\nFigure 4 :\n4(a) RNO accurately forecasts tipping point 16 seconds ahead in non-stationary KS system. Right after the abrupt change at the tipping point, the physics loss decays for post-tipping. (b) MNO fails to accurately forecasting tipping point in KS system. A false-positive rate of \u03b1 = 0.07 is used.\n\nFigure 5 :\n5For the cloud cover model, using only a mass conservation constraint (as opposed to the full system), RNO is still capable of identifying the true tipping point of the system with an error of 0.03 years, predicting T = 2.45 years ahead, at \u03b1 = 0.07.\n\nFigure 7 :\n7Kinetic energy of the non-stationary KS system. The predicted tipping point aligns nearly exactly with the true tipping point of the system, which can be seen as a qualitative change in the kinetic energy of the system. The false-positive rate is \u03b1 = 0.07.\n\n\u2212 1 Figure 8 :\n18] and q t is the total water specific humidity [kg kg\u22121 ]. V is the surface wind speed [m s \u22121 ], \u2206R is the cloud-top radiative cooling per unit density [W m kg \u22121 ], which is a function of CO 2 and H 2 O, and s exp and q t,exp are export terms representing the effect of large-scale dynamics (synoptic eddies and Hadley circulation) transporting energy and moisture laterally out of the model domain into other regions. Lorenz-63 tipping point forecasting performance 64 seconds ahead using approximate physical constraints, where \u03b7 is the perturbation on the ODE parameters for the physical law.\n\nFigure 9 :\n9Comparison of tipping point forecasting performance between RNO and MNO for various false-positive rates \u03b1 for the KS equation. For all presented \u03b1, RNO outperforms MNO in tipping point forecasting error.\n\nFigure 10 :\n10Using the full ODE as the physical constraint, RNO is capable of identifying the true tipping point of the simplified cloud cover system with an error of 0.03 years, predicting T = 2.45 years ahead, at \u03b1 = 0.08.\n\nFigure 11 :\n11Tipping point forecasting performance for RNO for various false-positive rates \u03b1 in the simplified cloud cover setting. RNO achieves very low forecasting error for the majority of \u03b1. Further, RNO achieves approximately zero error for \u03b1 = 0.08.\n\nFigure 12 :\n12Comparison of tipping point forecasting performance between MNO and RNN for various false-positive rates \u03b1 in the simplified cloud cover setting. RNO (in\n\nFigure 13 :\n13Convergence plot of test relative L 2 error for RNO trained on the pre-tipping regime of the KS equation. We observe that RNO ultimately converges to near-zero error for all steps.\n\nFigure 14 :\n14(a) Physics loss threshold comparison between RNO and MNO for non-stationary Lorenz-63 system predicting 64 seconds ahead. (b) Physics loss threshold comparison between proposed RNO and the state-of-the-art baseline MNO in non-stationary Kuramoto-Sivashinsky system for forecasting the tipping point 16 seconds ahead. Negative numbers correspond to tipping points predicted early. RNO consistently outperforms MNO in tipping point forecasting for a given physics loss threshold, demonstrating that RNO learns the physics of the underlying system better that MNO.\n\nFigure 15 :\n15Variance and lag-1 autocorrelation applied as early warning signals to forecast impending tipping points in the surrogate cloud cover system. Kendall's \u03c4 values for (a) variance: \u22120.83, autocorrelation: \u22120.48, (b) variance: \u22120.69, autocorrelation: \u22120.59, and (c) variance: 0.89, autocorrelation: \u22120.33. Despite approaching a tipping point, autocorrelation did not tend to increase. Only the variance of cloud fraction increased.\n\n3 .\n3Propose a novel conformal prediction method to quantify deviations in model physics error in a statistically rigorous manner.4. Achieve accurate tipping point forecasting far in advance for PDE and ODE systems.5. Demonstrate high accuracy in tipping point forecasting using approximate and partial physical constraints, generalizing our method to settings when full physics laws are unknown.RNO block \n\nRNO block \nRNO block \nRNO block \n\n\n\n\nAt the forecast time t, we do haveLoss value \n\n1 \n\n\u03b1 \n\n( ) \n\nLoss CDF \n\nLoss empirical CDF \n\n\u03b5 \n\nCDF \n\n\n\nTable 2 :\n2Relative L 2 errors on non-stationary KS equation for different n-step prediction settings. RNO outperforms both MNO and the baseline RNN on every forecasting intervals.\n\n\n's physics loss has little2-step \n4-step \n8-step \n16-step \n\nRNO \n\n0.0234 0.0326 0.0449 0.0614 0.0852 \n\nMNO \n\n0.0246 \n0.0342 \n0.0472 \n0.0647 \n0.0860 \n\nRNN \n\n0.0285 \n0.0360 \n0.0481 \n0.0645 \n0.0868 \n\n\n\nTable 5 :\n5Relative L 2 errors on non-stationary Lorenz-63 for different n-step prediction settings and number of RNN fine-tuning steps M (denoted \"RNN-M \"). RNO outperforms RNN on every forecasting interval and for every RNN training setup.\nThe code is available at: https://github.com/neuraloperator/tipping-point-forecast. Preprint. Under review. arXiv:2308.08794v1 [cs.LG] 17 Aug 2023\nNote that\u02d9 u(t) can in principle be computed to arbitrary precision due to the discretization invariance of RNO in time. In practice, we find that approximating\u02d9 u(t) using finite difference methods is sufficient.\nAcknowledgments and Disclosure of FundingM. Liu-Schiaffini is supported in part by the Mellon Mays Undergradaute Fellowship. C.E. Singer and T. Schneider were supported by the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program and by Charles Trimble. A. Anandkumar is supported in part by Bren endowed chair.\nWatch your step: optimal policy in a tipping climate. Derek Lemoine, Christian Traeger, American Economic Journal: Economic Policy. 61Derek Lemoine and Christian Traeger. Watch your step: optimal policy in a tipping climate. American Economic Journal: Economic Policy, 6(1):137-166, 2014.\n\nPossible climate transitions from breakup of stratocumulus decks under greenhouse warming. Tapio Schneider, Colleen M Kaul, Kyle G Pressel, Nature Geoscience. 123Tapio Schneider, Colleen M Kaul, and Kyle G Pressel. Possible climate transitions from breakup of stratocumulus decks under greenhouse warming. Nature Geoscience, 12(3):163-167, 2019.\n\nLearning internal representations by error propagation. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, California Univ San Diego La Jolla Inst for Cognitive ScienceTechnical reportDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal represen- tations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.\n\nUsing machine learning to anticipate tipping points and extrapolate to post-tipping dynamics of non-stationary dynamical systems. Dhruvit Patel, Edward Ott, arXiv:2207.00521arXiv preprintDhruvit Patel and Edward Ott. Using machine learning to anticipate tipping points and extrapolate to post-tipping dynamics of non-stationary dynamical systems. arXiv preprint arXiv:2207.00521, 2022.\n\nStratocumulus-cumulus transition explained by bulk boundary layer theory. Clare E Singer, Tapio Schneider, 10.22541/essoar.168167352.24863390/v1J. Climate2023in reviewClare E. Singer and Tapio Schneider. Stratocumulus-cumulus transition explained by bulk boundary layer theory. J. Climate (in review), 2023. doi: 10.22541/essoar.168167352.24863390/ v1.\n\nCO 2 -driven stratocumulus cloud breakup in a bulk boundary layer model. Clare E Singer, Tapio Schneider, 10.22541/essoar.168167204.45220772/v1J. Climate. 2023in reviewClare E. Singer and Tapio Schneider. CO 2 -driven stratocumulus cloud breakup in a bulk boundary layer model. J. Climate (in review), 2023. doi: 10.22541/essoar.168167204.45220772/ v1.\n\nEarlywarning signals for critical transitions. Marten Scheffer, Jordi Bascompte, A William, Victor Brock, Brovkin, R Stephen, Vasilis Carpenter, Hermann Dakos, Held, Nature. 4617260Marten Scheffer, Jordi Bascompte, William A Brock, Victor Brovkin, Stephen R Carpenter, Vasilis Dakos, Hermann Held, Egbert H Van Nes, Max Rietkerk, and George Sugihara. Early- warning signals for critical transitions. Nature, 461(7260):53-59, 2009.\n\nDeep learning for early warning signals of tipping points. M Thomas, Bury, Induja Sujith, Marten Pavithran, Timothy M Scheffer, Madhur Lenton, Chris T Anand, Bauch, Proceedings of the National Academy of Sciences. 118392106140118Thomas M Bury, RI Sujith, Induja Pavithran, Marten Scheffer, Timothy M Lenton, Mad- hur Anand, and Chris T Bauch. Deep learning for early warning signals of tipping points. Proceedings of the National Academy of Sciences, 118(39):e2106140118, 2021.\n\nNikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2108.08481Neural operator: Learning maps between function spaces. arXiv preprintNikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint arXiv:2108.08481, 2021.\n\nLearning chaotic dynamics in dissipative systems. Zongyi Li, Miguel Liu-Schiaffini, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, Advances in Neural Information Processing Systems. 35Zongyi Li, Miguel Liu-Schiaffini, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Learning chaotic dynamics in dissipative systems. Advances in Neural Information Processing Systems, 35:16768-16781, 2022.\n\nTipping points in open systems: bifurcation, noise-induced and rate-dependent examples in the climate system. Peter Ashwin, Sebastian Wieczorek, Renato Vitolo, Peter Cox, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 370Peter Ashwin, Sebastian Wieczorek, Renato Vitolo, and Peter Cox. Tipping points in open systems: bifurcation, noise-induced and rate-dependent examples in the climate system. Philo- sophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 370(1962):1166-1184, 2012.\n\nTipping elements in the earth's climate system. Hermann Timothy M Lenton, Elmar Held, Jim W Kriegler, Wolfgang Hall, Stefan Lucht, Hans Joachim Rahmstorf, Schellnhuber, Proceedings of the national Academy of Sciences. the national Academy of Sciences105Timothy M Lenton, Hermann Held, Elmar Kriegler, Jim W Hall, Wolfgang Lucht, Stefan Rahmstorf, and Hans Joachim Schellnhuber. Tipping elements in the earth's climate system. Proceedings of the national Academy of Sciences, 105(6):1786-1793, 2008.\n\nExceeding 1.5\u00b0C global warming could trigger multiple climate tipping points. David I Armstrong Mckay, Arie Staal, Jesse F Abrams, Ricarda Winkelmann, Boris Sakschewski, Sina Loriani, Ingo Fetzer, Sarah E Cornell, Johan Rockstr\u00f6m, Timothy M Lenton, 10.1126/science.abn7950Science. 37766117950David I. Armstrong McKay, Arie Staal, Jesse F. Abrams, Ricarda Winkelmann, Boris Sakschewski, Sina Loriani, Ingo Fetzer, Sarah E. Cornell, Johan Rockstr\u00f6m, and Timothy M. Lenton. Exceeding 1.5\u00b0C global warming could trigger multiple climate tipping points. Science, 377(6611):eabn7950, 2022. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.abn7950.\n\nMechanisms and Impacts of Earth System Tipping Elements. Seaver Wang, Adrianna Foster, Elizabeth A Lenz, John D Kessler, Julienne C Stroeve, Liana O Anderson, Merritt Turetsky, Richard Betts, Sijia Zou, Wei Liu, William R Boos, Zeke Hausfather, 10.1029/2021RG000757Reviews of Geophysics. 611Seaver Wang, Adrianna Foster, Elizabeth A. Lenz, John D. Kessler, Julienne C. Stroeve, Liana O. Anderson, Merritt Turetsky, Richard Betts, Sijia Zou, Wei Liu, William R. Boos, and Zeke Hausfather. Mechanisms and Impacts of Earth System Tipping Elements. Reviews of Geophysics, 61(1):e2021RG000757, 2023. ISSN 8755-1209, 1944-9208. doi: 10.1029/2021RG000757.\n\nThermohaline ocean circulation. Encyclopedia of quaternary sciences. Stefan Rahmstorf, Stefan Rahmstorf. Thermohaline ocean circulation. Encyclopedia of quaternary sciences, 5, 2006.\n\nTransient and equilibrium responses of the atlantic overturning circulation to warming in coupled climate models: The role of temperature and salinity. Andrew F David B Bonan, Emily R Thompson, Shantong Newsom, Maria Sun, Rugenstein, Journal of Climate. 3515David B Bonan, Andrew F Thompson, Emily R Newsom, Shantong Sun, and Maria Rugenstein. Transient and equilibrium responses of the atlantic overturning circulation to warming in coupled climate models: The role of temperature and salinity. Journal of Climate, 35(15):5173-5193, 2022.\n\nMulticentury instability of the atlantic meridional circulation in rapid warming simulations with giss modele2. David Rind, A Gavin, Jeff Schmidt, Ron Jonas, Larissa Miller, Max Nazarenko, Joy Kelley, Romanski, Journal of Geophysical Research: Atmospheres. 12312David Rind, Gavin A Schmidt, Jeff Jonas, Ron Miller, Larissa Nazarenko, Max Kelley, and Joy Romanski. Multicentury instability of the atlantic meridional circulation in rapid warming simulations with giss modele2. Journal of Geophysical Research: Atmospheres, 123(12): 6331-6355, 2018.\n\nIce plug prevents irreversible discharge from east antarctica. M Mengel, A Levermann, 10.1038/nclimate2226Nature Climate Change. 4M. Mengel and A. Levermann. Ice plug prevents irreversible discharge from east antarctica. Nature Climate Change, 4:451-455, 2014. doi: 10.1038/nclimate2226.\n\nRetreat of pine island glacier controlled by marine ice-sheet instability. Lionel Favier, Gael Durand, L Stephen, Cornford, Olivier Hilmar Gudmundsson, Fabien Gagliardini, Thomas Gillet-Chaulet, Zwinger, Anne M Le Payne, Brocq, Nature Climate Change. 42Lionel Favier, Gael Durand, Stephen L Cornford, G Hilmar Gudmundsson, Olivier Gagliardini, Fabien Gillet-Chaulet, Thomas Zwinger, AJ Payne, and Anne M Le Brocq. Retreat of pine island glacier controlled by marine ice-sheet instability. Nature Climate Change, 4(2):117-121, 2014.\n\nSensitivity of the dynamics of pine island glacier, west antarctica, to climate forcing for the next 50 years. H Seroussi, Morlighem, Rignot, Mouginot, M Larour, A Schodlok, Khazendar, The Cryosphere. 85H Seroussi, M Morlighem, E Rignot, J Mouginot, E Larour, M Schodlok, and A Khazendar. Sensitivity of the dynamics of pine island glacier, west antarctica, to climate forcing for the next 50 years. The Cryosphere, 8(5):1699-1710, 2014.\n\nOcean-induced melt volume directly paces ice loss from pine island glacier. Ian Joughin, Daniel Shapero, Pierre Dutrieux, Ben Smith, Science advances. 7435738Ian Joughin, Daniel Shapero, Pierre Dutrieux, and Ben Smith. Ocean-induced melt volume directly paces ice loss from pine island glacier. Science advances, 7(43):eabi5738, 2021.\n\nA simplified, dataconstrained approach to estimate the permafrost carbon-climate feedback. Charles D Koven, Christina Schuur, Sch\u00e4del, Bohn, Guangsheng Burke, Xiaodong Chen, Philippe Chen, Guido Ciais, Jennifer W Grosse, Harden, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 37320140423Charles D Koven, EAG Schuur, Christina Sch\u00e4del, TJ Bohn, EJ Burke, Guangsheng Chen, Xiaodong Chen, Philippe Ciais, Guido Grosse, Jennifer W Harden, et al. A simplified, data- constrained approach to estimate the permafrost carbon-climate feedback. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 373 (2054):20140423, 2015.\n\nPermafrost carbon emissions in a changing arctic. R Kimberley, Miner, R Merritt, Edward Turetsky, Annett Malina, Johanna Bartsch, David Tamminen, Andreas Mcguire, Colm Fix, Sweeney, D Clayton, Charles E Elder, Miller, Nature Reviews Earth & Environment. 31Kimberley R Miner, Merritt R Turetsky, Edward Malina, Annett Bartsch, Johanna Tamminen, A David McGuire, Andreas Fix, Colm Sweeney, Clayton D Elder, and Charles E Miller. Permafrost carbon emissions in a changing arctic. Nature Reviews Earth & Environment, 3(1): 55-67, 2022.\n\nUsing machine learning to predict statistical properties of non-stationary dynamical processes: System climate, regime transitions, and the effect of stochasticity. Dhruvit Patel, Daniel Canaday, Michelle Girvan, Andrew Pomerance, Edward Ott, Chaos: An Interdisciplinary Journal of Nonlinear Science. 31333149Dhruvit Patel, Daniel Canaday, Michelle Girvan, Andrew Pomerance, and Edward Ott. Using machine learning to predict statistical properties of non-stationary dynamical processes: System climate, regime transitions, and the effect of stochasticity. Chaos: An Interdisciplinary Journal of Nonlinear Science, 31(3):033149, 2021.\n\nMachine learning prediction of critical transition and system collapse. Ling-Wei Kong, Hua-Wei Fan, Celso Grebogi, Ying-Cheng Lai, Physical Review Research. 3113090Ling-Wei Kong, Hua-Wei Fan, Celso Grebogi, and Ying-Cheng Lai. Machine learning prediction of critical transition and system collapse. Physical Review Research, 3(1):013090, 2021.\n\nPredicting critical transitions in multiscale dynamical systems using reservoir computing. Ludovico Theo Soon Hoe Lim, Woosok Giorgini, John S Moon, Wettlaufer, Chaos: An Interdisciplinary Journal of Nonlinear Science. 3012123126Soon Hoe Lim, Ludovico Theo Giorgini, Woosok Moon, and John S Wettlaufer. Predicting critical transitions in multiscale dynamical systems using reservoir computing. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(12):123126, 2020.\n\nTipping point detection using reservoir computing. Xin Li, Qunxi Zhu, Chengli Zhao, Xuzhe Qian, Xue Zhang, Xiaojun Duan, Wei Lin, Research. 6174Xin Li, Qunxi Zhu, Chengli Zhao, Xuzhe Qian, Xue Zhang, Xiaojun Duan, and Wei Lin. Tipping point detection using reservoir computing. Research, 6:0174, 2023.\n\nMachine learning methods trained on simple models can predict critical transitions in complex natural systems. Smita Deb, Sahil Sidheekh, F Christopher, Clements, C Narayanan, Partha S Krishnan, Dutta, Royal Society Open Science. 92211475Smita Deb, Sahil Sidheekh, Christopher F Clements, Narayanan C Krishnan, and Partha S Dutta. Machine learning methods trained on simple models can predict critical transitions in complex natural systems. Royal Society Open Science, 9(2):211475, 2022.\n\nA generative adversarial network for climate tipping point discovery (tip-gan). Jennifer Sleeman, David Chung, Anand Gnanadesikan, Jay Brett, Yannis Kevrekidis, Marisa Hughes, Thomas Haine, Marie-Aude Pradal, Renske Gelderloos, Chace Ashcraft, arXiv:2302.10274arXiv preprintJennifer Sleeman, David Chung, Anand Gnanadesikan, Jay Brett, Yannis Kevrekidis, Marisa Hughes, Thomas Haine, Marie-Aude Pradal, Renske Gelderloos, Chace Ashcraft, et al. A generative adversarial network for climate tipping point discovery (tip-gan). arXiv preprint arXiv:2302.10274, 2023.\n\nThe era5 global reanalysis. Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr\u00e1s Hor\u00e1nyi, Joaqu\u00edn Mu\u00f1oz-Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, Quarterly Journal of the Royal Meteorological Society. 146730Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr\u00e1s Hor\u00e1nyi, Joaqu\u00edn Mu\u00f1oz- Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. Quarterly Journal of the Royal Meteorological Society, 146(730):1999-2049, 2020.\n\nA tutorial on conformal prediction. Glenn Shafer, Vladimir Vovk, Journal of Machine Learning Research. 93Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(3), 2008.\n\nAsymptotic minimax character of the sample distribution function and of the classical multinomial estimator. Aryeh Dvoretzky, Jack Kiefer, Jacob Wolfowitz, The Annals of Mathematical Statistics. Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. Asymptotic minimax character of the sam- ple distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics, pages 642-669, 1956.\n\nThe tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability. Pascal Massart, Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability, pages 1269-1283, 1990.\n\nPhysical invariant measures and tipping probabilities for chaotic attractors of asymptotically autonomous systems. Peter Ashwin, Julian Newman, The European Physical Journal Special Topics. 23016Peter Ashwin and Julian Newman. Physical invariant measures and tipping probabilities for chaotic attractors of asymptotically autonomous systems. The European Physical Journal Special Topics, 230(16):3235-3248, 2021.\n\nTipping phenomena in typical dynamical systems subjected to parameter drift. B\u00e1lint Kasz\u00e1s, Ulrike Feudel, Tam\u00e1s T\u00e9l, Scientific reports. 91B\u00e1lint Kasz\u00e1s, Ulrike Feudel, and Tam\u00e1s T\u00e9l. Tipping phenomena in typical dynamical systems subjected to parameter drift. Scientific reports, 9(1):1-12, 2019.\n\nEarlywarning signals for critical transitions. Marten Scheffer, Jordi Bascompte, William A Brock, Victor Brovkin, Stephen R Carpenter, Vasilis Dakos, Hermann Held, Egbert H Van Nes, Max Rietkerk, George Sugihara, 10.1038/nature08227Nature. 4617260Marten Scheffer, Jordi Bascompte, William A. Brock, Victor Brovkin, Stephen R. Carpenter, Vasilis Dakos, Hermann Held, Egbert H. van Nes, Max Rietkerk, and George Sugihara. Early- warning signals for critical transitions. Nature, 461(7260):53-59, 2009. ISSN 0028-0836. doi: 10.1038/nature08227.\n\nEarly warning of climate tipping points. Timothy M Lenton, 10.1038/nclimate1143Nature Climate Change. 14Timothy M. Lenton. Early warning of climate tipping points. Nature Climate Change, 1(4): 201-209, 2011. ISSN 1758-678X. doi: 10.1038/nclimate1143.\n\n. Marten Scheffer, Stephen R Carpenter, Timothy M Lenton, Jordi Bascompte, William Brock, Vasilis Dakos, Johan Van De Koppel, Ingrid A Van De Leemput, Simon A Levin, Egbert H Van Nes, Mercedes Pascual, John Vandermeer, 10.1126/science.1225244Anticipating Critical Transitions. Science. 3386105Marten Scheffer, Stephen R. Carpenter, Timothy M. Lenton, Jordi Bascompte, William Brock, Vasilis Dakos, Johan van de Koppel, Ingrid A. van de Leemput, Simon A. Levin, Egbert H. van Nes, Mercedes Pascual, and John Vandermeer. Anticipating Critical Transitions. Science, 338 (6105):344-348, 2012. ISSN 0036-8075. doi: 10.1126/science.1225244.\n\nEarly warning of climate tipping points. Timothy M Lenton, Nature climate change. 14Timothy M Lenton. Early warning of climate tipping points. Nature climate change, 1(4): 201-209, 2011.\n\nTipping points: Early warning and wishful thinking. D Peter, Ditlevsen, J Sigfus, Johnsen, Geophysical Research Letters. 3719Peter D Ditlevsen and Sigfus J Johnsen. Tipping points: Early warning and wishful thinking. Geophysical Research Letters, 37(19), 2010.\n\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, arXiv:2302.06534Fourier-rnns for modelling noisy physics data. arXiv preprintVignesh Gopakumar, Stanislas Pamela, and Lorenzo Zanisi. Fourier-rnns for modelling noisy physics data. arXiv preprint arXiv:2302.06534, 2023.\n\nLatent ordinary differential equations for irregularly-sampled time series. Yulia Rubanova, T Q Ricky, David K Chen, Duvenaud, Advances in neural information processing systems. 32Yulia Rubanova, Ricky TQ Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. Advances in neural information processing systems, 32, 2019.\n\nNeural ordinary differential equation based recurrent neural network model. Mansura Habiba, A Barak, Pearlmutter, 2020 31st Irish signals and systems conference (ISSC). IEEEMansura Habiba and Barak A Pearlmutter. Neural ordinary differential equation based recurrent neural network model. In 2020 31st Irish signals and systems conference (ISSC), pages 1-6. IEEE, 2020.\n\nLearning macroscopic internal variables and history dependence from microscopic models. Burigede Liu, Margaret Trautner, M Andrew, Kaushik Stuart, Bhattacharya, arXiv:2210.17443arXiv preprintBurigede Liu, Margaret Trautner, Andrew M Stuart, and Kaushik Bhattacharya. Learning macroscopic internal variables and history dependence from microscopic models. arXiv preprint arXiv:2210.17443, 2022.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2003.03485Neural operator: Graph kernel network for partial differential equations. arXiv preprintZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485, 2020.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2010.08895Fourier neural operator for parametric partial differential equations. arXiv preprintZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differen- tial equations. arXiv preprint arXiv:2010.08895, 2020.\n\nZachary E Md Ashiqur Rahman, Kamyar Ross, Azizzadenesheli, arXiv:2204.11127U-shaped neural operators. U-noarXiv preprintMd Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators. arXiv preprint arXiv:2204.11127, 2022.\n\nLearning phrase representations using rnn encoderdecoder for statistical machine translation. Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, arXiv:1406.1078arXiv preprintKyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder- decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n\nNonparametric quantile estimation. Ichiro Takeuchi, Quoc Le, Timothy Sears, Alexander Smola, Journal of Machine Learning Research. 7Ichiro Takeuchi, Quoc Le, Timothy Sears, Alexander Smola, et al. Nonparametric quantile estimation. Journal of Machine Learning Research, 7:1231-1264, 2006.\n\nOff-policy risk assessment in contextual bandits. Audrey Huang, Liu Leqi, Zachary Lipton, Kamyar Azizzadenesheli, Advances in Neural Information Processing Systems. 34Audrey Huang, Liu Leqi, Zachary Lipton, and Kamyar Azizzadenesheli. Off-policy risk assessment in contextual bandits. Advances in Neural Information Processing Systems, 34: 23714-23726, 2021.\n\nDeterministic nonperiodic flow. N Edward, Lorenz, Journal of atmospheric sciences. 202Edward N Lorenz. Deterministic nonperiodic flow. Journal of atmospheric sciences, 20(2): 130-141, 1963.\n\nSolar geoengineering may not prevent strong warming from direct effects of CO 2 on stratocumulus cloud cover. Tapio Schneider, Colleen M Kaul, Kyle G Pressel, 10.1073/pnas.2003730117Proc. Natl. Acad. Sci. Natl. Acad. Sci117Tapio Schneider, Colleen M. Kaul, and Kyle G. Pressel. Solar geoengineering may not prevent strong warming from direct effects of CO 2 on stratocumulus cloud cover. Proc. Natl. Acad. Sci., 117(48):30179-30185, 2020. ISSN 10916490. doi: 10.1073/pnas.2003730117.\n\nIntermittency and the lorenz model. Paul Manneville, Yves Pomeau, Physics Letters A. 751-2Paul Manneville and Yves Pomeau. Intermittency and the lorenz model. Physics Letters A, 75 (1-2):1-2, 1979.\n\nBulk boundary-layer concepts for simplified models of tropical dynamics. Bjorn Stevens, 0935-4964. doi: 10.1007/ s00162-006-0032-zTheor. Comput. Fluid Dyn. 20Bjorn Stevens. Bulk boundary-layer concepts for simplified models of tropical dynam- ics. Theor. Comput. Fluid Dyn., 20(5-6):279-304, 2006. ISSN 0935-4964. doi: 10.1007/ s00162-006-0032-z.\n\nMoisture transport, lower-tropospheric stability, and decoupling of cloud-topped boundary layers. Christopher S Bretherton, C Matthew, Wyant, J. Atmos. Sci. 54Christopher S. Bretherton and Matthew C Wyant. Moisture transport, lower-tropospheric stability, and decoupling of cloud-topped boundary layers. J. Atmos. Sci., 54:148-167, 1997.\n\nA simple model for stratocumulus to shallow cumulus cloud transitions. D Chung, J Teixeira, 10.1175/JCLI-D-11-00105.1J. Climate. 257D. Chung and J. Teixeira. A simple model for stratocumulus to shallow cumulus cloud transi- tions. J. Climate, 25(7):2547-2554, 2012. ISSN 0894-8755. doi: 10.1175/JCLI-D-11-00105.1.\n\nThe cumulus and stratocumulus CloudSat-CALIPSO dataset (CASCCAD). Gr\u00e9gory Cesana, Anthony D Del Genio, H\u00e9l\u00e8ne Chepfer, 10.5194/essd-11-1745-2019Earth Syst. Sci. Data. 114Gr\u00e9gory Cesana, Anthony D. Del Genio, and H\u00e9l\u00e8ne Chepfer. The cumulus and stratocumulus CloudSat-CALIPSO dataset (CASCCAD). Earth Syst. Sci. Data, 11(4):1745-1764, 2019. ISSN 1866-3516. doi: 10.5194/essd-11-1745-2019.\n\nClimatology of cloud-top radiative cooling in marine shallow clouds. Youtong Zheng, Yannian Zhu, Daniel Rosenfeld, Zhanqing Li, 10.1029/2021GL094676Geophys. Res. Lett. 4819Youtong Zheng, Yannian Zhu, Daniel Rosenfeld, and Zhanqing Li. Climatology of cloud-top radiative cooling in marine shallow clouds. Geophys. Res. Lett., 48(19):e2021GL094676, 2021. ISSN 0094-8276. doi: 10.1029/2021GL094676.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nMethods for detecting early warnings of critical transitions in time series illustrated using simulated ecological data. Vasilis Dakos, R Stephen, Carpenter, A William, Aaron M Brock, Vishwesha Ellison, Guttal, R Anthony, Sonia Ives, Valerie K\u00e9fi, Livina, A David, Egbert H Seekell, Van Nes, PloS one. 7741010Vasilis Dakos, Stephen R Carpenter, William A Brock, Aaron M Ellison, Vishwesha Guttal, Anthony R Ives, Sonia K\u00e9fi, Valerie Livina, David A Seekell, Egbert H van Nes, et al. Methods for detecting early warnings of critical transitions in time series illustrated using simulated ecological data. PloS one, 7(7):e41010, 2012.\n\newstools: A python package for early warning signals of bifurcations in time series data. M Thomas, Bury, Journal of Open Source Software. 8825038Thomas M Bury. ewstools: A python package for early warning signals of bifurcations in time series data. Journal of Open Source Software, 8(82):5038, 2023.\n\nAlasdair Tran, Alexander Mathews, Lexing Xie, Cheng Soon Ong, arXiv:2111.13802Factorized fourier neural operators. arXiv preprintAlasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators. arXiv preprint arXiv:2111.13802, 2021.\n\nSpeeding up fourier neural operators via mixed precision. Colin White, Renbo Tu, Jean Kossaifi, Gennady Pekhimenko, Kamyar Azizzadenesheli, Anima Anandkumar, arXiv:2307.15034arXiv preprintColin White, Renbo Tu, Jean Kossaifi, Gennady Pekhimenko, Kamyar Azizzadenesheli, and Anima Anandkumar. Speeding up fourier neural operators via mixed precision. arXiv preprint arXiv:2307.15034, 2023.\n\nJaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, arXiv:2202.11214A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprintJaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.\n", "annotations": {"author": "[{\"end\":102,\"start\":75},{\"end\":138,\"start\":103},{\"end\":178,\"start\":139},{\"end\":232,\"start\":179},{\"end\":273,\"start\":233},{\"end\":332,\"start\":274},{\"end\":374,\"start\":333}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":82},{\"end\":121,\"start\":114},{\"end\":161,\"start\":147},{\"end\":194,\"start\":186},{\"end\":256,\"start\":249},{\"end\":296,\"start\":281},{\"end\":357,\"start\":350}]", "author_first_name": "[{\"end\":81,\"start\":75},{\"end\":113,\"start\":103},{\"end\":144,\"start\":139},{\"end\":146,\"start\":145},{\"end\":185,\"start\":179},{\"end\":238,\"start\":233},{\"end\":248,\"start\":239},{\"end\":280,\"start\":274},{\"end\":338,\"start\":333},{\"end\":349,\"start\":339}]", "author_affiliation": "[{\"end\":101,\"start\":87},{\"end\":137,\"start\":123},{\"end\":177,\"start\":163},{\"end\":231,\"start\":217},{\"end\":272,\"start\":258},{\"end\":331,\"start\":317},{\"end\":373,\"start\":359}]", "title": "[{\"end\":72,\"start\":1},{\"end\":446,\"start\":375}]", "venue": null, "abstract": "[{\"end\":1746,\"start\":448}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2376,\"start\":2373},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2378,\"start\":2376},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3140,\"start\":3137},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3295,\"start\":3292},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3297,\"start\":3295},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3344,\"start\":3341},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3712,\"start\":3709},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3740,\"start\":3737},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3770,\"start\":3767},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4045,\"start\":4042},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4086,\"start\":4083},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4314,\"start\":4311},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4671,\"start\":4668},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5105,\"start\":5101},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6127,\"start\":6124},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6129,\"start\":6127},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6321,\"start\":6317},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6565,\"start\":6561},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6569,\"start\":6565},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6573,\"start\":6569},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6640,\"start\":6636},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6644,\"start\":6640},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6648,\"start\":6644},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6676,\"start\":6672},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6680,\"start\":6676},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6684,\"start\":6680},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6688,\"start\":6684},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6710,\"start\":6706},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6713,\"start\":6710},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6746,\"start\":6743},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6867,\"start\":6864},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6869,\"start\":6867},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6871,\"start\":6869},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7052,\"start\":7049},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7055,\"start\":7052},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7198,\"start\":7195},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7316,\"start\":7313},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7318,\"start\":7316},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7322,\"start\":7318},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7326,\"start\":7322},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7330,\"start\":7326},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7334,\"start\":7330},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7338,\"start\":7334},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7342,\"start\":7338},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7934,\"start\":7930},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8147,\"start\":8144},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8150,\"start\":8147},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9033,\"start\":9029},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9421,\"start\":9417},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9424,\"start\":9421},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9821,\"start\":9818},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9823,\"start\":9821},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10897,\"start\":10894},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11084,\"start\":11080},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11087,\"start\":11084},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11090,\"start\":11087},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11267,\"start\":11263},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11607,\"start\":11603},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11611,\"start\":11607},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11615,\"start\":11611},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11791,\"start\":11787},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11794,\"start\":11791},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11876,\"start\":11872},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12009,\"start\":12006},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12011,\"start\":12009},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12333,\"start\":12329},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12336,\"start\":12333},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13063,\"start\":13060},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13065,\"start\":13063},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13069,\"start\":13065},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13073,\"start\":13069},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13077,\"start\":13073},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13081,\"start\":13077},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13085,\"start\":13081},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13089,\"start\":13085},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13140,\"start\":13137},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13142,\"start\":13140},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13173,\"start\":13169},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13240,\"start\":13237},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13243,\"start\":13240},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13566,\"start\":13563},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13569,\"start\":13566},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14116,\"start\":14112},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14120,\"start\":14116},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14124,\"start\":14120},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14237,\"start\":14233},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14241,\"start\":14237},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14245,\"start\":14241},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14308,\"start\":14305},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16120,\"start\":16117},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":17612,\"start\":17608},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18347,\"start\":18343},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21059,\"start\":21055},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21587,\"start\":21583},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21829,\"start\":21825},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22290,\"start\":22286},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23321,\"start\":23318},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23324,\"start\":23321},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25343,\"start\":25339},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25346,\"start\":25343},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27384,\"start\":27380},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27688,\"start\":27685},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29036,\"start\":29033},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29310,\"start\":29306},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29343,\"start\":29340},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29438,\"start\":29434},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33029,\"start\":33026},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33031,\"start\":33029},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33502,\"start\":33499},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33505,\"start\":33502},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35353,\"start\":35350},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36385,\"start\":36382},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":36594,\"start\":36590},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":36747,\"start\":36743},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36905,\"start\":36901},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36938,\"start\":36935},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41858,\"start\":41855},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":41860,\"start\":41858},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":41924,\"start\":41920},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":43802,\"start\":43798},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":43805,\"start\":43802},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":43870,\"start\":43866},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":43873,\"start\":43870},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":43875,\"start\":43873},{\"end\":45328,\"start\":45316},{\"end\":46151,\"start\":46148},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49647,\"start\":49643},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49808,\"start\":49804},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":50103,\"start\":50099},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":52020,\"start\":52016},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":52542,\"start\":52538},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":54182,\"start\":54178},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":55582,\"start\":55579},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":55584,\"start\":55582},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":56445,\"start\":56441},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":56948,\"start\":56945},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56950,\"start\":56948},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":56954,\"start\":56950},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":56958,\"start\":56954},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":56962,\"start\":56958},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":56966,\"start\":56962},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":56970,\"start\":56966},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":56974,\"start\":56970},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56993,\"start\":56990},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":57098,\"start\":57094},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":57519,\"start\":57516},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":57523,\"start\":57519},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":57527,\"start\":57523},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":57531,\"start\":57527},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":57535,\"start\":57531},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":57710,\"start\":57707},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":58371,\"start\":58367},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":58781,\"start\":58777},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":59019,\"start\":59015},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":59318,\"start\":59314},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":59769,\"start\":59766},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":59771,\"start\":59769},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":60054,\"start\":60051},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":60057,\"start\":60054},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":60143,\"start\":60139},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":60738,\"start\":60734},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":60819,\"start\":60815},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":61810,\"start\":61806},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":61849,\"start\":61845},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":66260,\"start\":66256},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":67295,\"start\":67291},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":67298,\"start\":67295},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":72749,\"start\":72748},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":72834,\"start\":72833}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":69376,\"start\":69070},{\"attributes\":{\"id\":\"fig_2\"},\"end\":69639,\"start\":69377},{\"attributes\":{\"id\":\"fig_4\"},\"end\":69909,\"start\":69640},{\"attributes\":{\"id\":\"fig_5\"},\"end\":70525,\"start\":69910},{\"attributes\":{\"id\":\"fig_6\"},\"end\":70743,\"start\":70526},{\"attributes\":{\"id\":\"fig_7\"},\"end\":70970,\"start\":70744},{\"attributes\":{\"id\":\"fig_9\"},\"end\":71229,\"start\":70971},{\"attributes\":{\"id\":\"fig_10\"},\"end\":71398,\"start\":71230},{\"attributes\":{\"id\":\"fig_11\"},\"end\":71594,\"start\":71399},{\"attributes\":{\"id\":\"fig_12\"},\"end\":72172,\"start\":71595},{\"attributes\":{\"id\":\"fig_13\"},\"end\":72616,\"start\":72173},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":73060,\"start\":72617},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":73166,\"start\":73061},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":73348,\"start\":73167},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":73548,\"start\":73349},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":73791,\"start\":73549}]", "paragraph": "[{\"end\":2859,\"start\":1762},{\"end\":3345,\"start\":2861},{\"end\":3614,\"start\":3347},{\"end\":3996,\"start\":3625},{\"end\":4265,\"start\":4022},{\"end\":5266,\"start\":4267},{\"end\":6130,\"start\":5268},{\"end\":6872,\"start\":6132},{\"end\":7199,\"start\":6874},{\"end\":7637,\"start\":7201},{\"end\":8895,\"start\":7639},{\"end\":9638,\"start\":8897},{\"end\":10227,\"start\":9640},{\"end\":10530,\"start\":10229},{\"end\":10755,\"start\":10532},{\"end\":11507,\"start\":10772},{\"end\":12159,\"start\":11509},{\"end\":13434,\"start\":12161},{\"end\":14125,\"start\":13436},{\"end\":14815,\"start\":14127},{\"end\":15366,\"start\":14833},{\"end\":15996,\"start\":15424},{\"end\":16521,\"start\":15998},{\"end\":16697,\"start\":16569},{\"end\":17348,\"start\":16773},{\"end\":17770,\"start\":17412},{\"end\":18255,\"start\":17806},{\"end\":18571,\"start\":18257},{\"end\":18906,\"start\":18573},{\"end\":19133,\"start\":18978},{\"end\":19248,\"start\":19205},{\"end\":19532,\"start\":19331},{\"end\":19784,\"start\":19599},{\"end\":20075,\"start\":19953},{\"end\":20236,\"start\":20102},{\"end\":20335,\"start\":20282},{\"end\":20635,\"start\":20337},{\"end\":20989,\"start\":20657},{\"end\":21278,\"start\":20991},{\"end\":22368,\"start\":21308},{\"end\":23052,\"start\":22370},{\"end\":23325,\"start\":23244},{\"end\":23710,\"start\":23327},{\"end\":25217,\"start\":23772},{\"end\":25347,\"start\":25252},{\"end\":25433,\"start\":25349},{\"end\":26056,\"start\":25477},{\"end\":26757,\"start\":26058},{\"end\":27259,\"start\":26782},{\"end\":27478,\"start\":27295},{\"end\":27689,\"start\":27557},{\"end\":28164,\"start\":27691},{\"end\":28578,\"start\":28166},{\"end\":28799,\"start\":28632},{\"end\":29344,\"start\":28855},{\"end\":30152,\"start\":29381},{\"end\":30384,\"start\":30154},{\"end\":31446,\"start\":30429},{\"end\":31982,\"start\":31476},{\"end\":32225,\"start\":32054},{\"end\":32734,\"start\":32227},{\"end\":33627,\"start\":32797},{\"end\":34215,\"start\":33629},{\"end\":35175,\"start\":34217},{\"end\":36262,\"start\":35190},{\"end\":36522,\"start\":36299},{\"end\":36817,\"start\":36524},{\"end\":37017,\"start\":36819},{\"end\":37960,\"start\":37058},{\"end\":38095,\"start\":37962},{\"end\":38344,\"start\":38132},{\"end\":38603,\"start\":38427},{\"end\":38950,\"start\":38605},{\"end\":39932,\"start\":38952},{\"end\":40282,\"start\":39934},{\"end\":41029,\"start\":40336},{\"end\":41384,\"start\":41031},{\"end\":41783,\"start\":41386},{\"end\":42045,\"start\":41832},{\"end\":42656,\"start\":42402},{\"end\":43461,\"start\":42658},{\"end\":43618,\"start\":43463},{\"end\":43876,\"start\":43721},{\"end\":44606,\"start\":43878},{\"end\":44683,\"start\":44608},{\"end\":44946,\"start\":44723},{\"end\":46844,\"start\":44990},{\"end\":46892,\"start\":46846},{\"end\":47846,\"start\":46954},{\"end\":48196,\"start\":47906},{\"end\":48650,\"start\":48198},{\"end\":49306,\"start\":48689},{\"end\":50042,\"start\":49308},{\"end\":50958,\"start\":50063},{\"end\":51267,\"start\":50960},{\"end\":51901,\"start\":51307},{\"end\":52387,\"start\":51903},{\"end\":52822,\"start\":52389},{\"end\":53648,\"start\":52857},{\"end\":54028,\"start\":53650},{\"end\":55459,\"start\":54030},{\"end\":55904,\"start\":55489},{\"end\":56748,\"start\":55906},{\"end\":56827,\"start\":56750},{\"end\":57686,\"start\":56829},{\"end\":58326,\"start\":57688},{\"end\":58520,\"start\":58328},{\"end\":59283,\"start\":58522},{\"end\":59540,\"start\":59285},{\"end\":60318,\"start\":59599},{\"end\":60693,\"start\":60320},{\"end\":61052,\"start\":60695},{\"end\":61100,\"start\":61054},{\"end\":61428,\"start\":61135},{\"end\":62403,\"start\":61430},{\"end\":62880,\"start\":62405},{\"end\":63329,\"start\":62882},{\"end\":64094,\"start\":63331},{\"end\":64447,\"start\":64143},{\"end\":65047,\"start\":64449},{\"end\":65405,\"start\":65049},{\"end\":66261,\"start\":65407},{\"end\":67022,\"start\":66263},{\"end\":67517,\"start\":67077},{\"end\":67988,\"start\":67519},{\"end\":69069,\"start\":67990}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4021,\"start\":3997},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15423,\"start\":15367},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16568,\"start\":16522},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16772,\"start\":16698},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17411,\"start\":17349},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18977,\"start\":18907},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19204,\"start\":19134},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19330,\"start\":19249},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19598,\"start\":19533},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19837,\"start\":19785},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19897,\"start\":19837},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19952,\"start\":19897},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20101,\"start\":20076},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20281,\"start\":20237},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23122,\"start\":23053},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23243,\"start\":23122},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23771,\"start\":23711},{\"attributes\":{\"id\":\"formula_17\"},\"end\":25251,\"start\":25218},{\"attributes\":{\"id\":\"formula_18\"},\"end\":25476,\"start\":25434},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27556,\"start\":27479},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28854,\"start\":28800},{\"attributes\":{\"id\":\"formula_21\"},\"end\":30428,\"start\":30385},{\"attributes\":{\"id\":\"formula_22\"},\"end\":32053,\"start\":31983},{\"attributes\":{\"id\":\"formula_23\"},\"end\":38131,\"start\":38096},{\"attributes\":{\"id\":\"formula_24\"},\"end\":38426,\"start\":38345},{\"attributes\":{\"id\":\"formula_25\"},\"end\":42401,\"start\":42046},{\"attributes\":{\"id\":\"formula_26\"},\"end\":43720,\"start\":43619},{\"attributes\":{\"id\":\"formula_27\"},\"end\":44722,\"start\":44684}]", "table_ref": "[{\"end\":3780,\"start\":3773},{\"end\":13374,\"start\":13367},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30506,\"start\":30499},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31207,\"start\":31200},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36072,\"start\":36065},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":37117,\"start\":37110},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":45191,\"start\":45184},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":45627,\"start\":45620},{\"end\":46159,\"start\":46152},{\"end\":47013,\"start\":47006},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":50970,\"start\":50963},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":65928,\"start\":65921}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1760,\"start\":1748},{\"end\":3623,\"start\":3617},{\"attributes\":{\"n\":\"2\"},\"end\":10770,\"start\":10758},{\"attributes\":{\"n\":\"3\"},\"end\":14831,\"start\":14818},{\"attributes\":{\"n\":\"4\"},\"end\":17804,\"start\":17773},{\"attributes\":{\"n\":\"2.\"},\"end\":20655,\"start\":20638},{\"attributes\":{\"n\":\"5\"},\"end\":21306,\"start\":21281},{\"attributes\":{\"n\":\"6\"},\"end\":26780,\"start\":26760},{\"attributes\":{\"n\":\"6.1\"},\"end\":27293,\"start\":27262},{\"attributes\":{\"n\":\"6.2\"},\"end\":28630,\"start\":28581},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":29379,\"start\":29347},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":31474,\"start\":31449},{\"attributes\":{\"n\":\"6.3\"},\"end\":32795,\"start\":32737},{\"attributes\":{\"n\":\"7\"},\"end\":35188,\"start\":35178},{\"end\":36297,\"start\":36265},{\"end\":37056,\"start\":37020},{\"end\":40334,\"start\":40285},{\"end\":41830,\"start\":41786},{\"end\":44988,\"start\":44949},{\"end\":46952,\"start\":46895},{\"end\":47904,\"start\":47849},{\"end\":48687,\"start\":48653},{\"end\":50061,\"start\":50045},{\"end\":51305,\"start\":51270},{\"end\":52855,\"start\":52825},{\"end\":55487,\"start\":55462},{\"end\":59597,\"start\":59543},{\"end\":61133,\"start\":61103},{\"end\":64141,\"start\":64097},{\"end\":67075,\"start\":67025},{\"end\":69081,\"start\":69071},{\"end\":69388,\"start\":69378},{\"end\":69651,\"start\":69641},{\"end\":69925,\"start\":69911},{\"end\":70537,\"start\":70527},{\"end\":70756,\"start\":70745},{\"end\":70983,\"start\":70972},{\"end\":71242,\"start\":71231},{\"end\":71411,\"start\":71400},{\"end\":71607,\"start\":71596},{\"end\":72185,\"start\":72174},{\"end\":72621,\"start\":72618},{\"end\":73177,\"start\":73168},{\"end\":73559,\"start\":73550}]", "table": "[{\"end\":73060,\"start\":73014},{\"end\":73166,\"start\":73097},{\"end\":73548,\"start\":73377}]", "figure_caption": "[{\"end\":69376,\"start\":69083},{\"end\":69639,\"start\":69390},{\"end\":69909,\"start\":69653},{\"end\":70525,\"start\":69928},{\"end\":70743,\"start\":70539},{\"end\":70970,\"start\":70759},{\"end\":71229,\"start\":70986},{\"end\":71398,\"start\":71245},{\"end\":71594,\"start\":71414},{\"end\":72172,\"start\":71610},{\"end\":72616,\"start\":72188},{\"end\":73014,\"start\":72623},{\"end\":73097,\"start\":73063},{\"end\":73348,\"start\":73179},{\"end\":73377,\"start\":73351},{\"end\":73791,\"start\":73561}]", "figure_ref": "[{\"end\":2390,\"start\":2379},{\"end\":3154,\"start\":3146},{\"end\":5448,\"start\":5440},{\"end\":6769,\"start\":6760},{\"end\":10226,\"start\":10215},{\"end\":10705,\"start\":10697},{\"end\":20235,\"start\":20227},{\"end\":22783,\"start\":22775},{\"end\":25671,\"start\":25663},{\"end\":25826,\"start\":25818},{\"end\":27919,\"start\":27910},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32463,\"start\":32446},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34866,\"start\":34858},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38833,\"start\":38823},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38967,\"start\":38957},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39201,\"start\":39191},{\"end\":39554,\"start\":39546},{\"end\":40196,\"start\":40180},{\"end\":41107,\"start\":41101},{\"end\":41636,\"start\":41619},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45349,\"start\":45341},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45790,\"start\":45780},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":46690,\"start\":46682},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46715,\"start\":46696},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":48091,\"start\":48082},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":48195,\"start\":48187},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":48426,\"start\":48408},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":49515,\"start\":49506},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":54884,\"start\":54875},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":55381,\"start\":55372},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":60317,\"start\":60308}]", "bib_author_first_name": "[{\"end\":74556,\"start\":74551},{\"end\":74575,\"start\":74566},{\"end\":74883,\"start\":74878},{\"end\":74902,\"start\":74895},{\"end\":74904,\"start\":74903},{\"end\":74915,\"start\":74911},{\"end\":74917,\"start\":74916},{\"end\":75198,\"start\":75190},{\"end\":75200,\"start\":75199},{\"end\":75228,\"start\":75220},{\"end\":75668,\"start\":75661},{\"end\":75682,\"start\":75676},{\"end\":75997,\"start\":75992},{\"end\":75999,\"start\":75998},{\"end\":76013,\"start\":76008},{\"end\":76350,\"start\":76345},{\"end\":76352,\"start\":76351},{\"end\":76366,\"start\":76361},{\"end\":76679,\"start\":76673},{\"end\":76695,\"start\":76690},{\"end\":76708,\"start\":76707},{\"end\":76724,\"start\":76718},{\"end\":76742,\"start\":76741},{\"end\":76759,\"start\":76752},{\"end\":76778,\"start\":76771},{\"end\":77118,\"start\":77117},{\"end\":77139,\"start\":77133},{\"end\":77154,\"start\":77148},{\"end\":77173,\"start\":77166},{\"end\":77175,\"start\":77174},{\"end\":77192,\"start\":77186},{\"end\":77206,\"start\":77201},{\"end\":77208,\"start\":77207},{\"end\":77543,\"start\":77537},{\"end\":77560,\"start\":77554},{\"end\":77573,\"start\":77565},{\"end\":77585,\"start\":77579},{\"end\":77610,\"start\":77603},{\"end\":77631,\"start\":77625},{\"end\":77645,\"start\":77640},{\"end\":78021,\"start\":78015},{\"end\":78032,\"start\":78026},{\"end\":78055,\"start\":78049},{\"end\":78072,\"start\":78066},{\"end\":78098,\"start\":78090},{\"end\":78111,\"start\":78104},{\"end\":78132,\"start\":78126},{\"end\":78146,\"start\":78141},{\"end\":78599,\"start\":78594},{\"end\":78617,\"start\":78608},{\"end\":78635,\"start\":78629},{\"end\":78649,\"start\":78644},{\"end\":79118,\"start\":79111},{\"end\":79142,\"start\":79137},{\"end\":79152,\"start\":79149},{\"end\":79154,\"start\":79153},{\"end\":79173,\"start\":79165},{\"end\":79186,\"start\":79180},{\"end\":79206,\"start\":79194},{\"end\":79646,\"start\":79641},{\"end\":79658,\"start\":79647},{\"end\":79670,\"start\":79666},{\"end\":79683,\"start\":79678},{\"end\":79685,\"start\":79684},{\"end\":79701,\"start\":79694},{\"end\":79719,\"start\":79714},{\"end\":79737,\"start\":79733},{\"end\":79751,\"start\":79747},{\"end\":79765,\"start\":79760},{\"end\":79767,\"start\":79766},{\"end\":79782,\"start\":79777},{\"end\":79801,\"start\":79794},{\"end\":79803,\"start\":79802},{\"end\":80268,\"start\":80262},{\"end\":80283,\"start\":80275},{\"end\":80301,\"start\":80292},{\"end\":80303,\"start\":80302},{\"end\":80314,\"start\":80310},{\"end\":80316,\"start\":80315},{\"end\":80334,\"start\":80326},{\"end\":80336,\"start\":80335},{\"end\":80351,\"start\":80346},{\"end\":80353,\"start\":80352},{\"end\":80371,\"start\":80364},{\"end\":80389,\"start\":80382},{\"end\":80402,\"start\":80397},{\"end\":80411,\"start\":80408},{\"end\":80424,\"start\":80417},{\"end\":80426,\"start\":80425},{\"end\":80437,\"start\":80433},{\"end\":80930,\"start\":80924},{\"end\":81197,\"start\":81191},{\"end\":81199,\"start\":81198},{\"end\":81220,\"start\":81215},{\"end\":81222,\"start\":81221},{\"end\":81241,\"start\":81233},{\"end\":81255,\"start\":81250},{\"end\":81697,\"start\":81692},{\"end\":81705,\"start\":81704},{\"end\":81717,\"start\":81713},{\"end\":81730,\"start\":81727},{\"end\":81745,\"start\":81738},{\"end\":81757,\"start\":81754},{\"end\":81772,\"start\":81769},{\"end\":82193,\"start\":82192},{\"end\":82203,\"start\":82202},{\"end\":82499,\"start\":82493},{\"end\":82512,\"start\":82508},{\"end\":82522,\"start\":82521},{\"end\":82549,\"start\":82542},{\"end\":82576,\"start\":82570},{\"end\":82596,\"start\":82590},{\"end\":82631,\"start\":82622},{\"end\":83063,\"start\":83062},{\"end\":83104,\"start\":83103},{\"end\":83114,\"start\":83113},{\"end\":83469,\"start\":83466},{\"end\":83485,\"start\":83479},{\"end\":83501,\"start\":83495},{\"end\":83515,\"start\":83512},{\"end\":83843,\"start\":83834},{\"end\":83877,\"start\":83867},{\"end\":83893,\"start\":83885},{\"end\":83908,\"start\":83900},{\"end\":83920,\"start\":83915},{\"end\":83936,\"start\":83928},{\"end\":83938,\"start\":83937},{\"end\":84493,\"start\":84492},{\"end\":84513,\"start\":84512},{\"end\":84529,\"start\":84523},{\"end\":84546,\"start\":84540},{\"end\":84562,\"start\":84555},{\"end\":84577,\"start\":84572},{\"end\":84595,\"start\":84588},{\"end\":84609,\"start\":84605},{\"end\":84625,\"start\":84624},{\"end\":84642,\"start\":84635},{\"end\":84644,\"start\":84643},{\"end\":85147,\"start\":85140},{\"end\":85161,\"start\":85155},{\"end\":85179,\"start\":85171},{\"end\":85194,\"start\":85188},{\"end\":85212,\"start\":85206},{\"end\":85690,\"start\":85682},{\"end\":85704,\"start\":85697},{\"end\":85715,\"start\":85710},{\"end\":85735,\"start\":85725},{\"end\":86054,\"start\":86046},{\"end\":86059,\"start\":86055},{\"end\":86080,\"start\":86074},{\"end\":86095,\"start\":86091},{\"end\":86097,\"start\":86096},{\"end\":86483,\"start\":86480},{\"end\":86493,\"start\":86488},{\"end\":86506,\"start\":86499},{\"end\":86518,\"start\":86513},{\"end\":86528,\"start\":86525},{\"end\":86543,\"start\":86536},{\"end\":86553,\"start\":86550},{\"end\":86848,\"start\":86843},{\"end\":86859,\"start\":86854},{\"end\":86871,\"start\":86870},{\"end\":86896,\"start\":86895},{\"end\":86914,\"start\":86908},{\"end\":86916,\"start\":86915},{\"end\":87310,\"start\":87302},{\"end\":87325,\"start\":87320},{\"end\":87338,\"start\":87333},{\"end\":87356,\"start\":87353},{\"end\":87370,\"start\":87364},{\"end\":87389,\"start\":87383},{\"end\":87404,\"start\":87398},{\"end\":87422,\"start\":87412},{\"end\":87437,\"start\":87431},{\"end\":87455,\"start\":87450},{\"end\":87819,\"start\":87815},{\"end\":87834,\"start\":87830},{\"end\":87845,\"start\":87841},{\"end\":87863,\"start\":87858},{\"end\":87880,\"start\":87874},{\"end\":87897,\"start\":87890},{\"end\":87919,\"start\":87913},{\"end\":87935,\"start\":87929},{\"end\":87950,\"start\":87944},{\"end\":87963,\"start\":87957},{\"end\":88353,\"start\":88348},{\"end\":88370,\"start\":88362},{\"end\":88650,\"start\":88645},{\"end\":88666,\"start\":88662},{\"end\":88680,\"start\":88675},{\"end\":89052,\"start\":89046},{\"end\":89314,\"start\":89309},{\"end\":89329,\"start\":89323},{\"end\":89691,\"start\":89685},{\"end\":89706,\"start\":89700},{\"end\":89720,\"start\":89715},{\"end\":89961,\"start\":89955},{\"end\":89977,\"start\":89972},{\"end\":89996,\"start\":89989},{\"end\":89998,\"start\":89997},{\"end\":90012,\"start\":90006},{\"end\":90029,\"start\":90022},{\"end\":90031,\"start\":90030},{\"end\":90050,\"start\":90043},{\"end\":90065,\"start\":90058},{\"end\":90078,\"start\":90072},{\"end\":90080,\"start\":90079},{\"end\":90093,\"start\":90090},{\"end\":90110,\"start\":90104},{\"end\":90499,\"start\":90492},{\"end\":90501,\"start\":90500},{\"end\":90711,\"start\":90705},{\"end\":90729,\"start\":90722},{\"end\":90731,\"start\":90730},{\"end\":90750,\"start\":90743},{\"end\":90752,\"start\":90751},{\"end\":90766,\"start\":90761},{\"end\":90785,\"start\":90778},{\"end\":90800,\"start\":90793},{\"end\":90813,\"start\":90808},{\"end\":90835,\"start\":90829},{\"end\":90837,\"start\":90836},{\"end\":90859,\"start\":90854},{\"end\":90861,\"start\":90860},{\"end\":90875,\"start\":90869},{\"end\":90877,\"start\":90876},{\"end\":90895,\"start\":90887},{\"end\":90909,\"start\":90905},{\"end\":91580,\"start\":91579},{\"end\":91600,\"start\":91599},{\"end\":91796,\"start\":91789},{\"end\":91817,\"start\":91808},{\"end\":91833,\"start\":91826},{\"end\":92144,\"start\":92139},{\"end\":92156,\"start\":92155},{\"end\":92158,\"start\":92157},{\"end\":92171,\"start\":92166},{\"end\":92173,\"start\":92172},{\"end\":92517,\"start\":92510},{\"end\":92527,\"start\":92526},{\"end\":92901,\"start\":92893},{\"end\":92915,\"start\":92907},{\"end\":92927,\"start\":92926},{\"end\":92943,\"start\":92936},{\"end\":93206,\"start\":93200},{\"end\":93217,\"start\":93211},{\"end\":93234,\"start\":93228},{\"end\":93260,\"start\":93252},{\"end\":93273,\"start\":93266},{\"end\":93294,\"start\":93288},{\"end\":93308,\"start\":93303},{\"end\":93670,\"start\":93664},{\"end\":93681,\"start\":93675},{\"end\":93698,\"start\":93692},{\"end\":93724,\"start\":93716},{\"end\":93737,\"start\":93730},{\"end\":93758,\"start\":93752},{\"end\":93772,\"start\":93767},{\"end\":94131,\"start\":94124},{\"end\":94133,\"start\":94132},{\"end\":94159,\"start\":94153},{\"end\":94483,\"start\":94474},{\"end\":94493,\"start\":94489},{\"end\":94517,\"start\":94511},{\"end\":94535,\"start\":94528},{\"end\":94551,\"start\":94546},{\"end\":94568,\"start\":94562},{\"end\":94584,\"start\":94578},{\"end\":94921,\"start\":94915},{\"end\":94936,\"start\":94932},{\"end\":94948,\"start\":94941},{\"end\":94965,\"start\":94956},{\"end\":95226,\"start\":95220},{\"end\":95237,\"start\":95234},{\"end\":95251,\"start\":95244},{\"end\":95266,\"start\":95260},{\"end\":95563,\"start\":95562},{\"end\":95836,\"start\":95831},{\"end\":95855,\"start\":95848},{\"end\":95857,\"start\":95856},{\"end\":95868,\"start\":95864},{\"end\":95870,\"start\":95869},{\"end\":96246,\"start\":96242},{\"end\":96263,\"start\":96259},{\"end\":96483,\"start\":96478},{\"end\":96862,\"start\":96851},{\"end\":96864,\"start\":96863},{\"end\":96878,\"start\":96877},{\"end\":97164,\"start\":97163},{\"end\":97173,\"start\":97172},{\"end\":97480,\"start\":97473},{\"end\":97496,\"start\":97489},{\"end\":97498,\"start\":97497},{\"end\":97516,\"start\":97510},{\"end\":97872,\"start\":97865},{\"end\":97887,\"start\":97880},{\"end\":97899,\"start\":97893},{\"end\":97919,\"start\":97911},{\"end\":98238,\"start\":98237},{\"end\":98254,\"start\":98249},{\"end\":98539,\"start\":98532},{\"end\":98548,\"start\":98547},{\"end\":98570,\"start\":98569},{\"end\":98585,\"start\":98580},{\"end\":98587,\"start\":98586},{\"end\":98604,\"start\":98595},{\"end\":98623,\"start\":98622},{\"end\":98638,\"start\":98633},{\"end\":98652,\"start\":98645},{\"end\":98668,\"start\":98667},{\"end\":98682,\"start\":98676},{\"end\":98684,\"start\":98683},{\"end\":99136,\"start\":99135},{\"end\":99356,\"start\":99348},{\"end\":99372,\"start\":99363},{\"end\":99388,\"start\":99382},{\"end\":99404,\"start\":99394},{\"end\":99683,\"start\":99678},{\"end\":99696,\"start\":99691},{\"end\":99705,\"start\":99701},{\"end\":99723,\"start\":99716},{\"end\":99742,\"start\":99736},{\"end\":99765,\"start\":99760},{\"end\":100017,\"start\":100010},{\"end\":100034,\"start\":100026},{\"end\":100053,\"start\":100048},{\"end\":100073,\"start\":100066},{\"end\":100086,\"start\":100080},{\"end\":100109,\"start\":100102},{\"end\":100127,\"start\":100119},{\"end\":100140,\"start\":100135},{\"end\":100153,\"start\":100147},{\"end\":100164,\"start\":100158}]", "bib_author_last_name": "[{\"end\":74564,\"start\":74557},{\"end\":74583,\"start\":74576},{\"end\":74893,\"start\":74884},{\"end\":74909,\"start\":74905},{\"end\":74925,\"start\":74918},{\"end\":75218,\"start\":75201},{\"end\":75235,\"start\":75229},{\"end\":75245,\"start\":75237},{\"end\":75674,\"start\":75669},{\"end\":75686,\"start\":75683},{\"end\":76006,\"start\":76000},{\"end\":76023,\"start\":76014},{\"end\":76359,\"start\":76353},{\"end\":76376,\"start\":76367},{\"end\":76688,\"start\":76680},{\"end\":76705,\"start\":76696},{\"end\":76716,\"start\":76709},{\"end\":76730,\"start\":76725},{\"end\":76739,\"start\":76732},{\"end\":76750,\"start\":76743},{\"end\":76769,\"start\":76760},{\"end\":76784,\"start\":76779},{\"end\":76790,\"start\":76786},{\"end\":77125,\"start\":77119},{\"end\":77131,\"start\":77127},{\"end\":77146,\"start\":77140},{\"end\":77164,\"start\":77155},{\"end\":77184,\"start\":77176},{\"end\":77199,\"start\":77193},{\"end\":77214,\"start\":77209},{\"end\":77221,\"start\":77216},{\"end\":77552,\"start\":77544},{\"end\":77563,\"start\":77561},{\"end\":77577,\"start\":77574},{\"end\":77601,\"start\":77586},{\"end\":77623,\"start\":77611},{\"end\":77638,\"start\":77632},{\"end\":77656,\"start\":77646},{\"end\":78024,\"start\":78022},{\"end\":78047,\"start\":78033},{\"end\":78064,\"start\":78056},{\"end\":78088,\"start\":78073},{\"end\":78102,\"start\":78099},{\"end\":78124,\"start\":78112},{\"end\":78139,\"start\":78133},{\"end\":78157,\"start\":78147},{\"end\":78606,\"start\":78600},{\"end\":78627,\"start\":78618},{\"end\":78642,\"start\":78636},{\"end\":78653,\"start\":78650},{\"end\":79135,\"start\":79119},{\"end\":79147,\"start\":79143},{\"end\":79163,\"start\":79155},{\"end\":79178,\"start\":79174},{\"end\":79192,\"start\":79187},{\"end\":79216,\"start\":79207},{\"end\":79230,\"start\":79218},{\"end\":79664,\"start\":79659},{\"end\":79676,\"start\":79671},{\"end\":79692,\"start\":79686},{\"end\":79712,\"start\":79702},{\"end\":79731,\"start\":79720},{\"end\":79745,\"start\":79738},{\"end\":79758,\"start\":79752},{\"end\":79775,\"start\":79768},{\"end\":79792,\"start\":79783},{\"end\":79810,\"start\":79804},{\"end\":80273,\"start\":80269},{\"end\":80290,\"start\":80284},{\"end\":80308,\"start\":80304},{\"end\":80324,\"start\":80317},{\"end\":80344,\"start\":80337},{\"end\":80362,\"start\":80354},{\"end\":80380,\"start\":80372},{\"end\":80395,\"start\":80390},{\"end\":80406,\"start\":80403},{\"end\":80415,\"start\":80412},{\"end\":80431,\"start\":80427},{\"end\":80448,\"start\":80438},{\"end\":80940,\"start\":80931},{\"end\":81213,\"start\":81200},{\"end\":81231,\"start\":81223},{\"end\":81248,\"start\":81242},{\"end\":81259,\"start\":81256},{\"end\":81271,\"start\":81261},{\"end\":81702,\"start\":81698},{\"end\":81711,\"start\":81706},{\"end\":81725,\"start\":81718},{\"end\":81736,\"start\":81731},{\"end\":81752,\"start\":81746},{\"end\":81767,\"start\":81758},{\"end\":81779,\"start\":81773},{\"end\":81789,\"start\":81781},{\"end\":82200,\"start\":82194},{\"end\":82213,\"start\":82204},{\"end\":82506,\"start\":82500},{\"end\":82519,\"start\":82513},{\"end\":82530,\"start\":82523},{\"end\":82540,\"start\":82532},{\"end\":82568,\"start\":82550},{\"end\":82588,\"start\":82577},{\"end\":82611,\"start\":82597},{\"end\":82620,\"start\":82613},{\"end\":82637,\"start\":82632},{\"end\":82644,\"start\":82639},{\"end\":83072,\"start\":83064},{\"end\":83083,\"start\":83074},{\"end\":83091,\"start\":83085},{\"end\":83101,\"start\":83093},{\"end\":83111,\"start\":83105},{\"end\":83123,\"start\":83115},{\"end\":83134,\"start\":83125},{\"end\":83477,\"start\":83470},{\"end\":83493,\"start\":83486},{\"end\":83510,\"start\":83502},{\"end\":83521,\"start\":83516},{\"end\":83832,\"start\":83817},{\"end\":83850,\"start\":83844},{\"end\":83859,\"start\":83852},{\"end\":83865,\"start\":83861},{\"end\":83883,\"start\":83878},{\"end\":83898,\"start\":83894},{\"end\":83913,\"start\":83909},{\"end\":83926,\"start\":83921},{\"end\":83945,\"start\":83939},{\"end\":83953,\"start\":83947},{\"end\":84503,\"start\":84494},{\"end\":84510,\"start\":84505},{\"end\":84521,\"start\":84514},{\"end\":84538,\"start\":84530},{\"end\":84553,\"start\":84547},{\"end\":84570,\"start\":84563},{\"end\":84586,\"start\":84578},{\"end\":84603,\"start\":84596},{\"end\":84613,\"start\":84610},{\"end\":84622,\"start\":84615},{\"end\":84633,\"start\":84626},{\"end\":84650,\"start\":84645},{\"end\":84658,\"start\":84652},{\"end\":85153,\"start\":85148},{\"end\":85169,\"start\":85162},{\"end\":85186,\"start\":85180},{\"end\":85204,\"start\":85195},{\"end\":85216,\"start\":85213},{\"end\":85695,\"start\":85691},{\"end\":85708,\"start\":85705},{\"end\":85723,\"start\":85716},{\"end\":85739,\"start\":85736},{\"end\":86072,\"start\":86060},{\"end\":86089,\"start\":86081},{\"end\":86102,\"start\":86098},{\"end\":86114,\"start\":86104},{\"end\":86486,\"start\":86484},{\"end\":86497,\"start\":86494},{\"end\":86511,\"start\":86507},{\"end\":86523,\"start\":86519},{\"end\":86534,\"start\":86529},{\"end\":86548,\"start\":86544},{\"end\":86557,\"start\":86554},{\"end\":86852,\"start\":86849},{\"end\":86868,\"start\":86860},{\"end\":86883,\"start\":86872},{\"end\":86893,\"start\":86885},{\"end\":86906,\"start\":86897},{\"end\":86925,\"start\":86917},{\"end\":86932,\"start\":86927},{\"end\":87318,\"start\":87311},{\"end\":87331,\"start\":87326},{\"end\":87351,\"start\":87339},{\"end\":87362,\"start\":87357},{\"end\":87381,\"start\":87371},{\"end\":87396,\"start\":87390},{\"end\":87410,\"start\":87405},{\"end\":87429,\"start\":87423},{\"end\":87448,\"start\":87438},{\"end\":87464,\"start\":87456},{\"end\":87828,\"start\":87820},{\"end\":87839,\"start\":87835},{\"end\":87856,\"start\":87846},{\"end\":87872,\"start\":87864},{\"end\":87888,\"start\":87881},{\"end\":87911,\"start\":87898},{\"end\":87927,\"start\":87920},{\"end\":87942,\"start\":87936},{\"end\":87955,\"start\":87951},{\"end\":87972,\"start\":87964},{\"end\":88360,\"start\":88354},{\"end\":88375,\"start\":88371},{\"end\":88660,\"start\":88651},{\"end\":88673,\"start\":88667},{\"end\":88690,\"start\":88681},{\"end\":89060,\"start\":89053},{\"end\":89321,\"start\":89315},{\"end\":89336,\"start\":89330},{\"end\":89698,\"start\":89692},{\"end\":89713,\"start\":89707},{\"end\":89724,\"start\":89721},{\"end\":89970,\"start\":89962},{\"end\":89987,\"start\":89978},{\"end\":90004,\"start\":89999},{\"end\":90020,\"start\":90013},{\"end\":90041,\"start\":90032},{\"end\":90056,\"start\":90051},{\"end\":90070,\"start\":90066},{\"end\":90088,\"start\":90081},{\"end\":90102,\"start\":90094},{\"end\":90119,\"start\":90111},{\"end\":90508,\"start\":90502},{\"end\":90720,\"start\":90712},{\"end\":90741,\"start\":90732},{\"end\":90759,\"start\":90753},{\"end\":90776,\"start\":90767},{\"end\":90791,\"start\":90786},{\"end\":90806,\"start\":90801},{\"end\":90827,\"start\":90814},{\"end\":90852,\"start\":90838},{\"end\":90867,\"start\":90862},{\"end\":90885,\"start\":90878},{\"end\":90903,\"start\":90896},{\"end\":90920,\"start\":90910},{\"end\":91396,\"start\":91380},{\"end\":91586,\"start\":91581},{\"end\":91597,\"start\":91588},{\"end\":91607,\"start\":91601},{\"end\":91616,\"start\":91609},{\"end\":91806,\"start\":91797},{\"end\":91824,\"start\":91818},{\"end\":91840,\"start\":91834},{\"end\":92153,\"start\":92145},{\"end\":92164,\"start\":92159},{\"end\":92178,\"start\":92174},{\"end\":92188,\"start\":92180},{\"end\":92524,\"start\":92518},{\"end\":92533,\"start\":92528},{\"end\":92546,\"start\":92535},{\"end\":92905,\"start\":92902},{\"end\":92924,\"start\":92916},{\"end\":92934,\"start\":92928},{\"end\":92950,\"start\":92944},{\"end\":92964,\"start\":92952},{\"end\":93209,\"start\":93207},{\"end\":93226,\"start\":93218},{\"end\":93250,\"start\":93235},{\"end\":93264,\"start\":93261},{\"end\":93286,\"start\":93274},{\"end\":93301,\"start\":93295},{\"end\":93319,\"start\":93309},{\"end\":93673,\"start\":93671},{\"end\":93690,\"start\":93682},{\"end\":93714,\"start\":93699},{\"end\":93728,\"start\":93725},{\"end\":93750,\"start\":93738},{\"end\":93765,\"start\":93759},{\"end\":93783,\"start\":93773},{\"end\":94151,\"start\":94134},{\"end\":94164,\"start\":94160},{\"end\":94181,\"start\":94166},{\"end\":94487,\"start\":94484},{\"end\":94509,\"start\":94494},{\"end\":94526,\"start\":94518},{\"end\":94544,\"start\":94536},{\"end\":94560,\"start\":94552},{\"end\":94576,\"start\":94569},{\"end\":94591,\"start\":94585},{\"end\":94930,\"start\":94922},{\"end\":94939,\"start\":94937},{\"end\":94954,\"start\":94949},{\"end\":94971,\"start\":94966},{\"end\":95232,\"start\":95227},{\"end\":95242,\"start\":95238},{\"end\":95258,\"start\":95252},{\"end\":95282,\"start\":95267},{\"end\":95570,\"start\":95564},{\"end\":95578,\"start\":95572},{\"end\":95846,\"start\":95837},{\"end\":95862,\"start\":95858},{\"end\":95878,\"start\":95871},{\"end\":96257,\"start\":96247},{\"end\":96270,\"start\":96264},{\"end\":96491,\"start\":96484},{\"end\":96875,\"start\":96865},{\"end\":96886,\"start\":96879},{\"end\":96893,\"start\":96888},{\"end\":97170,\"start\":97165},{\"end\":97182,\"start\":97174},{\"end\":97487,\"start\":97481},{\"end\":97508,\"start\":97499},{\"end\":97524,\"start\":97517},{\"end\":97878,\"start\":97873},{\"end\":97891,\"start\":97888},{\"end\":97909,\"start\":97900},{\"end\":97922,\"start\":97920},{\"end\":98247,\"start\":98239},{\"end\":98261,\"start\":98255},{\"end\":98265,\"start\":98263},{\"end\":98545,\"start\":98540},{\"end\":98556,\"start\":98549},{\"end\":98567,\"start\":98558},{\"end\":98578,\"start\":98571},{\"end\":98593,\"start\":98588},{\"end\":98612,\"start\":98605},{\"end\":98620,\"start\":98614},{\"end\":98631,\"start\":98624},{\"end\":98643,\"start\":98639},{\"end\":98657,\"start\":98653},{\"end\":98665,\"start\":98659},{\"end\":98674,\"start\":98669},{\"end\":98692,\"start\":98685},{\"end\":98701,\"start\":98694},{\"end\":99143,\"start\":99137},{\"end\":99149,\"start\":99145},{\"end\":99361,\"start\":99357},{\"end\":99380,\"start\":99373},{\"end\":99392,\"start\":99389},{\"end\":99408,\"start\":99405},{\"end\":99689,\"start\":99684},{\"end\":99699,\"start\":99697},{\"end\":99714,\"start\":99706},{\"end\":99734,\"start\":99724},{\"end\":99758,\"start\":99743},{\"end\":99776,\"start\":99766},{\"end\":100024,\"start\":100018},{\"end\":100046,\"start\":100035},{\"end\":100064,\"start\":100054},{\"end\":100078,\"start\":100074},{\"end\":100100,\"start\":100087},{\"end\":100117,\"start\":100110},{\"end\":100133,\"start\":100128},{\"end\":100145,\"start\":100141},{\"end\":100156,\"start\":100154},{\"end\":100180,\"start\":100165}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15856108},\"end\":74785,\"start\":74497},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":134307699},\"end\":75132,\"start\":74787},{\"attributes\":{\"id\":\"b2\"},\"end\":75529,\"start\":75134},{\"attributes\":{\"doi\":\"arXiv:2207.00521\",\"id\":\"b3\"},\"end\":75916,\"start\":75531},{\"attributes\":{\"doi\":\"10.22541/essoar.168167352.24863390/v1\",\"id\":\"b4\"},\"end\":76270,\"start\":75918},{\"attributes\":{\"doi\":\"10.22541/essoar.168167204.45220772/v1\",\"id\":\"b5\"},\"end\":76624,\"start\":76272},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4001553},\"end\":77056,\"start\":76626},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":237583501},\"end\":77535,\"start\":77058},{\"attributes\":{\"doi\":\"arXiv:2108.08481\",\"id\":\"b8\"},\"end\":77963,\"start\":77537},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":258509645},\"end\":78482,\"start\":77965},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2324694},\"end\":79061,\"start\":78484},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11714212},\"end\":79561,\"start\":79063},{\"attributes\":{\"doi\":\"10.1126/science.abn7950\",\"id\":\"b12\",\"matched_paper_id\":252161375},\"end\":80203,\"start\":79563},{\"attributes\":{\"doi\":\"10.1029/2021RG000757\",\"id\":\"b13\",\"matched_paper_id\":256970204},\"end\":80853,\"start\":80205},{\"attributes\":{\"id\":\"b14\"},\"end\":81037,\"start\":80855},{\"attributes\":{\"id\":\"b15\"},\"end\":81578,\"start\":81039},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":135341644},\"end\":82127,\"start\":81580},{\"attributes\":{\"doi\":\"10.1038/nclimate2226\",\"id\":\"b17\",\"matched_paper_id\":15030652},\"end\":82416,\"start\":82129},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":128595190},\"end\":82949,\"start\":82418},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":55042912},\"end\":83388,\"start\":82951},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":239470490},\"end\":83724,\"start\":83390},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13395271},\"end\":84440,\"start\":83726},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":245917526},\"end\":84973,\"start\":84442},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":232771330},\"end\":85608,\"start\":84975},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":227254410},\"end\":85953,\"start\":85610},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":220935748},\"end\":86427,\"start\":85955},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":258976377},\"end\":86730,\"start\":86429},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":232283023},\"end\":87220,\"start\":86732},{\"attributes\":{\"doi\":\"arXiv:2302.10274\",\"id\":\"b28\"},\"end\":87785,\"start\":87222},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219477521},\"end\":88310,\"start\":87787},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":795794},\"end\":88534,\"start\":88312},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":122299729},\"end\":88952,\"start\":88536},{\"attributes\":{\"id\":\"b32\"},\"end\":89192,\"start\":88954},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":235990653},\"end\":89606,\"start\":89194},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":189927151},\"end\":89906,\"start\":89608},{\"attributes\":{\"doi\":\"10.1038/nature08227\",\"id\":\"b35\",\"matched_paper_id\":4001553},\"end\":90449,\"start\":89908},{\"attributes\":{\"doi\":\"10.1038/nclimate1143\",\"id\":\"b36\",\"matched_paper_id\":86317267},\"end\":90701,\"start\":90451},{\"attributes\":{\"doi\":\"10.1126/science.1225244\",\"id\":\"b37\"},\"end\":91337,\"start\":90703},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":86317267},\"end\":91525,\"start\":91339},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":16906203},\"end\":91787,\"start\":91527},{\"attributes\":{\"doi\":\"arXiv:2302.06534\",\"id\":\"b40\"},\"end\":92061,\"start\":91789},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":202784022},\"end\":92432,\"start\":92063},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":218719544},\"end\":92803,\"start\":92434},{\"attributes\":{\"doi\":\"arXiv:2210.17443\",\"id\":\"b43\"},\"end\":93198,\"start\":92805},{\"attributes\":{\"doi\":\"arXiv:2003.03485\",\"id\":\"b44\"},\"end\":93662,\"start\":93200},{\"attributes\":{\"doi\":\"arXiv:2010.08895\",\"id\":\"b45\"},\"end\":94122,\"start\":93664},{\"attributes\":{\"doi\":\"arXiv:2204.11127\",\"id\":\"b46\"},\"end\":94378,\"start\":94124},{\"attributes\":{\"doi\":\"arXiv:1406.1078\",\"id\":\"b47\"},\"end\":94878,\"start\":94380},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":12254164},\"end\":95168,\"start\":94880},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":233296294},\"end\":95528,\"start\":95170},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":15359559},\"end\":95719,\"start\":95530},{\"attributes\":{\"doi\":\"10.1073/pnas.2003730117\",\"id\":\"b51\",\"matched_paper_id\":226984882},\"end\":96204,\"start\":95721},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":120040097},\"end\":96403,\"start\":96206},{\"attributes\":{\"doi\":\"0935-4964. doi: 10.1007/ s00162-006-0032-z\",\"id\":\"b53\",\"matched_paper_id\":59369442},\"end\":96751,\"start\":96405},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":123117279},\"end\":97090,\"start\":96753},{\"attributes\":{\"doi\":\"10.1175/JCLI-D-11-00105.1\",\"id\":\"b55\",\"matched_paper_id\":129906451},\"end\":97405,\"start\":97092},{\"attributes\":{\"doi\":\"10.5194/essd-11-1745-2019\",\"id\":\"b56\",\"matched_paper_id\":212849630},\"end\":97794,\"start\":97407},{\"attributes\":{\"doi\":\"10.1029/2021GL094676\",\"id\":\"b57\",\"matched_paper_id\":244200707},\"end\":98191,\"start\":97796},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b58\"},\"end\":98409,\"start\":98193},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":3923616},\"end\":99043,\"start\":98411},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":256813760},\"end\":99346,\"start\":99045},{\"attributes\":{\"doi\":\"arXiv:2111.13802\",\"id\":\"b61\"},\"end\":99618,\"start\":99348},{\"attributes\":{\"doi\":\"arXiv:2307.15034\",\"id\":\"b62\"},\"end\":100008,\"start\":99620},{\"attributes\":{\"doi\":\"arXiv:2202.11214\",\"id\":\"b63\"},\"end\":100626,\"start\":100010}]", "bib_title": "[{\"end\":74549,\"start\":74497},{\"end\":74876,\"start\":74787},{\"end\":76343,\"start\":76272},{\"end\":76671,\"start\":76626},{\"end\":77115,\"start\":77058},{\"end\":78013,\"start\":77965},{\"end\":78592,\"start\":78484},{\"end\":79109,\"start\":79063},{\"end\":79639,\"start\":79563},{\"end\":80260,\"start\":80205},{\"end\":81189,\"start\":81039},{\"end\":81690,\"start\":81580},{\"end\":82190,\"start\":82129},{\"end\":82491,\"start\":82418},{\"end\":83060,\"start\":82951},{\"end\":83464,\"start\":83390},{\"end\":83815,\"start\":83726},{\"end\":84490,\"start\":84442},{\"end\":85138,\"start\":84975},{\"end\":85680,\"start\":85610},{\"end\":86044,\"start\":85955},{\"end\":86478,\"start\":86429},{\"end\":86841,\"start\":86732},{\"end\":87813,\"start\":87787},{\"end\":88346,\"start\":88312},{\"end\":88643,\"start\":88536},{\"end\":89307,\"start\":89194},{\"end\":89683,\"start\":89608},{\"end\":89953,\"start\":89908},{\"end\":90490,\"start\":90451},{\"end\":91378,\"start\":91339},{\"end\":91577,\"start\":91527},{\"end\":92137,\"start\":92063},{\"end\":92508,\"start\":92434},{\"end\":94913,\"start\":94880},{\"end\":95218,\"start\":95170},{\"end\":95560,\"start\":95530},{\"end\":95829,\"start\":95721},{\"end\":96240,\"start\":96206},{\"end\":96476,\"start\":96405},{\"end\":96849,\"start\":96753},{\"end\":97161,\"start\":97092},{\"end\":97471,\"start\":97407},{\"end\":97863,\"start\":97796},{\"end\":98530,\"start\":98411},{\"end\":99133,\"start\":99045}]", "bib_author": "[{\"end\":74566,\"start\":74551},{\"end\":74585,\"start\":74566},{\"end\":74895,\"start\":74878},{\"end\":74911,\"start\":74895},{\"end\":74927,\"start\":74911},{\"end\":75220,\"start\":75190},{\"end\":75237,\"start\":75220},{\"end\":75247,\"start\":75237},{\"end\":75676,\"start\":75661},{\"end\":75688,\"start\":75676},{\"end\":76008,\"start\":75992},{\"end\":76025,\"start\":76008},{\"end\":76361,\"start\":76345},{\"end\":76378,\"start\":76361},{\"end\":76690,\"start\":76673},{\"end\":76707,\"start\":76690},{\"end\":76718,\"start\":76707},{\"end\":76732,\"start\":76718},{\"end\":76741,\"start\":76732},{\"end\":76752,\"start\":76741},{\"end\":76771,\"start\":76752},{\"end\":76786,\"start\":76771},{\"end\":76792,\"start\":76786},{\"end\":77127,\"start\":77117},{\"end\":77133,\"start\":77127},{\"end\":77148,\"start\":77133},{\"end\":77166,\"start\":77148},{\"end\":77186,\"start\":77166},{\"end\":77201,\"start\":77186},{\"end\":77216,\"start\":77201},{\"end\":77223,\"start\":77216},{\"end\":77554,\"start\":77537},{\"end\":77565,\"start\":77554},{\"end\":77579,\"start\":77565},{\"end\":77603,\"start\":77579},{\"end\":77625,\"start\":77603},{\"end\":77640,\"start\":77625},{\"end\":77658,\"start\":77640},{\"end\":78026,\"start\":78015},{\"end\":78049,\"start\":78026},{\"end\":78066,\"start\":78049},{\"end\":78090,\"start\":78066},{\"end\":78104,\"start\":78090},{\"end\":78126,\"start\":78104},{\"end\":78141,\"start\":78126},{\"end\":78159,\"start\":78141},{\"end\":78608,\"start\":78594},{\"end\":78629,\"start\":78608},{\"end\":78644,\"start\":78629},{\"end\":78655,\"start\":78644},{\"end\":79137,\"start\":79111},{\"end\":79149,\"start\":79137},{\"end\":79165,\"start\":79149},{\"end\":79180,\"start\":79165},{\"end\":79194,\"start\":79180},{\"end\":79218,\"start\":79194},{\"end\":79232,\"start\":79218},{\"end\":79666,\"start\":79641},{\"end\":79678,\"start\":79666},{\"end\":79694,\"start\":79678},{\"end\":79714,\"start\":79694},{\"end\":79733,\"start\":79714},{\"end\":79747,\"start\":79733},{\"end\":79760,\"start\":79747},{\"end\":79777,\"start\":79760},{\"end\":79794,\"start\":79777},{\"end\":79812,\"start\":79794},{\"end\":80275,\"start\":80262},{\"end\":80292,\"start\":80275},{\"end\":80310,\"start\":80292},{\"end\":80326,\"start\":80310},{\"end\":80346,\"start\":80326},{\"end\":80364,\"start\":80346},{\"end\":80382,\"start\":80364},{\"end\":80397,\"start\":80382},{\"end\":80408,\"start\":80397},{\"end\":80417,\"start\":80408},{\"end\":80433,\"start\":80417},{\"end\":80450,\"start\":80433},{\"end\":80942,\"start\":80924},{\"end\":81215,\"start\":81191},{\"end\":81233,\"start\":81215},{\"end\":81250,\"start\":81233},{\"end\":81261,\"start\":81250},{\"end\":81273,\"start\":81261},{\"end\":81704,\"start\":81692},{\"end\":81713,\"start\":81704},{\"end\":81727,\"start\":81713},{\"end\":81738,\"start\":81727},{\"end\":81754,\"start\":81738},{\"end\":81769,\"start\":81754},{\"end\":81781,\"start\":81769},{\"end\":81791,\"start\":81781},{\"end\":82202,\"start\":82192},{\"end\":82215,\"start\":82202},{\"end\":82508,\"start\":82493},{\"end\":82521,\"start\":82508},{\"end\":82532,\"start\":82521},{\"end\":82542,\"start\":82532},{\"end\":82570,\"start\":82542},{\"end\":82590,\"start\":82570},{\"end\":82613,\"start\":82590},{\"end\":82622,\"start\":82613},{\"end\":82639,\"start\":82622},{\"end\":82646,\"start\":82639},{\"end\":83074,\"start\":83062},{\"end\":83085,\"start\":83074},{\"end\":83093,\"start\":83085},{\"end\":83103,\"start\":83093},{\"end\":83113,\"start\":83103},{\"end\":83125,\"start\":83113},{\"end\":83136,\"start\":83125},{\"end\":83479,\"start\":83466},{\"end\":83495,\"start\":83479},{\"end\":83512,\"start\":83495},{\"end\":83523,\"start\":83512},{\"end\":83834,\"start\":83817},{\"end\":83852,\"start\":83834},{\"end\":83861,\"start\":83852},{\"end\":83867,\"start\":83861},{\"end\":83885,\"start\":83867},{\"end\":83900,\"start\":83885},{\"end\":83915,\"start\":83900},{\"end\":83928,\"start\":83915},{\"end\":83947,\"start\":83928},{\"end\":83955,\"start\":83947},{\"end\":84505,\"start\":84492},{\"end\":84512,\"start\":84505},{\"end\":84523,\"start\":84512},{\"end\":84540,\"start\":84523},{\"end\":84555,\"start\":84540},{\"end\":84572,\"start\":84555},{\"end\":84588,\"start\":84572},{\"end\":84605,\"start\":84588},{\"end\":84615,\"start\":84605},{\"end\":84624,\"start\":84615},{\"end\":84635,\"start\":84624},{\"end\":84652,\"start\":84635},{\"end\":84660,\"start\":84652},{\"end\":85155,\"start\":85140},{\"end\":85171,\"start\":85155},{\"end\":85188,\"start\":85171},{\"end\":85206,\"start\":85188},{\"end\":85218,\"start\":85206},{\"end\":85697,\"start\":85682},{\"end\":85710,\"start\":85697},{\"end\":85725,\"start\":85710},{\"end\":85741,\"start\":85725},{\"end\":86074,\"start\":86046},{\"end\":86091,\"start\":86074},{\"end\":86104,\"start\":86091},{\"end\":86116,\"start\":86104},{\"end\":86488,\"start\":86480},{\"end\":86499,\"start\":86488},{\"end\":86513,\"start\":86499},{\"end\":86525,\"start\":86513},{\"end\":86536,\"start\":86525},{\"end\":86550,\"start\":86536},{\"end\":86559,\"start\":86550},{\"end\":86854,\"start\":86843},{\"end\":86870,\"start\":86854},{\"end\":86885,\"start\":86870},{\"end\":86895,\"start\":86885},{\"end\":86908,\"start\":86895},{\"end\":86927,\"start\":86908},{\"end\":86934,\"start\":86927},{\"end\":87320,\"start\":87302},{\"end\":87333,\"start\":87320},{\"end\":87353,\"start\":87333},{\"end\":87364,\"start\":87353},{\"end\":87383,\"start\":87364},{\"end\":87398,\"start\":87383},{\"end\":87412,\"start\":87398},{\"end\":87431,\"start\":87412},{\"end\":87450,\"start\":87431},{\"end\":87466,\"start\":87450},{\"end\":87830,\"start\":87815},{\"end\":87841,\"start\":87830},{\"end\":87858,\"start\":87841},{\"end\":87874,\"start\":87858},{\"end\":87890,\"start\":87874},{\"end\":87913,\"start\":87890},{\"end\":87929,\"start\":87913},{\"end\":87944,\"start\":87929},{\"end\":87957,\"start\":87944},{\"end\":87974,\"start\":87957},{\"end\":88362,\"start\":88348},{\"end\":88377,\"start\":88362},{\"end\":88662,\"start\":88645},{\"end\":88675,\"start\":88662},{\"end\":88692,\"start\":88675},{\"end\":89062,\"start\":89046},{\"end\":89323,\"start\":89309},{\"end\":89338,\"start\":89323},{\"end\":89700,\"start\":89685},{\"end\":89715,\"start\":89700},{\"end\":89726,\"start\":89715},{\"end\":89972,\"start\":89955},{\"end\":89989,\"start\":89972},{\"end\":90006,\"start\":89989},{\"end\":90022,\"start\":90006},{\"end\":90043,\"start\":90022},{\"end\":90058,\"start\":90043},{\"end\":90072,\"start\":90058},{\"end\":90090,\"start\":90072},{\"end\":90104,\"start\":90090},{\"end\":90121,\"start\":90104},{\"end\":90510,\"start\":90492},{\"end\":90722,\"start\":90705},{\"end\":90743,\"start\":90722},{\"end\":90761,\"start\":90743},{\"end\":90778,\"start\":90761},{\"end\":90793,\"start\":90778},{\"end\":90808,\"start\":90793},{\"end\":90829,\"start\":90808},{\"end\":90854,\"start\":90829},{\"end\":90869,\"start\":90854},{\"end\":90887,\"start\":90869},{\"end\":90905,\"start\":90887},{\"end\":90922,\"start\":90905},{\"end\":91398,\"start\":91380},{\"end\":91588,\"start\":91579},{\"end\":91599,\"start\":91588},{\"end\":91609,\"start\":91599},{\"end\":91618,\"start\":91609},{\"end\":91808,\"start\":91789},{\"end\":91826,\"start\":91808},{\"end\":91842,\"start\":91826},{\"end\":92155,\"start\":92139},{\"end\":92166,\"start\":92155},{\"end\":92180,\"start\":92166},{\"end\":92190,\"start\":92180},{\"end\":92526,\"start\":92510},{\"end\":92535,\"start\":92526},{\"end\":92548,\"start\":92535},{\"end\":92907,\"start\":92893},{\"end\":92926,\"start\":92907},{\"end\":92936,\"start\":92926},{\"end\":92952,\"start\":92936},{\"end\":92966,\"start\":92952},{\"end\":93211,\"start\":93200},{\"end\":93228,\"start\":93211},{\"end\":93252,\"start\":93228},{\"end\":93266,\"start\":93252},{\"end\":93288,\"start\":93266},{\"end\":93303,\"start\":93288},{\"end\":93321,\"start\":93303},{\"end\":93675,\"start\":93664},{\"end\":93692,\"start\":93675},{\"end\":93716,\"start\":93692},{\"end\":93730,\"start\":93716},{\"end\":93752,\"start\":93730},{\"end\":93767,\"start\":93752},{\"end\":93785,\"start\":93767},{\"end\":94153,\"start\":94124},{\"end\":94166,\"start\":94153},{\"end\":94183,\"start\":94166},{\"end\":94489,\"start\":94474},{\"end\":94511,\"start\":94489},{\"end\":94528,\"start\":94511},{\"end\":94546,\"start\":94528},{\"end\":94562,\"start\":94546},{\"end\":94578,\"start\":94562},{\"end\":94593,\"start\":94578},{\"end\":94932,\"start\":94915},{\"end\":94941,\"start\":94932},{\"end\":94956,\"start\":94941},{\"end\":94973,\"start\":94956},{\"end\":95234,\"start\":95220},{\"end\":95244,\"start\":95234},{\"end\":95260,\"start\":95244},{\"end\":95284,\"start\":95260},{\"end\":95572,\"start\":95562},{\"end\":95580,\"start\":95572},{\"end\":95848,\"start\":95831},{\"end\":95864,\"start\":95848},{\"end\":95880,\"start\":95864},{\"end\":96259,\"start\":96242},{\"end\":96272,\"start\":96259},{\"end\":96493,\"start\":96478},{\"end\":96877,\"start\":96851},{\"end\":96888,\"start\":96877},{\"end\":96895,\"start\":96888},{\"end\":97172,\"start\":97163},{\"end\":97184,\"start\":97172},{\"end\":97489,\"start\":97473},{\"end\":97510,\"start\":97489},{\"end\":97526,\"start\":97510},{\"end\":97880,\"start\":97865},{\"end\":97893,\"start\":97880},{\"end\":97911,\"start\":97893},{\"end\":97924,\"start\":97911},{\"end\":98249,\"start\":98237},{\"end\":98263,\"start\":98249},{\"end\":98267,\"start\":98263},{\"end\":98547,\"start\":98532},{\"end\":98558,\"start\":98547},{\"end\":98569,\"start\":98558},{\"end\":98580,\"start\":98569},{\"end\":98595,\"start\":98580},{\"end\":98614,\"start\":98595},{\"end\":98622,\"start\":98614},{\"end\":98633,\"start\":98622},{\"end\":98645,\"start\":98633},{\"end\":98659,\"start\":98645},{\"end\":98667,\"start\":98659},{\"end\":98676,\"start\":98667},{\"end\":98694,\"start\":98676},{\"end\":98703,\"start\":98694},{\"end\":99145,\"start\":99135},{\"end\":99151,\"start\":99145},{\"end\":99363,\"start\":99348},{\"end\":99382,\"start\":99363},{\"end\":99394,\"start\":99382},{\"end\":99410,\"start\":99394},{\"end\":99691,\"start\":99678},{\"end\":99701,\"start\":99691},{\"end\":99716,\"start\":99701},{\"end\":99736,\"start\":99716},{\"end\":99760,\"start\":99736},{\"end\":99778,\"start\":99760},{\"end\":100026,\"start\":100010},{\"end\":100048,\"start\":100026},{\"end\":100066,\"start\":100048},{\"end\":100080,\"start\":100066},{\"end\":100102,\"start\":100080},{\"end\":100119,\"start\":100102},{\"end\":100135,\"start\":100119},{\"end\":100147,\"start\":100135},{\"end\":100158,\"start\":100147},{\"end\":100182,\"start\":100158}]", "bib_venue": "[{\"end\":79313,\"start\":79281},{\"end\":94230,\"start\":94226},{\"end\":95941,\"start\":95926},{\"end\":74627,\"start\":74585},{\"end\":74944,\"start\":74927},{\"end\":75188,\"start\":75134},{\"end\":75659,\"start\":75531},{\"end\":75990,\"start\":75918},{\"end\":76425,\"start\":76415},{\"end\":76798,\"start\":76792},{\"end\":77270,\"start\":77223},{\"end\":77728,\"start\":77674},{\"end\":78208,\"start\":78159},{\"end\":78753,\"start\":78655},{\"end\":79279,\"start\":79232},{\"end\":79842,\"start\":79835},{\"end\":80491,\"start\":80470},{\"end\":80922,\"start\":80855},{\"end\":81291,\"start\":81273},{\"end\":81835,\"start\":81791},{\"end\":82256,\"start\":82235},{\"end\":82667,\"start\":82646},{\"end\":83150,\"start\":83136},{\"end\":83539,\"start\":83523},{\"end\":84053,\"start\":83955},{\"end\":84694,\"start\":84660},{\"end\":85274,\"start\":85218},{\"end\":85765,\"start\":85741},{\"end\":86172,\"start\":86116},{\"end\":86567,\"start\":86559},{\"end\":86960,\"start\":86934},{\"end\":87300,\"start\":87222},{\"end\":88027,\"start\":87974},{\"end\":88413,\"start\":88377},{\"end\":88729,\"start\":88692},{\"end\":89044,\"start\":88954},{\"end\":89382,\"start\":89338},{\"end\":89744,\"start\":89726},{\"end\":90146,\"start\":90140},{\"end\":90551,\"start\":90530},{\"end\":90987,\"start\":90945},{\"end\":91419,\"start\":91398},{\"end\":91646,\"start\":91618},{\"end\":91903,\"start\":91858},{\"end\":92239,\"start\":92190},{\"end\":92601,\"start\":92548},{\"end\":92891,\"start\":92805},{\"end\":93409,\"start\":93337},{\"end\":93870,\"start\":93801},{\"end\":94224,\"start\":94199},{\"end\":94472,\"start\":94380},{\"end\":95009,\"start\":94973},{\"end\":95333,\"start\":95284},{\"end\":95611,\"start\":95580},{\"end\":95924,\"start\":95903},{\"end\":96289,\"start\":96272},{\"end\":96559,\"start\":96535},{\"end\":96908,\"start\":96895},{\"end\":97219,\"start\":97209},{\"end\":97572,\"start\":97551},{\"end\":97962,\"start\":97944},{\"end\":98235,\"start\":98193},{\"end\":98711,\"start\":98703},{\"end\":99182,\"start\":99151},{\"end\":99461,\"start\":99426},{\"end\":99676,\"start\":99620},{\"end\":100288,\"start\":100198}]"}}}, "year": 2023, "month": 12, "day": 17}