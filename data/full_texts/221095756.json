{"id": 221095756, "updated": "2023-10-06 12:21:33.242", "metadata": {"title": "Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks", "authors": "[{\"first\":\"Jinyuan\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Xiaoyu\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Neil\",\"last\":\"Gong\",\"middle\":[\"Zhenqiang\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 8, "day": 11}, "abstract": "In a \\emph{data poisoning attack}, an attacker modifies, deletes, and/or inserts some training examples to corrupt the learnt machine learning model. \\emph{Bootstrap Aggregating (bagging)} is a well-known ensemble learning method, which trains multiple base models on random subsamples of a training dataset using a base learning algorithm and uses majority vote to predict labels of testing examples. We prove the intrinsic certified robustness of bagging against data poisoning attacks. Specifically, we show that bagging with an arbitrary base learning algorithm provably predicts the same label for a testing example when the number of modified, deleted, and/or inserted training examples is bounded by a threshold. Moreover, we show that our derived threshold is tight if no assumptions on the base learning algorithm are made. We empirically evaluate our method on MNIST and CIFAR10. For instance, our method can achieve a certified accuracy of $70.8\\%$ on MNIST when arbitrarily modifying, deleting, and/or inserting 100 training examples.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2008.04495", "mag": "3048759177", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/JiaCG21", "doi": "10.1609/aaai.v35i9.16971"}}, "content": {"source": {"pdf_hash": "5d15dfc3be39915a03c8d50172694613929144cf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.04495v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "87444e5c02d049569bd9209d5cf6b29b245f786b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5d15dfc3be39915a03c8d50172694613929144cf.txt", "contents": "\nIntrinsic Certified Robustness of Bagging against Data Poisoning Attacks\n\n\nJinyuan Jia jinyuan.jia@duke.edu \nDuke University Duke University Duke University\n\n\nXiaoyu Cao xiaoyu.cao@duke.edu \nDuke University Duke University Duke University\n\n\nNeil Zhenqiang Gong neil.gong@duke.edu \nDuke University Duke University Duke University\n\n\nIntrinsic Certified Robustness of Bagging against Data Poisoning Attacks\n\nIn a data poisoning attack, an attacker modifies, deletes, and/or inserts some training examples to corrupt the learnt machine learning model. Bootstrap Aggregating (bagging) is a well-known ensemble learning method, which trains multiple base models on random subsamples of a training dataset using a base learning algorithm and uses majority vote to predict labels of testing examples. We prove the intrinsic certified robustness of bagging against data poisoning attacks. Specifically, we show that bagging with an arbitrary base learning algorithm provably predicts the same label for a testing example when the number of modified, deleted, and/or inserted training examples is bounded by a threshold. Moreover, we show that our derived threshold is tight if no assumptions on the base learning algorithm are made. We empirically evaluate our method on MNIST and CIFAR10. For instance, our method can achieve a certified accuracy of 70.8% on MNIST when arbitrarily modifying, deleting, and/or inserting 100 training examples.\n\nIntroduction\n\nMachine learning models trained on user-provided data are vulnerable to data poisoning attacks [30,6,41,23,35,34], in which malicious users carefully poison (i.e., modify, delete, and/or insert) some training examples such that the learnt model is corrupted and makes predictions for testing examples as an attacker desires. In particular, the corrupted model predicts incorrect labels for a large fraction of testing examples indiscriminately (i.e., a large testing error rate) or for some attacker-chosen testing examples. Unlike adversarial examples [37,10], which carefully perturb each testing example such that a model predicts an incorrect label for the perturbed testing example, data poisoning attacks corrupt the model such that it predicts incorrect labels for many clean testing examples. Like adversarial examples, data poisoning attacks pose severe security threats to machine learning systems.\n\nTo mitigate data poisoning attacks, various defenses [13,3,36,38,17,19,25,32] have been proposed in the literature. Most of these defenses [13,3,36,38,17,19] achieve empirical robustness against certain data poisoning attacks and are often broken by strong adaptive attacks. To end the cat-and-mouse game between attackers and defenders, certified defenses [25,32] were proposed. We say a learning algorithm is certifiably robust against data poisoning attacks if it can learn a classifier that provably predicts the same label for a testing example when the number of poisoned training examples is bounded. For instance, Ma et al. [25] showed that a classifier trained with differential privacy certifies robustness against data poisoning attacks. Rosenfeld et al. [32] leveraged randomized smoothing [12], which was originally designed to certify robustness against adversarial examples, to certify robustness against a particular type of data poisoning attacks called label flipping attacks, which only flip the labels of existing training examples. This randomized smoothing based defense can also be generalized to certify robustness against data poisoning attacks that modify both features and labels of existing training examples. However, these certified defenses suffer from two major limitations. First, they are only applicable to limited scenarios, i.e., Ma et al. [25] is limited to learning algorithms that can be differentially private, while Rosenfeld et al. [32] is limited to data poisoning attacks that only modify existing training examples. Second, their certified robustness guarantees are loose, meaning that a learning algorithm is certifiably more robust than their guarantees indicate. We note that Steinhardt et al. [35] derives an approximate upper bound of the loss function for data poisoning attacks. However, their method cannot certify that the learnt model predicts the same label for a testing example. We aim to address these limitations in this work. Our approach is based on a well-known ensemble learning method called Bootstrap Aggregating (bagging) [9]. Given a training dataset, bagging first generates N subsamples by sampling from the training dataset with replacement uniformly at random, where each subsample includes k training examples. Then, bagging uses a base learning algorithm to train a base classifier on each subsample. Given a testing example, bagging uses each base classifier to predict its label and takes majority vote among the predicted labels as the final predicted label. We show that bagging with any base learning algorithm is certifiably robust against data poisoning attacks. Figure 1 shows a toy example to illustrate why bagging certifies robustness against data poisoning attacks. When the poisoned training examples are minority in the training dataset, the sampled k training examples do not include any poisoned training examples with a high probability. Therefore, a majority of the N base classifiers in bagging and bagging's predicted labels for testing examples are not influenced by the poisoned training examples. Formally, we show that bagging predicts the same label for a testing example when the number of poisoned training examples is no larger than a threshold. We call the threshold certified poisoning size. Moreover, we show that our derived certified poisoning size is tight if no assumptions on the base learning algorithm are made. Our certified poisoning size is the optimal solution to an optimization problem and we design an efficient algorithm to solve the optimization problem. We also empirically evaluate our method on MNIST and CIFAR10. For instance, our method can achieve a certified accuracy of 70.8% on MNIST when 100 training examples are arbitrarily poisoned, where k = 100 and N = 1, 000. Under the same setting, Ma et al. [25] and Rosenfeld et al. [32] achieve 0 certified accuracy. Finally, we show that training the base classifiers using transfer learning can significantly improve the certified accuracy.\n\nOur contributions are summarized as follows:\n\n\u2022 We derive the first intrinsic certified robustness of bagging against data poisoning attacks and prove the tightness of our robustness guarantee.\n\n\u2022 We develop an efficient algorithm to compute the certified poisoning size in practice.\n\n\u2022 We empirically evaluate our method on MNIST and CIFAR10.\n\nAll the proofs to our theorems are shown in the Appendix.\n\n\nCertified Robustness of Bagging against Data Poisoning Attacks\n\nAssuming we have a training dataset D = {(x 1 , y 1 ), (x 2 , y 2 ), \u00b7 \u00b7 \u00b7 , (x n , y n )} with n examples, where x i and y i are the feature vector and label of the ith training example, respectively. Moreover, we are given an arbitrary deterministic or randomized base learning algorithm A, which takes a training dataset D as input and outputs a classifier f , i.e., f = A(D). f (x) is the predicted label for a testing example x. For convenience, we jointly represent the training and testing processes as A(D, x), which is x's label predicted by a classifier that is trained using algorithm A and training dataset D. Data poisoning attacks: In a data poisoning attack, an attacker poisons the training dataset D such that the learnt classifier makes predictions for testing examples as the attacker desires. In particular, the attacker can carefully modify, delete, and/or insert some training examples in D such that A(D, x) = A(D , x) for many testing examples x or some attackerchosen x, where D is the poisoned training dataset. We note that modifying a training example means modifying its feature vector and/or label. We denote the set of poisoned training datasets with at most r poisoned training examples as follows: Bootstrap aggregating (Bagging) [9]: Bagging is a well-known ensemble learning method. Roughly speaking, bagging creates many subsamples of a training dataset with replacement and trains a classifier on each subsample. For a testing example, bagging uses each classifier to predict its label and takes majority vote among the predicted labels as the label of the testing example. Next, we describe a probabilistic view of bagging, which makes it possible to theoretically analyze the certified robustness of bagging against data poisoning attacks. Specifically, we denote by g(D) a list of k examples that are sampled from D with replacement uniformly at random. Since g(D) is random, the predicted label A(g(D), x) is also random. We denote by p j = Pr(A(g(D), x) = j) the label probability for label j, where j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c}. Bagging predicts the label with the largest label probability for x, i.e., h(D, x) =arg max j\u2208{1,2,\u00b7\u00b7\u00b7 ,c} p j is the label that bagging predicts for x. h(D , x) is the label that bagging predicts for x when the training dataset is poisoned.\nB(D, r) = {D | max{|D|, |D |} \u2212 |D \u2229 D | \u2264 r}.(1)\nCertified robustness of bagging: We show the certified robustness of bagging. In particular, we show that bagging predicts the same label for a testing example when the number of poisoned training examples is no larger than some threshold (called certified poisoning size). Moreover, we show that our derived certified poisoning size is tight. Our major theoretical results are summarized in the following two theorems.\n\nTheorem 1 (Certified Poisoning Size of Bagging). Suppose we have a training dataset D, a base learning algorithm A, and a testing example x. g(D) is a list of k training examples sampled from D uniformly at random with replacement. l, s, p l \u2208 [0, 1], and p s \u2208 [0, 1] satisfy the following:\nPr(A(g(D), x) = l) \u2265 p l \u2265 p s \u2265 max j =l Pr(A(g(D), x) = j),(2)\nwhere l and s are the labels with the largest and second largest probabilities under bagging, respectively. p l is a lower bound of the largest label probability, while p s is an upper bound of the second largest label probability. Then, bagging predicts label l for x when the number of poisoned training examples is bounded by r * , i.e., we have:\nh(D , x) = l, \u2200D \u2208 B(D, r * ),(3)\nwhere r * is called certified poisoning size and is the solution to the following optimization problem:\nr * = arg max r r s.t. max n\u2212r\u2264n \u2264n+r ( n n ) k \u2212 2 \u00b7 ( max(n, n ) \u2212 r n ) k + 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s ) < 0,(4)\nwhere n = |D|, n = |D |, \u03b4 l = p l \u2212 ( p l \u00b7 n k )/n k , and \u03b4 s = ( p s \u00b7 n k )/n k \u2212 p s . n + r and n \u2212 r are respectively the maximum and minimum sizes of the poisoned training dataset when the number of poisoned training examples is r.\n\nGiven Theorem 1, we have the following corollaries.  (4) is  \nr * = n \u00b7 (1 \u2212 k 1 \u2212 p l \u2212p s \u2212\u03b4 l \u2212\u03b4s 2 ) \u2212 1 .* = n \u00b7 (1 \u2212 k 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s )) \u2212 1 .= n \u00b7 ( k 1 + (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s ) \u2212 1) \u2212 1 .\nTheorem 2 (Tightness of the Certified Poisoning Size). Assuming we have p l + p s \u2264 1, p l + (c \u2212 1) \u00b7 p s \u2265 1, and \u03b4 l = \u03b4 s = 0. Then, for any r > r * , there exist a base learning algorithm A * consistent with (2) and a poisoned training dataset D with r poisoned training examples such that arg max j\u2208{1,2,\u00b7\u00b7\u00b7 ,c} Pr(A * (g(D ), x) = j) = l or there exist ties.\n\nWe have several remarks about our theorems. Remark 1: Our Theorem 1 is applicable for any base learning algorithm A. In other words, bagging with any base learning algorithm is provably robust against data poisoning attacks.\n\nRemark 2: For any lower bound p l of the largest label probability and upper bound p s of the second largest label probability, our Theorem 1 derives a certified poisoning size. In particular, our certified poisoning size is related to the gap between the two probability bounds. If we can estimate tighter probability bounds, then we may certify a larger poisoning size. We use the probability bounds instead of the exact label probabilities p l and p s , because it is challenging to exactly compute them.\n\nRemark 3: Theorem 2 shows that when no assumptions on the base learning algorithm are made, it is impossible to certify a poisoning size that is larger than ours.\n\n\nComputing the Certified Poisoning Size\n\nGiven a learning algorithm A, a training dataset D, parameter k, and e testing examples in D e , we aim to estimate the predicted label and certified poisoning size for each testing example. Specifically, for a testing example, our certified poisoning size relies on a lower bound of the largest label probability and an upper bound of the second largest label probability. Therefore, we use a Monte-Carlo method to estimate these probability bounds with a probabilistic guarantee. Next, we describe estimating the probability bounds, solving the optimization problem in (4) using the probability bounds, and our complete certification algorithm.\n\nEstimating probability bounds p l and p s : One way to estimate p l and p s is to use the Monte-Carlo method proposed by [12]. In particular, p l is estimated using the one-sided Clopper-Pearson method [11] and p s is estimated as 1 \u2212 p l . However, such estimated p s may be loose. To address the challenge, we adopt the simultaneous confidence interval estimation method called SimuEM [20] to estimate p l and p s simultaneously. Specifically, we first randomly sample N subsamples L 1 , L 2 , \u00b7 \u00b7 \u00b7 , L N from D with replacement, each of which has k training examples. Then, we train a classifier f o for each subsample L o using the learning algorithm A, where o = 1, 2, \u00b7 \u00b7 \u00b7 , N . We can use the N classifiers to estimate the predicted label l, p l , and p s for x with a confidence level at least 1 \u2212 \u03b1. A naive procedure is to train such N classifiers for each testing example, which is very computationally expensive. To address the computational challenge, we propose to train such N classifiers for each e testing examples. Our key idea is to divide the confidence level among e testing examples such that we can estimate their predicted labels and certified poisoning sizes using the same N classifiers with a simultaneous confidence level at least 1 \u2212 \u03b1.\n\nSpecifically, for each testing example x i in D e , we count the frequency of each label predicted by the N classifiers, i.e.,\nn j = N o=1 I(f o (x i ) = j),\nwhere I is the indicator function. Each n j follows a binomial distribution with parameters N and p j . Thus, we can adopt the Clopper-Pearson method to obtain a one-sided confidence interval for each label probability p j . Then, we can leverage Bonferroni correction to obtain simultaneous confidence intervals for all label probabilities. Formally, we estimate l as the label with the largest frequency n l and we have the following probability bounds [20]:\np l = Beta( \u03b1/e c ; n l , N \u2212 n l + 1) (5) Algorithm 1 Certify Input: A, D, k, N , D e , \u03b1.\nOutput: Predicted label and certified poisoning size for each testing example.\nf 1 , f 2 , \u00b7 \u00b7 \u00b7 , f N \u2190 TrainUnderSample(A, D, k, N ) for x i in D e do counts[j] \u2190 N o=1 I(f o (x i ) = j), j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,\nc} l i , s i \u2190 top two indices in counts (ties are broken uniformly at random).\np l i , p s i \u2190 SimuEM(counts, \u03b1 e ) if p l i > p s i then y i \u2190 l \u00ee r * i \u2190 BinarySearch(p l i , p s i , k, |D|) els\u00ea y i ,r * i \u2190 ABSTAIN, ABSTAIN end if end for return\u0177 1 ,\u0177 2 , \u00b7 \u00b7 \u00b7 ,\u0177 e andr * 1 ,r * 2 , \u00b7 \u00b7 \u00b7 ,r * e p j = Beta(1 \u2212 \u03b1/e c ; n j , N \u2212 n j + 1), \u2200j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c} \\ {l},(6)\nwhere 1 \u2212 \u03b1/e is the confidence level and Beta(\u03b2; \u03bb, \u03b8) is the \u03b2th quantile of the Beta distribution with shape parameters \u03bb and \u03b8. One natural method to estimate p s is that p s = max j =l p j . However, this bound may be loose. For example, p l + p s may be larger than 1. Therefore, we estimate p s as p s = min(max j =l p j , 1 \u2212 p l ).\n\nComputing the certified poisoning size: Given the estimated label probability bounds for a testing example, we solve the optimization problem in (4) to obtain its certified poisoning size r * . We design an efficient binary search based method to solve r * . Specifically, we use binary search to find the largest r such that the constraint in (4) is satisfied. We denote the left-hand side of the constraint as max n\u2212r\u2264n \u2264n+r L(n). For a given r, a naive way to check whether the constraint max n\u2212r\u2264n \u2264n+r L(n ) < 0 holds is to check whether L(n ) < 0 holds for each n in the range [n \u2212 r, n + r], which could be inefficient when r is large. To reduce the computation cost, we derive the following analytical form of n at which L(n ) reaches its maximum value for a given r:\narg max n\u2212r\u2264n \u2264n+r L(n ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 n, if r \u2264 n \u00b7 (1 \u2212 k\u22121 1 2 ) n + r, if r \u2265 n \u00b7 ( k\u22121 \u221a 2 \u2212 1) r 1\u2212 k\u22121 1 2 or r 1\u2212 k\u22121 1 2 , otherwise.(7)\nTherefore, for a given r, we only need to check whether L(n ) < 0 holds for at most two different n . The details of deriving (7) are shown in Supplemental Material. Complete certification algorithm: Algorithm 1 shows our certification process to estimate the predicted labels and certified poisoning sizes for e testing examples in D e . The function TrainUnderSample randomly samples N subsamples and trains N classifiers. The function SimuEM estimates the probability bounds p l i and p s i . The function BinarySearch solves the optimization problem in (4) to obtain the certified poisoning sizer * i for testing example x i . Roughly speaking, the following theorem shows that, with probability at least 1 \u2212 \u03b1, if Certify does not ABSTAIN, then it returns a valid certified poisoning size, for every testing example in D e . In other words, the probability that Certify returns an incorrect certified poisoning size for at least one testing example is at most \u03b1.\n\nTheorem 3. Algorithm Certify has the following probabilistic guarantee:\nPr(\u2229 x i \u2208De ((\u2200D \u2208 B(D,r * i ), h(D , x i ) =\u0177 i )|\u0177 i = ABSTAIN)) \u2265 1 \u2212 \u03b1.(8)\n\nExperiments\n\n\nExperimental Setup\n\nDatasets and classifiers: We perform experiments on MNIST and CIFAR10. The base learning algorithm is neural network, and we use the example neural network architectures 12 in Keras for the two datasets. The number of training examples in the two datasets are 60, 000 and 50, 000, respectively, which are the training datasets that we aim to certify. Both datasets have 10,000 testing examples, which are the D e in our algorithm. Evaluation metric: We use certified accuracy as our evaluation metric. In particular, for a given r (i.e., number of poisoned training examples), the certified accuracy can be computed as follows:\nCA r = x i \u2208De I(\u0177 i = y i ) \u00b7 I(r * i \u2265 r) |D e | ,(9)\nwhere y i is the ground truth label for testing example x i , and\u0177 i andr * i respectively are the predicted label and certified poisoning size returned by our Certify algorithm for x i . Intuitively, the certified accuracy is the fraction of testing examples whose labels are correctly predicted and whose certified poisoning sizes are no smaller than r. In other words, when the number of poisoned training examples is r, bagging's testing accuracy for D e is at least CA r with a confidence level 1 \u2212 \u03b1.\n\nParameter setting: Our method has three parameters, i.e., k, \u03b1, and N . Unless otherwise mentioned, we adopt the following default settings for them: k = 100, \u03b1 = 0.001, and N = 1, 000 for MNIST; and k = 1, 000, \u03b1 = 0.001, and N = 1, 000 for CIFAR10. In our experiments, we will study the impact of each parameter while setting the remaining parameters to their default values. Note that training the N classifiers can be easily parallelized. We performed experiments on a server with 80 CPUs@2.1GHz, 8 GPUs (RTX 6,000), and 385 GB main memory.\n\nCompared methods: We compare with a differential privacy based method [25] and a randomized smoothing based method [32]. Since these methods are not scalable because they train N classifiers on the entire training dataset, we perform comparisons on the MNIST 1/7 dataset that just consists of the digits 1 and 7. This subset includes 13,007 training examples and 2,163 testing examples.\n\n\u2022 Ma et al. [25]. Ma et al. [25] showed that a classifier trained with differential privacy achieves certified robustness against data poisoning attacks. Suppose ACC r is the testing accuracy on D e of a differentially private classifier that is trained using a poisoned training dataset with r poisoned training examples. Based on the Theorem 3 in [25] (i.e., via treating the testing accuracy as their cost function), we have the expected testing accuracy E(ACC r ) is lower bounded by a certain function of E(ACC), r, and ( , \u03b4) (the function can be found in their Theorem 3), where E(ACC) is the expected testing accuracy of a differentially private classifier that is trained using the clean training dataset and ( , \u03b4) are the differential privacy parameters. The randomness in E(ACC r ) and E(ACC) are from differential privacy. This lower bound is the certified accuracy that the method achieves. A lower bound of E(ACC) can be further estimated with confidence level 1 \u2212 \u03b1 via training N differentially private classifiers on the entire clean training dataset. However, for simplicity, we estimate E(ACC) as the average testing accuracies of the N differentially private classifiers, which gives advantages for this method. We use DP-SGD [1] 3 to train differentially private classifiers. Moreover, we set = 0.3 and \u03b4 = 10 \u22125 such that this method and our method achieve comparable certified accuracies when r = 0.\n\n\u2022 Rosenfeld et al. [32]. Rosenfeld [32] proposed a randomized smoothing based method to certify robustness against label flipping attacks, which only flip the labels of existing training examples. This method can be generalized to certify robustness against data poisoning attacks that modify both features and labels of existing training examples via randomly flipping both features and labels of training examples. In particular, we binarize the features to apply this method. Like our method, they also train N classifiers to estimate the certified accuracy with a confidence level 1 \u2212 \u03b1. 4 However, unlike our method, when training a classifier, they flip each feature/label value in the training dataset with probability \u03b2 and use the entire noisy training dataset. When predicting the label of a testing example, this method takes a majority vote among the N classifiers. We set \u03b2 = 0.3 such that this method and our method achieve comparable certified accuracies when r = 0. We note that this method certifies the number of poisoned features/labels in the training dataset. We transform this certificate to the number of poisoned training examples as F d+1 , where F is the certified number of features/labels and d + 1 is the number of features/label of a training example (d features + one label). We have d = 784 for MNIST.\n\n\nExperimental Results\n\nImpact of k, \u03b1, and N : Figure 2 shows the impact of k, \u03b1, and N on the certified accuracy of our method. As the results show, k controls a tradeoff between accuracy under no poisoning and robustness. Specifically, when k is larger, our method has a higher accuracy when there are no data poisoning attacks (i.e., r = 0) but the certified accuracy drops more quickly as the number of poisoned training examples increases. The reason is that a larger k makes it more likely to sample poisoned training examples when creating the subsamples in bagging. The certified accuracy increases as \u03b1 or N increases. The reason is that a larger \u03b1 or N produces tighter estimated probability bounds, which make the certified poisoning sizes larger. We also observe that the certified accuracy is relatively insensitive to \u03b1.\n\nTransfer learning improves certified accuracy: Our method trains multiple classifiers and each classifier is trained using k training examples. Improving the accuracy of each classifier can improve the certified accuracy. We explore using transfer learning to train more  accurate classifiers. Specifically, we use the Inception-v3 classifier pretrained on ImageNet to extract features and we leverage a public implementation 5 to train our classifiers on CIFAR10. Figure 3(a) shows that transfer learning can significantly increase our certified accuracy, where k = 100, \u03b1 = 0.001, and N = 1, 000.\n\nComparing with Ma et al. [25] and Rosenfeld et al. [32]: Figure 3(b) compares our method with previous methods on the MNIST 1/7 dataset, where k = 50, \u03b1 = 0.001, and N = 1, 000. Our method significantly outperforms existing methods. For example, our method can achieve 96.95% certified accuracy when the number of poisoned training examples is r = 50, while the certified accuracy is 0 under the same setting for the two existing methods. Figure 3(c) shows that our method is also more efficient than existing methods. The reason is that our method trains classifiers on a small number of training examples while existing methods train classifiers on the entire training dataset. Ma et al. [25] outperforms Rosenfeld et al. [32] because differential privacy directly certifies robustness against modification/deletion/insertion of training examples while randomized smoothing was designed to certify robustness against modifications of features/labels.\n\n\nRelated Work\n\nData poisoning attacks carefully modify, delete, and/or insert some training examples in the training dataset such that a learnt model makes incorrect predictions for many testing examples indiscriminately (i.e., the learnt model has a large testing error rate) or for some attacker-chosen testing examples. For instance, data poisoning attacks have been shown to be effective for Bayes classifiers [30], SVMs [6], neural networks [42,29,36,34], linear regression models [27,19], PCA [33], LASSO [41], collaborative filtering [23,43,16,15], clustering [7,8], graph-based methods [46,40,21,44], federated learning [14,4,2], and others [28,26,22,45]. We note that backdoor attacks [18,24] also poison the training dataset. However, unlike data poisoning attacks, backdoor attacks also inject perturbation (i.e., a trigger) to testing examples.\n\nOne category of defenses [13,3,36,38] aim to detect the poisoned training examples based on their negative impact on the error rate of the learnt model. Another category of defenses [17,19] aim to design new loss functions, solving which detects the poisoned training examples and learns a model simultaneously. For instance, Jagielski et al. [19] proposed to jointly optimize the selection of a subset of training examples with a given size and a model that minimizes the loss function; and the unselected training examples are treated as poisoned ones. Steinhardt et al. [35] assumes that a model is trained only using examples in a feasible set and derives an approximate upper bound of the loss function for any data poisoning attacks under these assumptions. However, all of these defenses cannot certify that the learnt model predicts the same label for a testing example under data poisoning attacks.\n\nMa et al. [25] shows that differentially private models certify robustness against data poisoning attacks. Rosenfeld et al. [32] leverages randomized smoothing to certify robustness against label flipping attacks, which can be generalized to certify robustness against data poisoning attacks that modify both features and labels of existing training examples. Wang et al. [39] proposes to use randomized smoothing to certify robustness against backdoor attacks, which is also applicable to certify robustness against data poisoning attacks. However, these defenses achieve loose certified robustness guarantees. Moreover, Ma et al. [25] is only applicable to learning algorithms that can be differentially private, while Rosenfeld et al. [32] and Wang et al. [39] are only applicable to data poisoning attacks that modify existing training examples. Biggio et al. [5] proposed bagging as an empirical defense against data poisoning attacks. However, they did not derive the certified robustness of bagging.\n\n\nConclusion\n\nData poisoning attacks pose severe security threats to machine learning systems via poisoning the training dataset. In this work, we show the intrinsic certified robustness of bagging against data poisoning attacks, i.e., bagging can transform any learning algorithm to be certifiably robust against data poisoning attacks. Specifically, we show that bagging predicts the same label for a testing example when the number of poisoned training examples is bounded. Moreover, we show that our derived bound is tight if no assumptions on the learning algorithm are made. We also empirically demonstrate the effectiveness of our method using MNIST and CIFAR10. Our results show that our method achieves much better certified robustness and is more efficient than existing certified defenses. Interesting future work includes: 1) generalizing our method to other types of data, e.g., graphs, and 2) improving our method by leveraging meta-learning.\n\n\nA Proof of Theorem 1\n\nWe first define some notations that will be used in our proof. Given a training dataset D and its poisoned version D , we define the following two random variables:\nX = g(D) (10) Y = g(D ),(11)\nwhere X and Y respectively are two random lists with k examples sampled from D and D with replacement uniformly at random. We denote by I = D \u2229 D the set of overlapping training examples in the two datasets. We use \u2126 to denote the space of random lists g(D \u222a D ), i.e., each element in \u2126 is a list with k examples sampled from D \u222a D with replacement uniformly at random. For convenience, we define operators , as follows:\nDefinition 1 ( , ).\nAssuming \u03c9 \u2208 \u2126 is a list of k examples and S is a set of examples, we say \u03c9 S if \u2200w \u2208 \u03c9, w \u2208 S. We say \u03c9 S if \u2203w \u2208 \u03c9, w \u2208 S.\n\nFor instance, we have X D and Y D . Before proving our theorem, we show a variant of the Neyman-Pearson Lemma [31] that will be used in our proof.\n\nLemma 1 (Neyman-Pearson Lemma). Suppose X and Y are two random variables in the space \u2126 with probability distributions \u00b5 x and \u00b5 y , respectively. Let M : \u2126 \u2212 \u2192 {0, 1} be a random or deterministic function. Then, we have the following:\n= 1 t \u00b7 ( \u03c9\u2208\u2126 M (1|\u03c9) \u00b7 \u00b5 x (\u03c9) \u2212 \u03c9\u2208S \u00b5 x (\u03c9)) (18) = 1 t \u00b7 (Pr(M (X) = 1) \u2212 Pr(X \u2208 S))(19)\n\u22650.\n\nWe obtain (17) from (15) because \u00b5 x (\u03c9) \u2265 t \u00b7 \u00b5 y (\u03c9), \u2200\u03c9 \u2208 S and \u00b5 x (\u03c9) \u2264 t \u00b7 \u00b5 y (\u03c9), \u2200\u03c9 \u2208 S c . We have the last inequality because Pr(M (X) = 1) \u2265 Pr(X \u2208 S).\n\nNext, we prove our Theorem 1. Our goal is to show that h(D , x) = l, i.e., Pr(A(Y, x) = l) > max j =l Pr(A(Y, x) = j). Our key idea is to derive a lower bound of Pr(A(Y, x) = l) and an upper bound of max j =l Pr(A(Y, x) = j), where the lower bound and upper bound can be easily computed. We derive the lower bound and upper bound using the Neyman-Pearson Lemma. Then, we derive the certified poisoning size by requiring the lower bound to be larger than the upper bound. Next, we derive the lower bound, the upper bound, and the certified poisoning size.\n\nDeriving a lower bound of Pr(A(Y, x) = l): We first define the following residual:\n\u03b4 l = p l \u2212 ( p l \u00b7 n k )/n k .(21)\nWe define a binary function M (\u03c9) = I(A(\u03c9, x) = l) over the space \u2126, where \u03c9 \u2208 \u2126 and I is the indicator function. Then, we have Pr(A(Y, x) = l) = Pr(M (Y ) = 1). Our idea is to construct a subspace for which we can apply the first part of Lemma 1 to derive a lower bound of Pr(M (Y ) = 1). We first divide the space \u2126 into three subspaces as follows:\nB = {\u03c9 \u2208 \u2126|\u03c9 D, \u03c9 I},(22)C = {\u03c9 \u2208 \u2126|\u03c9 D , \u03c9 I},(23)E = {\u03c9 \u2208 \u2126|\u03c9 I}.(24)\nSince we sample k training examples with replacement uniformly at random, we have the following:\nPr(X = \u03c9) = 1 n k , if \u03c9 \u2208 B \u222a E 0, otherwise(25)Pr(Y = \u03c9) = 1 (n ) k , if \u03c9 \u2208 C \u222a E 0, otherwise(26)\nWe denote by m the size of I, i.e., m = |I|. Then, we have the following:\nPr(X \u2208 E) = ( m n ) k , Pr(X \u2208 B) = 1 \u2212 ( m n ) k , and Pr(X \u2208 C) = 0.(27)Pr(Y \u2208 E) = ( m n ) k , Pr(Y \u2208 C) = 1 \u2212 ( m n ) k , and Pr(Y \u2208 B) = 0.(28)\nWe have Pr(X \u2208 E) = ( m n ) k because each of the k examples is sampled independently from I with probability m n . Furthermore, since Pr(X \u2208 B)+Pr(X \u2208 E) = 1, we obtain Pr(X \u2208 B) = 1\u2212( m n ) k . Since X D , we have Pr(X \u2208 C) = 0. Similarly, we can compute the probabilities in (28).\n\nWe assume p l \u2212 \u03b4 l \u2212 (1 \u2212 ( m n ) k ) \u2265 0. We can make this assumption because we only need to find a sufficient condition for h(D , x) = l. We define B \u2286 E, i.e., B is a subset of E, such that we have the following:\nPr(X \u2208 B ) = p l \u2212 \u03b4 l \u2212 Pr(X \u2208 B) = p l \u2212 \u03b4 l \u2212 (1 \u2212 ( m n ) k ).(29)\nWe can find such subset because p l \u2212 \u03b4 l is an integer multiple of 1 n k . Moreover, we define R as follows:\nR = B \u222a B .(30)\nThen, based on (2), we have:\nPr(A(X, x) = l) \u2265 p l \u2212 \u03b4 l = Pr(X \u2208 R).(31)\nTherefore, we have the following:\nPr(M (X) = 1) = Pr(A(X, x) = l) \u2265 Pr(X \u2208 R).(32)\nFurthermore, we have Pr(X = \u03c9) > \u03b3 \u00b7 Pr(Y = \u03c9) if and only if \u03c9 \u2208 B and Pr(X = \u03c9) = \u03b3 \u00b7 Pr(Y = \u03c9) if \u03c9 \u2208 B , where \u03b3 = ( n n ) k . Therefore, based on the definition of R in (30) and the condition (32), we can apply Lemma 1 to obtain the following:\nPr(M (Y ) = 1) = Pr(A(Y, x) = l) \u2265 Pr(Y \u2208 R).(33)\nPr(Y \u2208 R) is a lower bound of Pr(A(Y, x) = l) and can be computed as follows:\nPr(Y \u2208 R) (34) =Pr(Y \u2208 B) + Pr(Y \u2208 B ) (35) =Pr(Y \u2208 B )(36)\n=Pr(X \u2208 B )/\u03b3 (37)\n= 1 \u03b3 \u00b7 (p l \u2212 \u03b4 l \u2212 (1 \u2212 ( m n ) k )),(38)\nwhere we have (36) from (35) because Pr(Y \u2208 B) = 0, (37) from (36) because Pr(X = \u03c9) = \u03b3 \u00b7 Pr(Y = \u03c9) for \u03c9 \u2208 B , and the last equation from (29).\n\nDeriving an upper bound of max j =l Pr(A(Y, x) = j): We define the following residual:\n\u03b4 j = ( p j \u00b7 n k )/n k \u2212 p j , \u2200j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c} \\ {l}.(39)\nWe leverage the second part of Lemma 1 to derive such an upper bound. We assume Pr(X \u2208 E) \u2265 p j + \u03b4 j , \u2200j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c} \\ {l}. We can make the assumption because we derive a sufficient condition for h(D , x) = l. For \u2200j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c} \\ {l}, we define C j \u2286 E such that we have the following:\nPr(X \u2208 C j ) = p j + \u03b4 j .(40)\nWe can find such C j because p j + \u03b4 j is an integer multiple of 1 n k . Moreover, we define the following space:\nQ j = C \u222a C j .(41)\nTherefore, based on (2), we have:\nPr(A(X, x) = j) \u2264 p j + \u03b4 j = Pr(X \u2208 Q j ).(42)\nWe define a function M j (\u03c9) = I(A(\u03c9, x) = j), where \u03c9 \u2208 \u2126. Based on Lemma 1, we have the following:\nPr(M (Y ) = 1) = Pr(A(Y, x) = j) \u2264 Pr(Y \u2208 Q j ),(43)\nwhere Pr(Y \u2208 Q j ) can be computed as follows:\nPr(Y \u2208 Q j ) (44) =Pr(Y \u2208 C) + Pr(Y \u2208 C j ) (45) =1 \u2212 ( m n ) k + Pr(Y \u2208 C j ) (46) =1 \u2212 ( m n ) k + Pr(X \u2208 C j )/\u03b3 (47) =1 \u2212 ( m n ) k + 1 \u03b3 \u00b7 (p j + \u03b4 j ).(48)\nTherefore, we have:\nmax j =l Pr(A(Y, x) = j) (49) \u2264 max j =l Pr(Y \u2208 Q j ) (50) =1 \u2212 ( m n ) k + 1 \u03b3 \u00b7 max j =l (p j + \u03b4 j ) (51) \u22641 \u2212 ( m n ) k + 1 \u03b3 \u00b7 (p s + \u03b4 s ),(52)\nwhere p s + \u03b4 s \u2265 max j =l (p j + \u03b4 j ).\n\nDeriving the certified poisoning size: To reach the goal Pr(A(Y, x) = l) > max j =l\nPr(A(Y, x) = j)\n, it is sufficient to have the following:\n1 \u03b3 \u00b7 (p l \u2212 \u03b4 l \u2212 (1 \u2212 ( m n ) k )) > 1 \u2212 ( m n ) k + 1 \u03b3 \u00b7 (p s + \u03b4 s ) (53) \u21d0\u21d2( n n ) k \u2212 2 \u00b7 ( m n ) k + 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s ) < 0.(54)\nTaking all poisoned training datasets D (i.e., n \u2212 r \u2264 n \u2264 n + r) into consideration, we have the following sufficient condition:\nmax n\u2212r\u2264n \u2264n+r ( n n ) k \u2212 2 \u00b7 ( m n ) k + 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s ) < 0.(55)\nNote that m = max(n, n ) \u2212 r. Furthermore, when the above condition (55) is satisfied, we have p l \u2212 \u03b4 l \u2212 (1 \u2212 ( m n ) k ) \u2265 0 and Pr(X \u2208 E) = ( m n ) k \u2265 p j + \u03b4 j , \u2200j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c} \\ {l}, which are the conditions when we can construct the spaces B and C j . The certified poisoning size r * is the maximum r that satisfies the above sufficient condition. In other words, our certified poisoning size r * is the solution to the following optimization problem:\nr * = arg max r r s.t. max n\u2212r\u2264n \u2264n+r ( n n ) k \u2212 2 \u00b7 ( max(n, n ) \u2212 r n ) k + 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s ) < 0.(56)\n\nB Proof of Theorem 2\n\nOur idea is to construct a learning algorithm A * such that the label l is not predicted by the bagging predictor or there exist ties. When r > r * and \u03b4 l = \u03b4 s = 0, there exists a poisoned training dataset D with a certain n \u2208 [n \u2212 r, n + r] such that we have:\n( n n ) k \u2212 2 \u00b7 ( max(n, n ) \u2212 r n ) k + 1 \u2212 (p l \u2212 p s ) \u2265 0 (57) \u21d0\u21d2( n n ) k \u2212 2 \u00b7 ( m n ) k + 1 \u2212 (p l \u2212 p s ) \u2265 0 (58) \u21d0\u21d21 + ( n n ) k \u2212 2 \u00b7 ( m n ) k \u2265 p l \u2212 p s (59) \u21d0\u21d2 1 \u03b3 \u00b7 (p l \u2212 (1 \u2212 ( m n ) k ) \u2264 1 \u2212 ( m n ) k + 1 \u03b3 \u00b7 p s ,(60)\nwhere m = max(n, n ) \u2212 r and \u03b3 = ( n n ) k . We let Q s = C \u222a C s , where C s satisfies the following:\n\nC s \u2286 E, C s \u2229 B = \u2205, and Pr(X \u2208 C s ) = p s .\n\nNote that we can construct such C s because p l + p s \u2264 1. Then, we divide the remaining space \u2126 \\ (R \u222a Q s ) into c \u2212 2 subspaces such that Pr(X \u2208 Q j ) \u2264 p s , where j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c} \\ {l, s}.\n\nWe can construct such subspaces because p l + (c \u2212 1) \u00b7 p s \u2265 1. Then, based on these subspaces, we construct the following learning algorithm:\nA * (\u03c9, x) = l, if \u03c9 \u2208 R j, if \u03c9 \u2208 Q j(62)\nThen, we have the following based on the above definition of the learning algorithm A * :\n\nPr(A * (X, x) = l) = Pr(X \u2208 R) = p l (63)\n\nPr(A * (X, x) = s) = Pr(X \u2208 Q s ) = p s\n\nPr(A * (X, x) = j) = Pr(X \u2208 Q j ) \u2264 p s , j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c} \\ {l, s}.\n\nTherefore, the learning algorithm A * is consistent with (2). Next, we show that l is not predicted by the bagging predictor or there exist ties when the training dataset is D . In particular, we have the following:\nPr(A * (Y, x) = l)(66)\n=Pr(Y \u2208 R) (67)\n= 1 \u03b3 \u00b7 (p l \u2212 (1 \u2212 ( m n ) k )) (68) \u22641 \u2212 ( m n ) k + 1 \u03b3 \u00b7 p s (69) =Pr(Y \u2208 Q s ) (70) =Pr(A * (Y, x) = s),(71)\nwhere \u03b3 = ( n n ) k and we have (69) from (68) because of (60). Therefore, label l is not predicted for x or there exist ties when the training dataset is D .\n\n\nC Proof of Theorem 3\n\nBased on the definition of SimuEM and [20], we have:\nPr(p l i \u2264 Pr(A(g(D), x i ) = l i ) \u2227 p j \u2265 Pr(A(g(D), x i ) = j), \u2200j = l i ) \u2265 1 \u2212 \u03b1 e(72)\nTherefore, the probability that Certify returns an incorrect certified poisoning size for a testing example x i is at most \u03b1 e , i.e., we have:\nPr((\u2203D \u2208 B(D,r * i ), h(D , x i ) =\u0177 i )|\u0177 i = ABSTAIN) \u2264 \u03b1 e .(73)\nThen, we have the following:\nPr(\u2229 x i \u2208De ((\u2200D \u2208 B(D,r * i ), h(D , x i ) =\u0177 i )|\u0177 i = ABSTAIN)) (74) = 1 \u2212 Pr(\u222a x i \u2208De ((\u2203D \u2208 B(D,r * i ), h(D , x i ) =\u0177 i )|\u0177 i = ABSTAIN)) (75) \u2265 1 \u2212 x i \u2208De Pr((\u2203D \u2208 B(D,r * i ), h(D , x i ) =\u0177 i )|\u0177 i = ABSTAIN)(76)\n\u2265 1 \u2212 e \u00b7 \u03b1 e (77)\n= 1 \u2212 \u03b1(78)\nWe have (76) from (75) according to the Boole's inequality.\n\n\nD Derivation of Equation 7\n\nL(n ) = ( n n ) k \u2212 2 \u00b7 ( max(n, n ) \u2212 r n\n) k + 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s )(79)\nWe aim to derive arg max n\u2212r\u2264n \u2264n+r L(n ). When n \u2212 r \u2264 n \u2264 n, we have the following:\nL(n ) = ( n n ) k \u2212 2 \u00b7 ( n \u2212 r n ) k + 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s ).(80)\nTherefore, when n \u2212 r \u2264 n \u2264 n, L(n ) increases as n increases. Thus, L(n ) reaches its maximum value when n \u2264 n \u2264 n + r. When n \u2264 n \u2264 n + r, we have the following:\nL(n ) = ( n n ) k \u2212 2 \u00b7 ( n \u2212 r n ) k + 1 \u2212 (p l \u2212 p s \u2212 \u03b4 l \u2212 \u03b4 s ).(81)\nFigure 1 :\n1An example to illustrate why bagging is robust against data poisoning attacks, where (x 5 ,y 5 ) is the poisoned training example, N = 3, and k = 2. Base classifiers f 1 and f 2 are trained using clean training examples and bagging predicts the correct label for a testing example after majority vote among the three base classifiers.\n\n\nIntuitively, max{|D|, |D |} \u2212 |D \u2229 D | is the minimum number of modified/deleted/inserted training examples that can change D to D . For simplicity, we denote n = |D|, n = |D |, and m = |D \u2229 D |, where m is the number of training examples that are in both D and D .\n\nCorollary 1 .\n1Suppose a data poisoning attack only modifies existing training examples. Then, we have n = n and the solution to optimization problem\n\nCorollary 2 .\n2Suppose a data poisoning attack only deletes existing training examples. Then, we have n = m and r\n\nCorollary 3 .\n3Suppose a data poisoning attack only inserts new training examples. Then, we have m = n and r *\n\nFigure 2 :\n2Impact of k, \u03b1, and N on the certified accuracy of our method. The first row is the result on MNIST and the second row is the result on CIFAR10.\n\nFigure 3 :\n3(a) Transfer learning improves our certified accuracy on CIFAR10. Comparing our method with existing methods with respect to (b) certified accuracy and (c) running time.\nhttps://keras.io/examples/mnist_cnn/ 2 https://keras.io/examples/cifar10_cnn/\nhttps://github.com/tensorflow/privacy 4 Their method only needs one classifier when label flipping attacks and linear classifiers are considered.\nhttps://github.com/alexisbcook/keras_transfer_cifar10\n\u2022 If S 1 = {\u03c9 \u2208 \u2126 : \u00b5 x (\u03c9) > t \u00b7 \u00b5 y (\u03c9)} and S 2 = {\u03c9 \u2208 \u2126 : \u00b5 x (\u03c9) = t \u00b7 \u00b5 y (\u03c9)} for some t > 0. Let S = S 1 \u222a S 3 , where S 3 \u2286 S 2 . If we have Pr(M (X) = 1) \u2265 Pr(X \u2208 S), then Pr(M (Y ) = 1) \u2265 Pr(Y \u2208 S).\u2022 If S 1 = {\u03c9 \u2208 \u2126 : \u00b5 x (\u03c9) < t \u00b7 \u00b5 y (\u03c9)} and S 2 = {\u03c9 \u2208 \u2126 : \u00b5 x (\u03c9) = t \u00b7 \u00b5 y (\u03c9)} for some t > 0. Let S = S 1 \u222a S 3 , where S 3 \u2286 S 2 . If we have Pr(M (X) = 1) \u2264 Pr(X \u2208 S), then Pr(M (Y ) = 1) \u2264 Pr(Y \u2208 S).\nProof. We show the proof of the first part, and the second part can be proved similarly. For simplicity, we use M (1|\u03c9) and M (0|\u03c9) to denote the probabilities that M (\u03c9) = 0 and M (\u03c9) = 1, respectively. We use S c to denote the complement of S, i.e., S c = \u2126 \\ S. We have the following:Moreover, we have:k\u00b7x k\u22121 n k is larger than 0. Moreover, 1 \u2212 2 \u00b7 (1 \u2212 r x ) k\u22121 decreases as x increases when x \u2265 r and it only has one root that is no smaller than r which is as follows:increases as x increases in the range [r, , +\u221e). Therefore, we have the following three cases:Case I: When r \u2264 n \u00b7 (1 \u2212 k\u22121 1 2 ), L(n ) reaches its maximum value at n = n since L(n ) decreases as n increases in the range [n, n + r].Case II: When n \u00b7 ( k\u22121 \u221a 2 \u2212 1) < r < n \u00b7 ( k\u22121 \u221a 2 \u2212 1), L(n ) reaches its maximum value at n = Case III: When r \u2265 n \u00b7 ( k\u22121 \u221a 2 \u2212 1), L(n ) reaches its maximum value at n = n + r since L(n ) increases as n increases in the range [n, n + r].\nDeep learning with differential privacy. Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H Brendan Mcmahan, Ilya Mironov, Kunal Talwar, Li Zhang, CCS. Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In CCS, 2016.\n\nHow to backdoor federated learning. Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov, International Conference on Artificial Intelligence and Statistics. Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pages 2938-2948, 2020.\n\nThe security of machine learning. Marco Barreno, Blaine Nelson, D Anthony, J D Joseph, Tygar, Machine Learning. Marco Barreno, Blaine Nelson, Anthony D Joseph, and JD Tygar. The security of machine learning. Machine Learning, 2010.\n\nAnalyzing federated learning through an adversarial lens. Supriyo Arjun Nitin Bhagoji, Prateek Chakraborty, Seraphin Mittal, Calo, International Conference on Machine Learning. Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, pages 634-643, 2019.\n\nBagging classifiers for fighting poisoning attacks in adversarial classification tasks. Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, Fabio Roli, International workshop on multiple classifier systems. SpringerBattista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. Bagging classifiers for fighting poisoning attacks in adversarial classification tasks. In International workshop on multiple classifier systems, pages 350-359. Springer, 2011.\n\nPoisoning attacks against support vector machines. Battista Biggio, Blaine Nelson, Pavel Laskov, ICML. Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In ICML, 2012.\n\nIs data clustering in adversarial settings secure? In AISec. Battista Biggio, Ignazio Pillai, Samuel Rota Bul\u00f2, Davide Ariu, Marcello Pelillo, Fabio Roli, Battista Biggio, Ignazio Pillai, Samuel Rota Bul\u00f2, Davide Ariu, Marcello Pelillo, and Fabio Roli. Is data clustering in adversarial settings secure? In AISec, 2013.\n\nPoisoning behavioral malware clustering. Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, Fabio Roli, AISecBattista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, and Fabio Roli. Poisoning behavioral malware clustering. In AISec, 2014.\n\nBagging predictors. Machine learning. Leo Breiman, 24Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.\n\nTowards evaluating the robustness of neural networks. Nicholas Carlini, David Wagner, 2017 ieee symposium on security and privacy (sp). IEEENicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39-57. IEEE, 2017.\n\nThe use of confidence or fiducial limits illustrated in the case of the binomial. J Charles, Clopper, Egon S Pearson, Biometrika. 264Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the case of the binomial. Biometrika, 26(4):404-413, 1934.\n\nElan Jeremy M Cohen, J Zico Rosenfeld, Kolter, arXiv:1902.02918Certified adversarial robustness via randomized smoothing. arXiv preprintJeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019.\n\nCasting out demons: Sanitizing training data for anomaly sensors. Gabriela F Cretu, Angelos Stavrou, Michael E Locasto, Salvatore J Stolfo, Angelos D Keromytis, IEEE S & P. Gabriela F. Cretu, Angelos Stavrou, Michael E. Locasto, Salvatore J. Stolfo, and Angelos D. Keromytis. Casting out demons: Sanitizing training data for anomaly sensors. In IEEE S & P, 2008.\n\nLocal model poisoning attacks to byzantine-robust federated learning. Minghong Fang, Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong, Usenix Security Symposium. Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Local model poisoning attacks to byzantine-robust federated learning. In Usenix Security Symposium, 2020.\n\nInfluence function based data poisoning attacks to top-n recommender systems. Minghong Fang, Jia Neil Zhenqiang Gong, Liu, Proceedings of The Web Conference 2020. The Web Conference 2020Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. Influence function based data poisoning attacks to top-n recommender systems. In Proceedings of The Web Conference 2020, pages 3019-3025, 2020.\n\nPoisoning attacks to graph-based recommender systems. Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, Jia Liu, Proceedings of the 34th Annual Computer Security Applications Conference. the 34th Annual Computer Security Applications ConferenceMinghong Fang, Guolei Yang, Neil Zhenqiang Gong, and Jia Liu. Poisoning attacks to graph-based recommender systems. In Proceedings of the 34th Annual Computer Security Applications Conference, pages 381-392, 2018.\n\nRobust logistic regression and classification. Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan, NIPS. Jiashi Feng, Huan Xu, Shie Mannor, and Shuicheng Yan. Robust logistic regression and classification. In NIPS, 2014.\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, arXiv:1708.06733arXiv preprintTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\n\nManipulating machine learning: Poisoning attacks and countermeasures for regression learning. Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, Bo Li, IEEE S & P. Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In IEEE S & P, 2018.\n\nCertified robustness for top-k predictions against adversarial perturbations via randomized smoothing. Jinyuan Jia, Xiaoyu Cao, Binghui Wang, Neil Zhenqiang Gong, International Conference on Learning Representations. Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing. In International Conference on Learning Representations, 2020.\n\nCertified robustness of community detection against adversarial structural perturbation via randomized smoothing. Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong, Proceedings of The Web Conference 2020. The Web Conference 2020Jinyuan Jia, Binghui Wang, Xiaoyu Cao, and Neil Zhenqiang Gong. Certified robustness of community detection against adversarial structural perturbation via randomized smoothing. In Proceedings of The Web Conference 2020, pages 2718-2724, 2020.\n\nStronger data poisoning attacks break data sanitization defenses. Pang Wei Koh, Jacob Steinhardt, Percy Liang, arXiv:1811.00741arXiv preprintPang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. arXiv preprint arXiv:1811.00741, 2018.\n\nData poisoning attacks on factorization-based collaborative filtering. Bo Li, Yining Wang, Aarti Singh, Yevgeniy Vorobeychik, Advances in neural information processing systems. Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. In Advances in neural information processing systems, pages 1885-1893, 2016.\n\nTrojaning attack on neural networks. Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, Xiangyu Zhang, ISOC Network and Distributed System Security Symposium. Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In ISOC Network and Distributed System Security Symposium, 2017.\n\nData poisoning against differentially-private learners: Attacks and defenses. Yuzhe Ma, Xiaojin Zhu, Justin Hsu, International Joint Conference on Artificial Intelligence. Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against differentially-private learners: Attacks and defenses. In International Joint Conference on Artificial Intelligence, 2019.\n\nThe security of latent dirichlet allocation. Shike Mei, Xiaojin Zhu, Artificial Intelligence and Statistics. Shike Mei and Xiaojin Zhu. The security of latent dirichlet allocation. In Artificial Intelligence and Statistics, pages 681-689, 2015.\n\nUsing machine teaching to identify optimal training-set attacks on machine learners. Shike Mei, Xiaojin Zhu, Twenty-Ninth AAAI Conference on Artificial Intelligence. Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine learners. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n\nSystematic poisoning attacks on and defenses for machine learning in healthcare. Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand Raghunathan, Niraj K Jha, IEEE journal of biomedical and health informatics. 196Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand Raghunathan, and Niraj K Jha. Systematic poisoning attacks on and defenses for machine learning in healthcare. IEEE journal of biomedical and health informatics, 19(6):1893-1905, 2014.\n\nTowards poisoning of deep learning algorithms with back-gradient optimization. Luis Mu\u00f1oz-Gonz\u00e1lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, C Emil, Fabio Lupu, Roli, AISecLuis Mu\u00f1oz-Gonz\u00e1lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongras- samee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In AISec, 2017.\n\nExploiting machine learning to subvert your spam filter. Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Udam Benjamin Ip Rubinstein, Saini, A Charles, Doug Sutton, Kai Tygar, Xia, LEET. 8Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Ru- binstein, Udam Saini, Charles A Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert your spam filter. LEET, 8:1-9, 2008.\n\nOn the problem of the most efficient tests of statistical hypotheses. Jerzy Neyman, Egon Sharpe Pearson, Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character. 231Jerzy Neyman and Egon Sharpe Pearson. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289-337, 1933.\n\nCertified robustness to label-flipping attacks via randomized smoothing. Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, J Zico Kolter, ICML. Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and J Zico Kolter. Certified robustness to label-flipping attacks via randomized smoothing. In ICML, 2020.\n\nAntidote: understanding and defending against poisoning of anomaly detectors. Blaine Benjamin Ip Rubinstein, Ling Nelson, Huang, D Anthony, Shing-Hon Joseph, Satish Lau, Nina Rao, J D Taft, Tygar, ACM IMC. Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and JD Tygar. Antidote: understanding and defending against poisoning of anomaly detectors. In ACM IMC, 2009.\n\nPoison frogs! targeted clean-label poisoning attacks on neural networks. Ali Shafahi, Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein, In NIPS. Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In NIPS, 2018.\n\nCertified defenses for data poisoning attacks. Jacob Steinhardt, Pang Wei W Koh, Percy S Liang, Advances in neural information processing systems. Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In Advances in neural information processing systems, pages 3517-3529, 2017.\n\nWhen does machine learning fail? generalized transferability for evasion and poisoning attacks. Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume, Iii , Tudor Dumitras, Usenix Security Symposium. Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. When does machine learning fail? generalized transferability for evasion and poisoning attacks. In Usenix Security Symposium, 2018.\n\nIntriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, ICLRChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. ICLR, 2014.\n\nSpectral signatures in backdoor attacks. Brandon Tran, Jerry Li, Aleksander Madry, NIPS. Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In NIPS, 2018.\n\nOn certifying robustness against backdoor attacks via randomized smoothing. Binghui Wang, Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong, CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision. Binghui Wang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. On certifying robust- ness against backdoor attacks via randomized smoothing. In CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision, 2020.\n\nAttacking graph-based classification via manipulating the graph structure. Binghui Wang, Neil Zhenqiang Gong, Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. the 2019 ACM SIGSAC Conference on Computer and Communications SecurityBinghui Wang and Neil Zhenqiang Gong. Attacking graph-based classification via ma- nipulating the graph structure. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 2023-2040, 2019.\n\nIs feature selection secure against training data poisoning? In ICML. Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, Fabio Roli, Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature selection secure against training data poisoning? In ICML, 2015.\n\nChaofei Yang, Qing Wu, Hai Li, Yiran Chen, arXiv:1703.01340Generative poisoning attack method against neural networks. arXiv preprintChaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative poisoning attack method against neural networks. arXiv preprint arXiv:1703.01340, 2017.\n\nFake co-visitation injection attacks to recommender systems. Guolei Yang, Neil Zhenqiang Gong, Ying Cai, NDSS. Guolei Yang, Neil Zhenqiang Gong, and Ying Cai. Fake co-visitation injection attacks to recommender systems. In NDSS, 2017.\n\nZaixi Zhang, Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong, arXiv:2006.11165Backdoor attacks to graph neural networks. arXiv preprintZaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. Backdoor attacks to graph neural networks. arXiv preprint arXiv:2006.11165, 2020.\n\nTransferable clean-label poisoning attacks on deep neural nets. Chen Zhu, Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, Tom Goldstein, International Conference on Machine Learning. Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein. Transferable clean-label poisoning attacks on deep neural nets. In International Conference on Machine Learning, pages 7614-7623, 2019.\n\nAdversarial attacks on neural networks for graph data. Daniel Z\u00fcgner, Amir Akbarnejad, Stephan G\u00fcnnemann, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningDaniel Z\u00fcgner, Amir Akbarnejad, and Stephan G\u00fcnnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2847-2856, 2018.\n", "annotations": {"author": "[{\"end\":159,\"start\":76},{\"end\":241,\"start\":160},{\"end\":331,\"start\":242}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":170,\"start\":167},{\"end\":261,\"start\":257}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":166,\"start\":160},{\"end\":246,\"start\":242},{\"end\":256,\"start\":247}]", "author_affiliation": "[{\"end\":158,\"start\":110},{\"end\":240,\"start\":192},{\"end\":330,\"start\":282}]", "title": "[{\"end\":73,\"start\":1},{\"end\":404,\"start\":332}]", "venue": null, "abstract": "[{\"end\":1435,\"start\":406}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1550,\"start\":1546},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1552,\"start\":1550},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1555,\"start\":1552},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1558,\"start\":1555},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1561,\"start\":1558},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1564,\"start\":1561},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2008,\"start\":2004},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2011,\"start\":2008},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2418,\"start\":2414},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2420,\"start\":2418},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2423,\"start\":2420},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2426,\"start\":2423},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2429,\"start\":2426},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2435,\"start\":2432},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2438,\"start\":2435},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2504,\"start\":2500},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2506,\"start\":2504},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2509,\"start\":2506},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2512,\"start\":2509},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2515,\"start\":2512},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2518,\"start\":2515},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2722,\"start\":2718},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2725,\"start\":2722},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2997,\"start\":2993},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3131,\"start\":3127},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3167,\"start\":3163},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3742,\"start\":3738},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3840,\"start\":3836},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4108,\"start\":4104},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4454,\"start\":4451},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6197,\"start\":6193},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6223,\"start\":6219},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8116,\"start\":8113},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13121,\"start\":13117},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13202,\"start\":13198},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13387,\"start\":13383},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14882,\"start\":14878},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19802,\"start\":19798},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19847,\"start\":19843},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20132,\"start\":20128},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20148,\"start\":20144},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20469,\"start\":20465},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21564,\"start\":21560},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21580,\"start\":21576},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22134,\"start\":22133},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24341,\"start\":24337},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24367,\"start\":24363},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25006,\"start\":25002},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25040,\"start\":25036},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25684,\"start\":25680},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25694,\"start\":25691},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25716,\"start\":25712},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25719,\"start\":25716},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25722,\"start\":25719},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25725,\"start\":25722},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25756,\"start\":25752},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25759,\"start\":25756},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25769,\"start\":25765},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25781,\"start\":25777},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25811,\"start\":25807},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25814,\"start\":25811},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25817,\"start\":25814},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25820,\"start\":25817},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25836,\"start\":25833},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25838,\"start\":25836},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":25864,\"start\":25860},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25867,\"start\":25864},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25870,\"start\":25867},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25873,\"start\":25870},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25898,\"start\":25894},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25900,\"start\":25898},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25902,\"start\":25900},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25919,\"start\":25915},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25922,\"start\":25919},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25925,\"start\":25922},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25928,\"start\":25925},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25964,\"start\":25960},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25967,\"start\":25964},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26153,\"start\":26149},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26155,\"start\":26153},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26158,\"start\":26155},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26161,\"start\":26158},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26310,\"start\":26306},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26313,\"start\":26310},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26471,\"start\":26467},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26701,\"start\":26697},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27047,\"start\":27043},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27161,\"start\":27157},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27409,\"start\":27405},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27669,\"start\":27665},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27775,\"start\":27771},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27796,\"start\":27792},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27900,\"start\":27897},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29897,\"start\":29893},{\"end\":29961,\"start\":29939},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30278,\"start\":30274},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32231,\"start\":32227},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33334,\"start\":33330},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":33450,\"start\":33446},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37728,\"start\":37724}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39245,\"start\":38898},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39513,\"start\":39246},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39664,\"start\":39514},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39779,\"start\":39665},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39891,\"start\":39780},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40049,\"start\":39892},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40232,\"start\":40050}]", "paragraph": "[{\"end\":2359,\"start\":1451},{\"end\":6379,\"start\":2361},{\"end\":6425,\"start\":6381},{\"end\":6574,\"start\":6427},{\"end\":6664,\"start\":6576},{\"end\":6724,\"start\":6666},{\"end\":6783,\"start\":6726},{\"end\":9156,\"start\":6850},{\"end\":9626,\"start\":9207},{\"end\":9919,\"start\":9628},{\"end\":10334,\"start\":9985},{\"end\":10472,\"start\":10369},{\"end\":10829,\"start\":10589},{\"end\":10892,\"start\":10831},{\"end\":11406,\"start\":11041},{\"end\":11632,\"start\":11408},{\"end\":12141,\"start\":11634},{\"end\":12305,\"start\":12143},{\"end\":12994,\"start\":12348},{\"end\":14263,\"start\":12996},{\"end\":14391,\"start\":14265},{\"end\":14883,\"start\":14423},{\"end\":15054,\"start\":14976},{\"end\":15264,\"start\":15185},{\"end\":15904,\"start\":15564},{\"end\":16681,\"start\":15906},{\"end\":17801,\"start\":16834},{\"end\":17874,\"start\":17803},{\"end\":18617,\"start\":17990},{\"end\":19180,\"start\":18674},{\"end\":19726,\"start\":19182},{\"end\":20114,\"start\":19728},{\"end\":21539,\"start\":20116},{\"end\":22874,\"start\":21541},{\"end\":23710,\"start\":22899},{\"end\":24310,\"start\":23712},{\"end\":25264,\"start\":24312},{\"end\":26122,\"start\":25281},{\"end\":27031,\"start\":26124},{\"end\":28039,\"start\":27033},{\"end\":28996,\"start\":28054},{\"end\":29185,\"start\":29021},{\"end\":29636,\"start\":29215},{\"end\":29781,\"start\":29657},{\"end\":29929,\"start\":29783},{\"end\":30166,\"start\":29931},{\"end\":30262,\"start\":30259},{\"end\":30427,\"start\":30264},{\"end\":30983,\"start\":30429},{\"end\":31067,\"start\":30985},{\"end\":31454,\"start\":31104},{\"end\":31623,\"start\":31527},{\"end\":31799,\"start\":31726},{\"end\":32232,\"start\":31949},{\"end\":32451,\"start\":32234},{\"end\":32632,\"start\":32523},{\"end\":32677,\"start\":32649},{\"end\":32756,\"start\":32723},{\"end\":33054,\"start\":32806},{\"end\":33182,\"start\":33105},{\"end\":33261,\"start\":33243},{\"end\":33451,\"start\":33306},{\"end\":33539,\"start\":33453},{\"end\":33910,\"start\":33606},{\"end\":34055,\"start\":33942},{\"end\":34109,\"start\":34076},{\"end\":34258,\"start\":34158},{\"end\":34358,\"start\":34312},{\"end\":34540,\"start\":34521},{\"end\":34731,\"start\":34691},{\"end\":34816,\"start\":34733},{\"end\":34874,\"start\":34833},{\"end\":35151,\"start\":35022},{\"end\":35701,\"start\":35233},{\"end\":36104,\"start\":35842},{\"end\":36446,\"start\":36344},{\"end\":36494,\"start\":36448},{\"end\":36695,\"start\":36496},{\"end\":36840,\"start\":36697},{\"end\":36973,\"start\":36884},{\"end\":37016,\"start\":36975},{\"end\":37057,\"start\":37018},{\"end\":37132,\"start\":37059},{\"end\":37349,\"start\":37134},{\"end\":37388,\"start\":37373},{\"end\":37661,\"start\":37503},{\"end\":37738,\"start\":37686},{\"end\":37974,\"start\":37831},{\"end\":38071,\"start\":38043},{\"end\":38316,\"start\":38298},{\"end\":38388,\"start\":38329},{\"end\":38461,\"start\":38419},{\"end\":38586,\"start\":38501},{\"end\":38824,\"start\":38661}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9206,\"start\":9157},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9984,\"start\":9920},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10368,\"start\":10335},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10588,\"start\":10473},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10941,\"start\":10893},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10991,\"start\":10941},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11040,\"start\":10991},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14422,\"start\":14392},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14975,\"start\":14884},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15184,\"start\":15055},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15563,\"start\":15265},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16833,\"start\":16682},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17954,\"start\":17875},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18673,\"start\":18618},{\"attributes\":{\"id\":\"formula_14\"},\"end\":29214,\"start\":29186},{\"attributes\":{\"id\":\"formula_15\"},\"end\":29656,\"start\":29637},{\"attributes\":{\"id\":\"formula_16\"},\"end\":30258,\"start\":30167},{\"attributes\":{\"id\":\"formula_18\"},\"end\":31103,\"start\":31068},{\"attributes\":{\"id\":\"formula_19\"},\"end\":31480,\"start\":31455},{\"attributes\":{\"id\":\"formula_20\"},\"end\":31506,\"start\":31480},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31526,\"start\":31506},{\"attributes\":{\"id\":\"formula_22\"},\"end\":31673,\"start\":31624},{\"attributes\":{\"id\":\"formula_23\"},\"end\":31725,\"start\":31673},{\"attributes\":{\"id\":\"formula_24\"},\"end\":31874,\"start\":31800},{\"attributes\":{\"id\":\"formula_25\"},\"end\":31948,\"start\":31874},{\"attributes\":{\"id\":\"formula_26\"},\"end\":32522,\"start\":32452},{\"attributes\":{\"id\":\"formula_27\"},\"end\":32648,\"start\":32633},{\"attributes\":{\"id\":\"formula_28\"},\"end\":32722,\"start\":32678},{\"attributes\":{\"id\":\"formula_29\"},\"end\":32805,\"start\":32757},{\"attributes\":{\"id\":\"formula_30\"},\"end\":33104,\"start\":33055},{\"attributes\":{\"id\":\"formula_31\"},\"end\":33242,\"start\":33183},{\"attributes\":{\"id\":\"formula_32\"},\"end\":33305,\"start\":33262},{\"attributes\":{\"id\":\"formula_33\"},\"end\":33605,\"start\":33540},{\"attributes\":{\"id\":\"formula_34\"},\"end\":33941,\"start\":33911},{\"attributes\":{\"id\":\"formula_35\"},\"end\":34075,\"start\":34056},{\"attributes\":{\"id\":\"formula_36\"},\"end\":34157,\"start\":34110},{\"attributes\":{\"id\":\"formula_37\"},\"end\":34311,\"start\":34259},{\"attributes\":{\"id\":\"formula_38\"},\"end\":34520,\"start\":34359},{\"attributes\":{\"id\":\"formula_39\"},\"end\":34690,\"start\":34541},{\"attributes\":{\"id\":\"formula_40\"},\"end\":34832,\"start\":34817},{\"attributes\":{\"id\":\"formula_41\"},\"end\":35021,\"start\":34875},{\"attributes\":{\"id\":\"formula_42\"},\"end\":35232,\"start\":35152},{\"attributes\":{\"id\":\"formula_43\"},\"end\":35818,\"start\":35702},{\"attributes\":{\"id\":\"formula_44\"},\"end\":36343,\"start\":36105},{\"attributes\":{\"id\":\"formula_46\"},\"end\":36883,\"start\":36841},{\"attributes\":{\"id\":\"formula_49\"},\"end\":37372,\"start\":37350},{\"attributes\":{\"id\":\"formula_50\"},\"end\":37502,\"start\":37389},{\"attributes\":{\"id\":\"formula_51\"},\"end\":37830,\"start\":37739},{\"attributes\":{\"id\":\"formula_52\"},\"end\":38042,\"start\":37975},{\"attributes\":{\"id\":\"formula_53\"},\"end\":38297,\"start\":38072},{\"attributes\":{\"id\":\"formula_54\"},\"end\":38328,\"start\":38317},{\"attributes\":{\"id\":\"formula_55\"},\"end\":38500,\"start\":38462},{\"attributes\":{\"id\":\"formula_56\"},\"end\":38660,\"start\":38587},{\"attributes\":{\"id\":\"formula_57\"},\"end\":38898,\"start\":38825}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1449,\"start\":1437},{\"attributes\":{\"n\":\"2\"},\"end\":6848,\"start\":6786},{\"attributes\":{\"n\":\"3\"},\"end\":12346,\"start\":12308},{\"attributes\":{\"n\":\"4\"},\"end\":17967,\"start\":17956},{\"attributes\":{\"n\":\"4.1\"},\"end\":17988,\"start\":17970},{\"attributes\":{\"n\":\"4.2\"},\"end\":22897,\"start\":22877},{\"attributes\":{\"n\":\"5\"},\"end\":25279,\"start\":25267},{\"attributes\":{\"n\":\"6\"},\"end\":28052,\"start\":28042},{\"end\":29019,\"start\":28999},{\"end\":35840,\"start\":35820},{\"end\":37684,\"start\":37664},{\"end\":38417,\"start\":38391},{\"end\":38909,\"start\":38899},{\"end\":39528,\"start\":39515},{\"end\":39679,\"start\":39666},{\"end\":39794,\"start\":39781},{\"end\":39903,\"start\":39893},{\"end\":40061,\"start\":40051}]", "table": null, "figure_caption": "[{\"end\":39245,\"start\":38911},{\"end\":39513,\"start\":39248},{\"end\":39664,\"start\":39530},{\"end\":39779,\"start\":39681},{\"end\":39891,\"start\":39796},{\"end\":40049,\"start\":39905},{\"end\":40232,\"start\":40063}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5014,\"start\":5006},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22931,\"start\":22923},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24185,\"start\":24177},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24377,\"start\":24369},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24759,\"start\":24751}]", "bib_author_first_name": "[{\"end\":41929,\"start\":41923},{\"end\":41941,\"start\":41937},{\"end\":41950,\"start\":41947},{\"end\":41964,\"start\":41963},{\"end\":41972,\"start\":41965},{\"end\":41986,\"start\":41982},{\"end\":42001,\"start\":41996},{\"end\":42012,\"start\":42010},{\"end\":42225,\"start\":42219},{\"end\":42246,\"start\":42239},{\"end\":42259,\"start\":42253},{\"end\":42272,\"start\":42265},{\"end\":42287,\"start\":42281},{\"end\":42621,\"start\":42616},{\"end\":42637,\"start\":42631},{\"end\":42647,\"start\":42646},{\"end\":42658,\"start\":42657},{\"end\":42660,\"start\":42659},{\"end\":42880,\"start\":42873},{\"end\":42909,\"start\":42902},{\"end\":42931,\"start\":42923},{\"end\":43294,\"start\":43286},{\"end\":43308,\"start\":43303},{\"end\":43324,\"start\":43317},{\"end\":43340,\"start\":43333},{\"end\":43356,\"start\":43351},{\"end\":43744,\"start\":43736},{\"end\":43759,\"start\":43753},{\"end\":43773,\"start\":43768},{\"end\":43974,\"start\":43966},{\"end\":43990,\"start\":43983},{\"end\":44005,\"start\":43999},{\"end\":44010,\"start\":44006},{\"end\":44023,\"start\":44017},{\"end\":44038,\"start\":44030},{\"end\":44053,\"start\":44048},{\"end\":44275,\"start\":44267},{\"end\":44290,\"start\":44284},{\"end\":44304,\"start\":44298},{\"end\":44320,\"start\":44311},{\"end\":44339,\"start\":44334},{\"end\":44355,\"start\":44348},{\"end\":44371,\"start\":44366},{\"end\":44597,\"start\":44594},{\"end\":44744,\"start\":44736},{\"end\":44759,\"start\":44754},{\"end\":45073,\"start\":45072},{\"end\":45281,\"start\":45277},{\"end\":45304,\"start\":45298},{\"end\":45637,\"start\":45629},{\"end\":45639,\"start\":45638},{\"end\":45654,\"start\":45647},{\"end\":45671,\"start\":45664},{\"end\":45673,\"start\":45672},{\"end\":45692,\"start\":45683},{\"end\":45694,\"start\":45693},{\"end\":45710,\"start\":45703},{\"end\":45712,\"start\":45711},{\"end\":46005,\"start\":45997},{\"end\":46018,\"start\":46012},{\"end\":46031,\"start\":46024},{\"end\":46051,\"start\":46037},{\"end\":46343,\"start\":46335},{\"end\":46353,\"start\":46350},{\"end\":46699,\"start\":46691},{\"end\":46712,\"start\":46706},{\"end\":46723,\"start\":46719},{\"end\":46733,\"start\":46724},{\"end\":46743,\"start\":46740},{\"end\":47148,\"start\":47142},{\"end\":47159,\"start\":47155},{\"end\":47168,\"start\":47164},{\"end\":47186,\"start\":47177},{\"end\":47402,\"start\":47396},{\"end\":47414,\"start\":47407},{\"end\":47438,\"start\":47429},{\"end\":47750,\"start\":47743},{\"end\":47767,\"start\":47762},{\"end\":47783,\"start\":47775},{\"end\":47797,\"start\":47792},{\"end\":47811,\"start\":47803},{\"end\":47827,\"start\":47825},{\"end\":48163,\"start\":48156},{\"end\":48175,\"start\":48169},{\"end\":48188,\"start\":48181},{\"end\":48209,\"start\":48195},{\"end\":48622,\"start\":48615},{\"end\":48635,\"start\":48628},{\"end\":48648,\"start\":48642},{\"end\":48668,\"start\":48654},{\"end\":49053,\"start\":49049},{\"end\":49068,\"start\":49063},{\"end\":49086,\"start\":49081},{\"end\":49352,\"start\":49350},{\"end\":49363,\"start\":49357},{\"end\":49375,\"start\":49370},{\"end\":49391,\"start\":49383},{\"end\":49707,\"start\":49701},{\"end\":49720,\"start\":49713},{\"end\":49731,\"start\":49725},{\"end\":49748,\"start\":49739},{\"end\":49758,\"start\":49754},{\"end\":49772,\"start\":49765},{\"end\":49786,\"start\":49779},{\"end\":50133,\"start\":50128},{\"end\":50145,\"start\":50138},{\"end\":50157,\"start\":50151},{\"end\":50458,\"start\":50453},{\"end\":50471,\"start\":50464},{\"end\":50744,\"start\":50739},{\"end\":50757,\"start\":50750},{\"end\":51086,\"start\":51080},{\"end\":51113,\"start\":51106},{\"end\":51130,\"start\":51125},{\"end\":51151,\"start\":51144},{\"end\":51531,\"start\":51527},{\"end\":51556,\"start\":51548},{\"end\":51570,\"start\":51565},{\"end\":51587,\"start\":51581},{\"end\":51602,\"start\":51597},{\"end\":51618,\"start\":51617},{\"end\":51630,\"start\":51625},{\"end\":51928,\"start\":51922},{\"end\":51942,\"start\":51937},{\"end\":51959,\"start\":51952},{\"end\":51964,\"start\":51960},{\"end\":51977,\"start\":51970},{\"end\":51979,\"start\":51978},{\"end\":51992,\"start\":51988},{\"end\":52025,\"start\":52024},{\"end\":52039,\"start\":52035},{\"end\":52051,\"start\":52048},{\"end\":52372,\"start\":52367},{\"end\":52385,\"start\":52381},{\"end\":52875,\"start\":52871},{\"end\":52891,\"start\":52887},{\"end\":52908,\"start\":52901},{\"end\":52926,\"start\":52920},{\"end\":53182,\"start\":53176},{\"end\":53211,\"start\":53207},{\"end\":53228,\"start\":53227},{\"end\":53247,\"start\":53238},{\"end\":53262,\"start\":53256},{\"end\":53272,\"start\":53268},{\"end\":53279,\"start\":53278},{\"end\":53281,\"start\":53280},{\"end\":53598,\"start\":53595},{\"end\":53613,\"start\":53608},{\"end\":53627,\"start\":53621},{\"end\":53644,\"start\":53636},{\"end\":53661,\"start\":53652},{\"end\":53675,\"start\":53670},{\"end\":53689,\"start\":53686},{\"end\":53963,\"start\":53958},{\"end\":53980,\"start\":53976},{\"end\":53999,\"start\":53992},{\"end\":54340,\"start\":54332},{\"end\":54352,\"start\":54348},{\"end\":54372,\"start\":54364},{\"end\":54382,\"start\":54379},{\"end\":54393,\"start\":54390},{\"end\":54401,\"start\":54396},{\"end\":54705,\"start\":54696},{\"end\":54723,\"start\":54715},{\"end\":54737,\"start\":54733},{\"end\":54753,\"start\":54749},{\"end\":54768,\"start\":54761},{\"end\":54779,\"start\":54776},{\"end\":54795,\"start\":54792},{\"end\":55023,\"start\":55016},{\"end\":55035,\"start\":55030},{\"end\":55050,\"start\":55040},{\"end\":55250,\"start\":55243},{\"end\":55263,\"start\":55257},{\"end\":55276,\"start\":55269},{\"end\":55296,\"start\":55282},{\"end\":55679,\"start\":55672},{\"end\":55700,\"start\":55686},{\"end\":56168,\"start\":56163},{\"end\":56183,\"start\":56175},{\"end\":56197,\"start\":56192},{\"end\":56212,\"start\":56205},{\"end\":56228,\"start\":56221},{\"end\":56242,\"start\":56237},{\"end\":56423,\"start\":56416},{\"end\":56434,\"start\":56430},{\"end\":56442,\"start\":56439},{\"end\":56452,\"start\":56447},{\"end\":56763,\"start\":56757},{\"end\":56774,\"start\":56770},{\"end\":56784,\"start\":56775},{\"end\":56795,\"start\":56791},{\"end\":56937,\"start\":56932},{\"end\":56952,\"start\":56945},{\"end\":56965,\"start\":56958},{\"end\":56986,\"start\":56972},{\"end\":57282,\"start\":57278},{\"end\":57293,\"start\":57288},{\"end\":57308,\"start\":57301},{\"end\":57318,\"start\":57313},{\"end\":57336,\"start\":57327},{\"end\":57348,\"start\":57345},{\"end\":57692,\"start\":57686},{\"end\":57705,\"start\":57701},{\"end\":57725,\"start\":57718}]", "bib_author_last_name": "[{\"end\":41935,\"start\":41930},{\"end\":41945,\"start\":41942},{\"end\":41961,\"start\":41951},{\"end\":41980,\"start\":41973},{\"end\":41994,\"start\":41987},{\"end\":42008,\"start\":42002},{\"end\":42018,\"start\":42013},{\"end\":42237,\"start\":42226},{\"end\":42251,\"start\":42247},{\"end\":42263,\"start\":42260},{\"end\":42279,\"start\":42273},{\"end\":42297,\"start\":42288},{\"end\":42629,\"start\":42622},{\"end\":42644,\"start\":42638},{\"end\":42655,\"start\":42648},{\"end\":42667,\"start\":42661},{\"end\":42674,\"start\":42669},{\"end\":42900,\"start\":42881},{\"end\":42921,\"start\":42910},{\"end\":42938,\"start\":42932},{\"end\":42944,\"start\":42940},{\"end\":43301,\"start\":43295},{\"end\":43315,\"start\":43309},{\"end\":43331,\"start\":43325},{\"end\":43349,\"start\":43341},{\"end\":43361,\"start\":43357},{\"end\":43751,\"start\":43745},{\"end\":43766,\"start\":43760},{\"end\":43780,\"start\":43774},{\"end\":43981,\"start\":43975},{\"end\":43997,\"start\":43991},{\"end\":44015,\"start\":44011},{\"end\":44028,\"start\":44024},{\"end\":44046,\"start\":44039},{\"end\":44058,\"start\":44054},{\"end\":44282,\"start\":44276},{\"end\":44296,\"start\":44291},{\"end\":44309,\"start\":44305},{\"end\":44332,\"start\":44321},{\"end\":44346,\"start\":44340},{\"end\":44364,\"start\":44356},{\"end\":44376,\"start\":44372},{\"end\":44605,\"start\":44598},{\"end\":44752,\"start\":44745},{\"end\":44766,\"start\":44760},{\"end\":45081,\"start\":45074},{\"end\":45090,\"start\":45083},{\"end\":45106,\"start\":45092},{\"end\":45296,\"start\":45282},{\"end\":45314,\"start\":45305},{\"end\":45322,\"start\":45316},{\"end\":45645,\"start\":45640},{\"end\":45662,\"start\":45655},{\"end\":45681,\"start\":45674},{\"end\":45701,\"start\":45695},{\"end\":45722,\"start\":45713},{\"end\":46010,\"start\":46006},{\"end\":46022,\"start\":46019},{\"end\":46035,\"start\":46032},{\"end\":46056,\"start\":46052},{\"end\":46348,\"start\":46344},{\"end\":46373,\"start\":46354},{\"end\":46378,\"start\":46375},{\"end\":46704,\"start\":46700},{\"end\":46717,\"start\":46713},{\"end\":46738,\"start\":46734},{\"end\":46747,\"start\":46744},{\"end\":47153,\"start\":47149},{\"end\":47162,\"start\":47160},{\"end\":47175,\"start\":47169},{\"end\":47190,\"start\":47187},{\"end\":47405,\"start\":47403},{\"end\":47427,\"start\":47415},{\"end\":47443,\"start\":47439},{\"end\":47760,\"start\":47751},{\"end\":47773,\"start\":47768},{\"end\":47790,\"start\":47784},{\"end\":47801,\"start\":47798},{\"end\":47823,\"start\":47812},{\"end\":47830,\"start\":47828},{\"end\":48167,\"start\":48164},{\"end\":48179,\"start\":48176},{\"end\":48193,\"start\":48189},{\"end\":48214,\"start\":48210},{\"end\":48626,\"start\":48623},{\"end\":48640,\"start\":48636},{\"end\":48652,\"start\":48649},{\"end\":48673,\"start\":48669},{\"end\":49061,\"start\":49054},{\"end\":49079,\"start\":49069},{\"end\":49092,\"start\":49087},{\"end\":49355,\"start\":49353},{\"end\":49368,\"start\":49364},{\"end\":49381,\"start\":49376},{\"end\":49403,\"start\":49392},{\"end\":49711,\"start\":49708},{\"end\":49723,\"start\":49721},{\"end\":49737,\"start\":49732},{\"end\":49752,\"start\":49749},{\"end\":49763,\"start\":49759},{\"end\":49777,\"start\":49773},{\"end\":49792,\"start\":49787},{\"end\":50136,\"start\":50134},{\"end\":50149,\"start\":50146},{\"end\":50161,\"start\":50158},{\"end\":50462,\"start\":50459},{\"end\":50475,\"start\":50472},{\"end\":50748,\"start\":50745},{\"end\":50761,\"start\":50758},{\"end\":51104,\"start\":51087},{\"end\":51123,\"start\":51114},{\"end\":51142,\"start\":51131},{\"end\":51155,\"start\":51152},{\"end\":51546,\"start\":51532},{\"end\":51563,\"start\":51557},{\"end\":51579,\"start\":51571},{\"end\":51595,\"start\":51588},{\"end\":51615,\"start\":51603},{\"end\":51623,\"start\":51619},{\"end\":51635,\"start\":51631},{\"end\":51641,\"start\":51637},{\"end\":51935,\"start\":51929},{\"end\":51950,\"start\":51943},{\"end\":51968,\"start\":51965},{\"end\":51986,\"start\":51980},{\"end\":52015,\"start\":51993},{\"end\":52022,\"start\":52017},{\"end\":52033,\"start\":52026},{\"end\":52046,\"start\":52040},{\"end\":52057,\"start\":52052},{\"end\":52062,\"start\":52059},{\"end\":52379,\"start\":52373},{\"end\":52400,\"start\":52386},{\"end\":52885,\"start\":52876},{\"end\":52899,\"start\":52892},{\"end\":52918,\"start\":52909},{\"end\":52933,\"start\":52927},{\"end\":53205,\"start\":53183},{\"end\":53218,\"start\":53212},{\"end\":53225,\"start\":53220},{\"end\":53236,\"start\":53229},{\"end\":53254,\"start\":53248},{\"end\":53266,\"start\":53263},{\"end\":53276,\"start\":53273},{\"end\":53286,\"start\":53282},{\"end\":53293,\"start\":53288},{\"end\":53606,\"start\":53599},{\"end\":53619,\"start\":53614},{\"end\":53634,\"start\":53628},{\"end\":53650,\"start\":53645},{\"end\":53668,\"start\":53662},{\"end\":53684,\"start\":53676},{\"end\":53699,\"start\":53690},{\"end\":53974,\"start\":53964},{\"end\":53990,\"start\":53981},{\"end\":54005,\"start\":54000},{\"end\":54346,\"start\":54341},{\"end\":54362,\"start\":54353},{\"end\":54377,\"start\":54373},{\"end\":54388,\"start\":54383},{\"end\":54410,\"start\":54402},{\"end\":54713,\"start\":54706},{\"end\":54731,\"start\":54724},{\"end\":54747,\"start\":54738},{\"end\":54759,\"start\":54754},{\"end\":54774,\"start\":54769},{\"end\":54790,\"start\":54780},{\"end\":54802,\"start\":54796},{\"end\":55028,\"start\":55024},{\"end\":55038,\"start\":55036},{\"end\":55056,\"start\":55051},{\"end\":55255,\"start\":55251},{\"end\":55267,\"start\":55264},{\"end\":55280,\"start\":55277},{\"end\":55301,\"start\":55297},{\"end\":55684,\"start\":55680},{\"end\":55705,\"start\":55701},{\"end\":56173,\"start\":56169},{\"end\":56190,\"start\":56184},{\"end\":56203,\"start\":56198},{\"end\":56219,\"start\":56213},{\"end\":56235,\"start\":56229},{\"end\":56247,\"start\":56243},{\"end\":56428,\"start\":56424},{\"end\":56437,\"start\":56435},{\"end\":56445,\"start\":56443},{\"end\":56457,\"start\":56453},{\"end\":56768,\"start\":56764},{\"end\":56789,\"start\":56785},{\"end\":56799,\"start\":56796},{\"end\":56943,\"start\":56938},{\"end\":56956,\"start\":56953},{\"end\":56970,\"start\":56966},{\"end\":56991,\"start\":56987},{\"end\":57286,\"start\":57283},{\"end\":57299,\"start\":57294},{\"end\":57311,\"start\":57309},{\"end\":57325,\"start\":57319},{\"end\":57343,\"start\":57337},{\"end\":57358,\"start\":57349},{\"end\":57699,\"start\":57693},{\"end\":57716,\"start\":57706},{\"end\":57735,\"start\":57726}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207241585},\"end\":42181,\"start\":41882},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":49557410},\"end\":42580,\"start\":42183},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2304759},\"end\":42813,\"start\":42582},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":54203999},\"end\":43196,\"start\":42815},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12680508},\"end\":43683,\"start\":43198},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9089716},\"end\":43903,\"start\":43685},{\"attributes\":{\"id\":\"b6\"},\"end\":44224,\"start\":43905},{\"attributes\":{\"id\":\"b7\"},\"end\":44554,\"start\":44226},{\"attributes\":{\"id\":\"b8\"},\"end\":44680,\"start\":44556},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2893830},\"end\":44988,\"start\":44682},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":121902459},\"end\":45275,\"start\":44990},{\"attributes\":{\"doi\":\"arXiv:1902.02918\",\"id\":\"b11\"},\"end\":45561,\"start\":45277},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":131024},\"end\":45925,\"start\":45563},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208310035},\"end\":46255,\"start\":45927},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":211171666},\"end\":46635,\"start\":46257},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52198423},\"end\":47093,\"start\":46637},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7703595},\"end\":47313,\"start\":47095},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b17\"},\"end\":47647,\"start\":47315},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4551073},\"end\":48051,\"start\":47649},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":209439434},\"end\":48499,\"start\":48053},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211069579},\"end\":48981,\"start\":48501},{\"attributes\":{\"doi\":\"arXiv:1811.00741\",\"id\":\"b21\"},\"end\":49277,\"start\":48983},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16687466},\"end\":49662,\"start\":49279},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":31806516},\"end\":50048,\"start\":49664},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":85498668},\"end\":50406,\"start\":50050},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9969658},\"end\":50652,\"start\":50408},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9746839},\"end\":50997,\"start\":50654},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":18466660},\"end\":51446,\"start\":50999},{\"attributes\":{\"id\":\"b28\"},\"end\":51863,\"start\":51448},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15667091},\"end\":52295,\"start\":51865},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":85550403},\"end\":52796,\"start\":52297},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":211069632},\"end\":53096,\"start\":52798},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1553099},\"end\":53520,\"start\":53098},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4626477},\"end\":53909,\"start\":53522},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":35426171},\"end\":54234,\"start\":53911},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3971778},\"end\":54652,\"start\":54236},{\"attributes\":{\"id\":\"b36\"},\"end\":54973,\"start\":54654},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":53298804},\"end\":55165,\"start\":54975},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":211532446},\"end\":55595,\"start\":55167},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":67856444},\"end\":56091,\"start\":55597},{\"attributes\":{\"id\":\"b40\"},\"end\":56414,\"start\":56093},{\"attributes\":{\"doi\":\"arXiv:1703.01340\",\"id\":\"b41\"},\"end\":56694,\"start\":56416},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4956860},\"end\":56930,\"start\":56696},{\"attributes\":{\"doi\":\"arXiv:2006.11165\",\"id\":\"b43\"},\"end\":57212,\"start\":56932},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":155092992},\"end\":57629,\"start\":57214},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":29169801},\"end\":58149,\"start\":57631}]", "bib_title": "[{\"end\":41921,\"start\":41882},{\"end\":42217,\"start\":42183},{\"end\":42614,\"start\":42582},{\"end\":42871,\"start\":42815},{\"end\":43284,\"start\":43198},{\"end\":43734,\"start\":43685},{\"end\":44734,\"start\":44682},{\"end\":45070,\"start\":44990},{\"end\":45627,\"start\":45563},{\"end\":45995,\"start\":45927},{\"end\":46333,\"start\":46257},{\"end\":46689,\"start\":46637},{\"end\":47140,\"start\":47095},{\"end\":47741,\"start\":47649},{\"end\":48154,\"start\":48053},{\"end\":48613,\"start\":48501},{\"end\":49348,\"start\":49279},{\"end\":49699,\"start\":49664},{\"end\":50126,\"start\":50050},{\"end\":50451,\"start\":50408},{\"end\":50737,\"start\":50654},{\"end\":51078,\"start\":50999},{\"end\":51920,\"start\":51865},{\"end\":52365,\"start\":52297},{\"end\":52869,\"start\":52798},{\"end\":53174,\"start\":53098},{\"end\":53593,\"start\":53522},{\"end\":53956,\"start\":53911},{\"end\":54330,\"start\":54236},{\"end\":55014,\"start\":54975},{\"end\":55241,\"start\":55167},{\"end\":55670,\"start\":55597},{\"end\":56755,\"start\":56696},{\"end\":57276,\"start\":57214},{\"end\":57684,\"start\":57631}]", "bib_author": "[{\"end\":41937,\"start\":41923},{\"end\":41947,\"start\":41937},{\"end\":41963,\"start\":41947},{\"end\":41982,\"start\":41963},{\"end\":41996,\"start\":41982},{\"end\":42010,\"start\":41996},{\"end\":42020,\"start\":42010},{\"end\":42239,\"start\":42219},{\"end\":42253,\"start\":42239},{\"end\":42265,\"start\":42253},{\"end\":42281,\"start\":42265},{\"end\":42299,\"start\":42281},{\"end\":42631,\"start\":42616},{\"end\":42646,\"start\":42631},{\"end\":42657,\"start\":42646},{\"end\":42669,\"start\":42657},{\"end\":42676,\"start\":42669},{\"end\":42902,\"start\":42873},{\"end\":42923,\"start\":42902},{\"end\":42940,\"start\":42923},{\"end\":42946,\"start\":42940},{\"end\":43303,\"start\":43286},{\"end\":43317,\"start\":43303},{\"end\":43333,\"start\":43317},{\"end\":43351,\"start\":43333},{\"end\":43363,\"start\":43351},{\"end\":43753,\"start\":43736},{\"end\":43768,\"start\":43753},{\"end\":43782,\"start\":43768},{\"end\":43983,\"start\":43966},{\"end\":43999,\"start\":43983},{\"end\":44017,\"start\":43999},{\"end\":44030,\"start\":44017},{\"end\":44048,\"start\":44030},{\"end\":44060,\"start\":44048},{\"end\":44284,\"start\":44267},{\"end\":44298,\"start\":44284},{\"end\":44311,\"start\":44298},{\"end\":44334,\"start\":44311},{\"end\":44348,\"start\":44334},{\"end\":44366,\"start\":44348},{\"end\":44378,\"start\":44366},{\"end\":44607,\"start\":44594},{\"end\":44754,\"start\":44736},{\"end\":44768,\"start\":44754},{\"end\":45083,\"start\":45072},{\"end\":45092,\"start\":45083},{\"end\":45108,\"start\":45092},{\"end\":45298,\"start\":45277},{\"end\":45316,\"start\":45298},{\"end\":45324,\"start\":45316},{\"end\":45647,\"start\":45629},{\"end\":45664,\"start\":45647},{\"end\":45683,\"start\":45664},{\"end\":45703,\"start\":45683},{\"end\":45724,\"start\":45703},{\"end\":46012,\"start\":45997},{\"end\":46024,\"start\":46012},{\"end\":46037,\"start\":46024},{\"end\":46058,\"start\":46037},{\"end\":46350,\"start\":46335},{\"end\":46375,\"start\":46350},{\"end\":46380,\"start\":46375},{\"end\":46706,\"start\":46691},{\"end\":46719,\"start\":46706},{\"end\":46740,\"start\":46719},{\"end\":46749,\"start\":46740},{\"end\":47155,\"start\":47142},{\"end\":47164,\"start\":47155},{\"end\":47177,\"start\":47164},{\"end\":47192,\"start\":47177},{\"end\":47407,\"start\":47396},{\"end\":47429,\"start\":47407},{\"end\":47445,\"start\":47429},{\"end\":47762,\"start\":47743},{\"end\":47775,\"start\":47762},{\"end\":47792,\"start\":47775},{\"end\":47803,\"start\":47792},{\"end\":47825,\"start\":47803},{\"end\":47832,\"start\":47825},{\"end\":48169,\"start\":48156},{\"end\":48181,\"start\":48169},{\"end\":48195,\"start\":48181},{\"end\":48216,\"start\":48195},{\"end\":48628,\"start\":48615},{\"end\":48642,\"start\":48628},{\"end\":48654,\"start\":48642},{\"end\":48675,\"start\":48654},{\"end\":49063,\"start\":49049},{\"end\":49081,\"start\":49063},{\"end\":49094,\"start\":49081},{\"end\":49357,\"start\":49350},{\"end\":49370,\"start\":49357},{\"end\":49383,\"start\":49370},{\"end\":49405,\"start\":49383},{\"end\":49713,\"start\":49701},{\"end\":49725,\"start\":49713},{\"end\":49739,\"start\":49725},{\"end\":49754,\"start\":49739},{\"end\":49765,\"start\":49754},{\"end\":49779,\"start\":49765},{\"end\":49794,\"start\":49779},{\"end\":50138,\"start\":50128},{\"end\":50151,\"start\":50138},{\"end\":50163,\"start\":50151},{\"end\":50464,\"start\":50453},{\"end\":50477,\"start\":50464},{\"end\":50750,\"start\":50739},{\"end\":50763,\"start\":50750},{\"end\":51106,\"start\":51080},{\"end\":51125,\"start\":51106},{\"end\":51144,\"start\":51125},{\"end\":51157,\"start\":51144},{\"end\":51548,\"start\":51527},{\"end\":51565,\"start\":51548},{\"end\":51581,\"start\":51565},{\"end\":51597,\"start\":51581},{\"end\":51617,\"start\":51597},{\"end\":51625,\"start\":51617},{\"end\":51637,\"start\":51625},{\"end\":51643,\"start\":51637},{\"end\":51937,\"start\":51922},{\"end\":51952,\"start\":51937},{\"end\":51970,\"start\":51952},{\"end\":51988,\"start\":51970},{\"end\":52017,\"start\":51988},{\"end\":52024,\"start\":52017},{\"end\":52035,\"start\":52024},{\"end\":52048,\"start\":52035},{\"end\":52059,\"start\":52048},{\"end\":52064,\"start\":52059},{\"end\":52381,\"start\":52367},{\"end\":52402,\"start\":52381},{\"end\":52887,\"start\":52871},{\"end\":52901,\"start\":52887},{\"end\":52920,\"start\":52901},{\"end\":52935,\"start\":52920},{\"end\":53207,\"start\":53176},{\"end\":53220,\"start\":53207},{\"end\":53227,\"start\":53220},{\"end\":53238,\"start\":53227},{\"end\":53256,\"start\":53238},{\"end\":53268,\"start\":53256},{\"end\":53278,\"start\":53268},{\"end\":53288,\"start\":53278},{\"end\":53295,\"start\":53288},{\"end\":53608,\"start\":53595},{\"end\":53621,\"start\":53608},{\"end\":53636,\"start\":53621},{\"end\":53652,\"start\":53636},{\"end\":53670,\"start\":53652},{\"end\":53686,\"start\":53670},{\"end\":53701,\"start\":53686},{\"end\":53976,\"start\":53958},{\"end\":53992,\"start\":53976},{\"end\":54007,\"start\":53992},{\"end\":54348,\"start\":54332},{\"end\":54364,\"start\":54348},{\"end\":54379,\"start\":54364},{\"end\":54390,\"start\":54379},{\"end\":54396,\"start\":54390},{\"end\":54412,\"start\":54396},{\"end\":54715,\"start\":54696},{\"end\":54733,\"start\":54715},{\"end\":54749,\"start\":54733},{\"end\":54761,\"start\":54749},{\"end\":54776,\"start\":54761},{\"end\":54792,\"start\":54776},{\"end\":54804,\"start\":54792},{\"end\":55030,\"start\":55016},{\"end\":55040,\"start\":55030},{\"end\":55058,\"start\":55040},{\"end\":55257,\"start\":55243},{\"end\":55269,\"start\":55257},{\"end\":55282,\"start\":55269},{\"end\":55303,\"start\":55282},{\"end\":55686,\"start\":55672},{\"end\":55707,\"start\":55686},{\"end\":56175,\"start\":56163},{\"end\":56192,\"start\":56175},{\"end\":56205,\"start\":56192},{\"end\":56221,\"start\":56205},{\"end\":56237,\"start\":56221},{\"end\":56249,\"start\":56237},{\"end\":56430,\"start\":56416},{\"end\":56439,\"start\":56430},{\"end\":56447,\"start\":56439},{\"end\":56459,\"start\":56447},{\"end\":56770,\"start\":56757},{\"end\":56791,\"start\":56770},{\"end\":56801,\"start\":56791},{\"end\":56945,\"start\":56932},{\"end\":56958,\"start\":56945},{\"end\":56972,\"start\":56958},{\"end\":56993,\"start\":56972},{\"end\":57288,\"start\":57278},{\"end\":57301,\"start\":57288},{\"end\":57313,\"start\":57301},{\"end\":57327,\"start\":57313},{\"end\":57345,\"start\":57327},{\"end\":57360,\"start\":57345},{\"end\":57701,\"start\":57686},{\"end\":57718,\"start\":57701},{\"end\":57737,\"start\":57718}]", "bib_venue": "[{\"end\":42023,\"start\":42020},{\"end\":42365,\"start\":42299},{\"end\":42692,\"start\":42676},{\"end\":42990,\"start\":42946},{\"end\":43416,\"start\":43363},{\"end\":43786,\"start\":43782},{\"end\":43964,\"start\":43905},{\"end\":44265,\"start\":44226},{\"end\":44592,\"start\":44556},{\"end\":44816,\"start\":44768},{\"end\":45118,\"start\":45108},{\"end\":45397,\"start\":45340},{\"end\":45734,\"start\":45724},{\"end\":46083,\"start\":46058},{\"end\":46418,\"start\":46380},{\"end\":46821,\"start\":46749},{\"end\":47196,\"start\":47192},{\"end\":47394,\"start\":47315},{\"end\":47842,\"start\":47832},{\"end\":48268,\"start\":48216},{\"end\":48713,\"start\":48675},{\"end\":49047,\"start\":48983},{\"end\":49454,\"start\":49405},{\"end\":49848,\"start\":49794},{\"end\":50220,\"start\":50163},{\"end\":50515,\"start\":50477},{\"end\":50818,\"start\":50763},{\"end\":51206,\"start\":51157},{\"end\":51525,\"start\":51448},{\"end\":52068,\"start\":52064},{\"end\":52528,\"start\":52402},{\"end\":52939,\"start\":52935},{\"end\":53302,\"start\":53295},{\"end\":53708,\"start\":53701},{\"end\":54056,\"start\":54007},{\"end\":54437,\"start\":54412},{\"end\":54694,\"start\":54654},{\"end\":55062,\"start\":55058},{\"end\":55372,\"start\":55303},{\"end\":55792,\"start\":55707},{\"end\":56161,\"start\":56093},{\"end\":56533,\"start\":56475},{\"end\":56805,\"start\":56801},{\"end\":57050,\"start\":57009},{\"end\":57404,\"start\":57360},{\"end\":57833,\"start\":57737},{\"end\":46443,\"start\":46420},{\"end\":46880,\"start\":46823},{\"end\":48738,\"start\":48715},{\"end\":55864,\"start\":55794},{\"end\":57916,\"start\":57835}]"}}}, "year": 2023, "month": 12, "day": 17}