{"id": 218486987, "updated": "2023-10-06 16:27:14.221", "metadata": {"title": "A Transformer-based Approach for Source Code Summarization", "authors": "[{\"first\":\"Wasi\",\"last\":\"Ahmad\",\"middle\":[]},{\"first\":\"Saikat\",\"last\":\"Chakraborty\",\"middle\":[]},{\"first\":\"Baishakhi\",\"last\":\"Ray\",\"middle\":[]},{\"first\":\"Kai-Wei\",\"last\":\"Chang\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens\u2019 position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2005.00653", "mag": "3034689979", "acl": "2020.acl-main.449", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/AhmadCRC20", "doi": "10.18653/v1/2020.acl-main.449"}}, "content": {"source": {"pdf_hash": "d5ca2448ed8633e62618262f77b1d21aa52a32ab", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.acl-main.449.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.acl-main.449.pdf", "status": "HYBRID"}}, "grobid": {"id": "2b0fe17340f5dadafc4723fd057e173cb35ab71d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d5ca2448ed8633e62618262f77b1d21aa52a32ab.txt", "contents": "\nA Transformer-based Approach for Source Code Summarization\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020\n\nWasi Uddin Ahmad wasiahmad@cs.ucla.edu \nSaikat Chakraborty saikatc@cs.columbia.edu \nBaishakhi Ray rayb@cs.columbia.edu \nKai-Wei Chang kwchang@cs.ucla.edu \n\nUniversity of California\nLos Angeles\n\n\nColumbia University\nColumbia University\nUniversity of California\nLos Angeles\n\nA Transformer-based Approach for Source Code Summarization\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\nthe 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 5 -10, 2020. 2020\nGenerating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available 1 to facilitate future research.\n\nIntroduction\n\nProgram comprehension is an indispensable ingredient of software development and maintenance (Xia et al., 2018). A natural language summary of source code facilitates program comprehension by reducing developers' efforts significantly (Sridhara et al., 2010). Source code summarization refers to the task of creating readable summaries that describe the functionality of a program.\n\nWith the advancement of deep learning and the availability of large-scale data through a vast number of open-source repositories, automatic source code summarizing has drawn attention from researchers. Most of the neural approaches generate source code summaries in a sequence-to-sequence fashion. One of the initial works Iyer et al. (2016) trained an embedding matrix to represent the individual code tokens and combine them with a Re-1 https://github.com/wasiahmad/NeuralCodeSum current Neural Network (RNN) via an attention mechanism to generate a natural language summary. Subsequent works (Liang and Zhu, 2018;Hu et al., 2018a,b) adopted the traditional RNNbased sequence-to-sequence network (Sutskever et al., 2014) with attention mechanism (Luong et al., 2015) on different abstractions of code.\n\nThe RNN-based sequence models have two limitations in learning source code representations. First, they do not model the non-sequential structure of source code as they process the code tokens sequentially. Second, source code can be very long, and thus RNN-based models may fail to capture the long-range dependencies between code tokens. In contrast to the RNN-based models, Transformer (Vaswani et al., 2017), which leverages self-attention mechanism, can capture long-range dependencies. Transformers have been shown to perform well on many natural language generation tasks such as machine translation (Wang et al., 2019), text summarization (You et al., 2019), story generation (Fan et al., 2018), etc.\n\nTo learn the order of tokens in a sequence or to model the relationship between tokens, Transformer requires to be injected with positional encodings (Vaswani et al., 2017;Shaw et al., 2018;Shiv and Quirk, 2019). In this work, we show that, by modeling the pairwise relationship between source code tokens using relative position representation (Shaw et al., 2018), we can achieve significant improvements over learning sequence information of code tokens using absolute position representation (Vaswani et al., 2017).\n\nWe want to emphasize that our proposed approach is simple but effective as it outperforms the fancy and sophisticated state-of-the-art source code summarization techniques by a significant margin. We perform experiments on two wellstudied datasets collected from GitHub, and the results endorse the effectiveness of our approach over the state-of-the-art solutions. In addition, we provide a detailed ablation study to quantify the effect of several design choices in the Transformer to deliver a strong baseline for future research.\n\n\nProposed Approach\n\nWe propose to use Transformer (Vaswani et al., 2017) to generate a natural language summary given a piece of source code. Both the code and summary is a sequence of tokens that are represented by a sequence of vectors, x = (x 1 , . . . , x n ) where x i \u2208 R d model . In this section, we briefly describe the Transformer architecture ( \u00a7 2.1) and how to model the order of source code tokens or their pairwise relationship ( \u00a7 2.2) in Transformer.\n\n\nArchitecture\n\nThe Transformer consists of stacked multi-head attention and parameterized linear transformation layers for both the encoder and decoder. At each layer, the multi-head attention employs h attention heads and performs the self-attention mechanism.\n\nSelf-Attention. We describe the self-attention mechanism based on Shaw et al. (2018). In each attention head, the sequence of input vec-\ntors, x = (x 1 , . . . , x n ) where x i \u2208 R d model are transformed into the sequence of output vectors, o = (o 1 , . . . , o n ) where o i \u2208 R d k as: o i = n j=1 \u03b1 ij (x j W V ), e ij = x i W Q (x j W K ) T \u221a d k ,\nwhere \u03b1 ij = exp e ij n k=1 exp e ik and W Q , W K \u2208 R d model \u00d7d k , W V \u2208 R d model \u00d7dv are the parameters that are unique per layer and attention head.\n\nCopy Attention. We incorporate the copying mechanism (See et al., 2017) in the Transformer to allow both generating words from vocabulary and copying from the input source code. We use an additional attention layer to learn the copy distribution on top of the decoder stack (Nishida et al., 2019). The copy attention enables the Transformer to copy rare tokens (e.g., function names, variable names) from source code and thus improves the summarization performance significantly ( \u00a7 3.2).\n\n\nPosition Representations\n\nNow, we discuss how to learn the order of source code tokens or model their pairwise relationship. Encoding absolute position. To allow the Transformer to utilize the order information of source code tokens, we train an embedding matrix W Pe that learns to encode tokens' absolute positions into vectors of dimension d model . However, we show that capturing the order of code tokens is not helpful to learn source code representations and leads to poor summarization performance ( \u00a7 3.2). It is important to note that we train another embedding matrix W P d that learns to encode the absolute positions of summary tokens. 2\n\nEncoding pairwise relationship. The semantic representation of a code does not rely on the absolute positions of its tokens. Instead, their mutual interactions influence the meaning of the source code. For instance, semantic meaning of the expressions a+b and b+a are the same.\n\nTo encode the pairwise relationships between input elements, Shaw et al. (2018) extended the self-attention mechanism as follows.\no i = n j=1 \u03b1 ij (x j W V + a V ij ), e ij = x i W Q (x j W K + a K ij ) T \u221a d k ,\nwhere, a V ij and a K ij are relative positional representations for the two position i and j. Shaw et al. (2018) suggested clipping the maximum relative position to a maximum absolute value of k as they hypothesize that precise relative position information is not useful beyond a certain distance.\na K ij = w K clip(j\u2212i,k) , a V ij = w V clip(j\u2212i,k) , clip(x, k) = max(\u2212k, min(k, x)).\nHence, we learn 2k + 1 relative position representations:  Table 2: Comparison of our proposed approach with the baseline methods. The results of the baseline methods are directly reported from . The \"Base Model\" refers to the vanilla Transformer (uses absolute position representations) and the \"Full Model\" uses relative position representations and includes copy attention.\n(w K \u2212k , . . . , w K k ), and (w V \u2212k , . . . , w V k ).\nIn this work, we study an alternative of the relative position representations that ignores the directional information (Ahmad et al., 2019). In other words, the information whether the j'th token is on the left or right of the i'th token is ignored.\na K ij = w K clip(|j\u2212i|,k) , a V ij = w V clip(|j\u2212i|,k) , clip(x, k) = min(|x|, k).\n\nExperiment\n\n\nSetup\n\nDatasets and Pre-processing. We conduct our experiments on a Java dataset (Hu et al., 2018b) and a Python dataset (Wan et al., 2018). The statistics of the two datasets are shown in Table 1. In addition to the pre-processing steps followed by , we split source code tokens of the form CamelCase and snake case to respective sub-tokens 3 . We show that such a split of code tokens improves the summarization performance. Metrics. We evaluate the source code summarization performance using three metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE-L (Lin, 2004). Baselines. We compare our Transformer-based source code summarization approach with five baseline methods reported in  and their proposed Dual model. We refer the readers to  for the details about the hyperparameter of all the baseline methods. Hyper-parameters. We follow  to set the maximum lengths and vocabulary sizes for code and summaries in both the datasets. We train the Transformer models using Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 10 \u22124 . We set the mini-batch size and dropout rate to 32 and 0.2, respectively. We train the Transformer models for a maximum of 200 epochs and perform early stop if the validation performance does not improve for 20 consecutive iterations. We use a beam search during inference and set the beam size to 4. Detailed hyperparameter settings can be found in Appendix A.\n\n\nResults and Analysis\n\nOverall results. The overall results of our proposed model and baselines are presented in Table 2. The result shows that the Base model outperforms the baselines (except for ROUGE-L in java), while the Full model improves the performance further. 4 We ran the Base model on the original datasets (without splitting the CamelCase and snake case code tokens) and observed that the performance drops by 0.60, 0.72 BLEU and 1.66, 2.09 ROUGE-L points for the Java and Python datasets respectively. We provide a few qualitative examples in Appendix C showing the usefulness of the Full model over the Base model.\n\nUnlike the baseline approaches, our proposed model employs the copy attention mechanism. As shown in Table 2, the copy attention improves the performance 0.44 and 0.88 BLEU points for the Java and Python datasets respectively.\n\n\nImpact of position representation.\n\nWe perform an ablation study to investigate the benefits   of encoding the absolute position of code tokens or modeling their pairwise relationship for the source code summarization task, and the results are presented in Table 3 and 4. Table 3 demonstrates that learning the absolute position of code tokens are not effective as we can see it slightly hurts the performance compared to when it is excluded. This empirical finding corroborates the design choice of Iyer et al. (2016), where they did not use the sequence information of the source code tokens.\n\nOn the other hand, we observe that learning the pairwise relationship between source code tokens via relative position representations helps as Table  4 demonstrates higher performance. We vary the clipping distance, k, and consider ignoring the directional information while modeling the pairwise relationship. The empirical results suggest that the directional information is indeed important while 16, 32, and 2 i relative distances result in similar performance (in both experimental datasets).\n\nVarying model size and number of layers. We perform ablation study by varying d model and l and the results are presented in Table 5. 5 In our experiments, we observe that a deeper model (more layers) performs better than a wider model (larger d model ). Intuitively, the source code summariza-  tion task depends on more semantic information than syntactic, and thus deeper model helps.\n\nUse of Abstract Syntax Tree (AST). We perform additional experiments to employ the abstract syntax tree (AST) structure of source code in the Transformer. We follow Hu et al. (2018a) and use the Structure-based Traversal (SBT) technique to transform the AST structure into a linear sequence. We keep our proposed Transformer architecture intact, except in the copy attention mechanism, we use a mask to block copying the nonterminal tokens from the input sequence. It is important to note that, with and without AST, the average length of the input code sequences is 172 and 120, respectively. Since the complexity of the Transformer is O(n 2 \u00d7 d) where n is the input sequence length, hence, the use of AST comes with an additional cost. Our experimental findings suggest that the incorporation of AST information in the Transformer does not result in an improvement in source code summarization. We hypothesize that the exploitation of the code structure information in summarization has limited advantage, and it diminishes as the Transformer learns it implicitly with relative position representation.\n\nQualitative analysis. We provide a couple of examples in Table 6 to demonstrate the usefulness of our proposed approach qualitatively (more examples are provided in Table 9 and 10 in the Appendix). The qualitative analysis reveals that, in comparison to the Vanilla Transformer model, the copy enabled model generates shorter summaries  with more accurate keywords. Besides, we observe that in a copy enabled model, frequent tokens in the code snippet get a higher copy probability when relative position representations are used, in comparison to absolute position representations. We suspect this is due to the flexibility of learning the relation between code tokens without relying on their absolute position.\n\n\nRelated Work\n\nMost of the neural source code summarization approaches frame the problem as a sequence generation task and use recurrent encoder-decoder networks with attention mechanisms as the fundamental building blocks (Iyer et al., 2016;Liang and Zhu, 2018;Hu et al., 2018a,b). Different from these works, Allamanis et al. (2016) proposed a convolutional attention model to summarize the source codes into short, name-like summaries. Recent works in code summarization utilize structural information of a program in the form of Abstract Syntax Tree (AST) that can be encoded using tree structure encoders such as Tree-LSTM Among other noteworthy works, API usage information (Hu et al., 2018b), reinforcement learning (Wan et al., 2018), dual learning , retrieval-based techniques (Zhang et al., 2020) are leveraged to further enhance the code summarization models. We can enhance a Transformer with previously proposed techniques; however, in this work, we limit ourselves to study different design choices for a Transformer without breaking its' core architectural design philosophy.\n\n\nConclusion\n\nThis paper empirically investigates the advantage of using the Transformer model for the source code summarization task. We demonstrate that the Transformer with relative position representations and copy attention outperforms state-of-the-art approaches by a large margin. In our future work, we want to study the effective incorporation of code structure into the Transformer and apply the techniques in other software engineering sequence generation tasks (e.g., commit message generation for source code changes). A Hyper-Parameters   While conducting our study using the Transformer on the Python dataset, we observed a significant gain over the state-of-the-art methods as reported in . However, our initial experiments on this dataset using recurrent sequence-to-sequence models also demonstrated higher performance compared to the results report in . We suspect that such lower performance is due to not tuning the hyperparameters correctly. So for the sake of fairness and to investigate the true advantages of Transformer, we present a comparison on recurrent Seq2seq model and Transformer in Table 8 using our implementation. 6\n\n\n(Shido et al., 2019),Tree-Transformer (Harer  et al., 2019), and Graph Neural Network (LeClair  et al., 2020). In contrast, Hu et al. (2018a) proposed a structure based traversal (SBT) method to flatten the AST into a sequence and showed improvement over the AST based methods. Later,LeClair et al. (2019)  used the SBT method and de-coupled the code structure from the code tokens to learn better structure representation.\n\nTable 3 :\n3Ablation study on absolute positional representations using the \"Base Model\" on the Java dataset.k Directional BLEU METEOR ROUGE-L \n\n8 \n\n44.22 \n26.35 \n53.86 \n\n42.61 \n24.67 \n51.10 \n\n16 \n\n44.14 \n26.34 \n53.95 \n\n44.06 \n26.31 \n53.51 \n\n32 \n\n44.55 \n26.66 \n54.30 \n\n43.95 \n26.28 \n53.24 \n\n2 i \n\n44.37 \n26.58 \n53.96 \n\n43.58 \n25.95 \n52.73 \n\n\n\nTable 4 :\n4Ablation study on relative positional represen-\ntations (in encoding) for Transformer. While 8, 16, and \n32 represents a fixed relative distance for all the layers, \n2 i (where i = 1, . . . , L; L = 6) represents a layer-wise \nrelative distance for Transformer. \n\n\n\nTable 5 :\n5Ablation study on the hidden size and number \nof layers for the \"Base Model\" on the Java dataset. We \nuse d model = H, d f f = 4H, h = 8, and d k = d v = 64 \nin all settings. We set l = 6 and d model = 512 while \nvarying d model and l respectively. #Param. represents \nthe number of trainable parameters in millions (only \nincludes Transformer parameters). \n\n\n\n\nBase Model: evaluates the xpath expression to a xpath expression . Full Model w/o Relative Position: evaluates the xpath expression . Full Model w/o Copy Attention Attention: evaluates the xpath expression as a single element . Full Model: evaluates the xpath expression as a text string . Human Written: evaluates the xpath expression as text .public static String selectText(XPathExpression expr, Node context) { \ntry { \nreturn (String)expr.evaluate(context, XPathConstants.STRING ); \n} catch (XPathExpressionException e) { \nthrow new XmlException(e); \n} \n} \n\ndef get_hosting_service(name): \ntry: \nreturn hosting_service_registry.get(u'hosting service id', name) \nexcept ItemLookupError: \nreturn None \n\nBase Model: returns the color limits from the current service name . \nFull Model w/o Relative Position: return the hosting service . \nFull Model w/o Copy Attention: return the name of the service . \nFull Model : return the hosting service name . \nHuman Written: return the hosting service with the given name . \n\n\n\nTable 6 :\n6Qualitative example of different models' performance on Java and Python datasets.\n\n\nXing Hu, Ge Li, XinXia, David Lo, and Zhi Jin. 2018a.  Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and  Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2073-2083, Berlin, Germany. Association for Computational Linguistics. Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations. Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Open-source toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstrations, pages 67-72, Vancouver, Canada. Association for Computational Linguistics. Alexander LeClair, Sakib Haque, Linfgei Wu, and Collin McMillan. 2020. Improved code summarization via a graph neural network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. In Proceedings of the 41st International Conference on Software Engineering, page 795-806. IEEE Press. Yuding Liang and Kenny Qili Zhu. 2018. Automatic generation of text descriptive comments for code blocks. In Thirty-Second AAAI Conference on Artificial Intelligence. Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412-1421, Lisbon, Portugal. Association for Computational Linguistics. Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style generative reading comprehension. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2273-2284, Florence, Italy. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.Miltiadis Allamanis, Hao Peng, and Charles A. Sut-\nton. 2016. A convolutional attention network for \nextreme summarization of source code. In Proceed-\nings of the 33nd International Conference on Ma-\nchine Learning, ICML 2016, New York City, NY, \nUSA, June 19-24, 2016, volume 48 of JMLR Work-\nshop and Conference Proceedings, pages 2091-\n2100. JMLR.org. \n\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: \nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65-72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics. \n\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa \nTsuruoka. 2016. Tree-to-sequence attentional neu-\nral machine translation. In Proceedings of the 54th \nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages \n823-833, Berlin, Germany. Association for Compu-\ntational Linguistics. \n\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. \nHierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association \nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 889-898, Melbourne, Australia. Asso-\nciation for Computational Linguistics. \n\nJacob Harer, Chris Reale, and Peter Chin. 2019. Tree-\ntransformer: A transformer-based method for cor-\nrection of tree-structured data. \narXiv preprint \narXiv:1908.00449. \n\nDeep code comment generation. In Proceedings of \nthe 26th Conference on Program Comprehension, \npage 200-210, New York, NY, USA. Association for \nComputing Machinery. \n\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi \nJin. 2018b. Summarizing source code with trans-\nferred api knowledge. In Proceedings of the Twenty-\nSeventh International Joint Conference on Artificial \nIntelligence, IJCAI-18, pages 2269-2275. Interna-\ntional Joint Conferences on Artificial Intelligence \nOrganization. \n\nGuillaume \n\nTable 7\n7summarizes the hyper-parameters that we used in our experiments.Hyper-parameter Value \nEmbedding \nk \n16 \n\nModel \n\nl \n6 \nh \n8 \nd model \n512 \nd k , d v \n64 \nd f f \n2048 \n\nTraining \n\ndropout \n0.2 \noptimizer \nAdam \nlearning rate \n0.0001 \nbatch size \n32 \nTesting \nbeam size \n4 \n\n\n\nTable 7 :\n7Hyper-parameters in our experiments. l and h indicates the number of layers and heads in Transformer respectively. k refers to the clipping distance in relative position representations in Transformer.B Recurrent Encoder-Decoder vs.Transformer on Python DatasetModels \nBLEU METEOR ROUGE-L \nSeq2seq \n30.57 \n17.86 \n43.64 \nSeq2seq  *  \n29.08 \n17.12 \n42.97 \nTransformer \n31.08 \n18.57 \n44.31 \nTransformer  *  \n31.38 \n18.69 \n44.68 \n\n\n\nTable 8 :\n8Comparison between recurrent sequence-tosequence (Seq2seq) model and Transformer on the Python dataset. * indicates models are equipped with the copy attention mechanism.\nIn this work, we do not study alternative ways of learning position representation for the summary tokens.\nThe CamelCase and snake case tokenization reduces the vocabulary significantly. For example, the number of unique tokens in Java source code reduced from 292,626 to 66,650.\nWe observe a more significant gain on the Python dataset and a detailed discussion on it is provided in Appendix B.\nConsidering the model complexity, we do not increase the model size or number of layers further.\nOur implementation is based on Open-NMT(Klein et al.,  2017)  and PyTorch 1.3. We can see fromTable 8, the performance of the recurrent Seq2seq model is much better than the results reported in prior works. However, to our surprise, the copy attention mechanism does not result in improvement for the recurrent Seq2seq model. When we looked into the training perplexity and the validation performance, we also observed lower performance in comparison to the base recurrent Seq2seq model. In comparison, our proposed Transformer-based approach outperforms the recurrent Seq2seq models by a large margin showing its effectiveness for source code summarization.\nAcknowledgmentsThis work was supported in part by National Science Foundation Grant OAC 1920462, CCF  1845893, CCF 1822965, CNS 1842456.\nOn difficulties of cross-lingual transfer with order differences: A case study on dependency parsing. Wasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei Chang, Nanyun Peng, 10.18653/v1/N19-1253Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Wasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei Chang, and Nanyun Peng. 2019. On difficulties of cross-lingual transfer with order dif- ferences: A case study on dependency parsing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers), pages 2440-2452, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nGet to the point: Summarization with pointergenerator networks. Abigail See, J Peter, Christopher D Liu, Manning, 10.18653/v1/P17-1099Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers1Association for Computational LinguisticsAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073- 1083, Vancouver, Canada. Association for Compu- tational Linguistics.\n\nSelf-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, 10.18653/v1/N18-2074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana2Short Papers. Association for Computational LinguisticsPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position represen- tations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 464-468, New Orleans, Louisiana. Association for Computa- tional Linguistics.\n\nAutomatic source code summarization with extended tree-lstm. Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, Tadayuki Matsumura, 2019 International Joint Conference on Neural Networks (IJCNN). IEEEYusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and Tadayuki Matsumura. 2019. Automatic source code summarization with ex- tended tree-lstm. In 2019 International Joint Con- ference on Neural Networks (IJCNN), pages 1-8. IEEE.\n\nNovel positional encodings to enable tree-based transformers. Vighnesh Shiv, Chris Quirk, Advances in Neural Information Processing Systems. Curran Associates, Inc32Vighnesh Shiv and Chris Quirk. 2019. Novel positional encodings to enable tree-based transformers. In Ad- vances in Neural Information Processing Systems 32, pages 12081-12091. Curran Associates, Inc.\n\nTowards automatically generating summary comments for java methods. Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, K Vijay-Shanker, 10.1145/1858996.1859006Proceedings of the IEEE/ACM International Conference on Automated Software Engineering. the IEEE/ACM International Conference on Automated Software EngineeringNew York, NY, USAAssociation for Computing MachineryGiriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K. Vijay-Shanker. 2010. Towards automatically generating summary comments for java methods. In Proceedings of the IEEE/ACM In- ternational Conference on Automated Software En- gineering, page 43-52, New York, NY, USA. Asso- ciation for Computing Machinery.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in Neural Information Processing Systems. Curran Associates, Inc27Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in Neural Information Pro- cessing Systems 27, pages 3104-3112. Curran As- sociates, Inc.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30, pages 5998-6008. Curran Asso- ciates, Inc.\n\nImproving automatic source code summarization via deep reinforcement learning. Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, Philip S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Im- proving automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Auto- mated Software Engineering, pages 397-407. ACM.\n\nLearning deep transformer models for machine translation. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, Lidia S Chao, 10.18653/v1/P19-1176Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep transformer models for ma- chine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 1810-1822, Florence, Italy. Associa- tion for Computational Linguistics.\n\nCode generation as a dual task of code summarization. Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, Zhi Jin, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a dual task of code summariza- tion. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Ad- vances in Neural Information Processing Systems 32, pages 6563-6573. Curran Associates, Inc.\n\nMeasuring program comprehension: A large-scale field study with professionals. Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, Shanping Li, 10.1145/3180155.3182538Proceedings of the 40th International Conference on Software Engineering, ICSE '18. the 40th International Conference on Software Engineering, ICSE '18New York, NY, USA584Association for Computing MachineryXin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E. Hassan, and Shanping Li. 2018. Mea- suring program comprehension: A large-scale field study with professionals. In Proceedings of the 40th International Conference on Software Engineering, ICSE '18, page 584, New York, NY, USA. Associa- tion for Computing Machinery.\n\nImproving abstractive document summarization with salient information modeling. Yongjian You, Weijia Jia, Tianyi Liu, Wenmian Yang, 10.18653/v1/P19-1205Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsYongjian You, Weijia Jia, Tianyi Liu, and Wenmian Yang. 2019. Improving abstractive document sum- marization with salient information modeling. In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2132- 2141, Florence, Italy. Association for Computa- tional Linguistics.\n\nRetrieval-based neural source code summarization. Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Xudong Liu, Proceedings of the 42nd International Conference on Software Engineering. the 42nd International Conference on Software EngineeringIEEEJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization. In Proceedings of the 42nd International Conference on Software Engi- neering. IEEE.\n", "annotations": {"author": "[{\"end\":215,\"start\":176},{\"end\":259,\"start\":216},{\"end\":295,\"start\":260},{\"end\":330,\"start\":296},{\"end\":369,\"start\":331},{\"end\":448,\"start\":370}]", "publisher": "[{\"end\":101,\"start\":60},{\"end\":710,\"start\":669}]", "author_last_name": "[{\"end\":192,\"start\":187},{\"end\":234,\"start\":223},{\"end\":273,\"start\":270},{\"end\":309,\"start\":304}]", "author_first_name": "[{\"end\":180,\"start\":176},{\"end\":186,\"start\":181},{\"end\":222,\"start\":216},{\"end\":269,\"start\":260},{\"end\":303,\"start\":296}]", "author_affiliation": "[{\"end\":368,\"start\":332},{\"end\":447,\"start\":371}]", "title": "[{\"end\":59,\"start\":1},{\"end\":507,\"start\":449}]", "venue": "[{\"end\":596,\"start\":509}]", "abstract": "[{\"end\":1630,\"start\":733}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1757,\"start\":1739},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1904,\"start\":1881},{\"end\":2370,\"start\":2352},{\"end\":2645,\"start\":2624},{\"end\":2664,\"start\":2645},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2751,\"start\":2727},{\"end\":2797,\"start\":2777},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3245,\"start\":3223},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3460,\"start\":3441},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3499,\"start\":3481},{\"end\":3536,\"start\":3518},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3716,\"start\":3694},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3734,\"start\":3716},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3755,\"start\":3734},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3908,\"start\":3889},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4061,\"start\":4039},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4671,\"start\":4649},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5415,\"start\":5397},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5913,\"start\":5895},{\"end\":6138,\"start\":6116},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7343,\"start\":7325},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7590,\"start\":7572},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8439,\"start\":8419},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8787,\"start\":8769},{\"end\":9187,\"start\":9164},{\"end\":9222,\"start\":9189},{\"end\":10365,\"start\":10364},{\"end\":14503,\"start\":14484},{\"end\":14523,\"start\":14503},{\"end\":14542,\"start\":14523},{\"end\":14959,\"start\":14941},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15002,\"start\":14984},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15067,\"start\":15047},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16527,\"start\":16507},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19076,\"start\":19058}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16930,\"start\":16505},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":17272,\"start\":16931},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":17549,\"start\":17273},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":17921,\"start\":17550},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":18942,\"start\":17922},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":19036,\"start\":18943},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":23584,\"start\":19037},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":23869,\"start\":23585},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":24309,\"start\":23870},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":24492,\"start\":24310}]", "paragraph": "[{\"end\":2027,\"start\":1646},{\"end\":2832,\"start\":2029},{\"end\":3542,\"start\":2834},{\"end\":4062,\"start\":3544},{\"end\":4597,\"start\":4064},{\"end\":5066,\"start\":4619},{\"end\":5329,\"start\":5083},{\"end\":5467,\"start\":5331},{\"end\":5840,\"start\":5686},{\"end\":6330,\"start\":5842},{\"end\":6983,\"start\":6359},{\"end\":7262,\"start\":6985},{\"end\":7393,\"start\":7264},{\"end\":7776,\"start\":7477},{\"end\":8240,\"start\":7864},{\"end\":8549,\"start\":8299},{\"end\":10092,\"start\":8655},{\"end\":10723,\"start\":10117},{\"end\":10951,\"start\":10725},{\"end\":11548,\"start\":10990},{\"end\":12048,\"start\":11550},{\"end\":12437,\"start\":12050},{\"end\":13544,\"start\":12439},{\"end\":14259,\"start\":13546},{\"end\":15351,\"start\":14276},{\"end\":16504,\"start\":15366}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5685,\"start\":5468},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7476,\"start\":7394},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7863,\"start\":7777},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8298,\"start\":8241},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8633,\"start\":8550}]", "table_ref": "[{\"end\":7930,\"start\":7923},{\"end\":8844,\"start\":8837},{\"end\":10833,\"start\":10826},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11218,\"start\":11211},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11233,\"start\":11226},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":11702,\"start\":11694},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":12182,\"start\":12175},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":13610,\"start\":13603},{\"end\":13718,\"start\":13711},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":16476,\"start\":16469}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1644,\"start\":1632},{\"attributes\":{\"n\":\"2\"},\"end\":4617,\"start\":4600},{\"attributes\":{\"n\":\"2.1\"},\"end\":5081,\"start\":5069},{\"attributes\":{\"n\":\"2.2\"},\"end\":6357,\"start\":6333},{\"attributes\":{\"n\":\"3\"},\"end\":8645,\"start\":8635},{\"attributes\":{\"n\":\"3.1\"},\"end\":8653,\"start\":8648},{\"attributes\":{\"n\":\"3.2\"},\"end\":10115,\"start\":10095},{\"end\":10988,\"start\":10954},{\"attributes\":{\"n\":\"4\"},\"end\":14274,\"start\":14262},{\"attributes\":{\"n\":\"5\"},\"end\":15364,\"start\":15354},{\"end\":16941,\"start\":16932},{\"end\":17283,\"start\":17274},{\"end\":17560,\"start\":17551},{\"end\":18953,\"start\":18944},{\"end\":23593,\"start\":23586},{\"end\":23880,\"start\":23871},{\"end\":24320,\"start\":24311}]", "table": "[{\"end\":17272,\"start\":17040},{\"end\":17549,\"start\":17285},{\"end\":17921,\"start\":17562},{\"end\":18942,\"start\":18269},{\"end\":23584,\"start\":21578},{\"end\":23869,\"start\":23659},{\"end\":24309,\"start\":24143}]", "figure_caption": "[{\"end\":16930,\"start\":16507},{\"end\":17040,\"start\":16943},{\"end\":18269,\"start\":17924},{\"end\":19036,\"start\":18955},{\"end\":21578,\"start\":19039},{\"end\":23659,\"start\":23595},{\"end\":24143,\"start\":23882},{\"end\":24492,\"start\":24322}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":25888,\"start\":25884},{\"end\":25903,\"start\":25896},{\"end\":25917,\"start\":25911},{\"end\":25928,\"start\":25922},{\"end\":25942,\"start\":25935},{\"end\":25956,\"start\":25950},{\"end\":26852,\"start\":26845},{\"end\":26859,\"start\":26858},{\"end\":26878,\"start\":26867},{\"end\":26880,\"start\":26879},{\"end\":27534,\"start\":27529},{\"end\":27546,\"start\":27541},{\"end\":27564,\"start\":27558},{\"end\":28381,\"start\":28375},{\"end\":28396,\"start\":28389},{\"end\":28415,\"start\":28408},{\"end\":28433,\"start\":28426},{\"end\":28452,\"start\":28444},{\"end\":28851,\"start\":28843},{\"end\":28863,\"start\":28858},{\"end\":29226,\"start\":29216},{\"end\":29242,\"start\":29237},{\"end\":29254,\"start\":29249},{\"end\":29270,\"start\":29266},{\"end\":29281,\"start\":29280},{\"end\":29913,\"start\":29909},{\"end\":29930,\"start\":29925},{\"end\":29946,\"start\":29940},{\"end\":30268,\"start\":30262},{\"end\":30282,\"start\":30278},{\"end\":30296,\"start\":30292},{\"end\":30310,\"start\":30305},{\"end\":30327,\"start\":30322},{\"end\":30340,\"start\":30335},{\"end\":30342,\"start\":30341},{\"end\":30355,\"start\":30350},{\"end\":30795,\"start\":30792},{\"end\":30805,\"start\":30801},{\"end\":30815,\"start\":30812},{\"end\":30830,\"start\":30822},{\"end\":30842,\"start\":30835},{\"end\":30853,\"start\":30849},{\"end\":30866,\"start\":30858},{\"end\":31395,\"start\":31390},{\"end\":31405,\"start\":31402},{\"end\":31414,\"start\":31410},{\"end\":31427,\"start\":31421},{\"end\":31443,\"start\":31433},{\"end\":31453,\"start\":31448},{\"end\":31455,\"start\":31454},{\"end\":31467,\"start\":31462},{\"end\":31469,\"start\":31468},{\"end\":32104,\"start\":32099},{\"end\":32112,\"start\":32110},{\"end\":32120,\"start\":32117},{\"end\":32131,\"start\":32126},{\"end\":32139,\"start\":32136},{\"end\":32688,\"start\":32685},{\"end\":32702,\"start\":32694},{\"end\":32713,\"start\":32708},{\"end\":32727,\"start\":32718},{\"end\":32739,\"start\":32734},{\"end\":32741,\"start\":32740},{\"end\":32758,\"start\":32750},{\"end\":33406,\"start\":33398},{\"end\":33418,\"start\":33412},{\"end\":33430,\"start\":33424},{\"end\":33443,\"start\":33436},{\"end\":34060,\"start\":34056},{\"end\":34070,\"start\":34068},{\"end\":34083,\"start\":34077},{\"end\":34098,\"start\":34091},{\"end\":34110,\"start\":34104}]", "bib_author_last_name": "[{\"end\":25894,\"start\":25889},{\"end\":25909,\"start\":25904},{\"end\":25920,\"start\":25918},{\"end\":25933,\"start\":25929},{\"end\":25948,\"start\":25943},{\"end\":25961,\"start\":25957},{\"end\":26856,\"start\":26853},{\"end\":26865,\"start\":26860},{\"end\":26884,\"start\":26881},{\"end\":26893,\"start\":26886},{\"end\":27539,\"start\":27535},{\"end\":27556,\"start\":27547},{\"end\":27572,\"start\":27565},{\"end\":28387,\"start\":28382},{\"end\":28406,\"start\":28397},{\"end\":28424,\"start\":28416},{\"end\":28442,\"start\":28434},{\"end\":28462,\"start\":28453},{\"end\":28856,\"start\":28852},{\"end\":28869,\"start\":28864},{\"end\":29235,\"start\":29227},{\"end\":29247,\"start\":29243},{\"end\":29264,\"start\":29255},{\"end\":29278,\"start\":29271},{\"end\":29295,\"start\":29282},{\"end\":29923,\"start\":29914},{\"end\":29938,\"start\":29931},{\"end\":29949,\"start\":29947},{\"end\":30276,\"start\":30269},{\"end\":30290,\"start\":30283},{\"end\":30303,\"start\":30297},{\"end\":30320,\"start\":30311},{\"end\":30333,\"start\":30328},{\"end\":30348,\"start\":30343},{\"end\":30362,\"start\":30356},{\"end\":30374,\"start\":30364},{\"end\":30799,\"start\":30796},{\"end\":30810,\"start\":30806},{\"end\":30820,\"start\":30816},{\"end\":30833,\"start\":30831},{\"end\":30847,\"start\":30843},{\"end\":30856,\"start\":30854},{\"end\":30869,\"start\":30867},{\"end\":31400,\"start\":31396},{\"end\":31408,\"start\":31406},{\"end\":31419,\"start\":31415},{\"end\":31431,\"start\":31428},{\"end\":31446,\"start\":31444},{\"end\":31460,\"start\":31456},{\"end\":31474,\"start\":31470},{\"end\":32108,\"start\":32105},{\"end\":32115,\"start\":32113},{\"end\":32124,\"start\":32121},{\"end\":32134,\"start\":32132},{\"end\":32143,\"start\":32140},{\"end\":32692,\"start\":32689},{\"end\":32706,\"start\":32703},{\"end\":32716,\"start\":32714},{\"end\":32732,\"start\":32728},{\"end\":32748,\"start\":32742},{\"end\":32761,\"start\":32759},{\"end\":33410,\"start\":33407},{\"end\":33422,\"start\":33419},{\"end\":33434,\"start\":33431},{\"end\":33448,\"start\":33444},{\"end\":34066,\"start\":34061},{\"end\":34075,\"start\":34071},{\"end\":34089,\"start\":34084},{\"end\":34102,\"start\":34099},{\"end\":34114,\"start\":34111}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.18653/v1/N19-1253\",\"id\":\"b0\",\"matched_paper_id\":67856712},\"end\":26779,\"start\":25782},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1099\",\"id\":\"b1\",\"matched_paper_id\":8314118},\"end\":27472,\"start\":26781},{\"attributes\":{\"doi\":\"10.18653/v1/N18-2074\",\"id\":\"b2\",\"matched_paper_id\":3725815},\"end\":28312,\"start\":27474},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":195069474},\"end\":28779,\"start\":28314},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":202784970},\"end\":29146,\"start\":28781},{\"attributes\":{\"doi\":\"10.1145/1858996.1859006\",\"id\":\"b5\",\"matched_paper_id\":9790585},\"end\":29855,\"start\":29148},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7961699},\"end\":30233,\"start\":29857},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13756489},\"end\":30711,\"start\":30235},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52069701},\"end\":31330,\"start\":30713},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1176\",\"id\":\"b9\",\"matched_paper_id\":174799399},\"end\":32043,\"start\":31332},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":202769028},\"end\":32604,\"start\":32045},{\"attributes\":{\"doi\":\"10.1145/3180155.3182538\",\"id\":\"b11\",\"matched_paper_id\":52988209},\"end\":33316,\"start\":32606},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1205\",\"id\":\"b12\",\"matched_paper_id\":196174574},\"end\":34004,\"start\":33318},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":211163834},\"end\":34455,\"start\":34006}]", "bib_title": "[{\"end\":25882,\"start\":25782},{\"end\":26843,\"start\":26781},{\"end\":27527,\"start\":27474},{\"end\":28373,\"start\":28314},{\"end\":28841,\"start\":28781},{\"end\":29214,\"start\":29148},{\"end\":29907,\"start\":29857},{\"end\":30260,\"start\":30235},{\"end\":30790,\"start\":30713},{\"end\":31388,\"start\":31332},{\"end\":32097,\"start\":32045},{\"end\":32683,\"start\":32606},{\"end\":33396,\"start\":33318},{\"end\":34054,\"start\":34006}]", "bib_author": "[{\"end\":25896,\"start\":25884},{\"end\":25911,\"start\":25896},{\"end\":25922,\"start\":25911},{\"end\":25935,\"start\":25922},{\"end\":25950,\"start\":25935},{\"end\":25963,\"start\":25950},{\"end\":26858,\"start\":26845},{\"end\":26867,\"start\":26858},{\"end\":26886,\"start\":26867},{\"end\":26895,\"start\":26886},{\"end\":27541,\"start\":27529},{\"end\":27558,\"start\":27541},{\"end\":27574,\"start\":27558},{\"end\":28389,\"start\":28375},{\"end\":28408,\"start\":28389},{\"end\":28426,\"start\":28408},{\"end\":28444,\"start\":28426},{\"end\":28464,\"start\":28444},{\"end\":28858,\"start\":28843},{\"end\":28871,\"start\":28858},{\"end\":29237,\"start\":29216},{\"end\":29249,\"start\":29237},{\"end\":29266,\"start\":29249},{\"end\":29280,\"start\":29266},{\"end\":29297,\"start\":29280},{\"end\":29925,\"start\":29909},{\"end\":29940,\"start\":29925},{\"end\":29951,\"start\":29940},{\"end\":30278,\"start\":30262},{\"end\":30292,\"start\":30278},{\"end\":30305,\"start\":30292},{\"end\":30322,\"start\":30305},{\"end\":30335,\"start\":30322},{\"end\":30350,\"start\":30335},{\"end\":30364,\"start\":30350},{\"end\":30376,\"start\":30364},{\"end\":30801,\"start\":30792},{\"end\":30812,\"start\":30801},{\"end\":30822,\"start\":30812},{\"end\":30835,\"start\":30822},{\"end\":30849,\"start\":30835},{\"end\":30858,\"start\":30849},{\"end\":30871,\"start\":30858},{\"end\":31402,\"start\":31390},{\"end\":31410,\"start\":31402},{\"end\":31421,\"start\":31410},{\"end\":31433,\"start\":31421},{\"end\":31448,\"start\":31433},{\"end\":31462,\"start\":31448},{\"end\":31476,\"start\":31462},{\"end\":32110,\"start\":32099},{\"end\":32117,\"start\":32110},{\"end\":32126,\"start\":32117},{\"end\":32136,\"start\":32126},{\"end\":32145,\"start\":32136},{\"end\":32694,\"start\":32685},{\"end\":32708,\"start\":32694},{\"end\":32718,\"start\":32708},{\"end\":32734,\"start\":32718},{\"end\":32750,\"start\":32734},{\"end\":32763,\"start\":32750},{\"end\":33412,\"start\":33398},{\"end\":33424,\"start\":33412},{\"end\":33436,\"start\":33424},{\"end\":33450,\"start\":33436},{\"end\":34068,\"start\":34056},{\"end\":34077,\"start\":34068},{\"end\":34091,\"start\":34077},{\"end\":34104,\"start\":34091},{\"end\":34116,\"start\":34104}]", "bib_venue": "[{\"end\":26125,\"start\":25983},{\"end\":27002,\"start\":26915},{\"end\":27736,\"start\":27594},{\"end\":28526,\"start\":28464},{\"end\":28920,\"start\":28871},{\"end\":29406,\"start\":29320},{\"end\":30000,\"start\":29951},{\"end\":30425,\"start\":30376},{\"end\":30962,\"start\":30871},{\"end\":31583,\"start\":31496},{\"end\":32194,\"start\":32145},{\"end\":32868,\"start\":32786},{\"end\":33557,\"start\":33470},{\"end\":34188,\"start\":34116},{\"end\":26276,\"start\":26127},{\"end\":27093,\"start\":27004},{\"end\":27887,\"start\":27738},{\"end\":29496,\"start\":29408},{\"end\":31040,\"start\":30964},{\"end\":31672,\"start\":31585},{\"end\":32954,\"start\":32870},{\"end\":33646,\"start\":33559},{\"end\":34247,\"start\":34190}]"}}}, "year": 2023, "month": 12, "day": 17}