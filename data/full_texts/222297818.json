{"id": 222297818, "updated": "2022-08-10 13:20:09.52", "metadata": {"title": "A Deep Context-wise Method for Coreference Detection in Natural Language Requirements", "authors": "[{\"first\":\"Yawen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Mingyang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Qing\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yun\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "2020 IEEE 28th International Requirements Engineering Conference (RE)", "journal": "2020 IEEE 28th International Requirements Engineering Conference (RE)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously. Therefore inconsistency caused by specialized jargons and different domains, is inevitable. In particular, entity coreference in Requirement Engineering (RE) is that different linguistic expressions refer to the same real-world entity. It leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively. Manual detection entity coreference is labor-intensive and time-consuming. In this paper, we propose a DEEP context-wise semantic method named DeepCoref to entity COREFerence detection. It consists of one fine-tuning BERT model for context representation and a Word2Vec-based network for entity representation. We use a multi-layer perception in the end to fuse and make a trade-off between two representations for obtaining a better representation of entities. The input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. The evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06% respectively. We also compare DeepCoref with three variants to demonstrate the performance enhancement from different components.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3092431304", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/re/WangSLWY20", "doi": "10.1109/re48521.2020.00029"}}, "content": {"source": {"pdf_hash": "6541e569a8b5ea40c24ec27bcc18218928fe9e45", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "611a7609f4993568810381ff3eb314f10b90d799", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6541e569a8b5ea40c24ec27bcc18218928fe9e45.txt", "contents": "\nA Deep Context-wise Method for Coreference Detection in Natural Language Requirements\n\n\nYawen Wang \nInstitute of Software\nLaboratory for Internet Software Technologies\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nLin Shi \nInstitute of Software\nLaboratory for Internet Software Technologies\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nMingyang Li mingyang@itechs.iscas.ac.cn \n\u2020 \u2021 \nQing Wang \nInstitute of Software\nLaboratory for Internet Software Technologies\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nInstitute of Software\nState Key Laboratory of Computer Sciences\nChinese Academy of Sciences\nBeijingChina\n\nYun Yang yyang@swin.edu.au \nInstitute of Software\nLaboratory for Internet Software Technologies\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nSchool of Software and Electrical Engineering\nSwinburne University of Technology\nAustralia\n\nA Deep Context-wise Method for Coreference Detection in Natural Language Requirements\n10.1109/RE48521.2020.00029Index Terms-Requirement engineeringentity coreferencedeep learningfine-tuning BERT\nRequirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously. Therefore inconsistency caused by specialized jargons and different domains, is inevitable. In particular, entity coreference in Requirement Engineering (RE) is that different linguistic expressions refer to the same real-world entity. It leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively. Manual detection entity coreference is labor-intensive and timeconsuming. In this paper, we propose a DEEP context-wise semantic method named DEEPCOREF to entity COREFerence detection. It consists of one fine-tuning BERT model for context representation and a Word2Vec-based network for entity representation. We use a multi-layer perception in the end to fuse and make a trade-off between two representations for obtaining a better representation of entities. The input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. The evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06% respectively. We also compare DEEPCOREF with three variants to demonstrate the performance enhancement from different components.\n\nI. INTRODUCTION\n\nMost software requirements are specified in natural language with the flexibility to accommodate the arbitrary abstraction [1]- [4]. It is a challenging but essential task to write requirements clearly without inconsistency and ambiguity before passing to the later stages of the development [5], [6]. The inconsistency, which is one of the quality principles related to linguistic aspects of natural language requirements [7], might occur between requirements analysts and domain experts because of their specialized jargons, or stakeholders from different domains.\n\nIn particular, stakeholders could use different linguistic expressions to refer to the same real-world entity in natural language requirements, and we define such phenomena as Entity Coreference (EC). More specifically, Figure 1 presents an example of EC. The three requirements have their related entities: \"industry-related term list\" in R1, \"finance vocabulary * Corresponding authors. list\" in R2 and \"finance word list\" in R3. However, according to their contexts, the three entities refer to the same thing. EC might lead to misconception on entities, thus impairing the readability and understandability of requirements. This work takes the first step to resolve EC in RE.\n\nIn the literature, some works have been proposed to tackle the problem of inconsistency or ambiguity in textual requirements. Pattern-based methods [8]- [15] use Part-of-Speech (POS) patterns or heuristics. Learning-based methods [5], [16], [17] use information retrieval (IR) technique such as Latent Semantic Indexing (LSI) or unsupervised clustering algorithms. Similarity-based methods include word embeddings [3] and syntactic methods (e.g., Jaccard [18] and Levenstein [19]). However, these methods cannot be directly utilized in EC due to the following challenges:\n\n\u2022 Multi-word entity. In textual requirements, entities are more about noun phrases [20], [21] than a single word. As shown in Figure 1, all entities in examples consist of multiple words. Based on observations of our industry data, the average length of entities is 3.52. Multi-word entities are difficult to represent with word-level representation. For example, although E1 refers to the same entity as E2 and E3, E1 is quite different from the other two expressions that they only share one identical word \"list\". If we simply use the word-wise similarity methods such as word embedding, incorrect EC will be given that E2 and E3 are coreferent while E1 is a different entity. \u2022 Missing contextual semantics. Existing solutions lack sentence-level contextual semantic information, which can provide extra information for resolving EC. In most cases, we infer whether two entities are coreferent based on their contexts. Coreferent entities usually have similar contexts. For example, all the three requirements in Figure 1 have similar contextual words such as \"user\", \"online help tool\", etc., which indicate three entities are coreferent. Therefore, how to fuse contextual semantic in entity representation is important as well. \u2022 Insufficient annotated resources. EC detection in RE is a domain-specific task, which cannot directly benefit from large general corpora or public knowledge bases like general coreference detection tasks. In addition, annotating coreferent entities in requirements requires domain expertise and intensive manual effort, resulting in insufficient annotated data for effective learning. How to use limited annotation data and benefit from pre-trained models trained on large general corpora is difficult.\n\nWe propose a DEEP context-wise semantic method named DEEPCOREF to resolve entity COREFerence in natural language requirements. First, we truncate context for each entity and then convert context, entity pairs to model input format. Then we build a context-wise similarity network to infer whether two entities are coreferent. The network consists of two parts. One is a deep fine-tuning BERT context model for context representation, and the other is a Word2Vec-based entity network for entity representation. Finally, we use a Multi-Layer Perceptron (MLP) to fuse two representations. The input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. The combined sentence-level context representation and word-level entity representation can be trained jointly with other parameters, thus obtaining a better entity representation. In addition, with models pre-trained on large corpora (e.g., BERT and word embeddings) and finetuning technique, we only need to annotate small amounts of data for fine-tuning. It alleviates insufficient annotated resource problems and the high cost of manual annotation as well.\n\nWe investigate the effectiveness of DEEPCOREF with data from our industry partner. The experimental results show that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06%. The results confirm that our approach could effectively detect EC from textual requirements, thus can facilitate reaching a shared understanding on entities among multiple stakeholders from different domains in an automated way. We also compare DEEPCOREF with three variants to demonstrate the performance enhancement from different components.\n\nThe main contributions of this paper are as follows:\n\n\u2022 We highlight the importance of detecting EC in RE. \u2022 A deep context-wise semantic method with a powerful representation for entities in textual requirements for automatically detect EC. \u2022 Experimental evaluation on 21 projects with 1853 samples from the industry community with promising results.\n\n\u2022 Public-access of source code 1 to facilitate the replication of our study and its application in other contexts. The rest of the paper is organized as follows. Section II describes the background. Section III presents the design of our proposed approach. Sections IV and V show the experimental setup and evaluation results respectively. Section VI provides a detailed discussion. Section VII describes threats to validity. Section VIII surveys related work. Finally, we summarize the paper in Section IX.\n\n\nII. BACKGROUND\n\nThis section describes key techniques related to this research: word embeddings, fine-tuning BERT and Coreference Resolution (CR). We include them here because our work is based on these techniques.\n\n\nA. Word Embeddings\n\nEmbedding (also known as distributed representation [22], [23]) is a technique for learning vector representations of entities such as words, sentences and images in such a way that similar entities have vectors close to each other [22], [24]. A typical embedding technique is word embedding, which represents words as fixed-length vectors so that similar words are close to each other in the vector space [22], [24], [25]. Comparing with Levenstein [19], here \"similar\" means semantic similarity instead of string similarity. Word embeddings are based on the distributional hypothesis of Harris [26]. We can estimate distances and identify semantic relations from their vectors.\n\nWord embedding is usually implemented by a model such as Continuous Bag-of-Words (CBOW) and Skip-Gram [24]. These models build a neural network that captures the relations between a word and its contextual words. The vector representations of words, as parameters of the network, are trained with a text corpus [22]. Another word embedding model is GloVe [25], which is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\nInformation captured from corpora substantially increases the value of word embeddings to both unsupervised and semisupervised Natural Language Processing (NLP) tasks. For example, good representations of both the target word and the given context are helpful to various tasks, including word sense disambiguation [27], coreference resolution and named entity recognition (NER) [23], [28], [29]. The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings [30]. Likewise, a sentence (i.e., a sequence of words) can also be embedded as a vector [31]. A simple way of sentence embedding is, for example, to consider it as a bag of words and add up all its word vectors [32]. \n\n\nB. Fine-tuning BERT\n\nBERT (Bidirectional Encoder Representations from Transformers) [33] is a deep bidirectional transformer encoder [34] trained with the objective of masked language modeling and the next-sentence prediction task, which proves effective in various NLP tasks.\n\nBERT framework has two steps: 1) pre-training, where the model is trained on unlabeled data over different pre-training tasks. 2) fine-tuning, where the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. BERT has two model sizes: BERT BASE (L=12, H=768, A=12, Total Parameters=110M) and BERT LARGE (L=24, H=1024, A=16, Total Parameters=340M), where the number of layers (i.e., Transformer blocks) is denoted as L, the hidden size as H, and the number of self-attention heads as A.\n\nBERT is designed to unambiguously represent both a single sentence and a pair of sentences in one token sequence, for handling a variety of downstream tasks. As for output, the token representations are fed into an output layer for tokenlevel tasks, and the [CLS] representation is fed into an output layer for classification. The pre-trained BERT can be simply plugged by the task-specific inputs and outputs and fine-tuned all the parameters end-to-end, which is relatively inexpensive compared to pre-training.\n\n\nC. Preliminaries on Coreference Resolution\n\nCoreference is defined as occurring when one or more expressions in a document refer to one entity. CR is a classical NLP task of finding all expressions that are coreferent with any of the entities found in a given text [35]- [38]. In CR, an entity refers to an object or set of objects in the world, while a mention is the textual reference to an entity [36].\n\nThere two types of tasks in CR [37]: 1) resolving entities versus events 2) whether co-referring mentions occur within a single document (WD: within-document) or across a document collection (CD: cross-document). Compared to entity CR, event coreference is considered to be a more difficult task, mostly due to the more complex structure of event mentions [37], [39]. Entity mentions are mostly noun phrases, while event mentions may consist of a verbal predicate (acquire) or a nominalization (acquisition), where these are attached to arguments, including event participants and spatio-temporal information [37]. WDCR approaches provide techniques for the identification of mentions in one document that refer to the same underlying entity/event, while CDCR approaches provide techniques for the identification of mentions in different documents [38].\n\n\nIII. APPROACH\n\nTo address the challenges mentioned in Section I, we propose an approach named DEEPCOREF for resolving EC detection. Figure 2 presents the overview of DEEPCOREF. Given a set of textual requirements written in natural language and its related entity, we firstly truncate their corresponding contexts (see Section III-A). Then we build a context-wise similarity classification network (see Section III-B) to predict whether a pair of entities are semantically equivalent. The network mainly consists of two parts. One is a deep fine-tuning BERT model for encoding the contexts, the other is a Word2Vecbased network for encoding entities. The output of two parts is representations of contexts and entities respectively, which are then fed into an MLP for similarity classification. Finally, we infer the predictive class based on the probabilities produced by the softmax layer. \n\n\nA. Context Truncation\n\nSince entity extraction has been widely developed by many NLP researches [20], [21], [40], DEEPCOREF does not focus on entity extraction, and utilizes entities that have already been extracted as the basic data. In our study, entities are readymade and manually reviewed to avoid error accumulation from entity extraction tools.\n\nIn this study, the context refers to the neighboring words in a window around a certain entity. This step is to truncate requirement text centered on an entity with a window size as the context related to the entity. Given an entity and its related requirement text, we first locate the entity and then truncate text centered on the entity according to the window size. Entities might occur in different positions of one sentence (e.g., near the beginning, near the middle and near the end). So we tackle different cases according to the rules below. We assume window size is M , the length of entity denoted as N , the length of text sequence before entity denoted as l pre , the length of text sequence after entity denoted as l sub :\n\n, the previous text sequence is truncated by length l pre , and all subsequent words are reserved. The final extracted context is a concatenation of truncated previous sequence (denoted as pre), the entity itself and truncated subsequent sequence (denoted as sub):\n[pre \u2295 entity \u2295 sub].\nFinally, we use a special symbol [P AD] padding to the length of window size. Figure 3 demonstrates an example of context extraction for each case. By context truncation, we obtain the entity and its related context (e.g., context, entity ).\n\n\nB. Build Context-wise Similarity Network\n\nThe context-wise similarity network takes two pairs (e.g., context 1 , entity 1 and context 2 , entity 2 ) as input and predicts whether two pairs are coreferent. The network consists of two parts. One is a fine-tuning BERT model for learning context representations, and another is a Word2Vec-based network for learning entity representations. We concatenate two representations for better combining semantic information about the entire contextual sentences and individual words.\n\nFinally, we use an MLP and softmax layer to infer the predictive label.\n\n1) Fine-tuning BERT Context Model: A powerful context representation is helpful for measuring context-wise similarity [41]. In many NLP tasks (e.g., entity disambiguation and entity coreference resolution), the context representations are commonly a collection of the individual embedding of contextual words, (e.g., a weighted average of these embeddings). Such approaches do not include any mechanism for optimizing the representation of the entire contextual sentences [30].\n\nTo obtain a good context representation, we use BERT which is a fine-tuning based and bidirectional pretraining representation model [33]. It takes a sentence pair (e.g., context 1 , context 2 , discomposed from two context, entity pairs) as input, and produces a context vector representation. Due to limited computing resources, we use the model BERT BASE with a relatively small model size, which has 12 layers, hidden dimension size 768 and 12 attention heads. In BERT, the input can be a pair of sentences. Each sentence is represented by 128 word-piece tokens (window size M = 128 in Section III-A). Two contexts are concatenated and fed to the model as a sequence pair together with special start and separator tokens:\n([CLS] context 1 [SEP ] context 2 [SEP ]).\nThe transformer encoder produces a context vector representation (denoted as v ctx ) of the input pair, which is the output of the last hidden layer at the special pooling token [CLS] [33], [42].\n\n2) Word2Vec-based Entity Network: To capture the wordlevel information of entities, we also build a Word2Vec-based network to learn an entity representation using word embeddings [22]. It takes an entity pair (e.g., entity 1 , entity 2 , discomposed from two context, entity pairs) as input, and produces an entity vector representation. We use the 300dimensional word embeddings which are pre-trained on a 1.3G Wikipedia corpus 2 with 223M tokens and 2129K vocabularies. It is trained with three features (word features, n-gram features and character features) using the skip-gram model with negative sampling [43].\n\nFor each entity in the pair entity 1 , entity 2 , we first segment words and obtain the word embedding of each word. Then we use the average of embeddings of all words in one entity to represent the embedding of this entity (denoted as te). So the entity pair can be represented as pe = [te 1 \u2295 te 2 ]. Because the dimension of word embeddings is 300, the dimension of te is 300 and the dimension of pe is 600. After that, pe is fed into a fully connected layer to produce an entity vector representation (denoted as v t ).\n\n3) Representation Fusion: The output of two parts of context-wise similarity network: v ctx is a representation of context pair, and v t is a representation of entity pair. We need to fuse two representations to obtain semantic information in both sentence level and word level. The output is the label which represents whether two entities are coreferent.\nFirst, we concatenate v ctx and v t (v f = [v ctx \u2295 v t ]).\nThen we input v f into MLP. MLP has three layers:\n\n\u2022 A fully connected layer, which is to fuse v ctx and v t into one vector by w v f , where w is a learned parameter vector. w can be trained to make a trade-off between v ctx and v t . \u2022 A dropout layer, which is used to avoid over-fitting [44] by randomly masking some neuron cells. \u2022 An output layer, which transforms the vector into a 2-dimensional vector [s 1 , s 2 ], representing two labels (coreferent or non-coreferent).\n\nThe output of MLP is a similarity measure [s 1 , s 2 ] that represents the scores of the two classes respectively, where s i \u2208 R. Finally, we perform softmax on this 2-dimensional vector, which can be specified as:\nSof tmax(s i ) = e si 2 j=1 e sj Then [s 1 , s 2 ] can be normalized to probabilities [p, 1 \u2212 p], where p \u2208 [0, 1].\n\nC. Implementation\n\nWe implement our approach DEEPCOREF using Transformers 3 which is an open-source library for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pre-trained models built on Pytorch 4 .\n\nTraining Details: As for some crucial settings, the learning rate is set 10 \u22125 . The optimizer is Adam [45] algorithm. We use the mini-batch technique for speeding up the training process with batch size 8. The drop rate is 0.1, which means 10% of neuron cells will be randomly masked to avoid over-fitting. Since the task is a classification problem, we use cross-entropy as the loss function, which is specified as:\nLoss = x p(x) \u00b7 log( 1 q(x) )\nwhere p(x) and q(x) are the probability distribution of predicted label and ground-truth label respectively. The design of the context-wise similarity network makes all parameters jointly fine-tuned on a specific task (e.g., similarity classification), which can benefit from large corpora pretraining in a relatively inexpensive way. It also alleviates insufficient annotated resource problems to some extent. Parameters in BERT are fine-tuned to obtain a better context representation according with specific tasks and data. Parameters in Word2Vec-based network are trained to obtain a better entity representation based on pre-trained word embeddings. Parameters in MLP are trained to better fuse both representations, and make a trade-off between two representations to reach a more accurate classification result.\n\n\nIV. EXPERIMENT DESIGN\n\n\nA. Research Questions\n\nOur evaluation addresses the following three research questions.\n\n\u2022 RQ1 (Advantage) Can DEEPCOREF outperform existing techniques on coreference detection?\n\nTo investigate the advantage of our approach, we conduct 10-fold cross-validation on EC detection using data from our industry partner. We compare the performances of three baselines (see Section IV-C). These approaches include syntactic or semantic similarity measures to detect coreference on word level or sentence level. We compare these approaches to demonstrate the advantage of combining context and entity representations. Besides, we also present statistical results by the project to examine the stability and generalizability across different projects.\n\n\u2022 RQ2 (Effectiveness) How effective does each component facilitate EC detection?\n\nTo examine the performance enhancement introduced by context representations and entity representations respectively, we construct three variants: DEEPCOREF-ctx, which only contains the fine-tuning BERT model for context representations without the Word2Vec-based network for entity representations. DEEPCOREF-entity, which is totally opposite to DEEPCOREF-ctx only with Word2Vec-based network but BERT model (see Section IV-D). In addition, to demonstrate the advantage of fine-tuning BERT over IR-based technique for context representations, we build DEEPCOREF-LSI by repacing the BERT with LSI to produce context vectors. We conduct 10-fold cross-validation on DEEPCOREF, DEEPCOREF-ctx, DEEPCOREF-entity, and DEEPCOREF-LSI respectively to demonstrate the effectiveness for combining two representations.\n\n\u2022 RQ3 (Sensitivity) To what degree does data size influence experimental results?\n\nIn RQ3, we conduct an experiment by increasingly enlarging the size of data to examine the sensitivity between performance enhancement and data augmentation. The amount of data verifies whether fine-tuning approaches can alleviate low-resource problem.\n\n\nB. Data Preparation\n\nOur experimental data is collected from the repositories of China Merchants Bank (CMB) 5 . We retrieve 21 projects from its repository, and each project has a set of requirement texts with corresponding entities that occurred in that text. The total number of text-entity pairs is 1949. The entities are recognized by requirement engineers, audited by project management department and well-maintained through the requirement evolution. We prepare data in the following steps: 1) Pre-processing: For each entity and its related requirement text, we filter noisy tokens such as URL, HTML tags and SQL statements using regular expressions. These tokens are produced by their management system but not removed when dumped from the system. Then we filter template words (e.g., \"I would like to\") which cannot contribute to the result but introduce noise, especially for contextual semantic similarity.\n\n2) Sampling: After pre-processing, we truncate the context for each entity, which is demonstrated in Section III-A. We obtain 1949 context, entity (denoted as ctx-entity pair) pairs in total across all projects. Entities from different projects are definitely different no matter how similar their semantics are, so we sample two ctx-entity (e.g., ctx-entity 1 , ctxentity 2 ) from the same project in a combination way. The combination step is to build relations among context, entity pairs for assigning labels.\n\n3) Ground-truth Labeling: These requirements and entities are domain-specific, so it is challenging for data labeling. To guarantee the accuracy of the labeling results, the labeling process follows two steps: 1) The project management department of CMB assigns samples (e.g., ctx-entity 1 , ctx-entity 2 pairs) as well as original requirement texts for reference to requirement engineers according to the project team so that each annotator can label the samples belonging to his/her own products. 2) The labeling results withdrawn from each project team are reviewed by the project management department. Only those samples where both teams make a full agreement can be included in our dataset. As for samples which are annotated to different labels, two teams would discuss and decide through voting.\n\nThe two classes are extremely imbalanced (e.g., the majority of the samples are non-coreferent) after labeling. We use under-sampling technique to balance two classes. Finally, we obtain 1853 labeled samples ( ctx-entity 1 , ctxentity 2 , label ). The positive labels (897, 48.41%) mean ctxentity 1 and ctx-entity 2 are coreferent, negative labels (956, 51.59%) for non-coreference.\n\n\nC. Baselines\n\nTo further demonstrate the advantages of DEEPCOREF, we compare it with three commonly-used techniques for coreference detection.\n\nWord2Vec: Ferrari et al. [3] present an approach based on word embeddings to support the identification of potentially ambiguous terms in the context of requirements elicitation interviews and group meetings. The \"terms\" in their context is still word-level, or the word itself is a phrase processed by word segment or text chunking. Word embedding provides a good semantic representation on word level. However, in our work, entities are not just single words but several words. We use an average of word embeddings to represent an entity, and then compute a similarity score for coreference detection.\n\nLSI: Falessi et al. [17] use LSI to identify equivalent requirements after comparing several NLP techniques on a given dataset. It is an IR-based semantic sentence-level approach for representing a set of documents as vectors in a common vector space. LSI has been employed in a wide range of software engineering activities such as categorizing source code files [46], detecting high-level conceptual code clones [47], and recovering traceability links between documentation and source code [48], which is considered to be able to resolve the polysemy problem as well [49]- [51]. We build an LSI model to demonstrate its capability for context representations.\n\nLevenstein: It is a syntactic similarity measure by calculating a score for a given pair of entities by finding the best sequence of edit operations to convert one entity into the other [19], [20]. We use the implementation in library Distance 6 .\n\n\nD. Experimental Setup\n\nWe conduct 10-fold cross-validation [52] on the dataset collected from CMB in RQ1 and RQ2. We randomly divide our dataset into ten parts. We use nine of those parts for training and reserve one part for testing. We repeat this procedure 10 times each time reserving a different part for testing. All the experiments are conducted based on the same data folds to avoid the impact of different data partitions. In RQ3, we randomly split data into the training set and testing set according to a training set ratio, which indicates that we use the ratio percentage of data to train model and evaluate the remaining testing set. The experimental environment is a desktop computer equipped with a NVIDIA 1060 GPU, intel core i7 CPU, 16GB RAM, running on Ubuntu OS.\n\nExperiment I (Advantage): To demonstrate the advantage of DEEPCOREF, we compare the performance of DEEPCOREF with three approaches (Word2Vec, LSI, Levenstein distance). Word2Vec uses an average of word embeddings to represent an entity which is to investigate the performance of entity representation with word embeddings. LSI is built on the concatenation of both context and entity to investigate the performance of the combination of context and entity representation with LSI. Levenstein is used to measure the distance between entities to investigate the performance of simple syntactic approaches. The output of Levenstein is a similarity score. The outputs of Word2Vec and LSI are vector representations, and subsequently, we infer the predicted label by computing similarity score via cosine similarity [53]. So we need a similarity ratio to decide whether two entities are coreferent, which should be tuned carefully. We set the ratio of Word2Vec, LSI and Levenstein as 0.85, 0.67 and 0.25 respectively. We investigate the ratio selection in Section VI-A. In addition, we give statistical results by the project to examine the stability and generalizability across different projects.\n\nExperiment II (Effectiveness): We demonstrate the performance enhancement introduced by each component by constructing DEEPCOREF-ctx and DEEPCOREF-entity. DEEP-COREF-ctx only keeps BERT model and DEEPCOREF-entity only keeps Word2Vec network. In addition, we build DEEP-COREF-LSI, which is a variant of DEEPCOREF by replacing the BERT with LSI, and other parts remain unchanged. DEEP-COREF-LSI is to demonstrate the performance enhancement from BERT for computing context representation.\n\n\nExperiment III (Sensitivity):\n\nWe conduct an experiment by increasingly enlarging the size of data to examine the sensitivity between performance enhancement and data augmentation. We present the time consumption of each experiment as well. The experiment is conducted by splitting all data into two parts (training set and testing set) randomly according to the training set ratio which is increased from 5% to 90%. For each ratio, we perform five experiments with a boxplot to show the evaluation metrics, and take the average time of five experiments as consuming time.\n\n\nE. Evaluation Metrics\n\nWe use precision recall, and F1-Score, which are commonly-used metrics, to evaluate the performance of DEEP-COREF. We mentioned that we collect and annotate data in cooperation with CMB (see Section IV-B). Given the groundtruth label and predicted label from DEEPCOREF, we compute the metrics of all testing data for each round of 10-fold crossvalidation to measure the performance. As for the performance by the project in RQ1, we compute the metrics for each project.\n\n1) Precision, which refers to the ratio of the number of correct predictions of positive labels to the total number of predictions of positive labels. 2) Recall, which refers to the ratio of the number of correct predictions of positive labels to the total number of positive labels. 3) F1-Score, which is the harmonic mean of precision and recall.\n\nWe calculate metrics for each label and take their unweighted mean as final results. In addition, some baseline approaches need to measure the similarity to decide whether two entities are coreferent, so we use cosine similarity [53] to compute the distance between two vector representations.\n\n\nV. RESULTS AND ANALYSIS\n\nA. Answering RQ1: Advantage of DEEPCOREF Figure 4 presents the EC detection performance on DEEP-COREF and baselines respectively across the 10-fold crossvalidation. We can see that DEEPCOREF can achieve 96.10% precision and 96.06% recall on average, which are much higher than other baselines. The precision and recall of Word2Vec are 84.57% and 84.21% respectively, LSI 84.12% and 84.01%, Levenstein 84.65% and 83.46%. In addition, the length of the box of DEEPCOREF is relatively lower than baselines, further signifying the stability of the performance. Figure 5 presents the precision and recall by 21 projects. We can see both precision and recall of DEEPCOREF are more stable and higher than other baselines across projects. The text presentation styles are distinct in different projects, so the results of Word2Vec and Levenstein indicate large differences in performance on different projects. These two approaches lack sentence-level information of context, thus cannot capture the contextual semantic differences across projects only using entity information. LSI fluctuates largely in several projects although it can capture the sentential context semantics. This is mainly because LSI is constructed based on statistical information on current training data, the representation ability is less powerful than models pre-trained on large corpora and fine-tuned with training data. By contrast, DEEPCOREF which is more stable, obtains a more powerful representation by combining the context semantics, thus more adaptable to different presentation styles.\n\nThe reasons why DEEPCOREF noticeably outperforms the three baselines are: 1) DEEPCOREF uses both sentence-level and word-level semantics thus can capture more information from contexts and entities. 2) DEEPCOREF uses pre-trained models thus can benefit from large general corpora pretraining. 3) DEEPCOREF uses the fine-tuning technique, which can improve adaptation on domain-specific tasks.\n\nB. Answering RQ2: Effectiveness of DEEPCOREF Figure 6 presents the performance on DEEPCOREF and three variants respectively across the 10-fold cross-validation. The average of precision and recall of DEEPCOREF-ctx reach 79.83% and 68.21%, DEEPCOREF-entity 63.17% and 61.77%, DEEPCOREF-LSI 66.25% and 62.62% respectively. The performance of DEEPCOREF is much higher and more stable than three variants.\n\nThe comparison among DEEPCOREF, DEEPCOREF-ctx and DEEPCOREF-entity indicates the performance enhancement from different components. More specifically, the fine-tuning BERT model improves the performance of precision and recall by 32.93% and 34.29% (differences between DEEPCOREF and DEEPCOREF-entity). The Word2Vec-based network improves performance by 16.27% and 27.85% (differences between DEEPCOREF and DEEPCOREF-ctx). The comparison between DEEPCOREF-ctx and DEEPCOREF-entity indicates that context semantics are more effective than entity semantics. The improvement of precision and recall reaches 16.66% and 6.44% respectively. The comparison between DEEPCOREF and DEEPCOREF-LSI indicates the stronger contextual representation from BERT than LSI, where the improvement reaches 29.85% and 33.44% respectively.  In summary, each component of our network improves the performance to varying degrees. The combination can obtain a quite promising performance. The application of fine-tuning BERT model significantly enhances performance.\n\nC. Answering RQ3: Sensitivity of DEEPCOREF Figure 7 represents the relationship between performance and time consumption of DEEPCOREF when enlarging the size of the training set. When the training set ratio increases from 5% to 90%, the performance of DEEPCOREF rises sharply before 20% and has a small increase later. The variance of data at each point after 40% is also similar. The last point is the results coming from RQ1 for comparison, which shows the best performance. The results indicate that DEEPCOREF is not very sensitive after the training data is greater than 60% (i.e., around 1100 in our experimental settings). Moreover, we can find that just using 20% data to train, the performance is also greater than 92% on average. It demonstrates that DEEPCOREF can address the low-resource problem well. In addition, the time consumption increases approximatively linearly from 159.42s to 412.52s.\n\nIn summary, benefiting from large corpora pre-training and the fine-tuning technique, DEEPCOREF can reach a promising performance on a relatively small dataset.\n\n\nVI. DISCUSSION\n\n\nA. Parameter Settings on Baselines\n\nThe performances of baselines are affected by the value selection of similarity ratios. Here we discuss the parameter determination process in our experiments. To achieve the best performance of these baselines, we conduct a set of experiments to find the sweet parameters. The best parameter settings are used in the comparison. Baselines are similaritybased methods, which are sensitive to the value of the ratio, so the parameter we analyze is similarity ratio. We vary the values of similarity ratios for Word2Vec, LSI, and Levenstein respectively, and evaluate their impact on the performance. We present the box-plot changing curve (each box includes 10 results from 10-fold cross-validation) of F1-Score for each approach, when the ratio increases in [0, 1] by step 0.01 (for readability, the step in the figure is 0.03). We also present the optimal value of the ratio for each round of 10-fold crossvalidation of each approach. The final similarity ratio of each approach is computed by an average of 10 optimal values. Figure 8 shows the F1-Score when the similarity ratio increases from 0 to 1 for each baseline. We can see that the ratio can influence the performance of these similarity-based approaches significantly, and the optimal values are distinct for each approach. Generally with the increase of similarity ratio, the F1-Score first rises and then declines for all approaches. Nevertheless, this general trend exhibits a slight difference among these approaches. For Word2Vec, the curve is steep, rising when the ratio is less than 0.85 and declining after that, which means that the optimal values of the ratio are stable around 0.85 for each round of 10-fold cross-validation. For LSI, the curve rises slowly before 0.5, then keeps steady between 0.5 and 0.7, and finally declines after 0.7. This means that the optimal values fluctuate in the interval (0.5, 0.7) for all rounds. For Levenstein, the cure rises dramatically before 0.25, then declines slowly between 0.25 and 0.65, and declines dramatically after 0.65. The optimal values are around 0.25.\n\nFor each round of 10-fold cross-validation, the optimal similarity ratios of Word2Vec and Levenstein are the same values of 0.85 and 0.25 respectively, while the optimal ratio of LSI fluctuates slightly around 0.67, which is consistent with changing curves in Figure 8. The final ratio is computed by the average of these optimal values, where the ratios of Word2Vec, LSI and Levenstein are 0.85, 0.67 and 0.25. Hence one should carefully tune the similarity ratios for each approach, in order to achieve the best performance for a fair comparison.\n\n\nB. Applicability\n\nWe list some key points when applying our method: 1) Our method is evaluated on short texts, where contexts can contain enough semantic information. When applying to long texts, some contexts truncated by window may lack useful information which is far from entities. Tuning window size might alleviate the problem. 2) Our data is from financial domain. One should annotate about 1000 samples for fine-tuning the whole model to tackle domain adaption.\n\n3) The entities in our data are ready-made. If somenone wants to apply our method but has no entities, he/she needs to extract entities firstly using mature NLP techniques such as text trunking [20], [21] and POS patterns [40]. However, the error brought by these tools needs manual correction inevitably. 4) When applying to other languages, BERT and word embeddings must be pre-trained on corpus of corresponding language.\n\n\nVII. THREATS TO VALIDITY\n\nExternal Validity: The external threats are related to the generalizability of the approach. Although the data is collected from the industry community, we retrieve as many projects as possible. The evaluation results by projects show that our approach is generalizable across projects, which largely alleviates the threat. In addition, our approach uses models pre-trained on large general corpus and the fine-tuning technique, which alleviates the low-resource and generalizability problem.\n\nInternal Validity: The internal threats relate to experimental errors and biases. Threats to internal validity may come from the entity generation. The entities in our data are readymade and well-maintained by our industry partners, which has a slight impact on our results.\n\nConstruct Validity: The construct threats relate to the suitability of evaluation metrics. We utilize precision and recall for evaluation, where we use cosine similarity to measure whether two entities are coreferent. The threats might come from the selection of similarity ratio. To reduce that threat, we perform an experiment on tuning ratios and use the average of optimal values as ratios (see Section VI-A). In addition, both predictive positive and negative labels are equally important in predictions, so we calculate the evaluation metrics for each label and take their unweighted mean as final results.\n\n\nVIII. RELATED WORK\n\nOur work is related to previous studies that focused on 1) detection of inconsistency in requirements written in natural language; and 2) coreference resolution. We briefly review the recent works in each category.\n\n\nA. Detection of Inconsistency\n\nThe amount of research on inconsistency detection has increased significantly in the past years. Mezghani et al. [5] used unsupervised machine learning algorithm, k-means, for a redundancy and inconsistency detection in the RE context. They introduced a filtering approach to eliminate \"noisy\" requirements and a pre-processing step based on the NLP technique and used POS tagging and noun chunking to detect technical business terms. PBURC [16] is a pattern-based unsupervised requirements clustering framework (based on k-means algorithm), which makes use of machine-learning methods for requirements validation. The approach aimed to overcome data inconsistencies and effectively determine appropriate requirements clusters for the optimal definition of software development sprints. Traditional techniques such as bag-of-words (BOW), Term Frequency and Inverse Document Frequency (TF-IDF) frequency matrix and n-gram language modeling were firstly used on redundancy detection. Juergens et al. [54] found that clone detection, a technique widely applied to source code, is promising to assess redundancy in an automated way. They used ConQAT to identify copy&paste operations in software requirements specifications. Falessi et al. [17] conjectured and assessed that NLP techniques identifying equivalent requirements perform on a given dataset according to both ability and the odds of making correct identification. Also, they proposed a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. They used IR methods such as Latent Semantic Analysis. Rago et al. [55] introduced a novel approach called ReqAligner that aids analysts to spot signs of duplication in use cases in an automated fashion. ReqAligner combines several text processing techniques, such as a use case classifier and a customized algorithm for sequence alignment.\n\nAmbiguity is usually related to inconsistency. In the literature, many works have been proposed to tackle the problem of ambiguity in written requirements. Ferrari et al. [3] presented an NLP approach to identify ambiguous terms between different domains and rank them by ambiguity score. The approach is based on building domain-specific language models in each domain. They compared different word embeddings of one identical term from different domains to estimate its potential ambiguity across the domains of interest. There are some works using special terms and expressions with different POS or patterns [8]- [13]. Other works use heuristics to tackle coordination ambiguities (i.e., ambiguities brought by \"and\" or \"or\" conjunctions) [14] and anaphoric ones (i.e., ambiguities brought by pronouns) [15].\n\nOur work complements to the existing researches in two aspects: 1) It is a method to resolving EC in RE. Detecting EC can improve the readability and understandability of requirements. 2) It is a deep learning approach, which is more powerful and generic.\n\n\nB. Coreference Resolution\n\nOur work is inspired by CDCR, so we review representative works on CR in recent years. For WDCR, Lee et al. [35] introduced the first end-to-end coreference resolution model without using a syntactic parser or handengineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. Joshi et al. [56] finetuned BERT to coreference resolution, achieving the state-ofthe-art performance. However, they considered there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. As for CDCR, Lee et al. [57] introduced a novel coreference resolution system that models entities and events jointly by iteratively constructing clusters of entity and event mentions using linear regression to model cluster merge operations. The joint formulation allowed information from event coreference to help entity coreference, and vice versa. Inspired by [57], Barhom et al. [37] proposed a neural architecture for cross-document coreference resolution, which represents an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures.\n\nCR presented in our work related to RE differs from NLP in two aspects: 1) Requirements in RE are domain-specific, so we cannot directly benefit from large general corpora or public knowledge bases. 2) In RE, the requirements and entities are related to a specific domain, where data annotation needs domain expertise and intensive manual effort. In our work, we use fine-tune technique and pre-training models to tackle these pivotal challenges.\n\n\nIX. CONCLUSION AND FUTURE WORK\n\nThis paper resolves entity coreference in requirement engineering. We propose a DEEP context-wise semantic method named DEEPCOREF for entity COREFerence detection. It consists of two parts: one is a fine-tuning BERT model for context representation, and the other is a Word2Vec-based network for entity representation. Then a multi-layer perceptron is followed to fuse and make a trade-off between two representations in order to obtain a better representation of the entity. We investigate the effectiveness of DEEPCOREF with 1853 samples on 21 projects from the industry community. The experimental results show that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06% respectively. In order to demonstrate the performance enhancement from different components, we compare DEEPCOREF with three variants as well.\n\nIn the future, we plan to add some event features into the entity representations based on what we have proposed in this work, because event information can help distinguish entities more precisely. \n\nFig. 1 .\n1Examples of coreferent entities in textual requirements, which make the requirements difficult to understand.\n\nFig. 2 .\n2The Overview of DEEPCOREF\n\nFig. 3 .\n3An example of context truncation. The bold word (e.g., user) is entity word. The red dotted rectangle represents the window. Here window size M = 5, and the length of entity N = 1.\n\nFig. 4 .\n4RQ1: The advantage of DEEPCOREF over baselines. The cross is the mean value of 10-fold cross validation.\n\nFig. 5 .\n5RQ1: The advantage of DEEPCOREF over baselines by project. The number of projects is 21.\n\nFig. 6 .\n6RQ2: The performance of DEEPCOREF and its variants. The cross is the mean value of 10-fold cross validation.\n\nFig. 7 .\n7RQ3: The performance of DEEPCOREF by data augmentation. The dotted line is time consuming.\n\nFig. 8 .\n8The boxplot changing curve of F1-Score with the similarity ratio increasing from 0 to 1 by step 0.03 for each approach. Each box contains results of one 10-fold cross validation.\n\n\nACKNOWLEDGMENTSThis work is supported by the National Key Research and Development Program of China under grant No.2018YFB140-3400, the National Science Foundation of China under grant No.61802374, No.61432001, No.61602450. This work is also supported by China Merchants Bank Intelligent Software Research and Development Effectiveness Research Project.\nhttps://github.com/MeloFancy/DeepCoref\nhttps://dumps.wikimedia.org/\nhttps://github.com/huggingface/transformers 4 https://pytorch.org\nIt is one of the world's top five hundred commercial banks.\nhttps://github.com/doukremt/distance\n\nM E C Hull, K Jackson, J Dick, Requirements Engineering. LondonSpringerM. E. C. Hull, K. Jackson, and J. Dick, Requirements Engineering. Springer London, 2002.\n\nMining requirements knowledge from collections of domain documents. X Lian, M Rahimi, J Cleland-Huang, L Zhang, R Ferrai, M Smith, 24th IEEE International Requirements Engineering Conference. Beijing, ChinaIEEE Computer SocietyX. Lian, M. Rahimi, J. Cleland-Huang, L. Zhang, R. Ferrai, and M. Smith, \"Mining requirements knowledge from collections of domain documents,\" in 24th IEEE International Requirements Engineering Conference, RE 2016, Beijing, China, September 12-16, 2016. IEEE Computer Society, 2016, pp. 156-165.\n\nAn NLP approach for cross-domain ambiguity detection in requirements engineering. A Ferrari, A Esuli, Autom. Softw. Eng. 263A. Ferrari and A. Esuli, \"An NLP approach for cross-domain ambiguity detection in requirements engineering,\" Autom. Softw. Eng., vol. 26, no. 3, pp. 559-598, 2019.\n\nMining domain knowledge [requirements]. J Cleland-Huang, IEEE Software. 323J. Cleland-Huang, \"Mining domain knowledge [requirements],\" IEEE Software, vol. 32, no. 3, pp. 16-19.\n\nIndustrial requirements classification for redundancy and inconsistency detection in SEMIOS. M Mezghani, J Kang, F S\u00e8des, 26th IEEE International Requirements Engineering Conference. G. Ruhe, W. Maalej, and D. AmyotBanff, AB, CanadaM. Mezghani, J. Kang, and F. S\u00e8des, \"Industrial requirements classifi- cation for redundancy and inconsistency detection in SEMIOS,\" in 26th IEEE International Requirements Engineering Conference, RE 2018, Banff, AB, Canada, August 20-24, 2018, G. Ruhe, W. Maalej, and D. Amyot, Eds. IEEE Computer Society, 2018, pp. 297-303.\n\nNaming the pain in requirements engineering -contemporary problems, causes, and effects in practice. D M Fern\u00e1ndez, S Wagner, M Kalinowski, M Felderer, P Mafra, A Vetro, T Conte, M Christiansson, D Greer, C Lassenius, T M\u00e4nnist\u00f6, M Nayabi, M Oivo, B Penzenstadler, D Pfahl, R Prikladnicki, G Ruhe, A Schekelmann, S Sen, R O Sp\u00ednola, A Tuzcu, J L De La Vara, R J Wieringa, Empirical Software Engineering. 225D. M. Fern\u00e1ndez, S. Wagner, M. Kalinowski, M. Felderer, P. Mafra, A. Vetro, T. Conte, M. Christiansson, D. Greer, C. Lassenius, T. M\u00e4nnist\u00f6, M. Nayabi, M. Oivo, B. Penzenstadler, D. Pfahl, R. Prik- ladnicki, G. Ruhe, A. Schekelmann, S. Sen, R. O. Sp\u00ednola, A. Tuzcu, J. L. de la Vara, and R. J. Wieringa, \"Naming the pain in requirements engineering -contemporary problems, causes, and effects in practice,\" Empirical Software Engineering, vol. 22, no. 5, pp. 2298-2338, 2017.\n\nThe linguistic approach to the natural language requirements quality: Benefits of the use of an automatic tool. F Fabbrini, M Fusani, S Gnesi, G Lami, Proceedings. 26th Annual NASA Goddard. 26th Annual NASA GoddardSoftware Engineering WorkshopF. Fabbrini, M. Fusani, S. Gnesi, and G. Lami, \"The linguistic approach to the natural language requirements quality: Benefits of the use of an automatic tool,\" in Software Engineering Workshop, 2001. Proceedings. 26th Annual NASA Goddard, 2001.\n\nThe syntactically dangerous all and plural in specifications. D M Berry, E Kamsties, IEEE Software. 221D. M. Berry and E. Kamsties, \"The syntactically dangerous all and plural in specifications,\" IEEE Software, vol. 22, no. 1, pp. 55-57, 2005.\n\nThe design of SREE -A prototype potential ambiguity finder for requirements specifications and lessons learned. S F Tjong, D M Berry, Requirements Engineering: Foundation for Software Quality -19th International Working Conference, REFSQ 2013. J. D\u00f6rr and A. L. OpdahlEssen, GermanySpringer7830Proceedings, ser. Lecture Notes in Computer ScienceS. F. Tjong and D. M. Berry, \"The design of SREE -A prototype potential ambiguity finder for requirements specifications and lessons learned,\" in Requirements Engineering: Foundation for Software Quality -19th International Working Conference, REFSQ 2013, Essen, Germany, April 8-11, 2013. Proceedings, ser. Lecture Notes in Computer Science, J. D\u00f6rr and A. L. Opdahl, Eds., vol. 7830. Springer, 2013, pp. 80-95.\n\nAmbiguity detection: Towards a tool explaining ambiguity sources. B Gleich, O Creighton, L Kof, Requirements Engineering: Foundation for Software Quality, 16th International Working Conference, REFSQ 2010. R. J. Wieringa and A. PerssonEssen, GermanySpringer6182Proceedings, ser. Lecture Notes in Computer ScienceB. Gleich, O. Creighton, and L. Kof, \"Ambiguity detection: Towards a tool explaining ambiguity sources,\" in Requirements Engineering: Foun- dation for Software Quality, 16th International Working Conference, REFSQ 2010, Essen, Germany, June 30 -July 2, 2010. Proceedings, ser. Lecture Notes in Computer Science, R. J. Wieringa and A. Persson, Eds., vol. 6182. Springer, 2010, pp. 218-232.\n\nUsing NLP to detect requirements defects: An industrial experience in the railway domain. B Rosadini, A Ferrari, G Gori, A Fantechi, S Gnesi, I Trotta, S Bacherini, Requirements Engineering: Foundation for Software Quality -23rd International Working Conference. P. Gr\u00fcnbacher and A. PeriniEssen, GermanySpringer10153Proceedings, ser. Lecture Notes in Computer ScienceB. Rosadini, A. Ferrari, G. Gori, A. Fantechi, S. Gnesi, I. Trotta, and S. Bacherini, \"Using NLP to detect requirements defects: An industrial experience in the railway domain,\" in Requirements Engineering: Foun- dation for Software Quality -23rd International Working Conference, REFSQ 2017, Essen, Germany, February 27 -March 2, 2017, Pro- ceedings, ser. Lecture Notes in Computer Science, P. Gr\u00fcnbacher and A. Perini, Eds., vol. 10153. Springer, 2017, pp. 344-360.\n\nOn the impact of passive voice requirements on domain modelling. H Femmer, J Kucera, A Vetro, 2014 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM '14. M. Morisio, T. Dyb\u00e5, and M. TorchianoTorino, ItalyACM214H. Femmer, J. Kucera, and A. Vetro, \"On the impact of passive voice requirements on domain modelling,\" in 2014 ACM-IEEE Interna- tional Symposium on Empirical Software Engineering and Measurement, ESEM '14, Torino, Italy, September 18-19, 2014, M. Morisio, T. Dyb\u00e5, and M. Torchiano, Eds. ACM, 2014, pp. 21:1-21:4.\n\nRapid quality assurance with requirements smells. H Femmer, D M Fern\u00e1ndez, S Wagner, S Eder, Journal of Systems and Software. 123H. Femmer, D. M. Fern\u00e1ndez, S. Wagner, and S. Eder, \"Rapid quality assurance with requirements smells,\" Journal of Systems and Software, vol. 123, pp. 190-213, 2017.\n\nIdentifying nocuous ambiguities in natural language requirements. F Chantree, B Nuseibeh, A De Roeck, A Willis, 14th IEEE International Requirements Engineering Conference (RE'06). F. Chantree, B. Nuseibeh, A. De Roeck, and A. Willis, \"Identifying nocuous ambiguities in natural language requirements,\" in 14th IEEE International Requirements Engineering Conference (RE'06), 2006.\n\nAnalysing anaphoric ambiguity in natural language requirements. H Yang, A De Roeck, V Gervasi, A Willis, B Nuseibeh, Requir. Eng. 163H. Yang, A. De Roeck, V. Gervasi, A. Willis, and B. Nuseibeh, \"Analysing anaphoric ambiguity in natural language requirements,\" Requir. Eng., vol. 16, no. 3, pp. 163-189, 2011.\n\nPBURC: a patternsbased, unsupervised requirements clustering framework for distributed agile software development. P Belsis, A Koutoumanos, C Sgouropoulou, Requir. Eng. 192P. Belsis, A. Koutoumanos, and C. Sgouropoulou, \"PBURC: a patterns- based, unsupervised requirements clustering framework for distributed agile software development,\" Requir. Eng., vol. 19, no. 2, pp. 213-225, 2014.\n\nEmpirical principles and an industrial case study in retrieving equivalent requirements via natural language processing techniques. D Falessi, G Cantone, G Canfora, IEEE Trans. Software Eng. 391D. Falessi, G. Cantone, and G. Canfora, \"Empirical principles and an industrial case study in retrieving equivalent requirements via natural language processing techniques,\" IEEE Trans. Software Eng., vol. 39, no. 1, pp. 18-44, 2013.\n\nA comparison of string distance metrics for name-matching tasks. W W Cohen, P Ravikumar, S E Fienberg, Proceedings of IJCAI-03 Workshop on Information Integration on the Web (IIWeb-03). S. Kambhampati and C. A. KnoblockIJCAI-03 Workshop on Information Integration on the Web (IIWeb-03)Acapulco, MexicoW. W. Cohen, P. Ravikumar, and S. E. Fienberg, \"A comparison of string distance metrics for name-matching tasks,\" in Proceedings of IJCAI-03 Workshop on Information Integration on the Web (IIWeb-03), August 9-10, 2003, Acapulco, Mexico, S. Kambhampati and C. A. Knoblock, Eds., 2003, pp. 73-78.\n\nIntroduction to Information Retrieval. C D Manning, P Raghavan, H Sch\u00fctze, C. D. Manning, P. Raghavan, and H. Sch\u00fctze, Introduction to Informa- tion Retrieval, 2010.\n\nAutomated extraction and clustering of requirements glossary terms. C Arora, M Sabetzadeh, L C Briand, F Zimmer, IEEE Trans. Software Eng. 4310C. Arora, M. Sabetzadeh, L. C. Briand, and F. Zimmer, \"Automated extraction and clustering of requirements glossary terms,\" IEEE Trans. Software Eng., vol. 43, no. 10, pp. 918-945, 2017.\n\nAutomatic glossary term extraction from large-scale requirements specifications. T Gemkow, M Conzelmann, K Hartig, A Vogelsang, 26th IEEE International Requirements Engineering Conference. Banff, AB, CanadaT. Gemkow, M. Conzelmann, K. Hartig, and A. Vogelsang, \"Automatic glossary term extraction from large-scale requirements specifications,\" in 26th IEEE International Requirements Engineering Conference, RE 2018, Banff, AB, Canada, August 20-24, 2018, 2018, pp. 412-417.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held. Lake Tahoe, Nevada, United StatesT. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Distributed representations of words and phrases and their composi- tionality,\" in Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, 2013, pp. 3111-3119.\n\nWord representations: A simple and general method for semi-supervised learning. J P Turian, L Ratinov, Y Bengio, ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. J. Hajic, S. Carberry, and S. ClarkUppsala, SwedenThe Association for Computer LinguisticsJ. P. Turian, L. Ratinov, and Y. Bengio, \"Word representations: A simple and general method for semi-supervised learning,\" in ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden, J. Hajic, S. Carberry, and S. Clark, Eds. The Association for Computer Linguistics, 2010, pp. 384-394.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, 1st International Conference on Learning Representations, ICLR 2013. Bengio and Y. LeCunScottsdale, Arizona, USAWorkshop Track ProceedingsT. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient estimation of word representations in vector space,\" in 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2013.\n\nGlove: Global vectors for word representation. J Pennington, R Socher, C D Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. A. Moschitti, B. Pang, and W. Daelemansthe 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarA meeting of SIGDAT, a Special Interest Group of the ACLJ. Pennington, R. Socher, and C. D. Manning, \"Glove: Global vectors for word representation,\" in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, A. Moschitti, B. Pang, and W. Daelemans, Eds. ACL, 2014, pp. 1532-1543.\n\nDistributional structure. Z S Harris, WORD. 102-3Z. S. Harris, \"Distributional structure,\" WORD, vol. 10, no. 2-3, pp. 146-162, 1954.\n\nA unified model for word sense representation and disambiguation. X Chen, Z Liu, M Sun, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. A. Moschitti, B. Pang, and W. Daelemansthe 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarA meeting of SIGDAT, a Special Interest Group of the ACLX. Chen, Z. Liu, and M. Sun, \"A unified model for word sense repre- sentation and disambiguation,\" in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, A. Moschitti, B. Pang, and W. Daelemans, Eds. ACL, 2014, pp. 1025-1035.\n\nNatural language processing (almost) from scratch. R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P P Kuksa, J. Mach. Learn. Res. 12R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa, \"Natural language processing (almost) from scratch,\" J. Mach. Learn. Res., vol. 12, pp. 2493-2537, 2011.\n\nThe role of context types and dimensionality in learning word embeddings. O Melamud, D Mcclosky, S Patwardhan, M Bansal, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. K. Knight, A. Nenkova, and O. RambowSan Diego California, USAThe Association for Computational LinguisticsNAACL HLT 2016O. Melamud, D. McClosky, S. Patwardhan, and M. Bansal, \"The role of context types and dimensionality in learning word embeddings,\" in NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, K. Knight, A. Nenkova, and O. Rambow, Eds. The Association for Computational Linguistics, 2016, pp. 1030-1040.\n\ncontext2vec: Learning generic context embedding with bidirectional LSTM. O Melamud, J Goldberger, I Dagan, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language LearningBerlin, GermanyO. Melamud, J. Goldberger, and I. Dagan, \"context2vec: Learning generic context embedding with bidirectional LSTM,\" in Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, 2016, pp. 51-61.\n\nDeep sentence embedding using the long short term memory network: Analysis and application to information retrieval. H Palangi, L Deng, Y Shen, J Gao, X He, J Chen, X Song, R K Ward, abs/1502.06922CoRR. H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. K. Ward, \"Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval,\" CoRR, vol. abs/1502.06922, 2015.\n\nDistributed representations of sentences and documents. Q V Le, T Mikolov, Proceedings of the 31th International Conference on Machine Learning. the 31th International Conference on Machine LearningBeijing, China32ser. JMLR Workshop and Conference ProceedingsQ. V. Le and T. Mikolov, \"Distributed representations of sentences and documents,\" in Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, ser. JMLR Workshop and Conference Proceedings, vol. 32. JMLR.org, 2014, pp. 1188-1196.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USA1J. Devlin, M. Chang, K. Lee, and K. Toutanova, \"BERT: pre-training of deep bidirectional transformers for language understanding,\" in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), 2019, pp. 4171-4186.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. GarnettLong Beach, CA, USAA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 5998- 6008.\n\nEnd-to-end neural coreference resolution. K Lee, L He, M Lewis, L Zettlemoyer, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. M. Palmer, R. Hwa, and S. Riedelthe 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsK. Lee, L. He, M. Lewis, and L. Zettlemoyer, \"End-to-end neural coreference resolution,\" in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, M. Palmer, R. Hwa, and S. Riedel, Eds. Association for Computational Linguistics, 2017, pp. 188-197.\n\nThe automatic content extraction (ACE) program -tasks, data, and evaluation. G R Doddington, A Mitchell, M A Przybocki, L A Ramshaw, S M Strassel, R M Weischedel, Proceedings of the Fourth International Conference on Language Resources and Evaluation, LREC. the Fourth International Conference on Language Resources and Evaluation, LRECLisbon, PortugalEuropean Language Resources AssociationG. R. Doddington, A. Mitchell, M. A. Przybocki, L. A. Ramshaw, S. M. Strassel, and R. M. Weischedel, \"The automatic content extraction (ACE) program -tasks, data, and evaluation,\" in Proceedings of the Fourth International Conference on Language Resources and Evalu- ation, LREC 2004, May 26-28, 2004, Lisbon, Portugal. European Language Resources Association, 2004.\n\nRevisiting joint modeling of cross-document entity and event coreference resolution. S Barhom, V Shwartz, A Eirew, M Bugert, N Reimers, I Dagan, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers, A. Korhonen, D. R. Traum, and L. M\u00e0rquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1S. Barhom, V. Shwartz, A. Eirew, M. Bugert, N. Reimers, and I. Da- gan, \"Revisiting joint modeling of cross-document entity and event coreference resolution,\" in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. M\u00e0rquez, Eds. Association for Computational Linguistics, 2019, pp. 4179-4189.\n\nA systematic review and comparative analysis of cross-document coreference resolution methods and tools. S Beheshti, B Benatallah, S Venugopal, S H Ryu, H R Motahari-Nezhad, W Wang, Computing. 994S. Beheshti, B. Benatallah, S. Venugopal, S. H. Ryu, H. R. Motahari- Nezhad, and W. Wang, \"A systematic review and comparative analysis of cross-document coreference resolution methods and tools,\" Computing, vol. 99, no. 4, pp. 313-349, 2017.\n\nJoint learning for event coreference resolution. J Lu, V Ng, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers, R. Barzilay and M. Kanthe 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1J. Lu and V. Ng, \"Joint learning for event coreference resolution,\" in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, R. Barzilay and M. Kan, Eds. Association for Computational Linguistics, 2017, pp. 90-101.\n\nSAFE: A simple approach for feature extraction from app descriptions and app reviews. T Johann, C Stanik, A M A B , W Maalej, 25th IEEE International Requirements Engineering Conference. Lisbon, PortugalT. Johann, C. Stanik, A. M. A. B., and W. Maalej, \"SAFE: A simple approach for feature extraction from app descriptions and app reviews,\" in 25th IEEE International Requirements Engineering Conference, RE 2017, Lisbon, Portugal, September 4-8, 2017, 2017, pp. 21-30.\n\nImproving word representations via global context and multiple word prototypes. E H Huang, R Socher, C D Manning, A Y Ng, The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Jeju IslandLong Papers1E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng, \"Improving word representations via global context and multiple word prototypes,\" in The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012, Jeju Island, Korea - Volume 1: Long Papers, 2012, pp. 873-882.\n\nZero-shot entity linking by reading entity descriptions. L Logeswaran, M Chang, K Lee, K Toutanova, J Devlin, H Lee, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers, A. Korhonen, D. R. Traum, and L. M\u00e0rquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1L. Logeswaran, M. Chang, K. Lee, K. Toutanova, J. Devlin, and H. Lee, \"Zero-shot entity linking by reading entity descriptions,\" in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. M\u00e0rquez, Eds. Association for Computational Linguistics, 2019, pp. 3449-3460.\n\nAnalogical reasoning on chinese morphological and semantic relations. S Li, Z Zhao, R Hu, W Li, T Liu, X Du, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsS. Li, Z. Zhao, R. Hu, W. Li, T. Liu, and X. Du, \"Analogical reasoning on chinese morphological and semantic relations,\" in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\n\nIdentification of high-level concept clones in source code. A Marcus, J I Maletic, 16th IEEE International Conference on Automated Software Engineering. San Diego, CA, USACoronado IslandA. Marcus and J. I. Maletic, \"Identification of high-level concept clones in source code,\" in 16th IEEE International Conference on Automated Software Engineering (ASE 2001), 26-29 November 2001, Coronado Island, San Diego, CA, USA, 2001, pp. 107-114.\n\n. Short Papers. 2ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, 2018, pp. 138-143.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, J. Mach. Learn. Res. 151N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: a simple way to prevent neural networks from overfitting,\" J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929-1958, 2014.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimiza- tion,\" in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nUsing latent semantic analysis to identify similarities in source code to support program understanding. J I Maletic, A Marcus, 12th IEEE International Conference on Tools with Artificial Intelligence. Vancouver, BC, CanadaICTAI 2000J. I. Maletic and A. Marcus, \"Using latent semantic analysis to identify similarities in source code to support program understanding,\" in 12th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2000), 13-15 November 2000, Vancouver, BC, Canada, 2000, pp. 46-53.\n\nRecovering documentation-to-source-code traceability links using latent semantic indexing. Proceedings of the 25th International Conference on Software Engineering. the 25th International Conference on Software EngineeringPortland, Oregon, USA--, \"Recovering documentation-to-source-code traceability links us- ing latent semantic indexing,\" in Proceedings of the 25th International Conference on Software Engineering, May 3-10, 2003, Portland, Oregon, USA, 2003, pp. 125-137.\n\nOn the role of semantics in automated requirements tracing. A Mahmoud, N Niu, Requir. Eng. 203A. Mahmoud and N. Niu, \"On the role of semantics in automated requirements tracing,\" Requir. Eng., vol. 20, no. 3, pp. 281-300, 2015.\n\nIndexing by latent semantic analysis. S C Deerwester, S T Dumais, T K Landauer, G W Furnas, R A Harshman, JASIS. 416S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman, \"Indexing by latent semantic analysis,\" JASIS, vol. 41, no. 6, pp. 391-407, 1990.\n\nEnhancing automated requirements traceability by resolving polysemy. W Wang, N Niu, H Liu, Z Niu, 26th IEEE International Requirements Engineering Conference. Banff, AB, CanadaW. Wang, N. Niu, H. Liu, and Z. Niu, \"Enhancing automated require- ments traceability by resolving polysemy,\" in 26th IEEE International Requirements Engineering Conference, RE 2018, Banff, AB, Canada, August 20-24, 2018, 2018, pp. 40-51.\n\nA study of cross-validation and bootstrap for accuracy estimation and model selection. R Kohavi, Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. the Fourteenth International Joint Conference on Artificial IntelligenceMontr\u00e9al Qu\u00e9bec, Canada95R. Kohavi, \"A study of cross-validation and bootstrap for accuracy estimation and model selection,\" in Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, IJCAI 95, Montr\u00e9al Qu\u00e9bec, Canada, August 20-25 1995, 2 Volumes, 1995, pp. 1137-1145.\n\nA survey of text similarity approaches. W Gomaa, A A Fahmy, International Journal of Computer Applications. 6813W. Gomaa and A. A. Fahmy, \"A survey of text similarity approaches,\" International Journal of Computer Applications, vol. 68, no. 13, 2013.\n\nCan clone detection support quality assessments of requirements specifications. E J\u00fcrgens, F Deissenboeck, M Feilkas, B Hummel, B Sch\u00e4tz, S Wagner, C Domann, J Streit, Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering. J. Kramer, J. Bishop, P. T. Devanbu, and S. Uchitelthe 32nd ACM/IEEE International Conference on Software EngineeringCape Town, South AfricaACM2ICSE 2010E. J\u00fcrgens, F. Deissenboeck, M. Feilkas, B. Hummel, B. Sch\u00e4tz, S. Wag- ner, C. Domann, and J. Streit, \"Can clone detection support quality assessments of requirements specifications?\" in Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering -Volume 2, ICSE 2010, Cape Town, South Africa, 1-8 May 2010, J. Kramer, J. Bishop, P. T. Devanbu, and S. Uchitel, Eds. ACM, 2010, pp. 79-88.\n\nIdentifying duplicate functionality in textual use cases by aligning semantic actions. A Rago, C A Marcos, J A Diaz-Pace, Software and Systems Modeling. 15A. Rago, C. A. Marcos, and J. A. Diaz-Pace, \"Identifying duplicate functionality in textual use cases by aligning semantic actions,\" Software and Systems Modeling, vol. 15, no. 2, pp. 579-603, 2016.\n\nBERT for coreference resolution: Baselines and analysis. M Joshi, O Levy, L Zettlemoyer, D S Weld, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. K. Inui, J. Jiang, V. Ng, and X. Wanthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsM. Joshi, O. Levy, L. Zettlemoyer, and D. S. Weld, \"BERT for coreference resolution: Baselines and analysis,\" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process- ing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3- 7, 2019, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds. Association for Computational Linguistics, 2019, pp. 5802-5807.\n\nJoint entity and event coreference resolution across documents. H Lee, M Recasens, A X Chang, M Surdeanu, D Jurafsky, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. J. Tsujii, J. Henderson, and M. Pascathe 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningJeju Island, KoreaH. Lee, M. Recasens, A. X. Chang, M. Surdeanu, and D. Jurafsky, \"Joint entity and event coreference resolution across documents,\" in Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, July 12-14, 2012, Jeju Island, Korea, J. Tsujii, J. Henderson, and M. Pasca, Eds. ACL, 2012, pp. 489-500.\n", "annotations": {"author": "[{\"end\":266,\"start\":89},{\"end\":441,\"start\":267},{\"end\":482,\"start\":442},{\"end\":487,\"start\":483},{\"end\":770,\"start\":488},{\"end\":1056,\"start\":771}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":95},{\"end\":274,\"start\":271},{\"end\":453,\"start\":451},{\"end\":497,\"start\":493},{\"end\":779,\"start\":775}]", "author_first_name": "[{\"end\":94,\"start\":89},{\"end\":270,\"start\":267},{\"end\":450,\"start\":442},{\"end\":486,\"start\":483},{\"end\":492,\"start\":488},{\"end\":774,\"start\":771}]", "author_affiliation": "[{\"end\":209,\"start\":101},{\"end\":265,\"start\":211},{\"end\":384,\"start\":276},{\"end\":440,\"start\":386},{\"end\":607,\"start\":499},{\"end\":663,\"start\":609},{\"end\":769,\"start\":665},{\"end\":907,\"start\":799},{\"end\":963,\"start\":909},{\"end\":1055,\"start\":965}]", "title": "[{\"end\":86,\"start\":1},{\"end\":1142,\"start\":1057}]", "venue": null, "abstract": "[{\"end\":2649,\"start\":1252}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2794,\"start\":2791},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2799,\"start\":2796},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2963,\"start\":2960},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2968,\"start\":2965},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3094,\"start\":3091},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4068,\"start\":4065},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4074,\"start\":4070},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4150,\"start\":4147},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4156,\"start\":4152},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4162,\"start\":4158},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4334,\"start\":4331},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4376,\"start\":4372},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4396,\"start\":4392},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4577,\"start\":4573},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4583,\"start\":4579},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9173,\"start\":9169},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9179,\"start\":9175},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9353,\"start\":9349},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9359,\"start\":9355},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9527,\"start\":9523},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9533,\"start\":9529},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9539,\"start\":9535},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9571,\"start\":9567},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9717,\"start\":9713},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9904,\"start\":9900},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10113,\"start\":10109},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10157,\"start\":10153},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10765,\"start\":10761},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10829,\"start\":10825},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10835,\"start\":10831},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10841,\"start\":10837},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11076,\"start\":11072},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11164,\"start\":11160},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11287,\"start\":11283},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11380,\"start\":11376},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11429,\"start\":11425},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12938,\"start\":12934},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12944,\"start\":12940},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13073,\"start\":13069},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13111,\"start\":13107},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13436,\"start\":13432},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13442,\"start\":13438},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13689,\"start\":13685},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13928,\"start\":13924},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14927,\"start\":14923},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14933,\"start\":14929},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14939,\"start\":14935},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17169,\"start\":17165},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17523,\"start\":17519},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17663,\"start\":17659},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18483,\"start\":18479},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18489,\"start\":18485},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18675,\"start\":18671},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19107,\"start\":19103},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20347,\"start\":20343},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20940,\"start\":20939},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21214,\"start\":21210},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24481,\"start\":24480},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27169,\"start\":27166},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27770,\"start\":27766},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28114,\"start\":28110},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28164,\"start\":28160},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28242,\"start\":28238},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":28319,\"start\":28315},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28325,\"start\":28321},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28599,\"start\":28595},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28605,\"start\":28601},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":28722,\"start\":28718},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30258,\"start\":30254},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":32779,\"start\":32775},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":40696,\"start\":40692},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40702,\"start\":40698},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":40724,\"start\":40720},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":42720,\"start\":42717},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43049,\"start\":43045},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":43606,\"start\":43602},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":43844,\"start\":43840},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":44232,\"start\":44228},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":44677,\"start\":44674},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":45118,\"start\":45115},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":45124,\"start\":45120},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":45250,\"start\":45246},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":45314,\"start\":45310},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":45714,\"start\":45710},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":46005,\"start\":46001},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":46257,\"start\":46253},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":46597,\"start\":46593},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":46617,\"start\":46613}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48531,\"start\":48411},{\"attributes\":{\"id\":\"fig_1\"},\"end\":48568,\"start\":48532},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48760,\"start\":48569},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48876,\"start\":48761},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48976,\"start\":48877},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49096,\"start\":48977},{\"attributes\":{\"id\":\"fig_6\"},\"end\":49198,\"start\":49097},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49388,\"start\":49199},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49744,\"start\":49389}]", "paragraph": "[{\"end\":3234,\"start\":2668},{\"end\":3915,\"start\":3236},{\"end\":4488,\"start\":3917},{\"end\":6228,\"start\":4490},{\"end\":7439,\"start\":6230},{\"end\":8014,\"start\":7441},{\"end\":8068,\"start\":8016},{\"end\":8368,\"start\":8070},{\"end\":8877,\"start\":8370},{\"end\":9094,\"start\":8896},{\"end\":9796,\"start\":9117},{\"end\":10445,\"start\":9798},{\"end\":11289,\"start\":10447},{\"end\":11568,\"start\":11313},{\"end\":12151,\"start\":11570},{\"end\":12666,\"start\":12153},{\"end\":13074,\"start\":12713},{\"end\":13929,\"start\":13076},{\"end\":14824,\"start\":13947},{\"end\":15178,\"start\":14850},{\"end\":15916,\"start\":15180},{\"end\":16182,\"start\":15918},{\"end\":16446,\"start\":16205},{\"end\":16972,\"start\":16491},{\"end\":17045,\"start\":16974},{\"end\":17524,\"start\":17047},{\"end\":18251,\"start\":17526},{\"end\":18490,\"start\":18295},{\"end\":19108,\"start\":18492},{\"end\":19633,\"start\":19110},{\"end\":19991,\"start\":19635},{\"end\":20101,\"start\":20052},{\"end\":20531,\"start\":20103},{\"end\":20747,\"start\":20533},{\"end\":21105,\"start\":20884},{\"end\":21524,\"start\":21107},{\"end\":22373,\"start\":21555},{\"end\":22487,\"start\":22423},{\"end\":22577,\"start\":22489},{\"end\":23142,\"start\":22579},{\"end\":23224,\"start\":23144},{\"end\":24032,\"start\":23226},{\"end\":24115,\"start\":24034},{\"end\":24369,\"start\":24117},{\"end\":25290,\"start\":24393},{\"end\":25805,\"start\":25292},{\"end\":26610,\"start\":25807},{\"end\":26994,\"start\":26612},{\"end\":27139,\"start\":27011},{\"end\":27744,\"start\":27141},{\"end\":28407,\"start\":27746},{\"end\":28656,\"start\":28409},{\"end\":29441,\"start\":28682},{\"end\":30636,\"start\":29443},{\"end\":31124,\"start\":30638},{\"end\":31699,\"start\":31158},{\"end\":32194,\"start\":31725},{\"end\":32544,\"start\":32196},{\"end\":32839,\"start\":32546},{\"end\":34433,\"start\":32867},{\"end\":34827,\"start\":34435},{\"end\":35230,\"start\":34829},{\"end\":36271,\"start\":35232},{\"end\":37179,\"start\":36273},{\"end\":37341,\"start\":37181},{\"end\":39474,\"start\":37397},{\"end\":40024,\"start\":39476},{\"end\":40496,\"start\":40045},{\"end\":40922,\"start\":40498},{\"end\":41443,\"start\":40951},{\"end\":41719,\"start\":41445},{\"end\":42333,\"start\":41721},{\"end\":42570,\"start\":42356},{\"end\":44501,\"start\":42604},{\"end\":45315,\"start\":44503},{\"end\":45572,\"start\":45317},{\"end\":46855,\"start\":45602},{\"end\":47303,\"start\":46857},{\"end\":48209,\"start\":47338},{\"end\":48410,\"start\":48211}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16204,\"start\":16183},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18294,\"start\":18252},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20051,\"start\":19992},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20863,\"start\":20748},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21554,\"start\":21525}]", "table_ref": null, "section_header": "[{\"end\":2666,\"start\":2651},{\"end\":8894,\"start\":8880},{\"end\":9115,\"start\":9097},{\"end\":11311,\"start\":11292},{\"end\":12711,\"start\":12669},{\"end\":13945,\"start\":13932},{\"end\":14848,\"start\":14827},{\"end\":16489,\"start\":16449},{\"end\":20882,\"start\":20865},{\"end\":22397,\"start\":22376},{\"end\":22421,\"start\":22400},{\"end\":24391,\"start\":24372},{\"end\":27009,\"start\":26997},{\"end\":28680,\"start\":28659},{\"end\":31156,\"start\":31127},{\"end\":31723,\"start\":31702},{\"end\":32865,\"start\":32842},{\"end\":37358,\"start\":37344},{\"end\":37395,\"start\":37361},{\"end\":40043,\"start\":40027},{\"end\":40949,\"start\":40925},{\"end\":42354,\"start\":42336},{\"end\":42602,\"start\":42573},{\"end\":45600,\"start\":45575},{\"end\":47336,\"start\":47306},{\"end\":48420,\"start\":48412},{\"end\":48541,\"start\":48533},{\"end\":48578,\"start\":48570},{\"end\":48770,\"start\":48762},{\"end\":48886,\"start\":48878},{\"end\":48986,\"start\":48978},{\"end\":49106,\"start\":49098},{\"end\":49208,\"start\":49200}]", "table": null, "figure_caption": "[{\"end\":48531,\"start\":48422},{\"end\":48568,\"start\":48543},{\"end\":48760,\"start\":48580},{\"end\":48876,\"start\":48772},{\"end\":48976,\"start\":48888},{\"end\":49096,\"start\":48988},{\"end\":49198,\"start\":49108},{\"end\":49388,\"start\":49210},{\"end\":49744,\"start\":49391}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3464,\"start\":3456},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4624,\"start\":4616},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5515,\"start\":5507},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14072,\"start\":14064},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16291,\"start\":16283},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32916,\"start\":32908},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33432,\"start\":33424},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34882,\"start\":34874},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36324,\"start\":36316},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38433,\"start\":38425},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39744,\"start\":39736}]", "bib_author_first_name": "[{\"end\":49978,\"start\":49977},{\"end\":49982,\"start\":49979},{\"end\":49990,\"start\":49989},{\"end\":50001,\"start\":50000},{\"end\":50207,\"start\":50206},{\"end\":50215,\"start\":50214},{\"end\":50225,\"start\":50224},{\"end\":50242,\"start\":50241},{\"end\":50251,\"start\":50250},{\"end\":50261,\"start\":50260},{\"end\":50746,\"start\":50745},{\"end\":50757,\"start\":50756},{\"end\":50993,\"start\":50992},{\"end\":51224,\"start\":51223},{\"end\":51236,\"start\":51235},{\"end\":51244,\"start\":51243},{\"end\":51791,\"start\":51790},{\"end\":51793,\"start\":51792},{\"end\":51806,\"start\":51805},{\"end\":51816,\"start\":51815},{\"end\":51830,\"start\":51829},{\"end\":51842,\"start\":51841},{\"end\":51851,\"start\":51850},{\"end\":51860,\"start\":51859},{\"end\":51869,\"start\":51868},{\"end\":51886,\"start\":51885},{\"end\":51895,\"start\":51894},{\"end\":51908,\"start\":51907},{\"end\":51920,\"start\":51919},{\"end\":51930,\"start\":51929},{\"end\":51938,\"start\":51937},{\"end\":51955,\"start\":51954},{\"end\":51964,\"start\":51963},{\"end\":51980,\"start\":51979},{\"end\":51988,\"start\":51987},{\"end\":52003,\"start\":52002},{\"end\":52010,\"start\":52009},{\"end\":52012,\"start\":52011},{\"end\":52023,\"start\":52022},{\"end\":52032,\"start\":52031},{\"end\":52034,\"start\":52033},{\"end\":52048,\"start\":52047},{\"end\":52050,\"start\":52049},{\"end\":52686,\"start\":52685},{\"end\":52698,\"start\":52697},{\"end\":52708,\"start\":52707},{\"end\":52717,\"start\":52716},{\"end\":53126,\"start\":53125},{\"end\":53128,\"start\":53127},{\"end\":53137,\"start\":53136},{\"end\":53421,\"start\":53420},{\"end\":53423,\"start\":53422},{\"end\":53432,\"start\":53431},{\"end\":53434,\"start\":53433},{\"end\":54134,\"start\":54133},{\"end\":54144,\"start\":54143},{\"end\":54157,\"start\":54156},{\"end\":54860,\"start\":54859},{\"end\":54872,\"start\":54871},{\"end\":54883,\"start\":54882},{\"end\":54891,\"start\":54890},{\"end\":54903,\"start\":54902},{\"end\":54912,\"start\":54911},{\"end\":54922,\"start\":54921},{\"end\":55672,\"start\":55671},{\"end\":55682,\"start\":55681},{\"end\":55692,\"start\":55691},{\"end\":56222,\"start\":56221},{\"end\":56232,\"start\":56231},{\"end\":56234,\"start\":56233},{\"end\":56247,\"start\":56246},{\"end\":56257,\"start\":56256},{\"end\":56534,\"start\":56533},{\"end\":56546,\"start\":56545},{\"end\":56558,\"start\":56557},{\"end\":56570,\"start\":56569},{\"end\":56914,\"start\":56913},{\"end\":56922,\"start\":56921},{\"end\":56934,\"start\":56933},{\"end\":56945,\"start\":56944},{\"end\":56955,\"start\":56954},{\"end\":57276,\"start\":57275},{\"end\":57286,\"start\":57285},{\"end\":57301,\"start\":57300},{\"end\":57682,\"start\":57681},{\"end\":57693,\"start\":57692},{\"end\":57704,\"start\":57703},{\"end\":58044,\"start\":58043},{\"end\":58046,\"start\":58045},{\"end\":58055,\"start\":58054},{\"end\":58068,\"start\":58067},{\"end\":58070,\"start\":58069},{\"end\":58615,\"start\":58614},{\"end\":58617,\"start\":58616},{\"end\":58628,\"start\":58627},{\"end\":58640,\"start\":58639},{\"end\":58811,\"start\":58810},{\"end\":58820,\"start\":58819},{\"end\":58834,\"start\":58833},{\"end\":58836,\"start\":58835},{\"end\":58846,\"start\":58845},{\"end\":59155,\"start\":59154},{\"end\":59165,\"start\":59164},{\"end\":59179,\"start\":59178},{\"end\":59189,\"start\":59188},{\"end\":59627,\"start\":59626},{\"end\":59638,\"start\":59637},{\"end\":59651,\"start\":59650},{\"end\":59659,\"start\":59658},{\"end\":59661,\"start\":59660},{\"end\":59672,\"start\":59671},{\"end\":60326,\"start\":60325},{\"end\":60328,\"start\":60327},{\"end\":60338,\"start\":60337},{\"end\":60349,\"start\":60348},{\"end\":60974,\"start\":60973},{\"end\":60985,\"start\":60984},{\"end\":60993,\"start\":60992},{\"end\":61004,\"start\":61003},{\"end\":61485,\"start\":61484},{\"end\":61499,\"start\":61498},{\"end\":61509,\"start\":61508},{\"end\":61511,\"start\":61510},{\"end\":62175,\"start\":62174},{\"end\":62177,\"start\":62176},{\"end\":62350,\"start\":62349},{\"end\":62358,\"start\":62357},{\"end\":62365,\"start\":62364},{\"end\":63055,\"start\":63054},{\"end\":63068,\"start\":63067},{\"end\":63078,\"start\":63077},{\"end\":63088,\"start\":63087},{\"end\":63098,\"start\":63097},{\"end\":63113,\"start\":63112},{\"end\":63115,\"start\":63114},{\"end\":63407,\"start\":63406},{\"end\":63418,\"start\":63417},{\"end\":63430,\"start\":63429},{\"end\":63444,\"start\":63443},{\"end\":64212,\"start\":64211},{\"end\":64223,\"start\":64222},{\"end\":64237,\"start\":64236},{\"end\":64805,\"start\":64804},{\"end\":64816,\"start\":64815},{\"end\":64824,\"start\":64823},{\"end\":64832,\"start\":64831},{\"end\":64839,\"start\":64838},{\"end\":64845,\"start\":64844},{\"end\":64853,\"start\":64852},{\"end\":64861,\"start\":64860},{\"end\":64863,\"start\":64862},{\"end\":65179,\"start\":65178},{\"end\":65181,\"start\":65180},{\"end\":65187,\"start\":65186},{\"end\":65752,\"start\":65751},{\"end\":65762,\"start\":65761},{\"end\":65771,\"start\":65770},{\"end\":65778,\"start\":65777},{\"end\":66530,\"start\":66529},{\"end\":66541,\"start\":66540},{\"end\":66552,\"start\":66551},{\"end\":66562,\"start\":66561},{\"end\":66575,\"start\":66574},{\"end\":66584,\"start\":66583},{\"end\":66586,\"start\":66585},{\"end\":66595,\"start\":66594},{\"end\":66605,\"start\":66604},{\"end\":67318,\"start\":67317},{\"end\":67325,\"start\":67324},{\"end\":67331,\"start\":67330},{\"end\":67340,\"start\":67339},{\"end\":68020,\"start\":68019},{\"end\":68022,\"start\":68021},{\"end\":68036,\"start\":68035},{\"end\":68048,\"start\":68047},{\"end\":68050,\"start\":68049},{\"end\":68063,\"start\":68062},{\"end\":68065,\"start\":68064},{\"end\":68076,\"start\":68075},{\"end\":68078,\"start\":68077},{\"end\":68090,\"start\":68089},{\"end\":68092,\"start\":68091},{\"end\":68787,\"start\":68786},{\"end\":68797,\"start\":68796},{\"end\":68808,\"start\":68807},{\"end\":68817,\"start\":68816},{\"end\":68827,\"start\":68826},{\"end\":68838,\"start\":68837},{\"end\":69668,\"start\":69667},{\"end\":69680,\"start\":69679},{\"end\":69694,\"start\":69693},{\"end\":69707,\"start\":69706},{\"end\":69709,\"start\":69708},{\"end\":69716,\"start\":69715},{\"end\":69718,\"start\":69717},{\"end\":69737,\"start\":69736},{\"end\":70052,\"start\":70051},{\"end\":70058,\"start\":70057},{\"end\":70728,\"start\":70727},{\"end\":70738,\"start\":70737},{\"end\":70748,\"start\":70747},{\"end\":70754,\"start\":70749},{\"end\":70758,\"start\":70757},{\"end\":71193,\"start\":71192},{\"end\":71195,\"start\":71194},{\"end\":71204,\"start\":71203},{\"end\":71214,\"start\":71213},{\"end\":71216,\"start\":71215},{\"end\":71227,\"start\":71226},{\"end\":71229,\"start\":71228},{\"end\":71744,\"start\":71743},{\"end\":71758,\"start\":71757},{\"end\":71767,\"start\":71766},{\"end\":71774,\"start\":71773},{\"end\":71787,\"start\":71786},{\"end\":71797,\"start\":71796},{\"end\":72560,\"start\":72559},{\"end\":72566,\"start\":72565},{\"end\":72574,\"start\":72573},{\"end\":72580,\"start\":72579},{\"end\":72586,\"start\":72585},{\"end\":72593,\"start\":72592},{\"end\":73034,\"start\":73033},{\"end\":73044,\"start\":73043},{\"end\":73046,\"start\":73045},{\"end\":73591,\"start\":73590},{\"end\":73605,\"start\":73604},{\"end\":73607,\"start\":73606},{\"end\":73617,\"start\":73616},{\"end\":73631,\"start\":73630},{\"end\":73644,\"start\":73643},{\"end\":73938,\"start\":73937},{\"end\":73940,\"start\":73939},{\"end\":73950,\"start\":73949},{\"end\":74381,\"start\":74380},{\"end\":74383,\"start\":74382},{\"end\":74394,\"start\":74393},{\"end\":75335,\"start\":75334},{\"end\":75346,\"start\":75345},{\"end\":75542,\"start\":75541},{\"end\":75544,\"start\":75543},{\"end\":75558,\"start\":75557},{\"end\":75560,\"start\":75559},{\"end\":75570,\"start\":75569},{\"end\":75572,\"start\":75571},{\"end\":75584,\"start\":75583},{\"end\":75586,\"start\":75585},{\"end\":75596,\"start\":75595},{\"end\":75598,\"start\":75597},{\"end\":75854,\"start\":75853},{\"end\":75862,\"start\":75861},{\"end\":75869,\"start\":75868},{\"end\":75876,\"start\":75875},{\"end\":76288,\"start\":76287},{\"end\":76803,\"start\":76802},{\"end\":76812,\"start\":76811},{\"end\":76814,\"start\":76813},{\"end\":77095,\"start\":77094},{\"end\":77106,\"start\":77105},{\"end\":77122,\"start\":77121},{\"end\":77133,\"start\":77132},{\"end\":77143,\"start\":77142},{\"end\":77153,\"start\":77152},{\"end\":77163,\"start\":77162},{\"end\":77173,\"start\":77172},{\"end\":77917,\"start\":77916},{\"end\":77925,\"start\":77924},{\"end\":77927,\"start\":77926},{\"end\":77937,\"start\":77936},{\"end\":77939,\"start\":77938},{\"end\":78242,\"start\":78241},{\"end\":78251,\"start\":78250},{\"end\":78259,\"start\":78258},{\"end\":78274,\"start\":78273},{\"end\":78276,\"start\":78275},{\"end\":79191,\"start\":79190},{\"end\":79198,\"start\":79197},{\"end\":79210,\"start\":79209},{\"end\":79212,\"start\":79211},{\"end\":79221,\"start\":79220},{\"end\":79233,\"start\":79232}]", "bib_author_last_name": "[{\"end\":49987,\"start\":49983},{\"end\":49998,\"start\":49991},{\"end\":50006,\"start\":50002},{\"end\":50212,\"start\":50208},{\"end\":50222,\"start\":50216},{\"end\":50239,\"start\":50226},{\"end\":50248,\"start\":50243},{\"end\":50258,\"start\":50252},{\"end\":50267,\"start\":50262},{\"end\":50754,\"start\":50747},{\"end\":50763,\"start\":50758},{\"end\":51007,\"start\":50994},{\"end\":51233,\"start\":51225},{\"end\":51241,\"start\":51237},{\"end\":51250,\"start\":51245},{\"end\":51803,\"start\":51794},{\"end\":51813,\"start\":51807},{\"end\":51827,\"start\":51817},{\"end\":51839,\"start\":51831},{\"end\":51848,\"start\":51843},{\"end\":51857,\"start\":51852},{\"end\":51866,\"start\":51861},{\"end\":51883,\"start\":51870},{\"end\":51892,\"start\":51887},{\"end\":51905,\"start\":51896},{\"end\":51917,\"start\":51909},{\"end\":51927,\"start\":51921},{\"end\":51935,\"start\":51931},{\"end\":51952,\"start\":51939},{\"end\":51961,\"start\":51956},{\"end\":51977,\"start\":51965},{\"end\":51985,\"start\":51981},{\"end\":52000,\"start\":51989},{\"end\":52007,\"start\":52004},{\"end\":52020,\"start\":52013},{\"end\":52029,\"start\":52024},{\"end\":52045,\"start\":52035},{\"end\":52059,\"start\":52051},{\"end\":52695,\"start\":52687},{\"end\":52705,\"start\":52699},{\"end\":52714,\"start\":52709},{\"end\":52722,\"start\":52718},{\"end\":53134,\"start\":53129},{\"end\":53146,\"start\":53138},{\"end\":53429,\"start\":53424},{\"end\":53440,\"start\":53435},{\"end\":54141,\"start\":54135},{\"end\":54154,\"start\":54145},{\"end\":54161,\"start\":54158},{\"end\":54869,\"start\":54861},{\"end\":54880,\"start\":54873},{\"end\":54888,\"start\":54884},{\"end\":54900,\"start\":54892},{\"end\":54909,\"start\":54904},{\"end\":54919,\"start\":54913},{\"end\":54932,\"start\":54923},{\"end\":55679,\"start\":55673},{\"end\":55689,\"start\":55683},{\"end\":55698,\"start\":55693},{\"end\":56229,\"start\":56223},{\"end\":56244,\"start\":56235},{\"end\":56254,\"start\":56248},{\"end\":56262,\"start\":56258},{\"end\":56543,\"start\":56535},{\"end\":56555,\"start\":56547},{\"end\":56567,\"start\":56559},{\"end\":56577,\"start\":56571},{\"end\":56919,\"start\":56915},{\"end\":56931,\"start\":56923},{\"end\":56942,\"start\":56935},{\"end\":56952,\"start\":56946},{\"end\":56964,\"start\":56956},{\"end\":57283,\"start\":57277},{\"end\":57298,\"start\":57287},{\"end\":57314,\"start\":57302},{\"end\":57690,\"start\":57683},{\"end\":57701,\"start\":57694},{\"end\":57712,\"start\":57705},{\"end\":58052,\"start\":58047},{\"end\":58065,\"start\":58056},{\"end\":58079,\"start\":58071},{\"end\":58625,\"start\":58618},{\"end\":58637,\"start\":58629},{\"end\":58648,\"start\":58641},{\"end\":58817,\"start\":58812},{\"end\":58831,\"start\":58821},{\"end\":58843,\"start\":58837},{\"end\":58853,\"start\":58847},{\"end\":59162,\"start\":59156},{\"end\":59176,\"start\":59166},{\"end\":59186,\"start\":59180},{\"end\":59199,\"start\":59190},{\"end\":59635,\"start\":59628},{\"end\":59648,\"start\":59639},{\"end\":59656,\"start\":59652},{\"end\":59669,\"start\":59662},{\"end\":59677,\"start\":59673},{\"end\":60335,\"start\":60329},{\"end\":60346,\"start\":60339},{\"end\":60356,\"start\":60350},{\"end\":60982,\"start\":60975},{\"end\":60990,\"start\":60986},{\"end\":61001,\"start\":60994},{\"end\":61009,\"start\":61005},{\"end\":61496,\"start\":61486},{\"end\":61506,\"start\":61500},{\"end\":61519,\"start\":61512},{\"end\":62184,\"start\":62178},{\"end\":62355,\"start\":62351},{\"end\":62362,\"start\":62359},{\"end\":62369,\"start\":62366},{\"end\":63065,\"start\":63056},{\"end\":63075,\"start\":63069},{\"end\":63085,\"start\":63079},{\"end\":63095,\"start\":63089},{\"end\":63110,\"start\":63099},{\"end\":63121,\"start\":63116},{\"end\":63415,\"start\":63408},{\"end\":63427,\"start\":63419},{\"end\":63441,\"start\":63431},{\"end\":63451,\"start\":63445},{\"end\":64220,\"start\":64213},{\"end\":64234,\"start\":64224},{\"end\":64243,\"start\":64238},{\"end\":64813,\"start\":64806},{\"end\":64821,\"start\":64817},{\"end\":64829,\"start\":64825},{\"end\":64836,\"start\":64833},{\"end\":64842,\"start\":64840},{\"end\":64850,\"start\":64846},{\"end\":64858,\"start\":64854},{\"end\":64868,\"start\":64864},{\"end\":65184,\"start\":65182},{\"end\":65195,\"start\":65188},{\"end\":65759,\"start\":65753},{\"end\":65768,\"start\":65763},{\"end\":65775,\"start\":65772},{\"end\":65788,\"start\":65779},{\"end\":66538,\"start\":66531},{\"end\":66549,\"start\":66542},{\"end\":66559,\"start\":66553},{\"end\":66572,\"start\":66563},{\"end\":66581,\"start\":66576},{\"end\":66592,\"start\":66587},{\"end\":66602,\"start\":66596},{\"end\":66616,\"start\":66606},{\"end\":67322,\"start\":67319},{\"end\":67328,\"start\":67326},{\"end\":67337,\"start\":67332},{\"end\":67352,\"start\":67341},{\"end\":68033,\"start\":68023},{\"end\":68045,\"start\":68037},{\"end\":68060,\"start\":68051},{\"end\":68073,\"start\":68066},{\"end\":68087,\"start\":68079},{\"end\":68103,\"start\":68093},{\"end\":68794,\"start\":68788},{\"end\":68805,\"start\":68798},{\"end\":68814,\"start\":68809},{\"end\":68824,\"start\":68818},{\"end\":68835,\"start\":68828},{\"end\":68844,\"start\":68839},{\"end\":69677,\"start\":69669},{\"end\":69691,\"start\":69681},{\"end\":69704,\"start\":69695},{\"end\":69713,\"start\":69710},{\"end\":69734,\"start\":69719},{\"end\":69742,\"start\":69738},{\"end\":70055,\"start\":70053},{\"end\":70061,\"start\":70059},{\"end\":70735,\"start\":70729},{\"end\":70745,\"start\":70739},{\"end\":70765,\"start\":70759},{\"end\":71201,\"start\":71196},{\"end\":71211,\"start\":71205},{\"end\":71224,\"start\":71217},{\"end\":71232,\"start\":71230},{\"end\":71755,\"start\":71745},{\"end\":71764,\"start\":71759},{\"end\":71771,\"start\":71768},{\"end\":71784,\"start\":71775},{\"end\":71794,\"start\":71788},{\"end\":71801,\"start\":71798},{\"end\":72563,\"start\":72561},{\"end\":72571,\"start\":72567},{\"end\":72577,\"start\":72575},{\"end\":72583,\"start\":72581},{\"end\":72590,\"start\":72587},{\"end\":72596,\"start\":72594},{\"end\":73041,\"start\":73035},{\"end\":73054,\"start\":73047},{\"end\":73602,\"start\":73592},{\"end\":73614,\"start\":73608},{\"end\":73628,\"start\":73618},{\"end\":73641,\"start\":73632},{\"end\":73658,\"start\":73645},{\"end\":73947,\"start\":73941},{\"end\":73953,\"start\":73951},{\"end\":74391,\"start\":74384},{\"end\":74401,\"start\":74395},{\"end\":75343,\"start\":75336},{\"end\":75350,\"start\":75347},{\"end\":75555,\"start\":75545},{\"end\":75567,\"start\":75561},{\"end\":75581,\"start\":75573},{\"end\":75593,\"start\":75587},{\"end\":75607,\"start\":75599},{\"end\":75859,\"start\":75855},{\"end\":75866,\"start\":75863},{\"end\":75873,\"start\":75870},{\"end\":75880,\"start\":75877},{\"end\":76295,\"start\":76289},{\"end\":76809,\"start\":76804},{\"end\":76820,\"start\":76815},{\"end\":77103,\"start\":77096},{\"end\":77119,\"start\":77107},{\"end\":77130,\"start\":77123},{\"end\":77140,\"start\":77134},{\"end\":77150,\"start\":77144},{\"end\":77160,\"start\":77154},{\"end\":77170,\"start\":77164},{\"end\":77180,\"start\":77174},{\"end\":77922,\"start\":77918},{\"end\":77934,\"start\":77928},{\"end\":77949,\"start\":77940},{\"end\":78248,\"start\":78243},{\"end\":78256,\"start\":78252},{\"end\":78271,\"start\":78260},{\"end\":78281,\"start\":78277},{\"end\":79195,\"start\":79192},{\"end\":79207,\"start\":79199},{\"end\":79218,\"start\":79213},{\"end\":79230,\"start\":79222},{\"end\":79242,\"start\":79234}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":50136,\"start\":49977},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":20897558},\"end\":50661,\"start\":50138},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":186207291},\"end\":50950,\"start\":50663},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":34241423},\"end\":51128,\"start\":50952},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52983579},\"end\":51687,\"start\":51130},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3794890},\"end\":52571,\"start\":51689},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":62262576},\"end\":53061,\"start\":52573},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":31707842},\"end\":53306,\"start\":53063},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5031395},\"end\":54065,\"start\":53308},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1569926},\"end\":54767,\"start\":54067},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":28898017},\"end\":55604,\"start\":54769},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7332832},\"end\":56169,\"start\":55606},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9602750},\"end\":56465,\"start\":56171},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13575966},\"end\":56847,\"start\":56467},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15676954},\"end\":57158,\"start\":56849},{\"attributes\":{\"id\":\"b15\"},\"end\":57547,\"start\":57160},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6446687},\"end\":57976,\"start\":57549},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10625463},\"end\":58573,\"start\":57978},{\"attributes\":{\"id\":\"b18\"},\"end\":58740,\"start\":58575},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":24746895},\"end\":59071,\"start\":58742},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52986064},\"end\":59547,\"start\":59073},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16447573},\"end\":60243,\"start\":59549},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":629094},\"end\":60909,\"start\":60245},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5959482},\"end\":61435,\"start\":60911},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1957433},\"end\":62146,\"start\":61437},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":86680084},\"end\":62281,\"start\":62148},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2434362},\"end\":63001,\"start\":62283},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":351666},\"end\":63330,\"start\":63003},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5527031},\"end\":64136,\"start\":63332},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7890036},\"end\":64685,\"start\":64138},{\"attributes\":{\"doi\":\"abs/1502.06922\",\"id\":\"b30\",\"matched_paper_id\":3337266},\"end\":65120,\"start\":64687},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2407601},\"end\":65667,\"start\":65122},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52967399},\"end\":66500,\"start\":65669},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13756489},\"end\":67273,\"start\":66502},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1222212},\"end\":67940,\"start\":67275},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9776219},\"end\":68699,\"start\":67942},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":174799117},\"end\":69560,\"start\":68701},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":8129251},\"end\":70000,\"start\":69562},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":28250703},\"end\":70639,\"start\":70002},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4703509},\"end\":71110,\"start\":70641},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":372093},\"end\":71684,\"start\":71112},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":189999659},\"end\":72487,\"start\":71686},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":21716001},\"end\":72971,\"start\":72489},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16315378},\"end\":73410,\"start\":72973},{\"attributes\":{\"id\":\"b44\"},\"end\":73521,\"start\":73412},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":6844431},\"end\":73891,\"start\":73523},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":6628106},\"end\":74273,\"start\":73893},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":10354564},\"end\":74794,\"start\":74275},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7730498},\"end\":75272,\"start\":74796},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":9008961},\"end\":75501,\"start\":75274},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":3252915},\"end\":75782,\"start\":75503},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52985876},\"end\":76198,\"start\":75784},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2702042},\"end\":76760,\"start\":76200},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2703920},\"end\":77012,\"start\":76762},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":9588476},\"end\":77827,\"start\":77014},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":15770855},\"end\":78182,\"start\":77829},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":201646551},\"end\":79124,\"start\":78184},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":2294115},\"end\":79952,\"start\":79126}]", "bib_title": "[{\"end\":50204,\"start\":50138},{\"end\":50743,\"start\":50663},{\"end\":50990,\"start\":50952},{\"end\":51221,\"start\":51130},{\"end\":51788,\"start\":51689},{\"end\":52683,\"start\":52573},{\"end\":53123,\"start\":53063},{\"end\":53418,\"start\":53308},{\"end\":54131,\"start\":54067},{\"end\":54857,\"start\":54769},{\"end\":55669,\"start\":55606},{\"end\":56219,\"start\":56171},{\"end\":56531,\"start\":56467},{\"end\":56911,\"start\":56849},{\"end\":57273,\"start\":57160},{\"end\":57679,\"start\":57549},{\"end\":58041,\"start\":57978},{\"end\":58808,\"start\":58742},{\"end\":59152,\"start\":59073},{\"end\":59624,\"start\":59549},{\"end\":60323,\"start\":60245},{\"end\":60971,\"start\":60911},{\"end\":61482,\"start\":61437},{\"end\":62172,\"start\":62148},{\"end\":62347,\"start\":62283},{\"end\":63052,\"start\":63003},{\"end\":63404,\"start\":63332},{\"end\":64209,\"start\":64138},{\"end\":64802,\"start\":64687},{\"end\":65176,\"start\":65122},{\"end\":65749,\"start\":65669},{\"end\":66527,\"start\":66502},{\"end\":67315,\"start\":67275},{\"end\":68017,\"start\":67942},{\"end\":68784,\"start\":68701},{\"end\":69665,\"start\":69562},{\"end\":70049,\"start\":70002},{\"end\":70725,\"start\":70641},{\"end\":71190,\"start\":71112},{\"end\":71741,\"start\":71686},{\"end\":72557,\"start\":72489},{\"end\":73031,\"start\":72973},{\"end\":73588,\"start\":73523},{\"end\":73935,\"start\":73893},{\"end\":74378,\"start\":74275},{\"end\":74885,\"start\":74796},{\"end\":75332,\"start\":75274},{\"end\":75539,\"start\":75503},{\"end\":75851,\"start\":75784},{\"end\":76285,\"start\":76200},{\"end\":76800,\"start\":76762},{\"end\":77092,\"start\":77014},{\"end\":77914,\"start\":77829},{\"end\":78239,\"start\":78184},{\"end\":79188,\"start\":79126}]", "bib_author": "[{\"end\":49989,\"start\":49977},{\"end\":50000,\"start\":49989},{\"end\":50008,\"start\":50000},{\"end\":50214,\"start\":50206},{\"end\":50224,\"start\":50214},{\"end\":50241,\"start\":50224},{\"end\":50250,\"start\":50241},{\"end\":50260,\"start\":50250},{\"end\":50269,\"start\":50260},{\"end\":50756,\"start\":50745},{\"end\":50765,\"start\":50756},{\"end\":51009,\"start\":50992},{\"end\":51235,\"start\":51223},{\"end\":51243,\"start\":51235},{\"end\":51252,\"start\":51243},{\"end\":51805,\"start\":51790},{\"end\":51815,\"start\":51805},{\"end\":51829,\"start\":51815},{\"end\":51841,\"start\":51829},{\"end\":51850,\"start\":51841},{\"end\":51859,\"start\":51850},{\"end\":51868,\"start\":51859},{\"end\":51885,\"start\":51868},{\"end\":51894,\"start\":51885},{\"end\":51907,\"start\":51894},{\"end\":51919,\"start\":51907},{\"end\":51929,\"start\":51919},{\"end\":51937,\"start\":51929},{\"end\":51954,\"start\":51937},{\"end\":51963,\"start\":51954},{\"end\":51979,\"start\":51963},{\"end\":51987,\"start\":51979},{\"end\":52002,\"start\":51987},{\"end\":52009,\"start\":52002},{\"end\":52022,\"start\":52009},{\"end\":52031,\"start\":52022},{\"end\":52047,\"start\":52031},{\"end\":52061,\"start\":52047},{\"end\":52697,\"start\":52685},{\"end\":52707,\"start\":52697},{\"end\":52716,\"start\":52707},{\"end\":52724,\"start\":52716},{\"end\":53136,\"start\":53125},{\"end\":53148,\"start\":53136},{\"end\":53431,\"start\":53420},{\"end\":53442,\"start\":53431},{\"end\":54143,\"start\":54133},{\"end\":54156,\"start\":54143},{\"end\":54163,\"start\":54156},{\"end\":54871,\"start\":54859},{\"end\":54882,\"start\":54871},{\"end\":54890,\"start\":54882},{\"end\":54902,\"start\":54890},{\"end\":54911,\"start\":54902},{\"end\":54921,\"start\":54911},{\"end\":54934,\"start\":54921},{\"end\":55681,\"start\":55671},{\"end\":55691,\"start\":55681},{\"end\":55700,\"start\":55691},{\"end\":56231,\"start\":56221},{\"end\":56246,\"start\":56231},{\"end\":56256,\"start\":56246},{\"end\":56264,\"start\":56256},{\"end\":56545,\"start\":56533},{\"end\":56557,\"start\":56545},{\"end\":56569,\"start\":56557},{\"end\":56579,\"start\":56569},{\"end\":56921,\"start\":56913},{\"end\":56933,\"start\":56921},{\"end\":56944,\"start\":56933},{\"end\":56954,\"start\":56944},{\"end\":56966,\"start\":56954},{\"end\":57285,\"start\":57275},{\"end\":57300,\"start\":57285},{\"end\":57316,\"start\":57300},{\"end\":57692,\"start\":57681},{\"end\":57703,\"start\":57692},{\"end\":57714,\"start\":57703},{\"end\":58054,\"start\":58043},{\"end\":58067,\"start\":58054},{\"end\":58081,\"start\":58067},{\"end\":58627,\"start\":58614},{\"end\":58639,\"start\":58627},{\"end\":58650,\"start\":58639},{\"end\":58819,\"start\":58810},{\"end\":58833,\"start\":58819},{\"end\":58845,\"start\":58833},{\"end\":58855,\"start\":58845},{\"end\":59164,\"start\":59154},{\"end\":59178,\"start\":59164},{\"end\":59188,\"start\":59178},{\"end\":59201,\"start\":59188},{\"end\":59637,\"start\":59626},{\"end\":59650,\"start\":59637},{\"end\":59658,\"start\":59650},{\"end\":59671,\"start\":59658},{\"end\":59679,\"start\":59671},{\"end\":60337,\"start\":60325},{\"end\":60348,\"start\":60337},{\"end\":60358,\"start\":60348},{\"end\":60984,\"start\":60973},{\"end\":60992,\"start\":60984},{\"end\":61003,\"start\":60992},{\"end\":61011,\"start\":61003},{\"end\":61498,\"start\":61484},{\"end\":61508,\"start\":61498},{\"end\":61521,\"start\":61508},{\"end\":62186,\"start\":62174},{\"end\":62357,\"start\":62349},{\"end\":62364,\"start\":62357},{\"end\":62371,\"start\":62364},{\"end\":63067,\"start\":63054},{\"end\":63077,\"start\":63067},{\"end\":63087,\"start\":63077},{\"end\":63097,\"start\":63087},{\"end\":63112,\"start\":63097},{\"end\":63123,\"start\":63112},{\"end\":63417,\"start\":63406},{\"end\":63429,\"start\":63417},{\"end\":63443,\"start\":63429},{\"end\":63453,\"start\":63443},{\"end\":64222,\"start\":64211},{\"end\":64236,\"start\":64222},{\"end\":64245,\"start\":64236},{\"end\":64815,\"start\":64804},{\"end\":64823,\"start\":64815},{\"end\":64831,\"start\":64823},{\"end\":64838,\"start\":64831},{\"end\":64844,\"start\":64838},{\"end\":64852,\"start\":64844},{\"end\":64860,\"start\":64852},{\"end\":64870,\"start\":64860},{\"end\":65186,\"start\":65178},{\"end\":65197,\"start\":65186},{\"end\":65761,\"start\":65751},{\"end\":65770,\"start\":65761},{\"end\":65777,\"start\":65770},{\"end\":65790,\"start\":65777},{\"end\":66540,\"start\":66529},{\"end\":66551,\"start\":66540},{\"end\":66561,\"start\":66551},{\"end\":66574,\"start\":66561},{\"end\":66583,\"start\":66574},{\"end\":66594,\"start\":66583},{\"end\":66604,\"start\":66594},{\"end\":66618,\"start\":66604},{\"end\":67324,\"start\":67317},{\"end\":67330,\"start\":67324},{\"end\":67339,\"start\":67330},{\"end\":67354,\"start\":67339},{\"end\":68035,\"start\":68019},{\"end\":68047,\"start\":68035},{\"end\":68062,\"start\":68047},{\"end\":68075,\"start\":68062},{\"end\":68089,\"start\":68075},{\"end\":68105,\"start\":68089},{\"end\":68796,\"start\":68786},{\"end\":68807,\"start\":68796},{\"end\":68816,\"start\":68807},{\"end\":68826,\"start\":68816},{\"end\":68837,\"start\":68826},{\"end\":68846,\"start\":68837},{\"end\":69679,\"start\":69667},{\"end\":69693,\"start\":69679},{\"end\":69706,\"start\":69693},{\"end\":69715,\"start\":69706},{\"end\":69736,\"start\":69715},{\"end\":69744,\"start\":69736},{\"end\":70057,\"start\":70051},{\"end\":70063,\"start\":70057},{\"end\":70737,\"start\":70727},{\"end\":70747,\"start\":70737},{\"end\":70757,\"start\":70747},{\"end\":70767,\"start\":70757},{\"end\":71203,\"start\":71192},{\"end\":71213,\"start\":71203},{\"end\":71226,\"start\":71213},{\"end\":71234,\"start\":71226},{\"end\":71757,\"start\":71743},{\"end\":71766,\"start\":71757},{\"end\":71773,\"start\":71766},{\"end\":71786,\"start\":71773},{\"end\":71796,\"start\":71786},{\"end\":71803,\"start\":71796},{\"end\":72565,\"start\":72559},{\"end\":72573,\"start\":72565},{\"end\":72579,\"start\":72573},{\"end\":72585,\"start\":72579},{\"end\":72592,\"start\":72585},{\"end\":72598,\"start\":72592},{\"end\":73043,\"start\":73033},{\"end\":73056,\"start\":73043},{\"end\":73604,\"start\":73590},{\"end\":73616,\"start\":73604},{\"end\":73630,\"start\":73616},{\"end\":73643,\"start\":73630},{\"end\":73660,\"start\":73643},{\"end\":73949,\"start\":73937},{\"end\":73955,\"start\":73949},{\"end\":74393,\"start\":74380},{\"end\":74403,\"start\":74393},{\"end\":75345,\"start\":75334},{\"end\":75352,\"start\":75345},{\"end\":75557,\"start\":75541},{\"end\":75569,\"start\":75557},{\"end\":75583,\"start\":75569},{\"end\":75595,\"start\":75583},{\"end\":75609,\"start\":75595},{\"end\":75861,\"start\":75853},{\"end\":75868,\"start\":75861},{\"end\":75875,\"start\":75868},{\"end\":75882,\"start\":75875},{\"end\":76297,\"start\":76287},{\"end\":76811,\"start\":76802},{\"end\":76822,\"start\":76811},{\"end\":77105,\"start\":77094},{\"end\":77121,\"start\":77105},{\"end\":77132,\"start\":77121},{\"end\":77142,\"start\":77132},{\"end\":77152,\"start\":77142},{\"end\":77162,\"start\":77152},{\"end\":77172,\"start\":77162},{\"end\":77182,\"start\":77172},{\"end\":77924,\"start\":77916},{\"end\":77936,\"start\":77924},{\"end\":77951,\"start\":77936},{\"end\":78250,\"start\":78241},{\"end\":78258,\"start\":78250},{\"end\":78273,\"start\":78258},{\"end\":78283,\"start\":78273},{\"end\":79197,\"start\":79190},{\"end\":79209,\"start\":79197},{\"end\":79220,\"start\":79209},{\"end\":79232,\"start\":79220},{\"end\":79244,\"start\":79232}]", "bib_venue": "[{\"end\":50032,\"start\":50008},{\"end\":50328,\"start\":50269},{\"end\":50782,\"start\":50765},{\"end\":51022,\"start\":51009},{\"end\":51311,\"start\":51252},{\"end\":52091,\"start\":52061},{\"end\":52761,\"start\":52724},{\"end\":53161,\"start\":53148},{\"end\":53550,\"start\":53442},{\"end\":54271,\"start\":54163},{\"end\":55030,\"start\":54934},{\"end\":55797,\"start\":55700},{\"end\":56295,\"start\":56264},{\"end\":56646,\"start\":56579},{\"end\":56977,\"start\":56966},{\"end\":57327,\"start\":57316},{\"end\":57738,\"start\":57714},{\"end\":58162,\"start\":58081},{\"end\":58612,\"start\":58575},{\"end\":58879,\"start\":58855},{\"end\":59260,\"start\":59201},{\"end\":59832,\"start\":59679},{\"end\":60455,\"start\":60358},{\"end\":61078,\"start\":61011},{\"end\":61607,\"start\":61521},{\"end\":62190,\"start\":62186},{\"end\":62457,\"start\":62371},{\"end\":63142,\"start\":63123},{\"end\":63580,\"start\":63453},{\"end\":64329,\"start\":64245},{\"end\":64888,\"start\":64884},{\"end\":65265,\"start\":65197},{\"end\":65948,\"start\":65790},{\"end\":66730,\"start\":66618},{\"end\":67440,\"start\":67354},{\"end\":68198,\"start\":68105},{\"end\":68939,\"start\":68846},{\"end\":69753,\"start\":69744},{\"end\":70150,\"start\":70063},{\"end\":70826,\"start\":70767},{\"end\":71337,\"start\":71234},{\"end\":71896,\"start\":71803},{\"end\":72685,\"start\":72598},{\"end\":73124,\"start\":73056},{\"end\":73426,\"start\":73414},{\"end\":73679,\"start\":73660},{\"end\":74011,\"start\":73955},{\"end\":74475,\"start\":74403},{\"end\":74959,\"start\":74887},{\"end\":75363,\"start\":75352},{\"end\":75614,\"start\":75609},{\"end\":75941,\"start\":75882},{\"end\":76384,\"start\":76297},{\"end\":76868,\"start\":76822},{\"end\":77263,\"start\":77182},{\"end\":77980,\"start\":77951},{\"end\":78443,\"start\":78283},{\"end\":79380,\"start\":79244},{\"end\":50040,\"start\":50034},{\"end\":50344,\"start\":50330},{\"end\":51362,\"start\":51345},{\"end\":52787,\"start\":52763},{\"end\":53590,\"start\":53576},{\"end\":54316,\"start\":54302},{\"end\":55073,\"start\":55059},{\"end\":55849,\"start\":55836},{\"end\":58279,\"start\":58197},{\"end\":59279,\"start\":59262},{\"end\":59867,\"start\":59834},{\"end\":60507,\"start\":60492},{\"end\":61123,\"start\":61099},{\"end\":61730,\"start\":61648},{\"end\":62580,\"start\":62498},{\"end\":63643,\"start\":63618},{\"end\":64415,\"start\":64331},{\"end\":65334,\"start\":65267},{\"end\":66113,\"start\":65950},{\"end\":66851,\"start\":66832},{\"end\":67564,\"start\":67474},{\"end\":68294,\"start\":68200},{\"end\":69087,\"start\":68994},{\"end\":70276,\"start\":70187},{\"end\":70844,\"start\":70828},{\"end\":71350,\"start\":71339},{\"end\":72044,\"start\":71951},{\"end\":72759,\"start\":72687},{\"end\":73144,\"start\":73126},{\"end\":74031,\"start\":74013},{\"end\":74498,\"start\":74477},{\"end\":75039,\"start\":74961},{\"end\":75960,\"start\":75943},{\"end\":76481,\"start\":76386},{\"end\":77405,\"start\":77316},{\"end\":78642,\"start\":78481},{\"end\":79558,\"start\":79419}]"}}}, "year": 2023, "month": 12, "day": 17}