{"id": 39847715, "updated": "2023-09-29 13:26:24.861", "metadata": {"title": "Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding", "authors": "[{\"first\":\"Jiaxi\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Ke\",\"last\":\"Wang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining", "publication_date": {"year": 2018, "month": 9, "day": 19}, "abstract": "Top-$N$ sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-$N$ ranked items that a user will likely interact in a `near future'. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. In this paper, we propose a Convolutional Sequence Embedding Recommendation Model (\\emph{Caser}) as a solution to address this requirement. The idea is to embed a sequence of recent items into an `image' in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. The experiments on public datasets demonstrated that Caser consistently outperforms state-of-the-art sequential recommendation methods on a variety of common evaluation metrics.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1809.07426", "mag": "2949517518", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1809-07426", "doi": "10.1145/3159652.3159656"}}, "content": {"source": {"pdf_hash": "8403871024d49a01b0887397baa3fab3a12ca98a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1809.07426v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1809.07426", "status": "GREEN"}}, "grobid": {"id": "b112fab9256f0e197e1517093b7d0ffc90e36649", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8403871024d49a01b0887397baa3fab3a12ca98a.txt", "contents": "\nPersonalized Top-N Sequential Recommen-dation via Convolutional Sequence Embedding\n2018. February 5-9. 2018\n\nJiaxi Tang jiaxit@sfu.ca \nSchool of Computing Science\nSchool of Computing Science\nSimon Fraser University British Columbia\nCanada\n\nKe Wang wangk@cs.sfu.ca \nSimon Fraser University British Columbia\nCanada\n\nPersonalized Top-N Sequential Recommen-dation via Convolutional Sequence Embedding\n\nProceedings of Eleventh ACM International Conference on Web Search and Data Mining\nEleventh ACM International Conference on Web Search and Data MiningMarina Del Rey, CA, USA92018. February 5-9. 201810.1145/3159652.3159656CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and rankingKEYWORDS Recommender SystemSequential PredictionConvolutional Neural Networks\nTop-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a \"near future\". The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. In this paper, we propose a Convolutional Sequence Embedding Recommendation Model (Caser) as a solution to address this requirement. The idea is to embed a sequence of recent items into an \"image\" in the time and latent spaces and learn sequential patterns as local features of the image using convolutional lters. This approach provides a uni ed and exible network structure for capturing both general preferences and sequential patterns. The experiments on public data sets demonstrated that Caser consistently outperforms state-of-the-art sequential recommendation methods on a variety of common evaluation metrics.\n\nINTRODUCTION\n\nRecommender systems have become a core technology in many applications. Most systems, e.g., top-N recommendation [9] [19], recommend the items based on the user's general preferences without paying attention to the recency of items.\n\nFor example, some user always prefer Apple's products to Samsung's products. General preferences represent user's long term Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. and static behaviors. Another type of user behaviors is sequential patterns where the next item or action more likely depends on the items or actions the user engaged recently. Sequential patterns represent the user's short term and dynamic behaviors and come from a certain relationship between the items within a close proximity of time. For example, a user likely buys phone accessories soon after buying an iPhone, though in general the user does not buy phone accessories. In this case, the systems that consider only general preferences will miss the opportunity of recommending phone accessories after selling an iPhone since buying phone accessories is not a long term user behavior.\n\n\nTop-N Sequential Recommendation\n\nTo model user's sequential patterns, the work in [17,21] considers top-N sequential recommendation that recommends N items that a user likely interacts with in a near future. This problem assumes a set of users U = {u 1 , u 2 , \u00b7 \u00b7 \u00b7 , u |U | } and a universe of items I = {i 1 , i 2 , \u00b7 \u00b7 \u00b7 , i |I | }. Each user u is associated with a sequence of some items from I, S u = (S u 1 , \u00b7 \u00b7 \u00b7 , S u |S u | ), where S u i \u2208 I. The index t for S u t denotes the order in which an action occurs in the sequence S u , not the absolute timestamp as in temporal recommendation like [14,31,34]. Given all users' sequences S u , the goal is to recommend each user a list of items that maximize her/his future needs, by considering both general preferences and sequential patterns. Unlike conventional top-N recommendation, top-N sequential recommendation models the user behavior as a sequence of items, instead of a set of items.\n\n\nLimitations of Previous Work\n\nThe Markov chain based model [2,6,21,30] is an early approach to top-N sequential recommendation, where an L-order Markov chain makes recommendations based on L previous actions. The rstorder Markov chain is an item-to-item transition matrix learnt using maximum likelihood estimation. Factorized personalized Markov chains (FPMC) [21] proposed by Rendle et al. and its variant [2] improved this method by factorizing this transition matrix into two latent and low-rank sub-matrices. Factorized Sequential Prediction with Item Similarity ModeLs (Fossil) [6] proposed by He et al. generalizes this method to high-order Markov chains using a weighted sum aggregation over previous items' latent representations. However, existing approaches su ered from two major limitations: Fail to model union-Level sequential patterns. As shown in Figure 1a, the Markov chain models only point-level sequential patterns where each of the previous actions (blue) in uences the target action (yellow) individually, instead of collectively. FPMC and Fossil fall into this taxonomy. Although Fossil [6] considers a high-order Markov chain, the overall in uence is a weighted sum of previous items' latent representations factorized from rstorder Markov transition matrices. Such aggregation of point-level in uences is not su cient to model the union-level in uences shown in Figure 1b where several previous actions, in that order, jointly in uence the target action. For example, buying both milk and butter together leads to a higher probability of buying our than buying milk or butter individually; buying both RAM and Hard Drive is a better indication of buying Operating System next than buying only one of the components. Fail to allow skip behaviors. Existing models don't consider skip behaviors of sequential patterns as shown in Figure 1c, where the impact from past behaviors may skip a few steps and still have strength. For example, a tourist has check-ins sequentially at airport, hotel, restaurant, bar, and attraction. While the check-ins at the airport and hotel do not immediately precede the check-in of the attraction, they are strongly associated with the latter. On the other hand, the check-in at the restaurant or bar has little in uence on the check-in of the attraction (because they do not necessarily occur). A L-order Markov chain does not explicitly model such skip behaviors because it assumes that the L previous steps have an in uence on the immediate next step.\n\u2026 \u2026 \" # $ \" |\" $ | $ \" &'( $ \" &') $ \" &'# $ \" & $ \" &*# $ (a) point-level \u2026 \" # $ \" %&' $ \" %&( $ \" %&# $ \" % $ \u2026 \" |\" $ | $ \" %*# $ (b) union-level, no skip \u2026 \" # $ \" %&' $ \" %&( $ \" %&# $ \" % $ \u2026 \" |\" $ | $ \" %*# $ (c) union-level, skip once\nTo provide evidences of union-level in uences and skip behaviors, we mine sequential association rules [1,4] of the following form from two real life data sets, MovieLens and Gowalla (see the details of these data sets in Section 4)\n(S u t \u2212L , \u00b7 \u00b7 \u00b7 , S u t \u22122 , S u t \u22121 ) \u2192 S u t .(1)\nFor a rule X \u2192 Y of the above form, the support count sup(XY ) is the number of sequences in which X and Y occur in order as in the rule, and the con dence,\nsup(X Y )\nsup(X ) , is the percentage of the sequences in which Y follows X among those in which X occurs. This rule represents the joint in uence of all the items in X on Y . By changing the right hand side to S u t +1 or S u t +2 , the rule also captures the in uences with one or two step skips. Figure 2 summarizes the number of rules found versus the Markov order L and skip steps with the minimum support count = 5 and the minimum con dence = 50% (we also tried the minimum con dence of 10%, 20%, and 30%, these trends are similar). Most rules have the orders L = 2 and L = 3 and the con dence of rules gets higher for larger L. The gure also tells that a sizable number of rules have skip steps 1 or 2. These ndings support the existence of union-level in uences and skip behaviors. The minimum support count = 5 and the minimum condence = 50%.\n\n\nContributions\n\nTo address these above limitations of existing works, we propose a ConvolutionAl Sequence Embedding Recommendation Model, or Caser for short, as a solution to top-N sequential recommendation. This model leverages the recent success of convolution lters of Convolutional Neural Network (CNN) to capture local features for image recognition [11,16] and natural language processing [12]. The novelty of Caser is to represent the previous L items as an L \u00d7 d matrix E, where d is the number of latent dimensions and the rows preserve the order of the items. Similar to [12], we regard this embedding matrix as the \"image\" of the L items in the latent space and search for sequential patterns as local features of this \"image\" using various convolutional lters. Unlike image recognition, however, this \"image\" is not given in the input and must be learnt simultaneously with all lters. Compared to existing methods, Caser o ers several distinct advantages. (1) Caser uses horizontal and vertical convolutional lters to capture sequential patterns at point-level, union-level, and of skip behaviors. (2) Caser models both users' general preferences and sequential patterns, and generalizes several existing state-of-theart methods in a single uni ed framework. (3) Caser outperforms state-of-the-art methods for top-N sequential recommendation on real life data sets. In the rest of the paper, we discuss further related work in Section 2, the Caser method in Section 3, and experimental studies in Section 4.\n\n\nFURTHER RELATED WORK\n\nConventional recommendation methods, e.g., collaborative ltering [24], matrix factorization [15,22], and top-N recommendation [9] [19], are not suitable for capturing sequential patterns because they do not model the order of actions. Early works on sequential pattern mining [1,4] nd explicit sequential association rules based on statistical co-occurrences [17]. This approach depends on The rectangular boxes represent items S u 1 , \u00b7 \u00b7 \u00b7 , S u |S u | in user sequence, whereas a rectangular box with circles inside stands for a certain vector e.g., user embedding P u . The dash rectangular boxes are convolutional lters with di erent sizes. The red circles in convolutional layers stand for the max values in each of the convolution results. Here we are using previous 4 actions (L = 4) to predict which items this user will interact with in next 2 steps (T = 2). the explicit representation of patterns, thus, could miss patterns in unobserved states. Also, it su ers from a potentially large search space, sensitivity to threshold settings, and a large number of rules, most being redundant.\n\nRestricted Bolzmann Machine (RBM) [23] is the rst successful 2layers neural network that is applied to recommendation problems. Auto-encoder framework [25,29] and its variant denoising autoencoder [32] also produce a good recommendation performance. Convolutional neural network (CNN) [36] has been used to extract users' preferences from their reviews. None of these works is for sequential recommendation.\n\nRecurrent neural networks (RNN) was used for session-based recommendation [8,10]. While RNN has shown to have an impressive capability in modeling sequences [18], its sequentially connected network structure may not work well under sequential recommendation setting. Because in sequential recommendation problem, not all adjacent actions have dependency relationships (e.g. a user bought i 2 after i 1 only because she loves i 2 ). Our experimental results in Section 4 verify this point: RNN-based method performs better when data sets contains considerable sequential patterns. While our proposed method doesn't model sequential pattern as adjacent actions, it adopts convolutional lters from CNN and model sequential patterns as local features of the embeddings of previous items. This approach o ers the exibility of modeling sequential patterns at both point level and union level, and skip behaviors in a single uni ed framework. In fact, we will show that Caser generalizes several state-of-the-art methods.\n\nA related but di erent problem is temporal recommendation [26,31,34]. For example, temporal recommendation recommends co ee in the morning, instead of evening, whereas our top-N sequential recommendation would recommend phone accessories soon after a user bought an iPhone, independently of the time. Clearly, the two problems are di erent and require di erent solutions.\n\n\nPROPOSED METHODOLOGY\n\nThe proposed model, ConvolutionAl Sequence Embedding Recommendation (Caser), incorporates the Convolutional Neural Network (CNN) to learn sequential features, and Latent Factor Model (LFM) to learn user speci c features. The goal of Caser's network design is multi-fold: capture both user's general preferences and sequential patterns, at both union-level and point-level, and capture skip behaviors, all in unobserved spaces. Shown in Figure 3 Caser consists of three components: Embedding Look-up, Convolutional Layers, and Fully-connected Layers. To train the CNN, for each user u, we extract every L successive items as input and their next T items as the targets from the user's sequence S u , shown on the left side of Figure 3. This is done by sliding a window of size L + T over the user's sequence, and each window generates a training instance for u, denoted by a triplet (u, previous L items, next T items).\n\n\nEmbedding Look-up\n\nCaser captures sequence features in the latent space by feeding the embeddings of previous L items into the neural network. The embedding Q i \u2208 R d for item i is a similar concept to its latent factors. Here d is the number of latent dimensions. The embedding look-up operation retrieves the previous L items' embeddings and stacks them together, resulting in a matrix E (u,t ) \u2208 R L\u00d7d for user  \nE (u,t ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 Q S u t \u2212L . . . Q S u t \u22122 Q S u t \u22121 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb .\n(\n\nAlong with the item embeddings, we also have an embedding P u \u2208 R d for a user u, representing user features in latent space. These embeddings are represented by blue and purple circles in the box of Embedding Look-up in Figure 3.\n\n\nConvolutional Layers\n\nOur approach leverages the recent success of convolution lters of CNN in capturing local features for image recognition [11,16] and natural language processing [12]. Borrows the idea of using CNN in text classi cation [12], our approach regards the L \u00d7 d matrix E as the \"image\" of the previous L items in the latent space and regard sequential patterns as local features of this \"image\". This approach enables the use of convolution lters to search for sequential patterns. Figure 4 shows two \"horizontal lters\" that capture two union-level sequential patterns. These lters, represented as h \u00d7 d matrices, have the height h = 2 and the full width equal to d. They pick up signals for sequential patterns by sliding over the rows of E. For example, the rst lter picks up the sequential pattern \"(Airport, Hotel) \u2192 Great Wall\" by having larger values in the latent dimensions where Airport and Hotel have larger values. Similarly, a \"vertical lter\" is a L \u00d7 1 matrix and will slide over the columns of E. More details are explained below. Unlike image recognition, the \"image\" E is not given because the embedding Q i for all items i must be learnt simultaneously with all lters. Horizontal Convolutional Layer. This layer, shown in the upper part of the second component in Figure 3, has n horizontal lters \nF k \u2208 R h\u00d7d , 1 \u2264 k \u2264 n. h \u2208 {1, \u00b7 \u00b7 \u00b7 ,i, 1 \u2264 i \u2264 L \u2212 h + 1.\nThe result of the interaction is the i-th convolution value given by\nc k i = \u03d5 c (E i:i+h\u22121 F k ).(3)\nwhere the symbol denotes the inner product operator and \u03d5 c (\u00b7) is the activation function for convolutional layers. This value is the inner product between F k and the sub-matrix formed by the row i to row i \u2212 h + 1 of E, denoted by E i:i+h\u22121 . The nal convolution result of F k is the vector\nc k = c k 1 c k 2 \u00b7 \u00b7 \u00b7 c k L\u2212h+1 .(4)\nWe then apply a max pooling operation to c k to extract the maximum value from all values produced by this particular lter. The maximum value captures the most signi cant feature extracted by the lter. Therefore, for the n lters in this layer, the output value\no \u2208 R n is o = {max(c 1 ), max(c 2 ), \u00b7 \u00b7 \u00b7 , max(c n )}.(5)\nHorizontal lters interact with every successive h items through their embeddings E. Both the embeddings and the lters are learnt to minimize an objective function that encodes the prediction error of target items (more in Section 3.4). By sliding lters of various heights, a signi cant signal will be picked up regardless of location. Therefore, horizontal lters can be trained to capture union-level patterns with multiple union sizes. Vertical Convolutional Layer. This layer is shown in the lower part of the second component in Figure 3. We use tilde (\u223c) for the symbols of this layer. Suppose that there are\u00f1 vertical lters F k \u2208 R L\u00d71 , 1 \u2264 k \u2264\u00f1. Each lterF k interacts with the columns of E by sliding d times from left to right on E, yielding the vertical convolution resultc k :c\nk = c k 1c k 2 \u00b7 \u00b7 \u00b7c k d .(6)\nFor the inner product interaction, it is easy to verify that this result is equal to the weighted sum over the L rows of E withF k as the weights:c\nk = L l =1F k l \u00b7 E l ,(7)\nwhere E l is the l-th row of E. Therefore, with vertical lters we can learn to aggregate the embeddings of the L previous items, similar to Fossil's [6] weighted sum to aggregate the L previous items' latent representations. The di erence is that each lterF k is acting like a di erent aggregator. Thus, similar to Fossil, these vertical lters are capturing point-level sequential patterns through weighted sums over previous items' latent representations. While Fossil uses a single weighted sum for each user, we can use\u00f1 global vertical lters to produce\u00f1 weighted sums\u00f5 \u2208 R d\u00f1 for all users:\no = c 1c2 \u00b7 \u00b7 \u00b7c\u00f1 .(8)\nSince their usage is aggregation, vertical lters have some di erences from horizontal ones: (1) The size of each vertical lter is xed to be L \u00d7 1. This is because each column of E is latent for us, it is meaningless to interact with multiple successive columns at one time. (2) There is no need to apply max pooling operation over the vertical convolution results, as we want to keep the aggregation for every latent dimension. Thus, the output of this layer is\u00f5.\n\n\nFully-connected Layers\n\nWe concatenate the outputs of the two convolutional layers and feed them into a fully-connected neural network layer to get more high-level and abstract features:\nz = \u03d5 a (W \u00f5 o + b),(9)\nwhere W \u2208 R d \u00d7(n+d\u00f1) is the weight matrix that projects the concatenation layer to a d-dimensional hidden layer, b \u2208 R d is the corresponding bias term and \u03d5 a (\u00b7) is the activation function for fully-connected layer. z \u2208 R d is what we called convolutional sequence embedding, which encodes all kinds of sequential features of the L previous items.\n\nTo capture user's general preferences, we also look-up the user embedding P u and concatenate the two d-dimensional vectors, z and P u , together and project them to an output layer with |I| nodes, written as\n(u,t ) = W z P u + b ,(10)\nwhere b \u2208 R | I | and W \u2208 R | I |\u00d72d are the bias term and weight matrix for output layer, respectively. As explained in Section 3.4, the value (u,t ) i in the output layer is associated with the probability of how likely user u will interact with item i at time step t. z intends to capture short term sequential patterns, whereas the user embedding P u captures user's long-term general preferences. Here we put the user embedding Pu in the last hidden layer for several reasons: (1) As we shall see in Section 3.6, it can have the ability to generalize other models. (2) we can pre-train our model's parameters with other generalized models' parameters. As stated in [7], such pretraining is critical to model performance\n\n\nNetwork Training\n\nTo train the network, we transform the values of the output layer, (u,t ) , to probabilities by:\np(S u t | S u t \u22121 , S u t \u22122 , \u00b7 \u00b7 \u00b7 , S u t \u2212L ) = \u03c3 ( (u,t ) S u t ),(11)\nwhere \u03c3 (x) = 1/(1 + e \u2212x ) is the sigmoid function. Let C u = {L + 1, L + 2, ..., |S u |} be the collection of time steps for which we would like to make predictions for user u. The likelihood of all sequences in the dataset is:\np(S|\u0398) = u t \u2208 C u \u03c3 ( (u,t ) S u t ) j S u t (1 \u2212 \u03c3 ( (u,t ) j )).(12)\nTo further capture skip behaviors, we could consider the next T target items, D u t = {S u t , S u t +1 , ..., S u t +T }, at once by replacing the immediate next item S u t in the above equation with D u t . Taking the negative logarithm of likelihood, we get the objective function, also known as binary cross-entropy loss:\n= u t \u2208 C u i \u2208 D u t \u2212log(\u03c3 ( (u,t ) i )) + j i \u2212log(1 \u2212 \u03c3 ( (u,t ) j )). (13)\nFollowing previous works [6,21,32], for each target item i, we randomly sample several (3 in our experiments) negative instances j in the second term.\n\nThe model parameters \u0398 = {P, Q, F ,F ,W ,W , b, b } are learned by minimizing the objective function in Eqn (13) on the training set, whereas the hyperparameters (e.g., d, n,\u00f1, L,T ) are tuned on the validation set via grid search. We adopt an variant of Stochastic Gradient Descent (SGD) called Adaptive Moment Estimation (Adam) [13] for faster convergence, with a batch size of 100. To control model complexity and avoid over-tting, we use two kinds of regularization methods: the L2 Norm is applied for all model parameters and Dropout [27] technique with 50% drop ratio is used on fully-connected layers. We implemented Caser with MatCon-vNet [28]. The whole training time is proportional to the number of training instances. For example, it took around 1 hour for Movie-Lens data and 2 hours for Gowalla data, 2 hours for Foursquare and 1 hour for Tmall on a 4-cores i7 CPU and 32GB RAM machine. These times are comparable to Fossil's [6] running time and can be further reduced by using GPU.\n\n\nRecommendation\n\nAfter obtaining the trained neural network, to make recommendations for a user u at time step t, we take u's latent embedding P u and extract his last L items' embeddings given by Eqn (2) as the neural network input. We recommend the N items that have the highest values in the output layer . The complexity for making recommendations to all users is O(|U||I|d), where the complexity of convolution operations is ignored. Note that the number of target items T is a hyperparameter used during the model training, whereas N is the number of items recommended after the model is trained.\n\n\nConnection to Existing Models\n\nWe show that Caser is a generalization of several previous models.\n\nCaser vs. MF. By discarding all convolutional layers and all bias terms, our model becomes a vanilla LFM with user embeddings as user latent factors and its associated weights as item latent factors. MF usually contains bias terms 1 , which is b in our model. After discarding all convolutional layers, the resulting model is the same as MF:\nu i = W i 0 P u + b i .(14)\nCaser vs. FPMC. FPMC fuses factorized rst-order Markov chain with LFM and is optimized by Bayesian personalized ranking (BPR). Although Caser uses a di erent optimization criterion, i.e., the crossentropy, it is able to generalize FPMC by copying the previous item's embedding to the hidden layer z and not using any bias terms:\n(u,t ) i = W i Q S u t \u22121 P u .(15)\nAs FPMC uses BPR as the criterion, our model is not exactly the same as FPMC. However, BPR is limited to have only 1 target and negative sample at each time step. Our cross-entropy loss does not have these limitations. Caser vs. Fossil. By omitting the horizontal convolutional layer and using one vertical lter and copying the vertical convolution resultc to the hidden layer z, we get\n(u,t ) i = W i c P u + b i .(16)\nAs discussed for Eqn (7), this vertical lter serves as the weighted sum of the embeddings of the L previous items, like in Fossil, though Fossil uses Similarity Model instead of LFM and factorizes it in the same latent space as Markov model. Another di erence is that Fossil uses one local weighting for each user while we use a number of global weighting through vertical lters.\n\n\nEXPERIMENTS\n\nWe compare Caser with state-of-the-art methods. The source code of Caser and processed data sets are available online 2 .\n\n\nExperimental Setup\n\nDatasets. Sequential recommendation makes sense only when the data set contains sequential patterns. To identify such data sets, we applied sequential association rule mining to several public data sets and computed their sequential intensity de ned by:\n\nSequential Intensity (SI ) = #rules #users .\n\nThe numerator is the total number of rules in the form of Eqn (1) found using a minimum threshold on support (i.e., 5) and condence(i.e., 50%) with Markov order L range from 1 to 5. The denominator is the total number of users. We use SI to estimate the intensity of sequential signals in a data set.\n\nThe four data sets with their SI are described in Table 1. Movie-Lens 3 is the widely used movie rating data. Gowalla 4 constructed by [3] and Foursquare obtained from [33] contain implicit feedback through user-venue check-ins. Tmall, the largest B2C platform in China, is a user-purchase data obtained from IJCAI 2015 competition 5 , which aims to forecast repeated buyers. Following previous works [6,20,32], we converted all numeric ratings to implicit feedback of 1. We also removed cold-start users and items of having less than n feedbacks, as dealing with cold-start recommendation is usually treated as a separate issue in the literature [6,7,21,32]. n is 5,15,10,10 for MovieLens, Gowalla, Foursquare, and Tmall. The Amazon data previously used in [5,6] was not used due to its SI (0.0026 for 'O ce Products' category, 0.0019 for 'Clothing, Shoes, Jewelry' and 'Video Games' category), in other words, its sequential signals are much weaker than the above data sets.\n\nFollowing [17,33,35], we hold the rst 70% of actions in each user's sequence as the training set and use the next 10% of actions as the validation set to search the optimal hyperparameter settings for all models. The remaining 20% actions in each user's sequence are used as the test set for evaluating a model's performance. Evaluation Metrics. As in [19,21,29,32], we evaluate a model by Precision@N , Recall@N , and Mean Average Precision (MAP). Given a list of top N predicted items for a user, denotedR 1:N , and the last 20% of actions in her/his sequence (i.e., denoted R (i.e., the test set), Precision@N and Recall@N are computed by \nPrec@N = |R R 1:N | N , Recall@N = |R R 1:N | |R| .(18)AP = |R | N =1 Prec@N \u00d7 rel(N ) |R| ,(19)\nwhere rel(N ) = 1 if the N -th item inR is in R. The Mean Average Precision (MAP) is the average of AP for all users.\n\n\nPerformance Comparison\n\nWe compare our method, Caser, proposed in Section 3 with the following baselines.\n\n\u2022 POP. All items are ranked by their popularity in all users' sequences, and the popularity is determined by the number of interactions. \u2022 BPR. Combined with Matrix Factorization model, Bayesian personalized ranking [20] is the state-of-the-art method for non-sequential item recommendation on implicit feedback data. \u2022 FMC and FPMC. As introduced in [21], FMC factorizes the rst-order Markov transition matrix into two lowdimensional sub-matrices, and FPMC is a fusion of FMC and LFM. These are the state-of-the-art sequential recommendation methods. FPMC allows a basket of several items at each step. For our sequential recommendation problem, each basket has a single item. \u2022 Fossil. Fossil [6] models high-order Markov chains and uses Similarity Model instead of LFM for modeling general user preferences. \u2022 GRU4Rec. This is the session-based recommendation proposed by [8]. This model uses RNN to capture sequential dependencies and make predictions.\n\nFor each method, the grid search is applied to nd the optimal settings of hyperparameters using the validation set. These include latent dimensions d from {5, 10, 20, 30, 50, 100}, regularization hyperparameters, and the learning rate from {1, 10 \u22121 , ..., 10 \u22124 }. For Fossil, Caser and GRU4Rec, the Markov order L is from {1, \u00b7 \u00b7 \u00b7 , 9}. For Caser itself, the height h of horizontal lters is from {1, \u00b7 \u00b7 \u00b7 , L}, the target number T is from {1, 2, 3}, the activation functions \u03d5 a and \u03d5 c are from {identit , si moid, tanh, relu}. For each height h, the number of horizontal lters is from {4, 8, 16, 32, 64}. The number of vertical lters is from {1, 2, 4, 8, 16}. We report the result of each method under its optimal hyperparameter settings.\n\nThe best results of the six baselines and Caser are summarized in Table 2. The best performer on each row is highlighted in bold face. The last column is the improvement of Caser relative to the best baseline, de ned as Caser \u2212baseline baseline . Except for MovieLens, Caser improved the best baseline on all N tested by a large margin w.r.t. the three metrics. Among the baseline methods, the sequential recommenders (e.g., FPMC and Fossil) usually outperform nonsequential recommenders (i.e., BPR) on all data sets, suggesting the importance of considering sequential information. FPMC and Fossil outperform FMC on all data sets, suggesting the e ectiveness of personalization. On MovieLens, GRU4Rec achieved a performance close to Caser's, but got a much worse performance on the other three data sets. In fact, MovieLens has more sequential signals than   the other three data sets, thus, the RNN-based GRU4Rec could perform well on MovieLens but can easily get biased on training sets of the other three data sets despite the use of regularization and dropout as described in [8]. In addition, GRU4Rec's recommendation is session-based, instead of personalized, which enlarge the generalization error to some extent.\n\nIn the following studies, we examine the impact of the hyperparameters d, L,T one at a time by holding the remaining hyperparameters at their optimal settings. We focus on MAP as it is an overall performance indicator and consistent with other metrics. Figure 5 shows MAP for various d while keeping the other optimal hyperparameters unchanged. On the denser MovieLens, a larger d does not always lead to a better model performance. A model achieves its best performance when d is chosen properly and gets worse for a larger d because of over-tting. But for the other three sparser data sets, each model requires more latent dimensions to achieve their best results. For all data sets, Caser beats the strongest baseline performance by using a relatively small number of latent dimensions. \n\n\nInfluence of Latent Dimensionality d.\n\n\nInfluence of Markov\n\nOrder L and Target Number T . We vary L to explore how much of Fossil, GRU4Rec and Caser can gain from high-order information while keeping other optimal hyperparameters unchanged. Caser-1, Caser-2, and Caser-3 denote Caser with the target number T at 1, 2, 3 to study the e ect of skip behaviors. The results are shown in Figure 6. On the dense MovieLens, Caser best utilizes the extra information provided by a larger L and Caser-3 performs the best, suggesting the bene ts of skip steps. However, for the sparser data sets, all models do not consistently bene t from a larger L. This is reasonable, because for a sparse data set, a higher order Markov chain tends to introduce both extra information and more noises. In most cases, Caser-2 slightly outperforms the other models on these three data sets. \n\n\nAnalysis of Caser\n\nComponents. Finally, we evaluate the contribution of each of Caser's components, the horizontal convolutional layer (i.e., o), the vertical convolutional layer (i.e.,\u00f5), and personalization (i.e., P u ), to the overall performance while keeping all hyperparameters at their optimal settings. The result is shown in Table 3 for MovieLens and Gowalla; the results of the other two data sets are similar. For x \u2208 {p, h, , h, ph, p , p h}, Caser-x denotes Caser with the components x enabled. h denotes horizontal convolutional layer; v denotes vertical convolutional layer; p denotes personalization, which is similar to BPR and uses LFM only. Any missing component is represented by setting its corresponding o,\u00f5, and P u to zero. For example, vh denotes both vertical convolutional layer and horizontal convolutional layer by setting P u to all zeros, and pv denotes vertical convolutional layer and personalization by setting o to all zeros. Caser-p performs the worst whereas Caserh, Caser-v, and Caser-vh improve the performance signi cantly, suggesting that treating top-N sequential recommendation as the conventional top-N recommendation will lose useful information, and that modeling both sequential patterns at the union-level and point-level is useful for improving the prediction. For both data sets, the best performance is achieved by jointly using all parts of Caser, i.e., Caser-pvh.\n\n\nNetwork Visualization\n\nWe have a closer look at some trained networks and prediction. Figure 7 shows the values of four vertical convolutional lters after training Caser on MovieLens with L = 9. In the micro perspective, the four lters are trained to be diverse, but in the macro perspective, they follow an ascending trend from past positions to recent positions. With each vertical lter serving as a way of weighting the embeddings of previous actions (see the related discussion in Section 3), this trend indicates that Caser puts more emphasis on recent actions, demonstrating a major di erence from the conventional top-N recommendation. To see the e ectiveness of horizontal lters, Figure 8(a) shows top N = 3 ranked movies recommended by Caser, i.e.,R 1 (Mad Max), R 2 (Star War),R 3 (Star Trek) in that order, for a user with L = 5 previous movies, i.e., S 1 (13th Warrior), S 2 (American Beauty), S 3 (Star Trek), S 4 (Star Trek III), and S 5 (Star Trek IV).R 3 is the ground truth (i.e., the next movie in the user sequence). Note thatR 1 and R 2 are quite similar toR 3 , i.e., all being action and science ction movies, so are also recommended to the user. Figure 8(b) shows the new rank ofR 3 after masking some of the L previous movies by setting their item embeddings to zeros in the trained network. Masking S 1 and S 2 actually increases the rank ofR 3 to 2 (from 3); in fact, S 1 and S 2 are history or romance movies and act like noises for recommendingR 3 . Masking each of S 3 , S 4 and S 5 decreases the rank ofR 3 because these movies are in the same category asR 3 . The most decrease occurs after masking S 3 , S 4 and S 5 all together. This study clearly indicates that our model correctly captures the dependence\n\nWSDM 2018 ,\n2018February 5-9, 2018, Marina Del Rey, CA, USA \u00a9 2018 ACM. 978-1-4503-5581-0/18/02. . . $15.00 DOI: 10.1145/3159652.3159656\n\nFigure 1 :\n1An example of point and union level dynamic pattern in uences, the order of Markov chain L = 3\n\nFigure 2 :\n2The number of association rules vs L and skip steps.\n\nFigure 3 :\n3The network architecture of Caser.\n\nFigure 4 :\n4Darker colors mean larger values. The rst lter captures \"(Airport, Hotel) \u2192 Great Wall\" by interacting with the embedding of airport and hotel and skipping that of fast food and restaurant. The second lter captures \"(Fast Food, Restaurant) \u2192 Bar\".\n\n\nu at time step t:\n\nFigure 5 :\n5MAP (y-axis) vs. the number of latent dimensions d (x-axis).\n\nFigure 6 :\n6MAP (y-axis) vs. the Markov order L (x-axis). Caser-1, Caser-2, and Caser-3 denote Caser with the number of targets T set to 1, 2, 3.\n\nFigure 7 :\n7Visualization for four vertical convolutional lters of a trained model on MovieLens data when L = 9.\n\n\nL} is the height of a lter. For example, if L = 4, one may choose to have n = 8 lters, two for each h in {1, 2, 3, 4}. F k will slide from top to bottom on E and interact with all horizontal dimensions of E of the items\n\nTable 1 :\n1Statistics of the datasetsDatasets \nSequential \n#users \n#items \navg. actions \nSparsity \nIntensity \nper user \nMovieLens \n0.3265 \n6.0k \n3.4k \n165.50 \n95.16% \nGowalla \n0.0748 \n13.1k \n14.0k \n40.74 \n99.71% \nFoursquare \n0.0378 \n10.1k \n23.4k \n30.16 \n99.87% \nTmall \n0.0104 \n23.8k \n12.2k \n13.93 \n99.89% \n\n\n\nTable 2 :\n2Performance comparison on the four data sets.Dataset \nMetric \nPOP \nBPR \nFMC \nFPMC \nFossil \nGRU4Rec \nCaser \nImprov. \n\nMovieLens \n\nPrec@1 \n0.1280 \n0.1478 \n0.1748 \n0.2022 \n0.2306 \n0.2515 \n0.2502 \n-0.5% \nPrec@5 \n0.1113 \n0.1288 \n0.1505 \n0.1659 \n0.2000 \n0.2146 \n0.2175 \n1.4% \nPrec@10 \n0.1011 \n0.1193 \n0.1317 \n0.1460 \n0.1806 \n0.1916 \n0.1991 \n4.0% \nRecall@1 \n0.0050 \n0.0070 \n0.0104 \n0.0118 \n0.0144 \n0.0153 \n0.0148 \n-3.3% \nRecall@5 \n0.0213 \n0.0312 \n0.0432 \n0.0468 \n0.0602 \n0.0629 \n0.0632 \n0.5% \nRecall@10 \n0.0375 \n0.0560 \n0.0722 \n0.0777 \n0.1061 \n0.1093 \n0.1121 \n2.6% \nMAP \n0.0687 \n0.0913 \n0.0949 \n0.1053 \n0.1354 \n0.1440 \n0.1507 \n4.7% \n\nGowalla \n\nPrec@1 \n0.0517 \n0.1640 \n0.1532 \n0.1555 \n0.1736 \n0.1050 \n0.1961 \n13.0% \nPrec@5 \n0.0362 \n0.0983 \n0.0876 \n0.0936 \n0.1045 \n0.0721 \n0.1129 \n8.0% \nPrec@10 \n0.0281 \n0.0726 \n0.0657 \n0.0698 \n0.0782 \n0.0571 \n0.0833 \n6.5% \nRecall@1 \n0.0064 \n0.0250 \n0.0234 \n0.0256 \n0.0277 \n0.0155 \n0.0310 \n11.9% \nRecall@5 \n0.0257 \n0.0743 \n0.0648 \n0.0722 \n0.0793 \n0.0529 \n0.0845 \n6.6% \nRecall@10 \n0.0402 \n0.1077 \n0.0950 \n0.1059 \n0.1166 \n0.0826 \n0.1223 \n4.9% \nMAP \n0.0229 \n0.0767 \n0.0711 \n0.0764 \n0.0848 \n0.0580 \n0.0928 \n9.4% \n\nFoursquare \n\nPrec@1 \n0.1090 \n0.1233 \n0.0875 \n0.1081 \n0.1191 \n0.1018 \n0.1351 \n13.4% \nPrec@5 \n0.0477 \n0.0543 \n0.0445 \n0.0555 \n0.0580 \n0.0475 \n0.0619 \n6.7% \nPrec@10 \n0.0304 \n0.0348 \n0.0309 \n0.0385 \n0.0399 \n0.0331 \n0.0425 \n6.5% \nRecall@1 \n0.0376 \n0.0445 \n0.0305 \n0.0440 \n0.0497 \n0.0369 \n0.0565 \n13.7% \nRecall@5 \n0.0800 \n0.0888 \n0.0689 \n0.0959 \n0.0948 \n0.0770 \n0.1035 \n7.9% \nRecall@10 \n0.0954 \n0.1061 \n0.0911 \n0.1200 \n0.1187 \n0.1011 \n0.1291 \n7.6% \nMAP \n0.0636 \n0.0719 \n0.0571 \n0.0782 \n0.0823 \n0.0643 \n0.0909 \n10.4% \n\nTmall \n\nPrec@1 \n0.0010 \n0.0111 \n0.0197 \n0.0210 \n0.0280 \n0.0139 \n0.0312 \n11.4% \nPrec@5 \n0.0009 \n0.0081 \n0.0114 \n0.0120 \n0.0149 \n0.0090 \n0.0179 \n20.1% \nPrec@10 \n0.0007 \n0.0063 \n0.0084 \n0.0090 \n0.0104 \n0.0070 \n0.0132 \n26.9% \nRecall@1 \n0.0004 \n0.0046 \n0.0079 \n0.0082 \n0.0117 \n0.0056 \n0.0130 \n11.1% \nRecall@5 \n0.0019 \n0.0169 \n0.0226 \n0.0245 \n0.0306 \n0.0180 \n0.0366 \n19.6% \nRecall@10 \n0.0026 \n0.0260 \n0.0333 \n0.0364 \n0.0425 \n0.0278 \n0.0534 \n25.6% \nMAP \n0.0030 \n0.0145 \n0.0197 \n0.0212 \n0.0256 \n0.0164 \n0.0310 \n21.1% \n\n\n\nTable 3 :\n3MAP vs. Caser ComponentsMovieLens \nGowalla \nCaser-p \n0.0935 \n0.0777 \nCaser-h \n0.1304 \n0.0805 \nCaser-v \n0.1403 \n0.0841 \nCaser-vh \n0.1448 \n0.0856 \nCaser-ph \n0.1372 \n0.0911 \nCaser-pv \n0.1494 \n0.0921 \nCaser-pvh \n0.1507 \n0.0928 \n\n\nTop-N recommendation ranks the items for each user individually, which is invariant to user bias and global bias.\nCONCLUSIONCaser is a novel solution to top-N sequential recommendation by modeling recent actions as an \"image\" among time and latent dimensions and learning sequential patterns using convolutional lters. This approach provides a uni ed and exible network structure for capturing many important features of sequential recommendation, i.e., point-level and union-level sequential patterns, skip behaviors, and long term user preferences. Our experiments and case studies on public real life data sets suggested that Caser outperforms the state-of-the-art methods for top-N sequential recommendation.ACKNOWLEDGEMENTThe work of the second author is partially supported by a Discovery Grant from Natural Sciences and Engineering Research Council of Canada.\nMining sequential patterns. Rakesh Agrawal, Ramakrishnan Srikant, International Conference on Data Engineering. IEEERakesh Agrawal and Ramakrishnan Srikant. 1995. Mining sequential patterns. In International Conference on Data Engineering. IEEE, 3-14.\n\nWhere You Like to Go Next: Successive Point-of-Interest Recommendation. Chen Cheng, Haiqin Yang, Irwin Michael R Lyu, King, International Joint Conference on Arti cial Intelligence. Chen Cheng, Haiqin Yang, Michael R Lyu, and Irwin King. 2013. Where You Like to Go Next: Successive Point-of-Interest Recommendation.. In International Joint Conference on Arti cial Intelligence. 2605-2611.\n\nFriendship and mobility: user movement in location-based social networks. Eunjoon Cho, A Seth, Jure Myers, Leskovec, International Conference on Knowledge Discovery and Data Mining. ACMEunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility: user movement in location-based social networks. In International Conference on Knowledge Discovery and Data Mining. ACM, 1082-1090.\n\nData mining: concepts and techniques. Jiawei Han, Jian Pei, Micheline Kamber, ElsevierJiawei Han, Jian Pei, and Micheline Kamber. 2011. Data mining: concepts and techniques. Elsevier.\n\nTranslation-based recommendation. R He, W.-C Kang, J Mcauley, ACM Conference on Recommender systems. R. He, W.-C. Kang, and J. McAuley. 2017. Translation-based recommendation. In ACM Conference on Recommender systems.\n\nFusing Similarity Models with Markov Chains for Sparse Sequential Recommendation. R He, J Mcauley, International Conference on Data Mining. IEEER. He and J. McAuley. 2016. Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation. In International Conference on Data Mining. IEEE.\n\nNeural collaborative ltering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, International Conference on World Wide Web. ACMXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative ltering. In International Conference on World Wide Web. ACM, 173-182.\n\nBal\u00e1zs Hidasi, Alexandros Karatzoglou, arXiv:1511.06939Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprintBal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015).\n\nCollaborative ltering for implicit feedback datasets. Yifan Hu, Yehuda Koren, Chris Volinsky, International Conference on Data Mining. IEEEYifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative ltering for implicit feedback datasets. In International Conference on Data Mining. IEEE, 263-272.\n\nWhen Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation. Dietmar Jannach, Malte Ludewig, ACM Conference on Recommender systems. ACMDietmar Jannach and Malte Ludewig. 2017. When Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation. In ACM Conference on Recommender systems. ACM, 306-310.\n\nLarge-scale video classi cation with convolutional neural networks. Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei, IEEE conference on Computer Vision and Pattern Recognition. Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Suk- thankar, and Li Fei-Fei. 2014. Large-scale video classi cation with convolutional neural networks. In IEEE conference on Computer Vision and Pattern Recognition. 1725-1732.\n\nConvolutional Neural Networks for Sentence Classi cation. Yoon Kim, Conference on Empirical Methods on Natural Language Processing. ACL. Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classi cation. In Conference on Empirical Methods on Natural Language Processing. ACL, 1756- 1751.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza- tion. arXiv preprint arXiv:1412.6980 (2014).\n\nCollaborative ltering with temporal dynamics. Yehuda Koren, Commun. ACM. 53Yehuda Koren. 2010. Collaborative ltering with temporal dynamics. Commun. ACM 53, 4 (2010), 89-97.\n\nMatrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech- niques for recommender systems. Computer 42, 8 (2009).\n\nImagenet classi cation with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geo Rey E Hinton, Advances in Neural Information Processing Systems. Alex Krizhevsky, Ilya Sutskever, and Geo rey E Hinton. 2012. Imagenet classi ca- tion with deep convolutional neural networks. In Advances in Neural Information Processing Systems. 1097-1105.\n\nA hybrid of sequential rules and collaborative ltering for product recommendation. Duen-Ren, Chin-Hui Liu, Wang-Jung Lai, Lee, Information Sciences. 179Duen-Ren Liu, Chin-Hui Lai, and Wang-Jung Lee. 2009. A hybrid of sequen- tial rules and collaborative ltering for product recommendation. Information Sciences 179, 20 (2009), 3505-3519.\n\nRecurrent neural network based language model. Tomas Mikolov, Martin Kara \u00c1t, Lukas Burget, Jan Cernock\u1ef3, Sanjeev Khudanpur, Interspeech. 2Tomas Mikolov, Martin Kara \u00e1t, Lukas Burget, Jan Cernock\u1ef3, and Sanjeev Khu- danpur. 2010. Recurrent neural network based language model.. In Interspeech, Vol. 2. 3.\n\nOne-class collaborative ltering. Rong Pan, Yunhong Zhou, Bin Cao, N Nathan, Rajan Liu, Martin Lukose, Qiang Scholz, Yang, International Conference on Data Mining. IEEERong Pan, Yunhong Zhou, Bin Cao, Nathan N Liu, Rajan Lukose, Martin Scholz, and Qiang Yang. 2008. One-class collaborative ltering. In International Confer- ence on Data Mining. IEEE, 502-511.\n\nBPR: Bayesian personalized ranking from implicit feedback. Christoph Ste En Rendle, Zeno Freudenthaler, Lars Gantner, Schmidt-Thieme, Conference on Uncertainty in Arti cial Intelligence. AUAI PressSte en Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt- Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Conference on Uncertainty in Arti cial Intelligence. AUAI Press, 452-461.\n\nFactorizing personalized markov chains for next-basket recommendation. Christoph Ste En Rendle, Lars Freudenthaler, Schmidt-Thieme, International Conference on World Wide Web. ACMSte en Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Fac- torizing personalized markov chains for next-basket recommendation. In Inter- national Conference on World Wide Web. ACM, 811-820.\n\nProbabilistic Matrix Factorization. Ruslan Salakhutdinov, Andriy Mnih, Ruslan Salakhutdinov and Andriy Mnih. 2007. Probabilistic Matrix Factorization..\n\nAdvances in Neural Information Processing Systems. 1In Advances in Neural Information Processing Systems, Vol. 1. 2-1.\n\nRestricted Boltzmann machines for collaborative ltering. Ruslan Salakhutdinov, Andriy Mnih, Geo Rey Hinton, International Conference on Machine learning. ACMRuslan Salakhutdinov, Andriy Mnih, and Geo rey Hinton. 2007. Restricted Boltzmann machines for collaborative ltering. In International Conference on Machine learning. ACM, 791-798.\n\nItem-based collaborative ltering recommendation algorithms. Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, International Conference on World Wide Web. ACMBadrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative ltering recommendation algorithms. In International Conference on World Wide Web. ACM, 285-295.\n\nAutorec: Autoencoders meet collaborative ltering. Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, Lexing Xie, International Conference on World Wide Web. ACMSuvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015. Autorec: Autoencoders meet collaborative ltering. In International Conference on World Wide Web. ACM, 111-112.\n\nMulti-rate deep learning for temporal recommendation. Yang Song, Ali Mamdouh Elkahky, Xiaodong He, International ACM SIGIR conference on Research and Development in Information Retrieval. ACMYang Song, Ali Mamdouh Elkahky, and Xiaodong He. 2016. Multi-rate deep learning for temporal recommendation. In International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 909-912.\n\nDropout: A Simple Way to Prevent Neural Networks from Over tting. Nitish Srivastava, Geo Rey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Journal of Machine Learning Research. 15Nitish Srivastava, Geo rey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Over tting. Journal of Machine Learning Research 15, 1 (2014), 1929-1958.\n\nMatconvnet: Convolutional neural networks for matlab. Andrea Vedaldi, Karel Lenc, International conference on Multimedia. ACMAndrea Vedaldi and Karel Lenc. 2015. Matconvnet: Convolutional neural net- works for matlab. In International conference on Multimedia. ACM, 689-692.\n\nCollaborative deep learning for recommender systems. Hao Wang, Naiyan Wang, Dit-Yan Yeung, International Conference on Knowledge Discovery and Data Mining. ACMHao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning for recommender systems. In International Conference on Knowledge Discovery and Data Mining. ACM, 1235-1244.\n\nLearning hierarchical representation model for nextbasket recommendation. Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, Xueqi Cheng, International ACM SIGIR conference on Research and Development in Information Retrieval. ACMPengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2015. Learning hierarchical representation model for nextbasket recom- mendation. In International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 403-412.\n\nRecurrent Recommender Networks. Chao-Yuan, Amr Wu, Alex Ahmed, Alexander J Beutel, How Smola, Jing, International Conference on Web Search and Data Mining. ACMChao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J. Smola, and How Jing. 2017. Recurrent Recommender Networks. In International Conference on Web Search and Data Mining. ACM, 495-503.\n\nCollaborative denoising auto-encoders for top-n recommender systems. Yao Wu, Christopher Dubois, Alice X Zheng, Martin Ester, International Conference on Web Search and Data Mining. ACMYao Wu, Christopher DuBois, Alice X Zheng, and Martin Ester. 2016. Collabora- tive denoising auto-encoders for top-n recommender systems. In International Conference on Web Search and Data Mining. ACM, 153-162.\n\nGraph-based point-of-interest recommendation with geographical and temporal in uences. Quan Yuan, Gao Cong, Aixin Sun, International Conference on Information and Knowledge Management. ACMQuan Yuan, Gao Cong, and Aixin Sun. 2014. Graph-based point-of-interest recommendation with geographical and temporal in uences. In International Conference on Information and Knowledge Management. ACM, 659-668.\n\nLatent factor transition for dynamic collaborative ltering. Chenyi Zhang, Ke Wang, Hongkun Yu, Jianling Sun, Ee-Peng Lim, SIAM International Conference on Data Mining. SIAM. Chenyi Zhang, Ke Wang, Hongkun Yu, Jianling Sun, and Ee-Peng Lim. 2014. Latent factor transition for dynamic collaborative ltering. In SIAM International Conference on Data Mining. SIAM, 452-460.\n\nStellar: spatial-temporal latent ranking for successive point-of-interest recommendation. Shenglin Zhao, Tong Zhao, Haiqin Yang, Irwin Michael R Lyu, King, AAAI Conference on Arti cial Intelligence. AAAI PressShenglin Zhao, Tong Zhao, Haiqin Yang, Michael R Lyu, and Irwin King. 2016. Stellar: spatial-temporal latent ranking for successive point-of-interest recom- mendation. In AAAI Conference on Arti cial Intelligence. AAAI Press, 315-321.\n\nJoint Deep Modeling of Users and Items Using Reviews for Recommendation. Lei Zheng, Vahid Noroozi, Philip S Yu, International Conference on Web Search and Data Mining. ACMLei Zheng, Vahid Noroozi, and Philip S. Yu. 2017. Joint Deep Modeling of Users and Items Using Reviews for Recommendation. In International Conference on Web Search and Data Mining. ACM, 425-434.\n", "annotations": {"author": "[{\"end\":240,\"start\":110},{\"end\":314,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":120,\"start\":116},{\"end\":248,\"start\":244}]", "author_first_name": "[{\"end\":115,\"start\":110},{\"end\":243,\"start\":241}]", "author_affiliation": "[{\"end\":239,\"start\":136},{\"end\":313,\"start\":266}]", "title": "[{\"end\":83,\"start\":1},{\"end\":397,\"start\":315}]", "venue": "[{\"end\":481,\"start\":399}]", "abstract": "[{\"end\":1726,\"start\":763}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1858,\"start\":1855},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1863,\"start\":1859},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3452,\"start\":3448},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3455,\"start\":3452},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3975,\"start\":3971},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3978,\"start\":3975},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3981,\"start\":3978},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4382,\"start\":4379},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4384,\"start\":4382},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4387,\"start\":4384},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4390,\"start\":4387},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4685,\"start\":4681},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4731,\"start\":4728},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4907,\"start\":4904},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5434,\"start\":5431},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7181,\"start\":7178},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7183,\"start\":7181},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8732,\"start\":8728},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8735,\"start\":8732},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8772,\"start\":8768},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8958,\"start\":8954},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9986,\"start\":9982},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10013,\"start\":10009},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10016,\"start\":10013},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10046,\"start\":10043},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10051,\"start\":10047},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10196,\"start\":10193},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10198,\"start\":10196},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10280,\"start\":10276},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11055,\"start\":11051},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11172,\"start\":11168},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11175,\"start\":11172},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11218,\"start\":11214},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11306,\"start\":11302},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11503,\"start\":11500},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11506,\"start\":11503},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11587,\"start\":11583},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12504,\"start\":12500},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12507,\"start\":12504},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12510,\"start\":12507},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14645,\"start\":14641},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14648,\"start\":14645},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14685,\"start\":14681},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14743,\"start\":14739},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17795,\"start\":17792},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18538,\"start\":18535},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20199,\"start\":20196},{\"end\":20344,\"start\":20338},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21181,\"start\":21178},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21184,\"start\":21181},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21187,\"start\":21184},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21639,\"start\":21635},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21848,\"start\":21844},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21956,\"start\":21952},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22248,\"start\":22245},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24187,\"start\":24184},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25443,\"start\":25440},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25477,\"start\":25473},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25709,\"start\":25706},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25712,\"start\":25709},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25715,\"start\":25712},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25955,\"start\":25952},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25957,\"start\":25955},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25960,\"start\":25957},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25963,\"start\":25960},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26066,\"start\":26063},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26068,\"start\":26066},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26297,\"start\":26293},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26300,\"start\":26297},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26303,\"start\":26300},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26639,\"start\":26635},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26642,\"start\":26639},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26645,\"start\":26642},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26648,\"start\":26645},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27470,\"start\":27466},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27605,\"start\":27601},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28128,\"start\":28125},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30038,\"start\":30035},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34735,\"start\":34734}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35137,\"start\":35000},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35245,\"start\":35138},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35311,\"start\":35246},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35359,\"start\":35312},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35620,\"start\":35360},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35640,\"start\":35621},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35714,\"start\":35641},{\"attributes\":{\"id\":\"fig_7\"},\"end\":35861,\"start\":35715},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35975,\"start\":35862},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36197,\"start\":35976},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36506,\"start\":36198},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38677,\"start\":36507},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38915,\"start\":38678}]", "paragraph": "[{\"end\":1974,\"start\":1742},{\"end\":3363,\"start\":1976},{\"end\":4317,\"start\":3399},{\"end\":6829,\"start\":4350},{\"end\":7307,\"start\":7075},{\"end\":7519,\"start\":7363},{\"end\":8371,\"start\":7530},{\"end\":9892,\"start\":8389},{\"end\":11015,\"start\":9917},{\"end\":11424,\"start\":11017},{\"end\":12440,\"start\":11426},{\"end\":12813,\"start\":12442},{\"end\":13756,\"start\":12838},{\"end\":14174,\"start\":13778},{\"end\":14264,\"start\":14263},{\"end\":14496,\"start\":14266},{\"end\":15828,\"start\":14521},{\"end\":15959,\"start\":15891},{\"end\":16286,\"start\":15993},{\"end\":16586,\"start\":16326},{\"end\":17436,\"start\":16648},{\"end\":17615,\"start\":17468},{\"end\":18237,\"start\":17643},{\"end\":18724,\"start\":18261},{\"end\":18913,\"start\":18751},{\"end\":19288,\"start\":18938},{\"end\":19498,\"start\":19290},{\"end\":20250,\"start\":19526},{\"end\":20367,\"start\":20271},{\"end\":20674,\"start\":20445},{\"end\":21072,\"start\":20747},{\"end\":21303,\"start\":21153},{\"end\":22302,\"start\":21305},{\"end\":22906,\"start\":22321},{\"end\":23006,\"start\":22940},{\"end\":23349,\"start\":23008},{\"end\":23706,\"start\":23378},{\"end\":24129,\"start\":23743},{\"end\":24542,\"start\":24163},{\"end\":24679,\"start\":24558},{\"end\":24955,\"start\":24702},{\"end\":25001,\"start\":24957},{\"end\":25303,\"start\":25003},{\"end\":26281,\"start\":25305},{\"end\":26925,\"start\":26283},{\"end\":27140,\"start\":27023},{\"end\":27248,\"start\":27167},{\"end\":28206,\"start\":27250},{\"end\":28952,\"start\":28208},{\"end\":30175,\"start\":28954},{\"end\":30967,\"start\":30177},{\"end\":31838,\"start\":31031},{\"end\":33257,\"start\":31860},{\"end\":34999,\"start\":33283}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7074,\"start\":6830},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7362,\"start\":7308},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7529,\"start\":7520},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14262,\"start\":14175},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15869,\"start\":15829},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15890,\"start\":15869},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15992,\"start\":15960},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16325,\"start\":16287},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16647,\"start\":16587},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17467,\"start\":17437},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17642,\"start\":17616},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18260,\"start\":18238},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18937,\"start\":18914},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19525,\"start\":19499},{\"attributes\":{\"id\":\"formula_15\"},\"end\":20444,\"start\":20368},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20746,\"start\":20675},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21152,\"start\":21073},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23377,\"start\":23350},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23742,\"start\":23707},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24162,\"start\":24130},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26981,\"start\":26926},{\"attributes\":{\"id\":\"formula_23\"},\"end\":27022,\"start\":26981}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25362,\"start\":25355},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29027,\"start\":29020},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32182,\"start\":32175}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1740,\"start\":1728},{\"attributes\":{\"n\":\"1.1\"},\"end\":3397,\"start\":3366},{\"attributes\":{\"n\":\"1.2\"},\"end\":4348,\"start\":4320},{\"attributes\":{\"n\":\"1.3\"},\"end\":8387,\"start\":8374},{\"attributes\":{\"n\":\"2\"},\"end\":9915,\"start\":9895},{\"attributes\":{\"n\":\"3\"},\"end\":12836,\"start\":12816},{\"attributes\":{\"n\":\"3.1\"},\"end\":13776,\"start\":13759},{\"attributes\":{\"n\":\"3.2\"},\"end\":14519,\"start\":14499},{\"attributes\":{\"n\":\"3.3\"},\"end\":18749,\"start\":18727},{\"attributes\":{\"n\":\"3.4\"},\"end\":20269,\"start\":20253},{\"attributes\":{\"n\":\"3.5\"},\"end\":22319,\"start\":22305},{\"attributes\":{\"n\":\"3.6\"},\"end\":22938,\"start\":22909},{\"attributes\":{\"n\":\"4\"},\"end\":24556,\"start\":24545},{\"attributes\":{\"n\":\"4.1\"},\"end\":24700,\"start\":24682},{\"attributes\":{\"n\":\"4.2\"},\"end\":27165,\"start\":27143},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":31007,\"start\":30970},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":31029,\"start\":31010},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":31858,\"start\":31841},{\"attributes\":{\"n\":\"4.3\"},\"end\":33281,\"start\":33260},{\"end\":35012,\"start\":35001},{\"end\":35149,\"start\":35139},{\"end\":35257,\"start\":35247},{\"end\":35323,\"start\":35313},{\"end\":35371,\"start\":35361},{\"end\":35652,\"start\":35642},{\"end\":35726,\"start\":35716},{\"end\":35873,\"start\":35863},{\"end\":36208,\"start\":36199},{\"end\":36517,\"start\":36508},{\"end\":38688,\"start\":38679}]", "table": "[{\"end\":36506,\"start\":36236},{\"end\":38677,\"start\":36564},{\"end\":38915,\"start\":38714}]", "figure_caption": "[{\"end\":35137,\"start\":35017},{\"end\":35245,\"start\":35151},{\"end\":35311,\"start\":35259},{\"end\":35359,\"start\":35325},{\"end\":35620,\"start\":35373},{\"end\":35640,\"start\":35623},{\"end\":35714,\"start\":35654},{\"end\":35861,\"start\":35728},{\"end\":35975,\"start\":35875},{\"end\":36197,\"start\":35978},{\"end\":36236,\"start\":36210},{\"end\":36564,\"start\":36519},{\"end\":38714,\"start\":38690}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5193,\"start\":5184},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5717,\"start\":5708},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6182,\"start\":6173},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7827,\"start\":7819},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13282,\"start\":13274},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13571,\"start\":13563},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14495,\"start\":14487},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15004,\"start\":14996},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15803,\"start\":15795},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17188,\"start\":17180},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30438,\"start\":30430},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31362,\"start\":31354},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33354,\"start\":33346},{\"end\":33956,\"start\":33948},{\"end\":34440,\"start\":34429}]", "bib_author_first_name": "[{\"end\":39817,\"start\":39811},{\"end\":39839,\"start\":39827},{\"end\":40112,\"start\":40108},{\"end\":40126,\"start\":40120},{\"end\":40138,\"start\":40133},{\"end\":40507,\"start\":40500},{\"end\":40514,\"start\":40513},{\"end\":40525,\"start\":40521},{\"end\":40866,\"start\":40860},{\"end\":40876,\"start\":40872},{\"end\":40891,\"start\":40882},{\"end\":41042,\"start\":41041},{\"end\":41051,\"start\":41047},{\"end\":41059,\"start\":41058},{\"end\":41309,\"start\":41308},{\"end\":41315,\"start\":41314},{\"end\":41569,\"start\":41561},{\"end\":41578,\"start\":41574},{\"end\":41592,\"start\":41585},{\"end\":41607,\"start\":41600},{\"end\":41616,\"start\":41613},{\"end\":41629,\"start\":41621},{\"end\":41866,\"start\":41860},{\"end\":41885,\"start\":41875},{\"end\":42276,\"start\":42271},{\"end\":42287,\"start\":42281},{\"end\":42300,\"start\":42295},{\"end\":42614,\"start\":42607},{\"end\":42629,\"start\":42624},{\"end\":42940,\"start\":42934},{\"end\":42957,\"start\":42951},{\"end\":42975,\"start\":42968},{\"end\":42990,\"start\":42984},{\"end\":43003,\"start\":42998},{\"end\":43018,\"start\":43016},{\"end\":43399,\"start\":43395},{\"end\":43685,\"start\":43677},{\"end\":43699,\"start\":43694},{\"end\":43907,\"start\":43901},{\"end\":44093,\"start\":44087},{\"end\":44107,\"start\":44101},{\"end\":44119,\"start\":44114},{\"end\":44346,\"start\":44342},{\"end\":44363,\"start\":44359},{\"end\":44384,\"start\":44375},{\"end\":44738,\"start\":44730},{\"end\":44753,\"start\":44744},{\"end\":45028,\"start\":45023},{\"end\":45044,\"start\":45038},{\"end\":45059,\"start\":45054},{\"end\":45071,\"start\":45068},{\"end\":45089,\"start\":45082},{\"end\":45318,\"start\":45314},{\"end\":45331,\"start\":45324},{\"end\":45341,\"start\":45338},{\"end\":45348,\"start\":45347},{\"end\":45362,\"start\":45357},{\"end\":45374,\"start\":45368},{\"end\":45388,\"start\":45383},{\"end\":45709,\"start\":45700},{\"end\":45729,\"start\":45725},{\"end\":45749,\"start\":45745},{\"end\":46141,\"start\":46132},{\"end\":46161,\"start\":46157},{\"end\":46490,\"start\":46484},{\"end\":46512,\"start\":46506},{\"end\":46784,\"start\":46778},{\"end\":46806,\"start\":46800},{\"end\":46816,\"start\":46813},{\"end\":47126,\"start\":47120},{\"end\":47141,\"start\":47135},{\"end\":47157,\"start\":47151},{\"end\":47171,\"start\":47167},{\"end\":47473,\"start\":47467},{\"end\":47489,\"start\":47483},{\"end\":47497,\"start\":47490},{\"end\":47510,\"start\":47505},{\"end\":47525,\"start\":47519},{\"end\":47822,\"start\":47818},{\"end\":47832,\"start\":47829},{\"end\":47840,\"start\":47833},{\"end\":47858,\"start\":47850},{\"end\":48243,\"start\":48237},{\"end\":48259,\"start\":48256},{\"end\":48276,\"start\":48272},{\"end\":48293,\"start\":48289},{\"end\":48311,\"start\":48305},{\"end\":48656,\"start\":48650},{\"end\":48671,\"start\":48666},{\"end\":48928,\"start\":48925},{\"end\":48941,\"start\":48935},{\"end\":48955,\"start\":48948},{\"end\":49298,\"start\":49291},{\"end\":49312,\"start\":49305},{\"end\":49324,\"start\":49318},{\"end\":49333,\"start\":49330},{\"end\":49347,\"start\":49338},{\"end\":49358,\"start\":49353},{\"end\":49772,\"start\":49769},{\"end\":49781,\"start\":49777},{\"end\":49798,\"start\":49789},{\"end\":49800,\"start\":49799},{\"end\":49812,\"start\":49809},{\"end\":50141,\"start\":50138},{\"end\":50157,\"start\":50146},{\"end\":50171,\"start\":50166},{\"end\":50173,\"start\":50172},{\"end\":50187,\"start\":50181},{\"end\":50557,\"start\":50553},{\"end\":50567,\"start\":50564},{\"end\":50579,\"start\":50574},{\"end\":50933,\"start\":50927},{\"end\":50943,\"start\":50941},{\"end\":50957,\"start\":50950},{\"end\":50970,\"start\":50962},{\"end\":50983,\"start\":50976},{\"end\":51336,\"start\":51328},{\"end\":51347,\"start\":51343},{\"end\":51360,\"start\":51354},{\"end\":51372,\"start\":51367},{\"end\":51759,\"start\":51756},{\"end\":51772,\"start\":51767},{\"end\":51788,\"start\":51782},{\"end\":51790,\"start\":51789}]", "bib_author_last_name": "[{\"end\":39825,\"start\":39818},{\"end\":39847,\"start\":39840},{\"end\":40118,\"start\":40113},{\"end\":40131,\"start\":40127},{\"end\":40152,\"start\":40139},{\"end\":40158,\"start\":40154},{\"end\":40511,\"start\":40508},{\"end\":40519,\"start\":40515},{\"end\":40531,\"start\":40526},{\"end\":40541,\"start\":40533},{\"end\":40870,\"start\":40867},{\"end\":40880,\"start\":40877},{\"end\":40898,\"start\":40892},{\"end\":41045,\"start\":41043},{\"end\":41056,\"start\":41052},{\"end\":41067,\"start\":41060},{\"end\":41312,\"start\":41310},{\"end\":41323,\"start\":41316},{\"end\":41572,\"start\":41570},{\"end\":41583,\"start\":41579},{\"end\":41598,\"start\":41593},{\"end\":41611,\"start\":41608},{\"end\":41619,\"start\":41617},{\"end\":41634,\"start\":41630},{\"end\":41873,\"start\":41867},{\"end\":41897,\"start\":41886},{\"end\":42279,\"start\":42277},{\"end\":42293,\"start\":42288},{\"end\":42309,\"start\":42301},{\"end\":42622,\"start\":42615},{\"end\":42637,\"start\":42630},{\"end\":42949,\"start\":42941},{\"end\":42966,\"start\":42958},{\"end\":42982,\"start\":42976},{\"end\":42996,\"start\":42991},{\"end\":43014,\"start\":43004},{\"end\":43026,\"start\":43019},{\"end\":43403,\"start\":43400},{\"end\":43692,\"start\":43686},{\"end\":43702,\"start\":43700},{\"end\":43913,\"start\":43908},{\"end\":44099,\"start\":44094},{\"end\":44112,\"start\":44108},{\"end\":44128,\"start\":44120},{\"end\":44357,\"start\":44347},{\"end\":44373,\"start\":44364},{\"end\":44391,\"start\":44385},{\"end\":44728,\"start\":44720},{\"end\":44742,\"start\":44739},{\"end\":44757,\"start\":44754},{\"end\":44762,\"start\":44759},{\"end\":45036,\"start\":45029},{\"end\":45052,\"start\":45045},{\"end\":45066,\"start\":45060},{\"end\":45080,\"start\":45072},{\"end\":45099,\"start\":45090},{\"end\":45322,\"start\":45319},{\"end\":45336,\"start\":45332},{\"end\":45345,\"start\":45342},{\"end\":45355,\"start\":45349},{\"end\":45366,\"start\":45363},{\"end\":45381,\"start\":45375},{\"end\":45395,\"start\":45389},{\"end\":45401,\"start\":45397},{\"end\":45723,\"start\":45710},{\"end\":45743,\"start\":45730},{\"end\":45757,\"start\":45750},{\"end\":45773,\"start\":45759},{\"end\":46155,\"start\":46142},{\"end\":46175,\"start\":46162},{\"end\":46191,\"start\":46177},{\"end\":46504,\"start\":46491},{\"end\":46517,\"start\":46513},{\"end\":46798,\"start\":46785},{\"end\":46811,\"start\":46807},{\"end\":46827,\"start\":46817},{\"end\":47133,\"start\":47127},{\"end\":47149,\"start\":47142},{\"end\":47165,\"start\":47158},{\"end\":47177,\"start\":47172},{\"end\":47481,\"start\":47474},{\"end\":47503,\"start\":47498},{\"end\":47517,\"start\":47511},{\"end\":47529,\"start\":47526},{\"end\":47827,\"start\":47823},{\"end\":47848,\"start\":47841},{\"end\":47861,\"start\":47859},{\"end\":48254,\"start\":48244},{\"end\":48270,\"start\":48260},{\"end\":48287,\"start\":48277},{\"end\":48303,\"start\":48294},{\"end\":48325,\"start\":48312},{\"end\":48664,\"start\":48657},{\"end\":48676,\"start\":48672},{\"end\":48933,\"start\":48929},{\"end\":48946,\"start\":48942},{\"end\":48961,\"start\":48956},{\"end\":49303,\"start\":49299},{\"end\":49316,\"start\":49313},{\"end\":49328,\"start\":49325},{\"end\":49336,\"start\":49334},{\"end\":49351,\"start\":49348},{\"end\":49364,\"start\":49359},{\"end\":49767,\"start\":49758},{\"end\":49775,\"start\":49773},{\"end\":49787,\"start\":49782},{\"end\":49807,\"start\":49801},{\"end\":49818,\"start\":49813},{\"end\":49824,\"start\":49820},{\"end\":50144,\"start\":50142},{\"end\":50164,\"start\":50158},{\"end\":50179,\"start\":50174},{\"end\":50193,\"start\":50188},{\"end\":50562,\"start\":50558},{\"end\":50572,\"start\":50568},{\"end\":50583,\"start\":50580},{\"end\":50939,\"start\":50934},{\"end\":50948,\"start\":50944},{\"end\":50960,\"start\":50958},{\"end\":50974,\"start\":50971},{\"end\":50987,\"start\":50984},{\"end\":51341,\"start\":51337},{\"end\":51352,\"start\":51348},{\"end\":51365,\"start\":51361},{\"end\":51386,\"start\":51373},{\"end\":51392,\"start\":51388},{\"end\":51765,\"start\":51760},{\"end\":51780,\"start\":51773},{\"end\":51793,\"start\":51791}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12242182},\"end\":40034,\"start\":39783},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12592499},\"end\":40424,\"start\":40036},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1480192},\"end\":40820,\"start\":40426},{\"attributes\":{\"id\":\"b3\"},\"end\":41005,\"start\":40822},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10246046},\"end\":41224,\"start\":41007},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9124261},\"end\":41529,\"start\":41226},{\"attributes\":{\"id\":\"b6\"},\"end\":41858,\"start\":41531},{\"attributes\":{\"doi\":\"arXiv:1511.06939\",\"id\":\"b7\"},\"end\":42215,\"start\":41860},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":10537313},\"end\":42518,\"start\":42217},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":31923308},\"end\":42864,\"start\":42520},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206592218},\"end\":43335,\"start\":42866},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":253064918},\"end\":43631,\"start\":43337},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b12\"},\"end\":43853,\"start\":43633},{\"attributes\":{\"id\":\"b13\"},\"end\":44028,\"start\":43855},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":58370896},\"end\":44276,\"start\":44030},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":252403051},\"end\":44635,\"start\":44278},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10612832},\"end\":44974,\"start\":44637},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17048224},\"end\":45279,\"start\":44976},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7369746},\"end\":45639,\"start\":45281},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10795036},\"end\":46059,\"start\":45641},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207178809},\"end\":46446,\"start\":46061},{\"attributes\":{\"id\":\"b21\"},\"end\":46599,\"start\":46448},{\"attributes\":{\"id\":\"b22\"},\"end\":46719,\"start\":46601},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7285098},\"end\":47058,\"start\":46721},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8047550},\"end\":47415,\"start\":47060},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":16274986},\"end\":47762,\"start\":47417},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3329862},\"end\":48169,\"start\":47764},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6844431},\"end\":48594,\"start\":48171},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":207224096},\"end\":48870,\"start\":48596},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4833213},\"end\":49215,\"start\":48872},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4002880},\"end\":49724,\"start\":49217},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5362246},\"end\":50067,\"start\":49726},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6392154},\"end\":50464,\"start\":50069},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":17476857},\"end\":50865,\"start\":50466},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13246021},\"end\":51236,\"start\":50867},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2859106},\"end\":51681,\"start\":51238},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5180076},\"end\":52049,\"start\":51683}]", "bib_title": "[{\"end\":39809,\"start\":39783},{\"end\":40106,\"start\":40036},{\"end\":40498,\"start\":40426},{\"end\":41039,\"start\":41007},{\"end\":41306,\"start\":41226},{\"end\":41559,\"start\":41531},{\"end\":42269,\"start\":42217},{\"end\":42605,\"start\":42520},{\"end\":42932,\"start\":42866},{\"end\":43393,\"start\":43337},{\"end\":43899,\"start\":43855},{\"end\":44085,\"start\":44030},{\"end\":44340,\"start\":44278},{\"end\":44718,\"start\":44637},{\"end\":45021,\"start\":44976},{\"end\":45312,\"start\":45281},{\"end\":45698,\"start\":45641},{\"end\":46130,\"start\":46061},{\"end\":46776,\"start\":46721},{\"end\":47118,\"start\":47060},{\"end\":47465,\"start\":47417},{\"end\":47816,\"start\":47764},{\"end\":48235,\"start\":48171},{\"end\":48648,\"start\":48596},{\"end\":48923,\"start\":48872},{\"end\":49289,\"start\":49217},{\"end\":49756,\"start\":49726},{\"end\":50136,\"start\":50069},{\"end\":50551,\"start\":50466},{\"end\":50925,\"start\":50867},{\"end\":51326,\"start\":51238},{\"end\":51754,\"start\":51683}]", "bib_author": "[{\"end\":39827,\"start\":39811},{\"end\":39849,\"start\":39827},{\"end\":40120,\"start\":40108},{\"end\":40133,\"start\":40120},{\"end\":40154,\"start\":40133},{\"end\":40160,\"start\":40154},{\"end\":40513,\"start\":40500},{\"end\":40521,\"start\":40513},{\"end\":40533,\"start\":40521},{\"end\":40543,\"start\":40533},{\"end\":40872,\"start\":40860},{\"end\":40882,\"start\":40872},{\"end\":40900,\"start\":40882},{\"end\":41047,\"start\":41041},{\"end\":41058,\"start\":41047},{\"end\":41069,\"start\":41058},{\"end\":41314,\"start\":41308},{\"end\":41325,\"start\":41314},{\"end\":41574,\"start\":41561},{\"end\":41585,\"start\":41574},{\"end\":41600,\"start\":41585},{\"end\":41613,\"start\":41600},{\"end\":41621,\"start\":41613},{\"end\":41636,\"start\":41621},{\"end\":41875,\"start\":41860},{\"end\":41899,\"start\":41875},{\"end\":42281,\"start\":42271},{\"end\":42295,\"start\":42281},{\"end\":42311,\"start\":42295},{\"end\":42624,\"start\":42607},{\"end\":42639,\"start\":42624},{\"end\":42951,\"start\":42934},{\"end\":42968,\"start\":42951},{\"end\":42984,\"start\":42968},{\"end\":42998,\"start\":42984},{\"end\":43016,\"start\":42998},{\"end\":43028,\"start\":43016},{\"end\":43405,\"start\":43395},{\"end\":43694,\"start\":43677},{\"end\":43704,\"start\":43694},{\"end\":43915,\"start\":43901},{\"end\":44101,\"start\":44087},{\"end\":44114,\"start\":44101},{\"end\":44130,\"start\":44114},{\"end\":44359,\"start\":44342},{\"end\":44375,\"start\":44359},{\"end\":44393,\"start\":44375},{\"end\":44730,\"start\":44720},{\"end\":44744,\"start\":44730},{\"end\":44759,\"start\":44744},{\"end\":44764,\"start\":44759},{\"end\":45038,\"start\":45023},{\"end\":45054,\"start\":45038},{\"end\":45068,\"start\":45054},{\"end\":45082,\"start\":45068},{\"end\":45101,\"start\":45082},{\"end\":45324,\"start\":45314},{\"end\":45338,\"start\":45324},{\"end\":45347,\"start\":45338},{\"end\":45357,\"start\":45347},{\"end\":45368,\"start\":45357},{\"end\":45383,\"start\":45368},{\"end\":45397,\"start\":45383},{\"end\":45403,\"start\":45397},{\"end\":45725,\"start\":45700},{\"end\":45745,\"start\":45725},{\"end\":45759,\"start\":45745},{\"end\":45775,\"start\":45759},{\"end\":46157,\"start\":46132},{\"end\":46177,\"start\":46157},{\"end\":46193,\"start\":46177},{\"end\":46506,\"start\":46484},{\"end\":46519,\"start\":46506},{\"end\":46800,\"start\":46778},{\"end\":46813,\"start\":46800},{\"end\":46829,\"start\":46813},{\"end\":47135,\"start\":47120},{\"end\":47151,\"start\":47135},{\"end\":47167,\"start\":47151},{\"end\":47179,\"start\":47167},{\"end\":47483,\"start\":47467},{\"end\":47505,\"start\":47483},{\"end\":47519,\"start\":47505},{\"end\":47531,\"start\":47519},{\"end\":47829,\"start\":47818},{\"end\":47850,\"start\":47829},{\"end\":47863,\"start\":47850},{\"end\":48256,\"start\":48237},{\"end\":48272,\"start\":48256},{\"end\":48289,\"start\":48272},{\"end\":48305,\"start\":48289},{\"end\":48327,\"start\":48305},{\"end\":48666,\"start\":48650},{\"end\":48678,\"start\":48666},{\"end\":48935,\"start\":48925},{\"end\":48948,\"start\":48935},{\"end\":48963,\"start\":48948},{\"end\":49305,\"start\":49291},{\"end\":49318,\"start\":49305},{\"end\":49330,\"start\":49318},{\"end\":49338,\"start\":49330},{\"end\":49353,\"start\":49338},{\"end\":49366,\"start\":49353},{\"end\":49769,\"start\":49758},{\"end\":49777,\"start\":49769},{\"end\":49789,\"start\":49777},{\"end\":49809,\"start\":49789},{\"end\":49820,\"start\":49809},{\"end\":49826,\"start\":49820},{\"end\":50146,\"start\":50138},{\"end\":50166,\"start\":50146},{\"end\":50181,\"start\":50166},{\"end\":50195,\"start\":50181},{\"end\":50564,\"start\":50553},{\"end\":50574,\"start\":50564},{\"end\":50585,\"start\":50574},{\"end\":50941,\"start\":50927},{\"end\":50950,\"start\":50941},{\"end\":50962,\"start\":50950},{\"end\":50976,\"start\":50962},{\"end\":50989,\"start\":50976},{\"end\":51343,\"start\":51328},{\"end\":51354,\"start\":51343},{\"end\":51367,\"start\":51354},{\"end\":51388,\"start\":51367},{\"end\":51394,\"start\":51388},{\"end\":51767,\"start\":51756},{\"end\":51782,\"start\":51767},{\"end\":51795,\"start\":51782}]", "bib_venue": "[{\"end\":39893,\"start\":39849},{\"end\":40216,\"start\":40160},{\"end\":40606,\"start\":40543},{\"end\":40858,\"start\":40822},{\"end\":41106,\"start\":41069},{\"end\":41364,\"start\":41325},{\"end\":41678,\"start\":41636},{\"end\":42017,\"start\":41915},{\"end\":42350,\"start\":42311},{\"end\":42676,\"start\":42639},{\"end\":43086,\"start\":43028},{\"end\":43472,\"start\":43405},{\"end\":43675,\"start\":43633},{\"end\":43926,\"start\":43915},{\"end\":44138,\"start\":44130},{\"end\":44442,\"start\":44393},{\"end\":44784,\"start\":44764},{\"end\":45112,\"start\":45101},{\"end\":45442,\"start\":45403},{\"end\":45826,\"start\":45775},{\"end\":46235,\"start\":46193},{\"end\":46482,\"start\":46448},{\"end\":46650,\"start\":46601},{\"end\":46873,\"start\":46829},{\"end\":47221,\"start\":47179},{\"end\":47573,\"start\":47531},{\"end\":47950,\"start\":47863},{\"end\":48363,\"start\":48327},{\"end\":48716,\"start\":48678},{\"end\":49026,\"start\":48963},{\"end\":49453,\"start\":49366},{\"end\":49880,\"start\":49826},{\"end\":50249,\"start\":50195},{\"end\":50649,\"start\":50585},{\"end\":51039,\"start\":50989},{\"end\":51435,\"start\":51394},{\"end\":51849,\"start\":51795}]"}}}, "year": 2023, "month": 12, "day": 17}