{"id": 208290939, "updated": "2023-10-06 21:29:34.925", "metadata": {"title": "PIQA: Reasoning about Physical Commonsense in Natural Language", "authors": "[{\"first\":\"Yonatan\",\"last\":\"Bisk\",\"middle\":[]},{\"first\":\"Rowan\",\"last\":\"Zellers\",\"middle\":[]},{\"first\":\"Ronan\",\"last\":\"Bras\",\"middle\":[\"Le\"]},{\"first\":\"Jianfeng\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Yejin\",\"last\":\"Choi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 11, "day": 26}, "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (77%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1911.11641", "mag": "2998617917", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/BiskZLGC20", "doi": "10.1609/aaai.v34i05.6239"}}, "content": {"source": {"pdf_hash": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1911.11641v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6239/6095", "status": "GOLD"}}, "grobid": {"id": "cc699965868694e693ac20f45233d2a86eb1110b", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/04f4e55e14150b7c48b0287ba77c7443df76ed45.txt", "contents": "\nPIQA: Reasoning about Physical Commonsense in Natural Language\n\n\nYonatan Bisk \nAllen Institute for Artificial Intelligence\n\n\nMicrosoft Research AI\n\n\nCarnegie Mellon University\n\n\nPaul G. Allen School for Computer Science and Engineering\nUniversity of Washington\n\n\nRowan Zellers \nAllen Institute for Artificial Intelligence\n\n\nPaul G. Allen School for Computer Science and Engineering\nUniversity of Washington\n\n\nRonan Le Bras \nAllen Institute for Artificial Intelligence\n\n\nJianfeng Gao \nMicrosoft Research AI\n\n\nYejin Choi \nAllen Institute for Artificial Intelligence\n\n\nPaul G. Allen School for Computer Science and Engineering\nUniversity of Washington\n\n\nPIQA: Reasoning about Physical Commonsense in Natural Language\n923ED266B9F73CCC61B8CC74C323576D\nTo apply eyeshadow without a brush, should I use a cotton swab or a toothpick?Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems.While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains -such as news articles and encyclopedia entries, where text is plentiful -in more physical domains, text is inherently limited due to reporting bias.Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA .Though humans find the dataset easy (95% accuracy), large pretrained models struggle (\u223c77%).We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.\n\nIntroduction\n\nBefore children learn language, they already start forming categories and concepts based on the physical properties of objects around them (Hespos and Spelke 2004).This model of the world grows richer as they learn to speak, but already captures physical commonsense knowledge about everyday objects: their physical properties, affordances, and how they can be manipulated.This knowledge is critical for day-to-day human life, including tasks such as problem solving (what can I use as a pillow when camping?) and expressing needs and desires (bring me a harder pillow).Likewise, we hypothesize that modeling physical commonsense knowledge is a major challenge on the road to true AIcompleteness, including robots that interact with the world and understand natural language.\n\nMuch of physical commonsense can be expressed in language, as the versatility of everyday objects and common concepts eludes other label schemes.However, due to issues of reporting bias, these commonsense properties -facts like 'it is a bad idea to apply eyeshadow with a toothpick' are rarely directly reported.Although much recent progress\n\nTo separate egg whites from the yolk using a water bottle, you should\u2026 b.Place the water bottle and press it against the yolk.Keep pushing, which creates suction and lifts the yolk.\n\n???\n\na!\n\nFigure 1: PIQA : Given a physical goal expressed in natural language, like 'to separate egg whites...,' a model must choose the most sensible solution.Our dataset tests the ability of natural language understanding models to link text to a robust intuitive-physics model of the world.Here, humans easily pick answer a) because separating the egg requires pulling the yolk out, while machines are easily fooled.\n\nhas been made in Natural Language Processing through a shift towards large-scale pretrained representations from unlabeled text (Radford et al. 2018;Devlin et al. 2019;Liu et al. 2019), the bulk of the success of this paradigm has been on core abstract tasks and domains.State-of-theart models can reliably answer questions given an encyclopedia article (Rajpurkar et al. 2016) or recognize named entities (Tjong Kim Sang and De Meulder 2003), but it is not clear whether they can robustly answer questions that require physical commonsense knowledge.\n\nTo study this question and begin bridging the representational gap, we introduce Physical Interaction: Question Answering, or PIQA to evaluate language representations on their knowledge of physical commonsense.We focus on everyday situations with a preference for atypical solutions.Our dataset is inspired by instructables.com, which provides users with instructions on how to build, craft, arXiv:1911.11641v1[cs.CL] 26 Nov 2019 a. Shape, Material, and Purpose\n\n[Goal] Make an outdoor pillow [Sol1] Blow into a tin can and tie with rubber band [Sol2] Blow into a trash bag and tie with rubber band [Goal] To make a hard shelled taco, [Sol1] put seasoned beef, cheese, and lettuce onto the hard shell.\n\n[Sol2] put seasoned beef, cheese, and lettuce into the hard shell.\n\n[Goal] How do I find something I lost on the carpet?\n\n[Sol1] Put a solid seal on the end of your vacuum and turn it on.\n\n[Sol2] Put a hair net on the end of your vacuum and turn it on.\n\n\nb. Commonsense Convenience\n\n[Goal] How to make sure all the clocks in the house are set accurately?\n\n[Sol1] Get a solar clock for a reference and place it just outside a window that gets lots of sun.Use a system of call and response once a month, having one person stationed at the solar clock who yells out the correct time and have another person move to each of the indoor clocks to check if they are showing the right time.Adjust as necessary.\n\n[Sol2] Replace all wind-ups with digital clocks.That way, you set them once, and that's it.Check the batteries once a year or if you notice anything looks a little off.\n\nFigure 2: PIQA covers a broad array of phenomena.Above are two categories of example QA pairs.Left are examples that require knowledge of basic properties of the objects (flexibility, curvature, and being porous), while on the Right both answers may be technically correct but one is more convenient and preferable.\n\nbake, or manipulate objects using everyday materials.We asked annotators to provide semantic perturbations or alternative approaches which are otherwise syntactically and topically similar to ensure physical knowledge is targeted.\n\nThe dataset is further cleaned of basic artifacts using the AFLite algorithm introduced in (Sakaguchi et al. 2020;Sap et al. 2019) which is an improvement on adversarial filtering (Zellers et al. 2018;Zellers et al. 2019b).\n\nThroughout this work we first detail the construction of our new benchmark for physical commonsense.Second, we show that popular approaches to large-scale language pretraining, while highly successful on many abstract tasks, fall short when a physical model of the world is required.Finally, our goal is to elicit further research into building language representations that capture details of the real world.To these ends, we perform error and corpora analyses to provide insights for future work.\n\n\nDataset\n\nWe introduce a new dataset, PIQA , for benchmarking progress in physical commonsense understanding.The underlying task is multiple choice question answering: given a question q and two possible solutions s 1 , s 2 , a model or a human must choose the most appropriate solution, of which exactly one is correct.We collect data with how-to instructions as a scaffold, and use state-of-the-art approaches for handling spurious biases, which we will discuss below.\n\n\nInstructables as a source of physical commonsense\n\nOur goal is to construct a resource that requires concrete physical reasoning.To achieve this, we provide a prompt to the annotators derived from instructables.com.The instructables website is a crowdsourced collection of instructions for doing everything from cooking to car repair.In most cases, users provide images or videos detailing each step and a list of tools that will be required.Most goals are simultaneously rare and unsurprising.While an annotator is unlikely to have built a UV-Flourescent steampunk lamp or made a backpack out of duct tape, it is not surprising that someone interested in home crafting would create these, nor will the tools and materials be unfamiliar to the average person.Using these examples as the seed for their annotation, helps remind annotators about the less prototypical uses of everyday objects.Second, and equally important, is that instructions build on one another.This means that any QA pair inspired by an instructable is more likely to explicitly state assumptions about what preconditions need to be met to start the task and what postconditions define success.\n\n\nCollecting data through goal-solution pairs\n\nUnlike traditional QA tasks, we define our dataset in terms of Goal and Solution pairs (see Figure 2 for example Goal-Solution pairs and types of physical reasoning).The Goal in most cases can be viewed as indicating a post-condition and the solutions indicate the procedure for accomplishing this.The more detailed the goal, the easier it is for annotators to write both correct and incorrect solutions.As noted above, the second component of our annotation design is reminding people to think creatively.We initially experimented with asking annotators for (task, tool) pairs via unconstrained prompts, but found that reporting bias swamped the dataset.In particular, when thinking about how to achieve a goal, people most often are drawn to prototypical solutions and look for tools in the kitchen (e.g.forks and knives) or the garage (e.g.hammers and drills).They rarely considered the literal hundreds of other everyday objects that might be in their own homes (e.g.sidewalk chalk, shower curtains, etc).\n\nTo address this, and flatten the distribution of referenced objects (see Figure 5), we prompt the annotations with links to instructables.Specifically, annotators were asked to glance at the instructions of an instructable and pull out or have it inspire them to construct two component tasks.They would then articulate the goal (often centered on atypical materials) and how to achieve it.In addition, we asked them to provide a permutation to their own solution which makes it invalid, often subtly (Figure 3).To further assist diversity we seed annotators with instructables drawn from six categories (costume, outside, craft, home, food, and workshop).We asked that two examples be drawn per instructable to encourage one of them to come later in the process and require precise articulation of pre-conditions.\n\nDuring validation, examples with low agreement were removed from the data.This often meant that correct examples were removed that required expert level knowledge of a domain (e.g.special woodworking terminology) which should not fall under the umbrella of \"commonsense.\"Because, we focus on human generated tricks, annotators were free to come up with clever ways to hide deception.Often, this meant making very subtle changes to the solution to render it incorrect.In these cases, the two solutions may differ by as little as one word.We found that annotations used both simple linguistic tricks (e.g.negation and numerical changes) and often swapped a key action or item for another that was topically similar but not helpful for completing the given goal.For this reason, our interface also includes a diff button which highlights where the solutions differ.This improved annotator accuracy and speed substantially.Annotator pay averaged > 15$/hr according to both self-reporting on turkerview.com and our timing calculations.\n\n\nStatistics\n\nIn total our dataset is comprised of over 16,000 training QA pairs with an additional \u223c2K and \u223c3k held out for development and testing, respectively.Our goals, as tokenized by Spacy,3 average 7.8 words and both correct and incorrect solutions average 21.3 words.In total, this leads to over 3.7 million lexical tokens in the training data.\n\nFigure 4 shows a plot of the correct and incorrect sequence lengths (as tokenized by the GPT BPE tokenizer), with the longest 1% of the data removed.While there are minor differences, the two distributions are nearly identical.\n\nWe also analyzed the overlap in the vocabulary and find that in all cases (noun, verb, adjective, and adverb) we see at least an 85% overlap between words used in correct and incorrect solutions.In total we have 6,881 unique nouns, 2,493 verbs, 2,263 adjectives, and 604 adverbs in the training data..The most common of each are plotted in Figure 5 alongside their cumulative distributions.Again, this helps verify that the dataset revolves very heavily around physical phenomena, properties, and manipulations.For example, the top adjectives include state (dry, clean, hot) and shape (small, sharp, flat); adverbs include temporal conditions (then, when) and manner (quickly, carefully, completely).These properties often differentiate correct from incorrect answers, as shown in examples throughout the paper.We also color words according to their concreteness score (Brysbaert, Warriner, and Kuperman 2014), though many \"abstract\" words have concrete realizations in our dataset.\n\n\nRemoving Annotation Artifacts\n\nAs noted previously, we use AFLite (Sakaguchi et al. 2020) to remove stylistic artifacts and trivial examples from the data, which have been shown to artificially inflate model performance on previous NLI benchmarks (Poliak et al. 2018;Gururangan et al. 2018).The AFLite algorithm performs a systematic data bias reduction: it discards instances whose given feature representations are collectively highly indicative of the target label.In practice, we use 5,000 examples from the original dataset to fine-tune BERT-Large for this task and compute the corresponding embeddings of all remaining instances.AFLite uses an ensemble of linear classifiers trained on random subsets of the data to determine whether these pre-computed embeddings are strong  We see that the vast majority of concepts focus on physical properties (e.g.small, hot, plastic, wooden) and how objects can be manipulated (e.g.cut, cover, soak, push).Additionally, we see strongly zipfian behavior in all tags but the adverbs.Words are colored by the average concreteness scores presented by (Brysbaert, Warriner, and Kuperman 2014).\n\nindicators of the correct answer option.Instead of having to specifically identify the possible sources of biases, this approach enables unsupervised data bias reduction by relying on state-of-the-art methods to uncover undesirable annotation artifacts.For more information about AFLite, please refer to (Sakaguchi et al. 2020).\n\n\nExperiments\n\nIn this section, we test the performance of state-of-theart natural language understanding models on our dataset, PIQA.In particular, we consider the following three largescale pretrained transformer models: a. GPT (Radford et al. 2018) is a model that processes text left-to-right, and was pretrained using a language modeling objective.We use the original 124M parameter GPT model.b.BERT (Devlin et al. 2019) is a model that process text bidirectionally, and thus was pretrained using a special masked language modeling objective.We use BERT-Large with 340M parameters.c.RoBERTa (Liu et al. 2019) is a version of the BERT model that was made to be significantly more robust through pretraining on more data and careful validation of the pre-training hyperparameters.We use RoBERTa-Large, which has 355M parameters.We follow standard best practices in adapting these models for two-way classification.We consider the two solution choices independently: for each choice, the model is provided the goal, the solution choice, and a special [CLS] token.At the final layer of the transformer, we extract the hidden states corresponding to the positions of each [CLS] token.We apply a linear transformation to each hidden state and apply a softmax over the two options: this approximates the probability that the correct solution is option A or B. During finetuning, we train the model using a cross-entropy loss over the two options.For GPT, we follow the original implementation and include an additional language modeling loss, which improved training stability.\n\nGenerally, we found that finetuning was often unstable with some hyperparameter configurations leading to validation performance around chance, particularly for BERT.We follow best practices in using a grid search over learning rates, batch sizes, and the number of training epochs for each model, and report the best-scoring configuration as was found on the validation set.For all models and experiments, we used the transformers library and truncated examples at 150 tokens, which affects 1% of the data.Manual inspection of the development errors show that some \"mistakes\" are actually correct but required a websearch to verify.Human performance was calculated by a majority vote.Annotators were chosen to participate that achieved \u226590% on the qualification HIT from before.It is therefore, completely reasonable that automated methods trained on large web crawls may eventually surpass human performance here.Human evaluation was performed on development data, and the train, development, and test folds were automatically produced by AFLite.\n\n\nResults\n\nWe present our results in Table 1.As the dataset was constructed to be adversarial to BERT, it is not surprising that it performs the worst of three models despite generally outperforming GPT on most other benchmarks.Comparing GPT and RoBERTa we see that despite more training data, a larger vocabulary, twice the number of parameters and careful construction of robust training, there is only a 8pt performance gain and RoBERTa still falls roughly 18 points short of human performance on this task.As noted throughout, exploring this gap is precisely the purpose for PIQA existing and which facets of the dataset fool RoBERTa is the focus of the remainder of this paper.\n\n\nAnalysis\n\nIn this section, we unpack the results of state-of-the-art models on PIQA.In particular, we take a look at the errors made by the top-performing model RoBERTa, as a view towards the physical commonsense knowledge that can be learned through language alone.\n\n\nPIQA as a diagnostic for physical understanding\n\nThe setup of PIQA allows us to use it to probe the inner workings of deep pretrained language models, and to determine the extent of their physical knowledge.In this way, our dataset can augment prior work on studying to what extent models such as BERT understand syntax (Goldberg 2019).However, while syntax is a well studied problem within linguistics, physical commonsense does not have as rich a lit- erature to borrow from, making its dimensions challenging to pin down.Simple concepts.Understanding the physical world requires a deep understanding of simple concepts, such as \"water\" or \"ketchup,\" and their affordances and interactions with respect to other concepts.Though our dataset covers interactions between and with common objects, we can analyze the space of concepts in the dataset by performing a string alignment between solution pairs.Two solution choices that differ by editing a single phrase must by definition test the commonsense understanding of that phrase.\n\nIn Figure 6 we show the distribution of the edit distance between solution choices.We compute edit distance over tokenized and lowercased strings with punctuation removed.We use a cost of 1 for edits, insertions, and deletions.Most of the dataset covers simple edits between the two solution choices: roughly 60% of the dataset in both validation and training involves a 1-2 word edit between solutions.In the bottom of Figure 6, we show that the dataset complexity Though certain concepts such as water occur quite frequently, RoBERTa nevertheless finds those concepts difficult, with 75% accuracy.Additionally, on common relations such as 'cold', 'on', 'before', and 'after' RoBERTa performs roughly at chance.\n\ngenerally increases with the edit distance between the solution pairs.Nevertheless, the head of the distribution represents a space that is simple to study.Single-word edits.In Figure 7, we plot the accuracy of RoBERTa among dataset examples that differ by a single word.More formally, we consider examples (q, s 1 , s 2 ) whereby moving from s 1 to s 2 , or vice versa, requires editing a given word w. 4 We show examples of words w that occur frequently in both the training and validation splits of the dataset, which allows RoBERTa to refine representations of these concepts during training and gives us a large enough sample size to reliably estimate model performance.\n\nAs shown, RoBERTa struggles to understand certain highly flexible relations.In particular, Figure 7 highlights the difficulty of correctly answering questions that differ by the words 'before,' 'after', 'top', and 'bottom': RoBERTa performs nearly at chance when encountering these.\n\nInterestingly, the concepts shown in Figure 7 suggest that RoBERTa also struggles to understand many common, more versatile, physical concepts.Though there are 300 training examples wherein the solution choices s 1 , s 2 differ by the word 'water.'RoBERTa performs worse than average on these replacements.On the other hand, RoBERTa does much better at certain nouns, such as 'spoon.'\n\nCommon replacements in PIQA.We dig into this The most common replacements for three selected words: 'water,' 'spoon,' and 'freeze.'These cover several key dimensions: 'water' is a broad noun with many properties and affordances, whereas 'spoons' are much narrower in scope.Perhaps as a result, RoBERTa performs much butter at examples where 'spoon' is the pivot word (90%) versus 'water' (75%).Freeze has an accuracy of 66% on the validation set, and shows that verbs are challenging as well.\n\nfurther in Figure 8, where we showcase the most common replacements for three examples: 'water,' 'spoon,' and 'freeze.'While 'water' is prevalent in the training set, it is also highly versatile.One can try to substitute it with a variety of different household items, such as 'milk' or 'alcohol,' often to disastrous effects.However, 'spoons' have fewer challenging properties.A spoon cannot generally be substituted with a utensil that is sharp or has prongs, such as a fork, a knife, or a toothpick.RoBERTa obtains high accuracy on 'spoon' examples, which suggests that it might understand this simple affordance, but does not capture the long tail of affordances associated with 'water.'\n\n\nQualitative results\n\nOur analysis thus far has been on simple-to-analyze single word expressions, where we have shown that the stateof-the-art language model, RoBERTa, struggles at a nuanced understanding of key commonsense concepts, such as relations.To further probe the knowledge gap of these strong models, we present qualitative examples in Figure 9.\n\nThe examples are broadly representative of larger patterns: RoBERTa can recognize clearly ridiculous generations (Figure 9, top left) and understands differences between some commonsense concepts (bottom left).It's important to note, that in both cases the correct answer is prototypical and something we might expect the models to have seen before.However, it struggles to tell the difference between sub-\n\n\nCorrect examples\n\n[Goal] Best way to pierce ears. [Sol1] It is best to go to a professional to get your ear pierced to avoid medical problems later.\n\n[Sol2] The best way to pierce your ears would be to insert a needle half inch thick into the spot you want pierced.\n\n[Goal] How do you reduce wear and tear on the nonstick finish of muffin pans?[Sol1] Make sure you use paper liners to protect the nonstick finish when baking muffins and cupcakes in muffin pans.\n\n[Sol2] Make sure you use grease and flour to protect the nonstick finish when baking muffins and cupcakes in muffin pans.\n\n\nIncorrect examples\n\n[Goal] How can I quickly and easily remove strawberry stems?\n\n[Sol1] Take a straw and from the top of the strawberry push the straw through the center of the strawberry until the stem pops off.\n\n[Sol2] Take a straw and from the bottom of the strawberry push the straw through the center of the strawberry until the stem pops off.\n\n[Goal] how to add feet to a coaster.\n\n[Sol1] cut four slices from a glue stick, and attatch to the coaster with glue.\n\n[Sol2] place a board under the coaster, and secure with zip ties and a glue gun.tle relations such as top and bottom (top right of Figure 9).Moreover, it struggles with identifying non-prototypical situations (bottom right).Though using a gluestick as feet for a coaster is uncommon, to a human familiar with these concepts we can visualize the action and its result to verify that the goal has been achieved.Overall, these examples suggest that physical understanding -particularly involving novel combinations of common objects -challenges models that were pretrained on text only.\n\n\nRelated Work\n\nPhysical understanding is broad domain that touches on everything from scientific knowledge (Schoenick et al. 2016) to the interactive acquisition of knowledge by embodied agents (Thomason et al. 2016).To this end, work related to the goals of our benchmark span the NLP, Computer Vision and Robotics communities.\n\nLanguage.Within NLP, in addition to large scale models, there has also been progress on reasoning about cause and effect effects/implications within these models (Bosselut et al. 2019), extracting knowledge from them (Petroni et al. 2019), and investigating where large scale language models fail to capture knowledge of tools and elided procedural knowledge in recipes (Bisk et al. 2019).The notion of procedural knowledge and instruction following is a more general related task within vision and robotics.From text alone, work has shown that much can be understood about the implied physical situations of verb usage (Forbes and Choi 2017) and relative sizes of objects (Elazar et al. 2019).\n\nVision.Physical knowledge can be discovered and evaluated within the visual world.Research has studied predicting visual relationships in images (Krishna et al. 2016) and as well as actions and their dependent objects (Yatskar, Zettlemoyer, and Farhadi 2016).Relatedly, the recent HAKE dataset (Li et al. 2019) specifically annotates which object/body-parts are essential to completing or defining an action.Image data also allows for studying the concreteness of nouns and provides a natural path forward for further investigation (Hessel, Mimno, and Lee 2018).Related to physical commonsense, research in visual commonsense has studied intuitive physics (Wu et al. 2017), cause-effect relationships (Mottaghi et al. 2016), and what can be reasonably inferred beyond a single image (Zellers et al. 2019a).\n\nRobotics.Learning from interaction and intuitive physics (Agrawal et al. 2016) can also be encoded as priors when exploring the world (Byravan et al. 2018) and internal models of physics, shape, and material strength enable advances in tool usage (Toussaint et al. 2018) or construction (Nair, Balloch, and Chernova 2019).Key to our research aims in this work is helping to build language tools which capture enough physical knowledge to speed up the bootstrapping of robotic-language applications.Language tools should provide strong initial priors for learning (Tellex et al. 2011;Matuszek 2018) that are then refined through interaction and dialogue (Gao et al. 2016).\n\n\nConclusion\n\nWe have evaluated against large-scale pretrained models as they are in vogue as the de facto standard of progress within NLP, but are primarily interested in their performance and failings as a mechanism for advancing the position that learning about the world from language alone, is limiting.Future research, may \"match\" humans on our dataset by finding a large source of in-domain data and fine-tuning heavily, but this is very much not the point.Philosophically, knowledge should be learned from interaction with the world to eventually be communicated with language.\n\nIn this work we introduce the Physical Interaction: Question Answering or PIQA benchmark for evaluating and studying physical commonsense understanding in natural language models.We find the best available pretrained models lack an understanding of some of the most basic physical properties of the world around us.Our goal with PIQA is to provide insight and a benchmark for progress towards language representations that capture knowledge traditionally only seen or experienced, to enable the construction of language models useful beyond the NLP community.\n\nFigure 3 :\n3\nFigure 3: In the HIT design the instructable provides inspiration to think out-of-the-box (1 Sock, 3 Products) and annotators are asked for 1. a physical goal, 2. a valid solution, and 3. a trick.The trick should sound reasonable, but be wrong often due to a subtle misunderstanding of preconditions or physics.Additional HITs (not shown) were run for qualification prior to this stage and validation afterwards.2\n\n\nFigure 4 :\n4\nFigure 4: Sentence length distributions for both correct solutions and tricks are nearly identical across the training set.\n\n\nFigure 5 :\n5\nFigure5: Here we show the frequency distributions for the top seventy-five words tagged by Spacy as noun, verb, adverb or adjective.We see that the vast majority of concepts focus on physical properties (e.g.small, hot, plastic, wooden) and how objects can be manipulated (e.g.cut, cover, soak, push).Additionally, we see strongly zipfian behavior in all tags but the adverbs.Words are colored by the average concreteness scores presented by(Brysbaert, Warriner, and Kuperman 2014).\n\n\nFigure 6 :\n6\nFigure 6: Breaking down PIQA by edit distance between solution choices.Top: Cumulative histogram of examples in the validation and training sets, in terms of minimum edit distance d between the two solution choices.The majority of the dataset consists of small tweaks between the two solution pairs; nevertheless, this is enough to confuse state-ofthe-art NLP models.Bottom: RoBERTa accuracy over validation examples with a minimum edit distance of d.Dataset difficulty increases somewhat as the two solution pairs are allowed to drift further apart.\n\n\nFigure 7 :\n7\nFigure7: Common concepts as a window to RoBERTa's understanding of the physical world.We consider validation examples (q, s 1 , s 2 ) wherein s 1 and s 2 differ from each other by a given word w.Left, we show the validation accuracy for common words w, while the number of dataset examples are shown right.Though certain concepts such as water occur quite frequently, RoBERTa nevertheless finds those concepts difficult, with 75% accuracy.Additionally, on common relations such as 'cold', 'on', 'before', and 'after' RoBERTa performs roughly at chance.\n\n\n\n\nFigure8: The most common replacements for three selected words: 'water,' 'spoon,' and 'freeze.'These cover several key dimensions: 'water' is a broad noun with many properties and affordances, whereas 'spoons' are much narrower in scope.Perhaps as a result, RoBERTa performs much butter at examples where 'spoon' is the pivot word (90%) versus 'water' (75%).Freeze has an accuracy of 66% on the validation set, and shows that verbs are challenging as well.\n\n\nFigure 9 :\n9\nFigure 9: Qualitative analysis of RoBERTa's predictions with.Left: Two examples that RoBERTa gets right.Right: two examples that RoBERTa gets incorrect.Short phrases that differ between solution 1 and solution 2 are shown in bold and italics.\n\nIn addition to this design, we also include a qualification HIT which contained well constructed and underspecified (goal, solution) pairs. Annotators had to successfully (>80%) identify which were well formed to participate in the main HIT. Data was collected in batches of several thousand triples and validated by other annotators for correctness. Users will low agreement were de-qualed.\nhttps://spacy.io -all data was collected in English.\nWe additionally allow for an additional insertion; this helps to capture simple phrases like going from 'water' to 'olive oil.' Nevertheless, these multiword expressions tend to be less common, which is why we omit them in Figure7.\nAcknowledgementsWe thank the anonymous reviewers for their insightful suggestions.This research was supported in part by NSF (IIS-1524371, IIS-1714566), DARPA under the CwC program through the ARO (W911NF-15-1-0543), DARPA under the MCS program through NIWC Pacific (N66001-19-2-4031), and the NSF-GRFP No. DGE-1256082.Computations on beaker.orgwere supported in part by Google Cloud.\n. P Agrawal, A Nair, P Abbeel, J Malik, S Levine, \n\nLearning to poke by poking: Experiential learning of intuitive physics. In NeurIPS\n\nBenchmarking hierarchical script knowledge. Y Bisk, J Buys, K Pichotta, Y Choi, NAACL-HLT. 2019\n\nSe3-pose-nets: Structured deep dynamics models for visuomotor planning and control. A Bosselut, H Rashkin, M Sap, C Malaviya, A Celikyilmaz, Y Choi, M Warriner, A B Kuperman, V , Concreteness ratings for 40 thousand generally known english word lemmas. A Byravan, F Leeb, F Meier, D Fox, 2019. 2014. 2018ICRA\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL-HLT. 2019\n\nHow large are lions? inducing distributions over quantitative attributes. Y Elazar, A Mahabal, D Ramachandran, T Bedrax-Weiss, D Roth, ACL. 2019\n\nVerb physics: Relative physical knowledge of actions and objects. M Forbes, Y Choi, ACL. 2017\n\nPhysical causality of action verbs in grounded language understanding. Q Gao, M Doering, S Yang, J Chai, ACL. 2016\n\nAssessing BERT's Syntactic Abilities. Y Goldberg, arXiv:1901.052872019\n\nAnnotation artifacts in natural language inference data. S Gururangan, S Swayamdipta, O Levy, R Schwartz, S Bowman, N A Smith, NAACL-HLT. 2018\n\nConceptual precursors to language. S J Hespos, E S Spelke, Nature. 4302004\n\nQuantifying the visual concreteness of words and topics in multimodal datasets. J Hessel, D Mimno, L Lee, NAACL-HLT. 2018\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, M Bernstein, L Fei-Fei, arXiv:1602.073322016\n\nY.-L Li, L Xu, X Huang, X Liu, Z Ma, M Chen, S Wang, H.-S Fang, C Lu, Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1904.06539arXiv:1907.11692RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019. 2019arXiv preprintHake: Human activity knowledge engine\n\nGrounded Language Learning: Where Robotics and NLP Meet. C Matuszek, IJCAI. 2018\n\nlearning to predict the effect of forces in images. R Mottaghi, M Rastegari, A Gupta, A Farhadi, Leibe, B.Matas, J.Sebe, N.and Welling, M.2016what happens if.\n\nTool Macgyvering: Tool Construction Using Geometric Reasoning. L Nair, J Balloch, S Chernova, F Rocktschel, T Lewis, P Bakhtin, A Wu, Y Miller, A H Riedel, S , Joint Conference on Lexical and Computational Semantics (StarSem). Emnlp, A Poliak, J Naradowsky, A Haldar, R Rudinger, B Van Durme, 2019. 2019. 2018Hypothesis Only Baselines in Natural Language Inference\n\nImproving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018\n\nSquad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, EMNLP. 2016\n\nWinogrande: An adversarial winograd schema challenge at scale. K Sakaguchi, R Le Bras, C Bhagavatula, Y Choi, H Rashkin, D Chen, R Le Bras, Y Choi, Socialiqa: Commonsense reasoning about social interactions. 2020. 2019EMNLP\n\nUnderstanding natural language commands for robotic navigation and mobile manipulation. C Schoenick, P Clark, O Tafjord, P Turney, O Etzioni, S Kollar, T Dickerson, S Walter, M R Banerjee, A G Teller, S Roy, N , Proceedings of the National Conference on Artificial Intelligence. the National Conference on Artificial Intelligence2016. 2011Moving beyond the turing test with the allen ai science challenge\n\nLearning Multi-Modal Grounded Linguistic Semantics by Playing \"I Spy. J Thomason, J Sinapov, M Svetlik, P Stone, R J Mooney, IJCAI. 2016\n\nIntroduction to the CoNLL-2003 shared task: Language-independent named entity recognition. Tjong Kim, Sang , E F De Meulder, F , NAACL. 2003\n\nDifferentiable physics and stable modes for tooluse and manipulation planning. M Toussaint, K R Allen, K A Smith, J Tenenbaum, RSS. 2018\n\nSituation recognition: Visual semantic role labeling for image understanding. J Wu, E Lu, P Kohli, B Freeman, J Tenenbaum, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Neurips, M Yatskar, L Zettlemoyer, A Farhadi, CVPR. 2017. 2016Learning to see physics via visual de-animation\n\nSWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. R Zellers, Y Bisk, R Schwartz, Y Choi, EMNLP. 2018\n\nFrom recognition to cognition: Visual commonsense reasoning. R Zellers, Y Bisk, A Farhadi, Y Choi, R Holtzman, A Bisk, Y Farhadi, A Choi, Y , CVPR. 2019a. 2019bACL\n", "annotations": {"author": "[{\"end\":263,\"start\":66},{\"end\":409,\"start\":264},{\"end\":470,\"start\":410},{\"end\":508,\"start\":471},{\"end\":651,\"start\":509}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":74},{\"end\":277,\"start\":270},{\"end\":423,\"start\":416},{\"end\":483,\"start\":480},{\"end\":519,\"start\":515}]", "author_first_name": "[{\"end\":73,\"start\":66},{\"end\":269,\"start\":264},{\"end\":415,\"start\":410},{\"end\":479,\"start\":471},{\"end\":514,\"start\":509}]", "author_affiliation": "[{\"end\":124,\"start\":80},{\"end\":148,\"start\":126},{\"end\":177,\"start\":150},{\"end\":262,\"start\":179},{\"end\":323,\"start\":279},{\"end\":408,\"start\":325},{\"end\":469,\"start\":425},{\"end\":507,\"start\":485},{\"end\":565,\"start\":521},{\"end\":650,\"start\":567}]", "title": "[{\"end\":63,\"start\":1},{\"end\":714,\"start\":652}]", "venue": null, "abstract": "[{\"end\":1712,\"start\":748}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1891,\"start\":1867},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3601,\"start\":3580},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3620,\"start\":3601},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3636,\"start\":3620},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3829,\"start\":3806},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3894,\"start\":3858},{\"end\":4505,\"start\":4499},{\"end\":4557,\"start\":4551},{\"end\":4611,\"start\":4605},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6246,\"start\":6223},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6262,\"start\":6246},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6333,\"start\":6312},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6354,\"start\":6333},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13058,\"start\":13035},{\"end\":13236,\"start\":13216},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13259,\"start\":13236},{\"end\":14101,\"start\":14061},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14431,\"start\":14408},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14683,\"start\":14663},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14857,\"start\":14838},{\"end\":15045,\"start\":15021},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18347,\"start\":18333},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24834,\"start\":24811},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24920,\"start\":24898},{\"end\":25272,\"start\":25251},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25422,\"start\":25404},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25727,\"start\":25707},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25896,\"start\":25875},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25988,\"start\":25948},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26039,\"start\":26024},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26291,\"start\":26262},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26402,\"start\":26386},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26453,\"start\":26431},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26535,\"start\":26513},{\"end\":26616,\"start\":26595},{\"end\":26692,\"start\":26672},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26808,\"start\":26785},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26859,\"start\":26825},{\"end\":27121,\"start\":27101},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27134,\"start\":27121},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27208,\"start\":27191}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28785,\"start\":28357},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28924,\"start\":28786},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29422,\"start\":28925},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29988,\"start\":29423},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30556,\"start\":29989},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31017,\"start\":30557},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31275,\"start\":31018}]", "paragraph": "[{\"end\":2503,\"start\":1728},{\"end\":2846,\"start\":2505},{\"end\":3029,\"start\":2848},{\"end\":3034,\"start\":3031},{\"end\":3038,\"start\":3036},{\"end\":3450,\"start\":3040},{\"end\":4003,\"start\":3452},{\"end\":4467,\"start\":4005},{\"end\":4707,\"start\":4469},{\"end\":4775,\"start\":4709},{\"end\":4829,\"start\":4777},{\"end\":4896,\"start\":4831},{\"end\":4961,\"start\":4898},{\"end\":5063,\"start\":4992},{\"end\":5411,\"start\":5065},{\"end\":5581,\"start\":5413},{\"end\":5898,\"start\":5583},{\"end\":6130,\"start\":5900},{\"end\":6355,\"start\":6132},{\"end\":6855,\"start\":6357},{\"end\":7327,\"start\":6867},{\"end\":8494,\"start\":7381},{\"end\":9551,\"start\":8542},{\"end\":10367,\"start\":9553},{\"end\":11399,\"start\":10369},{\"end\":11753,\"start\":11414},{\"end\":11982,\"start\":11755},{\"end\":12966,\"start\":11984},{\"end\":14102,\"start\":13000},{\"end\":14432,\"start\":14104},{\"end\":16008,\"start\":14448},{\"end\":17058,\"start\":16010},{\"end\":17741,\"start\":17070},{\"end\":18010,\"start\":17754},{\"end\":19045,\"start\":18062},{\"end\":19759,\"start\":19047},{\"end\":20436,\"start\":19761},{\"end\":20720,\"start\":20438},{\"end\":21106,\"start\":20722},{\"end\":21600,\"start\":21108},{\"end\":22293,\"start\":21602},{\"end\":22651,\"start\":22317},{\"end\":23059,\"start\":22653},{\"end\":23210,\"start\":23080},{\"end\":23327,\"start\":23212},{\"end\":23523,\"start\":23329},{\"end\":23646,\"start\":23525},{\"end\":23729,\"start\":23669},{\"end\":23862,\"start\":23731},{\"end\":23998,\"start\":23864},{\"end\":24036,\"start\":24000},{\"end\":24117,\"start\":24038},{\"end\":24702,\"start\":24119},{\"end\":25032,\"start\":24719},{\"end\":25728,\"start\":25034},{\"end\":26536,\"start\":25730},{\"end\":27209,\"start\":26538},{\"end\":27795,\"start\":27224},{\"end\":28356,\"start\":27797},{\"end\":28784,\"start\":28371},{\"end\":28923,\"start\":28800},{\"end\":29421,\"start\":28939},{\"end\":29987,\"start\":29437},{\"end\":30555,\"start\":30003},{\"end\":31016,\"start\":30560},{\"end\":31274,\"start\":31032}]", "formula": null, "table_ref": "[{\"end\":17103,\"start\":17102}]", "section_header": "[{\"end\":1726,\"start\":1714},{\"end\":4990,\"start\":4964},{\"end\":6865,\"start\":6858},{\"end\":7379,\"start\":7330},{\"end\":8540,\"start\":8497},{\"end\":11412,\"start\":11402},{\"end\":12998,\"start\":12969},{\"end\":14446,\"start\":14435},{\"end\":17068,\"start\":17061},{\"end\":17752,\"start\":17744},{\"end\":18060,\"start\":18013},{\"end\":22315,\"start\":22296},{\"end\":23078,\"start\":23062},{\"end\":23667,\"start\":23649},{\"end\":24717,\"start\":24705},{\"end\":27222,\"start\":27212},{\"end\":28368,\"start\":28358},{\"end\":28797,\"start\":28787},{\"end\":28936,\"start\":28926},{\"end\":29434,\"start\":29424},{\"end\":30000,\"start\":29990},{\"end\":31029,\"start\":31019}]", "table": null, "figure_caption": "[{\"end\":28785,\"start\":28370},{\"end\":28924,\"start\":28799},{\"end\":29422,\"start\":28938},{\"end\":29988,\"start\":29436},{\"end\":30556,\"start\":30002},{\"end\":31017,\"start\":30559},{\"end\":31275,\"start\":31031}]", "figure_ref": "[{\"end\":3048,\"start\":3047},{\"end\":5591,\"start\":5590},{\"end\":8642,\"start\":8641},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9634,\"start\":9633},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10063,\"start\":10062},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11763,\"start\":11762},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12332,\"start\":12331},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19058,\"start\":19057},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19475,\"start\":19474},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19946,\"start\":19945},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20537,\"start\":20536},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20767,\"start\":20766},{\"end\":21621,\"start\":21620},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22650,\"start\":22649},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22775,\"start\":22774},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24258,\"start\":24257}]", "bib_author_first_name": "[{\"end\":32341,\"start\":32340},{\"end\":32352,\"start\":32351},{\"end\":32360,\"start\":32359},{\"end\":32370,\"start\":32369},{\"end\":32379,\"start\":32378},{\"end\":32519,\"start\":32518},{\"end\":32527,\"start\":32526},{\"end\":32535,\"start\":32534},{\"end\":32547,\"start\":32546},{\"end\":32656,\"start\":32655},{\"end\":32668,\"start\":32667},{\"end\":32679,\"start\":32678},{\"end\":32686,\"start\":32685},{\"end\":32698,\"start\":32697},{\"end\":32713,\"start\":32712},{\"end\":32721,\"start\":32720},{\"end\":32733,\"start\":32732},{\"end\":32735,\"start\":32734},{\"end\":32747,\"start\":32746},{\"end\":32825,\"start\":32824},{\"end\":32836,\"start\":32835},{\"end\":32844,\"start\":32843},{\"end\":32853,\"start\":32852},{\"end\":32964,\"start\":32963},{\"end\":32977,\"start\":32973},{\"end\":32986,\"start\":32985},{\"end\":32993,\"start\":32992},{\"end\":33097,\"start\":33096},{\"end\":33107,\"start\":33106},{\"end\":33118,\"start\":33117},{\"end\":33134,\"start\":33133},{\"end\":33150,\"start\":33149},{\"end\":33235,\"start\":33234},{\"end\":33245,\"start\":33244},{\"end\":33335,\"start\":33334},{\"end\":33342,\"start\":33341},{\"end\":33353,\"start\":33352},{\"end\":33361,\"start\":33360},{\"end\":33418,\"start\":33417},{\"end\":33509,\"start\":33508},{\"end\":33523,\"start\":33522},{\"end\":33538,\"start\":33537},{\"end\":33546,\"start\":33545},{\"end\":33558,\"start\":33557},{\"end\":33568,\"start\":33567},{\"end\":33570,\"start\":33569},{\"end\":33631,\"start\":33630},{\"end\":33633,\"start\":33632},{\"end\":33643,\"start\":33642},{\"end\":33645,\"start\":33644},{\"end\":33752,\"start\":33751},{\"end\":33762,\"start\":33761},{\"end\":33771,\"start\":33770},{\"end\":33885,\"start\":33884},{\"end\":33896,\"start\":33895},{\"end\":33903,\"start\":33902},{\"end\":33912,\"start\":33911},{\"end\":33923,\"start\":33922},{\"end\":33931,\"start\":33930},{\"end\":33942,\"start\":33941},{\"end\":33950,\"start\":33949},{\"end\":33967,\"start\":33963},{\"end\":33973,\"start\":33972},{\"end\":33975,\"start\":33974},{\"end\":33985,\"start\":33984},{\"end\":33998,\"start\":33997},{\"end\":34034,\"start\":34030},{\"end\":34040,\"start\":34039},{\"end\":34046,\"start\":34045},{\"end\":34055,\"start\":34054},{\"end\":34062,\"start\":34061},{\"end\":34068,\"start\":34067},{\"end\":34076,\"start\":34075},{\"end\":34087,\"start\":34083},{\"end\":34095,\"start\":34094},{\"end\":34101,\"start\":34100},{\"end\":34108,\"start\":34107},{\"end\":34115,\"start\":34114},{\"end\":34124,\"start\":34123},{\"end\":34130,\"start\":34129},{\"end\":34139,\"start\":34138},{\"end\":34147,\"start\":34146},{\"end\":34155,\"start\":34154},{\"end\":34164,\"start\":34163},{\"end\":34179,\"start\":34178},{\"end\":34400,\"start\":34399},{\"end\":34477,\"start\":34476},{\"end\":34489,\"start\":34488},{\"end\":34502,\"start\":34501},{\"end\":34511,\"start\":34510},{\"end\":34648,\"start\":34647},{\"end\":34656,\"start\":34655},{\"end\":34667,\"start\":34666},{\"end\":34679,\"start\":34678},{\"end\":34693,\"start\":34692},{\"end\":34702,\"start\":34701},{\"end\":34713,\"start\":34712},{\"end\":34719,\"start\":34718},{\"end\":34729,\"start\":34728},{\"end\":34731,\"start\":34730},{\"end\":34741,\"start\":34740},{\"end\":34819,\"start\":34818},{\"end\":34829,\"start\":34828},{\"end\":34843,\"start\":34842},{\"end\":34853,\"start\":34852},{\"end\":34865,\"start\":34864},{\"end\":35012,\"start\":35011},{\"end\":35023,\"start\":35022},{\"end\":35037,\"start\":35036},{\"end\":35049,\"start\":35048},{\"end\":35129,\"start\":35128},{\"end\":35142,\"start\":35141},{\"end\":35151,\"start\":35150},{\"end\":35162,\"start\":35161},{\"end\":35247,\"start\":35246},{\"end\":35260,\"start\":35259},{\"end\":35271,\"start\":35270},{\"end\":35286,\"start\":35285},{\"end\":35294,\"start\":35293},{\"end\":35305,\"start\":35304},{\"end\":35313,\"start\":35312},{\"end\":35324,\"start\":35323},{\"end\":35497,\"start\":35496},{\"end\":35510,\"start\":35509},{\"end\":35519,\"start\":35518},{\"end\":35530,\"start\":35529},{\"end\":35540,\"start\":35539},{\"end\":35551,\"start\":35550},{\"end\":35561,\"start\":35560},{\"end\":35574,\"start\":35573},{\"end\":35584,\"start\":35583},{\"end\":35586,\"start\":35585},{\"end\":35598,\"start\":35597},{\"end\":35600,\"start\":35599},{\"end\":35610,\"start\":35609},{\"end\":35617,\"start\":35616},{\"end\":35885,\"start\":35884},{\"end\":35897,\"start\":35896},{\"end\":35908,\"start\":35907},{\"end\":35919,\"start\":35918},{\"end\":35928,\"start\":35927},{\"end\":35930,\"start\":35929},{\"end\":36048,\"start\":36043},{\"end\":36058,\"start\":36054},{\"end\":36062,\"start\":36061},{\"end\":36064,\"start\":36063},{\"end\":36078,\"start\":36077},{\"end\":36174,\"start\":36173},{\"end\":36187,\"start\":36186},{\"end\":36189,\"start\":36188},{\"end\":36198,\"start\":36197},{\"end\":36200,\"start\":36199},{\"end\":36209,\"start\":36208},{\"end\":36311,\"start\":36310},{\"end\":36317,\"start\":36316},{\"end\":36323,\"start\":36322},{\"end\":36332,\"start\":36331},{\"end\":36343,\"start\":36342},{\"end\":36356,\"start\":36355},{\"end\":36365,\"start\":36364},{\"end\":36367,\"start\":36366},{\"end\":36378,\"start\":36377},{\"end\":36388,\"start\":36387},{\"end\":36399,\"start\":36398},{\"end\":36409,\"start\":36408},{\"end\":36425,\"start\":36424},{\"end\":36445,\"start\":36444},{\"end\":36456,\"start\":36455},{\"end\":36471,\"start\":36470},{\"end\":36623,\"start\":36622},{\"end\":36634,\"start\":36633},{\"end\":36642,\"start\":36641},{\"end\":36654,\"start\":36653},{\"end\":36736,\"start\":36735},{\"end\":36747,\"start\":36746},{\"end\":36755,\"start\":36754},{\"end\":36766,\"start\":36765},{\"end\":36774,\"start\":36773},{\"end\":36786,\"start\":36785},{\"end\":36794,\"start\":36793},{\"end\":36805,\"start\":36804},{\"end\":36813,\"start\":36812}]", "bib_author_last_name": "[{\"end\":32349,\"start\":32342},{\"end\":32357,\"start\":32353},{\"end\":32367,\"start\":32361},{\"end\":32376,\"start\":32371},{\"end\":32386,\"start\":32380},{\"end\":32524,\"start\":32520},{\"end\":32532,\"start\":32528},{\"end\":32544,\"start\":32536},{\"end\":32552,\"start\":32548},{\"end\":32665,\"start\":32657},{\"end\":32676,\"start\":32669},{\"end\":32683,\"start\":32680},{\"end\":32695,\"start\":32687},{\"end\":32710,\"start\":32699},{\"end\":32718,\"start\":32714},{\"end\":32730,\"start\":32722},{\"end\":32744,\"start\":32736},{\"end\":32833,\"start\":32826},{\"end\":32841,\"start\":32837},{\"end\":32850,\"start\":32845},{\"end\":32857,\"start\":32854},{\"end\":32971,\"start\":32965},{\"end\":32983,\"start\":32978},{\"end\":32990,\"start\":32987},{\"end\":33003,\"start\":32994},{\"end\":33104,\"start\":33098},{\"end\":33115,\"start\":33108},{\"end\":33131,\"start\":33119},{\"end\":33147,\"start\":33135},{\"end\":33155,\"start\":33151},{\"end\":33242,\"start\":33236},{\"end\":33250,\"start\":33246},{\"end\":33339,\"start\":33336},{\"end\":33350,\"start\":33343},{\"end\":33358,\"start\":33354},{\"end\":33366,\"start\":33362},{\"end\":33427,\"start\":33419},{\"end\":33520,\"start\":33510},{\"end\":33535,\"start\":33524},{\"end\":33543,\"start\":33539},{\"end\":33555,\"start\":33547},{\"end\":33565,\"start\":33559},{\"end\":33576,\"start\":33571},{\"end\":33640,\"start\":33634},{\"end\":33652,\"start\":33646},{\"end\":33759,\"start\":33753},{\"end\":33768,\"start\":33763},{\"end\":33775,\"start\":33772},{\"end\":33893,\"start\":33886},{\"end\":33900,\"start\":33897},{\"end\":33909,\"start\":33904},{\"end\":33920,\"start\":33913},{\"end\":33928,\"start\":33924},{\"end\":33939,\"start\":33932},{\"end\":33947,\"start\":33943},{\"end\":33961,\"start\":33951},{\"end\":33970,\"start\":33968},{\"end\":33982,\"start\":33976},{\"end\":33995,\"start\":33986},{\"end\":34006,\"start\":33999},{\"end\":34037,\"start\":34035},{\"end\":34043,\"start\":34041},{\"end\":34052,\"start\":34047},{\"end\":34059,\"start\":34056},{\"end\":34065,\"start\":34063},{\"end\":34073,\"start\":34069},{\"end\":34081,\"start\":34077},{\"end\":34092,\"start\":34088},{\"end\":34098,\"start\":34096},{\"end\":34105,\"start\":34102},{\"end\":34112,\"start\":34109},{\"end\":34121,\"start\":34116},{\"end\":34127,\"start\":34125},{\"end\":34136,\"start\":34131},{\"end\":34144,\"start\":34140},{\"end\":34152,\"start\":34148},{\"end\":34161,\"start\":34156},{\"end\":34176,\"start\":34165},{\"end\":34188,\"start\":34180},{\"end\":34409,\"start\":34401},{\"end\":34486,\"start\":34478},{\"end\":34499,\"start\":34490},{\"end\":34508,\"start\":34503},{\"end\":34519,\"start\":34512},{\"end\":34653,\"start\":34649},{\"end\":34664,\"start\":34657},{\"end\":34676,\"start\":34668},{\"end\":34690,\"start\":34680},{\"end\":34699,\"start\":34694},{\"end\":34710,\"start\":34703},{\"end\":34716,\"start\":34714},{\"end\":34726,\"start\":34720},{\"end\":34738,\"start\":34732},{\"end\":34816,\"start\":34811},{\"end\":34826,\"start\":34820},{\"end\":34840,\"start\":34830},{\"end\":34850,\"start\":34844},{\"end\":34862,\"start\":34854},{\"end\":34875,\"start\":34866},{\"end\":35020,\"start\":35013},{\"end\":35034,\"start\":35024},{\"end\":35046,\"start\":35038},{\"end\":35059,\"start\":35050},{\"end\":35139,\"start\":35130},{\"end\":35148,\"start\":35143},{\"end\":35159,\"start\":35152},{\"end\":35168,\"start\":35163},{\"end\":35257,\"start\":35248},{\"end\":35268,\"start\":35261},{\"end\":35283,\"start\":35272},{\"end\":35291,\"start\":35287},{\"end\":35302,\"start\":35295},{\"end\":35310,\"start\":35306},{\"end\":35321,\"start\":35314},{\"end\":35329,\"start\":35325},{\"end\":35507,\"start\":35498},{\"end\":35516,\"start\":35511},{\"end\":35527,\"start\":35520},{\"end\":35537,\"start\":35531},{\"end\":35548,\"start\":35541},{\"end\":35558,\"start\":35552},{\"end\":35571,\"start\":35562},{\"end\":35581,\"start\":35575},{\"end\":35595,\"start\":35587},{\"end\":35607,\"start\":35601},{\"end\":35614,\"start\":35611},{\"end\":35894,\"start\":35886},{\"end\":35905,\"start\":35898},{\"end\":35916,\"start\":35909},{\"end\":35925,\"start\":35920},{\"end\":35937,\"start\":35931},{\"end\":36052,\"start\":36049},{\"end\":36075,\"start\":36065},{\"end\":36184,\"start\":36175},{\"end\":36195,\"start\":36190},{\"end\":36206,\"start\":36201},{\"end\":36219,\"start\":36210},{\"end\":36314,\"start\":36312},{\"end\":36320,\"start\":36318},{\"end\":36329,\"start\":36324},{\"end\":36340,\"start\":36333},{\"end\":36353,\"start\":36344},{\"end\":36362,\"start\":36357},{\"end\":36375,\"start\":36368},{\"end\":36385,\"start\":36379},{\"end\":36396,\"start\":36389},{\"end\":36406,\"start\":36400},{\"end\":36422,\"start\":36410},{\"end\":36433,\"start\":36426},{\"end\":36442,\"start\":36435},{\"end\":36453,\"start\":36446},{\"end\":36468,\"start\":36457},{\"end\":36479,\"start\":36472},{\"end\":36631,\"start\":36624},{\"end\":36639,\"start\":36635},{\"end\":36651,\"start\":36643},{\"end\":36659,\"start\":36655},{\"end\":36744,\"start\":36737},{\"end\":36752,\"start\":36748},{\"end\":36763,\"start\":36756},{\"end\":36771,\"start\":36767},{\"end\":36783,\"start\":36775},{\"end\":36791,\"start\":36787},{\"end\":36802,\"start\":36795},{\"end\":36810,\"start\":36806}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32388,\"start\":32338},{\"attributes\":{\"id\":\"b1\"},\"end\":32472,\"start\":32390},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":174801175},\"end\":32569,\"start\":32474},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":27257366},\"end\":32879,\"start\":32571},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52967399},\"end\":33020,\"start\":32881},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":174798375},\"end\":33166,\"start\":33022},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10543068},\"end\":33261,\"start\":33168},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7518693},\"end\":33377,\"start\":33263},{\"attributes\":{\"doi\":\"arXiv:1901.05287\",\"id\":\"b8\"},\"end\":33449,\"start\":33379},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4537113},\"end\":33593,\"start\":33451},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4430144},\"end\":33669,\"start\":33595},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4935860},\"end\":33792,\"start\":33671},{\"attributes\":{\"doi\":\"arXiv:1602.07332\",\"id\":\"b12\"},\"end\":34028,\"start\":33794},{\"attributes\":{\"doi\":\"arXiv:1904.06539\",\"id\":\"b13\"},\"end\":34340,\"start\":34030},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":51609418},\"end\":34422,\"start\":34342},{\"attributes\":{\"id\":\"b15\"},\"end\":34582,\"start\":34424},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":60440519},\"end\":34948,\"start\":34584},{\"attributes\":{\"id\":\"b17\"},\"end\":35065,\"start\":34950},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11816014},\"end\":35181,\"start\":35067},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":199370376},\"end\":35406,\"start\":35183},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220828823},\"end\":35812,\"start\":35408},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9014123},\"end\":35950,\"start\":35814},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2470716},\"end\":36092,\"start\":35952},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46980516},\"end\":36230,\"start\":36094},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2424223},\"end\":36544,\"start\":36232},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52019251},\"end\":36672,\"start\":36546},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53734356},\"end\":36837,\"start\":36674}]", "bib_title": "[{\"end\":32516,\"start\":32474},{\"end\":32653,\"start\":32571},{\"end\":32961,\"start\":32881},{\"end\":33094,\"start\":33022},{\"end\":33232,\"start\":33168},{\"end\":33332,\"start\":33263},{\"end\":33506,\"start\":33451},{\"end\":33628,\"start\":33595},{\"end\":33749,\"start\":33671},{\"end\":34397,\"start\":34342},{\"end\":34645,\"start\":34584},{\"end\":35126,\"start\":35067},{\"end\":35244,\"start\":35183},{\"end\":35494,\"start\":35408},{\"end\":35882,\"start\":35814},{\"end\":36041,\"start\":35952},{\"end\":36171,\"start\":36094},{\"end\":36308,\"start\":36232},{\"end\":36620,\"start\":36546},{\"end\":36733,\"start\":36674}]", "bib_author": "[{\"end\":32351,\"start\":32340},{\"end\":32359,\"start\":32351},{\"end\":32369,\"start\":32359},{\"end\":32378,\"start\":32369},{\"end\":32388,\"start\":32378},{\"end\":32526,\"start\":32518},{\"end\":32534,\"start\":32526},{\"end\":32546,\"start\":32534},{\"end\":32554,\"start\":32546},{\"end\":32667,\"start\":32655},{\"end\":32678,\"start\":32667},{\"end\":32685,\"start\":32678},{\"end\":32697,\"start\":32685},{\"end\":32712,\"start\":32697},{\"end\":32720,\"start\":32712},{\"end\":32732,\"start\":32720},{\"end\":32746,\"start\":32732},{\"end\":32750,\"start\":32746},{\"end\":32973,\"start\":32963},{\"end\":32985,\"start\":32973},{\"end\":32992,\"start\":32985},{\"end\":33005,\"start\":32992},{\"end\":33106,\"start\":33096},{\"end\":33117,\"start\":33106},{\"end\":33133,\"start\":33117},{\"end\":33149,\"start\":33133},{\"end\":33157,\"start\":33149},{\"end\":33244,\"start\":33234},{\"end\":33252,\"start\":33244},{\"end\":33341,\"start\":33334},{\"end\":33352,\"start\":33341},{\"end\":33360,\"start\":33352},{\"end\":33368,\"start\":33360},{\"end\":33429,\"start\":33417},{\"end\":33522,\"start\":33508},{\"end\":33537,\"start\":33522},{\"end\":33545,\"start\":33537},{\"end\":33557,\"start\":33545},{\"end\":33567,\"start\":33557},{\"end\":33578,\"start\":33567},{\"end\":33642,\"start\":33630},{\"end\":33654,\"start\":33642},{\"end\":33761,\"start\":33751},{\"end\":33770,\"start\":33761},{\"end\":33777,\"start\":33770},{\"end\":33895,\"start\":33884},{\"end\":33902,\"start\":33895},{\"end\":33911,\"start\":33902},{\"end\":33922,\"start\":33911},{\"end\":33930,\"start\":33922},{\"end\":33941,\"start\":33930},{\"end\":33949,\"start\":33941},{\"end\":33963,\"start\":33949},{\"end\":33972,\"start\":33963},{\"end\":33984,\"start\":33972},{\"end\":33997,\"start\":33984},{\"end\":34008,\"start\":33997},{\"end\":34039,\"start\":34030},{\"end\":34045,\"start\":34039},{\"end\":34054,\"start\":34045},{\"end\":34061,\"start\":34054},{\"end\":34067,\"start\":34061},{\"end\":34075,\"start\":34067},{\"end\":34083,\"start\":34075},{\"end\":34094,\"start\":34083},{\"end\":34100,\"start\":34094},{\"end\":34107,\"start\":34100},{\"end\":34114,\"start\":34107},{\"end\":34123,\"start\":34114},{\"end\":34129,\"start\":34123},{\"end\":34138,\"start\":34129},{\"end\":34146,\"start\":34138},{\"end\":34154,\"start\":34146},{\"end\":34163,\"start\":34154},{\"end\":34178,\"start\":34163},{\"end\":34190,\"start\":34178},{\"end\":34411,\"start\":34399},{\"end\":34488,\"start\":34476},{\"end\":34501,\"start\":34488},{\"end\":34510,\"start\":34501},{\"end\":34521,\"start\":34510},{\"end\":34655,\"start\":34647},{\"end\":34666,\"start\":34655},{\"end\":34678,\"start\":34666},{\"end\":34692,\"start\":34678},{\"end\":34701,\"start\":34692},{\"end\":34712,\"start\":34701},{\"end\":34718,\"start\":34712},{\"end\":34728,\"start\":34718},{\"end\":34740,\"start\":34728},{\"end\":34744,\"start\":34740},{\"end\":35022,\"start\":35011},{\"end\":35036,\"start\":35022},{\"end\":35048,\"start\":35036},{\"end\":35061,\"start\":35048},{\"end\":35141,\"start\":35128},{\"end\":35150,\"start\":35141},{\"end\":35161,\"start\":35150},{\"end\":35170,\"start\":35161},{\"end\":35259,\"start\":35246},{\"end\":35270,\"start\":35259},{\"end\":35285,\"start\":35270},{\"end\":35293,\"start\":35285},{\"end\":35304,\"start\":35293},{\"end\":35312,\"start\":35304},{\"end\":35323,\"start\":35312},{\"end\":35331,\"start\":35323},{\"end\":35509,\"start\":35496},{\"end\":35518,\"start\":35509},{\"end\":35529,\"start\":35518},{\"end\":35539,\"start\":35529},{\"end\":35550,\"start\":35539},{\"end\":35560,\"start\":35550},{\"end\":35573,\"start\":35560},{\"end\":35583,\"start\":35573},{\"end\":35597,\"start\":35583},{\"end\":35609,\"start\":35597},{\"end\":35616,\"start\":35609},{\"end\":35620,\"start\":35616},{\"end\":35896,\"start\":35884},{\"end\":35907,\"start\":35896},{\"end\":35918,\"start\":35907},{\"end\":35927,\"start\":35918},{\"end\":35939,\"start\":35927},{\"end\":36054,\"start\":36043},{\"end\":36061,\"start\":36054},{\"end\":36077,\"start\":36061},{\"end\":36081,\"start\":36077},{\"end\":36186,\"start\":36173},{\"end\":36197,\"start\":36186},{\"end\":36208,\"start\":36197},{\"end\":36221,\"start\":36208},{\"end\":36316,\"start\":36310},{\"end\":36322,\"start\":36316},{\"end\":36331,\"start\":36322},{\"end\":36342,\"start\":36331},{\"end\":36355,\"start\":36342},{\"end\":36364,\"start\":36355},{\"end\":36377,\"start\":36364},{\"end\":36387,\"start\":36377},{\"end\":36398,\"start\":36387},{\"end\":36408,\"start\":36398},{\"end\":36424,\"start\":36408},{\"end\":36435,\"start\":36424},{\"end\":36444,\"start\":36435},{\"end\":36455,\"start\":36444},{\"end\":36470,\"start\":36455},{\"end\":36481,\"start\":36470},{\"end\":36633,\"start\":36622},{\"end\":36641,\"start\":36633},{\"end\":36653,\"start\":36641},{\"end\":36661,\"start\":36653},{\"end\":36746,\"start\":36735},{\"end\":36754,\"start\":36746},{\"end\":36765,\"start\":36754},{\"end\":36773,\"start\":36765},{\"end\":36785,\"start\":36773},{\"end\":36793,\"start\":36785},{\"end\":36804,\"start\":36793},{\"end\":36812,\"start\":36804},{\"end\":36816,\"start\":36812}]", "bib_venue": "[{\"end\":35737,\"start\":35687},{\"end\":32460,\"start\":32390},{\"end\":32563,\"start\":32554},{\"end\":32822,\"start\":32750},{\"end\":33014,\"start\":33005},{\"end\":33160,\"start\":33157},{\"end\":33255,\"start\":33252},{\"end\":33371,\"start\":33368},{\"end\":33415,\"start\":33379},{\"end\":33587,\"start\":33578},{\"end\":33660,\"start\":33654},{\"end\":33786,\"start\":33777},{\"end\":33882,\"start\":33794},{\"end\":34277,\"start\":34222},{\"end\":34416,\"start\":34411},{\"end\":34474,\"start\":34424},{\"end\":34809,\"start\":34744},{\"end\":35009,\"start\":34950},{\"end\":35175,\"start\":35170},{\"end\":35389,\"start\":35331},{\"end\":35685,\"start\":35620},{\"end\":35944,\"start\":35939},{\"end\":36086,\"start\":36081},{\"end\":36224,\"start\":36221},{\"end\":36485,\"start\":36481},{\"end\":36666,\"start\":36661},{\"end\":36820,\"start\":36816}]"}}}, "year": 2023, "month": 12, "day": 17}