{"id": 254220922, "updated": "2023-09-27 17:21:51.872", "metadata": {"title": "FedALA: Adaptive Local Aggregation for Personalized Federated Learning", "authors": "[{\"first\":\"Jianqing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Hua\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Zhengui\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Ruhui\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Haibing\",\"last\":\"Guan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.01197", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/ZhangHWSXMG23", "doi": "10.1609/aaai.v37i9.26330"}}, "content": {"source": {"pdf_hash": "8022ea93560908bb57b5ff0a668f21079d0ccdaa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.01197v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26330/26102", "status": "GOLD"}}, "grobid": {"id": "52bf0fb421e53912316e0f3a2cbe2c420ad69bea", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/8022ea93560908bb57b5ff0a668f21079d0ccdaa.txt", "contents": "\nFedALA: Adaptive Local Aggregation for Personalized Federated Learning\n\n\nJianqing Zhang \nShanghai Jiao Tong University\n\n\nYang Hua y.hua@qub.ac.uk \nQueen's University Belfast\n\n\nHao Wang haowang@lsu.edu \nLouisiana State University\n\n\nTao Song \nShanghai Jiao Tong University\n\n\nZhengui Xue \nShanghai Jiao Tong University\n\n\nRuhui Ma \nShanghai Jiao Tong University\n\n\nHaibing Guan hbguan@sjtu.edu.cn \nShanghai Jiao Tong University\n\n\nFedALA: Adaptive Local Aggregation for Personalized Federated Learning\n6C0DE6CC9D843FC0C0C2C0AC7D3EF3AA\nA key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client.To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL.The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration.To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains.FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy.Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.Code is available at https://github.com/TsingZ0/FedALA.\n\nIntroduction\n\nFederated learning (FL) can leverage distributed user data while preserving privacy by iteratively downloading models, training models locally on the clients, uploading models, and aggregating models on the server.A key challenge in FL is statistical heterogeneity, e.g., the not independent and identically distributed (Non-IID) and unbalanced data across clients.This kind of data makes it hard to obtain a global model that generalizes to each client (McMahan et al. 2017;Reisizadeh et al. 2020;T Dinh, Tran, and Nguyen 2020).\n\nPersonalized FL (pFL) methods have been proposed to tackle statistical heterogeneity in FL.Unlike traditional FL that seeks a high-quality global model via distributed training across clients, e.g., FedAvg (McMahan et al. 2017), pFL methods are proposed to prioritize the training of a local model for each client.Recent pFL studies on model aggregation on the server can be classified into three categories: (1) methods that learn a single global model and fine-tune it, including Per-FedAvg (Fallah, Mokhtari, and Ozdaglar 2020) and FedRep (Collins et al. 2021), (2) methods that learn additional personalized models, including pFedMe (T Dinh, Tran, and Nguyen 2020) and Ditto (Li et al. 2021a), and (3) methods that learn local models with personalized (local) aggregation, including FedAMP (Huang et al. 2021), Fed-PHP (Li et al. 2021b), FedFomo (Zhang et al. 2020), AP-PLE (Luo and Wu 2021) and PartialFed (Sun et al. 2021).\n\npFL methods in Category ( 1) and ( 2) take all the information in the global model for local initialization, i.e., initializing the local model before local training in each iteration.However, only the desired information that improves the quality of the local model is beneficial for the client.The global model has poor generalization ability since it has desired and undesired information for an individual client simultaneously.Thus, pFL methods in Category (3) intend to capture the desired information in the global model through personalized aggregation.\n\nHowever, pFL methods in Category (3) still have shortcomings.FedAMP/FedPHP performs personalized aggregation on the server/clients without considering the local objective.FedFomo/APPLE downloads other client models and locally aggregates them with the approximated/learned aggregating weights on each client.All the parameters in one client model are assigned with the same weight, i.e., modellevel weight.Besides, downloading client models among clients causes high communication overhead in each iteration and also has privacy concerns since the data from other clients can be recovered through these client models (Zhu, Liu, and Han 2019).In addition, FedFomo/AP-PLE also requires feeding data forward in the downloaded client models to obtain the aggregating weights, which introduces additional computation overhead.PartialFed locally learns aggregation strategies to select the parameters in the global model or the local model.Still, the layer-level and binary selection cannot precisely capture the desired information in the global model.Furthermore, PartialFed uses non-overlapping samples to learn the local model and the strategy, so it can hardly learn a strategy that fully satisfies the local objective.Due to the significant modification of the learning process in FedAvg, the personalized aggregation process in these methods cannot be directly applied to most existing FL methods.\n\nTo precisely capture the desired information in the downloaded global model for each client without additional communication overhead in each iteration, we propose a novel pFL method Federated learning with Adaptive Local arXiv:2212.01197v4[cs.LG] 17 Sep 2023\n\nLocal model < l a t e x i t s h a 1 _ b a s e 6 4 = \" O e G X s N Z A a A J + E F s b M r R z T d 9 + z 3 8 = \" > A A A B 8 3 i c b V D J S g N B E O 1 x j X G L y 8 3 L Y B C 8 G G Z E 1 J s B D 3 q M k A 2 S M f R 0 a p I m P T 1 D d 4 0 Q h / y G F w + K e N W v 8 A u 8 e f R P 7 C w H T X x Q 8 H i v i q p 6 f i y 4 R s f 5 s u b m F x a X l j M r 2 d W 1 9 Y 3 N 3 N Z 2 V U e J Y l B h k Y h U 3 a c a B J d Q Q Y 4 C 6 r E C G v o C a n 7 v c u j X 7 k B p H s k y 9 m P w Q t q R P O C M o p G a z X I X k N 6 m e O Q O W r m 8 U 3 B G s G e J O y H 5 i 4 / 7 7 6 v 3 3 b T U y n 0\n2 2 x F L Q p D I B N W 6 4 T o x e i l V y J m A Q b a Z a I g p 6 9 E O N A y V N A T t p a O b B / a B U d p 2 E C l T E u 2 R + n s i p a H W / d A 3 n S H F r p 7 2 h u J / X i P B 4 N x L u Y w T B M n G i 4 J E 2 B j Z w w D s N l f A U P Q N o U x x c 6 v N u l R R h i a m r A n B n X 5 5 l l S P C + 5 p 4 e T G y R e L Z I w M 2 S P 7 5 J C 4 5 I w U y T U p k Q p h J C Y P 5 I k 8 W 4 n 1 a L 1 Y r + P W O W s y s 0 P + w H r 7 A e T / l V E = < / l a t e x i t > \u21e5 t 1 Global model < l a t e x i t s h a 1 _ b a s e 6 4 = \" v s 2 l 9 k r T I N l b c z C Z C q q l a / 7 o k r Y = \" > A A A B 9 X i c b V D J S g N B E K 2 J W 4 x G o x 6 9 N A l C L o Y Z E f U Y 8 O I x Q j b I R k + n J 2 n S s 9 B d o 4 Q h / + H F g y J e / Q R v X g V v / o 2 d 5 a C J D w o e 7 1 V R V c + N p N B o 2 9 9 W a m 1 9 Y 3 M r v Z 3 Z 2 c 3 u 7 e c O D u s 6 j B X j N R b K U D V d q r k U A a + h Q M m b k e L U d y V v u K P r q d + 4 4 0 q L M K j i O O I d n w 4 C 4 Q l G 0 U j d d n X I k X Y T P H U m P d H L F e y S P Q N Z J c 6 C F M r 5 4 u f 7 R z V b 6 e W + 2 v 2 Q x T 4 P k E m q d c u x I + w k V K F g k k 8 y 7 V j z i L I R H f C W o Q H 1 u e 4 k s 6 s n 5 M Q o f e K F y l S A Z K b + n k i o r / X Y d 0 2 n T 3 G o l 7 2 p + J / X i t G 7 6 i Q i i G L k A Z s v 8 m J J M C T T C E h f K M 5 Q j g 2 h T A l z K 2 F D q i h D E 1 T G h O A s v 7 x K 6 m c l 5 6 J 0 f m v S K M M c a T i G P B T B g U s o w w 1 U o A Y M F D z A E z x b 9 9 a j 9 W K 9 z l t T 1 m L m C P 7 A e v s B i o m V i g = = < / l a t e x i t > \u21e5 t 1 i ALA < l a t e x i t s h a 1 _ b a s e 6 4 = \" h a d x D M / v + w J Y j / N N k E I s h i v z T 6 4 = \" > A A A B / H i c b V B N S 8 N A E N 3 U r 1 q / q j 1 6 W a y C p 5 K I q M d C L 9 5 s o V / Q x r D Z b t q l m 0 3 Y n Q g l x L + i B w + K e P X q f / D m v 3 H b e t D W B w O P 9 2 a Y m e f H g m u w 7 S 8 r t 7 K 6 t r 6 R 3 y x s b e / s 7 h X 3 D 9 o 6 S h R l L R q J S H V 9 o p n g k r W A g 2 D d W D E S + o J 1 / H F t 6 n f u m N I 8 k k 2 Y x M w N y V D y g F M C R v K K p f 6 I Q N p v j h i Q 7 D a F z O P Y K 5 b t i j 0 D X i b O D y l X 8 f H N Y 6 P 2 U f e K n / 1 B R J O Q S a C C a N 1 z 7 B j c l C j g V L C s 0 E 8 0 i w k d k y H r G S p J y L S b z o 7 P 8 I l R B j i I l C k J e K b + n k h J q P U k 9 E 1 n S G C k F 7 2 p + J / X S y C 4 c l M u 4 w S Y p P N F Q S I w R H i a B B 5 w x S i I i S G E K m 5 u x X R E F K F g 8 i q Y E J z F l 5 d J + 6 z i X F T O G y a N K p o j j w 7 R E T p F D r p E V X S N 6 q i F K J q g B / S M X q x 7 6 8 l 6 t d 7 m r T n r Z 6 a E / s B 6 / w b A O Z f H < / l a t e x i t > \u21e5t i Initialized local model Local model Train Download Upload < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 x f 7 k P S 2 k K 7 t Q K U l m B a R s X q X I w A = \" > A A A B 9 H i c b V D J S g N B E O 1 x j X G L e v T S G A V P Y U Z E P Q Z y 8 W Y C 2 S A Z Q 0 + n J m n S s 9 h d E w h D v s O L h 4 h 4 9 R f 8 B 2 / + j Z 3 l o I k P C h 7 v V V F V z 4 u l 0 G j b 3 9 b a + s b m 1 n Z m J 7 u 7 t 3 9 w m D s 6 r u s o U R x q P J K R a n p M g x Q h 1 F C g h G a s g A W e h I Y 3 K E 3 9 x h C U F l F Y x V E M b s B 6 o f A F Z 2 g k t 1 3 t A 7 L H F M c d Q T u 5 v F 2 w Z 6 C r x F m Q f J G e P 0 w q p c 9 y J / f V 7 k Y 8 C S B E L p n W L c e O 0 U 2 Z Q s E l j L P t R E P M + I D 1 o G V o y A L Q b j o 7 e k w v j N K l f q R M h U h n 6 u + J l A V a j w L P d A Y M + 3 r Z m 4 r / e a 0 E / T s 3 F W G c I I R 8 v s h P J M W I T h O g X a G A o x w Z w r g S 5 l b K + 0 w x j i a n r A n B W X 5 5 l d S v C s 5 N 4 b p i 0 i i S O T L k l J y R S + K Q W 1 I k 9 6 R M a o S T J / J M J u T V G l o v 1 p v 1 P m 9 d s x Y z J + Q P r I 8 f X 0 C U y Q = = < / l a t\n\nPersonalized Federated Learning\n\nRecently, personalization has attracted much attention for tackling statistical heterogeneity in FL (Kairouz et al. 2019).We consider the following three categories of pFL methods that focus on aggregating models on the server:\n\n(1) Methods that learn a single global model and finetune it.Per-FedAvg (Fallah, Mokhtari, and Ozdaglar 2020) considers the global model as an initial shared model based on MAML (Finn, Abbeel, and Levine 2017).By performing a few additional training steps locally, all the clients can easily fine-tune the reasonable initial shared model.Fe-dRep (Collins et al. 2021) splits the backbone into a global model (representation) and a client-specific head and finetunes the head locally to achieve personalization.\n\n(2) Methods that learn additional personalized models.pFedMe(T Dinh, Tran, and Nguyen 2020) learns an additional personalized model for each client with Moreau envelopes.In Ditto (Li et al. 2021a), each client learns its additional personalized model with a proximal term to fetch information from the downloaded global model.\n\n(3) Methods that learn local models with personalized (local) aggregation.To further capture the personalization, recent methods try to generate client-specific models through personalized aggregation.For example, FedAMP (Huang et al. 2021)  Our FedALA belongs to Category (3) but is more precise and requires less communication cost than FedFomo and APPLE.The fine-grained ALA in FedALA can elementwisely aggregate the global model and local model to adapt to the local objective on each client for local initialization.As FedALA only modifies the local initialization in FL, it can be applied to existing FL methods to improve their performance without modifying other learning processes.\n\n\nMethod Problem Statement\n\nSuppose we have N clients with their private training data D 1 , . . ., D N , respectively.These datasets are heterogeneous (Non-IID and unbalanced).Specifically, D 1 , . . ., D N are sampled from N distinct distributions and have different sizes.With the help of a central server, our goal is to collaboratively learn individual local models \u03981 , . . ., \u0398N using D 1 , . . ., D N for each client, without exchanging the private data.The objective is to minimize the global objective and obtain the reasonable local models\n{ \u03981 , . . . , \u0398N } = arg min G(L 1 , . . . , L N ),(1)\nwhere\nL i = L( \u0398i , D i ; \u0398), \u2200i \u2208 [N ] and L(\u2022) is the loss function. \u0398 is the global model, which brings external infor- mation to client i. Typically, G(\u2022) is set to N i=1 k i L i , where k i = |D i |/ N j=1 |D j | and |D i | is the amount of local data samples on client i.\n\nAdaptive Local Aggregation (ALA)\n\nThe server generates a global model by aggregating trained client models in heterogeneous settings, but this global model generalizes poorly on each client.To address this problem, we propose a pFL method FedALA with a finegrained ALA module that element-wisely aggregates the global model and local model to adapt to the local objective.We show the learning process in ALA in Figure 2. In traditional FL (e.g., FedAvg), after the server sends the old global model \u0398 t\u22121 to client i in iteration t, \u0398 t\u22121 overwrites the old local model \u0398\n+ \u2299 Local model < l a t e x i t s h a 1 _ b a s e 6 4 = \" v M s 1 m K l d g + 4 + G q h Q m 1 O / l g o q x f Q = \" > A A A B + H i c b V A 9 S w N B E N 2 L X z E a c 2 p p s y Q I a Q x 3 I m o Z s L G M k C 9 I 4 r G 3 2 U u W 7 O 0 d u 3 N C P P J L b C w U s f U P 2 N k K d v 4 b N x + F J j 4 Y e L w 3 w 8 w 8 P x Z c g + N 8 W 5 m 1 9 Y 3 N r e x 2 b m c 3 v 1 e w 9 w + a O k o U Z Q 0 a i U i 1 f a K Z 4 J I 1 g I N g 7 V g x E v q C t f z R 1 d R v 3 T G l e S T r M I 5 Z L y Q D y Q N O C R j J s w v d + p A B u U 3 h x J 1 4 H H t 2 y a k 4 M + B V 4 i 5 I q V o s f 7 5 / 1 P M 1 z / 7 q 9 i O a h E w C F U T r j u v E 0 E u J A k 4 F m + S 6 i W Y x o S M y Y B 1 D J Q m Z 7 q W z w y f 4 2 C h 9 H E T K l A Q 8 U 3 9 P p C T U e h z 6 p j M k M N T L 3 l T 8 z + s k E F z 2 U i 7 j B J i k 8 0 V B I j B E e J o C 7 n P F K I i x I Y Q q b m 7 F d E g U o W C y y p k Q 3 O W X V 0 n z t O K e V 8 5 u T B p V N E c W H a E i K i M X X a A q u k Y 1 1 E A U J e g B P a F n 6 9 5 6 t F 6 s 1 3 l r x l r M H K I / s N 5 + A F 0 N l e U = < / l a t e x i t > \u21e5 t 1 i Update < l a t e x i t s h a 1 _ b a s e 6 4 = \" C L / G A k h Y l F D K s y W C P l G z R Y f V a m s = \" > A A A C C H i c b V C 7 S g N B F J 2 N r x h f q 5 Y W D g Y h F g m 7 E t Q y Y G M Z I S 9 I 1 m V 2 M p s M m X 0 w c 1 c I S 0 o b f 8 X G Q h F b P 8 H O v 3 G S b B E T D 1 w 4 c 8 6 9 z L 3 H i w V X Y F k / R m 5 t f W N z K 7 9 d 2 N n d 2 z 8 w D 4 9 a K k o k Z U 0 a i U h 2 P K K Y 4 C F r A g f B O r F k J P A E a 3 u j 2 6 n f f m R S 8 S h s w D h m T k A G I f c 5 J a A l 1 z w t 9 R p D B u Q h h b I 9 w W W 8 + H T 5 h W s W r Y o 1 A 1 4 l d k a K K E P d N b 9 7 / Y g m A Q u B C q J U 1 7 Z i c F I i g V P B J o V e o l h M 6 I g M W F f T k A R M O e n s k A k + 1 0 o f + 5 H U F Q K e q Y s T K Q m U G g e e 7 g w I D N W y N x X / 8 7 o J + D d O y s M 4 A R b S + U d + I j B E e J o K 7 n P J K I i x J o R K r n f F d E g k o a C z K + g Q 7 O W T V 0 n r s m J f V a r 3 1 W K t l s W R R y f o D J W Q j a 5 R D d 2 h O m o i i p 7 Q C 3 p D 7 8 a z 8 W p 8 G J / z 1 p y R z R y j P z C + f g G B i p h i < / l a t e x i t > (\u21e5 t 1 \u21e5 t 1 i ) Train Adaptive weights < l a t e x i t s h a 1 _ b a s e 6 4 = \" B w h d O E v v 2 p u W P k N P i 0 V 4 X M X f F f Y = \" > A A A C A n i c b V D J S g N B E O 1 x j X E b 9 S R e B o P g x T A j Q T 0 G v H i M k A 0 y c e j p 1 C R N e h a 6 a 8 Q w C V 7 8 F S 8 e F P H q V 3 j z b + w s B 0 1 8 U P B 4 r 4 q q e n 4 i u E L b / j a W l l d W 1 9 Z z G / n N r e 2 d X X N v v 6 7 i V D K o s V j E s u l T B Y J H U E O O A p q J B B r 6 A h p + / 3 r s N + 5 B K h 5 H V R w k 0 A 5 p N + I B Z x S 1 5 J m H L s I D + k H m j O 6 y o V v t A V K P D 8 + S k W c W 7 K I 9 g b V I n B k p k B k q n v n l d m K W h h A h E 1 S p l m M n 2 M 6 o R M 4 E j P J u q i C h r E + 7 0 N I 0 o i G o d j Z 5 Y W S d a K V j B b H U F a E 1 U X 9 P Z D R U a h D 6 u j O k 2 F P z 3 l j 8 z 2 u l G F y 1 M x 4 l K U L E p o u C V F g Y W + M 8 r A 6 X w F A M N K F M c n 2 r x X p U U o Y 6 t b w O w Z l / e Z H U z 4 v O R b F 0 W y q U y 7 M 4 c u S I H J N T 4 p B L U i Y 3 p E J q h J F H 8 k x e y Z v x Z L w Y 7 8 b H t H X J m M 0 c k D 8 w P n 8 A 6 S m X w w = = < / l a t e x i t > 1 |\u21e5i| p Old weights < l a t e x i t s h a 1 _ b a s e 6 4 = \" B w h d O E v v 2 p u W P k N P i 0 V 4 X M X f F f Y = \" > A A A C A n i c b V D J S g N B E O 1 x j X E b 9 S R e B o P g x T A j Q T 0 G v H i M k A 0 y c e j p 1 C R N e h a 6 a 8 Q w C V 7 8 F S 8 e F P H q V 3 j z b + w s B 0 1 8 U P B 4 r 4 q q e n 4 i u E L b / j a W l l d W 1 9 Z z G / n N r e 2 d X X N v v 6 7 i V D K o s V j E s u l T B Y J H U E O O A p q J B B r 6 A h p + / 3 r s N + 5 B K h 5 H V R w k 0 A 5 p N + I B Z x S 1 5 J m H L s I D + k H m j O 6 y o V v t A V K P D 8 + S k W c W 7 K I 9 g b V I n B k p k B k q n v n l d m K W h h A h E 1 S p l m M n 2 M 6 o R M 4 E j P J u q i C h r E + 7 0 N I 0 o i G o d j Z 5 Y W S d a K V j B b H U F a E 1 U X 9 P Z D R U a h D 6 u j O k 2 F P z 3 l j 8 z 2 u l G F y 1 M x 4 l K U L E p o u C V F g Y W + M 8 r A 6 X w F A M N K F M c n 2 r x X p U U o Y 6 t b w O w Z l / e Z H U z 4 v O R b F 0 W y q U y 7 M 4 c u S I H J N T 4 p B L U i Y 3 p E J q h J F H 8 k x e y Z v x Z L w Y 7 8 b H t H X J m M 0 c k D 8 w P n 8 A 6 S m X w w = = < / l a t e x i t > 1 |\u21e5i| p < l a t e x i t s h a 1 _ b a s e 6 4 = \" V N / 9 I k A J E 0 x x 8 v P j T g n I y A S E J f c = \" > A A A B 8 H i c b V A 9 S w N B E J 2 L X z E a j V r a L A l C q n A n o p Y B G 8 s I + Z I k h r 3 N X r J k d + / Y 3 R P C c b / C x k I R W / + E n a 1 g 5 7 9 x 8 1 F o 4 o O B x 3 s z z M z z I 8 6 0 c d 1 v J 7 O 2 v r G 5 l d 3 O 7 e z m 9 / Y L B 4 d N H c a K 0 A Y J e a j a P t a U M 0 k b h h l O 2 5 G i W P i c t v z x 1 d R v 3 V O l W S j r Z h L R n s B D y Q J G s L H S b d J K 7 5 I o 7 b N + o e R W 3 B n Q K v E W p F Q t l j / f P + r 5 W r / w 1 R 2 E J B Z U G s K x 1 h 3 P j U w v w c o w w m m a 6 8 a a R p i M 8 Z B 2 L J V Y U N 1 L Z g e n 6 M Q q A x S E y p Y 0 a K b + n k i w 0 H o i f N s p s B n p Z W 8 q / u d 1 Y h N c 9 h I m o 9 h Q S e a L g p g j E 6 L p 9 2 j A F C W G T y z B R D F 7 K y I j r D A x N q O c D c F b f n m V N E 8 r 3 n n l 7 M a m U Y U 5 s n A M R S i D B x d Q h W u o Q Q M I C H i A J 3 h 2 l P P o v D i v 8 9 a M s 5 g 5 g j 9 w 3 n 4 A o Y i T 8 w = = < / l a t e x i t > W p i < l a t e x i t s h a 1 _ b a s e 6 4 = \" V N / 9 I k A J E 0 x x 8 v P j T g n I y A S E J f c = \" > A A A B 8 H i c b V A 9 S w N B E J 2 L X z E a j V r a L A l C q n A n o p Y B G 8 s I + Z I k h r 3 N X r J k d + / Y 3 R P C c b / C x k I R W / + E n a 1 g 5 7 9 x 8 1 F o 4 o O B x 3 s z z M z z I 8 6 0 c d 1 v J 7 O 2 v r G 5 l d 3 O 7 e z m 9 / Y L B 4 d N H c a K 0 A Y J e a j a P t a U M 0 k b h h l O 2 5 G i W P i c t v z x 1 d R v 3 V O l W S j r Z h L R n s B D y Q J G s L H S b d J K 7 5 I o 7 b N + o e R W 3 B n Q K v E W p F Q t l j / f P + r 5 W r / w 1 R 2 E J B Z U G s K x 1 h 3 P j U w v w c o w w m m a 6 8 a a R p i M 8 Z B 2 L J V Y U N 1 L Z g e n 6 M Q q A x S E y p Y 0 a K b + n k i w 0 H o i f N s p s B n p Z W 8 q / u d 1 Y h N c 9 h I m o 9 h Q S e a L g p g j E 6 L p 9 2 j A F C W G T y z B R D F 7 K y I j r D A x N q O c D c F b f n m V N E 8 r 3 n n l 7 M a m U Y U 5 s n A M R S i D B x d Q h W u o Q Q M I C H i A J 3 h 2 l P P o v D i v:= \u0398 t\u22121 i \u2299 W i,1 + \u0398 t\u22121 \u2299 W i,2 , s.t. w q 1 + w q 2 = 1, \u2200 valid q (2)\nwhere \u2299 is a Hadamard product and w q 1 and w q 2 are the q-th parameters in the aggregating weights W i,1 and W i,2 , respectively.For overwriting, \u2200 valid q, w q 1 \u2261 0 and w q 2 \u2261 1.However, it is hard to learn W i,1 and W i,2 with the constraint through the gradient-based learning method.Thus, we combine W i,1 and W i,2 by viewing Equation (2) as:\n\u0398t i := \u0398 t\u22121 i + (\u0398 t\u22121 \u2212 \u0398 t\u22121 i ) \u2299 W i ,(3)\nwhere we call the term (\u0398 t\u22121 \u2212 \u0398 t\u22121 i ) as the \"update\".Inspired by the previous methods (Courbariaux et al. 2016;Luo et al. 2018), we utilize element-wise weight clipping \u03c3(w) = max(0, min(1, w)) for regularization (Arjovsky, Chintala, and Bottou 2017) and let w \u2208 [0, 1], \u2200w \u2208 W i .\n\nSince the lower layers in the DNN learn more general information than the higher layers (Yosinski et al. 2014;Zhu, Hong, and Zhou 2021), the client desires most of the information in the lower layers of the global model.To reduce computation overhead, we introduce a hyperparameter p to control the range of ALA by applying it on p higher layers and overwriting the parameters in the lower layers like FedAvg for local initialization:\n\u0398t i := \u0398 t\u22121 i + (\u0398 t\u22121 \u2212 \u0398 t\u22121 i ) \u2299 [1 |\u0398i|\u2212p ; W p i ],(4)\nwhere |\u0398 i | is the number of layers (or blocks) in \u0398 t\u22121 i and 1 |\u0398i|\u2212p has the same shape of the lower layers in \u0398 t\u22121 i .The elements in 1 |\u0398i|\u2212p are ones (constants).The weight W p i has the same shape as the remaining p higher layers.\n\nWe initialize the value of each element in W p i to one in the beginning and learn W p i based on old W p i in each iteration.To further reduce computation overhead, we randomly sample s% of D i in iteration t and denote it as D s,t i .Client i trains W p i through the gradient-based learning method:\nW p i \u2190 W p i \u2212 \u03b7\u2207 W p i L( \u0398t i , D s,t i ; \u0398 t\u22121 ),(5)\nwhere \u03b7 is the learning rate for weight learning.We freeze other trainable parameters in ALA, including the entire global model and entire local model.After local initialization, client i performs local model training as in FedAvg.\n\nWe can significantly reduce the number of the trainable parameters in ALA by choosing a small p with negligible performance decline.We show the details in Section .Besides, we observe that once we train W p i to converge in the second iteration (initial stage), it hardly changes in the subsequent iterations, as shown in Figure 3.In other words, W p i can be reused.Similar to APPLE and PartialFed, we train only one epoch for W p i when t > 2 to adapt to the changing model parameters.Note that ALA is meaningless and deactivated in the first iteration since\n\u0398 0 = \u0398 0 i , \u2200i \u2208 [N ]\n. Algorithm 1 presents the overall FL process in FedALA.\n\n\nAnalysis of ALA\n\nWe omit \u03c3(\u2022) and set p = |\u0398 i | here for simplicity without affecting the analysis.According to Equation (4) and Equation (5), we have\n\u2207 Wi L t i = \u03b7(\u0398 t\u22121 \u2212 \u0398 t\u22121 i ) \u2299 \u2207 \u0398i L t i ,i \u2190 \u0398t i \u2212\u03b7(\u0398 t\u22121 \u2212\u0398 t\u22121 i )\u2299(\u0398 t\u22121 \u2212\u0398 t\u22121 i )\u2299\u2207 \u0398i L t i . (6)\nThe gradient \u2207 \u0398i L t i is element-wisely scaled in iteration t.In contrast to local model training (or fine-tuning) that only focuses on the local data, the whole process of updating in Equation ( 6) can be aware of the generic information in the global model.Across iterations, the dynamic term (\u0398 t\u22121 \u2212 \u0398 t\u22121 i ) brings dynamic information for ALA to adapt to the complex scenarios, although it is a constant in iteration t.\n\n\nExperiments Experimental Setup\n\nIn this section, we firstly compare FedALA with eleven SOTA  We conduct extensive experiments in computer vision (CV) and natural language processing (NLP) domains.\n\nFor the CV domain, we study the image classification tasks with four widely used datasets including MNIST (Le-Cun et al. 1998), Cifar10/100 (Krizhevsky and Geoffrey 2009) and Tiny-ImageNet (Chrabaszcz, Loshchilov, and Hutter 2017) (100K images with 200 classes) using the 4layer CNN (McMahan et al. 2017).For the NLP domain, we study the text classification tasks with AG News (Zhang, Zhao, and LeCun 2015) and fastText (Joulin et al. 2017).To evaluate the effectiveness of FedALA on a larger model, we also use ResNet-18 (He et al. 2016) on Tiny-ImageNet.We set the local learning rate to 0.005 for the 4-layer CNN (0.1 on MNIST following FedAvg) and 0.1 for both fastText and ResNet-18.We set the batch size to 10 and the number of local model training epochs to 1, following FedAvg.We run all the tasks for 2000 iterations to make all the methods converge empirically.Following pFedMe, we have 20 clients and set \u03c1 = 1 by default.\n\nWe simulate the heterogeneous settings with two widely used scenarios.The first one is the pathological heterogeneous setting (McMahan et al. 2017;Shamsian et al. 2021), where we sample 2/2/10 classes for MNIST/Ci-far10/Cifar100 from a total of 10/10/100 classes for each client, with disjoint data samples.The second scenario is the practical heterogeneous setting (Lin et al. 2020;Li, He, and Song 2021), which is controlled by the Dirichlet distribution, denoted as Dir(\u03b2).The smaller the \u03b2 is, the more heterogeneous the setting is.We set \u03b2 = 0.1 for the default heterogeneous setting (Lin et al. 2020;Wang et al. 2020c).\n\nWe use the same evaluation metrics as pFedMe, which reports the test accuracy of the best single global model for the traditional FL and the average test accuracy of the best local models for pFL.To simulate the practical pFL setting, we evaluate the learned model on the client side.25% of the local data forms the test dataset, and the remaining 75% data is used for training.We run all the tasks five times and report the mean and standard deviation.\n\nWe implement FedALA using PyTorch-1.8and run all experiments on a server with two Intel Xeon Gold 6140 CPUs (36 cores), 128G memory, and eight NVIDIA 2080 Ti GPUs, running CentOS 7.8.\n\n\nEffect of Hyperparameters\n\nEffect of s.From Table 1, high accuracy corresponds to large s, where \"TINY*\" represents using ResNet-18 on Tiny-ImageNet in the default heterogeneous setting.We can balance the performance and the computational cost by choosing a reasonable value for s.FedALA can also achieve excellent performance with only 5% local data for ALA.\n\nSince the improvement from s = 80 to s = 100 is negligible, we set s = 80 for FedALA.\n\nEffect of p.By decreasing the hyperparameter p, we can shrink the range of ALA with negligible accuracy decrease, as shown in Table 1.When p decreases from 6 to 1, the number of trainable parameters in ALA also decreases, especially from p = 2 to p = 1, as the last block in ResNet-18 contains most of the parameters (He et al. 2016).Although FedALA performs the best when p = 2 here, we set p = 1 for ResNet-18 to reduce computation overhead.Similarly, we also set p = 1 for the 4-layer CNN and fastText.This also shows that the lower layers of the global model mostly contain generic information which is desired by the client.\n\n\nPerformance Comparison and Analysis\n\nPathological heterogeneous setting.Clients are separated into groups, which benefits the methods that measure the similarity among clients, such as FedAMP.Even so, Table 2 shows that FedALA outperforms all the baselines.Due to the poor generalization ability of the global model, Fe-dAvg and FedProx perform poorly in this setting.\n\npFL methods in Category (1).Compared to the traditional FL methods, the personalized methods perform better.The accuracy for Per-FedAvg is the lowest among these methods since it only finds an initial shared model corresponding to the learning trends of all the clients, which may not capture the needs of an individual client.Fine-tuning the global model in FedAvg-C/FedProx-C generates client-specific local models, which improves the accuracy for FedAvg/Fed-Prox.However, fine-tuning only focuses on the local data and cannot be aware of the generic information during local training like FedALA.Although FedRep also fine-tunes the head at each iteration, it freezes the downloaded representation part when fine-tuning and keeps most of the generic information in the global model, thus performing excellently.However, the generic information of the head part is lost without sharing the head among clients.pFL methods in Category (2).Although both pFedMe and Ditto use the proximal term to learn their additional personalized models, pFedMe learns the desired information from the local model while Ditto learns it from the global model.Thus, Ditto learns more generic information locally, and it performs better.However, learning the personalized model with the proximal term is an implicit way to extract the desired information.pFL methods in Category (3).Practical heterogeneous setting.We add two additional tasks using Tiny-ImageNet and one text classification task on AG News in the default practical heterogeneous setting.The results in the default heterogeneous setting with Dir(0.1) are shown in Table 2, where \"TINY\" represents using the 4-layer CNN on Tiny-ImageNet.FedALA is still superior to other baselines, which achieves an accuracy of 3.27% higher than FedRep on TINY.\n\nIn the practical heterogeneous setting, due to the complex data distribution of each client, it is hard to measure the similarity among clients.Thus, FedAMP can not precisely assign importance to the local models through the attention-inducing function to generate aggregated models with personalized aggregation.After downloading the global model/representation, Ditto and FedRep can capture the generic information from it instead of measuring the similarity among local models.In this way, they achieve excellent performance in most of the tasks.The trainable weights are more informative than the approximated one, so APPLE performs better than FedFomo.Although FedPHP performs well on TINY, the standard deviation is relatively high.Since FedALA can adapt to the changing circumstances through ALA, it still outperforms all the baselines in the practical set-ting.If we reuse the aggregated weights learned in the initial stage without adaptation, the accuracy drops to 33.81% on TINY.Due to the fine-grained feature of ALA, it still outperforms Per-FedAvg, pFedMe, Ditto, FedAMP, and FedFomo.Compared to the 4-layer CNN, ResNet-18 is a larger backbone.With ResNet-18, most methods achieve a higher accuracy, including FedALA.However, it is harder for FedAMP to measure the model similarity, and the heuristic local aggregation in FedPHP performs worse when using ResNet-18.As we set p = 1, the number of trainable parameters in ALA is much less than that of ResNet-18, but FedALA can still achieve superior performance.\n\nComputation Overhead.We record the total time cost for each method until convergence, as shown in Table 3. Except for the 7 min cost in the initial stage, FedALA costs 1.93 min (similar to FedAvg) in each iteration.In other words, ALA only costs an additional 0.34 min for the great accuracy improvement.However, FedAvg-C, FedProx-C, and FedRep cost relatively more time than most of the methods because of the fine-tuning for the entire local model (or only the head).Due to the additional training steps for the personalized model, Ditto and pFedMe have the top computation overhead per iteration among SOTA methods.FedFomo and APPLE feed data forward in the downloaded client-side model to approximate aggregate weights and update directed relation (DR) vectors, respectively, taking additional time.\n\nCommunication Overhead.We show the communication overhead for one client in one iteration in Table 3.The communication overhead for most of the methods is the same as FedAvg, which uploads and downloads only one model.Since FedRep only transmits the representation and keeps the head locally, it has less communication overhead.FedFomo and APPLE require the highest communication overhead, as they download M client models in each iteration (Zhang et al. 2020;Luo and Wu 2021).To achieve the results in Table 2, we set M = 20 for them.\n\nHeterogeneity.To study the effectiveness of FedALA in the settings with different degrees of heterogeneity, we vary the \u03b2 in Dir(\u03b2) on Tiny-ImageNet and AG News.The smaller the \u03b2 is, the more heterogeneous the setting is.In Table 4, most of the pFL methods have better performance in the more heterogeneous settings.However, except for FedPHP, APPLE, PartialFed, and FedALA, these methods excessively focus on personalization but underestimate the significance of the generic information.When the data heterogeneity becomes moderate with Dir(0.5) for Tiny-ImageNet, they perform worse than FedAvg.Scalability.To show the scalability of FedALA, we conduct two experiments with 50 and 100 clients in the default heterogeneous setting.In Table 4, most of the pFL methods degenerate greatly as the client amount increases to 100, while FedALA drops less than 1% in accuracy.Since the data amount on Cifar100 is constant, the data amount on the client decreases with more clients.This aggravates the lack of local data, so precisely capturing the desired information in the global model becomes more critical.\n\n\nApplicability of ALA\n\nAs the ALA module only modifies the local initialization in FL, it can be applied to most existing FL methods.We apply ALA to the SOTA FL methods except for FedFomo and APPLE (as they download multiple client models) without modifying other learning processes to evaluate the effectiveness of ALA.We report the accuracy and improvements on Tiny-ImageNet and Cifar100 using the 4-layer CNN in default heterogeneous setting with s = 80 and p = 1.\n\nIn Table 4, the accuracy improvement for FedAvg and FedProx is apparent, and the improvement for Per-FedAvg, Ditto, and FedPHP is also remarkable.This indicates the applicability of ALA to the traditional FL and pFL methods.\n\nHowever, the improvement to other pFL methods is relatively small.In FedAMP, only the information in the important local models is emphasized through the attentioninducing function with the generic information in unimportant models ignored, so ALA can hardly find the ignored information back without modifying other learning processes.According to Table 1, the representation part mostly contains generic information, which is desired by the client.It leaves little room for ALA to take effect, but ALA still improves FedRep by more than 0.60% in accuracy.pFedMe learns a personalized model to approximate the local model at each local training batch, so it benefits little (0.57% on Ci-far100) from ALA.In PartialFed, the local aggregation happens in each training batch, so the initialized local model generated by ALA is later re-aggregated by its strategy, thus eliminating the effect of ALA.\n\n\nUpdate Direction Correction\n\n\nConclusion\n\nIn this paper, we propose an adaptive and fine-grained method FedALA in pFL to facilitate the local model training with the received global model.The extensive experiments demonstrate the effectiveness of FedALA.Our method outperforms eleven SOTA methods.Also, the ALA module in FedALA can improve other FL methods in accuracy.\n\n\nAdditional Details of weight learning in ALA\n\nHere, we omit the superscript t for all the notations.Given \u0398i := \u0398 i + (\u0398 \u2212 \u0398 i ) \u2299 [1 |\u0398i|\u2212p ; W p i ], we update W p i by Equation ( 7) shown as below with other learnable weights frozen, including the entire global model and entire local model.\nW p i \u2190 W p i \u2212 \u03b7\u2207 W p i L( \u0398i , D s i ; \u0398) = W p i \u2212 \u03b7 \u2202L i \u2202W p i = W p i \u2212 \u03b7 \u2202L i \u2202 \u0398p i \u2299 \u2202 \u0398p i \u2202W p i = W p i \u2212 \u03b7 \u2202L i \u2202 \u0398p i \u2299 (\u0398 \u2212 \u0398 i ) p ,(7)\nwhere L i represents L( \u0398i , D s i ; \u0398), \u0398p i and (\u0398 \u2212 \u0398 i ) p represent the higher layers of \u0398i and (\u0398 \u2212 \u0398 i ), respectively.The term \u2202Li \u2202 \u0398p i can be easily obtained with the backpropagation.Only the gradients of the model parameters in the higher layers are calculated in ALA.As shown in Figure 5, the training loss value of the global objective keeps decreasing when considering either the trained local models (green square dots) or the initialized local models before local training (red circle dots).When the number of global iterations becomes larger than 1000, the loss of the green dots and the red dots become almost the same, representing the convergence of FedALA.\n\n\nConvergence of FedALA\n\n\nThe Range of ALA\n\nFor different backbones, the ranges of ALA with different p are shown in Table 5.We use the notations from the corresponding papers.As p increases, the range increases, so the ALA module can cover more layers.\n\n\nEffect of p on CNN and fastText\n\nWith different p, the accuracy of FedALA and the number of learnable weights in ALA also varies.In Table 6, as p increases, the number of learnable weights also increases.However, the change in the accuracy is negligible.The accuracy reaches the best for the four-layer CNN and fastText when p = 1.\n\n\n\ngenerates an aggregated model for an individual client by the attention-inducing function and personalized aggregation.To utilize the historical local model, FedPHP (Li et al. 2021b) locally aggregates the global model and local model with the rule-based moving average and a pre-defined weight (hyperparameter).Fed-Fomo (Zhang et al. 2020) focuses on aggregating other client models locally for local initialization in each iteration and approximates the client-specific weights for aggregation using the local models from other clients.Similar to FedFomo, APPLE (Luo and Wu 2021) also aggregates client models locally but learns the weights instead of approximation and performs the local aggregation in each training batch rather than just local initialization.FedAMP and APPLE are proposed for the cross-silo FL setting, which require all clients to join each iteration.By learning the binary and layer-level aggregation strategy for each client, PartialFed (Sun et al. 2021) selects the parameters in the global model or the local model to construct the new local model in each batch.\n\n\nFigure 2 :\n2\nFigure 2: The learning process in ALA.LA denotes \"local aggregation\".Here, we consider a five-layer model and set p = 3.The lighter the color, the larger the value.\n\n\n\n\nFL baselines including FedAvg(McMahan et al. 2017), FedProx(Li et al. 2020), Per-FedAvg(Fallah, Mokhtari, and Ozdaglar 2020), FedRep(Collins et al. 2021), pFedMe (T Dinh, Tran, and Nguyen 2020), Ditto(Li et al. 2021a), FedAMP(Huang et al. 2021), FedPHP(Li et al.  2021b), FedFomo(Zhang et al. 2020), APPLE(Luo andWu 2021), and PartialFed (Sun et al. 2021).To show the superiority of weight learning in FedALA over additional local training steps, we also compare FedALA with FedAvg-C and FedProx-C, which locally fine-tune the global model for local initialization at each iteration.Then we apply ALA to SOTA FL methods and show that ALA can improve them.\n\n\nFigure 3 :\n3\nFigure 3: The local loss on client #8 regarding weight learning epochs in ALA on MNIST and Cifar10.Here, we train the weights for at least six epochs in each iteration.The details of the settings are described in Section .\n\n\n\n\ns = 10 s = 20 s = 40 s = 60 s = 80 s = 100 p = 6 p = 5 p = 4 p = 3 p = 2 p = 1 Acc.\n\n\nFigure 4 :\n4\nFigure 4: 2D visualization of local learning trajectory (from iteration 140 to 200) and the local loss surface on MNIST in the pathological heterogeneous setting.The red circles and green cubes represent the local model at the beginning and end of each iteration, respectively.The black and blue trajectories with the arrows represent FedAvg and FedALA, respectively.The local models are projected to the 2D plane using PCA.C1 and C2 are the two principal components generated by the PCA.\n\n\n\n\nRecall that our global objective is{ \u03981 , . . ., \u0398N } = arg min G(L 1 , . . ., L N ),(8)whereL i = L( \u0398i , D i ; \u0398), \u2200i \u2208 [N ]and L(\u2022) is the loss function.\u0398 is the global model, which brings external information to client i. Typically G(\u2022) is set to N i=1 k i L i , where k i = |D i |/ N j=1 |D j | and |D i | is the amount of local training samples on client i.\n\n\nFigure 6 :Figure 7 :Figure 8 :Figure 9 :\n6789\nFigure 6: The data distribution of each client on Tiny-ImageNet in three heterogeneous settings.The size of the circle represents the number of samples.With the \u03b2 in Dir(\u03b2) increasing, the degree of heterogeneity decreases.\n\n\n\n\nLocal learning process on client i in the t-th iteration.Specifically, client i downloads the global model from the server, locally aggregates it with the old local model by ALA module for local initialization, trains the local model, and finally uploads the trained local model to the server.\nby aggregating their local models. However, it often sufferse x i t > \u21e5 t iin statistically heterogeneous settings, e.g., FL with Non-IID and unbalanced data (Kairouz et al. 2019; Zhao et al.2018). To address this issue, FedProx (Li et al. 2020) im-proves the stability of the FL process through a proximalterm. To counteract the bias introduced by the Non-IID data,ClientFAVOR (Wang et al. 2020a) selects a subset of clients basedon deep Q-learning (Van Hasselt, Guez, and Silver 2016)at each iteration. By generating the global model layer-Figure 1: Aggregation (FedALA) to adaptively aggregates the down-wise, FedMA (Wang et al. 2020b) can adapt to statistical heterogeneity with the matched averaging approach. How-ever, with statistical heterogeneity in FL, it is hard to ob-tain a single global model which generalizes well to each client (Kairouz et al. 2019; Huang et al. 2021; T Dinh, Tran, and Nguyen 2020).loaded global model and local model towards the local ob-jective for local initialization. FedALA only downloads oneglobal model and uploads one local model on each clientwith the same communication overhead as in FedAvg, whichalso has fewer privacy concerns and is more communication-efficient than FedFomo and APPLE. By adaptively learningreal-value and element-wise aggregation weights towardsthe local objective on the full local dataset, FedALA cancapture the desired information in the global model at theelement level, which is more precise than the binary andlayer-wise weight learning in PartialFed. Since the lowerlayers in a deep neural network (DNN) learn more genericinformation than the higher layers (Yosinski et al. 2014;LeCun, Bengio, and Hinton 2015), we can further reducethe computation overhead by only applying Adaptive LocalAggregation (ALA) module on the higher layers. The wholelocal learning process is shown in Figure 1.To evaluate the effectiveness of FedALA, we conductextensive experiments on five benchmark datasets. Re-sults show that FedALA outperforms eleven state-of-the-art(SOTA) methods. Besides, we apply the ALA module to thetraditional FL methods and pFL methods to improve theirperformance. To sum up, our contributions are as follows.\u2022 We propose a novel pFL method FedALA that adaptivelyaggregates the global model and local model towards thelocal objective to capture the desired information fromthe global model in an element-wise manner.\u2022 We empirically show the effectiveness of FedALA,which outperforms eleven SOTA methods by up to3.27% in test accuracy without additional communica-tion overhead in each iteration.\u2022 Attributed to the minor modification of FL process, theALA module in FedALA can be directly applied to ex-isting FL methods to enhance their performance by up to24.19% in test accuracy on Cifar100.Related WorkTraditional Federated LearningThe widely known traditional FL method FedAvg (McMa-han et al. 2017) learns a single global model for all clients\n\n\n\nIn FedALA, we element-wisely aggregate the global model and local model instead of overwriting.Formally, \u0398t i\ncal model \u0398tt\u22121 ito obtain the initialized lo-\ni for local model training, i.e., \u0398t i := \u0398 t\u22121 .\n\n\n\n\nAlgorithm 1: FedALA Input: N clients, \u03c1: client joining ratio, L: loss function, \u0398 0 : initial global model, \u03b1: local learning rate, \u03b7: the learning rate in ALA, s%: the percent of local data in ALA, p: the range of ALA, and \u03c3(\u2022): clip function.Output: Reasonable local models \u03981 , . . ., \u0398N 1: Server sends \u0398 0 to all clients to initialize local models.\n2: Clients initialize W p i , \u2200i \u2208 [N ] to ones. 3: for iteration t = 1, . . . , T do 4: Server samples a subset I t of clients according to \u03c1. 5: Server sends \u0398 t\u22121 to |I t | clients. 6: for Client i \u2208 I t in parallel do 7: Client i samples s% of local data. \u25b7 ALA8: 9: 10: 11:if t = 2 then while W p i does not converge do \u25b7 Initial stage Client i trains W p i by Equation (5). Client i clips W p i using \u03c3(\u2022).12: 13:else if t > 2 then Client i trains W p i by Equation (5).16: 17:Client i obtains \u0398 t i by \u25b7 Local model training \u0398 t i \u2190 \u0398t Client i sends \u0398 t i to the server. \u25b7 Uploading18:\n14:Client i clips W p i using \u03c3(\u2022).15: Client i obtains \u0398t i by Equation (4).i \u2212 \u03b1\u2207 \u0398i L( \u0398t i , D i ; \u0398 t\u22121 ).Server obtains \u0398 t by \u0398 t \u2190 i\u2208I t ki j\u2208I t kj \u0398 t i .19: return \u03981 , . . ., \u0398N where L t i represents L( \u0398t i , D s,t i ; \u0398 t\u22121 ).Based on Equation (4), we can view updating W i as updating \u0398t i in ALA: \u0398t\n\n\nTable 1 :\n1\nThe test accuracy (%) and the number of trainable parameters (in millions) for ALA on TINY*.\n39.5340.6240.0240.2341.1141.9442.1141.7141.5441.6241.86 42.47 41.94Param.0.00511.182 11.172 11.024 10.499 8.399 0.005SettingsPathological heterogeneous settingPractical heterogeneous settingMethodsMNISTCifar10Cifar100Cifar10Cifar100TINYTINY*AG NewsFedAvg97.93\u00b10.05 55.09\u00b10.83 25.98\u00b10.13 59.16\u00b10.47 31.89\u00b10.47 19.46\u00b10.20 19.45\u00b10.13 79.57\u00b10.17FedProx98.01\u00b10.09 55.06\u00b10.75 25.94\u00b10.16 59.21\u00b10.40 31.99\u00b10.41 19.37\u00b10.22 19.27\u00b10.23 79.35\u00b10.23FedAvg-C99.79\u00b10.00 92.13\u00b10.03 66.17\u00b10.03 90.34\u00b10.01 51.80\u00b10.02 30.67\u00b10.08 36.94\u00b10.10 95.89\u00b10.25FedProx-C99.80\u00b10.04 92.12\u00b10.03 66.07\u00b10.08 90.33\u00b10.01 51.84\u00b10.07 30.77\u00b10.13 38.78\u00b10.52 96.10\u00b10.22Per-FedAvg 99.63\u00b10.02 89.63\u00b10.23 56.80\u00b10.26 87.74\u00b10.19 44.28\u00b10.33 25.07\u00b10.07 21.81\u00b10.54 93.27\u00b10.25FedRep99.77\u00b10.03 91.93\u00b10.14 67.56\u00b10.31 90.40\u00b10.24 52.39\u00b10.35 37.27\u00b10.20 39.95\u00b10.61 96.28\u00b10.14pFedMe99.75\u00b10.02 90.11\u00b10.10 58.20\u00b10.14 88.09\u00b10.32 47.34\u00b10.46 26.93\u00b10.19 33.44\u00b10.33 91.41\u00b10.22Ditto99.81\u00b10.00 92.39\u00b10.06 67.23\u00b10.07 90.59\u00b10.01 52.87\u00b10.64 32.15\u00b10.04 35.92\u00b10.43 95.45\u00b10.17FedAMP99.76\u00b10.02 90.79\u00b10.16 64.34\u00b10.37 88.70\u00b10.18 47.69\u00b10.49 27.99\u00b10.11 29.11\u00b10.15 94.18\u00b10.09FedPHP99.73\u00b10.00 90.01\u00b10.00 63.09\u00b10.04 88.92\u00b10.02 50.52\u00b10.16 35.69\u00b13.26 29.90\u00b10.51 94.38\u00b10.12FedFomo99.83\u00b10.00 91.85\u00b10.02 62.49\u00b10.22 88.06\u00b10.02 45.39\u00b10.45 26.33\u00b10.22 26.84\u00b10.11 95.84\u00b10.15APPLE99.75\u00b10.01 90.97\u00b10.05 65.80\u00b10.08 89.37\u00b10.11 53.22\u00b10.20 35.04\u00b10.47 39.93\u00b10.52 95.63\u00b10.21PartialFed99.86\u00b10.01 89.60\u00b10.13 61.39\u00b10.12 87.38\u00b10.08 48.81\u00b10.20 35.26\u00b10.18 37.50\u00b10.16 85.20\u00b10.16FedALA99.88\u00b10.01 92.44\u00b10.02 67.83\u00b10.06 90.67\u00b10.03 55.92\u00b10.03 40.54\u00b10.02 41.94\u00b10.05 96.52\u00b10.08\n\nTable 2 :\n2\nThe test accuracy (%) in the pathological heterogeneous setting and practical heterogeneous setting.\n\n\nTable 3 :\n3\nAggregating the models using the rule-based method is aimless in capturing the desired information in the global model, so FedPHP performs worse than FedRep and Ditto.The model-level personalized aggregation in FedAMP, FedFomo, and APPLE, as well as the The computation overhead on TINY* and the communication overhead (transmitted parameters per iteration).\u03a3 is the parameter amount in the backbone.\u03b1 f (\u03b1 f < 1) is the ratio of the parameters of the feature extractor in the backbone.M (M \u2265 1) is the number of the received client models on each client in FedFomo and APPLE.layer-level and binary selection in PartialFed, are imprecise, which may introduce undesired information in the global model to the local model.Furthermore, downloading multiple models on each client in each iteration has additional communication costs for FedFomo and APPLE.By adaptively learning the aggregating weights, FedALA can explicitly capture the desired information precisely in the global model to facilitate the local model training.\nComputationCommunicationTotal time Time/iter.Param./iter.FedAvg FedProx FedAvg-C FedProx-C Per-FedAvg FedRep pFedMe Ditto FedAMP FedPHP FedFomo APPLE PartialFed FedALA365 min 325 min 607 min 24.28 min 1.59 min 1.99 min 711 min 28.44 min 121 min 3.56 min 471 min 4.09 min 1157 min 10.24 min 318 min 11.78 min 92 min 1.53 min 264 min 4.06 min 193 min 2.72 min 132 min 2.93 min 693 min 2.13 min 7+116 min 1.93 min2  *  \u03a3 2  *  \u03a3 2  *  \u03a3 2  *  \u03a3 2  *  \u03a3 2  *  \u03b1 f  *  \u03a3 2  *  \u03a3 2  *  \u03a3 2  *  \u03a3 2  *  \u03a3 (1 + M )  *  \u03a3 (1 + M )  *  \u03a3 2  *  \u03a3 2  *  \u03a3\n\nTable 4 :\n4\nThe test accuracy (%) (and improvement (%)) on Tiny-ImageNet, AG News, and Cifar100.\nHeterogeneityScalabilityApplicability of ALADatasetsTiny-ImageNetAG NewsCifar100Tiny-ImageNetCifar100MethodsDir(0.01)Dir(0.5)Dir(1)50 clients100 clientsAcc.Imps.Acc.Imps.FedAvg15.70\u00b10.46 21.14\u00b10.47 87.12\u00b10.19 31.90\u00b10.27 31.95\u00b10.37 40.54\u00b10.17 21.08 55.92\u00b10.15 24.03FedProx15.66\u00b10.36 21.22\u00b10.47 87.21\u00b10.13 31.94\u00b10.30 31.97\u00b10.24 40.53\u00b10.26 21.16 56.18\u00b10.65 24.19FedAvg-C49.88\u00b10.11 16.21\u00b10.05 91.38\u00b10.21 49.82\u00b10.11 47.90\u00b10.12----FedProx-C49.84\u00b10.02 16.36\u00b10.19 92.03\u00b10.19 49.79\u00b10.14 48.02\u00b10.02----Per-FedAvg 39.39\u00b10.30 16.36\u00b10.13 87.08\u00b10.26 44.31\u00b10.20 36.07\u00b10.24 30.90\u00b10.28 5.83 48.68\u00b10.36 4.40FedRep55.43\u00b10.15 16.74\u00b10.09 92.25\u00b10.20 47.41\u00b10.18 44.61\u00b10.20 37.89\u00b10.31 0.62 53.02\u00b10.11 0.63pFedMe41.45\u00b10.14 17.48\u00b10.61 87.08\u00b10.18 48.36\u00b10.64 46.45\u00b10.18 27.30\u00b10.24 0.37 47.91\u00b10.21 0.57Ditto50.62\u00b10.02 18.98\u00b10.05 91.89\u00b10.17 54.22\u00b10.04 52.89\u00b10.22 40.75\u00b10.06 8.60 56.33\u00b10.07 3.46FedAMP48.42\u00b10.06 12.48\u00b10.21 83.35\u00b10.05 44.39\u00b10.35 40.43\u00b10.17 28.18\u00b10.20 0.19 48.03\u00b10.23 0.34FedPHP48.63\u00b10.02 21.09\u00b10.07 90.52\u00b10.19 52.44\u00b10.16 49.70\u00b10.31 40.16\u00b10.24 4.47 54.28\u00b10.21 3.76FedFomo46.36\u00b10.54 11.59\u00b10.11 91.20\u00b10.18 42.56\u00b10.33 38.91\u00b10.08----APPLE48.04\u00b10.10 24.28\u00b10.21 84.10\u00b10.18 55.06\u00b10.20 52.81\u00b10.29----PartialFed49.38\u00b10.02 24.20\u00b10.10 91.01\u00b10.28 48.95\u00b10.07 39.31\u00b10.01 35.40\u00b10.02 0.14 48.99\u00b10.05 0.18FedALA55.75\u00b10.02 27.85\u00b10.06 92.45\u00b10.10 55.61\u00b10.02 54.68\u00b10.57----\n\nTable 6 :\n6\nThe test accuracy (%) and number of learnable weights in ALA module on Tiny-ImageNet in the default heterogeneous setting.s is set to 80%.\nItemsFour-layer CNNfastTextp = 4p = 3p = 2p = 1p = 3p = 2 p = 1Acc.39.7539.8939.9440.5496.4596.40 96.52Param. 582026 581194 529930 5130 3157508 1188132\n\nTable 7 :\n7\nThe test accuracy (%) using the four-layer CNN on MNIST and Cifar100 in the practical setting with \u03b2 = 0.1\nDatasetsMNISTCifar100Client Amount20100\u03c1 = 1\u03c1 = 0.5  *FedAvg98.81\u00b10.01 39.51\u00b11.23FedProx98.82\u00b10.01 33.87\u00b12.39FedAvg-C99.65\u00b10.00 47.94\u00b10.26FedProx-C99.64\u00b10.00 48.11\u00b10.17Per-FedAvg98.90\u00b10.05 47.96\u00b10.83FedRep99.48\u00b10.02 41.48\u00b10.05pFedMe99.52\u00b10.02 43.27\u00b10.46Ditto99.64\u00b10.00 48.94\u00b10.04FedAMP99.47\u00b10.02-FedPHP99.58\u00b10.00 49.99\u00b10.73FedFomo99.33\u00b10.04 37.70\u00b10.10APPLE99.66\u00b10.02-PartialFed99.67\u00b10.01 36.49\u00b10.07FedALA99.71\u00b10.00 54.81\u00b10.03\nAcknowledgementsThis work was supported in part by National NSF of China (NO.61872234, 61732010), Shanghai Key Laboratory of Scalable Computing and Systems, Innovative Research Foundation of Ship General Performance (NO.25622114), the Key Laboratory of PK System Technologies Research of Hainan, Intel Corporation (UFunding 12679), and the Louisiana BoR LAMDA.Additional Results on MNISTIn addition to the results shown in Table2in the main body, we present the results on MNIST in the practical heterogeneous setting here, as shown in Table7.According to Table 7, FedALA still outperforms all the baselines on MNIST.Additional Results on Cifar100 with \u03c1 = 0.5Following pFedMe, we have shown the superiority of the FedALA on extensive experiments in the main body with \u03c1 = 1.0.Here, we conduct additional experiments on Ci-far100 (100 clients) with \u03c1 = 0.5, i.e., only half of the clients randomly join FL in each iteration.Besides, we only show the averaged results collected from the joining clients and denote it as \u03c1 = 0.5 * in Table7. FedAMP and APPLE are proposed for the cross-silo FL setting, and they require all clients to join each iteration.Thus, we do not compare them with other methods when \u03c1 = 0.5.Attributed to the adaptive module ALA, FedALA maintains its superiority.Hyperparameter SettingsWe tune all the hyperparameters in the default practical setting by grid search (the search range is included in [. ..]).The notations mentioned here are only related to the corresponding method.For FedProx, we set the parameter for proximal term \u00b5 to 0.001 (selecting from [0.1, 0.01, 0.001, 0.0001]).For Per-FedAvg, we set the step size \u03b1 equal to the local learning rate.For FedRep, we set the number of local updates \u03c4 = 5 (selecting from[1,2,3,4,5,6,7,8,9,10]) and set the step size \u03b1 the same as the local learning rate.For pFedMe, we set its personalized learning rate to 0.01 (selecting from [0.1, 0.01, 0.001, 0.0001]), the additional parameter \u03b2 to 1.0 (selecting from [0.1, 0.5, 0.9, 1.0]), the regularization parameter \u03bb to 15 (selecting from [0, 0.1, 1, 5, 10, 15, 20, 50]) and the number of local computation K to 5 (selecting from[1,5,10,20]).For Ditto, we set the number of local epochs for personalized model s = 2 (selecting from[1,2,3,4,5,6,7,8,9,10]) and the coefficient of proximal term \u03bb = 0.1 (selecting from [0.0001, 0.001, 0.01, 0.1, 1, 10]).For FedAMP, we set the step size of gradient descent \u03b1 k to 1000 (selecting from [10000, 1000, 100, 10, 1, 0.1]), the regularization parameter \u03bb to 1 (selecting from [100, 10, 1, 0.1, 0.01]) and the parameter for attention-inducing function \u03c3 to 0.1 (selecting from [1, 0.5, 0.1, 0.05, 0.01]).For FedPHP, we set the coefficient \u00b5 = 0.9 (selecting from [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) and \u03bb = 0.01 (selecting from [0.1, 0.05, 0.01, 0.005, 0.001]).For FedFomo, we set the number of received local models M to the total number of clients.For APPLE, we set the loss scheduler type to 'cos', directed relationship (DR) learning rate \u03b7 2 = 0.01 (selecting from [0.1, 0.05, 0.01, 0.005, 0.001]), \u00b5 = 0.1 (selecting from [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]), L = 0.2 (selecting from [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) and the number of received local models M to the total number of clients.For PartialFed, we initialize the temperature parameter \u03c4 = 5.0 and anneal it to 0 with the decay rate of 0.965, based on the original paper.Besides, we set the updating frequency f m = 8, f s = 2 (selecting from [(9, 1), (8, 2), (7, 3), (6, 4), (5, 5), (4, 6), (3, 7), (2, 8), (1, 9)]) for it.For FedALA, we set the weights learning rate \u03b7 = 1.0 (selecting from [0.1, 1.0, 10.0]), random sample percent s = 80 (selecting from[5,10,20,40,60,80,100]), ALA range p = 1 (selecting from [1, 2, ...] 1 ).Dataset URLsMNIST 2 ; Cifar10/100 3 ; Tiny-ImageNet 4 ; AG News 5 .Data Distribution VisualizationWe illustrate the data distributions in the experiments in Figure 6, Figure7, Figure8and Figure9.1 The maximum search range varies using different backbones 2 https://pytorch.org/vision/stable/datasets.html#mnist 3 https://pytorch.org/vision/stable/datasets.html#cifarl 4 http://cs231n.stanford.edu/tiny-imagenet-200.zip 5 https://pytorch.org/text/stable/datasets.html#ag-news\nWasserstein generative adversarial networks. M Arjovsky, S Chintala, L Bottou, ICML. 2017\n\nA Downsampled Variant of Imagenet as an Alternative to the Cifar Datasets. P Chrabaszcz, I Loshchilov, F Hutter, arXiv:1707.088192017arXiv preprint\n\nExploiting Shared Representations for Personalized Federated Learning. L Collins, H Hassani, A Mokhtari, S Shakkottai, ICML. 2021\n\nM Courbariaux, I Hubara, D Soudry, R El-Yaniv, Y Bengio, arXiv:1602.02830Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. 2016arXiv preprint\n\nPersonalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach. A Fallah, A Mokhtari, A Ozdaglar, NeurIPS. 2020\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. C Finn, P Abbeel, S Levine, K Zhang, X Ren, S Sun, J , ICML. He. 2017. 2016CVPR\n\nPersonalized Cross-Silo Federated Learning on Non-IID Data. Y Huang, L Chu, Z Zhou, L Wang, J Liu, J Pei, Y Zhang, AAAI. 2021\n\nBag of Tricks for Efficient Text Classification. A Joulin, E Grave, P Bojanowski, T Mikolov, EACL. 2017\n\nLearning Multiple Layers of Features From Tiny Images. P Kairouz, H B Mcmahan, B Avent, A Bellet, M Bennis, A N Bhagoji, K Bonawitz, Z Charles, G Cormode, R Cummings, arXiv:1912.04977Advances and Open Problems in Federated Learning. Citeseer2019. 2009arXiv preprint\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, nature. 52175532015\n\nGradient-based Learning Applied to Document Recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 86111998\n\nVisualizing the Loss Landscape of Neural Nets. H Li, Z Xu, G Taylor, C Studer, T Goldstein, CVPR. Neurips, Q Li, B He, D Song, 2018Model-Contrastive Federated Learning\n\nDitto: Fair and Robust Federated Learning Through Personalization. T Li, S Hu, A Beirami, V Smith, ICML. 2021a\n\nFedPHP: Federated Personalization with Inherited Private Models. T Li, A K Sahu, M Zaheer, M Sanjabi, A Talwalkar, V Smith, ECML PKDD. Mlsys, X.-C Li, D.-C Zhan, Y Shao, B Li, S Song, 2020Federated Optimization in Heterogeneous Networks\n\nEnsemble Distillation for Robust Model Fusion in Federated Learning. T Lin, L Kong, S U Stich, M Jaggi, NeurIPS. 2020\n\nJ Luo, S Wu, arXiv:2110.08394Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning. 2021arXiv preprint\n\nAdaptive Gradient Methods with Dynamic Bound of Learning Rate. L Luo, Y Xiong, Y Liu, X Sun, ICLR. 2018\n\nFedpaq: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization. B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, AISTATS. Aistats, A Reisizadeh, A Mokhtari, H Hassani, A Jadbabaie, R Pedarsani, 2017. 2020Communication-Efficient Learning of Deep Networks from Decentralized Data\n\nPartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization. A Shamsian, A Navon, E Fetaya, G Chechik, B Huo, H Yang, Y Bai, B Dinh, C Tran, N Nguyen, T D , ICML. Aaai, H Wang, Z Kaplan, D Niu, B Li, 2021. 2021. 2020. 2016Optimizing Federated Learning on Non-IID Data with Reinforcement Learning. In InfoComm\n\nTackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization. H Wang, M Yurochkin, Y Sun, D Papailiopoulos, Y Khazaeni, J Wang, Q Liu, H Liang, G Joshi, H V Poor, arXiv:2002.06440Federated learning with matched averaging. 2020b. 2020carXiv preprintIn NeurIPS\n\nJ Yosinski, J Clune, Y Bengio, H Lipson, How Transferable Are Features in Deep Neural Networks? In NeurIPS. 2014\n\nPersonalized Federated Learning with First Order Model Optimization. M Zhang, K Sapra, S Fidler, S Yeung, J M Alvarez, ICLR. 2020\n\nCharacter-level Convolutional Networks for Text Classification. X Zhang, J Zhao, Y Lecun, Neurips, Y Zhao, M Li, L Lai, N Suda, D Civin, V Chandra, arXiv:1806.00582learning with non-iid data. 2015. 2018arXiv preprint\n\nData-Free Knowledge Distillation for Heterogeneous Federated Learning. L Zhu, Z Liu, S Han, ICML. Neurips, Z Zhu, J Hong, J Zhou, 2019Deep leakage from gradients\n", "annotations": {"author": "[{\"end\":121,\"start\":74},{\"end\":176,\"start\":122},{\"end\":231,\"start\":177},{\"end\":273,\"start\":232},{\"end\":318,\"start\":274},{\"end\":360,\"start\":319},{\"end\":425,\"start\":361}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":83},{\"end\":130,\"start\":127},{\"end\":185,\"start\":181},{\"end\":240,\"start\":236},{\"end\":285,\"start\":282},{\"end\":327,\"start\":325},{\"end\":373,\"start\":369}]", "author_first_name": "[{\"end\":82,\"start\":74},{\"end\":126,\"start\":122},{\"end\":180,\"start\":177},{\"end\":235,\"start\":232},{\"end\":281,\"start\":274},{\"end\":324,\"start\":319},{\"end\":368,\"start\":361}]", "author_affiliation": "[{\"end\":120,\"start\":90},{\"end\":175,\"start\":148},{\"end\":230,\"start\":203},{\"end\":272,\"start\":242},{\"end\":317,\"start\":287},{\"end\":359,\"start\":329},{\"end\":424,\"start\":394}]", "title": "[{\"end\":71,\"start\":1},{\"end\":496,\"start\":426}]", "venue": null, "abstract": "[{\"end\":1551,\"start\":530}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2042,\"start\":2021},{\"end\":2065,\"start\":2042},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2095,\"start\":2065},{\"end\":2325,\"start\":2297},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2628,\"start\":2591},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2661,\"start\":2640},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2794,\"start\":2777},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2911,\"start\":2892},{\"end\":2938,\"start\":2921},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2967,\"start\":2948},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2993,\"start\":2976},{\"end\":3026,\"start\":2998},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4233,\"start\":4209},{\"end\":5236,\"start\":5232},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9735,\"start\":9715},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9953,\"start\":9916},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10053,\"start\":10022},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10210,\"start\":10190},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10552,\"start\":10535},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10924,\"start\":10905},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19843,\"start\":19818},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19859,\"start\":19843},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19982,\"start\":19945},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20125,\"start\":20103},{\"end\":20150,\"start\":20125},{\"end\":23185,\"start\":23160},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23287,\"start\":23258},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23320,\"start\":23301},{\"end\":23418,\"start\":23393},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23963,\"start\":23942},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23984,\"start\":23963},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24199,\"start\":24182},{\"end\":24221,\"start\":24199},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24422,\"start\":24405},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24440,\"start\":24422},{\"end\":25865,\"start\":25849},{\"end\":29424,\"start\":29377},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31118,\"start\":31099},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31134,\"start\":31118},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37309,\"start\":37288},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37334,\"start\":37318},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":37383,\"start\":37346},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":37412,\"start\":37391},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":37476,\"start\":37459},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37503,\"start\":37484},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37557,\"start\":37538},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37585,\"start\":37572}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37075,\"start\":35982},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37255,\"start\":37076},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37915,\"start\":37256},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38153,\"start\":37916},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38241,\"start\":38154},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38745,\"start\":38242},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39113,\"start\":38746},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39385,\"start\":39114},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42617,\"start\":39386},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42828,\"start\":42618},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44098,\"start\":42829},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45769,\"start\":44099},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45884,\"start\":45770},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47464,\"start\":45885},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":48899,\"start\":47465},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":49203,\"start\":48900},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49749,\"start\":49204}]", "paragraph": "[{\"end\":2096,\"start\":1567},{\"end\":3027,\"start\":2098},{\"end\":3590,\"start\":3029},{\"end\":4990,\"start\":3592},{\"end\":5251,\"start\":4992},{\"end\":5844,\"start\":5253},{\"end\":9842,\"start\":9615},{\"end\":10354,\"start\":9844},{\"end\":10682,\"start\":10356},{\"end\":11374,\"start\":10684},{\"end\":11925,\"start\":11403},{\"end\":11987,\"start\":11982},{\"end\":12832,\"start\":12295},{\"end\":19678,\"start\":19326},{\"end\":20013,\"start\":19727},{\"end\":20449,\"start\":20015},{\"end\":20752,\"start\":20513},{\"end\":21055,\"start\":20754},{\"end\":21344,\"start\":21113},{\"end\":21906,\"start\":21346},{\"end\":21987,\"start\":21931},{\"end\":22141,\"start\":22007},{\"end\":22680,\"start\":22253},{\"end\":22879,\"start\":22715},{\"end\":23814,\"start\":22881},{\"end\":24441,\"start\":23816},{\"end\":24896,\"start\":24443},{\"end\":25081,\"start\":24898},{\"end\":25443,\"start\":25111},{\"end\":25530,\"start\":25445},{\"end\":26161,\"start\":25532},{\"end\":26532,\"start\":26201},{\"end\":28324,\"start\":26534},{\"end\":29851,\"start\":28326},{\"end\":30656,\"start\":29853},{\"end\":31193,\"start\":30658},{\"end\":32299,\"start\":31195},{\"end\":32768,\"start\":32324},{\"end\":32994,\"start\":32770},{\"end\":33893,\"start\":32996},{\"end\":34265,\"start\":33938},{\"end\":34562,\"start\":34314},{\"end\":35393,\"start\":34715},{\"end\":35647,\"start\":35438},{\"end\":35981,\"start\":35683},{\"end\":37074,\"start\":35985},{\"end\":37254,\"start\":37090},{\"end\":37914,\"start\":37259},{\"end\":38152,\"start\":37930},{\"end\":38240,\"start\":38157},{\"end\":38744,\"start\":38256},{\"end\":39112,\"start\":38749},{\"end\":39384,\"start\":39161},{\"end\":39682,\"start\":39389},{\"end\":42730,\"start\":42621},{\"end\":42827,\"start\":42778},{\"end\":43186,\"start\":42832},{\"end\":44097,\"start\":43781},{\"end\":44204,\"start\":44112},{\"end\":45883,\"start\":45783},{\"end\":46920,\"start\":45898},{\"end\":47562,\"start\":47478},{\"end\":49051,\"start\":48913},{\"end\":49323,\"start\":49217}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9580,\"start\":5845},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11981,\"start\":11926},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12259,\"start\":11988},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19251,\"start\":12833},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19325,\"start\":19251},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19726,\"start\":19679},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20512,\"start\":20450},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21112,\"start\":21056},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21930,\"start\":21907},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22189,\"start\":22142},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22252,\"start\":22189},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34714,\"start\":34563}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25135,\"start\":25134},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25665,\"start\":25664},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28151,\"start\":28150},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29958,\"start\":29957},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30758,\"start\":30757},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31168,\"start\":31167},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":31426,\"start\":31425},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":31937,\"start\":31936},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32780,\"start\":32779},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33352,\"start\":33351},{\"end\":35518,\"start\":35517},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":35789,\"start\":35788}]", "section_header": "[{\"end\":1565,\"start\":1553},{\"end\":9613,\"start\":9582},{\"end\":11401,\"start\":11377},{\"end\":12293,\"start\":12261},{\"end\":22005,\"start\":21990},{\"end\":22713,\"start\":22683},{\"end\":25109,\"start\":25084},{\"end\":26199,\"start\":26164},{\"end\":32322,\"start\":32302},{\"end\":33923,\"start\":33896},{\"end\":33936,\"start\":33926},{\"end\":34312,\"start\":34268},{\"end\":35417,\"start\":35396},{\"end\":35436,\"start\":35420},{\"end\":35681,\"start\":35650},{\"end\":37087,\"start\":37077},{\"end\":37927,\"start\":37917},{\"end\":38253,\"start\":38243},{\"end\":39155,\"start\":39115},{\"end\":44109,\"start\":44100},{\"end\":45780,\"start\":45771},{\"end\":45895,\"start\":45886},{\"end\":47475,\"start\":47466},{\"end\":48910,\"start\":48901},{\"end\":49214,\"start\":49205}]", "table": "[{\"end\":42617,\"start\":39683},{\"end\":42777,\"start\":42731},{\"end\":43780,\"start\":43187},{\"end\":45769,\"start\":44205},{\"end\":47464,\"start\":46921},{\"end\":48899,\"start\":47563},{\"end\":49203,\"start\":49052},{\"end\":49749,\"start\":49324}]", "figure_caption": "[{\"end\":37075,\"start\":35984},{\"end\":37255,\"start\":37089},{\"end\":37915,\"start\":37258},{\"end\":38153,\"start\":37929},{\"end\":38241,\"start\":38156},{\"end\":38745,\"start\":38255},{\"end\":39113,\"start\":38748},{\"end\":39385,\"start\":39160},{\"end\":39683,\"start\":39388},{\"end\":42731,\"start\":42620},{\"end\":43187,\"start\":42831},{\"end\":44205,\"start\":44111},{\"end\":45884,\"start\":45782},{\"end\":46921,\"start\":45897},{\"end\":47563,\"start\":47477},{\"end\":49052,\"start\":48912},{\"end\":49324,\"start\":49216}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12680,\"start\":12679},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21676,\"start\":21675},{\"end\":35015,\"start\":35014}]", "bib_author_first_name": "[{\"end\":54080,\"start\":54079},{\"end\":54092,\"start\":54091},{\"end\":54104,\"start\":54103},{\"end\":54201,\"start\":54200},{\"end\":54215,\"start\":54214},{\"end\":54229,\"start\":54228},{\"end\":54346,\"start\":54345},{\"end\":54357,\"start\":54356},{\"end\":54368,\"start\":54367},{\"end\":54380,\"start\":54379},{\"end\":54406,\"start\":54405},{\"end\":54421,\"start\":54420},{\"end\":54431,\"start\":54430},{\"end\":54441,\"start\":54440},{\"end\":54453,\"start\":54452},{\"end\":54712,\"start\":54711},{\"end\":54722,\"start\":54721},{\"end\":54734,\"start\":54733},{\"end\":54828,\"start\":54827},{\"end\":54836,\"start\":54835},{\"end\":54846,\"start\":54845},{\"end\":54856,\"start\":54855},{\"end\":54865,\"start\":54864},{\"end\":54872,\"start\":54871},{\"end\":54879,\"start\":54878},{\"end\":54969,\"start\":54968},{\"end\":54978,\"start\":54977},{\"end\":54985,\"start\":54984},{\"end\":54993,\"start\":54992},{\"end\":55001,\"start\":55000},{\"end\":55008,\"start\":55007},{\"end\":55015,\"start\":55014},{\"end\":55085,\"start\":55084},{\"end\":55095,\"start\":55094},{\"end\":55104,\"start\":55103},{\"end\":55118,\"start\":55117},{\"end\":55196,\"start\":55195},{\"end\":55207,\"start\":55206},{\"end\":55209,\"start\":55208},{\"end\":55220,\"start\":55219},{\"end\":55229,\"start\":55228},{\"end\":55239,\"start\":55238},{\"end\":55249,\"start\":55248},{\"end\":55251,\"start\":55250},{\"end\":55262,\"start\":55261},{\"end\":55274,\"start\":55273},{\"end\":55285,\"start\":55284},{\"end\":55296,\"start\":55295},{\"end\":55423,\"start\":55422},{\"end\":55432,\"start\":55431},{\"end\":55442,\"start\":55441},{\"end\":55530,\"start\":55529},{\"end\":55539,\"start\":55538},{\"end\":55549,\"start\":55548},{\"end\":55559,\"start\":55558},{\"end\":55652,\"start\":55651},{\"end\":55658,\"start\":55657},{\"end\":55664,\"start\":55663},{\"end\":55674,\"start\":55673},{\"end\":55684,\"start\":55683},{\"end\":55712,\"start\":55711},{\"end\":55718,\"start\":55717},{\"end\":55724,\"start\":55723},{\"end\":55841,\"start\":55840},{\"end\":55847,\"start\":55846},{\"end\":55853,\"start\":55852},{\"end\":55864,\"start\":55863},{\"end\":55951,\"start\":55950},{\"end\":55957,\"start\":55956},{\"end\":55959,\"start\":55958},{\"end\":55967,\"start\":55966},{\"end\":55977,\"start\":55976},{\"end\":55988,\"start\":55987},{\"end\":56001,\"start\":56000},{\"end\":56031,\"start\":56027},{\"end\":56040,\"start\":56036},{\"end\":56048,\"start\":56047},{\"end\":56056,\"start\":56055},{\"end\":56062,\"start\":56061},{\"end\":56193,\"start\":56192},{\"end\":56200,\"start\":56199},{\"end\":56208,\"start\":56207},{\"end\":56210,\"start\":56209},{\"end\":56219,\"start\":56218},{\"end\":56243,\"start\":56242},{\"end\":56250,\"start\":56249},{\"end\":56436,\"start\":56435},{\"end\":56443,\"start\":56442},{\"end\":56452,\"start\":56451},{\"end\":56459,\"start\":56458},{\"end\":56580,\"start\":56579},{\"end\":56591,\"start\":56590},{\"end\":56600,\"start\":56599},{\"end\":56610,\"start\":56609},{\"end\":56621,\"start\":56620},{\"end\":56623,\"start\":56622},{\"end\":56650,\"start\":56649},{\"end\":56664,\"start\":56663},{\"end\":56676,\"start\":56675},{\"end\":56687,\"start\":56686},{\"end\":56700,\"start\":56699},{\"end\":56883,\"start\":56882},{\"end\":56895,\"start\":56894},{\"end\":56904,\"start\":56903},{\"end\":56914,\"start\":56913},{\"end\":56925,\"start\":56924},{\"end\":56932,\"start\":56931},{\"end\":56940,\"start\":56939},{\"end\":56947,\"start\":56946},{\"end\":56955,\"start\":56954},{\"end\":56963,\"start\":56962},{\"end\":56973,\"start\":56972},{\"end\":56975,\"start\":56974},{\"end\":56991,\"start\":56990},{\"end\":56999,\"start\":56998},{\"end\":57009,\"start\":57008},{\"end\":57016,\"start\":57015},{\"end\":57218,\"start\":57217},{\"end\":57226,\"start\":57225},{\"end\":57239,\"start\":57238},{\"end\":57246,\"start\":57245},{\"end\":57264,\"start\":57263},{\"end\":57276,\"start\":57275},{\"end\":57284,\"start\":57283},{\"end\":57291,\"start\":57290},{\"end\":57300,\"start\":57299},{\"end\":57309,\"start\":57308},{\"end\":57311,\"start\":57310},{\"end\":57416,\"start\":57415},{\"end\":57428,\"start\":57427},{\"end\":57437,\"start\":57436},{\"end\":57447,\"start\":57446},{\"end\":57599,\"start\":57598},{\"end\":57608,\"start\":57607},{\"end\":57617,\"start\":57616},{\"end\":57627,\"start\":57626},{\"end\":57636,\"start\":57635},{\"end\":57638,\"start\":57637},{\"end\":57725,\"start\":57724},{\"end\":57734,\"start\":57733},{\"end\":57742,\"start\":57741},{\"end\":57760,\"start\":57759},{\"end\":57768,\"start\":57767},{\"end\":57774,\"start\":57773},{\"end\":57781,\"start\":57780},{\"end\":57789,\"start\":57788},{\"end\":57798,\"start\":57797},{\"end\":57950,\"start\":57949},{\"end\":57957,\"start\":57956},{\"end\":57964,\"start\":57963},{\"end\":57986,\"start\":57985},{\"end\":57993,\"start\":57992},{\"end\":58001,\"start\":58000}]", "bib_author_last_name": "[{\"end\":54089,\"start\":54081},{\"end\":54101,\"start\":54093},{\"end\":54111,\"start\":54105},{\"end\":54212,\"start\":54202},{\"end\":54226,\"start\":54216},{\"end\":54236,\"start\":54230},{\"end\":54354,\"start\":54347},{\"end\":54365,\"start\":54358},{\"end\":54377,\"start\":54369},{\"end\":54391,\"start\":54381},{\"end\":54418,\"start\":54407},{\"end\":54428,\"start\":54422},{\"end\":54438,\"start\":54432},{\"end\":54450,\"start\":54442},{\"end\":54460,\"start\":54454},{\"end\":54719,\"start\":54713},{\"end\":54731,\"start\":54723},{\"end\":54743,\"start\":54735},{\"end\":54833,\"start\":54829},{\"end\":54843,\"start\":54837},{\"end\":54853,\"start\":54847},{\"end\":54862,\"start\":54857},{\"end\":54869,\"start\":54866},{\"end\":54876,\"start\":54873},{\"end\":54975,\"start\":54970},{\"end\":54982,\"start\":54979},{\"end\":54990,\"start\":54986},{\"end\":54998,\"start\":54994},{\"end\":55005,\"start\":55002},{\"end\":55012,\"start\":55009},{\"end\":55021,\"start\":55016},{\"end\":55092,\"start\":55086},{\"end\":55101,\"start\":55096},{\"end\":55115,\"start\":55105},{\"end\":55126,\"start\":55119},{\"end\":55204,\"start\":55197},{\"end\":55217,\"start\":55210},{\"end\":55226,\"start\":55221},{\"end\":55236,\"start\":55230},{\"end\":55246,\"start\":55240},{\"end\":55259,\"start\":55252},{\"end\":55271,\"start\":55263},{\"end\":55282,\"start\":55275},{\"end\":55293,\"start\":55286},{\"end\":55305,\"start\":55297},{\"end\":55429,\"start\":55424},{\"end\":55439,\"start\":55433},{\"end\":55449,\"start\":55443},{\"end\":55536,\"start\":55531},{\"end\":55546,\"start\":55540},{\"end\":55556,\"start\":55550},{\"end\":55567,\"start\":55560},{\"end\":55655,\"start\":55653},{\"end\":55661,\"start\":55659},{\"end\":55671,\"start\":55665},{\"end\":55681,\"start\":55675},{\"end\":55694,\"start\":55685},{\"end\":55709,\"start\":55702},{\"end\":55715,\"start\":55713},{\"end\":55721,\"start\":55719},{\"end\":55729,\"start\":55725},{\"end\":55844,\"start\":55842},{\"end\":55850,\"start\":55848},{\"end\":55861,\"start\":55854},{\"end\":55870,\"start\":55865},{\"end\":55954,\"start\":55952},{\"end\":55964,\"start\":55960},{\"end\":55974,\"start\":55968},{\"end\":55985,\"start\":55978},{\"end\":55998,\"start\":55989},{\"end\":56007,\"start\":56002},{\"end\":56025,\"start\":56020},{\"end\":56034,\"start\":56032},{\"end\":56045,\"start\":56041},{\"end\":56053,\"start\":56049},{\"end\":56059,\"start\":56057},{\"end\":56067,\"start\":56063},{\"end\":56197,\"start\":56194},{\"end\":56205,\"start\":56201},{\"end\":56216,\"start\":56211},{\"end\":56225,\"start\":56220},{\"end\":56247,\"start\":56244},{\"end\":56253,\"start\":56251},{\"end\":56440,\"start\":56437},{\"end\":56449,\"start\":56444},{\"end\":56456,\"start\":56453},{\"end\":56463,\"start\":56460},{\"end\":56588,\"start\":56581},{\"end\":56597,\"start\":56592},{\"end\":56607,\"start\":56601},{\"end\":56618,\"start\":56611},{\"end\":56629,\"start\":56624},{\"end\":56647,\"start\":56640},{\"end\":56661,\"start\":56651},{\"end\":56673,\"start\":56665},{\"end\":56684,\"start\":56677},{\"end\":56697,\"start\":56688},{\"end\":56710,\"start\":56701},{\"end\":56892,\"start\":56884},{\"end\":56901,\"start\":56896},{\"end\":56911,\"start\":56905},{\"end\":56922,\"start\":56915},{\"end\":56929,\"start\":56926},{\"end\":56937,\"start\":56933},{\"end\":56944,\"start\":56941},{\"end\":56952,\"start\":56948},{\"end\":56960,\"start\":56956},{\"end\":56970,\"start\":56964},{\"end\":56988,\"start\":56984},{\"end\":56996,\"start\":56992},{\"end\":57006,\"start\":57000},{\"end\":57013,\"start\":57010},{\"end\":57019,\"start\":57017},{\"end\":57223,\"start\":57219},{\"end\":57236,\"start\":57227},{\"end\":57243,\"start\":57240},{\"end\":57261,\"start\":57247},{\"end\":57273,\"start\":57265},{\"end\":57281,\"start\":57277},{\"end\":57288,\"start\":57285},{\"end\":57297,\"start\":57292},{\"end\":57306,\"start\":57301},{\"end\":57316,\"start\":57312},{\"end\":57425,\"start\":57417},{\"end\":57434,\"start\":57429},{\"end\":57444,\"start\":57438},{\"end\":57454,\"start\":57448},{\"end\":57605,\"start\":57600},{\"end\":57614,\"start\":57609},{\"end\":57624,\"start\":57618},{\"end\":57633,\"start\":57628},{\"end\":57646,\"start\":57639},{\"end\":57731,\"start\":57726},{\"end\":57739,\"start\":57735},{\"end\":57748,\"start\":57743},{\"end\":57757,\"start\":57750},{\"end\":57765,\"start\":57761},{\"end\":57771,\"start\":57769},{\"end\":57778,\"start\":57775},{\"end\":57786,\"start\":57782},{\"end\":57795,\"start\":57790},{\"end\":57806,\"start\":57799},{\"end\":57954,\"start\":57951},{\"end\":57961,\"start\":57958},{\"end\":57968,\"start\":57965},{\"end\":57983,\"start\":57976},{\"end\":57990,\"start\":57987},{\"end\":57998,\"start\":57994},{\"end\":58006,\"start\":58002}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2057420},\"end\":54123,\"start\":54034},{\"attributes\":{\"doi\":\"arXiv:1707.08819\",\"id\":\"b1\"},\"end\":54272,\"start\":54125},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":231924497},\"end\":54403,\"start\":54274},{\"attributes\":{\"doi\":\"arXiv:1602.02830\",\"id\":\"b3\"},\"end\":54607,\"start\":54405},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":227276412},\"end\":54758,\"start\":54609},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6719686},\"end\":54906,\"start\":54760},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":227311284},\"end\":55033,\"start\":54908},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1210515},\"end\":55138,\"start\":55035},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b8\",\"matched_paper_id\":18268744},\"end\":55405,\"start\":55140},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1779661},\"end\":55470,\"start\":55407},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14542261},\"end\":55602,\"start\":55472},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3693334},\"end\":55771,\"start\":55604},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":235446706},\"end\":55883,\"start\":55773},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236325014},\"end\":56121,\"start\":55885},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":219636007},\"end\":56240,\"start\":56123},{\"attributes\":{\"doi\":\"arXiv:2110.08394\",\"id\":\"b15\"},\"end\":56370,\"start\":56242},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":67856101},\"end\":56475,\"start\":56372},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":203593931},\"end\":56795,\"start\":56477},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":245022219},\"end\":57129,\"start\":56797},{\"attributes\":{\"doi\":\"arXiv:2002.06440\",\"id\":\"b19\",\"matched_paper_id\":220525591},\"end\":57413,\"start\":57131},{\"attributes\":{\"id\":\"b20\"},\"end\":57527,\"start\":57415},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":229188065},\"end\":57658,\"start\":57529},{\"attributes\":{\"doi\":\"arXiv:1806.00582\",\"id\":\"b22\",\"matched_paper_id\":368182},\"end\":57876,\"start\":57660},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":235125689},\"end\":58039,\"start\":57878}]", "bib_title": "[{\"end\":54077,\"start\":54034},{\"end\":54343,\"start\":54274},{\"end\":54709,\"start\":54609},{\"end\":54825,\"start\":54760},{\"end\":54966,\"start\":54908},{\"end\":55082,\"start\":55035},{\"end\":55193,\"start\":55140},{\"end\":55420,\"start\":55407},{\"end\":55527,\"start\":55472},{\"end\":55649,\"start\":55604},{\"end\":55838,\"start\":55773},{\"end\":55948,\"start\":55885},{\"end\":56190,\"start\":56123},{\"end\":56433,\"start\":56372},{\"end\":56577,\"start\":56477},{\"end\":56880,\"start\":56797},{\"end\":57215,\"start\":57131},{\"end\":57596,\"start\":57529},{\"end\":57722,\"start\":57660},{\"end\":57947,\"start\":57878}]", "bib_author": "[{\"end\":54091,\"start\":54079},{\"end\":54103,\"start\":54091},{\"end\":54113,\"start\":54103},{\"end\":54214,\"start\":54200},{\"end\":54228,\"start\":54214},{\"end\":54238,\"start\":54228},{\"end\":54356,\"start\":54345},{\"end\":54367,\"start\":54356},{\"end\":54379,\"start\":54367},{\"end\":54393,\"start\":54379},{\"end\":54420,\"start\":54405},{\"end\":54430,\"start\":54420},{\"end\":54440,\"start\":54430},{\"end\":54452,\"start\":54440},{\"end\":54462,\"start\":54452},{\"end\":54721,\"start\":54711},{\"end\":54733,\"start\":54721},{\"end\":54745,\"start\":54733},{\"end\":54835,\"start\":54827},{\"end\":54845,\"start\":54835},{\"end\":54855,\"start\":54845},{\"end\":54864,\"start\":54855},{\"end\":54871,\"start\":54864},{\"end\":54878,\"start\":54871},{\"end\":54882,\"start\":54878},{\"end\":54977,\"start\":54968},{\"end\":54984,\"start\":54977},{\"end\":54992,\"start\":54984},{\"end\":55000,\"start\":54992},{\"end\":55007,\"start\":55000},{\"end\":55014,\"start\":55007},{\"end\":55023,\"start\":55014},{\"end\":55094,\"start\":55084},{\"end\":55103,\"start\":55094},{\"end\":55117,\"start\":55103},{\"end\":55128,\"start\":55117},{\"end\":55206,\"start\":55195},{\"end\":55219,\"start\":55206},{\"end\":55228,\"start\":55219},{\"end\":55238,\"start\":55228},{\"end\":55248,\"start\":55238},{\"end\":55261,\"start\":55248},{\"end\":55273,\"start\":55261},{\"end\":55284,\"start\":55273},{\"end\":55295,\"start\":55284},{\"end\":55307,\"start\":55295},{\"end\":55431,\"start\":55422},{\"end\":55441,\"start\":55431},{\"end\":55451,\"start\":55441},{\"end\":55538,\"start\":55529},{\"end\":55548,\"start\":55538},{\"end\":55558,\"start\":55548},{\"end\":55569,\"start\":55558},{\"end\":55657,\"start\":55651},{\"end\":55663,\"start\":55657},{\"end\":55673,\"start\":55663},{\"end\":55683,\"start\":55673},{\"end\":55696,\"start\":55683},{\"end\":55846,\"start\":55840},{\"end\":55852,\"start\":55846},{\"end\":55863,\"start\":55852},{\"end\":55872,\"start\":55863},{\"end\":55956,\"start\":55950},{\"end\":55966,\"start\":55956},{\"end\":55976,\"start\":55966},{\"end\":55987,\"start\":55976},{\"end\":56000,\"start\":55987},{\"end\":56009,\"start\":56000},{\"end\":56199,\"start\":56192},{\"end\":56207,\"start\":56199},{\"end\":56218,\"start\":56207},{\"end\":56227,\"start\":56218},{\"end\":56249,\"start\":56242},{\"end\":56255,\"start\":56249},{\"end\":56442,\"start\":56435},{\"end\":56451,\"start\":56442},{\"end\":56458,\"start\":56451},{\"end\":56465,\"start\":56458},{\"end\":56590,\"start\":56579},{\"end\":56599,\"start\":56590},{\"end\":56609,\"start\":56599},{\"end\":56620,\"start\":56609},{\"end\":56631,\"start\":56620},{\"end\":56894,\"start\":56882},{\"end\":56903,\"start\":56894},{\"end\":56913,\"start\":56903},{\"end\":56924,\"start\":56913},{\"end\":56931,\"start\":56924},{\"end\":56939,\"start\":56931},{\"end\":56946,\"start\":56939},{\"end\":56954,\"start\":56946},{\"end\":56962,\"start\":56954},{\"end\":56972,\"start\":56962},{\"end\":56978,\"start\":56972},{\"end\":57225,\"start\":57217},{\"end\":57238,\"start\":57225},{\"end\":57245,\"start\":57238},{\"end\":57263,\"start\":57245},{\"end\":57275,\"start\":57263},{\"end\":57283,\"start\":57275},{\"end\":57290,\"start\":57283},{\"end\":57299,\"start\":57290},{\"end\":57308,\"start\":57299},{\"end\":57318,\"start\":57308},{\"end\":57427,\"start\":57415},{\"end\":57436,\"start\":57427},{\"end\":57446,\"start\":57436},{\"end\":57456,\"start\":57446},{\"end\":57607,\"start\":57598},{\"end\":57616,\"start\":57607},{\"end\":57626,\"start\":57616},{\"end\":57635,\"start\":57626},{\"end\":57648,\"start\":57635},{\"end\":57733,\"start\":57724},{\"end\":57741,\"start\":57733},{\"end\":57750,\"start\":57741},{\"end\":57759,\"start\":57750},{\"end\":57767,\"start\":57759},{\"end\":57773,\"start\":57767},{\"end\":57780,\"start\":57773},{\"end\":57788,\"start\":57780},{\"end\":57797,\"start\":57788},{\"end\":57808,\"start\":57797},{\"end\":57956,\"start\":57949},{\"end\":57963,\"start\":57956},{\"end\":57970,\"start\":57963}]", "bib_venue": "[{\"end\":54117,\"start\":54113},{\"end\":54198,\"start\":54125},{\"end\":54397,\"start\":54393},{\"end\":54587,\"start\":54478},{\"end\":54752,\"start\":54745},{\"end\":54890,\"start\":54882},{\"end\":55027,\"start\":55023},{\"end\":55132,\"start\":55128},{\"end\":55371,\"start\":55323},{\"end\":55457,\"start\":55451},{\"end\":55592,\"start\":55569},{\"end\":55700,\"start\":55696},{\"end\":55876,\"start\":55872},{\"end\":56018,\"start\":56009},{\"end\":56234,\"start\":56227},{\"end\":56350,\"start\":56271},{\"end\":56469,\"start\":56465},{\"end\":56638,\"start\":56631},{\"end\":56982,\"start\":56978},{\"end\":57375,\"start\":57334},{\"end\":57521,\"start\":57456},{\"end\":57652,\"start\":57648},{\"end\":57850,\"start\":57824},{\"end\":57974,\"start\":57970}]"}}}, "year": 2023, "month": 12, "day": 17}